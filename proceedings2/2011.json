[
    {
        "title": "Iranian Traditional Music Dastgah Classification.",
        "author": [
            "Sajjad Abdoli"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1417425",
        "url": "https://doi.org/10.5281/zenodo.1417425",
        "ee": "https://zenodo.org/records/1417425/files/Abdoli11.pdf",
        "abstract": "In this study, a system for Iranian traditional music Dastgah classification is presented. Persian music is based upon a set of seven major Dastgahs. The Dastgah in Persian music is similar to western musical scales and also Maqams in Turkish and Arabic music. Fuzzy logic type 2 as the basic part of our system has been used for modeling the uncertainty of tuning the scale steps of each Dastgah. The method assumes each performed note as a Fuzzy Set (FS), so each musical piece is a set of FSs. The maximum similarity between this set and theoretical data indicates the desirable Dastgah. In this study, a collection of small-sized dataset for Persian music is also given. The results indicate that the system works accurately on the dataset.",
        "zenodo_id": 1417425,
        "dblp_key": "conf/ismir/Abdoli11",
        "keywords": [
            "Dastgah classification",
            "Iranian traditional music",
            "Fuzzy logic type 2",
            "uncertainty modeling",
            "scale tuning",
            "Maqams",
            "Persian music",
            "seven major Dastgahs",
            "Fuzzy Set (FS)",
            "theoretical data"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nIRANIAN TRADITIONAL MUSIC DASTGAH CLASSIFICATION  \nSajjadAbdoli  \nComputer Department,  Islamic Azad University, \nCentral Tehran Branch, Tehran , Iran  \nSaj.abdoli@gmail.com  \nABSTRACT  \nIn this study, a system for Iranian traditional music Dastgah \nclassification  is presented . Persian music is based upon a set \nof seven major Dastgahs.  The Das tgah in Persian music is \nsimilar to western musical scales and  also Maqams in \nTurkish and Arabic music.  Fuzzy logic type 2 as the basic \npart of our system  has been used  for modeling the \nuncertainty of tuning the scale steps of each D astgah . The \nmethod assum es each performed note as a Fuzzy Set (FS) , so \neach musical piece is a set of FSs. The maximum similarity \nbetween this set and theoretical data indicates the desirable \nDastgah.  In this study, a collect ion of small -sized dataset for \nPersian music  is also given . The result s indicate that the \nsystem work s accurately  on the  dataset.  \n1. INTRODUCTION  \nMusic Information Retrieval (MIR) has grown in many fields \nbut, there is still a significant gap between western and non -\nwestern, especially mid dle-eastern, MIR. As me ntioned by \nDownie et al. [15], it is  one of the most important challenges  \nfor the second decade of International Society of Musi c \nInformation Retrieval (ISMIR) to expand  its musical \nhorizons to non â€“western music. To reduce this gap, we \ndevelop a system for Iranian traditional musical Dastgah \nclassification.  \nThe Dastgah concept in Persian music is similar to \nwestern musical scales and Maqams in Turkish and Arabic \nmusic. Middle -eastern music has not been considered in MIR \nstudies l argel y, however, Gedik et al. [10] constructed a \nTurkish music Maqam recognition system based on the \nsimilarity between pitch histograms ; and Heydarian et al.  \n[16] described the Ir anian musical Santur instrumen t and  \nthey also implemented  an algorithm for the calculation of  \nfundamental frequency . \nIn this paper , we introduce a Dastgah recognition system \nbased on the similarity between Interval Type 2 Fuzzy Sets \n(IT2FS s). Fuzzy logic is also used by Bosteels  et al.  [17] for \ndefining dynamic playlist  generation heuristics . Sanghoon  et \nal. [18] also used fuzzy logic in a music emotion recognition \nsystem.  Leon et al.  [19] also modeled musical n otes by fuzzy logic to integrate  music  tuning  theory and practice . \nAfter feature extraction , the proposed  system  assumes \neach performed note as an IT2FS , so each musical piece is a \nset of  IT2FSs.The maximum similarity between this set and \ntheoreti cal Dastgah prototypes,  which are also sets of \nIT2FSs, indicates the desirable Dastgah.  Gedik et al.  [10] \nused the songs of the datase t to construct the patterns, \nwhereas  in this study , the system makes no assumption about \nthe data except  that different Dastgahs have different  pitch \nintervals. Figure 1 shows the schematic diagram of the \nsystem . We also show that the system can recognize  the \nDastgah of the songs of the proposed dataset  with overall \naccuracy of 85% . \n \nFigure 1.  Dastgah classification system . \n2. IRANIAN TRADITIONAL MUSIC  \nPersian music is a very old eastern music and has had \noutstanding impacts on other eastern musical cultures like  \nCentral Asia, Northern Africa, Southern Europe and  also the \ncountries around the Persian Gulf.  \nIranian traditional music intervals consist of 2 4 equal \nQuartertones per each octave. This division first suggested by \nVaziri [1]. H e called half -sharp quartertone  Sori and half -flat \nquartertone Koron. In practice,  Sori and Koron are not \nexactly half -sharp o r half -flat and can reside anywhere  \nbetween tw o semitones . \nPersian music is based on  a set of seven major Dastgahs: \nShur, Segah, Chahargah, Homayun, Ma hur, Nava and Rast -\npanjgah. The Dastgah  in Persian  music  is similar to the \nwestern musical scale s (major and minor) and  also Maqams \nin Turkish and Arab ic music. Like western musical  scales , \nDastgah  represents a specific pattern of the pitch ratios of \nPitch estimation\n(SWIPE)Note Segmentation\n(Subtractive Clust .)Pitch/ Unpitched\nDetection \nFolding Notes\nIn One OctavePost-Clustering\n(Mahal . Dist.)\nFuzzy Similarity \nMeasureFuzzifier\nFWAFuzzy Distance \nMeasurePitch Detection \nInput SignalPitch Data\nPitch StrengthPitch \nData\nN notes\nN  IT2FSs\nL M notes Theoretical \nDataL M \nIT2FSs\nL M \nsimilarity \nmatrixesL M \nIT2FSsL M\nweights\nClassified\nDastgahFuzzifier \nPermission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fee provided that copies are \nnot made or distributed for profit or commercial advantage and that \ncopies bear this notice and the full cit ation on the first page.  \nÂ© 2011 International Society  for Music  Information  Retrieval  \n275Poster Session 2\nsuccessive notes. Each Dastgah  consist s of some partial \nmelodies, called Gushe , which  are created according to \nDastgah patterns ; however, some of them  are not compatible  \nto those patterns ; therefore,  their tuning might  be different  \nsince  they are used for moving from  one Dastgah to another \none ( modulation) or  for making the performance more \npleasant , like Salmak  Gushe in Shur  Dastgah.  \nThe arrangement of Gush es in each D astgah during the \nperformance is known as Radif  which is  presented by the \nmasters of Persian  music ; such as Mahmud karimiâ€™s  Radif \nfor vocal or Mirza -abdollahâ€™s Radif for fret instrument s. \nFor representing each Dastgah, w e prefer the cent scale to \ntempered western intervals (note, half note, etc.) . As it is \nmentioned, Sori and Korons can be resided anywhere \nbetween two half notes. Better results will be obtained if the \ncent scale is used rather than dividing the octave into equal \ndivisions (12,  24 etc.).  The scale steps of each D astgah \naccording to Karimiâ€™s  Radif and Farhat [2] is shown in Table \n1. Dastgahs like Mahur and Rast -panjgah, and also Nava and \nShur have the same tuning.  \nTable 1 . The scale steps for each Dastgah of Persian music . \nDastgah  Tuning Cents  \n1.Chahargah  (134,397,497,634,888,994,1200)  \n2.Homayun  (100, 398,502,715,800,990,1200 ) \n3.Mahur&Rst.  (208,397,497,702,891,994,1200)  \n4.Segah  (198,352,495,707,826,1013,1200)  \n5.Shur&Nava  (149,300,500,702,783,985,1200 ) \n3. PITCH DETECTION  \nThe proposed model for Iranian traditional music Dastgah  \nrecognition must be applicable  on new and old songs. The \nmajority of available old songs are convert ed to digital form \nfrom tape, so  the white noise is an inseparable part of them , \nand we need a system to discriminate pitch form unpitched \nsignals.  \nIn order to do this, SWIPE ' algorithm  [3] is used which \ncan estimate the pitch and its strength at (discrete) time ğ‘› as \nthe spectral similarity between the signal (in the proximity of \nğ‘›) and a sawtooth waveform with missing non -prime \nharmonics and same ( estimated)  pitch as the signal. The  \npitch vector is refined and classified to pitch/unpitched \nclusters using the method was presented by Camachao   [4]. It \ntrack s the pitch strength t race of the signal and searches  for \nclusters of pitch and unpitched sound according to the local \nmaximization of the distance between the centroids.  \nThe result of using SWIPE ' is shown in Figure 2 . The \npitches are retrieved from the vocal of Mahmud Karimi in \nShur  Dastgah.  The system estimates the pitch of the signal at \neach 45 millisecond . The bold black circles are the pitch \ncluster  centers  which will be described in S ection  4.1. \nPersian music is a center orient ed music, as it shown in \nFigure 2 the vocalist s tarts with the Shahed (tonic) note,  here \nabout 180 Hz, and circulates around it during the \nperformance and again backs to it .  \nFigure 2. The pitches of vocal of Karimi in Shur mode.  \nCircles  are the pitches and  the bold black circles are the pitch \ncluster centers.  \n4. PREPROCESSING  \n4.1 Note Segmentation  \nFirst of all our system needs to recogni ze which musical \nnotes are  used during the performance; moreover, it is \nneeded to eliminate the wrong estimated pi tches. A special \nsituation may o ccur when we use vocal as our raw d ata. As it \nis shown  in Figure 2 at the beginning of each note, it takes \nsome milliseconds that the vocalist achieves the desirable \nfrequency of voice and also at the end of each note we have \nsome ir relevant p oints. To omit  the redundant points , we \nneed to use a clustering method to discriminate the not es \nform irrelevant data. Subtractive Clustering [5]  is used . \nThis algorithm uses the data points , in time -frequency \nscale,  as candidates for the center s of the clusters . Also, t he \nnumber of clusters is not needed to be predefined. Since each \npoint of data ( ğ‘‹ğ‘–) is a candidate of clusters centers, a function \nfor measuring the density in  ğ‘‹ğ‘–  is defined as  \nğ·ğ‘–= ğ‘’ğ‘¥ğ‘ âˆ’ ğ‘‹ğ‘–âˆ’ğ‘‹ğ‘— 2\nğ‘Ÿğ‘2 2 ğ‘›\nğ‘—=1 ,                       (1) \nWhere ğ‘Ÿğ‘ is a positive constant repre senting a neighborhood \nradius, t hus a data point with many neighboring data points \nwill have a high potential value.  After computing the \npotential value of every data point, we select the da ta point \nwith the highest potential value as the cluster center. Let ğ‘‹ğ‘1 \nto be the location of the first cluster center, then the potential \nof each data point ( ğ‘‹ğ‘–) will be revised as  \nğ·ğ‘–=ğ·ğ‘–âˆ’ğ·ğ‘1ğ‘’ğ‘¥ğ‘ âˆ’ ğ‘‹ğ‘–âˆ’ğ‘‹ğ‘1 2\nğ‘Ÿğ‘2 2 ,                   (2) \nWhere  ğ·ğ‘– and ğ·ğ‘1 is the potential value of  ğ‘‹ğ‘– and the first \ncluster center, respectively and ğ‘Ÿğ‘ is a positive constant \nwhich defines a neighborhood that has measurable reductions \nin density measure (typically  ğ‘Ÿğ‘=1.5ğ‘Ÿğ‘). Thus we subtract \nthe amount of potential value of each data point as a function \nof its distance from the first cluster center.  After revising the \ndensity function, the next cluster center is selected as the \npoint having the greatest density value. This process \ncontinues until ğ·ğ‘˜<ğœ€ğ·ğ‘1 at the ğ‘˜th iteration, where Îµ is a \nsmall fraction.  An algorithm is presented  by Chiu  [5] for \nfinding the suitable amount of ğœ€. Figure  2 shows the \nextracted  pitch cluster centers . Note that, e ach pitch cluster \n27612th International Society for Music Information Retrieval Conference (ISMIR 2011)\ncenter has two features (Time and Pitch). However, the pitch \nfeature of each cluster center will be used in the next steps.  \n4.2 Folding Notes  \nIt is convenient to fold all the extracted notes in one octave \nbecause the process of classification will be easier if we deal \nwith one octave. The distance between A3 to A4 (220 Hz to \n440 Hz ) is selected. We fold the note  ğ‘“ğ‘– in the propose d \noctave  by \nğ¹ğ¿ ğ‘“ğ‘– =\n     ğ‘“ğ‘–\n2ğ‘ğ‘’ğ‘–ğ‘™ ğ‘™ğ‘œğ‘”2ğ‘“ğ‘–\n440 ,           ğ‘“ğ‘–>440   \nğ‘“ğ‘–âˆ—2ğ‘ğ‘’ğ‘–ğ‘™ ğ‘™ğ‘œğ‘”2220\nğ‘“ğ‘– ,        ğ‘“ğ‘–<220\nğ‘“ğ‘– ,                        ğ‘œğ‘¡ğ‘•ğ‘’ğ‘Ÿğ‘¤ğ‘–ğ‘ ğ‘’   .             (3) \nAfter that, all the notes will be translated into cents with \nrespect to 220  Hz. In order of brevity, it is not included here.  \n4.3 Post-Clustering  \nAfter folding notes in one octave , Mahalanobis distance  [6] \nis applied  to recognize which point on the reference octave \ncorresponds to each musical note.  Little et al. [7] also used \nthis method for note segmentation of a query by humming \nsystem.  \nWe find the distance between adjacent frames in the \nsequence using the Mahalanobis distance measure, Shown in \nEq. ( 4). Given a frame ğ‘ğ‘–, we assume a new note has begun \nwherever  the distance between two adjacent frames ğ‘ğ‘– and \nğ‘ğ‘–+1 exceeds a threshold, T  \n (ğ‘ğ‘–âˆ’ğ‘ğ‘–+1)ğ‘€âˆ’1(ğ‘ğ‘–âˆ’ğ‘ğ‘–+1),2>T â†’ new note       (4) \nWhere the matrix ğ‘€ is a covaria nce matrix, which calculated \nfrom the variance within a rectangular window around the \nframe ğ‘ğ‘– as \nğ‘€ ğ‘,ğ‘ =1\n2ğœ  ğ‘ğ‘˜âˆ’ğ‘  ğ‘ğ‘˜âˆ’ğ‘ ğ‘–+ğœ\nğ‘˜=ğ‘–âˆ’ğœ ,                (5) \nWhere ğœ is the size of a window surrounding the current \nframe and the average for ğ‘, ğ‘ are calculated over this \nwindow.  \nThe amount of T is set  according to the quarter notes of  \nPersian music, about 0.22, and a small window size for \ncalculating the matrix ğ‘€(ğœ=4 frames)  is used . The result of \nthis process is shown in Figure 3 which the performed notes \nof Hoseyni  Gusheh in Shur mode base d on Karimiâ€™s  vocal \nare classified. The green thick lines and dashed red lines are \nthe begi nning and the end of each note, respectively.  \n \nFigure 3.  Clustering notes within one octave . 5. FUZZY LOGIC TYPE 2 A S DASTGAH \nCLASSIFIER  \n5.1 Interval Type 2 Fuzzy Sets  \nType -2 fuzzy logic  is an extension of type -1 fuzzy logic  that \nfirst was introduced by Zadeh  [8]. It can describe the \nuncertainty associated with our data when it is vague or \nincomplete, effectively. A  special kind of t ype-2 fuzz y set , \nIT2FS,  is used  as the basic element of the  classifier.  IT2FSs \ninclude a secondary membership function to model the \nuncertainty of exact (crisp) type-1 fuzzy sets .1 \nAn IT2FS in the universal set ğ‘‹, denoted as ğ´ , can be \nexpressed as     \nğ´ = ğœ‡ğ´ ğ‘¥âˆˆğ‘‹(ğ‘¥)/ğ‘¥=   ğ‘“ğ‘¥(ğ‘¢)/ğ‘¢ğ‘¥âˆˆğ½ğ‘¥ /ğ‘¥âˆˆğ‘‹ğ‘¥ğ½ğ‘¥âŠ† 0,1 , (6) \nWhere ğ‘“ğ‘¥(ğ‘¢) is the secondary membership function and ğ½ğ‘¥ is \nthe primary membership of ğ‘¥ which is the domain of the \nsecondary membership function [9].  Figure 4 shows this \nregion. The shaded region bounded by an upper and lower \nmembership function is called the footprint of uncertainty \n(FOU). The FOU of ğ´  can be expressed by the union of all \nthe primary memberships as  \nğ¹ğ‘‚ğ‘ˆ ğ´  = ğ½ğ‘¥âˆ€ğ‘¥âˆˆğ‘‹ =  ğ‘¥,ğ‘¢ :ğ‘¢âˆˆğ½ğ‘¥âŠ† 0,1  ,       (7) \nThe upper membership function (UMF) and lower \nmembership function (LMF) of ğ´  are two type -1 Fuzzy \nMembership f unctions that bound the FOU. The UMF \ndenoted by ğœ‡ ğ´  ğ‘¥  is associated with the upper bound of \nFOU, and the LMF denoted by ğœ‡ğ´  ğ‘¥  is associated  with the \nlower bound of FOU. They can be represented as  \nğœ‡ ğ´ (ğ‘¥)â‰¡ğ¹ğ‘‚ğ‘ˆ(ğ´ )           âˆ€ğ‘¥âˆˆğ‘‹,                         (8) \nğœ‡ğ´ (ğ‘¥)â‰¡ğ¹ğ‘‚ğ‘ˆ(ğ´ )âˆ€ğ‘¥âˆˆğ‘‹.                         (9) \n \nFigure 4.  An Interval Type 2 Fuzzy set.  \n5.2 Fuzzifiers  \n5.2.1 Theoretical Data and the Data From Signal  \nWe must manage the uncertainty  associated  with both each \nperformed note and eac h note of the theoretical data . First;  \nwe must define a boundary for each note . We find it \n                                                           \n1The membership value for ordinary fuzzy sets is a crisp number in  [0,1] . \n0 2 4 6 8 1000.20.40.60.81\nXPrimary Membership\nxj\n)(~xAï­\n)(~xAï­\n277Poster Session 2\nconvenient to use a region of about 67 cent s for each note. \nGedik  et al. [10] also used this region for the width s of \nGaussians  of theoretical  patterns  for Turkish Maqams . \nThe mean of each segment , which  are received from  the \npost-clustering phase,  is considered  as a reference.  Then,  the \nupper bound ğœ“ğ‘˜ and the lower bound ğœ™ğ‘˜ of ğ‘˜th frame in 67 \ncent scale  are computed as  \nğœ“ğ‘˜=ğ‘šğ‘–ğ‘›((ğœğ‘˜+33.96),1200 ),                   (10)  \nğœ™ğ‘˜=ğ‘šğ‘ğ‘¥  ğœğ‘˜âˆ’33.96 ,1 ,                      (11)  \nğœğ‘˜=ğ‘ˆğ‘˜âˆ’ğ¿ğ‘˜\n2+ğ¿ğ‘˜,                               (12)  \nWhere ğ‘ˆğ‘˜ and  ğ¿ğ‘˜ are the beginning and the end of the ğ‘˜th \nsegment, respectively. \n5.2.2 Fuzzifing Upper and Lower Bounds  \nThe upper and lower bounds of each note  must be fuzzified \nin a [0, 1] scale with a membership function.  Considering one \noctave,  there  is a non -linier relation between the cent degree \nand frequency of each note that can be expressed as  \n ğ‘“ ğ‘¥ =ğµâˆ—2ğ‘¥âˆ’1200\n1200 ,                          (13) \nWhere ğ‘¥ is the degree of cent  of any note and ğµ is the \nfrequency of the final note of the proposed octave (e.g. 440 \nHz). If we assign the membership value  zero and one to the \nfirst and the last note,  respectively Eq. ( 13) is rewritten  as \nğ‘“ ğ‘¥ = ğ´âˆ—2 âˆ—2ğ‘¥âˆ’1200\n1200\nğ´âˆ’1,                     (14) \nWhere ğ´ is the  frequency of the first note of the proposed \noctave (e.g. 220 Hz).  After simplification, Eq. ( 14) can be \nrewritten  as \nğ‘“ ğœ“ =2ğœ“\n1200âˆ’1,                           (15) \nğ‘“ ğœ™ =2ğœ™\n1200âˆ’1.                           (16) \nWhere  ğœ“ and ğœ™ are the upper and lower bound s of any note, \nrespectively.  Both Eq. ( 15) and Eq. ( 16) can be considered as \nsuitable type -1 fuzzy membership functions for fuzzifing \nmusical notes . We call them Musical Fuzzy Membership \nFunctions (MFMF).   \n5.2.3 Creating Footprint of Uncertainty  \nTwo Gaussians  are used  for creating FOU s. Kreinovich  et al.  \n[11] also prove that  Gaussian membership functions are the \nbest choice for representing uncertainty in measurement.  The \nconstructed Gaussians  are also mapped  on MFMF to obtain \nmore  similarity degree  between  overlapped  IT2FSs . \nThe UMF and LMF of the FOU  for a note with a domain \nfrom ğœ™ to ğœ“ are constructed as  \nğœ‡ ğ´  ğ‘¥,ğœ“ = ğ‘’âˆ’ ğ‘¥âˆ’ğ‘ 2\n2ğœ12\nğ‘‹ğ‘‘ğ‘¥âˆ—ğ‘“ ğœ“ ,                        (17) \nğœ‡ğ´ (ğ‘¥,ğœ™)= ğ‘’âˆ’ ğ‘¥âˆ’ğ‘ 2\n2ğœ22\nğ‘‹ğ‘‘ğ‘¥âˆ—ğ‘“(ğœ™).                        (18) Where ğ‘‹=[ğœ™,ğœ“], ğ‘ is the center of the [ğœ™,ğœ“]  boundary \nand ğœ12and ğœ22 are the standard deviation s and ğ‘“(ğœ“) and \nğ‘“(ğœ™) are the fuzzification functions for fuzzifing the upper \nand lower bounds of each note  with MFMF , respectively . \nThe pattern of S hur and Nava scale is shown in Figure  5. \n \nFigure  5. Shur  Dastgah  prototype that consists  of seven \nIT2FSs  which  are mapped on MFMF (dashed line).  \n5.3 Fuzzy Similarity Measure  \nA suitable Fuzzy Similarity Measure (FSM)  is used  for \ncomputing the degree of similarity between prototypes and \nunknown patterns.  \nBasically, a robust FSM must satisfies four properties \nsuch as reflexivity, symmetry, transitivity and overlapping \n[13]. There are only six methods for computing the similarity \nbetween IT2FSs.  Wu et al. [13] evaluated  the six m ethods . \nWu et al.  [13] defined a new FSM,  called Jaccard similarity \nmeasure (JSM) , which  satisfies the  mentioned properties.  It \nis also the fastest algorithm  among the other FSMs [ 9]. It is \nused for our classifier and i t can be defined as  \nğ‘† (ğ´ ,ğµ )= ğ‘šğ‘–ğ‘›(ğœ‡ ğ´  ğ‘¥ ,ğœ‡ ğµ (ğ‘¥))ğ‘‘ğ‘¥ğ‘‹+ ğ‘šğ‘–ğ‘›(ğœ‡ğ´ (ğ‘¥),ğœ‡ğµ (ğ‘¥))ğ‘‘ğ‘¥ğ‘‹\n ğ‘šğ‘ğ‘¥(ğœ‡ ğ´  ğ‘¥ ,ğœ‡ ğµ (ğ‘¥))ğ‘‘ğ‘¥ğ‘‹+ ğ‘šğ‘ğ‘¥(ğœ‡ğ´ (ğ‘¥),ğœ‡ğµ (ğ‘¥))ğ‘‘ğ‘¥ğ‘‹,     (19) \nWhere X is the domain of the data  (here 1 to 1200).  \n5.4 Fuzzy Distance Measure  \nThe distance between two IT2FSs are computed as  \nğ· (ğ´ ,ğµ )=1âˆ’ğ‘† (ğ´ ,ğµ ),                            (20) \nWhere ğ‘† (ğ´ ,ğµ ) can be any FSM  for IT2FSs  [14]. \nThe average distance between ğ‘–th note  (IT2FS) of any   \nDastgah prototype  and the other notes from differe nt \nDastgahs is  assigned  as a weight to the ğ‘–th note. This \nassignment helps  to establish  more discrimination between \nDastgahs . It also indicates  the degree of  the uniqueness of \neach specific note. A constant weight (0.10)  is assigned  to \nthe seventh and common note of each Dastgah. The assigned \nweight to each note is shown in Tabl e 2. \n5.5 Fuzzy Weighted Average  \nFuzzy Weighted Average (FWA) is  computed by Eq. ( 21). \nMendel  et al. [9] discussed about five different situations of \nthe variables  of Eq. ( 21) which make its computation \ndifferent.  MFMF  \n27812th International Society for Music Information Retrieval Conference (ISMIR 2011)\nğ‘¦= ğ‘¥ğ‘–ğ‘¤ğ‘–ğ‘›\nğ‘–=1\n ğ‘¤ğ‘–ğ‘›\nğ‘–=1,                                    (21) \nWhere ğ‘¥ğ‘– and ğ‘¤ğ‘– are two crisp numbers , so Eq. ( 21) can be \ncomputed as simple as ordinary weighted  average.  \nTable 2.  The assigned weight to each step of Dastgah scales . \n5.6 Dastgah Classification  \nAssume that ğ‘›âˆˆ 1,2,â€¦,ğ‘  IT2FSs  are extracted  from the \ninput signal  and also ğ‘šâˆˆ 1,2,â€¦,M  IT2FS s for each \nDastgah prototype is proposed. W e also have  ğ‘™âˆˆ 1,2,â€¦,ğ¿  \nDastgahs. Assume that  ğ‘ğ‘šÃ—ğ‘›ğ‘™=ğ‘†  ğ‘€ ,ğ‘   is a similarity \nmatrix between ğ‘š IT2FSs of ğ‘™th Dastgah prototype and ğ‘›  \nextracted IT2FSs from input signal where ğ‘†  ğ‘€ ,ğ‘   can be \nany fuzzy similarity measure for ğ‘€  and ğ‘ . Let  ğ‘ ğ‘šğ‘™=\nğ‘šğ‘ğ‘¥ğ‘›(ğ‘ğ‘šÃ—ğ‘›ğ‘™) to be the maximum amount of each row of  \nmatrix ğ‘ğ‘šÃ—ğ‘›ğ‘™, then we may write the process of clas sifying  or \nassigning, the unknown pattern to the Dastgah prototypes as  \nğ¿âˆ—=ğ‘ğ‘Ÿğ‘”ğ‘šğ‘ğ‘¥ğ‘™(ğ¹ğ‘Šğ´ğ‘š(ğ‘ ğ‘šğ‘™,ğ‘Šğ‘šğ‘™)),                (22) \nWhere ğ‘Šğ‘šğ‘™ is the assigned we ight to each  note (IT2FS ) of \neach the Dastgah prototype.  \n6. RESULTS  \n6.1 Dataset  \nLack of reliable dataset for Persian music was one of our \nmain problems, so  for evaluating the system a dataset for \nIranian traditional music  is collected . The dataset consist s of \n210 tracks from different Dastgah types.  The Dastgah types \nand the number of  recordings from each Dastgah type are as \nfollows:  89-Shur & Nava,  30-Segah,  41-Mahur &  Rast-\npanjgah,  26-Homayun and 24 -Chahargah.  \nThe colle ction was mainly based on vocal , and some  \nmonophonic musical pieces from some popular traditional \ninstruments such a s Santur,  Tar, Setar and Kamancheh. The \nvocals w ere from three prominent Iranian vocalists such as \nMahmud Karimi,  69 tracks, Abdullah Davami,  57 tracks,  \nMuhammad Reza Shajarian,  20 tracks and also some other \nwell trained vocalist. For a better evaluation, we also used 21  \ntracks from Arabian Maqams .2 \n6.2 Pattern Similarity  \nThe Persian musical scales  are so similar to each other and  \n                                                           \n2Segah Maqam (Dastgah) is a common mode in Iranian and Arabian music.  \nAjam Maqam in Arabian music is also so similar to Iranian Chahargah scale.  it is a considerable obstacle for  Dastgah  detection . Table 3  \nshows the degree of similarity between our Dastgah  \nprototypes  based on JSM for IT2FSs.  The Chaharga, Mahur \nand Rast -panjgah modes have the maximum similarity \ndegree, about 73%,  while Chahargah and  Segah mode s have \nthe minimum similarity degree, about 43%.  \nTable 3. The similarity degree between Dastgah prototypes . \nPattern Sim. % A B C D E \nA.Chahargah  100 59.22  73.30  43.07  49.28  \nB.Homayun  59.22  100 63.36  50.73  59.16  \nC.Mahur&Rst.  73.30  63.36  100 60.25  56.19  \nD.Segah  43.07  50.73  60.25  100 50.38  \nE.Shur&Nava  49.28  59.16  56.19  50.38  100 \n6.3 Evaluation  \nFor system  evaluation , both ori ginal and segmented songs of \nthe dataset  are used . We segment each son g of our dataset to \nseveral portions  with arbitrary lengths . By evaluating the \nsystem with  the song segments , it is found  that about one \nminu te of any  song is necessary and sufficient  for Dastgah  \ndetection , so we can use only one minute of a given song  to \nmake the process of Dastgah  detection faster . \nThe Dastgah recognition system can r ecognize the modes \nwith overall accuracy  of 85%. It is evaluated by computing \nthe para meters such as Rec all, Precision,  Accuracy,  F-\nmeasure and Matthews C orrel ation Coefficient (MCC). Table \n4 shows the performance of  the classifier  according to above \nmeasures. The MCC is computed as  \nğ‘€ğ¶ğ¶= ğ‘‡ğ‘ƒâˆ—ğ‘‡ğ‘ âˆ’(ğ¹ğ‘ƒâˆ—ğ¹ğ‘)\n  ğ‘‡ğ‘ƒ+ğ¹ğ‘ƒ  ğ‘‡ğ‘ƒ+ğ¹ğ‘  ğ‘‡ğ‘+ğ¹ğ‘ƒ  ğ‘‡ğ‘+ğ¹ğ‘  ,            (23)                                      \nWhere TP: True Positive, TN: True Negative, FP: False \nPositive and FN: False Negative.  \nThe MCC is used as a measure of the quality of binary \n(two-class) classifications. It balances true and false positives \nand negatives . It can be used even if the classes are of very \ndifferent sizes, like our dataset which the number of  songs \nvaries for each Dastgah. T he MCC is  also the best way for \ndescribing the confusion matrix. The confusion matrix of the \nclassifier  is presented in Table 5.  \nTable 4.  The results of the  evaluation of the classifier . \nDastgah  Recal  Precision  Acc. F.mes.  MCC  \nMahur&Rst.  90.24  90.24  96.19  90.24  0.87 \nShur&Nava  85.39  98.70  93.33  91.56  0.86 \nSegah  83.33  83.33  95.23  83.33  0.80 \nHomayun  80.76  75.00  94.31  77.77  0.74 \nChahargah  87.50  61.76  92.38  72.41  0.69 \nMoreover Receiver Operating Characteris tic (ROC) space \nis shown in Figure 6. The ROC space is a graphical plot of \nthe recall, or true positive rate (benefits), versus  false positive \nrate ( costs).  The best possible prediction method would yield \na point in the upper left corner or coordinate (0 ,100) and t he Dastgah  Weight Of Each Scale Step  \n1.Chahargah  (0.78,0.45,0.07,1.00,0.76,0.20,0.10)  \n2.Homayun  (0.90,0.45,0.15,0.47,0.74,0.23,0.10)  \n3.Mahur&Rst.  (0.80,0.45,0.07,0.36,0.77,0.20,0.10)  \n4.Segah  (0.21,0.16,0.89,0.62,0.17,0.51,0.10)  \n5.Shur&Nava  (0.77,0.96,0.10,0.36,0.80,0.30,0.10)  \n279Poster Session 2\nworst prediction method is a point in the lower right corner \nor coordinate (100 ,0) of the RO C space, respectively . \nTable 5 . Confusion mat rix. \n According to ROC space, confusion matr ix and the \nmentioned measures , Dastgah recognition system  is \nsuccessful for the Dastgah types M ahur, Rast -Panjgah, Shur, \nNava and S egah but, it is not very successful  for the Dastgah \ntypes Homayun and Chahargah. The system can work \nprecisely on small -sized dataset however,  the dataset  is \nneeded  to be expanded for  a better  evaluation of the system.  \n \nFigure 6 .ROC space . \n7. CONCLUSION  \nWe presented a method for Iranian traditional music Dastgah \nclassification.  The method works by assuming each piece of \nmusic as a set of IT2FSs an d recognize  the Dastgah of the \nsong by finding the maximum similarity between IT2FSs of   \nthe song and Dastgah prototypes. The system makes no \nassumption about the data except that the Dastgahs have \ndifferent scale steps. The met hod was shown to work on \nsmall -sized dataset  accurately.  \nUsing Gaussian shaped FOUs; the Dastgah  recognition \nsystem only supports intrauncertainty, which is the \nuncertainty a musician has about the scale steps. It is a \ncandidate of future work to collect  data from several \nmusic ians about tuning the scale steps of  Dastgahs. Then use \nthe method of liu et al. [12] for constructing the FOUs . After \nthat, the system  can support interuncertainty, which is the \nuncertainty that a group of musicians have about the sc ale \nsteps [9].  Moreover , in the future work,  the system must be \nequipped with  a Gushe (or melody) recognition sy stem .   \n8. REFERENCES  \n[1] Vaziri, A. N.:  Dastur -e Tar , Tehran,1913 . \n[2] Farhat, H.:  The Dastgah Concept in Persian Music , \nCambridge University Press,  1990. [3] Camacho A., Harris, J. G.: â€œ A sawtooth waveform \ninspired pitch estimator for speech and music,â€  Journal \nof the Acoustical Society of America , vol. 124, pp. 1638 -\n1652,  2008 . \n[4]  Camacho A.: â€œDetection of Pitched/Unpitched    Sound \nUsing Pitch Strength Clustering,â€  In Proceedings of \nISMIR , pp. 533 -537, 2008 . \n[5] Chiu, S.: â€œFuzzy Model Identification Based on Cluster \nEstimation ,â€ Journal of Intelligent & Fuzzy Systems , \nVol. 2, No. 3,  pp.267 -278, 1994.  \n[6] Mahalanobis  P. C.: â€œOn the generalized distance in \nstatisti cs,â€ Proceedings of the National Institute of \nScience of India , pp. 49 -55, 1936. \n[7] Little D.,  Raffensperger D., Pardo B.: â€œA Query by \nHumming System that Learns from Experience ,â€ In \nProceedings of  ISMIR , pp. 335-338, 2007 . \n[8] Zadeh L.A. : â€œThe Concept of a Linguistic Variable and \nits Application to Approximate Reasoning  I,â€ \nInformation Sciences , Vol.8, No.3, pp.199 -249, 1975.  \n[9] Mendel J . M. , Wu D. : Perpetual  Computing, John \nWiley & IEEE Press, 2010 . \n[10] Gedik A.,  Bozkurt B.:  â€œPitch -frequency histogram -\nbased music information retrieval for Turkish music,â€  \nSignal Processing , Vol.90,  No.4,  pp.1049 -1063,  2010 . \n[11] Kreinovich  V., Quintana C., Reznik L.:  â€œGaussian \nmembership functions are most adequate in representin g \nuncertainty in measurements,â€ In Proceedings of \nNAFIPS'92 , pp.618 -24, 1999.  \n[12] Liu F., Mendel J. M. : â€œEncoding Words into Interval \nType -2 Fuzzy S ets Using an Interval Approach,â€  IEEE \nTrans. on Fuzzy Systems , vol. 16, pp. 1503 -1521,  2008.  \n[13] Wu D.,  Mendel J. M.: â€œ A comparative study of ranking \nmethods, similarity measures and uncertainty measures \nfor interval type -2 fuzzy sets,â€  Information Sciences , \nvol. 179, pp. 1169 -1192, 2009.  \n[14] Zhang H., Zhang W.: â€œ Hybrid monotonic inclusion \nmeasure and its use in measuring similarity and distance \nbetween fuzzy sets,â€  Fuzzy Sets and Systems , Vol.160, \nNo.1, pp.107 -118, 2009 , \n[15] Dowine J. S., Byrd D.,  Crawford T.:  â€œTen years of \nISMIR: reflections on challenges and opportunities,â€ In \nProceedings of ISMIR , pp. 13-18, 2009.  \n[16] Heydarian  P., Reiss J. D.: â€œThe Persian music and the \nSantur instrument,â€ In Proceedings of ISMIR , pp. 524-\n527, 2005.  \n[17] Bosteels  K., Pampalk  E., Kerre  E.: â€œEvaluating and \nanalysing dynamic playlist generation heuristic  using \nradio logs and fuzzy set theory,â€ In Procee dings of \nISMIR , 2009 . \n[18] Sanghoon  J., et al.:  â€œA fuzzy inference -based music \nemotion recognition system ,â€ In Proceedings  of VIE , \npp.673 -677, 2008.  \n[19] Leon  T., Liern V.:  â€œFuzzy logic helps to integrate music \ntheory and practice ,â€ In Proceedings of FUZZ -IEEE , \npp.1-5, 2010 . \n Dastgah  A B C D E \nA.Chahargah  0 1 1 1 0 \nB.Homayun  4 0 0 1 0 \nC.Mahur&Rst.  2 2 0 0 0 \nD.Segah  2 1 1 0 1 \nE.Shur&Nava  5 3 2 3 0 \n0 10 20 30 40 50 60 70 80 90 100 10 20 30 40 50 60 70 80 90 100 True Positive Rate  \nBetter \nWorse Segah / FPR=2.777  /TPR=83.333  Chahargah / FPR=6.989  /TPR=87.500  Random Guess  \nHomayun / FPR=3.260  /TPR=80.769  \n                      SHUR & NAVA / FPR= 0.826  \n                       / TPR=85.393   Mahur & Rast -e-Panjgah / FPR=2.366  /TPR=90.243  \nFalse Positive Rate  \n280"
    },
    {
        "title": "Modeling Musical Attributes to Characterize Two-Track Recordings with Bass and Drums.",
        "author": [
            "Jakob AbeÃŸer",
            "Olivier Lartillot"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1417529",
        "url": "https://doi.org/10.5281/zenodo.1417529",
        "ee": "https://zenodo.org/records/1417529/files/AbesserL11.pdf",
        "abstract": "In this publication, we present a method to characterize twotrack audio recordings (bass and drum instruments) based on musical attributes. These attributes are modelled using different regression algorithms. All regression models are trained based on score-based audio features computed from given scores and human annotations of the attributes. We compare five regression model configurations that predict values of different attributes. The regression models are trained based on manual annotations from 11 participants for a data-set of 70 double-track recordings. The average estimation errors within a cross-validation scenario are computed as evaluation measure. Models based on Partial Least Squares Regression (PLSR) with preceding Principal Component Analysis (PCA) and on Support Vector Regression (SVR) performed best.",
        "zenodo_id": 1417529,
        "dblp_key": "conf/ismir/AbesserL11",
        "keywords": [
            "audio recordings",
            "bass and drum instruments",
            "musical attributes",
            "regression algorithms",
            "score-based audio features",
            "human annotations",
            "cross-validation scenario",
            "evaluation measure",
            "Partial Least Squares Regression",
            "Principal Component Analysis"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nMODELLING MUSICAL ATTRIBUTES TO CHARACTERIZE TWO-TRACK\nRECORDINGS WITH BASS AND DRUMS\nJakob AbeÃŸer\nSemantic Music Technologies Group,\nFraunhofer IDMT, Ilmenau, GermanyOlivier Lartillot\nFinnish Centre of Excellence in Interdisciplinary\nMusic Research, University of Jyv Â¨askyl Â¨a, Finland\nABSTRACT\nIn this publication, we present a method to characterize two-\ntrack audio recordings (bass and drum instruments) based\non musical attributes. These attributes are modelled using\ndifferent regression algorithms. All regression models are\ntrained based on score-based audio features computed from\ngiven scores and human annotations of the attributes. We\ncompare ï¬ve regression model conï¬gurations that predict\nvalues of different attributes. The regression models are\ntrained based on manual annotations from 11 participants\nfor a data-set of 70 double-track recordings. The average\nestimation errors within a cross-validation scenario are com-\nputed as evaluation measure. Models based on Partial Least\nSquares Regression (PLSR) with preceding Principal Com-\nponent Analysis (PCA) and on Support Vector Regression\n(SVR) performed best.\n1. INTRODUCTION\nA lot of music pieces show stylistic inï¬‚uences from multiple\nmusic genres. These inï¬‚uences usually can be linked to the\nindividual instrument tracks of a song. Instead of modelling\nmusic pieces as a whole, we believe that it is more meaning-\nful to characterize them on a track-level. In this publication,\nwe investigate double-track recordings including bass and\ndrum instruments. Both instruments are essential parts of\nthe so-called â€œrhythm sectionâ€ that establishes the rhythmic\nand harmonic foundation of a band that performs a piece\nof music. The bass track and drum track usually follow a\nrepeating, pattern-based structure.\nThe contribution of this paper is two-fold. First, we\npresent new features for the rhythmic and tonal analysis of\ninstrument tracks. Second, we investigate the applicability\nof regression models to model semantic attributes of instru-\nment tracks based on human ratings. Since these attributes\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proï¬t or commercial advantage and that copies\nbear this notice and the full citation on the ï¬rst page.\nc/circlecopyrt2011 International Society for Music Information Retrieval.have a continuous scale, we use regression algorithms rather\nthan classiï¬cation algorithms to automatically predict their\nvalues for a given recording. The attributes introduced in\nthis work (see Sect. 5) allow to describe a piece of music on\na more abstract level than features derived from music the-\nory allow. This semantic level opens up a more general per-\nspective to characterize, to compare, and to retrieve music\npieces. It is furthermore accessible to a broader selection of\nusers since it does not require detailed musical knowledge.\n2. GOALS & CHALLENGES\nWe aim to develop a regression-based prediction system that\nautomatically characterizes double-track bass and\ndrum recordings in terms of ï¬ve different tonal and rhythmic\nattributes. Since the recordings we investigate cover various\nmusic styles from different regional backgrounds, we need\nto identify features that allow a robust semantic description\nindependent of stylistic idiosyncrasies.\n3. PREVIOUS WORK\nIn the last decade, score-based audio features (high-level\nfeatures) were mainly applied for classiï¬cation tasks such\nas genre classiï¬cation [2, 3, 7]. In contrast to low-level and\nmid-level audio features such as the spectral ï¬‚ux or the Mel-\nFrequency Cepstral Coefï¬cients (MFCC), high-level\nfeatures relate to expressions of music theory to character-\nize instrument tracks in terms of rhythmic and tonal prop-\nerties. These features are derived based a score representa-\ntion of a music piece, which can be generated either by an\nautomatic transcription of real audio ï¬les or directly from\nsymbolic formats such as MIDI. In the past, most methods\nto extract high-level features comprise a statistical analysis\nof note onsets, pitches, and intervals [3, 8]. In [4], differ-\nent regression algorithms were compared to predict differ-\nent emotion ratings based on extracted audio features. Mu-\nsic recordings with guitar, bass guitar, and drums were ana-\nlyzed as presented in [1] based on rhythmic high-level fea-\ntures. In this publication, three different conï¬gurations of\nregression models were compared to model 8 different mu-\nsical attributes related to different instruments.\n209Poster Session 2\nFigure 1 . Processing steps including the manual annotation\nstep, feature extraction, and regression analysis.\n4. DATASET\nIn this study, we use a collection of 70 two-track record-\nings including a drum track and a bass track taken from\ninstructional bass literature [10] as dataset. These tracks\ncover different Western music styles such as blues, funk,\nboogie, and modern jazz, Non-western styles from Latin and\nSouth America such as Cuban mambo, reggae, and samba as\nwell as some African styles. All audio recordings were per-\nformed by professional musicians in a recording studio. The\nprocessing steps pursued in this study are depicted in Fig. 1.\nWe used the audio recordings for the manual annotation of\nthe given attributes as explained in Sect. 5. In addition, we\nextracted a score representation of the bass track based on\nthe related score sheets and manually transcribed the drum\ntrack. Both track transcriptions were stored as MIDI ï¬les for\nfurther analysis. The question of automatic bass and drum\ntranscription is not within the scope of this paper.\n5. ANNOTATION PROCESS\nFor the annotations, we recruited 11 participants of differ-\nent levels of musical education (most of them being semi-\nprofessional musicians). The participants were asked to an-\nnotate each audio track according to the attributes harmonic\nclarity (HClar ),harmonic predictability (HPred ),rhythmic\nclarity (RClar ),rhythmic coherence (RCoh ), and danca-\nbility (Dan) using a 7-point numeric scale between 1 (very\nlow) and 7 (very high) with 4 being the neutral value. All\nattributes were introduced to the participants based on ex-\nplanatory questions as shown in Tab. 1. The Annotation\nTool previously presented in [11] was used for the subjects\nto manually assign attribute values for all recordings within\nthe dataset. The participants were allowed to skip single an-\nnotations if they were unsure of their annotations for those\nparticular tracks.6. FEATURE EXTRACTION\nWe used the MIDI toolbox [5] to extract the basic score pa-\nrameters absolute pitch Î¸P,Aof all notes of the bass track\nas well as onset Ï•O(in fractions of bar lengths) and du-\nrationÏ•D(in fractions of bar lengths) of all notes of both\ntracks from the MIDI ï¬les. Based on these note parame-\nters, we compute high-level features related to rhythmic and\ntonal properties of both tracks as explained in the following\nsections. Both Ï•OandÏ•Dprovide a tempo-independent\nrhythmic representation of the bass line. In addition to the\nbass track ( BA), we split the drum track ( DR) into the three\ninstrument sub-tracks bass-drum ( BD), snare drum & rim-\nshot ( SD), and hi-hat & cymbals ( HH). In this section, we\nï¬rst explain pre-processing steps and then illustrate the ex-\ntracted rhythmic and tonal features. For each feature, the\ncorresponding instrument tracks are given in brackets.\n6.1 Rhythmic features\n6.1.1 Pre-processing\nMetric level\nIn order to emphasize notes that occur on strong metric po-\nsitions, we compute the metric level liof each note within\nthe metric hierarchy of the corresponding bar. All examples\nwithin the dataset are in a4\n4time signature, thus we deï¬ne\nthe quarter notes as the beat-level . If the note onset corre-\nsponds to a beat position within the beat-level, we obtain\nli= 1, if it is not on the beat level but still on the ï¬rst sub-\nbeat level (eight-notes), we obtain li= 2, and so forth. For\nsimpliï¬cation, we assign both triplets as well as duplets to\nthe same rhythmic level.\nSimilarity matrix (based on Levenshtein distance)\nFor each instrument track and each bar, we extract sequences\nmade of the corresponding notes. Each note is represented\nby its modiï¬ed note onset Ë†Ï•O=Ï•Omod1. This repre-\nsentation neglects the associated bar number of a note and\nonly takes its relative position within its bar into account.\nWe compute a rhythmic similarity measure sm,nbetween\nbarmand barnbased on the Levenshtein distance measure\ndm,nassm,n= 1âˆ’dm,n/dmax. For each pair of bars, the\nscaling factor dmax corresponds to the length of the longer\nnote sequence. See [1] for further details.\n6.1.2 Features\nAverage metric level (BA,DR)\nIn order to characterize the rhythmic complexity of an in-\nstrument track, we compute the average metric level liover\nall notes of this track as feature.\nTempo (All)\nWe use the tempo in BPM derived from the function get-\ntempo from the MIDI toolbox.\n21012th International Society for Music Information Retrieval Conference (ISMIR 2011)\nAttribute Related instru-\nment track(s)Explanatory questions\nHClar BA How clear could you imagine the harmonic content / harmonic progression of the music by\njust listening to the bass line?\nHPred BA When listening to the excerpt for the ï¬rst time, did you ï¬nd the harmonic progression im-\nplied by the bass line predictable, or was it on the contrary surprising and unexpected?\nRClar BA&DR How clear could you perceive the rhythmic structure (beat positions) by listening to the bass\nand the drums?\nRCoh BA&DR Did the two instruments contribute to a coherent rhythmic structure, or did they contradict?\nDan BA&DR While listening to the music, could you imagine that it is easy to dance to it or not?\nTable 1 . Attributes used for manual annotations.\nNote density (BA,DR)\nWe compute the number of notes Nmper bar. Then we take\nthe mean and standard deviation of all values of Nmas fea-\ntures.\nRhythmic similarity within instrument tracks (BA,DR)\nWe compute mean and standard deviation over all similarity\nvaluessm,nwithm/negationslash=nto measure the average similarity\nbetween all bars of an instrument track as well as its vari-\nance.\nRhythmic similarity between instrument tracks (BA-BD,BA-\nSD,BA-HH,BD-SD,BD-HH, and HH-SD)\nSimilar to the previously explained feature, we compute the\nbar-wise similarity between the bass and drum track pairs\nBA-BD,BA-SD,BA-HH, and the drum track pairs BD-SD,BD-HH,\nand HH-SD. For instance, this allows to detect whether the\nbass and the bass-drum track play rhythmically in unison or\nnot. The participants agreed that this particular conï¬gura-\ntion contributes to the perception of a high rhythmic coher-\nence between the bass and the drum instrument.\nDivergence from a (Western) prototype rhythm (DR)\nIn accordance to the statements of various participants, we\nidentiï¬ed a prototypic drum rhythm1as illustrated in Fig.\n2 that was said to serve as a rhythmic orientation for locat-\ning the beat positions in an unknown bass and drum groove.\nTherefore, we assume that the similarity between a given\ndrum track and this prototype rhythm can be interpreted as a\nmeasure that is proportional to the perceived rhythmic clar-\nity. For each of the drum instrument tracks BD,SD, and HH,\nwe compute the similarity based on the Levenshtein distance\nas explained above between the real drum track and the cor-\nresponding track in the prototype rhythm. Finally, we av-\n1This rhythm can be found in different Western music genres. Consid-\nering that most of the participants said to have only minor listening experi-\nence with Latin American and African rhythms, we only take this rhythm\nas a basis of comparison even though a couple of Latin American bass and\ndrum grooves are present in the database.\nPage 1/1]441B^^B^^B^^B^^Figure 2 . (Western) prototype drum-rhythm. The three\ndrum classes introduced in Sec. 6 are represented by the\nlowest note (bass drum - BD), the middle note (snare drum -\nSD), and the cross-note (hi-hat - HH).\nerage the similarity over all three instruments to derive an\noverall similarity measure for the complete drum track. This\nmeasure is averaged over all bars and taken as feature.\n6.2 Tonal features\n6.2.1 Pre-processing\nChromatic pitch representation\nThe chromatic pitch class Î¸P,Crepresents all absolute pitch\nvalues mapped to one octave as Î¸P,C=Î¸P,Amod 12with\nÎ¸P,Câˆˆ[0,11]. The note name Ccorresponds to Î¸P,C= 0.\nDiatonic interval representation\nÎ¸Idenotes the intervals between adjacent notes in semi-\ntones. After all intervals are mapped to a maximum abso-\nlute value of 12, we derive a diatonic interval representation\nÎ¸I,Dthat corresponds to the musical interval labels unison\n(Î¸I,D= 1), second (Î¸I,D= 2), and so forth up to seventh\n(Î¸I,D= 7). The octave ( Î¸I= 12 orÎ¸I=âˆ’12) is consid-\nered as a unison ( Î¸I= 0) here according to the modulo-12\noperation. For reasons of simpliï¬cations, we convert all de-\nscending intervals Î¸I,D<0into their complementary inter-\nvals, i.e., a descending second ( Î¸I,D=âˆ’2) to an ascending\nseventh (Î¸I,D= 7) etc.\nBass note detection\nWe aim to detect the dominant bass note in each bar. Since\nno other instrument track is available for harmonic analysis,\nwe use this bass note as harmonic reference for the compu-\n211Poster Session 2\ntation of different tonal features. First, we retrieve all chro-\nmatic pitch classes Ë†Î¸P,C apparent in a bar of the bass line.\nThen, we compute a chromatic presence valueÎ±, which ac-\ncumulates the duration values Ï•D,iof all notes associated to\nthe same chromatic pitch class Ë†Î¸P,C,k within this bar:\nÎ±(Ë†Î¸P,C,k ) =/summationdisplay\nâˆ€iâ†”Î¸P,C,i=Ë†Î¸P,C,k1\nliÏ•D,i (1)\nEach note is weighted according to its metrical level by the\nweighting factor 1/li(see Sect. 6.1.1). This is because notes\non strong metric positions are assumed to be more likely\nperceived as bass notes than notes on weak metric positions.\nFinally, we obtain the chromatic pitch class of the bass note\nÎ¸P,C,B in this bar by maximizing Î±over all apparent chro-\nmatic pitch classes as\nÎ¸P,C,B =Ë†Î¸P,C,kâˆ—â†”kâˆ—= arg max\nkÎ±(Ë†Î¸P,C,k ).(2)\n6.2.2 Features\nPercentage of bass note changes (BA)\nSince we assume that the bass note acts as an indicator for\nthe predominant harmony in a bar, we compute the number\nof bass note changes in a bass line divided by its length in\nbars as feature.\nDiatonic intervals related to the bass note (BA)\nIn each bar, we compute the interval between the chromatic\npitch class of all bass notes and the chromatic pitch class\nof the estimated bass-note Î¸P,C,B . Then, we derive the di-\natonic representation Î¸I,Dof this interval in the same way\nas previously explained in Sect. 6.2.1. If the bass note re-\nlates to the root note of the current chord and the bass line\nplays mainly thirds and ï¬fths ( Î¸I,D= 3,Î¸I,D= 5), we\nexpect the harmonic predictability to be high since the bass\nuses main chord tones. Therefore, we compute n(Î¸I,D=\n3) +n(Î¸I,D= 5))//summationtextn(Î¸I,D)as feature with n(Î¸I,D)in-\ndicating the number of notes with the given diatonic interval\nvalue. If only a small number of different diatonic intervals\nare present, we assume the harmonic complexity of a bass\nline to be low. Therefore, we compute the zero-order en-\ntropy over the probability values\np(Î¸I,D) =n(Î¸I,D)//summationtextn(Î¸I,D)as second feature:\nH0=âˆ’/summationdisplay\np(Î¸I,D)log2[p(Î¸I,D)] (3)\nTonal similarity between subsequences (BA)\nTo measure the tonal complexity of a bass line, we inves-\ntigate, whether it is repeated after a certain number of bars.\nTherefore, we subdivide the bass line into adjacent\nsub-sequences of a length of L= 1,L= 2, andL= 4\nbars. Each sub-sequence is represented by the absolute pitchvalues of the included notes. Again, we compute a similar-\nity measure based on the Levenshtein distance as described\nin the previous section. Bass lines are often repeated after\na few bars but played in a transposed form, i.e., translated\nin pitch by a constant term. To cope with that, we sub-\ntract the lowest pitch value from all absolute pitch values\nin each sub-sequence that is to be compared. Finally, we\naverage the similarity values between all adjacent pairs of\nsub-sequences (e.g. for L= 1, we compare bar 1 with bar\n2, bar 2 with bar 3, and so on) and derive one feature value\nfor each sub-sequence length.\n7. EVALUATION\n7.1 Regression analysis\nWe compare 5 different conï¬gurations of regression mod-\nels based on Robust Regression (RR), Partial Least-Squares\nRegression (PLSR), and Support Vector Regression (SVR).\nThe RR uses an iteratively algorithm to assign a weight to\neach data point within the training data. This way, outliers\nhave a smaller inï¬‚uence on the regression model. A differ-\nent approach is followed by PLSR. A smaller number of less\ncorrelated predictor variables is derived from a linear com-\nbination of the initial feature dimensions. For the PLSR, we\ninvestigate the inï¬‚uence of a preceding feature selection via\nPrincipal Component Analysis (PCA). Therefore, we select\nall feature dimensions with eigenvalues Î» > 1during the\nPCA. We then determine the optimal model order for the\nPLSR models by minimizing the Akaike information crite-\nrion (AIC). For the SVR, we compare Î½-SVR and/epsilon1-SVR as\nprovided by the LibSVM toolbox [6]. We used the RBF\nkernel with parameter Î³and cost factor Cfor both con-\nï¬gurations. Based on a three-stage grid search, we deter-\nmine the optimal parameters {C,Î³,Î½}for theÎ½-SVR and\n{C,Î³,/epsilon1}for the/epsilon1-SVR2by minimizing the mean squared\nerror (MSE) value. For more details on the regression meth-\nods, see for instance [6] and [9].\nFor each attribute, we select the features that are used\nfor the model training as illustrated in the third column of\nTab. 3. Leave-one-out cross-validation is used to evaluate\neach conï¬guration-attribute pair and to avoid model over-\nï¬tting, i.e., a different sample is used within each fold for\ntesting and the remaining 69 samples are used for training\nof the regression models. Within each fold, all vectors of the\ntraining set are normalized to zero mean and unit variance.\nThen, the feature vector of the test set is normalized using\nthe mean and standard deviation vectors derived from the\ntraining set. The MSE is computed between the predicted\nvalues and the ground-truth values of the test set and av-\neraged over 70 folds. For each conï¬guration-attribute pair,\n2Search area for Î½is0.01 :.05 :.5and for/epsilon1is(0.1 : 0.1 : 1)Â·10âˆ’3.\nThe parameters CandÎ³are selected via grid-search as proposed in [6].\n21212th International Society for Music Information Retrieval Conference (ISMIR 2011)\nwe store the test set ground truth values as well as the cor-\nresponding model predictions over all folds in two vectors.\nThen, we compute the sample correlation between both vec-\ntors. The correlation is considered as signiï¬cant if p < . 05\nholds true for the corresponding p-value.\nHClar HPred RClar RCoh Dan\nHClar / 0.82âˆ—0.48âˆ—0.5âˆ—0.24\nHPred 0.82âˆ—/ 0.5âˆ—0.58âˆ—-\nRClar 0.48âˆ—0.5âˆ—/ 0.77âˆ—0.38\nRCoh 0.5âˆ—0.58âˆ—0.77âˆ—/ 0.31\nDan 0.24 - 0.38 0.31 /\nTable 2 . Correlation coefï¬cients rbetween human anno-\ntations of different attributes. Only signiï¬cant correlations\n(p<. 05orp<. 001âˆ—) are shown.\n7.2 Results\nCorrelation between attributes\nAs illustrated in Tab. 2, the annotations show that many of\nthe attributes are signiï¬cantly correlated, especially the two\ntonal attributes HClar andHPred (r=.82) and the two\nrhythmic attributes RClar andRCoh (r=.77). The danca-\nbility of a bass and drum groove seems to be mainly inï¬‚u-\nenced by its rhythmic attributes ( rDan,RClar =.38,\nrDan,RCoh =.31).\nRegression experiment\nThe results of the regression experiments outlined in Sect.\n7.1 are illustrated in Tab. 3. As depicted in the upper part\nof the table, the SVR models lead to the smallest MSE val-\nues for all 5 attributes where the PLSR models performed\nonly slightly worse. The models for HClar andRClar show\nthe smallest prediction errors, while harmonic predictability\nshow the highest errors. The RR performed worse for all\nattributes.\nThe sample correlation coefï¬cients and the correspond-\ning p-values are given in the lower part of the table. In\ncontrast to the MSE values, highest (signiï¬cant) correlation\ncoefï¬cients can be observed for the attributes HPred with\nr=.59andDan withr=.46. All signiï¬cant correlations\ncan be observed for models based on PLSR with preceding\nPCA or based on /epsilon1-SVR. No model show signiï¬cant corre-\nlation for the attribute HClar .\nComments of participants\nWe identiï¬ed a couple of problems during additional inter-\nviews with the participants after the annotation step. Two\nparticipants generally had difï¬culties to distinguish between\nclarity and predictability. The attributes HClar andHPred\nwere said be the most complicated ones to annotate since\nthe majority of the participants were not used to listen just tothe bass and the drum instrument without any accompanying\nharmony instrument. Since the attribute HPred achieved the\nhighest estimation errors as shown above, we assume that\nfurther score-based features need to be extracted from the\nharmony track of a given music recording in order to model\nthis attribute.\n8. CONCLUSION\nIn this paper, we compared ï¬ve different regression algo-\nrithms for the estimation of values related to ï¬ve differ-\nent tonal and rhythmic attributes to characterize two-track\nrecordings of bass and drums. Score-based features were\nextracted and used as predictor variables and manual user\nannotations of 70 audio excerpts were used as response vari-\nables to train and evaluate the regression models. For all ï¬ve\nattributes, the PLSR+PCA model and the SVR models per-\nformed best (and comparably well) in terms of estimations\nerrors. Signiï¬cant correlations between annotated and es-\ntimated attribute values were only observed for four of the\nattributes and in particular for PLSR+PCA models and the\n/epsilon1-SVR models. Since the highest (signiï¬cant) correlation\ncoefï¬cient is r=.59, we assume that further important as-\npects of the musical performance are not well captured by\nthe applied features so far.\nIn general, we believe that the presented approach can\nbe generalized to multi-track recordings including other in-\nstruments. However, we think that human attribute ratings\nshould be based on listening to the isolated tracks instead of\nlistening to the mixture signal. One issue of future work is\nto investigate how strong the perception of these attributes\ndiffers when human annotators listen to mixture of multiple\ninstruments instead.\n9. ACKNOWLEDGEMENTS\nThe authors would like to thank all participants who took\npart in the annotation process. The Thuringian Ministry of\nEconomy, Employment and Technology supported this re-\nsearch by granting funds of the European Fund for Regional\nDevelopment to the project Songs2See3, enabling transna-\ntional cooperation between Thuringian companies and their\npartners from other European regions. Furthermore, this\nwork has been partly supported by the German research\nproject SyncGlobal4funded by the Federal Ministry of Ed-\nucation and Research (BMBF-FKZ: 01/S11007D).\n10. REFERENCES\n[1] J. AbeÃŸer, O. Lartillot, C. Dittmar, T. Eerola, and\nG. Schuller. Modeling musical attributes to characterize\n3http://www.songs2see.net\n4http://www.syncglobal.de\n213Poster Session 2\nAttribute (short) Features RR PLRS (+ PCA) PLRS Î½-SVR /epsilon1-SVR\nAverage MSE\nHarmonic clarity ( HClar ) Tonal 0.38 0.32 0.33 0.29 0.33\nHarmonic predictability ( HPred ) Tonal 0.59 0.5 0.59 0.49 0.51\nRhythmic clarity ( RClar ) Rhythmic 0.43 0.27 0.3 0.26 0.26\nRhythmic coherence ( RCoh ) Rhythmic 0.56 0.4 0.43 0.35 0.36\nDancability ( Dan) Rhy. & Ton. 0.96 0.47 0.53 0.39 0.43\nAverage model order - number of features\nHarmonic clarity ( HClar ) Tonal 11 - 11 1.03 - 11 1.37 - 11 11 - 11 11 - 11\nHarmonic predictability ( HPred ) Tonal 11 - 11 1.37 - 11 2.99 - 11 11 - 11 11 - 11\nRhythmic clarity ( RClar ) Rhythmic 17 - 17 1.3 - 17 2.34 - 17 17 - 17 17 - 17\nRhythmic coherence ( RCoh ) Rhythmic 17 - 17 1.53 - 17 2.86 - 17 17 - 17 17 - 17\nDancability ( Dan) Rhy. & Ton. 28 - 28 1.27 - 28 1.06 - 28 28 - 28 28 - 28\nSample correlation (absolute value) between ground truth & prediction - p-values\nHarmonic clarity ( HClar ) Tonal 0.04 - 0.76 0.05 - 0.69 0.06 - 0.62 0.2 - 0.1 0.17 - 0.17\nHarmonic predictability ( HPred ) Tonal 0.14 - 0.26 0.59 -0 0.2 - 0.1 0.09 - 0.46 0.34 -0\nRhythmic clarity ( RClar ) Rhythmic 0.02 - 0.86 0.22 - 0.07 0.03 - 0.83 0.07 - 0.58 0.34 -0\nRhythmic coherence ( RCoh ) Rhythmic 0.1 - 0.41 0.32 -0.01 0.01 - 0.93 0.05 - 0.66 0.03 - 0.82\nDancability ( Dan) Rhy. & Ton. 0.12 - 0.32 0.24 -0.04 0.23 - 0.05 0.08 - 0.5 0.46 -0\nTable 3 . Results of the regression analysis for 5 attributes as introduced in Sect. 5 based on 5 different conï¬gurations as\nexplained in Sect. 7.1. The mean squared error (MSE) and the model order of the regression models were averaged of 70 folds\nof a leave-one-out cross-validation. The number of input features are given for each attribute. In the lower part of the table, the\nsample correlation value and p-value between the test set ground truth values and the predicted attribute values (collected over\nall folds) are shown. Signiï¬cant correlations ( p<. 05) are denoted in bold print.\nensemble recordings using rhythmic audio features. In\nProc. of the IEEE Int. Conf. on Acoustics, Speech, and\nSignal Processing (ICASSP) , 2011.\n[2] J. AbeÃŸer, H. Lukashevich, P. Br Â¨auer, and G. Schuller.\nBass playing style detection based on high-level fea-\ntures and pattern similarity. In Proc. of the Int. Society of\nMusic Information Retrieval (ISMIR), Utrecht, Nether-\nlands , 2010.\n[3] P. J. Ponce de L Â´eon and J. M. I Ëœnesta. Pattern recogni-\ntion approach for music style identiï¬cation using shal-\nlow statistical descriptors. IEEE Transactions on Sys-\ntem, Man and Cybernetics - Part C : Applications and\nReviews , 37(2):248â€“257, March 2007.\n[4] T. Eerola, O. Lartillot, and P. Toiviainen. Prediction of\nmultidimensional emotional ratings in music from au-\ndio using multivariate regression models. In Proc. of the\nInternational Society for Music Information Retrieval\nConference (ISMIR), Kobe, Japan , 2009.\n[5] Tuomas Eerola and Petri Toiviainen. MIDI Toolbox:\nMATLAB Tools for Music Research . University of\nJyvÂ¨askyl Â¨a, Jyv Â¨askyl Â¨a, Finland, 2004.[6] Chih-wei Hsu, Chih-chung Chang, and Chih-jen Lin. A\nPractical Guide to Support Vector Classiï¬cation. 1(1):1â€“\n16, 2010.\n[7] C. McKay and I. Fujinaga. jSymbolic: A feature ex-\ntractor for MIDI ï¬les. In Int. Computer Music Conf.\n(ICMC), New Orleans, USA , pages 302â€“305, 2006.\n[8] C. P Â´erez-Sancho, P. J. Ponce de Le Â´on, and J. M. I Ëœnesta.\nA comparison of statistical approaches to symbolic\ngenre recognition. In Proceedings of the Int. Computer\nMusic Conf. (ICMC), New Orleans, USA , pages 545â€“\n550, 2006.\n[9] B. Sch Â¨olkopf, A. J. Smola, R. C. Williamson, and P. L.\nBartlett. New support vector algorithms. 12:1207â€“1245,\n2000.\n[10] P. Westwood. Bass Bible . AMA, 1997.\n[11] P. Woitek, P. Br Â¨auer, and H. GroÃŸmann. A novel tool for\ncapturing conceptualized audio annotations. In Proceed-\nings of the Audio Mostly Conf., Pite Ëša, Sweden , 2010.\n214"
    },
    {
        "title": "Compression-based Similarity Measures in Symbolic, Polyphonic Music.",
        "author": [
            "Teppo E. Ahonen",
            "Kjell LemstrÃ¶m",
            "Simo Linkola"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1414816",
        "url": "https://doi.org/10.5281/zenodo.1414816",
        "ee": "https://zenodo.org/records/1414816/files/AhonenLL11.pdf",
        "abstract": "We present a novel compression-based method for measuring similarity between sequences of symbolic, polyphonic music. The method is based on mapping the values of binary chromagrams extracted from MIDI files to tonal centroids, then quantizing the tonal centroid representation values to sequences, and finally measuring the similarity between the quantized sequences using Normalized Compression Distance (NCD). The method is comprehensively evaluated with a test set of classical music variations, and the highest achieved precision and recall values suggest that the proposed method can be applied for similarity measuring. Also, we analyze the performance of the method and discuss what should be taken into consideration when applying the method for measurement tasks.",
        "zenodo_id": 1414816,
        "dblp_key": "conf/ismir/AhonenLL11",
        "keywords": [
            "binary chromagrams",
            "tonal centroids",
            "quantizing",
            "Normalized Compression Distance (NCD)",
            "comparing music sequences",
            "symbolic music",
            "polyphonic music",
            "comprehensively evaluated",
            "test set",
            "classical music variations"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nCOMPRESSION-BASED SIMILARITY MEASURES\nINSYMBOLIC, POLYPHONIC MUSIC\nTeppoE.Ahonen,Kjell Lemstr Â¨om,SimoLinkola\nDepartment of Computer Science\nUniversity ofHelsinki\n{teahonen,klemstro,slinkola }@cs.helsinki.fi\nABSTRACT\nWe present a novel compression-based method for measur-\ning similarity between sequences of symbolic, polyphonic\nmusic. The method is based on mapping the values of bi-\nnary chromagrams extracted from MIDI ï¬les to tonal cen-\ntroids,thenquantizingthetonalcentroidrepresentation val-\nues to sequences, and ï¬nally measuring the similarity be-\ntweenthequantizedsequencesusingNormalizedCompres-\nsionDistance(NCD).Themethodiscomprehensivelyeval-\nuated with a test set of classical music variations, and the\nhighestachievedprecisionandrecallvaluessuggestthatt he\nproposed method can be applied for similarity measuring.\nAlso, we analyze the performance of the method and dis-\ncusswhatshouldbetakenintoconsiderationwhenapplying\nthe method for measurement tasks.\n1. INTRODUCTION\nMeasuring similarity between symbolically encoded music\nhas been studied extensively, with several approaches ex-\nisting. Similarity measuring between pieces of polyphonic\nmusic is far from trivial, but the extensive amount of ap-\nplications(forexample,querybyexamplemusicretrieving )\nthatrequiresuchmeasuringmotivatestoexplorenoveltech -\nniques for thetask.\nHere, we present an approach that is based on mapping\nthe pitches present in a given time frame to tonal centroid\nvectors, quantizing the tonal centroid values, and represe nt-\ning the obtained information as a sequence of characters.\nFor measuring similarity between sequences, we apply nor-\nmalizedcompressiondistance(NCD)[2],whichisaparamete r-\nfree, quasi-universal similaritymetric[2].\nWe believe that NCD is applicable to polyphonic, sym-\nbolic music similarity measuring, since there are already\nPermission to make digital or hard copies of all or part of this w ork for\npersonal or classroom use is granted without fee provided th at copies are\nnot made or distributed for proï¬t or commercial advantage and th at copies\nbear this noticeand thefull citation ontheï¬rst page.\nc/circlecopyrt2011International Society forMusic InformationRetrieval .several existing, well-performing classiï¬cation and clus ter-\ningapproachesutilizingNCDasthesimilaritymetric. How-\never,thepreviouslydevelopedNCD-basedmethodsallseem\ntorelyonrathercruderepresentationsofmusic,suchassky -\nline reduction or melodic contour description, both which\nlose a signiï¬cant amount of tonal information. We wish to\nkeep as much of the tonal information included as possible\nbut the amount of parallel pitch values may be large, even\nif the octave information is ignored and values are reduced\nintoa12-dimensionchromagram(alsoknownaspitchclass\nproï¬le). Therefore, dimension reduction is needed, and for\nthis, we use a method with musical knowledge. This is\nwhere the tonal centroid representation [4] seems a feasi-\nble solution, as it turns a 12-dimension chromagram into a\n6-dimensional representation, still holding most of the ha r-\nmonic information and also some of the melodic informa-\ntion.\nTo see how well our approach performs in a particular\nsimilarity measuring task, we use it to determine whether a\ngiven piece of music is a variation of an original theme in-\ncluded in the training data. This task is challenging and a\nvery suitable way to evaluate our method, as it is objective\n(in comparison to, for example, genre classiï¬cation), and\nin order to be successful, the method must retain tonal in-\nformation and still be able to measure the essential musical\nsimilarity without too much complexity. We evaluate our\napproach with a set of 18 classical compositions and their\nvariations, and based on the results, discuss the remarks we\ndiscoveredinourevaluations,suggestingseveralissuest hat\nneed to be addressed when using NCD for similarity mea-\nsuring with polyphonic music, and present ideas for future\nresearch work.\nThe rest of this paper is organized as follows. In Sec-\ntion 2 we review methods of similarity measuring between\nsymbolic, polyphonic pieces of music. In Section 3 we\npresent the methods used for extracting tonal information\nfrom MIDI ï¬les and representing the information in a for-\nmat suitable for compressor-based similarity measurement .\nExperiments on the method are presented in Section 4 and\nconclusions inSection 5.\n91Poster Session 1\n2. RELATED WORK\nIn the early days of similarity measuring of symbolically\nencoded music, linear string representation together with\nstringmatchingmethodswasmostoftenused. Theapproach,\nhowever, does not work with polyphonic music in general,\nexceptforsomeveryspeciï¬ccases(seee.g.[3,7]). Recentl y\nseveral authors have applied geometric modeling of music\nfor the task (see e.g. [9,13,16]). Many of these algorithms\nare based on computing translation vectors, which makes\nthem transposition-invariant and also allows for extra int er-\nveningnotesthatappearinoneofthepiecesofmusicunder\nconsideration but not in the other. Recent geometric meth-\nodshavealsochallengedtimingproblems;thepiecesofmu-\nsic under consideration may be either time-scaled [5,13] or\ntime-warped [6] copies of each other.\nInteresting alternative approaches can be found in [10,\n17]. In his PhD thesis [17], Rizo introduces a tree repre-\nsentationforpolyphonicmusicandshowshowtoapplydif-\nferent tree matching algorithms for various similarity cas es\nincluding variation recognition. He also shows how to use\nhis generic representation for implementing and illustrat ing\nschenkerian reduction. In [10], Marsden concentrates on\nschenkerian reduction in recognizing polyphonic variatio ns\nof the classical era. To this end, he divides polyphony in\nthree â€˜voicesâ€™: melody, middle and bass. Melody and bass\ncontainthehighestandlowestnotes,respectively,whilem id-\ndle voice contains all the notes belonging to neither of the\nprevioustwo. Usingsuchareductionofpolyphonyhestud-\nieswhetheramethodbasedonschenkerianreductionwould\noutperform another method based just on surface analysis,\nbut found noevidence tosupport that hypothesis.\nIn the literature, one can also ï¬nd several compression-\nbased approaches for similarity measuring in symbolic mu-\nsic. In[2],NCDisusedasthesimilaritymeasureforcompose r-\nand genre-based clustering experiments. The method ex-\ntracts key-invariant melodic contours from the MIDI ï¬les,\nandconstructsadistancematrixfortheclusteringalgorit hm\nusing NCD as the similarity metric. In [8], Kolmogorov\ncomplexity is estimated as the size of the dictionary pro-\nduced by the LZ78 compression algorithm. Based on this\nestimation, k-NN classiï¬cation is applied for melodies rep -\nresented as both absolute and relative values. In [11], NCD\nisappliedassimilaritymeasureforstringrepresentation sof\nmusic,obtained byconverting symbolicmusicwithagraph\nstructure representation. In a recent study, NCD is used as\none of the possible similarity metrics for measuring simi-\nlarity between bass lines [15]. The bass line melodic in-\nterval histogram similarities are used as a feature for genr e\nclassiï¬cation. In [1], NCD is used for measuring similar-\nity between MIDI pieces. The polyphonic MIDI melodies\nare converted into monophonic versions by taking only the\nhighestpitchvaluepresentinatimeslice(alsoknownasthe\nskyline representation).Another method based on measuring the amount of sim-\nilarinformationbetweenpiecesofpolyphonicmusicispre-\nsented in [12]. Their work is based on using Kullback-\nLeibler divergence to measure similarity between chroma-\ngram sequences. Based on the chromagrams, a 24-chord\nlexicon (all the major and minor triad chords in the western\ntonal scale) is used to create a probability distribution, a nd\nthen Kullback-Leibler divergence is applied to measure the\nsimilarity between the distributions of the query and targe t\nmodels. The method is evaluated with classical variation\nrecognition.\n3. METHODOLOGY\nLet us next introduce the methods that we will use for ex-\ntracting tonal information from the MIDI ï¬les and how to\nrepresent the information in an appropriate format for the\ncompression-based similarity measurement. Figure 1 de-\npictsablueprintofthesystemcomponentsandthedatapro-\ncessing.\n3.1 Binary Chroma Representation\nChroma vector is a 12-dimension representation of notes,\nstrippedfromoctaveinformation,thatarepresentinagive n\ntime frame. We encode MIDI ï¬les as binary chroma vector\nsequences by ï¬rst chopping the piece of music into slices\nof1\n4the length of the duration of a quarter note, then con-\ncatenating these slices to form the sequences and, ï¬nally,\nmappingeachsequencewithnotesplayinginthatparticular\ntimeframe.\n3.2 Tonal Centroid Representation\nToreducethevariationof 212= 4096possiblechromagram\nvectors, the method explained in [4] is used to transform\nchromavectorsintotonalcentroidvectors. Asthetonalcen -\ntroid vector has values ranging in âˆ’1â‰¤kâ‰¤1,kâˆˆR, for\nall six dimensions, we quantize each value to 0 or 1 using\nthe median of all the possible values of chroma vectors in\ntheparticulardimensionoftonalcentroidrepresentation asa\nthreshold,thuseffectivelylesseningthealphabetto 26= 64.\nThe tonal centroid vector for time frame tis given by\nformula:\nÎ¶t(d) =1\n||ct||11/summationdisplay\np=0Î¦(d,p)ct(p) (1)\nwhere 0â‰¤dâ‰¤5and0â‰¤pâ‰¤11.||ct||denotes the L1-\nnorm of chroma vector ct,pis the pitch class index in ct,d\nrepresentswhichofthedimensionsoftonalcentroidisbein g\ncalculated and Î¦ = [ Ï†0,Ï†1,... ,Ï† 11]is the transformation\nmatrixwhere\n9212th International Society for Music Information Retrieval Conference (ISMIR 2011)\nFigure 1. Blueprint of the components.\nÏ†p=ï£«\nï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­Î¦(0,p)\nÎ¦(1,p)\nÎ¦(2,p)\nÎ¦(3,p)\nÎ¦(4,p)\nÎ¦(5,p)ï£¶\nï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£¸=ï£«\nï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­sinp7Ï€\n6\ncosp7Ï€\n6\nsinp3Ï€\n2\ncosp3Ï€\n2\n1\n2sinp2Ï€\n3\n1\n2cosp2Ï€\n3ï£¶\nï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£¸,0â‰¤pâ‰¤11. (2)\n3.3 Normalized Compression Distance\nTo measure the similarity of two different pieces of music\nwe use NCD to see how close the quantized 6-dimensional\ntonal centroid vectors are to each other. The NCD is shown\nto be aquasi-universal similaritymetricin[2] as it approx i-\nmatesnormalizedinformationdistance(NID)uptoanerror\ndepending on the quality of the compressor that is used in\nthe calculation.\nNormalizedinformationdistanceisbasedonKolmogorov\ncomplexity of the given object and is a universal metric inthesensethatituncoversallthesimilaritiesoftheobject sat\nthe same time. Kolmogorov complexity of an object is the\nlengthoftheshortestbinaryprogramthatoutputstheobjec t\nonauniversalcomputer. If K(x)denotesKolmogorovcom-\nplexity of x, then K(x|y)denotes conditional Kolmogorov\ncomplexity of xgiven yas an input. NID for xandyis\ngiven by theformula\nNID(x,y) =max{K(x|y),K(y|x)}\nmax{K(x),K(y)}.(3)\nAsKolmogorovcomplexityofanobjectisnon-computable,\nwe cannot calculate NID. However, we can approximate it\nwithstandard compression algorithms.\nLetCbe a lossless data-compression algorithm which\nsatisï¬estherequirementsofreferencecompressormention ed\nin [2]. Ccan be used to approximate K.C(x)is used to\ndenotethelengthof xcompressedwith CandC(xy)tode-\nnotethelengthofconcatenated xandycompressedwith C.\nC(x|y)can also be deï¬ned as C(x|y) =C(xy)âˆ’C(y),\nwhich tells us the amount of bits of information in xre-\nlated to y. Now the normalized compression distance can\nbe given by theformula\nNCD (x,y) =C(xy)âˆ’min{C(x),C(y)}\nmax{C(x),C(y)},(4)\nwhichisshownin[2]toapproximatetheNIDformulamen-\ntioned inEquation 3.\n4. EXPERIMENTS\n4.1 Test Data\nWe use the polyphonic classical music variation dataset de-\nscribed in [17] for our experiments. It is a fairly extensive\ncollection of classical themes and their variations, rangi ng\nover different values in terms of instrumentation, lengths ,\neras and numbers of voices. In addition, we also included\ntwo more compositions: the Haydn variations by Brahms,\nOp. 56 (nine variations of a theme), and the Piano Sonata\nnumber 11 by Mozart, KV 331 (six variations of a theme).\nThe total size of our dataset is 18 themes and 84 variations,\ntotalling 102 pieces of music, with the highest number of\nvariations for atheme being 30 and lowest being 1.\n4.2 Evaluations and results\nFor evaluations, we used the original themes as the training\ndata and the variations as queries. We ran the classiï¬cation\ntests with several different parameters. For each differen t\nevaluation, the overall accuracy and average precision, av -\nerage recall, and average f-measureare reported.\nFirst, we had to select the data-compression algorithm.\nWe ran the classiï¬cation test with bzip2 and prediction by\n93Poster Session 1\nCompressor bzip2PPMZ\nAccuracy 0.3930.536\nPrecision 0.5260.632\nRecall 0.3910.474\nF-measure 0.4480.542\nTable 1. Comparison between used compression algo-\nrithms.\npartial matching (PPMZ) compression algorithms. The re-\nsults for the two algorithms are presented in Table 1. Since\nusing the PPMZ algorithm produced slightly better results,\nwe conducted the rest of the evaluations using it as the se-\nlected compression algorithm.\nInadditiontotheassumptionthatvariationsareperformed\ninthesamekeyastheoriginal,wewantedtobeabletomea-\nsuresimilaritybetweenpiecesofmusicindifferentkeys. I n\nordertotransposetwochromasequencesintothesamekey,\nweusedOptimalTranspositionIndex(OTI)[14],wherethe\nmost likely transposition between two chromagrams is cal-\nculated by measuring the dot product between all 12 pos-\nsible transpositions of the chromagrams summed over time\nandnormalized. HavingcalculatedOTI,werotatedthequery\nbinarychromagramaccordingtotheOTIvaluebeforemak-\ning the tonal centroid transformation and writing the trans -\nformed sequence toaï¬le.\nThe sequences produced by our method have occasional\nshort sections of outliers, caused by transitional anomali es\nproduced by our time-slicing MIDI-extraction method and\nresulting in overall noisy sequences. Such noise can be\nharmful for compression-based similarity measuring, sinc e\nnoiseindatareducesthecompressibility,thusresultingp os-\nsibly in a lower performance in classiï¬cation. However, the\nanomalies could also be signiï¬cant distinguishing feature s\ninthesequences. Anillustrationofchangesinthesequence\nindices is depicted in Figure 2, with several noisy spikes\nclearly visible. To get rid of the transitions and make the\nsequences smoother, we experimented with median ï¬lter-\ning,andranmedianï¬lteroforder5tothesequencesbefore\nwriting them into ï¬les. An example of a median-ï¬ltered se-\nquence isdepicted inFigure 3.\nThe results for the evaluations with OTI and median-\nï¬ltered sequences are presented inTable 2.\n4.2.1 Clustering experiment\nIn addition to the supervised classiï¬cation, we carried out\nan experiment with unsupervised machine learning, and for\nthis, we ran a k-medians clustering for the whole dataset,\nusing the NCD values as the distances between the objects\nwhen formingtheclusters.\nWe ran the NCD-based k-medians clustering by setting\nK= 18(there are 18 different original themes and their0 200 400 600 800 1000 12000102030405060Tchaikovsky: Variations on a rococo theme, Op. 33\nTime frameIndex\nFigure 2. Sequence illustration of theme of Variations on a\nrococo theme, Op. 33 by Tchaikovsky.\nvariations). As the k-medians algorithm selects the initia l\ncluster centroids randomly, we ran the evaluation 5 times.\nThe results reported here are averaged over the evaluation\nruns.\nWedidnotexpectthe18differentthemesandtheirvaria-\ntions to group into clean clusters, but to evaluate the perfo r-\nmance, we measured the number of different clusters (i.e.\nclusters that have centroids that are not original versions\nor variations of the centroids of the other clusters) and the\nnumber of correct clusterings (i.e. cases where the piece of\nmusic is clustered into a cluster with a centroid that is the\noriginal version or a variation of the piece). The number of\ndifferentclusterswas11.8,andthenumberofcorrectlyclu s-\ntered compositions was 68. Thus, although not every theme\nandvariationsfamilyofthedatasetresultsinasingle,sep a-\nratecluster, asigniï¬cant number of thecompositions issti ll\nclustered correctly.\n4.3 Discussion\nBasedontheresultsoftheprevioussubsection,thepropose d\nmethoddoesseemtohavepotentialformeasuringsimilarity\nbetweenpolyphonic,symbolicpiecesofmusic. Forcompar-\nison, the results reported for three different methods in [1 7]\nall have precision and recall values ranging from 0.4 to 0.5,\nwith even a slightly smaller test data set (16 themes and 70\nvariations). The highest performance of our method is on a\npar withthese results.\nWhenusingNCD,themostcrucialchoiceisthedatarep-\nresentation. Having ï¬xed the representation, the next im-\nportant choice to be made is to select an appropriate data-\ncompression algorithm. Considering the idea that using a\nmore efï¬cient compressor algorithm yields a better approx-\nimationofKolmogorov complexity, itwouldseemtrivialto\n9412th International Society for Music Information Retrieval Conference (ISMIR 2011)\n0 200 400 600 800 1000 12000102030405060Tchaikovsky: Variations on a rococo theme, Op. 33, median filtered\nTime frameIndex\nFigure 3. Sequence illustration of theme of Variations on\na rococo theme, Op. 33 by Tchaikovsky, with a median\nï¬ltering oforder 5.\nProcessing noneOTIMFOTI & MF\nAccuracy 0.5360.2500.291 0.190\nPrecision 0.6320.2310.409 0.165\nRecall 0.4740.1520.393 0.193\nF-measure 0.5420.1840.401 0.178\nTable2. Classiï¬cationresultswithnoadditionalprocessing\napplied, with OTI applied, with median ï¬ltering (MF) ap-\nplied, and with both OTI and median ï¬ltering applied. All\nevaluations are conducted using PPMZ as compression al-\ngorithm.\nusethemostefï¬cientcompressor. However,thereisnoway\nof knowing how well the compression algorithm actually\napproximates Kolmogorov complexity, and thus, selecting\nonly an efï¬cient compressor does not necessarily guarantee\nthat the NCD used (Equation 4) is actually a valid approx-\nimation of NID (Equation 3). As stated in [2], it is theo-\nretically possible that when the compression gets more efï¬-\ncient, theNCD value disentangles fromthe NID value.\nIn our experiments, the more efï¬cient PPMZ algorithm\ndideventuallyyieldbetterresults. Wesupposethatthisha p-\npens due to the statistical nature of PPMZ, where the dif-\nferences in ï¬le lengths is a lesser problem than with other\ncompression approaches, and the effect of normalization in\nNCDismorelikelytohappen. Keepinginmindthatthese-\nquencesweoperatewitharerelativelyshort,thedifferenc es\nbetweenlongestandshortestï¬lescanpotentiallycausebia s,\nasthelonger xbecomes,thebetter C(x)approximates K(x).\nThekeyoftheperformancesisanimportantfactorwhen\nmeasuring similarity between pieces of music. In our eval-\nuations the results are better when OTI is not calculated,as most of the variations are in the same key as the orig-\ninal theme. This is clearly a problem when considering\nto apply the method for other similarity measuring tasks.\nIt is possible that the OTI algorithm, although very use-\nful with audio-extracted chromagrams, might not be a suit-\nable method when approximating the tonal similarity be-\ntween two binary chromagrams extracted from MIDI data,\nandsomeotherkey-estimationalgorithmcouldperformbet-\nter for the task. It is also noteworthy that even though sev-\neral variations are in different keys they are still classiï¬ ed\ncorrectly, possibly because our quantization method of the\ntonal centroid vectors maps several combinations of notes\nintothe samecharacters, assuming they are innearby keys.\nThe noise in the sequences, caused by transients of the\ntime-slice chopping in the MIDI extraction method, may\nseem like an identiï¬cation-distracting feature. Based on\nthe results, however, it seems that the median ï¬ltering, al-\nthough making the sequences smoother, does not provide\nbetter classiï¬cation accuracy. This suggests that noise it self\nis not a hindrance as long as a suitable compression algo-\nrithmisused,andover-reducingthesequenceslosesimpor-\ntant information that could be rather useful for distinguis h-\ning.\nItshouldbenotedthatthevariationdatabaseissomewhat\nunevenlydistributed,withseveralthemeshavingonlyasin -\ngle variation included. The average precision of our system\nis slightly biased due to the high success rate of correctly\nclassifying such variations, but the accuracy still suppor ts\nthat the method can be used for the selected task. Also, an\ninteresting notion is that in some cases, there is confusion\nbetween different pieces of music by the same composer:\nThe Goldberg Variations and English Suites by Johann Se-\nbastian Bach are occasionally confused. This suggests that\ntheNCD-basedsimilaritymeasuringcouldbeusedforcom-\nposeridentiï¬cation,assomestylisticinformationofthec om-\nposer seems tobecaptured withthe method.\n5. CONCLUSIONS\nWe have presented a method for measuring similarity be-\ntween symbolic, polyphonic pieces of music. Our method\ntakestheMIDIdata,extractsabinarychromagramoutofit,\nmaps the binary chromagram to tonal centroid representa-\ntion and ï¬nally quantizes it, casting the original MIDI data\nintoasequenceofcharacterscomprisinganalphabetofsize\n64. Then, the similarity between character sequences is\nmeasured usingacompression-based similaritymetric.\nWeexperimentedwithbothsupervisedandunsupervised\nmachinelearningwithadatasetconsistingofclassicalthe mes\nand their variations. The classiï¬cation yielded results th at\narecomparablewiththestate-of-the-artresultswiththes ame\ndataset. The clustering method used was a compression-\nbased variant of k-medians, and using this novel method\n95Poster Session 1\nproduced rather clean, well-structuredclusters.\nWeintendtodevelopourapproachtoamoregeneralsim-\nilarity measuring technique suitable for different classi ï¬ca-\ntion, clustering and identiï¬cation tasks. In order to be mor e\nsuccessful, several issues need to be reconsidered. One of\nthe challenges is caused by the possible differences in keys\nbetween the pieces of music. Here, we solved the transpo-\nsition problem by using OTI, which has been successfully\nused with audio data. However, applying OTI caused infe-\nrior results, and we need to either examine what could be\ndone by using OTI with symbolic data, or select another\nmethod or representation to allow key-invariant similarit y\nmeasuring.\n6. ACKNOWLEDGEMENTS\nThe work of Teppo E. Ahonen was supported by Helsinki\nGraduateSchoolinComputerScienceandEngineering(Hecse ).\nThe authors also wish to thank David Rizo for his poly-\nphonic variation collection.\n7. REFERENCES\n[1] Z. Cataltepe, Y. Tsuchihashi, and A. Sonmez. Mu-\nsic genre classiï¬cation using midi and audio features.\nEURASIP Journal on Advances in Signal Processing ,\n24(1), 2007.\n[2] R. Cilibrasi and P. Vitanyi. Clustering by compression.\nIEEETransactionsonInformationTheory ,51(4):1523â€“\n1545, April 2005.\n[3] M.J. Dovey. A technique for â€œregular expressionâ€ style\nsearching in polyphonic music. In Proc. ISMIRâ€™01 ,\npages 179â€“185, 2001.\n[4] C. Harte, M. Sandler, and M. Gasser. Detecting har-\nmonicchangeinmusicalaudio.In Proc.1stACMWork-\nshoponAudioandMusicComputingMultimedia ,pages\n21â€“26, 2006.\n[5] K. Lemstr Â¨om. Towards more robust geometric content-\nbased music retrieval. In Proc. ISMIRâ€™10 , pages 577â€“\n582, 2010.\n[6] K. Lemstr Â¨om and M. Laitinen. Transposition and time-\nwarp invariant geometric music retrieval algorithms. In\nProc. ADMIREâ€™11, Third International Workshop on\nAdvances inMusicInformation Research , 2011.\n[7] K. Lemstr Â¨om and J. Tarhio. Transposition invariant pat-\ntern matching for multi-track strings. Nordic Journal of\nComputing , 10(3):185â€“205, 2003.\n[8] M. Li and R. Sleep. Melody classiï¬cation using a sim-\nilarity metric based on Kolmogorov Complexity. Sound\nand MusicComputing , 2004.[9] A. Lubiw and L. Tanur. Pattern matching in polyphonic\nmusic as a weighted geometric translation problem. In\nProc. ISMIRâ€™04 , pages 289â€“296, 2004.\n[10] A. Marsden. Recognition of variations using automatic\nschenkerian reduction. In Proc. ISMIRâ€™10 , pages 501â€“\n506, 2010.\n[11] B.Mokbel,A.Hasenfuss,andB.Hammer.Graph-based\nrepresentation of symbolic musical data. In Proc. 7th\nIAPR-TC-15 International Workshop on Graph-Based\nRepresentations in Pattern Recognition , GbRPR â€™09,\npages 42â€“51, 2009.\n[12] J. Pickens, J. Bello, G. Monti, M. Sandler, T. Crawford,\nM. Dove, and D. Byrd. Polyphonic score retrieval us-\ningpolyphonicaudioqueries: Aharmonicmodelingap-\nproach.JournalofNewMusicResearch ,32(2):223â€“236,\n2003.\n[13] C.A. Romming and E. Selfridge-Field. Algorithms for\npolyphonic music retrieval: The hausdorff metric and\ngeometric hashing. In Proc. ISMIRâ€™07 , pages 457â€“462,\n2007.\n[14] J.Serra, E.G Â´omez, andP.Herrera. Transposingchroma\nrepresentations to a common key. In Proc. IEEE CS\nConference on The Use of Symbols to Represent Music\nand Multimedia Objects , pages 45â€“48, 2008.\n[15] U. Simsekli. Automatic music genre classiï¬cation us-\ning bass lines. In Proc. 20th International Conference\nonPattern Recognition , pages 4137â€“4140, 2010.\n[16] E. Ukkonen, K. Lemstr Â¨om, and V. M Â¨akinen. Geometric\nalgorithmsfortranspositioninvariantcontent-basedmu-\nsicretrieval. In Proc. ISMIRâ€™03 , pages 193â€“199, 2003.\n[17] D. Rizo Valero. Symbolic music comparison with tree\ndata structures . PhD thesis, Universidad de Alicante,\nAugust 2010.\n96"
    },
    {
        "title": "Music Genre Classification using Similarity Functions.",
        "author": [
            "Yoko Anan",
            "Kohei Hatano",
            "Hideo Bannai",
            "Masayuki Takeda"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1418019",
        "url": "https://doi.org/10.5281/zenodo.1418019",
        "ee": "https://zenodo.org/records/1418019/files/AnanHBT11.pdf",
        "abstract": "We consider music classification problems. A typical machine learning approach is to use support vector machines with some kernels. This approach, however, does not seem to be successful enough for classifying music data in our experiments. In this paper, we follow an alternative approach. We employ a (dis)similarity-based learning framework proposed by Wang et al. This (dis)similarity-based approach has a theoretical guarantee that one can obtain accurate classifiers using (dis)similarity measures under a natural assumption. We demonstrate the effectiveness of our approach in computational experiments using Japanese MIDI data.",
        "zenodo_id": 1418019,
        "dblp_key": "conf/ismir/AnanHBT11",
        "keywords": [
            "music classification",
            "support vector machines",
            "kernels",
            "Japanese MIDI data",
            "dis/similarity-based learning",
            "Wang et al.",
            "accuracy",
            "computational experiments",
            "natural assumption",
            "effective"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nMUSIC GENRE CLASSIFICATIONUSING SIMILARITY FUNCTIONS\nYokoAnan, KoheiHatano, Hideo Bannai and Masayuki Takeda\nDepartmentof Informatics, KyushuUniversity\n{yoko.anan, hatano, bannai, takeda }@inf.kyushu-u.ac.jp\nABSTRACT\nWe consider music classiï¬cation problems. A typical ma-\nchine learning approach is to use support vector machines\nwith some kernels. This approach, however, does not seem\nto be successful enough for classifying music data in our\nexperiments. In this paper, we follow an alternative ap-\nproach. We employ a (dis)similarity-based learning frame-\nworkproposedbyWangetal. This(dis)similarity-basedap-\nproachhas a theoreticalguarantee thatone canobtain accu-\nrate classiï¬ers using (dis)similarity measures under a natu-\nralassumption. Wedemonstratetheeffectivenessofourap-\nproach in computational experiments using Japanese MIDI\ndata.\n1. INTRODUCTION\nMusic classiï¬cation is an important problem in information\nretrieval from music data. There are a lot of researches to\ntackletheproblem(see,e.g.,[1,3,4,10,11,14,18]),ashighly\naccurate music classiï¬ers are useful for music search and\nfeature extraction.\nOne of typical approaches to classify music is to rep-\nresent each music data as a feature vector, which is then\nclassiï¬ed by standard machine learning methods. On the\nother hand, ï¬nding good features for music classiï¬cation is\na non-trivial task. For example, performance worm [15],\nperformance alphabet [16], and other approaches includ-\ning [1,10,11,18].\nAnotherpopularapproachinMachineLearningistouse\nsupport vector machines (SVMs) with kernels [7â€“9,12,19].\nOne way to improve accuracy of music classiï¬cation is to\ndesign a good kernel for music data. This approach, how-\never, does not seem to be very successful so far. As we\nwill show later, well known string kernels such as n-gram\nkernels [12] and mismatch kernels [8] for texts do not ob-\ntain satisfactory results for music classiï¬cation in our ex-\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proï¬t or commercial advantage and that copies\nbear this notice and the full citation on the ï¬rst page.\nc/circlecopyrt2011 International Society for Music Information Retrieval.periments. Further, to design a kernel, the function to be\ndesignedneedstobepositivesemideï¬nite,whichisalimita-\ntionwhenwetrytoexploitthestructureofmusictoimprove\nclassiï¬cation accuracy.\nInthispaper,wefollowanalternativeapproach. Weem-\nploy a (dis)similarity-based learning framework proposed\nbyWangetal.[20]. This(dis)similarity-basedapproachhas\na theoretical guarantee that one can obtain accurate classi-\nï¬ersusing(dis)similaritymeasuresunderanaturalassump-\ntion. In addition, the advantage of this approach is able to\nuseany(dis)similarity measures which do not have to be\npositivesemideï¬nite and anydata.\nFurther, we combine this (dis)similarity-based learning\napproach with 1-norm soft margin optimization formula-\ntion[5,22]. Anadvantageoftheformulationisthatitisuse-\nful for feature selection because of the sparse nature of the\nunderlying solution. In other words, the formulation help\nus to ï¬nd â€œrelevantâ€ instances (i.e., music data) to classify\nmusic. Suchrelevantinstancesmightcontainrepresentative\nfeaturesoftheclass. Therefore,itmightbeusefultoextract\ngood features.\nFor simplicity, throughout the paper, we deal with clas-\nsiï¬cation problems of symbolic music data such as MIDI\nï¬lesonly. Thuswedonotconsideraudiosignaldataandwe\nassume (dis)similarity functions over texts. Note that our\nframework using (dis)similarity functions does not depend\non the data format. We can deal with audio signal data as\nwell if we employ(dis)similarity functions oversignals.\nWedemonstratetheeffectivenessofourapproachincom-\nputational experiments using Japanese music data. Our ap-\nproach,combinedwithnon-positivesemideï¬nite(dis)similarity\nmeasures such as edit distance, shows better performance\nthan SVMs with string kernels.\n2. LEARNING FRAMEWORKUSING\nDISSIMILARITYFUNCTION\nIn this section, we review a learning framework using dis-\nsimilarity function proposed by Wang et al. [20]. Let Xbe\nthe instance space. We assume that a dissimilarity function\nd(x, x/prime)is a function from XÃ—XtoR+. A pair (x, y)of\ninstance xâˆˆXand label yâˆˆ {âˆ’ 1,1}is called an example.\nFor instance, Xmight be some set of MIDI data and then\n693Poster Session 6\nan example is a pair of a MIDI ï¬le and positive or negative\nlabel. The learner is given a set Sof examples, where each\nexample is drawn randomly and independently from an un-\nknowndistribution Pover XÃ—{âˆ’ 1,+1}. Then,thelearner\nis supposed to output a hypothesis h(x) : X â†’ {âˆ’ 1,1}.\nThe goal of the learner is to minimize the error of the hy-\npothesis hw. r. t. the distribution P, i.e., the probability\nthathmisclassiï¬es the label of a randomly drawn example\n(x, y)according to P,Pr\n(x,y)âˆ¼P(h(x)/negationslash=y).In particular, we\nassumethatahypothesisisconstructedusingadissimilarity\nfunction d. Also, we will use the notation that sgn[a] = 1if\na >0andâˆ’1otherwise.\nThen we show a deï¬nition of â€œgoodâ€ dissimilarity func-\ntion.\nDeï¬nition 1 (Strong (/epsilon1, Î·)-goodness, Wanget al. [20])\nA dissimilarity function d(x, x/prime)is said to be strongly ( /epsilon1,Î·)-\ngood,ifatleast 1âˆ’/epsilon1probabilitymassofexamples zsatisfy:\nPr\nz/prime,z/prime/primeâˆ¼P(d(x, x/prime)< d(x, x/prime/prime)|y/prime=y, y/prime/prime=âˆ’y)â‰¥1/2+Î·/2\n(1)\nwheretheprobabilityisoverrandomexamples z/prime= (x/prime, y/prime)\nandz/prime/prime= (x/prime/prime, y/prime/prime).\nRoughly speaking, this deï¬nition says that for the most of\nrandomexamples z= (x, y)andrandompositiveandnega-\ntive examples, the instance xis likely to be closer to the in-\nstancewiththesamelabel. Then,underthenaturalassump-\ntionthatthegivendissimilarityfunction dis(/epsilon1, Î·)-good,we\ncan construct an accurate classiï¬er based on d, as is shown\nin the followingtheorem.\nTheorem1 (Wanget al. [20]) Ifdisastrongly (/epsilon1, Î·)-good\ndissimilarity function, then with probability at least 1âˆ’Î´\nover the choice of m= (4 /Î·2) ln(1 /Î´)pairs of examples\n(z/prime, z/prime/prime)with labels y/prime= 1, y/prime/prime=âˆ’1, i= 1,2, ..., m, the\nfollowing classiï¬er F(x) = sgn[ f(x)]where\nf(x) =1\nmnâˆ‘\ni=1sgn[d(x, x/prime/prime\ni)âˆ’d(x, x/prime\ni)]\nhas an errorrateof no morethan /epsilon1+Î´. That is\nPr\nzâˆ¼P(F(x)/negationslash=y) = Pr\nzâˆ¼P(yf(x)â‰¤0)â‰¤/epsilon1+Î´.\nThis theorem says that an unweighted voting classiï¬er con-\nsistingofsufï¬cientlymanyrandomlydrawnexamplesisac-\ncurate enough with high probability. We should note that\nthe existence of a (/epsilon1, Î·)-good dissimilarity function might\nbe too restrictive in some cases. For such cases, Wang et al.\nalsoproposedmorerelaxeddeï¬nitionsofgooddissimilarity\nfunctions. Under such relaxed deï¬nitions, it can be shown\nthat there existsa weighted combination\nf(x) =mâˆ‘\ni=1wihi(x),where each wiâ‰¥0,âˆ‘\niwi= 1,hi(x) = sgn[ d(x/prime/prime\ni, x)âˆ’\nd(x/prime\ni, x)]andx/prime/prime\niandx/prime\niare positive and negative instances,\nsuch that sgn[f(x)]is accurate enough (see [20] for the de-\ntails).\n3. OUR FORMULATION\nInthissection,weconsiderhowtoï¬ndanaccurateweighted\ncombination of base classiï¬ers consisting of a pair of posi-\ntiveandnegativeinstances. Todoso,weemploythe 1-norm\nsoft margin optimization, which is a standard formulation\nof classiï¬cation problems in Machine Learning (see,e.g, [5,\n21]). Simply put, the problem is to ï¬nd a linear combi-\nnation of base classiï¬ers (or a hyperplane over the space\ndeï¬ned by base classiï¬ers) which has large margin with re-\nspecttoexamples,wherethemarginofalinearcombination\nwwithrespecttoanexample zisadistancebetween wand\nz. Infact,thelargemargingeneralizationtheory(e.g.,[17])\nguarantees that a weighted combination of base classiï¬er\nis likely to have higher accuracy when it has larger margin\nw.r.t. examples. Further, an additional advantage of 1-norm\nsoft margin optimization is that the resulting linear combi-\nnation of base classiï¬ers is likely to be sparse since we reg-\nularize 1-norm of the weight vector. This property is useful\nfor feature selection tasks.\n3.1 The 1-norm soft marginformulation\nSupposethatwearegivenaset S={(x1, y1), . . . , (xm, ym)},\nwhere each (xi, yi)is an example in XÃ— {âˆ’ 1,+1}. Here,\nfollowing the dissimilarity-based approach in the previous\nsection, we assume the set of hypotheses, H={h(x) =\nsgn[d(xi, x)âˆ’d(xj, x)]|xiandxjare positive and neg-\native instances in S, respectively }. For simplicity of the\nnotation, we denote HasH={h1, . . . , h n}, where nis\nthe number of pairs of positive and negative examples in S.\nThen, the 1-norm soft margin optimization problem is for-\nmulated as follows(e.g. [5,21]):\nmax\nÏ,bâˆˆR,wâˆˆRn,Î¾âˆˆRmÏâˆ’1\nÎ½mâˆ‘\ni=1Î¾i (2)\nsub.to\nyi(âˆ‘\njwjhj(xi) +b)â‰¥Ïâˆ’Î¾i(i= 1, . . . , m ),\nwâ‰¥0,nâˆ‘\nj=1wj= 1\nÎ¾â‰¥0.\nHeretheterm yi(âˆ‘\njwjhj(xi)+b)representsthemargin\nofthehyperplane (w, b)w.r.t. anexample (xi, yi)whenthe\n1-norm of wis constrained to be 1. It is known that the\nmarginismeasuredas âˆ-normdistancebetween (w, b)and\n69412th International Society for Music Information Retrieval Conference (ISMIR 2011)\n(xi, yi)[13]. The parameter Ïmeans the minimum margin.\nNote that if the margin is positive w.r.t. all the examples,\nthe examples are linearly-separable. For the case when the\ndata is inseparable, we allow each example to violate the\nminimum margin Ïby the amount of Î¾i. So, the problem\nis to maximize the minimum margin Ïwhile minimizing\nthe sum of losses deï¬ned as Î¾i. The parameter Î½âˆˆ[1, m]\ncontrols the tradeoff between maximization of the margin\nand minimization of losses.\nBy using Lagrangian duality (e.g. [2]), the dual problem\nis givenas follows:\nmin\nÎ³,dÎ³(3)\nsub.to\nEdged(hj) =âˆ‘\nidiyihj(xi)â‰¤Î³(j= 1, . . . , n ),\ndâ‰¤1\nÎ½1,\ndâ‰¥0,mâˆ‘\ni=1di= 1\ndÂ·y= 0.\nThe dual problem is about ï¬nding a distribution dover\nexamples satisfying linear constraints. In particular, since\nyihj(xi) = 1if and only if hj(xi) = yi,Edged(hj)can\nbe viewed as a weighted accuracy of the hypothesis of hj\nw.r.t. the distribution d. So, in other words, a solution dâˆ—\nof the dual problem is the most â€œdifï¬cultâ€ distribution w.r.t.\nhypothesesin H. Notethat,sincethebothproblems(2)and\n(3)arelinearprograms,theseproblemsareequivalent. That\nis, if we solve one problem, we can obtain a solution of the\nother problem as well.\nWe solve the dual problem (3) using LPBoost [5], which\nis shown in Algorithm 1. LPBoost chooses a hypothesis\nhâˆˆHand solve a sub-problem of the dual problem (3) it-\neratively until some termination condition is satisï¬ed. It is\nknown that after sufï¬cient number of iterations, output by\nLPBoost converges to a solution of the problem (3). More\nprecisely,thefollowingstatementholdsforanygivenpreci-\nsion parameter Î» > 0.\nTheorem2 (Demiriz et al. [5]) LPBoostoutputsaï¬nalhy-\npothesis such that the corresponding solution (Î³T,dT)sat-\nisï¬es Î³Tâ‰¤Î³âˆ—+Î»,where (Î³âˆ—,dâˆ—)isanoptimalsolutionof\nthe dual problem(3).\n4. COMPUTATIONALEXPERIMENT\nIn this section, we show preliminary experimental results.\nThe task we consider is classiï¬cation problems over a data\nset of Japanese songs.Algorithm 1 LPBoost( S,Î»)\n(1) Let d1be the uniform distributionover S.\n(2) For t= 1, . . . ,\n(a) Chooseahypothesis h(t)âˆˆHwhoseedgew.r.t.\ndtismore than Î³t+Î».\n(b) Ifsuchahypothesisdoesnotexistin H,letT=\ntâˆ’1and break.\n(c) Solve the soft margin optimization prob-\nlem (3) w.r.t. the restricted hypothesis set\n{h(1), . . . , h(t)}. Let (Î³t+1,dt+1)beasolution.\n(Î³t+1,dt+1) = arg min\nÎ³,dÎ³\nsub. to\nâˆ‘\nidiyjh(j)(xi)â‰¤Î³(j= 1, . . . , t )\ndâ‰¤1\nÎ½1.\n(3) Output f(x) =âˆ‘T\nt=1wth(t)(x), where each wt(t=\n1, . . . , T) is a Lagrange dual of the soft margin opti-\nmizationproblem (3).\n4.1 Data set\nOur data set of Japanese songs consists of 119pop songs\n(JPOP)and 119Enkasongs,whereEnkaisagenreofJapanese\nsongswhosestyleisratherclosetotraditionalfolkloresongs.\nWe convert MIDI format into string data according to the\nmethod speciï¬ed in Kadota et al. [6].\nFor the original data in the MIDI format, we specify a\nparticular channel which corresponds to principal melody,\nand extract a single sequence consisting of notes and rests,\nwhereanoteisapairofpitchanddurationvaluesandarest\nhas only a duration value. We choose the highest pitch if\nmore than one pitch is â€œNOTE ONâ€ at an instant. In addi-\ntion we quantize the obtained data so that all the duration\nvaluesaremultiplesoftheMIDIdeltatimecorrespindingto\nthe sixteenth note. Then we convert the quantized note/rest\nsequences into string data of three types (see Figure 1):\nPitch string Wedivideeachnote(rest)intosixteenthnotes\n(rests) to produce a string consisting of pitches and\nrests. For simplicity, we ignore an octave difference,\nandthereforethenumberofpossiblepitchesistwelve.\nThealphabet size is thus 13.\nRhythmstring Similarly, we divide each note (rest) into\nsixteenthnotes(rests)andproduceastringconsisting\nof four symbols: N(beginning fragment of a note),\n695Poster Session 6\nClassiï¬er SVM our method\n(dis)similaritymeasure n-gramkernel mismatchkernel n-gramkernel mismatchkernel edit distance LCS\nPitch Nontransposed 61.34 65.55 70.16 73.52 86.12 79.42\nTransposed 61.34 65.55 70.16 73.52 86.12 79.42\nRhythm 86.97 86.97 88.67 89.90 87.79 92.87\nNote Nontransposed 66.39 71.01 76.46 79.81 87.38 85.33\nTransposed 66.39 71.01 76.46 79.81 87.38 85.33\nTable1. Classiï¬cation accuracy(%)\n/gid12/gid1/gid12/gid1/gid12/gid1/gid1/gid1/gid1/gid1/gid13/gid1/gid11/gid1/gid11/gid1/gid11/gid1\n/gid5/gid1/gid9/gid1/gid6/gid1/gid10/gid1/gid5/gid1/gid5/gid1/gid9/gid1/gid9/gid1/gid9/gid1/gid2/gid7/gid10/gid4/gid6 /gid1\n/gid3/gid6/gid11/gid10/gid6/gid8/gid1\n/gid2/gid1/gid7/gid1/gid6/gid1/gid10/gid1/gid3/gid1/gid4/gid1/gid8/gid1/gid8/gid1/gid8/gid1 /gid1/gid9/gid10/gid5 /gid1/gid2/gid1/gid2/gid1/gid6/gid1/gid6/gid1/gid3/gid1/gid4/gid1/gid4/gid1/gid4/gid1/gid4/gid1\nFigure 1 . How to extract string data from note/rest se-\nquence.\nn(non-beginning fragment of a note), R(beginning\nfragment of a rest) and r(non-beginning fragment of\narest).\nNote string Compositionofpitchandrhythmstrings. That\nis,frompitchstring a1. . . a mandrhythmstring b1. . .\nbmfor a same note/rest sequence, we composed the\nstring (a1, b1). . .(am, bm). The alphabet size is 26.\nFor pitch strings and note strings, we have an option to\ntranspose them into C major (C minor).\n4.2 Classiï¬cation algorithms\nThe algorithms we examined are SVMs with string kernels\nand LPBoost with the (dis)similarity-based learning frame-\nwork (our method). For SVMs, we used n-gram kernels [9]\nwith n= 1, . . . , 10and mismatch kernels [8] with parame-\ntersn= 2, . . . , 20andk= 1, . . . , n âˆ’1. Forothersettings,\nWeused defaultparameters of LIBSVM for SVMs.\nFor our method, we used two (dis)similarity measures:\nthelengthofLongestCommonSubsequence(LCS)andthe\neditdistance,inadditiontothestringkernelsusedforSVMs.\nFor the parameter Î½, we set Î½=cm, where mis the given\nsample size and c= 0.05,0.1,0.15,0.2,0.25.0.3. As de-\nscribed in Section 3, we used base classiï¬ers h(x) = sgn[\nd(xi, x)âˆ’d(xj, x)]associatedwithpairsofpositiveinstancexiand negative instance xj. In total, we used 119âˆ—119 =\n14161baseclassiï¬ers.\nWe evaluated SVMs and our method by performing 5-\nfoldcrossvalidation. TheresultsaresummarizedinTable1,\nwhere the accuracies of respective methods are shown with\nbest parameters.\n4.3 Result\nAs is shown in Table 1, our method shows better perfor-\nmancethanSVMswithallkernels. Forpitchstringandnote\nstring, the best value was obtained by our method with the\nedit distance. For rhythm string, the best value was gained\nby our method with LCS. For pitch string and note string,\ntransposition in the note did not affect the classiï¬cation ac-\ncuracyin our experiments.\nOur methods with the edit distance and with LCS have\nbetter results than those with the n-gram and the mismatch\nkernels. This might be because the edit distance and LCS\ncapture characteristics of JPOP and Enka better. Over all of\nthe (dis)similarity measures and kernels we used, the best\nclassiï¬cation results were obtained on rhythm string. This\nmight be because JPOP has rather high tempo while Enka\nhas slowtempo.\nFinally,we investigatewhich base classiï¬ers\nh(x) = sgn[ d(xi, x)âˆ’d(xj, x)],\nassociatedwithpairsofJPOP xiandEnka xj,contributean\naccurate classiï¬cation.\nInthecaseofourmethodwiththeeditdistanceonrhythm\nstrings, among all possible 14,161 pairs, at most 66 pairs\nhave a non-zero weight in the ï¬nal weighted combination\nfor all the parameters c. So, the obtained weighted combi-\nnation is quite sparse.\nWeobservethattheresultingï¬nalweightedcombination\nis sparser when we employ(dis)similarity measures.\nFor rhythm string, we choose c= 0.3andc= 0.15\nwhich give the best classiï¬cation results for LCS and the\nedit distance, respectively. We arrange all the pairs in de-\ncreasing order of their weights, and the top 10 pairs are dis-\nplayed in Tables2 and 3.\n69612th International Society for Music Information Retrieval Conference (ISMIR 2011)\ncJPOPTitle ENKATitle Weight Totalof weight\n0.3SecretHeaven Norennohana 0.430940 0.43094\nSecretHeaven AihaKirameite 0.146408 0.577349\nInMy Room Akashiabanka 0.140884 0.718233\nKimigaSuki Amagigoe 0.135359 0.853591\nRaven NyoninKouya 0.116022 0.969613\nRaven Okuhidabojou 0.024862 0.994475\nKimigaSuki Unga 0.005525 1\nTable2. Top10 pairs with largeweight in the ï¬nal weighted combination for the edit distance.\ncJPOP Title ENKATitle Weight Totalof weight\n0.15Tsukiyo no koibitotachi Ohsakawan 0.208197 0.208197\nAmenimo Makezu Kaettekoiyo 0.127717 0.335914\nTotsuzen Otokogi 0.059798 0.395712\nOnly You Matsuri 0.054587 0.450299\nTotsuzen ShiroiYuki 0.050715 0.501013\nFINALDISTANCE Yukimoete 0.050270 0.551284\nTsukiyo no koibitotachi Hashi 0.046641 0.597925\nGoodbye Yesterday YoshidaShoin 0.044228 0.642153\nSecret Heaven KokohaMinatomachi 0.039791 0.681944\nFINALDISTANCE EttouTsubame 0.037098 0.719042\nTable3. Top10 pairs with largeweight in the ï¬nal weighted combination for LCS.\nIn the case of the edit distance, only the top 3 pairs oc-\ncupy more than 70% of total weight, and the top 5 pairs\noccupy more than 90% of total weight. We omitted the last\nthree pairs in the top 10 list of Table 2 since their weights\nare less than 10âˆ’17. So, only at most 5pairs of JPOP and\nEnka contribute the ï¬nal classiï¬cation signiï¬cantly. Simi-\nlarly, in the case of LCS, the top 10 pairs have about 70%\noftotalweight. Thesesongsinthetoplistsmightbeâ€œrepre-\nsentativesâ€ of JPOP or Enka, from which we might be able\nto extractgood feature representations.\n5. CONCLUSION\nInthispaperweaddressedthemusicclassiï¬cationproblem.\nWe employed the (dis)similarity-based learning framework\nproposed by Wang et al. [20]. Computational experiments\nshow that our method combined with string kernels such as\nthen-gramandthemismatchkernelsoutperformSVMwith\nthem. One advantage of our approach is that it can be used\ncombined with any(dis)similarity measure, which do not\nhave to be positive semideï¬nite. In fact, our method with\nLCS and the edit distance show better classiï¬cation accu-\nracy than with the string kernels. Among the three types\nof string data we examined, the rhythm string seems most\nsuited for genre classiï¬cation in our experiments. Songs in\nthe pairs with large weight in the resulting weighted com-\nbination might be representatives of respective music gen-res. We challenge classiï¬cation problem with data set of\n238 songs, however, the data set is too low to be general-\nityofthisapproach. Weneedtoexperimentbiggeramounts\nof data, and we measure classiï¬cation accuracy of not only\nsymbolic data but also audio data. Future work is not only\nmusic genre classiï¬cation but also automatic extraction of\nfeatures of music genres or composers.\n6. REFERENCES\n[1] James Bergstra, Norman Casagrande, Dumitru Erhan,\nDouglas Eck, and Bal Â´azs KÂ´egl. Aggregate features and\nAdaBoost for music classiï¬cation. Machine Learning ,\n65:473â€“484,2006.\n[2] Stephen Boyd and Lieven Vandenberghe, editors. Con-\nvexOptimization . Cambridge UniversityPress, 2004.\n[3] RudiCilibrasi,PaulVit Â´anyi,andRonalddeWolf.Algo-\nrithmicclusteringofmusicbasedonstringcompression.\nComputerMusic Journal , 28(4):49â€“67, 2004.\n[4] Christopher DeCoro, Zafer Barutcuoglu, and Rebecca\nFiebrink. Bayesian aggregation for hierarchical genre\nclassiï¬cation. In Proceedings of the 8th International\nConferenceonMusicInformationRetrieval(ISMIRâ€™07) ,\n2007.\n697Poster Session 6\n[5] A. Demiriz, K. P. Bennett, and J. Shawe-Taylor. Linear\nprogramming boosting via column generation. Machine\nLearning,46(1-3):225â€“254, 2002.\n[6] Takashi Kadota, Masahiro Hirao, Akira Ishino,\nMasayukiTakeda,AyumiShinohara,andFumihiroMat-\nsuo. Musical sequence comparison for melodic and\nrhythmic similarities. In Proc. 8th International Sym-\nposium on String Processing and Information Retrieval\n(SPIREâ€™01) , pages 111â€“122, 2001.\n[7] Chiristina Leslie and Rui Kang. Fast string kernels us-\ning inexact matching for protein sequences. Journal of\nMachineLearning Research , 5:1435â€“1455, 2004.\n[8] Christina S. Leslie, Eleazar Eskin, Adiel Cohen, Jason\nWeston, and William Stafford Noble. Mismatch string\nkernels for discriminative protein classiï¬cation. Bioin-\nformatics , 20(4):467â€“476, 2004.\n[9] ChristinaS.Leslie,EleazarEskin,andWilliamStafford\nNoble. The spectrum kernel: A string kernel for SVM\nproteinclassiï¬cation.In Proc.thePaciï¬cSymposiumon\nBiocomputing ,pages 566â€“575, 2002.\n[10] Thomas Lidy and Andreas Rauber. Evaluation of fea-\nture extractors and psycho-acoustic transformations for\nmusic genre classiï¬cation. In Proceedings of the 6th In-\nternational Conference on Music Information Retrieval\n(ISMIRâ€™05) , pages 34â€“41, 2005.\n[11] Thomas Lidy, Andreas Rauber, Antonio Pertusa, and\nJosÂ´e Manuel I Ëœnesta. Improving genre classiï¬cation by\ncombination of audio and symbolic descriptors using a\ntranscriptionsystem.In Proceedingsof8thInternational\nConferenceonMusicInformationRetrieval(ISMIRâ€™07) ,\n2007.\n[12] HumaLodhi,CraigSaunders,JohnShawe-Taylor,Nello\nCristianini, Chris Watkins, and Bernhard Scholkopf.\nText classiï¬cation using string kernels. Journal of Ma-\nchineLearning Research , 2:563â€“569, 2002.\n[13] O. L. Mangasarian. Arbitrary-norm separating plane.\nOperationsResearchLetters ,24:15â€“23, 1999.\n[14] Carlos P Â´erez-Sancho, David Rizo, and Jos Â´e Manuel\nIËœnesta. Genre classiï¬cation using chords and stochas-\ntic language models. Connection Science , 21:145â€“159,\n2009.\n[15] P.Zanon and G.Widmer. Learning to recognize famous\npianists with machine learning techniques. In Proceed-\nings of the Stockholm Music Acoustics Conference ,\n2003.[16] Craig Saunders, David R. Hardoon, John Shawe-taylor,\nand Gerhard Widmer. Using string kernels to identify\nfamous performers from their playing style. In Pro-\nceedings of the 15th European Conference on Machine\nLearning,pages 384â€“395, 2004.\n[17] Robert E. Schapire, Yoav Freund, Peter Bartlett, and\nWen S. Lee. Boosting the margin: a new explanation\nfor the effectiveness of voting methods. The Annals of\nStatistics, 26(5):1651â€“1686, 1998.\n[18] George Tzanetakis, Georg Essl, and Perry Cook. Au-\ntomatic musical genre classiï¬cation of audio signals. In\nProceedingsofthe2ndInternationalSymposiumonMu-\nsicInformation Retrieval(ISMIRâ€™ 01) ,2001.\n[19] S. V. N. Vishwanathan and A. J. Smola. Fast kernels\nonstirngsandtrees.In AdvancesonNeuralInformation\nProccessingSystems 14 ,2002.\n[20] Liwei Wang, Masashi Sugiyama, Cheng Yang, Kohei\nHatano, and Jufu Feng. Theory and algorithm for learn-\ning with dissimilarity functions. Neural Computation ,\n21:1459â€“1484,2009.\n[21] M. Warmuth, K. Glocer, and G. R Â¨atsch. Boosting algo-\nrithms for maximizing the soft margin. In Advances in\nNeuralInformationProcessingSystems20 ,pages1585â€“\n1592,2008.\n[22] Manfred K. Warmuth, Karen A. Glocer, and S. V. N.\nVishwanathan.Entropyregularizedlpboost.In Proceed-\ningsofthe19thinternationalconferenceonAlgorithmic\nLearningTheory (ALTâ€™08) ,pages 256â€“271, 2008.\n698"
    },
    {
        "title": "Multiscale Scattering for Audio Classification.",
        "author": [
            "Joakim AndÃ©n",
            "StÃ©phane Mallat"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1415750",
        "url": "https://doi.org/10.5281/zenodo.1415750",
        "ee": "https://zenodo.org/records/1415750/files/AndenM11.pdf",
        "abstract": "Mel-frequency cepstral coefficients (MFCCs) are efficient audio descriptors providing spectral energy measurements over short time windows of length 23 ms. These measurements, however, lose non-stationary spectral information such as transients or time-varying structures. It is shown that this information can be recovered as spectral co-occurrence coefficients. Scattering operators compute these coefficients with a cascade of wavelet filter banks and modulus rectifiers. The signal can be reconstructed from scattering coefficients by inverting these wavelet modulus operators. An application to genre classification shows that second-order cooccurrence coefficients improve results obtained by MFCC and Delta-MFCC descriptors. 1",
        "zenodo_id": 1415750,
        "dblp_key": "conf/ismir/AndenM11",
        "keywords": [
            "Mel-frequency cepstral coefficients (MFCCs)",
            "spectral energy measurements",
            "non-stationary spectral information",
            "transients",
            "time-varying structures",
            "spectral co-occurrence coefficients",
            "wavelet filter banks",
            "modulus rectifiers",
            "signal reconstruction",
            "genre classification"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nMULTISCALE SCATTERING FOR AUDIO CLASSIFICATION\nJoakim And Â´en\nCMAP, Ecole Polytechnique, 91128 Palaiseau\nanden@cmap.polytechnique.frStÂ´ephane Mallat\nCMAP, Ecole Polytechnique, 91128 Palaiseau\nABSTRACT\nMel-frequency cepstral coefï¬cients (MFCCs) are efï¬cient\naudio descriptors providing spectral energy measurements\nover short time windows of length 23 ms . These measure-\nments, however, lose non-stationary spectral information such\nas transients or time-varying structures. It is shown that this\ninformation can be recovered as spectral co-occurrence co-\nefï¬cients. Scattering operators compute these coefï¬cients\nwith a cascade of wavelet ï¬lter banks and modulus recti-\nï¬ers. The signal can be reconstructed from scattering coefï¬-\ncients by inverting these wavelet modulus operators. An ap-\nplication to genre classiï¬cation shows that second-order co-\noccurrence coefï¬cients improve results obtained by MFCC\nand Delta-MFCC descriptors.1\n1. INTRODUCTION\nMany speech and music classiï¬ers use mel-frequency cep-\nstral coefï¬cients (MFCCs), which are cosine transforms of\nmel-frequency spectral coefï¬cients (MFSCs). Over a ï¬xed\ntime interval, MFSCs measure the signal frequency energy\nover mel-frequency intervals of constant- Qbandwidth. As\na result, they lose information on signal structures that are\nnon-stationary on this time interval. To minimize this loss,\nshort time windows of 23 ms are used in most applications\nsince at this resolution most signals are locally stationary.\nThe characterization of audio properties on larger time scales\nis then done by aggregating MFSC coefï¬cients in time, with\nmultiple ad-hoc methods such as Delta-MFCC [5] or MFCC\nsegments [1]. This paper shows that the non-stationary be-\nhavior lost by MFSC coefï¬cients is captured by a scatter-\ning transform which computes multiscale co-occurrence co-\nefï¬cients. A scattering representation includes MFSC-like\nmeasurements together with higher-order co-occurence co-\nefï¬cients that can characterize audio information over much\n1This work is funded by the ANR grant 0126 01.\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proï¬t or commercial advantage and that copies\nbear this notice and the full citation on the ï¬rst page.\nc/circlecopyrt2011 International Society for Music Information Retrieval.longer time intervals, up to several seconds. This yields ef-\nï¬cient representations for audio classiï¬cation.\nSection 2 relates MFSCs and wavelet ï¬lter bank coefï¬-\ncients. It is shown that information lost by spectral energy\nmeasurements can be recovered by a scattering operator in-\ntroduced in [8]. It computes co-occurrence coefï¬cients by\ncascading wavelet ï¬lter banks and rectiï¬ers calculated with\nmodulus operators. A scattering transform has strong sim-\nilarities with auditory physiological models based on cas-\ncades of constant- Qï¬lter banks and rectiï¬ers [4, 10]. It is\nshown that second-order co-occurrence coefï¬cients carry an\nimportant part of the signal information. Section 3 gives\nan application to musical genre classiï¬cation, which shows\nthat scattering co-occurence coefï¬cients reduce classiï¬ca-\ntion errors obtained with MFCCs and Delta-MFCCs. A\nMATLAB software is available at http://www.cmap.\npolytechnique.fr/scattering/ .\n2. SCATTERING REPRESENTATION\n2.1 From Mel-Frequency Spectra to Wavelets\nTo understand the information lost by mel-frequency spec-\ntral coefï¬cients, we relate them to a wavelet transform. The\nFourier transform of x(t)is written Ë†x(Ï‰) =/integraltext\nx(u)eâˆ’iÏ‰udu.\nA short-time Fourier transform of xis computed as the Fourier\ntransform of xt,T(u) =x(u)wT(uâˆ’t), wherewTis a time\nwindow of size T:\nË†xt,T(Ï‰) =/integraldisplay\nxt,T(u)eâˆ’iÏ‰udu.\nMFSCs are obtained by averaging the spectrogram |Ë†xt,T(Ï‰)|2\nover mel-frequency intervals. These intervals have a con-\nstant frequency bandwidth below 1000 Hz and a constant\noctave bandwidth above 1000 Hz. The MFSCs can thus be\nwritten\nMTx(t,j) =1\n2Ï€/integraldisplay\n|Ë†xt,T(Ï‰)|2|Ë†Ïˆj(Ï‰)|2dÏ‰ (1)\nwhere each Ë†Ïˆj(Ï‰)covers a mel-frequency interval indexed\nbyj. Applying Parsevalâ€™s theorem yields\nMTx(t,j) =/integraldisplay\n|xt,Tâ‹†Ïˆj(u)|2du. (2)\n657Poster Session 6\nIt results that MTx(t,j)is the energy of xin a neighborhood\noftof sizeTand in the mel-frequency interval indexed by\nj. It is unable to capture non-stationary structures of dura-\ntion shorter than T, which is why Tis chosen to be small,\ntypically 23 ms .\nAt high frequencies, the ï¬lters Ïˆjare constructed by di-\nlating a single ï¬lter Ïˆwhose octave bandwidth is 1/Q:\nÏˆj(t) =aâˆ’jÏˆ(aâˆ’jt)witha= 21/Qandjâ‰¤J. (3)\nThese ï¬lters can thus be interpreted as dilated wavelets. The\nï¬lterÏˆis normalized so that its support is about 1 s. It is a\ncomplex ï¬lter whose transfer function approximately covers\nthe frequency interval [2QÏ€âˆ’Ï€,2QÏ€+Ï€]. Forj < J ,\nthe time support of Ïˆjis thus smaller than aJand it covers\nthe frequency interval [2QÏ€aâˆ’jâˆ’Ï€aâˆ’j,2QÏ€aâˆ’j+Ï€aâˆ’j].\nFrequencies below 2Ï€Qaâˆ’Jare covered by Pï¬ltersÏˆj(for\nJâ‰¤j < J +P), having the same frequency bandwidth\nasÏˆJ, which is 2Ï€aâˆ’J, and a time support equal to aJ.\nAlthough these low-frequency ï¬lters are not dilations of Ïˆ,\nfor the sake of simplicity we shall still call them wavelets.\nThe resulting wavelet transform is a ï¬lter bank deï¬ned by:\nWJx(t) =/parenleftbiggxâ‹†Ï†J(t)\nxâ‹†Ïˆj(t)/parenrightbigg\nj<J +P.\nThe ï¬rst ï¬lter Ï†Jis a low-pass ï¬lter covering the interval\n[âˆ’Ï€aâˆ’J,Ï€aâˆ’J], which is not covered by other wavelet ï¬l-\nters and whose temporal support is about aJ.\nWavelet ï¬lters are designed so that for all frequencies Ï‰\n1âˆ’/epsilon1â‰¤|Ë†Ï†J(Ï‰)|2+1\n2/summationdisplay\nj<J +P|Ë†Ïˆj(Ï‰)|2+|Ë†Ïˆj(âˆ’Ï‰)|2â‰¤1(4)\nfor a small/epsilon1. The squared norm of a signal is written /bardblx/bardbl2=/integraltext\n|x(t)|2dtand the norm of its wavelet transform is deï¬ned\nby:\n/bardblWJx/bardbl2=/bardblxâ‹†Ï†J/bardbl2+/summationdisplay\nj<J +P/bardblxâ‹†Ïˆj/bardbl2.\nThus by applying Parsevalâ€™s theorem one can verify that the\nï¬lter admissibility condition (4) implies that\n(1âˆ’/epsilon1)/bardblx/bardbl2â‰¤/bardblWJx/bardbl2â‰¤/bardblx/bardbl2.\nThe wavelet ï¬lter bank is thus contractive and if /epsilon1= 0, it\nis also unitary. This energy equivalence also implies that x\ncan be recovered from its wavelet transform.\nIn numerical applications we use Gabor ï¬lters Ïˆ(t) =\nÎ¸(t)ei2Ï€QtwhereÎ¸is Gaussian, with Q= 16 andP= 23 ,\nwhich satisfy (4) for /epsilon1= 0.02. The resulting ï¬lter bank is\nshown in Figure 1.\n0 1000 2000 3000 4000 5000 600001!Figure 1 . Wavelet ï¬lter bank of Gabor ï¬lters at sampling\nfrequency 11025 Hz .\n2.2 Scattering Wavelets\nAn MFSC coefï¬cient MTx(t,j)in (2) gives the squared en-\nergy of wavelet coefï¬cients at the scale aj, over a time inter-\nval of sizeTaroundt. Let us choose the maximum wavelet\nscale to beaJ=T. The square does not play an important\nrole on the derived MFCC audio descriptors which are cal-\nculated with a logarithm. Replacing the squared amplitude\nby the amplitude yields similar measurements which can be\ncomputed directly by averaging the wavelet coefï¬cient am-\nplitudes ofx:\n|xâ‹†Ïˆj|â‹†Ï†J(t). (5)\nThis measures the signal amplitude in the frequency interval\ncovered byÏˆj, averaged over a neighborhood of tof dura-\ntionT=aJ. The larger T, the more information is lost by\nthis averaging.\nTo recover the information lost by averaging, observe\nthat|xâ‹†Ïˆj1|â‹†Ï†Jcan be written as the low-frequency com-\nponent of the wavelet transform of |xâ‹†Ïˆj1|:\nWJ|xâ‹†Ïˆj1|(t) =/parenleftbigg|xâ‹†Ïˆj1|â‹†Ï†J(t)\n|xâ‹†Ïˆj1|â‹†Ïˆj2(t)/parenrightbigg\nj2<J+P.\nSince the wavelet transform is invertible, the information\nlost by the convolution with Ï†Jis recovered by the wavelet\ncoefï¬cients|xâ‹†Ïˆj1|â‹†Ïˆj2(t). Averaged measurements are\nobtained with a low-pass ï¬ltering of the modulus of these\ncomplex wavelet coefï¬cients:\n||xâ‹†Ïˆj1|â‹†Ïˆj2|â‹†Ï†J(t). (6)\nThese provide co-occurrence information at the scales aj1\nandaj2. Such coefï¬cients are called scattering coefï¬cients\nbecause they compute the interferences of the signal xwith\ntwo successive wavelets Ïˆj1andÏˆj2. They measure the am-\nplitude of time variations of |xâ‹†Ïˆj1(t)|in the frequency\nintervals covered by the wavelets Ïˆj2. Figure 2 shows ï¬rst-\norder scattering coefï¬cients of a musical recording sampled\nat11025 Hz , calculated with T= 800 ms . Co-occurrence\ncoefï¬cients||xâ‹†Ïˆj1|â‹†Ïˆj2|â‹†Ï†J(t)are shown in Figure 2,\nfor a ï¬xed scale aj1.\nAveraging||x â‹† Ïˆj1|â‹† Ïˆj2|byÏ†Jin (6) again entails\na loss of high frequencies, which can be recovered by a\nnew wavelet transform. The same procedure is thus iterated,\n65812th International Society for Music Information Retrieval Conference (ISMIR 2011)\nFigure 2 . Top: log[|xâ‹†Ïˆj1|â‹†Ï†J(t)]as a function of time\ntand ofÏ‰1= 2Ï€Qaâˆ’j1forT=aJ= 800 ms . Middle:\ngraph of|xâ‹†Ïˆj1|forÏ‰1= 855 Hz . Bottom: log[||xâ‹†Ïˆj1|â‹†\nÏˆj2|â‹†Ï†J(t)]as a function of tand ofÏ‰2= 2Ï€Qaâˆ’j2for\n|xâ‹†Ïˆj1|shown above.\ndeï¬ning a cascade of ï¬lter banks and modulus operators il-\nlustrated in Figure 3.\nLetUJbe the wavelet modulus operator which computes\nthe modulus of complex wavelet coefï¬cients while keeping\nthe phase of xâ‹†Ï†J:\nUJx(t) =/parenleftbiggxâ‹†Ï†J(t)\n|xâ‹†Ïˆj(t)|/parenrightbigg\nj<J +P. (7)\nA scattering transform ï¬rst computes UJxand outputs the\nlow-frequency signal xâ‹†Ï†J. At the next layer, each |xâ‹†Ïˆj1|\nis retransformed by UJ, which outputs|xâ‹†Ïˆj1|â‹†Ï†Jand\ncomputes||xâ‹†Ïˆj1|â‹†Ïˆj2|. These coefï¬cients are themselves\nagain transformed by UJ, which outputs||xâ‹†Ïˆj1|â‹†Ïˆj2|â‹†Ï†J\nand computes third-order wavelet signals, which are further\nsubdecomposed by UJ, and so on.\nApplying this transformation mtimes and discarding the\ncoefï¬cients not ï¬ltered by Ï†Jyields a scattering vector of\nsizem+ 1at timet:\nSJx(t) =ï£«\nï£¬ï£¬ï£¬ï£¬ï£¬ï£­xâ‹†Ï†J(t)\n|xâ‹†Ïˆj1|â‹†Ï†J(t)\n||xâ‹†Ïˆj1|â‹†Ïˆj2|â‹†Ï†J(t)\n...\n||Â·Â·Â·|xâ‹†Ïˆj1|Â·Â·Â·|â‹†Ïˆjm|â‹†Ï†J(t)ï£¶\nï£·ï£·ï£·ï£·ï£·ï£¸\nj1,j2,...<J +P\nThis scattering transform is a cascade of modulated ï¬l-\nter banks and non-linear rectiï¬cations, as in the auditory\nphysiological models studied in [4, 10]. It has an architec-\nture similar to convolutional networks used in computer vi-\nsion [6] and to convolutional deep belief networks used in\nxxâ‹†Ï†J|xâ‹†Ïˆj1|â‹†Ï†Jâˆ€j1||xâ‹†Ïˆj1|â‹†Ïˆj2|â‹†Ï†Jâˆ€j1,j2|xâ‹†Ïˆj1|UJUJ||xâ‹†Ïˆj1|â‹†Ïˆj2|UJ||xâ‹†Ïˆj1| Â· Â· Â·â‹†Ïˆjm|||xâ‹†Ïˆj1| Â· Â· Â·â‹†Ïˆjm|â‹†Ï†Jâˆ€j1...jmUJÂ· Â· Â·Â· Â· Â·Figure 3 . A scattering operator is a cascade of wavelet mod-\nulus operators UJ. It outputs convolutions with Ï†Jshown\nin boxes.\naudio classiï¬cation [7]. However, a scattering gathers out-\nputs from all layers as opposed the last one. Indeed, the\nenergy of coefï¬cients of order qdecays to zero when qin-\ncreases.\nThe squared norm of this scattering signal is the sum of\nthe squared norms of its components:\n/bardblSJx/bardbl2=/summationdisplay\nq/summationdisplay\nj1,...,j q<J+P/bardbl||xâ‹†Ïˆj1|Â·Â·Â·â‹†Ïˆjq|â‹†Ï†J/bardbl2.\nSinceWJand the modulus are both contractive operators,\nthe wavelet modulus operator UJis also contractive. Be-\ncauseSJis calculated with a cascade of UJ, it remains con-\ntractive, and thus for any signals xandy\n/bardblSJxâˆ’SJy/bardblâ‰¤/bardblxâˆ’y/bardbl.\nThe wavelet transform is unitary if the wavelet ï¬lters satisfy\nthe admissibility condition (4) with /epsilon1= 0. For wavelets\nsatisfying this and additional criteria, it is proved in [8] that\nthe energy of all scattering coefï¬cients of order qdecays to\nzero asqincreases. It results that the whole signal energy\nis carried by a scattering vector consisting of co-occurrence\ncoefï¬cients of all orders from q= 0toq=âˆ:\n/bardblSJx/bardbl=/bardblx/bardbl.\nTable 1 gives the average value of /bardblSJx/bardbl//bardblx/bardblover all\naudio signals xin the GTZAN dataset, sampled at 11025 Hz ,\nas a function of mandT. Form= 0,SJx(t) =f â‹†Ï†J(t).\nObserve that for Tâ‰¤6 s, ï¬rst- and second-order coefï¬cients\ncarry more than 98% of the energy.\n2.3 Second-Order Scattering Decomposition and\nReconstruction\nIn the following, the scattering transform is computed for\nm= 2 because ï¬rst- and second-order scattering coefï¬-\ncients carry most of the signal energy in the interesting range\n659Poster Session 6\nTm= 0m= 1m= 2m= 3\n23 ms 23.7% 98.9% 99.6% 99.6%\n93 ms 1.9% 97.7% 99.4% 99.4%\n370 ms 1.2% 92.7% 99.3% 99.4%\n1.5 s 1.0% 82.0% 98.9% 99.3%\n5.9 s 0.99% 73.0% 98.1% 99.1%\n22 s 0.97% 67.5% 96.5% 99.0%\nTable 1 . Averaged ratio/bardblSJx/bardbl//bardblx/bardblon the GTZAN dataset,\nas a function of the maximum scattering order mand of\nT=aJ.\nof window sizes T. The signals|f â‹†Ïˆj1|â‹†Ï†J(t)and||xâ‹†\nÏˆj1|â‹†Ïˆj2|â‹†Ï†J(t)are uniformly sampled at intervals T=aJ\nbecause the frequency bandwidth of Ë†Ï†Jis2Ï€aâˆ’J. A sam-\npled second-order scattering vector is thus deï¬ned by:\nSJx(naJ) =/parenleftbigg|xâ‹†Ïˆj1|â‹†Ï†J(naJ)\n||xâ‹†Ïˆj1|â‹†Ïˆj2|â‹†Ï†J(naJ)/parenrightbigg\nj1,j2<J+P.\n(8)\nWe now show that if j2<j 1+ logaQ/2then||xâ‹†Ïˆj1|â‹†\nÏˆj2|â‹†Ï†J(t)â‰ˆ0, so second-order coefï¬cients need only be\ncalculated for j2â‰¥j1+ logaQ/2. Indeed, since Ïˆ(t) =\nÎ¸(t)ei2Ï€Qt, it results that\n|xâ‹†Ïˆj1(t)|=|xj1â‹†Î¸j1(t)|withxj1(t) =x(t)eâˆ’i2Ï€Qaâˆ’j1t.\nThe Fourier transform of |xâ‹†Ïˆj1(t)|is thus approximately\nlocated in the low-frequency interval covered by Ë†Î¸j1where\nÎ¸j(t) =aâˆ’jÎ¸(aâˆ’jt). One can verify that if j2< j 1+\nlogaQ/2then the supports of Ë†Ïˆj2andË†Î¸j1barely overlap,\nwhich implies that ||xâ‹†Ïˆj1|â‹†Ïˆj2|â‹†Ï†J(t)â‰ˆ0. Non-zero\nscattering coefï¬cients (8) are computed with the following\nalgorithm.\nAlgorithm 1 Second-order scattering calculations\nforj1<J+Pâˆ’1do\nCompute||f â‹†Ïˆj1(aj1n)|âˆ€n\nOutput||f â‹†Ïˆj1|â‹†Ï†J(aJn)âˆ€n\nforj2=j1+ loga(Q/2)toJ+Pâˆ’1do\nCompute and output ||f â‹†Ïˆj1|â‹†Ïˆj2|â‹†Ï†J(aJn)âˆ€n\nend for\nend for\nAn audio frame of duration T=aJcontainingNsam-\nples yields about Qlog2(N/Q )andQ2/2 log2\n2(N/Q2)ï¬rst-\norder and second-order scattering coefï¬cients, respectively.\nIfN= 8192 , there are 150ï¬rst-order coefï¬cients and 5500\nsecond-order coefï¬cients, approximately. Using FFTs, these\ncoefï¬cients are computed with O(Nlog(N/Q ))operations.\nSince the scattering transform is computed by iterating\nthe wavelet modulus operator UJ, its inversion is reduced toinvertingUJ. The wavelet transform WJis invertible with a\nstable inverse but UJloses the complex phase of wavelet\ncoefï¬cients. Inverting UJthen amounts to retrieving the\ncomplex phase from the modulus information. A surpris-\ning new result [12] proves that for appropriate wavelets, the\noperatorUJis invertible and that its inverse is continuous,\nwhich is a weak stability result. This inversion is made pos-\nsible because of the redundancy of wavelet signals xâ‹†Ïˆj(t),\nwhich can be exploited with a reproducing kernel projector.\nNumerical reconstructions are computed with an alternating\nprojection algorithm, which alternates between a projector\non the modulus constraint and the wavelet transform repro-\nducing kernel projector [12]. However, this algorithm does\nnot compute the exact inverse of UJbecause it is a non-\nconvex optimisation which can be trapped in local minima.\nEven though UJis invertible, xcannot be recovered ex-\nactly fromSJxcalculated at a ï¬nite order mbecause all\nscattering coefï¬cients of order larger than mare set to 0.\nForTâ‰¤100 ms most of the audio signal energy is con-\ncentrated in ï¬rst-order coefï¬cients according to Table 1 and\nthe reconstruction from these ï¬rst-order coefï¬cients (which\ncorrespond to MFSCs) is indeed of good audio quality. As T\nincreases, reconstructions from ï¬rst-order coefï¬cients pro-\ngressively lose more information on transient structures and\nlose all melodic structures for Tâ‰¥3 s. Second-order coef-\nï¬cients recover this transient information and fully restores\nmelodic structures when T= 3 s . Reconstruction examples\nare available at http://www.cmap.polytechnique.\nfr/scattering/audio/ .\n2.4 Cosine Log-Scattering\nMFCC coefï¬cients are computed as a cosine transform of\nthe logarithm of MFSC coefï¬cients. Indeed, many musi-\ncal and voiced sounds can be approximated by an excita-\ntione(t)ï¬ltered by resonator corresponding to a ï¬lter h(t):\nx(t) =eâ‹†h(t)[2]. MFCCs separate hfromewith a log-\narithm and a discrete cosine transform (DCT). The same\nproperty applies to scattering coefï¬cients, which are there-\nfore retransformed with a logarithm and a DCT.\nThe impulse response h(t)is typically very short so Ë†h(Ï‰)\nis a regular function of Ï‰. Supposing that Ë†h(Ï‰)is nearly\nconstant over the frequency support of Ë†Ïˆj1, one can verify\nthat\nxâ‹†Ïˆj1(t)â‰ˆË†h(2Ï€Qaâˆ’j1)Â·eâ‹†Ïˆj1(t). (9)\nIt results that\nlog|xâ‹†Ïˆj1|â‹†Ï†J(t)â‰ˆlog|Ë†h(2Ï€Qaâˆ’j1)| (10)\n+ log [|eâ‹†Ïˆj1(t)|â‹†Ï†J(t)].\nSince|Ë†h(Ï‰)|is a regular function of Ï‰,log|Ë†h(2Ï€Qaâˆ’j1)|is\nalso a regular function of j1whereas this is typically false\n66012th International Society for Music Information Retrieval Conference (ISMIR 2011)\n50 100 15010âˆ’1100101102q = 1\nCoefficientVariance\n2000 4000q = 2\nCoefficientFigure 4 . Variances, in decreasing order, of log-scattering\ncoefï¬cients in different bases for q= 1andq= 2computed\non GTZAN for T= 1.5 s. Solid curve: Variance of log-\nscattering coefï¬cients. Dashed curve: Variance of a PCA\nbasis computed on log-scattering coefï¬cients. Dotted curve:\nVariance of cosine log-scattering coefï¬cients.\nfor|eâ‹†Ïˆj1(t)|. Both components can thus be partially sep-\narated with a DCT along j1, which carries the information\ndepending on hover to low-frequency DCT coefï¬cients.\nSimilarly, (9) implies\n||xâ‹†Ïˆj1|â‹†Ïˆj2|â‹†Ï†J(t)â‰ˆ|Ë†h(2Ï€Qaâˆ’j1)|Â·|eâ‹†Ïˆj1|â‹†Ïˆj2|â‹†Ï†J(t),\nand hence\nlog [||xâ‹†Ïˆj1|â‹†Ïˆj2|â‹†Ï†J(t)]â‰ˆlog|Ë†h(2Ï€Qaâˆ’j1)|\n+ log [|eâ‹†Ïˆj1|â‹†Ïˆj2|â‹†Ï†J(t)].\nThese coefï¬cients are transformed with a DCT along j2and\nthen alongj1, yielding a representation parametrized by k2\nandk1respectively. The ï¬rst term, depending only on j1,\nonly contributes to the zero DCT coefï¬cient ( k2= 0) along\nj2. The second DCT along j1separates the remaining low-\nfrequency components along j1from high-frequency ones.\nFigure 4 indicates that the DCTs efï¬ciently decorrelate\nlog-scattering coefï¬cients and concentrate the energy over\nfewer coefï¬cients. Variances were calculated for q= 1and\nq= 2on part of the GTZAN dataset in three bases: standard\nlog-scattering (solid), a PCA basis computed on another part\nof the dataset (dashes), and the DCT basis (dotted). The\nPCA basis decorrelates the log-scattering coefï¬cients and\nsince the variances in the DCT basis closely follow those in\nPCA basis, the DCT basis decorrelates them as well.\nFor classiï¬cation, the ï¬nal representation using cosine\nlog-scattering (CLS) coefï¬cients is obtained by keeping only\nthe low-frequency DCT coefï¬cients as with MFCCs. For\nq= 1, the ï¬rsta1coefï¬cients are retained. When q= 2,\na square deï¬ned by k1< a 1andk2< a 2is selected. This\naddsa2bands of information on the non-stationary part cor-\nresponding to the coefï¬cients in q= 1. In addition, fork1< b 1, whereb1/lessmucha1(capturing the spectral outline),\nb2/greatermucha2bands are included instead of a2to better model\nthe time-varying aspects of the spectral shape (e.g. the ï¬l-\nterhmentioned). For the numerical results presented in this\npaper, we have a1= 100 ,b1= 10 ,a2= 2 andb2= 10\n(chosen so that classiï¬cation errors do not differ from the\nuncompacted representation for relevant scales). The size\nof the representation is then at most 100 coefï¬cients for\nm= 1 and380coefï¬cients for m= 2. Form= 1, this\nis larger than the standard MFCC vector of 20coefï¬cients\nwhenT= 23 ms since the compacitiï¬cation is optimized\nfor all scales and smaller scales need less coefï¬cients.\n3. CLASSIFICATION\nMusic and speech classiï¬cation algorithms are often based\non MFCCs computed over 23 ms time windows. To capture\nlonger-range structures, these MFCCs are either aggregated\nin segments [1] that cover longer time intervals or are com-\nplemented with other features such as Delta-MFCCs [5].\nSophisticated GMM, HMM, AdaBoost, sparse coding clas-\nsiï¬ers have been developed on such feature vectors to op-\ntimize audio classiï¬cation. The next section studies classi-\nï¬cations results obtained with simple classiï¬ers to concen-\ntrate on the properties of feature vectors as opposed to a\nspeciï¬c classiï¬er.\n3.1 Musical Genre Classiï¬cation\nThe performance of MFCC and log-scattering vectors are\ncompared for musical genre classiï¬cation, on the GTZAN\ngenre database [11]. This database includes 10genres, each\ncontaining 100clips of 30seconds each.\nDelta-MFCC coefï¬cients [5] are deï¬ned as the differ-\nence between MFCC coefï¬cients of two consecutive au-\ndio frames and thus cover a time interval of twice the size.\nThese complement the ordinary MFCCs, providing infor-\nmation on the temporal audio dynamics over longer time\nintervals. The classiï¬cation performances of feature vectors\nare evaluated with an SVM classiï¬er computed with a Gaus-\nsian kernelk(x1,x2) = exp(âˆ’Î³/bardblx1âˆ’x2/bardbl2)or an afï¬ne\nspace classiï¬er.\nEach audio track is decomposed in frames of duration\nTwhich are represented using MFCCs, Delta-MFCCs, or\ncosine log-scattering. A multi-class SVM is implemented\nover the audio frames with a 1vs1 approach which trains\nan SVM to discriminate each pair of classes. To classify\na whole track, each frame is classiï¬ed using the SVM and\nthe class with the largest number of frames in the track is\nselected. The Gaussian kernel parameter Î³and the SVM\nslack variable Care optimized with a cross-validation on a\nsubset of the training set.\nDue to the large number of training examples available\nfor small window sizes, training an SVM in these circum-\n661Poster Session 6\nT/classiï¬er 0.023 s /PCA 0.19 s/PCA 1.5 s/SVM\nMFCC 46 36 28\nDelta-MFCC 37 33 26\nCLS,m= 1 46 36 28\nCLS,m= 2 34 23 18\nTable 2 . Error rates (in percent) on GTZAN using ï¬ve-fold\ncross-validation for different window sizes (T) and features.\nstances is infeasible. Therefore, we also compare the per-\nformance of the features using an afï¬ne space classiï¬er [3]\nwhich uses a PCA to create an afï¬ne space approximation\nfor each class and then assigns a given track to the class\nwhose afï¬ne space model best approximates the feature vec-\ntor.\nThe results of ï¬ve-fold cross-validation on the GTZAN\ndataset are shown in Table 2. As expected, the error rates\nfor MFCC and ï¬rst-order CLS are close since they mea-\nsure similar quantities. Second-order CLS vectors achieve\nsigniï¬cantly higher accuracy since they recover lost non-\nstationary structure of the signal. Delta-MFCC perform bet-\nter than regular MFCCs, but are outperformed by CLS vec-\ntors which provide richer representations. With increasing\nT, the error decreases as larger-scale musical information is\nencoded, yielding the lowest error of 18% forT= 1.5 s\nwith an SVM. At larger time scales, however, classiï¬ca-\ntion suffers since even second-order CLS vectors are un-\nable to accurately represent the signal, as seen during recon-\nstruction. Incorporating third-order scattering coefï¬cients\n(m= 3) marginally improves the classiï¬cation results while\ngreatly increasing the computational load.\nState-of-the-art results on GTZAN are obtained with clas-\nsiï¬ers better adapted than SVMs. These classiï¬ers can also\nbe applied to CLS vectors to improve classiï¬cation results.\nWith MFCCs on 23 ms and other local features, an Ad-\naBoost classiï¬er yields an error of 17% in [1]. The cascade\nï¬lter bank of cortical representations in [10], which is simi-\nlar to a scattering representation, yields an error of 7.6%[9]\nwith a sparse coding classiï¬er.\n4. CONCLUSION\nScattering representations are shown to provide complemen-\ntary co-occurence information which reï¬nes MFCC descrip-\ntors. We demonstrated that second-order scattering coefï¬-\ncients can bring an important improvement over MFCCs for\nclassiï¬cation. The ability to characterize non-stationary sig-\nnal structures opens the possibility to discriminate more so-\nphisticated phenomena such as transients, time-varying ï¬l-\nters and rhythms with co-occurrence scattering coefï¬cients,\nwhich is not possible with MFCCs. It opens a wide range of\napplications for music and speech signal processing.5. REFERENCES\n[1] J. Bergstra, N. Casagrande, D. Erhan, D. Eck, and\nB. K Â´egl, â€œAggregate Features and AdaBoost for Mu-\nsic Classiï¬cation,â€ Machine Learning, V ol. 65, No. 2-3,\npp. 473â€“484, 2006.\n[2] J. Brown: â€œComputer identiï¬cation of musical instru-\nments using pattern recognition with cepstral coefï¬-\ncients as features,â€ Journal of the Acoustical Society of\nAmerica , V ol. 105, No. 3, pp. 1933â€“1941, 1999.\n[3] J. Bruna and S. Mallat: â€œClassiï¬cation with Scattering\nOperators,â€ Proc. of CVPR , 2011.\n[4] T. Dau, B. Kollmeier, and A. Kohlrausch, â€œModeling\nauditory processing of amplitude modulation. I. Detec-\ntion and masking with narrow-band carriers,â€ Journal\nof the Acoustical Society of America , V ol. 102, No. 5,\npp. 2892â€“2905, 1997.\n[5] S. Furui, â€œSpeaker-Independent Isolated Word Recog-\nnition Using Dynamic Features of Speech Spectrum,â€\nIEEE Trans. on Acoustics, Speech, and Signal Process-\ning, V ol. ASSP-34, No. 1, pp. 52â€“59, 1986.\n[6] Y . LeCun, K. Kavukvuoglu, and C. Farabet: â€œConvo-\nlutional Networks and Applications in Vision,â€ Proc. of\nISCAS , 2010.\n[7] H. Lee, P. Pham, Y . Largman, and A. Ng: â€œUnsuper-\nvised feature learning for audio classiï¬cation using con-\nvolutional deep belief networks,â€ Proc. of NIPS , 2009.\n[8] S. Mallat: â€œGroup Invariant Scattering,â€\nhttp://arxiv.org/abs/1101.2286, to appear in Com-\nmunications in Pure and Applied Mathematics .\n[9] Y . Panagakis, C. Kotropoulos, and G. Arce, â€œMusic\nGenre Classiï¬cation Using Locality Preserving Non-\nNegative Tensor Factorization and Sparse Representa-\ntions,â€ Proc. of the International Society for Music In-\nformation Retrieval , 2009.\n[10] T. Chi, P. Ru, and S. Shamma: â€œMultiresolution spec-\ntrotemporal analysis of complex sounds,â€ Journal of the\nAcoustical Society of America , V ol. 118, No. 2, pp. 887â€“\n906, 2005.\n[11] G. Tzanetakis and P. Cook: â€œMusical Genre Classiï¬ca-\ntion of Audio Signals,â€ IEEE Trans. on Speech and Au-\ndio Processing , V ol. 10, No. 5, pp. 293â€“302, 2002.\n[12] I. Waldspurger and S. Mallat: â€œWavelet and Scat-\ntering Phase Retrievalâ€, CMAP Tech. Report,\nhttp://www.cmap.polytechnique.fr/scattering/, 2011.\n662"
    },
    {
        "title": "Humming Method for Content-Based Music Information Retrieval.",
        "author": [
            "Cristina de la Bandera",
            "Ana M. Barbancho",
            "Lorenzo J. TardÃ³n",
            "Simone Sammartino",
            "Isabel Barbancho"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1416610",
        "url": "https://doi.org/10.5281/zenodo.1416610",
        "ee": "https://zenodo.org/records/1416610/files/BanderaBTSB11.pdf",
        "abstract": "In this paper a humming method for music information retrieval is presented. The system uses a database with real songs and does not need another type of symbolic representation of them. The system employs an original fingerprint based on chroma vectors to characterize the humming and the references songs. With this fingerprint, it is possible to get the hummed songs without needed of transcription of the notes of the humming or of the songs. The system showed a good performance on Pop/Rock and Spanish folk music.",
        "zenodo_id": 1416610,
        "dblp_key": "conf/ismir/BanderaBTSB11",
        "keywords": [
            "humming method",
            "music information retrieval",
            "database with real songs",
            "chroma vectors",
            "fingerprint based",
            "characterize humming",
            "references songs",
            "transcription of notes",
            "Pop/Rock",
            "Spanish folk music"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nHUMMING METHOD FOR CONTENT-BASED MUSIC INFORMATION\nRETRIEV AL\nCristina de la Bandera, Ana M. Barbancho, Lorenzo J. Tard Â´on,\nSimone Sammartino and Isabel Barbancho\nDept. Ingenier Â´Ä±a de Comunicaciones, E.T.S. Ingenier Â´Ä±a de Telecomunicaci Â´on\nUniversidad de M Â´alaga, Campus Universitario de Teatinos s/n, 29071, M Â´alaga, Spain\n{cdelabandera, abp, lorenzo, ssammartino, ibp }@ic.uma.es\nABSTRACT\nIn this paper a humming method for music information re-\ntrieval is presented. The system uses a database with real\nsongs and does not need another type of symbolic represen-\ntation of them. The system employs an original ï¬ngerprint\nbased on chroma vectors to characterize the humming and\nthe references songs. With this ï¬ngerprint, it is possible t o\nget the hummed songs without needed of transcription of the\nnotes of the humming or of the songs. The system showed\na good performance on Pop/Rock and Spanish folk music.\n1. INTRODUCTION\nIn recent years, along with the development of Internet, peo -\nple can access to a huge amount of contents like music. The\ntraditional information retrieval systems are text-based but\nthis might not be the best approach for music. There is a\nneed for retrieving the music based on its musical content,\nsuch as humming the melody, which is the most natural way\nfor users to make a melody based query [3].\nQuery by humming systems are having a great expansion\nand their use is integrated not only in computer but also in\nsmall devices like mobile phones [10]. A query by hum-\nming system can be considered as an integration of three\nmain stages: construction of songs database, transcriptio n of\nusersâ€™ melodic information query and matching the queries\nwith songs in the database [5].\nFrom the ï¬rst query by humming system [3] to nowa-\ndays, many systems have appeared. Most of these systems\nuse Midi representation of the songs [2], [6], [9] or they pro -\ncess the songs to obtain a symbolic representation of the\nmain voice [8] or, also, these systems may use special for-\nmats such as karaoke music [11] or other hummings [7] to\nobtain the Midi or other symbolic representation [9] of the\nPermission to make digital or hard copies of all or part of this w ork for\npersonal or classroom use is granted without fee provided th at copies are\nnot made or distributed for proï¬t or commercial advantage and th at copies\nbear this notice and the full citation on the ï¬rst page.\nc/circlecopyrt2011 International Society for Music Information Retrieval .main voice of the songs in the database. In all the cases the\nmain voice or main melody must be obtained because it is\nthe normal content of the humming. Somehow, the normal\nquery by humming systems are based on the melody tran-\nscription of the humming queries [5], [7], [11] to be com-\npared with the main voice melody obtained from the songs\nin the database.\nThe approach employed in this paper is rather different\nfrom other proposals that can be found in the literature. The\ndatabase contains real stereo songs (CD quality). These\nsongs are processed in order to enhance the main voice.\nThen, the humming as well as the signal with the main voice\nenhanced, follow the same process: ï¬ngerprints of the hum-\nming and of the main voice are obtained. In this process, it\nis not necessary to obtain the onset or the exact tone of the\nsound, so, this ï¬ngerprint is a robust representation for th e\nimprecise humming or main voice enhancement.\nThe paper is organized as follows. Section 2 will present\na general overview of the proposed method. Section 3 will\npresent the method of enhancement of the main voice of a\nstereo sound ï¬le. Next, section 4 will propose the ï¬nger-\nprint used to compare the humming and the songs. Section 5\nwill present the comparison and search methods used in the\nproposed system. Section 6 will present some performance\nresults and ï¬nally, Section 7 draws some conclusions.\n2. OVERVIEW OF THE PROPOSED METHOD\nIn this section, a general overview of the structure of the\nhumming method for MIR is given. Figure 1 shows the\ngeneral structure of the proposed method in which both the\nhumming and the songs with the main voice enhanced fol-\nlow the same process.\nAs Figure 1 shows, a phrase fragmentation is needed for\nthe songs. The reason for this is the following: when people\nsing or hum after hearing a song, they normally sing certain\nmusical phrases, not random parts of the songs [11]. So, the\nmain voice enhancement will be performed in the phrases\nof the songs. The result of the main voice enhancement\nof the phrases of the songs and the humming pass through\na preprocessing stage that obtains a representation of thes e\n49Poster Session 1\n/g44/g437/g373/g373/g349/g374/g336 /;###\n/g87/g396/g286/g393/g396/g381/g272/g286/g400/g400/g349/g374/g336 /;### /g87/g396/g286/g393/g396/g381/g272/g286/g400/g400/g349/g374/g336 /;###\n/g38/g349/g374/g336/g286/g396/g87/g396/g349/g374/g410/;###\n/g272/g258/g367/g272/g437/g367/g258/g410/g349/g381/g374/;###/g38/g349/g374/g336/g286/g396/g87/g396/g349/g374/g410/;###\n/g272/g258/g367/g272/g437/g367/g258/g410/g349/g381/g374/;###\n/;##################/g381/g373/g393/g258/g396/g349/g400/g381/g374/;###\n/g920/;###\n/g94/g286/g258/g396/g272/g346/;###/g68/g258/g349/g374/;###/g448/g381/g349/g272/g286/;###\n/g286/g374/g346/g258/g374/g272/g286/g373/g286/g374/g410/;###/g87/g346/g396/g258/g400/g286/;###\n/g296/g396/g258/g336/g373/g286/g374/g410/g258/g410/g349/g381/g374/;###/;########################/g258/g410/g258/g271/g258/g400/g286/;###/g381/g296 /;###\n/g396/g286/g258/g367/;###/g400/g381/g374/g336/g400/;###\n/g75/g396/g282/g286/g396/g286/g282/;###/g367/g349/g400/g410/;###/g381/g296/;###\n/g396/g286/g367/g286/g448/g258/g374/g410/;###/g400/g381/g374/g336/g400/;###\nFigure 1 . General structure of the proposed method.\nsignals in the frequency domain. Then, the ï¬ngerprints are\ncalculated. The ï¬ngerprints are the representation used fo r\nthe comparison and search of the humming songs and hum-\nming. Note that, the proposed method does not perform any\nconversion to Midi or other symbolic music representation.\nFinally, the system provides a list of songs ordered by their\nsimilitude with the humming entry.\n3. ENHANCEMENT OF THE MAIN VOICE\nThe reference method selected to enhance the main voice is\nbased on the previous knowledge of the pan of the signal to\nenhance [1]. The database considered contains internation al\nPop/Rock and Spanish folk music. In this type of music the\nmain voice or melody of the songs is performed by a singer\nand this voice is placed in the center of the audio mix [4].\nIn Fig. 2, the general structure of the algorithm of en-\nhancement of the main voice is presented. The base of this\nalgorithm is the deï¬nition of the stereo signal produced by\na recording studio. A model for this signal is as follows:\nxc(t) =/bracketleftBiggN/summationdisplay\ni=1acjsj(t)/bracketrightBigg\n(1)\nwhere:Nis the number of sources of the mix, the sub-\nscriptcindicates the channel ( 1-left and 2-right),acjare\nthe amplitude-panning coefï¬cients and sj(t)are the differ-\nent audio sources. For amplitude-panned sources it can be/g87/g258/g374/;###/g449/g349/g374/g282/g381/g449 /;###\n/g90/g286/g448/g286/g396/g400/g286/;###\n/g400/g393/g286/g272/g410/g396/g381/g336/g396/g258/g373 /;###\n/g68/g381/g374/g381/;###/g400/g349/g336/g374/g258/g367/;###/g381/g296/;###/g410/g346/g286/;###\n/g286/g374/g346/g258/g374/g272/g286/g282/;###/g373/g258/g349/g374/;###/g448/g381/g349/g272/g286/;###\n/g894/g258/g437/g282/g349/g381/;###/g449/g258/g448/g286/g296/g381/g396/g373/g895/;###/g94/g393/g286/g272/g410/g396/g381/g336/g396/g258/g373/;###\n/g272/g258/g367/g272/g437/g367/g258/g410/g349/g381/g374/;###/g94/g410/g286/g396/g286/g381/;###/g400/g349/g336/g374/g258/g367/;###\n/g894/g258/g437/g282/g349/g381/;###/g449/g258/g448/g286/g296/g396/g381/g373/g895/;###\n/;############################/g374/g346/g258/g374/g272/g286/g373/g286/g374/g410/;###/g381/g296/;###\n/g410/g346/g286/;###/g373/g258/g349/g374/;###/g448/g381/g349/g272/g286 /;###/g94/g349/g373/g349/g367/g258/g396/g349/g410/g455/;###\n/g373/g286/g258/g400/g437/g396/g286/g373/g286/g374/g410/;###/;##################/g346/g258/g374/g374/g286/g367/;###/g62 /;### /;##################/g346/g258/g374/g374/g286/g367/;###/g90/;###\n/g1845/g3013/;### /g1845/g3019/;###\n/g1845/g3049/g3030 /;###\nFigure 2 . General structure of the process of enhancement\nof the main voice.\nassumed that the sinusoidal energy-preserving panning law\nisa2j=/radicalBig\n(1âˆ’a2\n1j), witha1j<1.\nThe spectrogram is calculated in temporal windows of\n8192 samples for signals sampled to 44100Hz. This selec-\ntion is a balance between temporal resolution ( 0.18s) and\nfrequency resolution ( 5Hz).\nThe panning mask, Î¨(m,k), is estimated using the me-\nthod proposed in [1], based on the difference of the ampli-\ntude of the spectrograms of the left channel ( SL(m,k))and\nright channel ( SR(m,k)). The values of Î¨(m,k)vary from\nâˆ’1to1. To avoid distortions due to abrupt changes in ampli-\ntude between adjacent points of the spectrogram produced\nby the panning mask, Î¨(m,k), a Gaussian window func-\ntion is applied to Î¨(m,k)[1]:\nÎ˜(m,k) =Î½+(1âˆ’Î½)Â·eâˆ’1\n2Î¾(Î¨(m,k)âˆ’Î¨o)2(2)\nwhereÎ¨ois the panning factor to locate (from âˆ’1totally left\nand1totally right), Î¾controls the width of the window that\nhas an inï¬‚uence in the distortion/interference allowed, th at\nis, the wider the window, the lower distortion but the larger\nthe interference between other sources and vice versa. Î½is\na ï¬‚oor value to avoid setting spectrogram values to 0.\nThe enhancement of the main voice is made as:\nSvc(m,k) = (SL(m,k)+SR(m,k))Â·Î˜(m,k)Â·Î²(3)\nwhereSvc(m,k)is the spectrogram of the signal with the\nmain voice enhanced. Once the spectrogram Svc(m,k)is\nobtained, the reverse spectrogram is calculated to obtain t he\nwaveform of the enhanced main voice (Figure 2).\n5012th International Society for Music Information Retrieval Conference (ISMIR 2011)\nThe parameters of equation 2, have been set experimen-\ntally to achieve a good result in our humming method. The\nselected values are: Î½= 0.15,Î¨o= 0 due to the fact that\nthe desired source is in the center of the mix and Î¾is calcu-\nlated with the following equation:\nÎ¾=âˆ’Î¨câˆ’Î¨2\no\n20logA(4)\nwhereÎ¨c= 0.2is the margin around Î¨owhere the mask\nwill have an amplitude Asuch that 20logA=âˆ’60dB[1].\nThere are several conditions that are going to negatively\naffect the localization of the main voice; the overlapping o f\nsources with the same panning and the addition of digital\neffects, like reverberation. However, since the aim of the\nproposed method is just the enhancement of the main voice,\ncertain level of interference can be allowed to avoid distor -\ntions in the waveform of the main voice.\n0 2 4 6 8 10âˆ’0.4âˆ’0.3âˆ’0.2âˆ’0.100.10.20.30.4\nTime [s]Amplitude [V]\n(a) Left channel of an stereo\nsignal0 2 4 6 8 10âˆ’0.4âˆ’0.3âˆ’0.2âˆ’0.100.10.20.3\nTime [s]Amplitude [V]\n(b) Right channel of an stereo\nsignal\n0 2 4 6 8 10âˆ’0.25âˆ’0.2âˆ’0.15âˆ’0.1âˆ’0.0500.050.10.150.20.25\nTime [s]Amplitude [V]\n(c) Original main voice signal\nwithout any mixer0 2 4 6 8 10âˆ’1âˆ’0.8âˆ’0.6âˆ’0.4âˆ’0.200.20.40.60.81\nTime [s]Amplitude [V]\n(d) Signal with the enhace-\nment of the main voice\nFigure 3 . Waveforms of the (a) left channel and the (b)\nright channel of an stereo signal. (c) Original main voice\nwithout any mixer. (d) Waveform obtained after the process\nof enhancement of the main voice\nAs an example of the performance of the enhancement\nprocess of the main voice, Figure 3 shows the waveform of\nthe two channels of a stereo signal (Figure 3(a) and Figure\n3(b)), the original main voice (Figure 3(c)) and the wave-\nform obtained after our main voice enhancement process\n(Figure 3(d)). Theses ï¬gures show how the main voice is ex-\ntracted from the mix although some distortion appears. This\nhappens because the gaussian window selected is designed\nto avoid audio distortion but it allows some interference.\n4. FINGERPRINT CALCULATION\nFigure 4, shows the block diagram of the ï¬ngerprint cal-\nculation procedure for the humming and the music in the/;###FingerPrint \ncalculation /;###Preprocessing /;### Threshold \ncalculation \nThreshold \nPeaks detection Windowing \nSpectrum simplification \nCalculation of chroma vectors \nStorage in the chroma matrix \nFingerPrint Spectrum calculation \n/;###\nSpectrum \ncalculat ion \nFigure 4 . Block diagram of the ï¬ngerprint calculation.\ndatabase. Two main stages can be observed: the preprocess-\ning and the chroma matrix calculation. In subsection 4.1,\nthe preprocessing stage is presented and then, in subsectio n\n4.2, the estimation of the chroma matrix, the ï¬ngerprint, is\npresented.\n4.1 Preprocessing of humming and music database\nIn the preprocessing, the ï¬rst step consists on calculate th e\nspectrum of the whole signal, to determine the threshold.\nThe threshold is ï¬xed to the 75thpercentile of the values of\nthe power spectrum. This threshold determines the spec-\ntral components with enough power to belong to a voice\nfragment. Now, the signal is windowed without overlap-\nping with a Hamming window of 8192 samples. For each\nwindow the spectrum is computed. Then, we select the fre-\nquency range from 82Hzto1046Hz, that corresponds to\nE2toC6, because this is a normal range for signing voice.\nIn this range, a peaks detection procedure is performed.\nThe local maxima and minima are located and the ascend-\ning and descending slopes are calculated. We consider sig-\nniï¬cative peaks the maxima detected over the threshold that\npresent an ascending or descending slope larger than or equa l\nto the25% of the maximum slope found. Between these\npeaks, the four peaks with larger power are selected to rep-\nresent the tonal distribution of the window. Ideally, the fo ur\npeaks selected should correspond to the fundamental fre-\nquency and the ï¬rst three harmonics of the signing note. The\nnumber of peaks has been restricted to four because the ob-\njective is just to gather information of the main voice (mono -\nphonic sound), which has several interferences from other\nsound sources, or because of the enhacement process of the\nmain voice (Section 3). If we selected more peaks, these\npeaks would corresponding to other notes different from the\nnotes sung by the main voice and then, the comparison with\n51Poster Session 1\nthe humming would be worse. In Fig. 5, an example of this\nprocess is shown.\n200 400 600 800 1000 120000.10.20.30.40.50.60.70.80.91\nFrequency [Hz]Normalized power spectrum\n  \nPower spectrum\nLocal maximum\nLocal minimum\nDetected peaks\nThreshold\nFigure 5 . Example of peaks selected.\nNext, the new signal spectrum that contains just the se-\nlected peaks, is simpliï¬ed making use of the Midi numbers.\nThe frequency axis is converted to Midi numbers, using:\nMIDI= 69+12 log2/parenleftbiggf\n440/parenrightbigg\n(5)\nwhere MIDI is the Midi number corresponding to the fre-\nquencyf. The simpliï¬cation consists of assigning to each\nof the selected peaks the nearest Midi number. When two\nor more peaks are ï¬xed to the same Midi number, only the\npeak with the largest value is taken into account. The sim-\npliï¬ed spectrum is represented by Xs(n). In our case, the\nï¬rst element of the simpliï¬ed spectrum, Xs(1), represents\nthe spectral amplitude of the note E2, that corresponds with\nthe frequency 82Hz(Midi number 40). Likewise, the last\nelement of the simpliï¬ed spectrum, Xs(45), represent the\nspectral amplitude of then note C6, that corresponds to the\nfrequency 1046Hz(Midi number 84).\n4.2 Chroma matrix\nNow, to obtain the ï¬ngerprint of each signal, the chroma ma-\ntrix, the chroma vector is computed for each temporal win-\ndow. The chroma vector is a 12-dimensional vector (from\nCtoB) obtained by the sum of the spectral amplitudes for\neach tone, spawning through the notes considered (from E2\ntoC6). Eachkâˆ’thelement of the chroma vector, with\nkâˆˆ {1,2,Â·Â·Â·,12}of the window, t, is computed as fol-\nlows:\nchroma t(k) =3/summationdisplay\ni=0Xs((k+7)mod12+12Â·i+1) (6)\nThe chroma vectors for each temporal window tare com-\nputed and stored in a matrix denominated chroma matrix,C. The chroma matrix has 12rows and a column for each\nof the temporal windows of the signal analyzed.\nIn order to unify the dimensions of all the chroma matri-\nces of all the phrase fragments of the songs and humming,\nthe matrix is interpolated. To perform the interpolation, t he\nnumber of selected columns is 86, this value corresponds,\napproximately, to 16seconds. This number of columns has\nbeen selected taking into account the length of the phrase\nfragments of the songs in the database and the reasonable\nduration of the humming. Let C=/bracketleftbigÂ¯F1,Â¯F2,Â·Â·Â·,Â¯F86/bracketrightbig\n, de-\nnote this matrix, where Â¯Fi, represents the column iin the\ninterpolated chroma matrix that represents the ï¬ngerprint .\nIn Figure 6, an example of a chroma matrix with interpo-\nlation is represented.\nC\nC#\nD\nD#\nE\nF\nF#\nG\nG#\nA\nA#\nB\n0102030405060708000.20.40.60.81\nInterpolated temporal windowChromatic scale noteNormalized energy\nFigure 6 . Chroma matrix with interpolation.\n5. COMPARISON AND SEARCH METHOD\nOnce the ï¬ngerprint has been deï¬ned, the ï¬ngerprints for\neach phrase fragment of the songs in the database are com-\nputed. Now, the task is to ï¬nd the song in the database that\nis the most similar to a certain humming. To this end, the\nï¬ngerprint of the humming is obtained, then, the search for\nthe most similar ï¬ngerprint is made. This search is based\non the deï¬nition of the distance between the ï¬ngerprint of\nthe humming signal and the ï¬ngerprints of the songs in the\ndatabase.\nThe objective is to create a distance vector with length\nequal to the number of phrase fragments in the database.\nThen, a list of ordered songs from the most similar song to\nthe less similar one can be obtained. The distance between\nï¬ngerprints is computed using:\nDstk(Chumm,Ck) =median({dkj}) (7)\ndkj=/vextenddouble/vextenddoubleÂ¯Fhumm\njâˆ’Â¯Fk\nj/vextenddouble/vextenddouble (8)\nwhereDstkis the distance of the humming to a phrase frag-\nmentk,kis the index of all the phrase fragments in the\ndatabase. Chummis the ï¬ngerprint of the humming and Ck\n5212th International Society for Music Information Retrieval Conference (ISMIR 2011)\nis the ï¬ngerprint of each phrase fragment. The euclidean\ndistance between columns of the ï¬ngerprints dkj, is calcu-\nlated. Afterwards, the median of the set of euclidean dis-\ntances,{dkj}, is stored in Dstk.\nThe distance values Dstkare ordered from the smallest\nvalue to the largest value. Now, since for each song several\nphrase fragments have been considered, the phrase closest\nto the humming is selected to deï¬ne the closest songs. The\nlist of similar songs is created likewise.\nAn illustration of the utilization of the ï¬ngerprints to ï¬nd\nsimilar songs to a given humming is shown in Figure 7. The\nï¬ngerprint of a humming (Figure 7(a)), the nearest song,\nthat is, the corresponding song (Figure 7(b)) and the farthe st\nsong (Figure 7(c) ) are presented. It can be observed how the\nï¬ngerprint of the humming and the corresponding song look\nvery similar. On the contrary, the ï¬ngerprint of the farthes t\nsong looks totally different.\n(a) Fingerprint of a humming\n(b) Fingerprint of the nearest\nsong\n(c) Fingerprint of the farthest\nsong\nFigure 7 . Fingerprint of (a) a humming, (b) the nearest song\nand (c) the farthest song.\n6. RESULTS\nThe music database used in this study contained 140 songs\nextracted from commercial CDs of different genres: Pop/-\nRock and Spanish folk music. The selected phrase frag-\nments of each song are segments of 5 to 20 seconds, de-\npending on the predominant melodic line of each song.\nFor the evaluation of the system, we have used 70hum-\nmings from three male and three female users, whose ages\nare between 25and57years, and 50% of the users have mu-sical knowledge. The hummings were recorded at a sam-\npling rate of 44.1kHz and the duration of each humming\nranges from 5 to 20 seconds.\nThe retrieval performance was measured on the basis of\nSong accuracy . In general, we computed the Top-N accu-\nracy, that is the percentage of humming whose target songs\nwere among the Topâˆ’Nranked songs. The Top-N accu-\nracy is deï¬ned as:\nTopâˆ’N accuracy (%) =#Songs in Top âˆ’N\n#hummingsÃ—100%\n(9)\nDifferent experiments have been made to test the system ef-\nfectiveness as a function of the musical genre. The musi-\ncal genre has inï¬‚uence on the harmonic complexity of the\nsongs, the number of musical instruments played, the kind\nof accompaniment and the presence of rhythm instruments\nsuch as drums. All these musical aspects affect in the main\nvoice enhancement process.\nIn Table 1, the evaluation of the proposed method in the\ncomplete database, for all hummings, for 5different rank-\ning are presented. These results are rather similar to the\nones presented in [7] and [8], with the difference that our\nmethod uses real songs instead of other hummings [7] and\nour method does not need to obtain the symbolic notation of\nneither the database nor the humming [8]. Thus, a mathe-\nmatical comparison against other systems has not been pos-\nsible since other systems found do not use real audio wave-\nforms. The Table 1 also includes the Top-N accuracy for\nmusical genres: Pop/Rock and Spanish folk. It can be ob-\nserved that the performance of the system is better for the\nSpanish folk music. This is due to the fact that in this type\nof music the main voice is the most important part in the mu-\nsic and does not have digital audio effects like reverberati on,\ntherefore the main voice enhancement process performs bet-\nter.\nTable 1 . Evaluation of the proposed method in the complete\ndatabase for all hummings, Pop/Rock and Spanish folk.\nTop-N accuracy (%)\nRanking All Pop/Rock Spanish folk\nTop-1 37.12 33.33 47.33\nTop-5 52.86 43.14 78.95\nTop-10 55.71 45.10 84.21\nTop-20 60.00 49.02 89.47\nTop-30 61.43 49.02 94.74\nIn Table 2, the evaluation of the proposed method is done\nwith the database divided into two music collection: one\ncorresponding to Pop/Rock music ( 70% of songs in the data-\nbase) and other corresponding to Spanish folk music ( 30%\nof songs in the database). The hummings are divided in the\nsame percentages as the music in the database. In Table 2, it\n53Poster Session 1\nTable 2 . Evaluation of the proposed method with the\ndatabase divided into two music collections: Pop/Rock and\nSpanish folk.\nTop-N accuracy (%)\nRanking Pop/Rock Spanish folk\nTop-1 35.29 57.89\nTop-5 45.10 84.21\nTop-10 47.06 89.47\nTop-20 52.94 94.74\ncan be observed that the performace of the system is better\nfor the Spanish folk music, like in the previous experiment\nshown in Table 1.\nIn Figure 8, the evolution of the Top-N accuracy (%) as a\nfunction of Nas percentage of the music collection in which\nthe humming is expected to be found, is shown. This evo-\nlution is presented for the complete database, the Pop/Rock\nmusic collection and the Spanish folk music collection. Fig -\nure 8 shows that the Spanish folk music obtains the best re-\nsults, as presented the Table 2. This ï¬gure also shows that if\nthe user or the system have some knowledge of the musical\ngenre, the humming method becames more effective.\nFigure 8 . Evolution of the Top-N accuracy (%) as a func-\ntion ofNas percentage of the music collection in which the\nhumming is expected to be found.\n7. CONCLUSIONS\nIn this paper a humming method for content-based music in-\nformation retrieval has been presented. The system employs\nan original ï¬ngerprint based on chroma vectors to character -\nize the humming and the reference songs. With this ï¬nger-\nprint, it is possible to ï¬nd songs similar to humming with-\nout any transcription or Midi data. The performance of the\nmethod is better in Spanish folk music, due to the main voice\nenhancement procedure in relation with the mixing style\nused in this type of music, than in Pop/Rock music. The\nmethod performance could be improved if an estimation of\nthe musical genre is included. Also, the parameters of thepanning window could be tuned for each musical genre to\nimprove the performance of the main voice enhancement.\nFinally, the system could also be made robust to transposed\nhummings, employing a set of transposed chroma matrices\nfor each humming.\n8. ACKNOWLEDGMENTS\nThis work has been funded by the Ministerio de Ciencia e\nInnovaci Â´on of the Spanish Government under Project No.\nTIN2010-21089-C03-02.\n9. REFERENCES\n[1] C. Avendano: â€œFrequency-domain source identiï¬ca-\ntion and manipulation in stereo mixes for enhancement,\nsuppression and re-panning applications,â€ IEEE Work-\nshop on Applications of Signal Processing to Audio and\nAcoustics , pp. 55â€“58, 2003.\n[2] L. Chen and B.-G. Hu: â€œAn implementation of web\nbased query by humming system,â€ International Confer-\nence on Multimedia and Expo (ICME2007), pp. 1467â€“\n1470, 2007.\n[3] A. Ghias, J. Logan and D. Chamberlin: â€œQuery\nby humming-musical information retrieval in an\naudio database,â€ Proceedings of ACM Multimedia\n(ACM1995), pp. 231â€“236, 1995.\n[4] D. Gibson: The art of mixing , MixBooks, Michigan,\n1997.\n[5] J. Li, J. Han, Z. Shi and J. Li: â€œAn efï¬cient approach\nto humming transcription for Query-by-Humming Sys-\ntem,â€ 3rd International Congress on Image and Signal\nProcessing (CSIP2010), pp. 3746â€“3749, 2010.\n[6] J. Li, L.-m. Zheng, L. Yang, L.-j. Tian, P. Wu and\nH. Zhu: â€œImproved Dynamic Time Warping Algorithm\nthe research and application of Query by Humming,â€\nSixth International Conference on Natural Computation\n(ICNC2010), pp. 3349â€“3353, 2010.\n[7] T. Liu, X. Huang, L. Yang and P.Zhang: â€œQuery\nby Humming: Comparing V oices to V oices,â€ Interna-\ntional Conference on Management and Service Science\n(MASSâ€™09), pp. 1â€“4, 2009.\n[8] J. Song, S.-Y . Bae and K. Yoon: â€œQuery by Hum-\nming: Matching humming query to polyphonic audio,â€\nIEEE International Conference on Multimedia and Expo\n(ICME02), V ol. 1, pp. 329â€“332, 2002.\n[9] E. Unal, E. Chew, P.G. Georgiou and S.S. Narayanan:\nâ€œChallenging Uncertainty in Query by Humming Sys-\ntems: A Fingerprinting approach,â€ IEEE Transactions\non Audio, Speech and Language Processing , V ol. 16,\nNo. 2, pp. 359â€“371, 2008.\n[10] X. Xie, L. Lu, M. Jia, H. Li, F. Seide and W.-Y . Ma:\nâ€œMobile search with multimodal queries,â€ Proceedings\nof the IEEE , V ol. 96, No. 4, pp. 589â€“601, 2008.\n[11] H.-M. Yu, W.-H. Tsai and H.-M. Wang: â€œA Query-by-\nSinging System for Retrieving Karaoke Music,â€ IEEE\nTransactions on multimedia , V ol. 10, No. 8, pp. 1626â€“\n1637, 2008.\n54"
    },
    {
        "title": "Ethnographic Observations of Musicologists at the British Library: Implications for Music Information Retrieval.",
        "author": [
            "Mathieu Barthet",
            "Simon Dixon"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1415670",
        "url": "https://doi.org/10.5281/zenodo.1415670",
        "ee": "https://zenodo.org/records/1415670/files/BarthetD11.pdf",
        "abstract": "Without a rich understanding of user behaviours and needs, music information retrieval (MIR) systems might not be ideally suited to their potential users. In this study, we followed an ethnographic methodology to elicit some of the strategies used by musicologists to explore and document musical performances, in order to investigate if and how technologies could enhance such a process. Observations of musicologists studying historical recordings of classical music were conducted at the British Library. The observations show that the musicologists alternate between a closed listening practice, relying exclusively on aural observations, and a multimodal listening practice, where they interact with various music representations and information sources using different media (e.g. metadata about the recordings and performers, sound visualisations, scores, lyrics and performance videos). The spoken parts of broadcast recordings brought historical/extra-musical clues helping to understand music performance practices. Sound visualisation and computational methods fostered the analysis of specific musical expression patterns. We suggest that software designed for musicologists should facilitate switching between closed and multimodal listening modes, interaction with scores and lyrics, and analysis and annotation of speech and music performance using content-based MIR techniques.",
        "zenodo_id": 1415670,
        "dblp_key": "conf/ismir/BarthetD11",
        "keywords": [
            "music information retrieval",
            "ethnographic methodology",
            "musicologists",
            "historical recordings",
            "multimodal listening practice",
            "closed listening practice",
            "sound visualisation",
            "computational methods",
            "analysis and annotation",
            "content-based MIR techniques"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nETHNOGRAPHIC OBSERVATIONS OF MUSICOLOGISTS AT THE BRITISH\nLIBRARY: IMPLICATIONS FOR MUSIC INFORMATION RETRIEVAL\nMathieu Barthet\nCentre for Digital Music\nQueen Mary University of London\nmathieu.barthet@eecs.qmul.ac.ukSimon Dixon\nCentre for Digital Music\nQueen Mary University of London\nsimon.dixon@eecs.qmul.ac.uk\nABSTRACT\nWithout a rich understanding of user behaviours and needs,\nmusic information retrieval (MIR) systems might not be ide-\nally suited to their potential users. In this study, we fol-\nlowed an ethnographic methodology to elicit some of the\nstrategies used by musicologists to explore and document\nmusical performances, in order to investigate if and how\ntechnologies could enhance such a process. Observations\nof musicologists studying historical recordings of classical\nmusic were conducted at the British Library. The observa-\ntions show that the musicologists alternate between a closed\nlistening practice, relying exclusively on aural observations,\nand a multimodal listening practice, where they interact with\nvarious music representations and information sources us-\ning different media (e.g. metadata about the recordings and\nperformers, sound visualisations, scores, lyrics and perfor-\nmance videos). The spoken parts of broadcast recordings\nbrought historical/extra-musical clues helping to understand\nmusic performance practices. Sound visualisation and com-\nputational methods fostered the analysis of speciï¬c musi-\ncal expression patterns. We suggest that software designed\nfor musicologists should facilitate switching between closed\nand multimodal listening modes, interaction with scores and\nlyrics, and analysis and annotation of speech and music per-\nformance using content-based MIR techniques.\n1. INTRODUCTION\nThe interdisciplinary research area of music information re-\ntrieval (MIR) has developed from two needs: managing in-\ncreasing collections of music material in digital form, and\nsolving fundamental problems related to music analysis and\nperception [1]. Over the past decade, a wide variety of\nMIR techniques and tools have been developed using var-\nious types of music representations (audio, symbolic, vi-\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proï¬t or commercial advantage and that copies\nbear this notice and the full citation on the ï¬rst page.\nc/circlecopyrt2011 International Society for Music Information Retrieval.sual and metadata). However, as Cunningham [2] points\nout, they have often been designed based on anecdotal evi-\ndence, intuitive feelings, or a priori assumptions of user be-\nhaviours and needs. Without a rich understanding of the\nlatter, systems designed using MIR research might not be\nideally suited to their potential users. Bridging the gap be-\ntween the research laboratory and real-world situations is\none of the goals of this study. This requires working with\nspeciï¬c user groups in order to better understand their activ-\nity and how they interact with technologies.\nWe focused on eliciting some of the strategies used by\nmusicologists to explore musical documents, and the inter-\nactions with music-related technologies during this process.\nBonardi [3] proposed interesting solutions to improve musi-\ncologistsâ€™ workstations using MIR technologies by examin-\ning their needs when analysing the contemporary catalogue\nat IRCAMâ€™s digital library. He stated that the workstations\nshould allow various representations of music (e.g. graphi-\ncal, sound, and symbolic), listening to recordings while con-\nsulting different musical documents (â€˜activeâ€™ listening), and\nreading (e.g. the score) and writing using the same media.\nIn this study, we sought to obtain evidence to test the\nvalidity of such statements, and whether they would be rel-\nevant in a different context (setting, different types of mu-\nsicological studies, and musical repertoires). We conducted\nan ethnographic study based on the observation of musicol-\nogists working with classical music recordings at the British\nLibrary in London. The ethnographic method is a qualita-\ntiveapproach by which ï¬ndings are not inferred from sta-\ntistical tests but from the detailed analysis of the behaviours\nand actions of the participants across a large number of ï¬eld\nobservations. The outcomes of our research are twofold:\nthey give insights on how to adapt or improve existing digi-\ntal music technologies to ï¬t user needs better (e.g. MIR tech-\nniques, software features, user interface design), and they\ncan raise ideas for the development of new systems.\nThe remainder of the paper is structured as follows. Sec-\ntion 2 presents the setting and methods of the ethnographic\nstudy. Sections 3, 4, and 5 are devoted to thematic analy-\nses based on the observations. We discuss the ï¬ndings in\nSection 6 and give a conclusion in Section 7.\n353Oral Session 5: User Studies\n2. SETTING AND METHODS\nVia the Edison Fellowship scheme, the British Library (BL)\nencourages musicological studies devoted to the history of\nrecordings of classical music and music in performance, by\ncreating the conditions for concentrated use of the Libraryâ€™s\nrecordings collection to scholars selected on a yearly ba-\nsis. The BLâ€™s sound archive counts more than 3.5 million\npublished and unpublished recordings of sounds and mu-\nsic including many unique historic items. In an era where\nthe Library develops the access and analysis of digitised\nrecordings, and given the close link between MIR and li-\nbrary science (e.g. representation, classiï¬cation, metadata),\nthe BL is a good setting for investigating the possible roles\nof music-related technologies in musicological research. We\ncontacted four Edison Fellows with the collaboration of the\nBLâ€™s music department staff, and obtained their consent to\nparticipate in the study. The names used in the ï¬eldnotes\npresented throughout the article are pseudonyms. The group\nwas formed of two British and two American males (average\nage 38). Their professional activities included research and\nteaching positions (PhD in musicology, lecturer in music,\nsinging teacher), as well as performance (pianist, singer).\nTwo of them had received training in science and technol-\nogy. The musical repertoires they studied were varied: early\nmusic (e.g. medieval dance, vocal and consort music), clas-\nsical and romantic music (e.g. art songs, operas, piano solo\npieces), and contemporary music (electronic music).\nWe chose an ethnographic methodology primarily based\non participant observation [4]. One of the advantages of ob-\nserving the actions of participants performed in a concrete\nsetting is that it gives access to what people do and how\n(behaviours) rather than what people say (attitudes), the lat-\nter being obtained with other qualitative methods such as\nsurvey, questionnaire, or interviews. Furthermore, staying\nfor a relatively long period of time in the environment of\nthe group studied fosters the collection of rich details which\nwould otherwise demand a high degree of self-awareness\nand a great power of recall for people to report out of the\ncontext of the activity. To achieve this level of detail, it is\nnecessary to focus on a small number of participants. This\ntradeoff of quantity for quality is common in disciplines re-\nlying on qualitative methods (e.g. psychology) [4, 5].\nThe observations were made by focusing (i) on the pro-\ncesses underlying musicological research, and (ii) on the re-\nlationships with music-related technologies and their roles\nduring such processes. The observations took place in the\nmusic department of the BL where the Fellows had a re-\nserved desk space at their disposal. They were conducted by\none ethnographer during repeated visits (twice a week, on\naverage) over a period of three months. The observational\ndata were collected by taking ï¬eldnotes using a notebook\nand pen. Due to the regulations of the British Library and\nin order to minimize disturbance to the staff, video/audio\nrecordings were not used. Ancillary sources of informationwere also used in addition to the observations. Ethnographic\ninterviews [4] occurred during the research in the ï¬eld in or-\nder to shed light on speciï¬c tasks and to have a deeper under-\nstanding of the scope of the studies of the participant. Some\nof the participantsâ€™ own working notes were also employed,\nwith their consent. The collected data were analysed using\nthe approach proposed in [6], which draws from methods\ndeveloped by sociologists following the grounded theory:\ncoding of the ï¬eldnotes (identifying and naming speciï¬c an-\nalytic dimensions and categories), and analysis by themes\nwhich reï¬‚ect recurrent or underlying patterns of activity.\n3. USE OF RECORDINGS\n3.1 Retrieval and metadata\nMetadata were used to facilitate the retrieval of recordings in\nthe BLâ€™s catalogue (by using details such as the record num-\nber, the label, or the conductorâ€™s name). Additional meta-\ndata were fetched during the listening process (see Subsec-\ntion 4.2), using various sources of information: the knowl-\nedge of the Libraryâ€™s curators, the web, the recordingsâ€™ car-\nriers, liner notes, or accompanying manuscript documents\n(e.g. a paper card system that an original collector had kept).\n3.2 Format and playback technologies\nThe recordings already digitized were immediately accessi-\nble through the British Library Sound Server as MP3 ï¬les.\nWhen the recordings were unique or held on fragile for-\nmats (e.g. reel to reel tapes), the Fellows were provided with\nanalog copies of the recordings or digitized versions on au-\ndio CDs, or less commonly, VHS tapes (PCM). The ana-\nlog formats included reel to reel tapes, compact audio cas-\nsette (K7), as well as long-playing (LP) and 78 rpm discs.\nThe recordings held on a physical support were played us-\ning dedicated playback equipment connected to an ampli-\nï¬er. The MP3 ï¬les from the Sound Server were played from\nthe desktop computer using Windows Media Player Clas-\nsic. In some cases, they also listened to and analysed owned\ncommercial recordings with their laptops using iTunes to\nplay recordings, and Sonic Visualiser1as a player and anal-\nysis tool (see Subsection 4.2). For some of the Fellows, the\nformat was not an issue since they were interested in the\ncontent of the recordings and not the carrier itself. In that\ncase, they were not bothered by use of MP3 ï¬les rather than\nuncompressed digital or original analog recordings. On the\ncontrary, digital recordings were preferred because the navi-\ngation in recordings was made easier and quicker. However,\nothers preferred to deal with recordings in their original for-\nmat ( â€œThere is more context when you have the original, the\nlabels, how it was held for instance. With most MP3s you\ndo lose something. I do wonder whether sometimes youâ€™re\nlosing the core product. â€ ).\n1http://www.sonicvisualiser.org/\n35412th International Society for Music Information Retrieval Conference (ISMIR 2011)\n4. LISTENING AND OBSERVING\n4.1 Listening practices\nThe listening process lay at the center of the study of the\nrecordings. The musicologists commonly alternated two\ndistinct but complementary practices of listening. In the\nï¬rst listening practice, the analysis of the recordings was\nperformed exclusively through aural observations. The sec-\nond listening practice was multimodal and characterised by\nan interactivity with musical, textual or visual documents\nenriching or modifying the aural observations.\n4.1.1 Closed listening\nClosed listening was characterised by a careful and focused\nlistening to the recording without using any other source of\ninformation than that provided by the sound: William put\nhis headphones on to listen to Telemannâ€™s Concerto in F for\n3 recorders, 2 oboes, 2 violins, and continuo, performed by\nthe Early Music Consort. After starting the recording in the\nCD player, he sat back in his chair, closed his eyes, and lis-\ntened carefully to the music. A moment later, I noticed that\nhe was tapping the beat with his foot. This example shows\nhow the aural experience became a physical one (tapping the\nbeat with the foot) while retrieving information about the\ntiming of the musical piece (tempo). In the closed listening\nmode, the musicologists drew aural observations involving\nperceptual and cognitive aspects (a recollection of the score,\nfor instance). Either in parallel or shortly after the listening\nprocess, they wrote down their aural observations by hand\nor using a text editor. Typed notes had the advantage that\nthey could be queried quickly by using keywords such as\nthe name of a composer.\n4.1.2 Multimodal listening\nA different practice was characterised by the use of vari-\nous music-related documents (e.g. the biography of a com-\nposer, information on the recording) and music represen-\ntations (e.g. scores, feature visualisations) while listening.\nThis listening practice can be described as an active pro-\ncess [3], since it does not just consist of receiving musical\ninformation, but is on the contrary based on a set of mul-\ntimodal interactions between the listeners and musical doc-\numents. The advantages of using multiple modalities were\nan increased access to meaning, uncovering the context of\na recording and the intentions of composers, conductors, or\nperformers, and better understanding of the perception of\nthe music. Multimodal listening was performed by varying\nthe media and technologies used to document the musical\nrecordings.4.2 Documenting the music recordings\n4.2.1 Contextual information\nIn the multimodal listening practice, the musicologists com-\nmonly used web resources to seek several types of informa-\ntion related to the recordings: contextual (ï¬nding metadata\nabout a musical piece, for instance), bibliographic (music\nartistsâ€™ websites, Wikipedia), as well as visual and icono-\ngraphic (YouTube videos were sometimes used to uncover\nvisual aspects of performance, Google Images was used to\nprovide pictures of speciï¬c musicians). Such resources were\nalso used without listening to the recordings.\n4.2.2 Scores and lyrics\nThe online music sheet database from the International Mu-\nsic Score Library Project2was often used to retrieve public\ndomain editions of scores, which are provided as scanned\nimages in PDF format. Some of the musicologists read the\nscore using a printed copy, while others used the electronic\nformat and followed the music with the mouse while listen-\ning. When they were available, scores were used both in\norder to retrieve general information such as the key of a\npiece, and more detailed information through a close analy-\nsis of the notes and expressive notation. Singing while read-\ning the score was sometimes used to ï¬nd the scale used by\nthe composer (e.g. Lydian mode). Scores acted as a refer-\nence against which to test whether the intentions of the com-\nposer were respected by performers, as the following notes\nshow: â€œSeems really consistent with markings in the score.\nBeautifully sung - singing the note values and generally the\ndynamics written by Samuel Coleridge-Taylor. â€ ,â€œIs much\nfreer with the interpretation of the score. Interpolates a high\nnote at the end and changes the melodic line at the end of\nthe song. â€ In the case of vocal music compositions, reading\nthe lyrics while listening also helped to follow the musical\nstructure and to understand the expression, as shown in this\nnote describing the timbre of the performerâ€™s tones by ref-\nerence to the lyrics rather than the pitch: â€œquite shrill and\nshaky on â€˜A wind comes and let me beâ€™, and more mellow\non â€˜said it slowâ€™. â€\n4.2.3 Sound visualisation, acoustical analyses, and\ntime-stretching\nMusicologists with previous background in music technolo-\ngies (coming either from their education, personal training,\nor from collaboration with computer scientists) also used\nsoftware (Sonic Visualiser) to analyse and visualise music\nrecordings. The visualisation of the waveform was help-\nful to navigate digital recordings by jumping between sec-\ntions that have different dynamics (e.g. between a spoken\npart and the start of an orchestral part, for instance). Spec-\ntrogram representations were used to analyse the subtleties\nof expressive effect such as the vibrato: â€œIf Iâ€™m looking at\n2http://www.imslp.org/\n355Oral Session 5: User Studies\na waveform [the one from a toneâ€™s partial] and I can see\nthere is vibrato in the note, I hear it much betterâ€ . Such\nacoustical analyses helped to understand the perceptual ef-\nfects experienced when listening: â€œThe spectrogram shows\nyou that the real skill to her [Emma Kirkbyâ€™s] vibrato use\nis that the note starts with very very minimal vibrato. So\nyour mind is fed a very accurate pitch, before the pitch is\nthen decorated by vibrato. So thatâ€™s why you hear it as such\na pure voice, because sheâ€™s already told you the informa-\ntion about exactly what the note is before it vibrates, so your\nbrain somehow keeps on that central tuning issue during the\nvibration [...] Whereas singers that immediately start with\nvibrato, you can never really tell what theyâ€™re singing. â€\nAcoustical measurements were performed from the spec-\ntrogram representations (a measure tool is provided in Sonic\nVisualiser) in order to characterise the properties of vibrato\n(frequency, and pitch extent). These measurements were\nconducted in a systematic way for various performers by\ncomparing long sustained notes. The resulting quantitative\ndata gave clues to understand or nuance aural observations\nmade on vibrato by other musicologists: â€œThe minimal vi-\nbrato sounds that Munrow listed [...] were all faster and\nshallower than the other examples of vibrato. When this is\ncombined with Munrowâ€™s own explicit disapproval of con-\nstant vibrato, we begin to understand that he is suggesting a\npreference for â€˜controlledâ€™ vibratoâ€.\nThe time-stretching technique provided by Sonic Visu-\naliser which preserves the original pitch and timbre was also\nused to produce slowed-down versions of notes or musical\npassages. These slowed-down excerpts were played while\nvisualising scrolling spectrogram representations giving the\ntime to the ear and the eye to uncover ï¬ne details: â€œI knew\nsomething was up through listening but I couldnâ€™t tell what\nwas up, and then when I visualised ... when I slowed down,\nmore of it made sense, I realised the vibrato was not consis-\ntent, but I couldnâ€™t work out that it started without vibrato\nwithout the spectrogramâ€ . Spectrogram analyses and time-\nstretching were also used to validate intuitions obtained with\naural observations to explain the technique and expression\nof a pianist. By looking at the alignment of the notes on the\nspectrogram while listening to a slowed-down passage, sub-\ntle differences of timing between chord notes played by the\nleft and the right hands were noticed.\n5. MUSICAL FEATURES\n5.1 Instrumentation and tuning\nDetails about the instrumentation were retrieved in several\nways: from the recordingsâ€™ metadata, from the announcer\nin the case of broadcast music programmes, or by ear when\nlistening to a musical piece. The choice of instrumentation\nwas an important aspect in the study of historically informed\nperformances of early music (e.g. choice of epoch instru-\nments rather than modern ones), especially since early mu-sic scores do not indicate instruments. The recognition of\nmodern versus epoch instruments in musical performances\nwas not a trivial process to perform aurally ( â€œIâ€™m assuming\nthese are modern instruments, 440 etc. â€ ). Similarly, iso-\nlating a speciï¬c instrument amongst an ensemble (e.g. the\nviolin in a string ensemble: â€œâ€˜Quan je voy le ducâ€™ - most at-\ntractive instrumental piece of collection but horrid scratchy\nstring playing, ï¬ddle?â€ ), or retrieving the number of musi-\ncians playing a part in a speciï¬c register ( â€œTwo sopranos?â€ )\nwere not easy tasks. The tuning of the instruments was also\nused to judge musical interpretations (e.g. â€œLamento della\nNymfa particularly telling with too many harpsichords I feel\n- each one slightly out of tune. â€ ,â€œTuning of violins not great\nin second trackâ€ ).\n5.2 Musical expression\nVarious musical features correlated to musical expression\nwere recurrently analysed, including: dynamics (e.g. â€œDeller\nusing great sweeping phrases with many dynamic nuances. â€ ),\ntiming (e.g. tapping the beat while listening to increase the\nsensation of the tempo, measuring the duration of a perfor-\nmance, detailed analyses of pianistsâ€™ hand asynchronies us-\ning spectrograms), timbre (e.g. â€œWhen she sings softer, she\ndoesnâ€™t have the same quality. The notes sound mellower. â€ ),\npitch (e.g. â€œThe Kingâ€™s Singersâ€™ style hasnâ€™t changed much\nbut alto sound is ï¬‚at!â€ ), vocal style (e.g. â€œOverabundance\nof rolled â€˜Râ€™s - stylistically ok, but a bit obtrusive in an oth-\nerwise beautiful rendition. â€ ), vibrato (e.g. â€œDeller consort\nstill has a lot of vibrato in tenor(s) but very good ensem-\nble singing. â€ ), and phrasing (e.g. â€œReminiscent of baroque\nphrasing rather than renaissance. â€ ).\nMusical expression was analysed either by considering\na speciï¬c performer (e.g. the singer Deller), or by consid-\nering an ensemble (e.g. the Kingâ€™s Singers, the tenors), by\nfocusing on the notes, or on phrases, the latter showing the\nuse of different time scales in the analyses. Some features\nwere more difï¬cult to describe solely based on aural obser-\nvations than others. If dynamics variations seemed to be\neasily perceived, some variations of pitch and timbre were\nmore difï¬cult to detect conï¬dently (e.g. â€œNot sure it stays\nin tune too well, sinking over the whole perf, less than a\nsemi. â€ ,â€œMay be because of the choice of quality for notes\nof the same pitch on two different pieces, the voice doesnâ€™t\nsound the same: it doesnâ€™t sound as shrill as it did on the G.\nMay be due to the key D /flat.â€). Often the expertise of the musi-\ncologists as performers was employed to ï¬nd causal expla-\nnations of sound effects based on instrumental techniques:\nsingers were able to associate vocal timbre variations with\nthe vocal technique used to produce them ( â€œDeller seems\nto use chest voice for the second, lower, â€˜Zionâ€™. â€ ), pianists\nwere able to detect timing effects between notes by focusing\non the hand technique (hand asynchrony in chords) charac-\nteristic of the style of the performer. Musical expression was\nalso described in a critical way by using aesthetic judgments\n35612th International Society for Music Information Retrieval Conference (ISMIR 2011)\n(â€œSoprano sound is rather lovely it must be saidâ€ ,â€œâ€˜des-\nolataâ€™ is quite seasickâ€ ,â€œI love the bottom of her voiceâ€ ,\nâ€œBeautiful - very clear renditionâ€ ,â€œVery rousing and exhil-\narating rendition by Webster Boothâ€ ).\n6. DISCUSSION\n6.1 Visualisation and computational analysis enrich the\nempirical evidence\nEven though, as educated and expert listeners, musicolo-\ngists were able to perceive extremely ï¬ne details, visuali-\nsation and computational analysis conveyed empirical ev-\nidence which helped them to conï¬rm and prove aural ob-\nservations ( â€œThe tools on one hand, I donâ€™t need them, I\ncould describe that, on the other hand I canâ€™t prove it. This\ntool [Sonic Visualiser] is allowing me to express that in\nsome way it [the ï¬nding] is objective. â€ ). As put forward by\nCook [7], computational methods bring the potential for mu-\nsicology to be pursued as a more data-rich discipline. The\nobservations reported in Subsection 4.2.3 show the utility\nof multiple sources of information to analyse music perfor-\nmance practices. Visualisations and quantitative data re-\ntrieved through signal measurements were helpful in dis-\ncussing, interpreting, or proving hypotheses about qualita-\ntive data collected through aural observations. Furthermore,\nthese analyses enabled systematic comparison of the mu-\nsical expression of various performers in different musical\npieces (e.g. measurement of the rate and extent of the vi-\nbrato on long sustained notes based on spectrogram analy-\nses) and led to explanations of expressive techniques which\ncould not be reached through aural observations alone ( â€œYou\ncan only hear the pitch aspect of the vibrato as an educated\nlistener with no software or technology. â€ ).\n6.2 Cross-modal effects exist between auditory and\nvisual feedback\nMany of the examples given in Subsection 4.2.3 also show\nthat the visualisation and listening processes (either at the\noriginal speed or using slowed playback) affect each other.\nFor example, the spectrogram helps to hear vibrato much\nbetter, the slowed playback of a tone helps to uncover that\nthe vibrato is not constant, while the spectrogram aids in\nunderstanding that the variation comes from the fact that the\nnote starts without vibrato. Hence, new empirical evidence\nemerges from the cross-modal effects between auditory and\nvisual feedback. Visualisation was described by one of the\nFellows as a â€œlearning processâ€ ( â€œNow Iâ€™ve seen the spec-\ntrogram, I can only hear it [the vibrato], itâ€™s there now ...\nin my understanding. â€). However, cross-modal effects be-\ntween auditory and visual feedback also raise a paradox: if\nvisualisation brings to the aural experience an â€œincreased\nemphasis on what you can seeâ€ , it concomitantly â€œdeem-\nphasises what you canâ€™t seeâ€ . Therefore the ear may dis-card relevant aspects when the eye focuses on a spectrogram\nrepresentation while listening. After performing analyses\nbased on spectrograms, one Fellow noted â€œI completely for-\ngot about the bassoon, it feels like it is unimportant now,\nbut I was once struck by it. â€ . For this reason, being able to\nlisten to a musical piece at ï¬rst without visuals was deemed\nto be important, otherwise visualisation may â€œirreversibly\nedit stuff out of your brain that you canâ€™t seeâ€ . The design-\ners and users of music feature visualisation software need to\nbe aware of cross-modal interactions which might affect the\nobjectivity of their observations [8].\n6.3 Software for musicologists should support closed\nand multimodal listening practices\nWe suggest that software designed for assisting musicolo-\ngists in their analyses of recordings should be in line with\ntheir listening practices by supporting both closed and mul-\ntimodal listening. Due to the cross-modal effects mentioned\nin the previous section, it would be helpful for the user in-\nterface ï¬rst to provide a closed listening mode without visu-\nals, and then offer the possibility of switching to a more ad-\nvanced listening mode offering multimodal feedback. The\nmultimodal mode should link the music documents and rep-\nresentations using aural, visual, textual, and symbolic in-\nformation (see Subsection 4.2). Different software or user\ninterfaces may be needed to handle primary (e.g. scores and\nsound visualisations) and secondary (e.g. music biographies)\ninformation sources.\nOne way of providing textual and visual information re-\nlated to a recording (e.g. metadata, pictures) is via semantic\nweb technologies. Linked data offer promising ways to fa-\ncilitate the retrieval of metadata describing the recordings\n(date, album art covers, etc.) and the musicians (biogra-\nphies, photos, etc.). In addition to visualisations of acoustic\nparameters (see Subsections 4.2.3 and 6.1), the visualisation\nof scores and/or lyrics within the software would facilitate\nthe analysis of music recordings. Semantic web technolo-\ngies may also provide ways to retrieve scores from online\ndatabases directly from the audio player. Scores could then\nbe used as a reference to compute the performersâ€™ expres-\nsive deviations using content-based MIR techniques. The\nvisualisation of expressive deviations could help musicolo-\ngists to determine the extent to which expressive markings\nin the score are followed in the performance (see the note\nmentioned earlier: â€œSeems really consistent with markings\nin the score. â€ ), and to characterise the artistic intentions of\nthe conductor and/or performers.\nBased on the observations reported in Subsection 4.2,\nthe alignment of scores, lyrics or other time-based meta-\ndata to audio recordings could also aid performance prac-\ntice analysis, by facilitating multimodal listening and pro-\nviding better navigation of audio documents. For annota-\ntion of recordings, the inclusion of text editing function-\nality into analysis and playback software would be a wel-\n357Oral Session 5: User Studies\ncome feature, since musicologists generally write down ob-\nservations while listening. This could indeed be a means to\nconnect notes up with the actual point-in-time of the mu-\nsic which would ease further proof-reading or enrichment\nof the notes. Controlling the audio playback, with either the\nkeyboard or with a transcription foot pedal, would facilitate\ntasks such as the transcription of interviews from broadcast\nrecordings including speech and music, and avoid the con-\nstant switches between various computer software or differ-\nent devices which are time-consuming ( â€œItâ€™s so irritating\ntranscribing from a computer ï¬le because youâ€™re also try-\ning to write on the same computer, so you have to keep go-\ning into that program to move the recording back a bit, go\nback to the word program to type up that sentence more ac-\ncurately. So [...] if itâ€™s my ï¬le on my iPod, I can start and\nstop using a different device than the computer, or here Iâ€™m\nusing the CD player. â€ ).\n6.4 Can content-based MIR aid musicological study?\nSeveral areas of content-based MIR are relevant for musico-\nlogical purposes. For instance, automatic speech/music seg-\nmentation would help the navigation between spoken and\nmusic parts of documentaries and other broadcast material.\nSpeech recognition software would also be of considerable\nhelp to automatically transcribe interviews, enabling search\nof the non-music audio segments for conversations about\nspeciï¬c topics or musicians. Regarding the analysis of per-\nformance practices, automatic source separation techniques\ncould facilitate separate analysis of the musical expression\nof different performers or groups of performers (see Sec-\ntion 5.2). Variations of timbre are more difï¬cult to qualify\naurally than other variations such as in timing. Therefore\nMIR techniques improving timbre characterisation (e.g. at\nthe note level) and identiï¬cation of instrumentation or per-\nformers could help answer questions like: â€œIs that Janita\nusing some vibrato in the solos?â€\n7. CONCLUSION\nIn this paper, we presented and analysed ethnographic ob-\nservations of musicologists studying classical music record-\nings. The observed patterns revealed the importance of: (i)\nthe alternation of closed and multimodal listening modes;\n(ii) the use of visualisation and computational methods to\nprovide empirical evidence about listenersâ€™ impressions; (iii)\nscores and lyrics acting as a reference in performance analy-\nsis; and (iv) web sites and speech recordings supplying his-\ntorical and extra-musical information.\nThese ï¬ndings give clues regarding how to improve soft-\nware designed for musicologists. Such software should both\nsupport closed and multimodal listening, minimising dis-\ntractions and allowing the user to decide on the display of\nany feature visualisations during listening. The features ofinterest for computer-assisted musicology are those charac-\nterising artistic choices such as performersâ€™ expressive in-\ntentions (e.g. tuning, temperament, timing, pitch, timbre,\ndynamics, articulation and vibrato), most usefully displayed\nin conjunction with scores and lyrics. Content-based meta-\ndata soniï¬cation should be handled to facilitate the interpre-\ntation of the features (e.g. pitch). Interfaces managing the\nretrieval of contextual information (e.g. metadata, biogra-\nphies, articles, pictures) during multimodal listening would\nbeneï¬t the historical approach to musicology. Linked data\noffers a promising way to connect such extra-musical in-\nformation with the recordings by exploiting web resources\nsuch as the open music encyclopedia MusicBrainz3.\n8. ACKNOWLEDGMENTS\nThe authors wish to thank the Edison Fellows and the British\nLibrary for their kind participation and help during this study.\nThis study was conducted as part of the RCUK Digital Econ-\nomy project EP/I001832/1, Musicology for the Masses4.\n9. REFERENCES\n[1] J. Futrelle, and J. Stephen Downie: â€œInterdisciplinary\nCommunities and Research Issues in Music Information\nRetrievalâ€, Proceedings of the International Symposium\non Music Information Retrieval , 2001.\n[2] S.-J. Cunningham, N. Reeves, and M. Britland: â€œAn\nEthnographic Study of Music Information Seeking: Im-\nplications for the Design of a Music Digital Libraryâ€,\nProceedings of the 2003 Joint Conference on Digital Li-\nbraries (JCDLâ€™03) , 2003.\n[3] A. Bonardi: â€œIR for Contemporary Music: What the\nMusicologists Needsâ€, Proceedings of the International\nSymposium on Music Information Retrieval , 2000.\n[4] G. Gobo: â€œDoing Ethnographyâ€ , Sage, London, 2008.\n[5] R. Collins: â€œTheoretical Sociologyâ€ , Harcourt, San\nDiego, 1988.\n[6] R. M. Emerson, R. I. Fretz, and L. L. Shaw: â€œWriting\nEthnographic Fieldnotesâ€ , The University of Chicago\nPress, Chicago, 1995.\n[7] N. Cook: â€œComputational and Comparative Musi-\ncologyâ€, in Empirical Musicology: Aims, Methods,\nProspects , Oxford University Press, New York, 2004.\n[8] S. Dixon, W. Goebl and E. Cambouropoulos: â€œPercep-\ntual Smoothness of Tempo in Expressively Performed\nMusicâ€, Music Perception , V ol. 23, No. 3, pp. 195â€“214,\n2006.\n3http://musicbrainz.org/ andhttp://linkedbrainz.\nc4dmpresents.org/content/linkedbrainz-summary\n4http://www.elec.qmul.ac.uk/digitalmusic/m4m/\n358"
    },
    {
        "title": "Urgency Analysis of Audible Alarms in The Operating Room.",
        "author": [
            "Christopher Bennett",
            "Richard McNeer",
            "Colby Leider"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1416602",
        "url": "https://doi.org/10.5281/zenodo.1416602",
        "ee": "https://zenodo.org/records/1416602/files/BennettML11.pdf",
        "abstract": "Recent studies by researchers, governmental agencies, and safety organizations have recognized a deficiency in the performance of medically related audible alarms [1â€“4]. In the clinical setting, care providers can suffer from alarm fatigue, a condition in which audible alarms in an operating room are perceived as a nuisance. In this study, we explore the auditory features associated with current audible alarms using tools from the music information retrieval community, and then we examine how those auditory features correlate to listenersâ€™ perception of urgency. The results show that aperiodic changes in the auditory spectrum over time are the most salient contributor to the perception of urgency in sound. These results could inform the development of a novel standard regarding the composition of medical audible alarms.",
        "zenodo_id": 1416602,
        "dblp_key": "conf/ismir/BennettML11",
        "keywords": [
            "recent studies",
            "deficiency in performance",
            "medically related audible alarms",
            "alarm fatigue",
            "clinical setting",
            "care providers",
            "auditory features",
            "music information retrieval community",
            "listeners perception of urgency",
            "novel standard regarding the composition of medical audible alarms"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\n   URGENCY ANALYSIS OF AUDIBLE ALARMS                                 IN THE OPERATING ROOM Christopher Bennett Richard McNeer Colby Leider Anesthesiology Miller School of Medicine University of Miami cbennett4@med.miami.edu Anesthesiology Miller School of Medicine University of Miami mcneer@miami.edu Music Engineering Frost School of Music University of Miami cleider@miami.edu ABSTRACT Recent studies by researchers, governmental agencies, and safety organizations have recognized a deficiency in the performance of medically related audible alarms [1â€“4]. In the clinical setting, care providers can suffer from alarm fatigue, a condition in which audible alarms in an operating room are perceived as a nuisance. In this study, we explore the auditory features associated with current audible alarms using tools from the music information retrieval communi-ty, and then we examine how those auditory features corre-late to listenersâ€™ perception of urgency. The results show that aperiodic changes in the auditory spectrum over time are the most salient contributor to the perception of urgen-cy in sound. These results could inform the development of a novel standard regarding the composition of medical au-dible alarms. 1. INTRODUCTION The need to reevaluate audible medical alarms has been identified by several organizations, including The Joint Commission (www.jointcommission.org, the accreditation and certification organization of U.S. hospitals), the U.S. Food and Drug Administration (www.fda.gov), the Anes-thesia Patient Safety Foundation (www.apsf.org), and the American Society of Anesthesiologists (www.asahq.org). Serious errors associated with audible medical alarm per-ception have also received much recent press [1â€“4]. Previ-ous attempts have been made by the International Electro-technical Commission (IEC) to standardize these alarms by providing normative and informative guidelines [5]. These recommendations stipulated that alarms should consist of a series of pulsed tones, each forming a three-note melody.  Under IEC guidelines, melodies are reserved for one of several sentinel events for two levels of priority: caution-ary and emergency. Emergency status, which is meant to Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that cop-ies bear this notice and the full citation on the first page.  Â© 2011 International Society for Music Information Retrieval   engender a greater sense of urgency in the listener, is dif-ferentiated from cautionary status by simply lengthening and repeating the melodies and increasing the tempo. However, several previous studies have suggested that the-se IEC alarms are ineffective at conveying the appropriate level of urgency and are difficult to learn [6â€“8]. Worse yet, these alarms are often perceived as a nuisance in the in-traoperative environment, often leading to physiciansâ€™ manual silencing of audible alarms, which presents a po-tentially serious patient safety issue [9]. In this study, we sought to observe the acoustic features of audible alarms currently used in intraoperative environ-ments. Because alarm sounds vary by equipment manufac-turer, the current IEC-recommended alarms were used [5]. As mentioned, IEC alarms are differentiated by various short melodies. For example, a cardiovascular event is rep-resented by a major triad in first inversion and ventilation by a major triad in second inversion. (For a complete list of examples, see Table 1.) In addition to the IEC alarms, an experimental alarm set that incorporates additional audito-ry features besides melody and tempo was synthesized and used in this study for comparison.  Event Melody Description General 1âˆ’1âˆ’1 Ostinato Oxygenation 8âˆ’7âˆ’6 Falling pitch Ventilation 1âˆ’6âˆ’4 NBC chime Cardiovascular 1âˆ’3âˆ’5 Kumbaya Temperature 1âˆ’2âˆ’3 Major scale Drug Infusion 8âˆ’2âˆ’5 Quartal Perfusion 1âˆ’4#âˆ’1 Tritone Power 8âˆ’1âˆ’1 Octave Table 1. IEC alarms by source with melodic and descrip-tive annotations.   \n771Poster Session 6\n   The first objective of this study was to observe the primary salient auditory features of the IEC alarm set and to com-pare it to those of the experimental set. The researchers hypothesized that features would differ significantly be-tween the two sets because the IEC group uses only two auditory dimensions (melody and tempo), and the experi-mental group employs additional dimensions. The second objective was to investigate the acoustical correlates of ur-gency perception, thereby laying a foundation for a more robust and comprehensive audible medical alarm protocol. In this paper, we first present our experimental methods, followed by results and a discussion of the results. We conclude with a description of ongoing and future work. 2. METHODS 2.1 Stimuli Two sets of alarm sounds were used: IEC alarms and ex-perimental alarms. The experimental alarm set was synthe-sized by applying various audio effects to a pure-tone car-rier; we devised this second set of sounds in order to intro-duce additional auditory dimensions. These additional au-ditory effects included amplitude modulation, waveshap-ing, frequency modulation, phase randomization, and other basic sound synthesis and processing techniques. We con-structed a listening test in which alarm sounds were pre-sented to users in a random order; while listening to the alarms, users completed a simple questionnaire. Stimuli were presented once to the subject as 16-bit, 44.1-kHz WAV files and presented over two loudspeakers in a mu-sic-rehearsal space. Section 2.1 summary: IEC/ISO alarms were used as the control set and newly developed alarms as the experi-mental set.  These were played to subjects who completed a questionnaire about the alarm sounds. 2.2 Subjective Experimental Protocol A total of 21 undergraduate and graduate students in our music technology program were enrolled. Subjects were presented with eight IEC alarms and seven experimental alarms total.  Each IEC alarm was presented at both the cautionary and emergency levels, and each experimental alarm was presented at nine levels of auditory effect strength (20% to 100%, in steps of 10%). As an example of effect strength, consider amplitude modulation, in which the effect strength is determined by the modulation depth. These sounds can be found online at http://mue.music.miami.edu/soundSurvey. Subjects were asked to perceptually rate the alarms on a nine-point Likert scale [10] according to their perceived sense of urgency, anxiety, attention, and severity. These terms were prede-fined for subjects according to the definitions given in Ta-ble 2.   \nTable 2. Definitions of emotional descriptors used in the Likert questionnaire. Section 2.2 summary: Music students were asked to rate the IEC and experimental alarm sets on a Likert scale based on their perceived sense of urgency, anxiety, atten-tion, and severity.  2.3 Data Analysis Subject responses were first tested for normality by apply-ing a Lilliefors test. If the responses were tagged as being significantly different from normal distribution, then those responses were normalized using a power transformation to reduce intra-subject variability [11]. A power transform is rank-preserving but stabilizes the variance to make the distribution more Gaussian. This was useful for comparing responses across subjects. Next, inter-subject statistics were calculated to tag those responses that fell outside of Â±3 standard deviations from the inter-subject mean. These outliers were removed from further analysis. Finally, to deal with missing responses, the intra-subject mean was used in place of an empty element for statistical computa-tions.  Section 2.3 summary: Subjectsâ€™ emotional response ratings were normalized, then outliers were removed, and finally blank responses were filled in. 2.4 Auditory Feature Selection Auditory feature selection consisted of three primary phas-es: automatic feature extraction, automatic feature selec-tion, and informed feature parsing. Automatic feature ex-traction was conducted using MIRToolbox [12]. Each alarm was segmented using one-second frames, and we computed 25 common audio features for each frame. These features describe the dynamics, rhythm, spectral, timbral, and tonal characteristics of each frame. In order to make direct comparisons between features, the inter-frame statis-tics for each feature were analyzed instead of the feature Descriptor Definition Urgency A sense of requiring immediate action Anxiety A sense of apprehension due to uncer-tainty or doubt Attention A sense of drawing your focus or ob-servation Severity A sense of harshness or intensity \n77212th International Society for Music Information Retrieval Conference (ISMIR 2011)\n   vectors themselves. These statistics included mean, stand-ard deviation, slope, and periodicity.  After obtaining the feature statistics, automatic feature se-lection was performed. Features were parsed and included for consideration only if the following criteria were met: features were significant at p<0.05, features and urgency ratings had a correlation coefficient Ï>0.25, and linear re-gression of the features against the urgency ratings pro-duced a square correlation coefficient of R2>0.25. These minimum criteria ensured that the selected auditory fea-tures would be at least moderately and reproducibly corre-lated to the four perceptual dimensions of urgency, anxiety, attention, and severity. Features were then sorted using a statistical mapping between the subjectsâ€™ emotional ratings and the two sets of alarm sounds using methods developed by Lartillot, et al. [13]. Section 2.4 summary: Several auditory features were com-puted for each alarm sound. Next, these features for each alarm were compared to how that alarm was rated by the subjects on average. Features were removed if they were not significantly correlated to one of the emotional scales. The remaining features were ranked in order of correla-tion. 3. RESULTS 3.1 Subject Responses In our initial analysis of subject responses, we performed a squared multiple correlation to test for collinearity among the descriptor dimensions. The results showed that urgency was collinear (!!=0.95) with all of the other descriptors. As a result, the other descriptions (anxiety, attention, and severity) were removed from further analysis. Next, scale validity was confirmed by testing for inter-subject correlation (Ï) and Cronbachâ€™s Î±. In general, Ï in-dicates the degree of linear dependence across subjects and varies from â€“1 (anti-correlated) to +1 (perfect positive cor-relation), while Î± measures the internal consistency or agreement of a psychometric test score across a population, and it varies from 0 (inconsistent responses) to 1 (perfect consistency). A high degree of internal consistency be-tween subject responses was found, both in Ï (0.39) as well as Î± (0.92). Section 3.1 summary: The test methodology was validated by exhibiting consistent subject responses. â€˜Urgencyâ€™ was highly collinear with the remaining emotional scales, so the others were removed from further analysis.  3.2 Independent Analysis of Alarm Sets  After the combined subjective data were shown to exhibit internal consistency, we proceeded to analyze the features computed within the IEC and experimental alarm sets. The mirmap command from MIRToolbox was used to discov-er which of approximately 300 standard audio features (in-cluding statistics on feature vectors) showed strongly posi-tive or negative correlation to the mean urgency rating. Additionally, only features exhibiting statistical signifi-cance and sufficient independence (rxy>0.6) were selected. On the IEC alarm set, the only feature that correlated strongly to urgency was the magnitude of the spectral cen-troid periodicity (Ï=0.9). For the IEC alarm set, the cardi-ovascular alarm at the emergency level exhibited the strongest perceptual urgency and the highest rhythmic at-tack slope. Analysis of the experimental data set yielded a more robust and descriptive set of correlated features. Five features ex-hibited strong correlation to mean urgency rating: standard deviation and mean of the rhythmic attack slope (i.e., var-iation in the â€œtransientnessâ€ over time), entropy of the â€œma-jornessâ€/â€œminorness,â€ variation of the tonal centroid, and the mean spectral roughness. These correlations are illus-trated in Figure 1. Section 3.2 summary: IEC alarms exhibited one auditory feature correlated with urgency, and the experimental alarms exhibited five (as shown in Figure 1).  \n Figure 1. The five most significantly correlated features relating to perceived urgency in the experimental alarm set.  3.3 Post-Hoc Analysis of the Combined Alarm Set Data from the listening tests of each alarm set were then combined and analyzed together instead of separately to seek additional insights. We performed automatic feature selection by statistically mapping the combined set of fea-tures to the combined set of perceived urgency ratings. We found eleven features that met the criteria of Ï>0.25 and p<0.05. Next, R2 values were computed by determining how linearly the feature correlated to urgency. However, after removing those features with poor R2 values, only three features remained, as shown in Figure 2: periodic en-\n773Poster Session 6\n   tropy of spectral flatness (Ï=â€“0.56*, R2=0.45), standard deviation of tonal chromagram centroid (Ï=â€“0.49*, R2=0.32), and standard deviation of spectral irregularity (Ï=â€“0.36*, R2=0.31). Curiously, the three most correlated features in the combined data set were different than those reported by the analysis of each alarm set individually. However, of these, the standard deviation of spectral irreg-ularity ranked sixth in the IEC alarm set and fourth in the experimental alarm set. Section 3.3 summary: When the IEC and experimental alarm sets and responses were combined, it was found that three features correlated linearly with perceived urgency.  These three features were all related to the change of spec-tral characteristics over time.   \n Figure 2. Linear regression of auditory features against perceived urgency.  3.4 Multivariable Linear Regression Perceptual urgency responses were used to sort the alarms from lowest perceived urgency to highest perceived urgen-cy. The same sorting order was used for each of the three selected auditory features that were highly correlated to perceived urgency. Next, linear regression was calculated from these two data sets to produce a line of best fit (Fig-ure 2) for each feature. We performed multivariable linear regression to compute the vector of weighting coefficients, W, that best fits the equation !=!âˆ™!  (1) Here, u represents the mean inter-subject urgency rating vector, where u[0] is the collective mean rating of the first alarm, and so on for every alarm sound.  The vector f con-tains the three auditory features described in Â§3.3, whereby f T = {SFE, SIS, TCS}, and SFE is the spectral flatness en-tropy, SIS is the spectral irregularity standard deviation, and TCS is the tonal chromagram centroid standard devia-tion.  Performing a least-squares regression results in the follow-ing weighting coefficients yields  !={7.5,âˆ’2.8,âˆ’0.4}.  (2) Together, SFE and SIS account for 80% of the predicted urgency. These two features were plotted against perceived urgency in a 3D scatter plot, along with a mesh of predict-ed urgency, in Figure 3.  Section 3.4 summary: An equation for predicting perceived urgency was formulated based on the three most correlated features selected from a set of hundreds of features.  The weighting coefficients for those features, W, was deter-mined.  \n Figure 3. Perceived urgency (â€¢) and predicted urgency (â«) are shown. Urgency can be predicted based on the auditory features of inter-frame spectral irregularity standard devia-tion and inter-frame spectral flatness entropy.  4. DISCUSSION AND FUTURE WORK Our hypothesis that an experimental alarm set could be constructed that utilized more auditory dimensions was validated by independent objective and subjective analysis of the IEC alarm set and a new, experimental alarm set. The IEC set exhibited only one statistically significant fea-ture that correlated to user perception of urgency, whereas the experimental data set exhibited five. This indicates that a broader set of features can be considered when construct-ing audible alarms, thereby providing a larger number of â€œhandlesâ€ to manipulate when constructing new alarms.  This could be a useful tool in addressing the alarm prob-lem, as it has been suggested that more heterogeneity among alarms could improve identification of alarms [14]. \n77412th International Society for Music Information Retrieval Conference (ISMIR 2011)\n   Combined analyses of both data sets indicate that changes in an alarmâ€™s spectral features over time are the largest contributor to perceived urgency. In each case (periodic entropy of spectral flatness, standard deviation of tonal chromagram centroid, and standard deviation of spectral irregularity), the feature represents an inter-frame statistic. Furthermore, in each case, the feature is anti-correlated to urgency, indicating that fluctuating spectral content is a key to determining perceived urgency.  There are a large number of audible alarms present in the clinical arena, and many of these are false alarms.  This large number of false alarms leads to clinicians routinely ignoring them by suffering from alarm fatigue (a concept similar to listener fatigue) [15].  It has been suggested that improving the encoding of alarm information and reducing the number of false alarms could help reduce alarm fatigue [16].  This study provides the framework for establishing new audible alarms that do properly convey urgency, thus augmenting the transfer of information to the clinician. This study ties together concepts from music information retrieval, such as auditory feature selection, with auditory displays commonly found in a clinical setting. Leveraging these findings, we are currently working on a comprehen-sive syntax for operating room alarms that is able to con-vey multiple usable dimensions of data in addition to ur-gency. This might be done, for example, by mapping one auditory feature to urgency and another feature to indicate the alarm recipient (e.g., anesthesiologist, surgeon, nurse). Ultimately, we hope this work will lead to a robust alarm protocol that will minimize alarm fatigue in the operating room, thereby increasing patient safety. 5. REFERENCES [1] L. Kowalczyk: â€œGroups Target Alarm Fatigue at Hospitalsâ€”Accrediting Panel Finds Problem Worsening,â€ The Boston Globe 18 Apr, 2011. [2] Emergency Care Research Institute: â€œTop 10 Health Technology Hazards for 2011,â€ Health Devices Vol. 39, No. 11, pp. 389â€“390, 2010. [3] M.A. Olympio: â€œAPSF Workshop Recommends New Standards,â€ APSF Newsletter Winter, Vol. 19, No. 4, pp. 41, 52â€“53, 2005. [4] T. Swartz (ed.): â€œTechnology Overload: An Alarming Danger â€“ Preventing Alarm Fatigue To Improve Patient Safety,â€ Patient Safety Monitor Journal, Vol. 12, No. 3, pp. 9â€“10, 2011. [5] IEC 60601-1-8: â€œAudible Alarms in Medical Equipment,â€ International Electrotechnical Commission, Geneva, Switzerland, 13 Nov, 2006. [6] R.R. McNeer, J. BohÃ³rquez, Ã–. Ã–zdamar, A.J. Varon, and P. Barach: â€œA New Paradigm for the Design Of Audible Alarms That Convey Urgency Information,â€ J Clin Monitor Comp, Vol. 2, No. 6, pp. 353â€“63, 2007. [7] T.A. Mondor and G.A. Finley: â€œThe Perceived Urgency Of Auditory Warning Alarms Used in the Hospital Operating Room Is Inappropriate,â€ Gen Anesth, Vol. 50, No, 3, pp. 221â€“228, 2003. [8] A.N. Wee and P.M. Sanderson: â€œAre Melodic Medical Equipment Alarms Easily Learned?â€ Anesth Analg, Vol. 106, No. 2, pp. 501â€“508, 2008. [9] P.C. Beatty and S.F. Beatty: â€œAnaesthetistsâ€™ Intentions to Violate Safety Guidelines,â€ Anaesthesia, Vol. 59, No. 6, pp. 528â€“540, 2004. [10] R. Likert: â€œA Technique for the Measurement of Attitudes,â€ Psychology, Vol. 22, pp. 55, 1932. [11] G.E.P. Box, D.R. Cox: â€œAn Analysis of Transformations,â€ J Roy Statist Soc, Vol. 26, pp. 211â€“252, 1964. [12] O. Lartillot, P. Toiviainen, and T. Eerola: â€œA Matlab Toolbox for Music Information Retrieval,â€ in C. Preisach, et al. (eds.), Data Analysis, Machine Learning and Applications, Studies in Classification, Data Analysis, and Knowledge Organization, Springer-Verlag, New York, 2008. [13] O. Lartillot, T. Eerola, P. Toiviainen, and J. Fornari: â€œMulti-Feature Modeling of Pulse Clarity: Design, Validation, and Optimization,â€ Proceedings of the International Symposium on Music Information Retrieval, pp. 521â€“526, 2008. [14] J. Edworthy, E. Hellier, K. Titchener, A. Naweed, and R. Roels: â€œHeterogeneity in Auditory Alarm Sets Makes Them Easier to Learn,â€ Int J Ind Ergonomics, Vol. 41, pp. 136â€“146, 2011. [15] F. Schmid, M.S. Goepfert, D. Kuhnt, V. Eichorn, S. Diedrichs, H. Reichenspurner, A.E. Goetz, and D.A. Reuter: â€œThe Wolf is Crying in the Operating Room: Patient Monitor and Anesthesia Workstation Alarming Patterns During Cardiac Surgery,â€ Anesth Analg, Vol. 112, pp. 78â€“83, 2011. [16] J. Edworthy, E. Hellier: â€œFewer But Better Auditory Alarms Will Improve Patient Safety,â€ Qual Saf Health Care, Vol. 14, No. 3, pp. 212â€“215, 2005. \n775"
    },
    {
        "title": "The Million Song Dataset.",
        "author": [
            "Thierry Bertin-Mahieux",
            "Daniel P. W. Ellis",
            "Brian Whitman",
            "Paul Lamere"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.3860557",
        "url": "https://doi.org/10.5281/zenodo.3860557",
        "ee": "https://zenodo.org/records/3860557/files/msd_played_songs_essentia.csv.gz",
        "abstract": "This upload includes the ESSENTIA analysis output of (a subset of) song snippets from the Million Song Dataset, namely those included in the Taste Profile subset. The audio snippets were collected from 7digital.com and were subsequently analyzed with ESSENTIA 2.1-beta3. Pre-trained SVM models provided by the ESSENTIA authors on their website were applied.\n\nThe file msd_song_jsons.rar contains the ESSENTIA analysis output after applying the SVM models for highlevel feature extraction. Please note that these are 204317 files.\n\nThe file msd_played_songs_essentia.csv.gz contains all one-dimensional real-valued fields of the jsons merged into one csv file with 204317 rows.\n\nThe full procedure and subsequent analysis is described in\n\nFricke, K. R., Greenberg, D. M., Rentfrow, P. J.,  Herzberg, P. Y. (2019). Measuring musical preferences from listening behavior: Data from one million people and 200,000 songs. Psychology of Music, 0305735619868280.",
        "zenodo_id": 3860557,
        "dblp_key": "conf/ismir/Bertin-MahieuxEWL11",
        "keywords": [
            "ESSENTIA analysis output",
            "subset of song snippets",
            "Taste Profile subset",
            "audio snippets collected",
            "ESSENTIA 2.1-beta3",
            "pre-trained SVM models",
            "high-level feature extraction",
            "merged csv file",
            "one-dimensional real-valued fields",
            "204317 files"
        ]
    },
    {
        "title": "Methodology and Resources for The Structural Segmentation of Music Pieces into Autonomous and Comparable Blocks.",
        "author": [
            "FrÃ©dÃ©ric Bimbot",
            "Emmanuel Deruty",
            "Gabriel Sargent",
            "Emmanuel Vincent 0001"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1417611",
        "url": "https://doi.org/10.5281/zenodo.1417611",
        "ee": "https://zenodo.org/records/1417611/files/BimbotDSV11.pdf",
        "abstract": "The approach called decomposition into autonomous and comparable blocks specifies a methodology for producing music structure annotation by human listeners based on a set of criteria relying on the listening experience of the human annotator [12]. The present article develops further a number of fundamental notions and practical issues, so as to facilitate the usability and the reproducibility of the approach. We formalize the general methodology as an iterative process which aims at estimating both a structural metric pattern and its realization, by searching empirically for an optimal compromise describing the organization of the content of the music piece in the most economical way, around a typical timescale. Based on experimental observations, we detail some practical considerations and we illustrate the method by an extensive case study. We introduce a set of 500 songs for which we are releasing freely the structural annotations to the research community, for examination, discussion and utilization. 1",
        "zenodo_id": 1417611,
        "dblp_key": "conf/ismir/BimbotDSV11",
        "keywords": [
            "decomposition",
            "autonomous",
            "comparable",
            "blocks",
            "music",
            "structure",
            "annotation",
            "human",
            "listeners",
            "criteria"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\n  \nMETHODOLOGY AND RESOURCES \nFOR THE STRUCTURAL SEGMENTATION OF MUSIC PIECES \nINTO AUTONOMOUS AND COMPARABLE BLOCKS \n \nFrÃ©dÃ©ric BIMBOT  1 \nfrederic.bimbot@irisa.fr \n Emmanuel DERUTY 2 \nemmanuel.deruty@gmail.com Gabriel SARGENT 3 \ngabriel.sargent@irisa.fr \n Emmanuel VINCENT 2 \nemmanuel.vincent@inria.fr \nMETISS - Speech and Audio Research Group \n(1)  IRISA, CNRS - UMR 6074 - (2) INRIA, Rennes Bretagne  Atlantique â€“ (3) UniversitÃ© de Rennes 1 \nCampus Universitaire de Beaulieu, 35042 RENNES cede x, France \n \nABSTRACT \nThe approach called decomposition into autonomous and \ncomparable blocks  specifies a methodology for producing \nmusic structure annotation by human listeners based  on a set \nof criteria relying on the listening experience of the human \nannotator [12]. The present article develops furthe r a number \nof fundamental notions and practical issues, so as to facilitate \nthe usability and the reproducibility of the approa ch. \nWe formalize the general methodology as an iterativ e process \nwhich aims at estimating both a structural metric  pattern  and \nits realization , by searching empirically for an optimal com- \npromise describing the organization of the content of the mu- \nsic piece in the most economical way, around a typi cal time- \nscale. \nBased on experimental observations, we detail some practical \nconsiderations and we illustrate the method by an e xtensive \ncase study. We introduce a set of 500 songs for whi ch we are \nreleasing freely the structural annotations to the research com- \nmunity, for examination, discussion and utilization .  1 \n1.  INTRODUCTION \nGiven its numerous applications, the automatic infe rence of \nmusical structure is a key subject in MIR [1], whic h has been \nfocusing significant research effort in the past ye ars [2-10]. It \nhas also triggered several studies [11,12] and proj ects [13,14] \nsupporting this research with the investigation of methodo- \nlogical issues and the collection of annotated data . \nIn this context, the structural description approac h called de- \ncomposition into autonomous and comparable blocks  was \nrecently introduced [12] in terms of general concep ts, in- \nspired from structuralism and generativism. It has been de- \nsigned to be applicable to a wide range of â€œconvent ionalâ€ \nmusic, including pop music. \nThe present follow-up paper develops further this a pproach, \nwith the purpose of facilitating the usability and the reproduci- \nbility of the method. With hindsight resulting from  our own \nannotation experience and from reactions of fellow scientists \nto our first paper, this new contribution provides more practi- \ncal elements and a number of novel points in terms of prob- \nlem statement (section 2), introduction of a structural metric \n                                                           \nPermission to make digital or hard copies of all or  part of this work for per- \nsonal or classroom use is granted without fee provi ded that copies are not \nmade or distributed for profit or commercial advant age and that copies bear \nthis notice and the full citation on the first page .  \nÂ© 2011 International Society for Music Information Retrieval  \n pattern  as a central concept (section 3.4), reformulation of \nthe former concept of musical consistency  preservation (sec- \ntion 4.1), clarification of the notion of affixes ( section 4.3) \nand practical illustrations of the annotation proce ss (sections \n5.2 and 6). This paper also announces the release o f 500 an- \nnotated and partially commented music pieces in acc ordance \nwith the proposed conventions. \n2.  PROBLEM STATEMENT \n2.1  Levels of musical organization \nIt is commonly agreed that the composition and the percep- \ntion of music pieces rely on simultaneous processes  which \nvary at different timescales. Similarly to [15], we  consider \nthe three following levels corresponding to three d ifferent \nranges of timescales : \nâ€¢ the low-level elements which correspond to fine-gra in \nevents such as notes, beats, silences, etcâ€¦ We call  this \nlevel the acoustic level  and its time scale is typically be- \nlow or around 1 second. \nâ€¢ the mid-level organization of the musical content, based \non compositional units such as bars or hyper-bars o r on \nperceptual units such as musical cells and phrases,  rang- \ning typically between 1 and 16 seconds. We will ref er to \nthis level as the morpho-syntagmatic  level. \nâ€¢ the high-level structure of the musical piece, whic h de- \nscribes the long term regularities and relationship s be- \ntween its successive parts, and which we will call the lev- \nel of the semiotic structure , typically at a time scale \naround or above 16 seconds. \nThe figure of section 6 provides an illustration of  these three \nlevels. Note that we use the term semiotic  in a quite restricted \nscope, (compared for instance to that of Nattiez [1 6]) as de- \nnoting the high-level symbolic  and metaphoric  representation \nof musical content 1. \n2.2  Semiotic structure \nWhat we consider as the semiotic structure  of a music piece \nis something that may look like :  \nA B C D E F B C D E G D E D E H \n                                                           \n1 We thus avoid the term semantic , referring to some musical mean- \ning  of objects (for instance, chorus, verse, etcâ€¦) : s uch a notion \nfalls completely outside the scope of this paper. \n \n287Poster Session 2\n  \nthus reflecting : \n1)  some sort of high-level decomposition/segmentation of \nthe whole piece into a limited number of blocks (he re 16 \nblocks) of comparable size, and \n2)  some form of similarity or equivalence relationship  be- \ntween blocks bearing identical labels (here, 8 dist inct \nsymbols) \nProviding a semiotic description for a music piece requires \nprimarily the identification of the most adequate granularity  \n(block size and number of blocks) which then condit ions the \ninventory of labels. \nFrom the example below, choosing a finer granularit y could \nlead to a sequence of labels such as: \nAAâ€™BBâ€™CCâ€™DDâ€™EEâ€™FFâ€™BBâ€™CCâ€™DDâ€™EEâ€™GGâ€™DDâ€™EEâ€™DDâ€™EEâ€™HHâ€™ \nwhere any symbol X is systematically followed by symbol Xâ€™, \ntherefore yielding a rather redundant semiotic desc ription. \nConversely, a coarser granularity would require eit her the \nuneven grouping of the units into irregular  segments (i.e. of \nmore diverse sizes)  :  \nA  BC  DE  F  BC  DE  G  DE  DE  H \nor a very misleading representation such as : \nAB  CD  EF  BC  DE  GD  ED  EH \nwhich completely hides the similarities existing be tween por- \ntions of the piece which had identical labels at a lower scale. \nThis example thus illustrates a simple case where t here exist \nclearly a preferable granularity at which the semio tic level of \nthe music piece can be described with some form of optimal \ncompromise  between : \n- The minimality of the set of labels \n- The informativeness of the sequence of labels \n- The regularity of the block size \nThe goal of this work is to present a set of method ological \nprinciples for : \n1)  identifying the most appropriate granularity for de scrib- \ning the semiotic structure, and  \n2)  locating as univocally as possible the correspondin g \nblock boundaries. \nIn this article, the granularity referred to in ite m 1 is defined \nas the structural metric  of the music piece and the actual bor- \nders of the segmental units (item 2) as the realization  of the \nstructural meter.  \nThe proposed process relies on the listening of a m usic piec- \nes, but can be extended to music in written form (s cores). \nHowever, note that scores may not be available and some- \ntimes are even meaningless w.r.t. the type of music al content \nunder consideration. \n3.  BASIC CONCEPTS \n3.1  Definitions \nAs exposed in the previous section, the hypothesis of this work \nis that the semiotic structure of â€œconventionalâ€ mu sic pieces is \nbuilt on structural blocks , characterized by the content of their \nmusical layers. One of the aim of semiotic structur e annotation is therefore to locate the block boundaries  (with the convention \nthat they are synchronized with the first beat of a  bar). We call \nsize  the dimension of the blocks relative to a snap  scale propor- \ntional to that of the beat (see 3.3). \nWe call structural metric pattern , the underlying high-level \norganization of the musical content which is the mo st ade- \nquate for representing economically the semiotic le vel, and \nwe assume that block boundaries rest on the (potent ially ir- \nregular) realization  of that structural metric pattern. The an- \nnotation task thus consists in jointly inferring th e structural \nmetric pattern and its realization. \n3.2  Musical information layers \nEven though this is a simplified view of reality, w e consider \nthat a piece of music is characterized by 4 main reference \nproperties , potentially evolving over time 1 : \nâ€¢ intensity (amplitude / sound level) \nâ€¢ tonality/modality (reference key and scale) \nâ€¢ tempo (speed / pace of the piece) \nâ€¢ timbre (instrumentation / audio texture) \nWe also consider that a piece of music shows 4 main levels of \ntemporal organization :  \nâ€¢ rhythm (relative duration and accentuation of notes ) \nâ€¢ melody (pitch intervals between successive notes) \nâ€¢ harmony (chord progression) \nâ€¢ lyrics (linguistic content and, in particular, rhym es) \nThese levels of description form 8 musical layers 2.  \nBecause of their cyclic properties in conventional music, the \nlevels of temporal organization are central to the determina- \ntion  of block boundaries, in our approach. Indeed, as e x- \nplained in section 4.1, we assume that block bounda ries coin- \ncide with the convergence of cyclic behaviors takin g place \nsimultaneously in the 4 levels of temporal organiza tion. \nOn the opposite, blocks may globally differ in term s of inten- \nsity, tonality, tempo or timbre but these propertie s may hap- \npen to change within a block without corresponding to a \nstructural boundary.  \n3.3  Block size \nA primary property of blocks is their size, which w e describe \nin a custom unit that we call snap , and which is defined as the \nnumber of times a listener would snap his fingers t o accom- \npany the music, at a rate which is as close as poss ible to 1 bps \n(beat per second). As opposed to the beat (which is  a compo- \nsitional notion), the snap is a perceptual unit. \nAlthough we may come to consider the blocks from a variety \nof perspectives during their identification, their ultimate de- \nscription within the scope of this paper is their s ize in snaps. \nThe definition of the snap requires further consoli dation, \nsince a tempo-invariant unit would be desirable. Ho wever, an \nevolution of the definition of the snap would not a ffect the \nstructural segmentation per se , as the snap is only a measure \nof the block size. \n                                                           \n1 In previous work, we identified 3 reference propert ies only, but we \nconsider now that intensity  should also be part of the list.  \n2 These layers may not all be active simultaneously a nd some addi- \ntional layers may be observed in some music pieces.  \n \n28812th International Society for Music Information Retrieval Conference (ISMIR 2011)\n  \n3.4  Structural metric pattern \nA fundamental assumption of the proposed method is based \non the hypothesis that the semiotic structure can b e described \nin reference to a structural metric pattern , i.e. a prototypical \npartition of the beat or the snap scale. As an exam ple, a very \ncommon structural metric pattern is the repetition of blocks \nof 16 snaps (structural pulsation period Î¨ = 16) \nThe high-level structure of the music piece is gove rned by the \nstructural meter but actual semiotic blocks result from the re- \nalization  of the structural meter and this realization may l ead \nto blocks of irregular  size. For example, even if the structural \nperiod of a piece is equal to 16, the size of some blocks may \ndeviate from the prototypical value (for instance, 18). We de- \nvelop further the fact that, in a large number of c ases, irregu- \nlar blocks can be reduced to regular stems that conform to the \nstructural metric pattern.  \nThe structural metric pattern is analogous to the b ar, but op- \nerates at a higher level : whereas the bar is the o rganizational \nentity of low-level elements such as beats and note s, the \nstructural metric pattern governs the organization of mid-\nlevel elements (bars, cells, phases, etcâ€¦). \n4.  ANNOTATION CRITERIA AND NOTATION \n4.1  Detection of cycles (syntagmatic analysis) \nIn conventional music, the various temporal organiz ation lay- \ners tend to show (quasi-)cyclic behaviors, which we  define as \nthe recurrent return of the considered layer to som e specific \nstate or set of states 1. For instance, rhythmic patterns general- \nly show a short-term recurrence which participates to the \nmid-level organization of the music piece, melodies  tend to \nreturn to tonic or to exhibit particular intervals (depending on \nthe piece), specific chords sequences conclude harm onic pro- \ngressions (cadences), etcâ€¦ \nWe consider that, in conventional music pieces, the re exist \ntime instants for which the 4 levels of temporal or ganization \nexhibit some phase convergence  towards their respective \nends of cycles, which creates identifiable cues  of the piece \nstructure. In other words, block boundaries should corre- \nspond to some form of recurrent convergence of all levels of \ntemporal organization. \nThese instants of convergence take very versatile f orms, as \nthey can be signaled in the music content by very d iverse \ncombination of structuring cues , such as a particular rhyth- \nmic pattern combined with the return to a specific note or \nchord, the completion of a system of rhymes in the lyrics the \nconclusion of a carrure  and a recurrent sound effectâ€¦ \nEven though these cues and their combinations are p artly con- \nventional (at least within a particular music genre ), they gener- \nally vary from one piece to another and their ident ification is \npart of the empirical analysis conducted by the ann otator. \nIn our approach, cyclicity plays a central role for  identifying \nstructural blocks through the 2 ensuing properties : \n1)  iterability  : structural blocks can be looped to yield a \nconsistent (larger) musical stream \n2)  suppressibility  : structural blocks can be skipped in the \n                                                           \n1 Note that cyclic  does not necessarily mean periodic, the latter be-\ning a stronger property. For example, the zero-cros sing of a se- \nquence of values form a set of cycles which may not  be periodic.  music piece without creating the perception of a di scon- \ntinuity in the remaining musical stream \nIndeed, if one thinks of a periodic signal, each pe riod can be \nrepeated indefinitely and can be removed from the s ignal \nwithout disrupting seriously the organization of th e remain- \ning signal. This generalizes conceptually to quasi- cyclic pro- \ncesses, as defined above. \nThe property of cyclicity gives a founded ground fo r the syn- \ntagmatic definition of structural blocks. It establ ishes more \nclearly the criterion formerly based on the preserv ation of \nâ€œmusical consistencyâ€ [12] and also brings addition al sub- \nstance to the concept of Constitutive Solid Loop [1 1]. \nThe listenerâ€™s ability to identify iterable and sup pressible \nsegments in the music piece is a key point in the p roposed \nanalysis and it does not require the annotator to b e able to ex- \npress in musicological terms the actual properties of the \nstructuring cues. \nWhen necessary, the analysis can be complemented by  an \nexplicit designation of the structuring cues, but a ttention must \nbe paid that these cues should not be expected to b e univocal- \nly associated to blocks boundaries : all structurin g cues are \nnot systematically observed at all segment borders and some \ncues can also be observed within block boundaries.  \n4.2  Detection of similarities (paradigmatic analysis) \nThe identification of actual block boundaries is fu rther (or, in \npractice, simultaneously) carried out by performing  paradig- \nmatic analysis on the musical content, for reinforc ing and \ndisambiguating the set of candidate borders hinted by the de- \ntection of cyclic segments. \nIt consists in searching for â€œrepeatingâ€ patterns a cross the \nmusical content, which are identical, similar or, m ore gener- \nally speaking, easy to explain economically  relative to one \nanother (for instance, transposition, change in the  level of in- \nstrumental support, superimposition of a melodic mo tif, in- \nsertion of a musical segment, â€¦). \nAs for the syntagmatic analysis of section 4.1, the  locations \nof such paradigms do not coincide univocally with b lock \nboundaries : they only constitute additional cues o f such \nboundaries. \nNote that the paradigmatic analysis performed at th is stage \ncalls for similar processes to those that are neede d for label- \ning the segments. However, whereas the labeling sta ge re- \nquires the determination of a global system of cont rasts be- \ntween segments, the extraction of paradigmatic stru ctural \ncues simply requires pairwise  comparisons of musical seg- \nments for the only purpose of identifying and locat ing candi- \ndate blocks. \n4.3  Regularity and reduction \nFor many conventional music pieces, it can be assum ed that a \nmajority of blocks within the piece have a comparab le size in \nsnaps, hence corresponding to some structural pulsa tion peri- \nod (Î¨). Blocks whose size is equal to the structural pul sation \nperiod are called regular  blocks. \nSome blocks have a smaller size than Î¨, which can generally \nbe interpreted as corresponding to a shortened real ization of a \nregular block. This is especially true for half-siz e target seg- \nments, which can often be matched with the first or  second \n289Poster Session 2\n  \nhalf of a regular block observed somewhere else in the piece. \nAlternatively such blocks may be considered as a ha lf reali- \nzation of the structural metric (this is often the case for pre-\nchorus and bridges). \nIn a significant number of cases, blocks are longer  than the \nstructural period. However, in these cases, they ca n often be \nreduced into a stem  of size Î¨ and an affix . An affix is a subset \nof snaps which can be viewed as having been inserte d into a \n(regular) stem and affixes are therefore suppressib le from the \noriginal block (but not necessarily iterable), i.e. , the stem \nforms, on its own, an admissible block. If the inse rtion of the \naffix takes place at the beginning (resp. at the en d) of the \nblock, it is called a prefix (resp. suffix).  \nAffixes are particularly easy to identify and locat e within a \nblock when there exist, somewhere else in the song,  another \nblock which corresponds to the realization of the s tem alone.  \nBut sometimes, the stem has to be hypothesized base d on \nmore subtle considerations, because it is not attes ted alone in \nthe piece (but, for instance, with a different affi x). \nFrequent examples of suffixes are observed when for  instance \na block is extended by lengthening the last snap ov er 2 more \nsnaps (resulting in some form of break), by doublin g the dura- \ntion values of the notes on the last 2 snaps of the  block or by \nrepeating the last 4 snaps twice (thus rendering an  insistence \neffect). Affixes within blocks can be more tricky t o detect, and \nmay take versatile forms, for instance the repetiti on of a p-snap \nsegment, a tonal excursion of a few snap or a segme nt with to- \ntally different properties from the rest of the blo ck. \nBy convention, prefixes and suffixes should be of m aximum \nsize equal to half of that of the block (preferably  strictly less) \nand they should not alter the harmonic valence  of the block, \ni.e. the harmonic properties at the block boundarie s \n4.4  Structural metric pattern notation \nTo describe the structural metric pattern, we use t he follow- \ning notation : \nn a constant stem size of n snaps throughout the piec e \n{n 1,n2}  2 stem sizes in the piece, n 1 and n 2, occurring in any \norder but in decreasing frequency (can be generaliz ed \nto more than 2 values)  \n(n 1,n 2)  a systematic alternance of stem sizes n 1 and n 2, starting \nwith n 1 (can be generalized to more than 2 values)  \nThese notations are superscripted with a star (n*, {n 1,n2}*, \netcâ€¦), if the piece contains only within-blocks irr egularities, \nor very few short blocks considered by the annotato r as non-\nrepresentative of the dominant structure of the pie ce (in par- \nticular, in intros, outros, re-intros, etcâ€¦). If re levant, the an- \nnotator can combine further the notations, for inst ance \n{16,(12,8)}, but these needs are quite exceptionalâ€¦  \nIn conventional pop music, the most common segmenta l struc- \nture is m x 16 * (m being the number of blocks, which is itself \nusually close to 16), but pieces from the genre blues  have usu- \nally block sizes based on 24 snaps. More complex pa tterns \nsuch as {16,12}, (16,8) or (16,16,8) happen to be o bserved. \n4.5  Block size notation \nFollowing are the corresponding notation convention s which \nwe use to designate the size of (realized) blocks, in reference \nto a structural pulsation period of n snaps: \n [n+p] Insertion of a p-snap suffix after stem  \n[p+n] Insertion of a p-snap prefix before stem  \n[n&p] Insertion of a p-snap infix (somewhere) inside stem \n[p-n] Omission of p snaps at the end of stem \n[-p+n] Omission of p snaps at the beginning of stem \n[n\\p] Omission of p snaps (somewhere) inside stem \n[n/2] Half-size block (undetermined place of missing half)  \n[x] Undeterminable size (usually owing to a lack of snap)  \nSometimes, two structural blocks may overlap over p  snaps, \nwhich we call block tiling . This is the case when the realiza- \ntion of a new block starts while the previous block s is still p \nsnaps before its final boundary and continues in th e mean- \ntime (for instance, in canons). It is also the case  when some \nsnaps function simultaneously as the end of a given block and \nthe beginning of the next one. The notation convent ion for \ntiling situations is : [n-p [p] â€“p+n]. \nNote that the internal structure of blocks could be  further \nspecified by decomposing the block size into sub-bl ocks ac- \ncording to paradigmatic properties within the block  (for in- \nstance 4x4 as the internal structure of a size 16 b lock), but \nthis goes beyond the scope of the current paper. \n5.  GENERAL METHODOLOGY \n5.1  Annotation process \nBased on the notions introduced in the previous sec tion, the \nannotation of a music piece X can be understood as an (em- \npirical) joint estimation task, namely the determin ation of  : \nâ€¢ The most likely structural metric pattern (M) for t he piece  \nâ€¢ The most likely decomposition of the piece into a s et of \nblocks (S), i.e. the realization of M. \nIn practice, the annotator proceeds iteratively as follows : \n1.  hypothetize  a structural period Î¨, or (more generally) a \nstructural metric pattern M from the listening of X  \n2.  (attempt to) decompose  X into blocks following M, by \nintroducing, if and only if necessary, irregulariti es (af- \nfixes, irregular blocks) so as to satisfy cyclicity  of \nblocks and to maximize similarities across blocks ( resp. \nsections 4.1 and 4.2). \n3.  consider  possible alternatives to Î¨ or M \n4.  if such alternative(s)  seem to be worth considering , re- \nturn to step 2 and test the new hypothesis \nThe understanding of step 2 is crucial to the propo sed meth- \nodology : at that stage, the annotator is actually trying to es- \ntimate the realization of M via the minimization of  the neces- \nsary distortion that M should undergo to make it ma tch the \nproperties of the actual musical content of X. \nUltimately, among various hypotheses for M and the corre- \nsponding decompositions, the annotator retains that  which \nseems globally more economical for describing the s emiotic \nlevel, i.e. the solution which results in a satisfa ctory com- \npromise between : \nâ€¢ the simplicity and typicality of the structural met ric \nâ€¢ the regularity of the decomposition \nâ€¢ the non-redundancy of successive blocks \nâ€¢ the closeness of the structural period(s) to a refe rence val- \nue (currently set to 15 seconds) \n29012th International Society for Music Information Retrieval Conference (ISMIR 2011)\n  \nFigure 1 : illustration of the case study of sectio n 6.     5.2  Hypothesizing the structural metric pattern \n5.2.1  A priori properties and typical values \nPrevious work [12] has put forward arguments based on the \nâ€œPredictive Information Contextâ€ (PIC) suggesting t hat an a \npriori  economical description of the structure of a music  \npiece is based on segments of typical length equal to the \nsquare root  \u0001\u0002âˆš\u0004  of \nthe length of the piece. In \nthe annex section, we pro- \npose complementary con- \nsiderations based on in- \nformation theory concepts, \nwhich strengthen this \npoint. \nWe assume that structural \nblocks of approximate size \nâˆš\u0004  happen to be a rea- \nsonable initial assumption  \nwhen estimating the struc- \ntural pulsation period. \nHowever, the actual analy- \nsis of the musical content \nmay lead to a final ( a pos- \nteriori ) result which devi- \nates significantly from this \ninitial guess. \nOn the basis of an average song length of 240 secon ds, âˆš\u0004 \nfalls in the range of 15.5 s. With a snap around 1 s, the size of \na block will therefore typically be of 16 snaps. He re again, \nthis property should only be considered an a priori  hypothe- \nsis (the one to start with). \nFrom these consideration, a canonical model  which summa- \nrizes all the a prioris  can be laid down : it consists in 16 \nblocks of 16 snaps of 1s each. For a given piece, t he structur- \nal metric pattern and its realization are thus sear ched as the \nminimal deviation from this canonical model , which enables \na structural description compatible with the musica l content. \n5.2.2  Estimating plausible snap and structural period(s) \nBy definition, the snap is the multiple of the beat  correspond- \ning to a duration as close as possible (in logarith mic scale) to \n1 s (in fact, it usually corresponds to the downbea t, but not \nalways). Identifying the snap is, in general, rathe r straight- \nforward from the listening of parts of the piece, p referably \naway from the beginning or the end, which may exhib it par- \nticular beat and tempo properties. Depending on the  type of \nbar, admissible intervals for the snap are  : [0.71 , 1.41] for \nbinary bars and [0.58, 1.73] for ternary ones (for more com- \nplex, odd bars, the snap can be unevenly alternatin g between \ndifferent numbers of beats). \nOnce the snap is determined, plausible values of th e structur- \nal pulsation period(s) are hypothesized by listenin g to the \npiece and considering in priority its most salient and steady \nparts : typically the chorus (if any), the developm ents of re- \ncurring motifs or phrases, the parts of the piece p erceived as \nhomogeneous, etcâ€¦ From these segments, the annotato r can \ngenerally infer rapidly one or two plausible values  of pulsa- \ntion period(s), from which he/she will start a more  compre- \nhensive analysis of the piece, looking for particul ar patterns \nand locating irregularities. Given the central role played by the canonical mode l, the \nvalue of 16 is usually investigated in priority, un less obvious \nevidence in the musical content direct the annotato r towards \nanother hypothesis (for instance, 24 in many pieces  of blues). \n6.  A CASE STUDY \nFigure 1 illustrates the analysis of song Genre 08 from the \nRWC database [17] (labeled as Rock ). Structural blocks are \ndepicted both as \ntheir span on the \nx-axis (time in \nsnap) and their \nheight on the y-\naxis (in log \nscale). Each \nblock is identi- \nfied by a distinct \nroman number. \nThe duration of \nthe song is 3â€™26â€ \n(including initial \nand final silenc- \nes) and the size \nof the song in \nsnaps is 200 \n(snap is almost \nequal to 1 s).  \nSegments IV, VII, XII and XIII present a clear para digmatic \nrelationship (chorus of this piece). Three of them last 18 \nsnaps but XII lasts only 16 snaps and can be consid ered as \nthe stem on which the three other blocks are built by length- \nening the harmonic content over the last 2 snaps. \nSegments II, V, X form a second paradigm, with the return to \ntonic as a clear (conventional) structuring cue. Be ing of size \n16, they are in line with the Î¨=16 hypothesis. An alternative \nhypothesis would be to consider them as the repetit ion of 2 al- \nmost identical (half-)blocks of 8 snaps, but i) thi s would need \nthe introduction of a second structural period, ii)  no occurrence \nof such a half-block alone  is observed in the song and iii) it \nwould split the rhyme pattern of block V.  \nSegments III, VI and XI constitute a third paradigm . Their raw \nform amounts for 14 snaps, but they can be describe d as a 4x4 \nsnap carrure  of the abab  type, whose last quarter has been trun- \ncated of the last 2 snaps, hence the notation 16-2,  This comforts \n(or at least does not contradict) the hypothesis Î¨=16. \nSegments I and IX are very similar, I being an inst rumental \nintro of 16 snaps and IX the second half of I, used  as an in- \nstrumental bridge (hence the notation -8+16). Final ly, VIII is \na solo, which conveniently lasts exactly 16 snaps. \nThe segmental structure of the piece is therefore c onsidered to \nbe 13 x 16 *, i.e. a basic 16-snap pattern realized 13 times wi th \na few within-block irregularities. Alternative opti ons could \nhave been 25 x 8*, but this would introduce much redundancy \nin the underlying semiotic description, since almos t all seg- \nments would be observed in systematical pairs, with out bring- \ning significantly down the number of irregular segm ents (only \nIX would thus become regular). A pattern such as (1 6,14,18) * \ncould be envisaged given the recurrence of this par ticular size \nsequence in II-III-IV and V-VI-VII but the existenc e of XII as \na 16-snap realization of the chorus just in between  XI and XIII \nmakes this complicated alternative a non-sustainabl e option. \n291Poster Session 2\n  \n7.  CORPUS DESCRIPTION \n7.1  RWC Pop set \nA first set of annotations is composed of the 100 s ongs from the \nRWC Popular Music database [17], written and produc ed for \nresearch purposes. Their structural annotations hav e been re- \nleased and used last year for the MIREX 2010 evalua tion [18] in \nstructural segmentation and since then, they have b een marginal- \nly revised. \nRWC Pop   100 titles  \n7.2  Quaero set \nThe Quaero set  is composed of 159 titles selected by IRCAM \nwhich are being used in the Quaero project [13] for  the eval- \nuation of music structure detection algorithms : \nQuaero 2009 Development set 20 titles \nQuaero 2009 Evaluation set 49 titles \nQuaero 2010 Evaluation set 45 titles \nQuaero 2011 Evaluation set 45 titles \nTotal   159 titles  \nThe average length of songs is approximately 4 minu tes. A \nsubset of 97 titles contains several pieces from th e same artists \n(see below). The remaining 62 titles correspond to 62 other \ndistinct artists. This corpus covers a large range of music gen- \nres but the vast majority of artists are American o r English. \nThe Beatles : 21 - Jedi Mind Tricks : 14 - Eric Cla pton : 11 \nPink Floyd : 9 - Queen : 8 - The Cure : 8 - D Angel o : 4 \nACDC, Black Sabbath, Buenavista Social Club and Sha ck : 3 \nEminem, F. Zappa, Madonna, M. Jackson and Plastikma n : 2 \n  \n7.3  Eurovision set \nThe Eurovision set  is currently composed of 124 titles, corre- \nsponding to the songs which participated to the sem i-finals \nand/or the final in years 2008, 2009 and 2010, in t heir studio \nversion (as recorded on the â€œofficialâ€ albums) : \n2008 (Belgrade) ref # 5 099921 699726 43 titles \n2009 (Moscow) ref # 5 099969 968020 42 titles \n2010 (Oslo) ref # 5 099964 171722 39 titles \nTotal   124 titles  \nEurovision songs are limited, to a 3â€™00â€ maximum du ration \nby the rules of the contest, and tend to show other  properties \n(including their structure) influenced by the conte stâ€™s format \nand to its target public. These titles however cove r a variety \nof languages and a diversity of sub-genres within Euro-pop . \n7.4  Ongoing effort \nAt the time of finalizing this paper, we are comple ting the \nannotation of the RWC Music Genre database (100 tit les) and \nwe intend to annotate shortly an additional set of titles, so as \nto reach a total of 500 annotated titles before the  end of 2011. \n7.5  Release \nAll the aforementioned annotations are available at  : \nhttp://musicdata.gforge.inria.fr  \nand on an experimental web site where some of the d ata are \naccompanied with comments and which offers the poss ibility \nof consulting and debating the proposed annotations  : \nhttp://metissannotation.irisa.fr \n 8.  CONCLUSIONS \nThe work presented in this paper constitutes a cont ribution \ntowards the general strategic goal of defining, bui lding and \ndisseminating consistent re-usable resources for re search and \ndevelopment in MIR. It proposes operational concept s, con- \nsistent procedures and freely available data for th e descrip- \ntion of music structure. \nOur current work direction is to consolidate connec tions be- \ntween music structure description and information t heory, so \nas to encompass a wider range of concepts and, in p articular, \nto integrate several timescales in the structural d escription.  \n9.  REFERENCES  \n[1] Paulus J, Muller M., Klapuri A., â€œAudio-based m usic structure \nanalysisâ€, Proc. ISMIR 2010. \n[2] Peeters, G. â€œDeriving Musical Structures from S ignal Analysis for \nMusic Audio Summary Generation: Sequence and State Approachâ€, \nin Lecture Notes in Computer Science, Springer-Verl ag, 2004. \n[3] Abdallah, S. Noland, K., Sandler, M., Casey, M.,  and Rhodes, C. \nâ€œTheory and evaluation of a Bayesian music structure  extractorâ€, in \nProc. ISMIR, 2005. \n[4] Goto, M. â€œA Chorus Section Detection Method for Musical Au- \ndio Signals and Its Application to a Music Listenin g Stationâ€, IEEE \nTrans. on ASLP, 2006. \n[5] Paulus, J. and Klapuri, A. â€œMusic structure ana lysis by finding repeat- \ned partsâ€, in Proc. AMCMM, Santa Barbara, Californi a, USA, 2006. \n[6] Peeters, G. â€œSequence Representation of Music St ructure Using \nHigher-Order Similarity Matrix and Maximum-Likeliho od Ap- \nproachâ€, in Proc. ISMIR, Vienna, Austria, 2007. \n[7] Levy, M., Sandler, M. â€œStructural Segmentation of musical au- \ndio by constrained clusteringâ€,  IEEE Trans on ASLP , 2008. \n[8] Kelly C, Locating Tune Changes and Providing a S emantic La- \nbelling of Sets of Irish Traditional Tunes, Proc. I SMIR 2010. \n[9] Weiss R, Bello J, â€œIdentifying Repeated Pattern s in Music Using \nSparse Convolutive Non-Negative Matrix Factorizatio nâ€, ISMIR 2010. \n[10] Kaiser F, Sikora T, â€œMusic structure discovery  in popular mu- \nsic using non-negative matrix factorizationâ€, Proc.  ISMIR 2010. \n[11] Peeters G, Deruty E : Is Music Structure Annot ation Multi-\nDimensional ? LSAS, Graz (Austria) 2009. \n[12] Bimbot F, Le Blouch O, Sargent G, Vincent E. : D ecomposi- \ntion into Autonomous and Comparable Blocks. Proc. ISM IR 2010. \n[13] QUAERO Project :  http://www.quaero.org \n[14] SALAMI Project : http://salami.music.mcgill.ca  \n[15] Bob Snyder, Music and Memory, M.I.T. Press, 20 00 \n[16] Nattiez, J.-J. : Musicologie gÃ©nÃ©rale et sÃ©mio logie, 1987. \n[17] RWC : http://staff.aist.go.jp/m.goto/RWC-MDB \n[18] MIREX 2010 : http://www.music-ir.org/mirex/2010  \nANNEX \nLetâ€™s consider a song represented as a sequence of discrete elements \nat a given time-scale  \u0005\u0002\u0006\u0007\b\t\n\u000b\b\u000b\f\tand letâ€™s now consider a bi-\ndimensional organization of X into blocks of size  n, i.e. a \n\u000e\t(lines)Ã—\u0001\t(columns)  matrix representation of X : \n\u0005\u0002\u001b\u0007\u001c\u001d \u001e\n\u000b\u001c,\u001d\u000b ,!     with    \u000e\u0002\u0004/\u0001     and    #\u0002($âˆ’1)Ã—\u0001+( \nGiven this structure, the quantity of information n eeded to index all \nelements in the matrix requires : \n)!\u0002\u000elog 2\u000e+\u0001log 2\u0001\u0002\t\u0004\n\u0001log 2\u0004\n\u0001+\u0001log 2\u0001\t \nThus, the index of each line in matrix X can be coded with log 2 m \nbits, and the total number of bits required to inde x all lines in X is m \nlog 2 m (the same applies for the columns, hence n log 2 n). \nSeeking for the minimum of )! (by zeroing the derivative of  )! \nw.r.t. n) yields \u0001\u0002âˆš\u0004. \nHence, in the absence of any particular knowledge c oncerning the \nredundancies in X, the most economical way to index  it bi-\ndimensionally is to shape it as a â€œsquareâ€ matrix s tructure.  \n292"
    },
    {
        "title": "How Much Metadata Do We Need in Music Recommendation? A Subjective Evaluation Using Preference Sets.",
        "author": [
            "Dmitry Bogdanov",
            "Perfecto Herrera"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1415104",
        "url": "https://doi.org/10.5281/zenodo.1415104",
        "ee": "https://zenodo.org/records/1415104/files/BogdanovH11.pdf",
        "abstract": "In this work we consider distance-based approaches to music recommendation, relying on an explicit set of music tracks provided by the user as evidence of his/her music preferences. Firstly, we propose a purely content-based approach, working on low-level (timbral, temporal, and tonal) and inferred high-level semantic descriptions of music. Secondly, we consider its simple refinement by adding a minimum amount of genre metadata. We compare the proposed approaches with one content-based and three metadata-based baselines. As such, we consider content-based approach working on inferred semantic descriptors, a tag-based recommender exploiting artist tags, a commercial black-box recommender partially employing collaborative filtering information, and a simple genre-based random recommender. We conduct a listening experiment with 19 participants. The obtained results reveal that although the low-level/semantic content-based approach does not achieve the performance of the baseline working exclusively on the inferred semantic descriptors, the proposed refinement provides significant improvement in the listenersâ€™ satisfaction comparable with metadata-based approaches, and surpasses these approaches by the number of novel relevant recommendations. We conclude that the proposed content-based approach refined by simple genre metadata is suited for music discovery not only in the long-tail but also within popular music items.",
        "zenodo_id": 1415104,
        "dblp_key": "conf/ismir/BogdanovH11",
        "keywords": [
            "distance-based approaches",
            "explicit music tracks",
            "content-based approach",
            "low-level timbral",
            "temporal and tonal",
            "inferred high-level semantic",
            "genre metadata",
            "simple refinement",
            "metadata-based approaches",
            "novel relevant recommendations"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nHOW MUCH METADATA DO WE NEED IN MUSIC RECOMMENDATION?\nA SUBJECTIVE EVALUATION USING PREFERENCE SETS\nDmitry Bogdanov and Perfecto Herrera\nMusic Technology Group, Universitat Pompeu Fabra, Barcelona, Spain\n{dmitry.bogdanov,perfecto.herrera }@upf.edu\nABSTRACT\nIn this work we consider distance-based approaches to\nmusic recommendation, relying on an explicit set of mu-\nsic tracks provided by the user as evidence of his/her music\npreferences. Firstly, we propose a purely content-based ap-\nproach, working on low-level (timbral, temporal, and tonal)\nand inferred high-level semantic descriptions of music. Sec-\nondly, we consider its simple reï¬nement by adding a mini-\nmum amount of genre metadata. We compare the proposed\napproaches with one content-based and three metadata-based\nbaselines. As such, we consider content-based approach\nworking on inferred semantic descriptors, a tag-based rec-\nommender exploiting artist tags, a commercial black-box\nrecommender partially employing collaborative ï¬ltering in-\nformation, and a simple genre-based random recommender.\nWe conduct a listening experiment with 19 participants. The\nobtained results reveal that although the low-level/semantic\ncontent-based approach does not achieve the performance\nof the baseline working exclusively on the inferred seman-\ntic descriptors, the proposed reï¬nement provides signiï¬cant\nimprovement in the listenersâ€™ satisfaction comparable with\nmetadata-based approaches, and surpasses these approaches\nby the number of novel relevant recommendations. We con-\nclude that the proposed content-based approach reï¬ned by\nsimple genre metadata is suited for music discovery not only\nin the long-tail but also within popular music items.\n1. INTRODUCTION\nMusic recommendation is a challenging topic in the Music\nInformation Research community. A rapid growth of digital\nmusic industry has led to vast amounts of music available\nfor easy and fast access. Nevertheless, ï¬nding relevant and\nnovel music is a difï¬cult task for listeners, especially in the\nsituation when new music appears every day. To fulï¬ll their\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proï¬t or commercial advantage and that copies\nbear this notice and the full citation on the ï¬rst page.\nc/circlecopyrt2011 International Society for Music Information Retrieval.needs, researchers and practitioners strive for better recom-\nmendation systems, which are able to facilitate music search\nand retrieval based on aggregated user proï¬les, or simple\nqueries-by-example speciï¬ed by users. To this end, im-\nprovements of suitable underlying user models and/or music\nsimilarity measures are necessary. Currently, the state-of-\nthe-art approaches to music recommendation exploit both\nmetadata information about music items (metadata-based\napproaches) and the information extracted from the audio\nsignal itself (content-based approaches). Moreover, there\nexist hybrid approaches utilizing both types of information.\nPossible metadata includes editorial information, social\ntags, and user listening/consumption behavior in form of\nlistening statistics, such as playcounts and artist charts, sell\nhistories, and user ratings. This information is found to be\neffective to provide satisfactory recommendations for users\nwhen dealing with popular music and operating on large\ncollaborative ï¬ltering datasets. Nevertheless, the disadvan-\ntages of using metadata lie in the long-tail and cold-start\nproblems [6]. A system may not have sufï¬cient and correct\nmetadata, including social tags, user ratings, or even edito-\nrial information, for unpopular items. This can signiï¬cantly\nlimit the quality of recommendations or even make them im-\npossible. Moreover, gathering such metadata requires time\nand a large user base, which complicates the workability of\nthe system on initial stages even for popular items.\nIn contrast, content-based information, extracted from\nthe audio itself, can be valuable to overcome these prob-\nlems as it can be used independently of the popularity of\nmusic items or availability of a user base. A number of re-\nsearch works exist on both content-based music similarity\nmeasures, or distances,1suitable for music recommenda-\ntion, and approaches to user modeling. Objective content-\nbased distances generally employ sets of low-level timbral,\ntemporal, and tonal descriptors and/or high-level descriptors\ninferred from the low-level ones [2, 4, 5, 16, 17, 20]. Dif-\nferent works evidence usefulness of high-level semantic de-\nscriptions employed in place of, or in addition to, low-level\nmusic descriptions in the task of assessing music similar-\nity [2, 4, 5]. There are also evidences that content-based\n1We will refer to any music similarity measure with a term â€œdistanceâ€.\n97Poster Session 1\napproaches can be close, or even comparable, to success-\nful metadata-based approaches in terms of the relevance of\nrecommendations [1, 3, 15], especially in the long tail. In\naddition to objective distances, their personalization accord-\ning to a concrete user is considered in some works. Al-\nternatively, there exist research on user models for music\nrecommendation which employ classiï¬cation into interest\ncategories using content-based information [8,10] or hybrid\nsources [19], apply distances starting from a set of preferred\nitems in a content-based vector space [3, 13], or propose\nmore complex hybrid probabilistic approaches [12, 21].\nOne of the problems of existing research on music rec-\nommendation lies in a difï¬culty to conduct comprehensive\nsubjective evaluations with real listeners. Up to our knowl-\nedge, few existing research works involve evaluations with\nreal participants, and they are signiï¬cantly limited by the\nnumber of participants [3,10] or by the number of evaluated\ntracks per approach [1, 14], being in a trade-off situation.\nIn the present work, we consider music recommendation\napproaches which are based on sets of music tracks explic-\nitly given by users as an evidence of their musical prefer-\nences (the henceforth called â€œpreference setsâ€ ). We focus\non content-based and hybrid approaches, striving for both\nrelevance and novelty of recommendations. It is important\nto highlight the novelty aspect, as the existing metadata-\nbased approaches working on collaborative ï¬ltering princi-\nples are known to have a drawback to produce recommen-\ndations already familiar to listeners [6]. We follow the re-\nsearch presented in [3,9] in another peculiarity of this work,\nnamely, using explicitly given preference examples. Such\nan explicit strategy was shown to capture the essence of\nusersâ€™ musical preferences being suitable for preference vi-\nsualization and distance-based music recommendation. Al-\nthough requiring additional user effort to provide a list of\npreferred tracks, this strategy does not require any â€œadapta-\ntionâ€ period, which is common to the cold-start prone sys-\ntems gathering implicit user information. Starting from this\nstrategy, we strive to improve distance-based approaches to\nmusic recommendation, working on content, evaluate them\nin comparison to metadata-based approaches on real listen-\ners, and understand to what extent metadata is necessary to\nmake a satisfactory music recommender.\nWe propose two distance-based approaches to music rec-\nommendation working on content-based and hybrid infor-\nmation (Section 2.1). Firstly, we consider a complex dis-\ntance combining a set of low-level (timbral, temporal, and\ntonal) and inferred high-level semantic descriptors. This\ndistance has been successfully evaluated in the task of ob-\njective music similarity [4], but it requires additional atten-\ntion in the context of music recommendation. Secondly,\nwe consider how a minimum amount of metadata can im-\nprove purely content-based recommendations, and propose\na ï¬ltering approach relying on single, but sufï¬ciently de-scriptive, genre tags to reï¬ne recommendations. We eval-\nuate these approaches against four baselines (Section 2.2).\nAs such, we consider a content-based distance working on\nsemantic descriptors, being a component of the proposed\ncomplex distance, and three approaches working purely on\nmetadata. We employ a semantic tag-based approach, which\noperates on artist tags obtained from the Last.fm2service,\nand a state-of-the-art commercial recommender on the ex-\nample of iTunes Genius ,3which relies on a collaborative\nâ€œwisdom of crowdsâ€. We also consider genre-based recom-\nmendations as the simplest metadata-based baseline. Char-\nacterization of subjects is presented in Section 3.1, while\nSection 3.2 explains the listening experiment instructions,\nstimuli and procedure. Section 3.3 presents and discusses\nthe evaluation results, and we conclude with general obser-\nvations and lessons learned from this study in Section 4.\n2. STUDIED APPROACHES\nTo provide recommendations from our music collection (the\nhenceforth called music collection ), the approaches we con-\nsider here apply distance measures from a set of tracks, given\nby the user as evidence of his/her musical preferences (a\npreference set ) to the tracks in the collection. In order to\ncreate such a preference set, the user is asked to gather a\nminimal set of music tracks, which he/she believes to be\nsufï¬cient to grasp or convey his/her musical preferences,\nand submit them in audio format (e.g. mp3) or by edito-\nrial metadata sufï¬cient to reliably identify and retrieve each\ntrack. The amount of required tracks is not speciï¬ed being\nleft to a decision of the user. We retrieve or clean the edito-\nrial metadata for all provided tracks by means of audio ï¬n-\ngerprinting4to be able to use metadata-based approaches.\nAs the source for recommendations, we employed a large\nin-house music collection, covering a wide range of gen-\nres, styles, and arrangements. This collection contains 68K\nmusic excerpts (30 sec.) by 16K artists with a maximum\nof 5 tracks per artist. For consistency, in our experiments\nwe assume each of the recommendation approaches to out-\nput 15 tracks by different artists (1 track per artist) not be-\ning present among the artists in the userâ€™s preferences set.\nTherefore, each approach applies an artist ï¬lter.\n2.1 Proposed Approaches\n2.1.1 Semantic/Low-level Content-based Distance\n(C-SEMLL)\nAs our ï¬rst proposed approach, we follow the ideas pre-\nsented in [4] and employ a complex content-based distance,\n2http://last.fm , all tags were obtained on March, 2011.\n3http://www.apple.com/itunes/features/ , all experi-\nments were conducted using iTunes 10.1.1.4 on March, 2011.\n4We used MusicBrainz service: http://musicbrainz.org/\ndoc/MusicBrainz_Picard .\n9812th International Society for Music Information Retrieval Conference (ISMIR 2011)\nwhich is a weighted combination of three components:\nâ€¢A Euclidean distance on a set of timbral, temporal, and\ntonal descriptors with a preliminary principle compo-\nnent analysis.\nâ€¢A timbral distance based on the Kullback-Leibler diver-\ngence between single Gaussian models of MFCCs.\nâ€¢A simple tempo distance, based on matches of BPM and\nonset rate values.\nâ€¢A semantic distance, working on a set of high-level se-\nmantic descriptors (genres, musical culture, moods, in-\nstrumentation, rhythm, and tempo) inferred by support\nvector machines (SVMs) from low-level timbral, tem-\nporal, and tonal features.\nThe latter semantic distance has been previously evaluated\nin the similar context of music recommendation based on\npreference sets [3], and was shown to surpass common low-\nlevel timbral approaches. The interested reader is referred to\nthe aforecited literature for further details about the descrip-\ntors used, the component distances, and their weighting.\nWe retrieve recommendations using this distance by the\nfollowing procedure. For each track Xin the userâ€™s pref-\nerence set (a recommendation source), we apply the dis-\ntance to retrieve the closest track CX(a recommendation\noutcome candidate) from the music collection and form a\ntriplet (X,CX,distance (X,CX)). We sort the triplets by\nthe obtained distances, delete the duplicates of the recom-\nmendation sources (i.e. each track from the preference set\nproduces only one recommendation outcome), and apply an\nartist ï¬lter. We return the recommendation outcome candi-\ndates from the top 15 triplets as recommendations. If it is\nimpossible to produce 15 recommendations due to the small\nsize of the preference set (less than 15 tracks) or the applied\nartist ï¬lter, we increase the amount of possible recommen-\ndation outcome candidates per recommendation source.\n2.1.2 Semantic/Low-level Content-based Distance Reï¬ned\nBy Genre Metadata (C-SEMLL+M-GENRE)\nWe consider the inclusion of metadata in purpose to reï¬ne\nthe recommendations provided by content-based methods\non the example of C-SEMLL. We strive to include the mini-\nmum amount of metadata, preferably being low-cost to gather\nand maintain, but however sufï¬ciently descriptive for effec-\ntive ï¬ltering. The experiments conducted in [3] point us\nto the fact, that simple genre/style tags can be a reasonable\nsource of information to provide recommendations superior\nto the common low-level timbral music similarity based on\nMFCCs. Therefore, we propose a simple ï¬ltering to expand\nthe C-SEMLL approach. We apply the same sorting proce-\ndure, but we solely consider the tracks of the same genre\nlabels as possible recommendation outcomes. Moreover,\nwe suppose that increasing the speciï¬city of genre tags to\ncertain amount (e.g. from â€œrockâ€ to â€œprog rockâ€) would in-crease the quality of ï¬ltering.\nTo this end, we annotate the music collection and the\nuserâ€™s preference set with genre tags. Such information can\nbe obtained for the music collections by manual expert an-\nnotations, from social tagging services, or can be already\navailable in the ID3 tags for audio ï¬les or in other meta-\ndata description formats generated on the music production\nstage. As a proof-of-concept, we opt for obtaining artist tags\nwith the Last.fm API to simulate manual single-genre anno-\ntations of each track. Last.fm provides tag information for\nboth artists and tracks. We opt for artist tags due to the fact\nthat track tags tend to be more sparse, generally more difï¬-\ncult to obtain, and can be insufï¬cient for the music retrieval\nin the long tail, and assign to the tracks the same tags that\nwere assigned to the artists.\nWe analyze a set of possible tags suitable for the music\ncollection. For each track, we select the Last.fm artist tags\nwith the maximum weight (100.0) and add them to the pool\nof possible tags for genre annotation (â€œtop-tagsâ€). We then\nï¬lter the pool deleting the tags with less than 100 occur-\nrences (this threshold was selected in accordance with the\ntop-tag histogram and the collection size) and blacklisting\nthe tags which do not correspond to genres (â€œ60sâ€, â€œ80sâ€,\nâ€œunder 2000 listenersâ€, â€œjapaneseâ€, â€œspanishâ€, etc.) We then\nrevise the music collection to annotate each track with a\nsingle top-tag. For each track, we consider the candidates\namong its artist tags, selecting the tags with the maximum\npossible weight, which are also present in the top-tag pool.\nIf there are several candidates (e.g. both â€œrockâ€ and â€œprog\nrockâ€ have weight 100.0 and are present in the top-tag pool),\nwe select the top-tag, which is the least frequent in the pool.\nThereafter, we annotate the tracks from the userâ€™s preference\nset in the same manner using the created pool. The idea be-\nhind this procedure is to select the most salient tags (top-\ntags) for the music collection, skip possible tag outliers, and\nannotate each track with the most speciï¬c of these top-tags\nkeeping the maximum possible conï¬dence level.\n2.2 Baseline Approaches\n2.2.1 Semantic Content-based Distance (C-SEM)\nAs our ï¬rst baseline, we employ a content-based distance,\nworking on a set of inferred high-level semantic descriptors,\nwhich was used as a component of the complex distance in\nthe C-SEMLL approach (see Section 2.1.1). Using this dis-\ntance, we retrieve recommendations with the same sorting\nprocedure as followed for the C-SEMLL approach.\n2.2.2 Artist Similarity based on Last.fm Tags (M-TAGS)\nAlternatively, we consider a metadata-based distance work-\ning on the artist level. We gather social tags provided by\ntheLast.fm API for the artists from the preference set and\nthe music collection. For each artist, the API provides a\n99Poster Session 1\nweight-normalized tag list with weights in the [0,100.0]in-\nterval. We select a minimum weight threshold of 10.0to\nï¬lter possibly inaccurate tags. We assign the resulting tags\nto each track in the preference set and the music collection.\nWe then apply the latent semantic analysis [11,18] to reduce\nthe dimensionality to 300 latent dimensions. We apply the\nPearson correlation distance [6] on the resulting topic space,\nand retrieve recommendations with the same procedure as\nfollowed for the C-SEMLL.\n2.2.3 Black-box Similarity by iTunes Genius (M-GENIUS)\nWe consider commercial black-box recommendations ob-\ntained from the iTunes Genius playlist generation algorithm.\nGiven a music collection and a query, this algorithm is capa-\nble to generate a playlist by means of the underlying music\nsimilarity measure, which works on metadata and partially\nemploys collaborative ï¬ltering of large amounts of user data\n(music sales, listening history, and track ratings) [1]. From\nthe preference set we randomly select 15 tracks annotated\nby artist, album, and track title information, sufï¬cient to be\nrecognized by Genius . For each of the selected tracks (a\nrecommendation source), we generate a playlist, apply the\nartist ï¬lter, and select the top track as the recommendation\noutcome. We increase the amount of possible outcomes per\nsource when it is impossible to produce 15 recommenda-\ntions.\n2.2.4 Random Tracks From the same Genre (M-GENRE)\nFinally, as the simplest and low-cost metadata-based base-\nline, we consider random recommendations relying on genre\ncategories of the userâ€™s preference set. We annotate the mu-\nsic collection and the userâ€™s preference set with genre labels\nby the same procedure as in the C-SEMLL+M-GENRE ap-\nproach (see Section 2.1.2). We randomly preselect 15 tracks\nfrom the preference set and for each of the tracks we re-\nturn a random track of the same genre label from the music\ncollection. Again, we increase the amount of possible rec-\nommendation outcomes per recommendation source when\nit is impossible to produce 15 recommendations.\n3. EVALUATION\n3.1 Subjects\nA total of 19 voluntary subjects (selected from the authorsâ€™\ncolleagues, their acquaintances and families) were asked to\nprovide their respective preference sets and additional in-\nformation, including personal data (gender, age, interest for\nmusic, musical background), and a description of the strat-\negy and criteria followed to select the music pieces. The\nparticipants were not informed about any further usage of\nthe gathered data, such as giving music recommendations.\nThe participantsâ€™ age varied between 26 and 46 ( Âµ= 33.72,\nÏƒ= 4.65). All participants showed a very high interest inmusic (rating with Âµ= 9.24andÏƒ= 1.01, where 0 means\nno interest and 10 means passionate). In addition, 17 par-\nticipants play at least one musical instrument. The number\nof tracks selected by the participants to convey their mu-\nsical preferences was very varied, ranging from 10 to 178\nmusic pieces ( Âµ= 67.26,Ïƒ= 42.53) with the median\nbeing 61 tracks. The time spent for this task also differed\na lot, ranging from half an hour to 60 hours ( Âµ= 6.22,\nÏƒ= 15.06) with the median being 2 hours. The strategy\nfollowed by the participants to gather preference sets var-\nied as well. Driving criteria for the selection of tracks in-\ncluded musical genre, mood, uses of music (listening, danc-\ning, singing, playing), expressivity, musical qualities, and\nchronological order. Taking into account this information,\nwe expect our population to represent music enthusiasts.\n3.2 Evaluation Methodology\nWe performed subjective listening tests on the 19 partici-\npants using our in-house music collection (see Section 2).\nOne recommendation playlist per each of the 6 considered\napproaches was generated for each participant. Each playlist\nconsisted of 15 tracks returned by the respected approach\nspeciï¬cs. Due to the applied artist ï¬lter, the playlists nei-\nther contained more than one track of the same artist nor\ncontained artists present in the preference set. We merged,\nrandomized, and anonymized all playlists. This allowed\nto avoid any response bias due to presentation order, rec-\nommendation approach, or contextual recognition of tracks\n(e.g. by artist names) by participants. Moreover, the par-\nticipants were not aware of the amount of recommendation\napproaches, their names and their rationales.\nA questionnaire was given for the subjects to express dif-\nferent subjective impressions related to the recommended\nmusic. A â€œ familiarity â€ rating ranged from the identiï¬cation\nof artist and title (4) to absolute unfamiliarity (0), with inter-\nmediate steps for knowing the title (3), the artist (2), or just\nfeeling familiar with the music (1). A â€œ liking â€ rating mea-\nsured the enjoyment of the presented music with 0 and 1\ncovering negative liking, 2 being a kind of neutral position,\nand 3 and 4 representing increasing liking for the musical\nexcerpt. A rating of â€œ listening intentions â€ measured pref-\nerence, but in a more direct and behavioral way than the\nâ€œliking â€ scale, as an intention is closer to action than just\nthe abstraction of liking. Again this scale contained 2 posi-\ntive and 2 negative steps plus a neutral one. Finally, an even\nmore direct rating was included with the name â€œ give-me-\nmore â€ allowing just 1 or 0 to respectively indicate a request\nfor, or a reject of, more music like the one presented. The\nusers were also asked to provide title and artist for those\ntracks rated high in the familiarity scale. The textual mean-\ning of the ratings was presented to the participants together\nwith the rating values.\n10012th International Society for Music Information Retrieval Conference (ISMIR 2011)\n3.3 Evaluation Results\nFirst, we manually corrected the familiarity rating when the\nartist/title, provided by the user, was wrong (hence a famil-\niarity rating of â€œ3â€ or, more frequently, â€œ4â€, was sometimes\nlowered to 1). These corrections represented less than 3%\nof the total familiarity judgments.\nConsidering the subjective ratings used and our focus on\nmusic discovery, i.e. relevant and novel recommendations,\nwe expect a good recommender system to provide high lik-\ning, listening intentions, and â€œgive-me-moreâ€ ratings for a\nmajority of the retrieved tracks and, most importantly, for\nlow-familiarity tracks. We recoded user ratings for each\nevaluated track into 3 main categories - hits,fails, and trusts\n- referring to the type of the recommendation. In the case of\nliking, hits were the tracks which received low-familiarity\nrating (<2) and a high ( >2) liking rating. Fails were the\ntracks having a low ( <3) liking rating. Trusts were the\ntracks which got a high familiarity ( >1) and a high ( >2)\nliking rating. We similarly recoded the intentions and â€œgive-\nme-moreâ€ ratings, and obtained three different recommen-\ndation outcome categories per recommended track. We then\ncombined the into a ï¬nal category requiring the coincidence\nof all three outcome categories in order to consider it to be\na hit, a fail, or a trust. Otherwise, the recommendation was\nconsidered as â€œunclearâ€ (e.g. when a track is a hit using the\nliking, but it is a fail by other two indexes), which, in total,\namounted to 20.4% of all recommendations. We excluded\nthese recommendations from further analysis.\nTable 1 reports the percent of each outcome category\nper recommendation approach. As we can see, the pro-\nposed C-SEMLL+M-GENRE approach yielded the largest\namount of hits (32.0%), followed by M-TAGS (29.7%) and\nM-GENIUS (28.2%). The C-SEMLL+M-GENRE was the\nonly (partially) content-based approach that provided con-\nsiderably large amount of successful recommendations. We\ncan evidence that inclusion of genre metadata improved the\namount of hits by 11% for the C-SEMLL, making its re-\nï¬ned version comparable to the metadata-based baselines.\nOn the other side, the M-GENIUS and M-TAGS approaches\nprovided the largest amount of trusts (18.3% and 10.6% re-\nspectively), while the rest of approaches yielded only scarce\ntrusts (5.3% for C-SEMLL+M-GENRE, the rest below 3%).\nTrusts, provided their overall amount is low, can be use-\nful for a user to feel that the recommender is understanding\nhis/her preferences [1,7]. Nevertheless, their amount should\nnot be excessive, especially in the use-case of music discov-\nery. Finally, we can see that all recommendation approaches\nprovided more than 33% of fails, which means that at least\neach third recommendation was possibly annoying for the\nuser. In order to test if the approach and the outcome are\nassociated (i.e. if certain approaches provide hit, fails or\ntrust percents that are statistically different than those pro-\nvided by other methods) we performed a chi-square test thatApproach fail hit trust unclear\nC-SEMLL+M-GENRE 41.9 32.0 5.3 20.8\nM-TAGS 38.9 29.7 10.6 20.8\nM-GENIUS 33.1 28.2 18.3 20.4\nM-GENRE 51.2 26.0 2.8 20.0\nC-SEM 53.3 23.9 2.8 20.0\nC-SEMLL 58.1 21.1 0.4 20.4\nTable 1 . Percent of fail, trust, hit, and unclear categories per\nrecommendation approach.\nFigure 1 . Means of liking and listening intentions ratings\nper recommendation approach.\nprovided support for that ( Ï‡2(15) = 131.5,p<0.001).\nIn addition, we conducted three separate between-subjects\nANOV As in order to test the effects of the recommendation\napproaches on the liking, intentions, and â€œgive-me-moreâ€\nsubjective ratings. The effect was conï¬rmed in all of them\n(F(5,1705) = 15 .237,p < 0.001 for the liking rating,\nF(5,1705) = 14 .578,p < 0.001 for the intentions rat-\ning, andF(5,1705) = 11 .420,p < 0.001for the â€œgive-\nme-moreâ€ rating). Pairwise comparisons using Tukeyâ€™s test\nrevealed the same pattern of differences between the ap-\nproaches, irrespective of the 3 tested indexes. It highlights\nthe following groups with no statistically signiï¬cant differ-\nence inside each group: 1) M-GENIUS, M-TAGS, and C-\nSEMLL+M-GENRE having the highest ratings, 2) C-SEM\nand C-SEMLL+M-GENRE, and 3) C-SEM, M-GENRE, and\nC-SEMLL having the lowest. Note, that these groups are\npartially intersected with the C-SEMLL+M-GENRE and C-\nSEM both belonging to two different groups. The mean lik-\ning and listening intentions ratings are presented in Figure 1.\n4. CONCLUSIONS\nWe have considered different distance-based approaches to\nmusic recommendation, working on content information and\nmetadata to generate recommendations from a set of music\n101Poster Session 1\ntracks explicitly provided by a user as an evidence of her/his\nmusical preferences. We proposed a complex content-based\nlow-level/semantic approach and its simple reï¬nement us-\ning genre labels as a minimum amount of metadata. We\nhypothesized that such single-genre information is consid-\nerably low-cost to gather and maintain meanwhile it is suf-\nï¬ciently descriptive for effective ï¬ltering.\nThe proposed approaches were evaluated against the four\nbaselines on a population of 19 music enthusiasts. Con-\nsidering purely content-based approaches, we did not ï¬nd\nany improvements over the baseline semantic recommender\nusing a complex low-level/semantic distance instead. This\nsuggests that such a complex distance, previously found to\novercome the semantic distance in the task of music simi-\nlarity, is not well suited for the music recommendation use-\ncase. Further study to reveal its nature will be necessary.\nNevertheless, the reï¬ning of the proposed complex distance\nby simple genre labels showed a signiï¬cant improvement.\nFurthermore, such a reï¬ned approach surpasses the consid-\nered metadata-based recommenders in terms of successful\nnovel recommendations (hits) and provides satisfying rec-\nommendations, comparable to these baselines with no sta-\ntistically signiï¬cant difference.\nThe conducted evaluation corroborates a similar study\npresented in [3], in which similar patterns of no statisti-\ncally signiï¬cant difference between a content-based seman-\ntic distance and a simple genre-based baseline were found.\nThe gap between both of them and commercial metadata-\nbased recommendations, partially exploiting collaborative\nï¬ltering data, was also shown there. We extend this results\nnow with the proposed reï¬ning approach making possible to\novercome such a gap. We may conclude that the proposed\napproach, operating on complex content-based distance, re-\nï¬ned by simple genre metadata is well suited for the use-\ncase of music discovery not only for the long-tail but also\nfor popular items.\n5. ACKNOWLEDGMENTS\nThe authors would like to thank all participants involved in the evalua-\ntion. This research has been partially funded by the FI Grant of Generalitat\nde Catalunya (AGAUR) and the Buscamedia (CEN-20091026), Classical\nPlanet (TSI-070100-2009-407, MITYC), and DRIMS (TIN2009-14247-\nC02-01, MICINN) projects.\n6. REFERENCES\n[1] L. Barrington, R. Oda, and G. Lanckriet. Smarter than genius? human\nevaluation of music recommender systems. In Int. Society for Music\nInformation Retrieval Conf. (ISMIRâ€™09) , pages 357â€“362, 2009.\n[2] L. Barrington, D. Turnbull, D. Torres, and G. Lanckriet. Se-\nmantic similarity for music retrieval. In Music Information Re-\ntrieval Evaluation Exchange (MIREXâ€™07) , 2007. http://www.music-\nir.org/mirex/abstracts/2007/AS barrington.pdf.\n[3] D. Bogdanov, M. Haro, F. Fuhrmann, E. G Â´omez, and P. Herrera.\nContent-based music recommendation based on user preference ex-amples. In ACM Conf. on Recommender Systems. Workshop on Music\nRecommendation and Discovery (Womrad 2010) , 2010.\n[4] D. Bogdanov, J. Serr `a, N. Wack, P. Herrera, and X. Serra. Unifying\nlow-level and high-level music similarity measures. IEEE Trans. on\nMultimedia , 13(4):687â€“701, 2011.\n[5] D. Bogdanov, J. Serr `a, N. Wack, and P. Herrera. From low-level to\nhigh-level: Comparative study of music similarity measures. In IEEE\nInt. Symp. on Multimedia (ISMâ€™09) , pages 453â€“458, 2009.\n[6] O. Celma. Music recommendation and discovery in the long tail . PhD\nthesis, UPF, Barcelona, Spain, 2008.\n[7] H. Cramer, V . Evers, S. Ramlal, M. Someren, L. Rutledge, N. Stash,\nL. Aroyo, and B. Wielinga. The effects of transparency on trust in and\nacceptance of a content-based art recommender. User Modeling and\nUser-Adapted Interaction , 18(5):455â€“496, 2008.\n[8] M. Grimaldi and P. Cunningham. Experimenting with music taste pre-\ndiction by user proï¬ling. In ACM SIGMM Int. Workshop on Multimedia\nInformation Retrieval (MIRâ€™04) , pages 173â€“180, 2004.\n[9] M. Haro, A. Xamb Â´o, F. Fuhrmann, D. Bogdanov, E. G Â´omez, and P. Her-\nrera. The musical avatar - a visualization of musical preferences by\nmeans of audio content description. In Audio Mostly (AM â€™10) , 2010.\n[10] K. Hoashi, K. Matsumoto, and N. Inoue. Personalization of user pro-\nï¬les for content-based music retrieval based on relevance feedback. In\nACM Int. Conf. on Multimedia (MULTIMEDIAâ€™03) , pages 110â€“119,\n2003.\n[11] M. Levy and M. Sandler. Learning latent semantic models for mu-\nsic from social tags. Journal of New Music Research , 37(2):137â€“150,\n2008.\n[12] Q. Li, S. H. Myaeng, and B. M. Kim. A probabilistic music recom-\nmender considering user opinions and audio features. Information Pro-\ncessing & Management , 43(2):473â€“487, 2007.\n[13] B. Logan. Music recommendation from song sets. In Int. Conf. on Mu-\nsic Information Retrieval (ISMIRâ€™04) , pages 425â€“428, 2004.\n[14] C. Lu and V . S. Tseng. A novel method for personalized music rec-\nommendation. Expert Systems with Applications , 36(6):10035â€“10044,\n2009.\n[15] T. Magno and C. Sable. A comparison of signal-based music recom-\nmendation to genre labels, collaborative ï¬ltering, musicological anal-\nysis, human recommendation, and random baseline. In Int. Conf. on\nMusic Information Retrieval (ISMIRâ€™08) , pages 161â€“166, 2008.\n[16] E. Pampalk. Computational models of music similarity and their appli-\ncation in music information retrieval . PhD thesis, Vienna University of\nTechnology, 2006.\n[17] T. Pohle, D. Schnitzer, M. Schedl, P. Knees, and G. Widmer. On rhythm\nand general music similarity. In Int. Society for Music Information Re-\ntrieval Conf. (ISMIRâ€™09) , pages 525â€“530, 2009.\n[18] M. Sordo, O. Celma, M. Blech, and E. Guaus. The quest for musical\ngenres: Do the experts and the wisdom of crowds agree? In Int. Conf.\nof Music Information Retrieval (ISMIRâ€™08) , pages 255â€“260, 2008.\n[19] J. H. Su, H. H. Yeh, and V . S. Tseng. A novel music recommender by\ndiscovering preferable perceptual-patterns from music pieces. In ACM\nSymp. on Applied Computing (SACâ€™10) , pages 1924â€“1928, 2010.\n[20] K. West and P. Lamere. A model-based approach to constructing music\nsimilarity functions. EURASIP Journal on Advances in Signal Process-\ning, 2007:149â€“149, 2007.\n[21] K. Yoshii, M. Goto, K. Komatani, T. Ogata, and H. G. Okuno. Hybrid\ncollaborative and content-based music recommendation using proba-\nbilistic model with latent user preferences. In Int. Conf. on Music In-\nformation Retrieval (ISMIRâ€™06) , 2006.\n102"
    },
    {
        "title": "Neo-Riemannian Cycle Detection with Weighted Finite-State Transducers.",
        "author": [
            "Jonathan Bragg",
            "Elaine Chew",
            "Stuart M. Shieber"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1416152",
        "url": "https://doi.org/10.5281/zenodo.1416152",
        "ee": "https://zenodo.org/records/1416152/files/BraggCS11.pdf",
        "abstract": "This paper proposes a finite-state model for detecting harmonic cycles as described by neo-Riemannian theorists. Given a string of triads representing a harmonic analysis of a piece, the task is to identify and label all substrings corresponding to these cycles with high accuracy. The solution method uses a noisy channel model implemented with weighted finitestate transducers. On a dataset of four works by Franz Schubert, our model predicted cycles in the same regions as cycles in the ground truth with a precision of 0.18 and a recall of 1.0. The recalled cycles had an average edit distance of 3.2 insertions or deletions from the ground truth cycles, which average 6.4 labeled triads in length. We suggest ways in which our model could be used to contribute to current work in music theory, and be generalized to other music pattern-finding applications.",
        "zenodo_id": 1416152,
        "dblp_key": "conf/ismir/BraggCS11",
        "keywords": [
            "finite-state model",
            "harmonic cycles",
            "neo-Riemannian theorists",
            "triads",
            "harmonic analysis",
            "high accuracy",
            "weighted finitestate transducers",
            "dataset",
            "Franz Schubert",
            "ground truth"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nNEO-RIEMANNIAN CYCLE DETECTION\nWITH WEIGHTED FINITE-STATE TRANSDUCERS\nJonathan Bragg\nHarvard University\njbragg@post.harvard.eduElaine Chew\nQueen Mary, University of London\nelaine.chew@eecs.qmul.ac.ukStuart Shieber\nHarvard University\nshieber@seas.harvard.edu\nABSTRACT\nThis paper proposes a ï¬nite-state model for detecting har-\nmonic cycles as described by neo-Riemannian theorists. Given\na string of triads representing a harmonic analysis of a piece,\nthe task is to identify and label all substrings correspond-\ning to these cycles with high accuracy. The solution method\nuses a noisy channel model implemented with weighted ï¬nite-\nstate transducers. On a dataset of four works by Franz Schu-\nbert, our model predicted cycles in the same regions as cy-\ncles in the ground truth with a precision of 0.18 and a re-\ncall of 1.0. The recalled cycles had an average edit distance\nof 3.2 insertions or deletions from the ground truth cycles,\nwhich average 6.4 labeled triads in length. We suggest ways\nin which our model could be used to contribute to current\nwork in music theory, and be generalized to other music\npattern-ï¬nding applications.\n1. INTRODUCTION\nThough signiï¬cant attention has been devoted to segmenta-\ntion and labeling algorithms for discovering chords [14, 16,\n19] and keys [4,16,18] in music scores, little work has been\ndone on automating higher-level music analysis. One reason\nfor the small body of research on this topic is that such anal-\nysis is highly subjective and relies heavily on musical intu-\nition. Another reason is that there are numerous methods of\nanalysis, which are often best suited to a particular corpus\nof music. We take a step toward bridging this gap between\nlabeling and higher-level analysis by tackling the problem\nof ï¬nding neo-Riemannian cycles in chord sequences using\na ï¬nite-state approach.\nNeo-Riemannian music theory [17] posits that harmonies\nare related by means of transformations, rather than a com-\nmon tonic. The theory deï¬nes three primary transformations\nP,L, andRthat operate over the set of 24 major and minor\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proï¬t or commercial advantage and that copies\nbear this notice and the full citation on the ï¬rst page.\nc/circlecopyrt2011 International Society for Music Information Retrieval.triads (assuming enharmonic equivalence). Each transfor-\nmation involves two triads that share two common tones.\nPtransforms a triad to its parallel major or minor triad, L\ntransforms a major triad to a minor triad whose root is four\nsemitones higher (and vice-versa), and Rtransforms a triad\nto its relative major or minor triad. A cycle is generated by\nobtaining a triad, and repeatedly applying an identical per-\nmutation of either LP,RP,LRP , orLRat least until the\noriginating triad is reached again. These cycles partition the\nharmonic space and give structure to certain musical works.\nWhen neo-Riemannian theorists analyze a musical work,\nthey locate a passage and identify harmonies that â€œpartici-\npateâ€ in a cycle. There are several motivations for automat-\ning this process. The ï¬rst is to attempt to formalize the task,\nand in the process arrive at a more rigorous deï¬nition and\nunderstanding of what constitutes a cycleâ€”and by exten-\nsion what musical judgements are made during an analysis.\nThe second is to facilitate a more comprehensive study of\nthese cycles than currently exists [3]. Computer-aided anal-\nysis could provide a critique of the theory itself, as well as\nshed light on other music theoretic issues.\nThe existence of insertions and deletions presents chal-\nlenges to accurately ï¬nding neo-Riemannian cycles. Sup-\nposeTnis the composition of ntransformations along a cy-\ncle. In theory, a cycle consists of a sequence of triads, such\nthat each successive triad is generated by a single T1trans-\nformation. In practice, inserted harmonies intermix with the\ntriads that participate in the theoretical cycle; and, triads\nin the theoretical cycle can be missing from the observable\ncycle due to the use of compound operations ( Tn, where\nn>1), or because the cycle is incomplete. On the surface,\nthis problem may appear best solved by string matching\nalgorithms. Approximate string matching algorithms [12]\ncan handle insertions and deletions, and some methods have\nbeen developed to search for multiple strings [2]. The main\nproblem with this approach is the representation of the search\nstrings.LPcycles, for instance, consist of all strings begin-\nning withLPLPLP orPLPLPL and continuing in like\nfashion, of which there are many. LPcycles alone partition\nthe set of triads into four distinct cycles, each of which has\nsix distinct originating triads and two directions of motion.\n399Poster Session 3\nIn contrast, a ï¬nite-state model facilitates the concise en-\ncoding of a cycle using transformations. It also enables us to\nrepresent transformational music theory in a visual and in-\ntuitive way. Speciï¬cally, we propose a noisy channel model\nto represent the task of ï¬nding an intended message (a cy-\ncle) given an observation sequence (of chords). Our imple-\nmentation of the model uses weighted ï¬nite-state transduc-\ners (WFSTs). [15] describes this method, as applied to the\nrealm of speech recognition.\nFinite-state transducers (FSTs) are used extensively in\nlanguage and speech processing [9], with potential applica-\ntions to music. WFSTs, which are used to represent proba-\nbilistic ï¬nite-state machines in speech processing [11], could\nbe used similarly in audio music processing. [10] uses WF-\nSTs in the task of audio music identiï¬cation as both an acous-\ntic model and a compact language model. Drawing on ef-\nforts in language processing that implement the noisy chan-\nnel model with WFSTs [13], our model is a novel applica-\ntion of this technique to symbolic music analysis.\nThe remainder of the paper is organized as follows. In\nSection 2, we formalize the problem statement and present\nthe noisy channel model. In Section 3, we describe the input\ndata, as well as the training and evaluation methods for our\nmodel. Finally, in Section 4 and Section 5, we present the\nresults of our experiment and discuss our conclusions.\n2. THE MODEL\nOur goal is to design a system that will accurately iden-\ntify and label all strings of harmonies corresponding to neo-\nRiemannian cycles in a music score. The input to the system\nis a string of triad labels representing a harmonic analysis,\nand the desired output is a version of that analysis with all\nmusically salient cycles demarcated and labeled.\n2.1 Problem Statement\nLetÎ£1be the alphabet consisting of symbols representing\nthe 24 enharmonically distinct major and minor triads, and\nletÎ£2={P,L,R}, the alphabet of basic neo-Riemannian\ntransformations. Also let Î£3={[,]}, an alphabet of special\ndemarcation symbols outside of Î£1andÎ£2. Now, suppose\nwis a string of symbols in Î£1, corresponding to a harmonic\nanalysis of a music score. The task is to identify exactly\nthe substrings of wthat correspond to neo-Riemannian cy-\ncles. These cycles should be labeled with the corresponding\ntransformations from Î£2and bounded by symbols from Î£3.\n2.2 Noisy Channel Model\nWe implement the proposed noisy channel model with a\ncascade of WFSTs. Each component of the noisy channel\nmodelâ€”a theory model, a noisy channel, and an observa-\ntion sequenceâ€”is encoded as an FST. For simplicity of im-plementation, we reverse the direction of the model. Our re-\nverse implementation is equivalent to the formal deï¬nition\ndue to the closure of FSTs under inversion.\nOur implementation is the composition\nScoreâ—¦ScoreEditâ—¦Cycles\nof FSTs representing chords in the observation sequence,\nchord edits in the noisy channel, and a model of (theoret-\nical) cycles, respectively. We use the OpenFst library [1]\nimplementation of FSTs and the Viterbi algorithm with the\ntropical semiring to calculate the path of lowest cost from\nScore toCycles . This scheme is appropriate to our transi-\ntions, which use weights rather than probabilities.\n2.2.1 Score\nScore is the FST over Î£1that represents the observation\nsequence. As shown in Figure 1, Score accepts and outputs\nexactly the string corresponding to our input data with no\npenalty. Its construction is simple to automate, since each\ntransition from the start state to the ï¬nal state corresponds\nto a triad in the input (in order). While we have not used this\ncapability, our model can accommodate multiple weighted\nanalyses of a piece, as shown in Figure 2.\nFigure 1 . The Score FST representing the score â€œC G C.â€\nFigure 2 . An FST representing a probabilistic encoding of\ntwo possible analyses of a hypothetical score.\n2.2.2 ScoreEdit\nScoreEdit is the FST that represents the noisy channel (in\nreverse). It transduces from Î£1toÎ£1âˆªÎ£3and is deï¬ned as\nScoreEdit =AddBracketsâ—¦TriadsEdit, (1)\nwhere AddBrackets andTriadsEdit are two smaller FSTs\ndescribed below.\nAddBrackets , shown in Figure 3, is a formatting step\nthat demarcates cycles by inserting non-overlapping pairs\nof brackets into the score. In order to prevent an excessive\nnumber of cycles, we associate a cost Bwith the insertion\n40012th International Society for Music Information Retrieval Conference (ISMIR 2011)\nof a bracket pair, denoted /epsilon1: [/B, meaning â€œDo not read an\ninput chord. Add a bracket, at cost B.â€ A transition labeled\nÎ£1: Î£1/0is shorthand for all possible transitions labeled\nÏƒi:Ïƒi/0such thatÏƒiâˆˆÎ£1.\nFigure 3 . The AddBrackets FST.\nBracketing cycles in this way enables TriadsEdit to per-\nform edits on the score that are sensitive to cycle boundaries.\nTriadsEdit operates over Î£1âˆªÎ£3and is deï¬ned as\nTriadsEdit =OutsideEditÂ·(OpenBracket\nÂ·InsideEditÂ·ClosedBracket\nÂ·OutsideEdit )âˆ—,(2)\nwhere OpenBracket andClosedBracket are simple two-\nstate FSTs that recognize the languages {[}and{]}, re-\nspectively. As shown in Figure 4, OutsideEdit is a single-\nstate FST over Î£1that deletes any number of triads (with\ncostX), andInsideEdit is a single-state FST over Î£1that\ndeletes, inserts, and reads any number of triads (with costs\nD,I, and 0, respectively). By construction of Equation (2),\nOutsideEdit operates only outside of cycles, InsideEdit\noperates only inside cycles, and zero or more cycles can oc-\ncur anywhere in the score. We describe a method of training\nthese weights (costs) in Section 3.2.\nFigure 4 . The OutsideEdit (left) and InsideEdit (right)\nFSTs.\n2.2.3 Cycles\nCycles is the FST from Î£1âˆªÎ£3toÎ£2âˆªÎ£3. It transduces\nneo-Riemannian transformations from the cycles and is de-\nï¬ned as\nCycles = (OpenBracketÂ·MapÂ·ClosedBracket )âˆ—\nâ—¦(OpenBracketÂ·Deï¬nitions\nÂ·ClosedBracket )âˆ—,(3)\nwhere Map andDeï¬nitions are the FSTs described below.Map transduces from Î£1toÎ£2and converts triads into\ntransformations. It has a start state with transitions to each\nof the 24 other states corresponding to the major and minor\ntriads. Each state corresponding to a triad is a ï¬nal state, and\nhas outgoing transitions to three other states according to P,\nL, andRtransformations. Whenever Map in the start state\nreads a triad corresponding to a particular state, it moves to\nthat state and outputs /epsilon1(with cost 0). From there, it is able\nto read successive triads and output the appropriate transfor-\nmation symbols. For clarity, Figure 5 shows only a portion\nofMap corresponding to an LRP cycle, which contains 6\nout of the 24 possible triads.\nFigure 5 . The Map FST (abbreviated).\nDeï¬nitions is the FST over Î£2that recognizes any de-\nï¬ned neo-Riemannian cycle. By construction, Equation (3)\nensures that one of those cycles occurs within each set of\nbrackets. Deï¬nitions is the union of all FSTs that represent\na desired cycle, like the one shown in Figure 6.\nFigure 6 . TheLPCycle FST. Each transition exiting the\nstart state is shorthand for the transitions and intermediary\n(non-ï¬nal) states necessary to transduce the labeled sub-\nstring to itself with zero weight.\n2.3 Generalizability\nOur model is highly generalizable, and could be adapted to\nrecognize various properties in a variety of music-theoretic\nsystems. One could deï¬ne new edit operations by modifying\n401Poster Session 3\nScoreEdit , incorporate other types of harmonies [6, 7] or\ntransformations [5,8], or change Map to accommodate other\nconceptions of harmonic distance [20]. One could envision\nusing our model to detect cycles in other music features such\nas rhythm (where the symbols might be durations rather than\nneo-Riemannian transformations), and patterns other than\ncycles.\n3. EXPERIMENT\n3.1 Input Data\nWe were able to obtain only a small quantity of input data\nfrom scores in the desired corpus of late Romantic music\nscores, to which neo-Riemannian analysis is typically ap-\nplied. Neither a dataset of harmonic analyses, nor a reliable\nway of automatically converting music scores into analyses\nis presently available. Thus, the ï¬rst author performed all\nanalyses manually prior to the automated analysis. Seventh\nand other extended chords were reduced to their underlying\ntriads, and vertical sonorities without a prominent major or\nminor triad identity were ignored.\nOur input data are selections from four works by Franz\nSchubert in which [17] identiï¬es LPandRPcycles. [17]\nanalyzes two LPcycles in the exposition of the ï¬rst move-\nment of the A major Piano Sonata, D. 959, one LPcycle in\nthe fourth movement of the G major Piano Sonata, D. 894,\noneLPcycle in the coda of the ï¬rst movement of the E-\nï¬‚at major Piano Trio, D. 929, and one RPcycle in the ï¬rst\nmovement of the C major String Quintet, D. 956. Since the\nfocus of this experiment is LPandRPcycles, we deï¬ne the\nDeï¬nitions FST to recognize either one. Given the small\nsize of our dataset, it was not necessary to perform the usual\ndeterminization and minimization algorithms to make the\nFSTs in our model time- and space-efï¬cient, respectively.\nIn order to describe and classify the cycles that comprise\nour ground truth, we identify properties of cycles that are\nvisible to our model. Let pbe the number of triads in an\nobservable cycle that are labeled with transformations, let o\nbe the number of triads that are not labeled (insertions), and\nletn=o+pbe the overall length. Also, let mbe the number\nof deletions, and let lbe the length of the shortest complete\ntheoretical cycle of the type being labeled (e.g. l= 7forLP\ncycles). Note that p+m=l, except for extended cycles,\nwherep+m>l . Table 1 shows o,m,p, andlfor each of\nthe cycles in our input data.\nWe also calculate two quantities in Table 1 that help us\nto classify cycles.o\no+pis the proportion of insertions rela-\ntive to the observable length, andm\nm+pis the proportion of\ndeletions relative to the length of the corresponding theoret-\nical cycle. We will use these two quantities, also graphed in\nFigure 8, to explain our results.Piece Measures o m p lo\no+pm\nm+p\nD. 959 (ex. 1) 28â€“36 9 4 5 7 0.64 0.44\nD. 959 (ex. 2) 82â€“103 24 0 9 7 0.73 0\nD. 894 154â€“160 21 3 4 7 0.84 0.43\nD. 956 233â€“250 9 2 7 9 0.56 0.22\nD. 929 585â€“612 9 0 7 7 0.56 0\nTable 1 . Cycles in the ground truth and their properties.\n3.2 Training Method\nTraining our model consists of setting four parameters: B,\nD,X, andI, which are the costs of bracketing cycles, delet-\ning chords inside cycles, deleting chords outside of cycles,\nand inserting chords, respectively (described in Section 2.2).\nWhile systems can be trained with musically-informed rules [19],\nwe calculate weights empirically. Our method involves set-\nting up a system of linear inequalities by determining the\nbehavior of our system over isolated strings of ntriads.\nTo privilege labeling a cycle of ntriads over deletion, we\nuse equations of the form\nB+oD+mI <nX. (4)\nTo privilege deletion, we would simply reverse the inequal-\nity. We generate instances of Equation (4) from a ground\ntruth labeling of a score by selecting each cycle and calcu-\nlatingo,m, andn. In order to prevent our system from\narbitrarily extending cycles it labels, we also require that\nD>X. (5)\nWe solve the resulting system by minimizing the objective\nfunctionB+D+I+X.\n3.3 Evaluation\nThe desired performance metric should measure the success\nof both segmentation and labeling of cycles.\nWe propose an evaluation method that uses global string\nalignment applied separately to each region in the score with\none or more overlapping cycles in either the ground truth or\nthe prediction. Since a string of transformations does not\nuniquely determine the underlying triads, we do not com-\npare those strings. Instead, we calculate the edit distance\nbetween the string of triads labeled with transformations\n(i.e. not insertions) in the prediction with the corresponding\nstring in the ground truth. Allowable edit distance opera-\ntions are insertion and deletion, like in our model. If a cycle\ndoes not exist in one labeling, the edit distance is simply the\ncost of deleting all symbols in the other string. This metric\nhas the property that segmentation errors are proportional to\npand noto; it is a measure of divergence in transformational\ncontent rather than overall observable content.\n40212th International Society for Music Information Retrieval Conference (ISMIR 2011)\nPiece 1 2 3 4 5 6 7 8 9 10 11 SnSpSt\nD. 959 5 460 5 4 16 20\nD. 894 8 10 6 8 10 22 32\nD. 956 6 5 9 7 07 6 5 0 45 45\nD. 929 6 5 6 7 8 7 7 8 4 7 2 2 65 67\nTable 2 . Alignment costs for each piece, broken down by\nregion. Bold formatting indicates that the region contains a\ncycle in the ground truth.\nThe evaluation score Stof a prediction is equal to the\nsum of all edit distances calculated as just described, i.e.\nSt=Sn+Sp, whereSnis the sum of all edit distance op-\nerations on regions with a cycle in the ground truth, and Sp\nis likewise deï¬ned on all other aligned regions. SnandSp\nmeasure in some sense the amount of â€œfalse-negativenessâ€\nand â€œfalse-positiveness,â€ respectively, in a prediction.\nWe use leave-one-out cross-validation on our four pieces\nof input data. Training for validation on D. 959, D. 956,\nand D. 929 each yielded weights I= 1,B= 1,D=\n1.0065 , andX= 1.0055 , and training for validation on\nD. 894 yielded weights I= 1,B= 1,D= 1.003, and\nX= 1.002. Table 2 shows a breakdown of performance by\naligned region for each score.\n4. RESULTS\nIn our experiment, we used the cycles analyzed by [17] as\nour â€œground truth.â€ If we deï¬ne successful retrieval of a\ncycle in the ground truth as prediction of a cycle in the\nsame aligned region, our model achieved precision and re-\ncall scores of 0.18 and 1.0. (The model predicted a cycle in\nevery aligned region containing a cycle in the ground truth.)\nThe cycles recalled from the ground truth, on average, had\nlengthp= 6.4and alignment score 3.2.\nOur choice of ground truth cycles impacted our preci-\nsion score and led to many predicted cycles in regions not\nanalyzed. Viewed as strings of harmonies, these predicted\ncycles are difï¬cult to distinguish from cycles in the ground\ntruth. In particular, our model predicted an RP cycle in\nmeasures 304â€“329 (aligned region 7) of D. 929 with dimen-\nsionso= 8,m= 2,p= 7, andl= 9, which almost exactly\nmatch the dimensions of the ground truth RP cycle in D.\n956 (see Table 1). We arrive at the conclusion that either\nthe ground truth is incomplete, or that other factors affect\ntheoristsâ€™ decisions on what constitutes a cycle.\nOur model also labels cycles on a more detailed level\nthan is often done in music analysis. In practice, theorists\noften describe transformations acting on a cluster of chords\nwith a prominent harmonic identity, rather than a particu-\nlar chord with that identity. By contrast, our model always\nlabels speciï¬c chords with transformations. Our evaluation\nmeasure does not penalize this type of over-speciï¬cation.Aligned region 5 of D. 956, which received one of two per-\nfect alignment scores, illustrates this point. In translating the\nanalysis in [17] to the ground truth labeling, the ï¬rst author\nselected the second D major chord shown in Figure 7 for\nparticipation in the theoretical cycle based on cadential and\ninversional information in the score. Our model selected the\nï¬rst D major chord instead, but was not penalized by con-\nstruction of our evaluation method.\nFigure 7 . Aligned region 5 of D. 956 (mm. 233â€“250), with\nground truth labels (curved connectors) and predicted labels\n(elbow connectors).\nWhile our model predicted a cycle in each aligned re-\ngion containing a cycle in the ground truth, misalignments\nof varying severity also occurred. The predicted cycles in\naligned region 11 of D. 929, aligned region 2 of D. 959,\nand aligned region 2 of D. 894 received increasingly large\nevaluation scores. These increasing scores reï¬‚ect the costs\nof identifying an extended cycle, a cycle with the desired\nharmonic content but opposite direction, and a cycle with\naltogether different harmonic content, respectively.\nIn order to understand why these cycles posed challenges\nto our model, consider Figure 8. Distance from the origin\ncorrelates with the alignment scores of these three cycles.\nIn addition, there seems to be a direct link between distance\nfrom thex-axis (corresponding to the relative number of\ndeletions) and poor performance. Tellingly, the three cycles\nwith the best scores (aligned region 4 of D. 959, aligned\nregion 5 of D. 956, and aligned region 11 of D. 929) are\nlocated on or near the x-axis, but not particularly near the\ny-axis, suggesting that the model is able to handle many in-\nserted triads, so long as there are few deletions. The two\nremaining cycles in the ï¬gure, located furthest from the x-\naxis, were more costly to align. Each consists of strictly\nT2transformations, resulting in many deletions. The ï¬nite-\nstate model is not in general well-equipped to reward regu-\nlarity in patterns, and in this case was not able to recognize\nregularity of motion within a cycle.\nTo view the complete set of musical excerpts and ex-\ntracted harmonic analyses, please visit http://www.\njonathanbragg.com/ismir2011 .\n5. CONCLUSION\nThis paper presents the essential design and performance\nof a ï¬nite-state approach to harmonic cycle detection. The\nmodel performed well on the task at hand: with access to\n403Poster Session 3\nFigure 8 . Plot of proportion of deletions vs. proportion of\ninsertions (data from Table 1).\nvery little music feature data, it predicted all cycles in the\nground truth, some with very high accuracy, and suggested\nother potentially viable cycles. As more harmonic analysis\ndata becomes available, it will be possible to do more exten-\nsive testing of the model, and to incorporate other features.\nIn its current form, the model could be used as a tool for\ntheorists, to propose potential cycles which might be ana-\nlyzed and catalogued, and ultimately contribute to a better\nunderstanding of cycles and neo-Riemannian theory. This\napproach is highly generalizable and can be applied to other\nkinds of pattern matching in music.\n6. ACKNOWLEDGEMENTS\nThis work was supported in part by the Harvard College\nProgram for Research in Science and Engineering and NSF\nGrant No. 0347988. Any opinions, ï¬ndings, and conclu-\nsions or recommendations expressed in this material are those\nof the authors, and do not necessarily reï¬‚ect those of Har-\nvard University or NSF.\n7. REFERENCES\n[1] C. Allauzen, M. Riley, J. Schalkwyk, W. Skut, and\nM. Mohri. OpenFst: A general and efï¬cient weighted\nï¬nite-state transducer library. In CIAA 2007 , vol-\nume 4783 of LNCS , pages 11â€“23. Springer, 2007.\nhttp://www.openfst.org .\n[2] R. Baeza-Yates and G. Navarro. Multiple approximate\nstring matching. In WADS 1997 , volume 1272 of LNCS ,\npages 174â€“184. Springer, 1997.\n[3] M. Bribitzer-Stull. The Ab-C-E complex: The ori-\ngin and function of chromatic major third collections\nin nineteenth-century music. Music Theory Spectrum ,\n28(2):167â€“190, 2006.\n[4] E. Chew. Regards on two regards by Messiaen: Post-\ntonal music segmentation using pitch context distancesin the spiral array. Journal of New Music Research ,\n34(4):341â€“354, 2005.\n[5] R. Cohn. Square dances with cubes. Journal of Music\nTheory , 42(2):283â€“296, 1998.\n[6] E. Gollin. Some aspects of three-dimensional â€œton-\nnetzeâ€. Journal of Music Theory , 42(2):195â€“206, 1998.\n[7] J. Hook. Uniform triadic transformations. Journal of\nMusic Theory , 46(1):57â€“126, 2002.\n[8] B. Hyer. Reimag (in) ing Riemann. Journal of Music\nTheory , 39(1):101â€“138, 1995.\n[9] M. Mohri. Finite-state transducers in language\nand speech processing. Computational Linguistics ,\n23(2):269â€“311, 1997.\n[10] M. Mohri, P. Moreno, and E. Weinstein. Efï¬cient and\nrobust music identiï¬cation with weighted ï¬nite-state\ntransducers. Audio, Speech, and Language Processing,\nIEEE Transactions on , 18(1):197â€“207, 2010.\n[11] M. Mohri, F. Pereira, and M. Riley. Weighted ï¬nite-state\ntransducers in speech recognition. Computer Speech &\nLanguage , 16(1):69â€“88, 2002.\n[12] G. Navarro. A guided tour to approximate string match-\ning.ACM Computing Surveys , 33(1):31â€“88, 2001.\n[13] R. Nelken and S. Shieber. Arabic diacritization using\nweighted ï¬nite-state transducers. Computational Ap-\nproaches to Semitic Languages , 8:79, 2005.\n[14] B. Pardo and W. Birmingham. Algorithms for chordal\nanalysis. Computer Music Journal , 26(2):27â€“49, 2002.\n[15] F. Pereira and M. Riley. Speech recognition by compo-\nsition of weighted ï¬nite automata. In Finite-State Lan-\nguage Processing , pages 431â€“453. MIT Press, 1996.\n[16] C. Raphael and J. Stoddard. Functional harmonic analy-\nsis using probabilistic models. Computer Music Journal ,\n28(3):45â€“52, 2004.\n[17] M. Siciliano. Neo-Riemannian Transformations and the\nHarmony of Franz Schubert . PhD thesis, University of\nChicago, 2002.\n[18] D. Temperley. Music and Probability . MIT Press, Cam-\nbridge, Massachusetts, 2007.\n[19] D. Temperley and D. Sleator. Modeling meter and har-\nmony: A preference-rule approach. Computer Music\nJournal , 23(1):10â€“27, 1999.\n[20] D. Tymoczko. Three conceptions of musical distance. In\nMathematics and Computation in Music , volume 38 of\nCCIS , pages 258â€“272. Springer, 2009.\n404"
    },
    {
        "title": "Musical Influence Network Analysis and Rank of Sample-Based Music.",
        "author": [
            "Nicholas J. Bryan",
            "Ge Wang 0002"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1415768",
        "url": "https://doi.org/10.5281/zenodo.1415768",
        "ee": "https://zenodo.org/records/1415768/files/BryanW11.pdf",
        "abstract": "Computational analysis of musical influence networks and rank of sample-based music is presented with a unique outside examination of the WhoSampled.com dataset. The exemplary dataset maintains a large collection of artist-to-artist relationships of sample-based music, specifying the origins of borrowed or sampled material on a song-by-song basis. Directed song, artist, and musical genre networks are created from the data, allowing the application of social network metrics to quantify various trends and characteristics. In addition, a method of influence rank is proposed, unifying song-level networks to higher-level artist and genre networks via a collapse-and-sum approach. Such metrics are used to help interpret and describe interesting patterns of musical influence in sample-based music suitable for musicological analysis. Empirical results and visualizations are also presented, suggesting that sampled-based influence networks follow a power-law degree distribution; heavy influence of funk, soul, and disco music on modern hip-hop, R&B, and electronic music; and other musicological results.",
        "zenodo_id": 1415768,
        "dblp_key": "conf/ismir/BryanW11",
        "keywords": [
            "Computational analysis",
            "musical influence networks",
            "sample-based music",
            "WhoSampled.com dataset",
            "artist-to-artist relationships",
            "sampled material origins",
            "song-by-song basis",
            "social network metrics",
            "quantify trends and characteristics",
            "collapse-and-sum approach"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nMUSICAL INFLUENCE NETWORK ANALYSIS AND RANK OF\nSAMPLE-BASED MUSIC\nNicholas J. Bryan and Ge Wang\nCenter for Computer Research in Music and Acoustics,\nDepartment of Music, Stanford University\n{njb, ge}@ccrma.stanford.edu\nABSTRACT\nComputational analysis of musical inï¬‚uence networks and\nrank of sample-based music is presented with a unique out-\nside examination of the WhoSampled.com dataset. The ex-\nemplary dataset maintains a large collection of artist-to-artist\nrelationships of sample-based music, specifying the origins\nof borrowed or sampled material on a song-by-song basis.\nDirected song, artist, and musical genre networks are cre-\nated from the data, allowing the application of social net-\nwork metrics to quantify various trends and characteristics.\nIn addition, a method of inï¬‚uence rank is proposed, unify-\ning song-level networks to higher-level artist and genre net-\nworks via a collapse-and-sum approach. Such metrics are\nused to help interpret and describe interesting patterns of\nmusical inï¬‚uence in sample-based music suitable for mu-\nsicological analysis. Empirical results and visualizations\nare also presented, suggesting that sampled-based inï¬‚uence\nnetworks follow a power-law degree distribution; heavy in-\nï¬‚uence of funk, soul, and disco music on modern hip-hop,\nR&B, and electronic music; and other musicological results.\n1. INTRODUCTION\nNetwork analysis has become a signiï¬cant tool for under-\nstanding the dynamics of complex systems. Social network\nanalysis, in particular, has increasingly garnered the atten-\ntion of researchers across sociology, computer science, and\nstatistics. Within the music information retrieval commu-\nnity, this has led to the creation of artist collaboration, rec-\nommendation, similarity, and inï¬‚uence networks.\nEarly music-based networks are found in Cano and Kop-\npenberger [1] and Cano et al. [2]. Similarity networks from\nvarious online data sources are constructed with results show-\ning the potential of how network analysis can help design\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proï¬t or commercial advantage and that copies\nbear this notice and the full citation on the ï¬rst page.\nc/circlecopyrt2011 International Society for Music Information Retrieval.recommendation systems. Further work of Jacobson [3, 4]\nand Fields [5,6] continued to show applications of automatic\nplaylist generation, artist community detection, musicology,\nand sociology. Most recently, Collins investigated what is\npresumably the ï¬rst computational analysis of musical in-\nï¬‚uence using web scraping, web services, and audio simi-\nlarity to construct inï¬‚uence graphs of a collection of synth\npop music [7]. The work outlines the difï¬culty of construct-\ning inï¬‚uence networks and motivates further investigation.\nFigure 1 . Visualization of Genre Flow. The size and opacity\nof a directed edge indicates the relative ï¬‚ow of samples from\none genre to another.\nThe musicological and sociological impact of musical in-\nï¬‚uence has considerable scope. Understanding how artists,\nmusical styles, and music itself evolves over time can help\nus understand the creative process of music-making. Over-\nall inï¬‚uence rank is also of considerable attraction, as music\ncritics continually create top artist or producer lists within\n329Oral Session 4: Web\npopular music (e.g. Rolling Stone Magazine). We present\nwork towards this goal by studying inï¬‚uence found within\nsample-based music.1Directed inï¬‚uence graphs are con-\nstructed using a dataset from WhoSampled.com [8], a music\nwebsite that chronicles sampling behavior via a community\nof contributors. Network analysis metrics and visualization\nsuch as Fig. 1 are employed on song, artist, and genre inï¬‚u-\nence graphs in an effort to gain musicological understand-\ning of the compositional act of sampling. In addition, a\nmethod of inï¬‚uence rank and analysis is proposed to help\nunify song-level networks to higher-level artist and genre\nnetworks via a collapse-and-sum approach. Empirical re-\nsults found on constructed network graphs suggest musical\ninï¬‚uence-based networks follow a power-law degree distri-\nbution; heavy inï¬‚uence of funk, soul, and disco music on\nmodern hip-hop, R&B, and electronic music; and various\nother anecdotal discussions of the unique corpus.\n2. UNIQUE DATASET\nThe dataset was provided in agreement with WhoSampled.com\nand provides 42,447 user-generated records of sampling, ex-\ncluding any entry involving cover song sampling. A base-\nline entry or sample of the dataset consists of a song-artist\ndestination (who sampled the musical material) and song-\nartist source (source of the musical material sampled). In\naddition, other meta-data is provided, including destination\nand source release year, collaborating artists, featured artists,\nproducers, genre, and part-sampled (i.e. vocals, drums, etc.).\nFor the purposes of this work, it is assumed that the large,\nhigh-quality dataset is a good representation of sampling be-\nhavior found within modern popular music and independent\nof any form of bias imposed by the user community. Labels\nof genre include hip-hop/R&B (H), electronic dance (E),\nrock/pop (P), soul/funk/disco (F), jazz/blues (J), reggae (R),\ncountry (C), world (W), soundtrack (S), classical (L), spo-\nken word (K), easy listening (Y), gospel (G), and other (O).\nThe part-sampled labels include: whole track (W), drum\nloop (D), bass line (B), vocals (V), hook (H), or other (O).\n2.1 Genre & Part-Sampled Trends\nTo understand the data, we ï¬rst take a look at the genre\nand part-sampled trends. The relative proportions of each\ngenre are plotted in Fig. 2. Hip-hop/R&B, electronic dance,\nrock pop, and soul/funk/disco are dominate sources of mu-\nsical samples, while hip-hop/R&B and electronic music are\ndominate destinations. The relative proportions and counts\nof each part-sampled are (W) 7.20% (3060), (D) 37.25%\n(15811), (B) 33.76% (14329), (V) 2.15% (913), (H) 17.25%\n(7321), (O) 2.39% (1013). Drum and bass components are\n1Within this work, sample-based music is deï¬ned as a musical work\nthat in borrows material from another musical source, whether it be a direct\nmanipulation of a recorded sound or less direct transcribed material.\nFigure 2 . Source (upper) and Destination (lower) Genre\nDistributions with Absolute Counts.\nFigure 3 . Visualization of Part-Sampled Flow. The size and\nopacity of a directed edge indicates the relative ï¬‚ow of part-\nsampled type to different genres.\nthe most dominant part-sampled followed by hook compo-\nnents.\nFig. 1 and Fig. 3 show more advanced visualizations em-\nphasizing the ï¬‚ow of inï¬‚uence between genres [9]. Node\nsize represents the destination proportions, while the directed\nedge opacity and thickness represent the conditional distri-\nbution of source genre given the destination genre. As seen,\nhip-hop/R&B consumes the most samples out of all genres,\nand within hip-hop/R&B most of the source material is from\nsoul, funk, and disco as well as prior hip-hop/R&B material.\nIn addition, it is also noticeable that electronic dance mu-\nsic more likely samples vocal material, while hip-hop/R&B\nmore likely samples an entire portion of a song.\nTo measure how homogeneous the source material is for\neach destination genre, it is useful to employ the concept\nofgenre entropy H , similar to discussions found in Jacob-\nson [3] and Lambiotte [10]. Within this work, genre entropy\nis deï¬ned as\nHgk=âˆ’/summationdisplay\ngjâˆˆÎ“Pgj|gklogPgj|gk, (1)\nwheregkis thekthgenre in the set of genres Î“andPgj|gk\nis the probability of source genre gjgiven the destination\ngenregk. If genregksamples only from a single other genre\ngj, the entropy will be zero. If destination genre gksam-\n33012th International Society for Music Information Retrieval Conference (ISMIR 2011)\nFigure 4 . Distribution of Unique Samples Over Time. All\nsamples (blue, solid), soul/funk/disco (magenta, triangles),\nhip-hop/R&B (red, plus), electronic dance (green, circle),\nrock pop (black, x) are shown across time for source mate-\nrial (upper) and destination material (lower).\nples uniformly from each source genre gj, the entropy will\nbe maximized. The genre entropy for the top ï¬ve destina-\ntion genres is shown in Table 1. The source samples used in\nGenre Entropy (bits)\nelectronic dance (E) 2.83\nrock pop (P) 2.745\nhip-hop/R&B (H) 2.356\nsoul/funk/disco (F) 2.242\nreggae (R) 2.129\nTable 1 . Genre Entropy For Popular Destination Genres.\nreggae music are the most homogeneous, while electronic\ndance music is the most heterogeneous. A closer look at\nelectronic music reveals a near equal split of source ma-\nterial from hip-hop/R&B, electronic music, rock/pop, and\nsoul/funk/disco with a slight preference towards the latter.\nSuch evidence suggests differences in the creative process\nof sampling between genres.\n2.2 Time-Based Trends\nInitial observations of time-based trends are found when we\nview the proportion of samples per year within each genre.\nThe trends can be viewed for both unique source and des-\ntination material normalized by the total instances of sam-\npling as shown in Fig. 4. Plotting unique instances of source\nand destination material indicates general trends within each\ngenre and eliminates the effect of a single popular sample\nswaying the proportions (as is the case without uniqueness\nenforced).\nThe general shape of the source material plot (upper) out-\nlines the musical time frame of each genre (in terms of sam-pled source material), showing a rough outline of the rise\nand fall of soul/funk/disco and the rise of hip-hop/R&B.\nThe general shape of the destination material (lower) out-\nlines the increased popularity of sampling and/or listener\ntrends within the WhoSampled.com user community. In-\nterestingly, there is a sharp decrease in sample-based mu-\nsic centered around 2003. While further investigation is re-\nquired, it interesting to note that this event directly coin-\ncides with the Recording Industry Association of America\n(RIAA) ï¬rst litigation on Internet piracy and music copy-\nright infringement [11]. Such legal policy would have cre-\nated a more conservative and limited view of the musical\npractice of sampling, thus signiï¬cantly affecting the music-\nmaking process.\n3. NETWORK ANALYSIS\nA discussion of network analysis, inï¬‚uence measures, and\nrank is presented with the motivation of observing how in-\ndividual songs, artists, and genres inï¬‚uence one another.\nComplex network analysis provides signiï¬cant tools for such\ncharacterization and begins with the formulation of a net-\nwork graph. A graph G= (N,E )is deï¬ned by a set of\nnodesNand edgesEor equivalently an adjacency matrix\nA. A weighted directed edge between node iandjis de-\nï¬ned via Aij=wijand0otherwise, where wijis the cor-\nresponding weight. For unweighted networks, all weights\nare either zero or one.\n3.1 Degree Distributions\nFor a ï¬rst general measure of how the music sample-based\nnetworks are constructed, degree centrality can be used to\nmeasure the inï¬‚uence from each node (song, artist, or genre)\nof a network. For a given node, the in- and out-degree\ncentrality is deï¬ned as the respective in or out edge counts\nnormalized by the total number of nodes |N|. The in- and\nout-degree distribution is then the proportion of degree k=\n1,2,3,...nodes and can be used to characterize the network.\nPower-law distributions f(k)âˆkâˆ’Î³are an important\nfamily of distributions. Such distributions promote the con-\ncept of preferential attachment and are referred to as scale-\nfree. To test the hypothesis that musical sampling follows\na power-law, we can construct an unweighted acyclic song\nnetwork using unique songs as nodes and sampling instances\nto create directed edges from destination to source. The\nin-degree distribution can then be computed and tested to\nfollow a power-law distribution or not. Using methods de-\nscribed in [12], we ï¬nd that the network is consistent with\nthe hypothesis (p-value =.16forkâ‰¥3andÎ³= 2.72) and\nshow the cumulative in-degree distribution in Fig. 5.\nIn terms of networks based on musical sampling, a scale-\nfree network suggests the idea that very popular samples\nwill only continue to increase in popularity. In addition, if\n331Oral Session 4: Web\nFigure 5 . Cumulative In-degree Distribution P(k)of the\nSample-Based Song Network (log-log scale).\nany of the very popular samples were to be removed, large\nportions of sample-based music would cease to exist (or at\nleast be altered).\n3.2 Inï¬‚uence Measures\nTo analyze and rank inï¬‚uence within each network, four\nclosely related measures are commonly used: degree cen-\ntrality, eigenvector centrality, Katz centrality, and PageR-\nank. Degree centrality does not capture any indirect form of\ninï¬‚uence (as in the case of sample chains), motivating alter-\nnative methods. Eigenvector centrality extends degree cen-\ntrality by weighting the importance of neighboring nodes to\nallow for indirect inï¬‚uence, but has limited application for\nacyclic networks [13].2Katz centrality and PageRank ap-\npropriately modify eigenvector centrality. Both also provide\na mechanism to capture indirect inï¬‚uence between nodes,\ncompute an overall inï¬‚uence rank among each node, and\nobserve the inï¬‚uence of one node to another. PageRank,\nhowever, down-weights inï¬‚uence created by a destination\nnode that samples more than once, or in the case of artist\nnodes, down-weights inï¬‚uence from artists with lengthy ca-\nreers. While this is desirable in numerous other contexts\nsuch as web search, we wish to equally weight each instance\nof sampling and restrict ourselves to Katz centrality.\nThe Katz inï¬‚uence matrix IKis deï¬ned via\nIK= (Iâˆ’Î±A)âˆ’1âˆ’I (2)\nwhereIis an identity matrix, Ais the adjacency matrix as\nbefore, andÎ±is a decay factor which scales the indirect in-\nï¬‚uence allowed to propagate though the network (larger Î±\nimplies greater weight on indirect inï¬‚uence). This can be\nwritten in equivalent form as\nIK=Î±A+Î±2A2+...+Î±kAk+..., (3)\nwhere we can see that the inï¬‚uence is a weighted sum of\nthe powers of the adjacency matrix [14]. When the values\nofAare zero or one, the powers of the adjacency matrix\n2The song network is exactly acyclic and the artist network is nearly\nacyclic.Akhave elements representing the number of sample chains\nof corresponding length kcapturing various levels of indi-\nrect inï¬‚uence. For stability, 1/Î±must be greater than the\nlargest eigenvalue of Aand for large networks, (2) becomes\nincreasingly difï¬cult to invert. Typically, only the overall in-\nï¬‚uence rank is desired and is computed iteratively in a fash-\nion to avoid a large memory footprint and matrix inverse\nrequired for IK.\nFor our purposes, it is desirable to have both the entire in-\nï¬‚uence matrix and overall rank. Given IK, we can view the\ncolumn of a node to ï¬nd who inï¬‚uenced the node, or view\nthe row of the node to ï¬nd who the node inï¬‚uenced [15].\nSumming the columns of the inï¬‚uence matrix produces a\nranking of the most inï¬‚uential nodes, while summing the\nrows results in a ranking of the most inï¬‚uenced nodes. Such\nanalysis is nicely suited for musicological analysis and mo-\ntivates further improvements discussed below.\n3.3 Collapse-and-Sum Inï¬‚uence Rank\nFor the given dataset, we would like to understand and ana-\nlyze song, artist, and genre inï¬‚uence individually, as well as\nhow each network relates to one another. To do so, individ-\nual networks can be constructed for song, artist, and genre\nnetworks with inï¬‚uence matrices and rank computed via (2)\nor (3). Building separate graphs, however, has several draw-\nbacks. Most notably, there is no straightforward mechanism\nto relate the inï¬‚uence matrices of each network together ap-\npropriately. Furthermore, we would like to model the inï¬‚u-\nence propagation on the song-level topology and then derive\nartist and genre inï¬‚uence measures, as the compositional act\nof sampling is presumably based on the musical material it-\nself, rather than artist or genre connections.\nTo address this issue, a single inï¬‚uence matrix is con-\nstructed using the song-level network (see Section 3.1) and\nis used to create the artist and genre inï¬‚uence matrices, re-\nsulting in the proposed relational collapse-and-sum approach.\nTo construct the artist-level inï¬‚uence matrix IAfrom the\nsong-level network, the song-level network is ï¬rst used to\ncompute the song inï¬‚uence matrix IS. Given IS, we then\ncompute a derived artist inï¬‚uence matrix IA, knowing the\nsource and destination song sets Ss\naiandSd\naibelonging to\neach artistai. To do so, we take each artist aiin the set of\nartistsAand\nâ€¢Sum over the destination song sets of each artist Sd\nai,\ncollapsing the appropriate columns of IS.\nâ€¢Sum over the source song set of each artist Ss\nai, col-\nlapsing the appropriate rows of IS.\nThe result of the process produces an artist-level inï¬‚uence\nmatrixIAwhich is directly derived from the song-level mu-\nsical material, and is done so via linear combinations of the\nsong-level inï¬‚uence matrix. The process can be duplicated\n33212th International Society for Music Information Retrieval Conference (ISMIR 2011)\nor further reduced for other relations, such as artist-to-genre\nand song-to-genre inï¬‚uence.\nGiven the linear relationship from one network to the\nother, we can compute the relative proportions of inï¬‚uence\nbetween networks. As a result, we can analyze how in-\nï¬‚uential a given song is to an overall artistâ€™s inï¬‚uence or\nhow inï¬‚uential an artist is to a genre by taking ratios be-\ntween the respective inï¬‚uence graphs, among other tasks.\nSecondly, by solely computing the inï¬‚uence on the acyclic,\nunweighted song-level network, we can compute ISfrom a\nshort, ï¬nite linear combination of the powers of the adja-\ncency matrix without any iterative procedure and by know-\ning that the powers of the adjacency matrix Ak,k= 1,2,3...\nwill go to zero when kis greater than the maximum sample\nchain length of the network. With sparse matrix represen-\ntations, this modiï¬cation can greatly reduced computation,\nincrease the allowable in-memory network size, and addi-\ntionally releases any restriction on Î±, allowing the user to\nchoose any suitable weighting function. Application of this\napproach is found below in Section 4.\n4. APPLICATION\nThree levels of inï¬‚uence analysis and rank are computed for\nsong, artist, and genre representations, providing a small-to-\nlarge inspection of the data. Various values of Î±are used to\ncompare direct to indirect inï¬‚uence. For this purpose, (3) is\nrescaled to IK=A+Î±1A2+...+Î±kâˆ’1Ak+..., allowing\nÎ±= 0to only account for direct sampling, Î±= 1to equally\naccount for direct and all indirect sampling, and values be-\ntween zero and one to preferentially weight direct samples,\nbut also account for indirect sampling.\n4.1 Song Inï¬‚uence\nThe song-level inï¬‚uence matrix ISis computed from the\nsong network described in Section 3.1. The most inï¬‚uential\nsongs are found in Table 2. We can observe the presence of\nmany popular samples including â€œChange the Beatâ€ by Fab\n5 Freddy and the â€œAmenâ€ break by The Winstons. It is par-\nticularly interesting to note that, for the â€œAmenâ€ break, as Î±\nincreases, the credit of inï¬‚uence intuitively moves from The\nWinstons to The Impressions, and ï¬nally to Jester Hairston.\nThis is a result of a sample chain between material origi-\nnating from Jester Hairston, that was ï¬rst sampled by The\nImpressions, and then massively popularized by The Win-\nstons.\n4.2 Artist Inï¬‚uence\nStarting with the song-level inï¬‚uence, we can collapse IS\nto form an artist-based inï¬‚uence matrix IA. Table 3 shows\nthe top inï¬‚uential artists.3We can also inspect the inï¬‚u-\n3Entries with Fab 5 Freddy also include producers Material and Bee-\nside. All three artists achieved high inï¬‚uence from â€œChange the Beatâ€.James Brown (1.0) James Brown (1.0) James Brown (1.0)\nDr. Dre (0.34) Dr. Dre (0.28) Run-DMC (0.25)\nMarley Marl (0.29) George Clinton (0.25) Fab 5 Freddy (0.23)4\nGeorge Clinton (0.28) Marley Marl (0.25) George Clinton (0.22)\nPublic Enemy (0.27) Public Enemy (0.23) Russell Simmons (0.19)\nRick Rubin (0.25) Rick Rubin (0.22) Kool & the Gang (0.19)\nDJ Premier (0.25) Fab 5 Freddy (0.22) Marley Marl (0.18)\nMaterial (0.24) Material (0.21) Rick Rubin (0.17)\nFab 5 Freddy (0.24) Run-DMC (0.21) Public Enemy (0.17)\nHank Shocklee (0.23) DJ Premier (0.21) Larry Smith (0.16)\nTable 3 . Artist Sample-Based Inï¬‚uence Rank for Î±= 0.0\n(left),Î±= 0.2(middle), and Î±= 1.0(right).\nence of an individual artist by looking at the correspond-\ning row or column. Table 4, for example, names the top\nï¬ve inï¬‚uential and inï¬‚uenced artists of Jay-Z. Finally, we\nInï¬‚uential (Î±= 0.2) Inï¬‚uenced (Î±= 0.2)\nThe Notorious B.I.G. (0.97) Girl Talk (1.0)\nDr. Dre (0.91) Lil Wayne (0.80)\nPuff Daddy (0.53) The Game (0.53)\nNas (0.5) DJ Premier (0.40)\nJames Brown (0.42) Linkin Park (0.39)\nTable 4 . Top Inï¬‚uential and Inï¬‚uenced Artists of Jay-Z .\ncan also compute the relative proportion of inï¬‚uence cre-\nated by each song within an artistâ€™s overall inï¬‚uence. The\ntop three most inï¬‚uential songs of James Brown, for exam-\nple, include â€œFunky Drummerâ€ (14%), â€œThink (About It)â€\nby Lyn Collins and produced by James Brown (9%), and\nâ€œFunky Presidentâ€ (7.5%). Similar measures can be com-\nputed to indicate whether an artist gets more credit as a pro-\nducer or performer.\n4.3 Genre Inï¬‚uence\nThe song-level inï¬‚uence matrix can further be reduced to a\ngenre-based inï¬‚uence IG. The most inï¬‚uential genres found\nare: soul/funk/disco, hip-hop/R&B, rock/pop, jazz/blues, and\nelectronic dance, while the top inï¬‚uenced genres are hip-\nhip/R&B, electronic dance, rock/pop, other, and reggae (for\nall values of Î±). Alternatively, the top songs and artist for\neach genre can also be computed (omitted due to space con-\nstraints).\n5. CONCLUSIONS\nAn analysis of music inï¬‚uence and rank of sample-based\nmusic is presented using the WhoSampled.com dataset. Gen-\neral genre and time-based trends are found, identifying where\nand when the sampling source material is coming from as\nwell as differences in how various genres are sampling oth-\ners. Network graphs are employed to both understand gen-\neral trends of sampling behavior, but to also ï¬nd inï¬‚uence\nrank over songs, artists, and genre. A method of inï¬‚uence\n333Oral Session 4: Web\nChange the Beat (Female Version) by Fab 5 Freddy (1.0) Change the Beat (Female Version) by Fab 5 Freddy (1.0) Change the Beat (Female Version) by Fab 5 Freddy (1.0)\nAmen, Brother by The Winstons (0.82) Amen, Brother by The Winstons (0.74) Funky Drummer by James Brown (0.84)\nFunky Drummer by James Brown (0.63) Funky Drummer by James Brown (0.71) Impeach the President by The Honey Drippers (0.62)\nLa Di Da Di by Doug E. Fresh (0.53) La Di Da Di by Doug E. Fresh (0.51) Synthetic Substitution by Melvin Bliss (0.55)\nThink (About It) by Lyn Collins (0.49) Impeach the President by The Honey Drippers (0.49) Get Up, Get Into It, Get Involved by James Brown (0.54)\nImpeach the President by The Honey Drippers (0.44) Think (About It) by Lyn Collins (0.45) The Big Beat by Billy Squier (0.51)\nFunky President by James Brown (0.35) Funky President by James Brown (0.37) Scratchinâ€™ by The Magic Disco Machine (0.50)\nHere We Go (Live at the Funhouse) by Run-DMC (0.34) Synthetic Substitution by Melvin Bliss (0.36) Weâ€™re a Winner by The Impressions (0.46)\nBring the Noise by Public Enemy (0.33) Here We Go (Live at the Funhouse) by Run-DMC (0.34) Assembly Line by Commodores (0.46)\nSynthetic Substitution by Melvin Bliss (0.32) Bring the Noise by Public Enemy (0.32) Amen by Jester Hairston (0.46)\nTable 2 . Song Sample-Based Inï¬‚uence Rank for Î±= 0.0(left),Î±= 0.2(middle), and Î±= 1.0(right).\nrank is proposed, in an effort to unify higher-level artist and\ngenre inï¬‚uence measures as appropriate linear combinations\nof song-level network inï¬‚uence. Empirical results suggest\nsample-based musical networks follow a power-law degree\ndistribution; heavy inï¬‚uence of funk, soul, and disco music\non modern hip-hop, R&B, and electronic music; and other\nmusicological results.\n6. ACKNOWLEDGEMENTS\nThis work was made possible from a generous collaboration\nwith founder of WhoSampled.com, Nadav Poraz, who pro-\nvided access to the rich and unique dataset. Additional help\nwas provided by National Science Foundation Creative IT\ngrant No. IIS-0855758.\n7. REFERENCES\n[1] P. Cano and M. Koppenberger: â€œThe Emergence of\nComplex Network Patterns in Music Artist Networks,â€\nIn Proceedings of the International Symposium on Mu-\nsic Information Retrieval , pp. 466â€“469, 2004.\n[2] P. Cano, O. Celma, M. Koppenberger, and J. Martin-\nBuld Â´u: â€œTopology of Music Recommendation Net-\nworks,â€ Chaos An Interdisciplinary Journal of Nonlin-\near Science , 2006.\n[3] K. Jacobson, M. Sandler, and B. Fields: â€œUsing Audio\nAnalysis and Network Structure to Identify Communi-\nties in On-line Social Networks of Artists,â€ In Proceed-\nings of the International Symposium on Music Informa-\ntion Retrieval , pp. 269â€“274, 2008.\n[4] K. Jacobson and M. Sandler: â€œMusically Meaningful\nor Just Noise? An Analysis of On-line Artist Net-\nworks,â€ Computer Music Modeling and Retrieval. Gen-\nesis of Meaning in Sound and Music , eds. S. Ystad,\nR Kronland-Martinet, and K. Jension. Springer-Verlag,\nBerlin, pp. 107â€“118, 2009.\n[5] B. Fields, K. Jacobson, M. Casey, and M. Sandler: â€œDo\nYou Sound Like Your Friends? Exploring Artist Simi-\nlarity via Artist Social Network Relationships and AudioSignal Processing,â€ In Proceedings of the International\nConference on Computer Music , 2008.\n[6] B. Fields, C. Rhodes, M. Casey, and K. Jacobson: â€œSo-\ncial Playlists and Bottleneck Measurements: Exploiting\nMusician Social Graphs Using Content-Based Dissimi-\nlarity and Pairwise Maximum Flow Values,â€ In Proceed-\nings of the International Symposium on Music Informa-\ntion Retrieval , pp. 559â€“564, 2008.\n[7] N. Collins: â€œComputational Analysis of Musical Inï¬‚u-\nence: A Musicological Case Study Using MIR Tools,â€\nIn Proceedings of the International Symposium on Mu-\nsic Information Retrieval , pp. 177â€“182, 2010.\n[8] WhoSampled.com, Exploring and Discussing the DNA\nof Music, 2011. www.whosampled.com .\n[9] P. Shannon, A. Markiel, O. Ozier, N. S. Baliga, J. T.\nWang, D. Ramage, N. Amin, B. Schwikowski, and T.\nIdeker: â€œCytoscape: A Software Environment for Inte-\ngrated Models of Biomolecular Interaction Networks.â€\nGenome Research , 13(11), pp. 2498-2504, 2003.\n[10] R. Lambiotte and M. Ausloos: â€œOn the Genre-ï¬cation of\nMusic: A Percolation Approach (Long Version).â€ The\nEuropean Physical Journal B , 50 (1-2), pp. 183-188,\n2006.\n[11] P. R. La Monica: â€œMusic Industry Sues Swappers:\nRIAA Says 261 Cases Pursued for Illegal Distribution of\nCopyrighted Music; Amnesty Program Offered.â€ CNN\nMoney, Technology News , September 8, 2003.\n[12] A. Clauset, C. R. Shalizi, and M. E. J. Newman: â€œPower-\nlaw Distributions in Empirical Data.â€ SIAM Review\n51(4), 661-703 (2009).\n[13] M. E. J. Newman: Networks: An Introduction , Oxford\nUniversity Press, New York, 2010.\n[14] L. Katz: â€œA New Status Index Derived From Sociomet-\nric Analysis,â€ Psychometrika 18, 1953.\n[15] C. H. Hubbel: â€œAn Input-Output Approach to Clique\nIdentiï¬cation,â€ Sociometry 28 (4), pp. 377â€“399, 1965.\n334"
    },
    {
        "title": "Using Sequence Alignment and Voting to Improve Optical Music Recognition from Multiple Recognizers.",
        "author": [
            "Esben Paul Bugge",
            "Kim Lundsteen Juncher",
            "Brian SÃ¸borg Mathiasen",
            "Jakob Grue Simonsen"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1418175",
        "url": "https://doi.org/10.5281/zenodo.1418175",
        "ee": "https://zenodo.org/records/1418175/files/BuggeJMS11.pdf",
        "abstract": "Digitalizing sheet music using Optical Music Recognition (OMR) is error-prone, especially when using noisy images created from scanned prints. Inspired by DNA-sequence alignment, we devise a method to use multiple sequence alignment to automatically compare output from multiple third party OMR tools and perform automatic error-correction of pitch and duration of notes. We perform tests on a corpus of 49 one-page scores of varying quality. Our method on average reduces the amount of errors from an ensemble of 4 commercial OMR tools. The method achieves, on average, fewer errors than each recognizer by itself, but statistical tests show that it is significantly better than only 2 of the 4 commercial recognizers. The results suggest that recognizers may be improved somewhat by sequence alignment and voting, but that more elaborate methods may be needed to obtain substantial improvements. All software, scanned music data used for testing, and experiment protocols are open source and available at: http://code.google.com/p/omr-errorcorrection/",
        "zenodo_id": 1418175,
        "dblp_key": "conf/ismir/BuggeJMS11",
        "keywords": [
            "Digitalizing",
            "Optical Music Recognition",
            "Pitch",
            "Duration",
            "Error-correction",
            "Sequence alignment",
            "Multiple sequence alignment",
            "Voting",
            "Open source",
            "Google Code"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nUSING SEQUENCE ALIGNMENT AND VOTING TO IMPROVE OPTICAL\nMUSIC RECOGNITION FROM MULTIPLE RECOGNIZERS\nEsben Paul Bugge Kim Lundsteen Juncher Brian SÃ¸borg Mathiasen Jakob Grue Simonsen\nDepartment of Computer Science, University of Copenhagen (DIKU)\nNjalsgade 126â€“128, 2300 Copenhagen S, Denmark\n{ebugge,juncher,soborg,simonsen }@diku.dk\nABSTRACT\nDigitalizing sheet music using Optical Music Recognition\n(OMR) is error-prone, especially when using noisy images\ncreated from scanned prints. Inspired by DNA-sequence\nalignment, we devise a method to use multiple sequence\nalignment to automatically compare output from multiple\nthird party OMR tools and perform automatic error-correction\nof pitch and duration of notes.\nWe perform tests on a corpus of 49 one-page scores of\nvarying quality. Our method on average reduces the amount\nof errors from an ensemble of 4 commercial OMR tools.\nThe method achieves, on average, fewer errors than each\nrecognizer by itself, but statistical tests show that it is sig-\nniï¬cantly better than only 2 of the 4 commercial recogniz-\ners. The results suggest that recognizers may be improved\nsomewhat by sequence alignment and voting, but that more\nelaborate methods may be needed to obtain substantial im-\nprovements.\nAll software, scanned music data used for testing, and\nexperiment protocols are open source and available at:\nhttp://code.google.com/p/omr-errorcorrection/\n1. INTRODUCTION AND RELATED WORK\nOptical music recognition (OMR) is an active ï¬eld, but suf-\nfers from a number of technical pitfalls, even in the â€œtyp-\nicalâ€ case where only music notation in modern, conven-\ntional western style is considered [3,8,13]. While affordable\ncommercial tools for OMR are available, imperfections in\nscanned sheet music make these error-prone (see Fig. 1).\nOne possibility for improving the accuracy of OMR pro-\ngrams is to use multiple recognizers : Let several programs\n(recognizers ) perform OMR independepently, and combine\nthe results afterwards using a combined recognizer . The\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proï¬t or commercial advantage and that copies\nbear this notice and the full citation on the ï¬rst page.\nc/circlecopyrt2011 International Society for Music Information Retrieval.\nFigure 1 . Example of a recognizer missing a note. Left: Bar\n6 of the bass part of a piano arrangement of â€œGod save the\nQueenâ€ by T.A. Arne. Right: The output of Capella-Scan\n1.6.\npractical possibility of using multiple recognizers has been\ninvestigated by Byrd et al. [5â€“7], and appears promising, but\nbrings new pitfalls with it; in extreme cases, OMR programs\ncould fail dismally at different tasks, henceâ€“in theoryâ€“making\nthe combined result worse than the output of the individual\nrecognizer.\nIn contrast, we take a workmanlike approach to multi-\nple recognizers: The basic tenet is that every commercially\navailable tool will not fail dismally on a single aspect of\nOMR in most cases (the product would be too poor to use),\nand that different tools are likely to fail in different aspects.\nByrd et al. [6,7] suggest amassing a set of rules, or attaching\nweights to certain single recognizers, based on their prior\nperformance, to obtain maximal increase of accuracy in a\nmulti-recognizer tool; however, they also note that this is\na moving target, due to new versions of existing products\nimproving on some aspect of recognition. In contrast, we\nare simply satisï¬ed if a multi-recognizer is, on average, bet-\nter than any single-recognizer, to a high degree of statistical\nsigniï¬cance .\nTo account for the fact that different recognizers may\nmake different errors, hence causing misalignment of their\nrespective outputs (see Fig. 2) we align their outputs using\na multiple sequence alignment algorithm and subsequently\nuse a simple voting procedure to resolve conï¬‚icts. A pre-\nrequisite for such an approach to work is that no single rec-\nognizer signiï¬cantly outperforms the others, as a multiple\nrecognizer would then perform worse as the suboptimal rec-\n405Poster Session 3\nFigure 2 . Misaligned notes from the third bar of â€œMon beau\nsapinâ€ by E. Ansch Â¨utz. Top: Original; middle: Capella-\nScan 6.1; bottom: Photoscore Ultimate 6.\nognizers introduce noise in the sequence alignment.\nOur work was originally motivated by our desire to ex-\namine melodic and harmonic progression as used by dif-\nferent composers, and how the statistical properties of such\nprogressions changed over the lifetime of composers. Our\nresults are thus restricted to aspects of melody and harmony;\nwe thus consider only notes, rests, bars, keys, etc., but omit\ndynamic indications (p, pp, etc.) and theâ€“admittedly more\ndifï¬cultâ€“problem of slurs and complex annotations.\n1.1 Related work\nByrd et al. [5â€“7] report on several experiments using an\nOMR system based on several different recognizers, includ-\ning a prototype system for sequence alignment, but do not\ngive details on the numerical improvement of the multi-recog-\nnizer system. Szwoch [15] uses alignment within bars to au-\ntomatically obtain error counts for OMR systems, but does\nnot give numerical evidence. Pardo and Sanghi [12] em-\nploy multiple sequence alignment to ï¬nd optimal matching\nworks in databases of polyphonic music when queried with\nmonophonic pieces; they consider an alphabet where each\nmusical symbol is a note with pitch and duration, and each\npart in a polyphonic score corresponds to a sequence. Al-\nlali et al. substantially extend this approach to encompass\npolyphonic queries [1, 2].\nWhile the work of Byrd et al. is very similar to ours, we\nbelieve our work offers the following incremental beneï¬ts:\n(i) conï¬rmation of the positive results obtained in the exper-\niments of Byrd et al., (ii) comparison of different commer-\ncial tools with each other and with a system based on multi-\nple recognizers with statistical signiï¬cance testing, (iii) full,\nnumerical reporting of results, (iv) full release of all tools\nas open-source software, including the MusicXiMpLe XML\nSchema Deï¬nition (XSD) and sequence alignment software.\nMusicXML\nResult in\nMusicXiMpLeResulting\nsequenceAligned\nsequencesSequencesMusicXiMpLe\nSequencerVoterSequence alignerSequencerConverterFigure 3 . Pipeline for the OMR system. Rectangles rep-\nresent data objects and boxes machinery for processing or\nconverting data. The left topmost rectangle contains ndif-\nferent pieces of MusicXML data from ndifferent OMR pro-\ngrams.\n2. ALIGNMENT OF OUTPUT FROM MULTIPLE\nRECOGNIZERS: PRACTICAL OVERVIEW\nOur combined recognizer takes the output from several rec-\nognizers in a common format, converts the output to sev-\neral sequences of musical symbols which are then aligned\nwith conï¬‚icts resolved by majority (colloquially: â€œThe pro-\ngrams vote for the symbolsâ€ after alignment); the resulting\nsequence is then converted to the common format (see Fig.\n3.\nWe employed four commercial recognizers: Capella-Scan\n6.1, SmartScore X Pro 10.2.6, PhotoScore Ultimate 6, and\nSharpEye 2. VivaldiScan was brieï¬‚y investigated, but dis-\ncarded as it (for our purposes) was only a wrapper for the\nOMR procedures of SharpEye. All tools support several\noutput formats; we chose MusicXML as all programs sup-\nport it and the format is amenable to manipulation.\nTheconverter converts MusicXML to a standard format\ncalled MusicXiMpLe (see Section 2.1) with the purpose of\nnormalizing notation. The sequencer converts MusicXiM-\npLe to an internal representation of music as a sequence\nof symbols (see Section 3.1). The Sequence aligner (see\nSection 3.2) uses multiple sequence alignment to align the\nsequences, and the Voter is used to settle disputes among\nOMR programs after alignment. The Sequencer is then used\nagain to convert from the internal sequence representation to\nthe standard format.\n40612th International Society for Music Information Retrieval Conference (ISMIR 2011)\n2.1 A common output format: MusicXiMpLe\nDue to the ambiguities in MusicXML, a piece of music can\nbe represented in different ways, and different recognizers\nmay output starkly different MusicXML, even if all recog-\nnizers read the music correctly. Furthermore, MusicXML\nis quite verbose, containing more information and metadata\nthan needed for our experiment. To address these issues,\nwe created an XML Schema Deï¬nition (XSD) containing\nsolely those elements needed for analysis. We call the set of\nXML-data conforming to our XSD â€œMusicXiMpLeâ€; note\nthat valid MusicXiMpLe is also valid MusicXML.\nBrieï¬‚y, MusicXiMpLe holds the following data. In con-\ntrast to ordinary MusicXML, restrictions are noted in [square\nbrackets]: (i) parts [each part holds exactly one staff], (ii)\nmeasures [only part-wise structures are allowed, not time-\nwise], (iii) notes [only pitch, duration, octave, alternation\nand simultaneity are recorded], (iv) rests [only duration is\nrecorded], (v) the MusicXML â€œmusical counterâ€, (vi) re-\npeats and alternative endings, (vii) time-signature, (viii) key,\n(ix) chord symbols.\n3. MUSICAL SYMBOLS AND MUSIC DATA AS A\nSEQUENCE\nSequence alignment is the task of comparing and aligning\nn > 1sequences of symbols. As an example, consider the\nsequencess1ands2constructed using the symbol set {A,B,\nC,D,E}:\ns1=AABBCCDA\ns2=ABCE\nSequence alignment of s1ands2might give the following\nresult (depending on the algorithm used):\na1=AABBCCDA\na2=-AB-CE--\nwherea1anda2represents the aligned sequences of s1and\ns2respectively and â€™ -â€™ represents a gap inserted by the align-\nment algorithm.\nSequence alignment algorithms calculate similarity scores\nfor the elements in the sequences; high similarity will occur\nat points in a score where two recognizers output the same\nsymbols, for instance barlines in the same place. These\nscores are then used to align the sequences. When given\nNsequences as input, multiple sequence alignment returns\nNaligned sequences, possibly with gaps inserted. In our\ncase, this corresponds to Naligned scores; we will reduce\nthese to a single score, by letting each recognizers â€œvoteâ€ for\neach single element in the Naligned sequences (ties broken\nrandomly).3.1 Symbolic music data as a sequence\nWe consider music data as any sequence of elements ewhere\neis generated from the following grammar:\ne:= note+|rest|barline|repeat|ending|key|\ntime|clef\nAnote above is a quadruple (p,a,o,l )wherepâˆˆ{A,...,G}\nis the pitch class ,aâˆˆ{ï¬‚at,natural,sharp}thealterna-\ntion,oâˆˆ{0,..., 9}theoctave , andlâˆˆQtheduration of\nthe note. An element holds one or more notes, hence may\nfunction as a chord. The notes in an element may have dif-\nferent lengths (see Fig. 4). Intuitively, the sequence has an\nelement for each â€œchangeâ€ in the music. With chords con-\ntaining notes of different lengths, a single note missed by a\nrecognizer may lead to very distinct sequences of elements\nfor two different recognizers (this problem is addressed in\nthe sequence alignment, as similarity scores between ele-\nments are computed in such a way that elements that only\ndiffer by â€œfewâ€ notes are counted â€œalmost similarâ€).\nFigure 4 . Bars 11â€“12 of â€œO Christmas tree!â€ by E. An-\nschÂ¨utz . Elements of the sequence alphabet are indicated by\nred outlines (accidentals and duration are included in each\nelement).\nWe consider each staff to hold a single sequence of sym-\nbols, and perform sequence alignment per-staff. For sheet\nmusic with notes where it is unclear to which staff a given\nnote belongs, different recognizers may assign notes to dif-\nferent staves, negatively affecting subsequent sequence align-\nment.\n3.2 Progressive sequence alignment of symbolic music\ndata and voting\nWe brieï¬‚y outline the method for multiple sequence align-\nment below. Note that our choice of algorithms is not due\nto any intrinsic properties of symbolic music; the employed\n407Poster Session 3\nalgorithms could very likely be replaced by other algorithms\nfrom the sequence alignment literature without detrimental\neffect to correctness or performance.\nDue to its tradeoff between speed and precision, we em-\nploy progressive multiple sequence alignment [16] in which\n(a) pair-wise alignment of all sequence-pairs is performed,\nfollowed by (b) computation of a similarity-score Dfor each\npair, and (c) the two most similar are aligned ï¬rst, producing\ntwo new sequences that are then (d) progressively aligned\nwith the remaining sequences in descending order of simi-\nlarity score.\nProgressive alignment is greedy and non-optimalâ€“as op-\nposed to dynamic programming methodsâ€“but is signiï¬cantly\nfaster. For pairwise alignment, we use the classic Needleman-\nWunsch algorithm [11]. This method ï¬nds the alignment of\ntwo sequences s1ands2of lengthkandlby ï¬rst creat-\ning the similarity matrix Mdeï¬ned by the (k+ 1,l+ 1) -\ndimensional matrix Mi,jwhereM0,j=gÂ·j, and\nMi,j= max/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleMiâˆ’1,jâˆ’1+Î±(s1[i],s2[j])\nMiâˆ’1,j+g\nMi,jâˆ’1+g(1)\nwhereiâˆˆ {0,1,...,k},jâˆˆ {0,1,...,l}, the function\nÎ±(x,y)returns a score based on whether the two elements\nxandyare similar or not, and gis the gap penalty which\nis the score of inserting a gap into one of the sequences. In\naddition, the algorithm maintains a trace matrix Tof iden-\ntical dimensions. This matrix holds information about how\nthe value of each element in Mwas found. If for example\nthe value of M1,2isM0,1+Î±(s1[1],s2[2]),T1,2will hold\nthe coordinates (0,1).\nFor two musical elements e1,e2, we deï¬ne their similar-\nity asÎ±(e1,e2) =dif the elements are completely distinct,\nandÎ±(e1,e2) =ks/n if the elements are similar, where\nnis the combined number of symbols in e1ande2, andk\nis the number of symbols they have in common (note that\nnotes of identical pitch, but different length are counted as\nbeing distinct). The parameters g,dandscan be set accord-\ning to preference or performance. All our experiments were\nconducted with g=âˆ’2,d=âˆ’1ands= 1. To avoid spuri-\nous â€œelementsâ€ containing notes in combinations with time\nsignatures, bar lines or clefs, such combinations were heav-\nily penalized by setting their similarity scores effectively to\nâˆ’âˆ.\nWhen the matrices MandThave been constructed, pair-\nwise alignment proceeds by following the path from Tk,l\nback toT0,0using the coordinates stored in the cells of T.\nIn the example above, the returned solution is a1=ABBCE ,\na2=A--CD .\nTo extend the pairwise alignment to multiple alignent, a\nso-called guide is constructed that speciï¬es the sequence in\nwhich pairwise alignments are performed. The guide is con-\nstructed by the standard technique of neighbor-joining [14].ABBC E\n0 -2 -4 -6 -8 -10\nA-2 1 -1 -3 -5 -7\nC-4 -1 0 -2 -2 -4\nD-6 -3 -2 -1 -3 -3\nFigure 5 . A similarity matrix Musing input strings\ns1=ABBCE and s2=ACD.\nA B B C E\n(0,0) (0,1) (0,2) (0,3) (0,4)\nA (0,0) (0,0) (1,1) (1,2) (1,3) (1,4)\nC (1,0) (1,1) (1,1) (1,2) (2,2) (1,3) (2,4)\nD (2,0) (2,1) (2,1) (2,2) (2,2) (2,3) (3,3) (2,4)\nFigure 6 . The trace matrix Tcorresponding to the similar-\nity matrix from Figure 5. Entry Ti,jholds the coordinates\nof the entry that led to the value of the lower-right entry\nofM. Multiple coordinates in an entry give rise to multi-\nple paths. The path corresponding to the optimal solution\nis highlighted in bold: ((2,4) â†’(1,3)â†’(1,2)â†’(1,1)â†’\n(0,0)).\nFor every position in the set of Naligned sequences, we\ncollect all symbols from all sequences and their count. For\na symbol to be included in the ï¬nal output, it must have an\nabsolute majority (exceptions are clefs and time signatures\nthat only need half the votes, as we found that the existing\nrecognizers tend to miss them).\n4. EXPERIMENT\nWe collected a corpus ( Corpus A ) of 25 scanned, public do-\nmain, one-page pieces of western classical music. The cor-\npus consisted solely of western classical music ranked in\n5 groups of 5 each according to quality (1 worst, 5 best;\nsee Fig. 7). The corpus was composed prior to any OMR\nscanning by the various recognizers; the music ranged from\n1â€“15 staves with either chords or multiple voices present in\nmost staves. We supplemented Corpus A by acquiring the\n24 scanned pages from the original study of Byrd et al. [6]\n(Corpus B ). This corpus consisted mostly of high-quality\nscans (qualities 3â€“5 on our scale) with mostly a single voice\non a single staff. In both corpora, we employed 300DPI\nscans, using the â€œuncleanedâ€ scans of Corpus B. We ap-\nplied the four commercial products to the combined corpus\nA+B, using Finale Songwriter 2010 to read the output Mu-\nsicXML, and performed error counts by hand.\n4.1 Error counting\nError counting in OMR is notoriously difï¬cult and ambigu-\nous [3, 4, 6, 9]. Droettboom and Fujinaga [9] and Bellini et\nal. [4] argue that error counting at the level of atomic sym-\nbols such as noteheads, ï¬‚ags etc. is markedly different from\nthe case with composite symbols (beamed notes, chords,\netc.), and that a single error in an atomic symbol may cause\n40812th International Society for Music Information Retrieval Conference (ISMIR 2011)\nFigure 7 . Scores of quality 5 (left: Bar 12 of â€œGod save the\nQueenâ€ by T.A. Arne) and quality 1 (right: Staff 3, Bar 17\nof â€œLa Baladine Capriceâ€ by C.B. Lysberg).\nnumerous errors in composite symbols out of proportion.\nIn addition, there are inherent ambiguities in error counting\n(see Tab. 1). There appears to be no consensus in the lit-\nerature on the â€œcorrectâ€ way to resolve ambiguities, so we\nchose as a guideline that the sound (pitch, duration, etc.) of\nthe music should be preserved, that is, if a recognizer fails\nto read symbols correctly, but replaces them with identically\nsounding ones (e.g. replacing a whole note rest by two half\nnote rests), we do notcount it as an error. However, to avoid\npenalizing OMR tools for missing the beginning clef or key\nsignature (in which case most or all of the notes in the piece\nwould be counted as in error), we only count oneerror for\nsuch a miss. For potential ambiguities in the error count,\nwe followed a strict disambiguation procedure, described in\nTable 1 along with their resolution.\nOriginal score Post-OMR score Ambiguity (A) and Resolution (R)\n/noteheads.s1/noteheads.s1\n/noteheads.s1\nA: Unclear which of the two notes is miss-\ning. R: Count one note missing error.\n/noteheads.s2\n/noteheads.s1\nA: Note has been misread both in duration\nand pitch. R: Counts as one note error.\n/noteheads.s1/noteheads.s2\n/noteheads.s1\nA: Unclear which note is missing and which\nnote has been transposed. R: Count one\nmissing note and one transformed note,\nyielding two errors.\n/noteheads.s2/noteheads.s2/noteheads.s2\n/noteheads.s2/noteheads.s2\nA: Unclear which of three notes is missing.\nR: Count one missing note and one trans-\nformed note, yielding two errors.\n/clefs.F/noteheads.s2/noteheads.s2/noteheads.s2\n/noteheads.s2/noteheads.s2/noteheads.s2\nA: Unclear how the remaining notes after\nmissing clef should be read. R: Count one\nmissing clef, no note errors, yielding one er-\nror.\n/noteheads.s2/accidentals.sharp/noteheads.s2/noteheads.s2/noteheads.s2\n/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2\nA: Unclear of the effect of the missing sharp\npitch. R: Missing accidentals results in note\nerrors for every alterated note within the tab,\nyielding two errors.\n/accidentals.flat/clefs.G/noteheads.s2 /noteheads.s2\n/accidentals.flatflat/noteheads.s2 /noteheads.s2/accidentals.flat/clefs.G\nA: Unclear how to count the added acciden-\ntals. R: The MusicXiMpLe format adds the\nextra accidentals, and these are denoted for\neach note. This yields no errors\n/noteheads.s2 /noteheads.s2\n/flags.u3\n/noteheads.s2/flags.u3\n/noteheads.s2\nA: The resulting document from conversion\nto MusicXiMpLe breaks beams. R: Cos-\nmetic issue, yields no errors.\nTable 1 . (Non-exhaustive) list of common ambiguities for\nerror counts and their resolution4.2 Qualitative assessment\nNaked-eye inspection during error counts revealed that all\nrecognizers have errors on most pages. Furthermore, the\ncombined recognizer seems to perform better on Corpus B\nthan on Corpus A, containing mostly single-staff, single-\nvoice music. It would thus appear that sequence alignment\nand voting is impaired by chords, and that a reï¬ned distance\nmetric between â€œsimilarâ€ chords is needed. Another oppor-\ntunity for improvement is that the sequence alignment is af-\nfected negatively if several recognizers misread a clef: All\nnotes will be dissimilar, to the detriment of the alignment al-\ngorithm; this problem could possibly be avoided by letting\neach recognizer output using a notation format or relative\npitch notation, rather than a music format (where pitches\nare absolute).\n4.3 Quantitative assessment\nFor testing whether one recognizer signiï¬cantly outperformed\nthe other, we performed an experiment with our two cor-\npora (N= 49 ). To avoid spurious assumptions about the\nnormality of the error rate of each recognizer, we eschewed\nparametric tests and instead performed (a) non-parametric\nFriedman tests on the ensemble of all tools, (b) sign tests\non each pair of recognizers against the null hypothesis that\napplying a pair of recognizers to a random score the recog-\nnizers are equally likely to yield fewer errors than the other.\nBoth tests avoid debatable comparisons of the absolute num-\nber of errors per page, comparing only the relative number\nof errors for each pair of recognizers. Tests were performed\nat signiï¬cance level of p<. 05.\nRanking the ï¬ve recognizers from least errors (rank 1)\nto most errors (rank 5), the combined recognizer (CR) per-\nformed best on average: CR: 2.43, Sharpeye: 2.83, Smart-\nscore: 2.86, Photoscore: 3.26, Capella-Scan: 3.62. The\nFriedman test showed a signiï¬cant difference in the set of\nranks of the ï¬ve recognizers ( Ï‡2= 16.286, df = 4,p=\n.003) . A post-hoc sign test with Bonferroni correction only\nyielded signiï¬cance for the pair CR vs. Capella-Scan ( Z=\nâˆ’3.166,p < . 005). The sign test on all pairs of recogniz-\ners yielded signiï¬cant results for CR vs. Photoscore ( Z=\nâˆ’1.960,p=.049), CR vs. Capella ( Z=âˆ’3.166,p=\n.001), and Capella-Scan vs. Sharpeye ( Z=âˆ’2.261,p=\n.023), while the remaining pairwise comparisons were non-\nsigniï¬cant.\nThe results suggest that Capella-Scan often made more\nerrors than the remaining tools, and that Sharpeye often made\nfewer errors. The sign test also revealed that none of the\nrecognizers consistently outperform each other, for exam-\nple in the 46 scores that both recognizers were able to scan,\nCapella-Scan had fewer errors than Sharpeye in 14, 2 ties,\nand more errors in 30.\nWhile the average rankings of the tools suggest that the\n409Poster Session 3\ncombined recognizer generally performs better, the fact that\nwe can only give reasonable statistical evidence for this sup-\nposition for two of the commercial tools tempers the conclu-\nsion somewhat. We have little doubt that given a test corpus\nof scores in the hundreds we would obtain signiï¬cant differ-\nences for the remaining tools, but clearly the improvement is\nsmall. Even for high-quality (4 and 5) scores, all recognizers\nhad error counts above 0(only on a single score in Corpus\nB did every tool perform spotlessly). It appears that fully-\nautomated, error-free music recognition is not possible and\nthat human post-correction is almost invariably warranted.\n5. CONCLUSION AND FUTURE WORK\nWe have shown that a simple OMR system based on multi-\nple recognizers and sequence alignment can outperform the\ncommercially available tools. Our results conï¬rm the ear-\nlier work of Byrd et al. suggesting that recognizers may be\nimproved somewhat by sequence alignment and voting, but\nthat more elaborate methods may be needed to obtain sub-\nstantial improvements. For future work, we suggest tack-\nling dynamics, slurs, articulations, ornaments, arpeggiated\nchords, and other embellishments. We advocate the estab-\nlishment of sizable online repositories of scores both for\nbenchmarking multiple recognizers and for the output of\nsuch systems (i.e., reliable, error-free scores), using a suit-\nable interchange format, e.g. [10].\n6. REFERENCES\n[1] Julien Allali, Pascal Ferraro, Pierre Hanna, Costas S. Il-\niopoulos, and Matthias Robine. Toward a general frame-\nwork for polyphonic comparison. Fundam. Inform. ,\n97(3):331â€“346, 2009.\n[2] Julien Allali, Pascal Ferraro, Pierre Hanna, and Matthias\nRobine. Polyphonic alignment algorithms for sym-\nbolic music retrieval. In CMMR/ICAD , volume 5954\nofLecture Notes in Computer Science , pages 466â€“482.\nSpringer, 2009.\n[3] David Bainbridge and Tim Bell. The challenge of opti-\ncal music recognition. Computers and the Humanities ,\n35(2):95â€“121, 2001.\n[4] P. Bellinni, I. Bruno, and P. Nesi. Assessing opti-\ncal music recognition tools. Computer Music Journal ,\n31(1):68â€“93, 2007.\n[5] David Byrd and Ian Knopke. Towards musicdiff: A\nfoundation for improved optical recognition using mul-\ntiple recognizers. In Proceedings of ISMIR â€™07 , pages\n123â€“126, 2007.\n[6] Donald Byrd, William Guerin, Megan Schindele, and\nIan Knopke. Omr evaluation and prospects for improvedOMR via multiple recognizers. Submitted for publica-\ntion.\n[7] Donald Byrd and Megan Schindele. Prospects for im-\nproving OMR with multiple recognizers. In Proceedings\nof ISMIR â€™06 , pages 41â€“46, 2006.\n[8] Jaime S. Cardoso and Ana Rebelo. Robust stafï¬‚ine\nthickness and distance estimation in binary and gray-\nlevel music scores. In CPR , pages 1856â€“1859. IEEE,\n2010.\n[9] Michael Droettboom and Ichiro Fujinaga. Micro-level\ngroundtruthing environment for OMR. In ISMIR , 2004.\n[10] Andrew Hankinson, Laurent Pugin, and Ichiro Fujinaga.\nAn interchange format for optical music recognition ap-\nplications. In Proceedings of ISMIR â€™10 , pages 51â€“56,\n2010.\n[11] Saul B. Needleman and Christian D. Wunsch. A general\nmethod applicable to the search for similarities in the\namino acid sequence of two proteins. Journal of molec-\nular biology , 48(3):443â€“453, March 1970.\n[12] Bryan Pardo and Manan Sanghi. Polyphonic musical se-\nquence alignment for database search. In Proceedings of\nISMIR â€™05 , pages 215â€“222, 2005.\n[13] Ana Rebelo, G. Capela, and Jaime S. Cardoso. Optical\nrecognition of music symbols - a comparative study. In-\nternational Journal of Document Analysis and Recogni-\ntion, 13(1):19â€“31, 2010.\n[14] Naruya Saitou and Masatoshi Nei. The neighbor-joining\nmethod: a new method for reconstructing phylogenetic\ntrees. Molecular Biology and Evolution , 4:406â€“425,\n1987.\n[15] Mariusz Szwoch. Using musicxml to evaluate accuracy\nof OMR systems. In Diagrams , volume 5223 of Lecture\nNotes in Computer Science , pages 419â€“422. Springer,\n2008.\n[16] Julie D. Thompson, Desmond G. Higgins, and Toby J.\nGibson. CLUSTAL W: improving the sensitivity of pro-\ngressive multiple sequence alignment through sequence\nweighting, position-speciï¬c gap penalties and weight\nmatrix choice. Nucleic Acids Research , 22(22):4673â€“\n4680, 1994.\n410"
    },
    {
        "title": "An Expert Ground Truth Set for Audio Chord Recognition and Music Analysis.",
        "author": [
            "John Ashley Burgoyne",
            "Jonathan Wild",
            "Ichiro Fujinaga"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1417547",
        "url": "https://doi.org/10.5281/zenodo.1417547",
        "ee": "https://zenodo.org/records/1417547/files/BurgoyneWF11.pdf",
        "abstract": "Audio chord recognition has attracted much interest in recent years, but a severe lack of reliable training dataâ€”both in terms of quantity and range of samplingâ€”has hindered progress. Working with a team of trained jazz musicians, we have collected time-aligned transcriptions of the harmony in more than a thousand songs selected randomly from the Billboard â€œHot 100â€ chart in the United States between 1958 and 1991. These transcriptions contain complete information about upper extensions and alterations as well as information about meter, phrase, and larger musical structure. We expect that these transcriptions will enable significant advances in the quality of training for audio-chord-recognition algorithms, and furthermore, because of an innovative sampling methodology, the data are usable as they stand for computational musicology. The paper includes some summary figures and statistics to help readers understand the scope of the data as well as information for obtaining the transcriptions for their own research.",
        "zenodo_id": 1417547,
        "dblp_key": "conf/ismir/BurgoyneWF11",
        "keywords": [
            "audio chord recognition",
            "training data",
            "jazz musicians",
            "Billboard Hot 100",
            "time-aligned transcriptions",
            "harmony",
            "complete information",
            "meter",
            "phrase",
            "larger musical structure"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nAN EXPERT GROUND-TRUTH SET\nFOR AUDIO CHORD RECOGNITION AND MUSIC ANALYSIS\nJohn Ashley Burgoyne Jonathan Wild Ichiro Fujinaga\nCentre for Interdisciplinary Research in Music Media and Technology\nMcGill University, Montr Â´eal, Qu Â´ebec, Canada\n{ashley,jon,ich }@music.mcgill.ca\nABSTRACT\nAudio chord recognition has attracted much interest in re-\ncent years, but a severe lack of reliable training dataâ€”both\nin terms of quantity and range of samplingâ€”has hindered\nprogress. Working with a team of trained jazz musicians, we\nhave collected time-aligned transcriptions of the harmony\nin more than a thousand songs selected randomly from the\nBillboard â€œHot 100â€ chart in the United States between 1958\nand 1991. These transcriptions contain complete information\nabout upper extensions and alterations as well as information\nabout meter, phrase, and larger musical structure. We ex-\npect that these transcriptions will enable signiï¬cant advances\nin the quality of training for audio-chord-recognition algo-\nrithms, and furthermore, because of an innovative sampling\nmethodology, the data are usable as they stand for computa-\ntional musicology. The paper includes some summary ï¬gures\nand statistics to help readers understand the scope of the data\nas well as information for obtaining the transcriptions for\ntheir own research.\n1. WHY CHORDS?\nEver since Alexander Sheh and Dan Ellisâ€™s ï¬rst foray into rec-\nognizing musical chords directly from audio [11], this chal-\nlenging problem has fascinated researchers at ISMIR. From\nthe beginning, however, the challenges have been more than\njust engineering: there has not been nearly enough labelled,\ntime-aligned data to train reliable recognizers. Sheh and Ellis\nworked with just twenty songs. Gradually, more data has be-\ncome available, most famously Christopher Harteâ€™s transcrip-\ntions of the entire output of the Beatles [8], but even the most\nrecent Music Information Retrieval Evaluation Exchange\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proï¬t or commercial advantage and that copies\nbear this notice and the full citation on the ï¬rst page.\nÂ©2011 International Society for Music Information Retrieval.(MIREX) contests1have had only 210 songs available [10].\nSome researchers have tried to circumvent the problem by\nsynthesizing audio from MIDI [9], but there has remained a\nsigniï¬cant interest in developing a larger, human-annotated\ndata set of chords from commercial recordings.\nAudio chord recognition is not the only use for a larger\ndata set. The analysis of harmony in popular music has been\ndrawing more and more attention from music theorists [2, 6].\nDue to the limitations on the amount of available data, these\nanalyses and theories are usually based on a very limited\nnumber of examples and cannot be generalized with statisti-\ncal guarantees of accuracy. A large-scale empirical analysis\nof harmony in popular music would be an enormous contri-\nbution to musicology, but such analysis would require not\nonly more data, just as audio chord recognition does, but also\na wider range of data. Of the 210 songs in the MIREX data\nset, 174 (83 percent) are by the Beatles. While that may be\nadmirable in terms of musical quality, it makes it impossi-\nble to draw more general conclusions about how harmony\noperated in the music of other artists and other periods. We\nbelieve that a single, well-conceived data set can address the\nneeds of both communities.\nWe are pleased to announce the release of a new data set\nthat comprises detailed transcriptions of the chords in more\nthan one thousand songs selected at random from Billboard\nmagazineâ€™s â€œHot 100â€ charts. Each transcription represents\nthe combined opinion of three or more experts in jazz and\npopular music, and the chord symbols have been time-aligned\nwith the musical meter and with commercially available au-\ndio recordings. This paper describes the methodology for\nselecting songs (section 2), explains the process used to tran-\nscribe them (section 3), and presents some basic descriptive\nstatistics to help readers understand how they might use these\ndata (section 4). In addition to the contribution of the data set,\nwe hope that information about how we produced themâ€”a\nprocess that was considerably more involved than we had\noriginally expectedâ€”will beneï¬t other research groups who\nare interested in transcribing still more chords themselves.\n1http://www.music-ir.org/mirex/\n633Oral Session 8: Chord Analysis\n2. THE BILLBOARD â€œHOT 100â€\nThe Billboard â€œHot 100â€ is a weekly compilation of the\nmost popular music singles in the United States, all genres\nincluded, based on a combination of radio airplay and retail\nsales (and more recently, digital downloads).2The â€œHot 100â€\nhas been published continuously in Billboard magazine since\n4 August 1958, replacing earlier charts like â€œBest Sellers\nin Stores,â€ â€œMost Played by Jockeys,â€ and â€œMost Played in\nJukeboxes.â€ Although it is far from a perfect representation of\npopularity, like any ranking, it is generally regarded to be the\ngold standard among charts of popular music in North Amer-\nica [4]. Because it includes all genres, it seemed particularly\nwell-suited to the goals of training broadly-applicable chord\nrecognizers and drawing broadly-applicable musicological\nconclusions. It has also been the basis for several previous\nattempts to draw statistical conclusions about the behavior\nof popular singles over time [1, 4, 7].\n2.1 Sampling Methodology\nThe date of the ï¬rst chart, 4 August 1958, is a natural starting\ndate for selecting songs, but choosing an end date is less\nstraightforward. Hip-hop music does not lend itself readily to\nharmonic analysis as traditionally understood, and because\nhip-hop became more popular in the 1990s and 2000s, a\nlarger portion of the music on the â€œHot 100â€ chart from these\nperiods falls out of the scope of the data set. Furthermore,\nthere have been several changes to the formula for computing\nthe â€œHot 100â€ over time, including a particularly signiï¬cant\nshift in December 1991, when the data for generating the\ncharts shifted from being self-reported to being generated\nautomatically through Nielsenâ€™s BDS and SoundScan sys-\ntem.3After this date, songs tended to stay on the charts for\nso much longer than before that Billboard established lim-\nits on how many weeks any given single would be allowed\nto remain on the â€œHot 100â€ chart, added a â€œRecurrent Sin-\nglesâ€ chart to capture singles knocked o ï¬€the chart due to the\nnew rule, and has averaged songs pre-1991 di ï¬€erently from\nthose post-1991 when generating historical summaries like\nthe â€œ50th-Anniversaryâ€ charts [3]. We chose to restrict our\nsample to charts prior to December 1991 in order to avoid\nthese problems.\nAs stated earlier, our goal in constructing this data set\nwas not only to provide a higher-quality set for audio chord\nrecognition but also to provide a data set that would be useful\nfor computational musicology and the analysis of popular\nmusic. As such, it was important to choose a sample of songs\nthat would allow for general questions about how popular\nmusic and the factors that made it popular evolved through-\nout the latter half of the twentieth century. Like most projects,\n2http://www.billboard.com/charts/hot-100\n3http://nielsen.com/us/en/industries/\nmedia-entertainment.html1. Divide the set of all chart slots into three eras:\n(a) 4 August 1958 to 31 December 1969,\n(b) 1 January 1970 to 31 December 1979, and\n(c) 1 January 1980 to 30 November 1991.\n2.Subdivide the chart slots in each era into ï¬ve sub-\ngroups corresponding to quintiles on the chart:\n(a) ranks 1 to 20,\n(b) ranks 21 to 40,\n(c) ranks 41 to 60,\n(d) ranks 61 to 80, and\n(e) ranks 81 to 100.\n3.Select a ï¬xed percentage pof possible chart slots\nat random from each era-quintile pair.\n4. For each selected chart slot:\n(a)attempt to acquire the single at the target slot;\n(b)if that fails, toss a virtual coin to choose be-\ntween either the single directly above or di-\nrectly below the target slot on the chart from\nthe same week;\n(c)if that fails, choose the single that was not se-\nlected by the coin toss in 4b;\n(d)if that fails, toss a virtual coin to choose be-\ntween either the single two ranks above or two\nranks below the target single on the chart from\nthe same week;\n(e)if that fails, choose the single that was not se-\nlected by the coin ï¬‚ip in 4d; and\n(f)if that fails, consider the chart position to be a\nmissing data point.\nFigure 1 . Sampling algorithm for the Billboard â€œHot 100.â€\nThe algorithm is designed to minimize the distortion from\nâ€œconvenience samplingâ€ while reducing the expense of col-\nlecting an audio collection. We believe that this algorithm\nyields a data set that, as cost-e ï¬€ectively as possible, is\nvalid for drawing conclusions about relative positioning and\nchanges in the behavior of music on the charts over time.\n63412th International Society for Music Information Retrieval Conference (ISMIR 2011)\n020406080100050100150\nHighest rank on any chartNumber of instancesFigure 2 . Histogram of the highest rank achieved on any\nchart among singles in the random sample. Because of the\nbehavior of popular songsâ€”namely that they tend to stay on\nthe chart for a long time and rise and fall through di ï¬€erent\nranksâ€”our sampling method still weighs the most popular\nsongs more heavily. We consider this behavior desirable.\nhowever, the budget was limited, and we wanted to make the\nbest use possible of the recordings we already had available\nwithout unduly biasing the ï¬nal data set. In consultation with\na professional statistician, we devised the sampling method-\nology detailed in ï¬gure 1. The ï¬rst two steps guarantee that\neven the most unfavorable random draw would still provide\nsome information about time and chart position. The ï¬nal\nstep balances the desire to maximize use of recordings on\nhand with the need to achieve a sample that is representative\nof the underlying charts; it works on the assumption that\nsingles within two chart positions of each other in any given\nweek should behave similarly. In limit of an inï¬nite number\nof samples drawn in this way, one would expect to retrieve\nall recordings on hand weighted proportionally to their be-\nhavior on the charts. The more recordings of â€œmissingâ€ chart\npositions that one acquires later, the more accurately the ï¬nal\nsample will represent the underlying charts.\n2.2 Properties of the Sample\nOverall, from a sample of 2000 slots, we were able to acquire\naudio for 1365 slots (68 percent): 424 of 683 from before\nthe 1970s, 505 of 664 from the 1970s, and 436 of 653 from\nafter the 1970s. Because the sample was taken over slots\nand not individual singles, some singles, especially popu-\nlar singles, appear more than once (and would need to be\nweighted accordingly for the most accurate statistics). Of the\n1100 unique singles in our sample, performed by 533 unique\nartists, the great majority of singles (869) do appear only\nonce, but 202 appear twice, 24 three times, and 5 four times.\nA more interesting artifact of sampling over slots instead of\nsingles is that even though the original sample was drawn\nevenly across all chart ranks, there is still more weight in the\nsample toward the most popular songs. Songs tend to remain# Love Will Keep Us Together\n# Captain and Tenille\n# 4/4\n# key: B\n| B | B | B | B |\n| B | B | D#:hdim7/b5 | D#:hdim7/b5 | G#:7 | G#:7 |\n| E | E | E:min | E:min |\n| B | B:aug | B:maj6 | B:7 |\n| E E/7 | C#:min7 F#:9(*3,11) . . |\n| B | B | B | B |\n| B | B | D#:hdim7/b5 | D#:hdim7/b5 | G#:7 | G#:7 |\nFigure 3 . Prototypical transcription illustrating features of\nthe transcription format. The format encodes a number of\nhigh-level musicological features such as key, meter, beat,\nand phrase. Chord symbols follow the format proposed in [8]\nand include as much detail as possible about inversions and\nupper extensions.\non the charts for many weeks (10 on average, although this\nï¬gure is much greater for the most popular songs and much\nless for the least popular), rising and falling through di ï¬€erent\nranks. Figure 2 illustrates the distribution of peak ranks in\nour sample, which corresponds well to that of the full set of\nchart slots during the time period spanned in the sample.\n3. THE TRANSCRIPTION PROCESS\nAnnotating such a large data set was a considerably greater\nundertaking than we had expected, ultimately involving a\nteam of more than two dozen people. We began by devel-\noping a ï¬le format for transcriptions that would capture as\nmuch musicologically-relevant information as possible, de-\nsigned a web site to manage transcriptions, and organized a\nseries of auditions to identify musicians with su ï¬ƒcient skill\nto transcribe reliably and e ï¬ƒciently at a high level of detail.\n3.1 The Transcription Format\nThe transcription format was a plain-text format in order to\nfacilitate transfer across platforms. The full speciï¬cation is\navailable for download with the transcriptions themselves,\nbut the basic premises are illustrated in ï¬gure 3. All non-\nmusical material is preceded by a comment character ( #), and\ncomments are allowed at the end of any line. The annotators\nused them freely. Each transcription begins with a four-line\nheader containing the title of the song, the name of the artist,\nthe meter, and the key, and new meter and key lines are added\nas necessary to reï¬‚ect changes throughout the song. Each\ntranscription is broken with line breaks into phrases, which\nare deï¬ned loosely as any point where a group might choose\nto start playing during a rehearsal. Pipes ( |) denote barlines,\nand although transcribers were allowed to mark chords using\nwhatever notation came most naturally to them, all have since\nbeen converted to the format proposed in [8].\n635Oral Session 8: Chord Analysis\nFigure 4 . Screenshot of the web site that annotators used\nto manage their work. The page contains a list of all assign-\nments as well as information about to whom each single was\nassigned and when.\nChords are marked for every beat, with some shorthand\nto improve readability. For quadruple meters, which are the\nmost common, a bar with a single chord symbol is assumed to\nhave the same chord for all four beats. Bars with two symbols\nare assumed to have the chord change on beat 3. For bars\nwith less than four chords that follow other patterns, periods\nare used to denote chords that have not changed. For example,\nin the ï¬rst bar of the ï¬fth line of the transcription in ï¬gure\n3 contains E on the ï¬rst two beat and E /D/sharpon the second\ntwo beats, whereas the second bar contains C /sharpmin7 on the\nï¬rst beat only followed by what might be noted as F /sharp11 in\na fake book on the last three beats. Chord changes that are\nfaster than the beat level are simpliï¬ed. Notable silences in\nthe music are marked with the special tag &pause .\n3.2 Auditions and the Transcription Process\nOver several recruitment periods between April and Decem-\nber 2010, 30 musicians were invited to audition for the project.\nWith one exception (an undergraduate), these musicians were\neither graduate students in music performance or professional\njazz performers (often both). Of those invited to audition, 23\ncompleted the audition and 17 were ultimately hired. We\nprepared a detailed description of the ï¬le format for those\nauditioning, as well as a set of six sample songs with full\ntranscriptions, in order to help the potential transcribers un-\nderstand the format and the level of detail expected. After\nstudying these materials, all those auditioning transcribed a\nset of ï¬ve test songs that were chosen to be representative of\nthe more di ï¬ƒcult songs one would encounter. We reviewed\nthese test transcriptions, decided whether the annotator had\nsuï¬ƒcient potential to continue, and provided detailed feed-\nback on the audition to each transcriber we hired in order to\nensure as much consistency as possible across transcriptions.\nAfter hiring, following the principle of double-keying to\nminimize mistakes, two annotators were assigned to each\n020406080Minutes to transcribeFigure 5 . Transcribing times for each annotator. Box widths\nare scaled proportional to the square root of the number of\ntranscriptions completed. Points more than 1 Â½times the\ninter-quartile range are plotted as outliers. The majority of\nsongs took between 8 and 18 minutes to transcribe, although\na few extremely di ï¬ƒcult songs took more than an hour.\nsong. Working with a custom-designed web interface (see\nï¬gure 4), the annotators were able to access the audio for\ntheir assignments and, although they were asked to work in-\ndependently, to see who their partner annotator was in case of\nany di ï¬ƒcult questions. Annotators worked at di ï¬€erent speeds,\nand in order to reward more e ï¬ƒcient annotators, we paid per\nsong with a bonus system to compensate for songs that were\nunusually di ï¬ƒcult to transcribe. The majority of songs were\ntranscribed in 8 to 18 minutes (median 12 minutes), but the\nmost di ï¬ƒcult songs could take an hour or more (see ï¬gure 5).\nMost annotators also reported that regardless of the amount\nof time spent, it was di ï¬ƒcult to do more than a dozen songs\nin a single day: due to the intense concentration necessary, it\nwas simply too exhausting for them to work more.\nAfter the two assigned annotators for any given song had\ncompleted their transcriptions, a third meta-annotator com-\npared the two versionsâ€”inevitably, there were usually dif-\nferences in notation or musical opinion in addition to actual\nerrorsâ€”and combined them into a master transcription. This\ncombined version was then time-aligned and annotated with\nstructural information based on musical similarity, functional\ninformation (verse, chorus, etc.), and instrumentation [12].\nFactoring in the salaries of all involved, it cost more than $20\nper song to arrive at this ï¬nal ï¬le, but we believe that the\nrichness and accuracy of the data justify the cost.\n4. THE DATA SET\nThere are 414 059 labeled beats in our corpus, spread over\n638 distinct chords and 99 chord classes. Each song contains\n11.8 unique chords on average, ranging from a minimum of\n1 to a maximum of 84; songs from the late 1970s exhibit the\nmost harmonic variety. Figures 6 and 7 present the relative\n63612th International Society for Music Information Retrieval Conference (ISMIR 2011)\nC:majD:majG:majA:majE:majF:majBb:majAb:majB:majEb:majA:minD:minE:minDb:majB:minG:7F:7D:7C:7F#:minD:min7A:7G:minA:min7F#:majE:min7Ab:7E:7Gb:majG:min7B:min7Bb:7Eb:7C:min7C:minF:minC#:minB:7Eb:min7Db:7C#:min7Bb:min7F#:min7Eb:minD:5F:maj7F:min7G:5F#:7G:maj70.000.010.020.030.040.050.06\nFigure 6 . Frequency distribution of the 50 most common chords in the data set. There is a sharp drop after the most common major triads and a long tail afterward.\nmajmin7min7maj751maj(9)maj6sus4(b7,9)sus4(b7)sus4min97(#9)maj9maj(11)119min115(b7)sus2min6maj6(9)min(9)dim13hdim77(b9)7(11)sus4(9)min(11)augmin7(11)maj7(#11)sus4(b7,9,13)maj131(11)min(b13)sus2(b7)7(#11)min13minmaj71(b7)maj9(13)maj9(13,#11)aug(b7)min7(b13)sus2(13)7(b13)dim70.00.10.20.30.40.5\nFigure 7 . Frequency distribution of the 50 most common chord classes in the data set. Major chords alone account for more than half of the data set, followed by\nminor chords and the basic 7th chords.\n637Oral Session 8: Chord Analysis\nfrequencies of the top 50 chords and chord classes from the\nnew data set. The most noticeable pattern is a sharp fallo ï¬€\nafter the seven most popular chords (all major): C, D, G, A,\nE, F, and B /flat. Indeed, a milder fallo ï¬€begins even after the\nfour most popular chords. Certainly these chords are a use-\nful setâ€”they are su ï¬ƒcient to play in the ï¬ve most common\nmajor keysâ€”but such a sharp decline even for minor chords\nwas unexpected. For chord classes, the fallo ï¬€is even more\nextreme, although this is to be expected. The dominance of\nmajor and minor chords and simple seventh chords is con-\nsistent with most approaches to simplifying chords symbols\n(see [10], among others). The ordering suggests that with a\ndata set of this size, it might be reasonable to start training\nsystems that can also recognize simple 9th and 11th chords.\nTo our knowledge, there is no other curated corpus of\npopular harmony that equals this new data set in terms of\nsize or scope. It is roughly ï¬ve times the size of the existing\nMIREX set and contains a considerably broader range of\nartists, genres, and time periods. Trevor de Clercq and David\nTemperley have annotated another impressive data set of\n200 songs from Rolling Stone â€™s â€œ500 Greatest Songs of All\nTime,â€ but their set is not time-aligned with audio [5]. We\nare currently working on a corpus analysis to compare our\nset to theirs and to explore deeper structures that may be\ndiscoverable with a larger data set.\n5. SUMMARY AND CONCLUSION\nSeeking to beneï¬t both researchers interested in audio chord\nrecognition and researchers interested in computational ap-\nproaches to studying harmony in popular music, we have\ncreated a database more than four times the size of any exist-\ning database with detailed, curated musicological information\nand time-alignment with commercial audio recordings. The\ndata set beneï¬ts from a special sampling methodology that\nwas designed to maximize its utility both for musicological\nand for engineering purposes. Other researchers who wish to\nextend this data set or build a similar one of their own should\nbe warned that the process is labor-intensive, but the statis-\ntics in this paper should provide guidelines for planning and\nbudgeting. We are very excited to start working on the many\nquestions this database will allow researchers to answer, and\nwe are proud to make it available to the community at no\ncost and with minimally restrictive licensing.4\n6. ACKNOWLEDGEMENTS\nWe would like to thank the Social Sciences and Humani-\nties Research Council of Canada for funding this research,\nRhonda Amsel for her advice on sampling, and all of the an-\nnotators who worked on the project, especially Reiko Yamada\nand Tristan Paxton for their tirelessness as meta-annotators.\n4http://billboard.music.mcgill.ca/7. REFERENCES\n[1]S. Bhattacharjee, R. D. Gopal, J. R. Marsden, and\nR. Telang. A survival analysis of albums on ranking\ncharts. In E. M. Noam and L. M. Pupillo, editors, Peer-\nto-Peer Video: The Economics, Policy, and Culture of\nTodayâ€™s New Mass Medium , pages 181â€“204. Springer,\nNew York, NY , 2008.\n[2]N. Biamonte. Triadic modal and pentatonic patterns in\nrock music. Music Theory Spectrum , 32(2):95â€“110, 2010.\n[3]Billboard Magazine. Hot 100 50th anniversary charts\nFAQ, 2008. Available http://www.billboard.com/\nspecials/hot100/charts/hot100faq.shtml .\n[4]E. T. Bradlow and P. S. Fader. A Bayesian lifetime model\nfor the â€œHot 100â€ Billboard songs. Journal of the Ameri-\ncan Statistical Association , 96(454):368â€“81, 2001.\n[5]T. de Clercq and D. Temperley. A corpus analysis of rock\nharmony. Popular Music , 30(1):47â€“70, 2011.\n[6]W. Everett. The Foundations of Rock: From â€œBlue Suede\nShoesâ€ to â€œSuite: Judy Blue Eyes. â€ Oxford University\nPress, New York, NY , 2008.\n[7]D. E. Giles. Survival of the hippest: Life at the top of the\nHot 100. Applied Economics , 39(15):1877â€“87, 2007.\n[8]C. Harte, M. Sandler, S. A. Abdallah, and E. G Â´omez.\nSymbolic representation of musical chords: A proposed\nsyntax for text annotations. In Proc. 6th ISMIR , pages\n66â€“71, London, England, 2005.\n[9]K. Lee and M. Slaney. Acoustic chord transcription and\nkey extraction from audio using key-dependent HMMs\ntrained on synthesized audio. IEEE Transactions on Au-\ndio, Speech, and Language Processing , 16(2):291â€“301,\n2008.\n[10] M. Mauch. Automatic Chord Transcription from Audio\nUsing Computational Models of Musical Context . PhD\nthesis, Queen Mary, University of London, London, Eng-\nland, 2010.\n[11] A. Sheh and D. P. W. Ellis. Chord segmentation and\nrecognition using EM-trained hidden Markov models. In\nProc. 4th ISMIR , pages 185â€“91, Baltimore, MD, 2003.\n[12] J. B. L. Smith, J. A. Burgoyne, I. Fujinaga, D. De Roure,\nand J. S. Downie. Design and creation of a large-scale\ndatabase of structural annotations. In Proc. 12th ISMIR ,\nMiami, FL, 2011.\n638"
    },
    {
        "title": "Identifying Emotion Segments in Music by Discovering Motifs in Physiological Data.",
        "author": [
            "Rafael Cabredo",
            "Roberto Sebastian Legaspi",
            "Masayuki Numao"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1416872",
        "url": "https://doi.org/10.5281/zenodo.1416872",
        "ee": "https://zenodo.org/records/1416872/files/CabredoLN11.pdf",
        "abstract": "Music can induce different emotions in people. We propose a system that can identify music segments which induce specific emotions from the listener. The work involves building a knowledge base with mappings between affective states (happiness, sadness, etc.) and music features (rhythm, chord progression, etc.). Building this knowledge base requires",
        "zenodo_id": 1416872,
        "dblp_key": "conf/ismir/CabredoLN11",
        "keywords": [
            "Music",
            "emotions",
            "listener",
            "system",
            "identify",
            "mapping",
            "features",
            "knowledge",
            "base",
            "work"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nIDENTIFYINGEMOTIONSEGMENTS INMUSIC BYDISCOVERING\nMOTIFSINPHYSIOLOGICAL DATA\nRafaelCabredo, Roberto Legaspi, Masayuki Numao\nTheInstituteofScientiï¬cand IndustrialResearch, OsakaU niversity\n{cabredo,roberto,numao }@ai.sanken.osaka-u.ac.jp\nABSTRACT\nMusiccaninducedifferentemotionsin people. We propose\nasystemthatcanidentifymusicsegmentswhichinducespe-\nciï¬cemotionsfromthelistener. Theworkinvolvesbuilding\na knowledge base with mappings between affective states\n(happiness,sadness,etc.) andmusicfeatures(rhythm,cho rd\nprogression, etc.). Building this knowledge base requires\nbackground knowledge from music and emotions psychol-\nogy. Psychophysiological responses of a user, particularl y,\nthebloodvolumepulse,aretakenwhilehelistenstomusic.\nThese signals are analyzed and mapped to various musical\nfeatures of the songs he listened to. A motif discovery al-\ngorithm used in data mining is adapted to analyze signals\nof physiological data. Motif discovery ï¬nds patterns in the\ndata thatindicatepointsofinterestin themusic. Thediffe r-\nentmotifsarestoredinalibraryofpatternsandusedtoiden -\ntify other songs that have similar musical content. Results\nshow that motifs selected have similar chord progressions.\nSome of which include frequently used chords in western\npopmusic.\n1. INTRODUCTION\nMusichasbecomeaubiquitousformofentertainment. Peo-\nple listen to music in various situations: while travelling ,\ndoingsports,studying,orrelaxing. Musicstructureandfe a-\ntures can be used to select music appropriate to the emo-\ntional interest of its listeners. This has been researched i n\nvariousï¬eldslikemusicandemotionpsychology,musicin-\nformationretrieval,andmorerecentlyaffectivecomputin g.\nAutomaticallydetecting the emotion or moodcontent of\nmusic is still in its early stages. Some of the work involve\nmanually annotating songs with emotion tags by individual\nhuman annotators [16], social tagging [13], and even using\ngamestomakethetaskmoreinterestingforannotators[10].\nPermission to make digital or hard copies of all or part of thi s work for\npersonal or classroom use is granted without fee provided th at copies are\nnot made or distributed for proï¬t or commercial advantage an d that copies\nbear this notice and the full citation on theï¬rstpage.\nc/circlecopyrt2011 International Society for MusicInformation Retrieva l.Human assessment of music emotion or mood is based\nfromwhatisheard. Assuch,a lot ofworkisdevotedto un-\nderstandinghowvariousmusicfeaturesandmusicstructure\nplay a role in inducingemotion. A detailed review of these\nworkscanbefoundin[4,8,11].\nThe work of Livingstone, et al. [11] also demonstrates\nthat by changing speciï¬c music elements, the emotion per-\nceived by the listener also changes. A similar research is\nalso done in [14] but instead of relyingon verbal reportsof\nfeelings, emotion data is derived from analyzing change in\nactivityinthe autonomicnervoussystem.\nAnother approach to identifying emotion is using psy-\nchophysiological data. Researchers observed that changes\nin musical features lead to a change in physchophysiologi-\ncalresponse. Forexample,changeintempoleadtochanges\ninrespirationrate[3,7]. Krumhansl[9]alsonotedincreas es\ninheartratevariabilityduringsad,fearfulandhappymusi c.\nThe use of physiological response also reï¬‚ect an unbiased,\nobjectiveemotionalresponsetomusiclisteningascompare d\nto self-reportingofemotions.\nIn this paper, we propose an approach for identifying\nmusic features that affect emotion. We identify patterns in\npsychophysiologicaldatausingamotifdiscoveryalgorith m\nandanalyzethemusicelementsusedatthetimethepatterns\nwerediscovered.\nWe begin by deï¬ning some concepts and notations im-\nportant for understanding the approach used. In section 3\nand 4, we describe the framework used for the research. In\nsection 5, we describe details of data collection and imple-\nmentation of the algorithms presented. Next, results of our\nexperimentsarediscussedtogetherwithobservationsmade .\nFinal sectionincludestheconclusionandfuturework.\n2. TIME SERIES MOTIFS\nForclarity,ï¬rstwedeï¬neconceptsandterminologyneeded\nto understand our work. These deï¬nitions are taken from\n[2]. The physiological signals are a continuous stream of\nreal-valued data measured at a constant sampling rate. In\ndata mining, this can be considered a time series . A time\nseriesTisdeï¬nedasanorderedsetofreal-valuedvariables.\nAmotifis described as a pair of subsequences from the\n753Poster Session 6\nFigure1. Architecturalframework\ntime seriesthatarefoundto besimilar. A subsequence Cis\na samplingoflength nofcontiguouspositionsin T.\nSimilarity between two subsequencesis measured using\na distance metric D(Ci,Ck). It is possible to ï¬nd many\nmotifs in one time series, the most signiï¬cant of these is\nreferred to as 1-motif. To ensure that the 1-motifdoes not\nshare elementswith othermotifs, a range Ris deï¬nedsuch\nthatD(Ci,Ck)>2R,forall1â‰¤i < k.\n3. ARCHITECTURAL FRAMEWORK\nTheproposedframeworkofoursystemisshowninFigure1.\nOur approach requires collecting psychophysiological dat a\nfrom a subject while he listens to music. We consider an-\nalyzing data from : blood volume pulse (BVP), respiration\n(RR), andskin conductance(SC). These are thenpassed on\nto a motif discovery module that attempts to discover pat-\nterns in the time series data. Details of this module are dis-\ncussedin thenextsection.\nAmusicfeatureextractionmoduleisalsoincludedtode-\ntermine various information from the music (i.e., beat oc-\ncurrences, tempo, chordsused, etc.). These are used by the\nmotifdiscoverymoduletoannotatediscoveredmotifs.\nEachmotifisanalyzedandannotatedwithmusicfeatures\nthat were present when the signal occurred. A library of\ndifferentmotifsisbuiltandthedatacontainedwithinisus ed\nbyamusicrecommendationsystemthatwillgenerateaplay\nlistofsongsthathavesimilarmusicfeatures. Intuitively ,we\nexpect that the subject will enjoy listening to music simila r\nto thathehasexperienced.\nThis paper discusses the work done upto the motif dis-\ncoverymoduleusingBVPdata. Themusicrecommendation\nsystemiscurrentlybeingdevelopedandwillbedescribedin\nfuturepublications.\n4. MOTIF DISCOVERY\nThe process of motif discovery is illustrated in Figure 2.\nThis algorithm is adapted from the work in [2] where they\nusedaprojectionalgorithmbyBuhlerandTompa[15]. The\nFigure2. Dataï¬‚ow diagramformotifdiscovery\nobjectiveofthealgorithmistoï¬ndsignalsthatareverysim -\nilar to each other. Physiological signals that keep on recur -\nringwouldindicatethatmusicpassagesheardatthesepoint s\nare interesting to the listener (i.e., it makes him relaxed, or\nheenjoysthemusicsegment).\nThe motif discovery algorithm can be separated into 3\nmainparts: datapreparation,conversionofthe datatosym-\nbolic form using the Symbolic Aggregate ApproXimation\n(SAX) representation, and motif discovery using the pro-\njection algorithm. Each part is described in the following\nsubsections.\n4.1 Datapreparation\nPrior to motif discovery, the physiological data undergoes\noffset and amplitude scaling transformations using (1) and\n(2), respectively[1,6,17,18].\nQoffset=Qâˆ’/summationtextn\ni=1qi\nn, (1)\nwhereQis deï¬ned as a time series with nlength and\nQoffsetisthe timeseriesafteroffsettransformation.\nQscaled=Qoffset\nÏƒ, (2)\nwhereÏƒis the standarddeviationof the data and Qscaledis\nthetime seriesafteramplitudescalingtransformation.\nInordertoreducefurtherproblemswhencomparingdif-\nferentsubsequences,alldataisnormalizedtotherange[0, 1]\nusing(3).\nQ=Qâˆ’min(Q)\nmax(Q)âˆ’min(Q)(3)\n4.2 SAX representation\nThe Symbolic Aggregate ApproXimation (SAX) represen-\ntationisusedtoconvertanytimeseriesintoastringofsym-\nbols. By using SAX, powerful algorithms on string pat-\ntern analysis developed in other ï¬elds can be used. The\nï¬rst step is to convert the time series Cof length nto a\nw-dimensional space by a vector Â¯C= Â¯c1,...,Â¯cw. Theith\n75412th International Society for Music Information Retrieval Conference (ISMIR 2011)\nHHHHHÎ²ia3 4 5 6\nÎ²1âˆ’0.43âˆ’0.67âˆ’0.84âˆ’0.97\nÎ²20.43 0 âˆ’0.25âˆ’0.43\nÎ²3 0.67 0.25 0\nÎ²4 0.84 0.43\nÎ²5 0.97\nTable 1. A lookuptablecontainingbreakpointsthatdivides\na Gaussian distributionin anarbitrarynumber(from3to 6)\nofequiprobableregions\nFigure3. Thephysiologicalsignal(thinsmoothline)isdis-\ncretized by ï¬rst obtaining a PAA approximation and then\nusing predetermined breakpoints to map the PAA coefï¬-\ncients into symbols (bold letters). In the example above,\nwithn= 190,w= 12anda= 4,thetimeseriesismapped\nto theword acdddcbbacdd\nelementof Â¯Ciscalculatedbytheequation:\nÂ¯ci=w\nnn\nwi/summationdisplay\nj=n\nw(iâˆ’1)+1cj (4)\nUsing this equation, the time series is divided into w\nequal sized frames. The average values of data in each\nframe is calculated and a dimensionality-reducedrepresen -\ntation known as the Piecewise Aggregate Approximation\n(PAA) [5]isproduced.\nAfter transforming the time series into PAA represen-\ntation, another transformation is applied to obtain the dis -\ncrete representation. Assuming that the subsequences have\na Gaussian distribution, we determine â€œbreakpointsâ€ that\nwill produce equal-sized areas under the Gaussian curve.\nAbreakpoint is a sorted list of numbers B=Î²1,...Î²aâˆ’1\nsuch that the area under a N(0,1)Gaussian curve from Î²i\ntoÎ²i+1= 1/a(Î²0andÎ²aaredeï¬nedas- âˆandâˆ,respec-\ntively).areferstothealphabetsize usedforSAX.\nThe breakpoints are stored in a look-up table similar to\nTable 1. Using the breakpoints, the time series can be dis-\ncretizedbygoingthrougheachPAAcoefï¬cients. Allcoefï¬-\ncientsbelowthesmallestbreakpointaremappedtothesym-\nbol â€œaâ€,all coefï¬cientsgreaterthan orequalto the smallest\nbreakpointand less than the secondsmallest breakpointare\nmappedtothesymbolâ€œ bâ€,etc. Figure3illustratestheidea.\nThe concatenationof symbolsof the subsequencethat isa b c d\na000.671.34\nb0000.67\nc0.67000\nd1.340.6700\nTable 2. A lookuptable for MINDIST function. This table\nis fora SAX representationhaving a= 4. Thedistancecan\nbe obtained by matching the row and column. For example\ndist(a,b)= 0anddist(a,c)= 0.67\nformed is deï¬ned as a word. Each PAA approximation is\nmapped to a symbol using Equation (5). aidenotes the ith\nelementofthealphabet,i.e. a1=a,a2=b, etc.\nË†ci=aiiffÎ²jâˆ’1â‰¤Â¯ci< Î²j (5)\nThedistancebetweentwowordscanbemeasuredbyus-\ning aMINDIST functionthat returnsthe minimumdistance\nbetweentheoriginaltimeseriesofthetwo words:\nMINDIST (Ë†Q,Ë†M)â‰¡/radicalbiggn\nw/radicaltp/radicalvertex/radicalvertex/radicalbtw/summationdisplay\ni=1(dist(Ë†qi,Ë†mi))2(6)\nThis function resembles the original Euclidean distance\n(7) used for comparing the distance between two time se-\nriesQandM. The function MINDIST uses a subfunction\ndist(),whichcanbeimplementedusingatablelookupasil-\nlustrated in Table 2. The value in cell ( r,c) for any lookup\ntablecanbe calculatedbytheexpressionin (8).\nD(Q,M)â‰¡/radicaltp/radicalvertex/radicalvertex/radicalbtn/summationdisplay\ni=1(qiâˆ’mi)2 (7)\ncellr,c=/braceleftbigg0,if|râˆ’c| â‰¤1\nÎ²max(r,c)âˆ’1âˆ’Î²min(r,c),otherwise(8)\n4.3 Projectionalgorithm\nThe motif discovery algorithm proceeds by extracting sub-\nsequencesfromthe SAX representation. Each subsequence\nof length wis placed into a matrix Ë†S. Once the matrix has\nbeenconstructed,weproceedtorandomprojection. Weran-\ndomlyselectw\n2columnsof Ë†Stoactasamask. Forexample,\ngivenw= 4, columns {1,3}can be chosen to act as mask.\nAfterwards,all wordsin theË†Smatrix are hashedinto buck-\netsbasedonlyontheirvaluesinthe 1stand3rdcolumns. If\ntwowordscorrespondingtosubsequences iandjarehashed\nto the same bucket, we increase the count of cell (i,j)in a\ncollisionmatrix .\nThis hashing process is repeated ktimes, with new, ran-\ndomly chosen masks every iteration. Once completed, the\nhighest value stored in the collision matrix correspond to\n755Poster Session 6\nthe candidatemotif. For example,if the largest value in the\ncollision matrix is at cell (2,43)thenC2andC43are the\nsubsequences of the candidate motif. We conï¬rm this by\ncomparingtheoriginaltimeseriesdataandusingEuclidean\ndistancetocomputethedistance.\nAt this point, it is possible to ï¬nd other members of the\nmotif. To ï¬nd other members, we consider the other values\nof the collision matrix at (i,2)and(i,43). Once all the\nmatching subsequences within RofC2andC43have been\nfound,resultsarereportedto theuser.\n5. METHODOLOGY\n5.1 DataCollection\nForthisresearch,weconcentrateonanalysingdatafromone\nsubject(a22-yearmalegraduatestudent). Thesongshelis-\ntenedtoare partofthemusicdataset describedin[12]. The\ncollectionincludes301songsfromvariousartistsaswella s\nannotations for song key, chords, beat and metric position,\nand segmentation(i.e. intro, verse, chorus, etc.). Songs f or\nthe experiments were selected based on three constraints.\nFirst, the songshouldnot haveanykey andtempochanges.\nSecond, the song should have complete chord and beat an-\nnotations. Last, the song is in a major key. Using this cri-\nteria, 83 songs were selected which include 77 songs from\nTheBeatles, fourQueensongs,andtwoCaroleKingsongs.\nOur subject listened to songs via audio-technica closed\nheadphones(ATH-T400)connectedto a computerin a con-\ntrolled experimentroom. Using the BioGraph Inï¬nity Sys-\ntem1, the BVP was recorded. The sensor is attached to the\nsubjectasshownintheexperimentsetupin Figure4.\nSeveral sessions were needed for the subject to listen to\nallthesongswithoutmakinghimfeelstressed. Eachsession\ntook approximately 20 minutes, which allowed the subject\nto listen to seven to nine songs per session. One week was\nneeded to complete the data collection. Sessions were held\nat thesame timeofthedaythroughouttheweek.\nBeforeeachsession ended,the subjectalso self-reported\nthemoodhehadwhilelisteningtothesongs. Ascaleofone\ntoï¬vewasusedtodescribehowhappyandhowexcitingthe\nsongmadehimfeel.\nAlthough83songswereusedforthedatacollection,only\ndata from 64 songs are included for analysis for this exper-\niment. Only songs that made the subject happy (i.e. songs\nrated three and above) and have a tempo between 76 â€“ 168\nbeats per minute (bpm) are included. The tempo and key\ninformationofthemusicdataset isshowninTable3.\n1About BioGraph Inï¬nity System. Thought Technology Ltd. 14 M ay\n2011. http://www.thoughttechnology.com\nFigure 4. Data collection setup: BVP sensor worn on right\nindexï¬ngerwhilelisteningtomusicviaclosedheadphones\nKeyTempoTotalAndante Moderato Allegro\nC 1 1 3 5\nD 1 1 7 9\nE 3 3 8 14\nF 2 1 2 5\nF/sharp 0 0 1 1\nG 5 2 3 10\nA/flat 1 0 0 1\nA 5 4 5 14\nB/flat 1 0 1 2\nB 1 1 1 3\nTotal 20 13 31 64\nAndante: 76â€“108bpm Allegro: 120-168bpm\nModerato: 108â€“120bpm\nTable 3. Summaryofmusicincludedformotifdiscovery\n5.2 Music featureextraction\nSince the isophonics dataset already includes chord, beat,\nkey and segment annotationsfor the differentsongs, only a\nsimple text parser to read the different ï¬le annotations was\nneeded. These annotations were manually done by music\nexpertsandstudents[12].\nForthemotifdiscovery,thephysiologicaldataismapped\nto the chord information to determine what chord is being\nheard at that instance. The music features and the motif\nsubsequences are stored in a ï¬le for cross-reference after\nmotifdiscovery.\n5.3 Motifdiscovery\nAllthe64physiologicalreadingswereanalyzedusingthree\nsets of parameters. Each set has varying sizes for motif\nlength (n) and word size ( w). However, all sets used an\nalphabet size of a= 4and a range R= 1.0. The parame-\ntersusedforeachsetareshowninTable4. Themotiflength\nvaluesweresetassuchtovarythechordprogressionlength\nthat was associated to a motif. The word size was adjusted\nto maintaina compressionratioofn\nw= 8.\n75612th International Society for Music Information Retrieval Conference (ISMIR 2011)\nSetNo. n w Sequence length\n11024 128 8seconds\n2768 96 6seconds\n3512 64 4seconds\nTable 4. Parametersusedforthedifferentsets\nFigure 5 .(top)The BVP signal of subject listening to\nPlease Mister Postman from the Beatles has a motif of\nlength 512foundas subsequence C518andC2264.(bottom)\nBy overlaying the two motifs, we see the similarity of the\ntwo signalstoeachother.\n6. RESULTS\nUsing the motif discovery algorithm, the most signiï¬cant\nmotif (1-motif) were obtained from the dataset. Figure 5\nillustratesanexampleofamotifdiscovered.\nWeobservethatthemotiflengthisinverselyproportional\nto the number of motifs found. Using set number 1 ( n=\n1024), for example, only the song With A Little Help From\nMy Friends was identiï¬ed to have a motif (see Figure 6).\nAnalyzing the music features of the 1-motifpair show that\nthese have similar chord progressions : C764has the chord\nprogressionF /sharp-B-E-B-F /sharpm,andC1618hasB-F/sharpm-B-\nE-D-A.Thissuggeststhatusingthechordprogressionwill\nproduce a similar physiological response. This phenomena\ncan also be observed in most motif pairs taken from other\nphysiologicaldata. Table5showstheamountofmotifsthat\nwerediscoveredto havesimilar chordprogressions.\nFromtheresultsofthemotifdiscovery,onaverage,amo-\ntiflengththatwillgivefourtosixsecondsofchordprogres -\nsion is desirable. The complexity of the chord progression\nwilldependonthelengthofmotif. Sincetheexactlengthof\nthemotifisnotknown,analgorithmthatdoesnotusemotif\nlengthasa parametershouldbeusedinstead.\nOther chord progressions identiï¬ed by motif discovery\nusing parameter set 3 are found in Table 6. The chord pro-\ngressions I-IV, I-IV-V and I-IV-V-I from the song Please\nPlease Me are mapped to the 1-motiffor that song. These\nchords sound similar and possibly invoke the same emo-\ntionalresponseforthatsong. Somemotifswill havesimilar\nFigure 6. The motif discovered for the song With A Little\nHelp From My Friends withn= 1024occurring at subse-\nquenceC764andC1618.\nSetNo. motifcountmotifswith similar\nchordprogressions\n1 1 ( 1/64 = 1.5%) 1 ( 1/1 = 100.0%)\n2 25 ( 25/64= 39.0%) 17 ( 17/25 = 68.0%)\n3 61 ( 61/64= 95.3%) 39 ( 39/61 = 63.9%)\nTable 5. Number of motifs discovered for each parameter\nset andstatistics formotifswith similarchordprogressio ns\nchord progressions but not in all cases. There are also mo-\ntifs that have differentchord progressionsmapped to it, i. e.\nchordsfoundin GoodDaySunshine .\nUsingmotifdiscovery,weareabletodiscoverchordpro-\ngressions that are commonly used in western pop music.\nGiven enough data, the library of motif could be used to\nidentify the most frequently used chord progressions that\ninvoke an emotionalresponse by clustering similar psycho-\nphysiologicalmotifs. Thiscanbeusedincomposingorrec-\nommendingmusicwitha desiredemotionormood.\n7. CONCLUSIONAND FUTUREWORK\nIn this work, psychophysiological readings from a subject\nlistening to music was collected. A motif discovery algo-\nrithm was used to discover motifs from the BVP data. We\nobservethatpartsofmusicwherethemotifoccur,havesim-\nilarchordprogressionsandpossiblyothermusicfeaturesa s\nwell. By improving the algorithms used in this work, a li-\nbraryofdifferentmotifscanbebuilt.\nFutureworkincludesadditionalanalysisonthemotifsto\nincludeothermusicfeatures. Improvingthemotifdiscover y\nalgorithm to dynamically identify motif length is also de-\nsired in order to have a more accurate account of the chord\nprogressionsthat are important. Another roundof data col-\nlectionwillalsobedoneusingadifferentsetofparticipan ts.\nAnalysis of other physiological data, (i.e. respiration ra te\nandskinconductance)isalsoplanned. Amusicrecommen-\ndation system is also beingdesignedthat will use the infor-\nmationfrommotifstogenerateaplaylistofsongsthathave\nsimilar emotioncontent.\n757Poster Session 6\nSong Key Chordprogression\nActNaturally G G-D-G I-V-I\nG-D I-V\nDizzyMissLizzy A D-A IV-I\nA-D I-IV\nE-D-A V-IV-I\nForYouBlue D D-A-D I-V-I\nD-A I-V\nD-A-G7 I-V-IV\nGoodDaySunshine A B7-E7-A ii-V-I\nF/sharp-B-F/sharp vi-ii-vi\nPlease Please Me E E-A I-IV\nE-A-B I-IV-V\nE-A-B-E I-IV-V-I\nWithA Little Help E B-E-B V-I-V\nFromMy Friends F /sharpm-B-E ii-V-I\nYesterday F B /flat/7-Gm-C-F IV-ii-V-I\nGm-C-F-F7 ii-V-I-I\nTable 6. Subsetofresultsusingparameterset 3\n8. REFERENCES\n[1] K. Chan and A.W. Fu. Efï¬cient time series matching\nby wavelets. In Proc. of the 15th IEEE Int'l Conf. on\nData Eng. , page 126â€“133. Sydney, Australia, Mar 23-\n261999.\n[2] B. Chiu, E. Keogh, and S. Lonardi. Probabilistic dis-\ncovery of time series motifs. In Proc. of the 9th ACM\nSIGKDDInt'lConf.onKnowledgeDiscoveryandData\nMining,page493â€“498,2003.\n[3] J. A. Etzel, E. L. Johnsen, J. Dickerson, D. Tranel, and\nR. Adolphs. Cardiovascular and respiratory responses\nduringmusicalmoodinduction. InternationalJournalof\nPsychophysiology , 61(1):57â€“69, 2006. Psychophiosol-\nogyandCognitiveNeuroscience.\n[4] A.GabrielssonandP.N.Juslin.Emotionalexpressionin\nmusic.InR.J.Davidson,K.R.Scherer,andH.H.Gold-\nsmith, editors, Handbook of affective sciences , page\n503â€“534.NewYork: OxfordUniversityPress, 2003.\n[5] E. Keogh, K. Chakrabarti, M. Pazzani, and Mehrotra.\nDimensionality reduction for fast similarity search in\nlarge time series databases. Journal of Knowledge and\nInformationSystems , page67â€“74,2000.\n[6] E.KeoghandS.Kasetty.Ontheneedfortimeseriesdata\nminingbenchmarks: Asurveyandempiricaldemonstra-\ntion. InProc. of the 8th ACM SIGKDD Int'l Conf. on\nKnowledgeDiscovery andDataMining ,page102â€“111.\nEdmonton,Alberta,Canada,July2002.\n[7] S. Khalfa, M. Roy, P. Rainville, S. Dalla Bella, and\nI. Peretz. Role of tempo entrainment in psychophysio-logicaldifferentiationofhappyandsadmusic? Interna-\ntionalJournalof Psychophysiology ,68(1):17â€“26,2008.\n[8] Y. E. Kim, E. M. Schmidt, R. Migneco, B. G. Morton,\nP. Richardson, J. Scott, J. A. Speck, and D. Turnbull.\nMusic emotionrecognition: A state ofthe art review.In\n11thInt'lSocietyforMIRConf. ,pages255â€“266,August\n2010.\n[9] C. L Krumhansl. An exploratorystudy of musical emo-\ntions and psychophysiology. Canadian Journal of Ex-\nperimental Psychology/Revue canadienne de psycholo-\ngieexprimentale ,51(4):336,1997.\n[10] E. L. M. Law, L. von Ahn, R. B. Dannenberg, and\nM. Crawford. Tagatune: A game for music and sound\nannotation. In Proc. of the Int'l Conf. on MIR . Vienna,\nAustria,2007.\n[11] S. R Livingstone, R. Muhlberger, A. R Brown, and\nW. F Thompson. Changing musical emotion: A com-\nputational rule system for modifying score and perfor-\nmance.ComputerMusicJournal ,34(1):41â€“64,2010.\n[12] M. Mauch, C. Cannam, M. Davies, C. Harte,\nS.Kolozali,D.Tidhar,andM.Sandler.OMRAS2meta-\ndata project 2009. In 10th Int'l Conf. on MIR Late-\nBreakingSession .Kobe,Japan,2009.\n[13] F. Miller, M. Stiksel, and R. Jones. Last.fm in numbers.\nLast.fmpressmaterial,February2008.\n[14] M. Numao, T. Nishikawa, T. Sugimoto, S. Kurihara,\nand R. Legaspi. Constructive adaptive user interfaces\nbased on brain waves. In Proc. of the 13th Int'l Conf.\non Human-Computer Interaction. Part II , volume 5611\nofLecture Notes in Computer Science , pages 596â€“605,\nSanDiego,CA, 2009.Springer-Verlag.\n[15] M. Tompa and J. Buhler. Finding motifs using ran-\ndom projections. In Proc. of the 5th Intl Conf. on Com-\nputational Molecular Biology , page 67â€“74. Montreal,\nCanada,Apr22-252001.\n[16] D.Turnbull,L.Barrington,D. Torres,andG.Lanckriet .\nSemantic annotation and retrieval of music and sound\neffects.IEEE Trans. on Audio, Speech and Language\nProcessing ,16(2),2008.\n[17] M. Vlachos, G. Kollios, and G. Gunopulos. Discover-\ningsimilarmultidimensionaltrajectories.In Proc.ofthe\n18thInt'lConf.onDataEng. ,page673â€“684,2002.\n[18] B. K. Yi and C. Faloutsos. Fast time sequence indexing\nforarbitrarylpnorms.In Proc.ofthe26thInt'lConf.on\nVeryLargeDatabases ,page385â€“394,2000.\n758"
    },
    {
        "title": "Survey and Evaluation of Audio Fingerprinting Schemes for Mobile Query-by-Example Applications.",
        "author": [
            "Vijay Chandrasekhar 0001",
            "Matt Sharifi",
            "David A. Ross"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1415260",
        "url": "https://doi.org/10.5281/zenodo.1415260",
        "ee": "https://zenodo.org/records/1415260/files/ChandrasekharSR11.pdf",
        "abstract": "We survey and evaluate popular audio fingerprinting schemes in a common framework with short query probes captured from cell phones. We report and discuss results important for mobile applications: Receiver Operating Characteristic (ROC) performance, size of fingerprints generated compared to size of audio probe, and transmission delay if the fingerprint data were to be transmitted over a wireless link. We hope that the evaluation in this work will guide work towards reducing latency in practical mobile audio retrieval applications.",
        "zenodo_id": 1415260,
        "dblp_key": "conf/ismir/ChandrasekharSR11",
        "keywords": [
            "audio fingerprinting schemes",
            "common framework",
            "cell phones",
            "Receiver Operating Characteristic (ROC)",
            "fingerprint size",
            "wireless link",
            "mobile applications",
            "transmission delay",
            "latency",
            "evaluation results"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nSURVEY AND EVALUATION OF AUDIO FINGERPRINTING SCHEMES\nFOR MOBILE QUERY-BY-EXAMPLE APPLICATIONS\nVijay Chandrasekhar\nvijayc@stanford.eduMatt Shariï¬\nmns@google.comDavid A. Ross\ndross@google.com\nABSTRACT\nWe survey and evaluate popular audio ï¬ngerprinting sche-\nmes in a common framework with short query probes cap-\ntured from cell phones. We report and discuss results im-\nportant for mobile applications: Receiver Operating Char-\nacteristic (ROC) performance, size of ï¬ngerprints generated\ncompared to size of audio probe, and transmission delay if\nthe ï¬ngerprint data were to be transmitted over a wireless\nlink. We hope that the evaluation in this work will guide\nwork towards reducing latency in practical mobile audio re-\ntrieval applications.\n1. INTRODUCTION\nAudio ï¬ngerprinting provides the ability to derive a com-\npact representation which can be efï¬ciently matched against\nother audio clips. With smart phones becoming ubiquitous,\nthere are several applications of audio ï¬ngerprinting on mo-\nbile devices. A common use case is query-by-example mu-\nsic recognition: a user listens to a song in a restaurant, shop-\nping mall, or in a car, and wants to know more information\nabout the song. Shazam [1] and SoundHound [2] are ex-\namples of popular music recognition applications on cell-\nphones. Other applications of audio ï¬ngerprinting on mo-\nbile devices include copyright detection [4], personalized\nentertainment and interactive television without extraneous\nhardware [8].\nMobile query-by-example applications pose a unique set\nof challenges. First, the application has to be low-latency\nto provide users with an interactive experience. To achieve\nlow latency, the retrieval framework has to adapt to stringent\nmemory, computational, power and bandwidth requirements\nof the mobile client. It is important that the size of the data\ngenerated needs to be as small as possible to reduce network\nlatency, which is typically the bottleneck in 3G networks.\nSecond, the length of the audio required to get a match\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proï¬t or commercial advantage and that copies\nbear this notice and the full citation on the ï¬rst page.\ncâƒ2011 International Society for Music Information Retrieval.should be short for mobile applications (e.g., <10 seconds).\nCurrent applications Shazam [1] and SoundHound [2] often\nrequire >10 seconds for retrieval. For copyright detection,\none might use 30-60 second probes for retrieval [4], which\nis not feasible for interactive mobile applications. Third, the\ndistortions introduced by cell phones tend to be more severe\nthan simple degradations like compression artifacts, time-\noffsets, amplitude compression or structured noise present\nin near-duplicate detection problems [4]. On mobile de-\nvices, we need to be mindful of ambient noise present in\nshopping malls or cafes, errors in sampling through tele-\nphony equipment, low bit-rate voice compression and other\nquality-enhancement algorithms that might be built into the\nmobile device or introduced by the carrier network. In this\nwork, we evaluate the state-of-the-art in content-based au-\ndio retrieval with focus on query-by-example mobile appli-\ncations.\n2. PRIOR WORK AND MOTIVATION\nState-of-the-art audio retrieval applications use a set of low\nlevel ï¬ngerprints extracted from the audio sample for re-\ntrieval. The ï¬ngerprints are typically computed on the spec-\ntrogram - a time frequency representation of the audio. Hait-\nsma et al. [11] propose ï¬ngerprints based on Bark Frequency\nCepstrum Coefï¬cients (BFCC). Highly overlapping frames\nare considered to ensure that the query probe can be detected\nat arbitrary time-alignment. Each ï¬ngerprint is 32 bits and\ncan be compared efï¬ciently with Hamming distances. Ke\net al. [14] improve the performance of the ï¬ngerprinting\nscheme in [11] using the AdaBoost technique from com-\nputer vision. Baluja et al. [4] propose a scheme based on\nwavelets. The overlapping spectrogram images are trans-\nformed into a sparse wavelet representation and the pop-\nular min-hash technique [5] is used to obtain a 100 byte\nï¬ngerprint which can be compared directly with byte-wise\nHamming distances. In contrast to the three schemes above,\nWang [17,18] proposes looking only at spectrogram peaks.\nThe authors are not aware of a comprehensive evaluation\nof the different ï¬ngerprinting schemes in a common frame-\nwork. In contrast, several such evaluations exist for im-\nage features in the computer vision community for content-\nbased image retrieval [15, 19]. Fingerprints developed for\n801Oral Session 10\napplications like query-by-humming and cover song detec-\ntion are outside the scope of this paper. In particular, we\nare interested in factors affecting practical query-by-exact-\nexample mobile applications. The questions that are most\ncritical for mobile applications are:\nâ€¢How much ï¬ngerprint data does each scheme generate?\nâ€¢How does the size of the ï¬ngerprint data compare to\nthe size of the compressed audio needed for accurate re-\ntrieval?\nâ€¢What would the transmission delay be if the ï¬ngerprints\nwere transmitted over a typical 3G network?\nâ€¢How discriminative are the different ï¬ngerprinting sche-\nmes?\nâ€¢How do the different schemes perform for really short( âˆ¼5\nseconds) and noisy query probes captured by cell phones\n?\nâ€¢How does the performance of each scheme vary as a\nfunction of probe length in the range of 5 to 15 seconds\ntypical for mobile applications?\n3. CONTRIBUTIONS\nWe survey and evaluate popular audio ï¬ngerprinting sche-\nmes in a common framework with short noisy query audio\nprobes captured from cell phones. We report and discuss re-\nsults important for mobile applications: Receiver Operating\nCharacteristic (ROC) performance, size of ï¬ngerprints gen-\nerated compared to size of audio probe, and transmission\ndelay if the ï¬ngerprint data were to be transmitted over a\nwireless link. We hope that the evaluation in this paper will\nprovide key insights and guide us towards developing low\nlatency retrieval systems. In Section 4, we survey the differ-\nent audio ï¬ngerprinting schemes. In Section 5, we describe\nthe evaluation framework and provide experimental results.\n4. SURVEY OF FINGERPRINTING SCHEMES\nBefore we survey popular audio ï¬ngerprinting schemes, we\ndiscuss the typical pipeline for audio retrieval applications.\nFirst, a set of ï¬ngerprints are extracted from the query song.\nThe ï¬ngerprints could be extracted at uniform sampling rate,\nor only around points of interest in the spectrogram (e.g.,\nspectrogram peaks in the case of Wang [18]). For mobile\napplications, it is critical that individual ï¬ngerprints be ro-\nbust against ambient noise, compared to the corresponding\ndatabase ï¬ngerprint.\nNext the query is compared with a database of reference\ntracks to ï¬nd candidate matches. To avoid pairwise com-\nparison between the query and all of the reference tracks,\nthe database is partitioned. The partitioning of the database\nis precomputed for the database, and each partition is asso-\nciated with a list of database songs (also called an inverted\nindex). The partitioning on the database could be done bydirect hashing of the ï¬ngerprints (e.g., a 32 bit ï¬ngerprint\ncould be directly hashed into a table with 4 billion entries),\nLocality Sensitive Hashing or techniques based on Vector\nQuantization. This partitioning allows approximate-nearest-\nneighbor-search as exact-nearest-neighbor search is infeasi-\nble in a database with billions of ï¬ngerprints. The inverted\nï¬le for each cell consists of a list of song IDs and the timing\noffsets at which the ï¬ngerprints appear. The timing infor-\nmation is used in the ï¬nal step of the pipeline. Based on the\nnumber of ï¬ngerprints they have in common with the query\nprobe from the inverted index, a short list of potentially sim-\nilar database songs is selected from the database.\nFinally, a temporal alignment step is applied to the most\nsimilar matches in the database. Techniques like Expecta-\ntion Maximization [14], RANSAC [9], or Dynamic Time\nWarping [6] are used for temporal alignment. In the case of\nlinear correspondence (i.e., the tempo of the database and\nquery songs are the same), Wang [18] proposes using a sim-\nple and fast technique that looks for a diagonal in the time-\nvs-time plot for matching database and query ï¬ngerprints.\nThe existence of a strong diagonal indicates a valid match.\nThe temporal alignment step is used to get rid of false posi-\ntives, and enables very high precision retrieval.\nIn this Section, we review three ï¬ngerprinting schemes in\ndetail: Ke [14], Baluja [4] and Wang [18]. In the interest of\nspace, we omit the scheme proposed by Haitsma [11] as the\nï¬ngerprint by Ke improves directly upon their scheme [14].\nFor a comparison of the two schemes by Ke and Haitsma,\ninterested readers are referred to [14]. For each scheme, we\nï¬rst discuss the details of the scheme and the motivation\nbehind the approach, followed by system parameters sug-\ngested by the authors that provide good trade-off between\naccuracy and computational complexity.\n4.1 Ke, Hoiem and Sukthankar\n4.1.1 Description\nKeâ€™s approach builds on popular classiï¬cation techniques in\nthe computer vision community. Ke provides the important\ninsight that 1-D audio signals can be processed as conven-\ntional images when viewed in the time-frequency spectro-\ngram representation. The time-frequency spectrogram data\nis treated as a set of overlapping images. To compute a com-\npact ï¬ngerprint on each image, the authors ï¬rst train simple\nAdaBoost classiï¬ers based on box-ï¬lters, a technique pop-\nular in face detection. The training data for classiï¬cation\nis obtained by considering audio samples and their corre-\nsponding versions degraded by noise. The output of each\nclassiï¬er yields a binary value. E.g., each classiï¬er outputs\na 1 or a 0 based on the differences between values aggre-\ngated in two sub-rectangular regions of the spectrogram im-\nage. The concatenated output of the set of classiï¬ers is then\nused as a ï¬ngerprint of the spectrogram image.\n80212th International Society for Music Information Retrieval Conference (ISMIR 2011)\n4.1.2 System Parameters\nKe and Haitsma use the same set of parameters for comput-\ning the spectrogram. The spectrogram, obtained by Short\nTerm Fourier Transform (STFT), represents the power in\n33 logarithmically-spaced frequency bands spaced 300 Hz\nand 2000 Hz. Overlapping spectrogram images measured\nover 0.372s windows are considered in 11.6 ms increments\n(âˆ¼100 ï¬ngerprints/second). The short increments coupled\nwith large spectrogram images at each step are used to make\nthe scheme robust to sampling errors and small time-offsets.\nFor a 10 second probe, the scheme produces 860 ï¬nger-\nprints. For the AdaBoosting step, 32 classiï¬ers are chosen\nout of a candidate list of 25000 ï¬lters. We use the training\ndata sets and code provided by the authors at [13]. Two ï¬n-\ngerprints are considered to be a match if they have a Ham-\nming distance <2, in the feature matching step of the re-\ntrieval pipeline.\n4.2 Baluja and Covell\n4.2.1 Description\nSimilar to Keâ€™s work, Balujaâ€™s ï¬ngerprint is also inspired\nfrom the image retrieval community. The pipeline for com-\nputing â€œwaveprintsâ€(the term used by the authors to describe\ntheir wavelet-based ï¬ngerprints) is illustrated in Fig. 1, and\nin inspired from [12].\nFirst, the authors compute overlapping spectrogram im-\nages using the same approach proposed by Ke. Next, the\nspectrogram images are decomposed using multi-resolution\nHaar wavelets. Wavelets are chosen due to their effective-\nness in the retrieval work presented in [12]. An image pro-\nduces as many wavelet co-efï¬cients as pixels. Next, the au-\nthors retain only the top- tfew wavelets, where tis chosen\nto be much smaller than the size of the spectrogram im-\nage. Next, the authors observe that the top- twavelets are\nsparse. To obtain a compact represenation, the authors only\nretain the sign information (an approach also found effective\nin [12]), and use the Min-Hash technique to generate a set\nofpbytes that is used to represent the original spectrogram\nimage. Two spectrogram images can now be compared di-\nrectly by computing the byte-wise Hamming distance of the\npbytes. For this approach to be effective, pneeds to be\nlarge (typically chosen to be 100). Nearest neighbor search-\ning in a 100 dimensional space is non-trivial. Hence, in the\nï¬nal step, Locality Sensitive Hashing (LSH) is used to ï¬nd\napproximate-nearest-neighbor ï¬ngerprints in this space.\n4.2.2 System Parameters\nThe authors optimize system parameters for accuracy and\ncomputational complexity in [3, 4]. We use the parame-\nters recommended by the authors in [3]. Overlapping spec-\ntrogram images measured over 0.372 second windows are\nconsidered in 0.09 second strides ( âˆ¼10 ï¬ngerprints/second).\nt= 200 top wavelets are considered. pis chosen to be\nFigure 1 .Pipeline for extracting waveprint features proposed by\nBaluja [4]. Spectrogram images are represented as pbytes obtained from\nMin-Hashing, which can be compared byte-wise directly for computing\nsimilarity.\n100, i.e., each ï¬ngerprint is represented as 100 bytes. For\nLSH, the 100-byte ï¬ngerprint is divided into 25 equal 4-byte\nbands. Each 4-byte band is stored as a 32 bit hash table. In\nthe feature-matching step, two ï¬ngerprints are considered to\nbe a match if their 4-byte representations match in at least\none of the 25 LSH bands.\nFigure 2 .Illustration of audio ï¬ngerprints proposed by Wang [17].\nTriplet information ( (t2âˆ’t1, f1,(f2âˆ’f1))is quantized to form the ï¬n-\ngerprint.)\n4.3 Wang\n4.3.1 Description\nWhile the schemes by Ke and Baluja use dense sampling\nand compute ï¬ngerprints over fairly large spectrogram im-\nages, Wang proposes looking only at spectrogram peaks.\nThere are two reasons for choosing spectrogram peaks: First,\nspectrogram peaks are more likely to survive ambient noise.\nSecond, spectrogram peaks satisfy the property of linear su-\nperposition, i.e., a spectrogram peak analysis of music and\nnoise together will contain spectral peaks due to the music\nand the noise as if they were analyzed separately [17]. The\nï¬ngerprinting scheme is illustrated in Fig. 2. For pairs of\npeaks (t1, f1)and(t2, f2), the ï¬ngerprint is computed on\na triplet of ((t2âˆ’t1), f1,(f2âˆ’f1)). Each number in the\ntriplet is quantized and the concatenated value is treated as\nthe ï¬ngerprint.\n4.3.2 System Parameters\nFor this scheme, we adapt the implementation provided by\nEllis [7]. We optimize over a parametric space, and choose\nthe following set of parameters. The frequency data in the\nspectrogram is divided into 256 levels linearly. We con-\nsider neighboring peaks in an adjacent frequency range of\n64 units, and timing range of 64 units (sampling rate of the\naudio signal is set to 8 KHz). The values ((t2âˆ’t1), f1,(f2âˆ’\n803Oral Session 10\nf1))are represented as 6,8 and 6 bits respectively to ob-\ntain a 20 bit ï¬ngerprint. For this data set, the 20 bit ï¬n-\ngerprint works better than a 32-bit ï¬ngerprint suggested by\nWang in [18] - note that over quantization could affect per-\nformance adversely. We generate 20 ï¬ngerprints per second.\n5. EXPERIMENTAL RESULTS\nWe use our own data set as we were not able to ï¬nd any\npublicly available data sets captured from mobile phones.\nMost existing data sets introduce artiï¬cial distortions to the\naudio (e.g., adding noise), and are not representative of dis-\ntortions typical in the mobile scenario. We captured audio\nclips on a Nexus One phone from a set of 39 songs played\non TV and from laptop speakers in noisy environments. In\nour data collection, we tried to capture noise from differ-\nent ambient noise sources. Our song data set contains pop-\nular songs from artists like Lady Gaga, Michael Jackson,\nGreen Day, Avril Lavigne, to name a few. Each of these\nclips is between 60 and 90 seconds long, which we divide\ninto non-overlapping 5, 10 and 15 second snippets to use\nas query probes. This gives us a ground truth data set of\nover a 1000 pairs of query probes and their corresponding\nuncorrupted reference songs. All pairs between query and\nreference, both positive and negative examples, are consid-\nered to generate Receiver Operating Characteristic (ROC)\ncurves.\n5.1 Receiver Operating Characteristic\nWe evaluate the different ï¬ngerprinting schemes ï¬rst after\nthe ï¬ngerprint indexing step, and subsequently, the temporal\nalignment step.\n5.1.1 Fingerprint Indexing\nThe inverted index on the database enables fast retrieval and\nprovides a shortlist of candidates to be considered for a more\nextensive temporal alignment check. Each query ï¬ngerprint\nvotes for all the database ï¬ngerprints that it ï¬nds in the in-\nverted index. The similarity between the database song and\nquery song is the number of ï¬ngerprints in common be-\ntween them, based on the approximate-nearest-neighbor in-\ndexing strategy. For Ke, the similarity measure is the num-\nber of ï¬ngerprints that have <2 Hamming distance. For\nBaluja, the similarity measure is the number of ï¬ngerprints\nthat have >=1 matches in the 25 LSH bands. For Wang, the\nsimilarity measure is the number of 20-bit ï¬ngerprints that\nget hashed to the same bin.\nWe compute such a similarity score for matching and\nnon-matching pairs of ground-truth query and database songs,\nfor the different schemes. From these similarity scores, we\nform two histograms, one for matching pairs and one for\nnon-matching pairs, as illustrated in Fig. 3. The overlap-\nping between the two histograms depends on the ï¬ngerprint-\ning scheme, and more importantly, the length of the queryprobe. The longer the query probe, the lower the overlap be-\ntween the two histograms, and the better the performance of\nthe scheme. Also, the more discriminative the ï¬ngerprint,\nthe lower the overlap between the two histograms. From\nthe two histograms we obtain a Receiver Operating Char-\nacteristic (ROC) curve which plots correct match fraction\nagainst incorrect match fraction. The different points on the\nROC curve are obtained by adjusting the similarity measure\nthreshold. The higher the ROC curve, the more effective the\nretrieval system.\n10âˆ’410âˆ’310âˆ’210âˆ’110000.10.20.30.40.50.60.70.80.91\nIncorrect match factionCorrect match factionROC Curve\n  \nWang et al. (5)\nWang et al. (10)\nWang et al. (15)\nKe et al. (5)\nKe et al. (10)\nKe et al. (15)\nBaluja et al. (5)\nBaluja et al. (10)\nBaluja et al. (15)\nFigure 4 .ROC performance of different schemes. The number in brack-\nets is the length of the query probe in seconds. The performance of each\nï¬ngerprinting scheme increases as the query length increases. Balujaâ€™s\nscheme performs the best.\nWe plot the ROC performance of the three schemes in\nFig. 4. For each scheme, we note that the ROC performance\nimproves as the length of the query probe increases from 5\nto 15 seconds, as expected. Typically, the returns are dimin-\nishing beyond 10 seconds. Balujaâ€™s ï¬ngerprinting scheme\nperforms the best for all query probe lengths. The Min-\nHash based ï¬ngerprints (100 bytes each) are highly discrim-\ninative and capture information over a longer time-duration\nthan Wangâ€™s scheme.\nThe Wang ï¬ngerprints are far more compact - however,\nthe ï¬ngerprints are sensitive to small offsets in spectrogram\npeak localization. The low dimensionality of the ï¬nger-\nprint makes it less discriminative, causing the scheme to re-\nquire a longer probe to achieve a comparable performance to\nBalujaâ€™s scheme. Also, the lower dimensionality of the de-\nscriptor implies that it does not scale well as the size of the\ndatabase grows. As the length of the query probe increases\nto 15 seconds, Wangâ€™s scheme catches up in performance.\nFinally, we observe that Keâ€™s scheme performs poorly\nfor the short query probes that we are interested in. For\nKeâ€™s scheme to catch up in ROC performance, much longer\nprobes would be required. The scheme also suffers due to\nits dependence on the set of AdaBoost classiï¬ers used to\ngenerate the ï¬ngerprint. For our evaluation, we used the\nAdaBoost classiï¬ers provided by the authors in [13]. A\n80412th International Society for Music Information Retrieval Conference (ISMIR 2011)\n0 1000 2000 3000 400000.10.20.30.40.50.60.70.8\nSimilarityProbabilityDistribution of similarity scores of matching and nonâˆ’matching pairs\n  \nMatching pairs\nNonâˆ’matching pairs\n0 100 200 300 400 50000.10.20.30.40.50.60.70.8\nSimilarityProbabilityDistribution of similarity scores of matching and nonâˆ’matching pairs\n  \nMatching pairs\nNonâˆ’matching pairs\n0 50 100 150 20000.10.20.30.40.50.60.70.8\nSimilarityProbabilityDistribution of similarity scores of matching and nonâˆ’matching pairs\n  \nMatching pairs\nNonâˆ’matching pairs(Ke) (Baluja ) (Wang )\nFigure 3 .Distribution of scores for matching and non-matching pairs of query probe and reference songs illustrated for the different ï¬ngerprinting schemes.\nIdeally, we would like to have the matching pairs to have very high scores, and non-matching pairs to be exactly 0. The overlap in the distributions causes\nerrors in retrieval. This overlap depends on the discriminativeness of the ï¬ngerprinting scheme and also on the length of the query probe. Longer query probes\nprovide a better separation between the two distributions.\nmismatch between training and test data can affect the per-\nformance of this scheme adversely. We require robustness\nagainst a broad range of mobile environments and noise\nsources, and training a set of AdaBoost classiï¬ers for dif-\nferent environments is not practical.\n5.1.2 Temporal Alignment\nBased on computational resources available, accuracy re-\nquirements and the size of the database, retrieval systems\nchoose an operating point on the curve shown in Fig. 4.\nE.g., state-of-the-art retrieval systems would typically op-\nerate in the 80-90% True Positive Rate regime. At the oper-\nating point, we apply the Temporal Alignment (TA) scheme\nproposed by Wang to get rid of false positives. It is rela-\ntively easy to achieve high precision for audio retrieval ap-\nplications. By requiring a minimum number of ï¬ngerprint\nmatches to satisfy TA, we can get rid of most false positives.\nWe set the minimum number of temporally aligned matches\nto 5 for this experiment. We plot the percentage of queries\npassing the temporal alignment check as a function of query\nprobe length in Fig. 5. Again, we observe Balujaâ€™s scheme\nperforms the best, followed by Wang and Ke respectively.\nThe performance for each scheme improves as the length\nof the query probe increases. We conclude that highly dis-\ncriminative ï¬ngerprints help signiï¬cantly for short 5 second\nquery probes. Next, we study the amount of data generated\nfor each ï¬ngerprinting scheme.\n5 10 150.40.50.60.70.80.91\nQuery Probe Length(%) of query probes that pass TA\n  \nWang\nKe\nBaluja\nFigure 5 .Recall as a function of query probe length for different sc-\nhemes. Precision is 100% as the temporal alignment step eliminates false\npositives.5.2 Data Size and Transmission Delay\nThe different ï¬ngerprinting schemes generate different amo-\nunts of data. Here, we present results for a 10 second probe,\nas 10 second probes provide a balance between accuracy and\nlatency for all three schemes. Keâ€™s scheme produces 729\n4-byte ï¬ngerprints, Balujaâ€™s scheme produces 87 100-byte\nï¬ngerprints, and Wangâ€™s scheme produces 587 20-bit ï¬nger-\nprints on average for 10 second probes. The amount of data\ngenerated for the different schemes is shown in Fig. 7. We\ncompare the size of ï¬ngerprint data to the size of a 10 second\nVorbis compressed audio at 64 kbps (80 KB). We observe\nthat the size of ï¬ngerprint data is signiï¬cantly lower than\nthe size of the compressed audio for all ï¬ngerprinting sche-\nmes ( <10 KB). This motivates computing the ï¬ngerprints\non the device, whenever possible. We note that Wangâ€™s\nscheme produces less data than Balujaâ€™s or Keâ€™s scheme.\nFor a fair comparison between the different schemes, we\nplot the bitrate-Equal Error Rate (EER) performance in Fig-\nure 6. We note that the reduction in data for Wangâ€™s scheme\ncomes at the cost of ROC performance shown in Fig. 6.\nIf ï¬ngerprinting were to be done on the device, how long\nwould the transmission delay be for sending the ï¬ngerprint\ndata? The transmission delay would depend on the wireless\nnetwork used: 3G or WLAN (Wireless LAN). WLAN sys-\ntems provide much higher bandwidth compared to 3G, and\ntransmission delay is negligible even for large packet sizes.\nHere, we present transmission delay numbers only for a 3G\nconnection, as it is the most prevalent on mobile phones to-\nday [16]. For network transmission delay experiments, we\nuse the data presented in [10, 16]. The authors conduct ex-\nperiments in an AT&T 3G wireless network, with a total of\nmore than 5000 transmissions at locations where a typical\naudio retrieval system would be used.\nWe present the time it would take to transmit ï¬ngerprint\ndata for the different schemes in Fig. 7(b). Transmitting ï¬n-\ngerprint data takes in the order of a few seconds, while trans-\nmitting the compressed audio could take tens of seconds,\nbased on the wireless link. Note that the delay numbers\nshown here only represent the data transmission delay for\n805Oral Session 10\n0 5 10 150510152025303540\nBitrate (KB)Equal Error Rate (EER) (%)\n  \nWang\nKe\nBalujaFigure 6 .Equal Error Rate (EER) vs. bitrate tradeoff. Baluja scheme\nworks well at high bitrates, while Wangâ€™s scheme works well at low bi-\ntrates.\ndifferent ï¬ngerprinting schemes. The end-to-end system la-\ntency would depend on the streaming protocol, the length of\nquery probe considered, transmission delay and processing\ndelay on the server. Based on the experimental results pre-\nsented here and in [10], we would expect the transmission\ndelay to be the bottleneck in 3G networks, which motivates\ncomputing ï¬ngerprints on the device.\n01020304050607080Data Size (KB)\nWang Ke Baluja Audio\n(a)\n024681012141618Network Latency (s)\nWang Ke Baluja Audio (b)\nFigure 7 .Fig.(a) shows size of data generated by different schemes.\nFig.(b) shows the associated transmission delay if the data were to be trans-\nferred over a 3G network. The data and transmission delay numbers are\npresented for 10 second query probes. Data for 5 and 15 second probes can\nbe extrapolated linearly.\nFinally, we draw some parallels between mobile image\nretrieval and audio retrieval. We note that Ke and Baluja\nwere both inspired by work in computer vision literature.\nInterest point detectors and descriptors have been well stud-\nied in computer literature: readers are referred to the sur-\nvey papers [15, 19]. What has pushed the ï¬eld forward is\nthe availability of good image and patch level data sets that\ncapture the distortions (e.g., perspective and lighting in im-\nages) that interest point detectors and descriptors need to\nbe robust against. The availability of similar ground-truth\ndata sets will be useful for designing interest point detectors\nand descriptors for audio retrieval. Spectrogram peaks pro-\nposed by Wang is one example of interest point detection,\nbut other schemes need to be explored. Interest point de-\ntectors are the ï¬rst step in the pipeline, and improvements\nhere could affect blocks further down the pipeline. Next,\nwe note that the best descriptors in the vision literature are\nhigh-dimensional and capture salient characteristics in a lo-\ncal neighborhood around the interest point. In the case of\naudio retrieval, we need descriptors around interest points tobe robust against small timing offset errors, and distortions\nintroduced by ambient noise. Both interest point detectors\nand descriptors for audio retrieval in highly noisy environ-\nments are interesting areas for future work. We conclude by\nnoting that techniques and algorithms developed in recent\nimage retrieval literature can be used to further improve ef-\nï¬ciency and performance of audio retrieval systems.\n6. CONCLUSION\nWe perform a thorough survey and evaluation of popular\naudio ï¬ngerprinting schemes in a common framework. We\nreport and discuss results important for mobile applications:\nReceiver Operating Characteristic (ROC) performance, size\nof ï¬ngerprints generated compared to size of the compressed\naudio sample, transmission delay if the ï¬ngerprint data were\nto be transmitted over a 3G wireless link and computational\ncost of ï¬ngerprint generation.\n7. REFERENCES\n[1] Shazam Music Recognition Service .http://www.shazam.com/ .\n[2] SoundHound .http://www.soundhound.com/ .\n[3] S. Baluja and M. Covell. Audio ï¬ngerprinting: Combining computer vision and\ndata stream processing. In Proc. of IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP) , Honolulu, Hawaii, USA, April,2007.\n[4] S. Baluja and M. Covell. Content ï¬ngerprinting using wavelets. In Proc. of Euro-\npean Conference on Visual Media Production (CVMP) , London, UK, Nov,2006.\n[5] E. Cohen, M. Datar, S. Fujiwara, A. Gionis, P. Indyk, R. Motwani, J. D. Ullman,\nand C. Yang. Finding interesting associations without support pruning. In Proc.\nof the 16th International Conference on Data Engineering (ICDE) , 1999.\n[6] M. Covell and S. Baluja. Known audio detection using waveprint: Spectrogram\nï¬ngerprinting by wavelet hashing. In Proc. of IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP) , Honolulu, Hawaii, USA,\nApril,2007.\n[7] D. Ellis. Robust Landmark-Based Audio Fingerprinting .http://labrosa.\nee.columbia.edu/matlab/fingerprint/ .\n[8] M. Fink, M. Covell, and S. Baluja. Social and interactive-television applications\nbased on realtime ambient-audio identiï¬catio. In Proc. of European Conference\non Interactive TV (EuroITV) , Athens, Greece, 2006.\n[9] M. A. Fischler and R. C. Bolles. Random Sample Consensus: A paradigm for\nmodel ï¬tting with applications to image analysis and automated cartography.\nCommunications of ACM , 24(6):381â€“395, 1981.\n[10] B. Girod, V. Chandrasekhar, D. M. Chen, N. M. Cheung, R. Grzeszczuk,\nY. Reznik, G. Takacs, S. S. Tsai, and R. Vedantham. Mobile Visual Search. In\nProceedings of IEEE Signal Processing Magazine, Special Issue on Mobile Me-\ndia Search , 2010.\n[11] J. Haitsma and T. Kalker. A highly robust audio ï¬ngerprinting system. In Proc. of\nInternational Conference on Music Information Retrieval (ISMIR) , Paris, France,\n2002.\n[12] C. E. Jacobs, A. Finkelstein, and D. H. Salesin. Fast multiresolution image query-\ning. In Proc. of the 22nd annual Conference on Computer graphics and Interac-\ntive Techniques (SIGGRAPH) , pages 277â€“286, New York, NY, USA, 1995. ACM.\n[13] Y. Ke, D. Hoiem, and R. Sukthankar. Software .http://www.cs.cmu.edu/\nËœyke/musicretrieval/ .\n[14] Y. Ke, D. Hoiem, and R. Sukthankar. Computer vision for music identiï¬cation. In\nProc. of IEEE Conference on Computer Vision and Pattern Recognition (CVPR) ,\nSan Diego, USA, June,2005.\n[15] K. Mikolajczyk and C. Schmid. Performance evaluation of local descriptors.\nIEEE Transactions on Pattern Analysis and Machine Intelligence , 27(10):1615â€“\n1630, 2005.\n[16] S. S. Tsai, D. M. Chen, V. Chandrasekhar, G. Takacs, N. M. Cheung, R. Vedan-\ntham, R. Grzeszczuk, and B. Girod. Mobile Product Recognition. In Proc. of ACM\nMultimedia (ACM MM) , Florence, Italy, October 2010.\n[17] A. Wang. The shazam music recognition service. Communications of the ACM ,\n49(8):44â€“48, 2006.\n[18] A. Wang. An industrial-strength audio search algorithm. In Proc. of International\nConference on Music Information Retrieval (ISMIR) , Baltimore, Maryland, USA,\nOctober,2003.\n[19] S. Winder and M. Brown. Learning Local Image Descriptors. In Proc. of IEEE\nConference on Computer Vision and Pattern Recognition (CVPR) , pages 1â€“8,\nMinneapolis, Minnesota, 2007.\n806"
    },
    {
        "title": "Music Structural Segmentation by Combining Harmonic and Timbral Information.",
        "author": [
            "Ruofeng Chen",
            "Ming Li"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1414774",
        "url": "https://doi.org/10.5281/zenodo.1414774",
        "ee": "https://zenodo.org/records/1414774/files/ChenL11.pdf",
        "abstract": "We propose a novel model for music structural segmentation aiming at combining harmonic and timbral information. We use two-level clustering with splitting initialization and random turbulence to produce segment labels using chroma and MFCC separately as feature. We construct a score matrix to combine segment labels from both aspects. Finally Nonnegative Matrix Factorization and Maximum Likelihood are applied to extract the final segment labels. By comparing sparseness, our method is capable of automatically determining the number of segment types in a given song. The pairwise F-measure of our algorithm can reach 0.63 without rules of music knowledge, running on 180 Beatles songs. We show our model can be easily associated with more sophisticated structural segmentation algorithms and extended to probabilistic models.",
        "zenodo_id": 1414774,
        "dblp_key": "conf/ismir/ChenL11",
        "keywords": [
            "harmonic",
            "timbral",
            "segmentation",
            "chroma",
            "MFCC",
            "feature",
            "Nonnegative Matrix Factorization",
            "Maximum Likelihood",
            "sparseness",
            "pairwise F-measure"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nMUSIC STRUCTURAL SEGMENTATION BY COMBINING\nHARMONIC AND TIMBRAL INFORMATION\nRuofeng Chen\nGeorgia Tech Center for Music Technology\nGeorgia Institute of Technology\nruofengchen@gatech.eduMing Li\nInstitute of Acoustics\nChinese Academy of Sciences\nliming@mail.ioa.ac.cn\nABSTRACT\nWe propose a novel model for music structural segmentation\naiming at combining harmonic and timbral information. We\nuse two-level clustering with splitting initialization and ran-\ndom turbulence to produce segment labels using chroma and\nMFCC separately as feature. We construct a score matrix to\ncombine segment labels from both aspects. Finally Non-\nnegative Matrix Factorization and Maximum Likelihood are\napplied to extract the ï¬nal segment labels. By comparing\nsparseness, our method is capable of automatically deter-\nmining the number of segment types in a given song. The\npairwise F-measure of our algorithm can reach 0.63 without\nrules of music knowledge, running on 180 Beatles songs.\nWe show our model can be easily associated with more so-\nphisticated structural segmentation algorithms and extended\nto probabilistic models.\n1. INTRODUCTION\nIdentifying music structural segmentation is one of the most\nimportant and difï¬cult problems in music information re-\ntrieval (MIR). Its goal is to automatically locate the musi-\ncally repetitive parts within a piece of music (e.g. verse,\nbridge and chorus in popular music). It has applications\nsuch as music thumbnail, segment-based editing and segment-\nbased navigation. It may also facilitate other MIR tasks like\nbeat tracking and chord detection.\nThere are some noteworthy existing systems, which in-\nspire our proposed model. Foote [1] proposed self-similarity\nmatrix for structure representation. Levy et al [2] proposed a\ntwo-level model for structural segmentation problem. In the\nThis work was performed while interning at Institute of Acoustics,\nChinese Academy of Sciences.\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proï¬t or commercial advantage and that copies\nbear this notice and the full citation on the ï¬rst page.\nc/circlecopyrt2011 International Society for Music Information Retrieval.lower level, they introduced Hidden Markov Models (HMMs)\nto quantize audio feature vectors into discrete states; in the\nupper level, they formed histograms by counting the HMM\nstates in local windows and designed a clustering algorithm\nto quantize histogram vectors into segment labels. Weiss at\nel [3] showed the potential of Non-negative Matrix Factor-\nization (NMF) in the structural segmentation problem. No-\ntably, they make use of sparseness constraint to automat-\nically determine the number of segment types in a song.\nKaiser et al [4] exploited NMF on self-similarity matrix and\nclustering to differentiate segment types.\nUndoubtedly, music structure is perceived based on many\nsources of information, among which harmony and timbre\nare primary players. Some existing systems use multiple\nfeatures as starting points, listed in [5], but few found a good\nmodel to combine them. As is shown in [4], combining har-\nmonic and timbral information works even worse than using\ntimbral information alone. In this paper, we focus on build-\ning a model to combine the two sources to reach higher seg-\nmentation performance.\nOur model is comprised of two parts. The ï¬rst part is a\ntwo-level clustering algorithm, which produces segment la-\nbels using either harmonic or timbral information. The sec-\nond part is a novel algorithm to bring segment labels from\ntwo different aspects together into a score matrix, and ex-\nploiting NMF to extract segment labels and sparseness to\nautomatically determine the number of segment types in a\ngiven song. We call the score matrix and NMF based algo-\nrithm SM-NMF for short.\nIn Section 2 we describe our two-level clustering algo-\nrithm. In Section 3 we describe the SM-NMF algorithm. In\nSection 4 we present our experimental results and explain\nfor them. In Section 5 we introduce possible extension of\nour model in future research. For convenience, we deï¬ne the\nfollowing symbols that will be used in the paper. n: number\nof frames in a song, each corresponds to a state label. m:\ndimension of feature vectors. N: number of windows in a\nsong, each corresponds to a segment label. k: number of\nsegment types.\n477Poster Session 4\n2. TWO-LEVEL CLUSTERING\nOur two-level clustering algorithm is shown in Figure 1. From\nthe feature extraction module we get frame-based feature\nvectors used for the lower-level feature clustering module,\nwhich quantizes feature vectors into states. The histogram\nmodule counts states in windows and forms histogram vec-\ntors used for the higher-level histogram clustering module,\nwhich quantizes histogram vectors into segment labels. This\nalgorithm is similar to [2], except that we substitute HMMs\nwith another clustering and no constraint is imposed.\nfeature \nextraction  \nfeature \nextraction \nfeature \nclustering histogram histogram \nclustering two-level clustering \n  \n    \n       \n      frame \nframe window \nwindow \nFigure 1 . The ï¬‚owchart and illustration of intermediate re-\nsults of two-level clustering. The lower two graphsâ€™ colors\nonly illustrate different labels for better looking.\n2.1 Feature Extraction\nWe extract two types of vector features separately from au-\ndio ï¬les. Chroma is a 12-dimension representation indicat-\ning the power within each of the 12 pitch classes. So chroma\nhas a close relationship with the harmonic characteristics of\nmusic. See [6] for algorithm of extracting chroma. Mel-\nfrequency Cepstrum Coefï¬cients (MFCC) is usually a 13-\ndimension representation describing the spectral envelope.\nIt is easy to calculate and potential to reveal timbral simi-\nlarity in feature space. See [7] for algorithm of extracting\nMFCC.\nWe divide the whole song with ï¬xed frame length of Lf\nms and hop size of Lfhms, then calculate feature for each\nframe.\n2.2 Clustering Algorithm\nClustering is a process of gathering points in the feature\nspace to a ï¬xed number of clusters so that hopefully neigh-\nboring points would have the same cluster label. K-means\nis one of the most straightforward algorithms to perform\nclustering [8]. Firstly, a ï¬xed number of kcluster centers\nÂµ1,Âµ2...Âµ kare initialized, often randomly. Then two stepsalternate iteratively: a) assign each point xjto its closest\ncluster center; b) recalculate each cluster center, until the\nobjective function\nG(x,Âµ) =k/summationdisplay\ni=1/summationdisplay\nxjâˆˆCiDistanceMeasure (xj,Âµi)\nconverges, where Ciis the set of feature vectors assigned to\ntheith cluster. Note that k-means is the coordinate descent\nofG(x,Âµ)so only local minimum is guaranteed.\nIn our experiments, we ï¬nd that using uniform distribu-\ntion to randomly initialize cluster centers sometimes con-\nverges to unreasonable local minima, so we apply an â€œini-\ntial guess by splittingâ€ method described in [9] instead. If\nthe target number of clusters is not power of 2, we split the\ncluster with largest variance until we achieve the right num-\nber. We ï¬nd that using this technique most unreasonable\nresults are avoided.\nWe have described one level of clustering. Now we move\nto two-level clustering. Firstly, we perform clustering on ei-\nther chroma or MFCC into klclusters, using Euclidean dis-\ntance as distance measure, to obtain a state label for each\nframe, which can be interpreted as harmonic unit or timbre\nunit. Then we slide a window with length of Lwframes\nand hop size of Lwhframes throughout the whole song, and\ncount the occurrence of every state. Now we have an ar-\nray of histogram vectors, which are further normalized to\nbe probabilistic. We perform clustering on histogram vec-\ntors intokhclusters, using symmetric Kullback-Leiber (KL)\ndivergence [10] as distance measure.\nKL(P||Q) =1\nLwkl/summationdisplay\ni=1Pilog2Pi\nPi+Qi+Qilog2Qi\nPi+Qi\nSymmetric KL divergence describes how dissimilar Pand\nQare to the assumed actual distribution (P+Q)/2. The\nresulting labels indicate segment types.\nTo further reduce G(x,Âµ), we insert a random turbulence\nmodule between splitting initialization and two-step itera-\ntion, for both levels of clustering. To do this, we add a vector\nwith tiny norm and random direction to each cluster center.\nMake sure the shifted centers satisfy probability constraints\nfor KL divergence. Then we perform clustering for Ttimes\nto getTslightly different solutions. We can pick out the\nsolution with lowest G(x,Âµ).\nIn our experiments, we notice in most cases the solutions\nwith lowest G(x,Âµ)do not necessarily correspond to good\nresults (see Table 2 for results). Therefore, to further im-\nprove the performance, we have to keep all Tsolutions for\nfurther analysis.\n47812th International Society for Music Information Retrieval Conference (ISMIR 2011)\n3. COMBINING HARMONIC AND TIMBRAL\nINFORMATION\nIn this section, we describe how to combine harmonic and\ntimbral information, i.e. the two-level clustering results from\nchroma and MFCC, to produce better segmentation results.\nFor convenience, we name the segment labels produced by\nchroma as chroma solution. Similarly we have MFCC so-\nlution. We name the segment labels produced by the SM-\nNMF algorithm described below as ï¬nal solution.\nTo motivate our idea, we show the typical results from\nchroma and MFCC respectively in Figure 2. Although both\nfeatures produce fair results (pairwise F-measure 0.61 and\n0.62), they are from completely different perspectives. For\nexample, the chroma solution fails to distinguish verse and\nverse(instrumental) because the underlying harmonic pat-\nterns are exactly the same, but is good at distinguishing\nverse and bridge because of different harmonic patterns; the\nMFCC solution separates verse(instrumental) successfully\nbecause the timbre in this segment is very different from oth-\ners, but cannot distinguish verse and bridge for their similar\ntimbres.\nTherefore, we set up the following rule for combination:\ntwo windows should have identical segment labels only if\nthe two windows are both harmonically and timbrally simi-\nlar. However, we cannot simply mix a chroma solution and\nan MFCC solution because segments from two aspects usu-\nally do not have common boundaries. There will often be\nlots of fragments in the outcoming results. In order to obtain\na result with the same level of detail as chroma or MFCC so-\nlution, we make use of all Tchroma solutions and TMFCC\nsolutions to smooth the boundaries. Now we describe how\nto bring all 2Tsolutions into one ï¬nal solution.\nchroma \nMFCC \nintro  verse     bridge half  verse    bridge      verse      bridge  outro intro (instrumental) \nFigure 2 . The results of clustering using chroma and MFCC\nrespectively, along with the ground truth, of â€œIn My Lifeâ€.\n3.1 Score Matrix\nBy analyzing Tdifferent chroma or MFCC solutions from\nclustering with random turbulence, we ï¬nd that typically\nsome pairs of windows always have identical labels. Thesewindows are lying within steady regions of a song. By con-\ntrast, some pairs occasionally have identical labels. Then ei-\nther of them is lying within boundary regions (for example\nthe short transition between segments with complicated in-\nstrumentation changes). Therefore, counting the times two\nwindows having identical labels can reveal the steady re-\ngions and boundary regions in a song. We can construct a\nscore matrix to describe how likely it is for two windows to\nhave identical labels. This idea can be directly extended to\na score matrix describing how likely it is for two windows\nto have identical labels in both chroma solution and MFCC\nsolution.\nTo implement this, initialize an NÃ—Nmatrix with all ze-\nros. Perform two-level clustering using chroma and MFCC\nas feature separately, with splitting initialization and random\nturbulence, for Ttimes. Then investigate all the T2chroma-\nMFCC solution pairs: If the ith andjth windows in both\nchroma solution and MFCC solution have identical labels,\nthe corresponding element in the score matrix increases by\none. Finally, normalize all the elements by dividing by T2.\nThe resulting score matrix serves the same purpose of\nvisualizing music structure as Footeâ€™s self-similarity matrix,\nbut the score matrix is much more well-structured and smooth.\nSee Figure 3 for a graphical example.\nFigure 3 . The score matrix of â€œHelp!â€. The same songâ€™s\nself-similarity matrix is shown in [4].\n3.2 Non-Negative Matrix Factorization\nWe can view the score matrix as an array of column vec-\ntors. Each vector corresponds to a window. Suppose we\nhave a set of vector templates. Vectors in the steady regions\nof a song may be directly found in the set, while vectors in\nthe boundary regions may be approximated by linear com-\nbination of vector templates. This observation pushes us to\nNon-negative Matrix Factorization (NMF) [11].\nTheNÃ—Nscore matrix is approximately factorized into\nproduct of a NÃ—kmatrix Wand akÃ—Nmatrix H. The\njth column of Wcan be viewed as the vector template for\nthejth segment type. The jth column of Hdescribes the\nintensities of the ksegment types for the jth window. An\nexample in shown in Figure 4.\n479Poster Session 4\nWe implement NMF using the multiplicative update rules\n[11]. Similar to clustering, NMF can only guarantee a local\nminimum of the sum of errors between the score matrix and\nWÃ—H. So we run NMF for several times with uniformly\ndistributed random initialization and pick out the factoriza-\ntion result with lowest sum of errors.\nAfter we obtain H, we apply Maximum Likelihood by\nassigning the segment label associated with the largest en-\nergy to each window. Note that in [4], clustering was used\nfor the same purpose. In our experiment, we ï¬nd that clus-\ntering and Maximum Likelihood produce almost the same\nperformance. We choose Maximum Likelihood because itâ€™s\nsimpler and more consistent.\n  \nscore matrix ( Ã— )   ( Ã— )   ( Ã— ) \nFigure 4 . The score matrix is approximately factorized into\nthe product of WandH, from â€œDrive My Carâ€.\n3.3 Automatic Determination of the Number of\nSegment Types\nAutomatically determining of the number of segment types\nin a song is hard for two-level clustering, because clustering\nis a process of hard decision and all information about a win-\ndow is its associated cluster label. However, using NMF, we\nhave the matrix Hwhose columns involve intensities of all\nsegment types. An example is shown in Figure 5. Intuitively\none will agree k= 3is the optimal number of segment types\nbecause the Hwithk= 3 is the most â€œresoluteâ€ one with\nleast windows having much energy spread into multiple seg-\nment types. So we want a measure to quantify how much\nenergy of a column is concentrated in as few components\nas possible. Sparseness [12] is a good measure which can\nsatisfy the need.\nsparseness (h) =âˆš\nkâˆ’(/summationtext|hi|)//radicalBig/summationtexthi2\nâˆš\nkâˆ’1\nwhere his a column of H. The sparseness listed in Figure 5\nis the average sparseness of all Ncolumns. We hope the\ncolumns of Hto be as sparse as possible, so we factorize\nthe score matrix with different k, then we pick out the H\nwith largest average sparseness.\nTo summarize, we show the whole process of our model\nin Figure 6.\n =2,           =0.789  \n =3,           =0.939  \n =4,           =0.881  \n =5,          =0.879 (optimal)  Figure 5 . Obtaining Hwith different k, we can use the\nresult with largest average sparseness.\nfeature \nextraction \nfeature \nextraction two-level \nclustering  two-level \nclustering two-level \nclustering  two-level \nclustering two-level \nclustering  two-level \nclustering two-level \nclustering  two-level \nclustering â‹± â‹± â‹± \n  1 \n2 \ntwo-level \nclustering  two-level \nclustering two-level \nclustering  two-level \nclustering two-level \nclustering  two-level \nclustering two-level \nclustering  two-level \nclustering â‹± â‹± â‹± \n  1 \n2 score \nmatrix NMF \nNMF \nNMF \nwith splitting initialization \nand random turbulence with different   \nlargest \nsparseness chroma \nMFCC   \nMaximum Likelihood \nlabels:  \nfinal solution labels:  \nchroma solutions \nlabels:  \nMFCC solutions \nFigure 6 . The complete ï¬‚owchart of our proposed model.\nSee Figure 1 for detail of two-level clustering.\n4. EV ALUATION\n4.1 Parameters Conï¬guration\nWe describe how to set up parameters (shown in Table 1)\nfor two-level clustering. LfandLfhare set by assuming\nthe audio signal is stationary for all frequency components\nin this short time duration. klshould be set a large num-\nber according to [2]. In our experiment, we see kl= 64\nworks best. LwandLwhare not affecting the performance\n(pairwise F-measure) much, except that too small Lwmight\nmake a very short segment longer than its actual length.\nkhcan be viewed as the number of types of harmoni-\ncally similar segment or timbrally similar segment. kh= 3\nis a reasonable number, because a typical song has about 3\nharmonic patterns (such as intro, verse and bridge) and also\nabout 3 timbral patterns (such as intro, verse/bridge and in-\nstrument solo).\nframe length Lf 100 ms\nframe hop size Lfh 50 ms\n# of stateskl 64\nslide window length Lw 10 s\nslide window hop size Lwh 1 s\n# of segment types kh 3\n# of loopsT 7\nTable 1 . Parameters used in two-level clustering.\n48012th International Society for Music Information Retrieval Conference (ISMIR 2011)\n4.2 Overall Results\nOur database comprises 180 Beatlesâ€™ songs, consistent with\nthe available ground truth annotations in Isophonics1. All\nsongs are in the wav format of 16kHz/16bit/mono. We eval-\nuate pairwise F-measure (PFM) [2] of our algorithms on the\nwhole database referencing Isophonics annotations. Clus-\ntering processes with chroma and MFCC share the same set\nof parameters in Table 1.\nTable 2 shows the PFM of two-level clustering with split-\nting initialization and without random turbulence, and the\nPFM of running two-level clustering with splitting initial-\nization and random turbulence for Ttimes and minimiza-\ntion with regard to G(x,Âµ).kcannot be automatically de-\ntermined in clustering, so we ï¬x k= 3, which can produce\nhighest PFM in our experiments.\nchroma MFCC\nrandom no yes no yes\nPFM 0.58 0.58 0.58 0.60\nTable 2 . PFM of two-level clustering with and without ran-\ndom turbulence.\nWe note that minimizing with regard to G(x,Âµ)can re-\nduceG(x,Âµ)dramatically, but not necessarily improve PFM.\nSo the relationship between PFM and G(x,Âµ)is not straight-\nforward.\nThen we evaluate our proposed SM-NMF algorithm. kis\nautomatically selected from {3,4,5}according to the largest\nsparseness in the corresponding H. We note that although\nmany songs have more than 5 segment types according to\nannotations, such as the one shown in Figure 2, intro and\nhalf-intro are both harmonically and timbrally identical so\nit is impossible to discriminate them using only harmonic\nand timbral information. Therefore it is normal that the au-\ntomatically determined kis smaller than the actual number\nof segment types in annotations. In Table 3, besides SM-\nNMF, we also show the results using NMF with ï¬xed kfor\ncomparison. We see that SM-NMF produces better results\nthan two-level clustering (Table 2) and sparseness is a good\nmeasure for the number of segment types.\nï¬xk automatically\nk= 3k= 4k= 5 determinek\nPFM 0.62 0.62 0.61 0.63\nTable 3 . PFM of SM-NMF.\nIn Table 4 we show the results of a different way to form\nscore matrix â€“ by counting how many times two windows\nhave identical labels using only one type of feature. The\nresults indicate it is combining harmonic and timbral infor-\n1www.isophonics.netmation that actually makes the main contribution to the per-\nformance of SM-NMF.\nonly chroma only MFCC both\nPFM 0.59 0.61 0.63\nTable 4 . Forming score matrix with either harmonic or tim-\nbral information versus both information.\nFinally, in Table 5, we compare our results with other\nstate-of-the-art methods, which use the same Isophonics an-\nnotation, listed in [3]. To be more informative, we also\nlist pairwise precision rate (PPR) and pairwise recall rate\n(PRR).\nSystem PFM PPR PRR\nMauch et al [13] 0.66 0.61 0.77\nSM-NMF 0.63 0.61 0.69\nWeiss et al [3] 0.60 0.58 0.68\nLevy et al [2] 0.54 0.58 0.53\nTable 5 . Segmentation performance of SM-NMF and other\nstate-of-the-art methods on the Beatles data set.\nOur algorithm does not involve any post-processes based\non music knowledge such as eliminating too short segments\nor restricting segment length to multiples of 4 beats [13].\nThese rules can help reduce fragments, so we can expect\nour algorithm to produce higher PRR, and thus higher PFM,\nif we consider them.\n4.3 Case Study\nWe study an example shown in Figure 7. In the chroma\nsolution, we see that a verse is oversegmented into three\nsegments (blue, red, green). We see in the score matrix\nthat the red-labeled segment is tolerated in larger boxes but\nthe green-labeled segment is not. This is because the red-\nlabeled segment is a correctable mistake produced by some\nunstable clustering results, while the green-labeled segment\nis an uncorrectable mistake produced by the interference\nfrom heavy drumming. In the MFCC solution, we see that\nthe ï¬rst and second verse are given different label from the\nthird and fourth verse. This is produced by the differences\nin background choir, by which MFCC solution is conï¬dent\nthat they have two distinct timbres. So we see in the score\nmatrix the upper left four large boxes are completely sep-\narated from the lower right four large boxes. The ï¬nal so-\nlution will hide all correctable mistakes but display all un-\ncorrectable mistakes. Therefore, SM-NMF performs well\nwhen the front-end structural segmentation algorithm (two-\nlevel clustering for this paper) makes as few uncorrectable\nmistakes as possible.\n481Poster Session 4\nintro  verse    verse   bridge   verse  bridge  verse outro \n  \nFigure 7 . Example:â€œYou Wonâ€™t See Meâ€. The last four la-\nbels are respectively ï¬nal solution, chroma solution, MFCC\nsolution and ground truth.\n5. SUMMARY AND FUTURE WORKS\nWe have described a novel model for music structural seg-\nmentation, to bring the results of two-level clustering using\nchroma and MFCC separately into one ï¬nal solution, aim-\ning at combining harmonic and timbral information. We\nuse splitting initialization and random turbulence to pro-\nduce slightly different chroma and MFCC solutions from\ntwo-level clustering. Then we construct a score matrix to\nexhibit the pairwise relation between chroma solutions and\nMFCC solutions. We apply NMF and Maximum Likelihood\nto reveal music structure and sparseness to automatically de-\ntermine the number of segment types in a given song. The\nPFM of our proposed SM-NMF method outperforms two-\nlevel clustering using single feature.\nThere is lots of space for improvement. We have shown\nin Section 4.3 that one obstacle in SM-NMF method is the\nreliability of solutions of the front-end algorithm. The two-\nlevel clustering can be replaced by any structural segmen-\ntation algorithm as long as random turbulence is included\nto produce slightly different solutions. We note that the re-\nliability is not equivalent to the value of PFM, because for\nexample we cannot expect MFCC alone to identify harmon-\nically different segments or discriminate intro and half-intro.\nWe need ground truth directly related to harmonically simi-\nlar segments or timbrally similar segments.\nBesides, NMF might produce better results with someconstraints exploiting symmetry and sparsity. The score ma-\ntrix is a ï¬‚exible representation, which might be associated\nwith probabilistic models. For example, if we view the score\nmatrix as a â€œterm frequency-inverse document frequency (tf-\nidf)â€ matrix, we might make use of Probabilistic Latent Se-\nmantic Analysis [14] to give a more elegant algorithm. We\nmight also introduce constraints such as segment length and\ninter-segment transition probabilities to produce more mu-\nsically meaningful results.\n6. REFERENCES\n[1] J. Foote: â€œVisualizing Music and Audio using Self-\nSimilarityâ€, ACM Multimedia , pp. 77â€“80, 1999.\n[2] M. Levy, M. Sandler: â€œStructural Segmentation of Mu-\nsical Audio by Constrained Clusteringâ€, IEEE Trans-\nactions on Audio, Speech, and Language Processing ,\n16(2):318326, 2008.\n[3] R.J.Weiss, J.P.Bello: â€œIdentifying Repeated Patterns in\nMusic Using Sparse Convolutive Non-Negative Matrix\nFactorizationâ€, ISMIR , Utrecht, Netherlands, 2010.\n[4] F.Kaiser, T.Sikora: â€œMusic Structure Discovery in Pop-\nular Music Using Non-Negative Matrix Factorizationâ€,\nISMIR , Utrecht, Netherlands, 2010.\n[5] J.Paulus, M.Muller, A.Klapuri: â€œAudio-Based Music\nStructure Analysisâ€, ISMIR , Utrecht, Netherlands, 2010.\n[6] M. Goto: â€œA Chorus-Section Detecting Method for\nMusical Audio Signalsâ€, In Proc. ICASSP ,V-437-440,\n2003.\n[7] B. Logon: â€œMel Frequency Cepstral Coefï¬cients for\nMusic Modelingâ€, ISMIR , 2000.\n[8] C.M. Bishop: â€œPattern Recognition and Machine Learn-\ningâ€, Springer , 2006.\n[9] Y . Linde, A. Buzo, R.M. Gray: â€œAn Algorithm for Vec-\ntor Quantizer Designâ€, IEEE Transactions on Commu-\nnications , V ol.Com-28, No.1, January, 1980.\n[10] S. Abdallah, M. Sandler, C. Rhodes, M. Casey: â€œUs-\ning Duration Models to Reduce Fragmentation in Audio\nSegmentationâ€, Mach Learn , 65:485-515, 2006.\n[11] D. D. Lee, H. S. Seung: â€œAlgorithms for Non-negative\nMatrix Factorizationâ€, Advances in Neural Information\nProcessing Systems , 2001.\n[12] P. O. Hoyer: â€œNon-negative Matrix Factorization with\nSparseness Constraintsâ€, Journal of Machine Learning\nResearch 5 , 1457-1469, 2004.\n[13] M. Mauch, K. C. Noland, and S. Dixon: â€œUsing Musical\nStructure to Enhance Automatic Chord Transcriptionâ€,\nProc. ISMIR , pages 231236, 2009.\n[14] T. Hofmann: â€œProbabilistic Latent Semantic Analysisâ€,\nUncertainty in Artiï¬cial Intelligence , 1999.\n482"
    },
    {
        "title": "An Acoustic-Phonetic Approach to Vocal Melody Extraction.",
        "author": [
            "Yu-Ren Chien",
            "Hsin-Min Wang",
            "Shyh-Kang Jeng"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1416450",
        "url": "https://doi.org/10.5281/zenodo.1416450",
        "ee": "https://zenodo.org/records/1416450/files/ChienWJ11.pdf",
        "abstract": "This paper addresses the problem of extracting vocal melodies from polyphonic audio. In short-term processing, a timbral distance between each pitch contour and the space of human voice is measured, so as to isolate any vocal pitch contour. Computation of the timbral distance is based on an acousticphonetic parametrization of human voiced sound. Longterm processing organizes short-term procedures in such a manner that relatively reliable melody segments are determined first. Tested on vocal excerpts from the ADC 2004 dataset, the proposed system achieves an overall transcription accuracy of 77%.",
        "zenodo_id": 1416450,
        "dblp_key": "conf/ismir/ChienWJ11",
        "keywords": [
            "vocal melodies",
            "polyphonic audio",
            "timbral distance",
            "pitch contour",
            "human voice",
            "acousticphonetic parametrization",
            "voiced sound",
            "longterm processing",
            "melody segments",
            "ADC 2004 dataset"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nAN ACOUSTIC-PHONETIC APPROACH TO VOCAL MELODY\nEXTRACTION\nYu-Ren Chien,1,2Hsin-Min Wang,2Shyh-Kang Jeng1,3\n1Graduate Institute of Communication Engineering, National Taiwan University, Taiwan\n2Institute of Information Science, Academia Sinica, Taiwan\n3Department of Electrical Engineering, National Taiwan Uni versity, Taiwan\nyrchien@ntu.edu.tw ,whm@iis.sinica.edu.tw ,skjeng@ew.ee.ntu.edu.tw\nABSTRACT\nThis paper addresses the problem of extracting vocal melodi es\nfrom polyphonic audio. In short-term processing, a timbral\ndistance between each pitch contour and the space of human\nvoice is measured, so as to isolate any vocal pitch contour.\nComputation of the timbral distance is based on an acoustic-\nphonetic parametrization of human voiced sound. Long-\nterm processing organizes short-term procedures in such a\nmanner that relatively reliable melody segments are deter-\nmined ï¬rst. Tested on vocal excerpts from the ADC 2004\ndataset, the proposed system achieves an overall transcrip -\ntion accuracy of 77%.\n1. INTRODUCTION\nMusic lovers have always been faced with a large collec-\ntion of music recordings or concert performances for them\nto choose from. While successful choices are possible with\na small set of metadata, disappointment still recurs becaus e\nthe metadata only provides limited information about the\nmusical contents. This has motivated researchers to work\non systems that extract essential musical information from\naudio recordings. Hopefully, such systems will enable per-\nsonalized recommendations for music purchase decisions.\nIn this paper, we focus on the extraction of vocal melodies\nfrom polyphonic audio signals. A melody is deï¬ned as a\nsuccession of pitches and durations; as one might expect,\nmelodies represent the most signiï¬cant piece of informatio n\namong all the features one can identify from a piece of mu-\nsic. In various musical cultures including popular music in\nparticular, predominant melodies are commonly carried by\nsinging voices. In view of this, this work aims at analyzing a\nPermission to make digital or hard copies of all or part of this w ork for\npersonal or classroom use is granted without fee provided th at copies are\nnot made or distributed for proï¬t or commercial advantage and th at copies\nbear this notice and the full citation on the ï¬rst page.\nc/circlecopyrt2011 International Society for Music Information Retrieval .singing voice accompanied by musical instruments. Instru-\nmental accompaniment is common in vocal music, where\nthe main melodies are exclusively carried by a solo singing\nvoice, with the musical instruments providing harmony. In\nbrief, the goal of the analysis considered in this work is\nï¬nding the fundamental frequency of the singing voice as\na function of time.\nThe speciï¬c problem outlined above is challenging be-\ncause melody extraction is prone to interference from the\naccompaniment unless a mechanism is in place for distin-\nguishing human voice from instrumental sound. [6], [13],\nand [9] determined the predominant pitch as it accounts for\nthe most of the signal power among all the simultaneous\npitches. The concept of pitch predominance is also pre-\nsented in [12] and [2], which deï¬ned the predominance in\nterms of harmonicity. For these methods, the problem proves\ndifï¬cult whenever the signal is dominated by a harmonic\nmusical instrument rather than by the singing voice. [3] and\n[5] realized the timbre recognition mechanism by classiï¬-\ncation techniques; on the other hand, pitch classiï¬cation\nentails quantization of pitch, which in turn causes loss of\nsuch musical information as vibrato, portamento, and non-\nstandard tuning.\nThe contribution of this paper is an acoustic-phonetic\napproach to vocal melody extraction. To make judgments\nabout whether or not each particular pitch contour detected\nin the polyphonic audio is vocal, we measure a timbral dis-\ntance between the pitch contour and a space of human voiced\nsound derived from acoustic phonetics [4]. In this space,\nhuman voiced sound is parameterized by a small number of\nacoustic phonetic variables, and the timbral distance from\nthe space to any harmonic sound can be efï¬ciently estimated\nby a coordinate descent search that ï¬nds the minimum dis-\ntance between a point in the space and the point representing\nthe harmonic sound.\nThe proposed method offers practical advantages over\nprevious approaches to vocal melody extraction. By im-\nposing acoustic-phonetic constraints on the extraction, t he\nproposed method can better distinguish human voice from\n25Oral Session 1: Melody and Singing\ninstrumental sound than the predominant pitch estimators i n\n[2, 6, 9, 12, 13]. Furthermore, with pitch contours composed\nof continuous sinusoidal frequency estimates taken from in -\nterpolated spectra, the proposed method is free from the\nquantization errors in pitch estimation that are commonly\nencountered by classiï¬cation-based systems [3, 5].\nFigure 1 . Short-term processing for vocal melody extrac-\ntion. The goal is to extract a vocal pitch contour around\ntime point tfrom the polyphonic audio. TDM stands for\ntimbral distance measurement.\n2. OVERVIEW OF SHORT-TERM PROCESSING\nIn this section, we consider the problem of extracting a vo-\ncal pitch contour around time point tfrom the polyphonic\naudio, provided that a singing voice exists at t. As shown in\nFigure 1, the extraction proceeds in three steps: 1) detect-\ning pitch contours that each start before and end after t, 2)\nmeasuring the timbral distance between each of the detected\ncontours and the space of human voiced sound, and 3) ex-\ntracting the most salient pitch contour among any detected\ncontours that lie in the space of human voiced sound.\nIn particular, the pitch contours simultaneously detected\nin Step 1 form a set of candidates for the vocal pitch contour.\nIf exactly one vocal exists at this moment, then the vocal\ncontour may be identiï¬ed by timbre. Timbral distance mea-\nsurement is intended here to provide the timbral informatio n\nessential to the identiï¬cation. In contrast to frame-based\nprocessing, here the duration of processing depends on how\nfar pitches can actually be tracked continuously away from\ntin the analyzed audio. At the frame rate of 100 frames per\nsecond, it is observed that most pitch contours last for more\nthan 10 frames; obviously, one would expect more reliable\ntimbral judgments from contour-based processing than from\nframe-based processing.\n3. PITCH CONTOUR DETECTION\nIn this section, we describe the procedure for detecting pit ch\ncontours around time point tfrom the polyphonic audio. It\nstarts by detecting multiple pitches from the audio frame\natt. Next, pitch tracking is performed separately for each\ndetected pitch, from tforwards, and then also from tback-\nwards, as depicted in Figure 2. Consequently, this procedur e\ngives as many pitch contours as pitches are detected at t.\nFigure 2 . Bi-directional multi-pitch tracking around time\npointt.\n3.1 Pitch Detection\nIn order to detect pitches at the time point t, we apply si-\nnusoidal analysis to the short-time spectrum of the poly-\nphonic audio signal at t. The analysis extracts (quadrati-\ncally interpolated) frequencies of the loudest three peaks in\nthe ï¬rst-formant section (200â€“1000 hertz) of the magnitude\nspectrum. The loudness of a sinusoid is computed by cor-\nrecting its amplitude according to the trends in the 40-phon\nequal-loudness contour (ELC) [8], which quantiï¬es the de-\npendency of human loudness perception on frequency. For\neach extracted sinusoidal frequency Ëœf(hertz), the procedure\nâ€œdetectsâ€ up to three pitches in the 80â€“1000 hertz vocal pitc h\nrange, at Ëœf,Ëœf/2, andËœf/3, regarding the sinusoid as the fun-\ndamental, the second partial, or the third partial of a pitch .\nAs a result, the pitch detector gives nine pitches at the most\nfor the time point t. The ambiguity among the ï¬rst three\npartials will not be resolved until a selection is made among\npitch contours.\n3.2 Pitch Tracking\nSuppose that we are now appending a new pitch to the end\nof a growing pitch contour. Calculation of the new pitch\nproceeds in three steps: 1) ï¬nding in the new spectrum a set\nof sinusoids around (within one half tone of) the ï¬rst three\npartials of the last pitch in the contour, 2) ï¬nding among the\nsinusoids the one with the highest amplitude, and 3) dividin g\nthe frequency (hertz) of this sinusoid by the corresponding\nharmonic multiple (1, 2, or 3). In other words, the pitch\ncontour is guided by nearby high-energy pitch candidates.\nThe growth of a pitch contour stops once the amplitude of\nthe loudest partial drops (cumulatively) from a peak value\nby more than 9 dB, i.e., a speciï¬c form of onset or offset\nis detected, with the loudness of each partial evaluated ove r\nthe entire contour as a time average.\n4. TIMBRAL DISTANCE MEASUREMENT\nIn this section, we develop a method for measuring the tim-\nbral deviation of a pitch contour Cfrom human voiced sound,\nwhich is based on an acoustic-phonetic parameterization of\n2612th International Society for Music Information Retrieval Conference (ISMIR 2011)\nhuman voiced sound, and ï¬nding within the space of human\nvoiced sound the minimum distance from C, as illustrated in\nFigure 3.\nFigure 3 . Measuring the timbral distance between a pitch\ncontour (star) and the space of human voiced sound.\n4.1 Parameterization of Human Voiced Sound\nIn order to model the space of human voiced sound, it is\ndesirable to identify every point in the space with a set of\nacoustic-phonetic parameters. To this end, we let each shor t-\ntime magnitude spectrum of human voiced sound be repre-\nsented by seven parameters: the amplitude, the fundamental\nfrequency, the ï¬rst three formant frequencies, and the nasa l\nformant and anti-formant frequencies [11]. Such a parame-\nterization is appropriate for specifying human voiced soun d\nin that sinusoidal parameters of the voice can be obtained\nfrom the acoustic-phonetic parameters through well-deï¬ne d\nprocedures. Obviously, partial frequencies of the human\nvoiced sound can be derived as integer multiples of the fun-\ndamental frequency. On the other hand, partial amplitudes\nof the human voiced sound can be derived on the basis of\nformant synthesis [4], which has been applied to synthesiz-\ning a wide range of realistic singing voice [15].\nConsider a point in the space of human voiced sound\ns= (a,f0,f1,f2,f3,fp,fz)T, (1)\nwhereais the amplitude (in dB), f0is the fundamental fre-\nquency (in quarter tones), f1,f2, andf3are the ï¬rst three\nformant frequencies (in hertz), and fpandfzare the nasal\nformant and anti-formant frequencies (in hertz). Amplitud e\nof partials can be calculated from sby [4]\nap\ni=a+20log10/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleUR(ifh\n0)KR(ifh\n0)/productdisplay\nnâˆˆIfHn(2Ï€Â·ifh\n0)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle,\n(2)\nwhereap\niis the amplitude of the ith partial in dB, iâ‰¤10,\nfh\n0denotes the fundamental frequency in hertz:\nfh\n0= 440Â·2(f0âˆ’105)/24, (3)UR(Â·)represents the (radiated) spectrum envelope of the\nglottal excitation [4]:\nUR(f) =f/100\n1+(f/100)2, (4)\nKR(Â·)represents all formants of order four and above [4]:\n20log10KR(f)â‰ˆ0.72/parenleftBig\nf\n500/parenrightBig2\n+0.0033/parenleftBig\nf\n500/parenrightBig4\n,\nfâ‰¤3000,(5)\nIf={1,2,3,p,z}, andHn(Â·)represents frequency response\nof formant n[4]:\nHn(Ï‰) =1/parenleftBig\n1âˆ’jÏ‰\nÏƒn+jÏ‰n/parenrightBig/parenleftBig\n1âˆ’jÏ‰\nÏƒnâˆ’jÏ‰n/parenrightBig,n= 1,2,3,p,\n(6)\nHz(Ï‰) =/parenleftbigg\n1âˆ’jÏ‰\nÏƒz+jÏ‰z/parenrightbigg/parenleftbigg\n1âˆ’jÏ‰\nÏƒzâˆ’jÏ‰z/parenrightbigg\n.(7)\nIn (6),Ï‰nis the frequency of formant nin rad/s, i.e., Ï‰n=\n2Ï€fn, andÏƒnis half the bandwidth of formant nin rad/s,\nwhich can be approximated as a function of Ï‰nby a poly-\nnomial regression model [7].\n4.2 Distance Minimization\nSuppose that the instantaneous pitch values in contour C\nhave mean fC. Now, let the vector\nx= (a,f1,f2,f3,fp,fz)T(8)\ndenote any point on the hyperplane f0=fCin the space\nof human voiced sound. Then we can deï¬ne the distance\nbetweenxandCas\nDC(x) =/radicalbigg\n/summationtext10\ni=1/parenleftBig\naq\niâˆ’ap\ni\nÏƒa/parenrightBig2\n, (9)\nwhereaq\niis the mean amplitude (in dB) of the ith partial of\nC,ap\niis the amplitude (computed as in (2)) of the ith partial\nofx, andÏƒais an empirical constant set to 12. The timbral\ndistance between Cand the space of human voiced sound\ncan now be measured as\nmin\nxâˆˆXDC(x), (10)\nwhereXdescribes constraints imposed on the formant fre-\nquencies:\nX=ï£±\nï£´ï£´ï£´ï£´ï£´ï£´ï£²\nï£´ï£´ï£´ï£´ï£´ï£´ï£³xâˆˆR6/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle250â‰¤f1â‰¤1000\n600â‰¤f2â‰¤3000\n1700â‰¤f3â‰¤4100\n200â‰¤fpâ‰¤500\n200â‰¤fzâ‰¤700\nfp,fzâ‰¤f1â‰¤f2â‰¤f3ï£¼\nï£´ï£´ï£´ï£´ï£´ï£´ï£½\nï£´ï£´ï£´ï£´ï£´ï£´ï£¾. (11)\n27Oral Session 1: Melody and Singing\nThe accuracy in determining whether or not Cis vocal\ndepends on how well the distance in (9) is numerically min-\nimized. To be speciï¬c, if Cis vocal and the timbral distance\nbetweenCand the space of human voiced sound is over-\nestimated due to distance minimization being trapped in a\nlocal minimum, then Cmay very likely turn out to be mis-\ntaken by the procedure for an instrumental contour. Our\nnumerical experience revealed that the best of twenty local\nsearches for the minimum deï¬ned in (10), which are ini-\ntialized respectively with twenty different reference poi nts,\nshows great consistency in associating vocal pitch contour s\nwith short timbral distances. These reference points diffe r\nonly in the oral formant frequencies f1,f2, andf3, with nu-\nmerical values taken from the gender-speciï¬c averages for\nten vowels of American English [10]: i, I,E, Ã¦,A,O,U, u,\n2, and Ã‡. Although each individual search is local by nature\nand can only be expected to give a local minimum in some\nneighborhood of the corresponding starting point, the glob al\nminimum can be found as long as it can be reached from one\nof the twenty initial points.\nFigure 4 . Each update in the local search for the minimum\ndistance consists of a series of one-variable subproblems.\nThe local search for the minimum deï¬ned in (10) may\nbe achieved with any local optimization technique. Here we\nuse a simple coordinate descent algorithm, as represented i n\nFigure 4, where each (all-variable) update consists of a se-\nries of one-variable updates. Each one-variable update min -\nimizes the distance with respect to the variable alone while\nï¬xing the other variables. For instance, the update of the\nformant frequency f2in thejth all-variable update operates\non the current point\n(a(j),f(j)\n1,f(jâˆ’1)\n2,f(jâˆ’1)\n3,f(jâˆ’1)\np,f(jâˆ’1)\nz)T(12)\nby computing\nf(j)\n2= arg min\nf2âˆˆI2\nDC/parenleftBig\n(a(j),f(j)\n1,f2,f(jâˆ’1)\n3,f(jâˆ’1)\np,f(jâˆ’1)\nz)T/parenrightBig\n,\nI2={f2âˆˆR|600â‰¤f2â‰¤3000,f(j)\n1â‰¤f2â‰¤f(jâˆ’1)\n3}.\n(13)\nIn our implementation, the subproblem (13) is solved by\nï¬nding a local minimum over a 100-hertz-spaced samplingoff2aroundf(jâˆ’1)\n2 . The subproblem for updating the am-\nplitudeacan be solved analytically, as it is equivalent to\nminimizing a quadratic function of a. The ï¬nal numerical\nsolution to the problem (10) is reï¬ned by continuing the\nlocal search with a 10-hertz spacing of formant frequency\nsampling.\n5. PITCH CONTOUR SELECTION\nIn this section, we present a procedure for selecting the vo-\ncal pitch contour from a set of pitch contours detected aroun d\ntime point t. To begin with, it prunes those pitch contours\nthat have been associated with a long timbral distance from\nthe space of human voiced sound. A pitch contour is ac-\ncepted only if the timbral distance does not exceed the em-\npirical threshold ofâˆšâˆ’2log0.4. In addition, if the mean\namplitude over even partials of a pitch contour exceeds that\nover odd partials by more than 7 dB, the contour is rejected,\ntaken as the octave below a true pitch contour.\nSecondly, the procedure prunes some pitch contours that\ncan be seen as an overtone as related to another pitch con-\ntour. To this end, the overlap time interval between each\npair of contours is calculated, and the pitch interval betwe en\ntwo contours is determined on the basis of the mean pitch\nduring the overlap. The procedure rejects any pitch contour\nthat has a mean pitch at the 2nd, 3rd, or 4th partial of another\ncontour.\nLastly, the procedure selects the loudest pitch contour\nfrom any contours that survived the prunings, thereby pro-\nviding a mechanism for identifying the predominant lead\nvocal out of several simultaneous singing voices. The loud-\nness of each pitch contour is deï¬ned as the mean of its\ninstantaneous loudness values, which are each calculated\nby summing the linear-scale, ELC-corrected instantaneous\npower over the partials.\n6. LONG-TERM PROCESSING\nAt the excerpt level, the goal of processing is an interleave d\nsequence of vocal pitch contours and pauses. To this end, we\nmaintain a list of visited frames throughout the segmentation\nprocess. A frame is considered visited whenever a vocal\npitch contour has been extracted whose duration covers the\nframe.\nSuppose that at this moment the procedure has extracted\nkvocal pitch contours from the excerpt, with the list of vis-\nited frames updated accordingly. The procedure attempts to\nextract the (k+ 1) th contour around time point t, which is\nset to the unvisited frame that has the highest signal loud-\nness among all the unvisited frames. Here, the loudness\nof a frame is calculated by summing the linear-scale, ELC-\ncorrected power over sharp peaks in the spectrum. The sharp-\nness threshold of each spectral local maximum is set to 9\n2812th International Society for Music Information Retrieval Conference (ISMIR 2011)\ndB above the mean amplitude over the neighboring 5 fre-\nquency bins. In case that the new contour should overlap\nwith an existing contour, the new contour would be trun-\ncated to resolve the conï¬‚ict. This procedure continues unti l\nthe loudness of every unvisited frame is below the excerpt-\nwide median. These remaining unvisited frames form the\nï¬nal pauses between vocal pitch contours.\n7. EXPERIMENTS\nIn this section, to provide comparison of our method with\nsome existing methods, we conduct vocal melody extraction\nexperiments on a publicly available dataset.\n7.1 Dataset Description\nThe dataset is a subset of the one built for the Melody Ex-\ntraction Contest in the ISMIR2004 Audio Description Con-\ntest (ADC 2004). The whole ADC 2004 dataset consists of\n20 audio recordings, each around 20 seconds in duration,\namong which eight recordings have instrumental melodies,\nand the other twelve have vocal melodies. Since this work\nconsiders vocal melodies only, experiments are carried out\nexclusively on the 12 vocal recordings, including four pop\nsong excerpts, four song excerpts with synthesized vocal,\nand four opera excerpts. The dataset has been in use in\nseveral Music Information Retrieval Evaluation Exchange\n(MIREX) contests since 2006; therefore, it affords exten-\nsive comparison among methods.\nBefore melody extraction, each audio ï¬le in the dataset\nis resampled at 11,025 hertz and constant- Qtransformed [1]\n(Q= 34 ) into a sequence of short-time spectra. Each result-\ning spectrum is a quarter-tone-spaced sampling of a contin-\nuous spectrum that is capable of resolving the interference\nbetween two half-tone-spaced sinusoids from 21.827 hertz\nall the way to 5,428.6 hertz.\n7.2 Performance Measures\nIn the experiments documented here, the tested system gives\nvocal melodies in the format of a voicing/pitch value for\neach frame (at the rate of 100 frames per second). If a frame\nis estimated to be within the duration of a vocal pitch con-\ntour, the output speciï¬es the pitch estimate for the frame;\notherwise, the output speciï¬es that the frame is estimated t o\nbe not voiced.\nMIREX adopts several measures for evaluating the per-\nformance of a melody extraction system [14]. In the ï¬rst\nplace, to determine how well the system performs voicing\ndetection, we use the voicing detection rate, the voicing fa lse\nalarm rate, and the discriminability. The voicing detectio n\nrate is computed as the fraction of frames that are both la-\nbeled and estimated to be voiced, among all the frames that\nare labeled voiced. The voicing false alarm rate is computedas the fraction of frames that are estimated to be voiced but\nare actually not voiced, among all the frames that are not\nvoiced according to the reference transcription. The dis-\ncriminability combines the above two measures in such a\nway that it can be deemed independent of the value of any\nthreshold involved in the decision of voicing detection:\ndâ€²=Qâˆ’1(PF)+Qâˆ’1(1âˆ’PD), (14)\nwhereQâˆ’1(Â·)denotes the inverse of the Gaussian tail func-\ntion,PFdenotes the false alarm rate, and PDdenotes the\ndetection rate.\nSecond, to determine how well the system performs pitch\nestimation, we use the raw pitch accuracy and the raw chroma\naccuracy. The raw pitch accuracy is computed as the frac-\ntion of frames that are labeled voiced and have pitch esti-\nmated within one quarter tone of the true pitch, among all\nthe frames that are labeled voiced. To focus on pitch class\nestimation while ignoring octave errors, we compute the raw\nchroma accuracy, which is computed in the same way as the\nraw pitch accuracy, except that the pitch is here measured in\nterms of chroma, or pitch class, a quantity derived from the\npitch by wrapping the pitch into one octave.\nFinally, the performance of voicing detection and pitch\nestimation can be measured jointly by the overall transcrip -\ntion accuracy, deï¬ned as the fraction of frames that receive\ncorrect voicing classiï¬cation and, if voiced, a pitch esti-\nmate within one quarter tone of the true pitch, among all\nthe frames.\nTable 1 . Experimental results.\n7.3 Results\nThe results are listed in Table 1. The overall transcription\naccuracies listed in the column titled â€œAllâ€ range from 61%\nto 96% and have their average at 77.007%. The minimum\nis found at the excerpt â€œopera_fem2.â€ A close look at a sig-\nniï¬cant error made in the analysis of this excerpt revealed\nthat the system mistakenly selected the octave below a true\n29Oral Session 1: Melody and Singing\nvocal pitch contour because the octave had a timbral dis-\ntance ofâˆšâˆ’2log0.41, slightly shorter than the upper limit\nset for a vocal contour. Still, the distance measured for the\ntrue vocal pitch contour was much shorter, atâˆšâˆ’2log0.98.\nThis suggests that a relative threshold for the timbral dis-\ntance may be implemented along with the absolute threshold\nto further improve the accuracy. To see the effect of timbral\ndistance measurement on the average accuracy, we repeated\nthe experiments with the distance threshold set to inï¬nity, so\nthat no contour was pruned because of a large timbral devi-\nation from human voiced sound. This turned out to reduce\nthe mean accuracy from 77.007% to 75.233%, which veri-\nï¬es the beneï¬t of timbral distance measurement. The raw\npitch accuracies in the column titled â€œV oicedâ€ are highly\ncorrelated with the overall transcription accuracies, whi ch\nsuggests that further improvement of this system should be\nmade in pitch estimation, not in voicing detection. The col-\numn titled â€œChromaâ€ contains raw chroma accuracies sim-\nilar to the raw pitch accuracies, which suggests that octave\nerrors were successfully avoided by the system.\nShown in Table 2 is a comparison of the proposed method\nwith the MIREX 2009 submissions in terms of the over-\nall transcription accuracy (OTA). Notably, if the proposed\nmethod had entered the evaluation in 2009, it would have\nranked 5th out of a total of 13 submissions. Moreover, the\naccuracy of the proposed system is within 10% of the high-\nest accuracy in the 2009 evaluation.\nTable 2 . Comparison with the MIREX 2009 Audio Melody\nExtraction results.\n8. CONCLUSION\nWe have presented a novel method for vocal melody extrac-\ntion which is based on an acoustic-phonetic model of human\nvoiced sound. The performance of this method is evaluated\non a publicly available dataset and proves comparable with\nstate-of-the-art methods.1\n9. ACKNOWLEDGMENTS\nThis work was supported in part by the Taiwan e-Learning\nand Digital Archives Program (TELDAP) sponsored by the\nNational Science Council of Taiwan under Grant: NSC 100-\n2631-H-001-013.\n1Octave code available at http://www.iis.sinica.edu.tw/\n~yrchien/english/melody.htm10. REFERENCES\n[1] J. C. Brown and M. S. Puckette. An efï¬cient algorithm\nfor the calculation of a constant Q transform. JASA ,\n92(5):2698â€“2701, 1992.\n[2] J.-L. Durrieu, G. Richard, and B. David. Singer melody\nextraction in polyphonic signals using source separation\nmethods. In ICASSP , 2008.\n[3] D. P. W. Ellis and G. E. Poliner. Classiï¬cation-based\nmelody transcription. Mach. Learn. , 65(2-3):439â€“456,\n2006.\n[4] G. Fant. Acoustic theory of speech production with cal-\nculations based on X-ray studies of Russian articula-\ntions . The Hague: Mouton, 1970.\n[5] H. Fujihara, T. Kitahara, M. Goto, K. Komatani,\nT. Ogata, and H. G. Okuno. F0 estimation method for\nsinging voice in polyphonic audio signal based on statis-\ntical vocal model and Viterbi search. In ICASSP , 2006.\n[6] M. Goto and S. Hayamizu. A real-time music scene de-\nscription system: Detecting melody and bass lines in au-\ndio signals. In IJCAI-CASA , 1999.\n[7] J. W. Hawks and J. D. Miller. A formant band-\nwidth estimation procedure for vowel synthesis. JASA ,\n97(2):1343â€“1344, 1995.\n[8] ISO 226. Acousticsâ€”normal equal-loudness contours,\n2003.\n[9] S. Jo and C. D. Yoo. Melody extraction from polyphonic\naudio based on particle ï¬lter. In ISMIR , 2010.\n[10] Ray D. Kent and Charles Read. The acoustic analysis of\nspeech . Singular/Thomson Learning, 2002.\n[11] D. H. Klatt. Software for a cascade/parallel formant sy n-\nthesizer. JASA , 67(3):971â€“995, 1980.\n[12] M. Lagrange, L.G. Martins, J. Murdoch, and G. Tzane-\ntakis. Normalized cuts for predominant melodic source\nseparation. IEEE Trans. on ASLP , 16(2):278â€“290, 2008.\n[13] R. P. Paiva, T. Mendes, and A. Cardoso. On the detection\nof melody notes in polyphonic audio. In ISMIR , 2005.\n[14] G. E. Poliner, D. P. W. Ellis, A. F. Ehmann, E. GÃ³mez,\nS. Streich, and B. Ong. Melody transcription from mu-\nsic audio: Approaches and evaluation. IEEE Trans. on\nASLP , 15(4):1247â€“1256, 2007.\n[15] J. Sundberg. The KTH synthesis of singing. Advances in\nCognitive Psychology , 2(2-3):131â€“143, 2006.\n30"
    },
    {
        "title": "A Feature Smoothing Method for Chord Recognition Using Recurrence Plots.",
        "author": [
            "Taemin Cho",
            "Juan Pablo Bello"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1417557",
        "url": "https://doi.org/10.5281/zenodo.1417557",
        "ee": "https://zenodo.org/records/1417557/files/ChoB11.pdf",
        "abstract": "In this paper, we propose a feature smoothing technique for chord recognition tasks based on repeated patterns within a song. By only considering repeated segments of a song, our method can smooth the features without losing chord boundary information and fine details of the original feature. While a similar existing technique requires several hard decisions such as beat quantization and segmentation, our method uses a simple pragmatic approach based on recurrence plot to decide which repeated parts to include in the smoothing process. This approach uses a more formal definition of the repetition search and allows shorter (â€œchordsizeâ€) repeated segments to contribute to the feature improvement process. In our experiments, our method outperforms conventional and popular smoothing techniques (a moving average filter and a median filter). In particular, it shows a synergistic effect when used with the Viterbi decoder.",
        "zenodo_id": 1417557,
        "dblp_key": "conf/ismir/ChoB11",
        "keywords": [
            "chord recognition tasks",
            "repeated patterns within a song",
            "feature smoothing technique",
            "chord boundary information",
            "fine details of the original feature",
            "recurrence plot",
            "pragmatic approach",
            "short repeated segments",
            "synergistic effect",
            "Viterbi decoder"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nA FEATURE SMOOTHING METHOD FOR CHORD RECOGNITION USING\nRECURRENCE PLOTS\nTaemin Cho and Juan P. Bello\nMusic and Audio Research Laboratory (MARL)\nNew York University, New York, USA\n{tmc323,jpbello }@nyu.edu\nABSTRACT\nIn this paper, we propose a feature smoothing technique for\nchord recognition tasks based on repeated patterns within\na song. By only considering repeated segments of a song,\nour method can smooth the features without losing chord\nboundary information and ï¬ne details of the original fea-\nture. While a similar existing technique requires several\nhard decisions such as beat quantization and segmentation,\nour method uses a simple pragmatic approach based on re-\ncurrence plot to decide which repeated parts to include in the\nsmoothing process. This approach uses a more formal deï¬-\nnition of the repetition search and allows shorter (â€œchord-\nsizeâ€) repeated segments to contribute to the feature im-\nprovement process. In our experiments, our method out-\nperforms conventional and popular smoothing techniques (a\nmoving average ï¬lter and a median ï¬lter). In particular, it\nshows a synergistic effect when used with the Viterbi de-\ncoder.\n1. INTRODUCTION\nThe majority of state of the art chord recognition systems\nare based on frame-wise analysis of chroma features ex-\ntracted from an input signal. The chord sequence is deter-\nmined by a pattern matching process that measures the ï¬t\nbetween a set of predeï¬ned chord models and each frame of\nthe input chromagram. In order to precisely identify chord\nboundaries, the frame rate of the chroma features is typi-\ncally faster than the rate of chord changes in music. How-\never, this makes the chroma features sensitive to local tran-\nsients and noise in the signal. A popular choice to cope\nwith this problem is to pre-process the chromagram using\neither a low-pass ï¬lter or a median ï¬lter prior to the pat-\ntern matching process. Both ï¬lters blur out transients and\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proï¬t or commercial advantage and that copies\nbear this notice and the full citation on the ï¬rst page.\nc/circlecopyrt2011 International Society for Music Information Retrieval.noise in the signal by smoothing the features across neigh-\nboring frames. Another favored approach is using a Viterbi\ndecoder that ï¬nds the most likely sequence of chords based\non the chord-type probabilities estimated from the pattern\nmatching process. By reducing the number of chord tran-\nsitions using a relatively high self-transition probability (the\nprobability of remaining in a chord), the Viterbi decoder can\nï¬lter out spurious transitions caused by short bursts of noise.\nIn our previous work [4], we found that the combina-\ntion of pre-ï¬ltering (either a moving average ï¬lter or a me-\ndian ï¬lter) and post-ï¬ltering (the Viterbi decoder) does not\nyield a synergistic impact on performance, although many\nsystems use the combination [2, 6]. This is because the ef-\nfects of pre-ï¬ltering substantially overlap with those of post-\nï¬ltering, i.e. they carry out essentially the same function in\nthe sense of constraining sudden movements over a short\nseries of local frames.\nIn this paper, we propose a feature smoothing technique\nbased on an important aspect of music, repetition. By aver-\naging repeated chroma patterns within a piece of music, our\nmethod attenuates unsystematic deviations and noise and\nreinforces harmonic information of chroma frames. This\nmethod is inspired by the one proposed by Mauch et al. [6].\nIn their approach, the information about the repetitive struc-\nture of songs is used to enhance chroma features for chord\nestimation. They use a conventional frame-by-frame self-\nsimilarity matrix generated from a beat-synchronous chro-\nmagram. From the matrix, they extract repeated chord pro-\ngressions of equal length by examining all diagonal lines.\nThe beat and bar information estimated from a song play a\ncrucial role in their greedy algorithm to ï¬nd repeated sec-\ntions. The found segments are merged into larger segment\ntypes (e.g. verse and chorus) without overlapping. Their\nnew features are then obtained by averaging chroma features\nfrom multiple occurrences of the same segment type.\nUnlike Mauch et al., our method decides which repeated\nparts to include in the smoothing process by a simple thresh-\nolding operation using the technique of recurrence plots. As\nour method doesnâ€™t use beat and bar information, it avoid\nthe errors in the initial feature analysis (e.g. onset detec-\n651Oral Session 8: Chord Analysis\nRecurrence Plot0100020003000400050006000700080009000âˆ’0.1âˆ’0.0500.050.10.15AudioChroma FeaturesFeature reconstructionNew Chroma FeaturesTime-delay embeddingWeight MatrixFigure 1 . Block diagram of feature smoothing process\nbased on recurrence plot.\ntion or beat tracking), that propagate through the subsequent\nprocessing stages and may hurt overall performance [8]. In\nour method, repeated sections are not limited to a few large\nunits (e.g. chorus or verse), but include smaller units such\nas chords. Thus, our method can generate new chroma fea-\ntures using relatively many repeated frames collected from\nall across the song. As the repeated frames are assumed to\nhave the same harmonic content, the smoothing only occurs\nwithin the same chords, thus preserving boundary informa-\ntion. In our experiments, this smoothing method yields bet-\nter results than the conventional methods in all cases includ-\ning the combination with the Viterbi decoder.\nThe remainder of this paper is structured as follows. In\nSection 2, we provide a detailed description of our method.\nIn Section 3, we describe the data and evaluation method-\nology used in our experiments. The results and discussions\nare provided in Section 4, and our conclusions and direc-\ntions for future work are presented in Section 5.\n2. APPROACH\nThe block diagram of our feature smoothing process is shown\nin Figure 1. First, the audio signal is segmented and trans-\nformed into chroma features. The chroma features are then\nprojected into phase space using time-delay embedding prior\nto calculating the recurrence plot. The weight matrix is de-\nrived from the recurrence plot, and combined with the orig-\ninal chroma features as a coefï¬cient set in the feature re-\nconstruction process. To measure the performance of our\nmethod on various types of chroma features, we evaluate our\nmethod on conventional chroma features and one of their\nmost recent variants, CRP features [7]. The following sub-\nsections discuss the details of the approach including the\nfeature set and our methodology for generating and apply-\ning the weight matrix to construct new chroma features.2.1 Chroma Features\nPitch Class Proï¬le (PCP), or chroma features, represent the\nenergy of the audio signal present in each of the twelve pitch\nclasses of the chromatic scale. In this paper, the chroma\nfeatures are derived from a slightly modiï¬ed version of the\nconstant-Q transform [3] by mapping each frequency bin of\nthe constant-Q spectrum to a corresponding pitch class. Let\nus deï¬ne the kthbin constant-Q kernel function as:\nKk(m) =Ï‰k(m)eâˆ’j2Ï€fkm, mâˆˆ[0,Nkâˆ’1] (1)\nwhereÏ‰kis a Hamming window of length Nk, which varies\nwith the center frequency fkso that it has a ï¬xed Q-value.\nThe center frequency fkis based on the equal tempered\nscale such that:\nfk= 2k/Î²fmin (2)\nwhereÎ²is the number of bins per octave, and fminis the\nminimum analysis frequency.\nThe constant-Q transform Xcqof a segmented audio sig-\nnalx(m),mâˆˆ[0,Nsegâˆ’1]is then calculated as:\nXcq(k) =1\nmin(Nseg,Nk)Nâˆ’1/summationdisplay\nÎ½=0X(Î½)Kâˆ—\nk(Î½) (3)\nwhereN >Nkâˆ€k,X(Î½)is theN-point DFT of the signal,\nandKâˆ—\nk(Î½)is the conjugate of the N-point DFT of the kth\nkernel function. The signal and kernel functions are padded\nwith trailing zeros to length Nprior to applying the DFT. To\nprevent underestimation of low frequencies where Nseg<\nNk, the smaller value between NsegandNkis used as the\nnormalization factor. In this paper, we use Î²= 36 , with\nthe analysis performed between fmin= 27.5 Hz and fmax=\n4186 Hz (i.e. corresponding to the MIDI pitches 21 to 108).\nThe STFT window length Nsegis 8192 (186 ms), and hop\nsize is 4096 (93 ms) samples at 44100 Hz sample rate.\nA 12-bins per octave spectrum P(p),pâˆˆ[1,Np]is ob-\ntained by combining adjacent bins of the Xcq(k)usingÎ²/12-\nwide non-overlapping Gaussian windows. To avoid percus-\nsive noise (e.g. bass drums) in low frequencies and to atten-\nuate the effect of non-harmonic tones caused by high-order\nharmonics in high frequencies, P(p)is windowed with a\nGaussian centered at C4 (MIDI pitch 60). Finally, a chroma\nvectorC={cb},bâˆˆ[1,12]can simply be calculated by\nfolding the spectrum P(p).\n2.2 CRP Features\nCRP (C hroma DCT-R educed log P itch) features, proposed\nby M Â¨uller et al. [7], are one of the most recent variants of\nconventional chroma features. Their derivation is inspired\nby Mel-frequency cepstral coefï¬cients (MFCCs) which are\npopular in speech and music recognition. First, the spectrum\nP(p)is logarithmized using log(P(p)Â·Î³+1) with a suitable\n65212th International Society for Music Information Retrieval Conference (ISMIR 2011)\n  \n10020030040050060070080090010001002003004005006007008009001000\n00.10.20.30.40.50.60.70.80.91(a)Si,j\n  \n10020030040050060070080090010001002003004005006007008009001000\n01 (b)Ri,j\n  \n10020030040050060070080090010001002003004005006007008009001000\n00.10.20.30.40.50.60.70.80.91 (c)Wi,j\nFigure 2 . (a) a similarity matrix ( M= 25,Ï„= 1), (b) a recurrence plot ( Î¸= 50 ), (c) a weight matrix\ncompression factor Î³ >1âˆˆR, and transformed to the cep-\nstral domain using the Discrete Cosine Transform (DCT).\nTheÎ¾-lowest coefï¬cients of the resulting cepstrum are then\nset to zero. Finally, the cepstrum is transformed back using\nthe inverse DCT, and the resulting pitch vectors are summa-\nrized into the 12-dimensional chroma vectors, CRP. It is\nimportant to note that by removing the DC component from\nthe cepstrum, a CRPvector contains both positive and neg-\native values. The feature vectors are then normalized by the\n/lscript2-norm. In this paper, we use Î³= 1000 andÎ¾= 25 as\nsuggested by [7].\nThe main advantage of using CRP features for chord recog-\nnition comes from applying logarithmic compression on the\nspectrumP(p). In conventional chroma features, melodies\nand bass lines are problematic, because they generate sin-\ngle high-energy peaks that dominate the chroma feature dis-\ntributions to the detriment of the background harmony of\nthe frame. The logarithm de-emphasizes the dominant pitch\nsalience while boosting the background harmonic contents.\nIn addition, by removing low coefï¬cients from the cepstrum\n(i.e. formants, spectral shape), CRP features maximize the\neffect of compression and become invariant to changes in\ntimbre.\n2.3 Recurrence Plot and Weight Matrix\nThe weight matrix is computed using recurrence plot (RP)\ntheory, which provides a sophisticated way to analyze se-\nquential data [5], and have been previously used with chroma\nfeatures in other MIR tasks such as cover version identiï¬-\ncation [9], and recently, in structural similarity [1]. A key\nfeature of recurrence plots is the use of time-delay embed-\nding. Time-delay embedding is a method for transforming\na time series into a multidimensional sequence of lagged\ndata. In other words, it provides a way to transform frame-\nby-frame analysis into n-gram analysis (i.e. subsequence-\nby-subsequence).\nThenthtime-delay embedded chroma vector Ë‡C(n)can be\nconstructed by concatenating all the elements of a chroma\nsequenceC(n) ={cb(n)},bâˆˆ[1,12]from timenton+(Mâˆ’1)Ï„as:\nË‡C(n) =/parenleftbig\nc1(n),c1(n+Ï„),...,c 1(n+ (Mâˆ’1)Ï„),...\nc12(n),c12(n+Ï„),...,c 12(n+ (Mâˆ’1)Ï„)/parenrightbig\n(4)\nË‡C(n)is then normalized to have unit length. The self-similarity\nmatrixSi,jis calculated as:\nSi,j=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleË‡C(i)âˆ’Ë‡C(j)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle\n2(5)\nwherei,jâˆˆ[1,N],Nis the length of the time-delay em-\nbedded chroma sequence, and ||Â·|| is the Euclidean norm.\nThe normalization factor (the constant 2 in the denominator)\nis the maximum possible distance value between unit length\nvectors. Hence, 0â‰¤Si,jâ‰¤1,âˆ€i,jâˆˆ[1,N].\nUnlike a conventional frame-by-frame self-similarity ma-\ntrix (i.e. a special case of Si,jwith parameters M= 1 and\nÏ„= 1), the additional embedding process makes the matrix\nmore robust to short term noise or deviations by evaluating\nvectors of sample sequences (i.e. MÂ·Ï„length sequence)\ninstead of using only samples. An RP can be obtained from\nSi,jwith a suitable threshold /epsilon1as:\nRi,j=H(/epsilon1âˆ’Si,j), i,jâˆˆ[1,N] (6)\nwhereHis the Heaviside step function. The choice of /epsilon1\nis important because it is the only criterion to determine\nwhich parts are actually repeated. However, a global thresh-\nolding with a ï¬xed threshold is not appropriate in our case,\nbecause the useful range of thresholds can vary greatly be-\ntween songs or even within a given song. A better strategy\nis to simply match the number of nearest neighbors in the\nphase space constructed by Eqn. (4). In this approach, /epsilon1(n)\nis deï¬ned as a threshold to ensure that Ri,n= 1 for theÎ¸\npoints closest to the nthpoint of the trajectory. In practice,\nwe expand this approach to both columns and rows of RP\nto include every possible repeated pattern in the smoothing\nprocess as:\nRi,j=H(/epsilon1(n)âˆ’Si,n)âˆ¨H(/epsilon1(n)âˆ’Sn,j) (7)\n653Oral Session 8: Chord Analysis\nij1234567891010987654321\n(a)Wi,j\ni4567891010987654321\nnew chroma vectors (b)Wi,j\nFigure 3 . Reconstruction process: (a) overlapped chroma\nsegments (white boxes), (b) chroma summation over over-\nlapped segments with weight values.\nwherei,j,nâˆˆ[1,N]. Finally, a weight matrix Wi,jcan be\ncalculated using information from Si,jandRi,jas:\nWi,j= (1âˆ’Si,j)Â·Ri,j (8)\nHence, the matrix is sparse and has real values indicating\nthe similarity degrees of repeated sections. Figure 2 shows\nexamples of a similarity matrix Si,j, a recurrence plot Ri,j\nand a weight matrix Wi,jwhereM= 25 ,Ï„= 1andÎ¸= 50 .\nIn this paper, we ï¬x Ï„= 1(i.e. no skipping frames).\n2.4 Feature Reconstruction\nEach column (or row) of Wi,jcontains information about re-\ncurrences of the current event across the whole song. More\nspeciï¬cally, the ithactivated component (i.e. non-zero com-\nponents) in the jthcolumn vector indicates that the ithseg-\nment is similar to the jthsegment. For example, the ï¬rst\ncolumn of Figure 3(a) shows that the 5thand 8thsegments\nare similar to the ï¬rst segment. For M= 3 andN= 10 ,\nFigure 3(a) depicts the temporal validity of chroma vector\nM-grams.\nTo generate the ï¬rst smoothed chroma vector from the\nexample in Figure 3(a), the activated weights at i={1,5,8}\nof the ï¬rst column are multiplied with the ï¬rst frames of the\ncorresponding segments. Then the results are summed up\nin the ï¬rst smoothed chroma vector (see the left most down\narrow in Figure 3(b)). Similarly, the second frame of the\nsmoothed chromagram uses the weights on the second col-\numn and the ï¬rst frames of the corresponding chroma seg-\nments (i.e.i={2,6,9}). However, the overlapping means\nthat the second frames from the previous segments should\nalso be considered (see the second column in Figure 3(b)).\nMore generally, the nthframe of the smoothed chromagram\nis computed from the weights in the previous nâˆ’Mâˆ’1\ncolumns. This process can be described as:\nË†C(n) =Mâˆ’1/summationdisplay\nm=0/summationtextN\ni=1Wi,nâˆ’mÂ·C(i)\n/summationtextN\ni=1Wi,nâˆ’m(9)\n1700 1750 1800 1850 1900 1950 2000 2050 2100CC#DD#EFF#GG#AA#B\nÃ¯0.500.5(a) Original\n1700 1750 1800 1850 1900 1950 2000 2050 2100CC#DD#EFF#GG#AA#B\nÃ¯0.4Ã¯0.200.20.40.60.8\n(b) Moving average ï¬lter ( Î»= 14 )\n1700 1750 1800 1850 1900 1950 2000 2050 2100CC#DD#EFF#GG#AA#B\nÃ¯0.6Ã¯0.4Ã¯0.200.20.40.60.8\n(c) Median ï¬lter ( Î»= 14 )\n1700 1750 1800 1850 1900 1950 2000 2050 2100CC#DD#EFF#GG#AA#B\nÃ¯0.4Ã¯0.200.20.40.6\n(d) Our method ( M= 25 ,Î¸= 50 )\nFigure 4 . Chromagrams: (a) an original chromagram ex-\ncerpt from â€œLet It Beâ€ by The Beatles, (b) a smoothed\nchromagram using a moving average ï¬lter with Î»= 14 ,\n(c) a median ï¬lter with Î»= 14 , and (d) our method with\nM= 25 ,Î¸= 50 .\nwhere the denominator is a normalization factor that adjusts\nfor the contribution of overlapping chroma segments.\nFigure 4(a) shows a chromagram and its smoothed ver-\nsions using a moving average ï¬lter (Figure 4(b)), a median\nï¬lter (Figure 4(c)) and our method (Figure 4(d)). The mov-\ning average ï¬lter used in Figure 4(b) is calculated as:\nÂ¯C(n) =1\nÎ»Î»âˆ’1/summationdisplay\nd=0C/parenleftbigg\nn+dâˆ’/floorleftBigÎ»âˆ’1\n2/floorrightBig/parenrightbigg\n(10)\nand the median ï¬lter used in Figure 4(c) is deï¬ned as:\nËœC(n) = median\ndC(d),\ndâˆˆN, nâˆ’/floorleftBigÎ»âˆ’1\n2/floorrightBig\nâ‰¤dâ‰¤n+/ceilingleftBigÎ»âˆ’1\n2/ceilingrightBig (11)\nwhereÎ»is the number of adjacent frames to be processed.\nIn Figure 4, the chromagram generated by our method is\nmuch cleaner than the original chromagram, while keeping\nsharp boundaries between chord segments. Figure 4(b), on\nthe other hand, shows blurred boundaries, and the median\nï¬lter in Figure 4(c) removes both the noise and the ï¬ne de-\ntail since it canâ€™t distinguish the difference between those\nsignals.\n65412th International Society for Music Information Retrieval Conference (ISMIR 2011)\nWithout Viterbi Decoder With Viterbi Decoder\nNone Mean Median Our method None Mean Median Our method\nC 49.93 65.51 (14) 66.22 (14) 69.23 (25, 47) 72.02 71.67 (4) 72.54 (4) 74.81 (25, 10)\nCRP 54.26 71.16 (14) 71.05 (14) 72.85 (25, 50) 75.36 75.76 (4) 75.64 (4) 77.91 (25, 15)\nTable 1 . Average accuracies of the binary template model with no ï¬ltering (labeled â€˜Noneâ€™), a moving average ï¬lter (labeled\nâ€˜Meanâ€™), a median ï¬lter, our method, and their combinations with the Viterbi decoder. The optimal parameters are given in\nparentheses, ( Î») for both a moving average ï¬lter and a median ï¬lter, and ( M,Î¸) for our method.\n3. EVALUATION METHODOLOGY\nThe experiments are performed on 249 chord annotated songs.\nThe data set comprises 179 songs1from Christopher Harteâ€™s\nBeatles dataset, 20 songs from Matthias Mauchâ€™s Queen\ndataset and 50 pop songs from the RWC (Real World Com-\nputing) database manually annotated by music students at\nNYU. The evaluations are performed on 12 major, 12 minor\ntriads and a no-chord detection task. In the evaluation, audio\nframes where the RMS is under -57 dB are assumed to be\nno-chords.\nFor the pattern matching process, binary chord templates\nand multivariate Gaussian Mixture Models (GMMs) are used.\nThe binary chord templates (for 12 major and 12 minor tri-\nads) are manually generated based on basic chord theory. In\na 12-dimensional binary chord template vector, each com-\nponent corresponding to a chord-tone is set to 1, and the\nother components are set to 0 (e.g. [1 0 0 0 1 0 0 1 0 0\n0 0] for a C Major triad, where the left to right order of\nthe vector components follows the chromatic scale from C).\nThe detected chord on one given frame is the one whose\ntemplate is closest to the chroma vector of the frame in an\nEuclidean sense. The pseudo-probabilities for applying the\nViterbi decoder are calculated by taking the reciprocal of the\nEuclidean distances.\nThe parameters of the multivariate GMMs are estimated\nfrom annotated training data using the EM algorithm. For\ntraining, the data is segmented based on the chord annota-\ntions and transposed to the C-based chord. The root-normal-\nized chord collection is used to train C-major and C-minor\nmodels that are then re-transposed to the remaining roots to\ndeï¬ne the 22 models. In this paper, we use a mixture of 15\nGaussians with diagonal covariance matrices.\nFor the Viterbi decoder, the transition penalty Ïis ap-\nplied. The transition penalty adjusts the strength of the self-\ntransition probability relative to transitions between differ-\nent chords [4]. It is applied as follows:\nlog(Ë†ai,j) =/braceleftBigg\nlog(ai,j)âˆ’Ïfori/negationslash=j\nlog(ai,j) fori=j(12)\nwhereA= [ai,j]is the original transition probability matrix\nandË†A= [Ë†ai,j]is the modiï¬ed matrix with penalty Ï. ForA,\n1â€œRevolution 9â€ from The White Album is removed from the experiment\ndue to its lack of harmonic content.None Mean Median Our method\nC 73.85 73.52 (3) 74.41 (4) 75.78 (25, 6)\nCRP 77.82 77.63 (4) 77.69 (4) 79.61 (25, 9)\nTable 2 . Average accuracies of GMMs. The optimal param-\neters are given in parentheses, ( Î») for both a moving average\nï¬lter and a median ï¬lter, and ( M,Î¸) for our method.\nwe use a uniform transition probability matrix in which all\nchord transitions have the same probability, hence Ai,j=\n1/24,âˆ€i,jâˆˆ[1,24]\nFor statistical models (GMMs), each experiment is per-\nformed using a 10-fold cross validation on 10 randomly clas-\nsiï¬ed groups; 9 groups contain 25 songs each, and one group\ncontains 24 songs. For each iteration, one group is selected\nas a test set, and the remaining 9 groups are used for train-\ning. The chord recognition rate is calculated as follows:\nAccuracy =total duration of correct chords\ntotal duration of datasetÃ—100% (13)\n4. RESULTS AND DISCUSSION\nTable 1 shows the average accuracies of the binary template\nmodel with a moving average ï¬lter, a median ï¬lter, our method,\nand their combinations with the Viterbi decoder. The results\nshow thatCRPyields better results than Cin every case.\nAlso they show that our method outperforms the use of con-\nventional ï¬lters regardless of the types of features. Table 2\nshows the result of using GMMs with the different combi-\nnations of the ï¬lters. Similar to the case of the binary tem-\nplate model, CRPperforms better than C, and our method\nmaintains its advantages against both a moving average ï¬l-\nter and a median ï¬lter. All differences between conventional\nmethods and our method are signiï¬cant in paired t-test at\np<0.01.\nOne notable difference between our method and the con-\nventional ï¬lters is its compatibility with the Viterbi decoder.\nAs shown in both tables, unlike our method, the moving av-\nerage ï¬lter has almost no impact on the overall performance\nwhen used in combination with the Viterbi decoder. This is\ndue to the blurred boundaries caused by the ï¬lter, as seen in\nFigure 4(b). Figure 5 shows the distributions of deviations\n(in frames) between annotated and detected boundaries of\n655Oral Session 8: Chord Analysis\nâˆ’30 âˆ’20 âˆ’10 0 10 20 3001000200030004000500060007000(a) The Viterbi decoder only\nâˆ’30 âˆ’20 âˆ’10 0 10 20 3001000200030004000500060007000 (b) Moving average ï¬lter ( Î»= 14 )\nâˆ’30 âˆ’20 âˆ’10 0 10 20 3001000200030004000500060007000\n(c) Median ï¬lter ( Î»= 14 )\nâˆ’30 âˆ’20 âˆ’10 0 10 20 3001000200030004000500060007000 (d) Our method ( M= 25 ,Î¸= 50 )\nFigure 5 . Distributions of deviations between annotated\nand detected boundaries: CRPand binary template model\nwith the Viterbi decoder: (a) The Viterbi decoder only, pre-\nï¬ltering with (b) a moving average ï¬lter, (c) a median ï¬lter,\nand (d) our method. In the graph, the X-axis means the\ndistance between annotated and detected chord boundaries\nin frames, and the Y-axis means the number of boundaries\nbelonging to the distances.\nthe combinations of different pre-ï¬ltering methods and the\nViterbi decoder. For our goal, a sharp and narrow distribu-\ntion is ideal, since it means little deviation from the ground\ntruth. In the case of Figure 5(d), the number of frames used\nto generate a new frame is at least 50 ( Î¸= 50 ). As shown\nin Figure 5(b), although the moving average ï¬lter employs a\nrelatively small number of frames ( Î»= 14 ) for smoothing,\nit shows larger deviations than our method in Figure 5(d).\nAlthough the median ï¬lter is much better at preserving\nsharp edges than the moving average ï¬lter as shown in Fig-\nure 5(c), the results in Table 1 and Table 2 are not much bet-\nter than those of the moving average ï¬lter. In the case of\nCRP, the median ï¬lter shows about the same performance\nas the moving average ï¬lter. The median ï¬lter is efï¬cient\nat removing impulsive noise. However, in whitened fea-\nture space such as CRP, it has little inï¬‚uence on the per-\nformance, but rather may lead to appreciable loss in sig-\nnal details, because it uses only rank-order information of\nthe input data within the ï¬lter window without consider-\ning its original temporal-order information. These charac-\nteristic errors of conventional ï¬lters hurt the performance.\nIn the case of Figure 5(b), the accuracy rate is 72.2%, and\nfor Figure 5(c), the accuracy rate is 72.7% for CRPfea-\ntures (compared to 75.4% for Figure 5(a) and 76.4% for\nFigure 5(d)). On the contrary, since our method keeps de-\nviations low and also preserves ï¬ne details, it maximizes\nthe beneï¬ts of both our method and the Viterbi decoder.5. CONCLUSION\nIn this paper, we provided a feature smoothing method based\non repeated patterns. By applying recurrence plot theory,\nour method smoothes chroma features using information from\nharmonically-related frames from the whole sequence, as\nopposed to conventional smoothing where only a few ad-\njacent frames are used. We showed that this method con-\ntributes to performance improvement by preserving the ben-\neï¬t of a fast-frame-rate analysis (i.e. sensing precise chord\nboundaries) while alleviating its problems (i.e. noise and\ntransients). This advantage is maintained among different\ntypes of chroma features.\nIn our experiments, we applied the same parameters ( M\nandÎ¸) to all songs, despite the risk of over-smoothing. In\nthe future, we plan to develop adaptive methods for opti-\nmally choosing these parameters for each individual track.\nWe fully expect this adaptation to improve performance be-\nyond what is reported in this paper.\n6. REFERENCES\n[1] J.P. Bello. Measuring structural similarity in music.\nIEEE Trans. on Audio, Speech, and Language Process-\ning, 19(7):2013 â€“ 2025, 2011.\n[2] J.P. Bello and J. Pickens. A robust mid-level represen-\ntation for harmonic content in music signals. In Proc.\nISMIR , pages 304â€“311, 2005.\n[3] J.C. Brown and M.S. Puckette. An efï¬cient algorithm\nfor the calculation of a constant Q transform. Journal of\nthe Acoustical Society of America , 92:2698â€“2701, 1992.\n[4] T. Cho, R.J. Weiss, and J.P. Bello. Exploring common\nvariations in state of the art chord recognition systems.\nInProc. SMC , 2010.\n[5] N. Marwan, M. Carmen Romano, M. Thiel, and\nJ. Kurths. Recurrence plots for the analysis of complex\nsystems. Physics Reports , 438(5-6):237â€“329, 2007.\n[6] M. Mauch, K. Noland, and S. Dixon. Using musical\nstructure to enhance automatic chord transcription. In\nProc. ISMIR , pages 231â€“236, 2009.\n[7] Meinard M Â¨uller and Sebastian Ewert. Towards timbre-\ninvariant audio features for harmony-based music. IEEE\nTrans. on Audio, Speech, and Language Processing ,\n18(3):649â€“662, 2010.\n[8] L. Oudre, Y . Grenier, and C. F Â´evotte. Chord recognition\nby ï¬tting rescaled chroma vectors to chord templates.\nTechnical report, Telecom Paritech, 2009.\n[9] J. Serr `a, X. Serra, and R.G. Andrzejak. Cross recurrence\nquantiï¬cation for cover song identiï¬cation. New Journal\nof Physics , 11:093017, 2009.\n656"
    },
    {
        "title": "A Comparison of Statistical and Rule-Based Models for Style-Specific Harmonization.",
        "author": [
            "Ching-Hua Chuan"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1417365",
        "url": "https://doi.org/10.5281/zenodo.1417365",
        "ee": "https://zenodo.org/records/1417365/files/Chuan11.pdf",
        "abstract": "The process of generating chords for harmonizing a melody with the goal of mimicking an artistâ€™s style is investigated in this paper. We compared and tested three different approaches, including a rule-based model, a statistical model, and a hybrid system of the two, for such tasks. Experiments were conducted using songs from seven stylistically identifiable pop/rock bands, and the chords generated by the systems were compared to the ones in the artistsâ€™ original work. Evaluations were performed on multiple aspects, including calculating the average percentage of chords that were the same and those that were related, studying the manner in which the size of the training set affects the output harmonization, and examining a systemâ€™s behaviors in terms of the ability of generating unseen chords and the number of unique chords produced per song. We observed that the rule-based system performs comparably well while the result of the system with learning capability varies as the training set grows.",
        "zenodo_id": 1417365,
        "dblp_key": "conf/ismir/Chuan11",
        "keywords": [
            "chords",
            "melody",
            "harmonizing",
            "mimicking",
            "artists style",
            "approaches",
            "rule-based model",
            "statistical model",
            "hybrid system",
            "pop/rock bands"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\n   A COMPARISON OF STATISTICAL AND RULE-BASED MODELS FOR STYLE-SPECIFIC HARMONIZATION  Ching-Hua Chuan   University of North Florida School of Computing Jacksonville, FL c.chuan@unf.edu  ABSTRACT The process of generating chords for harmonizing a melody with the goal of mimicking an artistâ€™s style is investigated in this paper. We compared and tested three different ap-proaches, including a rule-based model, a statistical model, and a hybrid system of the two, for such tasks. Experiments were conducted using songs from seven stylistically identi-fiable pop/rock bands, and the chords generated by the sys-tems were compared to the ones in the artistsâ€™ original work. Evaluations were performed on multiple aspects, in-cluding calculating the average percentage of chords that were the same and those that were related, studying the manner in which the size of the training set affects the out-put harmonization, and examining a systemâ€™s behaviors in terms of the ability of generating unseen chords and the number of unique chords produced per song. We observed that the rule-based system performs comparably well while the result of the system with learning capability varies as the training set grows. 1. INTRODUCTION Automatic generation of harmony is a natural extension and application of harmonic analysis, an essential component in music information retrieval. Previous research in automatic harmonization focuses on Western classical music, apply-ing various techniques ranging from rule-based models [4] to genetic algorithms [10] in order to automate the process of harmonization in styles. An example would be the four-part harmonization in the Baroque period.  Recently, sys-tems have been developed for automatic harmonization in popular music [3, 7, 9], i.e., creating a sequence of chords for a given melody representing the vocal part in a song. However, the concept of style is loosely defined or even missing in most of these systems. As the Beatles represents a firmly defining role in pop/rock music, the style of the individual artist must be considered.  In this paper we compare three different approaches for style-specific harmonization in popular music. The three approaches demonstrate a wide spectrum of techniques: a knowledge-driven model, a data-driven model, and a hybrid system combining the two. We conducted experiments by taking the melody of songs from seven identifiable pop/rock bands as the input for the three systems, and com-pared the system-generated chords with the ones in the original artistsâ€™ work. For systems with learning capabili-ties, we analyzed the relationship between the size of the training set and the quality of the output harmonization. We also examined the characteristics of each system in terms of the number of unique chords it generates for each song, and its ability to produce chords that are not included in training sets. 2. PROBLEM DEFINITION Suppose a melody consists of m monophonic notes, {a1, â€¦, am}, harmonized by a sequence of n chords {C1, â€¦, Cn}, 1â‰¤nâ‰¤m. The melody can also be represented as a set of n melody segments, {M1, â€¦, Mn}, and each of the seg-ments contains notes harmonized by a particular chord. For example, the melody segment Mi, harmonized by the chord Ci, can be represented as: Mi = {1)||(11+âˆ‘âˆ’=ijMja,â€¦, ||)||(11MiMjija+âˆ‘âˆ’=},                           (1) where |Mj| is the number of notes in the melody segment Mj. The location of a chord often aligns with the bar line between two measures, but not necessarily, as more than one chord may appear in a bar. Chords for two adjacent melody segments may be identical or different.  In order to generate chords for a given melody, the har-monization task requires two steps: segmenting the melody into melody segments and selecting a chord for each melo-dy segment. In this paper we focus on the second step, chord selection, and assume the information about segmen-tation is given. Each chord Ci is selected among 24 candi-dates, 12 major triads and 12 minor ones. The choice of the  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that cop-ies bear this notice and the full citation on the first page.  Â© 2011 International Society for Music Information Retrieval  \n221Poster Session 2\n   24 triads is partially due to the fact, as indicated in [8], that 96% of the chords in the collected work by the Beatles are major and minor triads. And the choice of triads is also be-cause of our intention to focus on the fundamental chords. 3. SYSTEMS 3.1 The Rule-Based Harmonic Analyzer The Harmonic Analyzer [11] (HA) proposed by Temperley and Sleator applies preference rules to rhythm analysis and harmonization in the Western classical music tradition. To harmonize a melody, the system first divides it into seg-ments, and then assigns the root of the chord for each seg-ment, without indicating the mode (major or minor). For the purpose of this paper, we focus on the process of root finding. The system operates on the application of the four Harmonic Preference Rules (HPR): HPR 1 (Compatibility Rule): prefer certain TPC (tonal pitch-class)-root relations over others, in the following or-der: ,1Ë†,5Ë†,3Ë†,3Ë†b,7Ë†b,5Ë†b,9Ë†bornamental; HPR 2 (Strong Beat Rule): prefer chord-spans that start on strong beats of the meter; HPR 3 (Harmonic Variance Rule): prefer roots that are close to the roots of nearby segments on the line of fifths; HPR 4 (Ornamental Dissonance Rule): prefer ornamental dissonances that are (a) closely followed by an event a step or half-step away in pitch height, and (b) metrically weak. Given a melody segment, a score is calculated for each of the possible 12 roots as a weighted sum using the four preference rules. The compatibility rule (HPR 1) assigns a score to each note in the melody segment depending on the relationship of the note to the root. If the note is the tonic (1Ë†) of the root, it receives the highest score. Notes that are not listed in the compatibility rule are given penalties, de-pending on the inter-onset interval between the note to the next note a step or half-step apart in pitch and the noteâ€™s metrical strength (HPR 4). Whenever a new root is selected for a segment, i.e., a chosen root is different from the one in the previous segment, it receives a penalty based on the strength of the beat where the new root starts. If the new root starts at a strong beat, it will receive a lower penalty (HPR 2). To apply the harmonic variance rule (HPR 3), a center of gravity is calculated as the average position of roots in all previous segments on the line of fifths, weighted by the length and how recent the segments are. The current root is then assigned a penalty based on its distance to the center of gravity. The scores calculated on HPR1 and HPR 3 are further weighted by the length of the segment. Finally, a dynamic programming algorithm is applied to retrieve the path of roots that report the highest overall score. We used the implementation of the system provided by Temperley and Sleator [12] for comparison in this paper. We converted melodies in the MIDI format to text files containing a sequence of note events with beat structures as the required input for the HA system. In order to make the output of the HA system comparable to the ones from other systems, we expanded the output root into a major or a mi-nor triad. We interpreted the chords as being the common ones as described in the textbook for Music Theory [6]. The common chords, written in Roman Numerals, include I, ii, iii, IV, V, vi, and vii. For example, when a root G is report-ed by the HA system in a song in the key of C major, we assign a G major (V) instead of a G minor (v) chord. For a root not listed as either major or minor in the set of com-mon chords, we randomly assign a mode to the root. 3.2 Hidden Markov Models Statistical approaches, particularly Markov Models, have been commonly utilized for harmonic analysis and genera-tion in Western classical music [1, 5]. More recently, My-Song [9] uses HMMs to automatically choose chords to ac-company a vocal melody. Five categories of triads are con-sidered in the MySong system, including major, minor, augmented, diminished and suspended triads. Chords are represented as their functional roles in relation to the key, which is given along with each song. The system models two types of relations: the co-occurrence of a chord and the distribution of pitches in the melody segment, and the co-occurrence of two chords observed adjacently. Two proba-bility matrices are constructed to record the statistical in-formation about the two relations. The first matrix, melody observation matrix, records duration-weighted melodic pitch class histogram observed in training examples for all the chords in consideration. The second matrix, chord tran-sition matrix, shows the logarithmic likelihood of the tran-sition from one chord to another observed in the training examples. To generate chords for an input melody, a pitch class histogram is first produced for each melody segment as the observed state, and the likelihood of a chord chosen for that melody segment is calculated using melody obser-vation matrix. Combining the resulting logarithmic likeli-hood with chord transition probabilities, the Viterbi algo-rithm is then applied to retrieve the most likely possible chord sequence for the entire melody.   The main design goal of MySong is different from the topic concerned in this paper. The system was trained on hundreds of songs by various artists across many genres at once, without concentrating on any particular style. The fi-nal chord sequence was controlled by users through the ad-\n22212th International Society for Music Information Retrieval Conference (ISMIR 2011)\n   justment of two options: â€œhappy factorâ€ generates more ma-jor triads, while â€œjazz factorâ€ assigns more weights on the melody observation matrix than on the chord transition ma-trix. Regardless of the different design goal, the underlying HMMs in MySong can be easily adapted to the generation of style-specific harmonization with proper modifications. Inspired by MySong, we implemented a HMM-based mod-el for style-specific harmonization. We maintained the two matrices and the way they were calculated, and also applied the Viterbi algorithm to retrieve the final chord sequence. However, we discarded the two user options with the result that the generated chord sequence completely depends on the statistical information observed in the training exam-ples. We also limited chord selection to among major and minor triads only, resulting in a 24-by-12 melody observa-tion matrix and a 12-by-12 chord transition matrix. Infor-mation such as melody segment and key is given. During the process of training, only songs written by one artist or band are supplied.    It is important to discuss the differences between the rule-based HA system and the HMM approach. In addition to the basic musical terms such as pitch, pitch class, chord and key that exist in both systems, the HA system has em-bedded more knowledge of abstract musical structures, in-cluding scale, rhythmic hierarchy, ornamental and circle-of-fifths. The functional role of each melody note and that of each chord in relation to the hierarchical and abstract struc-ture of the song are well defined in the HA system as pref-erence rules. To generate harmonization for a given melo-dy, chords are selected by a series of calculations using pre-defined scores and penalties. In contrast, none of these ab-stract structures are considered in the HMM approach. Only two relations are modeled in the HMM system: pitch class distribution in melody for each segment (the observed state) and transitions between adjacent chords (transitions be-tween states). The preference of such relations in HMM is completely determined by the training examples without using any pre-set scores or penalties. 3.3 Automatic Style-Specific Accompaniment System In [3], Chuan and Chew proposed an Automatic Style-Specific Accompaniment (ASSA) system that generates accompaniments in a particular style to a melody given on-ly a few training examples. The system takes a hybrid ap-proach, applying statistical learning on top of a music theo-retic framework. In ASSA, the relation between melodic notes and chordal harmonies is modelled as a binary classi-fication task called chord tone determination: if the note is part of the chord structure, then the note is classified as a chord tone; otherwise it is labelled a non-chord tone. Each melody note is represented using 73 attributes, including pitch, duration, metrical strength, its relation to the neigh-bouring tones, phrase location, etc. These attributes de-scribe the functional role of each melody note in the various abstract musical structures of the song. However, unlike the HA system, the preference or suitability of a certain type of note or chord is not pre-programmed into the system; it is learned from the training examples. Therefore, the resulting classifier, a trained decision tree in ASSA, is completely determined by the style shared in common by the training songs.   Instead of representing chord transitions as pairs (source chord and destination chord) as in the HMM approach, the ASSA system applies neo-Riemannian transforms [2] to focus on the musical relationship between the two chords involved in the transition and the movements of pitches from one chord to another. For example, a transition from a C major triad to an E minor triad is described using the leading tone exchange (L) operation1 because the two triads share the pitches e and g, but the pitch c in C major is re-placed by the E minorâ€™s pitch b, which is the leading tone in C major. The transition from F major triad to A minor triad is also described using the same L operation, while such transition is recognized as a different chord pair (C major, E minor) in the HMM approach. Chord transitions in the ASSA system are represented in a manner that reflects their relation on the circle-of-fifths and voice leading be-tween the chord tones. But unlike the HA system, which always prefers the movement in the shortest distance on the circle-of-fifths, the applicability of the transition type is de-termined by the training examples.   Another difference between ASSA and the previous two systems can be observed in the generation of the final chord sequence for harmonization. ASSA generates harmoniza-tion in a divide-and-conquer fashion. The system first di-vides the input melody into sub-phrases delineated by bars in which melody notes strongly imply triads; then it gener-ates a sequence of chords for each sub-phrase independent-ly. For each sub-phrase, a Markov model is used to calcu-late probabilities of all possible chord series. Given a series of n chords, {C1, â€¦, Cn}, where each chord is indexed by its segment number, the probability that this chord series occurs can be expressed as: ),...|,...(11nnSSCCP ),,|()...,,|()|(11211211nnnnSSCCPSSCCPSCPâˆ’âˆ’= ),,|,()...,|()|(11212,111nnnnSSNROPSSNROPSCPâˆ’âˆ’=  (2)                                                              1 The four fundamental operations in neo-Riemannian transforms are I (Identify), L (Leading-tone exchange), P (Parallel) and R (Relative). \n223Poster Session 2\n   where NROi-1, i is the neo-Riemannian operation between chord Ci-1 and Ci, and Si is the phrase position of segment i, which falls into one of four possible categories: start, mid-dle, ending and final. These sub-phrases of chords are at last combined, with refinements, to produce the chord pro-gression for the entire melody.  4. EXPERIMENTS AND RESULTS 4.1 Experiments The objective of the paper is to examine the effectiveness of the three approaches â€“ a rule-based system, a statistical model and a hybrid system â€“ for the automatic generation of style-specific harmonization. We used 140 songs by sev-en stylistically distinct pop/rock bands, including the Beat-les (B), Bon Jovi (BJ), Green Day (GD), Guns Nâ€™ Roses (GR), Indigo Girls (I), Keane (K) and Radiohead (R). Songs by the same band are considered to have similar styles. We obtained information about each song such as melody, chord and key from the commercial lead sheet. Melodies were encoded in the MIDI format while chords and keys were written in text files with melody segments specified.   For systems with learning abilities, we conducted the Leave-One-Out test. We selected one song as the test song and formed a training set using the remaining songs by the same artist. We then compared the generated chords with the ones given in the commercial lead sheet (the ground truth) of the test song. To examine the manner in which the number of training examples affects the performance of the systems, we constructed training sets with various sizes by gradually adding one song into the set. Suppose we have m songs by an artist and n represents the number of songs in the training set, 1â‰¤ n â‰¤ m-1. For each test song, we can con-struct 1âˆ’mnCdifferent training sets. Therefore, for each n, we will have results from m x1âˆ’mnCdifferent test instances. The number of test instances grows quickly and becomes infea-sible as m and n increase. For example, if we have 20 songs by an artist and we form test sets of 10 songs, the resulting number of test instances is 20 x 1910C = 1847560. We lim-ited the number of training sets by randomly choosing 120 training sets for each test song if the total number of possi-ble training sets exceeds 120. Therefore, for each n, the number of test instances is bounded by 120 x m. On the other hand, for the rule-based HA system that does not re-quire training examples, the total number of test instances for an artist is equivalent to m.      4.2 Results 4.2.1 Same Chord Percentage Figure 1 shows the average percentage of generated chords that are identical to the ones in the ground truth with 95% confidence interval. Notice that the ASSA system reports a higher same chord percentage when the number of training songs increases. But the same chord percentage of HMM decreases as the increment of training songs increases in all cases except the one shown in Figure 1 (b). In general, ASSA reports higher or at least equivalent same chord per-centage as HMM. However, comparing with ASSA and HMM, it is difficult to make general comments on the re-sult of rule-based HA (the one with zero training songs) be-cause of its wide confidence interval.  \n Figure 1. Same chord percentage with different sizes of training sets. 4.2.2 Related Chord Percentage Figure 2 shows the average percentage of generated chords that are closely related to the ground truth. Two chords are considered closely related if they show one of the following relations: identical, dominant, subdominant, relative, paral-lel, dominant/relative, dominant/parallel, subdomi-nant/relative and subdominant/parallel. For example, if the ground truth is C major, the closely related chords in the order are C major, G major, F major, A minor, C minor, E minor, G minor, D minor and F minor. When related chord \n22412th International Society for Music Information Retrieval Conference (ISMIR 2011)\n   percentage is considered, the rule-based HA performs the best in general. HMM and ASSA perform similarly, but as the number of training songs increases, the results for ASSA improves while those for HMM decline. \n Figure 2. Related chord percentage with different sizes of training sets. 4.2.3 Average Number of Unique Chords We also examine the number of unique chords generated for each song by the three systems, and compare that with the number of unique chords in each bandâ€™s original songs. Each unique chord chosen by a composer is analogous to a color used by a painter, and the number of colors that ap-pear in a painting is usually considered a contributing factor of a painting style. The number of unique chords equals the total number of chords in the sequence subtract the number of duplicate chords. Figure 3 shows the average number of unique chords generated by the three systems and in the original songs. Notice that the average number of unique chords generated by the rule-based HA system is the closest to but slightly lower than the ground truth (GT). The num-ber of unique chords generated by HMM grows as the number of training examples increases, which provides more chords as cases for HMM to learn from. In contrast, the number of unique chords generated by ASSA drops and becomes closer to the ground truth when the number of training examples increases. This may result from the use of the neo-Riemannian transform, which only represents the relative relation in the transition between chords, allowing more freedom to choose chord pairs that are not included in the training set as long as they share the same transition. \n Figure 3. Average number of unique chords per song using HMM, ASSA, HA and in the ground truth (GT). 4.2.4 Average Number of New Chords For systems that require training examples, it is important to study how these examples affect the output. Particularly, we are interested in the systemâ€™s ability to generate chords that are not given in the training examples. For comparison, we also investigate the original songs to observe the num-ber of chords in a song that do not appear in a given set of other songs by the same artist. We label these unseen chords as new chords.  Figure 4 presents the average number of new chords generated by HMM and ASSA, and in the original accom-paniment, the GT. In the original accompaniments, when the training set is small, there are always one or two new chords in each song. As the training set grows, the training examples gradually cover all the chords in each song. In ASSA, because of the neo-Riemannian framework, it demonstrates the ability to create new chords but tends to generate too many when the training examples are too few. More training examples help ASSA become stable. On the other hand, the output chords of HMM are fully limited by the chords given in the training examples.  \n225Poster Session 2\n   \n Figure 4. Average number of new chords per song using HMM, ASSA, HA and in the ground truth (GT). 5. CONCLUSIONS AND FUTURE WORK In this paper we compared three different approaches, a rule-based model, a statistical model, and a hybrid system combining the two, for automatic style-specific harmoniza-tion in popular music. We conducted experiments by using songs from several stylistically identifiable pop/rock bands, having the systems generate chords to harmonize given melodies, and compared the generated chords with the orig-inal. We observed that the rule-based system generates the most chords within a close range of the original. As the number of training examples increases, the hybrid system reports more chords identical to the original than the other systems. Although the hybrid system has the ability to gen-erate chords that were not present in the training set, it tends to produce too many types of chords for a given song. The HMM-based system, however, produces fewer and fewer chords that are similar to the original as the size of the training set grows. In the future we plan to study differ-ent approaches for dividing melodies into melody segments for the harmonization task. We also plan to explore other methods for evaluating system-generated harmonization in a particular style. Besides comparing the generated chords with the original, we will investigate means for measuring the tension and relaxation created in the harmonization.  6. REFERENCES [1] M. Allan, C. K. I. Williams: â€œHarmonising Chorales by Probabilistic Inference,â€ Proceedings of the Neural Information Processing Systems Conference, Vancouver, 2004. [2] G. Capuzzo: â€œNeo-Riemannian Theory and the Analysis of Pop-Rock Music,â€ Music Theory Spectrum, vol. 26, no. 2, pp. 177-199, 2004. [3] C. H. Chuan and E. Chew: â€œA Hybrid System for Automatic Generation of Style-Specific Accompaniment,â€ Proceedings of the fourth International Joint Workshop on Computational Creativity, London, 2007. [4] K. Ebcioglu, â€œAn Expert System for Harmonizing Four-Part Chorales,â€ Machine Models of Music, Cambridge, The MIT Press, 1993. [5] M. Farbood and B. Schoner: â€œAnalysis and Synthesis of Palestrina-Style Counterpoint Using Markov Chains,â€ Proceedings of the International Computer Music Conference, Havana, 2001. [6] S. Kostka and D. Payne: Tonal Harmony, McGraw-Hill, New York, 2003. [7] H. R. Lee and J. S. Jang, â€œi-Ring: A System for Humming Transcription and Chord Generation,â€ Proceedings of the IEEE International Conference on Multimedia and Expo, Taipei, Taiwan, 2004. [8] M. Mauch, S. Dixon, M. Casey, C. Harte and B. Fields, â€œDiscovering Chord Idioms through Beatles and Real Book Songs,â€ Proceedings of the 8th International Conference on Music Information Retrieval, Vienna, 2007. [9] D. Morris, I. Simon and S. Basu, â€œMySong: Automatic Accompaniment Generation for Vocal Melodies,â€ Proceedings of Computer-Human Interaction, Florence, 2008. [10] S. Phon-Amnuaisuk, A. Tuwson and G. Wiggins, â€œEvolving Music Harmonization,â€ Artificial Neural Nets and Genetic Algorithm: Proceedings of Fourth International Conference in Portoroz, Slovenia, 1999.  [11] D. Temperley: The Cognition of Basic Musical Structures, MIT Press, Cambridge, 2004. [12] D. Temperley and D. Sleator, Harmonic Analyzer, www.cs.cmu.edu/sleator/harmonic-analysis/ \n226"
    },
    {
        "title": "The Melodic Signature Index for Fast Content-based Retrieval of Symbolic Scores Camelia Constantin.",
        "author": [
            "CamÃ©lia Constantin",
            "CÃ©dric du Mouza",
            "ZoÃ© Faget",
            "Philippe Rigaux"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1416860",
        "url": "https://doi.org/10.5281/zenodo.1416860",
        "ee": "https://zenodo.org/records/1416860/files/ConstantinMFR11.pdf",
        "abstract": "NEUMA is an on-line library that stores collections of symbolic scores and proposes a public interface to search for melodic pieces based on several kinds of patterns: pitchesbased, with or without rhythms, transposed or not. In addition, searches can be either exact or approximate. We describe an index structure apt at supporting all these searches in a consistent setting. Its distinctive feature is an encoding of the various information that might be involved in the pattern-matching process with algebraic signatures. The properties of these signatures are suitable to represent in a compact and expressive way the sequences of complex features that constitute a melodic description.",
        "zenodo_id": 1416860,
        "dblp_key": "conf/ismir/ConstantinMFR11",
        "keywords": [
            "on-line library",
            "symbolic scores",
            "public interface",
            "search for melodic pieces",
            "patterns",
            "pitches-based",
            "with or without rhythms",
            "transposed or not",
            "exact or approximate",
            "index structure"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nTHE MELODIC SIGNATURE INDEX FOR\nFAST CONTENT-BASED RETRIEV AL OF SYMBOLIC SCORES\nCamelia Constantin\nLIP6, Univ. Paris 6, Paris, France\ncamelia.constantin@lip6.fr\nZoÂ´e Faget\nArmadillo & Univ. Paris-Dauphine, France\nzoe@armadillo.frCÂ´edric du Mouza\nCEDRIC, CNAM, France\ndumouza@cnam.fr\nPhilippe Rigaux\nCEDRIC, CNAM, France\nphilippe.rigaux@cnam.fr\nABSTRACT\nNEUMA is an on-line library that stores collections of sym-\nbolic scores and proposes a public interface to search for\nmelodic pieces based on several kinds of patterns: pitches-\nbased, with or without rhythms, transposed or not. In addi-\ntion, searches can be either exact or approximate. We de-\nscribe an index structure apt at supporting all these searches\nin a consistent setting. Its distinctive feature is an encod-\ning of the various information that might be involved in the\npattern-matching process with algebraic signatures . The\nproperties of these signatures are suitable to represent in a\ncompact and expressive way the sequences of complex fea-\ntures that constitute a melodic description.\n1. INTRODUCTION\nContext and motivation. NEUMA is a Digital Score Li-\nbrary devoted to the publication of digital music scores. Putting\nthis material on-line offers an opportunity for web-based\nsharing of musical scores archives, including collaborative\nproduction, annotation, and large-scale corpus analysis. In\nthe present paper, we focus on the functionalities that per-\nmit to undertake large-scale studies of melodic, harmonic\nor stylistic material. One of the musical investigations cur-\nrently conducted by our fellow musicologists working with\nNEUMA considers a melodic rÂ´epertoire in a given cultural\narea, and studies how this r Â´epertoire is exchanged and bor-\nrowed throughout various styles, periods and composers.\nUsing efï¬cient tools to retrieve and compare similar melodies\nleverages the scope of investigations that can be conducted\nfor such a study. To this end, N EUMA provides a set of\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proï¬t or commercial advantage and that copies\nbear this notice and the full citation on the ï¬rst page.\nc/circlecopyrt2011 International Society for Music Information Retrieval.functions that support the analysis process. The pattern-\nmatching function takes a pattern Pand carries out a search\nover the score collections, looking for allthe melodic frag-\nments that â€œmatchâ€ P. The function can be parameterized\nby combining one of the following options: Exact search ,\nwhich can itself be reï¬ned as Transposed/non transposed\nand/or With/without rhythm , and Approximate search , which\ncomparesPto melodic fragments considered in their full\ndimensions (pitch, rhythm) and applies a similarity func-\ntion. The user is free to choose an appropriate combination\nof these choices (called an interpretation in the following),\nand this yields a quite appreciated ï¬‚exibility to the system.\nThis ï¬‚exibility has a cost, though, since the system must be\nready to face several possible pattern interpretations.\nIndexing the pattern-matching retrieval process. As our\ncollections grow, the need for an indexing mechanism able\nto directly access the scores of interest for a given pattern\nbecame prominent. Building an index for each possible in-\nterpretation would have been cumbersome due to the major\nredundancy of information in the associated descriptors. We\nrather chose to design a specialized index, able to satisfy\nseveral interpretations. This design, and the experiments\nthat validate the resulting structure, constitute the purpose\nof the present paper.\nIn short, the principles of our index, called Melodic Sig-\nnature Index (MSI), can be summarized as follows: (i) its\nkernel structure is that of a traditional hash ï¬le , with an in-\nmemory directory that refers to a list of on-disk buckets ; (ii)\neach entryein the directory corresponds to the hash value\nheof some ï¬xed-size melodic fragments, called n-grams,\npresent in at least one score of the collections; the associ-\nated bucket actually contains the list of allthen-gram occur-\nrences that hash to he; (iii) the index implementation is con-\nsistently built over algebraic signatures computed from the\nmelodicn-grams, and representing the various aspects that\nmight be addressed by one of the possible pattern-matching\ninterpretations.\n363Poster Session 3\nWhereas the ï¬rst two aspects are drawn from the state-\nof-the-art in terms of large text-encoded indexing [14], the\nlast one is inspired by recent work on signature-based text\nprocessing [7, 10], tailored to the speciï¬cs of symbolic mu-\nsic retrieval. The resulting structure enjoys several features\nthat make it a suitable choice for large score libraries index-\ning, namely (i) ï¬‚exibility â€“ a single index supports several\ndistinct pattern-matching operations, (ii) compactness â€“ in\nspite of the rich information content it contains, the index\nspace requirement is only a fragment of the overall collec-\ntion storage, and (iii) efï¬ciency â€“ as shown by our analytic\nstudy and experiments, a few milliseconds sufï¬ce to retrieve\nthe result, even for very large patterns searched for in very\nlarge collections.\nRelated work. Two main approaches for off-line indexing\nscore collections have been investigated: tree-based [9, 13,\n20] and inverted ï¬les [3,5,16]. [5,16] propose to index both\nthe pitch interval and rhythm sequences in an inverted ï¬le.\nWe adopt a similar approach, with a much richer encoding\nthat allows to reach a constant search complexity and more\nï¬‚exibility in terms of search options.\nThe subjective nature of measuring music similarity lead\nto the introduction of several error measures. The Î´and\n(Î´,Î±) approximations [2] use exact matching algorithms for\nsimilarity search. Many algorithms for efï¬cient computa-\ntion of similarity matching through exhaustive search have\nbeen proposed [1, 4]. In general, indexing can be achieved\nwith a high-dimensional structure whose performances are\nknown to deteriorate as the dimension increases. In the spe-\nciï¬c context of the edit distance, several indexing methods\nhave been suggested, an overview of which can be found\nin [15]. A classical technique is to introduce an measure ap-\nproximating the edit distance but easier to index [12]. The\nidea of using n-gram for melody retrieval and measuring\nmusic similarity is not new in monophonic [17,19] as well as\npolyphonic pieces [6, 8], although they usually model only\nsome of the music information. Our structure enjoys the\nnice feature of being able to index both exact search with\nmany variants, and approximate search based on the edit\ndistance. This makes it a structure of choice to solve the\naddressed problem of index pattern searches in large score\ndatabases.\nThe rest of the paper presents our structure (Section 2)\nand the pattern-matching algorithms (Section 3). Section 4\nbrieï¬‚y reports the performance results obtained over a large\ncollection of scores, and Section 5 concludes the paper.\n2. THE MELODIC SIGNATURE INDEX\nWe outline in this section the index structure in N EUMA ,\nwith emphasis on algebraic information put in index records.2.1 Index overview\nNEUMA interprets scores content according to a â€œmodelâ€ of\nsymbolic music. The model of interest to this work relies\non a synchronized time series approach that sees a score as\na superposition of voices . Each voice is a sequence of ele-\nments inEÃ—D , whereEis the domain of musical â€œeventsâ€\n(notes, chords, rest, etc.) and Dthe musical duration. A de-\nscriptor can be text-encoded in the form <e1-d1; e 2-d2;\n...; en-dn>where each e iencodes an event and each d i\nits duration. In the following, we shall blur the distinction\nbetween a descriptor and its textual encoding. Given a de-\nscriptord, we denote as /epsilon1(d)the sequence of events (without\ndurations) and as Ï(d)the sequence of durations (without\nevents) ofd.\nExample 1 Voicev, in score 354, encodes a melody begin-\nning with a G3 (half), followed by an A3 (half), a B3 (ï¬‚at,\nquarter), etc. Its descriptor dvis: (22-2;24-2;25-4;24-4;22-\n4;21-4;22-4;. . . ) Moreover, /epsilon1(dv)= (22, 24, 25, 24, 22, 21,\n22, . . . ) and Ï(dv)=(2, 2, 4, 4, 4, 4, 4, . . . ).\nIn the example above, note heights are encoded with chro-\nmatic notation (number of semi-tones from the lowest pos-\nsible sound). Rest, chords, and silence are encoded with\nother, non ambiguous, symbols: we do not elaborate fur-\ntherEwhich provides a compact representation of melodic\nsequences.\nGiven a pattern P, a search retrieves the scores such that\nforat least a voicev, and at least an offset (position) oin\nv,Pmatches the fragment v[o]v[o+ 1].... The seman-\ntics of a matching attempt depends on the interpretation of\nP, chosen by the user at query time. We explain the pro-\ncess with an example: let Pbe the pattern described by\n37-4;35-4;34-2 . Then, under the exact search, trans-\nposed, without rhythm interpretation, Pmatches the voice\nvof Example 1 at offset 3 (offsets start at 0). If we take\nthe rhythm into account, this is no longer true. Using a non-\ntransposed interpretation also leads to a failure, with or with-\nout rhythm. Finally, an approximate search likely detects a\nhigh similarity between Pandvat position 3.\n2.2 Algebraic signatures\nWe interpret our melodic events in Eas elements of a Galois\nï¬eldGF(2f)of size 2f. The elements of GFare bit strings\nof lengthf. Since|E|â‰¤ 255, we letf= 8in the following.\nA Galois ï¬eld is a ï¬nite set that supports addition and mul-\ntiplication. These operations are associative, commutative\nand distributive, have neutral elements 0and1, and there ex-\nist additive and multiplicative inverses. A primitive element\nÎ±ofGFis such that its powers enumerate all the non-zero\nelements of the Galois ï¬eld. Let D=e0e1Â·Â·Â·eMâˆ’1be a\ndescriptor encoding a sequence of Mevents interpreted as\nGF elements. We deï¬ne an AS signature as follows.\n36412th International Society for Music Information Retrieval Conference (ISMIR 2011)\nDeï¬nition 1 TheASÎ±-signature of a descriptor Dis de-\nï¬ned by\nASÎ±(D) =e0+e1Â·Î±+e2Â·Î±2...+eMâˆ’1Â·Î±Mâˆ’1(1)\nIf we consider mprimitive elements Î±1,Î±2,...,Î±m, the\nm-symbols signature NASm(D)is obtained by concatenat-\ning the set of ASÎ±i(D),1â‰¤iâ‰¤m, seen as bit strings. This\nallows to obtain a signature of size m.\nGiven a descriptor D, we are interested in partial alge-\nbraic signatures calculated from substrings of D.\nDeï¬nition 2 Letlâˆˆ[0,Mâˆ’1]be any offset inD. The\nCumulative Algebraic Signature (CAS) atl,CAS (D,l), is\nthe algebraic signature of the preï¬x of Dending atel, i.e.,\nCAS (D,l) =AS(e0...el).\nThePartial Algebraic Signature (PAS) from l/primetolis the\nvaluePAS (D,l/prime,l) =AS(el/primeel/prime+1Â·Â·Â·el), with 0â‰¤l/primeâ‰¤l,\nWe most often use the PAS of sub-sequences of length n,\ni.e., ofn-grams.\nDeï¬nition 3 Then-gram Algebraic Signature (NAS) of D\natlisNAS (D,l) =PAS (D,lâˆ’n+ 1,l), forlâ‰¥nâˆ’1.\nelelâˆ’n+1elâ€™e0eMâˆ’1CAS(l) PAS(lâ€™, l) NAS(l)\nDescriptor D\nFigure 1 .CAS (l),PAS (l/prime,l)andNAS (l)in descriptor D\nWe may drop Dwhenever it is implicit for brevityâ€™s sake.\nFigure 1 shows the respective parts of the record that deï¬ne\ntheCAS ,PAS andNAS at offsetl. The following simple\nproperties of algebraic signatures are useful for what fol-\nlows. Properties 2 and 3 let us incrementally calculate next\nCAS and NAS while indexing the score, or preprocessing\nthe pattern, instead of recomputing the signature entirely.\nThis speeds up the process considerably.\nCAS (l) =CAS (lâˆ’1) +elÂ·Î±l(2)\nNAS (l) =NAS (lâˆ’1)âˆ’elâˆ’n\nÎ±+elÂ·Î±nâˆ’1(3)\nProperty 4 ï¬nally is fundamental for the match attempt\ncalculus. For 0â‰¤l/prime<l:\nCAS (l) =CAS (l/prime) +Î±l/prime+1PAS (l/prime+ 1,l) (4)\nWe refer the reader to [11] for more details about deï¬ni-\ntions and properties of algebraic signatures. The above are\nsufï¬cient to describe the MS-index features.2.3 The Melodic Signature index\nThe Melodic Signature Index (MS-Index) is a classical hash\nï¬le, denoted HD[ 0..Lâˆ’1], with directory length L= 2v\nbeing a power of 2 (Figure 2). Elements of HD refer to\nbuckets or lines of variable length.\nBucketsrecords for CAS c\nr2 <i1,o1,c1,câ€™1,A1,u1>\nLâˆ’10\nCC0\niHash directory HDr1\nrecords for CAS câ€™Structure of a bucket\n... <ik,ok,ck,câ€™k,Ak,uk><i2,o2,c2,câ€™2,A2,u2>\n...\nFigure 2 . Structure of the MS-Index\n.\nEach bucket stores a list of hash records (records in short),\neach indexing some ï¬xed-size fragment of a voice descrip-\ntor, calledn-gram. Fragment (24-4;22-4;21-4) is for in-\nstance a 3-gram extracted from the descriptor of Example 1.\nThe actual value of nis a parameter of the MS-Index, to\nbe discussed next. To build the index, we process all n-\ngrams in the score library. From each n-gramGof the form\ne1-d1;...;en-dnwe derive a number of algebraic sig-\nnatures that determine the index organization and content.\nWe ï¬rst use signatures to calculate the index iof the line\nthat refers to G. LetÏ„be the transform that extracts from\nGa (n-1)-gram with the sequence of pitch intervals. We\ncalculateiby hashing on the intervals signature. Let s=\nNASm(/epsilon1(G))be them-symbol signature of Gfor some\nm(see below), interpreted as a large, unsigned integer and\ncompute index ias:\ni=hL(S) =SmodL\nSinceL= 2v, this amounts to extracting the last vbits of\nS.mshould be such that mâ‰¤nandmâ‰¥âŒˆv/fâŒ‰.\nExample 2 LetGbe the 4-gram (24-4;22-4;21-4;22-4). Then\n/epsilon1(G)=(24, 22, 21, 22) and Ï„(/epsilon1(G))=(-2, -1, 1) (e.g., the\npitch interval encoding). Assume m= 3. We select three\nindependent primitive elements Î±1,Î±2, andÎ±3in the Ga-\nlois Field. The index of Gin the hash ï¬le is:\nASÎ±1(Ï„).ASÎ±2(Ï„).ASÎ±3(Ï„) modL\nwhere.represents bit string concatenation.\nThe properties of AS signatures ensure a balanced distri-\nbution of the hash values in the range [0..Lâˆ’1]. Next, we\ninsert in HD [i]arecord describingG, deï¬ned as follows:\n365Poster Session 3\nDeï¬nition 4 LetGbe ann-gram at offset oin a descrip-\ntorD. The record indexingG, denotedR(G), is a 6-uplet\n(id(D),o,c/epsilon1,cÏ,ASÏ,âŠ¥)where\n1.c/epsilon1isCAS (/epsilon1(D),o), i.e., the event CAS ofGato;\n2.cÏisCAS (Ï(D),o), i.e., the rhythm CAS ofGato;\n3.ASÏisNASm(Ï(D),o), i.e., its rhythm signature;\n4.âŠ¥is the minimal pitch index in G, representing (along\nwith the previous signatures) its absolute height.\nThe hash record of an n-gram contains all the informa-\ntion necessary to evaluate matching attempts at run time, by\ncombining the signatures with the Galois Field operators to\nevaluate the required pattern interpretation.\nExample 3 Consider again the 4-gram Gof Example 2, as-\nsuming it is found at offset 3. Then c/epsilon1andcÏare obtained\nfrom the cumulative values at offset oâˆ’1, thanks to Prop-\nerty 2;AÏis theNAS signature of Ï(G)=(4, 4, 4, 4);âŠ¥is\n21, the minimal pitch of the n-gram.\nConstruction time complexity. The MS-index is built in lin-\near time in the size of the score library. Note in particular\nthat the cumulative signature at offset ocan be derived from\nthe cumulative at offset oâˆ’1.\nSpace complexity. The size of the directory, HD, is neg-\nligible. Given a descriptor D, a record occupies 3 + 2 +\n1 + 1 + 1 + 1 = 9 bytes, and the index size is therefore\n|L|Ã—Ï„DÃ—9, whereÏ„Ddenotes the ratio of descriptorâ€™s\nsize with respect to a full score size. Standard indexed ï¬le\ncompression techniques (e.g., variable bytes compression)\nfurther reduce the space requirements. As shown by our ex-\nperiments,Ï„Dis typically of the order of 10/00and, in spite\nof its rich content, our index occupies a small fraction of the\nwhole library space.\n3. SCORE RETRIEV AL\nDue to space limitation, we give in this section an informal\npresentation of the algorithms.\n3.1 Exact search, basic algorithm\nWe explain (Figure 3) an exact search, transposed and with-\nout rhythm (that is, we consider as a match any sequence\nof pitch intervals similar to that of P). First, we preprocess\nPfor three signatures: (i) of the initial n-gramS1, (ii) of\nthe ï¬naln-gramS2and (iii) of the sufï¬x SpofPafterS1.\nHashing onS1locates the bucket with every record r1hash-\ning to the signature of S1. Likewise, hashing on S2locates\nthe bucket with every r2hashing to the signature of S2. We\nonly consider pairs of records that are in the same voice and\nat the right distance among them (looking at offsets). We\nfailure\nPattern PS1Sp\nS2h(S1)\nh(S2)\nHash directorye1\ne2AS(e1, e2, Sp)\nsuccessFigure 3 . A matching attempt with MS-Index\nthus locate any descriptor DmatchingPon its initial and\nterminaln-gram, at least by signature. An algebraic calcu-\nlationAS(r1,r2,Sp), based on the cumulative signatures,\ndetermines whether Spmay match the sufï¬x of Das well.\nSearch complexity. By limiting disk accesses to the two\nbuckets associated to the ï¬rst and last n-grams of the P,\nMS-Index search runs independently from Pâ€™s size. The\ncost of the search procedure outlined above is reduced to\nthat of reading two buckets. The hash directory is cached\nin RAM. With an appropriate dynamic hashing mechanism\nthat evenly distributes the records in the structure and scales\ngracefully, the bucket size is expected to remain uniform\nenough to let the MS-Index run in constant time.\n3.2 Exact search, other interpretations\nOther interpretations than the basic one are obtained with\nstraightforward extensions to the above algorithm, namely\n1) non-transposed search, without rhythm, is obtained by\ncomparing the minimal pitch index of Pâ€™s initialn-gram\nand the valueâŠ¥ofr1; 2) searching with rhythm implies\na calculus similar to that on intervals, using r1.cÏ,r2.cÏand\nAÏas input ; 3) any combination of these criteria is possible\nto achieve the required interpretation.\nThe cost analysis remains similar, since the signatures\ncomparison is negligible regarding that of buckets access.\n3.3 Approximate search\nOur index supports the similarity measure using n-grams\nintroduced by Ukkonen [18]. The more n-grams the two\nstrings have in common, the higher the similarity. The nâˆ’gram\nproï¬le is a vectorGPsuch thatGP[S]is the number of oc-\ncurrences of the n-gramSinP. The â€œdistanceâ€ between\ntwo stringsPandQis then:\nAn(P,Q) = Î£vâˆˆÎ£n|GP[v]âˆ’GQ[v]|,\nwhere Î£nis the set of all possible nâˆ’grams.\n36612th International Society for Music Information Retrieval Conference (ISMIR 2011)\ncollection1# ï¬les ï¬les size # desc. desc. size\nbach 280 27.1 MB 1,243 539 KB\ngut 137 197.2 MB 352 2,413 KB\nhausmusik 452 140.9 MB 1,218 1,944 KB\nhymns 1,752 84.6 MB 3,885 1,954 KB\nmusicxml 405 38.9 MB 1,738 713 KB\nwikifonia 3,583 302.7 MB 3,570 2,787 KB\nwima 961 427.3 MB 3,110 4,624 KB\nmisc 94 8.9 MB 101 89 KB\nall 7,664 1,227.6 MB 15,517 15,063 KB\nTable 1 . MusicXML collections used in N EUMA\nThe approximate search of a pattern Pin a symbolic\nscore proceeds as follows. Given a descriptor D=e1...eN,\na patternP=p1...pmwe pre-process Pto get all the n-\ngramsS1,S2, . . .Sqoccurring in P. We access the MS\nindex and retrieve, for each Si,iâ‰¤q, the list of the records\nfeatured in the document with the same signature than h(Si).\nWe then sort-merge all lists into one list, ordered with re-\nspect to each descriptor. We take the ï¬rst list of offsets and\napply a moving window of size L= 2mâˆ’n+ 1in which\nwe solve the approximate search problem. Indeed we can\nshow that a window of size Lhas2mâˆ’2n+ 2n-grams,\nfrom which at most mâˆ’n+ 1 belong toPand at least\nmâˆ’n+ 1do not belong to P. For windows of size greater\nthan2mâˆ’n+ 1,n-grams not belonging to Pwill always\noutnumber those who do.\nWe compute the Andistance between the pattern and all\nsubsequences starting on the left edge of the window, and\nkeep track of the ending position for the best one inside the\nwindow. We repeat this process for all offsets of the list\nby sliding the window along the list. We return all triplets\n(istart,iend,di)which comply to the maximum error toler-\nance.\n4. EXPERIMENTS\nWe built a library of MusicXML scores collected from sev-\neral public on-line collections, reported in Table 1. There\nexists an important discrepancy in the size of the descrip-\ntors. The average descriptor size is 967 bytes, and it ranges\nfrom 444B on average in bach to 7,020B in gutenberg\n(noted gut). The ratio (descriptorsize/documentsize )\nvaries from 90/00inwikifonia to 230/00inhymns .\nTable 2 reports the building time and the size of the MS-\nIndex for different datasets. For bach ,gut andwima , we\nchoose 4-grams. The building time does not linearly in-\ncrease with the descriptors size. For instance gut, whose\ndescriptors size is half that of wima , requires a third of the\n1bach : www.jsbchorales.net, hausmusik : www.hausmusik.ch,\ngut: www.gutenberg.org/wiki/Gutenberg:The Sheet Music Project,\nhymns : www.hymnsandcarolsofchristmas.com,\nmusicxml : www.musicxml.org, wikifonia : www.wikifonia.org,\nwima : www.icking-music-archive.orgbuilding time of wima , while all (4-gram), with a descrip-\ntor size 3 times larger than wima , needs 7.5 times more\ntime. This results from both the handling of hash collisions\nand variable-bytes compression (not detailed here).\nAs expected, the size of the index linearly depends on the\ndescriptors size. Finally using larger n-grams has a minor\nimpact on the index size, but an important one on the build-\ning time: e.g7-gram index requires 25% more space than\n3-gram index thanks to lower compression rate, but a build-\ning time 7 times higher, due to less collisions to handle and\nless compression to perform.\ncollection building time size\nbach 0.7 s 1.0 MB\ngut 3.3 s 5.1 MB\nwima 11.4 s 9.5 MB\nall (3-gram) 206.6 s 28.5 MB\nall (4-gram) 82.6 s 29.7 MB\nall (5-gram) 47.0 s 31.3 MB\nall (6-gram) 35.9 s 33.2 MB\nall (7-gram) 33.2 s 35.1 MB\nTable 2 . Building time for different collections\nnâˆ’gram size 3 4 5 6 7time (ms)\n20406080\nwith rhythmtransposed\nexact\nFigure 4 . Impact of the n-gram size on matching time\nFigure 4 shows that the longer the n-grams, the faster\nthe search, whatever the interpretation2. Longern-grams\nmeans less collisions, and thus smaller buckets. Differences\nbetween exact, transposed or without rhythm search perfor-\nmances are mostly due to the selectivity of the search cri-\nteria. Unlike transposed search (T R), we eliminate for an\nexact search (E X) records in the ï¬rst bucket (retrieved using\nthe NAS of the ï¬rst n-gram) by checking the ï¬rst note on the\nn-gram. This decreases the comparisons to perform. Search\ntransposed with rhythm and search exact with rhythm ex-\nhibit similar performances, and run faster than T Ror E X\nsince we ï¬lter records using an additional signature.\nFinally we study the search time in Table 3 and compare\nperformances with those of an exhaustive scan. MS-Index\noverperforms for all datasets the exhaustive search (the ra-\ntio ranging from 800% to 10,000%). The search time with\nMS-Index does not depend on the descriptors size: wima is\ntwice larger than gut but searches are performed 4 times\n2We limit the presentation of the results to exact search.\n367Poster Session 3\ncoll. TR TR+RY EX EX+RY\ngutMS-index 38.1 27.8 36.9 32.4\nSC 323.1 212.2 293.4 302.1\nspeed-up 8.5 7.6 7.9 9.3\nwimaMS-index 10.4 7.5 9.7 7.5\nSC 637.4 432.1 581.1 595.2\nspeed-up 61.3 57.6 59.9 79.3\nallMS-index 41.6 20.7 33.3 24.5\nSC 2,514.2 1,490.2 2,305.3 2,030.1\nspeed-up 60.4 72.0 69.2 82.9\nTable 3 . Impact of the dataset size on search time (ms)\nfaster, and the same ratio holds when comparing to all\nwhereas its size is 3 times larger. Our index performances\nare more sensitive to the data distribution since skewness\nleads to large bucket, thus a larger number of tests. Searches\nwith rhythm are faster since they ï¬lter out records in the ï¬rst\nbucket (resp. n-grams) for the MS-Index (resp. exhaustive\nscan), skipping useless comparisons. The speed-up is lower\nforgut than for other collections. The rationale is that gut\npresents a few, large ï¬les (137) with more records for each\ndocument in a bucket. Since the id of the document is also\na ï¬ltering condition (we try to match an entry of the ï¬rst\nbucket with one of the second bucket from the same docu-\nment), more matching attempts are carried out.\n5. CONCLUSION\nWe described in this paper a practical approach to the prob-\nlem of indexing pattern-based searches in a large score li-\nbrary. Our solution supports exact and approximate searches,\nand permits to reï¬ne exact searches by taking account of the\nmany components that constitute a melodic descriptor. Our\nexperiments show that a few milliseconds sufï¬ce to obtain\nthe result in all cases even for signiï¬cantly large datasets.\nA nice feature of our index is that it also acts as an ini-\ntial ï¬lter in a two-steps similarity search method that per-\nforms a ï¬nal check on the candidates against the full de-\nscriptor. This leaves the opportunity to adapt the edit dis-\ntance to the speciï¬cs of music score similarity search. We\nare currently investigating the relevance of such adaptations\nwith our users.\n6. REFERENCES\n[1] E. Cambouropoulos, M. Crochemore, C. S. Iliopoulos, M. Mo-\nhamed, and M.-F. Sagot. A Pattern Extraction Algorithm for\nAbstract Melodic Representations that Allow Partial Overlap-\nping of Intervallic Categories. In ISMIR , pages 167â€“174, 2005.\n[2] D. Cantone, S. Cristofaro, and S. Faro. Solving the (Î´,Î± )-\nApproximate Matching Problem Under Transposition Invari-\nance in Musical Sequences. In ISMIR , pages 460â€“463, 2005.\n[3] C.-W. Chang and H. C. Jiau. An Efï¬cient Numeric Indexing\nTechnique for Music Retrieval System. In ICME , 2006.[4] R. Clifford and C. Iliopoulos. Approximate string matching\nfor music analysis. Soft Computing - A Fusion of Foundations,\nMethodologies and Applications , 8, 2004.\n[5] S. Doraisamy and S. M. R Â¨uger. A Polyphonic Music Retrieval\nSystem Using N-Grams. In ISMIR , 2004.\n[6] S Doraisamy and S M R Â¨uger. An approach towards a poly-\nphonic music retrieval system. In ISMIR , pages 187â€“93, 2001.\n[7] C. du Mouza, W. Litwin, P. Rigaux, and T. J. E. Schwarz. AS-\nindex: a Structure for String Search Using N-grams and Alge-\nbraic Signatures. In CIKM , pages 295â€“304, 2009.\n[8] R. Hillewaere, B. Manderick, and D. Conklin. String quartet\nclassiï¬cation with monophonic models. In ISMIR , pages 537â€“\n542, 2010.\n[9] I. Karydis, A. Nanopoulos, A. N. Papadopoulos, and\nY . Manolopoulos. Audio Indexing for Efï¬cient Music Infor-\nmation Retrieval. In MMM , pages 22â€“29, 2005.\n[10] W. Litwin, R. Mokadem, P. Rigaux, and Th. Schwarz. Fast\nnGram Based String Search over Data Encoded Using Alge-\nbraic Signatures. In VLDB , 2007.\n[11] W. Litwin and T. Schwarz. Algebraic Signatures for Scalable\nDistributed Data Structures. In ICDE , pages 412â€“423, 2004.\n[12] N.-H. Liu, Yi-Hung Wu, and A. L. P. Chen. An Efï¬cient Ap-\nproach to Extracting Approximate Repeating Patterns in Music\nDatabases. In DASFAA , pages 240â€“252, 2005.\n[13] Y .-L. Lo and S.-J. Chen. The Numeric Indexing For Music\nData. In ICDCSW , pages 258â€“266, 2002.\n[14] C. D. Manning, P. Raghavan, and H. Sch Â¨utze. Introduction to\nInformation Retrieval . Cambridge University Press, 2008.\n[15] G. Navarro, R. Baeza-yates, E. Sutinen, and J. Tarhio. Indexing\nMethods for Approximate String Matching. IEEE Data Engi-\nneering Bulletin , 24:2001, 2000.\n[16] G. Neve and N. Orio. Indexing and Retrieval of Music Docu-\nments through Pattern Analysis and Data Fusion Techniques.\nInISMIR , 2004.\n[17] I. Suyoto and R. Uitdenbogerd. Mirex 2005 symbolic melodic\nsimilarity: Simple efï¬cient n-gram indexing for effective\nmelody retrieval. Music Information Retrieval Evaluation eX-\nchange , 2005.\n[18] E. Ukkonen. Approximate String Matching with q-grams and\nMaximal Matches. Theoretical Computer Science , 92:191â€“\n211, 1992.\n[19] Juli Â´an Urbano, Juan Llor Â´ens, Jorge Morato, and Sonia\nSÂ´anchez-Cuadrado. Mirex 2010 symbolic melodic similarity:\nLocal alignment with geometric representations. Music Infor-\nmation Retrieval Evaluation eXchange , 2010.\n[20] J.-Y . Won, J.-H. Lee, K.-I. Ku, J. Park, and Y .-S. Kim. A\nContent-Based Music Retrieval System Using Representative\nMelody Index from Music Databases. In CMMR , pages 280â€“\n294, 2004.\n368"
    },
    {
        "title": "Finding Community Structure in Music Genres Networks.",
        "author": [
            "DÃ©bora C. CorrÃªa",
            "Alexandre L. M. Levada",
            "Luciano da F. Costa"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1418147",
        "url": "https://doi.org/10.5281/zenodo.1418147",
        "ee": "https://zenodo.org/records/1418147/files/CorreaLC11.pdf",
        "abstract": "Complex networks have shown to be promising mechanisms to represent several aspects of nature, since their topological and structural features help in the understanding of relations, properties and intrinsic characteristics of the data. In this context, we propose to build music networks in order to find community structures of music genres. Our main contributions are twofold: 1) Define a totally unsupervised approach for music genres discrimination; 2) Incorporate topological features in music data analysis. We compared different distance metrics and clustering algorithms. Each song is represented by a vector of conditional probabilities for the note values in its percussion track. Initial results indicate the effectiveness of the proposed methodology.",
        "zenodo_id": 1418147,
        "dblp_key": "conf/ismir/CorreaLC11",
        "keywords": [
            "Complex networks",
            "music genres discrimination",
            "topological features",
            "community structures",
            "music data analysis",
            "distance metrics",
            "clustering algorithms",
            "note values",
            "initial results",
            "proposed methodology"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nFINDING COMMUNITY STRUCTURE IN MUSIC GENRES NETWORKS\nDÂ´ebora C. Corr Ë†ea, Luciano da F. Costa\nInstituto de F Â´Ä±sica de S Ëœao Carlos\nUniversidade de S Ëœao Paulo\ndebcris.cor@gmail.com, luciano@ifsc.usp.brAlexandre L. M. Levada\nDepartamento de Computac Â¸ Ëœao\nUniversidade Federal de S Ëœao Carlos\nalexandre@dc.ufscar.br\nABSTRACT\nComplex networks have shown to be promising mechanisms\nto represent several aspects of nature, since their topological\nand structural features help in the understanding of relations,\nproperties and intrinsic characteristics of the data. In this\ncontext, we propose to build music networks in order to ï¬nd\ncommunity structures of music genres. Our main contribu-\ntions are twofold: 1) Deï¬ne a totally unsupervised approach\nfor music genres discrimination; 2) Incorporate topological\nfeatures in music data analysis. We compared different dis-\ntance metrics and clustering algorithms. Each song is rep-\nresented by a vector of conditional probabilities for the note\nvalues in its percussion track. Initial results indicate the ef-\nfectiveness of the proposed methodology.\n1. INTRODUCTION\nComplex networks have received much attention in recent\nyears due to their capability of characterizing and helping in\nthe understanding of many interdisciplinary aspects of the\nreal-world [3]. Regarding music and artistic aspects, music\nnetworks have been studied and their topological character-\nistics shown to be useful for the analysis of dynamics and\nrelations between the involved elements. Examples are the\nwork of Gleiser and Danon [13] concerning a collaboration\nnetwork of jazz artists and bands; the work of Parket et al [8]\nabout a social network of contemporaneous musicians; and\nthe work of Cano et al [12] involving an analysis of the sim-\nilarities between songs and bands.\nCommunity structures have also been studied in music\nnetworks. Teitelbaum et al [19] analysed two different so-\ncial networks using similarities and collaborative attributes\nof music artists. They described some organization patterns\nand they comment aspects that reï¬‚ect in the growth of such\nnetworks. Lambiotte and Ausloos [17] addressed the difï¬-\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proï¬t or commercial advantage and that copies\nbear this notice and the full citation on the ï¬rst page.\nc/circlecopyrt2011 International Society for Music Information Retrieval.culty for a general agreement of the genre taxonomy through\nan empirical analysis of web-downloaded data.\nAlthough there are several works in the literature that\nprovide signiï¬cant results for more complex case of audio-\nbased analysis [7], in audio ï¬les all information is mixed\ntogether. Differently, the use of symbolic format like MIDI,\nmay indicate a clearer analysis of what is in fact contributing\nfor the discrimination of the genres [15]. On the other hand,\nMarkov models on high-level rhythm features is an area rel-\natively few explored nowadays. Markov chains in rhythm\nfeatures and their capability for discriminating music gen-\nres has been studied by [3]. The authors investigated that\nuse of Markov chains with memory one and two suggests\nan evidence that the pattern of note values in the percussion\nmay differ from one genre to another.\nOur main goal is to analyse the community structure of\nmusic networks, which is a new and promissing research\narea. We believe that mixing temporal features (rhythmic\npatterns) and global topology information from proper mu-\nsic networks can be effective in understanding the relation-\nship of music genres. We summarize our main contribu-\ntions as: comparison of different 1) distance metrics, and\n2) community detection algorithms in order to ï¬nd commu-\nnity structures in the music networks, deï¬ning a completely\nunsupervised and low computational cost approach.\nThe remainder of the paper is organized as follows: sec-\ntion 2 describes the proposed method; and section 3 presents\nthe primarily experiments and provide some discussions. Fi-\nnally, section 4 shows the conclusions and ï¬nal remarks.\n2. METHOD\n2.1 Data Description\nThe database consists of 280 samples (or songs) in MIDI\nformat equally divided into four genres: blues, mpb (Brazil-\nian popular music), reggae and rock. Although it indicates a\nsmall database, these songs contain high variability in their\nrhythmic patterns. Besides, this database allows a qualita-\ntive investigation of the music graphs (by visual inspection\nof their topology). Our motivation for choosing these four\ngenres is the availability of online MIDI samples with con-\n447Poster Session 3\nFigure 1 . Example of a percussion track.\nBeat 4 4 4.5 5 5 5 5.5 5.5 6 6 6.5 7 7 7\nRelative 0.5 1 0.5 0.5 0.5 0.5 0.5 0.5 0.5 1 0.5 0.5 0.5 1\nDuration\nTable 1 . Matrix representation of second measure of the\npercussion in Figure 1. First beat starts at 0.\nsiderable quality and the different tendencies they represent.\nDespite being simpler to analyse than audio ï¬les, MIDI\nformats have the advantage of being a symbolic representa-\ntion, which offers a deeply analysis of the involved elements\nand takes much less space. We used the Sibelius software\nand the free Midi Toolbox for Matlab computing environ-\nment [18]. In this toolbox a MIDI ï¬le is represented as a\nnote matrix that provides information like relative duration\n(in beats), MIDI channel, MIDI pitch, among others. The\nrelative note duration is represented in this matrix through\nrelative numbers (for example, 1 for quarter note, 0.5 for\neighth note, 0.25 for sixteenth note and so on). Sibelius\nsoftware has an option called â€œLive Playbackâ€. If this op-\ntion is not marked, the note values in the MIDI ï¬le respects\ntheir relative proportion (e.g., the eighth note is always 0.5).\nIn this way, we can solve possible ï¬‚uctuations in tempo.\nFor each song the track related to the percussion is ex-\ntracted. We propose that the percussion track of a song is\nintrinsically suitable to represent the rhythm in terms of note\nvalues dynamics. Once we have separated the percussion\ntrack, we can obtain a vector that contains the sequence of\nrelative note values present in it. The instrumentation is not\nbeen considered. If two or more note events occurs at the\nsame beat, the median duration of them is taken. To illus-\ntrate the idea, Figure 1 shows the ï¬rst measures of the per-\ncussion track of the music From Me To You (The Beatles).\nPart of the percussion matrix corresponding to the second\nmeasure is indicated in Table 1. As we can see, different\ninstrument events occur at a same beat. Taking the median\nvalue in such cases, the ï¬nal note duration vector of this\nmeasure will be: [0.5 0.5 0.5 0.5 0.5 0.5 0.5]. For each song\nin the database, we compute the note vector of the whole\npercussion. All these steps can be automatically performed.\n2.2 Markov modeling for note duration dynamics\nMarkov chains use a conditional probability structure to cal-\nculate the probability of future events based on one or more\npast events [5]. We can analyse different numbers of pastevents, which indicates the order of the chain. A ï¬rst order\nMarkov chain takes into consideration only a predecessor of\na event. If instead, the predecessorâ€™s predecessor is consid-\nered, then we have a second order Markov chain, and so on.\nGenerally, an nth-order Markov chain is represented by a\ntransition matrix of n+ 1 dimensions. This is an interesting\nmatrix, since it gives the information about the likelihood of\nan eventâ€™s occurrence, given the previous nstates.\nIn our case, the events are the relative note values of the\npercussion in the songs, obtained with the steps described\nin section 2.1. For each song (represented by a vector of\nnote values), we compute the ï¬rst and second order transi-\ntion matrices. Therefore, we have the probability that each\nnote value or a pair of note values is followed by other note\nduration in the song. Higher-order Markov chains tend to\nincorporate senses of phrasal structure [2], while ï¬rst-order\nones help to identify more often subsequent notes.\nIn order to reduce data dimensionality, we performed a\npreliminary analysis of the relative frequency of note values\nand pairs of note values concerning all the songs, in a way\nthat extremelly rare transitions were discarded. For the ï¬rst\norder Markov chain we have a matrix of probabilities with\n18 rows and 18 columns (we considered 18 different note\nvalues in this dataset). Each entry (i, j)of this matrix ex-\npresses the probability that a note value iis followed by a\nnote value jin the percussion of the respective song. Then\nthis matrix is treated as a 1 x 364 feature vector.\nFor the second order Markov chain, the matrix of proba-\nbilities for each song is 167 (rows) x 18 columns, treated as\na 1 x 3006 (167 * 18) feature vector (we considered 167 dif-\nferent pair of note values). Similar, each entry (i, j)of this\nmatrix expresses the probability that a speciï¬c pair of note\nvalues represented in line ifollows a speciï¬c note value j.\nIf we concatenate both feature vectors we will have the ï¬nal\nfeature vector of each song with 3330 elements. It is in-\nteresting to mention that, we experimented to built the mu-\nsic networks considering ï¬rst and second order probabilities\nseparately. However, for both isolated cases, the Clauset-\nNewman-Moore community detection algorithm clustered\n5 different groups, while considering feature vectors com-\nposed by the concatenation of ï¬rst and second order models\nled to the detection of 4 groups. This fact suggests that a\nsingle Markov chain is not sufï¬ciently to model all the dy-\nnamics that characterizes the 4 original genres. Another ev-\nidence is that when we consider both Markov chains, the\naccuracy obtained in the classiï¬cation of these four gen-\nres is higher: 70% for ï¬rst-other Markov chain, 85% for\nthe second-orther, and 92% for both chains. (We used the\nBayessian classiï¬er under Gaussian hyphothesis.)\n2.3 Music Networks\nA complex network is a graph that exhibits a relatively so-\nphisticated structure between its elements when compared\n44812th International Society for Music Information Retrieval Conference (ISMIR 2011)\nto regular and uniformly random structures. Basically speak-\ning, a network may be composed by vertices, edges (or links)\nand a mapping that associates a weight in the connection of\ntwo vertices. The edges usually has the form w(i, j)indicat-\ning a link from vertex ito vertex jwith weight w(i, j). Rep-\nresenting music genres as complex networks may be inter-\nesting to study relations between the genres characteristics,\nthrough a systematic analysis of topological and structural\nfeatures of the network.\nFrom the ï¬rst and second-order transition matrixes of the\nMarkov chains we can build a music network. Each vertex\nrepresents a song. The links between them represent the dis-\ntance of the two respective songs, considering their vectors\nof conditional probabilities of the note values. However,\nwith a full-connected network it may be difï¬cult to obtain\nintricate structure. There are several forms to deï¬ne which\nvertices will be connected and several distance metrics. We\npropose some possibilities in the following and try to form\nclusters of vertices that can represent the music genres.\n3. EXPERIMENTS AND DISCUSSION\nIt is worthwhile to mention that the proposed characteriza-\ntion of the music genres is performed in an unsupervised\nway (community ï¬nding algorithms). The obtained groups\nare based on similarities in the feature set and the classes\nare not supposed to be known in advance. To illustrate the\ncomplexity of the problem, Figure 2 presents the ï¬rst and\nsecond components (new features) obtained by LDA (Lin-\near Discriminant Analysis), which is a supervised technique\nfor feature analysis whose principal aim is to maximize class\nseparability. Even with the LDA new features, reggae and\nrock classes are still overlapped. This overlapping could\nbe observed in all performed experiments. Considering the\nrhythms patterns, rock and reggae music are pretty similar.\nWe know that the use of only four genres with seventy\nsamples each may represent a small dataset. Our purpose is\nto perform an initial study of rhythmic features and its rep-\nresentation, but with an evidence that the proposed features\nmay be useful and viable for genre characterization.\n3.1 Community detection on K-NN graphs\nThrough the dynamics of the note values in the percussion\nwe built several networks. From the point of view of parti-\ntioning the genres into communities, different groups may\nbe obtained, depending on the used criteria. In this sec-\ntion, we used the Clauset, Newman and Moore [1] and Gir-\nvan and Newman [10] algorithms for community detection.\nSuch algorithms are widely known in the complex networks\nliterature. The former is based on a hierarchical clustering\nof the dataset. The latter is based on centrality metrics to\ndetermine the community boundaries.\nFigure 2 . The ï¬rst and second features obtained by LDA.\nMpb Rock Blues Reggae\nG1 41 29 15 21\nG2 4 13 1 11\nG3 17 15 50 20\nG4 8 13 4 18\nTable 2 . The groups in network of Figure 3.\nFor each of the following cases, the networks may be\nbuilt as follows: 1) From the feature matrix (with 280lines\n(the songs) and 3330 columns (the features)), we computed\nthe distance between each pair of feature vector (or each\npair of song). This led to a 280 x280symmetric matrix\nof distances, with zero values in the diagonal. In this case,\nwe have a full network, with all vertices connected to each\nother; 2) For each song (or vertex), we only link the Knear-\nest songs of it. The weight of each link is the distance be-\ntween this par of songs; 3) Consider the obtained K-regular\nnetwork. Or; 4) For each vertex, take the mean distance,\nconsidering the linked vertices. Keep the link between ver-\ntices only with their distance is smaller than the mean dis-\ntance. The main variations of the networks analysed here\nare consequence of the choice of different distance metrics,\ndifferent values of K, and the execution or not of step 4.\nFor the network showed in Figure 3 we used the cosine\ndistance, K= 10 and kept the network 10regular. The\nsongs are spread as indicated in Table 2. Each group has a\ndifferent dominant class. Blues and mpb songs are concen-\ntrated in G3 and G1, respectively. Reggae songs are almost\nequally divided into the groups. Rock songs are almost 50%\nin G1, overlapping with mpb songs. The other 50% divided\ninto the remaining groups. This behavior substantially re-\nï¬‚ects the projections of LDA in Figure 2. The G3 group\nreï¬‚ects the blues songs that are more discriminative. The\nG1 group reï¬‚ects mainly the overlapping present in mpb,\nreggae and rock. And G2 and G4 mainly reï¬‚ect the overlap-\nping between reggae and rock songs.\nFor the same network, Figure 4 shows the groups ob-\n449Poster Session 3\nFigure 3 . The network of genres. Cosine distance.\nGroups formed by the Clauset-Newman-Moore algorithm.\nAll colored images available at http://cyvision.ifsc.usp.br/ deboracor-\nrea/MusicandComplexNetworks.html\ntained by the Girvan and Newman algorithm. Since it is\nan algorithm based on vertex centrality indices, the network\nwas split into nine groups. The result is still interesting\nsince many songs of a same genre are placed together in\neach group. In addition, this result opens a promising fur-\nther studies aimed at analysing the presence of sub-genres\nin these small groups. Are, for example, blues-rock or pop-\nrock songs more concentrated in a speciï¬c group? This is an\ninteresting study that can beneï¬t of this investigative work.\nFigure 4 . The network of genres. Cosine distance. Groups\nformed by the Girvan and Newman algorithm.\nIf instead of cosine distance, we use the Euclidian dis-\ntance, we will get the network in Figure 5, according to the\nClauset, Newman and Moore algorithm. Table 3 shows the\ngroups. Reggae songs are more concentrated (31 in G3);\nand G4 is smaller than in the ï¬rst case, with only 12 songs.\nConsidering all the experiments, including those not pre-\nFigure 5 . The network genres. Euclidian distance. Groups\nformed by the Clauset-Newman-Moore algorithm.\nMpb Blues Reggae Rock\nG1 35 18 15 30\nG2 19 41 23 10\nG3 13 10 31 23\nG4 3 1 1 7\nTable 3 . The groups in network of Figure 5.\nsented here, we can describe some overall characteristics\nof the clusters found by the Clauset-Newman-Moore algo-\nrithm. The most discriminative genre is blues. In most ex-\nperiments one group was always small. Actually, in some\nvariations the algorithm returned three large groups. This\nmay indicate that, although we have four genres labeled by\nthe usual taxonomy, in terms of the proposed rhythm fea-\ntures there are only three. If we listen to the whole song, we\nmay differ the genres in a successful way. But if we listen to\nonly the percussion track of each song, this discrimination\nmay be harder and one song could be labeled into more than\none genre. Therefore, considering that we have a completely\nunsupervised approach, the proposed investigation indicates\nthat note duration dynamics can be a useful information in\ncharacterizing and discriminating music genres.\n3.2 Spectral graph partitioning\nTopologic-based graph metrics are generally correlated and\ndependent [16]. For this reason, spectral analysis is a pow-\nerful tool that has been widely explored in the characteriza-\ntion of graphs and complex networks. The basic idea can\nbe summarized as follows: in mathematical terms, when we\nanalyze a graph in the spectral domain we have a represen-\ntation in terms of orthogonal components, which means that\ninformation is somehow uncorrelated. Thus, proper analy-\nsis of eigenvalues and eigenvectors of adjacency or laplacian\nmatrices idenï¬cates aspects that cannot be seen in the topol-\n45012th International Society for Music Information Retrieval Conference (ISMIR 2011)\nReggae Blues Rock Mpb\nG1 20 4 13 8\nG2 21 46 6 15\nG3 17 9 23 20\nG4 12 11 18 27\nTable 4 . The groups in network of Figure 6.\nogy domain. Please, refer to [4,11] for a good review on the\nmathematical fundamentals of algebraic graph theory.\nIn this paper, we use a spectral graph partitioning method\nbased on the analysis of the eigenvalues of the Laplacian\nmatrix. Let AandBbe the adjacency and incidence matri-\nces of a graph G={V, E}, where Vis a set of vertices and\nEis a set of edges. The Laplacian matrix, Q, is given by:\nQ=BBT= âˆ†âˆ’A (1)\nwhere âˆ†is a diagonal matrix of the degrees of V.\nThe second smallest eigenvalue of the Laplacian matrix\nis known as the algebraic connectivity of a graph and it has\nmany interesting properties. More precisely, the eigenvector\nassociated to this eigenvalue, known as the Fiedler vector\n[9], has proven to be directly related to graph connectivity.\nOften, in practice, the signs of the Fiedler vector can be used\nto partition a graph in two regions. This can be seen as a\nquantization to binary digits, zero or one.\nHere, we propose to do a quantization of the Fiedler vec-\ntor coefï¬cients in Cvalues, where Crepresents the num-\nber of desirable clusters or groups. By doing so, we are\nessentially partitioning a graph or network in Csubgraphs\nor communities, which is equivalent to ï¬nding Câˆ’1val-\nleys in the histogram that represents the distribution of its\ncoefï¬cient values. In this paper, the thresholds were chosen\nby visual inspection of the histogram, but several methods\nfor automatic multilevel threshold estimation are available\nin the image processing literature [14]. A deeper mathemat-\nical analysis and discussion about the eigenvectors of the\nLaplacian matrix and its properties can be found in [16].\nFor the following experiment, we used the non-regular\nnetwork generated by ï¬rst building a K-NN graph with K=\n30and then, for each vertex v, cutting the edges whose\nweights were above a threshold obtained by averaging the\nweights of every edge incident on v. Thus, the resulting net-\nwork is not modeled as a k-regular graph anymore. Figure 6\nshows the resulting network, with the four detected clusters.\nThe Fiedler vector for this graph and the corresponding his-\ntogram for the distribution of its coefï¬cients are plotted in\nFigures 7 and 8, respectively. The distribution of coefï¬cient\nvalues of the second smallest eigenvector of the Laplacian\nmatrix clearly indicates the presence of different clusters or\ncommunities in the network. Table 4 shows the groups for\nthe spectral partition. Rock and mpb songs are more spread\nin the four groups than in the former cases.\nFigure 6 . The network of genres by the Fiedler vector.\nFigure 7 . The Fiedler vector for the network in Figure 6.\n4. FINAL REMARKS AND ONGOING WORK\nIn this investigative study we proposed a characterization of\nmusic genres by detecting communities in complex music\nnetworks. Each vertex represents a song through a feature\nvector that captures the likelihood of ï¬rst and second order\nMarkov chains of the note values in the percussion track.\nThe distance between the feature vectors (or between the\nsongs) deï¬nes the weight of the links. We tested two differ-\nent distance metrics (cosine and Euclidian) and two different\napproaches for ï¬nding clusters in the network (traditional\nalgorithms on K-NN graphs and spectral partioning).\nRegarding the formed clusters, we found that the results\nare promising since in most experiments each cluster is dom-\ninated by a different genre. Observing the LDA projections,\nit is possible to see that many samples from different gen-\nres are overlapped (mainly reggae and rock samples). LDA\nis a supervised technique that maximizes class separability.\nTherefore, even without any supervised analysis, signiï¬cant\nresults could be obtained. In addition, most MIDI databases\navailable in the Internet are single-labeled, sometimes with\ndifferent taxonomies of music genres. In some situations,\n451Poster Session 3\nFigure 8 . Distribution of coefï¬cient values of the Fiedler\nvector for the network depicted in Figure 6.\na sample receives different labels in different sites (for ex-\nample, wikipedia). This introduces noise to the system and\nreï¬‚ects in the evaluation of the results.\nFrom the obtained communities and considering the four\ngenres used in this study, we can say that blues is the more\ndiscriminative genre. Representing the older genre, and hav-\ning speciï¬c characteristics, blues may have inï¬‚uenced the\nfollowing genres, which contributted along years for a mix-\nture of some features between genres. Reggae, rock and\nmpb are more similar genres, sharing many overlapped sam-\nples. In fact, along years mpb music started to include dif-\nferent rhythms like rock and latine music such as reggae and\nsamba. Reggae music, on the other hand, had stylistc origns\nin jazz, R&B, rocksteady and others. These tendencies are\ninteresting and are somehow reï¬‚ected in the results. Actu-\nally, the use of graph representation (instead of clustering\nmethods in a vector space) is promising, since it combines\ngraph topological features and similarity characteristcs in\norder to infer the data structures.\nMusic networks is somehow a new reseach area in the\nliterature. To the best of our knowledge, we could not ï¬nd\na different approach that used partitional network methods\nfor music genres. Comparing with the hierarquical cluster-\ning with Euclidian distance metric used in [3], the groups\nin Table 3 have some differences: the blues songs are sig-\nniï¬cantly more concentrated in one group; the largest group\ndoes not concentrate too many samples of all genres, which\nis not the case in the hierarquical clustering. An advantage\nof this kind of unsupervised analysis relies on the possibil-\nity of the characterization of music sub-genres, which can\ncontribute to the deï¬nition of a more uniï¬ed taxonomy.\nThere are many possibilities for future works. First, many\nother rhythm attributes can be analysed (like the intensity\nof the beat), as well as other open music databases [15].\nAnother interesting work that has been started is the inves-\ntigation of sub-genres present in sub-clusters of the main\ngroups. It would be promising if a system could be sen-\nsitive to various styles inside a genre. Contextual analysis\nthrough Markov Random Field models may also bring ben-eï¬ts, since with this kind of modeling we can measure how\nindividual elements are inï¬‚uenced by their neighbors, ana-\nlyzing spatial conï¬guration patterns of vertices.\n5. ACKNOWLEDGMENTS\nDebora Correa thanks Fapesp ï¬nancial support (2009/50142-\n0) and Luciano da F. Costa thanks CNPq (301303/06-1 and\n573583/2008-0) and Fapesp (05/00587-5) ï¬nancial support.\n6. REFERENCES\n[1] A. Clauset, M. E. J. Newman, C. Moore: â€œFinding Community Struc-\nture in Very Large Networks,â€ Phys. Rev. E V ol. 70, No. 066111, 2004.\n[2] C. Roads : The Computer Music Tutorial , MIT Press, 1996.\n[3] D. C. Correa, J. H. Saito, L. da F. Costa: â€œMusical Genres: Beating to\nthe Rhythms of Different Drumsâ€, New Journal of Physics V ol. 12, N.\n053030, 2010.\n[4] D. M. Cvetkovic, M. Doob and H. Sachs: Spectra of Graphs, Theory\nand Applications , Johann Ambrosius Barth (Heidelberg), 3 ed., 1995.\n[5] E. Miranda: Composing with computers , Focal Press, Oxford, 2001.\n[6] J. Clarck and D. A. Holton: A First Look at Graph Theory , World\nScientiï¬c, 1991.\n[7] J-J. Aucouturier and F. Pachet â€œRepresenting Musical Genre: A State\nof the Artâ€, J. of New Music Research V ol. 32, No. 1, pp.8393, 2003.\n[8] J. Park, O. Celma, M. Koppenberger, P. Cano, and J. M. Buld: â€œThe so-\ncial network of contemporary popular musiciansâ€, International Jour-\nnal of Bifurcation and Chaos V ol.17. N. 7, pp. 2281-2288, 2007.\n[9] M. Fiedler: â€œAlgebraic Connectivity of graphsâ€, Czechoslovak Math-\nematical Journal , V ol. 23, No. 98, pp. 298-305, 1973.\n[10] M. Girvan and M. E. Newman: â€œCommunity structure in social and\nbiological networksâ€, Statistical Mechanics - Proc. Natl. Acad. Sci.\nUSA V ol.99, pp. 7821-7826, 2002.\n[11] N. Biggs: Algebraic Graph Theory , Cambridge Univ. Press, 1994.\n[12] P. Cano, O. Celma, M. Koppenberger, e J. M. Buld: â€œTopology of\nmusic recommendation networks.â€, Chaos V ol.16, N.013107, 2006.\n[13] P. M. Gleiser and L. Danon: â€œCommunity structure in jazzâ€, Advances\nin Complex Systems V ol 6 N. 4, 2003.\n[14] P. S. Liao, T. S. Chen and P. C. Chung: â€œA Fast Algorithm for Multi-\nlevel Thresholdingâ€, Journal of Information Science and Engineering ,\nV ol. 17, pp. 713-727, 2001.\n[15] C. McKay, I. Fujinaga â€œAutomatic Genre Classiï¬cation Using Large\nHigh-Level Musical Feature Setsâ€, Proc. of the International Confer-\nence on Music Information Retrieval , pp. 525-530, 2004.\n[16] P. V . Mieghem: Graph Spectra for Complex Networks , Cambridge\nUniv. Press, 2011.\n[17] R. Lambiotte and M. Ausloos: â€œOn the genre-ï¬cation of music: a\npercolation approachâ€, The European Physical Journal B - Condesend\nMatter and Complex Systems V ol. 50, N. 1-2, pp.183-188, 2006.\n[18] T. Eerola and P. Toiviainen: MIDI Toolbox: MATLAB Tools for Music\nResearch , University of Jyv Â¨askyl Â¨a, 2004\n[19] T. Teitelbaum,P. Balenzuela, P. Cano, and J. M. Buld: â€œCommunity\nstructures and role detection in music networksâ€, Chaos V ol. 18, N.\n043105, 2008.\n452"
    },
    {
        "title": "Combining Content-Based Auto-Taggers with Decision-Fusion.",
        "author": [
            "Emanuele Coviello",
            "Riccardo Miotto",
            "Gert R. G. Lanckriet"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1415666",
        "url": "https://doi.org/10.5281/zenodo.1415666",
        "ee": "https://zenodo.org/records/1415666/files/CovielloML11.pdf",
        "abstract": "To automatically annotate songs with descriptive keywords, a variety of content-based auto-tagging strategies have been proposed in recent years. Different approaches may capture different aspects of a songâ€™s musical content, such as timbre, temporal dynamics, rhythmic qualities, etc. As a result, some auto-taggers may be better suited to model the acoustic characteristics commonly associated with one set of tags, while being less predictive for other tags. This paper proposes decision-fusion, a principled approach to combining the predictions of a diverse collection of content-based autotaggers that focus on various aspects of the musical signal. By modeling the correlations between tag predictions of different auto-taggers, decision-fusion leverages the benefits of each of the original auto-taggers, and achieves superior annotation and retrieval performance.",
        "zenodo_id": 1415666,
        "dblp_key": "conf/ismir/CovielloML11",
        "keywords": [
            "content-based auto-tagging",
            "musical content",
            "timbre",
            "temporal dynamics",
            "rhythmic qualities",
            "acoustic characteristics",
            "decision-fusion",
            "principled approach",
            "correlations",
            "tag predictions"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nCOMBINING CONTENT-BASED AUTO-TAGGERS WITH\nDECISION-FUSION\nEmanuele Coviello\nUniversity of California, San Diego\necoviell@ucsd.eduRiccardo Miotto\nUniversity of Padova\nmiottori@dei.unipd.itGert R. G. Lanckriet\nUniversity of California, San Diego\ngert@ece.ucsd.edu\nABSTRACT\nTo automatically annotate songs with descriptive keywords,\na variety of content-based auto-tagging strategies have been\nproposed in recent years. Different approaches may capture\ndifferent aspects of a songâ€™s musical content, such as tim-\nbre, temporal dynamics, rhythmic qualities, etc. As a result,\nsome auto-taggers may be better suited to model the acous-\ntic characteristics commonly associated with one set of tags,\nwhile being less predictive for other tags. This paper pro-\nposes decision-fusion , a principled approach to combining\nthe predictions of a diverse collection of content-based auto-\ntaggers that focus on various aspects of the musical signal.\nBy modeling the correlations between tag predictions of dif-\nferent auto-taggers, decision-fusion leverages the beneï¬ts of\neach of the original auto-taggers, and achieves superior an-\nnotation and retrieval performance.\n1. INTRODUCTION\nThe recent age of music proliferation has raised the need\nfor automatic algorithms to efï¬ciently search and discover\nmusic. Many successful recommendation systems rely on\ntextual metadata provided by expert musicologists or social\nservices in the form of semantic tags â€“ keywords or short\nphrases that capture relevant characteristics of music pieces,\nranging from genre and instrumentation, to mood and usage.\nBy bridging the gap between music and human semantics,\ntags allow semantic retrieval based on transparent textual\ndescriptions, or query-by-example recommendation based\non semantic similarity (as opposed to acoustic similarity) to\na query song.\nMeta-data-based methods work well in practice, provided\nthat enough annotations are available. However, the cold\nstart problem and the prohibitive cost of manual labour limit\ntheir applicability to large-scale applications. Therefore, the\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proï¬t or commercial advantage and that copies\nbear this notice and the full citation on the ï¬rst page.\nc/circlecopyrt2011 International Society for Music Information Retrieval.deployment of modern music recommendation systems can\nbeneï¬t from the development of auto-taggers, i.e., machine-\nlearning algorithms that automatically analyze and index mu-\nsic with semantic tags, which can then be used to improve\nthe search experience and speed up the discovery of desired\ncontent.\n1.1 Previous work\nMost auto-taggers are based on music content analysis and\nare trained from a database of annotated songs (e.g., see [8,\n10, 12, 20]). After extracting a set of acoustic features from\neach training song, a series of statistical models are esti-\nmated, each of which capturing the characteristic acoustic\npatterns in the songs that are associated with one of the tags\nfrom a given vocabulary. When analyzing a new song, the\nauto-tagger processes the time series of acoustic features of\nthe song and outputs a vector of tag-afï¬nities. The afï¬nity-\nvector can then be transformed into a semantic multinomial\n(SMN), i.e., a probability distribution characterizing the rel-\nevance of each tag to a song. A song is then annotated\nby selecting the top-ranking tags in its SMN, or the SMN\nitself can be used as a high-level descriptor, e.g., for re-\ntrieving songs based on semantic similarity. A number of\ndiscriminative (e.g., see [3, 8, 9, 12, 18, 23]) and generative\n(e.g., see [10, 17, 20, 21]) machine learning algorithms have\nbeen proposed to model predictive acoustic patterns in au-\ndio content based on a bag-of-features (BoF) representation,\nwhich treats audio features independently and ignores their\ntemporal order. Recently, Coviello et al. [6] proposed to\nleverage dynamic texture mixture (DTM) models for auto-\ntagging purposes. More precisely, DTM-based auto-taggers\nmodel audio fragments (i.e., time series of audio features\nextracted from a few seconds of musical signal) as the out-\nput of linear dynamical systems. This approach explicitly\ncaptures temporal structures in the musical signal, whereas\na BoF representation discards such dynamics.\nAt a higher level of abstraction, contextual approaches\nhave focused on modeling the semantic context that drives\nthe correlation between different tags (e.g., a song tagged\nwith â€œdrumsâ€ is more likely to also be tagged with â€œelectric\nguitarâ€ than â€œviolinâ€). While content-based models oper-\n705Poster Session 6\nate on low-level acoustic features to predict semantic multi-\nnomials, contextual models are designed to capture mean-\ningful tag correlations in these SMNs, to reinforce accu-\nrate tag predictions while suppressing spurious ones. So,\na contextual model naturally complements a content-based\nmodel, which usually treats tags independently. Combin-\ning them has been shown to improve performance. State-\nof-the-art solutions are based on discriminative approaches\n(e.g., support vector machines [14], boosting [1], ordinal re-\ngression [24]) as well as generative models (e.g., Dirichlet\nmixture models (DMM) [13]).\n1.2 Original contribution\nThe main contribution of this paper is to propose decision-\nfusion , which uses semantic context modeling to simultane-\nously leverage the beneï¬ts of different content-based auto-\ntaggers. Using two or more content-based auto-taggers that\nemphasize diverse aspects of the musical signal (e.g., only\ntimbre vs. temporal dynamics), we collect alternative opin-\nions on each song-tag association. We expect that, besides\nmodeling the context between tags predicted from the same\nauto-tagger, context modeling can capture the correlations\nthat arise between tag predictions based on different auto-\ntaggers, leading to a more sophisticated system.\nThis offers a solution to the problem of selecting or com-\nbining alternative annotation models that previous work has\npointed out. Coviello et al. [6], for example, noted that\neven though their DTM-based auto-tagger generally outper-\nformed a BoF approach based on Gaussian mixture models\n(GMM), the improvements were most signiï¬cant on tags\nwith clear temporal characteristics; for some tags, in fact,\nthe GMM-based model was still favorable (i.e., tags where\nâ€œtimbre says it allâ€).\nExperimental results show that decision-fusion leads to\nimproved annotation and retrieval performance compared\nto i) each individual auto-tagger, ii) each individual auto-\ntagger in tandem with a contextual model (the â€œtraditionalâ€\ncontext-based approach) and iii) various other approaches\nto combining multiple content-based auto-taggers, such as\nï¬xed-combination rules and the regression-based combina-\ntion algorithms proposed by Tomasik et al. [19]. We note\nthat the focus of the latter was slightly different from our\nwork, since it investigates the combination of tags predicted\nfrom different information sources (i.e., content-based auto-\ntags, social tags, collaborative-ï¬ltering-based tags), rather\nthan from different content-based auto-taggers only. In ad-\ndition, as semantic context modeling is naturally comple-\nmentary to any content-based auto-tagger, we corroborate\nthe intuition that there is a beneï¬t in combining DTM-based\ntemporal modeling and semantic context modeling, which\nhas not been shown before.\nThe remainder of this paper is organized as follows. A\nbrief review of the automatic music tagging problem and themodels used in this work are presented in Section 2. Sec-\ntion 3 discusses decision-fusion. Lastly, the experimental\nsetup and results are reported in Sections 4 and 5, respec-\ntively.\n2. AUTOMATIC MUSIC TAGGING\nThe automatic task of music tagging is widely tackled as\na supervised multi-class labeling problem [2], where each\nclass corresponds to a tag wiof a semantic vocabulary V\n(e.g., â€œrockâ€, â€œdrumâ€, â€œtenderâ€, â€œmellowâ€). The music con-\ntent of a song is represented as a time series of low-level\nacoustic featuresY={y1,...,yT}, where each feature\nis extracted from a short snippet of the audio signal and\nTdepends on the length of the song. The semantic con-\ntent with respect to Vis represented as an annotation vector\nc= (c1,...,c|V|), whereci>0only if there is a posi-\ntive association between a song and the tag wi. The goal\nof an auto-tagging system is to infer the relevant semantic\nannotations of unseen songs.\nAt this aim, a set of statistical models is trained to capture\nthe patterns in the audio feature space associated with each\ntag inV, from a databaseD={(Yd,cd)}|D|\nd=1of annotated\nsongs. Based on the learned tag models, the auto-tagger can\nprocess the acoustic features extracted from a novel song\nYand produce a vector of tag-afï¬nities, which is mapped\ninto a semantic multinomial Ï€= (Ï€1,...,Ï€|V|)lying on a\nsemantic space (i.e.,/summationtext\niÏ€i= 1 withÏ€iâ‰¥0), whereÏ€i=\nP(wi|Y)represents the probability that the ithtag applies\nto songY.\nIn order to leverage high level relationships that arise in\nthe tag predictions of content-based auto-taggers, contex-\ntual approaches additionally introduce a second modeling\nlayer to capture meaningful tag correlations in the SMNs.\nIn particular, a content-based auto-tagger is used to produce\na SMNÏ€dfor each songYdinD, while a second layer of\nstatistical models is trained onto {(Ï€d,cd)}|D|\nd=1, to capture\nwhich patterns in the SMNs are predictive for each tag. For\na novel songY, the contextual tag models can therefore be\nused to reï¬ne the semantic multinomial Ï€produced by the\ncontent-based auto-tagger.\nMusic annotation involves ï¬nding the tags that best de-\nscribe a song; this is achieved by selecting the subset of tags\nthat peak in its semantic multinomial. Retrieval given a one-\ntag query, requires ranking all songs in a database based on\ntheir relevance to the query, e.g., the corresponding entry in\nthe semantic multinomials [20].\nIn the following we review a variety of content-based\nauto-tagging strategies, where low-level acoustic content is\nrepresented either as a bag-of-features (Sections 2.1.1 and\n2.1.2) or as a time series of features (Section 2.1.3). Ad-\nditionally, Section 2.2 introduces a contextual approach for\nmodeling tag correlations as well.\n70612th International Society for Music Information Retrieval Conference (ISMIR 2011)\n2.1 Content modeling\nContent-based auto-taggers have been designed to model\nthe acoustic content associated with tags and represented as\na bag-of-features using both generative and discriminative\nmodels, as in Sections 2.1.1 and 2.1.2, respectively; con-\nversely, the use of time series of audio features for music\ntagging has been considered in the generative approach of\nSection 2.1.3 only.\n2.1.1 The Gaussian mixture model (GMM)\nTurnbull et al. [20], proposed to capture the most prominent\nacoustic textures associated to each tag wiinVwith a prob-\nability distribution p(y|wi)over the space of audio features\ny, which is a Gaussian mixture model (GMM):\np(y|wi) =R/summationdisplay\nr=1awi\nrN(y|Âµwi\nr,Î£wi\nr), (1)\nwhereRis the number of mixture components, N(Â·|Âµ,Î£)a\nmultivariate Gaussian distribution with mean Âµand covari-\nance matrix Î£, andawirthe mixing weights. The parame-\nters{awir,Âµwir,Î£wi\nr}R\nr=1of each tag model p(y|wi)arees-\ntimated from the bag-of-features extracted from the songs\ninDthat are positively associated with wi, using the hierar-\nchical expectation-maximization (EM) algorithm [22].\nGiven the audio content of a new song Y={y1,...,yT},\nthe relevance of each tag wiis computed using the Bayes\nrule:\nÏ€i=P(wi|Y) =p(Y|wi)P(wi)\np(Y), (2)\nwhereP(wi)is the tag prior (assumed to be uniform) and\np(Y)the song prior, i.e., p(Y) =/summationtext|V|\nj=1p(Y|wj)P(wj).\nThe likelihood term in (2) is computed as the geometric av-\nerage of the individual sequence likelihoods, i.e., p(Y|wi) =/producttextT\nt=1p(yt|wi)1\nT.\n2.1.2 Boosting (BST)\nThe boosting approach proposed by Eck et al. [8] is a super-\nvised discriminative algorithm that learns a binary classiï¬er\nfor each tag wiin the vocabulary V, from both the posi-\ntive and the negative training examples for that tag. More\nspeciï¬cally, it constructs a strong classiï¬er which combines\na set of simpler classiï¬ers, called weak learners , in an itera-\ntive way. As weak learners, according to [1], we use single\nstumps (i.e., binary thresholding on one low-level acoustic\nfeature).\nA novel songYis classiï¬ed by each of the binary clas-\nsiï¬ers and Platt scaling is applied to produce a probability\nestimateÏ€i=P(wi|Y)for each tag wi. We will refer to\nthis approach as BST.2.1.3 Temporal modeling (DTM)\nCoviello et al. [6] proposed a novel auto-tagger built upon\nthe DTM model, which explicitly captures both the timbral\nand the temporal structures of music that are most predic-\ntive for each tag. Speciï¬cally, the dynamic texture (DT)\nmodel [7] treats an audio fragment y1:Ï„as output of a linear\ndynamical system. The model consists of a double embed-\nded stochastic process, in which a lower dimensional Gauss-\nMarkov process xtencodes the dynamics (evolution) of the\nacoustic component ytover time\nEach tag distribution is modeled with a dynamic texture\nmixture (DTM) [4] probability density over sequences of\naudio feature vectors:\np(y1:Ï„|wi) =R/summationdisplay\nr=1a(wi)\nrp(y1:Ï„|Î˜(wi)\nr), (3)\nwhereRis the number of mixtures and Î˜(wi)\nr is therth\nDT component. The parameters {a(wi)\nr,Î˜(wi)\nr}R\nr=1are es-\ntimated based on the audio fragments extracted from the\nsongs inDpositively associated with the tag wi, using an ef-\nï¬cient hierarchical EM algorithm for DTM (HEM-DTM) [5].\nGiven the audio fragments extracted from a new song\nY={y1\n1:Ï„, ...,yF\n1:Ï„}, whereFdepends on the length of\nthe song, the relevance of tag wiis computed using Bayesâ€™\nrule (2), with the likelihood computed as the geometric aver-\nage of the individual sequence likelihoods smoothed by the\nsequence length Ï„, i.e.,p(Y|wi) =/producttextF\nt=1p(yt\n1:Ï„|wi)1\nFÏ„.\n2.2 Context modeling (DMM)\nAs mentioned in Section 1.1, different approaches have been\nproposed to model contextual relationships in SMNs; in this\nwork, we use the DMM [13]. The DMM is a generative\nmodel that assumes the SMNs Ï€of the songs positively as-\nsociated to a tag wiare distributed accordingly to a mixture\nof Dirichlet distributions over the semantic space deï¬ned by\nV:\np(Ï€|wi; â„¦w) =R/summationdisplay\nr=1Î²wiDir(Ï€|Î±wi\nr), (4)\nwhere R is the number of mixtures, Î²wi\nkare the mixing\nweights, and Dir (Â·|Î±)is a Dirichelet distribution of param-\netersÎ±= (Î±1,...,Î±|V|). The parameters of the DMM for\neach tagwiinVare estimated from the semantic multino-\nmials extracted from the songs in Dpositively associated\nwith the tag, via the generalized EM algorithm [16].\nHence, given a new song described by the SMN\nÏ€= (Ï€1,...,Ï€|V|), the relevance of a tag wiis computed\nusing Bayesâ€™ rule to get the tag posterior probabilities in the\ncontext space:\nÎ¸i=P(wi|Ï€) =p(Ï€|wi)P(wi)\np(Ï€). (5)\n707Poster Session 6\nAll the tag posterior probabilities form the contextual multi-\nnomial distribution of the song, i.e., Î¸= (Î¸1, ...,Î¸|V|),\nwhich can then be used for semantic annotation and retrieval.\n3. DECISION-FUSION\nEach content-based auto-tagger generally emphasizes par-\nticular aspects of the musical signal. Despite some auto-\ntaggers could be preferred over others based on average per-\nformances (Table 1, part (a)), the spread in performances\nregistered on speciï¬c tags (e.g., see Figure 1) makes unclear\nif any auto-tagger may be the best. This leaves open the\nproblem of choosing the most appropriate method for each\ntag, or, indeed, the one of combining different auto-taggers.\nIn this paper we argue that semantic context modeling\ncan also be used as a strategy to combine different content-\nbased auto-taggers, which we name decision-fusion . Indeed,\nby modeling the patterns that arise from the tag predictions\ngenerated by different content-based auto-taggers, decision-\nfusion combines all the different opinions into a single pre-\ndiction and leverages the beneï¬ts of each of the acoustic\ncharacteristics emphasized by the original auto-taggers.\nFormally, let us assume a group Aof different content-\nbased auto-tagging algorithms is available. For each song\ndin the databaseD, semantic multinomials Ï€a\ndfora=\n1,...,|A|are computed (i.e., one for each auto-tagger in\nA) and pooled together into the aggregated semantic multi-\nnomial:\nÏ€A\nd= (Ï€1\nd,...,Ï€|A|\nd), (6)\nwhich is intended to be normalized to sum to 1. In practice,\nit is as we are now working with a new semantic vocabulary\nVA=V1Ã—Â·Â·Â·Ã—V|A|of size|A|Â·|V| , where each tag\nis replicated|A|times, one for each auto-tagger. Decision-\nfusion consists in training a set of semantic context models,\ni.e.,p(Ï€A|wi)forwi= 1,...,|V|, over the aggregated se-\nmantic multinomials {(Ï€A\nd,cd)}|D|\nd=1to capture both intra-\nand inter-auto-taggers tag correlations. Note that traditional\ncontext modeling acts on the SMNs of a single auto-tagger,\nthus capturing only intra-auto-tagger correlations.\nDecision-fusion can be implemented through a variety of\ncontext-modeling algorithms. In particular, in this work we\ntested the DMM presented in Section 2.2. Therefore, the ag-\ngregated SMNs Ï€Aof songs positively associated with tag\nwiare assumed to be distributed accordingly to a mixture of\nDirichlet distributions over the semantic space VA:\np(Ï€A|wi) =R/summationdisplay\nr=1Î²wiDir(Ï€A|Î±wi\nr), (7)\nwhereÎ±= (Î±1,...,Î±|A|Â·|V| ).\nAn unseen songYis ï¬rst processed by each of the content-\nbased auto-taggers available to produce the semantic multi-\nnomialsÏ€afora= 1,...,|A|, which are then aggregated inÏ€A. Finally, Bayesâ€™ rule as in Equation 5 is applied to com-\npute the posteriors Î¸A\ni=p(wi|Ï€A)for each tag wi, and to\nform a decision-fusing multinomial Î¸A= (Î¸A\n1,...,Î¸A\n|V|).\n4. EXPERIMENTAL SETUP\n4.1 Dataset\nIn our experiments, we used the CAL500 dataset [20], which\nconsists of 502 popular Western songs by as many different\nartists. The CAL500 dataset provides binary annotations,\nwhich are 1 when a tag applies to the song and 0 otherwise,\nbased on the opinions of human annotators. To accurately ï¬t\nthe experimental models, we restrict ourselves to the subset\nof 97 tags that have at least 30 songs positively associated\nwith them (11 genre, 14 instrument, 25 acoustic quality, 6\nvocal characteristics, 35 emotion and 6 usage tags).\n4.2 Audio features\nThe acoustic content of each song in the collection is repre-\nsented by computing a time series of 34-bin Mel-frequency\nspectral features [15], extracted over half-overlapping win-\ndows of 92ms of audio signal. For the auto-tagger based on\nthe DTM, Mel-frequency spectral features are grouped into\nfragments of approximately 6s. (with 80% overlap), which\ncorresponds to Ï„= 125 consecutive feature vectors. For the\nauto-tagger based on the GMM, the Mel-frequency spectral\nfeatures are decorrelated using the DCT, and the resulting\nï¬rst 13 Mel-frequency cepstral coefï¬cients are augmented\nwith ï¬rst and second derivatives (MFCC-deltas). Lastly, for\nthe auto-tagger based on boosting, ï¬rst and second order\nstatistics of the MFCC deltas are computed every 5s., in\norder to reduce the computational burden [8] .\n4.3 Evaluation\nIn our experiments, we consider the models reviewed in Sec-\ntion 2.1, which are the content-based auto-taggers referred\nas GMM, BST, and DTM, and the semantic context mod-\neling based on the DMM. We obtained the authorsâ€™ code\nto run each algorithm. We study model combination via\ndecision-fusion using the DMM and investigate all the pos-\nsible combinations among the content-based auto-taggers\nconsidered. For instance, when combining all the three auto-\ntaggers (i.e., when A={GMM,BST,DTM}) Equation 7\nacts on the aggregated semantic multinomials deï¬ned as:\nÏ€A\nd= (Ï€GMM\nd,Ï€BST\nd,Ï€DTM\nd). (8)\nTo investigate the advantages of model combination via\ndecision-fusion, we compared its performances to a variety\nof combination techniques, such as ï¬xed-combination rules\n[11] and trained-combiners based on regression [19], all of\n70812th International Society for Music Information Retrieval Conference (ISMIR 2011)\nwhich are applied on the outputs of the different content-\nbased auto-taggers (i.e., GMM, BST, DTM). We tested dif-\nferent ï¬xed-combination rules (i.e., sum, product, arithmetic\naverage, minimum and maximum rule) in preliminary ex-\nperiments, with the sum rule (/summationtextrule) being the best. So,\nfor example, when/summationtextrule combines GMM, BST and DTM\nsumming the corresponding SMNs, the ï¬nal semantic multi-\nnomial of each song sis:\nÏ€SUM\ns=Ï€GMM\ns+Ï€BST\ns+Ï€DTM\ns, (9)\nwhich is intended to be normalized to 1.\nAdditionally, we implemented the trained-combiner based\non linear regression (LinReg), which Tomasik et. al [19]\nshowed to outperform alternative regression techniques. In\nparticular, we use LinReg to learn, on a tag-by-tag bases,\nthe optimal coefï¬cients to combining different auto-taggers\nto predict a ground truth of annotated songs. We refer the\nreader to Section 3.3 of [19] for more details on this strategy.\nAnnotation and retrieval performances are measured fol-\nlowing [20]. Test set songs are annotated with the 10 most\nlikely tags in their SMNs, and annotation accuracy is re-\nported by computing precision, recall and F-score for each\ntag. Retrieval performance are evaluated with respect to\neach one-tag query in our vocabulary; we report mean av-\nerage precision (MAP), area under the receiver operating\ncharacteristic curve (AROC) and top-10 precision (P10). All\nmetrics are averaged over all tags and are intended to be re-\nsult of 5 fold cross validation, where each song appeared in\nthe test set exactly once.\n5. RESULTS\nAnnotation and retrieval results are presented in Table 1.\nResults for (a) individual auto-taggers are in the ï¬rst block\nof the table, results for (b) standard contextual approaches\nare in the second block, and results for (c) content-based\nauto-tagger combination are in the last four blocks.\nFirst, we notice that for each combination of the content-\nbased auto-taggers considered, decision-fusion outperforms\nall the other combination techniques, except in recall, where\nLinReg is generally the best one. Second, differently from/summationtextrule and LinReg, decision-fusion always improves with\nrespect to the original content-based auto-taggers combined.\nDecision-fusion performs better by capturing the corre-\nlations that arise between tag predictions based on differ-\nent auto-taggers and, consequently, by indirectly leveraging\nvarious aspects of the musical signal emphasized by each\nof those auto-taggers. Indeed, decision-fusion of BoF auto-\ntaggers with the DTM has major beneï¬ts, as it takes advan-\ntage of predictions that are based on different fundamentals,\ni.e., timbre and temporal dynamics vs. only timbre. On the\nother hand, decision-fusion of GMM and BST, which bothretrieval annotation\nModel MAP AROC P10 P R F-score\nGMM 0.417 0.686 0.425 0.374 0.205 0.213\nBST 0.432 0.701 0.453 0.334 0.144 0.170\nDTM 0.446 0.708 0.460 0.446 0.217 0.264\n(a)content-based auto-taggers\nGMM 0.447 0.711 0.465 0.436 0.238 0.253\nBST 0.457 0.711 0.476 0.424 0.201 0.241\nDTM 0.464 0.723 0.480 0.461 0.236 0.275\n(b)context-modeling with DMM\ntwo BoF modelsA=(GMM, BST)Prule 0.440 0.709 0.463 0.369 0.153 0.185\nLinReg [19] 0.444 0.708 0.459 0.371 0.239 0.226\ncontext fusion 0.460 0.719 0.475 0.425 0.224 0.255\na BoF and a time-series model A=(BST, DTM)Prule 0.454 0.721 0.475 0.385 0.156 0.189\nLinReg [19] 0.445 0.711 0.457 0.388 0.237 0.228\ncontext fusion 0.475 0.729 0.495 0.434 0.221 0.265\na BoF and a time-series model A=(GMM, DTM)Prule 0.461 0.726 0.474 0.445 0.229 0.267\nLinReg [19] 0.456 0.722 0.460 0.360 0.248 0.222\ncontext fusion 0.470 0.730 0.487 0.484 0.230 0.291\ntwo BoF and a time-series model A=(GMM, BST, DTM)Prule 0.457 0.725 0.478 0.39 0.163 0.202\nLinReg [19] 0.452 0.715 0.465 0.384 0.242 0.232\ncontext fusion 0.475 0.731 0.496 0.456 0.217 0.270\n(c)auto-tagger combination\nTable 1 . Annotation and retrieval for the different models\non the CAL500 dataset. The best results for each scenario\nare indicated in bold.\nmodel only the timbre, does not achieve comparable im-\nprovements over the corresponding standard context-models.\nIn addition, the combination of all three auto-taggers with\ndecision-fusion leads to the best retrieval performance; yet\nthe modest improvements over the combination of BST and\nDTM in retrieval are compensated by improvements in pre-\ncision and F-score over the same method.\nFigure 1 depicts the MAP score achieved by a subset of\ntags, for the content-based auto-taggers (i.e., GMM, BST,\nDTM) and for decision-fusion using GMM, BST and DTM.\nEven if DTM could be preferred over both GMM and BST\nbased on the average performances reported in Table 1, the\nï¬‚uctuation in performance on speciï¬c tags shown in Fig-\nure 1 suggests that each content-based auto-tagger may be\nbetter suited for a subset of the tags than the others. How-\never, leveraging a rich contextual information that beneï¬ts\nfrom various acoustic characteristics of the musical signal,\ndecision-fusion using GMM, BST and DTM performs best\non the majority of all the tags reported.\nFinally, part (b) of Table 1 also reports that standard con-\ntext modeling always improves over the individual perfor-\nmance of the original content-based auto-taggers. While\n709Poster Session 6\n0.2 0.3 0.4 0.5 0.6 0.7alternativeelectronicapopr&bbacking vocalsemotional vocalsfemale lead vocalsacoustic guitarelectric guitarsequenceraggressiveboringlikemellowrelaxsadweaknegative feelingspositive feelingsdrivinggoing to sleep\nMAP  GMM\nBST\nDTM\ncontext fusion (all)Figure 1 . Retrieval performance (MAP) for a subset of the\nCAL500 vocabulary for GMM, BST, DTM, and decision-\nfusion of GMM, BST and DTM. Among the content-based\nauto-tagger, each one appears to be best on a subset of tags.\nHowever, decision-fusion is superior on the majority of tags.\nMiotto et al. [13] already showed this for the BoF models\n(i.e., GMM and BST), we have demonstrated that it holds\ntrue for the DTM as well.\n6. CONCLUSION\nIn this paper we have proposed decision-fusion as a strat-\negy for combining different content-based auto-taggers. It\nuses semantic context modeling to simultaneously leverage\nthe beneï¬ts of different content-based auto-taggers. Experi-\nmental results demonstrate especially that it achieves better\nannotation and retrieval performance than individual auto-\ntaggers and various other techniques to combining multiple\ncontent-based auto-taggers.\n7. ACKNOWLEDGEMENTS\nThe authors thank L. Barrington and T. Bertin-Mahieux for\nproviding the code of [20] and [8] respectively, and acknowl-\nedge support from Qualcomm, Inc., Yahoo! Inc., the Hell-\nman Fellowship Program, and NSF Grants CCF-0830535\nand IIS-1054960. This research was supported in part by the\nUCSD FWGrid Project, NSF Research Infrastructure GrantNumber EIA-0303622. R.M. thanks Nicola Orio for helpful\ndiscussion.\n8. REFERENCES\n[1] T. Bertin-Mahieux, D. Eck, F. Maillet, and P. Lamere. Autotagger: a model for\npredicting social tags from acoustic features on large music databases. Journal of\nNew Music Research , 37(2):115â€“135, June 2008.\n[2] G. Carneiro, A.B. Chan, P.J. Moreno, and N. Vasconcelos. Supervised learning of\nsemantic classes for image annotation and retrieval. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence , 29(3):394â€“410, 2007.\n[3] M. Casey, C. Rhodes, and M. Slaney. Analysis of minimum distances in high-\ndimensional musical spaces. IEEE Transactions on Audio, Speech and Language\nProcessing , 16(5):1015â€“1028, 2008.\n[4] A. B. Chan and N. Vasconcelos. Modeling, clustering, and segmenting video with\nmixtures of dynamic textures. IEEE Transactions on Pattern Analysis and Ma-\nchine Intelligence , 30(5):909â€“926, 2008.\n[5] A.B. Chan, E. Coviello, and G. Lanckriet. Clustering dynamic textures with the\nhierarchical EM algorithm. In Proc. IEEE CVPR , 2010.\n[6] E. Coviello, A. Chan, and G. Lanckriet. Time Series Models for Semantic Music\nAnnotation. Audio, Speech, and Language Processing, IEEE Transactions on ,\n19(5):1343â€“1359, July 2011.\n[7] G. Doretto, A. Chiuso, Y . N. Wu, and S. Soatto. Dynamic textures. Intl. J. Com-\nputer Vision , 51(2):91â€“109, 2003.\n[8] D. Eck, P. Lamere, T. Bertin-Mahieux, and S. Green. Automatic generation of\nsocial tags for music recommendation. In Advances in Neural Information Pro-\ncessing Systems , 2007.\n[9] A. Flexer, F. Gouyon, S. Dixon, and G. Widmer. Probabilistic combination of\nfeatures for music classiï¬cation. In Proc. ISMIR , pages 111â€“114, 2006.\n[10] M. Hoffman, D. Blei, and P. Cook. Easy as CBA: A simple probabilistic model\nfor tagging music. In Proc. ISMIR , pages 369â€“374, 2009.\n[11] J. Kittler. Combining classiï¬ers: A theoretical framework. Pattern Analysis and\nApplications , 1(1):18â€“27, 1998.\n[12] M.I. Mandel and D.P.W. Ellis. Multiple-instance learning for music information\nretrieval. In Proc. ISMIR , pages 577â€“582, 2008.\n[13] R. Miotto, L. Barrington, and G. Lanckriet. Improving auto-tagging by modeling\nsemantic co-occurrences. In Proc. ISMIR , pages 297â€“302, 2010.\n[14] S.R. Ness, A. Theocharis, G. Tzanetakis, and L.G. Martins. Improving automatic\nmusic tag annotation using stacked generalization of probabilistic svm outputs. In\nProc. ACM MULTIMEDIA , pages 705â€“708, 2009.\n[15] L. Rabiner and B. H. Juang. Fundamentals of Speech Recognition . Prentice Hall,\nUpper Saddle River (NJ, USA), 1993.\n[16] N. Rasiwasia and N. Vasconcelos. Holistic context modeling using semantic co-\noccurences. In Proc. IEEE CVPR , pages 1889â€“1895, 2009.\n[17] J. Reed and C.H. Lee. A study on music genre classiï¬cation based on universal\nacoustic models. In Proc. ISMIR , pages 89â€“94, 2006.\n[18] M. Slaney, K. Weinberger, and W. White. Learning a metric for music similarity.\nInProc. ISMIR , pages 313â€“318, 2008.\n[19] B. Tomasik, J.H. Kim, M. Ladlow, M. Augat, D. Tingle, R. Wicentowski, and\nD. Turnbull. Using regression to combine data sources for semantic music dis-\ncovery. In Proc. ISMIR , pages 405â€“410, 2009.\n[20] D. Turnbull, L. Barrington, D. Torres, and G. Lanckriet. Semantic annotation and\nretrieval of music and sound effects. IEEE Transactions on Audio, Speech and\nLanguage Processing , 16(2):467â€“476, February 2008.\n[21] G. Tzanetakis and P. Cook. Musical genre classiï¬cation of audio signals. IEEE\nTransactions on speech and audio processing , 10(5):293â€“302, 2002.\n[22] N. Vasconcelos and A. Lippman. Learning mixture hierarchies. In Advances in\nNeural Information Processing Systems , pages 606â€“612, 1998.\n[23] B. Whitman and D. Ellis. Automatic record reviews. In Proc. ISMIR , pages 470â€“\n477, 2004.\n[24] Y .H. Yang, Y .C. Lin, A. Lee, and H. Chen. Improving musical concept detection\nby ordinal regression and context fusion. In Proc. ISMIR , pages 147â€“152, 2009.\n710"
    },
    {
        "title": "Feature Extraction and Machine Learning on Symbolic Music using the music21 Toolkit.",
        "author": [
            "Michael Scott Cuthbert",
            "Christopher Ariza",
            "Lisa Friedland"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1416288",
        "url": "https://doi.org/10.5281/zenodo.1416288",
        "ee": "https://zenodo.org/records/1416288/files/CuthbertAF11.pdf",
        "abstract": "Machine learning and artificial intelligence have great potential to help researchers understand and classify musical scores and other symbolic musical data, but the difficulty of preparing and extracting characteristics (features) from symbolic scores has hindered musicologists (and others who examine scores closely) from using these techniques. This paper describes the â€œfeatureâ€ capabilities of music21, a general-purpose, open source toolkit for analyzing, searching, and transforming symbolic music data. The features module of music21 integrates standard featureextraction tools provided by other toolkits, includes new tools, and also allows researchers to write new and powerful extraction methods quickly. These developments take advantage of the systemâ€™s built-in capacities to parse diverse data formats and to manipulate complex scores (e.g., by reducing them to a series of chords, determining key or metrical strength automatically, or integrating audio data). This paperâ€™s demonstrations combine music21 with the data mining toolkits Orange and Weka to distinguish works by Monteverdi from works by Bach and German folk music from Chinese folk music.",
        "zenodo_id": 1416288,
        "dblp_key": "conf/ismir/CuthbertAF11",
        "keywords": [
            "music21",
            "feature capabilities",
            "symbolic music data",
            "feature extraction",
            "toolkits",
            "data mining",
            "Orange",
            "Weka",
            "Monteverdi",
            "Bach"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\n  \n \nFEATURE EXTRACTION AND MACHINE LEARNING \nON SYMBOLIC MUSIC USING THE music21  TOOLKIT \nMichael Scott Cuthbert Christopher Ariza Lisa Friedland \nMusic and Theater Arts \nM.I.T. \ncuthbert@mit.edu  Music and Theater Arts \nM.I.T. \nariza@mit.edu  Department of Computer Science  \nUniversity of Massachusetts Amherst \nlfriedl@cs.umass.edu  \nABSTRACT \nMachine learning and artificial intelligence have great po-\ntential to help researchers understand and classify musical scores and other symbolic musical data, but the difficulty of preparing and extracting ch aracteristics (features) from \nsymbolic scores has hindered musicologists (and others who examine scores closely) from using these techniques. \nThis paper describes the â€œfeatureâ€ capabilities of \nmusic21 , \na general-purpose, open source toolkit for analyzing, \nsearching, and transforming symbolic music data. The fea-tures module of \nmusic21  integrates standard feature-\nextraction tools provided by other toolkits, includes new \ntools, and also allows researchers to write new and power-ful extraction methods quickly. These developments take advantage of the systemâ€™s built-in capacities to parse di-verse data formats and to manipulate complex scores (e.g., by reducing them to a series of chords, determining key or metrical strength automatically, or integrating audio data). This paperâ€™s demonstrations combine \nmusic21  with the \ndata mining toolkits Orange and Weka to distinguish works \nby Monteverdi from works by Bach and German folk mu-sic from Chinese folk music. \n1. INTRODUCTION \nAs machine learning and data mining tools become ubiqui-\ntous and simple to implement, their potential to classify da-ta automatically, and to point out anomalies in that data, is extending to new disciplines. Most machine learning algo-rithms run on data that can be represented as numbers. While many types of datasets naturally lend themselves to numerical representations, much  of the richness of music \n(especially music expressed in symbolic forms such as scores) resists easily being converted to the numerical forms that enable classifica tion and clustering tasks.  The amount of preprocessing needed to extract the most \nmusically relevant data from notation encoded in Finale or Sibelius files, or even MIDI files, is often underestimated: musicologists are rarely content to work only with pitch classes and relative note lengthsâ€”to name two easily ex-tracted and manipulated types of information. They also want to know where a pitch fits within the currently im-plied key, whether a note is metrically strong or weak, what text is being sung at the same time, whether chords are in open or closed position, and so on. Such processing and analysis steps need to run ra pidly to handle the large reper-\ntories now available. A robust system for data mining needs to integrate reliable and well-developed classification tools with a wide variety of methods for extracting data from large collections of scores in a variety of encodings. \nThe features module newly added to the Python-\nbased, open source toolkit \nmusic21 , provides this needed \nbridge between the demands of music scholars and of com-\nputer researchers. Music21 [3] already has a well-\ndeveloped and expandable frame work for importing scores \nand other data from the most common symbolic music for-mats, such as MusicXML [4 ] (which Finale, Sibelius, \nMuseScore, and other notation software can produce), Kern/Humdrum [6], CCARHâ€™s MuseData [11], Notewor-thy Composer, the common folk-music format ABC [10], and MIDI. Scores can easily be transformed from symbolic to sounding representations (b y uniting tied notes or mov-\ning transposing instruments to C, for instance); simultanei-ties can be reduced to chords  that represent the pitches \nsounding at any moment; and the key or metrical accents of a passage can be analyzed (e ven for passages that change \nkey without a change in key signature).  \nThe features module expands\n music21 â€™s data mining \nabilities by adding a battery of commonly used numeric \nfeatures, such as numerical re presentations of elements pre-\nsent or absent in a piece (0s or  1s, used, for example, to in-\ndicate the presence of a change in a time signature), or con-tinuous values representing prevalence (for example, the percentage of all chords in a piece that are triadic). Collec-\ntions of these features can be used to train machine learning \nsoftware to classify works by composer, genre, or dance type. Or, making use of notational elements found in cer-tain input formats, they could classify works by graphical characteristics of particular interest to musicologists study- \nPermission to make digital or hard copi es of all or part of this work fo r\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for profit or commercial advantage and that copies \nbear this notice and the full citation on the first page.  Â© 2011 International Society for Music Information Retrieval  \n387Poster Session 3\n  \n \ning the reception of the work . Such graphical elements \nmight identify the scribe, edito r, or publisher of a piece. \nIn the following sections, we will describe the feature-\nextraction methods ( FEMS ) of music21 . Because music21  \nhas many powerful, high-level tools for analysis and trans-\nformation, FEMS can be tailored to th e characteristics of \nparticular repertories and can be combined to create more powerful \nFEMS than those available in existing software \npackages. This paper describes how new FEMS can be add-\ned to music21  and demonstrates their usefulness in classi-\nfying both classical and popular works. \n2. FEATURE EXTRACTION IN MUSIC21 \n2.1 Feature Extractors from jSymbolic \nOne of the most useful aspects of the Features module is \nthe integration of 57 features of the 111 implemented in Cory McKayâ€™s jSymbolic toolkit [9], a subset of his larger jMIR toolkit that classifies music encoded in MIDI [8].  \n(\nMusic21  aims for full jSymbolic compatibility in the near \nfuture.) Because music21  is â€œencoding agnostic,â€ files in \nany supported format now have access to these FEMS , so \nthat MusicXML and ABC files (among others) can, without conversion, be run through the same extractors that jSym-bolic provided for MIDI files. In addition, Music21 \nFEMS \nare optimized so that closely related feature extractors that \nrequire the same preprocessing routines automatically use \ncached versions of the processe d data, rather than recreat-\ning it.  \nExample 1 shows how a single feature extractor, bor-\nrowed from jSymbolic, can be applied to data from several different sources and datatypes. While using a single fea-ture extractor on one or two works is not a useful way to classify these works, it is a convenient and informative way to understand the system and test the \nFEMS . All FEMS have \ndocumentation and code examples on the music21 website at http://mit.edu/music21. The website also gives instruc-tions for obtaining and installing the software, as well as tutorials and references on using the toolkit. \nExample 1 shows how the fraction of ascending notes \nin a movement of Handelâ€™s Messiah (encoded as MuseDa-ta) can be found.  \nfrom music21 import * \nhandel = corpus.parse('hwv56/movement3-05.md') fe = features.jSymbolic.\\          DirectionOfMotionFeature(handel) feature = fe.extract() print feature.vector \n[0.5263] \nExample 1 . Feature extraction on a MuseData score. Example 2 shows feature extraction run first on a lo-\ncal file, and then on a file fro m the Internet. The feature ex-\ntractor determines whether the initial time signature is a tri-ple meter and returns 1 or 0.  The result is returned in a Py-thon list, since some \nFEMS return an array of results, such as \na 12-element histogram showing the count of each pitch class.  Like Example 1, this example uses file formats (ABC and MusicXML) that cannot be directly processed by jSymbolic. (In all further examples, the initial line, â€œfrom music21 import *â€ is omitted.)  \n# a 4/4 basse danse in ABC format \nbd = converter.parse(\"/tmp/basseDanse20.abc\") fe = features.jSymbolic.TripleMeterFeature(bd) print fe.extract().vector \n[0] \n# softly-softly by Mark Paul, in 3/4 \nsoft = converter.parse(  \"http://static.wikifonia.org/10699/musicxml.xml\") fe.setData(soft) print fe.extract().vector \n[1] \nExample 2 . A local file and a web file in two different \nformats run through a triple-meter feature extractor. \n2.2 Feature Extractors Native to music21   \nIn addition to recreating the feature extraction methods of \njSymbolic, music21 â€™s features.native sub-module includes \n17 new FEMS . These FEMS take advantage of the analytical \ncapabilities built into music21 , its ability to work with no-\ntational aspects (such as a noteâ€™s spelling or representation \nas tied notes), or the richer, object-oriented programming environment of Python. For example, native \nmusic21  \nFEMS can distinguish between correctly or incorrectly \nspelled triads within a polyphonic context. (The Incorrect-lySpelledTriadPrevalence \nFEM, called on Mozartâ€™s pieces, \nreturns approximately 0.5% of all triads, mostly reflecting chromatic lower neighbors). Notational features that do not affect playback, such as a sc ribeâ€™s predilection for beaming \neighth notes in pairs (as opposed to in groups of four) in 4/4, can similarly form the ba sis for feature extraction. Fea-\nture extractors can also use a workâ€™s metadata, along with the larger capabilities of the Python language, to add pow-erful classification methods. An example of this is the ComposerPopularity feature, which returns a base-10 loga-rithm of the number of Google hits for a composerâ€™s name (see Example 3). \ns = corpus.parse('mozart/k155', 2)  \nprint s.metadata.composer \nW. A. Mozart \nfe = features.native.ComposerPopularity(s) \nprint fe.extract().vector \n[7. 0334237554869485] \n38812th International Society for Music Information Retrieval Conference (ISMIR 2011)\n  \n \nExample 3.  The ComposerPopularity feature extractor re-\nports that there are about 10 million Google results, or ap-\nproximately 107, for the form of Mozartâ€™s name encoded \nin the version of K155 movement 2 that appears in the \nmusic21  corpus, a collection of approximately ten thou-\nsand works provided with the toolkit. \nSeveral of the native FEMS  are adaptations of jSym-\nbolic extractors, expanded by capabilities offered by other modules in \nmusic21 . For instance, McKayâ€™s â€œQualityâ€ \nfeature classifies a piece as either in major or in minor \nbased on information encoded within the initial key signa-ture of some MIDI files. For files without this information, \nmusic21 â€™s enhancement of this FEM (fea-\ntures.native.QualityFeature) will also run a Krumhansl-\nSchmuckler probe-tone key analysis (with the default Aarden-Essen weightings) [7] on the work to determine the most likely mode. The native module also includes many chord-related \nFEMS  that were proposed by McKay but not \nincluded in the present release of jSymbolic. \n2.3 Writing Custom Feature Extractors \nOne of the strengths of music21 â€™s feature system is the \nease of writing new FEMS . After inheriting the common su-\nperclass FeatureExtractor, new FEMS  can be created and \nused alongside existing FEMS . The core functionality is im-\nplemented in a private method called _ process() , which sets \nthe values of the vector of an  internally stored Feature ob-\nject. The FeatureExtractor superclass provides automatic access to a variety of presentati ons of the score, from a flat \nrepresentation (using the .flat property) to a reduction as \nchords, along with  histograms of commonly requested mu-sical features such as pitch class or note duration. These \nrepresentations are cached for quicker access later as keys on a property called data (such as self.data['chordify']). The \nobject also allows direct acce ss to the source score through \nthe stream property. \nExample 4 creates a new feature extractor that reports \nthe percentage of notes that  contain accident als (including \ndouble sharps and flats, but excluding naturals) that are not B-flats. This feature could he lp chart the in creased usage \nover the course of the Renaissance of musica ficta , that is, \nchromatic notes beyond B-flat  (the only accidental common \nto Medieval and Renaissance music). \n# Feature Extractor definition \nclass MusicaFictaFeature( \n             features.FeatureExtractor):  name = 'Musica Ficta'  discrete = False  dimensions = 1  id = 'mf'       def _process(self):   allPitches = self.stream.flat.pitches \n  # N.B.: self.data['flat.pitches'] works  \n  # equally well and caches the result for    # faster access by other FEMS. \n  fictaPitches = 0   for p in allPitches:    if p.name == \"B-\":     continue    elif p.accidental is not None \\      and p.accidental.name != 'natural':     fictaPitches += 1   self._feature.vector[0] = \\      fictaPitches / float(len(allPitches))  \n# example of usage of the new method on two pieces # (1) D. Luca early 15th c. Gloria \nluca = corpus.parse('luca/gloria.mxl') fe = MusicaFictaFeature(luca) print fe.extract().vector \n[0.01616915422885572] \n# (2) Monteverdi, late 16th c. madrigal  \nmv = corpus.parse('monteverdi/madrigal.3.1.xml') fe.setData(mv) print fe.extract().vector \n[0.05728727885425442] \nExample 4. A custom feature extractor to find musica fic-\nta, applied to an early 15th-century Gloria and a late 16th-\ncentury madrigal. \n3. MULTIPLE FEATURE EXTRACTORS AND \nMULTIPLE SCORES \nSince the previous examples ha ve extracted single features \nfrom one or two scores, similar results could have just as well been obtained through the object model or analytical routines of the \nmusic21  toolkit. But machine learning \ntechniques require a large group of scores and many fea-\ntures. The features module shines for such studies by mak-ing it easy and, through caching, fa st to run many scores (or \nscore excerpts) through many \nFEMS , and to graph the results \nor output them in the formats commonly used by machine learning programs. \n3.1 Extracting Information from DataSets \nThe DataSet object of the features module is used for clas-\nsifying a group of scores by a particular class value using a set of \nFEMS . Its method addFeatureExtractors()  takes a list \nof FEMS  that will be run on the data. (For ease of getting a \nlarge set of FEMS , each feature extractor has a short id \nwhich allows it to be found by the method extractorsById() . \nThe special id â€œallâ€ gets all feature extractors from both na-\ntive and jSymbolic libraries.) The addData() method adds a \nmusic21  Stream [1] (i.e., a score, a part, a fragment of a \nscore, or any other symbolic musical data) to the DataSet, \noptionally specifying a class va lue (such as the composer, \nwhen the task at hand is cla ssifying composers) and an id \n(such as a catalogue number or file name). For conven-ience, addData() can also take a string containing a file \npath to the data (in any of several formats), a URL to the score on the internet, or a re ference to the work in the \nmu-\n389Poster Session 3\n  \n \nsic21  corpus. Example 5 sets up a DataSet to run three \nFEMS related to note length on f our pieces: two by Bach, \none by Handel, and an â€œunknownâ€  work (also by Handel).  \nIf a file has been read in on ce and is unmodified since the \nlast reading, its parsed vers ion is cached in a Python â€œpick-\nleâ€ file for quicker reading in subsequent runs. \nds = features.DataSet(classLabel='Composer') \nfes = features.extractorsById(['ql1','ql2','ql3']) ds.addFeatureExtractors(fes)  b1 = corpus.parse('bwv1080', 7).measures(0,50) ds.addData(b1, classValue='Bach', id='artOfFugue') ds.addData('bwv66.6.xml', classValue='Bach') ds.addData('c:/handel/hwv56/movement3-05.md',                classValue='Handel') ds.addData('http://www.midiworld.com/midis/other/handel/gfh-jm01.mid') ds.process() \nExample 5 . Setting up and processing a DataSet with \nthree FEMS  and four scores. \nExtracting the data from a DataSet is simple once pro-\ncess() has been called.  The simplest way of getting the \noutput of multiple feature ex tractors is through DataSetâ€™s \nwrite() method, which can take a filename or a file format \n(if no file path is given, a file is saved to the userâ€™s â€œtempâ€ directory). File formats are speci fied as strings that call the \nappropriate OutputFormat object. \nMusic21  comes with \nOutputFormats for comma-separated values (csv), tab-\ndelimited output (tab) for Orange, and Attribute-Relation File Format (arff) for Weka. The OutputFormat object is \nsubclassable, so additional formats for R, Matlab, native Excel (an .xls reader/wr iter is packaged with \nmusic21 ), or \njson (for Java, Max/MSP, or other systems) can easily be \ndeveloped.  \nOther ways of obtaining extracted features include \nDataSetâ€™s getFeaturesAsList() method, which returns a list \nof lists, one list of feature results for each piece, and getString(), which returns the data as a single string in any \nof the supported formats. If the optional Python package Matplotlib is installed, the data can also be graphed from within \nmusic21 . Finally, because the DataSet is fully inte-\ngrated with the rest of the t oolkit, specific Streams can be \nexamined in notation. Exampl e 6 takes the DataSet object \nfrom Example 5 and examines it in several ways.  Part (a) writes it out as an comma-separat ed file; (b) prints the at-\ntribute labels; (c) gets the entire feature output as a list of lists and prints one line of it; (d) displays the entire feature data in OrangeTab output. Part (e) examines the feature vectors and displays as pngs (via Lilypond) any scores where the most common note value is an eighth note (length = 0.5); the resulting output contains the two Handel scores. Part (f) plots the last  two features (most common \nnote length and the prevalence of that length) for each \npiece. \n (a) \nds.write('/usr/cuthbert/baroqueQLs.csv') \n \n(b) \nprint ds.getAttributeLabels() \n['Identifier', 'Unique_Note_Quarter_Lengths', \n'Most_Common_Note_Quarter_Length', 'Most_Common_Note_Quarter_Length_Prevalence', 'Composer'] \n \n(c) \nfList = ds.getFeaturesAsList() \nprint fList[0] \n['artOfFugue', 15, 0.25, 0.6287328490718321, 'Bach'] \n \n(d) \nprint features.OutputTabOrange(ds).getString() \nIdentifier Unique_Noteâ€¦ Most_Commonâ€¦ Most_Com..Prevalence Composer \nstring discrete continuo us continuous discrete \nmeta    class artOfFugue 15 0.25 0.628732849072 Bach bwv66.6.xml 3 1.0 0.601226993865 Bach hwv56/movemâ€¦ 7 0.5 0.533333333333 Handel http://www.mid... 14 0.5 0.768951612903  \n \n(e) \nfor i in range(len(fList)): \n  if fList[i][2] == 0.5:     ds.streams[i].show('lily.png') \n[HWV 56 3-5, from the Messiah ] \n \n[â€œMourn ye afflicted Children,â€ from Judas Maccabaeus] \n \n(f) \np = graph.PlotFeatures(ds.streams, \n           fes[1:], roundDigits = 2) p.process() \n \nExample 6. Viewing the contents of a DataSet object. \n39012th International Society for Music Information Retrieval Conference (ISMIR 2011)\n  \n \n3.2 Using Feature Data for Classification \nOnce the DataSet object has been  plotted or viewed as mu-\nsical data to check the results for obvious errors, then the outputted data can be fed into any number of standard data mining packages for analyses su ch as clustering or classifi-\ncation. The package Orange (http://orange.biolab.si) inte-grates well with \nmusic21  since it provides a Python inter-\nface to its classification algor ithms (in addition to having a \nGUI); other toolkits such as Weka [5] can also easily be \nused. Below, we include sample code for using Orange, but the results of Examples 8 and 9 were produced in Weka. Complete code examples, along with our sample data, can be found in the demos directory in the \nmusic21  distribu-\ntion.   \n4. DEMONSTRATIONS AND RESULTS \nWe end this paper with two demonstrations of the power of \nfeature extraction in music21  to enable automatic classifi-\ncation of musical styles and composers from symbolic data \nencoded in many formats. The first example uses 24 pitch- and rhythm-based feature extractors (p1â€“16, 19â€“21, and r31â€“35) to classify monophonic folksongs from four files in the Essen folksong database as being from either China or Central Europe (mostly Germany). Two files, folkTrain.tab and folkTest.tab , are created according to the \nsame model as Example 5. (Full source for this part of the example is available in the \nmusic21  distribution as de-\nmos/ismir2011/prepareChinaEurope().) The files contain \n969 and 974 songs, respectively, and the extractors de-scribed above result in 174 feat ures, although about half are \ndiscarded during preprocessing b ecause they have the same \nvalue for every song.  \nExample 7 applies two classification methods (or \nlearners) to the pair of data files, using the songs in the first file for training the classifier and those in the second for testing (i.e., validating) the classifierâ€™s predictions. The first method, MajorityLearner, simply chooses the classification that is most common in the training data (e.g., for the data in Examples 5-6, it would label the unknown data as Bach, because Bach is represented tw ice as often as Handel in the \nlabeled data), and thus reports  a baseline accuracy for other \nclassification methods to be measured against. The second method, k-nearest neighbors (k NN) [12], assigns to each \ntest example the majority label among the k most similar \ntraining examples. After assigni ng an origin to each song in \nfolkTest, the program consu lts the correct answer or \nâ€œground truth,â€ and in the end it prints the fraction of songs correctly labeled by each cla ssifier: 69% for the baseline \n(MajorityLearner) and over 94% for kNN. The perfor-mance of kNN over MajorityLearner stands only to in-crease with the development,  in the near future, of \nFEMS \nmore suited to the nuances of folk music. \n import orange, orngTree \ntrainData = orange.ExampleTable('/folkTrain.tab') testData  = orange.ExampleTable('/folkTest.tab')  majClassifier = orange.MajorityLearner(trainData) knnClassifier = orange.kNNLearner(trainData)      majWrong = 0 knnWrong = 0      for testRow in testData:   majGuess = majClassifier(testRow)   knnGuess = knnClassifier(testRow)   realAnswer = testRow.getclass()   if majGuess == realAnswer:     majCorrect += 1   if knnGuess = realAnswer:     knnCorrect += 1     total = float(len(testData)) print majCorrect/total, knnCorrect/total \n0.68788501026694049 0.94353182751540043 \nExample 7. Using data output from the features module of \nMusic21  to classify folksongs in Orange. \n \nIn Example 7, the training and testing data are split approx-imately 50-50. We can increase both the amount of data used to train the models and the number of predictions they make by using a technique called 10-fold cross-validation. Example 8 shows the results of doing this, on the same da-ta, using a variety of classifiers in Weka.  \nClassifier Accuracy \nMajority (baseline) 63% \nNaÃ¯ve Bayes 79% \nNaÃ¯ve Bayes (using supervised         discretization option) 91% \nDecision tree 93% \nLogistic regression 95% K-nearest neighbor (using k = 3) 96% \nExample 8. Accuracy of classifiers for distinguishing \nChinese from Central Eu ropean folk music. \nWhile kNN was the best classifier in all our experi-\nments, decision tree-based cla ssification systems [2] can be \nhelpful for users wishing to understand how a classifier  decides which features are important. Example 9 shows a decision tree built to distinguish  the vocal works of Bach \nand Monteverdi. Given a data  set of 46 works from each \ncomposer, and the same features used previously, the clas-sifier has selected just 6 f eatures as informative when \nbuilding this tree. (In a 10-fold cross-validation experiment, trees like this achieved about 86% classification accuracy.)  \nAlthough it is not always possible to explain the algo-\nrithm's choices intuitively, some of them make sense upon examination. For example, although Monteverdi uses sharped notes, he does not ever use sharps in his key signa-tures, and thus sharped notes remain uncommon in his pieces. The decision tree picks up on this predilection in its \n391Poster Session 3\n  \n \ntop-level split, the single most informative rule learned (fi-\nnal line of Example 9): if mo re than 14.4% of the pieceâ€™s \nnotes are MIDI note 54 (F#3), then the piece is by Bach (true all 30 out of 30 times in the data set).  \n \nBasic_Pitch_Histogram_54 <= 0.144578 | Initial_Time_Signature_0 <= 3: Bach (4.0) | Initial_Time_Signature_0 > 3 | | Range <= 32: Bach (6.0) | | Range > 32 | | | Basic_Pitch_Histogram_64 <= 0.05: Bach (3.0) | | | Basic_Pitch_Histogram_64 > 0.05 | | | | Basic_Pitch_Histogram_60\n <= 0.921569: Monteverdi (47.0/1.0) \n| | | | Basic_Pitch_Histogram_60 > 0.921569 | | | | | Relative_Strength_of_Top_Pitches <= 0.96875: Bach (4.0) | | | | | Relative_Strength_of_Top_Pitches \n> 0.96875: Monteverdi (2.0)  \nBasic_Pitch_Histogram_54 > 0.144578: Bach (30.0) \nExample 9. Decision tree algorithm applied to distinguish \nBach and Monteverdi â€™s choral pieces. \nThe results of these classification tests of folk and baroque \nmusic demonstrate music21 â€™s utility in automatically de-\ntermining musical style from a score without human inter-\nvention. Sophisticated style analysis tools open up oppor-tunities in other areas, such as more accurate notation and \nplayback. For instance, a program could choose appropriate instruments for digital performance depending on the esti-mated location in which the piece was composed: fiddles for Irish jigs, kotos and shÅ for Japanese folk music. By \nlowering the barriers to using feature extraction, \nmusic21  \ncan bring the fruits of MIR to a wide audience of computer \nmusic professionals.  \n5. FUTURE WORK \nThough these tools are extremely powerful already, the de-\nvelopment of new FEMS in music21  and application of the-\nse features to the classification of musical scores is still in \nits infancy. The authors and the music21  community will \ncontinue to add new feature extractors to solve problems \nthat range from assigning composer names to anonymous works of the Middle Ages and Renaissance, to genre classi-fication of popular music leadsheets. to charting the slow change in use of chromatic harmony in the nineteenth cen-tury. More sophisticated data mining tools such as support vector machines and clustering algorithms can be explored to improve the accuracy of the classification methods. The newest releases of \nmusic21  can take audio data as input; \nthus we hope to combine MIR of symbolic music data with \nfeature extraction methods applied to audio files, inching closer to the goal of creating software for sophisticated mu-sical listening. \n6. ACKNOWLEDGEMENTS \nDevelopment of the feature extraction aspects of the \nmu-\nsic21  toolkit is supported by funds from the Seaver Insti-\ntute. Thanks to Seymour Shlien and Ewa Dahlig-Turek for \npermission to distribute ABC versions of the Essen folk-song database with \nmusic21 . 7. REFERENCES \n[1] C. Ariza and M. Cuthbert: â€œThe music21  Stream: A \nNew Object Model for Representing, Filtering, and \nTransforming Symbolic Musical Structures,â€ Proceedings of the Inter national Computer Music \nConference , 2011. \n[2] L. Breiman et al.: Classification and Regression Trees . \nChapman & Hall, Boca Raton, 1984. \n[3] M. Cuthbert and C. Ariza: â€œ\nmusic21 : A Toolkit for \nComputer-Aided Musicology and Symbolic Music \nData,â€ Proceedings of the Inte rnational Symposium on \nMusic Information Retrieval , pp. 637â€“42, 2010. \n[4] M. Good: â€œAn Internet-Friendly Format for Sheet \nMusic.â€ Proceedings of XML 2001 . \n[5] M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. \nReutemann, I. H. Witten: â€œThe WEKA Data Mining Software: An Update.â€ SIGKDD Explorations , 11(1), \n2009. \n[6] D. Huron: â€œHumdrum and Kern: Selective Feature \nEncoding.â€ In Beyond MIDI: the Handbook of \nMusical Codes . E. Selfridge-Field, ed. MIT Press, \nCambridge, Mass., pp. 375â€“401, 1997. \n[7] C. Krumhansl: Cognitive Foundations of Musical \nPitch. Oxford University Press, Oxford, 1990. \n[8] C. McKay: â€œAutomatic Music Classification with \njMIR,â€ Ph.D. Dissertation,  McGill University, 2010. \n[9] C. McKay and I. Fujinaga: â€œjSymbolic: A feature \nextractor for MIDI files.â€ Proceedings of the \nInternational Computer Music Conference , pp. 302â€“5, \n2006. \n[10] I. Oppenheim, C. Walshaw, and J. Atchley.  \nâ€œThe abc standard 2.0.â€ http://abcnotation.com/wik i/abc:standard:v2.0. 2010. \n[11] C. S. Sapp: â€œMuseinfo: Musical Information \nProgramming in C++.â€  http://museinfo.sapp.org, 2008. \n[12] G. Shakhnarovich, T. Darrell, and P. Indyk: Nearest-\nNeighbor Methods in Learning and Vision,  MIT Press, \nCambridge, Mass. 2006. \n392"
    },
    {
        "title": "A System for Evaluating Singing Enthusiasm for Karaoke.",
        "author": [
            "Ryunosuke Daido",
            "Seongjun Hahm",
            "Masashi Ito",
            "Shozo Makino",
            "Akinori Ito"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1417343",
        "url": "https://doi.org/10.5281/zenodo.1417343",
        "ee": "https://zenodo.org/records/1417343/files/DaidoHIMI11.pdf",
        "abstract": "Evaluation of singing skill is a popular function of karaoke machines. Here, we introduce a different aspect of evaluating the singing voice of an amateur singer: â€œenthusiasmâ€. First, we investigated whether human listeners can evaluate enthusiasm consistently and whether the listenerâ€™s perception matches the singerâ€™s enthusiasm. We then identified three acoustic features relevant to the perception of enthusiasm: A-weighted power, â€œfall-downâ€, and vibrato extent. Finally, we developed a system for evaluating singing enthusiasm using these features, and obtained a correlation coefficient of 0.65 between the system output and human evaluation.",
        "zenodo_id": 1417343,
        "dblp_key": "conf/ismir/DaidoHIMI11",
        "keywords": [
            "evaluation",
            "singing skill",
            "karaoke machines",
            "enthusiasm",
            "human listeners",
            "acoustic features",
            "system",
            "correlation coefficient",
            "enthusiasm",
            "evaluation"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nA SYSTEM FOR EVALUATING SINGING ENTHUSIASM FOR KARAOKE\nRyunosuke Daidoâ‹†, Seong-Jun Hahmâ‹†, Masashi Itoâ€ , Shozo Makinoâ€¡and Akinori Itoâ‹†\nGraduate School of Engineering, Tohoku Universityâ‹†\nTohoku Institute of Technologyâ€ Tohoku Bunka Gakuen Universityâ€¡\n{ryunosuke, branden65, aito }@spcom.ecei.tohoku.ac.jpâ‹†\nitojin@tohtech.ac.jpâ€ makino@ait.tbgu.ac.jpâ€¡\nABSTRACT\nEvaluation of singing skill is a popular function of karaoke\nmachines. Here, we introduce a different aspect of evaluat-\ning the singing voice of an amateur singer: â€œenthusiasmâ€.\nFirst, we investigated whether human listeners can evaluate\nenthusiasm consistently and whether the listenerâ€™s percep-\ntion matches the singerâ€™s enthusiasm. We then identiï¬ed\nthree acoustic features relevant to the perception of enthu-\nsiasm: A-weighted power, â€œfall-downâ€, and vibrato extent.\nFinally, we developed a system for evaluating singing en-\nthusiasm using these features, and obtained a correlation\ncoefï¬cient of 0.65 between the system output and human\nevaluation.\n1. INTRODUCTION\nKaraoke is a form of singing entertainment found world-\nwide, which enables anyone to sing like a professional. Karaoke\nmachines not only provide backing music for singing, but\nalso evaluate the singerâ€™s voice as another entertaining fea-\nture. Studies of analyzing the singing voice have been mak-\ning progress. For example, Nakano et al. reported good re-\nsults of a system for classifying â€œgoodâ€ and â€œpoorâ€ singing\nbased on SVM [2]. Mayor et al. proposed a categorization\nand segmentation system for singing voice expression using\npre-deï¬ned rules and HMM [1]. In this paper, we describe\nour attempt to develop a new service for karaoke: a system\nfor evaluating the singerâ€™s enthusiasm.\nBy â€œenthusiasmâ€, we mean how eager the singer is to\nsing. The term â€œenthusiasmâ€ for singing a song as used\nin this paper is a translation of the Japanese word nessho ,\nwhich literally means â€œhot singingâ€ and is often used for ex-\npressing the energy of a singerâ€™s performance. As karaoke\nis the entertainment for amateur singers, we believe that\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proï¬t or commercial advantage and that copies\nbear this notice and the full citation on the ï¬rst page.\ncâƒ2011 International Society for Music Information Retrieval.singing skill is not the only aspect worth evaluating because\npoor singers can never get a high score. However, even poor\nsingers can sing enthusiastically, so we focused on this as-\npect. We consider that a system which evaluate singing en-\nthusiasm would be an exciting service for amateur karaoke\nusers.\nSinging enthusiasm is similar to the emotion of music\n[3], especially the â€œarousal-calmâ€ aspect. However, there\nare signiï¬cant differences between enthusiasm and emotion.\nFirst, enthusiasm is not an expressed emotion. Karaoke is\nbasically a form of self-entertainment, and most karaoke\nsingers who sing enthusiastically are not trying to convey\ntheir enthusiasm to the audience but are just enjoying them-\nselves. Also, enthusiasm is not an induced emotion, because\na listener who listens to an enthusiastically-sung karaoke\nsong does not necessarily become excited. In our opinion,\nenthusiasm is more like an attitude of singing, rather than an\nemotion.\nAs our study on objectively evaluating enthusiasm was a\nnew attempt, there were several issues to investigate:\nâ€¢Is a feeling of â€œenthusiasmâ€ shared by many listeners?\nâ€¢Is enthusiastic singing also perceived to be â€œenthusi-\nasticâ€ by listeners?\nâ€¢What are the physical features related to enthusiasm?\nâ€¢How can we build a system that evaluates enthusiasm\nautomatically?\nThis paper is organized as follows. In Sections 2 and\n3, we describe the procedures and results of analyzing a\nsinging voice corpus and subjective evaluations, and show\nthat humans can perceive enthusiasm appropriately. In Sec-\ntion 4, we describe our method for choosing acoustic fea-\ntures of a singing voice and discuss the efï¬ciency of each\nfeature. In Section 5, we describe an overview and evalua-\ntions of the system.\n31Oral Session 1: Melody and Singing\n2. SINGING VOICE CORPUS\n2.1 Selection of a Song\nFor this ï¬rst study on singing enthusiasm using a simple and\nreliable scheme, we decided to use just one pop song for all\nthe experiments. â€œItoshi no Ellieâ€ by the Southern All Stars\n(which was covered as â€œEllie My Loveâ€ by Ray Charles)\nwas ï¬nally selected as it satisï¬ed the following conditions:\nâ€¢Not too difï¬cult for amateur singers to sing both â€œen-\nthusiasticallyâ€ and â€œnormallyâ€ i.e., no extremely high,\nlow or long notes.\nâ€¢Well known by all the singers and human subjects of\nthe subjective evaluation (Japanese, in our research).\nAll the recordings should be in the same key because dif-\nferences of key may affect the subjective evaluations. Con-\nsidering the vocal range of amateur singers, we chose to use\nC-Maj. transposed from the original key of D-Maj. As a\nresult, the lowest note is E3 and the highest is G4 for male\nsingers (it can be an octave higher for female singers). The\ntempo is 69-70 bpm.\n2.2 Recording Procedures\nThirty-four singers participated in the recording, none of\nwhom were professional singers. The sound accompani-\nment, which had been directly recorded using a karaoke\nmachine beforehand, was played through headphones and\nthe singers sang along to it. The singers sang into a mi-\ncrophone on a stand with a pop-ï¬lter attached to prevent\nhandling noise and pop noise. The singers were instructed\nnot to move much during the recording and stay almost a\nconstant distance from the microphone. In order to obtain\nvarious voices with a wide range of enthusiasm and to label\nsingersâ€™ intended enthusiasm to each voice, they were each\nasked to sing two times, once â€œenthusiasticallyâ€ and once\nâ€œnormallyâ€. The singers themselves could choose in which\nstyle to sing ï¬rst, and informed us before they sang.\nThe voices were recorded at 44.1-kHz/16-bit sampling in\na soundproof chamber.\n3. SUBJECTIVE EVALUATIONS\nWe conducted subjective evaluations for the following three\npurposes: (1) investigate whether humans can perceive singing\nenthusiasm using the same criteria, (2) investigate whether\nlisteners can distinguish whether singers sang enthusiasti-\ncally or not, and (3) investigate listenersâ€™ intuition about the\nenthusiasm, and obtain clues for choosing acoustic features\nfor automatically evaluating singing enthusiasm.\nFigure 1 . Stimuli for subjective evaluations (parenthesized\nwords are English words)\nEvaluation word Value\nenthusiastic 2\nneither selected 1\nnot enthusiastic 0\nTable 1 . Evaluation words and the values for the subjective\nevaluations\n3.1 Stimuli\nFor the subjective evaluations, we chose short stimuli (about\n1.5 to 9 seconds) from the recordings to facilitate the decision-\nmaking. Figure 1 shows the prepared stimuli.\nIn this study, the absolute sound-pressure level (SPL) is\nof no interest because the SPL depends on not only the mag-\nnitude of a singerâ€™s voice but also the distance between the\nsinger and the microphone. As our method should be ap-\nplied to karaoke machines, it is difï¬cult to measure the mag-\nnitude of the singerâ€™s voice precisely, so we decided to ex-\nclude the effect of absolute SPL, even though our prelimi-\nnary experiment proved that absolute SPL is important for\nperception of enthusiasm. All the stimuli were normalized\nto the same power after passing through a high-pass ï¬lter\n(80 Hz cut-off) to reduce low-frequency noise.\nAs Figure 1 shows, two sets of stimuli were prepared. Set\nA was a collection of 272 stimuli of a phrase that appears\nfour times in the song with the same melody and the same\nlyrics, and set B was a collection of four varieties of phrases,\neach of which was sung 68 times. (B1) is the beginning of\nthis song, (B2) is from the early part, (B3) is from the middle\npart (the bridge or the climax) and (B4) is from the last part.\n3.2 Evaluation Procedure\nFor each set of stimuli, 30 human subjects were asked to\nlisten to the stimuli, and selected one of three evaluation\nwords for each stimulus. Table 1 shows the evaluation words\n3212th International Society for Music Information Retrieval Conference (ISMIR 2011)\nand the associated values. Evaluations were conducted for\neach set of stimuli using the same procedure as follows:\n1. The subjects listened to the stimuli through headphones\nin a soundproof chamber and the volume was ï¬xed for\nall the subjects.\n2. For training, the subjects evaluated 20 stimuli selected\nat random.\n3. The subjects evaluated 100 stimuli for three times.\nThe stimuli were selected so that each stimulus was\nevaluated by almost the same number of subjects. The\nstimuli used in the training phase were excluded.\n4. After the evaluation, the subjects ï¬lled in a question-\nnaire about the vocal features they felt relevant to en-\nthusiasm.\nAfter the evaluation, one stimulus had 30 to 36 evaluation\nvalues given by 10 or 12 subjects. We took the average of\nall evaluation values, and the average was regarded as the\nresult of the subjective evaluation for that stimulus.\n3.3 Results\nIn order to investigate whether the subjects perceived singing\nenthusiasm consistently, we examined the correlation be-\ntween the evaluation values given by a subject and the aver-\nage of those given by all the other subjects.\nLetxsiâˆˆ {0,1,2}be an evaluation value for stimulus s\ngiven by the i-th subject. Let Â¯xsibe\nÂ¯xsi=1\nNsâˆ’1/summationdisplay\njÌ¸=ixsj (1)\nwhere Nsis the number of subjects who evaluated the stim-\nuluss. Then calculate Ïi, which is the correlation coefï¬cient\nbetween xsiandÂ¯xsiwith respect to s. IfÏiis high, it means\nthat the i-th subject evaluated the stimuli in the same way as\nthe other subjects. Note that we calculated Ïifor sets A and\nB independently, which are represented by ÏA\niandÏB\ni.\nFigure 2 is a histogram of ÏA\niandÏB\ni. This ï¬gure shows\nthat the correlation coefï¬cients are more than 0.7 for most\nof the subjects, so it is reasonable to suppose that the sub-\njects perceived singing enthusiasm consistently. We can\nalso observe that the correlation coefï¬cients for set B are\nhigher than those for the set A. This difference was caused\nby phrase-by-phrase differences in enthusiasm. Set A con-\ntained only one phrase, while set B had four phrases taken\nfrom different parts of the song. Different parts of the song\nhad different enthusiasm; for example, phrase B1 (the ï¬rst\npart) had smaller subjective evaluation values than phrase\nB3 (the hook line), which matches our intuition.\nNext, we investigated the relationship between â€œintended\nenthusiasmâ€ and â€œperceived enthusiasm.â€ In this experi-\nment, we asked singers to sing the song with two degrees\nFigure 2 . Correlation coefï¬cients of the evaluations by the\nnumber of subjects\n(a) set A\n (b) set B\nFigure 3 . Average of subjective evaluation for different\nsinging styles (the error bars represent the standard devia-\ntion)\nof enthusiasm: â€œenthusiasticâ€ and â€œnormalâ€, to see whether\nthis â€œintended enthusiasmâ€ could actually be perceived by\nthe subjects or not. To answer this question, we calculated\nthe average of subjective evaluation values for the two â€œin-\ntended enthusiasmâ€ sets. The results are shown in Figure 3.\nThe paired Wilcoxon-signed rank test revealed signiï¬cant\ndifferences ( p<0.01) for both sets A and B, indicating that\nthe subjects could distinguish the â€œintended enthusiasmâ€ by\nlistening to the voice.\nFinally, we asked the subjects to describe the features\nof the singing voice that they felt were relevant to the per-\nception of enthusiasm. Table 2 summarizes the features re-\nported by the subjects. As the goal of this questionnaire was\nto identify acoustic features for automatically evaluating en-\nthusiasm, we excluded opinions that were not related to the\nacoustic aspect of singing.\n4. ACOUSTIC PARAMETERS\nWe examined several acoustic features for automatically eval-\nuating enthusiasm based on the results of the questionnaire.\nThe fundamental frequencies (F0) were extracted at 10-ms\nintervals using The Snack Sound Toolkit [4], and converted\ninto log-scale (cent scale).\n33Oral Session 1: Melody and Singing\nloud voice\nstrong attack\nsudden rise in loud voice\nloud voice on high notes\narticulated dynamics\nstrong articulation of each note\nscooping up the pitch at the beginning\npitched on key\npitched higher than the correct note\nstable pitch\nvoice with vibrato\nenthusiastic forceful voice\nshouting voice\nbright voice\nhoarse voice\nkeeping forced voice until just before the release\nclearly pronounced lyrics\narticulated consonants\nstrong breath sounds\nportamento\nsome improvisation of rhythm\nsome improvisation of melody\ngetting into the rhythm\nsoft voice\nmonotonous voice\npitched clearly off key\npitched lower than the correct note\nnot forceless voice\nenthusiastic dark voice\nmufï¬‚ed voice\nbreathy voice\nreleased in short\nnot getting into the rhythm\nTable 2 . Factors relevant to enthusiasm listed in the ques-\ntionnaires\n4.1 Examined Features\nFirst, we focused on the loudness of the voice. Some sub-\njects reported that they felt the â€œloud voiceâ€ was more enthu-\nsiastic, although all the stimuli were normalized to the same\npower. We guessed that this happened because the stimuli\nhad different loudness levels. As the loudness depends not\nonly on the power of the signal but also on its frequency, the\nâ€œloud voiceâ€ might have larger loudness even though the\nphysical power of all stimuli were equal. To investigate the\nrelationship between loudness and enthusiasm, we calculate\nthe A-weighted power of the stimuli, and examined a cor-\nrelation between the A-weighted power and the enthusiasm.\nWe used the A-weighted power instead of the loudness be-\ncause it can be calculated more easily, and is widely used in\nacoustic measurements such as sound level meters. We de-\nsigned an FIR ï¬lter which implements the A-weighting [6]\nshown in Figure 4, and calculated the power of the signals\nin dB after applying the ï¬lter.\nSecond, we focused on the change of power. There were\nseveral opinions on the change of sound power, such as\nâ€œstrong attackâ€ or â€œstrong articulation of each note.â€ We\nexamined the ï¬rst derivatives of sound power ( âˆ†power) of\nFigure 4 . A-weighting curve\nthe voices as a physical feature expressing change of sound\npower, and took the maximum values for the feature. The\nâˆ†power was computed by\nâˆ†P(n) =/braceleftBiggn0/summationdisplay\nk=âˆ’n0P(n+k)k/bracerightBigg\n/n0/summationdisplay\nk=âˆ’n0k2(2)\nwhere P(n)is the power at the n-th frame and n0is the\nnumber of side frames. The conditions were decided by\nthe preliminary experiment: the frame size was 20 ms, the\nframe shift was 10 ms and the number of side frames was 4.\nThird, we examined features related to F0 change at the\nbeginning or end of a phrase. From the questionnaire, opin-\nions concerning F0 change were observed such as â€œscoop-\nupâ€ pitch at the beginning of phrases. Figure 5 shows an\nexample of F0 with scoop-up and fall-down. Observing F0s\nof recorded voices, we found some of them were scoop-\ning up at the beginning, and some were falling-down at the\nend of phrases. The durations were within about 250 ms\nfor both, and the frequency extent was under about 2000\ncent for scoop-up, and under about 900 cent for fall-down.\nThese features were described by Mayor et al. [1] as kinds\nof singing expressions, however no researches have revealed\nthe relevance of the features to human perception of the\nsinging voice.\nAs an acoustic feature that expresses these kinds of F0\nchange, we calculated the root mean square error (RMSE)\nvalue of F0 in regions of a constant duration, using Eq. (3):\nERMS(ts, T) =/radicaltp/radicalvertex/radicalvertex/radicalbt1\nTts+Tâˆ’1/summationdisplay\nt=ts(Fmax(ts, T)âˆ’F0(t))2(3)\nFmax(ts, T) = max\n0â‰¤t<TF0(ts+t) (4)\nwhere F0(t)is the fundamental frequency of the t-th frame,\ntsis the beginning time of the calculation region, and Tis\nthe length of the region. The duration Twas 200 ms. Here,\na phrase is deï¬ned by a region not shorter than 500 ms with\n3412th International Society for Music Information Retrieval Conference (ISMIR 2011)\ncontinuous F0. We calculate two RMSE values correspond-\ning to scoop-up and fall-down:\nEup=ERMS(tS, T) (5)\nEdown=ERMS(tEâˆ’T, T) (6)\nwhere tSandtEare the beginning and end of the phrase,\nrespectively.\nFinally, we examined vibrato-related features. Vibrato\nis one of the most basic features of the singing voice, and\nmany studies have revealed its acoustic features. The results\nof the questionnaire suggested that vibrato is an important\nfactor relevant to human perception of enthusiasm.\nTo detect vibrato, we computed â€œvibrato likelinessâ€ pro-\nposed by Nakano et al. [2] Short-time Fourier transforma-\ntion with a 32-point (320 ms) hanning window was applied\ntoâˆ†F0(t)which is the ï¬rst-order ï¬nite differential of F0(t).\nThe amplitude spectrum X(f, t)is expected to have a\nsharp peak range in the vibrato rate. Vibrato likeliness Pv(t)\nis deï¬ned by Eq. (9) using the power Î¨v(t)and the sharp-\nnessSv(t).\nÎ¨v(t) =RH/summationdisplay\nf=RLË†X(f, t) (7)\nSv(t) =RH/summationdisplay\nf=RL|âˆ†fË†X(f, t)| (8)\nPv(t) = Î¨ v(t)Sv(t) (9)\nwhere Ë†X(f, t)isX(f, t)normalized over f, andâˆ†fË†X(f, t)\nis the ï¬rst-order derivative of Ë†X(f, t)with respect to f.RL\nandRHare 5 and 8 Hz, respectively. Then we detect vibrato\nwhen Pv(t)is higher than a threshold and F0(t)crosses its\nregression line more than ï¬ve times, as shown in Figure 6.\nWe derived three parameters of vibrato: (1) the rate Vr\n[Hz], (2) the extent Ve[cent], and (3) the ratio of time with\nvibrato in all the vocal regions Vtcalculated as follows:\nVr=1\nNN/summationdisplay\ni=11\n2ri(10)\nVe=1\nNN/summationdisplay\ni=1ei (11)\nVt=1\ntF0N/summationdisplay\ni=1ri (12)\nwhere N,ri, and eiare as shown in Figure 6 and tF0is the\ntotal time of detected F0. However, if ( Vr<5orVr>8) or\n(Ve<30orVe>150), the values were discarded because\nsuch values are likely to be caused by ï¬ne F0 ï¬‚uctuation or\nanalysis error. Note that the three vibrato parameters are 0\nfor voices when no vibrato is detected.\nFigure 5 . An example of scoop-up and fall-down\nFigure 6 . Calculation of vibrato-related feature\n4.2 Results\nAs an evaluation of acoustic features, we calculated the cor-\nrelation coefï¬cient between individual features and the aver-\nage human evaluation of enthusiasm. The results are shown\nin Table 3. From these results, we picked up three features\nthat had relatively high correlations for both sets A and B:\nA-weighted power, Edown, and Ve.\nThe maximum âˆ†power and Eupgave only low correla-\ntion for set B. All of the three vibrato-related features gave\nrelatively high correlation because the correlation between\nthese three features are high (from 0.70 to 0.88), therefore\nwe chose only one of these features.\nThe A-weighted power gave the best correlation among\nthe examined features. From our observation, the A-weighted\npower seemed to be related to the quality of voice. The voice\nwith high A-weighted power did not only sounded louder\nbut also gave a clear and rich impression. The A-weight\nampliï¬es the frequency range around 3 kHz, which coin-\ncides with the frequency of the singing formant [5]. The A-\nweighted power and existence of the singing formant may\nbe related, but the singing formant was not necessarily ob-\nserved clearly in the voice even when the voice had high\nA-weighted power.\n5. SINGING ENTHUSIASM EVALUATION SYSTEM\n5.1 System Overview\nBased on the observations described in the previous section,\nwe constructed the Singing Enthusiasm Evaluation System\n35Oral Session 1: Melody and Singing\nSet A Set B B1 B2 B3 B4\nA-weighted power 0.47 0.54 0.36 0.50 0.51 0.49\nMax.âˆ†power 0.23 -0.22 0.05 0.13 -0.10 -0.09\nEup 0.20 0.07 -0.09 0.21 0.14 -0.12\nEdown 0.35 0.36 0.13 0.38 0.29 0.50\nVibrato time Vt 0.37 0.30 0.42 0.25 0.30 0.36\nVibrato extent Ve 0.37 0.37 0.38 0.27 0.38 0.47\nVibrato rate Vr 0.37 0.37 0.39 0.29 0.38 0.47\nTable 3 . Correlation coefï¬cients between acoustic parame-\nters and subjective evaluations\n(SEES), as outlined in Figure 7. The SEES consists of three\nsubsystems: SEES front-end, core and back-end.\nThe SEES front-end consists of a high-pass ï¬lter for noise\nreduction, signal power normalizer, and F0 extractor. The\nSEES core is the main part of the system, and extracts the\nacoustic features: the A-weighted power, the RMSE for fall-\ndown and the vibrato extent. The SEES back-end is the\npart where ï¬nal evaluation values are computed by linear\nsum features. The multiplier coefï¬cients correspond to the\nweights of the features and they must be determined before-\nhand. In our experiment, the coefï¬cients were determined\nby a multiple linear regression analysis on set A using the\nsubjective evaluation values as the response variables and\nfeature values as the explanatory variables.\n5.2 Evaluation of the System\nFinally, we evaluate the system by comparing the systemâ€™s\noutput with the human evaluation values. Set A was used as\na training set for determining the multiplier coefï¬cient. We\nexamined both sets A and B for testing the system, which\ncorresponded with the closed test and open test, respectively.\nThe results are shown in Figure 8. The correlation coefï¬-\ncients between the system output and the human evaluation\nwere 0.60 for set A (closed test), and 0.65 for set B (open\ntest). We obtained good correlations not only for set A but\nalso for set B, so we consider the system will produce stable\nevaluations for various melodies and lyrics.\n6. CONCLUSIONS\nIn this paper we introduced â€œenthusiasmâ€ as an aspect of\nevaluating the singing voice for karaoke, and obtained the\nfollowing results by experiments.\nFirst, subjective evaluations revealed that humans per-\nceive singing enthusiasm almost consistently, and listeners\ncan distinguish whether singers are singing enthusiastically\nor not only by listening to the voice.\nSecond, questionnaires revealed three effective acoustic\nfeatures of voices: the A-weighted power, the RMSE for\nfall-down and the vibrato extent.\nFinally, we developed a singing enthusiasm evaluation\nFigure 7 . Overview of the SEES\n(a) input: set A (closed)\n (b) input: set B (open)\nFigure 8 . Comparison of SEES output and subjective eval-\nuations\nsystem using the three features and achieved correlation co-\nefï¬cients of more than 0.6 for unknown input.\nAs a future work, we need to evaluate our system us-\ning various inputs such as different songs that contain more\nvariations of key, tempo, and genre.\n7. REFERENCES\n[1] O. Mayor, J. Bonada, A. Loscos: â€œThe Singing Tu-\ntor: Expression Categorization and Segmentation of the\nSinging V oice,â€ Proc. AES Convention, 2006.\n[2] T. Nakano, M. Goto, Y . Hiraga: â€œAn automatic singing\nskill evaluation method for unknown melodies using\npitch interval accuracy and vibrato features,â€ Proc. In-\ntespeech, pp. 1706â€“1709, 2006.\n[3] K. R. Scherer, â€œWhich emotions can be induced by mu-\nsic? What are the underlying mechanisms? And how\ncan we measure them?,â€ J. New Music Research, vol.\n33, no. 3, pp. 239-251, 2004.\n[4] K. Sj Â¨olander: â€œThe Snack Sound Toolkit,â€\nhttp://www.speech.kth.se/snack/, 1997-2001.\n[5] J. Sundberg: â€œArticulatory interpretation of the â€˜singing\nformantâ€™,â€ J. Acoust. Soc. Am., vol. 55, no. 4, pp. 838â€“\n844, 1974.\n[6] Int. Electrotechnical Commission, â€œElectroacoustics â€“\nSound level metersâ€“ Part 1: Speciï¬cations,â€ IEC 61672-\n1, 2002.\n36"
    },
    {
        "title": "Musical Moods: A Mass Participation Experiment for Affective Classification of Music.",
        "author": [
            "Sam Davies",
            "Penelope Allen",
            "Mark Mann",
            "Trevor J. Cox"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1415920",
        "url": "https://doi.org/10.5281/zenodo.1415920",
        "ee": "https://zenodo.org/records/1415920/files/DaviesAMC11.pdf",
        "abstract": "In this paper we present our mass participation experiment, Musical Moods. This experiment placed 144 theme tunes online, taken from TV and radio programmes from the last 60 years of the British Broadcasting Corporations (BBC) output.  Members of the public were then invited to audition then rate these according to a set of semantic differentials based on the affective categories of evaluation, potency and activity.  Participants were also asked to rate their familiarity of the theme tune and how much they liked the theme tune.  A final question asked participants to identify the genre of the TV programme with which they associated the tune.  The purpose of this is to aid in the affective classification of large-scale TV archives, such as those possessed by the BBC.  We find correlations between evaluation and potency, potency and activity but none between activity and evaluation but no clear correlation between affect and genre.  This paper presents our key findings from an analysis of the results along with our plans for further analysis. The initial results from this experiment are based on an analyses of over 51,000 answers from over 13,000 participants.",
        "zenodo_id": 1415920,
        "dblp_key": "conf/ismir/DaviesAMC11",
        "keywords": [
            "mass participation experiment",
            "Musical Moods",
            "144 theme tunes",
            "publicly available",
            "ratings and feedback",
            "semantic differentials",
            "evaluation and potency",
            "activity and familiarity",
            "genre identification",
            "affective classification"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\n  \n \nMUSICAL MOODS: A MASS PARTICIPATION EXPERIMENT \nFOR AFFECTIVE CLASSIFICATION OF MUSIC \nSam Davies, Penelope Allen, Mark Mann Trevor Cox \nBBC Research \n& Development \n{firstname.surname}@bbc.co.uk  University of Salford \nt.j.cox@salford.ac.uk  \nABSTRACT \nIn this paper we present our mass participation exp eriment, \nMusical Moods. This experiment placed 144 theme tun es \nonline, taken from TV and radio programmes from the  last \n60 years of the British Broadcasting Corporations ( BBC) \noutput.  Members of the public were then invited to  audition \nthen rate these according to a set of semantic diff erentials \nbased on the affective categories of evaluation, po tency and \nactivity.  Participants were also asked to rate the ir familiar- \nity of the theme tune and how much they liked the t heme \ntune.  A final question asked participants to ident ify the \ngenre of the TV programme with which they associate d the \ntune.  The purpose of this is to aid in the affecti ve classifi- \ncation of large-scale TV archives, such as those po ssessed \nby the BBC.  We find correlations between evaluatio n and \npotency, potency and activity but none between acti vity and \nevaluation but no clear correlation between affect and \ngenre.  This paper presents our key findings from a n analy- \nsis of the results along with our plans for further  analysis.  \nThe initial results from this experiment are based on an \nanalyses of over 51,000 answers from over 13,000 pa rtici- \npants.   \n1.  INTRODUCTION \nMusic is an inherent part of nearly all broadcast p ro- \ngrammes (TV and radio) and is often used to heighte n the \naffective content of a scene or programme.  Program me \nmaking teams have access to vast â€˜production musicâ€™  librar- \nies, which provide detail of not only the music tra cks title \nand composer, but also keywords about the music whi ch \ndescribes itâ€™s mood. This production music is used as back- \ning music within a programme; providing an accompan i- \nment to a scene.  The British Broadcasting Corporat ion \n(BBC) provide internal access to a service called â€˜ Desktop \nJukeboxâ€™ a production music library which contains over \n38,000 production tracks along with a range of affe ctive de- \nscriptors such as â€˜confidentâ€™, â€˜brightâ€™ or â€˜sensiti veâ€™.  This is \nan invaluable tool for helping programme producers to \nchoose the right music as an accompaniment to a par ticular \nscene. Yet music is not just used as a background i n pro- \nductions.  Most programmes also contain a theme tun e â€“ a piece of music designed to be recognizable and iden tifiable \nto introduce the programme.  Generally especially c ommis- \nsioned, these pieces of music convey some idea of t he affec- \ntive content of the upcoming programme â€“ a precurso r to \nset the tone.  For example, in preparation for the coverage \nof the 2010 UK General Election coverage, the BBC \nbriefed the composer Blair-Oliphant that he should com- \npose music that was â€œserious, important and classyâ€  to re- \nflect the fact that â€œthis is likely to be a fairly historic elec- \ntionâ€[1]. \nIn this paper we present our mass participation ex-\nperiment Musical Moods that explored the link betwe en \ntheme tune and affect.  Members of the public were asked \nto listen to theme tunes spanning 60 years of the B BCs out- \nput from 1950  and across 10 different genres, and rate each \none on an affective differential scale.  In the exp eriment, \n144 theme tunes from 135 different programmes were made \navailable as some long running programmes had multi ple \ntheme tunes.  The breakdown of programmes, theme tu nes \nand responses by genre is shown in table 1.  \nEach participant listened to a maximum of five \ntheme tunes, chosen at random per experiment.  Afte r one \nmonth of the experiment being live, 13,183 particip ants had \nauditioned and ranked 51, 374 themes.   \n The aim of this experiment is to help develop \nautomatic systems to classify the BBC archive using  affec- \ntive metadata.  Currently all BBC programmes that a re \nlikely to be reused (either through re-broadcast or  as clips) \nhave manually created metadata, contents of which r ange \nfrom brief synopses to detailed shot listings.  Thi s results in \nLondon Classification (LonClass) database entry.  L on- \nClass, a Universal Decimal Classification extension  devel- \noped by the BBC, is designed specifically to give f actual \ninformation about a programme such as genre, shot t ype or \nrecording location. Some programmes also have more in-\ndepth analyses consisting of a full transcription a nd shot \nlisting.  However, this is a time and resource expe nsive \nprocess; a detailed analysis of a 30 minute program me can \ntake a professional archivist 8 to 9 hours. \nThe purpose of this manually generated metadata \nis to allow for professional reuse. Frame accurate metadata \nis designed to allow users such as producers, and r esearch- \ners to find stock shots such as landscapes or peopl e, key in- \nterviews or other clips. However as the BBC open up  their \n741Poster Session 6\n  \n \narchives, this level of detail or type of metadata may not be \nbest suited for non-professional users: viewers.  \n \nGenre Number of \nProgrammes \n(percentage \nof total) Number of \ntheme \ntunes  \n(percentage \nof total) Number of \nresults  \n(percentage \nof total) \nChildrenâ€™s 16 (11.8%) 19 (13.2%) 6836  \n(13.3 %) \nComedy 33 (24.4%) 33 (23.0%) 11786  \n(22.9%) \nDrama 38 (28.1 %) 40 (27.8%) 14204 \n(27.6%) \nEntertainment 21 (15.6%) 22 (12.3%) 7867 \n(15.3%) \nFactual 7  (5.2%) 8 ( 5.6%) 2769 \n(5.4%) \nLifestyle 8 (5.9%) 8 (5.6%) 2882 \n(5.6%) \nNews 7 (5.2%) 8 (5.6%) 2912 \n(5.7%) \nSoaps 3 (2.2%) 3 (2.1%) 1047 \n(2.0%) \nSports 2 (1.5%) 3(2.1%) 1071 \n(2.1%) \nTable 1.  Breakdown of theme tunes, genres and results re- \nceived.   \nBBC Information and Archives (BBC I&A), the \nsection of the BBC that archive programmes and crea te as- \nsociated metadata, periodically release digitised c ollections \nof programmes online to the UK viewing public [2]. These \ncollections are grouped by theme and have semantic meta- \ndata; programme title, original transmission date, contribu- \ntors and a brief synopsis; metadata similar to that  in Lon- \nClass.  This allows a user to accurately find what factual \ninformation is contained within a programme.  This method \nof indexing may not be suitable when a viewer is lo oking \nfor a programme for entertainment, not information.   Thus, \nsome form of semantic or affective metadata is requ ired. \n BBC Research and Development (R&D) are cur- \nrently investigating automatic classification techn iques [3].  \nThese aim to create semantic and affective metadata  from \nan archived programme through an analysis of the av ailable \naudio and video or a programme.  Current analysis t ech- \nniques focus on non-music audio â€“ speech and sound ef- \nfects.  The purpose of collecting metadata about th eme \ntunes is to extend this to begin investigating how well music \ncan aid automatic classification.  Theme tunes are used to \nidentify a programme; making it recognizable to the  audi- \nence and setting up affective expectations.   By af fectively analysing the theme tune, we hope to be able to aid  in an \naffective analysis of an entire programme.  \n In this paper we present our work as follows.  We \npresent an overview of the experiment and our metho dology \nis given in section two and an initial analysis of our results \nin section three.  We discuss these results in sect ion four \nand conclude and present our plans for the future i n section \nfive. \n2.  METHODOLOGY \nMusical Moods was an online experiment, accessible from \nthe URL www.musicalmoods.org.uk.  The experiment wa s \nlaunched as part of the British Science Association â€™s Na- \ntional Science and Engineering Week, a yearly event  in the \nUK with the aim of promoting participation in and t he un- \nderstanding of science in the UK.  The experiment w as also \nfeatured on the BBC television show, Bang Goes The The- \nory, a weekly science show with an average audience  of \naround 2.5 million.  \nOne of the key factors in this experiment was ease \nof use for participants.  As such, each participant  on hearing \na clip of a theme tune was asked to rate a theme tu ne on one \nof the possible six semantic differentials.  The cl ip was 15-\n20 seconds long, with the participants able to re-a udition \nthe theme tune if required.  These clips were edite d to en- \nsure they contained the main musical themes of the theme \ntune, and did not contain any lyrics which alluded to the TV \nshows content.  Participants were played five rando mly \nchosen theme tunes in each experiment.  Participant s could \ntake part in the experiment as many times as possib le and \nno record was made of how many times a participant took \npart.  Upon hearing a clip, the participants were a sked to \nrate each semantic differential on a discrete scale  of one to \nfive, with each scale extreme labelled with an oppo sing pair \nof adjectives.   \n2.1  Semantic Differentials \nOne of the key issues found with previous music and  affect \nresearch is the lack of a standard definition for s emantic \nscales.  Hevner was one of the first to attempt to define a \ntaxonomy for music and affect [4].  This creates ei ght \ngroups of adjectives arranged diametrically on a ci rcle.  \nAnother approach is to use a valence/arousal space similar \nto that defined in [5].  The semantic differentials  in this ex- \nperiment were based upon Osgoodâ€™s dimensional space ; a \nthree dimensional space incorporating Evaluation, P otency \nand Activity (EPA) [6].   Each of these allow mappi ngs be- \ntween different adjectives that have similar affect ive mean- \ning allowing for affective unification of them.  Ev aluation \nrelates to positive or negative feelings like happy  or sad and \naccounts for around 50% of affective meaning.  Pote ncy \nrelates to size or power, such as heavy or light an d activity \nrelates to amount of action. Potency and activity c ount for \napproximately the other 50% of semantic meaning, al though \n74212th International Society for Music Information Retrieval Conference (ISMIR 2011)\n  \n \nthere are 4 more minor categories. The affective ad jectives \nused in this experiment were taken from mapping adj ectives \nin [4] to the affective space in [6].  The semantic  differen- \ntials were happy/sad, playful/serious for evaluatio n, mascu- \nline/feminine and heavy/light for potency and dra- \nmatic/relaxing, exciting/calm for activity.  A scor e of one to \nfive was used for these, with five relating to a ma ximum \nvalue of happy, playful, masculine, heavy, dramatic  and ex- \nciting and one relating to a maximum score for the opposing \nadjectives although this numbering was not shown to  the \nuser.  The adjective scales used for each EPA diffe rential \nwere found to correlate to each other, as is shown in table1.  \nHere, a Pearson Correlation matrix shows the correl ations \nbetween the different semantic differentials.  Idea lly there \nwould be high correlation between semantic differen tials in \nthe same affective vector space (i.e. dramatic corr elating \nhighly with exciting .  Whilst this is true for eva luation and \nactivity, there is a weaker correlation for potency  (heavy \nand masculine), with masculine correlating more wit h dra- \nmatic then heavy.   \n \n Dramatic Happy Heavy Masculine Playful Exciting \nDramatic 1 -0.061  0.697  0.673  -0.3 50  0.81 2 \nHappy -0.061  1 -0.66 9 -0.0 60  0.90 2 0.40 4 \nHeavy 0.697 -0.669 1 0.620 -0.836 0.297 \nMasculine 0.67 4 -0.0 60  0.61 0 1 -0.261  0.57 4 \nPlayful -0.346  0.90 2 -0.83 6 -0.261  1 0.127  \nExciting 0.812 0.404 0.297 0.574 0.127 1 \n \nTable 2 . Pearson Correlation  for Affective Results \n2.2  Genre Identification \nParticipants were then asked to identify with which  genre \nthey associated the music clip.  The options for ge nres were \ntaken from an amended list to that on the BBC iPlay er ser- \nvice, an online programme catch-up facility availab le in the \nUK [7].  These were Childrenâ€™s, Comedy, Drama, Ente r- \ntainment, Factual, Lifestyle, Soaps, News and Sport .  The \npurpose of this was to look at the link between the me tune \nand genre â€“ looking to identify if genres were read ily identi- \nfiable from theme tunes and also if there was any l ink be- \ntween the affective score and perceived genre. \n2.3  Liked or Familiar \nParticipants were also asked either if they liked a  theme \ntune, on a scale of â€˜Yesâ€™, â€˜Noâ€™ and â€˜No Opinionâ€™ or  how \nfamiliar they were with a theme tune, on a scale of  â€˜Veryâ€™,  \nâ€˜Not Veryâ€™ and â€˜Sort ofâ€™.  A very familiar or liked  tune \nscored one and a unfamiliar or not very liked tune scoring \n1.  The purpose of asking these was to ascertain if  there was \nany link between familiarity and liking a theme tun e, with \nthe affective score given.  This was not done to ra nk theme tunes by popularity or find out which were the best  known \ntheme tunes. \n3.  RESULTS \nAfter over 51,000 results had been gathered a preli minary \nanalysis was performed.  Results were calculated by  collat- \ning all the scores for each of the 144 programmes.  The av- \nerage and standard deviation for each answer was th en cal- \nculated.  To calculate the EPA values, each semanti c differ- \nential for each of evaluation, potency and activity  was com- \nbined, with equal weighting.  The average and stand ard de- \nviation of each of these was then calculated. \n3.1  Participation \nOf the 13,183 participants who took part in the exp eriment,  \n54% of participants identified themselves as female  and \n46% male.  Age band results are shown in table 3. \n \nAge Band  Percentage \n< 16 44% \n16 â€“ 24 18% \n25-39 18% \n40-54 14% \n55-69 5% \n > 70 2% \nTable 3.  Age band breakdown of participants \n3.2  Affective Scores \nThese results looked at how different theme tunes w ere \nclassified according to the semantic differentials of evalua- \ntion, potency and activity.  These are shown in fig ures 2, 3 \nand 4 respectively.   \nFigure 2 shows the average evaluation score \nagainst the average potency.  Here, it can be quite  clearly \nseen that across all theme tune genres there is a s light nega- \ntive correlation of -0.2 between the potency of a t heme tune \nand the evaluation (shown as a line), meaning tunes  classi- \nfied as happy are also broadly classified as light.   Also, \nwhilst genres do spread over the range of the scale , there is \na tendency for theme tunes associated with children â€™s (â€˜+â€™) \nand comedy (â€˜oâ€™) programmes to rate higher on evalu ation \nand for those associated with dramatic (â€˜*â€™) progra mmes to \nrate higher on the potency scale. \n \nIn Figure 3, the average potency against the averag e activity \nis shown for each programme.  A positive correlatio n  of \n0.6 is shown between activity and potency meaning t heme \ntunes perceived as heavier are also perceived as mo re excit- \ning.  Also, whilst clustering is less visible than figure 2, \nthere is some, with theme tunes associated with dra ma (â€˜*â€™) \ntending to rate higher on potency and activity (i.e . more \ndramatic and heavy) with theme tunes accompanying c hil- \ndrenâ€™s and comedy programmes (â€˜+â€™ and â€˜oâ€™ respectiv ely) \ntending to be less so (i.e. slightly calmer and lig hter), \n743Poster Session 6\n  \n \nthough childrenâ€™s programming does tend towards bei ng \nmore exciting. \n \n \nFigure 2.  Potency and evaluation average classifications \n \n \nFigure 3.  Activity and potency average classifications \nFigure 4 shows no real correlation overall, but doe s show \nsome clearer grouping.  One of the most interesting  group- \nings shown is that for theme tunes accompanying new s and \ncurrent affairs programmes (circled on figure 4).  These can \nbe seen to cluster around the centre scale for eval uation, but \nwith a marked increase in activity meaning that the y are \nclassified as being neither happy nor sad but more dramatic. \n \n \nFigure 4.  Activity and evaluation average classifications 3.2.1  Standard Deviation \nIn all instances, it was found that the standard de viation was \nsignificantly lower at the extremes of each semanti c scale.  \nThis is shown in figure 5. \n \n \nFigure 5.  Standard deviation for semantic differentials \n3.3  Genre, liking and familiarity \nThese looked at the results from the familiarity, h ow much \nparticipants liked a theme tune and genre identific ation \nquestions. \nUnsurprisingly, when participants were more fa- \nmiliar with the theme tune, they were generally abl e to iden- \ntify the accompanying programmes genre.  The result s for \ncorrectly identified programmes are shown in figure  6. \n \n \nFigure 6.  Genre classifications. \n \nAn interesting result from this genre classificatio n is that \nparticipants did not seem to be able to correctly i dentify \nwhich genre a theme tune accompanied with only soap s be- \ning correctly identified more than 50% of the time.   Partici- \npants also had most trouble identifying lifestyle p ro- \ngrammes where only 35% of participants who stated t hey \nknew the theme tune being correct and 22% of those who \nstated they knew the theme tune incorrectly choosin g enter- \ntainment.  A further error was in factual genre ide ntification \n74412th International Society for Music Information Retrieval Conference (ISMIR 2011)\n  \n \nwhere 23% of participants who did not know the them e tune \nidentified the programme as factual and 24% as dram a. \nAs can be seen from figure 7, theme tunes which \nwere more familiar to people were also rated as bei ng more \nliked, with one key exception, the theme tune to th e pro- \ngramme the Weakest Link , where the theme tune is less fa- \nmiliar to participants, but liked.  Familiarity and  a value for \nliking was found to have a correlation coefficient of 0.69.  \nHowever, familiarity was not seen to have a marked differ- \nence on  the affective scores, with only a 7% diffe rence \nnoted between the scores of familiar and unfamiliar  theme \ntunes. \n \n \nFigure 7. Familiarity against Likedness for Programme \naverages \nAll results and music will be available for downloa d from \nwww.musicalmoods.org . \n4.  DISCUSSION \nFrom the results analysed so far some clear trends are visi- \nble.  As figure 2 shows, there is a negative correl ation be- \ntween potency and evaluation.  This suggests that t unes \nclassified as happy or playful were also classified  as light or \nfeminine.  When the genres of the programmes associ ated \nwith the theme tunes is taken into account, this sh ows that \nthese are generally genres which one would imagine as be- \ning happy or light â€“ mainly childrenâ€™s, comedy and lifestyle \nprogrammes.  This genre generally includes day-time  TV \nprogrammes about home improvements or gardening and  so \nagain this is expected.  However, no large scale af fective \nanalysis of the programmes has so far been conducte d and \nso no strong link can as yet be drawn.  At the othe r end of \nthe scale, where theme tunes are classified as bein g heavier \nand sadder, mainly dramas are found.  This would fi t in \nwith our understanding of drama programmes themselv es â€“ \nthat they generally feature â€˜heavierâ€™ and less happ y story- \nlines.  A brief analysis of the programmes by the a uthors \nwhose theme tunes were found to have the lowest eva lua- \ntion and highest potency scores found the overall a ffect of \nthe programmes (the detective shows  Silent Witness and  Ashes to Ashes ) found they also fitted into the same affec- \ntive space . Conversely the two programmes which were \nfound to have the highest evaluation scores and the  lowest \npotency scores were found to be both childrenâ€™s TV pro- \ngrammes; Blue Peter , a childrenâ€™s magazine programme \nand the Teletubbies , a show aimed at pre-school children.  \nFrom the authors analysis of these programmes, it c an be \nseen that programmes such as these which have theme  tunes \nwhich score at extremes of the affective scales the mselves \nwould score highly at these extremes too. \nIn figure 3 a clear correlation between activity an d po- \ntency is visible, meaning that theme tunes classifi ed as be- \ning more dramatic and exciting (activity) are also classified \nas being heavier and more masculine (potency).  How ever \nin this case much less grouping is observed.  Where as in \nfigure 2 it was possible to see groupings in the ge nres chil- \ndrenâ€™s and comedy and then a further group for dram atic \nprogrammes, here both childrenâ€™s, comedy and drama are \nspread along the scales.  Theme tunes to the genres  comedy \nand drama have classifications at both extremes of the scale.  \nThis is unsurprising when considering the affective  nature \nof programmes.  Genreâ€™s such as comedy and drama re ly far \nmore on affect for their programme substance than f act \nbased programmes such as factual or news which rely  more \non their informative content.  From this, it follow s that the \ntheme tunes to childrenâ€™s, comedy and drama program mes \nshould have more affective spread.  The correlation  be- \ntween activity and potency allows follows the findi ngs in \n[6], which found that there was a high degree inter change- \nability between them.  \nLooking at the results shown in figure 4 there is n o \ncorrelation shown between activity (dramatic or cal m) and \nevaluation (playful or serious) shown in TV theme t unes.  \nTherefore no link was found between theme tunes bei ng \nhappy or playful, and how dramatic or exciting they  were \nfound to be.  However it is possible to see groupin gs of the \ngenres of the programmes associated with the theme tunes \nmore clearly.  For example, childrenâ€™s, comedy and lifestyle \nand entertainment all tend to group towards the hig her end \nof the evaluation scale.  Conversely, theme tunes a ssociated \nwith drama tend towards the lower end of the evalua tion \nscale, indicating a more sad or serious classificat ion.  One \ninteresting cluster is that of theme tunes associat ed with \nprogrammes of the genre news, circled on figure 4.  These \ncluster around the centre of the evaluation axis, s uggesting \nthat in terms of happy or sad they are neutral.  Ho wever, \nthese have the largest standard deviation (with an average \nof 1.2) so it could be that with classifying these types of \ntheme tunes participants had the most trouble. \nWhat is most interesting in looking at the genres a sso- \nciated with the theme tunes is that whilst some cle ar group- \ning occurs, individual genres spread out over seman tic dif- \nferential scales.  From this, it is possible to con clude that \nusing these affective values are not an accurate me thod for \nclassifying genre i.e. childrenâ€™s programmes do not  all clas- \n745Poster Session 6\n  \n \nsify as happy.  This is further backed up by lookin g at the \nresults shown in figure 6.  Here, it can be seen th at, in gen- \neral, participants were not that accurate in associ ating gen- \nres and theme tunes.  Even when participants stated  they \nwere very familiar with the theme tune, the highest  percent- \nage of correct genre identification was only 57% (f or \nsoaps). The lowest score was for factual at 29% and  the av- \nerage correct identification rate was only 44%.  Wh en the \nresults for those who were not familiar with the th eme tune \nare taken into account, correct identification fall s even fur- \nther with the most correct classifications being fo r the News \ngenre, with 54% correct classifications and the low est for \nFactual with 23%.  The average was 37%.  This again  sug- \ngests that the genre of a programme and its associa ted \ntheme tunes affective value do not show a strong li nk.  This \nwould have important implications in the design of any \nclassification and recommendation system that looks  to use \ngenre and affect as a basis. \nOne of the problems with this could be the choice o f \ngenres that were made available.  These were based on \nthose offered by the BBC iPlayer service.  These ar e very \nbroad categories, with some ambiguity as to which g enres \nsome programmes belong too (for example between pro - \ngrammes in the  entertainment and lifestyle genre) and with \nmany programmes placed in multiple categories (for exam- \nple childrenâ€™s news programmes).  Further research is re- \nquired to identify what genre classification system  best \nmaps to affect. \nIn looking at figure 6, it is clear that there is a  positive \ncorrelation between the average value for liking a tune and \nfamiliarity with it.  This suggests that the more f amiliar \ntheme tunes were also more liked by participants.  Whilst \nthis in itself does not give any insight into how e ither liking \na theme tune or being familiar with one has an effe ct on \nhow participants affectively perceive a theme tune,  it does \nsuggest that when evaluating retrieval systems for pro- \ngramme archives, this correlation should be noted. One in- \nteresting outlier in this set is the theme tune to the enter- \ntainment programme The Weakest Link  â€“ a quiz show where \nthe familiarity is slightly below average but the t une has a \nhigh value for being liked.  Further analysis is re quired to \ndetermine possible causes for this. \n5.  CONCLUSION AND FURTHER WORK \nIn this paper we have presented our experiment Musi cal \nMoods and our first analysis of the results.  We ha ve found \nthat whilst some the genres of the TV programmes wi th \nwhich theme tunes are associated show some grouping  on \nthe affective scales, there is no real link between  a pro- \ngrammes genre and the perceived affective value of its \ntheme tune.  We have also found that theme tune alo ne is \nnot an accurate indication of programme genre.  A f urther \nfinding is there is strong correlation between fami liarity and \nliking a theme tune. Further work is also designed to musically analyse the \ntheme tunes and correlate these with the experiment  results.  \nIt is planned to perform a full musical analysis on  each track \nused â€“ looking at features including key, harmonic progres- \nsion, instrumentation and orchestration.  These wou ld then \nbe analysed against the affective scores for the th eme tunes.  \nThis would then be used as a ground truth dataset, looking \nto use automated musical analysis and machine learn ing \ntechniques to identify the affective content of oth er pro- \ngramme theme tunes.   \nAnother area of work is to affectively analyse the pro- \ngramme themselves, and look at any correlation betw een \nthe affective classification of the video and audio  content \nwith the theme tune.  This is planned through more large \nscale user evaluations. \nWe are also looking to get more participants to in-\ncrease the validity of our results. \n6.  ACKNOWLEDGMENT \nThis project was launched as part of the British Sc ience As- \nsociationâ€™s National Science and Engineering Week a nd the \nBBC R&Ds Multimedia Classification Project. \n7.  REFERENCES \n[1] K. Young, \"TV theme tunes set tone for general \nelection night,\" in BBC News , ed, 2010. \n[2] BBC. (2010, 17th July). BBC Archive Collections . \nAvailable: \nhttp://www.bbc.co.uk/archive/collections.shtml  \n[3] -, \"A Framework for Automatic Mood \nClassification of TV Programmes,\" in 5th \nInternational Conference on Semantic and Digital \nMedia Technologies Saarbrucken, Germany, 2010. \n[4] K. Hevner, \"Experimental Studies of the element s \nof expression in music,\" American Journal of \nPsychology, vol. 48, p. 246:268, 1936. \n[5] J. Russell, \"A Circumplex model of Affect,\" \nJournal of Personality and Social Psychology, vol. \n39, pp. 1161-1178, 1980. \n[6] C. E. Osgood, G. Suci, and P. Tannenbaum, The \nmeasurement of meaning . Urbana, USA: \nUniversity of Illinois Press, 1957. \n[7] BBC. (2010, 17th July). BBC iPlayer . Available: \nhttp://www.bbc.co.uk/iplayer/  \n746"
    },
    {
        "title": "Audio-based Music Classification with a Pretrained Convolutional Network.",
        "author": [
            "Sander Dieleman",
            "Philemon Brakel",
            "Benjamin Schrauwen"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1415188",
        "url": "https://doi.org/10.5281/zenodo.1415188",
        "ee": "https://zenodo.org/records/1415188/files/DielemanBS11.pdf",
        "abstract": "Recently the â€˜Million Song Datasetâ€™, containing audio features and metadata for one million songs, was made available. In this paper, we build a convolutional network that is then trained to perform artist recognition, genre recognition and key detection. The network is tailored to summarize the audio features over musically significant timescales. It is infeasible to train the network on all available data in a supervised fashion, so we use unsupervised pretraining to be able to harness the entire dataset: we train a convolutional deep belief network on all data, and then use the learnt parameters to initialize a convolutional multilayer perceptron with the same architecture. The MLP is then trained on a labeled subset of the data for each task. We also train the same MLP with randomly initialized weights. We find that our convolutional approach improves accuracy for the genre recognition and artist recognition tasks. Unsupervised pretraining improves convergence speed in all cases. For artist recognition it improves accuracy as well.",
        "zenodo_id": 1415188,
        "dblp_key": "conf/ismir/DielemanBS11",
        "keywords": [
            "convolutional network",
            "artist recognition",
            "genre recognition",
            "key detection",
            "unsupervised pretraining",
            "musically significant timescales",
            "labeled subset",
            "randomly initialized weights",
            "MLP",
            "accuracy"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nAUDIO-BASED MUSIC CLASSIFICATION WITH A PRETRAINED\nCONVOLUTIONAL NETWORK\nSander Dieleman, Phil Â´emon Brakel and Benjamin Schrauwen\nElectronics and Information Systems department, Ghent University\n{sander.dieleman, philemon.brakel, bschrauw }@elis.ugent.be\nABSTRACT\nRecently the â€˜Million Song Datasetâ€™, containing audio fea-\ntures and metadata for one million songs, was made avail-\nable. In this paper, we build a convolutional network that is\nthen trained to perform artist recognition, genre recognition\nand key detection. The network is tailored to summarize the\naudio features over musically signiï¬cant timescales. It is\ninfeasible to train the network on all available data in a su-\npervised fashion, so we use unsupervised pretraining to be\nable to harness the entire dataset: we train a convolutional\ndeep belief network on all data, and then use the learnt pa-\nrameters to initialize a convolutional multilayer perceptron\nwith the same architecture. The MLP is then trained on a\nlabeled subset of the data for each task. We also train the\nsame MLP with randomly initialized weights. We ï¬nd that\nour convolutional approach improves accuracy for the genre\nrecognition and artist recognition tasks. Unsupervised pre-\ntraining improves convergence speed in all cases. For artist\nrecognition it improves accuracy as well.\n1. INTRODUCTION\nRecently, the Laboratory for the Recognition and Organiza-\ntion of Speech and Audio (LabROSA)1of Columbia Uni-\nversity released a large dataset of music consisting of audio\nfeatures and metadata for one million songs, aptly named\nthe â€˜Million Song Datasetâ€™ [4].\nBecause the dataset is almost completely labeled, it lends\nitself well for developing and testing classiï¬cation methods.\nIn this paper, we attempt to classify songs according to their\ngenre, artist and key. To this end, we design a convolutional\nnetwork that summarizes the input features over musically\nsigniï¬cant timescales.\n1http://labrosa.ee.columbia.edu/\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proï¬t or commercial advantage and that copies\nbear this notice and the full citation on the ï¬rst page.\nc/circlecopyrt2011 International Society for Music Information Retrieval.Developing techniques that can harness the entire dataset\nis quite a challenge. We use the majority of the data in an\nunsupervised learning phase, where the network learns to\nmodel the audio features. Due to its size, the dataset is very\nsuitable for unsupervised learning. This is followed by a\nsupervised training phase, where only a small task-speciï¬c\nsubset of the dataset is used to train a discriminative model\nusing the same network. We have investigated the gains that\ncan be achieved by using a convolutional architecture, and\nthe additional gains that unsupervised pretraining can offer.\nThis paper is structured as follows: the layout of the\ndataset is detailed in Section 2. An introduction to convo-\nlutional deep belief networks (DBNs) follows in Section 3.\nSection 4 describes the classiï¬cation tasks that were used to\nevaluate the model. Section 5 provides an overview of our\napproach, and Section 6 describes our experimental setup.\nResults are given in Section 7.\n2. DATASET\n2.1 The Million Song Dataset\nThe Million Song Dataset is a collection of all the infor-\nmation that is available through The Echo Nest API2for\none million popular songs. This means that a lot of the\ndata was automatically derived from musical audio signals,\nwhich should be taken into account when it is used for learn-\ning. Metadata available includes artist and album informa-\ntion and the year of the performance. Musical information\nderived directly from the audio signal includes the key, the\nmode and the time signature. Next to this, some other de-\nrived features like â€œenergyâ€ and â€œdanceabilityâ€ and user-\nassigned tags are also available.\nThe audio features in the dataset were obtained by ï¬rst\ndividing each song into so-called segments. Segment bound-\naries roughly correspond to onsets of notes or other musical\nevents. For each segment, a feature vector consisting of 12\ntimbre and 12 chroma components was computed, as well\nas the maximal loudness within the segment.\nThe chroma features describe the pitch content of the mu-\nsic. Each of the 12 components corresponds to a pitch class\n2http://the.echonest.com/\n669Poster Session 6\n(ranging from CtoB). Their values indicate the relative\npresence of the pitches, with the most prominent one al-\nways having a value of 1. All components lie within the\ninterval [0,1]. The timbre features are the coefï¬cients of 12\nbasis functions which capture certain timbral characteristics\nlike brightness, ï¬‚atness and attack. They are unbounded and\nroughly centered around 0.\nUnfortunately, the automated methods used to build the\ndataset lead to the presence of a relatively large number of\nduplicate tracks. When the dataset is divided into a train and\na test set in a naive fashion, some examples might occur in\nboth subsets, which is undesirable. Luckily, the authors of\nthe dataset have published an extensive list of known dupli-\ncates. Using this list, over 78,000 tracks were removed.\n2.2 Beat-aligned Features\nAlthough the segmentation that was performed to compute\nthe audio features has its merits, we are more interested in\nbeat-aligned features such as those used in [3]. The beat\nis the basic unit of time in music. Chord progressions and\nchanges in musical texture tend to occur on the beat, and\nseeing as it is one of our goals to encode these characteristics\nin higher level features, it makes sense to use beat-aligned\nfeatures as a starting point.\nThe features from the dataset can be converted to beat-\naligned features using the rhythm information that is also\nsupplied. The segments are mapped to beats, and then the\nfeature vectors for all segments corresponding to the same\nbeat are averaged.\n3. CONVOLUTIONAL DEEP BELIEF NETWORKS\n3.1 Deep Learning\nA fairly recent trend in machine learning is the use of deep\narchitectures, with many layers of processing [1]. Tradition-\nally, such architectures were not very popular because they\nwere very difï¬cult to train. In 2006, Hinton demonstrated\na fast training method for deep belief networks (DBNs), a\nparticular type of deep models [11]. This led to a surge in\npopularity of these models, establishing deep learning as a\nnew area of research.\nThe popularity of deep architectures can be attributed at\nleast partially to their biological plausibility; humans typi-\ncally use hierarchies and abstractions to organize their thoughts\nand evidence of hierarchical structures has been found in the\nbrain (e.g. in the visual cortex [1]).\nDeep belief networks are probabilistic generative mod-\nels, which are obtained by stacking multiple restricted Boltz-\nmann machines (RBMs) on top of eachother.3.2 Restricted Boltzmann Machines\nA restricted Boltzmann machine is a probabilistic model\nconsisting of a set of visible units and a set of hidden units\nwhich form a bipartite graph; there are no connections be-\ntween pairs of visible units or pairs of hidden units, but ev-\nery visible unit is connected to every hidden unit. They are a\nkind of undirected graphical model. A schematic represen-\ntation is shown in Figure 1.\nThe visible units of an RBM correspond to the input vari-\nables of the data that is to be modelled. In image processing,\neach visible unit typically represents one pixel. The hidden\nunits capture correlations between visible units and can be\nseen as feature detectors . The model learns the underlying\ndistribution of the data by representing it in terms of features\nthat are derived from the data itself.\nEach connection has a particular weight, and each of the\nunits can also have a bias. These trainable parameters can be\nlearnt from data. Unfortunately, maximum likelihood learn-\ning is intractable in RBMs. Instead, the contrastive diver-\ngence learning rule, which is an approximation to maximum\nlikelihood learning, can be used [9].\nFigure 1 . Schematic representation of an RBM, with the\nvisible units at the bottom and the hidden units at the top.\nNote how there are no lateral connections between two vis-\nible or two hidden units.\nRBMs typically consist of binary units, which can be on\nor off. This makes sense for the hidden units, which are\nfeature detectors, but it is not always the best choice for the\nvisible units. It is also possible to construct an RBM for\ncontinuous data, with Gaussian visible units.\n3.3 Deep Belief Networks\nA deep belief network (DBN) consists of multiple RBMs\nstacked on top of eachother, with the hidden units of RBM i\nbeing used as visible units of RBM i+ 1. The bottom RBM\nlearns a shallow model of the data. The next one then learns\nto model the hidden units of the ï¬rst, and so on: higher-level\nfeatures are extracted from lower-level features. Each RBM\nis trained separately; learning would be considerably harder\nif all layers would be trained jointly using backpropagation.\nTop-level features learnt by DBNs can be used to train\ndiscriminative models. In this fashion, they have been ap-\nplied succesfully to image processing problems like hand-\n67012th International Society for Music Information Retrieval Conference (ISMIR 2011)\nwriting recognition [11] and object recognition [12], but\nalso to classiï¬cation of audio signals [14], and even music\nclassiï¬cation [8]. For a detailed technical overview of deep\nlearning, RBMs and DBNs, see [1].\n3.4 Convolutional Networks\nA convolutional networks is a type of network model with\nconstrained weights. There are two kinds of constraints:\nâ€¢locality: each unit in layer iis only connected to a group\nof units in layer iâˆ’1that is local;\nâ€¢translation invariance: each unit in layer iis replicated\nsuch that every local group of units in layer iâˆ’1is con-\nnected to a unit in layer iwith the same weight conï¬gu-\nration ( weight sharing ). A set of units in layer iwith the\nsame weight conï¬guration is called a feature map .\nThis conï¬guration is visualized in Figure 2.\nIn layered network models, we typically wish for higher\nlayers to represent higher levels of abstraction. Weight con-\nstraints in convolutional networks would make it hard for\nneurons in higher layers to learn high-level abstractions; they\nonly see a small local portion of the input, whereas high-\nlevel abstractions usually involve long-range dependencies.\nTo increase the scope of higher layer neurons, convolutional\nlayers are alternated with max-pooling layers.\nMax-pooling is a downsampling operation: units in layer\niare grouped into small non-overlapping blocks. Each block\nis aggregated into a single unit in layer i+ 1, with as its\nactivation the maximal activation over all units in the block.\nThis operation reduces the dimensionality of the data by a\nfactor equal to the size of the blocks. This layout is also\nshown in Figure 2.\nItâ€™s clear that inserting max-pooling layers between con-\nvolutional layers increases the scope of higher layer neu-\nrons. Furthermore, it also makes the model invariant to\nsome small displacements of the input data, increasing its\nrobustness.\nlayer i+ 1\nlayer i\nlayer iâˆ’1\nFigure 2 . A max-pooling layer ( i+ 1) stacked on top of a\nconvolutional layer ( i). Note that layer iâˆ’1and layer iare\nnot fully connected. The connections are drawn in different\nstyles to indicate which weights are shared.\nConvolutional networks are typically used for image pro-\ncessing, where stronger correlations between nearby pix-els and the translation invariance of image features are ex-\nploited to signiï¬cantly reduce the number of parameters.\nAudio signals have similar characteristics, although the lo-\ncality is temporal rather than spatial.\nDeep belief networks can be made convolutional by ap-\nplying the described weight constraints in the RBM layers,\nand inserting max-pooling layers between the RBM layers.\nConvolutional deep belief networks have been have been\nused for object recognition [13, 16], and to extract features\nfrom audio signals, for speech recognition as well as for mu-\nsic classiï¬cation [14].\n3.5 Supervised Finetuning\nAs mentioned earlier, we can use top-level DBN features\nas input for a classiï¬cation method; common choices are\nsupport vector machines or logistic regression. We can train\na logistic regression classiï¬er by gradient descent, using the\nDBN to preprocess the input data.\nIt is also possible to convert a DBN into a convolutional\nmultilayer perceptron (MLP). We can simply reuse the weights\nof the interconnections and the biases of the hidden units.\nWe then stack a logistic regression layer on top of this MLP\nand train the whole model jointly using gradient descent.\nThis approach is called supervised ï¬netuning : the DBN weights\nthat were initially learnt to model the data are now ï¬netuned\nfor a speciï¬c discriminative task using backpropagation.\n4. TASKS\nWe performed several classiï¬cation tasks on music tracks:\nartist recognition, genre recognition and key detection. La-\nbeled datasets for each of the tasks were extracted from the\nMillion Song Dataset. Three (partially overlapping) subsets\nwere selected:\nâ€¢artist recognition: the 50 artists with the most tracks in\nthe dataset were identiï¬ed, and 100 tracks of each artist\nwere selected (5000 tracks in total);\nâ€¢genre recognition: 20 common genres were selected man-\nually using tags3that are included in the dataset: folk,\npunk, metal, jazz, country, blues, classical, rnb, new wave,\nworld, soul, latin, dance, reggae, techno, funk, rap, hip\nhop, rock and pop. For each genre, 250 tracks were se-\nlected (5000 tracks in total);\nâ€¢key detection: the key information in the dataset was au-\ntomatically annotated, so it may be unreliable. To avoid\nproblems with incorrect labels, we selected 250 tracks\nwith a high key conï¬dence for each of the 12 possible\nkeys (3000 tracks in total).\nThe subsets were then divided into balanced train, evalu-\nation and test sets according to a 80% / 10% / 10% split.\n3The dataset provides different kinds of tags. We used the MusicBrainz\ntags because these are the most reliable [4].\n671Poster Session 6\n5. APPROACH\nWe built a convolutional network, designed to aggregate the\nfeatures from the dataset on musically signiï¬cant timescales.\nProperties that are typical for certain genres, artists or keys,\nshould become apparent at this level. We used the same net-\nwork to tackle all three classiï¬cation tasks.\nThe network was ï¬rst trained as a DBN on the entire Mil-\nlion Song Dataset4. We then trained and evaluated the net-\nwork as an MLP with backpropagation, for each of the clas-\nsiï¬cation tasks. We used the Theano Python library to im-\nplement all experiments, so they could be GPU-accelerated\neasily [2].\n5.1 Network Layout\nThe input of the network consists of beat-aligned chroma\nand timbre features for a given track, so there are 24 input\ndimensions in total. The maximal loudness component was\nnot used, as the timbre features already include a loudness\ncomponent. Note that tracks vary considerably in length, but\nthe convolutional nature of the network allows us to cope\neasily with variable-length input.\nFirst, we separated the chroma and timbre features into\ntwo input layers (layers 0a and 0b). Then, separate convo-\nlutional layers were stacked onto both input layers (layers\n1a and 1b). These layers learn features with a width of 8\nbeats. It was observed that most of the tracks in the dataset\nhave a 4/4 time signature (which is also true for contempo-\nrary music in general). This means that there are 4 beats in\na bar. The width of the features was chosen to be two bars,\nseeing as this is the timescale on which chord progressions\nand changes in musical texture are most likely to occur. We\nused 100 feature maps for each layer.\nBy using separate layers, the network does not learn cor-\nrelations between chroma and timbre features at this level.\nThis allows it to focus on learning correlations between tim-\nbre components and between chroma components separately;\nsuch correlations are likely to be easier to discover. A simi-\nlar approach was used in [15] to learn features over multiple\nmodalities.\nThe output of the convolutional layers was then max-\npooled in the time dimension with a pool size of 4 (layers 2a\nand 2b). Once again, we made use of the observation that\nmost of the tracks in the dataset have a 4/4 time signature,\nwith 4 beats per bar; the output of the max-pooling layer is\ninvariant to all displacements of less than one bar (up to 3\nbeats).\nThe max-pooled outputs of both layers were then con-\ncatenated, yielding 200 features with a granularity of ap-\nproximately 1 bar. We stacked another convolutional layer\nwith 100 feature maps on top of this, which learns features\n4Excluding known duplicates and tracks used for validation and testing\nfor any of the tasks.with a width of 8 bars (layer 3). This width was selected\nbecause musical themes are often contained within a length\nof 8 bars. Correlations between timbre and chroma compo-\nnents can now be discovered as well.\nFinally, another max-pooling layer with a pool size of 4\nwas added (layer 4). The features obtained from this layer\nhave a granularity of 4 bars and a scope of roughly 8 bars.\nTo perform the classiï¬cation tasks, a ï¬fth layer perform-\ning logistic regression was added. To classify a track, each\ntimestep of the layer 4 is classiï¬ed separately, and the result-\ning posterior distributions over the class labels are averaged.\nThe most probable class is then selected. The layout of the\nnetwork is shown in Figure 3.\nbeatsbarsthemes5: logistic regression\n4: max-pooling\n3: convolution\n2a: max-pooling\n1a: convolution\n0a: chroma features2b: max-pooling\n1b: convolution\n0b: timbre features 12 12100 100100 100100100\nFigure 3 . The network layout. The number of dimensions\nor feature maps for each layer is indicated on the side. The\nlayers have also been grouped according to the timescale on\nwhich they operate.\n5.2 Unsupervised Pretraining\nIt would be impossible to train the network in a supervised\nfashion with the entire Million Song Dataset. This is this\ncomputationally infeasible, and on top of that the provided\nlabels are not perfect; some are missing, others are incorrect\nor have a very low conï¬dence.\nAs mentioned before, we pretrained the network using\ntimbre and chroma features for all tracks in the dataset. We\nused the beat-aligned chroma features directly as inputs to\nthe network; the timbre features were ï¬rst normalized per\ntrack to have zero mean and unit variance.\nTo train the RBM in layer 1b (timbre), we use Gaussian\nvisible units, which allow for the continuous input data to\nbe modeled. For layer 1a (chroma), we used binary units.\nTechnically, this is not possible because the chroma features\nare continuous values that lie between 0and1. However,\nwe can interpret these values as probabilities and sample\nfrom them, yielding binary input data. In practice, we do\nnot perform this sampling explicitly, but we use the mean\n67212th International Society for Music Information Retrieval Conference (ISMIR 2011)\nï¬eld approximation (see Section 5.2.2). Learning is much\nmore stable for binary units than for Gaussian units, so be-\ning able to use binary units is a signiï¬cant advantage.\nWe used single step constrastive divergence (CD-1) ev-\nerywhere. A learning rate of 0.005 was used to train the\nRBMs with binary visible units; a learning rate of 0.0001\nwas used for the RBM with Gaussian visible units. We per-\nformed only a single run through the entire dataset; perform-\ning multiple epochs turned out to be unnecessary (and would\nrequire too much computation time).\n5.2.1 Sparsity\nWe modiï¬ed the hidden unit activations according to [7] to\nencourage them to be sparse. Convolutional RBMs are over-\ncomplete models, so adding a sparsity penalty term ensures\nthat the learnt feature representations are useful [14]. In ad-\ndition, sparse activations are essential for max-pooling to\nwork properly [5, 17].\nWe used a sparsity target of 0.05for layers 1a and 1b,\nand a target of 0.1for layer 3. A relative sparsity cost of 0.1\nwas used in all cases.\n5.2.2 Mean Field Approximation\nWhere possible, we eliminated sampling steps by using the\nmean ï¬eld approximation. This eliminates sampling noise\nand often positively affects convergence. We used this for\nthe chroma inputs and in the contrastive divergence algo-\nrithm, except when updating the hidden states, as recom-\nmended in [10]. Interpreting continuous input values that\nare constrained to a ï¬nite interval as input probabilities to\ntrain an RBM is common practice [9].\n6. EXPERIMENTS\nWe trained the network as a convolutional MLP for each of\nthe classiï¬cation tasks described in Section 4: ï¬rst with ran-\ndom initialization of the weights, and then using the weights\nlearnt by the DBN (supervised ï¬netuning), yielding six ex-\nperiments. We tried learning rates of 0.05,0.005and0.0005\nand trained for 30 epochs. To initialize the random weights,\nwe sampled them from a Gaussian distribution with a mean\nand variance corresponding to those of the weights learnt by\nthe DBN. This ensures that the results are comparable.\nWe also trained a naive Bayes classiï¬er and a logistic re-\ngression classiï¬er that operate on windows of features from\nthe dataset, resulting in six more experiments. We chose a\nwindow size of 32 beats (8 bars), which is comparable to\nthe timescale on which the convolutional network operates.\nFor the logistic regression classiï¬er, we tried learning rates\nof0.005,0.0005 ,5Â·10âˆ’5,5Â·10âˆ’6and5Â·10âˆ’7and also\ntrained for 30 epochs. Both the chroma features and the tim-\nbre features were normalized to have a zero mean and a unit\nvariance in this case.For each of the twelve experiments, we determined the\noptimal parameters using the validation sets, and then com-\nputed the classiï¬cation accuracies on the test sets using these\nparameters. The results can be found in Table 1.\n7. RESULTS\nThe ï¬rst thing to notice is that the key detection task seems\nto be fairly simple. The achieved accuracies are much higher\nthan for the other tasks, and even the simplest technique per-\nforms quite well. Windowed logistic regression performs\nbest. There are multiple possible explanations for this:\nâ€¢the property we are trying to determine is quite â€˜low-\nlevelâ€™. The key of a track is in a very close relationship\nwith the chroma features and how they evolve through\ntime. Relating the genre or the artist to these features is\nmuch more difï¬cult;\nâ€¢to construct the dataset for this task, we selected tracks\nwith a high key conï¬dence. This implies that the al-\ngorithm used to annotate key information in the Million\nSong Dataset could identify the key of these tracks with\nrelative ease. It would make sense that the same is true\nfor our models. Unfortunately, there is no way to verify\nthis, except by constructing a manually labeled dataset.\nFor the other tasks, the convolutional network has a deï¬-\nnite edge over the other approaches: the classiï¬cation accu-\nracies increase signiï¬cantly.\nThe gains obtained with pretraining on the other hand\nseem to be much more modest; this is only advantageous for\nthe artist recognition task, which is quite difï¬cult because it\nis a 50-way classiï¬cation problem. The utility of pretraining\nfor this task could stem from the fact that the number of\ntracks per class available for training (80) is much lower\ncompared to the other tasks (200). Indeed, it has been shown\nthat gains from unsupervised pretraining are maximal when\nthe amount of available labeled training data is limited [6].\nThis data scarcity is inherent to the task at hand - few artists\nhave a discography with more than 100 tracks.\nThe optimal learning rate for key detection with the con-\nvolutional network differs depending on whether pretrain-\ning is used or not. This is because the training for this task\nwithout pretraining did not converge after 30 epochs using\na learning rate of 0.005. This indicates that convergence\nis faster when pretraining is used. To investigate this, we\nalso compared classiï¬cation accuracies obtained after only\n20 training epochs, which can be found in the bottom half\nof Table 1. We now observe that pretraining is beneï¬cial for\nall tasks. This conï¬rms that it improves convergence speed.\n8. CONCLUSION AND FUTURE WORK\nWe have trained a convolutional network on beat-aligned\ntimbre and chroma features obtained from music audio data\n673Poster Session 6\ngenre recognition artist recognition key detection\nnaive Bayes 10.02% 6.80% 73.74%\n30 epochswindowed logistic regression 25.90% ( 5Â·10âˆ’6)32.13% ( 5Â·10âˆ’5)86.53% (5Â·10âˆ’5)\nconv. MLP without pretraining 29.52% (0.005) 34.34% ( 0.05) 83.84% ( 0.05)\nconv. MLP with pretraining 29.12% ( 0.005) 35.74% (0.05) 83.84% ( 0.005)\n20 epochsconv. MLP without pretraining 24.90% ( 0.05) 33.94% ( 0.05) 83.84% ( 0.05)\nconv. MLP with pretraining 27.31% (0.005) 35.54% (0.05) 84.51% (0.005)\nTable 1 . Test accuracies and corresponding learning rates for each of the classiï¬cation tasks, with and without pretraining.\nto perform a number of classiï¬cation tasks. The convolu-\ntional nature of the network allowed us to summarize these\nfeatures over musically signiï¬cant timescales, leading to an\nincrease in accuracy. We used unsupervised pretraining with\na very large dataset, which improved convergence speed and,\nfor the artist recognition task, classiï¬cation accuracy. It is\nclear that the ability to harness a large amount of unlabeled\ndata is advantageous for tasks where the amount of available\ntraining data is limited.\nIn future work, we would like to reï¬ne a couple of as-\npects about the architecture of the network, such as the way\nthe input features are modeled in the lower layers: other\ntypes of visible units might be more suitable. We will also\ninvestigate different ways to encourage the RBMs to learn\ninteresting features, besides the sparsity penalty term that\nwe used for these experiments.\n9. REFERENCES\n[1] Yoshua Bengio. Learning deep architectures for AI. Technical\nreport, Dept. IRO, Universit Â´e de Montreal, 2007.\n[2] James Bergstra, Olivier Breuleux, Fr Â´edÂ´eric Bastien, Pas-\ncal Lamblin, Razvan Pascanu, Guillaume Desjardins, Joseph\nTurian, David Warde-Farley, and Yoshua Bengio. Theano: a\nCPU and GPU math expression compiler. In Proceedings of\nthe Python for Scientiï¬c Computing Conference (SciPy) , June\n2010. Oral.\n[3] T. Bertin-Mahieux, R. Weiss, and D. Ellis. Clustering beat-\nchroma patterns in a large music database. In Proceedings of\nthe 11th International Conference on Music Information Re-\ntrieval (ISMIR) , 2010.\n[4] Thierry Bertin-Mahieux, Daniel P.W. Ellis, Brian Whitman,\nand Paul Lamere. The million song dataset. In Proceedings of\nthe 11th International Conference on Music Information Re-\ntrieval (ISMIR 2011) , 2011. (submitted).\n[5] Y-Lan Boureau, Jean Ponce, and Yann Lecun. A theoretical\nanalysis of feature pooling in visual recognition. In 27th Inter-\nnational Conference on Machine Learning , 2010.\n[6] Dumitru Erhan, Pierre-Antoine Manzagol, Yoshua Bengio,\nSamy Bengio, and Pascal Vincent. The difï¬culty of training\ndeep architectures and the effect of unsupervised pre-training.\npages 153â€“160, April 2009.[7] Hanlin Goh, Nicolas Thome, and Matthieu Cord. Biasing re-\nstricted boltzmann machines to manipulate latent selectivity\nand sparsity. In Deep Learning and Unsupervised Feature\nLearning Workshop â€” NIPS , 2010.\n[8] Philippe Hamel and Douglas Eck. Learning features from mu-\nsic audio with deep belief networks networks. In Proceedings\nof the 11th International Conference on Music Information Re-\ntrieval (ISMIR) , 2010.\n[9] Geoffrey E. Hinton. Training products of experts by mini-\nmizing contrastive divergence. Neural Computation , 14:2002,\n2000.\n[10] Geoffrey E. Hinton. A practical guide to training restricted\nboltzmann machines. Technical report, University of Toronto,\n2010.\n[11] Geoffrey E. Hinton, Simon Osindero, and Yee-Whye Teh. A\nfast learning algorithm for deep belief nets. Neural Comput. ,\n18(7):1527â€“1554, 2006.\n[12] Alex Krizhevsky. Convolutional deep belief networks on cifar-\n10. Technical report, University of Toronto, 2010.\n[13] Honglak Lee, Roger Grosse, Rajesh Ranganath, and An-\ndrew Y . Ng. Convolutional deep belief networks for scalable\nunsupervised learning of hierarchical representations. In Pro-\nceedings of the 26th Annual International Conference on Ma-\nchine Learning , ICML â€™09, pages 609â€“616, New York, NY ,\nUSA, 2009. ACM.\n[14] Honglak Lee, Peter Pham, Yan Largman, and Andrew Ng. Un-\nsupervised feature learning for audio classiï¬cation using con-\nvolutional deep belief networks. In Y . Bengio, D. Schuurmans,\nJ. Lafferty, C. K. I. Williams, and A. Culotta, editors, Ad-\nvances in Neural Information Processing Systems 22 , pages\n1096â€“1104. 2009.\n[15] Jiquan Ngiam, Aditya Khosla, Mingyu Kim, Juhan Nam,\nHonglak Lee, and Andrew Y . Ng. Multimodal deep learning.\nInNIPS Workshop on Deep Learning and Unsupervised Fea-\nture Learning , 2010.\n[16] M. Norouzi, M. Ranjbar, and G. Mori. Stacks of convolutional\nrestricted boltzmann machines for shift-invariant feature learn-\ning. In Computer Vision and Pattern Recognition, 2009. CVPR\n2009. IEEE Conference on , pages 2735 â€“2742, 2009.\n[17] Dominik Scherer, Andreas M Â¨uller, and Sven Behnke. Evalu-\nation of pooling operations in convolutional architectures for\nobject recognition. In Proceedings of the 20th International\nConference on Artiï¬cial Neural Networks (ICANN) , 2010.\n674"
    },
    {
        "title": "The Temperament Police: The Truth, the Ground Truth, and Nothing but the Truth.",
        "author": [
            "Simon Dixon",
            "Dan Tidhar",
            "Emmanouil Benetos"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1418197",
        "url": "https://doi.org/10.5281/zenodo.1418197",
        "ee": "https://zenodo.org/records/1418197/files/DixonTB11.pdf",
        "abstract": "The tuning system of a keyboard instrument is chosen so that frequently used musical intervals sound as consonant as possible. Temperament refers to the compromise arising from the fact that not all intervals can be maximally consonant simultaneously. Recent work showed that it is possible to estimate temperament from audio recordings with no prior knowledge of the musical score, using a conservative (high precision, low recall) automatic transcription algorithm followed by frequency estimation using quadratic interpolation and bias correction from the log magnitude spectrum. In this paper we develop a harpsichord-specific transcription system to analyse over 500 recordings of solo harpsichord music for which the temperament is specified on the CD sleeve notes. We compare the measured temperaments with the annotations and discuss the differences between temperament as a theoretical construct and as a practical issue for professional performers and tuners. The implications are that ground truth is not always scientific truth, and that content-based analysis has an important role in the study of historical performance practice.",
        "zenodo_id": 1418197,
        "dblp_key": "conf/ismir/DixonTB11",
        "keywords": [
            "tuning system",
            "musical intervals",
            "consonance",
            "temperament compromise",
            "audio recordings",
            "automatic transcription",
            "frequency estimation",
            "log magnitude spectrum",
            "harpsichord-specific transcription",
            "historical performance practice"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nTHE TEMPERAMENT POLICE: THE TRUTH, THE GROUND TRUTH, AND\nNOTHING BUT THE TRUTH\nSimon Dixon1, Dan Tidhar2, and Emmanouil Benetos1\n1Centre for Digital Music, Queen Mary University of London\n2AHRC Research Centre for Musical Performance as Creative Practice, Kingâ€™s College London\n1{simond,emmanouilb }@eecs.qmul.ac.uk ,2dan.tidhar@kcl.ac.uk\nABSTRACT\nThe tuning system of a keyboard instrument is chosen so\nthat frequently used musical intervals sound as consonant\nas possible. Temperament refers to the compromise arising\nfrom the fact that not all intervals can be maximally con-\nsonant simultaneously. Recent work showed that it is pos-\nsible to estimate temperament from audio recordings with\nno prior knowledge of the musical score, using a conserva-\ntive (high precision, low recall) automatic transcription al-\ngorithm followed by frequency estimation using quadratic\ninterpolation and bias correction from the log magnitude\nspectrum. In this paper we develop a harpsichord-speciï¬c\ntranscription system to analyse over 500 recordings of solo\nharpsichord music for which the temperament is speciï¬ed\non the CD sleeve notes. We compare the measured temper-\naments with the annotations and discuss the differences be-\ntween temperament as a theoretical construct and as a prac-\ntical issue for professional performers and tuners. The im-\nplications are that ground truth is not always scientiï¬c truth,\nand that content-based analysis has an important role in the\nstudy of historical performance practice.\n1. INTRODUCTION\nRecent years have seen a renewed interest in keyboard tem-\nperament both in scholarly work [14] and in more popu-\nlar literature [9]. The modern tuning literature is abundant\nwith detailed speciï¬cations of hundreds of different key-\nboard temperaments; some are directly taken from historical\nmanuscripts and some are based on reconstruction or specu-\nlation [3,7]. A prescriptive approach taken by some scholars\nand performers regards adherence to speciï¬c temperaments\nas a desirable aim, and moreover, promotes the notion that\nfor particular styles or even particular pieces there exists the\nE. Benetos is funded by a Westï¬eld Trust research studentship (Queen\nMary University of London).\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proï¬t or commercial advantage and that copies\nbear this notice and the full citation on the ï¬rst page.\nÂ© 2011 International Society for Music Information Retrieval.â€œrightâ€ temperament [14]. An alternative approach, not less\ncommon amongst tuners and performers, is based on the\nview that since temperament is by deï¬nition a compromise,\nit is primarily a practical matter, and allows room for devia-\ntions from the underlying theoretical constructs. Rather than\nmistakes, such deviations are considered creative solutions\nto constraints arising from different instrument characteris-\ntics, inharmonicity, stylistic preferences, and the combina-\ntions of keys (tonalities) played in a concert programme.\nNot all harpsichord CD sleeve notes specify the temper-\nament, but when they do, there appears to be a tendency\ntoward the former, prescriptive, approach. It is therefore in-\ntriguing to analyse such recordings and explore their adher-\nence to the advertised temperaments. In this work, we anal-\nyse a dataset of over 500 harpsichord recordings for which\ntemperament information is speciï¬ed on the CD sleeve notes,\naiming to shed some light on the relation between tuning\ntheory and tuning practice, and more generally, on the na-\nture of human â€œground truthâ€ annotations. We extend recent\nwork demonstrating the feasibility of temperament estima-\ntion from solo harpsichord recordings [8,18]. The proposed\nsystem uses a conservative NMF-based automatic transcrip-\ntion algorithm followed by frequency estimation using quad-\nratic interpolation and bias correction. Multiple pitch es-\ntimates for each pitch class are combined with a median\nweighted by the pitch salience output of the transcription\nsystem. Results show signiï¬cant gaps between advertised\nand actual temperaments, which can be interpreted as evi-\ndence for the more pragmatic approach to tuning.\n2. BACKGROUND\n2.1 Temperament\nFor the last two centuries, the scales used in Western mu-\nsic have been built predominantly upon equal temperament.\nThis situation has been changing since the second half of the\ntwentieth century, as part of the revival of interest in histor-\nical performance practice of early music on period instru-\nments, resulting in increased attention to historical, unequal\ntemperaments. We give a brief introduction to temperament,\nreferring the reader to thorough treatments elsewhere [3, 7].\nExplanations of musical consonance are based on the fact\n281Poster Session 2\n61\n61\n61\n61\n61\n61\n656161616161F\nBb\nEb\nAb\nDbF#BEADGCSixth Comma Meantone\n51\n51\n51\n51\n51F\nBb\nEb\nG#\nC#F#BEADGCFifth CommaFigure 1 . Circle of ï¬fths representations for 2 temperaments\nused in this paper. The deviation of each ï¬fth from a pure\nï¬fth (the lighter cicle) is represented by the positions of the\ndarker segments. The fractions specify the distribution of\nthe comma between the ï¬fths (if omitted the ï¬fth is pure).\nthat listeners prefer sounds with harmonic spectra and with-\nout beats [17]. For combinations of harmonic tones, the sen-\nsation of consonance correlates to small integer frequency\nratios between fundamental frequencies, and particularly ra-\ntios of the formn+1\nnwherenâ‰¤5(corresponding to the\nfollowing pure intervals for successive values of n: octave,\nperfect ï¬fth, perfect fourth, major third and minor third).\nThe two most consonant intervals, the octave (ratio2\n1)\nand perfect ï¬fth (ratio3\n2) correspond to intervals of 12 and\n7 semitones respectively in Western music. From a given\nstarting note, either a succession of 7 octave steps or a suc-\ncession of 12 perfect ï¬fth steps will lead to the same note.\nHowever, (3\n2)12/negationslash= 27, so it is not possible for all of these\nintervals to be pure simultaneously. Temperament refers to\nthe various methods of adjusting some or all of the ï¬fth in-\ntervals (octaves are always kept pure) with the aim of re-\nducing the dissonance in the most commonly used intervals\nin a piece or programme of music. One way of represent-\ning temperament is by the distribution of the â€œPythagorean\ncommaâ€ (the ratio (3\n2)12: 27â‰ˆ1.0136 ) around the cycle\nof ï¬fths (see Figure 1). For example, equal temperament\ndiminishes all ï¬fths by1\n12of a comma relative to the pure\nratio 3:2. The other common way to represent temperament\nis by the frequency differences of each pitch class from their\nequal tempered counterparts, which is the representation we\nuse in our results and analysis.\nTheoretical models of temperament ignore the fact that\nstringed instruments are slightly inharmonic. This means\nthat a pure ï¬fth, maximally consonant when the 3rd partial\nof the lower tone coincides with the 2nd partial of the upper\ntone, will not correspond to a fundamental frequency ratio\nof3\n2, as the partials are not precisely at integer multiples of\nthe fundamental. We have shown [8] that this effect is of\nthe order of a fraction of a cent for the harpsichord, which is\nnegligible. The modelling of inharmonicity in the frequency\nestimation step is however important, and this is addressed\nin section 5.2.2 Precise Frequency Estimation\nDespite the vast literature on frequency and pitch detection\n(reviewed in [5, 12]), there is no general purpose method\nsuitable for all signals and applications. Many systems as-\nsume monophonicity, stationarity and/or harmonicity, none\nof which hold for polyphonic harpsichord music, and only\nfew papers address high-precision frequency estimation to a\nresolution of cents, which we require for the present work.\nThe highest precision is obtained using the FFT with quad-\nratic interpolation and correction of the bias due to the win-\ndow function [1], which outperforms instantaneous frequency\nestimation using phase information [18]. Given a local peak\napin the log magnitude spectrum log|X(n,p)|at framen,\nthat is,apâˆ’1< apandap> ap+1, then the three points\n(âˆ’1,apâˆ’1),(0,ap), and (1,ap+1)uniquely deï¬ne a parabola\nwith maximum at:\nÎ´=apâˆ’1âˆ’ap+1\n2(apâˆ’1âˆ’2ap+ap+1)(1)\nwhereâˆ’0.5â‰¤Î´â‰¤0.5is the fractional offset from the inte-\nger bin location p. This estimate is further reï¬ned using the\nfollowing formula for bias correction, based on the window\nshape and zero padding factor [1, equations 1 and 3]:\nÎ´/prime=Î´+Î¾zÎ´(Î´âˆ’0.5)(Î´+ 0.5) (2)\nwhereÎ´/primeis the bias-corrected offset in bin location, zis the\nzero-padding factor, Î¾z=c0zâˆ’2+c1zâˆ’4is the bias cor-\nrection factor and the constants c0= 0.124188 andc1=\n0.013752 were determined empirically for the Blackman-\nHarris window [1, table 1].\n3. DATA\nThe dataset used for this study consists of 526 tracks from\n22 CDs and the 48 tracks from [18]1. Generally, the CDs\npresent a rather balanced sample of recorded harpsichord\nmusic, including famous and less famous players, and a\nrange of composers including J. S. Bach, D. Scarlatti, F. Cou-\nperin, M. Locke, and J. P. Sweelinck. The CDs provide de-\ntails of the temperament used for the recordings. A few pro-\nvide details of the reference frequency as well (e.g. A = 415\nHz), but this is mostly not speciï¬ed. In some cases the\ntemperament information is precise and unambiguous, as\nin â€œWerckmeister IIIâ€ or â€œSixth comma meantone with the\nwolf between BandG/flatâ€. In other cases it is underspeciï¬ed,\nsuch as with â€œNeidhardt 1724â€, for which different versions\nexist both in the original manuscripts and in the secondary\nliterature, or with â€œQuarter comma meantoneâ€ where the\nwolf interval (i.e. the widened ï¬fth) is not speciï¬ed. Some\nunderspeciï¬cation can be resolved by convention: although\nâ€œmeantoneâ€ can refer to several different temperaments â€“\ne.g. quarter comma or ï¬fth comma meantone â€“ the normal\nuse of â€œmeantoneâ€ without any qualiï¬cation refers to quar-\nter comma meantone.\n1For details, see http://www.eecs.qmul.ac.uk/~simond/ismir11\n28212th International Society for Music Information Retrieval Conference (ISMIR 2011)\n4. TRANSCRIPTION\nOur pitch estimation algorithm in Section 5 assumes that\nthe existence and timing of each note is known. There-\nfore a transcription system for solo harpsichord was devel-\noped, using pre-extracted harpsichord templates, NMF with\nbeta-divergence [13] for multiple-F0 estimation, and hidden\nMarkov models (HMMs) [16] for note tracking. NMF with\nbeta-divergence is a computationally inexpensive multiple-\nF0 estimation method which has been used for piano tran-\nscription [6]. It has been shown to produce reliable results\nfor instrument-speciï¬c transcription, being highly ranked in\nthe MIREX 2010 piano-only note tracking task.\n4.1 Extracting Pitch Templates\nFirstly, spectral templates were extracted from three differ-\nent harpsichords, from the RWC musical instrument sounds\ndatabase [11]. For extracting the note templates, the constant-\nQ transform (CQT) was computed with spectral resolution\nof 120 bins per octave. The standard NMF algorithm [15]\nwith one component was employed for template extraction:\nVâ‰ˆwh, where VâˆˆRfÃ—nis the input CQT spectrum,\nwâˆˆRfÃ—1is the extracted spectral template, and hâˆˆR1Ã—n\nis the component gain (since only one component was set, it\ncorresponds to the frame energy).\nFor template extraction, the complete harpsichord note\nrange was used (F1 to F6). Thus, three spectral template\nmatrices were extracted, W(1),W(2),W(3)âˆˆRfÃ—61, corre-\nsponding to each harpsichord model.\n4.2 Multiple-F0 estimation\nFor the multiple-F0 estimation step, we used the NMF al-\ngorithm with beta-divergence [13]. The basic model is the\nsame as in the standard NMF algorithm: Vâ‰ˆWH, where\nWâˆˆRfÃ—r,HâˆˆRrÃ—n, andris the number of compo-\nnents. The beta-divergences (or Î²-divergences) are a para-\nmetric family of distortion functions which can be used in\nthe NMF cost function to inï¬‚uence the NMF update rules\nforWandH. Since in our case the spectral template matrix\nis ï¬xed, only the gains Hare updated as:\nhâ†hâŠ—WT((Wh)Î²âˆ’2âŠ—v)\nWT(Wh)Î²âˆ’1(3)\nwhere vâˆˆRfÃ—1is a single frame from the test signal and\nÎ²âˆˆRthe divergence parameter, set to 0.5 for this work,\nas in [6]. Although the update rule (Equation 3) does not\nensure convergence, non-negativity is ensured [6].\nFor the harpsichord transcription case, the spectral tem-\nplate matrix was created by concatenating the spectral tem-\nplates from all instrument models:\nW= [W(1)W(2)W(3)] (4)\nthus, WâˆˆRfÃ—183. After the NMF update rule was applied\nto the input log-spectrum V, the pitch activation matrix wascreated by summing the component vectors from Hthat cor-\nrespond to the same pitch p:\nH/prime\np,n=Hp,n+Hp+61,n+Hp+122,n (5)\n4.3 Note tracking\nInstead of simply thresholding the pitch activation H/primeas was\ndone in [6], additional postprocessing is applied in order to\nperform note smoothing and tracking. Here, the approach\nused in [4] was employed, where each pitch pis modeled by\na two-state HMM, denoting pitch activity/inactivity.\nThe hidden state sequence for each pitch is given by Qp=\n{qp[t]}. MIDI ï¬les from the RWC database [11] from the\nclassic and jazz subgenres were employed in order to esti-\nmate the state priors P(qp[1])and the state transition matrix\nP(qp[t]|qp[tâˆ’1])for each pitch p. For each pitch, the most\nlikely state sequence is given by:\nË†Qp= arg max\nqp[t]/productdisplay\ntP(qp[t]|qp[tâˆ’1])P(op[t]|qp[t])(6)\nwhich can be computed using the Viterbi algorithm [16].\nFor estimating the observation probability for each active\npitchP(op[t]|qp[t] = 1) , we use a sigmoid curve which has\nas input the pitch activation hp=H/primep,nfrom the output of\nthe transcription model:\nP(op[t]|qp[t] = 1) =1\n1 +eâˆ’(hpâˆ’Î»)(7)\nwhereÎ»is a parameter that controls the smoothing (a high\nvalue will discard pitch candidates with low energy). The\nresult of the HMM postprocessing step is a binary piano-\nroll transcription which can be used for evaluation.\nFor setting the parameter Î»for the harpsichord transcrip-\ntion experiments, we employed a training dataset consisting\nof the 7 harpsichord recordings present in the RWC classical\nmusic database [11]. As a ground truth for the recordings,\nthe syncRWC MIDI ï¬les were used2. Since for the present\nsystem a conservative transcription with high precision is fa-\nvorable,Î»was set to 0.25, which results in a false alarm rate\nof 5.33% with a missed detection rate of 46.49% (see [4]\nfor metric deï¬nitions). An example harpsichord transcrip-\ntion is shown in Figure 2, where the piano-roll transcription\nof recording RWC MDB-C-2001 No. 24b is seen along with\nits respective MIDI ground truth.\n5. PRECISE F0 ESTIMATION\nBased on the transcription results, we search for spectral\npeaks corresponding to the partials of each identiï¬ed note.\nFor identiï¬cation of the correct peaks, the tuning reference\nfrequency and inharmonicity of the tone also need to be es-\ntimated. For Baroque music, the tuning reference frequency\n(expressed as the fundamental frequency of the note A4) is\n2http://staff.aist.go.jp/m.goto/RWC-MDB/AIST-Annotation/SyncRWC\n283Poster Session 2\n(b)MIDIPitch(a)MIDIPitch\n1000 2000 3000 4000 5000 6000 7000 80001000 2000 3000 4000 5000 6000 7000 8000\n304050607080304050607080Figure 2 . (a) The piano-roll transcription of J.S. Bachâ€™s\nMenuet in G minor (RWC MDB-C-2001 No. 24b). (b) The\npitch ground truth of the same recording. Units on the ab-\nscissa correspond to 10ms.\nusually lower than the modern standard of 440 Hz. For our\ndata set, the CD sleeve notes mention reference frequencies\nof 405, 415 and 440 Hz, with the majority of CDs not giving\nany value. This introduces a problem: without knowing the\nscore (or at least the key) of a piece of music, it is not pos-\nsible to determine the reference frequency unambiguously,\nsince, for example, a note with F0 around 415 Hz could be\nA4 (reference 415 Hz) or G /sharp4 (reference 440 Hz).\nThe tuning frequency is ascertained by the following iter-\native process: 40 frames are selected (equally spaced through-\nout the piece) and the fundamental frequency estimation stage\ndescribed below is computed, using an initial value of 440\nHz for the tuning frequency and taking the inharmonicity es-\ntimates from measurements of other harpsichords [8]. The\nfrequencies are divided by their nominal values (given the\nreference frequency and assuming equal temperament), and\na weighted average of the deviations is computed. The ref-\nerence frequency is updated by the result and the process is\nrepeated for 5 iterations, or until it converges (the update is\nless than one cent) if sooner.\nThe inharmonicity of each note is estimated jointly with\nits fundamental frequency. For a string with (ideal) funda-\nmental frequency f0and inharmonicity constant B, the fre-\nquencyfkof thekth partial is given by [10]:\nfk=kf0/radicalbig\n1 +Bk2 (8)\nwhere the constants f0andBdepend on the physical prop-\nerties of the string. Given any two partials of a note, it is\npossible to solve for f0andB, assuming the partial num-\nbers are known. We compute these two parameters for each\npair of partials estimated below, and use a robust statistic,the median over all frames and partial pairs, to estimate the\ntrue values, using the inter-quartile range as an inverse mea-\nsure of conï¬dence in the estimates.\nThe fundamental frequency and inharmonicity of each\ntranscribed note are computed as follows:\n1) Compute the STFT using the following parameters: fs=\n44100 Hz, Blackman-Harris window with support size of\n4096 samples (93 ms), zero padding factor z= 4 (N=\n16384 ), and hop size of 1024 samples.\n2) For each note wgiven by the transcription, compute an\ninitial estimate of the frequency fw\nkof partialk= 1...40\nwith equation 8, using the reference frequency computed\nabove, the inharmonicity estimate from [8], and assuming\nequal temperament for the fundamental.\n3) For each partial frequency, a local spectral peak in a win-\ndow ofÂ±30cents around fw\nkis sought, and if found the\nfrequency estimate is reï¬ned as described in subsection 2.2.\n4) Using the transcription, any overlapping partials are iden-\ntiï¬ed and deleted from the estimate, as they are likely to give\nunreliable values. Partials are deemed to overlap if their fre-\nquency separation is less than 3.03fsz/N [2].\n5) For each pair of partials remaining, solve for F0 and B\nusing equation 8.\n6) For each pitch class k, convert each frequency estimate\nto cents deviation from equal temperament and return the\nweighted median Ë†ckas the overall tuning value for the pitch\nclass, where the weights are given by the pitch activation\nH/primep,n(Equation 5). This gives a 12-dimensional temper-\nament vector, which can be compared with the proï¬les of\nknown theoretical temperaments. For simplicity we repre-\nsent the pitch class kby an integer from 0 (C) to 11 (B),\ncorresponding to the MIDI pitch number modulo 12.\n6. TEMPERAMENT ESTIMATION\nOur temperament classiï¬er recognises the following tem-\nperaments: equal, ï¬fth comma, Vallotti, quarter comma mean-\ntone (QCMT), ï¬fth comma meantone (FCMT), sixth comma\nmeantone (SCMT), Kellner, Werckmeister III, Lehman, Nei-\ndhardt (1,2 and 3), Kirnberger (2 and 3) and just intonation.\nWe also recognise rotations of these temperaments, although\nthis is not a typical tuning practice for all temperaments, as\nillustrated by the example of the Young II temperament, a\nrotation of the Vallotti temperament, which is considered a\ndifferent temperament in its own right. Rotations are spec-\niï¬ed via the wolf interval where applicable (e.g. SCMT-FD\nhas wolf interval F /sharp-D/flat, as in Figure 1), otherwise by the\nnumber of semitones rotated (e.g. Vall+7).\nGiven the estimate Ë†c= (Ë†c0,..., Ë†c11)and a temperament\nproï¬leci= (ci\n0,...,ci\n11)for temperament i, we calculate\nthe divergence between estimate and proï¬le, d(Ë†c,ci):\nd(Ë†c,ci) =11/summationdisplay\nk=0uk(Ë†ckâˆ’ci\nkâˆ’r)2\n/summationtext11\nj=0uj(9)\nwhereuk=/summationtext\nn/summationtext\npâ‰¡kmod 12H/primep,nis the weight for pitch\n28412th International Society for Music Information Retrieval Conference (ISMIR 2011)\nclassk, andr=/summationtext11\nj=0ui(Ë†cjâˆ’ci\nj)//summationtext11\nj=0ujis the offset in\ncents which minimises the divergence and thus compensates\nfor deviations in the reference tuning frequency (pitch A4)\nfrom the reference computed above in previous calculations.\nA piece is classiï¬ed as having the temperament iwhose pro-\nï¬lecigives the least divergence d(Ë†c,ci). We also consider\nrotations of temperaments, ci,r, given byci,r\nk=ci\nm, where\nmâ‰¡(k+r) mod 12 , in order to deal with different posi-\ntions of the wolf interval in meantone temperaments, as well\nas the tuning ambiguity discussed in section 5.\n7. SUMMARY OF RESULTS\nThe results are summarised in Table 13. Column 1 is our\nCD index, where letters are used to distinguish groups of\ntracks with different temperament metadata. Column 2 shows\nthe annotated reference tuning, while the mean and stan-\ndard deviation of the estimated reference tuning are given in\ncolumns 3 and 4 respectively. Columns 5 to 8 give the an-\nnotated temperament, the average divergence d(Ë†c,ci)from\nthis temperament, the most frequent highest ranked temper-\nament according to d(Ë†c,ci), and the average difference in\ndivergence between the annotated temperament and the best\nranked temperament.\nThe results for tuning show agreement with the ground\ntruth values where they were available, with the exception\nof CD 21, which had only 2 tracks at 440 Hz. The CDs gen-\nerally show tuning consistency across all tracks, with high\nstandard deviations ( >2 Hz) being due to a bimodal dis-\ntribution of tuning frequency (CD 18) and 5 outlier tracks\n(CDs 2,7,19). Summarising by CD assumes ï¬xed tuning for\nall tracks, which is clearly not always the case.\nThe temperament results vary from close agreement to\nthe metadata (CDs 4,5,8,9,16,21,22) to moderate agreement\n(e.g. CDs 15, 18) to disagreement (e.g. CDs 12,13, 17). An\nexample is shown in Figure 3. For a number of tracks it was\nnot possible to ï¬nd a single â€œbest ï¬tâ€, as some temperaments\nare only distinguished by a pitch class which does not appear\n(or is not detected) within the piece. The large divergences\nof CDs 2 and 19 are explained by the tuning frequency being\nat the half-way point between two semitones relative to the\n440 Hz reference assumed by the transcription algorithm,\nmaking the transcriptions unreliable.\nOn CD 17 and some other tracks specifying QCMT, the\ntemperament was often closer to FCMT. This is an inter-\nesting tendency, as two are fairly similar, with FCMT be-\ning milder (slightly larger major thirds and a smaller wolf\ninterval). It seems plausible that QCMT was intended but\nthen tempered to bring it (inadvertantly) closer to the less\nextreme FCMT. However, the opposite tendency appears on\nCD 3a. Werckmeister 3 is speciï¬ed on ï¬ve CDs, but only\nfulï¬ls the claim on two. The reason may be that Werck-\nmeister 3 is popular as a starting point for tuners while they\nexperiment and develop their own temperaments, or that it\n3It is not possible to ï¬t all results into this paper. For more details,\nplease see: http://www.eecs.qmul.ac.uk/~simond/ismir11Tuning Temperament\nCD Not. Est. StD Notated Div. Estimated âˆ†Div.\n1 417.6 0.2 Ordinaire Neid2\n2 405 405.7 3.2 FCMT 21.8 Various 16.4\n3a 416.8 0.2 SCMT-BG 3.3 FCMT-BG 2.5\n3b 413.9 0.2 Kellner* 8.5 Various 1.2\n3c 414.2 0.2 Kellner 3.3 Kellner 0.0\n4b 416.9 0.3 FCMT-FD 1.1 FCMT-FD 0.0\n5 415 417.1 0.9 QCMT 1.4 QCMT-GE 0.0\n6 413.8 0.7 Late17 Vall+7\n7 432.6 4.8 FCMT 7.6 Various 4.1\n8b 416.8 0.4 QCMT 1.2 QCMT-GE 0.0\n9 415 415.3 0.3 Neid 1.1 Neid1/2 0.0\n10 415 416.5 0.4 Werck3 3.4 Various 1.7\n11 415 416.6 0.6 Werck3 3.0 Various 0.9\n12 415 415.3 0.2 Kirn3 11.1 Neid1 9.4\n13 415 415.1 0.3 Kirn3 7.3 Neid1 5.9\n14a (415) 412.7 0.3 QCMT 10.0 Various 7.0\n14c (415) 435.2 0.2 QCMT 2.7 QCMT-GE 0.0\n15 415.7 1.3 Werck3 3.4 Werck3 0.5\n16 416.1 1.1 Werck3 0.0 Werck3 0.9\n17 413.9 1.2 QCMT 6.0 FCMT 2.2\n18 440.5 2.4 QCMT 5.0 QCMT-GE 2.7\n19 440 447.6 5.6 QCMT 19.5 FCMT 15.2\n20 412.9 0.6 Werck3 2.6 Various 0.8\n21 414.5 1.6 FCMT 1.0 FCMT-GE 0.0\n22 408.7 0.3 Lehman 1.1 Lehman 0.1\nRH 415 415.5 0.8 Various 7.1 Various 0.3\nPT 415 415.6 0.7 Various 0.1 All correct 0.0\nTable 1 . Summary of results, with columns for CD number,\nnotated reference tuning, estimated reference tuning, stan-\ndard deviation across tracks of CD, notated temperament,\nhighest ranked temperament (Eqn 9), and average difference\nin divergence d(Ë†c,ci)between notated and highest ranked\ntemperaments. The last two rows refer to the data from [18].\nis very close to other temperaments such as Kellner (note\nthe low value of âˆ†Div in each case).\nSince we are claiming that CD sleeve notes are a ques-\ntionable source of â€œground truthâ€, we need an independent\nmeans of ascertaining the reliability of our system. The bot-\ntom row of Table 1 shows the results for 4 pieces recorded\nwith six different temperaments using the physical mod-\nelling synthesiser Pianoteq [18]. Using the current approach,\nthese tracks were all classiï¬ed correctly from the set of 180\npossible temperaments (15 temperaments by 12 rotations).\nConï¬dence in classiï¬cation results can also be gained by\nconsidering the divergence value and consistency of results\n(i.e. if a number of related tracks are classiï¬ed with the same\nlabel and low divergence from the given temperament).\n8. CONCLUSION\nWe have presented a method for analysing harpsichord tem-\nperament directly from audio recordings, using an NMF-\nbased transcription system, followed by bias-corrected quad-\nratically interpolated short-time spectral analysis to estimate\npartial frequencies, estimation of inharmonicity, deletion of\noverlapping partials, and robust statistics weighted by the\npitch salience given by the transcription system. We anal-\n285Poster Session 2\nCC#DD#EFF#GG#AA#Bâˆ’10010\nPitch classCents deviationFigure 3 . Estimated temperament proï¬le (solid line, cir-\ncles) compared with the temperament speciï¬ed on the CD\n(dot-dash) and that with least divergence from the estimate\n(dotted line, crosses). In this case the data matches the Val-\nlotti proï¬le (d = 2.2) more closely than the speciï¬ed Fifth\nComma Meantone (d = 17.1).\nysed a collection of CDs which provide metadata about the\ntuning system, and found that while this information is mostly\ncorrect, there were several cases in which another tempera-\nment matches the data more closely than the advertised one.\nThis is perhaps more surprising to a music theorist than to a\npractising tuner or performer, reï¬‚ecting the dichotomy be-\ntween those who see temperament as a mathematical system\nand those who have to retune their instrument during the in-\nterval of a concert. This also raises an interesting issue about\nthe nature of human annotations and their use as â€œground\ntruthâ€. The metadata provided with the CD is intended to\ngive an indication of the tuning system rather than scientiï¬-\ncally accurate documentation, and we need to be discerning\nin the use of metadata that has been collected for a purpose\nother than scientiï¬c analysis or evaluation.\n9. REFERENCES\n[1] M. Abe and J. Smith. CQIFFT: Correcting bias in a sinu-\nsoidal parameter estimator based on quadratic interpola-\ntion of FFT magnitude peaks. Technical Report STAN-\nM-117, CCRMA, Dept of Music, Stanford University,\n2004.\n[2] M. Abe and J. Smith. Design criteria for the quadrat-\nically interpolated FFT method (II): Bias due to in-\nterfering components. Technical Report STAN-M-115,\nCCRMA, Dept of Music, Stanford University, 2004.\n[3] J.M. Barbour. Tuning and Temperament, A Historical\nSurvey . Dover, Mineola, NY, 2004/1951.\n[4] E. Benetos and S. Dixon. Polyphonic music transcrip-\ntion using note onset and offset detection. In IEEE In-ternational Conference on Acoustics, Speech and Signal\nProcessing , 2011. 37â€“40.\n[5] A. de Cheveign Â´e. Multiple f0 estimation. In D.L. Wang\nand G.J. Brown, editors, Computational Auditory Scene\nAnalysis: Principles, Algorithms and Applications ,\npages 45â€“79. IEEE Press/Wiley, Piscataway, NJ, 2006.\n[6] A. Dessein, A. Cont, and G. Lemaitre. Real-time poly-\nphonic music transcription with non-negative matrix\nfactorization and beta-divergence. In 11th International\nSociety for Music Information Retrieval Conference ,\npages 489â€“494, 2010.\n[7] C. Di Veroli. Unequal Temperaments: Theory, History,\nand Practice . Bray Baroque, Bray, Ireland, 2009.\n[8] S. Dixon, M. Mauch, and D. Tidhar. Estimation of harp-\nsichord inharmonicity and temperament from musical\nrecordings. Journal of the Acoustical Society of Amer-\nica, 2011. To appear.\n[9] R. E. Dufï¬n. How equal temperament ruined harmony\n(and why you should care) . W. W. Norton, 2007.\n[10] H. Fletcher. Normal vibration frequencies of a stiff pi-\nano string. Journal of the Acoustical Society of America ,\n36(1):203â€“209, 1964.\n[11] M. Goto, H. Hashiguchi, T. Nishimura, and R. Oka.\nRWC music database: Music genre database and musi-\ncal instrument sound database. In 4th International Con-\nference on Music Information Retrieval , pages 229â€“230,\n2003.\n[12] A. Klapuri and M. Davy, editors. Signal Processing\nMethods for Music Transcription . Springer, New York,\nNY, 2006.\n[13] R. Kompass. A generalized divergence measure for\nnonnegative matrix factorization. Neural Computation ,\n19(3):780â€“791, 2007.\n[14] B. Lehman. Bachâ€™s extraordinary temperament: our\nrosetta stone. Early Music , 33(1):3â€“23, 2005.\n[15] D. D. Li and H. S. Seung. Learning the parts of objects\nby non-negative matrix factorization. Nature , 401:788â€“\n791, October 1999.\n[16] L. Rabiner. A tutorial on hidden Markov models and se-\nlected applications in speech recognition. Proceedings\nof the IEEE , 77(2):257â€“286, 1989.\n[17] E. Terhardt. The two-component theory of musical con-\nsonance. In E. Evans and J. Wilson, editors, Psy-\nchophysics and Physiology of Hearing , pages 381â€“390.\nAcademic, London, 1977.\n[18] D. Tidhar, M. Mauch, and S. Dixon. High precision fre-\nquency estimation for harpsichord tuning classiï¬cation.\nInProceedings of the IEEE International Conference on\nAcoustics, Speech and Signal Processing , pages 61â€“64,\n2010.\n286"
    },
    {
        "title": "An Auditory Streaming Approach for Melody Extraction from Polyphonic Music.",
        "author": [
            "Karin Dressler"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1416112",
        "url": "https://doi.org/10.5281/zenodo.1416112",
        "ee": "https://zenodo.org/records/1416112/files/Dressler11.pdf",
        "abstract": "This paper proposes an efficient approach for the identification of the predominant voice from polyphonic musical audio. The algorithm implements an auditory streaming model which builds upon tone objects and salient pitches. The formation of voices is based on the regular update of the frequency and the magnitude of so called streaming agents, which aim at salient tones or pitches close to their preferred frequency range. Streaming agents which succeed to assemble a big magnitude start new voice objects, which in turn add adequate tones. The algorithm was evaluated as part of a melody extraction system during the MIREX audio melody extraction evaluation, where it gained very good results in the voicing detection and overall accuracy.",
        "zenodo_id": 1416112,
        "dblp_key": "conf/ismir/Dressler11",
        "keywords": [
            "polyphonic",
            "musical",
            "audio",
            "voice",
            "identification",
            "auditory",
            "streaming",
            "model",
            "tone",
            "objects"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nAN AUDITORY STREAMING APPROACH FOR MELODY EXTRACTION\nFROM POLYPHONIC MUSIC\nKarin Dressler\nFraunhofer Institute for Digital Media Technology IDMT, Ilmenau, Germany\nkadressler@gmail.com\nABSTRACT\nThis paper proposes an efï¬cient approach for the identiï¬ca-\ntion of the predominant voice from polyphonic musical au-\ndio. The algorithm implements an auditory streaming model\nwhich builds upon tone objects and salient pitches. The\nformation of voices is based on the regular update of the\nfrequency and the magnitude of so called streaming agents,\nwhich aim at salient tones or pitches close to their preferred\nfrequency range. Streaming agents which succeed to assem-\nble a big magnitude start new voice objects, which in turn\nadd adequate tones. The algorithm was evaluated as part of a\nmelody extraction system during the MIREX audio melody\nextraction evaluation, where it gained very good results in\nthe voicing detection and overall accuracy.\n1. INTRODUCTION\nMelody is deï¬ned as a linear succession of tones which is\nperceived as a single entity. One important characteristic\nof the tone sequence is the smoothness of the melody pitch\ncontour. There are different techniques to avoid large fre-\nquency intervals in the tone sequence â€“ at present two main\nalgorithm types can be distinguished:\nOn the one hand, there are probabilistic frameworks that\ncombine pitch salience values and smoothness constraints\nin a cost function that is evaluated by optimal path ï¬nd-\ning methods like the hidden Markov Model (HMM), the\nViterbi algorithm or dynamic programming (DP). On the\nother hand, there are rule based approaches that trace multi-\nple F0 contours over time using criteria like magnitude and\npitch proximity in order to link salient pitch candidates of\nadjacent analysis frames. Subsequently, a melody line is\nformed from these tone-like pitch trajectories, using rules\nthat take the necessary precautions to assure a smooth melodic\ncontour. Of course such a division is rather artiï¬cial. It is\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proï¬t or commercial advantage and that copies\nbear this notice and the full citation on the ï¬rst page.\nc/circlecopyrt2011 International Society for Music Information Retrieval.easy to imagine a system that uses tone trajectories as input\nfor a probabilistic framework. And vice versa a statistical\napproach can be used to model tones. In fact, Ryyn Â¨anen\nand Klapuri have implemented a method for the automatic\ndetection of singing melodies in polyphonic music, where\nthey derive a HMM for note events from fundamental fre-\nquencies, their saliences and an accent signal [8].\nThere are many stable probabilistic relationships that can\nbe observed in melody tone sequences [6]. This fact makes\nthe application of a statistical model so useful, because such\ncharacteristics can easily be expressed mathematically in or-\nder to ï¬nd the optimal succession of tones. Hence, most\napproaches to voice processing are statistical methods that\naccomplish the tone trajectory forming and the identiï¬ca-\ntion of the melody voice simultaneously [4, 5, 7]. Rao and\nRao advocate DP over variants of partial and tone tracking,\nbut also clearly state the problems of most statistical meth-\nods [7].\nWhile for rule-based approaches alternative melody lines\ncan be recovered quite easily, there is no effective possibil-\nity to retrieve alternative paths for DP approaches, because\nthe mathematical optimization of the methods depends on\nthe elimination of concurrent paths. Hence, it is not easy to\nstate whether the most likely choice stands out from all other\npaths. This problem is most evident if two or more voices\nof comparable strength occur simultaneously within a mu-\nsical piece. Work towards a solution to this problem was\npresented in [7], giving an example for DP with dual fun-\ndamental frequency tracking. The system tracks an ordered\npair of two pitches, but it cannot ensure that the two contours\nwill remain faithful to their respective sound sources.\nAnother challenging problem is the identiï¬cation of non-\nvoiced portions, e.g. frames where no melody voice occurs.\nThe simultaneous identiï¬cation of the optimal path together\nwith the identiï¬cation of melody frames is not easy to ac-\ncomplish within one statistical model, so often the voic-\ning detection is performed by a separate processing step.\nNonetheless, optimal path ï¬nding algorithms may be con-\nfused by breaks in the tone sequence, especially because\nthe usual transition probabilities do not apply in between\nmelodic phrases.\n19Oral Session 1: Melody and Singing\npitch spectrogram \n&\nactive tone liststreaming agents\ncapture salient\ntones & pitches\nâ— tone magnitudeâ— voice to \n tone distanceâ— freq. dynamics melody tonesvoice objects \nadd tones \nto voicesidentify\nmelody voice\nâ— voice magnitudeâ— (frequency)Figure 1 . Overview of the voice estimation algorithm\nIn this paper, we present an algorithm for the identiï¬-\ncation of predominant voices in music that addresses some\nof the above-mentioned problems. Although no statistical\nmodel is implemented, probabilistic relationships that can\nbe observed in melody tone sequences are exploited.\n2. METHOD\n2.1 Overview\nFigure 2.1 shows an overview of the algorithm. The in-\nput to the proposed algorithm are the tone objects and/or\nsalient pitches of the current frame. The formation of mu-\nsical voices is a continuous process destined by the frame-\nwise evolution of so-called streaming agents, which are dis-\ntributed along the frequency spectrum. A streaming agent\ngains power by the capturing of salient tones or pitches.\nMoreover, it changes its position in the frequency spectrum\nin order to move towards salient sounds. V oice objects can\nbe derived from the streaming agents. Then, adequate tone\nobjects are assigned to the respective voices. Finally, the\nmelody voice is chosen from the set of voices. The main cri-\nterion for the selection is the magnitude of the voice. Only\ntone objects of the melody voice qualify as melody tones.\n2.2 Formation of Streaming Agents\nThe voice detection is based on 18 streaming agents (SA).\nEach streaming agent denotes a very simple voice forma-\ntion unit, which independently selects a succession of strong\ntones or pitches. It is mainly characterized by its magnitude\nÂ¯Asa, and two frequency based measures: a variable position\nÂ¯fsaand a ï¬xed home position fsahome, which are both given\nin cent. The home positions of the streaming agents are dis-\ntributed evenly with a distance of 300 cent over the allowed\nmelody frequency range.\nOver time, each streaming agents gradually moves to-\nwards the selected sound sources and assembles a magni-\ntude corresponding to the rating magnitude of the captured\ntone objects.\n15\n 10\n 5\n 0 5 10 15\nfrequency distance [semitone]0.00.20.40.60.81.0weight\nw_1\nw_2Figure 2 . Gaussian Weighting Functions\n2.2.1 Selection of Tones\nIn each analysis frame, the streaming agents searches for\nstrong tones and pitches. In the further description, we refer\nonly to tone objects, although the method can be also used\nfor frame-wise estimated pitch magnitudes as described for\nexample in [3]. For the identiï¬cation of the best matching\ntone a rating is calculated from four criteria:\nâ€¢magnitude: The tone magnitude Atoneis a good indi-\ncator for the perceptual importance of a tone.\nâ€¢frequency distance weight: It is due to the ï¬xed home\nposition that each SA may pick different notes in a\npolyphonic signal. While at the one hand a strong se-\nlection criterion is the magnitude of the tone object,\nat the other hand the agentâ€™s choice is strongly biased\ntowards its own home position. The frequency dis-\ntance âˆ†fin cent between the toneâ€™s pitch ftoneand the\nstreaming agentâ€™s home position fsahome enters into\nthe rating as a weighting factor that is calculated us-\ning a Gaussian function w1(âˆ†f):\nw1(âˆ†f) =eâˆ’0.5(âˆ†f)2\n6402 (1)\nFigure 2.2.1 shows the weighting function, which reaches\nhalf the maximum value at a frequency difference of\napproximately 750 cent.\nâ€¢frequency deviation: Human listeners draw particular\nattention to all sounds with changing attributes. If a\ntone has a varying frequency deviation (persistently\nmore than 20 cent frequency difference in between\nanalysis frames) the rating is doubled. Accordingly,\nthe deviation factor Dis set to one or two in the ï¬nal\nrating.\nâ€¢capture mode: There should be a tendency of the SA\nto continuously track an already captured tone object.\nIf a tone object has already been captured by a SA,\nthe rating for the tone object is boosted by the factor\nC= 1.5. Otherwise, the factor is set to one. (See\nsection 2.2.2 for a detailed explanation of the capture\nmode.)\n2012th International Society for Music Information Retrieval Conference (ISMIR 2011)\nThe rating is estimated for all streaming agents and ï¬-\nnally, each streaming agent â€picksâ€ only one tone â€“ the ob-\nject with the maximum rating magnitude Arating:\nArating=DÂ·CÂ·AtoneÂ·w1(ftoneâˆ’fsahome) (2)\nFor the rating of pitches the boost factors DandCare omit-\nted â€“ the rating is simply the product of pitch magnitude and\nthe frequency distance weight.\n2.2.2 Modes of Tone Capturing\nAs the streaming agent approaches salient sound sources,\ntwo different modes are distinguished within the tone cap-\nturing process: aim andcaptured . In the aim mode the\nstreaming agent aims at a distinct tone object and moves\nslowly towards the selected pitch or tone.\nIn order to capture a tone, the SA must aim at the distinct\ntone for a speciï¬c time span. The demanded time depends\non the difference between the variable position of the SA Â¯fsa\nand the toneâ€™s frequency ftone1. As long as the SA aims at\nthe same tone object, a capture counter nis incremented in\neach analysis frame. The tone is captured if2:\nn>1\n30/vextendsingle/vextendsingleÂ¯fsaâˆ’ftone/vextendsingle/vextendsingle. (3)\nAs the SA moves towards the selected pitch, the frequency\ndifference between tone and streaming agent becomes smaller\nduring the capturing process. Since the adaptation speed of\nthe variable position Â¯fsadepends on many parameters, the\nduration needed to capture a tone cannot be immediately\nassessed from the frequency difference between successive\nnotes. As soon as the SA aims at a sound object in a differ-\nent frequency region, the capture counter is set to zero.\nThe mode captured might not be reached by every tone\nin very complex or noisy music signals. Yet, it is not neces-\nsary that a tone is captured by a streaming agent to qualify\nas a melody tone. The aimmode is generally sufï¬cient to\nensure the propagation of the streaming agents towards the\nmost signiï¬cant sound sources. Still, the additional mode\nenhances the movement of the streaming agent towards the\nselected tone objects.\n2.2.3 Magnitude Update\nThe streaming agent is able to increase its magnitude Â¯Asa\nwhenever it reaches the capture mode captured . The mag-\nnitude it assembles depends on the current rating magnitude\nof the selected tone as given in equation 2, but without tak-\ning into account the boosting factor C. The slightly altered\nrating magnitude is labeled Aâˆ—\nrating. The use of this rating\nmagnitude implies that a streaming agent which captures a\n1All frequencies are measured in cent.\n2The condition assumes a hop-size of 5.8 ms between two analysis\nframes.tone far away from the home positions will not build up a\nhigh magnitude.\nHowever, for the computation of the magnitude, the ini-\ntial ratingAâˆ—\nrating is weighted by a second frequency distance\nweighting which exploits the distance between the variable\nposition of the streaming agent Â¯fsaand the toneâ€™s frequency\nftone. The weighting function remains the same: the Gaus-\nsian function w1given in equation 1. The additional weight-\ning assures that the streaming agent proï¬ts more from tone\nmagnitudes which are close to its current position Â¯fsa.\nIn order to update the magnitude values we use the expo-\nnential moving average (EMA)3:\nÂ¯Asaâ†’Î±xÂ·Â¯Asa+ (1âˆ’Î±x)Â·Aâˆ—\nratingÂ·w1(ftoneâˆ’Â¯fsa).(4)\nThe start value for the iterative calculation of the EMA is\nzero. The smoothing factor Î±xdepends on the current weighted\nratingAâˆ—\nratingÂ·w1(ftone,Â¯fsa). If the current value is higher\nthan the actual EMA value, Î±xcorresponds to a half life pe-\nriod of 1 second, otherwise the half life period is set to 500\nms. If the streaming agent is only in aimcapture mode, the\nmagnitude of the streaming agent is damped with a half life\nperiod of 500 ms.\n2.2.4 Position Update\nThe streaming agent changes its variable position Â¯fsato-\nwards salient tones or pitches. The speed of the position\nadaptation is mainly determined by three factors:\nâ€¢the toneâ€™s magnitude: the bigger the tone magnitude\nin comparison to the long term average weightings,\nthe faster the SA changes its position.\nâ€¢the distance between captured tone and the streaming\nagentâ€™s home position: the SA tends to move faster\ntowards its own home position. This behavior ensures\nthe stream segregation for a cycle of quickly alternat-\ning high and low tones as described in [1, chapter 2].\nâ€¢the frequency deviation: the SA moves faster towards\nfrequency modulated tones.\nâ€¢the capture mode : the SA moves faster towards cap-\ntured tones.\nFrom this it follows that the basic weighing for the posi-\ntion update is similar to the rating magnitude Arating for the\ntone selection process as given in equation 2. In order to\n3The EMA applies weighting factors to all previous data points which\ndecrease exponentially, giving more importance to recent observations\nwhile still not discarding older observations entirely. The smoothing factor\nÎ±determines the impact of past events on the actual EMA. It is a number\nbetween 0 and 1. A lower smoothing factor discards older results faster. A\nmore intuitive measure than the smoothing factor is the so called half-life\nperiod. It denotes the time span over which the initial impact of an observa-\ntion decreases by a factor of two. Taking into account the desired half-life\nthand the time period between two EMA calculations âˆ†tâ‰ˆ5.8ms, the\ncorresponding smoothing factor is calculated as follows: Î±= 0.5âˆ†t/th.\n21Oral Session 1: Melody and Singing\nestimate the signiï¬cance of the current rating magnitude, it\nhas to be set into relation with the ratings of previous anal-\nysis frames. Thatâ€™s why we introduce a position magnitude,\nwhich is the exponential moving average of previous rat-\nings:\nÂ¯Aposâ†’Î±1.5sÂ·Â¯Apos+ (1âˆ’Î±1.5s)Â·Arating. (5)\nIn order to adapt the variable position Â¯fsaof the streaming\nagent, the current rating is set into relation with the EMA of\nprevious ratings:\nÂ¯fsaâ†’Â¯AposÂ¯fsa+ (1âˆ’Î±500ms)Â·AratingÂ·ftone\nÂ¯Apos+ (1âˆ’Î±500ms)Â·Arating. (6)\nThe initial value for the iterative calculation is the home\npositionfsahome. Parameter Î±500ms is a smoothing factor,\nwhich corresponds to a half life time of 500 ms4.\n2.3 Formation of Voices\nThe positions and magnitudes of the 18 streaming agents are\nthe foundation for the voice estimation. Figure 3 shows how\nthe progress of the multiple streaming agents is inï¬‚uenced\nby salient tone objects. It can be noted that the approximate\nprogression of musical voices is already suggested by the\ndistribution of the streaming agents.\nEach streaming agent which poses a local magnitude max-\nimum is a candidate for the formation of a voice object. This\nmeans that each voice object is in general linked to a stream-\ning agent with the peak magnitude compared to the magni-\ntude of the neighboring agents. Of course, the local maxi-\nmum may shift from one streaming agent to another. In this\ncase, the voice may gradually change the assigned link to a\nneighboring streaming agent within the duration of approx-\nimately 20 analysis frames. The position of a voice fvoice\nis deï¬ned by the position Â¯fsaof the linked streaming agent.\nThe magnitude of the voice Avoiceis deï¬ned by the streamer\nmagnitude Â¯Asa. If the voice is currently adapting to a new\nstreaming agent, weighted average values of the concerning\ntwo streaming agents are used.\nIf a streaming agent with a local maximum magnitude\nis not assigned to a voice object, it may start a new one.\nHowever, a new voice is created only if the streaming agent\nis more than 4 streaming agents away from a any stream-\ning agent linked to another voice, or if the frequency dif-\nference between the streaming agent and all other existing\nvoices is greater than 600 cent. A voice object is eliminated\nif the voice magnitude is smaller than 5 percent of the global\nmaximum voice magnitude or if two voices aim at the same\nstreaming agent. In the latter case the voice with the smaller\nmagnitude is eliminated.\n4Since the position weight depends on many factors, parameter Î±does\nnot exactly set any half life period for the position update. Yet the corre-\nsponding time span gives a reference point for the approximate adaptation\nspeed.2.4 Adding Tones to Voices\nNow that voice objects have been deï¬ned, adequate tone ob-\njects must be added. The only voice tone candidate is actu-\nally the currently selected tone of the corresponding stream-\ning agent. If the voice is adapting to a new streaming agent,\nthe closest streaming agent is used as a reference. Several\nmeasures are taken to ensure a reliable voicing detection.\nThis means even if the corresponding streaming agent has\nselected a tone, the tone candidate has to be validated in or-\nder to qualify as a voice tone.\n2.4.1 Distance Threshold\nAlthough the proposed algorithm does not apply a common\nstatistical model, it takes advantage of the most eminent\nprobabilistic relationships in melodic tone sequences [6]: 1)\nMelodies consist typically of tones that are close to one an-\nother in pitch. 2) There is a strong tendency for a regression\nto the mean pitch.\nThe frequency of the voice represents the weighted av-\nerage frequency of the recently selected tone objects, so in\na way the voice position can be seen as the adaptive com-\nputation of the mean pitch. Consequently, the best voice\ntone candidates are close to the actual voice position. Ade-\nquate voice tones have to be within an octave range of the\nactual voice position. Another obvious thing to do would be\nthe adjustment of the magnitude thresholds according to the\nfrequency distance. This idea is implemented in the short\nterm magnitude threshold described in section 2.4.3.\n2.4.2 Global Long Term Magnitude Threshold\nThe global long term magnitude threshold is implemented\nas an adaptive threshold that is valid for all voices. It decays\nwith a half life period of 5 seconds. If a tone magnitude\nappears which is larger than the current long term magni-\ntude value, the magnitude threshold is updated to the new\nmaximum.\nThe magnitude of the candidate voice tone is compared\nto the long term maximum value. In order to pass the global\nthreshold, tones should not be more than 8 dB below the\ndecaying maximum value. Still, other criteria may alter the\neffective threshold value â€“ in the best case the allowed dy-\nnamic range is increased from 8 dB to 20 dB:\nâ€¢The capture level of the assigned streaming agent and\nits two neighbors are evaluated. Depending on how\nmany streaming agents are in capture mode captured\nconcerning the candidate voice tone, the effective thresh-\nold may decreased to 14 dB below the decaying max-\nimum. On the other hand, the threshold is increased\nfor all tones that are not selected (aimed) by at least 5\nstreaming agents in the long term average.\nâ€¢A variation of the fundamental frequency (vibrato or\nglides) increases the noticeability of tones. In this\n2212th International Society for Music Information Retrieval Conference (ISMIR 2011)\n0.0 1.0 2.0 3.0 4.0\ntime (s)0100020003000frequency (cent)Figure 3 . Streaming Agents: It can be seen how the streaming agents (thin lines) move towards salient tones and pitches. To\nmaintain clarity salient pitches are not shown. The identiï¬ed tone objects are indicated by dark bold lines. When the bass voice\ncomes in, some streaming agents turn to the bass voice as it is closer to their preferred home position.\ncase the threshold is lowered by 6 dB.\n2.4.3 Short Term Magnitude Threshold\nThe short term magnitude threshold is estimated separately\nfor each voice. It secures that shortly after a strong tone is\nï¬nished no weaker tone is included as a voice tone, so it is\nespecially useful to bridge small time gaps between strong\ntones of a voice. Furthermore, the threshold delays the in-\nclusion of tones that are far away from the current voice po-\nsition. To achieve this the tone magnitude is again weighted\nwith a frequency distance weight, evaluating the frequency\noffset between tone and voice:\nw2=r+ (1âˆ’r)Â·w1(ftoneâˆ’fvoice). (7)\nFigure 2.2.1 shows that the weighting function w2is asym-\nmetric. Tones in the lower frequency range of an instrument\nor the voice are often softer. Hence, parameter ris set to\n0.4 for tones with a lower frequency than the current voice\nposition, otherwise r= 0.2.\nThe short term threshold is adaptive and decays with a\nhalf life time of 100 ms. If a weighted tone magnitude\nw2Â·Atoneappears which is larger than the current short term\nmagnitude threshold, the threshold is updated to the new\nmaximum. The tone passes the threshold if it is no more\nthan 6 dB below the current threshold value.\n2.5 The Identiï¬cation of the Melody Voice\nThe most promising feature to distinguish melody tones from\nall other sounds is the magnitude. The magnitude of the\ntones is of course reï¬‚ected by the voice magnitude. Hence,\nin general the voice with the highest magnitude is selected\nas the melody voice. It may happen that two ore more voices\nhave about the same magnitude and thus no clear decision\ncan be taken. In this case, the voices are weighted accord-\ning to their frequency: voices in very low frequency regions\nreceive a lower weight.3. EV ALUATION\n3.1 Qualitative Evaluation\nA striking advantage of the proposed method is its compu-\ntational efï¬ciency and the continuously updated voice in-\nformation in real time. Moreover, the algorithm is ï¬‚exi-\nble enough to track a variable number of concurrent voices.\nThis is the main reason for the good melody detection ac-\ncuracy for instrumental music excerpts with two or more\nstrong voices like the one shown in ï¬gure 4.\nThe segregation of notes into different auditory streams\ndepends on many aspects â€“ like for example the magnitude,\nfrequency and timbre of tones. Psychoacoustic experiments\nhave shown that the grouping of tones also depends on the\nrate [1]. Due to the delayed capturing of tone objects, the\npresented method is able to take into account temporal as-\npects of the evolving signal. For example a series of alternat-\ning high and low tones will be integrated into one auditory\nstream at a low playback speed. Yet, with increasing rate\nhigh and low tones are grouped into individual voices.\nNonetheless, it must be noted that many aspects of hu-\nman perception cannot be covered. Although the algorithm\nallows a broad dynamic range for melody tones, in some in-\nterpretations an even greater dynamic range can be found,\nespecially if the melody is sung by a human. Still, by low-\nering the magnitude thresholds many tones from the accom-\npaniment will be selected by mistake. A simple magnitude\nthreshold cannot avoid all errors.\n3.2 MIREX Audio Melody Extraction Task\nThe presented method for the detection of predominant voices\nhas been implemented as part of a melody extraction algo-\nrithm which was evaluated at the Music Information Re-\ntrieval Evaluation eXchange (MIREX) [2]. Algorithm pa-\nrameters regarding the width and the shape of the weight-\ning functions as well as the timing constants of the adaptive\nthresholds have been adjusted using the melody extraction\ntraining data of ISMIR 2004 and MIREX 2005. Although\nthe presented parameter sets maximize accuracy in the two\n23Oral Session 1: Melody and Singing\n0.0 1.0 2.0 3.0 4.0\ntime (s)0100020003000frequency (cent)Figure 4 . V oices: When the bass voice comes in, a second voice object is created. Two predominant voices are recognized: the\nmelody voice (blue) and the bass voice (pink).\nAlgorithm\nproposed90.941.080.673.424\ndr192.451.774.466.923040\ndr287.741.272.166.2524\n91.351.172.265.226\npc79.340.364.162.94677\n61.029.473.356.63726\ncl280.357.463.555.233\ncl193.080.763.552.228\nhjc143.69.766.150.5344\nhjc243.69.751.149.0584Voicing\nRecall (%)Voicing \nFalse \nAlarm (%)Raw \nPitch\n(%)Overall \nAccuracy \n(%)Runtime \n(min)\nrr\njjy\nFigure 5 . Melody Extraction Results of MIREX 2009\ndata sets, acceptable results are achieved on a wide parame-\nter range. Moreover, the MIREX results show that the given\nsettings generalize well on different kinds of data.\nTable 5 shows the analysis results for systems that per-\nform voicing detection. The melody extraction algorithm\nachieved the best overall accuracy and at the same time stands\nout due to very short run-times. The Raw Pitch measure\nrepresents the estimation performance for all voiced frames.\nFor this measure the evaluation is constrained to time in-\nstants where the melody voice is present. The measure Over-\nall Accuracy requires a voicing detection â€“ the algorithm\nhas to indicate whether the melody voice is present in the\ncurrent frame or not. The MIREX results show that the im-\nplemented method allows a high V oicing Recall and at the\nsame time a low V oicing False Alarm.\n4. CONCLUSION\nIn this paper we presented an efï¬cient approach to auditory\nstream segregation in polyphonic music. The MIREX re-\nsults show that the proposed method allows a reliable iden-\ntiï¬cation of the predominant voice in different kinds of poly-\nphonic music. The qualitative evaluation shows that the al-\ngorithm mimics some characteristics of stream segregation\nin the human auditory system, taking into account the mag-\nnitude of tones, note intervals and playback speed. How-ever, timbral features are not exploited to group tones. In\norder to reach a higher accuracy an instrument/singing voice\nrecognition is required.\n5. REFERENCES\n[1] A. S. Bregman. Auditory Scene Analysis: The Percep-\ntual Organization of Sound , volume 1 MIT Press paper-\nback. MIT Press, Cambridge, Mass., Sept. 1994.\n[2] K. Dressler Audio Melody Extraction for MIREX\n2009. In 5th Music Information Retrieval Evaluation\neXchange (MIREX) , 2009.\n[3] K. Dressler Pitch estimation by the pair-wise evaluation\nof spectral peaks. In AES 42nd Conference , Ilmenau,\nGermany, July 2011.\n[4] J.-L. Durrieu, G. Richard, B. David and C.Fvotte\nSource/Filter model for unsupervised main melody ex-\ntraction from polyphonic audio signals. IEEE Trans-\nactions on Audio, Speech and Language Processing ,\n18(3):564â€“575, Mar. 2010.\n[5] C.-L. Hsu, L.-Y . Chen, J.-S. R. Jang, and H.-J. Li.\nSinging pitch extraction from monaural polyphonic\nsongs by contextual audio modeling and singing har-\nmonic enhancement. In Proc. of the 10th International\nSociety for Music Information Retrieval Conference (IS-\nMIR) , Kobe, Japan, Oct. 2009.\n[6] D. Huron. Sweet Anticipation: Music and the Psychol-\nogy of Expectation . The MIT Press, Cambridge, Mas-\nsachusetts, 2006.\n[7] V . Rao and P. Rao. Improving polyphonic melody ex-\ntraction by dynamic programming based dual f0 track-\ning. In Proc. of the 12th International Conference on\nDigital Audio Effects (DAFx) , Como, Italy, Sept. 2009.\n[8] M. Ryyn Â¨anen and A. Klapuri. Transcription of the\nsinging melody in polyphonic music. In Proc. of the 7th\nInternational Society for Music Information Retrieval\nConference (ISMIR) , Victoria, Canada, Oct. 2006.\n24"
    },
    {
        "title": "Aligning Semi-Improvised Music Audio with Its Lead Sheet.",
        "author": [
            "Zhiyao Duan",
            "Bryan Pardo"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1417325",
        "url": "https://doi.org/10.5281/zenodo.1417325",
        "ee": "https://zenodo.org/records/1417325/files/DuanP11.pdf",
        "abstract": "Existing audio-score alignment methods assume that the audio performance is faithful to a fully-notated MIDI score. For semi-improvised music (e.g. jazz), this assumption is strongly violated. In this paper, we address the problem of aligning semi-improvised music audio with a lead sheet. Our approach does not require prior training on performances of the lead sheet to be aligned. We start by analyzing the problem and propose to represent the lead sheet as a MIDI file together with a structural information file. Then we propose a dynamic-programming-based system to align the chromagram representations of the audio performance and the MIDI score. Techniques are proposed to address the chromagram scaling, key transposition and structural change (e.g. a performer unexpectedly repeats a section) problems. We test our system on 3 jazz lead sheets. For each sheet we align a set of solo piano performances and a set of fullband commercial recordings with different instrumentation and styles. Results show that our system achieves promising results on some highly improvised music.",
        "zenodo_id": 1417325,
        "dblp_key": "conf/ismir/DuanP11",
        "keywords": [
            "audio-score alignment",
            "semi-improvised music",
            "lead sheet",
            "dynamic-programming-based system",
            "chromagram representations",
            "structural information file",
            "key transposition",
            "structural change",
            "promising results",
            "highly improvised music"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nALIGNING SEMI-IMPROVISED MUSIC AUDIO WITH ITS LEAD SHEET\nZhiyao Duan and Bryan Pardo\nNorthwestern University\nDepartment of Electrical Engineering & Computer Science\nzhiyaoduan00@gmail.com, pardo@northwestern.edu\nABSTRACT\nExisting audio-score alignment methods assume that the au-\ndio performance is faithful to a fully-notated MIDI score.\nFor semi-improvised music (e.g. jazz), this assumption is\nstrongly violated. In this paper, we address the problem\nof aligning semi-improvised music audio with a lead sheet.\nOur approach does not require prior training on performances\nof the lead sheet to be aligned. We start by analyzing the\nproblem and propose to represent the lead sheet as a MIDI\nï¬le together with a structural information ï¬le. Then we\npropose a dynamic-programming-based system to align the\nchromagram representations of the audio performance and\nthe MIDI score. Techniques are proposed to address the\nchromagram scaling, key transposition and structural change\n(e.g. a performer unexpectedly repeats a section) problems.\nWe test our system on 3 jazz lead sheets. For each sheet\nwe align a set of solo piano performances and a set of full-\nband commercial recordings with different instrumentation\nand styles. Results show that our system achieves promising\nresults on some highly improvised music.\n1. INTRODUCTION\nIn this work we investigate the problem of aligning an audio\nrecording of semi-improvised music to a lead sheet. This\nproblem belongs to a more general research problem called\nscore alignment , i.e. ï¬nding the time mapping between a\nmusical performance and its score. The fulï¬llment of this\ntask would be very useful for a number of applications like\nsynchronizing multiple sources (video, audio, score, etc.) of\nmusic in a digital library and automatically accompanying a\nmusical performance.\nIn the last two decades, many methods have been pro-\nposed for score alignment in different problem settings: MIDI\nto MIDI, audio to MIDI, monophonic or polyphonic audio\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proï¬t or commercial advantage and that copies\nbear this notice and the full citation on the ï¬rst page.\ncâƒ2011 International Society for Music Information Retrieval.performances, online or ofï¬‚ine, etc. [4]. However, most\nmethods assume faithful performances to a fully-notated score,\nwith at most a tempo change and key transposition.\nWe call modern jazz semi-improvised , because many sig-\nniï¬cant elements of the music are improvised but deeper-\nlevel structural aspects remain relatively ï¬xed. The score for\nsemi-improvised music is called a lead sheet . A lead sheet\nspeciï¬es only essential elements like a basic melody, har-\nmony, lyric and a basic musical form. A performer typically\nimprovises all the notes in a solo, changes in tempo, ac-\ncompaniment ï¬guration and even some structural elements\nof a piece (e.g. repeating a chorus). The nature of semi-\nimprovised music makes the alignment to a lead sheet very\nchallenging. Even for an educated musician it is sometimes\ndifï¬cult to align an improvisation to the lead sheet when the\nimprovisation has high degree of freedom.\nFor aligning such performances, a few methods have been\nproposed. Dannenberg and Mont-Reynaud [5] aligned a\njazz solo performance with the chord progression on the\nscore. Pardo and Birmingham [10] aligned a polyphonic\nsemi-improvised MIDI performance with its lead sheet. They\nalso proposed a method [11] to follow a performance with\npossible structural variations, i.e., deviating from the ex-\npected path written on the score by skipping or repeating\na section. The above-mentioned methods have loosened the\nfaithful performance assumption, however, they are either\nlimited to deal with MIDI performances [10,11], or can only\nfollow a solo performance under a 12-bars blues form [5].\nArzt and Widmer [1] also proposed an alignment system\nto handle structural variations, but only for non-improvised\n(classical) music. To our knowledge, there is no existing\nmethods that align a semi-improvised (polyphonic) audio\nperformance under an arbitrary form with its lead sheet.\nThis problem is in some ways similar to Cover Song\nIdentiï¬cation (CSI), i.e. identifying different performances\n(usually by different artists) of the same song [7]. How-\never, variations of these performances are generally much\nless than those in what we called semi-improvised music\nsuch as modern jazz. In addition, the alignment methods\nused in CSI only serve as an intermediate step for similarity\ncalculation, and no precise time mappings are required.\nIn this paper, we attempt to address the semi-improvised\n513Poster Session 4\nmusic audio-score alignment problem, without prior train-\ning on example performances of the lead sheet to be aligned.\nWe ï¬rst analyze the problemâ€™s unique properties in Section\n2, then propose an alignment system regarding these prop-\nerties in Section 3. In Section 4 we describe experiments to\ntest the system on real performances of solo piano and jazz\ncombo. Section 5 concludes this paper.\n2. PROBLEM ANALYSIS\n2.1 Basic Properties\nThe problem considered in this paper is aligning an audio\nrecording of a semi-improvised music performance to its\nlead sheet. A lead sheet usually only speciï¬es a basic melody,\nharmony, lyric and a basic musical form (structure). Take\nFigure 1(a) as an example. The melody is indicated by note\nheads. Harmony is indicated by chord symbols above the\nstaff. Lyrics are indicated as text below the staff. The text\nâ€œAâ€ with a square indicates the start of Section A, and the\nrepeat sign besides it suggests that this section is often re-\npeated in a performance. We can translate this lead sheet\ninto a MIDI ï¬le by setting a tempo (e.g. 120BPM), render-\ning harmony as block chords with root notes in the C2-C3\noctave and discarding the lyric and music structure informa-\ntion. The piano-roll representation of this MIDI is shown in\nFigure 1(b). We mark measures with vertical dash lines.\n(a) Lead sheet\n0 2 4 6 8C2C3C4C5\nTime (seconds)Pitch\n(b) Lead sheet converted to MIDI\n0 2.532 4.579 7.134 9.414C2C3C4C5\nTime (seconds)Pitch\n(c) First improvised performance\n0 1.828 3.627 5.479 7.376C2C3C4C5\nTime (seconds)Pitch\n(d) Second improvised performance\nFigure 1 . Four measures of the lead sheet for Dindi by An-\ntionio Carlos Jobim, and its two semi-improvised piano per-\nformances.In semi-improvised performances, the performer views\nthe lead sheet as a reference and continuously creates new\nmusical elements that are not on the score. Figures 1(c) and\n1(d) show the piano-rolls of two semi-improvised piano per-\nformances by two different pianists of the lead sheet, with\nmeasure times marked by vertical dash lines. We can see\nthat the two performances have different tempi from the lead\nsheet. Also, harmony is rendered in free rhythmic patterns.\nWe also notice that the melody contour of the lead sheet re-\nmains in the ï¬rst performance, while is signiï¬cantly altered\nin the second performance.\n2.2 Representing Harmonic Content\nHarmonic content is the most similar feature that an semi-\nimprovised performance and its lead sheet shares. We need\nto ï¬nd a representation of harmonic content, robust to vari-\nations among different performances, on which to do the\nalignment. The chromagram is a good representation which\nhas been used in many audio-score alignment methods [4].\nIn these methods, chroma features are usually calculated for\nevery short time frame (e.g. 46 ms), so that the alignment\ncan be precise at the millisecond level. However, this choice\nis not suitable in our problem, as we can see in Figure 1 that\nperformed notes can be signiï¬cantly different from the notes\nwritten on the lead sheet at any one 46 ms frame. In fact,\nchord labels on the lead sheet are more like sets of high-\nlikelihood notes to be played over given time periods (e.g.\ntwo beats of D minor 7), and aggregating performed notes\nacross larger time spans (e.g. two beats) makes for a clearer\ncorrespondence to the score. Therefore we choose to calcu-\nlate chroma features in this scale.\n2.3 Utilizing Structural Information\nStructural information on the lead sheet is also important\nfor an alignment system. Performers often modify the ba-\nsic musical form, but not arbitrarily. For example, the basic\nform of Dindi is â€œIntro-[A-A-B-C]â€, where the bracket rep-\nresents a repeat sign. Performers may skip the Intro section\nat the beginning but play it at the end. They may change the\nrepeat bracket by including the Intro section or excluding\nthe A sections. Basically, they view musical sections as toy\nbricks, selecting and shufï¬‚ing them during a performance.\nHowever, it is not common to make other structural changes\nsuch as making a jump at the middle of a section.\nHowever, structural information on the lead sheet is not\nencoded in the MIDI representation shown in Figure 1(b).\nTherefore, we encode it in an additional ï¬le, as shown in\nTable 1. Basically, this ï¬le stores two kinds of information:\n1) musical section deï¬nitions and boundaries; 2) possible\njumps that an semi-improvised performance might make.\n51412th International Society for Music Information Retrieval Conference (ISMIR 2011)\nSections from to Jumps from to\nIntro 1 16 48 1\nA 17 24 48 17\nA 25 32 48 33\nB 33 40\nC (A) 41 48\nTable 1 . Structural information extracted from the lead\nsheet for Dindi . Section C is very similar to Section A.\n3. PROPOSED SYSTEM\nBased on the above analysis, we design our system as shown\nin Figure 2. We represent both the audio and MIDI with a\nchromagram where chroma vectors are extracted at the 2-\nbeats scale, then use a modiï¬ed string alignment algorithm\nthat can handle structural changes to align the chromagrams.\nFigure 2 . Overview of the proposed system.\n3.1 Audio Beat Tracking\nIn order to extract chroma features from audio at the 2-beat\nscale, we need audio beat times of the performance. We\nuse the original implementation of the beat tracking algo-\nrithm proposed by Ellis [6]. While this is a high-quality beat\ntracker, the estimated tempo often has halï¬ng/doubling er-\nrors, as described in [6]. In addition, when the performancehas an unstable tempo, the algorithm may ï¬nd extra beats or\nmiss some beats.\n3.2 Audio Chroma Feature Extraction\nWe ï¬rst chop the audio signal into 46 ms long time frames\nwith a 23 ms hop size and calculate a chroma vector for\neach frame. The frame-level chroma vector is 12-d, and is\ncalculated by â€œfoldingâ€ the local maxima of the hamming-\nwindowed Short Time Fourier Transform (STFT) spectrum\nto the 12-pitch classes. This tends to suppress the non-\nharmonic part of the spectrum.\nAs discussed in Section 2.2, the ideal analysis unit is not\nthe 46 ms frame, but something on the order of 2 musi-\ncal beats. We therefore average the chroma vectors of the\nframes into segments of length land a hop size h, where\nthese values are measured in beats. The resulting chroma-\ngram is a sequence of the segment-level chroma vectors. In\nour experiments, we set landhto 2 beats and1\n4beats, re-\nspectively. A segment size of two beats worked well for the\nharmonic rhythm of the music analyzed, with the shortest\nduration chords typically being 2 beats. For the hop size h,\ntheoretically a smaller hleads to a more precise alignment.\nHowever, the computational complexity increases quickly\nashshrinks ( O(1/h2)). We investigate the inï¬‚uence of dif-\nferent parameters on the alignment result in Section 4.\n3.3 MIDI Chroma Feature Extraction\nAs with the audio chromagram, we segment the MIDI repre-\nsentation of the lead sheet into segments of length land hop\nsizeh, and calculate a chroma vector for each segment. We\nsimply sum up the lengths of notes in each segment to their\ncorresponding pitch-class bins. We generate 12 transposed\nMIDI chromagrams to cope with the possible key transposi-\ntion of the audio performance.\n3.4 Chromagram Scaling Problem\nIn Section 3.1, we note that the estimated tempo of the au-\ndio might be half or twice the true tempo. Therefore the au-\ndio and MIDI chromagrams might be on temporal different\nscales, which will strongly inï¬‚uence the alignment result.\nTo address this problem, we also segment the MIDI ï¬le\nand calculate the chromagram in three ways, with segment\nlength and hop size of (l, h),(2l,2h)and(1\n2l,1\n2h), respec-\ntively. Therefore, for each audio-MIDI pair, we have 1 audio\nchromagram and 36 MIDI chromagrams, corresponding to\n3 scales and 12 key transpositions. It is noted that the idea\nof time scaling and key transposition has been used in other\nmusic information retrieval systems such as [3].\n3.5 Aligning Chromagrams\nLetA= (a1,a2,Â· Â· Â·,am)be the audio chromagram, S=\n(s1,s2,Â· Â· Â·,sn)be the score chromagram, where aiis the\n515Poster Session 4\nchroma vector of the i-th audio segment and sjis the chroma\nvector of the j-th score segment. We describe a dynamic-\nprogramming algorithm to align them. Unlike standard string\nalignment algorithms, this algorithm utilizes structural in-\nformation provided by the lead sheet (as shown in Table 1)\nto handle possible structural changes in the semi-improvised\nperformance. To do so, we deï¬ne a parent-index set P(j)\nfor each score segment index j. Each element kofP(j)is a\nscore segment index, from which a semi-improvised perfor-\nmance might transition to j. This transition can be a smooth\nprogression i.e. k=jâˆ’1, or a forward/backward jump. In\nthe latter case, the pair (k, j)is a possible jump listed in the\nstructural information ï¬le as Table 1.\nNow we recursively deï¬ne a (m+ 1)Ã—(n+ 1) align-\nment cost matrix C, where the value C(i, j)is the lowest\ncost of the alignment between the initial sub-chromagrams\n(a1,Â· Â· Â·,ai)and(s1,Â· Â· Â·,sj). For all i= 1,Â· Â· Â·, mand\nj= 1,Â· Â· Â·, n,C(i, j)are calculated as follows:\nC(0,0) = 0 ,C(i,0) = iÂ·c1,C(0, j) = 0 (1)\nC(i, j) = minï£±\nï£´ï£²\nï£´ï£³C(i, jâˆ’1) + c1\nC(iâˆ’1, j) +c2\nminkâˆˆP(j)C(iâˆ’1, k) +d(ai,sj)\n(2)\nwhere c1andc2are constants specifying the costs of skip-\nping one segment of audio and score in the alignment, re-\nspectively. d(ai,sj)speciï¬es the cost of mismatching the\ni-th audio segment with the j-th score segment.\nNote that Eq. (1) is not symmetric, i.e. C(i,0)is set to\niÂ·c1, butC(0, j)is set to 0 instead of jÂ·c2. This means\nthat we penalize skipping audio segments at the beginning\nbut do not penalize skipping score segments, i.e. we as-\nsume that the performance can start anywhere but must be\non the lead sheet. Although sometimes performers play sev-\neral measures that are unrelated to the lead sheet at the be-\nginning, this is short compared to the whole performance\nand we ignore this case. In addition, the third line in Eq.(2)\nis calculated from C(iâˆ’1, k)for all possible parents kof\nthej-th score segment, while in an standard string alignment\nalgorithm it is only calculated from C(iâˆ’1, jâˆ’1). This al-\nlows the performance to play to the j-th score segment in all\npossible ways, either progress smoothly from the previous\nsegment jâˆ’1or jumping from other segments.\nThe mismatch cost function d(ai,sj)is deï¬ned as:\nd(ai,sj) = arccos(aT\nisj\nâˆ¥aiâˆ¥âˆ¥sjâˆ¥)\n(3)\nWe use cosine angle distance instead of Euclidean distance\nto make it loudness insensitive. This is because the loudness\nof the audio may vary from the loudness calculated from the\nscore differently in different performances. Since angle dis-\ntance between an arbitrary audio-score chroma vector pair is\naround 1, we set c1=c2= 1to match the three penalties.While calculating C, we ï¬ll another mÃ—nmatrix P,\nwhereP(i, j)stores the index pair (iâ€², jâ€²)from which C(i, j)\nis calculated in Eq. (2). When the calculation of Cis ï¬n-\nished, the ï¬nal alignment cost is calculated as minjC(m, j).\nLetj1= arg min jC(m, j). We than trace back from the in-\ndex pair (m, j 1)through Pto some index pair (1, j2). The\nsequence of index pairs (1, j2),Â· Â· Â·,(m, j 1)give the align-\nment between AandB. Note that the last pair is (m, j 1)\ninstead of (m, n). This allows the audio performance to end\nat any position of the score.\nIf we view each score segment as a state, each audio seg-\nment as an observation, then the proposed algorithm is es-\nsentially equivalent to the forward-backward algorithm for a\nHidden Markov Model (HMM) [12]. The transition matrix\nThas a positive value t1on the diagonal, corresponding to\nthe penalty of skipping an audio segment c1. It also has a\npositive value t2on the superdiagonal (elements (jâˆ’1, j))\nand elements (k, j)for all kâˆˆ P(j), corresponding to the\npenalty of skipping a score segment c2by smooth progres-\nsions and jumps, respectively. If c1=c2, then t1=t2. We\nalso notice that this algorithm is equivalent to the one pro-\nposed by Fremerey et al. [8], which also handles jumps and\nrepeats in synchronizing a score with a performance.\nFinally, for each audio-MIDI pair, we do the alignment\n36 times corresponding to the 36 MIDI chromagrams. The\nalignment that achieves the lowest ï¬nal alignment cost is\nselected as the output of the system.\n4. EXPERIMENT\n4.1 Dataset\nOur dataset consists of 36 semi-improvised performances of\n3 jazz lead sheets: Dindi by Antonio Carlos Jobim, Nicasâ€™s\nDream by Horace Silver and Without A Song by Vincent\nYoumans, selected from commonly used jazz fake books.\nFor each song, the performances consist of two subsets. The\nï¬rst subset contains MIDI recordings performed by profes-\nsional Chicago jazz pianists obtained from [9]. In [9], four\npianists each gave three different performances scaled to\nthree subjective levels of difï¬culty, ranging from a perfor-\nmance closely adhering to the given lead sheet to a more\nâ€œfreeâ€ interpretation. After recording, these pianists also\nannotated their own performances with beat, measure and\nstructural branch point information, encoded as MIDI data.\nWe include the two less difï¬cult levels into our dataset (de-\nnoted as easy and medium ), totalling 8 jazz piano perfor-\nmances for each song. We render these MIDI performances\ninto audio recordings with the Logic Audio software using\nGrand Piano sound samples. We use the pianistsâ€™ annota-\ntions to generate the ground-truth audio-score alignment.\nThe second subset contains 4 commercially released record-\nings for each lead sheet. Table 2 shows basic information for\nthem. To generate the ground-truth audio-score alignment,\n51612th International Society for Music Information Retrieval Conference (ISMIR 2011)\ntwo musicians listened to these recordings, marked beat and\nmeasure time points and identiï¬ed the score position (score\nmeasure number) of each measure of the audio. Audio mea-\nsures that are unrelated to the lead sheet (e.g. an improvised\ncadenza) were labeled score measure number 0.\nID Performer(s) InstrumentsDindi1 Astrud Gilberto female, violin, guitar\n2 Charlie Byrd guitar, saxophone\n3 Ohta San guitar\n4 Sadao Watanabe string, saxophoneNicaâ€™s...1 Art Farmer trumpet, trombone, brass\n2 Benjamin Koppel Quintet saxophone, piano, conga\n3 Cal Tjader vibraphone, piano\n4 The Hot Club violin, guitarWithout...1 Diane Schuur female, piano, bass\n2 Joe Henderson saxophone, brass, piano\n3 Oscar Peterson piano, brass\n4 Sonny Rollins saxophone, brass, guitar\nTable 2 . Improvised performances played by jazz bands.\nFor each improvised performance, we use two experi-\nmental settings. In the ï¬rst setting, we align the whole per-\nformance with the lead sheet. This is to observe our sys-\ntemâ€™s behavior on a larger time scale (usually several min-\nutes). In the second setting, we randomly select 10 excerpts\nof the performance and align them with the lead sheet. The\nlength of each excerpt ranges from 16 measures to 48 mea-\nsure. This is to observe our systemâ€™s behavior on a smaller\nscale (usually 30 seconds to 2 minutes) and would be repre-\nsentative of the task of selecting a portion of audio in a mu-\nsic player and asking to be shown the corresponding place\non the lead sheet. The second setting is in general more\nchallenging, as there is less context information.\n4.2 Evaluation Measures\nA commonly used measure for audio-score alignment is Align\nRate (AR) as proposed in [2]. It is deï¬ned as the percent-\nage of correctly aligned notes in the score, where â€œcorrectâ€\nmeans that the note onset is aligned to an audio time which\ndeviates less than a short time (e.g. 250 ms) from the ground-\ntruth audio time. In our problem, however, there is no bijec-\ntive correspondence between score notes and audio notes,\nhence it is very hard to deï¬ne the ground-truth audio time\nfor each score note and AR is not suitable.\nWe formulate our problem as a classiï¬cation problem,\nby assigning to each audio frame a score measure number.\nGiven this, we simply use Accuracy as our measure. It is\ncalculated as the proportion of audio frames which are cor-\nrectly assigned score measure numbers as the ground-truth.\nWe exclude those audio frames where the performance is\nunrelated to the score. This measure ranges from 0 to 1.4.3 Results\nFigure 3 shows overall results of aligning whole performances.\nAmong the 36 performances, 11 have accuracies higher than\n75%, 13 between 50% and 75%, while 6 lower than 10%.\nTheir average is 54.8%. It is noted that a random guess\nalignment would get an accuracy as the reciprocal of the\nnumber of measures on the lead sheet, about 2%.\nDNWDNWDNW00.20.40.60.81\nEasy piano    Medium piano   Jazz comboAccuracy\nFigure 3 . Alignment accuracies of all the 36 whole perfor-\nmances. â€™Dâ€™, â€™Nâ€™ and â€™Wâ€™ represents the lead sheet names\nDindi ,Nicaâ€™s Dream andWithout A Song , respectively.\nWe show three examples with different alignment accu-\nracies in Figure 4. In the upper panel, the systemâ€™s output\nalignment matches with the ground-truth perfectly except\nin two parts (51-58 seconds, 193 seconds - end). In both\nparts the performance is unrelated to the lead sheet. It is\nnoted that the accuracy measures always underestimate the\nperformance of the system, because the audio beat bound-\naries estimated by the beat tracking module are not perfectly\naligned with the ground-truth beat boundaries, hence the as-\nsigned score measure numbers of the audio frames that are\nclose to these boundaries are often off for Â±1measures.\nIn the middle panel, the performance sometimes repeats\nfrom the Intro section and sometimes from Section A. Our\nsystem handles this uncertain structural change well. How-\never, it incorrectly identiï¬es the two B sections around 150\nseconds (also the two B sections around 250 seconds) as\nonly one B section with about half the tempo. Interestingly,\nit comes back to the right position after this error. In addi-\ntion, after incorrectly identifying Section A (175-192 sec-\nonds) as C and B, the system identiï¬es another A section\n(192-210 seconds) as Section C. Since Section A and C are\nalmost the same on the lead sheet, this error is reasonable.\nExcluding this error causes accuracy to increase to 65.8%.\nIn the bottom panel, our system fails totally. Audio frames\nare constantly skipped after about 16 seconds. This exam-\nple played by Diane Schuur, however, is very difï¬cult. First,\nthere are four parts (0-12, 91-97, 162-165 seconds and 179\nseconds - end) that the performance is unrelated to the lead\nsheet. Second, the performance plays at half the tempo\n517Poster Session 4\nin Section C (142-162 seconds). Third, the performance\nswitches to a new key at 165 seconds till the end. The au-\ndio, MIDI and alignment results of these and other examples\ncan be accessed at http://www.cs.northwestern.\nedu/ Ëœzdu459/ismir2011/examples .\nIntroAABC(A)Acc: 87.9%\nDindi\nMedium piano\n0 50 100 150 20011725334148\nIntroABC(A)Acc: 57.4%Nica's DreamEasy piano)\n050100150200250210273752\nAB(A)CD(A)Acc:2.6%Without A SongJazz combo\n0 50 100 150118344257\nFigure 4 . Three alignment examples. The horizontal axis\nis audio time in seconds. The left vertical axis shows sec-\ntion names of the lead sheet. The right vertical axis and the\nhorizontal dash lines show the boundaries of the sections in\nmeasure numbers. Red solid lines show the systemâ€™s align-\nments. Blue dash lines show the ground-truth alignments.\nFigure 5 shows the average alignment accuracies over all\n360 performance excerpts with different chroma length land\nhop size hsettings. Our choice of l= 2, h= 1/4achieves\nan accuracy of 49.3%, which is one of the highest among\nall the parameter settings. This is in accordance to the anal-\nysis in Section 2.2. This result shows that with much less\ncontextual information, our system still works well on some\nhighly improvised audio excerpts.\n5. CONCLUSION\nIn this paper, we attempted to align semi-improvised mu-\nsic audio with its lead sheet. We proposed a simple sys-\ntem to align chromagram representations of audio and score\nbased on a modiï¬ed string alignment algorithm, which uti-\nlizes structural information of the lead sheet. Experiments\n1/81/41/2 1240.20.30.40.50.6\nChroma length/hop size (Beat)Accuracy\n  \nlength varies\nhop size variesFigure 5 . Average accuracies over all 360 excerpt perfor-\nmances, versus chroma length (ï¬x hop size = 1/4) or hop\nsize (ï¬x chroma length = 2).\non 36 audio performances and their 360 excerpts of 3 lead\nsheets showed promising results. This work is supported by\nNSF grant IIS-0643752.\n6. REFERENCES\n[1]A. Arzt and G. Widmer, â€œTowards Effective â€˜Any-Timeâ€™ Music\nTracking,â€ in Proc. of the Starting AI Researchers Symposium\n(STAIRS) , 2010.\n[2]A. Cont, D. Schwarz, N. Schnell and C. Raphael, â€œEvaluation\nof real-time audio-to-score alignment,â€ in Proc. ISMIR , 2007.\n[3]R.B. Dannenberg, W.P. Birmingham, B. Pardo, N. Hu,\nC. Meek, G. Tzanetakis, â€œA comparative evaluation of\nsearch techniques for query-by-humming using the MUSART\ntestbed,â€ Journal of the American Society for Information Sci-\nence and Technology , vol. 58, no. 3, 2007.\n[4]R.B. Dannenberg, C. Raphael, â€œMusic score alignment and\ncomputer accompaniment,â€ Commun. ACM , vol. 49, no. 8,\npp. 38â€“43, 2006.\n[5]R.B. Dannenberg and B. Mont-Reynaud, â€œFollowing an im-\nprovisation in real time,â€ in Proc. ICMC , 1987, pp. 241â€“248.\n[6]D. Ellis, â€œBeat tracking by dynamic programming,â€ J. New\nMusic Research, Special Issue on Beat and Tempo Extraction ,\nvol. 36 no. 1, pp. 51â€“60, 2007.\n[7]D. Ellis and G. Poliner, â€œIdentifying â€˜cover songsâ€™ with\nchroma features and dynamic programming beat tracking,â€ in\nProc. ICASSP , 2007.\n[8]C. Fremerey, M. M Â¨uller, M. Clausen, â€œHandling repeats and\njumps in score-performance synchronization,â€ in Proc. ISMIR ,\n2010.\n[9]J. Moshier and B. Pardo, â€œA database for the accommodation\nof structural and stylistic variability in improvised jazz piano\nperformances,â€ ISMIR, Late-Breaking/Demo Session , 2008.\n[10] B. Pardo and W. Birmingham, â€œFollowing a musical perfor-\nmance from a partially speciï¬ed score,â€ in Proc. IEEE Multi-\nmedia Technology and Applications Conference , 2001.\n[11] B. Pardo and W. Birmingham, â€œModeling form for on-line fol-\nlowing of musical performances,â€ in Proc. AAAI , 2005.\n[12] L.R. Rabiner, â€œA tutorial on hidden Markov models and se-\nlected applications in speech recognition.â€ in Proc. IEEE ,\nvol. 77, no. 2, pp. 257â€“286, 1989.\n518"
    },
    {
        "title": "Music Structure Segmentation Algorithm Evaluation: Expanding on MIREX 2010 Analyses and Datasets.",
        "author": [
            "Andreas F. Ehmann",
            "Mert Bay",
            "J. Stephen Downie",
            "Ichiro Fujinaga",
            "David De Roure"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1418151",
        "url": "https://doi.org/10.5281/zenodo.1418151",
        "ee": "https://zenodo.org/records/1418151/files/EhmannBDFR11.pdf",
        "abstract": "Music audio structure segmentation has been a task in the Music Information Retrieval Evaluation eXchange (MIREX) since 2009. In 2010, five algorithms were evaluated against two datasets (297 and 100 songs) with an almost exclusive focus on western popular music. A new annotated dataset significantly larger in size and with a more diverse range of musical styles became available in 2011. This new dataset comprises over 1,300 songs spanning pop, jazz, classical, and world music styles. The algorithms from the 2010 iteration of MIREX are re-evaluated against this new dataset. This paper presents a detailed analysis of these evaluation results in order to gain a better understanding of the current state-of-the-art in automatic structure segmentation. These expanded analyses focus on the interaction of algorithm performance and rankings with datasets, musical styles, and annotation level. Because the new dataset contains multiple annotations for each song, we also introduce a baseline for expected human performance for this task.",
        "zenodo_id": 1418151,
        "dblp_key": "conf/ismir/EhmannBDFR11",
        "keywords": [
            "Music Structure Segmentation",
            "MIREX Evaluation",
            "Structural Analysis of Music",
            "Music Information Retrieval (MIR)",
            "SALAMI Project",
            "Dataset Annotation",
            "Western Popular Music",
            "Algorithm Performance Evaluation",
            "Music Styles Diversity",
            "Human Performance Baseline"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\n  \n \n MUSIC STRUCTURE SEGMENTATION ALGORITHM \nEVALUATION: EXPANDING ON MIREX 2010 ANALYSES AND \nDATASETS  \nAndreas F. Ehmann1    Mert Bay1    J. Stephen Downie1    Ichiro Fujinaga2    David De Roure3 \n1GSLIS  \nUniversity of Illinois Urbana -\nChampaign  \n{aehmann, mertbay, \njdownie} @illinois .edu 2Schulich School of Music  \nMcGill University  \nich@music.mcgill.ca  3Oxford e -Research Centre  \nUniversity of Oxford  \ndavid.deroure@oerc.ox.ac.uk  \nABSTRACT  \nMusic audio structure segmentation has been a task in the \nMusic Information Retrieval Evaluation eXchange \n(MIREX) since 2009 . In 2010, five algorithms were ev a-\nluated against two datasets (297 and 100 songs) with an al-\nmost exclusive focus on western popular music. A new  an-\nnotated  dataset  significantly larger in size and with a more \ndiverse range of mus ical styles became available in 2011 . \nThis new dataset comprises over 1,3 00 songs spanning pop, \njazz, classical, and world music styles. The algorithms from \nthe 2010 iteration of MIREX are re -evaluated against this \nnew dataset. This paper presents a detailed analysis of these \nevaluation re sults in order to gain a better understanding of \nthe current state -of-the-art in automatic structure segment a-\ntion. These expanded analyses focus on the interaction of \nalgorithm performance and rankings wi th datasets , musical \nstyle s, and annotation level . Because the new dataset co n-\ntains multiple annotations for each song, we also introduce \na baseline for expected human performance for this task.   \n1. INTRODUCTION  \nThe structural, or formal , analysis  of music is one of the  \nmost fundamental of analyses performed by musicologists. \nVery simply, the main goal of structural analysis is to se g-\nment music into sections that share similar characteristics, \nand apply labels to these sections . These segmentations \ntake forms such as AAB B, or ABAC, etc. With further \nanalysis, certain descriptors can also be applied to these \nsection s, such as verse, chorus, and so on [3]. \nIn recent years, there has been increasing interest in  de-\nvelop ing methods for performing structural  analyses aut o-\nmatically. For a good overview on the state of automatic \nmusic audio structural segmentation we refer the reader to [10]. The growing interest in structural segmentation alg o-\nrithms is evidenced by the establishment of the structural \nsegmentation task of  the Music Information Retrieval Eva l-\nuation eXchange (MIREX) campaign [ 2]. Evaluations of \nstructural segmentation algorithms were performed  in 200 9 \nand 2010 . These evaluations were performed over colle c-\ntions  with a strong bias towards western, popular music.  \nTo perform a novel and potentially more thorough eva l-\nuation of the performance of structural segmentation alg o-\nrithms, the set of algorithms submitted to MIREX 2010 i n \nJuly 2010 was re-evaluated in May 2011 using a newly \nconstructed dataset . For the purposes o f this paper, we are \ncalling this new test collection the MIREX 2010 Version 2 \n(MRX10V2) dataset . MRX10V2  is much larger in size than  \nthe da tasets used in earlier MIREX  evaluations. It also con-\ntains a much broader range of music styles. Moreover, the \nMRX10V2 database contains multiple annota tions per \npiece. Having multiple annotations per song allows us, for \nthe first time, to  explore how well  algorithms perform this \ntask relative to human experts .  \nThe main motivation for this work stems from an ong o-\ning project called the Structural Analysis of Large Amounts \nof Music Information (SALAMI)  [3]. The SALAMI project \nis an endeavor to use music stru cture algorithms to annotate \nand segment a large corpus of music (on the order of \n300,000 songs). Its main goal is to test the feasibility and \nusefulness of current music information retrieval algorithms \non a larger scale than has commonly been performed. As a \npilot to the SALAMI project, the work presented in this p a-\nper aims to further our understanding of how current state -\nof-the-art algorithms perform at music segmentation.  \nThe rest of this paper is formatted as follows. Section 2 \ngives a description of the dataset used in this mid -cycle \nMIREX evaluation. Section 3 briefly describes the alg o-\nrithms. Section 4 presents the evaluation results.  Section 5 \noffers  some conclusions  and suggests future work .   \nPermission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fee provided that copies are \nnot made or distributed for profit or commercial advantage and that co p-\nies bear this notice and the full citation on the first page.   \nÂ© 2011  International Society for Music Information Retrieval  \n561Poster Session 4\n  \n \n 2. DATASET S: OLD AND NEW  \nThe evaluation of structure segme ntation algorithms on a \nlarge dataset requires the creation of a suitable ground \ntruth . As in virtually all cases of MIREX -style evaluations, \nground truth creation is carried out by human annotators. \nThe use of human annotators brings up two significant \nchallenges. First, there is a large labor cost involved in m a-\nnually annotating music pieces. Second, and perhaps more \nimportantly, is the notion that it is difficult to truly assert \nthat any subjective interpretation of something as complex \nas musical form i s â€œtruth. â€ Many considerations must be \ntaken into account regarding such annotations. Both [ 1] \nand [11] lay out methodologies  for annotating musical \nstruc ture. In this work , the dataset , and subsequent annot a-\ntion methodologies  described in [ 14] are used.  \nThe MIREX 2009 and the MIREX 2010  iterations of the \nMIREX structural segmentation task had an over bias t o-\nward popular music. The dataset known as MIREX 2009 \ncontains 297 popular song annotations donated by Tampere \nUniversity of Technology, Vienna Universi ty of Technol o-\ngy and Queen Mary, University of London. Music of The \nBeatles  makes up a significant proportion of the MIREX \n2009 dataset. The MIREX 2010 dataset consists of an  anno-\ntated version of the RWC  [4] database's popular music col-\nlection . Note that the published results to the MIREX 2010 \ndataset are evaluated against a ground truth  donated by  \nmembers of the QUAERO Project.1 However, these annot a-\ntions consist of only segment boundary annotations with no \nlabeling. Hereafter,  results pertainin g to the MIREX 2010 \ndataset are evaluated against the original , labeled  structural \nannotations as distributed with the RWC collection.  \nIn order to compensate for  the popular music bias exh i-\nbited by the older datasets, the new MRX10V2 dataset was \ndeliberate ly create d to include a much wide r variety of \nmusical styles . In addition to popular music, the new dat a-\nset contains classical, jazz, live, and world music. Table 1 \npresents the distribution of styles across the MRX10V2 d a-\ntaset.  While â€œliveâ€ may not truly be considered a musical \nstyle, live pieces are separated as they raise unique co n-\ncerns such as applause sections, etc.  \n    The â€œDouble -keyedâ€  pieces noted in Table 1 are those \nthat have been annotated by two separate individuals.  As \nTable 1 shows , the majo rity of pieces  (1048 of 1383)  have \nbeen anno tated by two annotators . In addition, each annot a-\ntion of a piece contains two levels of structural h ierarchy. \nThere is a fine -grained annotation and a coarse grain ed an-\nnotation, with each coarse -grained segm ent c omprising one \nor more fine -grained segments. Therefore, a â€œfineâ€ annot a-\ntion may have form abaabacdaba , with equivalent â€œcoarse â€ \nannotation of AABA  where A represents an aba  \n                                                           \n1 See http://www.quaero.org .  Table 1. Breakdown of the MRX10V2 structure segment a-\ntion dataset by musical style.  \nsequence and B represents a cd sequence.  The ne w dataset \ncontains 1,383 pieces  which is over 4 times larger than ear-\nlier datasets used for evaluation.  \n3. ALGORITHMS  \nThe algorithms used in this off -cycle MIREX  evaluation \nare the same as the  ones submitted to  MIREX  2010 . Five \nunique algorithms, including one with two  distinct  param e-\nter settings  (resulting in six overall algorithms ), were run \nagainst the new  1,383 song  dataset and evaluated . The alg o-\nrithms are referred to in this paper using the code names \nassigned to them during MIREX 201 0.2   \nEach of the algorithms under evaluation is composed of \na unique combination of extracted features, segmentation \nmethods, and labeling/grouping techniques. BV1 -2 [13] \nuses beats, Mel Frequency Cepstral Coefficients ( MFCCs ) \nand chroma vectors  as featur es, segments the song base d on \ngeneralized likelihood s of three  different criteria and gat h-\ners the segments using agglomerative hierarchical  cluste r-\ning. GP7 [12] uses MFCC, c hroma vectors, spectral flatness \nand valley factors as features, calculates a weighted sum of \n4 different distance matrices that is used to segment the \nsignal. The segments are merged using hierarchical aggl o-\nmerative clustering.  MHRAF2 [ 8] uses chroma feat ures and \nemploys string matching techniques to identify strong ha r-\nmonic redundancies using an iterative detection of major \nrepetitions . MND1  [9] uses c hroma vectors  and calculates a \nsimilarity matrix using Pearson 's correlation coefficient. \nMND1  searches t he diagon als for repeated sequences and \nuses a greedy algorithm to decide on the segments.  WB1  \n[16] uses beat sync hronous chromagram s decompo sed into \nbasis patterns by  shift-invariant probabilistic latent comp o-\nnent analysis as features. Song s are segmented by comp u-\nting the path of the basis patterns through a likelihood fun c-\ntion that represents the structure of the song  using the V i-\nterbi algorithm .   \n                                                           \n2 See http://nema/mirex/wiki/2010:MIREX2010  Style  Double -\nkeyed  Single -\nkeyed  Total  Percentage  \nClassical  159 66 225 16% \nJazz 225 12 237 17% \nPopular  205 117 322 23% \nWorld  186 31 217 16% \nLive 273 109 382 28% \nTotal  1048  335 1383  100%  \n56212th International Society for Music Information Retrieval Conference (ISMIR 2011)\n  \n \n On average , the runtimes for the algorithms is appro x-\nimately  two to six minutes per file. Table 2 present s the a v-\nerage runtimes per -file for each algorithm . We can see that \nmost algorithms run roughly real -time. Therefore, any very \nlarge -scale effort to automatically segment music audio will \nrequire significant computational resources.  \n \nAlgorithms  Average processing time \n(min. / piece)  \nWB1 [ 16] 2.28 \nGP7 [ 12] 2.64 \nBV1 & BV2 [ 13] 2.94 \nMND1 [ 9] 5.60 \nMHRAF2 [ 8] 6.38 \n \n  \n4. EVALUATION AND RESUL TS \n4.1 Evaluation Method s \nThe same evaluation methods and metrics used in previous \nstructural segmentation MIREX evaluations were used to \nevaluate the algorithms. The boundary retrieval metrics of \n[15] evaluate how close segment boundaries between alg o-\nrithm result s and ground truth  are in time . This metric is \nlabel -agnostic and simply measures the segmentation of the \npiece and not whether similar section s are similarly labeled. \nThe â€œhit rate â€ of the boundary retrieval measures if a r e-\nturned segment boundary is within T seconds of a ground \ntruth boundary. The hit rate is measured at two time -\nthresholds: T = 0.5 s and T = 3.0 s. The segment boundary \nhit rat e measures encompass an F-measure ( SBR-F), as well \nas a precision ( SBR-P) and recall ( SBR-R) measure. In add i-\ntion, the median deviation, in seconds, between de tected and ground truth boundaries is measured.  AB-2-RB meas-\nures the median time difference betwe en an annotated \nboundary and the nearest result boundary. Similarly , RB-2-\nAB measures the median time difference between a result \nboundary and the nearest annotated boundary.  \nFrame -pair clustering, as introduced in [ 6], divides the \nresults and ground truth into short time frames (e.g. 100 \nms). This metric then considers every possible pair of \nframes  and their corresponding labels . Denoti ng the set of \nall frame -pairs  that share the same label (i.e. , same cluster) \nin the result as PE, and likewise the set of all frame -pairs \nsharing the same label in the ground truth as PA, we can d e-\nfine the pairwise precision, P, pairwise recall, R, and pai r-\nwise F -measure, F as \n         \n                     \n                 \n                                                          \nThe frame -pair clustering F -measure, precision, and recall \nare to as FPC -F, FPC -P, and FPC -R, respectively.  \nThe normalized conditional entropies introduced in [ 7] \nalso represent st ructural annotations as sequences of short \nframes, similar to the frame -pair clustering metrics. Cond i-\ntional entropies are calculated and normalized to yield a \nmeasure in [0, 1], the details of which are beyond the scope \nof this paper and can be found in t he reference. The norm a-\nlized conditional entropy measures are a dual measure with  \nover-segmentation  (NCE -OSS) and under -segmentation \nscores  (NCE -USS). Because structure annotation can exist \nat multiple levels of granularity (as it does in the  new \nground t ruth), the two metrics will indicate if an algorithm \ntended to be too coarse (low under -segmentation score) or \ntoo fine (low over -segmentation scores). Finally, a random \nclustering index (RCI) measure is also calculated  [5]. Table 2.  Algorithm names, corresponding references, \nand runtimes.  \n(a) \nAlgorithm  NCE -\nOSS NCE -\nUSS FPC-\nF FPC-\nP FPC-\nR RCI SBR -\nF@0.5 s SBR -\nP@0.5 s SBR -\nR@0.5 s SBR -\nF@3s SBR -\nP@3 s SBR -\nR@3 s AB-2-\nRB RB-2-\nAB \nBV1  0.605  0.441  0.520  0.513  0.669  0.549  0.190  0.151  0.289  0.450  0.361  0.669  1.797  7.554  \nBV2  0.454  0.715  0.427  0.678  0.350  0.638  0.189  0.150  0.286  0.449  0.361  0.666  1.812  7.552  \nGP7 0.499  0.683  0.485  0.675  0.424  0.654  0.188  0.146  0.306  0.440  0.346  0.695  2.073  6.634  \nMHRAF2  0.546  0.591  0.559  0.617  0.583  0.659  0.195  0.218  0.197  0.435  0.485  0.440  7.262  5.338  \nMND1  0.624  0.625  0.556  0.649  0.586  0.662  0.291  0.302  0.326  0.470  0.479  0.534  8.565  5.389  \nWB1  0.609  0.540  0.546  0.583  0.608  0.630  0.237  0.240  0.272  0.393  0.395  0.446  10.780  3.881  \n(b) \nAlgorithm  NCE -\nOSS NCE -\nUSS FPC-\nF FPC-\nP FPC-\nR RCI SBR -\nF@0.5 s SBR -\nP@0.5 s SBR -\nR@0.5 s SBR -\nF@3s SBR -\nP@3 s SBR -\nR@3 s AB-2-\nRB RB-2-\nAB \nBV1  0.643  0.323  0.384  0.321  0.680  0.505  0.179  0.236  0.159  0.567  0.744  0.499  2.905  2.007  \nBV2  0.521  0.567  0.373  0.452  0.386  0.712  0.177  0.234  0.157  0.565  0.741  0.497  2.937  1.980  \nGP7 0.584  0.557  0.432  0.467  0.482  0.720  0.163  0.208  0.153  0.472  0.605  0.436  4.946  2.300  \nMHRAF2  0.599  0.442  0.440  0.395  0.615  0.655  0.124  0.276  0.087  0.356  0.776  0.253  11.311  1.885  \nMND1  0.666  0.478  0.435  0.426  0.609  0.635  0.200  0.376  0.150  0.415  0.749  0.314  13.944  1.835  \nWB1  0.675  0.420  0.442  0.382  0.653  0.632  0.148  0.277  0.112  0.317  0.588  0.239  16.031  1.975  \nTable 3.  Evaluations against coarse (a) and fine (b) ground truth annotations.  \n \n563Poster Session 4\n  \n \n 5.  RESULTS  & DISCUSSION  \nThe evaluation results of the six algorithms  using the new \nMRX10V2 dataset  can be seen in Table s 3a (coarse -\ngrained) and 3b (fine-grained) . The figures in the tables \nrepresent weighted averages over the dataset, where the a v-\neraging was carried out as follo ws. All algorithms were \nevaluated over a single ground truth for the entire annotated \ndataset (~1300 pieces). Those pieces that were double -\nkeyed were then used as a second ground truth and  sepa-\nrately evaluated. These two separate evaluations were then \nweighted by the number of pieces in each set and averaged \nto produce the final results.  \nWe take immediate note that the algorithms tend to per-\nform better when  evaluated using the coarser of the two \nhuman annotations (mostly evidenced by the FPC -F meas-\nure and  low NCE -USS scores in Table 3b).  With regard to \nthe FPC -F data, the average performance for all algorithm s \nusing the coarse -grained  ground truth is 0.520 versus 0.423 \nfor the fin e-grained. A Friedman's ANOVA test1  run using \nthe FPC -F measure data  confirms that there exists a stati s-\ntically significant difference in performance between the \ncoarse and fine result sets ( p=0.0 1). This result is not su r-\nprising, as the algorithms are designed for coarse annot a-\ntion. We will talk about the relative performa nces of alg o-\nrithms using  only the coarse FPC -F scores later.  \nIn comparing the MRX10V2 results with  the previous \nMIREX datasets, we see that the evaluation results for all \nalgorithms seem to be in the same general range . Using \nFPC -F-measure  for comparison  (as it provides a good ba l-\nance between segmentation and labeling accuracy) , Table 4 \ncontains algorithm performances on the new dataset, the \nMIREX 2009 dataset , and the MIREX  2010 dataset. In \ngeneral, average performance seems to be slightly worse  on \nthe new MRX10V2 dataset. Some algorithms seem to have \nbeen more strongly affected, with significant performance \ndrops (e.g. BV2 and GP7). Some algorithms, however, also \nimproved slightly on the MRX10V2  dataset  over the \nMIREX  2009 dataset  (e.g. MND1 and  WB1) . The smallest \ndataset, RWC, appears to generate the best performances.  A \nFriedman's ANOVA test run against the Table 4 data ind i-\ncated a statistically significant difference in performance \namong the three datasets ( p=0.02 ). A subsequent Tukey -\nKramer H onestly Significant Difference (TKHSD) test tells \nus that the MIREX  2010 collection results are significantly \ndifferent than the other two collections. The same TKHSD \nalso shows that MIREX  2009 and MRX10V2 are not di f-\nferent from each other. We suspect that  the MIREX 2010 \nresults are significantly better than the other two datasets \n                                                           \n1 See [2] for an in -depth discussion of the applications of \nFriedman's ANOVA and the Tukey -Kramer Honestly Si g-\nnificant Difference (TKHSD) tests used in MIREX.  because the RWC popular music database which makes up \nthe MIREX 2010 set was artificially  composed and pe r-\nformed  to represent generic popular musi c and to overcome \ncopyright probl ems. \n \nTable 4 . Comparison of algorithms over datasets  \nRecall that the earlier  MIREX datasets have a strong bias \ntoward western popular music.  As mentioned in S ection 2, \nMRX10 V2 dataset was deliberately created to represent a \nwider range of musical styles to evalu ate algorithmic pe r-\nformance across different genres. Table 5 presents a  break-\ndown of algorithm per formance across musical styles.  \nAgain, the  FPC -F measure is used as a summary measure \nfor comparison , and only the coarse annotations are cons i-\ndered . A Friedman's ANOVA test run against the Table 5 \ndata indicates  that there is no statistically significant diffe r-\nences in performance across  musical style s (p=0.9 0). This \nis a promising result because it suggest s that although, to \ndate, most algorithms have b een evaluated on popular m u-\nsic, they do seem to perform reasonably  well on other \nstyles.  Such a claim is not meant to imply that individual \nalgorithms do not perform significantly better on some \nmusical styles than others. Rather, when all algorithms are \nlooked at as a whole, musical style does not seem to have a \nlarge effect  (i.e., individual idiosyncrasies average out) . \n \nTable 5 . Results by musical style  considering only coarse \nannotations . \n \n Algorithm  MIREX09  MIREX10  MRX10V2  Ave.  \nBV1  0.502  0.520  0.520  0.514  \nBV2  0.493  0.531  0.427  0.484  \nGP7 0.536  0.592  0.485  0.538  \nMHRAF2  0.555  0.600  0.559  0.571  \nMND1  0.613  0.625  0.556  0.598  \nWB1  0.544  0.602  0.546  0.564  \nAve.  0.541  0.578  0.516  0.545  \nAlgorithm  Live Classical  Jazz Popular  World  Ave.  \nBV1  0.504  0.513  0.544  0.519  0.521  0.520  \nBV2  0.432  0.426  0.398  0.451  0.439  0.429  \nGP7 0.510  0.427  0.475  0.513  0.484  0.482  \nMND1  0.532  0.564  0.574  0.574  0.545  0.558  \nMHRAF2  0.557  0.590  0.556  0.543  0.555  0.560  \nWB1  0.560  0.524  0.547  0.548  0.537  0.543  \nAve.  0.516  0.507  0.516  0.525  0.514  0.515  \n56412th International Society for Music Information Retrieval Conference (ISMIR 2011)\n  \n \n  \nTable 6 . Fined -grained vs. coarse -grained FPC -F results.  \n5.1 Best Performances: Algorithms vs. Humans  \nIn order to gain some general idea on how a human might \nperform relative to  another human using the standard  \nevaluat ion measures, the ground trut hs of all double -keyed \nfiles were  compared. We perform ed the human -to-human \ncomparison on both the coarse and fine -grained annotations. \nThe double -keyed subset allows us to evaluate the human -\ngenerated annotations in the same m anner as the alg o-\nrithms. The Human  results li ne in Table 6 was generated by \ndecla ring one human annotation set to be an â€œalgorithm â€ \nwhile the other played the role of â€œground truth. â€ The \nalgorithms were  evaluated on only this subset of the data to \nallow fo r direct comparison o f the results on the 794 \ndouble -keyed pieces that have both fine and coarse \nannotations . \nTable 6 shows that structural annotation by music \nexperts  seems to be  itself somewhat subjective. For \nexample the average coarse -grained FPC -F score is 0.721. \nThis indicates that some disagreement d oes exist amongst \nhuman experts.  A higher degree of disagreement exists for \nthe fine -grained annotations .  \nWhile  algorithmic segmentations seem  to perform \nsimilarly to each other, automatic segmentation has not \nreached human performance. We performed Friedman's \nANOVA on the coarse -grained FPC -F scores for the \nalgorithmic  and  human annotations across 794 tracks. At \np<0.01, the Friedman's test in dicates a statistically \nsignificant difference in performance among the annotation \nsources. The subsequent TKHSD multiple comparison tests \nshow a set of four distinct performance groupings with each \ngroup being significantly different from  the other groups  \n(with no significant differences with each grouping). Figure \n1 presents the results of the TKHSD test.  \nIn the first performance group, we find, by itself, the \nresults for the human annotations. These are noticeably \nbetter than any of the algorithmic results. This is to be \nexpected given the relatively few years the community has \nbe working on the structural  segmentation problem. The \nsecond grouping  (highlighted by an oval in Figure 1)  consists of MHRAF2, MND1, and WB1.  These three \nalgorithms are not significantly different.  BV1, GP7, and \nBV2 all have statistically significant performance \ndifferences . These r esults remind us of t wo important facts. \nFirst, the top performing algorithms are not significantly \ndifferent in the  MIREX 2010 Version 2 evaluations. We \nneed to look at the stronger algorithms as a group to see \nwhat factors can be merged to build an impro ved \nsegementation system. Second, notwithstanding human \nvariations in strucutural annotations, we as a community \nstill have a great way to go before our structural \nsegmentation algorithms can be said to be acheiving \nhuman -like performances . \n  \n  \nFigure 1. Tukey -Kramer HSD comparison  plots of \nthe human and algorithm mean performance ranks \nacross 794 double -keyed tracks  \n6. CONCLUSIONS  AND FUTURE WORK  \nIn this paper we reported upon t he most extensive  \nevaluation of music structure segmentation algorithms to \ndate. Our evaluation was performed on a new dataset \nspanning multiple musical styles.  Top-ranked techniques \nfor th e autom atic segmentation of music quantitatively \nperform similarly. Musical style does not seem to have an \nadverse affect on general performance, but individual \nalgorithms have a nonuniform performances across styles.  \nWe can also conclude that the state of automatic \nsegmentation is relatively immature. Even  though we \nassert that structural or formal analysis is in itse lf a \nsubjective endeavor, the comparison of two human \nannotators to one another far outperforms current \nalgorithms. In summary, we have no current single \ntechnique that is  clearly better than the others  and none \napproach the capabilities of a music expert in this task.  \nThe evidence that there is still a large room for \nimprovement of current segmentation algorithms does not \npreclude them from being useful in their present form. Even \nHuman MHRAF2 MND1 WB1 BV1 GP7 BV222.533.544.555.566.5Mean Column RanksAlgorithm  Fine Coarse  \nBV1  0.392  0.525 \nBV2  0.371  0.434 \nGP7 0.433  0.485  \nMHRAF2  0.448 0.565 \nMND1  0.442  0.559  \nWB1  0.449  0.552  \nHuman  0.629 0.721  \n565Poster Session 4\n  \n \n though it is understood that the algorithms have not yet met \nthe sort of baseline that most researchers set for themselves \n(i.e., approaching human performance) it is important to \nnote that these goals are far from being met in many facets \nof music information retrieval  (MIR) , be it chord \nestimation, mult ipitch detection, and s o forth. The primary \ngoal of the SALAMI project, and much of the future work \nthat will stem from the evaluation performed here, is to \nassess just how u seful current MIR algorithms can be.  \nFor future work, we see the need to increase the size of \nour test collections. We would like to gather more \nannotations per song to augment  our ability to explore the \nsimilarities and differences in human segmenting \nperceptions. We would also like to expand  the number of \nstyles and time periods repre sented in our test collections. \nFinally, we would like to perform a set of failure analyses \non those songs that consistently scored poorly in order to \ndiscern what musical traits might be proving difficult for \nthe annotators, both human and algorithmic, to  process.  \n7. ACKNOWLEDGMENTS  \nWe would like to thank the past music structure \nsegmentation MIREX participants for allowing us access \nand use of their algorithms in evaluating them on this new \nstructure dataset.  This material is based upon work \nsupported by th e National Science Foundation under Grant \nNo. IIS 10 -42727.  \n8. REFERENCES  \n[1] F. Bimbot, O. Le Blouch, G. Sargent, and E. Vincent. \nâ€œDecomposition into Autonomous and Comparable \nBlocks: A Structural Description of Music Pieces, â€ \nProceedings of the 11th Internation al Society for Music \nInformation Retrieval Conference , pp. 189â€“94, 2010.  \n[2] J.S. Downie . â€œThe Music Informaton Retrieval Eval u-\nation Exchange (2005 -2007): A Window into Music \nInformation Retrieval Research, â€ Acoustical Science \nand Technology , 29 (4), pp. 247 -55, 2008.  \n[3] A.F. Ehmann, M. Bay, J.S. Downie, I.Fujinaga, D. De \nRoure. â€œExploiting Music Structures for Digital Libr a-\nries, â€œ Proceedings of the ACM/IEEE Joint Conference \non Digital Libraries,  pp. 479 -80, 2011.  \n[4] M. Goto , H. Hashiguchi, T. Nishimura, and R. Oka. \nâ€œRWC Music Database: Popular, classical, and jazz \nmusic databases ,â€ Proceedings of the International \nConference on Music Information Retrieval , pp. 287 â€“8, \n2002.  \n[5] L. Hubert and R. Arabie. â€œComparing Partitions, â€ \nJournal of Classification , 2 (1), 193 -218, 1985.  [6] C. Levy and M. Sandler . â€œStructural Segmentation of \nMusical Audio by Constrained Clustering ,â€ IEEE \nTransaction on Audio, Speech, and Language \nProcessing  , 16 (2) , 318â€“26, 2008.  \n[7] H. Lukashevich. â€œTowards Quantita tive Measures of \nEvaluating Song Segmentation, â€ Proceedings of the \nInternational Conference on Music Information R e-\ntrieval , pp. 375â€“80, 200 8. \n[8] B. Martin, P. Hanna, M. Robine, and P.Ferraro. â€œIn-\ndexing Musical Pieces Using their Major Repetition, â€ \nACM/IEEE Jo int Conference on Digital Libraries , Ot-\ntawa, Canada, 2011 . \n[9] M. Mauch , K. C. Noland, and S. Dixon . â€œUsing mus i-\ncal structure to enhance automatic chord tran scription, â€ \nProceedings of the International Society for Music I n-\nformation Retrieval Conference,  231â€“6, 2009.  \n[10] J. Paulus, M. Mueller, and A. Klapuri : â€œState of the \nArt Report: Audio -Based Music Structure Analysis ,â€ \nProceedings of the 11th International Society for  Music \nInformation Retrieval  Conference , pp. 625â€“36, 2010. \n[11] G. Peeters and E. Deruty . â€œIs music structure annot a-\ntion multi -dimensional? A proposal for robust local \nmusic annotation ,â€ Proceedings of the International \nWorkshop on Learning the Semantics of Audio Signals , \npp. 75â€“90, 2009.  \n[12] G. Peeters . â€œSequence representation of music stru c-\nture usin g higher -order similarity matrix and max i-\nmum likelihood approach ,â€ Proceeding s of the Inte r-\nnational Conference on Music Information Retrieval , \npp. 35 -40, 2007 . \n[13] G. Sargent , F. Bimbot, and E. Vincent . â€œUn systÃ¨me \nde dÃ©tection  de rupture de timbre pour la description \nde la structure des morceaux de musique ,â€  Procee d-\nings of JournÃ©es dâ€™Informatique Musicale,  pp. 177â€“86, \n2010 . \n[14] J.B.L. Smith, J. A. Burgoyne, I.  Fujinaga, D . De Roure \nand J. S. Downie .  â€œDesign and creation of a large -\nscale database of structural annotations ,â€ Proceedings \nof the 1 2th International Society for Music Information \nRetrieval Conference , 2011.  \n[15]  D. Turnbull , G. Lanckriet , E Pamalk, and M. Goto, \nâ€œA Supe rvised Approach for Detecting Boundaries in \nMusic Using Difference Features and Boosting, â€ Pro-\nceedings of the International Conference on Music I n-\nformation Retrieva l,  pp. 51-54, 200 7. \n[16] R. J. Weiss and J. P. Bello. â€œIdentifying repeated pa t-\nterns in music using sparse convolutive non -negative \nmatrix f actorization ,â€ Proceedings of the International \nSociety for Music Information Retrieval Conference , \npp. 123 -8, 2010 . \n566"
    },
    {
        "title": "Semantic Annotation and Retrieval of Music using a Bag of Systems Representation.",
        "author": [
            "Katherine Ellis",
            "Emanuele Coviello",
            "Gert R. G. Lanckriet"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1416390",
        "url": "https://doi.org/10.5281/zenodo.1416390",
        "ee": "https://zenodo.org/records/1416390/files/EllisCL11.pdf",
        "abstract": "We present a content-based auto-tagger that leverages a rich dictionary of musical codewords, where each codeword is a generative model that captures timbral and temporal characteristics of music. This leads to a higher-level, concise â€œBag of Systemsâ€ (BoS) representation of the characteristics of a musical piece. Once songs are represented as a BoS histogram over codewords, traditional algorithms for text document retrieval can be leveraged for music autotagging. Compared to estimating a single generative model to directly capture the musical characteristics of songs associated with a tag, the BoS approach offers the flexibility to combine different classes of generative models at various time resolutions through the selection of the BoS codewords. Experiments show that this enriches the audio representation and leads to superior auto-tagging performance.",
        "zenodo_id": 1416390,
        "dblp_key": "conf/ismir/EllisCL11",
        "keywords": [
            "content-based",
            "auto-tagger",
            "rich dictionary",
            "musical codewords",
            "generative models",
            "timbral characteristics",
            "temporal characteristics",
            "Bag of Systems",
            "musical piece",
            "Bag of Systems representation"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nSEMANTIC ANNOTATION AND RETRIEVAL OF MUSIC USING A BAG OF\nSYSTEMS REPRESENTATION\nKatherine Ellis\nUniversity of California,\nSan Diego\nkellis@ucsd.eduEmanuele Coviello\nUniversity of California,\nSan Diego\necoviell@ucsd.eduGert R.G. Lanckriet\nUniversity of California,\nSan Diego\ngert@ece.ucsd.edu\nABSTRACT\nWe present a content-based auto-tagger that leverages a\nrich dictionary of musical codewords, where each codeword\nis a generative model that captures timbral and temporal\ncharacteristics of music. This leads to a higher-level, con-\ncise â€œBag of Systemsâ€ (BoS) representation of the charac-\nteristics of a musical piece. Once songs are represented as a\nBoS histogram over codewords, traditional algorithms for\ntext document retrieval can be leveraged for music auto-\ntagging. Compared to estimating a single generative model\nto directly capture the musical characteristics of songs as-\nsociated with a tag, the BoS approach offers the ï¬‚exibility\nto combine different classes of generative models at vari-\nous time resolutions through the selection of the BoS code-\nwords. Experiments show that this enriches the audio repre-\nsentation and leads to superior auto-tagging performance.\n1. INTRODUCTION\nGiven a vast and constantly growing collection of online\nsongs, music search and recommendation systems increas-\ningly rely on automated algorithms to analyze and index\nmusic content. In this work, we investigate a novel ap-\nproach for automated content-based tagging of music with\nsemantically meaningful tags (e.g., genres, emotions, instru-\nments, usages, etc.). Most previously proposed auto-taggers\nrely either on discriminative algorithms [2, 7, 11â€“13], or on\ngenerative probabilistic models, including Gaussian mixture\nmodels (GMMs) [19, 20], hidden Markov models (HMMs)\n[13, 15], hierarchical Dirichlet processes (HDPs) [9], code-\nword Bernoulli average models (CBA) [10], and dynamic\ntexture mixture models (DTMs) [5].\nMost generative approaches ï¬rst propose a general prob-\nabilistic model â€” the base model â€” that can adequately\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proï¬t or commercial advantage and that copies\nbear this notice and the full citation on the ï¬rst page.\nc/circlecopyrt2011 International Society for Music Information Retrieval.capture the typical characteristics of musical audio signals.\nThen, for each tag in a given vocabulary, an instance of this\nbase model is ï¬ne-tuned to directly model the audio pat-\nterns that are speciï¬c and typical for songs associated with\nthat tag. For example, Turnbull et. al. [19] propose Gaus-\nsian mixture models (GMMs) over a â€œbag of featuresâ€ (BoF)\nrepresentation, where each acoustic feature represents the\ntimbre of a short snippet of audio. Coviello et. al. [5]\nuse dynamic texture mixture models (DTMs) over a â€œbag\nof fragmentsâ€ representation, where each fragment is a se-\nquence of acoustic features extracted from a few seconds of\naudio. DTMs capture information about the temporal dy-\nnamics (e.g. rhythm, beat, tempo) of an audio fragment, as\nwell as instantaneous timbral content.\nSuch direct generative approaches may suffer from two\ninherent limitations. First, their ï¬‚exibility is determined by\nthe choice of the base model. Since different base models\nmay capture complementary characteristics of a musical sig-\nnal, selecting a single base model may restrict the modeling\npower a priori. For example, Coviello et al. [5] reported that\nDTMs are particularly suitable to model tags with signiï¬-\ncant temporal characteristics, while GMMs are favorable for\nsome tags for which â€œtimbre says it allâ€. Moreover, speci-\nfying a base model implies setting its time scale parameters.\nThis limits direct generative approaches to detecting musi-\ncal characteristics (timbre, temporal dynamics, etc.) at one\nï¬xed time resolution, for each tag in the vocabulary. This is\nsuboptimal, since the acoustic patterns that characterize dif-\nferent tags may occur at different time resolutions. Second,\nestimating tag models may require tuning a large number of\nparameters, depending on the complexity of the base model.\nFor tags with relatively few observations (i.e., songs associ-\nated with the tag), this may be prone to overï¬tting.\nTo address these limitations, we propose to use genera-\ntive models to indirectly represent tag-speciï¬c musical char-\nacteristics, by leveraging them to extract a high-level song\nrepresentation. In particular, we propose to model a song\nusing a â€œbag of systemsâ€ (BoS) representation for music.\nThe BoS representation is analogous to the â€œbag of wordsâ€\n(BoW) framework employed in text retrieval [1], which rep-\nresents documents by a histogram of word counts from a\n723Poster Session 6\ngiven dictionary. In the BoS approach, each word is a gen-\nerative model with ï¬xed parameters. Given a rich dictio-\nnary of such â€œmusical codewordsâ€, a song is represented\nby â€œcountingâ€ the of occurrences of each codeword in the\nsong â€” by assigning song segments to the codeword with\nlargest likelihood. Finally, BoS histograms can be modeled\nby appealing to standard text mining methods (e.g., logis-\ntic regression, topic models, etc.), to obtain tag-level mod-\nels for automatic annotation and retrieval. A BoS approach\nhas been used for the classiï¬cation of videos [4, 14], and a\nsimilar idea has inspired the anchor modeling for speaker\nidentiï¬cation [16].\nBy leveraging the complementary modeling power of var-\nious classes of generative models, the BoS approach is more\nï¬‚exible than direct generative approaches. In this work, we\ndemonstrate how combining Gaussian and dynamic texture\ncodewords with different time resolutions enriches the rep-\nresentation of a songâ€™s acoustic content and improves per-\nformance. A second advantage of the BoS approach is that\nit decouples modeling music from modeling tags. This al-\nlows us to leverage sophisticated generative models for the\nformer, while avoiding overï¬tting by resorting to relatively\nsimpler BoW models for the latter. More precisely, in a ï¬rst\nstep, a dictionary of sophisticated codewords may be esti-\nmated from anylarge collection of representative audio data,\nwhich need not be annotated. This allows to learn a general,\nrich BoS representation of music robustly. Next, tagmod-\nels are estimated to capture the typical codeword patterns in\nthe BoS histograms of songs associated with each tag. As\neach tag model already leverages the descriptive power of a\nsophisticated codebook representation, relatively simple tag\nmodels (with fewer tunable parameters) may be estimated\nreliably, even from small sets of tag-speciï¬c training songs.\nIn summary, we present a new approach to auto-tagging\nthat constructs a rich dictionary of musically meaningful\nwords and represents each song as a histogram over these\nwords. This simple, compact representation of the musical\ncontent of a song is computationally efï¬cient once learned\nand expected to be more robust than a single low-level audio\nrepresentation. It can beneï¬t from the modeling capabilities\nof several classes of generative models, and exploit infor-\nmation at multiple time scales.\n2. THE BAG OF SYSTEMS REPRESENTATION OF\nMUSIC\nAnalogous to the BoW representation of text documents, the\nBoS approach represents songs with respect to a codebook,\nin which generative models are used in lieu of words. These\ngenerative models compactly characterize typical audio fea-\ntures, musical dynamics or other acoustic patterns in songs.\nWe discuss codebook generation in Section 2.1, the gen-\nerative models used as codewords in Section 2.2, and therepresentation of songs using the codebook in Section 2.3.\n2.1 Codebook generation\nTo build a codebook, we ï¬rst choose Mclasses of base\nmodels (each with a certain allocation of time scale param-\neters). From each model we derive a set of representative\ncodewords, i.e., instances of that model class that capture\nmeaningful musical patterns. We do this ï¬rst by deï¬ning a\nrepresentative collection of songs, i.e., a codebook set, Xc,\nand then modeling each song in Xcas a mixture of Ksmod-\nels from each model class. After parameter estimation, the\nmixture components provide us with characteristic instances\nof that model class and become codewords. Finally, we ag-\ngregate all codewords to form the BoS codebook, V, which\ncontains|V|=MKs|Xc|codewords.\nEach codeword in the BoS codebook can be seen as char-\nacterizing a prototypical audio pattern or texture, and code-\nwords from different classes of generative models capture\ndifferent types of musical information. If the codebook set,\nXc, is sufï¬ciently diverse, the estimated codebook will be\nrich enough to represent songs well.\n2.2 The codewords\nTo obtain a diverse codebook, we consider Gaussian mod-\nels (to characterize timbre) and dynamic texture (DT) mod-\nels [6] (to capture temporal dynamics) at various time res-\nolutions. First, a time resolution is chosen by representing\nsongs as a sequence of feature vectors, Y={y1,...,yT},\nextracted from half-overlapping time windows of length Î·.\nThe sampling rate and the length Î·of the windows deter-\nmines the time resolution of the generative models. Second,\na generative model (Gaussian or DT) is chosen, and mixture\nmodels are estimated for all songs in the codebook set, Xc.\n2.2.1 Gaussian codewords\nTo learn Gaussian codewords, we ï¬t a Gaussian mixture\nmodel (GMM) to each song in Xc, to capture the most promi-\nnent audio textures it exhibits. More speciï¬cally, for each\nsong inXc, we treat the sequence of its feature vectors, Y,\nas an unordered bag of features, and use the EM algorithm\nto estimate the parameters of a GMM from these features.\nFinally, each mixture component is considered as a code-\nword, characterized by parameters Î˜i={Âµi,Î£i}, where\nÂµiandÎ£iare the mean and covariance of the ithmixture\ncomponent of the GMM, respectively.\n2.2.2 Dynamic Texture codewords\nDynamic texture (DT) codewords are learned by modeling\neach song inXcas a mixture of DTs, and considering each\nindividual DT as a codeword.\nDTs explicitly model the temporal dynamics of audio by\nmodeling ordered sequences of audio features rather than in-\n72412th International Society for Music Information Retrieval Conference (ISMIR 2011)\ndividual features. From the sequence of feature vectors ex-\ntracted from a song, Y, we sample subsequences, i.e., frag-\nments,y1:Ï„, of lengthÏ„everyÎ½seconds. We then repre-\nsent the song by an unordered bag of these audio fragments,\nY={y1\n1:Ï„, ..., yT\n1:Ï„}.\nA DT treats an audio fragment y1:Ï„as the output of a\nlinear dynamical system (LDS):\nxt=Axtâˆ’1+vt, (1)\nyt=Cxt+wt+ Â¯y, (2)\nwhere the random variable ytâˆˆRmencodes the timbral\ncontent (audio feature vector) at time t, and a lower dimen-\nsional hidden variable xtâˆˆRnencodes the dynamics of the\nobservations over time. The model is speciï¬ed by param-\neters Î˜ ={A,Q,C,R,Âµ,S, Â¯y}, where the state transition\nmatrixAâˆˆRnÃ—nencodes the evolution of the hidden state\nxtover time,vtâˆ¼N(0,Q)is the driving noise process, the\nobservation matrix CâˆˆRmÃ—nencodes the basis functions\nfor representing the observations yn,Â¯yis the mean of the\nobservation vectors, and wtâˆ¼N (0,R)is the observation\nnoise. The initial condition is distributed as x1âˆ¼N(Âµ,S).\nWe model a song by a dynamic texture mixture (DTM)\nthat summarizes the dominant temporal dynamics, where an\nassignment variable zâˆˆ{1,2,...,Ks}selects which of Ks\nDTs is generating an audio fragment. For a given a song,\nthe DTM parameters are estimated via the EM algorithm [3]\nand, once again each mixture component Î˜iis a codeword.\n2.3 Representing songs with the codebook\nOnce a codebook is available, a song is represented by a\ncodebook multinomial (CBM) bâˆˆR|V|that reports how\noften each codeword appears in that song, where b[i]is the\nweight of codeword iin the song.\nTo build the CBM for a given song, we count the number\nof occurrences of each codeword in the song by computing\nits likelihood at various points in the song (e.g., every Î½sec-\nonds) and comparing it to the likelihood of other codewords\nderived from the same base model class (since likelihoods\nare only comparable between similar models with the same\ntime resolution). To compute the likelihood of a given code-\nword at a certain point in the song, we extract a fragment of\naudio information ytdepending on the time scale and model\nclass of the codeword in question. I.e., for GMM code-\nwords,ytis a single audio feature vector, extracted from\na window of width Î·, while for DTM codewords, ytis a se-\nquence ofÏ„such feature vectors. We count an occurrence of\nthe codeword under attention if it has the highest likelihood\nof all the codewords in that class.\nWe construct the histogram bfor songYby counting the\nfrequency with which each codeword Î˜iâˆˆV is chosen torepresent a fragment:\nb[i] =1\nM|Ym|/summationdisplay\nytâˆˆYm1[Î˜i= argmax\nÎ˜âˆˆVmP(yt|Î˜)] (3)\nwhereVmâŠ†V is the subset of codewords derived from the\nmodel class mwhich codeword Î˜iis derived. Normalizing\nby the number of fragments |Ym|(according to class m) in\nthe song and the number of model classes Mleads to a valid\nmultinomial distribution.\nWe ï¬nd that the codeword assignment procedure out-\nlined above tends to assign only a few different codewords\nto each song. In order to diversify the CBMs, we general-\nize equation 3 to support the assignment of multiple code-\nwords at each point in the song. Hence, for a threshold\nkâˆˆ {1,2,...,|Vm|}, we assign the kmost likely code-\nwords (again comparing only within a model class) to each\nfragment. The softened histogram is then constructed as:\nb[i] =1\nM|Ym|/summationdisplay\nytâˆˆYm1\nk1[Î˜i= argmax\nÎ˜âˆˆVmkP(yt|Î˜)] (4)\nwhere the additional normalization factor of 1/kensures\nthatbis still a valid multinomial for k>1.\n3. MUSIC ANNOTATION AND RETRIEVAL USING\nTHE BAG-OF-SYSTEMS REPRESENTATION\nOnce a BoS codebook Vhas been generated and songs are\nrepresented by codebook histograms (i.e., CBMs), a content-\nbased auto-tagger may be obtained based on this represen-\ntation â€” by modeling the characteristic codeword patterns\nin the CBMs of songs associated with each tag in a given\nvocabulary. In this section, we formulate annotation and\nretrieval as a multiclass multi-label classiï¬cation of CBMs\nand discuss the algorithms used to learn tag models.\n3.1 Annotation and retrieval with BoS histograms\nFormally, assume we are given a training dataset Xt, i.e., a\ncollection of songs annotated with semantic tags from a vo-\ncabularyT. Each song sinXtis associated with a CBM\nbswhich describes the songâ€™s acoustic content with respect\nto the BoS codebook V. The songsis also associated with\nan annotation vector cs= (c1,..., c|T|)which express the\nsongâ€™s semantic content with respect to T, whereci= 1\nifshas been annotated with tag wiâˆˆT, andci= 0 oth-\nerwise. A dataset is a collection of CBM-annotation pairs\nXt={(bs,cs)}|Xt|\ns=1.\nGiven a training set Xt, standard-text mining algorithms\nare used to learn tag-level models to capture which patterns\nin the CBMs are predictive for each tag in T. Given the\nCBM representation of a novel song, b, we can then resort to\nthe previously trained tag-models to compute how relevant\n725Poster Session 6\neach tag inTis to the song. In this work, we consider algo-\nrithms that have a probabilistic interpretation, for which it is\nnatural to deï¬ne probabilities p(wi|b), fori= 1,...,|T|,\nwhich we rescale and aggregate to form a semantic multino-\nmial (SMN) p= (p1, ..., p|T|), wherepiâˆp(wi|b)and/summationtext|T|\ni=1pi= 1. Hence we deï¬ne the relevance of a tag to the\nsong as the corresponding entry in the SMN.\nAnnotation involves selecting the most representative tags\nfor a new song, and hence reduces to selecting the tags with\nhighest entries in p. Retrieval consists of rank ordering a set\nof songsS={s1,s2... sR}according to their relevance\nto a query. When the query is a single tag wifromT, we\ndeï¬ne the relevance of a song to the tag by p(wi|b), and\ntherefore we rank the songs in the database based on the ith\nentry in their SMN.\n3.2 Learning tag-models from CBMs\nThe CBM representation of songs is amenable to a variety of\nannotation and retrieval algorithms. In this work, we investi-\ngate one generative algorithm, Codeword Bernoulli Average\nmodeling (CBA), and one discriminative algorithm, multi-\nclass kernel logistic regression (LR).\n3.2.1 Codeword Bernoulli Average\nThe CBA model proposed by Hoffman et. al. [10] is a gener-\native process that models the conditional probability of a tag\nword appearing in a song. Hoffman et al. deï¬ne CBA based\non a vector quantized codebook representation of songs. For\nour work, we adapt the CBA model to use a BoS codebook.\nFor each song, CBA deï¬nes a collection of binary ran-\ndom variables ywâˆˆ{0,1}, which determine whether or not\ntagwapplies to the song. These variables are generated in\ntwo steps. First, given the songâ€™s CBM b, a codeword zwis\nchosen according to the CBM, i.e., zwâˆ¼Multinomial (b1,...,\nb|V|). Then a value for ywis chosen from a Bernoulli distri-\nbution with parameter Î²kw,\np(yw= 1|zw,Î²) =Î²zww (5)\np(yw= 0|zw,Î²) = 1âˆ’Î²zww. (6)\nWe use the authorâ€™s code [10] to ï¬t the CBA model. To\nbuild the SMN of a novel song we compute the posterior\nprobabilities p(ywi= 1|b,Î²) =piunder the estimated\nCBA model, and normalize p= (p1,...,p|V|).\n3.2.2 Multiclass Logistic Regression\nLogistic regression deï¬nes a linear classiï¬er with a prob-\nabilistic interpretation by ï¬tting a logistic function to all\nCBMs associated to each tag:\nP(wi|b,Î²i)âˆexpÎ²T\nib (7)\nKernel logistic regression ï¬nds a linear classiï¬er after ap-\nplying a non-linear transformation to the data, Ï•:Rdâ†’RdÏ•. The feature mapping Ï•is indirectly deï¬ned via a ker-\nnel function K(a,b) =/angbracketleftÏ•(a),Ï•(b)/angbracketright, where aandbare\nCBMs.\nIn our experiments we use the histogram intersection ker-\nnel [17], which is deï¬ned by the kernel function: K(a,b) =/summationtext\njmin(aj,bj). In our implementation we use the software\npackage Liblinear [8] and learn an L2-regularized logistic\nregression model for each tag using the â€œone-vs-the restâ€\napproach. As with CBA, we collect the posterior probabili-\ntiesp(wi|b)and normalize to build the SMN.\n4. EXPERIMENTAL SETUP\n4.1 Music Datasets\nTheCAL500 [19] dataset consists of 502 Western popular\nsongs from 502 different artists. Each song-tag association\nhas been evaluated by at least 3 humans, using a vocabulary\nof 149 tags. CAL500 provides binary annotations, i.e., ci=\n1when a tagiapplies to the song and 0 when the tag does\nnot apply. We restrict our experiments to the 97 tags with\nat least 30 example songs and use 5-fold cross-validation,\nwhere each song appears in the test set exactly once.\nTheCAL10k dataset [18] is a collection of over ten thou-\nsand songs from 4,597 different artists, weakly labeled from\na vocabulary of over 500 tags. The song-tag associations are\nmined from Pandoraâ€™s website. We restrict our experiments\nto the 55 tags in common with CAL500.\n4.2 Codebook parameters\nFor our experiments, we build codebooks using three classes\nof generative models: one class of GMMs and two classes\nof DTMs at different time resolutions. To learn DTM code-\nwords, we use feature vectors consisting of 34 Mel-frequency\nbins. The feature vectors used to learn GMM codewords are\nMel-frequency cepstral coefï¬cients appended with ï¬rst and\nsecond derivatives (MFCC-delta). Window and fragment\nlength for each class of codewords are speciï¬ed in Table 1.\nModel Class Window length ( Î·) Fragment length Fragment step ( Î½)\nBoS-DTM 1 12 ms 726 ms 145 ms\nBoS-DTM 2 93 ms 5.8 s 1.16 s\nBoS-GMM 1 46 ms 46 ms 23 ms\nTable 1 . Time resolutions of model classes\n4.3 Experiments\nOur ï¬rst experiment is cross-validation on CAL500, using\nthe training setXtas the codebook set Xcand re-training\nthe codebook for each split. We learn Ks= 4codewords of\neach model class per song. We build 5 codebooks: one for\neach of the 3 classes of codewords, one combining the two\n72612th International Society for Music Information Retrieval Conference (ISMIR 2011)\nclasses of DTM codewords (BoS-DTM 1,2) and one combin-\ning all three classes of codewords (BoS-DTM 1,2-GMM 1).\nThese results are discussed in Section 5.1.\nA second experiment investigates using a codebook set\nXcthat is disjoint from any of the training sets Xt. By\nsamplingXcas a subset of the CAL10k dataset, we illus-\ntrate how a codebook may be learned from any collection\nof songs (whether annotated or not). Training and testing\nof tag models is still performed as ï¬ve-fold cross-validation\non CAL500. We perform one experiment with |Xc|= 400 ,\nKs= 4, to obtain a codebook of the same size as those\nlearned on the CAL500 training set. Another experiment\nuses|Xc|= 4,597, for which one song was chosen from\neach artist in CAL10k, and Ks= 2. The results are dis-\ncussed in Section 5.2.\nFinally, we conduct an experiment learning codebooks\nand training tag models on the CAL10k dataset and testing\nthese models on CAL500, in order to determine how well\nthe BoS approach adapts to training on a separate, weakly\nlabeled dataset. We use the same codebook learned from\none song from each artist in CAL10k as above, with |Xc|=\n4,597, andKs= 2 codewords per song for each model\nclass. Now our training set Xtis the entire CAL10k dataset.\nWe train tag models with the settings (regularization of LR,\netc.) found through cross-validation on CAL500, in order\nto avoid overï¬tting, and test these models on the CAL500\nsongs. These results are discussed in Section 5.3.\n4.4 Annotation and retrieval\nWe annotate each test song CBM with 10 tags, as described\nin Section 3. Annotation performance is measured using\nmean per-tag precision, recall and F-score. Retrieval per-\nformance is measured using area under the receiver oper-\nating characteristic curve (AROC), mean average precision\n(MAP), and precision at 10 (P10) [19].\n5. EXPERIMENTAL RESULTS\n5.1 Results on CAL500\nResults on the CAL500 dataset are shown in Table 2. In gen-\neral, we achieve the best results with the softened histogram\nCBM representation (see Section 2.3), using a threshold of\nk= 10 for CBA and k= 5for LR. For comparison we also\nshow results using the hierarchical EM algorithm (HEM) to\ndirectly build GMM tag models (HEM-GMM) [19] and to\ndirectly build DTM tag models (HEM-DTM) [5]. These ap-\nproaches are state of the art auto-tagging algorithms that use\nthe same generative models we use to build BoS codebooks,\nin a more traditional framework. The HEM-GMM experi-\nments use GMM tag models consisting of 4 mixture com-\nponents, with the same audio features as the BoS-GMM 1\nexperiments. The HEM-DTM experiments use DTM tagAnnotation Retrieval\nPrecision Recall F-Score AROC MAP P10\nHEM-GMM 0.374 0.205 0.213 0.686 0.417 0.425\nHEM-DTM 0.446 0.217 0.264 0.708 0.446 0.460\nBoS-DTM 1CBA 0.369 0.251 0.237 0.722 0.465 0.482\nLR 0.416 0.257 0.270 0.730 0.471 0.483\nBoS-DTM 2CBA 0.382 0.241 0.233 0.717 0.457 0.471\nLR 0.404 0.251 0.260 0.725 0.466 0.480\nBoS-GMM 1CBA 0.359 0.243 0.227 0.714 0.450 0.463\nLR 0.396 0.251 0.257 0.724 0.464 0.479\nBoS-DTM 1,2CBA 0.375 0.254 0.240 0.729 0.473 0.495\nLR 0.413 0.264 0.274 0.738 0.480 0.496\nBoSâ€“DTM 1,2-GMM 1CBA 0.378 0.262 0.248 0.738 0.482 0.505\nLR 0.434 0.272 0.281 0.748 0.493 0.508\nTable 2 . BoS codebook performance on CAL500, com-\npared to Gaussian tag modeling (HEM-GMM) and DTM\ntag modeling (HEM-DTM).\nFigure 1 . Retrieval performance of the BoS approach with\nLR, relative to HEM-DTM, as a function of the maximum\ncardinality of tag-speciï¬c training examples.\nmodels consisting of 16 mixture components with the same\nfeatures and time scale parameters as the BoS-DTM 2exper-\niments. The BoS approach outperforms the direct tag mod-\neling approach for all metrics except precision, where HEM-\nDTM is still best. Additionally, the greatest improvements\nare seen with codebooks that combine the richest variety of\ncodewords. These codebooks capture the most information\nfrom the audio features, which leads to more descriptive tag\nmodels and increases the quality of the tag estimation.\nSince the classiï¬cation algorithms we use to model tags\nhave fewer parameters than direct tag modeling approaches,\nthe BoS approach is more robust for tags with fewer exam-\nple songs. We demonstrate this in Figure 1, which plots the\nimprovement in MAP over HEM-DTM as a function of the\ntagâ€™s training set cardinality. The BoS approach shows the\ngreatest improvement for tags with few training examples.\n5.2 Results learning codebook from unlabeled songs\nTable 3 shows results using BoS codebooks learned from\nunlabeled songs. These results are roughly equivalent to us-\ning codebooks learned from CAL500, and in fact outper-\n727Poster Session 6\nAnnotation Retrieval\n|Xc| Precision Recall F-score AROC MAP P10\nCAL500 400CBA 0.378 0.262 0.248 0.738 0.482 0.505\nLR 0.434 0.272 0.281 0.748 0.493 0.508\nCAL10k400CBA 0.355 0.263 0.244 0.741 0.484 0.505\nLR 0.429 0.269 0.277 0.749 0.492 0.498\n4,597CBA 0.377 0.263 0.249 0.744 0.489 0.505\nLR 0.434 0.273 0.282 0.751 0.497 0.517\nTable 3 . Results using codebooks learned from unlabeled\ndata (CAL10k), compared with codebooks from CAL500,\nwith codewords from model classes BoS-DTM 1,2-GMM 1,\nwhere|Xc|is the cardinality of the codebook training set.\nAnnotation Retrieval\nPrecision Recall F-Score AROC MAP P10\nHEM-GMM 0.297 0.404 0.264 0.714 0.350 0.315\nHEM-DTM 0.289 0.391 0.259 0.702 0.354 0.314\nBoS-DTM 1,2-GMM 1CBA 0.310 0.495 0.295 0.756 0.414 0.361\nLR 0.336 0.493 0.319 0.757 0.414 0.353\nTable 4 . Summary of results training on CAL10k.\nform the CAL500 codebooks with a larger codebook set.\nThis shows that a dictionary of musically meaningful code-\nwords may be estimated from anylarge collection of songs,\nwhich need not be labeled, and that a performance gain can\nbe achieved by adding unlabeled songs to the codebook set.\n5.3 Results training on CAL10k\nResults training codebooks and tag models on the CAL10k\ndataset, in Table 4, show that the BoS approach still outper-\nforms the direct tag modeling approaches when trained on a\nseparate dataset. We also see that the generative CBA model\ncatches up to the discriminative LR model in some perfor-\nmance metrics, which is expected, since generative models\ntend to be more robust on weakly labeled datasets.\n6. CONCLUSION\nWe have presented a semantic auto-tagger that leverages a\nrich â€œbag of systemsâ€ representation of music. The latter\ncan be learned from any representative set of songs, which\nneed not be annotated, and allows to integrate the descrip-\ntive quality of various generative models of musical content,\nwith different time resolutions. This approach improves per-\nformance over directly modeling tags with a single type of\ngenerative model. It also proves signiï¬cantly more robust\nfor tags with few training examples.\n7. ACKNOWLEDGMENTS\nThe authors thank L. Barrington and M. Hoffman for pro-\nviding the code of [19] and [10] respectively, and acknowl-\nedge support from Qualcomm, Inc., Yahoo! Inc., the Hell-\nman Fellowship Program, NSF Grants CCF-0830535 andIIS-1054960, and the UCSD FWGrid Project, NSF Research\nInfrastructure Grant Number EIA-0303622.\n8. REFERENCES\n[1] D. Aldous. Exchangeability and related topics. 1985.\n[2] Michael Casey, Christophe Rhodes, and Malcolm Slaney. Analysis of\nminimum distances in high-dimensional musical spaces. IEEE Trans-\nactions on Audio, Speech and Language Processing , 16(5):1015â€“1028,\n2008.\n[3] A. B. Chan and N. Vasconcelos. Modeling, clustering, and segmenting\nvideo with mixtures of dynamic textures. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence , 30(5):909â€“926, 2008.\n[4] A.B. Chan, E. Coviello, and G. Lanckriet. Clustering dynamic textures\nwith the hierarchical EM algorithm. In Proc. IEEE CVPR , 2010.\n[5] E. Coviello, A. Chan, and G. Lanckriet. Time Series Models for Se-\nmantic Music Annotation. Audio, Speech, and Language Processing,\nIEEE Transactions on , 19(5):1343â€“1359, July 2011.\n[6] G. Doretto, A. Chiuso, Y . N. Wu, and S. Soatto. Dynamic textures. Intl.\nJ. Computer Vision , 51(2):91â€“109, 2003.\n[7] D. Eck, P. Lamere, T. Bertin-Mahieux, and S. Green. Automatic gener-\nation of social tags for music recommendation. In Advances in Neural\nInformation Processing Systems , 2007.\n[8] Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and\nChih-Jen Lin. Liblinear: A library for large linear classiï¬cation. Jour-\nnal of Machine Learning Research , 9:1871â€“1874, 2008.\n[9] M. Hoffman, D. Blei, and P. Cook. Content-based musical similarity\ncomputation using the hierarchical Dirichlet process. In Proc. ISMIR ,\npages 349â€“354, 2008.\n[10] M. Hoffman, D. Blei, and P. Cook. Easy as CBA: A simple probabilis-\ntic model for tagging music. In Proc. ISMIR , pages 369â€“374, 2009.\n[11] M.I. Mandel and D.P.W. Ellis. Multiple-instance learning for music in-\nformation retrieval. In Proc. ISMIR , pages 577â€“582, 2008.\n[12] S.R. Ness, A. Theocharis, G. Tzanetakis, and L.G. Martins. Improving\nautomatic music tag annotation using stacked generalization of prob-\nabilistic svm outputs. In Proc. ACM MULTIMEDIA , pages 705â€“708,\n2009.\n[13] E. Pampalk, A. Flexer, and G. Widmer. Improvements of audio-based\nmusic similarity and genre classiï¬cation. In Proc. ISMIR , pages 628â€“\n633, 2005.\n[14] A. Ravichandran, R. Chaudhry, and R. Vidal. View-invariant dynamic\ntexture recognition using a bag of dynamical systems. In CVPR , 2009.\n[15] J. Reed and C.H. Lee. A study on music genre classiï¬cation based on\nuniversal acoustic models. In Proc. ISMIR , pages 89â€“94, 2006.\n[16] D.E. Sturim, DA Reynolds, E. Singer, and JP Campbell. Speaker in-\ndexing in large audio databases using anchor models. In icassp , pages\n429â€“432. IEEE, 2001.\n[17] M.J. Swain and D.H. Ballard. Color indexing. International Journal of\nComputer Vision , 7(1):11â€“32, 1991.\n[18] Derek Tingle, Youngmoo E. Kim, and Douglas Turnbull. Exploring\nautomatic music annotation with â€acoustically-objectiveâ€ tags. In Proc.\nMIR, pages 55â€“62, New York, NY , USA, 2010. ACM.\n[19] D. Turnbull, L. Barrington, D. Torres, and G. Lanckriet. Semantic an-\nnotation and retrieval of music and sound effects. IEEE Transactions\non Audio, Speech and Language Processing , 16(2):467â€“476, February\n2008.\n[20] G. Tzanetakis and P. Cook. Musical genre classiï¬cation of audio sig-\nnals. IEEE Transactions on speech and audio processing , 10(5):293â€“\n302, 2002.\n728"
    },
    {
        "title": "Score-Informed Voice Separation For Piano Recordings.",
        "author": [
            "Sebastian Ewert",
            "Meinard MÃ¼ller"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1417581",
        "url": "https://doi.org/10.5281/zenodo.1417581",
        "ee": "https://zenodo.org/records/1417581/files/EwertM11.pdf",
        "abstract": "The decomposition of a monaural audio recording into musically meaningful sound sources or voices constitutes a fundamental problem in music information retrieval. In this paper, we consider the task of separating a monaural piano recording into two sound sources (or voices) that correspond to the left hand and the right hand. Since in this scenario the two sources share many physical properties, sound separation approaches identifying sources based on their spectral envelope are hardly applicable. Instead, we propose a score-informed approach, where explicit note events specified by the score are used to parameterize the spectrogram of a given piano recording. This parameterization then allows for constructing two spectrograms considering only the notes of the left hand and the right hand, respectively. Finally, inversion of the two spectrograms yields the separation result. First experiments show that our approach, which involves high-resolution music synchronization and parametric modeling techniques, yields good results for realworld non-synthetic piano recordings.",
        "zenodo_id": 1417581,
        "dblp_key": "conf/ismir/EwertM11",
        "keywords": [
            "musical",
            "sound",
            "sources",
            "score",
            "informed",
            "approach",
            "spectrograms",
            "inversion",
            "realworld",
            "synthetic"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nSCORE-INFORMEDVOICE SEPARATION FORPIANORECORDINGS\nSebastian Ewert\nComputer Science III, University of Bonn\newerts@iai.uni-bonn.deMeinardM Â¨uller\nSaarlandUniversity andMPIInformatik\nmeinard@mpi-inf.mpg.de\nABSTRACT\nThe decomposition of a monaural audio recording into mu-\nsicallymeaningfulsoundsourcesorvoicesconstitutesafu n-\ndamental problem in music information retrieval. In this\npaper, we consider the task of separating a monaural piano\nrecordingintotwosoundsources(orvoices)thatcorrespon d\nto the left hand and the right hand. Since in this scenario\nthe two sources share many physical properties, sound sep-\naration approaches identifying sources based on their spec -\ntral envelope are hardly applicable. Instead, we propose a\nscore-informed approach, where explicit note events speci -\nï¬ed by the score are used to parameterize the spectrogram\nof a given piano recording. This parameterization then al-\nlowsforconstructingtwospectrogramsconsideringonlyth e\nnotes of the left hand and the right hand, respectively. Fi-\nnally, inversion of the two spectrograms yields the separa-\ntionresult. Firstexperimentsshowthatourapproach,whic h\ninvolves high-resolution music synchronization and para-\nmetric modeling techniques, yields good results for real-\nworldnon-synthetic piano recordings.\n1. INTRODUCTION\nIn recent years, techniques for the separation of musically\nmeaningful sound sources from monaural music recordings\nhave been applied to support many tasks in music infor-\nmation retrieval. For example, by extracting the singing\nvoice, the bassline, or drum and instrument tracks, signif-\nicantimprovementshavebeenreportedfortaskssuchasin-\nstrument recognition [7], melody estimation [1], harmonic\nanalysis [10], or instrument equalization [9]. For the sepa -\nration, most approaches exploit speciï¬c spectral or tempo-\nral characteristics of the respective sound sources, for ex -\nample the broadband energy distribution of percussive ele-\nments [10] or the spectral properties unique to the human\nvocal tract [1].\nPermission to make digital or hard copies of all or part of this w ork for\npersonal or classroom use is granted without fee provided th at copies are\nnot made or distributed for proï¬t or commercial advantage and th at copies\nbear this noticeand thefull citation ontheï¬rst page.\nc/circlecopyrt2011International Society forMusic InformationRetrieval .\n0 2 4 6 8 10C3  E3  G#3 C4  E4  G#4 C5  E5  G#5 \n02468101214160246810121416\n0246810121416\nFigure 1.Decomposition of a piano recording into two sound\nsources corresponding to the left and right hand as speciï¬ed by a\nmusicalscore. Shownaretheï¬rstfourmeasuresofChopinOp.28\nNo. 15.\nIn this paper, we present an automated approach for the\ndecomposition of a monaural piano recording into sound\nsourcescorrespondingtotheleftandtherighthandasspeci -\nï¬edbyascore,seeFigure1. Playedonthesameinstrument\nand often being interleaved, the two sources share many\nspectral properties. As a consequence, techniques that rel y\non statistical differences between the sound sources are no t\ndirectly applicable. To make the separation process feasi-\nble, we exploit the fact that a musical score is available for\nmany pieces. We then use the explicitly given note events\nof the score to approximate the spectrogram of the given\npiano recording using a parametric model. Characterizing\nwhichpartofthespectrogrambelongstoagivennoteevent,\nthe model is then employed to decompose the spectrogram\ninto parts related to the left hand and to the right hand. As\nan application, our goal is to extend the idea of an instru-\nment equalizer as presented in [9] to a voice equalizer that\ncannotonlyemphasizeorattenuatewholeinstrumenttracks\nbutalsoindividualvoicesorevensinglenotesplayedbythe\nsame instrument. While we restrict the task in this paper\nto the left/right hand scenario, our approach is sufï¬cientl y\ngeneral to isolate any kind of voice (or group of notes) that\nisspeciï¬ed by agiven score.\nSo far, score-informed sound separation has received\n245Poster Session 2\nonly little attention in the literature. In [11], the author s\nreplace the pitch estimation step of a sound separation sys-\ntem for stereo recordings with pitch information provided\nby an aligned MIDI ï¬le. In [6], a score-informed sys-\ntem for the elimination of the solo instrument from poly-\nphonic audio recordings is presented. For the description\nof the spectral envelope of an instrument, the approach re-\nlies on pretrained information from a monophonic instru-\nment database. In [4], score information is used as prior\ninformation in a separation system based on probabilistic\nlatent component analysis (PLCA). This approach is in [8]\ncompared to a score-informed approach based on paramet-\nricatoms. In[9],ascore-informedsystemfortheextractio n\nof individual instrument tracks is proposed. To counterbal -\nancetheirharmonicandinharmonicsubmodels,theauthors\nhave to incorporate complex regulation terms into their ap-\nproach. Furthermore, the authors presuppose that, for each\naudio recording, a perfectly aligned MIDI ï¬le is available,\nwhich isnot arealisticassumption.\nIn this paper, our main contribution is to extend the idea\nof an instrument equalizer to a voice equalizer that does\nnot rely on statistical properties of the sound sources. As\nafurthercontribution,wedonopresupposetheexistenceof\nprealigned MIDI ï¬les. Instead, we revert to high-resolutio n\nmusicsynchronizationtechniques[3]toautomaticallyali gn\nanaudiorecordingtoacorresponding musicalscore. Using\nthealignedscoreasaninitialization,wefollowtheparame t-\nric model paradigm [2,6,7,9] to obtain a note-wise param-\neterization of the spectrogram. As another contribution we\nshow how separation masks that allow for a construction of\nvoice-speciï¬cspectrogramscanbederivedfromourmodel.\nFinally, applying a Grifï¬n-Lim based inversion [5] to the\nseparated spectrograms yields theï¬nal separation result.\nThe remainder of this paper is organized as follows. In\nSection 2, we introduce our parametric spectrogram model.\nThen,inSection3,wedescribehowourmodelisemployed\nto decompose a piano recording into two voices that cor-\nrespond to the left hand and the right hand. In Section 4,\nwe report on our systematic experiments using real-world\nas well as synthetic piano recordings. Conclusions and\nprospects on future work are given in Section 5. Further\nrelated workisdiscussed inthe respective sections.\n2. PARAMETRIC MODEL\nTo describe an audio recording of a piece of music using\na parametric model, one has to consider many musical and\nacoustical aspects [7,9]. For example, parameters are re-\nquired to encode the pitch as well as the onset position and\nduration of note events. Further parameters might encode\ntuning aspects, the timbre of speciï¬c instruments, or ampli -\ntude progressions. In this section, we describe our model\nand show how its parameters can be estimated by an itera-\ntive method.  \n1 2 3 4050010001500\n02468101214161820(a)\n  \n00.511.522.5050010001500\n02468101214161820(b)\n  \n1 2 3 4050010001500\n02468101214161820(c)\n  \n1 2 3 4050010001500\n02468101214161820(d)\n  \n1 2 3 4050010001500\n02468101214161820(e)\nFigure2.Illustrationoftheï¬rstiterationofourparameterestima-\ntion procedure continuing the example shown in Figure 1 (shown\nsection corresponds to the ï¬rst measure). (a):Audio spectrogram\nYto be approximated. (b)-(e)Model spectrogram YÎ»after cer-\ntain parameters are estimated. (b):Parameter Sis initialized with\nMIDInoteevents. (c):Noteeventsin Saresynchronizedwiththe\naudio recording. (d):ActivityÎ±and tuning parameter Ï„are esti-\nmated.(e):Partialsâ€™ energy distributionparameter Î³isestimated.\n2.1 Parametric Spectrogram Model\nLetXâˆˆCKÃ—Ndenote the spectrogram and Y=|X|the\nmagnitude spectrogram of a given music recording. Fur-\nthermore, let S:={Âµs|sâˆˆ[1:S]}denote a set of\nnote events as speciï¬ed by a MIDI ï¬le representing a mu-\nsical score. Here, each note event is modelled as a triple\nÂµs= (ps,ts,ds), withpsencoding the MIDI pitch, tsthe\nonset position and dsthe duration of the note event. Our\nstrategy is to approximate Yby means of a model spectro-\ngramYS\nÎ», whereÎ»denotes a set of free parameters repre-\nsenting acoustical properties of the note events. Based on\nthenoteeventset S,themodelspectrogram YS\nÎ»willbecon-\nstructed as a superposition of note-event spectrograms Ys\nÎ»,\nsâˆˆ[1:S]. More precisely, we deï¬ne YS\nÎ»at frequency bin\nkâˆˆ[1:K]and timeframe nâˆˆ[1:N]as\nYS\nÎ»(k,n) :=/summationdisplay\nÂµsâˆˆSYs\nÎ»(k,n), (1)\n24612th International Society for Music Information Retrieval Conference (ISMIR 2011)\nwhere each Ys\nÎ»denotes the part of YS\nÎ»that is attributed to\nÂµs. EachYs\nÎ»consists of a component describing the ampli-\ntude or activity over time and a component describing the\nspectralenvelopeofanoteevent. Moreprecisely,wedeï¬ne\nYs\nÎ»(k,n) :=Î±s(n)Â·Ï•Ï„,Î³(Ï‰k,ps), (2)\nwhereÏ‰kdenotesthefrequencyinHertzassociatedwiththe\nk-thfrequencybin. Furthermore, Î±sâˆˆRN\nâ‰¥0encodestheac-\ntivity of the s-th note event. Here, we set Î±s(n) := 0, if the\ntimepositionassociatedwithframe nliesinR\\[ts,ts+ds].\nThe spectral envelope associated with a note event is de-\nscribed using a function Ï•Ï„,Î³:RÃ—[1 :P]â†’Râ‰¥0, where\n[1:P]withP=127denotes the set of MIDI pitches. More\nprecisely, to describe the frequency and energy distributi on\nof the ï¬rst Lpartials of a speciï¬c note event with MIDI\npitchpâˆˆ[1:P], the function Ï•Ï„,Î³depends on a parame-\nterÏ„âˆˆ[âˆ’0.5,0.5]Prelated to the tuning and a parameter\nÎ³âˆˆ[0,1]LÃ—Prelated to the energy distribution over the L\npartials. We deï¬ne for a frequency Ï‰given in Hertz the en-\nvelope function\nÏ•Ï„,Î³(Ï‰,p) :=/summationdisplay\nâ„“âˆˆ[1:L]Î³â„“,pÂ·Îº(Ï‰âˆ’â„“Â·f(p+Ï„p)),(3)\nwherethefunction Îº:Râ†’Râ‰¥0isasuitablychosenGaus-\nsian centered at zero, which is used to describe the shape of\na partial in frequency direction, see Figure 3. Furthermore ,\nf:Râ†’Râ‰¥0deï¬nedby f(p) := 2(pâˆ’69)/12Â·440mapsthe\npitch to the frequency scale. To account for non-standard\ntunings, we use the parameter Ï„pto shift the fundamental\nfrequency upwards or downwards by up to half a semitone.\nFinally,Î»:= (Î±,Ï„,Î³)denotes the set of free parameters\nwithÎ±:={Î±s|sâˆˆ[1 :S]}. The number of free param-\neters is kept low since the parameters Ï„andÎ³only depend\non the pitch but not on the individual note events given by\nS. Here, a low number allows for an efï¬cient parameter es-\ntimation process as described below. Furthermore, sharing\nthe parameters across the note events prevents model over-\nï¬tting.\nNow, ï¬nding a meaningful parameterization of Ycan be\nformulated as the followingoptimization task:\nÎ»âˆ—= argmin\nÎ»/bardblYâˆ’YS\nÎ»/bardblF, (4)\nwhere/bardblÂ·/bardblFdenotes the Frobenius norm. In the following,\nwe illustrate the individual steps in our parameter estima-\ntionprocedureinFigure2,whereagivenaudiospectrogram\n(Figure 2a) isapproximated by our model (Figure 2b-2e).\n2.2 Initializationand Adaption of NoteTiming\nParameters\nTo initialize our model, we exploit the available MIDI in-\nformation represented by S. For the s-th note event Âµs=0 500 1000 1500 2000 250000.20.40.60.81Ï•Ï„,Î³(Ï‰,p)\nFrequencyin Hzâ„“=1â„“=2â„“=3â„“=4â„“=5â„“=6â„“=7â„“=8â„“=9Î³1,p\nÎ³2,p\nÎ³3,p\nÎ³4,pÎ³5,p\nÎ³6,pÎ³7,pÎ³8,pÎ³9,p\nFigure3.Illustrationofthespectralenvelopefunction Ï•Ï„,Î³(Ï‰,p)\nforp= 60(middle C), Ï„= 0and some example values for pa-\nrametersÎ³.\n(ps,ts,ds), we setÎ±s(n) := 1if the time position associ-\natedwithframe nliesin[ts,ts+ds]andÎ±s(n) := 0other-\nwise. Furthermore, we set Ï„p:= 0,Î³1,p:= 1andÎ³â„“,p:= 0\nforpâˆˆ[1 :P],â„“âˆˆ[2 :L]. An example model spectrogram\nYS\nÎ»after theinitializationisgiven inFigure 2b.\nNext, we need to adapt and reï¬ne the model parameters\nto approximate the given audio spectrogram as accurately\nas possible. This parameter adaption is simpliï¬ed when the\nMIDI ï¬le is assumed to be perfectly aligned to the audio\nrecording as in [9]. However, in most practical scenarios\nsuch a MIDI ï¬le is not available. Therefore, in our ap-\nproach,weemployahighresolutionmusicsynchronization\napproach as described in [3] to adapt the onset positions\nof the note events set S. Based on Dynamic Time Warp-\ning (DTW) and chroma features, the approach also incor-\nporatesonset-basedfeaturestoyieldahighalignmentaccu -\nracy. Using the resulting alignment, we determine for each\nnote event the corresponding position in the audio record-\ning and update the onset positions and durations in Sac-\ncordingly. After the synchronization, the note event set S\nremains unchanged during all further parameter estimation\nsteps. Figure2cshowsanexamplemodelspectrogramafter\nthe synchronization step.\n2.3 Estimationof Model Parameters\nTo estimate the parameters in Î», we look for (Î±,Ï„,Î³)that\nminimize the function d(Î±,Ï„,Î³) :=/bardblYâˆ’YS\n(Î±,Ï„,Î³)/bardblF, thus\nminimizing the distance between the audio and the model\nspectrogram. Additionally, we need to consider range con-\nstraints for the parameters. For example, Ï„is required to be\nan element of [âˆ’0.5,0.5]P. To approximatively solve this\nconstraintoptimizationproblem,weemployaslightlymod-\niï¬ed version of approach exerted in [2]. In summary, this\nmethod works iteratively by ï¬xing two parameters and by\nminimizing dwithregardtothethirdoneusingatrustregion\nbased interior-points approach. For example, to get a bette r\nestimate for Î±, we ï¬xÏ„andÎ³and minimize d(Â·,Ï„,Î³). This\nprocess is repeated until convergence similar to the well-\nknownexpectation-maximizationalgorithm. Figures2dand\n2e illustrate the ï¬rst iteration of our parameter estimatio n.\nHere, Figure 2d shows the model spectrogram YS\nÎ»after the\nestimationofthetuningparameter Ï„andtheactivityparam-\n247Poster Session 2\n  \n0 5 10 15050010001500\n05101520(a)\n  \n0 5 10 15050010001500\n05101520\n  \n0 5 10 15050010001500\n05101520(b)\n  \n0 5 10 15050010001500\n00.20.40.60.81\n  \n0 5 10 15050010001500\n00.20.40.60.81(c)\n  \n0 5 10 15050010001500\n05101520\n  \n0 5 10 15050010001500\n05101520(d)\n0 5 10 15\n0 5 10 15(e)\nFigure 4.Illustration of our voice separation process continuing the example shown in Figure 1. (a)Model spectrogram YS\nÎ»after the\nparameter estimation. (b)Derived model spectrograms YL\nÎ»andYR\nÎ»corresponding to the notes of the left and the right hand. (c)Separation\nmasksMLandMR.(d)Estimatedmagnitude spectrograms Ë†YLandË†YR.(e)Reconstructedaudio signals Ë†xLandË†xR.\neterÎ±. Figure 2e shows YS\nÎ»after the estimation of the par-\ntialsâ€™energy distributionparameter Î³.\n3. VOICESEPARATION\nAftertheparameterestimation, YS\nÎ»yieldsanote-wisepara-\nmetricapproximationof Y. Inanextstep,weemployinfor-\nmation derived from the model to decompose the original\naudio spectrogram into separate channels or voices. To this\nend, we exploit that YS\nÎ»is a compound of note-event spec-\ntrograms Ys\nÎ». WithT âŠ‚ S, we deï¬ne YT\nÎ»as\nYT\nÎ»(k,n) :=/summationdisplay\nÂµsâˆˆTYs\nÎ»(k,n). (5)\nThenYT\nÎ»approximates the part of Ythat can be attributed\ntothenoteeventsin T. Onewaytoyieldanaudiblesepara-\ntionresultcouldbetoapplyaspectrograminversiondirect ly\ntoYT\nÎ». However, to yield an overall robust approximation\nresult our model does not attempt to capture every possi-\nble spectral nuance in Y. Therefore, an audio recording\ndeduced directly from YT\nÎ»would miss these nuances and\nwould consequently sound rather unnatural. Instead, we re-\nvert to the original spectrogram again and use YT\nÎ»only to\nextract suitable parts of Y. To this end, we derive a sepa-\nration mask MTâˆˆ[0,1]KÃ—Nfrom the model which en-\ncodes how strongly each entry in Yshould be attributed to\nT. More precisely, we deï¬ne\nMT:=YT\nÎ»\nYS\nÎ»+Îµ, (6)\nwhere the division is understood entrywise. The small con-\nstantÎµ >0is used to avoid a potential division by zero.\nFurthermore, Îµprevents that relatively small values in YT\nÎ»\nlead to large masking values, which would not be justiï¬ed\nby themodel. For our experiments, we set Îµ= 10âˆ’2.\nFor the separation, we apply MTto a magnitude spec-\ntrogram via\nË†YT:=MTâ—¦Y, (7)whereâ—¦denotes entrywise multiplication (Hadamard prod-\nuct). Theresulting Ë†YTisreferredtoas estimatedmagnitude\nspectrogram . Here, using a mask for the separation allows\nfor preserving most spectral nuances of the original audio.\nIn a ï¬nal step, we apply a spectrogram inversion to yield an\naudible separation result. Here, a commonly used approach\nis to combine Ë†YTwith the phase information of the origi-\nnal spectrogram Xin a ï¬rst step. Then, an inverse FFT in\ncombinationwithanoverlap-addtechniqueisappliedtothe\nresulting spectrogram [7]. However, this usually leads to\nclicking and ringing artifacts in the resulting audio recor d-\ning. Therefore, we apply a spectrogram inversion approach\noriginally proposed by Grifï¬n and Lim in [5]. The method\nattenuatestheinversionartifactsbyiterativelymodifyi ngthe\noriginalphaseinformation. Theresulting Ë†xTconstitutesour\nï¬nalseparationresultreferredtoas reconstructedaudiosig-\nnal (relativeto T).\nNext, we transfer these techniques to our left/right hand\nscenario. Each step of the full separation process is illus-\ntrated by Figure 4. Firstly, we assume that the score is\npartitioned into S=LË™âˆªR, whereLcorresponds to the\nnote events of the left hand and Rto the note events of the\nright hand. Starting with the model spectrogram YS\nÎ»(Fig-\nure4a)wederivethemodelspectrograms YL\nÎ»andYR\nÎ»using\nEqn. (5) (Figure 4b) and then the two masks MLandMR\nusing Eqn. (6) (Figure 4c). Applying the two masks to the\noriginalaudiospectrogram Y,weobtaintheestimatedmag-\nnitude spectrograms Ë†YLandË†YR(Figure 4d). Finally, ap-\nplying the Grifï¬n-Lim based spectrogram inversion yields\nthe reconstructed audio signals Ë†xLandË†xR(Figure 4e).\n4. EXPERIMENTS\nIn this section, we report on systematically conducted ex-\nperiments to illustrate the potential of our method. To this\nend, we created a database consisting of seven representa-\ntive pieces from the Western classical music repertoire, se e\nTable1. Usingonlyfreelyavailableaudioandscoredataal-\n24812th International Society for Music Information Retrieval Conference (ISMIR 2011)\nComposer Piece MIDI Audio 1 Audio 2 Identiï¬er\nBach BWV875-01 MUT Synthetic SMD â€˜Bach875â€™\nBeethoven Op031No2-01 MUT Synthetic SMD â€˜Beet31No2â€™\nBeethoven Op111-01 MUT Synthetic EA â€˜BeetOp111â€™\nChopin Op028-01 MUT Synthetic SMD â€˜Chop28-01â€™\nChopin Op028-04 MUT Synthetic SMD â€˜Chop28-04â€™\nChopin Op028-15 MUT Synthetic SMD â€˜Chop28-15â€™\nChopin Op064No1 MUT Synthetic EA â€˜Chop64No1â€™\nChopin Op066 MUT Synthetic SMD â€˜Chop66â€™\nTable1.Piecesandaudiorecordings(withidentiï¬er)usedinour\nexperiments.\nlows for a straightforward replication of our experiments.\nHere, we used uninterpreted score-like MIDI ï¬les from\ntheMutopiaProject1(MUT),high-qualityaudiorecordings\nfrom the Saarland Music Database2(SMD) as well as dig-\nitized versions of historical gramophone and vinyl record-\nings from theEuropean Archive3(EA).\nIn a ï¬rst step, we indicate the quality of our approach\nquantitatively using synthetic audio data. To this end, we\nused the Mutopia MIDI ï¬les to create two additional MIDI\nï¬les for each piece using only the notes of the left and the\nright hand, respectively. Using a wave table synthesizer,\nwe then generated audio recordings from these MIDI ï¬les\nwhich are used as ground truth separation results in the fol-\nlowing experiment. We denote the corresponding magni-\ntude spectrograms by YLandYR, respectively. For our\nevaluation we use a quality measure based on the signal-to-\nnoiseratio(SNR)4.Moreprecisely,tocompareareference\nmagnitude spectrogram YRâˆˆRKÃ—N\nâ‰¥0to an approximation\nYAâˆˆRKÃ—N\nâ‰¥0wedeï¬ne\nSNR(YR,YA) := 10Â·log10/summationtext\nk,nYR(k,n)2\n/summationtext\nk,n(YR(k,n)âˆ’YA(k,n))2.\nThe second and third column of Table 2 show SNR val-\nues for all pieces, where the ground truth is compared to\nthe estimated spectrogram for the left and the right hand.\nFor example, the left hand SNR for â€˜Chop28-15â€™ is 17.79\nwhereas the right hand SNR is 13.35. The reason the SNR\nbeing higher for the left hand than for the right hand is that\nthe left hand is already dominating the mixture in terms of\noverall loudness. Therefore, the left hand segregation is p er\nse easier compared the the right hand segregation. To indi-\ncate which hand is dominating in a recording, we addition-\nallygiveSNRvaluescomparingthegroundtruthmagnitude\nspectrograms YLandYRtothemixturemagnitudespectro-\ngramY, see column six and seven of Table 2. For example\nfor â€˜Chop28-15â€™, SNR(YL,Y)=3.48is much higher com-\npared toSNR(YR,Y)=âˆ’2.47thus revealing the left hand\ndominance.\n1http://www.mutopiaproject.org\n2http://www.mpi-inf.mpg.de/resources/SMD/\n3http://www.europarchive.org\n4Even though SNR values are often not perceptually meaningful , they\nat least givesometendencies onthequality ofseparation res ults.Identiï¬er SNR SNR SNR SNR SNR SNR\n(YL,Ë†YL) (YR,Ë†YR)(YL,Ë†YL) (YR,Ë†YR)(YL,Y) (YR,Y)\nprealigned distorted\nBach875 11.24 12.97 11.17 12.89 -1.99 3.03\nBeet31No2 12.65 10.38 12.47 10.23 1.24 -0.09\nBeetOp111 13.21 12.26 12.92 11.99 0.16 0.97\nChop28-01 10.52 13.96 10.43 13.84 -3.38 4.48\nChop28-04 17.63 10.48 17.58 10.45 8.65 -7.55\nChop28-15 17.79 13.35 17.56 13.18 3.48 -2.47\nChop64No1 12.93 11.86 12.60 11.55 -0.06 1.31\nChop66 11.61 11.17 11.46 11.03 -0.41 2.01\nAverage 13.45 12.05 13.27 11.90 0.96 0.21\nTable 2.Experimental results using ground truth data consisting\nof synthesized versionsof the pieces in our database.\nUsing synthetic data, the audio recordings are already\nperfectly aligned to the MIDI ï¬les. To further evaluate the\ninï¬‚uence of the music synchronization step, we randomly\ndistorted the MIDI ï¬les by splitting them into 20segments\nof equal length and by stretching or compressing each seg-\nment by a random factor within an allowed distortion range\n(in our experiments we used a range of Â±50%). The results\nfor these distorted MIDI ï¬les are given in column four and\nï¬ve of Table 2. Here, the left hand SNR for â€˜Chop28-15â€™\ndecreases only moderately from 17.79(prealigned MIDI)\nto17.56(distorted MIDI), and from 13.35to13.18for the\nright hand. Similarly, the average SNR also decreases mod-\neratelyfrom 13.45to13.27forthelefthandandfrom 12.05\nto11.90fortherighthand,whichindicatesthatoursynchro-\nnization works robustly in these cases. The situation in rea l\nworld scenarios becomes more difï¬cult, since here the note\nevents of the given MIDI may not correspond one-to-one to\nthe played note events of a speciï¬c recording. An example\nwillbe discussed inthenext paragraph, see alsoFigure 5.\nAs mentioned before, signal-to-noise ratios and similar\nmeasures cannot capture the perceptual separation quality .\nTherefore, to give a realistic and perceptually meaningful\nimpression of the separation quality, we additionally pro-\nvide a website5with audible separation results as well as\nvisualizations illustrating the intermediate steps in our pro-\ncedure. Here,weonlyusedreal,non-syntheticaudiorecord -\nings from the SMD and EA databases to illustrate the per-\nformance of our approach in real world scenarios. Lis-\ntening to these examples does not only allow to quickly\nget an intuition of the methodâ€™s properties but also to efï¬-\nciently locate and analyze local artifacts and separation e r-\nrors. For example, Figure 5 illustrates the separation pro-\ncess for â€˜BeetOp111â€™ using an interpretation by Egon Petri\n(European Archive). As a historical recording, the spectro -\ngramofthisrecording(Figure5c)israthernoisyandreveal s\nsome artifacts typical for vinyl recordings such as rumblin g\nand cranking glitches. Despite these artifacts, our model\napproximates the audio spectrogram well (w.r.t. to the eu-\nclidean norm) in most areas (Figure 5d). Also the resulting\n5http://www.mpi-inf.mpg.de/resources/MIR/\n2011-ISMIR-VoiceSeparation/\n249Poster Session 2\n(a)\n0 1 2 3 4 5 6 7 8 9 10G#1 C2  E2  G#2 C3  E3  G#3 C4  E4  G#4 C5  E5  (b)\n  \n0246810050010001500\n020406080100120(c)\n  \n0246810050010001500\n020406080100120(d)\n  \n0246810050010001500\n00.10.20.30.40.50.60.70.80.91(e)\n  \n0246810050010001500\n020406080100120(f)\nFigure 5.Illustration of the separation process for â€˜BeetOp111â€™.\n(a):Score corresponding to the ï¬rst two measures. (b):MIDI\nrepresentation (Mutopia Project). (c):Spectrogram of an interpre-\ntation by Petri (European Archive). (d):Model spectrogram after\nparameter estimation. (e):Separation mask ML.(f):Estimated\nmagnitude spectrogram Ë†YL. The area corresponding to the funda-\nmental frequency of the trills in measure one is indicated using a\ngreen rectangle.\nseparation results are plausible, with one local exception .\nListening to the separation results reveals that the trills to-\nwards the end of the ï¬rst measure were assigned to the left\ninstead of the right hand. Investigating the underlying rea -\nsons shows that the trills are not correctly reï¬‚ected by the\ngiven MIDI ï¬le (Figure 5b). As a consequence, our score-\ninformedapproachcannotmodelthisspectrogramareacor-\nrectly as can be observed in the marked areas in Figures 5c\nand 5d. Applying the resulting separation mask (Figure 5e)\nto the original spectrogram leads to the trills being misas-\nsigned to the left hand in the estimated magnitude spectro-\ngram as shown inFigure 5f.\n5. CONCLUSIONS\nInthispaper,wepresentedanovelmethodforthedecompo-\nsition of a monaural audio recording into musically mean-ingful voices. Here, our goal was to extend the idea of an\ninstrument equalizer to a voice equalizer which does not\nrelyonstatisticalpropertiesofthesoundsourcesandwhic h\nis able to emphasize or attenuate even single notes played\nby the same instrument. Instead of relying on prealigned\nMIDI ï¬les, our score-informed approach directly addresses\nalignment issues using high-resolution music synchroniza -\ntion techniques thus allowing for an adoption in real world\nscenarios. Initial experiments showed good results using\nsynthetic as well as real audio recordings. In the future,\nwe plan to extend our approach with an onset model while\navoiding thedrawbacks discussed in[9].\nAcknowledgement. This work has been supported by the\nGerman Research Foundation (DFG CL 64/6-1) and the\nCluster of Excellence on Multimodal Computing and Inter-\naction at Saarland University.\n6. REFERENCES\n[1] J.-L.Durrieu,G.Richard,B.David,andC.F Â´evotte.Source/ï¬ltermodel\nfor unsupervised main melody extraction from polyphonic audi o sig-\nnals.IEEE Transactions on Audio, Speech and Language Processing ,\n18(3):564â€“575,2010.\n[2] S. Ewert and M. M Â¨uller. Estimating note intensities in music record-\nings. InProceedings of the IEEE International Conference on Acous-\ntics,Speech,andSignalProcessing(ICASSP) ,pages385â€“388,Prague,\nCzech Republic,2011.\n[3] S. Ewert, M. M Â¨uller, and P. Grosche. High resolution audio synchro-\nnization using chroma onset features. In Proceedings of IEEE In-\nternational Conference on Acoustics, Speech, and Signal Pr ocessing\n(ICASSP) ,pages 1869â€“1872,Taipei, Taiwan, 2009.\n[4] J.Ganseman,P.Scheunders,G.J.Mysore,andJ.S.Abel.So urcesepa-\nrationbyscoresynthesis.In ProceedingsoftheInternationalComputer\nMusic Conference (ICMC) ,pages 462â€“465,New York,USA, 2010.\n[5] D.W.Grifï¬nandJ.S.Lim.Signalestimationfrommodiï¬edsho rt-time\nFouriertransform. IEEETransactionsonAcoustics,SpeechandSignal\nProcessing , 32(2):236â€“243,1984.\n[6] Y.HanandC.Raphael.Desoloingmonauralaudiousingmixtu remod-\nels. InProceedings of the International Society for Music Informa tion\nRetrieval Conference (ISMIR) ,pages 145â€“148,Vienna, Austria, 2007.\n[7] T. Heittola, A. Klapuri, and T. Virtanen. Musical instru ment recogni-\ntion in polyphonic audio using source-ï¬lter model for sound s epara-\ntion.InProceedingsoftheInternationalSocietyforMusicInforma tion\nRetrieval Conference (ISMIR) ,pages 327â€“332,Kobe, Japan, 2009.\n[8] R. Hennequin, B. David, and R. Badeau. Score informed audi o source\nseparation using a parametric model of non-negative spectrog ram.\nInProceedings of the IEEE International Conference on Acoust ics,\nSpeech and Signal Processing (ICASSP) , pages 45â€“48, Prague, Czech\nRepublic, 2011.\n[9] K. Itoyama, M. Goto, K. Komatani, T. Ogata, and H. G. Okuno. I n-\nstrument equalizer for query-by-example retrieval: Improvin g sound\nsource separation based on integrated harmonic and inharmoni c mod-\nels.InProceedingsoftheInternationalConferenceforMusicInfo rma-\ntion Retrieval (ISMIR) ,pages 133â€“138,Philadelphia, USA, 2008.\n[10] Y.Ueda,Y.Uchiyama,T.Nishimoto,N.Ono,andS.Sagayama. HMM-\nbased approach for automatic chord detection using reï¬ned ac ous-\ntic features. In Proceedings of the IEEE International Conference on\nAcoustics,SpeechandSignalProcessing(ICASSP) ,pages5518â€“5521,\nDallas, USA, 2010.\n[11] J. Woodruff, B. Pardo, and R. B. Dannenberg. Remixing ste reo mu-\nsicwithscore-informedsourceseparation. In ProceedingsoftheInter-\nnational Conference on Music Information Retrieval (ISMIR ), pages\n314â€“319,2006.\n250"
    },
    {
        "title": "The Studio Ontology Framework.",
        "author": [
            "GyÃ¶rgy Fazekas",
            "Mark B. Sandler"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1417735",
        "url": "https://doi.org/10.5281/zenodo.1417735",
        "ee": "https://zenodo.org/records/1417735/files/FazekasS11.pdf",
        "abstract": "This paper introduces the Studio Ontology Framework for describing and sharing detailed information about music production. The primary aim of this ontology is to capture the nuances of record production by providing an explicit, application and situation independent conceptualisation of the studio environment. We may use the ontology to describe real-world recording scenarios involving physical hardware, or (post) production on a personal computer. It builds on Semantic Web technologies and previously published ontologies for knowledge representation and knowledge sharing.",
        "zenodo_id": 1417735,
        "dblp_key": "conf/ismir/FazekasS11",
        "keywords": [
            "Studio",
            "Ontology",
            "Framework",
            "music",
            "production",
            "nuances",
            "record",
            "production",
            "explicit",
            "conceptualisation"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nTHE STUDIO ONTOLOGY FRAMEWORK\nGyÂ¨orgy Fazekas and Mark B. Sandler\nQueen Mary University of London, Centre for Digital Music\ngyorgy.fazekas@eecs.qmul.ac.uk\nABSTRACT\nThis paper introduces the Studio Ontology Frame-\nwork for describing and sharing detailed information\nabout music production. The primary aim of this on-\ntology is to capture the nuances of record production by\nproviding an explicit, application and situation indepen-\ndent conceptualisation of the studio environment. We\nmay use the ontology to describe real-world recording\nscenarios involving physical hardware, or (post) pro-\nduction on a personal computer. It builds on Semantic\nWeb technologies and previously published ontologies\nfor knowledge representation and knowledge sharing.\n1. INTRODUCTION\nRecognising that simple metadata based approaches are\ninsufï¬cient in complex music information management\nand retrieval scenarios, researchers has been focusing\non using cultural information and the use of content-\nbased features extracted from commercially released au-\ndio mixtures. Certain types of these information are\nrapidly becoming available on the Semantic Web and\nvia a number of Web services. For example, events\n(concerts, tour dates) and artist relations can be obtained\nand used in intuitive ways to ï¬nd connections in mu-\nsic [12]. However, these data remain largely editorial,\nand focussed on artists as opposed to music and pro-\nduction. We argue that another invaluable source of in-\nformation exist, largely neglected to date, pertaining to\nthe composition context, history, production and pre-\nrelease master recordings of music. Due to the lack of\ncomprehensive open standards and methodologies for\ncollecting production information, its use hasnâ€™t been\nexplored yet.\nWhile music making is an increasingly social activ-\nity, the Semantic Web could become a platform for shar-\ning not just music, but ideas between artists and engi-\nneers. To facilitate this process, our ontologies can be\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies\nare not made or distributed for proï¬t or commercial advantage and\nthat copies bear this notice and the full citation on the ï¬rst page.\nc/circlecopyrt2011 International Society for Music Information Retrieval.utilised to denote information about music production,\nand propagate it through the recording workï¬‚ow. They\nenable building better models for music information re-\ntrieval (MIR), and answering queries such as: How was\nthis song produced? What effects and parameters were\nused to achieve that particular sound of the guitar? How\nwas the microphone array conï¬gured when recording\nthis orchestra?\nIn the rest of this paper, we ï¬rst discuss why we de-\npart from existing metadata standards, and use Semantic\nWeb technologies and the Music Ontology [20] instead.\nNext, we introduce the Studio Ontology framework fo-\ncussing on its foundations. Finally, we discuss some\napplications and conclude.\n2. RELATED WORK\nNumerous metadata standards are available to capture\nat least parts of the information we outlined previously.\nHowever, their adaptation in audio applications remains\nlow, while a large number of concerns have been re-\nported by researchers, developers and end-users [3], [11],\n[21], [22], [1]. The reasons are complex, and beyond the\nscope of our discussion, see [17], [6]. for thorough re-\nviews. Instead, we summarise the main causes which\nmakes us move away from the adaptation of existing\nmetadata standards.\nPerhaps the most important problem is the prevail-\ning use of XML instead of logical data models. XML\nspeciï¬es the structure of a document, but it is insufï¬-\ncient in itself for deï¬ning relationships and constraints\nover a set of terms, hence their meaning remains am-\nbiguous [14]. Interoperability is hindered by the lack\nof semantics, which also prevents automated reasoning\nover data sets. Essential vocabulary terms are scattered\nacross different domains. While harmonisation is pos-\nsible, it requires reverse engineering [3], [11], [1] and\nit remains unclear if and how these efforts can converge\ninto a clear common conceptual model. Finally, the lack\nof shared unique identiï¬ers makes publishing, linking\nand the use of these data difï¬cult in anything but small\nMIR problems.\nNotable frameworks to facilitate interoperability in\nresearch include ACE XML [16], for sharing content-\n471Poster Session 3\nbased features in MIR, and the Integra Extensible Data\nformat (IXD) [4] linking audio processing and com-\nposition environments such as PD or Max/MSP. These\nXML-based formats however are too speciï¬c for our\nuse, difï¬cult to extend, and suffer from the same draw-\nbacks mentioned above. In the next section, we outline\nhow Semantic Web technologies can be used to avoid\nthese drawbacks.\n3. KNOWLEDGE REPRESENTATION IN THE\nRECORDING STUDIO\nThe dual role of the sound engineer can be characterised\nby the aim of fulï¬lling artistic goals on one hand, and\nby the use of speciï¬c domain knowledge on the other.\nCapturing this knowledge, the aesthetic choices, and the\nuse of tools in music production workï¬‚ows is the pri-\nmary focus of our research. It requires formalised data\nmodels and languages to represent, structure, transfer,\nstore and query this information.\nA na Â¨Ä±ve model for information management simply\nattaches metadata tags to audio items, but further de-\nscriptions of the entities described by tags is not possi-\nble. A relational data model resolves this issue, how-\never its common implementation is not sufï¬cient in it-\nself for knowledge representation: We can not describe\na hierarchy between tables or constraints over the use of\nterms in relational database schemata. Object orientated\nmodels resolve these limitations, but they have no sound\ntheoretical foundations, do not support efï¬cient query\nevaluation, or logical reasoning. Graph based models,\nsuch as the Resource Description Framework (RDF)1,\nand expressive Description Logic (DL) [10] and Seman-\ntic Web ontology languages provide a better alternative.\nWe brieï¬‚y introduce these techniques next.\n3.1 Semantic Web Technologies\nSemantic Web technologies include Web standards for\ncommunication and information sharing. The Uniform\nResource Identiï¬er (URI), provides a unique naming\nscheme for concepts and relationships (resources), while\nRDF allows structuring data using simple statements\nconsisting of subjectâ€”predicateâ€”object triples. A set\nof triples is seen as a graph of semantic relationships.\nEach term is identiï¬ed using a URI, which enables them\nto quote other resources creating a Web of structured\nandlinked data2. RDF ensures clear separation of syn-\ntax from semantics and conceptual model. There are\nconcise human readable serialisations like N33and an\nefï¬cient query language called SPARQL4supported by\n1http://www.w3.org/TR/rdf- syntax/\n2http://linkeddata.org\n3http://www.w3.org/DesignIssues/Notation3.html\n4http://www.w3.org/TR/rdf-sparql-query/several databases and open source libraries.\nUsing RDF alone, one can make rather arbitrary state-\nments however, therefore to have common ground for\napplications to interpret our data, we need to be able to\ndeï¬ne, and later refer to concepts such as a Song or an\naudio processing Plugin and its parameters, as well as\ntheir pertinent relationships. Ontology languages pro-\nvide for these deï¬nitions to be declared, while knowl-\nedge representation schema describing a domain is what\nwe call an ontology .\n3.2 Knowledge Representation and Ontologies\nOntology languages such as the Ontology Web Lan-\nguage (OWL)5are formal languages to express a shared\nconceptualisation6of a domain. Although using a for-\nmal language facilitates syntactic interoperability in it-\nself, making ontological commitments7pertaining to\nthe meaning of terms require higher level constructs of a\nlogical system. The presence or lack of this system sig-\nniï¬es the difference between data models and knowl-\nedge representations. Most Semantic Web ontologies\nare based on Description Logics corresponding to frag-\nments of First Order Logic for which practical reason-\ning procedures [10] can be created. The Music Ontol-\nogy and the Studio Ontology are published in OWL.\n4. OVERVIEW OF THE MUSIC ONTOLOGY\nThe Music Ontology provides a clear conceptualisation\nof the music domain to facilitate publishing music-related\ndata on the Semantic Web. It was introduced in [20]\nand thoroughly described in [19]. We refer the reader\nto the literature for an introduction and its applications.\nHere, we outline some features which make the Music\nOntology more suitable for our work than its alterna-\ntives [1], [13], [8] [11].\nâ€¢Modular and extensible design : Published as a\nmodular ontology library whose components may\nbe reused or extended outside of its framework.\nâ€¢Workï¬‚ow-based conceptualisation of the music\ndomain: It is built on the life-cycle of intellectual\nworks â€” deï¬ned in the Functional Requirements\nfor Bibliographic Records (FRBR) [18], â€” rang-\ning from abstract to concrete entities: Musical-\nWork ,Expression ,Manifestation ,Item.\nâ€¢Event decomposition model: Events are mod-\nelled as ï¬rst-class objects with participating agents\nand passive factors, and may be decomposed into\nsub-events.\n5http://www.w3.org/TR/owl-ref/\n6Formally, a set of relations Rover a universe of discourse D. [9]\n7We say that an agent commits to an ontology if its observable\nactions are consistent with the deï¬nitions in the ontology. [9]\n47212th International Society for Music Information Retrieval Conference (ISMIR 2011)\nâ€¢Timelines and temporal entities can be used to\nlocalise events on different timelines: abstract ,\ndiscrete , orcontinuous ;relative , orphysical .\nâ€¢Adaptation : It has become a de-facto standard to\npublish music-related data on the web.\nThe above models provide the basis for content an-\nnotation as well as the decomposition of events in com-\nplex workï¬‚ows, so that we can precisely say who did\nwhat andwhen . While elements of these models can\nalso be found in other ontologies, they are not present\nall at once in a single uniï¬ed framework. The Music\nOntology provides a model to describe the production\nworkï¬‚ow from composition to delivery, including mu-\nsic recording, but it lacks some very basic concepts to\ndo so in detail. The Studio Ontology ï¬lls this gap.\n5. THE STUDIO ONTOLOGY FRAMEWORK\nThe Studio Ontology8is presented as a modular and\nextensible ontology library. It is designed to reuse exist-\ning terms and models published elsewhere that ï¬t its re-\nquirements. The framework contains some general, do-\nmain independent elements, a set of core concepts and\nrelationships to describe the studio domain, and some\nextensions covering more speciï¬c areas like microphone\ntechniques and multitrack production tools.\n5.1 Foundational elements\nThe foundational parts of the ontology deal with de-\nscribing tools in audio engineering workï¬‚ows.\n5.1.1 Workï¬‚ows, Events and Timelines\nWe distinguish between two types of workï¬‚ows: pre-\nscriptive anddescriptive . Prescriptive workï¬‚ows are\nbest understood as templates describing common data\naccess and manipulation steps. Descriptive workï¬‚ows\nmay be seen as denotation of speciï¬c instances of the\nabove, broadly speaking a description of who (orwhat )\nproduced what ,when , and how,using what . Such a de-\nscription requires a workï¬‚ow based conceptualisation\nof entities existing at various stages. The Music Ontol-\nogy provides such a conceptualisation: A composition\n(MusicalWork ) may be performed producing a sound,\nwhich may be recorded producing a signal ( MusicalEx-\npressions ). We obey this model and hook into it exactly\nat this level. When the sound engineer manipulates a\nsound or a signal, new expressions are created to which\nadditional information can be attached on how it was\nproduced. In order to describe this process, we need\nto be able to talk about events (performance, record-\ning, mixing, transformation), which may be spatially\n8http://isophonics.net/content/studio-ontologyand temporally localised, and linked with agents (engi-\nneer) and factors (tools). We use the Event and Timeline\nOntologies [20] for this purpose. The Music Ontology\nsets aside the problems of how andusing what from the\nworkï¬‚ow above. We address this issue next.\n5.1.2 Technological Artefacts\nThe Device Ontology can be used to describe artefacts\nof technology. The Device concept may be subsumed\nby anything, a watch, a plugin, or a microphone in a\nmore speciï¬c ontology. Our ontology generalises con-\ncepts from [7], [2], which are speciï¬c for their applica-\ntion domains, namely smart phones and computer net-\nworks. Similarly to the Event and Timeline Ontologies,\nthe Device Ontology approaches a foundational domain\nindependent status in the sense described in [15]\nrdfs:subClassOfrdfs:subClassOfdevice:servicedevice:statedevice:Devicedevice:component\ndevice:AbstractDevicedevice:PhysicalDevicedevice:Statedevice:Service\nFigure 1 . Overview of the Device Ontology\nA device may participate in an event as a passive fac-\ntor, providing a particular service in a particular state.\nAstate may be useful to represent a conï¬guration , such\nas the polar pattern or sensitivity settings of a micro-\nphone during a recording. We borrow knowledge repre-\nsentation elements from the OWL description of UML\nstate machines of [5]. This resembles the paradigm of\nevent driven ï¬nite state machines, in that it describes\nevents related to an application of a device as reason for\nstate changes. Events are tied together as sub-events of\na main event. This has the beneï¬t of encoding chains\nof state changes (in a temporal context), and the ability\nto assign additional information to entry and exit condi-\ntions modelled as events themselves. This may be a link\nto an engineer to encode details such as an option turned\non by one engineer and then turned off by another, or\nclassiï¬cations of change events, such as automatic con-\ntrol, fault conditions or engineering decisions.\nOur ontology commits to a categorical distinction\nbetween physical and abstract devices, which worth mak-\ning for the following considerations. Physical and ab-\nstract objects have different primary characteristics. For\ninstance, physical devices have size and weight, and\nmay be decomposed into physical or abstract compo-\n473Poster Session 3\nnents, such as an extension module or ï¬rmware. Ab-\nstract devices on the other hand may be intangible mod-\nels of physical devices. Form a mereological point of\nview our model expresses a partial order relation on the\nset of components of a device, which is a reï¬‚exive, tran-\nsitive and anti-symmetric property9.\n5.1.3 Signal Processing Devices\nAn important class of devices in music production are\ntools for manipulating audio signals. We deï¬ne the con-\ncept SignalProcessingDevice as a subclass of the more\ngeneral device concept described in Â§5.1.2, having in-\nputs and outputs for signal connectivity. From an onto-\nlogical point of view this is sufï¬cient to identify a sig-\nnal processing device. It is interpreted broadly, and may\nstand for anything from a basic ï¬lter to a complex unit\nsuch as a mixing console or an audio effect. The con-\ncept is deï¬ned in a dedicated ontology called the Signal\nProcessing Device Ontology, together with some funda-\nmental signal processing components.\n5.1.4 Device Connectivity\nThe Connectivity Ontology allows for describing how\nsignal processing devices, or other tools, such as mi-\ncrophones, in a recording and processing workï¬‚ow are\ninterconnected. Its paramount concept Terminal repre-\nsents inputs and outputs in an abstract way, encompass-\ning electrical or software interfaces and may be linked\nwith a particular physical connector and communica-\ntion protocol. In ï¬gure 2 we illustrate its basic struc-\nture. The exempliï¬ed instances of Connector andPro-\ntocol can be thought to represent the output of a digital\nmicrophone having a 3 pin male XLR connector, and\nusing the AES42 digital microphone interface protocol.\nThe ontology deï¬nes some individuals of connectors\nand protocols common in audio production. An inter-\nesting feature of the ontology is that we can use it to\nmatch signal characteristics to interface characteristics,\nfor instance the number of accepted channels.\n5.2 Core components\nThe core Studio Ontology parallels the three levels of\nexpressiveness of the Music Ontology and provides stu-\ndio speciï¬c extensions. On the ï¬rst level it provides for\ndescribing recording studios and facilities. For exam-\nple, we can differentiate between commercial, project\nand home studios, different audio engineering roles such\nas mixing or mastering engineer, describe various record-\ning rooms and the equipment in them. This includes a\nlarge vocabulary of tools with top level concepts such as\nAmpliï¬er ,Analyser ,MixerDevice ,MonitoringSystem ,\nEffectUnit ,DigitalAudioWorkstation orPlugin .\n9Note that it requires OWL2 to express all constraints.rdfs:subClassOfrdfs:subClassOfcon:connectorcon:protocolcon:Terminal\ncon:OpticalTerminalcon:ElectricalTerminalcon:Protocolcon:Connector\nrdfs:subClassOfrdfs:subClassOfcon:AnalogTerminalcon:DigitalTerminalcon:XLR_3Mrdf:typecon:AES42rdf:type\nFigure 2 . Overview of the Connectivity Ontology (with\nsimpliï¬ed examples)\nThe second level includes complex events such as\ndifferent types of recording and post production ses-\nsions, and provides for describing the production work-\nï¬‚ow on the level of audio transformations and signal\nprocessing as described in Â§5.2.1\nThe third level provides some extension points to de-\nscribe speciï¬c tools, such as multitrack audio produc-\ntion software (see Â§5.3.4); the audio editing workï¬‚ow\nand project structure.\n5.2.1 Signal Processing Workï¬‚ows\nTo describe how a piece of music is processed in the\nstudio, it is insufï¬cient in itself to describe a signal ï¬‚ow\n(i.e. ï¬‚ow chart) or a set of transformations. We need\nto consider a random set of mixing or transformation\nevents, as in non-linear editing, as well as real-rime,\nquasi-simultaneous10transformations, such as a signal\nrouted through several processing units for recording.\nTo fulï¬l both requirements, we consider parallel signal\nand event ï¬‚ows linked using signal entities that are in-\nstances of the mo:Signal concept. This is illustrated\nin ï¬gure 3. The concepts Recording, Mixing, andTrans-\nform are subclasses of Event deï¬ned in the Event Ontol-\nogy (see Â§5.1.1) while MixerDevice andEffectUnit sub-\nsume SignalProcessingDevice deï¬ned in Â§5.1.3. Sev-\neral signals (not shown for brevity) can be attached to a\nmixing event and corresponding device. This set up sig-\nniï¬es our ontological commitment to changing identi-\nties, a problem thoroughly discussed in philosophy [23].\nOnce transformed, a signal receives new identity which\nalleviates difï¬cult transaction management problems in\nour system regarding the changing attributes of signals.\n5.3 Extensions\nOntology extensions are useful to allow the user to choose\na desired level of granularity, given some domain spe-\n10Apart from the small latency of signal processing units, these\nhave the same duration as the recording event itself.\n47412th International Society for Music Information Retrieval Conference (ISMIR 2011)\nsignal flowRecordingSessionPostProductionSessionevent flow\nmo:Music Ontologystudio:Studio Ontologycon:Connectivity Ontologymo:Recordingcon:Output Terminalstudio: Microphonestudio:microphonedevice:outputstudio:signalmo:produced_signalcon:Output Terminalstudio:Mixing Consoledevice:outputcon:Input Terminalstudio:signalstudio:produced_signalstudio:consoledevice:inputcon:Output Terminalstudio:Effect Unitdevice:outputstudio:signalcon:Input Terminalconsumed_signalstudio:signalstudio:produced_signalstudio:effectdevice:inputstudio:Transformstudio:Mixingstudio:signalmo:Signalmo:Signalmo:Signalconsumed_signalstudio:studio:\nFigure 3 . Recording, mixing and transformation events with an associated signal ï¬‚ow\nciï¬c details provided by the modeller. In this section we\ndescribe some extensions of the Studio Ontology.\n5.3.1 Audio Recording\nThe Microphone Ontology includes a small taxonomy\nof microphones organised by their transducer principle\n(i.e. CondenserMicrophone, RibbonMicrophone etc...).\nIt also allows for describing most properties one may\nï¬nd in a microphone data sheet, for instance diaphragm\ntype andsizeorpolar pattern . The Conï¬guration con-\ncept (subclass of device:State ) can be used to de-\nscribe variable parameters of microphones such as sen-\nsitivity, or variable polar pattern setting, in a particular\nrecording event. The ontology includes the concept Mi-\ncrophoneArrangement and allows for describing stereo\nand spatial recording techniques, such as a Blumlein-\nPair orDeccaTree , with their constituent microphones,\nand their distances, angles and conï¬gurations.\n5.3.2 Audio Mixing\nThe Audio Mixer Ontology allows detailed description\nof mixing consoles both in terms of static characteris-\ntics and particular settings (such as channel strip con-\nï¬guration) in an event. The ontology is modelled after a\ngeneralised blueprint of mixing consoles obtained from\nstudying several commercial hardware designs, how-\never software implementations were also taken into ac-\ncount. It deï¬nes concepts such as Channel, Bus orIn-\nsertTerminal and properties to describe fader levels, pan-\nning, equalisation (linked to an Audio Effect Ontology)\nand routing in a particular event, including automation.\n5.3.3 Audio Effects\nThe core ontology includes concepts to refer to audio\neffect units and plugins that are particular hardware or\nsoftware devices, and a small taxonomy of audio ef-\nfects based on their typical applications in audio engi-\nneering. However, audio effects are best conceptualised\nas physical phenomena, separated from implementation\n(circuit designs or algorithms), concrete devices, and\ntheir applications to signals. Therefore, we have fourconceptual layers which include the concepts: AudioEf-\nfect,Model ,Implementation ,EffectDevice ,Transform .\nThe Studio Ontology sets the problem of implementa-\ntion details aside. Creating an Audio Effects Ontology\nbased on multidisciplinary classiï¬cation [24] is ongoing\nwork in our lab.\n5.3.4 Audio Editing\nModern digital audio workstations organise recording\nprojects into a set of tracks â€” which may correspond to\ninput channels or created in an ad hoc way â€” and poten-\ntially overlapping clips contained in them correspond-\ning to various takes during a recording session. The\nMultitrack Ontology relates the the hierarchy of Clips\nandTrack s to other concepts in the Music and Studio\nontologies. It deï¬nes terms such as MultitrackProject ,\nMediaTrack ,AudioTrack , and AudioClip [6].\nA small Edit Ontology provides for describing a suc-\ncession of edit decisions modelled as events linked to\nthe universal timeline using event:time and the au-\ndio signal timeline using edit:media_time . These\nontologies may be subsumed to describe operations in a\nspeciï¬c tool such as a multitrack audio editor.\n6. APPLICATIONS AND IMPLEMENTATION\nThe ability to provide machine-processable representa-\ntions of the information one may ï¬nd on web pages\nof recording studios is a contribution to the Semantic\nWeb in itself. It facilitates ï¬nding studios with speciï¬c\nequipment or personnel using complex queries. How-\never, a more signiï¬cant beneï¬t comes with the ability\nto denote how a piece of music was produced. We can\nargue that contributions form the producer or the sound\nengineer are just as important in modern music as com-\nposition, but we had no way to record his/her actions\nand choices with the transparency music is denoted us-\ning scores. Collecting these data in production is a sig-\nniï¬cant effort, however a lot can be done automatically\nif ontology based models are available in digital mix-\ning consoles and post production tools. The Meta Ob-\n475Poster Session 3\nject Facility Speciï¬cation11enables source code gener-\nation from conceptual models. To take the continuously\nevolving nature of ontologies into account, we provide\nan alternative using run-time model generation [6].\n7. CONCLUSIONS AND FUTURE WORK\nWe presented a novel conceptualisation of the record-\ning studio environment and its implementation as a Se-\nmantic Web ontology. Our framework is unique in it-\nself, therefore we have no grounds for direct compar-\nison, but we evaluated it against a music production\ntext corpus, and found that it has good lexical coverage,\nand represents approximately 75% of commonly occur-\nring production situations. Further extensions remain\nfuture work, as well as audio editor prototypes which\nenable automatic collection of production information,\nand provide easy to use data entry facilities for captur-\ning data external to a computer system. However, to\nachieve its full potential, our system should be included\nin digital music production tools.\n8. REFERENCES\n[1]R. Arndt, R. Troncy, S. Staab, L. Hardman, and\nM. Vacura. Comm: Designing a well-founded multimedia\nontology for the web. In In Proceedings of the 6th Inter-\nnational Semantic Web Conference (ISWCâ€™2007), Busan,\nKorea , pages 11â€“15, 2007.\n[2]A. Bandara, T. R. Payne, D. de Roure, and G. Clemo.\nAn ontological framework for semantic description of de-\nvices. in Proc. International Semantic Web Conference\n(ISWC), Hiroshima, Japan , 2004.\n[3]D. Beenham, P. Schmidt, and G. Sylvester-Bradley. XML\nbased dictionaries for MXF/AAF applications. Technical\nreport, Sony Broadcast and Professional Research Labo-\nratories, UK, 2000.\n[4]J. Bullock and H. Frisk. libintegra: A system for software-\nindependent multimedia module description and storage.\nin Proceedings of the International Computer Music Con-\nference, Copenhagen, Denmark , 2007.\n[5]P. Dolog. Model-Driven Navigation Design For Semantic\nWeb Applications with the UML-Guide . in Maristella Mat-\nera and Sara Comai (Eds.) Engineering Advanced Web\nApplications, Rinton Press., 2005.\n[6]G. Fazekas and M. Sandler. Novel methods in information\nmanagement for advanced audio workï¬‚ows. In proceed-\nings of 12th International Conference on Digital Audio\nEffects, Como, Italy , 2009.\n[7]FIPA. Device ontology speciï¬cation. Foundation for In-\ntelligent Physical Agents, Working Spec., 2002.\n[8]R. Garc Â´Ä±a and O. Celma. Semantic integration and re-\ntrieval of multimedia metadata. Proceedings of the 5th In-\nternational Workshop on Knowledge Markup and Seman-\ntic Annotation , 2005.\n11http://www.omg.org/mof/[9]T. R. Gruber. Toward principles for the design of ontolo-\ngies used for knowledge sharing. International Journal of\nHuman-Computer Studies , 43:907â€“928, 1993.\n[10] I. Horrocks. Ontologies and the Semantic Web. Commu-\nnications of the ACM , Vol. 51((12)):pp. 58â€“67., 2008.\n[11] J. Hunter. Enhancing the semantic interoperability of mul-\ntimedia through a core ontology. IEEE Transactions on\nCircuits and Systems for Video Technology , 13(1):49â€“58,\n2003.\n[12] K. Jacobson, Y. Raimond, and M. Sandler. An ecosys-\ntem for transparent music similarity in an open world.\nin Proc. 10th International Society for Music Information\nRetrieval Conference (ISMIR 2009), Kobe, Japan , 2009.\n[13] M. Kanzaki. Music Vocabulary. Available Online: http:\n//www.kanzaki.com/ns/music , 2007.\n[14] M. Klein, D. Fensel, F. van Harmelen, and I. Horrocks.\nThe relation between ontologies and XML schemas.\nLinkÂ¨oping Electronic Articles in Compiter and Informa-\ntion Science , 2001.\n[15] C. Masolo, S. Borgo, A. Gangemi, N. Guarino, and\nA. Oltramari. Wonderweb deliverable d18: Ontology li-\nbrary. Technical report, Laboratory For Applied Ontology\n- ISTC-CNR, 2003.\n[16] C. McKay, J. A. Burgoyne, J. Thompson, and I. Fujinaga.\nUsing ACE XML 2.0 to store and share feature, instance\nand class data for musical classiï¬cation. Proceedings of\nthe International Society for Music Information Retrieval\nConference , (303-8.), 2009.\n[17] F. Nack, J. van Ossenbruggen, and L. Hardman. That ob-\nscure object of desire: multimedia metadata on the web,\npart 2. IEEE Multimedia , 12(1):54â€“63, 2005.\n[18] M.-F. Plassard, editor. Functional Requirements for Bibli-\nographic Records, Final Report . International Federation\nof Library Associations and Institutions, 1998.\n[19] Y. Raimond. A Distributed Music Information System .\nPhD Thesis, Queen Mary University, School of Electronic\nEngineering and Computer Science, 2008.\n[20] Y. Raimond, S. Abdallah, M. Sandler, and G. Frederick.\nThe music ontology. in Proc. 7th International Confer-\nence on Music Information Retrieval (ISMIR 2007), Vi-\nenna, Austria , 2007.\n[21] J. R. Smith and P. Schirling. Metadata standards roundup.\nIEEE Multimedia, April-June 2006 , Vol. 13.(No. 2.):pp.\n84â€“88, 2006.\n[22] B. Smithers. Going DAW to DAW. Electronic Musician ,\nOctober 2007.\n[23] P. F. Strawson. Individuals. An Essay in Descriptive Meta-\nphysics . Routledge, London and New York, 1959.\n[24] V. Verfaille, C. Guastavino, and C. Traube. An interdis-\nciplinary approach to audio effect classiï¬cation. Proceed-\nings of the 9th International Conference on Digital Audio\nEffects (DAFx-06), Montreal, Canada , 2006.\n476"
    },
    {
        "title": "A Scalable Audio Fingerprint Method with Robustness to Pitch-Shifting.",
        "author": [
            "SÃ©bastien Fenet",
            "GaÃ«l Richard",
            "Yves Grenier"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1417593",
        "url": "https://doi.org/10.5281/zenodo.1417593",
        "ee": "https://zenodo.org/records/1417593/files/FenetRG11.pdf",
        "abstract": "Audio fingerprint techniques should be robust to a variety of distortions due to noisy transmission channels or specific sound processing. Although most of nowadays techniques are robust to the majority of them, the quasi-systematic use of a spectral representation makes them possibly sensitive to pitch-shifting. This distortion indeed induces a modification of the spectral content of the signal. In this paper, we propose a novel fingerprint technique, relying on a hashing technique coupled with a CQT-based fingerprint, with a strong robustness to pitch-shifting. Furthermore, we have associated this method with an efficient post-processing for the removal of false alarms. We also present the adaptation of a database pruning technique to our specific context. We have evaluated our approach on a real-life broadcast monitoring scenario. The analyzed data consisted of 120 hours of real radio broadcast (thus containing all the distortions that would be found in an industrial context). The reference database consisted of 30.000 songs. Our method, thanks to its increased robustness to pitch-shifting, shows an excellent detection score.",
        "zenodo_id": 1417593,
        "dblp_key": "conf/ismir/FenetRG11",
        "keywords": [
            "Audio fingerprint techniques",
            "robust to distortions",
            "pitch-shifting",
            "spectrum representation",
            "hashing technique",
            "CQT-based fingerprint",
            "post-processing",
            "database pruning",
            "real-life broadcast monitoring",
            "detection score"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nA SCALABLE AUDIO FINGERPRINT METHOD\nWITH ROBUSTNESS TO PITCH-SHIFTING\nSÂ´ebastien Fenet, Ga Â¨el Richard, Yves Grenier\nInstitut TELECOM, TELECOM ParisTech, CNRS-LTCI\n37 rue Dareau, 75014 Paris, France\n{sebastien.fenet, gael.richard, yves.grenier }@telecom-paristech.fr\nABSTRACT\nAudio ï¬ngerprint techniques should be robust to a variety\nof distortions due to noisy transmission channels or speciï¬c\nsound processing. Although most of nowadays techniques\nare robust to the majority of them, the quasi-systematic use\nof a spectral representation makes them possibly sensitive\nto pitch-shifting. This distortion indeed induces a modiï¬-\ncation of the spectral content of the signal. In this paper,\nwe propose a novel ï¬ngerprint technique, relying on a hash-\ning technique coupled with a CQT-based ï¬ngerprint, with\na strong robustness to pitch-shifting. Furthermore, we have\nassociated this method with an efï¬cient post-processing for\nthe removal of false alarms. We also present the adaptation\nof a database pruning technique to our speciï¬c context. We\nhave evaluated our approach on a real-life broadcast moni-\ntoring scenario. The analyzed data consisted of 120 hours\nof real radio broadcast (thus containing all the distortions\nthat would be found in an industrial context). The reference\ndatabase consisted of 30.000 songs. Our method, thanks to\nits increased robustness to pitch-shifting, shows an excellent\ndetection score.\n1. INTRODUCTION\nAudio identiï¬cation consists of retrieving the meta data as-\nsociated with an unknown audio excerpt. The typical use\ncase is the music identiï¬cation service which is nowadays\navailable on numerous mobile phones. The user captures\nan audio excerpt with his mobile phone microphone and\nthe service returns metadata such as the title of the song,\nTHIS WORK WAS ACHIEVED AS PART OF THE QUAERO PRO-\nGRAMME, FUNDED BY OSEO, FRENCH STATE AGENCY FOR IN-\nNOV ATION.\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proï¬t or commercial advantage and that copies\nbear this notice and the full citation on the ï¬rst page.\nc/circlecopyrt2011 International Society for Music Information Retrieval.the artist, the album... Other applications include jingle de-\ntection, broadcast monitoring for statistical purposes or for\ncopyright control (see [1] for more details).\nAudio ï¬ngerprint is the most common way of performing\naudio identiï¬cation when no meta data has been embedded\nin the unknown audio excerpt. It consists of extracting from\neach audio reference a compact representation (the ï¬nger-\nprint) which is then stored in a database. When identifying\nan unknown excerpt, its ï¬ngerprint is calculated. Then the\nbest match with the unknown ï¬ngerprint is looked for in\nthe database. The difï¬culty is dual. First, the captured sig-\nnal has undergone a series of distortions (equalization, con-\nversion, time-stretching, pitch-shifting, reverberation, ...).\nSecond, the algorithm has to manage a database containing\nhuge amounts of audio references.\nAudio ï¬ngerprint has been dealt with in many previous\nworks. Two main trends can be observed: exact-hashing\nand approximate-search. Exact hashing algorithms [2, 3]\nstate that there are features in the signal which are preserved\nagainst the distortions. They extract these features and use\na hash table to do the matching. Approximate search al-\ngorithms [4, 5] decode the unknown excerpt on a given al-\nphabet and look for the closest transcription in the database.\nA variant is proposed in [6] where the unknown excerpt is\ndecoded on different alphabets according to the references.\nThe best-suited (with respect to the unknown excerpt) al-\nphabet gives the closest reference.\nIn this work, we propose a novel audio ï¬ngerprint method\nbased on hashing with a particular focus on robustness to\npitch-shifting. Indeed, this distortion appears to be quite\ncommon in radio broadcasts and taking it into account al-\nlows us to show excellent results on a radio-monitoring ori-\nented evaluation.\nThe paper is organized as follows. In the ï¬rst section, we\ndescribe the broadcast monitoring use case. It is a typical\napplication for ï¬ngerprinting that constitutes a demanding\nevaluation framework for the algorithms. It includes a wide\nvariety of distortions that are actually performed by the ra-\ndio stations. The whole methodology described in this paper\ncan however be easily transposed to any other use case. In\n121Poster Session 1\nthe second section, we describe in detail our method for ï¬n-\ngerprinting. This includes the ï¬ngerprint model, the search\nstrategy and the post-processing designed to prevent false\nalarms. We also describe an optional step of database prun-\ning allowing a lower computation time while keeping a high\nratio of identiï¬cation. In the last section we show the results\nof experiments performed on real broadcast data.\n2. BROADCAST MONITORING\n2.1 Use case description\nThe task consists of detecting the broadcasting of any audio\nreference of a given database in an audio stream. Practically\nthe database will be a set of songs and the stream will be the\none of a radio station. We have to note that the broadcast\nstream not only contains references but also non-referenced\nitems (such as advertisements, speech, unreferenced songs).\nAlso the broadcast references have undergone a series of\nprocesses applied by the radio station, such as: compression,\nequalization, enhancement, stereo widening, pitch-shifting,\n... (see [4] for more details about the radio stations process-\ning). If we denote by m1, m2, ..., m Nthe references, by\nËœm1,Ëœm2, ...,ËœmNtheir broadcast (and distorted) versions and\nbynthe rest of the broadcast (considered as noise for the\nalgorithm), the task can be illustrated as in Figure 1.\nn Ëœmk1 n Ëœmk2Ëœmk3 n timedetection\nmk1detection\nmk2detection\nmk3\nFigure 1 : Broadcast monitoring\n2.2 Focus on pitch shifting\nThe large majority of the methods from the state of the art\nrely on a spectral representation of the signal. Therefore\nthese methods are possibly sensitive to modiï¬cations of the\nfrequency content [7].\nA very common distortion in the radio broadcasts is pitch-\nshifting. When this distortion occurs, all the frequencies in\nthe spectrum are multiplied by a factor K. Pitch-shifting\ncould be generated on its own by some signal processing on\nthe frequency content. But in the context of the radio broad-\ncasts, it is strongly linked with time-stretching. Indeed, the\nradio stations frequently shorten the music they play. To this\nend, most radio sound engineers will simply accelerate the\nreading of the music (by changing the sampling rate). This\nwill change the duration of the music, but will also cause\npitch-shifting as a side effect. This processing allows thestations to precisely ï¬t their time constraints and to give the\nimpression that the music is more lively in their broadcasts.\n3. SYSTEM OVERVIEW\n3.1 Architecture\nAs shown in Figure 2, the system is made of four units.\nFirst, the audio stream is cut in analysis frames of length la\nwith an overlap oa. The ï¬ngerprint of each analysis frame\n(called frame-based ï¬ngerprint) is calculated according to\nthe methodology described in section 3.2. The matching\nunit then ï¬nds in the database the best match to the frame-\nbased ï¬ngerprint. Finally, the best match is post-processed\nin order to discriminate out-of-base queries (when the audio\nstream corresponds to none of the references).\nStream Framing Fingerprint\nMatchingPost-\nprocessingIdentiï¬cation\nReferences\nï¬ngerprints\nFigure 2 : Architecture of the system\n3.2 Fingerprint\nOur ï¬ngerprint relies on a spectrogram calculated with â€con-\nstant Q transforms (CQT)â€ [8] [9]. The constant Q trans-\nform is well adapted to musical signals in the sense that\nits frequency bins are geometrically spaced. As the notes\nof the western scale are geometrically spaced as well, this\ntransform yields a constant number of bins per note. More-\nover pitch-shifting becomes a translation in the CQT do-\nmain. That is, a frequency which is located in bin bwill have\nits pitch-shifted version located in bin b+K/prime. In our imple-\nmentation, we use a CQT with 3 bins per note performed on\nframes of signal with a 10ms increment.\nIn order to compact the spectrogram, we use a 2 dimen-\nsional peak-picking inspired by [2]. We tile the spectrogram\nwith rectangles of width âˆ†Tseconds and height âˆ†Bbins of\nfrequency (typical values for âˆ†Tandâˆ†Bareâˆ†T= 0.4s,\nâˆ†B= 12 bins). In each rectangle, we set the maximum\npoint to 1 and all the other points to 0. The result is a binary\nspectrogram containing sparse points at 1. They correspond\nto the points with the highest energy in the original spectro-\ngram.\n12212th International Society for Music Information Retrieval Conference (ISMIR 2011)\nThe methodology used ensures that there is one point set\nto 1 per rectangle of size âˆ†TÃ—âˆ†B(2-dimensional homo-\ngeneity). Thus, this representation is robust to compres-\nsors (which change the dynamic of the audio with respect\nto time) and equalizers (which change the dynamic of the\naudio with respect to frequency). Furthermore, the fact that\nwe do only keep points with maximum energy makes the\nrepresentation robust to most additive noises.\n3.3 Indexing the references\nAs we are dealing with an exact-hashing approach, the match-\ning step relies on the indexing of the references. As Wang\nsuggests, we use pairs of peaks (points set to 1 in the ï¬n-\ngerprint step) to index the ï¬ngerprints of the references. We\nwill ï¬rst describe how to encode a pair of peaks. Then we\nwill describe the hash function.\nt1andt2being the times of occurrence of the two peaks\ninvolved in a pair, b1andb2being their frequency bins, the\nencoding we suggest for a pair of peaks is the following:\n[/hatwideb1;b2âˆ’b1;t2âˆ’t1]\nwith/hatwideb1=âŒŠb1\n6âŒ‹, a sub-resolved version of b1. The ï¬rst\ncomponent (/hatwideb1) is a rough frequency location of the pair\nof peaks. The second component ( b2âˆ’b1) is the spectral\nextent of the pair in the CQT domain. The third compo-\nnent ( t2âˆ’t1) is its time extent. This encoding has sev-\neral advantages. As it only takes into account relative time\ninformation, it is robust to cropping. Also, it is robust to\npitch-shifting. Indeed the use of the constant Q transform\nimplies the pitch-shifting invariance for the second compo-\nnent: a reference having peaks at frequency bins b1andb2\nwill have them at frequencies b1+K/primeandb2+K/primein its\npitch-shifted version. And we actually have:\n(b2+K/prime)âˆ’(b1+K/prime) =b2âˆ’b1 (1)\nThe ï¬rst component ( /hatwideb1) is chosen on a sufï¬ciently coarse\nrepresentation (bin resolution divided by 6) to make it in-\nvariant with the common pitch-shifting ratios ( â‰¤5%). It\nis worth mentioning that pitch-shifting will still move some\npairs close to the border of one sub-resolved bin to the next.\nHowever, similarly to Wangâ€™s methodology, an exact match-\ning of all pairs is not required. Indeed, the histogram step\ndescribed thereafter only requires that the majority of the\npairs are preserved.\nAs for the hash function, we build an index over all the\npairs of peaks of all the references. More precisely, we build\na function h1which, for any pair of peaks preturns all the\nreferences containing this pair with the time of occurrence\nofpin the references.\nh1:p/mapstoâˆ’â†’{ (mi, tp,m i)/p occurs in miattp,m i}(2)Let us note that in order to prevent an explosion of the\nnumber of pairs, we only consider pairs of peaks whose\nspectral extent is smaller than a threshold âˆ†bmaxand whose\ntemporal extent is smaller than a threshold âˆ†tmax (typical\nsetup for this limitation is âˆ†tmax = 1.2s and âˆ†bmax =\n24bins).\n3.4 Matching\nWhen identifying the ï¬ngerprint of an analysis frame, we\nextract all its pairs of peaks with their times of occurrence\n{(p, tp,af)}. Thanks to the hash function h1we can efï¬-\nciently compute the differences {tp,m iâˆ’tp,af}for all pairs\nof the frame-based ï¬ngerprint and for each reference mi.\nWe store these differences in histograms (one histogram per\nreference).\nIf the analysis frame is actually an excerpt of the refer-\nencem0starting at time s, them0histogram will show a\nmaximum at value s. Moreover this maximum should be\nhigher than any other histogram maximum. Indeed if the\nanalysis frame corresponds to m0its ï¬ngerprint will have\nmore pairs in common with m0â€™s ï¬ngerprint than with any\nother reference ï¬ngerprint. Furthermore, the pairs should all\noccur in the frame-based ï¬ngerprint sseconds earlier than in\nthe referenceâ€™s. Thus the histogram should show a majority\naccumulation for this reference at this value.\nSo, in order to perform the identiï¬cation we look for the\nreference whose histogram has the highest maximum. This\nreference is considered to match the analysis frame. The\nargument of the maximum of the histogram gives the start\ntime of the analysis frame in the reference.\n3.5 Post-processing\nFor any analysis frame, the matching unit returns its best\nmatch among the references. This means that the case of an\nout-of-base query is not managed.\nA simple approach would consist of setting a threshold\non the common number of pairs between the frame-based\nï¬ngerprint and its best match. If the frame-based ï¬nger-\nprint has more than threshold pairs in common with the best\nmatch, we deduce that the identiï¬cation is correct. Other-\nwise we deduce that this is an out-of-base query. Unfortu-\nnately, on real data with classical distortions such a thresh-\nold is virtually impossible to setup. It happens that, due to\nthe distortions applied to the stream, a best match has a low\nnumber of pairs in common with the frame-based ï¬nger-\nprint even though it is a correct identiï¬cation. Besides, such\nathreshold would depend on the transmission channel and\nwould have to be tuned for each different use case.\nThis is why we propose a post-processing unit based on\na majority vote. The unit considers Psuccessive analysis\nframes{aj}j=1..Pand their matching results (mj, sj). If\namong these Pidentiï¬cations, more than Tvoteof them are\n123Poster Session 1\ncoherent the best match is considered to be a correct identiï¬-\ncation. Otherwise, it is an out-of-base query. Two matching\nresults (mi,âˆ†ti)and(mj,âˆ†tj)of the ithand the jthanal-\nysis frames are coherent if:\n/braceleftbiggmi=mj\nsiâˆ’i.la.(1âˆ’oa) =sjâˆ’j.la.(1âˆ’oa)(3)\nTvotecan take any integer value between 0andP. A small\nvalue for Tvotewill increase the risk of false alarms whereas\na high value for Tvotewill increase the risk of missed detec-\ntions. In practice, a reasonable value for Tvoteis:\nTvote=/ceilingleftbiggP\n2/ceilingrightbigg\n(4)\n3.6 Database pruning\nWe propose an optional step meant to decrease the complex-\nity of the overall processing. First, we deï¬ne a simpliï¬ed\nhashing function which, for each pair of spectral peaks, re-\nturns only the references possessing that pair.\nh2:p/mapstoâˆ’â†’{ mi/ poccurs in mi} (5)\nNbeing the total number of references, we deï¬ne the sig-\nniï¬cance of a spectral pair pby:\ns(p) =Nâˆ’card(h2(p))\nN(6)\nBasically a pair which appears in many references will not\nbring a lot of information during the identiï¬cation process\n(and thus has a low signiï¬cance). Furthermore, it will in-\ntervene in many reference histograms and will thus involve\nmany calculations. On the other hand, a pair which points\nto a small number of references allows to converge more\nquickly towards the best match.\nPruning the database consists of, for a given threshold\nTprune , erasing from the database all the pairs verifying\ns(p)< Tprune . When doing so, we suppose that for any\nreference there will be a sufï¬cient number of pairs kept in\norder to ensure a correct identiï¬cation. This, of course, de-\npends on the statistical distribution of the pairs and on the\nselected threshold Tprune . We have experimentally veriï¬ed\nthat the use of a reasonable threshold leads to a signiï¬cant\ncomplexity gain while keeping similar performances (see\nsection 4.3.4).\n4. EV ALUATION\n4.1 Framework\nThe evaluation framework used in this work is similar to the\none developed in the European project OSEO-Quaero1. It\n1http://quaero.orgis deï¬ned as follows. The audio stream is the broadcast of\na radio station. As the corpus comes from real radio broad-\ncasts, it potentially contains all the radio sound processing\nwe described (see section 2). The references are 1 minute-\nlong excerpts of songs. The broadcast stream has been man-\nually annotated and can thus serve for direct evaluation. For\neach broadcast reference, the annotation states the identiï¬er\nof the reference, its broadcast time and duration.\nThe task of the algorithm is to scan the broadcast and\noutput a detection message whenever a song among the ref-\nerences occurs in the stream. The algorithm gives the iden-\ntiï¬er of the detected song as well as its occurrence time. If\nthe detection time is comprised between the annotated start\ntime and the annotated end time of one occurrence of the\nsame song, we make this occurrence a detected occurrence .\nLet us note that multiple detection messages of the same oc-\ncurrence will be counted only once. If the algorithm detects\na song during an empty slot, or during a slot containing an-\nother song, we count one false alarm. We do not limit the\ncounting of false alarms.\n4.2 Comparative experiment\n4.2.1 Objectives\nWe have compared three different algorithms according to\nthe framework described above. The ï¬rst one (â€œWangâ€) is\nour own implementation of Wangâ€™s method [2]. The second\none (â€œI B&Sâ€) is the algorithm called IRCAM Bark & Sone\nin [10]. The last one (â€œSAFâ€, for Scalable Audio Fingerprint\nmethod) is the method exposed in this article.\nAs far as our implementations are concerned (Wang and\nSAF), they both rely on the same architecture, as described\nin section 2. All the parameters which are not directly linked\nto the ï¬ngerprint (framing parameters and post-processing\nparameters) are the same for both algorithms. In other words,\nthe two systems have the same architecture with the same\nparameters. Only the ï¬ngerprint model does differ.\n4.2.2 Data\nIn this experiment, the stream is made of 7 days of the French\nradio RTL. The one minute long references are extracted\nfrom 7309 songs. The broadcast stream contains 459 occur-\nrences of these references.\nLet us note that it happens that a given version of a music\ntitle is in the references, whereas another version of the same\ntitle is broadcast. This typically happens when an artist is\ninvited on a radio show and performs some of his titles live.\nIn this case, even if the studio versions of the artistâ€™s titles\nare in the references, the algorithm is not required to match\nthe studio version with the live performance. Indeed, the\nrecognition of different interpretations of the same song is\nconsidered to be out of the scope of this work.\n12412th International Society for Music Information Retrieval Conference (ISMIR 2011)\n4.2.3 Parameters\nWe have used 5s long analysis frames with a 50% over-\nlap. The post-processing parameters have been set to P=\n12andTvote= 6. This means that the detection is per-\nformed on 30s of signal, and requires that at least half of the\nmatching during these 30s has given a coherent identiï¬ca-\ntion. Such parameters insure a very low rate of false alarms,\nwhich is required in many use-cases for audio-ï¬ngerprint.\n4.2.4 Results\nAlgorithm Detected occ. / Total nb False Alarms\nWang [2] 381 / 459 (=83.0%) 0\nI B&S [10] 445 / 459 (=96.9%) 2\nSAF (proposed) 447 / 459 (=97.4%) 0\nTable 1 : Results of the comparative experiment\nWe can see in Table 1 that the detection ratio is much\nhigher with our ï¬ngerprint than the original model of Wang.\nAs far as we can tell, this really comes from the fact that a\nnon-negligible number of broadcast songs are pitch-shifted.\nThese results therefore show that, in addition to being robust\nto the same distortions as Wangâ€™s model, our ï¬ngerprint has\nan increased robustness to pitch-shifting. Besides, we can\nsee that the post-processing plays its role very efï¬ciently. It\nhas prevented all the false alarms (in both algorithms Wang\nand SAF) and still has allowed a very high detection rate.\n4.2.5 Runtime\nWe will give here some ï¬gures about the processing times\nof the algorithms. These ï¬gures are given on the basis of\nour MatlabR/circlecopyrt64-bits implementations, running on an IntelR/circlecopyrt\nCore 2 Duo @ 3,16 GHz with 6MB of Cache and 8GB of\nRAM. We are aware that these ï¬gures give no absolute truth,\nsince the processing times highly depend on the machines,\nthe programming language and the optimization of the code.\nThey nevertheless give an order of magnitude of the run-\ntimes with such a conï¬guration. Besides, they allow a com-\nparison of the different algorithms since all running times\nare given on the same basis.\nThe algorithm â€œWangâ€ has a processing time of 0.08s per\nsecond of signal. The algorithm â€œSAFâ€ has a processing\ntime of 0.43 seconds per second of signal. The difference\nmainly comes from the extra time required for the calcula-\ntion of the constant Q transform. If we apply the pruning\ntechnique described in section 3.6 with Tprune = 0.5, we\nobtain a speed-up factor of 35%. This reduces the process-\ning time of the second algorithm to 0.28 seconds per second\nof signal with the exact same identiï¬cation score.4.3 Scaling experiment\n4.3.1 Objectives\nWe have led a second experiment in order to validate the po-\ntential scalability of the system we propose. The framework\nis the same as in the previous experiment, but we now run\nthe algorithm with a much larger references database.\n4.3.2 Data\nIn this experiment the stream is made of 5 days of radio\nbroadcast coming from 2 different French radio stations (RTL,\nVirgin Radio). The references set is much larger as it con-\ntains 30.000 songs.\n4.3.3 Results\nAlgorithm Detected occ. / Total nb. False Alarms\nSAF (proposed) 496 / 506 (=98.0%) 0\nTable 2 : Results of the scaling experiment\n(30.000 songs)\nThe results clearly show that the algorithm is scalable.\nIt has achieved a detection performance which is compara-\nble to its performance in the ï¬rst experiment. Though, the\nreferences database is more than 4 times larger in this ex-\nperiment. It is particularly noticeable that in spite of the\nenlargement of the database, the system has still not out-\nput any false alarm. The multiplication of the songs in the\ndatabase had yet highly increased the risk of having close\nï¬ngerprints for different songs.\nAs far as the detection performance is concerned, the re-\nsults of this experiment show that the algorithm we propose\nhas the ability to handle industrial sized databases.\n4.3.4 Runtime\nThe basis for the following calculation time is the same as in\nsection 4.3.4. With the 30.000 songs database, the algorithm\n(without pruning) runs at a speed of 1.44 seconds per sec-\nond of signal. If we compare this running time with the one\nof the smaller scale experiment, we notice that the multipli-\ncation of the database size by 4 has lead to a multiplication\nof the processing time by 3,3. The increase of the running\ntime is thus sub-linear with the number of references. We\ncan also note that, even though the code has not been fully\noptimized, the algorithm almost runs in real-time.\n5. CONCLUSION\nIn this article, we have proposed a new ï¬ngerprint model.\nWe have included this ï¬ngerprint in a global architecture.\n125Poster Session 1\nThe overall system is able to process audio streams in accor-\ndance with a radio monitoring use-case. The ï¬ngerprint we\npropose is inspired by Wangâ€™s work [2] from which we have\nreproduced the indexing scheme based on pairs of spectral\npeaks. But our use of the constant Q transform and our\nproposition of a different encoding for pairs of peaks allows\nus to show a much increased robustness to pitch-shifting.\nThis, in turn, greatly improves our identiï¬cation results on\nreal radio broadcasts, as it has been shown in the compar-\native experiment presented. As far as scalability is con-\ncerned, we presented a second experiment which is based\non a 30.000 songs database. This proved that our system\neasily scales up, while keeping a high detection ratio and a\nreasonable calculation time. In the future, we will focus on\nthe problem brought up in section 4.2.2. The annotations\nwe used indeed contain an average 7% of live versions of\ntitles stored in the references database in their studio ver-\nsions. Matching the ones with the others is a problem that\nlies somewhere between audio ï¬ngerprint and cover song\ndetection. It will be interesting to study an extend of the\nï¬ngerprint system which would be able to do this match-\ning. Such an extended system will probably need to inte-\ngrate more semantically based information.\n6. REFERENCES\n[1] P. Cano, E. Batlle, E. Gomez, L. de C.T. Gomes, and\nM. Bonnet, â€œAudio Fingerprinting: Concepts and Appli-\ncations,â€ in 1st International Conference on Fuzzy Sys-\ntems and Knowledge Discovery , (Singapore), November\n2002.\n[2] A. Wang, â€œAn Industrial-strength Audio Search Al-\ngorithm,â€ in ISMIR 2003, 4th Symposium Conference\non Music Information Retrieval , (Baltimore, Maryland,\nUSA), pp. 7 â€“ 13, October 2003.\n[3] J. Haitsma, T. Kalker, and J. Oostveen, â€œRobust audio\nhashing for content identiï¬cation,â€ in CBMI, Content-\nBased Multimedia Indexing , (Brescia, Italy), September\n2001.\n[4] P. Cano, E. Battle, H. Mayer, and H. Neuschmied, â€œRo-\nbust Sound Modeling for Song Detection in Broadcast\nAudio,â€ in AES, 112th Audio Engineering Society Con-\nvention , (Munich, Germany), p. 5531, May 2002.\n[5] E. Weinstein and P. Moreno, â€œMusic identiï¬cation with\nweighted ï¬nite-state transducers,â€ in ICASSP â€™07, IEEE\nInternational Conference on Acoustics, Speech and Sig-\nnal Processing , vol. 2, (Honolulu, HI), pp. 689â€“692,\nApril 2007.\n[6] E. Allamanche, J. Herre, O. Hellmuth, B. Frba, T. Kast-\nner, and M. Cremer, â€œContent-based Identiï¬cation ofAudio Material Using MPEG-7 Low Level Descrip-\ntion,â€ in ISMIR 2001, 2nd International Symposium on\nMusic Information Retrieval , (Bloomington, Indiana,\nUSA), October 2001.\n[7] E. Dupraz and G. Richard, â€œRobust frequency-based au-\ndio ï¬ngerprinting,â€ in ICASSP 2010, IEEE International\nConference on Acoustics, Speech and Signal Processing ,\n(Dallas,USA), pp. 2091â€“2094, March 2010.\n[8] J. C. Brown, â€œCalculation of a constant Q spectral trans-\nform,â€ Journal of the Acoustical Society of America ,\nvol. 89, no. 1, pp. 425â€“434, 1991.\n[9] J. C. Brown and M. S. Puckette, â€œAn efï¬cient algorithm\nfor the calculation of a constant Q transform,â€ Journal\nof the Acoustical Society of America , vol. 92, no. 5,\npp. 2698â€“2701, 1992.\n[10] M. Ramona and G. Peeters, â€œAudio Identiï¬cation based\non Spectral Modeling of Bark-bands Energy and Syn-\nchronization through Onset Detection,â€ in ICASSP\n2011, IEEE International Conference on Acoustics,\nSpeech and Signal Processing , (Prague, Czech Repub-\nlic), May 2011.\n126"
    },
    {
        "title": "Causal Prediction of Continuous-Valued Music Features.",
        "author": [
            "Peter Foster",
            "Anssi Klapuri",
            "Mark D. Plumbley"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1418337",
        "url": "https://doi.org/10.5281/zenodo.1418337",
        "ee": "https://zenodo.org/records/1418337/files/FosterKP11.pdf",
        "abstract": "This paper investigates techniques for predicting sequences of continuous-valued feature vectors extracted from musical audio. In particular, we consider prediction of beatsynchronous Mel-frequency cepstral coefficients and chroma features in a causal setting, where features are predicted as they unfold in time. The methods studied comprise autoregressive models, N-gram models incorporating a smoothing scheme, and a novel technique based on repetition detection using a self-distance matrix. Furthermore, we propose a method for combining predictors, which relies on a running estimate of the error variance of the predictors to inform a linear weighting of the predictor outputs. Results indicate that incorporating information on long-term structure improves the prediction performance for continuous-valued, sequential musical data. For the Beatles data set, combining the proposed self-distance based predictor with both N-gram and autoregressive methods results in an average of 13% improvement compared to a linear predictive baseline.",
        "zenodo_id": 1418337,
        "dblp_key": "conf/ismir/FosterKP11",
        "keywords": [
            "predicting",
            "sequences",
            "continuous-valued",
            "feature",
            "vectors",
            "extracted",
            "musical",
            "audio",
            "causal",
            "setting"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nCAUSAL PREDICTION OF CONTINUOUS-VALUED MUSIC FEATURES\nPeter Foster, Anssi Klapuri, Mark D. Plumbley\nCentre for Digital Music\nQueen Mary University of London\nMile End Road, London E1, UK\n{peter.foster,anssi.klapuri,mark.plumbley }@eecs.qmul.ac.uk\nABSTRACT\nThis paper investigates techniques for predicting sequences\nof continuous-valued feature vectors extracted from musi-\ncal audio. In particular, we consider prediction of beat-\nsynchronous Mel-frequency cepstral coefï¬cients and chroma\nfeatures in a causal setting, where features are predicted as\nthey unfold in time. The methods studied comprise autore-\ngressive models, N-gram models incorporating a smoothing\nscheme, and a novel technique based on repetition detec-\ntion using a self-distance matrix. Furthermore, we propose\na method for combining predictors, which relies on a run-\nning estimate of the error variance of the predictors to in-\nform a linear weighting of the predictor outputs. Results in-\ndicate that incorporating information on long-term structure\nimproves the prediction performance for continuous-valued,\nsequential musical data. For the Beatles data set, combining\nthe proposed self-distance based predictor with both N-gram\nand autoregressive methods results in an average of 13% im-\nprovement compared to a linear predictive baseline.\n1. INTRODUCTION\nOur goal is to devise methods for predicting music in a\ncausal setting. Given a stream of observed music feature\nvectors extracted from an audio signal, we seek to predict\nfuture values of feature vectors. Furthermore, we seek to\nincorporate domain knowledge about the underlying music\nsignal into the prediction process: Across musical genres,\nmusic exhibits hierarchical temporal structure, arising cen-\ntrally from the identity relations between structural elements\n[11]. In Western music, elementary events are typically\nrhythmic, melodic or harmonic and give rise to long-term\nstructure characteristic of a pieceâ€™s musical form, through\napplication of variation and repetition. Conversely, identify-\ning parallelism in music â€” the occurrence of variation and\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proï¬t or commercial advantage and that copies\nbear this notice and the full citation on the ï¬rst page.\nc/circlecopyrt2011 International Society for Music Information Retrieval.repetition â€” is agreed to bear great importance in music-\ntheoretical analysis [2].\nThis view may be considered to encompass cognitive pro-\ncesses involved in music listening. Here, music consists of\na stream of events unfolding in time and experienced by a\nlistener [9]. The listening process is associated with predic-\ntions of future events, which depend on the listenerâ€™s evolv-\ning internal model of musical structure generated by previ-\nously observed events in the stream of music. This work\nis based on this causal prediction setting, where at a given\npoint in time only events in the past inform predictions.\nAccurate prediction of spectro-temporal features, such\nas Mel-frequency cepstral coefï¬cients (MFCCs), chroma or\nrhythmograms [15], is motivated by a number of applica-\ntions. Firstly, audio visualisation tasks might beneï¬t from\nprediction, since live performance environments typically\nconstrain the permissible amount of latency introduced in\nthe audio processing chain [6]. Similarly, it is of interest to\ninvestigate robust real-time audio streaming applications for\nlive music performance [10]. In the latter case, employing\nprediction techniques might allow the effect of network la-\ntency to be offset. Further applications of audio based pre-\ndiction are automated musical accompaniment [8, 20] and\naudio feature models for automated music transcription.\nIn addition, prediction accuracy can be related to the as-\nsumed model of the underlying distribution of observations.\nIn terms of inductive inference [19], accurate prediction re-\nlates to effective data compression of observations. This re-\nlationship might be exploited in online music content analy-\nsis applications. Existing work has examined the problem of\nofï¬‚ine music content analysis, where compressibility is used\nto evaluate structural similarity between pieces of music [1].\nA related application is information-dynamic modelling of\nmusical audio [4].\nIn this work, we evaluate several prediction methods, in-\ncluding autoregressive models, N-gram models, and a novel\ntechnique based on utilising the long-term structure of mu-\nsic signals. In addition, we propose a method for combining\npredictors by estimating predictorsâ€™ error variance. We con-\nsider chroma and MFCC features, which describe harmonic\nand timbral information in musical audio signals [15]. Re-\n501Poster Session 4\nsults indicate that combining the self-distance approach with\nautoregressive or N-gram models substantially improves the\naccuracy of predicting continuous-valued music features.\n1.1 Causal music feature prediction\nSuppose we have a sequence of vectors v1, . . . ,vT, corre-\nsponding to Tfeature observations made at times Ï„1, . . . , Ï„ T.\nEach vector occupies k-dimensional feature space, vâˆˆRk,\naccording to an unknown probability distribution. Causal\nprediction involves approximating the unknown conditional\nprobability distribution p(vt|v1, . . . ,vtâˆ’1). The predicted\nfeature at time Ï„tis then obtained by computing the expecta-\ntionE[vt|v1, . . . ,vtâˆ’1]. The prediction task is causal, since\nobservations v1, . . . ,vtâˆ’1inform predictions vt. Succes-\nsive predictions are formed by increasing t, so that the ob-\nservation history accumulates over time.\nCausal predictive models have been applied to music in\nsymbolic formats [16]. In the audio domain, the concern of\nour presented work, [8] proposes an approach for prediction\ndriven musical expectation modelling. In [3] prediction is\nexamined in the context of planning, as a means of creating\nanticipatory music systems. In [20] a method is proposed\nfor automatic harmonic accompaniment based on repetition\ndetection.\n2. PREDICTION TECHNIQUES\nWe investigate prediction techniques for beat-synchronous\nchroma and MFCC features, as described in the following.\n2.1 Autoregressive models\nIn a multivariate autoregressive (MAR) model [12], pre-\ndicted feature vectors vtare computed as linear combina-\ntions of Npreceding feature vectorsâ€™ components. Correla-\ntion between separate components is taken into account, so\nthat\nvt=N/summationdisplay\nn=1Anvtâˆ’n+rt (1)\nwhere matrices Anincorporate information on correlations\nbetween between components of vtâˆ’nandvt. Vector rt\nis an independent and identically distributed Gaussian noise\nterm.\nLet us use vt,uto denote the uth component of vector\nvt. A special case of the MAR model arises when indepen-\ndence between feature components is assumed. In that case,\nmatrices Anare diagonal, so that\nvt,u=N/summationdisplay\nn=1an,uvtâˆ’n,u+rt,u (2)\nwith 1â‰¤uâ‰¤k. Coefï¬cients rt,uare described by kuni-\nvariate Gaussian noise processes with ï¬nite mean and vari-ance. The model in Equation 2 is equivalent to a component-\nwise linear predictive coding (LPC) model, with each LPC\nmodel deï¬ned by index u.\n2.2 N-gram prediction\nN-gram models have been used to model symbolic music\n[16]. In this model, observations are quantised. Let etde-\nnote a quantised observation symbol. Symbols are members\nof a speciï¬ed alphabet A. For convenience, we use etâˆ’1\ntâˆ’n\nto denote the sequence of symbols etâˆ’n, etâˆ’n+1, . . . , e tâˆ’1.\nThe conditional probability of predicted event et, given the\nhistory of observations is assumed to obey the Markov prop-\nerty. That is, p(et|etâˆ’1\n1) =p(et|etâˆ’1\ntâˆ’n), where nis the or-\nder of the Markov model. An estimator for this conditional\nprobability is\np(et|etâˆ’1\ntâˆ’n) =c(et|etâˆ’1\ntâˆ’n)/summationtext\neâˆˆAc(e|etâˆ’1\ntâˆ’n)(3)\nwhere c(et|etâˆ’1\ntâˆ’n)denotes the number of times symbol et\nhas been observed following context etâˆ’1\ntâˆ’n, computed over\nthe entire observation sequence etâˆ’1\n1. To estimate the prob-\nability of unobserved events, we incorporate a smoothing\napproach [14], so that recursively,\np(et|etâˆ’1\ntâˆ’n) =/braceleftBigg\nÎ±(et|etâˆ’1\ntâˆ’n) forc(et|etâˆ’1\ntâˆ’n)>0\nÎ³(etâˆ’1\ntâˆ’n)p(et|etâˆ’1\ntâˆ’n+1)otherwise.\n(4)\nIn Equation 4, Î±(Â·|Â·)is deï¬ned as follows. It is used as\nlong as the sequence et\ntâˆ’nhas previously been observed at\nleast once. Alternatively, the conditional probability is re-\ncursively evaluated using a function Î³(Â·)and a lower order\nestimation p(et|etâˆ’1\ntâˆ’n+1).\nAs employed in [8], Î±(Â·|Â·)andÎ³(Â·)are deï¬ned as\nÎ³(etâˆ’1\ntâˆ’n) =d(etâˆ’1\ntâˆ’n)/summationtext\neâˆˆAc(e|etâˆ’1\ntâˆ’n) +d(etâˆ’1\ntâˆ’n)(5)\nÎ±(et|etâˆ’1\ntâˆ’n) =c(et|etâˆ’1\ntâˆ’n)/summationtext\neâˆˆAc(e|etâˆ’1\ntâˆ’n) +d(etâˆ’1\ntâˆ’n)(6)\nwhere d(etâˆ’1\ntâˆ’n)denotes the number of distinct symbols ob-\nserved as continuations of context etâˆ’1\ntâˆ’n. Intuitively, as d(Â·)\nincreases, more emphasis is placed on shorter contexts when\nestimating unobserved symbol probabilities.\nSince the N-gram model is based on an alphabet of dis-\ncrete symbols, we quantise our continuous-valued feature\nvectors prior to learning this model. This is achieved using\nonline k-means clustering, described in Section 3.2.\n2.3 Repetition detection\nWe propose the use of a repetition detection algorithm to in-\nform predictions in conjunction with autoregressive and N-\ngram approaches. To incorporate information on long-term\n50212th International Society for Music Information Retrieval Conference (ISMIR 2011)\nstructure as described in Section 1, the similarity between\nfeature vector sequences is computed during the prediction\nprocess. As incorporated in [20], the approach uses a self-\ndistance matrix (SDM). Given observations v1, . . . ,vtâˆ’1,\nthe SDM Dis deï¬ned as [D]i,j=d(vi,vj), with 1â‰¤i, j <\nt. As proposed in [7], for the distance function d(Â·,Â·)we use\nthe cosine distance,\nd(vi,vj) = 0 .5/parenleftbigg\n1âˆ’viÂ·vj\n/bardblvi/bardbl /bardblvj/bardbl/parenrightbigg\n. (7)\nAssume a predeï¬ned sequence comparison length L. We\nuse the SDM to consider all alignments between past se-\nquences vsâˆ’L, . . . ,vsâˆ’1and the most recently observed fea-\nture vectors vtâˆ’L, . . . ,vtâˆ’1, with L < s < t . Comparing\nvector-wise with the most recently observed feature vectors,\nthe past sequence with minimal average distance is selected\nas the conjectured repeated sequence. This sequence is used\nfor prediction, assuming that vtâ‰ˆvs. With L < t â‰¤T,\nthetth prediction wtis obtained using index pof past vector\nvp, with\np= argmin\nL <s <t{dÂµ(s, t)}, (8)\nwhere dÂµ(s, t)denotes the average distance between two\nsubsequences of length L,\ndÂµ(s, t) =1\nLL/summationdisplay\n/lscript=1[D]sâˆ’/lscript,tâˆ’/lscript. (9)\nComputing the entire sequence of predictions has poly-\nnomial time complexity against the total sequence length T,\nsince each prediction at step trequires O(T)operations. We\nobserve that using beat-synchronous features results in an\naverage sequence length of approximately 650, for the data\nset of popular music chosen for evaluation. Therefore scal-\nability is not thought to restrict the algorithmâ€™s utility, for\nmusic signals with similar duration to those in the data set.\nFurthermore, it is possible to deal with longer music sig-\nnals by imposing a maximum size on the SDM, discarding\nobservations which fall outside a speciï¬ed history limit.\n2.4 Combining multiple predictors\nTo combine the predictions generated by the SDM and N-\ngram approaches, we propose a linear weighting scheme\nbased on estimated variance of error1. For a set of Mpre-\ndictors, deï¬ne the tth prediction by each predictor vi\nt, with\n1â‰¤iâ‰¤M. Deï¬ne the true value of the tth vector to be vâˆ—\nt.\nWe assume an observation model where predictions vi\ntare\nthe sum of observations vâˆ—\ntand an error term /epsilon1i\nt,\nvi\nt,u=vâˆ—\nt,u+/epsilon1i\nt,u (10)\nwhere indices udenote vector components, with 1â‰¤uâ‰¤k.\nWe assume components /epsilon1i\nt,uto be normally distributed, with\n1The method is similar in spirit to aggregation methods reviewed in\n[21].variance Ïƒ2\ni,u. Using Hpredictions as samples, the variance\nof the error Ïƒ2\ni,ucan be estimated as\nË†Ïƒ2\ni,u=1\nHâˆ’1H/summationdisplay\nh=1/parenleftbig\nvi\ntâˆ’h,uâˆ’vâˆ—\ntâˆ’h,u/parenrightbig2. (11)\nBecause the error is assumed to be normal, we have p(vi\nt,u|vâˆ—\nt,u) =\nN(vâˆ—\nt,u, Ïƒ2\ni,u). Using Bayesâ€™ theorem, we have\np(vâˆ—\nt,u|vi\nt,u) =p(vi\nt,u|vâˆ—\nt,u)p(vâˆ—\nt,u)\np(vi\nt,u). (12)\nIf we assume the ratio of p(vi\nt,u)andp(vâˆ—\nt,u)is non-\ninformative, we then have p(vi\nt,u|vâˆ—\nt,u) =p(vâˆ—\nt,u|vi\nt,u). We\nfurther assume independence between predictors and denote\nÎ²i,u= 1/Ïƒ2\ni,ufor notational convenience. Then, the distri-\nbution of vâˆ—\nt,ucan be expressed as\np(vâˆ—\nt,u|v1\nt,u, . . . , vM\nt,u) =M/productdisplay\ni=1N/parenleftbigg\nvâˆ—\nt,u;vi\nt,u,1\nÎ²i,u/parenrightbigg\n=N/parenleftBigg\nvâˆ—\nt,u;/summationtextM\ni=1Î²i,u/summationtextM\nj=1Î²j,u,1/summationtextM\ni=1Î²i,u/parenrightBigg\n.\n(13)\nGiven all predictions, the expected value of vâˆ—\nt,u,E[vâˆ—\nt,u]is\nthen the weighted sum\nE/bracketleftbig\nvâˆ—\nt,u/bracketrightbig\n=/summationtextM\ni=1Î²i,uvi\nt,u/summationtextM\nj=1Î²j,u. (14)\nEquation 14 describes the weighting scheme used to com-\nbine multiple predictions. Note that values Î²i,udescribe the\nprecision of prediction method i, estimated over prediction\nhistory of length H.\n3. METHOD\nThe data set used for evaluation consists of 180 mono audio\ntracks of songs by The Beatles, with each track sampled at\n44.1kHz [13].\n3.1 Feature extraction\nWe extract beat-synchronous chroma features using the ap-\nproach and implementation described in [5]. These chroma\nfeatures are based on the mapping of FFT bins to twelve\npitch class components, using phase derivatives to reduce\nthe inï¬‚uence of non-tonal components present in the spec-\ntrum. Chroma frames are based on an FFT window size\nof 2048 with 75% overlap. This approach compensates for\nmistuning by computing the optimal alignment between fre-\nquency peaks and chroma bins over the entire signal.\nFurthermore, we extract beat-synchronous MFCCs, us-\ning the approach and implementation described in [18]. The\nMFCCs are based on an FFT window size of 512 with 50%\n503Poster Session 4\noverlap. The ï¬lter bank consists of 13 linearly spaced ï¬l-\nters and 27 log spaced ï¬lters. We extract the 12 ï¬rst cepstal\ncoefï¬cients, omitting the d.c. coefï¬cient. Beat-synchronous\nMFCCs are then obtained by computing mean feature val-\nues within each beat onset interval, applying the same onset\nintervals used for chroma feature extraction.\nThe beat onset times are estimated using the code and\napproach described in [5]. In terms of the causal predic-\ntion problem which this work addresses, we note that the\nmethodâ€™s application of dynamic programming is non-causal.\nIn this work, we treat the beat tracking routines as an oracle\nfor obtaining beat onset times.\n3.2 Online clustering\nTo obtain discrete symbols for the N-gram predictor, we\nquantise observed feature vectors using online k-means clus-\ntering. As described in [8], an initial codebook of Kcen-\ntroids Âµ1, . . . , Âµ Kis constructed according to the ï¬rst Qob-\nserved symbols. Thereafter, upon observing feature vâˆ—\nt, the\nclosest centroid\nÂµt= argmin\n1â‰¤kâ‰¤K/braceleftbig\n/bardblvâˆ—\ntâˆ’Âµk/bardbl2/bracerightbig\n(15)\nis updated according to\nÂµt:=Âµt+Î·(vâˆ—\ntâˆ’Âµk). (16)\nIn our evaluation, we set Q=K. A hold-out set of 60 ran-\ndom songs is formed. A learning factor of Î·= 0.4is deter-\nmined, based on MFCC and chroma prediction performance\nand using the described data set with a ï¬xed codebook size\nofK= 64 . For ï¬xed Î·= 0.1, alternative strategies for\ncodebook construction were evaluated, involving initialisa-\ntion to held out data. However, these revealed no compelling\nimprovement over the aforementioned method, in terms of\nN-gram prediction performance.\nFor the N-gram predictor, prediction proceeds causally,\nso that after the tth prediction, N-gram probabilities are up-\ndated to include the actually observed symbol eâˆ—\ntand its con-\ntextetâˆ’1\ntâˆ’n. The N-gram predictor is learned using only ob-\nservations from the target song. Given the average length of\n650 symbols per song, we estimate the required codebook\nsize to be in the order ofâˆš\n650â‰ˆ25symbols. Consider-\ning that the N-gram model incorporates a smoothing scheme\n(cf. Equation 4), we set the Markov order to constant n= 5,\nobserving similar prediction performance for n= 2. Using\nthe held-out data set of 60 songs, we set respective SDM\nprediction lengths L= 22 ,L= 36 , which maximise pre-\ndiction performance for chroma and MFCCs.\n3.3 Performance statistics\nThe statistics used for evaluation are the sum of squares er-\nror (SSE), the Jensen-Shannon divergence (JSD) and the\nabsolute deviation (AD). The SSE for the tth prediction iscomputed as\nSSE(vt,vâˆ—\nt) =/bardblvtâˆ’vâˆ—\nt/bardbl2. (17)\nThe JSD is a symmetrised version of the Kullback-Leibler\ndivergence. It is computed as\nJSD(vt/bardblvâˆ—\nt) =1\n2KL(vt, F) +1\n2KL(vâˆ—\nt, F)(18)\nwhere KL(Â·/bardblÂ·)denotes the Kullback-Leibler divergence and\nFis deï¬ned as\nF=1\n2(vt+vâˆ—\nt). (19)\nFinally, the absolute deviation is computed as\nAD(vt,vâˆ—\nt) =/bardblvtâˆ’vâˆ—\nt/bardbl1 (20)\nwhere /bardbl Â· /bardbl1denotes the /lscript1-norm.\nWe compute the statistics for all predictions and average\nover predictions in the entire data set. For example, the av-\nerage sum of squares error SSE Âµis computed as\nSSE Âµ=1\nTT/summationdisplay\nt=1SSE(vt,vâˆ—\nt). (21)\nAverage prediction results therefore describe vector-wise pre-\ndiction error and do not account for variability in song du-\nration. We compute 99% conï¬dence intervals on average\nperformance data. Relative to LPC prediction performance,\nconï¬dence intervals do not exceed 3.4%,1.8%,0.2%, in\nterms of average SSE, JSD and AD, respectively.\n4. RESULTS\nWe evaluate autoregressive, N-gram and SDM predictors.\nDesignating the LPC predictor as a baseline, Figure1 illus-\ntrates prediction performance relative to the LPC baseline,\nin terms of average SSE, JSD, AD. Performance values are\nexpressed as the quotient S/B, where Sis the average pre-\ndiction error of the sample and Bis the average prediction\nerror of the LPC baseline.\n4.1 Single predictor performance\nWe ï¬rst consider the accuracy of individual predictors, with\nno method of combining them applied. On the left hand\nside of Figure 1 (a), (b), we include results for four predic-\ntion techniques. Based on the assumption of local station-\narity, the predictor termed â€˜Copyâ€™ estimates the tth predic-\ntion as vc\nt=vâˆ—\ntâˆ’1. The predictor termed â€˜LPCâ€™ applies\nthe linear predictor described in Equation 2. The predic-\ntor termed â€˜MARâ€™ performs multivariate autoregression ac-\ncording to Equation 1. The predictor termed â€˜SDMâ€™ corre-\nsponds to repetition detection using a self-distance matrix,\nas described in Section 2.3. For both LPC and MAR pre-\ndictors, all observations vâˆ—\n1, . . . ,vâˆ—\ntâˆ’1are incorporated into\n50412th International Society for Music Information Retrieval Conference (ISMIR 2011)\na least-squares regression [12]. Results are reported for sec-\nond order LPC models (chroma), third order LPC models\n(MFCC) and ï¬rst order MAR models (chroma and MFCC),\nwith orders selected to maximise held-out data performance.\nConsidering chroma feature prediction in Figure 1 (a),\nwe observe that Copy prediction is signiï¬cantly outperformed\nby all remaining predictors, for all evaluated statistics. Ob-\nserving that the MAR model is outperformed by LPC based\nprediction, it appears that for the given sequence lengths and\nthe chosen features, it is preferable to assume independence\nbetween feature components.\nFor the considered codebook sizes, the N-gram model is\nalmost consistently outperformed by the LPC predictor. To\nreduce the error that is due to quantisation alone, we weight\npredicted feature vectors using the linear combination (1âˆ’\nÎ³)vn+Î³vc, where vnis the discrete N-gram prediction.\nParameter Î³is varied within the unit interval, in steps of\n0.1. Based on 10 Ã—2 cross-validation on the remaining 120\nsongs, results are reported for Î³= 0.4, which maximises\nSSE performance for both chroma and MFCC features. In\nFigure 1, this predictor is termed â€˜Weightedâ€™.\nConsidering MFCC feature prediction in Figure 1 (b),\nwe observe that SDM prediction offers less advantage over\nCopy prediction, compared to chroma prediction.\nTurning to the effect of increasing codebook size, we\nobserve that SSE performance improves for chroma pre-\ndictions. Surprisingly, for MFCC prediction increasing the\ncodebook size adversely affects SSE performance. In both\ncases, JSD and AD performance degrades when increasing\ncodebook size.\n4.2 Combined predictor performance\nResults for combinations of predictors are shown on the\nright hand side of Figure 1 (a), (b). To restrict the param-\neter space, the evaluation is based on the aforementioned\nbaseline results. Thus, the linear chroma weighting factor is\nset to Î³= 0.4. Based on Equation 14, a running estimate of\nprecision values Î²iis formed using min{H, tâˆ’1}preceding\npredictions.\nResults for chroma and MFCC feature prediction reveal\nthat combining SDM with weighted N-gram approaches re-\nsults in substantial improvement over single predictor per-\nformance. The result is largely consistent across the evalu-\nated SSE, JSD and AD statistics. We observe a similar re-\nsult when combining LPC and SDM predictors. Compared\nto the latter result, combining LPC, SDM and weighted N-\ngram predictors further improves performance.\nFor comparison, a linear and constant weighting scheme\nwas evaluated. As reported in Figure 1 (â€˜constantâ€™), no im-\nprovement over history based weighting is obtained using\nthis approach.(a) Chroma feature prediction\n  0.8  0.9  1  1.1  1.2  1.3  1.4\nCopy\nLPC\nMAR\nSDM\nNâˆ’gram, K=16Nâˆ’gram, K=32Nâˆ’gram, K=64\nWeighted, K=32\nWeighted+SDM, H=100\nWeighted+SDM, H=10\nWeighted+SDM, H=3\nWeighted+SDM(constant)\nWeighted+LPC, H=10\nLPC+SDM, H=10\nWeighted+LPC+SDM, H=10Relative prediction errorSum of squares errorJensenâˆ’Shannon divergenceAbsolute deviation\n(b) MFCC feature prediction\n  0.8  0.9  1  1.1  1.2  1.3  1.4\nCopy\nLPC\nMAR\nSDM\nNâˆ’gram, K=16Nâˆ’gram, K=32Nâˆ’gram, K=64\nWeighted, K=32\nWeighted+SDM, H=100\nWeighted+SDM, H=10\nWeighted+SDM, H=3\nWeighted+SDM(constant)\nWeighted+LPC, H=10\nLPC+SDM, H=10\nWeighted+LPC+SDM, H=10Relative prediction errorSum of squares errorJensenâˆ’Shannon divergenceAbsolute deviation\nFigure 1 . Performance results for chroma and MFCC fea-\nture prediction. Parameter Kdenotes codebook size. Pa-\nrameter Hdenotes amount of prediction history used to in-\nform predictor combination. See main text for a descrip-\ntion of predictor labels. Absolute chroma performance val-\nues for the LPC baseline are 0.0568 (SSE) 0.0882 (JSD)\n0.453 (AD). Absolute MFCC performance values for the\nLPC baseline are 0.893 (SSE) 0.406 (JSD) 2.282 (AD).\n505Poster Session 4\nApproach Chroma MFCC Average\nN-gram (weighted) 5% 5% 5%\nLPC + SDM 14% 6% 10%\nN-gram (weighted) + SDM 15% 7% 11%\nN-gram (weighted) + SDM + LPC 16% 10% 13%\nTable 1 . Summary of average chroma and MFCC prediction\nperformance. Scores are gains relative to the LPC baseline.\n4.3 Summary of results\nTable 1 summarises the obtained results. For each statistic,\nwe describe performance gains relative to the LPC baseline,\naveraged across SSE, JSD and AD statistics.\nWe observe that using the weighted N-gram approach\nyields minor improvement over the baseline LPC method.\nThis result is consistent for both chroma and MFCC predic-\ntion tasks. A further result concerns the inclusion of the\nSDM approach: In combination with either weighted N-\ngram or LPC approaches, we observe average performance\ngains in excess of 6%. Average chroma prediction perfor-\nmance improves by at least 14%. Furthermore, combin-\ning N-gram and SDM predictors yields minor improvement\nover the analogous LPC and SDM combination.\n5. CONCLUSIONS AND FURTHER WORK\nIn this work, we have considered the problem of causal mu-\nsic prediction using MFCC and chroma features. We have\ncomparatively evaluated the performance of predictors for\nseries of continuous-valued and quantised feature vectors.\nWe have considered how musical parallelism might be har-\nnessed for causal prediction of spectro-temporal features.\nThe prediction approach proposed in this work is based on\nrepetition detection using a self-distance matrix.\nFor the evaluated statistics, combining the SDM predic-\ntor with LPC or N-gram approaches allows substantial im-\nprovements in prediction accuracy to be made, compared\nto the baseline. This suggests that incorporating informa-\ntion on long-term musical structure might have utility for\nthe causal prediction of spectro-temporal features.\nConsidering the obtained results, we plan investigations\nto determine the effectiveness of online quantisation, the\nprerequisite for applying discrete-event models such as the\nN-gram model used in this work. Furthermore, we aim\nto perform an evaluation of hierarchical language models\nbased on the N-gram model used in this work. Finally, we\naim to consider music prediction from a perceptual perspec-\ntive, to identify correlates between perceived musical simi-\nlarity and prediction accuracy.\n6. ACKNOWLEDGEMENTS\nThis work beneï¬ted from advice from Andrew Robertson,\nAdam Stark and Roger Dean. In addition, we would like tothank the anonymous reviewers for their comments.\n7. REFERENCES\n[1] J. Bello: â€œGrouping Recorded Music by Structural Similarity,â€\nProc. ISMIR , pp. 531â€“536, 2009.\n[2] I. Bent and W. Drabkin: Analysis. New Grove Handbooks in\nMusic , Macmillan, London, 1987.\n[3] A. Cont: Modeling musical anticipation: From the Time of\nMusic to the Music of Time , Ph.D. Thesis, University of Cali-\nfornia, San Diego, San Diego, 2010.\n[4] S. Dubnov: â€œUniï¬ed View of Prediction and Repetition Struc-\nture in Audio Signals with Application to Interest Point De-\ntection,â€ IEEE Transactions on Audio, Speech, and Language\nProcessing , Vol. 16, No. 2, pp.327â€“337, 2008.\n[5] D. Ellis and G. Poliner: â€œIdentifying â€˜Cover Songsâ€™ with\nBeat-synchronous Chroma Features,â€ Proc. Intern. Confer-\nence on Acoustics, Speech and Signal Processing , pp. 1429-\n1432, 2007.\n[6] S. Farner, A. Solvang, A. Saebo and U. Svensson: â€œEnsemble\nHand-clapping Experiments Under the Inï¬‚uence of Delay and\nVarious Acoustic Environments,â€ Journal of the Audio Engi-\nneering Society , Vol. 57, No. 12, pp. 1028â€“1041, 2009.\n[7] J. Foote: â€œVisualizing Music and Audio Using Self-similarity,â€\nProc. ACM Intern. Conference on Multimedia , pp. 77â€“80,\n1999.\n[8] A. Hazan: Musical Expectation Modelling from Audio: A\nCausal Mid-level Approach to Predictive Representation and\nLearning of Spectro-temporal Events , Ph.D. Thesis, Universi-\ntat Pompeu Fabra, Barcelona, 2010.\n[9] D. Huron: Sweet Anticipation: Music and the Psychology of\nExpectation , The MIT Press, Cambridge, MA, 2006.\n[10] B. Jung, J. Hwang, S. Lee, G. Kim, and H. Kim: â€œIncorporat-\ning Co-presence in Distributed Virtual Music Environment,â€\nProc. ACM Symposium on Virtual Reality Software and Tech-\nnology , pp. 206â€“211, 2000.\n[11] F. Lerdahl and R. Jackendoff: A Generative Theory of Tonal\nMusic , The MIT Press, Cambridge, MA. 1996.\n[12] H. L Â¨utkepohl: New Introduction to Multiple Time Series Anal-\nysis, Springer, Berlin, 2005.\n[13] M. Mauch, C. Cannam, M. Davies, S. Dixon, C. Harte, S.\nKolozali, D. Tidhar, and M. Sandler: â€œOMRAS2 Metadata\nProject 2009,â€ Proc. ISMIR , 2009.\n[14] A. Moffat: â€œImplementing the PPM Data Compression\nScheme,â€ IEEE Transactions on Communications , Vol. 38,\nNo. 11, pp. 1917â€“1921, 1990.\n[15] J. Paulus and A. Klapuri: â€œAcoustic Features for Music Piece\nStructure Analysis,â€ Proc. Intern. Conference on Digital Audio\nEffects , pp. 309â€“312, 2008.\n[16] M. Pearce and G. Wiggins: â€œImproved Methods for Statisti-\ncal Modelling of Monophonic Music,â€ Journal of New Music\nResearch , Vol. 33, No. 4, pp. 367â€“385, 2004.\n[17] T. Schneider and A. Neumaier: â€œAlgorithm 808: ARï¬tâ€“A Mat-\nlab Package for the Estimation of Parameters and Eigenmodes\nof Multivariate Autoregressive Models,â€ ACM Transactions on\nMathematical Software , Vol. 27, No. 1, pp. 58â€“65, 2001.\n[18] M. Slaney: â€œAuditory Toolbox: A MATLAB Toolbox for Au-\nditory Modeling Work,â€ Interval Research Corporation , 1998.\n[19] R. Solomonoff: â€œA Formal Theory of Inductive Inference: Part\n1 and 2,â€ Inform. Control , Vol. 7, pp. 224â€“254, 1964.\n[20] A. Stark and M. Plumbley: â€œPerformance Following: Real-\nTime Prediction of Musical Sequences Without a Score,â€ To\nappear in IEEE Transactions on Audio, Speech, and Language\nProcessing .\n[21] V. Vovk: â€œCompetitive On-line Statistics,â€ Intern. Statistical\nReview , Vol. 69, pp. 213â€“248, 2001.\n506"
    },
    {
        "title": "Multi-scale temporal fusion by boosting for music classification.",
        "author": [
            "RÃ©mi Foucard",
            "Slim Essid",
            "Mathieu Lagrange",
            "GaÃ«l Richard"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1416904",
        "url": "https://doi.org/10.5281/zenodo.1416904",
        "ee": "https://zenodo.org/records/1416904/files/FoucardELR11.pdf",
        "abstract": "Short-term and long-term descriptors constitute complementary pieces of information in the analysis of audio signals. However, because they are extracted over different time horizons, it is difficult to exploit them concurrently in a fully effective manner. In this paper we propose a novel temporal fusion method that leverages the effectiveness of a given set of features by efficiently combining multi-scale versions of them. This fusion is achieved using a boosting technique exploiting trees as weak classifiers, which has the advantage of performing an embedded feature selection. We apply our algorithm to two standard classification tasks, namely musical instrument recognition and multi-tag classification. Our experiments indicate that the multi-scale approach is able to select different features at different scales and significantly outperforms the mono-scale systems in terms of classification performance.",
        "zenodo_id": 1416904,
        "dblp_key": "conf/ismir/FoucardELR11",
        "keywords": [
            "short-term",
            "long-term",
            "audio signals",
            "complementary pieces",
            "time horizons",
            "exploit concurrently",
            "temporal fusion",
            "weak classifiers",
            "embedded feature selection",
            "classification performance"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nMULTI-SCALE TEMPORAL FUSION BY BOOSTING FOR MUSIC\nCLASSIFICATION\nRÂ´emi Foucard(1), Slim Essid(1), Mathieu Lagrange(2), GaÂ¨el Richard(1)\n(1)TELECOM ParisTech, CNRS-LTCI(2)Ircam, CNRS-STMS\n37, rue Dareau 1, place Igor Stravinsky\n75014 Paris, France 75004 Paris, France\nremi.foucard@telecom-paristech.fr\nABSTRACT\nShort-term and long-term descriptors constitute comple-\nmentary pieces of information in the analysis of audio sig-\nnals. However, because they are extracted over different\ntime horizons, it is difï¬cult to exploit them concurrently in\na fully effective manner. In this paper we propose a novel\ntemporal fusion method that leverages the effectiveness of\na given set of features by efï¬ciently combining multi-scale\nversions of them. This fusion is achieved using a boost-\ning technique exploiting trees as weak classiï¬ers, which has\nthe advantage of performing an embedded feature selection.\nWe apply our algorithm to two standard classiï¬cation tasks,\nnamely musical instrument recognition and multi-tag clas-\nsiï¬cation. Our experiments indicate that the multi-scale ap-\nproach is able to select different features at different scales\nand signiï¬cantly outperforms the mono-scale systems in\nterms of classiï¬cation performance.\n1. INTRODUCTION\nAutomatic classiï¬cation of audio signals is one of the main\nresearch areas in the ï¬eld of music information retrieval.\nThis task consists in assigning audio signals to one or more\ncategories (classes), according to a chosen criterion, which\ncan be the musical instrument played, the speaker gender,\nthe corresponding musical genre, etc. Classiï¬cation can\nbe very useful for many applications scenarios, such as\ndatabase annotation, stream segmentation, and smart orga-\nnization and search of large libraries.\nMost audio classiï¬cation systems represent the signal by\nsplitting it into ï¬xed-duration frames, from which several\nThis work was realized as part of the Quaero Programme, funded by\nOSEO, French State agency for innovation\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proï¬t or commercial advantage and that copies\nbear this notice and the full citation on the ï¬rst page.\nc/circlecopyrt2011 International Society for Music Information Retrieval.features are computed to be used by a learner. Given such\ntraining examples, the learner will then build a rule for deter-\nmining the relevant class of any previously unseen example,\nonly by considering its features. However, using frames of\nthe same length limits the duration of the observable phe-\nnomena. While describing signal characteristics at different\nscales has become frequent in image processing [15], few\naudio-related studies use several temporal horizons for de-\nscribing the signal.\nThe purpose of the present work is to setup a classiï¬-\ncation scheme that leverages the discrimination power of\nthe features considered, by extracting them at different time\nscales and using a boosting technique to combine them efï¬-\nciently. To precisely demonstrate the advantage brought by\nthe use of different scales, we keep the same representation\nat every scale ( i.e.compute the same features at different\nscales), but our system is ï¬‚exible enough to handle different\ntypes of descriptions through varying scales.\nIn the remainder of this paper, we ï¬rst brieï¬‚y review au-\ndio classiï¬cation algorithms and related temporal integra-\ntion techniques in Section 2. Then we describe our multi-\nscale classiï¬cation method (Section 3), and in Section 4, we\npresent our experiments and results.\n2. RELATED WORK\nAudio classiï¬cation makes use of machine learning to build\nrules for predicting the relevant class of a previously un-\nknown audio excerpt. A good overview of the music clas-\nsiï¬cation problems and most common techniques can be\nfound in [5].\nFirst, the signal is described by a set of features. Among\nthe most common, we can name: Fourier transform coefï¬-\ncients, mel-frequency cepstral coefï¬cients (MFCC), delta-\nMFCC, chromagrams or zero-crossing rates [17]. Most of\nthe time, several features are computed from a single frame,\nthen they are concatenated into one high-dimensional fea-\nture vector.\nIn order to map the obtained description to class labels,\n663Poster Session 6\nvarious classiï¬ers have been considered in previous works.\nThe two most used ones are probably Gaussian mixture\nmodels (GMM) [16] and Support vector machines (SVM)\n[11]. Alternatively, several recent works have made use of\nboosting, a meta-classiï¬er training several complementary\nversions of other learners [3, 4].\nMost systems choose to represent the signal using ï¬xed-\nlength frames. However, the concepts behind each class\nmay be conveyed by signal properties that have heteroge-\nneous temporal dynamics. Therefore, potentially useful de-\nscriptors may need to be built at various time scales. Hence,\na problem occurs when one tries to fuse such descriptions,\nbecause simple concatenation of the features (as done in\nmost works) is infeasible.\nEarly integration [9] can be used to solve this problem,\nsimply by integrating the features computed over shorter\nframes, over the duration of the longest analysis window.\nThis synchronization of all descriptors allows for their con-\ncatenation, but the temporal precision of the shorter-term\nfeatures is reduced. Therefore, potentially useful high-\nfrequency content lost due to the integration low-pass ï¬l-\ntering effect.\nIn [2], the authors fuse MFCC, along with chroma, web\ndocuments analysis and Last.fm tags1, by means of kernel\nfusion. The boosting algorithm can also be used for classi-\nï¬er fusion [18]. In [1], fusion by boosting is applied to audio\ndata, but all representations are done at the same scale: one\nvector per song. We can also cite [12], where the authors\ndiscriminate speech/nonspeech segments with features built\nusing a constant-Q ï¬lterbank. In this kind of transform, the\nï¬lters do not usually have the same temporal support. How-\never, once the feature vector is built, no information is kept\nabout the temporal support.\nFurthermore, studies pointed out that representing the\nsignal on different scales, and jointly considering all scales\nduring the whole learning process, may lead to a more com-\nplete analysis of the signal than using a single temporal hori-\nzon [14]. Indeed, short-term features can precisely capture\nshort events and quick changes in the signal. On the other\nhand, long-term features are able to represent larger phe-\nnomena, but with a poor temporal resolution. Using features\nbuilt over several scales should then allow for describing\njointly more diverse aspects of the signal.\n3. PROPOSED METHOD\nWe propose a novel boosting scheme to achieve multi-scale\ninformation fusion at a decision level. The boosting al-\ngorithm trains a weak classiï¬er several times, putting the\nemphasis on different examples among iterations. As men-\n1Last.fm is an online music listening service, where any user can asso-\nciate any tag to a song. These tags can be automatically retrieved through\nan API.tioned in Section 2, boosting has already been adapted for\nclassiï¬er fusion. This fusion can be achieved by simply con-\nsidering several weak classiï¬ers in parallel, and selecting,\nat each iteration, the best performing one. This constitutes\na convenient framework for heterogeneous classiï¬er fusion\nbecause it does not make any assumption on the nature of\nthe weak classiï¬ers. It considers only their decisions on the\ntraining examples.\n3.1 Multi-scale representation\nIn this work, we evaluate the merit of a multi-scale feature\nrepresentation compared to the classical mono-scale repre-\nsentation. In order to clearly identify the usefulness of the\nmulti-scale approach compared to the mono-scale one, the\nsame set of features is used at every scale. Further details\non the features used are given in Section 4.\nThe multi-scale feature representation is built as follows.\nFirst, the sequence of descriptors is computed at the ï¬ner\nscale, and then the other ones are obtained by temporal in-\ntegration, which allows for fast feature computation. We\nintegrate feature vectors by temporal averaging.\n3.2 Boosting trees\nFor every scale s, our weak learner Hsis a CART2classi-\nï¬cation tree [7] using Ls-sample length frames. Trees are\nconvenient, as they can be trained fast, and have proven ef-\nï¬cient when boosted [3]. Furthermore, they present the ad-\nvantage of performing feature selection during their train-\ning.\nDecision trees are built from a root containing all training\nexamples. At each node, the data is split in two (or possibly\nmore) children nodes, only using a threshold on a particular\nbin of the feature vector. The bin and threshold values are\nchosen so that the two children nodes are the â€œpurestâ€ pos-\nsible ( i.e.the probabilities of the two classes are the furthest\npossible from 0.5). Here, we use binary trees, with the Gini\nimpurity measure. The depth is ï¬xed in advance, and we\nseparately experiment depths 1 (which is also referred to as\na stump) and 2.\n3.3 Decision ranges\nAt each boosting iteration, the boosting algorithm chooses\nthe weak classiï¬er with the lowest weighted error rate. Mak-\ning a fair comparison between the classiï¬ers implies that the\ndecisions, for each of them, must be taken on the same au-\ndio segments. Because the frames of the different classiï¬ers\ndo not describe the same portions of signal, we have to set\nthe length on which the decisions are taken, for all scales.\nFor this purpose we introduce decision ranges . These\nranges represent the portions of signal on which the deci-\nsions of the weak classiï¬ers are taken. Figure 1 shows how\n2Classiï¬cation And Regression Tree.\n66412th International Society for Music Information Retrieval Conference (ISMIR 2011)\na decision range i, in gray, includes the feature frames from\nthe different scales. Each xn\ni,sis a description vector, where\nsis the temporal scale level, nis the index of the frame\nwithin the decision range, and irepresents the surrounding\ndecision frame. We consider a frame to belong to range i\nif its center is included in the temporal bounds of i. In the\nfollowing, we will denote by xithe set of all frames from\nall scales, that belong to range i.\nTimeFeature scale s\n...\n...\n...\n...Decision range i\nx1\ni,S\nx1\ni,1 xN1\ni,1...xn\ni,s\nFigure 1 . A decision range (in gray), covering a different\nnumber of frames on different scales.\nFigure 1 also shows that a decision range cannot be\nshorter than the frames at the largest scale. Otherwise,\nthe largest scale could be favored because it uses a greater\namount of signal. On the contrary, decision ranges longer\nthanLmaxwould decrease the number of training examples.\nThis is why each range spans exactly Lmaxsamples.\n3.4 Core algorithm\nThe whole learning procedure is detailed in Algorithm 1.\nWe start from the examples xn\ni,s, with class labels yi. The\nlabels neither depend on snorn, but only on the current\nsong which comprises segment i, as we are assuming class\nlabels always span the whole song duration. Thus, although\nï¬nal decisions may be taken at a song-level, they are ob-\ntained by combining intermediate decisions taken on seg-\nments of a song, referred to as decision ranges , based on a\ncorresponding set of feature-vector instances xn\ni,s.\nEach of these decision ranges gets an associated weight,\nrepresenting the relative focus of the algorithm during the\ncurrent iteration. In the beginning, all weights are equal for\nranges belonging to the same class.\nAt each iteration r, the weights wr,iare normalized so\nthey sum to 1, before the weak classiï¬ers hr,s(the CART\ntrees) are trained. These trainings must take into account\nthe weights of the examples. For each scale, the decision on\nrangeiis a majority vote on all frames belonging to i. Us-\ning these decisions, we can compute an error rate for every\nscale. The scale Ë†srwith the lowest error rate is then selected\nfor the ï¬nal strong decision, with weight Î±r. After that, theAlgorithm 1 Adaboost for multi-scale classiï¬er fusion.\nInput: Annotated examples from all scales (xn\ni,s,yi),\n1â‰¤iâ‰¤I,1â‰¤sâ‰¤S, 1â‰¤nâ‰¤Ns\nInput: Weak learnersHs\nw1,iâ†âˆ’1\n2m,1\n2l, resp. foryi= 0,1, wheremandl\nare the number of negative and positive examples, respec-\ntively\nforr= 1,...,R do\nwr,iâ†âˆ’wr,iPI\nj=1wr,j// Normalize the weights\nTrain classiï¬ers hr,swith the modelsHsand weights\nwr,i\n// Decisions of hr,son the observation ranges i\ndr,s,i=/braceleftbigg\n1 if1\nNs/summationtextNs\nn=1hr,s(xn\ni,s)>0.5\n0 otherwise,\n// Compute weighted error rate\n/epsilon1r,sâ†âˆ’/summationtext\niwr,i|dr,s,iâˆ’yi|\n// Best scale\nË†srâ†âˆ’argmins/epsilon1r,s\n/epsilon1râ†âˆ’/epsilon1r,Ë†sr\nhrâ†âˆ’/summationtext\nnhr,Ë†sr\n// Coefï¬cient associated with hr\nÎ±râ†âˆ’log1\nÎ²r, whereÎ²r=/epsilon1r\n1âˆ’/epsilon1r,\n// Update the example weights\nfor all rangesido\n// test whether dr,Ë†sr,i=yi\nifxiwell classiï¬ed then\nwr+1,iâ†âˆ’wr,iÎ²r\nelse\nwr+1,iâ†âˆ’wr,i\nend if\nend for\nend for\nOutput:H(x) =/summationtext\nrÎ±rhr(x)\n665Poster Session 6\nweights of the correctly classiï¬ed examples are decreased,\nthus reducing their importance for future iterations.\nThe ï¬nal output H(x)is used during the testing phase as\nfollows. When tagging a range i, one decision is taken for\neach component rby applying hrto the observations from\ncorresponding scale ( xn\ni,Ë†sr). Then,H(xi)is a weighted sum\nof thehr(xi), as stated at the end of Algorithm 1. Finally,\nthe global decision for a whole song ais a standard late\nintegration over all decision ranges within a. It is done by\ntaking the thresholded mean of the H(xi):\nDa=/braceleftbigg1 if mean iâˆˆaH(xi)>t\n0 otherwise(1)\n4. EXPERIMENTS\nTo show the usefulness of our multi-scale system compared\nto mono-scale systems, we perform experiments on two\ndatasets, corresponding to two usual tasks is audio classi-\nï¬cation. We ï¬rst validate our method on a musical instru-\nment recognition database. Then, we test our system perfor-\nmance for multi-tag classiï¬cation on the now well-known\nCAL500 [16]. The two experiments are done with different\nsets of features and different scale choices.\n4.1 Musical instrument recognition\nThe task of instrument recognition presents the advantages\nof being well deï¬ned and strongly related to the audio con-\ntent. This is why we run the ï¬rst experiment on a database\ncontaining a set of solo real-music performances, featuring\nsix instruments: Piano, Guitar, Bassoon, Oboe, Cello and\nViolin. The database contains 73 ï¬les (31 for training, 42\nfor testing), totalling 449 minutes of music. For each instru-\nment, we have between 28 and 39 minutes of performance\nin the training set, and between 22 and 64 minutes in the test\nset.\nFrom this data, we extract a selection of 30 feature coefï¬-\ncients obtained by applying Inertia Ratio Maximisation [13]\nto an initial set of cepstral, spectral, perceptual and temporal\nfeatures used in a previous work [10].\nWe extract these descriptors at four distinct scales. The\nshortest one ( S1) has an analysis window of L1= 320 ms,\nwhich is approximately the duration of an eighth note at\n90 BPM. The other scales ( S2,S3andS4) have windows\nof lengths 2L1,4L1and8L1. The frames do not overlap.\nOn this data, we trained our systems with 500 boosting\niterations, using trees of depth 1.\nEach example is annotated with one of the six instru-\nments. We decompose this multiclass problem into six\ndistinct bi-class problems, following the one-versus-all ap-\nproach. During the test phase, all decisions are integrated\nto the largest scale 8L1= 2.6s, and the most probable in-\nstrument is chosen. For the mono-scale systems with scalesshorter than 8L1, the late integration is done by summing\nthe classiï¬er output on the frames within the considered de-\ncision range.\nWith these predictions on the test set, we calculate the\nrecognition rate as:\nR= meani 1H(xi)=yi (2)\n4.2 Multi-tag classiï¬cation using CAL500\nFor this experiment, we use the CAL500 database [16],\na database containing 500 pop songs, annotated by non-\nexperts through a survey. Each song has been annotated by\nat least three people. We keep the 61 tags used in [2]. These\ntags describe different properties of the whole songs, such\nas: mood, genre, instrument, etc.\nTests are conducted using 10-fold cross-validation, with\n450 songs used for training, and 50 songs for testing. The\ntest sets are not overlapping between the different folds. For\ncomplexity reduction, we only use 30s of each song: ex-\ntracted between instants 30 s and 60 s.\nThe features we use for describing each frame of signal\nare: the 15 psychoacoustic-related features recommended\nin [19] (loudness, tonal dissonance, . . . ), completed by the\ncommon ï¬rst 13 MFCC (dropping the energy), chroma,\nzero-crossing rate, and spectral spread, skewness and kur-\ntosis.\nWe have chosen ï¬ve different scales: frames covering 2,\n3.3, 5.5, 9 and 15 s of signal, with 50% overlap. A prelim-\ninary experiment indicated that, for this kind of data, scales\nunder 2 s were less useful. And we also considered that\n15 s was long enough to capture a wide range of long-term\nphenomena. The other scales are chosen to have a constant\nlogarithmic spacing between each consecutive values.\nWe examine the performance on the test set, with 100\nboosting iterations, using the same two evaluation measures\nas in [2]. These ranking metrics measure the ability of a soft\nprediction system to output higher scores for relevant doc-\numents compared to irrelevant ones. Soft predictions are\nnon-binary scores, representing the amount of conï¬dence\nthe predictor has in the positive association of a considered\ntag to a given song. We can obtain soft outputs from our\nsystem, simply by averaging instead of thresholding the ï¬-\nnal decision:\nËœDa= meaniâˆˆaH(xi) (3)\nThis framework will make performance evaluation indepen-\ndent from the detection threshold t, that we choose for Equa-\ntion 1.\nFrom these decisions, we compute the Mean Average\nPrecision (MAP) and Area under the ROC3curve (AUC).\nFor a precise description of their calculation, see [8].\n3Receiver Operating Characteristic.\n66612th International Society for Music Information Retrieval Conference (ISMIR 2011)\nScale Recognition rate (in %)\nS1 59.8\nS2 53.0\nS3 62.9\nS4 44.2\nMulti-Scale 64.5\nTable 1 . Performance of the different systems on the instru-\nment recognition database.\nScale Tree depth MAP AUC\nScale 11 0.432 0.641\n2 0.449 0.653\nScale 21 0.442 0.652\n2 0.454 0.660\nScale 31 0.448 0.658\n2 0.451 0.662\nScale 41 0.456 0.667\n2 0.458 0.667\nScale 51 0.457 0.664\n2 0.451 0.661\nMulti-Scale1 0.466 0.671\n2 0.458 0.665\nTable 2 . Performance of the different systems on CAL500.\n4.3 Results and discussion\nThe recognition rates yielded by the different systems on the\ninstrument database are presented in Table 1. It is found that\nthe multi-scale system has the best recognition rate. The\ndifference between multi-scale and scale 3 systems is sig-\nniï¬cant, according to a McNemar test [6], which yielded a\np-value of 0.003. This means that the difference is statisti-\ncally signiï¬cant with a 99.7% conï¬dence level.\nThe features selected by the trees along the boosting iter-\nations differ greatly from one instrument to another, but the\nmost selected scales are the shortest and the longest ones\n(S1andS4). Surprisingly, these two scales do not corre-\nspond to the best performing mono-scale systems. This may\nbe due to the fact that S1gives the most temporally precise\ndescription, while S4is good at taking decisions on a 2.6 s\ndecision range, since it has the same length. Most of all,\nthis indicates that the information brought by the whole set\nof scales is structurally different from just one scale.\nA closer look at the detailed results, on a per-instrument\nbasis, also revealed that the multiscale system is not the best\nperforming one for all instruments. However, its perfor-\nmance is less variable among instruments. This shows that\nthe multi-scale approach performs best, as it is more ï¬‚exi-\nble, and can focus on the most appropriate representation.\nThe results for the multi-tag task on CAL500 are pre-\nsented in Table 2. The best MAP and AUC are given bythe multi-scale system using trees of depth 1. The statisti-\ncal signiï¬cance of the difference between this system and\nthe best performing mono-scale one has been veriï¬ed by a\ncross-validated paired ttest [6]. This test indicated a signif-\nicance of more than 99%.\nDepth 1 trees yield better results for the multi-scale sys-\ntems, but the choice of depth seems to have variable effects\namong mono-scale systems.\nFor comparison, in [2], the authors obtain a MAP and\nAUC of 0.54 and 0.73, respectively, on the same data and\ntags. But their system uses content-based and context-based\ninformation, whereas the one presented in this paper only\nrelies on the audio content. However, the focus of this study\nis intentionally set on the methodological validation of the\nalgorithm proposed, rather than achieving the best possi-\nble performance. Though, it shall be noticed that the abil-\nity of our new algorithm to handle data drawn on different\nscales makes it applicable to descriptors of different seman-\ntic levels, especially semantic information that may be valid\nat a smaller scale than the entire song (type of instrument,\ntempo, etc.). This very kind of data fusion will be explored\nin future works.\n5. CONCLUSION\nWe proposed a new multi-scale fusion system for classiï¬-\ncation that is designed to be convenient for fusing hetero-\ngeneous features, both in terms of content description and\nscale. Fusion is done thanks to an adapted boosting algo-\nrithm using decision trees.\nIn this study, we focused on validating the ability of the\nproposed system to conveniently fuse features expressed at\ndifferent scales. We experimented two classiï¬cation tasks\nand the results show that the multi-scale system is the best\none. Future work will study the ability of the system to fuse\nfeatures that are describing different aspects of the musical\npieces of interest, both in terms of content and scale.\n6. REFERENCES\n[1] L. Barrington, D. Turnbull, M. Yazdani, and G. Lanck-\nriet. Combining audio content and social context for se-\nmantic music discovery. In SIGIR , pages 387â€“394, New\nYork, NY , USA, 2009. ACM.\n[2] L. Barrington, M. Yazdani, D. Turnbull, and G. Lanck-\nriet. Combining feature kernels for semantic music re-\ntrieval. In ISMIR , pages 614â€“619, 2008.\n[3] J. Bergstra and B. K Â´egl. Meta-features and adaboost\nfor music classiï¬cation. In Machine Learning Journal ,\n2006.\n[4] T. Bertin-Mahieux, D. Eck, F. Maillet, and P. Lamere.\nAutotagger: a model for predicting social tags from\n667Poster Session 6\nacoustic features on large music databases. Journal of\nNew Music Research , 37(2):115â€“135, June 2008.\n[5] T. Bertin-Mahieux, D. Eck, and M.I. Mandel. Automatic\ntagging of audio: The state-of-the-art. In Wenwu Wang,\neditor, Machine Audition: Principles, Algorithms and\nSystems . IGI Publishing, 2010.\n[6] T.G. Dietterich. Approximate Statistical Tests for Com-\nparing Supervised Classiï¬cation Learning Algorithms.\nNeural Computation , 10(7), 1998.\n[7] T. Hastie, R. Tibshirani, and J. Friedman. The Ele-\nments of Statistical Learning . Springer Series in Statis-\ntics. Springer New York Inc., New York, NY , USA, 3\nedition, 2009.\n[8] J.L. Herlocker, J.A. Konstan, L.G. Terveen, and J.T.\nRiedl. Evaluating collaborative ï¬ltering recommender\nsystems. ACM Trans. Inf. Syst. , 22:5â€“53, January 2004.\n[9] C. Joder, S. Essid, and G. Richard. Temporal Integration\nfor Audio Classiï¬cation With Application to Musical In-\nstrument Classiï¬cation. TASLP , 17(1):174â€“186, 2009.\n[10] M. Lardeur. Robustesse des syst `emes de classiï¬cation\nautomatique des signaux audio-fr Â´equences aux effets\nsonores . Master thesis, Universit Â´e Pierre et Marie Curie,\n2008.\n[11] M.I. Mandel and D.P.W. Ellis. Multiple-instance learn-\ning for music information retrieval. In ISMIR , 2008.\n[12] N. Mesgarani, M. Slaney, and S.A. Shamma. Discrim-\nination of speech from nonspeech based on multiscale\nspectro-temporal Modulations. TASLP , 14(3):920â€“930,\nMay 2006.\n[13] G. Peeters and X. Rodet. Hierarchical Gaussian Tree\nwith Inertia Ratio Maximization for the Classiï¬cation of\nLarge Musical Instruments Databases. In Int. Conf. On\nDigital Audio Effects , 2003.\n[14] B.L. Sturm, M. Morvidone, and L. Daudet. Musical in-\nstrument identiï¬cation using multiscale mel-frequency\ncepstral coefï¬cients. In EUSIPCO , 2010.\n[15] B.M. Taar Romeny. Front-end vision and multi-scale\nimage analysis . Springer, 1st edition, 2003.\n[16] D. Turnbull, L. Barrington, D. Torres, and G. Lanckriet.\nSemantic annotation and retrieval of music and sound\neffects. TASLP , 16(2):467â€“476, February 2008.\n[17] G. Tzanetakis and P.R. Cook. Musical genre classiï¬ca-\ntion of audio signals. Transactions on Speech and Audio\nProcessing , 10(5):293â€“302, 2002.[18] P. Viola and M. Jones. Rapid object detection using a\nboosted cascade of simple features. In CVPR , volume 1,\npages 511â€“518, 2001.\n[19] Y .H. Yang, Y .C. Lin, Y .F. Su, and H.H. Chen. A regres-\nsion approach to music emotion recognition. TASLP ,\n16(2):448â€“457, 2008.\n668"
    },
    {
        "title": "Quantifying the Relevance of Locally Extracted Information for Musical Instrument Recognition from Entire Pieces of Music.",
        "author": [
            "Ferdinand Fuhrmann",
            "Perfecto Herrera"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1416350",
        "url": "https://doi.org/10.5281/zenodo.1416350",
        "ee": "https://zenodo.org/records/1416350/files/FuhrmannH11.pdf",
        "abstract": "In this work we study the problem of automatic musical instrument recognition from entire pieces of music. In particular, we present and evaluate 4 different methods to select, from an unknown piece of music, relevant excerpts in terms of instrumentation, on top of which instrument recognition techniques are applied to infer the labels. Since the desired information is assumed to be redundant (we may extract just a few labels from a thousands of audio frames) we examine the recognition performance, the amount of data used for processing, and their possible correlation. Experimental results on a collection of Western music pieces reveal state-ofthe-art performance in instrument recognition together with a great reduction of the required input data. However, we also observe a performance ceiling with the currently applied instrument recognition method.",
        "zenodo_id": 1416350,
        "dblp_key": "conf/ismir/FuhrmannH11",
        "keywords": [
            "Automatic",
            "Instrumental",
            "Recognition",
            "Extraction",
            "Redundant",
            "Information",
            "Audio",
            "Frames",
            "Performance",
            "Data"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nQUANTIFYING THE RELEV ANCE OF LOCALLY EXTRACTED\nINFORMATION FOR MUSICAL INSTRUMENT RECOGNITION FROM\nENTIRE PIECES OF MUSIC\nFerdinand Fuhrmann and Perfecto Herrera\nMusic Technology Group\nUniversitat Pompeu Fabra\nBarcelona, Spain\n{ferdinand.fuhrmann,perfecto.herrera }@upf.edu\nABSTRACT\nIn this work we study the problem of automatic musical in-\nstrument recognition from entire pieces of music. In partic -\nular, we present and evaluate 4 different methods to select,\nfrom an unknown piece of music, relevant excerpts in terms\nof instrumentation, on top of which instrument recognition\ntechniques are applied to infer the labels. Since the desire d\ninformation is assumed to be redundant (we may extract just\na few labels from a thousands of audio frames) we examine\nthe recognition performance, the amount of data used for\nprocessing, and their possible correlation. Experimental re-\nsults on a collection of Western music pieces reveal state-o f-\nthe-art performance in instrument recognition together wi th\na great reduction of the required input data. However, we\nalso observe a performance ceiling with the currently ap-\nplied instrument recognition method.\n1. INTRODUCTION\nContent-based Music Information Retrieval (MIR) aims at\nautomatically extracting higher-level concepts from musi c\ndata in order to enhance methods for an intelligent and user-\nfriendly management of music collections. Here, informa-\ntion about the instrumentation plays a fundamental role in\nthe semantic description of a music piece. Given the sizes\nof nowadays music archives, typical MIR applications such\nas indexing or retrieval demand for algorithms with low or\nmoderate computational load. However, related literature in\nthe ï¬eld of automatic musical instrument recognition from\npolyphonies mostly concentrated on developing discrimi-\nnation strategies, while disregarding aspects related to t he\nPermission to make digital or hard copies of all or part of this w ork for\npersonal or classroom use is granted without fee provided th at copies are\nnot made or distributed for proï¬t or commercial advantage and th at copies\nbear this notice and the full citation on the ï¬rst page.\nc/circlecopyrt2011 International Society for Music Information Retrieval .computational complexity of the algorithms. Therefore,\nmany approaches towards musical instrument recognition\nare costly and were designed for simpliï¬ed test scenarios\n(e.g. [7, 8]). Furthermore, global properties of the music r e-\nlated to the instrumentation, which can help to reduce the\namount of data to analyse and improve recognition robust-\nness, were either only partially used or completely neglect ed\n(e.g. [1,9,10]). Moreover, most of the works incorporate re -\nstrictions such as reduced number of instruments, aseptic o r\nlimited data, and/or other a priori assumptions (e.g. [3, 6] ).\nIn general, the auditory scene produced by a musical com-\nposition can be regarded as a multiple source environment,\nwhere the different sound sources â€“ the musical instruments\nâ€“ are temporarily active, while often recurring along the\npiece. We therefore expect that the instrumentationâ€™s tem-\nporal evolution of a given music piece shows a repetitive\ncharacter, so that the information related to the individua l\nsources becomes redundant (we may extract a few labels\nfrom a thousands of audio frames). This suggests that, for\nautomatic recognition systems, analysing only a fraction o f\nthe data is enough to extract the available information.\nThereby the overall computational load of such algorithms\nis reduced which enables the implementation of fast recog-\nnition systems, indispensable for analysing big music col-\nlections. Moreover, this so-obtained data reduction can fu r-\nther be exploited by any other MIR related algorithm, e.g.\nmusic visualisation or summarisation.\nIn the present work we study the effect of data reduc-\ntion on instrument recognition performance from entire mu-\nsic pieces for real world applications, e.g. music collecti on\nindexing. We thereby address two of the above-identiï¬ed\naspects lacking in the related literature, namely the devel -\nopment of both robust and efï¬cient methods for automatic\ninstrument recognition. In particular, we introduce and co m-\npare several track-level approaches, i.e. aimed to roughly\nassign labels to a whole track, which pre-process a given\nmusic piece to output a set of segments. Labels are then in-\nferred from these segments using our previously presented\n239Poster Session 2\npitched \nlabelling \ndrumset \ndetection 11 \nraw audio0\nfilter /theta.ipa1multiclass \nSVM \nbinary\nSVM piano drums \nvoice sax \nfilter /theta.ipa2filter /theta.ipa3framing & \nfeature\nextraction \nFigure 1 . Graphical illustration of the label inference.\nrecognition method [5]. We further show that by applying\nthis methodology we can signiï¬cantly reduce the amount of\ndata needed for analysis, while maintaining high recogni-\ntion performance. In doing so we explore the redundancy of\nthe information along a music track, and study the inï¬‚uence\nof locally obtained data on recognition, i.e. how much data\nneeds to be extracted from which part of the track to obtain\na sufï¬cient description of its instrumentation.\nSince our focus lies on developing approaches for real\nworld applications, e.g. music collection indexing, we do\nnot impose any restrictions on the input data, hence evaluat -\ning our approaches only on music pieces taken from real\nrecordings. Furthermore, all information used in the la-\nbelling process is directly taken from the mixture signal wi th-\nout applying a priori information.\nBelow, we ï¬rst present the basic methodology to extract\ninstrumental labels from an unknown musical excerpt\nof arbitrary length (Sec. 2). We then give details about the\ndifferent approaches to process entire pieces of music\n(Sec. 3), which is followed by a description of the data used\nin the experiments (Sec. 4). In Sec. 5 we deï¬ne the evalua-\ntion metrics and present the obtained results. After a discu s-\nsion, Sec. 6 concludes this article.\n2. LABEL INFERENCE\nHere we describe the basic process of extracting instrumen-\ntal labels given an unknown audio excerpt of arbitrary lengt h.\nFirst, the method sequentially applies previously trained pre-\ndominant instrument classiï¬ers to the audio. The resulting\nframe series is then analysed to extract the labels (Fig. 1).\n2.1 Classiï¬cation\nTo extract information about musical instruments from a\nshort section of the audio signal we applied parts of the\nwork previously presented in [4]. That is, our method uses\nstatistical models of predominant musical instruments to e s-\ntimate the presence of both pitched and percussive instru-\nments for a 3-seconds excerpt of a polyphonic mixture sig-\nnal. In particular, we applied the support vector machine\n(SVM) model1for 11 pitched instruments ( Cello ,Clarinet ,\n1We used the libSVM implementation, available at http://www.\ncsie.ntu.edu.tw/ Ëœcjlin/libsvm/ .Flute ,acoustic andelectric Guitar ,Hammond Organ ,Pi-\nano,Saxophone ,Trumpet ,Violin , and singing Voice ) as de-\nveloped in [4] (â€œmulticlass SVMâ€ in Fig. 1), and a sepa-\nrate model for estimating the presence of the drumkit (â€œbi-\nnary SVMâ€ in Fig. 1). Both SVMs output probabilistic es-\ntimates, i.e. a real value between 0 and 1, for each of the\ntarget classes. The models were trained with automatically\npre-selected low-level audio features, describing the spe ctral\nand pitch related properties of the signal2, extracted from\nproper training data. In particular, the features were com-\nputed frame-wise in the applied 3 second window, using a\nframe size of 46 ms with 50% overlap, and integrated over\ntime via mean and variance statistics of the instantaneous\nand ï¬rst difference values.\nThe training data itself consisted of 3 second excerpts\ncontaining predominant pitched target instruments, taken\nfrom more than 2,000 â€“ presumably polytimbral â€“ music\nrecordings [4]. Besides for training the pitched instrumen ts\nmodel, this collection was also annotated according to the\npresence of the drumkit, i.e. labels drums andno-drums , and\nused for constructing the percussive classiï¬er.\n2.2 Labelling\nTo extract labels of an audio signal of arbitrary length, the\nmethod ï¬rst sequentially applies the above-described clas -\nsiï¬ers, using a hop size of 0.5 sec. The temporal behaviour\nof the obtained probabilistic time series is then exploited\nfor label inference. Since the output of the pitched and the\npercussive model is merged (Fig. 1), we developed separate\napproaches corresponding to each of the two models for ex-\ntracting the desired labels.\n2.2.1 Percussive Instruments\nFirst, a decision boundary of 0.5 is applied to binarize each\nprediction of the classiï¬er. Then, a majority vote among all\nso-obtained binary decisions of the analysed signal is per-\nformed to indicate the target label. The corresponding con-\nï¬dence value is set to the relative amount of positive binary\ndecisions.\n2.2.2 Pitched Instruments\nThe method ï¬rst uses the mean values of each instrumentâ€™s\nprobabilistic curve along the analysed audio to determine\nthose instruments for label analysis. Thereby a threshold Î¸1\nis applied to these mean values; if all of them fall below\nthe threshold, the whole audio under analysis is skipped and\nnot labelled at all, indicating a potential confusion due to\nunknown or heavily overlapped instruments. If approved,\na second threshold Î¸2is applied to the mean values; if an\n2A complete list of all applied audio features can be accessed\nunderhttp://mtg.upf.edu/system/files/publications/\nismir11_ffuhrmann_sup.pdf .\n24012th International Society for Music Information Retrieval Conference (ISMIR 2011)\ninstrument falls below this threshold, it is regarded as ina c-\ntive and not used in the further analysis. The probabilistic\ncurves of the remaining instruments are then searched for\nsections, where a single instrument predominates the mix-\nture, i.e. it holds the highest probability value among all i n-\nstruments for a certain minimal amount of time. If such a\nsection is found, the corresponding instrument is added to\nthe list of labels for the analysed audio, along with a conï¬-\ndence value as deï¬ned by the sectionâ€™s length relative to the\noverall length of the audio3. This process is repeated for all\ndetermined active instrument. Finally, a label threshold Î¸3\nis applied to discard unreliable tags. Fig. 2 exempliï¬es the\nlabelling process for a 30 second excerpt.\n  \na b c d\nFrames Probability \nÎ¸1\nÎ¸2\nÎ¼ A \n B \n C \n D \n E \nFigure 2 . An example of the labelling method for pitched\ninstruments. The main ï¬gure shows the probabilistic es-\ntimates for sources A-E, the right panel the mean values\ntogether with the thresholds used for instrument ï¬ltering.\nSince E is discarded as its mean value falls below Î¸2, the\ncurves A-D are scanned for sections, where a single instru-\nments predominates. Depending on the parameter for the\nminimal length of these sections, up to three different in-\nstruments can be detected here (a,c,d â†’A,C,B), whereas\nsections containing instrument confusions are not used for\nlabelling (b).\n3. TRACK-LEVEL APPROACHES\nIn this section we present 4 different approaches to process\nand label an entire piece of music. Since the instrumenta-\ntion and its temporal evolution of a piece of music usually\nfollows a clear structural scheme, we expect, inside a given\nmusic track, a certain degree of repetitiveness of its diffe rent\ninstrumentations. This property of music and the resulting\nredundancy is exploited by the described approaches to re-\n3For multiple occurrences of the same instrument the respective conï¬-\ndence values are summed.C3 C2 C1 ALL \n30SEC \nNSEG\nCLU \nFigure 3 . Illustration of the presented track-level ap-\nproaches; the green ï¬lled frames denote the respective data\nused for labelling. Segmentation (red) and clustering (blu e)\nare indicated for the CLU method, while NSEG applies a\nvalue of n= 5. See text for details.\nduce the amount of data to process. We then apply the label\ninference method described in Sec. 2 on their respective out -\nput and evaluate the algorithms in terms of labelling perfor -\nmance and the amount of used data. In short, the presented\napproaches are accounting â€“ some of them more than others\nâ€“ for the time-varying character of instrumentation inside a\nmusic piece. Their output consist of a set of segments which\nare then used to infer the instrumental labels for the given\nmusic track. Fig. 3 depicts the underlying ideas.\n3.1 All-frame processing ( ALL )\nProbably the most straightforward approach given the above -\ndescribed labelling methodology. By processing all frames\nwe automatically account for the time-varying character of\nmusical instruments via a global analysis of the track. How-\never, no data reduction is performed. Since this approach\nuses all data available, it acts as a kind of upper baseline bo th\nin terms of recognition performance and amount of data pro-\ncessed, which all other methods using less data compete\nwith.\n3.2 30 seconds ( 30SEC )\nThis widely used approach in MIR assumes that by reduc-\ning the data to 30 sec of audio most of the semantic infor-\nmation is maintained. Many genre, mood, or artist classiï¬-\ncation systems use an excerpt of this length to represent an\nentire music track (e.g. [11]). The process can be regarded\nas an extrapolation of the locally obtained information to\nthe global scope, i.e. the entire piece of music. Since the\naforementioned concepts are rather stable across one sin-\ngle piece, the data reduction does not affect the signiï¬canc e\nof the obtained results. However, instrumentations usuall y\nchange with time, so that the targeted information is inade-\nquately represented by this data amount. In our experiments\nwe extracted the data from 0 to 30 sec of the track.\n241Poster Session 2\n3.3 Segment sampling ( NSEG )\nHere, we obtain excerpts by uniformly sampling the track\nto incorporate the time-varying characteristics of instru men-\ntation. This enables a local extraction of the information\nwhich is combined to a global estimate of the instrumental\nlabels. In particular we extract nequal-distant excerpts of\n10 seconds length from the track (for nequals 1 or 2 a sin-\ngle segment from the beginning, or one segment from the\nbeginning and the end of the music track is taken, respec-\ntively). The labels inferred from each of the segments are\nthen merged, where small values of nlead to a great data\nreduction while still considering the instrumentationâ€™s t ime-\nvarying character. The parameter nis kept variable for the\nexperiments conducted in Sec. 5.\n3.4 Cluster representation ( CLU )\nCertainly the most elaborated approach from the perceptual\npoint-of-view; a given piece of music is represented with a\ncluster structure where each cluster corresponds to a diffe r-\nent instrumentation. This approach explicitly uses an esti -\nmate of the global distribution of the musical instruments\nto locally infer the labels from a reduced set of the data by\nexploiting redundancies of the instrumentations inside th e\npiece of music. In particular, it applies unsupervised seg-\nmentation and clustering algorithms to locate the differen t\ninstrumentations and their repetitions. At the end, only on e\nsegment per cluster is taken for further analysis. Hence thi s\napproach is directly exploiting repetitions in the instrum en-\ntation to reduce the amount of data to process, while the\nlocal continuity of the individual instruments is preserve d to\nguarantee a maximum in recognition performance.\n3.4.1 Segmentation\nSince instrumentation is closely related to timbre, a timbr al\nrepresentation of the track is processed to ï¬nd local change s\ntherein, applying an unsupervised segmentation algorithm\nbased on the Bayesian Information Criterion (BIC) [2]. To\nrepresent timbre the approach uses 13 frame-wise extracted\nMel Frequency Cepstral Coefï¬cients (MFCCs).\n3.4.2 Clustering\nHere, an agglomerative clustering step builds a hierarchic al\ntree (i.e. a so-called dendrogram) on the pair-wise similar -\nities of all generated segments. The segments are merged\niteratively to form the tree, where a linkage method fur-\nther measures proximities between groups of segments at\nhigher levels [12]. The ï¬nal clusters are then found by cut-\nting the tree according to an inconsistency coefï¬cient, whi ch\nmeasures the compactness of each link in the tree. Fur-\nthermore, to estimate the pair-wise segment similarities, we\nmodel each segment as a single Gaussian distribution of theraw MFCC frames with diagonal covariance matrix and cal-\nculate the symmetric Kullback-Leibler divergence (KL) be-\ntween pairs of segments.\nFinally, the longest segment of each resulting cluster is\npassed to the label inference algorithm. The predictions\nfrom all segments are then merged to form the set of labels\nfor the track under analysis.\n4. DATA\nFor our experiments we used a data corpus consisting of 220\nmusic pieces taken from various genres of Western music.\nIn these tracks, all perceptually audible instruments were an-\nnotated manually along with their start and end times. Since\nno limitations in the vocabulary size were imposed to the\nhuman annotators, this evaluation data includes, addition al\nto the 12 modelled classes, instruments which are not mod-\nelled by the classiï¬er. Moreover, if the annotator could not\nrecognize a certain instrumentâ€™s sound, the label unknown\nwas used4.\nAn analysis of the set of labels used in the annotations\nrevealed 28 different instrumental categories, at which th e\nlabel unknown was the third-most frequently used, directly\nafter the labels bass anddrums . It should be noted that none\nof the tracks used for training the instrumental models was\nused in this evaluation collection.\n5. GENERAL RESULTS\n5.1 Metrics\nTo estimate the labelling performance we regarded the prob-\nlem as multi-class, multi-label classiï¬cation. That is, ea ch\ninstance to evaluate can hold an arbitrary number of unique\nlabels of a given dictionary. Given a collection of music\ntracks X={xi},i= 1... N , with Nitems, we deï¬ne,\nrespectively, Ë†Y={Ë†yi},i= 1... N , and ËœY={Ëœyi},i=\n1... N , the set of ground truth and predicted labels for each\nxi. Together with the label dictionary L={li},i= 1... M ,\nwe deï¬ne the weighted precision and recall metrics,\nP=1/summationtext\nl,iËœyl,i/summationdisplay\nl,iËœyl,iÂ·Ë†yl,i,R=1/summationtext\nl,iË†yl,i/summationdisplay\nl,iËœyl,iÂ·Ë†yl,i,\n(1)\nwhere Ë†yl,i(Ëœyl,i) represents a boolean variable indicating\nthe presence or absence of the label lin the annotation (gen-\nerated instrumental tags) of track i. Additionally, we deï¬ne\nan F-measure to estimate the overall labelling performance ,\n4A complete list of all tracks contained in the evaluation data set,\nalong with the annotated instruments and genre labels, can be accessed\nviahttp://mtg.upf.edu/system/files/publications/\nismir11_ffuhrmann_sup.pdf .\n24212th International Society for Music Information Retrieval Conference (ISMIR 2011)\nF=2/summationtext\nl,iËœyl,iÂ·Ë†yl,i/summationtext\nl,iËœyl,i+/summationtext\nl,iË†yl,i. (2)\n5.2 Results\nIn order to provide a robust estimate of the methodsâ€™ per-\nformance with respect to the parameters to evaluate, we per-\nformed a 3-fold Cross Validation (CV). For each turn we\nused the data of 2 folds for estimating the optimal parame-\nter settings and subsequently tested on the remaining fold.\nWe then obtained mean values and corresponding standard\ndeviations by averaging the evaluation results of the respe c-\ntive predictions of all three runs5.\nThe upper panel of Table 1 contains the results (mean\nvalues) of the CV obtained for the studied algorithms. The\nparameter nof the NSEG method was set to 3 and 6, gen-\nerating systems processing 30 sec ( 3SEG â€“ an equivalent\nin terms of data size to the 30SEC method) and 1 min of\naudio data ( 6SEG ). Additionally, ï¬gures regarding the rela-\ntive amount of data used for label inference are shown in the\nlower panel (relative with respect to the all-frame process ing\nalgorithm ALL). A lower bound was generated by drawing\neach label from its respective prior binomial distribution , in-\nferred from all tracks of the collection, averaging the resu lt-\ning performance over 100 independent runs ( PRIOR ).\nTable 1 .Precision, recall, and F measures of the studied\napproaches together with the relative amount of data used\nfor label inference (data). The asterisk indicates average\nvalues over 100 independent runs.\nPRIOR *30SEC 3SEG 6SEG CLU ALL\nP 0.4 0.62 0.64 0.60 0.64 0.66\nR 0.4 0.5 0.6 0.71 0.74 0.73\nF 0.4 0.55 0.62 0.65 0.69 0.69\ndata â€“ 0.11 0.11 0.25 0.66 1\nThe ï¬gures presented in Table 1 show that all considered\napproaches are outperforming the prior baseline PRIOR , op-\nerating well above a knowledge-informed chance level.\nMoreover, two clear dependencies of the resulting perfor-\nmance can be observed; ï¬rst, a correlation with the absolute\namount of data processed (e.g. 3SEG â†’6SEG â†’ALL), and\nsecond, a dependency on the location where the information\nis extracted ( 30SEC â†’3SEG ).\nComparing the sampling methods with the timbre analy-\nsis of CLU we can see that the knowledge introduced by the\nlatter positively affects the recognition performance. Be -\nsides the greater values of RandF, the precision Pis re-\n5Parameter estimation itself was performed via a grid search pro cedure\nover the relevant parameter space. For each of the studied app roaches de-\nscribed in Sec. 3 the parameters were evaluated separately to guarantee\nmaximal comparativeness of the respective results.markable here, which holds the same value as for the 3SEG\nmethod, although CLU processes 55 percent points more\ndata. The segmentation and clustering preserves the tem-\nporal continuity of the instrumentation, therefore exhibi ting\nless data variability, ensuring the high value of the Pmetric.\nThe same local continuity of musical instruments otherwise\nenforces the lower recall value in the 30SEC approach, in\ncomparison to the 3SEG method. However, with more anal-\nysed segments from different parts of the track, the variati on\nin the data increases. This affects the recall value R, result-\ning in a trade-off between the two aforementioned metrics.\nFurthermore, the similar performance ï¬gures of the CLU\nand ALL approaches suggest that there exists a minimal\namount of data from which all the extractable information\ncan be derived6. Hence more data will then not result in an\nimprovement of the labelling performance. The next section\nwill examine this phenomenon in more detail, in particular\nby determining the minimum of audio data required to max-\nimize labelling performance.\n5.3 Scaling and computational aspects\nThe observations in the previous section suggest that there\nseems to be a strong amount of repetitiveness present inside\na music piece. Additionally, many excerpts â€“ even though\ndiffering in instrumentation â€“ produce the same label outpu t\nwhen processed with the used label inference method. To\nquantify those effects we used the CLU andNSEG meth-\nods to process the entire piece under analysis, as both offer\na straightforward way to vary the amount of data used by\nthe label inference algorithm. In particular, we studied th e\neffect of an increasing amount of segments to process on\nthe labelling performance. In case of the NSEG method we\nconstantly increased the amount of segments used by the la-\nbel inference, thus augmenting the methodâ€™s parameter n.\nFor the CLU method we sorted the clusters downwards by\nthe accumulated length of their respective segments, start ed\nprocessing just the ï¬rst one, and iteratively added the next\nlongest cluster. For both methods we then tracked the per-\nformance ï¬gures as well as the amount of data used for in-\nference. Fig. 4 depicts both performance and amount of data\nfor the ï¬rst 20 steps on the evaluation data (mean value of\nCV outputs).\nAs can be seen from Fig. 4 the performance of both CLU\nandNSEG systems stagnates at a certain amount of seg-\nments processed. Due to the different amount of data pro-\ncessed, those values represent, respectively, 3 and 5 seg-\nments. Hence, incorporating global timbral structure, as i m-\nplemented by CLU , beneï¬ts labelling performance at the ex-\n6The small differences in PandRresult from individual parameter set-\ntings, estimated by the CV by determining the best performing co nï¬gura-\ntion in respect to the Fmetric, and can be compensated by manually choos-\ning proper values. However, the F metric would not be affected , since there\nwill always be a trade-off between PandR.\n243Poster Session 2\n5 10 15 20 0.50.60.7Performance (F) \n  \n025 50 75 100\nRelative data size [%] \nNumber of segments CLU (F)\nCLU (data)\nNSEG (F) \nNSEG (data) \nFigure 4 . Scaling properties of the studied algorithms.\nSolid lines refer to the respective labelling performance i n\nterms of F, dashed ones show the respective data amount\nused for label inference, relative to the maximum as pro-\nduced by ALL. Mean values across CV-Folds are shown.\npense of algorithmic pre-processing. By preserving the con -\ntinuity of musical instruments the method shows a slightly\nsuperior performance compared to NSEG , which segment\nextraction is unaware of any contextual properties. In term s\nof the used data amount, NSEG is superior whilst process-\ning less than around 40% of the data (i.e. nâ‰¤10), whereas\nwhen processing more, CLU returns the better overall la-\nbelling performance. However, the results suggest that, on\naverage, a timbre-informed clustering does not result in a\nsigniï¬cant increase in performance, thus it might be of ad-\nvantage in specialized applications (e.g. working on a sing le\ngenre which exhibits clear recurrent structural sections) .\nFinally, the stagnation of labelling performance indicate s\na kind-of â€œglass ceilingâ€ that has been reached. It seems tha t\nwith the presented classiï¬cation and labelling methodolog y\nwe are not able to extract more information about the instru-\nmentation. Nevertheless, we can observe that predominant\ninstrumental information is highly redundant inside a give n\nWestern piece of music from which 70% of the labels can be\nobtained. Furthermore, this fact allows for a reduction of t he\neffective amount of data used for label inference of around\n55%. Remarkably, the same factor of about 1/2can also be\nobserved when comparing the number of different instru-\nmentations to the overall number of segments in the ground\ntruth annotations of all ï¬les in the used music collection.\n6. CONCLUSIONS\nIn this article we studied the problem of extracting labels\ncorresponding to the instrumentation from entire pieces of\nmusic. We designed our approach to be applied in a real\nworld context, hence the presented methods work on any\npiece of music, without imposing restrictions to the inputdata. In particular we analysed different methods to pre-\nprocess the entire tracks, studying the effect of data reduc -\ntion on recognition performance. Evaluation on a dataset of\n220 musical pieces showed that by using the best perform-\ning approach we are able to score a global F-measure of\n0.69 while examining 12 musical instruments. On the other\nhand, a proper preprocessing of the data allows for a reduc-\ntion of the amount of data used for label inference of more\nthan 50% while the recognition performance is preserved.\n7. ACKNOWLEDGEMENTS\nThis work has been supported by the following projects:\nClassical Planet: TSI-070100- 2009-407 (MITYC) and\nDRIMS: TIN2009-14247-C02-01 (MICINN).\n8. REFERENCES\n[1] J. Burred, A. Robel, and T. Sikora. Dynamic spectral envelope\nmodeling for timbre analysis of musical instrument sounds.\nIEEE Transactions on Audio, Speech, and Language Process-\ning, 18(3):663â€“674, 2010.\n[2] S. Chen and P. Gopalakrishnan. Speaker, environment and\nchannel change detection and clustering via the bayesian in-\nformation criterion. In Proc. of the DARPA , 1998.\n[3] S. Essid, G. Richard, and B. David. Instrument recognition\nin polyphonic music based on automatic taxonomies. IEEE\nTransactions on Audio, Speech, and Language Processing ,\n14(1):68â€“80, 2006.\n[4] F. Fuhrmann, M. Haro, and P. Herrera. Scalability, generality\nand temporal aspects in the automatic recognition of predomi-\nnant musical instruments in polyphonic music. In Proc. of IS-\nMIR, 2009.\n[5] F. Fuhrmann and P. Herrera. Polyphonic instrument recogni-\ntion for exploring semantic similarities in music. In Proc. of\nDAFx , 2010.\n[6] T. Heittola, A. Klapuri, and T. Virtanen. Musical instrument\nrecognition in polyphonic audio using source-ï¬lter model for\nsound separation. In Proc. of ISMIR , 2009.\n[7] T. Kitahara, M. Goto, K. Komatani, T. Ogata, and H. Okuno.\nInstrogram: A new musical instrument recognition technique\nwithout using onset detection nor f0 estimation. In Proc. of\nICASSP , 2006.\n[8] P. Leveau, D. Sodoyer, and L. Daudet. Automatic instrument\nrecognition in a polyphonic mixture using sparse representa-\ntions. In Proc. of ISMIR , 2007.\n[9] A. Livshin and X. Rodet. Musical instrument identiï¬cation in\ncontinuous recordings. In Proc. of DAFx , 2004.\n[10] S. Pei and N. Hsu. Instrumentation analysis and identiï¬cation\nof polyphonic music using beat-synchronous feature integra-\ntion and fuzzy clustering. In Proc. of ICASSP , 2009.\n[11] N. Scaringella, G. Zoia, and D. Mlynek. Automatic genre clas-\nsiï¬cation of music content: A survey. IEEE Signal Processing\nMagazine , 23(2):133â€“141, 2006.\n[12] R. Xu and D. Wunsch. Clustering . IEEE Press Series on Com-\nputational Intelligence, 2008.\n244"
    },
    {
        "title": "A Real-Time Signal Processing Framework of Musical Expressive Feature Extraction Using Matlab.",
        "author": [
            "Ren Gang",
            "Gregory Bocko",
            "Justin Lundberg",
            "Stephen Roessner",
            "Dave Headlam",
            "Mark F. Bocko"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1418081",
        "url": "https://doi.org/10.5281/zenodo.1418081",
        "ee": "https://zenodo.org/records/1418081/files/GangBLRHB11.pdf",
        "abstract": "In this paper we propose a real-time signal processing framework for musical audio that 1) aligns the audio with an existing music score or creates a musical score by automated music transcription algorithms; and 2) obtains the expressive feature descriptors of music performance by comparing the score with the audio. Real-time audio segmentation algorithms are implemented to identify the onset points of music notes in the incoming audio stream. The score related features and musical expressive features are extracted based on these segmentation results. In a realtime setting, these audio segmentation and feature extraction operations have to be accomplished at (or shortly after) the note onset points, when an incomplete length of audio signal is captured. To satisfy real-time processing requirements while maintaining feature accuracy, our proposed framework combines the processing stages of prediction, estimation, and updating in both audio segmentation and feature extraction algorithms in an integrated refinement process. The proposed framework is implemented in a MATLAB real-time signal processing framework.",
        "zenodo_id": 1418081,
        "dblp_key": "conf/ismir/GangBLRHB11",
        "keywords": [
            "real-time signal processing",
            "musical audio alignment",
            "automated music transcription",
            "musical score creation",
            "expressive feature descriptors",
            "audio segmentation algorithms",
            "note onset points",
            "score related features",
            "musical expressive features",
            "real-time processing requirements"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\n  \n \nA REAL-TIME SIGNAL PROC ESSING FRAMEWORK OF \nMUSICAL EXPRESSIVE FEAT URE EXTRACTION USING \nMATLAB  \nRen Gang1, Gregory Bocko1, Justin Lundberg2, Stephen Roessner1, Dave Headlam1,2, Mark F. Bocko1,2 \n1Dept. of Electrical and Computer Engineering, Edmund A.  Hajim School of Engineering and Applied Sciences, \nUniversity of Rochester; 2Dept. of Music Theory, Eastman School of Music, University of Rochester   \ng.ren@rochester.edu,gregory.bocko@rochester.edu,justin.lundberg@rochester.edu, \nstephen.roessner@rochester.edu,dheadlam@esm.rochester.edu, mark.bocko@rochester.edu \n \nABSTRACT \nIn this paper we propose a real-time signal processing \nframework for musical audio that  1) aligns the audio with \nan existing music score or creates a musical score by auto-mated music transcription algorithms; and 2) obtains the \nexpressive feature descriptors of music performance by \ncomparing the score with the audio. Real-time audio seg-\nmentation algorithms are implemented to identify the onset points of music notes in the incoming audio stream. The score related features and musical expressive features are extracted based on these segmentation results. In a real-\ntime setting, these audio segmentation and feature extrac-\ntion operations have to be accomplished at (or shortly after) the note onset points, when an incomplete length of audio signal is captured. To satisfy real-time processing require-\nments while maintaining feature accuracy, our proposed \nframework combines the processing stages of prediction, \nestimation, and updating in both audio segmentation and feature extraction algorithms in an integrated refinement process. The proposed framework is implemented in a MATLAB real-time signal processing framework.  \n1. INTRODUCTION \nMusic performance adds interpretative information to the \nshorthand representation in a music score [1]. These per-formance dimensions can be extracted from performance \naudio as musical expressive features using signal processing algorithms as in [2]. These features quantitative-ly model the performance dimensions that reflect both the \ninterpretation of performance musicians and the artistic in-\ntention of composers [1] and are important for various mu-sic signal processing [2] and se mantic musical data analysis \n[3,4] applications.        Existing automatic music transcription [5] and musical \nexpressive feature extraction al gorithms [2] are designed in \npost-processing frameworks. Th ese existing algorithms are \nessentially multimedia file pr ocess systems, which assume \nthat the entire duration of the audio performance is already \nrecorded. However, various real-time signal processing ap-\nplications, such as visualization, automatic music mixing, stage lighting control, interactive music media, and elec-tronic games, require that mu sical expressive features be \nextracted and synchronized with the ongoing audio. In such a real-time signal processing framework, the musical ex-\npressive features have to be obtained from the audio signal \nthat is still in progression to facilitate simultaneous interac-tions with external applications. Thus, the complete music event is not observed at the â€œd ecision pointâ€ since the mu-\nsic transcription and expressive features have to be obtained \nat (or shortly after) the onset of each music event. In this \npaper we extend the feature extraction and recognition functionalities of conventional music transcription and musical expressive feature ex traction algorithms and estab-\nlish a real-time processing fr amework which includes the \nprocessing stages of prediction, estimation and updating. \nFirst, signal features (signal features here include segmenta-\ntion features, score-level features and musical expressive features) are predicted using generative probabilistic graph-\nical models [6,7] based on a â€œh istoryâ€ of these features (or \nother available information, e. g., features extracted from a \nsynchronized rehearsal track) . Then we estimate these sig-\nnal features when a short audio segment in the beginning \npart of a music event is available. When additional audio frames are captured, we refine the estimations and make necessary updating.   \n     â€œTrueâ€ real-time methods can only be achieved in a fea-\nture prediction framework: the signal features are obtained \nbefore the actual music event. For example, in an automatic \nmusic mixing system, we are ex pected to adjust the fader \nsettings according to the â€œpastâ€ signal features before  a \nloud section begins.  That is, the expressive loudness fea-ture and its related fader instruction must be generated at a \nÂ \nPermission to make digital or hard copi es of all or part of this work fo r\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for profit or  commercial advantage and that cop-\nies bear this notice and the full citation on the first page.  \nÂ© 2011 International Society for Music Information Retrieval  \n115Poster Session 1\n  \n \ntime point when the music event of the â€œfutureâ€ loud sec-\ntion is not observed at all! In our proposed processing framework, a generative probabilistic graphical model [6] is \nemployed to enable such predictions. A probabilistic graph-\nical model depicts the causalit y (or statistical) relations be-\ntween signal features [7]. A prediction of â€œfutureâ€ signal \nfeatures is inferred from these statistical relations and a fi-nite length of an observed â€œhistoryâ€. Such predictions might fail, as any prediction that peeks into an unknown \nâ€œfutureâ€. To improve the reliability of our proposed system, \nseveral levels of relaxation are applied. These pseudo-real-time processing frameworks are essentially buffer and post-processing frameworks that allow us to take glimpses at the music event and be more â€œco nfidentâ€.  If the signal \nprocessing delays they introduce are kept within the percep-\ntual limit (about 25ms [8]), the live performance, audio and \nthe feature processing results would appear to be percep-tually well synchronized for the audience.  \n       A pseudo-real-time processing framework allows a \nshort audio frame to be captu red near the predicted music \nnote onset. The signal features extracted from this short au-\ndio frame confirms or rejects the predicted onset location and other signal feature dimensions. If the pseudo-real-time constrains, including the percep tual delay limit and/or the \naudio reinforcement delay limit, are satisfied, a short signal capturing and processing delay would be effectively con-\ncealed from the audience. The perceptual delay limit is the \nlimit of human perceptual capabilities of discerning the time sequence of two perceptual events [8,9]. For applica-tion scenarios such as visualization, a short delay such as 10ms in the visualization interface is not perceptible since human visual perception is a relatively slow responding \nprocess [9]. However, a proc essing delay that exceeds 20ms \nresults in a sloppy â€œthunder first, lightning secondâ€ effect.  An audio reinforcement delay can be utilized in application scenarios where sound reinforc ement systems are employed \nto further enhance synchronizations. The reinforced sound \nis briefly delayed to compensate for the signal processing \ndelays so the reinforced sound is still synchronized with the feature extraction and processing results\n1. Because the sig-\nnal features extracted usually trigger the most dramatic vis-\n                                                           \n1 In a staged music setting, for instan ce, the music expressive features and \nthe aural-visual events controlled by these features are delayed behind the \nonset of stage scenes because a short audio frame have to be captured and \nprocessed. Taking the stage light cont rol application as an example, the \nlight controlled by loudness feature tu rns on 10ms after an actor begin to \nsing a music phrase. In this â€œpreciousâ€ 10ms, a short audio frame is cap-\ntured and analyzed so the â€œlight on â€ stage lighting instruction could be \ninferred. The reinforced audio is also  delayed 10ms to compensate for the \ndelay of the lighting effect. For the a udience the reinforced audio onset is \nperfectly synchronized with the lighting effect since they are both delayed 10ms behind the actor, while the 10ms delay between stage scene and au-\ndio/lighting is still imperceptible.\n      ual and aural events and the reinforced audio carries the \nmost prominent aural event, this audio reinforcement delay effectuates the most critical synchronizations and is thus \nstrongly recommended whenever applicable. The sound \nreinforcement delay must be kept low (less than 20ms, with a typical value of 10ms) to maintain the perceptual syn-chronizations of other aural and visual events. On the aural aspect, the audio reinforcement delay limit ensures that the direct sound from actors can blend seamlessly with the \nreinforced sound for front-row audiences. On the visual \nside, the reinforced audio lags behind the stage scenes so this limit insures that the time lag is perceptually tolerable.     \n     The proposed system architecture as detailed in Sec. 2 \nutilizes both real-time music event prediction and pseudo-real-time processing, with an  emphasis on pseudo-real-time \nprocessing. Key processing components are introduced in \nSec. 3. Sec. 4 discusses the MATLAB implementation is-sues and Sec. 5 provides a brief summary. \n2. SYSTEM ARCHITECTURE \nThe system architecture of our proposed system is illu-\nstrated in Figure 1. Figure 1( a) is the system architecture \nfor application scenarios when a music score database is available and a matching music score is retrieved. In the initialization phase, a short audio segment (5-20 seconds) is first captured as the audio qu ery for finding the matching \nmusic score using score-audio matching algorithms [10]. \nThe feature estimation blocks include audio segmentation \nand features extraction algo rithm. The real-time score-\naudio alignment algorithm segments the audio by identify-ing the onset points based on the music score and the seg-mentation features extracted from the audio. If a music on-set is detected, the following short audio frame is captured \nand passed on to the musical expressive feature extraction \nalgorithm to obtain an initial estimation of musical expres-sive features. These musical ex pressive features are then \nformatted as a control data stre am for external applications. \nFigure 1(b) presents alternat ive system architecture for the \napplication scenarios when a music score is not available. \nIn this system we implement a real-time music transcription framework parallel with the real-time musical expressive feature extraction process. Fo r both systems music event \nprediction and feature updating algorithms are implemented to further improve performance. The music event prediction \nalgorithm predicts the â€œfutur eâ€ feature values based on a \nâ€œhistoryâ€ and use the prediction values as priors for the â€œcurrentâ€ music event segmentation and feature estimation process. The alignment/featur e updating algorithm refines \nsignal features when additional audio frames are captured and submits essential corrections. The refined features also \nimprove subsequent probabilistic predictions. \n11612th International Society for Music Information Retrieval Conference (ISMIR 2011)\n  \n \n \n(a) \n \n(b) \nFigure 1 . System Architecture.  (a) is the system architec-\nture when a music score database is available. (b) is the \nsystem architecture when the mu sic score is not available.   \n3. REAL-TIME PROCESSING ALGORITHMS \nReal-time processing algorithms for key functional blocks \nare introduced in this section. Algorithms both for applica-\ntion scenarios with and with out a music score are intro-\nduced.   \n3.1 Audio Segmentation \nIf a music score is available, the note boundaries are identi-\nfied using score-audio alignment or score following algo-rithms based on real-time dynamic time warping as detailed \nin [10]. These algorithms optimally align a music score to \nthe dynamic performance timeline of an audio file by searching-and-finding an optimal alignment path corres-ponding to the alignment featur es extracted from the score \nand the audio.  \n    If music score is not available, the conventional onset \ndetection and music event segmentation algorithms [11] are extended to f it in our proposed real-time processing \nframework. These onset detection algorithms compare the audio features (for example, energy value or spectrograph-\nic content) and track their variations. The magnitude of the \nvariations is encoded as an onset detection function Ü¦áˆºİáˆ»  \nand the time points correspond to significant variation are selected as onsets or segmentation points. In our proposed real-time framework only the â€˜pastâ€™ part of the detection \nfunction Ü¦áˆºİáˆ»,İ àµ‘ İ\nà¯– is available, where İà¯– is the current \ntime. To ensure real-time pr ocessing performance, we can-\nnot delay the segmentation decision until a downward \nslope of Ü¦áˆºİáˆ» is observed. Instead of peak-picking [11], the \nsegmentation decisions have to be generated using a thre-shold detection method, which do not guarantee that a \nÜ¦áˆºİáˆ» peak is reached.  \n     Our proposed real-time processing framework is im-\nplemented by providing two types of threshold for onset \ndetection. An initial detection threshold is set as İ„İ\nà¬µ. If \nÜ¦áˆºİà¯–áˆ»àµİ„İ à¬µ and no segmentation decisions have be gener-\nated in a time interval of  İà¯–, an initial segmentation point \nis identified. A â€˜regrettingâ€™ threshold is set as İ„İà¬¶. If \nÜ¦áˆºİà¯–áˆ»àµİ„İ à¬¶ and the time distance İà¯¥ to the previous seg-\nmentation decision satisfies İà¯¥à¬µàµ‘İ à¯¥àµ‘İ à¯¥à¬¶, a forward up-\ndating of the segmentation point is performed to erase an existing segmentation point a nd substitute the current time \npoint. Here İ\nà¯¥à¬µ is the segmentation error tolerance. If the \nprevious segmentation point is within this range, a correc-\ntion is not necessary. İà¯¥à¬¶ is the maximum correction range. \nIf the time interval to the previous segmentation point is \ngreater than İà¯¥à¬¶, another segmentation point is generated \nusing threshold İ„İà¬µ. These thresholds are time varying with \nthe current beat tracking result obtained using the algo-rithms in [12]. The rhythmically significant locations are assigned a lower detection threshold as in Fig. 2 to push detected onsets towards these interpolated locations, as a \ncombined process of prediction and real-time detection.   \n \nFigure 2 . A typical profile of segmentation detection \nthresholds.  The lower detection threshold at predicted \nrhythmic locations pushes the segmentation point towards \na predicted rhythmic grid. \n117Poster Session 1\n  \n \n3.2 Feature Extraction \nThe musical expressive features we implemented include \nfeature dimensions of the relatively small but continually \nchanging adjustments in pitch, timing, auditory loudness, \ntimbre, articulation and vibrato that performers use to create expression [1,2]. Defin itions of these feature dimen-\nsions are briefly summarized in Table 1 and more details can be found in [2]. In this section the real-time extraction \nprocess of symbolic pitch and expressive feature dimen-\nsion of pitch deviation is detailed in an application scenario \nwhen a music score is not available. Pitch deviation meas-ures the difference between performance pitch and the score specified pitch [2]. The expressive pitch processing \nis more sophisticated compared to other feature dimensions since the quantized score pitch, the expressive pitch devia-\ntions, and the calibration of a temperament grid\n1 have to be \nupdated simultaneously. The other feature dimensions are briefly summarized in Table 1 and their feature extraction algorithms are similar extensions based on [2].  \n      For estimation of pitc h deviation an accurate mapping \nbetween symbolic pitch and fundamental frequency (F0) \nhas to be established since the expressive pitch deviation is just a small fraction of the fundamental frequency. The fundamental frequency is first obtained from the audio frames captured at the segmentation point using a pitch de-tection algorithm as in [13]. Suppose that the fundamental \nfrequency is detected from the first short audio frame of \nmusic note İ‰ and denoted as İ‚\náˆºà¬µáˆ» and the initial tempera-\nment grid we implemented as àµ£ İ‚áˆ™à¯ ,İ‚áˆ˜à¯ ;İ‚Ò§à¯ ,İŒà¯ àµ§, İ‰ àµŒ\n1, â€¦ , Ü¯ . Here İ‚áˆ™à¯  and İ‚áˆ˜à¯  is the decision boundary of the \npitch quantization grid. İ‚Ò§à¯  is the quantized frequency value \nthat would be selected if İ‚áˆ™à¯ àµ‘İ‚ à¯ àµ‘ İ‚áˆ˜à¯  and İŒà¯  is its sym-\nbolic value. For equal temper ament scale, the quantized \nvalue İ‚Ò§à¯ s form a temperament grid which is derived from a \nreference frequency point İ‚Ò§à¯‹ with symbolic pitch value İŒà¯‹ \nas: \n                İ‚Ò§à¯  àµŒ t p a à­£áˆºİ‚Ò§à¯‹,İŒà¯‹;İŒà¯ áˆ»àµŒ  2à³›à³˜à°·à³›à³ƒ\nà°­à°® Â·İ‚Ò§à¯‹            (1) \nwhere İŒà¯  is the symbolic pitch value of quantization inter-\nval áˆ¾İ‚áˆ™à¯ ,İ‚áˆ˜à¯ áˆ»,  here İŒà¯  and İŒà¯‹ is specified in MIDI value. \nSince human frequency discernment is most acute at mid-\nfrequency region, the frequency reference point áˆ¾İŒà¯‹:İ‚Ò§à¯‹áˆ¿ \ncould be selected at this frequency region. In our imple-mentation the reference point [69:440Hz] is selected. Us-\ning this initial temperament grid, we obtain the initial sym-\nbolic pitch value as İŒ\náˆºà¬µáˆ». When additional audio frames are \ncaptured from the audio stream, we might revise our esti-\n                                                           \n1 For expressive feature extraction this  calibration is crucial because the \ncalibration level is within the same  range of pitch deviation value.    mation of the İ‚áˆºà¬µáˆ» and İŒáˆºà¬µáˆ» values within a music note \nbased on the pitch detected in the extended musical note \nduration. To ensure a smooth updating process we only \nupdate the F0 estimation after a time interval. We also only update the estimated value of fundamental frequency and pitch deviation if the difference of two adjacent estimated F0 values will exceed the detection grid of one semitone.  \n      When an adequate number of music notes are captured, \nthe temperament grid is updated by fitting a temperament \ngrid to the detected F0 values in a calibration process.    Suppose the F0 sequence we obtained is represented as \nİ‚\nà¬µâ€¦İ‚ à¯†, these frequency points find their quantized values \nİ‚Ò§à¬µâ€¦İ‚Ò§à¯† as the nearest neighbors in an initial quantization \ngrid with frequency reference point áˆ¾İŒà¯‹:İ‚Ò§à¯‹áˆ¿. The residual \nvalues of this quantization process are denoted as İ€à¬µâ€¦İ€ à¯†. \nThen we shift the frequency reference point within 1/6 of a \nsemitone interval and find the best reference frequency \npoint İ‚Ò§à¯‹àµ…âˆ† İ‚ Ò§à¯‹ where the sum of the residual values \nâˆ‘| İ€à¯ |à¯†\nà¯ à­€à¬µ  is minimized. After this calibration process the \nresidual frequency value İ€à¬µâ€¦İ€ à¯† is calculated as pitch \ndeviation values. The pitch deviation in the units of cents is \ncalculated as 1200 Â· İƒİ‹İˆ à¬¶àµ«İ‚İ€ Ò§â„àµ¯. An example of pitch fea-\nture extraction and feature updating process is illustrated in \nFigure 3. \n \n                                                   (a) \n \n                                                  (b) \n \n                                                  (c) \nFigure 3 . Estimation and Updating Process of Musical \nPitch Related Features.  (a) audio waveform; (b) quantized \nmusical pitch; (c) expressive pitch deviation.  \n11812th International Society for Music Information Retrieval Conference (ISMIR 2011)\n  \n \n \n \n \n  \n \n  \n \n  \n \n \n \n  \n \n  \n \n  \n \n  \n \n3.3 Music Event Prediction \nCertain aspects of music event prediction have been intro-\nduced in the real-time audio segmentation algorithm as in \nSec. 3.1, where we perform a beat detection algorithm and \ninterpolate the beat detection resu lts as predictors of â€œfutureâ€ \nrhythmic structure. The statistical relations within a time series of audio features are codified using probabilistic graphical models [7] as a prediction framework to infer the â€œfutureâ€ feature values based on available observations.  \nComplete learning and inference algorithms of a music \nevent prediction framework are detailed in [6]. Most real-time applications require an early â€œdecision pointâ€, where \nthe available audio segment is still insufficient for unambi-guously estimating most feature dimensions. Thus in our \nproposed frameworks these probabilistic predictions are \nintegrated into the audio segmentation and feature extrac-tion process. The signal features are predicted before the onset of an actual music event as prior information for fea-ture estimations. Additional reference feature tracks includ-ing a music score or a matching expressive music transcrip-\ntion obtained from a rehearsal track can be further incorpo-\nrated in this prediction framework, as an extension to the alignment process that assigns  reference features as the \nprediction values to real-time music events. The integration \nof prediction and estimation also allows the prediction point to be closer to the â€œdecision pointâ€, as the shortened predic-\ntion distance enhances the prediction accuracy [6].   \n  \n \n     \n \n     \n \n \n3.4 Feature Updating \nThe real-time segmentation decision process here is essen-\ntially a hit-or-miss process: once a segmentation decision is made based on the audio signal features of the â€œcurrentâ€ audio  frame  (we  may also utilize the â€œpastâ€ audio frames deposited in the captured signal stream and some predic-tion) any audio frames captured later will not count even if \nthe â€˜hitâ€™ (the attack point) is at the wrong place. If we \nâ€œmissâ€ a segmentation point due to a stringent detection threshold, we may find that the subsequently captured au-dio frames are inappropriate for allocating a segmentation \npoint. The design of real-time feature extraction algorithms \nalso have to balance these requirements of real-time per-\nformance and feature accuracy. To reconcile these conflict-ing real-time performance criteria we implement an updat-ing mechanism which enables th e system to â€œregretâ€ pre-\nvious prediction/estimation when subsequent events in the audio stream are captured an d processed. These refine-\nments are buffered for improving future predictions and \nessential updates are submitted to the external applications. Although for some application scenarios a real-time deci-\nsion is irreversible, certain minor corrections can still be effectively disguised using perceptual models [9]. Because frequent revisions give the system user an unstable impres-\nsion, the number of segmentation point modifications must \nbe restricted. An example of a feature updating process is illustrated in Figure 3. Feature  Definition Real-Time Musical Expre ssive Feature Extraction Algorithms Typical \nValue \nPitch  Deviation The difference be-\ntween performance \npitch and score pitch (1) The fundamental frequency of an audio segm ent is detected using a pitch analysis algo-\nrithm as described in [13]. \n(2) A temperament grid is initialized and fit to the fundamental frequency sequence as the \nmusic note number increase. The deviation of the optimum temperament grid is utilized as \nthe pitch calibration value. \n(3) The pitch deviation is calcula ted by comparing the audio pitch İ‚ and with score pitch İŒ- .15 cents  \nto \n15 cents \nAuditory Loudness The perceptual intensi-\nty of sound   Calculate the strength of auditory response [2] of an short audio segment of 20ms based on \nits energy distribution in the frequency do main, using a computational auditory model 30 dB dy-\nnamic range \nTiming The time difference of \nmusic events between \nthe score and the au-dio.          Ü¨\nà¯áˆºİŠáˆ»àµŒİáˆºİŠàµ…1 áˆ»àµ†İáˆºİŠáˆ»\nİÌ‚áˆºİŠàµ…1 áˆ»àµ†İÌ‚áˆºİŠáˆ» The time deviation of onset İŠ is calculated the normalized  onset time deviation as: \nwhere İáˆºİŠáˆ» is the audio onset timing  and İÌ‚áˆºİŠáˆ» is the interpolated score timing. İáˆºİŠàµ…1 áˆ» \ndenotes the next onset location. Ü¨à¯áˆºİŠáˆ» can be viewed as an indicator of timing extension \n(Ü¨à¯áˆºİŠáˆ»àµ1) or compression ( Ü¨à¯áˆºİŠáˆ»àµ1). From 0.6 \n(compres-\nsion) to 1.5 (exten-\nsion) \nTimbre The energy distribu-\ntion pattern of the fre-\nquency domain  (1) The short time Fourier analysis result is Üµà¯†áˆºİ‡,İ… áˆ» is calaulated, where İ‡ is the frequency \nbin index.  İ… is the time frame index.  \n(2) The timbre centroid is calculated as the â€œw eight centerâ€ of the frequency spectrum of a \nanalysis segment as: \n                                        Ü¿áˆºİ…áˆ»àµŒâˆ‘ Üµİ‡à¯†à¬¶áˆºİ‡,İ… áˆ»à¯„\nà¯à­€à¬µ İ‡à®¿âˆ‘ Üµà¯†à¬¶áˆºİ‡,İ… áˆ»à¯„\nà¯à­€à¬µ â„                   \nwhere İ‡à®¿ is the frequency bin index of  fundamental sonic partial. \n(3) Timbre width is define d as the frequency width Ü¾áˆºİ…áˆ» required to include a pre-defined \nportion ßŸ( with a typical value of 90%) of the total energy.  Timbre cen-\ntroid from \n1.2 to 4.  \nTimbre width from  \n1.5 to 3. \nAttack The transient characte-\nristics of music onset  The attack feature [2] is calculated as the ratio of  the energy content of the first 1/3 of the \nnote duration.  from 0.5 to \n3.  \nVibrato The amplitude and \nfrequency modulation inside a musical note (1) A band-pass filter is implemented to extract  a single sonic partial from the complex \nharmonic sound for analysis.  (2) A musical vibrato recognition algorithm is  implemented as in [14]. The modulation \ncomponents of a vibrato note is extracte d using analytic signal methods [2].  Amplitude \nmodulation depth from 0.1 to 0.4.  \n \nTable 1. The definitions and real-t ime feature extraction al gorithms of musical ex pressive features\n119Poster Session 1\n  \n \n4. MATLAB IMPLEMENTATION \nIn a MATLAB real-time signal processing framework a ti-\nmer object [15] is implemented to handle the looping op-\neration and schedule the subsequent processing operations.  \nIn a timer  object loop a block of main code is executed ite-\nratively in a prescribed short time slot until an error or user interruption is detected. In our implementation the audio capturing and processing functionalities are programmed within the main timer  loop so for every timer  slot an audio \nframe is captured, analyzed and the feature data is submit-\nted to the external application. If the timer  slot is short \nenough (i.e., 10ms), the buffering and processing delay is negligible. If the capturing and processing time exceeds the allocated timer  object slot, the error handling function of \nthe timer  object is implemented. The error handling code \ncontains the same processing steps as in a regular \nprocessing timer slot and the code to resumes regular timer  \ncycles after error processing. This mechanism allows extra \nprocessing time when necessa ry. The audio capturing func-\ntionality is implemented by programming two audiorecord-\ner objects in each processing cy cle to make sure that there \nis no missing audio segment due to the processing delays. For the odd-numbered processing loops (including timer  \nloops and error handling loops), we capture the recorded audio segment from audiorecorder1, read the time location, \nclear and restart the recorder, and then append the audio \nsegment to the corresponding time location of the main au-\ndio stream for subsequent processing. For the even-numbered loops, we perform the same instructions on audi-\norecorder2 . In MATLAB, multiple audiorecorder  objects \nare run-time independent so their functionalities are per-formed simultaneously without interference.   \n5. SUMMARY \nOur proposed real-time signal processing framework of \nmusical expressive feature ex traction obtains musical fea-\ntures from an incoming audio stream and provides impor-tant music data for various multimedia applications such as \nvisualization, electronic game s, interactive media and au-\ntomatic music production. By implementing a processing framework that combines prediction, estimation and updat-ing, musical features are obtai ned at the music note onset. \nThis capability effectively sy nchronizes the musical expres-\nsive features with interactive content and avoids the delay \neffect of conventional post -processing frameworks. The \nproposed updating processing enables important feature modifications to be updated to  the user interface when addi-\ntional lengths of audio signal are captured. In a perfor-mance evaluation the performa nce of our proposed real-\ntime processing framework and an automatic post-\nprocessing framework [2] is compared with a benchmark dataset of manually annotated musical feature analysis. If \nany feature dimension of automatic processing is different from the benchmark dataset, the music note is considered \nan error. The error rate is th en calculated as the proportion \nof notes with errors. The test dataset is composed of oboe performance recordings that contain 162 music notes. The \nerror rate of real-time processi ng without music score, real-\ntime processing with music score, post-processing without music score, and post-processing with music score is \n19.75% (14.81% after update), 3.70% (1.23% after up-\ndates), 13.58%, and 1.23% respectively. These perfor-mances prove to be adequate for our proposed applications. \n6. REFERENCES \n[1] H. Schenker and H. Esser (Ed.), and I. S. Scott (Trans.): The Art of \nPerformance , Oxford University Press, New York, NY, 2000, pp. 3-\n6. \n[2] G. Ren, J. Lundberg, G. Bocko, D. Headlam, and M. F. Bocko: \nâ€œWhat Makes Music Musical?  A Framework for Extracting \nPerformance Expression and Emotion in Musical Sound,â€ \nProceedings of the IEEE Digital Signal Processing Workshop , pp. \n301â€“306, 2011. \n[3] M. Balaban, K. Ebcioglu, and O. Laske (Ed.): Understanding music \nwith AI : perspectives on music cognition , AAAI Press, Menlo Park, \nCA, 1992. \n[4] C. Raphael: â€œRepresentation and Sy nthesis of Melodic Expression,â€ \nProceedings of IJCAI09 , pp. 1474-1480, 2009. \n[5] A. Klapuri: â€œIntroduction to Music Transcriptionâ€. In A. Klapuri \nand M. Davy (Ed.): Signal Processing Methods for Music \nTranscription , Springer, New York, NY, 2006, pp. 3-20. \n[6] G. Ren, J. Lundberg, G. Bocko, D. Headlam, and M. F. Bocko: \nâ€œGenerative modeling of temporal si gnal features using hierarchical \nprobabilistic graphical models,â€ Proceedings of the IEEE Digital \nSignal Processing Workshop , pp. 307â€“312, 2011. \n[7] D. Koller and N. Friedman: Probabilistic Graphical Models: \nPrinciples and Techniques , The MIT Press, Boston, MA, 2009, \npp.1-14. \n[8] B. Moore: An Introduction of the Psychology of Hearing, 5th ed., \nAcademic Press, London, UK, 2000, pp. 160-165. \n[9] E. B. Goldstein: Sensation and Perception, 8th ed., Wadsworth \nPublishing, Belmont, CA, 2009. \n[10] M. MÃ¼ller: Information Retrieval for Music and Motion , Springer, \nNew York, NY, 2007, pp. 85-139. \n[11] J. P. Bello, L. Daudet, S. Abdallah, C. Duxbury, M. Davis, and M. \nB. Sandler: â€œA Tutorial on Onset Detection in Music Signalsâ€, IEEE \nTrans. Speech Audio Process. , Vol. 13, No. 5, pp.1035 â€“ 1046, \n2005. \n[12] M. Goto: â€œAn Audio-based Real- time Beat Tracking System for \nMusic With or Without Drum-soundâ€, Journal of New Music \nResearch , Vol. 30, No. 2, pp.159-171, 2001. \n[13] A. Klapuri: â€œAuditory Model-Based Methods for Multiple \nFundamental Frequency Estimati onâ€. In A. Klapuri and M. Davy \n(Ed.): Signal Processing Methods for Music Transcription , Springer, \nNew York, NY, 2006, pp. 229-265. \n[14] H. Pang, D. Yoon: â€œAutomatic De tection of Vibrato in Monophonic \nMusicâ€, Pattern Recognition , Vol. 38, pp.1135 â€“ 1138, 2005. \n[15] S. T. Smith: MATLAB Advanced GUI Development , Dog Ear \nPublishing, Indianapolis, IN, 2006, pp. 241-278. \n120"
    },
    {
        "title": "A simple-cycles weighted kernel based on harmony structure for similarity retrieval.",
        "author": [
            "Silvia GarcÃ­a-DÃ­ez",
            "Marco Saerens",
            "Mathieu Senelle",
            "FranÃ§ois Fouss"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1415172",
        "url": "https://doi.org/10.5281/zenodo.1415172",
        "ee": "https://zenodo.org/records/1415172/files/Garcia-DiezSSF11.pdf",
        "abstract": "This paper introduces a novel methodology for music similarity retrieval based on chord progressions. From each chord progression, a directed labeled graph containing the interval transitions is extracted. This graph will be used as input for a graph comparison method based on simple cycles â€“ cycles where the only repeated nodes are the first and the last one. In music, simple cycles represent the repetitive sub-structures of, e.g., modern pop/rock music. By means of a kernel function [10] whose feature space is spanned by these simple cycles, we obtain a kernel matrix (similarity matrix) which can then be used in music similarity retrieval tasks. The resulting algorithm has a time complexity of O(n+m(c+1)), where n is the number of vertices, m is the number of edges, and c is the number of simple cycles. The performance of our method is tested on both an idiom retrieval task, and a cover song retrieval task. Empirical results show the improved accuracy of our method in comparison with other string-matching, and graph-comparison",
        "zenodo_id": 1415172,
        "dblp_key": "conf/ismir/Garcia-DiezSSF11",
        "keywords": [
            "music similarity retrieval",
            "chord progressions",
            "directed labeled graph",
            "interval transitions",
            "graph comparison method",
            "simple cycles",
            "kernel function",
            "feature space",
            "kernel matrix",
            "music retrieval tasks"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nA SIMPLE-CYCLES WEIGHTED KERNEL BASED ON HARMONY\nSTRUCTURE FOR SIMILARITY RETRIEV AL\nSilvia Garc Â´Ä±a-DÂ´Ä±ez and Marco Saerens\nUniversit Â´e catholique de Louvain\n{silvia.garciadiez,marco.saerens }@ucLouvain.beMathieu Senelle and Franc Â¸ois Fouss\nUniversit Â´e catholique de Louvain â€“ Site de Mons\n{mathieu.senelle,francois.fouss }@fucam.ac.be\nABSTRACT\nThis paper introduces a novel methodology for music sim-\nilarity retrieval based on chord progressions. From each\nchord progression, a directed labeled graph containing the\ninterval transitions is extracted. This graph will be used as\ninput for a graph comparison method based on simple cy-\ncles â€“ cycles where the only repeated nodes are the ï¬rst and\nthe last one. In music, simple cycles represent the repetitive\nsub-structures of, e.g., modern pop/rock music. By means\nof a kernel function [10] whose feature space is spanned\nby these simple cycles, we obtain a kernel matrix (similar-\nity matrix) which can then be used in music similarity re-\ntrieval tasks. The resulting algorithm has a time complexity\nofO(n+m(c+1)) , wherenis the number of vertices, mis\nthe number of edges, and cis the number of simple cycles.\nThe performance of our method is tested on both an idiom\nretrieval task, and a cover song retrieval task. Empirical re-\nsults show the improved accuracy of our method in com-\nparison with other string-matching, and graph-comparison\nmethods used as baseline.\n1. INTRODUCTION\nSince the beginning of the 15th century, motivic elements\nhave made part of Western music, becoming common prac-\ntice during the 18th century. We can ï¬nd numerous exam-\nples of this phenomenon nowadays in modern pop/rock mu-\nsic which contain repetitive sub-structures, e.g., the chorus,\nverse, etc. According to [5], such repetitive structures, or\nmotifs , act as cues in music perception. â€œA cueis a very\nrestricted entity ... often shorter than the group itself, but\nalways embodying striking attributesâ€. This notion of cue,\nwould let a listener encode information in a more efï¬cient\nway, allowing longer structures to be memorized by means\nof smaller, more salient, features.\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proï¬t or commercial advantage and that copies\nbear this notice and the full citation on the ï¬rst page.\nc/circlecopyrt2011 International Society for Music Information Retrieval.Although motifs can be found in a songâ€™s harmony or\nmelody, in this paper we will focus on harmonic motifs for\nthree reasons: (i) many songs share a part of their harmonic\nstructure, as the number of chord progressions that are pop-\nular in a musical style ( idioms ) remain limited, while the\nmelodic structure can vary greatly from one song to an-\nother; (ii) studies in experimental psychology have shown\nthe essential role of harmony in musical sequence percep-\ntion [6]; (iii) although the amount of chord progression data\nis increasing thanks to chord estimation algorithms (see e.g.\n[13]) and user-generated data (which is readily available from\nthe web), few efforts have been put on harmony-based sim-\nilarity measures.\nOn the other hand, human listeners, due to their musical\nbackground, are more susceptible to like songs with a fa-\nmiliar harmonic structure, but yet different enough from the\nsongs they already know [14]1. We believe, thus, that com-\nparing songs thanks to their harmonic motifs would yield\nin a similarity measure that takes into account its repetitive\nharmonic sub-structures.\nOne efï¬cient way for motif extraction is the use of graphs.\nMotif extraction on graphs has attracted a lot of attention\nin the past years, e.g. in community detection [1], or in\ngraph comparison [10]. A motif is formally deï¬ned in [1] as\na connected undirected sub-graph (or weakly connected di-\nrected sub-graph) which appears frequently in a graph show-\ning some kind of structure. Examples of motifs are cliques,\npaths, cycles, or sub-trees. The method presented in this pa-\nper relies on the concept of cycle as a motif for similarity\ndetection between graphs (isomorphism). By transforming\nthe chord sequences into graphs, and comparing their simple\ncycles, we obtain a similarity measure based on the musical\nmotifs of a song (see Section 4.1 for a more precise descrip-\ntion). The contributions of our work are as follows:\n1. It is based on the repetitive harmonic features of\nsongs (which can be easily extracted from web re-\nsources, as done in the present work).\n2. The similarity measure deals with large structural\n1This is explained by [12] as â€˜the compromise between the repetition\nand the surpriseâ€ in the expectation of a human listener.\n61Poster Session 1\nchanges in chord progression (e.g., addition of repe-\ntitions, bridge, etc.).\n3. The similarity matrix can be extracted by means of\nkernel functions .\n4. The similarity is transposition invariant (the inter-\nvals between chords are used, instead of the chords\nthemselves).\n5. We provide a simple, general, methodology for com-\nputing similarities from chord progressions (from the\ntext mining step to acquire the data, to the automatic\nclassiï¬cation step with an SVM).\n6. We exploit a novel source of user-generated data\nthat is readily available on the Internet (in form of gui-\ntar chord progressions).\n7. Empirical tests show that music similarity retrieval\ncan be performed solely on the basis of chords.\nWe will brieï¬‚y review the related work about chord se-\nquence similarity in Section 2. Section 3 introduces the\ncyclic pattern kernels, on which our method is based. The\ndetails of our algorithm can be found in Section 4, as well\nas the graph extraction technique. Empirical testing is pre-\nsented on two music retrieval tasks in Section 5, and even-\ntually, Section 6 presents our conclusions.\n2. RELATED WORK\nHarmonic similarity has recently attracted the attention of\nthe MIR (Music Information Retrieval) community thanks\nto the improvement in chord estimation techniques [13], as\nwell as the increase of the available data. One of the ad-\nvantages of harmonic similarity is its ability to infer similar\nsongs whose melodies differ. In this context, [4] proposes an\napproach based on the Tonal Pitch Space (TPS) which com-\npares the change of chordal distance to the tonic over time.\nThis local distance is then used to build a step function that\ncomputes the global distance between two chord progres-\nsions by minimizing the non-overlapping area of the two\nstep functions. However, this method requires information\nabout the key of the piece and does not support structural\nchanges (e.g., introduction of repetitions).\nWe can also ï¬nd techniques based on approximate string\nmatching, such as the one proposed by [9]. This technique\nextracts the most similar regions of the two chord sequences,\nand computes a distance based on the number of simple op-\nerations (insertion, deletion, substitution) that are needed to\ntransform the ï¬rst region into the second. This algorithm\nhas complexity O(nm)wherenandmare the length of the\nsequences to compare, and edition costs must be provided.\nGenerative models are the third type of harmony similar-\nity techniques. Such models assume that harmony variationsoccur according to an underlying model. The authors of [15]\npropose to model chord transitions of a song by means of\nanth-order Markovian model, which serves as basis for\na Kullback-Leibler scoring function. A generative model\nbased on linguistics has also been applied in [3]. This har-\nmony similarity method is based on the assumption of a gen-\nerative grammar of tonal harmony. By applying a weighted\nversion of this grammar, a unique parse tree representing the\nchord sequence is obtained for each song â€“ note that context\nfree grammars produce multiple ambiguous parse trees, thus\na weighting of the rules is needed to choose among all possi-\nbilities. In order to measure the similarity of a pair of parse\ntrees, the largest embeddable tree is extracted. However,\nits time complexity is O(min(n,m)nm)and this technique\nmay reject a sequence which is considered as ungrammati-\ncal.\n3. CYCLIC PATTERNS KERNEL\nCyclic patterns represent harmonic motifs in chord progres-\nsions. In order to extract these motifs for music similarity,\nwe will rely upon the cyclic pattern kernels from [10]. In\nthis section we will present the key concepts of this tech-\nnique which computes a kernel based on the set of cyclic\nand tree patterns of a graph.\n3.1 Graphs and cycles\nLet us ï¬rst give some deï¬nitions concerning graphs and cy-\ncles. LetG= (V,E,label )be a directed graph deï¬ned as\na ï¬nite set of vertices V, edgesEâŠ†[V]2, and their labels.\nThe cardinalities of VandEarenandm, respectively. We\ndeï¬ne a simple cycle onGas a sequence\nC={v0,v1},{v1,v2},...,{vkâˆ’1,vk} (1)\nwherev0=vkand all others vi/negationslash=vjfor everyi,j(1â‰¤iâ‰¤\njâ‰¤k). Although a cycle may have several permutations,\nonly one of them (and the same in all cases) will be kept for\nour purposes. We can now deï¬ne the set S(G)as the set of\nsimple cycles of G, the set of unrestricted cyclic patterns as\nC(G), and its relation:\nS(G)âŠ‚C(G) (2)\nSimilarly, we can deï¬ne the set of tree patterns ,T(G), as:\nT(G) ={Tis a connected component of B(G)} (3)\nwhereBis the set of bridges of G(see [10] for more details).\n3.2 Kernel methods\nThe method presented in this paper belongs to the family of\nkernel methods [7, 17], a well-founded technique in statisti-\ncal learning which comprises three steps:\n6212th International Society for Music Information Retrieval Conference (ISMIR 2011)\n1. A mappingÏ†of the data from the input space, x, (di-\nrected labeled graphs G, in our case) into some mean-\ningful, application-dependent, feature space, F, (sim-\nple cycles):\nÏ†:xâ†’Ï†(x)âˆˆF (4)\n2. An inner product deï¬ned in the feature space, Ï†(x),\nin order to obtain the kernel matrix (a positive deï¬nite\nmatrix of similarities):\nk(x,y) =/angbracketleftÏ†(x),Ï†(y)/angbracketright (5)\n3. A learning algorithm for discovering patterns in that\nspace (in our case, an RBF SVM for the automatic\nclassiï¬cation based on the similarity matrix).\nOne interesting property of kernel functions is that, although\nthe feature space may have inï¬nite dimension (the number\nof possible cycles, in our case), it is often possible to com-\npute them in polynomial time. The obtained kernel matrix\ncan then be used as a similarity matrix for music retrieval\ntasks.\n3.3 Cyclic patterns kernel function\nA cyclic patterns kernel function is proposed by [10], which\ntakes two graphs as input, extracts their cyclic C(G)and tree\npatternsT(G)and uses them to build a mapping Î¦CP(G)\ninto the feature space:\nÎ¦CP(G) =C(G)âˆªT(G) (6)\nThecyclic pattern kernel is deï¬ned as the set of all simple\ncycles and tree patterns that appear in both graphs:\nkCP(Gi,Gj) =|C(Gi)âˆ©C(Gj)|+|T(Gi)âˆ©T(Gj)|(7)\nHowever, the problem of computing cyclic pattern kernels\nisNP-hard. For overcoming this issue, the authors in [10],\nrestrict the set of cyclic patterns to S(G), so that only sim-\nple cycles are computed (those cycles whose only repeated\nnodes are the ï¬rst and the last one). The advantage of sim-\nple cycles is that they can be computed in polynomial time.\nThe authors use the algorithm from [16], which extracts the\nsimple cycles of a graph by means of a depth-ï¬rst search in\ntimeO(n+m(c+1)) , wherenis the number of vertices, m\nis the number of edges, and cis the number of simple cycles.\nIt is important, thus, that there exists a bound ( well-behaved\ndata) on the number of simple cycles for the sake of efï¬-\nciency of the algorithm. As empirically shown in Section 5\n(see Figure 2), this is the case for our chord data.\n4. PROPOSED SIMPLE-CYCLE\nWEIGHTED KERNEL\nIn this section we present the proposed kernel, which is a\nvariant of the cyclic pattern kernels [10] introduced in theprevious section. We propose to focus our kernel only on\nsimple cycles which will represent the repetitive harmonic\nsub-structure of a song. In order to favor longer simple cy-\ncles, a weighted (normalized) version of the kernel will be\ncomputed.\n4.1 Graph extraction\nChord sequences represent the harmonic progression of a\nsong which may modulate over time, i.e., its key changes\nthrough time. This is an important issue for the detection of\nharmonic similarities, as the transposed chords may not co-\nincide. In order to make our method transposition-invariant\nwe will thus convert the chord sequence into interval se-\nquences, from which input graphs will be extracted. As\nonly structure matters for us, and not the â€œmusical distanceâ€\nbetween a pair of chords (in semitones), a label Î»iwill be\nassigned to each chord transition with the same â€œmusical\ndistanceâ€ (key invariant)2 3. For example, the transition\nCâ†’D#mwill share the same label as Fâ†’G#mand\nits enharmonic Câ†’E/flatm , i.e.,\n(C,D#m) = (C,E/flatm ) = (F,G#m) =Î»k (8)\nChords C,G,Am,F,C,G,F,C,G,Am,C,G,F,C,G,Am,...\nLabels (C,G ) = (F,C ) =Î»1,(G,Am ) =Î»2\n(Am,F ) =Î»3,(G,F ) =Î»4,(Am,C ) =Î»5\nIntervals Î»1,Î»2,Î»3,Î»1,Î»1,Î»4,Î»1,Î»2,Î»5,Î»1,Î»4,Î»1,...\nGraph\nTable 1 . Transformation of an extract of the chord progres-\nsion of â€œLet it beâ€ from The Beatles into an interval graph.\nBy sequentially reading the obtained interval sequence\nx={Î»1,Î»2,...,Î» 1,...,Î» l}, we will extract a directed graph\nG(see Table 1) where each node represents a chord transi-\ntion or interval ( n=|{Î»i}|), and each interval transition is\nrepresented by an edge ( m=|{Î»iâ†’Î»j}|).\n2For the sake of consistency we have not made the distinction between\nascending or descending intervals.\n3Please note that the chord type (minor, major, diminished, etc.) is\nalready incorporated in the graph representation through the Î»values, e.g.,\n(Cdim,Am ) = (Edim,C #m) =Î»k.\n63Poster Session 1\nFigure 1 . Computation of the simple-cycles weighted kernel on two initial graphs, G1andG2.\n4.2 Kernel function\nBased on the algorithm from [10], we build a kernel which\ntakes any two interval graphs from the input space, extracts\ntheir simple cycles to build a feature space, and computes a\nsimilarity as the weighted inner product in the feature space.\nIn our case, the mapping function Î¦is deï¬ned as a mapping\nto the set of all possible simple cycles of the graph\nGâ†’Î¦SC(G) =S(G) (9)\nwhich represent the repetitive sub-structures of an interval\ngraph. For a particular graph Gj, its feature vector has en-\ntries[Ï†(Gj)]iwhich are equal to 1 if the simple cycle with\nindexi(denoted as cycle iin the sequel) is present in the\ngraph and 0 otherwise. We then compute the kernel func-\ntion as the weighted inner product between the feature vec-\ntors (simple cycles vector)\nk(x,y) =/angbracketleftÏ†(x),Ï†(y)/angbracketrightËœW=Ï†(x)TDÏ†(y) (10)\nwhereDis the normalized diagonal weight matrix\n[D]ii=dii=wi/summationtext\njâˆˆS(Gk)âˆªS(Gl)wj(11)\nandwiis the length of the i-th cycle. The motivation for\nthis weighting is to favor longer cycles, so that two graphs\nsharing a long cycle are considered as more similar as two\ngraphs sharing one short cycle. Furthermore, the kernel\nweights are normalized by dividing them by their sum. The\ncomplete procedure is described in Algorithm 1 and an ex-\nample on how to compute the weighted kernel is given in\nFigure 1.\n5. EMPIRICAL TESTING\nTo evaluate empirically the retrieval performance of our ker-\nnel, two different tasks will be evaluated: (i) a cover song\nretrieval task, and (ii) an idiom retrieval task. We will ï¬rst\npresent the data used in the experiments, as well as the cho-\nsen lexicon. Our simple-cycles weighted kernel method isAlgorithm 1 Simple-cycles Weighted Kernel: computa-\ntion of the kernel matrix.\nInput:\nâ€¢maxL> 0: maximum length of extracted simple cy-\ncles.\nâ€¢s1,...,s r: list of chord sequences to be compared.\nOutput:\nâ€¢K: the Simple-cycles Weighted Kernel matrix.\n1.fork,l= 1 tordo\n2. Transform chord sequences skandslinto directed\nlabeled graphs GkandGlfollowing the procedure\nfrom Table 1.\n3. Extract all simple cycles of length < maxL ,S(Gk)\nandS(Gl), fromGkandGl, with the algorithm de-\nscribed in [16].\n4. Create the feature vectors, Ï†(Gk)andÏ†(Gl), of\nlength|S(Gk)âˆªS(Gl)|, whose entry [Ï†(Gk)]i= 1\nif thei-th cycle is inS(Gk)and 0 otherwise.\n5. For all the cycles of S(Gk)âˆªS(Gl), compute the\ncorresponding elements iof the diagonal matrix D\nfrom Equation (11).\n6. Compute [K]kl=Ï†(Gk)TDÏ†(Gl).\n7.end for\ncompared to several measures from string matching, as well\nas graph comparison techniques.\n5.1 The chord data sets\nThe cover song data set has been extracted from two dif-\nferent sources: the Beatles chord annotations from the Iso-\nphonics4data base (Queen Mary, University of London),\nand the user-generated chord ï¬les from the Ultimate-Guitar5\ndata base. Although our ï¬rst source of chord progressions\nhas already been used in MIR, we are the ï¬rst to use, to\nthe best of our knowledge, a popular Internet guitarâ€™s chord\n4isophonics.net\n5www.ultimate-guitar.com\n6412th International Society for Music Information Retrieval Conference (ISMIR 2011)\ndata base for similarity retrieval. The Ultimate-Guitar data\nbase contains more than 250,000 user-generated sequences\nof guitarâ€™s chords of popular pop/rock music. Although sev-\neral versions are available for each of the Beatlesâ€™ songs,\nonly well-ranked songs have been extracted (5 star rated\nsongs with at least 5 votes), making a total of 71 songs.\nThese same songs have been extracted afterwards from the\nIsophonics data base, forming 71 classes of two songs each\n(142 songs in total), where the songs from the Isophonics\ndata base are used as query over the remaining 71 songs\nfrom the Ultimate-Guitar data base (one relevant song per\nquery). Although there exists a well-known MIREX audio\ncover song task, this evaluation task takes audio signals as\ninput while our work is centered on chords, so that it cannot\nbe applied here.\nTheidiom data set has been fully extracted from the Ultimate-\nGuitar data base and contains 296 songs partitioned in two\nclasses (101 songs for the ï¬rst class, sharing a common 4-\nchords idiom6, and 195 songs for the second class). Both\ndata sets are available from www.isys.ucl.ac.be/staff/\nsilvia/research.htm .\nIn both cases, a modest lexicon containing all major and\nminor root chords (ï¬‚at and sharp) has been used. We be-\nlieve that this choice is representative enough for our pur-\npose, while avoiding bad transcription issues from users in\nthe Ultimate-Guitar data base, e.g., the chord C5appears\ninstead ofC.\n5.2 Cover song retrieval task\nCover song retrieval (see for instance [2]) is a popular task in\nMIR which aims at identifying the versions of a given song.\nFor this purpose, the cover song data set described above\nhas been used. We query the Ultimate-Guitar database with\neach song from the Isophonics chord annotation (the â€query\nsongâ€), providing a ranking of the Ultimate-Guitar songs\nin decreasing order of similarity with the Isophonics query\nsong (please see Section 5.1 for more details). The average\nranking position of all retrieved songs, as well as two recall\nmeasures describing the accuracy of our method have been\nreported in Table 2: the average ï¬rst tier (the number of\ncorrectly retrieved songs among the best (ncâˆ’1)matches\ndivided by (ncâˆ’1)withncthe class size, i.e., in our case\nnc= 2), and the average second tier (number of correctly\nretrieved songs among the best (2ncâˆ’1)matches divided\nby(ncâˆ’1)).\nIn order to compare our method to other base line meth-\nods, the same methodology7has been applied to three string\nmatching techniques â€“ the edit distance and longest com-\n6The sequence â€œC,G,Am,Fâ€ is considered as an idiom in modern\npop/rock composition. It appears in songs such as Let it be (The Beatles),\nandWith or without you (U2).\n7Interval sequences have been provided as input for each baseline\nmethod, so that all compared methods are transposition invariant and eval-\nuated under similar conditions.mon subsequence widely used in sequence matching (see,\ne.g., [8]) and the all-subsequences kernel [17] which is an\nefï¬cient method that compares all sub-sequences of two strings\nâ€“, and a graph comparison kernel â€“ the fast sub-tree kernel,\na similarity measure between graphs that is fast to compute\nand that outperforms other graph kernels [18]. For methods\nneeding a parameter, the fast sub-tree kernel and the simple-\ncycles kernel, we have chosen a maximum cycle length (tree\ndepth) of 7 â€“ longer cycles or deeper trees become too song-\nspeciï¬c, and are not of interest for us. Although chosen base\nline methods may appear simplistic, our aim is to compare\nour algorithm with a variety of methods under the same con-\nditions. Purpose-built methods using different chord repre-\nsentations, or needing parameter tuning are not compared in\nthe present article for obvious reasons of adaptation, leaving\nthis task for further work.\nAlthough results show no improvement for the ï¬rst tier,\nand just a slight improvement of the second tier (see average\nï¬rst and second tier in Table 2) from the base line methods,\nthere is a clear improvement in the average general ranking\nof retrieved songs. These results are encouraging for using\nthe Ultimate-Guitar data base as a future source for chord\nprogression data.\n5.3 Idiom retrieval task\nIdioms have recently attracted the attention of MIR as a new\nobject of musicological interest. An idiom is deï¬ned in [11]\nas a â€œprominent chord sequence in a particular style, genre\nor historical periodâ€. Users have also discovered this notion\nof idiom as shown in a youtube video8, where a sequence of\n4 chords is used to assemble the melody of several pop/rock\nsongs. Interestingly, people who liked a few of these songs\ntended to also appreciate the others.\nWe have tried to recover the songs containing the idiom\nâ€œC,G,Am,Fâ€ (or â€œI-V-VI-IVâ€) by applying a 10-fold double\ncross validation with an RBF SVM on the idiom data set\nfrom the Ultimate-guitar web site. Classiï¬cation rates with\na 95% conï¬dence interval are reported in Table 3. These\nresults show an increase of performance of our method of\n7% from the closest base-line method.\nSimilarity First tier Second tier Average\naverage average ranking\nEdit distance 78.87%Â±6.76 87.32%Â±5.51 4.169Â±1.64\nLongest common subs. 60.56%Â±8.10 69.01%Â±7.66 8.662Â±2.59\nAll-subsequence kernel 28.17%Â±7.45 43.66%Â±8.22 15.929Â±3.32\nFast sub-tree kernel 52.11%Â±8.27 61.97%Â±8.04 11.943Â±2.78\nSimple-cycles kernel 78.87%Â±6.76 88.73%Â±5.24 2.915Â±1.09\nTable 2 . Average ï¬rst tier, second tier, and average ranking\nfor the cover retrieval task with 95% conï¬dence intervals.\n8http://www.youtube.com/watch?v=qHBVnMf2t7w\n65Poster Session 1\nSimilarity Classiï¬cation rate and\nconï¬dence interval\nEdit distance 68.56%Â±1.53\nLongest common subsequence 69.91%Â±2.41\nAll-subsequence kernel 68.56%Â±1.53\nFast sub-tree kernel 81.06%Â±4.22\nSimple-cycles kernel 88.50%Â±2.02\nTable 3 . Classiï¬cation rates with a 95% conï¬dence interval\nfor the idiom retrieval task.\nFigure 2 . Error bar showing the average number of simple\ncycles per song and per cycle length of our chord progres-\nsions data. 95% conï¬dence intervals are also shown.\n6. CONCLUSION AND FUTURE WORK\nIn this paper we have introduced a simple-cycle similarity\nmethod based on the harmonic progression of a song. We\nhave presented the notions of a theoretically well-founded\nmethod, and shown its applicability to our problem. This\napproach has furthermore been validated on an idiom and\na cover song retrieval task. The obtained results suggest\nthe utility of extracting repetitive sub-structures for music\nsimilarity purposes by means of a simple-cycles weighted\nkernel. Further work will try to improve the presented algo-\nrithm by performing an approximate cycle matching, and by\nreplacing labels by musical distances between chords.\n7. ACKNOWLEDGMENTS\nThis work is partially supported by the Fonds pour la formation `a\nla Recherche dans lâ€™Industrie et dans lâ€™Agriculture (F.R.I.A.) under\ngrant reference F3/5/5-MCF/ROI/BC-21716. Part of this work has\nalso been funded by projects with the â€œR Â´egion Wallonneâ€ and the\nBelgian â€œPolitique Scientiï¬que F Â´edÂ´eraleâ€. We thank these insti-\ntutions for giving us the opportunity to conduct both fundamental\nand applied research. We also thank Juan Felipe Avila from the\nUniversidad Nacional de Colombia for his clariï¬cations regarding\nmusical concepts.8. REFERENCES\n[1] A. Arenas, A. Fernandez, S. Fortunato, and S. Gomez. Motif-based\ncommunities in complex networks. Journal of Physics A: Mathemati-\ncal and Theoretical , 41:224001, 2008.\n[2] J. P. Bello. Audio-based cover song retrieval using approximate chord\nsequences: Testing shifts, gaps, swaps and beats. In Proceedings of the\n8th International Society for Music Information Retrieval Conference\n(ISMIR) , pages 239â€“244, 2007.\n[3] W. Bas de Haas, M. Rohrmeier, R. C. Veltkamp, and F. Wiering. Mod-\neling harmonic similarity using a generative grammar of tonal har-\nmony. In Proceedings of the 10th International Society for Music In-\nformation Retrieval Conference (ISMIR) , pages 549â€“554, 2009.\n[4] W. Bas de Haas, R. C. Veltkamp, and F. Wiering. Tonal pitch step dis-\ntance: a similarity measure for chord progressions. In Proceedings of\nthe 9th International Society for Music Information Retrieval Confer-\nence (ISMIR) , pages 51â€“56, 2008.\n[5] I. Deli `ege, M. M Â´elen, D. Stammers, and I. Cross. Musical schemata in\nreal time listening to a piece of music. Music Perception , 14(2):117â€“\n160, 1996.\n[6] C. Drake. Psychological processes involved in the temporal organiza-\ntion of complex auditory sequences: Universal and acquired processes.\nMusic Perception , 16(1):11â€“26, 1998.\n[7] T. G Â¨artner. Kernels For Structured Data . World Scientiï¬c Publishing,\n2009.\n[8] D. Gusï¬eld. Algorithms on strings, trees, and sequences . Cambridge\nUniversity Press, 1997.\n[9] Pierre Hanna, Matthias Robine, and Thomas Rocher. An align-\nment based system for chord sequence retrieval. In Proceedings of\nthe IEEE/ACM International Joint Conference on Digital Libraries\n(JCDL) , pages 101â€“104, 2009.\n[10] T. Horv Â´ath, T. G Â¨artner, and S. Wrobel. Cyclic pattern kernels for pre-\ndictive graph mining. In Proceedings of Knowledge Discovery and\nData mining (KDD) , pages 158â€“167, 2004.\n[11] M. Mauch, S. Dixon, C. Harte, M. Casey, and B. Fields. Discovering\nchord idioms through beatles and real book songs. In Proceedings of\nthe 8th International Society for Music Information Retrieval Confer-\nence (ISMIR) , pages 255â€“258, 2007.\n[12] F. Pachet. Surprising harmonies. In D. M. Dubois, editor, Proceedings\nof the 2nd International Conference on Computing Anticipatory Sys-\ntems, 1998.\n[13] H. Papadopoulos and G. Peeters. Joint estimation of chords and down-\nbeats from an audio signal. IEEE Transactions on Audio, Speech &\nLanguage Processing , 19(1):138â€“152, 2011.\n[14] J. Paulus, M. M Â¨uller, and A. Klapuri. Audio-based music structure\nanalysis. In Proceedings of the 11th International Society for Music\nInformation Retrieval Conference (ISMIR) , pages 625â€“636, 2010.\n[15] J. Pickens and T. Crawford. Harmonic models for polyphonic music\nretrieval. In Proceedings of the ACM International Conference on In-\nformation and Knowledge Management (CIKM) , pages 430â€“437, 2002.\n[16] R. C. Read and R. E. Tarjan. Bounds on backtrack algorithms for listing\ncycles, paths, and spanning trees. Networks , 5(3):237â€“252, 1975.\n[17] J. Shawe-Taylor and N. Cristianini. Kernel Methods for Pattern Analy-\nsis. Cambridge University Press, 2004.\n[18] N. Shervashidze and K. M. Borgwardt. Fast subtree kernels on graphs.\nInAdvances in Neural Information Processing Systems (NIPS) , pages\n1660â€“1668, 2009.\n66"
    },
    {
        "title": "Songle: A Web Service for Active Music Listening Improved by User Contributions.",
        "author": [
            "Masataka Goto",
            "Kazuyoshi Yoshii",
            "Hiromasa Fujihara",
            "Matthias Mauch",
            "Tomoyasu Nakano"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1416256",
        "url": "https://doi.org/10.5281/zenodo.1416256",
        "ee": "https://zenodo.org/records/1416256/files/GotoYFMN11.pdf",
        "abstract": "This paper describes a public web service for active music listening, Songle, that enriches music listening experiences by using music-understanding technologies based on signal processing. Although various research-level interfaces and technologies have been developed, it has not been easy to get people to use them in everyday life. Songle serves as a showcase to demonstrate how people can benefit from music-understanding technologies by enabling people to experience active music listening interfaces on the web. Songle facilitates deeper understanding of music by visualizing music scene descriptions estimated automatically, such as music structure, hierarchical beat structure, melody line, and chords. When using music-understanding technologies, however, estimation errors are inevitable. Songle therefore features an efficient error correction interface that encourages people to contribute by correcting those errors to improve the web service. We also propose a mechanism of collaborative training for music-understanding technologies, in which corrected errors will be used to improve the music-understanding performance through machine learning techniques. We hope Songle will serve as a research platform where other researchers can exhibit results of their music-understanding technologies to jointly promote the popularization of the field of music information research.",
        "zenodo_id": 1416256,
        "dblp_key": "conf/ismir/GotoYFMN11",
        "keywords": [
            "public web service",
            "active music listening",
            "music-understanding technologies",
            "signal processing",
            "music scene descriptions",
            "error correction interface",
            "collaborative training",
            "music information research",
            "research platform",
            "popularization"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nSONGLE: A WEB SERVICEFOR ACTIVEMUSIC LISTENING\nIMPROVEDBY USER CONTRIBUTIONS\nMasataka Goto, KazuyoshiYoshii,HiromasaFujihara, Matthias Mauch, and TomoyasuNakano\nNational Institute of AdvancedIndustrial Science and Technology(AIST), Japan\nABSTRACT\nThis paper describes a public web service for active mu-\nsic listening, Songle, that enriches music listening expe-\nriences by using music-understanding technologies based\non signal processing. Although various research-level in-\nterfaces and technologies have been developed, it has not\nbeeneasytogetpeopletousethemineverydaylife. Songle\nserves as a showcase to demonstrate how people can bene-\nï¬tfrommusic-understandingtechnologiesbyenablingpeo-\nple to experience active music listening interfaces on the\nweb. Songlefacilitatesdeeperunderstandingofmusicbyvi-\nsualizing music scene descriptions estimated automatically,\nsuch as music structure, hierarchical beat structure, melody\nline, and chords. When using music-understanding tech-\nnologies, however, estimation errors are inevitable. Songle\ntherefore features an efï¬cient error correction interface that\nencourages people to contribute by correcting those errors\nto improve the web service. We also propose a mechanism\nofcollaborativetrainingformusic-understandingtechnolo-\ngies, in which corrected errors will be used to improve the\nmusic-understanding performance through machine learn-\ning techniques. We hope Songle will serve as a research\nplatformwhereotherresearcherscanexhibitresultsoftheir\nmusic-understanding technologies to jointly promote the\npopularization of the ï¬eld of music information research.\n1. INTRODUCTION\nThegoalofthisresearchistoenrichmusiclisteningexperi-\nences by using music-understanding technologies based on\nsignal processing. Toward this goal, we have already de-\nveloped various active music listening interfaces [1], where\nactivemusiclisteningisawayoflisteningtomusicthrough\nactive interactions. In this research, the word activeis not\nmeanttoconveythatthelistenerscreatenewmusic,butthat\nthey take control of their own listening experience. For ex-\nample, the active music listening interface SmartMusicK-\nIOSK[2]hasachorus-searchfunctionthatenablesauserto\naccessdirectlyhisorherfavoritepartofasong(andtoskip\nothers) while viewing a visual representation of its music\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proï¬t or commercial advantage and that copies\nbear this notice and the full citation on the ï¬rst page.\ncâƒ2011 International Society for Music Information Retrieval.structure, which facilitates deeper understanding. However,\nuptonowthegeneralpublichasnothadthechanceofusing\nsuchresearch-levelinterfacesandtechnologiesindailylife.\nWe therefore developed a web service called Songlethat\nallowsanonymouswebuserstoenjoymusicbyusingactive\nmusiclisteninginterfacesonawebbrowser. Songleusesau-\ntomatic music-understanding technologies to estimate mu-\nsic scene descriptions (musical elements) [3,4] of musical\npieces (audio ï¬les) available on the web. A user of Songle\ncan enjoy playing back a musical piece while seeing the\nvisualization of the estimated descriptions. In our current\nimplementation, four major types of descriptions are auto-\nmatically estimated and visualized for content-based mu-\nsic browsing: music structure (chorus sections and repeated\nsections), hierarchical beat structure (musical beats and bar\nlines),melodyline(fundamentalfrequency(F0)ofthevocal\nmelody), and chords (root note and chord type). In partic-\nular, Songle implements all functions of the SmartMusicK-\nIOSK interface, and a user can jump and listen to the cho-\nrus with just a push of the next-chorus button. Songle thus\nmakesit easier for a user to ï¬nd desired parts of a piece.\nGiventhevarietyofmusicalpiecesontheweb,however,\nit is difï¬cult to estimate music scene descriptions with high\naccuracy. Because of the diversity of music genres, com-\nplexity of sound mixtures, and recording conditions, auto-\nmatic music-understanding technologies cannot avoid mak-\ningsomeerrors,eventhoughthetechnologiesareconstantly\nimproving. As a result, users of such a web service might\nbe disappointed by its performance.\nTo overcome this difï¬culty, Songle enables anonymous\nusers to contribute by correcting music-understanding er-\nrors. Each user can see the music-understanding visualiza-\ntions on a web browser, with a moving cursor indicating\nthe audio playback position. If a user ï¬nds an error while\nlistening, the user can easily correct the error by select-\ning from a list of candidates, or by providing an alternative\ndescription on Songleâ€™s efï¬cient error correction interface.\nThe resulting corrections are then shared and used to im-\nmediately improve the user experience with the corrected\npiece. Wealsoplantousesuchcorrectionstograduallyim-\nprove music-understanding technologies through adaptive\nmachine learning techniques, so that descriptions of other\nmusical pieces can be estimated more accurately. This ap-\nproachcanbedescribedas collaborativetrainingformusic-\nunderstandingtechnologies onthe web.\nDevelopment for Songle started in June 2009, and the\nSongle website http://songle.jp will be open to the public\n311Oral Session 4: Web\nFigure 1. Songle screen snapshot of the main interface for\nmusic playback with the visualization of music scene de-\nscriptions estimated automatically.\nbefore the ISMIR 2011 conference. In addition to the con-\ntribution of enriching music listening experiences, Songle\nwillserveasashowcaseinwhicheverybodycanexperience\nmusic-understanding technologies and understand their na-\nture: for example, what kinds of music or sound mixture\nare difï¬cult for the technologies to handle. Furthermore,\nwe hope to extend Songle so that it can serve as a research\nplatform able to support various music-understanding tech-\nnologies developedby differentresearchers.\n2. OVERVIEWOF SONGLE\nSongle is a social annotation web service where users can\nretrieve, browse, and annotate musical pieces on the web.\nFigure 1 shows the web page of the Songle interface after\na musical piece is selected. During the initial stage of the\nSongle launch we are focusing on popular songs with vo-\ncals. Songs recently released on music web services such\nas Magnatune (http://magnatune.com/) and PIAPRO1are\nadded (registered) to Songle. A user can also register any\nsongavailableonthewebbyprovidingtheURLofitsMP3\nï¬le, the URL of a web page including multiple MP3 URLs,\nor the URL of a music podcast (an RSS syndication feed\nincluding multiple MP3 URLs).\nEverybody can enjoy active music listening and correct\nerrors as an anonymous user without logging in, but a user\nhas to log in with OpenID to register a new song. In addi-\n1PIAPRO (http://piapro.jp/) is a web service to which musicians can\nupload their ownsongs created using singing synthesizers.tion,alogged-inusercangenerateaplaylist. Suchaplaylist\nhas a title (theme) and the user can choose whether to share\nit with other users. A logged-in user can also enter and\nrecordapreference(likeordislike)foreachsongtogetbet-\nter song recommendations in the future.\nSongle supports three main functions: retrieving, brows-\ning,andannotatingsongs. Theretrievalandbrowsingfunc-\ntionsfacilitatedeeperunderstandingofmusic,andtheanno-\ntation (error correction) function allows users to contribute\nto improve music scene descriptions. These improved de-\nscriptions can then lead to a better user experience of re-\ntrievingand browsingsongs.\n2.1 RetrievalFunction\nThis is a function that enables a user to retrieve a song\nthrough a text search of the song title or artist name, or\nthrough selection from a list of artists or a list of songs\nwhose descriptions were recently estimated or corrected.\nThis function also shows various kinds of ranking, such as\nartist ranking, song ranking, and user ranking (by the num-\nber of corrected errors).\nFollowingtheideaoftheactivemusiclisteninginterface\nVocalFinder [5], which ï¬nds songs with similar vocal tim-\nbres, a similarity graph of songs is also visualized so that\na user can retrieve a song according to vocal timbre simi-\nlarity. The graph is a radially-connected network in which\nnodes (songs) of similar vocal timbre are connected to the\ncenter node (a recommended or user-speciï¬ed song). By\ntraversingagraphwhilelisteningtonodes,ausercanï¬nda\nsong havingthe favoritevocaltimbre.\nA user can play back a song in any list of songs (e.g.,\nplaylist, retrieved list, and ranking) for trial listening to\njudgewhetheritisofinterest. Byselectingoneofthesongs,\nthe user switches over to the next browsing function. While\nusing the browsing function, songs in the playlist are au-\ntomatically switched one after another if a user selects and\nlistens to a playlist.\n2.2 Within-songBrowsingFunction\nThis is a function that provides a content-based playback-\ncontrol interface for within-song browsing as shown in the\nupperhalfofFigure 1. Theupperwindowistheglobalview\nshowing the entire song and the lower window is the local\nviewmagnifying the selected region.\nWith this function, a user can view the following four\ntypes of music scene descriptions estimated automatically:\n1.Music structure(chorussections and repeatedsections)\nIn the global view, the music map of the SmartMusicK-\nIOSK interface [2] is shown below the playback controls\nincluding the buttons, time display, and playback slider.\nThe music map is a graphical representation of the entire\nsong structure consisting of chorus sections (the top row)\nand repeated sections (the ï¬ve lower rows). On each row,\ncolored sections indicate similar (repeated) sections. This\nmap helps a user decide where to jump. Clicking directly\n31212th International Society for Music Information Retrieval Conference (ISMIR 2011)\n(a) Correcting music structure\n(chorus sections and repeated sections)\n(b)Correcting hierarchical beat structure\n(musicalbeats and bar lines)\n(c) Correcting melody line (F0 of the vocalmelody)\n (d)Correcting chords (root note and chord type)\nFigure2. Songle screen snapshots of the annotation function for correcting music scene descriptions.\nonacoloredsectionplaysthatsection. Therearealsobut-\ntons for jumping to the next or previous chorus sections,\nandthe nextor previousrepeated sections.\n2.Hierarchicalbeatstructure(musicalbeatsandbarlines)\nAtthebottomofthelocalview,musicalbeatscorrespond-\ningtoquarternotesarevisualizedbyusingsmalltriangles.\nThe top of each triangle indicates its temporal position.\nBarlines are markedby largertriangles.\n3.Melody line (F0 of the vocal melody)\nThe piano roll representation of the melody line is shown\nabove the beat structure in the local view. It is also shown\nin the lower half of the global view. For simplicity, the\nfundamental frequency (F0) can be visualized after being\nquantizedto the closest semitone.\n4.Chords(rootnote and chordtype)\nChord names are written in the text at the top of the local\nview. Twelve different colors are used to represent twelve\ndifferent root notes so that a user can notice the repetition\nofchord progressions.\nIn the lower half of Figure 1, a user can add and share\nsocialtagsandtime-synchronouscommentsfor CrowdMu-\nsic Listening [6]. Clicking on a time-synchronous comment\nstarts playback from that position.\n2.3 Annotation (ErrorCorrection)Function\nThis function allows users to add annotations to correct any\nestimation errors they may come across while listening tomusic. Here, annotation means describing the contents of a\nsong, either by modifying the estimated descriptions or by\nselectingthecorrectcandidateifavailable. Forthispurpose,\nwe provide an efï¬cient error correction interface (editor) as\nshownin Figure 2.\nEditorsforfourtypesofmusicscenedescriptionscanbe\nswitched between in the local view.\n1.Music structure (Figure2(a))\nThe beginning and end points of every chorus or repeated\nsection can be adjusted. It is also possible to add, move,\nordeleteeachsection. Repeatedsectionscanserveascan-\ndidates for the chorus sections, but since it is not obvious\nhowtocorrectrepeatedsections,weplantoletausertype\ninasectionlabel(e.g.,verseA,verseB,etc.) oneachrow.\nThiscorrectionfunctionimprovestheSmartMusicKIOSK\nexperience.\n2.Hierarchicalbeat structure (Figure2(b))\nSeveral alternativecandidates for the beat structure can be\nselected at the bottom of the local view. If none of the\ncandidates are useful, a user can enter the beat position by\ntapping a key during music playback. Each beat position\nor bar line can also be changed directly. For ï¬ne adjust-\nment it is possible to play back the audio with click tones\natbeats.\n3.Melody line (Figure2(c))\nSongle allows note-level correction on the piano roll rep-\n313Oral Session 4: Web\nMusical piece \n(MP3 music file) Musical piece \n(MP3 music file) Speech recognizer Speech recognizer Database manager \nWeb server Speech recognizer Speech recognizer Music-understanding module Musical piece \n(MP3 music file) Musical piece \n(MP3 music file) \nWeb crawler Music-understanding manager User interface \nUser interface User interface Figure3. Implementation overviewof Songle.\nresentationofthemelodyline. Sincethemelodylineisin-\nternallyrepresentedasthetemporaltrajectoryofF0,more\nprecise correction is also possible by choosing from F0\ncandidates. Ausercanlistentothemelodylineonlyorthe\nmelody-cancelled background playback. More accurate\nmelody annotations will lead to better similarity graphs of\nsongs.\n4.Chords(Figure2(d))\nChord names can be corrected by choosing from candi-\ndates or by explicit typing of chord names. Each chord\nboundarycanalsobeadjusted. Chordscanbeplayedback\nalong with the original song to make it easier to check the\ncorrectness.\nNote that users can simply enjoy active music listening\nwithout correcting errors. We understand that it is too difï¬-\ncult for some users to correct the above descriptions (espe-\ncially, chords). Designing an interface that makes it easier\nfor them to correct will be another future challenge. More-\nover, users are not expected to correct all errors, only some\naccording to each userâ€™sinterests.\nWhen the music-understanding results are corrected by\nusers, the original values are visualized as trails with dif-\nferent colors (white, gray, or yellow marks in Figure 2)\nthat can be distinguished by anybody. These trails\nare important to prevent overestimation of the automatic\nmusic-understandingperformanceaftertheusercorrections.\nMoreover, all the correction histories are recorded, and de-\nscriptions before and after corrections can be compared.\n3. IMPLEMENTATIONOF SONGLE\nThe implementation overview of Songle is shown in\nFigure 3. The web crawler collects musical pieces (MP3\nï¬les) on the basis of their URLs and RSS feeds, which can\nbe added by users, and stores the pieces in the database.\nSeveralmusic-understanding modules , each corresponding\nto a particular type of music scene description, then pro-\ncesseachmusicalpiece. Forexample,thebeatstructureand\nthe music structure are estimated by two different modules.\nWhen a request from an idle music-understanding module\nis received by the music-understanding manager , the next\navailable musical piece lacking an estimation result for the\ncorresponding description is handed over. After the music-\nunderstanding module ï¬nishes processing its piece, the es-\ntimation result is passed to the database manager via the\nmusic-understanding manager. The database manager con-\ntrols the processing state of the musical pieces and stores\ntheir estimation results in a database together with the cor-\nrections when provided by users. Finally, the web serverworks as a website that provides the Songle user interface ,\nwhich directly plays back the MP3 ï¬le from the original\nURL.\nThe web server of Songle was implemented by using a\nweb application framework Ruby on Rails , a programming\nlanguage Ruby, a web server Passenger andApache, and a\ndatabaseMySQL.Theclientuserinterfacewasimplemented\nby using a scripting language ActionScript 3 , an Action-\nScript3compiler AdobeFlexCompiler ,andascriptinglan-\nguageJavaScript .\nMusicscene descriptions are estimated as follows.\n1.Music structure\nChorussectionsandrepeatedsectionsareestimatedbyus-\ningthechorus-sectiondetectionmethod RefraiD[2]which\nfocuses on popular music. By analyzing relationships be-\ntween various repeated sections, the RefraiD method can\ndetect all the chorus sections in a song and estimate both\nends of each section. It can also detect modulated chorus\nsections.\n2.Hierarchicalbeat structure\nThe beats are estimated using a hidden Markov model\n(HMM) with 43 tempo states, each having 18 to 60 sub-\nstates corresponding to the beat phase of different tempi.\nIn each tempo a beat is modeled as a left-to-right HMM\nin which only some states have non-deterministic transi-\ntion probabilities to allow for tempo ï¬‚uctuations or tempo\nchanges. The emission probability of a sub-state is calcu-\nlated via the cosine similarity between a comb ï¬lter and a\nsimple onset detection function. Five different comb-ï¬lter\nshapes are used to output ï¬ve different beat-tracking re-\nsults, including those that are likely true candidates if the\ndefaultalgorithmtrackstheback-beatorbeatsattwicethe\ntrue tempo. This strategy maximizes the likelihood of of-\nferingthe beat-tracking result desired by the user.\nThebarlinesareestimatedusingharmoniccues. First,we\nextract tatum-synchronous bass and treble chromagrams\nusing NNLS Chroma [7]. We build a simple chord detec-\ntion model and calculate posterior probabilities of chord\nchanges. Using a sliding window, we compute the cosine\nsimilaritybetweenthechordchangeprobabilitiesandsev-\neral different bar patterns that cover the 3/4, 4/4 and 6/8\nmeters and all possible bar phases. We normalize the co-\nsine similarities at each frame and use them as emissions\ninanother HMM similar to the beat-tracking model.\n3.Melody line\nThe fundamental frequency (F0) of the vocal part is es-\ntimated by using the F0 estimation method for the vo-\ncal melody [8], which is implemented by extending the\npredominant-F0 estimation method PreFEst [4]. This\nmethod focuses only on the vocal melody by evaluating a\nGMM-based vocal probability for each F0 candidate esti-\nmated by PreFEst. Moreover, vocal activity detection was\nimplementedby using a method described in [9].\n4.Chords\nWe transcribe chords using 14 chord types: major, ma-\njor 6th, major 7th, dominant 7th, minor, minor 7th, half-\n31412th International Society for Music Information Retrieval Conference (ISMIR 2011)\ndiminished, diminished, augmented, and ï¬ve variants of\nmajor chords with different bass notes: /2, /3, /5, /b7, and\n/7. The resulting 14 types Ã—12 root notes = 168 chords\nand one â€˜no chordâ€™ label are estimated using an HMM ap-\nproach on the same tatum-synchronous chromagram used\nfor the bar-line estimation. Chord changes are allowed to\nhappenonly on beats.\nWe also include knowledge from the bar-line estimation\nand a key estimate. Instead of building one large model\nin which chords and keys are modeled simultaneously\n(e.g. [10]), we chose a less memory-intensive, progressive\napproach. First, we model the key in a simple separate\nHMM with three different key scales: major, natural mi-\nnor, and harmonic minor. Every key state has observation\nprobabilities for all different chords, based on an expert\nfunction [10]. The posterior probability obtained from the\nHMMisthenusedtoweightthechordprobabilitiesforthe\nchord HMM. During Viterbi decoding we use the bar-line\nestimates for dynamic transition probability weighting in\norderto encourage chord changes at bar lines.\nTo show the similarity graph of songs, the vocal timbre\nsimilarity between songs is calculated by using the method\nused in VocalFinder [5]. The current recommendation of\nsongs simply uses the same similarity, which shall be im-\nprovedin the future.\n4. DISCUSSION\nWhile research dealing with a public web service for active\nmusic listening and social annotation has not been pursued\nin the past, there have been various approaches related to\nmusic annotation. In the following, we introduce related\nwork and then discuss how Songle could contribute to soci-\nety and academic research.\n4.1 Related Research\nSeveral approaches have been proposed to collect a large\namount of ground-truth annotations. Such annotations\nare useful for improving the accuracy of music informa-\ntion retrieval (MIR) systems using machine learning tech-\nniques, and for evaluating MIR systems [11]. For exam-\nple,Lee[12]usedawebservicecalledAmazonMechanical\nTurk (MTurk) to ask people to make similarity judgments.\nMandeletal.[13]alsousedMTurkfortagcollection. These\napproaches showed that the quality of the collected anno-\ntations was sufï¬ciently high. However, the human effort\nnecessary increases in proportion to the required number of\nannotations.\nTo solve this problem, it is interesting to let non-experts\ncontributetomakingground-truthannotations. Onepromis-\ning approach is based on games. For example, Turnbull et\nal.[14,15]proposedanannotationgame,whereplayersare\nasked to choose the most and least suitable descriptions for\na given musical piece. Mandel and Ellis [16] proposed an-\notherannotationgame,whereplayerscangetpointsbypro-\nviding useful descriptions that were not provided by other\nplayers. Law et al.[17] proposed another annotation gamebasedontheESPGame[18]inwhichplayersarenotasked\nto describe a sound, but told to guess what their randomly\npairedpartnersarethinking. Songleprovidesastrongermo-\ntivationforuserstocontributethantheserelatedapproaches\nbecause the user is awareof improving the service for other\nusers.\nTo annotate musical pieces, various useful editors have\nbeendeveloped,suchasSonicVisualiser[19],Audacityex-\ntension [20], CLAM [21], and MUCOSA [22]. The Echo\nNest API (http://developer.echonest.com/) is also useful for\naccesstovariousannotations. Songleistheï¬rstsystemthat\nallows anonymous users to collaborate to edit various mu-\nsic scene descriptions (chorus, beats, melody, and chords)\ndirectly on the web without stand-alone applications.\n4.2 Contributionsof Songle\nSongle makes a social contribution by providing the\nworldâ€™s ï¬rst public web service for enjoying active mu-\nsic listening interfaces with music-understanding technolo-\ngies. It also promotes the popularization and use of\nmusic-understanding technologies by raising user aware-\nness. Users can grasp the nature of music-understanding\ntechnologies just by seeing results of the technologies ap-\nplied to songs available on the web. When there are many\nerrors, we run the risk of attracting criticism, but we be-\nlieve that sharing these results with users will promote fur-\nther popularization of this research ï¬eld.\nThe academic contribution of this study is to propose a\nnewresearchapproachtomusicunderstandingbasedonsig-\nnal processing; this approach aims at improving both the\nmusic-understanding performance and the usage rate while\nbeneï¬ting from the cooperation of anonymous end users.\nThisapproachisdesignedtosetintomotiona positivespiral\nwhere (1) we enable users to experience a service based on\nmusicunderstandingtoletthembetterunderstanditsperfor-\nmance, (2) users contribute to improved performance, and\n(3) the improved performance leads to a better user expe-\nrience, which encourages further use of the service at step\n(1) of this spiral. This is a social correction framework,\nwhere users can improve the performance by sharing their\ncorrection results over a web service. The game-based ap-\nproach of Human Computation or GWAPs (games with a\npurpose) [23] like the ESP Game [18] often lacks step (3)\nand depends on the feeling of fun. In this framework, users\ngain a real sense of contributing for their own beneï¬t and\nthat of others and can be further motivated to contribute by\nseeing corrections made by other users. In this way, we can\nusethewisdomofcrowds orcrowdsourcing toachieveabet-\nter user experience.\nAnother important technical contribution of this study\nis to investigate how far the performance of music-\nunderstanding technologies can be improved by getting er-\nrors corrected through the cooperative efforts of users. Al-\nthough we have not yet implemented a machine-learning\nmechanismtoimprovetheperformanceonthebasisofuser\ncorrections,wecouldimplementsuchamechanismoncewe\n315Oral Session 4: Web\nhavecollectedenoughcorrections. Thisstudywillthenpro-\nvide a framework for amplifying user contributions in mu-\nsic research. In a typical Web 2.0service like Wikipedia ,\nimprovements are limited to an item directly contributed\n(edited) by users. In Songle, improvements will automat-\nically spread to other songs because of the improvement\nof the music-understanding technologies through machine\nlearningtechniques. Thiswillbeanoveltechnologyofam-\nplifyingusercontributions,whichcouldbebeyondWeb2.0\nand Human Computation [23]. We hope that this study will\nshowtheimportanceandpotentialofincorporatingandam-\nplifying user contributions in music research, as have been\ndemonstrated by PodCastle in speech research [24,25].\nWe think we can trust users with respect to the quality\nof correction according to our experiences from PodCas-\ntle[25]. Evenifsomeusersdeliberatelymakeinappropriate\ncorrections (the vandalism problem), we will be able to de-\nvelop countermeasures to acoustically evaluate the reliabil-\nity of corrections. For example, we could validate whether\nthecorrecteddescriptionscanbesupportedbyacousticphe-\nnomena. This will be another interesting topic of research.\n4.3 Songle as a ResearchPlatform\nIn the future, we hope to extend Songle to serve as a re-\nsearchplatformwhereotherresearcherscanalsoexhibitre-\nsults of their own music-understanding technologies. When\neach technology is implemented as a music-understanding\nmodulein Figure 3, which can be executed anywhere in the\nworld even in our current implementation, it is not neces-\nsary to share its source and binary codes. Each in-house\nmodule, even inside an Internet ï¬rewall, can just connect\nto themusic-understanding manager to receive an audio\nï¬le and send back music-understanding results via HTTP.\nTheresultsshouldalwaysbeshownwithclearacknowledg-\nments/credits so that users can distinguish the sources. We\nare also interested in adding other types of music scene de-\nscriptions, which are not yet supported.\nOnce this happens, it will be interesting to com-\npare/visualize differences of modules on each music scene\ndescription. Results from different researchers can also be\nusedascandidatesforthecorrectionandevenasvotestolet\ndifferentresults converge.\n5. CONCLUSION\nWehavedescribedSongle,anactivemusiclisteningservice\nthat is continually improved by user contributions. In our\ncurrent implementation, four types of music scene descrip-\ntionsareestimatedandexposedthroughweb-basedinterac-\ntive user interfaces. Since automatic music-understanding\ntechnologies are not perfect, Songle allows users to make\nerror corrections, which are shared with other users, thus\ncreating a positive spiral and an incentive to keep correct-\ning. FortheMIRcommunitythisplatformwillactbothasa\ntest-bed or showcase for new technologies, and as a way of\ncollecting valuableannotations.ACKNOWLEDGMENTS: We thank Utah Kawasaki for\nthewebserviceimplementationandMinoruSakuraiforthe\nweb design of Songle. This work was supported in part by\nCrestMuse, CREST,JST.\n6. REFERENCES\n[1] M.Goto,â€œActivemusiclisteninginterfacesbasedonsignalprocess-\ning,â€inProc.of ICASSP 2007 ,2007.\n[2] M.Goto,â€œAchorus-sectiondetectionmethodformusicalaudiosig-\nnals and its application to a music listening station,â€ IEEE Trans. on\nASLP, vol.14, no. 5, pp. 1783â€“1794, 2006.\n[3] M. Goto, â€œMusic scene description project: Toward audio-based\nreal-timemusicunderstanding,â€in Proc.ofISMIR2003 ,pp.231â€“232,\n2003.\n[4] M. Goto, â€œA real-time music scene description system:\nPredominant-F0 estimation for detecting melody and bass lines\nin real-world audio signals,â€ Speech Communication , vol. 43, no. 4,\npp.311â€“329, 2004.\n[5] H. Fujihara et al., â€œA modeling of singing voice robust to accom-\npanimentsoundsanditsapplicationtosingeridentiï¬cationandvocal-\ntimbre-similarity-based music information retrieval,â€ IEEE Trans. on\nASLP, vol.18, no. 3, pp. 638â€“648, 2010.\n[6] M. Goto, â€œMusic listening in the future: Augmented Music-\nUnderstandingInterfacesandCrowdMusicListening,â€in Proc.ofthe\nAES42nd International Conf.on Semantic Audio ,pp. 21â€“30, 2011.\n[7] M. Mauch and S. Dixon, â€œApproximate note transcription for the\nimproved identiï¬cation of difï¬cult chords,â€ in Proc. of ISMIR 2010 ,\npp.135â€“140, 2010.\n[8] H. Fujihara et al., â€œF0 estimation method for singing voice in\npolyphonic audio signal based on statistical vocal model and Viterbi\nsearch,â€in Proc.of ICASSP 2006 ,pp. Vâ€“253â€“256,2006.\n[9] H. Fujihara et al., â€œAutomatic synchronization between lyrics and\nmusic CD recordings based on Viterbi alignment of segregated vocal\nsignals,â€in Proc.of ISM 2006 ,pp. 257â€“264, 2006.\n[10] M. Mauch and S. Dixon, â€œSimultaneous estimation of chords and\nmusical context from audio,â€ IEEE Trans. on ASLP , vol. 18, no. 6,\npp.1280â€“1289, 2010.\n[11] J.S.Downie,â€œThemusicinformationretrievalevaluationexchange\n(2005â€“2007): A window into music information retrieval research,â€\nAcousticalScience and Technology , vol.29, pp. 247â€“255, 2008.\n[12] J. H. Lee, â€œCrowdsourcing music similarity judgments using Me-\nchanicalTurk,â€in Proc.of ISMIR 2010 ,pp. 183â€“188, 2010.\n[13] M.I.Mandel,D.EckandY.Bengio,â€œLearningtagsthatvarywithin\nasong,â€in Proc.of ISMIR 2010 ,pp. 399â€“404, 2010.\n[14] D. Turnbull et al., â€œA game-based approach for collecting semantic\nannotationsof music,â€in Proc.of ISMIR 2007 ,pp. 535â€“538, 2007.\n[15] D. Turnbull, L. Barrington and G. Lanckriet, â€œFive approaches to\ncollectingtagsformusic,â€in Proc.ofISMIR2008 ,pp.225â€“230,2008.\n[16] M. I. Mandel and D. P. W. Ellis, â€œA Web-based game for collecting\nmusicmetadata,â€in Proc.of ISMIR 2007 ,pp. 365â€“366, 2007.\n[17] E.L.M.Law etal.,â€œTagATune: Agameformusicandsoundanno-\ntation,â€in Proc.of ISMIR 2007 ,pp. 361â€“364, 2007.\n[18] L. von Ahn and L. Dabbish, â€œLabeling images with a computer\ngame,â€in Proc.of CHI 2004 ,pp. 319â€“326, 2004.\n[19] C. Cannam et al., â€œThe Sonic Visualiser: A visualisation platform\nfor semantic descriptors from musical signals,â€ in Proc. of ISMIR\n2006,pp. 324â€“327, 2006.\n[20] B. Li, J. A. Burgoyne and I. Fujinaga, â€œExtending Audacity as a\ngrouth-truth annotation tool,â€ in Proc. of ISMIR 2006 , pp. 379â€“380,\n2006.\n[21] X. Amatriain et al., â€œThe CLAM annotator: A cross-platform audio\ndescriptorsediting tool,â€in Proc.of ISMIR 2005 , pp. 426â€“429, 2005.\n[22] P. Herrera et al., â€œMUCOSA: A music content semantic annotator,â€\ninProc.of ISMIR 2005 ,pp. 77â€“83, 2005.\n[23] L. von Ahn, â€œGames with a purpose,â€ IEEE Computer Magazine ,\nvol.39, pp. 92â€“94, June 2006.\n[24] M. Goto, J. Ogata and K. Eto, â€œPodCastle: A Web 2.0 approach to\nspeechrecognition research,â€in Proc.of Interspeech2007 ,2007.\n[25] M. Goto and J. Ogata, â€œPodCastle: Recent advances of a spoken\ndocument retrieval service improved by anonymous user contribu-\ntions,â€in Proc.of Interspeech2011 , 2011.\n316"
    },
    {
        "title": "An Interactive System for Electro-Acoustic Music Analysis.",
        "author": [
            "SÃ©bastien Gulluni",
            "Slim Essid",
            "Olivier Buisson",
            "GaÃ«l Richard"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1416134",
        "url": "https://doi.org/10.5281/zenodo.1416134",
        "ee": "https://zenodo.org/records/1416134/files/GulluniEBR11.pdf",
        "abstract": "This paper, presents an interactive approach for the analysis of electro-acoustic music. An original classification scheme is devised using relevance feedback and active-learning segment selection in an interactive loop. Validation and correction information given by the user is injected in the learning process at each iteration to achieve more accurate classification. An experimental study is conducted to evaluate and compare the different classification and relevance feedback approaches that are envisaged, using a database of polyphonic pieces (with a varying degree of polyphony). The results show that the different approaches are adapted to different applications and they achieve satisfying performance in a reasonable number of iterations.",
        "zenodo_id": 1416134,
        "dblp_key": "conf/ismir/GulluniEBR11",
        "keywords": [
            "interactive approach",
            "electro-acoustic music",
            "classification scheme",
            "relevance feedback",
            "active-learning segment selection",
            "user feedback",
            "learning process",
            "experimental study",
            "polyphonic pieces",
            "performance"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nAN INTERACTIVE SYSTEM FOR ELECTRO-ACOUSTIC MUSIC ANALYSIS\nSÂ´ebastien Gulluni, Olivier Buisson\nInstitut National de lâ€™Audiovisuel\nBry-sur-marne, France\n{sgulluni,obuisson }@ina.frSlim Essid, Ga Â¨el Richard\nInstitut Telecom, Telecom ParisTech,\nParis, France\n{slim.essid,gael.richard }@telecom-paristech.fr\nABSTRACT\nThis paper, presents an interactive approach for the analysis\nof electro-acoustic music. An original classiï¬cation scheme\nis devised using relevance feedback and active-learning seg-\nment selection in an interactive loop. Validation and correc-\ntion information given by the user is injected in the learning\nprocess at each iteration to achieve more accurate classiï¬-\ncation. An experimental study is conducted to evaluate and\ncompare the different classiï¬cation and relevance feedback\napproaches that are envisaged, using a database of poly-\nphonic pieces (with a varying degree of polyphony). The\nresults show that the different approaches are adapted to dif-\nferent applications and they achieve satisfying performance\nin a reasonable number of iterations.\n1. INTRODUCTION\nBeing composed directly with the â€œsound materialâ€ using\nrecording techniques [18], electro-acoustic music differs from\nother more conventional musical forms. Composers of the\ngenre do not use score sheets to write music and there is\nno common agreement on a standard notation system to be\nused to create symbolic representations for such composi-\ntions. Electro-acoustic music is traditionally organized in\nsound objects . Here, we deï¬ne â€œsound objectâ€ as any sound\nevent perceived as a whole [18]. Most of the time a musi-\ncal piece does not expose separate sound objects as simul-\ntaneous sounds are masking each others due to polyphony.\nConsequently, the analysis of this music is quite complex\nand totally user-centered as it is essentially concerned with\nthe subjective identiï¬cation of sound objects of interest to\nthe user. The reader can refer to [1] for examples of electro-\nacoustic compositions.\nThis work presents an interactive classiï¬cation system\nfor electro-acoustic music analysis using relevance feedback.\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proï¬t or commercial advantage and that copies\nbear this notice and the full citation on the ï¬rst page.\nc/circlecopyrt2011 International Society for Music Information Retrieval.In previous works, relevance feedback has been widely used\nin content-based image retrieval tasks (see [4] for an overview).\nBy contrast, in the ï¬eld of music information retrieval, rele-\nvance feedback and active learning have only been exploited\nin a few music information retrieval studies, for pop mu-\nsic retrieval based on user preferences [11] or mood/style\nclassiï¬cation [15]. More closely related works in this ï¬eld\nhave focused on â€œstandardâ€ instruments and percussion tim-\nbre classiï¬cation [7, 8, 14] by building supervised systems\nbased on large databases. In the electro-acoustic case, com-\nposers exploit various sound sources and one does not have\na-priori knowledge about these sources which are most of\nthe time polyphonic and heterogeneous.\nIn this paper, following our previous works [9, 10], we\npropose a complete system for electro-acoustic music anal-\nysis, and evaluate and compare different relevance feedback\napproaches to our problem. The initialisation of the sys-\ntem is achieved through an interactive segmentation phase\n(mostly similar to [9]) to obtain initial texture segments (see\nFigure 1 and 2). Then, these segments are processed by an\ninteractive classiï¬cation module using relevance feedback\nand active learning segment selection. From a userâ€™s point\nof view, the search for a target sound object begins with the\nselection of a characteristic segment for each sound class.\nThen, the system enters in an interaction loop and suggests,\nat each iteration, segments to be annotated by the user so as\nto make learning progress. On each new proposed segment,\nthe user can correct the systemâ€™s label prediction. The inter-\naction loop ends when the user is satisï¬ed with the labels.\nWe compare different classiï¬cation and relevance feedback\napproaches for different degrees of polyphonic complexity.\nThis study shows that different methods are more adapted to\ndifferent applications.\nThe paper is organized as follows: Section 2 presents\nthe musical motivations and the results of musicologistsâ€™ in-\nterviews that were carried out to acquire prior knowledge\non their approach to the analysis of electro-acoustic mu-\nsic. Section 3 describes the interactive system including the\nuser interaction scenario and active learning segment selec-\ntion strategy. Section 4 is dedicated to the evaluation of the\nmethod and the last section suggests some conclusions.\n145Poster Session 1\n2. MUSICAL MOTIV ATIONS\nOur work attempts to address musicologistsâ€™s need for new\ntools for the analysis of non written music. Thus, for a\nbetter understanding of their expectations, interviews were\nheld with three musicologists with a special expertise in\nelectro-acoustic music analysis. The questions were about\ntheir personal methodology for analysis and the utility of\ncomputer-based sound analysis tools to their work. By ana-\nlyzing their answers, some common habits can be identiï¬ed\nin their methodologies. Of note is the fact that they always\nlisten to the whole piece from 4 to 10 times to locate promi-\nnent sound objects and build a viewpoint to begin the anal-\nysis. Another common habit is to listen to the same piece\nseveral times and focus on one sound category at each time.\nIn all the interviews, the musicologists approach the analy-\nsis as a sound object transcription task . For some of them,\nthe transcription helps forming a viewpoint of the piece be-\ning analysed, whereas the others already have one when they\nbegin the transcription. All the subjects mentioned that they\ndo not transcribe all sound objects of the piece but only those\nwhich are useful for their personal analysis viewpoint.\nFor the question about the utility of computer-based tools,\nthey expressed some wishes which are all related to the sound\nobject transcription . The ï¬rst was to locate the main sound\nobjects of the piece and help them verify their transcription.\nAnother important wish was to ï¬nd all the instances of one\nsound object by giving a segment of the target sound to the\ntool. This function could also help them to discover sound\ninstances that they did not notice.\nThis work takes those musical motivations into account\nand proposes an interactive system for helping musicolo-\ngists in the transcription task.\n3. INTERACTIVE CLASSIFICATION SYSTEM\nIn this section, we describe all the aspects of the system in-\ncluding the expression of the user point of view.\n3.1 Architecture\nFigure 2 (A) is a representation of a polyphonic piece which\ninvolves potential sound masking: the distinct sound lay-\ners are arranged in parallel timelines (one for each sound\nclass). The goal of the transcription is to mark the pres-\nence of all target sounds in the whole piece. The classiï¬ca-\ntion operates on texture segments ,i.e.temporal fragments\nof homogeneous timbre (as shown with vertical red lines in\nFigure 2). The system architecture is divided in two dis-\ntinct parts: the initialisation and the interaction loop which\nperforms the classiï¬cation of the texture segments and asks\nfeedback from the user. We compare two different inter-\naction loop approaches in this work. The ï¬rst approach is\nmulti-pass : the interaction loop focuses on one sound class\nInitial SegmentsValidated SegmentsPredicted SegmentsAll SegmentsUser\nClassiï¬erActive SelectionPredicted SegmentsValidated SegmentsPredicted Segments\nSelected Segment Validation RequestValidation/Correction\nInteractive LoopLearningClassiï¬er ParametersValidated SegmentsInter Onset Segments\nInteractive  Clustering\nOnset SegmentationSlider MoveSegment selectionSegment selectionInitialisationFigure 1 . Overview of the interactive system\nat each pass, following the habits of the musicologists who\nare used to listen to the same piece several times and fo-\ncus on one sound category for one listening (see Section 2).\nThe other approach is referred to as one-pass : the interac-\ntion loop considers all the sound objects simultaneously at\neach time and consequently the user feedback applies to all\nthe classes of interest.\nThe interactions of the user with the system can be sum-\nmarized as follows:\n1. Initialisation\n(a) The system starts with an interactive segmenta-\ntion phase. If the user is transcribing Nclasses,\nfor each class Cia characteristic segment Siis\nassociated with iâˆˆ{1...N}(see Figure 2). In\norder to obtain the initial characteristic segments\nof all the sound classes corresponding to the userâ€™s\npoint of view, in this ï¬rst interaction phase, the\nuser moves a slider which controls the global\nsegmentation level until the most adapted seg-\nmentation is reached. Texture segments are cre-\nated from this segmentation.\n(b) The user selects a characteristic texture segment\nSifor each target sound class\n2. Interaction loop\n(a) The system learns from the validated segments\nand enters in the classiï¬cation process to auto-\n14612th International Society for Music Information Retrieval Conference (ISMIR 2011)\nmatically predict labels for the remaining parts\nof the signal.\n(b) In order to improve the previous classiï¬cation,\nthe system selects a segment, based on the active\nlearning strategies described in Section 3.3.5, and\nasks feedback from the user. In the multi-pass\napproach, the system predicts the presence/absence\nof the current target class and the user validates\nor corrects the selected segments prediction. In\ntheone-pass approach, the user corrects the pres-\nence/absence prediction of all the target classes\nfor the selected segment.\n(c) In the multi-pass approach, one vs all classiï¬-\ncation and feedback ((a) and (b)) iterations pro-\nceed until the user is satisï¬ed with the result for\nthe current class, before entering a new pass,\nthat is a new interaction loop, for the next classes,\nuntil all classes have been covered. In the one-\npass approach all classes are considered jointly\nfrom the very beginning of the interaction loop\nand the system iterates multi-class classiï¬cation\nand feedback until the user is satisï¬ed with the\noverall prediction.\nFigure 2 . Time-line representation of a polyphonic piece\nwith 3 sound classes and the characteristic segments of the\ntarget classes. Though the distinct sound layers are here dis-\nplayed in parallel time lines (A), in real situations the user\ncan actually only see the ï¬nal mix made by the composer\nthat appears as a single track (B). The initial user selection\nand subsequent validations are done by listening.\nA total of 217 feature coefï¬cients are extracted from 25\nclassic audio descriptors on 20 ms windows with 50% over-\nlap, to be used both for the initial segmentation and the sub-\nsequent classiï¬cation. The reader can refer to [6, 17] for a\ncomplete description of the features. All the feature vec-\ntors used and the corresponding dimensions are listed in the\nwebsite of the paper1. Feature extraction was performed\n1http://www.tsi.enst.fr/ Ëœgulluni/ismir2k11/using the YAAFE software [16].\n3.2 Interactive Clustering\nThe goal of the clustering is to obtain a segmentation adapted\nto the usersâ€™ point of view as described in the initialisation\nparagraph of section 3.1. The reader can refer to [9] for a\ndetailed explanation of the initial clustering. First, onset de-\ntection is performed and the resulting detection function is\nused to obtain inter-onset segments . Subsequently, a clus-\ntering is performed on the inter-onsets vectorsXjwith an\nagglomerative hierarchical approach to obtain texture seg-\nments. The number of target clusters of the algorithm is\ncontrolled by the user in the interface with a slider to obtain\nan adapted segmentation.\n3.3 Classiï¬cation\nIn this system, the classiï¬cation task consists in detecting\nthe presence of given sound classes in every texture seg-\nment of the musical piece. Support Vector Machine (SVM)\nclassiï¬ers [2] with probabilistic outputs2are used in a â€œone\nvs allâ€ fashion. Three different methods are compared to\nobtain the ï¬nal prediction: a multi-pass approach and two\nvariants of a one-pass approach.\n3.3.1 Feature Selection\nAfter the initialisation phase, a feature selection based on\nthe Fisher discriminant [5] is performed. The algorithm iter-\natively selects the attributes which maximize the Fisher dis-\ncriminant and the dbest features are kept to deï¬ne the fea-\nture space for the target class. The parameter dwas exper-\nimentally determined using a separate database and a value\nofd= 10 has been found to be an appropriate trade-of be-\ntween performance and complexity. The goal of the selec-\ntion is to create a relevant descriptor for each sound class.\nAs this selection is part of the interaction loop , the sound\ndescriptors may evolve accordingly with the user feedback.\nThis method is adapted to our problem since we do not have\nprior knowledge on the sound sources.\n3.3.2 Multi-pass (MP)\nIn this approach, the Nsound classes are treated sequen-\ntially: the user tries to spot all occurrences of the current\nclassCibefore beginning the next class. This enables the\nuser to focus on one sound category at each time following\nthe habits described in Section 2. Therefore, the correspond-\ningfeedback is quite simple: the user validates/corrects the\npresence or absence of the current class for the segment\nselected by active learning (see 3.3.5). For the learning\nphase, positive samples are those which contain the target\nsound class and negative samples are those which do not.\nThis implies that the positive segments may be complex\n2we use the libSVM implementation [3].\n147Poster Session 1\nsound mixtures which contain other sounds. Using proba-\nbilistic SVMs, posterior probabilities p(Ci|Xk)are obtained\nfor each frame observation Xk.\n3.3.3 One-pass\nIn the one-pass approach, the classiï¬cation is carried out\nas in a â€standardâ€ multiclass problem, where all classes are\njointly taken into account. Consequently, the user tries to\ntranscribe all the sound classes at the same time and the cor-\nresponding feedback requested to the user is to validate/correct\nthe presence or absence of all the sound classes for the se-\nlected segment (see 3.3.5). Two classiï¬cation methods are\ncompared in this approach.\nThe ï¬rst one ( one-pass 1 ) uses the same classiï¬cation\nmethod as the MP approach: for Nsound classes, Nclas-\nsiï¬ers are trained with positive samples being those which\ncontain the target sound class and negative samples being\nthose which do not.\nThe second method ( one-pass 2 ) differs in that it can in-\ntroduce new classes through the iterations by considering\ntexture classes deduced from the userâ€™s feedback, i.e.for a\ngiven feedback iteration, if the user formulates that the cor-\nresponding selected segment contains more than one sound\nclass, say classes A and B, a new texture class is created, that\nis composed of the union of those classes ( i.e.AâˆªB), and\nthe corresponding classiï¬er trained. Hence Mclassiï¬ers are\nhere used in the polyphonic case, with Nâ‰¤Mâ‰¤2N.\n3.3.4 Segment-level predictions\nGiven the posterior probability p(Ci|Xk)of classCion each\nframe feature vector Xk,P(Ci|XkÏ„,...,XkÏ„+LÏ„âˆ’1)(a seg-\nment-level probability) is computed for each texture seg-\nment obtained in the clustering phase. For this, the sum of\nall frame-level log probabilities is used. The probability on\ntheÏ„thtexture segment of length LÏ„is given by:\nP(Ci|XkÏ„,...,XkÏ„+LÏ„âˆ’1) =/summationtextkÏ„+LÏ„âˆ’1\nk=kÏ„logp(Ci|Xk).\nThen, the label of a texture segment is given by the maxi-\nmum probability criterion.\n3.3.5 Active learning for segment selection\nRelevance feedback has been widely used in multimedia In-\nformation Retrieval. The reader can refer to [12] for an\noverview. In the context of this work, our approach con-\nsists in gradually adding new segments validated by the user\nin the learning process. As a consequence, the labels pre-\ndicted for the other segments may evolve at each iteration\nof the algorithm. The process begins with a limited num-\nber of segments for training the classiï¬er and the training\nsegment dataset grows step by step as user-validated seg-\nments are injected. The goal of this approach is to obtain the\ncorrect labeling of samples in a reasonable number of iter-\nations. Active learning theory proposes sampling strategies\nwhich are used to select the segments to be user-validatedï¬rst. The two interaction loop approaches use different sam-\npling strategies. The Multi-pass approach uses the most\nambiguous strategy: in the SVM classiï¬er, most ambigu-\nous samples are the closest to the hyperplane in the feature\nspace. This strategy is adapted to binary classiï¬cation prob-\nlems and was shown to give the best results in a previous\nstudy [10]. The one-pass approach uses the best versus sec-\nond best strategy which has been successfully used in image\nclassiï¬cation [13]. This strategy uses the difference between\nthe probabilities of the two classes having the highest esti-\nmated probability value which provides an estimation of the\nconfusion about class membership.\nFor each frame-level probability, we compute a score s(k)\nin accordance with the sampling strategy used. Given this\nscore, for each frame of audio, we obtain a score for each\ntexture segment by temporal integration, where the segment\nscore is the mean of the underlying frame scores: SÏ„=\n1/LÏ„/summationtextkÏ„+LÏ„âˆ’1\nk=kÏ„s(k)for theÏ„thtexture segment. The tem-\nporal integration allows us to obtain a unique sampling strat-\negy score for each segment and to rank them. Therefore, the\nsegment which maximizes the score is selected by the sys-\ntem and a feedback request is sent to the user.\n4. EV ALUATION\nUser-based experiments are very time consuming and re-\nquire the creation of ground-truth annotation of numerous\nmusic pieces, which often turns out to be even more tricky,\nespecially as far as electro-acoustic music is concerned. In-\ndeed, there exists only a few annotations in this case which\nmix the description of sound objects with the annotatorsâ€™\nsubjective interpretation of the pieces. As a result, to val-\nidate our method with a descent number of ï¬les and eas-\nily compare the different parameters settings, we opted for\na user simulation with synthetic music pieces generation.\nNevertheless, much care has been taken in order to make this\nprocedure completely realistic as will be further explained\nhereafter.\n4.1 Synthetic pieces generation\nThe synthetic pieces generation process is similar to our pre-\nvious work. The reader can refer to [10] for a complete de-\nscription. 24 homogeneous sounds (hence 24 classes) of dif-\nferent lengths (from a second to a minute) were selected by\ncomposers of the Groupe de Recherches Musicales3(INA-\nGRM) for the generation process. 100 pieces of 2 minutes\ncontaining 5 different sound classes for each were gener-\nated. 5 versions of each piece were obtained by varying\nthe polyphonic degree from 1 to 5 ( i.e.500 synthetic sound\nï¬les). Consequently, the nthvariation of a given piece will\nhave a maximum number of nsounds playing simultane-\n3http://www.inagrm.com/\n14812th International Society for Music Information Retrieval Conference (ISMIR 2011)\nously. In the generation process, 5 distinct sounds are se-\nlected randomly for a given piece and different instances of\neach sounds are concatenated/juxtaposed accordingly with\nthe polyphonic degree of the piece.\n4.2 User simulation\nIn this work, we exploit a user feedback simulator to facili-\ntate the evaluation. It is used both in the initialisation phase\nand in the interaction loop. For the initialisation, the slider\nposition controls the overall segmentation level of the piece\nand the user has to choose the position which best matches\nhis/her viewpoint assuming that the user will tend to try to\nmaximise the segmentation F-measure score. Hence, the lat-\nter is computed for all the slider positions, i.e.all possible\nlevels of the hierarchical clustering used for segmentation.\nThe F-measure is computed with a temporal precision win-\ndow of 0.5 s over the segmentationâ€™s boundaries. The slider\nposition which maximizes the F-measure is used as the ini-\ntial segmentation level before entering the interaction loop .\nFor the selection of class initialisation segments Si, the seg-\nments in which the target sound class Ciis the loudest were\nselected. For each texture segment Ï„we compute an en-\nergy ratio:RCi,Ï„=ECi,Ï„//summationtext\nl/negationslash=iECl,Ï„whereECi,Ï„is the\nroot mean square energy of the Ï„thtexture segment for the\nclassCi.ECi,Ï„=/radicalBig\n1/LÏ„/summationtextkÏ„+LÏ„âˆ’1\nk=kÏ„x2\ni(k)withxithe\nsignal of the class Ci. For a given sound class Ci, the tex-\nture segment which maximizes the ratio RCiis selected as\nthe initialisation segment for Ci. In the interaction loop , the\nsuccessive interaction steps of the user with the system, ex-\nposed in Section 3.1 were simulated for the 500 sound ï¬les\nof the whole corpus. In this work, for active learning seg-\nment selection, we ï¬lter segments shorter than 0.5 s since\nthey could be misjudged by the user when asked for valida-\ntion, due to human perception limitations. A basic version\nof the function undo is also simulated: if an acceptable level\nof satisfaction (F-measure â‰¥0.85) is reached for a given\nclassCl, the results must not decrease in the next iterations.\nTherefore, if the results decrease, we suppose that the user\nwill use the undo function and lock the class Clto retain\nthe previous classiï¬er predictions and re-use them (without\nfurther updating) for the next iterations.\n4.3 Results\nWe monitored the behaviour of the F-measure scores for 500\npieces over the iterations of the algorithm with the differ-\nent interactive approaches. In the different methods, we ï¬x\na maximum number of iterations of 30 since good results\nshould be obtained in a reasonable number of interactions.\nAs it was observed in our previous work [10], the re-\nsults decrease accordingly with the polyphonic complexity\nof the pieces. Figure 3 shows the F-measure results across\nthe iterations for a particular class with the MP approach\n5 10 15 200.50.550.60.650.70.750.80.850.90.951Fmeasure of an individual class (Polyphony = 4)\nIteration numberMPMPMP\nMPFigure 3 . F-measure versus number of iterations for the MP\napproach (polyphony = 4). The central mark is the median,\nthe edges of the box are the 25th and 75th percentiles and the\nwhiskers extend to the minimum and maximum data points.\n5 10 15 20 25 300.50.60.70.80.91FÃ¯measure for a whole file (Polyphony = 2)\nIteration numberOP2OP1OP2OP1 OP2\nOP1OP2OP1OP1OP2 OP1OP2\nFigure 4 . F-measure versus number of iterations for the\nOP1 and OP2 approaches (polyphony = 2).\n5 10 15 20 25 300.50.60.70.80.91FÃ¯measure for a whole file (Polyphony = 4)\nIteration number  \nOP2 OP1\nOP1OP2OP1OP1OP2\nOP1OP2OP1OP2\nOP2\nFigure 5 . F-measure versus number of iterations for the\nOP1 and OP2 approaches (polyphony = 4).\n(polyphony=4). It is observed that good results can be ob-\ntained after 10 iterations with this reasonable polyphonic de-\ngree. Given the nature of this approach, which permits the\nuser to focus on one class for the whole process, the obtained\nnumber of iterations must be multiplied by the number of\n149Poster Session 1\nclasses to adress in the music piece.\nFigure 4 compares the one-pass approaches (OP1 and\nOP2) for a polyphonic degree of 2. The Figure 5 compares\nthe same approaches for a polyphonic degree of 4. The re-\nsults show that the method OP2 which introduces new mix-\nture classes with user feedback gives better results. These\napproaches are both considering all the classes of interest\nat the same time and we observe that they can reduce the\ntotal number of iterations comparing to the MP approach\nin which the user must repeat the process to address all the\nclasses. A satisfying median F-measure of 0.85 can be ob-\ntained in 20 iterations with OP2 for a whole piece.\n5. CONCLUSION\nIn this paper, we have proposed two different interactive ap-\nproaches for helping the analysis of electro-acoustic music.\nIn the multi-pass approach, the user focuses on one sound\nclass at each time. In the one-pass approaches, the user\ngives a more informative feedback to treat all the classes\nof the ï¬le simultaneously. The results show that the MP ap-\nproach is more adapted to a small number of classes: if the\nnumber of classes to transcribe is important, satisfying re-\nsults can be obtained in a smaller number of iterations with\nOP2 (the most effective one-pass approach).\nFuture works will focus on integrating the labeling infor-\nmations of the initial clustering. To validate the system with\nreal pieces, we will extend the evaluation to real users and\nwork on the design of an appropriate user interface.\n6. REFERENCES\n[1]http://www.inagrm.com/sites/default/\nfiles/polychromes/problematique/\nmodulePP/index.html .\n[2] C. J. C. Burges. A Tutorial on Support Vector Machines\nfor Pattern Recognition. Data Mining and Knowledge\nDiscovery , 2(2):121â€“167, 1998.\n[3] Chih-Chung Chang and Chih-Jen Lin. LIBSVM: a\nlibrary for support vector machines , 2001. Soft-\nware available at http://www.csie.ntu.edu.\ntw/Ëœcjlin/libsvm .\n[4] M. Crucianu, M. Ferecatu, and N. Boujemaa. Relevance\nfeedback for image retrieval: a short survey. In State\nof the Art in Audiovisual Content-Based Retrieval, In-\nformation Universal Access and Interaction including\nDatamodels and Languages (DELOS2 Report) , 2004.\n[5] R. Duda, P. Hart, and D. E. Stork. Pattern classiï¬cation,\n2001. New York: Wiley-Interscience.\n[6] S. Essid, G. Richard, and B. David. Musical instrument\nrecognition by pairwise classiï¬cation strategies. In IEEETransactions on Speech, Audio and Language Process-\ning, volume 14, pages 1401â€“1412, 2006.\n[7] M. R. Every. Discriminating Between Pitched Sources\nin Music Audio. Audio, Speech, and Language Process-\ning, IEEE Transactions on , 16(2):267â€“277, 2008.\n[8] F. Fuhrmann, M. Haro, and P. Herrera. Scalability, gen-\nerality and temporal aspects in automatic recognition of\npredominant musical instruments in polyphonic music.\nin Proc. of ISMIR , 2009.\n[9] S. Gulluni, S. Essid, O. Buisson, and G. Richard. Inter-\nactive segmentation of electro-acoustic music. Interna-\ntional Workshop on Machine Learning and Music , 2009.\n[10] S. Gulluni, S. Essid, O. Buisson, and G. Richard. In-\nteractive classiï¬cation of sound objects for polyphonic\nelectro-acoustic music annotation. in Proc. AES 42nd\nConference on Semantic Audio , 2011.\n[11] K. Hoashi, K. Matsumoto, and N. Inoue. Personalization\nof user proï¬les for content-based music retrieval based\non relevance feedback. In Proceedings of the eleventh\nACM international conference on Multimedia , pages\n110â€“119, 2003.\n[12] X. Jin, J. French, and J. Michel. Toward Consistent\nEvaluation of Relevance Feedback Approaches in Mul-\ntimedia Retrieval. Adaptive Multimedia Retrieval: User,\nContext, and Feedback , pages 191â€“206, 2006.\n[13] A. J. Joshi, F. Porikli, and N. Papanikolopoulos. Multi-\nclass active learning for image classiï¬cation. IEEE Con-\nference on Computer Vision and Pattern Recognition\n(2009) , pages 2372â€“2379, 2009.\n[14] T. Kitahara, M. Goto, K. Komatani, T. Ogata, and H. G.\nOkuno. Instrument Identiï¬cation in Polyphonic Mu-\nsic: Feature Weighting to Minimize Inï¬‚uence of Sound\nOverlaps. EURASIP J. Appl. Signal Process. , 2007:155â€“\n155, January 2007.\n[15] M. Mandel, G. Poliner, and D. Ellis. Support vector ma-\nchine active learning for music retrieval. In ACM Multi-\nmedia Systems Journal , 2006.\n[16] B. Mathieu, S. Essid, T. Fillon, J. Prado, and G. Richard.\nYaafe, an easy to use and efï¬cient audio feature extrac-\ntion software. in Proc. of ISMIR , 2010.\n[17] G. Peeters. A large set of audio features for\nsound description (similarity and classiï¬cation) in the\nCUIDADO project. Tech. rep., IRCAM, 2004.\n[18] D. Teruggi. Technology and Musique Concrete: The\nTechnical Developments of the Groupe de Recherches\nMusicales and Their Implication in Musical Composi-\ntion. Organised Sound , 12(3):213â€“231, 2007.\n150"
    },
    {
        "title": "Using Network Sciences to Rank Musicians and Composers in Brazilian Popular Music.",
        "author": [
            "Charith Gunaratna",
            "Evan Stoner",
            "Ronaldo Menezes"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1415818",
        "url": "https://doi.org/10.5281/zenodo.1415818",
        "ee": "https://zenodo.org/records/1415818/files/GunaratnaSM11.pdf",
        "abstract": "Music fascinates and touches most people. This fascination leads to opinions about the music pieces that reflects peopleâ€™s exposure and personal experience. This inherent bias of people towards music indicates that personal opinion is inappropriate for defining the quality of music and musicians. This paper takes a holistic view of the problem and delves into the understanding of the structure of Brazilian music rooted in Network Sciences. In this paper we work with a large database of albums of Brazilian music and study the structure of collaborations between all the musicians and composers. The collaboration is modelled as a social network of musicians and then analyzed from different perspectives with the goal of describing what we call the structure of that musical genre as well as provide a ranking of musicians and composers.",
        "zenodo_id": 1415818,
        "dblp_key": "conf/ismir/GunaratnaSM11",
        "keywords": [
            "Music",
            "fascination",
            "personal opinion",
            "quality of music",
            "musicians",
            "network sciences",
            "collaborations",
            "social network",
            "musicians",
            "composers"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nUSING NETWORK SCIENCES TO RANK MUSICIANS AND COMPOSERS IN\nBRAZILIAN POPULAR MUSIC\nCharith Gunaratna\nComputer Sciences\nFlorida Tech, USA\nagunaratna2009@my.fit.eduEvan Stoner\nComputer Sciences\nFlorida Tech. USA\nestoner2010@my.fit.eduRonaldo Menezes\nComputer Sciences\nFlorida Tech, USA\nrmenezes@cs.fit.edu\nABSTRACT\nMusic fascinates and touches most people. This fascina-\ntion leads to opinions about the music pieces that reï¬‚ects\npeopleâ€™s exposure and personal experience. This inherent\nbias of people towards music indicates that personal opin-\nionis inappropriate for deï¬ning the quality of music and\nmusicians. This paper takes a holistic view of the prob-\nlem and delves into the understanding of the structure of\nBrazilian music rooted in Network Sciences. In this paper\nwe work with a large database of albums of Brazilian music\nand study the structure of collaborations between all the mu-\nsicians and composers. The collaboration is modelled as a\nsocial network of musicians and then analyzed from differ-\nent perspectives with the goal of describing what we call the\nstructure of that musical genre as well as provide a ranking\nof musicians and composers.\n1. INTRODUCTION\nBrazilian Music is admired worldwide due to its diversity\nand richness of sounds. The music from Brazil is in fact a\nconï¬‚uence of many different cultural inï¬‚uences [1, 2]. This\nprocess of globalization of the popular music of Brazil has\ncome to a full circle when other genres around the world\nstarted to incorporate Brazilian rhythms and refer to Brazil-\nian music as an inï¬‚uence to them. It is known that world\ngreats such as Miles Davis and Frank Sinatra, and more re-\ncently the likes of Pat Metheny and Bill Frisell (jazz gui-\ntarists), have been inï¬‚uenced by and even worked with many\nBrazilian musicians.\nWhen it comes to the arts, is hard to deï¬ne a canon due\nto subjective opinions. For classical art, the use of networks\nhas improved our ability to understand the importance of\nmany works [13]. In popular art, the deï¬nition is a lit-\ntle harder because it could depend on many factors such as\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proï¬t or commercial advantage and that copies\nbear this notice and the full citation on the ï¬rst page.\nc/circlecopyrt2011 International Society for Music Information Retrieval.sales, and playtime on the radio. However, this paper pro-\nposes to use techniques from networks sciences to model\nthe network of collaborations among musicians and derive\nfrom the social network a good ranking of musicians and\ncomposers in Brazilian music.\nMany Brazilian musicians are well-known to people in\nBrazil and respected for their body of work. In Brazilian\npopular music ( MÂ´usica Popular Brasileira in Portuguese)\n[11], hereafter referred to as MPB, names such as Tom Jo-\nbim, Chico Buarque, and Noel Rosa are likely to be fa-\nvorites. But does the social network of collaborations in\nBrazilian support the view of critics about musicians such\nas the ones mentioned above? What makes a person impor-\ntant to his art? This paper looks initially at collaborations\nbetween musicians from a point of view albums recorded.\nWe have build a dataset of Brazilian albums (CDs, LPs, etc)\nand created a network of musicians in where they are linked\nif they participated together in at least one album. We then\nrepeat the study with composers who are linked to one an-\nother if they wrote a song together. In both instances, the\nweight of the collaboration is given by how many times the\ncollaboration was repeated. The goal of the study is to im-\nprove the understanding of the structure of Brazilian music\nas well as to use networks for providing a ranking of musi-\ncians and composers in MPB.\n2. A BRIEF HISTORY OF BRAZILIAN MUSIC\nBrazil is a country of continental proportions and, as such,\npresents a rich variety of sounds and rhythms in its mu-\nsic. Brazil has long been seen as a source of inspiration\nto many world-class musicians. It is easy to understand that\nthe universality of the music of Brazil is a reï¬‚ection of the\ncountryâ€™s history that includes native Brazilians with their\nrhythms and harmonies, being mixed with European (Por-\ntuguese primarily) and African sounds.\nBrazilian music was also inï¬‚uenced by sounds from other\nparts of the world. By the end of the 1950s, one of the\nmost important movements in MPB came to light: the Bossa\nNova , which introduced to the world names such as Tom Jo-\nbim, Jo Ëœao Gilberto and Luiz Bonf Â´a. By the end of 1960s,\nthe inï¬‚uence of rock has reached Brazil leading to a move-\nment called Tropicalismo led by the likes of Caetano Veloso,\n441Poster Session 3\nGilberto Gil and Tom Z Â´e. Other smaller movements: Jovem\nGuarda (driven by a need for songs with simple lyrics) Pes-\nsoal de Minas (from Minas Gerais State) and Pessoal do\nCear Â´a(from Cear Â´a State).\nWhat is important to notice is that these movements were\ninï¬‚uenced different styles: jazz, rock, regional sounds, coun-\ntry music, etc.\n3. MUSICIAN COLLABORATIONS AS SOCIAL\nNETWORKS\nThe understanding of musical relationships between styles\nand cultures, as well as the relation between music and other\nsciences (particularly Math) have for a long time been of in-\nterest to musicologists, independently of the music origin,\nbe it classical, popular, or other genre [3, 7, 14, 17]. More\nrecently we have seen a revival of works on musical rela-\ntionships due to the demand for recommendation systems\nin the online world [6, 10, 15]. Companies would like to\nknow more about peopleâ€™s taste based on prior knowledge\nabout their likes and dislikes. There are many approaches\nfor recommendation systems and in one way or the other\nthey require some understanding of musical relationships.\nSince the late 1990s we have been seeing the emergence\nof a new multidisciplinary ï¬eld, named Network Sciences .\nThis ï¬eld provides a framework for modeling interactions\nbetween entities so as to reveal properties at a macro level\nwhich may not be noticeable at the individual level.\nTechniques from Network Sciences have been success-\nfully applied to music. In general, the works relating mu-\nsic and networks do not attempt to create recommendation\nsystems although that can be seen as a consequence of the\nunderstanding of the relationships. Park et al. [12] have de-\nscribed a study in which a social network of contemporary\nmusicians have been created from the allmusic.com (AMG)\nand compared it with another music network in which mu-\nsicians are connected based on critics views of their simi-\nlarities. Gleiser and Danon [9] studied communities in Jazz\nusing the edge-betweenness community detection algorithm\nfrom Girvan and Newman [8]. The network was created by\nlinking musicians if they played in the same band. The com-\nmunity analysis found that racial divisions exists within Jazz\nbands with groups members being mostly black or mostly\nwhite. Gleiser and Danon have also created a jazz band net-\nwork in which bands are linked if they have a musician in\ncommon. The jazz band analysis found that communities\nof bands are divided based on the location they generally\nrecord.\nRecently, the application of concepts of complex net-\nworks have been discussed as very useful to systems dealing\nwith music recommendation [5]. As we move increasingly\ntowards online delivery of music and as the concept of an\nalbum is replaced by people picking and choosing individ-\nual songs they enjoy, recommendation becomes an impor-\ntant process to the music industry. Music recommendation\nsystems are also crucial in a world where the availability ofmusic can easily overwhelm the listener. In this paper, we\nmove closer towards understanding the structure of the net-\nwork of collaborations in Brazilian music which in turn may\naid the development of recommendation systems for MPB.\n4. BUILDING SOCIAL NETWORKS FROM\nCOLLABORATIONS\nThe ï¬rst step in our study was to collect a dataset related to\nBrazilian Music. There are many sites available online with\ncatalogues of records (CDs, LPs) of MPB. The two most fa-\nmous ones are: Ricardo Cravo Albinâ€™s dictionary of Brazil-\nian music1and Maria Luiza Kfouriâ€™s personal discogra-\nphy2. Although the former is more extensive it lacks a in-\nformation about the songs and the musicians of each album.\nWe opted to go with the later because it is quite complete\nabout musicians who participate on the record, all songs in\nthe album, the composers of each song, and the musicians\ninvolved in the recording.\nAfter all was done, we had a dataset with 6,149 albums\nof which 5,302 feature musicians. There are 506 albums\nwith only one musician, therefore, because of the way we\ndeï¬ne an edge, these musicians would not appear in the net-\nwork unless they appear in another album that feature two\nor more musicians. There are 16,718 musicians that con-\ntributed to 85,133 tracks. There are 10,490 composers and\n1,913 artists. In order to better understand the structure of\nBrazilian music we concentrate on musicians (who play the\nmusic) and composers (who write the music).\n4.1 Metrics\nThe literature in Network Sciences includes a number of\nmetrics that can be computed to characterize a network which,\nin turn, may reveal interesting patterns in the relationships\nof nodes. The analysis of metrics related to the topology\nof networks have long been used in Social Networks in an\narea generally referred to as Social Network Analysis (SNA)\n[16]. In this paper we concentrate on two measures of nodes\nin the social networks we deal with because they enable us\nto rank nodes.\nNode Degree: The degree of a node is a metric that refers\nto how many connections the entity represented by\nthe node has in the social network. Higher degree\nis generally associated with a higher inï¬‚uence in the\nnetwork because that node can quickly reach many\nothers.\nPagerank: Although the degree looks at the importance of\na node, it considers the importance in isolation. How-\never, it is generally the case that the importance of a\nnode depends on the importance of nodes that have a\nrelation with it. In PageRank, important nodes pass\non their importance to other nodes they are connected\n1www.dicionariompb.com.br\n2www.discosdobrasil.com.br\n44212th International Society for Music Information Retrieval Conference (ISMIR 2011)\nto. If an important node points to many other nodes,\nits importance is weighted by the number of connec-\ntions it has.\n4.2 Networks of Collaborations in Brazilian Music\nOne pre-condition to perform network analysis is to cor-\nrectly chose what the nodes in the network represent and\nwhat is used for the relationship between these nodes [4].\nIn this paper we would like to understand the structure of\nBrazilian music by looking at networks of musicians and\ncomposers. These networks will allow us to move a step\ncloser to answering questions like: who are the seminal in-\ndividuals in the Brazilian music world?\nIn our ï¬rst network, we look at the structure of people\nwho play the music, what we call the Network of Musicians\n(NoM). Secondly, we look at who is writing the music being\nplayed, what we call Network of Composers (NoC). To cre-\nate these networks we have to look at the dataset and ï¬nd ap-\npropriate information by projecting the dataset on these two\nkids of relationships. In the NoM, a musician is linked to an-\nother if they have participated together in at least one album.\nFor the NoC we have used composers as nodes and the re-\nlationship between them exists if they have composed some\nmusic togetherâ€”Brazilian music is in fact quite unique in\nthis sense since most songs are born out of collaborations.\nIn both network instances, since a person can participate in\nmore than one collaboration, we use a weighted represen-\ntation of the relationship in which the weight of the edge\n(i, j),wij, represents the total number of albums the mu-\nsicians iand jhave played together for the case of NoM,\nand how many songs they have composed together for the\ncase of NoC. The NoM contains 16,442 nodes and 844,223\nedges, while the NoC is a much sparser network with 8,152\nnodes and 12,923 edges.\n5. ANALYSIS OF THE NETWORKS\nThe social network we analyzed contains works from more\nthan 60 years of Brazilian music. When discussing the in-\nï¬‚uence of a person in a social network the number of collab-\norations she has is of prime importance. In social network\nterms, the number of collaborations is expressed by the de-\ngree of the node in the network. For instance, if a node x\nrepresenting a person collaborated with 4 others his degree,\ndeg(x) = 4 . Note however that degree does not consider\nthe â€œsizeâ€ of the collaboration, so if a person collaborated\nwith the another 5 times, only the weighted degree, wdeg\ncaptures this information. In order to have a complete pic-\nture we need both the degree (number of different collab-\norations) and weighted degree (number of total collabora-\ntions). We have used the entire dataset and ranked musi-\ncians and composers by the number of collaborators. Tables\n1 and 2 show the rankings by degree but we also display the\nweighted degree.\nTable 1 shows the list of musicians in Brazilian music.\nMost of these are probably unknown to the general pub-lic because they form what we like to call the â€œscaffold-\ning of Brazilian musicâ€. With a few exceptions, these are\nthe musicians who are respected in their art but generally\ndo not work as leaders in recordings. Some of the num-\nbers presented are quite impressive. Despite the incomplete-\nness of our dataset (see Section 6 for description of our fu-\nture work), we see many musicians who have collaborated\nwith more than 2,000 others, a feat not so easily achievable.\nThese musicians are able to carry inï¬‚uences from an album\nto another and are major contributors of cross-fertilization\nbetween brazilian styles.\nAnother interesting observation from Table 1 is that the\nnational instrument from Brazil, the classical guitar (Hornbostel-\nSachs number 321.322), is not present. We believe that this\nis the case because the musicians above belong to this â€œscaf-\nfoldingâ€ class which works on albums as supporting mem-\nbers and not as the main personnel. The table shows the\nimportance of classical instruments even for popular music.\nTable 2 describes the ranking of composers according to\ndegrees. Here the disparities are more prominent between\ndegand wdeg . This is expected because some composers\ncollaborate with few others but write many compositions\nwith them. For instance, this is the case with Vinicius de\nMoraes (in bold in Table 2) has deg=59 but wdeg =3,392. It\nis worth noticing that our wdeg is based on the total number\nof compositions that appears in the dataset (not on unique\ncompositions); this choice is made on purpose for the com-\nposers study because we want wdeg to be more than just a\ncount of different compositions but also give a notion of im-\nportance of the individual. If a composers has then one col-\nlaboration ( deg=1) but that composition has been recorded\n1,000 times in the dataset, his wdeg =1,000. For us that\ncomposer is important to the structure of brazilian music\nalthough she has not composed many piecesâ€”she would\nbe important because his composition has been frequently\nrecorded.\nTable 2 . List of top 30 composers by the number of dif-\nferent collaborations ( deg). However some of the collabo-\nrations are repeated, meaning that the composers may write\nmore than one song with a collaborator. The weighted de-\ngree ( wdeg ) column is an indication of repeated collabora-\ntions. Names in shown in bold are used as speciï¬c examples\nin the text.\ndeg Name wdeg deg Name wdeg\n83 Paulo C Â´esar Pinheiro 1,047 50 Chico Buarque 1,186\n74 Arnaldo Antunes 411 47 Francis Hime 488\n65 Caetano Veloso 320 45 Moraes Moreira 344\n61 Aldir Blanc 960 45 Ataulfo Alves 257\n61 Ivan Lins 668 44 Nei Lopes 192\n60 Milton Nascimento 917 44 Tom Z Â´e 142\n60 Gilberto Gil 427 44 Martinho da Vila 118\n59 Vinicius de Moraes 3,392 43 Wilson Batista 260\n59 Noel Rosa 731 43 Itamar Assumpc Â¸ Ëœao 115\n59 Luiz Gonzaga 706 42 Heitor Villa-Lobos 263\n57 JoËœao Donato 474 41 Carlinhos Brown 212\n56 Nelson Cavaquinho 634 40 Pedro Lu Â´Ä±s 72\n52 Ronaldo Bastos 364 39 Tom Jobim 2,486\n52 Herm Â´Ä±nio Bello de Carvalho 348 39 Zeca Baleiro 89\n52 DÂ´elcio Carvalho 190 37 Fausto Nilo 165\n443Poster Session 3\nTable 1 . List of top 30 musicians by the number of different collaborations ( deg). However some of the collaborations are\nrepeated, meaning that the musicians may play in many albums with the same musicians. The weighted degree ( wdeg ) column\nis an indication of repeated collaborations.\ndeg Name wdeg Instrument deg Name wdeg Instrument\n3,002 MÂ´arcio Eymard Mallard 15,635 Cello 2,062 Jorge Helder 7,633 Bass\n2,782 JosÂ´e Alves da Silva 15,122 Violin 2,051 Wilson das Neves 7,891 Drums\n2,659 Jorge Kundert Ranevsky 13,474 Cello 1,990 Ricardo Amado 7,878 Violin\n2,579 Paschoal Perrota 13,398 Violin 1,990 Gordinho 7,477 Percussion\n2,563 Jaques Morelenbaum 10,558 Cello 1,967 Alfredo Vidal 9,864 Violin\n2,470 Walter Hack 13,290 Violin 1,949 Jamil Joanes 7,317 Bass\n2,445 Alceu de Almeida Reis 12,319 Cello 1,897 JesuÂ´Ä±na Noronha Passaroto 7,968 Viola\n2,405 Robertinho Silva 7,135 Drums 1,862 ZÂ´e Carlos Bigorna 6,593 Sax, Flute\n2,400 JoËœao Daltro de Almeida 11,280 Violin 1,849 Aizik Meilach Geller 9,263 Violin\n2,331 Carlos Eduardo Hack 11,696 Violin 1,810 OvÂ´Ä±dio Brito 5,376 Percussion\n2,314 Frederick Stephany 11,071 Viola 1,804 Nailor Proveta 4,438 Sax\n2,251 Bernardo Bessler 9,751 Violin 1,792 Crist Â´ovËœao Bastos 8,373 Piano\n2,268 Giancarlo Pareschi 12,405 Violin 1,771 MÂ´arcio Montarroyos 7,243 Trumpet\n2,251 Michel Bessler 10,254 Violin 1,759 Carlos Malta 4,529 Flute\n2,201 Marcos Suzano 5,640 Tambourine 1,748 Marie Christine Springuel 7,076 Viola\nThe list in Table 2 is somewhat surprising at ï¬rst because\nof names such as Arnaldo Antunes, Carlinhos Brown, Pedro\nLuÂ´Ä±s, and Zeca Baleiro (also in shown in bold). However\nthese names represent the new generation of Brazilian com-\nposers who make very good use of social media and collab-\norate with many other musicians. The rankings in the table\nconsiderers all data in the dataset. To better understand the\nevolution of these rankings we performed a temporal analy-\nsis but using pagerank rather than degree ranks.\nThe ï¬rst study we have performed using pagerank is shown\nin Figure 1. These ranks are per decade and follow the po-\nsition of the top 50 musicians and composers in the most\nrecent decade. It is important to understand that the rank-\ning in decades other than the most recent one is relative to\neach other. We took the top 50 musicians and composers\nin the most recent decade and followed their relative ranks\nin other decades. For instance, Noel Rosa appears as the\ntop ranked composer in Figure 1(right) for the most recent\ndecade but in the 14thposition in the 80s; this 14thmeans\nrelative to the 50 composers listed in the 2000 decade. In ab-\nsolute terms, Noel Rosa can (it probably is) lower than the\n14thposition. The connections in the social networks for\neach decade considers only the collaborations in albums of\nthat decade, which explain sudden changes in the rankings.\nA musician or composer that was top in a decade may be\nirrelevant in others because he was not active or because his\ncompositions were not recorded by musicians in that period.\nWe can see in Figure 1 that composers ranks are more\nstable than the musicians meaning that the relative ranks are\nbetter maintained for composers (the lines not cross as of-\nten and as radically). We can also argue that musicians do\nnot have as high longevity as composers. A musician who\nis very active today may not have been very active a few\nyears back. A clear example of this is the musician Michel\nBessler who does not even appear in albums prior to 1980 al-though he is 10thmost important musician of today. Bessler\nis the spalla of the Brazilian Symphony Orchestra and has\nparticipated in many popular albums (see Table 1). Figure 1\nconï¬rms this longevity observation, which is expected since\nthe NoM requires active participation of the musician in the\nrecording while the NoC includes people who may have\neven be deceased but continue to have their music recorded\n(e.g. Noel Rosa).\nLast we look at the evolution of rankings using accu-\nmulative networks. While Figure 1 looks at collaborations\nin isolation, Figure 2 shows the ranking (also according to\npagerank) of musicians and composers using an accumula-\ntive approach. Here we want to see how the ranks evolve\nif we consider the collaborations until a particular year but\nincluding all information since the ï¬rst date we have infor-\nmation on the dataset. For instance, the ranking in 2010\nconsiders all the works available in the dataset, that is, the\nfull collaboration network. Antecedent years (2007, 2004,\netc.) consider collaborations from the ï¬rst data available in\nthe dataset until the given year. Hence, the change from one\nyear to another (3 years appart) is due to the work produced\nin the last 3 years.\nThe use of accumulated networks allows us to see a lit-\ntle better how the structure changes as new musicians and\ncomposers become active. An excellent example of this is\nArnaldo Antunes who appears in Figure 2(right) in 4thposi-\ntion but decrease his relative rank quite rapidly until disap-\npearing completely in 1986. Arnaldo Antunes appeared to\nin Brazilian music scene as a member of a rock band called\nTitËœas in the mid-80s. After leaving the band, he emerged\nas one of the main composers in Brazil with many collab-\norators (which inï¬‚uences his pagerank). Most recently, his\ncompositions have been part of recordings of many respected\nbrazilian singers such as Marisa Monte and C Â´assia Eller.\nAnother interesting class of composers that we can see in\n44412th International Society for Music Information Retrieval Conference (ISMIR 2011)\nMÃ¡rcio Eymard MallardJorge Kundert RanevskyJosÃ© Alves da SilvaJorge HelderBernardo BesslerPaschoal PerrotaJaques MorelenbaumJessÃ© Sadoc do Nascimento FilhoJoÃ£o Daltro de AlmeidaMichel BesslerRicardo AmadoCarlos Eduardo HackRobertinho SilvaCarlos MaltaAlceu de Almeida ReisWalter HackNailor ProvetaMarcos SuzanoDirceu LeiteJesuÃ­na Noronha PassarotoAntonella Lima PareschiMarie Christine SpringuelJosÃ© RogÃ©rio RosaMarcos EsgulebaWilson das NevesOvÃ­dio BritoTeco CardosoToninho FerraguttiRoberto MarquesJamil JoanesFrederick StephanyJosÃ© Ricardo Volker TaboadaCarlinhos 7 CordasUbaldo VersolatoCristiano AlvesRui AlvimTrambiqueCristÃ³vÃ£o BastosZÃ© Carlos BigornaMÃ¡rcio AlmeidaArmando MarÃ§alZeca AssumpÃ§Ã£oGordinhoAndrÃ©a Ernest DiasMÃ¡rcio MontarroyosVittor SantosDener de Castro CampolinaMarcelo MartinsJairo Diniz SilvaPaula Vianna Prates Barbato195019601970198019902000\nNoel RosaPaulo CÃ©sar PinheiroArnaldo AntunesDÃ©lcio CarvalhoNelson CavaquinhoAdoniran BarbosaLuiz GonzagaIvan LinsJoÃ£o DonatoZeca BaleiroAtaulfo AlvesHermÃ­nio Bello de CarvalhoWilson BatistaNei LopesVinicius de MoraesAldir BlancChico BuarqueCarlinhos BrownGilberto GilMilton NascimentoChico CÃ©sarHerivelto MartinsJoÃ£o do ValeBenedito LacerdaTom JobimCaetano VelosoJackson do PandeiroItamar AssumpÃ§Ã£oElton MedeirosHeitor Villa-LobosCartolaFrancis HimeAry BarrosoCarlos LyraMoreira da SilvaTom ZÃ©Seu JorgeAlceu ValenÃ§aPedro LuÃ­sJoÃ£o de BarroPixinguinhaArlindo CruzBebel GilbertoGeraldo PereiraCatulo da PaixÃ£o CearenseMarino PintoMoraes MoreiraLenineBidGereba195019601970198019902000\nFigure 1 . Rank of the top 50 musicians (left) of the last decade and how these ranks evolve per decade. The pagerank of\n2000 is absolute but for the other decades it represents how these musicians rank against each other. For instance, none of the\nmusicians ranked today were present in recordings from 1950s (1950-1959). On the right picture, we have the rank of the top\n50 composers of the last decade and how these ranks evolve per decade. Individuals marked with an â†’are examples discussed\nin the text.\nFigure 2 is well represented by Caetano Veloso. The accu-\nmulated ranking shows that Caetano Veloso has maintained\nhimself active through several decades (by composing and\nhaving his songs recorded by other artists) and he is today\nstill the 5thmost important composer in Brazil. Compare\nthis to his position in Figure 1; since that analysis considers\nonly recordings per decade in isolation we see that Caetano\nVeloso is not so well positioned in more recent years. The\nfact is that Figures 2 and 1 taken together give us a good\nidea of the ranking of a musician and composer and how it\nevolves.\nLastly, our results allow us to observe scenarios like what\nhappens to Zeca Baleiro in Figure 1. Because the study takes\ndecades in isolation we can see that he appears high in the\nrankings but not at all in the accumulative ranking in Figure\n2. This is a case where we have a upcoming composer who\nhas been active only very recently and is part of the ranking\nof the last decade but not yet part of the entire history.\n6. CONCLUSION AND FUTURE WORK\nIn this paper we demonstrated that the construction of social\nnetworks and the use of metrics rooted in network sciences\nmay help us understand the structure of Brazilian music.\nRankings related to music are always controversial because\nof the attachment people feel to music. However we believe\nour approach is less biased and provides a good understand-\ning of the structure of Brazilian music. Our work shows\nthat the network of musicians is less stable than the networkof composers. This result is expected because musicians\nactively participate in the recording while composers par-\nticipate by having their songs recordedâ€”a composer may\neven be deceased when his composition is recorded. Our\nhope was to have a social network of musicians based on\nthem playing together on speciï¬c tracks rather than on an\nalbum as we believe this is a more accurate representation\nof the collaboration. However to our knowledge, no dataset\nof MPB includes the information per track.\nThe two kinds of rankings provided (and their visualiza-\ntion) also allows us to understand how the rankings change\nwith time. An analysis not included in this paper (due to\nspace restriction) seem to indicate that a composer needs to\nbe well ranked for about 30 years to appear in the accumu-\nlative rankings. This appears to indicate that 30 years for\nBrazilian music a measure of â€œsuccessâ€ for a composersâ€”\nwhat differentiates them from one-hit composers.\nWe continue to work on the current dataset on many fronts.\nWe are currently collecting more data to make the dataset\nmore complete since it is still incomplete particularly with\nregards to older recordings. Next, we intend to consider the\ndate of the composition in our analysis although this data is\nappearing to be very hard to gather. With this information\nwe believe we can have another dimension of the structure\nof composers. Last, our ultimate goal is to be able to add\nthe concept of reputation to the study and for that we may\nhave to consider a third category of individuals. A composer\nmay become part of the rankings by having one of his com-\npositions recorded by major singers (e.g. Elis Regina). We\n445Poster Session 3\nMÃ¡rcio Eymard MallardJosÃ© Alves da SilvaJaques MorelenbaumRobertinho SilvaJorge Kundert RanevskyMarcos SuzanoPaschoal PerrotaWalter HackAlceu de Almeida ReisJoÃ£o Daltro de AlmeidaJorge HelderFrederick StephanyCarlos Eduardo HackBernardo BesslerGiancarlo PareschiMichel BesslerNailor ProvetaWilson das NevesGordinhoJamil Joanes195919621965196819711974197719801983198619891992199519982001200420072010\nLuiz GonzagaPaulo CÃ©sar PinheiroNoel RosaArnaldo AntunesCaetano VelosoNelson CavaquinhoIvan LinsVinicius de MoraesAldir BlancJoÃ£o DonatoGilberto GilMilton NascimentoHeitor Villa-LobosDÃ©lcio CarvalhoHermÃ­nio Bello de CarvalhoItamar AssumpÃ§Ã£oTom ZÃ©Nei LopesAtaulfo AlvesChico Buarque195919621965196819711974197719801983198619891992199519982001200420072010\nFigure 2 . This picture shows the rank of the top 20 musicians (left) and composers (right) for the year 2010 and how they rank\nagainst each other in the antecedent years. Note that the ranks for all other years are not absolute. This means that if a person is\nlisted in the 1stposition it only means that the person is in the 1stposition relative to the other people listed in the year 2010.\nIn this case, the network of collaborations is not taken in isolation, so the data for the year 2007 includes all collaborations until\n2007. Individuals marked with an â†’are examples discussed in the text.\nare currently considering how this reputation can be added\nto the study given that some of these singers and have never\ncomposed songs are not musicians either.\n7. REFERENCES\n[1]Rafael Bastos. The origin of samba as the invention of brazil\n(why do songs have music?). British Journal of Ethnomusicol-\nogy, 8 IS -:67â€“96, Jan 1999.\n[2]Gerard B Â´ehague. Rap, reggae, rock, or samba: The local\nand the global in brazilian popular music (1985-95). Latin\nAmerican Music Review / Revista de M Â´usica Latinoamericana ,\n27(1):79â€“90, Apr 2006.\n[3]Wendy S. Boettcher, Sabrina S. Hahn, and Gordon L. Shaw.\nMathematics and music: A search for insight into higher brain\nfunction. Leonardo Music Journal , 4:53â€“58, 1994.\n[4]Carter T. Butts. Revisiting the Foundations of Network Analy-\nsis.Science , 325(5939):414â€“416, 2009.\n[5] `Oscar Celma. Music Recommendation and Discovery: The\nLong Tail, Long Fail, and Long Play in the Digital Music\nSpace . Springer Verlag, 2010.\n[6]Hung-Chen Chen and Arbee L. P. Chen. A music recommen-\ndation system based on music and user grouping. Journal of\nIntelligent Information Systems , 24:113â€“132, 2005.\n[7]Janet M. Cliff. On relationships between folk music and folk\ngames. Western Folklore , 51(2):129â€“151, 1992.\n[8]M Girvan and Mark E Newman. Community structure in social\nand biological networks. PNAS , 99(12):7821â€“7826, Jun 2002.[9]Pablo M. Gleiser and Leon Danon. Community structure in\njazz. Advances in Complex Systems: A Multidisciplinary Jour-\nnal, 6(4):565â€“573, 2003.\n[10] Beth Logan. Music recommendation from song sets. In Pro-\nceedings of the International Society for Music Information\nRetrieval Conference , pages 425â€“428, 2004.\n[11] Chris McGowan and Ricardo Pessanha. The Brazilian Sound .\nTemple University Press, 1998.\n[12] Juyong Park, `Oscar Celma, Markus Koppenberger, Pedro\nCano, and Javier M. Buld Â´u. The social network of contempo-\nrary popular musicians. International Journal of Bifurcation\nand Chaos , 17(7):2281â€“2288, 2007.\n[13] Maximilian Schich, Sune Lehmann, and Juyong Park. Dissect-\ning the canon: Visual subject co-popularity networks in art re-\nsearch. In International Workshop on Challenges and Visions\nin the Social Sciences , 2008.\n[14] John Shepherd. Music, culture and interdisciplinarity: Reï¬‚ec-\ntions on relationships. Popular Music , 13(2):127â€“141, 1994.\n[15] Ja-Hwung Su, Hsin-Ho Yeh, P.S. Yu, and V .S. Tseng. Music\nrecommendation using content and context information min-\ning.IEEE Intelligent Systems , 25(1):16â€“26, 2010.\n[16] Stanley Wasserman. Social Network Analysis: Methods and\nApplications . Structural Analysis in the Social Sciences. Cam-\nbridge University Press, 1994.\n[17] Olly Wilson. The signiï¬cance of the relationship between afro-\namerican music and west african music. The Black Perspective\nin Music , 2(1):3â€“22, 1974.\n446"
    },
    {
        "title": "HarmTrace: Improving Harmonic Similarity Estimation Using Functional Harmony Analysis.",
        "author": [
            "W. Bas de Haas",
            "JosÃ© Pedro MagalhÃ£es",
            "Remco C. Veltkamp",
            "Frans Wiering"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1417063",
        "url": "https://doi.org/10.5281/zenodo.1417063",
        "ee": "https://zenodo.org/records/1417063/files/HaasMVW11.pdf",
        "abstract": "Harmony theory has been essential in composing, analysing, and performing music for centuries. Since Western tonal harmony exhibits a considerable amount of structure and regularity, it lends itself to formalisation. In this paper we present HARMTRACE, a system that, given a sequence of symbolic chord labels, automatically derives the harmonic function of a chord in its tonal context. Among other applications, these functional annotations can be used to improve the estimation of harmonic similarity in a local alignment of two annotated chord sequences. We evaluate HARMTRACE and three other harmonic similarity measures on a corpus of 5,028 chord sequences that contains harmonically related pieces. The results show that HARMTRACE outperforms all three other similarity measures, and that information about the harmonic function of a chord improves the estimation of harmonic similarity between two chord sequences.",
        "zenodo_id": 1417063,
        "dblp_key": "conf/ismir/HaasMVW11",
        "keywords": [
            "Harmony theory",
            "tonal harmony",
            "formalisation",
            "HARMTRACE",
            "chord sequences",
            "harmonic function",
            "harmonic similarity",
            "local alignment",
            "corpus",
            "estimation"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nHARMTRACE: IMPROVING HARMONIC SIMILARITY ESTIMATION\nUSING FUNCTIONAL HARMONY ANALYSIS\nW. Bas de Haas\nUtrecht University\nW.B.deHaas@uu.nlJosÂ´e Pedro Magalh Ëœaes\nUtrecht University\nJ.P.Magalhaes@uu.nlRemco C. Veltkamp\nUtrecht University\nR.C.Veltkamp@uu.nlFrans Wiering\nUtrecht University\nF.Wiering@uu.nl\nABSTRACT\nHarmony theory has been essential in composing, ana-\nlysing, and performing music for centuries. Since Western\ntonal harmony exhibits a considerable amount of structure\nand regularity, it lends itself to formalisation. In this paper\nwe present HARM TRACE , a system that, given a sequence of\nsymbolic chord labels, automatically derives the harmonic\nfunction of a chord in its tonal context. Among other appli-\ncations, these functional annotations can be used to improve\nthe estimation of harmonic similarity in a local alignment of\ntwo annotated chord sequences. We evaluate HARM TRACE\nand three other harmonic similarity measures on a corpus\nof 5,028 chord sequences that contains harmonically related\npieces. The results show that HARM TRACE outperforms all\nthree other similarity measures, and that information about\nthe harmonic function of a chord improves the estimation of\nharmonic similarity between two chord sequences.\n1. INTRODUCTION\nWith the rapid expansion of digital repositories of music,\nsuch as iTunes, Spotify, last.fm, and the like, efï¬cient meth-\nods to provide content-based access to this kind of music\nrepositories have become increasingly important. To be able\nto cluster documents, a notion of the similarity between these\ndocuments is essential. Hence, within Music Information\nRetrieval (MIR), the development of musical similarity mea-\nsures plays a prominent role. Music can be related in many\ndifferent aspects, e.g. melody, genre, rhythm, etc.; this paper\nfocuses on similarity of musical harmony. Music retrieval\nbased on harmony offers obvious beneï¬ts: it allows for ï¬nd-\ning cover songs (especially when melodies vary), songs of a\ncertain family (like Blues or Rhythm Changes), or variations\nover a theme in baroque music, to name a few.\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proï¬t or commercial advantage and that copies\nbear this notice and the full citation on the ï¬rst page.\nc/circlecopyrt2011 International Society for Music Information Retrieval.\nI V IV I V/V\nC F D G7 7C\n{ Ton Ton Sub DomFigure 1 . A typical chord sequence. The chord labels are\nprinted below the score, and the scale degrees and func-\ntional analysis above the score. Tonic, dominant, and sub-\nsominant are denoted with Ton,Dom , and sub, respectively.\nTo be able to understand why two chord sequences are\nharmonically related, we believe it is important to examine\nchords not only in isolation but also the context in which\nthey occur. For this, we draw greatly on classical and jazz\nharmony theory. In the last decades, many music theorists\nhave studied tonal harmony and observed that within a se-\nquence not every chord is equally important. This suggests\nthat tonal harmony is organised hierarchically. Within a\nsequence of chords, some chords can be removed leaving\nthe global harmony structure intact, while removing other\nchords can signiï¬cantly change how the chord sequence is\nperceived. For example in Figure 1, the D7chord could be\nremoved without changing the general structure of the har-\nmony, while removing the G7or the C at the end would\nchange the harmony structure. This suggests that chords\ncan have different functional roles, and therefore different\nimportance.\nNowadays there is a rich body of literature that aims to\nexplain the order and regularities in Western tonal harmony,\nand various ways to analyse the function of a chord in its\ntonal context have been proposed [9, 14, 18]. Unfortunately,\nthe majority of these theories are formulated rather infor-\nmally and lack descriptions with mathematical precision or\ncomputational executability. Although there are exceptions,\nlike the Tonal Pitch Space model [8] and David Temperleyâ€™s\nMelisma [22], the lack of mathematical precision has ham-\npered the successful application of harmony models to prac-\ntical MIR related tasks, such as automatic analysis, similar-\nity estimation, content-based retrieval, or the improvement\nof low-level feature extraction.\n67Poster Session 1\nContribution We present HARM TRACE1, a system for\nanalysing Western tonal harmony and determining harmonic\nsimilarity, implemented robustly and efï¬ciently in the pure,\ntype-safe functional programming language Haskell. It is\nï¬‚exible, in the sense that it can be easily adapted and main-\ntained, robust against noisy data, and capable of displaying\nharmonic analyses in a clear way. We evaluate the retrieval\nperformance of HARM TRACE by comparing it to a baseline\nalignment system and to two earlier approaches to harmonic\nsimilarity in a retrieval experiment, using a corpus of 5,028\nchord sequences. The results show that HARM TRACE out-\nperforms all other harmonic similarity measures and that ex-\nploiting knowledge about the harmonic function of a chord\nimproves retrieval performance.\nThe rest of this paper is organised as follows. After a\nreview of related work in Section 2, we explain how an au-\ntomatic harmony analysis is performed by a music theoret-\nically founded knowledge system of tonal harmony (Sec-\ntion 3). Next, we deï¬ne harmonic similarity of two se-\nquences of annotated chords as the maximum local align-\nment score (Section 4). In Section 5 we compare the re-\ntrieval performance of HARM TRACE to three other harmonic\nsimilarity measures. Finally, we conclude the paper with\na short discussion on harmonic similarity and pointing out\ndirections for future research (Section 6).\n2. RELATED WORK\nGrammatical models of tonal harmony harmony have a long\nhistory in music research, e.g. [9, 15, 20]. The harmony\nmodel of HARM TRACE is based on the generative formalism\nproposed by Rohrmeier [16, 17]. He models tonal harmony\nas an elaborate recursive context-free grammar (CFG). His\nmodel extends ideas of the Generative Theory of Tonal Mu-\nsic(GTTM) [9] and Schenkerian Analysis [18], and cap-\ntures form, theoretical harmonic function [14], phrasing, and\nmodulation. De Haas et al. [4] performed a ï¬rst attempt at\nimplementing Rohrmeierâ€™s grammar and using it for deï¬n-\ning harmonic similarity. HARM TRACE transports these ideas\nto a functional setting, solving many of the typical problems\naccociated with context free parsing.\nThere exist other systems that address polyphonic mu-\nsic similarity, but generally these are embedded into larger\nretrieval systems and take audio or score information as in-\nput, e.g. [13]. We are aware of two other systems that focus\nsolely on harmonic similarity and compute similarity values\nfrom textual chord descriptions: the Tonal Pitch Step Dis-\ntance ( TPSD ) [5], and the Chord Sequence Alignment Sys-\ntem\n(CSAS ) [6]. A beneï¬t of evaluating only a similarity mea-\nsure is that errors caused by the feature extraction or chord\n1Harmony Analysis and Retrieval of Music with Type-level Represen-\ntations of Abstract Chords Entitieslabelling methods do not inï¬‚uence the retrieval evaluation.\nThe TPSD and CSAS are compared elaborately in [3]; we\nintroduce them brieï¬‚y here.\nThe TPSD uses Lerdahlâ€™s [8] Tonal Pitch Space ( TPS)\nas its main musical model. TPS is a model of tonality that\nï¬ts musicological intuitions, correlates well with empirical\nï¬ndings from music cognition, and can be used to calculate\na distance between two arbitrary chords. The TPS model\ntakes into account the number of steps on the circle of ï¬fths\nbetween the roots of the chords, and the amount of over-\nlap between the chord structures of the two chords and their\nrelation to the global key.\nThe general idea behind the TPSD is to use the TPS to\ncompare the change of perceived chordal distance to the\ntonic over time. For every chord, the TPS distance to the\nkey of the sequence is calculated, resulting in a step func-\ntion. Next, the distance between two chord sequences is\ndeï¬ned as the minimal area between the two step functions\nover all possible horizontal circular shifts. To prevent that\nlonger sequences yield larger distances, the score is normal-\nized by the duration of the shortest song.\nThe CSAS [6] is based on local alignment: by perform-\ning elementary deletion, insertion, and substitution opera-\ntions, one chord sequence is transformed into the other. The\nactual similarity value is deï¬ned as the total sum of all edit\noperations at all beat positions. To improve the retrieval per-\nformance of the classical alignment approach, Hanna et al.\nexperimented with various musical data representations and\nsubstitution functions. They found a key-relative represen-\ntation, based on the interval between the root of the chord\nand the key, to work well and preferred substituting only\nwhen the chord root and triad were not identical. In the ex-\nperiments in [3] the CSAS outperformed the TPSD in 4 of\nthe 6 tasks.\n3. HARMONY MODEL\nThe HARM TRACE harmony model implements and extends\nthe ideas of Rohrmeier [16,17]. However, HARM TRACE dif-\nfers from Rohrmeierâ€™s grammar in several aspects. Rohr-\nmeierâ€™s model is more elaborate, as it includes phrasing and\nmodulation. However, we believe that modulation and phras-\ning cannot be implemented as context-free rules in the way\nRohrmeier formulates them. Rohrmeierâ€™s CFG allows for\nmodulating into any key at any point in a sequence; from an\nimplementation perspective, this would generate too many\nambiguous solutions for a single sequence of chords. Fur-\nthermore, whereas Rohrmeierâ€™s grammar aims to explain\nthe core rules of tonal harmony, our model exhibits a bias\ntowards jazz harmony, due to the nature of the data used in\nSection 5.\nWe model tonal harmony as a complex Haskell datatype.\nTo explain our model in a clear manner, that does not re-\n6812th International Society for Music Information Retrieval Conference (ISMIR 2011)\nPiece\nDom\nV7\nG7Ton\nIm\nCm6Dom\nDom\nDom\nV7\nG7/sharp5/sharp9Sub\nIIm\nDm7/flat5Sub\nII/flat\nD/flatâˆ†V/II/flat7\nVI/flat7\nA/flat7V/VI/flatm\nIII/flatm\nE/flatm7Ton\nIm\nCm6Dom\nDom\nDom\nV7\nG7/sharp5/sharp9Sub\nIIm\nDm7/flat5Sub\nIVm\nFm7Ton\nIm\nCm6\nFigure 2 . An analysis of the jazz standard Blue Bossa in\nC minor. Every chord belongs to a Tonic, Dominant, or\nSubdominant category ( Ton,Dom , orSub) and the V/X7\ndenote chains of secondary dominants.\nquire elaborate knowledge of the Haskell programming lan-\nguage, we chose a syntax that closely resembles a (very con-\nstrained) CFG. A CFG deï¬nes a language: it accepts only\ncombinations of words that are valid sequences of the lan-\nguage. A collection of Haskell datatypes can be viewed as\na very powerful CFG: the type-checker accepts a combina-\ntion of values if their structure matches the structure pre-\nscribed by the datatype, and rejects this combination if it\ndoes not. Within HARM TRACE , the chords are the values and\nthe datatypes represent the relations between the structural\nelements in tonal harmony.\n3.1 A model of Western tonal harmony\nFigure 2 shows an example analysis as produced by HARM -\nTRACE . We start by introducing a variable (denoted with\nbold font) for the mode of the key of the piece, which can\nbe major or minor. The mode variable is used to parametrise\nall the speciï¬cations of our harmony model; some speciï¬-\ncations hold for both modes ( m), while other speciï¬cations\nhold only for the major (Maj) or minor mode (Min). The\nmode is displayed as a subscript, which we leave out when\nit is clear from the context. Currently, HARM TRACE cannot\nyet derive the key of the piece automatically. Hence, to be\nable to use key-relative representations, external informa-\ntion about the key of the piece is essential.\n1Piece mâ†’Func+m\n2Func mâ†’Tonm|Dom m\n3Dom mâ†’SubmDom mmâˆˆ{Maj,Min}\nSpec. 1â€“3 deï¬ne that a valid chord sequence, Piece m, con-\nsists of at least one and possibly more functional categories.\nA functional category classiï¬es chords as being part of a\ntonic ( Tonm), dominant ( Dom m), or subdominant ( Sub m)\nstructure, where a subdominant must always precede a dom-\ninant. The order of the dominants and tonics is not con-\nstrained by the model, and they are not grouped into larger\nphrases.4Ton Majâ†’IMaj|IMajIVMajIMaj\n5Ton Minâ†’Im\nMin|Im\nMinIVm\nMinIm\nMin\n6Dom mâ†’V7m|Vm\n7Sub Majâ†’IVm\nMaj|IIm\nMaj|...\n8Sub Minâ†’IVMin|IIm\nMin|...câˆˆ{âˆ…,m,7,0}\nSpec. 4â€“8 translate dominants, tonics, and sub-dominants\ninto scale degrees (denoted with Roman numerals). A scale\ndegree is a datatype that is parametrised by a mode, a chord\nclass, and the interval between the chord root and the key.\nThe chord class is used to constrain the application of certain\nspeciï¬cations, e.g. Spec. 13 and 14, and can represent the\nclass of major (no superscript), minor ( m), dominant seventh\n(7), and diminished seventh chords (0). A tonic translates\ninto a ï¬rst degree in both major and minor mode, albeit with\na minor triad in the latter case, or it allows for initiation of a\nplagal cadence. A dominant type is converted into the ï¬fth\nor seventh scale degree with a dominant or diminished class,\nrespectively. Similarly, a sub-dominant is converted into the\nfourth or second degree.\n9IMajâ†’\"C:maj\"|\"C:maj6\"|\"C:maj7\"|...\n10Im\nMinâ†’\"C:min\"|\"C:min7\"|\"C:min9\"|...\n11V7mâ†’\"G:7\"|\"G:7(b9,13)\"|\"G:(#11)\"|...\n12VII0mâ†’\"B:dim(bb7)\"\nFinally, scale degrees are translated into the actual sur-\nface chords that are used as input for the model. The chord\nnotation used is that of Harte et al. [7]. The conversions are\ntrivial and illustrated by a small number of speciï¬cations\nabove. The model uses a key-relative representation, and\nin Spec. 9â€“12 we used chords in the key of C. Hence, a IMaj\ntranslates to the set of C chords with a major triad, option-\nally augmented with additional chord notes that do not make\nthe chord minor or dominant. Similarly, V7\nMajtranslates to\nall G chords with a major triad and a minor seventh, etc.\n13Xcmâ†’V/X7mXcm\n14X7mâ†’V/XmmX7mcâˆˆ{âˆ…,m,7,0}\nXâˆˆ{I,II/flat,II,..., VII}\nSpec. 13 accounts for the classical preparation of a scale\ndegree by its secondary dominant, stating that every scale\ndegree, independently of its mode, chord class, and root in-\nterval, can be preceded by a chord of the dominant class,\none ï¬fth up. The function V/Xwhich transposes an arbi-\ntrary scale degree Xa ï¬fth up. Similarly, every scale de-\ngree of the dominant class can be prepared with the minor\nchord one ï¬fth above (Spec. 14). These two speciï¬cations\ntogether allow for the derivation of the typical and promi-\nnently present ii-V motions in jazz harmony.\n15X7mâ†’V/flat/X7m\n16X7mâ†’II/flat/X0m\n69Poster Session 1\n17X0mâ†’III/flat/X0m\nThe harmony model in HARM TRACE further allows vari-\nous scale degree transformations. Every dominant chord can\nbe transformed into its tritone substitution with Spec. 15.\nThis speciï¬cation uses another transposition function V/flat/X\nwhich transposes a scale degree Xa diminished ï¬fthâ€”a\ntritoneâ€”up. Likewise, diminished seventh chords are treated\nas regular dominant seventh chords without a root and with\na/flat9 (rule 16). For instance, an A /flat0, consisting of A /flat, B, D,\nand F, is viewed as a G7/flat9, which consists of G, B, D, F, and\nA/flat0. An exceptional characteristic of diminished seventh\nchordsâ€”consisting only of notes separated by minor third\nintervalsâ€”is that they are completely symmetrical. Hence,\na diminished seventh chord has four enharmonic equivalent\nchords that can be reached by transposing the chord a minor\nthird up with the transposition function III/flat/X(Spec. 17).\nBecause we want the application of the Spec. 13â€“17 to ter-\nminate, we limit the number of recursive applications of\nthese rules. For the technical details about how this is done,\nwe refer to [10].\nWe have presented a condensed view on the core spec-\niï¬cations of the model, but due to space limitation we had\nto omit some speciï¬cation for diatonic chains of ï¬fths, bor-\nrowings from the parallel mode and the Neapolitan chord\n(see Figure 2). For the full speciï¬cation of the model we\nrefer to [2] and to the code bundle found online.2\n3.2 Parsing\nHaving a formal speciï¬cation as a datatype, the next step is\nto deï¬ne a parser to transform textual chord labels into val-\nues of our datatype. Writing a parser that parses labels into\nour datatype would normally mean writing tedious code that\nclosely resembles the datatype speciï¬cation. However, in\nHaskell we can use datatype-generic programming3tech-\nniques to avoid writing most of the repetitive portions of the\ncode. Moreover, not only the parser can be derived auto-\nmatically, but also a pretty-printer for displaying the har-\nmony analysis in tree form, and functions for comparing\nthese analyses. This makes the development and ï¬ne-tuning\nof the model much easier, as only the datatype speciï¬cations\nhave to be changed, and the code adapts itself automatically.\nFor technical details of the implementation and the generic\nprogramming techniques we refer to [10].\nBecause music is an ever changing, culturally dependent,\nand extremely diverse art form, we cannot hope to model\nall valid harmonic relations in our datatype. Furthermore,\nsongs may contain mistakes or mistyped chords, perhaps\nfeature extraction noise, or malformed data of dubious har-\nmonic validity. This is problematic if we reject chord se-\nquences that do not ï¬t the grammatical speciï¬cation without\n2http://hackage.haskell.org/package/HarmTrace-0.7\n3Not to be confused with regular polymorphism, as in Java generics.returning any information about harmony analysis. How-\never, these problems often occur at a speciï¬c position in the\npiece and most of the song still makes sense. In HARM -\nTRACE we use a parsing library [21] that features error-cor-\nrection: chords that do not ï¬t the structure are automatically\ndeleted or preceded by inserted chords, according to heuris-\ntics computed from the grammar structure. For most songs,\nparsing proceeds with none or very few corrections. Songs\nwith a very high error ratio denote bad input or wrong key\nassignment, which results in meaningless scale degrees.\nMusic, and harmony in particular, is intrinsically am-\nbiguous. Hence, certain chords can have multiple meanings\nwithin a tonal context. This is reï¬‚ected in the model above.\nWe control the number of possible analyses by constrain-\ning the application of most speciï¬cations. Examples hereof\nare the restriction of secondary dominants to scale degrees\nof the dominant class, and limiting the number of possible\nrecursive applications of the secondary dominant rule.\n4. SIMILARITY ESTIMATION\nAfter having obtained an harmonic analysis from our model,\na chord is categorised as being part of either a dominant,\nsub-dominant, or tonic structure (Spec. 4â€“8). Furthermore,\nwe also annotate whether a chord is part of secondary dom-\ninant preparation (Spec. 13â€“14) and label whether it has\nbeen transformed (Spec. 15â€“17). We hypothesise that these\nannotations are helpful in determining harmonic similarity.\nHence, we represent an annotated chord as a quintuple of\nthe following form: ( X,c,func,prep,trans ), where Xrepre-\nsents a scale degree, ca chord class (as deï¬ned in Section 3),\nfunc the functional category, prep the functional preparation,\ne.g. being part of a secondary dominant (V/X), and trans a\nscale degree transformation, e.g. a tritone or diminished sev-\nenth substitution. For estimating the similarity between two\nsequences of these annotated chords we calculate the align-\nment score obtained in a classical alignment procedure [19].\nThe quality of an alignment heavily depends on the inser-\ntion, deletions, match, and mismatch parameters. We use a\nconstant insertion and deletion penalty of âˆ’2 and we deï¬ne\nthe similarity between the annotated chords as a function,\nsim(ai,bj)â†’[âˆ’1,6], that takes a pair of chords, aiandbj,\nand returns an integer denoting the (dis-) similarity. Here i\nandjdenote the beat position of aiandbjin the compared\nchord sequences AandB.\nsim(X1,c1,func1,prep1,trans 1) (X2,c2,func2,prep2,trans 2) =\nifX1â‰¡X2âˆ§c1â‰¡c2then 2+mprep+mtranselseâˆ’1\nwhere mprep=simprep(Prep1,Prep2)\nmtrans=ifTrans 1â‰¡Trans 2then 1else0\nWithin sim, the function sim prepâ†’[0,3]compares two pos-\nsible scale degree preparations, returning 3 is the prepara-\ntion is identical, 2 if both preparations involve the same ï¬fth\n7012th International Society for Music Information Retrieval Conference (ISMIR 2011)\njump, 1 if they are both a preparation, and 0 in all other\ncases.\nThe ï¬nal similarity score is obtained by calculating the\noptimal alignment between two annotated chord sequences\nand normalising the alignment score. Because the preï¬x of\nan optimal alignment is also an optimal alignment, an op-\ntimal solution can be found by exploiting the dynamic pro-\ngramming paradigm. To ensure that the alignment is max-\nimal, we construct an array Twhich stores the cumulative\nalignment score so far. Tis ï¬lled by calculating the recur-\nrence below for every combination of annotated chords in\nthe sequence AandBin a standard dynamic programming\nprocedure.\nT[i,j] =maxï£±\nï£´ï£´ï£²\nï£´ï£´ï£³M[i,jâˆ’1]âˆ’2,\nM[iâˆ’1,j]âˆ’2,\nM[iâˆ’1,jâˆ’1]+sim(ai,bj),\n0\nThe actual alignment can be obtained by keeping track of\nthe path trough T, starting at T[n,m], where nandmare the\nsizes of AandB, respectively. We obtain our ï¬nal similar-\nity measure, SIM(A,B)â†’[0,1], by normalising the sum of\nalignment scores, T[n,m], by the sizes of AandB:\nSIM(A,B) =T[n,m]\nnÂ·T[n,m]\nm\n5. EV ALUATION\nTo evaluate the effect of the HARM TRACE harmony model on\nretrieval performance, we compare it to a baseline alignment\nsystem, named TRIAD ALIGN. In TRIAD ALIGN we use the\nexact same alignment code, but the similarity function for\nindividual chords, sim, is replaced by simtriadthat does not\nuse any additional model information.\nsimtriad(X1,triad 1) (X2,triad 2) =\nifX1â‰¡X2âˆ§triad 1â‰¡triad 2then 4elseâˆ’1\nHere, triad denotes only whether the chord is major or mi-\nnor, and the Xrepresents the scale degree, as deï¬ned in the\nprevious sections. Note that the TRIAD ALIGN system is very\nsimilar to the CSAS , but uses slightly different parameters\nand normalises the alignment score.\nWe compare the retrieval performance of HARM TRACE ,\nTRIAD ALIGN,TPSD , and CSAS methods (see Section 2) in\na retrieval experiment for which we use the same chord se-\nquence corpus as in [3]. This corpus consists of 5,028 unique\nuser-generated Band-in-a-Box ï¬les that are collected from\nthe Internet. Band-in-a-Box [1] is a commercial software\npackage for generating musical accompaniment based on a\nlead sheet. For extracting the chord label information from\nthe Band-in-a-Box ï¬les we have extended software in [12].TPSD CSAS T RIAD ALIGN HARM TRACE\nMAP 0 .580 0 .696 0 .711 0 .722\nTable 1 . The mean average precision of the rankings based\non the compared similarity measures.\nWithin the corpus, 1,775 songs contain two or more sim-\nilar versions, forming 691 classes of songs. Within a song\nclass, songs have the same title and share a similar melody,\nbut may differ in a number of ways. They may, for instance,\ndiffer in key and form, in the number of repetitions, or may\nsimply use different chords at certain positions. Having\nmultiple chord sequences describing the same song allows\nfor setting up a cover-song -ï¬nding experiment. The title of\nthe song is used as ground-truth and the retrieval challenge\nis to ï¬nd the other chord sequences representing the same\nsong. Although the dataset was automatically ï¬ltered to ex-\nclude identical or erroneous pieces, it still includes many\nsongs that are harmonically atypical. The reason for this is\nthat the ï¬les are user-generated, and contain peculiar and un-\nï¬nished pieces, wrong key assignments, and other errors; it\ncan therefore be considered a â€œreal lifeâ€ dataset. The chord\nsequence corpus is available to the research community on\nrequest.\nWe analyse the rankings obtained from the compared sim-\nilarity measures by calculating the Mean Average Precision\n(MAP). The MAP is the average precision averaged over\nall queries, and is a single-ï¬gure measure between 0 and\n1 [11, Chap. 8, p. 160]. We tested whether the differences in\nMAP are signiï¬cant by performing a non-parametric Fried-\nman test with a signiï¬cance level of Î±=0.05. We chose the\nFriedman test because the underlying distribution of the data\nis unknown, and, in contrast to an ANOV A, the Friedman\ndoes not assume a speciï¬c distribution of variance.4To de-\ntermine which pairs of measurements differ signiï¬cantly we\nconducted a post-hoc Tukey HSD test. This way of signiï¬-\ncance testing is standard in MIREX.\nThe MAP scores are displayed in Table 1. There are sig-\nniï¬cant differences between the runs, Ï‡2(3,N=1775) =\n350, p<0.0001 and also all pairwise differences are statisti-\ncally signiï¬cant. Hence, we can conclude that HARM TRACE\nsigniï¬cantly outperforms the other similarity measures, and\nthat using the harmonic information obtained by our model\nimproves similarity estimation on this dataset.\n6. DISCUSSION\nThe results show that using information about the function\nof a chord improves harmonic similarity. However, not all\nharmony annotations appeared to be beneï¬cial: although in\nour experiments the functional categories ( Ton,Dom ,Sub)\n4All statistical tests were performed in Matlab 2009a.\n71Poster Session 1\ndid not have a negative effect on the similarity estimation,\nthey did not improve the harmonic similarity either. Perhaps\nthe categories are not distinctive enough to be advantageous.\nWe noticed that similarity measures that did not easily clas-\nsify chords as similar performed best.\nThe retrieval task of Section 5 is a difï¬cult one because\nthe song class sizes are very small. Often there is only one\nrelated piece in the corpus, and ï¬nding it based on its har-\nmony alone is challenging. We believe that this is a sound\nway of evaluating of harmonic similarity, since nothing else\ncould have inï¬‚uence the results but the chords available in\nthe data. Nevertheless, it is stimulating to think about other\nways of evaluating harmonic similarity that go beyond the\nconcept of a cover-song. A fundamental problem is that cur-\nrently there is no good ground-truth that actually captures\nthe harmonic similarity on a gradual (non-binary) scale. But\nhow should such a ground-truth be established: by perform-\ning a large scale user study, or by consulting musical ex-\nperts? These questions remain unanswered, and pose chal-\nlenges for future MIR research.\nBesides similarity estimation, a model of tonal harmony\nmight be useful for answering other MIR-related questions.\nFor instance, chord labelling or optical music recognition\nsystems often recognise chords from audio or score data.\nOur model could be used to suggest harmonically-ï¬tting so-\nlutions when there is high uncertainty in the data. Another\npotential application of HARM TRACE would be in the gener-\nation of harmonically well-formed chord sequences for soft-\nware such as Band-in-a-Box. The TPSD and CSAS do not\noffer such beneï¬ts.\nThe many possible applications of harmony models, like\nthe one in HARM TRACE , together with its positive results\nin retrieval performance, make us believe that formalising\ntonal harmony is crucial in understanding the true nature of\nmusical harmony and harmonic similarity.\nAcknowledgements This work has been partially funded\nby the Dutch ICES/KIS III Bsik project MultimediaN, and\nby the Portuguese Foundation for Science and Technology\n(FCT) via the SFRH/BD/35999/2007 grant. We thank Mar-\ntin Rohrmeier for all fruitful discussions, Anja V olk for her\nhelpful comments on an earlier draft, and the anonymous\nreviewers for their constructive comments and suggestions.\n7. REFERENCES\n[1] P. Gannon. Band-in-a-Box. PG Music, 1990.\n[2] W. B. de Haas, J. P. Magalh Ëœaes, F. Wiering, and R. C.\nVeltkamp. HarmTrace: Automatic functional harmonic analy-\nsis. Technical Report UU-CS-2011-023, Department of Infor-\nmation and Computing Sciences, Utrecht University, 2011.\n[3] W. B. de Haas, M. Robine, P. Hanna, R. C. Veltkamp, and\nF. Wiering. Comparing harmonic similarity measures. In Pro-\nceedings of the 7th International Symposium on Computer Mu-\nsic Modeling and Retrieval , pages 299â€“315, June 2010.[4] W. B. de Haas, M. Rohrmeier, R. C. Veltkamp, and F. Wiering.\nModeling harmonic similarity using a generative grammar of\ntonal harmony. In Proceedings of ISMIR , 2009.\n[5] W. B. de Haas, R. C. Veltkamp, and F. Wiering. Tonal pitch\nstep distance: A similarity measure for chord progressions. In\nProceedings of ISMIR , pages 51â€“56, 2008.\n[6] P. Hanna, M. Robine, and T. Rocher. An alignment based sys-\ntem for chord sequence retrieval. In Proceedings of the 2009\nJoint International Conference on Digital Libraries , pages\n101â€“104. ACM New York, NY , USA, 2009.\n[7] C. Harte, M. Sandler, S. Abdallah, and E. G Â´omez. Symbolic\nrepresentation of musical chords: A proposed syntax for text\nannotations. In Proceedings of ISMIR , pages 66â€“71, 2005.\n[8] F. Lerdahl. Tonal Pitch Space . Oxford University Press, 2001.\n[9] F. Lerdahl and R. Jackendoff. A Generative Theory of Tonal\nMusic . MIT Press, 1996.\n[10] J. P. Magalh Ëœaes and W. B. de Haas. Functional Modelling of\nMusical Harmonyâ€”an Experience Report. In Proceedings of\nthe 16th ACM SIGPLAN International Conference on Func-\ntional Programming , 2011.\n[11] C.D. Manning, P. Raghavan, and H. Sch Â¨utze. Introduction to\nInformation Retrieval . Cambridge University Press, New York,\nNY , USA, 2008.\n[12] M. Mauch, S. Dixon, C. Harte, M. Casey, and B. Fields. Dis-\ncovering chord idioms through Beatles and Real book songs.\nInProceedings of ISMIR , pages 255â€“258, 2007.\n[13] J. Pickens and T. Crawford. Harmonic models for polyphonic\nmusic retrieval. In Proceedings of the 11th International Con-\nference on Information and Knowledge Management , pages\n430â€“437. ACM New York, NY , USA, 2002.\n[14] H. Riemann. Vereinfachte Harmonielehre; oder, die Lehre von\nden tonalen Funktionen der Akkorde . Augener, 1893.\n[15] C. Roads. Grammars as representations for music. Computer\nMusic Journal , 3(1):48â€“55, 1979.\n[16] M. Rohrmeier. A generative grammar approach to diatonic har-\nmonic structure. In Anagnostopoulou Georgaki, Kouroupet-\nroglou, editor, Proceedings of the 4th Sound and Music Com-\nputing Conference , pages 97â€“100, 2007.\n[17] M. Rohrmeier. Towards a generative syntax of tonal harmony.\nJournal of Mathematics and Music , 5(1), 2011.\n[18] H. Schenker. Der Freie Satz. Neue musikalische Theorien und\nPhantasien, 1935.\n[19] T. F. Smith and M. S. Waterman. Identiï¬cation of Com-\nmon Molecular Subsequences. Journal of Molecular Biology ,\n147:195â€“197, 1981.\n[20] M. J. Steedman. A Generative Grammar for Jazz Chord Se-\nquences. Music Perception , 2(1):52â€“77, 1984.\n[21] S. D. Swierstra. Combinator Parsing: A Short Tutorial , pages\n252â€“300. Springer-Verlag, 2009.\n[22] D. Temperley. The cognition of basic musical structures . Cam-\nbridge, MA, MIT Press, 2001.\n72"
    },
    {
        "title": "Temporal Pooling and Multiscale Learning for Automatic Annotation and Ranking of Music Audio.",
        "author": [
            "Philippe Hamel",
            "Simon Lemieux",
            "Yoshua Bengio",
            "Douglas Eck"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1418237",
        "url": "https://doi.org/10.5281/zenodo.1418237",
        "ee": "https://zenodo.org/records/1418237/files/HamelLBE11.pdf",
        "abstract": "This paper analyzes some of the challenges in performing automatic annotation and ranking of music audio, and proposes a few improvements. First, we motivate the use of principal component analysis on the mel-scaled spectrum. Secondly, we present an analysis of the impact of the selection of pooling functions for summarization of the features over time. We show that combining several pooling functions improves the performance of the system. Finally, we introduce the idea of multiscale learning. By incorporating these ideas in our model, we obtained state-of-the-art performance on the Magnatagatune dataset.",
        "zenodo_id": 1418237,
        "dblp_key": "conf/ismir/HamelLBE11",
        "keywords": [
            "Automatic Annotation",
            "Music Audio Ranking",
            "Principal Component Analysis",
            "Mel-Scaled Spectrum",
            "Feature Pooling",
            "Multiscale Learning",
            "Temporal Summarization",
            "Music Recommendation",
            "Playlist Generation",
            "Magnatagatune Dataset",
            "Deep Learning"
        ],
        "content": "TEMPORAL POOLING AND MULTISCALE LEARNING FOR AUTOMATIC\nANNOTATION AND RANKING OF MUSIC AUDIO\nPhilippe Hamel, Simon Lemieux, Yoshua Bengio\nDIRO, UniversitÃ© de MontrÃ©al\nMontrÃ©al, QuÃ©bec, Canada\n{hamelphi,lemiesim,bengioy}@iro.umontreal.caDouglas Eck\nGoogle Inc.\nMountain View, CA, USA\ndeck@google.com\nABSTRACT\nThis paper analyzes some of the challenges in performing\nautomatic annotation and ranking of music audio, and pro-\nposes a few improvements. First, we motivate the use of\nprincipal component analysis on the mel-scaled spectrum.\nSecondly, we present an analysis of the impact of the selec-\ntion of pooling functions for summarization of the features\nover time. We show that combining several pooling func-\ntions improves the performance of the system. Finally, we\nintroduce the idea of multiscale learning. By incorporating\nthese ideas in our model, we obtained state-of-the-art per-\nformance on the Magnatagatune dataset.\n1. INTRODUCTION\nIn this paper, we consider the tasks of automatic annotation\nand ranking of music audio. Automatic annotation consists\nof assigning relevant word descriptors, or tags, to a given\nmusic audio clip. Ranking, on the other hand, consists of\nï¬nding an audio clip that best corresponds to a given tag,\nor set of tags. These descriptors are able to represent a wide\nrange of semantic concepts such as genre, mood, instrumen-\ntation, etc. Thus, a set of tags provides a high-level descrip-\ntion of an audio clip. This information is useful for tasks like\nmusic recommendation, playlist generation and measuring\nmusic similarity.\nIn order to solve automatic annotation and ranking, we\nneed to build a system that can extract relevant features from\nmusic audio and infer abstract concepts from these features.\nMany content-based music recommendation systems follow\nthe same recipe with minor variations (see [5] for a review).\nFirst, some features are extracted from the audio. Then,\nthese features are summarized over time. Finally, a classiï¬-\ncation model is trained over the summarized features to ob-\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proï¬t or commercial advantage and that copies\nbear this notice and the full citation on the ï¬rst page.\nc\r2011 International Society for Music Information Retrieval.tain tag afï¬nities. We describe several previous approaches\nthat follow these steps and have been applied to the Mag-\nnatune dataset [13] in Section 3.1. We then present an ap-\nproach that deviates somewhat from the standard recipe by\nintegrating learning steps before and after the temporal sum-\nmarization.\nThis paper has three main contributions. First, we de-\nscribe a simple adaptive preprocessing procedure of the mu-\nsic audio that incorporates only little prior knowledge on\nthe nature of music audio. We show that the features ob-\ntained through this adaptive preprocessing give competitive\nresults when using a relatively simple classiï¬er. Secondly,\nwe study the impact of the selection and mixing of pool-\ning functions for summarization of the features over time.\nWe introduce the idea of using min-pooling in conjunction\nwith other functions. We show that combining several pool-\ning functions improves the performance of the system. Fi-\nnally, we incorporate the idea of multiscale learning. In or-\nder to do this, we integrate feature learning, time summa-\nrization and classiï¬cation in one deep learning step. Using\nthis method, we obtain state-of-the-art performance on the\nMagnatagatune dataset.\nThe paper is divided as follows. First, we motivate our\nexperiments in Section 2. Then, we expose our experimen-\ntal setup in Section 3. We present and discuss our results in\nSection 4. Finally, we conclude in Section 5.\n2. MOTIVATION\n2.1 Choosing the right features\nChoosing the right features is crucial for music classiï¬ca-\ntion. Many automatic annotation systems use features such\nas MFCCs [8,12] because they have shown their worth in the\nspeech recognition domain. However, music audio is very\ndifferent from speech audio in many ways. So, MFCCs,\nwhich have been engineered for speech analysis might not\nbe the optimal feature to use for music audio analysis.\nAlternatives have been proposed to replace MFCCs. Re-\ncent work have shown that better classiï¬cation performance\ncan be achieved by using mel-scaled energy bands of thespectrum [4]. Octave-based spectral contrast features [11]\nhave been shown to also outperform MFCCs for genre clas-\nsiï¬cation. Thus, ï¬nding optimal features for audio classiï¬-\ncation is still an open problem.\nIn section 3.3, we present a relatively simple audio pre-\nprocessing based on spectral energy bands and principal com-\nponent analysis (PCA).\n2.2 Summarization of the features over time\nAnother important aspect of any automatic tagging system\nworking on music audio is the question of the summariza-\ntion of features over time, potentially allowing one to map a\nvariable-length sequence into a ï¬xed-size vector of features\nthat can be fed to a classiï¬er. The objective of summariza-\ntion is to transform a joint feature representation into a more\nuseful one that preserves important information while dis-\ncarding noise, redundancy or irrelevant information. Sum-\nmarizing features either in space (e.g. in visual recognition),\nor in time (e.g. in audio analysis) yields representations that\nare compact, invariant to shifts in space or time and robust\nto clutter.\nOne of the most straightforward ways to summarize fea-\ntures is feature pooling. Pooling consists in extracting sim-\nple statistics such as the mean or maximum of the features\nover an excerpt of a given time length. The choice of the\npooling function has a great impact on the performance of\nthe system. In [7], feature pooling in the domain of visual\nrecognition is analyzed. The authors come to the conclu-\nsion that, depending on the data and features, neither max-\npooling or mean-pooling might be optimal, but something\nin between might be. This underlines the importance of a\nthorough analysis of pooling functions for the speciï¬c task\nof music audio classiï¬cation.\nThe choice of the temporal scale at which the pooling is\napplied also has a great impact on a systemâ€™s performance.\nIf we choose a time-scale that is too long, we discard too\nmuch information in the process, and the performance of\nthe system suffers. If we choose a time-scale that is too\nsmall, the representation becomes less compact and looses\nthe temporal shift invariance. It is possible to use onset\ndetection to determine an optimized aggregation window\nlength [20]. However, this method relies on onset detection\nmethods which are not always reliable in all types of music.\n2.3 Feature Learning and Deep Learning\nIt has been argued that features extracted by task-speciï¬c\nsignal processing might be replaced by features learned over\nsimpler low-level features, i.e., for object recognition [2,\n15]. For instance, features learned with a Deep Belief Net-\nwork over spectral amplitudes has been shown to outper-\nform MFCCs for genre recognition and automatic annota-\ntion [10, 16].Feature learning consists in exploiting the structure of the\ndata distribution to construct a new representation of the in-\nput. This representation can be considered as a set of la-\ntent variables within a probabilistic model of the input. The\ntransformation can be learned via unsupervised or super-\nvised learning. Feature learning allows one to build systems\nrelying less on prior knowledge and more on data, which\ngrants more ï¬‚exibility to adapt to a given task.\nDeep learning algorithms attempt to discover multiple\nlevels of features or multiple levels of representation. Sev-\neral theoretical results and arguments [1] suggest that shal-\nlow architectures (with 1 or 2 levels, as in SVMs with a\nï¬xed kernel, for example) may be less efï¬cient at represent-\ning functions that can otherwise be represented compactly\nby a deep architecture. The advantage of a deep architecture\nis that concepts or features at one level can be represented\nby combining features at lower levels, and these low-level\nfeatures can be re-used in exponentially many ways as one\nconsiders deep architectures.\nConvolutional Neural Networks (CNN) [15] were the ï¬rst\ndeep models to be applied successfully to real-world prob-\nlems such as character recognition. CNNs present a hier-\narchical structure. Inserting a feature pooling layer between\nconvolutional layers allows different layers of the network to\nwork at different time scales and introduces more and more\ntranslation invariance (as well as robustness to other kinds\nof local distortions) as one moves up the hierarchy of the\narchitecture. Hierarchical network structures such as CNNs\nseem ideal for representing music audio, since music also\ntends to present this hierarchical structure in time and dif-\nferent features of the music may be more salient at different\ntime scales. Thus, in Section 3.5.2, we propose a hierachical\nmodel strongly inspired by CNNs.\n3. EXPERIMENTAL SETUP\n3.1 Magnatagatune Dataset\nThe Magnatagatune dataset consists of 29-second clips with\nannotations that were collected using an online game called\nTagATune. This dataset was used in the MIREX 2009 con-\ntest on audio tag classiï¬cation [14]. In our experiments, we\nused the same set of tags and the same train/test split as in\nthe contest. The training, valid and test set were composed\nof 14660, 1629 and 6499 clips respectively. The clips were\nannotated with a set of 160 tags, each clip being associated\nwith between 1 and 30 tags.\nWe describe here the systems used by the four best con-\ntestants: Marsyas [19], Mandel [17], Manzagol [18] and\nZhi [9]. All submissions use MFCCs as features, except\nfor Mandel, which instead uses a cepstral transform that is\nclosely related to MFCCs. Mandel also computes a set of\ntemporal features. In addition, Marsyas includes a set of\nspectral features: spectral centroid, rolloff and ï¬‚ux. Zhiuses Gaussian Mixture Models to obtain a song-level rep-\nresentation and uses a semantic multiclass labeling model.\nManzagol summarizes the features with vector quantization\n(VQ) and applies an algorithm called PAMIR (passive-aggressive\nmodel for image retrieval). Mandel trains balanced SVMs\nfor each tag. Finally, Marsyas uses running means and stan-\ndard deviations of the features as input to a two-stage SVM\nclassiï¬er.\n3.2 Performance evaluation\nTo evaluate the performance of our model, we compute the\nArea Under the ROC Curve (AUC). The ROC curve of a\nclassiï¬er is deï¬ned by the ratio of true positives over the\npositive outputs in function of the ratio of false positives\nover the negative outputs. The AUC gives the probability\nthat, given one random positive and one random negative\nexample, the classiï¬er will rank the positive one higher than\nthe negative one. Since the AUC is deï¬ned for a binary\nclassiï¬cation, and our task requires multi-label classiï¬ca-\ntion, there are two ways we can compute the AUC. By com-\nputing the average of the AUC for each tag (AUC-tag), we\nobtain a global measure of how good a classiï¬er is at rank-\ning clips given a tag (e.g. Which clip is more â€™Reggaeâ€™?).\nAlternatively, we can compute the average of the AUC for\neach clip (AUC-clip) to obtain a measure of how good clas-\nsiï¬er is at ranking tags for a given clip (e.g. Is this clip more\nâ€™sadâ€™ or â€™metalâ€™ ?).\nAnother measure which is closely related to the AUC is\nthe precision at kwhere kis an integer. Given an ordered list\nof tags for a clip, it is deï¬ned by the ratio of true positives\nin the top kpositions.\n3.3 Audio Preprocessing\nOur audio preprocessing involves three steps: discrete Fourier\ntransform (DFT), mel-compression and principal component\nanalysis whitening (PCA).\nFirstly, to transform the audio in the spectral domain, we\ncompute DFTs over windows of 1024 samples on audio at\n22.1 KHz (i.e. roughly 46ms) with a frame step of 512 sam-\nples. Then, we run the spectral amplitudes through a set\nof 128 mel-scaled triangular ï¬lters to obtain a set of spec-\ntral energy bands. We compute the principal components\nof a random sub-sample of the training set and throw away\nonly the components with very low variance (low eigenval-\nues), yielding 120 components in total. In order to obtain\nfeatures with unitary variance, we multiply each component\nby the inverse square root of its eigenvalue, a transforma-\ntion known as PCA whitening. We will refer to the pre-\nprocessed audio features as Principal Mel-Spectrum Com-\nponents (PMSC).3.4 Pooling functions\nIn our experiments, we used a set of pooling functions and\nsome of their combinations. The functions we used are:\nmean, variance (var), maximum (max), minimum (min), and\n3rd and 4th centered moments. The ith centered moment is\ndeï¬ned by:1\nnPn\ni=1(x\u0000\u0016x)i. By this deï¬nition, the variance\ncorresponds to the second centered moment.\n3.5 Models\nWe used two different models in our experiments. The ï¬rst\none, described in Section 3.5.1, is a rather conventional sys-\ntem that applies feature extraction, pooling and classiï¬ca-\ntion in three separate steps. The second one, described in\nSection 3.5.2, applies learning both before and after the tem-\nporal pooling. The models are illustrated in Figure 1.\n3.5.1 Pooled Features Classiï¬er (PFC)\nThe ï¬rst model we evaluate applies a given set of pooling\nfunctions to the PMSC features, and sends the pooled fea-\ntures to a classiï¬er. Each pooling window is considered as a\ntraining example for the classiï¬er, and we average the pre-\ndictions of the classiï¬er over all the windows of a given clip\nto obtain the ï¬nal classiï¬cation. The classiï¬er is a single\nhidden layer neural network, also known as multi-layer per-\nceptron (MLP). We used a hidden layer of 1000 units, sig-\nmoid activation, L2 weight decay and cross-entropy cost.\nWe chose to use the MLP as a classiï¬er for three main rea-\nsons. First, the hidden layer of the MLP should allow the\nmodel to learn dependencies between tags. Second, the MLP\ntraining time scales well (sub-linearly) with the size of the\ntraining set. Third, neural networks such as the MLP allows\ngreat ï¬‚exibility in the structure of the network. This will al-\nlow us to extend the model to a multiscale structure, as we\nwill see in section 3.5.2. We will refer to this model as the\nPooled Features Classiï¬er (PFC) model.\n3.5.2 Multi-Time-Scale Learning model (MTSL)\nThe second model is structurally similar to the ï¬rst one, ex-\ncept for the fact that we add a hidden layer between the in-\nput features and the pooling function. Thus the pooling is\nnow applied on the activation of this new hidden layer. In\nthis manner, the model is able to learn a representation of\nthe features to be pooled. The weights connecting the input\nto the ï¬rst layer are shared across all frames. We keep the\nsame MLP structure as in the PFC model on top of the pool-\ning. As for the PFC model, learning is purely supervised.\nDuring training, the error is back-propagated from the MLP,\nthrough the pooling functions, down to the ï¬rst hidden layer.\nThus, it is required to choose pooling functions for which a\ngradient can be deï¬ned, which is the case for all the func-\ntions described in section 3.4.MLP\nPMSCUHidden Layer\n(learned frame-level features)\nPooled Features\nPoolingV\nMLP Hidden Layer\n(learned pool-level features)\nT ag predictions\nW\nMLP\nPMSC\nPooled Features\nPoolingV\nMLP Hidden Layer\n(learned pool-level features)\nT ag predictions\nWPFC Model MTSL ModelFigure 1 .Comparison of the PFC and the MTSL model. Upward\narrows represent the ï¬‚ow of feed-forward information. Downward\narrows illustrate the ï¬‚ow of the error back-propagation. U,Vand\nWare weight matrices to be learned.\nIn this model, while the ï¬rst layer is learning on frames\nat a time scale of about 46ms, the second layer works at the\nscale of the pooling window. Since this model learns on\ndifferent time scales, we will refer to it as the Multi-Time-\nScale Learning (MTSL) model.\n4. RESULTS AND DISCUSSION\nWe ran a few experiments to understand how much each\npiece of the puzzle contributes to the performance of the\nsystem. First, we evaluated how much the PCA step in\nthe preprocessing improves the input representation. Then,\nwe tested the performance of the system vs. the length of\nthe pooling window. Afterwards, we compared different\npooling functions and combined them for maximum per-\nformance. Finally, by adding a hidden layer to our model\nbefore the pooling, we trained a multiscale learning model.\nIn most experiments, we present the AUC-tag as our per-\nformance measure. Since it was the most stable valid mea-\nsure during training, we chose it as our early-stopping cri-\nterion. However, the AUC-clip and precision at ktend to\nfollow the same trend as the AUC-tag (i.e. good ranking\nmodels also give good annotations).\n4.1 PCA\nWe measure the effect of the PCA on the mel-spectrum.\nWe applied the PFC model on the features with and with-\nout PCA as well as MFCCs for comparison. Results are\nshown in Table 1. We can see that the mel-spectrum fea-\ntures perform better than MFCCs, and that adding the PCA\nstep further improves performance, as well as greatly reduc-\ning training time.\nIt has been shown in [4] that using the full covariance\nmatrix of spectral energy bands improves classiï¬cation per-\nformance. The PCA whitening uncorrelates the spectral fea-valid AUC-tag mean time\nMFCC(20) 0.77 +/- 0.04 5.9h\nMel-spectrum(128) 0.853 +/- 0.008 5.2h\nPMSC(120) 0.876 +/- 0.004 1.5h\nTable 1 .Mean performance (higher is better) and mean train-\ning time of different features on the PFC model. In parantheses is\nindicated the dimensionality of the input\n1 2 4 8 16\nPooling Window Length (s)0.810.820.830.840.850.860.87valid AUC-tagPerformance vs. Pooling Window Length\nFigure 2 .Performance w.r.t. length of pooling window.\ntures, and thus encapsulate most information in the diagonal\nof the covariance matrix. In consequence, relevant informa-\ntion ï¬‚ows better through the pooling functions, which gives\nbetter pooled features and allows faster and more efï¬cient\ntraining.\n4.2 Finding the optimal pooling window\nIn order to ï¬nd the best pooling time scale for our task, we\ntrained a set of PFC models using different pooling win-\ndows. The results on the validation set is shown in Figure 2.\nWe see that the performance reaches a plateau when the\npooling window is around 2.3 seconds. The models illus-\ntrated in the ï¬gure used a combination of mean, variance\nand maximum pooling, but the same tendency was obtained\nwith other pooling functions and combinations.\n4.3 Pooling functions\nWe compared the performance of different pooling func-\ntions and some of their combinations on the PFC model. For\neach type of pooling we trained 10 models with the same\ndistribution of hyper-parameters. The results are illustrated\nin Figure 3. The label all moments refer to the com-\nbination of mean, variance and 3rd and 4th centered mo-\nments. We see that the max and min functions perform well\nby themselves. The third and fourth centered moments give\npoor results. Even when combined with other pooling func-\ntions, they hinder performance. Combining mean, variance,3rd moment 4th moment all moments   mean        var    \nall functions    min        max    \nmean_varmean_max\nmean_var_max  max_min  \nmean_max_min\nmean_var_max_min0.700.750.800.85test AUC-tagPFC pooling functionsFigure 3 .Performance of different combinations of pooling func-\ntions for the PFC model\nvar meanmean_varmin\nmax_minmax\nmean_var_maxmean_max\nmean_max_minmean_var_max_min0.810.820.830.840.850.86test AUC-tagMTSL pooling functions\nFigure 4 .Performance of different combinations of pooling func-\ntions for the MTSL model\nmaximum and minimum gave the best performance.\n4.4 Multiscale learning\nWe trained sets of MTSL models with different pooling func-\ntions combinations. For this experiment, we ï¬xed the pool-\ning window at about 2.3 seconds, following the results from\nSection 4.2. The results for different sets of pooling func-\ntions is given in Figure 4. We see that, once again, com-\nbining pooling functions gives better classiï¬cation perfor-\nmance. In particular, all the models that combined mean\nand max pooling tend to perform better than others. Also,\nvariance pooling seems to perform worse than other pool-\ning functions. It helps when combined with the mean, but it\ndoes not give any signiï¬cant improvement when combined\nwith max and min pooling.\nOne might think that models combining pooling func-\ntions would require more time to train. However, there was\nno signiï¬cant difference in training time for the differentpooling combinations, except for var andmean_var that\nrequired more time. This can be explained by the fact that,\neven though the number of pooled features is greater, the\ncombination of pooling functions allows the error informa-\ntion to ï¬‚ow better to the ï¬rst layer, thus facilitating learning.\nWe used between 100 and 200 units in the ï¬rst layer for\nthe experiments presented in Figure 4. Using more units\nfurther improves performance, but requires more computing\ntime. The best MTSL models used around 350 units.\n4.5 Comparative test performance\nWe compare the results of our models to those of the MIREX\n2009 contest1. In Table 2, we report the test performance\nof models that performed best on the validation dataset. We\nsee that, even without multiscale learning, PMSC features\nwith the PFC model outperform the best results from the\ncompetition. Applying multiscale learning gives an addi-\ntional boost to the performance.\n5. CONCLUSION\nIn this paper we have proposed a few improvements for au-\ntomatic annotation and ranking systems:\n\u000fWe introduced the PMSC features and demonstrated\ntheir performance.\n\u000fWe demonstrated how combining pooling functions\nhelps learning.\n\u000fWe proposed the MTSL model, adding multiscale struc-\nture in a deep architecture, and it obtains state-of-the-\nart performance.\nWe have demonstrated step-by-step the positive impact\nof each of these elements. These conclusions were demon-\nstrated on the task of automatic music annotation and rank-\ning, but may be transferable to other MIR task.\nThe MTSL model we proposed presents a relatively sim-\nple hierarchical structure. There are many ways that we\ncould still improve it further. For instance, using a deeper\nmodel with more time scales and smaller pooling windows\nmight allow to learn a better representation of the music\naudio. Also, applying unsupervised training would proba-\nbly improve the performance, especially for deeper models.\nFurthermore, the use of larger convolutional ï¬lters instead\nof our frame-by-frame hidden-layer could allow a richer rep-\nresentation of time dynamics. Another possible improve-\nment would be to also use the time derivatives of the latent\nfeatures as features to be pooled.\nIt would also be interesting to apply our model to a larger\ndataset such as the Million Song Dataset [6] to test how well\nit scales to much larger music databases.\n1http://www.music-ir.org/mirex/wiki/2009:Audio_\nTag_Classiï¬cation_Tagatune_ResultsMeasure Manzagol Zhi Mandel Marsyas Mel-spec+PFC PMSC+PFC PSMC+MTSL\nAverage AUC-Tag 0.750 0.673 0.821 0.831 0.820 0.845 0.861\nAverage AUC-Clip 0.810 0.748 0.886 0.933 0.930 0.938 0.943\nPrecision at 3 0.255 0.224 0.323 0.440 0.430 0.449 0.467\nPrecision at 6 0.194 0.192 0.245 0.314 0.305 0.320 0.327\nPrecision at 9 0.159 0.168 0.197 0.244 0.240 0.249 0.255\nPrecision at 12 0.136 0.146 0.167 0.201 0.198 0.205 0.211\nPrecision at 15 0.119 0.127 0.145 0.172 0.170 0.175 0.181\nTable 2 .Performance of different models for the TagATune audio classiï¬cation task. On the left are the results from the MIREX 2009\ncontest. On the right are our results.\n6. ACKNOWLEDGMENTS\nThe authors were ï¬nancially supported by FQRNT and NSERC\ngrants. The machine learning models presented in this paper\nwere built using the Theano python library [3]. The authors\nwould like to thank the Theano developper team.\n7. REFERENCES\n[1] Y . Bengio. Learning Deep Architectures for AI. Founda-\ntions and Trends in Machine Learning , 2:1â€“127, 2009.\n[2] Yoshua Bengio and Yann LeCun. Scaling learning algo-\nrithms towards AI. In L. Bottou, O. Chapelle, D. De-\nCoste, and J. Weston, editors, Large Scale Kernel Ma-\nchines . MIT Press, 2007.\n[3] J. Bergstra, O. Breuleux, F. Bastien, P. Lamblin, R. Pas-\ncanu, G. Desjardins, J. Turian, D. Warde-Farley, and\nY . Bengio. Theano: a CPU and GPU math expression\ncompiler. In Proceedings of the Python for Scientiï¬c\nComputing Conference (SciPy) , June 2010. Oral.\n[4] James Bergstra, Michael Mandel, and Douglas Eck.\nScalable genre and tag prediction with spectral covari-\nance. In ISMIR , 2010.\n[5] T. Bertin-Mahieux, D. Eck, and M. Mandel. Automatic\ntagging of audio: The state-of-the-art. In Machine Audi-\ntion: Principles, Algorithms and Systems . IGI Publish-\ning, 2010.\n[6] Thierry Bertin-Mahieux, Daniel P.W. Ellis, Brian Whit-\nman, and Paul Lamere. The million song dataset. In IS-\nMIR, 2011. (submitted).\n[7] Y-Lan Boureau, Jean Ponce, and Yann Lecun. A theo-\nretical analysis of feature pooling in visual recognition.\nInIntl. Conf. on machine learning (ICML) , 2010.\n[8] Shi-Huang Chen and Shih-Hao Chen. Content-based\nmusic genre classiï¬cation using timbral feature vectors\nand support vector machine. In Proc. Intl. Conf. on Inter-\naction Sciences: Information Technology, Culture and\nHuman , ICIS. ACM, 2009.[9] Zhi-Sheng Chen and Jyh-Shing Roger Jang. On the Use\nof Anti-Word Models for Audio Music Annotation and\nRetrieval. IEEE Transactions on Audio, Speech, and\nLanguage Processing , 17(8), 2009.\n[10] Philippe Hamel and Douglas Eck. Learning features\nfrom music audio with deep belief networks. In ISMIR ,\n2010.\n[11] Dan-Ning Jiang, Lie Lu, Hong-Jiang Zhang, Jian-Hua\nTao, and Lian-Hong Cai. Music type classiï¬cation by\nspectral contrast feature . IEEE, 2002.\n[12] Thibault Langlois. A music classiï¬cation method based\non timbral features. In ISMIR , 2009.\n[13] Edith Law and Luis von Ahn. Input-agreement: a new\nmechanism for collecting data using human computa-\ntion games. In Proc. Intl. Conf. on Human factors in\ncomputing systems , CHI. ACM, 2009.\n[14] Edith Law, Kris West, Michael Mandel, Mert Bay,\nand J. Stephen Downie. Evaluation of algorithms using\ngames: the case of music tagging. In ISMIR , 2009.\n[15] Y . LeCun, B. Boser, J. S. Denker, D. Henderson, R. E.\nHoward, W. Hubbard, and L. D. Jackel. Backpropaga-\ntion applied to handwritten zip code recognition. Neural\nComput. , 1:541â€“551, December 1989.\n[16] H. Lee, Y . Largman, P. Pham, and A. Y . Ng. Unsuper-\nvised feature learning for audio classiï¬cation using con-\nvolutional deep belief networks. Advances in Neural In-\nformation Processing Systems (NIPS) 22. , 2009.\n[17] M. I. Mandel and D. P. W. Ellis. Multiple-instance learn-\ning for music information retrieval. In ISMIR , 2008.\n[18] Pierre-Antoine Manzagol and Samy Bengio. Mirex spe-\ncial tagatune evaluation submission. In MIREX , 2009.\n[19] George Tzanetakis. Marsyas submissions to MIREX\n2009. In MIREX , 2009.\n[20] Kris West and Stephen Cox. Finding an optimal segmen-\ntation for audio genre classiï¬cation. In ISMIR , 2005."
    },
    {
        "title": "The Music Encoding Initiative as a Document-Encoding Framework.",
        "author": [
            "Andrew Hankinson",
            "Perry Roland",
            "Ichiro Fujinaga"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1417609",
        "url": "https://doi.org/10.5281/zenodo.1417609",
        "ee": "https://zenodo.org/records/1417609/files/HankinsonRF11.pdf",
        "abstract": "Recent changes in the Music Encoding Initiative (MEI) have transformed it into an extensible platform from which new notation encoding schemes can be produced. This paper introduces MEI as a document-encoding framework, and illustrates how it can be extended to encode new types of notation, eliminating the need for creating specialized and potentially incompatible notation encoding standards.",
        "zenodo_id": 1417609,
        "dblp_key": "conf/ismir/HankinsonRF11",
        "keywords": [
            "Extensible",
            "Platform",
            "New Notation",
            "Encoding",
            "Schemes",
            "Document-Encoding",
            "Framework",
            "Notation",
            "Encoding",
            "Standards"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\n   THE MUSIC ENCODING INITIATIVE AS A  DOCUMENT-ENCODING FRAMEWORK Andrew Hankinson1 Perry Roland2 Ichiro Fujinaga1 1CIRMMT / Schulich School of Music, McGill University 2University of Virginia andrew.hankinson@mail.mcgill.ca, pdr4h@eservices.virginia.edu, ich@music.mcgill.ca  ABSTRACT Recent changes in the Music Encoding Initiative (MEI) have transformed it into an extensible platform from which new notation encoding schemes can be produced. This paper introduces MEI as a document-encoding framework, and illustrates how it can be extended to encode new types of notation, eliminating the need for creating specialized and potentially incompatible notation encoding standards. 1. INTRODUCTION The Music Encoding Initiative (MEI)1 is a community-driven effort to define guidelines for encoding musical documents in a machine-readable structure. The MEI closely mirrors work done by text scholars in the Text Encoding Initiative (TEI)2 and while the two encoding initiatives are not formally related, they share many common characteristics and development practices. MEI, like TEI, is an umbrella term to simultaneously describe an organization, a research community, and a markup language [1]. It brings together specialists from various music research communities, including technologists, librarians, historians, and theorists in a common effort to discuss and define best practices for representing a broad range of musical documents and structures. The results of these discussions are then formalized into the MEI schema, a core set of rules for recording physical and intellectual characteristics of music notation documents. This schema is developed and maintained by the MEI Technical Group. The latest version of the MEI schema is scheduled for release in Fall 2011. The most ambitious feature of the 2011 release is the transformation of the MEI schema from a single, static XML schema language to an extensible and customizable music document-encoding framework. This framework approach gives individuals a platform on which to build custom schemas for encoding new types of music documents by adding features that support the unique aspects of these documents, while leveraging existing rules and guidelines in the MEI schema. This eliminates the                                                 1 http://www.music-encoding.org 2 http://www.tei-c.org 2 http://www.tei-c.org duplication of effort that comes with building entire encoding schemes from the ground up. In this paper we introduce the new tools and techniques available in MEI 2011. We start with a look at the current state of music document encoding techniques. Then, we discuss the theory and practice behind the customization techniques developed by the TEI community and how their application to MEI allows the development of new extensions that leverage the existing music document-encoding platform developed by the MEI community. We also introduce a new initiative for sharing these customizations, the MEI Incubator. Following this, we present a sample customization to illustrate how MEI can be extended to more accurately capture new and unique music notation sources. We then introduce two new software libraries written to allow application developers to add support for MEI-encoded notation. Finally, we end with a discussion on how this will transform the landscape for music notation encoding. 2. MUSIC NOTATION ENCODING There have been many attempts to create structural representations of music notation in machine-readable formats [2]. Some formats, like **kern or MuseData, use custom ASCII-based structures that are then parsed into machine-manipulable representations of music notation. Others, like NIFF or MIDI, use binary file formats. In recent years, XML has been the dominant platform for structural music encoding, employed by initiatives like MusicXML3 and IEEE15994.  The wide variety of encoding formats and approaches to music representation may be attributed to the complexity of music notation itself. Music notation often conveys meaning in multiple dimensions. Variations in placement on the horizontal or vertical axes manifest different dimensions in meaning, along with size, shape, colour, and spacing. To these, however, are added cultural and temporal dimensions that result in different types of music notation expressing different meanings through visually similar notation, depending on where and when that notation system was in use. This complexity prohibits the                                                 3 http://www.recordare.com/musicxml 4 http://www.mx.dico.unimi.it/index.php \n293Oral Session 3: Symbolic Music, OMR\n   construction of a single, unified set of rules and theories about how music notation operates without encountering contradictions and fundamental incompatibilities between notation systems. Consequently, formulating a single, unified notation encoding scheme for representing the full breadth of music notation in a digital format becomes very difficult. As a result, representing music notation in a computer-manipulable format generally takes two approaches. The first approach is to identify the greatest amount of commonality among as many different types of music as possible, and target a general encoding scheme for all of them. The consequence is a widely accepted encoding scheme, which serves as a system that is â€œgood enoughâ€ to represent common features among most musical documents, but extremely poor at representing the unique features that exist in every musical document. For example, the MIDI system of encoding pitch and timing as a stream of events functions very well if the only musical elements of interest are discrete volume, timing, and pitch values. It is, however, notoriously poor at representing features like phrase markings or distinctions between enharmonic pitch values. The second general approach is to build an encoding system that takes into account all the subtle variation and nuance that makes a particular form of music notation different from all others. With this approach, highly specialized methods for encoding the unique features of a given notation system may be designed and customized for a given set of users. The disadvantage, however, is that these systems are largely developed independent of each other, and may exhibit entirely incompatible ways of approaching notation encoding. This approach can be seen in many current encoding formats, where the choice to support the features of common music notation (CMN) in MusicXML, for example, creates a fundamental incompatibility with accurately capturing nuance in mensural notation. This is then addressed by developing entirely new encoding formats, such as the Computerized Mensural Music Editing (CMME) format5, specifically built to handle the unique features of mensural music but ultimately incompatible with other formats without the creation of lossy translators. This creates a highly fragmented music notation ecosystem, where software developers must choose which types of notation they can support in their applications and which ones are specifically out of scope. Earlier versions of the MEI schema focused on the second approach, initially built to represent CMN with all other systems declared out of scope. This led to a number of criticisms about its ability to accurately capture notation nuance; for example, Bradley and Vetch commented: â€œAlthough the scholarly orientation of the MEI markup scheme seemed extremely promisingâ€¦considerable further                                                 5 http://www.cmme.org work would be needed to extend it so that it could appropriately express these very subtle notational differencesâ€ [3]. Later revisions of the MEI schema added support for different types of music notation, but still it was criticized for being unable to capture particular nuances in highly specialized repertoires. A pointed criticism of the representation of neumed notation in MEI was given in [4], which makes entirely valid points about the ability of a generalized notation encoding system to capture highly specific details about a particular notation type. There are inevitable commonalities between different systems of music notation, yet there are simply no universal commonalities across all systems of music notation. This suggests a possible third approach to the creation of music notation encoding schemes that has yet to be fully explored. This third approach exemplifies what we will call the â€œframeworkâ€ approach, where parties interested in supporting new types of notation can leverage existing description methods for common aspects of music notation documents, yet are able to extend this to cover unique aspects of a given repertoire. This allows developers to focus specifically on the features that make that music notation system unique, while still leveraging a large body of existing research and development in common encoding tasks. We call this the framework approach because it mirrors the use of software development frameworks, like Appleâ€™s Cocoa framework6. A framework provides a large number of â€œpre-packagedâ€ methods designed to alleviate the burden of mundane and repetitive tasks, and allows application developers to focus on the features that make their application unique. It significantly reduces duplication of effort, and provides a platform that can easily be bug tested and re-used by many other people. The MEI 2011 Schema marks the first release where extension and customization can be very easily applied to the core set of elements to produce custom encoding systems that extend support for new types of musical documents. This has been accomplished by adopting the tools and development processes pioneered by the TEI community and will be discussed further in the next section. 3. TEI AND MEI: TOOLS AND CUSTOMIZATION The TEI was established to develop and maintain a set of standard practices and guidelines for encoding texts for the humanities. The scope of this project is extensive, but even with a comprehensive set of guidelines in place, there is a recognition that the guidelines developed by the core community do not cover all possible current or future use cases or applications for the TEI.                                                  6 http://developer.apple.com/technologies/mac/cocoa.html \n29412th International Society for Music Information Retrieval Conference (ISMIR 2011)\n   To address this, the TEI community has developed a process where custom TEI schemas may be generated through a formalized extension process. There is no single TEI schema or TEI standard [5]. The full set of TEI elements is arranged in 21 modules according to their utility in encoding certain features of a text (e.g., names and dates, drama, transcriptions of speech, and others). A customization definition file is then applied to the full set of elements specifying which features from these modules should be present in the output schema, and a custom schema for encoding a particular set of sources is generated by running the source and customization files through a processor.  The most powerful feature of this customization process is that new elements, attributes, or content models may be included in the customization definition, allowing the addition of new elements into the TEI that can address the differences presented by new types of documents. What this customization approach represents is the transition from a single, monolithic encoding schema to an extensible  \nTable 1: MEI core modules. document-encoding framework. Validation schemas for ensuring conformance with TEI guidelines can be dynamically generated from a central source and shared among other community members interested in encoding similar document types.  3.1 MEI as an encoding framework The MEI core is divided into 23 modules, each used to encapsulate unique characteristics of musical source encoding (Table 1). There are a total of 259 elements defined in the 2011 version of the MEI core, up from 238 in the 2010 release. The MEI core, like the TEI core, is expressed in an XML meta-schema language, the â€œOne Document Does-it-allâ€ (ODD) format. The ODD meta-schema language provides developers with the facility for easily capturing encoding rules, grouping similar functionality into re-useable classes, and providing a central place for documentation, following a literate programming style. We use the term â€œmeta-schema,â€ since it does not actually provide XML validation on its own, but provides MEI developers with the ability to express definitions of the MEI elements, the rules of how these elements may or may not be used, and their accompanying documentation. The Roma processor7 can then be used to create validation schemas expressed in three popular schema languages: RelaxNG (RNG), W3C Schema (XSD), and Document Type Definition (DTD).  (Of these three, RelaxNG is the preferred schema validation language for MEI). To generate these custom validation schemas, two ODD-encoded files are needed: the MEI core, containing all possible elements and maintained by the MEI Technical Group; and a customization file containing directives that specify the modules that should be activated in the resulting custom MEI schema. A complete set of HTML documentation may also be produced for a specific customization. This documentation includes usage guidelines for elements and their accompanying attributes, as well as automatically generated information about where a given element may or may not appear in a source tree. This process is illustrated in Figure 1. \nFigure 1: The MEI customization process. The most powerful feature of this system is that the ODD modification file allows for the definition of new elements and the re-definition or removal of core elements in the resulting schema. This functionality gives schema developers the ability to define extensions to MEI,                                                 7 http://www.tei-c.org/Guidelines/Customization/use_roma.xml Module Name Module content  MEI  MEI infrastructure  Shared  Shared components  Header  Common metadata  CMN  Common music notation  Mensural  Mensural music notation  Neumes  Neume notation  Analysis  Analysis and interpretation  CMNOrnaments  CMN ornamentation  Corpus  Metadata for music corpora  Critapp  Critical apparatus  Edittrans  Scholarly editions and interpretations  Facsimile  Facsimile documents  Figtable  Figures and tables  Harmony  Harmonic analysis  Linkalign  Temporal linking and alignment  Lyrics  Lyrics  MIDI  MIDI-like structures  Namesdates  Names and dates  Performance  Recorded performances  Ptrref  Pointers and references  Tablature  Basic tablature  Text  Narrative textual content  Usersymbols  Graphics, shapes and symbols  \n295Oral Session 3: Symbolic Music, OMR\n   customizing the core set of elements to accurately capture nuance and unique features of a given repertoire or set of documents. These customizations may be targeted at specifically addressing the needs of these documents, building on and extending the base set of MEI elements. The customization functionality of MEI challenges the idea of building a common encoding system. The infinite and deep customization functionality available in the framework approach allows the development of incompatible â€œdialectsâ€ of MEI. Does this actually represent an advance in music document encoding over a more fragmented encoding landscape with separate encoding initiatives focused on specific areas? While the creation of incompatible document-encoding systems is a possibility, we believe that there are specific advantages to the MEI and TEI approach, based on three assumptions about the nature of document-encoding languages and their development. The first assumption is that the developers of custom schemas want to address a perceived need for encoding a given musical document type, and typically do not want to reinvent entire document structures. Without a formal customization and extension process, however, developers of music encoding schemas have needed to construct entirely new encoding platforms from the ground up. The second assumption is that there are fewer encoding system developers than there are potential users of a given encoding system. A single developer who needs to develop a method of accurately capturing a given document typeâ€”German lute tablature, for exampleâ€”will take the time to learn the customization process, while most encoding projects will be largely satisfied by the capabilities in the MEI core or pre-made and distributed customizations. Once a customization has been completed, that work can then be made available for others to use and extend, reducing further duplication of effort. Finally, the third assumption is that developing compatible encoding formats is a social and political process, as well as a technical one [6].  The TEI has addressed this by forming Special Interest Groups (SIGs) in which groups of individuals and organizations develop and propose extensions to the TEI core that deal with encoding specific types of documents, like correspondence and manuscripts. The fragmentation of an encoding language into incompatible dialects is not a technical problem, but one that can be addressed through discussion among stakeholders. The advantage that the customization approach brings to the process, however, is that it provides a common platform on which to base development and discussions. The customization tools allow a formalization of these discussions into a well-defined set of rules and guidelines. These assumptions have yet to be extensively scrutinized and only time and further discussion will tell if they accurately reflect reality. In the next section we will discuss a new MEI community initiative to allow developers to share their MEI extensions among other interested parties in an open development process. 3.2 The MEI Incubator The MEI Incubator was created to provide community members with a common space for developing and sharing their MEI extension customizations. Incubator projects are proposed by a Special Interest Group (SIG) from the community to address specific needs that members of the SIG feel are not adequately addressed in the MEI core. The Incubator website8 hosts a common code repository and documentation wiki. As Incubator projects mature, the SIG may then propose that the work of the SIG be incorporated into the MEI core as a new module, or an update to an existing module. An editorial committee will review the proposed extension for its suitability and ensure that the proposal does not duplicate existing functionality or create incompatibilities with existing MEI core modules.  The complexity of document encoding and the needs of communities to accurately describe sources may ultimately result in modifications that are fundamentally incompatible with the MEI core. While this means that it is unlikely that this extension will make it into the MEI core, the work done by the SIG can still be made available to others, making it possible to leverage a common platform to share existing work in specialized document encoding. Incubator projects are designed to be a means through which community members can participate in MEI development and propose new means and methods for musical document encoding. In the next section, we will demonstrate this process by examining a current Incubator project and illustrate how ODD modifications may be used to extend the MEI core. \nFigure 2: An example of the Solesmes neume notation showing a four-line staff, neumes, and divisions (vertical lines). 3.3 Sample Extension: The Solesmes Module The monks at Solesmes, France, were responsible for creating a large number of liturgical service books for the Catholic Church in the late 19th and early 20th centuries. These books included missals, graduals and, perhaps most famously, the Liber Usualis [7], a book containing most of the chants for the daily offices and masses of the Catholic Church. These books were notated using a revival of 12th-century Notre Dame notation, featuring square note groups (neumes) on a four-line staff (Figure 2).                                                 8 http://code.google.com/p/mei-incubator \n \n29612th International Society for Music Information Retrieval Conference (ISMIR 2011)\n   There are a number of features of this particular type of notation that make it different from other types of earlier notation. Although MEI has included functionality for encoding neume notation since 2007, it was ultimately found to be insufficient for accurately capturing Solesmes-style neume notation for a project dedicated to automatically transcribing the contents of these books. Certain features, like divisions (similar, but not equivalent to breath marks, graphically represented by a vertical line across the staff), episema (note stresses) and Solesmes-specific neume names and forms were not present in the existing MEI core. A new Incubator project was proposed to address the need for an updated method of handling this type of neume notation. The ODD modification file created for this project defines four new elements for MEI, as well as their accompanying attributes. Due to space considerations we cannot reproduce the entire modification file, but we will illustrate the process by focusing on the method used to define the <division> element. We follow the convention of using angle brackets (<\t\r Â >) to identify XML elements, and the @ symbol to identify XML attributes.  <elementSpec ident=\"division\"      module=\"MEI.solesmes\" mode=\"add\">   <desc>Encodes the presence of a division          on a staff.</desc>   <classes>     <memberOf key=\"att.common\"/>     <memberOf key=\"att.facsimile\"/>     <memberOf key=\"att.solesmes.division\" />   </classes> </elementSpec> Figure 3: Declaration of the <division> element in ODD. This <elementSpec> definition (Figure 3) creates a new element, <division>, with the name specified in the @ident attribute. The @module attribute specifies the MEI module to which this element belongs, and the @mode attribute specifies the mode the Roma processor should use for this element. The @mode attribute may be one of â€œadd,â€ for adding a new element, â€œdelete,â€ for removing an existing element from the resulting schema, or â€œreplace,â€ for re-defining an existing element (the â€œdeleteâ€ and â€œreplaceâ€ attribute use are not shown in Figure 3). The <desc> tags provide the documentation string for this element. The Roma processor will use this information to create the HTML documentation for the resulting schema customization. The <classes> element specifies the classes this element belongs to. In this case, the <division> element will automatically inherit the XML attributes specified in the att.common, att.facsimile, and att.solesemes.division classes. Of these three classes, two are defined in the MEI core while the third is declared elsewhere in the Solesmes ODD file. The <classSpec> declaration (Figure 4) creates a new class of attributes, att.solesmes.division. This class is used to define a new group of attributes that may be used on any element that is a member of this class; in this case, only the <division> element is a member of this class, but more general classes of attributes may be defined that apply to multiple XML elements (like the att.common class). The new @form attribute is declared by the <attDef> element. Additional attributes may be declared by creating more <attDef> children of the <attList> element. The @usage attribute on <attDef> declares this attribute to be optional, meaning that it is acceptable if a <division> element does not possess a @form attribute. Required attributes may be specified by setting this to â€œreq.â€ \nFigure 4: Declaration of the att.solesmes.division class to describe a common attribute group. The <valList> element defines the possible values that the @form attribute may have; in this case the only valid values for the @form attribute are given by the <valItem> elements. Since the value list here is a closed set, any values supplied in the @form attribute that is not one of those specified will not pass validation. \nFigure 5: Valid and invalid use of the <division> element defined in the Solesmes module. These definitions will result in a schema that allows a <division> element in an MEI file, something that is not considered valid in unmodified MEI. Figure 5 llustrates valid and non-valid examples of this in practice. The full Solesmes module contains definitions for four new elements, <division>, <episema>, <neume>, and <nc> (neume component) and eight new attributes to accompany these elements. When this customization is processed with the Roma processor against the 2011 MEI core, a schema is produced that can be used to validate MEI instances. <division form=â€commaâ€ /> Valid, @form can take comma as a value. <division /> Valid, @form is optional. <division form=â€bellâ€ /> Invalid, @form must be one of the specified values. <division name=â€longâ€ /> Invalid, @name is not allowed on this element.  <classSpec ident=\"att.solesmes.division\"     type=\"atts\" mode=\"add\">   <desc>Divisions are breath and          phrasing indicators.</desc>   <attList>     <attDef ident=\"form\" usage=\"opt\">       <desc>Types of divisions.</desc>       <valList type=\"closed\">         <valItem ident=\"comma\" />         <valItem ident=\"major\" />         <valItem ident=\"minor\" />         <valItem ident=\"small\" />         <valItem ident=\"final\" />       </valList>     </attDef>   </attList> </classSpec> \n297Oral Session 3: Symbolic Music, OMR\n   4. MEI SOFTWARE LIBRARIES For software developers looking to integrate MEI into their applications, we have developed two new software libraries to support reading and writing MEI files. Libmei is written in C++, and PyMEI is written in Python. Using object-oriented programming principles, these software libraries were designed to reflect the same modular structure as MEI, and are extensible by others to add support for new customizations. PyMEI 1.0 was developed as a rapid prototype for testing and designing a common API, which was then written in C++ as libmei. PyMEI 2.0, scheduled for release in Fall 2011, will adopt libmei as the base platform, unifying the two projects and serving as a reference implementation for the creation of MEI software libraries in other languages. Architecturally, every element in the MEI core is mirrored in the software libraries by a corresponding classâ€”the <note> element has a Note class, and so on. Every element class inherits from a base MeiElement class. This base class contains methods and attributes common to all MEI elements, like getting and setting names, values, child objects, and element attributes. Subclasses that inherit from this base class gain all of these functions. In the subclasses, however, are musical methods and attributes that are specific to the semantic function of that particular MEI element. For example, a Note class has get and set methods for pitch-related attributes, while a Measure class has methods for working with measure numbers. To extend this software, developers can easily add new classes to reflect new elements that they have added to an MEI customization. For example, a developer who wishes to support the <division> element specified in the Solesmes module would only need to create a Division class that inherits from the base MeiElement class, and then implement any methods that he or she wants to support for this class. For example, a developer may wish to add explicit getForm and setForm methods to set the @form attribute on the <division> element. The libmei and PyMEI projects are available as open source projects on GitHub9,10, licensed under the MIT license. 5. CONCLUSION With the 2011 release of the MEI Schema and the adoption of tools developed by the TEI project, MEI has moved beyond a static music document schema to an extensible document-encoding framework, providing developers with a formalized method of customizing and extending MEI to meet specific needs. An extensive set of elements and guidelines for creating valid MEI documents forms the core of MEI, but the complexity of music makes it impossible to anticipate every context in which users may want to use it.                                                 9 http://github.com/ahankinson/pymei 10 http://github.com/ddmal/libmei To help support and direct these efforts, we have created a new MEI community initiative, the MEI Incubator. This initiative will provide community members with a common space to â€œgrowâ€ their customizations and share them with other members of the community, reducing duplication of effort. As Incubator projects mature, they may be proposed as extensions to the MEI core, subject to editorial review, and finally adopted into the specification itself.  To support MEI in software applications, we are also releasing software libraries that assist developers with providing MEI import and export functionality. Currently we are targeting two common programming languages, C++ and Python, but we are also investigating support in other languages as well. MEI goes beyond simple notation encoding. It is a powerful platform for creating, sharing, storing, and analysing music documents. We are investigating methods of integrating MEI into optical music recognition platforms, as well as searching, analysing, and displaying MEI-encoded document facsimiles in a digital environment. 6.  ACKNOWLEDGEMENTS The authors would like to thank Erin Mayhood for her support and encouragement, the MEI Technical Group for their valuable musical and technical insights, and our colleagues at the Distributed Digital Music Archives and Libraries Lab. This work was supported by the Social Sciences and Humanities Research Council of Canada, the National Endowment for the Humanities, and the Deutsche Forschungsgemeinschaft (DFG). 7. REFERENCES [1] Jannidis, F. 2009. TEI in a crystal ball. Lit. & Ling. Comp. 24 (3): 253â€“65. [2] Selfridge-Field, E. 1997. Beyond MIDI: The handbook of musical codes. Cambridge, MA: MIT Press. [3] Bradley, J., and P. Vetch. 2007. Supporting annotation as a scholarly tool: Experiences from the Online Chopin Variorum Edition. Lit. & Ling. Comp. 22 (2): 225â€“41. [4] Barton, L. 2008. KISS considered harmful in digitization of medieval chant manuscripts. In Proc. Int. Conf. Automated Solutions for Cross Media Content and Multi-channel Distribution at Florence, Italy. 195â€“203. [5] Ide, N., and C. Sperberg-McQueen. 1995. The TEI: History, goals, and future. Comp. and the Humanities 29: 5â€“15. [6] Bauman, S., and J. Flanders. 2004. Odd customizations. In Proc. Extreme Markup Lang.. MontrÃ©al, Quebec. [7] Catholic Church. 1963. The Liber Usualis, with introduction and rubrics in English. Tournai, Belgium: DesclÃ©e. \n298"
    },
    {
        "title": "Unsupervised Learning of Sparse Features for Scalable Audio Classification.",
        "author": [
            "Mikael Henaff",
            "Kevin Jarrett",
            "Koray Kavukcuoglu",
            "Yann LeCun"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1416086",
        "url": "https://doi.org/10.5281/zenodo.1416086",
        "ee": "https://zenodo.org/records/1416086/files/HenaffJKL11.pdf",
        "abstract": "In this work we present a system to automatically learn features from audio in an unsupervised manner. Our method first learns an overcomplete dictionary which can be used to sparsely decompose log-scaled spectrograms. It then trains an efficient encoder which quickly maps new inputs to approximations of their sparse representations using the learned dictionary. This avoids expensive iterative procedures usually required to infer sparse codes. We then use these sparse codes as inputs for a linear Support Vector Machine (SVM). Our system achieves 83.4% accuracy in predicting genres on the GTZAN dataset, which is competitive with current state-of-the-art approaches. Furthermore, the use of a simple linear classifier combined with a fast feature extraction system allows our approach to scale well to large datasets.",
        "zenodo_id": 1416086,
        "dblp_key": "conf/ismir/HenaffJKL11",
        "keywords": [
            "unsupervised",
            "overcomplete dictionary",
            "sparsely decompose",
            "log-scaled spectrograms",
            "efficient encoder",
            "sparse representations",
            "Support Vector Machine (SVM)",
            "GTZAN dataset",
            "linear classifier",
            "large datasets"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nUNSUPERVISED LEARNING OF SPARSE FEATURES FOR SCALABLE\nAUDIO CLASSIFICATION\nMikael Henaff, Kevin Jarrett, Koray Kavukcuoglu and Yann LeCun\nCourant Institute of Mathematical Sciences\nNew York University\nmbh305@nyu.edu ; yann@cs.nyu.edu\nABSTRACT\nIn this work we present a system to automatically learn fea-\ntures from audio in an unsupervised manner. Our method\nï¬rst learns an overcomplete dictionary which can be used to\nsparsely decompose log-scaled spectrograms. It then trains\nan efï¬cient encoder which quickly maps new inputs to ap-\nproximations of their sparse representations using the learned\ndictionary. This avoids expensive iterative procedures usu-\nally required to infer sparse codes. We then use these sparse\ncodes as inputs for a linear Support Vector Machine (SVM).\nOur system achieves 83.4% accuracy in predicting genres\non the GTZAN dataset, which is competitive with current\nstate-of-the-art approaches. Furthermore, the use of a sim-\nple linear classiï¬er combined with a fast feature extraction\nsystem allows our approach to scale well to large datasets.\n1. INTRODUCTION\nOver the past several years much research has been devoted\nto designing feature extraction systems to address the many\nchallenging problems in music information retrieval (MIR).\nConsiderable progress has been made using task-dependent\nfeatures that rely on hand-crafted signal processing tech-\nniques (see [13] and [26] for reviews). An alternative ap-\nproach is to use features that are instead learned automat-\nically. This has the advantage of generalizing well to new\ntasks, particularly if the features are learned in an unsuper-\nvised manner.\nSeveral systems to automatically learn useful features from\ndata have been proposed over the years. Recently, Restricted\nBoltzmann Machines (RBMs), Deep Belief Networks (DBNs)\nand sparse coding (SC) algorithms have enjoyed a good deal\nof attention in the computer vision community. These have\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proï¬t or commercial advantage and that copies\nbear this notice and the full citation on the ï¬rst page.\nc/circlecopyrt2011 International Society for Music Information Retrieval.led to solid and state-of-the-art results on several object recog-\nnition benchmarks [8, 15, 23, 30].\nSome of these methods have also began receiving inter-\nest as means to automatically learn features from audio data.\nThe authors of [22] explored the use of sparse coding using\nlearned dictionaries in the time domain, for the purposes of\ngenre recognition. Convolutional DBNs were used in [16]\nto learn features from speech and music spectrograms in an\nunsupervised manner. Using a similar method, but with su-\npervised ï¬ne-tuning, the authors in [12] were able to achieve\n84.3% accuracy on the Tzanetakis genre dataset, which is\none of the best reported results to date.\nDespite their theoretical appeal, systems to automatically\nlearn features also bring a speciï¬c set of challenges. One\ndrawback of DBNs noted by the authors of [12] were their\nlong training times, as well as the large number of hyper-\nparameters to tune. Furthermore, several authors using sparse\ncoding algorithms have found that once the dictionary is\nlearned, inferring sparse representations of new inputs can\nbe slow, as it usually relies on some kind of iterative proce-\ndure [14, 22, 30]. This in turn can limit the real-time appli-\ncations or scalability of the system.\nIn this paper, we investigate a sparse coding method called\nPredictive Sparse Decomposition (PSD) [11, 14, 15] that at-\ntempts to automatically learn useful features from audio data,\nwhile addressing some of these drawbacks. Like many sparse\ncoding algorithms, it involves learning a dictionary from a\ncorpus of unlabeled data, such that new inputs can be rep-\nresented as sparse linear combinations of the dictionaryâ€™s\nelements. It differs in that it also trains an encoder that ef-\nï¬ciently maps new inputs to approximations of their opti-\nmal sparse representations using the learned dictionary. As\na result, once the dictionary is learned inferring the sparse\nrepresentations of new inputs is very efï¬cient, making the\nsystem scalable and suitable for real-time applications.\n2. THE ALGORITHM\n2.1 Sparse Coding Algorithms\nThe main idea behind sparse coding is to express signals\nxâˆˆRnas sparse linear combinations of basis functions\n681Poster Session 6\nchosen out of an overcomplete set. Letting BâˆˆRnÃ—m\n(n < m )denote the matrix consisting of basis functions\nbjâˆˆRnas columns with weights z= (z1,...,zm), this\nrelationship can be written as:\nx=m/summationdisplay\njzjbj=Bz (1)\nwhere most of the ziâ€™s are zero. Overcomplete sparse\nrepresentations tend to be good features for classiï¬cation\nsystems, as they provide a succinct representation of the sig-\nnal, are robust to noise, and are more likely to be linearly\nseparable due to their high dimensionality.\nDirectly inferring the optimal sparse representation zof\na signal xgiven a dictionary Brequires a combinatorial\nsearch, intractable in high dimensional spaces. Therefore,\nvarious alternatives have been proposed. Matching Pursuit\nmethods [21] offer a greedy approximation to the solution.\nAnother popular approach, called Basis Pursuit [7], involves\nminimizing the loss function:\nLd(x,z,B) =1\n2||xâˆ’Bz||2\n2+Î»||z||1 (2)\nwith respect to z. HereÎ»is a hyper-parameter setting the\ntradeoff between accurate approximation of the signal and\nsparsity of the solution. It has been shown that the solu-\ntion to (2) is the same as the optimal solution, provided it\nis sparse enough [10]. A number of works have focused on\nefï¬ciently solving this problem [1, 7, 17, 20], however they\nstill rely on a computationally expensive iterative procedure\nwhich limits the systemâ€™s scalability and real-time applica-\ntions.\n2.2 Learning Dictionaries\nIn classical sparse coding, the dictionary is composed of\nknown functions such as sinusoids, gammatones, wavelets\nor Gabors. One can also learn dictionaries that are adaptive\nto the type of data at hand. This is done by ï¬rst initializing\nthe basis functions to random unit vectors, and then iterating\nthe following procedure:\n1. Get a sample signal xfrom the training set\n2. Calculate its optimal sparse code zâˆ—by minimizing\n(2) with respect to z. Simple optimization methods\nsuch as gradient descent can be used, or more sophis-\nticated approaches such as [1, 7, 20].\n3. Keeping zâˆ—ï¬xed, update Bwith one step of stochas-\ntic gradient descent: Bâ†Bâˆ’Î½âˆ‚Ld\nâˆ‚B, whereLdis\nthe loss function in (2). The columns of Bare then\nrescaled to unit norm, to avoid trivial minimizations\nof the loss function where the code coefï¬cients go to\nzero while the bases are scaled up.\nâˆ’5 âˆ’4 âˆ’3 âˆ’2 âˆ’1 0 1 2 3 4 5âˆ’4âˆ’2024Figure 1 . Shrinkage function with Î¸= 1\nThere is evidence that sparse coding could be a strategy\nemployed by the brain in the early stages of visual and audi-\ntory processing. The authors in [24] found that basis func-\ntions learned on natural images using the above procedure\nresembled the receptive ï¬elds in the visual cortex. In an\nanalogous experiment [28], basis functions learned on nat-\nural sounds were found to be highly similar to gammatone\nfunctions, which have been used to model the action of the\nbasilar membrane in the inner ear.\n2.3 Predictive Sparse Decomposition\nIn order to avoid the iterative procedure typically required\nto infer sparse codes, several works have focused on de-\nveloping nonlinear, trainable encoders which can quickly\nmap inputs to approximations of their optimal sparse codes\n[11, 14, 15]. The encoderâ€™s architecture is denoted z=\nfe(x,U), where xis an input signal, zis an approxima-\ntion of its sparse code, and Ucollectively designates all the\ntrainable parameters of the encoder. Training the encoder\nis performed by minimizing the encoder loss Le(x,U), de-\nï¬ned as the squared error between the predicted code zand\nthe optimal sparse code zâˆ—obtained by minimizing (2), for\nevery input signal xin the training set:\nLe(x,U) =1\n2||zâˆ—âˆ’fe(x,U)||2(3)\nSpeciï¬cally, the encoder is trained by iterating the fol-\nlowing process:\n1. Get a sample signal xfrom the training set and com-\npute its optimal sparse code zâˆ—as described in the pre-\nvious section.\n2. Keeping zâˆ—ï¬xed, update Uwith one step of stochas-\ntic gradient descent: Uâ†Uâˆ’Î½âˆ‚Le\nâˆ‚U, whereLeis\nthe loss function in (3).\nIn this paper, we adopt a simple encoder architecture\ngiven by:\nfe(x,W,b) =hÎ¸(Wx +b) (4)\nwhere Wis a ï¬lter matrix, bis a vector of trainable bi-\nases andhÎ¸is the shrinkage function given by hÎ¸(x)i=\n68212th International Society for Music Information Retrieval Conference (ISMIR 2011)\nsgn(xi)(|xi|âˆ’Î¸i)+(Figure 1). The shrinkage function sets\nany code components below a certain threshold Î¸to zero,\nwhich helps ensure that the predicted code will be sparse.\nTraining the encoder is done by iterating the above process,\nwithU={W,b,Î¸}. Note that once the encoder is trained,\ninferring sparse codes is very efï¬cient, as it essentially re-\nquires a single matrix-vector multiplication.\n3. LEARNING AUDIO FEATURES\nIn this section we describe the features learned on music\ndata using PSD.\n3.1 Dataset\nWe used the GTZAN dataset ï¬rst introduced in [29], which\nhas since been used in several works as a benchmark for\nthe genre recognition task [2, 3, 6, 12, 18, 25]. The dataset\nconsists of 1000 30-second audio clips, each belonging to\none of 10 genres: blues, classical, country, disco, hiphop,\njazz, metal, pop, reggae and rock. The classes are balanced\nso that there are 100 clips from each genre. All clips are\nsampled at 22050 Hz.\n3.2 Preprocessing\nTo begin with, we divided each clip into short frames of\n1024 samples each, corresponding to 46.4ms of audio. There\nwas a 50% overlap between consecutive frames. We then\napplied a Constant-Q transform (CQT) to each frame, with\n96 ï¬lters spanning four octaves from C 2to C 6at quarter-\ntone resolution. For this we used the toolbox provided by\nthe authors of [27]. An important property of the CQT is\nthat the center frequencies of the ï¬lters are logarithmically\nspaced, so that consecutive notes in the musical scale are\nlinearly spaced. We then applied subtractive and divisive\nlocal contrast normalization (LCN) as described in [15],\nwhich consisted of two stages. First, from each point in the\nCQT spectrogram we subtracted the average of its neigh-\nborhood along both the time and frequency axes, weighted\nby a Gaussian window. Each point was then divided by the\nstandard deviation of the new neighborhood, again weighted\nby a Gaussian window. This enforces competition between\nneighboring points in the spectrogram, so that low-energy\nsignals are ampliï¬ed while high-energy ones are muted. The\nentire process can be seen as a simple form of automatic\ngain control.\n3.3 Features Learned on Frames\nWe then learned dictionaries on all frames in the dataset, us-\ning the process described in 2.2. The dictionary size was\nset to 512, so as to get overcomplete representations. Once\nthe dictionary was learned, we trained the encoder to pre-\ndict sparse representations using the process in 2.3. In both\nFigure 2 . A random subset of the 512 basis functions\nlearned on full CQT frames. The horizontal axis represents\nlog-frequency and ranges from 67 Hz to 1046 Hz.\ncases, we used the Fast Iterative Shrinkage-Thresholding al-\ngorithm (FISTA) [1] to compute optimal sparse codes. Some\nof the learned basis functions are displayed in Figure 2. One\ncan see single notes and what appear to be series of linearly\nspaced notes, which could correspond to chords, harmonics\nor harmonies. Note that some of the basis functions appear\nto be inverted, since the code coefï¬cients can be negative.\nA number of the learned basis functions also seem to have\nlittle recognizable structure.\n3.4 Features Learned on Octaves\nWe also tried learning separate dictionaries on each of the\nfour octaves, in order to capture local frequency patterns.\nTo this end we divided each frame into four octaves, each\nconsisting of 24 channels, and learned dictionaries of size\n128 on each one. We then trained four separate encoders\nto predict the sparse representations for each of the four oc-\ntaves. Some of the learned basis functions are shown in Fig-\nure 3. Interestingly, we ï¬nd that a number of basis functions\ncorrespond to known chords or intervals: minor thirds, per-\nfect ï¬fths, sevenths, major triads, etc. A number of basis\nfunctions also appear to be similar versions of each other\nshifted across frequency. Other functions have their en-\nergy spread out across frequency, which could correspond\n683Poster Session 6\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nFigure 3 . Some of the functions learned on individual oc-\ntaves. The horizontal axis represents log-frequency. Recall\nthat each octave consists of 24 channels a quarter tone apart.\nChannel numbers corresponding to peaks are indicated. a)\nA minor third (two notes 3 semitones apart) b) A perfect\nfourth (two notes 5 semitones apart) c) A perfect ï¬fth (two\nnotes 7 semitones apart) d) A quartal chord (each note is 5\nsemitones apart) e) A major triad f) A percussive sound.to sounds caused by percussive instruments.\n3.5 Feature Extraction\nOnce the dictionaries were learned and the encoders trained\nto accurately predict sparse codes, we ran all inputs through\ntheir respective encoders to obtain their sparse representa-\ntions using the learned dictionaries. In the case of dictio-\nnaries learned on individual octaves, for each frame we con-\ncatenated the sparse representations of each of its four oc-\ntaves, all of length 128, into a single vector of size 512. Ex-\ntracting sparse features for the entire dataset, which contains\nover 8 hours of audio, took less than 3 minutes, which shows\nthat this feature extraction system is scalable to industrial-\nsize music databases.\n4. CLASSIFICATION USING LEARNED FEATURES\nWe now describe the results of using our learned features\nas inputs for genre classiï¬cation. We used a linear Support\nVector Machine (SVM) as a classiï¬er, using the LIBSVM\nlibrary [5]. Linear SVMs are fast to train and scale well to\nlarge datasets, which is an important consideration in MIR.\n4.1 Aggregated Features\nSeveral authors have found that aggregating frame-level fea-\ntures over longer time windows substantially improves clas-\nsiï¬cation performance [2, 3, 12]. Adopting a similar ap-\nproach, we computed aggregate features for each song by\nsumming up sparse codes over 5-second time windows over-\nlapping by half. We applied absolute value rectiï¬cation to\nthe codes beforehand to prevent components of different\nsign from canceling each other out. Since each sparse code\nrecords which dictionary elements are present in a given\nCQT frame, these aggregate feature vectors can be thought\nof as histograms recording the number of occurrences of\neach dictionary element in the time window.\n4.2 Classiï¬cation\nTo produce predictions for each song, we voted over all ag-\ngregate feature vectors in the song and chose the genre with\nthe highest number of votes. Following standard practice,\nclassiï¬cation performance was measured by 10-fold cross-\nvalidation. For each fold, 100 songs were randomly selected\nto serve as a test set, with the remaining 900 serving as train-\ning data. This procedure was repeated 10 times, and the re-\nsults averaged to produce a ï¬nal classiï¬cation accuracy.\nOur classiï¬cation results, along with several other results\nfrom the literature, are shown in Figure 4. We see that PSD\nfeatures learned on individual octaves perform signiï¬cantly\nbetter than those learned on entire frames.1Furthermore,\n1In an effort to capture chords which might be split among two of the\noctaves, we also tried dividing the frequency range into 7 octaves, overlap-\nping by half, and similarly learning features on each one. However, this did\n68412th International Society for Music Information Retrieval Conference (ISMIR 2011)\nClassiï¬er Features Acc. (%)\nCSC Many features [6] 92.7\nSRC Auditory cortical feat. [25] 92\nRBF-SVM Learned using DBN [12] 84.3\nLinear SVM Learned using PSD on octaves 83.4Â±3.1\nAdaBoost Many features [2] 83\nLinear SVM Learned using PSD on frames 79.4Â±2.8\nSVM Daubechies Wavelets [19] 78.5\nLog. Reg. Spectral Covariance [3] 77\nLDA MFCC + other [18] 71\nLinear SVM Auditory cortical feat. [25] 70\nGMM MFCC + other [29] 61\nFigure 4 . Genre recognition accuracy of various algorithms\non the GTZAN dataset. Our results with standard deviations\nare marked in bold.\nour approach outperforms many existing systems which use\nhand-crafted features. The two systems that signiï¬cantly\noutperform our own rely on sophisticated classiï¬ers based\non sparse representations (SRC) or compressive sampling\n(CSC). The fact that our method is still able to reach compet-\nitive performance while using a simple classiï¬er indicates\nthat the features learned were able to capture useful proper-\nties of the audio that distinguish between genres. One possi-\nble interpretation is that some of the basis functions depicted\nin Figure 3 represent chords speciï¬c to certain genres. For\nexample, perfect ï¬fths (e.g. power chords) are very com-\nmon in rock, blues and country, but rare in jazz, whereas\nquartal chords, which are common in jazz and classical, are\nseldom found in rock or blues.\n4.3 Discussion\nOur results show that automatic feature learning is a viable\nalternative to using hand-crafted features. Our approach per-\nformed better than most systems that pair signal processing\nfeature extractors with standard classiï¬ers such as SVMs,\nNearest Neighbors or Gaussian Mixture Models. Another\npositive point is that our feature extraction system is very\nfast, and the use of a simple linear SVM makes this method\nviable on any size dataset. Furthermore, the fact that the fea-\ntures are learned in an unsupervised manner means that they\nare not limited to a particular task, and could be used for\nother MIR tasks such as chord recognition or autotagging.\nWe also found that features learned on octaves performed\nbetter than features learned on entire frames. This could be\ndue to the fact that in the second case we are learning four\ntimes as many parameters as in the ï¬rst, which could lead\nto overï¬tting. Another possibility is that features learned on\noctaves tend to capture relationships between fundamental\nnotes, whereas features learned on entire frames also seem\nnot yield an increase in accuracy.to capture patterns between fundamentals and their harmon-\nics, which could be less useful for distinguishing between\ngenres.\nOne aspect that needs mentioning is that since we per-\nformed the unsupervised feature learning on the entire dataset\n(which includes the training and test sets without labels for\neach of the cross-validation folds), our system is technically\nakin to â€œtransductive learningâ€. Under this paradigm, test\nsamples are known in advance, and the system is simply\nasked to produce labels for them. We subsequently con-\nducted a single experiment in which features were learned\non the training set only, and obtained an accuracy of 80%.\nThough less than our overall accuracy, this result is still\nwithin the range observed during the 10 different cross-validation\nexperiments, which went from 77% to 87%. The seemingly\nlarge deviation in accuracy is likely due to the variation of\nclass distributions between folds.\nThere are a number of directions in which we would like\nto extend this work. A ï¬rst step would be to apply our sys-\ntem to different MIR tasks, such as autotagging. Further-\nmore, the small size of the GTZAN dataset does not ex-\nploit the systemâ€™s ability to leverage large amounts of data\nin a tractable amount of time. For this, the Million Song\nDataset [4] would be ideal.\nA limitation of our system is that it ignores temporal de-\npendencies between frames. A possible remedy would be\nto learn features on time-frequency patches instead. Pre-\nliminary experiments we conducted in this direction did not\nyield improved results, as many â€™learnedâ€™ basis functions\nresembled noise. This requires further investigation. We\ncould also try training a second layer of feature extractors\non top of the ï¬rst, since a number of works have demon-\nstrated that using multiple layers can improve classiï¬cation\nperformance [12, 15, 16].\n5. CONCLUSION\nIn this paper, we have investigated the ability for PSD to\nautomatically learn useful features from constant-Q spec-\ntrograms. We found that the features learned capture infor-\nmation about which chords are being played in a particular\nframe. Furthermore, these learned features can perform at\nleast as well as hand-crafted features for the task of genre\nrecognition. Finally, the system we proposed is fast and uses\na simple linear classiï¬er which scales well to large datasets.\nIn future work, we will apply this method to larger datasets,\nas well as a wider range of MIR tasks. We will also exper-\niment with different ways of capturing temporal dependen-\ncies between frames. Finally, we will investigate using hi-\nerarchical systems of feature extractors to learn higher-level\nfeatures.\n685Poster Session 6\n6. REFERENCES\n[1] A. Beck and M..Teboulle: â€œA Fast iterative Shrinkage-\nThresholding Algorithm with Application to Wavelet-Based\nImage Deblurring,â€ ICASSP â€™09 , pp. 696-696, 2009.\n[2] J. Bergstra, N. Casagrande, D. Erhan, D. Eck and B. Kegl:\nâ€œAggregate features and AdaBoost for music classiï¬cation,â€\nMachine Learning , 65(2-3):473-484, 2006.\n[3] J. Bergstra, M. Mandel and D. Eck: â€œScalable genre and tag\nprediction using spectral covariance,â€ Proceedings of the 11th\nInternational Conference on Music Information Retrieval (IS-\nMIR) , 2010.\n[4] T. Bertin-Mahieux, D. Ellis, B. Whitman and P. Lamere: â€œThe\nmillion song dataset,â€ Proceedings of the 12th International\nConference on Music Information Retrieval (ISMIR) , 2011.\n[5] Chih-Chung Chang and Chih-Jen Lin: â€œLIBSVM: a li-\nbrary for support vector machines,â€ software available at\nhttp://www.csie.ntu.edu.tw/ cjlin/libsvm , 2001.\n[6] K. Chang, J. Jang and C. Iliopoulos: â€œMusic genre classiï¬ca-\ntion via compressive sampling,â€ Proceedings of the 11th Inter-\nnational Conference on Music Information Retrieval (ISMIR) ,\npages 387-392, 2010.\n[7] S.S. Chen, D.L. Donoho, and M.A. Saunders: â€œAtomic De-\ncomposition by Basis Pursuit,â€ SIAM Journal on Scientiï¬c\nComputing, 20(1):33-61, 1999\n[8] A. Courville, J. Bergstra and Y . Bengio: â€œA Spike and Slab\nrestricted Bolztmann Machineâ€ Journal of Machine Learning\nResearch , W&CP 15, 2011.\n[9] I. Daubechies, M. Defrise and C. De Mol : â€œAn Iterative\nThresholding Algorithm for Linear Inverse Problems with a\nSparsity Constraint,â€ Comm. on Pure and Applied Mathemat-\nics, 57:1413-1457, 2004.\n[10] D.L. Donoho and M. Elad: â€œOptimally sparse representa-\ntion in general (nonorthogonal) dictionaries via L1minimiza-\ntion,â€. Proceedings of the National Academy of Sciences, USA\n, 199(5):2197-2202, 2003 Mar 4.\n[11] K. Gregor and Y . LeCun: â€œLearning Fast Approximations of\nSparse Coding,â€ Proceedings of the 27th International Confer-\nence on Machine Learning , Haifa, Israel, 2010.\n[12] P. Hamel and D. Eck: â€œLearning Features from Music Au-\ndio with Deep Belief Networks,â€ 11th International Society\nfor Music Information Retrieval Conference (ISMIR 2010). .\n[13] P. Herrera-Boyer, G. Peeters and S. Dubnov: â€œAutomatic Clas-\nsiï¬cation of Musical Instrument Sounds,â€ Journal of New Mu-\nsic Research , vol. 32, num 1, March 2003.\n[14] K. Kavukcuoglu, M.A. Ranzato, Y . LeCun: â€œFast Inference in\nSparse Coding Algorithms with Applications to Object Recog-\nnition,â€ Computational and Biological Learning Laboratory,\nTechnical Report , CBLL-TR-2008-12-01,\n[15] Y . LeCun, K. Kavukvuoglu and C. Farabet: â€œConvolutional\nNetworks and Applications in Vision,â€ Proc. International\nSymposium on Circuits and Systems (ISCASâ€™10), IEEE, 2010 .[16] H. Lee, Y . Largman, P. Pham, and A. Ng: â€œUnsupervised fea-\nture learning for audioclassiï¬cation using convolutional deep\nbelief networks,â€ Advances in Neural Information Processing\nSystems (NIPS) 22, 2009 .\n[17] H. Lee, A. Battle, R. Raina and A.Y . Ng: â€œEfï¬cient Sparse\nCoding Algorithms,â€ Advances in Neural Information Pro-\ncessing Systems (NIPS) , 2006.\n[18] T. Li and G. Tzanetakis: â€œFactors in automatic musical genre\nclassiï¬cation,â€ IEEE Workshop on Applications of Signal Pro-\ncessing to Audio and Acoustics , New Paltz, New York, 2003.\n[19] T. Li, M. Ogihara and Q. Li: â€œA comparative study on content-\nbased music genre classiï¬cation,â€ Proceedings of the 26th An-\nnual International ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval (SIGIR â€™03) , 2003.\n[20] Y . Li and S. Osher: â€œCoordinate descent optimization for\nl1 minimization with application to compressed sensing; a\ngreedy algorithm,â€ Inverse Problems and Imaging , 3(3):487-\n503, 2009.\n[21] S. Mallat and Z. Zhang: â€œMatching Pursuits with time-\nfrequency dictionaries,â€ IEEE Transactions on Signal Process-\ning,41(12):3397:3415, 1993.\n[22] P.-A. Manzagol, T. Bertin-Mahieux and D. Eck: â€œOn the use\nof sparse time relative auditory codes for music,â€ . In Proceed-\nings of the 9th International Conference on Music Information\nRetrieval (ISMIR), 2008 .\n[23] M. Nourouzi, M. Ranjbar and G. Mori: â€œStacks of Convolu-\ntional Restricted Boltzmann Machines for Shift-Invariant Fea-\nture Learning,â€ IEEE Computer Vision and Pattern Recogni-\ntion (CVPR) , 2009.\n[24] B. Olshausen and D. Field: â€œEmergence of simple-cell recep-\ntive ï¬eld properties by learning a sparse code for natural im-\nages,â€ Nature , 1996.\n[25] Y . Panagakis, C. Kotropoulos, and G.R. Arce: â€œMusic genre\nclassiï¬cation using locality preserving non-negative tensor\nfactorization and sparse representations,â€ Proceedings of the\n10th International Conference on Music Information Retrieval\n(ISMIR) , pages 249-254, 2009.\n[26] G. Peeters: â€œA large set of audio features for sound description\n(similarity and classiï¬cation) in the cuidado projectâ€. Techni-\ncal Report, IRCAM, 2004.\n[27] C. Schoerkhuber and A. Klapuri: â€œConstant-Q transform tool-\nbox for music processing,â€ 7th Sound and Music Computing\nConference , Barcelona, Spain, 2010.\n[28] E. Smith and M. Lewicki: â€œEfï¬cient Auditory Codingâ€ Nature ,\n2006.\n[29] G. Tzanetakis and P. Cook: â€œMusical Genre Classiï¬cation of\nAudio Signals,â€ IEEE Transactions on Speech and Audio Pro-\ncessing, 10(5):293-302, 2002.\n[30] Jianchao Yang, Kai Yu, Yihong Gong, Thomas Huang: â€œLin-\near Spatial Pyramid Matching Using Sparse Coding for Image\nClassiï¬cation,â€ IEEE Conference on Computer Vision and Pat-\ntern Recognition (CVPR) , 2009.\n686"
    },
    {
        "title": "NextOne Player: A Music Recommendation System Based on User Behavior.",
        "author": [
            "Yajie Hu",
            "Mitsunori Ogihara"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1418301",
        "url": "https://doi.org/10.5281/zenodo.1418301",
        "ee": "https://zenodo.org/records/1418301/files/HuO11.pdf",
        "abstract": "We present a new approach to recommend suitable tracks from a collection of songs to the user. The goal of the system is to recommend songs that are favored by the user, are fresh to the userâ€™s ear, and fit the userâ€™s listening pattern. We use â€œForgetting Curveâ€ to assess freshness of a song and evaluate â€œfavorednessâ€ using user log. We analyze userâ€™s listening pattern to estimate the level of interest of the user in the next song. Also, we treat user behavior on the song being played as feedback to adjust the recommendation strategy for the next one. We develop an application to evaluate our approach in the real world. The user logs of trial volunteers show good performance of the proposed method.",
        "zenodo_id": 1418301,
        "dblp_key": "conf/ismir/HuO11",
        "keywords": [
            "freshness",
            "favoredness",
            "listening pattern",
            "Forgetting Curve",
            "user log",
            "user behavior",
            "recommendation strategy",
            "real world",
            "application",
            "trial volunteers"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nNEXTONE PLAYER: A MUSIC RECOMMENDATION SYSTEM BASED ON\nUSER BEHA VIOR\nYajie Hu\nDepartment of Computer Science\nUniversity of Miami\nyajie.hu@umail.miami.eduMitsunori Ogihara\nDepartment of Computer Science\nUniversity of Miami\nogihara@cs.miami.edu\nABSTRACT\nWe present a new approach to recommend suitable tracks\nfrom a collection of songs to the user. The goal of the system\nis to recommend songs that are favored by the user, are fresh\nto the userâ€™s ear, and ï¬t the userâ€™s listening pattern. We use\nâ€œForgetting Curveâ€ to assess freshness of a song and evalu-\nate â€œfavorednessâ€ using user log. We analyze userâ€™s listen-\ning pattern to estimate the level of interest of the user in the\nnext song. Also, we treat user behavior on the song being\nplayed as feedback to adjust the recommendation strategy\nfor the next one. We develop an application to evaluate our\napproach in the real world. The user logs of trial volunteers\nshow good performance of the proposed method.\n1. INTRODUCTION\nAs users accumulate digital music in their digital devices,\nthe problem arises for them to manage the large number of\ntracks in them. If a device contains thousands of tracks, it\nis difï¬cult, painful, and even impractical for a user to pick\nsuitable tracks to listen to without using pre-determined or-\nganization such as albums, playlists or computationally gen-\nerated recommendation, which is the topic of this paper.\nA good recommendation system should be able to min-\nimize userâ€™s effort required to provide feedback and simul-\ntaneously to maximize the userâ€™s satisfaction by playing ap-\npropriate song at the right time. Reducing the amount of\nfeedback is an important point in designing recommenda-\ntion systems, since users are in general lazy. We thus evalu-\nate userâ€™s attitude towards a song from partitioning of play-\ning time. In particular, if a song is played from beginning\nto end, we infer that the user likes the song and it is a sat-\nisfying recommendation. On the other hand, if the song is\nskipped while just lasting a few seconds, we assume that the\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proï¬t or commercial advantage and that copies\nbear this notice and the full citation on the ï¬rst page.\nc/circlecopyrt2011 International Society for Music Information Retrieval.user dislikes the song at that time and the recommendation\nis less effective.\nUsing this idea we propose a method to automatically\nrecommend music in a userâ€™s device as the next song to be\nplayed. In order to keep short the computation time for rec-\nommendation, the method is based on metadata and user be-\nhavior rather than on content analysis. Which song should\nbe played next can be determined based on various factors.\nIn this paper, we use ï¬ve perspectives: genre, year, favor,\nfreshness and time pattern.\nThe rest of this paper is organized as follows. In Section 2,\nwe introduce recent related work. In Section 3 we describe\nour method for calculating recommendation. We will eval-\nuate this method in Section 4. We conclude by discussing\npossible future work in Section 5.\n2. RELATED WORK\nVarious song recommendation approaches have been devel-\noped so far. We can categorize these approaches in different\nviews.\nAutomatical playlist generation focuses on recommend-\ning songs that are similar to chosen seeds to generate a new\nplaylist. Ragno [1] provided an approach to recommend\nmusic that is similar to chosen seeds as a playlist. Sim-\nilarly, Flexer [2] provided a sequence of songs to form a\nsmooth transition from a start song till the end song. These\napproaches ignore userâ€™s feedback when the user listens to\nthe songs in the playlist. They have an underlying problem\nthat all seed-based approaches produce excessively uniform\nlists of songs if the dataset contains lots of music cliques.\nIn iTunes, Genius employs similar methods to generate a\nplaylist from a seed.\nDynamic music recommendation improves automatic play-\nlist generation by considering the userâ€™s feedback. In the\nmethod proposed by Pampalk [3], playlist generation starts\nwith an arbitrary song and adjusts the recommendation re-\nsult based on user feedback. This type of method is similar\nto Pandora.\nCollaborative-ï¬lter methods recommend pieces of music\nto a user based on rating of those pieces by other users with\n103Poster Session 1\nsimilar taste [4]. However, collaborative methods require\nmany users and many ratings and are unable to recommend\nsongs that have no ratings. Hence, users have to be well rep-\nresented in terms of their taste if they need effective recom-\nmendation. This principle has been used by various social\nwebsites, including Last.fm, myStrands.\nContent-based methods computes similarity between songs,\nrecommends songs similar to the favorite songs, and re-\nmoves songs that are similar to the skipped songs. In an\napproach proposed by Cano [5], acoustic features of songs\nare extracted, such as timbre, tempo, meter and rhythm pat-\nterns. Furthermore, some work expresses similarity accord-\ning to songs emotion. Cai [6] recommends music based only\non emotion.\nHybrid approaches , which combine music content and\nother information, are receiving more attention lately. Don-\naldson [7] leverages both spectral graph properties of an\nitem-base collaborative ï¬ltering as well as acoustic features\nof the music signal. Shao et al. [8] use both content features\nand user access pattern to recommend music.\nContext-based methods take context into consideration.\nLiu et al. [9] take the change in the interests of users over\ntime into consideration and add time scheduling to the mu-\nsic playlist. Su et al. [10] improve collaborative ï¬ltering us-\ning user grouping by context information, such as location,\nmotion, calendar, environment conditions and health condi-\ntions, while using content analysis assists system to select\nappropriate songs.\n3. METHOD\nWe determine whether a song is to be recommended as the\nnext one in the playlist from ï¬ve perspectives: genre, year,\nfavor, freshness and time pattern.\nWe use time series analysis of genre and year to predict\nthese attributes of the next song rather than to select the song\nwith similar genre and year to the current song. The reason\nis that some users like listening similar songs according to\ngenre and year while others perhaps love mixing songs and\nthe variance on genre and year. Hence, we cannot assume\nthat a similar song to the current one can be reasonably seen\nas a good choice for recommendation. Prediction using time\nseries analysis caters better to a userâ€™s taste.\nObviously, the system should recommend usersâ€™ favorite\nsongs to them. How many times a song has been actively\nplayed and how many times the song has been completely\nplayed can be used to infer the strength of favor to the song.\nWe collected userâ€™s behavior to analyze the favor of songs.\nIn common sense, a few users dislike listening to a song\nmany times in a short period of times, even though the song\ncould be the userâ€™s favorite. On the other hand, some songs\nthat the user favored many months ago may be now old and\na little bit insipid. However, if the system recommend them\nFigure 1 . Genre taxonomy screenshot in AllMusic.com\nat right time, the user may feel it is fresh and enjoy the expe-\nrience. Consequently, we take freshness of songs into con-\nsideration.\nDue to activities and biological clock, users have differ-\nent tastes in choosing music. In a different period of a day\nor a week, users tend to select different styles of songs. For\nexample, in the afternoon, a user may like a soothing kind\nof music for relaxation and may switch to energetic songs\nin the evening. This paper uses a Gaussian Mixture Model\nto represent the time pattern of listening and compute the\nprobability of playing a song at that time.\n3.1 Genre\nThe sequence of recent playing of a user represents the userâ€™s\nhabit of listening so we analyze the playing sequence us-\ning a time series analysis method to predict the genre of the\nnext song. The system records recent 16 songs that were\nplayed for at least a half of their length. Although most\nof the songs record their genres and years are available in\nID3v1 or ID3v2 tags, a part of tags are notoriously noisy.\nHence, we developed a web wrapper to collect genre in-\nformation from AllMusic.com, a popular music information\nwebsite, and use that information to retrieve songsâ€™ genres.\nThe ID3v1 or ID3v2 tags will be used unless AllMusic.com\nhas no information about the song.\nFurthermore, AllMusic.com not only has a hierarchical\ntaxonomy on genre but also provides subgenres with re-\nlated genres. The hierarchical taxonomy and related gen-\nres are shown in Figure 1. For example, Industrial\nMetal , whose parent is Alternative Metal , is re-\nlated to Alternative Pop/Rock .\nWe use the taxonomy to build an undirected distance graph,\nin which each node describes a genre and each edgeâ€™s value\nis the distance between two genres. The values of the graph\nare initialized by a maximum value. The parent and related\nrelationship are valued at a different distance, which varies\nby the depth in the taxonomy, that is, high level corresponds\nto larger distance while low level corresponds to smaller dis-\ntance. Then, we assume the distance is transitive and update\nthe distance graph as follows until there is no cell update.\n10412th International Society for Music Information Retrieval Conference (ISMIR 2011)\nEij= min\nk(Eij,Eik+Ekj), (1)\nwhereEijis the value of edge (i,j). Therefore, we obtain\nthe similarity between any two kinds of genre and the max-\nimum value in the matrix is 6.\nNow, the system converts the series of genres of recent\nsongs into a series of similarity between neighbor genres\nusing the similarity matrix. The series of similarity will be\nseen as the input for time series analysis method and we can\nestimate the next similarity. Then, the current genre and the\nestimated similarity will give us genre candidates.\nAutoregressive Integrated Moving Average (ARIMA) [11]\nmodel is a general class of models in time series analysis.\nAn ARIMA( p,d,q ) model can be expressed by following\npolynomial factorization.\nÎ¦ (B) (1âˆ’B)dyt=Î´+ Î˜ (B)Îµt (2)\nÎ¦ (B) = 1âˆ’p/summationdisplay\ni=1Ï†iBi(3)\nÎ˜ (B) = 1 +q/summationdisplay\ni=1Î¸iBi(4)\n,whereytis thetth value in the time series of data YandB\nis the lag operator; Ï†andÎ¸are the parameters of the model,\nwhich are calculated in analysis; pandqare orders of au-\ntoregressive process and moving average process, respec-\ntively; Anddis a unitary root of multiplicity.\nThe ï¬rst step of building ARIMA model is model identi-\nï¬cation, namely, estimating p,dandqby analyzing obser-\nvations in time series. Model identiï¬cation is beneï¬cial to\nï¬t the different pattern of time series. The second step is\nto estimate parameters of the model. Then, the model can\nbe applied to forecast the value at t+Ï„, forÏ„ >0. As an\nillustration consider forecasting the ARIMA( 1,1,1) process\n(1âˆ’Ï†B) (1âˆ’B)yt+Ï„= (1âˆ’Î¸B)Îµt+Ï„ (5)\nË†Îµt=ytâˆ’/bracketleftBigg\nÎ´+p+d/summationdisplay\ni=1Ï†iytâˆ’iâˆ’q/summationdisplay\ni=1Î¸iË†Îµtâˆ’i/bracketrightBigg\n(6)\nOur system uses ARIMA to ï¬t the series of similarity\nand to predict the next similarity. The process is shown in\nFigure 2.\nWe use Gaussian distributions to evaluate each possible\ngenre for the next track. We select the one with the biggest\nprobability.\n3.2 Recording year\nThe recording year is similar to genre so we use ARIMA to\npredict the next possible year and compute the probability\nof a recording year.\nFigure 2 . Predict the next genre\n3.3 Freshness\nAs a new feature of this paper, we take into consideration\nfreshness of a song to a user. Many recommendation sys-\ntems [12] based on metadata of music and user behavior\ncannot avoid to recommend same music under same situa-\ntions. As a result, a small set of songs will be recommended\nagain and again. Whatâ€™s worse is that these songs will still\nbe at the top of recommendation result since they have been\nrecommended and played many times and then are seen as\nfavorite songs. The iteration makes users fall into a â€œfavorite\ntrapâ€ and feel bored. Therefore, an intelligent recommenda-\ntion system should avoid to recommend same set of songs\nmany times in a short period. On the other hand, the system\nis supposed to recommend some songs that have not been\nplayed for a long time because these songs are fresh to users\neven though they once listened to them multiple times.\nFreshness can be considered as the strength of strangeness\nor the amount of experience forgotten. We apply Forgetting\nCurve [13] to evaluate the freshness of a song to a user. For-\ngetting Curve is shown as follows.\nR=eâˆ’t\nS, (7)\nwhereRis memory retention, Sis the relative strength of\nmemory and tis time.\nThe lesser the amount of memory retention of a song in a\nuserâ€™s mind, the fresher the song to the user. In our work, S\nis deï¬ned as the number of times the song has been played\nandtis the distance of present time to the last time the song\nwas played. The reciprocal of memory retention is normal-\nized to represent the freshness.\nThis metric contributes towards selecting fresh songs as\nrecommendation results rather than recommending a small\nset of songs repetitively.\n3.4 Favor\nThe strength of favor for a song plays an important role in\nrecommendation. In playing songs, the system should give\npriority to userâ€™s favorite songs. User behavior can be im-\nplied to estimate how favored the user feels about the song\nbased on a simple assumption: A user listens to a favorite\nsong more often many an unfavorite song and on average\nlistens to a larger fraction of the favorite song than the other.\nWe consider the favor of a song from four counts: ac-\ntive play times, passive play times, skip times and delete\n105Poster Session 1\ntimes. Passive play time means the song is played as a rec-\nommendation result or as the next one in playlist. The favor\nis assessed by the weighted average of the four factors.\n3.5 Time pattern\nSince users have different habits or tastes in different peri-\nods of a day or a week, our recommendation system takes\ntime pattern into consideration based on user log. The sys-\ntem records the time of the day and week that songs are\nplayed. It then employs Gaussian Mixture Model to esti-\nmate the probability of playing at a speciï¬c time. The play-\ning times of a song in different periods trains the model us-\ning Expectation Maximization algorithm. When the system\nrecommends songs, the model is used to estimate the prob-\nability of the song being played at that time.\n3.6 Integrate into ï¬nal score\nA song is assessed whether it is a ï¬t for recommendation\nas the next song from the ï¬ve perspectives described in the\nabove. In order to rank results and make a selection, the\nscores should be integrated into a ï¬nal score. At ï¬rst, the\nscores are normalized into the same scale. Since different\nusers have different tastes, these ï¬ve factors are assigned\ndifferent weights at integration. Hence, we refer to Gradi-\nent Descent in order to match usersâ€™ need. However, it is not\nuser friendly to offer too many possible recommendation re-\nsults and determine how to descent based on userâ€™s interac-\ntion. We use the recent recommendation results to adjust the\nweights, which is initialized by (1.0,1.0,1.0,1.0,1.0). The\nalgorithm is shown in Algorithm 1.\n3.7 Cold start\nCold start is a difï¬cult problem to tackle for recommenda-\ntion system. When a recommendation system begins with\nno idea as to what kinds of songs users like or dislike, it\nhardly gives any valuable recommendation. As a result, in\nthe cold start, the system randomly picks a song as the next\nsong and records the userâ€™s interaction, which is similar to\nPampalkâ€™s work [3]. After 16 songs has been played, the\nsystem uses the metadata of these songs and user behavior\nto recommend a song as the next one.\n4. EXPERIMENT\nThe goal of the recommendation system is to cater to usersâ€™\ntaste and recommend the next song at the right time and in\nthe right order. Therefore, here, we focus on the user expe-\nrience and compare usersâ€™ satisfaction between our method\nand a baseline method, which randomly picks a song as the\nnext one. We notice that most of the songs in a userâ€™s de-\nvice are their favorite, but it doesnâ€™t mean that every songALGORITHM 1: Adjust weights based on recent rec-\nommendation results\nInput: Recentkrecommendation results\n/Rfracturt(Rtâˆ’k+1,Rtâˆ’k+2,...,Rtâˆ’1,Rt)at timet.\nRicontains user interaction of this recommendation\nÏ‡i, which is like ordislike , and the score of each\nfactor of the recommendation iisÎ›i.\nDescent step Î´, which is positive.\nCurrent factor weights, W.\nOutput: New factor weights, W/prime.\nProcess:\nifÏ‡t=dislike then\nInitialize an array Fto record the contribution of\neach factor.\nfori=Rtâˆ’k+2toRtdo\nâˆ†Î›i=Î›iâˆ’Î›iâˆ’1\nmax = arg max\nj(âˆ†Î»j),1â‰¤jâ‰¤5\nmin = arg min\nj(âˆ†Î»j)\nifÏ‡i=Like then\nFmax=Fmax+ 1\nend\nelse\nFmax=Fmaxâˆ’2\nFmin=Fmin+ 1\nend\nend\ninIndex = arg max\nj(F)\nw/prime\nj=/braceleftbiggwj+Î´,j=inIndex\nwjâˆ’Î´/4,otherwise,j= 1,2,3,4,5\ndeIndex = arg min\ni(F)\nw/prime\nj=/braceleftbiggwjâˆ’Î´,j=deIndex\nwj+Î´/4,otherwise,j= 1,2,3,4,5\nend\nelse\nW/prime=W\nend\nreturn W/prime\nis ï¬t to be played at anytime. The feedback to random se-\nlections represents the quality of songs in usersâ€™ devices and\nthe comparison result between our method and random se-\nlection shows the value of our method.\n4.1 Data collection\nAn application system, named NextOne Player1, is imple-\nmented to collect run-time data and user behavior for this\nexperiment. It is developed in .NET Framework 4.0 using\nWindows Media Player Component 1.0. In addition to the\nfunctions of Windows Media Player, NextOne Player offers\n1Available at http://sourceforge.net/projects/nextoneplayer/\n10612th International Society for Music Information Retrieval Conference (ISMIR 2011)\nFigure 3 . The appearance of NextOne Player\nrecommendation function using the approach described in\nSection 3 and also collects data for performance evaluation.\nThe recommendation will work when the current song in the\nplaylist ends or NextOne button is clicked. The appearance\nof the application is shown as Figure 3. The Like it and\nDislike it buttons are used to collect user feedback.\nThe proportion of a song played is recorded and viewed as\nthe measure of satisfaction of a user for the song.\nIn order to compare our method with random selection,\nthe player selects one of the two methods when it is loaded.\nThe probability of running each method is set to 0.5. Every-\nthing is exactly the same except the recommendation me-\nthod. In a contrasting experiment, users cannot realize which\nmethod is selected.\nWe have collected data from 11 volunteers. They con-\nsist of 9 graduate students and 2 professors and include 3\nfemale students. They use the application in their devices\nwhich recommend songs from their own collections so the\nexperiment is run on open datasets.\n4.2 Results\nFirst, we show the running time of recommendation func-\ntion as it is known to have a major inï¬‚uence on the user\nexperience. The running time results appear to be in an ac-\nceptable range. We run the recommendation system for dif-\nferent magnitudes of the song library and at each size the\nsystem recommends 32 times2. Figure 4 shows the varia-\ntion in running time with the corresponding variations to the\nsize of song library. We observe that the running time in-\ncreases linearly with the increase in size of the song library.\nIn order to provide a user-friendly experience, the recom-\nmendation results are processed near the end of the current\nsong that is playing, and the result is generated when the\nnext song begins.\n2CPU: Intel i7, RAM: 4GB, OS: Windows 7\nFigure 4 . Running time of recommendation function\nFigure 5 . Representing the user logs to express favordness\nover a month\nIn order to evaluate the approach, the system records the\nplaying behavior of the user. We collected the user logs from\nvolunteers and calculated the average proportion of playing\nsong length, which means how much partition of a song is\nplayed before it is skipped. Under the assumption that the\npartition implies the â€œfavorednessâ€ of the song for a user, we\nevaluate the recommendation approach by the partition as\nshown in Figure 5, where the histograms represent the num-\nber of songs that were played on a day. The curves in the\ngraph represent the variation of the â€œplaying proportionâ€.\nMoreover, continuous skips have a signiï¬cant inï¬‚uence\non the user experience, hence they can play an important\nrole in evaluating the approach. A skip is deï¬ned as chang-\ning to the next track by the user before playing 5% of the\nlength of the current track. The number of continuous skips\ncan be used as a measure of user dissatisfaction. Figure 6\nshows the distribution of continuous skips using our method\nand random selection.\nFrom Figure 5 and 6, we can conclude that the recom-\nmendation approach surpasses the baseline and our recom-\nmendation is effective. Our approach is able to ï¬t to a userâ€™s\ntaste, and adjust the recommendation strategy quickly when-\never user skips a song.\n107Poster Session 1\nFigure 6 . The distribution of continuous skips\n5. CONCLUSION AND DISCUSSION\nThis paper presented a novel approach in recommending\nsongs one by one based on user behavior. The approach\nconsidered genre, recording year, freshness, favor and time\npattern as factors to recommend songs. The evaluation re-\nsults demonstrate that the approach is effective.\nIn further research, we can apply this technique to a mu-\nsic database in a server. Also other usersâ€™ behavior can be\napplied to recommend songs for a user. We can mix recom-\nmendation of music in a local device and an online server\ndata to overcome the issue of cold start and hence obtain\nnew favorite songs.\n6. REFERENCES\n[1] R. Ragno, C. Burges and C. Herley: â€œInferring similarity\nbetween music objects with application to playlist gen-\neration,â€ in Proc. of 7th ACM Multimedia, Workshop on\nMIR, pp. 73â€“80, 2005.\n[2] A. Flexer, D. Schnitzer, M. Gasser and G. Widmer:\nâ€œPlaylist generation using start and end songs,â€ in Proc.\nof 9th ISMIR , pp. 173â€“178, 2008.\n[3] E. Pampalk, T. Pohle and G. Widmer: â€œDynamic playlist\ngeneration based on skipping behaviorâ€ in Proc. of 6th\nISMIR , pp. 634â€“637, 2005.\n[4] W. W. Cohen and W. Fan: â€œWeb-collaborative ï¬ltering:\nRecommending music by crawling the web,â€ Computer\nNetwork , V ol. 33, pp. 685â€“698, 2000.\n[5] P. Cano, M. Koppenberger and N. Wack: â€œAn industrial-\nstrength content-based music recommendation system,â€\ninProc. of 28th ACM SIGIR , pp. 673, 2005.\n[6] R. Cai, C. Zhang, C. Wang, L. Zhang and W. Ma:\nâ€œMusicSense: Contextual music recommendation usingemotional allocation modeling,â€ in Proc. of ACM Multi-\nmedia , pp. 553â€“556, 2007.\n[7] J. Donaldson: â€œA hybrid social-acoustic recommenda-\ntion system for popular music,â€ in Proc. of the ACM\nRecommender Systems , pp. 187â€“190, 2007.\n[8] B. Shao, D. Wang, T. Li and M. Ogihara: â€œMusic rec-\nommendation based on acoustic features and user access\npatterns,â€ IEEE Trans. on Audio, Speech And Language\nProcessing , V ol. 17, No. 8, pp. 1602â€“1611, 2009.\n[9] N. Liu, S. Lai, C. Chen and S. Hsieh: â€œAdaptive music\nrecommendation based on user behavior in time slot,â€\nInternational Journal of Computer Science and Network\nSecurity , V ol. 9, pp. 219â€“227, 2009.\n[10] J. Su and H. Yeh: â€œMusic recommendation using con-\ntent and context information mining,â€ IEEE Intelligent\nSystems , V ol. 25, pp. 16â€“26, 2010.\n[11] G. E. P. Box, and D. A. Pierce: â€œDistribution of resid-\nual autocorrelations in autoregressive-integrated moving\naverage time series models,â€ Jour. of the American Sta-\ntistical Association , V ol. 65, pp. 1509â€“1526, 1970.\n[12] B. Logan: â€œMusic recommendation from song sets,â€ in\nProc. of 5th ISMIR , pp. 425â€“428, 2004.\n[13] H. Ebbinghaus: Memory: A Contribution to Experimen-\ntal Psychology , Columbia University, New York, 1913.\n108"
    },
    {
        "title": "Exploring The Relationship Between Mood and Creativity in Rock Lyrics.",
        "author": [
            "Xiao Hu 0001",
            "Bei Yu 0002"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1415690",
        "url": "https://doi.org/10.5281/zenodo.1415690",
        "ee": "https://zenodo.org/records/1415690/files/HuY11.pdf",
        "abstract": "The relationship between mood and creativity has been widely studied in psychology, however, no conclusion is reached in terms of which mood triggers high creativity, positive or negative. This paper provides new insights to this on-going argument by examining the relationship between lyrics creativity and music mood. We use three computational measures to gauge lyrics creativity: Type-toToken Ratio, word norms fraction, and WordNet similarity. We then test three hypotheses regarding differences in lyrics creativity between music with different moods on 2715 U.S. rock songs. The three measures led to consistent findings that lyrics of negative and sad songs demonstrate higher linguistic creativity than those of positive and happy songs. Our findings support previous studies in psycholinguistics that people write more creatively when the text conveys sad or negative sentiment, and contradict previous research that positive mood triggers more unusual word associations. The result also indicates that different measures capture different aspects of lyrics creativity.",
        "zenodo_id": 1415690,
        "dblp_key": "conf/ismir/HuY11",
        "keywords": [
            "creativity",
            "mood",
            "lyrics",
            "music",
            "computational measures",
            "U.S. rock songs",
            "negative and sad songs",
            "positive and happy songs",
            "previous studies",
            "unusual word associations"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\n  \n \nEXPLORING THE RELATIONSHIP  \nBETWEEN MOOD AND CREATIVITY IN ROCK LYRICS \n \nXiao Hu Bei Yu \nLibrary and Information Science Program \nMorgridge College of Education \nUniversity of Denver \nxiao.hu@du.edu  School of Information Studies \nSyracuse University \nbyu@syr.edu  \nABSTRACT \nThe relationship between mood and creativity has be en \nwidely studied in psychology, however, no conclusio n is \nreached in terms of which mood triggers high creati vity, \npositive or negative. This paper provides new insig hts to \nthis on-going argument by examining the relationshi p be- \ntween lyrics creativity and music mood. We use thre e com- \nputational measures to gauge lyrics creativity: Typ e-to-\nToken Ratio, word norms fraction, and WordNet simil arity. \nWe then test three hypotheses regarding differences  in lyr- \nics creativity between music with different moods o n 2715 \nU.S. rock songs. The three measures led to consiste nt find- \nings that lyrics of negative and sad songs demonstr ate high- \ner linguistic creativity than those of positive and  happy \nsongs. Our findings support previous studies in psy cholin- \nguistics that people write more creatively when the  text \nconveys sad or negative sentiment, and contradict p revious \nresearch that positive mood triggers more unusual w ord as- \nsociations. The result also indicates that differen t measures \ncapture different aspects of lyrics creativity.  \n1.  INTRODUCTION \nMusic is a product of humanâ€™s creativity, and yet f ew stu- \ndies have been done to analyze musical creativity u sing \ncomputational methods [2]. In the meantime, progres s has \nbeen made in the area of literature and language cr eativity \n(i.e., linguistic creativity). In this study, we bo rrow the \nmeasures of linguistic creativity to examine lyrics , the tex- \ntual part inherently integrated in many music piece s, aiming \nto provide an alternative approach to music creativ ity re- \nsearch that is complementary to modeling music comp osi- \ntion and music audio. This study is expected to pro vide new \ninsights to the relationship between mood and creat ivity in general in that the argument on whether positive or  nega- \ntive mood trigger higher creativity remains inconcl usive in \npsychology research [4].  \nIn Western English dictionaries, creativity is defi ned as \nâ€œâ€¦the ability to transcend traditional ideas, rules , patterns, \nrelationships, or the like, and to create meaningfu l new \nideas, forms, methods, interpretationsâ€ [18]. Based  on this \ndefinition, when measuring creativity, a central ta sk is to \nidentify new or unusual patterns. In this study, we  apply \nthree linguistic measures to gauging lyrics creativ ity signi- \nfied by vocabulary richness and unusual word associ ations. \nThis research is expected to contribute to research  on \ncreativity in music, psychology and linguistics. Be sides, \nmood has been identified as a new metadata type or facet of \nmusic in recent years. Findings in this study will help ana- \nlyzing music mood from a new angle, lyrics creativi ty.  \n2.  RELATED WORK \nTo date creativity in lyrics has rarely been studie d. Howev- \ner, research in the following related areas has ins pired and \ninformed this research.  \n2.1  Lyrics and Music Mood Classification \nLyrics have been used in predicting music mood, eit her \nstandalone (e.g., [6] [8]) or being combined with m usic au- \ndio (e.g., [9], [10], [20]). These studies identifi ed lyric fea- \ntures that were effective in mood classification su ch as \nhigher-order bag-of-words features (e.g., trigrams and bi- \ngrams) [6], psycholinguistic and stylistic features  [8] [9]. \nIn terms of the relationship between lyrics and mus ic mood, \nHu and Downie [10] found lyrics were less effective  for \nclassifying negative and passive categories, while Schuller \net al. [20] revealed lyrics were more helpful on th e classifi- \ncation of valence (positive and negative feelings).  These \nstudies are insightful but none of them examined th e aspect \nof creativity. Although mood classification is not the focus \nof this study, findings on the relationship between  mood \nand lyrics creativity suggest adding lyrics creativ ity fea- \ntures may help improve mood prediction accuracy.  \nPermission to make digital or hard copies of all or  part of this work for \npersonal or classroom use is granted without fee pr ovided that copies are \nnot made or distributed for profit or commercial ad vantage and that cop- \nies bear this notice and the full citation on the f irst page.  \nÂ© 2011 International Society for Music Information Retrieval  \n789Oral Session 9: Emotion and Mood\n  \n \n2.2  Linguistic Creativity  \nIn the text domain, creativity is a new topic compa red to \nother aspects of text analytics such as sentiment a nalysis \nand topic detection. There have been several worksh ops on \nlinguistic creativity which focused on â€œunusualâ€ la nguage \nphenomena, such as metaphor and plot. As one of the  few \nstudies on metrics and tools for measuring linguist ic crea- \ntivity, [21] proposed a set of creativity measures which in- \nspired this study. The measures will be described i n details \nin Section 4.  \n2.3  Mood and Creativity  \nMood and creativity, as two inherent traits of huma n nature, \nhave long been studied in psychology, sociology and  cul- \ntural studies (e.g., [1]), however, research on the  relation- \nship between mood and creativity has been inconclus ive. \nSome studies support that positive mood increases c ogni- \ntive flexibility and thus enhances creativity [12].  For ex- \nample, Isen et al. [13] found that human subjects g ave \nmore unusual word associations under positive-affec t con- \nditions, which suggested positive relationship betw een pos- \nitive mood and creativity. At the same time, other studies \nsupport that negative moods may promote artistic cr eativi- \nty and that positive moods restrain it [4][15]. Con sequently, \na context-dependent view is increasingly accepted r ecently \n[5].  \n3.  HYPOTHESES \nIn this research we focus on the relationship betwe en mood \nand linguistic creativity in lyrics. We test three hypotheses \nto investigate the relationship between mood and cr eativity. \nThe mood categories studied in this paper are liste d in Sec- \ntion 5.1, Table 1. \nHypothesis 1 (H1): Lyrics in sad mood category are \nmore creative than those in happy mood categories. \nHypothesis 2 (H2): Lyrics in negative mood category  are \nmore creative than those in sad mood categories. \nHypothesis 3 (H3): Lyrics in active mood category a re \nmore creative than those in passive mood categories .  \nH3  is based on the connection between creativity and ac- \ntive brain activities. High creativity is more like ly to be ob- \nserved in lyrics with intense emotion than in calm ones.  \n4.  LYRICS CREATIVITY MEASURES \nMeasuring creativity is difficult because the evalu ation \ncould be subjective to some extent. To obtain robus t result \nwe adopt multiple measures to gauge lyrics creativi ty. We \ncan draw strong conclusion if all measures led to c onsistent \nresults. The first measure that we adopt is Type-to -Token Ratio, which has long been used to measure vocabula ry \nrichness in creative writing [14]. However, we have  also \nbeen cautioned that Type-to-Token Ratio may not be relia- \nble for texts shorter than 350 words [7]. This is p articularly \nrelevant to this study because the average length o f lyrics is \nabout 200 words.  \nThe other measures used in this study are inspired by the \nwork of Zhu and colleagues [21]. As one of the few papers \napplying computational measures to predicting lingu istic \ncreativity, [21] proposed 13 linguistic measures an d built a \nlinear regression model to detect measures with mor e pre- \ndiction power. Two of the most salient measures in [21] are \nadopted in this study to measure lyrics creativity.  Both of \nthem are psycho-linguistic measures.  \n4.1  Type-to-Token Ratio \nType-to-Token Ratio is defined as the number of uni que \nterms in a piece of text divided by the number of t otal terms. \nIt is often used to measure the vocabulary richness  of text. \nSpecifically, this measure (denoted as r thereafter) is de- \nfined as:  \nnxCruniq / ) ( =                                                   (1) \nwhere uniq C denotes number of unique words in a piece of \nlyrics and n is the total number of words in it. In calculating  \nthis measure, we removed function words (also calle d \nstopwords) in the lyrics because they do not carry indepen- \ndent meanings and thus do not add to vocabulary ric hness. \nAs vocabulary richness is related to creativity, a lyric with \nhigher value of r is regarded as more creative.  \n4.2  Word Norms Fraction \nThis measure is to capture how â€œusualâ€ a text is. I n cogni- \ntive psychology experiments, word norms , which represent \nassociations between words, were collected by askin g hu- \nman subjects to freely recall associative words (re sponses) \nwhen they see the cue words (stimuli). Therefore, l yrics \nwith high occurrences of word norms should indicate  high \nâ€œusualnessâ€ and thus low creativity since creativit y often \ncorresponds to unusual patterns. \nSeveral existing word norms thesauri have been buil t by \ndifferent researchers in different countries. Becau se we are \ngoing to analyze U.S. rock lyrics, we choose a thes aurus \ndeveloped in U.S. to prevent cultural impact on wor d asso- \nciations. Specifically, we use the Free Association  Norms \nbuilt by researchers in University of South Florida  [16]. \nThis word norms dataset contains 72,176 pairs of as so- \nciated words. \nUsing the Free Association Norms, word norms fracti on \n(denoted as f thereafter) is calculated as:  \nny xCfnorm / ) , ( =                                             (2) \n79012th International Society for Music Information Retrieval Conference (ISMIR 2011)\n  \n \nWhere ) , (y xCnorm is the count of word pairs that appear \nin the Free Association Norms, and n represents number of \nwords in the text. \nAs lyrics are written in lines and, like a sentence , a lyric \nline can be regarded as a relatively independent un it, we \ncalculate the f measure for each lyric line and use the aver- \nage across all lines as a songâ€™s creativity score. A song with \nlower  value of  f measure is regarded as more creative than a \nsong with higher value of f measure. \n4.3  WordNet Similarity \nWordNet [3] is an English lexicon with marked lingu istic \nrelationships among word senses including hyponyms,  \nhypernyms, holonym, entailment, etc. With hyponyms and \nhypernyms, WordNet can be seen as a hierarchy of wo rd \nconcepts and from which similarities between concep ts can \nbe calculated. There are quite a few similarity mea sures de- \nfined to leverage WordNet. Following [21], we also used \npath similarity  in this study but we adopted a different \nsoftware implementation, namely the Word::Similarit y \nmodule in CPAN [17]. Path similarity between two co n- \ncepts is calculated by counting the nodes between t hem in \nthe WordNet concept hierarchy. The similarity score  be- \nlongs to the interval of (0, 1]. If two word senses  are in the \nsame synset , meaning the two are synonyms in WordNet, \nthe similarity score would be 1. The more nodes on the path \nconnecting two word senses, the smaller the similar ity \nwould be. If two word senses do not have a path in between, \nthen the similarity score is -1. \nJust as word norms fraction, we take one lyric line  as the \nunit to calculate the WordNet similarity (denoted a s s the- \nreafter). Stopwords are removed and all pairs of re maining \nwords in a lyric line are considered: \nny xSspath âˆ‘=) , (                                 (3) \nwhere ) , (y xSpath denote one word pair in a lyric line and n \nrepresents number of words in the line. The score o f a piece \nof lyrics is the average of scores across all lines . \nOne noteworthy issue is that WordNet is organized b y \nword senses instead of words. Because of the doubts  sur- \nrounding the effectiveness of Word Sense Disambigua tion \n(WSD), we did not conduct WSD and instead adopted a  \nsimplified approach that uses the highest similarit y between \nall senses of two words. \nSince s measures the similarity among words, a lyric \nwith lower s value would be regarded as more creative than \none with higher  s value.  5.  EXPERIMENT AND RESULTS \nTo test our hypotheses, we conducted an experiment to cal- \nculate and compare the aforementioned measures on a  data- \nset of lyrics with mood labels. \n5.1  The Dataset \nThis study adopts the Mood Tag Dataset 1 (MTD) used in \nthe Audio Tag Classification task in the Music Info rmation \nRetrieval Evaluation eXchange (MIREX) 2010. This da ta- \nset include 3,469 songs in 18 mood categories that cover \npositive, negative, active and passive moods. In th e MTD, \neach song is labeled one or more mood categories ac cord- \ning to the social tags applied to it [11]. It is no teworthy that \nmore than 90% of the songs in the MTD are in the ge nre of \nrock and more than 95% of them are U. S. songs. The refore, \nfindings of this study are limited to U. S. rock ly rics that are \nwritten in English.  \nFor the experiment, we constructed our dataset by c om- \nbining mood categories in the MTD into the categori es re- \nquired by the hypotheses. Combinations of categorie s are \nshown in Table 1.  \nLyrics in this study were downloaded from an online  lyr- \nic database, LyricWiki.com. One unique nature of ly rics is \nthat repetitions are very common (e.g., chorus is u sually \nrepeated multiple times). However, the creative mea sures \n(e.g., Type-to-Token Ratio, r) punish repetitions, which is \nlikely to be unfair in the case of lyrics. Unlike r epetitions in \nother genres of text, lyrics usually repeat a whole  line or \nparagraph. If a line is creative, then repeating it  is still, if \nnot more, creative. Therefore, to alleviate such bi as, repeti- \ntions of entire lines and paragraphs as well as not ations in \nthe lyrics were removed. \nIn order to avoid any bias caused by lyric length, we ex- \ncluded songs with lyrics that are too long (> 500 w ords) \nand too short (< 100 words). As the experiment resu lts sug- \ngested (see Section 6), the measures are indeed mor e or less \nsensitive to lyric length. We also balanced the dat asets by \nsetting the same number of songs in each comparable  cate- \ngories (e.g., positive vs. negative). In the cases where one \ncategory had more songs than the other as provided by the \nMTD, a random selection was conducted in the larger  cate- \ngory. Table 1 shows the combination of categories a nd lyr- \nics statistics. There are in total 2715 unique song s in this \nexperiment.  \n5.2  The Results \n5.2.1  Happy vs. Sad \nThe creativity measures on happy and sad songs are pre- \nsented in Table 2. A t-test was conducted to examine the \n                                                           \n1http://www.music-ir.org/mirex/wiki/2010:Audio_Tag_C lassification \n791Oral Session 9: Emotion and Mood\n  \n \nsignificance of the differences. The results indica te higher \ncreativity level (higher r, lower f and s) in sad lyrics based \non all the three creativity measures, and the diffe rence is \nconsistently significant across all measures. Our h ypothesis \nH1 is then not rejected. \n Category  Categories in \nMTD #.  of \nsongs  avg. lyric length \n(st.dev.) in words  \nH1  Happy glad, cheerful, \ngleeful  842  218.80 (77.19)  \nSad sad, mournful, \ngloomy, brooding  842  201.61 (73.36)  \nH2  Positive glad, cheerful, \ngleeful, confident \nhopeful, exiting 1470  220.45 (78.64)  \nNegative  sad, mournful, \ngloomy, brooding \nangry,  aggressive  1470    203.29 (75.62)  \nH3  Active aggressive, angry, \nexciting, gleeful  861  222.44 (83.05)  \nPassive calm, dreamy 861  206.07 (76.90)  \nTable 1. Lyrics categories and statistics. \nCategory  r f s \nHappy 0.5543  0.0563  -0.6344  \nSad 0.6042  0.0502  -0.6430  \np-value <0.0001  0.0030 0.0398  \nTable 2. Results of Hypothesis 1 \n5.2.2  Positive vs. Negative \nMeasures on positive and negative songs are present ed in \nTable 3. A t-test was conducted to examine the significance \nof the differences. We observed higher creativity l evel \n(higher r, lower f and s) in negative lyrics based on all the \nthree measures. The difference is significant accor ding to \neach of the measures. Our hypothesis H2 is not rejected. \nCategory  r F s \nPositive 0.5953  0.0557  -0.6366  \nNegative 0.6523  0.0490  -0.6431  \np-value <0.0001  <0.0001  0.0399  \nTable 3. Results of Hypothesis 2 \n5.2.3  Active vs. Passive \nMeasures on active and passive songs are presented in Ta- \nble 4. A t-test was conducted to examine the signif icance of \nthe differences. We have observed lower WordNet sim ilari- \nty (higher creativity) in passive songs, but higher  Type-to-\nToken Ratio (higher creativity) in active songs (al though \nthe difference was not significant for r). In addition, there was no significant difference in terms of unusual w ord as- \nsociations (as indicated by the f measure). Hence our hypo- \nthesis H3 is not consistently supported by all meas ures. \nCategory  r f s \nActive 0.5881  0.0525  -0.6322 \nPassive 0.5775  0.0526  -0.6419 \np-value 0.0648  0.4804  0.0149  \nTable 4. Results of Hypothesis 3  \n6.  DISCUSSION  \nTo further understand the relationship between lyri c crea- \ntivity and mood categories, we manually examined th e 10 \nmost creative and 10 least creative songs for each measure \n(except for f where 306 songs had the smallest value, 0). \nThe category distributions of these songs are liste d in \nTables 5 to 7. A general trend across these tables is that the \nmost creative songs include more sad and negative s ongs \nwhile the least creative songs consist of mostly ha ppy and \npositive songs. This observation is consistent with  the re- \nsults of statistical tests on Hypotheses 1 and 2. T he trend \nregarding active and passive songs differs across m easures, \nand thus once again we cannot draw consistent concl usion \non hypothesis 3. \nBesides statistics, it is helpful to look at the ly rics them- \nselves. The most creative song as measured by Type- to-\nToken Ratio ( r) is Elton Johnâ€™s â€œTiny Dancerâ€. Part of its \nlyrics is presented below. It is indeed more creati ve than \nmost songs, and it happens to be the only happy son g \namong the 10 most creative ones. This discrepancy w ith the \ngeneral trend is worth further study in the future.  \n... \nBallerina you must've seen her \nDancing in the sand \nAnd now she's in me always with me \nTiny dancer in my hand \n \nJesus freaks out in the street \nHanding tickets out for God \nTurning back she just laughs \nThe boulevard is not that bad \n... \n \nAs the second most creative song selected by WordNe t \nSimilarity ( s), â€œSomething in the Wayâ€ by Nirvana (Sad, \nNegative and Passive) contains the following lyrics : \nAnd the animals I trapped have all become my pets \nAnd I'm living off of grass and the drippings from \nmy ceiling \nIt's okay to eat fish 'cause they don't have any \nfeelings \nSomething in the way mmm \n... \n79212th International Society for Music Information Retrieval Conference (ISMIR 2011)\n  \n \nAs it can be seen, this piece of lyrics indeed has some \nunusual combination of words or concepts.    \n10 Most Creative \nSongs (0.931 â‰¤ r â‰¤ \n0.971)  Happy 1  Sad 5  \nPositive  2 Negative  4 \nActive 3  Passive 2  \n10 Least Creative \nSongs ( 0.105 â‰¤ r â‰¤ \n0.203)  Happy 9  Sad 1  \nPositive  9 Negative  1 \nActive 1  Passive 4  \nTable 5. Categories of most and least creative songs meas- \nured by Type-to-Token Ratio ( r) \n306 Most Creative \nSongs (f =0)  Happy 76  Sad 98  \nPositive  106  Negative  142  \nActive 84  Passive 113  \n10 Least Creative \nSongs ( 0.276 â‰¤ f â‰¤ \n0.369)  Happy 4  Sad 3  \nPositive  6 Negative  2 \nActive 6  Passive 3  \nTable 6 . Most and least creative songs as measured by \nWord Norm Fraction ( f) \n10 Most Creative \nSongs (-0.895 â‰¤ s â‰¤ \n-0.844) Happy 2  Sad 6  \nPositive  2 Negative  6 \nActive 0  Passive 3  \n10 Least Creative \nSongs (-0.244 â‰¤ s â‰¤ \n0.088) Happy 5  Sad 0  \nPositive  7 Negative  3 \nActive 4  Passive 2  \nTable 7 . Most and least creative songs as measured by \nWordNet Similarity ( s) \n  The fact that the word norm fraction measure ( f) was 0 \nfor 306 songs is interesting. A close inspection of  the lyrics \nreveals these 306 songs have short lyric lines. Thi s helps \nattribute the low f value to the way the measure was calcu- \nlated. The word pairs were formed with each line of  lyrics, \nand when the lines were short, there were few word pairs \nwhich resulted in fewer pairs matching the norms.  \nAnother observation is that there are controversial  songs. \nâ€œGangster Trippingâ€ by Fatboy Slim was listed as th e No.1 \nleast creative song by Type-to-Token Ratio ( r) but was the \nNo.1 most creative song selected by WordNet similar ity (s) \nand was among the most creative songs measured by w ord \nnorm fraction ( f). A close examination of the lyrics uncov- \ners the reason: there are many word repetitions and  thus its \nr value is very low. However, words in each line are  neither \nsimilar nor with usual associations. A typical snip pet of the \nlyrics is shown below:  \n... \nWe gotta kick that gangster shit \nC'mon we gotta kick that gangster shit C'mon we gotta get that \nget that get that get that get that get that get \nthat get that get that get that \n \nIt's what we're doin' when a \nWhat we're doin' when a \nWhat we're doin' when a fatboy's slippin' \n... \nSuch discrepancy on a single song discloses the lim ita- \ntions of the creativity measures. Type-to-Token Rat io just \ncaptures one kind of creativity, but it biases agai nst crea- \ntivity of repetitive patterns while repetition is a  common \nfeature of lyrics and does not necessarily indicate  less crea- \ntivity. Word norm fraction heavily relies on the gi ven asso- \nciation norms. Furthermore, it favors lyrics with s horter \nlines since there are fewer word pairs in shorter l ines, \nwhich possibly leads to lower scores (higher creati vity). On \nthe contrary, WordNet similarity favors longer lyri c lines as \nthose lines potentially contain more word pairs wit h simi- \nlarity value of -1 (no WordNet path between the wor ds) \nwhich contributed to a lower s value (higher creativity). In \naddition, WordNet similarity is limited by the hype rnym \nand hyponym hierarchy which is only available for n ouns \nand verbs. This analysis suggests that combination use of \nmultiple measures gives the most comprehensive esti mation \nof creativity as a multi-faceted linguistic phenome non. \n7.  CONCLUSIONS AND FUTURE WORK \nIn this study we examined the relationship between mood \nand creativity in U.S. rock lyrics. We used three c omputa- \ntional measures, Type-to-Token Ratio, Word Norm Fra c- \ntion, and WordNet Similarity, to gauge lyrics creat ivity, \nand then compared the difference in creativity betw een lyr- \nics in various mood categories. Because the three m easures \ncapture different aspects of linguistic creativity,  our result \nsuggests combination use of multiple measures to ga uge \nlyric creativity.  \nWe have also found that sad and negative lyrics cor res- \npond to higher linguistic creativity based on all t hree meas- \nures. This result supports previous studies on psyc holin- \nguistics that people write more creatively when the  text \nconveys sad or negative sentiment, but contradict p revious \nresearch that positive mood triggers more unusual w ord as- \nsociations. One interpretation is that the impact o f mood on \nthe task of writing a piece of text with certain th eme (like \nlyrics) is different from that on recalling free as sociation \nbetween words. The former one involves more specifi c de- \nscription. Furthermore, the correlation between cre ative \nwriting and negative emotion is actually reflected in voca- \nbulary of human languages. Sentiment lexicons in di fferent \nlanguages share a common feature that negative word s out- \nnumber positive words. Schrauf and Sanchez [19] als o \nfound that people recall more negative words than p ositive \n793Oral Session 9: Emotion and Mood\n  \n \nwords. These phenomena suggest that humans are actu ally \nbetter equipped with richer word choices when it co mes to \ndescribe negative emotions. \nThis study focuses on lexical creativity. As future  work \nit is worth investigating other dimensions of lingu istic crea- \ntivity: syntactic, morphological, and semantic crea tivity. \n8.  REFERENCES \n[1]  J. R., Averill, K. K., Chon, and D. W. Haan: \nâ€œEmotions and Creativity, East and West,â€ Asian \nJournal of Social Psychology , 4, pp.165-183. 2001. \n[2]  I. DeliÃ¨ge and G.A. Wiggins: Musical creativity: \nmultidisciplinary research in theory and practice , \nPsychology Press, New York, 2006.  \n[3]  C. Fellbaum ( eds. ): WordNet: An Electronic Lexical \nDatabase . MIT Press. 1998. \n[4]  K. Gasper: â€œWhen Necessity Is The Mother of \nInvention: Mood and Problem Solving,â€ Journal of \nExperimental Social Psychology , 39, 248 â€“262, 2003. \n[5]  J. M. George and J. Zhou: â€œUnderstanding When Bad \nMoods Foster Creativity and Good Ones Don't: The \nRole of Context and Clarity of Feelings,â€ Journal of \nApplied Psychology , 87(4), 687-697, 2002 \n[6]  H. He, J. Jin, Y. Xiong, B. Chen, W. Sun, and L. Zh ao: \nâ€œLanguage Feature Mining for Music Emotion Classi- \nfication via Supervised Learning From Lyrics,â€  In \nProceedings of Advances in the 3rd International \nSymposium on Computation and Intelligence, 2008. \n[7]  C. W. Hess and K. M. Sefton: â€œSample Size and Type-\nToken Ratio for Oral Language of Preschool Children ,â€ \nJournal of Speech and Hearing Research , vol. 29, pp. \n129-134. 1986. \n[8]  Y. Hu, X. Chen, and D. Yang: â€œLyric-Based Song \nEmotion Detection with Affective Lexicon and Fuzzy \nClustering Method,â€ In Proceedings of the 10th Inter- \nnational Conference on Music Information Retrieval , \n2009. \n[9]  X. Hu and J. S. Downie: â€œImproving Mood Classifica-\ntion in Music Digital Libraries by Combining Lyrics  \nand Audioâ€, In Proceedings of the 10th annual joint \nconference on Digital libraries, 2010. \n[10]  X. Hu and J.S. Downie: â€œWhen Lyrics Outperform \nAudio for Music Mood Classification: a Feature Anal - \nysisâ€, In Proceedings of the 11th International Sympo- \nsium on Music Information Retrieval , 2010. \n[11]  X. Hu, J. S. Downie, A. Ehmann: â€œLyric Text Mining \nin Music Mood Classificationâ€, In Proceedings of the \n10th International Symposium on Music Information \nRetrieval , 2009. [12]  E. R. Hirt, E. E. Devers, and S. M. McCrea: â€œI Want  \nto Be Creative: Exploring The Role of Hedonic \nContingency Theory in The Positive Mood-Creativity \nFlexibility Link,â€ Journal of Personality and Social \nPsychology , 94(2), pp. 214-230. 2008 \n[13]  A. M. Isen, M. M. Johnson, E. Mertz and G. F. \nRobinson: â€œThe Influence of Positive Affect on The \nUnusualness of Word Associations,â€ Journal of \nPersonality and Social Psychology , 48(6), pp. 1413-\n1426, 1985 \n[14]  D.A. Majid, A-G. Tan and K-C. Soh: â€œEnhancing \nChildrenâ€™s Creativity: An Exploratory Study on Usin g \nthe Internet and SCAMPER As Creative Writing \nToolsâ€, Korean Journal of Thinking and Problem \nSolving , 13(2), pp. 67-81, 2003. \n[15]  A. M, Mendes: â€œThe Dark Side of Creativity: \nBiological Vulnerability and Negative Emotions Lead  \nto Greater Artistic Creativity,â€ Personality and Social \nPsychology Bulletin  34(12), pp.1677-86, 2008  \n[16]  D. L., Nelson, C. L., McEvoy, and T. A. Schreiber: \nThe University of South Florida word association, \nrhyme, and word fragment norms . 1998. \n[17]  S. Patwardhan and T. Pedersen: â€œUsing WordNet-\nbased Context Vectors to Estimate the Semantic \nRelatedness of Concepts,â€ In the Proceedings of the \nEACL 2006 Workshop Making Sense of Sense - \nBringing Computational Linguistics and \nPsycholinguistics Together . Trento, Italy. 2006. \n[18]  J. R. Robinson: â€œWebster's Dictionary Definition of  \nCreativity,â€ Online Journal for Workforce Education \nand Development , 3(2), 2007, Available at: \nhttp://opensiuc.lib.siu.edu/ojwed/vol3/iss2/2  \n[19]  R. W. Schrauf and J. Sanchez: â€œThe Preponderance of  \nNegative Emotion Words across Generations nnd \nAcross Cultures,â€ Journal of Multilingual and \nMulticultural Development , 25(2-3), pp. 266-284, \n2004. \n[20]  B. Schuller, C. Hage, D. Schuller, and G. Rigoll: \nâ€œMister D.J., Cheer Me Up!: Musical and Textual \nFeatures for Automatic Mood Classificationâ€, Journal \nof New Music Research , 39(1), pp. 13-34, 2010. \n[21]  X. Zhu, Z. Xu, and T. Khot: â€œHow Creative Is Your \nWriting? A Linguistic Creativity Measure from \nComputer Science and Cognitive Psychology \nPerspectives,â€ In NAACL 2009 Workshop on \nComputational Approaches to Linguistic Creativity , \n2009. \n794"
    },
    {
        "title": "Melody Extraction based on Harmonic Coded Structure.",
        "author": [
            "Sihyun Joo",
            "Sanghun Park",
            "Seokhwan Jo",
            "Chang D. Yoo"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1417243",
        "url": "https://doi.org/10.5281/zenodo.1417243",
        "ee": "https://zenodo.org/records/1417243/files/JooPJY11.pdf",
        "abstract": "This paper considers a melody extraction algorithm that estimates the melody in polyphonic audio using the harmonic coded structure (HCS) to model melody in the minimum mean-square-error (MMSE) sense. The HCS is harmonically modulated sinusoids with the amplitudes defined by a set of codewords. The considered algorithm performs melody extraction in two steps: i) pitch-candidate estimation and ii) pitch-sequence identification. In the estimation step, pitch candidates are estimated such that the HCS best represents the polyphonic audio in the MMSE sense. In the identification step, a melody line is selected from many possible pitch sequences based on the properties of melody line. Posterior to the melody line selection, a smoothing process is applied to refine spurious pitches and octave errors. The performance of the algorithm is evaluated and compared using the ADC04 and the MIREX05 dataset. The results show that the performance of the proposed algorithm is better than or comparable to other algorithms submitted to MIREX2009.",
        "zenodo_id": 1417243,
        "dblp_key": "conf/ismir/JooPJY11",
        "keywords": [
            "melody extraction algorithm",
            "polyphonic audio",
            "harmonic coded structure",
            "MMSE sense",
            "pitch-candidate estimation",
            "pitch-sequence identification",
            "HCS",
            "amplitudes defined by a set of codewords",
            "posterior to the melody line selection",
            "smoothing process"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nMELODY EXTRACTION BASED ON HARMONIC CODED STRUCTURE\nSihyun Joo Sanghun Park Seokhwan Jo Chang D. Yoo\nDepartment of Electrical Engineering, Korea Advanced Institute of Science and Technology,\n373-1 Guseong-dong, Yuseong-gu, Daejeon, 305-701, Korea\n{s.joo,psh111,antiland00 }@kaist.ac.kr cdyoo@ee.kaist.ac.kr\nABSTRACT\nThis paper considers a melody extraction algorithm that es-\ntimates the melody in polyphonic audio using the harmonic\ncoded structure (HCS) to model melody in the minimum\nmean-square-error (MMSE) sense. The HCS is harmoni-\ncally modulated sinusoids with the amplitudes deï¬ned by a\nset of codewords. The considered algorithm performs melody\nextraction in two steps: i) pitch-candidate estimation and ii)\npitch-sequence identiï¬cation. In the estimation step, pitch\ncandidates are estimated such that the HCS best represents\nthe polyphonic audio in the MMSE sense. In the identiï¬ca-\ntion step, a melody line is selected from many possible pitch\nsequences based on the properties of melody line. Posterior\nto the melody line selection, a smoothing process is applied\nto reï¬ne spurious pitches and octave errors. The perfor-\nmance of the algorithm is evaluated and compared using the\nADC04 and the MIREX05 dataset. The results show that\nthe performance of the proposed algorithm is better than or\ncomparable to other algorithms submitted to MIREX2009.\n1. INTRODUCTION\nMost people recognize music as a sequence of notes re-\nferred to as melody. Melody extraction from polyphonic\naudio is developed for various applications such as content-\nbased music information retrieval (CB-MIR), audio plagia-\nrism search, automatic melody transcription, music analy-\nsis, and query by humming (QBH) [1, 2, 6]. Despite its im-\nportance in various applications, melody is not clearly de-\nï¬ned [3,4,6]. However, many people consider melody as the\nmost dominant single pitch sequence of a polyphonic audio\nand the considered algorithm extracts melody following this\nconsideration.\nDiverse melody extraction or transcription techniques have\nbeen proposed in recent years. Goto introduced a predomi-\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proï¬t or commercial advantage and that copies\nbear this notice and the full citation on the ï¬rst page.\ncâƒ2011 International Society for Music Information Retrieval.nant F0 estimation (PreFEst) algorithm [3]. It estimates the\nweights of prior tone-models over all possible fundamental\nfrequencies(F0s) based on the maximum a posteriori (MAP)\ncriterion and determines the F0â€™s temporal continuity by us-\ning a multiple-agent architecture. Paiva estimated possible\nF0s in the short-time Fourier transform (STFT) magnitude\ndomain and decides a single pitch sequence (melody line)\nbased on various properties of melody pitches between near\nframes [5]. Poliner and Ellis approached the melody line es-\ntimation problem as a classiï¬cation problem and use a sup-\nport vector machine (SVM) classiï¬er in the estimation [7].\nRyyn Â¨anen deï¬ned an acoustic model based on the hidden-\nMarkov model (HMM) to estimate melody, bass line and\nchords [1]. Durrieu extracted melody of singing voice by\nseparating singerâ€™s voice and background music [2].\nThere are two main obstacles in extracting accurate melody\nline [9]. The obstacles are listed below:\n1)Accompaniment interference: Accompaniment sound\nsuch as harmonics of subdominant melodies and per-\ncussive sound acts as noise in the melody pitch esti-\nmation.\n2)Octave mismatch: Inaccurate melody pitch values which\nare one octave higher or lower than the ground-truth\nare often inaccurately estimated: the true melody pitch\nharmonics appear at either all estimated pitch harmonic\nlocations or every other pitch harmonic locations.\nIn this paper, an effective melody extraction algorithm\nthat considers the above obstacles is proposed. The algo-\nrithm deï¬nes a harmonic structure as a model for melody.\nRelated models have been studied for other related applica-\ntions. Heittola modeled the signal as a sum of spectral bases\nfor sound separation [10]. Duan used pre-coded spectral\npeak/non-peak position of each possible pitches for pitch\ntracking [11]. Bay used pre-coded harmonic structure shape\nfor source separation [12]. Goto modeled a pitch harmonics\nas a Gaussian mixture model [3].\nThe proposed algorithm minimizes the mean-square er-\nror between the given polyphonic audio and the harmonic\ncoded structure (HCS) that is constructed from a codebook\n227Poster Session 2\n'UVKOCVGF\u0003\nOGNQF[\u0003NKPGN-best  Melody\nCandidates Selection\nRule-based\nL-best Melody \nPitch Sequence EstimationMelody Pitch Sequence\nDecision2QN[RJQPKE\u0003\nCWFKQStep 1. PITCH-CANDIDA TE ESTIMATION\nStep 2. PITCH-SEQUENCE IDENTIFICATIONMMSE\nEstimationHCS \nConstructor\nHCS\nSelection\nSmoothing\nProcessCodebook\n;\n; ZHLJKWFigure 1 . System Overview\nof harmonic amplitude set. The codebook was deï¬ned by k-\nmeans clustering the harmonic amplitudes of training melody\ndata. The algorithm ï¬nds N-best pitch candidates for each\nframe and subsequently determines the best melody line from\nthe pitch candidates by a rule-based identiï¬cation proce-\ndure.\nThe remainder of this paper is organized as follows. Sec-\ntion 2 describes the proposed melody extraction algorithm.\nSection 3 shows experimental results of the proposed algo-\nrithm and compares the performance to other previous algo-\nrithms. Finally, Section 4 concludes this paper.\n2. MELODY EXTRACTION ALGORITHM\nThe overall structure of the proposed algorithm is shown\nin Figure 1. The proposed algorithm extracts melody pitch\nsequence (melody line) in two steps: i) pitch-candidate esti-\nmation and ii) pitch-sequence identiï¬cation. In the estima-\ntion step, Nmelody pitch candidates are extracted by ï¬nd-\ningNmost dominant HCS by minimizing minimum-mean-\nsquared error between the magnitude of STFT of framed\npolyphonic audio using the window function w[n]and a\nweighted HCS. In the identiï¬cation step, the melody pitch\nsequence is estimated based on a certain set of rules of melody\nline, after which a simple smoothing process is applied. Melody\nline is decided by ï¬rst selecting L-best melody line from a\nsequence of Npitch candidates and then determining the\nmost appropriate melody line from the selection. The smooth-\ning process is performed to remove spurious pitch sequences\nand octave errors.2.1 Melody Pitch Candidate Estimation\n2.1.1 Construction of HCS\nIn this paper, a harmonic coded structure (HCS) is proposed\nto ï¬nd the dominant melody pitch harmonics in the STFT\ndomain. The windowed harmonic structure can be expressed\nas follows:\nhÎ·[n]=w[n]Hâˆ‘\nm=1bmcos(mÂ·2Ï€Î·Â·n+Ï•m), H=âŒŠfs\n2Î·âŒ‹,(1)\nwhere fs, Î·, w [n], bm, and Ï•mare sampling frequency, the\nfundamental frequency (F0) of the HCS, analysis window,\namplitude of the mth harmonic, and the phase of the mth\nharmonic, respectively. The discrete-time Fourier transform\n(DTFT) of hÎ·[n],HÎ·(Ï‰), can be expressed as follows:\nHÎ·(Ï‰)=Hâˆ‘\nm=1BmW(Ï‰âˆ’mÎ·), B m=bmeâˆ’jÏ•m,(2)\nwhere W(Ï‰)is the DTFT of w[n].\nThe number of harmonics within a certain bandwidth de-\npends on the pitch and the sampling frequency as deï¬ned in\n(1), but we observe that the harmonic amplitudes tend to de-\ncrease with increasing harmonic index ( |Bm|<|Bmâˆ’1|for\nm= 2,Â·Â·Â·, H). For this reason, we use only 11 harmonics.\nThe overall envelop of the harmonic amplitudes varies\nwith instrument and pitch [13]. Therefore, it is difï¬cult to\nconstruct one ï¬xed melody harmonic structure that ï¬ts all\nthe different harmonic amplitude patterns.\nTo construct a HCS to represent all the different har-\nmonic amplitudes of melody, a codebook is constructed from\nreal audio sample data. Harmonic amplitudes from 26,930\nframes of piano sound, 74,631 frames of saxophone sound\n[14], and 449,430 frames of singing voice [15] are used\n22812th International Society for Music Information Retrieval Conference (ISMIR 2011)\n(a)\n (b)\n(c)\nFigure 2 . Three estimated harmonic structures when k= 3\nand the F0 =400Hz: (a) The ï¬rst harmonic structure ( i=\n1), (b) the second harmonic structure ( i= 2), and (c) the\nthird harmonic structure ( i= 3).\nto build the codebook: these three sounds are present as\nmelody in all music considered.\nThe harmonic amplitude samples are clustered using the\nextended kâˆ’means clustering algorithm [16] and the cen-\ntroids of each cluster are used as codewords. Finally, the\nHCSs for every possible F0 are constructed using (1) and\n(2) based on the codebook. Figure 2 illustrates HCSs when\nk= 3and the F0 =400Hz.\n2.1.2 N-Best Melody Pitch Candidates Estimation\nThe proposed algorithm extracts Nmelody pitch candidates\nfrom each frame of a given polyphonic audio to reduce pitch\nestimation errors due to accompaniment interference and\noctave mismatch.\nThe pitch candidates are estimated based on the consen-\nsus that melody is considered as the single dominant pitch\nsequence in a polyphonic audio. To ï¬nd the dominant pitch\ncandidates of each frame, a cost function based on the ith\nHCS, Ji(Î·, l), is deï¬ned as follows:\nJi(Î·, l) =âˆ«Ï€\nâˆ’Ï€(\n|S(Ï‰, l)|\nâˆ’Ci(Î·, l)Hâˆ‘\nm=âˆ’H,\nmÌ¸=0Ai,m|W(Ï‰âˆ’mÎ·)|)2\ndÏ‰, (3)\nwhere S(Ï‰, l)andCi(Î·, l)are the STFT coefï¬cient of the\nlth frame at frequency Ï‰and the weight of the ith HCS\nwhich is constructed with the ith codeword in the lth frame\n(a)i= 1\n (b)i= 2\n(c)i= 3\nFigure 3 . The cost of the lth frame given by (3). The circles\n(â—¦) indicate Jâ€²\ni(l)of each HCS.\nwith F0 =Î·, respectively. Here, Ai,mis the harmonic am-\nplitude of the mth harmonic of the ith codeword. The STFT\nmagnitude of each frame and the HCS with F0 =Î·satisfy\nthe following constraints:\nâˆ«Ï€\nâˆ’Ï€|S(Ï‰, l)|dÏ‰= 1, (4)\nand\nâˆ«Ï€\nâˆ’Ï€Hâˆ‘\nm=âˆ’H,\nmÌ¸=0Ai,m|W(Ï‰âˆ’mÎ·)|dÏ‰= 1. (5)\nThe HCS represents only the form of the harmonics, not\nthe exact magnitude of harmonics so scaling is required where\nthe weight Ci(Î·, l)is chosen to minimize the cost given in\n(3), thus\nË†Ci(Î·, l) = argmin\nCi(Î·,l)Ji(Î·, l). (6)\nTo ï¬nd Ë†Ci(Î·, l),Ji(Î·, l)is differentiated with respect to\nCi(Î·, l)and set equal to zero. It yields\nË†Ci(Î·, l)=âˆ«Ï€\nâˆ’Ï€|S(Ï‰, l)|( Hâˆ‘\nm=âˆ’H,\nmÌ¸=0Ai,m|W(Ï‰âˆ’mÎ·)|)\ndÏ‰\nâˆ«Ï€\nâˆ’Ï€( Hâˆ‘\nm=âˆ’H,\nmÌ¸=0Ai,m|W(Ï‰âˆ’mÎ·)|)2\ndÏ‰.(7)\nPrior to extracting melody pitch candidates, the mini-\nmum cost of the lth frame using the ith HCS J(min)\ni (l)de-\nï¬ned below is estimated.\nJ(min)\ni (l) = min\nÎ·Ë†Ji(Î·, l), (8)\n229Poster Session 2\nwhere\nË†Ji(Î·, l) =âˆ«Ï€\nâˆ’Ï€(\n|S(Ï‰, l)|\nâˆ’Ë†Ci(Î·, l)Hâˆ‘\nm=âˆ’H,\nmÌ¸=0Ai,m|W(Ï‰âˆ’mÎ·)|)2\ndÏ‰. (9)\nFigure 3 shows the cost of each HCS of the lth frame when\nk= 3, and the costs of the circled peaks indicate J(min)\ni (l).\nNow, the index of the HCS of the lth frame I(l)is esti-\nmated by\nI(l) = argmin\niJ(min)\ni (l). (10)\nGenerally, harmonic amplitudes of consecutive frames are\nhighly correlated [9]. Thus, the index of HCS that appears\nfrequently within a neighborhood of few frames (including\nthe target frame) should be determined as a more consistent\nindex of the current frame. The updated index of the lth\nframe is expressed as follows:\nË†I(l) = mode[ I(lâˆ’M), I(lâˆ’M+ 1),\nÂ·Â·Â·, I(l+Mâˆ’1), I(l+M)]. (11)\nwhere Mis the number of neighbor frames considered on\neither side of the lth frame.\nThe costs of possible F0s can be ï¬nally calculated using\n(3) with the weight obtained from (7) and the index deter-\nmined by (11). To obtain a set of Npossible melody pitch\ncandidates of the lth frame, the following procedure is per-\nformed in obtaining the set Nlfor the lth frame.\nAlgorithm 1 N-best Pitch Candidates Determination\nNl={}\nforn= 1, ..., N do\nÂ¯Î·= argminÎ·Ì¸âˆˆNlJË†I(l)(Î·, l)\nNlâ†N lâˆªÂ¯Î·\nend for\nFigure 4 (a) and (b) illustrate the STFT magnitude of a\nframe and its cost, respectively for N= 5. The circles in\n(b) indicate the estimated melody pitch candidates of the\nframe.\n2.2 Melody Pitch Sequence Identiï¬cation\nOnce the N-best pitch candidates of each frame are obtained\nas described in the previous section, a single pitch sequence\n(melody line) that best represents the melody line is iden-\ntiï¬ed. An estimate of the melody line can be obtained by\nselecting the pitch candidate leading to the minimum cost\nfor each frame. This, however, often leads to inaccurate\nestimation due to accompaniment interference and octave\n(a)\n (b)\nFigure 4 . The STFT magnitude and the cost of the lth\nframe: (a)|S(Ï‰, l)|, (b) the cost of the lth frame obtained\nby an appropriate HCS.\nmismatch. Inaccuracy can be reduced by considering the\nforward and backward relationship among pitch candidates.\nThe proposed identiï¬cation algorithm estimates the melody\nline based on a rule-based method described below.\nA more robust melody pitch sequence is obtained by the\nfollowing two steps: i) L-best melody pitch sequences are\ndetermined and ii) melody is determined as the melody pitch\nsequence with the minimum sum cost. (see Figure 1).\n2.2.1 L-Best Melody Pitch Sequence Determination\nThe proposed melody line identiï¬cation algorithm estimates\nL-best melody lines from N-best pitch candidates of each\nframe based on the following properties of melody line.\nP1Thevibrato exhibits an extent of Â±60âˆ¼200cent for\nsinging voice and only Â±20âˆ¼30cent for music in-\nstruments such as saxophone, violin, and guitar [17].\nP2The note transitions within a musical structure are\ntypically limited to an octave [8].\nP3In general, a rest during singing is longer than 50 ms.\nBased on the above properties, the following rules are\ndeï¬ned to estimate the melody line.\nR1Any two pitch candidates of successive frames are\nconsidered to be included in same melody line seg-\nment when the difference between the pitch values is\nless than the threshold described in P1.\nR2When two non-consecutive frames with a time gap\nless than 50ms have pitch candidates satisfying P1,\nthen interpolate between the two pitch values (by P3).\nR3When any two pitch candidates of successive frames\nsatisfy only P2and not P1andP3, a transition is as-\nsumed to have occurred in the melody line.\nIn the proposed algorithm, the threshold discussed in R1\nis set to 100 centwhich was determined experimentally from\nthe validation data. When one of the L-melody lines does\nnot satisfy the given rules, all melody lines are disconnected\nand a new set of L-melody lines are started.\n23012th International Society for Music Information Retrieval Conference (ISMIR 2011)\n(a)\n (b)\nFigure 5 . Melody pitch sequence estimation: (a) three-best\nmelody pitch sequence estimation, (b) best melody pitch se-\nquence decision.\n2.2.2 Melody Pitch Sequence Decision\nA single melody pitch sequence must be selected from the\nL-best lines. The best melody pitch sequence is estimated\nbased on the melody deï¬nition: melody is a dominant pitch\nsequence in a polyphonic audio. Hence, after adding up\nthe costs in each melody line segment, the pitch sequence\nthat has the minimum summed-cost is selected as the best\nmelody line segment. Figure 5 (a) and (b) show the result of\nL-best melody pitch sequence estimation and melody pitch\nsequence decision, respectively. The vertical dotted lines in\n(a) represent the disconnecting positions, and the pitch se-\nquences between two vertical dotted lines are considered as\nmelody line candidates.\n2.2.3 Smoothing Process\nAlthough the procedures described in Section 2.2.1 and 2.2.2\neffectively reduce accompaniment interference and octave\nmismatch, it is difï¬cult to estimate the true melody pitch\nsequence if the interference occurs throughout the melody\nline. Thus, a smoothing process is applied to ï¬nd a more\nrobust melody line.\nAfter the single melody pitch sequence is estimated, spu-\nrious sequences are removed and replaced with interpolated\npitch values between non-spurious pitches. The spurious\nsequence is determined by following conditions. i) A pitch\nsequence which switches to another note and returns to the\noriginal note within short time is considered as the spurious\nsequence. ii) A pitch sequence which has a transition over\none octave is also regarded as an inaccurate estimate.\n3. EVALUATION\nTwo CD-quality (16-bit quantization, 44.1 kHz sample rate)\ntest datasets are used for evaluation. One dataset used for\nthe evaluation is the Audio Description Contest (ADC) 2004\ndataset, and the other is the Music Information Retrieval\nEvaluation eXchange (MIREX) 2005 dataset. Table 1 shows\nthe conï¬gurations of the evaluation datasets.\nIn the experiment, the possible fundamental frequency\nrange is set from 80Hz (3950 cent) to 1280Hz (8750 cent)Dataset Melody Number of ï¬les\nADC04 Vocal melody 8\nNonvocal melody 12\nMIREX05 Vocal melody 9\nNonvocal melody 4\nTable 1 . Evaluation dataset.\nDataset Algorithms RPA (%) RCA (%)\nADC04 Cao et al. 85.1 86.3\nDurrieu et al. 81.4 83.4\nHsu et al. 63.9 73.6\nDressler 87.1 87.6\nWendelboe 82.3 86.4\nCancela 82.9 83.4\nRao et al. 76.9 85.1\nTachibana et al. 61.0 71.8\nProposed 81.8 86.0\nMIREX05 Ryyn Â¨anen et al. [1] 67.3 69.1\nDurrieu et al. [19] 74.5 79.6\nTachibana et al. [20] 74.0 76.7\nProposed 76.1 80.7\nTable 2 . Result Comparison.\nand 3 clusters are used for building codebook ( k= 3). In\nthe melody pitch candidate estimation step, 3-best pitch can-\ndidates are chosen for each frame ( N= 3) and the number\nof neighbor frames for deciding harmonic structure is set to\n7 (M= 7). In the melody pitch sequence identiï¬caion step,\n3-best melody lines are estimated ( L= 3). These values are\ndetermined experimentally.\nThe estimated melody pitch is considered correct when\nthe absolute value of the difference between the ground-\ntruth and the estimated pitch frequency is less than quarter\ntone (50 cent). This is shown as\n|Fg(l)âˆ’Fe(l)| â‰¤1\n4tone (50cent ), (12)\nwhere Fg(l)andFe(l)denote ground-truth and estimated\npitch frequency of the lth frame, respectively.\nThe performance of the proposed algorithm is evaluated\nwith row pitch accuracy (RPA) and row chroma accuracy\n(RCA) [8].\nTable 2 shows the evaluation results for all algorithms\nconsidered. The results on the ADC04 dataset are from the\nMIREX 2009 homepage [18]. When obtaining the results\non the MIREX05 dataset, we referred the results in [20] or\nused the codes publicly released by the authors [1, 21]. The\nbest result on each dataset is underlined, and the result of the\nproposed algorithm is highlighted in bold. The proposed\n231Poster Session 2\nalgorithm achieved the best performance both in RPA and\nRCA on the MIREX05 dataset. It also performed compara-\nbly to the other algorithms on the ADC04 dataset.\n4. CONCLUSION\nIn this paper, an algorithm extracting melody from a poly-\nphonic audio using the HCS which is constructed from the\ncodebook of harmonic amplitude set obtained by k-means\nclustering is considered. The algorithm focuses on reducing\naccompaniment interference and octave mismatch. The al-\ngorithm consists of two steps: N-best pitch candidates esti-\nmation step and rule-based melody identiï¬cation step. First,\nmultiple pitch candidates of each frame are estimated us-\ning the cost function which determines the most dominant\nHCS of the frame in the MMSE sense. Second, a single\npitch sequence (melody line) is identiï¬ed based on certain\nrules of melody line. To handle the spurious pitch sequence\nproblem, the smoothing process is applied. The considered\nalgorithm is tested on two datasets: the ADC04 dataset and\nthe MIREX05 dataset. Experimental results show that the\nproposed algorithm is better than or comparable to the other\nmelody extraction algorithms.\n5. REFERENCES\n[1]M. P. Ryyn Â¨anen and A. P. Klapuri: â€œAutomatic tran-\nscription of melody, bass line, and chords in polyphonic\nmusic,â€ Computer Music Journal , Vol.32, No.3, pp. 72â€“\n86, 2008.\n[2]J.-L. Durrieu, G. Richard, and B. David: â€œSinger melody\nextraction in polyphonic signals using source separation\nmethods,â€ in Proceedings of the ICASSP , 2008.\n[3]M. Goto: â€œA real-time music-scene-description sys-\ntem: predominant-f0 estimation for detecting melody\nand bass lines in real-world audio signals,â€ Speech Com-\nmuncation , Vol.43, No.4, pp. 311â€“329, 2004.\n[4]R. P. Paiva: â€œAn approach for melody extraction from\npolyphonic audio: Using perceptual principles and\nmelodic smoothness,â€ The Journal of the Acoustical So-\nciety of America , Vol.122, No.5, pp. 2962â€“2969, 2007.\n[5]R. P. Paiva, T. Mendes, and A. Cardoso: â€œA methodol-\nogy for detection of melody in polyphonic music sig-\nnals,â€ AES 116th Convention , 2004.\n[6]V. Rao and P. Rao: â€œVocal melody extraction in the pres-\nence of pitched accompaniment in polyphonic music,â€\nIEEE ASLP , Vol.18, No.8, pp. 2145â€“2154, 2010.\n[7]G. E. Poliner and D. P. W. Ellis: â€œA classiï¬cation ap-\nproach to melody transcription,â€ in Proceedings of the\nISMIR , 2005.[8]G. E. Poliner, D. P. W. Ellis, and A. F. Ehmann: â€œMelody\ntranscription from music audio: approach and evalua-\ntion,â€ IEEE ASLP , Vol.15, No.4, pp. 1247â€“1256, 2007.\n[9]S. Jo, and C. D. Yoo: â€œMelody extraction from poly-\nphonic audio based on particle ï¬lter,â€ in Proceedings of\nthe ISMIR , 2010.\n[10] T. Heittola, A. Klapuri and T. Virtanen: â€œMusical instru-\nment recognition in polyphonic audio using source-ï¬lter\nmodel for sound separation,â€ in Proceedings of the IS-\nMIR, 2009.\n[11] Z. Duan, J. Han and B. Pardo: â€œHarmonically in-\nformed multi-pitch tracking,â€ in Proceedings of the IS-\nMIR, 2009.\n[12] Mert Bay, James W. Beauchamp: â€œHarmonic source\nseparation using prestored spectra,â€ in Proceedings of\nthe ICA , pp. 561â€“568, 2006.\n[13] Beauchamp, J. W.: â€œAnalysis and Synthesis of Musi-\ncal Instrument Soundsâ€ in Analysis, Synthesis, and Per-\nception of Musical Sounds: The Sound of Music , ed\n(Springer), pp. 1â€“89, 2007.\n[14] L. Fritts: University of Iowa musical instument samples ,\nhttp://theremin.music.uiowa.edu/MIS.html.\n[15] C.-L. Hsu and J.-S. R. Jang: MIR-1K Dataset ,\nhttp://sites.google.com/site/unvoicedsoundseparation/mir-\n1k.\n[16] D. Pelleg and A. W. Moore: â€œ X-means: Extending K-\nmeans with efï¬cient estimation of the number of clus-\nters,â€ in Proceedings of the ICML , 2000.\n[17] R. Timmers and P. W. M Desain: â€œVibrato: the questions\nand answers from musicians and science,â€ the Interna-\ntional Conference on Music Perception and Cognition ,\n2000.\n[18] MIREX2009:Audio Melody Extraction Results ,\nhttp://www.music-ir.org/mirex/wiki/2009:Audio\nMelody Extraction Results.\n[19] J.-L. Durrieu, G. Richard, B. David, and C. F Â´evotte:\nâ€œSource/Filter Model for Unsupervised Main Melody\nExtraction From Polyphonic Audio Signalsâ€ IEEE\nASLP , Vol.18, No.3, pp. 564â€“575, 2010.\n[20] H. Tachibana, T. Ono, N. Ono, and S. Sagayama:\nâ€œMelody line estimation in homophonic music au-\ndio signals based on temporal-variability of melodic\nsource,â€ Proceedings of the ICASSP , 2010.\n[21] http://www.durrieu.ch/phd/software.html\n232"
    },
    {
        "title": "Low Dimensional Visualization of Folk Music Systems Using the Self Organizing Cloud.",
        "author": [
            "ZoltÃ¡n JuhÃ¡sz"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1417433",
        "url": "https://doi.org/10.5281/zenodo.1417433",
        "ee": "https://zenodo.org/records/1417433/files/Juhasz11.pdf",
        "abstract": "We describe a computational method derived from self organizing mapping and multidimensional scaling algorithms for automatic classification and visual clustering of large vector databases. Testing the method on a large corpus of folksongs we have found that the performance of the classification and topological clustering was significantly improved compared to current techniques. Applying the method to an analysis of the connections of 31 Eurasian and North-American folk music cultures, a clearly interpretable system of musical connections was revealed. The results show the relevance of the musical language groups in the oral tradition of the humanity.",
        "zenodo_id": 1417433,
        "dblp_key": "conf/ismir/Juhasz11",
        "keywords": [
            "computational method",
            "self organizing mapping",
            "multidimensional scaling",
            "automatic classification",
            "visual clustering",
            "large vector databases",
            "performance improvement",
            "corpus of folksongs",
            "clearly interpretable system",
            "oral tradition"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\n  \n \nLOW DIMENSIONAL VISUALISATION OF FOLK MUSIC \nSYSTEMS USING THE SELF ORGANISING CLOUD \n ZoltÃ¡n JuhÃ¡sz  \n Res Inst. For Technical Physics and Mate- \nrials Sciences, H-1525 Budapest, Hungary \nP.O.B. 49. \njuhasz@mfa.kfki.hu   \nABSTRACT \nWe describe a computational method derived from sel f or- \nganizing mapping and multidimensional scaling algor ithms \nfor automatic classification and visual clustering of large \nvector databases. Testing the method on a large cor pus of \nfolksongs we have found that the performance of the  classi- \nfication and topological clustering was significant ly im- \nproved compared to current techniques. Applying the  \nmethod to an analysis of the connections of 31 Eura sian and \nNorth-American folk music cultures, a clearly inter pretable \nsystem of musical connections was revealed. The res ults \nshow the relevance of the musical language groups i n the \noral tradition of the humanity.  \n1.  INTRODUCTION \nThe comparative study of different folk music cultu res goes \nback to the early 20th century [1-2]. Although ethn omusi- \ncologists seemed to gradually forget the conception  of the \nclassical structural analysis and classification, t he develop- \nment of the computation tools led to a renaissance of the \nidea in recent years [3-4].  At the same time, the number of \nrepresentative national/regional digital folksong d atabases \nis also increasing rapidly. Therefore, a computer a ided \ncomparison of different musical cultures in order t o reveal \nhidden contacts of different musical cultures becam e very \ntopical. \nCurrent interdisciplinary research, based on the co operation \nof musicology, artificial intelligence research and  data min- \ning, focuses on automatic similarity measurement, s egmen- \ntation, contour analysis and classification using d ifferent \nstatistical characteristics, e.g. pitch-interval or  rhythm dis- \ntribution. A very widely used kind of artificial ne ural net- \nworks, the self organising map (SOM) proved to be a  very \nversatile tool of computing musicology [5]. SOM-bas ed \nsystems have been elaborated for simultaneous analy sis of \nthe contour as well as the pitch, interval and dura tion distri- \nbutions, based on the symbolic representation of th e music \n[6]. A cross-cultural study of different musical cu ltures was \nalso based on SOM technique [7].  \nThe operation of a SOM can be summarised for our ca se as \nfollows: Our input data to be classified are contou r vectors, containing subsequent pitch values of melodies of a  folk- \nsong database. The main goal of self organising map ping is \nto characterise the multidimensional point system c on- \nstructed by the set of these melody contour vectors  by a sig- \nnificantly smaller set of â€œcontour type vectorsâ€ de scribing \nthe average contours in the local condensations of the input \ncontour vectors. Although the details of the calcul ations are \ndifferent, this goal essentially corresponds to tha t of the so-\ncalled K-means algorithm [8].  However, the SOM pro - \nduces something more: it assigns the resulting cont our type \nvectors to the lattice points of a grid topographic ally.  The \ntopographic structure of the resulting map is provi ded by a \ncooperative learning, modifying the contour type ve ctors \nlocated in neighbouring lattice points in parallel.  As a result \nof this local cooperation, similar contour type vec tors are \nlocated in neighbouring lattice points after learni ng.  \nDue to the topographic lattice, the SOM allows us t o de- \nscribe the inherent relations of a melody collectio n in two \nlevels. Similar melodies are classified as variants  of a com- \nmon contour type in the first level, while the rela tions of the \nclasses represented by the contour types themselves  are \nmapped into the topographic lattice in the second o ne.   \nThe overall relations in a data set can be excellen tly repre- \nsented on a SOM, providing that these relations can  be well \napproximated by a two-dimensional structure. Howeve r, \nstretching a more complicated structure into a plai n lattice \nresults in a significant loss of the accuracy of th e classifica- \ntion on one hand, and a non-perspicuous map on the other \nhand. In principle, it is possible to extend the ma p dimen- \nsion, but the resulting exponential increase in the  number of \nlattice points dramatically increases the computing  time and \nthe memory demand.  Therefore, we need some other t ech- \nnique to increase the degree of freedom of the poin ts in the \nmap.  \nTherefore, we elaborated a system combining the SOM  \ntechnique with a special version of the multidimens ional \nscaling (MDS) algorithm [9]. In MDS technique, the input \ndata to be visualised are presented in a quadratic matrix \ncontaining some distance-like or similarity-like va lues be- \ntween some objects. (For instance, the matrix can c ontain \ngeographical distances between towns, or dissimilar ity rat- \nings of melodies, etc.) The aim of the algorithm is  to repre- \nsent the objects (towns or melodies) in a low dimen sional \n299Oral Session 3: Symbolic Music, OMR\n  \n \nspace (often in a plane) with the requirement that the dis- \ntances of the low dimensional points must optimally  corre- \nspond to the input values.  \nIn the present work, firstly we describe a method c on- \nstructed by two independent stages corresponding to  the \nabove-mentioned two-level characterisation of melod y cor- \npora. The first stage is a simplified, non-cooperat ive â€“ and \ntherefore non-topographic - version of SOM learning . In the \nsecond stage, the topographic low-dimensional mappi ng of \nthe resulting contour type vectors is accomplished by a \nvariant of the MDS algorithm. This allows us to pro ject the \nspatial regularities of the multidimensional input vector sys- \ntem to a continuous low-dimensional space without t he re- \nstrictions of the planar grid structure of the SOM.  In order \nto express the contact to the original SOM principl e and to \nemphasize the increased degree of freedom of the lo w di- \nmensional mapping, we call this technique â€œself org anising \ncloudâ€ (SOC).  \nAs a generalisation of the original SOM principle, we also \npresent the cooperative version of the above learni ng sys- \ntem, where the topographic arrangement is improved by a \nfeedback between the multidimensional learning and the \nlow dimensional mapping functions.  \nWe describe the results of a cross-cultural study o f 31 rep- \nresentative Eurasian and North-American folksong co llec- \ntions, based on the modelling by â€œself organising c loudâ€ \ntechnique. The studied cultures are as follows: Chi nese, \nMongolian, Kyrgyz, Mari-Chuvash-Tatar-Votiac (Volga  \nRegion), Sicilian, Bulgarian, Azeri, Anatolian, Kar achay, \nHungarian, Slovak, Moravian, Romanian, Cassubian \n(North-Poland), Warmian (East-Poland), Great-Polish  \n(Southern-Central Poland), Finnish, Norwegian, Germ an, \nLuxembourgish, French, Dutch, Irish-Scottish-Englis h \n(mainly Appalachian), Spanish, Dakota, Komi, Chanty , \nSerbian-Croatian (Balkan), Kurd, Russian (Pskov). O ur da- \ntabase contains digital notations of nearly 32000 f olk songs \narising from different written sources. All of thes e sources \napply the Western notation, thus, the microtonal ph enomena \nof the different cultures were eliminated by the au thors \nthemselves. The time duration and musical structure  of the \nmelodies is very variable, therefore we normalized the \nlength of the melody contours as follows.    \n \n2. THE MELODY CONTOUR VECTORS \n \nThe generation of vectors from melodies is summaris ed in \nFigure 1, showing the first section of a Hungarian folksong \nas an example. The continuous pitch-time function d erived \nfrom the score is represented by the thick line in Figure 1.  \n \nThere, the pitch is characterised by integer number s, in- \ncreasing 1 step by one semitone, with the zero leve l of the \npitch corresponding to the C tone. (In order to ass ure uni- \nform conditions, each melody was transposed to the final \ntone G.)  \n \nFigure 1.  The generation of the melody contour vectors x. \n \nOne can see in the figure that the duration of the temporal \nintervals of the pitch-time function is determined by the \nrhythmic value of the corresponding note. Thus, the  main \nrhythmic information is also encoded. For sampling,  the to- \ntal length of the pitch-time function was divided i nto D \nportions. Then, the â€œmelody vectorâ€    \n[ ]T\nk D kk k xxxx, , 2, 1,K = was constructed from the se- \nquence of the pitch-time samples of the kth melody (See \nFigure 1.).  \nSince D was uniform for the whole set, melodies could be \ncompared to each other using a distance function de fined in \nthe D-dimensional melody space, independently of their \nindividual length. Due to this normalisation, melod y con- \ntours can be compared independently of their measur e, \ntempo and syllabic structure. We studied the melody  vec- \ntors of the entire songs in the analysis, and we ha ve found \nthat a choice of 64 =D resulted in an appropriate accu- \nracy for each melody.  \n \n3. DETERMINATION OF THE CONTOUR TYPE \nVECTORS \n \nIn the first phase of the process, we determined N D=64 \ndimensional â€œcontour typeâ€ vectors ic, characterising the \nmost important melody forms in a database containin g M \nmelodies. In a training step, the distances between  a ran- \ndomly selected melody contour kx and the contour type \nvectors are determined, and the contour type of min imal \ndistance ic is considered as the â€œwinnerâ€. The winner con- \ntour type is moved closer to the melody contour. \n \nIn the initial state, the vectors ic were filled by randomly \nselected melodies of the database. The size of the contour \ntype sets varied between 400 and 576. The algorithm  con- \nsists of the following steps. \n \n1. A melody of the database was selected randomly a nd its \nmelody vector kx was compared to the contour type vec- \ntors ic using the Euclidean distance metric. \n \n30012th International Society for Music Information Retrieval Conference (ISMIR 2011)\n  \n \n2. The contour type vector of the minimal distance ic was \ndetermined as the â€œwinnerâ€ and it was modified usin g  \n \n                    )('\nik ii cxcc âˆ’+=Î»  ,                  (1) \n \nwhere Î»is a scalar factor controlling the rate of conver- \ngence and the accuracy.  \nThe above technique can be considered as a K-means algo- \nrithm [8], or equivalently, as a SOM with a learnin g radius \nof zero. This fact results in a remarkable simplifi cation of \nthe SOM algorithm and a significant improvement of the \nclassification as we will illustrate it below. Howe ver, these \nadvantages imply the disadvantage that the topograp hic ar- \nrangement of the contour types â€“ being a natural co nse- \nquence of the original SOM process - requires furth er com- \nputation. The algorithm producing a more comprehens ive \nand adequate spatial arrangement of the contour typ e vec- \ntors is a version of the multidimensional scaling t echnique, \nand is described below.     \n \n4. LOW DIMENSIONAL MAPPING OF THE \nCONTOUR TYPE VECTORS  \nThe basic idea of the multidimensional scaling algo rithm \ncan be formulated for our problem as follows: We ha ve a \nset of N pieces of  D=64  dimensional contour type vectors \nic, and we can calculate the N*N  dimensional quadratic, \nsymmetric matrix Q containing the squared Euclidean dis- \ntances j iq,of them. (The advantage of squaring will be ex- \nplained below.) We want to represent the N contour types \nby N vectors iv of a low dimensional point system, so that \nthe distances j id,between these points converge to the best \nlow-dimensional approximations of the  \n                      âˆ‘\n=âˆ’ =D\nkk j k i j i cc q\n12\n, , , ) (                           (2) \nvalues in the sense of \n           min ) (\n1 12\n, ,, =âˆ’ =âˆ‘âˆ‘\n= =N\niN\njj i j ij iqdw S ,             (3)                                                                                   \n \nwhere S  is the stress function to be minimised, and \ni j j iww, ,= are weights expressing the importance of the \ndistance of the corresponding points in the stress function. \n(For instance, the exact distance of very dissimila r vectors may not be important in certain cases. Thus, the we ight val- \nues can be defined as functions of the input distan ces j iq,. ) \nThe minimum of the stress function is searched by a  gradi- \nent algorithm. For sake of simplicity, we consider the case \nwhen the low dimensional space is a plane, but the results \ncan be easily generalised to higher dimensions. At the be- \nginning, the N points are randomly located in the plane with \nthe coordinates ()2 ,1 ,,mmvv , where m denotes the serial \nnumber of the points. The gradient components of th e stress \nfunction in the 2 N dimensional space of the point co-\nordinates are the partial derivatives  \n \n        ( )\nk mj iN\niN\njj i j i j i\nk m vdqdwvS\n,,\n1 1, ,,\n,2âˆ‚âˆ‚âˆ’ =âˆ‚âˆ‚âˆ‘ âˆ‘\n= = ,      \n         2 , 1=k  Nm... 1=.                                            (4 )                                             \n \nLet the â€œdistanceâ€ of the ith and jth points in the plane be \ndefined as \n \n( )22\n1, , ,21âˆ‘\n=âˆ’ =\nkk j k i j i vv d                 \n                    2 , 1=k Ni... 1=, Nj... 1= .               (5)                                \n \nThis definition yields a very simple expression for  \nk mj i\nvd\n,,\nâˆ‚âˆ‚\n, \nand the gradient components of the stress function in Equa- \ntion (4) become finally: \n \n( )( )m ii m m i i mN\nik i k mm i\nk mqqddvvwvS\n,, , ,\n1, ,,\n,2 âˆ’âˆ’+ âˆ’ =âˆ‚âˆ‚âˆ‘\n=\n                    2 , 1=k Nm... 1=                                     (6)                          \n \nAccording to the gradient search principle, the new  esti- \nmates of the optimal point co-ordinates are determi ned as \n \n                      \nk mk m k mvSvv\n,,'\n,âˆ‚âˆ‚âˆ’=Âµ ,                      (7 )                                                                                                \nwhere the small scalar value Âµ determines the rate and the \naccuracy of the convergence. \nIn the subsequent steps of the algorithm, the gradi ent com- \nponents of the stress function are re-calculated in  the new \n301Oral Session 3: Symbolic Music, OMR\n  \n \npoint locations using Equations (5) and (6), and th e points \nare replaced using Equation (7) again. The algorith m can be \neasily generalised to 3 or more dimensional point s ystems.   \nComparing the above algorithm to the self organisin g map \n(SOM), an important difference lies in the fact tha t the low \ndimensional vectors iv are not fixed to lattice points, so \nthey are allowed to roam in the low dimensional spa ce, in \nsearch of their own optimal position. In order to e xpress this \nfree roaming of the point system during learning, a nd to dis- \ntinguish between the original SOM and the above des cribed \nalgorithm, we call it â€œself organising cloudâ€ (SOC) . \nThis non-cooperative form of the SOC algorithm acco m- \nplishes a two-level systematisation of melody colle ctions. In \nthe first step, the contour type vectors ic are determined, \nrepresenting the centres of local clusters of the m elody con- \ntour vectors in the D=64 dimensional melody space. Thus, \nthe first level of the systematisation is assigning  the melo- \ndies to the most similar contour type vectors. Havi ng ac- \ncomplished this classification process, the connect ions of \nthe melodies can be described, the higher-level con nections \nof the resulting melody classes, however, remain un re- \nvealed. These latter relations are described by map ping the \nD=64 dimensional contour type vectors to a low dime n- \nsional space. Thus, the second level of the systema tisation \nis the low dimensional representation and visualisa tion of \nthe relations between the melody classes having bee n de- \ntermined in the first level.  \n \n5.  COOPERATIVE LEARNING \n \nUp to this point, we have emphasized the advantages  of the \nindependence of the non-topographic learning- and t he to- \npographic visualising parts of the SOC technique. H owever, \nthe system can easily be modified to learn the cont our types \nin a cooperative way. In this case, all of the cont our type \nvectors located in the surroundings of the winner a re modi- \nfied by the current training vector, and their new low di- \nmensional coordinates are re-calculated simultaneou sly with \nthe contour type learning steps, using Equations (5 ), (6) and \n(7). Since the vectors iv can freely move in the low dimen- \nsional space during the process, this cooperative l earning \napproaches similar vectors to each other, resulting  in a more \narticulated system of the low dimensional clusters.  How- \never, an uncontrolled cooperative process can lead to an ac- \ncelerated approach of neighbouring vectors, resulti ng in a \ntotal collapse of the whole system into one point. This prin- \ncipal problem can be solved by the prohibition of t he coop- \nerative training within a critical radius around th e winner. \nAlthough this version produces a suboptimal contour  type \nestimation - similarly to the SOM algorithm -, it m ay sig- \nnificantly improve the visual representation of the  clusters.  \n \n \n 6. CROSS-CULTURAL ANALYSIS OF 31 MUSICAL \nCULTURES USING THE SOC ALGORITHM \n \nAs an application of the SOC algorithm, we summaris e the \nprocedure and the results of a cross-cultural study  of 31 folk \nmusic cultures in this chapter. The cultures were r epre- \nsented by 31 databases containing 1000 â€“ 2500 melod ies by \nculture. The first step of the analysis was the det ermination \nof the contour type collections of the 31 cultures,  using non-\ncooperative SOC mapping of the databases one by one . In \nthe second phase, we unified the resulting 31 conto ur type \ncollections into one training set, and trained a tw o-\ndimensional â€œcommonâ€ SOC having 1000 contour type v ec- \ntors. After training by the nearly 12000 contour ty pe vectors \narising from the 31 collections (400-500 vectors by  cul- \nture), the resulting 1000 common vectors represent the most \ncharacteristic melody contours appearing in the 31 cultures. \nFigure 2 shows the resulting common musical maps ge ner- \nated by non-cooperative, as well as cooperative tra ining of \nthe SOC. The figure verifies that the cooperative l earning \nyields a much more arranged â€œmusical mapâ€. The musi cal \nmeaning of the main areas of this map is demonstrat ed by \nthe contour type examples in Figure 3. \n \n \nFigure 2. Self organising clouds of the common contour \ntype collection using non-cooperative (a), and coop erative \n(b) learning. \n \nAt this point, we have to define the concept of â€œac tivationâ€ \nof the common contour type vectors as follows: a co ntour \ntype vector of the common SOC is â€œactivatedâ€ by a t raining \nvector when the distance between them is less than a \nthreshold value (see Equation 2). For example, the black \npoints in Figure 2 correspond to the contour types activated \nby the Hungarian melody of Figure 4. The distributi on of \nthe points illustrates that the cooperative learnin g moves \nsimilar contour types into a more compact cluster. Extend- \ning this concept to national/areal sets of training  vectors, we \ncan say that the 31 contour type collections activa te differ- \nent subsets of the 1000 common vectors.  \n \nFigure 3 shows the common SOC with 6 different nati onal \nactivations and some contour type examples being ve ry \ncharacteristic in the given cultures. Since the arr angement \nof the SOC reflects purely musical conditions, it i s not a \ntrivial result that the different cultures are loca ted in more \nor less continuous areas. This fact refers to diffe rent musical \nstyles dominating in different cultures. Some of th ese very \ncharacteristic melody forms are also indicated in F igure 3.  \n30212th International Society for Music Information Retrieval Conference (ISMIR 2011)\n  \n \n \n \n \nFigure 3.  Activated area of the common contour type cloud \nby contour type collections of 6 different cultures . \n \nFor instance, contour example 1 shows that descendi ng \nmelodies with a high range are simultaneously domin ating \nin the Chinese, Hungarian and Turkish activation ar ea. An \nexample for such melodies with Hungarian, Chinese, Anato- \nlian and Dakota parallels is shown in Figure 4.  \n \n       Figure 4.  Melody examples of type 1 in Figure 3. \n \nContour example 2 and 5, representing melodies with  low \nrange demonstrate the musical background of the def inite \noverlap between Anatolian and Bulgarian cultures.  \nThe Hungarian area shows a significant overlap with  the \nChinese and Anatolian ones, but contour example 3 a lso \ndemonstrates a significant common musical style of domed \nmelody forms with the Irish-Scottish-English cultur e.  \nAt the same time, the Irish-Scottish-English corpus  has also \na significant overlap with the German one in the ar ea of as- \ncending forms moving beyond the final tone (see con tour \nexample 4).  The sizes of the overlaps benchmarked against the t otal \nsizes of the activated area refer to the intensity of the rela- \ntions of musical cultures [7]. We considered these relative \noverlap sizes as similarity ratings of musical cult ures, and \nrepresented the resulting system of musical languag e groups \nusing the MDS algorithm described above. The two-\ndimensional MDS plot of the connections is shown in  Fig- \nure 5. The edges indicate pairs of cultures with th e largest \noverlaps. We also indicated some sub-graphs where t he \nnodes mutually are in close musical contacts with e ach \nother. The graph shows a very clear structure with seven \nmusically well interpretable clusters. The right br anch of the \nsystem contains the mutually very closely related { Chinese \nâ€“ Volga â€“ Mongolian}, {Hungarian â€“ Slovak} and {Tur kish \nâ€“ Karachay â€“ Sicilian â€“ Dakota} groups. The left br anch is \nconstructed by the {Finnish â€“ Norwegian â€“ ISE} and \n{German â€“ Luxembourgian â€“ French â€“ Holland} cluster s, \nwhereas the {Bulgarian â€“ Balkan - Kurdish â€“ Azeri} and \n{Russian â€“ Komi - Warmian (East-Poland)} groups con - \nstruct clearly separate clusters.  \nThe close contacts of the above discussed seven â€œmu sical \nlanguage groupsâ€ can be traced back to certain musi cal \nstyles being simultaneously present in more culture s. Com- \nparing Figure 5 to Figure 3, one can recognise that  the six \nactivator cultures of the common musical map can be  con- \nsidered as representatives of the above mentioned â€œ musical \nlanguage groupsâ€. Therefore, contour examples 1-5 i n Fig- \nure 3 represent right the most characteristic commo n musi- \ncal forms contacting the musical language groups as  well.  \n \n \nFigure 5.  MDS plot of the connections of 31 folk music \ncultures. Connecting lines indicate the mutually la rgest rela- \ntive overlaps. \n \n \n7. CONCLUSIONS \n \nWe have described a technique which learns the grou p av- \nerages of the local condensations of multidimension al point \nsystems on the one hand and represents the similari ty condi- \n303Oral Session 3: Symbolic Music, OMR\n  \n \ntions of the learned average vectors in a low dimen sional \npoint system on the other hand. Basically, the algo rithm can \noperate in two modes: In the non-cooperative mode o nly \none average vector is modified in one training step  and the \nstate of the other vectors is independent of this m odifica- \ntion. In the cooperative mode the training is exten ded to a \ngroup of average vectors, and a feedback comes into  exis- \ntence between the learning of the multidimensional aver- \nages and the low dimensional arrangement. \nThe non-cooperative learning of the contour type ve ctors \npermits the convergence to the exact centres of the  local \ncondensations of the training vectors, therefore th e SOC \ncorresponds to the K-means algorithm in this case. The co- \noperative learning realises a compromise between th e accu- \nracy of the multidimensional learning and the low d imen- \nsional representation, therefore the system converg es into a \nsub-optimal state in this case. However, the cooper ativeness \ncan be tuned by the learning radius parameters, and  the \nbenefit of a well accomplished cooperative training  may be \na more transparent low dimensional representation o f the \nmultidimensional clusters, whereas the accuracy of the \nlearning also remains acceptable.  \nThe low dimensional topographic representation of t he con- \ntour type vectors is accomplished by a weighted MDS  algo- \nrithm. This increases the degree of freedom of the mapping, \nbecause the locations of the low dimensional points  are not \nbounded to a lattice, and their dimensionality can be opti- \nmised without a significant increase in the computi ng time.  \n  We applied the method to an analysis of the conne ctions \nof 31 Eurasian and North-American folk music cultur es. We \nhave found that the changeover to the continuous lo w di- \nmensional space of the SOC from the plain lattice s tructure \nof the SOM yields a more articulated low dimensiona l data \nrepresentation and a musically well interpretable s ystemati- \nsation of the melody contours. \nUsing the SOC technique, we have determined a conju gate \nmusical map of the most important melody forms in t he \nstudied cultures, and have found that the different  cultures \noccupy well defined continuous areas of this map. T he \ntechnique allowed us to trace back this â€œmusical ge ogra- \nphyâ€ to the dominance of certain well distinguishab le musi- \ncal styles in different cultures. Exactly the close  correlation \nof different cultures with certain areas of the mus ical map \ncalls the attention to the overlaps, referring to s ignificant \ninteractions of the studied cultures. The analysis of these \noverlaps revealed a perspicuous system of cross-cul tural \nconnections, which was represented by an MDS plot o f the \nprobabilities of deterministic interactions. The co mmon \nmusical forms standing in the background of the mos t im- \nportant cultural connections were also identified f rom the \noverlap areas. We hope that these results demonstra te the \ntimeliness of an extensive study of musical languag e groups \nand call the attention to the importance of the ora l musical \ntradition of the humanity.   This work was supported by the Hungarian National \nResearch Found (grant no. K81954). \n8.  REFERENCES \n \n[1] B. BartÃ³k: â€œThe Hungarian Folk Song by B. BartÃ³ k. â€œ \nEd: B. Suchoff, Transl: M.D. Calvocoressi, Animatio ns: Z. \nKodÃ¡ly. State University of  New York, 1981. \n \n[2] Z. KodÃ¡ly: â€œFolk Music of Hungaryâ€. Budapest, \nCorvina.  \n \n[3] P. Kranenburg1, J. Garbers, A. Volk, F. Wiering , L. P. \nGrijp, and R. C. Veltkamp1 (2010): â€œ Collaboration Per- \nspectives for Folk Song Research and Music Informat ion \nRetrieval: The Indispensable Role of Computational Musi- \ncologyâ€. Journal of interdisciplinary music studies  spring \n2010, volume 4, issue 1, art. #10040102, pp. 17-43  \n \n[4] O. Cornelis, M. Lesaffre, D. Moelants: â€œ Access to ethnic \nmusic: Advances and perspectives in content-based m usic \ninformation retrieval â€. SIGNAL PROCESSING   Volume : \n90   Issue: 4   Pages: 1008-1031  Published: APR 2010   \n \n[5] T. Kohonen: â€Self-organising Mapsâ€œ. Berlin:Spri nger-\nVerlag \n \n[6] P.Toiviainen, and T. Eerola: â€œ A computational Model of \nMelodic Similarity Based on Multiple Representation s and \nSelf-Organizing Maps â€. Proceedings of the 7 th  International \nConference on Music Perception and Cognition, Sidne y, \n2002 C. Stevens, D. Burham, G. McPherson, E. Schube rt, J. \nRewick (eds.) Adelaide: Causal Productions. P236-23 9. \n \n[7] Z. JuhÃ¡sz, J. Sipos: â€ A comparative analysis of \nEurasian folksong corpora, using self organising ma ps â€. \nJournal of Interdisciplinary Music Studies. (2009),    \ndoi: 10.4407/jims.2009.11.005 \n \n[8] T. Kanungo, D.M. Mount, N. S. Netanyahu, C. D. \nPiatko, R. Silverman, A. Y. Wu: \" An efficient k-means \nclustering algorithm: Analysis and implementation \". IEEE \nTrans. Pattern Analysis and Machine Intelligence 24 : 881â€“\n892. doi:10.1109/TPAMI.2002.1017616. \n \n[9] I. Borg, P. Groenen: â€ Modern Multidimensional \nScaling: theory and applications (2nd ed.) â€, Springer-\nVerlag New York, 2005. \n304"
    },
    {
        "title": "Probabilistic Modeling of Hierarchical Music Analysis.",
        "author": [
            "Phillip B. Kirlin",
            "David D. Jensen"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1417119",
        "url": "https://doi.org/10.5281/zenodo.1417119",
        "ee": "https://zenodo.org/records/1417119/files/KirlinJ11.pdf",
        "abstract": "Hierarchical music analysis, as exemplified by Schenkerian analysis, describes the structure of a musical composition by a hierarchy among its notes. Each analysis defines a set of prolongations, where musical objects persist in time even though others are present. We present a formal model for representing hierarchical music analysis, probabilistic interpretations of that model, and an efficient algorithm for computing the most probable analysis under these interpretations. We represent Schenkerian analyses as maximal outerplanar graphs (MOPs). We use this representation to encode the largest known data set of computer-processable Schenkerian analyses, and we use these data to identify statistical regularities in the human-generated analyses. We show that a dynamic programming algorithm can be applied to these regularities to identify the maximum likelihood analysis for a given piece of music.",
        "zenodo_id": 1417119,
        "dblp_key": "conf/ismir/KirlinJ11",
        "keywords": [
            "Hierarchical music analysis",
            "Schenkerian analysis",
            "Musical composition structure",
            "Hierarchy among notes",
            "Prolongations",
            "Probabilistic interpretations",
            "Efficient algorithm",
            "Maximal outerplanar graphs (MOPs)",
            "Data set of computer-processable Schenkerian analyses",
            "Statistical regularities in human-generated analyses"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nPROBABILISTIC MODELING OF HIERARCHICAL MUSIC ANALYSIS\nPhillip B. Kirlin andDavid D. Jensen\nDepartment of Computer Science, University of Massachusetts Amherst\n{pkirlin,jensen }@cs.umass.edu\nABSTRACT\nHierarchical music analysis, as exempliï¬ed by Schenkerian\nanalysis, describes the structure of a musical composition\nby a hierarchy among its notes. Each analysis deï¬nes a set\nof prolongations, where musical objects persist in time even\nthough others are present. We present a formal model for\nrepresenting hierarchical music analysis, probabilistic in-\nterpretations of that model, and an efï¬cient algorithm for\ncomputing the most probable analysis under these interpre-\ntations. We represent Schenkerian analyses as maximal out-\nerplanar graphs (MOPs). We use this representation to en-\ncode the largest known data set of computer-processable\nSchenkerian analyses, and we use these data to identify sta-\ntistical regularities in the human-generated analyses. We\nshow that a dynamic programming algorithm can be ap-\nplied to these regularities to identify the maximum likeli-\nhood analysis for a given piece of music.\n1. INTRODUCTION\nSchenkerian analysis [13] is a widely used and well-developed\napproach to music analysis. Analyses interpret composi-\ntions as a hierarchical structure of musical events, allow-\ning a user to view a tonal composition as a collection of\nrecursive musical elaborations of some fundamental struc-\nture. The method of analysis starts from the original com-\nposition and produces a sequence of intermediate analyses\nillustrating successive simpliï¬cations or reductions of the\nmusical structure of the piece, ultimately arriving at an irre-\nducible background structure. Each reduction is a claim that\na group of musical events (such as notes, intervals, or har-\nmonies)Xderives its function within the composition from\nthe presence of another group of events Y, and therefore\nthe overarching musical structure of the collection XâˆªYis\ndetermined predominantly by the events in Y. In Schenke-\nrian terms, we often say the events in Xconstitute a pro-\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proï¬t or commercial advantage and that copies\nbear this notice and the full citation on the ï¬rst page.\nc/circlecopyrt2011 International Society for Music Information Retrieval.longation of the events in Y, in that the events in Yremain\nâ€œin effect without being literally represented at every mo-\nment.â€ [2]\nSchenkerâ€™s ideas may be viewed as a set of tools for con-\nstructing a hierarchical analysis of a composition according\nto the analystâ€™s own musical intuition, or as theory of tonal-\nity such that every tonal composition, and only tonal compo-\nsitions, should be derivable from the â€œrulesâ€ of Schenkerian\nanalysis [1, 15].\nOpinions differ about the underlying goals of Schenke-\nrian analysis. However, one thing is clear: Schenkerâ€™s ideas\nalone do not prescribe an unambiguous and complete algo-\nrithm for analysis. That said, generations of music theo-\nrists have used Schenkerâ€™s ideas to construct analyses. In\nthis paper, we pursue an empirical strategy for discovering\nthe underlying regularities of those analyses and producing\nnew analyses based on those regularities. Speciï¬cally, we\nderive statistical regularities from the largest known corpus\nof machine-readable Schenkerian analyses, and we identify\nan algorithm for deriving the maximum likelihood analysis,\ngiven these regularities. We demonstrate that the algorithm\ncan reproduce the likelihood ranking implied by a proba-\nbility distribution over possible analyses. Together, these\nï¬ndings provide the foundation of an empirical strategy for\nunlocking the basic concepts underlying any method of hi-\nerarchical music analysis.\n2. REPRESENTATIONS AND ALGORITHMS FOR\nSCHENKERIAN ANALYSES\nTree-like data structures are natural representations for hier-\narchies. Combined with the Schenkerian idea of the analysis\nprocedure revealing multiple levels of musical structure in a\ncomposition, many researchers have used different types of\ntrees to represent an analysis and the structural levels within.\nA commonly used tree representation of an analysis uses\nleaf nodes to represent notes or chords of the original com-\nposition, and interior nodes to represent Schenkerian reduc-\ntions of each nodeâ€™s children. This formulation has been\nused by Frankel, Rosenschein and Smoliar [3,4], Rahn [12],\nLerdahl and Jackendoff [8], Marsden [9, 10] and Kirlin [7].\nAlgorithms for analysis that use such representations have\nhad varying levels of success [6, 10, 11, 14].\n393Poster Session 3\nYust argues for using a hierarchy of melodic intervalsâ€”\nthe spaces between the notesâ€”rather than the notes or chords\nthemselves. He contends that such a hierarchy of inter-\nvals better reï¬‚ects Schenkerâ€™s original ideas and reduces\nthe size of the search space of analyses [15]. Mavromatis\nand Brown [11] and Gilbert and Conklin [5] also suggest\nan interval-based hierarchy would alleviate some represen-\ntational problems.\nConsider Figure 1(a), an arpeggiation of a G major triad\nwith passing tones between the notes of the chord. Rep-\nresenting this musical ï¬gure as a hierarchy of notes forces\nus to choose a single parent note for each passing tone, ob-\nscuring the nature of a passing tone as a voice-leading con-\nnection from one note to another. Using a hierarchy among\nintervals between the notes, however, allows us to represent\nthe musical structure as the tree in Figure 1(b). If we then\nreplace the nodes of this tree with edges, we obtain the rep-\nresentation in Figure 1(c), a particular kind of graph called\namaximal outerplanar graph , or MOP, a representation for\nmusical analysis ï¬rst suggested by Yust [15]. MOPs are\nisomorphic to binary trees representing interval hierarchies\nsuch as that in Figure 1(b), though because the MOP does\nnot duplicate notes as the tree does, it is a more compact\nrepresentation of the hierarchy.\n/noteheads.s2/noteheads.s2/accidentals.sharp/clefs.G/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2(a)(b)(c)Dâ€“GDâ€“BBâ€“GDâ€“CCâ€“BBâ€“AAâ€“GDGBCA\nFigure 1 . (a) An arpeggiation of a chord with passing tones.\n(b) A hierarchy among the melodic intervals in the arpeggia-\ntion. (c) The MOP corresponding to the arpeggiation.\nEvery MOP deï¬ned on a given sequence of notes is a\ntriangulation of the polygon formed by the edges between\nconsecutive notes and the edge from the ï¬rst note to the\nlast note. Each triangle in the MOP speciï¬es a prolongation\namong three notes; we will occasionally refer to a triangle\nas containing two parent notes and a single child note , or\na single parent interval and two child intervals . Either in-\nterpretation is musically correct: the left parent note is pro-\nlonged by the child note during the time span between the\nleft and right parent notes, or the melodic interval between\nthe left and right parent notes is prolonged by the motion to\nand away from the child note.\nBecause every prolongation requires two parent notes, in-\ncomplete prolongations, such as incomplete neighbor notes,\npresent a representational challenge in MOPs. Yust arguesthat in these situations, it is appropriate to have the nearest\nstructural note substitute for the missing parent note. To al-\nlow for incomplete prolongations at the beginning or ending\nof a piece, the MOP model places special â€œinitiationâ€ and\nâ€œterminationâ€ events at the beginning and ending of the pas-\nsage being analyzed that may be used as parents for such\nprolongations.\nThe MOP model offers a new look at representation of\nanalyses that more closely parallels Schenkerian analysis in\npractice due to the MOPâ€™s emphasis on preserving voice\nleading connections. Further discussion of MOPs may be\nfound in Yustâ€™s dissertation [15].\n3. A GENERALIZATION OF MOP: OPC\nThe deï¬nition of a MOP stated above can only handle a sin-\ngle monophonic sequence of notes, though the model can\nbe extended to allow for a single structure to represent the\nanalysis of a contrapuntal or polyphonic composition [15].\nHowever, in the interest of simplicity, we have chosen to\nstore such analyses as collections of separate MOPs occur-\nring simultaneously in time. For instance, in a two-voice\ncomposition, there would be one MOP to represent the up-\nper voice, and one MOP to represent the lower voice. Tak-\ning both MOPs together as a collective representation of an\nanalysis gives us an OPC (outerplanar graph collection).\nThe OPC representation also relaxes one restriction on\nthe constituent MOPs, namely that the polygon formed by\nthe edges connecting the notes of the composition must be\ncompletely triangulated. This is allowed because many anal-\nyses done by humans contain prolongations with multiple\nchild notes. Such prolongations must necessarily be repre-\nsented by polygons larger than triangles; in general, a pro-\nlongation with nchildren will be represented in an OPC by\na polygon with n+ 2sides.\nWe devised a text-based ï¬le format that can encode many\nof the annotations found in a Schenkerian analysis, includ-\ning any type of prolongation (such as passing tones, neigh-\nbor tones, and similar diminutions), voice exchanges, ver-\nticalizations of notes, repeated notes merged in an analy-\nsis, and instantiations of the Ursatz (the fundamental back-\nground structure posited by Schenker). The format is easy\nfor the human to input and easy for the computer to parse.\nWe also developed an algorithm to convert an analysis in\nthis encoding into an OPC.\n4. EXPLORATION OF ANALYSES AS MOPS\nWe collected a set of eight excerpts of music along with\nSchenkerian analyses of the excerpts. The excerpts and anal-\nyses were drawn from Forte and Gilbertâ€™s Introduction to\nSchenkerian Analysis [2] and the accompanying instructorâ€™s\nmanual, and were chosen for their similar characteristics:\nthey are all from compositions for a keyboard instrument in\n39412th International Society for Music Information Retrieval Conference (ISMIR 2011)\na major key, do not modulate within the excerpt, and have a\ncomplete instance of the Ursatz , possibly with an interrup-\ntion. The analyses were algorithmically translated to OPCs.\nThe data set contained 66 measures of music and 617 notes.\nOverall, 270 prolongations were translated into 356 poly-\ngons in the OPCs. Though small, this corpus represents\nthe largest known data set of machine-readable Schenkerian\nanalyses.1\nBecause we are interested in prolongational patterns and\neach triangle in a MOP speciï¬es the prolongation of an in-\nterval by two other intervals, we examined how often certain\ntypes of triangles occurred in the human-produced analyses\nrepresented as OPCs. We deï¬ned a triangle by an ordered\ntriple of the size of the parent interval and the sizes of the\ntwo child intervals. Intervals were denoted by size only, not\nquality or direction (e.g., an ascending major third was con-\nsidered equivalent to a descending minor third), except in\nthe case of unisons, where we distinguished between per-\nfect and non-perfect unisons. Intervening octaves in inter-\nvals were removed (e.g., octaves were reduced to unisons),\nand furthermore, if any interval was larger than a fourth, it\nwas inverted in the triple. These transformations equate pro-\nlongations that are identical under octave displacement.\nBecause OPC analyses permit polygons larger than trian-\ngles, extra care was required to derive appropriate triangle\nfrequencies for these larger polygons. As any polygon can\nonly be triangulated in a ï¬xed number of ways, and each\nof those triangulations contains the same number of trian-\ngles, for every polygon larger than a triangle we counted the\nfrequencies of every possible triangle over all possible tri-\nangulations of the polygon and weighted the resulting fre-\nquencies so that they would sum to the number of triangles\nexpected in a triangulation.\nWe tested the triangle frequencies to see if they were sta-\ntistically signiï¬cant given the null hypothesis that the Forte\nand Gilbert analyses resemble random analyses (where any\ntriangulation of a MOP is as likely as any other) in their\ntriangle frequencies. The expected frequencies under the\nnull hypothesis are not uniformly distributed, even if all the\nnotes in a composition are considered distinguishable from\neach other. Therefore, for each excerpt in our corpus, we\ngenerated 5,000 analyses of the excerpt uniformly at ran-\ndom. Each of these analyses was produced by taking the\ncorresponding human-created analysis as an OPC and re-\ntriangulating each MOP inside. We used these random anal-\nyses to compute the expected frequencies of every type of\ntriangle possible and compared them to the observed fre-\nquencies from the human-produced analyses. We ran indi-\nvidual binomial tests for each type of triangle to determine\nif the observed frequency differed signiï¬cantly from the ex-\npected frequency.\nFive types of triangles had differences between their ob-\n1Analyses are available at http://www.cs.umass.edu/ âˆ¼pkirlin/schenker.served and expected frequencies that were statistically sig-\nniï¬cant at the 5% level; these are shown in Figure 2. A\ncanonical prolongation for each type of triangle is depicted\nat the far left of each row in the ï¬gure, though because inter-\nvals have had intervening octaves removed and are inverted\nif larger than a fourth, each type of triangle represents an\nentire class of prolongations. Triangles that contained a per-\nfect unison as a child interval are not shown in this table,\nas we suspect their frequencies are biased due to the way\nmerged notes are encoded in an analysis. Consecutive notes\nof the same pitch are often implicitly merged in a Schenke-\nrian analysis, and these are encoded as prolongations of the\ninterval from the ï¬rst note with the repeated pitch to the note\nfollowing the last note with the repeated pitch.\nWe can musically interpret each of the ï¬ve types of tri-\nangles shown in Figure 2 and hypothesize the reasons for\nthe differences in frequency. The ï¬rst row in the ï¬gure\n(p= 0.001) tells us that triangles describing an interval of\na third being elaborated by two seconds are more likely to\nappear in a human-produced analysis than in a randomly-\ngenerated analysis. A passing tone ï¬lling in the interval\nof a third would fall into this category. We suspect such\npatterns are numerous due to the theoristâ€™s preference for\nidentifying stepwise voice leading connections in an analy-\nsis. The second row ( p= 0.003) shows us the commonality\nof a melodic second being elaborated by a third and then a\nstep in the opposite direction, for instance, when the inter-\nval Câ€“D is elaborated as Câ€“Eâ€“D. Again, this corresponds to\nthe frequent situation of a stepwise pattern being decorated\nby an intermediate leap. The third row ( p= 0.02) shows\nthe preponderance of melodic ï¬fths (inverted fourths) be-\ning elaborated by consecutive thirds, corresponding to the\narpeggiation of a triad. Harmonies are frequently prolonged\nby arpeggiations of this type.\nThe fourth row in Figure 2 ( p= 0.03) shows that trian-\ngles corresponding to a melodic second elaborated by a step\nand then a leap of a third in the opposite direction occur less\nfrequently than expected. An example would be the interval\nCâ€“D being elaborated by the pattern Câ€“Bâ€“D. Interestingly,\nthis is the reverse case of the second row in the table. We\nhypothesize that analysts tend not to locate this type of pro-\nlongation because the leap of a third could suggest a change\nof harmony, and therefore it is more likely that the ï¬rst note\nof the new harmonyâ€”the B in the exampleâ€”would be the\nmore structural note and not the D as would be implied by\nsuch a prolongation. The last row ( p= 0.05) illustrates\nanother type of prolongation found less often than the ran-\ndom analyses would suggest: a melodic fourth being elabo-\nrated by a step and a leap in the same direction. Musically,\nthis type of prolongation could be located infrequently in an\nanalysis for the same reasons as the prolongation described\nin the fourth row.\nThese statistically signiï¬cant differences show that there\n395Poster Session 3\n/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s23rd2nd4th2nd3rd3rd2nd2nd3rd05030201040Parent intervalChild interval 1Child interval 2/noteheads.s2/noteheads.s2/noteheads.s22nd4th2nd2nd3rd3rd/noteheads.s2/noteheads.s2/noteheads.s2FrequencyExpectedObservedMore frequentthan expectedLess frequentthan expectedFigure 2 . Observed and expected frequencies of triangles in the corpus of OPC analyses.\nare consistencies in the prolongations that analysts locate\nduring Schenkerian analysis. Whether those consistencies\nare due to the analysis method or the analystâ€™s own procliv-\nities is irrelevant, as the consistencies can be exploited to\nproduce an analysis algorithm in either case.\n5. PROBABILISTIC INTERPRETATIONS OF MOPS\nWe now show how to harness the frequencies computed in\nthe previous section to produce an algorithm capable of hi-\nerarchical music analysis. Though we previously deï¬ned a\ntriangle by the intervals between the notes of its vertices,\nin this section we will explore triangles deï¬ned by the notes\nthemselves. Deï¬ning a triangle in this fashion requires more\ndata than we currently have to obtain statistical signiï¬cance,\nbut we believe using this formulation will lead to better per-\nformance in the future.\nWith a set of triangle frequencies deï¬ned by the end-\npoints of the triangles in a MOP, we may deï¬ne a number of\ndifferent probability distributions using these frequencies. If\nwe call the left parent note L, the right parent note R, and\nthe child note C, we deï¬ne the joint triangle distribution as\nP(L,R,C ). This distribution tells us the overall probability\nof seeing a certain type of triangle in any analysis. We also\ndeï¬ne the conditional triangle distribution asP(C|L,R),\nwhich tells us the probability that the interval between the\nleft parent note and the right parent note will be elaborated\nby the child note C.\nUsing either of these two distributions, we can deï¬ne the\nprobability of a Schenkerian analysis in the MOP model.\nGiven that a MOP is completely deï¬ned by its constituent\ntriangles, we deï¬ne the probability of a MOP analysis for\na given sequence of notes as the joint probability of all the\ntriangles that comprise the MOP. If a MOP analysis Afor\na given sequence of notes Ncontains triangles T1,...,T n,\nthen we state P(A|N) =P(T1,...,T n). However, train-\ning such a joint model directly would require orders of mag-\nnitude more data than we suspect could ever be collected.\nInstead, as an approximation, we will assume that the pres-\nence of a certain triangle in an analysis is independent ofthe presence of all the other triangles. Thus, P(A|N) =\nP(T1)Â·Â·Â·P(Tn).\nThe question remains whether to use the joint or condi-\ntional triangle distributions to deï¬ne P(Ti). The joint model\nbetter reï¬‚ects overall frequencies of triangles, but the con-\nditional model easily provides a generative strawman algo-\nrithm for producing an analysis: to analyze a sequence of\nnotesn1,...,n k, ï¬nd arg maxiâˆˆ{n2,...,n kâˆ’1}P(C=i|\nL=n1,R=nk)to ï¬nd an appropriate child note of n1\nandnk, then recursively perform the same operation on the\ntwo resulting child intervals.\nThe issue of triangle independence remains, regardless of\nthe speciï¬c triangle model chosen. An experiment justiï¬es\nour independence assumption. Our goal in the experiment\nis to use a random procedure to generate a multiset of anal-\nyses for a single piece of music, with the frequencies in the\nmultiset reï¬‚ecting the real-world distribution of how ana-\nlysts would interpret a piece. The ranking of the analyses by\nfrequency in the multiset serves as ground-truth. Using this\ncorpus of generated analyses, we compute triangle frequen-\ncies from the corpus as described in Section 4 (though using\ntriangle endpoints instead of intervals between endpoints)\nand obtain a probability estimate for each analysis by using\nthe independence of triangles assumption. We compare the\nground-truth ranking with a new ranking obtained by sorting\nthe analyses by the newly-obtained probability estimates.\nThe exact procedure is as follows. We assumed that every\nnote in the piece was distinguishable from every other note,\nsomething not feasible for earlier experiments but done here\nwith the knowledge that humans may use a noteâ€™s location\nwithin the piece as a feature of the note to guide the analysis\nprocedure. Therefore, each piece was a sequence of inte-\ngersN= 1,2,...,n . We took a uniform sample of 1,000\nMOPs from the space of possible MOPs over N.2We ran-\ndomly chose one MOP to be the â€œbestâ€ analysis, and created\nan arrayAwith the 1,000 MOPs sorted in decreasing order\nof similarity to the best MOP, where similarity was deï¬ned\nas the number of triangles in common between two MOPs.\n2The number of MOPs for a sequence of length nis the (n+ 2) th\nCatalan number [15], which is exponential in n, hence the sampling.\n39612th International Society for Music Information Retrieval Conference (ISMIR 2011)\nThe best MOP was placed at A[0]. We used a variation of\nthe normal distribution to sample one million MOPs from A\nas follows: each sample was the MOP at position iin the\narray, where iwas the absolute value of a normal random\nvariable with Âµ= 0 and varying Ïƒ, rounded down. Values\nofithat corresponded to MOPs outside of array Awere re-\nsampled. The one million sampled MOPs were placed into a\nmultisetMand sorted by decreasing frequency into an array\nR, representing the ground-truth ranking of MOPs.\nWe then computed the frequency of each triangle in mul-\ntisetM, calculated the probabilities for each triangle under\nthe joint and conditional models, and used the independence\nof triangles assumption to compute a probability estimate\nfor each MOP. We generated a new ranking R/primeof the MOPs\nfrom their probability estimates, and computed Spearmanâ€™s\nÏand Kendallâ€™s Ï„ranking correlation coefï¬cients for Rver-\nsusR/primeusing lengths of note sequences between 10 and 50,\nand standard deviations Ïƒfor the normal distribution vary-\ning between 1 and 20. Ïƒdetermines the number of analyses\nrranked inRandR/primeby the formula râ‰ˆ4.66Ïƒ+ 1.65. In\nother words, when Ïƒ= 1, the random procedure only se-\nlects ï¬ve or six analyses from the 1,000 available in A, but\nwhenÏƒ= 20 , approximately 95 are selected.\nFigure 3 shows heatmaps for Ï; darker values are closer\nto 1, indicating R/primebeing closer to R. The heatmaps for\nÏ„are similar. For the joint model, mean values of (Ï,Ï„)\nare(0.9630,0.8848) while for the conditional model they\nare(0.9478,0.8286) , indicating that the joint model slightly\noutperforms the conditional model.\n101520253035404550number of notes1234567891011121314151617181920standard deviationSpearman's rho, joint triangle model\n101520253035404550number of notes1234567891011121314151617181920standard deviationSpearman's rho, cond. triangle model\nÏ âˆˆ [0.7814, 1.0] Ï âˆˆ [0.8175, 1.0] \nFigure 3 . The joint model reproduces the ground-truth rank-\ning slightly better than the conditional model.\nAssuming independence among the triangles in a MOP\nprovides us with an algorithm for calculating the most prob-\nable MOP, regardless of whether we choose the joint or con-\nditional models for the probability of an individual triangle,\nor some other model of triangle probability. Because con-\nstructing a MOP is equivalent to triangulating a simple con-\nvex polygon, we may take advantage of the fact that this\noptimal triangulation problem can be solved in O(n3)time\nusing a Viterbi-like dynamic programming algorithm where\nnis the number of notes in the composition. We will referto this algorithm as O PT-MOP.\n6. EVALUATION\nTo evaluate O PT-MOPand the suitability of the joint and\nconditional triangle models, we performed a leave-one-out\ncross-validation test. We generated 1,000 optimal analyses\nof the MOPs contained in each of the eight excerpts in our\ncorpus by using, for each excerpt, triangle probabilities de-\nrived only from the ground-truth analyses of the other seven\nexcerpts. We needed to compute multiple optimal analyses\nas occasionally ties appeared among the probabilities; O PT-\nMOPbroke these ties randomly. Additionally, we generated\n1,000 analyses uniformly at random for each excerpt.\nTo measure the quality of a candidate analysis A, we cal-\nculated the number of triangles in Athat were compatible\nwith the corresponding ground-truth analysis. We say a tri-\nangle is compatible with the ground-truth if it is present in\nthe ground-truth (the three speciï¬c notes of the excerpt are\ntriangulated the same way in both analyses), or if there is\nnothing in the ground-truth analysis that would prevent such\na triangle from appearing in the ground-truth. The second\nprovision is required because the ground-truth is human-\nproduced and may contain prolongations that do not specify\na complete triangulation. Therefore, any triangle that could\nresult from further triangulation is deemed compatible.\nWe compared the mean percentage of compatible trian-\ngles in the optimal analyses with the corresponding per-\ncentage for the random analyses. Comparisons were done\nseparately for the joint and conditional models. Table 4\nshows the mean compatibility percentages under both mod-\nels, along with a p-value calculated under the null hypoth-\nesis that the O PT-MOPdoes not perform better than ran-\ndom. These data indicate that both models perform better\nthan random as a whole, because if the null hypothesis were\ntrue, we would expect only one of the eight pieces to have\nap-value less than 0.1 for either model. Furthermore, the\njoint model outperforms the conditional model on average.\nThere are a number of possible reasons why the results\nare not better. First, the ground-truth analyses are not com-\npletely triangulated, and this puts an upper bound on how\nwell O PT-MOPcan improve over random analyses. As an\nextreme example, if a MOP were not triangulated at all, then\nall triangles produced by any analysis algorithm would be\ncompatible with the ground-truth, and therefore both O PT-\nMOPâ€™s analyses and the random analyses would both obtain\nscores of 100%.\nSecond, it is not surprising that a training set of only\nseven pieces (due to leaving one out) did not appear to cap-\nture all of the statistical regularities of Schenkerian analysis.\nOur corpus is the largest available and we are actively en-\ngaged in increasing its size. We are gathering analyses from\nmusic journals, textbooks, and Schenkerâ€™s own published\n397Poster Session 3\nModel:Mozart, Piano Sonata in A major, K. 331, IMozart, Piano Sonata in B-ï¬‚at major, K. 333, IIIMozart, Piano Sonata in C major, K. 545, IIIMozart, 6 Variations on an Allegretto, K. Anh. 137Schubert, Impromptu in B-ï¬‚at major, Op. 142, No. 3Schubert, Impromptu in G-ï¬‚at major, Op. 90, No. 3Haydn, Divertimento in B-ï¬‚at major, Hob. II/46, IIHaydn, Piano Sonata in C major, Hob. XVI/35, I030201040%p-value \nMean percentage of triangles in predictedanalyses compatible with ground-truth analysisConditional modelJoint model0.1450.0180.0090.1580.0010.1370.0130.0330.1920.5860.0840.1750.1500.3690.0190.008JointCondExcerptFigure 4 . Leave-one-out cross-validation results for each of the eight excerpts in the corpus.\nworks. Third, we cannot overlook the possibility that O PT-\nMOPcannot produce a good analysis due to the model mak-\ning incorrect assumptions or being too simplistic. Along\nwith gathering more data, we are also working to improve\nour model of the analysis procedure.\n7. CONCLUSIONS\nOur work shows that actual Schenkerian analyses have sta-\ntistical regularities that can be represented, discovered, and\nreproduced. We have shown statistically signiï¬cant regu-\nlarities in a data set of Schenkerian analyses and illustrated\nhow those regularities may be exploited to design an algo-\nrithm for automatic analysis. Our experiment in ranking\nMOPs illustrates that assuming independence among the tri-\nangles comprising a MOP results in a satisfactory approxi-\nmation to the joint probability of all the triangles. The prob-\nabilities of individual triangles in a MOP may be deï¬ned in\nnumerous ways; in the future, we plan on collecting more\ncontextual information surrounding prolongations, such as\nmetrical positioning and harmonic information, and using\nthese features to derive better probabilities over triangles.\n8. REFERENCES\n[1] Matthew Brown, Douglas Dempster, and Dave Head-\nlam. The/sharpIV(/flatV) hypothesis: Testing the limits of\nSchenkerâ€™s theory of tonality. Music Theory Spectrum ,\n19(2):155â€“183, 1997.\n[2] Allen Forte and Steven E. Gilbert. Introduction to\nSchenkerian Analysis . W. W. Norton and Company, New\nYork, 1982.\n[3] R. E. Frankel, S. J. Rosenschein, and S. W. Smo-\nliar. Schenkerâ€™s theory of tonal musicâ€”its explication\nthrough computational processes. International Journal\nof Man-Machine Studies , 10(2):121â€“138, 1978.\n[4] Robert E. Frankel, Stanley J. Rosenschein, and\nStephen W. Smoliar. A LISP-based system for the study\nof Schenkerian analysis. Computers and the Humanities ,\n10(1):21â€“32, 1976.[5]Â´Edouard Gilbert and Darrell Conklin. A probabilistic\ncontext-free grammar for melodic reduction. In Pro-\nceedings of the International Workshop on Artiï¬cial In-\ntelligence and Music, 20th International Joint Confer-\nence on Artiï¬cial Intelligence , pages 83â€“94, Hyderabad,\nIndia, 2007.\n[6] Masatoshi Hamanaka and Satoshi Tojo. Interactive\nGTTM analyzer. In Proceedings of the 10th Interna-\ntional Society for Music Information Retrieval Confer-\nence, pages 291â€“296, 2009.\n[7] Phillip B. Kirlin and Paul E. Utgoff. A framework for\nautomated Schenkerian analysis. In Proceedings of the\nNinth International Conference on Music Information\nRetrieval , pages 363â€“368, 2008.\n[8] Fred Lerdahl and Ray Jackendoff. A Generative Theory\nof Tonal Music . MIT Press, Cambridge, Massachusetts,\n1983.\n[9] Alan Marsden. Automatic derivation of musical struc-\nture: A tool for research on Schenkerian analysis. In\nProceedings of the Eighth International Conference on\nMusic Information Retrieval , pages 55â€“58, 2007.\n[10] Alan Marsden. Schenkerian analysis by computer: A\nproof of concept. Journal of New Music Research ,\n39(3):269â€“289, 2010.\n[11] Panayotis Mavromatis and Matthew Brown. Parsing\ncontext-free grammars for music: A computational\nmodel of Schenkerian analysis. In Proceedings of the 8th\nInternational Conference on Music Perception & Cogni-\ntion, pages 414â€“415, 2004.\n[12] John Rahn. Logic, set theory, music theory. College Mu-\nsic Symposium , 19(1):114â€“127, 1979.\n[13] Heinrich Schenker. Der Freie Satz . Universal Edition,\nVienna, 1935. Published in English as Free Composi-\ntion, translated and edited by E. Oster, Longman, 1979.\n[14] Stephen W. Smoliar. A computer aid for Schenkerian\nanalysis. Computer Music Journal , 2(4):41â€“59, 1980.\n[15] Jason Yust. Formal Models of Prolongation . PhD thesis,\nUniversity of Washington, 2006.\n398"
    },
    {
        "title": "The potential for automatic assessment of trumpet tone quality.",
        "author": [
            "Trevor Knight",
            "Finn Upham",
            "Ichiro Fujinaga"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1418123",
        "url": "https://doi.org/10.5281/zenodo.1418123",
        "ee": "https://zenodo.org/records/1418123/files/KnightUF11.pdf",
        "abstract": "The goal of this study was to examine the possibility of training machine learning algorithms to differentiate between the performance of good notes and bad notes. Four trumpet players recorded a total of 239 notes from which audio features were extracted. The notes were subjectively graded by five brass players. The resulting dataset was used to train support vector machines with different groupings of ratings. Splitting the data set into two classes (â€•goodâ€– and â€•badâ€–) at the median rating, the classifier showed an average success rate of 72% when training and testing using cross-validation. Splitting the data into three roughly-equal classes (â€•good,â€– â€•medium,â€– and â€•badâ€–), the classifier correctly identified the class an average of 54% of the time. Even using seven classes, the classifier identified the correct class 46% of the time, which is better than the result expected from chance or from the strategy of picking the most populous class (36%).",
        "zenodo_id": 1418123,
        "dblp_key": "conf/ismir/KnightUF11",
        "keywords": [
            "machine learning",
            "training",
            "audio features",
            "brass players",
            "support vector machines",
            "cross-validation",
            "groupings of ratings",
            "classifiers",
            "success rate",
            "average success rate"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\n  \n \nTHE POTENTIAL FOR AUTOMATIC  ASSESSMENT  OF TRUMPET \nTONE QUALITY  \nTrevor Knight  Finn Upham  Ichiro Fuj inaga  \nCentre for Interdisciplinary  Research in Music Media and Technology  (CIRMM T), McGill University  \nTrevorKnight \n@gmail.com  Finn.Upham@gmail.com  Ich@music.mcgill.ca  \nABSTRACT  \nThe goal of this study was to examine th e possibility of \ntraining machine learning algorithms to differentiate b e-\ntween the performance of good notes and bad notes . Four \ntrumpet players recorded a total of 239 notes from which \naudio features were extracted. The notes were  subjectively \ngraded by five brass players. The resulting dataset was used \nto train support vector machines with different groupings of \nratings.  Splitting the data set into two classes (â€•goodâ€– and \nâ€•badâ€–) at the median rating, the classifier showed a n ave r-\nage success rate of 72%  when training and testing using  \ncross -validation . Splitting the data into three roughly -equal \nclasses  (â€•good,â€– â€• medium ,â€– and â€• badâ€–), the classifier co r-\nrectly ide ntified the class an average of 54% of the time. \nEven using  seven classes, the classifier identified the co r-\nrect class 46% of the time, which is better than the result \nexpected from chance or from the strategy of picking the \nmost populous class (36%).  \n1. INTRODUCTION  \n1.1 Motiv ation  \nFor some musical parameters, such as pitch  or loudness , \nthere are a well -established link s between signal features of \nthe audio file and perception [1]. Timbre is more compl i-\ncated as several factors contribute to its perception [2]. The \nsubjective quality of a musicianâ€™s performance is more \ncomplicated still, with assumed contributions from  pitch  or \nintonation , loudness, timbre and likely other unknown fa c-\ntors [3].  \nThe goal of this stu dy is to determine the feasibility  for \ncomputer analysis of performance quality . Given  sufficient \ntraining data,  is it possible  for a computer to identify good \nand poor quality notes  so as  to give feedback  to student  \nmusician s or for other pedagogical purposes. ? This study \nalso serves to create a dataset on which the signal comp o-\nnents of tone quality  may be examined . The work was carried out by recording isolated notes \nplayed on trumpet by players with a range of experience , \ncollecting subjective ratings of quality from human su b-\njects, and training a classifier to identify note quality using \nextracted  audio features . Because each of the notes were \nrated and analyzed in isolation, (i.e. as a single note without \naccompaniment or directed comparison ), the note quality  \njudgements  in question are not likely to be affected by  into-\nnation , nor would they be related to other aspects of note \nquality dependent  on musical context . \n1.2 Tone Quality  \nTimbre is frequently defined as the differences between two \nsounds of t he same pitch and loudness . This study was d e-\nsigned to isolate tone quality differences between notes of \nsimilar pitch, dyna mics, and instrument.  While n umerous  \nstudies have attempted to determine the comp onents  of \ntimbre  that differentiate  instruments an d sounds  [5-7], few \nstudies have examined the auditory differences contributing \nto judgments of performance quality of tones.  These  stu-\ndies most often  use a technique called perceptual scaling to \nidentify  principal  dime nsions of timbre  which generally \naligned with  the spectral content, the temp oral change in \nthe spectrum , and the quality of the attack  [6,8] . With \nacoustically produced musical tones, however, t hese factors \nare interdepende nt and affect the perception of one another .  \nThe contribution and inseparability  of the different \ncomponents of the sound is also found in pedagogical lit e-\nrature. I n his instructional book  on the trumpet , Delbert \nDale says, â€•the actual s ound of the attack (the moment the \nsound bursts out of the instrument) has a great deal to do \nwith the sound of the remainder of th e tone â€”at least to the \nlistener â€– [9].   \nThe few studies t hat have examined tone quality looked \nat specific  aspects of the notes. Madsen and Geringer [4] \nexamined preferences for â€•goodâ€– and â€•badâ€– tone quality in \ntrumpet performa nce. Though the  two tone qualities were \naudibly distinguishable when presented without accomp a-\nniment, the only difference their published analysis di s-\ncussed was the amplitude of the second funda mental . In a \ndifferent study, an equalizer was used to amplify or dampen \nthe third through eleventh  harmonics of recorded tones to \nbe rated in  tone quality  [10]. For the brass instruments  \nnotes , a darker  tone, caused by  dampened  harmonics , was  \nPermission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fee provided that copies are \nnot made or distribu ted for profit or commercial advantage and that co p-\nies bear this notice and the full citation on the first page.   \nÂ© 2011  International Society for Music Information Retrieval  \n573Poster Session 4\n  \n \njudged to have a lowe r tone quality than the stan dard or \nbright ened  condition s. \nThe factors other than the amplitudes of the harmonics \naffect  tone quality , and a n examination of these  is wa r-\nranted. For the trum pet, tone quality is a product of the \nâ€•balance and coordinationâ€– of embouchure, the oral cavity, \nand the airst ream  [11]. While â€•no two persons have the \nsame or even similar tonal idealsâ€– [9] and the standard for \ngood and bad ton e quality varies ,  common problems  such \nas â€•a shrill piercing quality in the upper register, and a \nfuzzy and unclear tone in the lower regi sterâ€– [9] have been \nidentif ied. \nThe goal of this study is to therefore see if it is possible  \nto train a classifier tha t can use extracted audio features to \nmake judgements about note quality consistent with aver-\nage human judgement s despite  such variable and subjective \ncriteria.  The instructions given to our human participants \n(described later) are therefore inte ntionally vague to avoid  \nbiasing or limiting judgements  and to avoid prescribing a \ndefinition of tone quality.   \n2. METHODS  \n2.1 Recordings  \nRecordings  of the t rumpet tones  took place in a room d e-\nsigned for performance recording . The position s of the m i-\ncrophones, music stand, and player  were the same for all \nrecordings. Recordings were done using a cartioid micro-\nphone (DPA  4011 -TL, Alleroed , Denmark) and a two \nchannel recorder ( Sound Devices 744T , Reedsburg, Wi s-\nconsin)  at a bit depth of 24 and a sample rate of 48 kHz.  \nThe players had a range of experience and education on the \ntrumpet . Player 1 is  a musician whose primary instrument is \nthe trombone and only pl ayed trumpet for this study . Player \n2 is a trumpet player with twelve years of private lessons \nand regular ensemble performances  at the university level \nboth of which , however,  ceased two years ago . Player 3 is \ncurrently an undergrad uate music perfo rmance  major who \nplays regularly with the university o rchestra . Player 4  has \nbeen playing for 14 years with no i nstruction at the unive r-\nsity level but with frequent live jazz performances .  \nThe recorded phrases were  three lines consisting of four \nhalf notes (minims) separat ed by half rests (minim rests) . \nThe same valve combination was repeated in the low range \n(A, Bb, B, C), mid range (E, F, F#, G), and high range (E, \nF, F#, G)  and the players were instructed on which valves \nto use when a choice existed . Before recor ding eac h line, \nthe players were given four clicks of a metr onome at 60 \nbpm.  The three lines were  played  with instructed dynamic \nlevels of piano,  then repeated at  mezzo -forte  and fortissimo.   \nWith the exception of the trombon e player , the mus i-\ncians all recorded on their own trumpet and mouthpi ece as \nwell as a control trumpet (Conn Director, Conn -Selmer , \nElkhart, Indiana) and mouthpiece  (Bach 7C, Conn -Selmer) . \nThat is to say, three players recorded twelve notes at three different dynamic levels on two trumpets for a contribution \nof 214 notes. The trombone player , player 1, could not play \nthe highest four notes and therefore  contributed just eight \nnotes at three dynamic levels on one trumpet for a total of \n24 notes. One note from the dataset was excluded due to \ncomputer error so t he total dataset had 239 notes .  \n2.2 Labeling  \nIndividual notes were manually  excised from the recordings  \nto make discrete stimuli for subjective rating . Five brass \nplayers (three  trum pet players, one  trombone  player , and \none French horn player , all undergraduate or graduate m u-\nsic students with extensive performance experience ) pro-\nvided subjective labeling of the quality of the notes  on a \ndiscrete  scale from 1 to 7 with 1 labeled as â€•worstâ€– and 7 \nlabeled  â€•best.â€– The raters were instructed to listen to the \nnote as many times as they wanted  and to make a subjective \nrating of the note using anything they could hear and any \ncriteria  they deemed im portant , including their specific \nknowledge of brass  instruments  and the dynamic level . The \nnotes were presented in three blocks  (all the piano notes, all \nthe mezzo -forte notes, all the fortissimo notes) but were \nrandomized within  each block .  \nNote quality judgements varied greatly per rater, as  ex-\npected. While the intersubject ratings correlat ions averaged \nat r=0.5 0, some stimuli were rated more consistently than \nothers. Dividing the 239 notes on the median standard dev i-\nation  of 1.14 (on the discrete range of 1 to 7), the intersu b-\nject co rrelations on the more consistent subset of 118 (less \nthan or equal to  1.14)  averaged to r  = 0.79. In co ntrast, the \nintersubject correlations  on the remaining 121 stimuli ave r-\naged at r  = 0.13, and failed to  correlate  signif icantly  (i.e., \nwith p<0.05 ) in 6 of 10 pair wise comparisons. Most of the  \nbulge in the  distribution of rounded average ratings, shown \nin figure 1 , is due to these notes of amb iguous quality  as \nthey averag e to 4 or 5  with a couple dozen 3 s and 6 s. In the \nfollowing analysis, all notes were represented only by their \naverage rating  across the five raters . The distribution  of av-\neraged ratings of the dataset is shown in Figure 1. \n2.3 Feature Extraction  \nWhile studies have examined appropriate features for ti m-\nbre recognition [12], timbre is just a subset of what pote n-\ntially makes up the quality of a note. The extracted audio \nfeatures we re therefore widely selected, using 56 different \nfeatures, of which  6 were  multidimensional  A complete list \nis given in the appendix. jAudio was used for feature e x-\ntraction .[13] \n \n57412th International Society for Music Information Retrieval Conference (ISMIR 2011)\n  \n \n \nFigure 1.  Histogram of the rounded average ratings  from \nall raters and showing  the contribution  from each player . \n2.4 Learning  \n2.4.1 Classifier Choice  \nACE (Autonomous Classification Engine) 2.0 , software \nused for testing, training, and running classifiers  [14] was \nused throughout the study for these purposes. ACE was \nused to experiment with different classifiers including k -\nnearest neighbour, support vector machines ( SVMs),  sever-\nal types of decision trees,  and neural  networks  on a couple \nsubsets of the data . .SVMs  tended to perform best  on these \nsubsets. . For this reason and because of the relative inte r-\nchangability of these techniques , SVMs  were  used throug h-\nout th is study . In multi -class situations, ho wever, SVMs do \nnot encode an ordering of classes  which makes the task \nslightly more difficult in the three and se ven-class problems \ndiscussed belo w. \n2.4.2 Groupings  \nDifferent groupings of the notes were used to test the acc u-\nracy of the classifiers, including  two, three, and seven \nclasses. While the judgments  from the five raters were only \ninteger values, e ach note was represented by a single ave r-\nage rati ng across all the  raters and was therefore often a d e-\ncimal number.  The notes were a ssigned to classes based on \nthis average rating.  \nTwo-class  problems were evaluated  for three diffe rent \ngroupings . The first grouping  takes just the ex tremes of the \ndata: the â€•goodâ€– class only has average ra tings above 5.5 \nand the â€•badâ€– class has average ratings below 2.5 , exclu d-\ning all points in between . The second grouping is more i n-\nclusive, including all data below 3.5 for â€•badâ€– an d above \n4.5 for â€•good,â€– again e xcluding data in between. The last \ngrouping includes all the data, split at the median rating, \n4.6. The distribution of this labeling is shown in Figure 2.  Secondly, a grouping of three classes was also ev a-\nluated, splitting  the data approximately into three groups, \nbelow 4.2, above or equal to 5.2, and the points in between.  \nLastly,  rounding the averaged ratings into the nearest \ncategory produced seven classes of data with labels 1 to 7. \nThe distribution of this class is the  same as seen in Figure \n1. \n2.4.3 Other tests  \nFurthermore, to test the performance of  the classifier on \nnotes from a n unseen player we used a leave -one-player -out \nmethodology. To do this, we repeated the above tests using \nthree of the players to train and finding  the success of cla s-\nsification on the fourth player. Because of the dominance of \nplayer 1 in ratings less than 2.5, we tested the seven class \ntest with and without player 1 and did not test the two class \nproblem using just the extremes of data (points less  than 2.5 \nand greater than 5.5).  \nA classifier was also trained to test the possibility of \ndiscriminating between performers. To do this, each note \nwas labeled only with a performer number, 1 through 4.  \n \n \nFigure 2: The distribution of the two cla sses when  using all \nof the data, divided at the  medan rating of 4.6.  \n3. RESULT S \nFor the two class problems, the most extreme data resulted \nin the highest success rate and increasing the inclusion of \nthe classes lowered the average success of the five-fold \ncross validation.  These results are summarized in Table 1.  \nFor the three class problem, with a five -fold cross val i-\ndation, an SVM correctly identified the class on average \n54.0% of the tones . This result is shown in Table 2.\n575Poster Session 4\n  \n \n \n\"Bad\"  \"Good\"  Average \nSuccess  Range  Number  Range  Number  \n1â€“2.4 19 5.6â€“7 47 96.9%  \n1â€“3.4 42 4.6â€“7 134 87.5%  \n1â€“4.5 105 4.6â€“7 134 72.0%  \nTable 1:  Classifier results with two classes and  five-fold \ncross validation   \n\"Bad\"  \"Middle\"  \"Good\"  Average \nSuccess  Range  Number  Range  Number  Range  Number  \n1â€“4.1 77 4.2â€“5.1 86 5.2â€“7 76 54.0%  \nTable 2:  Classifier results  with three classes and five-fold \ncross validation  \nThe five -fold cross -validation success of the seven class \nproblem is shown in Table 3  and the confusion matrix is \nshown in Table 4. The rows labels represent the true class i-\nfications of the instances and the columns labels are the \nclassifications assigned by the SVM. For instance, of the \nnotes of class 1, eight were correctly identif ied but one note \nwas labeled 3 and two were labeled 4.  \nClass  1 2 3 4 5 6 7  Avg. Success  \nNumber  11 8 23 63 87 43 4 46.03%  \nTable 3: Classifier results with seven classes and five -fold \ncross validation  \n  1 2 3 4 5 6 7 \n1 8   1 2       \n2 2     4 2     \n3     1 15 6 1   \n4       26 35 2   \n5       22 56 9   \n6       3 21 19   \n7         1 3   \nTable 4: The conf usion matrix for the seven -class pro blem; \nthe correct class es are given in the  row label s. \nWhen using the leave -one-player -out test, the success \nrate decreased. A summary is shown in Table 5.  \nFor the performer identification task, with five folds, the \nclassifier averaged 88.3% success. The confusion matrix is \nshown in Table 6. Again the correct label is the row label. For example, player one played 24 notes, of which 21 were \nidentified correctly, two were incorrectly labeled as player \n2 and one labeled as player 3.  \nPlayer tested    \n1 2 3 4 Avg.   \n23%  66%  84%  67%  60%  2 classes (1 â€“3.5, 4.6â€“7) \n67%  60%  47%  51%  56%  2 classes (split at 4.6)  \n58%  35%  39%  38%  42%  3 classes  \n0% 25%  24% 38%  22%  7 classes  \n 26%  25%  39%  30%  7 classes (w/o player 1)  \nTable 5: Results for leave -player-out classification.  \n \n  1 2 3 4 \n1 21 2 1   \n2 1 61   10 \n3   1 68 3 \n4 3 5 2 61 \nTable 6: The player identification confusion matrix; the \ncorrect player ident ifications are  given by the row -label s. \n4. DISCUSSION  \nThe classifiers show a surprising ability to discriminate b e-\ntween classes based on the extracted features with two, \nthree, and seven classes. Even with seven classes, the cla s-\nsifier identified the correct  class 46% of the time, which is \nbetter than chance or the success rate expected from pic k-\ning the most common class (36%). This shows promise for \nthe possibility to train a classifier to give automatic fee d-\nback on student musiciansâ€™ performance.  \nThere are , however,  severe limitations to this data se t. \nBecause there are only four players in the data set, each \nwith a distinct distribution of notes, there may be latent fe a-\ntures unrelated to performance quality that can help narrow  \nthe selection of class  and im prove classifier success . This \nhypothesis is bolstered by the high success in performer  \nidentification task . For comparison, a 1 -note attempt at \nidentifying the correct performer out of three possible pe r-\nformers gave at best a 43% success in a previous stu dy \n[15].  \nThe classifierâ€™s success with the subset of 118 notes \nwith rating standard deviation less than or equal to 1.14 was \nnot different than the dataset as a whole. This seems to i n-\ndicate th e classifier is not using the same cues or salient \n57612th International Society for Music Information Retrieval Conference (ISMIR 2011)\n  \n \nfeatures that allowed or encouraged agreement between the \nraters.  \nThe results for the leave -one-player -out task decreased \nsharply compared to the result using all players and testing \nwith cross -validation.  This could be because of the distinct \ndistribution of each player and/or other distinct features that \nidentify one performer compared to another.  \nIn the seven class identification task, m athematically, \nfor a note to be considered of class one (or 7 ) there had to \nbe strong agreement among the raters, as at least 3 of the \nraters had to rate that note as class one. This distinctively \nbad performance of class 1 notes probably led to the rel a-\ntively high success in identifying them (8 out of 11 correct) \ncompared to, for example,  class 2 which had no correct \nidentifications.  As well, because player 1 was not able to \nrecord the top four notes of the exercise, having a higher \npitch note skews the rating towards the upper end of ra t-\nings.  \nFurther work is needed  to examine the robustness of \nthese results with more players and with different recording \ncondit ions, such as notes of varying duration, or using \nphrases  of several notes . \n5. ACKNOWLEDGEMENTS  \nThis work was made possible by a CIRMMT Student \nAward and the amazing help of the Harold Kilianski, Yves \nMÃ©thot and Julien Boissinot of CIRMMT . Partial funding \nfor the  work was also provided by Fonds QuÃ©bÃ©cois de la \nRecherche sur la SociÃ©tÃ© et la Culture  (FQRSC)  and Social \nSciences and Humanities Research Council of Canada \n(SSHRC) . \n6. APPENDIX: FEATURES E XTRACTED  \nBeat Sum Overall Average  \nBeat Sum Overall Standard Dev iation  \nCompactness Overall Average  \nCompactness Overall Standard Deviation  \nDerivative of Partial Based Spectral Centroid Overall A v-\nerage  \nDerivative of Partial Based Spectral Centroid Overall Sta n-\ndard Deviation  \nDerivative of Root Mean Square Overall Average  \nDerivative of Root Mean Square Overall Standard Devi a-\ntion \nDerivative of Spectral Centroid Overall Average  \nDerivative of Spectral Centroid Overall Standard Deviation  \nDerivative of Spectral Flux Overall Average  \nDerivative of Spectral Flux Overall Standard De viation  \nDerivative of Spectral Rolloff Point Overall Average  \nDerivative of Spectral Rolloff Point Overall Standard Dev i-\nation  Derivative of Strongest Frequency Via Zero Crossings \nOverall Average  \nDerivative of Strongest Frequency Via Zero Crossings \nOverall S tandard Deviation  \nFraction Of Low Energy Windows Overall Average  \nFraction Of Low Energy Windows Overall Standard Devi a-\ntion \nLPC Overall Average  \nLPC Overall Standard Deviation  \nMethod of Moments Overall Average  \nMethod of Moments Overall Standard Deviation  \nMFC C Overall Average  \nMFCC Overall Standard Deviation  \nPartial Based Spectral Centroid Overall Average  \nPartial Based Spectral Centroid Overall Standard Deviation  \nRoot Mean Square Overall Average  \nRoot Mean Square Overall Standard Deviation  \nSpectral Centroid Over all Average  \nSpectral Centroid Overall Standard Deviation  \nSpectral Flux Overall Average  \nSpectral Flux Overall Standard Deviation  \nSpectral Rolloff Point Overall Average  \nSpectral Rolloff Point Overall Standard Deviation  \nSpectral Variability Overall Average  \nSpectral Variability Overall Standard Deviation  \nStandard Deviation of Compactness Overall Average  \nStandard Deviation of Compactness Overall Standard De v-\niation  \nStandard Deviation of Partial Based Spectral Centroid \nOverall Average  \nStandard Deviation of Partial  Based Spectral Centroid \nOverall Standard Deviation  \nStandard Deviation of Root Mean Square Overall Average  \nStandard Deviation of Root Mean Square Overall Standard \nDeviation  \nStandard Deviation of Spectral Centroid Overall Average  \nStandard Deviation of Spect ral Centroid Overall Standard \nDeviation  \nStandard Deviation of Spectral Flux Overall Average  \nStandard Deviation of Spectral Flux Overall Standard De v-\niation  \nStandard Deviation of Strongest Frequency Via Zero Cros s-\nings Overall Average  \nStandard Deviation of St rongest Frequency Via Zero Cros s-\nings Overall Standard Deviation  \nStandard Deviation of Zero Crossings Overall Average  \nStandard Deviation of Zero Crossings Overall Standard \nDeviation  \nStrength Of Strongest Beat Overall Average  \nStrength Of Strongest Beat Overa ll Standard Deviation  \nStrongest Frequency Via Zero Crossings Overall Average  \nStrongest Frequency Via Zero Crossings Overall Standard \nDeviation  \nZero Crossings Overall Average  \nZero Crossings Overall Standard Deviation  \n577Poster Session 4\n  \n \n \n \n7. REFERENCES  \n[1] R. Plomp, Aspects of Tone Sensation: A Psych o-\nphysical Study , New  York , NY : Academic Press, \n1976.  \n[2] S. McAdams, S. Winsberg, S. Donnadieu, G. Soete, \nand J. Krimphoff, â€•Perceptual scaling of synth e-\nsized musical timbres: Common dimensions, spec i-\nficities, an d latent subject classes,â€– Psycholog ical \nResearch , vol. 58, Dec. 1995, p. 177 â€“92. \n[3] J. Geringer a nd C . Madsen, â€•MusiciansÊ¼ ratings of \ngood versus bad vocal and string performances,â€– \nJournal of Research in Music Education , vol. 46, \n1998, p. 522 â€“34. \n[4] C. Madsen and J. Geringer, â€•Preferences for tru m-\npet tone quality versus intonation,â€– Bulletin for the  \nCouncil for Research in Music , vol. 46, 1976, p. \n13â€“22. \n[5] S. McAdams and J. -C. Cunible, â€•Perception of \nTimbral Analogies,â€– Philosophical Transactions: \nBiological Sciences , vol. 336, 1992, p. 383 â€“9. \n[6] C. Krumhansl, â€•Why is musical timbre so hard to \nunderstand?,â€– Structure and Perception of Ele c-\ntroacoustic Sound and Music: Proceedings  of the \nMarcus Walle nberg Symp osium , Lund, Sweden: \n1988, p. 43 â€“53. \n[7] P. Iverson and C. Krumhanslm, â€•Isolating the d y-\nnamic attributes of musical timbre,â€– Journal of \nAcoustical Society of America , vol. 94, 1993, p. \n2595 â€“603. [8] S. Handel, â€•Timbre perception and auditory object \nidentification,â€– in Hearing , B. Moore, ed., San D i-\nego: Academic Press, 1995, p. 425 â€“61. \n[9] D. Dale, Trumpet Technique , London: Oxford Un i-\nversi ty Press, 1975.  \n[10] J. Geringer and M . Worthy, â€•Effects of tone -\nquality changes on intonation and tone -quality ra t-\nings of high school and college instrumentalists ,â€– \nJournal of Research in Music Education , vol. 47, \nJan. 1999, p. 135 â€“49. \n[11] F. Campos, Trumpet Techni que, New York: Oxford \nUnive rsity Press, 2005.  \n[12] X. Zhang and W.R. Zbigniew, â€•Analysis of sound \nfeatures for music timbre recognition,â€– Internatio n-\nal Conference on Multimedia and Ubiquitous Eng i-\nneering (MUEÊ¼07) , IEEE, 2007, p. 3 â€“8. \n[13] C. McKay, I. Fujina ga, and P. Depalle, â€•jAudio: A \nfeature extraction library,â€– Proceedings of the I n-\nternational Conference on Music Information R e-\ntrieval , 2005, p. 600 â€“3. \n[14] J. Thompson, C. Mckay, J.A. Burgoyne,  and I. F u-\njinaga, â€•Additions and improvements to the ACE \n2.0 music classifier,â€– in Proceedings of the Intern a-\ntional Conference on Music Information Retrieval , \n2009.  \n[15] R. Ramirez, E. Maestre, A. Pertusa, E. Gomez, and \nX. Serra, â€•Performance -based interpr eter identific a-\ntion in saxophone audio recordings,â€– IEEE Tran s-\nactions on Circuits and Systems for Video Techno l-\nogy, vol. 17, Mar. 2007, p. 356 â€“64.  \n \n578"
    },
    {
        "title": "Computational Approaches for the Understanding of Melody in Carnatic Music.",
        "author": [
            "Gopala K. Koduri",
            "Marius Miron",
            "Joan SerrÃ ",
            "Xavier Serra"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1415238",
        "url": "https://doi.org/10.5281/zenodo.1415238",
        "ee": "https://zenodo.org/records/1415238/files/KoduriMSS11.pdf",
        "abstract": "The classical music traditions of the Indian subcontinent, Hindustani and Carnatic, offer an excellent ground on which to test the limitations of current music information research approaches. At the same time, studies based on these music traditions can shed light on how to solve new and complex music modeling problems. Both traditions have very distinct characteristics, specially compared with western ones: they have developed unique instruments, musical forms, performance practices, social uses and context. In this article, we focus on the Carnatic music tradition of south India, especially on its melodic characteristics. We overview the theoretical aspects that are relevant for music information research and discuss the scarce computational approaches developed so far. We put emphasis on the limitations of the current methodologies and we present open issues that have not yet been addressed and that we believe are important to be worked on.",
        "zenodo_id": 1415238,
        "dblp_key": "conf/ismir/KoduriMSS11",
        "keywords": [
            "Indian subcontinent",
            "Hindustani",
            "Carnatic",
            "music information research",
            "instrumentation",
            "musical forms",
            "performance practices",
            "social uses",
            "context",
            "melodic characteristics"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nCOMPUTATIONAL APPROACHES\nFOR THE UNDERSTANDING OF MELODY IN CARNATIC MUSIC\nGopala K. Koduri, Marius Miron, Joan Serr `a and Xavier Serra\nMusic Technology Group\nUniversitat Pompeu Fabra, Barcelona, Spain\ngopala.koduri@gmail.com,miron.marius@gmail.com,joan.serraj@upf.edu,xavier.serra@upf.edu\nABSTRACT\nThe classical music traditions of the Indian subcontinent,\nHindustani and Carnatic, offer an excellent ground on which\nto test the limitations of current music information research\napproaches. At the same time, studies based on these music\ntraditions can shed light on how to solve new and complex\nmusic modeling problems. Both traditions have very dis-\ntinct characteristics, specially compared with western ones:\nthey have developed unique instruments, musical forms, per-\nformance practices, social uses and context. In this article,\nwe focus on the Carnatic music tradition of south India, es-\npecially on its melodic characteristics. We overview the\ntheoretical aspects that are relevant for music information\nresearch and discuss the scarce computational approaches\ndeveloped so far. We put emphasis on the limitations of the\ncurrent methodologies and we present open issues that have\nnot yet been addressed and that we believe are important to\nbe worked on.\n1. INTRODUCTION\nThough all music traditions share common characteristics,\neach one can be recognized by particular features that need\nto be identiï¬ed and preserved. The information technolo-\ngies used for music processing have typically targeted the\nwestern music traditions, and current research is emphasiz-\ning this bias even more. However, to develop technologies\nthat can deal with the richness of our worldâ€™s music, we\nneed to study and exploit the unique aspects of other mu-\nsical cultures. By looking at the problems emerging from\nvarious musical cultures we will not only help those speciï¬c\ncultures, but we will open up our computational methodolo-\ngies, making them much more versatile. In turn, we will\nhelp preserve the diversity of our worldâ€™s culture [26].\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proï¬t or commercial advantage and that copies\nbear this notice and the full citation on the ï¬rst page.\nc/circlecopyrt2011 International Society for Music Information Retrieval.The two classical music traditions of the Indian subconti-\nnent, Hindustani1and Carnatic2, are among the oldest mu-\nsic and most unique traditions still alive. There are excellent\nmusicological and cultural studies about them, they main-\ntain performance practice traditions and they exist within\nreal social contexts. Thus, they are an excellent ground on\nwhich to build new information models and a way to chal-\nlenge the dominant western-centred paradigms. In this arti-\ncle we focus on Carnatic music, the tradition of south-India.\nCarnatic music shares with the Hindustani tradition some\nbasic foundations, such as the basic elements of shruti (the\nrelative musical pitch), swara (the musical sound of a sin-\ngle note), raaga (the melodic mode), and taala (the rhythmic\npattern). Although improvisation plays an important role,\nCarnatic music is mainly sung through compositions, dif-\nferently from Hindustani music where improvisation is fun-\ndamental. Carnatic music is usually performed by a small\nensemble of musicians, consisting of a principal performer\n(usually a vocalist), a melodic accompaniment (usually a\nviolin), a rhythm accompaniment (usually a mridangam),\nand a tambura, which acts as a drone throughout the per-\nformance. Other typical instruments used in Carnatic per-\nformances may include the ghatam, kanjira, morsing, veena\nand ï¬‚ute.\nThe computational study of Carnatic music offers a num-\nber of problems that require new research approaches. Its\ninstruments emphasize sonic characteristics that are quite\nparticular and not well understood yet. The concepts of\nraaga and taala are completely different to the western con-\ncepts used to describe melody and rhythm. Carnatic mu-\nsic scores serve a different purpose to those of western mu-\nsic. The tight musical and sonic relationship between the\nsinging voice, the other melodic instruments and the percus-\nsion accompaniment within a song, requires going beyond\nthe modular approaches commonly used in music informa-\ntion research (MIR). The special and participatory commu-\nnication established between performers and audience in con-\ncerts, offers great opportunities to study issues of social cog-\n1http://en.wikipedia.org/wiki/Hindustani classical music\n2http://en.wikipedia.org/wiki/Carnatic music\n263Poster Session 2\nnition. Its devotional aim is fundamental to understand the\nmusic. The study of the song lyrics is also essential to under-\nstand the rhythmic, melodic and timbre aspects of Carnatic\nmusic. And many more interesting music aspects could be\nidentiï¬ed of relevance to music information processing.\nIn the next section we focus on the melodic aspects of\nCarnatic music, over-viewing the theoretical aspects that are\nrelevant for MIR and discussing the scarce computational\napproaches that have been presented. In the last section we\npresent open issues that have not yet been addressed and that\nwe believe are important to be worked on.\n2. COMPUTATIONAL APPROACHES TO MELODY\nThe most fundamental melodic concept in Indian classical\nmusic is raaga. Matanga is the ï¬rst known person to deï¬ne\nwhat a raaga is [28]: â€œIn the opinion of the wise, that par-\nticularity of notes and melodic movements, or that distinc-\ntion of melodic sound by which one is delighted, is raagaâ€.\nTherefore, the raaga is neither a tune nor a scale [18]. It is\na set of rules which can together be called a melodic frame-\nwork. The notion that a raaga is not just a sequence of notes\nis important in understanding it and for developing compu-\ntational models. Also the concept of raga has been chang-\ning with time. Nowadays a given raaga can be described\nby properties such as: a set of notes (swaras), their progres-\nsions (arohana/avarohana), the way they are intonated using\nvarious movements (gamakaas), and their relative position,\nstrength and duration (types of swaras). In order to identify\nraagas computationally, swara intonation, scale, note pro-\ngressions and characteristic phrases are used (Secs. 2.1 and\n2.2). Unexploited properties of a raaga include gamakaas\nand the various roles the swaras play (Sec. 2.3).\n2.1 Swaras and shrutis\nIn Indian music, swaras are the seven notes in the scale, de-\nnoted by Sa, Ri, Ga, Ma, Pa, Da and Ni3[27]. Except for\nthe tonic and the ï¬fth, all the other swaras have two varia-\ntions each, which account for 12 notes in an octave, called\nswarasthanas. There are three kinds of scales that one gener-\nally encounters in Carnatic and Hindustani music theory: a\n12-note scale, a 16-note scale and the scale which claims 22\nshrutis4. The 16-note scale is the same as the 12-note scale\nexcept that 4 of the 12 notes have two names each, in order\nto be backward compatible with an older nomenclature.\nFew musicians and scholars claim that there are more\nshrutis in practice than those explained above. Though many\nof them argue the total number to be 22, that itself is de-\nbated [9]. A more important question to be asked is whether\nthey are used in current practice at all. Some musicologists\nsay that they are no more used [21]. It is also said that\n3This notation is analogous to e.g. Do, Re, Mi, Fa, So, La and Ti.\n4Shruti is the least perceptible interval as deï¬ned in Natyasastra [22].they are wrongly attributed to Bharata, who used shruti to\nmean â€œthe interval between two notes such that the differ-\nence between them is perceptibleâ€. Krishnaswamy [13] ar-\ngues that the microtonal intervals observed in Carnatic mu-\nsic are the perceptual phenomena caused by the gamakaas,\ni.e. that these microtonal intervals are what few scholars and\nmusicians claim as 22 shrutis. However, we believe that\nthese claims need to be veriï¬ed with perceptual and be-\nhavioural studies. In our encounters with most musicians,\nwe can only conclude that they are unaware of the usage of\n22 shrutis in practice. Few musicians who claim they are\nused, are not ready to demonstrate them in a raaga. In gen-\neral, more empirical, quantitative and large-scale evidence\nneeds to be gathered. Our preliminary research on this line\nshows no support for the usage of 22 shrutis [25].\nThe tuning itself, whether it is just-intonation or equi-\ntempered, is an issue of debate5[12, 25]. Since Indian\nclassical music is an orally transmitted tradition, perception\nplays a vital role. For instance, tuning seldom involves an\nexternal tool. And even the tambura, which is used as a\ndrone, and thus as a reference for tuning, has a very unsta-\nble frequency. Hence the analysis of empirical data coupled\nwith perceptual studies are important. In [25] we have car-\nried out an empirical analysis of the stable tunings employed\nby some Carnatic and Hindustani singers. The results sug-\ngest a clear tendency towards just-intonation in the case of\nCarnatic music while, at the same time, they point out to a\nstrong inï¬‚uence of equi-tempered tuning in the case of Hin-\ndustani music.\nFixed tunings are not the whole story. In fact, it is a well\naccepted notion that a note (swarasthana) is a region rather\nthan a point [7,27]. Thus, a ï¬xed, stable tuning for each note\nis not as important as it is in, say, western classical music.\nIn addition, Sa, the tonic, can be any frequency. It depends\non the comfort of the singer or the choice of the instrument\nplayer. A given note can have several variations in intona-\ntion depending on the raaga. This variability in intonation\narises from vocal articulations or the pulling of instrument\nstrings. Even if two raagas have the same scale, the intona-\ntion of notes vary signiï¬cantly. Belle et al [2] have used this\nclue to differentiate raagas that share the same scale. They\nevaluated their system on 10 audio excerpts accounting for\n2 distinct scale groups (two raagas each). They showed that\nthe use of swara intonation features improved the accuracies\nachieved with pitch-class distributions (c.f. [3]). This clearly\nindicates that intonation differences are signiï¬cant to under-\nstanding and modeling raagas computationally. Levy [16]\nanalyses the intonation in Hindustani raaga performances\nand notes that it is highly variable, and that it does not seem\nto agree with any standard tuning system. Subramanian [33]\nreports much the same for Carnatic music. These studies\ncall for the need to understand the extent to which a given\n5http://cnx.org/content/m12459/1.11\n26412th International Society for Music Information Retrieval Conference (ISMIR 2011)\nRaaga Singer Tested Correctly\nidentiï¬ed\nSankarabharanam Nithyasree 5 4\nSubbulakshmi 3 2\nBalamurali 2 1\nKanakangi Nithyasree 8 6\nIlayaraja 2 1\nKaraharapriya Nithyasree 10 6\nTable 1 . Results of Rajeswari & Geetaâ€™s raaga identiï¬cation\nmethod.\nnote can be intonated. In particular, this could be of interest\nto differentiate artists and styles.\nAll these works indicate that a complete characteriza-\ntion of swarasthanas must go beyond static frequency mea-\nsurements and that their dynamics need to be considered.\nThe problem implies much more than trying to discriminate\nwhether swarasthanas are tuned to just-intonation, equi-tem-\npered or following 22 shrutis. Much empirical data like the\none reported in [33] and [16] needs to be gathered to investi-\ngate the intervals, the range of intonations and the temporal\nevolution of each swarasthana.\n2.2 Arohana and avarohana\nTypically, a raaga is represented using ascending (arohana)\nand descending (avarohana) progressions of notes. There\nare certain note transition rules that are necessary to be fol-\nlowed when performing a raaga. The set of unique notes\nin these progressions form a scale. For raaga identiï¬cation,\nRajeswari et al [31] estimate the scale from the given tune\nby comparing it with template scales. Their test data con-\nsists of 30 tunes in 3 raagas sung by 4 artists. They use\nthe harmonic product spectrum algorithm [15] to extract the\npitch, giving the tonic manually. The other frequencies in\nthe scale are marked down based on the respective ratio with\nthe tonic. The results obtained are shown in Table 1, which\ndepicts a 67% accuracy. The authors claim that such a low\naccuracy could be due to discrepancies in the manually fed\ntonic. But considering that their system identiï¬es only the\nswaras that are used in a raaga and no other relevant data, the\nresult shows that the swaras alone can be very useful. How-\never, there are raagas which have the same swaras (since the\nscales of the raagas they considered are different, this is not\nan issue in their study).\nShetty et al [29] use a similar approach when they try\nto recognize raagas. The features extracted are the individ-\nual swaras and their relation in arohana-avarohana (swara\npairs). The features are represented as bit sequences which\nare later converted to decimal values. These features are\nused for training a neural network. They report an accuracyof 95% over 90 tunes from 50 raagas, using 60 tunes as train-\ning data and the remaining 30 tunes as test data. However,\nsuch a high accuracy is questionable due to the few data per\nclass used. Moreover, no cross-fold validation was done.\nSahasrabudde et al [23] model the raaga as ï¬nite automa-\nta. A ï¬nite automata has a set of states between which the\ntransitions take place. In the case of raaga, the swarasthanas\nare the states and the note transitions are observed. This idea\nis used to generate a number of audio samples for a raaga,\nwhich they claim are technically correct and indistinguish-\nable from human compositions. Inspired by this, Pandey et\nal[17] use HMM models to recognize the raagas. The rules\nto form a melodic sequence for a given raaga are well de-\nï¬ned in the musicology literature [24] and the number of\nnotes is ï¬nite. Therefore, intuitively, HMM models should\nbe good at capturing those rules in note transitions imposed\nby arohana and avarohana patterns (at least the ï¬rst-order,\nsimpler ones).\nEach raaga has also a few characteristic phrases. They\nare called swara sancharas in Carnatic and pakads in Hin-\ndustani. These phrases are said to be very crucial for con-\nveying the feeling of the raaga [9]. Typically, in a concert,\nthe artist starts by singing these phrases. They are the main\nclues for the listeners to identify which raaga it is. Pandey et\nalhave complemented their approach with values obtained\nfrom two modules that match characteristic phrases, taking\nadvantage of this information. In one such module, char-\nacteristic phrases are identiï¬ed with a substring matching\nalgorithm. In the other one, they are identiï¬ed by counting\nthe occurrences of frequency n-grams in the phrase.\nThe other important contributions by Pandey et al in-\nclude two heuristics to improve the transcription of Indian\nclassical music: the hill peak heuristic and the note dura-\ntion heuristic. As mentioned, Indian music has a lot of mi-\ncro tonal variations which makes even the monophonic note\ntranscription a challenging problem [17]. The two heuristics\nproposed in their approach try to get through these micro\ntonal ï¬‚uctuations in attaining a better transcription. The hill\npeak heuristic states that a signiï¬cant change in the slope of\na pitch contour (or the sign reversal of such slope) is closely\nassociated with the presence of a note. The note duration\nheuristic considers only the notes that are played for at least\na certain span of time. The approach was tested on two raa-\ngas. Table 2 shows the results obtained by using HMMs\nalone, and by complementing the models with characteristic\nphrase matching. Not much can be said about the reliability\nof the features they used since the number of classes con-\nsidered were just two. But the advantage of characteristic\nphrase matching is evident.\nSinith et al [30] also used HMMs of raagas to search for\nmusical patterns in a catalogue of monophonic Carnatic mu-\nsic. They build models for 6 typical music patterns corre-\nsponding to 6 raagas (they report a 100% accuracy in iden-\n265Poster Session 2\nRaaga Samples HMM HMM +\nPhrase matching\nYaman Kalyan 15 80% 80%\nBhupali 16 75% 94%\nTotal 31 77% 87%\nTable 2 . Accuracy of raaga identiï¬cation reported in [17].\ntifying an unknown number of tunes into 6 raagas). HMMs\nare also used by Das and Choudary [6] to automatically gen-\nerate Hindustani classical music.\nChordia and Rae [3] use pitch class proï¬les and bi-grams\nof pitches to classify raagas. The dataset used in their sys-\ntem consists of 72 minutes of monophonic instrumental (sa-\nrod) data in 17 raagas played by a single artist. Again, the\nharmonic product spectrum algorithm [15] is used to extract\nthe pitch. Note onsets are detected by observing the sudden\nchanges in the phase and the amplitude of the signal. Then,\nthe pitch-class proï¬les and the bi-grams are calculated. It\nis shown that bi-grams are useful in discriminating the raa-\ngas with the same scale. They use several classiï¬ers com-\nbined with dimensionality reduction techniques. The feature\nvector size is reduced from 144 (bi-grams) + 12 (pitch pro-\nï¬le) to 50 with principal-component analysis. Using just the\npitch class proï¬les, the system achieves an accuracy of 75%.\nUsing only bi-grams of pitches, the accuracy is 82%. Best\naccuracy of 94% is achieved using a maximum a posteriori\nrule with a multi-variate likelihood model. Comparison to\nother classiï¬ers is shown in [3].\n2.3 Unexploited properties of raaga\n2.3.1 Gamakaas\nIn Carnatic music the various forms of pitch movements\nare together called gamakaas. A sliding movement from\none note to another or a vibrato are examples of gamakaas.\nThere are various ways to group these movements, but the\nmost accepted classiï¬cation speaks of 15 types of gamakaas.\nGamakaas are not just decorative items or embellishments,\nbut very essential constituents of a raaga [9]. Each raaga\nhas some characteristic gamakaas. Thus, the detection of\ngamakaas is a crucial step to model and identify raagas.\nA gamakaa is often represented using discrete notes, but\nit does not necessarily mean that one plays them using dis-\ncrete steps. The representation is only a handy expression\nof a more continuous sounding pattern, which is difï¬cult\nto represent on the paper. A gamakaa is almost always a\nsmooth change in the dynamics of a pitch contour. Similar\nconcepts are used to describe the pitch inï¬‚ections in Hin-\ndustani music [19]. Owing to their tremendous inï¬‚uence on\nhow a tune sounds, the gamakaas and the related pitch in-\nï¬‚ections in Hindustani music are often considered the soulof Indian classical music.\nThere are two major issues that make identifying a gama-\nkaa a challenging problem. First, it requires a very precise\npitch transcription. Second, the variations found for differ-\nent artists in performing a gamakaa complicate it further.\nKrishnaswamy [14] and Subramanian [33] report such vari-\nations across different artists performing the same gamakaa.\nThey also propose some theoretical guidelines to resolve the\nsecond problem to some extent. These variations should\nbe exploited in performersâ€™ computational modeling, a ï¬eld\nthat lacks much research in the case of Indian classical mu-\nsic.\n2.3.2 Various roles played by the notes\nIn a given raaga, not all the notes play the same role. Though\ntwo given raagas have the same set of constituent notes, their\nfunctionality can be very different, leading to a different\nfeeling altogether [34]. For example, some swaras occur\nfrequently, some are prolonged, some occur either at the be-\nginning or the end of the phrases, etc. In addition, there are\nalankaras, patterns of note sequences which are supposed to\nbeautify and instil feelings when listened to.\nThough emotion is a subjective issue, it gets into almost\nevery discussion involving raagas. That is because each\nraaga is said to evoke characteristic emotions. To test this\nhypothesis, Chordia and Rae [4] have conducted a survey\nto check whether Hindustani raagas elicit emotions consis-\ntently across listeners. Positive results are reported, jointly\nwith the musical properties like relative weight of the notes,\nwhich partially explain the phenomenon. Koduri et al [11]\nhave conducted a similar survey with Carnatic raagas. Though\nnot as signiï¬cant as the pattern reported by Chordia et al, the\nresults indicate that Carnatic raagas elicit emotions which\nare consistent across listeners. Wieczorkowska et al [35]\ntests if raagas elicit emotions, and also arrive at a mapping\nbetween melodic sequences of 3 or 4 notes and the elicited\nemotions. Their work suggests that different compositions\nin the same raaga might elicit different emotions, what is\nconsistent with the observations made by Koduri et al [11].\nWieczorkowska et al note that these melodic sequences are\nrelated vaguely to the subjectsâ€™ emotional responses. Anoth-\ner interesting observation is the signiï¬cance in the similar-\nity between the responses of people from various cultures,\nwhich is consistent with the observations made in a previous\nstudy conducted by Balkwill et al [1].\n3. OPEN ISSUES: GAMAKAAS, TAALAS,\nINSTRUMENTS AND IMPROVISATION\nLittle research has been carried out on Carnatic music and\neven less on the speciï¬c characteristics that makes it so spe-\ncial. Few proposed computational approaches have focused\non raaga recognition and the results are quite preliminary\n26612th International Society for Music Information Retrieval Conference (ISMIR 2011)\ngiven that the data used is not representative of the exist-\ning variety of raagas. The high accuracies reported might\nbe due to the limited number of raagas used and the small\nsizes of the datasets. Moreover, important properties of the\nraagas, like their speciï¬c use of gamakaas, have not been\nexploited yet, and issues beyond recognition have neither\nbeen approached. We hypothesize that, as more represen-\ntative datasets are gathered, the features used will not be\nsufï¬cient to discriminate the raaga classes. Features such as\npitch-class proï¬les and pitch-class dyad distributions infer\npartial information about the raagas. But the other roles of\nnotes are not evident, which need to be exploited. Symbolic\nscores can also be used for building more complex models,\nespecially to model the characteristic melodic movements\nof particular raagas.\nWhile raaga is the fundamental concept related to melody,\ntaala is the fundamental concept related to rhythm [34]. A\ntaala is a rhythmic cycle, which is divided into speciï¬c un-\neven sections, each of them subdivided into even measures.\nThe ï¬rst beat of each taala section is accented, with notable\nmelodic and percussive events. The characteristics of a taala\nare related to the main instrument used to emphasize the\nrhythmic aspect in a song, the mridangam. Understanding\nthe acoustics of the mridangam and how it is played, is fun-\ndamental to model the taalas. Sambamoorty [24] lists all\ntaalas and provides the description for each. The recog-\nnition of the different types of strokes to play the mridan-\ngam, bols, is an open topic. Current MIR research on drum\ntranscription uses small numbers of drum stroke classes and\neach class is associated with a speciï¬c (single) drum, usu-\nally based on the typical western drum set. With mridan-\ngam, multiple bols are associated to each drum, and given\nthat is a tuned instrument, the recognition of the bols have to\ntake into account both timbre and pitch information. Some\nwork has been done on the recognition of bols in Hindus-\ntani music, with the tabla [8] [5], but no research has been\ncarried out in Carnatic music, with the mridangam. There\nis also no research focusing on the recognition or classiï¬-\ncation of taalas. As the musician always tries to embellish\nthe taala, there is a strong variation from performance to\nperformance, and the rhythmic complexity obtained is enor-\nmous. The main goal would be to gain insensitivity to these\nvariations in order to classify taalas or, otherwise, to model\nthese variations for understanding performance and impro-\nvisation. For this research we need to use top-down or other\ncontextual information to make sense of the audio data, for\nexample there is a well-deï¬ned structure to improvisation\nwhich should be exploited [9].\nWe have reported on previous work that has veriï¬ed whe-\nther raagas elicit emotions and tried to map the musical fea-\ntures which are responsible for such phenomenon. Besides\nthe note sequences, another important aspect of Indian clas-\nsical music which could play a crucial role in eliciting emo-tions is gamakaa. However, there are no studies which re-\nport their effect so far. The kind of instruments used and the\nrhythmic aspects also need to be accounted when dealing\nwith emotional aspects.\nAt the level of musical instruments there is practically\nnothing done. Physical modeling of their many non-linear\nbehaviours is quite complex and the lack of instrument stan-\ndardization does not help. Some research has been done on\nmodeling north-Indian instruments like the tabla and sitar\n[10] and there have been a few attempts in developing sound\nsynthesis systems [32]. The timbre of the tambura is at\nthe basis of the Indian sound. It has a special overtone-\nrich sound, a sustained â€buzzingâ€ resulting from the wide\nand arched bridge on which the strings rests and of the cot-\nton thread placed between the strings and the bridge. This\ntype of string termination results in a quite complex acoustic\nsystem ï¬rst discussed by Nobel Prize winning physicists C\nV Raman [20] and for which current F0-detection methods\nperform very poorly.\nThe performance practice tradition has not been studied\nat all. Music performance is mainly learned by imitation,\nwithout much use of symbolic representations. The vari-\nability in performances of the same song is quite large, es-\npecially due to the importance of improvisation. The same\ncomposition sung by two artists can be different in many\nmusical and expressive facets. These differences may chal-\nlenge the version identiï¬cation methods developed for west-\nern commercial music. In addition to the compositional\nforms, there are many improvisatory forms that are perfor-\nmed with well-deï¬ned structural criteria [9].\nThrough the article we have mentioned a number of char-\nacteristics of Carnatic music that deserve to be studied. Gi-\nven that this music tradition is so different from the ones\nused to develop the current computational methodologies,\nthere is a need to deal with some more fundamental issues\nrelated to music information processing. We need to study\nhow the musical concepts and terms in Indian music are un-\nderstood, specifying proper ontologies with which to frame\nour work. Also the cultural and community aspects of the\nmusic are so important that, without studying them, we will\nnot be able to develop proper musical models. In summary,\nto approach the computational modeling of Carnatic music,\nmaking justice to its richness, it is fundamental to take a\ncultural approach and, thus, take into account musicological\nand contextual information.\n4. ACKNOWLEDGEMENTS\nThe research leading to these results has received funding\nfrom the European Research Council under the European\nUnionâ€™s Seventh Framework Programme (FP7/2007-2013)\n/ ERC grant agreement 267583 (CompMusic).\n267Poster Session 2\n5. REFERENCES\n[1] L.L. Balkwill and W.F. Thompson. A cross-cultural investiga-\ntion of the perception of emotion in music: Psychophysical and\ncultural cues. Music Perception , 17(1):43â€“64, 1999.\n[2] S. Belle, R. Joshi, and P. Rao. Raga identiï¬cation by using\nswara intonation. Journal of ITC Sangeet Research Academy ,\n23, 2009.\n[3] P. Chordia and A. Rae. Raag recognition using pitch-class and\npitch-class dyad distributions. In Proc. of ISMIR , pages 431â€“\n436, 2007.\n[4] P. Chordia and A. Rae. Understanding emotion in raag: An\nempirical study of listener responses. In Proc. of International\nSymposium on Computer Music Modeling and Retrieval , pages\n110â€“124, 2009.\n[5] P. Chordia and A. Rae. Tabla gyan: an artiï¬cial tabla Impro-\nviser. In Proc. of International Conference on Computational\nCreativity , 2010.\n[6] D. Das and M. Choudhury. Finite state models for generation\nof Hindustani classical music. In Proc. of International Sym-\nposium on Frontiers of Research in Speech and Music , 2005.\n[7] A.K. Datta, R. Sengupta, N. Dey, and D Nag. Experimental\nanalysis of shrutis from performances in Hindustani music .\nITC Sangeet Research Academy, 2006.\n[8] O. Gillet and G. Richard. Automatic labelling of tabla signals.\nInProc. of ISMIR , 2003.\n[9] S. R. Janakiraman. Essentials of musicology in south Indian\nmusic . The Indian Music Publishing House, 2008.\n[10] A. Kapur, P. Davidson, P. Cook, W. Schloss, and P. Driessen.\nPreservation and extension of traditional techniques: digitiz-\ning north indian performance. Journal of New Music Research ,\n34(3):227â€“236, 2005.\n[11] G. K. Koduri and B Indurkhya. A Behavioral Study of Emo-\ntions in South Indian Classical Music and its Implications in\nMusic Recommendation Systems. In SAPMIA, ACM Multime-\ndia, pages 55â€“60, 2010.\n[12] A. Krishnaswamy. On the twelve basic intervals in south indian\nclassical music. In Proc. of Audio Engineering Society Conven-\ntion, 2003.\n[13] A. Krishnaswamy. Inï¬‚exions and microtonality in south Indian\nclassical music. In Proc. of International Symposium on Fron-\ntiers of Research in Speech and Music , 2004.\n[14] A. Krishnaswamy. Multi-dimensional musical atoms in South-\nIndian classical music. In Proc. of International Conference on\nMusic Perception and Cognition , 2004.\n[15] K Lee. Automatic chord recognition from audio using en-\nhanced pitch class proï¬le. In Proc. of the International Com-\nputer Music Conference , 2006.\n[16] M Levy and N. A. Jairazbhoy. Intonation in North Indian\nMusic: A Select Comparison of Theories with Contemporary\nPractice . Aditya Prakashan, New Delhi, 1982.[17] G. Pandey, C. Mishra, and P. Ipe. Tansen: A system for auto-\nmatic raga identiï¬cation. In Proc. of Indian International Con-\nference on Artiï¬cial Intelligence , pages 1350â€“1363, 2003.\n[18] H. S. Powers. The Background of the south Indian raaga-\nsystem . PhD thesis, Princeton University, 1959.\n[19] Pratyush. Analysis and classiï¬cation of ornaments in north In-\ndian (Hindustani) classical music. Masterâ€™s thesis, Universitat\nPompeu Fabra, 2010.\n[20] C. V . Raman. On some Indian stringed instruments. In Proc.\nIndian Assoc. Cultiv. Sci. , volume 33, pages 29â€“33, 1921.\n[21] N Ramanathan. Shrutis according to ancient texts. Journal of\nthe Indian Musicological Society , 12(3):31â€“37, 1981.\n[22] A Rangacharya. The Natyasastra . Munshiram Manoharlal\nPublishers, 2010.\n[23] H.V . Sahasrabuddhe and R. Upadhye. On the computational\nmodel of raag music of india. In Workshop on AI and Music:\nEuropean Conference on AI , 1992.\n[24] P. Sambamoorthy. South Indian Music . The Indian Music Pub-\nlishing House, 1998.\n[25] J. Serr `a, G. K. Koduri, M. Miron, and X. Serra. Assessing the\ntuning of sung Indian classical music. In Proc. of ISMIR , 2011.\n[26] X. Serra. A multicultural approach to music information re-\nsearch. In Proc. of ISMIR , 2011.\n[27] V . Shankar. The art and science of Carnatic music . Music\nAcademy Madras, Chennai, 1983.\n[28] P.L Sharma and K Vatsayan. Brihaddeshi of Sri Matanga Muni .\nSouth Asian Books, 1992.\n[29] S. Shetty and K. K. Achary. Raga mining of Indian music by\nextracting arohana-avarohana pattern. International Journal of\nRecent Trends in Engineering , 1(1), 2009.\n[30] M. S. Sinith and K. Rajeev. Hidden markov model based recog-\nnition of musical pattern in south Indian classical music. In\nProc. IEEE International Conference on Signal and Image\nProcessing , 2006.\n[31] R. Sridhar and T.V . Geetha. Raga identiï¬cation of carnatic mu-\nsic for music information retrieval. International Journal of\nRecent trends in Engineering , 1(1):1â€“4, 2009.\n[32] M Subramanian. Synthesizing Carnatic music with a computer.\nJournal of Sangeet Natak Akademi , pages 16â€“24, 1999.\n[33] M Subramanian. Carnatic ragam thodi - pitch analysis of notes\nand gamakams. Journal of the Sangeet Natak Akademi , pages\n3â€“28, 2007.\n[34] T. Viswanathan and M.H. Allen. Music in south India . Oxford\nUniversity Press, 2004.\n[35] A Wieczorkowska, A. Datta, R Sengupta, N Dey, and\nB Mukherjee. On search for emotion in Hindusthani vocal mu-\nsic.Advances in Music Information Retrieval , pages 285â€“304,\n2010.\n268"
    },
    {
        "title": "Knowledge Representation Issues in Musical Instrument Ontology Design.",
        "author": [
            "Sefki Kolozali",
            "Mathieu Barthet",
            "GyÃ¶rgy Fazekas",
            "Mark B. Sandler"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1416142",
        "url": "https://doi.org/10.5281/zenodo.1416142",
        "ee": "https://zenodo.org/records/1416142/files/KolozaliBFS11.pdf",
        "abstract": "This paper presents preliminary work on musical instruments ontology design, and investigates heterogeneity and limitations in existing instrument classification schemes. Numerous research to date aims at representing information about musical instruments. The works we examined are based on the well known Hornbostel and Sachâ€™s classification scheme. We developed representations using the Ontology Web Language (OWL), and compared terminological and conceptual heterogeneity using SPARQL queries. We found evidence to support that traditional designs based on taxonomy trees lead to ill-defined knowledge representation, especially in the context of an ontology for the Semantic Web. In order to overcome this issue, it is desirable to have an instrument ontology that exhibits a semantically rich structure.",
        "zenodo_id": 1416142,
        "dblp_key": "conf/ismir/KolozaliBFS11",
        "keywords": [
            "musical instruments ontology design",
            "heterogeneity",
            "existing instrument classification schemes",
            "Hornbostel and Sachâ€™s classification scheme",
            "Ontology Web Language (OWL)",
            "SPARQL queries",
            "traditional designs",
            "Semantic Web",
            "instrument ontology",
            "semantically rich structure"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nKNOWLEDGE REPRESENTATION ISSUES IN MUSICAL\nINSTRUMENT ONTOLOGY DESIGN\nSefki Kolozali, Mathieu Barthet, Gy Â¨orgy Fazekas, Mark Sandler\nCentre for Digital Music, Queen Mary University of London, London, UK\n{sefki.kolozali, gyorgy.fazekas, mathieu.barthet, mark.sandler }@eecs.qmul.ac.uk\nABSTRACT\nThis paper presents preliminary work on musical in-\nstruments ontology design, and investigates heterogene-\nity and limitations in existing instrument classiï¬cation\nschemes. Numerous research to date aims at represent-\ning information about musical instruments. The works\nwe examined are based on the well known Hornbostel\nand Sachâ€™s classiï¬cation scheme. We developed repre-\nsentations using the Ontology Web Language (OWL),\nand compared terminological and conceptual heterogene-\nity using SPARQL queries. We found evidence to sup-\nport that traditional designs based on taxonomy trees\nlead to ill-deï¬ned knowledge representation, especially\nin the context of an ontology for the Semantic Web.\nIn order to overcome this issue, it is desirable to have\nan instrument ontology that exhibits a semantically rich\nstructure.\n1. INTRODUCTION\nOntologies are used to represent knowledge in a formal\nway. For instance, they can be used to enable machines\nto make sense of the unstructured nature of informa-\ntion available on the Web. Compared to simple meta-\ndata encoding, ontologies provide meaning by deï¬ning\nconcepts and relationships in an application domain, as\nwell as constraints on their use. Furthermore, they per-\nmit interoperability, automatic reasoning and access to\ninformation using complex queries.\nKnowledge representation in the domain of musical\ninstruments is a complex issue, involving a wide range\nof instrument characteristics, for instance, physical as-\npects of instruments such as different types of sound\ninitiation, resonators, as well as the player-instrument\nrelationship. Since the 19th century, numerous studies\ndeveloped systems for representing information about\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies\nare not made or distributed for proï¬t or commercial advantage and\nthat copies bear this notice and the full citation on the ï¬rst page.\nc/circlecopyrt2011 International Society for Music Information Retrieval.musical instruments, for instance, (ethno)musicologists\nhave been working on creating a common vocabulary,\nwhich represents all instruments with relevant charac-\nteristics in a systematic way. The classiï¬cation of in-\nstruments has also been investigated by organologists\nand museologists [8]. Hornobostel and Sachs [14] pro-\nposed a musical instrument classiï¬cation scheme as an\nextension of Mahillonâ€™s scheme [9], originally designed\nto catalogue the worldwide collection of musical instru-\nments housed in the Brussels Conservatory Instrumen-\ntal museum.\nThe Hornobostel and Sachs classiï¬cation scheme (H-\nS system) relies on a downward taxonomy by logical di-\nvision. The method later coined Systematik by Dr Â´ager\n[4]. Although many attempts have since been made by\nscholars to improve the Hornobostel and Sachsâ€™ Sys-\ntematik, it is still predominant in museums around the\nworld. Kartomi [8] attributes the success of the classi-\nï¬cation system to the fact that it is essentially numer-\nical rather than lexical, making it an international sys-\ntem (e.g. 211.11-922 refers to the timpani or kettledrum\nin the H-S system). Elschek [5], was the ï¬rst to pro-\npose an upward method of classiï¬cation based on in-\nstrument attributes complementing downward classiï¬-\ncations schemes such as the Systematik.\nThe purpose of our paper is to investigate knowledge\nrepresentation issues of musical instruments on the Se-\nmantic Web, by taking various musical instrument clas-\nsiï¬cation schemes into account. The rest of the pa-\nper is organised as follows: In section 2, we give an\noverview of the Semantic Web standards used in this\nstudy. In section 3, we describe the Music Ontology\nand the related instrument ontologies. In section 4, we\ndetail knowledge representation issues of various mu-\nsical instrument classiï¬cation schemes, and highlight\ntheir conceptual heterogeneities. In section 5, the OWL\nrepresentations of these classiï¬cation schemes are ex-\namined using SPARQL queries. Finally, in the section\n6, we note on further difï¬culties of the research prob-\nlem, and outline our future work.\n465Poster Session 3\n2. SEMANTIC WEB TECHNOLOGIES\nThe Semantic Web is an initiative of the World Wide\nWeb Consortium (W3C) which proposes standards un-\nderlying the technologies of the Web [10]. The W3C in-\nvestigates how to maintain interoperability and univer-\nsality of the Web using open standards and languages.\nThe technologies relevant in our examination of issues\nin musical instrument ontology design are presented in\nthis section.\nRDF: The Resource Description Framework (RDF)1\nis a simple data model, that associates subjects andob-\njects using a predicate . A series of connections can\nbe made using triples or three-tuple associations, which\nform a graph of semantic relationships. RDF is the basis\nfor more complex knowledge representation languages\nsuch as the RDF Schema Language (RDFS). See for in-\nstance [2] for more details.\nSKOS: The Simple Knowledge Organization Systems\n(SKOS)2is a semi-formal model for expressing con-\ntrolled vocabularies (classiï¬cation schemes, thesauri, tax-\nonomies) in RDF. It deï¬nes skos:Concept , whose\nindividuals may be associated with one or more lex-\nical labels, skos:prefLabel, skos:altLabel\nand placed within a hierarchy using skos:broader ,\nskos:narrower , orskos:related properties, ex-\nhibiting a thesaurus model [1].\nOWL: The Ontology Web Language (OWL)3is a\na W3C recommendation for deï¬ning and instantiating\nweb ontologies. Like RDFS, OWL permits the deï¬-\nnition of classes, properties and their instances, and is\nused to explicitly represent the meaning and relation-\nships of terms in vocabularies, and express constraints\non their use. Such a representation is called ontology.\nOWL has a richer vocabulary than RDFS and SKOS, for\nexample, for specifying cardinality, equality, character-\nistics of properties such as transitivity or symmetry and\nenumerated classes. [1].\nSPARQL: Simple Protocol and RDF Query Language\n(SPARQL)4deï¬nes a standard access protocol for RDF\nthat provides Semantic Web developers with a power-\nful tool to extract information from large data sets. A\nquery consists of several graph patterns, which can be\ncombined recursively to form arbitrarily complex query\npatterns. It may be used for any data source that can be\nmapped to RDF.\n1http://www.w3.org/TR/rdf-primer\n2http://www.w3.org/TR/skos-reference\n3http://www.w3.org/TR/owl-primer\n4http://www.w3.org/TR/rdf-sparql-protocol3. RELATED WORK\nOur primary aim is to develop a semantically rich on-\ntology of instruments which can be used in conjunction\nwith the Music Ontology5. In this section, we outline\nthis ontology and previously published Semantic Web\nontologies of musical instruments.\nThe Music Ontology [13] provides a uniï¬ed frame-\nwork for describing music-related information (i.e. ed-\nitorial data including artists, albums and tracks) on the\nWeb. It is built on several ontologies such as the Time-\nline Ontology6, the Event Ontology7, the Functional\nRequirements for Bibliographic Records (FRBR) On-\ntology8, and the Friend Of A Friend (FOAF) Ontol-\nogy9. It subsumes speciï¬c terms from these ontologies,\nuseful to describe music related data. The Timeline and\nEvent ontologies, can be used to localise events in space\nand time. The FRBR model links books and other in-\ntellectual works with their creators, publishers or sub-\njects, and provides a model to describe the life cycle of\nthese works. This is reused by the Music Ontology to\ndescribe the music production workï¬‚ow from composi-\ntion to delivery. Finally, FOAF deï¬nes people, groups\nand organisations. The Music Ontology does not cover\nevery music related concept, rather, it provides exten-\nsion points where a domain speciï¬c ontology, such as\na musical instrument or a genre ontology may be inte-\ngrated.\nBased on the Musicbrainz10instrument tree, Her-\nman11published a musical instrument taxonomy ex-\npressed in SKOS. This serves as an extension to the Mu-\nsic Ontology. While SKOS is well suited for hierarchi-\ncal classiï¬cation schemes, it provides limited support\nfor other types of relationships; skos:related for\nexample, may be used to describe associative relations,\nbut only in a semi-formal way, without a more explicit\ndeï¬nition. Moreover, the transitivity of broader and nar-\nrower relations are not guaranteed in SKOS, therefore it\nis difï¬cult to infer for instance the instrument family of\na given instrument, without additional knowledge not\nexpressed in the model. While this taxonomy is suit-\nable for applications that require only a semantic label\nto represent instruments associated with audio items, it\nis insufï¬cient if the heterogeneity of instrument rela-\ntions has to be explicitly represented.\nThe Kanzaki Music Ontology12also contains a small\ninstrument taxonomy. However, there are only 5 instru-\n5http://musicontology.com/\n6http://purl.org/NET/c4dm/timeline.owl/\n7http://purl.org/NET/c4dm/event.owl/\n8http://vocab.org/frbr/core/\n9http://xmlns.com/foaf/spec/\n10http://musicbrainz.org/\n11http://purl.org/ontology/mo/mit#\n12http://www.kanzaki.com/ns/music\n46612th International Society for Music Information Retrieval Conference (ISMIR 2011)\nment families deï¬ned (e.g. string instruments, wood-\nwind instruments, brass instruments, percussion, and\nkeyboard instruments), with 26 corresponding instru-\nment classes. Although these works provide instrument\ntaxonomies that can be used on the Semantic Web, there\nremains a need for a semantically rich ontology, which\nrepresents the heterogeneity as well as different compo-\nnents and aspects of musical instruments on the Web.\nFinally, a recently published XML-based taxonomy\nserves as an extension to Music XML13. This system\ndeparts form Hornobostel and Sachs, and proposes a\nclassiï¬cation scheme based on materials and performance\nmechanism, instead of the sound production mechanism.\nHowever, it remains at a hierarchical design. Further-\nmore, XML in itself is insufï¬cient for rich knowledge\nrepresentations, therefore it is hard to see how this model\nmay be extended to account for the heterogeneity and\nthe diverse set of properties of musical instruments, and\nenable logical reasoning or answering complex queries.\n4. ISSUES IN MUSICAL INSTRUMENT\nONTOLOGY DESIGN\nConceptualising a domain is inherent in developing knowl-\nedge based systems. In the ï¬elds of ethno-musicology\nand Music Information Retrieval (MIR), most concep-\ntualisations of the domain of musical instruments are\nbased on the taxonomical H-S system, and very few\nstudies departed from this system. Taxonomies allow\nus to organise data in a hierarchical structure very efï¬-\nciently. However, taxonomies encode a strict relation-\nship between a parent node and a child node by us-\ningsub-class orpart-of axioms, without deï¬ning the\ndetailed relationships among instrument objects, there-\nfore they are semantically weak structures for express-\ning knowledge [3, 6, 7]. Musical instruments however\nhave a multi-relational model, thereby instruments can\nbelong to more than one instrumental family or sub-\nfamily. In order to illustrate the heterogeneity and taxo-\nnomic design problems occurring in current knowledge\nrepresentations of instruments, two different instrument\nclassiï¬cation systems were taken into account: i)one\nproposed by Henry Doktorski14which will be denoted\ntaxonomy â€˜Aâ€™, and ii)one proposed by Jeremy Montagu\n& John Burton [11] which will be denoted taxonomy\nâ€˜Bâ€™. We implemented both of the taxonomies in OWL,\nand they can be found at corresponding URL15. Figure\n1 illustrates an example from the ontology design of the\nchordophones/string instrument family based on Henry\nDoktorskiâ€™s taxonomy.\n13http://www.recordare.com/musicxml/\n14http://free-reed.net/description/taxonomy\n15http://isophonics.net/content/\nmusical-instrument-taxonomies\nFigure 1 . An example from musical instrument ontol-\nogy design of chordophone/string instruments based on\nHenry Doktorskiâ€™s instrument classiï¬cation system\nAs shown in Figure 1, the violin and cello are classi-\nï¬ed as bowed instruments, the guitar and banjo are clas-\nsiï¬ed as plucked instruments, and the piano is classiï¬ed\nas a struck instrument. However, violinist can vary their\nplaying technique depending on the expressive inten-\ntions: the strings can be excited by drawing the hair of\nthe bow across them (arco), or by plucking them (pizzi-\ncato). For these reasons, the violin should be classi-\nï¬ed as either a bowed or plucked instrument. In Fig-\nures 1 and 2, the concepts that occurred multiple times\nin various instrument families, are shown using dashed\nlined shapes ( e.g. struck, plucked and rubbed). We can\ndemonstrate similar examples in the family of percus-\nsion instruments. For instance, in Figure 2, the tam-\nbourine is classiï¬ed as a membranophone, whereas if it\nis only shaken, it jingles, and therefore it could be clas-\nsiï¬ed as an idiophone as well. Many examples may be\nobserver related to taxonomic classiï¬cation problems,\nnot only in the ethno-musicology, but also in other ap-\nplications that rely on musical instrument knowledge\nrepresentation or information management.\nIn taxonomy B, the use of classiï¬cations such as,\nspecies ,genus ,family ,sub-order ,order , based on the\ntaxonomical system of Carl Linnaeus known as the fa-\nther of modern taxonomy. However, this study only pro-\nvides a terminological departure from the H-S system,\nsince it is still based on the same taxonomy structure. A\npartial instrument ontology design of this classiï¬cation\nscheme is depicted in Figure 3.\nThe use of different words to refer to similar con-\ncepts, or different conceptualisations, induce termino-\nlogical or conceptual heterogeneities among ontologies,\nthat can be observed from the given graphical illustra-\ntions so far. For instance, in Figure 3, the idiophones\nand the membranophones are deï¬ned as a major instru-\n467Poster Session 3\nFigure 2 . An example from musical instrument ontol-\nogy design of percussion instruments based on Henry\nDoktorskiâ€™s instrument classiï¬cation system\nment family according to taxonomy B, whereas both of\nthese classes can be seen as sub-classes of the percus-\nsion instruments in taxonomy A (Figure 2).\nThe heterogeneity among these classes continues down-\nward towards to the sub-class nodes: For instance, id-\niophones are divided into unpitched and pitched sub-\ncategories, while membranophones are divided into de-\nterminate pitch and indeterminate pitch sub-categories\n(Figure 2). On the other hand, the idiophones have sub-\nclasses such as struck, shaken, striligilated and plucked\nsub-classes, while membranophones have kettle, single\nhead and double head sub-classes (Figure 3). Some con-\ncepts are present in the same taxonomic level without\ndeï¬ning the relationship among concepts, and the con-\ncepts are classiï¬ed according to sound initiation type\n(e.g. struck, plucked, or shaken), whereas others are\nclassiï¬ed according to the instrument construction type\n(e.g. single head, double head, harps, lyres and lutes).\nTherefore, the taxonomic classiï¬cations applied tradi-\ntionally are not only heterogeneous in structure, but also\nprovide an arbitrarily problematic solution to instrument\nclassiï¬cation, because of the inadequately deï¬ned knowl-\nedge representation.\n5. QUERY DRIVEN EVALUATION\nBoth taxonomies described in the previous section were\nimplemented in OWL and tested using SPARQL queries\ninvolving instruments present in both systems. In the\nfollowing examples, we query the ontology structure, as\nwell as RDF data corresponding to speciï¬c statementsabout instruments. Since in most knowledge-based en-\nvironments, data and ontology can be represented in the\nsame graph, these queries also demonstrate real-world\nuse cases for instrument knowledge representation. The\nï¬rst example is based on the tuba, which is available in\nboth taxonomies. The following paragraph taken from\n[12] provides a description of the tuba:\nThe tuba is the lowest pitched Aerophone . Sound is pro-\nduced by vibrating or buzzing the lips into a large cupped mouth-\npiece, which is coupled to a coiled tube about 18 feet in length\nwith a slow rate of conical ï¬‚are terminating in a large bell-\nshaped mouth. The tuba is usually equipped with three valves ,\neach of which adds a different length of tubing. With piston\nvalves it is possible to change the length of the air column.\nIdentifying an instrument by its sound can be a difï¬-\ncult task, even for someone with a decent musical back-\nground. For this reason, visual cues can be just as im-\nportant as hearing in instrument identiï¬cation. For ex-\nample, recognising the characteristic shape of an instru-\nment is important, since it has a profound effect on the\ngenerated sound. Based on these considerations, we\nprepared the following four queries to retrieve the in-\nformation underlined in the deï¬nition of the tuba above:\nWhat is the instrument family, the characteristic shape,\nthe sound initiation type and the number of valves of the\ntuba?\nPREFIX io: <http://example.org/io/taxonomyN#>\nPREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\nSELECT ?x WHERE { io:Tuba rdfs:subClassOf ?x }\nListing 1. Retrieving the immediate super class of the\ntuba.\nIn the ï¬rst query the non-determined variable ?xis\nassigned when the query engine ï¬nds the super class\nof the entity named Tuba . The query result for taxon-\nomy â€˜Aâ€™isio:WithValves , and for taxonomy â€˜Bâ€™is\nio:ValvesBugles . This demonstrates terminolog-\nical heterogeneity immediately on the ï¬rst upper level.\nNote that name space preï¬xes such as io: andrdfs:\nare expanded to full URIs by the query engine. In the\nfollowing queries, they will be omitted for brevity.\nIn order to retrieve the instrument family, we can ei-\nther expand the query until we reach the correspond-\ning node as shown in listing 2, or use a program to\ndo so appropriately. This assumes knowledge about\nthe depth and organisation of the taxonomy tree, that\nis, what information is described on each level given a\nspeciï¬c branch. Given this information, a reasoning en-\ngine could infer the instrument family relation, so that\n46812th International Society for Music Information Retrieval Conference (ISMIR 2011)\nFigure 3 . An example from musical instrument ontology design based on Jeremy Montagu & John Burtonâ€™s instrument\nclassiï¬cation system\nSELECT ?sc1 ... ?sc(N)\nWHERE { io:Tuba rdfs:subClassOf ?sc1 .\nOPTIONAL { ?sc1 rdfs:subClassOf ?sc2 } .\n.\n.\nOPTIONAL { ?sc(N-1) rdfs:subClassOf ?sc(N) } .\n}\nListing 2. Hypothetical query for ï¬nding the instrument\nfamily of the tuba.\na direct query could be written. However, taxonomy\nbased knowledge organisation systems do not contain\nthis type of information, which is their main drawback\nin answering complex queries.\nIntuitively, this query graph means that there exists\nan entity Tuba that is a subclass of ?sc1 having a rela-\ntion with another entity whose name is non-determined.\nWe may recursively go on until ï¬nding the entity Aero-\nphones , the super-class of the last non-determined class.\nThe query would succeed at the 4th super-class node for\nthe taxonomy â€˜Aâ€™ (e.g. WithValves, BrassInstrument, Pi-\npeAerophones, Aerophones ), whereas the correspond-\ning result would be obtained at the 10th node for the tax-\nonomy â€˜Bâ€™ (e.g. ValvedBugles, SingleBell, Valves, End-\nBlown, Metal, Conical, DoubleLipReed, Reeds, Aero-\nphones ).\nThe main problem with taxonomical representations\nis that itâ€™s difï¬cult to answer certain queries without\na more explicit knowledge representation. Taxonomic\nsystems propagate meaning via the parent child rela-\ntionship. We could infer that the tuba is an ( is-a, or\nrdf:type ) instrument with Valves , aBrass instrument\nand an Aerophone , according to taxonomy â€˜Aâ€™. The in-\nstrument family could be directly encoded using a se-\nmantically rich ontology. Although both taxonomies\nare based on the H-S system, it is easy to observe the\ndiversity among different instrument taxonomies from\nthese query results. The problem is not only the con-\nceptual heterogeneity of the instruments themselves, butalso the terminological heterogeneity among different\nknowledge representation schemes.\n@prefix io: <http://example.org/io/taxonomyN#>\n@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .\n@prefix owl: <http://www.w3.org/2002/07/owl#> .\n@prefix mo: <http://purl.org/ontology/owl/> .\n@prefix ex: <http://example.com/> .\nex:guy_klucevsek\na mo:MusicArtist ;\nfoaf:name \"Guy Klucevsek\" ;\nowl:sameAs <http://dbpedia.org/page/Guy_Klucevsek> ;\nex:guy_klucevseks_accordion\na io:Accordion .\nex:ellltl\na mo:Composition ;\ndc:title \"Eleven Large Lobsters Loose in the Lobby\"Ë†Ë†xsd:string ;\nmo:composer ex:guy_klucevsek ;\nmo:produced_work ex:w_ellltl;\nowl:sameAs\n<http://dbtune.org/musicbrainz/page/track/8093f69e-194f-4cb1-8943-2d11fac6dcc6> .\nex:p_ellltl\na mo:Performance ;\nrdfs:label \"A performance of the composition.\"Ë†Ë†xsd:string ;\nmo:performer ex:guy_klucevsek ;\nmo:performance_of ex:w_ellltl ;\nmo:instrument ex:guy_klucevseks_accordion .\nListing 3. RDF Data based on Music Ontology and Mu-\nsic Instrument Taxonomy (Herny Doktorski).\nThe second query is â€™What is the characteristic shape\nof the tuba?â€™ . To ï¬nd this information, an upward recur-\nsive query, such as the one in Listing 2, or downward\nrecursive query, which starts from the Conical concept,\ncan be used to verify that the tuba is a conical instru-\nment. However, both types of queries rely on external\nknowledge that can not be inferred from the pure taxo-\nnomical relationships directly. While taxonomy â€™Bâ€™at\nleast contains the information about the characteristic\nshape of the tuba, being Conical , taxonomy â€™Aâ€™does not\ncontain this information. In the third and fourth ques-\ntions, we ask â€™What is the sound initiation type of the\n469Poster Session 3\ntuba ?â€™ andâ€™How many valves the tuba has?â€™ . Unfor-\ntunately none of the implemented systems encode these\nrelationships, therefore it is not possible to write queries\nto answer these questions that would produce any re-\nsults.\nIn our second example shown in listing 3, we use\nthe Music Ontology to represent the Composition and\nPerformance events from the sentence below, assuming\nthe composer also performed the piece:\nThe American accordionist and composer Guy Klucevsek\nhas written a piece for solo accordion, â€™Eleven Large Lob-\nsters Loose In The Lobbyâ€™, which does not use the reeds of\nthe accordion. The performer produces sounds by clicking\nthe register switches, tapping the keys, and other percussive\nmeans. In this piece the accordion is used as an idiophone\nand not as a free-reed .\nThis example presents a case for knowledge discov-\nery using instrument taxonomies. As shown in the ex-\nample, lacking a more detailed ontological representa-\ntion, we could not describe the accordion further to take\ninto account the speciï¬c playing style. Since none of the\ntaxonomies may be used to encode information about\npossible alternative sound initiation types, we may only\nobtain the instrumentâ€™s default characteristics given a\ntaxonomy, using recursive queries such as query 2. Given\nthis representation a reasoner can only infer that the\nAccordion is a Hand blown, Free-reed, Aerophone in-\nstrument. However, in this particular example, the in-\nstrument was played using different techniques, such\nas clicking the register switches and tapping the keys,\nwhich implies its use as an idiophone. The inductive\nchallenge is to infer statements about the relations and\nobjects that are true but unobserved. Due to the draw-\nbacks of traditional taxonomies, the reasoner would not\nbe able to discover new knowledge about the particular\nindividual played as an idiophone in this speciï¬c exam-\nple.\n6. CONCLUSION\nIn this study, we investigated some issues arising in the\nrepresentation of knowledge about musical instruments.\nIn order to demonstrate their drawbacks in complex query\nanswering, we implemented two instrument taxonomies\nbased on the well-known H-S system in OWL. We found\nthat many instrument classiï¬cation schemes exhibit in-\nsufï¬cient or ill-deï¬ned semantics for our purposes, thus\na more ï¬‚exible representation is required. We demon-\nstrated using different SPARQL queries that depend-\ning on the terminology and conceptualisation used by\n(ethno)musicologists, we obtain different results for the\nsame instrument object. It also became evident, that\nontologies that deï¬ne relationships between entities are\nbetter than traditional taxonomies at providing mean-ingful answers to queries. Our work however represents\nonly a preliminary analysis of current musical instru-\nment schemes. Future work includes developing a mu-\nsical instrument ontology, and further investigation on\nhow to represent heterogeneous instrument classiï¬ca-\ntions in a Semantic Web environment.\n7. REFERENCES\n[1] Dean Allemang and Jim Hendler. Semantic Web for the\nWorking Ontologists . Morgan Kaufmann, 2008.\n[2] Grigoris Antoniou and Frank can Harmelen. Semantic\nWeb Premier 2nd Edition . Massachusetts Institute of\nTechnology, 2008.\n[3] Michael C. Daconta, Leo J. Obrst, and Kevin T. Smith.\nThe Semantic Web: A Guide to the Future of XML Web\nServices, and Knowledge Management . Wiley Publishing,\n2003.\n[4] Hans Heinz Dr Â´ager. Prinzip einer systematik der musikin-\nstrumente. Kassel und Basel:Barenreiter. , 1948.\n[5] Osk Â´ar Elschek. System of graphical and symbolic signs\nfor the typology of aerophones. Bratislava:VydatelstvS\nlovenskej Academi Vied. , 1969.\n[6] Martin Hepp. Representing the hierarchy of industrial tax-\nonomies in OWL:the gen/tax approach. ISWC Workshop\nSemantic Web Case Studies and Best Practices for eBusi-\nness (SWCASE05) , 2005.\n[7] Martin Hepp and Jos de Bruijn. Gentax: A generic\nmethodology for deriving OWL and RDF-S ontolo-\ngies from hierarchical classiï¬cations, thesauri, and in-\nconsistent taxonomies. The Semantic Web: Research\nand Applications, Lecture Notes in Computer Science ,\n4519/2007:129â€“144, 2007.\n[8] Margaret Kartomi. The classiï¬cation of musical instru-\nments: Changing trends in research from the late nine-\nteenth century, with special reference to the 1990s. Ethno-\nmusicology , 45:283â€“314, 2001.\n[9] Rene T. A. Lysloff and Jim Matson. A new approach to\nthe classiï¬cation of sound-producing instruments. Ethno-\nmusicology , 29:213â€“236, 1985.\n[10] Brian Matthews. Semantic web technologies. JISC Tech-\nnology and Standards Watch , 2005.\n[11] Jeremy Montagu and John Burton. A proposed new sys-\ntem for musical instruments. Society for Ethnomusicol-\nogy, 15:49â€“70, 1971.\n[12] Harry F. Olson. Music, Physics and Engineering . Dover\nPublications, 1967.\n[13] Yves Raimond. A Distributed Music Information System .\nPhD thesis, Queen Mary University of London, 2008.\n[14] Erich Moritz von Hornbostel and Curt Sachs. System-\natik der musikinstrumente: Einversuch. Zeitschriftfï¬r Eth-\nnologie, Translated by A. Baines and K. Wachsmann as\nA Classiï¬cation of Musical Instruments. Galpin Society\nJournal. , 1914.\n470"
    },
    {
        "title": "A Computational Investigation of Melodic Contour Stability in Jewish Torah Trope Performance Traditions.",
        "author": [
            "Peter van Kranenburg",
            "DÃ¡niel PÃ©ter BirÃ³",
            "Steven R. Ness",
            "George Tzanetakis"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1415612",
        "url": "https://doi.org/10.5281/zenodo.1415612",
        "ee": "https://zenodo.org/records/1415612/files/KranenburgBNT11.pdf",
        "abstract": "The cantillation signs of the Jewish Torah trope are of particular interest to chant scholars interested in the gradual transformation of oral music performance into notation. Each sign, placed above or below the text, acts as a â€œmelodic ideaâ€ which either connects or divides words in order to clarify the syntax, punctuation and, in some cases, meaning of the text. Unlike standard music notation, the interpretations of each sign are flexible and influenced by regional traditions, practices of given Jewish communities, larger musical influences beyond Jewish communities, and improvisatory elements incorporated by a given reader. In this paper we describe our collaborative work in developing and using computational tools to assess the stability of melodic formulas of cantillation signs based on two different performance traditions. We also show that a musically motivated alignment algorithm obtains better results than the more commonly used dynamic time warping method for calculating similarity between pitch contours. Using a participatory design process our team, which includes a domain expert, has developed an interactive web-based interface that enables researches to explore aurally and visually chant recordings and explore the relations between signs, gestures and musical representations.",
        "zenodo_id": 1415612,
        "dblp_key": "conf/ismir/KranenburgBNT11",
        "keywords": [
            "cantillation signs",
            "oral music performance",
            "notation",
            "melodic idea",
            "syntax",
            "punctuation",
            "meaning",
            "flexible interpretations",
            "regional traditions",
            "musical influences"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nA COMPUTATIONAL INVESTIGATION OF MELODIC CONTOUR\nSTABILITY IN JEWISH TORAH TROPE PERFORMANCE TRADITIONS\nPeter van Kranenburg\nMeertens Institute\npeter.van.kranenburg@meertens.knaw.nlDÂ´aniel P Â´eter Bir Â´o\nUniversity of Victoria\ndpbiro@finearts.uvic.caSteven Ness, George Tzanetakis\nUniversity of Victoria\nsness@sness.net, gtzan@cs.uvic.ca\nABSTRACT\nThe cantillation signs of the Jewish Torah trope are of\nparticular interest to chant scholars interested in the gradual\ntransformation of oral music performance into notation.\nEach sign, placed above or below the text, acts as a â€œmelodic\nideaâ€ which either connects or divides words in order\nto clarify the syntax, punctuation and, in some cases,\nmeaning of the text. Unlike standard music notation, the\ninterpretations of each sign are ï¬‚exible and inï¬‚uenced by\nregional traditions, practices of given Jewish communities,\nlarger musical inï¬‚uences beyond Jewish communities, and\nimprovisatory elements incorporated by a given reader. In\nthis paper we describe our collaborative work in developing\nand using computational tools to assess the stability of\nmelodic formulas of cantillation signs based on two differ-\nent performance traditions. We also show that a musically\nmotivated alignment algorithm obtains better results than\nthe more commonly used dynamic time warping method\nfor calculating similarity between pitch contours. Using\na participatory design process our team, which includes\na domain expert, has developed an interactive web-based\ninterface that enables researches to explore aurally and\nvisually chant recordings and explore the relations between\nsigns, gestures and musical representations.\n1. INTRODUCTION\nIn the last ten years there has been a growing interest in\nmusic information retrieval (MIR). A variety of techniques\nfor automatically analyzing music based on both symbolic\nand audio representations have been developed. In most\ncases the target user of MIR systems has been the average\nmusic listener rather than the specialist. There is an even\nlonger tradition of computational musicology dating back\nto the 1950s of using mathematics, statistics and eventually\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proï¬t or commercial advantage and that copies\nbear this notice and the full citation on the ï¬rst page.\nc/circlecopyrt2011 International Society for Music Information Retrieval.computers to study music. Most of this work in compu-\ntational musicology has focused on the symbolic domain\nand western music notation. More recently the idea of\nComputational Ethnomusicology in which MIR techniques\nare used to support research in musics from around the\nworld has been proposed [9]. Audio analysis techniques can\nbe used for empirical research on ï¬eld recordings for which\nno transcription is available or feasible.\nThe study of religious chant is of particular interest to\nmusicologists as it can help understand the transition from\noral transmission to codiï¬ed notation. Jewish Torah trope\nis â€œreadâ€ using the thirty cantillation signs of the teâ€™amei\nhamikra , developed by the Masoretic School between the\nsixth to the tenth centuries. The Masoretes concurrently\ninscribed the teâ€™amim along with the vowels of the Hebrew\nletters in order to ensure accuracy in future Torah reading,\nthereby altering the previous mode of oral transmission. The\nmelodic formulae of Torah trope govern syntax, pronun-\nciation and meaning and their clearly identiï¬able melodic\ndesign, determined by their larger musical environment,\nis produced in a cultural realm that combines melodic\nimprovisation with ï¬xed melodic reproduction within a\nstatic system of notation.\nTheteâ€™amim consist of thirty graphic signs. Each sign,\nplaced above or below the text, acts as a â€œmelodic idea,â€\nwhich either melodically connects or divides words in\norder to make the text understandable by clarifying syntax,\npronounciation and, in some cases, musical meaning. The\nsigns serve to indicate the melodic contour of a given\nmelody. Although the thirty signs of the teâ€™amim are\nemployed in a consistent manner throughout the Hebrew\nBible, their interpretation is ï¬‚exible: each signâ€™s modal\nstructure and melodic gesture is determined by the text\nportion, the liturgy, by regional traditions as well as by\nimprovisatory elements incorporated by a given â€œreaderâ€.\nIn the liturgical performance, the baâ€™al koreh (â€˜the owner\nof readingâ€™) embellishes the text with a melodic code,\nproviding the framework to decode the textual syntax of\nthe read Torah text by the reading religious community,\nfor whom text, and not melody, is primary. Since their\ninscription, the primary functionality of the teâ€™amim , to\n163Oral Session 2: Non-Western Music\nstructure pronunciation and syntax, remained intact. But\nas the Jewish people were dispersed throughout the world,\nsecondary levels of musical code were incorporated into the\nteâ€™amim . Borrowed melodies and modal structures, taken\nfrom surrounding musical cultures allowed not only for\nnew melodic interpretations but also for external semiotic\nmusical meaning to permeate the musical interpretation of\nthe text. As an example, the left part of Figure 1 shows the\nsign for the etnachta and the right part shows the melodic\ncontour of the performance of an etnachta .\n×˜Ö‘Ö¹×•×‘\nFigure 1 . The notational sign of the etnachta (indicated by\nthe arrow), and the melodic contour of an etnachta .\nChant scholars have investigated historical and phe-\nnomenological aspects of melodic formulas within Jewish\nTorah trope in order to discover how improvised melodies\nmight have developed to become stable melodic entities in\ngiven Jewish communities. In this paper we investigate how\ncomputational approaches can be used to support research\nin this area. More speciï¬c, audio analysis is combined\nwith content-based similarity retrieval to explore the ways\nin which melodic contour deï¬nes melodic identities. In\nparticular the question of melodic stability is investigated.\nObserving certain key teâ€™amim such as etnachta andtipha\nwe investigate aspects of self-similarity within Torah trope\nwithin and across various Jewish communities (based on\nrecordings of Hungarian and Moroccan Torah trope). This\nmight give us a better sense of the role of melodic gesture\nin melodic formulae in Jewish Torah trope practice and\npossibly a new understanding of the relationship between\nimprovisation and notation-based chant in and amongst\nthese divergent traditions.\nIt is also possible that some of the teâ€™amim have pre-\ncursors to music (for instance basic syntactical divisions,\nexclamations and sentence cadence structures). The ac-\ntual performance of the teâ€™amim also points to musical\naspects that, as scholars have pointed out, were coming\nfrom musical cultures outside of Judaism (see e.g., [2]).\nThat which has been historically studied, the relationship\nbetween Ashkenazi Torah trope and Christian plainchant,\ncan now be tested in terms of musical data analysis. By\nmeasuring the ï¬‚exibility and variability of the teâ€™amim we\ncan show how ï¬xed musical structures and improvisation\nwithin these traditions co-exist.2. RELATED WORK\nAlthough most of existing work in music information re-\ntrieval has focused on either classical music or modern pop-\nular music, in recent years there has been a growing interest\nin applying MIR techniques to other music traditions. The\nterm Computational Ethnomusicology [9] has been used\nto describe such work. There are both challenges and\nopportunities in applying MIR techniques to ethnic music\n[5]. Some representative examples include: classiï¬cation of\nraag using pitch class distributions [4], comparative analysis\nof western and non-western traditions using automatic tonal\nfeature extraction [8], rhtyhmic similarity applied to greek\nand african traditional music [10], and singer identiï¬cation\nin rembetiko music [1].\nThe goal of this project is develop tools to study Torah\ncantillation [14]. Of particular intest is the inï¬‚uence of\noutside music cultures such as christian plainchant to the\nperformance of Jewish Torah trope [2]. The primary method\nthat has been used in the past to study chant recodings\nhas been listening and manual annotation. We believe\nthat the combination of automatic analysis with web-based\ninteractive visualizations can open new possibilities in em-\npirical musicological analysis of chant recordings. In the\ndevelopment of both our techniques and web-based in-\nterface we have followed an iterative participatory design\nprocess where the domain expert (one of the authors) has\nbeen regularly providing feedback and suggestions. Our\napproach is based on ideas from the ï¬eld of query-by-\nhumming (QBH) [6, 7] adapted to the particular charac-\nteristics and constraints of our domain. In previous work\n[12] we compared various representations and methods of\nquantizing pitch contours in various chant traditions using\na similarity retrieval paradigm. In this paper we focus\non Jewish Torah trope, propose an alternative alignment\nmethod and show how the developed techniques can be\nused to inform musicological inquiries. To the best of our\nknoweldge a data-rich approach to the study of Torah trope\nas presented in this paper has not been attempted before.\n3. DATA ANALYSIS\nFor this small-scale study, we use the recordings of two\nreadings of the same Torah passage, one from the Hungarian\n(Ashkenazi) tradition1and the one from the Moroccan\n(Sephardic) tradition. The two recordings used in this study\ncan be consulted at: http://cantillion.sness.\nnet/ismir2011 . Both recordings have manually been\nsegmented into the individual teâ€™amim by the author who is\na domain expert. Even though we considered the possibility\n1Recordings used with permission of the Feher Music Center in Tel\nAviv, Israel. Although this version was catalogued as being an example\nof Hungarian cantillation, the trope melody and pronunciation correspond\nmore to Italian practices of Torah trope.\n16412th International Society for Music Information Retrieval Conference (ISMIR 2011)\nof creating an automatic segmentation tool, it was decided\nthat the task was too subjective and critical to automate.\nEach segment is annotated with a word/symbol that is re-\nlated to the corresponding cantillation sign. Each recording\ncontains approximately 130 realizations of each taâ€™am with\na total of 12 unique teâ€™amim .\n3.1 Pitch Contour Representation\nEach recording has been converted to a sequence of fre-\nquency values using the SWIPEP fundamental frequency\nestimator [3] by estimating the fundamental frequency in\nnon-overlapping time-windows of 10ms. The frequency\nsequences have been converted to sequences of real-valued\nMIDI pitches with a precision of 1 cent (which is 1/100 of\nan equally tempered semitone, corresponding to a frequency\ndifference of about 0.06%). By allowing real-valued pitches\nwe have a one-to-one correspondence to the frequencies,\nand a linear scale in the pitch domain. For each of the\nrecordings, we derive a melodic scale by detecting the peaks\nin a non-parametric density estimation of the distribution of\npitches, using a Gaussian kernel. This can be viewed as\na smoothed frequency histogram. Prominent peaks in the\nhistogram correspond to salient pitches and can be used to\nform a discrete pitch scale that is speciï¬c to the recording\nrather than any particular tuning system.\nIn a previous study [12], mean average precision values\nwere computed for each of the scales containing 1 to 13\npitches, taking all realizations of the same taâ€™am as the\nquery segment as relevant items, and using a distance\nmeasure based on dynamic time warping. The ï¬nding was\nthat quantizing the melodic contours according to the scale\ncontaining twopitches resulted in the highest mean average\nprecision. Apparently, the two most prevalent pitches have\nstructural meaning.\nIn the current study we use a different approach. Instead\nof quantizing the melodic contours, we scale them linearly\naccording to the two most prevalent pitches in the entire\nrecording. We denote the higher and lower of the two\nprevalent pitches as phigh andplow, respectively. Each pitch\nis scaled relative to plowin units of the difference between\nphigh andplow. Thus, scaled pitches with value <0are\nbelow the lowest of the two prevalent pitches and pitches\nwith value>1are above the highest of the two and pitches\nbetween 0 and 1 are between the two prevalent pitches.\nAs a result, different trope performances, sung at different\nabsolute pitch heights, are comparable.\n3.2 A distance measure for melodic segments\nOn the acquired scaled pitch contours we apply an align-\nment algorithm as described in [13], interpreting the align-\nment score as similarity measure. This approach is closely\nrelated to the use of dynamic time warping in [12], butthe current approach uses a more advanced, musicologically\ninformed, scoring function for the individual elements of the\npitch sequences.\nWe use the Needleman-Wunsch global alignment algo-\nrithm [11]. This algorithm ï¬nds an optimal alignment of\ntwo sequences of symbols, which, in our case, are sequences\nof pitches. The quality of an alignment is measured by the\nalignment score, which is the sum of the alignment scores\nof the individual symbols. If we consider two sequences of\nsymbols x:x1,...,x i,...,x n, andy:y1,...,y j,...,y m,\nthen symbol xican either be aligned with a symbol from\nsequence yor with a gap. Both operations have a score,\nrespectively the substitution score and the gap score. The\ngap score is mostly expressed as penalty, i.e. a negative\nscore. The optimal alignment and its score are found by\nï¬lling a matrix Drecursively according to:\nD(i,j) = maxï£±\nï£²\nï£³D(iâˆ’1,jâˆ’1) +S(xi,yj)\nD(iâˆ’1,j)âˆ’Î³\nD(i,jâˆ’1)âˆ’Î³,(1)\nin whichS(xi,yj)is a similarity measure for symbols,\nÎ³is the gap penalty, D(0,0) = 0 ,D(i,0) =âˆ’iÎ³,\nandD(0,j) =âˆ’jÎ³.D(i,j)contains the score of the\noptimal alignment up to xiandyjand therefore, D(m,n)\ncontains the score of the optimal alignment of the complete\nsequences. We can obtain the alignment itself by tracing\nback fromD(m,n)toD(0,0); the standard dynamic pro-\ngramming algorithm has both time and space complexity\nO(nm).\nThe similarity measure for symbols, which returns values\nin the interval [âˆ’1,1], is in our case deï¬ned as:\nS(x,y) =/braceleftbigg1âˆ’4|spxâˆ’spy|if|spxâˆ’spy|â‰¤0.5\nâˆ’1 otherwise,\nin which scaled pitch of symbol xis\nspx=pxâˆ’plow,x\npheigh,xâˆ’plow,x,\nin whichpxis the pitch of symbol x, represented in\ncontinuous midi encoding, and plow,x andphigh,x are the\nlowest and highest pitch in the entire recording to which\nsymbolxbelongs.spyis computed in the same way. We\nuse a linear gap penalty function with Î³= 0.6.\nSince the score of an alignment depends on the length of\nthe sequences, normalization is needed to compare different\nalignment scores. The alignment of two long sequences\nresults in a much higher score than the alignment of two\nshort sequences. Therefore, we divide the alignment score\nby the length of the shortest sequence. Thus, an exact\nmatch results in score 1, which is the maximal score. The\nscores are converted into distances by taking one minus the\nnormalized score, resulting in distances greater than or equal\nto zero.\n165Oral Session 2: Non-Western Music\nFigure 2 . Web-based visualization interface which allows users to listen to audio, see pitch contour visualization of different\nsigns, and to enable interactive similarity-based querying.\n4. USER INTERFACE\nWe have developed a browsing interface that allows re-\nsearchers to organize and analyze chant segments in a\nvariety of ways. Each recording is manually segmented\ninto teâ€™amim . The pitch contours of these segments can\nbe viewed at different levels of detail. They can also\nbe rearranged in a variety of ways both manually and\nautomatically.\nThe interface shown in Figure 2 has four main sections: a\nsound player, a main window to display the audio segments,\na control window, and a histogram window. The sound\nplayer window displays a spectrogram representation of\nthe sound ï¬le with shuttle controls to let the user choose\nthe current playback position in the sound ï¬le. The main\nwindow shows all the segments of the recording as icons\nthat can be repositioned automatically based on a variety of\nsorting criteria, or alternatively can be manually positioned\nby the user. The name of each segment (from the initial\nsegmentation step) appears above its F0 contour. The shuttle\ncontrol of the main sound player is linked to the shuttle\ncontrols in each of these icons, allowing the user to set the\ncurrent playback state either way.\nWhen an icon in the main F0 display window is clicked,\nthe histogram window shows a histogram of the distribution\nof quantized pitches in the selected segment. Below this\nhistogram is a slider to choose how many of the largest\nhistogram bins will be used to generate a simpliï¬ed contour\nrepresentation of the F0 curve. In the limiting case of\nselecting all histogram bins, the reduced curve is exactly the\nquantized F0 curve. At lower values, only the histogrambins with the most items are used to draw the reduced\ncurve, which has the effect of reducing the impact of outliers\nand providing a smoother â€œabstractâ€ contour. Shift-clicking\nselects multiple segments; in this case the histogram win-\ndow includes the data from all the selected segments. We\noften select all segments with the same word or taâ€™am ; this\ncauses the simpliï¬ed contour representation to be calculated\nusing the sum of all the pitches found in that particular\ntaâ€™am , enhancing the quality of the simpliï¬ed contour\nrepresentation. Figure 2 shows a screenshot of the browsing\ninterface. We have implemented a mode that allows the\nresearcher to sort the segments based on the alignment score\nfrom one segment to the other. The interface allows the user\nto select an arbitrary segment from the interface, and then\nperform a sorting of all other segments to it. In the example\nshown in Figure 2, the user has chosen a revia , and has\nsorted all the other segments based on their alignment-based\ndistance from this ï¬rst revia . One can see that the segment\nclosest to this revia is another revia from a different section\nof the audio ï¬le.\n5. RESULTS AND INTERPRETATION\nTo investigate the stability in performance of the various\nteâ€™amim , we use two approaches. Firstly, we compute the\nmean average precision for each of the teâ€™amim based on the\nalignment-distance. Each segment is taken as query and all\nrenditions of the same taâ€™am are taken as relevant items. The\nhigher the mean average precision, the higher the relevant\nitems are on the ranked lists that are obtained by sorting all\nsegments according to the distance to the query segment.\n16612th International Society for Music Information Retrieval Conference (ISMIR 2011)\nTaâ€™am Average Taâ€™am Average\n(Morocco) Precision (Hungary) Precision\n(Morocco) (Hungary)\nsofpasuq 0.550 sofpasuq 0.994\nkaton 0.399 revia 0.967\ntipha 0.306 etnachta 0.945\nmapah 0.299 pashta 0.683\npashta 0.269 tipha 0.673\nrevia 0.245 katon 0.562\netnachta 0.234 mapah 0.550\nzakef 0.206 merha 0.530\nmerha 0.158 zakef 0.231\nmunach 0.147 munach 0.179\nkadma 0.036 kadma 0.040\nTable 1 . Mean average precision for different teâ€™amim\nbased on the alignment distances.\nThe values are shown in Table 1.\nSecondly, we show the distribution of distances between\nrenditions of the same taâ€™am by plotting histograms of those\ndistances. Figure 3 shows the distribution of alignment-\nbased distances between unrelated segments. This his-\ntogram can be used as reference for comparing distances\nbetween related segments. The interface, as described in the\nprevious section, is used to examine the relations between\nindividual audio segments.\n 0 500 1000 1500 2000 2500 3000\n 0 0.5  1 1.5  2 2.5  3 3.5  4Count\nDistance\nFigure 3 . Distribution of distances between unrelated\nsegments.\nThe obtained overall mean average precisions are 0.644\nfor the Hungarian rendition and 0.309 for the Moroccan one,\nwhich are improvements concerning the results that were\npreviously achieved in [12] (0.505 and 0.229 respectively).\nUsing the current alignment-approach, the segments are\nbetter recognized, but the overall trend appears the same,\nnamely a better retrieval result for the Hungarian rendition\nas compared to the Moroccan. Since we do not know a-\npriori whether every taâ€™am has a high level of distinction, we\ncannot draw conclusions about the quality of our distance\nmeasure from the MAP-values. A low MAP-value does not\nnecessarily mean that the distance measure fails, but could\nalso indicate that the performance of the speciï¬c taâ€™am is\n 0 5 10 15 20 25 30 35\n 0 0.5  1 1.5  2 2.5  3 3.5  4Count\nDistance\n 0 5 10 15 20 25 30 35 40 45\n 0 0.5  1 1.5  2 2.5  3 3.5  4Count\nDistanceFigure 4 . Distribution of distances between renditions\nof the tipha in the Moroccan interpretation (left) and the\nHungarian interpretation (right).\n 0 5 10 15 20 25\n 0 0.5  1 1.5  2 2.5  3 3.5  4Count\nDistance\n 0 5 10 15 20 25 30 35\n 0 0.5  1 1.5  2 2.5  3 3.5  4Count\nDistance\nFigure 5 . Distribution of distances between renditions of\nthesof pasuq in the Moroccan interpretation (left) and the\nHungarian interpretation (right).\nvariable or not distinct from performance of other teâ€™amim .\nTherefore, in remainder of our analysis, we focus on var-\nious key teâ€™amim , using differences between distances and\nmean-average-precisions, along with musicological domain\nknowledge, to draw conclusions. Observing the renditions\nofsof pasuq and tipha in the Hungarian tradition, one\ncan derive that they inhibit a deï¬nite melodic stability.\nFor the sof pasuq we obtain a mean average precision as\nhigh as 0.994 and for the tipha 0.673 (for comparison, the\nï¬gures for the Moroccan performance are 0.550 and 0.306\nrespectively). This indicates that the 17 sof pasuqs are both\nsimilar to each other and distinct from all other teâ€™amim .\nThe same applies to a somewhat lesser extent to the 24\ntiphas . These ï¬ndings are conï¬rmed by the distributions\nof distances as shown in Figures 4 and 5.\n 0 0.5 1 1.5 2 2.5 3\n 0 0.5  1 1.5  2 2.5  3 3.5  4Count\nDistance\n 0 1 2 3 4 5\n 0 0.5  1 1.5  2 2.5  3 3.5  4Count\nDistance\nFigure 6 . Distribution of distances between renditions of\ntheetnachta in the Moroccan interpretation (left) and the\nHungarian interpretation (right).\nAnalyzing the distribution of distances between Moroc-\n167Oral Session 2: Non-Western Music\ncan renditions of the etnachta , as shown in the left histogram\nin Figure 6, one ï¬nds increased melodic variation while\nthe Hungarian interpretation shows greater melodic stability.\nThis is signiï¬cant, as etnachta is an example of a disjunctive\ntaâ€™am , that has a clear functionality as a syntactical divider\nwithin a given sentence. Such a melodic stability might\nhave been due to the inï¬‚uence of Christian Chant on\nJewish communities in Europe, as is the thesis of Avenary\n[2]. Simultaneously, our approach using two structurally\nimportant pitches also corresponds to the possible inï¬‚uence\nof recitation and ï¬nal tone as being primary tonal indicators\nwithin Askenazi chant practice (which the Hungarian Torah\nTrope is part of), thereby allowing for a greater melodic\nstability per trope sign than in Sephardic chant.\nThe ï¬ndings are interesting when observed in connection\nwith musicological and music historical studies of Torah\ntrope. It has long been known that the variety of melodic\nformulae in Ashkenazi trope exceeded that of Sephardic\ntrope renditions. The teâ€™amim actually entail more symbols\nthan necessary for syntactical divisions. That being said, in\ncertain teâ€™amim , like in the version of etnachta , a greater\namount of melodic variability is presented. This is not\nmirrored in the example of tipha , which serves to combine\nwords to make a clear syntactical unit. In both Hungarian\nand Moroccan variants this taâ€™am shows a greater degree\nof stability. This shows that certain conjunctive teâ€™amim ,\nwhich show greater melodic stability, might also act as more\nstable syntactical anchors in both traditions. One might\ninvestigate if this is also true in other traditions (Iranian,\nYemenite and Lithuanian).\n6. FUTURE WORK\nIn the current study, we took the two most prevalent pitches\nfor scaling. There are reasons to assume that for various\nperformance traditions different numbers of pitches are of\nstructural importance. We will investigate this in future\nresearch. The presented method proves useful for the two\nrecordings under investigation. In a next stage, we will\ncollect much more data, with the aim to study stability\nand variation between and within performance traditions\nof Torah trope on a large scale, integrating the results into\nongoing musicological and historical research on this topic.\n7. REFERENCES\n[1] Y . Stylianou A. Holzapfel. Signer identiï¬cation in\nrembetiko music. In Proc. Sound and Music Computing\n(SMC) , 2009.\n[2] H. Avenary. The Ashkenazi Tradition of Biblical Chant\nBetween 1500 and 1900. Tel-Aviv and Jerusalem . Tel-\nAviv University, Faculty of Fine Arts, School of Jewish\nStudies, 1978.[3] A. Camacho. A Sawtooth Waveform Inspired Pitch\nEstimator for Speech and Music . PhD thesis, University\nof Florida, 2007.\n[4] P. Chordia and A. Rae. Raag recognition using pitch-\nclass and pitch-class dyad distributions. In Proc. Int.\nConf. on Music Information Retrieval (ISMIR) , 2007.\n[5] O. Cornelis, M. Lesaffre, D. Moelants, and M. Leman.\nAccess to ethnic music: Advances and perspectives\nin content-based music informatino retrieval. Signal\nProcessing , 90(4):1008â€“1031, 2010.\n[6] R. Dannenberg, W.P. Birmingham, B. Pardo, N. Hu,\nC. Meek, and G. Tzanetakis. A comparative evaluation\nof search techniques for query-by-humming using the\nmusart testbed. J. Am. Soc. Inf. Sci. Technol. , 58(5):687â€“\n701, 2007.\n[7] A. Ghias, J. Logan, D. Chamberlin, and B.C. Smith.\nQuery by humming: musical information retrieval in an\naudio database. In Proc. ACM Int. Conf. on Multimedia ,\npages 231â€“236, 1995.\n[8] E. Gomez and P. Herrera. Comparative Analysis of\nMusic Recordings from Western and non-western\ntraditions by Automatic Tonal Feature Extraction.\nEmpirical Musicology Review , 3(3):140â€“156, 2008.\n[9] G.Tzanetakis, A. Kapur, W.A. Schloss, and M. Wright.\nComputational ethnomusicology. Journal of interdisci-\nplinary music studies , 1(2):1â€“24, 2007.\n[10] A. Pikrakis I. Antonopoulos, S. Theodoridis, O. Cor-\nnelis, D. Moelants, and M. Leman. Music retrieval\nby rhythmic similarity applied on greek and african\ntraditional music. In Proc. Int. Conf. on Music\nInformation Retrieval (ISMIR) , 2007.\n[11] Saul B. Needleman and Christian D. Wunsch. A general\nmethod applicable to the search for similarities in\nthe amino acid sequence of two proteins. Journal of\nMolecular Biology , 48(3):443â€“453, 1970.\n[12] Steven R. Ness, D Â´aniel P Â´eter Bir Â´o, and George\nTzanetakis. Computer-assisted cantillation and chant\nresearch using content-aware web visualization tools.\nMultimedia Tools Appl. , 48(1):207â€“224, 2010.\n[13] P. Van Kranenburg, A. V olk, F. Wiering, and R.C.\nVeltkamp. Musical models for folk-song melody align-\nment. In Proc. Int. Conf. on Music Information Retrieval\n(ISMIR) , pages 507â€“512, 2009.\n[14] Heidi Zimmermann. Untersuchungen zur Musikauffas-\nsung des rabbinischen Judentums . Peter Lang, Bern,\n2000.\n168"
    },
    {
        "title": "Dynamic Programming in Transposition and Time-Warp Invariant Polyphonic Content-Based Music Retrieval.",
        "author": [
            "Mika Laitinen",
            "Kjell LemstrÃ¶m"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1416094",
        "url": "https://doi.org/10.5281/zenodo.1416094",
        "ee": "https://zenodo.org/records/1416094/files/LaitinenL11.pdf",
        "abstract": "We consider the problem of transposition and time-warp invariant (TTWI) polyphonic content-based music retrieval (CBMR) in symbolically encoded music. For this setting, we introduce two new algorithms based on dynamic programming. Given a query point set, of size m, to be searched for in a database point set, of size n, and applying a search window of width w, our algorithms run in time O(mnw) for finding exact TTWI occurrences, and O(mnw2) for partial occurrences. Our new algorithms are computationally more efficient as their counterparts in the worst case scenario. More importantly, the elegance of our algorithms lies in their simplicity: they are much easier to implement and to understand than the rivalling sweepline-based algorithms. Our solution bears also theoretical interest. Dynamic programming has been used in very basic content-based retrieval problems, but generalizing them to more complex cases has proven to be challenging. In this special, seemingly more complex case, however, dynamic programming seems to be a viable option.",
        "zenodo_id": 1416094,
        "dblp_key": "conf/ismir/LaitinenL11",
        "keywords": [
            "dynamic programming",
            "transposition and time-warp invariant",
            "polyphonic content-based music retrieval",
            "symbolically encoded music",
            "search window",
            "exact TTWI occurrences",
            "partial occurrences",
            "searching for in a database",
            "computational efficiency",
            "simpler implementation"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nDYNAMIC PROGRAMMING IN TRANSPOSITION AND TIME-WARP\nINV ARIANT POLYPHONIC CONTENT-BASED MUSIC RETRIEV AL\nMika Laitinen\nDepartment of Computer Science\nUniversity of Helsinki\nmikalait@cs.helsinki.fiKjell Lemstr Â¨om\nDepartment of Computer Science\nUniversity of Helsinki\nklemstro@cs.helsinki.fi\nABSTRACT\nWe consider the problem of transposition and time-warp\ninvariant ( TTWI ) polyphonic content-based music retrieval\n(CBMR) in symbolically encoded music. For this setting,\nwe introduce two new algorithms based on dynamic pro-\ngramming. Given a query point set, of size m, to be searched\nfor in a database point set, of size n, and applying a search\nwindow of width w, our algorithms run in time O(mnw )\nfor ï¬nding exact TTWI occurrences, and O(mnw2)for par-\ntial occurrences. Our new algorithms are computationally\nmore efï¬cient as their counterparts in the worst case sce-\nnario. More importantly, the elegance of our algorithms lies\nin their simplicity: they are much easier to implement and to\nunderstand than the rivalling sweepline-based algorithms.\nOur solution bears also theoretical interest. Dynamic\nprogramming has been used in very basic content-based re-\ntrieval problems, but generalizing them to more complex\ncases has proven to be challenging. In this special, seem-\ningly more complex case, however, dynamic programming\nseems to be a viable option.\n1. INTRODUCTION\nIn this paper we study how to search for excerpts of music in\na large database resembling a given query pattern. We allow\nboth the query pattern and the database to be polyphonic.\nTypically the query pattern constitutes a subset of instru-\nments appearing in the database while the database may\nrepresent a full orchestration of a musical piece. The gen-\neral setting requires methods based on symbolic representa-\ntion capable of dealing with true polyphonic subset match-\ning; audio-based methods are only applicable to rudimen-\ntary cases where queries are directed to clearly separable\nmelodies.\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proï¬t or commercial advantage and that copies\nbear this notice and the full citation on the ï¬rst page.\nc/circlecopyrt2011 International Society for Music Information Retrieval.Except for some trivial cases, the straightforward CBMR\napproach of linear string representation combined with a\nstring matching algorithm does not properly capture the poly-\nphonic CBMR problem. Recently, a more appropriate, geo-\nmetric modeling of music has been succesfully used by sev-\neral authors [5â€“7]. This approach models polyphonic music\nvery naturally, but usually also takes into account another\nimportant feature intrinsic to the problem: the matching pro-\ncess ignores extra intervening notes in the database that do\nnot appear in the query. Extra notes may occur because of\ndifferent polyphonic arrangements, musical decorations and\nunexpected noise. Recent geometric methods [2, 3, 6] have\nchallenged different timing problems. In the ï¬rst setting,\nthe occurrences may be tranposed and time-scaled copies\nof the query [2, 6]. Under the transposition and time-scale\ninvariance (theTTSI setting), however, the queries need to\nbe given exactly in tempo. In a realistic application local\ntime jittering occur in every note-onset in the query, and a\nstronger, transposition and time-warp invariance is required\nfor a successful matching (the TTWI setting). The latter is\nthe setting for our algorithms to be introduced. The ï¬rst\nsolutions for the TTWI setting was recently presented by\nLemstr Â¨om and Laitinen [3].\nOur algorithms are based on the pitch-against-time rep-\nresentation of note-on information (see Fig 1). The musical\npieces in a database are concatenated in a single geometri-\ncally represented ï¬le, denoted by T;T=t0,t1,...,tnâˆ’1,\nwhere each element tjâˆˆR2for0â‰¤jâ‰¤nâˆ’1and the el-\nements are sorted in the lexicographic order. Any symbolic\nmusic ï¬le is convertible in this representation. Later it may\nbe possible to convert audio ï¬les and sheet music by using\naudio transcription and optical music recognition. Although\nboth processes are error prone, it may be the case that the\nresulting representations are usable due to the robustness\nof our algorithms against noise. In a typical retrieval case\nthe query pattern P,P=p0,p1,...,pmâˆ’1;piâˆˆR2for\n0â‰¤iâ‰¤mâˆ’1, to be searched for is monophonic and much\nshorter than the polyphonic database Tto be searched; our\nalgorithms, however, deal equally well with monophonic\nand polyphonic input. Sometimes a search window wis\n369Poster Session 3\n/noteheads.s2\n/dots.dot/noteheads.s2/noteheads.s2\n/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot\n/noteheads.s1/dots.dot\n/dots.dot/dots.dot/noteheads.s1 /dots.dot43\n/clefs.F43/noteheads.s2/noteheads.s2\n/noteheads.s2/noteheads.s2 /clefs.G43/clefs.G\n/noteheads.s2/noteheads.s2\n/accidentals.sharp/flags.d3\n/noteheads.s2Lei/noteheads.s2/flags.d3\n/noteheads.s1/noteheads.s2/rests.1steht/noteheads.s2 /noteheads.s2/flags.u3\ner/noteheads.s2\n/dots.dot/flags.d3\nund/noteheads.s2\n/noteheads.s1/noteheads.s1/rests.1drecht, ne/noteheads.s2/flags.d3\nsei/noteheads.s2/flags.u3\n/noteheads.s1/flags.d3\n/noteheads.s1nim/noteheads.s2/flags.d3\nihm/noteheads.s2 /rests.1\nstill,/noteheads.s2/flags.u3\nmer/noteheads.s2\n/accidentals.sharp/dots.dot/flags.u3\n/noteheads.s1/dots.dot/dots.dot\nc1\ntimepitch\nCc\n2 3 1Figure 1 .On top, an excerpt from Schubertâ€™s Der Leiermann .\nBelow, the related point-set representation. The points associated\nwith the vocal part are depicted by squares.\napplied and typically wâ‰¤m, i.e.wâ‰¤m/lessmuchn.\nThe problems under consideration are modiï¬ed versions\nof two problems originally represented in [7]. Below we\ngive the original problems P1 and P2 (pure transposition\ninvariance, TI), their transposition and time-scale invariant\nversions S1 and S2 ( TTSI ), and the transposition and time-\nwarp invariant modiï¬cations W1 and W2 under considera-\ntion ( TTWI ). For the partial matches in P2, S2 and W2, one\nmay either use a threshold Î±to limit the minimum size of\nan accepted match, or to search for maximally sized matches\nonly.\nâ€¢Find pure (P1) / time-scaled (S1) / time-warped (W1)\ntranslations ofPsuch that each point in Pmatches\nwith a point in T.\nâ€¢Find pure (P2) / time-scaled (S2) / time-warped (W2)\ntranslations ofPthat give a partial match of the points\ninPwith the points in T.\nFig. 2 gives six query patterns to be searched for in the ex-\ncerpt of Fig. 1, exemplifying the six problems P1, S1, W1,\nP2, S2 and W2 given above.\nUkkonen et al. introduced online algorithms for problems\nP1 and P2 that run in times O(mn)andO(mnlogm)in the\nworst case, respectively, and in O(m)additional space [7].\nLemstr Â¨om et al. [4] showed that the practical performance\ncan be improved at least by an order of magnitude by com-\nbining sparse indexing and ï¬ltering. P2 is known to belong\n/noteheads.s2A\n/noteheads.s2/noteheads.s2B\n/clefs.G /noteheads.s2 /noteheads.s2/noteheads.s2/noteheads.s2 /timesig.C44 /accidentals.flat/accidentals.flat/noteheads.s2/noteheads.s2 /accidentals.sharp /noteheads.s2/noteheads.s2/noteheads.s2\n/noteheads.s2/dots.dot/noteheads.s2/dots.dot/noteheads.s2/dots.dot /noteheads.s1/dots.dot /noteheads.s1 /dots.dot/dots.dot/noteheads.s2/dots.dot/noteheads.s2D\n/dots.dot/noteheads.s1/noteheads.s2 /accidentals.flat/accidentals.flat\n46C\n/noteheads.s2/clefs.G /noteheads.s1/dots.dot /dots.dot /accidentals.sharp/dots.dot /noteheads.s2/dots.dot\n/noteheads.s2/noteheads.s2 /noteheads.s2 /clefs.G /noteheads.s2 /noteheads.s2/flags.u3\n/noteheads.s2/noteheads.s2/flags.u3\n/noteheads.s2/dots.dotE\n/noteheads.s2/noteheads.s2 /timesig.C44 /accidentals.flat/accidentals.flat/dots.dot /noteheads.s2/noteheads.s2F\n/accidentals.sharpFigure 2 .Example queries. For query Aan occurrence in Fig. 1\nwould be found in all the six problem cases P1-2, S1-2, W1-2; for\nBin cases P2, S2, W2; for Cin S1-2, W1-2; for Din S2, W2; for\nEin W1-2 and for Fin W2 only.\nto a problem family for which o(mn)solutions are conjec-\ntured not to exist. Nevertheless, there is an online approxi-\nmation algorithm for it running in time O(nlogn)[1].\nIn [6], Romming and Selfridge-Field gave a geometric-\nhashing based algorithm for S2 working in time O(wnm3)\nand spaceO(w2n). Lemstr Â¨om [2] generalized algorithms\nP1 and P2 to the time-scaled problems S1 and S2. The\nalgorithms work in O(mÎ£ log Î£) time andO(mÎ£)space,\nwhere Î£ =O(wn)when searching for exact occurrences\nandÎ£ =O(nw2)when searching for partial occurrences.\nThe ï¬rst algorithms for W1 and W2 were introduced only\nvery recently in [3]. The sweepline-based algorithms are\nfurther generalizations of those above. In this TTWI case\nthe windowing takes an invaluable role; the number of false\npositives would grow uncontrollably without it. The asymp-\ntotic time and space complexities, however, remain the same\nas with the solution for S1 and S2.\nIn this paper we introduce new algorithms for the TTWI\nsetting. Our algorithms are based on dynamic program-\nming and their asymptotic worst case complexities are lower\nthan those of the earlier rivals: for the case W1 we have\nanO(mnw )algorithm; for the W2 case our algorithm runs\nin timeO(mnw2). In our experiments, however, in usual\nquery settings the sweepline-based algorithms often outper-\nform our dynamic programming algorithms. The main con-\ntribution of the new algorithms is in their simplicity which\nmakes them easy-to-understand and easy-to-implement. In\naddition to this elegance, in the worst-case scenario our new\nalgorithms clearly outperforms the sweepline-based algo-\nrithms.\nIt is also theoretically very interesting to discover that dy-\nnamic programming is applicable in the TTWI setting. Ap-\nplying dynamic progamming for the more straightforward\nproblems, including the TTSI setting, has thus far proven to\nbe too challenging.\n37012th International Society for Music Information Retrieval Conference (ISMIR 2011)\nDPW2(P,T,w )\n1M=A four-dimensional array, ï¬lled with âˆ’1\n2fori= 0 toP.sizeâˆ’1\n3 forj= 0 toT.sizeâˆ’1\n4 F ILL-M(M,P,T,w,i + 1,j+ 1,1,1)\n5 R EPORT -RESULTS (M)\nFILL-M(M,P,T,w,pcur,tcur,x,y )\n1/ /Return result if it has been already calculated\n2ifM[pcur,tcur,x,y ]/negationslash=âˆ’1\n3 returnM[pcur,tcur,x,y ]\n4/ /Bounds checking, base case for recursion\n5iftcurâ‰¥T.size orpcurâ‰¥P.size\n6 return 0\n7best= 0\n8/ /Do the notes under investigation match each other?\n9ifTtcur.yâˆ’Ttcurâˆ’y.y==Ppcur.yâˆ’Ppcurâˆ’x.y\n10 a=FILL-M(M,P,T,w,pcur + 1,tcur + 1,1,1)\n11 best=max(a+ 1,best)\n12/ /Can we still extend the search inside the window?\n13 ify<w\n14 a=FILL-M(M,P,T,w,pcur,tcur + 1,x,y + 1)\n15 best=max(a,best)\n16/ /Finally, ï¬nd the matches with Ppnot included\n17a=FILL-M(M,P,T,w,pcur + 1,tcur,x + 1,y)\n18best=max(a,best)\n19M[pcur,tcur,x,y ] =best\n20 return best\nFigure 3 . Pseudocode illustration for D PW2. In D PW1 lines\n17-18 need to be removed.\n2. ALGORITHMS\nIn this section we describe two new algorithms to ï¬nd ex-\nact and partial transposition and time-warp invariant occur-\nrences of a pattern Pfrom a given database T. To distinct\nour new algorithms from the previous sweepline algorithms\nW1 and W2 (solving problems W1 and W2), we shall re-\nfer to our dynamic programming algorithms by D PW1 and\nDPW2, respectively.\nThe new algorithms require the input to be given as a list\nof notes, where each note is represented by a pair (x,y)in\na two-dimensional coordinate system. The x-component of\nthe pair represents the note-on time, the y-component rep-\nresents the pitch of the note. We assume both PandTto\nbelexicographically sorted , i.e.aprecedesbif and only if\na.x<b.x ora.x=b.xanda.y<b.y .\nLet us next introduce some important deï¬nitions. A trans-\nlation ofPwith vectorfresults inP+f=p0+f,p 1+\nf,...,pmâˆ’1+f, wherepi+f= (pi.x+f.x,pi.y+f.y).This translation captures two signiï¬cant musical phenom-\nena, asf.xaligns the excerpt time-wise, while f.ytrans-\nposes the excerpt to a lower or higher key. We also deï¬ne\nmusical time-scaling with Ïƒ,ÏƒâˆˆR+. This time-scaling\nonly affects horizontal translation, i.e. scales only the time\ncomponents.\nThe following examples and deï¬nition illustrate the type\nof occurrences we aim at ï¬nding with the algorithms.\nExample 2.1 Letp=/angbracketleft3,1/angbracketright,f=/angbracketleft2,5/angbracketrightandÏƒ= 2. Then\np+Ïƒf=/angbracketleft7,6/angbracketright.\nDeï¬nition 2.2 tÏ„0...tÏ„mâˆ’1, a subsequence of T, is a time-\nwarp occurrence of pÏ€0...pÏ€mâˆ’1, a subsequence of P, if\nfor eachi,0â‰¤iâ‰¤mâˆ’2, there is a time-scaling ÏƒiâˆˆR+\nsuch thatÏƒi(pÏ€i+1âˆ’pÏ€i) =tÏ„i+1âˆ’tÏ„iand0â‰¤Ï€j< m ,\nÏ€j<Ï€j+1,0â‰¤Ï„j<nandÏ„j<Ï„j+1for allj.\nLet us next illustrate the essence of the deï¬nition, where\nwe have an exact time-warping occurrence ofP.\nExample 2.3 Letp0=/angbracketleft2,7/angbracketright,p1=/angbracketleft4,8/angbracketright,p2=/angbracketleft6,8/angbracketright,p3=\n/angbracketleft9,7/angbracketrightandt0=/angbracketleft1,1/angbracketright,t1=/angbracketleft2,3/angbracketright,t2=/angbracketleft3,2/angbracketright,t3=/angbracketleft4,2/angbracketright,t4=\n/angbracketleft5,1/angbracketright. Thent0,t2,t3,t4is an exact time-warping occur-\nrence ofp0,p1,p2,p3withÏƒ0= 1,Ïƒ1=1\n2andÏƒ2=1\n3.\nHad we had t4=/angbracketleft5,0/angbracketrightin Example 2.3, then t0,t2,t3would\nhave been a partial time-warping occurrence of P, matching\np0,p1andp2.\nIn [3], Lemstr Â¨om and Laitinen deï¬ned two problems:\nï¬nding exact and partial translation and time-warp invariant\noccurrences of PfromT. The exact nature of an occurrence\nis captured in deï¬nition 2.2. These problems can be de-\nscribed followingly: in the exact case, we aim to ï¬nd a sub-\nsequencetÏ„0,tÏ„1,...,tÏ„mâˆ’1so that for each pi,i<mâˆ’1,\npi+1.yâˆ’pi.y=tÏ„i+1.yâˆ’tÏ„i.yholds. In the partial case, we\naim to ï¬nd longest subsequence from Pfor which we can\nï¬nd a matching subsequence from T, as in the deï¬nition 2.2.\nIn our setting, it is useful to apply a windowing restric-\ntion, which states that two consecutive notes in the database\nsubsequence cannot be more than wnotes away from each\nother in the database. The window size wis designed to\nlimit the number of senseless occurrences, and it is also able\nto signiï¬cantly speed up the algorithms.\nOur algorithms are recursive in nature, and are very sim-\nilar to each other. We will cover the more complex D PW2\nin depth, and pinpoint the differences to D PW1.\nIn the beginning, the aim of the algorithms is to ï¬ll the\nM-table by calling function F ILL-M (see Fig. 3) with ap-\npropriate base states. F ILL-M takes 8 parameters, 4 of which\nare variables: pcur,tcur,x andy. These variables deï¬ne the\nstate F ILL-M is currently solving.\nFILL-M returns the length of the longest occurrence we\ncan construct from the state it was given. In the case of\n371Poster Session 3\nFigure 4 . Time resource performance comparison of W2\nand D PW2.\nDPW2, the current state is deï¬ned by the four parameters.\nThe parameters deï¬ne the state followingly: ppcurâˆ’xis the\nlast note chosen from the pattern, ttcurâˆ’yis the last note\nchosen from the database, whereas ppcur,ppcur +1,...,pmâˆ’1\nandttcur,ttcur +1,...,tnâˆ’1are the notes that can be se-\nlected in future from pattern and database, respectively.\nIn the case of D PW2, F ILL-M has at maximum three pos-\nsible options in any state. F ILL-M evaluates, which of the\noptions is the best one, and returns the length of the longest\noccurrence. If the note under investigation can be legally\nadded to the pattern, then the algorithm adds the note, and\nmoves on to ï¬nd new ones. Also, if we have not yet reached\nthe windowing limit, then we can move on without adding\nany notes, and ï¬nding a new candidate further away in the\ndatabase. Our third option, which is available in the case of\nDPW2, is skipping ppcur altogether and not including it to\nthe match at all. In the case of D PW1, we can never skip any\nppcur, since otherwise the match being constructed would\nnot be exact anymore.\nThe algorithm can legally add notes to the occurrence, if\nnote pairs (ppcurâˆ’x,ppcur)and(ttcurâˆ’y,ttcur)match each\nother under the translation and time-warp invariances, i.e.\nppcur.yâˆ’ppcurâˆ’x.y=ttcur.yâˆ’ttcurâˆ’y.y. Then we can\ncall F ILL-M recursively with a state (pn,tn,xn,yn)where\nxn=yn= 1,pn=pcur + 1 andtn=tcur + 1. This\nmeans that in the new state, the previous notes that were\npicked from pattern and database, were ppnâˆ’1andttnâˆ’1,\nrespectively. Naturally, in the new state, we can ï¬nd new\nmatching notes from ppnandttnonwards.\nAlso, if the parameters for F ILL-M are same that have\nbeen used previously, then the algorithm can avoid calculat-\ning this state again, since every time F ILL-M is called with\nthe same parameters, it has to return the same result. There-\nfore every time we have ï¬nished calculating a state, we can\nstore the result, and return the stored result whenever F ILL-\nFigure 5 . Time resource performance comparison of W2\nand D PW2. Database used represented the worst case sce-\nnario for W2.\nM is again called with the same parameters.\nThe case of D PW1 is very similar to that of D PW2. In\nDPW1, however, we cannot allow the algorithm to skip any\nnotes from the pattern, which means that xwill always be 1.\nAsxis not a variable anymore, we do not have to store it in\ntheM-table; it is initialized it to be 3-dimensional.\nAs F ILL-M requires that at least one note has been se-\nlected from both the pattern and the database, we must ini-\ntialize theM-table by calling F ILL-M with all possible com-\nbinations of ï¬rst notes (see Fig. 3). Once the M-table is\nï¬lled, we can construct the matches we are interested in by\ninvestigating the M-table in a similar fashion to the way\nFILL-M does. Also, if we are only interested in the length\nof the longest occurrence, we do not need to investigate M-\ntable afterwards at all, as F ILL-M itself returns the length of\nthe longest occurrence.\nThe time complexities for D PW1 and D PW2 areO(mnw )\nandO(mnw2), respectively. The number of states depends\nof the possible values of the variables. The variables can\nvary followingly: 0â‰¤pcur < m ,0â‰¤tcur < t ,1â‰¤xâ‰¤\nwand1â‰¤yâ‰¤w. In D PW1xis not a variable, so there are\nO(mnw )states, and in D PW2 we getwtimes more states,\ndue to the fact that xcan vary. Since the amount of calcula-\ntion in each state is constant, the time complexities become\nsimply the number of states in both cases.\n3. EXPERIMENTS\nWe compared the performance of W2 to that of D PW2 in\ndifferent scenarios, and also W1 against D PW1 in a typical\nscenario. In our experiments, we used music data from Mu-\ntopia database so that the pieces of music were concatenated\ntogether to form a large database. In the worst case compar-\nison databases and patterns were speciï¬cally tailored. In\n37212th International Society for Music Information Retrieval Conference (ISMIR 2011)\nFigure 6 . Time performance comparison of W1 and D PW1.\nall tests, we kept the pattern and window sizes constant,\nm=w= 10 .\nIt was expected that in cases where the database size is\nsmall, W2 would be slightly faster than D PW2, since the\ncomplexity difference would not be able to kick in with\nsmaller database sizes, and the ability of being able to skip\nnon-compact matches would outweigh the additional loga-\nrithmic term. However, it seemed likely that D PW2 would\nbecome gradually faster with larger database sizes when com-\npared with W2.\nIn our experiments, W2 outpeformed D PW2 in the smaller\ncases, as expected. With growing database sizes, however,\nDPW2 was not able to catch up, and instead the performance\ndifference became even larger in favour of W2.\nIt seems that the fact that W2 calculates only the compact\nmatches, while D PW2 calculates exactly all matches, is re-\nsponsible for the difference. Even though theoretical time\ncomplexity suggests that W2 should eventually be slower\nwith larger databases, it seems that in a typical setting the\nability of W2 to eliminate matches grows faster than the ad-\nditional logarithmic term, as depicted in Fig. 4. This sug-\ngests that the expected complexity of W2 would be signiï¬-\ncantly smaller than its worst-case complexity.\nThe property of being able to skip non-compact matches\nis even more visible in the exact case, where D PW1 is signif-\nicantly slower than W1 in a real-world scenario (see Fig. 6).\nIt seems that the penalty for ï¬nding all possible matches is\neven larger here.\nTo further experiment on the effect of getting rid of addi-\ntional matches, we constructed the absolute worst case sce-\nnario for W2, where all the notes in both the pattern and the\ndatabase have the same pitch. In this setting, W2 would not\nbe able to eliminate many matches, which results in a large\namount of additional work. In Fig. 5, we depict the time us-\nage of the two algorithms in the worst case for W2. From the\nï¬gure it is evident that W2 uses a signiï¬cant amount of time\nin this type of setting, even with very small databases. It isalso noteworthy that the time usage of W2 grows quickly.\n4. CONCLUSIONS\nIn this paper we presented two new algorithms for the trans-\nposition and time-warp invariant ( TTWI ) content-based poly-\nphonic music retrieval setting. We used the geometric frame-\nwork where each note is represented as a point in the Eu-\nclidean plane (pitch value against on-set time). The frame-\nwork has several advantages: it is intuitive, it intrinsically\ndeals with polyphonic music, transposition invariance and\nsubset matching. The TTWI setting that allows for local\ntime jittering makes the approach usable in real-world ap-\nplications where queries are always somewhat out of tempo.\nOur D PW1 algorithm solves the exact matching problem un-\nder the TTWI setting while D PW2 is for the partial matching\nproblem under the same setting. The algorithms, based on\ndynamic programming, have better asymptotic worst-case\ntime complexities than their only existing rivals [3], here\ncalled W1 and W2, based on the sweepline techique.\nOur experiments revealed that in a typical query case W2\nis faster than D PW2. This is due to the capability of W2\nto eliminate non-compact matches while D PW2 thoroughly\nscrutinizes every possible match. The impact of the elimi-\nnation, however, was surprisingly strong given that W2 has\nan additional logarithmic term in its asymptotic complexity.\nNevertheless, when looking for consistent performance, our\nDPW2 is the choice to be taken as in complex query cases\nW2 freezes suddenly. The elegance of our new algorithms\nlie in their simplicity: they, unlike the rivaling algorithms,\nare very easy both to implement and to understand.\nAs hinted by Fig. 4, with the future very large music\ndatabases, neither W2 nor D PW2 alone would work in an\ninteractive setting. As a future work, we will study a dis-\ntributed calculation process. Even though the sweepline-\nbased solutions were somewhat faster in typical real-world\nqueries in our experiments, the distributed setting is pre-\nsumed to be signiï¬cantly different: dynamic programming\nalgorithms are generally easily distributable, while distribut-\ning sweepline-based algorithms may prove to be very chal-\nlenging.\n5. REFERENCES\n[1] R. Clifford, M. Christodoulakis, T. Crawford, D. Mered-\nith, and G. Wiggins. A fast, randomised, maximal subset\nmatching algorithm for document-level music retrieval.\nInProc. ISMIRâ€™06 , pages 150â€“155, Victoria, 2006.\n[2] K. Lemstr Â¨om. Towards more robust geometric content-\nbased music retrieval. In Proc. ISMIRâ€™10 , pages 577â€“\n582, Utrecht, 2010.\n[3] K. Lemstr Â¨om and M. Laitinen. Transposition and time-\nwarp invariant geometric music retrieval algorithms. In\n373Poster Session 3\nProc. ADMIREâ€™11, Third International Workshop on\nAdvances in Music Information Research , Barcelona,\n2011.\n[4] K. Lemstr Â¨om, N. Mikkil Â¨a, and V . M Â¨akinen. Filtering\nmethods for content-based retrieval on indexed sym-\nbolic music databases. Journal of Information Retrieval ,\n13(1):1â€“21, 2010.\n[5] A. Lubiw and L. Tanur. Pattern matching in polyphonic\nmusic as a weighted geometric translation problem. In\nProc. ISMIRâ€™04 , pages 289â€“296, Barcelona, 2004.\n[6] C.A. Romming and E. Selfridge-Field. Algorithms for\npolyphonic music retrieval: The hausdorff metric and\ngeometric hashing. In Proc. ISMIRâ€™07 , pages 457â€“462,\nVienna, 2007.\n[7] E. Ukkonen, K. Lemstr Â¨om, and V . M Â¨akinen. Geometric\nalgorithms for transposition invariant content-based mu-\nsic retrieval. In Proc. ISMIRâ€™03 , pages 193â€“199, Balti-\nmore, 2003.\n374"
    },
    {
        "title": "Social Capital and Music Discovery: An Examination of the Ties through Which Late Adolescents Discover New Music.",
        "author": [
            "Audrey Laplante"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1417253",
        "url": "https://doi.org/10.5281/zenodo.1417253",
        "ee": "https://zenodo.org/records/1417253/files/Laplante11.pdf",
        "abstract": "Research on everyday life information seeking has demonstrated that people often relied on other people to obtain the information they need. Weak ties (i.e., acquaintances) were found to be particularly instrumental to get new information. This study employed social network analysis to examine the characteristics of the ties through which late adolescents (15-17 years old) discover new music. In-depth interviews with 19 adolescents were conducted, which generated a sample of 334 ties. A statistical analysis of the ties showed that these adolescents relied mostly on strong ties to expand their music repertoire, that is, on people to which they felt very close and with whom they had frequent contacts. These ties were predominantly homophilous in terms of age, gender and musical taste. It was also found that parents were more likely than friends or other types of kins to be instrumental for music discovery. These findings suggest that a better knowledge of the characteristics of the ties through which people discover new music could provide useful insights for the design of recommender systems that include social networking features.",
        "zenodo_id": 1417253,
        "dblp_key": "conf/ismir/Laplante11",
        "keywords": [
            "weak ties",
            "acquaintances",
            "social network analysis",
            "late adolescents",
            "new music",
            "in-depth interviews",
            "334 ties",
            "strong ties",
            "homophilous",
            "parents"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\n   SOCIAL CAPITAL AND MUSIC DISCOVERY: AN EXAMINATION OF THE TIES THROUGH WHICH LATE ADOLESCENTS DISCOVER NEW MUSIC Audrey Laplante Ã‰cole de bibliothÃ©conomie et des sciences de lâ€™information, UniversitÃ© de MontrÃ©al audrey.laplante@umontreal.ca ABSTRACT Research on everyday life information seeking has demon-strated that people often relied on other people to obtain the information they need. Weak ties (i.e., acquaintances) were found to be particularly instrumental to get new infor-mation. This study employed social network analysis to ex-amine the characteristics of the ties through which late ado-lescents (15-17 years old) discover new music. In-depth in-terviews with 19 adolescents were conducted, which gener-ated a sample of 334 ties. A statistical analysis of the ties showed that these adolescents relied mostly on strong ties to expand their music repertoire, that is, on people to which they felt very close and with whom they had frequent con-tacts. These ties were predominantly homophilous in terms of age, gender and musical taste. It was also found that par-ents were more likely than friends or other types of kins to be instrumental for music discovery. These findings suggest that a better knowledge of the characteristics of the ties through which people discover new music could provide useful insights for the design of recommender systems that include social networking features. 1. INTRODUCTION Social psychology of music has long informed us that mu-sic practices are inherently social: the social context molds how people perceive, experience or engage with music [12]. Cultural taste and especially musical taste often serve as a mean of distinction and prestige [1]. Oneâ€™s music pref-erences reflect who one is or aspire to be. Therefore, it would be difficult to predict oneâ€™s musical taste solely by analyzing the objective and intrinsic characteristics of the music one loves. This explains why people often rely on their social network to expand their music repertoire: in ad-dition to considering oneâ€™s taste when making recommen-dations, friends and relatives are able to take into account the values, attitudes and beliefs associated with the music.  This also explains why most systems that provide per-sonalized recommendations for music (e.g., Last.fm, iTunes Genius) or for other cultural items such as books (e.g., Am-azon) or movies (e.g., MovieLens), rely on social or collab-orative filtering rather than content-based filtering. What allows each of these recommender systems to distinguish itself from others is the algorithm it uses to generate the recommendations and, more specifically, the type of infor-mation the algorithm makes use of, which can include im-plicit feedback (e.g., listening habits, previous purchases) and/or explicit feedback (e.g., user ratings, lists of favorite artists). However, as Celma [2] points out, these systems also have their drawbacks, such as the so called â€œcold start problem,â€ which applies to both new music and new users, and the difficulty these systems have to provide novel, non obvious recommendations. Possible solutions that have been proposed include the development of hybrid recom-mender systems that would combine collaborative and con-tent-based filtering [17], and the use of some characteristics of the users that are known to influence musical taste, such as demographic characteristics, socioeconomic background and personality traits [15].  Social network sites might also open new possibilities for generating music recommendations. A site like Face-book already offers several ways for members to express their music preferences, by means of implicit feedback (e.g., the sharing of links to music videos) and explicit feedback (e.g., the list of favorite music in the userâ€™s pro-file, the â€œlikeâ€ button that allows users to express interest in a music video shared by another user or in the page of a music artist). In addition to that, Facebook contains exten-sive information about oneâ€™s social network which, again, can be explicit (e.g., becoming â€œfriendâ€ with someone, in-dicating the type of kinship with another user) or implicit (e.g., the strength of a relationship can be estimated by cal-culating the number of interactions occurring between two members, the number of times they tag each others on pic-tures and/or the number of networks or friends they have in common). Considering the popularity of social networking sites and the impact of the social context on musical prac-tices, it seems relevant to explore whether relationships characteristics could be exploited to improve social filtering algorithms in music recommender systems that include so-cial networking features.  A first step in that direction is to examine the character-istics of the ties through which people discover new music in everyday life. It is with this objective in mind that this study was designed. Its aim was to study how music infor-mation circulates within the social networks of late adoles-\n Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that cop-ies bear this notice and the full citation on the first page.  Â© 2011 International Society for Music Information Retrieval  \n341Oral Session 5: User Studies\n   cents (15-17 years old) and, more specifically, to examine the attributes of the ties that are instrumental (or not) for music discovery. Older adolescents are a particularly inter-esting population to study. According to a survey conduct-ed for the Pew Internet & American Life Project, it is the age group that shows the highest percentage of social net-working sites users [11]. Adolescence is also the period during which people construct their identity, and music plays a central role in that process [12]. Furthermore, late adolescence is critical in the formation of oneâ€™s taste, since it is the period during which the â€œcrystallization of musical tasteâ€ generally occurs [11].   2. RELATED WORK Previous research has demonstrated that people play an important role in information provision, especially to an-swer everyday-life information needs [8, 9]. People also rely heavily on their social networks to discover new music [10, 17]. To better understand who people choose to ap-proach to get the information they need, several research-ers have adopted a social network perspective and found that weak ties (e.g., acquaintances) were usually more in-strumental than strong ties (e.g., kins and friends) to ac-quire new information [6, 9], which is at the origins of Granovetterâ€™s â€œStrength of Weak Tiesâ€ theory [6]. Within the social network approach, the term â€œsocial capitalâ€ has been used to refer to resources (e.g., information) embed-ded in a social structure [9]. Social network researchers have established that there was a clear relationship between similarity and association. People tend to interact more with people who have similar demographic characteristics (e.g., age, gender, education, social class), as well as people who share their behavior patterns, values and beliefs (e.g., political orientation) [13]. This is the principle of homophily, which posits that â€œdis-tance in terms of social characteristics translates into net-work distance, the number of relationships through which a piece of information must travel to connect two individu-alsâ€ [13]. In other words, homophilous ties are more likely to be strong ties. Granovetter also demonstrated that the stronger the tie between two individuals, the greater the overlap in their social networks. As a result of the ho-mophilous nature of strong ties and the extent of overlap in their social networks, strong ties usually have access or are exposed to similar information. This explains why weak ties were found to be more instrumental than strong ties in the acquisition of new information: not only are they usual-ly exposed to different information, they can also act as a bridge between two groups of densely knit networks of close friends [6].  3. METHODOLOGY This study is part of a larger project examining the way music information is shared within the social network of late adolescents living in an urban area. Pretest interviews with 6 adolescents were conducted in the summer of 2010. The main study was conducted in the winter and spring of 2011 and included 19 late adolescents (15 to 17 years old). 3.1 Social Network Analysis Social network analysis (SNA) was used to study how mu-sic information is shared within the social networks of ado-lescents. SNA focuses on â€œrelationships among social enti-ties, and on the patterns and implications of these relation-ships,â€ [16] in particular on the flow of resources (e.g., in-formation) among actors. It provides a set of methods and theoretical concepts that can be used to analyze and de-scribe the characteristics of social networks and the ties they are composed of. To perform SNA, an egocentric or personal network approach was adopted, which consists in examining social networks from the perspective of focal persons (the â€œEgosâ€). This approach is well suited to popu-lations that are large and difficult to delimit. It contrasts with the whole network approach, which looks at the ties that all members of a well-defined population (e.g., all members of an organization) maintain with each other. Alt-hough SNA was first developed and used by sociologists, it rapidly proved its utility in other fields of research, includ-ing information science. Its use in this domain was promot-ed by several researchers, among them Haythornthwaite who explained that â€œSince information is an important re-source, and one that often depends on making and main-taining contact with the right people, a social network ap-proach offers a rich variety of concepts and techniques to describe and explain information accessâ€ [7]. 3.2 Data Collection  Data were collected during in-depth, face-to-face individual interviews. Different instruments were used to obtain in-formation about the social network of each participant and the way music information is exchanged within the net-work. We used an adaptation of the social network mapping tool designed by Todd and described in [3], which consists of a set of seven concentric circles at the center of which is the participant (called â€œEgoâ€). To fill the map, participants were asked to think about how people were clustered in their life (e.g., school, family, friends from elementary school, friends from summer camp, etc.). These clusters or sectors were put on the map. To elicit the names of the per-sons to be included on their map (called â€œaltersâ€), we used three different methods. We used a name generator, which consists in asking, for each cluster identified, the names of the people to which they felt close or very close. Partici-pants were also asked to name all people through which they had discovered music in the last year or with whom they often discussed music. Finally, we used a method called critical incident technique [4], which attempts to rely on a concrete situation to generate a more accurate report of oneâ€™s behavior than a hypothetical question would. Partici-pants were therefore asked to recall how they had discov-ered their favorite artist and to provide a detailed account of the context. To help participants recall the situation, the re-searcher could ask additional questions, such as â€œWhen did you hear the music of that artist for the first time?â€ â€œDid someone make you listen to the music of that artist?â€ or \n34212th International Society for Music Information Retrieval Conference (ISMIR 2011)\n   â€œHad anyone discovered that artist because of you?â€ Partic-ipants were invited to position all alters generated by any of the three methods on their map, using the seven concentric circles to indicate the degree of emotional closeness with each of them.  Once the map was completed, a questionnaire was ad-ministered to the participants to collect information about each alter (e.g., age, gender, school level) and their rela-tionship with him or her (e.g., nature of relationship, fre-quency of contacts, duration of relationship). Participants were also asked to indicate, on a five-point scale ranging from â€œvery differentâ€ to â€œvery similarâ€, the degree of simi-larity between themselves and each alter in terms of musi-cal taste. With the objective of estimating the degree of in-strumentality of each alter for music discovery, participants were asked to indicate, on a five-point scale ranging from â€œneverâ€ to â€œvery often,â€ how often they discovered new music because of him or her.  Because of the complexity of the task and the length of the accompanying questionnaire, the interviews with the participants lasted between 61 and 95 minutes (mean=79).  3.3 Participants and Sample Size Participants were recruited from a public school located in downtown MontrÃ©al. This school offers programs in French and English, from kindergarten to grade 11. All 10th and 11th grade students enrolled in the French sector (i.e., 173 students) were invited to participate in the study. Nineteen accepted the invitation, for a response rate of 10.9%. All lived within the greater MontrÃ©al area, 14 were female and 5 were male; 12 were in grade 10 and 7 in grade 11. The interviews with the participants elicited the names of 334 alters, which means that our sample was composed of 334 dyads or Ego-alter ties.  4. FINDINGS Although qualitative data were also gathered during the in-terviews, the present paper focuses on the analysis of the quantitative data collected via the social network mapping tool and the questionnaire about Ego-alter dyads. More specifically, the analysis focuses on the characteristics of the people and the ties that were considered instrumental for music discovery by the participants. Each participant named between 8 and 29 alters (mean=18.6; median=17), for a total of 334 alters for the 19 participants. Of the 334 alters, 137 (41%) were not consid-ered instrumental for discovering new music, which means that 197 (59%) were considered instrumental at various de-grees. To the question â€œHow often do you discover music because of this person?â€ participants responded â€œrarelyâ€ for 61 alters (18%), â€œoccasionallyâ€ for 61 alters (18%), â€œoftenâ€ for 46 alters (14%) and â€œvery oftenâ€ for 29 alters (9%).  A multinomial logistic regression was performed to ex-amine the attributes of the persons and ties that were per-ceived as instrumental. Multinomial logistic regressions are employed to handle cases where the dependent variable (in this case, the degree of instrumentality of a tie) is nominal or ordinal and has more than two classes, and the independ-ent variables are nominal, ordinal and/or continuous. Con-sidering the relatively small size of our sample, we some-times combined categories to increase the validity of the analysis. For instance, for the instrumental variable, we combined the 4th and 5th points of the five-point scale to create the â€œvery instrumentalâ€ category; and the 2nd and 3rd points to create the â€œsomewhat instrumentalâ€ category. Cat-egories were also combined for the variables emotional closeness, age and frequency of contacts. Results of the lo-gistic regression analysis are shown in Table 1.  4.1 Strength of Instrumental Ties The strength of a tie is usually estimated by a combination of factors, including emotional closeness and frequency of contacts. The logistic regression analysis shows that emo-tional closeness was associated with instrumentality: the odds for a close person (6th and 7th grades of the seven-point scale combined) to be considered very instrumental compared to an acquaintance (1st, 2nd and 3rd grades com-bined) were increased by a factor of 3.8. Pearsonâ€™s chi-square significance test (36.6, p<0.05) also confirmed the correlation between these variables. The duration of a relationship, on the other hand, was not found to be a predictor of the likelihood of a tie to be instrumental, but the frequency of contacts was. People par-ticipants saw at least 3 times a week were 1.6 times more likely to be considered instrumental for music discovery than people they saw less than once a week, and 2.0 times more likely to be very instrumental. A high frequency of mediated contacts (i.e., contacts by phone calls, SMS, chat or email) was also positively related to instrumentality, alt-hough this association lost its significance when both levels of instrumentality were considered. In all, we can safely say that ties through which participants discovered new music were mostly strong ties.  4.2 Nature of Relationship and Instrumentality The nature of the relationship was significantly related to instrumentality. Of special interest is the surprising finding that parents were positively related to instrumentality. In-deed, the odds of a parent being very instrumental were 5.7 times greater than for a friend. To be more specific, fathers seemed to be particularly instrumental. Of the 15 fathers mentioned, all were considered instrumental at various de-grees, and 7 were considered very instrumental. Of the 16 mothers, 3 were not considered instrumental and only 4 were considered very instrumental. The grandparents clear-ly belonged to a different category: the odds of grandpar-ents being instrumental were decreased by a factor 0.03 compared to friends. As for siblings, they were not signifi-cantly related to instrumentality. This finding should be in-terpreted carefully, however. Although the sample size did not allow for the consideration of more specific categories for the logistic regression, a look at the data suggests that younger and older siblings should probably be treated sepa-rately. Of the 13 younger brothers/sisters mentioned by the participants, 9 were considered â€œnot instrumentalâ€, 2 were â€œsomewhat instru-mentalâ€ and 2 were â€œvery instrumentalâ€.  \n343Oral Session 5: User Studies\n   \n Table 1. Logistic regression analysis of the significance of people and relationship characteristics for instrumentality  Sample All Instrumentala Somewhat Instrumentalb Very Instrumentalc  % Odds ratio (95% CI) P Odds ratio (95% CI) P Odds ratio (95% CI) P Nature of relationshipd        Friende 58% 1.0  1.0  1.0  Parent 9% 5.3 (1.6-18.1) .008** 5.1 (1.4-18.1) .012* 5.7 (1.5-21.6) .010* Sibling 6% 0.6 (0.2-1.6) .343 0.6 (0.2-1.8) .352 0.7 (0.2-2.4) .558 Grandparent 7% 0.03 (0.003-0.187) <.001** 0.04 (0.005-0.296) .002**   Emotional closeness        Not closee 8% 1.0  1.0  1.0  Moderately close 33% 1.4 (0.6-3.4) .431 1.3 (0.5-3.3) .411 1.8 (0.5-6.8) .011 Close 59% 2.6 (1.1-5.9) .026* 2.1 (0.8-5.2) .041* 3.8 (1.1-14.0) .009** Duration of relationship  Less than 2 yearse 8% 1.0  1.0  1.0  2-5 years 35% 0.9 (0.4-2.1) .617 0.7 (0.3-1.8) .455 1.4 (0.4-5.0) .571 More than 5 years 57% 0.8 (0.4-1.9) .764 0.6 (0.3-1.5) .315 1.4 (0.4-4.6) .608 Frequency of in-person contacts Less than once a weeke 47% 1.0  1.0  1.0  1-2 times a week 9% 1.6 (0.7-3.5) .262 1.6 (0.7-3.8) .314 1.6 (0.6-4.5) .375 3 or more times a week  43% 1.6 (1.0-2.5) .043* 1.4 (0.9-2.4) .175 2.0 (1.1-3.6) .028* Frequency of mediated contacts Nevere 8% 1.0  1.0  1.0  Occasionally 53% 1.8 (0.8-4.3) .149 1.8 (0.7-4.8) .209 1.8 (0.6-6.0) .304 Often 39% 2.7 (1.1-6.4) .024* 2.5 (0.9-6.8) .063 3.0 (0.9-9.8) .074 Age of alter         > 50e 11% 1.0  1.0  1.0  25-49 15% 4.9 (1.9-12.3) .001** 4.6 (1.6-13.4) .005** 5.3 (1.5-19.0) .011* 18-24 6% 5.9 (1.8-19.2) .003** 5.0 (1.3-19.1) .020* 7.6 (1.7-34.5) .009** 15-17 59% 4.4 (2.0-9.3) <.001** 4.3 (1.8-10.5) .001** 4.5 (1.5-13.6) .009** < 15 9% 1.1 (0.4-3.1) .909 1.1 (0.3-3.8) .864 1.0 (9.7-70.1) .975 Education institution        Different schoole 8% 1.0  1.0  1.0  Same school 33% 2.5 (1.3-4.6) .005** 1.7 (0.9-3.2) .136 5.9 (2.2-15.7) <.001** *p<.05, **p<.01 a Alters for which egos answered any value other than â€œneverâ€ to the question â€œHow often do you discover music because of this person?â€  b Alters for which egos answered â€œoccasionallyâ€ or â€œrarelyâ€. c Alters for which egos answered â€œoftenâ€ or â€œvery oftenâ€. d Some categories were omitted as they did not include enough data (e.g., cousin, uncle/aunt, teacher, librarian)  e Reference category for the variable. \n34412th International Society for Music Information Retrieval Conference (ISMIR 2011)\n   In comparison, all 6 older brothers/sisters were con sidered instrumental at various degrees (4 were â€œsome-what instrumentalâ€ and 2 â€œvery instrumentalâ€).   4.3 Characteristics of Instrumental Alters An examination of the characteristics of the instrumental alters also revealed some patterns. The logistic regression analysis on friendship ties showed that friends who were current schoolmates of the ego were 2.5 times more likely to be instrumental for discovering music than the friends attending a different school. Age was also significantly related to instrumentality. Compared with people over 50, the likelihood of being very instrumental for discovering music were increased by a factor of 4.5 for people in the 15-17 age category, by 7.6 for people in the 18-24 catego-ry, and by 5.3 for the people in the 25-49 category.   4.4 Homophily/Heterophily of Very Instrumental Ties Ties can also be characterized by their degree of ho-mophily or heterophily. To investigate that aspect, we compared the demographic characteristics of participants with those of very instrumental ties, as well as the global characteristics of the sample (see Table 2). When we look at the sample data, we notice that the participantsâ€™ social networks were composed of a majority of homophilous ties in terms of age (60% of the alters mentioned were at most 1 year older or younger than the ego) and gender (69% of alters were the same sex than the ego). This is hardly surprising considering that, according to the prin-ciple of homophily (see Section 2), people tend to interact more with people who are similar to them. Perhaps of greater interest is the fact that people who were consid-ered by participants as being very instrumental for dis- covering new music followed almost exactly the same distribution for these variables, which means that instru- \nTable 2. Characteristics of very instrumental alters com-pared to characteristics of ego mental ties were also predominantly homophilous in age and gender. That being said, a non-negligible proportion of both the sample ties and the instrumental ties were much older than the ego (24% of all alters and 21% of very instrumental alters were more than 12 years older than Ego). In both cases, these people were mostly family members: Family members represented 93% of the much older alters and 94% of the much older very instrumental alters.  On some aspects, a greater proportion of the ties on which participants relied to expand their music repertoire were homophilous compared to the ties of the whole sam-ple. While 68% of the friends in the sample attended the same school than the ego, 92% of the friends who were very instrumental did. Unsurprisingly, an examination of the data on the similarity of musical taste shows that the distribution of the very instrumental ties was skewed to-wards the end of the scale whereas the ties of the whole sample seemed to follow a normal distribution. In other words, participants tended to prefer people in their social network who shared their musical taste to get music rec-ommendations. It should be noted, however, that not all alters who had very similar taste than Ego were consid-ered very instrumental (only 67% were), which means that a high degree of similarity in musical taste did not always lead to instrumentality for music discovery.  Overall, very instrumental ties included a similar or greater proportion of homophilous ties compared to all ties of the sample, depending on the characteristics we examine. This contradicts the idea that people tend to rely on weak and heterophilous ties to gain new information.  5. DISCUSSION AND CONCLUSION The data analysis revealed that discovering new music is dissimilar in many ways from other information-seeking situations. While previous research supports the im-portance of weak ties in the acquisition of new infor-mation, the present study concludes that the late adoles-cents we interviewed relied mostly on strong ties to ex-pand their music repertoire, that is, on people to which they felt very close and with whom they had frequent con-tacts. These ties were also predominantly homophilous in terms of age, gender and musical taste, although we high-lighted the fact that a minority but significant proportion of the very instrumental alters were much older than the participants. Related to that, we found that parents, and especially fathers, were more likely than friends or other types of kins to be instrumental for music discovery.  The analysis of the qualitative data collected during that project should help better understand these findings. In the meantime, we can provide some potential explana-tions. Three reasons can be offered for the important role strong ties play in this context.  Firstly, adolescents are very exposed to music, mainly because of recent techno-logical innovations. Music is widely available on the Web, legally or illegally, in streaming or for download, making it more accessible than it has ever been. Video-sharing sites such as YouTube also offer a wide variety of  Very instru-mental Sample  n % n % Age     Younger (more than 1 year) 3 4% 22 7% Same (Â± 1 year) 48 64% 199 60% Slightly older (2-12 years) 8 11% 33 10% Much older (more than 12 years) 16 21% 80 24% Gender     Same 22 29% 105 31% Different 53 71% 230 69% School (for friends)     Same 33 92% 105 68% Different 3 8% 50 32% Musical taste      Not similar 2 3% 36 13% Slightly similar 3 4% 62 22% Moderately similar 14 19% 77 28% Similar 34 45% 71 25% Very similar 22 29% 33 12% \n345Oral Session 5: User Studies\n   music videos, which can easily be shared using social networking sites. In addition, many adolescents have smartphones or portable music players on which they can carry large music collections they can share with their friends: seeing two adolescents splitting earphones to lis-ten to music together is very common. As a result, adoles-cents who are not highly invested in music might not feel the need to actively seek music recommendations. Sec-ondly, discovering music, although important in the con-struction of identity in adolescence, is certainly not as crucial in oneâ€™s life as seeking job- or health-related in-formation when needed. People might therefore be less inclined to make efforts to meet these needs and seek ad-vice from music mavens or from people whose job it is to recommend music (e.g., music store staff, librarians) but are less readily available. Thirdly, because of the subjec-tivity of music interpretation, as well as the attitudes and values associated with the music, recommending music requires a much better knowledge of the information-seeker than answering other types of information needs does, a knowledge that is difficult to grasp through a short interview. It is therefore plausible that adolescents prefer to rely on strong ties because they consider that people who know them well and know their taste provide more relevant recommendations.   We can also offer different possible explanations for the instrumentality of parents for music discovery. The Strength of Weak Ties theory could shed some light on this phenomenon. Although parents are strong ties, they are more heterophilous than friendship ties. Parents are much older and, as such, have been exposed to music from different periods. Considering that musical taste usually crystalizes in late adolescence, it is likely that the music to which they listen today dates, at least partly, from that period. Were it not for older people in their life, adolescents would possibly not be exposed to this music. However, this does not explain why parents were found to be more instrumental than uncles, aunts, or grandparents. Other explanations could be that they are more accessible and/or that music familiarity often leads to music appreci-ation [5]. As one participant of our pilot study put it, â€œThe songs you grew up with, whether you want it or not, you always end up listening to them again.â€ The findings of this research reiterate how complex the task of recommending relevant music is and how intri-cately bound musical taste is to social context. They sug-gest that a better understanding of the process through which people discover new music through friends, rela-tives or other acquaintances could provide some useful insights for the design of music recommender systems that integrated social networking features. Further re-search is now needed to investigate whether these results can be extended to other late adolescent populations or to older or younger populations.  Acknowledgements This work has been supported by the Fonds de recherche sur la sociÃ©tÃ© et la culture (FQRSC) du QuÃ©bec. 6. REFERENCES [1] P. Bourdieu: La Distinction: Critique Sociale du Jugement, Ã‰ditions de minuit, Paris, 1979.  [2] O. Celma: â€œFoafing the Music: Bridging the Semantic Gap in Music Recommendation,â€ Proc. of the Semantic Web-ISWC 2006, pp. 927-934, 2006. [3] W. R. Curtis: The Future Use of Social Networks in Mental Health, Social Matrix Research, Boston, 1979. [4] J. C. Flanagan: â€œThe Critical Incident Technique,â€ Psychological Bulletin, Vol. 51, pp. 327-358, 1954. [5] S. Frith: Performing Rites: On the Value of Popular Music. Harvard University Press, Cambridge, 1996. [6] M. S. Granovetter: â€œThe Strength of Weak Ties: A Network Theory Revisited,â€ Sociological Theory, Vol. 1, pp. 201-233, 1983. [7] C. Haythornthwaite: â€œSocial Network Analysis: An Approach and Technique for the Study of Information Exchange,â€ Library & Information Science Research, Vol. 18, No. 4, pp. 323-342, 1996. [8] H. Julien and D. Michels: â€œIntra-Individual Information Behaviour in Daily Life,â€ Information Processing & Management, Vol. 40, pp. 547-562, 2004.  [9] C. A. Johnson: â€œChoosing People: The Role of Social Capital in Information Seeking Behaviour,â€ Information Research, Vol. 10, pp. 10-1, 2004. [10] A. Laplante: â€œEveryday Life Music Information-Seeking Behaviour of Young Adults: An Exploratory Study,â€ Ph.D. thesis, McGill Univ., 2008. [11] A. Lenhart, et al.: â€œSocial Media and Young Adults,â€ Pew Internet & American Life Project, 2010.  [12] A. C. North and D. J. Hargreaves: The Social and Applied Psychology of Music, Oxford University Press, Oxford; New York, 2008. [13] M. McPherson, et al.: â€œBirds of a Feather: Homophily in Social Networks,â€ Annual Review of Sociology, Vol. 27, No. 1, pp. 415-444, 2001. [14] S. J. Tepper and E. Hargittai: â€œPathways to music exploration in a digital ageâ€ Poetics, Vol. 37, pp. 227-249, 2009. [15] A. Uitdenbogerd and R. V. Schyndel: â€œA Review of Factors Affecting Music Recommender Success,â€ Proceedings of the Third International Conference on Music Information Retrieval, pp. 204-208, 2002. [16] S. Wasserman and K. Faust: Social Network Analysis: Methods and Applications, Cambridge University Press, New York, 1994. [17] K. Yoshii, et al.: â€œAn Efficient Hybrid Music Recommender System Using an Incrementally Trainable Probabilistic Generative Model,â€ IEEE Transactions on Audio, Speech, and Language Processing, Vol. 16, No. 2, pp. 435-447, 2008.  \n346"
    },
    {
        "title": "How Similar Is Too Similar?: Exploring Users&apos; Perceptions of Similarity in Playlist Evaluation.",
        "author": [
            "Jin Ha Lee 0001"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1417124",
        "url": "https://doi.org/10.5281/zenodo.1417124",
        "ee": "https://zenodo.org/records/1417125/files/Lee11.pdf",
        "abstract": "The Audio Music Similarity and Retrieval (AMS) task in the annual Music Information Retrieval eXchange relies on human-evaluation. One limitation of the current design of AMS is that evaluators are provided with scarce contextual information as to why they are evaluating the similarity of the songs and how this information will be used. This study explores the potential use of AMS results for generating playlists based on similarity. We asked participants to listen to a subset of results from the 2010 AMS task and evaluate the set of candidates generated by the algorithms as a playlist generated from a seed song (the query). We found that while similarity does affect how people feel about the candidate set as a playlist, other factors such as variety, metadata, personal preference, familiarity, mix of familiar and new music, etc. also strongly affect users' perceptions of playlist quality as well. We discuss six user behaviors in detail and the implications for the AMS evaluation task.",
        "zenodo_id": 1417125,
        "dblp_key": "conf/ismir/Lee11",
        "keywords": [
            "Audio Music Similarity and Retrieval (AMS)",
            "human-evaluation limitation",
            "playlist generation",
            "results analysis",
            "user behavior study",
            "seed song evaluation",
            "algorithm-generated playlists",
            "playlist quality perception",
            "variety and metadata",
            "personal preference"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\n  \n \nHOW SIMILAR IS TOO SIMILAR?: EXPLORING USERSâ€™ \nPERCEPTION S OF SIMILARITY IN PLAYLIST EVALUATION  \n Jin Ha Lee   \n University of Washington  \njinhalee@uw.edu   \nABSTRACT  \nThe Audio Music Similarity and Retrieval (AMS) task in \nthe annual Music Information Retrieval eXchange relies  on \nhuman -evaluation. One limitation of the current design of \nAMS is that evaluators are provided with scarce contextual \ninformation as to why they  are evaluating the similarity of \nthe songs and how this information will be used. This study \nexplores the potential use of AMS results for generating \nplaylists based on similarity. We asked participants to listen \nto a subset of results from the 2010 AMS t ask and evaluate \nthe set of candidates generated by the algorithms  as a \nplaylist generated from a seed song (the query). We found \nthat while similarity does affect how people feel about the \ncandidate set as a playlist, other factors such as variety, \nmetada ta, personal preference , familiarity , mix of familiar \nand new music, etc. also strongly affect users' perceptions \nof playlist quality as well. We discuss six user behaviors  in \ndetail  and the implications for the AMS evaluation task.  \n1. INTRODUCTION  \nAudio Music Similarity and Retrieval (AMS) is one of the \nevaluation tasks conducted in Music Information Retrieval \nEvaluation eXchange  (MIREX).  AMS task  relies on human \nevaluation for ground truth. Evaluators are asked to listen to \na set of query -candidate  pairs and indicate  how similar they \nthink the songs are on a broad scale (i.e., very similar, \nsomewhat similar, not similar) as well as a fine scale (i.e., a \nscore between 0 -100). In 2010, the number of test quer ies \nwas 120 and each participating algorith m returned 5 results \nper query  [7]. Based on th e human evaluation, average pr e-\ncision scores are calculated for each algorithm and the \nranking of algorithms  is determined.     \nOne limitation with the design of the AMS  task is that \nevaluators are rating  only the similarity  between each of the \nquery -candidate pairs, not the candidate set  as a whole. \nMoreover, the evaluators are not given any background i n-formation on a use scenario; why  they are evaluating the \nsimilarity of the  songs and how those data will be used.   \nThe objective of this study is to explore one of the pote n-\ntial uses of t he AMS evaluation task . One way of using m u-\nsic similarity data is to generate playlists or recommend a-\ntions for users. How would users respond to the AMS re-\nsults if they w ere presented as playlist s generated for users \nto listen to? From the previous studies on playlists, we a l-\nready know that users value both variety and coherence in \ntheir playlists [ 4, 11], in other words, they want playlists \nwith songs that are similar to each other, but not too similar. \nHow does this Goldilocks -style similarity translate to AMS \nsimilarity metrics? W hen you compare the fine score given \nfor AMS results and the users â€™ evaluation of the results as \nplaylists, how similar or different are they?  Also can we \nlearn anything new about what users expect from playlists \nin addition to what we already know?  This paper presents \nthe findings from eight interviews conducted in order to a n-\nswer these questions.  \n2. DESIGN  OF THE STUDY  \n2.1 Test Collection  \nWe conducted in -depth interviews asking participants to \nlisten to a subset of the results of the MIREX 2010 AMS \ntask and evaluate the  candidate set a s playlists . 7 queries, \neach from different genres (i.e., blues, classical, country, \nelectronica, hip -hop, jazz, rock) were selected as test qu e-\nries. Of these 7 queries, each participant was asked to \nchoose at least 3 queries to evaluate. This was to ensure that \npartic ipants have some freedom to choose the  genre  that \nthey are familiar  with and most likely  to listen to in real life. \nTable 1 shows the list of queries tested in our study.  \nA total of 8 algorithms participated in 2010 AMS task, \nhowever, in order to reduce us er fatigue we tested candidate \nsets of only 3  algorithms for each query. The candidate sets \nfor blues, rock, and hip -hop were selected based on their  \naverage  fine scores such that they would have similar \nscores within genre and represent a spectrum of scores b e-\ntween genres. The classical, electronica, jazz, and country \ncandidate sets were chosen to represent a variety of average \nfine scores for the same query [See Table 2].  \n \nPermission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fee provided that copies are \nnot made or distributed for profit or commercial advantage and that co p-\nies bear this notice and the full cit ation on the first page.   \nÂ© 2011  International Society for Music Information Retrieval  \n109Poster Session 1\n  \n \nGenre  Query Title  Query Artist  \nBlues  Somebody's Been Talkin'   Blind Boy Fuller  \nClassical  Concerto No. 1 in C major \nPart 2  Monica Huggett  \nCountry  Sylvia's Mother  Bobby Bare  \nElectronica  Q-Works   Q-Factor  \nHip-Hop Paper Chase  The Mountain Brothers  \nJazz Time's Lie  Gary Meek  \nRock  Spank Thru  Nirvana  \nTable  1. List of test queries  \nGenre  No.  \nof  \nusers  Algorithm  Fine \nScore  Average \nUser  \nRating  Standard \nDevi a-\ntion \nBlues  3 BWL1  100 3.67 0.47 \nPSS1  100 4.50 0.40 \nSSPK2  100 4.17 0.85 \nRock  6 SSPK2  85.8 3.50 0.96 \nPS1 83 3.83 0.62 \nTLN1  81.6 2.75 1.31 \nHip-Hop 3 PSS1  69.2 2.67 1.03 \nSSPK2  66.6 3.77 0.56 \nTLN1  66.6 3.67 0.24 \nClassical  3 SSPK2  84 3.33 0.47 \nPS1 78 3.50 1.78 \nBWL1  66 3.00 0.41 \nElectron i-\nca 3 PSS1  81.8 3.50 0.71 \nPS1 63.6 3.67 1.25 \nTLN1  41 2.33 1.70 \nJazz 4 TLN1  73 3.50 0.79 \nSSPK2  57 2.70 1.63 \nPSS1  41 2.38 1.29 \nCountry  3 SSPK2  77.2 3.83 0.24 \nPS1 64.8 3.67 1.25 \nBWL1  50 3.67 0.47 \nTable  2. List of algorithms , the number of participants, \nsimilarity scores, and average ratings from participants   \n2.2 Task Design  \nEach candidate set was presented to participants as a \nplaylist consisting of 5 songs. The participants listened to \nthe 30 second clips of these songs , multiple times  if desired. \nWe used the 30 second clips rather than the whole songs to \nbe consistent with the A MS task and evaluation. The pa r-\nticipants were asked to imagine that these playlists were \ngenerated by 3 different systems that used the query as the \nseed song. After listening to the 3 candidate sets per query, \nthey were asked to rate each playlist on a 5 point scale and \nalso rank them. We asked the participants the reasons for \nliking or disliking the playlists, and also to imagine an id e-al playlist and what kinds of characteristics that playlist \nwould have or not have. The interview data were analyzed \nusing a grounded theory approach which allows us to ge n-\nerate a theory from  empirical  data [ 6].  \n2.3 Participants  \nParticipants were  recruited by using a snowball method \nstarting with the colleagues of the lead researcher  who are \ninterested in music . They were  selected so that they reflect \nsome variance in their preferred music genre and style. 3 of \nthe participants were in 20s, 2 were in 30s, and 3 were in \n40s. 6 participants were male and 2 were female. Most pa r-\nticipants listen to music at least occasionally,  although the \ndegree of their interests did vary. In the discussion below, \nresponses from different participants are identified by their \nassigned number ( i.e., P1â€“P8). \n3. DATA AND DISCUSSION  \n3.1 Overview  \nTable 2 shows the list of algorithms that were tested for \neach query. The 4th column shows the average fine scores \nassigned to the candidate lists by the human evaluators in \nAMS task  [7]. The 5th column shows the average rating \nfrom our participants.  \n3.2 User Behaviors  \nIn the following, we  provide a list of  patterns that emerged \nfrom the user behaviors we ob served.  \n \n \nFigure 1 . Fine scores and average user ratings   \n3.2.1 Similarity does matter, but variety is also important  \nWhen we compared the fine scores with t he average ratings \nfrom our participants  for the 21 algorithms , we did o bserve \nsome co rrelation (Pearsonâ€™s r  = 0.66) . Figure 1 shows the \nscatterplot of the fine scores and the average user ratings. \nThere seem s to be a stronger correlation between the fine \nscores and the user ratings for the playlists with either very \nhigh similarity score or  very low similarity score. The 020406080100120\n0 1 2 3 4 5Fine Score  \nAverage User Rating  \n11012th International Society for Music Information Retrieval Conference (ISMIR 2011)\n  \n \nplaylists with med ium similarity scores ( around  60-80), on \nthe other hand,  do show greater variation in user rating . \nThis su ggests  that similarity does have some effect on how \npeople feel about the pla ylists, but other factors are having \nan impact as well.   \nOne common theme that emerged  from all the responses \nwas the importance  of variety as a ll eight partici pants said \nthat they want  some variety in the playlist. In fact, a nu m-\nber of participants  (P1, P2, P4, P5, P6)  reacted positively to \nthe playlists that had coherent set of songs, but also said \nsome lists were too similar and needed more variation (P1, \nP3, P4, P6, P7, P8).  \n \nP8: Itâ€™s kind of monotonous . The songs all had pretty \nmuch the same tempo â€¦it was all on 10 all the time, and \nthereâ€™s no variety . And again all the tempos were the same,   \nthey all had big drums, and consistently all big sound .   \n \nFrom this quote, we  can see that for P8, the same tem po, \ninstrumentation, and style were the reasons why he felt that \nthe songs on the playlist were too similar. This participant  \nalso said it was okay to have multiple songs from the same \nartist, although other participants had different opinions.  \n \n P8: (Son gs from the same artists are) usually okay , be-\ncause if I like the band I want to hear more of it , and if I \ndonâ€™t like them I already changed the channel.  \nP1: There â€™s no lyrics in these so they kind of fit, but \nthree songs by the same artist on the same playlist is kind \nof, out of five, is a little bit, I think, extreme .  \n \nAs you can see, variety and coherence meant different \nthings for different participants . To name a few examples, \nP7 preferred  variation across genres, P1 wanted various  art-\nists, and P4 liked di verse musical style. Participants also \nfocused on different aspects when they said the playlist was \ncoherent. For example, P5 focused on lyrical content, \nwhereas P2 focused on tempo, P1 and P3 focused on mood, \netc. When user s perceive the variety or coherence of a \nplaylist, they will react different ly to different features, thus  \nthe transparen cy of the system  seems important as di s-\ncussed in [2, 13]. P4 expressed frustration that many of the \ncurrent systems provide playlists or recommendations \nwithout telling the user how exactly the songs were selected.  \n \nP4: There should be why did you get this song  maybe \nbecause I donâ€™t understand most of the times, like I would \nput in the seed song and get these and Iâ€™m like , what is \ngoing on here?  \n \nIn fact, in Ã…man and Liikkanen â€™s survey of music re c-\nommendation systems [ 1], most of the systems received \nvery low scores for transparency. Some music recommen d-\ner systems do provide at least limited information about the songs to users. For instanc e, Pandora provides information \nabout the features of the selected songs that are taken into \naccount when they generate their playlists (e.g., it features \npop/rock qualities, a subtle use of paired vocal harmony , \nmixed acoustic and electric instrumentation ). The users, \nhowever, are not able to  specify which of these features \nthey want the system to focus on  in generating playlists. \nFor instance, some user may use Everlong  by Foo Fighters \nas his seed song because he want s songs with similar i n-\nstrumentation whereas another user may use the same seed \nsong wanting songs with romantic lyrics.  \n3.2.2 Metadata affects how users feel about the music  \nIn addition to relying on the musical features , various types \nof metadata  can be used to further improve the selection of \nsongs in a playlist . In previous works,  use of  song /album  \ntitle, artist, genre , user rating, etc.  have been discussed  [10, \n11, 12, 14]. In addition to these, we believe metadata such \nas lyrics can play a significant role. P6, for in stance, gave \nthe highest rating for one of the blues list because  of the \nsimilar  lyrical content. P2, P4 and P5 said they also create \ntheir own lists based on lyrics . Moreover, P5 and P7 said  \nthey want to be able to filter songs that have graphic lyrics .  \nOther potentially  useful metadata mentioned by our pa r-\nticipants was the theme. P4, P5, and P8  said that they create \ntheir own lists based on a theme /story (e.g., trains, 4th of \nJuly). The theme of the song may be automatically extrac t-\ned, inferred from  the title or lyrics , or assigned by users \nthough social  tagging. Time period was also important for \nour participants . P1, P2, and P3 mentioned that the songs in \nthe given playlist were from different era and that negativ e-\nly affected how they felt about it (e.g., mix of different \nclassical music periods, rock music from 80s and 90s). P2 \nsaid that the song from different time period s â€œdisrupted the \nflowâ€ of the playlist. P1 also mentioned  the difference in \nthe quality of sound recordings from different eras.   \n \n P3: The slightly jarring thing is â€¦that first seed sounded \nto me more like sort of baroque music so like in the 16th or \n17th century  and then from what I remember of the ones in \nthe playlist, they were sort of like 18th century and some \n19th century  stuff at the end â€¦not the same time period .  \nP1: A lot of this older country music that was recorded \nbefore modern recording equipment, has this kind of echoy, \ntin canny sound to it, right? And I donâ€™t like that.  That re-\nally muffled poor recording â€¦I just c anâ€™t get over that.  \n3.2.3 Having a song users love or hate can significantly a f-\nfect how they feel about the whole playlist  \nPersonal preference  of music highly affected how people \nfelt about the playlist as a whole. Participants  seemed  de-\nlighted  when they heard the song or artist they liked. Some \n111Poster Session 1\n  \n \nspecifically s tated  that they were  rating the playlist higher \nbecause they really liked one of the songs.  \n \nP8: Actually the other one probably should have been 2  \n(which he rated 3), but I really love that Foo Fighter so ng \nsoâ€¦ How can you dislike that melody line? La la la la laâ€¦  \nP1: Learn to Fly was very poppy so it didnâ€™t quite fit but \nyou know, I like that song so that doesnâ€™t really bother me \ntoo much . And I like Soundgarden so thatâ€™s good .  \n \nThe overall preference of the genre or musical style also \nmattered. For instance, P3 said â€œI just prefer  more upbeat \nsongs in general â€, or some participants preferred a partic u-\nlar sub-genre (e.g., P1 likes country rock, P4 likes swing \njazz, P6 likes alt ro ck) and seeing songs  that fit those crit e-\nria on the playlist made them respond more positively.  \nSometimes userâ€™s preference overrode the similarity.    \n \nP4: I really like this playlist cause I like the music on it  \nbecause Iâ€™m more, Iâ€™m way more into the swing side of \njazz but I donâ€™t really think any of these songs go with the \nseed song very much .  \n \nWhile explaining the characteristics of their ideal playlist,  \na number of participants (P1, P4, P8) specifically said that \nthey want to see more songs by the seed artist . Currently in \nAMS task, the seed artist is filtered from the results.  \n \nP4: I never seem to get in your playlist the same artist \nas your seed song, what is up with that?  Obviously I like \nthis artist, why do you not intersperse more of that artist?   \n \nParticipants also had strong reactions to the music that \nthey hated  as discussed in [3] . Having even a single  song \nthat they dislike significantly affected how they rate d the \nplaylist as a whole. For instance, P3 and P4  who listened to \nthe jazz s et commented that one of the songs  sounded like  \nelevator music  which made them absolutely hate the list .  \n \nP3: Oh, god, elevator music . I loathe elevator music.  \nP4: This is like elevator music . Itâ€™s too  early in the \nmorning for this...and Iâ€™m offended that  like elevator m u-\nsic is associated with Jazz.  There is kind of a bit of an o f-\nfensive thing  going on there. You put in Jazz and you get \nlike, is that what people think?  This gets a zero. \n \nThis suggests  that perhaps providing a way to perm a-\nnently ba n a song , like in  systems like Musicovery , is i m-\nportant to users. 4  participants (P3, P4, P7, P8) specifically \nsaid that they  want to be able to remove songs that they \nhate so that it will never appear in any of their playlists. P3 \nsaid â€œIâ€™m not sure what I like, b ut I know for sure what I \nDON â€™T like .â€ P8 said â€œall the songs have to be able to \nstand by itself â€ without â€œthe second rate songs .â€  \n P8: (answering the question â€œAnything that this ideal \nplaylist should not have?â€) Van Halen. It shouldnâ€™t have \nmusic that s ucks .  \n3.2.4 Users like learning new things, but they still want \nthem contextualized in familiar  territory  \nSeveral participants stated  that they like learn ing something \nnew and be ing exposed to new songs by listening to these \nplaylists.  \n \n     P3: I kind of want the system that educates me . You \nknow, that picks things that I don â€™t really know about . \nP7: The Van Halen  cover of the Who. I had no idea that \nsong existed so that â€™s (good) , I love learning things , thatâ€™s \nwhy I go to things like Pandora .  \n \nFinding new songs by a known artist was also a positive \nthing , like P8 who was happy to learn one of the Foo Figh t-\ners songs. P7 said re -discovering songs that were once f a-\nmiliar but forgotten was also a positive experience. Another \nnotable pattern was that  all par ticipants wanted a mix of \nfamiliar and new songs, although there w ere disagreement s \non the ideal proportion of familiar and new songs. Field s [5] \nalso advocates a playlist (familiar  songs) with recomme n-\ndation (new songs) , although playlists  are typically d istin-\nguished from recommendations [4]. Having familiar  and \nnew songs together on the list can perhaps  help users by \nenabling them to establish the context, understand the co n-\nnections between the songs better and remember the new \nsongs better.  \n \nP6: A mix of things is good, cause I would like to disco v-\ner new artists , itâ€™s always a good way to  (be) introduced \nthrough somewhat similar artists you already like â€¦Iâ€™ll \nfeel more comfortable get ting into the genre  if there â€™s a \nfew (songs) I kind of knew , and then I could kind a deter-\nmine my likes from there.   \n3.2.5 Users tend to be  more generous for unfamiliar music  \nParticipants also reacted differently to the genre based on \ntheir familiarity. Better f amiliarity with the genre seemed to \nlead to stronger criticisms  and disappointment, higher e x-\npectation, and more intense reaction. It also led to lower \nratings  overall, compared to the playlists in a genre that \nparticipants were less familiar with. For instance, when \nevaluating the same electronic playlist , P8 thought  they all \nsounded okay and similar enough to the seed  whereas P1  \npoint ed out the mix of different sub -genres and gave lower \nscores.  The participants, in fact, were aware of this beha v-\nior themselves.  \n \nP8: I feel like  I can learn about the genres that I donâ€™ t \nknow much about â€¦so Iâ€™m way more likely to just sort of \n11212th International Society for Music Information Retrieval Conference (ISMIR 2011)\n  \n \ngo along, go with the flow and see what I learn  whereas \nusually if Iâ€™m listening to something  Iâ€™m familiar with â€¦I \nwant the songs to have like consistent values , production \nvalues, song writing values, um, Iâ€™m just way pickier.     \nP2: Because I know this genre so well, I feel like I be-\ncome pickier , yeah, my expectations are higher.  \n3.2.6 Users  know and will tell you  about  their boundaries   \nOverall, participants tended to be very aware and assertive \nabout their boundaries : what they like and dislike , how \nmuch variation they can tolerate , and other little quirks . For \ninstance, both P5 and P8 said they did not like songs that \nwere anti-social . P4 said the mix of vocal and instrumental \nmusic was intolerable .  \n \nP4:  That third choral thing has to go. That was wrong . I \nliked the rest of the playlist but that one just â€¦I couldn â€™t \nsee why it came up, I really didn â€™t like i tâ€¦To me they \nwould be different channels, different categories .  \n \nParticipants  also had different reaction s to the rando m-\nness of the playlists. P5 said â€œI didn â€™t like the song in fo r-\neign, Northern European ? Language because I couldn â€™t \nfigure out what it was a boutâ€ although P3 liked the ra n-\ndomness of the German jazz song .   \n \nP3: That was the most bizarre combination â€¦ it would \nbe kind of fun  to be sort of given this and just be playing it \nin the car thinking oh I wonder whatâ€™s gonna happen \nnext... this is like a  weird mystery gift , you know, like the \nChristmas present from your mad auntie.  \n4. IMPLICATIONS  ON SIMILARITY EVALUA TION  \nBased on our interview data, we have four recommend a-\ntions for possibly improving  the AMS task in MIREX.  The \nformer two recommendations aim to facilitate obtaining \nmore objective results, and the latter  two are for making the \nAMS task more user -centric.    \n4.1 Specif ication of Features  \nOur participants considered a variety of features when they \nevaluate d the playlists. Examples of commented features \ninclude mood, genre, lyrical content, tempo, instrument a-\ntion, delivery , time period, style, and so on. They assumed \nthat the songs on the playlists were selected because of \nsome combination of these different features , although u n-\ncertain as to whi ch exact features were used . This multi -\nfaceted notion of music similarity makes it difficult to eva l-\nuate similarity since there are so many different ways two \nmusic clips can be similar  (c.f., [8]) . In MIREX, we cu r-\nrently collect evaluatorsâ€™  opinion s on how similar the qu e-\nry-candidate pairs are , but not on which aspects they \nthought were similar. One possible way to remedy  this lim i-tation would be to inform  the evaluators which aspect s they \nshould focus on  during the evaluation in order  to obtain \nmore obje ctive results. Another measure would be to ask \nthe evaluators to tell us which  aspect s made them think the \nresults were similar or not. Although t his proposed solution  \nmay slightly increase users â€™ burden, we will be able to o b-\ntain more objec tive judgments as well as richer information \non the relative importance of features for users.   \n4.2 Identification of E valuator â€™s Genre P reference and \nFamiliari ty  \nCollecting information about evaluatorâ€™ s preference and \nfamiliarity may enable us to gauge how m uch we can trust \nthe response from each evaluator. As discussed in section s \n3.2.2 and 3.2.6 , participants did react differently to playlists \nof different genres based on their preference and familiarity . \nThe background knowledge of the familiar and liked genre \nallowed the participants to evaluate the playlist based on a \nlot of contextual  information  (e.g., P1: â€œ Van Halen was \nkind of like  hair metal â€ P6: â€œ if you are going to input Ni r-\nvana, maybe you want some other smaller , pacific Nort h-\nwest, grungy , 90sâ€). On the other hand, they found it diff i-\ncult to evaluate the lists if they did not know the genre very \nwell. For instance, all 3 participants (P5, P7, P8) said that \nthey listen to hip-hop but have limited knowledge of th e \ngenre which made it difficult to  evaluate the playlists.   \n4.3 Providing Metadata with the Music Clips  \nIn MIREX, human evaluators are not provided with \nmetadata such as artist or title of the music clips they eva l-\nuate. Although this will help ensure that the similarity \njudgment is strictly based on the music itself  not metadata, \nit does not reflect the real -life music experience of use rs. In \nany commercial system, the ultimate objective  is to deliver \nmusic to users  who will want to know what exactly they are \npurchasing. Even for non -commercial systems, metadata \nwill be crucial for educating the users about music. Note \nthat evalua tion of music playlist is also affected by the  \navailability of  metadata.  \n \nP1: (reacting to three songs from the same artist) I don â€™t \nknow. If you didn â€™t show me the metadata, I might not \nknow, or I might not have that kind of reaction.   \n \nAlso 4  of our participants (P1, P6, P7, P8) discussed the \nconnection of Nirvana to Foo Fighters when they evaluated \nthe rock playlists. P7 said â€œFoo Fighters is pretty obvious â€ \nand P1 said â€œit sounds different from the seed but it makes \nsense ,â€ demonstrating the importance of artist information. \nP7 also said contextual information like  â€œthe influences b e-\ntween bands â€ was important in making a good playlist . \nProviding even the basic metadata such as artist, song title \nand genre with the music clips can help users better unde r-\n113Poster Session 1\n  \n \nstand the context of music.  This is also much closer  to how \nusers will respond to music playlists in real life.    \n4.4 Tasks  Reflecting  Real-life Use Scenarios  \nThe current music similarity task relies on human evalu a-\ntors for generating ground truth;  however  it is unclear how \nthis information is going to be used in real life. Music simi-\nlarity  can be used for many user tasks other than just crea t-\ning playlists; for instance, Lee [9] discusses t he use of m u-\nsic similarity for known -item searches (e.g., trying to find a \nspecific song by providing other song titles that sound very \nsimilar)  on Google Answers . In this case, we suspect that  \ncandidates with higher similarity scores may be more useful \nfor the user task.  \nWe believe it is crucial that the MIR community as a \nwhole think about how the current tasks can be evolved into \ntasks that are more user -centric, in other words, closer  to \nthe user tasks that actually happen in their everyday life. \nOne possibility for evolving the current AMS task is to cr e-\nate different sub-tasks that use music similarity; for i n-\nstance, playlist generation task, known -item search task, \npersonal music collection management task , and so on.  \n5. CONCLUSION AND FUTURE  WORK  \nThe findings of our study suggest that  similarity is only one \nof the many factors that affect how people feel about \nplaylists . Although similarity does seem to affect the user \nrating of playlist, stronger  similarity do es not always  make \nbetter playlists for users. Overall, participants had fairly \nclear ideas about what they expe ct from a good playlist and \nwere able to articulate them . Their e valuation of playlists \ntended to be quite  subjective as they were highly affected \nby personal preference and familiarity with the music on \nthe list, although some common themes emerged , such as \nwanting a mix of familiar and new songs, more songs from \nthe seed artist , etc. Many of the user behaviors o bserved \nduring the interview s confirm and support various points \nthat were raised in previous literature on music sim ilarity, \nrecommendation , playlists, and evalua tion. This  is promi s-\ning as there do seem to be a set of features that we can i m-\nplement in current systems to make them more user -centric.  \nWe hope that findings  from this study will provide us e-\nful information for redesigning the current AMS task  and \nencourage the MIR community to think about how to \nevolve the current evaluation tasks.  In our future studies, \nwe plan to test more playlists generated by different alg o-\nrithms submitted to MIREX based on different set of seed \nsongs. Ins tead of researchers selecting random songs for \nusers to test, we plan to have the users select the seed songs \nthat they actually like and are more likely to use for elici t-\ning playlists in real life.  6. REFERENCES  \n[1] P. Ã…man and L. A. Liikkanen: â€œA survey of mus ic \nrecommendation aids ,â€ Proceedings of the Workshop \non Music Recommendation and Discovery , 2010.  \n[2] L. Barrington, R. Oda, and G. Lanckriet:  â€œSmarter \nthan genius? human evaluation of music recommender \nsystems ,â€ Proceedings of ISMIR, pp. 357 -362, 2009.  \n[3] S. J. Cunningham, D. Bainbridge, and A. Falconer: \nâ€œMore of an art than a science: supporting the creation \nof playlists and mixes ,â€ Proceedings of the ISMIR, pp. \n240-245, 2006.  \n[4] B. Fields and P. Lamere: â€œFinding a path through the \njuke box ,â€ tutorial p resent ed at the ISMIR , 2010.  \n[5] B. Fields, C. Rhodes, and M. d â€™Inverno: â€œUsing song \nsocial tags and topic models to describe and compare \nplaylists ,â€ Proceedings of WOMRA D, 2010.  \n[6] B. Glaser and A. Strauss: The discovery of grounded \ntheory: strategies for qualitative  research , Chicago, \n1967.  \n[7] IMIRSEL: â€œ2010 AMS  results, â€ http://www.  music -\nir.org/mirex/wiki/2010:Audio_Music_Similarity_and_\nRetrieval_Results , 2010.  \n[8] M. C. Jones, J. S. Downie, A. F. Ehmann: â€œHuman \nsimilarity judgments: implications for the design of \nformal evaluations, Proceedings of the ISMIR , pp. \n539-542, 200 7. \n[9] J. H. Lee: â€œAnalysis of user needs and information \nfeatures in natural language queries seeking music \ninformation ,â€ JASIS&T , 61( 5), pp. 1025 -1045, 2010.  \n[10] B. Logan: â€œContent -based playlist generation: \nexploratory experiments . Proceedings of the ISMIR, \npp. 295-296, 200 2. \n[11] S. Pauws and B. Eggen: â€œPats: realization and user \nevaluation of an automatic playlist generator ,â€ \nProceedings of the ISMIR , pp. 222-230, 2002.  \n[12] R. Ragno , C. J. C. Burges, and C. Herley: â€œInferring \nsimilarity between music objects with application to \nplaylist generation ,â€ Proceedings of the 7th ACM \nSIGMM MIR, pp. 73 -80, 2005  \n[13] R. Sinha and K. Swearingen: â€œThe role of \ntransparency in recommender systems ,â€ Proceedings \nof the ACM CHI , pp. 830 -831, 2002.  \n[14] M. Slaney: â€œWeb -scale multimedia analysis: does \ncontent matter ?â€ IEEE Multimedia , Vol. 18, No. 2. pp. \n12-15, 2011.  \n114"
    },
    {
        "title": "Rhythm Extraction from Polyphony Symbolic Music.",
        "author": [
            "Florence LevÃ©",
            "Richard Groult",
            "Guillaume Arnaud",
            "Cyril SÃ©guin",
            "RÃ©mi Gaymay",
            "Mathieu Giraud"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1416368",
        "url": "https://doi.org/10.5281/zenodo.1416368",
        "ee": "https://zenodo.org/records/1416368/files/LeveGASGG11.pdf",
        "abstract": "In this paper, we focus on the rhythmic component of symbolic music similarity, proposing several ways to extract a monophonic rhythmic signature from a symbolic polyphonic score. To go beyond the simple extraction of all time intervals between onsets (noteson extraction), we select notes according to their length (short and long extractions) or their intensities (intensity+/âˆ’extractions). Once the rhythm is extracted, we use dynamic programming to compare several sequences. We report results of analysis on the size of rhythm patterns that are specific to a unique piece, as well as experiments on similarity queries (ragtime music and Bach chorale variations). These results show that long and intensity+ extractions are often good choices for rhythm extraction. Our conclusions are that, even from polyphonic symbolic music, rhythm alone can be enough to identify a piece or to perform pertinent music similarity queries, especially when using wise rhythm extractions.",
        "zenodo_id": 1416368,
        "dblp_key": "conf/ismir/LeveGASGG11",
        "keywords": [
            "rhythmic component",
            "symbolic music similarity",
            "monophonic rhythmic signature",
            "noteson extraction",
            "short and long extractions",
            "intensity+/âˆ’extractions",
            "dynamic programming",
            "rhythm patterns",
            "unique piece",
            "music similarity queries"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nRHYTHM EXTRACTION FROM POLYPHONIC SYMBOLIC MUSIC\nFlorence Lev Â´e, Richard Groult, Guillaume Arnaud, Cyril S Â´eguin\nMIS, Universit Â´e de Picardie Jules Verne, AmiensRÂ´emi Gaymay, Mathieu Giraud\nLIFL, Universit Â´e Lille 1, CNRS\nABSTRACT\nIn this paper, we focus on the rhythmic component of sym-\nbolic music similarity, proposing several ways to extract\na monophonic rhythmic signature from a symbolic poly-\nphonic score. To go beyond the simple extraction of all\ntime intervals between onsets ( noteson extraction), we se-\nlect notes according to their length ( short andlong extrac-\ntions) or their intensities ( intensity+/âˆ’extractions). Once\nthe rhythm is extracted, we use dynamic programming to\ncompare several sequences. We report results of analysis on\nthe size of rhythm patterns that are speciï¬c to a unique piece,\nas well as experiments on similarity queries (ragtime music\nand Bach chorale variations). These results show that long\nandintensity+extractions are often good choices for rhythm\nextraction. Our conclusions are that, even from polyphonic\nsymbolic music, rhythm alone can be enough to identify a\npiece or to perform pertinent music similarity queries, espe-\ncially when using wise rhythm extractions.\n1. INTRODUCTION\nMusic is composed from rhythm, pitches, and timbres, and\nmusic is played with expression and interpretation. Omit-\nting some of these characteristics may seem unfair. Can the\nrhythm alone be representative of a song or a genre?\nSmall rhythmic patterns are essential for the balance of\nthe music, and can be a way to identify a song. One may\nï¬rst think of some clich Â´es: start of Beethoven 5th symphony ,\ndrum pattern from We will rock you or Ravelâ€™s BolÂ´ero. More\ngenerally, Query By Tapping (QBT) studies, where the user\ntaps on a microphone [10,12], are able in some situations to\nidentify a monophonic song. On a larger scale, musicolo-\ngists have studied how rhythm, like tonality, can structure a\npiece at different levels [5, 16].\nThis article shows how simple extractions can, starting\nfrom a polyphony, build relevant monophonic signatures,\nbeing able to be used for the identiï¬cation of songs or for\nthe comparison of whole pieces.\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proï¬t or commercial advantage and that copies\nbear this notice and the full citation on the ï¬rst page.\nc/circlecopyrt2011 International Society for Music Information Retrieval.In fact, most rhythm-only studies in Music Information\nRetrieval (MIR) concern audio signal . These techniques of-\nten rely in detection of auto-correlations in the signal. Some\nstudies output descriptors [9, 15, 17] that can be used for\nfurther retrieval or classiï¬cation. Several papers focus on\napplications of non-Western music [11, 13, 24].\nThere are other tools that mix audio with symbolic data ,\ncomparing audio signals against symbolic rhythmic pattern.\nFor example, the QBT wave task of MIREX 2010 proposed\nthe retrieval of monophonic MIDI ï¬les from wave input\nï¬les. Some solutions involve local alignments [10]. Another\nproblem is rhythm quantization, for example when aligning\naudio from music performances against symbolic data. This\ncan be solved with probabilistic frameworks [2]. Tempo and\nbeat detection are other situations where one extracts sym-\nbolic information from audio data [7, 18].\nSome rhythm studies work purely on symbolic MIDI data,\nbut where the input is not quantized [22], as in the QBT\nsymbolic task in MIREX 2010. Again, challenges can come\nfrom quantization, tempo changing and expressive interpre-\ntations. Finally, on the side of quantized symbolic music ,\nthe Mongeau and Sankoff algorithm takes into account both\npitches and rhythms [14]. Extensions concerning polyphony\nhave been proposed [1]. Other symbolic MIR studies focus\non rhythm [3, 4, 19â€“21].\nHowever, as far as we know, a framework for rhythmic\nextraction from polyphonic symbolic music has never been\nproposed. Starting from a polyphonic symbolic piece, what\nare the pertinent ways to extract a monophonic rhythmic\nsequence? Section 2 presents comparison of rhythmic se-\nquences through local alignment, Section 3 proposes dif-\nferent rhythm extractions, and Section 4 details evaluations\nof these extractions for the identiï¬cation of musical pieces\nwith exact pattern matching (Section 4.2) and on similarity\nqueries between complete pieces (Sections 4.3 and 4.4).\n2. RHYTHM COMPARISONS\n2.1 Representation of monophonic rhythm sequences\nFor tempo-invariance, several studies on tempo or beat track-\ning on audio signal use relative encoding [10]. As we start\nfrom symbolic scores, we suppose here that the rhythms are\nalready quantized on beats, and we will not study tempo and\n375Poster Session 3\nmeter parameters. If necessary, multiple queries handle the\ncases where the tempo is doubled or halved.\nRhythm can be represented in different ways. Here, we\nmodel each rhythm as a succession of durations between\nnotes, i.e. inter-onset intervals measured in quarter notes or\nfractions of them (Figure 1).\n /noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2\nMusic engraving by LilyPond 2.12.3â€”www.lilypond.org\nFigure 1 . The monophonic rhythm sequence (1, 0.5, 0.5, 2).\nThus, in this simple framework, there are no silences,\nsince each note, except the last one, is considered until the\nbeginning of the following note.\n2.2 Monophonic rhythm comparison\nSeveral rhythm comparisons have been proposed [21]. Here,\nwe compare rhythms while aligning durations. Let S(m,n)\nbe the best score to locally align a rhythm sequence x1...xm\nto another one y1...yn. This similarity score can be com-\nputed via a dynamic programming equation (Figure 2), by\ndiscarding the pitches in the Mongeau-Sankoff equation [14].\nThe alignment can then be retrieved through backtracking in\nthe dynamic programming table.\nS(a,b) = maxï£±\nï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£²\nï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£³S(aâˆ’1,bâˆ’1) +Î´(xa,yb)\n(match, substitution s)\nS(aâˆ’1,b) +Î´(xa,âˆ…)\n(insertion i)\nS(a,bâˆ’1) +Î´(âˆ…,yb)\n(deletion d)\nS(aâˆ’k,bâˆ’1) +Î´({xaâˆ’k+1...xa},yb)\n(consolidation c)\nS(aâˆ’1,bâˆ’k) +Î´(xa,{ybâˆ’k+1...yb})\n(fragmentation f)\n0 (local alignment)\nFigure 2 . Dynamic programming equation for ï¬nding the\nscore of the best local alignment between two monophonic\nrhythmic sequences x1...xaandy1...yb.Î´is the score\nfunction for each type of mutation. The complexity of com-\nputingS(m,n)isO(mnk), wherekis the number of al-\nlowed consolidations and fragmentations.\nThere can be a match or a substitution (s) between two\ndurations, an insertion (i) or a deletion (d) of a duration.\nThe consolidation (c) operation consists in grouping sev-\neral durations into a unique one, and the fragmentation (f)\nin splitting a duration into several ones (see Figure 3).\n /noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s1|/noteheads.s2/noteheads.s2/noteheads.s2c/noteheads.s2/flags.u3/noteheads.s2/noteheads.s2/noteheads.s2||/noteheads.s2/noteheads.s2|/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2|/noteheads.s2/noteheads.s2|/noteheads.s2/noteheads.s2f/noteheads.s2d|noteson long/noteheads.s2/noteheads.s2/noteheads.s2(3299) (1465) a./noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2(396) (279) b./noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/flags.u3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2(143) (114) c./noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/noteheads.s2(43) (53) d./noteheads.s2/noteheads.s2/dots.dot/noteheads.s2/flags.u3/noteheads.s2/flags.u3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s2(7) (18) e./noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s1/noteheads.s2/flags.u3/noteheads.s2/noteheads.s2/noteheads.s2(6) (10) f./noteheads.s2/noteheads.s2/flags.u3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/flags.u3/noteheads.s2/noteheads.s2/dots.dot/noteheads.s2/flags.u3/noteheads.s2(0) (1) g./flags.u3/noteheads.s2/flags.u3/noteheads.s2/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/flags.u3/noteheads.s2/noteheads.s2\nMusic engraving by LilyPond 2.12.3â€”www.lilypond.orgFigure 3 . Alignment between two rhythm sequences.\nMatches, consolidations and fragmentations respect the\nbeats and the strong beats of the measure, whereas substitu-\ntions, insertions and deletions may alter the rhythm structure\nand should be more highly penalized. Scores will be eval-\nuated in Section 4 where it is conï¬rmed that, most of the\ntime, the best results are obtained when taking into account\nconsolidation and fragmentation operations.\n3. RHYTHM EXTRACTION\nHow can we extract, from a polyphony, a monophonic rhyth-\nmic texture? In this section, we propose several rhythmic\nextractions. Figure 4 presents an example applying these\nextractions on the beginning of a chorale by J.-S. Bach.\nThe simplest extraction is to consider all onsets of the\nsong, reducing the polyphony to a simple combined mono-\nphonic track. This â€œ noteson extractionâ€ extracts durations\nfrom the inter-onset intervals of all consecutive groups of\nnotes. For each note or each group of notes played simul-\ntaneously, the considered duration is the time interval be-\ntween the onset of the current group of notes and the fol-\nlowing onset. Each group of notes is taken into account\nand is represented in the extracted rhythmic pattern. How-\never, such a noteson extraction is not really representative of\nthe polyphony: when several notes of different durations are\nplayed at the same time, there may be some notes that are\nmore relevant than others.\nIn symbolic melody extraction, it has been proposed to\nselect the highest (or the lowest) pitch from each group of\nnotes [23]. Is it possible to have similar extractions when\none considers the rhythms? The following paragraphs intro-\nduce several ideas on how to choose onsets and durations\nthat are most representative in a polyphony. We will see in\nSection 4 that some of these extractions bring a noticeable\nimprovement to the noteson extraction.\n3.1 Considering length of notes: long,short\nFocusing on the rhythm information, the ï¬rst idea is to take\ninto account the effective lengths of notes. At a given onset,\nfor a note or a group of notes played simultaneously:\nâ€¢in the long extraction, all events occurring during the\nlength of the longest note are ignored. For example,\nas there is a quarter on the ï¬rst onset of Figure 4, the\nsecond onset (eighth, tenor voice) is ignored;\n37612th International Society for Music Information Retrieval Conference (ISMIR 2011)\n/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2intensity+ /accidentals.sharp/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.sharp/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/brackettips.up/brackettips.down/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/flags.u3/flags.d3/accidentals.sharp\nintensityâ€“ /flags.u3/flags.d3\nlong /noteheads.s2/scripts.ufermata/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.sharp/timesig.C44/timesig.C44/timesig.C44/timesig.C44/clefs.G/accidentals.sharp/timesig.C44/clefs.G/accidentals.sharp/timesig.C44/clefs.G/accidentals.sharp/timesig.C44/clefs.F/accidentals.sharp/timesig.C44/timesig.C44/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/flags.u3/accidentals.sharp/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/scripts.ufermata/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/flags.u3/noteheads.s2/scripts.ufermata/noteheads.s2/scripts.ufermata/dots.dotnoteson /accidentals.sharpshort /noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.3/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s2/accidentals.sharp/dots.dot\n/flags.u3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.sharp/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/flags.u3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.sharp/flags.u3/accidentals.sharp/accidentals.sharp/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/flags.u3/flags.u3Figure 4 . Rhythm extraction on the beginning of the Bach\nchorale BWV 278.\nâ€¢similarly, for the short extraction, all events occur-\nring during the length of the shortest note are ignored.\nThis extraction is often very close to the noteson ex-\ntraction.\nIn both cases, as some onsets may be skipped, the con-\nsidered duration is the time interval between the onset of\nthe current group of notes and the following onset that is\nnot ignored. Most of the time, the short extraction is not\nvery different from the noteson , whereas the long extraction\nbrings signiï¬cant gains in similarity queries (see Section 4).\n3.2 Considering intensity of onsets: intensity+/âˆ’\nThe second idea is to consider a ï¬lter on the number of notes\nat the same event, keeping only onsets with at least knotes\n(intensity+) or strictly less than knotes ( intensityâˆ’), where\nthe threshold kis chosen relative to the global intensity of\nthe piece. The considered durations are then the time inter-\nvals between consecutive ï¬ltered groups. Figure 4 shows an\nexample with k= 3. This extraction is the closest to what\ncan be done on audio signals with peak detection.\n4. RESULTS AND EV ALUATION\n4.1 Protocol\nStarting from a database of about 7000 MIDI ï¬les (including\n501 classical, 527 jazz/latin, 5457 pop/rock), we selected\nthe quantized ï¬les by a simple heuristic (40 % of onsets on\nbeat, eighth or eighth tuplet). We thus kept 5900 MIDI ï¬les\nfrom Western music, sorted into different genres (including\n204 classical, 419 jazz/latin, 4924 pop/rock). When applica-\nble, we removed the drum track (MIDI channel 10) to avoid\nour rhythm extractions containing too many sequences of\neighth notes, since drums often have a repetitive structure in\npopular Western music. Then, for each rhythm extractionpresented in the previous section, we extracted all database\nï¬les. For each ï¬le, the intensity+/âˆ’thresholdkwas choosen\nas the median value between all intensities. For this, we\nused the Python framework music21 [6].\nOur ï¬rst results are on Exact Song identiï¬cation (Sec-\ntion 4.2). We tried to identify a song by a pattern of several\nconsecutive durations taken from a rhythm extraction, and\nlooked for the occurrences of this pattern in all the songs of\nthe database.\nWe then tried to determinate if these rhythm extractions\nare pertinent to detect similarities. We tested two particular\ncases, Ragtime (Section 4.3) and Bach chorales variations\n(Section 4.4). Both are challenging for our extraction meth-\nods, because they present difï¬culties concerning polyphony\nand rhythm: Ragtime has a very repetitive rhythm on the\nleft hand but a very free right hand, and Bach chorales have\nrhythmic differences between their different versions.\n4.2 Exact Song Identiï¬cation\nIn this section, we look for patterns of consecutive notes\nthat are exactly matched in only one ï¬le among the whole\ndatabase. For each rhythm extraction and for each length be-\ntween 5and50, we randomly selected 200 distinct patterns\nappearing in the ï¬les of our database. We then searched for\neach of these patterns in all the 5900 ï¬les (Figure 5).\n 1 10 100 1000\n 5  10  15  20  25  30  35Matching files (among 5900)\nPattern lengthnoteson\nshort\nlong\nintensity-\nintensity+\nFigure 5 . Number of matching ï¬les for patterns between\nlength 5 and 35. Curves with points indicate median values,\nwhereas other curves indicate average values.\nWe see that as soon as the length grows, the patterns are\nvery speciï¬c. For lengths 10, 15 and 20, the number of pat-\nterns (over 200) matching one unique ï¬le is as follows:\nExtraction 10 notes 15 notes 20 notes\nnoteson 49 85 107\nshort 58 100 124\nlong 85 150 168\nintensity+91 135 158\nintensityâˆ’109 137 165\n377Poster Session 3\nWe notice that the long andintensity+/âˆ’extractions are\nmore speciï¬c than noteson . From 12notes, the median val-\nues of Figure 5 are equal to 1 except for noteson andshort\nextractions. In more than 70% of these queries, 15 notes are\nsufï¬cient to retrieve a unique ï¬le.\nThe results for average values are disturbed by a few pat-\nterns that match a high number of ï¬les. Figure 6 displays\nsome noteworthy patterns with 10 notes. Most of the time,\nthe patterns appearing very frequently are repetitions of the\nsame note, such as pattern (a). With long extraction, 174\nï¬les contain 30 consecutive quarters, and 538 ï¬les contain\n30 consecutive eighths. As these numbers further increase\nwith noteson (and short ) extractions, this explains why the\nlong extraction can be more speciï¬c.\n /noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2noteson long/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2(3299) (1465) a./noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2(396) (279) b./noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/flags.u3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2(143) (114) c./noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/noteheads.s2(43) (53) d./noteheads.s2/noteheads.s2/dots.dot/noteheads.s2/flags.u3/noteheads.s2/flags.u3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s2(7) (18) e./noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s1/noteheads.s2/flags.u3/noteheads.s2/noteheads.s2/noteheads.s2(6) (10) f./noteheads.s2/noteheads.s2/flags.u3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/flags.u3/noteheads.s2/noteheads.s2/dots.dot/noteheads.s2/flags.u3/noteheads.s2(0) (1) g./flags.u3/noteheads.s2/flags.u3/noteheads.s2/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/flags.u3/noteheads.s2/noteheads.s2\nMusic engraving by LilyPond 2.12.3â€”www.lilypond.org\nFigure 6 . Some patterns with 10 durations, with the number\nof matching ï¬les in noteson andlong extractions.\nThe number of occurrences of each pattern is mostly de-\ntermined by its musical relevance. For example, in a pat-\ntern with three durations, (d) appears more often than (g),\nwhich is quite a difï¬cult rhythm. In the same way, among\npatterns with only quarters and eighths, (b) and (c) can be\nfound more often than (f). We also notice that patterns with\nlonger durations, even repetitive ones such as pattern (e),\ngenerally appear in general less frequently than those con-\ntaining shorter durations.\n4.3 Similarities in Ragtime\nIn this section and the following, we use the similarity score\ncomputation explained in Section 2.2. Ragtime music, one\nof the precursors of Jazz music, has a strict tempo main-\ntained by the pianistâ€™s left hand and a typical swing created\nby a syncopated melody in the right hand.\nFor this investigation, we gathered 17 ragtime ï¬les. Then\nwe compared some of these ragtime ï¬les against a set of ï¬les\ncomprising the 17 ragtime ï¬les and randomly selected ï¬les\nof the database. We tested several scores functions: always\n+1for a match, andâˆ’10,âˆ’5,âˆ’2,âˆ’1,âˆ’1/2orâˆ’1/3foran error. We further tested no penalty for consolidation and\nfragmentation (c/f).\n 0 0.2 0.4 0.6 0.8 1\n 0  0.2  0.4  0.6  0.8  1True positive rate (sensibility)\nFalse positive rate (1 - specificity)noteson [0.794]\nlong [0.861]\nFigure 7 . Best ROC Curves, with associated AUC, for re-\ntrieving 17 Ragtime pieces from the query A Ragtime Night-\nmare , by Tom Turpin, in a set of 100 ï¬les.\nFigure 7 shows ROC Curves for A Ragtime Nightmare . A\nROC Curve [8] plots sensibility (capacity to ï¬nd true posi-\ntives) and speciï¬city (capacity to eliminate false positives)\nover a range of thresholds, giving a way to ascertain the per-\nformance of a classiï¬er that outputs a ranked list of results.\nHere one curve represents one rhythm extraction with one\nscore function. For each score function, we computed the\ntrue positive and the false positive rates according to all dif-\nferent thresholds. The long extraction, used with scores +1\nfor a match andâˆ’1for all errors, gives here very good re-\nsults: for example, the circled point on Figure 7 corresponds\nto 0.88 sensitivity and 0.84 speciï¬city with a threshold of 45\n(i.e. requiring at least 45 matches).\nConsidering the whole curve, the performance of such a\nclassiï¬er can be measured with the AUC (Area Under ROC\nCurve). Averaging on 9 different queries, the best set of\nscores for each extraction is as follows:\nExtraction Scores Mean AUC\ns/i/d c/f match\nnotesonâˆ’5 0 +1 0.711\nshortâˆ’1âˆ’1 +1 0.670\nlongâˆ’1 0 +1 0.815\nintensity+âˆ’1/3 0 +1 0.622\nintensityâˆ’âˆ’1âˆ’1 +1 0.697\nMost of the time, the matching sequences are long se-\nquences of eighths, similar to pattern (a) of Figure 6. If such\npatterns are frequent in noteson database ï¬les (see previous\nsection), their presence in long ï¬les is more frequent in Rag-\ntime than in other musical styles. For example, pattern (a)\nis found in 76 % of Ragtime long extractions, compared to\nonly 25 % of the whole database.\nIndeed, in ragtime scores, the right hand is very swift\nand implies a lot of syncopations, while the left hand is bet-\n37812th International Society for Music Information Retrieval Conference (ISMIR 2011)\n /noteheads.s2/noteheads.s2/noteheads.s2/rests.4/noteheads.s2/flags.u3/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/flags.u3/accidentals.flat/accidentals.flat/accidentals.flat/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s242424242/accidentals.flat/accidentals.flat/accidentals.flat/accidentals.flat/clefs.F42/accidentals.flat/accidentals.flat/accidentals.flat/accidentals.flat/clefs.G/rests.4/dots.dot/dots.dot/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/flags.u3/dots.dot/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.4/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2\nintensity+ long noteson /brackettips.up/brackettips.down/accidentals.natural/accidentals.natural/accidentals.natural/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/flags.u3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.4/noteheads.s2/noteheads.s2/flags.u3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2\nMusic engraving by LilyPond 2.12.3â€”www.lilypond.orgFigure 8 .Possum Rag (1907), by Geraldine Dobyns.\nter structured. Here the syncopations are not taken into ac-\ncount in the long extraction, and the left hand (often made of\neighths, as in Figure 8) is preserved during long extractions.\nFinally, intensity+does not give good results here (un-\nlike Bach Chorales, see next Section). In fact, intensity+\nextraction keeps the syncopation of the piece, as accents in\nthe melody often involve chords that will pass through the\nintensity+ï¬lter (Figure 8, last note of intensity+).\n4.4 Similarities in Bach Chorales Variations\nSeveral Bach chorales are variations of each other, shar-\ning an exact or very similar melody. Such chorales present\nmainly variations in their four-part harmony, leading to dif-\nferences in their subsequent rhythm extractions (Figure 9).\nBWV 4.8/flags.u3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2BWV 158.4/dots.dot/timesig.C44/accidentals.sharp/clefs.G/noteheads.s2BWV 278/timesig.C44/timesig.C44/timesig.C44/timesig.C44/flags.u3/flags.u3/noteheads.s2/noteheads.s2/accidentals.sharp/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/scripts.ufermata/noteheads.s2BWV 277/dots.dot/accidentals.sharp/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/flags.u3/accidentals.sharp/noteheads.s2/noteheads.s2/noteheads.s2\nFigure 9 . Extraction of long rhythm sequences from differ-\nent variations of the start of the chorale Christ lag in Todes-\nbanden . The differences between variations are due to dif-\nferences in the rhythms of the four-part harmonies.\nFor this investigation, we considered a collection of 404\nBach chorales transcribed by www.jsbchorales.net\nand available in the music21 corpus [6]. We selected 5\nchorales that have multiple versions: Christ lag in Todes-\nbanden (5 versions, including a perfect duplicate), Wer nun\nden lieben Gott (6 versions), Wie nach einer Wasserquelle\n(6 versions), Herzlich tut mich verlangen (9 versions), and\nO Welt, ich muss dich lassen (9 versions).For each chorale, we used one version to query against\nthe set of all other 403 chorales, trying to retrieve the most\nsimilar results. A ROC curve with BWV 278 as a query is\nshown in Figure 10. For example, with intensity+extraction\nand scoresâˆ’1fors/i/d , 0 forc/f, and +1for a match,\nthe circled point corresponds to a threshold of 26, with 0.80\nsensitivity and 0.90 speciï¬city. Averaging on all 5 chorales,\nthe best set of scores for each extraction is as follows:\nExtraction Scores Mean AUC\ns/i/d c/f match\nnotesonâˆ’1 0 +1 0.769\nshortâˆ’1 0 +1 0.781\nlongâˆ’5âˆ’5 +1 0.871\nintensity+âˆ’1 0 +1 0.880\nintensityâˆ’âˆ’5 0 +1 0.619\nEven if the noteson extractions already gives good results,\nlong andintensity+bring noteworthy improvements. Most\nof the time, the best scores correspond to alignments be-\ntween 8 and 11 measures, spanning a large part of the cho-\nrales. We thus managed to align almost globally one chorale\nand its variations. We further checked that there is not a bias\non total length: for example, BWV 278 has a length of ex-\nactly 64 quarters, as do 15% of all the chorales, but the score\ndistribution is about the same in these chorales than in the\nother ones.\n 0 0.2 0.4 0.6 0.8 1\n 0  0.2  0.4  0.6  0.8  1True positive rate (sensibility)\nFalse positive rate (1 - specificity)noteson [0.636]\nlong [0.787]\nintensity [0.904]\nFigure 10 . Best ROC Curves, with associated AUC, for\nretrieving all 5 versions of Christ lag in Todesbanden from\nBWV 278 in a set of 404 chorales.\n5. DISCUSSION\nIn all our experiments, we showed that several methods are\nmore speciï¬c than a simple noteson extraction (or than the\nsimilar short extraction). The intensityâˆ’extraction could\nprovide the most speciï¬c patterns used as signature (see Fig-\nure 5), but is not appropriate to be used in similarity queries.\nThelong andintensity+extractions give good results in the\nidentiï¬cation of a song, but also in similarity queries inside\na genre or variations of a music.\n379Poster Session 3\nIt remains to measure what is really lost by discarding\npitch information: our perspectives include the comparison\nof our rhythm extractions with others involving melody de-\ntection or drum part analysis.\nAcknowledgements. The authors thank the anonymous referees for\ntheir valuable comments. They are also indebted to Dr. Amy Glen\nwho kindly read and corrected this paper.\n6. REFERENCES\n[1] Julien Allali, Pascal Ferraro, Pierre Hanna, Costas Il-\niopoulos, and Matthias Robine. Toward a general frame-\nwork for polyphonic comparison. Fundamenta Infor-\nmaticae , 97:331â€“346, 2009.\n[2] A. T. Cemgil, P. Desain, and H. J. Kappen. Rhythm\nQuantization for Transcription. Computer Music Jour-\nnal, 24:2:60â€“76, 2000.\n[3] J. C. C. Chen and A. L. P. Chen. Query by rhythm:\nAn approach for song retrieval in music databases. In\nProceedings of the Workshop on Research Issues in\nDatabase Engineering , RIDE â€™98, pages 139â€“, 1998.\n[4] Manolis Christodoulakis, Costas S. Iliopoulos, Moham-\nmad Sohel Rahman, and William F. Smyth. Identifying\nrhythms in musical texts. Int. J. Found. Comput. Sci. ,\n19(1):37â€“51, 2008.\n[5] Grosvenor Cooper and Leonard B. Meyer. The Rhythmic\nStructure of Music . University of Chicago Press, 1960.\n[6] Michael Scott Cuthbert and Christopher Ariza. music21:\nA toolkit for computer-aided musicology and symbolic\nmusic data. In Int. Society for Music Information Re-\ntrieval Conf. (ISMIR 2010) , 2010.\n[7] Simon Dixon. Automatic extraction of tempo and beat\nfrom expressive performances. Journal of New Music\nResearch , 30(1):39â€“58, 2001.\n[8] Tom Fawcett. An introduction to ROC analysis. Pattern\nRecognition Letters , 27(8):861â€“874, 2006.\n[9] Matthias Gruhne, Christian Dittmar, and Daniel Gaert-\nner. Improving rhythmic similarity computation by beat\nhistogram transformations. In Int. Society for Music In-\nformation Retrieval Conf. (ISMIR 2009) , 2009.\n[10] Pierre Hanna and Matthias Robine. Query by tapping\nsystem based on alignment algorithm. In IEEE Int. Conf.\non Acoustics, Speech and Signal Processing (ICASSP\n2009) , pages 1881â€“1884, 2009.\n[11] Andre Holzapfel and Yannis Stylianou. Rhythmic simi-\nlarity in traditional turkish music. In Int. Society for Mu-\nsic Information Retrieval Conf. (ISMIR 2009) , 2009.[12] Jyh-Shing Jang, Hong-Ru Lee, and Chia-Hui Yeh.\nQuery by Tapping: a new paradigm for content-based\nmusic retrieval from acoustic input. In Advances in\nMultimedia Information Processing (PCM 2001), LNCS\n2195 , pages 590â€“597, 2001.\n[13] Kristoffer Jensen, Jieping Xu, and Martin Zachariasen.\nRhythm-based segmentation of popular chinese music.\nInInt. Society for Music Information Retrieval Conf. (IS-\nMIR 2005) , 2005.\n[14] Marcel Mongeau and David Sankoff. Comparaison\nof musical sequences. Computer and the Humanities ,\n24:161â€“175, 1990.\n[15] Geoffroy Peeters. Rhythm classiï¬cation using spectral\nrhythm patterns. In Int. Society for Music Information\nRetrieval Conf. (ISMIR 2005) , 2005.\n[16] Marc Rigaudi `ere. La th Â´eorie musicale germanique du\nXIXe si `ecle et lâ€™id Â´ee de coh Â´erence . 2009.\n[17] Matthias Robine, Pierre Hanna, and Mathieu Lagrange.\nMeter class proï¬les for music similarity and retrieval. In\nInt. Society for Music Information Retrieval Conf. (IS-\nMIR 2009) , 2009.\n[18] Klaus Seyerlehner, Gerhard Widmer, and Dominik\nSchnitzer. From rhythm patterns to perceived tempo. In\nInt. Society for Music Information Retrieval Conf. (IS-\nMIR 2007) , 2007.\n[19] Eric Thul and Godfried Toussaint. Rhythm complexity\nmeasures: A comparison of mathematical models of hu-\nman perception and performance. In Int. Society for Mu-\nsic Information Retrieval Conf. (ISMIR 2008) , 2008.\n[20] Godfried Toussaint. The geometry of musical rhythm. In\nJapan Conf. on Discrete and Computational Geometry\n(JCDCG 2004), LNCS 3472 , pages 198â€“212, 2005.\n[21] Godfried T. Toussaint. A comparison of rhythmic sim-\nilarity measures. In Int. Society for Music Information\nRetrieval Conf. (ISMIR 2004) , 2004.\n[22] Ernesto Trajano de Lima and Geber Ramalho. On rhyth-\nmic pattern extraction in Bossa Nova music. In Int. Soci-\nety for Music Information Retrieval Conf. (ISMIR 2008) ,\n2008.\n[23] Alexandra L. Uitdenbogerd. Music Information Re-\ntrieval Technology . PhD thesis, RMIT University, Mel-\nbourne, Victoria, Australia, 2002.\n[24] Matthew Wright, W. Andrew Schloss, and George\nTzanetakis. Analyzing afro-cuban rhythms using\nrotation-aware clave template matching with dynamic\nprogramming. In Int. Society for Music Information\nRetrieval Conf. (ISMIR 2008) , 2008.\n380"
    },
    {
        "title": "Improving Perceptual Tempo Estimation with Crowd-Sourced Annotations.",
        "author": [
            "Mark Levy"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1417583",
        "url": "https://doi.org/10.5281/zenodo.1417583",
        "ee": "https://zenodo.org/records/1417583/files/Levy11.pdf",
        "abstract": "We report the design and results of a web-based experiment intended to support the development and evaluation of tempo estimation algorithms, in which users tap to music and select descriptive labels. Analysis of the tapping data and labels chosen shows that, while different listeners frequently entrain to different metrical levels for some pieces, they rarely disagree about which pieces are fast and which are slow. We show how this result can be used to improve both the evaluation metrics used for automatic tempo estimation and the estimation algorithms themselves. We also report the relative performance of two recent tempo estimation methods according to a further controlled experiment that does not depend on groundtruth values of any kind.",
        "zenodo_id": 1417583,
        "dblp_key": "conf/ismir/Levy11",
        "keywords": [
            "web-based experiment",
            "tempo estimation algorithms",
            "tapping data",
            "descriptive labels",
            "entrainment to metrical levels",
            "listener agreement",
            "evaluation metrics",
            "automatic tempo estimation",
            "tempo estimation methods",
            "controlled experiment"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nIMPROVING PERCEPTUAL TEMPO ESTIMATION WITH\nCROWD-SOURCED ANNOTATIONS\nMark Levy\nLast.fm Ltd., Karen House, 1-11 Baches Street, London N1 6DL, United Kingdom\nmark@last.fm\nABSTRACT\nWe report the design and results of a web-based experi-\nment intended to support the development and evaluation\nof tempo estimation algorithms, in which users tap to music\nand select descriptive labels. Analysis of the tapping data\nand labels chosen shows that, while different listeners fre-\nquently entrain to different metrical levels for some pieces,\nthey rarely disagree about which pieces are fast and which\nare slow. We show how this result can be used to improve\nboth the evaluation metrics used for automatic tempo esti-\nmation and the estimation algorithms themselves. We also\nreport the relative performance of two recent tempo estima-\ntion methods according to a further controlled experiment\nthat does not depend on groundtruth values of any kind.\n1. INTRODUCTION\nNumerous algorithms for estimating the tempo of music di-\nrectly from an audio signal have been developed in recent\nyears, motivated by the obvious value of tempo informa-\ntion to automated tools for use in playlisting and DJ mix-\ning [4]. Automatic estimation of the tempo of a track as\na simple value measured in beats per minute (bpm) is now\nregarded as an established technique. Bpm estimation algo-\nrithms have gained a place in widely-distributed commercial\nhardware mixers for DJs, as well as software applications\nand web service APIs aimed at musicians, recording labels\nand mobile application developers. Meanwhile the annual\nMIREX algorithm evaluation competition offers a more for-\nmal benchmark for the performance of tempo estimation\nsoftware. This has led to the proliferation of rival meth-\nods: seven different algorithms were submitted to the audio\ntempo estimation competition during the past year alone.\nA common observation made in both informal and for-\nmal evaluation of tempo estimation methods is that they fre-\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proï¬t or commercial advantage and that copies\nbear this notice and the full citation on the ï¬rst page.\nc/circlecopyrt2011 International Society for Music Information Retrieval.quently suffer from so-called octave error , where the ma-\nchine estimate is some simple multiple or fraction of the\nperceived tempo [5, 10]. These errors appear to be analo-\ngous to a phenomenon observed in studies of human percep-\ntion of the rhythmic properties of music: humans can also\nsometimes disagree about the frequency of the main beat of\na piece of music. In particular two inï¬‚uential experiments\non the perception of tempo attempt to generalise observed\nvariations in human responses into somewhat more formal\nmodels of tempo ambiguity [8, 9].\nThe deï¬nition of tempo ambiguity proposed in [8] is based\non the authorsâ€™ observation that, while users tend to agree on\na bpm value for many tracks, in the remaining cases opinion\nis divided between two candidates. They quantify ambiguity\nas the strength of support for the larger of these two candi-\ndates, divided by their mean support:\nA=2 max( H(T1), H(T2))\nH(T1) +H(T2)(1)\nwhere H(T1)andH(T2)are the number of users who tap at\nT1andT2, the most and second most commonly observed\nbpm values, respectively. The study attempts to model the\ntempo ambiguity of a track in two different ways. Firstly\nthe authors suggest that tempo ambiguity may be related\nto the mean of H(T1)andH(T2); and secondly they in-\nvestigate how a related resonance deviation statistic might\nbe predicted from the value of an acoustic periodicity dif-\nference feature computed from the audio signal. The ï¬rst\nmodel was found to be consistent with data collected from\na group of 33 listeners for a set of 24 ten-second excerpts,\nbut the result could not be replicated in a second study of 24\nsubjects who tapped to the beat of 60 thirty-second excerpts.\nThe second model was not supported convincingly by either\nexperiment, although a modiï¬ed formulation of resonance\ndeviation was found to be correlated with periodicity differ-\nence in a third study of 40 subjects [9].\nDespite the inconclusive results reported in [8, 9], the\nstudies have been indirectly inï¬‚uential in the research com-\nmunity due to the adoption of their experimental data, and\nof an evaluation methodology based on their observations,\nin recent rounds of the MIREX tempo estimation competi-\ntion. Algorithms entered for the competition have been re-\n317Oral Session 4: Web\nquired to output two different bpm estimates for each track,\ntogether with associated weights intended to represent â€œper-\nceptual strengthâ€, credit being given for weights similar to\nH(T1)andH(T2), as well as for estimates close to T1and\nT2.\nA separate line of research has, however, highlighted a\nnegative side-effect of putting tempo ambiguity at the heart\nof an evaluation metric for machine estimates. Imagine a\ntrack that has been submitted for automatic tempo estima-\ntion and marked as â€œ70bpm or 140bpmâ€. Is this track appro-\npriate for a playlist of pieces at walking pace, as suggested\nby an estimate of 70bpm? Is it more suitable for a high en-\nergy playlist, as suggested by 140bpm? Will the track really\nbe perceived as slow by some listeners and as fast by oth-\ners? Or is one of the values simply a poor estimate resulting\nfrom a shortcoming of the algorithm, which would be better\nignored?\nThe authors of [5] go so far as to suggest that such uncer-\ntainty means that machine bpm estimates are simply not us-\nable in practice for many potential applications, and should\nbe abandoned in favour of categorical labels such as slow\nandfast. The study goes on to report extremely high accura-\ncies achieved with a slow-fast classiï¬er trained on a bag of\nwell-known low level audio features, and using social tags\nas its groundtruth annotations. This suggests that the am-\nbiguity intrinsic to bpm estimation may simply not arise in\nrelation to perceptual tempo categories.\nIn this paper we attempt to reconcile these apparently\nconï¬‚icting views of perceptual tempo estimation by crowd-\nsourcing a large set of responses through a web-based exper-\niment. The responses include both tapping data and selec-\ntions from a list of categorical labels. The remainder of the\npaper is structured as follows: in Section 2 we describe the\ndesign of the experiment and give some background about\nweb experiments in general; in Section 3 we report results,\nin particular exploring the relationship between label selec-\ntion and human bpm estimates; in Section 4 we outline how\ncategorical labels might be used to improve bpm estimates\nfrom existing tempo estimation algorithms; in Section 5 we\ndescribe and report the results of a controlled experiment\nto compare different algorithms without reference to any\ngroundtruth values; and in Section 6 we draw conclusions\nand outline future work. Last but not least we provide links\nto our experimental data, making it available for future re-\nsearch and evaluation.\n2. EXPERIMENTAL DESIGN\nWhile studies of the perception of music are traditionally\ncarried out under laboratory conditions, in recent years the\nweb has begun to be regarded as a potential source of per-\nceptual data. Social tags, such as those submitted to themusic service Last.fm1, can be seen as an abundant source\nof perceptual responses although their quality is low: the\nâ€œexperimenterâ€ has no control whatsoever over the circum-\nstances in which a tag is applied, indeed there is no guar-\nantee that the user of a music tagging system has even lis-\ntened to the music which they are tagging. Social tags have\nnonetheless been used in several studies intended to cap-\nture listenersâ€™ characterizations of perceptual characteristics\nof music [6]. The appeal of tags to researchers is that the\ncost of acquiring them is essentially zero, and they are of-\nten available in sufï¬cient numbers for statistics to be ro-\nbust even if individual tags are unreliable. Other experi-\nments have been designed as appealing internet games [7].\nThese games give considerably more control over the cir-\ncumstances in which data is collected, but require a rela-\ntively large investment in design and development.\nFor this study we opted for a middle course, designing\nour experiment along the lines of a traditional laboratory\nquestionnaire, but hosting it on the web and simply appeal-\ning to visitors to contribute to our research. Besides pro-\nviding a source of data for the questions at hand, we were\nparticularly interested to ï¬nd out if visitors would take part\nin response to such a bald invitation. This approach, if suc-\ncessful, could offer a useful platform for future research,\noffering considerably more control than social tags, but at\nmuch lower cost than an internet game. The web page for\nthe experiment was hosted on the companion labs site to a\nlarge music website. Although the main site receives many\nmillions of pageviews per day, trafï¬c to the labs site is sev-\neral orders of magnitude lower, typically a few thousand\npageviews per day.\nOn each view of the experiment, the web page shows\nartist and title information, along with an associated set of\nquestions, for thirty-second excerpts of either one or two\ntracks. The excerpts are chosen at random from a pool of\nseveral thousand audio clips, described in more detail in\nSection 3. The ï¬rst excerpt starts playing as soon as the\npage has fully loaded, and the second excerpt starts as soon\nas the ï¬rst has ï¬nished. As shown in Figure 1, users are ï¬rst\nasked to select a speed label for each track, choosing either\nfrom a 3-point scale from slow tofast, or a visually sepa-\nrate category to report cases where they are not sure. On a\npage displaying two excerpts they are then asked whether\nthe second excerpt sounds slower ,the same speed orfaster\nthan the ï¬rst. Finally the visitor is asked to tap along with\nthe main beat of the music.\nThe sequence in which answers can be provided is not\nstrictly locked down, but highlighting on the page is used to\nencourage the visitor to answer the questions in order. In\nparticular the large call to action, shown in Figure 1, is dis-\nplayed only once the preceding question has been answered.\nConventional audio play/pause buttons are provided, so it is\n1e.g.http://www.last.fm/tag/slow\n31812th International Society for Music Information Retrieval Conference (ISMIR 2011)\nFigure 1 . Questions asked on the experiment web page.\nalso possible to stop and restart the tracks at will, or to lis-\nten to them more than once. Once tapping begins, the bpm\nmeter is highlighted in red, changing to green once ten taps\nhave been recorded, to give the visitor an idea of when they\nhave tapped for long enough to allow a reasonable estimate\nof bpm to be made. If a visitor resumes tapping after a pause\nof two seconds or more, the bpm meter and its internal coun-\nters are reset and the tapping is considered as a new attempt\nto answer the question. Although not explicitly messaged on\nthe page, this allows users to try again if they are unhappy\nwith their tapping for any particular track. It also imposes\na lower limit of 30bpm on the tempo which the experiment\ncan record. When the visitor presses the Save button, their\nlabel choices for each track are stored, together with a single\nbpm value computed simply as the mean interval between\ntheir taps.\nThe web can reasonably be regarded as a hostile environ-\nment for perceptual experiments when compared to a labo-\nratory setting, but provided the rules of engagement are un-\nderstood in advance then it is possible to design reasonable\nsafeguards into the way in which responses are collected.\nWe restrict access to the experiment to logged-in users of\nthe main website, allowing us to associate responses with\nthe users who have submitted them. To attract users to con-\ntribute more responses, we award points for each question\nanswered and display total scores for top contributors on\na separate leaderboard page, a ploy which unfortunately is\nalso known to encourage cheating. To mitigate the effects\nof cheating we store at most one set of responses per user\nfor each track. Although organised cheating of course re-\nmains possible, it would require a very determined attempt\ngiven the relatively low proï¬le of the experiment website,\nand spurious data associated with any particular set of users\ncan easily be ï¬ltered out of any analysis. In practice we dis-\ncarded only tapping estimates of over 300bpm, which most\nlikely correspond to misunderstanding of the interface.\nWith these considerations in mind, however, we do en-Listeners Tracks Responses\nLabels 2141 4006 21444\nBpm estimates 1919 3929 19451\nComparisons 1438 3825 7597\nTable 1 . Responses received at the time of writing.\nsure that the design of the experiment also allows us to col-\nlect data for a more robustly controlled comparison of dif-\nferent tempo estimation algorithms. This is discussed more\nfully in Section 5 below.\n3. ANALYSIS OF RESULTS\nThe experiment continues to be publicly available at\nhttp://playground.last.fm/demo/speedo .\nTable 1 summarises the number of responses received at the\ntime of writing. The tracks presented on any given view of\nthe experiment web page are chosen essentially at random,\nas described in detail in Section 5, and consequently the dis-\ntribution of responses between tracks is not uniform. Most\nof the following analysis concentrates on tracks which were\nannotated by at least ï¬ve listeners: in particular 1437 tracks\nreceived ï¬ve or more speed labels, while 1263 of those re-\nceived at least ï¬ve bpm estimates.\nThe annotated tracks are predominantly rock, country,\npop, soul, funk and rnb, jazz, latin, reggae, disco and rap,\nbut also include music from numerous other genres, includ-\ning punk, electronic, trance, industrial, house and folk. They\nrange from recent releases back to the 1960s. A full list of\ntracks used in the experiment is available (see Section 6).\n3.1 Ambiguity in perceptual tempo labels\nAs described in Section 2, visitors to the experiment were\nasked both to tap along to each excerpt and to describe its\n319Oral Session 4: Web\nFigure 2 . Observed distribution of all bpm estimates by\nspeed category.\nFigure 3 . Distribution of peak bpm estimates by speed cat-\negory.\nFigure 4 . Labels submitted by half-speed tappers for tracks\ngenerally considered to be fast.\nFigure 5 . Observed distribution of conï¬‚ict coefï¬cient C.\nspeed on a three-point scale of slow, medium and fast, or\nto indicate if they found it hard to decide. Figure 2 shows\ndistributions of all bpm estimates computed from tapping,\nfor tracks annotated by at least ï¬ve people of whom a ma-\njority described them as slow or fast respectively. Figure 3\nshows the corresponding distributions of single peak bpm\nestimates for each track, computed as follows. Individual\nlistenersâ€™ estimates are histogrammed into ten bins; the peak\nestimate is then the median value in the most populated bin.\nIf adjacent bins contain the same number of values they are\nmerged into a single bin before taking the median.\nThe shape of the distributions in Figure 2 suggests that\nwe can be speciï¬c about octave disagreement in human tap-\nping: when listening to tracks generally regarded as fast,\nsome listeners tap half as fast as the majority. Figure 4\nshows the distribution of labels supplied for these tracks by\nâ€œslow tappersâ€: there are cases in which they consider the\nmusic to be slow, but they are rare.\nTo model the extent of disagreement over perceptual slow\nand fast categories, by analogy with (1) we deï¬ne the con-\nï¬‚ict coefï¬cient for a track:\nC=min(Ls, Lf)\nmax(Ls, Lf)Â·Ls+Lf\nL(2)\nwhere Lis the total number of labels supplied, of which Ls\nare slow and Lfare fast. The ï¬rst term represents the ex-\ntent to which fast and slow labels conï¬‚ict, while the second\nterm applies a discount to this if other users have labelled\nthe excerpt as medium. Figure 5 gives the distribution of C\nover all tracks with at least ï¬ve labels, showing that, for the\nhuge majority of tracks, listeners do not disagree at all when\ndescribing excerpts as either slow or fast. To test whether\nlisteners disagree over perceptual categories in the face of\ntempo ambiguity, we deï¬ne ambiguous tracks to be those\nfor which more than 30% of listeners tap at either double\nor half the peak bpm estimate, allowing a 4% margin of er-\n32012th International Society for Music Information Retrieval Conference (ISMIR 2011)\nror when comparing bpm values. The mean conï¬‚ict coefï¬-\ncient for ambiguous tracks is 0.062, slightly higher than the\nmean of 0.058 for the remaining unambiguous tracks, but\nthe difference is not signiï¬cant ( p= 0.647). We conclude\nthat in general there is no evidence that listeners disagree\nover which excerpts sound slow, and which sound fast, even\nwhen they tap at different metrical levels.\n3.2 Evaluating machine bpm estimates\nIn order to demonstrate the potential value of crowd-sourced\nannotations in evaluating tempo estimation algorithms, we\nselected excerpts for the experiment for which bpm esti-\nmates were readily available from several sources. We re-\nport results here for the following three sources: estimates\nfrom the commercial EchoNest API, as distributed with the\nMillion Song Dataset [1]; the BPM List, a published list of\nbpm values claimed to be computed at least partly by hand,\nusing a variety of commercially-available tools [2]; and, ï¬-\nnally, estimates generated using an implementation of meth-\nods reported in [3] and distributed as a plugin for the V AMP\nframework for audio analysis2.\nSome selection of values was necessary for the EchoNest\nand V AMP sources. The Million Song Dataset was found in\na number of cases to contain data for different versions of\nthe same song: we rejected any songs for which the dupli-\ncate tempo estimates differed by more than 2%, and other-\nwise simply used the ï¬st value encountered. The V AMP plu-\ngin is designed to produce multiple segment-wise tempo es-\ntimates: we selected the estimate associated with the longest\nsegment(s) of audio.\nTable 2 shows evaluation results for the three sources rel-\native to peak human estimates. The evaluation is restricted\nto tracks for which at least ï¬ve crowd-sourced bpm values\nwere available. In order to observe systematic types of error\nin the sources, estimates not matching the human reference\nvalues are split between six categories, corresponding to six\ntypes of octave error, and a ï¬nal â€˜unrelatedâ€™ category for es-\ntimates that do not match any of the preceding ones. An\nestimate is considered to match the groundtruth bpm, or one\nof its related values, if it differs by less than 4% of the ref-\nerence3.\nThe results given in Table 2 show signiï¬cant differences\nin the performance of the three sources: the strongest source,\nthe BPM List, is correct some 70% more often than the\nweakest, the EchoNest. The BPM List also suffers the least\nfrom octave error, presumably conï¬rming that humans were\ninvolved in the creation of its estimates. While categorised\nresults like Table 2 are useful to understand the strengths\nand weaknesses of particular methods, a robust single per-\nformance value can also easily be computed as a weighted\n2http://www.vamp-plugins.org\n3The MIREX 2010 evaluation allows a relative error of 8%.ï¬rst faster same second faster\nEchoNest 39.2 27.3 33.4\nBpm List 33.9 34.7 31.4\nV AMP 34.0 34.0 32.0\nTable 4 . Percentage of answers given when comparing two\ntracks annotated with the same bpm by a particular source.\ncombination of the percentages of estimates classed as cor-\nrect and as unrelated. The weights can be tuned to reï¬‚ect\nthe potential harm caused by octave errors for any particular\napplication.\n4. IMPROVING AUTOMATIC BPM ESTIMATES\nResults presented in [5] report that classiï¬ers can be trained\nto recognise tracks belonging to perceptual slow and fast\ncategories with extremely high accuracy. The separable dis-\ntributions shown in Figure 3 suggest that we can use the out-\nput of such classiï¬ers to remove a great deal of octave error\nin machine estimates. The following simple algorithm can\nbe used to adjust bpm estimates in cases where they conï¬‚ict\nwith predicted labels: any estimate of over 100bpm for a\ntrack classiï¬ed as slow should be halved, and vice versa for\nfast tracks. While evaluating this approach directly remains\nfor future work, Table 3 illustrates the substantial gains pos-\nsible in the best case, by assuming a classiï¬er that always\npredicts the label chosen by the majority of humans in the\nexperiment.\n5. COMPARING ESTIMATION ALGORITHMS\nIn addition to allowing data collection for conventional eval-\nuation against a groundtruth, the experiment was designed\nto contain a controlled experiment enabling the comparison\nof different sources of bpm estimates without reference to\nany groundtruth. The experiment holds indexes from the\nbpm estimates of each source, rounded to the nearest inte-\nger, to a list of all tracks for which the source gave that es-\ntimate. When a visitor arrives at the experiment web page,\nthe server ï¬rst chooses a source at random. It then chooses\na rounded bpm value, and ï¬nally selects two correspond-\ning tracks from the index (or a single track, if the source\nonly annotated one track in the collection with that partic-\nular value). This ensures not only that visitors are asked to\nannotate tracks with a wide range of likely tempo, but in par-\nticular that any two tracks presented together are regarded\nby at least one of the sources as having the same tempo.\nSources can then be compared for consistency by exam-\nining responses to the second question shown in Figure 1,\nin which listeners are asked to say which of the two tracks\nsounds faster. This is clearly a leading question, likely to\n321Oral Session 4: Web\nbpm * 4 bpm * 3 bpm * 2 correct bpm / 2 bpm / 3 bpm / 4 unrelated\nEchoNest 0.6 1.7 30.5 40.7 2.4 0.0 0.1 24.0\nBpm List 0.0 0.2 8.2 68.1 5.2 0.1 0.0 18.3\nV AMP 0.7 1.6 23.0 58.3 4.0 1.6 0.0 12.3\nTable 2 . Performance of three sources of bpm estimates relative to peak crowd-sourced value. Numbers in each category are\npercentages of tracks evaluated for each source.\nbpm * 4 bpm * 3 bpm * 2 correct bpm / 2 bpm / 3 bpm / 4 unrelated\nEchoNest 0.0 0.5 19.5 53.0 1.7 0.0 0.0 25.2\nBpm List 0.0 0.0 5.7 72.8 3.0 0.1 0.0 18.5\nV AMP 0.1 0.1 10.9 73.6 1.6 0.0 0.0 13.9\nTable 3 . Upper bound performance of three sources of bpm estimates after adjustment for label conï¬‚ict.\ncause the listener either to attend to subtle differences be-\ntween tracks, or to pick faster or slower at random, on the\nassumption that the question would be unlikely to be posed\nin relation to two tracks known to be the same speed. Al-\nthough we cannot know in advance what proportion of lis-\nteners will choose each option, we can safely assume that,\nall things being equal, the proportion will be independent of\nthe source of the bpm estimates.\nAs the results given in Table 4 illustrate, this method is\nsuccessful in highlighting differences between the sources,\nwith the EchoNest estimates again shown to be signiï¬cantly\nless consistent than either other source.\n6. CONCLUSIONS\nThis study shows how, with a suitable experiment, simple\ncrowd sourcing of annotations can be used to evaluate al-\ngorithms such as bpm estimation. Analysis of tens of thou-\nsands of responses collected within just a few days leads\nto the proposal of a straightforward and robust approach to\nevaluation against a human groundtruth, which is both con-\nsonant with perceptions of tempo, and designed to reward\nthe estimates most likely to be useful in practical applica-\ntions. A second controlled experiment allows validation\nof these results without reference to any groundtruth val-\nues. Finally we outline a method to combine classiï¬cation\nwith conventional tempo estimation, which promises signif-\nicant improvements over current methods. Future work in-\ncludes implementing and evaluating this approach, and ex-\ntending crowd sourcing to evaluate a wider range of MIR\nalgorithms. Data collected for this study is freely available\nfor research purposes4.\n4http://users.last.fm/ Ëœmark/speedo.tgz7. REFERENCES\n[1] Thierry Bertin-Mahieux, Daniel P.W. Ellis, Brian Whit-\nman, and Paul Lamere. The million song dataset. In\nProc. ISMIR , 2011. (submitted).\n[2] D. Brusca. BPM List: A Music Reference Guide for Mo-\nbile DJs . Lulu.com, 7 edition, 2011.\n[3] M. E. P. Davies and M. D. Plumbley. Context-dependent\nbeat tracking of musical audio. IEEE Trans. ASLP ,\n15(3):1009â€“1020, 2007.\n[4] F. Gouyon, A. Klapuri, S. Dixon, M. Alonso, and\nG. Tzanetakis. An experimental comparison of au-\ndio tempo induction algorithms. IEEE Trans. ASLP ,\n14(5):1832â€“1844, 2006.\n[5] J. Hockman and I. Fujinaga. Fast vs slow: Learning\ntempo octaves from user data. In Proc. ISMIR , 2010.\n[6] C. Laurier, M. Sordo, J. Serr `a, and P. Herrera. Music\nmood representations from social tags. In Proc. ISMIR ,\n2009.\n[7] E. Law, K. West, M. Mandel, M. Bay, and J.S. Downie.\nEvaluation of algorithms using games: The case of mu-\nsic tagging. In Proc. ISMIR , 2009.\n[8] M. McKinney and D. Moelants. Deviations from the res-\nonance theory of tempo induction. In Proceedings of the\nConference on Interdisciplinary Musicology , 2004.\n[9] D. Moelants and M. McKinney. Tempo perception and\nmusical content: What makes a piece slow, fast, or tem-\nporally ambiguous? In Proc. ICMPC , 2004.\n[10] L. Xiao, A. Tian, W. Li, and J. Zhou. Using a statis-\ntic model to capture the association between timbre and\nperceived tempo. In Proc. ISMIR , 2008.\n322"
    },
    {
        "title": "Expressive Timing from Cross-Performance and Audio-based Alignment Patterns: An Extended Case Study.",
        "author": [
            "Cynthia C. S. Liem",
            "Alan Hanjalic"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1416992",
        "url": "https://doi.org/10.5281/zenodo.1416992",
        "ee": "https://zenodo.org/records/1416992/files/LiemH11.pdf",
        "abstract": "Audio recordings of classical music pieces reflect the artistic interpretation of the piece as seen by the recorded performing musician. With many recordings being typically available for the same music piece, multiple expressive rendition variations of this piece are obtained, many of which are induced by the underlying musical content. In earlier work, we focused on timing as a means of expressivity, and proposed a light-weight, unsupervised and audio-based method to study timing deviations among different performances through alignment patterns. By using the standard deviation of alignment patterns as a measure for the display of individuality in a recording, structural and interpretational aspects of a music piece turned out to be highlighted in a qualitative case study on five Chopin mazurkas. In this paper, we propose an entropy-based deviation measure as an alternative to the existing standard deviation measure. The obtained results for multiple short-time window resolutions, both from a quantitative and qualitative perspective, strengthen our earlier finding that the found patterns are musically informative and confirm that entropy is a good alternative measure for highlighting expressive timing deviations in recordings.",
        "zenodo_id": 1416992,
        "dblp_key": "conf/ismir/LiemH11",
        "keywords": [
            "Audio recordings",
            "artistic interpretation",
            "performing musician",
            "multiple expressive renditions",
            "alignment patterns",
            "structural and interpretational aspects",
            "quantitative and qualitative results",
            "musical informality",
            "entropy-based deviation measure",
            "expressive timing deviations"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nEXPRESSIVE TIMING FROMCROSS-PERFORMANCE ANDAUDIO-BASED\nALIGNMENTPATTERNS: ANEXTENDED CASESTUDY\nCynthiaC.S.Liemand Alan Hanjalic\nMultimedia Information RetrievalLab,Delft University of T echnology,The Netherlands\n{c.c.s.liem, a.hanjalic}@tudelft.nl\nABSTRACT\nAudiorecordingsofclassicalmusicpiecesreï¬‚ecttheartis tic\ninterpretation of the piece as seen by the recorded perform-\ning musician. With many recordings being typically avail-\nableforthesamemusicpiece,multipleexpressiverenditio n\nvariations of this piece are obtained, many of which are in-\nduced by the underlying musical content. In earlier work,\nwe focused on timing as a means of expressivity, and pro-\nposedalight-weight,unsupervisedandaudio-basedmethod\nto study timing deviations among different performances\nthrough alignment patterns. By using the standard devia-\ntion of alignment patterns as a measure for the display of\nindividuality in a recording, structural and interpretati onal\naspects of a music piece turned out to be highlighted in\na qualitative case study on ï¬ve Chopin mazurkas. In this\npaper, we propose an entropy-based deviation measure as\nan alternative to the existing standard deviation measure.\nThe obtained results for multiple short-time window reso-\nlutions,bothfromaquantitativeandqualitativeperspect ive,\nstrengthenourearlierï¬ndingthatthefoundpatternsaremu -\nsically informative and conï¬rm that entropy is a good alter-\nnativemeasureforhighlightingexpressivetimingdeviati ons\ninrecordings.\n1. INTRODUCTION\nIn classical music, music pieces are usually conceived by\ncomposersandtranslatedintoscores. Thesearestudiedand\ninterpretedbymusicians,whoeachgivetheirownpersonal,\nexpressive account of the score through their actual perfor -\nmanceofthepiece. Withanincreasingnumberofsuchper-\nformances becoming available in digital form, we also gain\naccess tomany different artisticreadings of musicpieces.\nThe availability of recordings of multiple performances\nof music pieces previously has strongly been exploited in\nPermission to make digital or hard copies of all or part of this w ork for\npersonal or classroom use is granted without fee provided th at copies are\nnot made or distributed for proï¬t or commercial advantage and th at copies\nbear this noticeand thefull citation ontheï¬rst page.\nc/circlecopyrt2011International Society forMusic InformationRetrieval .theï¬eldofaudiosimilarity-basedretrieval. Inthis,thef ocus\nwasonmatchingmusicallycloselyrelatedfragments( audio\nmatching [6,8]),orï¬ndingdifferentversionsofasongatthe\ndocument level, ranging from different performances of the\nsame notated score ( opus retrieval [2]) to potentially radi-\ncallydifferentnewrenditionsofapreviouslyrecordedson g\n(cover song identiï¬cation [11]). In general, matching and\nretrieval of classical music pieces were shown to be achiev-\nable with near-perfect results [1,4]. Another category of\nprevious work largely focused on analyzing and/or visual-\nizing the playing characteristics of individual performer s in\ncomparison toother performers [3,9,10].\nAt certain moments, a performer will display larger per-\nsonal expressive freedom than at other moments, guided by\ntheoretical and stylistic musical domain knowledge as well\nas personal taste and emotion. By comparing expressive\nmanifestations in multiple recordings of the same piece, we\ntherefore can gain insight in places in the piece where the\nnotated musical content invites performers to display more\nor less expressive individualism. Such information on the\ninterplay between performance aspects and the notated mu-\nsicalcontentprovidesanovelperspectiveontheimpliciti n-\nterpretative aspects of the content, which can be of a direct\nbeneï¬t for many Music Information Retrieval (MIR) tasks,\nranging from music-historical performance school analysi s\ntoquickandinformeddifferentiatingandpreviewingofmul -\ntiplerecordings of the samepiece inlarge databases.\nIn recent previous work [5], we proposed a light-weight,\nunsupervisedandaudio-basedmethodtostudytimingdevi-\nations among different performances. The results of a qual-\nitative study obtained for 5 Chopin mazurkas showed that\ntiming individualism as inferred by our method can be re-\nlated to the structure of a music piece, and even highlight\ninterpretational aspects of a piece that are not necessaril y\nvisible from the musical score. In this paper, we introduce\nan entropy-based approach as an alternative to our previous\nstandard deviation-based approach, and will study the char -\nacteristics of both methods in more depth at multiple short-\ntime window resolutions. While this task does not have a\nclear-cut ground truth, the introduction of our new entropy\nmethod allows for quantitative comparative analyses, pro-\nvidingdeeperandmoregeneralizableinsightintoourmeth-\n519Poster Session 4\nodsthanthelargelyqualitativepioneeringanalysesfrom[ 5].\nThis paper is organized as follows. After a summary\nof our previous work from [5], we will describe our new\nentropy-based method. This will be followed by a descrip-\ntionoftheexperimentalsetupandcorrespondingresults. F i-\nnally,thepaperwillendwithaconclusionanddiscussionof\nfuturedirections.\n2. AUDIO-BASED ALIGNMENT AND ANALYSIS\nOFMULTIPLE PERFORMANCES\n2.1 Audio-based alignment of multipleperformances\nIn [5], we proposed a method to infer timing expressivity\nin an audio-based, objective and unsupervised data-driven\nway, largely building on novel work in audio similarity-\nbased retrieval.\nAs short-time harmonic audio signal descriptor features,\nwe adopt the recent Chroma Discrete Cosine Transform-\nreduced Log Pitch (CRP) features, which outperformed tra-\nditionalchromarepresentationsintimbre-robustnessand au-\ndio matching performance [7]. We use the CRP feature im-\nplementation as made available by the original authors1. If\nAis a set with naudio recordings of the same piece, we\nobtainnCRP proï¬le vectors restablishing a set R, where\neachrrepresents an audio recording aâˆˆA.\nAs different performances of the same piece may differ\nin global tempo, the CRP proï¬le vectors râˆˆRwill have\ndifferent lengths. Through Dynamic Time Warping (DTW)\ntechniques, we can align the vectors and ï¬nd a time map-\nping between corresponding events in different recordings .\nForthis,weapplytheDTWalignmenttechniquefrom[11],\nwhich used a binary cost measure and imposed local con-\nstraints to avoid pathological warpings. This method was\nshown to be very powerful in cover song retrieval settings.\nWe choose a CRP proï¬le vector rrefâˆˆR, correspond-\ning to a reference recording that may be arbitrary chosen.\nBy aligning rrefwith the vectors râˆˆR\\{rref}, corre-\nsponding to all other recordings in the set, full alignment\nbetween performances is achieved through rref. For each\nalignment between rrefand anrâˆˆR, an alignment ma-\ntrixXis constructed. The alignment value Xi,jbetween\ntwoCRPproï¬les attimeinstances iandjinrrefandr, re-\nspectively ( rref[i]andr[j]), is computed adopting the local\nconstraints as suggested in [11]. Initialization procedur es,\nbinary similarity measures and other parameters were also\ntaken from this article, to which the interested reader is re -\nferredfor moredetails.\nAn explicit alignment path is obtained by tracing back\nfrom the point corresponding to the highest total alignment\nscore. If|rref|=m, for each alignment toa performance r\nweobtainanalignmentpath woflength m,withw[1...m]\n1http://www.mpi-inf.mpg.de/~mmueller/\nchromatoolbox/indicatingshort-timeinstanceindicesoftheCRPproï¬lesi n\nrthat align to rref[1...m]. Not all time instances 1...m\nmayhavebeenexplicitelycoveredintheoriginalalignment\npath. Assuming linear development for unknown instances,\nmissingvalues areestimated through linear interpolation .\n2.2 Performance alignment analysis\nAfter calculating all alignment paths following the proce-\nduresabove,wewillhaveobtainedaset Wwithnâˆ’1align-\nment paths wâˆˆW, each of length m. We post-process\nthesepathstoemphasizeirregularalignmentbehavior: ifa n\nalignmentsubpath w[k...l]showsconstantalignmentsteps\n(w[k] =w[k+ 1] =w[k+ 2] =Â·Â·Â·=w[lâˆ’1] =w[l]),\nthis means that the corresponding CRP feature vector ex-\ncerpt inris a linearly scaled version of rref[k...l], and\ntherefore does not reï¬‚ect any timing individualism. In or-\nder to highlight alignment step slope changes, we compute\ndiscrete second derivatives over the alignment path.\nFirst of all, for each alignment path w, we compute the\ndiscrete ï¬rstderivative Î´through thecentral difference:\nÎ´[i] =ï£±\nï£´ï£²\nï£´ï£³1\n2(w[i+1]âˆ’w[iâˆ’1]) 1â‰¤iâ‰¤m\nw[1]âˆ’w[0] i= 1\nw[m]âˆ’w[mâˆ’1] i=m.\nDue to an initial alignment index jump, a large â€˜startupâ€™\nderivative is found at the beginning of the path. As we are\nonlyinterestedinthealignmentstepdevelopmentwithinth e\ntrue alignment path (and the beginning of the recording for\nthegiventimesamplingratewillcontainsilence),wesetth e\nderivative values up to this startup point to 0. By repeating\nthecentraldifferenceprocedureontheenhanced Î´,asecond\nderivative approximation Î´2âˆˆâˆ†2isobtained.\nWeassumethatmomentsinthepieceshowingthelargest\ntiming deviations among performers (and thus, the highest\ndegree of individualism) must have given the performers a\nreason to do so, and therefore must be of a certain semantic\nrelevance. A measure is needed to express this individual-\nity of timing at all short-time instances of âˆ†2. For this, we\nproposed to adopt the standard deviation: for each time in-\nstancet= 1...m, we compute Ïƒ[t], which is the standard\ndeviation of all alignment second derivatives Î´2[t]âˆˆâˆ†2,\nacquiring astandard deviation sequence Ïƒof length m.\n3. ENTROPY AS INFORMATION MEASURE\nTheassumptionthatmomentswiththelargesttimingdevia-\ntions(â€˜disagreementâ€™)amongperformerswillbeofacertai n\nsemanticrelevanceresemblesthenotionofentropyininfor -\nmation theory, where items with the most uncertain actual\nrealization are considered to hold the largest amount of in-\nformation. Thus, as an alternative to our previous standard\n52012th International Society for Music Information Retrieval Conference (ISMIR 2011)\ndeviation method, we now propose to calculate the entropy\nofâˆ†2at each short-time instance. If âˆ†2has the possible\nvalues (â€˜symbolsâ€™) d2\nt,1...d2\nt,fat timet,then\nh[t] =âˆ’f/summationdisplay\ni=1p(d2\nt,i)log2p(d2\nt,i)\nwhereweapproximate p(d2\nt,i)bythefrequencyof d2\nt,iinâˆ†2\nat time instance t. While the previous standard deviation-\nbased approach treats the values at each Î´2[t]as cardinal\ndata, the entropy-based approach will treat the values as\nnominal data, only measuring diversity.\n4. EXPERIMENTAL EVALUATION\nWe initially conceived our methods with the goal to reveal\nimplicitly encoded expressive musical information in au-\ndio that would go beyond an objective score reading. This\nmeans that no explicit classiï¬cation is applied and an ob-\njective ground truth is absent. Because of this, in [5], the\nresults of the standard deviation-based method were largel y\ndiscussed in a qualititative way. With our new entropy-\nbased method, possibilities arise for quantitative compar -\nisonsbetweenthismethodandthestandarddeviation-based\nmethod, which we will discuss in this section, as an addi-\ntion to qualitative and musical interpretations of the resu lts\nof theentropy-based method.\nOurexperimentswillfocusontwoaspects: (1)verifying\nthatÏƒandharenorandomnoisesequencesand(2)focusing\nonthemainsimilaritiesanddissimilaritiesbetween Ïƒandh\nfrom a quantitative and qualitative perspective. While the\nwork in [5] only focused on a 2048-sample short-time au-\ndio analysis window, our current experiments will consider\nmultiplepossiblewindowlengths. Whilewearenotstriving\nto identify an â€˜optimalâ€™ time window length yet (which will\ndependonthedesiredmusicalunitresolution,e.g. smallor -\nnamental notes vs. harmonies on beats), we consider these\nmultiple window lengths to verify if the behavior of our\nmethods is stable enough to not only yield interpretable re-\nsultsat theearlier studied resolution of2048 samples.\n4.1 Experimental Setup\nFollowingourearlierwork,wefocuson5Chopinmazurkas\nthat were thoroughly annotated as part of the CHARM Ma-\nzurkaProject[9]: op.17no.4,op.24no.2,op.30no.2,op.\n63 no. 3 and op. 68 no. 3, with 94, 65, 60, 88 and 51 avail-\nable recordings, respectively. We follow the procedure as\noutlined in Section 2.1, choosing the shortest recording fo r\nwhich manually annotated beat data is available as the ref-\nerence recording, thus minimizing the size of the alignment\npaths. In order to interpret the results, we will use man-\nualmusicalstructureanalysesbytheauthorsasareference .\nThankstothecarefullyestablishedmanualbeatannotation s\nFigure 1. Histogram for Î´2values in âˆ†2measured at con-\nsecutiveshort-timewindowsformazurkaop.30no.2,fora\n2048-sample window length and with reference main struc-\ntural boundary labels (a, b, c, etc.) indicated over the time\ndimension.\nfrom the Mazurka dataset, these structure analyses can be\nrelated totheaudio as preciselyas possible.\nWeapplyourmethodstoallavailablerecordingsineach\nof the mazurkas, calculating standard deviations Ïƒand en-\ntropieshforthealignmentpatternsecondderivativesin âˆ†2,\nas obtained for 7 different short-time window lengths (from\n1024 to 4096 samples, in linearly increasing steps of 512\nsamples, at a sampling frequency of 22050 Hz and with\n50% overlap). A representative example of second deriva-\ntivevaluefrequenciesovertheshort-timeinstancesissho wn\nin Figure 1: the majority of values is zero (â€™constant align-\nment developmentâ€™), and frequency peaks for other values\nappear tooccur inbursts.\n4.2 Veriï¬cation of trends instandard deviations and\nentropies\nTo verify that both the sequences Ïƒandhare no random\nnoise sequences, we perform two statistical runs tests: one\ntesting the distribution of values above and under the se-\nquencemean,andonetestingthedistributionofupwardand\ndownward runs. In both cases and for all window lengths,\nthe tests very strongly reject the null hypothesis that the s e-\nquences are random. In Figure 2, the runs frequencies for\nthe test focusing on upward and downward runs are plot-\nted. Fromthisplot,wenoticethatentropysequencesconsis -\ntentlyhavelessup-anddownwardruns(andthusâ€˜smoother\nbehaviorâ€™)thanstandarddeviationsequences,especially for\nsmall window sizes. Furthermore, the relation between the\nnumber of runs and the window size does not appear to be\nlinear, implying that the choice of a larger short-time win-\n521Poster Session 4\n1024 1536 2048 2560 3072 3584 4096010002000300040005000\nwindow lengthruns frequency\n  \n17 no. 4\n24 no. 2\n30 no. 2\n63 no. 3\n68 no. 3\nFigure 2. Numbers of up- and downward runs (summed)\nfor different short-time window lengths. Dashed lines indi -\ncateÏƒsequences, solid lines indicate hsequences. Markers\nindicate mazurkas.\ndow does not uniformly smooth the results obtained with\na smaller window. Curves for the test focusing on values\naboveandunderthesequencemeanareomittedduetospace\nconsiderations, but strongly resemble the given plot. When\nplotting the resulting sequences over time, the resulting h\ncurves indeed are less noisy than the Ïƒcurves. Figure 3\nshows both curves for the opening phrase of mazurka op.\n17 no. 4 for a short-time window of 1024 samples. The Ïƒ\ncurve appears to be denser, due to the larger number of up-\nanddownwardruns. Lookingatthegeneraldevelopmentof\nthe curves, both Ïƒandhappear to show very similar be-\nhavior, with many co-occurring maxima and minima. As\na quantitative backing for this notion, Table 1 shows Pear-\nsonâ€™scorrelationcoefï¬cientbetween Ïƒandhforallwindow\nlengths considered. From the values in this table, it indeed\nbecomes clear that Ïƒandharestronglycorrelated.\n4.3 Standard deviations vs. entropies\nAs mentioned above, entropy sequences hare strongly cor-\nrelated with standard deviation sequences Ïƒ. Thus, as with\ntheÏƒsequences, they will be capable of highlighting devel-\nopmentsthatmusicallymakesense[5]. Nexttotheexample\nin Figure 3, where both the Ïƒandhvalues increased with\nornamentational variation, we also give an example where\nthe musical score does not clearly indicate the expressive\ndevelopment of phrases. In Figure 4, the â€˜câ€™ section of ma-\nzurka op. 30 no. 2 is shown, where a simple subphrase is\nalmost identically repeated 8 times. A performer will not\nplay this subphrase 8 times in an identical way, and this is\nreï¬‚ectedbothin Ïƒandh: themajordisplaysofindividuality\nin recordings can be found in subphrases 1 (ï¬rst statement\nof subphrase), 3 (following traditional binary period stru c-\ntures, here a new subphrase could be starting, but this is not\nthe case) and 8 (last statement of subphrase). Furthermore,\nFigure 3.Ïƒ(top) and h(bottom) sequence for opening\nphrase of mazurka op. 17 no. 4 with corresponding score\nfragments. 1024-sample window length, 20-point moving\naverage smoothed trendline indicated withthick line.\nforsubphrase4and8,theaveragevalueof Ïƒandhishigher\nthan in the other subphrases, and no minima are reached\nas large as in the other phrases. This can be explained be-\ncauseofthealteredornamentstartingthesubphrase,andth e\nfact that both subphrase 4 and 8 are the ï¬nal subphrase in a\nhigher-order phrase hierarchy of 4 + 4 subphrases. From\nboth Figure 3 and 4, the main difference between Ïƒandh\nappears to be that hhas a considerably larger range than Ïƒ,\nand especially tends toamplify positive peaks.\nWithitslessnoisybehaviorandstrongerpeakampliï¬ca-\ntion, the entropy-based method seems more attractive for\nour alignment analyses than the standard deviation-based\nmethod. As a ï¬nal experiment aimed at gaining more in-\nsightintothedifferencesbetweenbothmethods,welinearl y\nscale both Ïƒandhto unit range. This results in sequences\nÏƒnormandhnorm. We then test how often hnorm> Ïƒnorm\nforthreecases: (1)allshort-timeinstances,(2)allbeats tarts\n(with the beat timings obtained from the earlier manual an-\nnotations from the CHARM project) and (3) all subphrase\nstarts. While these cases consider a decreasing number of\nevents, the musical importance of the events increases: a\nsubphrase start should be more informative than a random\ninstance intime. Results aregiven inTable 2.\nIn general, Ïƒnormwill have larger values than hnorm.\nThismatcheswiththenotionthattheentropysequencesam-\nplify positive peaks: thus, the non-peak values will tend to\nskew under the mean entropy value, while standard devia-\n52212th International Society for Music Information Retrieval Conference (ISMIR 2011)\n1024 1536 2048 2560 3072 3584 4096\n17no. 4 0.9271 0.9225 0.9184 0.9117 0.9089 0.9022 0.9007\n24no. 2 0.9352 0.9308 0.9245 0.9218 0.9104 0.9105 0.9045\n30no. 2 0.9107 0.9094 0.9138 0.8955 0.8952 0.8911 0.8945\n63no. 3 0.9165 0.9103 0.9113 0.8992 0.8930 0.8877 0.8876\n68no. 3 0.9261 0.9274 0.9302 0.9387 0.9333 0.9291 0.9321\nTable 1. Pearsonâ€™s correlation coefï¬cient between Ïƒandhsequences for all ï¬ve mazurkas with different short-time wi ndow\nlengths (insamples).\n1024 1536 2048 2560 3072 3584 4096\n17no. 4overall 0.2736 0.2595 0.3994 0.3413 0.4303 0.2847 0.6966\n17no. 4atbeat starts 0.4217 0.3460 0.4798 0.3662 0.4571 0.2955 0.7020\n17no. 4atsubphrase starts 0.6462 0.5077 0.6769 0.4769 0.5231 0.4462 0.7385\n24no. 2overall 0.3645 0.5912 0.3172 0.4754 0.6417 0.5548 0.7307\n24no. 2atbeat starts 0.4903 0.6842 0.3767 0.5097 0.6898 0.5845 0.7895\n24no. 2atsubphrase starts 0.5085 0.7288 0.3559 0.5254 0.7966 0.6271 0.8644\n30no. 2overall 0.2238 0.2354 0.1944 0.1790 0.3030 0.4177 0.6508\n30no. 2atbeat starts 0.3212 0.3005 0.1606 0.1762 0.2902 0.4301 0.6321\n30no. 2atsubphrase starts 0.4375 0.4375 0.3125 0.3438 0.3750 0.5000 0.8125\n63no. 3overall 0.4901 0.5869 0.7861 0.6578 0.8038 0.5617 0.5956\n63no. 3atbeat starts 0.6348 0.6565 0.8348 0.6696 0.8261 0.5435 0.5739\n63no. 3atsubphrase starts 0.8684 0.8947 0.9474 0.7895 0.8421 0.5789 0.6053\n68no. 3overall 0.1574 0.3359 0.1383 0.2698 0.6095 0.4751 0.6628\n68no. 3atbeat starts 0.3039 0.4420 0.1823 0.3094 0.6575 0.5304 0.6906\n68no. 3atsubphrase starts 0.3000 0.5000 0.2333 0.4000 0.6333 0.7000 0.7000\nTable 2. Normalized entropies hnormvs. standard deviations Ïƒnorm: fractions of cases in which hnorm> Ïƒnormconsidered\noverallshort-timeinstances,overallbeatstarts,andove rallsubphrasestartsdifferentshort-timewindowlengths (insamples).\ntionsarecenteredaroundthemeaninamorebalancedway.\nMazurkaop.63no.3isanexception,butthismayhavebeen\ncaused by the noisiness of the historical reference record-\ning (Niedzielski 1931), which causes clicking and hissing\neffects at random moments throughout the piece, thus also\ncausing irregular alignment behavior at these random mo-\nments. However, in all cases, when only looking at time in-\nstanceswithbeatandsubphrasestarts,thefractionoflarg er\nnormalized entropies increases for all mazurkas. Especial ly\nfor subphrases in comparison to beat starts, the increase is\nconsiderable. This implies that the entropy sequence value s\nindeed amplify musicallymeaningful peaks.\nLooking at the differences between beat start and sub-\nphrase start fractions, the increases initially may not app ear\nto be stable or generalizable over different mazurkas. For\nsubphrase starts, the probability that hnorm> Ïƒnormis\nmuch larger than for beat starts in mazurkas op. 17 no. 4\nand op. 63 no. 3 (and to a lesser extent, op. 30 no. 2). On\nthe other hand, in mazurkas op. 24 no. 2 and op. 68 no. 3,\nthis is much less the case, with the beat and subphrase start\nfractions being much closer toeach other.\nFromamusicalperspective,thismaynotseemasstrange\nas from anumerical perspective: mazurkas op. 24 no. 2 and\nop. 68 no. 3 both are rather â€˜straightforwardâ€™ pieces, with\nmanyrepeatingblockswithlittlethematicdevelopment,an d\nconstant ongoing rhythms. Thus, there is not so much ï¬‚ex-\nibility to shape structural boundaries and subphrase start swith large timing differences. On the other hand, mazurkas\nop. 17 no. 4 and op. 63 no. 3 are very dramatical, have\nstrongly differing thematic blocks, and thus allow for em-\nphasizing of new subphrases. While resembling mazurkas\nop.24no.2andop.68no.3intermsofrhythmicalandthe-\nmaticstraightforwardness,mazurkaop.30no.2islessrigi d\nin terms of phrasing and musical movement, and thus will\nallow for more timing ï¬‚exibility, thus also sharing charac-\nteristicswiththeother two mazurkas.\n5. CONCLUSION AND RECOMMENDATIONS\nIn this paper, we proposed an entropy-based method as an\nalternative to a standard deviation-based method for study -\ning alignment patterns between multiple audio recordings,\nwhich were considered to contain interesting information\nabouttherecordedmusicthatcannotobjectivelybeinferre d\nfrom a score. Our entropy method yielded results that con-\nsistently were strongly correlated with the standard devia -\ntion results at multiple time resolutions, while being less\nnoisy and amplifying positive peaks, which both are desir-\nable properties for our purposes. It was shown that both the\nstandard deviation and entropy methods do not depict ran-\ndom noise, but can be relatedtoactual musical content.\nThedevelopmentovermultipletimeresolutionsofcorre-\nlations between standard deviation and entropy sequences,\nthe frequencies of up- and downward runs, as well as runs\n523Poster Session 4\n(a)Score with numberedsubphrases\n(b)Standarddeviationsequence Ïƒ\n(c)Entropysequence h\nFigure4. Mazurkaop.30no.2, Ïƒandhforâ€˜câ€™section. The\n8repeatingsubphrasesarenumbered. 1024-samplewindow\nlength, 20-point moving average smoothed trendline.\nabove and under the sequence mean, yields similar trends\noverdifferentmazurkas,implyingthatourmethodsaregen-\neralizable. We did not focus yet on further implications of\nthechoiceofshort-timewindowlength,whichstillneedsto\nbe done in future work. Another main future challenge is\nthe further solidiï¬cation and backing of the musical inter-\npretationsofourresults. Finally,wedidnotyetemployany\nnoise-ï¬lteringorsignalenhancement techniques. Whilethe\nresultsobtained forthenoisyop.68no.3Niedzielskirefer -\nence recording on runs frequency and correlation trends are\nlargely consistent with the results for other mazurkas with\nclean reference recordings, the reference recording quali ty\nwill inï¬‚uence results and this topic should be investigated\nmore infuturework.\nRendering MIDI ï¬les as audio and modifying them in\na controlled way may partially overcome the problem of a\nmissing ground truth and possible noise in real-life refer-encerecordings. Inaddition,theinterpretationofresult scan\nbestrengthenedthroughacombinationofourmethodswith\nother MIR techniques dealing with prior knowledge of the\nmusicalcontentinamoreexplicitandsupervisedway. Sup-\nported by our methods, such techniques will not have to be\ntediouslyappliedtoafulldatabase,butcanbelimitedtoon e\normorereferencerecordings. Thisintroducespromisingdi -\nrections for MIR tasks dealing with the real-life abundance\nof artisticallyvaluable digital recordings.\nAcknowledgements: Cynthia Liem is a recipient of the\nGoogle European Doctoral Fellowship in Multimedia, and\nthisresearchissupportedinpartbythisGoogleFellowship .\n6. REFERENCES\n[1] M. Casey, C. Rhodes, and M. Slaney. Analysis of minimum\ndistances in high-dimensional musical spaces. IEEE Trans. on\nAudio, Speech and Language Proc. , 16(5):1015â€“1028, July\n2008.\n[2] M.A.Casey,R.Veltkamp,M.Goto,M.Leman,C.Rhodes,and\nM.Slaney.Content-basedmusicinformationretrieval: Current\ndirectionsandfuturechallenges. Proc.oftheIEEE ,96(4):668â€“\n696,April 2008.\n[3] M. Grachten and G. Widmer. Who is who in the end? Recog-\nnizing pianists by their ï¬nal ritardandi. In Proc. Intl. Soc. for\nMIRConf.(ISMIR) ,Kobe, Japan, October 2009.\n[4] C.C.S.LiemandA.Hanjalic. Coversongretrieval: Acompar-\nativestudyofsystemcomponentchoices.In Proc.Intl.Soc.for\nMIRConf.(ISMIR) ,Kobe, Japan, October 2009.\n[5] C.C.S. Liem, A. Hanjalic, and C.S. Sapp. Expressivity in mu-\nsical timing in relation to musical structure and interpretation:\nAcross-performance,audio-basedapproach.In Proc.42ndInt.\nAES Conf. on Semantic Audio , pages 255â€“264, Ilmenau, Ger-\nmany, July2011.\n[6] M. MÃ¼ller. Information Retrieval for Music and Motion .\nSpringer Verlag, 2007.\n[7] M. MÃ¼ller and S. Ewert. Towards timbre-invariant audio fea-\nturesforharmony-basedmusic. IEEETrans.onAudio,Speech\nandLanguage Proc. ,18:649â€“662, March2010.\n[8] M. MÃ¼ller, F. Kurth, and M. Clausen. Audio matching via\nchroma-based statistical features. In Proc. Intl. Conf. on MIR\n(ISMIR),pages 288â€“295, 2005.\n[9] C.S. Sapp. Comparative analysis of multiple musical perfor-\nmances. In Proc. Intl. Conf. on MIR (ISMIR) , Vienna, Austria,\nSeptember 2007.\n[10] C.S. Sapp. Hybrid numeric/rank similarity metrics for musi-\ncalperformanceanalysis.In Proc.Intl.Conf.onMIR(ISMIR) ,\nPhiladelphia, USA, September 2008.\n[11] J. SerrÃ , E. GÃ³mez, P. Herrera, and X. Serra. Chroma binar y\nsimilarity and local alignment applied to cover song identiï¬-\ncation.IEEE Trans. on Audio, Speech and Language Proc. ,\n16:1138â€“1151, August 2008.\n524"
    },
    {
        "title": "Guitar Tab Mining, Analysis and Ranking.",
        "author": [
            "Robert Macrae",
            "Simon Dixon"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1416476",
        "url": "https://doi.org/10.5281/zenodo.1416476",
        "ee": "https://zenodo.org/records/1416476/files/MacraeD11.pdf",
        "abstract": "With over 4.5 million tablatures and chord sequences (collectively known as tabs), the web holds vast quantities of hand annotated scores in non-standardised text files. These scores are typically error-prone and incomplete, and tab collections contain many duplicates, making retrieval of high quality tabs difficult. Despite this, tabs are by far the most popular means of sharing musical instructions on the internet. We have developed tools that use text analysis and alignment for the automatic retrieval, interpretation and analysis of such tabs in order to filter and estimate the most accurate tabs from the multitude available. We show that the standard means of ranking tabs, such as search engine ranks or user ratings, have little correlation with the accuracy of a tab and that a better ranking method is to use features such as the concurrency between tabs of the same song. We also compare the quality of top-ranked tabs with state-of-the-art chord transcription output and find that the latter provides a more reliable source of chord symbols with an accuracy rate 10% higher than the ranked hand annotations.",
        "zenodo_id": 1416476,
        "dblp_key": "conf/ismir/MacraeD11",
        "keywords": [
            "4.5 million tabs",
            "non-standardised text files",
            "error-prone and incomplete",
            "tab collections duplicates",
            "high quality tabs difficult",
            "web holds vast quantities",
            "standard means of ranking",
            "concurrency between tabs",
            "state-of-the-art chord transcription",
            "more reliable source of chord symbols"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nGUITAR TAB MINING, ANALYSIS AND RANKING\nRobert Macrae\nCentre for Digital Music\nQueen Mary University of London\nrobert.macrae@eecs.qmul.ac.ukSimon Dixon\nCentre for Digital Music\nQueen Mary University of London\nsimon.dixon@eecs.qmul.ac.uk\nABSTRACT\nWith over 4.5 million tablatures and chord sequences (col-\nlectively known as tabs), the web holds vast quantities of\nhand annotated scores in non-standardised text ï¬les. These\nscores are typically error-prone and incomplete, and tab col-\nlections contain many duplicates, making retrieval of high\nquality tabs difï¬cult. Despite this, tabs are by far the most\npopular means of sharing musical instructions on the in-\nternet. We have developed tools that use text analysis and\nalignment for the automatic retrieval, interpretation and anal-\nysis of such tabs in order to ï¬lter and estimate the most ac-\ncurate tabs from the multitude available. We show that the\nstandard means of ranking tabs, such as search engine ranks\nor user ratings, have little correlation with the accuracy of a\ntab and that a better ranking method is to use features such\nas the concurrency between tabs of the same song. We also\ncompare the quality of top-ranked tabs with state-of-the-art\nchord transcription output and ï¬nd that the latter provides a\nmore reliable source of chord symbols with an accuracy rate\n10% higher than the ranked hand annotations.\n1. INTRODUCTION\nThere are a number of digital music notation formats, such\nas Music XML, the MIDI ï¬le format, and various formats\nfor images of scanned sheet music. However it is tabs, which\nare plain text ï¬les containing tablature and/or chord symbols\nand lyrics, that have become the most commonly used mu-\nsic notation format on the internet. A comparison of the\nmost popular MIDI, sheet music and tab websitesâ€™ unique\nvisitors per month can be seen in Table 1. The popularity of\ntabs is due to a simple, intuitive approach to the instructions\nthat requires no formal training to understand nor speciï¬c\nsoftware to read or write. Added to this is the fact that tabs\nare commonly free to use and the amount of data needed to\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proï¬t or commercial advantage and that copies\nbear this notice and the full citation on the ï¬rst page.\nc/circlecopyrt2011 International Society for Music Information Retrieval.File type Most popular site Visitors\ntabs ultimate-guitar.com 2,541,482\nsheet music 8notes.com 470,010\nMIDI freemidi.org 17,437\nTable 1 .Unique visitors per month to music score websites from\nhttp://siteanalytics.compete.com\ntransfer the text instructions is almost negligible. However,\ndue to the lack of standardisation there are many variations\nin how they are structured, making machine parsing of tabs\ndifï¬cult. Also, since even beginners can use the format to\nannotate music, many of the tabs found are of poor quality,\nsuffering from errors and incompleteness. A further prob-\nlem is that multiple tabs exist for many songs, making it dif-\nï¬cult for the user to locate the most accurate and complete\ninstance among the alternatives. These difï¬culties motivate\nthe current work.\nWe address these problems by developing a parser for\nguitar tabs and using music information retrieval methods to\nanalyse and compare the tabs. We propose several features\nand evaluate their effectiveness as predictors of tab accuracy,\nin order to improve the quality of tab retrieval. Overall, we\naim to evaluate the viability of data-mining a noisy source\nof metadata from the internet, and we compare our results\nwith those obtained by content-based analysis of audio for\ndetermining the chord sequence for a given song.\nDespite the popularity of tabs on Usenet groups such as\nalt.guitar.tab in the 1990â€™s and more recently on web sites\nsuch as ultimate-guitar.com, little attention has been given to\nthis source of data by the music information retrieval com-\nmunity. In recent work McVicar and De Bie [8, 9] showed\nhow chord sequences from guitar tabs, synchronised with\nthe music, can help improve machine learning methods for\nchord recognition. Audio and video analysis were used in\n[3] to ï¬nd the simplest tablature transcription of chords and\na guitar tablature score follower was demonstrated in [5] that\nused score following to display tabs on small screens.\n2. THE BEATLES DATA\nIn this work we focus on The Beatles due to the availability\nof ample annotated data and guitar tabs for this band.\n453Poster Session 3\n2.1 Ground Truth Chord Sequences\nThe ground truth chord sequence annotations for The Bea-\ntles used in this work come from transcriptions by Chris\nHarte [2]. This data includes chord sequences for 180 tracks\nfrom 12 Beatlesâ€™ studio albums, which is the set that we fo-\ncus on in our evaluation.\n2.2 Ground Truth Structure Segmentation\nThese are structural segmentations consisting of start time,\nend time and segment label for the same 180 The Beat-\nles tracks [6]. The labels are words such as verse, refrain,\nbridge, intro, outro, andsilence , which are often qualiï¬ed\nwith details, e.g. verse a, verse b,andverse (guitar solo) .\n2.3 Web-Mining\nWe used two search engines to locate guitar tabs. The ï¬rst,\n911tabs.com, is a guitar tab search engine with over 4.5 mil-\nlion tabs indexed. We wrote a web crawler that retrieved all\nthe correctly labelled Beatles tabs from 911tabs.com corre-\nsponding to the 180 tracks in our test set. For the second\nsearch engine, Google, we found a combination of search\nterms (â€˜guitarâ€™, â€˜tabâ€™ and some ï¬lters for unwanted content\nsuch as â€˜-videoâ€™) that, when combined with the artist and\ntrack name, gave a high ratio of tabs in the results. After\nthe ï¬rst 100 results for each tab search, the number of tabs\nreturned was low, so we focused on the top 100 results for\neach song. In total we found 24746 tabs relating to the 180\nBeatles tracks in our ground truth data. Additionally, we\nmined the web for an initial chord dictionary of 264 com-\nmon chords from chordie.com and guitarsite.com.\n3. TAB PARSING\nWe see decoding tabs as an example of noisy text analytics,\nwhich are often applied to determine meaning from web-\nmined resources such as online chat, forums, blogs and wikis.\nTo interpret the noisy semi-structured tab data, we imple-\nmented a large set of simple heuristics to handle the many\nvaried tab writing styles that exist. The following steps are\na brief outline of the stages involved in parsing tabs.\nâ€¢Interpret any HyperText Markup Language (HTML)\nspeciï¬c tags. For instance, &nbsp; and <br>tags are\nchanged to spaces and new lines, respectively.\nâ€¢Analyse each line to determine what (if any) type of\ntab line it is. For example the line could contain a\nâ€˜structural markerâ€™, â€˜chord lineâ€™, â€˜chord and lyrics lineâ€™,\nâ€˜tablature lineâ€™, etc. Non-tab-speciï¬c text is discarded.\nâ€¢For each tab line, decode the tab elements accord-\ningly. As such, chords will be extracted from any\nâ€˜chord lineâ€™ or â€˜chord and lyrics lineâ€™, notes will be[Intro]\nRiff1\ne------0-|-3---3---5---5-|-10-------------------|------8---------|\nB---3--1-|-3---3---7---7-|-12----12-0--0--12-0--|------10--10--7-|\nG--------|-4---4---7---7-|-12----12-12-12-12-12-|-9----9---------|\nD--------|---------------|----------12-12-12-12-|-10-------------|\nA--------|---------------|----------------------|(10)------------|\nE--------|---------------|----------------------|----------------|\nRiff2\ne--3--3--3--3--|--0--0------(0)-|\nB--3--3--3--3--|--3--3--(3)-----|\nG--0--0--2--2--|--0--0----------|\nD--0--0--0--0--|--2--2---0---2--|\nA--2--2--x--x--|--2--2----------|\nE--3--3--2--2--|--0--0----------|\nG D/F# Em\nG D/F# Em\nLove love love\nG D/F# Em\nLove love love\nD7/A G D7/F# D7/E\nLove love love\nD C Riff3\nFigure 1 .Tab Sample 1. Chords Extracted:\nG D/F# Em G D/F# Em G D/F# Em D7/A G D7/F# D7/E D C\nA taste of [Am]honey, [C]tasting much [G]sweeter than [Am]wine\nI [Am]dream of [C]your first [G7]kiss and [D]then\nI [Am]feel a[C]part, my [G7]lips are [D]gettâ€™n\nA taste of [Am]honey, [C]tasting much [G]sweeter than [Am]wine\n{Chorus:}\nI [A]will re[C]turn, yes [D]I will re[Em]turn\nIâ€™ll come [F]back for the [G]honey and [Am]you.\nFigure 2 .Tab Sample 2. Chords Extracted: Am C G Am Am C\nG7 D Am C G7 D Am C G Am A C D Em F G Am\nextracted from a â€˜tablature linesâ€™, new chords will be\nadded to the tabs chord dictionary from any â€˜chord\ndeï¬nition lineâ€™.\nâ€¢Reorganise the tab sections into an organised tab ac-\ncording to given structural information. Any indica-\ntors of repetitions will be expanded so that â€˜x2â€™ will\nresult in the current section being duplicated.\nWe developed our heuristics for parsing guitar tabs on\na set of 20 tabs for which we manually annotated ground\ntruth. The chord retrieval from these tabs, as an example,\nextracts 806 out of the 807 chords correctly. Figures 1 -\n2 demonstrates two different samples of tab formats along\nwith the chords extracted by our parsing tool in each case.\n4. EVALUATION\nIn this section we evaluate the precision of the tabs them-\nselves and then compare various means of ranking the tabs.\nIn order to do this, we ï¬rst describe the features used for\nmeasuring and predicting the tabsâ€™ accuracies. We also ex-\nplain existing ranking methods, such as 911.comâ€™s user rat-\ning and Googleâ€™s page rank. We then use correlation to\ndetermine the suitability of using these features as ranking\nmethods. Also, for each feature, we compare the selected\nchord sequences with the output of a state-of-the-art auto-\nmatic chord detection system.\n45412th International Society for Music Information Retrieval Conference (ISMIR 2011)\nC# C#6 Db Fm7 C/B A11 Dm/C#\nC# 0.0 0.25 0.0 0.5 1.0 0.8 0.75\nTable 2 .Chord Similarity (CS) cost examples.\n4.1 Features\n4.1.1 Chord Similarity (CS)\nIn order to measure two chordsâ€™ similarity, we use the Lev-\nenshtein Distance (LD) [4] of the alphabetically ordered notes\nin the chord, as interpreted from the chord deï¬nitions. The\nLD uses dynamic programming to ï¬nd a path P(U, V) =\n(p1, p2, ..., p W)through a matrix of costs between sequences\nU= (u1, u2, ..., u M)andV= (v1, v2, ..., v N). This cost\nmatrix is described as dU,V(m, n)where mâˆˆ[1 :M]and\nnâˆˆ[1 :N]where each pk= (mk, nk). LD uses a cost of\n0for matches and 1for any insertion, deletion or alteration.\nThe maximum cost is the length of the longest sequence.\nWe normalise and invert this cost to give a similarity value\nfrom 0to1, between two chords (note sequences), UandV.\nCS(U,V) =/parenleftbigg\n1âˆ’LD(U, V)\nmax(M, N )/parenrightbigg\n(1)\nDue to how tab parser interprets chord deï¬nitions, this\ncost function treats any enharmonic chords or notes equally.\nExamples of this cost function (CS) can be seen in Table 2.\n4.1.2 Chord Sequence Similarity (CSS)\nThe Chord Sequence Similarity is a measure of how similar\ntwo tab chord sequences, T1andT2are. For this method\nwe use DTW, a generalisation of LD, which has been used\nfor synchronisation in applications such as score following\n[1]. Unlike the binary comparison in LD, DTW can use a\nmore detailed cost function such as the inner product of the\npair of feature vectors, which returns a value between 0and\n1for each pair of feature vectors. In our case the DTW\nuses the CS cost function to compare chords. The overall\nsimilarity cost is given by the sum of the individual chord\nmatch costs along the DTW path Pand the maximum cost\nis the length of the longest sequence. We normalise and\ninvert this similarity cost and express it as a percentage:\nCSS(T1,T2)=/parenleftbigg\n1âˆ’DTW (T1, T2, CS)\nmax(|T1|,|T2|)/parenrightbigg\nÃ—100 (2)\nExamples of the CSS can be seen in Table 5.\n4.1.3 Chord Accuracy (CA)\nThe Chord Accuracy measures the similarity of the overall\nsequence of chords Tin a tab to the chord sequence Gin\nthe ground truth data for the song. Transpositions are not\nconsidered in this factor.\nCA(T, G) =CSS(T, G) (3)4.1.4 Segment Chord Accuracy (SCA)\nMany tabs have incomplete chord sequences, and rely on\nthe user to piece together the complete tab based on cues,\nintuition and knowledge of the song. A more ï¬‚exible ac-\ncuracy measurement, the Segment Chord Accuracy, ï¬nds\nthe accuracy of each segment in the tab independently. For\neach structural segment of a song, as deï¬ned in our struc-\ntural ground truth data, the SCA takes the closest matching\nsub-sequence from the tabâ€™s overall chord sequence. In ad-\ndition, chord sub-sequences which match to more than one\nsegment may be reused and transpositions of the data are\nallowed in the SCA measurement. The pseudo-code for the\nSCA is shown in Algorithm 1.\nInput :Segmentation S={s1, s2, ..., s l},Ground Truth\nChords G,Tab Chords T\nOutput :Segment Chord Accuracy SCA\nSCA = length(G) ;\nforTransposition Tr = 0to11do\nTranspositionCost = 0;\nfori =1toldo\nSegCost = length( si);\nforstart = 0 to length(T) do\nforlen = 1 to length(T) âˆ’start do\nT/prime= subsequence(T,start,len)\nifCSS(si,T/prime)<SegCost then\nSegCost = CSS( si, transpose( T/prime, Tr)) ;\nend\nend\nend\nTranspositionCost += SegCost ;\nend\nifTranspositionCost <SCAthen\nSCA = TranspositionCost;\nend\nend\nreturn SCA;\nAlgorithm 1 : Segment Chord Accuracy\n4.1.5 Chords Concurrence (CC)\nTo determine how well tabs of a song agree with each other,\nwe deï¬ne the Chords Concurrence as the average of the sim-\nilarities between a tabâ€™s chord sequence Tkand the chord\nsequences Ti(i/negationslash=k)of all the other tabs of the same song.\nCC(Tk) =n/summationdisplay\ni=1,i /negationslash=kCSS(Tk, Ti)/(nâˆ’1) (4)\n4.1.6 Structure Similarity (SS)\nIn order to calculate Structure Similarity we ï¬rst normalise\nthe labelling of structural segments, so that a musical struc-\nture such as (Intro, Verse, Chorus, Verse,...) is represented\nby the sequence of characters (A, B, C, B, ...). We then use\nthe LD, normalised and inverted to provide a percentage:\nSS(T1, T2) =/parenleftbigg\n1âˆ’LD(T1, T2)\nmax(T1, T2)/parenrightbigg\nÃ—100 (5)\nNote that we only compute Structure Similarity where\nthe structure is explicitly given in the tab.\n455Poster Session 3\n4.1.7 Structure Accuracy (SA)\nThe Structure Accuracy is a measure of how similar the\nstructural sequence Tof a tab is to the structural sequence\nGof the ground truth data.\nSA(T) =SS(T, G) (6)\n4.1.8 Structure Concurrence (SC)\nThe Structure Concurrence is the average of the similarities\nbetween a tabâ€™s structural sequence Tkand the structural se-\nquences Tiof all the other tabs of the same song.\nSC(Tk) =n/summationdisplay\ni=1,i /negationslash=kSS(Tk, Ti)/(nâˆ’1) (7)\n4.1.9 911 Rating\nThe 911 Rating is the average user rating assigned to the tab\nat www.911tabs.com from 1 (bad) to 5 (good). The number\nof votes that went into this average rating is not provided by\nthe tab site. 1246 tabs with chords had 911 Ratings.\n4.1.10 Google Rank\nThe tabâ€™s Google Rank corresponds to where the URL of the\ntab is found in the ordered list of Googleâ€™s ranked search re-\nsults [10]. Values range from 1 (best) to 100 (worst known).\n5619 tabs found had Google Ranks associated with them,\n1931 of which had chord sequences.\n4.1.11 Date Modiï¬ed\nIf posted tabs are edited and reposted, it might be the case\nthat more recent tabs are more accurate on average than ear-\nlier tabs. A tabâ€™s Date Modiï¬ed is the last modiï¬ed value\nof the HTML ï¬le on the tab server, expressed as the number\nof milliseconds since 00:00:00 January 1, 1970 GMT. 2022\nof the tabs with chord sequences had an associated last date\nmodiï¬ed.\n4.2 Guitar Tab Statistics\nOf the 24746 tabs found with our web-mining tool, 7547 had\nrecognisable chord content and 4643 had structure explicitly\ndeï¬ned, with at least 3 chords/sections. The average tab\nChord Accuracy (CA) for tabs, tabs that were duplicates and\nnon duplicates is 61.8%, 63.4%, and 58.3% respectively. A\nsimilar pattern was observed in the Structure Accuracy (SA)\nof 50.0%, 50.3%, and 49.1%, suggesting that more accurate\ntabs are more likely to be copied. The accuracy difference\nis however small, and the Pearson-rank correlation shows a\nvery weak correlation between accuracy and whether a tab\nis duplicated (0.12 for CA and 0.03 for SA).Filter Method Pearson-rank correlation Samples\nCA SCA\nChords Concurrence 0.54 0.51 7547\n911 Rating 0.07 0.06 1161\nGoogle Rank -0.07 -0.08 1935\nDate Modiï¬ed 0.03 0.01 2022\nTable 3 .Correlations between various features and the Chord\nAccuracy (CA) and Segment Chord Accuracy (SCA).\nFilter Method Pearson-rank correlation Samples\nSA\nStructure Concurrence 0.19 4643\n911 Rating 0.02 620\nGoogle Rank -0.07 1197\nDate Modiï¬ed 0.06 1337\nTable 4 .Number of samples and correlation values between var-\nious features and the Structure Accuracy (SA).\n4.3 Tab Ranking Systems\nThe Pearson-rank correlation is an indication of how effec-\ntive a ranking system is. For example, if there is a high\nand statistically relevant correlation between a tabâ€™s score\nin its 911 Rating and its CA, we can deduce the 911 Rat-\ning favours accurate tabs. Table 3 shows the correlations\nfound between the tabsâ€™ CA, SCA and 4relevant features\ndiscussed above. Similarly, we give the correlations with\nthe Structure Accuracy in Table 4.\nTwo of the correlations from Table 3 can be seen in the\nscatter plots in Figures 3 and 4. Each point represents the\nCA (vertical coordinate) plotted against a feature value (hor-\nizontal coordinate) for a single tab. The features used are\nGoogle Rank (Figure 3) and CC (Figure 4). A negative\ncorrelation in Figure 3, shows that tabs with higher Google\nRanks (lower numbers) are more accurate. A stronger trend\ncan be seen in Figure 4, where the tabs with a higher Chord\nConcurrence have a higher Chord Accuracy. Figure 5 shows\nthe correlation between Structure Accuracy and Structure\nConcurrence from Table 4. Again, there is a clear trend be-\ntween concurrence and accuracy.\nFor the sample sizes provided; the required absolute value\nfor statistical signiï¬cance is less than 0.1. Surprisingly, the\nrating given by users at 911tabs.com, the date the tab was\nmade and the Google Rank had no statistically signiï¬cant\ncorrelation with the accuracy of the tab. The strongest cor-\nrelation was provided by the Concurrence methods that had\na Pearson-rank correlation of 0.54 for CA, 0.51 for SCA,\nand 0.33 for SA.\nThese results show it is possible to improve the ranking\nof tabs by search engines based on analysing the contents of\ntabs in relation to other tabs of the same track.\n45612th International Society for Music Information Retrieval Conference (ISMIR 2011)\nFigure 3 . The scatter plot relates the Chord Accuracy to\nthe Google Rank. Note that lower numbers correspond to\nhigher rank. The weak negative trend between the Google\nRank and accuracy of the tabs is not signiï¬cant.\nFigure 4 . This scatter plot shows a strong trend relating\nChord Accuracy and Chord Concurrence.\nFigure 5 . This graph shows the trend between the Structure\nAccuracy and the Structure Concurrence.4.4 Automatic Chord Transcription\nOur ï¬nal experiment was to compare the results with auto-\nmatic chord detection methods. Both methods satisfy the\nsame information need: ï¬nding the chords to a given song.\nWe selected the top ranking tab for each feature and com-\npared the accuracy of its chord sequence with the output of\na state-of-the-art automatic chord detection system [7].\nIn Table 5 there is an example of the chord sequences\nproduced by the automatic chord recognition system, those\nselected by our features, and the ground truth annotations\nfor The Beatlesâ€™ Donâ€™t Pass Me By . The chord accuracies\nare also given. Table 6 shows the average accuracy of the\nmethods. There is a clear superiority in the automatic detec-\ntion algorithm which is over 10% more accurate, on average,\nthan the tabs selected by any of our features. Of the features,\nthe Chord Concurrence is the most successful feature for se-\nlecting the tab to use. Additionally, we can improve results\nby selecting only tabs with a high Chord Concurrence value.\nFor example, those with 90% CC or more have an average\nChord Accuracy of 79.9%. However, only 24 out of 7547\ntabs have such a high Chord Concurrence.\n4.5 Dependance on Sample Size\nA potential weakness of the Concurrence methods could be\nin being dependent on the number of tabs available for a\nparticular song. To see if this would effect performance, we\ncalculated the correlation between N (the number of tabs for\na particular song) and C (correlation between CA and CC)\nfor each of our 180 tracks. The result, 0.039, is not statisti-\ncally signiï¬cant for the sample size, suggesting that Chord\nConcurrence is a relevant indicator of Chord Accuracy re-\ngardless of the number of tabs on which it is based.\n5. DISCUSSION AND FUTURE WORK\nUsing tab concurrence, we are able to order tab search re-\nsults so that the more accurate tabs are given preference,\nthereby improving the tab search experience. If ranking\ntabs based on one feature leads to a clear improvement over\ncurrent ranking systems, it is possible that greater improve-\nments can be made by selecting tabs using more sophis-\nticated combinations of features. Whilst we have limited\nourselves to analysing just the guitar tabs themselves, we\nsee possible synergies in combining this work with other\nprojects based on web-mining multimodal music metadata\n[11], content-based analysis [7], and other scores.\nThe usefulness of Chord Concurrence is not surprising,\nas errors are less likely to be replicated in independently\nproduced tabs, than the correct chords. However, the au-\ntomatic transcription result shows that a machine listening\nmethod performs better than the average human annotator,\nand this result holds even when features are used to select\n457Poster Session 3\nSource CA Chord Sequence\nGround Truth - C C F G F C C F G F C C F C G F C C F G F C C F C G F C C/5 C C F C G F C F G Csus4 C C\nAuto (Mauch) 90.4% C C F G F C/5 F G F C Cmin7 F C G F C F G F C/5 F C G F C G Gmaj6 G7 C Cmin C F C G F C F G C C/5 F\nChord Concurrence 89.4% C F G F C C F G F C F C G G F C F G F C F C G F C\n911 Rating 82.9% C F G F C C F G F C C F C G F C F G F C\nGoogle Rank 63.4% G C D C G G C D C G G C G D C G G C D C G G C G D C G G C G D C G\nDate Modiï¬ed 63.4% G C D C G G C D C G G C G D C G G C D C G G C G D C G G C G D C G\nTable 5 . Example chord sequences retrieved by the various chord detection methods for the song Donâ€™t Pass Me By showing\nthe Chord Accuracy (CA) of these sequences.\nDetection Method Chord Accuracy\nAuto (Mauch) 79.3%\nChord Concurrence 68.8%\n911 Rating 66.9%\nGoogle Rank 65.6%\nDate Modiï¬ed 62.3%\nRandomly Selected 61.8%\nTable 6 . The average Chord Accuracy of the chord se-\nquences, over 180 Beatles tracks, that were provided by the\ntop-ranked tabs and the chord detection methods. The ï¬nal\nrow shows the average as if the tab was randomly selected.\nbetter-than-average tabs. This raises an interesting question\nabout ground truth: To what extent can human annotations\nfrom unknown sources be used as ground truth in MIR?\nIn future work we plan to improve on the ranking tech-\nniques demonstrated here for the purposes of recommenda-\ntion, synchronisation, tab generation and score following.\nThis work has shown that the concurrence of tabs indicates\ntheir accuracy, therefore we hypothesise that concurrency\nin subsequences and tablature notation will follow this rule.\nThe prevalence of tabs and the tools described here present\nmany interesting avenues of research, including artist simi-\nlarity, the use of chord idioms and inï¬‚uences across genres.\nOur experiments show, with others [8,9], that tabs are a use-\nful source of data for research in MIR.\n6. ACKNOWLEDGMENTS\nThe authors thank Matthias Mauch for the structural anno-\ntations, automatic chord transcription, and review of this\nwork. We also thank Chris Harte for the chord annotations.\nThis work was funded by an EPSRC DTA studentship.\n7. REFERENCES\n[1] R. B. Dannenberg. An on-line algorithm for real-time\naccompaniment. In International Computer Music Con-\nference , pages 193â€“198, 1984.\n[2] C. Harte, M. Sandler, and S. Abdallah. Symbolic rep-\nresentation of musical chords: A proposed syntax fortext annotations. In Proc. 6th International Conference\non Music Information Retrieval (ISMIR) , pages 66â€“71,\n2005.\n[3] Alex Hrybyk and Youngmoo E Kim. Combined audio\nand video analysis for guitar chord identiï¬cation. In\nProc. 11th International Society on Music Information\nRetrieval (ISMIR) , pages 159â€“164, 2010.\n[4] V . I. Levenshtein. Binary codes capable of correct-\ning deletions, insertions, and reversals. Soviet Physics ,\n10(8):707â€“710, February 1966.\n[5] R. Macrae and S. Dixon. A guitar tablature score fol-\nlower. In IEEE International Conference on Multimedia\n& Expo (ICME) , pages 725â€“726, 2010.\n[6] M. Mauch, C. Cannam, M. Davies, S. Dixon, C. Harte,\nS. Kolozali, D. Tidhar, and M. Sandler. OMRAS2 meta-\ndata project 2009. In Late-breaking session at the 10th\nInternational Society on Music Information Retrieval\n(ISMIR) , 2009.\n[7] M. Mauch and S. Dixon. Simultaneous estimation of\nchords and musical context from audio. IEEE Trans. Au-\ndio, Speech and Lang. Proc. , 18:1280â€“1289, 2010.\n[8] Matt McVicar, Yizhao Ni, Raul Santos-Rodriguez, and\nTijl De Bie. Leveraging noisy online databases for use\nin chord recognition. In Proc. 12th International Society\non Music Information Retrieval (ISMIR) , 2011.\n[9] Matt McVicar, Yizhao Ni, Raul Santos-Rodriguez, and\nTijl De Bie. Using online chord databases to enhance\nchord recognition. Journal of New Music Research ,\n40(2):139â€“152, 2011.\n[10] L. Page, S. Brin, R. Motwani, and T. Winograd. The\nPageRank citation ranking: Bringing order to the web.\nTechnical report, Stanford InfoLab, 1999.\n[11] M. Schedl, G. Widmer, P. Knees, and T. Pohle. A mu-\nsic information system automatically generated via web\ncontent mining techniques. Information Processing &\nManagement , 47(3):426â€“439, 2011.\n458"
    },
    {
        "title": "Music Mood Classification of Television Theme Tunes.",
        "author": [
            "Mark Mann",
            "Trevor J. Cox",
            "Francis F. Li"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1415884",
        "url": "https://doi.org/10.5281/zenodo.1415884",
        "ee": "https://zenodo.org/records/1415884/files/MannCL11.pdf",
        "abstract": "This paper introduces methods used for Music Mood Classification to assist in the automated tagging of television programme theme tunes for the first time. The methods employed use a knowledge driven approach with tailored parameters extractable from the Matlab MIR Toolbox [1]. Four new features were developed, three based on tonality and one on tempo, to enable a degree of quantified tagging, using support vector machines, employing various kernels, optimised along six mood axes. Using a â€œnearest neighbourâ€ method of optimisation, a success rate in the range of 80-94% was achieved in being able to classify musical audio on a five point mood scale.",
        "zenodo_id": 1415884,
        "dblp_key": "conf/ismir/MannCL11",
        "keywords": [
            "Music Mood Classification",
            "Automated tagging",
            "Television programme theme tunes",
            "Knowledge driven approach",
            "Matlab MIR Toolbox",
            "Four new features",
            "Tonality",
            "Tempo",
            "Support vector machines",
            "Kernel optimisation"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\n  \n \nMUSIC MOOD CLASSIFIC ATION OF TELEVISION THEME \nTUNES  \nM Mann  T J Cox  F F Li  \nBBC R&D, London , UK  \nmark.mann@bbc.co.uk  University of Salford, UK  \nT.J.Cox@salford.ac.uk  University of Salford, UK  \nF.F.Li@salford.ac.uk  \nABSTRACT  \nThis paper introduces methods  used for  Music Mood Cla s-\nsifica tion to assist in the automated tagging of television \nprogramme  theme tune s for the first time . The methods \nemployed use a knowledge driven approach with tailored \nparameters extractable from the  Matlab  MIR Toolbox  [1]. \nFour new feature s were developed, three based on tonality \nand one on tempo, to enable a degree of quantified tagging, \nusing support vector machines, employing various kernels, \noptimised along six mood axes. Using a â€œnearest \nneighbourâ€ method of optimisation, a success rate in the \nrange of 80 -94% was achieved in being able to classify m u-\nsical audio on a five point mood scale.  \n1. INTRODUCTION  \nThe BBC contains a vast archive of material estimated to be \nover a million hours, most of which has  not been seen since \nit was first broadcast. The corporation is in the process of \ndigitizing this archive, but very little is known about the \nprogrammeâ€Ÿs content. Consequently, various investigations \nare being carried out into the  automat ic classification of \ncontent and generat ion of  metadata in order to enable \nsearching and browsing of the archive when it is e ventually \npublished. However, because of the nature of the archive, \nthe user may not necessarily know what is available. Ther e-\nfore, researchers are investigating  whether the user can \nbrowse the archive according to the mood of the pr o-\ngramme they wish to se e. One aspect of this is to attempt to \ndetermine the mood of the music contained within the pr o-\ngramme and  together with other audio and image recogn i-\ntion techniques  [2], to tag a programme based on this.  \nTheme music is used to set the scene of a programme, so \none would expect a happy, light tune to accompany  an en-\ntertainment programme, and a dark, heavy tune for a s eri-\nous, factual programme [3].   \nThe field of  Music Information Retrieval (MIR) is a well -\nestablished area of research with many  methods and tec h-\nniques for extracting audio features widely reported  [4]. \nConsequently, the tools used in this paper  are not  in the m-\nselves  novel, but the way in which they have been applied \nin this work is. In addition, this is arguably the first attempt \nto classify theme music, which is typically shorter than \nother pieces, using mood . The most common method for \nMood -based Music Information Retrieval (M-MIR ) class i-\nfication in the literature thus far , has been to extract audio \ncharacteristics  from music which are standalone values  \ntaken as an average over the entire piece or clip6. Certain \naudio characteristics  can be very useful . For example, the \nmood  heaviness   scales roughly with the root mean square \nenergy. To classify other, more complex moods , further \nsuch audio characteristics  are added and processed with a \nsupport vector machine (SVM) classifier [5]. An SVM \nworks as a binary classifier by taking  a set of input data and \npredicts, for each given input, which of two possible classes \nthe input is a member of. The justification of this approach \nis based on the supposition  that the computer has the ability \nto cope with high -dimensionality and to determine trends to \ncrudely mimic human perception . However, this method of \napproach does not take into account the inherent structure, \norder and progression of music. Characteristics extracted \nare often carried over from previous work into speech re c-\nognition and include Mel Frequency Ceps tral Coefficients, \nentropy and flatness  [6]. Such feature s are very useful but \nimproved performance could be achieved by using common  \nmusical  feature s such as tonality, dynamic range or tempo  \n[7]. All but the most abstract music has a set  of harmony \nand progression rules which are generally followed and are \nnot always taken into account in determining the audio \ncharacteristics and features used in M-MIR literature to \ndate.  \n \nThis work  details exploratory work with small datasets \nwhich  will form the basis of more extensive investigations. \nIt covers two new techniques for establishing feature s of \nvariables which bear a greater resemblance to the tools used \nin musical composition , offer ing a better way for  classif y-\ning the emotion of mus ic. Th is include s a method for d e-\ntermining the overall tonality of the music, weighted tona l- \nPermission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fee provided that copies are \nnot made or distributed for profit or commercial advantage and that co p-\nies bear this notice and the full citation on the first page.   \nÂ© 2011  International Society for Music Information Retrieval  \n735Poster Session 6\n  \n \nity together with two new feature s of how th ese change  dur-\ning a piece  and a more reliable tempo extractor. This work \nmakes use of the  Matlab  MIR Toolbox but uses the output \nfrom the characteristics extractors to classify musical fe a-\ntures in new ways.  It differs from existing work in that it \nuses a knowledge -driven approach to quantify how extreme \na mood is (e.g. is it quite happy or very happy?) and b e-\ncause it  is examining theme music.  \n2. EXPERIMENTS  \nUpon starting this work it was clear that a n adequate  data-\nset for the aims of the project which described the mood of \nvarious theme tunes did not exist. Therefore, in order to \ngather sufficient ground truth data to tr ain an SVM, a pu b-\nlic engagement projec t entitled â€œMusical Moodsâ€  [8] was \nundertaken to obtain a dataset  (in which  the theoretical \nbackground , statistical data and reasoning for  the dataset \nand the dataset itself can be found) . This took the form of a \nsurvey in which 144 television theme tunes were ra ted by \nthe general public  on five point scale s along the following \nemotional axes: happy -sad, light -heavy, dramatic -calm, \nmasculine -feminine, playful -serious, relaxing -exciting.  The \naxes were chosen to corr elate with  the semantic from  Os-\ngoodâ€Ÿs dimensional space; a three dimensional space i n-\ncorporating Evaluation, Po tency and Activity (EPA) [ 9].    \nWhilst the Musical Moods dataset was being gathered, it \nwas necessary to use a development  dataset upon which to \nexperiment. Initial investigations attempted to find trends \nin features extracted using the MIR Toolbox  [1] and the \ntracks  tagged with mood -based adjectives in the Magnat a-\ngatune dataset  [10]. Though only a small proportion of the \ndataset contained binary , rather  than quantitative mood \ntags (i.e. happy -sad as opposed to very happy, quite sad) , \nthe Magnatagatune dataset was nevertheless  considered \nuseful for classification devel opment  and initially  used to \ntrain single SVMs using a combination of the feature e x-\ntraction tools available in MIR Toolbox.  \nCertain tools in the MIR Toolbox such as mirrms , which \nfinds the root mean square  of the  energy of the track, mir-\nlowenergy , which f inds the percentage of the track time in \nwhich the audio is below a certain energy value and \nmircentroid , which finds the â€centre of gravityâ€Ÿ in the fr e-\nquency spectrum, were found to be very useful features to \nbe incorporated into classification of some mo od scales. \nOther tools were found to produce very useful results, but \nwhich needed to be  enhanced and modified so that the  ex-\ntracted data  could be  converted into a useful, single nu m-\nber such as for the tools mentioned above in order to be \nused for classifi cation.  2.1 Tonality  \nThe first of these was mirkeystrength . There are 12 poss i-\nble basic major chords and twelve possible minor chords  in \nmusic . The function calculates and assigns a probability to \neach of the possible 24 chords at a sample rate that can be \ncontrolled with the function. For this investigation, half \nsecond intervals were used. The function calls another \nMIR toolbox function, mirch romagram  [11], which calc u-\nlates the energy distribution for each note in the diatonic \nscale. The pitches are then concatenated into one octave \nand normalized. Next, mirkeystrength  cross -correlates the \nchromagram with the chromagram one would expect for \neach of the 24 chords and assigns a probability to each \nchord, where a probability of +1 for the tested chord would \nindicate a definite match whilst -1 would indicate a definite \nmismatch.  \nFigure 1. A graphical representation of the possible chords \nused in Last of the Summer Wine  with time. Major chords \nare denoted with capital M, minor chords with a small m. \nThe red colours denote a high degree of matching. Cons e-\nquently, this piece is predominately in C major, though C \nminor gives quite a strong match a lso. \nThe reduction of a piece of music to major or minor chords \nis an oversimplification to a certain extent. Whilst major \nand minor chords are the basic construct of a piece of \nWestern -style music, other chord types such as dominant \nsevenths, diminished s evenths, extended, other added tone \nand dissonant chords are used to great effect in music to \nelicit different emotions. However, by their nature, they are \nmore complex and hence difficult to detect and can often \nbe confused with major and minor chords. Co nsequently, \nwhen such chords are present, one would expect the key \nclarity to dimini sh. This can be seen in figure 1 , which is \nthe mirkeystrength  chromagram for the theme tune of the \nBBC television programme Last of the Summer Wine . At \naround the 3 second mark  (indicated by the black box)  an \nadded tone chord of C, D, F and A is played. The software \nunderstandably struggles to differentiate between D minor, \nA minor and F major chords as a consequence, with no a s-\n \n73612th International Society for Music Information Retrieval Conference (ISMIR 2011)\n  \n \nsigned probability particularly high  and no red  in the figure \nat this point . \nNevertheless, mirkeystrength  is an excellent tool because \nof the probabilities it associates with each chord.  The abi l-\nity to calculate tonality was thought to be of  significant use \nbecause of it was perceived to have a correla tion with \nmood axes such as happy -sad. Three feature s based on t o-\nnality were developed.  \n2.1.1 Weighted tonality  \nData  taken from mirkeystrength  was used to find a mea n-\ningful feature  for tonality which would have a correlation \nwith the happy -sad axis and the Magnatagatune dataset.  \nBearing figure 1  in mind, it was clear that the feature  \nneeded to be weighted in some way. Indeed, without \nweighting, the correlation found between t he happy -sad \naxis and the dataset was found to be poor. Consequently, \nthe feature  developed was named weighted tonality, W, and \nis defined as:  \n  (1) \nWhere:  \nKmax  =  peak tonality probability amplitude whether m a-\njor or minor,  \nKmaj  =  peak major to nality probability amplitude,  \nKmin  =  peak minor tonality probability amplitude,  \nn = the number of time intervals used to classify the sa m-\nple of music,  \nsummed over all n and divided by n. Minor keys will ther e-\nfore be of negative W. \nThis feature  gives a much clearer representation of the \noverall tonality of the music under consideration because it \nemphasises certainty where it exists and minimizes unce r-\ntain contributions. This feature  was combined  with two \nother inputs,  mirrms  and mircentroid , to train  an SVM on \n99 tracks labelled with binary tags on the happy -sad axis. \n82% of a further  194 tracks  were then correctly classified,  \nwhich  is comparable with other success rates6 and was \nconsidered to be a solid basis for the full investigation13. \n2.1.2 Weighted tonality differential  \nAs well as the overall nature of the tonality in the music, it \nis also useful to know the frequency with which tonality \nchanges. After taking time to study the sample set, it was \nfound that moods  such as exciting and dramatic tended to \nexhibit  a more frequent change of tonality and dominant \nchord . Consequently, two feature s relating to  the change in the in dominant chord were  made. The first was a weighted \ntonality differential, which detects the rate at which the t o-\nnality  changes during the course of the music.  \nIt does this by finding the transitions and multiplying the \ntransition with the sum of the certainties associated with \nthe chords before and after the transition â”‚Kmaj - \nKminâ”‚j+â”‚Kmaj - Kminâ”‚j+1 (where j corresponds to the ce r-\ntainty before and j+1 to the certainty after) . It will only do \nthis at transition locations. Where there is not a transition, \nthe differential will be 0. This is then averaged over the \nnumber of time intervals, n. Again, bec ause this weights \nthe transitions with a certainty that the tonality change has \nhappened, it gives greater emphasis to clearer transitions, \nthus filtering out transitions which may not have occurred . \n2.1.3 Weighted chord differential  \nThe second feature  determined was a weighted chord di f-\nferential, which detected the rate at which the dominant \nchord changed in the piece; the chord may change but this \ndoes not necessarily mean a change in tonality (for i n-\nstance the chord can change from an A major to an E  major \nchord).  \nIt searches for the dominant chord, Kmax and detects Kmax \ntransitions. The transition is weighted with a chord trans i-\ntion certainty, which is calculated by looking at the change \nin certainty of the two keys in question before and after the \ntransition. Let us define Ki as the maximum certainty chord \nbefore the transition and Ki+1 as the probability of this \nchord after the  transition. Likewise Li is defined as the ce r-\ntainty of the new chord before the transition and Li+1 as the \ncertainty after it. The transition is weighted by the factor \n(Ki â€“ Li) + (Li+1 â€“ Ki+1). Again, where a transition does  not \noccur, the differentia l will be 0. This feature  is averaged \nover all time intervals , n. \nBecause these feature s weight the transitions with a ce r-\ntainty that the chord change has happened, it gives greater \nemphasis to clearer transitions, thus filtering out uncertai n \ntransitions .  \n2.1.4 Testing of tonality feature s \nThe Magnatagatune  dataset  contained few tags on the r e-\nlaxing -exciting axis. 150 production music clips  tagged  \nwith exciting and relaxing  in the BBC Archive  by expert \narchivists  were used as a ground truth dataset  instead  of \nMagnatagatune  and although the dataset contained the o c-\ncasional contentious tag, combining the differential fe a-\ntures with weighted tonality, mirrms and mirlowenergy  in-\ncreased successful  classification  (by ~10%)  and resulted in  \nthe correct classification of 37 out  of 50 tracks on the r e-\n737Poster Session 6\n  \n \nlaxing -exciting axis  (with the 100 remaining tracks used \nfor training)  which  is comparable with other success rates  \n[4] and was considered to be a solid basis for the full inve s-\ntigation.  \n2.2 Tempo  \nThe final feature  developed previous attempts to determine \nthe tempo of music. The extraction of tempo is desirable \nbecause it correlates with mood scales such as exciting -\nrelaxing. The field of beat extraction is a well -developed \none, with a number of beat extractors  comp etently able to \nextract the key beats  at the last ISMIR conference . How-\never, this is distinct from tempo, which is more subtle fea-\nture of the fundamental frequency and pace of the music. \nBeat extractors  such as beatroot  [13] and exist ing tempo \nextractors such as mirtempo  often overestimate the tempo \nbecause they count the half or third beat (depending on the \nnature of the music), especially in pieces where instr u-\nments with high frequency transients such as percussion \nexist. Consequently, whilst bein g able to detect pieces of \nmusic with high tempo is relatively straightforward, pieces \nwith low tempo are often labelled  with twice or three times \nthe actual value.  \n \nFigure 2. The filtered waveform of the BBC television \ntheme tune  Eastenders . Green  waveforms  (bands 3, 6, 8 & \n9) indicate the bands in which an autocorrelation of onsets \nreturns the musically correct tempo, red waveforms (bands \n4,5,7 & 10)  indicate where the function returns double the \ntempo . Blue  (bands 1 &2)  waveforms give neither.  \nFigure 2  illustrates a theme tune  filtered into ten, roughly \nlogarithmically equal frequency bands which roughly cor-\nrespond to  octaves using mirfilterbank . As can be seen, \nwhen this is done, the beat is clearly visible in certain bands, but the band in which  they occur is not necessarily \nthe same each time.  \nTempo calculations were carried out on the filtered wav e-\nforms in figure 2  using mirtempo . The function mirtempo  \ncalculates the tempo by picking the highest peak in the \nautocorrelation function of onset de tection. The green  \nbands  (3,6,8 & 9)  indicate where the tempo was correctly \nidentified, the red (4,5,7 & 10) where a tempo twice that of \nthe correct tempo was calculated. Note that in no instance \nhas a tempo half that of the correct tempo been found and \nthat the correct beat can be clearly identified in the green  \nwaveforms . \nThe solution is to apply the  mirtempo f unction to each of \nthe ten filtered waveforms. The modal  temp o is found  and \ngrouped into clusters . The standard deviation of the beats \nper minute inside each cluster is also noted to give a mea s-\nure of how precise the extracted tempo is. It is therefore \npossible to return an unspecified tempo should this value \ngo above a certain threshold.  \nWhen the data is clustered, the largest clust er (or mode) is \nfound. T he software  then searches for a tempo within 15% \nof half the value of the mode. If this exists, the s lower \ntempo is chosen as the correct tempo. The software then \nsearches for a tempo within 15% of a third of the value of \nthe mode. Again, if this exists, this slower tempo is chosen \nas the correct tempo. If neither a half nor a third tempo is \nfound, the me an value of the modal cluster is chosen as the \ntempo.  \nIn all cases tested so far, this has correctly identified the \ntempi of forty pieces of theme music. This is probably b e-\ncause the nature of the way in which the tempo is dete r-\nmined using the mirtempo  function means that the tempo is \nalways going to be over -estimated rather than underest i-\nmated and because tempo is a feature  of the fundamental \nbeat and pace of the music. Other extractors, such as \nmirtempo  alone and beatroot only achieved success rat es \nof 60 -70% on the same theme music . The tempo extractor  \nin this paper  does not work quite as well for pieces with \nunusual time signatures such as 5/4 or 7/4 , but these are not \ncommonly used in theme tunes . \nThe feature s developed above complemented  existing si m-\npler feature s. Therefore, for each of the 144 theme tunes in \nthe Musical Moods dataset, the following seven audio fe a-\ntures were extracted: mirrms , mirlowenergy , mircentroid , \nweighted tonality, weighted tonality differential, weighted \nchord differential and tempo.  \nA mean score for each mood scale for each track in the M u-\nsical Moods data was calculated  from the subjective testing  \nand then normalized so that the lowest score was 0.5 and \n73812th International Society for Music Information Retrieval Conference (ISMIR 2011)\n  \n \nthe highest score 5.49. The means were then rounded t o the \nnearest integer, giving a score 1 -5. This aligns the data with \nthe original scale with each number referenced to a tag (e.g. \non the happy -sad scale, 1 would be associated with very \nhappy  whilst 4 would be associated with quite sad ). Each \ninteger was separated by an SVM classifier and trained  as \nindicated in table 1 .  \n \nLogic score  0 1 \nSVM1  1 2-5 \nSVM2  1,2 3-5 \nSVM 3  1-3 4,5 \nSVM 4  1-4 5 \nTable 1. A summary of how each SVM separated the mean \nmood scores.  \nTo recover the mood score the classifications  are summed \ntogether and one added as in equation 2:   \n14 3 2 1 ï€«ï€«ï€«ï€«ï€½ C C C Cï­\n                                   (2) \nWhere:  \nC1 =  the classification of SVM1,  \nC2 =  the classification of SVM2,  \nC3 =  the classification of SVM3,  \nC4 =  the classification of SVM4,  \nï­   =  is the mood classification score.  \nTaking the example above, four classifications from each \nof the four SVMs of 1 1 1 0 would mean 3Ã—1+1 = 4. Occ a-\nsionally one would obtain a spurious r esult  such as 1010. \nThe same equation is applie d in this instance and the track \nwould therefore classify as a 3.  \nThe dataset was randomly  split into two, the first 94 tracks \nwere used to  train the SVMs, the final 50 to test the SVMs . \nThe program optimized the classifier by choosing from \nfive possible SVM kernels: linear, quadratic, cubic,  Gaus-\nsian radial basis function s and multi -layer perception  using \nthe bioinformatics toolbox in Matlab for all of the  possible \n255 combinations of th e 7 audio features.   \nThree methods for determining the best combination of \nfeatures and SVM kernel were found. A, is the percentage \nof time that the classifier correctly identifies the correct \nmood score. B, is the average classification success rate for \nall four SVMs, C, is the percentage of time the classifier \ncorrectly identifies the correct score or classifies with the \nnearest integer (i.e. if the correct mood score of a track is 3 \nand the SVMs classify it as 2, 3 or 4, this would still count \nas a posi tive result towards C whereas a classification of 1 \nor 5 would  not).  3. RESULTS  \nThe aim of this section is to determine whether mood can \nbe quantified  for television theme tunes . The results for the \nabove three methods are shown in table 2. \n \nMood Scale  A B C \nDramatic -calm  40% 85% 94% \nHappy -sad 44% 84% 88% \nLight -heavy  30% 79% 82% \nMasculine -feminine  32% 80% 84% \nPlayful -serious  48% 81% 80% \nRelaxing -exciting  36% 82% 88% \nTable 2. The results obtained for each optimized feature . \nThe testing and training sets were then swapped and the \nsame calculations carried out . On all mood axes A, B & C \nvaried by an average of 2% with a close match in the audio \nfeatures chosen. The use of weighted tonality, the differe n-\ntials and the tempo extractor increase  the successful class i-\nfication percentages  B & C by an average of ~20%.  Table \n3 uses the  data in table 2 and gives the root mean square \nerror with respect to a baseline  in which each track is \ntagged with  a score of 3 for each mood.  All except the \nlight-heavy  scale show a marked improvement on the bas e-\nline. Much of the dataset  results  contained scores of 2, 3 or \n4, and in general table 3 indicates the feasib ility of quant i-\nfying the data  by these methods . \n \nMood Scale  baseline  A B C \nDramatic -calm  1.16 0.96 0.99 0.93 \nHappy -sad 1.44 1.20 1.11 1.11 \nLight -heavy  1.15 1.19 1.64 1.64 \nMasculine -feminine  1.15 1.06 1.08 1.07 \nPlayful -serious  1.47 1.22 1.33 1.11 \nRelaxing -exciting  1.29 1.05 1.09 1.00 \nTable 3. The root mean square error for A, B & C with a \nbaseline of mood score 3.  \nThe results  in table 2  show that measure A gave the worst \nresults, which is  not entirely unexpected given the subtlety \nbetween the classifications . B has a higher success rate  \nthan A because it is a measure of how well the SVMs in \ntable 1 are working which does  not necessarily translate \ninto an exact classification.  The best success rates are \nachieved for the measure C, but this measure has the wid-\nest tolerance . However, what C does is to classify the audio \nso that most tracks are label led with the correct tag or the \none next to it. So, for instance, audio which is tagged as \nquite sad could actually be tagged as very sad or neither \nhappy nor sad. Figure 3 illustrates how  the distribution of \n739Poster Session 6\n  \n \nscores change s upon classification  of 50 BBC te levision \ntheme tunes on the dramatic -calm scale.  Classifications of \n1 or 5 decrease whilst classifications in the middle i n-\ncrease. Whilst the ground truth data is quite flat, classific a-\ntion optimised for B and C compresses the distribution into \na large peak in the middle. This indicates that the alg o-\nrithms for optimising the SVMs do not adequately account \nfor extremes in mood (which could be explained by the \ndataset being too small, whereby the number of extreme \nmood samples is small) . Optimising for A shows a better \nmatch in distribution in this example but this method \nshows great variation between the different mood axes and \nresults in classifications that are more often wrong than \ncorrect.  \n \nFigure 3 . The distr ibution of mood scores having opt i-\nmised for A (correct score), B (average SVM success pe r-\ncentage) and C (correct score or nearest integer).  \n4. CONCLUSION  & FUTURE WORK  \nThis is a promising first step towards  a scaled classification \nof television theme music  mood. The use of weighted t o-\nnality  enabled a correct  classification  of 171 out of 194 \ntracks  (82%)  using  a further 99 tracks which were  labelled \nwith binary tags on the happy -sad axis . The use of \nweighted tonality differentials  resulted in  the correct class i-\nfication of 37 out  of 50 tracks on the relaxing -exciting axis  \n(with the 100 remaining tracks used for training). The e n-\nhanced tempo extractor correctly identified 40/40 tempi of \ntheme music.  The use of the above combined  increase d the \nsucces sful classification percentages B & C by an aver age \nof ~20% to  accurac ies of up to  94% for some mood scales . \nImprovement s need to be able to classify  extremes of em o-\ntion. The work covered in this paper  does not cover  how \nthe extracted audio features coincide with each other  tem-\nporally . For instance, a sudden, very loud minor chord may \ninvoke a more complex mood than the time -averaged \nmoods determined here. Measures of dynamic progression \ncross -correlated with tonality, spectral centroid and tempo \nare a possible means to enable a better classification of more complex  and stronger  emotions over shorter tim e-\nscales and should be the focus of future work.  \n5. REFERENCES  \n[1] O. Lartillot, P. Toiviainen, â€œA Matlab Toolbox for \nMusical Feature Extraction from Audio â€, Proceedings \nof the International Conference on Digital Audio E f-\nfects, Bordeaux,  2007.  \n[2] S. Davies, D. Bland, and R. Grafton, \"A Framework \nfor Automatic Mood Classification of TV \nProgrammes,\" Proceedings of the 5th International \nConference on Semantic and Digi tal Media \nTechnologies , Saarbrucken, Germany, 2010.  \n[3] K. Negus and J . Street . â€œIntroduction to Music and \nTelevision ,â€ Special Issue, Popular Music, No. 21, pp \n245-248. \n[4] Y.E. Kim et al. â€œMusic Emotion Recognition: A State \nof the Art Review,â€ Proc. 11th Intl. Soc. for Music. Inf. \nRetrieval Conf ., pp. 255 -66, 2010.  \n[5] I. Steinwart , A. Christmann, â€œSupport Vector M a-\nchines,â€ Springer , 2008 . \n[6] M. Xu et al.  M Xu, LY Duan, J Cai , LT Chia, C X u. \nâ€œAdvances in Multimedia Information Processing ,â€ \nPCM , Springer , 2004 ,  \n[7] Anon ., â€œRudiments and Theory of Music, â€ Associated \nBoard of the Royal Schools of Music , 1958.  \n[8] S. Davies, T.J. Cox, P. Allen, \"Musical Moods: A \nMass Participation Experiment for Affective Classif i-\ncation of Music ,\" Proc. 1 2th Intl. Soc. for Music. Inf. \nRetrieval Conf ., (accepted), 2011.  \n[9] C. E. Osgood, G. Suci, P. Tannenbaum, â€œThe \nmeasurement of meaning ,â€ University of Illinois Press,  \nUrbana, USA,  1957.  \n[10] E. Law, K. West , M. Mandel,  M. Bay, S. Downie, \nâ€œEvaluation of algorithms using games: the case of \nmusic tagging, â€ Proc. 11th Intl. Soc. for Music. Inf. R e-\ntrieval Conf ., pp. 387 -392, 2009.  \n[11] O. Lartillot, P. Toiviainen, T. Eerola, â€ Studies in \nClassification, Data Analysis, and Knowledge Organ i-\nzation, â€ Springer -Verlag, 2008.  \n[12] M. Mann , â€œProcessing audio data for producing met a-\ndata,â€ UK  Pat. App. P/66699.GB01/IML/kz , 2011  \n[13] S. Dixon â€œ Evaluation of the Audio Beat Tracking \nSystem BeatRoot ,â€ Journal of New Music Research , \nVol. 36, No.  1, pp. 39-50, 2007.  \n740"
    },
    {
        "title": "Three Current Issues In Music Autotagging.",
        "author": [
            "GonÃ§alo Marques",
            "Marcos AurÃ©lio Domingues",
            "Thibault Langlois",
            "Fabien Gouyon"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1417757",
        "url": "https://doi.org/10.5281/zenodo.1417757",
        "ee": "https://zenodo.org/records/1417757/files/MarquesDLG11.pdf",
        "abstract": "The purpose of this paper is to address several aspects of music autotagging. We start by presenting autotagging experiments conducted with two different systems and show performances on a par with a method representative of the state-of-the-art. Beyond that, we illustrate via systematic experiments the importance of a number of issues relevant to autotagging, yet seldom reported in the literature. First, we show that the evaluation of autotagging techniques is fragile in the sense that small alterations to the set of tags to be learned, or in the set of music pieces may lead to dramatically different results. Hence we stress a set of methodological recommendations regarding data and evaluation metrics. Second, we conduct experiments on the generality of autotagging models, showing that a number of different methods at a similar performance level to the state-of-the-art fail to learn tag models able to generalize to datasets from different origins. Third we show that current performance level of a direct mapping between audio features and tags still appears insufficient to enable the possibility of exploiting natural tag correlations as a second stage to improve performance.",
        "zenodo_id": 1417757,
        "dblp_key": "conf/ismir/MarquesDLG11",
        "keywords": [
            "music autotagging",
            "state-of-the-art",
            "evaluation fragility",
            "tag generality",
            "audio features",
            "tag correlations",
            "performance improvement",
            "natural tag correlations",
            "second stage",
            "exploiting correlations"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nTHREE CURRENT ISSUES IN MUSIC AUTOTAGGING\nGonc Â¸alo Marques1, Marcos Aur Â´elio Domingues2, Thibault Langlois3, Fabien Gouyon4\n1gmarques@isel.pt DEETC-ISEL Lisboa,2maddomingues@gmail.com INESC Porto,\n3tl@di.fc.ul.pt DI-FCUL Lisboa,4fgouyon@inescporto.pt INESC Porto\nABSTRACT\nThe purpose of this paper is to address several aspects of\nmusic autotagging. We start by presenting autotagging ex-\nperiments conducted with two different systems and show\nperformances on a par with a method representative of the\nstate-of-the-art. Beyond that, we illustrate via systematic\nexperiments the importance of a number of issues relevant to\nautotagging, yet seldom reported in the literature. First, we\nshow that the evaluation of autotagging techniques is frag-\nile in the sense that small alterations to the set of tags to be\nlearned, or in the set of music pieces may lead to dramati-\ncally different results. Hence we stress a set of methodologi-\ncal recommendations regarding data and evaluation metrics.\nSecond, we conduct experiments on the generality of auto-\ntagging models, showing that a number of different methods\nat a similar performance level to the state-of-the-art fail to\nlearn tag models able to generalize to datasets from different\norigins. Third we show that current performance level of a\ndirect mapping between audio features and tags still appears\ninsufï¬cient to enable the possibility of exploiting natural tag\ncorrelations as a second stage to improve performance.\n1. INTRODUCTION\nMusic autotagging refers to the task of automatically clas-\nsifying music audio excerpts with respect to a number of\nhigh-level concepts (the â€œtagsâ€) from potentially very di-\nverse music facets such as Emotion, Musical instruments,\nGenre, Usage, etc. In the literature, a number of approaches\nto the task have been proposed that build upon previous\nwork in genre and artist classiï¬cation, where a direct map-\nping is sought via machine learning models between low-\nlevel features computed on short audio signal frames and\ntags [2, 4, 10, 11]. These approaches are tailored to the fact\nthat the task is more difï¬cult than genre classiï¬cation in that\nthe number of classes is usually much higher (genres corre-\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proï¬t or commercial advantage and that copies\nbear this notice and the full citation on the ï¬rst page.\nc/circlecopyrt2011 International Society for Music Information Retrieval.\nAudio feature extractionLearning algo\nLearning algoStage 1Stage 2\nBinarizationevalNfeatsÂ¯x=0.02\nfeature values...[0Â· Â· Â·1]\ntag probab.Ntags...p(guitar)=0.001p(sof t)=0.03\n{0,1}\ntag membershipp(guitar)=0p(soft)=1...Ntags[0Â· Â· Â·1]\ntag probab....p(guitar)=0.05p(sof t)=0.02Binarizationeval{0,1}\ntag membershipp(soft)=1...p(guitar)=1\naudioFigure 1 . Generic 2-stage music autotagging framework\n(training of learning algorithms not represented; audio fea-\nture extraction can be statistics or time series).\nspond in fact to one among many facets), and models must\naccount for the possibility that multiple labels usually apply\nto a given excerpt. Music tags are often correlated (for in-\nstance, Genre tags often co-occur with Instruments or Emo-\ntion tags), this is often the rationale behind implementing\na 2-stage architecture, where a second stage of processing,\nmodeling tag co-occurence relationships, can â€œcorrectâ€ [8]\nthe imperfect tag predictions of the ï¬rst stage (see illustra-\ntion in ï¬gure 1). A number of authors report on perfor-\nmance improvements with this procedure over the one-stage\napproach [1, 6â€“9].\nThis paper aims at demonstrating via systematic experi-\nments the relevance of a number of music autotagging issues\nthat we believe are, to the best of our knowledge, only ad-\ndressed superï¬cially in current literature. After presenting\nthe data and systems used and reporting on initial experi-\nments in sections 2 and 3, we address in section 4 the notion\nof â€œfragilityâ€ of evaluation methodologies and stress a num-\nber of methodological recommendations. In section 5, we\naddress the issue of generality of autotagging models, and\nin section 6, we address limitations of exploiting tag corre-\nlations in a second processing stage. We ï¬nally propose a\ndiscussion on these issues and directions for future work in\nsection 7.\n2. DATA AND SIGNAL FEATURES\nIn this paper we use two datasets with tag annotations made\navailable publicly to the community by fellow researchers\n795Oral Session 10\nand on which a number of papers have reported results.\nCAL500. The Computer Audition Lab 500 (CAL500)\ndataset ( http://cosmal.ucsd.edu/cal/projects/AnnRet/ )\nis made up of 500 Western popular song excerpts of differ-\nent lengths. Excerpts annotations are among a set of 174\ntags.\nMagnatagatune. The Magnatagatune dataset ( http://\ntagatune.org/Magnatagatune.html ) consists of 21642 ex-\ncerpts of length 30 s from 230 different artists. Excerpts an-\nnotations are among a set of 188 tags. Some pre-processing\nwas applied to yield a cleaner dataset, referred to as Mag-\ntag5k (see section 4.2 for more details), on which we ran\nmost of the experiments below.\nOther datasets. We made use of two other publicly\navailable datasets with only genre annotations: the Latin\nMusic Dataset (LMD, http://www.ppgia.pucpr.br/ Ëœsilla/\nlmd/index.html ) and the ISMIR04 dataset ( http://ismir\n2004.ismir.net/genre_contest/index.htm ) to evaluate the\ngeneralization capacity of our autotagging systems (see sec-\ntion 5).\nFeatures. We used MARSYAS to extract 16 audio fea-\ntures from 46ms frames of the audio signals with no overlap.\nThe features are: the spectral centroid, rolloff frequency,\nspectral ï¬‚ux, and 13 MFCCs, including MFCC0. These fea-\ntures as the same ones used in [7].\n3. AUTOTAGGING SYSTEMS\n3.1 Benchmark\nIn order to better compare our experiments with previous\nliterature and to facilitate the reproducibility of our experi-\nments, we use as a benchmark the system proposed in [7],\nwhich is available under GPL in MARSYAS.1Performance\nof the Benchmark have been reported in the 2010 MIREX\nevaluation. In this system, frame features are collapsed in\na two steps process (texture windowing and computation of\nglobal mean and standard deviation) into a 64-dimensional\nfeature vector for the whole audio excerpt [7]. This system\nimplements an architecture with two stages of processing,\nillustrated in ï¬gure 1. A multiclass SVM classiï¬er is used\nin both stages. We report below on the performance of using\njust the ï¬rst stage of processing alone, or the whole system.\n3.2 Alternative systems\n1.External multiclass SVM in both stages : This sys-\ntem (referred to as Sys1 ) is a 2-stage system similar\nto the Benchmark, with the difference that it external-\nizes the learning algorithm and directly uses the lib-\nSVM software package ( http://www.csie.ntu.edu.\n1The authors are grateful to Ness & Tzakenakis for kindly providing\nand commenting the code used for these experiments.tw/Ëœcjlin/libsvm ). The other difference is that nor-\nmalization of the data is done via the libSVM package\nand not in the MARSYAS code.\n2.One-stage Markov models-based classiï¬er : This ap-\nproach consists of using the method for genre classiï¬-\ncation based on Markov models previously described\nin [5]. In the context of autotagging, for each tag a\npair of models are estimated and used to assign a tag\nto a piece of audio. This approach is referred to as\nSys2 .\n3.3 Autotagging performance\nCAL500 Magtag5k\nBenchmark 0.452 |0.245 0.312 |0.083\nSys1 0.464 |0.269 0.423 |0.176\nSys2 0.480 |0.246 0.411 |0.171\nTable 1 . F-score g|F-score ptfor Benchmark, Sys1, and\nSys2 on CAL500 and Magtag5k. Evaluation methodology\ndescribed in section 4.2.\nTable 1 presents a comparison of the performance achieved\nwith the methods described previously and the performance\nobtained with the Benchmark. The performance measure is\nthe F-score computed on global classiï¬cation rates (denoted\nF-score g) and the F-score based on the average per-tag clas-\nsiï¬cation rates (denoted F-score pt, see section 4.1 for fur-\nther methodological considerations). For both datasets Sys1\nand Sys2 perform better than the Benchmark albeit in small\nproportions in some cases. The Benchmark was chosen in\norder to have a fair point of comparison to evaluate our ap-\nproaches: it is a recent contribution that rates among the best\nin the latest MIREX evaluation (2010).\nOther examples using the same datasets can be found in\nthe literature: Using CAL500, Turnbull et al. [11], Hoffman\net al. [4] and Mahieux et al. [2] obtain F-scores ptequal to\n0.20, 0.21 and 0.14 respectively but the evaluation is based\non a ranking of the ï¬rst 10 most probable tags and thus\nnot comparable with our results. Seyerlehner et al. [9] ob-\ntains F-score g= 0.50and F-score pt= 0.30on CAL500\nand 0.42|0.22 with the Magnatagatune dataset thus slightly\nabove our results. Zhao et al. [12] achieve F-score pt= 0.31\non CAL500 but tags that were not recognized in the dataset\nwere ignored in the evaluation (using this metric we were\nable to achieve F-score pt= 0.33using Sys2). Similarly,\nMiotto et al. [6] obtain a F-Score pt= 0.30on CAL500 but\nless frequent tags were removed which, as we will see in the\nnext section, affects signiï¬cantly the results. To summarize,\nwe claim that the approaches presented in this paper are on\na par with the state-of-the-art as described in the recent lit-\nerature.\n79612th International Society for Music Information Retrieval Conference (ISMIR 2011)\n4. ISSUE 1: METHODOLOGICAL ISSUES IN\nEV ALUATING AUTOTAGGING SYSTEMS\n4.1 On evaluation measures\nEvaluation for autotagging systems is mostly based on In-\nformation Retrieval measures, such as accuracy, precision,\nrecall, F-score, etc. These measures are generally computed\non a per-tag basis, separately for each tag and then averaged\nacross tags, or globally across the whole dataset. Music\ndatasets typically have a strong imbalance in tag distribu-\ntions, and results on a per-tag or global basis can differ sig-\nniï¬cantly. This imbalance drives global scores artiï¬cially\nhigh. The reason is simple: since the most common tags\naccount for a large percentage of all annotations, classiï¬ers\nthat predict these tags well start off with high global scores.\nFigure 2 shows the F-scores on CAL500 for Sys1 and Sys2,\nwhen the most frequent tags (left) or the least frequent tags\n(right) are removed from the dataset (tests with Magtag5k\nhad a similar outcome). Results conï¬rm the dependence\nof global scores on the most common tags [2, 6, 11]: the\nleft plot shows a sharp decrease in F-scores gwhen the top\ntags are removed (F-scores ptalso decrease, albeit relatively\nless). This indicates that the most frequent tags are on av-\nerage better classiï¬ed and have a substantial effect on the\noverall performance. This is also seen in ï¬gure 6, where the\nmost common tags (the ones represented by larger circles)\nhave high scores, and the least frequent tags low scores. On\nthe other hand, ï¬gure 2 (right plot) also shows that the least\nfrequent tags, with lower classiï¬cation rates, have little im-\npact on global scores but have a dramatic effect on per-tag\nscores, a fact that is most often ignored. We therefore stress\nthe importance when reporting results on reference data to\ninclude both global and per-tag metrics, and to consider the\ninï¬‚uence of both the least and most frequent tags. For in-\nstance, in [6] the evaluation is obtained excluding the 77\nleast frequent tags, which in our systems would result in a\nincrease in the F-scores ptabove 10%.\n20 40 60 80 1000.10.20.30.40.50.6\nnâ—¦of tags removedF-ScoresMost Frequent Tags\n  \nF-score pt\nF-score g\nSys1\nSys2\n20 40 60 80 1000.10.20.30.40.50.6\nnâ—¦of tags removedLeast Frequent Tags\n  \nF-score pt\nF-score g\nSys1\nSys2\nFigure 2 . F-score gand F-score pton CAL500 for Sys1 and\nSys2 autotaggers, as the most frequent (left) or the least fre-\nquent tags (right) are removed.Another important factor that can inï¬‚uence performance\nscores is how thoroughly the songs in the dataset are an-\nnotated. CAL500 has a high number of tags per song (an\naverage of 26tags per song): a trivial classiï¬er (i.e. always\npredicting all tags) has a precision of â‰ˆ15% (with 100% re-\ncall). This â€œstarting pointâ€ yields a F-score ptof26%, which\nis misleadingly high, and almost on a par with other results\nreported in the literature (see the F-scores ptreported in sec-\ntion 3.3). Note that in this case the F-score gequals F-score pt\nand is much lower than what is reported in the literature,\nhence a good indicator of the systemâ€™s sub-optimal perfor-\nmance.\nThe choice of evaluation measure can hinder compar-\nisons between different methods and can also conceal sub-\noptimal performances. It is therefore important to report\nboth per-tag and global scores, and ideally, also document\nhow the individual tag performances are related to the a pri-\nori tag frequencies in the datasets used.\n4.2 On data and evaluation methodology\nDepending on the data gathering method, tag-annotated data-\nsets can present several problems [11] such as misspelling,\nimpossible combinations of values, diverse types of noise,\netc. However, only few papers consider these potential prob-\nlems when reporting on autotagging experiments with the\nCAL500 or Magnatagatune datasets.\nThe Magnatagatune dataset reveals a signiï¬cant number\nof problems with annotation: (1) synonymy : we merged a\nnumber of tags (e.g. â€œclasicalâ€, â€œclassicalâ€ and â€œclassicâ€),\n(2)trivial cases : we removed excerpts with tags such as\ne.g. â€œsilenceâ€, (3) antonymy : we removed tag attributions\nof an excerpt when they were not compatible (e.g. having\nboth â€œdrumsâ€ and â€œno-drumsâ€ tags, or â€œfastâ€ and â€œslowâ€),\n(4)extreme sparseness : we removed excerpts with no tags,\nand (5) duplication : many excerpts in the Magnatagatune\ndataset are segments of the same original piece and have\ndifferent tag annotations, we kept those segments with the\nmaximum number of tags and removed the other segments.\nAfter pre-processing the Magnatagatune dataset as detailed\nabove, the remaining data, referred henceforth as Magtag5k ,\nconsists of 137 tags, 5259 excerpts from 230 artists. CAL500\ndid not require such pre-processing.\nTo avoid overï¬tting the data in building autotagging mod-\nels, the literature fosters a number of evaluation methodolo-\ngies, e.g. holdout validation, S-fold cross-validation, etc.\nHowever, it seldom takes into account artist ï¬ltering in the\ndeï¬nition of the training and test datasets, a method whose\nimportance has been demonstrated in music similarity re-\nsearch [3] (over-optimistic results can be achieved when the\nsame artists are present in both sets). Taking this additional\nfactor into account, the evaluation methodology should agree\nwith a number of constraints related to the statistics of the\ndata, i.e. the number of folds should not be higher than the\n797Oral Session 10\nnumber of artists per tag, nor than the number of excerpts\nper tag. For instance, constraints from CAL500 favors a\n2-fold cross-validation or holdout validation (instead of 10-\nfold cross-validation [11]). We report results with the lat-\nter (with a 50% split). In Magtag5k, some tags have few\ninstances, from few artists (e.g. tag â€œwaterâ€ has 16 songs\nfrom 6 artists). Thus, we chose to set the maximum number\nof folds to 3 (ensuring at least 2 different artists per tag per\nfold) and report on results with 3-fold cross-validation. We\ncan clearly see in table 2 that very different results are ob-\ntained when considering data and methodology issues dis-\ncussed here and when not. To facilitate reproducible re-\nsearch, the whole Magtag5k data pre-processing and result-\ning data are available2.\n5. ISSUE 2: WHAT ARE WE REALLY LEARNING?\nIn this section we present results of a set of experiments that\nwere conducted in order to evaluate the extent of the results\nobtained with the various systems. The objective was to\nevaluate modelsâ€™ ability to generalize when used with data\nfrom different origins. We selected songs annotated with 35\ntags common to both Magtag5k and CAL500.3Figure 3\nshows for both the Benchmark (left) and Sys2 (right) two\nF-scores for each of the 35 tags, these F-scores are obtained\nwith Magtag5k as testset, but with two different training\nsets for building models, either Magtag5k or CAL500.4\nThe F-score obtained with CAL500 is shown on the hor-\nizontal axis while the F-score obtained with is Magtag5k\nshown on the vertical axis. On these plots a model that per-\nform equally well when trained with either datasets would\nbe on the diagonal, those performing worse when trained\nwith CAL500 data are above the diagonal.\nWhen comparing performance obtained on the same test\nset (Magtag5k) we observe much lower performance for\nmodels based on CAL500 training than for those trained\nwith Magtag5k. This observation is valid for the Bench-\nmark, Sys1 (not shown here) and Sys2. Nearly every point\nis above the diagonal. Sys2 seems to perform slightly bet-\nter than other systems in terms of generalization but still the\nperformance is much lower for models trained with CAL500:\nonly three tags obtain a relatively high F-score for both train-\ning sets ( man.singing ,electro , and female.singing ).\nModels were also tested on the LMD and the ISMIR04\ngenre classiï¬cation datasets. These two datasets were not\ncreated for autotagging tasks therefore no ground truth is\navailable so our analysis is based on tag assignment fre-\nquency. We processed the music pieces from these datasets\nwith Sys1 models trained with CAL500 and Magtag5k. Fig-\n2Please follow this link: http://tl.di.fc.ul.pt/t/\nmagtag5k.zip .\n3Hence reducing Magtag5k to 4549 songs.\n4Note that artist ï¬ltering and non-overlap of training and test data are\nobserved for Magtag5k.\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.700.10.20.30.40.50.60.7\nacoustic.guitarambient\nbassdrums\nelectric.guitarfemale.singing\nhornsman.singing\norganpiano\nsaxstrings\nsynth\ntrumpetviolins\nduettalkingacoustic\nbluescountryelectro\nfolk\nfunk\nhip.hopjazzpoprock\nworldpunk\nsoft.rocksadsoft\nweird\nmellowhappyF-score pt\nTraining set: CAL500Training set: Magtag5kBenchmark\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.700.10.20.30.40.50.60.7\nacoustic.guitarambient\nbassdrums\nelectric.guitarfemale.singing\nhornsman.singing\norganpiano\nsaxstrings\nsynth\ntrumpetviolins\nduettalkingacoustic\nbluescountryelectro\nfolk funkhip.hopjazzpoprock\nworldpunksoft.rocksadsoft\nweird\nmellowhappyF-score pt\nTraining set: CAL500Sys2Figure 3 . F-score on Magtag5k testset for Sys2 (right) and\nBenchmark (left) autotaggers, either trained with CAL500\n(x-axis) or Magtag5k ( y-axis).\n00.51CAL500\nacoustic.guitarbassdrumselectric.guitarfemale.singinghornsman.singingorganpianosaxstringssynthtrumpetviolinsduettalkingacousticambientbluescountryelectrofolkfunkhip.hopjazzpoprockworldpunksoft.rocksadsoftweirdmellowhappy00.51Magtag5k\n00.51LMD\n00.51ISMIR\n  \nTraining set: CAL500 Training set: Magtag5k\nFigure 4 . Proportion of music pieces for which each tag\nwas assigned in the corresponding test set (rows). Sys1.\nure 4 shows the proportion of songs from a given test set to\nwhich each tag was assigned. Each color/shade corresponds\nto a training set and each row to a test set. We can see for ex-\nample that when testing with CAL500 (ï¬rst row) and train-\ning with Magtag5k (orange, light shade) nine tags are as-\nsigned to all songs. When testing with Magtag5k (second\nrow), models trained with CAL500 (blue, dark shade) rec-\nognize very few tags. When testing on LMD and ISMIR04\nwe observe a strange phenomenon: the proportion of music\nper tag is almost the same for both datasets and for all tags.\nThis indicates a strong bias on the models side and a weak\npower of generalization.\nFigure 5 shows the proportion of music pieces for which\neach tag was selected when trained with Magtag5k and tested\nwith both LMD and ISMIR04 datasets (different colors) for\ntwo modeling techniques (different rows). The ï¬rst row\nconï¬rms what was seen on ï¬gure 4: with Sys1 the pro-\nportion of songs per tag is almost the same independently\nof the test set. When Sys2 is used, a different anomaly is\nobserved: very few tags are recognized and these tags are\nover-represented. Moreover the same tags seem to be over-\n79812th International Society for Music Information Retrieval Conference (ISMIR 2011)\n00.51Sys1\nacoustic.guitarbassdrumselectric.guitarfemale.singinghornsman.singingorganpianosaxstringssynthtrumpetviolinsduettalkingacousticambientbluescountryelectrofolkfunkhip.hopjazzpoprockworldpunksoft.rocksadsoftweirdmellowhappy00.51Sys2\n  \nLMD testset ISMIR testsetFigure 5 . Proportion of music pieces for which each tag\nwas assigned for two kinds of models (rows) and two test\nsets (colors).\nrepresented in both datasets. When comparing the two rows\nof the plot, we can see that the two autotagging techniques\nhave a very low level of agreement, for both test sets.\nThese experiments show that models obtained with au-\ntotagging techniques at the level of the state-of-the-art show\nvery limited ability to generalize to new datasets and that the\nlevel of performance observed on a single ï¬nite dataset is\nsomewhat misleading. Current autotagging techniques are\nstill far from the long-term goal that is to allow automatic\ntagging of sounds independently of their origin.\n6. ISSUE 3: EXPLOITING TAG CORRELATIONS\nIN A SECOND PROCESSING STAGE\nMagtag5k 2-fold\nBench. stage 1 0.409 |0.164 0.342 |0.126\nBench. both stages 0.312 |0.083 0.347 |0.136\nSys1 stage 1 0.411 |0.165 0.341 |0.127\nSys1 both stages 0.423 |0.176 0.347 |0.136\nTable 2 . Comparison of F-score g|F-score ptfor differ-\nent conï¬gurations of the Magnatagatune dataset: Mag-\ntag5k, and 2-fold cross-validation over unprocessed Mag-\nnatagatune dataset (no artist ï¬lter).\nIn table 2, we compare Sys1 against the Benchmark, con-\nsidering either stage 1 only or both stages. The ï¬rst column\nreports results on Magtag5k while the second reports results\nwith the data and evaluation methodology from [7]: 2-fold\nover the whole Magnatagatune data, without artist ï¬ltering.\nLooking at results for the Benchmark, we can see that al-\nthough results of the ï¬rst stage (ï¬rst row, second column)\nare very similar to those published in [7], the second stage in\nfact impairs results from the ï¬rst stage only, i.e. the opposite\nphenomenon than [7]. Similar improvements for the second\nstage as those published can only be found when consider-\ning unadapted evaluation methodologies (e.g. no artist ï¬lter)\n0 0.2 0.4 0.6 0.800.20.40.60.8\nstage 1stage 2Benchmark\n0 0.2 0.4 0.6 0.800.20.40.60.8\nstage 1Sys1\n0 0.2 0.4 0.6 0.800.20.40.60.8\nstage 1Sys3Figure 6 . Performance of stage 1 vs both stages, Magtag5k.\nIndividual tag F-scores are represented by circle centers. x-\naxis are the stage 1 F-scores, and y-axis both stages. Radius\nare proportional to corresponding tag frequency.\nand noisy (see problems 1 to 4 in section 4.2) and redundant\ndata (see problem 5), as illustrated in the second column.\nResults also show that the second stage of Sys1 does ap-\npear to bring a small improvement on the ï¬rst stage. How-\never, we can gain more insights on the actual effect of the\nsecond stage by looking at ï¬gure 6 which illustrates the dif-\nference in tagâ€™s individual F-scores between using only one\nstage of processing vs using both stages. For a given data\npoint (i.e. a particular tag) to lie above the diagonal means\nthat the second stage improves results, while below the diag-\nonal means impairing results from stage 1. For the Bench-\nmark (left plot), the decrease in overall performance can be\nseen on almost all tags individually. For Sys1 (middle plot),\nif average results are better with both stages, we can see that\nnot all tags are affected in the same way by the second stage:\nsome improve (are above the diagonal) while others do not.\nIn our opinion, this distribution around both sides of the di-\nagonal indicates that no clear pattern of improvement can be\nidentiï¬ed with the 2-stage procedure.\nA possible reason for the inability of the system to take\nadvantage of existing tag correlations may reside in the na-\nture of the second stage classiï¬er. Hence we experimented\na different option for the second stage: a pool of binary\nSVMs (one per tag) [8]. These experiments are restricted\nto the particular task of tag co-occurrence modeling, i.e. we\ncompare classiï¬ers that process correct input (we are not\nevaluating the full system here, only what can serve as its\nsecond stage). Results show that binary SVMs are clearly\nbetter at the task than a multiclass SVM: in three-fold cross-\nvalidation on Magtag5k the former reaches a F-score gand\nF-score ptof 0.839 and 0.822 respectively while the latter\nreaches 0.581 and 0.567. A corollary of the above is that\nthe second stage may fail precisely because it is trained on\ndata that only represents estimations of these correlations\n(and relatively bad ones, as indicated by the performance\nof stage 1). Hence we modiï¬ed Sys1 with binary SVMs in\nstage 2, trained with true tag annotations instead of proba-\nbility estimations from stage 1. We refer to this system as\nSys3 . Overall, Sys3 reaches F-score gand F-score ptof 0.411\n799Oral Session 10\nand 0.162, therefore slightly below the performance of Sys1\nand comparable to using only stage 1 (see table 2). However,\nwhen looking at the case of individual tags, i.e. rightmost\nplot of ï¬gure 6, we can spot an interesting pattern: improve-\nments with stage 2 seem higher for tags with better perfor-\nmance in stage 1. In other words, this seems to indicate that\na minimum performance in stage 1 should be expected for a\ngiven tag â€”i.e. for its probability estimationâ€” to be useful\nin a second stage. Although proving this claim will require\nmore data, we wish to argue here that this pattern appears\nas a logical and desirable property for an autotagging sys-\ntem, and it indicates clear directions for future work: e.g.\nimproving stage 1; tailoring stage 2 classiï¬er to a selection\nof particular tags (e.g. the most reliable, the most â€œinï¬‚uen-\ntialâ€ [1]) instead of processing all tags the same way.\n7. DISCUSSION\nThe experiments described in this paper show that diverse\ntechniques on a par with the state-of-the art in music auto-\ntagging fail to achieve their goal in several aspects. It was\nshown that autotagging tasks must be evaluated more care-\nfully than what is usually done, that changing the set of tags\nor altering the evaluation measure (per tag vs global F-score)\nmay dramatically alter the results, sometimes hiding weak-\nnesses. It was also shown that current techniques used for\nautotagging fail the generalization test. Finally it was shown\nthat the performance achieved with these techniques is not\nsufï¬cient to be able to take advantage of the correlations be-\ntween tags. Research in music genre classiï¬cation and mu-\nsic similarity has seen recent progresses but its adaptation to\nautotagging shows severe drawbacks. What are the causes\nof these relatively negative results?\nIt is our opinion that some key differences between auto-\ntagging and genre classiï¬cation should be given more em-\nphasis in autotagging research. In particular with regards\nto data recollection and annotation [10]. Tags can corre-\nspond to music facets more subjective than music genre. Or\nthey can have multiple meanings, as in the case of Instru-\nment tags: a song tagged â€œpianoâ€ can mean e.g. that piano\nis salient all over the song, or that there is a piano accom-\npanying (but it may be relatively quiet), or that some parts\nhave piano (but may have a short temporal span). In au-\ntotagging the procedure used to obtain ground truth differs\nfrom one dataset to another, which results in a lack of con-\nsistency. Public datasets are limited in quantity and in many\ncases present errors or incompleteness. Also, where datasets\nfor genre classiï¬cation are usually limited to 10-20 genres,\nit is common to deal with hundreds of tags. This is not a\nproblem per-se but in these conditions it is much more dif-\nï¬cult to achieve good results for every tags and to follow\ngood practices (artist ï¬ltering, S-fold cross validation). It\nis hard to build models based on extremely unbalanced databut it is even harder if the ground truth lacks consistency.\nFuture work will include seeking for improvements in terms\nof generalization using recently published datasets like the\nMillion Songs or CAL10k datasets.\nThis paperâ€™s results and previous observations lead us to\npropose some directions regarding future work in music au-\ntotagging: Different processing could be applied depending\non categories of tags: (1) 2-stage architectures may be ben-\neï¬cial for some tags (e.g. tags with reasonable performance\nmight help build models for other tags) but not for others\n(discussion in [1] is also insightful on this matter). (2) Tag\nmodels could be differentiated according to temporal char-\nacteristics: models for tags that correspond to a short time\nspan should be based on local features whereas tags that cor-\nrespond to whole songs should use global features.\n8. ACKNOWLEDGMENTS\nThanks to Alessandro Koerich, Luiz Oliveira and Alceu Brito in Curitiba,\nthis research was supported by FCT and QREN-AdI grant for the project\nPalco3.0/3121, by FCT through LASIGE Multiannual Funding and VIRUS\nresearch project (PTDC/EIAEIA/101012/2008). The ï¬rst author is sup-\nported by PROTEC grant SFRH/BD/50118/2009.\n9. REFERENCES\n[1] J.-J. Aucouturier. Language, Evolution and the Brain, Frontiers in\nLinguistics Series , chapter Sounds like Teen Spirit: Computational\nInsights into the Grounding of Everyday Musical Terms. Academia\nSinica Press, 2009.\n[2] T. Bertin-Mahieux, D. Eck, F. Maillet, and P. Lamere. Autotagger: A\nmodel for predicting social tags from acoustic features on large music\ndatabases. JNMR , 37(2), 2008.\n[3] A. Flexer. A closer look on artists ï¬lters for musical genre classiï¬ca-\ntion. In ISMIR , 2007.\n[4] M. Hoffman, D. Blei, and P. Cook. Easy as CBA: A simple probabilis-\ntic model for tagging music. In ISMIR , 2009.\n[5] T. Langlois and G. Marques. A music classiï¬cation method based on\ntimbral features. In ISMIR , 2009.\n[6] R. Miotto, L. Barrington, and G. Lanckriet. Improving auto-tagging by\nmodeling semantic co-occurrences. In ISMIR , 2010.\n[7] S. Ness, A. Theocharis, G. Tzanetakis, and L. Martins. Improving auto-\nmatic music tag annotation using stacked generalization of probabilis-\ntic SVM outputs. In ACM Multimedia , 2009.\n[8] F. Pachet and P. Roy. Improving multilabel analysis of music titles:\nA large-scale validation of the correction approach. IEEE TASLP ,\n17(2):335 â€“343, 2009.\n[9] K. Seyerlehner, G. Widmer, M. Schedl, and P. Knees. Automatic music\ntag classiï¬cation based on block-level features. In SMC Conference ,\n2010.\n[10] D. Tingle, Y . E. Kim, and D. Turnbull. Exploring automatic music an-\nnotation with â€œacoustically-objectiveâ€ tags. In ACM Int. Conf. on Mul-\ntimedia Information Retrieval , 2010.\n[11] D. Turnbull, Barrington. L., D. Torres, and G. Lanckriet. Semantic\nannotation and retrieval of music and sound effects. IEEE TASLP ,\n16(2):467â€“476, 2008.\n[12] Z. Zhao, X. Wang, Q. Xiang, A. M. Sarroff, Z. Li, and Y . Wang. Large-\nscale music tag recommendation with explicit multiple attributes. In\nACM Multimedia , 2010.\n800"
    },
    {
        "title": "New Trends in Musical Genre Classification Using Optimum-Path Forest.",
        "author": [
            "Caio Miguel Marques",
            "Ivan Rizzo Guilherme",
            "Rodrigo Y. M. Nakamura",
            "JoÃ£o P. Papa"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1417657",
        "url": "https://doi.org/10.5281/zenodo.1417657",
        "ee": "https://zenodo.org/records/1417657/files/MarquesGNP11.pdf",
        "abstract": "Musical genre classification has been paramount in the last years, mainly in large multimedia datasets, in which new songs and genres can be added at every moment by anyone. In this context, we have seen the growing of musical recommendation systems, which can improve the benefits for several applications, such as social networks and collective musical libraries. In this work, we have introduced a recent machine learning technique named Optimum-Path Forest (OPF) for musical genre classification, which has been demonstrated to be similar to the state-of-the-art pattern recognition techniques, but much faster for some applications. Experiments in two public datasets were conducted against Support Vector Machines and a Bayesian classifier to show the validity of our work. In addition, we have executed an experiment using very recent hybrid feature selection techniques based on OPF to speed up feature extraction process.",
        "zenodo_id": 1417657,
        "dblp_key": "conf/ismir/MarquesGNP11",
        "keywords": [
            "Optimum-Path Forest (OPF)",
            "musical genre classification",
            "machine learning technique",
            "Support Vector Machines",
            "Bayesian classifier",
            "public datasets",
            "feature selection techniques",
            "feature extraction process",
            "social networks",
            "collective musical libraries"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nNEWTRENDS IN MUSICALGENRE CLASSIFICATIONUSING\nOPTIMUM-PATHFOREST\nC.Marques,I.R.Guilherme\nUNESP- UnivEstadualPaulista,Rio Claro, SP\nDep. of Statistics,Applied Math. andComputation\nmarques@caiena.net,ivan@rc.unesp.brR.Y.M.Nakamura,J. P.Papa\nUNESP- UnivEstadualPaulista,Bauru, SP\nDepartment ofComputing\n{rodrigo.mizobe,papa }@fc.unesp.br\nABSTRACT\nMusical genre classiï¬cation has been paramount in the last\nyears, mainly in large multimedia datasets, in which new\nsongs and genres can be added at every moment by any-\none. In this context, we have seen the growing of musical\nrecommendation systems, which can improve the beneï¬ts\nfor several applications, such as social networks and col-\nlective musical libraries. In this work, we have introduced\na recent machine learning technique named Optimum-Path\nForest (OPF) for musical genre classiï¬cation, which has\nbeen demonstrated to be similar to the state-of-the-art pat -\nternrecognitiontechniques,butmuchfasterforsomeappli -\ncations. Experimentsintwopublicdatasetswereconducted\nagainst Support Vector Machines and a Bayesian classiï¬er\nto show the validity of our work. In addition, we have exe-\ncuted an experiment using very recent hybrid feature selec-\ntiontechniquesbasedonOPFtospeedupfeatureextraction\nprocess.\n1. INTRODUCTION\nRecently, advances in technology have supported the stor-\nage of large amount of data. Therefore, fast information\nretrieval became a hot challenge. One of the most interest-\ning applications concerns with social network users, which\nhave looked forward to meet people that share common\npreferences, and also to discover new good music. Thus,\nan important task in this context is the music classiï¬cation\ninto different genres aiming a better organization of music\ndatasets, for furtherrecommendation.\nTzanetakis and Cook [22] proposed a work to deal with\nthe problem of musical genre classiï¬cation using three sets\nof features representing timbral texture, rhythmic and pit ch\ncontents, together with K-Nearest Neighbors and Gaussian\nPermission to make digital or hard copies of all or part of this w ork for\npersonal or classroom use is granted without fee provided th at copies are\nnot made or distributed for proï¬t or commercial advantage and th at copies\nbear this noticeand thefull citation ontheï¬rst page.\nc/circlecopyrt2011International Society forMusic InformationRetrieval .Mixture Models. Lambrou et al. [11] applied statistical fea -\ntures in temporal domain and three different wavelet trans-\nforms for the same task, and Soltau et al. [21] proposed a\nnew architecture called ETM-NN (Explicit Time Modeling\nwith Neural Networks), which employs statistical analysis\nof atemporal structure.\nXu et al. [24] applied Support Vector Machines (SVMs)\nto perform a hierarchical classiï¬cation of different music al\ngenres,andDellandreaetal.[6]comparedSVMswithNeu-\nralNetworksinthesamecontext. ChanandVasconcelos[2]\nproposed a Dynamic Texture Model (DTM) for automatic\nmusicsegmentation,andCovielloetal.[4]introducedDTM\nin the context of music tagging. McKay and Fujinaga [13]\nproposed a novel hybrid system to handle automatic classi-\nï¬cation of musical genres composed by a Feedfoward Neu-\nral Network and K-Nearest Neighbors algorithm together\nwithGeneticAlgorithms(GA)forfeatureselection. Finall y,\nDeepa et al. [5] used a brute force method for feature opti-\nmization using different feature vectors with SVMs. The\nidea istocombine the best ones at the ï¬nal of theprocess.\nIn order to combine efï¬ciency for training and effective-\nness in the classiï¬cation task, a novel framework that re-\nduces the pattern recognition problem to an optimum path\nforest computation (OPF) in the feature space induced by\na graph was presented in its unsupervised [20] and super-\nvised versions [14]. The OPF-based classiï¬ers do not in-\nterpret the classiï¬cation task as a hyperplanes optimizati on\nproblem, but as a combinatorial optimum-path computation\nfromsomekeysamples(prototypes)totheremainingnodes.\nEach prototype becomes a root from its optimum-path tree\nand each node is classiï¬ed according to its strongly con-\nnected prototype, that deï¬nes a discrete optimal partition\n(inï¬‚uence region) of the feature space. The OPF frame-\nwork has some advantages with respect to the aforemen-\ntioned classiï¬ers: (i) it is free of parameters (supervised\nversion), (ii) does not assume any shape/separability of th e\nfeature space and (iii)it runs trainingphase faster.\nIn this paper, we propose to introduce the supervised\nOPFinthecontextofmusicalgenreclassiï¬cation. Asfaras\nwe know, we are the ï¬rst to apply OPF for this task. In re-\n699Poster Session 6\ngardtofeatureselectioninthecontextofmusicalgenrecla s-\nsiï¬cation, it is not usual to ï¬nd many works on that. There-\nfore, we would like to shed light over that another main\ncontribution of this paper is to introduce three recently de -\nvelopedfeatureselectiontechniquesaimingtoimprovemu-\nsical genre classiï¬cation: HS-OPF (Harmony Search with\nOPF) [18], PSO-OPF (Particle Swarm Optimization with\nOPF) [17] and GSA-OPF (Gravitational Search Algorithm\nwith OPF) [15]. The experiments are conducted in two\nrounds: (i)intheformer,OPFiscomparedwithSVMsanda\nBayesian classiï¬er, and (ii) in the second round we present\na comparison between HS-OPF, PSO-OPF and GSA-OPF\nforfeatureselectioninthecontextofmusicalgenreclassi ï¬-\ncation. The remainder of the paper is organized as follows.\nTheOPFtheoryispresentedinSection2. Theexperimental\nresults are discussed in Section 3. Finally, conclusions ar e\nstatedinSection 4.\n2. SUPERVISED OPTIMUM-PATH FOREST\nThe OPF classiï¬er works by modeling the problem of pat-\nternrecognitionasagraphpartitioninagivenfeaturespac e.\nThe nodes are represented by the feature vectors and the\nedges connect all pairs of them, deï¬ning a full connected-\nness graph. This kind of representation is straightforward ,\ngiven that the graph does not need to be explicitly repre-\nsented, allowing us to save memory. The partition of the\ngraphiscarriedoutbyacompetitionprocessbetweensome\nkey samples ( prototypes ), which offer optimum paths to the\nremaining nodes of the graph. Each prototype sample de-\nï¬nes its optimum-path tree (OPT), and the collection of all\nOPTsdeï¬nesanoptimum-pathforest,whichgivesthename\ntothe classiï¬er [14].\nTheOPFcanbeseenasageneralizationofthewellknown\nDijkstraâ€™salgorithmtocomputeoptimumpathsfromasource\nnode to the remaining ones [7]. The main difference relies\non the fact that OPF uses a set of source nodes (prototypes)\nwithanypath-costfunction. IncaseofDijkstraâ€™salgorith m,\nafunctionthatsummedthearc-weightsalongapathwasap-\nplied. For OPF, we used a function that gives the maximum\narc-weight along a path, as explained before. Next section\nstates OPFtheory.\n2.1 Background Theory\nLetZ=Z1âˆªZ2be a dataset labeled with a function Î»,\nin which Z1andZ2are, respectively, a training and test\nsets such that Z1is used to train a given classiï¬er and Z2\nisusedtoassessitsaccuracy. Let SâŠ†Z1asetofprototype\nsamples. Essentially, the OPF classiï¬er creates a discrete\noptimal partition of the feature space such that any sample\nsâˆˆZ2can be classiï¬ed according to this partition. This\npartition is an optimum path forest (OPF) computed in â„œn\nby theimage forestingtransform (IFT)algorithm [8].The OPF algorithm may be used with any smoothpath-\ncost function which can group samples with similar proper-\nties [8]. Particularly, we used the path-cost function fmax,\nwhich iscomputed as follows:\nfmax(/a\\}bracketle{ts/a\\}bracketri}ht) =/braceleftbigg0ifsâˆˆS,\n+âˆotherwise\nfmax(Ï€Â·/a\\}bracketle{ts,t/a\\}bracketri}ht) = max {fmax(Ï€),d(s,t)},(1)\ninwhichd(s,t)meansthedistancebetweensamples sandt,\nandapath Ï€isdeï¬nedasasequenceofadjacentsamples. In\nsuch a way, we have that fmax(Ï€)computes the maximum\ndistance between adjacent samples in Ï€, whenÏ€is not a\ntrivial path.\nTheOPFalgorithmassignsoneoptimumpath Pâˆ—(s)from\nSto every sample sâˆˆZ1, forming an optimum path for-\nestP(a function with no cycles which assigns to each sâˆˆ\nZ1\\Sits predecessor P(s)inPâˆ—(s)or a marker nilwhen\nsâˆˆS. LetR(s)âˆˆSbe the root of Pâˆ—(s)which can be\nreached from P(s). The OPF algorithm computes for each\nsâˆˆZ1, the cost C(s)ofPâˆ—(s), the label L(s) =Î»(R(s)),\nand the predecessor P(s).\nThe OPF classiï¬er is composed of two distinct phases:\n(i) training and (ii) classiï¬cation. The former step con-\nsists, essentially, in ï¬nding the prototypes and computing\nthe optimum-path forest, which is the union of all OPTs\nrooted at each prototype. After that, we take a sample from\nthe test sample, connect it to all samples of the optimum-\npath forest generated in the training phase and we evaluate\nwhich node offered the optimum path to it. Notice that this\ntestsampleisnotpermanentlyaddedtothetrainingset,i.e .,\nit is used only once. The next sections describe in details\nthisprocedure.\n2.1.1 Training\nWe say that Sâˆ—is an optimum set of prototypes when the\nOPF algorithm minimizes the classiï¬cation errors for every\nsâˆˆZ1.Sâˆ—can be found by exploiting the theoretical rela-\ntion between minimum-spanning tree (MST) and optimum-\npath tree for fmax[1]. The training essentially consists in\nï¬ndingSâˆ—and an OPF classiï¬er rooted at Sâˆ—.\nBycomputinganMSTinthecompletegraph (Z1,A),we\nobtain a connected acyclic graph whose nodes are all sam-\nples ofZ1and the arcs are undirected and weighted by the\ndistances dbetween adjacent samples. The spanning tree is\noptimuminthesensethatthesumofitsarcweightsismini-\nmumascomparedtoanyotherspanningtreeinthecomplete\ngraph. In the MST, every pair of samples is connected by a\nsingle path which is optimum according to fmax. That is,\ntheminimum-spanningtreecontainsoneoptimum-pathtree\nfor any selected root node. The optimum prototypes are the\nclosestelementsoftheMSTwithdifferentlabelsin Z1(i.e.,\nelementsthatfallinthefrontieroftheclasses). Algorith m1\nimplements thetraining procedure for OPF.\n70012th International Society for Music Information Retrieval Conference (ISMIR 2011)\nAlgorithm 1 â€“OPF TRAINING ALGORITHM\nINPUT: A Î»-labeledtrainingset Z1andthepair (v,d)for\nfeature vector anddistance computations.\nOUTPUT: Optimum-path forest P1, cost map C1, label map\nL1,and ordered set Zâ€²\n1.\nAUXILIARY : Priority queue Q, setSof prototypes, and cost\nvariablecst.\n1.SetZâ€²\n1â†âˆ…and compute by MST theprototype set SâŠ‚Z1.\n2.For each sâˆˆZ1\\S,setC1(s)â†+âˆ.\n3.For each sâˆˆS,do\n4. C1(s)â†0,P1(s)â†nil,L1(s)â†Î»(s), insertsinQ.\n5.WhileQis notempty, do\n6. Remove from Qasample ssuchthat C1(s)isminimum.\n7. InsertsinZâ€²\n1.\n8. For each tâˆˆZ1suchthat C1(t)> C1(s),do\n9. Compute cstâ†max{C1(s),d(s,t)}.\n10. Ifcst < C 1(t), then\n11. IfC1(t)/\\e}atio\\slash= +âˆ, thenremove tfromQ.\n12. P1(t)â†s,L1(t)â†L1(s),C1(t)â†cst.\n13. InserttinQ.\n14.Returnaclassiï¬er [P1,C1,L1,Zâ€²\n1].\nThe time complexity for training is Î¸(|Z1|2), due to the\nmain (Lines 5-13) and inner loops (Lines 8-13) in Algo-\nrithm 1, which run Î¸(|Z1|)times each.\n2.1.2 Classiï¬cation\nFor any sample tâˆˆZ2, we consider all arcs connecting t\nwith samples sâˆˆZ1, as though twere part of the training\ngraph. Considering all possible paths from Sâˆ—tot, we ï¬nd\nthe optimum path Pâˆ—(t)fromSâˆ—and label twith the class\nÎ»(R(t))of its most strongly connected prototype R(t)âˆˆ\nSâˆ—. This path can be identiï¬ed incrementally by evaluating\nthe optimum cost C(t)as\nC(t) = min{max{C(s),d(s,t)}},âˆ€sâˆˆZ1.(2)\nLetthenode sâˆ—âˆˆZ1betheonethatsatisï¬esEquation2\n(i.e.,thepredecessor P(t)intheoptimumpath Pâˆ—(t)). Given\nthatL(sâˆ—) =Î»(R(t)),theclassiï¬cationsimplyassigns L(sâˆ—)\nas the class of t. An error occurs when L(sâˆ—)/\\e}atio\\slash=Î»(t). Al-\ngorithm 2implements thisprocedure.\nAlgorithm 2 â€“OPF CLASSIFICATION ALGORITHM\nINPUT: Classiï¬er [P1,C1,L1,Zâ€²\n1], evaluation set Z2(or\ntest setZ3), and the pair (v,d)for feature vector\nand distance computations.\nOUTPUT: Label L2and predecessor P2maps deï¬ned for\nZ2.\nAUXILIARY : Costvariables tmpandmincost.\n1.For each tâˆˆZ2,do\n2. iâ†1,mincostâ†max{C1(ki),d(ki,t)}.3. L2(t)â†L1(ki)andP2(t)â†ki.\n4. Whilei <|Zâ€²\n1|andmincost > C 1(ki+1),do\n5. Compute tmpâ†max{C1(ki+1,d(ki+1,t)}.\n6. Iftmp < mincost ,then\n7. mincostâ†tmp.\n8. L2(t)â†L(ki+1)andP2(t)â†ki+1.\n9. iâ†i+1.\n10.Return[L2,P2].\nInAlgorithm2 ,themainloop(Lines 1âˆ’9)performsthe\nclassiï¬cationofallnodesin Z2. Theinnerloop(Lines 4âˆ’9)\nvisits each node ki+1âˆˆZâ€²\n1,i= 1,2,...,|Zâ€²\n1|âˆ’1until an\noptimum path Ï€ki+1Â·/a\\}bracketle{tki+1,t/a\\}bracketri}htisfound.\n2.2 Accuracy Computation\nTheaccuraciesaremeasuredbytakingintoaccountthatthe\nclasses may have different sizes in Z2. If there are two\nclasses, for example, with very different sizes and a classi -\nï¬eralwaysassignsthelabelofthelargestclass,itsaccura cy\nwillfalldrasticallyduetothehigherrorrateonthesmalle st\nclass.\nLetNZ2(i),i= 1,2,...,c,bethenumberofsamplesin\nZ2from each class i. We deï¬ne\nei,1=FP(i)\n|Z2|âˆ’|NZ2(i)|andei,2=FN(i)\n|NZ2(i)|, i= 1,...,c\n(3)\nwhereFP(i)andFN(i)are the false positives and false\nnegatives, respectively. That is, FP(i)is the number of\nsamplesfromotherclassesthatwereclassiï¬edasbeingfrom\ntheclassiinZ2,andFN(i)isthenumberofsamplesfrom\nclassithat were incorrectly classiï¬ed as being from other\nclasses in Z2.\nThe errors ei,1andei,2areused todeï¬ne\nE(i) =ei,1+ei,2, (4)\nwhereE(i)is the partial sum error of class i. Finally, the\naccuracy iswrittenas\nAcc=2câˆ’/summationtextc\ni=1E(i)\n2c= 1âˆ’/summationtextc\ni=1E(i)\n2c.(5)\n3. EXPERIMENTAL RESULTS\nIn this section, we described the experiments concern-\ning automatic music genre classiï¬cation using two public\ndatasets: (i) GTZAN Genre Collection [22] and (ii) Mag-\nnatagatune [12]. Table 1 displays the description of the\ndatasets. Itisimportanttonoticethatwehaveusedasubset\nof GTZAN dataset.\nIn regard to music description, for GTZAN dataset we\nhave employed the Marsyas [23] software to extract Mel-\nFrequency Cepstral Coefï¬cients (MFCC) over sequential\nwindows with size â‰ˆ23ms each. We analyzed 30s of each\n701Poster Session 6\nDataset #Samples #Features #Labels\nGTZAN 999 33618 10\nMagnatagatune 11493 74 15\nTable1. Descriptionofthedatasetsusedintheexperiments.\nmusic,obtaining1293windowswith26cepstralcoefï¬cients\neach. Finally, with respect to Magnatagatune dataset, we\nhave used timbre features already extracted and available\nwiththatdatasettocomposeourfeaturevectorwith74char-\nacteristics.\nWe have conducted two round of experiments: (i) in the\nformer (Section 3.1) we address the robustness of super-\nvised classiï¬ers for musical genre classiï¬cation, and (ii)\nin the latter (Section 3.2) we assess the effectiveness of\nOPF after a feature selection procedure over the original\ndatasets. Notice that the feature selection algorithms are\nhybrid methodologies based on OPF and three optimiza-\ntion techniques: Harmony Search (HS) [9], Gravitational\nSearch Algorithm (GSA) [19] and Particle Swarm Opti-\nmization [10].\nThe main idea of such algorithms is to use the accuracy\noveranevaluatingsetastheï¬tnessfunctiontoguidetheop-\ntimization process. Thus, the feature selection algorithm is\ndesigned over training and evaluating sets in order to ï¬nd\nsuitable subsets of features that lead to good recognition\nrates over the unseen test set. These hybrid algorithms em-\nploy the OPF as the basis classiï¬er [15,17,18], since it is\nvery fast and robust,as one can seeinthenext sections.\n3.1 Musical Genre Classiï¬cation Through Supervised\nClassiï¬cation\nIn this section, we described the experiments conducted to\nassesstherobustnessofOPFinthecontextofmusicalgenre\nclassiï¬cation. In regard to classiï¬ers, we have compared\nOPFagainstSVMswithRadialBasisFunction(SVM-RBF)\nand Bayesian classiï¬er (Bayes). For OPF we adopted the\nLibOPF [16], and with respect to SVM-RBF we employed\nSVMTorch [3]. Finally, for Bayesian classiï¬er we used our\nimplementation.\nWe employed the traditional holdout method with 50%\nfor training and the remaining 50% to compose the test set.\nThe experiments were executed over 10 running with ran-\ndomly generated training and test sets in order to compute\nthe mean accuracy and training and test times (seconds).\nNotice that all parameters used in this experiment were em-\npirically chosen, based on our experience. Table 2 displays\nthe recognition rates.\nOne can see that OPF, Bayes and SVM-RBF achieved\nsimilar results for both datasets if one considers the stan-\ndard deviation. However, in GTZAN dataset OPF was 2.92\nand 6.23 times faster than SVMTorch for training and clas-Dataset Classiï¬er Acc Tr[s] Ts [s]\nGTZAN OPF 98.61Â±0.759.19 4.40\nGTZAN Bayes 98.54Â±0.821.7194.31\nGTZAN SVM-RBF 98.72Â±0.0926.98 27.23\nMagnatagatune OPF 62.34Â±0.823.55 3.73\nMagnatagatune Bayes 61.58Â±0.812.3346.53\nMagnatagatune SVM-RBF 63.15Â±0.03162.59 35.04\nTable2. Meanaccuracy,training(Tr)andtesting(Ts)times\ninseconds.\nsiï¬cation, respectively. In regard to Magnatagatune datas et,\nOPF was 45.80 and 9.39 times faster than SVM-RBF for\ntraining and classiï¬cation, respectively.\nAlthough Bayes has been the fastest classiï¬er for train-\ning,ifoneconsidersthewholeexecutiontime,i. e.,traini ng\nand classiï¬cation, OPF has been the fastestapproach.\n3.2 Feature selection\nIn regard to feature selection, we have evaluated three algo -\nrithms: PSO-OPF [17], HS-OPF [18] and GSA-OPF [15].\nFor that, we have used 30% to compose the training set,\n20% to the evaluating one and the remaining 50% for the\ntest set. Table 3 displays the parameters used to tune the\nalgorithms. The number of iterations for convergence has\nbeen set to 10for all approaches. The same occurs with\nthe number of initial solutions, i.e., number of particles f or\nPSO-OPF, number of harmonies for HS-OPF and number\nof masses for GSA-OPF, which has been set to 100. Notice\nthat these values were empirically chosen in order to avoid\nmeta-optimization.\nTable 3. PSO-OPF, HS-OPF and GSA-OPF parameters.\nPSO-OPF HS-OPF GSA-OPF\nc1= 1.4,c2= 0.6HMCR = 0.7Ç«= 0.7,G0= 10\nw= 0.7 k= 100\nTable 4 displays the results. One can see that all tech-\nniqueshaveobtainedthesameresultsforbothdatasets. The\ndifference relies on the execution time, in which PSO-OPF\nandHS-OPFhavebeenexecutedinasimilarperiodoftime,\nbeing up to 2times faster than GSA-OPF. We can see that\nPSO-OPFhasselected 16772out33618featuresforGTZAN\ndataset, which means about 100% of reduction in the num-\nber of features. In case of Magnatagatune, PSO-OPF has\nalso allowed 100% of reduction. It is important to shed\nlightoverthatthisreductioncanprovideafasterfeaturee x-\ntractionprocedure,withthecompromiseofsimilarandgood\nrecognitionratesasintheoriginaldatasets,i.e.,withou tfea-\ntureselection.\n70212th International Society for Music Information Retrieval Conference (ISMIR 2011)\nDataset Technique AccTime[s] #features\nGTZAN PSO-OPF 98.78300.8540 16772\nGTZAN GSA-OPF 98.78603.2372 16776\nGTZAN HS-OPF 98.78305.5086 16776\nMagnatagatune PSO-OPF 62.57242.3991 37\nMagnatagatune GSA-OPF 62.57475.2318 44\nMagnatagatune HS-OPF 62.57244.5305 38\nTable 4. Accuracy, time elapsed in seconds and number of\nselected features.\n4. CONCLUSIONS\nIn this paper, we have addressed the problem of musical\ngenre classiï¬cation by means of OPF classiï¬er, which has\nnever been applied tothis context uptodate.\nExperiments have been conducted in two rounds: in the\nformer we have compared OPF with SVMs and Bayesian\nclassiï¬erintwopublicdatasets(GTZANandMagnatagatune) ,\nand in the latter we have applied recent OPF-based feature\nselection techniques in order to speed up the feature extrac -\ntion process, and also to select the most important subset of\nfeatures that lead to high recognition rates over an evaluat -\ning set.\nIn regard to the ï¬rst round of experiments, all classiï¬ers\nhave obtained close and good recognition rates, being OPF\nfaster for training and classiï¬cation. It is important to hi gh-\nlight that this skill is very interesting in the context of ve ry\nlarge multimedia datasets. We would like to stress the im-\nportanceofuser-friendlymusicalrecommendationsystems ,\nin which training and classiï¬cation phases need to be con-\nductedinafeasiblemanner. Inthiscontext,OPFcanbesuit-\nable for real-time retraining systems, in which new musical\ngenres and songs can be added at any timetothedataset.\nIn addition, we have conducted an experiment to select\nthe most representative features using algorithms recentl y\ndeveloped, which have never been applied to this context to\ndate. WehaveemployedPSO-OPF,HS-OPFandGSA-OPF\nover GTZAN and Magnatagatune datasets, and the results\nseemedtobeinteresting,sinceonecanreducethenumberof\nfeaturesofbothdatasetswithoutcompromisingtherecogni -\ntion rates. For future works, we intend to employ unsuper-\nvised OPF to the same task, as well as to use evolutionary-\nbased featureselection algorithms.\n5. ACKNOWLEDGMENT\nTheauthorswouldliketothankFAPESPgrants#2009/16206-\n1 and #2010/11676-7. We would also like to thank Profes-\nsorGeorgeTzanetakisfromComputerScienceDepartment,\nUniversity of Victoria-Canada, for GTZAN dataset.6. REFERENCES\n[1] C. All `ene, J.-Y. Audibert, M. Couprie, J. Cousty, and\nR.Keriven.Somelinksbetweenmin-cuts,optimalspan-\nningforestsandwatersheds.In ProceedingsoftheInter-\nnationalSymposiumonMathematicalMorphology ,vol-\nume 1, pages 253â€“264, S Ëœao JosÂ´e dos Campos, 2007. In-\nstitutoNacional dePesquisas Espaciais (INPE).\n[2] A. B. Chan and N. Vasconcelos. Modeling, clustering,\nand segmenting video with mixtures of dynamic tex-\ntures.IEEE Transactions on Pattern Analysis and Ma-\nchine Intelligence , 30(5):909â€“926, 2008.\n[3] R. Collobert and S. Bengio. SVMTorch: support vector\nmachines for large-scale regression problems. Journal\nof Machine Learning Research , 1:143â€“160, 2001.\n[4] E. Coviello, L. Barrigton, A. B. Chau, and G. R. G.\nLanckeriet. Automatic music tagging with time series\nmodels. In 11th Internacional Society for Music Infor-\nmation Retrieval Conference , 2010.\n[5] P.L.DeepaandK.Suresh.Featureoptimizationformu-\nsicgenreclassiï¬cationbasedonsupportvectormachine.\nInProceedingsofthe10thNationalConferenceonTech-\nnological Trends , pages 315â€“318, 2009.\n[6] E. Dellandrea, H. Harb, and L. Chen. Zipf, neural net-\nworks and svm for musical genre classiï¬cation. In Pro-\nceedings of the Fifth IEEE International Symposium on\nSignal Processing and Information Technology , pages\n57â€“62, 2005.\n[7] E. W. Dijkstra. A note on two problems in connexion\nwithgraphs. NumerischeMathematik ,1:269â€“271,1959.\n[8] A. X. Falc Ëœao, J. Stolï¬, and R.A. Lotufo. The image\nforestingtransformtheory,algorithms,andapplications .\nIEEETransactionsonPatternAnalysisandMachineIn-\ntelligence , 26(1):19â€“29, 2004.\n[9] Z.W.Geem. RecentAdvancesInHarmonySearchAlgo-\nrithm, volume 270 of Studies in Computational Intelli-\ngence. Springer, 2010.\n[10] J. Kennedy and R. C. Eberhart. Swarm intelligence .\nMorgan Kaufmann Publishers Inc., San Francisco, CA,\nUSA, 2001.\n[11] T. Lambrou, P. Kudumakis, R. Speller, M. Sandler, and\nA. Linney. Classiï¬cation of audio signals using statis-\ntical features on time and wavelet transform domains.\nInProceedings of the IEEE International Conference\non Acoustics, Speech and Signal Processing , volume 6,\npages 3621â€“3624, may 1998.\n703Poster Session 6\n[12] E. Law and L. von Ahn. Input-agreement: a new mech-\nanism for collecting data using human computation\ngames. In Proceedings of the 27th international con-\nference on Human factors in computing systems , pages\n1197â€“1206, New York, NY, USA, 2009. ACM.\n[13] C. McKay and I. Fujinaga. Automatic genre classiï¬ca-\ntion using large high-level musical feature sets. In In-\nternationalConferenceonMusicInformationRetrieval ,\npages 525â€“530, 2004.\n[14] J. P. Papa, A. X. Falc Ëœao, and Celso T. N. Suzuki. Super-\nvised pattern classiï¬cation based on optimum-path for-\nest.InternationalJournalofImagingSystemsandTech-\nnology, 19(2):120â€“131, 2009.\n[15] J. P. Papa, A. F. Pagnin, S. A. Schellini, A. A. Spadotto,\nR. C. Guido, M. P. Ponti Jr., G. Chiachia, and A. X.\nFalcËœao. Feature selection through gravitational search\nalgorithm. In Proceedings of the 36th International\nConference on Acoustics, Speech and Signal Process-\ning, Prague, Czech Republic, 2011. accepted for pub-\nlication, details in http://wwwp.fc.unesp.br/Ëœpapa/opf -\nicassp11.pdf.\n[16] J. P. Papa, C. T. N. Suzuki, and A. X. Falc Ëœao.Li-\nbOPF: A library for the design of optimum-path for-\nest classiï¬ers , 2009. Software version 2.0 available at\nhttp://www.ic.unicamp.br/Ëœafalcao/LibOPF .\n[17] C. C. O Ramos, J. P. Papa, A. N. Souza, and A. X.\nFalcËœao. What is the importance of selecting features for\nnon-technical losses identiï¬cation? In Proceedings of\nthe IEEE International Symposium on Circuits and Sys-\ntems, Rio de Janeiro, Brazil, 2011. accepted for pub-\nlication, details in http://wwwp.fc.unesp.br/Ëœpapa/opf -\niscas11.pdf.\n[18] C.C.O.Ramos,A.N.Souza,andJ.P.Papa.Anovelal-\ngorithm for feature selection using harmony search and\nits application for non-technical losses detection. Com-\nputers & Electrical Engineering , 2011. (submitted).\n[19] E.Rashedi,H.Nezamabadi-pour,andS.Saryazdi.GSA:\nA gravitational search algorithm. Information Sciences ,\n179(13):2232â€“2248, June 2009.\n[20] L. M. Rocha, F. A. M. Cappabianco, and A. X. Falc Ëœao.\nDataclusteringasanoptimum-pathforestproblemwith\napplications in image analysis. International Journal of\nImaging Systems and Technology , 19(2):50â€“68, 2009.\n[21] H. Soltau, T. Schultz, M. Westphal, and A. Waibel.\nRecognitionofmusictypes.In ProceedingsoftheIEEE\nInternationalConference onAcoustics,SpeechandSig-\nnalProcessing ,volume2,pages1137â€“1140,may1998.[22] G. Tzanetakis and P. Cook. Musical genre classiï¬cation\nof audio signals. IEEE Transactions on Speech and Au-\ndioProcessing , 10(5):293â€“302, jul 2002.\n[23] George Tzanetakis and Perry Cook. Marsyas: A frame-\nworkfor audio analysis, 2000.\n[24] C. Xu, N. C. Madagge, and X. Shao. Automatic music\nclassiï¬cation and summarization. In IEEE Transactions\non Speech and Audio Processing , volume 13, pages 441\nâ€“450, 2005.\n704"
    },
    {
        "title": "Exemplar-based Assignment of Large Missing Audio Parts using String Matching on Tonal Features.",
        "author": [
            "Benjamin Martin 0001",
            "Pierre Hanna",
            "Ta Vinh Thong",
            "Myriam Desainte-Catherine",
            "Pascal Ferraro"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1417977",
        "url": "https://doi.org/10.5281/zenodo.1417977",
        "ee": "https://zenodo.org/records/1417977/files/MartinHTDF11.pdf",
        "abstract": "We propose a new approach for assigning audio data in large missing audio parts (from 1 to 16 seconds). Inspired by image inpainting approaches, the proposed method uses the repetitive aspect of music pieces on musical features to recover missing segments via an exemplar-based reconstruction. Tonal features combined with a string matching technique allows locating repeated segments accurately. The evaluation consists in performing on both musician and nonmusician subjects listening tests of randomly reconstructed audio excerpts, and experiments highlight good results in assigning musically relevant parts. The contribution of this paper is twofold: bringing musical features to solve a signal processing problem in the case of large missing audio parts, and successfully applying exemplar-based techniques on musical signals while keeping a musical consistency on audio pieces.",
        "zenodo_id": 1417977,
        "dblp_key": "conf/ismir/MartinHTDF11",
        "keywords": [
            "audio data",
            "missing audio parts",
            "inpainting approaches",
            "musical features",
            "repeated segments",
            "exemplar-based reconstruction",
            "tonal features",
            "string matching technique",
            "musical consistency",
            "signal processing problem"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nEXEMPLAR-BASED ASSIGNMENT OF LARGE MISSING AUDIO PARTS\nUSING STRING MATCHING ONTONAL FEATURES\nBenjamin Martin, Pierre Hanna,Vinh-Thong Ta, PascalFerra ro, MyriamDesainte-Catherine\nLaBRI, UniversitÂ´ edeBordeaux\nfirstname.name@labri.fr\nABSTRACT\nWeproposeanewapproachforassigningaudiodatainlarge\nmissing audioparts(from1to 16seconds). Inspiredbyim-\nage inpainting approaches, the proposed method uses the\nrepetitive aspect of music pieces on musical features to re-\ncover missing segments via an exemplar-based reconstruc-\ntion. Tonal features combined with a string matching tech-\nnique allows locating repeated segments accurately. The\nevaluationconsistsinperformingonbothmusicianandnon-\nmusician subjects listening tests of randomly reconstruct ed\naudio excerpts, and experiments highlight good results in\nassigning musically relevant parts. The contribution of th is\npaper is twofold: bringing musical features to solve a sig-\nnal processing problem in the case of large missing audio\nparts,andsuccessfullyapplyingexemplar-basedtechniqu es\non musical signals while keeping a musical consistency on\naudiopieces.\n1. INTRODUCTION\nAudio signal reconstruction has been of major concern for\nspeechandaudiosignalprocessingresearchersoverthelas t\ndecade, and a vast array of computational solutions have\nbeen proposed [6,7,9,10]. Audio signals are often subject\ntolocalizedaudioartefactsand/ordistortions,duetorec ord-\ning issues (unexpected noises, clips or clicks), or to packe t\nlosses in network transmissions, for instance [1]. Recov-\nering such missing data from corrupted audio excerpts to\nrestore consistent signals has thus been challenging for ap -\nplicative research, in order to restore polyphonic music re -\ncordings,toreduceaudiodistortionfromlossycompressio n,\nor to bring network communications robustness to back-\ngroundnoise,forexample[10].\nTheproblemofmissingaudiodatareconstructionisusu-\nally addressed either in the time domain, aiming at recov-\nPermission to make digital or hard copies of all or part of thi s work for\npersonal or classroom use is granted without fee provided th at copies are\nnot made or distributed for proï¬t or commercial advantage an d that copies\nbear this notice and the full citation on theï¬rstpage.\nc/circlecopyrt2011 International Society for MusicInformation Retrieva l.ering entire gaps or missing excerpts in audio pieces, or\nin the time-frequency domain, aiming at recovering miss-\ningfrequenciesthatcauselocalizeddistortionsofaudiop ie-\nces [18]. A typical trend for the latter one, often referredt o\nas audio inpainting, is to treat distorted samples as missin g\nand to attempt to restore original ones from a local analy-\nsis around missing parts. Common approaches include lin-\near prediction for sinusoidal models [9], Bayesian estima-\ntors [7], autoregressive models [6] or non-negative matrix\nfactorizationsolving[10]. Thesestudiesusuallyeitherb ase\non the analysis of distributions of signal features around\nmissing samples, or use local or global statistical charac-\nteristicsoveraudioexcerpts[18].\nHowever, missing data problems are usually addressed\non relatively small segments of audio data at the scale of\naudio piece duration. Indeed, most audio reconstruction\nsystems proposed so far are based on signal features. The\nnon-stationary aspect of such features makes it particular ly\ndifï¬cult to assign data for large missing parts. Thus, audio\ngapsaregenerallyreducedtoamaximumdurationof1or2\nseconds under particular conditions for the recovered qual -\nity to remain satisfying (see [9] for instance). In this pape r,\nweaddressthechallengingproblemofreconstructinglarge r\nmissingaudioparts,namelyaudiogapsoverseveralseconds\n(from 1 up to 16 seconds of missing data), in music audio\npieces.\nAsimilarproblemisalreadyaddressedinimageprocess-\ning. Indeed, image inpainting aims at restoring and recov-\neringmissing data in imagesin a not easily detectableform\n(seeforinstance[2]andreferencestherein). Acommonand\nsimple approach, from texture synthesis, uses the notion of\nself-distance by considering that an image has a lot of rep-\netitions of local information. This approach can be seen as\nanexemplar-basedcopy-and-pastetechnique[3,5].\nSimilarlytoexemplar-basedimageinpaintingapproaches,\nthe proposed method analyses perceived repetitions in mu-\nsic audiotorecoverlargemissingparts. Notethatwhilepo-\ntentially allowing the reconstruction of large parts, such an\nexemplar-basedapproachinducesthelimitofreconstructi ng\nexclusively parts that are approximately repeated to main-\ntain a musical consistency. To restore such an amount of\n507Poster Session 4\nmissing information,we consider the signal not only as au-\ndio excerpts but also as music pieces, therefore taking into\naccount that sounds are temporally organizedand may fea-\nture redundancies. Indeed, it is the organization and rela-\ntionships between sound events in music that make music\ndiffer from random sound sequences[14]. In Western pop-\nular music, for instance, choruses and verses often are ap-\nproximately repeated parts whose occurrences share a high\ndegreeofperceptualsimilarity. Otherexamplesincludecl as-\nsical music pieces, where the repetition of musical phrases\nstructures the forms, or electronic music where repetitive\nlooptechniquesarefrequentlyemployed. Weproposetouse\nthiskindofmusical redundancyinordertorecovermissing\ndata. Note that the method described in this paper aims at\nassigning a musically consistent part, and could be easily\ncombinedwithsignal-basedapproachestobeusedforprac-\ntical signalreconstructionoflargemissingparts.\nOurmethodconsists inrepresentingeachmusic pieceas\na sequence of tonal features employed to describe the per-\nceivedharmonicprogressions. Then,astringmatchingtech -\nnique is applied to retrieve the part that best ï¬ts the miss-\ningsegment,accordingtoitsleft-andright-sidedtonalco n-\ntexts. The identiï¬ed repetition is ï¬nally used as a referenc e\nto ï¬ll-in missing data. Technical details of the method are\ndescribedinSection2. WedetailinSection3thetestproto-\ncol employedfor evaluatingthe effectivenessof the system\non human listeners and present the results obtained on mu-\nsician and non-musician subjects. Section 4 ï¬nally brings\nconcludingremarksanddepictsfuturework.\n2. METHOD\n2.1 Musical representation\nIn a ï¬rst step, audio signals are represented on musical-\nbased criteria. The key to a well-suited representation in\nthe particular application of ï¬nding perceivedrepetition s is\nto characterize some meaningful local variations in music\nwhile being robust to musical changes. As such, pitch con-\ntent is particularlyadaptedto retrievemusical repetitio nsin\nthe context of analyzing Western music. Indeed, harmonic\nandmelodicprogressionsareconstantlyidentiï¬edbyliste n-\ners, consciously or not, and composers classically organiz e\nthewholestructureoftheirpiecesaroundsuchprogression s\nandtheirvariationsorrepetitions. Moststateoftheartme th-\nodsdealingwithmusicalstructureanalysis[16]orrelated to\nthedetectionofmusicalrepetitions[11]relyontherichne ss\nof tonalinformationto retrievesimilar segments. We there -\nfore chose to use pitch-related features to represent audio\npiecesontheirmusicalstructure.\nHarmonic Pitch Class Proï¬les (HPCP) are often used to\ndescribe this type of musical informations [8]. These fea-\ntures can be summarized as a classiï¬ed representation of\nspectral energies into separate bins that correspond to thefrequencyclasswheretheyappear. Theconsideredfrequen-\ncy classes take into account the cyclical perceptionof pitc h\nin humanauditorysystem: thus, two harmonicsoundscon-\ntribute to the same chroma bin, or pitch class. Moreover,\nHPCP features were proven to be rather insensitive to non-\npitchedvariationsinnoise,timbre,dynamic,tuningorlou d-\nness forinstance,whichmakesthem veryefï¬cient in quali-\nfyingonlytonalcontextsinaudiopieces[8].\n2.2 Tonal featuresextraction\nAudio signals are ï¬rst divided into nsegments, or audio\nframes. We chose to use constant-length frames (as oppo-\nsite to beat-synchronouswindows, for instance) in order to\noptimize the proposed mono-parametric signal representa-\ntion and to enable our system to be potentially used on di-\nverse musical genres. Each frameis representedby a B-di-\nmensional vector h= (h1,Â· Â· Â·, hB)that corresponds to a\nHPCP holding its local tonal context. The dimension value\nBstands for the precision of the note scale, or tonal reso-\nlution, usually set to 12, 24 or, in our case, 36 bins. Each\nHPCP feature is normalized by its maximum value; each\nvector his thus deï¬ned on [0,1]B. Hence, each audio sig-\nnal can be represented as a sequence u=h1h2Â· Â· Â·hnofn\nB-dimensionalvectors.\nIn the following process, we need a similarity measure\nto compareaudiofeaturesbetweeneachother. ThePearson\ncorrelation measure ris better adapted to pitch class pro-\nï¬les comparisons than Euclidean-based measures, for in-\nstance, because it provides invariance to scaling. Such a\nmeasurethenyieldsagoodestimationoftonalcontextsim-\nilarities[20],andisusedinthefollowing. It isdeï¬nedas:\nr(hi, hj) =/summationtextB\nk=1(hi\nkâˆ’hi)(hj\nkâˆ’hj)/radicalBig/summationtextB\nk=1(hi\nkâˆ’hi)2/radicalBig/summationtextB\nk=1(hj\nkâˆ’hj)2(1)\nwhere hiandhjdenote the mean value over the vectors hi\nandhj,respectively.\nIn the particular case of comparing HPCP features, an\nenhanced measure was proposed by Serr` a et al.[17] based\non theOptimal Transposition Index (OTI). The principle is\ntocomputethelocalsimilaritymeasure,here r,betweenthe\nï¬rst HPCP vector and each musical transposition ( i.e., cir-\ncularshift)ofthesecondcomparedvector. TheOTIdenotes\nthetranspositionindexofthelowestdistancefound. Final ly,\naccording to the OTI, a binary score is assigned as the re-\nsult of the comparison. In the case of a 12-split note scale\n(B= 12), for instance, a low cost is assigned to the OTI\nequals to 0 (no transposition was necessary: the local tonal\ncontext is similar) whereas a higher cost is given for any\ngreater value of the OTI. Authorshighlightedin their paper\nthe superiorityof sucha binarymeasureoverusual similar-\nity metrics for HPCP. Based on this comparison technique,\nthesimilarity measure semployedforoursystemis:\n50812th International Society for Music Information Retrieval Conference (ISMIR 2011)\ns(hi, hj) =/braceleftbiggÂµ+if OTI(hi, hj)âˆˆ {0,1, Bâˆ’1}\nÂµâˆ’otherwise(2)\nwhere Âµ+andÂµâˆ’, are two possible scores assigned for the\ncomparisonof hiandhj.\nTheï¬rstrepresentationstepofoursystemthuscomputes\nan HPCP vectorforeach frame,which providesa sequence\nof chroma features that can now be treated as an input for\nstringmatchingtechniques.\n2.3 Stringmatchingtechniques\nAstring uis a sequence of zero or more symbols deï¬ned\non an alphabet Î£. In our context, each HPCP vector repre-\nsents a symbol. We introduce a particular â€œjokerâ€ symbol\nÏ†assigned to each frame that contains at least one missing\naudio sample. Thus, the alphabet consideredin our context\nis denoted by Î£ = [0 ,1]Bâˆª {Ï†}. We denote by Î£âˆ—the\nset of all possible strings whose symbols are deï¬ned on Î£.\nTheithsymbol of uis denoted by u[i], anducan be writ-\nten as a concatenation of its symbols u[1]u[2]Â· Â· Â·u[|u|]or\nu[1Â· Â· Â· |u|]where |u|isthelengthofthestring u. Astring v\nis asubstring ofuif thereexist two strings w1andw2such\nthatu=w1vw2.\nNeedlemanandWunsch[15]proposedanalgorithmthat\ncomputes a similarity measure between two strings uand\nvas a series of elementary operations needed to transform\nuintov, and represent the series of transformationsby dis-\nplaying an explicit alignment between strings. A variant of\nthiscomparisonmethod,theso-called localalignment [19],\nallows ï¬nding and extracting a pair of regions, one from\neachofthetwogivenstrings,whichexhibitthehighestsim-\nilarity. Inordertoevaluatethescoreofanalignment,seve ral\nscores are deï¬ned: one for substituting a symbol aby an-\nother symbol b(possibly the same symbol), denoted by the\nfollowingfunction Cm(a, b), andone forinsertingor delet-\ning symbols,denotedby the function Cg(a). Theparticular\nvalues assigned to these scores form the scoring scheme of\nthealignment.\nThe local alignmentalgorithm[19] computesa dynamic\nprogramming matrix Msuch that M[i][j]contains the lo-\ncal alignment scores between the substrings u[1Â· Â· Â·i]and\nv[1Â· Â· Â·j],accordingto therecurrence:\nM[i][j] =maxï£±\nï£´ï£´ï£²\nï£´ï£´ï£³0\nM[iâˆ’1][j] +Cg(u[i]) ( Î±)\nM[i][jâˆ’1] +Cg(v[j]) ( Î²)\nM[iâˆ’1][jâˆ’1] +Cm(u[i], v[j]) (Î³)\n(3)\nwhere uandvrepresentthetwostrings(HPCPsequences)\nto be compared, and with the initial condition M[0][0] =\nM[i][0] = M[0][j] = 0,âˆ€i= 1. . .|u|,âˆ€j= 1. . .|v|.(Î±)ß¶ İ‘ İà¬µ İà¬¶ \nİ’à¯¥ İ’à¯Ÿßœßœ \nİà¬¶Ô¢ İà¬µÔ¢ áˆºÜ½áˆ» \nİ’à°¥ \nß¶ Ç¥ ß¶ \nİ‘ \n\u0003İŠİƒİ…İˆÜ½Í³  \u0003İŠİƒİ…İˆÜ½Í´  \nİ‘à¬¶ İ’à¬¶ \nİ’à°¥ İ„à¯œ İ„à¯ İ„à¯¨ áˆºİ…áˆ» \náˆºİ…İ…áˆ» \náˆºİ…İ…İ…áˆ»  \náˆºÜ¾áˆ» \nFigure 1. Overview of the algorithm. (a): audio wave-\nform with missing data. (i): string provided by the mu-\nsical representation step (Section 2.2). (ii): string align-\nments performed by our algorithm. (iii): aligned strings\n(Section 2.4). (b): reconstructedaudio waveform. Dashed-\ncircled regionscorrespondto an overlap-addreconstructi on\n(Section2.5).\nrepresentsthedeletionofthesymbol u[i],(Î²)representsthe\ninsertionofthesymbol v[j],and(Î³)representsthesubstitu-\ntionofthesymbol u[i]bythe symbol v[j].\nInthefollowing,thelocalalignmentalgorithmisdenoted\nby the function align(u, v). As a result, it yields a triplet\n(x, uâ€², vâ€²)where xis the best similarity score between two\nstrings, and uâ€²andvâ€²are the two alignedsubstringsrespec-\ntivelyin uandv.\nConsidering two HPCP features hiandhj, the scoring\nschemeusedinourexperimentsisdeï¬nedasfollows:\nÂµ+= 1\nÂµâˆ’=âˆ’0.9\nCg(hi) =âˆ’0.7ifhi/ne}ationslash=Ï†,0otherwise\nCm(hi, hj) =ï£±\nï£²\nï£³s(hi, hj)ifhi/ne}ationslash=Ï†andhj/ne}ationslash=Ï†\n0.1 hi=Ï†xorhj=Ï†\n0 otherwise\n(4)\nNumerical values were obtained empirically on a subset\nof 80 songs fromthe datasets presentedin Section 3.2. The\ndisjunction case for symbol Ï†is motivated by constraints\nover the alignment of frames that correspond to frames of\nmissingdata.\n2.4 Algorithm\nThe general principle of our exemplar-based method is to\nidentifyinthepartiallyalteredmusicpiecesequencethep art\nthat best ï¬ts the missing section. We call this best-ï¬tting\npart thereference part . We denote as local tonal context\ntonalprogressionsthatoccurpriorandafterthemissingpa rt.\nMore formally,we introducea threshold Î´that corresponds\nto the size of tonal contexts consideredbefore and after the\nmissingsegment,asa numberofframes.\n509Poster Session 4\nFigure 1 depicts an overview of the applied algorithm.\nFormally,thecomputationisperformedasfollows:\n(i)Letubethestringrepresentingamusicpiece, i.e.,the\nHPCP sequence obtained from the signal representa-\ntion step. By hypothesis, ucontains a string vÏ†=\nÏ†Â· Â· Â·Ï†of joker symbols, and there exists t1, t2inÎ£âˆ—\nsuchthat u=t1vÏ†t2.\n(ii)Deï¬ne as the left (resp. the right) context string vl\n(resp. vr) ofvÏ†the unique string of length Î´such\nthat there exists tâ€²\n1andtâ€²\n2âˆˆÎ£âˆ—verifying t1=tâ€²\n1vl\nandt2=vrtâ€²\n2. Compute (x1, u1, v1)as the result\nofalign(t1, vlvÏ†vr)and(x2, u2, v2)as the result of\nalign(t2, vlvÏ†vr).\n(iii)Ifx1> x2, then keep u1as the reference part, u2\notherwise.\nThisprocessprovidesbotha referencepart uâ€²(u1oru2)\ncorresponding to the excerpt that best ï¬ts the missing sec-\ntion,anda destinationpart vâ€²(v1foru1,v2foru2)thatwas\naligned with uâ€². Note that the scoring constraintsdescribed\nin Eq. 4 ensure that the identiï¬ed part vâ€²containsthe miss-\ningsegment vÏ†.\n2.5 Audiodataassignment\nIn order to ï¬ll-in missing data, the method consists in as-\nsigning data from the identiï¬ed reference part into the des-\ntinationpart. Sincethe identiï¬eddestinationpart vâ€²maybe\nlonger than the missing data segment vÏ†, the samples as-\nsignment may overlap existing samples in the audio piece.\nIn order to ensure a smooth audio transition, overlap-add\nreconstructionsareperformed[4].\nNote that we deliberately chose not to implement any\nbeat,onsetoranykindofsynchronization,inordertoavoid\nthe addition of potential analysis errors and to enable the\nstrictevaluationofthisexemplar-basedaudioalignmentm e-\nthod. We leave as a perspective such more advanced audio\nsynchronizationsoroverlappingtechniques.\n3. EXPERIMENTS AND RESULTS\nOur alignment system is based on musical features. The\nidentiï¬ed repetitions only depend on a musical criterion:\npitch content. Therefore, variations in timbre, rhythm or\nlyricsmayappearbetweenoccurrencesofanidentiï¬edrep-\netition and original and reconstructedaudio signals may be\ncompletelydifferent. Hence,standardsignalprocessingm et-\nrics such as SNR seem inadequate to the evaluation of mu-\nsical resemblance. Since it works on a musical abstraction,\nthe aim of the method is to produceperceptuallyconsistent\nresults,i.e., reconstructions satisfactory for human listen-\ners. The proposed experiments are therefore based on hu-\nmansubjectiveevaluationofreconstructedaudioï¬les.3.1 Test datageneration\nThe tests of our method consist in erasing random audio\nparts in a dataset of music pieces, recovering missing data\nwith our system and asking human listeners to evaluate the\naudio reconstruction. Since our method uses an exemplar-\nbased approach, a part needs to be approximately repeated\nin the same piece at least once in order for our system to\nrecover it. Thus, we introduce a repetitiveness hypothe-\nsisprior to the evaluation of the proposed system: every\nconcealed part for audio tests must belong to a repeated\nstructural section, according to a structural ground truth .\nFor instance, for a music piece annotated with the structure\nABCAAB,thehypothesisforceconcealedpartstobe chosen\nwithinoneoftherepeatedpatterns A,BorAB.\nThe test data generation is performed according to the\nfollowingprocess:\n1.Select randomly a concealment length lbetween 5 and\n16seconds.\n2.Accordingto an annotatedstructural groundtruth, select\nrandomlya repeatedsectionlastingat least l.\n3.Selectrandomlyabeginningtimeinstant dinthischosen\npart.\n4.Perform the concealment: erase everysample between d\nandd+l.\n5.Performthereconstructionusingthealgorithmdescribed\nin Section2.4.\n6.Finally,selecttworandomdurations t1, t2between5and\n10 seconds,andtrimthe reconstructedaudiopiece between\ndâˆ’t1andd+l+t2.\nThe last step is dedicated to reducing the duration of ex-\ncerptsinordertoreducethetestduration. Notethatwherea s\nthislaststepmakestheexperimentmorecomfortable(faste r)\nfor the testers, it tends to sharpen up their attention aroun d\nto the reconstructed region, and requires the reconstructi on\nto bespeciallyaccurate.\n3.2 Dataset\nAsatestdataset,weelectedtheOMRAS2MetadataProject\ndataset[13]thatprovidesstructuralannotationsforWest ern\npopular audio music of different artists1. For our experi-\nments, we chose to test on 252 music pieces mostly from\nThe Beatles (180 pieces), Queen(34 pieces) and Michael\nJackson(38 pieces). These artists were most likely to be\nknown by listeners, hence reinforcingtheir judgment. Note\nthat audio pieces were taken from mp3-encodedmusic col-\nlectionscompressedwitha minimumbit-rateof192kbps.\nIn orderto computeHPCP featureson audio signals, we\nchose the window size of 46msin order to keep accurate\nalignment on audio data. Performingpreliminarytests on a\nfew songs, the local context threshold value of Î´= 4sec-\nondsappearedto besufï¬cientforconsistentalignments.\n1http://www.isophonics.net/content/reference-annotat ions\n51012th International Society for Music Information Retrieval Conference (ISMIR 2011)\n20 30 40 50 60 70 80 \n0 5 10 15 Probability of choosing a repetition (%) \nDuration of a randomly chosen part (s) \nFigure 2. Probability of randomly choosing repeated parts\naccording to the groundtruth. Plain line shows the average\nvalues over the whole dataset, while dashed lines stand for\nthe different artistsâ€™ songs: square points for Queen, circle\npointsfor MichaelJackson andtrianglepointsfor The Bea-\ntles.\nTo evaluate how restrictive the repetitiveness hypothesis\nmay be on this speciï¬c dataset, we computed the average\npercentageofpartsinaudiopiecesthatarerepeatedaccord -\ning to the structural ground truth. Figure 2 shows the aver-\nage probability of ï¬nding a repetition as a function of the\nsize of the randomly chosen part. The plain line shows the\naverage values over the dataset. The graphic shows for in-\nstance that a random part that lasts 8seconds corresponds\nto a fully repeated section in structural ground truth 48%\nof the time on average. Repetitiveness seems to vary be-\ntween artists in the dataset, as suggested by the different\ndashedlines. Thus,theprobabilityofï¬ndingrepeatedpart s\nin pieces from The Beatles , for instance, is between 8.7%\nand16.2%higherthanonpiecesfrom Queen. Thehypothe-\nsisofdeletingexclusivelyrandompartsinsiderepeatedse c-\ntions therefore induces the consideration of 35%of15sec-\nonds parts in audio pieces, to 65%for1second parts on\naverage.\nThe previously described data generation process was\nperformed once for each music piece in the dataset. 252\nexcerpts were thus generated, each lasting between 10and\n30seconds, with an average duration of 21.8seconds over\nthe set. The artiï¬cial data concealment durations were ran-\ndomlygeneratedbetween 1and16seconds,withanaverage\nvalueof 8.2seconds.\n3.3 User tests\nThe test protocol employedfor evaluatingour system is in-\nspiredfromtheMUSHRAaudiosubjectivetestmethod[12].\nIn order to respect a maximum test duration of approxi-\nmately10minutes,eachsubjectisaskedtolistenfor26au-dio excerpts from the generated test dataset. Among these,\n5excerptsareproposedineverytestandcorrespondtonon-\naltered audio excerpts. These are supposed to observe in-\ndividual effect, enabling for instance the detection of ran -\ndomly answering subjects. The 21 remaining excerpts are\nrandomly chosen among the reconstructed database. Each\nsubjectisaskedtolistentoeachoftheseexcerptsonce,wit h\nno interruption, and to indicate whether or not he detected\nany audio artefact or distortion. If so, the subject is asked\ntoratethequalityofthereconstructionapplied: 1)Verydi s-\nturbing,2)Disturbing,3)Acceptable,4)Hardlyperceptib le.\nThe rate of 5 is assigned for no distortion heard. Note that\nthe exactmeaningof termsin thecontextofthe experiment\nisnotprovidedtothetesters,hencelettingthemdeï¬nethei r\nown subjective scale. Finally, a few additional informatio n\nis asked, such as which audio restitution material is used,\nandwhetherornotthetester isamusician.\n3.4 Results\nTests werecarriedouton 80distinct listeners, 34musicians\nand46nonmusicians. Theaveragenumberof observations\nper audio excerpt is 7.1, values ranging from 1to15ob-\nservations for altered excerpts. The 5common non-altered\npieces logically led to 400observations among which 10\nwere incorrectly evaluated (artefacts perceived). Since a ll\nof these invalid rates were attributed by distinct users, we\nchose to take into account every subject in the evaluation\n(noabnormalbehavior). Table1summarizestheresultsob-\ntained for both classes of testers and for the differentarti sts\nin the dataset. Note that the rates attributed to the 5non-\naltered excerptswere not used for computingthese average\nvalues. Overall resultshighlightan averagerate of 4.04out\nof5for the quality of the applied data assignment. More\nprecisely, 30%ofreconstructedexcerptswereattributedthe\nrate5byalloftheirlisteners,whichhighlightsveryaccurate\naudioassignmentsonathirdofthedataset. Thedistributio n\nof other average rates is as follows: 31%pieces rated be-\ntween 4and5,17%pieces between 3and4,15%between\n2and3and7%between 1and2. Reminding that 4 corre-\nsponds to a â€œhardly perceptibleâ€ reconstruction and 5 to no\ndistortionperceived,the methodthereforeseemssuccessf ul\nin performinginaudibleor almostinaudiblereconstructio ns\nin61%ofthe cases.\nAs one could expect, musician subjects perceive more\ndistortionswithanaveragerateof 3.92against 4.13fornon\nmusicians. Scores obtained for each audio material class\nhighlight a slightly better perception of reconstructions for\nheadset restitution, with an average value of 3.98against\n4.05for other material. However, since all musician testers\nchose to use headset, musician and headset scores may be\nclosely related. Reported distortions include short rhyth -\nmic lags, unexpected changes in lyrics, sudden changes in\ndynamics or abrupt modiï¬cation of instruments. Results\n511Poster Session 4\nMusicians Nonmusicians Total\nTheBeatles 3.95 4.13 4.05\nMichaelJackson 4.21 4.26 4.24\nQueen 3.40 3.94 3.71\nWholedataset 3.92 4.13 4.04\nTable 1. Audio test results. Values correspond to average\nrates on a 1 (very disturbingreconstruction)to 5 (inaudibl e\nreconstruction)scale.\nalso vary between artists; for instance, reconstructions o n\nMichael Jackson songs seem to be better accepted, with an\naverage value around 4.24whether listeners are musicians\nornot. Contrastingly,reconstructionson Queenpieceswere\nmore often perceived, with an average value of 3.94, and\nmusicians assigned a 0.5lower rate on average. An ex-\nplanation for such gaps between artists may be the more or\nless repetitive aspect of similar structural sections, suc h as\nchoruses that tend to vary often along Queenmusic pieces.\nMoreover, a few pieces such as We will rock you byQueen\nwere assigned particularly low rates ( 1.25in this case for 8\nobservations) probably because their pitch content is insu f-\nï¬cient forthealgorithmtodetectlocalsimilarities.\n4. CONCLUSIONAND FUTUREWORK\nIn this paper, we addressed the problem of reconstructing\nmissing datainlargeaudioparts. We used atonalrepresen-\ntation to obtain a feature sequence on a musical criterion,\nandanalyzeditusingstringmatchingtechniquestoextract a\nmusicallyconsistentpartasareferenceforsubstitution. We\ngeneratedaudiotest data introducingrandomconcealments\nbetween 1and16seconds long in repeated structural parts,\nand tested out our music assignment system in an audio\nevaluation on 80 subjects. Results highlighted a good per-\nformance of the method in recoveringconsistent parts with\n30%random reconstructions undetected, and 31%hardly\nperceptible.\nAs a futurework, in orderto make this methoduseful in\npractice, the algorithm may be combinedwith other signal-\nbasedapproaches. Forinstance,audiosynchronizationsco uld\nbe applied by aligning assigned beats with original ones.\nOther possible audio improvements include the correction\nof dynamics, or the combineduse of other musical descrip-\ntions (timbre features, rhythm, etc.). We also leave as a\nperspective the improvement of the comparison algorithm,\nwhichcouldretrieveasetofpartslocallyï¬ttingthemissin g\ndatasectionandcombinesuchpartsiteratively,orthedeve l-\nopmentof an inspiredapproachperformingreal-time audio\nreconstruction.5. REFERENCES\n[1] A. Adler, V. Emiya, M. Jafari, M. Elad, R. Gribonval, and\nM.D. Plumbley. Audio inpainting. Research Report RR-7571,\nINRIA,2011.\n[2] M. Bertalmio, G. Sapiro, V. Caselles, and C. Ballester. I mage\ninpainting. Proc.of SIGGRAPH ,pp. 417â€“424, 2000.\n[3] A. Criminisi, P. PÂ´ erez, and K. Toyama. Region ï¬lling and ob-\njectremovalbyexemplar-basedimageinpainting. IEEETrans.\nonImage Processing , v. 13, pp. 1200â€“1212, 2004.\n[4] R. Crochiere. A weighted overlap-add method of short-ti me\nfourier analysis/synthesis. IEEE Trans. on Acoustics, Speech\nand Signal Processing , v. 28, pp. 99â€“102, 1980.\n[5] A.A. Efros and T.K. Leung. Texture synthesis by non-\nparametric sampling. Proc.of ICVV ,p.1033, 1999.\n[6] W. Etter. Restoration of a discrete-time signal segment by in-\nterpolation based on the left-sided and right-sided autore gres-\nsive parameters. IEEE Trans. on Signal Processing , v. 44,\npp. 1124â€“1135, 1996.\n[7] S.J. Godsill and P.J.W. Rayner. Digital Audio Restoration - A\nstatisticalmodel based approach . 1998.\n[8] E.GÂ´ omez. TonalDescriptionofMusicAudioSignals .PhDthe-\nsis,Universitat PompeuFabra, 2006.\n[9] M.Lagrange, S.Marchand, andJ.B.Rault.Longinterpola tion\nofaudiosignalsusinglinearpredictioninsinusoidalmode ling.\nJourn. of the Audio Engineering Society , v. 53, pp. 891â€“905,\n2005.\n[10] J. Le Roux, H. Kameoka, N. Ono, A. de CheveignÂ´ e, and\nS. Sagayama. Computational auditory induction by missing-\ndata non-negative matrix factorization. ISCA Tutorial and Re-\nsearchWorkshoponStatisticalAndPerceptualAudition ,2008.\n[11] B. Martin, P.Hanna, M. Robine, and P.Ferraro. Indexing mu-\nsicalpiecesusingtheirmajorrepetition. Proc.ofJointConfer-\nence on DigitalLibraries , 2011.\n[12] A.J.Mason.TheMUSHRAaudiosubjectivetestmethod. BBC\nR&DWhite Paper WHP ,38, 2002.\n[13] M. Mauch, C. Cannam, M. Davies, C. Harte, S. Kolozali,\nD. Tidhar, and M. Sandler. Omras2 metadata project 2009.\nProc.of ISMIR,Late-Breaking Session , 2009.\n[14] R. Middleton. â€œFormâ€, Key Terms in Popular Music and Cul-\nture. Wiley-Blackwell,1999.\n[15] S.B.NeedlemanandC.D.Wunsch. Ageneralmethodapplic a-\nble tothe search for similaritiesinthe amino acidsequence of\ntwoproteins. Journ. ofMolecularBiology ,v.48,pp.443â€“453,\n1970.\n[16] J. Paulus, M. MÂ¨ uller, and A. Klapuri. Audio-based musi c\nstructure analysis. Proc.of ISMIR ,pp. 625â€“636, 2010.\n[17] J. Serr` a, E. GÂ´ omez, P. Herrera, and X. Serra. Chroma bi nary\nsimilarityandlocalalignmentappliedtocoversongidenti ï¬ca-\ntion.IEEETrans.onAudio,SpeechandLanguage Processing ,\nv. 16, pp. 1138â€“1151, 2008.\n[18] P. Smaragdis, B. Raj, and M. Shashanka. Missing data im-\nputation for time-frequency representations of audio sign als.\nJourn. ofSignal ProcessingSystems , pp. 1â€“10, 2010.\n[19] T. F. Smith and M. S. Waterman. Identiï¬cation of common\nmolecular subsequences. Journ. of Molecular Biology , v. 147,\npp. 195â€“197, 1981.\n[20] D. Temperley. The Cognition of Basic Musical Structures .\np. 175, 2004.\n512"
    },
    {
        "title": "Cross-Modal Aesthetics from A Feature Extraction Perspective: A Pilot Study.",
        "author": [
            "Alison Mattek",
            "Michael A. Casey"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1414906",
        "url": "https://doi.org/10.5281/zenodo.1414906",
        "ee": "https://zenodo.org/records/1414906/files/MattekC11.pdf",
        "abstract": "This paper investigates perceptual relationships between art in the auditory and visual domains. First, we conducted a behavioral experiment asking subjects to assess similarity between 10 musical recordings and 10 works of across subjects as to which images correspond to which audio, even though neither the audio nor the images possessed semantic content. Secondly, we sought to find the relationship between audio and images within a defined feature space that correlated with the subjective similarity judgments. We trained two regression models using leaveone-subject-out and leave-one-audio-out crossvalidation respectively, and exhaustively evaluated each model's ability to predict features of subject-ranked similar images using only a given audio clip's features. A retrieval task used the predicted image features to retrieve likely related images from the data set. The task was evaluated using the ground truth of subjects' actual similarity judgments. Our results show a mean cross-validated prediction accuracy of 0.61 with p<0.0001 for the first model, and a mean prediction accuracy of 0.51 with p<0.03 for the second model.",
        "zenodo_id": 1414906,
        "dblp_key": "conf/ismir/MattekC11",
        "keywords": [
            "perceptual relationships",
            "auditory and visual domains",
            "behavioral experiment",
            "subjective similarity judgments",
            "feature space",
            "regression models",
            "leaveone-subject-out",
            "leave-one-audio-out",
            "crossvalidation",
            "predicted image features"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\n   \n CROSSMODAL AESTHETICS FROM A FEATURE EXTRACTION PERSPECTIVE: A PILOT STUDY Alison Mattek Michael Casey Dartmouth College Alison.M.Mattek@Dartmouth.edu Dartmouth College mcasey@Dartmouth.edu ABSTRACT This paper investigates perceptual relationships between art in the auditory and visual domains. First, we conduct-ed a behavioral experiment asking subjects to assess simi-larity between 10 musical recordings and 10 works of abstract art. We found a significant degree of agreement across subjects as to which images correspond to which audio, even though neither the audio nor the images pos-sessed semantic content. Secondly, we sought to find the relationship between audio and images within a defined feature space that correlated with the subjective similarity judgments. We trained two regression models using leave- one-subject-out and leave-one-audio-out cross-validation respectively, and exhaustively evaluated each model's ability to predict features of subject-ranked simi-lar images using only a given audio clip's features. A re-trieval task used the predicted image features to retrieve likely related images from the data set. The task was evaluated using the ground truth of subjects' actual simi-larity judgments. Our results show a mean cross-validated prediction accuracy of 0.61 with p<0.0001 for the first model, and a mean prediction accuracy of 0.51 with  p<0.03 for the second model. 1. INTRODUCTION Art, in any of its modes, affects us. Whether an acrylic or symphonic masterpiece, art has the tendency to attract our attention and stir our sentiments, sometimes in ways that are quite similar across modalities. An attempt to define what a work of art is or to identifying exactly why art affects us the way it does are both ambitious and elusive questions in the field of aesthetics. Yet, these seem to be some of the more progressive objectives of music infor-mation retrieval. Once we have diluted a sensuous experi-ence such as listening to a symphony into a concrete string of numbers, the source of our pleasure becomes slightly more objective (though our experience of it may remain quite ineffable). This objectivity has allowed us to examine correlations between sets of songs based on mu-sical features. Perhaps, then, feature extraction could also enlighten us to correlations across domains of art. For example, what features contribute to the phenomenon of a particular painting evoke the same feeling as a particular work of music?      This study attempts to bridge artistic domains from the perspective of feature extraction. If works of art that are emotionally ambiguous and culturally unrelated could still be considered similar, it is very possible that there is ob-jectivity in the similarity that lies at the feature level. This opens up an entirely new question in terms of cross- mod-al analysis: which auditory features and which visual fea-tures are important when considering crossmodal similari-ties? To simplify the plethora of possibilities, the study focuses on a few standard low-level features: course con-stant-Q spectrograms of the audio and eight band HSV histograms of the images. 2. RELATED WORK      Congruency across sensory modalities is a subject matter that has been discussed in the field of psychology since the seventies [1]. Cross-modal congruencies have been empirically shown to exist across the auditory and visual domains. This is not to be confused with cross- modal confusion, which is what occurs in individuals suffering from synesthesia. Typical audio-visual cross- modal congruency examples are sounds high in frequen-cy being associated with objects high in space and ob-jects small in volume, or vice versa: sounds low in fre-quency are associated with objects low in space or ob-jects large in volume. Studies in cross-modal congruen-cies support the hypothesis that art across different do-mains may affect us in similar ways.      Translations between visual and auditory art have been attempted in both directions. These attempts are known as music visualization when translating from au-ditory to visual, and image sonification when translating from visual to auditory. Traditional music player soft-ware generally come suited with some means of visualiz-ing the music. Researchers have also devised creative means of attempting the audio to visual translation, in-cluding the use of affective photos [2] and self-similarity [3]. Mardirossian and Chew also presented a way to visu-alize music in two dimensions based on the tonal pro-gressions [4]. The translation in the opposite direction, from images to music, has been investigated using the \n585Poster Session 4\n   \n geometric characteristics of images to create a time-based sequence that could be translated by musical instruments [5].      Although there has never been an explicit attempt to classify images with audio data (as in the current study), one recent study was able to classify music genre by ana-lyzing the promotional images of the artist [6]. This study used image histograms across three color spaces: RGB, HSV, and LAB to cluster image data into classes of mu-sical genre. All of the above mentioned related worksug-gests that there are some consistent perceptual relation-ships between the auditory and visual domains. 3. BEHAVIORAL STUDY 3.1 Data Collection The first step in finding similarities across modalities was to find pairs of images and audio that were thought to be similar by a group of subjects. This was done via the be-havioral experiment described in this section.  3.1.1 Stimuli Ten abstract art images by the following artists were cho-sen for this experiment: Betsy Eby, Gerhard Richter, Giles Hayter, Stephanie Willis, Ian Camleod, Madison Moore, Anne Kavanagh, Ernie Gerzabek, Paul Pulszartti, and Jason Stephen. Figure 1 shows \"Blueprint I\" by Stephanie Willis. All of the images were constructed ei-ther in the late twentieth century or early twenty-first cen-tury and all artists are Western, to avoid extreme cultural differences. The images were chosen selectively by the authors to encapsulate a range of colors and symmetries and to avoid any conceptual objects (e.g., figures that resemble a tree or a face). All of the image and audio stimuli used in this experiment can be viewed at: http://alisonmattek.wordpress.com/projects/academic/crossmodal/.      Ten ten-second solo piano clips by the following com-posers were chosen for this experiment: Handel, Mozart, Liszt, Debussy, Hindemith, Barber, Ligeti, Phillip Glass, Bill Evans, and David Lanz. This list represents Western composers across several centuries. The clips were chosen selectively by the author to encapsulate a range of tempos, pitches, and performers, but the timbre was kept relatively consistent, as all of the clips contained only the piano in the instrumentation.  \n Figure 1. â€œBlueprint Iâ€ by Stephanie Willis  predominantly modern works.  In the music selection, had solo piano works been chosen from only the twentieth century as well, there would have been a bias of chromat-icism in the harmonic quality of all of the works.  In order to achieve more variability in the harmonic structure (that is, to include extremely tonal music), we chose music from previous eras as well.  However, the cultural era in which a work was produced is likely a relevant variable, and should be considered in future investigations.  3.1.2 Listening Test Subjects between the ages of nineteen and thirty years (N = 16, 6 = female, 10 = male) completed a listening test in which they rated the similarities between all pairs of stimuli. Figure 2 shows the graphic user interface for the listening test. Some of the subjects had previous musical training (N = 10, 4 = female, 6 = male). The pairs were presented in a different random order for each subject. The first ten trials of the test were \"practice\" trials; the subjects were told they could adjust their strategy for choosing a similarity rating during the practice trials. Af-ter this, the subjects completed one hundred trials, one for every possible pair of the ten audio clips and ten images. The subjects rated the similarity between each pair on a scale of 1 - 30. 1 - 10 implied \"very dissimilar\", 11 - 20 implied \"average similarity\", and 21 - 30 implied \"very similar\". This 30-point scale was taken from Greyâ€™s methodology for multidimensional scaling of musical timbre [7]. The subjectsâ€™ responses were stored into a ten by ten similarity matrix.  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that cop-ies bear this notice and the full citation on the first page.  Â© 2011 International Society for Music Information Retrieval  \n58612th International Society for Music Information Retrieval Conference (ISMIR 2011)\n   \n \n Figure 2.  Listening Test GUI  3.2 Data The results showed correlation across subjects on certain pairs of the audio and images. Figure 3 shows the mean, z-scored similarity matrix across all subjects. High values indicate a pair that was rated as very similar across sub-jects and low values indicate a pair that was rated as very dissimilar across subjects. \n Figure 3.  Mean Similarity Matrix for All Subjects       The data was analyzed with plots, covariance matrices, and distance matrices of the z-scored subject responses. Figure 4 shows an analysis of the sixth audio clip, which was an excerpt from Samuel Barber's Excursion No. 1 for solo piano. The plot shows the z- scored subject responses to audio 6 when paired with each of the images, as indi-cated on the x-axis. What stands out on this plot is that the similarity ratings decrease when audio 6 is compared to image 7, increase when audio 6 is compared to image 8, and decrease again when audio 6 is compared to image 9. In other words, audio 6 was considered to be very similar to image 8, but very dissimilar to image 7 and image 9, with much agreement across subjects.      From this type of analysis on all of the data, the fol-lowing pairs of images and audio were thought to be simi-lar across subjects: audio 1 and audio 8 were similar to image 6; audio 2 and audio 10 were similar to image 7; audio 3 and audio 7 were similar to image 1, image 4, and image 9; audio 4 and audio 5 were similar to image 3; audio 6 was similar to image 8; and audio 9 was similar to image 10. Images 2 and 5 were not consistently rated as similar to any audio examples. Figure 5 shows image 5, which was not consistently rated as similar or dissimilar to any audio across subjects. \n Figure 4.  Analysis of Audio 6   \n Figure 5.  â€œComposition 114-Bâ€ by Ian Comleod was not con-sistently rated as similar to any of the audio examples. 4. IMAGE PREDICTION Given the subjective cross-modal similarity evaluation, we sought to determine whether there were correspond-ences in common between the underlying audio and im-age features spaces. To this end, from the 10 audio clips we extracted average power with a band rate of two con-stant- Q bands per octave [9]. From the images we ex-tracted eight-band HSV histograms. The HSV representa-tion was chosen over RGB because, like the choice of logarithmic frequency spectrum, the HSV color scale cor-responds more closely with human perception than the RGB scale [10]. The HSV values were binned into 3 groups of 8 scalars forming a 24 dimensional vector. The 16 audio bands and 24 image values were independently dimension reduced using a singular value decomposition (SVD) keeping those coefficients corresponding to the first 95% of the total variance in each modality. \n587Poster Session 4\n   \n 4.1 Multivariate Multiple regression To test the predictability of image features given audio features for an unseen music clip, we performed a retriev-al experiment using a cross-validated multivariate multi-ple regression model [11]. Regression is an optimization method that minimizes the response error for a training set of predictor/response vector pairs (in our case audio features / image features) using a linear model of the form: y=WTx+b, with weight matrix, W , predictor varia-bles, x, biases b, and response variables y. Our models consisted of multiple independent variables (audio-feature predictors), and multivariate dependent variables (image-feature responses). Such multivariate multiple regression has previously been applied, in a cross-modal context, to predicting fMRI images corresponding to concrete nouns; where the predictor variables were intermediate vector representations of single words and the response variables were fMRI image voxels [12].      Figure 6 illustrates the method of predicting image features from a regression model trained on audio feature / image feature pairs. Figure 7 shows an example of audio features, a weight matrix, and the predicted response, ac-tual response, and residual images. \n Figure 6.  Schematic diagram showing how regression is used to predict response variables from predictor variables. In this paper, the predictor variables are audio features, and the re-sponse variables are image features. \n Figure 7.  Example of audio features (upper left), trained regres-sion model weights (upper right), predicted image (lower left), and actual image (lower right), for a leave-one-audio-out regres-sion model. 4.2 Training We trained the regression models using the mvregress function from the Statistics Toolbox of the MATLAB numerical scientific package. The training data consisted of the dimension-reduced features of the audio clips as predictor variables and the dimension-reduced image fea-tures for each subjectâ€™s highest-rated image (i.e., the most similar image as determined by the similarity judgments) as the response variables. We trained two models: Model 1 was trained using subjects' image response ratings for each audio clip, leaving out one subject's data in each run; Model 2 was trained using all subjects' image response ratings, leaving out one of the audio clips in each run.    4.3 Ground Truth The data from the behavioral studyâ€” i.e. per subject similarity ratings between each audio clip and each im-ageâ€” yielded per subject 10x10 similarity matrices where each row consisted of the image rankings to one audio clip with integer values in the range of 1 to 30. Each sub-ject utilized the scale to a different extent; with some us-ing the full range and others using only part of the availa-ble range. To align the different ranges onto a common scale, each row was normalized to the range of 0 to 1. The individual normalized similarity matrices were then aver-aged yielding a cross-subject mean similarity matrix. From this matrix, a ground truth of relevant images was determined individually for each audio clip, but across subjects, by selecting all images with an average similari-ty greater than, or equal to, the mean plus one standard deviation of the normalized similarity ratings for that au-dio clip. This yielded a different number of relevant im-ages for each audio clip ranging from one to three rele-vant images. These were used as the target images for each audio clip in the retrieval experiments.  Note that for Model 1, the ground truth consisted of a mean similarity matrix that excluded the held-out subject; i.e. the test sub-ject's data.  4.4 Prediction One of the main utilities of regression is that responses can be computed for novel dataâ€” such that the response variables interpolate between the training data for previ-ously unseen data. Thus, the trained regression models were used to predict the response variables (image feature vector) for each test feature vector (held-out audio feature vector). A successful interpolation would indicate gener-ality of the model; specifically, the generalization of the subjective cross-modal feature-space mappings, such that the model could be used to predict the human subjective image response to unseen music audio data.  4.5 Evaluation To evaluate the degree of success of the models' predic-tions, the set of ground truth images per audio clip was \n58812th International Society for Music Information Retrieval Conference (ISMIR 2011)\n   \n used in a retrieval task. The two models performed slight-ly different retrieval tasks: Model 1 left a different sub-jectâ€™s predictor/response feature data out per run, for a total of 16 subjects. Here the goal was to was to assess the degree to which an individual subject's responses affect the image prediction result. The model was trained and tested repeatedly, omitting a single subjectâ€™s data each time, on the set of features corresponding to closest audio-image pairs from the remaining subjectsâ€™ similarity scores. To test, a response image feature was predicted for each audio feature using the regression weights. The co-sine distance was computed between the predicted image feature vector and the set of 10 feature vectors for the 10 images that the test subject ranked in the behavioral ex-periment. The distances were sorted such that those imag-es whose features were most similar to the predicted fea-tures were ranked more highly in the list of retrieved im-ages. Precision and recall values were computed by com-paring each ranked image with the relevant image set (ground truth). The recall level was also calculated; i.e. the proportion of ground truth images retrieved for each position in the retrieved image list. The mean precision was calculated by summing over all precision values and dividing by the total number of relevant items across all trials. Additionally, an f-score was computed using the 2P.R/(P+R) statistic and the mean f-score computed in a similar manner as the mean precision. Empirical p-values were computed using the distribution of mean precisions for 10,000 trials of randomly ordered image draws versus image draws ordered by similarity to the regression mod-el's predicted images. The resulting probability is inter-preted as the empirical probability that retrieval using randomly permuted image draws performed at least as well as retrieval using regression.       Model 2 was evaluated to test the generality of the model for unseen audio data. For this model, a leave-one-audio-out cross validation paradigm was used. Here, each training iteration omitted the audio / image feature pairs corresponding to one of the audio clips for all subjects. Testing consisted of predicting response image features for each held-out audio feature. As in Model 1, the cosine distance between each predicted image feature vector and the set of ground-truth images for the held-out audio clip yielded a ranked retrieval list of images that was used to calculate precision, recall, f-measure, and p-values, as discussed above.      By leaving one example out for testing, the models used 16-fold and 10-fold cross-validation respectively, a commonly used statistical technique for estimating the generalization power of a given model.  Furthermore, 10-fold cross-validation has been shown to be one of the best methods to use for model selection [13]. 5. RESULTS The results of both image prediction experiments are shown in Table 1. We performed a sensitivity analysis by systematically selecting subsets of features from the pre-dictor and response variables used for the regression and retrieval. In Table 1, results are shown both for the full ensemble and the best performing subsets of audio and image features. For the best-performing subset of fea-tures, 3 audio dimensions were left out and 2 image di-mensions. The p-values for the average precision were p<0.0001 for Model 1 and p<0.03 for Model 2. Figures 8 and 9 show the precision-recall curves for the two models for 1/10th percentile standardized recall levels. Model # trials avg. pre-cision avg.              f-score p-value 1 (full) 160 0.498 0.311 p<0.0001 2 (full) 18 0.299 0.248 0.867 1 (subset) 160 0.605 0.366 p<0.0001 2 (subset) 18 0.511 0.321 0.028 Table 1.  Cross-validation results for regression model audio-image feature prediction of 16 human subjectsâ€™ image response data to music stimuli. The subset model used four of seven audio features, and five of seven image features. Both versions of Model 1 perform significantly better than chance, with the per-subject-validation yielding a significance score of p<0.0001 (p=0 for 10,000 trials). However, only the feature subset version of Model 2 performed significantly above chance with p<0.03. The difference in performance between the two experiments is not wholly surprising. In the first experiment, the predictor/response data for a single subject is left out, but there are still 15 complete sets of audio-image data on which to train the regression model. Figure 8 illustrates the degree to which individual subjectsâ€™ data influences the overall result. The spread of the mean precision across individual runs is limited. Hence, we conclude that no one subject is contributing significantly more to the result than any other.  Figure 9 illustrates that the spread of results for the held-out audio-image data, across all subjects, varies significantly. This indicates unequal contributions to the model from different audio predictors and their corresponding cross- subject image responses. 6. CONCLUSIONS The results of this study show that it is possible to predict the relationship between artistic examples from both the audio and visual domains using feature extraction. Our perceptions of art are complex and multidimensional, even within a single domain, so multiple features from \n589Poster Session 4\n   \n each domain are likely contributing to the similarities perceived across domains. This makes the investigation of cross-modal congruencies within feature spaces particu-larly challenging. \n Figure 8. Retrieval performance for Model 1 showing the mean precision of individual runs (dashed lines) and the mean precision taken over all runs (solid line). \n Figure 9.  Retrieval performance for Model 2 showing the mean precision of individual runs (dashed lines) and the mean precision taken over all runs (solid line). Here, the model is trained on all subjectsâ€™ most similar audio-image feature pairs for left-in audio.       Further research can investigate the correlations be-tween multiple features of audio and images. The choice of features in this study was somewhat arbitrary, but seemed like an intuitive place to start.  The techniques used here demonstrated the use of low-level features. However, the complexity of the problem suggests that many more features are contributing to the relationship between domains.      A primary limitation of the results of this study is a possible lack of generalizability due to the small size of the data set.  The data set was kept small out of considera-tion for the behavioral experiment design.  The subjects had to give similarity ratings for all possible combinations of visual and auditory art, which amounted to 100 trials total.  With this amount of stimuli, the behavioral test took 30-40 minutes.  Adding more stimuli would cause the behavioral test to increase in length exponentially.  Considering the attention span of subjects is important in this regard, because an experiment that was much longer could have compromised the integrity of the responses.      Research in the area of cross-modal congruencies pro-vides a step towards understanding the perceptual pro-cesses related to cross-modal binding. Our minds our con-stantly receiving input streams from various senses and must use them to create the continuous and whole experi-ence of consciousness. Identifying how modality- specific features relate and integrate across domains is a funda-mental part of the discovery of our constant reality, e plu-ribus unum. 7. REFERENCES [1] Marks, Lawrence E.  \"On cross-modal similarity: Audio-visual interactions in speeded discrimination.\" Journal of Experimental Psychology: Human Perception and Performance.  Vol. 13, No. 3, pp. 384-394, 1987. [2] Chen, Chin-Han, Ming-Fang Weng, Shyh-Kang Jang, and Yung-Yu Chuang. â€œEmotion Based Music Visualization Using Photos.â€  Advances in Multimedia Modeling.  Springer, Berlin: 2008. [3] Cooper, M. and J. Foote.  â€œVisualizing Music and Rhythm via Self-Similarity.â€  Proceedings ICMC, 2002. [4] Mardiossian, Arpi and Elaine Chew.  â€œVisualizing Music: Tonal Progressions and Distributions.â€ Proceedings ISMIR, 2007. [5] Yeo, Woon Seung and Jonathon Berger. â€œApplication of Image Sonification to Music.â€Proceedings of ISMIR, 2005. [6] Libeks, Janis and Douglas Turnbull.  â€œExploring 'Artist Image' Using Content-Based Analysis of Promotional Photos.â€  Proceedings of the International Computer Music Conference, 2010. [7] Grey, John M.  \"Multidimensional perceptual scaling of musical timbres.\"  Journal of the Acoustical Society of America.  Vol. 61, Issue 5, pp. 1270 â€“ 1277, 1979. [8] Hoffman, Thomas, Jan Puzicha, and Michael I. Jordan.  \"Learning from dyadic data.\"  Proceedings of the 1998 conference on Advances in Neural Information Processing Systems II.  MIT Press, Cambridge, MA: 1999. [9] Tzanetakis, George and P. Cook.  â€œMusical Genre Classification of Audio Signals.â€ IEEE Transactions on Speech and Audio Processing, Vol. 10, No. 5, July 2002. [10] Deselaers, T., D. Keysers, and H. Ney.  â€œFeatures of image retrieval: An experimental comparison.â€ Information Retrieval, 2008. [11] McCullagh, P., and J.A. Nelder, Generalized Linear Models, 2nd edition, Chapman&Hall/CRC Press, 1990. [12] Mitchell, Tom M., S. V. Shinkareva, A. Carlson, K. Chang, V. Malave, R. Mason, and M. A. Just.  â€œPredicting Human Brain Activity Associated with the Meaning of Nouns.â€  Science, Vol. 320, No. 5880, 2008. [13] Kohavi, Ron.  â€œA Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection.â€ International Joint Conference on Artificial Intelligence (IJCAI), 1995. \n590"
    },
    {
        "title": "Timbre and Melody Features for the Recognition of Vocal Activity and Instrumental Solos in Polyphonic Music.",
        "author": [
            "Matthias Mauch",
            "Hiromasa Fujihara",
            "Kazuyoshi Yoshii",
            "Masataka Goto"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1415716",
        "url": "https://doi.org/10.5281/zenodo.1415716",
        "ee": "https://zenodo.org/records/1415716/files/MauchFYG11.pdf",
        "abstract": "We propose the task of detecting instrumental solos in polyphonic music recordings, and the usage of a set of four audio features for vocal and instrumental activity detection. Three of the features are based on the prior extraction of the predominant melody line, and have not been used in the context of vocal/instrumental activity detection. Using a support vector machine hidden Markov model we conduct 14 experiments to validate several combinations of our proposed features. Our results clearly demonstrate the benefit of combining the features: the best performance was always achieved by combining all four features. The top accuracy for vocal activity detection is 87.2%. The more difficult task of detecting instrumental solos equally benefits from the combination of all features and achieves an accuracy of 89.8% and a satisfactory precision of 61.1%. With this paper we also release to the public the 102 annotations we used for training and testing. The annotations offer not only vocal/nonvocal labels, but also distinguish between female and male singers, and different solo instruments. Keywords: vocal activity detection, pitch fluctuation, F0 segregation, instrumental solo detection, ground truth, SVM",
        "zenodo_id": 1415716,
        "dblp_key": "conf/ismir/MauchFYG11",
        "keywords": [
            "instrumental solos",
            "vocal activity detection",
            "support vector machine hidden Markov model",
            "audio features",
            "melody line extraction",
            "ground truth",
            "F0 segregation",
            "polyphonic music recordings",
            "vocal/nonvocal labels",
            "precision"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nTIMBRE AND MELODY FEATURES FOR THE RECOGNITION OF VOCAL\nACTIVITY AND INSTRUMENTAL SOLOS IN POLYPHONIC MUSIC\nMatthias Mauch Hiromasa Fujihara Kazuyoshi Yoshii Masataka Goto\nNational Institute of Advanced Industrial Science and Technology (AIST), Japan\n{m.mauch, h.fujihara, k.yoshii, m.goto }@aist.go.jp\nABSTRACT\nWe propose the task of detecting instrumental solos in poly-\nphonic music recordings, and the usage of a set of four audio\nfeatures for vocal and instrumental activity detection. Three\nof the features are based on the prior extraction of the pre-\ndominant melody line, and have not been used in the context\nof vocal/instrumental activity detection. Using a support\nvector machine hidden Markov model we conduct 14 exper-\niments to validate several combinations of our proposed fea-\ntures. Our results clearly demonstrate the beneï¬t of combin-\ning the features: the best performance was always achieved\nby combining all four features. The top accuracy for vocal\nactivity detection is 87.2%. The more difï¬cult task of de-\ntecting instrumental solos equally beneï¬ts from the combi-\nnation of all features and achieves an accuracy of 89.8% and\na satisfactory precision of 61.1%. With this paper we also\nrelease to the public the 102 annotations we used for train-\ning and testing. The annotations offer not only vocal/non-\nvocal labels, but also distinguish between female and male\nsingers, and different solo instruments.\nKeywords: vocal activity detection, pitch ï¬‚uctuation, F0\nsegregation, instrumental solo detection, ground truth, SVM\n1. INTRODUCTION\nThe presence and quality of vocals and other melody instru-\nments in a musical recording are understood by most listen-\ners, and often these are also the parts of the music listeners\nare interested in. Music enthusiasts, radio disk-jockeys and\nother music professionals can use the locations of vocal and\ninstrumental activity to efï¬ciently navigate to the song po-\nsition theyâ€™re interested in, e.g. the ï¬rst vocal activity, or the\nguitar solo. In large music collections, the locations of vo-\ncal and instrumental activity can be used to offer meaningful\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proï¬t or commercial advantage and that copies\nbear this notice and the full citation on the ï¬rst page.\nc/circlecopyrt2011 International Society for Music Information Retrieval.audio thumbnails (song previews) and better browsing and\nsearch functionality.\nDue to its apparent relevance to music listeners and in\ncommercial applications the automatic detection of vocals\nin particular has received considerable attention in the recent\nMusic Information Retrieval literature, which we review be-\nlow. Far less attention has been dedicated to the detection of\ninstrumental solos in polyphonic music recordings.\nIn the present publication we present a state-of-the-art\nmethod for vocal activity detection. We show that the use\nof several different timbre-related features extracted based\non a preliminary extraction of the predominant melody line\nprogressively improve the performance of locating singing\nsegments. We also introduce the new task of instrumental\nsolo detection and show that, here too, the combination of\nour proposed features leads to substantial performance in-\ncreases.\nSeveral previous approaches to singing detection in poly-\nphonic music have relied on multiple features. Berenzweig\n[2] uses several low-level audio features capturing the spec-\ntral shape, and learned model likelihoods of these. Fujihara\nuses both [3] a spectral feature and a feature that captures\npitch ï¬‚uctuation based on a prior estimation of the predom-\ninant melody. Thus more aspects of the complex human\nvoice can be captured and modelled. In fact, Regnier and\nPeeters [14] note that â€œthe singing voice is characterized\nby harmonicity, formants, vibrato and tremoloâ€. However,\nmost papers are restricted to a small number of (usually\nspectral) features [8, 9, 14]. Nwe and Li [12] have proposed\nthe most diverse set of features for vocal recognition that we\nare aware of, including spectral timbre, vibrato and a mea-\nsure of pitch height.\nOur method is similar to that of Nwe and Li in that we\nuse a wide range of audio features. However, our novel mea-\nsurement of pitch ï¬‚uctuation (similar to vibrato) is tuning-\nindependent and based on a prior extraction of the predom-\ninant melody. Furthermore, we propose two new features\nthat are also based on the preliminary melody extraction\nstep: the timbre (via Mel-frequency cepstral coefï¬cients) of\nthe isolated predominant melody, and the relative amplitude\nof the harmonics of the predominant melody.\n233Poster Session 2\nThe remainder of the paper is organised as follows: in\nSection 2 we describe the features used in our study. Sec-\ntion 3 describes a new set of highly detailed ground truth\nannotations for more than 100 songs published with this pa-\nper. The experimental setup and the machine learning tools\ninvolved in training and testing our methods are explained\nSection 4. The results are discussed in Section 5. Limi-\ntations of the present method and future directions are dis-\ncussed in Section 6.\n2. AUDIO FEATURES\nThis section introduces the four audio features considered in\nthis paper: the standard MFCCs, and three features based on\nthe extracted melody line: pitch ï¬‚uctuation, MFCCs of the\nre-synthesized predominant voice, and the relative harmonic\namplitudes of the predominant voice.\nWe ï¬rst extract all features from each track at a rate of\n100 frames per second from audio sampled at 16 kHz, then\nlow-pass ï¬lter and downsample them to obtain features at 10\nframes per second, which we use as the input to the training\nand testing procedures (Section 4).\n2.1 Mel-frequency cepstral coefï¬cients\nMel-frequency cepstral coefï¬cients [11] are a vector-shaped\nfeature which has the desirable property of describing the\nspectral timbre of a piece of audio while being largely ro-\nbust to changes in pitch. This property has made them the\nde facto standard input feature for most speech recognition\nsystems. The calculation of MFCCs consists of a discrete\nFourier transform of the audio samples to the frequency do-\nmain, applying an equally-spaced ï¬lter bank in the mel fre-\nquency scale (approximately linear in log frequency), and\nï¬nally applying the discrete cosine transform to the loga-\nrithm of the ï¬lter bank output. Details are extensively cov-\nered elsewhere, see e.g. [13]. In our implementation, the\nhop size is 160 samples (10 ms), the frame size is 400 sam-\nples (a 512-point FFT was used with zero-padding) and the\naudio window used is a Hamming window.\n2.2 Pitch Fluctuation\nThe calculation of pitch ï¬‚uctuation involves three steps:\nfundamental F0: estimate the fundamental frequency (F0)\nof the predominant voice at every 10ms frame using\nPreFEst [4], and take the logarithm to map them to\npitch space,\ntuning shift: infer a song-wide tuning from these estimates,\nshift the estimates so that they conform to a standard\ntuning and wrap them to a semitone interval,\nintra-semitone ï¬‚uctuation: calculate the standard devia-\ntion of the frame-wise frequency difference.We use the program PreFEst [4] to obtain an estimate of\nthe fundamental frequency (F0) of the predominant voice at\nevery 10ms frame. For a frame at position tâˆˆ{1,...,N}\nin which PreFEst detects any fundamental frequency f[t]we\nconsider its pitch representation fâˆ—\nlog[t] = log2f[t], i.e. the\ndifference between two adjacent semitones is1\n12.\nThe tuning shift in the second step is motivated as fol-\nlows: our ï¬nal pitch ï¬‚uctuation measure employs pitch esti-\nmates wrapped into the range of one semitone. The wrapped\nrepresentation has the beneï¬t of discarding sudden octave\njumps and similar transcription artifacts, but if the semitone\nboundary is very close to the tuning pitch of the piece, then\neven small ï¬‚uctuations will cross this boundary (they â€˜wrap\naroundâ€™) and lead to many artiï¬cial jumps of one semitone.\nThis can be avoided if we shift the frequency estimates such\nthat the new tuning pitch is at the centre of the wrapped\nsemitone interval. In order to calculate the tuning of the\npiece we use a histogram approach (like [6]): all estimated\nvaluesfâˆ—\nlog[t],tâˆˆ{1,...,N}are wrapped into the range of\none semitone,\nfâˆ—\nlog[t]/parenleftbigg\nmod1\n12/parenrightbigg\n,tâˆˆ{1,...,N}, (1)\nand sorted into a histogram (h1,...,h 100)with 100 his-\ntogram bins, equally-spaced at1\n1200, or one cent. The rela-\ntive tuning frequency is obtained from the histogram as\nfref\nlog =(arg max ihi)âˆ’1\n1200âˆ’0.5 (2)\nâˆˆ {âˆ’ 0.5,âˆ’0.49,..., 0.49},\nand the semitone-wrapped frequency estimates we use in the\nthird step are\nflog[t] =/parenleftbig\nfâˆ—\nlog[t]âˆ’fref\nlog/parenrightbig/parenleftbigg\nmod1\n12/parenrightbigg\n,tâˆˆ{1,...,N}.\nThe third step calculates a measure of ï¬‚uctuation on win-\ndows of the frame-wise values flog[t]. We use Fujiharaâ€™s for-\nmulation [3] of the frequency difference (up to a constant)\nâˆ†flog[t] =2/summationdisplay\nk=âˆ’2kÂ·flog[t+k] (3)\nand deï¬ne pitch ï¬‚uctuation as the Hamming-weighted stan-\ndard deviation of values âˆ†flog[.]in a neighbourhood of t,\nF[t] = 12Â·/radicaltp/radicalvertex/radicalvertex/radicalbt50/summationdisplay\nk=1wk(âˆ†flog[t+kâˆ’25]âˆ’Âµ[t])2,(4)\nwhereÂµ[t] =/summationtext50\nk=1wkâˆ†flog[t+kâˆ’25]is the Hamming-\nweighted mean, and wk,k= 1,..., 50is a Hamming win-\ndow scaled such that/summationtext\nkwk= 1.\nIn short, F [t]summarises the spread of frequency changes\nof the predominant fundamental frequency in a window around\nthetthframe.\n23412th International Society for Music Information Retrieval Conference (ISMIR 2011)\n2.3 MFCCs of Re-Synthesised Predominant Voice\nWe hypothesize that audio features that describe the pre-\ndominant voice in a polyphonic recording in isolation will\nimprove the characterisation of the singing voice and solo\ninstruments. To obtain such a feature we re-synthesize the\nestimated predominant voice and perform the MFCC fea-\nture extraction on the resulting monophonic waveform. For\nthe re-synthesis itself we use an existing method [3] which\nemploys sinusoidal modelling based on the PreFEst esti-\nmates of predominant fundamental frequency and the esti-\nmated amplitudes of the harmonic partials pertaining to that\nfrequency. MFCC features of the re-synthesized audio are\ncalculated as explained in Section 2.1. They describe the\nspectral timbre of isolated the most dominant note.\n2.4 Normalised Amplitudes of Harmonic Partials\nThe MFCC features described in Sections 2.1 and 2.3 cap-\nture the spectral timbre of a sound, but they do not con-\ntain information on another dimension of timbre: the nor-\nmalised amplitudes of the harmonic partials of the predom-\ninant voice. Unlike the MFCC feature of the re-synthesised\npredominant voice, this feature uses the amplitude values\nthemselves, i.e. at every frame the feature is derived from\nthe estimated harmonic amplitudes A= (A1,...,A 12)by\nnormalising them according to the Euclidean norm,\nHi=Ai/radicalbig/summationtext\niA2\ni(5)\n3. REFERENCE ANNOTATIONS\nWe introduce a new set of manually generated reference an-\nnotations to 112 full-length pop songs: 100 songs from the\npopular music collection of the RWC Music Database [5],\nand 12 further pop songs. The annotations describe ac-\ntivity in contiguous segments of audio using seven main\nclasses: fâ€“ female lead vocal, mâ€“ male lead vocal, gâ€“\ngroup singing (choir), sâ€“ expressive instrumental solo, pâ€“\nexclusively percussive sounds, bâ€“ background music that\nï¬ts none of the above, nâ€“ no sound (silence or near si-\nlence). Thereâ€™s also an additional elabel denoting the end\nof the piece. In practice, music does not always conform to\nthese labels, especially when several expressive sources are\nactive. In such situations we chose to annotate the predomi-\nnant voice (with precedence for vocals) and added informa-\ntion about the conï¬‚ict, separated by a colon, e.g.\nm:withf .\nSimilarly, the label for expressive instrumental solo, s, is\nalways further speciï¬ed by the instrument used, e.g.\ns:electricguitar .\nbackground 22.0 %female 30.6 %\nmale 32.8 %group 2.0 %inst. solo 12.6 %Figure 1 : Ground truth label distribution: the pie chart la-\nbels provide information on the distribution in the extended\nmodel with ï¬ve classes. The simple model joins all vocal\nclasses (dark grey, 65.4%) and all non-vocal classes (light\ngrey, 34.6%).\nThe reference annotations are freely available for download1.\n4. EXPERIMENTS\nWe used 102 of the ground truth songs and mapped the rich\nground truth annotation data down to fewer classes accord-\ning to two different schemes:\nsimple contains two classes: vocal (comprising ground truth\nlabels f,mandg) and non-vocal (comprising all other\nground truth labels)\nextended contains ï¬ve classes: female ,male ,group for the\nannotations f,mandg, respectively; solo(ground truth\nlabels); and remainder (all remaining labels)\nThe frequency of the different classes is visualised in Fig-\nure 1. Short background segments (ground truth label b)\nof less than 0.5 s duration were merged with the preceding\nregion.\nWe examine seven different feature conï¬gurations, the\nfour single features pitch ï¬‚uctuation (F), MFCCs (M), MFCCs\nof the re-synthesised melody line (R) and normalised apli-\ntudes of the harmonics (H), and the following progressive\ncombinations of the four: FM, FMR and FMRH.\nThe relevant features in each feature conï¬guration are\ncast into a single vector per frame. We use the support vec-\ntor machine version of a hidden Markov model [1] SVM-\nHMM [7] via an open source implementation2. We trained\na model with the default order of 1, i.e. with the probability\nof transition to a state depending only on the respective pre-\nvious state. The slack parameter was set to c= 50 , and\nthe parameter for required accuracy was set to e= 0.6.\nThe 102 songs are divided into ï¬ve sets for cross-validation.\nThe estimated sequence is of the same format as the mapped\nground truth, i.e. either two classes ( simple schema) or ï¬ve\nclasses ( extended schema).\n1http://staff.aist.go.jp/m.goto/RWC-MDB/\nAIST-Annotation/\n2http://www.cs.cornell.edu/people/tj/svm_light/\nsvm_hmm.html\n235Poster Session 2\nHRMFFMFMRFMRH\naccuracy0.0 0.2 0.4 0.6 0.8simple\next. 62.5%68.2%70.8%73.6%70.4%73.8%74.4%79.2%79.8%82.7%82.8%85.2%84.9%87.2%(a) accuracy\nHRMFFMFMRFMRH\nspecificity0.0 0.2 0.4 0.6 0.8simple\next. 52.2%12.0%64.9%37.1%58.1%38.3%68.2%60.7%73.6%67.6%80.2%73.7%82.4%77.8% (b) speciï¬city\nHRMFFMFMRFMRH\nsegmentation metric0.0 0.2 0.4 0.6 0.8simple\next. 54.4%39.3%61.5%60.3%61.6%54.9%53.6%63.1%66.7%67.6%67.5%69.9%68.8%72.4% (c) segmentation accuracy\nFigure 2 : V ocal activity detection (see Section 5.1).\n5. RESULTS\nIn order to give a comprehensive view of the results we\nuse four frame-wise evaluation metrics for binary classiï¬-\ncation: accuracy, precision, recall/sensitivity and speciï¬city.\nThese metrics can be represented in terms of the number of\ntrue positives (TP; method says its positive and ground truth\nagrees), true negatives (TN; method says itâ€™s negative and\nground truth agrees), false positives (FP; method says itâ€™s\npositive, ground truth disagrees) and false negatives (FN;\nmethod says itâ€™s negative, ground truth disagrees).\naccuracy =TP+TN\n# all frames, precision =TP\nTP+FP\nrecall =TP\nTP+FN, speciï¬city =TN\nTN+FP.\nWe also provide a measure of segmentation accuracy as one\nminus the minimum of the directional Hamming divergences,\nas proposed by Christopher Harte in the context of measur-\ning chord transcription accuracy. For details see [10, p. 52].\n5.1 Vocal Activity Detection\nTable 1 provides all frame-wise results of vocal activity\ndetection in terms of the four metrics shown above. The\nhighest overall accuracy of 87.2% is achieved by the simple\nFMRH method. The difference to the second-best algorithm\nin terms of accuracy ( simple FMRH) is statistically signiï¬-\ncant according to the Friedman test ( pvalue:<10âˆ’7).\nAccuracy of single features. Figure 2a shows the dis-\ntinct accuracy differences between the individual single au-\ndio features. The H feature by itself has a very low accu-\nracy of 68.2% (62.5% in the extended model). The accuracy\nobtained by either the MFCC-based features, M and R are\nalready considerably higherâ€”up to 73.8%â€”and the pitch\nï¬‚uctuation measure F is the measure with the highest accu-\nracy of 79.2% (73.4% in the extended model) among modelswith a single feature. This suggests that pitch ï¬‚uctuation is\nthe most salient feature of the vocals in our data.\nProgressively combining features. It is also very clear\nthat the methods using more than one feature have an ad-\nvantage: every additional feature increases the accuracy of\nvocal detection. In particular, the R featureâ€”MFCCs of\nthe re-synthesised melody lineâ€”signiï¬cantly increases ac-\ncuracy when added to the feature set that already contains\nthe basic MFCC features M. This suggests that R and M\nhave characteristics that complement each other. More sur-\nprising, perhaps, is the fact that the addition of the H feature,\nwhich is a bad vocal classiï¬er on its own, leads to a signiï¬-\ncant improvement in accuracy.\nPrecision and Speciï¬city. If we consider the accuracy\nvalues alone it seems to be clear that the simple model is\nbetter: it outperforms the extended model in every feature\nsetting. This is, however, not the conclusive answer. Accu-\nracy tells only part of the story, and other measures such as\nprecision and speciï¬city are helpful to examine different as-\npects of the methodsâ€™ performance. The recall measure does\nnot provide very useful information in this case, becauseâ€”\nunlike in usual information retrieval tasksâ€”the vocal class\noccupies more than half the database, see Figure 1. Hence,\nit is very easy to make a trivial high-recall classiï¬er by ran-\ndomly assigning a high proportion xof frames to the pos-\nitive class. To illustrate this, we have added theoretical re-\nsults for the trivial classiï¬ers â€˜rand- xâ€™ to Table 1. A more\ndifï¬cult problem, then, is to make a model that retains high\nrecall but also has high precision and speciï¬city. Speciï¬city\nis the recall of the negative class, i.e. the ratio of non-vocal\nframes that have been identiï¬ed as such, and precision is\nthe ratio of truly vocal frames in what the automatic method\nclaims it is. The extended methods outperform each cor-\nresponding simple method in terms of precision and speci-\nï¬city. Figure 2b also shows that better results are achieved\n23612th International Society for Music Information Retrieval Conference (ISMIR 2011)\naccuracy precision recall speciï¬city\nrand-0.500 0.500 0.654 0.500 0.500\nrand-0.654 0.547 0.654 0.654 0.346\nrand-1.000 0.654 0.654 1.000 0.000\nsimple H 0.682 0.678 0.979 0.120\nsimple R 0.736 0.736 0.930 0.371\nsimple M 0.738 0.739 0.926 0.383\nsimple F 0.792 0.811 0.891 0.607\nsimple FM 0.827 0.841 0.907 0.676\nsimple FMR 0.852 0.868 0.913 0.737\nsimple FMRH 0.872 0.887 0.921 0.778\next. H 0.625 0.729 0.680 0.522\next. R 0.708 0.799 0.740 0.649\next. M 0.704 0.775 0.770 0.581\next. F 0.744 0.822 0.777 0.682\next. FM 0.798 0.856 0.830 0.736\next. FMR 0.828 0.889 0.842 0.802\next. FMRH 0.849 0.903 0.863 0.824\nTable 1 : Recognition measures for vocal activity.\nby adding our novel audio features.\nSegmentation accuracy. As we would expect from the\nabove results, the segmentation accuracy, too, improves with\nincreasing model complexity. The top segmentation accu-\nracy of the top score of 0.724 is is approaching that of state-\nof-the-art chord segmentation techniques (e.g. [10, p. 88],\n0.782). For the four best feature combinations the simple\nmethods slightly outperform the extended ones, by 2 to 4\npercentage points.\nThe best extended method, extended FMRH, has the high-\nest precision (90.3%) and speciï¬city (82.4%) values of all\ntested algorithms, while retaining high accuracy and recall\n(84.9% and 86.3%, respectively). In most situations this\nwould be the method of choice, though the respective sim-\nplemethod has a slight advantage in terms of segmentation\naccuracy.\n5.2 Instrumental Solo Activity\nMore difï¬cult than detecting vocals is detecting the instru-\nmental solos in polyphonic pop songs because they occupy\na smaller fraction of the total number of frames (12.6%, see\nFigure 1). Hence, this situation is more similar to a tradi-\ntional retrieval task (the desired positive class is rare), and\nprecision and recall are the relevant measures for this task.\nTable 1 shows all results, andâ€”for comparisonâ€”the theo-\nrtical performance of the three classiï¬ers â€˜rand- xâ€™ that ran-\ndomly assign a ratio of xframes to the solo class.\nThe method that includes all our novel audio features,\nFMRH, achieves the highest accuracy of all methods. How-\never, all methods show high accuracy and speciï¬city; preci-\nsion and recall show the great differences between the meth-\nods. Figure 3 illustrates the differences in precision of solo\nHRMFFMFMRFMRH\nprecision0.0 0.1 0.2 0.3 0.4 0.5 0.629.8%46.5%52.5%22.4%53.8%57.7%61.1%Figure 3 : Detection of instrumental solos: precision of the\nextended methods.\naccuracy precision recall speciï¬city\nrand-0.126 0.780 0.126 0.126 0.874\nrand-0.500 0.500 0.126 0.500 0.500\nrand-1.000 0.126 0.126 1.000 0.000\next. H 0.829 0.298 0.262 0.911\next. R 0.866 0.465 0.406 0.933\next. M 0.877 0.525 0.290 0.962\next. F 0.860 0.224 0.045 0.977\next. FM 0.876 0.538 0.152 0.981\next. FMR 0.889 0.577 0.445 0.953\next. FMRH 0.898 0.611 0.519 0.952\nTable 2 : Recognition metrics for instrumental solo activity.\ndetection between the extended methods. The methods that\ncombine our novel features have a distinct advantage, with\nthe FMRH feature setting achieving the highest precision.\nNote, however, that the precision ranking of the individual\nfeatures is different from the vocal case, where the F fea-\nture was best and the M and R features showed very similar\nperformance: the method using the R feature alone is now\nsubstantially better than that of the simple MFCC feature M,\nsuggesting that using the isolated timbre of the solo melody\nis a decisive advantage. The F feature alone shows low pre-\ncision, which is expected because pitch ï¬‚uctuation is high\nfor vocals as well as instrumental solos.\nConsidering that the precision of a random classiï¬er in\nthis task is 12.6% the best performance of 61.1%â€”though\nnot idealâ€”makes it interesting for practical applications.\nFor example, in a situation where a TV editor requires an\nexpressive instrumental as a musical backdrop to the video\nfootage, a system implementing our method could substan-\ntially reduce the amount of time needed to ï¬nd suitable ex-\ncerpts.\n6. DISCUSSION AND FUTURE WORK\nA capability of the extended methods we have not discussed\nin this paper is to detect whether the singer in a song is\nmale or female. A simple classiï¬cation method is to take\nthe more frequent of the two cases in a track as the track-\n237Poster Session 2\nwise estimate, resulting in a 70.1% track-wise accuracy. In\nthis context, we are currently investigating hierarchical time\nseries models that allow us to represent a global song model,\ne.g. â€˜female songâ€™, â€˜female-male duetâ€™ or â€˜instrumentalâ€™. In-\nformal experiments have shown that this strategy can in-\ncrease overall accuracy, and as a side-effect it delivers a\nsong-level classiï¬cation which can be used to distinguish\nnot only whether a trackâ€™s lead vocal is male or female, but\nalso whether the song has vocals at all.\n7. CONCLUSIONS\nWe have proposed the usage of a set of four audio features\nand the new task of detecting instrumental solos in poly-\nphonic audio recordings of popular music. Among the four\nproposed audio features three are based on a prior transcrip-\ntion of the predominant melody line, and have not been used\nin the context of vocal/instrumental activity detection. We\nconducted 14 different experiments with 7 feature combina-\ntions and two different SVM-HMM models. Training and\ntesting was done using 5-fold cross-validation on a set of\n102 popular music tracks. Our results demonstrate the ben-\neï¬t of combining the four proposed features. The best per-\nformance for vocal detection is achieved by using all four\nfeatures, leading to a top accuracy of 87.2% and a satisfac-\ntory segmentation performance of 72.4%. The detection of\ninstrumental solos equally beneï¬ts from the combination of\nall features. Accuracy is also high (89.8%), but we argue\nthat the main improvement through the features can be seen\nin the increase in precision to 61.1%. With this paper we\nalso release to the public the annotations we used for train-\ning and testing. The annotations offer not only vocal/non-\nvocal labels, but also distinguish between female and male\nsingers, and different solo instruments.\nThis work was supported in part by CrestMuse, CREST,\nJST. Further thanks to Queen Mary University of London\nand Last.fm for their support.\n8. REFERENCES\n[1] Y . Altun, I. Tsochantaridis, and T. Hofmann. Hidden\nMarkov support vector machines. In Proceedings of the\nTwentieth International Conference on Machine Learn-\ning (ICML 2003) , 2003.\n[2] A.L. Berenzweig and D.P.W. Ellis. Locating singing\nvoice segments within music signals. In Applications of\nSignal Processing to Audio and Acoustics, 2001 IEEE\nWorkshop on the , pages 119â€“122. IEEE, 2001.\n[3] H. Fujihara, M. Goto, J. Ogata, K. Komatani, T. Ogata,\nand H.G. Okuno. Automatic synchronization between\nlyrics and music CD recordings based on Viterbi align-\nment of segregated vocal signals. In 8th IEEE Interna-tional Symposium on Multimedia (ISMâ€™06) , pages 257â€“\n264, 2006.\n[4] Masataka Goto. A real-time music scene descrip-\ntion system: Predominant-F0 estimation for detect-\ning melody and bass lines in real-world audio signals.\nSpeech Communication , 43(4):311â€“329, 2004.\n[5] Masataka Goto, Hiroki Hashiguchi, Takuichi\nNishimura, and Ryuichi Oka. RWC Music Database:\nPopular, classical, and jazz music databases. In Pro-\nceedings of the 3rd International Conference on Music\nInformation Retrieval (ISMIR 2002) , pages 287â€“288,\n2002.\n[6] Christopher Harte and Mark Sandler. Automatic chord\nidentifcation using a quantised chromagram. In Pro-\nceedings of 118th Convention . Audio Engineering So-\nciety, 2005.\n[7] T. Joachims, T. Finley, and C.N.J. Yu. Cutting-\nplane training of structural SVMs. Machine Learning ,\n77(1):27â€“59, 2009.\n[8] H. Lukashevich, M. Gruhne, and C. Dittmar. Effective\nsinging voice detection in popular music using arma ï¬l-\ntering. In Workshop on Digital Audio Effects (DAFxâ€™07) ,\n2007.\n[9] N.C. Maddage, K. Wan, C. Xu, and Y . Wang. Singing\nvoice detection using twice-iterated composite fourier\ntransform. In Proceedings of the IEEE International\nConference on Multimedia and Expo (ICME 2004) , vol-\nume 2, 2004.\n[10] Matthias Mauch. Automatic Chord Transcription from\nAudio Using Computational Models of Musical Context .\nPhD thesis, Queen Mary University of London, 2010.\n[11] P. Mermelstein. Distance measures for speech recogni-\ntion. In International Conference on Acoustics, Speech\nand Signal Processing , pages 708â€“711, 1978.\n[12] T.L. Nwe and H. Li. On fusion of timbre-motivated fea-\ntures for singing voice detection and singer identiï¬ca-\ntion. In IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP 2008) , pages\n2225â€“2228. IEEE, 2008.\n[13] Lawrence R. Rabiner and Ronald W. Schafer. Introduc-\ntion to Digital Speech Processing . Now Publishers Inc.,\n2007.\n[14] L. Regnier and G. Peeters. Singing voice detection in\nmusic tracks using direct voice vibrato detection. In\nIEEE International Conference on Acoustics, Speech,\nand Signal Processing (ICASSP 2009) , 2009.\n238"
    },
    {
        "title": "Structural Change on Multiple Time Scales as a Correlate of Musical Complexity.",
        "author": [
            "Matthias Mauch",
            "Mark Levy"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1416240",
        "url": "https://doi.org/10.5281/zenodo.1416240",
        "ee": "https://zenodo.org/records/1416240/files/MauchL11.pdf",
        "abstract": "We propose the novel audio feature structural change for the analysis and visualisation of recorded music, and argue that it is related to a particular notion of musical complexity. Structural change is a meta feature that can be calculated from an arbitrary frame-wise basis feature, with each element in the structural change feature vector representing the change of the basis feature at a different time scale. We describe an efficient implementation of the feature and discuss its properties based on three basis features pertaining to harmony, rhythm and timbre. We present a novel flowerlike visualisation that allows us to illustrate the overall structural change characteristics of a piece of audio in a compact way. Several examples of real-world music and synthesised audio exemplify the characteristics of the structural change feature. We present the results of a web-based listening experiment with 197 participants to show the validity of the proposed feature. Keywords: audio, musical complexity, visualisation",
        "zenodo_id": 1416240,
        "dblp_key": "conf/ismir/MauchL11",
        "keywords": [
            "audio",
            "structural change",
            "music",
            "analysis",
            "visualisation",
            "basis feature",
            "harmony",
            "rhythm",
            "timbre",
            "flowerlike visualisation"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nSTRUCTURAL CHANGE ON MULTIPLE TIME SCALES AS A\nCORRELATE OF MUSICAL COMPLEXITY\nMatthias Mauch Mark Levy\nLast.fm, Karen House, 1â€“11 Bacheâ€™s Street,\nLondon, N1 6DL. United Kingdom.\nmatthias@last.fm mark@last.fm\nABSTRACT\nWe propose the novel audio feature structural change for\nthe analysis and visualisation of recorded music, and argue\nthat it is related to a particular notion of musical complex-\nity. Structural change is a meta feature that can be calcu-\nlated from an arbitrary frame-wise basis feature, with each\nelement in the structural change feature vector representing\nthe change of the basis feature at a different time scale. We\ndescribe an efï¬cient implementation of the feature and dis-\ncuss its properties based on three basis features pertaining\nto harmony, rhythm and timbre. We present a novel ï¬‚ower-\nlike visualisation that allows us to illustrate the overall struc-\ntural change characteristics of a piece of audio in a compact\nway. Several examples of real-world music and synthesised\naudio exemplify the characteristics of the structural change\nfeature. We present the results of a web-based listening ex-\nperiment with 197 participants to show the validity of the\nproposed feature.\nKeywords: audio, musical complexity, visualisation\n1. INTRODUCTION\nA piece of music has many qualities that inï¬‚uence how it is\nperceived by human beings. These qualities include timbre,\nrhythm and harmony. One further, distinct property is the\nway in which timbre, rhythm, harmony and other features\nare temporally organised into units of various lengths over\nthe course of the piece, from the smallest note change to the\nchange between two sections. In this paper we propose an\naudio feature aimed at characterising part of this temporal,\nstructural organisation.\nA measure of structural change can be useful for mu-\nsic browsing within a track or in collections of music. In\nparticular, suitable visualisations of the feature can directly\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proï¬t or commercial advantage and that copies\nbear this notice and the full citation on the ï¬rst page.\nc/circlecopyrt2011 International Society for Music Information Retrieval.be used for concise thumbnail-like descriptions of musical\npieces. As a measure of complexity, structural change lends\nitself to the exploration of the cultural evolution of music.\nParry [8] provides an overview of research in music com-\nplexity and applies several measures of complexity on sym-\nbolic music. In the audio domain, Streich [10] gives a com-\nprehensive description of existing theories and techniques.\nHe also discusses many deï¬nitions of complexity in science\nand their application to music, noting that pure information-\ntheoretical and mathematical approaches such as entropy\nand Kolmogorov complexity can limit the exploration of\nhuman-perceived complexity.\nOur approach is inspired by a biological notion of com-\nplexity [1] according to which things are deï¬ned as more\ncomplex the less likely they could have come into existence\nby chance. More speciï¬cally, we focus on the aspect of dis-\ntinction, the fact that â€œdifferent parts of the complex behave\ndifferentlyâ€ [5]. As an example in the domain of audio, con-\nsider two ten-second waveforms: one exclusively consisting\nof pink noise, the other one consisting of ï¬ve seconds of\npink noise followed by ï¬ve seconds of white noise. Clearly,\nsomething must have happened in the middle of the second\nwaveform that resulted in this change, or, in musical terms,\nthe second piece must have had a â€˜composerâ€™.\nIn real music, such structural changes happen in all\nmusical qualities (including rhythm and harmony), andâ€”\nequally importantlyâ€”they happen on all time scales within\nthe range of the length of a piece. Our proposed feature\ncaptures these structural changes at several time scales. Our\nassumption is that it correlates with the degree to which the\nmusic was composed, an indication of complexity.\nWe would like to stress that the structural change feature\nis unrelated to any instantaneous complexity listeners may\nperceive. The timbre of a complete orchestra playing the\nsame note, or the harmony of a rare jazz chord may sound\ncomplex, but our method exclusively aims at discovering the\nquantity of change.\nGiven an arbitrary audio feature (for example chroma),\ncalculated for short frames across a piece of music, our pro-\nposed method calculates a meta-feature at every frame by\n489Poster Session 4\ncomparing statistics of the feature in a window before the\ncurrent frame with statistics of a window after the current\nframe, i.e. a it compares left to right. This method resembles\nFooteâ€™s convolution with a checkerboard kernel [2], which\nis used for structural segmentation. Our approach focuses\non the amount of change itself as a valid property of mu-\nsic. It is more similar in scope to Streichâ€™s tonal complexity\nmeasure [10, Chapter 4], which compares the harmonic con-\ntent in one short-term window to that in a longer window.\nHowever, we are concerned with multiple time scales, and\nin order to capture the structural changes at different time\nscales this calculation is done for several different window\nsizes, resulting in a vector-valued feature.\nThere has been previous research in multi-time-\nscale analysis of audio properties, most prominently the\nkeyscapes proposed by Sapp [9] and extensions thereof [4].\nThese analyses are aimed at providing information about\nwhat classes of harmonies are present in the signal at dif-\nferent time scales. While a visualisation of these classes\nmay reveal changes in the signal, our proposed feature is\nconcerned with the amount of change in any kind of frame-\nwise audio feature. In short, our approach combines Footeâ€™s\nmeasure of change with Sappâ€™s multi-time-scale approach,\nand Streichâ€™s application to musical complexity.\nThe remainder of the paper is structured as follows. Sec-\ntion 2 provides a general formulation of our proposed fea-\nture and outlines an efï¬cient implementation. In Section 3\nwe exemplify the use of the feature with three different ba-\nsis features and propose a visualisation that summarises the\nresulting structural change features for a whole track. In\nSection 5 we provide evidence for the validity of our fea-\nture based on a crowd-sourcing experiment. We discuss our\napproach and future work in Section 6.\n2. STRUCTURAL CHANGE ALGORITHM\nThis section formulates the structural change feature in\nmathematical terms and provides a description of an efï¬-\ncient implementation.\n2.1 Formulation\nThe formulation of the structural change feature is relatively\nstraight-forward. Since it is designed as a meta-feature, we\nassume that the m-dimensional audio feature vector xiâˆˆ\nRm, i= 1, . . . , N has been calculated for all Nframes of a\nmusic track.\nAt frame i, the idea is to compare a summary\ns[iâˆ’k+1:iâˆ’1]âˆˆRmof the features in the kframes to the\nâ€˜leftâ€™ to a summary s[i:i+k]âˆˆRmof the features in the k\nframes to the â€˜rightâ€™.1For example, in our implementation\nbelow the summary is the mean vector.\n1The dimension of the summary does not have to be the same mas that\nof the feature, but we use it here for simplicity.We also assume that we have a non-negative divergence\nfunction d:RmÃ—Rmâ†’R+that assigns a divergence\nto a pair of feature summaries, for example the Euclidean\ndistance or the Jenson-Shannon divergence (as in our im-\nplementation, see Section 3.2). Effectively, dwill compare\nthe windows to the left and right of the ithframe.\nThe characteristic of the structural change feature is that\nit samples the divergence of the left and right windows at\ndifferent window sizes wj, j= 1, . . . , n . The structural\nchange feature at the ithframe is the n-dimensional vector\nvi=/parenleftbig\nv1\ni, . . . , vn\ni/parenrightbig\nof the resulting divergences, where\nvj\ni=ï£±\nï£´ï£²\nï£´ï£³d(s[iâˆ’wj+1:iâˆ’1], s[i:i+wj]),\nifwj< i < Nâˆ’wj+ 1\n0 otherwise.\n(1)\nWhile the window widths are arbitrary, it is convenient to\nthink of them as increasing. For example, one possibility is\nto use window widths increasing by powers of 2:\nwj= 2jâˆ’1. (2)\nUsing several large windows increases the number of com-\nputations, an issue which we address below.\n2.2 An efï¬cient implementation strategy\nCalculation of the structural change is relatively costly be-\ncause 2nsummaries s[.:.]have to be calculated at every\nframe, two for every window width. Even in the case\nwhere the summary is simply the mean of the feature vec-\ntorsâ€™ elements over time computations can become expen-\nsive: calculating the sums (required for the means) leads to\n2mN/summationtextn\nj=1(wjâˆ’1) = 2 mnN (Wâˆ’1)additions for the\nwhole track, where Wis the average window width. For a\nfeature with m= 12 dimensions, a track with N= 2500\nframes, n= 8different window widths and an average win-\ndow size of W= 100 these are nearly 48 million additions.\nHowever, when the summary function is indeed the mean,\nthen we can calculate every single summary as just one vec-\ntor difference ( mdifferences)\ns[i1:i2] =ci2âˆ’ci1 (3)\nof two vectors from the cumulative feature matrix C=\n(c0, . . . ,cN). The matrix Ccan be easily pre-calculated\nas\nci=i/summationdisplay\ni/prime=0xi/prime, (4)\nwhere we set x0=0. Pre-calculating Cis cheap, it costs\nnNadditions, and the additions performed during the struc-\ntural change calculations are reduced to 2mnN , i.e. by a\nfactor of W. We have implemented the algorithm in C++\nas a library that can be directly included into Vamp feature\n49012th International Society for Music Information Retrieval Conference (ISMIR 2011)\nplugins2. The source code for this library can be obtained\nfrom http://github.com/lastfm/ .\nThe window sizes from Equation (2), the mean summary\nfunction and the Jenson-Shannon divergence are used in our\nexample implementation below, which represents one par-\nticular possibility of conï¬guring the algorithm.\n3. IMPLEMENTATION WITH\nTHREE BASIS FEATURES\nWe apply the structural change algorithm to three differ-\nent features chosen to represent three qualities of music:\nchroma (harmony), rhythm and timbre. This section de-\nscribes the design choices we have made to achieve this.\n3.1 The Basis Features\nFor each of the qualities described by the basis featuresâ€”\nchroma, rhythm and timbreâ€”we separately extract the\nstructural change features (SC) as described in Section 2:\nchroma SC, rhythm SC and timbre SC. All features are ex-\ntracted from mp3 ï¬les sampled at 44100 kHz.\nChroma. Chroma [3] is a 12-dimensional feature of\nactivity values pertaining to the twelve pitch classes (C,\nC/sharp, . . . , B), a representation of the instantaneous harmony.\nWe use an existing Vamp plugin implementation3. The\nmethod [6] makes use of the discrete Fourier transform to\nobtain a spectrogram, maps every spectral frame to the log-\nfrequency space (pitch space) via a linear transform and up-\ndates the values to adjust for tuning differences; the chroma\nvectors are weighted sums of the adjusted pitch space spec-\ntral bins. We do not use the approximate transcription\n(NNLS) step but otherwise use the default parameters with\na step size of 11025 samples (250 ms).\nRhythm. The ï¬‚uctuation patterns (FP) feature [7] was\ndesigned to describe the rhythmic signature of musical au-\ndio. The FPs are calculated on Hamming-windowed seg-\nments of approximately 3 seconds length, with a step size of\none second (44100 samples), which are further sub-divided\ninto 256 frames with a length of 512 samples. The main\nidea is to use the dB amplitude of these 256 frames at dif-\nferent frequency bands as a time series: the spectrum of this\ntime series at a particular frequency band is the FP of that\nfrequency band. We sum the FPs of all frequency bands into\none band in order to eliminate timbre inï¬‚uence.\nTimbre. The Mel-spectrum is a warped frequency spec-\ntrum obtained by taking the discrete Fourier transform of an\naudio signal, taking the logarithm of the spectral energies\nto obtain dB values, and mapping the spectrum onto Mel-\nfrequency spaced bins that are linear with respect to human\npitch perception. We use 36 Mel-frequency bins. Since the\nfeature is extracted together with the FP, the hop size is one\n2http://www.vamp-plugins.org/\n3http://isophonics.net/nnls-chromasecond and the spectral bins are means taken over 256 small\nframes (512 samples) across a 3 second window.\n3.2 Window, Summary and Divergence Functions\nWe choose power-of-two window widths (Equation 2). In\norder to align time-scales we set j= 1, . . . , 6for both\nrhythm and timbre features, and j= 3, . . . , 8for the chroma\nfeature. This means that the structural change feature is 6-\ndimensional with window widths (i.e. those of the left or\nright windows) are 1, 2, 4, . . ., 32 seconds.\nWe use the mean summary function s, which is imple-\nmented as described in Section 2.2. Since all basis fea-\ntures can be interpreted as distributions in their respective\ndomains, we normalise each summary vector, and use the\nJenson-Shannon divergence as our divergence measure d,\ni.e. for two normalised summary vectors s1ands2\nd(s1, s2) =KL(s1||M) +KL(s2||M)\n2(5)\nwhere M=s1+s2\n2and KL is the Kullback-Leibler diver-\ngence given by\nKL(x||y) =n/summationdisplay\ni=1xilog(xi/yi). (6)\n3.3 An Example\nWe have marked a few interesting aspects of the structural\nchange features for the song â€˜Luckyâ€™ in Figure 1 (light\ncolours mean high values). The labels amark two drum\nstops, before the ï¬rst chorus and the ï¬rst bridge, respec-\ntively. Timbre and rhythm SC both show a double bulge,\nespecially in the three bins of short time scales, one at the\nbeginning and one at the end of each drum stop. At bonly\nthe timbre SC shows a high value, indicating the beginning\nof the second chorus (without a clear rhythm change). La-\nbelcmarks a part with little musical movement: no actual\nchord changes, but lots of sound variation, including spo-\nken voice excerpts: this is reï¬‚ected in relatively low chroma\nSC activity, but relatively high timbre SC activity. Label d\nmarks a calm bridge section (no drums), followed by the\nkey change that leads into the next chorus. Two clear timbre\nSC peaks show the boundaries of the bridge, and the high\nchroma long-scale SC values reï¬‚ect the key change.\n4. TRACK-LEVEL SUMMARISATION AND\nVISUALISATION\nIn some contexts it is useful to be able to summarise the\nstructural change of a piece of music, for example, sum-\nmarising the feature for further processing by machine\nlearning algorithms. Summarisation is also necessary to\ngenerate track-level visualisations, such as the Audio Flow-\ners, which we present below.\n491Poster Session 4\naabcdtime scaletime scaletime scale\n     timechroma SCtimbre SCrhythm SCFigure 1 : Structural change in the three basis features for the song â€˜Luckyâ€™ as performed by Britney Spears. See Section 3.3.\n4.1 Statistics\nThe most straight-forward way of summarising the SC\nframes is to take the mean average over all structural change\nfeature frames of the whole piece, resulting in one mean fea-\nture vector. In cases where structural change is concentrated\nin a small part of the piece of music, however, the mean can\nbe misleading because it suggests that the rate of change in\nthe whole piece is relatively high. The median is a more ro-\nbust average statistic, since it discards such outliers. We use\nboth because mean, median and their difference are interest-\ning properties of a piece of music.\nWe extracted the structural change features for our three\nbasis features from mp3 ï¬les of 17,116 pieces of popular\nfrom the British singles charts between 1951 and 2011, then\naveraged them in two ways by taking the mean and median\nover time. Since we have six window widths, three basis\nfeatures and two averages for each of the combinations, each\nof the tracks has 6Ã—3Ã—2 = 36 values. For each of the\n36 dimensions we apply quantile normalisation (normalised\nranking) to spread values within the interval [0,1]with re-\nspect to the whole collection of songs.\n4.2 Audio Flowers\nIn order to turn the 36 values for each track into an intuitive\nvisual representation (examples in Figure 3), we treat each\nmusical quality separately to create a ï¬‚ower â€˜petalâ€™: red for\nrhythm, green for harmony, and blue for timbre. In any of\nthe three petals, the central, opaque part visualises the nor-\nmalised median values, the translucent part corresponds to\nthe normalised mean. The values closest to the centre of the\nAudio Flower represent short time scales, the values near\nthe tips of the petals represent the longest time scale. The\nplot is realised by calculating a 100-point smoothed inter-polation of the six values. We chose the median to be used\nfor the opaque part because it is a robust average of a trackâ€™s\nstructural change and is likely to be the most reliable mea-\nsure. The translucent part is only visible where the mean ex-\nceeds the median value. This happens in cases when strong\nstructural changes happen, but on a relatively short section\nof a track, as we will illustrate below.\nFigure 2 shows the results for a few artiï¬cially con-\nstructed pieces of audio. Figure 2a illustrates 300 seconds of\npink noise, Figure 2b 150 seconds of pink noise followed by\nanother 150 of white noise. The white noise Audio Flower\nshows virtually no sign of structural change, while the Au-\ndio Flower of the mixed pink and white noise ï¬le has a slight\nbulge indicating a rare long-term change in timbre (the cor-\nresponding rhythm value is slightly raised, too). This in-\ndication of â€˜composednessâ€™, or complexity, is exactly what\nwe would expect in that situation ( cf.Section 1). The other\ntwo Audio Flowers are closer to real music: Figure 2c rep-\nresents a single chord, played on a piano but with two differ-\nent rhythms alternating at a relativley long time scale of (24\nseconds). As we could expect, here too, harmonic change is\nvirtually absent, and the high values towards the tip of the\nred rhythm petal reï¬‚ects the long-term rhythm changes. The\nchange in timbre that comes with the rhythm change can be\nobserved, too. Figure 2d was produced from a piece of mu-\nsic with the same rhythm structure, but instead of a single\nchord we used a cadence, i.e. a more complex chord pattern.\nThe Audio Flower represents this added complexity as high\nvalues towards the origin of the green harmony petal, while\nthe rest of the ï¬‚ower remains virtually unchanged.\nFigure 3a shows the Audio Flower of the song â€˜Luckyâ€™,\nwhich we have already treated in Figure 1. The key change\nhappens only once during the piece, indicated through the\nhigh levels of chroma SC at din Figure 1. Due to this â€˜out-\n49212th International Society for Music Information Retrieval Conference (ISMIR 2011)\n(a) pink noise\n (b) pink noise, then white noise\n(c) chord, changing rhythm\n (d) cadence, changing rhythm\nFigure 2 : Artiï¬cial examples: (a) pink noise, (b) pink noise\nfollowed by white noise, (c) single major piano chord with\ndifferent rhythmic sections, (d) repeated major cadences\nwith different rhythmic sections.\nlierâ€™ the normalised median is smaller than the normalised\nmean at long time scalesâ€”the translucent part of the Audio\nFlower becomes visible.\nFigure 3b depicts the Audio Flower of the song â€˜Smells\nLike Teen Spiritâ€™ as recorded by the band Nirvana. The most\nstriking aspect of this song is the mushroom-shaped timbre\npetal (blue). This is common in songs that are organised\nalternating soft and loud sections.\nIn comparison, the timbre petal of the Audio Flowers in\nFigures 3c and 3d is decidedly thicker, especially at shorter\ntimescales (towards the origin). In fact, the shape of tim-\nbre and chroma petals is very similar between these two\nAudio Flowers. This is not surprising because they are in-\ndeed two renditions of the same song â€˜Time After Timeâ€™,\none by Cyndi Lauper, one by Ronan Keating. The shape of\nthe rhythm petal is, however, quite dissimilar, which sug-\ngests their approaches to rhythm are different. A gallery\nof further examples can be found at http://last.fm/\nplayground/demo/complexity .\n5. INTERNET-BASED EXPERIMENT\nFinding evidence to support our hypothesis that our features\ncorrespond with human perception of structural change is\nhard because unless the listeners are musicians we cannot\n(a) Lucky\n (b) Smells Like Teen Spirit\n(c) Time After Time (Lauper)\n (d) Time After Time (Keating)\nFigure 3 : Audio Flowers for the songs (a) â€˜Luckyâ€™ (as per-\nformed by Britney Spears), (b) â€˜Smells Like Teen Spiritâ€™,\nand two renditions of â€˜Time After Timeâ€™, (c) by Cyndi Lau-\nper, (d) by Ronan Keating.\nassume that they even think in terms of harmony, rhythm\nor timbre. In order to test whether any correlation can be\nobserved we set up an informal experiment on an Internet\npage. A participant would randomly be given two 30 second\nsound excerpts from our collection of chart singles and was\nthen asked to decide which changed more in terms of one\nof our three basis features. The tracks were chosen to dif-\nfer in their amount of structural change: the average of the\nnormalised median structural change values4for one track\nwas high ( >0.7) and that of the other one was low ( <0.3).\nThe web page clearly states that we look for change and di-\nversity. Upon casting their rating the listener is shown the\nAudio Flowers of the two songs in question as a reward and\nis told which of the two our analysis deemed more change-\nable. The rating was realised as a set of three radio-buttons\n(ï¬rst track, second track and a third one labelled â€˜not sureâ€™).\nWe had no control over whether the participants listened to\nthe tracks before voting.\nAt the time of writing we have collected 1428 votes from\n401 raters with an mean number of 3.9 ratings (median: 2).\nWe analysed the 1165 ratings of the 197 participants who\nvoted at least three times. There is moderate agreement be-\ntween user ratings and our high and low classes: in 61.4 %\n4Taking into account the short duration of the excerpts, only the ï¬rst\nfour dimensions of the features were used in the structural change value.\n493Poster Session 4\nof all cases users agreed with the automatic analysis. Test-\ning against the null hypothesis of users randomly choosing\nan answer, we obtain a very low pvalue of p < 10âˆ’14,\ni.e. we are very conï¬dent that the participantsâ€™ choice is not\nrandom. This also applies to the three qualities separately:\nusers agree with rhythm SC (60.0%, p <10âˆ’3), chroma SC\n(63.3%. p <10âˆ’6) and timbre SC (60.8%, p <10âˆ’4).\nIn all cases the agreement is not very high, but at this\nstage we can only speculate about the causes: our feature\nmight express something different from what we intended\nor what participants understood; the un-controlled nature\nof the experiment may have led participants to randomly\nchoose their rating; the participants may not have had the\nnecessary musical experience to provide meaningful ratings.\nHowever, the fact that we found signiï¬cant agreement for all\nthree features separately suggests that the structural change\nfeature capture musical qualities listeners can relate to.\n6. DISCUSSION AND FUTURE WORK\nOur implementation presented in Section 3 is only one way\nof using the structural change feature, and many can be\nadded by using alternatives for the window width func-\ntion, left/right summary function and divergence function\npresented here. We are particularly interested in exploring\ndifferent divergence functions, such as inverse correlation\nand Euclidean distance (see also [10, Chapter 4]). Using\na different divergence function will allow us to use features\nthat are not necessarily non-negative, such as mel-frequency\ncepstral coefï¬cients (MFCCs) or other chroma mappings.\nThe proposed feature will allow classic Music Informa-\ntion Retrieval tasks (such as cover song retrieval and genre\nclassiï¬cation) to access a semantic dimension that is not\ncovered by existing audio features, and hence may lead to\nimprovements in these areas.\nFinally, we hope that future studies will reveal how the\nstructural change feature is related to musical complexity as\nperceived by humans.\n7. CONCLUSIONS\nWe have proposed the novel audio feature structural change\nfor the analysis of audio recordings of music. The fea-\nture can be regarded as a meta-feature, since it measures\nthe change of an underlying basis feature at different time\nscales. As part of our proposal we have presented the gen-\neral algorithm and an efï¬cient implementation strategy of a\nspecial case. We have implemented the feature with three\ndifferent basis features representing chroma, rhythm and\ntimbre. Analysing more than 17,000 tracks of popular mu-\nsic allowed us to ï¬nd a meaningful normalisation to the fea-\nture values. Based on this normalisation we have introduced\na track-level visualisation of structural change in chroma,\nrhythm and timbre. Several of these visualisations, AudioFlowers, have been presented to illustrate the featuresâ€™ char-\nacteristics and show that interpreting the amount of struc-\ntural change as musical complexity is possible. We con-\nducted a informal web-based experiment whose results sug-\ngest that our proposed feature correlates with the human per-\nception of change in music.\n8. REFERENCES\n[1] R. Dawkins. The blind watchmaker: why the evidence\nof evolution reveals a universe without design . Norton,\n1996.\n[2] J. Foote. Automatic audio segmentation using a mea-\nsure of audio novelty. In Multimedia and Expo, 2000.\nICME 2000. 2000 IEEE International Conference on ,\nvolume 1, pages 452â€“455. IEEE, 2000.\n[3] T. Fujishima. Real time chord recognition of musical\nsound: a system using Common Lisp Music. In Pro-\nceedings of the International Computer Music Confer-\nence (ICMC 1999) , pages 464â€“467, 1999.\n[4] E. G Â´omez and J. Bonada. Tonality visualization of poly-\nphonic audio. In Proceedings of the International Com-\nputer Music Conference (ICMC 2005) , 2005.\n[5] F. Heylighen. The growth of structural and functional\ncomplexity during evolution. In F. Heylighen, J. Bollen,\nand A. Riegler, editors, The evolution of complexity ,\npages 17â€“44. Kluwer Academic, Dordrecht, 1999.\n[6] M. Mauch and S. Dixon. Approximate note transcrip-\ntion for the improved identiï¬cation of difï¬cult chords. In\nProceedings of the 11th International Society for Music\nInformation Retrieval Conference (ISMIR 2010) , pages\n135â€“140, 2010.\n[7] E. Pampalk, S. Dixon, and G. Widmer. On the evaluation\nof perceptual similarity measures for music. In Proceed-\nings of the Sixth International Conference on Digital Au-\ndio Effects (DAFx-03) , pages 7â€“12, 2003.\n[8] R. M. Parry. Musical complexity and top 40 chart per-\nformance. Technical report, Georgia Institute of Tech-\nnology, 2004.\n[9] C. Sapp. Harmonic visualizations of tonal music. In Pro-\nceedings of the International Computer Music Confer-\nence (ICMC 2001) , 2001.\n[10] S. Streich. Music Complexity: A Multi-Faceted Descrip-\ntion of Audio Content . PhD thesis, Universitat Pompeu\nFabra, Barcelona, Spain., 2006.\n494"
    },
    {
        "title": "Music Genre Classification by Ensembles of Audio and Lyrics Features.",
        "author": [
            "Rudolf Mayer",
            "Andreas Rauber"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1416844",
        "url": "https://doi.org/10.5281/zenodo.1416844",
        "ee": "https://zenodo.org/records/1416844/files/MayerR11.pdf",
        "abstract": "Algorithms that can understand and interpret characteristics of music, and organise them for and recommend them to their users can be of great assistance in handling the ever growing size of both private and commercial collections. Music is an inherently multi-modal type of data, and the lyrics associated with the music are as essential to the reception and the message of a song as is the audio. In this paper, we present advanced methods on how the lyrics domain of music can be combined with the acoustic domain. We evaluate our approach by means of a common task in music information retrieval, musical genre classification. Advancing over previous work that showed improvements with simple feature fusion, we apply the more sophisticated approach of result (or late) fusion. We achieve results superior to the best choice of a single algorithm on a single feature set.",
        "zenodo_id": 1416844,
        "dblp_key": "conf/ismir/MayerR11",
        "keywords": [
            "algorithms",
            "music",
            "characteristics",
            "organise",
            "recommend",
            "users",
            "size",
            "private",
            "commercial",
            "collections"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nMUSICAL GENRE CLASSIFICATION BY\nENSEMBLES OF AUDIO AND LYRICS FEATURES\nRudolf Mayer and Andreas Rauber\nInstitute of Software Technology and Interactive Systems\nVienna University of Technology, Austria\nABSTRACT\nAlgorithms that can understand and interpret characteristics\nof music, and organise them for and recommend them to\ntheir users can be of great assistance in handling the ever\ngrowing size of both private and commercial collections.\nMusic is an inherently multi-modal type of data, and the\nlyrics associated with the music are as essential to the recep-\ntion and the message of a song as is the audio. In this paper,\nwe present advanced methods on how the lyrics domain of\nmusic can be combined with the acoustic domain. We eval-\nuate our approach by means of a common task in music in-\nformation retrieval, musical genre classiï¬cation. Advancing\nover previous work that showed improvements with simple\nfeature fusion, we apply the more sophisticated approach of\nresult (or late) fusion. We achieve results superior to the best\nchoice of a single algorithm on a single feature set.\n1. INTRODUCTION AND RELATED WORK\nMusic incorporates multiple types of content: the audio it-\nself, song lyrics, album covers, social and cultural data, and\nmusic videos. All those modalities contribute to the percep-\ntion of a song, and an artist in general. However, often a\nstrong focus is put on the audio content only, disregarding\nmany other opportunities and exploitable modalities. Even\nthough music perception itself is based on sonic characteris-\ntics to a large extent, and acoustic content makes it possible\nto differentiate between acoustic styles, a great share of the\noverall perception of a song can be only explained when\nconsidering other modalities. Often, consumers relate to a\nsong for the topic of its lyrics. Some categories of songs,\nsuch as â€˜love songsâ€™ or â€˜Christmasâ€™ songs, are almost ex-\nclusively deï¬ned by their textual domain; many traditional\nâ€˜Christmasâ€™ songs were interpreted by modern artists and\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proï¬t or commercial advantage and that copies\nbear this notice and the full citation on the ï¬rst page.\nc/circlecopyrt2011 International Society for Music Information Retrieval.heavily inï¬‚uenced by their style: â€˜Punk Rockâ€™ variations are\nrecorded as well as â€˜Hip-Hopâ€™ or â€˜Rapâ€™ versions.\nThese examples show that there is a whole level of se-\nmantics inherent in song lyrics that can not be detected solely\nby audio based techniques. We thus assume that a songâ€™s\ntext content can help in better understanding its perception,\nand evaluate a new approach for combining descriptors ex-\ntracted from the audio domain of music with descriptors de-\nrived from the textual content of lyrics. Our approach is\nbased on the assumption that a diversity of music descrip-\ntors and a diversity of machine learning algorithms are able\nto make further improvements.\nMusic information retrieval (MIR) is concerned with ad-\nequately accessing (digital) audio. Important research di-\nrections include similarity retrieval, musical genre classi-\nï¬cation, or music analysis and knowledge representation.\nA comprehensive overviews of the research ï¬eld is given\nin [11]. The prevalent technique of music for MIR purposes\nis to analyse the audio signal. Popular feature sets include\nMFCCs, Chroma, or the MPEG-7 audio descriptors.\nPrevious studies reported about a glass ceiling being reached\nusing timbral audio features for music classiï¬cation [1]. Wev-\neral research teams have been working on analysing textual\ninformation, predominantly in the form of song lyrics and an\nabstract vector representation of the term information con-\ntained in other text documents. A semantic and structural\nanalysis of song lyrics is conducted in [8]. An evaluation of\nartist similarity via song lyrics is given in [7], suggesting a\ncombination of approaches might lead to better results.\nIn this paper, we employ feature sets derived from the\nlyrics content, capturing rhyme structures, part-of-speech of\nthe employed words, and style, such as diversiï¬cation of the\nwords used, sentence complexity, and punctuation. These\nfeature sets were introduced in [10], and applied to genre\nclassiï¬cation. This approach has further been extended to a\nbigger test collection and a combination of lyrics and audio\nfeatures in [9], reporting results superior to single feature\nsets. The combination based on simple feature fusion (early\nfusion), i.e. concatenating all feature subspaces is however\nsimplistic. Here, we rather apply late fusion , combining\nclassiï¬er outcomes rather than features. We create a two-\n675Poster Session 6\nFigure 1 . Overview of the Cartesian Ensemble System,\ncombining feature sets with a set of classiï¬cation schemes\ndimensional ensemble system, a Cartesian classiï¬er , com-\nbining different feature subspaces from different domains,\nand different classiï¬cation algorithms.\nThis paper is structured as follows. We describe the en-\nsemble approach in Section 2. We then evaluate and analyse\nits results on two corpora in Section 3. Finally, we conclude,\nand give a short outlook on future research in Section 4.\n2. CARTESIAN ENSEMBLE\nA schematic overview of the ensemble system, building on\na system introduced in [5], is given in Figure 1. The system\nis called Cartesian ensemble , as the set of models it uses\nas base classiï¬ers is composed as the Cartesian product of\nDfeature subspaces/sets by Cclassiï¬cation schemes. A\nmodel is built for each combination of a training classiï¬ca-\ntion scheme cion a feature subspace dj, yielding a total of\nDÃ—Cbase models as the ensemble. A classiï¬cation scheme\nis a speciï¬c classiï¬cation algorithm and parameters used.\nThe goal of the ensemble approach is two-fold. First,\nit is aimed at obtaining a sufï¬ciently diverse ensemble of\nmodels, which will guarantee, up to a certain degree, an\nimprovement of the ensemble accuracy over the best single\nmodel trained. Choosing this best single model a priori is a\ndifï¬cult task, and previous results have shown that there is\nno combination of algorithm (and parameters) and features\nwhich would yield the best result for each dataset and task.\nThus, the second goal of the approach is to abstract from the\nselection of a such a particular classiï¬er and feature set to\nuse for a particular problem. When a previously unknown\npiece of music is presented to the ensemble system, the se-\nlected models each produce a prediction for a speciï¬c cat-\negory. To obtain a ï¬nal result, these individual predictions\nare then combined to produce a single category prediction\noutcome. For this step, a number of different decision com-\nbination (or label fusion) rules , can be used. The Cartesian\nensemble system is built on the open-source WEKA toolkit,\nand uses classiï¬cation algorithms available therein.\nPareto-optimal Classiï¬er Selection: Model diversity is\na key design factor for building effective classiï¬er ensem-bles [4]. The system employs a strategy for selecting the\nbest set of models , based on ï¬nding the Pareto-optimal set of\nmodels by rating them in pairs, according to two measures.\nThe ï¬rst one is the inter-rater agreement diversity measure\nÎº, deï¬ned on the coincidence matrix Mof the two models.\nThe entrymr,sis the proportion of the dataset that model hi\nlabels asLrand modelhjlabels asLs. The second measure\nis the pair average error, computed by\neij= 1âˆ’Î±i+Î±j\n2(1)\nwhereÎ±iandÎ±jare the estimated accuracy of the two\nmodels. The Pareto-optimal set contains all non-dominated\npairs, i.e. pairs for which there is no other pair that is better\nthan on both criteria. For more details, pleas see [4].\nVote Combination Rules : The system provides weighted\nand unweighted vote combination rules. The unweighted\nrules employed are described e.g. in [2]. They comprise\nsimple majority voting (MAJ), which favours the class pre-\ndicted by most votes, and rules that combine the individual\nresults by the average (A VG), median (MED) or maximum\n(MAX) of the posterior probability P(Lk|xi)of instancex\nto belong to category Lk, as provided by model hi.\nTheweighted rules multiply model decisions by weights\nand select the label Lkthat gets the maximum score. Model\nweights are based on the estimated accuracy Î±iof the trained\nmodels. The authorityaiof each model hiis established\nas a function of Î±i, normalized, and used as its weight Ï‰i.\nThe Simple Weighted V ote (SWV) computes weights as a\nsimple weighted vote. The more complicated weight func-\ntions for the Rescaled Simple Weighted V ote (RSWV), Best-\nWorst Weighted V ote (BWWV) and Quadratic Best-Worst\nWeighted V ote (QBWWV) are depicted in Figure 2. There,\neBis the lowest estimated number of errors made by any\nmodel in the ensemble on a given validation dataset, and\neWis the highest estimated number of errors made by any of\nthose classiï¬ers. Weighted Majority V ote (WMV) is a theo-\nretically optimal weighted vote rule described in [4], where\nmodel weights are set proportionally to log(Î±i/(1âˆ’Î±i)).\nInner/Outer Cross Validation: To estimate how the re-\nsults from a classiï¬er will generalize on independent data,\nthe classiï¬cation model is tested on labelled data which was\nnot used for training the model, and measures such as ac-\ncuracy are recorded. To reduce the variability, often a tech-\nnique called cross-validation is employed: nmultiple rounds\nof partitioning the data in a training and test set are per-\nformed, and the recorded measures are averaged over all\nthe rounds. For weighted combination rules, we need to\nestimate the accuracy of individual ensemble models ( Î±i)\nto obtain their authorities ( ai). To avoid using test data of\nthe ensemble for single model accuracy estimation, an in-\n67612th International Society for Music Information Retrieval Conference (ISMIR 2011)\nekekekakakak\n0 N(1âˆ’1/M)1 1 1\neWeBeWeB0Figure 2 . Model weight computation\nner cross-validation relying on ensemble training data only\nis performed. The predicted accuracy of this inner cross-\nvalidation is then taken as the authority of the model.\n3. EV ALUATION\nIn this section, we ï¬rst present the feature subspaces and\ndatasets employed in our evaluation, followed by a detailed\nanalysis of the classiï¬cation results.\n3.1 Audio Feature Subspaces\nThe audio descriptors are extracted from a spectral repre-\nsentation of an audio signal, partitioned into segments of 6\nsec. Features are extracted segment-wise, and then aggre-\ngated for a piece of music computing the median (RP, RH)\nor mean (SSD) from features of multiple segments. For de-\ntails on the computation, please refer to the literature for\ndetails [6]. The feature extraction for a Rhythm Pattern\nis composed of two stages. First, the speciï¬c loudness sen-\nsation on 24 critical frequency bands is computed through\na Short Time FFT, grouping the resulting frequency bands\nto the Bark scale, and successive transformation into the\nDecibel, Phon and Sone scales. This results in a psycho-\nacoustically modiï¬ed Sonogram representation that reï¬‚ects\nhuman loudness sensation. Then, a discrete Fourier trans-\nform is applied, resulting in a spectrum of loudness ampli-\ntude modulation per modulation frequency for each critical\nband. A Rhythm Histogram (RH) aggregates the modu-\nlation amplitude values of the critical bands computed in\na Rhythm Pattern and is a descriptor for general rhythmic\ncharacteristics in a piece of audio [6]. The ï¬rst part of the\nalgorithm for computation of a Statistical Spectrum De-\nscriptor (SSD), the computation of speciï¬c loudness sen-\nsation, is equal to the Rhythm Pattern algorithm. Subse-\nquently at set of statistical valuesare calculated for each indi-\nvidual critical band. SSDs describe ï¬‚uctuations on the crit-\nical bands and capture additional timbral information very\nwell [6].\n3.2 Lyrics Feature Subspace\nThe following feature subspaces are all based on song lyrics,\nand analyse the content, and rhyme and style of them. For\nmore details on features please refer to [10] [9]. To account\nfor different document lengths, where applicable, values arenormalised by the number of words or lines of the lyrics\ndocument.\n3.2.1 Topic Features\nFor analysing the topical content of the lyrics, we rely on\nclassical bag-of-words indexing, which uses a set of words\nto represent each document. Let the number of documents\nin a collection be denoted by N, each single document by d,\nand a term or token by t. Accordingly, the term frequency\ntf(t,d)is the number of occurrences of term tin document\ndand the document frequency df(t)the number of docu-\nments termtappears in. We then apply weights to the terms,\naccording to their importance or signiï¬cance for the docu-\nment, using the popular model of term frequency times in-\nverse document frequency .This results in vectors of weight\nvalues for each document din the collection, i.e. each lyrics\ndocument. We do not perform stemming in this setup, ear-\nlier experiments showed only negligible differences for stemmed\nand non-stemmed features (the rationale behind using non-\nstemmed terms is the occurrence of slang language in some\ngenres).\n3.2.2 Rhyme and Style Features\nRhyme denotes the consonance or similar sound of two or\nmore syllables or whole words. The motivation for this\nset of features was that different genres of music should\nexhibit different styles of lyrics and rhymes. â€˜Hip-Hopâ€™\nor â€˜Rapâ€™ music, for instance, makes heavy use of rhymes,\nwhich (along with a dominant bass) leads to their character-\nistic sound. To identify such patterns we extract several de-\nscriptors from the phoneme transcription of the song lyrics.\nWe then distinguish two elements of subsequent lines in a\nsong text: AAandAB. The former represents two rhyming\nlines, while the latter denotes non-rhyming. Based on these,\nwe extract a set of rhyme patterns, such as a sequence of two\n(or more) rhyming lines (â€˜Coupletâ€™), alternating rhymes, or\nsequences of rhymes with a nested sequence (â€˜Enclosing\nrhymeâ€™), and count their frequency. Subsequently, we com-\npute the percentage of rhyming blocks, and deï¬ne the unique\nrhyme words as the fraction of unique terms used to build\nrhymes, describing whether rhymes are frequently formed\nusing the same word pairs.\nPart-of-speech (POS) tagging is a lexical categorisation\nor grammatical tagging of words. Different POS categories\nare e.g. nouns, verbs, articles or adjectives. We presume that\ndifferent genres will differ also in the category of words they\nare using; thus, we extract several POS descriptors from the\nlyrics. We count the numbers of: nouns ,verbs ,pronouns ,\nrelational pronouns (such as â€˜thatâ€™ or â€˜whichâ€™), prepositions ,\nadverbs ,articles ,modals , and adjectives .\nText documents can also be described by simple statisti-\ncal style measures based on word or character frequencies.\nMeasures such as the average length of words or the ratio\n677Poster Session 6\nof unique words in the vocabulary might give an indication\nof the complexity of the texts, and are expected to vary over\ndifferent genres. Further, the usage of punctuation marks\nsuch as exclamation or question marks may be speciï¬c for\nsome genres, and some genres might make increased use\nof apostrophes when omitting the correct spelling of word\nendings. Other features describe the words per line and the\nunique number of words per line, the ratio of the number\nof unique words and the total number of words, and the av-\nerage number of characters per word. A particular feature\nis words-per-minute, which is computed analogously to the\nwell-known beats-per-minute (BPM) value.\n3.3 Datasets\nMusic information retrieval research in general suffers from\na lack of standardised benchmark collections â€“ being mainly\nattributable to copyright issues. Nonetheless, some collec-\ntions have been used frequently in the literature, such as the\ntwo collections provided for the â€˜rhythmâ€™ and â€˜genreâ€™ re-\ntrieval tasks held in conjunction with the ISMIR conference\n2004, or the collection presented in [12].\nHowever, for the ï¬rst two collections, hardly any lyrics\nare available as they are either instrumental songs or free\nmusic for which lyrics were not published. For the latter, no\nmeta-data such as song titles is available, making automatic\nfetching of lyrics impossible. The collection used in [3] con-\nsists of only 260 pieces and was not initially used for genre\nclassiï¬cation. Further, it was compiled from only about 20\ndifferent artists â€“ we speciï¬cally wanted to avoid uninten-\ntionally classifying artists rather than genres.\nTherefore, we constructed two different test collections\nof differing size as a random sample from a private collec-\ntion [9]. The ï¬rst database consists of 600 songs, aimed\nat having a high number of different artists, with songs from\ndifferent albums to prevent biased results by too many songs\nfrom the same artist/album. It thus comprises songs from\n159 different artists and 241 different albums. They are or-\nganised in ten genres of 60 songs each (cf. left part of Ta-\nble 1). To conï¬rm the ï¬ndings from the smaller test col-\nlection, we created a larger, more diversiï¬ed database of\nmedium- to large-scale, consisting of 3,010 songs.The num-\nbers of songs per genre range from 179 in â€˜Folkâ€™ to 381\nin â€˜Hip-Hopâ€™. Detailed ï¬gures about this collection can be\ntaken from the right part of Table 1. To be able to better re-\nlate and match the results obtained for the smaller collection,\nwe only selected songs belonging to the same ten genres.\nWe then automatically fetched lyrics from popular lyrics\nportals on the Internet. In case the primary portal didnâ€™t pro-\nvide any lyrics, the other portals were used until all lyrics\nwere available. No checking of the quality of the texts with\nrespect to content or structure was performed; thus, the lyrics\ncan be considered a representative data source a simple au-\ntomated system could retrieve.Table 1 . Composition of the test collections; the left and\nright columns show the number of artists, albums and songs\nfor the small and large collection, respectively\nGenre Artists Albums Songs\nCountry 6 9 13 23 60 227\nFolk 5 11 7 16 60 179\nGrunge 8 9 14 17 60 181\nHip-Hop 15 21 18 34 60 381\nMetal 22 25 37 46 60 371\nPop 24 26 37 53 60 371\nPunk Rock 32 30 38 68 60 374\nR&B 14 18 19 31 60 373\nReggae 12 16 24 36 60 181\nSlow Rock 21 23 35 47 60 372\nTotal 159 188 241 370 600 3010\n3.4 Genre Classiï¬cation Results\nThe following tables give the classiï¬cation accuracies in per\ncent. For statistical signiï¬cant testing, we used a paired t-\ntest (Î±=0.05, micro-averaged accuracy); in the tables, im-\nprovement or degradation over datasets (column-wise) is in-\ndicated by ( +) or (âˆ’), respectively.\nTable 2 shows the classiï¬cation results of the single clas-\nsiï¬ers on single feature sets on the small dataset . It can\nbe noted that the SSD features are the best performing sin-\ngle feature set, and the SVM the best classiï¬er; here, the\nlinear kernel performed better than the quadratic. This com-\nbination of feature set and classiï¬cation scheme thus serves\nas the primary base-line to compare the Cartesian ensem-\nble results to. The results of the SSD features clearly out-\nperform the other audio feature sets (RH omitted, cf. [9]),\nby 10% points and more. k-NN is the second-best clas-\nsiï¬cation algorithm, achieving 52.17% accuracy with a k\nof 1 on SSD features, outperforming both Random Forests\nand Na Â¨Ä±ve Bayes. Regarding the lyrics features, the text\nstatistics features perform best from the rhyme and style\nfeatures, achieving 30% accuracy. The text statistics fea-\ntures are slightly outperformed by the bag-of-words features\nwhen using the linear SVM, and signiï¬cantly on Na Â¨Ä±ve Bayes,\nwhile they perform signiï¬cantly worse on k-NN, Random\nForests and the quadratic SVM.\nFurther, Table 2 also gives the set of best-performing\ncombinations of concatenating the single feature sets (early\nfusion). They are assumed as a secondary baseline for the\nensemble. Compared to the single feature sets, when com-\nbining SSD and lyrics style statistics features, we could sig-\nniï¬cantly improve the result, by almost 7% points. We can\nalso observe that the improvement is not of statistical signif-\nicance for the other classiï¬cation schemes. It is also inter-\nesting to note that combining with the bag-of-words features\ndoes improve the results over the SSD baseline when using\nthe SVM with the linear kernel, but not to the extent as when\ncombining with the rhyme and style features, even though\n67812th International Society for Music Information Retrieval Conference (ISMIR 2011)\nTable 2 . Results of the single classiï¬cation on the small datasets\nFeature set NB 1-NN 5-NN 10-NN SVMLin SVMPol RF\nRhyme 15.67 12.83 13.33 14.17 13.17 11.17 15.67\nPOS 19.67 14.50 18.00 18.50 20.33 20.17 17.83\nTextStat 21.50 20.50 22.00 24.33 30.00 28.17 25.50\nBOW 243 23.67 17.67 21.33 19.83 28.33 27.33 21.67\nBOW 725 27.67 12.67 14.67 12.17 31.00 26.33 22.67\nBOW 1302 30.00 13.83 11.67 12.83 32.17 23.17 23.50\nBOW 4695 31.17 10.33 10.67 10.50 31.17 12.83 23.33\nRP 38.67 33.17 32.67 29.83 49.17 46.33 32.67\nSSD (audio baseline) 45.50 52.17 50.17 51.50 59.00 58.67 48.67\nSSD/Stat (comb. baseline) 47.17 55.33 53.00 52.33 65.83+ 61.33 45.00\nSSD/Stat/Rhyme 47.33 54.17 52.67 54.00 63.50 62.17 48.67\nSSD/Stat/POS 46.67 51.50 50.33 52.67 64.00+ 60.50 50.67\nSSD/Stat/POS/Rhyme 47.17 52.17 50.67 53.50 64.00+ 60.33 48.00\nBOW 893/SSD 35.67 - 41.50 - 44.33 - 34.83 - 62.17 60.83 41.33\nBOW 893/SSD/POS/Rhyme/TextStat 39.33 - 45.83 46.67 36.33 - 64.00 63.83 44.83\nTable 3 . Ensemble classiï¬cation results\nSmall Database Large Database\nRule All subspaces SSD-only All subspaces SSD-only\nRSWV 63.67+ 59.00 73.65+ 69.33\nBWWV 63.67+ 59.33 74.08+ 69.69\nQBWWV 63.17 60.17 73.94+ 70.62\nthe bag-of-words features alone performed better. There is\nno increase on performance on any of the other classiï¬cation\nschemes; in contrary, on Na Â¨Ä±ve Bayes and k-NN, the results\nare statistically signiï¬cant worse. The rhyme and style fea-\ntures may thus be seen as more complimentary to the audio\nfeatures.\nTable 3 ï¬nally presents the results of a number of se-\nlected combination rules. These rules have been selected, as\nthey showed to be the most performing rules over a series of\nexperiments. We can see from that results that we are able\nto improve on the SSD audio baseline by up to 4.5% point.\nThe rules RSWV , BWWV , and QBWWV thereby show al-\nmost the same accuracy. While the Cartesian ensemble ap-\nproach failed to beat the best result of feature fusion, namely\nthe linear SVM classiï¬er on combined SSD and text statis-\ntics features, we obtained a better result than this very same\nconcatenation approach achieved when using the SVM with\na quadratic kernel. It has to be noted that ï¬nding this best\nfeature fusion result requires testing a number of different\nfeature combinations, as well as testing a lot of different\nalgorithms. This is a time-consuming and labour-intensive\ntask, as well as it is computationally expensive.\nThe results on the large dataset given in Table 4, in-\ncluding bag-of-words feature sets with different number of\nfeatures selected by simple document frequency threshold-\ning. SSD was again clearly the best audio feature set, clearly\noutperforming the RP features by more than 14% on the best\nSSD classiï¬er than on the best RP classiï¬er (SVM quadratic\nand SVM linear, respectively). However, it is worth to note\nthat on this dataset, the quadratic SVM kernel on SSD per-\nformed with 69.43% signiï¬cantly better than the linear one\nwith 66.37%, which was the best kernel on the small database.We can further note that text statistics are again the best fea-\nture of the rhyme and style features, reaching almost 30%\npoints with SVMs. The bag-of-words features, however,\nyield much better results than that, with 42.47% when us-\ning the linear SVM kernel and 8270 content terms. We can\nachieve almost 40% accuracy also with the Na Â¨Ä±ve Bayes al-\ngorithm, while Random Forests and k-NN predict much less\ncorrectly classiï¬ed instances.\nRegarding the results with early fusion, while we could\nsigniï¬cantly improve the linear Kernel on SSD features by\nconcatenating them with the lyrics features, the improve-\nments for the quadratic kernel are a bit less. It is also inter-\nesting to note that the better combination is with the rhyme\nand style features yields better results than adding the bag-\nof-words, even though the bag-of-words alone had more\nthan 12% points better accuracy results. When using our\nnovel result (late) fusion approach, results for which are\nshown in Table 3, we can achieve classiï¬cation accuracies\nwhich are in absolute numbers up to 5% points better than\nwith the best concatenation approach, which is statistically\nsigniï¬cantly better. In numbers, the improvement is from\n69.43% as the best result with SSD features to 74.08% as\nthe best ensemble result. It can be noted that the best com-\nbination rules RSWV , BWWV , and QBWWV all show al-\nmost the same accuracy, thus relying on any of those seems\nfeasible.\nAs a further baseline to the ensembles of multiple fea-\ntures, an ensemble of the above mentioned classiï¬cation\nschemes on SSD features only is given in Table 3. This base-\nline is to test whether the improvements reported above are\nachieved due to the use of different schemes, or only when\nalso using different feature sets. As the ensemble on SSD-\nonly features improves just 0.5% point over the best single\nresults, while the performance is 3 to 4% point better than\nthat baseline when using all feature sets, it can be concluded\nthat the gain in accuracy is largely due to the Cartesian en-\nsemble of both feature subspaces and algorithms.\n679Poster Session 6\nTable 4 . Results of the single classiï¬cation on the large datasets\nFeature set NB 1-NN 5-NN 10-NN SVMLin SVMPol RF\nRhyme 16.62 16.92 16.58 18.11 16.08 15.65 19.91\nPOS 23.53 20.94 21.64 22.60 23.66 24.53 24.59\nTextStat 17.91 23.40 25.09 25.86 28.38 25.49 34.30\nBOW 248 28.71 21.34 15.85 13.53 36.52 36.36 31.24\nBOW 1456 37.19 15.89 12.53 15.42 40.18 39.12 29.98\nBOW 4262 38.65 15.32 12.30 13.03 41.08 34.16 28.98\nBOW 8270 39.38 15.25 12.40 13.06 42.47 29.38 30.34\nRP 34.73 41.57 40.68 40.88 55.90 51.11 37.35\nSSD (audio baseline) 42.11 62.58 62.21 62.78 66.37 69.43 55.07\nSSD/Stat (comb. baseline) 43.87 63.88 63.01 62.12 68.60+ 69.99 57.06\nSSD/Stat/POS 44.50+ 62.51 63.18 62.48 68.86+ 69.46 55.90\nSSD/Stat/POS/Rhyme 44.80+ 62.74 62.41 61.78 67.83 69.69 57.63+\nSSD/BOW 4262 42.24 31.67âˆ’ 31.18âˆ’ 30.94âˆ’ 66.97 66.57âˆ’ 47.02âˆ’\nSSD/POS/Rhyme/BOW 4262 41.54 50.22âˆ’ 55.67âˆ’ 58.63âˆ’ 67.46 68.50 53.24âˆ’\n4. CONCLUSIONS\nWe presented an approach for multi-modal classiï¬cation of\nmusic. Contrary to earlier work on fusion of feature sub-\nspaces, the approach is built on classiï¬er ensemble tech-\nniques, i.e. fusion of the labels assigned by each single clas-\nsiï¬er. We evaluated the method by musical genre classiï¬-\ncation on two different datasets. We achieved better results\nthan when using the single feature sets alone, and for the\nlarger dataset also better results than with the best concate-\nnation approach. These improvements are up to 6% points\nabove the baseline, and statistically signiï¬cant.\nWe observed that the combination of the best performing\nfeature set and classiï¬cation algorithm can vary on different\ndatasets; even the choice of a different kernel for the SVM\nclassiï¬er yielded very different results on the small and large\ndataset. Using the ensemble approach, we can release the\nuser from having to make this choice explicitly, or from us-\ning computationally expensive approaches like model selec-\ntion. We have concluded from our experiments that a num-\nber of combination rules is promising, and the QBWWV\nmethod seems to show the overall best results.\n5. REFERENCES\n[1] Jean-Julien Aucouturier and Francois Pache. Improving\ntimbre similarity: How high is the sky? Journal of Neg-\native Results in Speech and Audio Sciences , 1(1), 2004.\n[2] Josef Kittler, Mohamad Hatef, Robert P.W. Duin, and\nJiri Matas. On combining classiï¬ers. IEEE Transac-\ntions on Pattern Analysis and Machine Intelligence ,\n20(3):226â€“239, 1998.\n[3] Peter Knees, Markus Schedl, and Gerhard Widmer.\nMultiple lyrics alignment: Automatic retrieval of song\nlyrics. In Proceedings of the 6th International Con-\nference on Music Information Retrieval , London, UK,\n2005.\n[4] Ludmila I. Kuncheva. Combining Pattern Classiï¬ers:\nMethods and Algorithms . Wiley-Interscience, 2004.[5] Thomas Lidy, Rudolf Mayer, Andreas Rauber, Pedro\nJ. Ponce de Le Â´on, Antonio Pertusa, and Jose M. I Ëœnesta.\nA cartesian ensemble of feature subspace classiï¬ers for\nmusic categorization. In Proceedings of the 11th Inter-\nnational Conference on Music Information Retrieval ,\nUtrecht, The Netherlands, August 9â€“13 2010.\n[6] Thomas Lidy and Andreas Rauber. Evaluation of fea-\nture extractors and psycho-acoustic transformations for\nmusic genre classiï¬cation. In Proceedings of the 6th In-\nternational Conference on Music Information Retrieval ,\nLondon, UK, 2005.\n[7] Beth Logan, Andrew Kositsky, and Pedro Moreno.\nSemantic analysis of song lyrics. In Proceedings of\nthe IEEE International Conference on Multimedia and\nExpo , Taipei, Taiwan, 2004.\n[8] Jose P. G. Mahedero, Â´Alvaro Mart Â´Ä±nez, Pedro Cano,\nMarkus Koppenberger, and Fabien Gouyon. Natural lan-\nguage processing of lyrics. In Proceedings of the ACM\nMultimedia , Singapore, 2005.\n[9] Rudolf Mayer, Robert Neumayer, and Andreas Rauber.\nCombination of audio and lyrics features for genre clas-\nsiï¬cation in digital audio collections. In Proceedings of\nthe ACM Multimedia . ACM New York, NY , USA, Octo-\nber 27-31 2008.\n[10] Rudolf Mayer, Robert Neumayer, and Andreas Rauber.\nRhyme and style features for musical genre classiï¬ca-\ntion by song lyrics. In Proceedings of the 9th Inter-\nnational Conference on Music Information Retrieval ,\nPhiladelphia, PA, USA, September 14-18 2008.\n[11] Nicola Orio. Music retrieval: A tutorial and re-\nview. Foundations and Trends in Information Retrieval ,\n1(1):1â€“90, 2006.\n[12] George Tzanetakis and Perry Cook. Marsyas: A frame-\nwork for audio analysis. Organized Sound , 4(30):169â€“\n175, 2000.\n680"
    },
    {
        "title": "Large-scale music similarity search with spatial trees.",
        "author": [
            "Brian McFee",
            "Gert R. G. Lanckriet"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1414930",
        "url": "https://doi.org/10.5281/zenodo.1414930",
        "ee": "https://zenodo.org/records/1414930/files/McFeeL11.pdf",
        "abstract": "Many music information retrieval tasks require finding the nearest neighbors of a query item in a high-dimensional space. However, the complexity of computing nearest neighbors grows linearly with size of the database, making exact retrieval impractical for large databases. We investigate modern variants of the classical KD-tree algorithm, which efficiently index high-dimensional data by recursive spatial partitioning. Experiments on the Million Song Dataset demonstrate that content-based similarity search can be significantly accelerated by the use of spatial partitioning structures.",
        "zenodo_id": 1414930,
        "dblp_key": "conf/ismir/McFeeL11",
        "keywords": [
            "music information retrieval",
            "nearest neighbors",
            "high-dimensional space",
            "complexity",
            "exact retrieval",
            "large databases",
            "KD-tree algorithm",
            "spatial partitioning",
            "content-based similarity search",
            "accelerated"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nLARGE-SCALE MUSIC SIMILARITY SEARCH WITH SPATIAL TREES\nBrian McFee\nComputer Science and Engineering\nUniversity of California, San DiegoGert Lanckriet\nElectrical and Computer Engineering\nUniversity of California, San Diego\nABSTRACT\nMany music information retrieval tasks require ï¬nding the\nnearest neighbors of a query item in a high-dimensional\nspace. However, the complexity of computing nearest neigh-\nbors grows linearly with size of the database, making exact re-\ntrieval impractical for large databases. We investigate modern\nvariants of the classical KD-tree algorithm, which efï¬ciently\nindex high-dimensional data by recursive spatial partitioning.\nExperiments on the Million Song Dataset demonstrate that\ncontent-based similarity search can be signiï¬cantly acceler-\nated by the use of spatial partitioning structures.\n1. INTRODUCTION\nNearest neighbor computations lie at the heart of many\ncontent-based approaches to music information retrieval prob-\nlems, such as playlist generation [4, 14], classiï¬cation and\nannotation [12, 18] and recommendation [15]. Typically,\neach item ( e.g., song, clip, or artist) is represented as a point\nin some high-dimensional space, e.g.,Rdequipped with Eu-\nclidean distance or Gaussian mixture models equipped with\nKullback-Leibler divergence.\nFor large music databases, nearest neighbor techniques\nface an obvious limitation: computing the distance from a\nquery point to each element of the database becomes pro-\nhibitively expensive. However, for many tasks, approximate\nnearest neighbors may sufï¬ce. This observation has moti-\nvated the development of general-purpose data structures\nwhich exploit metric structure to locate neighbors of a query\nin sub-linear time [1, 9, 10].\nIn this work, we investigate the efï¬ciency and accuracy\nof several modern variants of KD-trees [1] for answering\nnearest neighbor queries for musical content. As we will\ndemonstrate, these spatial trees are simple to construct, and\ncan provide substantial improvements in retrieval time while\nmaintaining satisfactory performance.\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proï¬t or commercial advantage and that copies\nbear this notice and the full citation on the ï¬rst page.\nc/circlecopyrt2011 International Society for Music Information Retrieval.2. RELATED WORK\nContent-based similarity search has received a considerable\namount of attention in recent years, but due to the obvious\ndata collection barriers, relatively little of it has focused on\nretrieval in large-scale collections.\nCai, et al. [4] developed an efï¬cient query-by-example\naudio retrieval system by applying locality sensitive hashing\n(LSH) [9] to a vector space model of audio content. Although\nLSH provides strong theoretical guarantees on retrieval per-\nformance in sub-linear time, realizing those guarantees in\npractice can be challenging. Several parameters must be care-\nfully tuned â€” the number of bins in each hash, the number\nof hashes, the ratio of near andfardistances, and collision\nprobabilities â€” and the resulting index structure can become\nquite large due to the multiple hashing of each data point.\nCai, et al.â€™s implementation scales to upwards of 105audio\nclips, but since their focus was on playlist generation, they\ndid not report the accuracy of nearest neighbor recall.\nSchnitzer, et al. developed a ï¬lter-and-reï¬ne system to\nquickly approximate the Kullback-Leibler (KL) divergence\nbetween timbre models [17]. Each song was summarized by\na multivariate Gaussian distribution over MFCC vectors, and\nthen mapped into a low-dimensional Euclidean vector space\nvia the FastMap algorithm [10], so that Euclidean distance\napproximates the symmetrized KL divergence between song\nmodels. To retrieve nearest neighbors for a query song, the\napproximate distances are computed from the query to each\npoint in the database by a linear scan (the ï¬lter step). The\nclosest points are then reï¬ned by computing the full KL\ndivergence to the query. This approach exploits the fact that\nlow-dimensional Euclidean distances are much cheaper to\ncompute than KL-divergence, and depending on the size of\nthe ï¬lter set, can produce highly accurate results. However,\nsince the ï¬lter step computes distance to the entire database,\nit requiresO(n)work, and performance may degrade if the\ndatabase is too large to ï¬t in memory.\n3. SPATIAL TREES\nSpatial trees are a family of data structures which recursively\nbisect a data setX âŠ‚ Rdofnpoints in order to facilitate\nefï¬cient (approximate) nearest neighbor retrieval [1,19]. The\nrecursive partitioning of Xresults in a binary tree, where\n55Poster Session 1\nFigure 1 . Spatial partition trees recursively split a data set\nXâŠ‚Rdby projecting onto a direction wâˆˆRdand splitting\nat the median b(dashed line), forming two disjoint subsets\nX/lscriptandXr.\nAlgorithm 1 Spatial partition tree\nInput: dataXâŠ‚Rd, maximum tree depth Î´\nOutput: balanced binary tree toverX\nPARTITION (X,Î´)\n1:ifÎ´= 0then\n2: returnX(leaf set)\n3:else\n4:wtâ†split(X){ï¬nd a split direction }\n5:btâ†median/parenleftbig/braceleftbig\nwT\ntx|xâˆˆX/bracerightbig/parenrightbig\n6:X/lscriptâ†/braceleftbig\nx|wT\ntxâ‰¤bt, xâˆˆX/bracerightbig\n7:Xrâ†/braceleftbig\nx|wT\ntx>bt, xâˆˆX/bracerightbig\n8:t/lscriptâ†PARTITION (X/lscript,Î´âˆ’1)\n9:trâ†PARTITION (Xr,Î´âˆ’1)\n10: returnt= (wt,bt,t/lscript,tr)\neach nodetcorresponds to a subset of the data XtâŠ†X\n(Figure 1 ). At the root of the tree lies the entire set X, and\neach nodetdeï¬nes a subset of its parent.\nA generic algorithm to construct partition trees is listed as\nAlgorithm 1. The set XâŠ‚Rdis projected onto a direction\nwtâˆˆRd, and split at the median btinto subsetsX/lscriptandXr:\nsplitting at the median ensures that the tree remains balanced.\nThis process is then repeated recursively on each subset, until\na speciï¬ed tree depth Î´is reached.\nSpatial trees offer several appealing properties. They are\nsimple to implement, and require minimal parameter-tuning:\nspeciï¬cally, only the maximum tree depth Î´, and the rule for\ngenerating split directions. Moreover, they are efï¬cient to\nconstruct and use for retrieval. While originally developed\nfor use in metric spaces, the framework has been recently\nextended to support general Bregman divergences (including,\ne.g., KL-divergence) [5]. However, for the remainder of\nthis article, we will focus on building trees for vector space\nmodels ( Rdwith Euclidean distance).\nIn order for Algorithm 1 to be fully speciï¬ed, we must\nprovide a function split(X)which determines the split di-\nrectionw. Several splitting rules have been proposed in theliterature, and our experiments will cover the four described\nby Verma, et al. [20]: maximum variance KD, principal\ndirection (PCA), 2-means, and random projection.\n3.1 Maximum variance KD-tree\nThe standard KD-tree ( k-dimensional tree) chooses wby cy-\ncling through the standard basis vectors ei(iâˆˆ{1,2,...,d}),\nso that at level jin the tree, the split direction is w=ei+1\nwithi=jmodd[1]. The standard KD-tree can be effec-\ntive for low-dimensional data, but it is known to perform\npoorly in high dimensions [16, 20]. Note also that if n<2d,\nthere will not be enough data to split along every coordinate,\nso some (possibly informative) features may never be used\nby the data structure.\nA common ï¬x to this problem is to choose was the coor-\ndinate which maximally spreads the data [20]:\nsplitKD(X) = argmax\nei/summationdisplay\nxâˆˆX/parenleftbig\neT\ni(xâˆ’Âµ)/parenrightbig2, (1)\nwhereÂµis the sample mean vector of X. Intuitively, this\nsplit rule picks the coordinate which provides the greatest\nreduction in variance (increase in concentration).\nThe maximum variance coordinate can be computed with\na single pass overXby maintaining a running estimate of the\nmean vector and coordinate-wise variance, so the complexity\nof computing splitKD(X)isO(dn).\n3.2 PCA-tree\nThe KD split rule ( Eqn. (1) ) is limited to axis-parallel direc-\ntionsw. The principal direction (or principal components\nanalysis, PCA) rule generalizes this to choose the direction\nwâˆˆRdwhich maximizes the variance, i.e., the leading\neigenvectorvof the sample covariance matrix /hatwideÎ£:\nsplitPCA(X) = argmax\nvvT/hatwideÎ£vs.t./bardblv/bardbl2= 1.(2)\nBy using the full covariance matrix to choose the split direc-\ntion, the PCA rule may be more effective than KD-tree at\nreducing the variance at each split in the tree.\n/hatwideÎ£can be estimated from a single pass over X, so (assum-\ningn>d ) the time complexity of splitPCAisO(d2n).\n3.3 2-means\nUnlike the KD and PCA rules, which try to maximally re-\nduce variance with each split, the 2-means rule produces\nsplits which attempt preserve cluster structure. This is ac-\ncomplished by running the k-means algorithm on Xwith\nk= 2, and deï¬ning wto be the direction spanned by the\ncluster centroids c1,c2âˆˆRd:\nsplit2M(X) =c1âˆ’c2. (3)\nWhile this general strategy performs well in practice [13],\nit can be costly to compute a full k-means solution. In our\nexperiments, we instead use an online k-means variant which\nruns inO(dn)time [3].\n5612th International Society for Music Information Retrieval Conference (ISMIR 2011)\n3.4 Random projection\nThe ï¬nal splitting rule we will consider is to simply take a\ndirection uniformly at random from the unit sphere Sdâˆ’1:\nsplitRP(X)âˆ¼USdâˆ’1, (4)\nwhich can equivalently be computed by normalizing a sample\nfrom the multivariate Gaussian distribution N(0,Id). The\nrandom projection rule is simple to compute and adapts to\nthe intrinsic dimensionality of the data X[8].\nIn practice, the performance of random projection trees\ncan be improved by independently sampling mdirections\nwiâˆ¼ Sdâˆ’1, and returning the wiwhich maximizes the\ndecrease in data diameter after splitting [20]. Since a full\ndiameter computation would take O(dn2)time, we instead\nreturn the direction which maximizes the projected diameter :\nargmax\nwimax\nx1,x2âˆˆXwT\nix1âˆ’wT\nix2. (5)\nThis can be computed in a single pass over Xby tracking the\nmaximum and minimum of wT\nixin parallel for all wi, so the\ntime complexity of splitRPisO(mdn). Typically, mâ‰¤d,\nsosplitRPis comparable in complexity to splitPCA.\n3.5 Spill trees\nThe main drawback of partition trees is that points near the\ndecision boundary become isolated from their neighbors\nacross the partition. Because data concentrates near the\nmean after (random) projection [8], hard partitioning can\nhave detrimental effects on nearest neighbor recall for a large\npercentage of queries.\nSpill trees remedy this problem by allowing overlap be-\ntween the left and right subtrees [13]. If a point lies close\nto the median, then it will be added to both subtrees, thus\nreducing the chance that it becomes isolated from its neigh-\nbors ( Figure 2 ). This is accomplished by maintaining two\ndecision boundaries b/lscript\ntandbr\nt. IfwT\ntx>br\nt, thenxis added\nto the right tree, and if wT\ntxâ‰¤b/lscript\nt, it is added to the left. The\ngap between b/lscript\ntandbr\ntcontrols the amount of data which\nspills across the split.\nThe algorithm to construct a spill tree is listed as Algo-\nrithm 2. The algorithm requires a spill threshold Ï„âˆˆ[0,1/2):\nrather than splitting at the median (so that a set of nitems\nis split into subsets of size roughly n/2), the data is split at\nthe(1/2+Ï„)-quantile, so that each subset has size roughly\nn(1/2+Ï„). Note that when Ï„= 0, the thresholds coin-\ncide (b/lscript\nt=br\nt), and the algorithm simpliï¬es to Algorithm 1.\nPartition trees, therefore, correspond to the special case of\nÏ„= 0.\n3.6 Retrieval algorithm and analysis\nOnce a spill tree has been constructed, approximate near-\nest neighbors can be recovered efï¬ciently by the defeatist\nsearch method [13], which restricts the search to only the\nFigure 2 . Spill trees recursively split data like partition trees,\nbut the subsets are allowed to overlap. Points in the shaded\nregion are propagated to both subtrees.\nAlgorithm 2 Spill tree\nInput: dataXâŠ‚Rd, depthÎ´, thresholdÏ„âˆˆ[0,1/2)\nOutput:Ï„-spill treetoverX\nSPILL(X,Î´,Ï„)\n1:ifÎ´= 0then\n2: returnX(leaf set)\n3:else\n4:wtâ†split(X)\n5:b/lscript\ntâ†quantile/parenleftbig\n1/2+Ï„,/braceleftbig\nwT\ntx|xâˆˆX/bracerightbig/parenrightbig\n6:br\ntâ†quantile/parenleftbig\n1/2âˆ’Ï„,/braceleftbig\nwT\ntx|xâˆˆX/bracerightbig/parenrightbig\n7:X/lscriptâ†/braceleftbig\nx|wT\ntxâ‰¤b/lscript\nt, xâˆˆX/bracerightbig\n8:Xrâ†/braceleftbig\nx|wT\ntx>br\nt, xâˆˆX/bracerightbig\n9:t/lscriptâ†SPILL(X/lscript,Î´âˆ’1,Ï„)\n10:trâ†SPILL(Xr,Î´âˆ’1,Ï„)\n11: returnt= (wt,b/lscript\nt,br\nt,t/lscript,tr)\nleaf sets which contain the query. For a novel query qâˆˆRd\n(i.e., a previously unseen point), these sets can be found by\nAlgorithm 3.\nThe total time required to retrieve kneighbors for a novel\nqueryqcan be computed as follows. First, note that for a spill\ntree with threshold Ï„, each split reduces the size of the set\nby a factor of (1/2+Ï„), so the leaf sets of a depth- Î´tree are\nexponentially small in Î´:n(1/2+Ï„)Î´. Note thatÎ´â‰¤logn,\nand is typically chosen so that the leaf set size lies in some\nreasonable range ( e.g., between 100 and 1000 items).\nNext, observe that in general, Algorithm 3 may map the\nqueryqto somehdistinct leaves, so the total size of the\nretrieval set is at most n/prime=hn(1/2+Ï„)Î´(although it may be\nconsiderably smaller if the sets overlap). For hleaves, there\nare at mosthpaths of length Î´to the root of the tree, and\neach step requires O(d)work to compute wT\ntq, so the total\ntime taken by Algorithm 3 is\nTRETRIEVEâˆˆO/parenleftbig\nh/parenleftbig\ndÎ´+n(1/2+Ï„)Î´/parenrightbig/parenrightbig\n.\nFinally, once the retrieval set has been constructed, the k\nclosest points can be found in time O(dn/primelogk)by using a\nk-bounded priority queue [7]. The total time to retrieve k\n57Poster Session 1\nAlgorithm 3 Spill tree retrieval\nInput: queryq, treet\nOutput: Retrieval setXq\nRETRIEVE (q,t)\n1:iftis a leaf then\n2: returnXt{all items contained in the leaf }\n3:else\n4:Xqâ†âˆ…\n5: ifwT\ntqâ‰¤b/lscript\ntthen\n6:Xqâ†XqâˆªRETRIEVE (q,t/lscript)\n7: ifwT\ntq>br\ntthen\n8:Xqâ†XqâˆªRETRIEVE (q,tr)\n9: returnXq\napproximate nearest neighbors for the query qis therefore\nTkNNâˆˆO/parenleftbig\nhd/parenleftbig\nÎ´+n(1/2+Ï„)Î´logk/parenrightbig/parenrightbig\n.\nIntuitively, for larger values of Ï„, more data is spread\nthroughout the tree, so the leaf sets become larger and re-\ntrieval becomes slower. Similarly, larger values of Ï„will\nresult in larger values of has queries will map to more leaves.\nHowever, as we will show experimentally, this effect is gen-\nerally mild even for relatively large values of Ï„.\nIn the special case of partition trees ( Ï„= 0), each query\nmaps to exactly h= 1leaf, so the retrieval time simpliï¬es to\nO(d(Î´+n/2Î´logk)).\n4. EXPERIMENTS\nOur song data was taken from the Million Song Dataset\n(MSD) [2]. Before describing the tree evaluation experi-\nments, we will brieï¬‚y summarize the process of constructing\nthe underlying acoustic feature representation.\n4.1 Audio representation\nThe audio content representation was developed on the 1%\nMillion Song Subset (MSS), and is similar to the model pro-\nposed in [15]. From each MSS song, we extracted the time\nseries of Echo Nest timbre descriptors (ENTs). This results in\na sample of approximately 8.5 million 12-dimensional ENTs,\nwhich were normalized by z-scoring according to the esti-\nmated mean and variance of the sample, randomly permuted,\nand then clustered by online k-means to yield 512 acoustic\ncodewords. Each song was summarized by quantizing each\nof its (normalized) ENTs and counting the frequency of each\ncodeword, resulting in a 512-dimensional histogram vector.\nEach codeword histogram was mapped into a probability\nproduct kernel (PPK) space [11] by square-rooting its entries,\nwhich has been demonstrated to be effective on similar audio\nrepresentations [15]. Finally, we appended the songâ€™s tempo,\nloudness, and key conï¬dence, resulting in a vector viâˆˆR515\nfor each song xi.Next, we trained an optimized similarity metric over audio\ndescriptors. First, we computed target similarity for each\npair of MSS artists by the Jaccard index between their user\nsets in a sample of Last.fm1collaborative ï¬lter data [6,\nchapter 3]. Tracks by artists with fewer than 30 listeners\nwere discarded. Next, all remaining artists were partitioned\ninto a training (80%) and validation (20%) set, and for each\nartist, we computed its top 10 most similar training artists.\nHaving constructed a training and validation set, the dis-\ntance metric was optimized by applying the metric learning\nto rank (MLR) algorithm on the training set of 4455 songs,\nand tuning parameters Câˆˆ{105,106,..., 109}andâˆ†âˆˆ\n{AUC, MRR, MAP, Prec@10 }to maximize AUC score on\nthe validation set of 1110 songs. Finally, the resulting metric\nWwas factored by PCA (retaining 95% spectral mass) to\nyield a linear projection LâˆˆR222Ã—515.\nThe projection matrix Lwas then applied to each viin\nMSD. As a result, each MSD song was mapped into R222\nsuch that Euclidean distance is optimized by MLR to retrieve\nsongs by similar artists.\n4.2 Representation evaluation\nTo verify that the optimized vector quantization (VQ) song\nrepresentation carries musically relevant information, we\nperformed a small-scale experiment to evaluate its predic-\ntive power for semantic annotation. We randomly selected\none song from each of 4643 distinct artists. (Artists were\nrestricted to be disjoint from MSS to avoid contamination.)\nEach song was represented by the optimized 222-dimensional\nVQ representation, and as ground truth annotations, we ap-\nplied the corresponding artistâ€™s terms from the top-300 terms\nprovided with MSD, so that each song xihas a binary anno-\ntation vector yiâˆˆ{0,1}300. For a baseline comparison, we\nadapt the representation used by Schnitzer, et al. [17], and\nfor each song, we ï¬t a full-covariance Gaussian distribution\nover its ENT features.\nThe set was then randomly split 10 times into 80%-training\nand 20%-test sets. Following the procedure described by\nKim, et al. [12], each test song was annotated by threshold-\ning the average annotation vector of its knearest training\nneighbors as determined by Euclidean distance on VQ repre-\nsentations, and by KL-divergence on Gaussians. Varying the\ndecision threshold yields a trade-off between precision and\nrecall. In our experiments, the threshold was varied between\n0.1 and 0.9.\nFigure 3 displays the precision-recall curves averaged\nacross all 300 terms and training/test splits for several values\nofk. At small values of k, the VQ representation achieves\nsigniï¬cantly higher performance than the Gaussian represen-\ntation. We note that this evaluation is by no means conclusive,\nand is merely meant to demonstrate that the underlying space\nis musically relevant.\n1http://last.fm\n5812th International Society for Music Information Retrieval Conference (ISMIR 2011)\n0 0.2 0.400.050.10.150.2k=5\nRecallPrecision\n  \n0 0.2 0.400.050.10.150.2k=15\nRecallPrecision\n  \n0 0.2 0.400.050.10.150.2k=30\nRecallPrecision\n  \n0 0.2 0.400.050.10.150.2\nRecallPrecisionk=100\n  \nVQ KLFigure 3 . Mean precision-recall for k-nearest neighbor an-\nnotation with VQ and Gaussian (KL) representations.\n4.3 Tree evaluation\nTo test the accuracy of the different spatial tree algorithms,\nwe partitioned the MSD data into 890205 training songs X\nand 109795 test songs X/prime. Using the optimized VQ repre-\nsentations onX, we constructed trees with each of the four\nsplitting rules (PCA, KD, 2-means, and random projection),\nvarying both the maximum depth Î´âˆˆ{5,6,..., 13}and\nspill threshold Ï„âˆˆ{0,0.01,0.05,0.10}. AtÎ´= 13 , this\nresults in leaf sets of size 109 with Ï„= 0, and 1163 for\nÏ„= 0.10. For random projection trees, we sample m= 64\ndimensions at each call to splitRP.\nFor each test song qâˆˆX/prime, and treet, we compute the\nretrieval set with Algorithm 3. The recall forqis the frac-\ntion of the true nearest-neighbors kNN(q)contained in the\nretrieval set:\nR(q,t) =|RETRIEVE (q,t)âˆ©kNN(q)|/k. (6)\nNote that since true nearest neighbors are always closer than\nany other points, they are always ranked ï¬rst, so precision\nand recall are equivalent here.\nTo evaluate the system, k= 100 exact nearest neighbors\nkNN(q)were found fromXfor each query qâˆˆX/primeby a full\nlinear search overX.\n4.4 Retrieval results\nFigure 4 lists the nearest-neighbor recall performance for all\ntree conï¬gurations. As should be expected, for all splitting\nrules and spill thresholds, recall performance degrades as the\nmaximum depth of the tree increases.\nAcross all spill thresholds Ï„and tree depths Î´, the relative\nordering of performance of the different split rules is essen-\ntially constant: splitPCAperforms slightly better than splitKD,\nand both dramatically outperform splitRPandsplit2M. This\nindicates that for the feature representation under considera-\ntion here (optimized codeword histograms), variance reduc-tion seems to be the most effective strategy for preserving\nnearest neighbors in spatial trees.\nFor small values of Ï„, recall performance is generally\npoor for all split rules. However, as Ï„increases, recall per-\nformance increases across the board. The improvements are\nmost dramatic for splitPCA. WithÏ„= 0, andÎ´= 7, the\nPCA partition tree has leaf sets of size 6955 (0.8% of X),\nand achieves median recall of 0.24. WithÏ„= 0.10and\nÎ´= 13 , the PCA spill tree achieves median recall of 0.53\nwith a comparable median retrieval set size of 6819 (0.7%\nofX): in short, recall is nearly doubled with no appreciable\ncomputational overhead. So, by looking at less than 1% of\nthe database, the PCA spill tree is able to recover more than\nhalf of the 100 true nearest neighbors for novel test songs.\nThis contrasts with the ï¬lter-and-reï¬ne approach [17], which\nrequires a full scan of the entire database.\n4.5 Timing results\nFinally, we evaluated the retrieval time necessary to answer\nk-nearest neighbor queries with spill trees. We assume that\nall songs have already been inserted into the tree, since this is\nthe typical case for long-term usage. As a result, the retrieval\nalgorithm can be accelerated by maintaining indices mapping\nsongs to leaf sets (and vice versa).\nWe evaluated the retrieval time for PCA spill trees of\ndepthÎ´= 13 and threshold Ï„âˆˆ{0.05,0.10}, since they\nexhibit practically useful retrieval accuracy. We randomly\nselected 1000 test songs and inserted them into the tree prior\nto evaluation. For each test song, we compute the time\nnecessary to retrieve the knearest training neighbors from\nthe spill tree (ignoring test songs), for kâˆˆ{10,50,100}.\nFinally, for comparison purposes, we measured the time to\ncompute the true knearest neighbors by a linear search over\nthe entire training set.\nOur implementation is written in Python/NumPy,2and\nloads the entire data set into memory. The test machine has\ntwo 1.6GHz Intel Xeon CPUs and 4GB of RAM. Timing\nresults were collected through the cProï¬le utility.\nFigure 5 lists the average retrieval time for each algorithm.\nThe times are relatively constant with respect to k: a full lin-\near scan typically takes approximately 2.4 seconds, while\ntheÏ„= 0.10spill tree takes less than 0.14 seconds, and\ntheÏ„= 0.05tree takes less than 0.02 seconds. In relative\nterms, setting Ï„= 0.10yields a speedup factor of 17.8, and\nÏ„= 0.05yields a speedup of 119.5 over the full scan. The\ndifference in speedup from Ï„= 0.10toÏ„= 0.05can be ex-\nplained by the fact that smaller overlapping regions result in\nsmaller (and fewer) leaf sets for each query. In practice, this\nspeed-accuracy trade-off can be optimized for the particular\ntask at hand: applications requiring only a few neighbors\nwhich may be consumed rapidly ( e.g., sequential playlist\ngeneration) may beneï¬t from small values of Ï„, whereas\n2http://numpy.scipy.org\n59Poster Session 1\n0.01% 0.1% 1% 10%00.20.40.60.81\nRetrieval sizeRecallÏ„=0.00\n  \nPCA\nKD\nRandom\n2âˆ’means\n0.01% 0.1% 1% 10%00.20.40.60.81\nRetrieval sizeRecallÏ„=0.01\n  \nPCA\nKD\nRandom\n2âˆ’means\n0.01% 0.1% 1% 10%00.20.40.60.81\nRetrieval sizeRecallÏ„=0.05\n  \nPCA\nKD\nRandom\n2âˆ’means\n0.01% 0.1% 1% 10%00.20.40.60.81\nRetrieval sizeRecallÏ„=0.10\n  \nPCA\nKD\nRandom\n2âˆ’meansFigure 4 . Median 100-nearest-neighbor recall for each splitting rule (PCA, KD, 2-means, and random projection), spill threshold\nÏ„âˆˆ{0,0.01,0.05,0.10}, and tree depth Î´âˆˆ{5,6,..., 13}. Each point along a curve corresponds to a different tree depth Î´,\nwith larger retrieval size indicating smaller Î´. For eachÎ´, the corresponding recall point is plotted at the median size of the\nretrieval set (as a fraction of the entire database). Error bars correspond to 25th and 75th percentiles of recall for all test queries.\n0.02     0.14 1 2 2.5k=10\nk=50\nk=100\nRetrieval time (s)  Full Ï„=0.10 Ï„=0.05\nFigure 5 . Average time to retrieve k(approximate) nearest\nneighbors with a full scan versus PCA spill trees.\napplications requiring more neighbors ( e.g., browsing recom-\nmendations for discovery) may beneï¬t from larger Ï„.\n5. CONCLUSION\nWe have demonstrated that spatial trees can effectively accel-\nerate approximate nearest neighbor retrieval. In particular,\nfor VQ audio representations, the combination of spill trees\nwith and PCA splits yields a favorable trade-off between\naccuracy and complexity of k-nearest neighbor retrieval.\n6. ACKNOWLEDGMENTS\nThe authors thank Jason Samarin, and acknowledge support\nfrom Qualcomm, Inc., Yahoo! Inc., the Hellman Fellowship\nProgram, and NSF Grants CCF-0830535 and IIS-1054960.\n7. REFERENCES\n[1]J.L. Bentley. Multidimensional binary search trees used for\nassociative searching. Commun. ACM , 18:509â€“517, Sep. 1975.\n[2]Thierry Bertin-Mahieux, Daniel P.W. Ellis, Brian Whitman, and\nPaul Lamere. The million song dataset. In Proceedings of the\n12th International Conference on Music Information Retrieval\n(ISMIR 2011) , 2011.\n[3]LÂ´eon Bottou and Yoshua Bengio. Convergence properties of\nthe kmeans algorithm. In Advances in Neural Information Pro-\ncessing Systems , volume 7. MIT Press, Denver, 1995.[4]Rui Cai, Chao Zhang, Lei Zhang, and Wei-Ying Ma. Scalable\nmusic recommendation by search. In International Conference\non Multimedia , pages 1065â€“1074, 2007.\n[5]Lawrence Cayton. Fast nearest neighbor retrieval for bregman\ndivergences. In International Conference on Machine Learning ,\npages 112â€“119, 2008.\n[6] O. Celma. Music Recommendation and Discovery in the Long\nTail. Springer, 2010.\n[7]T. H. Cormen, C. E. Leiserson, R. L. Rivest, and C. Stein.\nIntroduction to Algorithms . The MIT Press, 3rd edition, 2009.\n[8]Sanjoy Dasgupta and Yoav Freund. Random projection trees\nand low dimensional manifolds. In ACM Symposium on Theory\nof Computing , pages 537â€“546, 2008.\n[9]Mayur Datar, Nicole Immorlica, Piotr Indyk, and Vahab S.\nMirrokni. Locality-sensitive hashing scheme based on p-stable\ndistributions. In Proceedings of the twentieth annual symposium\non Computational geometry , SCG â€™04, pages 253â€“262, New\nYork, NY , USA, 2004. ACM.\n[10] Christos Faloutsos and King-Ip Lin. Fastmap: a fast algorithm\nfor indexing, data-mining and visualization. In Proceedings of\nACM SIGMOD , pages 163â€“174, 1995.\n[11] Tony Jebara, Risi Kondor, and Andrew Howard. Probability\nproduct kernels. JMLR , 5:819â€“844, December 2004.\n[12] J.H. Kim, B. Tomasik, and D. Turnbull. Using artist similarity\nto propagate semantic information. In ISMIR , 2009.\n[13] Ting Liu, Andrew W. Moore, Alexander Gray, and Ke Yang.\nAn investigation of practical approximate nearest neighbor al-\ngorithms. In NIPS , pages 825â€“832. 2005.\n[14] B. Logan. Music recommendation from song sets. In Interna-\ntional Symposium on Music Information Retrieval , 2004.\n[15] B. McFee, L. Barrington, and G.R.G. Lanckriet. Learn-\ning content similarity for music recommendation, 2011.\nhttp://arxiv.org/1105.2344.\n[16] J. Reiss, J.J. Aucouturier, and M. Sandler. Efï¬cient multidimen-\nsional searching routines for music information retrieval. In\nISMIR , 2001.\n[17] Dominik Schnitzer, Arthur Flexer, and Gerhard Widmer. A\nï¬lter-and-reï¬ne indexing method for fast similarity search in\nmillions of music tracks. In ISMIR , 2009.\n[18] M. Slaney, K. Weinberger, and W. White. Learning a metric for\nmusic similarity. In ISMIR , pages 313â€“318, September 2008.\n[19] J.K. Uhlmann. Satisfying general proximity/similarity queries\nwith metric trees. 40(4):175â€“179, 1991.\n[20] Nakul Verma, Samory Kpotufe, and Sanjoy Dasgupta. Which\nspatial partition trees are adaptive to intrinsic dimension? In\nUncertainty in Artiï¬cial Intelligence , pages 565â€“574, 2009.\n60"
    },
    {
        "title": "The Natural Language of Playlists.",
        "author": [
            "Brian McFee",
            "Gert R. G. Lanckriet"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1418119",
        "url": "https://doi.org/10.5281/zenodo.1418119",
        "ee": "https://zenodo.org/records/1418119/files/McFeeL11a.pdf",
        "abstract": "We propose a simple, scalable, and objective evaluation procedure for playlist generation algorithms. Drawing on standard techniques for statistical natural language processing, we characterize playlist algorithms as generative models of strings of songs belonging to some unknown language. To demonstrate the procedure, we compare several playlist algorithms derived from content, semantics, and meta-data. We then develop an efficient algorithm to learn an optimal combination of simple playlist algorithms. Experiments on a large collection of naturally occurring playlists demonstrate the efficacy of the evaluation procedure and learning algorithm.",
        "zenodo_id": 1418119,
        "dblp_key": "conf/ismir/McFeeL11a",
        "keywords": [
            "evaluation procedure",
            "playlist generation algorithms",
            "statistical natural language processing",
            "generative models",
            "unknown language",
            "comparison",
            "efficient algorithm",
            "learning",
            "optimal combination",
            "experiments"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nTHE NATURAL LANGUAGE OF PLAYLISTS\nBrian McFee\nComputer Science and Engineering\nUniversity of California, San DiegoGert Lanckriet\nElectrical and Computer Engineering\nUniversity of California, San Diego\nABSTRACT\nWe propose a simple, scalable, and objective evaluation pro-\ncedure for playlist generation algorithms. Drawing on stan-\ndard techniques for statistical natural language processing,\nwe characterize playlist algorithms as generative models of\nstrings of songs belonging to some unknown language. To\ndemonstrate the procedure, we compare several playlist algo-\nrithms derived from content, semantics, and meta-data. We\nthen develop an efï¬cient algorithm to learn an optimal combi-\nnation of simple playlist algorithms. Experiments on a large\ncollection of naturally occurring playlists demonstrate the\nefï¬cacy of the evaluation procedure and learning algorithm.\n1. INTRODUCTION\nMusic listeners typically do not listen to a single song in\nisolation. Rather, listening sessions tend to persist over a\nsequence of songs: a playlist . The increasing quantity of\nreadily available, digital music content has motivated the\ndevelopment of algorithms and services to automate search,\nrecommendation, and discovery in large music databases.\nHowever, playlist generation is fundamental to how users\ninteract with music delivery services, and is generally distinct\nfrom related topics, such as similarity and semantic search.\nAlthough many automatic playlist generation algorithms\nhave been proposed over the years, there is currently no\nstandard evaluation procedure. As a result, it is difï¬cult to\nquantitatively compare different algorithms and objectively\ndetermine if any progress is being made.\nAt present, the predominant approach to playlist algorithm\nevaluation is to conduct human opinion surveys, which can\nbe expensive, time-consuming and difï¬cult to reproduce.\nAlternatively, current automated evaluation schemes either\nreduce the problem to a (discriminative) information retrieval\nsetting, or rely on simplifying assumptions that may not hold\nin practice.\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proï¬t or commercial advantage and that copies\nbear this notice and the full citation on the ï¬rst page.\nc/circlecopyrt2011 International Society for Music Information Retrieval.In this work, we propose a simple, scalable, and objec-\ntive evaluation procedure for playlist algorithms that avoids\nthe pitfalls of previous approaches. Our approach is guided\nby the observation that playlist generation is not (only) an\ninformation retrieval problem, but a language modeling prob-\nlem. The proposed method can be applied to a large class of\nplaylist algorithms, and we provide several examples with\nexperimental results. Finally, we propose an algorithm to\nlearn an optimal ensemble algorithm from a collection of\nsimple playlist generators.\n2. A BRIEF HISTORY OF PLAYLIST EV ALUATION\nAlthough many algorithms for playlist generation have been\nproposed, evaluation procedures have received relatively lit-\ntle speciï¬c attention. Here, we brieï¬‚y summarize previously\nproposed evaluation strategies, which can broadly be grouped\ninto three categories: human evaluation ,semantic cohesion ,\nandsequence prediction . This section is not intended as a\ncomprehensive survey of playlist algorithms , for which we\nrefer the interested reader to [8, chapter 2].\n2.1 Human evaluation\nSince the eventual goal of playlist algorithms is to improve\nuser experience, the ideal method of algorithm evaluation\nis to directly measure human response. Numerous studies\nhave been conducted in which test subjects rate the quality\nof playlists generated by one or more algorithms. Pauws\nand Eggen [18] asked users to provide a query song with a\nparticular context-of-use in mind ( e.g.,lively music ), which\nwas used as a seed to generate a playlist. The user evaluated\nthe resulting playlist on a scale of 1â€“10, and how many tracks\nin the playlist ï¬t the userâ€™s intended use context. From these\nsurvey responses, the authors were able to derive various\nstatistics to demonstrate that their proposed algorithm signif-\nicantly outperforms randomly generated playlists. Similarly,\nBarrington, et al. [1] conducted experiments in which users\nwere presented with two playlists (generated by obscured,\ncompeting systems) and asked to indicate which one was\n(subjectively) better, and why.\nWhile direct human evaluation studies can provide evi-\ndence that one algorithm measurably outperforms another,\nthey also have obvious practical limitations. This can be labo-\nrious, difï¬cult to reproduce, and may require large numbers\n537Poster Session 4\nof test subjects and example playlists to achieve statistically\nmeaningful results and overcome the effects of subjectivity.\n2.2 Semantic cohesion\nThe general impracticality of large-scale user studies has\nmotivated the development of automated evaluation tech-\nniques. The most common approaches compute some easily\nmeasurable quantity from each song in a generated playlist\n(e.g., artist, album, or genre), which is used to determine\nthecohesion of the playlist. Cohesion may be deï¬ned by\nfrequency counts of meta-data co-occurrence ( e.g., songs\nby the same artist) [13, 14] or entropy of the distribution of\ngenres within the playlist [7, 12]. In this framework, it is\ntypically assumed that each song can be mapped to a unique\nsemantic tag ( e.g.,blues ). This assumption is often unreal-\nistic, as songs generally map to multiple tags. Assigning\neach song to exactly one semantic description may therefore\ndiscard a great deal of information, and obscure the semantic\ncontent of the playlist. A more general form of semantic\nsummarization was developed by Fields, et al. [9], and used\nto derive a distance measure between latent topic models of\nplaylists. However, it is not immediately clear how such a\ndistance metric would facilitate algorithm evaluation.\nIssues of semantic ambiguity aside, a more fundamental\nï¬‚aw lies in the assumption that cohesion accurately charac-\nterizes playlist quality. In reality, this assumption is rarely\njustiï¬ed, and evidence suggests that users often prefer highly\ndiverse playlists [20].\n2.3 Sequence prediction\nA more direct approach to automatic evaluation arises from\nformulating playlist generation as a prediction problem: given\nsome contextual query ( e.g., a userâ€™s preferences, or a partial\nobservation of songs in a playlist), the algorithm must predict\nwhich song to play next. The algorithm is then evaluated on\nthe grounds of its prediction, under some notion of correct-\nness. For example, Platt, et al. [19] observe a subset of songs\nin an existing playlist (the query ), and the algorithm predicts\na ranking of all songs. The quality of the algorithm is then\ndetermined by the position within the predicted ranking of\nthe remaining, unobserved songs from the playlist. Mail-\nlet, et al. [15] similarly predict a ranking over songs from a\ncontextual query â€” in this case, the preceding song or pair\nof songs â€” and evaluate by comparing the ranking to one\nderived from a large collection of existing playlists.\nEssentially, both of the above approaches transform playlist\nevaluation into an information retrieval (IR) problem: songs\nobserved to co-occur with the query are relevant , and all\nother songs as irrelevant . As noted by Platt, et al. [19], this\nnotion of relevance may be exceedingly pessimistic in prac-\ntice due to sparsity of observations. In even moderately large\nmusic databases (say, on the order of thousands of songs), the\nprobability of observing any given pair of songs in a playlistbecomes vanishingly small, and therefore, the overwhelming\nmajority of song predictions are considered incorrect. In\nthis framework, a prediction may disagree with observed\nco-occurrences, but still be equally pleasing to a user of the\nsystem, and therefore be unfairly penalized.\nThe IR approach â€” and more generally, any discrimina-\ntive learning approach â€” is only applicable when one can\nobtain negative examples, i.e.,badplaylists. In reality, nega-\ntive examples are difï¬cult to deï¬ne, let alone obtain, as users\ntypically only share playlists that they like.1This suggests\nthat discriminative evaluation may not be the most natural ï¬t\nfor playlist generation.\n3. A NATURAL LANGUAGE APPROACH\nIn contrast to discriminative approaches to playlist evalua-\ntion, we advocate the generative perspective when modeling\nplaylist composition. Rather than attempting to objectively\nscore playlists as good orbad, which generally depends on\nuser taste and unobservable contextual factors, we instead\nfocus on modeling the distribution of naturally occurring\nplaylists.\nFormally, letX={x1,x2,...,xn}denote a library of\nsongs. We deï¬ne a playlist as an ordered ï¬nite sequence of\nelements ofX. Any procedure which constructs such ordered\nsequences is a playlist algorithm (orplaylister ). In general,\nwe consider randomized algorithms, which can be used to\ngenerate multiple unique playlists from a single query. Each\nplaylister, be it randomized or deterministic, induces a prob-\nability distribution over song sequences, and may therefore\nbe treated as a probabilistic generative model.\nThis leads to our central question: how should generative\nmodels of song sequences be evaluated? Here, we take in-\nspiration from the literature of statistical natural language\nprocessing [16], in which statistical models are ï¬t to a sample\nof strings in the language ( e.g., grammatically valid sentences\nin English). A language model determines a probability dis-\ntribution Pover strings, which can be evaluated objectively\nby how well Pmatches the true distribution Pâˆ—. Since Pâˆ—\nis unknown, this evaluation is approximated by drawing a\nsampleS âˆ¼Pâˆ—of naturally occurring strings, and then\ncomputing the likelihood of the sample under the model P.\nReturning to the context of playlist generation, in place\nof vocabulary words, we have songs; rather than sentences,\nwe have playlists. The universe of human-generated playlists\ntherefore constitutes a natural language , and playlisters are\nmodels of the language of playlists. While this observation\nis not itself novel â€” it appears to be folklore among music\nresearchers â€” its implications for algorithm evaluation have\nnot yet been fully realized. We note that recent work by\nZheleva, et al. [21] evaluated playlisters in terms of perplexity\n1A notable exception is the work of Bosteels, et al. [4], in which explicit\nnegative feedback was inferred from skip behavior of Last.fm users. As\nnoted by the authors, skip behavior can be notoriously difï¬cult to interpret.\n53812th International Society for Music Information Retrieval Conference (ISMIR 2011)\n(exponentiated log-likelihood) of the genre distribution in a\nplaylist, rather than the song selection itself.\n3.1 Evaluation procedure\nTo evaluate a playlister A, we require the following:\n1. a library of nsongsX,\n2. a sample of playlists SâŠ†Xâˆ—,2and\n3. the likelihood PA[s]of any playlist sâˆˆXâˆ—.\nWhile the last requirement may seem like a tall order, we\nwill demonstrate that for large classes of playlisters, the\ncomputation can be quite simple.\nA playlisterAcan be evaluated by computing the average\nlog-likelihood of the sample S:\nL(S|A) =1\n|S|/summationdisplay\nsâˆˆSlogPA[s]. (1)\nThe average log-likelihood, on an absolute scale, is not\ndirectly interpretable â€” although it approximates the cross-\nentropy between PAand the true, unknown distribution\nPâˆ—[16] â€” but it is useful for performing relative compar-\nisons between two playlisters. Given a competing playlister\nA/prime, we can say that Ais a better model of the data than A/primeif\nL(S|A)>L(S|A/prime).\nThere is a subtle, but important distinction between the\nproposed approach and previous approaches to playlist eval-\nuation. Rather than evaluate the perceived quality of a gen-\nerated, synthetic playlist, we instead evaluate the algorithm\nin terms of how likely it is to produce naturally occurring\nplaylists.\n4. PLAYLIST ALGORITHMS\nTo demonstrate the proposed evaluation approach, we will\nderive playlist probabilities for several generic playlisters.\nAlthough the method is fully general, we restrict attention to\nplaylisters which satisfy the Markov property:\nP[(x0,x1,...,xk)] =\nP[X=x0]k/productdisplay\ni=1P[Xt+1=xi|Xt=xiâˆ’1].(2)\nWe assume that the ï¬rst song is chosen uniformly at ran-\ndom, and therefore contributes a ï¬xed constant log 1/nto the\noverall log-likelihood, which may be safely ignored. The\nlikelihood of an arbitrary playlist under a Markov model\ncan therefore be decomposed into the product of bigram\nlikelihoods, so the log-likelihood is proportional to the sum:\nlogP[(x0,...,xk)]âˆk/summationdisplay\ni=1logP[Xt+1=xi|Xt=xiâˆ’1].\n2Xâˆ—denotes the Kleene-âˆ—operation, and contains all sequences of any\nlength of elements drawn from X.Note that this reasoning can be extended to higher order\nMarkov models â€” e.g., second order would decompose into\ntrigrams â€” but to ease exposition, we focus on ï¬rst-order\nmodels. For the remainder of this article, we will assume\nthatSis a collection of bigrams.\n4.1 Uniform shufï¬‚e\nThe simplest playlister selects each song uniformly at ran-\ndom fromX. This can be reï¬ned somewhat by disallowing\nconsecutive repetitions, so that if the current song is xt, then\nxt+1is drawn uniformly at random from X\\{xt}. Since\nxt+1depends only on xt, it satisï¬es the Markov property,\nand the conditional bigram probability is\nPU[Xt+1=x|Xt=xt] =/braceleftBigg\n1/nâˆ’1x/negationslash=xt\n0x=xt.(3)\nThe uniform shufï¬‚e playlister provides an obvious baseline,\nand should be included in any comparative evaluation.\n4.2 Weighted shufï¬‚e\nA slight variation on the uniform shufï¬‚e is to draw the next\nsong not from a uniform distribution, but a weighted dis-\ntribution derived from a score function F(x)>0, which\nmay encode artist popularity, user preference, or any other\nsong-level property. The resulting bigram probability is\nPF[Xt+1=x|Xt=xt] =/braceleftBiggF(x)P\nx/prime/negationslash=xtF(x/prime)x/negationslash=xt\n0 x=xt.\n(4)\nIn general,Fmay be dynamic and can be used to incorporate\nuser feedback, thereby facilitating steerability [15]. Dynamic\nand interactive evaluation is beyond the scope of this article,\nand we focus on static score functions.\n4.3 K-Nearest neighbor and random walks\nAnother simple strategy for playlist generation is to construct\nak-nearest-neighbor ( kNN) graph over the song set by using\nsome previously constructed distance metric ( e.g., acoustic,\nsemantic, or social similarity), and form playlists by a ran-\ndom walk process on the graph. If the next song xt+1is\nchosen uniformly at random from the neighbors Î·(xt)of the\ncurrent song xt, then the bigram probability is\nPkNN[Xt+1=x|Xt=xt] =/braceleftBigg\n1/kxâˆˆÎ·(xt)\n0x /âˆˆÎ·(xt).(5)\nOne shortcoming of this approach â€” as well as any deter-\nministic playlister â€” is that it assigns 0 probability to some\ntransitions, in this case, those spanning non-adjacent nodes.\nAny such transition would be inï¬nitely unlikely under the\nmodel; however, it seems unreasonable to expect that every\nobserved bigram coincides with an edge in the graph (unless\n539Poster Session 4\nthe graph is complete). This can be remedied by smoothing\nwith the uniform distribution (weighted by a constant Âµ):\n/hatwidePkNN= (1âˆ’Âµ)PkNN+ÂµPUÂµâˆˆ(0,1].(6)\nSince probability distributions are closed under convex com-\nbinations, Eqn. (6) describes a valid distribution. Equiva-\nlently, this models a process which ï¬‚ips a Âµ-biased coin to\ndecide whether to jump to an adjacent song in the graph, or a\nrandom song in the library (adjacent or not). This modiï¬ca-\ntion to the algorithm increases diversity and ï¬‚exibility, and\nensures that log-likelihood computations remain ï¬nite.\n4.4 Markov chain mixtures\nAny non-trivial playlister requires some tuning of parameters.\nFor example, to implement kNN, one must select the under-\nlying features and similarity metric, the neighborhood size\nk, and the smoothing parameter Âµ. This leads to an obvious\nquestion: can these parameters be optimized automatically?\nMore generally, if we start with a collection of playlisters\nAi(say, derived from different features [10, 12], values of\nk,etc.), is it possible to intelligently integrate them to into a\nsingle playlister?\nEqn. (6) exploits the fact that distributions are closed\nunder convex combinations to combine two distributions\n(uniform and kNN) with ï¬xed proportion Âµ. This can be\ngeneralized to combine mdistributions as follows:\nPÂµ=m/summationdisplay\ni=1ÂµiPiâˆ€i:Âµiâ‰¥0,m/summationdisplay\ni=1Âµi= 1. (7)\nRather than using a ï¬xed weighting Âµ= (Âµ1,Âµ2,...,Âµm),\nwe can instead optimize Âµby maximizing the likelihood of\na collection of training examples under the mixture model.\nThis can be accomplished by solving the optimization prob-\nlem listed as Algorithm 1. Because the objective function\n(log-likelihood) is concave in Âµ, and the constraints are linear,\nthis problem can be solved efï¬ciently [5].\nAfter computing the maximum likelihood estimate Âµ,\nplaylists can be generated by sampling from the weighted\nensemble distribution PÂµ. The distribution described by\nEqn. (7) characterizes the ensemble playlist algorithm listed\nas Algorithm 2, which, given the current song xt, simply\nselects a playlister Aiat random according to the discrete\ndistribution characterized by Âµand returns a sample from the\nselected distribution Pi[X|Xt=xt].\n5. EXPERIMENTS\nTo demonstrate the proposed evaluation approach, we im-\nplemented several playlisters on a large song library, us-\ning acoustic-, semantic-, and popularity-based descriptors.\nThe simple playlisters described here are merely intended to\ndemonstrate plausible baselines against which more sophisti-\ncated algorithms may be compared in future work.Algorithm 1 Markov chain mixture optimization\nInput: Training bigrams\nS/prime={(x1,x/prime\n1),..., (x|S/prime|,x/prime\n|S/prime|)}\nMarkov chains P1,P2,...,Pm\nOutput: Combination weights Âµ1,Âµ2,...,Âµm\nmax\nÂµ1\n|S/prime|/summationdisplay\n(x,x/prime)âˆˆS/primelog/parenleftBiggm/summationdisplay\ni=1ÂµiPi[Xt+1=x/prime|Xt=x]/parenrightBigg\ns.t.âˆ€i:Âµiâ‰¥0,m/summationdisplay\ni=1Âµi= 1\nAlgorithm 2 Ensemble playlist generation\nInput: Current song xt, playlisters (Ai,Pi), weightsÂµi\nOutput: Next songxt+1\n1:Sampleiâˆ¼DISCRETE (Âµ){ChooseAi}\n2:returnxt+1âˆ¼Pi[Xt+1|Xt=xt]{RunAi(xt)}\n5.1 Song data: Million Song Dataset\nOur song data was taken from the Million Song Dataset\n(MSD) [3], upon which we constructed models based on\nartist terms (tags), familiarity, and audio content.\nTag representations were derived from the vocabulary of\n7643 artist terms provided with MSD. Each song is repre-\nsented as a binary vector indicating whether each term was\napplied to the corresponding artist, and nearest neighbors are\ndetermined by cosine-similarity between tag vectors.\nThe Echo Nest3artist familiarity is used to deï¬ne a static\nscore function Fover songs, which may be interpreted as a\nsurrogate for (average) user preference.\nThe audio content model was developed on the 1% Mil-\nlion Song Subset (MSS), and is similar to the model proposed\nin [17]. From each MSS song, we extracted the time series\nof Echo Nest timbre descriptors (ENTs). This results in a\nsample of approximately 8.5 million 12-dimensional ENTs,\nwhich were normalized by z-scoring according to the esti-\nmated mean and variance of the sample, randomly permuted,\nand then clustered by online k-means to yield 512 acoustic\ncodewords. Each song was summarized by quantizing each\nof its (normalized) ENTs and counting the frequency of each\ncodeword, resulting in a 512-dimensional histogram vector.\nEach codeword histogram was mapped into a probability\nproduct kernel (PPK) space [11] by square-rooting its entries,\nwhich has been demonstrated to be effective on similar audio\nrepresentations [17]. Finally, we appended the songâ€™s tempo,\nloudness, and key conï¬dence, resulting in a vector viâˆˆR515\nfor each song xi.\nNext, we trained an optimized similarity metric over audio\ndescriptors. We computed target similarity for each pair of\nMSS artists by the Jaccard index between their user sets in\n3http://developer.echonest.com\n54012th International Society for Music Information Retrieval Conference (ISMIR 2011)\na sample of Last.fm4collaborative ï¬lter data [6, chapter 3].\nTracks by artists with fewer than 30 listeners were discarded.\nThe remaining artists were partitioned 80/20 into a training\nand a validation set, and for each artist, we computed its\ntop 10 most similar training artists. The distance metric was\nsubsequently optimized by applying the metric learning to\nrank (MLR) algorithm on the training set of 4455 songs,\nand tuning parameters Câˆˆ{105,106,..., 109}andâˆ†âˆˆ\n{AUC, MRR, MAP, Prec@10 }to maximize AUC score on\nthe validation set of 1110 songs. Finally, the resulting metric\nWwas factored by PCA (retaining 95% of spectral mass) to\nyield a linear projection LâˆˆR222Ã—515which maps each vi\ninto a Euclidean space in which nearest neighbor is optimized\nto retrieve songs by similar artists.\n5.2 Playlists: Art of the Mix\nPlaylist data was taken from the Art of the Mix5(AotM)\ncorpus collected by Berenzweig, et al. [2]. We chose this\ncorpus primarily for two reasons. First, it is the largest pub-\nlicly available set that we know of. Second, each playlist\nwas (ostensibly) generated by a user â€” not a recommenda-\ntion service or commercial radio DJ â€” so the corpus is an\naccurate sample of real playlists that occur in the wild.6\nThe AotM data consists of approximately 29K playlists\nover 218K unique songs by 48K unique artists, which we\ncleaned with a two-step procedure. First, artist names were\nresolved to identiï¬ers by the Echo Nest artist search API.\nSecond, we matched each songâ€™s artist identiï¬er to the MSD\nindex, and if the artist was found, we matched the title against\nall MSD song titles by the artist. A match was accepted if\neither title was contained in the other, or the edit distance\nwas less than half the (AotM) title length. This was found\nby informal inspection to yield fewer false matches than a\ndirect ( artist ,title) query to the Echo Nest API.\nHaving resolved songs to MSD identiï¬ers, we then ï¬ltered\nthe playlist set down to bigrams in which both consecutive\nsongs were contained in MSD. This results in a collection S\nof 66250 bigrams over a library Xof 26752 unique songs by\n5629 unique artists.7\n5.3 Experimental procedure\nFor each song xiâˆˆX, we computed an optimized acoustic\ndescriptorviâˆˆR222, tag vectorwiâˆˆ{0,1}7643, and artist\nfamiliarity score F(xi)âˆˆ[0,1]. The familiarity score was\nused to construct a weighted shufï¬‚e Markov chain ( Eqn. (4) ).\nThe audio and tag spaces were used to generate kNN Markov\n4http://last.fm\n5http://www.artofthemix.org/\n6One could of course model playlists derived from alternative sources,\nbut be aware that such playlists may have different characteristics than user-\ngenerated playlists: e.g., terrestrial radio playlists may be constrained by\nbroadcast regulations or commercial factors.\n7The bigram data and example playlists for each algorithm\ncan be downloaded from http://www-cse.ucsd.edu/ Ëœbmcfee/\nplaylists/ .\nAudio k=16  \n32  \n64  \n128 \n256 \n512 \n1024\n2048\n4096\nTags k=16  \n32  \n64  \n128 \n256 \n512 \n1024\n2048\n4096\nUniform\nFamiliarity\nUnweighted\nOptimized\nâˆ’15 âˆ’14 âˆ’13 âˆ’12 âˆ’11 âˆ’10 âˆ’9\nAverage logâˆ’likelihoodâˆ’14.72\nâˆ’14.69\nâˆ’14.63\nâˆ’14.55\nâˆ’14.42\nâˆ’14.22\nâˆ’13.92\nâˆ’13.50\nâˆ’12.89\nâˆ’14.04\nâˆ’13.66\nâˆ’13.43\nâˆ’13.33\nâˆ’13.25\nâˆ’13.12\nâˆ’12.92\nâˆ’12.63\nâˆ’12.21\nâˆ’10.19\nâˆ’10.13\nâˆ’10.42\n âˆ’9.62Figure 1 . Average log-likelihood of test bigrams for each\nmodel under comparison. Scores are averaged across ten\nrandom training/test splits.\nchains ( Eqn. (5) ) forkâˆˆ{24,25,..., 212}. This results in a\ncollection of 9 audio-based Markov chains, 9 tag-based, and\none familiarity-based. Including the uniform shufï¬‚e model,\nwe have a total m= 20 simple playlisters.\nThe playlist setSwas randomly partitioned 10 times\ninto 10%-train, 90%-test sets; each split was performed over\nthe ï¬rst element of the bigram so that for each song xi, all\nbigrams (xi,Â·)belong to either the training or test set. On\naverage, this yields 6670.9 training and 59597.1 test bigrams.\nEach simple playlister was evaluated by computing the\naverage log-likelihood of test bigrams (x,x/prime)(Eqn. (1) ). All\nplaylisters were smoothed by Eqn. (6) with Âµ= 0.01.\nWe then ran Algorithm 1 on the training set, and evaluated\nthe resulting playlister on the test set. Our implementation\nof Algorithm 1 is written in NumPy,8and on average, con-\nverges to the global optimum in under 20 seconds on standard\nhardware. Since the ensemble includes the uniform model,\nno additional smoothing is necessary. Finally, for comparison\npurposes, we also compared to the unweighted combination\nby ï¬xing each Âµi=1/m.\n5.4 Results\nFigure 1 lists the average log-likelihood of each model un-\nder comparison. Although the audio- and tag-based models\ntend to generate playlists which are acoustically or seman-\ntically consistent,7they do not accurately model naturally\noccurring playlists. As illustrated in Figure 2 , the majority\nof bigrams disagree with adjacencies in the kNN graphs, so\nkNN methods are outperformed by uniform shufï¬‚e. While\nthe features described here do not sufï¬ce to model naturally\noccurring playlists, a richer feature set including lyrical or\nsocial information may signiï¬cantly improve performance,\nand will be the subject of future research.\nFor small values of k, the tag playlister is forced to select\n8http://numpy.scipy.org\n541Poster Session 4\n0.1% 1% 10%00.10.20.30.4\nk / nBigram %\n  \nAudio kâˆ’NN\nTag kâˆ’NNFigure 2 . Fraction of bigrams (x,x/prime)âˆˆS wherex/primeis a\nk-nearest neighbor of x(as a function of k/n).\nAudio Tags Familiarity Uniform\nÂµ 9% 27% 36% 28%\nTable 1 . Average weight assigned to each model when opti-\nmized by Algorithm 1. Audio andTagweights are aggregated\nacross all values of kâˆˆ{24,25,..., 212}.\namong songs with highly similar tag vectors. Tag-based\nplaylisters, therefore, tend to maximize semantic cohesion.\nThe relatively low performance of the tag playlister indicates\nthat semantic cohesion does not adequately describe naturally\noccurring playlists.\nThefamiliarity model performs slightly better than uni-\nform, and signiï¬cantly better than the audio and tag playlis-\nters. This suggests that popularity and social factors play\nsigniï¬cant roles in playlist composition; while not surprising,\nthis should be taken into account when designing a playlister.\nThe optimized model produced by Algorithm 2 substan-\ntially outperforms all other models, even when only exposed\nto an extremely small training set (10%). Note that the un-\nweighted combination degrades performance.\nTo help understand contributions of different components\nin the optimized model, we list the average weight assigned\nto each model by Algorithm 1 in Table 1 , grouped by feature\ntype. The content-based models receive a signiï¬cant amount\nof weight, suggesting that the models contain some amount\nof predictive power. The large weight assigned to the uniform\nmodel may be interpreted as the proportion of information\nnot modeled by content or familiarity, and thus constitutes\na secondary measure of the (lack of) quality of the other\nmodels in the ensemble.\n6. CONCLUSION\nWe have presented a simple, automatic evaluation procedure\nfor playlist algorithms. To demonstrate the technique, we\ndeveloped a suite of simple baseline playlisters, and evaluated\ntheir performance on naturally occurring playlists.\n7. ACKNOWLEDGMENTS\nThe authors thank Benjamin Fields and Matthew Hoffman\nfor many helpful conversations, and acknowledge supportfrom Qualcomm, Inc., Yahoo! Inc., the Hellman Fellowship\nProgram, and NSF Grants CCF-0830535 and IIS-1054960.\n8. REFERENCES\n[1]Luke Barrington, Reid Oda, and G.R.G. Lanckriet. Smarter than\ngenius? Human evaluation of music recommender systems. In\nISMIR , 2009.\n[2]A. Berenzweig, B. Logan, D.P.W. Ellis, and B. Whitman.\nA large-scale evaluation of acoustic and subjective music-\nsimilarity measures. CMJ , 28(2):63â€“76, 2004.\n[3]Thierry Bertin-Mahieux, Daniel P.W. Ellis, Brian Whitman,\nand Paul Lamere. The million song dataset. In ISMIR , 2011.\n[4]K. Bosteels, E. Pampalk, and E.E. Kerre. Evaluating and\nanalysing dynamic playlist generation heuristics using radio\nlogs and fuzzy set theory. In International Conference on Music\nInformation Retrieval , 2009.\n[5]Stephen Boyd and Lieven Vandenberghe. Convex Optimization .\nCambridge University Press, 2004.\n[6] O. Celma. Music Recommendation and Discovery in the Long\nTail. Springer, 2010.\n[7]M. Dopler, M. Schedl, T. Pohle, and P. Knees. Accessing music\ncollections via representative cluster prototypes in a hierarchical\norganization scheme. In ISMIR , 2008.\n[8]B. Fields. Contextualize Your Listening: The Playlist as Rec-\nommendation Engine . PhD thesis, Goldsmiths, University of\nLondon, April 2011.\n[9]B. Fields, C. Rhodes, and M. dâ€™Inverno. Using song social tags\nand topic models to describe and compare playlists. Workshop\non Music Recommendation and Discovery , 2010.\n[10] Ben Fields, Christophe Rhodes, Michael Casey, and Kurt Jacob-\nsen. Social playlists and bottleneck measurements: Exploiting\nmusician social graphs using content-based dissimilarity and\npairwise maximum ï¬‚ow values. In ISMIR , 2008.\n[11] Tony Jebara, Risi Kondor, and Andrew Howard. Probability\nproduct kernels. JMLR , 5:819â€“844, Dec 2004.\n[12] P. Knees, T. Pohle, M. Schedl, and G. Widmer. Combining\naudio-based similarity with web-based data to accelerate auto-\nmatic music playlist generation. In ACM international work-\nshop on multimedia information retrieval , 2006.\n[13] B. Logan. Content-based playlist generation: exploratory ex-\nperiments. In ISMIR , 2002.\n[14] B. Logan. Music recommendation from song sets. In ISMIR ,\n2004.\n[15] F. Maillet, D. Eck, G. Desjardins, and P. Lamere. Steerable\nplaylist generation by learning song similarity from radio sta-\ntion playlists. In ISMIR , 2009.\n[16] C.D. Manning and H. Sch Â¨utze. Foundations of Statistical Natu-\nral Language Processing . MIT Press, 1999.\n[17] B. McFee, L. Barrington, and G.R.G. Lanckriet. Learn-\ning content similarity for music recommendation, 2011.\nhttp://arxiv.org/1105.2344.\n[18] S. Pauws and B. Eggen. PATS: Realization and user evaluation\nof an automatic playlist generator. In ISMIR , 2002.\n[19] J.C. Platt, C.J.C. Burges, S. Swenson, C. Weare, and A. Zheng.\nLearning a gaussian process prior for automatically generating\nmusic playlists. In NIPS . MIT Press, 2002.\n[20] M. Slaney and W. White. Measuring playlist diversity for rec-\nommendation systems. In 1st ACM workshop on Audio and\nmusic computing multimedia , AMCMM â€™06, pages 77â€“82, New\nYork, NY , USA, 2006. ACM.\n[21] E. Zheleva, J. Guiver, E. Mendes Rodrigues, and N. Mili Â´c-\nFrayling. Statistical models of music-listening sessions in social\nmedia. In WWW , 2010.\n542"
    },
    {
        "title": "A Musical Web Mining and Audio Feature Extraction Extension to The Greenstone Digital Library Software.",
        "author": [
            "Cory McKay",
            "David Bainbridge 0001"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1415006",
        "url": "https://doi.org/10.5281/zenodo.1415006",
        "ee": "https://zenodo.org/records/1415006/files/McKayB11.pdf",
        "abstract": "This paper describes updates to the Greenstone open source digital library software that significantly expand its functionality with respect to music. The first of the two major improvements now allows Greenstone to extract and store classification-oriented features from audio files using a newly updated version of the jAudio software. The second major improvement involves the implementation and integration of the new jSongMiner software, which provides Greenstone with a framework for automatically identifying audio recordings using audio fingerprinting and then extracting extensive metadata about them from a variety of resources available on the Internet. Several illustrative use cases and case studies are discussed.",
        "zenodo_id": 1415006,
        "dblp_key": "conf/ismir/McKayB11",
        "keywords": [
            "Greenstone",
            "digital library",
            "music",
            "classification-oriented features",
            "audio files",
            "jAudio",
            "jSongMiner",
            "audio fingerprinting",
            "metadata",
            "Internet"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\n  \n \nA MUSICAL WEB MINING AND AUDIO FEATURE \nEXTRACTION EXTENSION TO THE GREENSTONE DIGITAL \nLIBRARY SOFTWARE \nCory McKay David Bainbridge \nMarianopolis College and CIRMMT \nMontrÃ©al, Canada \ncory.mckay@mail.mcgill.ca  University of Waikato \nHamilton, New Zealand \ndavidb@cs.waikato.ac.nz  \nABSTRACT \nThis paper describes updates to the Greenstone open  source \ndigital library software that significantly expand its func- \ntionality with respect to music. The first of the t wo major \nimprovements now allows Greenstone to extract and s tore \nclassification-oriented features from audio files u sing a \nnewly updated version of the jAudio software. The s econd \nmajor improvement involves the implementation and i nte- \ngration of the new jSongMiner software, which provi des \nGreenstone with a framework for automatically ident ifying \naudio recordings using audio fingerprinting and the n ex- \ntracting extensive metadata about them from a varie ty of \nresources available on the Internet. Several illust rative use \ncases and case studies are discussed. \n1.  INTRODUCTION \nUsers of modern digital music collections benefit f rom \nmany advantages relative to users of even a decade ago. \nAmongst the greatest of these advantages is cheap a nd con- \nvenient access to diverse and rich on-line sources of musi- \ncal data and metadata. Of particular convenience to  re- \nsearchers and programmers, many on-line sources pro vide \naccess to their data through convenient web service  APIs. \nSuch resources include The Echo Nest, Last.FM, Mu- \nsicBrainz, Amazon, Yahoo! and many others. \nIt is also possible to extract features directly fr om both \naudio and symbolic musical representations. The res ulting \nfeature values can then simply be stored directly a s part of \ndigital music collections. Alternatively, these fea tures can \nbe processed using data mining techniques in order to ar- \nrive at additional metadata, such as class labels o r links to \nother musical entities. \nIt is necessary to overcome certain important chall enges in order to effectively take advantage of the plent iful data \nand metadata that is available, however. One must f ind ef- \nficient and effective ways of automatically accessi ng and \nintegrating information about a given music collect ion from \nthe diverse and often inconsistent on-line resource s; one \nmust ensure that proper identifiers are used to uni quely re- \nfer to the individual entities about which informat ion is ac- \ncessed (e.g. recordings, albums, musicians, etc.), even when \nthe different resources from which data is extracte d may \nidentify entities in entirely different ways; one m ust filter \nout noisy or inaccurate information, which can be a  signifi- \ncant problem when dealing with much of the musical data \nthat is available on-line; one must structure acqui red data so \nthat it can be queried and otherwise accessed in wa ys that \nare consistent and meaningful; and one must make th e data \naccessible to users in ways that are convenient to them in a \nvariety of use cases. \nThis paper presents an upgrade to the well-establis hed \nand open-source Greenstone Digital Library software  [10] \nthat is intended to address these issues. This upgr ade dra- \nmatically expands Greenstoneâ€™s ability to collect m usical \ninformation and make it conveniently available to u sers. \nPart of this upgrade includes the integration of pa rts of the \njMIR [8] music information retrieval software into Green- \nstone, specifically jAudio [7,8], which allows cont ent-based \nfeatures to be extracted from audio recordings.  \nThe second major component of the Greenstone upgrad e \nis the creation and integration of the new jSongMin er soft- \nware, which provides a framework for automatically ac- \nquiring and structuring many types of metadata from  di- \nverse sources of information about music, including  both \non-line resources and metadata embedded in files. T his \nsoftware is highly configurable, in order to meet t he needs \nof a wide variety of different user types. It is al so specifi- \ncally designed to be easily extensible so that diff erent kinds \nof information can be extracted from different data  sources \nas they become available. \nSo, given a set of musical recordings of interest, users \ncan now have Greenstone automatically identify unkn own  \nPermission to make digital or hard copies of all or  part of this work for \npersonal or classroom use is granted without fee pr ovided that copies are \nnot made or distributed for profit or commercial ad vantage and that cop- \nies bear this notice and the full citation on the f irst page.  \nÂ© 2011 International Society for Music Information Retrieval  \n459Poster Session 3\n  \n \nrecordingsâ€”or verify the identity of labelled recor dingsâ€”\nusing audio fingerprinting, extract a wide variety of \nmetadata from different on-line sources related to each re- \ncording, extract content-based features from each r ecording \nand extract any metadata embedded in the tags of ea ch re- \ncording. All of this data is then automatically int egrated, \nstructured and saved. \nUsers may then take advantage of Greenstoneâ€™s estab - \nlished interface to organize, browse or search the newly-\nbuilt music collection. They may also use the Green stone \ninterface to further annotate or edit the collectio n if desired. \nThe musical data can also be published and maintain ed us- \ning Greenstoneâ€™s many existing tools and features. \n2.  RELATED RESEARCH \nThere are a number of software packages for buildin g digi- \ntal libraries that can serve as alternatives to Gre enstone, \nincluding both commercial and open source systems. Ex- \namples of the latter include DAITSS, DSpace, EPrint s, Fe- \ndora and Keystone DLS. Marill and Lucza provide a d is- \ncussion of their comparative merits [6]. Although m any of \nthese are excellent products, Greenstone has the pa rticular \nadvantage of a longstanding association with MIR re search \ndating to the beginnings of the ISMIR conference. \nThere are also a number of audio feature extraction  \npackages available that may be used as alternatives  to jAu- \ndio, including Marsyas [9], MIRtoolbox [5] and Soni c Vis- \nualiser [4]. Although these are all excellent syste ms, jAudio \nhas the special advantage of combining an easily ex tensible \nplug-in architecture for adding new features (as do es Sonic \nVisualiser) with a cross-platform Java implementati on.  \nThere are also a few existing software platforms fo r min- \ning a variety of Internet resources, such as Mozend a [15], \nand related research on integrating metadata is als o being \ndone in the semantic desktop community (e.g. NEPOMU K \n[17]). To the best of the authoursâ€™ knowledge, howe ver, \njSongMiner is the only such software focusing speci fically \non music, and has the essential advantages of being  both \nopen source and specifically designed for integrati ng ex- \ntracted data with digital repository software like Green- \nstone. The closest existing software is jMIRâ€™s jWeb Miner \n[8], which focuses on extracting statistically-deri ved nu- \nmerical features from the Internet, rather than the  raw \nmetadata mined by jSongMiner. \n3.  GREENSTONE \nGreenstone [10] is an open-source and multilingual soft- \nware suite for building and distributing digital li brary col- \nlections. A particular emphasis has been placed on promot- \ning digital libraries in developing countries and i n \nUNESCOâ€™s partner communities and institutions. Alt-hough Greenstone is intended for library collection s that \ncan consist of a wide and heterogeneous range of ma terials, \nnot just music, it has certainly effectively been a pplied to \nmusical collections in the past (e.g. in [2] and [3 ]).  \nA Greenstone library consists of one or more collec tions. \nThese can each store many different types of docume nts, \nsuch as HTML files, PDFs, images, videos, audio fil es, \nMIDI files, etc. Each such document can be annotate d with \nmetadata tags, which can in turn be used to index, browse, \nsearch or otherwise organize or process a collectio n. \nGiven a set of documents, Greenstone can automatica lly \nbuild and link a collection, a process that can inc lude the \nautomated extraction of metadata as well as the cre ation of \nnew documents. Greenstone comes packaged with a var iety \nof such metadata extractors for different types of docu- \nments, and can be extended with document plugins  for ad- \nditional document types, as has been done here with  jAudio \nand jSongMiner. For example, Greenstone can apply t he \nCANTOR [1] optical music recognition tool to scans of \nscores as they are added to collections in order to  automati- \ncally generate symbolic representations of the musi c. \nUsers can also use Greenstone to manually annotate re- \nsources with metadata using the librarianâ€™s interface . \nGreenstone collections can also be easily and autom atically \nexpanded by adding new documents to them. \nGreenstone can publish digital libraries either to the In- \nternet or to physical media such as CD-ROMs. The la tter \noption is particularly important when working to ma ke digi- \ntal libraries accessible in locations where network  access is \nlimited or unavailable, such as in developing count ries. The \nparticular metadata fields that are published, as w ell as how \nthey are formatted, are both highly configurable. \nThe Greenstone software and sample collections can be \naccessed at www.greenstone.org. \n4.  JMIR \njMIR [8] is a suite of software tools and other res ources \ndeveloped for use in automatic music classification  re- \nsearch. jMIR includes the following components: \nâ€¢ jAudio:  Extracts features from audio files. \nâ€¢ jSymbolic:  Extracts features from symbolic music files. \nâ€¢ jWebMiner 2.0:  Extracts statistical features from cul- \ntural and listener information available on the Int ernet. \nâ€¢ jLyrics:  Extracts features from lyric transcriptions. \nâ€¢ ACE 2.0: A metalearning-based automatic classifica- \ntion engine. \nâ€¢ jMusicMetaManager: Software for managing and de- \ntecting errors in musical datasets and their metada ta.  \nâ€¢ lyricFetcher: Mines lyrics from the Internet.  \nâ€¢ jMIRUtilities: Performs infrastructural tasks.  \nâ€¢ ACE XML: Standardized MIR file formats.  \n46012th International Society for Music Information Retrieval Conference (ISMIR 2011)\n  \n \nâ€¢ Codaich, Bodhidharma MIDI and SAC/SLAC:  Mu- \nsical research datasets. \nAll of the jMIR components emphasize extensibility,  \nand they may be used both individually and as integ rated \ngroups. All jMIR components are open-source and are  dis- \ntributed free-of-charge at jmir.sourceforge.net. \n5.  EXTRACTING FEATURES FROM AUDIO \nDOCUMENTS IN GREENSTONE \nAs noted above, jAudio [7,8] is a jMIR component th at ex- \ntracts content-based features from audio files. A n ew jAu- \ndio Greenstone plugin has been implemented so that \nGreenstone can now automatically run jAudio to extr act \nand store features from each audio file added to a Green- \nstone collection. jAudio itself has also been updat ed and \nexpanded in order to make it easier to install and use, and \nto expand the range of codecs that it can use. \nOne way to take advantage of the features extracted  by \njAudio is to simply use them as descriptors, just l ike any \nother Greenstone metadata, something that can be pa rticu- \nlarly useful for higher-level features than have an  explicit \nmusical meaning. The extracted features may also be  pro- \ncessed by classification softwareâ€”such as jMIR ACE \n[8]â€”in order to arrive at still further metadata la bels that \ncan themselves be stored, such as content-derived p redic- \ntions of labels like genre, mood, artist, etc. \njAudio can extract features from a variety of audio  file \nformats, including MP3, FLAC, WAV, AIFF and AU. It is \ndistributed with 28 base implemented features, incl uding \nboth low-level features (e.g. spectral flux and spe ctral cen- \ntroid) and higher-level features (e.g. rhythmic fea tures de- \nrived from beat histograms). This number of extract ed fea- \ntures can be dramatically expanded at runtime, as j Audio \nincludes metafeatures  and aggregators  [7,8] that can be \nused to automatically derive further features from base fea- \ntures, such as the standard deviation, rate of chan ge or av- \nerage of a given feature across a set of analysis w indows. \nIn addition, one of the most important advantages o f \njAudio is that it is a relatively simple matter to add newly \ndeveloped features using jAudioâ€™s plugin interface,  without \nthe need to recompile jAudio (or Greenstone). jAudi o is \nalso highly configurable, so users can decide which  features \nto extract, whether or not to apply pre-processing like nor- \nmalization or downsampling, etc. \nOnce features are extracted, they can simply be sto red \ndirectly in the Greenstone collection metadata. The y can \nalso be exported as ACE XML [8] or Weka ARFF [11] \nfiles for external processing if desired. 6.  USING JSONGMINER TO MINE METADATA  \nAs noted above, jSongMiner is a novel software pack age \nthat provides a framework for extracting metadata a bout \nmusical entities from resources available on the In ternet. \nAlthough it has been designed in the specific conte xt of \nGreenstone, jSongMiner has been implemented such th at it \ncan also be used as a stand-alone application if de sired, or \nused in conjunction with other jMIR components.  \njSongMiner begins by identifying unknown audio file s \nusing audio fingerprinting (The Echonestâ€™s [14] fin gerprint- \ning services are used by default). jSongMiner can a lso iden- \ntify recordings using metadata that is embedded in audio \nfiles or that is manually specified. \nOnce jSongMiner has identified a recording, it then  ex- \ntracts metadata about it from APIs offered by vario us on-\nline sources, or from metadata embedded in the audi o file. \njSongMiner keeps a record of resource identifiers i n as \nmany namespaces as possible while doing this, thus facili- \ntating the integration of information from differen t sources.  \nIn addition to collecting metadata about songs, jSo ng- \nMiner can also automatically acquire metadata about  artists \nand albums associated with songs. So, if given an u nidenti- \nfied song, jSongMiner will first identify it using audio fin- \ngerprinting, and then extract all available metadat a on this \nsong from all of the on-line resources that it has access to. \nIf this metadata includes artist and/or album ident ifiers, \nthen all available fields will also be extracted fo r this artist \nand/or album as well. In order to avoid redundant q ueries, \njSongMiner can be set to only extract metadata on a lbums \nand artists for which it has not already extracted metadata. \njSongMiner thus allows users to treat songs, artist s and \nalbums as separate resource types, and allows infor mation \nto be extracted and saved independently for each of  them, \nwhilst at the same time maintaining information out lining \nthe connections between resources of the same and d iffer- \nent types. Users also have the option of packaging artist and \nalbum metadata together with song metadata if they prefer. \nOnce metadata has been extracted relating to a song , art- \nist and/or album, this metadata can be saved as an ACE \nXML [8] file or as a return-delimited text file. In  the con- \ntext of Greenstone, the jSongMiner Greenstone plugi n al- \nlows all acquired data and metadata to be automatic ally in- \ncorporated into Greenstoneâ€™s internal data structur es. In any \nof these cases, jSongMiner allows the storage of me tadata \ncontaining diverse character sets. \nEach piece of metadata extracted by jSongMiner in- \ncludes the field label, the metadata value and an i dentifier \nfor the source from which the metadata was collecte d. The \nfield labels are standardized, so that a given type  of infor- \nmation will always be assigned the same field name by \njSongMiner, regardless of where it is acquired from . For \n461Poster Session 3\n  \n \nexample, jSongMiner will place the title of a song in the \nâ€œSong Titleâ€ field, regardless of whether one data source \nmight refer to it as â€œSong Nameâ€ and another as â€œTi tleâ€.  \nThe ability to identify the source of each piece of  \nmetadata is also important, as different sources mi ght sup- \nply different results for a given field. For exampl e, one \nsource might identify the artist associated with a song as \nâ€œCharles Mingusâ€, and another might specify â€œCharli e \nMingusâ€. For this reason, jSongMiner allows multipl e re- \nsults for the same field to be extracted and stored  in paral- \nlel. If the metadata is cleaned at some later point , the cor- \nrection algorithm (or person) can be defined as a n ew \nsource, and the original uncleaned metadata can be main- \ntained or deleted, as desired. All of this means th at jSong- \nMiner organizes metadata from diverse sources in a struc- \ntured and consistent way, whilst at the same time a llowing \nany idiosyncrasies and subtleties implicit in the o riginal da- \nta sources to be maintained and referenced if desir ed.  \njSongMinerâ€™s ability to store multiple values for a  given \nmetadata field, from the same or different sources,  also \nhelps to make it possible to move beyond simple fla t data \nstructuring. This is enhanced by jSongMinerâ€™s (and ACE \nXMLâ€™s) ability to link to external resources (inclu ding RDF \nontologies) via metadata field entries, as well as by the way \nin which jSongMiner treats songs, artists and album s as dis- \ntinct but linked entities. \nUsers can opt to have extracted metadata presented using \nunqualified or qualified Dublin Core [13] tags. In order to \nmake this possible, jSongMiner includes original Du blin \nCore schemas. This use of Dublin Core can be partic ularly \nuseful from a librarianâ€™s perspective. \nThe primary objective of jSongMiner is to provide a  \ngeneral framework that users can extend to incorpor ate \nwhatever web services and data sources they wish. I t was \nconsciously decided not to design jSongMiner as a f rame- \nwork linked to any specific web services, as APIs c hange, \nweb services go off-line and new ones appear. Furth ermore, \neach on-line resource has its own terms of service potential- \nly limiting which and how much data can be accessed  and \nstored. A strong emphasis was therefore placed on d esign- \ning jSongMiner in a modular way that allows it to b e easily \nextended so that it can be used with arbitrary data  sources, \nrather than biasing its architecture towards the AP Is of any \nparticular data sources. \nSo, one of the primary advantages of jSongMiner is the \nway in which it provides the basic extensible frame work for \nincorporating functionality for accessing particula r web \nservices. Furthermore, it standardizes the ways tha t extract- \ned metadata is labelled, structured and made access ible. \nHaving noted this, the decision was made to impleme nt \nfunctionality for accessing data made available thr ough the \nEcho Nest [14] and Last.FM [12] APIs, two of the ri chest sources of on-line metadata at the time of this wri ting. This \nwas done primarily as a proof of concept and to mak e \njSongMiner immediately useful out of the box. \nUsing the Echo Nest and Last.FM web services, jSong - \nMiner can currently extract over one hundred song, artist \nand album metadata fields. In addition, many of the se fields \ncan have multiple values. For example, there will u sually \nbe multiple artists listed in the â€œSimilar Artistâ€ field. \nThe jSongMiner fields range from standard musical \nfields (e.g. â€œSong Titleâ€ or â€œGenreâ€) to primary ke ys (e.g. \nâ€œEcho Nest Song IDâ€ or â€œMusic Brainz Artist IDâ€) to  con- \ntent-based information (e.g. â€œDuration (seconds)â€ o r â€œTem- \npo (BPM)â€) to consumption-based data (e.g. â€œLast.FM  \nTrack Play Countâ€ or â€œEcho Nest Artist Hotness (0 t o 1)â€) \nto links to external textual data (e.g. â€œArtist-Rel ated Blogâ€ \nor â€œLast.FM Album Wiki Textâ€) to links to multimedi a \n(e.g. â€œArtist-Related Imageâ€ or â€œArtist-Related Vid eoâ€). \nIn order to make jSongMiner as flexible as possible , the \nsoftware is highly customizable in terms of what ki nds of \ninformation are extracted, where it is extracted fr om and \nhow the data is structured. Such options can be set  through \njSongMinerâ€™s configuration files and its command li ne.  \nEvery effort has been made to make jSongMiner as ea sy \nto use as possible, with ample documentation in the  manu- \nal, so even users with only moderate computer backg rounds \nshould still have relatively little difficulty usin g the soft- \nware. In addition to including a command line and c onfigu- \nration file-based interface that makes the jSongMin er easy \nto run from other software, jSongMiner also has a w ell-\ndocumented API in order to facilitate the use of jS ong- \nMiner as a library incorporated into other software . \nIf the jSongMiner Greenstone plugin is being used, then \nthe user never needs to interact with jSongMiner di rectly \nwhile using Greenstone. The plugin simply has jSong Miner \nperform tasks in the background, and data it extrac ts is au- \ntomatically structured and linked within the collec tion pro- \nduced by Greenstone. jSongMiner configuration setti ngs \ncan also be specified within the Greenstone interfa ce. \nLike all jMIR components, jSongMiner is cross-\nplatform, open-source and available for free at \njmir.sourceforge.net. \n7.  USE CASES AND CASE STUDIES \nGreenstone is designed to be used for a variety of different \nmusical purposes by a variety of user types. This s ection \nbriefly describes a few of the many possible use ca ses. \nMIR researchers, especially those specializing in m usic \nclassification, are the first user group that will be consid- \nered. Such researchers often have a need for datase ts that \ncan be used to evaluate and compare algorithms. The se da- \ntasets should ideally also be well-annotated with m etadata \n46212th International Society for Music Information Retrieval Conference (ISMIR 2011)\n  \n \nthat can, among other things, serve as class labels . Green- \nstone could be used by those building MIR research da- \ntasets not only to harvest rich metadata about thei r music \nfiles, but also to export and publish information a bout the \ndataset to the web as linked HTML that other resear chers \ncould search and browse when choosing a dataset to use in \ntheir own research. It should be emphasized that Gr een- \nstoneâ€™s ability to extract content-based features i s especially \nuseful in this context, as this facilitates the pub lication and \ndistribution of a datasetâ€™s extracted features even  when the \nmusic itself cannot be distributed due to legal lim itations. \nTo serve as an example, a Greenstone collection was  \ngenerated from the audio files of SAC/SLAC, a resea rch \ndataset that has been used in a number of previous studies \n(e.g. [8]). Greenstone automatically extracted cont ent-based \nfeatures and mined metadata from the web, as descri bed \nabove. The result is an automatically annotated Gre enstone \ncollection, whose metadata can be browsed, searched , edit- \ned and published. Figure 1 shows a screen shot of o ne sam- \nple entry. The full published Greenstone collection  is post- \ned at www.nzdl.org/greenstone2-jmir. \n \n \nFigure 1:  A sample entry on Sing, Sing, Sing, by Benny \nGoodman, from the COSI-SLAC Greenstone research col - \nlection. Under the particular display configuration  settings \nthat were chosen for this collection, the main entr y dis- \nplays only basic summary information, and the audio  fea- \ntures and full detailed metadata are left to be dow nloaded \nas ACE XML files for machine processing or viewing.  \nMined images are also displayed, and the audio itse lf is \nstreamed. \nConsidered from a somewhat different perspective, s ome \nof the metadata mined by jSongMiner can also be use d di- \nrectly as features, even though this is not its pri mary pur- pose (e.g. â€œTempo BPMâ€, â€œKeyâ€, â€œTime Signatureâ€, et c.). \nThis type of usage is facilitated by jSongMinerâ€™s ( and \nGreenstoneâ€™s) ability to save metadata in ACE XML [ 8], a \nmachine learning-oriented format. \nThe SLAC collection was also used to investigate th is \napplication experimentally. jSongMiner identified e ach of \nthe audio recordings in SLAC using fingerprinting, and was \nthen used in combination with jMIRâ€™s jWebMiner [8] in \norder to mine a variety of features using the APIs of \nLast.FM and Yahoo. These features were then used to  per- \nform a 10-class 10-fold genre classification experi ment, \nwhere jMIRâ€™s ACE [8] provided the machine learning func- \ntionality. This resulted in an average 83% classifi cation \nsuccess rate, compared to 68% when only audio conte nt-\nbased features extracted by jAudio were used. For t he sake \nof comparison, 86% was achieved when the same web-\nderived features were used, but model curated ident ifiers \nwere used to extract them rather than identifiers d erived \nfrom jSongMinerâ€™s fingerprinting results. \nMusic librarians are another important potential us er of \nthe updated Greenstone software. Even those librari es with \nextensive digital collections tend to have relative ly limited \nmetadata available for the bulk of their collection s. The cost \nof manually annotating music is a major stumbling b lock, \nand Greenstone now allows the process to be cheaply  and \neasily automated. Librarians simply need to provide  music \nto Greenstone, which will then automatically annota te it \nwith metadata. Librarians can then validate the ext racted \nmetadata if they wish, a process much cheaper than actually \nentering it. The metadata can then be published to the web \nor CD using Greenstone to provide increased access to li- \nbrary patrons, and the Dublin Core tags generated b y \nGreenstone can be used for internal reference purpo ses. \nThere are also many other potential user types. Pri vate \nmusic collectors might wish to use Greenstone to an notate \ntheir collections, for example, or to detect wrongl y labelled \nrecordings using fingerprinting. To give another ex ample, \nthose in the music industry might use it to enrich their own \ncatalogue or marketing data in a variety of ways. I t is espe- \ncially important to emphasize that jSongMiner is de signed \nto be easily extended to mine data using arbitrary APIs, so \nthere may be many types of data which could potenti ally be \naccessed in the future which have not been envision ed yet. \n8.  CONCLUSIONS AND FUTURE RESEARCH \nThe incorporation of the updated jAudio and the new  \njSongMiner software into Greenstone significantly e x- \npands Greenstoneâ€™s value to those wishing to automa tically \nconstruct, annotate, organize and make accessible l arge \nmusic collections. Greenstone can now identify unkn own \naudio recordings, extract content-based information  from \n463Poster Session 3\n  \n \naudio files and mine Internet resources in order to  automat- \nically build a rich set of metadata about musical e ntities. \nSuch Greenstone collections can consist of many dif ferent \ntypes of documents associated with each musical pie ce, \nartist or album, such as audio files, scores, video s, images \nand PDFs. Users also have the ability to use jSongM iner or \njAudio outside of the Greenstone framework if they wish. \nOne of the main priorities of future research is to  more \nfully incorporate Greenstone and jMIR into the Netw orked \nEnvironment for Music Analysis [16] project. This w ill al- \nlow Greenstone collections to be built and accessed  in a \ndistributed framework that will further increase it s useful- \nness to MIR researchers. \nAnother priority is the design of Greenstone plugin s for \nother jMIR components, so that, for example, featur es may \nalso be automatically extracted from MIDI files add ed to a \nGreenstone collection using jSymbolic, or from lyri cal tran- \nscriptions using jLyrics. \nAn additional priority is the direct incorporation of fur- \nther web services into jSongMiner. Although the mai n val- \nue of jSongMiner is as a framework that facilitates  the in- \ncorporation of arbitrary web services as they becom e avail- \nable, it would still be advantageous for some poten tial users \nto build in immediate support for further currently  existing \non-line resources, such as MusicBrainz, Yahoo! and Ama- \nzon, to name just a few. \nThe fourth priority is the integration of functiona lity for \nautomatically detecting errors in collected metadat a. The \nalready existing functionality in jMIRâ€™s jMusicMeta - \nManager [8] will be a good starting point. This fun ctionali- \nty will also ideally be expanded to perform auto-co rrection \nthat can automatically update the data sources from  which \nerroneous metadata was mined, if permitted. \n9.  ACKNOWLEDGEMENTS \nThe authors would like to thank the Centre for Open  Soft- \nware Innovation (COSI) and the Andrew W. Mellon Fou n- \ndation for their generous financial support. Thanks  also to \nDaniel McEnnis for his work on jAudio, and to the m any \nothers who have contributed to jMIR or Greenstone i n the \npast, especially Prof. Ichiro Fujinaga. \n10.  REFERENCES \n[1]  Bainbridge, D., and T. Bell. 2003. A music notation  \nconstruction engine for optical music recognition. \nSoftware Practice and Experience  33 (2): 173â€“200. \n[2]  Bainbridge, D., S. J. Cunningham, and J. S. Downie.  \n2004. GREENSTONE as a music digital library toolkit . \nProceedings of the International Conference on Musi c \nInformation Retrieval . 42â€“7. [3]  Bainbridge, D., S. J. Cunningham, and J. S. Downie.  \n2004. Visual collaging of music in a digital librar y. \nProceedings of the International Conference on Musi c \nInformation Retrieval . 397â€“402. \n[4]  Cannam, C., C. Landone, M. Sandler, and J. P. Bello . \n2006. The Sonic Visualiser: A visualisation platfor m \nfor semantic descriptors from musical signals. \nProceedings of the International Conference on Musi c \nInformation Retrieval . 324â€“7. \n[5]  Lartillot, O., and P. Toiviainen. 2007. MIR in Matl ab \n(II): A toolbox for musical feature extraction from  \naudio. Proceedings of the International Conference on \nMusic Information Retrieval . 127â€“30. \n[6]  Marill, J. L., and E. C. Lucza. 2009. Evaluation of  \ndigital repository software at the National Library  of \nMedicine. D-Lib Magazine  15 (5/6). \n[7]  McEnnis, D., C. McKay, and I. Fujinaga. 2006. \njAudio: Additions and improvements. Proceedings of \nthe International Conference on Music Information \nRetrieval . 385â€“6. \n[8]  McKay, C. 2010. Automatic music classification with  \njMIR. Ph.D. Dissertation.  McGill University, Canada. \n[9]  Tzanetakis, G., and P. Cook. 2000. MARSYAS: A \nframework for audio analysis. Organized Sound  4 (3): \n169â€“75. \n[10]  Witten, I. H., D. Bainbridge, and D. M. Nichols. 20 10. \nHow to build a digital library.  San Francisco, CA: \nMorgan Kaufmann. \n[11]  Witten, I. H., and E. Frank. 2005. Data mining: \nPractical machine learning tools and techniques . New \nYork: Morgan Kaufman. \n[12]  API â€“ Last.fm. Retrieved 15 August 2011, from \nhttp://www.last.fm/api. \n[13]  Dublin Core Metadata Initiative. Retrieved 22 Augus t \n2011, from http://dublincore.org. \n[14]  Echo Nest API Overview. Retrieved 15 August 2011, \nfrom http://developer.echonest.com/docs/v4/. \n[15]  Mozenda. Retrieved 15 August 2011, from \nhttp://www.mozenda.com. \n[16]  Networked Environment for Music Analysis (NEMA). \nRetrieved 15 August 2011, from http://www.music-\nir.org/?q=nema/overview. \n[17]  Semantic Desktop with KDE. Retrieved 15 August \n2011, from http://nepomuk.kde.org. \n464"
    },
    {
        "title": "Mining the Correlation between Lyrical and Audio Features and the Emergence of Mood.",
        "author": [
            "Matt McVicar",
            "Tim Freeman 0001",
            "Tijl De Bie"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1415888",
        "url": "https://doi.org/10.5281/zenodo.1415888",
        "ee": "https://zenodo.org/records/1415888/files/McVicarFB11.pdf",
        "abstract": "Understanding the mood of music holds great potential for recommendation and genre identification problems. Unfortunately, hand-annotating music with mood tags is usually an expensive, time-consuming and subjective process, to such an extent that automatic mood recognition methods are required. In this paper we present a new unsupervised learning approach for mood recognition, based on the lyrics and the audio of a song. Our system thus eliminates the need for ground truth mood annotations, even for training the system. We hypothesize that lyrics and audio are both partially determined by the mood, and that there are no other strong common effects affecting these aspects of music. Based on this assumption, mood can be detected by performing a multi-modal analysis, identifying what lyrics and audio have in common. We demonstrate the effectiveness of this using Canonical Correlation Analysis, and confirm our hypothesis in a subsequent analysis of the results.",
        "zenodo_id": 1415888,
        "dblp_key": "conf/ismir/McVicarFB11",
        "keywords": [
            "unsupervised learning",
            "mood recognition",
            "lyrics",
            "audio",
            "ground truth",
            "multi-modal analysis",
            "Canonical Correlation Analysis",
            "hypothesis",
            "effectiveness",
            "confirmation"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nMINING THE CORRELATION BETWEEN LYRICAL AND AUDIO FEATURES\nAND THE EMERGENCE OF MOOD\nMatt McVicar\nIntelligent Systems Lab,\nUniversity of Bristol\nmatt.mcvicar@bris.ac.ukTim Freeman\nEngineering Mathematics\nUniversity of Bristol\ntf7960@bris.ac.ukTijl De Bie\nIntelligent Systems Lab,\nUniversity of Bristol\ntijl.debie@gmail.com\nABSTRACT\nUnderstanding the mood of music holds great potential for\nrecommendation and genre identiï¬cation problems. Unfortu-\nnately, hand-annotating music with mood tags is usually an\nexpensive, time-consuming and subjective process, to such\nan extent that automatic mood recognition methods are re-\nquired. In this paper we present a new unsupervised learn-\ning approach for mood recognition, based on the lyrics and\nthe audio of a song. Our system thus eliminates the need for\nground truth mood annotations, even for training the system.\nWe hypothesize that lyrics and audio are both partially de-\ntermined by the mood, and that there are no other strong com-\nmon effects affecting these aspects of music. Based on this as-\nsumption, mood can be detected by performing a multi-modal\nanalysis, identifying what lyrics and audio have in common.\nWe demonstrate the effectiveness of this using Canonical Cor-\nrelation Analysis, and conï¬rm our hypothesis in a subsequent\nanalysis of the results.\n1. INTRODUCTION\nDetecting the mood evoked by a musical piece is a task which\nis relatively easy for human listeners to perform. The ability\nto automate this process would be of use for music search, re-\ntrieval and recommendation, and for these reasons automatic\ntechniques that recognize emotion in music have been an ac-\ntive topic of research in the past few years (e.g. [5, 8, 10, 17]).\nThe most common method of quantifying a mood state is\nby associating it with a point in a 2-dimensional space with\nvalence (attractiveness/aversiveness) and arousal (energy) as\ndimensions, a concept ï¬rst proposed by Russell [14]. High\nvalence values correspond to positive moods such as â€˜pleasedâ€™\nor â€˜satisï¬edâ€™, with negative examples being emotions such\nas â€˜frustratedâ€™ or â€˜miserableâ€™. Arousal can range from neg-\native values (â€˜sleepyâ€™) to positive (â€˜excitedâ€™). This domain\nis known as the valence-arousal space (see Figure 1). Thus,\nautomatic methods for mood recognition would map a song\nonto a point in this 2-dimensional space. However, also other\nways of quantifying mood have been considered (e.g. [13]).\nPermission to make digital or hard copies of all or part of this work for per-\nsonal or classroom use is granted without fee provided that copies are not\nmade or distributed for proï¬t or commercial advantage and that copies bear\nthis notice and the full citation on the ï¬rst page.\nc/circlecopyrt2010 International Society for Music Information Retrieval.\nFrustrated\nMiserable\nSleepyExcited Aroused\nTiredDelighted\nAt ease SadAngry\nSatisfiedPleased VALENCEAROUSALFigure 1 . The 2âˆ’dimensional valence-arousal space, show-\ning a range of emotions on a attractiveness/energy scale.\nA major problem with evaluating (andâ€”for machine learn-\ning methodsâ€” training) such algorithms is that high-quality\nground truth mood annotations are hard to come by. Ideally\nthese would be obtained by questioning a range of people on\nwhich emotions (and to which degree) they experience when\nlistening to a range of songs in many styles. Such studies\nare expensive and time-consuming and clearly do not scale\nto the quantity of music required to tackle realistic research\nproblems. A further confounding factor is that the emotion or\nmood associated with a song is a subjective and often personal\nfeature.\n1.1 Contributions\nIn this paper, we conduct a bi-modal analysis of music, simul-\ntaneously studying the audio and the lyrics of songs. Our goal\nis to extract factors that simultaneously underly aspects of the\naudio and the lyrics of popular music, at least statistically. In\nother words, we ask the question: â€œWhat do the audio and the\nlyrics of songs have in common?â€\nOur hypothesis is that answering this question is likely\nto resolve the problems faced in developing and assessing\nthe quality of mood recognition systems, both those that are\nbased on audio and those based on lyrics (or both). Indeed, we\nassume that the intended mood of a song will inspire the song-\nwriter to use certain timbres, harmony, and rhythmic features,\nin turn affecting the choice of lyrics as well. A further hypoth-\n783Oral Session 9: Emotion and Mood\nesis is that factors unrelated to mood typically do not simulta-\nneously inï¬‚uence the audio and the lyrics. If these hypotheses\nhold, uncovering what lyrics and audio share is equivalent to\nuncovering the mood of a song.\nAs a partial veriï¬cation our hypotheses, below we ï¬rst\ndescribe an exploratory analysis investigating if audio fea-\ntures correlate with valence and arousal, as predicted by a\nnaive mood recognition algorithm based on lyrical informa-\ntion only.\nThe main result in this paper is the application of Canoni-\ncal Correlation Analysis (CCA) [6] between paired represen-\ntations of a songâ€™s audio and its lyrics. This is an unsupervised\nlearning method that is independent of human experiments,\nable to extract common factors affecting both modes under\nstudy. We illustrate results which intuitively seem to coincide\nremarkably well with a notions of valence, and with another\nnotion that is different but seems related to arousal.\n1.2 Related work\nPrevious work in the area of multi-mode (text and audio) mood\nrecognition has been focused on combining lyrics and audio\ninto combined features for classiï¬cation [7, 8]. This however\nstill depends on the availability of good quality mood anno-\ntations for a large number of songs. Most strongly related to\nour current work is the investigation of correlations between\nsocial (non-lyrical) tags and audio [16]. Note that it is far less\nobvious that lyrics contain information about mood than in\nsocial tags. However, lyrics are easy to obtain, less subject\nto spamming, and objective. Thus, our work combines the\nbeneï¬ts of the two types of prior work.\nDuring the ï¬nal stages of our study, the MusiXmatch lyrics\ndatabase that is paired with the Million Song dataset was re-\nleased [4]. Our study here is conducted on lyrics gathered by\nourselves, the size of which is smaller but of similar order of\nmagnitude as the MusiXmatch database. The approach pre-\nsented in the current paper can directly be used as a blueprint\nfor future research into the relationship between lyrics and\naudio based on this larger set of data.\n1.3 Outline\nThe remainder of this paper is organised as follows. In Sec-\ntion 2 we outline our general approach and hypotheses. In\nSection 3 we describe the set of audio and lyric features used\nin this paper. A simple experiment is conducted in Section\n4 exploring correlations between lyrics and audio. Section 5\ncontains our main result on CCA analysis and we conclude\nour ï¬ndings in Section 6.\n2. MOOD: THE SYNERGY OF LYRICS & AUDIO?\nSince 2007, the Music Information Retrieval Evaluation eX-\nchange (MIREX) has run a task on audio mood classiï¬cation.\nThe task is to â€˜tagâ€™ audio clips with an emotional label. Here,\nthe ground truth is provided by users of the musical radio\nsitewww.last.fm . There are generally three approaches to\ntackling mood classiï¬cation in these tasks and we summarise\nthem here to highlight the interplay between text and audio.2.1 Classiï¬cation based on Audio Features\nThe most common method for classiï¬cation is based on har-\nmonic and spectral features of the audio [8]. Commonly used\nfeatures include low level indicators such as spectral centroid,\nrolloff, ï¬‚ux, slope, skewness and kurtosis [3], harmonic fea-\ntures such as MFCCs [12] and those based on Short Time\nFourier Transforms [15]. In many cases Support Vector Ma-\nchines are used to discriminate between features and have\nproved to be successful in this setting [9].\n2.2 Classiï¬cation based on Lyrical Features\nOther approaches are based on lyrical content only. Bag-Of-\nWords (BOW) representations have recently been successful\nin identifying mood, as well as higher-order statistics such as\ncombinations of unigrams, bigrams and trigrams [5].\n2.3 Classiï¬cation using both Audio and Lyrics\nMore complex approaches simultaneously exploit lyrical and\naudio features. Such approaches generally achieve higher\nclassiï¬cation accuracy than those methods presented in Sub-\nsections 2.1 and 2.2 (see for example [11, 17]).\nA recent analysis by Hu et. al. [8] showed that lyrical fea-\ntures typically outperform audio when used as a classiï¬er, al-\nthough they note that in their study audio was more useful\nin determining emotions in the 3rdquadrant of the valence-\narousal space in Figure 1 (i.e. â€˜sadâ€™, â€˜depressedâ€™ etc.).\n2.4 Framework\nIn this paper, we will search for correlations between a set of\nfeatures from audio and from the lyrics, under the assumption\nthat the causal factor of any such correlations is the mood, i.e.\nthat emotion is the unique facet that lyrics and audio share. Of\ncourse, such patterns may be subtle and they will be present\nonly â€˜on averageâ€™, such that they cannot be reliably detected\non small samples. For this reason, we study such patterns on\na large scale, allowing even subtle correlations to emerge as\nstatistically signiï¬cant.\nInformally speaking, if xaâˆˆRdais ada-dimensional\naudio-based feature vector for a given song, and xlâˆˆRdl\nis adl-dimensional lyrical feature vector for the same compo-\nsition, we seek real-valued functions faandflsuch that for\nmany songs and to a good approximation:\nfa(xa)â‰ˆfl(xl). (1)\nA core assumption is that if such functions faandflcan\nbe found, they must be capturing some notion of mood of an\naudio piece. Due to variability in style, genre, instrumenta-\ntion and potential use of irony (i.e. different mood exhibited\nby the lyrics and the audio), we do not expect to ï¬nd this ap-\nproximate equality to be very strong, or to be valid for many\nsongs, but the size of the data used (see below) should never-\ntheless allow us to ï¬nd statistically signiï¬cant relation.\nOur strategy differs from previous ones in that it does not\nneed a training set of songs with ground truth mood anno-\ntations. Rather than supervising the learning process using\nground truth labels, we simultaneously train two mood recog-\nnizers, one based on lyrics and one on audio, which supervise\neach otherâ€™s learning.\n78412th International Society for Music Information Retrieval Conference (ISMIR 2011)\n3. THE DATA: SONG CORPUS AND FEATURES\nBelow we describe the feature representations of the lyrics\nand audio modes of songs we used in this paper, as well as\nthe corpus of songs used.\n3.1 Lyrics feature representation\nWe used the Term Frequency-Inverse Document Frequency\n(TF-IDF) measure to represent the lyrics in a song. The TF-\nIDF representation of a document is a reweighted version of\na BOW account, accounting for how rare a word is with re-\nspect to a document and the overall collection. Consider the\nithword in thejthlyric. Then the term frequency is the num-\nber of times word iappears in document j, normalised by the\ndocumentâ€™s length:\nTFi,j=|wordiappears in lyric j|\n|lyricj|\nThe inverse document frequency is a measure of the gen-\neral importance of the word in the lyric database:\nIDF i= logtotal number of lyrics\n|lyrics containing word i|\nThe TF-IDF for word iin lyricjis then the product\nTFIDF i,j=TFi,jÃ—IDF i\n3.2 Audio Feature Extraction\nWe used the Echonest API1to extract features from our au-\ndio and thus obtained 65 spectral, percussive, harmonic and\nstructural features, which are summarised in Table 1.\nField Feature\n1 Tempo\n2 Tempo Conï¬dence\n3-7 Time Signature\n8 Time Signature Conï¬dence\n9 Mode\n10 Mode Conï¬dence\n11 Number of Sections\n12 Energy\n13 Danceability\n14-25 Mean Chroma Pitches\n26-37 Standard Deviation Chroma Pitches\n38-49 Timbre Mean\n50-61 Timbre Standard Deviations\n62 Loudness Start Mean\n63 Loudness Start Standard Deviations\n64 Loudness Max Mean\n65 Loudness Max Standard Deviations\nTable 1 . Audio features extracted from Echonest.\nNote that some of these features (e.g. the Mean Chroma\nPitches) are unlikely to be relevant for mood recognition. Still,\nwe have included them in our experiments to validate our ap-\nproach.\n1http://developer.echonest.com/docs/v4/\n012345678910012345678910\nValenceArousalFigure 2 . Valence and arousal for the ANEW database.\n3.3 The song corpus\nUsing a simple web-scraper, we obtained lyrics from the pop-\nular lyrical database website www.lyricsmode.com , which\ncontains over 800,000 song entries. We also obtained audio\nfeatures using the Echonest API and found the intersection of\nthese two datasets to be 119,664lyric/audio pairs. We are not\naware of any other lyrical/audio combined studies carried out\non this scale.\n4. EXPLORING MOOD, AUDIO, AND LYRICS\nRELATIONS\nIn a ï¬rst exploratory study, we build a simple mood recog-\nnition system based on lyrics, and we verify which (if any)\naudio features are correlated with this mood estimate. This\nis to conï¬rm our basic hypothesis that on average both lyrics\nand audio reï¬‚ect the mood of a song. To this end we im-\nplemented a simple method for estimating mood from lyrics\nbased on the valence/arousal space described in Sec. 1.\n4.1 Valence/Arousal Estimation\nOne method of analysing emotive content of lyrics is to mea-\nsure the average valence or arousal over a song, picking out\nparticular words from a dictionary where the valence/arousal\nscores are known. We chose the Affective Norms for En-\nglish Words (ANEW) as our dictionary, which contains rat-\nings of 1030 words on pleasure, arousal and dominance col-\nlected by psycholinguistic experiments [2]. The words within\nwere chosen to cover a wide range of the valence-arousal\nspace [10] and we show their means (taken over participants)\nin Fig. 2.\nLetli= (w1,w2...w ni)be theithlyric, comprised of ni\nwords and letL={l1,l2,...l m}be the complete collection\nof lyrics. We then estimate the valence viand arousal aiof\n785Oral Session 9: Emotion and Mood\nlyricivia\nvi=1\nnini/summationdisplay\nj=1V(wni), ai=1\nnini/summationdisplay\nj=1A(wni), i= 1...m.\nVandAare functions that return the mean valence/arousal if\nwordwniis in the ANEW dictionary and zero otherwise.\nThis is obviously a crude mood recognition system. Note\nhowever that our goal here is to use a simple and transparent\nsystem, only to verify our hypothesis that audio and lyrics\nshare a common cause.\n4.2 Correlations between audio features and mood\nestimates based on lyrics\nGiven our simple mood recognition system based on lyrics,\nwe computed Pearsonâ€™s correlation coefï¬cient between each\nof the audio features and our valence/arousal estimate based\non lyrics. We found many of the correlations to be extremely\nstatistically signiï¬cant, but below 0.2in absolute value. For\nillustration, in Table 2 we show the audio features that are\ncorrelated with p-value numerically equal to 0, and from those\nonly the 5highest correlations by absolute value.\nAudio Feature Lyrical Feature Correlation\n12 Valenceâˆ’0.1943\n62 Valenceâˆ’0.1939\n38 Valenceâˆ’0.1897\n64 Valenceâˆ’0.1818\n61 Valence 0.1739\n57 Arousalâˆ’0.0591\n59 Arousalâˆ’0.0553\n39 Arousal 0.0511\n17 Arousal 0.0462\n24 Arousal 0.0434\nTable 2 . Top correlations with valence and arousal with p-\nvalue numerically 0(audio feature indices refer to Table 1).\nThe strongest relationship is valence against energy, with\na correlation ofâˆ’0.1943 . This suggests than an increase in\nâ€˜lyrical positivenessâ€™ corresponds to a decrease in energy, and\nis perhaps caused by love ballads, which typically will contain\nmany positive words (â€˜loveâ€™,â€˜heartâ€™ etc.) along with gentle\naudio. Several other audio features strongly correlated with\nvalence are loudness (62,64).\nThe correlations with arousal are more difï¬cult to inter-\npret. The top three correlations relate to timbre, and seem\nplausible. The features 17and24are mean chroma values\nover the song, and their apparent signiï¬cance to mood seems\ncounter-intuitive. However, the magnitude of the correlations\nis very small when compared to the valence correlations, and\nwe suspect that these correlations are due to artefacts (e.g.,\nmean chroma values may not be independent of certain loud-\nness features). Unfortunately, this is hard to verify, as the\nexact mechanism of how they are computed is unknown to us\n(they were obtained through the echonest API).\nThe overall conclusion that can be drawn is that a correla-\ntion between valence/arousal is present and signiï¬cant, which\nconï¬rms our hypothesis that, to some extent, mood is indeedsimultaneously related to both lyrics and audio. However, the\ncorrelations are not very strong. We suggest two possible\nexplanations for this. Firstly, the mood recognition method\nbased on lyrics is simple and imperfect. More crucially, prob-\nably none of the audio features by themselves relate strongly\nto moodâ€”probably that a combination of them is more rele-\nvant (in different combinations for valence and arousal) than\neach of the features individually.\nIn the next Section, we will demonstrate a method that is\nimmune to both these problems. We will simultaneously learn\nlinear combinations of the features in the lyrics and audio rep-\nresentations, so as to maximize the correlation between the\nresulting linear combinations. In this way, we avoid our de-\npendency on an initial method for mood recognition based on\nlyrics such as the one introduced in Sec. 4.1. Furthermore, by\nconsidering linear combinations of features, we expect to ï¬nd\nmuch stronger and more meaningful relations.\n5. CANONICAL CORRELATION ANALYSIS\nWe will ï¬rst discuss the theory of CCA before presenting our\nï¬ndings (see e.g. [1] for a more in depth treatment).\n5.1 Background\nCCA is a technique that can be used to ï¬nd information that\nis consistent in two datasets by revealing linear correlations\nbetween them, and is particularly useful in high-dimensional\ndatasets such as ours.\nGiven two datasets XâˆˆRnÃ—dxandYâˆˆRnÃ—dy, the ob-\njective of CCA is to ï¬nd weightings wxâˆˆRdxandwyâˆˆRdy\nthat maximise the correlation between the projections of X\nandY,XwxandXwy. Thinking of these projections as di-\nrections through the data spaces, CCA looks for a projection\nwhich will minimise the angle âˆ betweenXwxandXwy.\nMathematically, this optimization problem is written:\n{wâˆ—\nx,wâˆ—\ny}= argmin\nwx,wyâˆ (Xwx,Yw y),\n= argmax\nwx,wycos(âˆ (Xwx,Yw y)),\n= argmax\nwx,wy(Xwx)/prime(Ywy)/radicalbig\n(Xwx)/prime(Xwx)/radicalbig\n(Ywy)/prime(Ywy),\n= argmax\nwx,wyw/prime\nxX/primeYwy/radicalbig\nw/primexX/primeXwx/radicalbigw/primeyY/primeYwy.\nIt is known that this optimization problem can be solved\nby solving the following generalized eigenvalue problem (see\ne.g. [1] for a derivation):\n/parenleftbigg0X/primeY\nY/primeX 0/parenrightbigg/parenleftbiggwx\nwy/parenrightbigg\n=Î»/parenleftbiggX/primeX 0\n0Y/primeY/parenrightbigg/parenleftbiggwx\nwy/parenrightbigg\n.(2)\nThe eigenvalue Î»in Eq. (2) is equal to the achieved corre-\nlation between the projections of XandYon their respective\nweight vectors wxandwy. Thus, the eigenvector correspond-\ning to the largest eigenvalue is of greatest interest, with suc-\ncessive ones of decreasing importance. An additional prop-\nerty of CCA is that projections on successive components are\nindependent, such that each of the eigenvectors capture un-\ncorrelated information.\n78612th International Society for Music Information Retrieval Conference (ISMIR 2011)\n5.2 Experiments\nIn our setting, the data XandYrefer to audio and lyrical\nfeatures. For lyrical features independent of mood, we used\nthe TF-IDF measure described in Subsection 3.1.\nTo prevent overï¬tting of the method we performed 100-\nfold cross validation. I.e., we split the set of 119,664songs\ninto100disjoint subsets and apply CCA on the union of 99\nof them, after which we compute the correlation between the\nprojections of the remaining subset on the obtained weight\nvectors as a validation. This is repeated 100times, leaving\nout each of the 100subsets in turn. The mean training and\ntesting correlations over the folds are shown in Figure 3.\n5 10 1500.050.10.150.20.250.30.350.40.45\nCCA ComponantTraining/Test Correlations\nFigure 3 . Training/Testing (upper/lower bars) correlations of\nthe CCA components, with Error bars of 1 standard deviation.\nIt can be seen that training and test correlations are quite\nclose, especially in the ï¬rst two components (suggesting the\ndata is not signiï¬cantly overï¬tted). Correlations on the train-\ning set are likely to always be higher than on the test set, but\nit appears not signiï¬cantly so, as the error bars on the test set\noverlap those for the training data in these cases.\nConï¬dent that the CCA algorithm was not overï¬tting the\ntraining data, we proceeded to train the weights on all of the\ntraining data, and tested on the complete set. The ï¬rst com-\nponent is shown in detail in Table 3.\nInspecting Table 3, the ï¬rst component seems to closely\ncorrespond to valenceâ€”even though this was not imposed by\nthe algorithm. Low weights are associated with strongly neg-\native emotions/words, which would lie in the 4thquadrant of\nthe valence-arousal space (see Fig. 1). In contrast, the words\nwith high weights appear to correspond to positive moods ( 1st\nquadrant), although there are some outliers in the 3rdand4th\ncolumns. In the audio domain the features most negatively\nweighted in the CCA components were all related to Timbre,\nthe most positive to Loudness.\nTo verify that the ï¬rst component relates to valence, weLowest Highest\nWord Lyrical Weight Word Lyrical Weight\nDeath -0.075996 Love 0.1248\nDead -0.064387 Baby 0.049397\nHate -0.054789 Heart 0.047417\nPain -0.047474 Hay 0.029812\nEvil -0.04673 Home 0.028472\nLife -0.042257 Lonely 0.027777\nStench -0.040415 Good 0.027413\nHell -0.038346 Blue 0.026954\nWar -0.037502 Sin 0.026194\nDestroy -0.036671 Loved 0.026123\nFeature Audio Weight Feature Audio Weight\n38 -0.61774 64 0.3919\n50 -0.22214 62 0.28949\n42 -0.15033 65 0.19222\nTable 3 . First component of the CCA analysis, which ap-\npears to relate to valence. The 10most negatively and posi-\ntively weighted words and 3most weighted audio features are\nshown, along with their associated weights.\ncorrelated the weights which resulted from the CCA output\nto the valences from the ANEW database. The resulting cor-\nrelation wasâˆ’0.3519 , with apâˆ’value numerically equal to\n0. This is an important result, as it shows we have success-\nfully reconstructed words which carry the meaning of â€˜posi-\ntive/negativeâ€™ emotions without the need for expensive human\ninterventions. It shows that valence is the aspect of mood\nmost dominantly affecting both lyrics and audio.\nLowest Highest\nWord Lyrical Weight Word Lyrical Weight\nHeart -0.024301 Baby 0.02641\nLove -0.019733 Man 0.021014\nLost -0.018202 Hit 0.020528\nWorld -0.015552 Money 0.020241\nMoment -0.015103 Rock 0.019736\nFall -0.015003 Party 0.018319\nLonely -0.014069 Girl 0.017076\nDream -0.013675 Mad 0.015997\nHope -0.013444 Kick 0.015813\nSun -0.012514 Fat 0.012571\nFeature Audio Weight Feature Audio Weight\n38 -0.77382 64 0.49949\n12 -0.10808 62 0.26838\n43 -0.080392 5 0.092167\nTable 4 . Second component of the CCA analysis, which we\npostulate relates to arousal.\nThe second component is shown in Table 4, and is more\ndifï¬cult to interpret, although there seems to be a relation with\narousal. Words in the ï¬rst column (â€˜dreamâ€™, â€˜heartâ€™) are gen-\nerally calming and restful, whilst those in the third column\nare more energetic (â€˜kickâ€™,â€˜partyâ€™). Audio features with sig-\nniï¬cant weight relate to Timbre/Energy and Loudness.\n787Oral Session 9: Emotion and Mood\n5.3 Discussion\nIt is remarkable that our CCA analysis automatically detects\naspects of mood that appear to align with Russellâ€™s model for\nhuman perception of emotion [14], without any dependence\non human trials or mood annotations. We should point out\nthat further components (not shown here due to space con-\nstraints) are harder to interpret in terms of aspects of mood we\nare aware of. However, given the encouraging results for the\ndominant components we believe they are likely to be helpful\nin a multi-dimensional characterization of mood in audio and\nin lyrics. As such they may be helpful in applications such as\nmusic classiï¬cation and recommendation in particular.\nInterestingly, our approach also opens up possibilities of\ndetecting more high-level properties in music, such as irony\nand sarcasm. The ability to recognize strongly correlated as-\npects of mood from both audio and lyrics also allows us to\nidentify songs where there is a discrepancy or tension between\nthe mood in the audio and the mood in the lyrics, violating the\nglobal pattern of correlation.\n6. CONCLUSIONS\nIn this paper we investigated the correlation between audio\nand lyrics, demonstrating that there exist weak but highly sig-\nniï¬cant correlations between lyrical and audio features. Fol-\nlowing this, we used Canonical Component Analysis to un-\ncover strong correlations between linear combinations of lyri-\ncal and audio features which, at least in part, appear to corre-\nspond to known aspects of mood and valence and arousal.\nIn further work we intend to rerun our experiments includ-\ning also the MusiXmatch dataset [4]. Furthermore, we intend\nto use more features such as images, video, social tags and\nnâˆ’gram features in the lyrical domain.\n7. REFERENCES\n[1] T. De Bie, N. Cristianini, and R. Rosipal. Eigenproblems\nin pattern recognition. In E. Bayro-Corrochano, editor,\nHandbook of Computational Geometry for Pattern Recog-\nnition, Computer Vision, Neurocomputing and Robotics .\nSpringer-Verlag, 2004.\n[2] M.M. Bradley and P.J. Lang. Affective norms for english\nwords (anew): Instruction manual and affective ratings.\nUniversity of Florida: The Center for Research in Psy-\nchophysiology , 1999.\n[3] J.J. Burred, M. Ramona, F. Cornu, and G. Peeters. Mirex-\n2010 single-label and multi-label classiï¬cation tasks: ir-\ncamclassiï¬cation09 submission. MIREX 2010 , 2010.\n[4] The Echo Nest Corp. The million song dataset gets lyrics,\ntoo. http://blog.echonest.com/post/4578901170/the-\nmillion-song-dataset-gets-lyrics-too, May 2011.\n[5] H. He, J. Jin, Y . Xiong, B. Chen, W. Sun, and L. Zhao.\nLanguage feature mining for music emotion classiï¬cation\nvia supervised learning from lyrics. Advances in Compu-\ntation and Intelligence , pages 426â€“435, 2008.\n[6] H. Hotelling. Relations between two sets of variables.\nBiometrika , 28:321â€“377, 1936.[7] X. Hu and J.S. Downie. Improving mood classiï¬cation in\nmusic digital libraries by combining lyrics and audio. In\nProceedings of the 10th annual joint conference on Digi-\ntal libraries , pages 159â€“168. ACM, 2010.\n[8] X. Hu and J.S. Downie. When lyrics outperform audio for\nmusic mood classiï¬cation: a feature analysis. In ISMIR ,\npages 1â€“6, 2010.\n[9] X. Hu, J.S. Downie, C. Laurier, M. Bay, and A.F. Ehmann.\nThe 2007 mirex audio mood classiï¬cation task: Lessons\nlearned. In Proceedings of ISMIR , pages 462â€“467, 2008.\n[10] Y . Hu, X. Chen, and D. Yang. Lyric-based song emo-\ntion detection with affective lexicon and fuzzy clustering\nmethod. In Proceedings of ISMIR , 2009.\n[11] C. Laurier, J. Grivolla, and P. Herrera. Multimodal mu-\nsic mood classiï¬cation using audio and lyrics. In Machine\nLearning and Applications, 2008. ICMLAâ€™08. Seventh In-\nternational Conference on , pages 688â€“693. IEEE, 2008.\n[12] M.I. Mandel. Svm-based audio classiï¬cation, tagging,\nand similarity submissions. MIREX 2010 , 2010.\n[13] A. Pepe and J. Bolle. Between conjecture and memento:\nshaping a collective emotional perception of the future.\nInAAAI Spring Symposium on Emotion, Personality, and\nSocial Behavior , 2008.\n[14] J.A. Russell. A circumplex model of affect. Journal of\npersonality and social psychology , 39(6):1161, 1980.\n[15] K. Seyerlehner, M. Schedl, T. Pohle, and P. Knees. Using\nblock-level features for genre classiï¬cation, tag classiï¬ca-\ntion and music similarity estimation. MIREX 2010 , 2010.\n[16] D. Torres, D. Turnbull, L. Barrington, and G. Lanckriet.\nIdentifying words that are musically meaningful. Proc. IS-\nMIR07 , pages 405â€“410, 2007.\n[17] Y .H. Yang, Y .C. Lin, H.T. Cheng, I.B. Liao, Y .C. Ho,\nand H. Chen. Toward multi-modal music emotion classiï¬-\ncation. Advances in Multimedia Information Processing-\nPCM 2008 , pages 70â€“79, 2008.\n788"
    },
    {
        "title": "Leveraging Noisy Online Databases for Use in Chord Recognition.",
        "author": [
            "Matt McVicar",
            "Yizhao Ni",
            "Tijl De Bie",
            "RaÃºl Santos-Rodriguez"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1418311",
        "url": "https://doi.org/10.5281/zenodo.1418311",
        "ee": "https://zenodo.org/records/1418311/files/McVicarNBS11.pdf",
        "abstract": "The most significant problem faced by Machine Learningbased chord recognition systems is arguably the lack of highquality training examples. In this paper, we address this problem by leveraging the availability of chord annotations from guitarist websites. We show that such annotations can be used as partial supervision of a semi-supervised chord recognition methodâ€”partial since accurate timing information is lacking. A particular challenge in the exploitation of these data is their low quality, potentially even leading to a performance degradation if used directly. We demonstrate however that a curriculum learning strategy can be used to automatically rank annotations according to their potential for improving the performance. Using this strategy, our experiments show a modest improvement for a simple major/minor chord alphabet, but a highly significant improvement for a much larger chord alphabet.",
        "zenodo_id": 1418311,
        "dblp_key": "conf/ismir/McVicarNBS11",
        "keywords": [
            "high-quality training examples",
            "guitarist websites",
            "partial supervision",
            "curriculum learning",
            "annotation quality",
            "performance degradation",
            "curriculum learning strategy",
            "improvement for a simple major/minor chord alphabet",
            "significant improvement",
            "much larger chord alphabet"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nLEVERAGING NOISY ONLINE DATABASES FOR USE IN CHORD\nRECOGNITION\nMatt McVicar, Yizhao Ni, Tijl De Bie\nIntelligent Systems Lab\nUniversity of Bristol\nmatt.mcvicar@bris.ac.uk\n{yizhao.ni, tijl.debie }@gmail.comRaul Santos-Rodriguez\nUniversity Carlos III of Madrid\nrsrodriguez@tsc.uc3m.es\nABSTRACT\nThe most signiï¬cant problem faced by Machine Learning-\nbased chord recognition systems is arguably the lack of high-\nquality training examples. In this paper, we address this\nproblem by leveraging the availability of chord annotations\nfrom guitarist websites. We show that such annotations can\nbe used as partial supervision of a semi-supervised chord\nrecognition methodâ€” partial since accurate timing informa-\ntion is lacking. A particular challenge in the exploitation of\nthese data is their low quality, potentially even leading to a\nperformance degradation if used directly. We demonstrate\nhowever that a curriculum learning strategy can be used to\nautomatically rank annotations according to their potential\nfor improving the performance. Using this strategy, our\nexperiments show a modest improvement for a simple ma-\njor/minor chord alphabet, but a highly signiï¬cant improve-\nment for a much larger chord alphabet.\n1. INTRODUCTION\nChords are musical features which compactly describe the\nharmonic content of Western music. They have been used to\nsuccessfully identify keys [17], cover songs [2] and genres\n[1], conï¬rming their use in understanding and analysing mu-\nsical harmony, underscoring the importance of systems able\nto recognize chords from music audio. An important as-\npect of the chord recognition problem is the limited amount\nof high-quality audio annotations on which to train machine\nlearning systems, currently limited to 218 songs by The Bea-\ntles, Queen and Zweieck.1The result is that the perfor-\nmance of machine learning systems for chord recognition\n1available at http://isophonics.net/\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proï¬t or commercial advantage and that copies\nbear this notice and the full citation on the ï¬rst page.\nc/circlecopyrt2011 International Society for Music Information Retrieval.are starting to stagnate at around 80% in the MIREX evalu-\nation metric for an alphabet of major and minor chords only.\nIn this paper, we propose a system that complements the\nvaluable available data with annotations found in large on-\nline chord databases. In particular, here we make use of the\nchord database e-chords.com2, a guitarist website contain-\ning approximately 140,000 partially labelled chord annota-\ntions. Exploiting this data is non-trivial though: it does not\ncontain timing information, and the quality of the annota-\ntions is highly variable.\nThe proof-of-concept that such information can be ex-\nploited in a semi-supervised learning setting has already been\nprovided in a very small-scale study [15]. Unfortunately, it\nturns out that after scaling this up to more data this approach\nby itself is insufï¬ciently robust to overcome the quality is-\nsues with the online annotations. In the current paper, we\ntherefore adopt a curriculum learning approach, which at-\ntempts to add â€˜easyâ€™ data points ï¬rst and â€˜hardâ€™ ones only\nlater (if at all). To quantify â€˜easinessâ€™, we also introduce a\nnew metric to evaluate chord recognition performance when\nno ground truth annotation is available, but an online anno-\ntation is. This new metric by itself is a valuable contribu-\ntion, as it allows one to evaluate chord recognition systems\non artists other than The Beatles, Queen and Zweieck.\n2. PRELIMINARIES\nIn this section we describe our overall approach to chord\nrecognition, the audio features we make use of, as well as\nthe data we were able to extract from e-chords.\n2.1 Model Architecture\nAs a baseline system, we make use of a Hidden Markov\nModel (HMM), which has been used extensively and suc-\ncessfully for chord recognition [7, 17]. Here, the hidden\nchain represents the sequence of chords in a sequence of\ntime frames the song is segmented in. Assuming that chords\nrarely change between beats, we chose our frames to be\n2www.e-chords.com\n639Oral Session 8: Chord Analysis\nCCCFFFFAmAmAm\nC\nD\nE\nF\nG\nA\nB\n1 4 8\nTime Progression (frames)Pitch ClassFigure 1 . The HMM topology of our model, showing the\nhidden nodes of the HMM (chords) emitting 12-dimensional\nfeature vectors (chromagrams).\nthe time periods between consecutive beats as estimated us-\ning BeatRoot [6]. The observed chain corresponds to 12-\ndimensional chromagram feature vectors [6, 12] in the cor-\nresponding frames. The chromagram represents the distri-\nbution of energy across pitch classes of the harmonic content\nof the audio. The model is depicted in Fig. 2.1.\n2.2 Feature Extraction: the Loudness-Based\nChromagram\nThere is no single method to compute a chromagram feature\nvector, but the most popular ones are based on the Fourier\nand constant-Q transforms [4, 9, 11]. In this paper we will\nemploy a newly proposed variant, called the loudness-based\nchromagram [16]. The salient feature of this chromagram\nis that it is closer to how humans perceive the strength of\npitches. Similar to existing variants, the loudness chroma\nextraction process outputs a matrix CâˆˆR12Ã—Tfrom a monau-\nral signal x, whereTis the length of the feature in number\nof frames.\n2.3 Ground Truth Extraction\nFor each song for which a ground truth is available, we con-\nstructed the chromagram CâˆˆR12Ã—Tfeature vector, where\nTis the number of (estimated) beats. This is complemented\nwith a corresponding chord annotation AâˆˆATextracted\nfrom the ground truth annotations, where Ais a chord alpha-\nbet set. The fully annotated songs from The Beatles, Queen\nand Zweieck thus make for three sets of training data, de-\nnoted as{CB,GT B},{CQ,GT Q}and{CZ,GT Z}.\nYou know I love you\nAnd Iâ€™ll prove my love is true\nWant to show how I feel\nHoping that you love me\nâ€˜Cositâ€™s plain to see\nThat our love is realC F\nC G7\nDm9 G11Figure 2 . Example Untimed Chord Sequence (UCS) for\nâ€˜Our love is realâ€™ (Matt McVicar), showing chord labels\nabove lyrics.\n2.4 E-chords extraction\nAs in [15], we extracted Untimed Chord Sequences (UCSs)\nfrom the chord database e-chords.com. These UCS are re-\nferred to as â€˜untimedâ€™ as they only contain (noisy) infor-\nmation about the ordering of the chords, with no additional\ninformation on exact timing. From the e-chords website we\nwere able to scrape over 140,000 such UCSs, but we could\nonly use those for which we had access to the audio as well.\nWe combined our personal music collections and found the\noverlap with the UCS database to be 2008 tracks. Note that\nalthough it is unfortunate that we were only able to extract\na small proportion of UCSs from the database ( 2008 ), this\nnumber is signiï¬cantly larger than the number of currently\navailable training examples ( 218).\nWe calculated a loudness-based chromagram for each of\nthese 2008 songs in the echords dataset and refer to the e-\nchords chromagram/UCS set as {CEC,UCS}.\n3. EXPLOITING UCSâ€™S AS PARTIAL\nSUPERVISION DURING TRAINING\nThe UCSs clearly provide information about the true chords\nin an audio ï¬le, albeit only partial information. They convey\ninformation on the chords of many songs, but unfortunately\nthe explicit timings of the sequences are not known. Making\nuse of unlabelled (or partially labelled) data together with la-\nbelled data for training is known as Semi-Supervised Learn-\ning(SSL) [5].\n3.1 The semi-supervised learning approach\nThe general approach of exploiting UCSs during training\nwas introduced in [15], and we brieï¬‚y summarize it here.\nThe approach works by initially training the chord recogni-\ntion system (the HMM) based on the fully labelled training\ndata, here called the Core Training Set (CTS).\nSubsequently, it attempts to reconstruct the timings of the\nUCSs by aligning them to the chromagram feature vectors\n64012th International Society for Music Information Retrieval Conference (ISMIR 2011)\nextracted from the corresponding audio. An example UCS\nis shown in Figure 2. The ï¬rst six chords are to be repeated,\nalthough it is hard to infer this automatically without prior\nknowledge of the song. Unfortunately, this source of â€™struc-\ntural noiseâ€™ is hard to capture using automatic methods to\nscrape UCSs from websites, so we would miss this informa-\ntion.\nTo overcome this, the Jump Alignment (JA) algorithm\n(see [15]) can be used. The JA algorithm is able to align\nUCSs to audio, while allowing for jumps to the start of other\nlines (e.g. to allow a section to be repeated). The probabil-\nities of jumping forward or back in an annotation, as well\nas the key transposition and version are all chosen by maxi-\nmum likelihood. A different approach to dealing with struc-\ntural noise in online annotations has recently been proposed\nby the authors of [13], which could be combined with our\nalignment method to yield further improvements.\nAfter aligning our UCSs to their audio, they are in the\nform of fully labeled training data and can be added to the\nCTS. We refer to the resulting set of annotated data as the\nExpanded Training Set (ETS). Finally, the chord recogni-\ntion system can be retrained based on the ETS. The hope is\nthat this approach will allow one to train a chord recognition\nsystem to be able to recognize chords in genres that are dif-\nferent from those for which fully annotated chord sequences\nare available.\n3.2 Evaluation setup in this paper\nThis approach was introduced and tested on a small scale\nin [15], involving only songs for which a ground truth anno-\ntation is available. In this paper we test this approach on a\nsigniï¬cantly larger scale. In particular, as CTS, we use the\nQueen and Zweieck songs:\nCTS ={/uniondisplay\n{CQ,CZ},/uniondisplay\n{GT Q,GT Z}}\nThe ETS is the union of the CTS and the set of 2008 songs\nfor which we have the audio and a UCS from e-chords:\nETS ={/uniondisplay\n(CQ,CZ,CEC),/uniondisplay\n(GT Q,GT Z,AUCS )}\nThe test set consists of all The Beatles songs and their ground\ntruth annotations.\nThe ï¬‚ow-chart of this set-up is shown in Fig. 3, which\nalso shows the parameters that are inferred at various stages\n(the HMM initial and transition probability matrices Iand\nT, as well as the mean and covariance matrices for the Gaus-\nsian output probability densities, ÂµandÎ£). After retraining\nbased on the ETS, they are referred to as I/prime,T/prime,Âµ/primeandÎ£/prime.\nAs the results in Sec. 5 show, unfortunately in this setting\nthis basic approach deteriorates performance, rather than\nimproving it. To resolve this issue, here we propose to addi-\ntionally adopt a curriculum learning approach.\nEstimate \nParameters\nJump Alignment\nAligned UCSs\nUpdate \nParametersBeatles \nFeatures\nViterbi \nDecoder\nBeatles \nPredictionBeatles GTs\nPerformanceQueen Features\nQueen GTs\nÎ¸= {T, I, Î¼, Î£}\nÎ¸'= {T', I', Î¼', Î£'}Zweiek Features\nZweiek GTs\nUCS Features\nUCSsFigure 3 . The schematic of our experiments. Data are\nshown in square boxes, processes in curved. Detailed de-\nscriptions of the processes are found in the text.\n4. CURRICULUM LEARNING\nIn this section, we describe an addition to the scheme in\nFigure 3 which makes the most of the available data using\ncurriculum learning. We also outline our new evaluation\nmethod. We begin with some background information on\nthe subject.\n4.1 Background\nIt has been shown that humans and animals learn more ef-\nï¬ciently when training examples are presented in a mean-\ningful way, rather than in a homogeneous manner [8, 10].\nExploiting this feature of learners is referred to as Shaping\nin the animal training community and Curriculum Learning\nin the machine learning discipline [3].\nThe concept of the curriculum paradigm is that starting\nwith easy examples and slowly generalising leads to more\nefï¬cient learning, which can be realised in a machine learn-\n641Oral Session 8: Chord Analysis\ning setting by carefully selecting training data from a large\nset of examples. It was recently hypothesised that curricu-\nlum learning offers faster training (in both optimization and\nstatistical terms) in online training settings, owing to the\nway the learner wastes less time with noisy or harder to pre-\ndict examples, and that additionally guiding the training into\na desirable parameter space will lead to greater generaliza-\ntion [3].\nWe introduce an additional step into Figure 3 to deal with\ncurriculum learning in a novel way. Note that up to now we\nhave not deï¬ned what we understand by easy examples, or\nequivalently, how to sort the available examples into a series\nof increasing difï¬culty samples. Therefore, after the UCSs\nhave been aligned to the features, we will attempt to sort the\nexpansion set by appropriateness for learning. We propose\na new measure for evaluating how accurate the set AUCS\ncompared to its (unknown) ground truth annotations.\nThus we have the two following assumptions:\n1. Introducing â€˜easyâ€™ examples into the training set leads\nto faster learning.\n2. It is possible to estimate which training examples from\na varied set are â€˜easiestâ€™.\nWe will address these assumptions in the following sub-\nsection.\n4.2 Alignment Quality Proxy\nWhen we created the ETS , we were unable to evaluate how\nwell the UCSs aligned to the loudness-based chromagrams,\nsince the ground truths are not available for these songs.\nHowever, we were able to estimate the accuracy of the align-\nment in a different way.\nTo begin with, we noticed that many alignments con-\ntained only a few chords and were therefore extremely un-\nlikely to be accurate chord alignments. We therefore re-\nmoved all alignments which contained fewer than 5unique\nchords.\nAfter this pruning, we looked into a quantitative estimate\nfor the alignment quality. An output of the JA algorithm is\nthe log-likelihood of UCS correctly aligning to the loudness\nchroma. For each UCS âˆˆAUCS we used the log-likelihood\nof the alignment normalised by the length of the alignment\nas a proxy for the performance, and stored these in the align-\nment quality proxy vector Paqp:\nPi\naqp=log-likelihood of AUCS i\n|AUCS i|,i= 1...|AUCS|\nThe results of the Alignment Quality Proxy performances\non our songs are displayed as a histogram in Figure 4. There\nis a range fromâˆ’1.79(very poor alignment) to 7.03(excel-\nlent alignment), and we notice a skew towards good quality\nalignments.\nâˆ’2âˆ’1012345678020406080100120Frequency\nAlignment Quality Proxy PaqpFigure 4 . Histogram of our proposed alignment quality\nmeasure.\nWe then sorted the ETS with respect to Paqpand seg-\nmented the set into bands according to alignment perfor-\nmance. In order to investigate the quality of the proposed\nalignment performance we ran JA on 173 Beatles songs for\nwhich we had UCSs, with the alignment parameters from\nQueen and Zweieck, yielding PB\naqp. We also used these pa-\nrameters to make an HMM prediction for each of the 173\nsongs and measured the performance PBof these predic-\ntions against the Beatles ground truth sequences.\nFinally, we measured the correlation between the PB\naqp\nandPBusing Pearsonâ€™s linear correlation coefï¬cient, which\ngave a correlation of 0.73with ap-value of 0.4Ã—10âˆ’30,\nindicating a highly signiï¬cant result at the 5%level (p <\n0.05). This result indicates that Paqpis an excellent proxy\nfor alignment accuracy, i.e. we have answered assumption 2\nin Subsection 4.1 in the afï¬rmative.\nSatisï¬ed that Paqpoffers an approximation of how well\nJA aligns UCSs, we decreased the size of the ETS by placing\na threshold on the alignment quality. Mathematically, we al-\nlowed theithchromagram and aligned UCS pair {Ci,AUCSi}\ninto the training set if\nPi\naqpâ‰¥Î³\nforÎ³âˆˆR. The value Î³=âˆ’âˆ corresponds to being care-\nfree with our data - all training examples are included. If\n64212th International Society for Music Information Retrieval Conference (ISMIR 2011)\nwe wish to be stringent with our data, selecting a large Î³\nwill only allow high-quality alignments into the training set,\nalthough we may suffer from lack of examples in this sce-\nnario.\n5. EXPERIMENTS\n5.1 Simple Chord Prediction\nIn our ï¬rst experiment we set the alphabet Ato consist of\nmajor and minor chords, along with a â€˜No Chordâ€™ symbol.\nWe refer to this alphabet as minmaj . All chords in the Core\nTraining Set CTS and Expanded Training Set ETS were\nmapped to minor chords if they contained a minor third, oth-\nerwise they were mapped to the corresponding major chord.\nâ€˜No Chordâ€™ symbols were added to the beginning and end\nof each of the Untimed Chord Sequences in UCS to account\nfor the silences at the beginning and end of the pieces.\nTo re-iterate, we trained an HMM on the ETS and tested\non all 180 Beatles songs. Performance was measured by\nnumber of correctly identiï¬ed frames divided by the number\nof frames (Ã—100% ), averaged over the 180 songs, and are\nshown in Table 1.\nThe results seen in Table 1 seem initially discouraging.\nThe peak performance of 77.87% obtained using the 1021\nbest UCSs (in terms of alignment performance) only achieved\nan increase of 0.84%. However, upon performing a one-\nsided t-test of the performance of the system against the\nbaseline performances (no expansion set), we obtained a p-\nvalue of 0.0435 , indicating signiï¬cance at the 5%level.\nUsing additional data in a system which is already per-\nforming well is unlikely to offer a large performance in-\ncrease, since there is not much to be gained. On the con-\ntrary, when the difï¬culty of the task increases it is possible\nthat extra data becomes beneï¬cial. To investigate whether\nthis is the case, we will increase the complexity of the model\nby using a larger library of chords.\n5.2 Complex Chord Prediction\nThe results of subsection 5.1 showed that there is not much\nto be gained by using additional data sources on a simple\nchord model. To counteract this, we conducted the same\nexperiments using an unrestricted chord alphabet A=full.\nThis meant that each unique chord in the Core and Expanded\ntraining sets were considered a unique state of our model,\nas well as the transpositions of each of these chords into\neach root pitch. This left us with 253 states, one order of\nmagnitude larger than the major/minor chord alphabet.\nAs before we then retrained on the Expanded Set and\ntested on The Beatles. The results were measured as in Sub-\nsection 5.1. Figure 5 shows the results as well as the number\nof songs in the expansion set for each cut-off.\nFigure 5 . Performance of our model on The Beatles dataset\nwith increasing alignment quality threshold quality Î³. The\nbaseline performance ( Î³=âˆ) is shown as a dashed line.\nValues ofÎ³for which the performance approaches or ex-\nceeds the baseline is shown in higher resolution steps of 0.2\nincrements. Randomizations of the same expansion set size\nare shown in the dot-and-dashed line.\nImmediately from Figure 5 we see that blindly adding all\nof the available does not improve recognition. This is due\nto the large variety in style and genre seen in the database,\nalong with the potentially poor alignments which we in-\ncluded in the expansion set when Î³is small. Upon increas-\ningÎ³we allowed heuristically better quality alignments into\nthe training set, and saw a rapid increase in recognition ac-\ncuracy, which peaks at 58.52%,3.54% above the baseline\nof54.98%. Although this increase may seem incremental,\nwe performed a one-sided t-test of the performance of the\nsystem against the baseline at the optimal Î³of5and found\nthep-value to be 1.28Ã—10âˆ’7, indicating a signiï¬cant im-\nprovement at 5%conï¬dence level. This corresponded to an\nimprovement of 114 of the 180 songs.\nTo see if curriculum learning genuinely offered improve-\nments over homogeneous learning, we also included aligned\nUCSs into the training set in random batches of the same\nsize as the previous experiment, and repeated 100 times to\naccount for random variations. The mean and standard devi-\nations over the 100 repeats are shown as the dot-and-dashed\nline and bars in Figure 5. We can see that the speciï¬c order-\ning of the expansion set in section 4.2 offers substantial im-\nprovement over randomly selecting the expansion set. This\nis good evidence that curriculum learning is the method of\nchoice for navigating a large set of training examples, and\nalso demonstrates that assumption 1 in Subsection 4.1 holds.\n643Oral Session 8: Chord Analysis\nAlignment Quality threshold Î³ -2 -1 0 1 2 3 4 5âˆ\nNumber of Expansion songs |AUCS| 1027 1027 1021 993 899 705 390 67 0\nPerformance (%) 77.83 77.83 77.87 77.81 77.77 77.53 76.94 76.79 76.79\np-value of paired t-test 0.0516 0.0516 0.0435 0.0555 0.0561 0.1137 0.4779 0.6906 -\nTable 1 . Performance of our model on the simple chord alphabet, A=minmaj.Î³ increases to the right, with the number of\nexpansion songs this corresponds to underneath. Performances and corresponding p-values between the difference between the\nbaseline level Î³=âˆare shown in the ï¬nal two rows. Results which are signiï¬cant at the 5%level are shown in bold.\n6. CONCLUSIONS\nIn this paper we have made three breakthroughs. First of\nall, we demonstrated that chord databases can be used to\ncreate new sequences for training chord recognition algo-\nrithms. These sequences were shown to signiï¬cantly im-\nprove recognition accuracy on an unseen test set.\nAlso, we demonstrated a new technique for estimating\nthe quality of aligned chord sequences, which can be used to\nselect training examples from a large, noisy training data set.\nThis estimate allowed us to perform curriculum learning,\nwhich achieved faster learning and improved results.\nFinally, we also showed that with more data we are able\nto make a more complex chord model, which led to a more\nsigniï¬cant improvement in recognition accuracy. In order\nto gain the most from these data we plan to further increase\nthe complexity of the decoding model, by including distinct\nfeatures for the bass and treble frequency range [14], includ-\ning a hidden â€˜key chainâ€™ to model modulations [18] or using\nmore complex emission probability models.\n7. REFERENCES\n[1] A. Anglade, R. Ramirez, and S. Dixon. Genre classiï¬ca-\ntion using harmony rules induced from automatic chord\ntranscriptions. In Proc. ISMIR , 2009.\n[2] J.P. Bello. Audio-based cover song retrieval using ap-\nproximate chord sequences: testing shifts, gaps, swaps\nand beats. In Proc. ISMIR , pages 239â€“244, 2007.\n[3] Y . Bengio, J. Louradour, R. Collobert, and J. We-\nston. Curriculum learning. In Proc. ICML , pages 41â€“48.\nACM, 2009.\n[4] J. Brown. Calculation of a constant q spectral trans-\nform. Journal of the Acoustical Society of America ,\n89(1):425â€“434, 1991.\n[5] O. Chapelle, B. Sch Â¨olkopf, and A. Zien, editors. Semi-\nSupervised Learning . MIT Press, Cambridge, MA,\n2006.\n[6] D. Ellis and G. Poliner. Identifying â€˜cover songsâ€™ with\nchroma features and dynamic programming beat track-\ning. In Proc. of ICASSP , pages 1429â€“1433, 2007.[7] D. Ellis and A. Weller. The 2010 LABROSA chord\nrecognition system. In Proc. of ISMIR (MIREX submis-\nsion) , 2010.\n[8] J.L. Elman. Learning and development in neural net-\nworks: the importance of starting small. Cognition ,\n48(1):71â€“99, 1993.\n[9] T. Fujishima. Realtime chord recognition of musical\nsound: A system using common lisp music. In Proc.\nICMC, 1999 , pages 464â€“467, 1999.\n[10] K.A. Krueger and P. Dayan. Flexible shaping: How\nlearning in small steps helps. Cognition , 110(3):380â€“\n394, 2009.\n[11] O. Lartillot and P. Toiviainen. A matlab toolbox for mu-\nsical featureextraction from audio. In International Con-\nference on Digital Audio Effects , 2007.\n[12] K. Lee and M. Slaney. Automatic chord recognition\nfrom audio using an HMM with supervised learning. In\nProc. of ISMIR , 2006.\n[13] R. Macrae and S. Dixon. Guitar Tab Mining, Analysis\nand Ranking. In ISMIR 2011, Miami, Florida , 2011.\n[14] M. Mauch. Automatic chord transcription from audio\nusing computational models of musical context . PhD\nthesis, Queen Mary University of London, 2010.\n[15] M. McVicar, Yizhao. Ni, R. Santos-Rodriguez, and\nT. De Bie. Using online chord databases to enhance\nchord recognition. JNMR, special issue on music and\nmachine learning , 2011.\n[16] Y . Ni, M. Mcvicar, R. Santos-Rodriguez, and T. De Bie.\nAn end-to-end machine learning system for harmonic\nanalysis of music. In http://arxiv.org/abs/1107.4969v1 ,\n2011.\n[17] T. Rocher, M. Robine, P. Hanna, and L. Oudre. Con-\ncurrent Estimation of Chords and Keys from Audio. In\nProc. ISMIR , 2010.\n[18] T. Rocher, M. Robine, P. Hanna, L. Oudre, Y . Gre-\nnier, and C. F Â´evotte. Concurrent estimation of chords\nand keys from audio. In Proc. of ISMIR , pages 141â€“146,\n2010.\n644"
    },
    {
        "title": "Accelerating The Mixing Phase In Studio Recording Productions By Automatic Audio Alignment.",
        "author": [
            "Nicola Montecchio",
            "Arshia Cont"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1415228",
        "url": "https://doi.org/10.5281/zenodo.1415228",
        "ee": "https://zenodo.org/records/1415228/files/MontecchioC11.pdf",
        "abstract": "We propose a system for accelerating the mixing phase in a recording production, by making use of audio alignment techniques to automatically align multiple takes of excerpts of a music piece against a performance of the whole work. We extend the approach of our previous work, based on sequential Montecarlo inference techniques, that was targeted at real-time alignment for score/audio following. The proposed approach is capable of producing partial alignments as well as identifying relevant regions in the partial results with regards to the reference, for better integration within a studio mix workflow. The approach is evaluated using data obtained from two recording sessions of classical music pieces, and we discuss its effectiveness for reducing manual work in a production chain.",
        "zenodo_id": 1415228,
        "dblp_key": "conf/ismir/MontecchioC11",
        "keywords": [
            "accelerating",
            "audio alignment",
            "multiple takes",
            "music piece",
            "performance",
            "Montecarlo inference",
            "real-time alignment",
            "score/audio following",
            "partial alignments",
            "relevant regions"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nACCELERATING THE MIXING PHASE IN STUDIO RECORDING\nPRODUCTIONS BY AUTOMATIC AUDIO ALIGNMENT\nNicola Montecchio\nUniversity of Padova\nDepartment of Information Engineering\nnicola.montecchio@dei.unipd.itArshia Cont\nInstitut de Recherche et Coordination\nAcoustique/Musique (IRCAM)\narshia.cont@ircam.fr\nABSTRACT\nWe propose a system for accelerating the mixing phase in\na recording production, by making use of audio alignment\ntechniques to automatically align multiple takes of excerpts\nof a music piece against a performance of the whole work.\nWe extend the approach of our previous work, based on se-\nquential Montecarlo inference techniques, that was targeted\nat real-time alignment for score/audio following. The pro-\nposed approach is capable of producing partial alignments\nas well as identifying relevant regions in the partial results\nwith regards to the reference, for better integration within\na studio mix workï¬‚ow. The approach is evaluated using\ndata obtained from two recording sessions of classical music\npieces, and we discuss its effectiveness for reducing manual\nwork in a production chain.\n1. INTRODUCTION\nThe common practice in productions of studio recordings\nconsists of several phases. At ï¬rst the raw audio material\nis captured and stored on a support. This material is sub-\nsequently combined and edited in order to produce a mix,\nwhich is ï¬nalized in the mastering phase for commercial\nrelease. Nowadays, the whole process revolves around a\ncomputer Digital Audio Workstation (DAW).\nIn the case of instrumental recording, the initial task in-\nvolves capturing a complete reference run-through of the en-\ntire piece, after which additional takes of speciï¬c sections\nare recorded to allow the mixing engineer to mask perfor-\nmance mistakes or reduce eventual environmental noises.\nThe role of a mixing engineer is to integrate these takes\nwithin the global reference in order to achieve a seamless\nï¬nal mix [2]. The ï¬rst step in preparing a mix session con-\nsists in arranging the takes with regards to the global ref-\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proï¬t or commercial advantage and that copies\nbear this notice and the full citation on the ï¬rst page.\nc/circlecopyrt2011 International Society for Music Information Retrieval.erence. Figure 1 shows a typical DAW session prepared\nout of a reference run-through (the top track) and additional\ntakes aligned appropriately. Those takes usually require fur-\nther cleanup as they commonly include noise or conversa-\ntion that are not useful for the ï¬nal mix. This means that,\nin addition to alignment, the mixing engineer identiï¬es cut-\npoints for each take that correspond to relevant regions in\nthe reference. The additional takes are ï¬nally blended with\nthe reference by crossfading short overlapping audio regions\nto avoid perceptual discontinuities.\nFigure 1 . A typical DAW mixing session.\nThe purpose of this work is to facilitate the process of\nmixing by integrating automatic (audio to audio) alignment\ntechniques into the production chain. Special care is taken to\nconsider existing practices within the workï¬‚ow, such as au-\ntomatic identiï¬cation of interest points. In contrast to most\nliterature on audio alignment, we are concerned with two\nessential aspects: the ability to identify a partial alignment\nwith an unknown starting position and the detection of re-\ngions of interest inside the alignment. Moreover our ap-\nproach permits to achieve different degrees of accuracy de-\npending on efï¬ciency requirements.\nUsing audio material collected from two real-life record-\ning sessions, we show that it is possible to optimize the op-\nerations of sound engineers by automating time-consuming\ntasks. We further discuss how such framework can be inte-\ngrated pragmatically within common DAW software.\n627Oral Session 7: Structure Analysis and Mixing\n2. RELATED WORK\nAt the application level, alignment techniques were already\nintroduced in the literature in [3]. Alignment of audio to\nthe symbolic representation of a piece was integrated into\nthe workï¬‚ow, permitting the automation of the editing pro-\ncess through operations such as pitch and timing correc-\ntions. The application of these approaches is precluded in\nthe present context by the requirement of accessing a sym-\nbolic representation of the music. Nonetheless, despite this\nlimitation, the work provides important insights in the inte-\ngration within a DAW setup.\nAt the technological level, audio alignment has often been\nthe subject of extensive research; an overview of classical\napproaches in literature can be found in [6]. In contrast to\ntraditional methods, an important aspect of this work is the\nconsideration of partial results and detection of interest re-\ngions . An audio alignment method with similar aims was\nintroduced in [7], that explicitly deals with the synchroniza-\ntion of recordings that have different structural forms.\n3. GENERAL ARCHITECTURE\nThe proposed methodology was devised assuming that a\ngeneric algorithm is available that is capable of aligning\naudio sequences without a known starting position. Even\nthough methods such as HMM or DTW [4] could have been\nused for this aim, we chose to exploit our previous work [6]\non sequential Montecarlo inference because of its straight-\nforward applicability to the present context, its ï¬‚exibility\nregarding the degree of accuracy given by the availability of\nsmoothing algorithms and the possibility to trade accuracy\nfor computational efï¬ciency in an direct way.\nIn the ï¬rst phase a rough alignment is produced as in Fig-\nure 2(a); the initial uncertainty in the alignment is due to the\nfact that the initial position is not known a priori. In a second\nphase we identify a sufï¬ciently long region of the alignment\nthat can be reasonably approximated by a straight line, as in\nFigure 2(b); this region intuitively corresponds to the â€œcor-\nrectâ€ section of the alignment. These two phases solve the\ntask of placing the takes along the reference (Figure 1).\nThe remaining steps address the tasks in which a more\naccurate alignment is required. In the third phase, the ini-\ntial portion of the alignment is corrected, starting from a\nposition inside the region found in the previous phase and\nusing a reversed variant of the alignment algorithm (Fig-\nure 2(c)). Finally, a reï¬ned alignment is produced by ex-\nploiting a smoothing algorithm for sequential Montecarlo\ninference, as shown in Figure 2(d).\n4. METHODOLOGY\nThe four phases described in the previous section are high-\nlighted in Figure 2 and described below in detail.\n0 10 20 30 401900 1920 1940 1960\ntake time [s]reference time [s]p(x0) =U(0, L)(a) Initial alignment, using sequential Montecarlo inference.\n0 10 20 30 401900 1920 1940 1960\ntake time [s]reference time [s]convergence region\ninterest region\n(b) Identiï¬cation of the interest region of the alignment.\n0 10 20 30 401900 1920 1940 1960\ntake time [s]reference time [s]p(x(b)\nB) = diag(1 ,âˆ’1)p(xB|z0. . . z B)\n(c) Correction of the beginning of the alignment.\n0 10 20 30 401910 1930 1950\ntake time [s]reference time [s]\np(x(fb)\n0) = diag(1 ,âˆ’1)p(x(b)\n0|zB. . . z 0)\n(d) Final alignment obtained using smoothed inference.\nFigure 2 . Alignment methodology.\n62812th International Society for Music Information Retrieval Conference (ISMIR 2011)\n4.1 Initial Alignment\nThe alignment problem is formulated as the tracking of an\ninput data stream along a reference, using motion equations.\n4.1.1 System State Representation\nThe system state is modeled as a two-dimensional random\nvariablex= (s,t), representing the current position in the\nreference audio and tempo respectively; sis measured in\nseconds and tis the speed ratio of the performances. The\nincoming signal processing frontend is based on spectral\nfeatures extracted from the FFT analysis of an overlapping,\nwindowed signal representation, with hop size âˆ†T. In order\nto use sequential Montecarlo methods to estimate the hidden\nvariablexk= (sk,tk)using observation zkat time frame k,\nwe assume that the state evolution is Markovian.\n4.1.2 Observation Modeling\nLetp(zk|xk)denote the likelihood of observing an audio\nframezkof the take given the current position along the ref-\nerence performance sk. We consider a simple spectral sim-\nilarity measure, deï¬ned as the Kullback-Leibler divergence\nbetween the power spectra at frame kof the take and at time\nskin the reference.\n4.1.3 System State Transition Modeling\nLetp(xk|xkâˆ’1)denote the pdf for the state transition; we\nmake use of tempo estimation in the previous frame, assum-\ning that it does not change too quickly:\np(xk|xkâˆ’1) =N(/bracketleftbiggsk\ntk/bracketrightbigg\n|Âµk,Î£)\nÂµk=/bracketleftbiggskâˆ’1+ âˆ†Ttkâˆ’1\ntkâˆ’1/bracketrightbigg\nÎ£ =/bracketleftbiggÏƒ2\nsâˆ†T 0\n0Ïƒ2\ntâˆ†T/bracketrightbigg\nIntuitively, this corresponds to a performance where tempo\nis rather steady but can ï¬‚uctuate; the parameters Ïƒ2\ntandÏƒ2\ns\ncontrol respectively the variability of tempo and the pos-\nsibility of local mismatches that do not affect the overall\ntempo estimate.\n4.1.4 Inference Algorithm\nSequential Montecarlo inference methods work by recur-\nsively approximating the current distribution of the system\nstate using the technique of Sequential Importance Sam-\npling: a random measure {xi\nk,wi\nk}Ns\ni=1is used to charac-\nterize the posterior pdf with a set of Nsparticles over the\nstate domain and associated weights, and is updated at each\ntime step as in Algorithm 1. In particular, q(xk|xkâˆ’1,zk)is\nthe particle sampling function. In our implementation this\ncorresponds to the transition probability density function; in\nthis case the algorithm is known as condensation algorithm.An optional resampling step is used to address the de-\ngeneracy problem, common to particle ï¬ltering approaches;\nthis is discussed in detail in [1,5] and in the next paragraph.\nThe decoding of position and tempo is carried out by\ncomputing the expected value of the resulting random mea-\nsure (which is efï¬ciently computed as E[xk] =/summationtextNs\ni=1xi\nkwi\nk).\nAlgorithm 1 : SIS Particle Filter - Update step\nfori= 1...N sdo\nsamplexi\nkaccording to q(xi\nk|xi\nkâˆ’1,zk)\nË†wi\nkâ†wi\nkâˆ’1p(zk|xi\nk)p(xi\nk|xi\nkâˆ’1)\nq(xi\nk|xi\nkâˆ’1,zk)\nwi\nkâ†Ë†wi\nkP\njË†wj\nkâˆ€i= 1...N s\nNeffâ†(/summationtextNs\ni=1(wi\nk)2)âˆ’1\nifNeff<resampling threshold then\nresamplex1\nk...xNs\nkaccording to ddf w1\nk...wNs\nk\nwi\nkâ†Nâˆ’1\nsâˆ€i= 1...N s\n4.1.5 Initialization\nInitialization plays a central role in the performance of the\nalgorithm; in a probabilistic context this corresponds to an\nappropriate choice of the prior distribution p(x0).\nIn a real-time setup the player is expected to start the per-\nformance at a well known point of the reference; this fact is\nexploited in the design of the algorithm by setting an appro-\npriately shaped prior distribution, typically a low-variance\none around the beginning.\nIn the proposed situation however the initial point is not\nknown (it represents indeed the aim of our interest). To cope\nwith this, the prior distribution p(x0)is set to be uniform\nover the whole duration Lof the reference performance; the\nalgorithm is expected to â€œconvergeâ€ to the correct position\nafter a few iterations. Figure 3 shows the evolution of the\nprobability distribution for the position of the input at dif-\nferent moments of the alignment.\n4.1.6 Degeneracy Issues w.r.t. Realtime Alignment\nA relevant parameter of Algorithm 1 is the resampling thresh-\nold. The variable Neff, commonly known as effective sam-\nple size , is used to estimate the degree of degeneracy which\naffects the random measure; degeneracy is related to the\nvariance of the weights {wi\nk}Ns\n1, and it is proven to be al-\nways increasing in absence of resampling. In a degenerate\nsituation most particles have close-to-zero weight, resulting\nin most of the computation being spent in updating parti-\ncles which are subject to numerical approximation errors.\nResampling is introduced to obviate this issue. Intuitively,\nresampling replaces a random measure of the true distribu-\ntion with an equivalent one (in the limit of Nsâ†’âˆ ) that is\nbetter suited for the inference algorithm. Since resampling\n629Oral Session 7: Structure Analysis and Mixing\nref. time [s]0 100 200 300 400 500 600(a) prior distribution ( k= 0)\nref. time [s]0 100 200 300 400 500 600\n(b)k= 1\nref. time [s]0 100 200 300 400 500 600\n(c)k= 10\nref. time [s]125 130 135 140\n(d)k= 50\nFigure 3 . Evolution of p(sk|z1...zk).\nintroduces other problems (in particular, sample impover-\nishment , i.e., a small number of particles is selected multi-\nple times) its usage should be limited, thus producing the\nnecessity for a threshold on the effective sample size.\nIn the real-time score following case [6] the mass of the\ndistribution is always concentrated around a small region\nof the domain thus allowing the resampling threshold to be\nrelatively low. In contrast, in a situation such as the one\ndepicted in Figure 3, the sparsity of the distribution in the\ninitial phases of the alignment imposes a much higher re-\nsampling threshold, otherwise many relevant hypotheses are\nsoon lost in the resampling phase and cannot be recovered.\n4.2 Identiï¬cation of the Interest Region\nThis phase aims at identifying a region of the alignment ob-\ntained previously where it is certain that the alignment is\nindeed â€œcorrectâ€. As depicted in Figure 2(b), a typical align-\nment can be subdivided into two regions, the ï¬rst one being\ncharacterized by irregular oscillations (because not enough\ndata has been observed yet in order to select the most proba-\nble hypothesis with enough conï¬dence) and the second one\nresembling a straight line; we will refer to the former as\nconvergence region and to the latter as interest region .\nAs can be inferred by observing the plot in Figure 2(b),\nthe most important characteristic of the interest region is its\nslope. From a technical point of view, the slope should be\nas constant as possible for the alignment region to be sig-\nniï¬cant. From a musical perspective it should be roughly\nunitary, implying that the performance tempos of the single\ntake and the reference are approximately the same. In ad-\ndition to that, the duration of the interest region should be\nlong enough to discard noisy sections of the alignment.\nThe interest region is identiï¬ed in the following man-\n0 10 20 30 40\ntake time[s]reference time[s]\n1920 1940 1960 1980Figure 4 . Identiï¬cation of the interest region.\nner: each of many initial candidate regions w1...w Wis\niteratively expanded as long as it meets the criteria exposed\nabove; the longest of the resulting intervals is elected as the\ninterest region, unless none of them matches the require-\nments, in which case the alignment is identiï¬ed as incor-\nrect. The process described above is depicted in Figure 4\n(dashed horizontal lines represent the regions progressively\nexamined by the algorithm) and formalized in Algorithm 2.\nAlgorithm 2 : Identiï¬cation of interest region\nw1,...,w Wâ†regularly spaced intervals in [0,L]\ncandidatesâ†âˆ… fori= 1...W do\nwhile|wi|<L do\nwiâ†max(0,wstart\niâˆ’âˆ†T),min(L,wend\ni+âˆ†T)\naiâ†slope of LS-ï¬t line for points in wi\neiâ†mean difference with LS-ï¬t line in wi\nifaiâˆˆ[1âˆ’âˆ†A,1 + âˆ†A]âˆ©ei<âˆ†Ethen\ncandidatesâ†candidatesâˆªi\nelse\nbreak\nif|candidates|>0then\ninterest regionâ† max\niâˆˆcandidateswi\nelse\nalignment is incorrect\n4.3 Correction of the Convergence Region\nIn order to ï¬x the convergence region of the alignment, we\nexploit again the sequential Montecarlo inference method-\nology of 4.1, with some adaptations. The general idea is\nto run the algorithm â€œbackwardsâ€, i.e., to align the time-\nreversed audio streams, starting from a point in the previous\nalignment that is known to be correct.\nThe starting point Bis chosen inside the region of in-\nterest. The prior distribution for the backward alignment\nis equal to that of the forward alignment at B, however\nwith the value of the velocity for each particle inverted:\np(x(b)\nB) =diag(1,âˆ’1)p(xB|z0...zB). The audio stream of\n63012th International Society for Music Information Retrieval Conference (ISMIR 2011)\nthe take is then reversed and processed by Algorithm 1, as in\nFigure 2(c). Experimentation shows that a narrow uniform\nor gaussian prior centered in (B,âˆ’1)Tare for practical pur-\nposes equivalent to the form of p(x(b)\nB)mentioned above.\n4.4 Smoothing Inference\nSequential Montecarlo inference algorithms are typically for\nonline estimation; this implies that at each instant only the\ninformation about the past is exploited, instead of the whole\nobservation sequence. In the context of an ofï¬‚ine appli-\ncation however these real-time constraints can be dropped.\nBoth the Forward/Backward and Viterbi inference algorithms\ncan be deduced, respectively estimating the probability dis-\ntribution at each instant given the full observation sequence\nand the Maximum A Posteriori alignment. The running time\nof both algorithms is quadratic in the number of particles,\nhowever this issue can be mitigated by an appropriate choice\nof the prior distribution p(x(fb)\n0)such as a resampling of\ndiag(1,âˆ’1)p(x(b)\n0)with a smaller numer of samples.\n5. EV ALUATION\nAn ideal evaluation of the efï¬cacy of the proposed method-\nology in the context discussed in Section 1 should aim at\nmeasuring the amount of work saved in production with re-\nspect to the current workï¬‚ow. A discussion of our current\nwork in this area is presented in Section 6.\nBelow we evaluate the efï¬cacy of the proposed approach\nregarding the initial phase of laying out the takes as in Fig-\nure 1. The accuracy of the alignment in terms of latency and\naverage error was evaluated in our previous work [6]; a sim-\nilar analysis could not be performed in this case, due to the\nlack of a (manually annotated) reference linking the timings\nof each musical event for all takes to the reference record-\ning. Moreover, in this situation the aim is rather to position\ncorrectly the highest number of takes against the reference,\nrather than to align them with the highest possible precision.\n5.1 Dataset description\nWe collected the recordings produced in two real-life ses-\nsions by different groups of sound engineers, consisting of\napproximately 3 hours of audio data. The ï¬rst one is a\nrecording session of the second movement of J. Brahmsâ€™\nsextet op. 18; the second one was produced shortly after the\npremiere of P. Manouryâ€™s â€œTensioâ€, for string quartet and\nlive electronics, in December 2010. Table 1 summarizes\ntheir characteristics.\n5.2 Experimental Results\nWe performed the alignment of each take in the two databases\naccording to the procedure introduced in Section 4. We se-\nlect the center point of the interest region identiï¬ed in thedataset n. of rec. duration [s]\nref. takes (avg,std) total\nBrahms 20 + ref. 588.8 112.8, 92.0 2844.0\nManoury 49 + ref. 2339.4 113.5, 94.0 7900.4\nTable 1 . Datasets used for evaluation.\nsecond phase as the alignment reference for the whole take\n(we do not performs the optional two last steps).\nIn all the test we executed, we set the number of particles\nNsto be proportional to the duration of the reference (60\nparticles per second). Our implementation aligns a minute\nof audio in 2.29s for Ns= 105on a laptop computer with a\n2.4 Ghz Intel i5 processor (a single core is used).\n5.2.1 Brahms Dataset\nFor this dataset, a manual placement of all the takes with\nrespect to the reference recording was performed using a\nmusical score, in order to evaluate the correctness of the au-\ntomatic procedure. Aural inspection of the data showed that\nnone of the recordings but one presented undesired noises.\nAll the takes but one were correctly aligned. In the un-\nsuccessful case, the length of the recording itself was one\nsecond shorter than the minimum length for an interest re-\ngion (15s); using last alignment point as a reference, the\nplacement of this take also results to be correct.\n5.2.2 Manoury Dataset\nThe dataset contains a complete run-through and 49sepa-\nrate takes. The particularity of this dataset is the presence\nof undesired material for the ï¬nal mix in many of the indi-\nvidual takes (such as speech, practice sessions, volume and\ncalibration tests). Out of 49takes, 14contain exclusively\nnoise and 21partially. In the former case we consider the\nalignment correct if the ï¬le is discarded, in the latter we aim\nat aligning correctly the interesting portion of the take. This\nis in sharp contrast with the â€œcleannessâ€ of the Brahms set\nand presents difï¬culties that were not foreseen when formu-\nlating the alignment procedure.\nContrarily to the Brahms dataset, the evaluation of the\nalignment precision was done a posteriori: instead of per-\nforming a manual alignment in advance, the results of the\nautomatic alignment were checked. The reason for this lies\nbehind the length (approximately 40 minutes) and complex-\nity of the music: even with the score at our disposal, it was\nimmediately evident that a manual alignment would have\ntaken a very long time. It is precisely this difï¬culty that\nsound engineers had to face.\nOur ï¬rst experiment aligning this dataset yielded rather\npoor results on the 21ï¬les containing noise regions of sig-\nniï¬cant length (in some cases up to more than one minute);\nsince in almost all cases the noisy portion was at the begin-\nning, we decided to directly align the reversed audio streams\n631Oral Session 7: Structure Analysis and Mixing\nin the ï¬rst phase. With this simple adaptation the results are\nas follows: of the 35ï¬les containing interesting regions, 26\nwere correctly aligned; all of the 14takes that contained ex-\nclusively noises were correctly discarded by the algorithm.\nThe absence of false positives (no noise-only takes were\nmistakenly aligned) and the correct positioning of all the\naligned ï¬les suggest that the simple algorithm for identiï¬-\ncation of the interest region is robust enough to be applied\nto rather short audio segments, yielding the possibility of re-\npeating the alignment algorithm multiple times on different\nsubregions of the audio in order to avoid noisy sections.\n6. WORKFLOW ADAPTATION\nThe audio industry has established over the years common\nstandards for mixing that are adopted in most professional\nstudio records worldwide. Integration of new technologies\nwithin existing workï¬‚ow therefore requires special attention\nto existing practices within the community. To this end, we\nconducted several interviews with sound engineers.\nFrom an R&D standpoint, an ideal integration would be\na direct implementation of this technology into the graphi-\ncal user interface of common DAW softwares to maximize\nusability. Such integration would allow novel possibilities,\nsuch as linking two tracks by means of their alignment and\ndeï¬ning the placement of transition points between them\nfor crossfading, avoiding any destructive editing regarding\nthe discarded audio regions. Such integration requires di-\nrect contact with software houses which are mostly close to\npublic domain development.\nAn alternative solution is represented by standalone align-\nment tools, whose outputs should be directly importable into\na commercial DAW. Virtually all the major DAWs and video\npost production systems support the Open Media Frame-\nwork (OMF) and the Advanced Authoring Format (AAF),\nrespectively owned by Avid Technology, Inc. and by the\nAdvanced Media Workï¬‚ow Association (AMWA)1. These\nare employed as interchange formats to allow interoperabil-\nity between different software. An alignment software, that\nwe are currently developing, could automatically construct\nan initial session using an interchange format that audio en-\ngineers can use in their DAW to start the mixing process.\n7. CONCLUSIONS AND FUTURE WORK\nIn this paper, we attempted to address two issues: Intro-\nducing novels tools generalizing audio matching algorithms\nto partial alignment with relevant region detection, and their\nintegration within realistic studio mixing procedure to accel-\nerate mixing session preparation for audio engineers. The\nï¬rst task involves adapting audio alignment techniques to\nsituations where there is no speciï¬c prior knowledge on the\n1http://www.avid.com ,http://www.amwa.tvstarting point of the alignment. Such considerations would\nallow audio engineers to automatically obtain a global view\nof many different individual takes with regards to a refer-\nence run-through recording in a typical recording session, as\nwell as providing access to relevant parts within each take;\nthis is a time-consuming task if done manually. We further\ndiscussed how this procedure can realistically be integrated\ninto common mixing workï¬‚ows.\nApplications of the proposed technology are not limited\nto the preparation of the initial mixing session: mid-level in-\nformation obtained during the alignment task can in fact be\nfurther integrated in a studio mixing workï¬‚ow. For exam-\nple, our audio alignment provides useful information about\nthetempo of a performance with regards to the reference\nthat can be employed as an important factor for the mix-\ning engineer. Such integration requires further collaboration\nwith audio engineers to determine an optimal exploitation of\nthese informations in the context of existing practices.\n8. REFERENCES\n[1] M. S. Arulampalam, S. Maskell, and N. Gordon. A\nTutorial on Particle Filters for Online Nonlinear/Non-\nGaussian Bayesian Tracking. IEEE Transactions on Sig-\nnal Processing , 50:174â€“188, 2002.\n[2] B. Bartlett and J. Bartlett. Practical Recording Tech-\nniques: The step-by-step approach to professional audio\nrecording . Focal Press, 2008.\n[3] R. Dannenberg and N. Hu. Polyphonic Audio Match-\ning for Score Following and Intelligent Audio Editors.\nProc. of the International Computer Music Conference\n(ICMC) , 2003.\n[4] S. Dixon and G. Widmer. Match: a Music Alignment\nTool Chest. Proc. of the 6th International Conference\non Music Information Retrieval (ISMIR) , 2005.\n[5] R. Douc, O. Cappe, and E. Moulines. Comparison of\nResampling Schemes for Particle Filtering. In Proc. of\nthe 4th International Symposium on Image and Signal\nProcessing and Analysis (ISPA) , 2005.\n[6] N. Montecchio and A. Cont. A Uniï¬ed Approach to Real\nTime Audio-to-Score and Audio-to-Audio Alignment\nUsing Sequential Montecarlo Inference Techniques. In\nProc. of the IEEE International Conference on Acous-\ntics, Speech, and Signal Processing (ICASSP) , 2011.\n[7] M. M Â¨uller and D. Appelt. Path-Constrained Partial Mu-\nsic Synchronization. In Proc. of the 34th International\nConference on Acoustics, Speech and Signal Processing\n(ICASSP) , 2008.\n632"
    },
    {
        "title": "Chroma Toolbox: Matlab Implementations for Extracting Variants of Chroma-Based Audio Features.",
        "author": [
            "Meinard MÃ¼ller",
            "Sebastian Ewert"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1416032",
        "url": "https://doi.org/10.5281/zenodo.1416032",
        "ee": "https://zenodo.org/records/1416032/files/MullerE11.pdf",
        "abstract": "Chroma-based audio features, which closely correlate to the aspect of harmony, are a well-established tool in processing and analyzing music data. There are many ways of computing and enhancing chroma features, which results in a large number of chroma variants with different properties. In this paper, we present a chroma toolbox [13], which contains MATLAB implementations for extracting various types of recently proposed pitch-based and chroma-based audio features. Providing the MATLAB implementations on a welldocumented website under a GNU-GPL license, our aim is to foster research in music information retrieval. As another goal, we want to raise awareness that there is no single chroma variant that works best in all applications. To this end, we discuss two example applications showing that the final music analysis result may crucially depend on the initial feature design step.",
        "zenodo_id": 1416032,
        "dblp_key": "conf/ismir/MullerE11",
        "keywords": [
            "Chroma-based audio features",
            "processing and analyzing music data",
            "chroma features",
            "MATLAB implementations",
            "pitch-based and chroma-based",
            "well-established tool",
            "large number of chroma variants",
            "GNU-GPL license",
            "research in music information retrieval",
            "awareness of chroma variant properties"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nCHROMA TOOLBOX:MATLABIMPLEMENTATIONSFOREXTRACTING\nVARIANTS OFCHROMA-BASED AUDIO FEATURES\nMeinardM Â¨uller\nSaarland University\nandMPIInformatik\nmeinard@mpi-inf.mpg.deSebastian Ewert\nComputerScience III\nUniversityofBonn\newerts@iai.uni-bonn.de\nABSTRACT\nChroma-basedaudiofeatures,whichcloselycorrelatetoth e\naspect ofharmony,area well-establishedtoolinprocessin g\nandanalyzingmusicdata. Therearemanywaysofcomput-\ning andenhancingchromafeatures,which resultsin a large\nnumberofchromavariantswith differentproperties. Inthi s\npaper, we present a chroma toolbox [13], which contains\nMATLAB implementations for extracting various types of\nrecentlyproposedpitch-basedandchroma-basedaudiofea-\ntures. Providing the MATLAB implementations on a well-\ndocumented website under a GNU-GPL license, our aim is\nto foster research in music information retrieval. As an-\nother goal, we want to raise awareness that there is no sin-\ngle chroma variant that works best in all applications. To\nthis end, we discuss two exampleapplicationsshowingthat\nthe ï¬nal music analysis result may crucially depend on the\ninitial featuredesignstep.\n1. INTRODUCTION\nIt is a well-known phenomenon that human perception of\npitch is periodic in the sense that two pitches are perceived\nas similar in â€œcolorâ€ if they differ by an octave. Based on\nthis observation, a pitch can be separated into two com-\nponents, which are referred to as tone height andchroma,\nsee [19]. Assuming the equal-tempered scale, the chromas\ncorrespondto the set {C,Câ™¯,D,...,B}that consists of the\ntwelve pitch spelling attributes1as used in Western music\nnotation. Thus, a chroma feature is represented by a 12-\ndimensional vector x= (x(1),x(2),...,x(12))T, where\nx(1)corresponds to chroma C,x(2)to chroma Câ™¯, and so\n1Notethat intheequal-tempered scaledifferent pitch spell ings such Câ™¯\nandDâ™­refer to the samechroma.\nPermission to make digital or hard copies of all or part of thi s work for\npersonal or classroom use is granted without fee provided th at copies are\nnot made or distributed for proï¬t or commercial advantage an d that copies\nbear this notice and the full citation on theï¬rstpage.\nc/circlecopyrt2011 International Society for MusicInformation Retrieva l.\nCP\nCLP\nCENS\nCRPAudio\nrepresentationPitch\nrepresentationChroma\nrepresentationTuning\nestimationMultirate\npitch\nï¬lterbankSmoothing\nLogarithmic\ncompressionQuantization\nReduction\nNormalization\nFigure1. Overviewofthefeatureextractionpipeline.\non. In the feature extraction step, a given audio signal is\nconvertedinto a sequence of chromafeatureseach express-\ning how the short-time energy of the signal is spread over\nthetwelvechromabands.\nIdentifying pitches that differ by an octave, chroma fea-\ntures show a high degree of robustness to variations in\ntimbre and closely correlate to the musical aspect of har-\nmony. This is the reason why chroma-based audio fea-\ntures, sometimes also referred to as pitch class proï¬les, ar e\na well-established tool for processing and analyzing music\ndata [1, 5, 12]. For example, basically every chord recog-\nnition procedure relies on some kind of chroma represen-\ntation [2, 4, 11]. Also, chroma features have become the\nde facto standard for tasks such as music synchronization\nand alignment [7, 8, 12], as well as audio structure analy-\nsis [16]. Finally, chroma features have turned out to be a\npowerful mid-level feature representation in content-bas ed\naudio retrieval such as cover song identiï¬cation [3, 18] or\naudiomatching[10,15].\nTherearemanywaysforcomputingchroma-basedaudio\nfeatures. For example, the conversion of an audio record-\ning into a chroma representation (or chromagram) may be\nperformed either by using short-time Fourier transforms in\ncombination with binning strategies [1] or by employing\nsuitable multirate ï¬lter banks [12]. Furthermore, the prop -\nerties of chroma features can be signiï¬cantly changed by\n215Poster Session 2\nintroducingsuitable pre-and post-processingsteps modif y-\ningspectral,temporal,anddynamicalaspects. Thisleadst o\nalargenumberofchromavariants,whichmayshowaquite\ndifferentbehaviorinthecontextofaspeciï¬cmusicanalysi s\nscenario.\nIn this paper, we introducea chromatoolbox, which has\nrecently been released undera GNU-GPL license, see [13].\nThis well-documented toolbox contains MATLAB imple-\nmentations for extracting various types of recently intro-\nduced pitch-based and chroma-based audio features (re-\nferred to as Pitch,CP,CLP,CENS, andCRP), see also Fig-\nure 1 for an overview. In Section 2, we give a short sum-\nmary on how the various feature types are computed while\ndiscussing the role of the most important parameters that\ncanbeusedtomodifythefeaturesâ€™characteristics. Then,i n\nSection 3, we describe the functions of the toolbox for fea-\ntureextraction,visualization,andpost-processing. One par-\nticular goal of this paper is to emphasize the importance of\nthe feature design step by showingthat the results of a spe-\nciï¬c music analysis task may crucially depend on the used\nchromatype. Tothisend,wediscussin Section4two illus-\ntrative example applications, namely chord recognitionan d\naudiomatching.\n2. FEATURE EXTRACTION\nInthissection,wegiveanoverviewonhowthevariousfea-\nture types contained in the chroma toolbox are computed.\nAs illustration, Figure 3 shows the resulting feature repre -\nsentationsforanaudiorecordingoftheï¬rstsixmeasuresof\nOp.100,No.2 byFriedrichBurgmÂ¨ uller.\n2.1 PitchRepresentation\nAs basis for the chroma feature extraction, we ï¬rst decom-\npose a given audio signal into 88frequency bands with\ncenter frequencies corresponding to the pitches A0toC8\n(MIDI pitches p= 21top= 108). To obtain a sufï¬-\ncient spectral resolution for the lower frequencies, one ei -\nther needs a low sampling rate or a large temporal win-\ndow. Inourtoolbox,weemployaconstant Qmultirateï¬lter\nbank using a sampling rate of 22050 Hz for high pitches,\n4410 Hz for medium pitches, and 882 Hzfor low pitches,\nsee [12] for details. The employed pitch ï¬lters possess a\nrelativelywide passband,while still properlyseparating ad-\njacent notes thanks to sharp cutoffs in the transition bands ,\nsee Figure 2. Actually, the pitch ï¬lters are robust to devia-\ntions of up to Â±25cents2from the respective noteâ€™scenter\nfrequency. Toavoidlargephasedistortions,weuseforward -\nbackward ï¬ltering such that the resulting output signal has\npreciselyzerophasedistortionandamagnitudemodiï¬edby\nthesquareoftheï¬lterâ€™smagnituderesponse,see [17].\n2Thecentis a logarithmic unit to measure musical intervals. Thesemi -\ntone interval of the equally-tempered scale equals 100 cent s.0 0.1 0.2 0.3 0.4 0.5âˆ’60âˆ’40âˆ’200dB\nFrequency Ï‰69 93\nFigure2.MagnituderesponsesindBforsomeoftheï¬ltersofthe\nmultirate pitch ï¬lter bank. The shown ï¬lters correspond to M IDI\npitchespâˆˆ[69 : 93](withrespect tothe sampling rate 4410 Hz).\nIn the next step, for each of the 88pitch subbands, we\ncompute the short-time mean-square power (i.e., the sam-\nplesofeachsubbandoutputaresquared)usingawindowof\na ï¬xed length and an overlap of 50 %. For example, using\na window length of 200 millisecondsleads to a feature rate\nof10 Hz(10features per second). The resulting features,\nwhich we denote as Pitch, measure the short-time energy\ncontent of the audio signal within each pitch subband. We\nrefertoFigure3cforanillustrationandto[12]fordetails .\n2.2 Tuning\nTo account for the global tuning of a recording, one needs\ntosuitablyshiftthecenterfrequenciesofthesubband-ï¬lt ers\nof the multirate ï¬lter bank. To this end, we compute an\naverage spectrogram vector and derive an estimate for the\ntuning deviation by simulating the ï¬lterbank shifts using\nweighted binning techniquessimilar to [5]. In our toolbox,\nwe have pre-computed six different multirate ï¬lter banks\ncorresponding to a shift of Ïƒâˆˆ/braceleftbig\n0,1\n4,1\n3,1\n2,2\n3,3\n4/bracerightbig\nsemi-\ntones, respectively. From these ï¬lter banks, the most suit-\nable one is chosen according to the estimated tuning devia-\ntion.\n2.3 CPFeature\nFrom the pitch representation, one obtains a chroma repre-\nsentationsimplybyaddingupthecorrespondingvaluesthat\nbelong to the same chroma. For example, to compute the\nentry corresponding to chroma C, one adds up values cor-\nresponding to the musical pitches C1, C2, ..., C8 (MIDI\npitchesp= 24,36,...,108). For each window, this yields\na12-dimensional vector x= (x(1),x(2),...,x(12))T,\nwherex(1)corresponds to chroma C,x(2)to chroma Câ™¯,\nandsoon. Theresultingfeaturesarereferredtoas Chroma-\nPitchanddenotedby CP, see Figure3d.\n2.4 Normalization\nTo achieve invariance in dynamics, one can normalize\nthe features with respect to some suitable norm. In\nthe following, we only consider the â„“p-norm deï¬ned by\n||x||p:=/parenleftbig/summationtext12\ni=1|x(i)|p/parenrightbig1/pfor a given chroma vector x=\n(x(1),x(2),...,x(12))Tand some natural number pâˆˆN.\n21612th International Society for Music Information Retrieval Conference (ISMIR 2011)\nTo avoidrandomenergydistributionsoccurringduringpas-\nsages of very low energy (e.g., passages of silence before\nthe actual start of the recording or during long pauses), we\nreplace a chroma vector xby the uniform vector of norm\none in case ||x||pfalls below a certain threshold. Note that\nthecasep= 2yieldstheEuclideannormandthecase p= 1\nthe Manhattan norm. If not speciï¬ed otherwise, all chroma\nvectors to be considered are normalized with respect to the\nEuclideannorm,see alsoFigure3e.\n2.5 CLP Features\nTo account for the logarithmic sensation of sound inten-\nsity [20], one often applies a logarithmic amplitude com-\npression when computing audio features. To this end, each\nenergyvalues eofthepitchrepresentationisreplacedbythe\nvaluelog(Î·Â·e+1),whereÎ·isa suitablepositiveconstant.\nThen, the chromavaluesare computedas explainedin Sec-\ntion 2.3. The resulting features, which depend on the com-\npression parameter Î·, are referred to as Chroma-Log-Pitch\nand denoted by CLP[Î·], see Figure 3f. Note that a similar\nï¬‚atteningeffectcanbeachievedbyspectralwhiteningtech -\nniques, where the pitch subbandsare normalized according\nto short-timevariancesinthesubbands[5, 9].\n2.6 CENS Features\nAddingafurtherdegreeofabstractionbyconsideringshort -\ntime statistics over energy distributions within the chrom a\nbands, one obtains CENS(Chroma Energy Normalized\nStatistics) features, which constitute a family of scalabl e\nand robust audio features. These features have turned out\nto be very useful in audio matching and retrieval applica-\ntions [10, 15]. In computing CENSfeatures, each chroma\nvector is ï¬rst normalized with respect to the â„“1-norm thus\nexpressingrelativeenergydistribution. Then,aquantiza tion\nisappliedbasedonsuitablychosenthresholds. Here,choos -\ningthresholdsinalogarithmicfashionintroducessomekin d\noflogarithmiccompressionasabove,see[15]fordetails. I n\na subsequent step, the features are further smoothed over a\nwindow of length wâˆˆNand downsampled by a factor of\nd, see Section 2.8. The resulting features are normalized\nwithrespecttothe â„“2-normanddenotedby CENSw\nd,see also\nFigure3gandFigure3hforillustrations.\n2.7 CRPFeatures\nTo boost the degree of timbre invariance, a novel family of\nchroma-based audio features has been introduced in [14].\nThe general idea is to discard timbre-related information\nas is captured by the lower mel-frequency cepstral coef-\nï¬cients (MFCCs). Starting with the Pitchfeatures, one\nï¬rst applies a logarithmic compression and transforms the\nlogarithmized pitch representation using a DCT. Then, one\n0.5 11.5 22.5 33.5 44.5 55.5âˆ’0.2âˆ’0.100.10.2\n  \n0 1 2 3 4 52130405060708090100108\n0123456789\n  \n0 1 2 3 4 5C C#D D#E F F#G G#A A#B \n0246810\n  \n0 1 2 3 4 5C C#D D#E F F#G G#A A#B \n00.20.40.60.81\n  \n0 1 2 3 4 5C C#D D#E F F#G G#A A#B \n00.20.40.60.81\n  \n0 1 2 3 4 5C C#D D#E F F#G G#A A#B \n00.20.40.60.81\n  \n00.5 11.5 22.5 33.5 44.5 55.5C C#D D#E F F#G G#A A#B \n00.20.40.60.81\n  \n0 1 2 3 4 5C C#D D#E F F#G G#A A#B \nâˆ’1âˆ’0.500.51(a) Score\n(b) Waveform\n(c)Pitch\n(d)CP(without normalization)\n(e)CP\n(f)CLP[100]\n(g)CENS1\n1\n(h)CENS21\n5\n(i)CRP[55]\nTime [sec]\nFigure 3.Score and various feature representations for an audio\nrecording of the ï¬rstfour measures of Op.100, No.2byFriedr ich\nBurgmÂ¨ uller.\n217Poster Session 2\nFilename Mainparameters Description\nwav_to_audio.m â€“ ImportofWAV ï¬lesandconversiontoexpectedaudioformat.\nestimateTuning.m pitchRange Estimationoftheï¬lterbankshiftparameter Ïƒ.\naudio_to_pitch_via_FB.m winLenSTMSP Extractionofpitchfeaturesfromaudiodata.\npitch_to_chroma.m applyLogCompr, factorLogCompr /hatwide=Î· Derivationof CPandCLPfeaturesfrom Pitchfeatures.\npitch_to_CENS.m winLenSmooth /hatwide=w, downsampSmooth /hatwide=dDerivationof CENSfeaturesfrom Pitchfeatures.\npitch_to_CRP.m coeffsToKeep /hatwide=n, factorLogCompr /hatwide=Î·Derivationof CRPfeaturesfrom Pitchfeatures.\nsmoothDownsampleFeature.m winLenSmooth /hatwide=w, downsampSmooth /hatwide=dPost-processingoffeatures: smoothinganddownsampling.\nnormalizeFeature.m p Post-processingoffeatures: â„“p-normalization(default: p= 2).\nvisualizePitch.m featureRate Visualizationofpitchfeatures.\nvisualizeChroma.m featureRate Visualizationofchromafeatures.\nvisualizeCRP.m featureRate SpecializedversionofvisualizeChromafor CRPfeatures.\ngenerateMultiratePitchFilterbank.m â€“ Generationofï¬lterbanks(usedin audio_to_pitch_via_FB.m ).\nTable 1.Overview of the MATLABfunctions contained inthe chroma too lbox [13].\nonly keeps the upper coefï¬cients of the resulting pitch-\nfrequency cepstral coefï¬cients (PFCCs), applies an invers e\nDCT, and ï¬nally projects the resulting pitch vectors onto\n12-dimensionalchromavectors, which are then normalized\nwith respect to the â„“2-norm. These vectors are referred to\nas CRP (Chroma DCT-Reduced log Pitch) features. The\nupper coefï¬cients to be kept are speciï¬ed by a parameter\nnâˆˆ[1 : 120]. As reported in [14], the parameter n= 55\nyields good results and constitutesour default stetting. T he\nresultingfeaturesaredenotedby CRP[n],seeFigure3i. Note\nthat opposed to the previously introduced chroma variants,\nCRP featuresmayhavenegativeentries.\n2.8 Smoothing\nAs already mentioned in Section 2.6, one can further pro-\ncessthevariouschromavariantsbyapplyingsmoothingand\ndownsampling operations. For example, subsequent vec-\ntors of a feature sequences can be averaged using a slid-\ning windowofsize w(givenin frames)andthendownsam-\npled by a factor d. Starting with CENS,CP,CLP[Î·], and\nCRP[n], the resulting features are denoted by CENSw\nd,CPw\nd,\nCLP[Î·]w\nd, andCRP[n]w\nd, respectively. Even though being a\nsimple strategy,smoothingcan havea signiï¬cantimpacton\nthefeaturesâ€™behaviorwithinamusicanalysistasks. Forex -\nample, as reported in [15], the temporal blurring of CENS\nfeatures makes audio matching more robust to local tempo\nvariations. Furthermore,using the parameters wandd, one\nobtains a computationally inexpensive procedure to simu-\nlate tempo changes on the feature level. We illustrate this\nby means of a concrete example. Suppose, we start with a\nchromarepresentationhavinga feature rate of 10Hz. Then\nusingw= 41andd= 10, one obtains one chroma vector\nper second, each covering roughly 4100 ms of the original\naudio signal. Now, using w= 53(instead of w= 41) and\nd= 13(instead of d= 10) results in a temporally scaled\nversionofthefeaturessequencesimulatinga tempochange\nof10/13â‰ˆ0.77. Such tempo change strategies have been\nappliedsuccessfullyinthe contextofaudioindexing[10].3. TOOLBOX\nThefeatureextractioncomponentsasdescribedinSection2\nform the core of our chroma toolbox, which is freely avail-\nable at the well-documented website [13] under a GNU-\nGPL license. Table 1 gives an overview of the main MAT-\nLAB functions along with the most important parameters.\nNote that there are many more parameters not discussed in\nthispaper. However,forallparameterstherearedefaultse t-\ntings so that none of the parametersneed to be speciï¬ed by\ntheuser.\nTodemonstratehowourtoolboxcanbeapplied,wenow\ndiscuss the code example3shown in Table 2. Our example\nstarts with a call to the function wav_to_audio , which is a\nsimplewrapperaroundMATLABâ€™s wavread.m andconverts\nthe input WAV ï¬le into a mono version at a sampling rate\nof22050Hz. Furthermore, the struct sideinfo is returned\ncontaining meta information about the WAV ï¬le. In line\n3, the audio data is processed by estimateTuning , which\ncomputes an appropriate ï¬lter bank shift Ïƒfor the record-\ning. Next,inlines 5â€“9,Pitchfeaturesarecomputed. Here,\nthe struct paramPitch is used to pass optional parameters\nto the feature extraction function. If some parameters or\nthe whole struct are not set manually, then meaningful de-\nfault settings are used. This is a general principle through -\nout the toolbox. For the pitch computation, winLenSTMSP\nspeciï¬es the window length in samples. Here, 4410to-\ngether with a sampling frequency of 22050Hz results in\na window length corresponding to 200ms of audio. Using\nhalf-overlapped windows leads to a feature rate of 10 Hz.\nThe ï¬lterbank shift is speciï¬ed in line 6using the output\nofestimateTuning . Furthermore, an internal visualization\nis activated using the parameter visualize . Then, a call\ntoaudio_to_pitch_via_FB results in a 120Ã—N-matrix\nf_pitchthatconstitutesthe Pitchfeatures,where Nisthe\nnumber of time frames and the ï¬rst dimension corresponds\nto MIDI pitches. Actually, only the bands corresponding\n3This example is also contained in the toolbox as function\ndemoChromaToolbox.m .\n21812th International Society for Music Information Retrieval Conference (ISMIR 2011)\n1filename='Systematic_Chord-C-Major_Eight-Instrument s.wav';\n2[f_audio,sideinfo]=wav_to_audio('','data_WAV/',file name);\n3shiftFB=estimateTuning(f_audio);\n4\n5paramPitch.winLenSTMSP=4410;\n6paramPitch.shiftFB=shiftFB;\n7paramPitch.visualize=1;\n8[f_pitch,sideinfo]=...\n9 audio_to_pitch_via_FB(f_audio,paramPitch,sideinfo);\n10\n11paramCP.applyLogCompr=0;\n12paramCP.visualize=1;\n13paramCP.inputFeatureRate=sideinfo.pitch.featureRate ;\n14[f_CP,sideinfo]=pitch_to_chroma(f_pitch,paramCP,sid einfo);\n15\n16paramCLP.applyLogCompr=1;\n17paramCLP.factorLogCompr=100;\n18paramCLP.visualize=1;\n19paramCLP.inputFeatureRate=sideinfo.pitch.featureRat e;\n20[f_CLP,sideinfo]=pitch_to_chroma(f_pitch,paramCLP,s ideinfo);\n21\n22paramCENS.winLenSmooth=21;\n23paramCENS.downsampSmooth=5;\n24paramCENS.visualize=1;\n25paramCENS.inputFeatureRate=sideinfo.pitch.featureRa te;\n26[f_CENS,sideinfo]=pitch_to_CENS(f_pitch,paramCENS,s ideinfo);\n27\n28paramCRP.coeffsToKeep=[55:120];\n29paramCRP.visualize=1;\n30paramCRP.inputFeatureRate=sideinfo.pitch.featureRat e;\n31[f_CRP,sideinfo]=pitch_to_CRP(f_pitch,paramCRP,side info);\n32\n33paramSmooth.winLenSmooth=21;\n34paramSmooth.downsampSmooth=5;\n35paramSmooth.inputFeatureRate=sideinfo.CRP.featureRa te;\n36[f_CRPSmoothed,featureRateSmoothed]=...\n37 smoothDownsampleFeature(f_CRP,paramSmooth);\n38parameterVis.featureRate=featureRateSmoothed;\n39visualizeCRP(f_CRPSmoothed,parameterVis);\nTable 2.Code example.\nto MIDI pitches 21to108are computed and the values of\nthe other bands are set to zero. Furthermore, details on the\nfeature conï¬guration are appended to the sideinfo struct.\nUsing sideinfo to store all relevant meta information re-\nlated to the feature processingpipelineconstitutes a seco nd\ngeneralprinciplein ourtoolbox.\nIn lines11â€“31, various chroma representations are de-\nrived from the pitch features. First, in lines 11â€“14,CP\nfeatures are computed. Then, activating the logarithmic\ncompression using applyLogCompr ,CLP[100]features are\ncomputed in lines 16â€“20. The compression level is spec-\niï¬ed in line 17by the parameter factorLogCompr , which\ncorresponds to the parameter Î·introduced in Section 2.5.\nNext, in lines 22â€“26,CENS21\n5features are computed. Here,\nthe parameters winLenSmooth anddownsampSmooth corre-\nspond to the parameters wanddexplained in Section 2.8,\nrespectively. Finally, in lines 28â€“31,CRP[55]features\nare computed, where the parameter nof Section 2.7 cor-\nresponds to the lower bound of the range speciï¬ed by\ncoeffsToKeep , see line 28. Finally, the use of the function\nsmoothDownsampleFeature is demonstrated, where in lines\n33â€“34the parameters wanddare speciï¬ed as for the CENS\ncomputation. At the end of our example, we visualize the\nsmoothed CRPfeaturesusingthefunction visualizeCRP .4. ILLUSTRATING APPLICATIONS\nTodemonstratetheimportanceofthefeaturedesignstep,we\nnow discuss the various chroma variants within two differ-\nentmusicanalysisscenarios. Here,ratherthancommending\naspeciï¬cfeaturetype,ourgoalistoshowhowdifferentfea-\nturevariantsandparametersettingsmaycruciallyinï¬‚uenc e\ntheï¬nal analysisresults.\n4.1 ChordRecognition\nThecomputer-basedharmonicanalysisofmusicrecordings\nwith the goal to automatically extract chord labels directl y\nfrom the given audio material constitutes a major task in\nmusic information retrieval [2, 4, 11]. In most automated\nchord recognition procedures, the given music recording is\nï¬rst converted into a sequence of chroma-based audio fea-\ntures and then pattern matching techniques are applied to\nmapthechromafeaturesto chordlabels.\nWe now demonstrate by a small experiment, how the ï¬-\nnalrecognitionratessubstantiallydependontheunderlyi ng\nchromarepresentationandparametersthatcontroltempora l\nand spectral aspects. To this end, we revert to three differ-\nent pattern matching techniques. The ï¬rst two approaches\naresimpletemplate-basedapproaches,referredtoas Tband\nTa, where the ï¬rst approach uses data-independent binary\ntemplates and the second one data-dependent average tem-\nplates. As third approach,we employhiddenMarkovmod-\nels denoted by HMM. Using the annotated Beatles dataset\nas described in [6], which consists of 180Beatles songs,\nwe computed recognition rates based on conventional F-\nmeasures using 3-fold cross validation. Figure 4 shows the\nrecognition rates for the three pattern matching technique s\nin combinationwith differentchromavariants.\nAs these experimental results indicate, the used chroma\nrepresentationcan have a signiï¬cant inï¬‚uence on the chord\nrecognition accuracy. In particular, a logarithmic com-\npression step in the chroma extraction turns out to be cru-\ncial. Furthermore, the results reveal that temporal featur e\nsmoothing plays an important role in chord recognitionâ€“\nin particular for recognizers that work in a purely frame-\n0.350.40.450.50.550.60.650.70.750.8\n  \nTb\nTa\nHMM\nFeature typesF-measure\nCP CLP[100] CENS1\n1CENS11\n1CRP[55]CRP[55]11\n1\nFigure 4.Dependency of the recognition rates (F-measures) of\ndifferent chord recognition procedures on the used chroma v ariant\n(using a Beatlesdataset and 3-foldcross validation).\n219Poster Session 2\nwise fashion. Here, note that the Viterbi decoding in\nthe HMM-based recognizer already introduces a different\nkind of smoothing in the classiï¬cation stage so that feature\nsmoothinghasa lesssigniï¬cantimpactin thiscase.\n4.2 AudioMatching\nAs second applicationscenario,we considerthe task of au-\ndiomatching withthegoaltoautomaticallyretrieveallfrag-\nments from all recordings within a large audio collection\nthat musically correspond to a given query audio clip [15].\nIn this task, one challenge is to cope with variationsin tim-\nbreandinstrumentationastheyappearin differentinterpr e-\ntations, cover songs, and arrangementsof a piece of music.\nIn a typical procedure for audio matching, the query Qas\nwell as each database recording Dare ï¬rst converted into\nchroma feature sequences X(Q)andX(D), respectively.\nThen,alocalvariantofdynamictimewarpingisusedtolo-\ncally compare the query sequence X(Q)with the database\nsequence X(D)yieldinga distance function âˆ†. Each local\nminimumof âˆ†closetozeroindicatesafragmentwithinthe\ndatabase recordingthat is close to the given query, see [14]\nfordetails.\nIn view of this matching application, the following two\npropertiesof âˆ†areof crucialimportance. Ontheonehand,\nthesemanticallycorrectmatchesshouldcorrespondtoloca l\nminimaof âˆ†closetozerothusavoidingfalsenegatives. On\ntheotherhand, âˆ†shouldbewellabovezerooutsideaneigh-\nborhood of the desired local minima thus avoiding false\npositives. In view of these requirements, the used chroma\nvariant plays a major role. As an illustrative example, we\nconsidera recordingby Yablonskyof Shostakovichâ€™sWaltz\nNo. 2 from the Suite for Variety Orchestra No. 1 , which\nis used as the database recording. The theme of this piece\noccurs four times played in four different instrumentation s\n(clarinet, strings, trombone,tutti). Denotingthe fouroc cur-\nrences by E1,E2,E3, andE4and using E3as the query,\nFigure 5 shows several distance functions based on differ-\n0 20 40 60 80 100 120 140 160 18000.10.20.30.40.5\nTime(s)E1 E2 E3=Query E4\nFigure 5.Several distance functions shown for the Yablonsky\nrecordingof theShostakovichâ€™s WaltzNo.2fromthe Suite forVa-\nrietyOrchestraNo.1 usingtheexcerpt E3asquery. Thefollowing\nfeaturetypeswereused: CP(green),CLP[100](red),CENS41\n10(blue)\nandCRP[55](black). For the query, there are 4annotated excerpts\n(true matches).ent chroma variants. Note that one expects four local min-\nima. Using conventional chroma features such as CP, the\nexpected local minima are not signiï¬cant or not even exist-\ning. However,usingthechromavariant CRP[55],oneobtains\nforallfourtruematchesconciselocalminima,seetheblack\ncurveofFigure5. Foradetaileddiscussion,wereferto[14] .\nAcknowledgement. This work has been supported by the\nCluster of Excellence on Multimodal Computing and In-\nteraction at Saarland University and the German Research\nFoundation(DFGCL 64/6-1).\n5. REFERENCES\n[1] Mark A. Bartsch and Gregory H. Wakeï¬eld. Audio thumbnail ing of popular\nmusic using chroma-based representations. IEEE Transactions on Multimedia ,\n7(1):96â€“104, February 2005.\n[2] Juan Pablo Bello and Jeremy Pickens. A robust mid-level r epresentation for har-\nmonic content in music signals. In Proceedings of the International Conference\non MusicInformation Retrieval(ISMIR) , London,UK,2005.\n[3] DanielP.W.EllisandGraham.E.Poliner.Identifying â€˜c oversongsâ€™withchroma\nfeatures and dynamic programming beat tracking. In Proc. IEEE International\nConference on Acoustics, Speech and Signal Processing (ICA SSP), volume 4,\nHonolulu, Hawaii,USA,April 2007.\n[4] TakuyaFujishima. Realtimechordrecognitionof musica lsound: Asystemusing\ncommon lispmusic. In Proc.ICMC , pages464â€“467, Beijing,1999.\n[5] Emilia GÂ´ omez. Tonal Description of Music Audio Signals . PhD thesis, UPF\nBarcelona,2006.\n[6] Christopher Harte, Mark Sandler, Samer Abdallah, and Em ilia GÂ´ omez. Sym-\nbolic representation of musical chords: A proposed syntax f or text annotations.\nInProceedings of the International Conference on Music Infor mation Retrieval\n(ISMIR), London,GB,2005.\n[7] Ning Hu, Roger Dannenberg, and George Tzanetakis. Polyp honic audio match-\ning and alignment for music retrieval. In Proceedings of the IEEE Workshop on\nApplicationsofSignal ProcessingtoAudioand Acoustics(W ASPAA),NewPaltz,\nNY,US, October2003.\n[8] Cyril Joder, Slim Essid, and GaÂ¨ el Richard. A comparativ e study of tonal acous-\ntic features for a symbolic level music-to-score alignment . InProceedings of the\n35nd IEEE International Conference on Acoustics, Speech, a nd Signal Process-\ning (ICASSP) ,Dallas,USA, 2010.\n[9] AnssiKlapuri. Multipitch analysisof polyphonic music and speechsignalsusing\nanauditorymodel. IEEETransactionsonAudio,Speech,andLanguage Process-\ning,16(2):255â€“266, 2008.\n[10] Frank Kurth and Meinard MÂ¨ uller. Efï¬cient index-based audio matching. IEEE\nTransactions on Audio, Speech, and Language Processing , 16(2):382â€“395,\nFebruary 2008.\n[11] Matthias Mauch and Simon Dixon. Simultaneous estimati on of chords and mu-\nsical context from audio. IEEE Transactions on Audio, Speech, and Language\nProcessing ,18(6):1280â€“1289, 2010.\n[12] Meinard MÂ¨ uller. Information Retrieval for Music and Motion . Springer Verlag,\n2007.\n[13] Meinard MÂ¨ uller and Sebastian Ewert. Chroma toolbox. http://www.\nmpi-inf.mpg.de/resources/MIR/chromatoolbox/ , retrieved\n01.04.2011.\n[14] Meinard MÂ¨ uller and Sebastian Ewert. Towards timbre-i nvariant audio features\nfor harmony-based music. IEEE Transactions on Audio, Speech, and Language\nProcessing(TASLP) , 18(3):649â€“662, 2010.\n[15] MeinardMÂ¨ uller,FrankKurth,andMichaelClausen.Aud iomatchingviachroma-\nbased statistical features. In Proceedings of the 6th International Conference on\nMusicInformation Retrieval(ISMIR) , pages288â€“295,2005.\n[16] Jouni Paulus, Meinard MÂ¨ uller, and Anssi Klapuri. Audi o-based music structure\nanalysis.In Proceedingsof the11th International ConferenceonMusic I nforma-\ntion Retrieval(ISMIR) , pages625â€“636,Utrecht, Netherlands,2010.\n[17] JohnG. Proakis and Dimitris G.Manolakis. Digital Signal Processsing .Prentice\nHall,1996.\n[18] J. Serr` a, E. GÂ´ omez, P. Herrera, and X. Serra. Chroma bi nary similarity and lo-\ncal alignment applied to cover song identiï¬cation. IEEE Transactions on Audio,\nSpeechand Language Processing ,16:1138â€“1151, October 2008.\n[19] Roger N. Shepard. Circularity in judgments of relative pitch.Journal of the\nAcousticSocietyof America ,36(12):2346â€“2353, 1964.\n[20] Eberhard Zwicker and Hugo Fastl. Psychoacoustics, facts and models . Springer\nVerlag,NewYork, NY,US,1990.\n220"
    },
    {
        "title": "A Segment-Based Fitness Measure for Capturing Repetitive Structures of Music Recordings.",
        "author": [
            "Meinard MÃ¼ller",
            "Peter Grosche",
            "Nanzhu Jiang"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1416342",
        "url": "https://doi.org/10.5281/zenodo.1416342",
        "ee": "https://zenodo.org/records/1416342/files/MullerGJ11.pdf",
        "abstract": "In this paper, we deal with the task of determining the audio segment that best represents a given music recording (similar to audio thumbnailing). Typically, such a segment has many (approximate) repetitions covering large parts of the music recording. As main contribution, we introduce a novel fitness measure that assigns to each segment a fitness value that expresses how much and how well the segment â€œexplainsâ€ the repetitive structure of the recording. In combination with enhanced feature representations, we show that our fitness measure can cope even with strong variations in tempo, instrumentation, and modulations that may occur within and across related segments. We demonstrate the practicability of our approach by means of several challenging examples including field recordings of folk music and recordings of classical music.",
        "zenodo_id": 1416342,
        "dblp_key": "conf/ismir/MullerGJ11",
        "keywords": [
            "fitness measure",
            "audio segment",
            "repetitive structure",
            "enhanced feature representations",
            "tempo variations",
            "instrumentation changes",
            "modulations",
            "practicability",
            "challenging examples",
            "field recordings"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nASEGMENT-BASED FITNESSMEASURE FOR CAPTURING REPETITIVE\nSTRUCTURES OF MUSICRECORDINGS\nMeinardM Â¨uller, Peter Grosche, Nanzhu Jiang\nSaarland Universityand MPIInformatik\n{meinard,pgrosche,njiang }@mpi-inf.mpg.de\nABSTRACT\nIn this paper, we deal with the task of determining the au-\ndio segment that best represents a given music recording\n(similar to audio thumbnailing). Typically, such a segment\nhas many (approximate) repetitions covering large parts of\nthe music recording. As main contribution, we introduce a\nnovel ï¬tness measure that assigns to each segment a ï¬tness\nvalue that expresses how much and how well the segment\nâ€œexplainsâ€the repetitivestructureof the recording. In co m-\nbination with enhanced feature representations, we show\nthat our ï¬tness measure can cope even with strong varia-\ntions in tempo, instrumentation, and modulations that may\noccur within and across related segments. We demonstrate\nthe practicabilityof ourapproachbymeansof severalchal-\nlenging examples including ï¬eld recordings of folk music\nandrecordingsofclassical music.\n1. INTRODUCTION\nMusic structureanalysisconstitutesa fundamentalresear ch\ntopic within the ï¬eld of music information retrieval. One\nmajor goalof structureanalysisis to dividea music record-\ning into temporal segments corresponding to musical parts\nand then to group these segments into musically meaning-\nful categories [10]. Such segments may refer to chorus or\nverse sections of a popular piece of music, to stanzas of a\nfolk song, or to the ï¬rst theme, the second theme or the\nentire exposition of a symphony. Such important musical\nparts are often characterized by the property of being re-\npeated several times throughout the piece. Therefore, ï¬nd-\ningtherepetitivestructureofamusicrecordingisanimpor -\ntant and well-studied subtask within structure analysis, s ee,\ne.g.,[1,2,5,6,9]andtheoverviewarticles[3,10]. Mostof\nthese approachesworkwell formusic where the repetitions\nlargely agree. However, in general, â€œrepeating partsâ€ are\nPermission to make digital or hard copies of all or part of thi s work for\npersonal or classroom use is granted without fee provided th at copies are\nnot made or distributed for proï¬t or commercial advantage an d that copies\nbear this notice and the full citation on theï¬rstpage.\nc/circlecopyrt2011 International Society for MusicInformation Retrieva l.far frombeingsimple repetitions. Actually,audiosegment s\nthatrefertothesamemusicalpartmaydiffersigniï¬cantlyi n\nparameterssuch as dynamics, instrumentation,articulati on,\nandtemponottospeakofpronouncedmusicalvariations. In\nsuch cases, structure analysis becomes a hard and ill-posed\ntask withmanyyetunsolvedproblems.\nIn this paper, we address the problem of ï¬nding the\nmostrepresentativeandrepetitivesegmentofagivenmusic\nrecordings, a task often referred to as audio thumbnailing ,\nsee, e.g., [1]. Here, opposed to most of the previous ap-\nproaches we want to admit even strong musical variations.\nAs our main contribution, we introduce a ï¬tness measure\nthatassignstoeachaudiosegmentaï¬tnessvaluethatsimul-\ntaneouslycapturestwoaspects. Firstly,itindicates howwell\nthe given segment explains other similar segments (â€œpreci-\nsionsâ€) and, secondly, it indicates how much of the overall\nmusicrecordingsiscoveredbyallthesesegments(â€œrecallâ€ ).\nFurthermore, our ï¬tness measure is normalized and disre-\ngardstrivialself-explanations(reï¬‚exiverelations). As afur-\nthercontributionofthispaper,weintroduceacompacttime -\nlagrepresentationthatyieldsahigh-levelviewonthestru c-\ntural properties for the entire music recording. First expe r-\niments shows that our ï¬tness measure, in combinationwith\nenhancedfeaturerepresentations,cancopewithevenstron g\nvariations in tempo, instrumentation, and modulations tha t\noccurwithinandacrossthesegments.\nAt this point, we want to note that our work has been\ninspired by Paulus and Klapuri [9], even though the task\nandconceptsofthispaperarefundamentallydifferentto[9 ].\nTheï¬tnessmeasureintroducedin[9]expressespropertieso f\nanentire structure , whereas our ï¬tness measure expresses\nproperties of a single segment . In assigning a ï¬tness value\ntoagivensegment,ourideaistosimultaneouslyaccountfor\nall itsexistingrelationswithinthe entirerecording.\nThe remainder of this paper is organized as follows. In\nSection 2, we give a motivation of our approach, ï¬x some\nnotation, and quickly review the concept of self-similarit y\nmatrices. InSection3,asourmaincontribution,wedescrib e\nthe technical details on the construction of our ï¬tness mea-\nsure. Finally, experimentalresults and an outlook on futur e\nworkcanbefoundin Section4andSection5,respectively.\n615Oral Session 7: Structure Analysis and Mixing\n2. MOTIVATION ANDNOTATION\nIn the following, we distinguish between a piece of music\n(in an abstract sense) and a particular audio recording (a\nconcreteperformance)ofthepiece. Theterm partisusedin\nthe context of the abstract music domain, whereas the term\nsegmentisusedfortheaudiodomain[10]. Musicalpartsare\noftendenotedbytheletters A,B,C,... intheorderoftheir\nï¬rstoccurrence. Forexample,thesequence A1A2B1A3de-\nscribes the musical form consisting of three repeating A-\nparts interleaved with one B-part. Then, for a given music\nrecording of such a piece, the goal of the structure analysis\nproblem as tackled in this paper would be to ï¬nd the seg-\nmentswithinthe recordingthat correspondto the A-parts.\nMostrepetition-basedapproachestoaudiostructureanal-\nysis proceed as follows. In the ï¬rst step, the music record-\ning is transformed into a sequence X:= (x1,x2,...,x N)\nof feature vectors xnâˆˆ F,1â‰¤nâ‰¤N, whereFde-\nnotes a suitable feature space. In the second step, based on\na similarity measure s:F Ã— F â†’ R, one deï¬nes a self-\nsimilarity matrix S âˆˆRNÃ—NbyS(n,m) :=s(xn,xm),\n1â‰¤n,mâ‰¤N. In the following,a tuple p= (n,m)âˆˆ[1 :\nN]2is called a cellofS, and the value S(n,m)is referred\nto as thescoreof the cell p. The crucial observation is that\nrepeatingpatternsinthefeaturesequence Xappearasdiag-\nonal â€œstripesâ€ in S[2, 10]. Moreprecisely,these stripesare\npaths of cells of high score running in parallel to the main\ndiagonal. Therefore, in the third step, one extracts all suc h\npaths from S, where each path encodes the similarity of a\npairofsegments. (Thesetwosegmentsaregivenbythetwo\nprojectionsofthepathontothetwoaxisof S,seeFigure1.)\nIn the fourth step, from the given pairwise relations of seg-\nments, one derives entire groups of segments, where each\ngroup comprises all segments of a given type of a musical\npart(e.g. all segmentscorrespondingto A-parts). Thisstep\ncanbethoughtofformingsomekindoftransitiveclosureof\nthe given path relations [3, 6]. However, this groupingpro-\ncess constitutes a main challenge when the extracted paths\nare erroneousand incomplete. In [5], a groupingprocess is\ndescribed that balances out inconsistenciesin the path rel a-\ntions by exploitinga constant tempo assumption. However,\nwhen dealing with music of varying tempo, the grouping\nprocessconstitutesa challengingresearchproblem.\nAs one main idea of our approach, we suggest to jointly\nperform the third and fourth step thus circumventing the\nseparate grouping process. We realize this idea by assign-\ning a ï¬tness value to a givensegment in such a way that all\nrelatedsegmentssimultaneouslyinï¬‚uencetheï¬tness value .\nTo express relations between segments, we will introduce\nthe notion of a path family, see Section 3.1. Intuitively, in -\nsteadofextractingindividualpaths,weextractentiregro ups\nof paths, where the consistency within a group is automati-\ncallyenforcedbythe construction.\n  \n50 100 150 200 250 30050100150200250300\nâˆ’2âˆ’1.5âˆ’1âˆ’0.500.51\nsÎ±tÏ€1\n1Ï€1\n2Ï€1\n3\nÏ€1Ï€2Ï€3\nFigure 1. Idealized self-similarity matrix Sfor a recording of\nmusical form A1A2A3. The ï¬gures show an optimal path fam-\nilyP:={Ï€1,Ï€2,Ï€3}for the segment Î±= [s:t] = [120:190] =\nÏ€2\n1=Ï€2\n2=Ï€2\n3.\n2.1 DesiredProperties\nWe now motivate some basic properties that serve as a\nguideline for the construction of our ï¬tness measure. Let\nX= (x1,x2,...,x N)be the feature representation of the\ngiven audio recording. A segmentÎ±is deï¬ned to be a sub-\nsetÎ±= [s:t]âŠ†[1 :N]speciï¬ed by its starting point s\nand its end point t(given in terms of feature indices). Let\n|Î±|:=tâˆ’s+ 1denote the length of Î±. In our approach,\nwe introduce a ï¬tness measure Ï•that assigns to each seg-\nmentÎ±âŠ†[1 :N]a ï¬tnessvalue Ï•(Î±)âˆˆR. Intuitively,this\nï¬tness value should express to which extent the segment Î±\nâ€œexplainsâ€ the repetitive structure of X. In particular, the\nvalueÏ•(Î±)shouldbelargeinthecasethattherepetitionsof\nÎ±coverlargeportionsof X,otherwiseit shouldbesmall.\nNext, we impose some normalization constraints on Ï•.\nNote that the segment Î±= [1 :N]explains the entire se-\nquenceXperfectly. More generally, each segment Î±ex-\nplains itself perfectly (this information is encoded by the\nmain diagonal of a self-similarity matrix). We do not want\nsuch trivial, reï¬‚exiveself-explanationsto becapturedby Ï•.\nTherefore,we require\n0â‰¤Ï•(Î±)â‰¤Nâˆ’|Î±|\nN. (1)\nIn particular, one obtains Ï•([1 :N]) = 0. More generally,\na valueÏ•(Î±) = 0should mean that the segment Î±only ex-\nplains itself but no other portions of X. As an illustrative\nexample,weconsideranâ€œidealâ€recordingofapieceofmu-\nsic having the form A1A2...AK. LetÎ±kbe the segment\ncorrespondingto Ak,kâˆˆ[1 :K]. Thenourï¬tnessmeasure\nshould assume the value Ï•(Î±k) =Kâˆ’1\nKfor each segment\nÎ±k, see Figure1illustratingthecase K= 3.\n61612th International Society for Music Information Retrieval Conference (ISMIR 2011)\n2.2 Self-SimilarityMatrices\nIn general, repeating segments may differ signiï¬cantly re-\ngarding tempo, instrumentation and other musical proper-\nties. Thedegreeofthesimilaritybetweentworepeatingseg -\nmentsÎ±andÎ±â€²crucially depends on the used feature type,\nthe similarity measure, and the resulting self-similarity ma-\ntrixS. Our ï¬tness measure is generic in the sense that it\ncanworkwithgeneralself-similaritymatricesthatonlyfu l-\nï¬ll some basic normalization properties. Actually, we only\nrequire the property S(n,m)â‰¤1for1â‰¤n,mâ‰¤Nand\nS(n,n) = 1fornâˆˆ[1 :N]. Since the construction of S\nis not in the focus of this paper, we only give a quick de-\nscription of the type of self-similarity matrix as used in ou r\nexperiments. Figure 2 illustrates the following steps. Fir st\nofall,weuseavariantofchroma-basedaudiofeaturesasde-\nscribed in [6, Section 3.3]. Normalizing these features, we\nsimply use the inner productas similarity measure yielding\na value between 0and1. To enhance structural properties,\nwe applytemporalsmoothingtechniquesthat can deal with\ntempo variations, see [6, Section 7.2]. Furthermore, apply -\ningtechniquesasdescribedin[7],weobtainatranspositio n-\ninvariant matrix that can deal with modulation differences\nwithin and across repeating parts. Subsequently, using a\nsuitable threshold parameter Ï„ >0and a penalty param-\neterÎ´â‰¤0, we post-process the matrix by ï¬rst setting the\nscore values of all cells with a score below Ï„to the value Î´\nand then by linearly scaling the range [Ï„: 1]to[0 : 1]. Fi-\nnally,weset S(n,n) = 1fornâˆˆ[1 :N](thispropertymay\nhave beenlost bythe smoothingstep). In the following,we\nchooseÏ„in a relative fashion by keeping 25%of the cells\nhavingthe highestscoreandset Î´=âˆ’2.\n3. FITNESS MEASURE\nFollowing the guidelines motivated in Section 2, we now\nintroduce our novel ï¬tness measure. In assigning a ï¬tness\nvalue to a given segment Î±, our idea is to simultaneously\naccount for all other segments that are related to Î±. To this\nend,inSection3.1,weintroducethenotationofapathfam-\nily that allows for expressing these relations. Then, in Sec -\ntion 3.2, we explain how each path family can be assigned\na coverage (â€œrecallâ€) as well as an average score measure\n(â€œprecisionsâ€). The ï¬tness of the segment Î±is then deter-\nmined by the path family that simultaneously maximizes\ncoverageandscore.\n3.1 PathFamily\nLetX= (x1,x2,...,x N)be a feature sequence and Sa\nself-similarity matrix as introduced in Section 2.2. A path\nof length Lis a sequence Ï€= (p1,...,p L)of cellspâ„“=\n(nâ„“,mâ„“)forâ„“âˆˆ[1 :L]satisfying pâ„“+1âˆ’pâ„“âˆˆÎ£, whereÎ£\ndenotesaset ofadmissiblestepsizes. Inoursetting,weuse\n  \n0 50 100 150020406080100120140160180\n00.10.20.30.40.50.60.70.80.91\n  \n0 50 100 150020406080100120140160180\n00.10.20.30.40.50.60.70.80.91\n  \n0 50 100 150020406080100120140160180\n00.10.20.30.40.50.60.70.80.91\n  \n0 50 100 150020406080100120140160180\nâˆ’2âˆ’1.5âˆ’1âˆ’0.500.51\nTime(sec) Time(sec)Time(sec) Time(sec)(a) (b)\n(c) (d)\nFigure 2. Self similarity matrices for the song â€œIn the year 2525â€\nby Zager and Evans. (a)Initial self-similarity matrix. (b)Path-\nenhanced matrix. (c)Transposition-invariant matrix. (d)Thresh-\nolded matrixwith Î´=âˆ’2.\nÎ£ ={(1,2),(2,1),(1,1)}, which constrains the slope of\ntheadmissiblepathswithintheboundsof 1/2and2,see[6,\nChapter4]. The scoreÂµ(Ï€)ofa pathÏ€isdeï¬nedas\nÂµ(Ï€) =L/summationdisplay\nâ„“=1S(nâ„“,mâ„“). (2)\nConsidering the two projections, a path Ï€deï¬nes two seg-\nments denoted by Ï€1:= [n1:nL]andÏ€2:= [m1:mL],\nsee alsoFigure1. Vice versa,giventwosegments Î±andÎ±â€²,\na pathÏ€withÏ€1=Î±andÏ€2=Î±â€²is called an alignment\npathbetween the two segments. Given a segment Î±and a\nself-similarity matrix S, we deï¬ne a path family over Î±to\nbe a setP:={Ï€1,Ï€2,...,Ï€ K}that consists of paths Ï€k\nand satisï¬es the following conditions. Firstly, Ï€2\nk=Î±for\nallkâˆˆ[1 :K]. Secondly, the set/braceleftbig\nÏ€1\nk|kâˆˆ[1 :K]/bracerightbig\ncon-\nsists of pairwise disjoint segments, i.e., Ï€1\niâˆ©Ï€1\nj=âˆ…for\ni,jâˆˆ[1 :K],i/ne}ationslash=j. Next,extendingthedeï¬nitionin(2)in\na straightforwardway, the scoreÂµ(P)of the path family P\nisdeï¬nedas\nÂµ(P) :=K/summationdisplay\nk=1Âµ(Ï€k). (3)\nFinally, the scoreÂµ(Î±)of a segment Î±is deï¬ned to be the\nscore of a path family Pâˆ—having maximal score among all\npossiblepathfamiliesover Î±:\nPâˆ—:= argmax\nPÂµ(P) (4)\nÂµ(Î±) :=Âµ(Pâˆ—). (5)\nActually, the value Âµ(Î±)is not yet the ï¬tness value we are\nlooking for since neither does it fulï¬ll the basic propertie s\n617Oral Session 7: Structure Analysis and Mixing\n  \n0 50 100 150020406080100120140160180\nâˆ’2âˆ’1.5âˆ’1âˆ’0.500.51\n  \n0 50 100 150020406080100120140160180\nâˆ’2âˆ’1.5âˆ’1âˆ’0.500.51\n  \n0 20 40 60 80 100 120 140 160 180020406080100120\n00.050.10.150.20.250.3 (a) (b)Time(sec) Time(sec)\nTime(sec)Time(sec) Time-lag(sec)(a) (b)\n(c)\nFigure 3. Sand optimal path families Pover different segments\nÎ±= [s:t]for the song â€œIn the year 2525â€ by Zager and Evans.\n(a)Î±= [57:72] (maximal ï¬tness). (b)Î±= [80:150] (c)Fitness\nmatrix.\nformulatedinSection2nordoesitcapturehowmuchofthe\naudiomaterialisactuallycovered.\n3.2 Deï¬nitionofFitnessMeasure\nWe now give a formal deï¬nition of our ï¬tness measure,\nwhich has all the desired properties. Actually, at this poin t,\nwe only need the assumption that the given self-similarity\nmatrixS âˆˆRNÃ—Nhas the property that S(n,m)â‰¤1for\nallcells(n,m)âˆˆ[1 :N]2andS(n,n) = 1fornâˆˆ[1 :N].\nWe start by deï¬ning the normalized score Â¯Âµ(P)of the path\nfamilyPoverÎ±by\nÂ¯Âµ(P) :=Âµ(P)âˆ’|Î±|/summationtextK\nk=1Lk, (6)\nwhereLkdeï¬nesthelengthofpath Ï€k. Here,themotivation\nforsubtractingthelength |Î±|ofÎ±isthatthesegment Î±triv-\nially explains itself, see Section 2. It is not hard to see tha t\nthe score Â¯Âµfulï¬lls the conditions(1). From the assumption\nS(n,n) = 1,oneobtains Â¯Âµ(P)â‰¥0. Furthermorenotethat,\nwhen using Î£ ={(1,2),(2,1),(1,1)}, one has Lkâ‰¤ |Î±|\nand/summationtext\nkLkâ‰¤N. This together with S(n,m)â‰¤1implies\nthe property Â¯Âµ(P)â‰¤(Nâˆ’ |Î±|)/N. Intuitively, the value\nÂ¯Âµ(P)expresses the average score or precision of the given\npathfamily P.\nNext,wedeï¬nesomekindof coverage orrecallmeasure\nforP. Tothisend,let Î³(P) :=âˆªkâˆˆ[1:K]Ï€1\nkâŠ†[1 :N]bethe\nunion of all segments deï¬ned by the ï¬rst projection of the\npathsÏ€k. Then we deï¬nethe normalizedcoverage Â¯Î³(P)of\nPby\nÂ¯Î³(P) :=|Î³(P)|âˆ’|Î±|\nN. (7)Asabove,thelength |Î±|issubtractedtocompensatefortriv-\nial coverage. Obviously,onehas Â¯Î³(P)â‰¤(Nâˆ’Î±)/N.\nInspired by the F-measure that combines precision and\nrecall,wedeï¬nethe ï¬tnessÏ•(P)ofthepathfamily Ptobe\nÏ•(P) := 2Â·Â¯Âµ(P)Â·Â¯Î³(P)\nÂ¯Î³(P)+ Â¯Âµ(P). (8)\nIn other words, the ï¬tness integrates the normalized score\nand coverageinto one measure. Finally, the ï¬tnessÏ•(Î±)of\na segment Î±is deï¬ned to be the ï¬tness value of the score-\nmaximizingpathfamily Pâˆ—deï¬nedin(4):\nÏ•(Î±) :=Ï•(Pâˆ—). (9)\nNote that the path family Pâˆ—deï¬nes in a natural way a set\nof disjoint segments revealing the repetitions of Î±within\nthe sequence X, see Figure 1. An optimal path family Pâˆ—\nfor a segment Î±can be computed efï¬ciently with O(|Î±| Ã—\nN)operations using dynamic programming. Actually, the\nalgorithm, which we do not describe in this paper due to\nspace limitations, is an extension of classical dynamic tim e\nwarping(DTW),see [4, 6].\nWhen computing the ï¬tness Ï•(Î±)for all possible seg-\nmentsÎ±= [s:t]âŠ†[1 :N], one can obtain a com-\npact ï¬tness representation for the entire music recording.\nMore precisely, we arrange all ï¬tness values in some time-\nlag ï¬tness matrix Î¦âˆˆRNÃ—Ndeï¬ned by Î¦(s,â„“) :=Ï•([s:\ns+â„“âˆ’1])forthestartingpoint sâˆˆ[1 :N]andthesegment\nlengthâ„“âˆˆ[1 :Nâˆ’s+ 1], whereas all other entries of Î¦\naresettozero,seeFigure3cforanexample. Notethateach\ncell(s,â„“)of the ï¬tness matrix Î¦deï¬nes an optimal path\nfamily for the segment Î±= [s:s+â„“âˆ’1]. The maximal\nentryofÎ¦yieldsthesegmentwith thehighestï¬tnessvalue,\nwhichcanberegardedasthemostrepresentativesegmentof\nthe recording. In this sense, a solution to our thumbnailing\nproblemisgivenby\nÎ±âˆ—:= argmax\nÎ±Ï•(Î±), (10)\nwhere the path family associated to Î±âˆ—yields the structure\nanalysisresult.\n4. EXPERIMENTS\nTo investigate the behavior of our ï¬tness measure, we have\nconductedvariousexperimentsusinga numberofchalleng-\ning audio recordings that exhibit strong acoustic deforma-\ntions and musical variations. We ï¬rst discuss some repre-\nsentative examples and then report on an experiment con-\nductedona corpusofï¬eldrecordings.\nWe start with the song â€œIn the year 2525â€ by Za-\nger and Evans, which already served as example in Fig-\nure 2 and Figure 3. This song has the musical form\n61812th International Society for Music Information Retrieval Conference (ISMIR 2011)\n  \n0 50 100 150 200020406080100120140160180200\nâˆ’2âˆ’1.5âˆ’1âˆ’0.500.51\n  \n0 50 100 150 200020406080100120140160180200\nâˆ’2âˆ’1.5âˆ’1âˆ’0.500.51\n  \n0 20 40 60 80 100 120 140 160 180 200020406080100120\n0.020.040.060.080.10.120.140.160.180.2\nTime(sec)Time(sec) Time(sec)Time(sec) Time-lag(sec)(a) (b)(a) (b)\n(c)\nFigure 4. Sand optimal path families Pover different Î±for an\nOrmandy recording of Brahmsâ€™ Hungarian Dance No. 5. (a)Î±=\n[67:87](maximal ï¬tness) (b)Î±= [130:195] .(c)Fitnessmatrix.\nAB1B2B3B4C1B5B6C2B7EB8Fstarting with a slow in-\ntro (A-part) and continuing with eight repetitions of a cho-\nrus section ( B-part), which are interleaved by two tran-\nsitionalC-parts and one E-part. The ï¬rst four B-parts\nare rather similar, whereas the parts B5andB6are trans-\nposed by one and B7andB8by two semitones upwards.\nUsing a transposition-invariant self-similarity matrix S, all\neight repeating B-parts are revealed by the path structure,\nsee Figure 2. Figure 3 shows the time-lag ï¬tness matrix\nÎ¦along with optimal path families for two different seg-\nments. The path family of the ï¬tness-maximizing segment\nÎ±âˆ—= [57:72] ,whichisshowninFigure3aandcorresponds\ntoB3,consistsofeightpaths. Thesepathscorrespondtothe\neightB-parts thus yielding the expected and desired result.\nLooking at other segments, one can notice that the ï¬tness\nmeasure tries to balance out score and coverage. For ex-\nample, for the long segment shown in Figure 3b, the lower\npath accepts even cells of negativescore (as long as the ac-\ncumulated score of the entire path is positive) for the sake\nof coverage. Here recall that, by deï¬nition, all paths of the\nfamilyareforcedtorunoverthe entiresegment Î±.\nNext, we consider a recording by Ormandy of the Hun-\ngarianDanceNo.5byJohannesBrahms,seeFigure4. This\npiece has the musical form A1A2B1B2CA3B3B4Dcon-\nsistingofthreerepeating A-parts,fourrepeating B-parts,as\nwell as a C- andaD-part. As shownbythe ï¬gure,thepath\nstructureof Sagainreï¬‚ectsthismusicalform. Inparticular,\nthecurvedpathsrevealthatthe B-partsareplayedindiffer-\nenttempi. Theï¬tness-maximizingsegmentis Î±âˆ—= [67:87]\nand corresponds to B2. As shown by Figure 4a, the path\nfamily consists of four paths, which correctly identify all\nfourB-parts. The segment Î±= [130:195] shown in Fig-\n  \n0100 200 300 400 500050100150200250300350400450500\nâˆ’2âˆ’1.5âˆ’1âˆ’0.500.51\n  \n0100 200 300 400 500050100150200250300350400450500\nâˆ’2âˆ’1.5âˆ’1âˆ’0.500.51\n  \n0 50 100 150 200 250 300 350 400 450 500050100150200250300\n00.020.040.060.080.10.120.140.16(a) (b) lower bound\nTime(sec)Time(sec) Time(sec)Time(sec) Time-lag(sec)(a) (b)\n(c)\nFigure 5. Sand optimal path families Pover different Î±for a\nPollini recording of Beethovenâ€™s Op. 31, No. 2, ï¬rst movemen t\n(â€œTempestâ€). (a)Î±= [11:119] (maximal ï¬tness when using the\nlower bound Î»= 20 seconds .)(b)Î±= [483 : 487] (maximal\nï¬tness).(c)Fitnessmatrix.\nure 4b correspondsto A3B3B4. Here note that because our\nï¬tnessmeasuredisregardsself-explanations,theï¬tnesso fÎ±\niswell belowtheoneof Î±âˆ—.\nIn our third example, we consider a Pollini recording\nof the ï¬rst movement of Beethovenâ€™s piano sonata Op. 31,\nNo. 2 (â€œTempestâ€), see Figure 5. Being in the sonata form,\nthe rough musical form of this movement is A1A2BA3C\nwithA1beingtheexposition, A2therepetitionoftheexpo-\nsition,Bthe development, A3the recapitulation, and Ca\nshortcoda. Here,eventhough A3issomekindofrepetition\nofA1, there are signiï¬cant musical differences. For exam-\nple,theï¬rstthemein A3isextendedbyanadditionalsection\nnot present in A1and the secondtheme in A3is transposed\nï¬ve semitones upwards (and later transposed seven semi-\ntones downwards)relative to the second themein A1. Here\nnotethatthemodulationdoesnotapplytotheentire A3-part\nbut only to the second theme within the A3-part. Never-\ntheless, using transposition-invariance, our ï¬tness meas ure\ncan still identify the relation of the three A-parts when us-\ningÎ±= [11:119] , see Figure 5a. Interestingly, this is not\nthe ï¬tness-maximizingsegment, which is actually given by\nÎ±âˆ—= [483:487] , see Figure 5b. This example indicates a\nproblemthatoccurswhentheself-similaritymatrixcontai ns\na lot of noise, i.e., scattered cells of relatively high scor e.\nSuch cells may form numerous path fragments that, as a\nwhole family, may yield signiï¬cant average score as well\nas coveragevalues. To circumventsuch problems,onemay\nintroducealowerbound Î»fortheminimalpossiblesegment\nlength. For example, using a lower bound Î»= 20 seconds ,\ntheï¬tness-maximizingsegmentis Î±= [11:119] .\n619Oral Session 7: Structure Analysis and Mixing\nFinally, we report on an experiment using ï¬eld record-\nings of the folk song collection Onder de groene linde\n(OGL), which is part of the Nederlandse Liederenbank .1\nEach song basically consists of a numberof strophesyield-\ning the musical form A1A2...AK. The main challenge is\nthat the songs are performed by elderly non-professional\nsingers with serious intonation problems, large tempo\nchanges, and interruptionsâ€”not to speak of poor record-\ning conditions and background noise. In [8], a reference-\nbased segmentation algorithm, which reverts to an addi-\ntional MIDI ï¬le used as stanza reference, is described and\ntestedfor 47ofthesesongs. Asforevaluation,standardpre-\ncision, recall and F-measuresare used to measurethe accu-\nracyofthesegmentationboundaries(withatoleranceof Â±2\nseconds). Theresultsofthisreference-basedmethod,whic h\nareshowninthelast rowofTable1,serveasbaseline.\nOurapproachcanbeappliedforaccomplishingthesame\nsegmentation task without reverting to any reference. To\nthis end, we determine the ï¬tness-maximizing segment Î±âˆ—\nas in (10) and derive the segmentation from the associ-\nated path family. Using the same evaluation measures as\nin [8],ourreference-freemethodyieldsanF-measurevalue\nofF = 0.821, see Table 1. Assuming some prior knowl-\nedge on the minimal length of a stanza, this result can\nbe improved. For example, using the lower bound Î»=\n10 seconds oneobtains F = 0.855,see Table 1. This result\nis still worse than the results obtained from the reference-\nbased approach ( F = 0.926). Actually, a manual inspec-\ntion showed that this degrade was mainly caused by four\nparticular recordings, where the segmentationderived fro m\nÎ±âˆ—was â€œphase-shiftedâ€ comparedto the groundtruth. Em-\nployingaboundary-basedevaluationmeasureresultedinan\nF-measure of F = 0for these four recordings. Further-\nmore, we found out that these phase shifts were caused by\nthe fact that in all of these four recordings the singer com-\npletely failed in the ï¬rst stanza (omittingand confusingen -\ntire verse lines). In a ï¬nal experiment,we replacedthe four\nrecordings by a slightly shortened version by omitting the\nï¬rst stanzas, respectively. Repeating the previous experi -\nment on this modiï¬ed dataset produced an F-measure of\nF = 0.920, which is already close to the quality obtained\nbybaselinemethod. Overall,theseresultsdemonstratesth at\nourï¬tnessmeasurecancopeevenwith strongtemporaland\nspectralvariationsasoccurringinï¬eld recordings.\n5. CONCLUSIONS\nIn this paper, we introduced a novel ï¬tness measure that\nexpresses how representative a given segment is in terms\nof repetitiveness. Our experiments showed that the ï¬tness-\nmaximizingsegmentoftenyieldsa goodcandidatesolution\nforthethumbnailingproblem,eveninthepresenceofstrong\n1www.liederenbank.nlStrategy P R F\nMaximal ï¬tness 0.823 0.818 0.821\nMaximal ï¬tness ( Î»= 10) 0.863 0.847 0.855\nMaximal ï¬tness ( Î»= 10, modiï¬ed dataset) 0.932 0.909 0.920\nReference-based method [8] 0.912 0.940 0.926\nTable 1.Precision, recall,and F-measures for the reference-based\nsegmentation method [8] and the three reference-free metho ds de-\nscribed inthispaper.\nacoustic and musical variations across repeating parts. We\nalso introduced a time-lag ï¬tness matrix that yields a high-\nlevel view on the structural properties for the entire music\nrecording. For the future, we need to explore in more de-\ntailtheroleofthedifferentparametersettings,includin gthe\nroleofthe self-similaritymatrix. We areconvincedthat ou r\nï¬tnessmatrixhasgreatpotentialforvisualizingandsearc h-\ning in hierarchical music structures in novel ways. Finally ,\nefï¬ciency issues need to be addressed as well as iterative\napproachesthat allowforderivingtheentiremusicalform.\nAcknowledgement. This work has been supported by the\nCluster of Excellenceon Multimodal Computingand Inter-\nactionat SaarlandUniversity.\n6. REFERENCES\n[1] Mark A. Bartsch and Gregory H. Wakeï¬eld. Audio thumbnail ing of\npopular musicusingchroma-based representations. IEEETransactions\non Multimedia , 7(1):96â€“104, February 2005.\n[2] Matthew Cooper and Jonathan Foote. Summarizing popular music via\nstructural similarity analysis. In Proceedings ofthe IEEEWorkshop on\nApplications of Signal Processing to Audio and Acoustics (W ASPAA),\npages 127â€“130, New Paltz, NY, US,2003.\n[3] Roger B. Dannenberg and Masataka Goto. Music structure a naly-\nsis from acoustic signals. In David Havelock, Sonoko Kuwano , and\nMichael VorlÂ¨ ander, editors, Handbook of Signal Processing in Acous-\ntics, volume1, pages 305â€“331. Springer, New York,NY, USA,2008.\n[4] Roger B. Dannenberg and Ning Hu. Pattern discovery techn iques for\nmusicaudio. In Proceedings ofthe International Conference on Music\nInformation Retrieval (ISMIR) , Paris, France, 2002.\n[5] Masataka Goto. A chorus section detection method for mus ical audio\nsignals and its application to a music listening station. IEEE Transac-\ntions on Audio, Speech and Language Processing , 14(5):1783â€“1794,\n2006.\n[6] Meinard MÂ¨ uller. Information RetrievalforMusicandMotion . Springer\nVerlag, 2007.\n[7] Meinard MÂ¨ uller and Michael Clausen. Transposition-in variant self-\nsimilarity matrices. In Proceedings ofthe8thInternational Conference\non Music Information Retrieval (ISMIR) , pages 47â€“50, Vienna, Aus-\ntria, September 2007.\n[8] Meinard MÂ¨ uller, Peter Grosche, and Frans Wiering. Robu st segmen-\ntation and annotation of folk song recordings. In Proceedings of the\n10th International Society forMusicInformation Retrieva l Conference\n(ISMIR), pages 735â€“740, Kobe, Japan, October 2009.\n[9] JouniPaulusand AnssiKlapuri. Musicstructureanalysi s usingaprob-\nabilistic ï¬tness measureand agreedy search algorithm. IEEETransac-\ntions on Audio, Speech, and Language Processing , 17(6):1159â€“1170,\n2009.\n[10] Jouni Paulus, Meinard MÂ¨ uller, and Anssi Klapuri. Audi o-based music\nstructureanalysis. In Proceedingsofthe11thInternational Conference\non MusicInformation Retrieval (ISMIR) ,pages 625â€“636, Utrecht, The\nNetherlands, 2010.\n620"
    },
    {
        "title": "Constrained Spectrum Generation Using A Probabilistic Spectrum Envelope for Mixed Music Analysis.",
        "author": [
            "Toru Nakashika",
            "Tetsuya Takiguchi",
            "Yasuo Ariki"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1415774",
        "url": "https://doi.org/10.5281/zenodo.1415774",
        "ee": "https://zenodo.org/records/1415774/files/NakashikaTA11.pdf",
        "abstract": "NMF (Non-negative Matrix Factorization) has been one of the most widely-used techniques for musical signal analysis in recent years. In particular, the supervised type of NMF is garnering much attention in source separation with respect to the analysis accuracy and speed. In this approach, a large number of spectral samples is used for analyzing a signal. If the system has a minimal number of samples, the accuracy deteriorates. Because such methods require all the possible samples for the analysis, it is hard to build a practical analysis system. To analyze signals properly even when short of samples, we propose a novel method that combines a supervised NMF and probabilistic search algorithms. In this approach, it is assumed that each instrumental category has a model-invariant feature called a probabilistic spectrum envelope (PSE). The algorithm starts with learning the PSEs of each category using a technique based on Gaussian Process Regression. Using the PSEs for spectrum generation, an observed spectrum is analyzed under the framework of a supervised NMF. The optimum spectrum can be searched by Genetic Algorithm using sparseness and density constraints.",
        "zenodo_id": 1415774,
        "dblp_key": "conf/ismir/NakashikaTA11",
        "keywords": [
            "Non-negative Matrix Factorization",
            "source separation",
            "supervised type",
            "analysis accuracy",
            "speed",
            "spectrum samples",
            "probabilistic search algorithms",
            "probabilistic spectrum envelope",
            "Gaussian Process Regression",
            "Genetic Algorithm"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nCONSTRAINED SPECTRUM GENERATION USING A PROBABILISTIC\nSPECTRUM ENVELOPE FOR MIXED MUSIC ANALYSIS\nToru Nakashika, Tetsuya Takiguchi, Yasuo Ariki\nDepartment of Computer Science and Systems Engineering, Kobe University, Japan\nnakashika@me.cs.scitec.kobe-u.ac.jp, {takigu,ariki}@kobe-u.ac.jp\nABSTRACT\nNMF (Non-negative Matrix Factorization) has been one of\nthe most widely-used techniques for musical signal analysis\nin recent years. In particular, the supervised type of NMF is\ngarnering much attention in source separation with respect\nto the analysis accuracy and speed. In this approach, a large\nnumber of spectral samples is used for analyzing a signal. If\nthe system has a minimal number of samples, the accuracy\ndeteriorates. Because such methods require all the possible\nsamples for the analysis, it is hard to build a practical anal-\nysis system. To analyze signals properly even when short\nof samples, we propose a novel method that combines a su-\npervised NMF and probabilistic search algorithms. In this\napproach, it is assumed that each instrumental category has\na model-invariant feature called a probabilistic spectrum en-\nvelope (PSE). The algorithm starts with learning the PSEs\nof each category using a technique based on Gaussian Pro-\ncess Regression. Using the PSEs for spectrum generation,\nan observed spectrum is analyzed under the framework of a\nsupervised NMF. The optimum spectrum can be searched by\nGenetic Algorithm using sparseness and density constraints.\n1. INTRODUCTION\nMixed music analysis (estimating the pitch and instrument\nlabels of each musical note from a single-channel polyphonic\nmusic signal with multiple instruments) has been recognized\nas one of the most challenging tasks in musical signal pro-\ncessing. To achieve this, many approaches have been pro-\nposed so far: ICA-based methods [1, 3], HTTC (Harmonic-\nTemporal-Timbral Clustering) [6], Instrogram [5], etc. Of\nall these techniques, the methods based on NMF (Non-negative\nMatrix Factorization) have attracted considerable attention\nlately as a way to analyze signals more effectively and more\neasily. In many of these techniques, an observed spectro-\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proï¬t or commercial advantage and that copies\nbear this notice and the full citation on the ï¬rst page.\nc/circlecopyrt2011 International Society for Music Information Retrieval.gram matrix can be represented as a linear combination of\ntwo matrices: a basis matrix whose columns roughly indi-\ncate spectrums of each musical source with various pitches\nand instruments, and an activity matrix which shows tempo-\nral information of each basis vector.\nNMF-based analysis methods are broadly divided into\ntwo categories: an unsupervised approach [4] and a super-\nvised approach [2]. Since the former approach decomposes\nthe spectrogram without the assumption of the spectral struc-\ntures of audio sources, the unintended basis matrix and ac-\ntivity matrix will be obtained. Therefore, it is hard to an-\nalyze mixed-source audio correctly using an unsupervised\napproach.\nOn the other hand, a supervised approach decomposes a\nmixed musical signal using the spectral templates of each\nmusical source, which are learned beforehand. Compared\nto an unsupervised approach, this technique tends to pro-\nduce preferable results in terms of analysis speed and ac-\ncuracy. However, if unlearned sounds are contained in the\ntest signal, the accuracy may deteriorate because there are\nmany different types (models) of instrument that belong to\nthe same instrumental category. For example, the â€œPianoâ€\ncategory includes different models: â€œPiano1â€,â€œPiano2â€, and\nso on. To improve the decomposition accuracy, many kinds\nof spectral templates (not only different categories but dif-\nferent models in the categories) should be trained. However,\nthis is extremely difï¬cult to build into a real system.\nTo solve this problem, we propose a novel method of\nmixed music analysis, which uses a model-invariant fea-\nture (probabilistic spectrum envelope; PSE) of each cate-\ngory. This feature is derived from the following idea. An\ninstrumentâ€™s spectrum can differ slightly due to various fac-\ntors associated not only with the type of instrument (model)\nbut also the manufacturer, the materials used, the tempera-\nture, humidity, and playing-style, etc. However, the way the\nspectrum ï¬‚uctuates is not completely random, as it depends\non the instrumentâ€™s category. Therefore, we introduce the\nPSE feature that does not depend on the pitch, the model,\nthe material, and other various factors. This is similar to a\nspectrum envelope feature, which does not depend on the\npitch. The feature is deï¬ned as a set of the mean spec-\n181Poster Session 2\ntrum envelope and variance spectrum envelope in the time-\nfrequency domain as shown in Figure 1 (a). Once the PSE\nis estimated, any spectrum belonging to the category can be\nobtained by multiplying various comb ï¬lters and randomly-\ngenerated spectrum envelopes from the PSE.\nFigure 1 shows a system ï¬‚owchart of mixed music anal-\nysis under the PSE framework. In our approach, unsuper-\nvised NMF and extended Gaussian Process (SPGP+HS [7])\nare employed to estimate the PSE features of each category\non the training stage. At the analysis stage, we use super-\nvised NMF for the analysis, in which an optimum basis vec-\ntor can be searched using a Genetic Algorithm with sparse-\nness and density constraints.\nSTFTunsupervised NMFSTFTsupervised NMFTraining signalsTest signal\nSPGP+HSGenetic Algorithm+(b) Training stage(c) Analysis stagePSEs\nAmplitudeFrequency(a) PSEs of each category\nSpectrum GenerationSeparated sourcesSupervised NMF\nFigure 1 . Flowchart of mixed music analysis using propa-\nbilistic spectrum envelope (PSE). The red and blue color in-\ndicate the large and small values of probability, respectively.\nThe black and white lines are the mean envelope, and mean\nplus/minus variance envelope.\n2. PSE ESTIMATION\n2.1 Spectral peaks extraction\nThe probabilistic spectrum envelope (PSE) of each category\nis estimated by SPGP+HS regression [7] in this paper. In\nthis section, we will discuss the way spectral peaks (input\nsamples used for the regression) are obtained.\nFirst, we prepare some acoustic signals, each of which\ncontains only the needed musical sources of the instrumen-\ntal category. The various sources do not sound at the same\ntime. In this paper, 12 half-tone sources sound in sequence\nevery octave. Employing NMF to the amplitude spectro-\ngramV(âˆˆRFÃ—T)of the signal, Vis approximately de-\ncomposed into the product of a basis matrix W(âˆˆRFÃ—R)\nand an activity matrix H(âˆˆRRÃ—T)as follows:\nVâ‰ˆWH (1)\nâˆ€i, j, k,Wijâ‰¥0,Hjkâ‰¥0 (2)\nwhere F, T andRare the numbers of bins of frequency, time\nand bases, respectively (here, R= 12 ).\nWandHcan be obtained by iteratively calculating up-\ndate rules based on Euclidean divergence. The update rulesfor each matrix element are:\nWijâ†Wij(VHT)ij\n(WHHT)ij(3)\nHjkâ†Hjk(WTV)jk\n(WTWH)jk. (4)\nFrom the updated matrix W, a set of Nspectral peaks\nP= (f,y)={(fn, yn)}nare exploited, where fnandyn\nare frequency and amplitude of the n-th peak, respectively.\nThese peaks are found by searching for the harmonic peaks\nof each basis vector.\n2.2 PSE estimation using SPGP+HS\nIn this paper, the PSE of each category can be estimated\nby extended Gaussian Process (SPGP+HS [7]), which can\napproximate the shape of any function with varying variance\nmore accurately than the standard Gaussian Process.\nBy giving a set of peaks, P, to one-dimensional SPGP+HS,\nwe obtain PSE mean envelope Âµfand PSE variance enve-\nlopeÏƒf, as follows:\nÂµf=KffmQK fmfnÎ›âˆ’1y (5)\nÏƒf=Kffâˆ’Kffm(Kâˆ’1\nfmfmâˆ’Q)KT\nffm (6)\nwhere,Q=(\nKfmfm+KfmfnÎ›âˆ’1KT\nfmfn)âˆ’1\nandÎ›=\ndiag(Kfnfnâˆ’KT\nfmfnKâˆ’1\nfmfmKfmfn).Kabis a gram ma-\ntrix between aandbwith a parameter Î¸. Pseudo-inputs\nÂ¯f={Â¯fm}M\nm=1indicate the representatives of any inputs\nf, satisï¬ed M/lessmuchN.hmâˆˆhdenotes an uncertainty pa-\nrameter to the pseudo-input Â¯fm. We can ï¬nd the optimum\nparameters h, Î¸,Â¯fbased on a gradient-based method (for\nmore details, see [7]).\n3. ANALYSIS METHOD\n3.1 Spectrums generation based on PSE\nThe spectrum envelope ec(f)based on the PSE of category\ncis randomly generated as follows:\nec(f)âˆ¼N(Âµc\nf, Ïƒc\nf). (7)\nN(Âµ, Ïƒ)shows the normal distribution of mean Âµand vari-\nance Ïƒ.\nSpectrum p(f), with a fundamental frequency f0along\nthe envelope, ec(f)can be speciï¬cally calculated in Eq. (8).\np(f) = max(\nec(f),0)\nÂ·Î¨(f;f0) (8)\nThe reason for the maximum expression in Eq. (8) is that a\nspectrum cannot have negative values. Î¨(f;f0)is a comb\nï¬lter with a fundamental frequency f0, calculated as:\nÎ¨(f;f0) =âˆ‘\nlexp{\nâˆ’(fâˆ’f0Â·l)2\n2Î½2}\n(9)\n18212th International Society for Music Information Retrieval Conference (ISMIR 2011)\nwhere lis the index of Gaussian components, and Î½is a\nhyper-parameter to determine the kurtosis of each compo-\nnent.\nUsing the above procedure, we can obtain an intended\nbasis matrix ËœWwhose columns (spectrums) are randomly\ngenerated for various categories and fundamental frequen-\ncies.\n3.2 Basis matrix optimization using Genetic Algorithm\nWhat we want to do in the analysis stage is to ï¬nd the op-\ntimum NMF matrices Ë†WandË†Hfor a given test signal. To\ndo this, we introduce an optimization method based on ge-\nnetic algorithm (GA), which is a method for ï¬nding the op-\ntimum by repeating natural-evolution-inspired techniques:\nselection, crossover, mutation and inheritance.\nGiven an amplitude spectrogram Xof a test signal and\na randomly-generated basis matrix ËœW, the activity matrix\nHcan be calculated by applying supervised NMF with ËœW.\nThat is, each element of His repeatedly updated by Eq. (4)\nwhile keeping ËœWï¬xed. Since ËœWdetermines Hin this cal-\nculation, Hcan be considered as a function of ËœW. IfËœWhas\nbetter (more suited) spectral columns for the test signal, the\ndistance between XandËœWH must become smaller. There-\nfore, the minimization of Euclidean distance DEUC(X,ËœWH)\ncan be used as a criterion for ï¬nding the candidate ËœW. In\naddition to the distance criterion, we give two constraints\nsp(H)andden(H). The former sp(H)leads the matrix H\nto be sparse, which is\nsp(H) =#{(j, k)|Hjkâ‰¤/epsilon1}\nRÃ—T(10)\nwhere, /epsilon1(â‰¥0)is a small value (in our experiments, /epsilon1= 0.1).\nThe other constraint den(H)represents the â€œdensityâ€ of\nthe elements in H. This idea is inspired by the fact that\nmusical notes of each instrument tend to group together in\nregard to time and tone. We deï¬ne the constraint den(H)\nas:\nden(H) =âˆ‘\nk,l,l/primeexp{\nâˆ’(sk,lâˆ’sk+1,l/prime)2\n2Ï2}\nâˆ‘\nkNk(11)\n{sk,l}Nk\nl=1={j|Hjkâ‰¥/epsilon1} (12)\nwhere Ïis a constant factor for determing the allowance for\ndistant tones (in our test, Ï= 3).\nFinally, we set the criteria for the optimum search of the\ncandidate ËœWas follows:\nÎ˜(ËœW) =DEUC(X,ËœWH)âˆ’Î±Â·sp(H)âˆ’Î²Â·den(H)(13)\nwhere, Î±(â‰¥0)andÎ²(â‰¥0)are weight parameters that\nreï¬‚ect the effects of sparseness and density constraints, re-\nspectively.In our analysis method, the optimum basis matrix Ë†W\nis obtained using GA to minimize the objective function\n(13). The ï¬rst step of GA is to generate U(= 12 , in our\ntests) basis matrices {ËœWu}U\nu=1from pre-trained PSEs (See\n3.1.), and evaluate the objective function for each matrix by\nEq. (13). Note that fundamental frequency of each column\nin the u-th basis matrix ËœWuis different from the others, but\nthe fundamental frequency of the l-th column for all basis\nmatrices has the same fundamental frequency. To update\nthe whole set, the following process is repeated G(= 100 ,\nin this paper) times:\n1. Copy the best (smallest-objective) basis matrix of the\nprevious generation to the current generation.\n2. With a probability pcross , exchange two selected basis\nmatrices according to the uniform crossover.\n3. With a probability pmut, mutate a selected basis ma-\ntrix based on PSE.\n4. Repeat step 2 and 3 until the number of basis matrices\nof the current generation reaches L.\nConcerning the expression â€œselectâ€ above, the probability\nofu-th candidate selection is deï¬ned asÎ˜(ËœWu,ËœHu)PU\nu=1Î˜(ËœWu,ËœHu).\nThis shows that the better ËœWutends to be selected more.\npcross andpmutin steps 2 and 3 are respectively the prob-\nabilities of crossover and mutation, which satisfy pcross +\npmut= 1 (in this paper, pcross = 0.9, pmut= 0.1). Fur-\nthermore, our GA has the constraints that each basis matrix\nmutates without altering the fundamental frequencies. In\nother words, the mutated new vector is calculated by multi-\nplying the randomly-generated spectrum envelope from PSE\nby the comb ï¬lter that has the same fundamental frequency\nas the original one. Therefore, basis matrices of each gener-\nation can be generated without changing the information on\nthe fundamental frequency and category we set at ï¬rst.\nThe ï¬nal analysis result is the optimum NMF matrices\nË†WandË†H, which are the best matrices in G-th generation\n(Ë†His obtained by supervised NMF with the optimum basis\nmatrix Ë†W). Because Ë†Wcontains a category index c, a test\nsignal can be decomposed into each instrument.\n4. EXPERIMENTS\nTo evaluate our proposed method, â€œwav-to-midâ€ tests were\nconducted. In these experiments, an acoustic data synthe-\nsized with MIDI sounds is automatically converted into MIDI\nformat. A part of â€œRWC-MDB-C-2001 No. 43: Sicilienne\nop.78â€ from RWC Music Database1was used for the test\n(Figure 3 (a)). The monaural test signal was recorded at a\n16kHz sampling rate using multiple MIDI instruments: Pi-\nano and Flute (exactly, â€œPiano1â€ and â€œFlute1â€ instrumental\n1http://staff.aist.go.jp/m.goto/RWC-MDB/\n183Poster Session 2\nmodels of MIDI, respectively). Before the test, PSEs for the\ntwo categories were trained using the different sounds from\nthe test signal (â€œPiano2â€ for â€œPianoâ€ PSE and â€œFlute2â€ for\nâ€œFluteâ€ PSE). Using the PSEs, GA found the optimum ma-\ntrices Ë†WandË†H. By binarizing Ë†Hwith an adequate thresh-\nold, we obtained the ï¬nal results of MIDI format. The re-\nsults were compared for the cases in which the objective\nfunction of GA has sparseness and density constraints and\nwhen it does not (â€œsp+denâ€, â€œspâ€,â€œdenâ€,â€œw/oâ€). Since the\nresults depend on the initial values of {ËœWu}U\nu=1, we re-\npeated each method by 100times and computed the mean,\nmaximum, and minimum values of accuracy. We also com-\npared the results with the conventional method, supervised\nNMF (â€œs-NMFâ€ given the basis matrix of â€œPiano2â€, â€œidealâ€\ngiven that of â€œPiano1â€).\nFigure 2 illustrates MIDI-conversion accuracies for each\nmethod. The accuracy is calculated asNallâˆ’(Nins+Ndel)\nNallÃ—\n100, where Nall, NinsandNdelmean the total number of\nnotes, insertion errors, and deletion errors, respectively. Be-\ncause onset time and the duration of each sound source are\nnot necessarily correct in the above binarizing process, we\npermitted the duration to differ and the onset time to shift\nÏ„seconds (in this paper, Ï„= 0.3). The bar values of our\nmethods in the ï¬gure are average accuracies for 100tries,\nand the error bars indicate maximum and minimum values\nof the tries. Concerning the results of conventional meth-\nods, if the system knows exactly the same sounds as the test\nsignal, it yields high performance (ideal). However, if the\nsystem does not know, the accuracy deteriorates dramati-\ncally (s-NMF). Meanwhile, each of our approaches main-\ntains high accuracy even when the system does not learn\nthe sounds of the test data. The preferable results are due\nto the fact that each PSE can be estimated by only various\npitches, and it can cover spectrum envelopes of unknown\nmodels. Comparing within our approaches, the system with\nsparseness or density constraints achives better accuracy,\nand when both constraints were added (â€œsp+denâ€), for the\ntests with the best results, there were cases when the accu-\nracy even exceeded the ideal value.\nAn analysis example of â€œsp+denâ€ tries is shown in Fig-\nure 3 (b). Almost all the notes were estimated correctly,\nbut parts of them were mistaken as octave-different notes.\nTherefore, we will improve the accuracy by adding other\nconstraints to avoid octave differences in the future.\n020406080100sp+denspdenw/os-NMFidealAccuracy [%]\nFigure 2 . Accuracy rates of each method.\nTime [sec.]\nFlutePianoFlutePiano\nC2G#2E3C4G#4E5C4G#4E5\n024681012C2G#2E3C4G#4E5Figure 3 . (above) Piano-roll representation of test MIDI\ndata. The red and purple parts indicate piano and violin\ntones, respectively. (below) An example of analysis results\nwith sparseness and density constraints.\n5. CONCLUSIONS\nIn this paper, we proposed an algorithm for monaural sound\nsource decomposition and multiple-pitch estimation. The\nmethod categorizes several spectrum envelopes for each mu-\nsical category, inspired by invariance of spectral ï¬‚uctuation\nin a category. This categorized envelope, called the prob-\nabilistic spectrum envelope (PSE), has a characteristic of\nbeing able to absorb differences between models, pitches,\nmanufactures, playing-style, and so on. PSE consists of a\nmean envelope and variance envelope which can be simul-\ntaneously estimated by SPGP+HS regression as described\nin this paper. In the analysis stage, Genetic Algorithm (GA)\nwith supervised-NMF-based objective and sparseness/density\nconstraints was employed for an optimum search in all the\nspectrum envelopes that can be generated from the PSE.\nThe simulation experiments using MIDI sources show\nthat the proposed method is robust to instrumental model\nchanges. Since the results depend on the initial values, how-\never, future research will include designing a directly opti-\nmum search method, such as ML (Maximum likelihood) or\nMAP (Maximum a posteriori) estimations.\n6. REFERENCES\n[1] M.A. Casey and A. Westner: In Proceedings of the Interna-\ntional Computer Music Conference , pp. 154â€“161, 2000.\n[2] A. Cont, S. Dubnov, and D. Wessel: In Proceedings of Digital\nAudio Effects Conference (DAFx) , pp. 10â€“12, 2007.\n[3] Gil jin Jang and Te won Lee: Journal of Machine Learning\nResearch , pp. 1365â€“1392, 2003.\n[4] M. Kim and S. Choi: Independent Component Analysis and\nBlind Signal Separation , pp 617â€“624, 2006.\n[5] T. Kitahara, M. Goto, K. Komatani, T. Ogata, and H.G. Okuno:\nInformation and Media Technologies , pp. 279â€“291, 2007.\n[6] K. Miyamoto, H. Kameoka, T. Nishimoto, N. Ono, and\nS. Sagayama: ICASSP 2008 , pp. 113â€“116, 2008.\n[7] E. Snelson and Z. Ghahramani: In Proceedings of the 22nd\nUncertainty in Artiï¬cial Intelligence , 2006.\n184"
    },
    {
        "title": "A Classification-Based Polyphonic Piano Transcription Approach Using Learned Feature Representations.",
        "author": [
            "Juhan Nam",
            "Jiquan Ngiam",
            "Honglak Lee",
            "Malcolm Slaney"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1418351",
        "url": "https://doi.org/10.5281/zenodo.1418351",
        "ee": "https://zenodo.org/records/1418351/files/NamNLS11.pdf",
        "abstract": "Recently unsupervised feature learning methods have shown great promise as a way of extracting features from high dimensional data, such as image or audio. In this paper, we apply deep belief networks to musical data and evaluate the learned feature representations on classification-based polyphonic piano transcription. We also suggest a way of training classifiers jointly for multiple notes to improve training speed and classification performance. Our method is evaluated on three public piano datasets. The results show that the learned features outperform the baseline features, and also our method gives significantly better frame-level accuracy than other state-of-the-art music transcription methods.",
        "zenodo_id": 1418351,
        "dblp_key": "conf/ismir/NamNLS11",
        "keywords": [
            "unsupervised feature learning",
            "deep belief networks",
            "musical data",
            "polyphonic piano transcription",
            "classification-based",
            "jointly training classifiers",
            "multiple notes",
            "training speed",
            "classification performance",
            "public piano datasets"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nA CLASSIFICATION-BASED POLYPHONIC PIANO TRANSCRIPTION\nAPPROACH USING LEARNED FEATURE REPRESENTATIONS\nJuhan Nam\nStanford University\njuhan@ccrma.stanford.eduJiquan Ngiam\nStanford University\njngiam@cs.stanford.eduHonglak Lee\nUniv. of Michigan, Ann Arbor\nhonglak@eecs.umich.eduMalcolm Slaney\nYahoo! Research\nmalcolm@ieee.org\nABSTRACT\nRecently unsupervised feature learning methods have shown\ngreat promise as a way of extracting features from high di-\nmensional data, such as image or audio. In this paper, we\napply deep belief networks to musical data and evaluate the\nlearned feature representations on classiï¬cation-based poly-\nphonic piano transcription. We also suggest a way of train-\ning classiï¬ers jointly for multiple notes to improve training\nspeed and classiï¬cation performance. Our method is evalu-\nated on three public piano datasets. The results show that the\nlearned features outperform the baseline features, and also\nour method gives signiï¬cantly better frame-level accuracy\nthan other state-of-the-art music transcription methods.\n1. INTRODUCTION\nMusic transcription is the task of transcribing audio into a\nscore. It is a challenging problem because multiple notes\nare often played at once (polyphony), and thus individual\nnotes interfere by virtue of their harmonic relations.\nA number of methods have been proposed since Moorer\nï¬rst attempted to use computers for automatic music tran-\nscription [10]. State-of-the-art methods can be categorized\ninto three approaches: iterative F0 searches, joint source es-\ntimation and classiï¬cation-based approaches. Iterative F0-\nsearching methods ï¬rst ï¬nd the predominant F0 and subtract\nits relevant sources (e.g. harmonic partials) from the input\nsignal and then repeat the procedure on what remains until\nno additional F0s are found [6]. Joint source estimation ex-\namines possible combinations of sound sources by hypoth-\nesizing that the input signal is approximated by a weighted\nsum of the sound sources with different F0s [3].\nWhile these two methods are based on utilizing the struc-\nture of musical tones, classiï¬cation-based approaches ad-\ndress polyphonic transcription as a pattern-recognition prob-\nlem. The idea is to use multiple binary classiï¬ers, each of\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proï¬t or commercial advantage and that copies\nbear this notice and the full citation on the ï¬rst page.\nc/circlecopyrt2011 International Society for Music Information Retrieval.which corresponds to a note class. They are trained with\nshort-time acoustic features and labels for the corresponding\nnote class (i.e., note on/off) and then used to predict the note\nlabels for new input data. Although classiï¬cation-based ap-\nproaches make minimum use of knowledge of acoustics,\nthey show comparable results to iterative F0 searches and\njoint source estimation, particularly for piano music [9, 12].\nHowever, when the training set is limited or the piano in the\ntest set has different timbre, tuning or recording environ-\nments, classiï¬cation-based approaches can overï¬t the train-\ning data, a problem common to many supervised learning\ntasks [13]. As a means to obtain features robust to acoustic\nvariations, researchers have designed networks of adaptive\noscillators on auditory ï¬lter banks or normalized spectro-\ngram on the frequency axis [9, 12].\nThe majority of machine learning tasks rely on these kinds\nof hand-engineered approaches to extract features. Recently,\non the other hand, unsupervised feature learning methods\nthat automatically capture the statistical relationship in data\nand learn feature representations have shown great promise.\nIn particular, deep belief networks have been successfully\napplied to many computer-vision and speech-recognition ta-\nsks as an alternative to typical feature-extraction methods,\nbut also a few music-related tasks [4, 8].\nIn this paper, we apply deep belief networks to poly-\nphonic piano transcription. Speciï¬cally, we extend a previ-\nous classiï¬cation-based approach in two ways: (1) by using\nlearned feature representations for note classiï¬ers and (2) by\njointly training the classiï¬ers for multiple notes. In particu-\nlar, the latter associates deep belief networks with multi-task\nlearning. The results show that our approach outperforms\ncompared music transcription methods for several test sets.\n2. FEATURE LEARNING\nDeep belief networks (DBNs) are constructed by stacking\nrestricted Boltzmann machines (RBMs) and training them\nin a greedy layer-wise manner. In this section, we brieï¬‚y\nreview RBMs and how to build a deep structure.\n2.1 Sparse Restricted Boltzmann Machines\nThe RBM is a two layer undirected graphical model that has\nhidden nodes hand visible nodes v[11]. The visible nodes\n175Poster Session 2\nrepresent the data while the hidden nodes represent the fea-\ntures discovered by training the RBM. For each possible as-\nsignment to the hidden and visible nodes, the RBM speci-\nï¬es the probability of the assignment (Eq. 1). The RBM has\nsymmetric connections between the two layers denoted by a\nweight matrix W, but no connections within hidden nodes\nor visible nodes. This particular conï¬guration makes it easy\nto compute the conditional probability distributions, when v\norhis ï¬xed (Eq. 2). In practice, one uses this conditional\nprobability of the hidden nodes as the â€œlearnedâ€ features:\nâˆ’logP(v,h)âˆE(v,h) =\n1\n2Ïƒ2vTvâˆ’1\nÏƒ2/parenleftbig\ncTv+bTh+hTWv/parenrightbig\n(1)\np(hj|v) =sigmoid (1\nÏƒ2(bj+wT\njv)) (2)\nwhere Ïƒ2is a scaling parameter, bandcare learned bi-\nases, and Wis a learned weight matrix. This formulation\nmodels the visible nodes as real-valued Gaussian units and\nthe hidden nodes as binary units. We further regularize the\nmodel with sparsity by encouraging each hidden unit to have\na pre-determined expected activation using a regularization\npenalty [7].\n2.2 Deep Belief Network\nA deep network is composed of multiple non-linear hidden\nlayers (as opposed to a shallow network with a single hid-\nden layer). Each layer in a deep network builds upon rep-\nresentations discovered by the previous layer to represent\nmore complex features of the data. A DBN is trained by\nâ€œgreedy layer-wise stackingâ€ of RBMs. First, a single layer\nRBM is trained to model the data. This RBM learns a set of\nweights Wand biases b,cthat we ï¬x as the parameters of\nthe ï¬rst layer of the DBN. To learn the next layer of weights\nand biases, we compute the features discovered by the ï¬rst\nlayer RBM (Eq. 2) and apply them to a binary-binary RBM\n(which has binary input units instead of Gaussian) to learn\nanother layer of representation; this forms the parameters\nfor our next layer of features. Deeper layers are learned in\na similar fashion. Hinton et al. showed that the preceding\nlearning algorithm for a DBN always improves a variational\nlower bound on the log-likelihood of the data when training\nmore layers [5].\nAfter training, the features learned from a DBN are ex-\ntracted using a feed-forward approximation for the proba-\nbilities of the hidden nodes at the deepest layer (i.e. cas-\ncades of sigmoids) given the visible nodes. These features\ncan be used for tasks such as classiï¬cation. In practice,\none often further reï¬nes the features learned by the DBN\nby treating the feature extraction process and classiï¬er as\na deep feed-forward neural network. The initialization of\nthe deep neural network using RBMs is often known as\nFigure 1 : Randomly selected feature bases learned from\nspectrograms of piano music. Most feature bases capture\nharmonic distributions which correspond to various pitches\nwhile a few contain non-harmonic patterns.\nunsupervised â€œpre-training,â€ while supervised training with\nbackpropagation is often known as supervised â€œï¬netuning.â€\nThe pre-training/ï¬netuning approach for learning deep net-\nworks has been shown to be essential for training deep net-\nworks. Speciï¬cally, training a deep network with only su-\npervised backpropagation from random initialization does\nnot work as well as pre-training.\n2.3 Application To Audio Spectrogram\nIn this paper, we apply DBNs to audio spectrograms. The\nDBNs are built in two stages. The ï¬rst stage performs un-\nsupervised learning with sparse RBMs up to two hidden\nlayers in order to ï¬nd sparse hidden units that represent\nspectrogram frames. The second (optional) stage uses back-\npropagation to ï¬netune the representation so that note clas-\nsiï¬ers have better discrimination power to correctly iden-\ntify note on and off events. Figure 1 displays features bases\n(column vectors of matrix W) learned from spectrograms of\nclassical piano music by a sparse RBM.\n3. CLASSIFICATION-BASED TRANSCRIPTION\nWe build our polyphonic piano-transcription model based\non Poliner and Ellisâ€™ frame-level note classiï¬cation system\n[12,13]. Furthermore, we extend their system by using DBN-\nbased feature representations and by jointly training classi-\nï¬ers for multiple notes.\n3.1 Single-note Training\nPoliner and Ellisâ€™ piano transcription system consists of 87\nindependent support vector machine (SVM) classiï¬ers, each\nof which predicts the presence of a corresponding piano\nnote when given an audio feature vector (a single column\n17612th International Society for Music Information Retrieval Conference (ISMIR 2011)\n......InputMultiple-NoteTrainingLinear SVM(Baseline)Linear SVM+ Hidden Layers...Single-NoteTrainingOutput..................OutputInputHiddenLayers...Figure 2 : Network conï¬gurations for single-note and\nmultiple-note training. Features are obtained from feed-\nforward transformation as indicated by the bottom-up ar-\nrows. They can be ï¬netuned by back-propagation as indi-\ncated by the top-down arrows.\nof a normalized spectrogram). Their transcription system\nrequires individual supervised training for each note. Thus,\nwe refer to this as single-note training.\nWe constrained the SVM in our experiments to a lin-\near kernel because Poliner and Ellis reported that high-order\nkernels (e.g. RBF kernel) provided only modest performance\ngains with signiï¬cantly more computation [13] and also a\nlinear SVM is more suitable to large-scale data. We formed\nthe training data by selecting spectrogram frames that in-\nclude the note (positive examples) and those that do not in-\nclude it (negative examples). Poliner and Ellis randomly\nsampled 50 positive (when available) and negative exam-\nples from each piano song per note. We used their sampling\nparadigm for single-note training.\nWhile their system used a normalized spectrogram, we\nreplaced it with DBN-based feature representations on spec-\ntrogram frames. As shown in the left column of Figure 2, the\nprevious approach directly feeds spectrogram frames into\nSVM, whereas our approach transforms the spectrogram fra-\nmes into mid-level features via one or two layers of learned\nnetworks and then feeds them into the classiï¬er. We also\nï¬netuned the networks with the error from the SVM.\n3.2 Multiple-note Training\nWhen we experimented with single-note training described\nabove, we observed that the classiï¬ers are somewhat â€œag-\ngressiveâ€, that is, they produced even more â€œfalse alarmâ€ er-\nrors (detect inactive notes as active ones) than â€œmissâ€ errors\n(fail to detect active notes). In particular, this signiï¬cantly\ndegraded onset accuracy. Also, it was substantially slow to\nï¬netune the DBN networks separately for each note. Thus,\nwe suggest a way of training multiple binary classiï¬ers atthe same time. We refer to this as multiple-note training.\nThe idea is to sum 88 SVM objectives and train them\nwith shared audio features and 88 binary labels (at a given\ntime, a single audio feature has 88 corresponding binary la-\nbels), as if we train a single classiï¬er.1This allows cross-\nvalidation to be jointly performed for 88 SVMs, thereby sav-\ning a signiï¬cant amount of training time. On the other hand,\nthis requires a different way of sampling examples. Since\nwe combined all 88 notes in our experiments, all spectro-\ngram frames except silent ones are a positive example to at\nleast one SVM. Thus we sampled training data by selecting\nspectrogram frames at every K frame time. K was set to\n16 as a trade-off between data reduction and performance.\nNote that this makes the ratio of positive and negative exam-\nples for each SVM determined by occurrences of the note\nin the whole training set, thereby having signiï¬cantly more\nnegative examples than positive ones for most SVMs. It\nturned out that this â€œunbalancedâ€ data ratio makes the clas-\nsiï¬ers â€œless aggressive,â€ as a result, increasing overall per-\nformance.\nWe illustrate multiple-note training in the right column\nof Figure 2. In fact, without ï¬netuning the DBNs, multiple-\nnote training is equivalent to single-note training with the\nunbalanced data ratio. The only difference is that the single-\nnote training does separate cross-validation for each SVM.\nWe compared multiple-note training to the single-note train-\ning with the unbalanced data ratio, but found no noticeable\ndifference in performance. On the other hand, when we\nï¬netune the DBNs, these two training approaches become\ncompletely different. While single-note training produces\nseparate DBN parameters for each note, multiple-note train-\ning allows the networks to shares the parameters among all\nnotes by updating them with the errors from the combined\nSVMs. For example, when the multiple-note training looks\nat the presence of a C3 note given input features, it simulta-\nneously checks out if other notes (e.g. C4 or C5) are played.\nThis can be seen as an example of multi-task learning.\n3.3 HMM Post-processing\nThe frame-level classiï¬cation described above treats train-\ning examples independently without considering dependency\nbetween frames. Poliner and Ellis used HMM-based post-\nprocessing to temporally smooth the SVM prediction. They\nmodeled each note independently with a two-state HMM.\nWe also adopted this approach. In our implementation, how-\never, we converted the SVM output (distance to the bound-\nary) to a posterior probability using\np(yi= 1|xi) =sigmoid (Î±(Î¸Txi)), (3)\n1The classiï¬er we used is a linear SVM with a L2-regularized L2-\nloss [2]. We implemented the SVM in MATLAB using minFunc, which\nis a Matlab library found in http://www.cs.ubc.ca/âˆ¼schmidtm/\nSoftware/minFunc.html . Thus, summing 88 SVM objectives was\ndone by simply treating 88 binary labels as a vector.\n177Poster Session 2\nInput (Spectrogram )Hidden layer activation  HMM outputSVM output\nTime (10ms)Frequency [kHz]10020030040000.511.5\nTime (10ms)Hidden unit index10020030040050100150200250\nTime (10ms)MIDI note number100200300400406080100\nTime (10ms)MIDI note number100200300400406080100Figure 3 : Signal transformation through the DBNs and classiï¬cation stages\nwhere xiis a SVM input vector, Î¸are SVM parameters, yi\nis a label and Î±is a scaling constant. Î±was chosen from a\npre-determined list of values as part of the cross-validation\nstage. The smoothing process was performed for each note\nclass by running a Viterbi search based on a 2x2 transition\nmatrix and a note on/off prior obtained from the training\ndata, and the posterior probability.\nFigure 3 shows signal transformation through the DBN\nnetworks along with HMM post-processing. The SVM out-\nput was computed as the distance to the decision boundary\nin a linear SVM. Note that the hidden layer activation is\nmore similar to the ï¬nal output than the spectrogram.\n4. EVALUATION\n4.1 Datasets\nWe used three datasets to evaluate our method.\nPoliner and Ellis set consists of 124 MIDI ï¬les of clas-\nsical piano music. They were rendered into 124 synthetic\npiano sound and 29 real piano recordings [12]. The ï¬rst\n60-second excerpt of each song was used.\nMAPS is a large piano dataset that includes various pat-\nterns of playing and pieces of music [1]. We used 9 sets of\npiano pieces, each with 30 songs. They were created by var-\nious high-quality software synthesizers (7 sets) and Yamaha\nDisklavier (2 sets). We used the ï¬rst 30-second excerpt of\neach song in the validation and test sets but the same length\nat a random position for the training set.\nMarolt set consists of 3 synthetic piano and 3 real piano\nrecordings [9]. This set was used only for test.\n4.2 Pre-processing\nWe ï¬rst computed spectrogram from the datasets with a 128-\nms window and 10ms overlaps. To remove note dynamics,\nwe normalized each column by dividing entries with their\nsum, and then compressed it using cube root, commonly\nused as an approximation to the loudness sensitivity of hu-\nman ears. Furthermore, we applied PCA whitening to the\nnormalized spectrogram, retaining 99% of the training data\nvariance and adding 0.01 to the variance before the whiten-\ning. This yielded roughly 50-60% dimensionality reduction\nand lowpass ï¬ltering in the PCA domain. The ground truth\nwas created from the MIDI ï¬les. We extended note offset\ntimes by 100ms in all training data to make up for room ef-\nfect in the piano recordings. The extended note length wasexperimentally determined.\n4.3 Unsupervised Feature Learning\nWe trained the ï¬rst and second-layer DBN representations\nusing the pre-processed spectrogram. The hidden layer size\nwas chosen to 256 and the expected activation of hidden\nunits(sparsity) was cross-validated over 0.05, 0.1, 0.2 and\n0.3, while other parameters were kept ï¬xed.\n4.4 Evaluation Metrics\nWe primarily used the following metric of accuracy:\nAccuracy =TP\nFP+FN+TP, (4)\nwhere TP (true positive) is the number of correctly predicted\nexamples, FP (false positives) is the number of note-off ex-\namples transcribed as note-on, FN (false negative) is the\nnumber of note-on examples transcribed as note-off. This\nmetric is used for both frame-level and onset accuracy. Frame-\nlevel accuracy is measured by counting the correctness of\nframes every 10ms, and onset accuracy is by searching a\nnote onset of the correct pitch within 100 ms of the ground-\ntruth onset. In addition, we used the F-measure for frame-\nlevel accuracy to compare our results to those published us-\ning the metric.\n4.5 Training Scenarios\nOur method is evaluated in two different scenarios. In the\nï¬rst scenario, we mainly used the Poliner and Ellis set, split-\nting it into training, validation and test data following [12].\nIn order to avoid overï¬tting to the speciï¬c piano set, we se-\nlected 26 songs from two synthesizer pianos sets in MAPS\nand used them as an additional validation set. For conve-\nnience, we refer to this subset as MAPS 2. In the second\nscenario, we used ï¬ve remaining synthesizer piano sets in\nMAPS for training to examine if our method generalizes\nwell when trained on diverse types of timbre and record-\ning conditions. For validation, we randomly took out 26\nsongs from the ï¬ve piano sets, calling them MAPS 5to dis-\ntinguish it from the actual training data. We additionally\nused MAPS 2for validation in the second scenario as well.2\n2The lists of MAPS songs for training, validation and test are speciï¬ed\ninhttp://ccrma.stanford.edu/âˆ¼juhan/ismir2011.html\n17812th International Society for Music Information Retrieval Conference (ISMIR 2011)\n405060708090Accuracy (%)\nSingleâˆ’note training Multipleâˆ’note trainingPoliner Ellis\nMAPS2Poliner Ellis\nMAPS2\n  \nBaseline\nL1\nL1âˆ’finetuned\nL2\nL2âˆ’finetuned(a) Scenario 1\n405060708090Accuracy (%)\nSingleâˆ’note training Multipleâˆ’note trainingMAPS5MAPS2MAPS5MAPS2 (b) Scenario 2\nFigure 4 : Frame-level accuracy on validation sets in two scenarios. The ï¬rst and second-layer DBN features are referred to as\nL1 and L2.\n102030405060Accuracy (%)\nSingleâˆ’note training Multipleâˆ’note trainingMAPS5MAPS2MAPS5MAPS2\nFigure 5 : Onset accuracy on validation sets (scenario 2)\n4.6 Validation Results\nWe compare the baseline feature (normalized spectrogram\nby cube root) to the ï¬rst- and second-layer DBN features\nand their ï¬netuned versions on validation sets in the two\nscenarios. The results are shown in Figure 4 and Figure 5.\nIn scenario 1, DBN features generally outperform the\nbaseline. In single-note training, ï¬netuned L1-features give\nthe highest accuracy on both validation sets. In multiple-\nnote training, unsupervised L1- or L2-features achieve slight-\nly better results. In comparison of the two training methods,\neither one appears to be not superior to the other, showing\nsubtle differences: Multiple-note training gives slightly bet-\nter results when the same piano set are used for validation\n(Poliner and Ellis), whereas single-note training does a little\nbetter job when different pianos set (MAPS 2) are used.\nIn scenario 2, the results show that DBN L1-features al-\nways achieve better results than the baseline but DBN L2-\nfeatures generally give worse accuracy. Finetuning always\nimproves results on both validation sets, although the incre-\nment is very limited on MAPS 2in multiple-note training.\nIn comparison of the two training methods, multiple-note\ntraining outperforms single-note training for both validation\nsets, particularly giving the best accuracy on MAPS 2. The\nsuperiority of multiple-note training is even more apparent\nin onset accuracy as shown in Figure 5.\nFigure 6 shows the inï¬‚uence of sparsity (hidden layer ac-\ntivation in RBMs) on frame-level accuracy. The accuracy\nis the average value on two validation sets (MAPS 5and\nMAPS 2) when L1 features are used in multiple-note train-\ning and scenario 2. The results indicate that relatively less\nsparse features perform better before ï¬netuning; however,\n00.050.10.150.20.250.362646668\nSparsityAccuracy (%)\n  \nL1\nL1âˆ’finetunedFigure 6 : Frame-level accuracy VS sparsity (hidden layer\nactivation in RBMs)\nwith ï¬netuning, sparse features achieve the highest accuracy\nas well as the best improvement.\n4.7 Test Results: Comparison With Other Methods\nThe validation results show that a single layer of DBN is the\nbest-performing feature representation and multiple-note tra-\nining is better than single-note training. Thus, we chose\nDBN L1-features and multiple-training to run our system\non test sets. Also, we evaluated both unsupervised and ï¬ne-\ntuned features.\nTable 1 shows results on the Poliner and Ellis test set, and\nMarolt set. We divided the table into two groups to make a\nfair comparison. The upper group uses the same dataset for\nboth training and testing (the Poliner and Ellis set) whereas\nthe lower group assumes that the piano tones in the test sets\nwere â€œunheardâ€ in training or uses different transcription al-\ngorithms. In the upper group, Poliner and Ellisâ€™ transcrip-\ntion system adopted a normalized spectrogram and a non-\nlinear SVM. Our method outperformed their approach for\nboth test sets. In the lower group, our method trained with\nMAPS (scenario 2) also produced better accuracy than the\ntwo published results on both sets. Note that, in both groups,\nunsupervised features give better results than ï¬netuned fea-\ntures when different piano sets are used for training and test-\ning. As for onset accurary, we achieved 62 %in training sce-\nnario 1 on the Poliner and Ellis test set, which is very close\nto the Poliner and Ellisâ€™ result (62.3 %).\nTable 2 compares our method with other algorithms eval-\nuated on the MAPS test set, composed of 50 songs selected\nfrom the two Disklavier piano sets by [15]. The ï¬netuned\nDBN-features in our method give the highest frame-level\naccuracy among compared methods.\n179Poster Session 2\nAlgorithms P. and E. Marolt\nPoliner and Ellis [12] â€  67.7% 44.6%\nProposed (S1-L1) 71.5% 47.2%\nProposed (S1-L1-ï¬netuned) 72.5% 46.45%\nMarolt [9] â€  39.6% 46.4%\nRyyananen and Klapuri [14] â€  46.3% 50.4%\nProposed (S2-L1) 63.8% 52.0%\nProposed (S2-L1-ï¬netuned) 62.5% 51.4%\nTable 1 : Frame-level accuracy on the Poliner and Ellis, and\nMarolt test set. The upper group was trained with the Po-\nliner and Ellis train set while the lower group was with other\npiano recordings or uses different methods. S1 and S2 refer\nto training scenarios. â€ These results are from Poliner [12].\nAlgorithms Precision Recall F-measure\nMarolt [9] â€  74.5% 57.6% 63.6%\nVincent et al. [15] â€  71.6% 65.5% 67.0%\nProposed (S2-L1) 80.6% 67.8% 73.6%\nProposed (S2-L1-ft.) 79.6% 69.9% 74.4%\nTable 2 : Frame-level accuracy on the MAPS test set in F-\nmeasure. â€œftâ€ stands for ï¬netuned. â€ These results are from\nVincent [15].\n5. DISCUSSION AND CONCLUSIONS\nWe have applied DBNs to classiï¬cation-based polyphonic\npiano transcription. The results show that a learned feature\nrepresentation by a DBN, particularly L1 features, provide\nbetter transcription performance than the baseline features\nand our classiï¬cation approach outperforms compared pi-\nano transcription methods.\nOur evaluation shows that ï¬netuning generally improves\naccuracy, particularly when sparse features are used. How-\never, unsupervised features often work better when the sys-\ntem is tested on different piano sets. This indicates that un-\nsupervised features are more robust to acoustic variations.\nWe also suggested multiple-note training. Compared to\nsingle-note training, this method improved not only tran-\nscription accuracy but also training speed. In our comput-\ning environment, multiple-note training was more than ï¬ve\ntimes faster than single-note training when the DBNs are\nï¬netuned.\nOur method is based on frame-level feature learning and\nbinary classiï¬cation under simple two-state note event mod-\neling. We think that more reï¬nements will be possible by\nmodeling richer states to represent dynamic properties of\nmusical notes.\n6. REFERENCES\n[1] V . Emiya, R. Badeau and B. David: â€œMultipitch esti-\nmation of piano sounds using a new probabilistic spec-\ntral smoothness principle,â€ IEEE Transaction on Au-\ndio, Speech and Language Processing , vol.18, no.6,\npp.1643â€“1654, 2010.[2] R. Fan, K. Chang, C. Hseigh, X. Wang and C. Lin: â€œLI-\nBLINEAR: a Library for Large Linear Classiï¬cation,â€\nJournal of Machine Learning Research , 9:1871â€“1974,\n2008.\n[3] M. Goto: â€œA predominant-f0 estimation method for\nCD recordings:MAP estimation using EM algorithm for\nadaptive tone models,â€ Preceedings of IEEE Interna-\ntional Conference on Acoustics, Speech and Signal Pro-\ncessing , 2001.\n[4] P. Hamel and D. Eck: â€œLearning Features from Mu-\nsic Audio With Deep Belief Networks,â€ Proceedings of\nthe 11th International Society for Music Information Re-\ntrieval Conference , 2010.\n[5] G. E. Hinton, S. Osindero, and Y . W. Teh: â€œA fast learn-\ning algorithm for deep belief nets,â€ Neural computation ,\n18(7):1527â€“1554, 2006.\n[6] A. Klapuri: â€œA perceptually motivated multiple-f0 esti-\nmation method,â€ Proceedings of IEEE Workshop on Ap-\nplications of Signal Processing to Audio and Acoustics ,\n2005.\n[7] H. Lee, C. Ekanadham, and A. Ng: â€œSparse deep be-\nlief net model for visual area V2,â€ Advances in Neural\nInformation Processing Systems , 2007.\n[8] H. Lee, Y . Largman, P. Pham, and A.Y . Ng: â€œUnsu-\npervised feature learning for audio classiï¬cation using\nconvolutional deep belief networks,â€ Advances in Neu-\nral Information Processing Systems(NIPS) , 22, 2009.\n[9] M. Marolt: â€œA connectionist approach to automatic tran-\nscription of polyphonic piano music,â€ IEEE Transac-\ntions on Multimedia , vol.6, no.3, pp.439â€“449, 2004.\n[10] J.A.Moorer: â€œOn the transcription of musical sound by\ncomputer,â€ Computer Music Journal , vol.1, no.4, pp.32â€“\n38, 1987.\n[11] P. Smolensky: â€œInformation processing in dynamical\nsystems:Foundation of harmony theory,â€ In D.E. Rumel-\nhart, J.L. McClelland (Eds.), Parallel Distributed Pro-\ncessing , vol.1, chapter.6, pp.194â€“281, Cambridge, MIT\nPress, 1986\n[12] G. Poliner and D. Ellis: â€œA discriminative model for\npolyphonic piano transcription,â€ EURASIP Journal on\nAdvances in Signal Processing , vol.2007, 2007.\n[13] G. Poliner and D. Ellis: â€œImproving generalization\nfor classiï¬cation-based polyphonic piano transcription,â€\nProceedings of IEEE Workshop on Applications of Sig-\nnal Processing to Audio and Acoustics , 2007.\n[14] M. Ryynanen and A. Klapuri: â€œPolyphonic music tran-\nscription using note event modeling,â€ Proceedings of\nIEEE Workshop on Applications of Signal Processing to\nAudio and Acoustics , 2005.\n[15] E. Vincent, N. Bertin, R.Badeau: â€œAdaptive Harmonic\nSpectral Decomposition for Multiple Pitch Estimation,â€\nIEEE Transaction on Audio, Speech and Language Pro-\ncessing , vol.18, no.3, pp.528â€“537, 2010.\n180"
    },
    {
        "title": "Music Information Robotics: Coping Strategies for Musically Challenged Robots.",
        "author": [
            "Steven R. Ness",
            "Shawn Trail",
            "Peter F. Driessen",
            "W. Andrew Schloss",
            "George Tzanetakis"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1415868",
        "url": "https://doi.org/10.5281/zenodo.1415868",
        "ee": "https://zenodo.org/records/1415868/files/NessTDST11.pdf",
        "abstract": "In the past few years there has been a growing interest in music robotics. Robotic instruments that generate sound acoustically using actuators have been increasingly developed and used in performances and compositions over the past 10 years. Although such devices can be very sophisticated mechanically, in most cases they are passive devices that directly respond to control messages from a computer. In the few cases where more sophisticated control and feedback is employed it is in the form of simple mappings with little musical understanding. Several techniques for extracting musical information have been proposed in the field of music information retrieval. In most cases the focus has been the batch processing of large audio collections rather than real time performance understanding. In this paper we describe how such techniques can be adapted to deal with some of the practical problems we have experienced in our own work with music robotics. Of particular importance is the idea of self-awareness or proprioception in which the robot(s) adapt their behavior based on understanding the connection between their actions and sound generation through listening. More specifically we describe techniques for solving the following problems: 1) controller mapping 2) velocity calibration, and 3) gesture recognition.",
        "zenodo_id": 1415868,
        "dblp_key": "conf/ismir/NessTDST11",
        "keywords": [
            "music robotics",
            "robotic instruments",
            "sound generation",
            "actuators",
            "performances",
            "compositions",
            "control messages",
            "musical understanding",
            "batch processing",
            "real time performance"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nMUSIC INFORMATION ROBOTICS: COPING STRATEGIES FOR\nMUSICALLY CHALLENGED ROBOTS\nSteven Ness, Shawn Trail\nUniversity of Victoria\nsness@sness.net\nshawntrail@gmail.comPeter Driessen\nUniversity of Victoria\npeter@ece.uvic.caAndrew Schloss, George Tzanetakis\nUniversity of Victoria\naschloss@uvic.ca\ngtzan@cs.uvic.ca\nABSTRACT\nIn the past few years there has been a growing interest in mu-\nsic robotics. Robotic instruments that generate sound acous-\ntically using actuators have been increasingly developed and\nused in performances and compositions over the past 10\nyears. Although such devices can be very sophisticated me-\nchanically, in most cases they are passive devices that di-\nrectly respond to control messages from a computer. In the\nfew cases where more sophisticated control and feedback is\nemployed it is in the form of simple mappings with little mu-\nsical understanding. Several techniques for extracting mu-\nsical information have been proposed in the ï¬eld of music\ninformation retrieval. In most cases the focus has been the\nbatch processing of large audio collections rather than real\ntime performance understanding. In this paper we describe\nhow such techniques can be adapted to deal with some of the\npractical problems we have experienced in our own work\nwith music robotics. Of particular importance is the idea\nof self-awareness or proprioception in which the robot(s)\nadapt their behavior based on understanding the connection\nbetween their actions and sound generation through listen-\ning. More speciï¬cally we describe techniques for solving\nthe following problems: 1) controller mapping 2) velocity\ncalibration, and 3) gesture recognition.\n1. INTRODUCTION\nThere is a long history of mechanical devices that generate\nacoustic sounds without direct human interaction starting\nfrom mechanical birds in antiquity to sophisticated player\npianos in the early 19th century that could perform arbi-\ntrary scores written in piano roll notation. Using computers\nto control such devices has opened up new possibilities in\nterms of ï¬‚exibility and control while retaining the richness\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proï¬t or commercial advantage and that copies\nbear this notice and the full citation on the ï¬rst page.\nc/circlecopyrt2011 International Society for Music Information Retrieval.of the acoustic sound associated with actual musical instru-\nments. The terms music robots or music robotic instruments\nhave been used to describe such devices [6].\nWe believe these new robotic instruments have a legiti-\nmate place with potential to become part of an embedded\nconventional musical practice, not just a research curios-\nity. While musical-robotics might seem niche and esoteric\nat this point [2], historic innovations such as monophonic\nto polyphonic music, electrical ampliï¬cation of the guitar,\nor computers in the recording studio all brought skepticism,\nbut eventually became mainstay practices.\nAlthough such music robots have been used in perfor-\nmance of both composed and improvised music as well as\nwith or without human performers sharing the stage, they\nare essentially passive output devices that receive control\nmessages and in response actuate sound producing mecha-\nnisms. Their control is typically handled by software written\nspeciï¬cally for each piece by the composer/performer.\nMusicians through training acquire a body of musical\nconcepts commonly known as musicianship. Machine mu-\nsicianship [9] refers to the technology of implementing mu-\nsical process such as segmentation, pattern processing and\ninteractive improvisation in computer programs. The ma-\njority of existing work in this area has focused on sym-\nbolic digital representations of music, typically MIDI. The\ngrowing research body of music information retrieval, espe-\ncially audio-based, can provide the necessary audio signal\nprocessing and machine learning techniques to develop ma-\nchine musicianship involving audio signals.\nThe typical architecture of interactive music robots is that\nthe control software receives symbolic messages based on\nwhat the other performers (robotic or human) are playing\nas well as messages from some kind of score for the piece.\nIt then sends control messages to the robot in order to trig-\nger the actuators generating the acoustic sound. In some\ncases the audio output of the other performers is automat-\nically analyzed to generate control messages. For example\naudio beat tracking can be used to adapt to the tempo played.\nSelf listening is a critical part of musicianship as any-\none who has struggled to play music on a stage without a\n567Poster Session 4\nFigure 1 . The experimental setup for our robotic based\nframe drum experiments. In the foreground, three frame\ndrums are shown with solenoids placed to ensure optimal\nstriking of the drum surface. In the background of the pic-\nture, the control system is shown.\nproper monitor setup has experienced. However this ability\nis conspicuously absent in existing music robots. One could\nremove the acoustic drum actuated by a solenoid so that no\nsound would be produced and the robotic percussionist will\ncontinue â€œblissfullyâ€ playing along.\nThis work has been motivated by practical problems ex-\nperienced in a variety of performances involving percussive\nrobotic instruments. Figure 1 shows our experimental setup\nin which solenoid actuators supplied by Karmetik LLC.1\nare used to excite different types of frame drums.\nWe show how the ability of a robot to â€œlistenâ€ especially\nto its own acoustic audio output is critical in addressing\nthese problems and describe how we have adapted relevant\nmusic information retrieval techniques for this purpose. More\nspeciï¬cally, we describe how self-listening can be used to\nautomatically map controls to actuators as well as how it can\nbe used to provide self-adapting velocity response curves.\nFinally, we show how pitch extraction and dynamic time\nwarping can be used for high-level gesture analysis in both\nsensor and acoustic domains.\n2. RELATED WORK\nAn early example of an automated, programmable musi-\ncal instrument ensemble was described by al-Jazari (1136-\n1206) a Kurdish scholar, inventor, artist, mathematician that\nlived during the Islamic Golden Age (the Middle Ages in the\nwest). Best known for writing the Book of Knowledge of\nIngenious Mechanical Devices in 1206, his automata were\ndescribed as fountains on a boat featuring four automatic\n1http://karmetik.commusicians that ï¬‚oated on a lake to entertain guests at royal\ndrinking parties. It had a programmable drum machine with\npegs (cams) that bumped into little levers that operated the\npercussion. The drummer could be made to play different\nrhythms and different drum patterns if the pegs were moved\naround, performing more than ï¬fty facial and body actions\nduring each musical selection. This was achieved through\nthe innovative use of hydraulic switching. A modern exam-\nple of a robotic musical ensemble is guitarist Pat Methenyâ€™s\nOrchestrion which was speciï¬cally inï¬‚uenced by the Player\nPiano2. Metheny cites his grandfatherâ€™s player piano as\nbeing the catalyst to his interest in Orchestrions, which is a\nmachine that plays music and is designed to sound like an\norchestra or band.\nA seminal book in this ï¬eld is â€œMachine Musicianshipâ€\n[9], in which one of the sections describes a comprehensive\nsystem for the composition, creation and performance be-\ntween humans and robots. Rowe describes improvisational\nand composition systems that combine features of music\nfeature extraction, musical analysis and interactivity to gen-\nerate engaging experiences for the audience. In our work,\nthe integration of machine musicianship and music robotics\nhas been used to develop a robotic percussionist that can\nimprovise with a human performer playing a sitar enhanced\nwith digital sensors [7].\nAnother work closely related to ours is the Shimon human-\nrobot based Jazz improvisation system [3] that uses a ges-\nture based framework that recognizes that musicianship in-\nvolves not just the production of notes, but also of the in-\ntentional and consequential communication between musi-\ncians [4].\nOur system also uses these same basic building blocks,\nbut adds the power of machine learning and â€œpropriocep-\ntionâ€ to the process, enabling the robot itself to perform\nmany of the time consuming mapping and calibration pro-\ncesses that are often performed by hand in performance situ-\nations. In this context, a mapping refers to the process of de-\ntermining which controller output activates which solenoid.\nIn the next section we describe how some practical recurring\nproblems we have experienced with robots in music perfor-\nmance robots have led to the development of signal process-\ning and machine learning techniques informed by music in-\nformation retrieval ideas.\n3. MOTIV ATION\nOur team has extensive experience designing music robotic\ninstruments, implementing control and mapping strategies,\nand using them in live and interactive performances with\nhuman musicians, frequently in an improvisatory context.\nIn addition two of the co-authors are professional musicians\nwho have regularly performed with robotic instruments. One\n2http://www.patmetheny.com/orchestrioninfo/\n56812th International Society for Music Information Retrieval Conference (ISMIR 2011)\nof the most important precursors to any musical performance\nis the sound check/rehearsal that takes place before a con-\ncert in a particular venue. During this time the musicians\nsetup their instruments, adjust the sound levels of each in-\nstrument and negotiate information speciï¬c to the perfor-\nmance such as positioning, sequencing and cues. A sim-\nilar activity takes place in performance involving robotic\nacoustic instruments in which the robots are set up, their\nacoustic output is calibrated and adjusted to the particular\nvenue and mappings between controls and gestures are es-\ntablished. This process is frequently tedious and typically\nrequires extensive manual intervention. To some extent this\npaper can be viewed as an attempt to utilize techniques and\nideas from MIR to simplify and automate this process. This\nis in contrast to previous work in robotic musicianship that\nmostly deals with the actual performance. More speciï¬cally\nwe deal with three problems: automatic mapping, velocity\ncalibration, and melodic and kinetic gesture recognition.\nThe experimental setup that we have used consists of a\nmodular robotic design in which multiple solenoid-based\nactuators can be attached to a variety of different drums.\nWe use audio signal processing and machine learning tech-\nniques to have robotic musical instruments that â€listenâ€ to\nthemselves using a single centrally located microphone.\nIt is a time consuming and challenging process to setup\nrobotic instruments in different venues. One issue is that\nof mapping, that is, which signal sent from the computer\nmaps to which robotic instrument. As the number of drums\ngrows, it becomes more challenging to manage the cables\nand connections between the controlling computer and the\nrobotic instruments. The system we propose performs tim-\nbre classiï¬cation of the incoming audio, automatically map-\nping solenoids correctly in real-time to the note messages\nsent to the musically desired drum. For example rather than\nsending an arbitrary control message to actuator 40 the con-\ntrol message is addressed to the bass drum and will be routed\nto the correct actuator by simply â€œlisteningâ€ to what each\nactuator is playing in a sound-check stage. That way actua-\ntors can be moved or replaced easily even during the perfor-\nmance without changes in the control software. The same\napproach is also used to detect broken or malfunctioning\nactuators that do not produce sound.\nWhen working with mechanical instruments, there is a\ngreat deal of non-linearity and physical complexity that makes\nthe situation fundamentally different from working with elec-\ntronic sound, which is entirely â€œvirtualâ€ (or at least not phys-\nical) until it comes out of the speakers. The moving parts of\nthe actuators have momentum, and changes of direction are\nnot instantaneous. Gravity may also play a part, and there\nis friction to be overcome. Frequently actuators are on sep-\narate power supplies which can result in inconsistencies in\nthe voltage. The compositional process, rehearsal and per-\nformance of â€œThe Space Between Usâ€ by by David A. Jaffe,in which Andrew Schloss was soloist on robotic percussion,\ninvolved hand-calibrating every note of the robotic chimes,\nxylophone and glockenspiel. This required 18+23+35 sep-\narate hand calibrations and took valuable rehearsal time. In\nthis paper we describe a method for velocity calibration, that\nis, what voltage should be sent to a solenoid to generate\na desired volume and timbre from an instrument. Due to\nthe mechanical properties of solenoids and drums, a small\nmovement in the relative position of these two can lead to a\nlarge change in sound output. The most dramatic of these is\nwhen during performance a drum moves out of place enough\nthat a voltage that at the start of the performance allowed the\ndrum to be hit now fails to make the drum sound. Depend-\ning on the musical context, this can be disastrous in a per-\nformance context. Good velocity scaling is essential for a\npercussion instrument to give a natural graduated response\nto subtle changes in gesture, e.g. a slight increase in the\nstrength (velocity) of a stroke should not result in a sudden\nincrease in the loudness of sound.\nIssues like velocity calibration or control mapping seem\nquite pedestrian, or even trivial until one has grappled with\nthis problem with real instruments. We believe that the abil-\nity of a robotic instrument to perceive at some level its own\nfunctioning is important in making robust, adaptive systems\nthat do not require regular human intervention to function\nproperly. We refer to this ability as â€œproprioceptionâ€ which\nin its original deï¬nition refers to the ability of an organism\nto perceive its own status.\nFinally we also describe some experiments recognizing\nmelodic and kinetic gestures at different tempi and with vari-\nations in how they are performed. This can be viewed as\nan exchange of cues established before the performance es-\npecially in an improvisatory context. This allows higher-\nlevel gestures to be used as cues without requiring exact re-\nproduction from the human performer interacting with the\nrobotic instrument and enables a more ï¬‚uid and ï¬‚exible struc-\nturing of performances.\n4. EXPERIMENTS\n4.1 Drum Classiï¬cation for Automatic Mapping\nWe performed an experiment to investigate the performance\nof a audio feature extraction and machine learning system\nto classify drum sounds to perform automatic mapping. The\naudio features used were the well known Mel-Frequency\nCepstral Coefï¬cients (MFCC) calculated with a window size\nof 22.3ms. These were then used as input to a Support Vec-\ntor Machine (SVM) machine learning system. We collected\na dataset of audio with 4 different frame drums being struck\nby the robot with a time of 128ms between strikes, then cal-\nculated all the MFCC of this audio, and then found the 8\nhighest MFCC0 (roughly corresponding to perceptual loud-\nness) and marked these as onsets in the audio. The MFCC\n569Poster Session 4\nPeak Percent Peak Percent\noffset correct offset correct\n0 66.38 4 90.52\n1 91.95 5 86.49\n2 91.67 6 86.49\n3 91.95 7 77.59\nTable 1 . Classiï¬cation accuracy of an SVM classiï¬er The\nPeak offset is the offset from the time the drum is hit.\nfeature vectors corresponding to these onsets were used to\ntrain the classiï¬er. A separate test data set was also col-\nlected. Percussive sounds can be challenging to classify as\nthere is not a lot of steady state spectral information. The\nresults of this experiment gave a classiï¬cation accuracy of\n66.38%, as shown in the ï¬rst line (Peak offset 0) in Table 1.\nWe then performed the same experiment but using instead\ndifferent offsets from the highest peak in window sizes of\n22.3ms. When we classiï¬ed all frames with the frame im-\nmediately after the highest peak, we obtained a classiï¬ca-\ntion accuracy of 91.95%. We interpret this result to mean\nthat the resonance after the transient is clearly distinguish-\nable for different drums, whereas the transient at the onset is\nfairly similar for different drums. This performance quickly\ndegrades as we move away from the onset.\nThis performance quickly degrades as we move away\nfrom the onset. These results are for individual 22.3ms frames\nso it is easy to get 100% correct identiï¬cation by voting\nacross the entire recording which can then be used for the\nautomatic mapping. When we setup the robotic instrument\nwe actuate each solenoid in turn, classify the audio and then\nset the appropriate mappings so that the control software can\naddress the actual frame drums rather than the actuators.\n0 20 40 60 80 100 120 140\nCalibrated Input Velocity020406080100120140160Output Driving VelocityCalibration mapping\n1cm\n2cm\n3cm\nFigure 2 . Mapping from calibrated input velocities to out-\nput driving velocities for different distances4.2 Timbre-Adaptive Velocity Calibration\nThe acoustic response of a drum both in terms of perceived\nloudness and timbral quality is non-linear with respect to\nlinear increases in voltage as well as to the distance of the\nsolenoid to the vibrating surface. In the past calibration was\nperformed manually by listening to the output and adjust-\ning the mapping of input velocities to voltage until smooth\nchanges in loudness and timbre where heard. In this section\nwe describe how to derive an automatic data-driven map-\nping that is speciï¬c to the particular drum.\nOur ï¬rst objective is to achieve a linear increase in loud-\nness with increasing MIDI velocity for a given ï¬xed dis-\ntance between beater and drumhead. However, in practice,\nthe beater may be mounted on a stand and placed next to the\ndrumhead mounted on a different stand. Thus the distance\nbetween beater and drumhead will vary depending on setup,\nand may even change during a performance. Thus a sec-\nond objective is to achieve a similar loudness versus MIDI\nvelocity (corresponding to voltage) curve over a range of\ndistances between beater and drumhead.\nTo achieve these objectives we collected audio for all\nvelocity values and three distance conï¬guration (near 1cm,\nmedium 2cm, far 3cm). The loudness and timbre variation\npossible is captured by computing MFCC for each strike.\nMore speciï¬cally for each velocity value and a particular\ndistance we obtain a vector of MFCC values. The frequency\nof beating was kept constant at 8 strikes per second for these\nmeasurements. The ï¬rst MFCC coefï¬cient (MFCC0) at the\ntime of onset is used to approximate loudness. Plots of\nMFCC0 for the distance conï¬gurations are shown in 3(a).\nIn order to capture some of the timbral variation in addi-\ntion to the loudness variation we project our MFCC vectors\nto a single dimension (the ï¬rst principal component) using\nPrincipal Component Analysis (PCA) [5]. As can be seen\nin 3(c) the PCA0 values follow closely the loudness curve.\nThis is expected as loudness is the primary characteristic\nthat changes with increasing velocity. However, there is also\nsome information about timbre as can be seen by the â€œnearâ€\nplot that has higher variance in PCA0 than in MFCC0.\nOur goal is to obtain a mapping (from user input cali-\nbrated velocity to output driving velocity) such that linear\nchanges in input (MIDI velocity) will yield approximately\nlinear changes in the perceived loudness and timbre as ex-\nperessed in PCA0. We utilize data from all the three dis-\ntance conï¬gurations for the PCA computation so that the\ntimbrespace is shared. That way even though we get sep-\narate calibration mappings for each distance conï¬guration\nthey have the property that the same calibrated input value\nwill generate the same output in terms of loudness and tim-\nbre independently of distance.\nIn order to obtain this mapping we quantize the PCA0\nvalues for each distance conï¬guration into 128 bins that cor-\nrespond to the calibrated input velocities. The generated\n57012th International Society for Music Information Retrieval Conference (ISMIR 2011)\nmapping is the wrong way i.e from output driving velocities\nto calibrated input velocities and is not an injection (one-to-\none function) so it can not be directly inverted. To invert\nthe mapping for each calibrated input velocity (or equiva-\nlently quantized PCA bin) we take the average of all the\noutput driving velocities that map to it as the output driving\nvalue. This calibration mapping is shown in Figure 2. Fig-\nures 3(b) and 3(d) show how changing the calibrated input\nvelocity linearly results in a linearized progression through\nthe timbrespace (PCA0) and loudness (MFCC0). In these\ngraphs we show directly the results of this calibration but\nit is also possible to ï¬t lines to them. In either case (direct\ncalculated mapping or line ï¬t) the calibrated output changes\nsound more smooth than the original output.\n4.3 Gesture recognition using Dynamic Time Warping\nCollaborating musicians frequently utilize high-level cues to\ncommunicate with each other especially in improvisations.\nFor example a jazz ensemble might agree to switch to a dif-\nferent section/rhythm when the saxophone player plays a\nparticular melodic pattern during soloing. This type com-\nmunication through high level cues is difï¬cult to achieve\nwhen performing with robotic music instruments. In our\nperformances we have utilized a variety of less ï¬‚exible com-\nmunication strategies including pre-programmed output (the\nsimplest), direct mapping of sensors on a performer to robotic\nactions, and indirect mapping through automatic beat track-\ning. The ï¬nal experiments described in this paper show how\nhigh-level gesture recognition that is robust to changes in\ntempo and pitch contour can be correctly identiï¬ed and used\nas a cue. Our system is ï¬‚exible and can accept input from\na wide variety of input systems. We show experimental re-\nsults with the radiodrum as well as melodic patterns played\non a vibraphone. There has been considerable work done in\nthe area of using Dynamic Time Warping for gesture recog-\nnition, including work done by Akl and Valaee [1] and Liu\net al. [8].\nFor the ï¬rst experiment, we used the most recent itera-\ntion of the radiodrum system, a new instrument designed by\nBob Boie that dramatically outperforms the original radio-\ndrum in terms of both data rate and accuracy. We instructed\na professional musician to generate 8 different instances of\n5 types of gestures, which were an open stroke roll, a sweep\nof the stick through the air, a pinching gesture similar to\nthe pinch to zoom metaphor on touchscreens, a circle in the\nair and a buzz roll. We collected (X,Y,Z )triplets of data\nfrom the sensor at a sample rate of 44100Hz and then down-\nsampled this data to 120Hz to allow us to compare gestures\nthat were on average 1-2 seconds in length while remain-\ning within the memory limits of our computer system. We\nempirically determined that this rate captured most of the\ninformation relevant to gesture recognition.\nFrom this data, the similarity matrix of each gesture toradiodrum Vibraphone\nGestures AP P@1 Gesture AP P@1\nroll 0.866 1.0 pattern1 0.914 1.0\nsweep 0.980 1.0 pattern2 0.812 0.9\npinch 0.837 1.0 pattern3 0.771 0.9\ncircle 1.000 1.0 pattern4 0.882 1.0\nbuzz 0.978 1.0 pattern5 0.616 0.9\nMAP 0.931 1.0 MAP 0.799 0.94\nTable 2 . Average precision for different gestures on the\nradiodrum and vibraphone. The Mean Average Precisions\n(MAP) are 0.931 and 0.799.\neach other gesture is computed. Dynamic Time Warping\n[10] is used to compute an alignment score for each pair of\ngestures that correspond to how similar they are. For each\nquery gesture we return a ranked list based on the alignment\nscore and calculate the average precision for each gesture.\nAs can be seen from Table 2 gesture identiï¬cation is quite\nreliable in both cases.\n5. CONCLUSIONS AND FUTURE WORK\nWe have shown how techniques from MIR can be adapted\nand used to solve practical problems in music robotics. More\nspeciï¬cally we show how audio classiï¬cation can be used\nfor automatic mapping, principal component analysis can\nbe used for velocity/timbre calibration and dynamic time\nwarping for gesture recognition. This system has not yet\nbeen tried in performance, and we are currently working\nwith musicians to deploy this system in a live setting. In\nthe future we plan to extend this work utilizing more sen-\nsors including multiple microphones on both the robot and\nthe performers. To obtain the maximum possible dynamic\nrange we plan to have multiple actuators placed at different\ndistances on the same drum so that the ones that are far are\nused for loud sounds and the ones that are near are used for\nsoft sounds. The proposed calibration method will be used\nto drive seamlessly both actuators. We would also like to\ninvestigate how MIR techniques can be used to â€œteachâ€ the\nrobot to play and recognize rhythmic and melodic patterns.\n6. ACKNOWLEDGMENTS\nWe would like to thank Gabrielle Odowichuk and Anthony\nTheocharis for help in collecting data. We thank the Na-\ntional Sciences and Engineering Research Council (NSERC)\nand Social Sciences and Humanities Research Council (SSHRC)\nof Canada for their ï¬nancial support.\n571Poster Session 4\n0 20 40 60 80 100 120 140\nVelocity65\n60\n55\n50\n45\n40\nMFCC 0 valueOriginal MFCC 0 Values\n1cm\n2cm\n3cm(a) MFCC-values\n0 20 40 60 80 100 120 140\nCalibrated Input Velocity (from performer)65\n60\n55\n50\n45\n40\nMFCC ValueCalibrated MFCC 0 value corresponding to input velocity\n1cm\n2cm\n3cm (b) MFCC-inverse-mapping\n0 20 40 60 80 100 120 140\nVelocity0.00.20.40.60.81.01.2PCA valueOriginal PCA Values\n1cm\n2cm\n3cm\n(c) PCA-values\n0 20 40 60 80 100 120 140\nCalibrated Input Velocity (from performer)0.00.20.40.60.81.01.2Calibrated PCA ValueCalibrated PCA value corresponding to input velocity\n1cm\n2cm\n3cm (d) Calibrated PCA\nFigure 3 . Velocity Calibration based on loudness and timbre\n7. REFERENCES\n[1] A. Akl and S. Valaee. Accelerometer-based gesture\nrecognition via dynamic-time warping, afï¬nity propaga-\ntion, and compressive sensing. In ICASSP , pages 2270\nâ€“2273, 2010.\n[2] M. Burtner. A theory of modulated objects for new\nshamanic controller design. In Proc. Int. Conference on\nNew Interfaces for Musical Expression (NIME) , 2004.\n[3] G. Hoffman and G. Weinberg. Gesture-based human-\nrobot jazz improvisation. In Int. Conf. on Robotics and\nAutomation (ICRA) , pages 582â€“587, 2010.\n[4] G. Hoffman and G. Weinberg. Shimon: an interactive\nimprovisational robotic marimba player. In CHI Ex-\ntended Abstracts , pages 3097â€“3102, 2010.\n[5] I.T. Jolliffe. Principal Component Analysis . Springer,\n2002.\n[6] A. Kapur. A history of robotic musical instruments. In\nProc. of the Int. Computer Music Conf. (ICMC) , 2005.\n[7] A. Kapur, E. Singer, M. Benning, G. Tzanetakis, and\nTrimpin. Integrating hyperinstruments, musical robotsand machine musicianship for north indian classical mu-\nsic. In Proc. Int. Conf. on New Interfaces for Musical\nExpression (NIME) , 2005.\n[8] J. Liu, Z. Wang, L. Zhong, J. Wickramasuriya, and\nV . Vasudevan. uwave: Accelerometer-based personal-\nized gesture recognition and its applications. IEEE Int.\nConf. on Pervasive Computing and Communications ,\npages 1â€“9, 2009.\n[9] R. Rowe. Machine Musicianship . MIT Press, 2001.\n[10] H. Sakoe and S. Chiba. Dynamic programming algo-\nrithm optimization for spoken word recognition. IEEE\nTransactions on Acoustics, Speech and Signal Process-\ning, 26(1):43â€“49, 1978.\n572"
    },
    {
        "title": "Associations between Musicology and Music Information Retrieval.",
        "author": [
            "Kerstin Neubarth",
            "Mathieu Bergeron",
            "Darrell Conklin"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1416564",
        "url": "https://doi.org/10.5281/zenodo.1416564",
        "ee": "https://zenodo.org/records/1416564/files/NeubarthBC11.pdf",
        "abstract": "A higher level of interdisciplinary collaboration between music information retrieval (MIR) and musicology has been proposed both in terms of MIR tools for musicology, and musicological motivation and interpretation of MIR research. Applying association mining and content citation analysis",
        "zenodo_id": 1416564,
        "dblp_key": "conf/ismir/NeubarthBC11",
        "keywords": [
            "Music Information Retrieval",
            "Musicology",
            "Interdisciplinary Collaboration",
            "Association Mining",
            "Content Citation Analysis",
            "MIR Tools for Musicology",
            "Musicological Motivation",
            "Data Mining",
            "Citation Contexts",
            "MIR Trends"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nASSOCIATIONS BETWEEN MUSICOLOGY AND MUSIC INFORMATION\nRETRIEV AL\nKerstin Neubarth\nCanterbury Christ Church University\nCanterbury, UK\nkerstin.neubarth@canterbury.ac.ukMathieu Bergeron\nCIRMMT\nMcGill University\nMontreal, CanadaDarrell Conklin\nUniversidad del Pa Â´Ä±s Vasco\nSan Sebasti Â´an, Spain\nand IKERBASQUE,\nBasque Foundation for Science\nABSTRACT\nA higher level of interdisciplinary collaboration between\nmusic information retrieval (MIR) and musicology has been\nproposed both in terms of MIR tools for musicology, and\nmusicological motivation and interpretation of MIR research.\nApplying association mining and content citation analysis\nmethods to musicology references in ISMIR papers, this pa-\nper explores which musicological subject areas are of inter-\nest to MIR, whether references to speciï¬c musicology areas\nare signiï¬cantly over-represented in speciï¬c MIR areas, and\nprecisely why musicology is cited in MIR.\n1. INTRODUCTION\nAt the tenth anniversary ISMIR 2009 several contributions\ndiscussed challenges in the further development of music\ninformation retrieval (MIR) as a discipline, including re-\nquests for deeper musical motivation and interpretation of\nMIR questions and results, and envisaging closer interaction\nwith source disciplines such as computer science, cognitive\nscience and musicology [4, 9, 20].\nSuggestions for interdisciplinary collaboration have of-\nten considered musicology as a target discipline, emphasis-\ning the usefulness of MIR tools to musicology (e.g. [18]).\nOccasionally mutual beneï¬ts have been explored (e.g. [15]).\nThe current study addresses associations between MIR and\nmusicology as a source discipline. It presents a systematic\nanalysis of how MIR, as represented at ISMIR, has drawn\non musicology so far, by applying data mining and content\ncitation analysis to musicology references in ISMIR publi-\ncations.\nRelated quantitative ISMIR surveys mainly analyse top-\nics and trends [3, 7, 10]. The study by Lee et al. [10] per-\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proï¬t or commercial advantage and that copies\nbear this notice and the full citation on the ï¬rst page.\nc/circlecopyrt2011 International Society for Music Information Retrieval.formed a citation analysis of papers within ISMIR, counting\nreferences to individual authors and papers. It brieï¬‚y ad-\ndressed citer motivations such as identiï¬cation of data and\nmethods or paying homage, and concluded that: â€œWithout a\nmore in-depth analysis of the individual contexts surround-\ning each citation, it is difï¬cult to tease out the precise mo-\ntivations for all the referencesâ€ (p. 61). Functions of refer-\nences to one particular study were discussed in the editorial\nto the JNMR Special Issue on MIR in 2008 [1].\nThis study extends previous work in several ways: It\nanalyses inter-disciplinary references (musicology cited in\nMIR) rather than intra-MIR references; also, the references\nare analysed at the level of MIR and musicology subject cat-\negories instead of individual papers. The quantitative analy-\nsis goes beyond citation counts; association mining is used\nhere to yield interdisciplinary associations. In addition, ci-\ntation contexts are analysed in depth to reveal functions of\nmusicology citations in ISMIR papers, taking into account\nboth MIR-speciï¬c functions and more general referencing\npurposes to allow comparison with existing studies.\n2. DATA SELECTION AND ANALYSIS\nThis section presents the corpus development and the as-\nsociation mining and citation analysis methods used for ex-\ntracting and analysing associations between musicology and\nmusic information retrieval.\n2.1 Sampling\nFrom the cumulative ISMIR proceedings (www.ismir.net)\nas a sampling frame, ï¬rst all available full papers from 2000\nuntil 2007, and all oral/plenary session papers for 2008 to\n2010, were selected. This resulted in 416 papers. Then the\nreference lists of those papers were screened and a purposive\nsample was taken of all papers which contain references to\nmusicology as a source discipline, excluding self-citations\nand references to other ISMIR papers. The ï¬nal analysis\ncorpus consisted of 184 papers. These papers are identiï¬ed\nby their IDs within the cumulative proceedings (e.g. ID135).\n429Poster Session 3\n2.2 Encoding\nThe 184 papers of the analysis corpus were labelled accord-\ning to their MIR research topic and the musicology areas\nthat they cite, using the following categorisations.\nMIR Categories . In a ï¬rst step of encoding, the 184\npapers were classiï¬ed into MIR research areas (Table 1).\nAs no single standardised and comprehensive taxonomy of\nMIR topics exists [3,6], an organisation of topics was devel-\noped based on ISMIR calls and programs, harmonising cate-\ngories across conferences. Each ISMIR paper is assigned to\nexactly one MIR category. Numbers in brackets in Table 1\nindicate the number of papers in each category.\nResearch paradigms (6)\nEpistemology, interdisciplinarity\nRepresentation & metrics (24)\nRepresentation, metrics, similarity\nData & metadata (11)\nDatabases, data collection & organisation, metadata, anno-\ntation\nTranscription (42)\nSegmentation, voice & source separation, alignment, beat\ntracking & tempo estimation, key estimation, pitch tracking\n& spelling\nOMR (5)\nOptical music recognition, optical lyrics extraction\nComputational music analysis (21)\nPattern discovery & extraction, summarisation, chord la-\nbelling, musical analysis (melody & bassline, harmonic,\nrhythm and form analysis)\nRetrieval (19)\nQuery-by-example\nClassiï¬cation (32)\nGenre classiï¬cation, geographical classiï¬cation, artist clas-\nsiï¬cation & performer identiï¬cation, instrument-voice clas-\nsiï¬cation (instrument recognition, instrument vs voice dis-\ntinction, classiï¬cation of vocal textures), mood & emotion\nclassiï¬cation\nRecommendation (5)\nRecommendation methods & systems, playlist generation,\nrecommendation contexts\nMusic generation (4)\nMusic prediction, improvisation, interactive instruments\nSoftware systems (10)\nPrototypes & toolboxes, user interfaces & usability, visuali-\nsation\nUser studies (5)\nUser behaviour (music discovery, collection organisation)\nTable 1 . Thematic categories and examples of topics of\nMIR research.History, criticism & philosophy\nHistory of music (8)\nPhilosophy of music & music semiotics (3)\nTextual criticism, archival research & bibliography (22)\nElectronic & computer music (7)\nPopular & jazz music studies (5)\nFilm music studies (1)\nTheory & analysis\nMusic theory & analysis (36)\nPerformance studies (6)\nEthnomusicology\nEthnomusicology (non-Western) (5)\nEthnomusicology (folk music) (9)\nEthnomusicology (other) (1)\nSystematic Musicology\nAcoustics (11)\nPsychology of music (perception & cognition) (93)\nPsychology of music (emotion & affect) (8)\nPsychology of music (other) (1)\nSociology & sociopsychology (18)\nTable 2 . Thematic categories of musicology.\nMusicology Categories . In a second step, the musicol-\nogy references in the 184 papers were assigned to musicol-\nogy areas (Table 2). As the interest of this study is in mu-\nsicology as a source discipline, the labels used are based\non traditional subject organisations (e.g. [11, 14, 16]) rather\nthan more recent developments such as empirical or com-\nputational musicology which potentially overlap with music\ninformation retrieval (e.g. [18]). Category counts in Table 2\nrefer to the number of ISMIR papers citing this musicology\narea one or more times. A paper may reference more than\none musicology category.\n2.3 Association Mining\nData mining of the analysis corpus is used to reveal asso-\nciations between papers in speciï¬c MIR categories and pa-\npers that have citations to speciï¬c musicology categories.\nFor every musicology category Aand MIR category B, the\nsupport (number of papers) s(A)ands(B)were computed.\nAlso the support s(A,B )of an association/angbracketleftA,B/angbracketright(number\nof papers containing references to musicology category A\nwhich are also in MIR category B) and the statistical signif-\nicance of the association were computed.\nThe null hypothesis is that for an association /angbracketleftA,B/angbracketrightthe\ntwo categories are statistically independent, i.e. that the pro-\nportion of papers citing musicology category Athat are in\nMIR category Bdoes not differ signiï¬cantly from the rel-\native frequency of MIR category Bin the general popula-\ntion. Given the small corpus and low counts for many cat-\n43012th International Society for Music Information Retrieval Conference (ISMIR 2011)\negories, the appropriate test for statistical independence is\nFisherâ€™s one-tailed exact test on a 2x2 contingency table [5].\nFor an association /angbracketleftA,B/angbracketrightwith support s(A,B ), this gives\nthe probability (p-value) of ï¬nding s(A,B )or more papers\nof category Bins(A)samples (without replacement) in\nn= 184 total papers. If the computed p-value is less than\nthe signiï¬cance level Î±= 0.05, then we reject the null hy-\npothesis that the categories are independent.\nPrior to computing the p-values, the counts of all MIR\ncategoriesB(and hence the p-values of associations) are\nslightly adjusted upwards to account for the fact that only\npapers citing musicology were included in the sample of\nn= 184 papers from the larger corpus of 416 papers. Under\nthe null hypothesis of independence, it is assumed that the\nlarger set of papers has the same distribution of MIR cat-\negories as the smaller corpus. The adjustment is done by\nincreasingnto416ands(B)to416Ã—s(B)/184.\nIn line with the view of musicology as a source disci-\npline, signiï¬cant associations were oriented into rules from\nmusicology to MIR categories. For every signiï¬cant associ-\nation/angbracketleftA,B/angbracketright, theconï¬dence of the oriented rule Aâ†’Bwas\ncomputed as s(A,B )/s(A), indicating the empirical proba-\nbility of a paper being in MIR category Bgiven that it cites\na paper in musicology category A.\n2.4 Content Citation Analysis of Referencing Functions\nPapers supporting signiï¬cant associations were analysed in\nmore detail to reveal functions of musicology references.\nStudies of citation behaviour have proposed several clas-\nsiï¬cations of citer motivation (e.g. [2, 12, 13, 17]). In our\nanalysis, we are mainly interested in (a) the function of the\nreference in the citing paper rather than conclusions about\nthe cited work, and (b) referencing purposes that can be sug-\ngested from the content of the citing paper and the co-text of\nthe citation. Musicology references were analysed in their\ncontext in the ISMIR paper, and recurring referencing func-\ntions extracted and linked to existing citation classiï¬cations.\n3. RESULTS\nThis section presents the associations and referencing func-\ntions uncovered in the corpus.\n3.1 Associations\nFigure 1 presents the network of all associations with sup-\nportâ‰¥3extracted by the association mining method de-\nscribed in Section 2.3. The ï¬gure highlights that MIR areas\ngenerally draw on more than one musicology discipline. But\nthere are differences in the level of co-citation, i.e. occur-\nrences within the same ISMIR papers: For example, eight\nout of the ten papers on representation and metrics which\ncite music theory and analysis literature also cite psycholog-\nical work on perception and cognition. On the other hand,perception and cognition research and acoustics are cited by\ndifferent subsets of papers on transcription.\nComparing Figure 1 against MIR topics in Table 1, addi-\ntional links could have been expected e.g. between papers\non data and metadata and references to textual criticism,\narchival research and bibliography or history of music [15]\nor between classiï¬cation and performance studies (for per-\nformer identiï¬cation), acoustics (for instrument-voice clas-\nsiï¬cation) and popular music studies or history of music (for\ngenre classiï¬cation). However, these relations are supported\nby only one or two papers each and thus do not appear\nin Figure 1. Surprisingly, the category of ethnomusicology\n(folk music) (Table 2) does not feature in associations with\nMIR categories above the support threshold.\nOf the 21 associations shown in Figure 1, nine are statis-\ntically signiï¬cant (Section 2.3). Table 3 enumerates those\nassociations that have a p-value less than Î±= 0.05. In\nFigure 1 these particular associations are shown in bold lines,\nwith a directed arrow indicating in brackets the conï¬dence\nof the oriented rule. Overall the relatively low conï¬dence\nvalues conï¬rm that in general musicology areas are cited\nacross MIR categories.\nGenerally an association will be signiï¬cant if the asso-\nciation support s(A,B )is high relative to the size of one\ninvolved category. Here this applies in particular for small\ncategories like ethnomusicology (non-Western), OMR, user\nstudies or recommendation. Signiï¬cance becomes harder to\nachieve for associations between large categories; it is more\nlikely to achieve the observed level of support at random\ngiven the individual category distributions in the corpus. For\nexample, perception and cognition research is linked to sev-\neral MIR categories with high support, but the distribution\nof those MIR categories across the 93 papers citing percep-\ntion and cognition does not differ signiï¬cantly from their\ndistribution across all sampled papers.\n3.2 Referencing Functions\nFor the content citation analysis we selected the ISMIR pa-\npers supporting the associations in Table 3, as these papers\nare examples of musicology and MIR categories that are\nsigniï¬cantly correlated. Of these 47 papers, 17 papers (5\ncomputational music analysis papers, 8 representation and\nmetrics papers and 4 retrieval papers) also cite perception\nand cognition research; these references were also consid-\nered. The in-depth analysis of citation contexts in these\npapers demonstrates that musicology is used for a variety\nof purposes. Figure 2 presents a taxonomy of referencing\nfunctions and the referencesâ€™ contribution in the MIR work\n(boxes), with examples of co-text. Related features from\nthe citation analysis literature [2, 12, 13, 17] are included in\nitalics.\n431Poster Session 3\nData\n& metadataPsychology of music\n(emotion & affect)\n3 (0.4)\nComputational\nmusic analysisPsychology of music\n(perception & cognition)\n11\nMusic theory\n& analysis10 (0.3)Representation\n& metrics\n10 (0.3)Classification3\n19Sociology\n& sociopsychology\n4\nEthnomusicology\n(non Western)3 (0.6)Recommendation\n3 (0.2)User studies\n4 (0.2)\nTranscription27\n8\nAcoustics515Textual criticism,\narchival research\n& bibliography\n4Retrieval\n6 (0.3)\nOMR 5 (0.2)11History of music\n3 (0.4)\nSoftware\nsystems\n43Figure 1 . Associations (support â‰¥3) between musicology categories (dark boxes) and MIR categories. Edges are labelled with\nthe support of the association, and signiï¬cant ( Î±= 0.05) associations are indicated with dark oriented edges. Rule conï¬dence\nis indicated in brackets for signiï¬cant associations.\nA B s (A,B )p-value\ntextual criticism, archival research & bibliography omr 5 9.7e-05\nsociology & sociopsychology user studies 4 0.00068\nmusic theory & analysis computational music analysis 10 0.0035\npsychology of music (emotion & affect) data & metadata 3 0.0078\nsociology & sociopsychology recommendation 3 0.0091\nmusic theory & analysis representation & metrics 10 0.01\ntextual criticism, archival research & bibliography retrieval 6 0.016\nhistory of music retrieval 3 0.037\nethnomusicology (non western) classiï¬cation 3 0.038\nTable 3 . Signiï¬cant ( Î±= 0.05) associations found in the corpus. A: musicology category; B: MIR category; s(A,B ): support\nof the association; p-value of the association.\n43212th International Society for Music Information Retrieval Conference (ISMIR 2011)\nFunction Co-text examples\nRelevance â€œRepeated patterns [...] represent therefore one of the most salient characteristics of musical works\n[music theory references]â€ (ID242)\nContribution â€œThis paper addresses systematic differences in the performance of ï¬nal ritardandi by different pianists\n[...] the kinetic model is arguably too simple [...] In this work [...] [psychology of music references]â€\n(ID159)\nTask deï¬nition â€œAs stated in [music theory reference] musical analysis is â€™the resolution of a musical structure into\nrelatively simpler constituent elements, and the investigation of the functions of these elements within\nthat structureâ€â€™ (ID24)\nGeneral approach â€œThe [basic] idea is motivated by the results of musicological studies, such as [...]â€ (ID859)\nMethod/algorithm â€œHMM initialization [...] The covariance matrix should also reï¬‚ect our musical knowledge [...], gained\nboth from music theory as well as empirical evidence [psychology of music reference]â€ (ID30)\nData â€œwe evaluated both [OMR] systems on the same set of pages to measure their accuracy [...] [textual\ncriticism, archival research & bibliography reference]â€ (ID729)\nRelated work â€œdimensions of dissimilarity have been interpreted to be e.g. [...] [psychology of music reference]â€\n(ID345)\nFigure 2 . Taxonomy of referencing functions (top) and selected examples of co-text (bottom).\n4. DISCUSSION AND CONCLUSIONS\nThe ï¬ndings presented in this study are based on direct and\nexplicit references to musicology. However, not all refer-\nences are explicit: papers sometimes refer to musicologi-\ncal work reported in earlier MIR publications; incorporate\nconcepts or approaches like music-analytical methods into\nthe main text without including speciï¬c references; charac-\nterise the considered repertoire such as non-Western tradi-\ntions without making explicit whether the description is de-\nrived from musicological research, common cultural knowl-\nedge or the researchersâ€™ personal experience; or use music\nexamples without citing a musicological source. Taking into\naccount such references is expected to strengthen rather than\nchange the picture of associations presented here.\nFor this study the analysis corpus only contained full IS-\nMIR papers (Section 2.1). Future work could extend the\ncorpus to also include posters, in particular those from IS-\nMIR 2008 onward (because these are of equal length and\nstatus to full papers); apply multilevel association mining\nmethods [8] to hierarchical subject categorisations; allow a\npaper to be within multiple MIR categories; and evaluate\nwhether the associations found here persist and whether new\nsigniï¬cant associations arise. Furthermore, if an encoding\nof the complete ISMIR proceedings was available, other in-\nteresting types of analysis would be possible, e.g. exploringwhether certain MIR areas are over- or under-represented in\nthe corpus of papers citing musicology, or comparing use of\nmusicology references against other source disciplines like\ncomputer science or cognitive science.\nSeveral observations can be drawn from the results pre-\nsented in this paper. First, less than half of the full papers\nin the cumulative ISMIR proceedings (184 out of 416) cite\nmusicology. Given the close interdisciplinary links between\nMIR and musicology a larger percentage had been expected.\nSecond, the most frequently cited category is music per-\nception and cognition research (93 citing papers across all\nMIR categories in our corpus). On the other hand, histor-\nical musicology and especially history of music appear to\nbe under-represented in our corpus, compared to their tra-\nditional weight in musicology [16]. Third, the association\nmining has revealed signiï¬cant associations between certain\nmusicology and MIR categories. However, most pairings\nare not signiï¬cant, and this may indicate opportunities for\ncategory reï¬nement and for speciï¬c interdisciplinary collab-\noration. Fourth, the content citation analysis yields a range\nof citation purposes, from justifying the MIR topic and spec-\nifying the MIR task, through informing methods or provid-\ning data, to references which demonstrate awareness of the\nresearch context but remain without direct implications for\nthe MIR work.\n433Poster Session 3\nFollowing the discussions at ISMIR 2009 [4, 9, 20], in\nthe further development of MIR we would expect that with\nthe increasing interest in ethnic music (e.g. [3, 19]) ethno-\nmusicology will more strongly feed not only into classiï¬-\ncation but also MIR areas such as representation and met-\nrics, transcription or retrieval; envisage more musicology\nreferences, including history of music, in deï¬ning MIR re-\nsearch questions and in interpreting MIR results; and en-\ncourage more projective references highlighting potential of\nMIR achievements for musicology, beyond providing tech-\nnological tools. The association mining and content analy-\nsis methods applied in this paper will be invaluable to study\nthe continuing evolution of the ï¬eld of music information\nretrieval.\n5. REFERENCES\n[1] J.-J. Aucouturier and E. Pampalk. Introduction â€“ From\ngenres to tags: A little epistemology of music informa-\ntion retrieval research. Journal of New Music Research ,\n37(2):87â€“92, 2008.\n[2] T. A. Brooks. Private acts and public objects: An inves-\ntigation of citer motivations. Journal of the American\nSociety of Information Science , 36(4):223â€“229, 1985.\n[3] O. Cornelis, M. Lesaffre, D. Moelants, and M. Leman.\nAccess to ethnic music: Advances and perspectives in\ncontent-based music information retrieval. Signal Pro-\ncessing , 90:1008â€“1031, 2010.\n[4] J. Downie, D. Byrd, and T. Crawford. Ten years of IS-\nMIR: Reï¬‚ections on challenges and opportunities. In\nInternational Society for Music Information Retrieval\nConference , pages 13â€“18, 2009.\n[5] S. Falcon and R. Gentleman. Hypergeometric testing\nused for gene set enrichment analysis. In F. Hahne,\nW. Huber, R. Gentleman, and S. Falcon, editors, Biocon-\nductor Case Studies , pages 207â€“220. Springer, 2008.\n[6] J. Futrelle and J. Downie. Interdisciplinary research is-\nsues in music information retrieval: ISMIR 2000â€“2002.\nJournal of New Music Research , 32(2):121â€“131, 2003.\n[7] M. Grachten, M. Schedl, T. Pohle, and G. Widmer. The\nISMIR cloud: a decade of ISMIR conferences at your\nï¬ngertips. In International Society for Music Informa-\ntion Retrieval Conference , pages 63â€“68, 2009.\n[8] J. Han and Y . Fu. Discovery of multiple-level association\nrules from large databases. In International Conference\non Very Large Data Bases , pages 420â€“431, 1995.\n[9] P. Herrera, J. Serr `a, C. Laurier, E. Guaus, E. G Â´omez,\nand X. Serra. The discipline formerly known as MIR.InInternational Society for Music Information Retrieval\nConference, fMIR workshop , 2009.\n[10] J. Lee, M. Jones, and J. Downie. An analysis of ISMIR\nproceedings: Patterns of authorship, topic, and citation.\nInInternational Society for Music Information Retrieval\nConference , pages 57â€“62, 2009.\n[11] Library of Congress. Library of Congress\nClassiï¬cation Outline: Class M â€“ Music .\nhttp://www.loc.gov/catdir/cpso/lcco/.\n[12] M. Liu. Citation functions and related determinants: A\nstudy of Chinese physics publications. Journal of Li-\nbrary and Information Science , 19(1):1â€“13, 1993.\n[13] M. Moravcsik and P. Murugesan. Some results on the\nfunction and quality of citations. Social Studies of Sci-\nence, 5(1):86â€“92, 1975.\n[14] R. Parncutt. Systematic musicology and the history and\nfuture of western musical scholarship. Journal of Inter-\ndisciplinary Music Studies , 1(1):1â€“32, 2007.\n[15] J. Riley and C. Mayer. Ask a librarian: The role of li-\nbrarians in the music information retrieval community.\nInProceedings of the International Conference on Mu-\nsic Information Retrieval , 2006. w/o pages.\n[16] S. Sadie, editor. The New Grove Dictionary of Music and\nMusicians , chapter â€œEthnomusicologyâ€, â€œMusic Analy-\nsisâ€, â€œMusicologyâ€, â€œPsychology of Musicâ€, and â€œThe-\nory, Theorists. 15: New Theoretical Paradigms, 1980-\n2000â€. Macmillan, London, 2001.\n[17] J. Swales. Citation analysis and discourse analysis. Ap-\nplied Linguistics , 7(1):39â€“56, 1986.\n[18] G. Tzanetakis, A. Kapur, W. Schloss, and M. Wright.\nComputational ethnomusicology. Journal of Interdisci-\nplinary Music Studies , 1(2):1â€“24, 2007.\n[19] P. van Kranenburg, J. Garbers, A. V olk, F. Wiering,\nL. Grijp, and R. C. Veltkamp. Collaborative perspectives\nfor folk song research and music information retrieval:\nThe indispensable role of computational musicology.\nJournal of Interdisciplinary Music Studies , 4(1):17â€“43,\n2010.\n[20] F. Wiering. Meaningful music retrieval. In International\nSociety for Music Information Retrieval Conference,\nfMIR workshop , 2009.\n434"
    },
    {
        "title": "On the Importance of &quot;Real&quot; Audio Data for MIR Algorithm Evaluation at the Note-Level A Comparative Study.",
        "author": [
            "Bernhard Niedermayer",
            "Sebastian BÃ¶ck",
            "Gerhard Widmer"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1417923",
        "url": "https://doi.org/10.5281/zenodo.1417923",
        "ee": "https://zenodo.org/records/1417923/files/NiedermayerBW11.pdf",
        "abstract": "A considerable number of MIR tasks requires annotations at the note-level for the purpose of in-depth evaluation. A common means of obtaining accurately annotated data corpora is to start with a symbolic representation of a piece and generate corresponding audio data. This study investigates the effect of audio quality and source on the performance of two representative MIR algorithms â€“ Onset Detection and Audio Alignment. Three kinds of audio material are compared: piano pieces generated using a freely available software synthesizer with its default instrument patches; a commercial high-quality sample library; and audio recordings made on a real (computer-controlled) grand piano. Also, the effect of varying richness of artistic changes in tempo and dynamics or natural asynchronies is examined. We show that the algorithmsâ€™ performance on the different datasets varies considerably, but synthesized audio, does not necessarily yield better results.",
        "zenodo_id": 1417923,
        "dblp_key": "conf/ismir/NiedermayerBW11",
        "keywords": [
            "audio quality",
            "source",
            "Onset Detection",
            "Audio Alignment",
            "piano pieces",
            "commercial high-quality sample library",
            "real (computer-controlled) grand piano",
            "artistic changes in tempo and dynamics",
            "natural asynchronies",
            "performance varies considerably"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nON THE IMPORTANCE OF â€œREALâ€ AUDIO DATA FOR MIR ALGORITHM\nEVALUATION AT THE NOTE-LEVEL â€“ A COMPARATIVE STUDY\nBernhard Niedermayer1, Sebastian B Â¨ock1\n1Dept. of Computational Perception\nJohannes Kepler University Linz, Austria\nmusic@jku.atGerhard Widmer1,2\n2Austrian Research Institute for Artiï¬cial Intelligence\nVienna, Austria\nmusic@ofai.at\nABSTRACT\nA considerable number of MIR tasks requires annotations\nat the note-level for the purpose of in-depth evaluation. A\ncommon means of obtaining accurately annotated data cor-\npora is to start with a symbolic representation of a piece and\ngenerate corresponding audio data. This study investigates\nthe effect of audio quality and source on the performance of\ntwo representative MIR algorithms â€“ Onset Detection and\nAudio Alignment. Three kinds of audio material are com-\npared: piano pieces generated using a freely available soft-\nware synthesizer with its default instrument patches; a com-\nmercial high-quality sample library; and audio recordings\nmade on a real (computer-controlled) grand piano. Also, the\neffect of varying richness of artistic changes in tempo and\ndynamics or natural asynchronies is examined. We show\nthat the algorithmsâ€™ performance on the different datasets\nvaries considerably, but synthesized audio, does not neces-\nsarily yield better results.\n1. INTRODUCTION\nOnset Detection, Automatic Transcription, or Audio Align-\nment are only a small number of examples of MIR tasks that\nrequire ground truth data at the note-level for an in-depth\nevaluation. However, such data corpora are rare for several\nreasons. Starting from an audio recording, manual anno-\ntation is not only highly time consuming but also has cer-\ntain limits in terms of accuracy and level of detail. On the\none hand, it is questionable how precisely or consistently\na human annotator can determine note onsets â€“ particularly\nâ€œsoftâ€ ones. On the other hand, aspects like the loudness of\nan individual chord note might not be distinguishable even\nfor experienced listeners.\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proï¬t or commercial advantage and that copies\nbear this notice and the full citation on the ï¬rst page.\nc/circlecopyrt2011 International Society for Music Information Retrieval.\nMan.\nSynth.\nDiskl.\nAudio Onset Detection X\nReal-Time Audio to Score Alignment X\nAudio Melody Extraction X\nMultiple f0Estimation and Tracking X X X\nAudio Chord Estimation X\nAudio Beat Tracking X\nTable 1 . Overview of MIREX tasks and the respective\nsources of test data (manual annotation, synthesized from\nMIDI, playback on a Disklavier)\nStarting from a symbolic representation implies its own\nchallenges. To obtain a realistic audio representation, two\naspects have to be taken into account. First, the symbolic\ndata should describe a human-like performance, i.e. contain\nartistic variations in tempo, dynamics, or playing style and\nalso more subtle ones such as slight arpeggiations or asyn-\nchronies.\nThe second important aspect is the quality of the conver-\nsion from the symbolic to the audio domain. One option\nis to use computer controlled musical instruments (e.g. a\nplayer piano) preserving the whole acoustic complexity of\nthe sound source. Problems are the availability of such in-\nstruments and recording issues. An alternative would be the\nusage of (software) synthesizers. Although this method is\nrelatively common in the literature (see [2, 3, 8] for exam-\nple), it is not clear if and to what extent such data yields\ndifferent results in an evaluation process.\nTable 1 gives an overview of MIREX [4] tasks which re-\nquire note- or at least beat-level annotations for evaluation\npurposes. With the exception of one single task, where au-\ndio material is generated from a symbolic ground truth rep-\nresentation, there is a clear preference towards the usage of\nâ€œrealâ€ audio recordings and human annotations. However,\nit is not clear if this under-representation of evaluation data\ngenerated from a known ground truth is due to a lack of\n543Poster Session 4\nsuch symbolic data and adequate rendering mechanisms, or\nif such audio material would indeed adulterate evaluation\nresults.\nThis work presents a study on different approaches for\nthe generation of audio data from symbolic representation\nand their inï¬‚uence on evaluation results of two MIR algo-\nrithms â€“ Onset Detection and Audio Alignment. To this end,\nMIDI data from real piano performances were turned into\naudio recordings in three ways: (i) by recording the sound\nproduced by a computer-controlled piano when playing the\nMIDI ï¬les; (ii) by synthesizing the data using a commercial\nhigh-quality sample library; and (iii) by using a freely avail-\nable sound patch library. Also, since performances of pro-\nfessional musicians are rarely available in a symbolic repre-\nsentation, the inï¬‚uence of changes in the richness of artistic\nvariations (i.e. changing tempo, dynamics, pedal pressure)\nwas studied. The piano was chosen due to the availability\nof computer controlled instruments and thus the opportu-\nnity to obtain highly accurate audio data other version can\nbe compared to. Also, piano music is a common means of\nnote-level evaluation in literature.\n2. EVALUATION TASKS\nTo examine the effect of different sound sources on the per-\nformance of MIR algorithms, two sample subï¬elds have\nbeen selected â€“ (i) Onset Detection and (ii) Audio Align-\nment. These two task are representative insofar as they allow\ncertain conclusions to be drawn about various other MIR\ntasks they are either integral parts of (such as Audio Tran-\nscription or Cover Version Detection) or share crucial sub-\nroutines or features (such as Score Following, Structural\nAnalysis, or Beat Tracking).\n2.1 Onset Detection\nThe chosen algorithm for Onset Detection is the one that\nyielded the highest average f-measure in the MIREX 20101\nalgorithm comparison [5].\n2.1.1 Features\nFeatures are extracted in the spectral domain. The signal is\ntherefore transformed using two parallel STFTs with Ham-\nming windows of lengths 1024 (23 ms) and 2048 (46 ms)\nrespectively. The hop size, however, is 441 samples in both\ncases yielding a common time resolution of 10 ms per frame.\nAccording to the human perception of sounds, the (power)\nspectrograms are then converted to the Mel-scale using a\nï¬lterbank consisting of 40 triangular ï¬lters spread equidis-\ntantly on the Mel-scale. In a last step, the logarithm is taken\nto obtain the ï¬nal feature values.\n1MIREX 2010 â€“ Onset Detection Results\nhttp://nema.lis.illinois.edu/nema_out/mirex2010/\nresults/aod/summary.htmlIn addition to the absolute values, the half-wave rectiï¬ed\nï¬rst order difference is calculated as an indicator for new\nspectral components.\n2.1.2 Algorithm\nAs most other Onset Detection algorithms, the one used here\nworks in two steps. In the ï¬rst one, a detection function\nis calculated, representing novelty within the signal. In a\nsecond pass, peaks in the detection function are picked and\nclassiï¬ed as onsets.\nTo obtain the detection function, a bidirectional neural\nnetwork with Long Short-Term Memory (LSTM) units is\napplied. Its number of input units is 160, corresponding\nto the feature values as described above. The actual neu-\nral net consists of six hidden layers â€“ two for each direction\nâ€“ with 20 LSTM units each and two output units yoandyn\nrepresenting the classes â€™onsetâ€™ and â€™no onsetâ€™ respectively.\nThese outputs are normalized such that the range of values\nis[0,1]and the sum of yoandynis 1.\nTraining of the network was done iteratively by gradi-\nent descent with error backpropagation until no more im-\nprovement has been observed for 20 epochs. The training\nand validation sets used consist of samples from the dataset\nintroduced be Bello et al. [1] and the ballroom dataset by\nGouyon et al. [7].\nThe peak picking on the detection function applies a sim-\nple thresholding approach where a ï¬xed threshold depend-\ning on the median of the detection function is determined\nfor each piece. Each remaining peak is ï¬nally reported as\nan onset.\n2.2 Audio-to-Score Alignment\nConcerning audio alignment, a simple algorithm based on\nDynamic Time Warping (DTW) and Chroma vectors has\nbeen chosen. Although this approach dates back several\nyears and improvements concerning aspects like robustness\nor accuracy have been proposed, it is still used not only\nfor Audio-to-Score Alignment itself but also for Structural\nAnalysis, Cover Version Detection or Retrieval Tasks. For\nsimplicity reasons, the Audio-to-Score Alignment task will\nbe referred to as Audio Alignment only in the remainder of\nthis work.\n2.2.1 Features\nDue to their robustness to timbre, certain recording condi-\ntions, and varying degrees of polyphony, chroma vectors are\ncommonly used for synchronization tasks. They consist of\na 12-dimensional vector for each time frame, where each\nelement represents the relative energy of a pitch class (i.e.\nC, C#, D,. . . ). The extraction from audio signals is done in\nthe spectral domain based on a mapping of each bin to the\nnote where the fundamental frequency is closest to the binâ€™s\ncenter frequency. In a second step, coefï¬cients of all bins\n54412th International Society for Music Information Retrieval Conference (ISMIR 2011)\nmapped to notes of the same pitch class are summed up. Fi-\nnally, the vector is normalized by linear scaling such that its\nmaximum is equal to 1.\nThe (mechanic) score representation is segmented into\ntime frames such that the number of time frames and the\noverlap ratio are the same as for the corresponding audio\ndata. The energy of a pitch is then set to the fraction of the\nwindow length in which it is played. The octave folding and\nnormalization is then performed in analogous manner as for\nthe audio data.\n2.2.2 Algorithm\nTo compute the actual alignment, the approach described\nin [10] is used. In a ï¬rst pass, features are computed on\nwindows with a length of 4096 samples and an overlap ra-\ntio of 50%. Dynamic Time Warping is then performed to\nobtain an initial alignment. The resulting time resolution\nis relatively low. However, since the Dynamic Time Warp-\ning algorithm is of quadratic complexity in time and also in\nspace, this is necessary to also process long pieces.\nTo circumvent this tradeoff, a second pass is performed\nat a higher time resolution. Here, the features are calcu-\nlated using a window length of 1024 samples and a hop size\nof 256 samples. Computational costs are kept low by re-\nstricting the search for an optimal alignment to a certain area\naround the coarse initial alignment. Here, a radius of Â±1000\nframes has been chosen.\n3. EVALUATION DATA\nThe data set used throughout this study comprises the ï¬rst\nmovements of 13 piano sonatas by W. A. Mozart. Those\npieces have been performed by a professional pianist on\na computer monitored grand piano (B Â¨osendorfer SE 290),\nyielding an exact ground truth of all performance parame-\nters including timing, dynamics, and pedal pressure. The\ndata was originally represented in a proprietary, symbolic\nformat which was then converted into MIDI. As shown in\nTable 2, it covers almost 42000 notes and a performance\ntime of more than 80 minutes.\nFor the purpose of evaluation, the performance data was\nmatched to a symbolic score representation. Manual correc-\ntion was done, to ensure that playing errors and also short\nsections where the pianist did not stick to the score at all are\nannotated accordingly.\nAudio recordings were then obtained from this perfor-\nmance data using three different sources â€“ playback on the\nBÂ¨osendorfer 290 SE from which the symbolic data origi-\nnated, synthesizing using high quality instrument samples\nproduced by the Vienna Symphonic Library , and rendering\nusing the free synthesizer Timidity and its default instrument\npatches provided by the Freepats project.3.1 B Â¨osendorfer SE 290\nThe B Â¨osendorfer SE 290 is the computer controlled grand\npiano which was used to obtain the symbolic performance\ndata. It relies on optical sensors to detect movements of\nindividual keys and hammers. One such sensor consists\nof a phototransistor and a coupled LED about 3 mm apart.\nPrecision-cut aluminum shutters attached to the keys and\nhammers discontinue the corresponding beam of light and\nthus trigger a sensor event. The system is set up such that\na key movement is reported as soon as it is minutely de-\npressed. A hammer movement and its velocity, on the other\nhand, are detected at the instant a hammer hits the string [9].\nThe playback mechanism is based on small linear motors\nunderneath the key bed actuating the keys. They are con-\nstructed such that the only contact between key and actuator\nis during playback mode and no interference occurs while a\npianist is playing the instrument.\nIn [6] the SE 290 was compared to the Yamaha Disklavier\ngrand piano â€“ another system commonly used in perfor-\nmance research. It has been found that the SE 290 is more\naccurate than the Disklavier at monitoring and also at play-\nback. Both systems were affected by systematic timing de-\nviations (linearly increasing over time) likely to be caused\nby inaccuracies of the internal clock-pulse generators. This\nï¬‚aw aside, the residual mean timing errors in monitoring\nmode accounted for 0.2 ms (stddev: 2.1 ms) for B Â¨osendor-\nferâ€™s and for 1.4 ms (stddev: 3.8 ms) for Yamahaâ€™s grand pi-\nano. Considering reproduction accuracy, the Disklavier was\nagain clearly outperformed by the SE system where timing\ndeviations rarely exceeded 3 ms.\nThe recordings on this instrument were made at 44.1 kHz\nusing a single high-quality microphone near the corpus of\nthe piano and a DAT recorder.\n3.2 Vienna Symphonic Library\nThe Vienna Symphonic Library2(VSL) is a commercial\nvendor of high quality instrument samples not only cover-\ning a wide range of musical instruments but also different\nplaying styles. While synthesizing MIDI data, a special se-\nquencer plug-in analyzes the stream of events for repeated\nnotes and other certain patterns and determines the appropri-\nate articulation or nuance in real-time. An example are pas-\nsages played in legato on wind or string instruments, where\nnot only tones themselves but also real note transitions are\nsampled to yield a more natural sound.\nTheSpecial Edition â€“ Standard of the sample library con-\ntains the B Â¨osendorfer 290 â€Imperialâ€œ, which is the same\ntype of grand piano the SE system, as described above, was\nintegrated into. This provides the opportunity to compare\nthe authentic sound of the grand piano to its generated repro-\nduction. The objective is to show if and how potential devi-\n2http://vsl.co.at/\n545Poster Session 4\nations inï¬‚uence MIR algorithms and their respective evalu-\nation results.\nSince the software is not a sequencer of its own, Garage-\nBand3was used for synthesizing. Although GarageBand\ncan not be considered a high-end product, the audio mate-\nrial obtained as described above beneï¬ts from the plug-in\nprovided by the VSL.\n3.3 Timidity++/Freepats\nTimidity++4is a free software synthesizer distributed under\nthe GNU General Public License and available for a variety\nof operating systems. Although it can be conï¬gured to work\nwith any set of instrument samples given in GUS/patch for-\nmat, it, by default, uses the voice data provided by the Freep-\nats5project. Timidity has been included in this comparison\nbecause, on the one hand, the software as well as the instru-\nment samples are freely available and, on the other hand, it\nhas been used in recent MIR research (e.g. [2, 3, 8]).\n4. DIFFERENT RENDERING METHODS\nIn a ï¬rst experiment, the inï¬‚uence of the rendering method\nwas examined. Therefore, audio signals were obtained from\nthe three sources as described above â€“ the computer con-\ntrolled B Â¨osendorfer SE 290 grand piano, the Vienna Sym-\nphonic Library, and Timidity using its default sound patches.\nThe results yielded by the Onset Detection and the Audio-\nto-Score Alignment are shown in Table 2. The Onset De-\ntection performance is determined analogous to the MIREX\nevaluation. The reported onsets are compared to the ground\ntruth allowing a timing deviation of Â±50 ms. The quality of\nthe result is then given in terms of the f-measure. The accu-\nracy of the Audio Alignment is expressed by the percentage\nof individual notes for which the onset time in the alignment\ndeviates by also less than 50 ms from the ground truth.\nThe evaluation presented here deviates from the one per-\nformed at MIREX in one aspect, which is, however, justiï¬ed\nby the nature of the ground truth data. Merged onsets, i.e.\ntwo adjacent onsets are reported as one single onset, are not\npenalized here. Since each individual noteâ€™s onset time is\nknown, it occurs that there is more than one onset within a\nsingle or two adjacent audio frames. Such onsets cannot be\ndistinguished without also transcribing the notesâ€™ pitches.\nConcerning the Onset Detection, the performance on the\ndata synthesized using the Vienna Symphonic Library is the\nhighest on all individual pieces with only one exception â€“\nk283-1 â€“ where the signal from the SE 290 yields the high-\nest f-value. On the other hand, the audio data obtained from\nTimidity results in the lowest f-measure for each piece. This\n3http://www.apple.com/de/ilife/garageband/\n4http://timidity.sourceforge.net\n5http://freepats.zenvoid.orgcontradicts the possible speculation that lower quality syn-\nthesizers (instrument patches) would produce somehow â€ar-\ntiï¬cialâ€ sounds and in doing so reduce the complexity of the\nresulting audio ï¬le. Looking at the spectra of two tones â€“\none played on the SE 290 and one generated by timidity â€“\nreveals that the tone obtained from timidity contains a sig-\nniï¬cant proportion of noise in the high frequency bins (see\nFigure 1). This phenomenon was observed to be consistent\nthroughout the whole pitch range and is therefore a likely\nexplanation for the worse performance of the Onset Detec-\ntion on the timidity dataset.\nAlthough the evaluation of the Audio Alignment does\nnot draw such a clear picture, some of the results are con-\nï¬rmed. Again, the performance on the timidity dataset was\nsigniï¬cantly lower than the one on the â€realâ€œ audio from\nthe SE 290. However, the VSL dataset results in the lowest\noverall accuracy. Comparing the spectra of tones generated\nby the VSL to those played on the SE 290 shows differences\nin the relative strengths of individual harmonics. This will\ninï¬‚uence the chroma feature and is therefore a likely expla-\nnation for the discrepancy in the results.\n5. VARYING RICHNESS OF EXPRESSIVE\nDETAILS\nThe symbolic representation used to obtain the audio ma-\nterials for the above experiment derives from a real perfor-\nmance (on the Boesendorfer SE290) by a skilled concert pi-\nanist. It thus contains detailed information about expressive\nperformance aspects (expressive timing, dynamics nuances,\nexact pressure on the pedals). In many controlled MIR ex-\nperiments, the starting MIDI data will be based on a score\ninstead of real performances, and will therefore be impover-\nished in the sense that it will not correspond to the kind of\nmusical material usually encountered in practice.\nIn order to ï¬nd out whether the lack (or presence) of ex-\npressive timing etc. signiï¬cantly impact MIR algorithms,\nour MIDI ï¬les were deliberately â€cleanedâ€ from such ex-\npressive performance aspects. Speciï¬cally, the usage of the\npedals, varying dynamics, and intra-chord timings (i.e. ar-\npeggiations and asynchronies) were suppressed by deleting\nthe according events, setting velocities to a constant, and as-\nsigning asynchronous chord notes a uniform onset time.\nThe means of synthesizing was chosen to be timidity for\ntwo reasons. First, we assumed that if a computer controlled\ninstrument were available, it could be used to obtain the\ncomplete performance information. Second, the VSL soft-\nware and its mechanism to use different samples according\nto the musical context would interfere with the experiment.\nWe found that suppressing the usage of the pedals, chang-\ning dynamics, or both had only negligible inï¬‚uence on the\noverall performance. Likely explanations are that the us-\nage of pedals plays a relatively minor role when performing\n54612th International Society for Music Information Retrieval Conference (ISMIR 2011)\npiece # notes durationOnset Detection Audio-to-Score Alignment\nSE 290 VSL timidity SE 290 VSL timidity\nk279-1 2803 4:55 96.31 98.00 92.11 90.37 85.52 87.73\nk280-1 2491 4:48 98.08 98.80 95.64 85.27 79.37 85.47\nk281-1 2648 4:29 95.83 97.83 92.20 88.37 85.08 86.48\nk282-1 1907 7:35 97.70 98.87 96.42 76.68 71.93 74.93\nk283-1 3304 5:22 97.08 96.53 92.45 93.89 85.05 90.89\nk284-1 3700 5:17 94.82 98.58 93.40 92.08 90.35 86.97\nk330-1 3160 6:14 97.19 99.32 95.50 95.13 90.03 90.19\nk331-1 6123 13:35 98.02 98.50 95.55 73.00 66.62 70.70\nk332-1 3470 6:02 94.84 98.26 94.01 87.61 83.52 81.07\nk333-1 3774 6:44 96.83 98.31 93.13 93.51 93.19 92.29\nk457-1 2993 6:15 95.92 96.80 92.33 88.31 79.45 80.09\nk475-1 1284 4:58 96.69 98.29 95.60 61.21 59.04 43.04\nk533-1 4339 8:25 95.30 98.11 94.06 92.90 87.14 89.91\nall 41994 1:24.39 96.51 98.18 94.00 86.85 81.93 82.99\nTable 2 . Performance of the example algorithms on the datasets generated using different rendering methods\n(a)\n (b)\n (c)\nFigure 1 . Spectra of a C3 as played on the B Â¨osendorfer grand piano (a) and synthesized by the VSL (b) and timidity (c)\ncalculated applying a Blackman-Harris window of length 8192 starting 50 ms after the note onset\npieces by Mozart. Also, the chroma vectors used for Audio\nAlignment are normalized to reduce the inï¬‚uence of vary-\ning loudness and the neural network seems to have learned\na similar concept.\nHowever, the inï¬‚uence of micro timings (i.e. asynchro-\nnies) on the Audio Alignment was signiï¬cant compared to\na version where the onsets of all notes of a chord were set\nto same time (see Table 3). This is partly due to the fact\nthat Audio-to-Score Alignment using Dynamic Time Warp-\ning without post-processing at the note-level is inherently\nerror prone as soon as asynchronies occur. The algorithm\ncannot assign different times to events which are simultane-\nous in the score.\nAlthough we expected the Onset Detection to also ben-\neï¬t substantially from having one simultaneous onset for\na whole chord instead of several onsets of the individual\nnotes, results disproved this assumption. A further inspec-tion showed that while chord onsets have been correctly de-\ntected, onsets of notes played one at a time were missed.\nThis is due to a masking effect caused be the exceptionally\nhigh values in the detection function caused by the exact\nconcurrence of several notesâ€™ onsets.\nTo get an idea on the actual extent of asynchronies in\na natural performance, the time spreads of chords according\nto their degree of polyphony was determined. Table 4 shows\nthat two notes which are notated concurrently in the score\ncan be up to half a second apart in the actual performance,\nhighlighting that natural timings contribute signiï¬cantly to\nthe complexity of a musical performance.\n6. CONCLUSION\nWe have presented an extensive comparison of different ap-\nproaches to generate audio material from a symbolic repre-\n547Poster Session 4\npieceOnset Detection Audio Alignment\nfull time full time\nk279-1 92.11 98.10 87.73 95.33\nk280-1 95.64 99.30 85.47 95.19\nk281-1 92.20 82.53 86.48 91.66\nk282-1 96.42 92.55 74.93 96.89\nk283-1 92.45 97.15 90.89 99.64\nk284-1 93.40 99.52 86.97 98.57\nk330-1 95.50 89.56 90.19 96.52\nk331-1 95.55 98.49 70.70 99.11\nk332-1 94.01 99.15 81.07 99.17\nk333-1 93.13 99.73 92.29 96.88\nk457-1 92.33 99.32 80.09 95.07\nk475-1 95.60 91.56 43.04 80.58\nk533-1 94.06 92.24 89.91 97.29\nall 96.51 96.01 82.99 96.61\nTable 3 . Performance of the example algorithms on the\ndatasets exhibiting all aspects of expressive variations (full)\nand with suppressed micro timings (time)\np# occurrences min avg max stddev\n1 15999 - - - -\n2 6742 0.000 0.015 0.286 0.017\n3 2732 0.000 0.020 0.471 0.023\n4 840 0.001 0.035 0.391 0.051\n5 130 0.005 0.125 0.529 0.131\n6 46 0.005 0.155 0.511 0.121\n7 3 0.010 0.014 0.017 0.003\n8 1 -0.009 - -\nTable 4 . Asynchronies and arpeggiations in [sec]for each\ndegree of polyphony p\nsentation and its inï¬‚uence on the evaluation results of two\nrepresentative MIR algorithms. On the one hand, the useful-\nness of synthesized data for evaluation purposes was proven\nby the large number of consistencies concerning the ranking\nof individual results. On the other hand, however, it became\nevident, that synthesized data can have their own speciï¬ci-\nties carrying the inherent risk of overï¬tting.\nWe have shown that the quality of instrument samples\nused for synthesizing has a signiï¬cant inï¬‚uence on evalu-\nation results. Also, natural timings including asynchronies\nand arpeggiations are a crucial aspect to account for in the\nground truth data in order to obtain most meaningful evalua-\ntion results. This does not only refer to a algorithms perfor-\nmance on different audio data but also to evaluation itself,\nwhere such rich data would allow for criteria more accu-\nrate than, for example, the Â±50ms tolerance threshold com-\nmonly used in onset detection.7. ACKNOWLEDGMENTS\nThis research is supported by the Austrian Research Fund\n(FWF) under grants TRP109-N23 and Z159. Special thanks\nare due to the Vienna Symphonic Library andBÂ¨osendorfer\nfor providing access to instrument samples and the instru-\nment itself respectively.\n8. REFERENCES\n[1] J. P. Bello, L. Daudet, S. Abdallah, C. Duxbury, M.\nDavies, and M. Sandler: â€œA tutorial on onset detection in\nmusic signals,â€ IEEE Transactions on Speech and Audio\nProcessing , V ol. 13, No. 5, pp. 1035â€“1047, 2005.\n[2] R. B. Dannenberg, and N. Hu: â€œPolyphonic Audio\nMatching for Score Following and Intelligent Audio\nEditors,â€ International Computer Music Conference\n(ICMC 2003) , pp. 27â€“34, San Francisco, 2003.\n[3] S. Dixon: â€œOn the computer recognition of solo pi-\nano music,â€ Australasian Computer Music Conference ,\npp. 31â€“37, Brisbane, 2000.\n[4] J. S. Downie, A. F. Ehmann, and J. H. Lee: â€œThe Music\nInformation Retrieval Evaluation eXchange (MIREX):\nCommunity-led formal evaluations,â€ Digital Humanities\n2008 , pp. 239â€“240, Oulu, 2008.\n[5] F. Eyben, S. B Â¨ock, B. Schuller, and A. Graves: â€œUni-\nversal Onset Detection with Bidirectional Long Short-\nTerm Memory Neural Networks,â€ 11thInternational So-\nciety for Music Information Retrieval Conference (IS-\nMIR 2010) , pp. 589â€“594, Utrecht, 2010.\n[6] W. Goebel, and R. Bresin: â€œMeasurement and Repro-\nduction Accuracy of Computer-Controlled Grand Pi-\nanos,â€ Stockholm Music Acoustics Conference (SMAC\n03), pp. 155â€“158, Stockholm, 2003.\n[7] F. Gouyon, A. Klapuri, S. Dixon, M. Alonso, G. Tzane-\ntakis, C. Uhle, and P. Cano: â€œAn experimental compari-\nson of audio tempo induction algorithmsâ€, IEEE TASLP ,\nV ol. 14, No. 5, pp. 1832â€“1844, 2006.\n[8] A. Klapuri: â€œA method for visualizing the pitch content\nof polyphonic music signals,â€ 10thInternational Soci-\nety of Music Information Retrieval Conference (ISMIR\n2009) , pp. 615â€“620, Kobe, 2009.\n[9] R. A. Moog, T. L. Rhea: â€œEvolution of the keyboard in-\nterface: The B Â¨osendorfer 290 SE recording piano and\nthe Moog multiply-touch-sensitive keyboards,â€ Com-\nputer Music Journal , V ol. 14, No. 2, pp. 52â€“60, 1990.\n[10] B. Niedermayer: â€œTowards Audio to Score Alignment\nin the Symbolic Domainâ€, 6thSound and Music Com-\nputing Conference (SMC 2008) , pp. 77â€“82, Porto, 2008.\n548"
    },
    {
        "title": "Classifying Bach&apos;s Handwritten C-Clefs.",
        "author": [
            "Masahiro Niitsuma",
            "Yo Tomita"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1415822",
        "url": "https://doi.org/10.5281/zenodo.1415822",
        "ee": "https://zenodo.org/records/1415822/files/NiitsumaT11.pdf",
        "abstract": "The aim of this study is to explore how we could use computational technology to help determination of the chronology of music manuscripts. Applying a battery of techniques to Bachâ€™s manuscripts reveals the limitation in current image processing techniques, thereby clarifying future tasks. Analysis of C-clefs, the chosen musical symbol for this study, extracted from Bachâ€™s manuscripts dating from 1708â€“1748, is also carried out. Random forest using 15 features produces significant accuracy for chronological classification.",
        "zenodo_id": 1415822,
        "dblp_key": "conf/ismir/NiitsumaT11",
        "keywords": [
            "computational technology",
            "chronology of music manuscripts",
            "image processing techniques",
            "Bachâ€™s manuscripts",
            "C-clefs",
            "music symbol",
            "future tasks",
            "random forest",
            "15 features",
            "chronological classification"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nCLASSIFYING BACHâ€™S HANDWRITTEN C-CLEFS\nMasahiro Niitsuma Yo Tomita\nSchool of Muisc and Sonic Arts, Queenâ€™s University, Belfast\nmniitsuma01@qub.ac.uk y.tomita@qub.ac.uk\nABSTRACT\nThe aim of this study is to explore how we could use\ncomputational technology to help determination of the chronol-\nogy of music manuscripts.\nApplying a battery of techniques to Bachâ€™s manuscripts\nreveals the limitation in current image processing techniques,\nthereby clarifying future tasks. Analysis of C-clefs, the cho-\nsen musical symbol for this study, extracted from Bachâ€™s\nmanuscripts dating from 1708â€“1748, is also carried out. Ran-\ndom forest using 15 features produces signiï¬cant accuracy\nfor chronological classiï¬cation.\n1. INTRODUCTION\nIn the development of western music, handwritten scores\nand parts have played a signiï¬cant role even after the inven-\ntion of making prints because they allowed composers to ex-\npress their ideas in a personalized way. In manuscripts, the\nwriterâ€™s intention is assumed to be present, and manuscripts\nare often the only surviving witness for them and their work,\nand for this reason, they should be analyzed with utmost\ncare and attention.\nAlthough optical music recognition (OMR) has been in-\nvestigated actively for this, there has been little research in-\nvestigating such aspects of music manuscripts beyond OMR.\nEnote history1[3, 4] and the researches by Fornes [10] are\nsuch examples, which deals with such as writer identiï¬ca-\ntion or how just a subtle change of handwriting could reveal\nthe situation under which the writer was working.\nThis paper explores the analysis of Bachâ€™s C-clefs and we\nassociates the image processing issues. C-clefs have been\n1Enote history is a name of the project which mainly concerns scriber\nidentiï¬cation in handwritten music manuscripts from the 18th century. This\nwas achieved by the cooperation of several research institutes: the library\nof the university of Rostock, the department of musicology at the univer-\nsity of Rostock, the database research group at the department of computer\nscience, and the Fraunhofer institute for computer graphics.\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proï¬t or commercial advantage and that copies\nbear this notice and the full citation on the ï¬rst page.\nc/circlecopyrt2011 International Society for Music Information Retrieval.identiï¬ed by Bach scholars as one of the most crucial criteria\nto date the manuscript. Musicologists such as Dadelsen [21]\nand Emery [9] claim that Bachâ€™s C-clef can be categorized\ninto three or four groups and each group mainly appears\nin a speciï¬c period. Dadelsen applied this to identiï¬cation\nof the chronological order of Bachâ€™s manuscripts. One of\nthe weaknesses of their discussions seems to be the lack of\nany quantitative evaluation of their hypothesis. Their inves-\ntigation is apparently supported by their deep background\nknowledge and experience, which cannot be easily emulated\nby computer. This lack of reproducibility of their research\ncan be addressed in the musicology of the future. High re-\nproducibility is in fact one of the biggest advantages of com-\nputational analysis.\nFigure 1 shows the C-clefs found in Bachâ€™s manuscripts\narranged in a chronological order suggested by musicolo-\ngists [14], which demonstrates that the shape of Bachâ€™s hand-\nwriting changed over time. Bach scholars investigate the is-\nsue of chronology by examining various types of evidence\nholistically. Evidence typically include watermarks, hand-\nwriting, a documented use of the manuscripts giving clues\nto speciï¬c dates, notational styles, and librettists. It seems\nrisky, therefore, to draw a conclusion by contemplating only\na single type of evidence such as C-clefs.\nHowever, computational analysis can offer a totally ob-\njective and independent result, which can then be combined\nwith other sources and knowledge such as the evidence men-\ntioned above, which will hopefully lead to more reliable re-\nsults. Can computational analysis offer the same conclu-\nsions as those arrived at by musicologists? The remainder\nof the paper is focused on this question by addressing the\ncomputational analysis of C-clefs.\n2. IMAGE PROCESSING OF BACHâ€™S\nMANUSCRIPTS\nThe extraction of C-clefs from the manuscripts requires ac-\ncurate segmentation. However, the segmentation of old hand-\nwritten manuscripts proves to be a difï¬cult task [6,17]. The\nmain difï¬culty seems to be caused by degradation such as\nshow-through and bleed-through effects.\n417Poster Session 3\n(a) 1708\n (b) 1723\n (c) 1733\n (d) 1736\n (e) 1739\n (f) 1748\nFigure 1 . The C-clefs of Bachâ€™s handwriting in the order of\nchronology suggested by musicologists.\nIn addition, microï¬che, the primary medium of the Bachâ€™s\nmanuscripts in the study, gives the images in low-resolution,\nwhich creates further problems for image processing.\n(a) Clear manuscript\n (b) Obtained image by adaptive binariza-\ntion and staff-line removal from (a)\n(c) Unclear and degraded manuscript\n (d) Obtained image by adaptive binariza-\ntion and staff-line removal from (c)\nFigure 2 . Two results of staff-line removal; almost all the\nstaff-lines are left in (d).\nAs text line localization is the essential part of the OCR\nprocess [12], staff line detection is one of the most difï¬-\ncult but important aspects of OMR, since staff lines, which\nare used to give meaning to certain symbols such as note-\nheads, prevent the segmentation of musical symbols. Al-\nthough there are arguments about the necessity of the staff-\nline removal, most research regards it as essential. The vol-\nume of research dealing with staff-line detection and re-\nmoval [5, 7, 8, 15, 16, 19] indicates the difï¬culty inherent in\nthis process, especially in the case of handwritten music.\n(a)\n (b)\n (c)\n (d)\n (e)\nFigure 3 . C-clefs cropped by the proposed method and pre-\npared for feature extraction: (a)original clef; (b)binarization\nusing Niblackâ€™s method; (c)line removal using Dalitzâ€™s\nmethod; (d)(e)other examples including irrelevant pixels.\nWe experimented with several staff-line removal meth-\nods implemented in Gamera2, and we found Dalitzâ€™s method\n[7] effective although it has sometimes failed to ï¬nd staff-\nlines, probably because it is sensitive to deformation. This\nhappened especially when the staff-lines were curved or sig-\nniï¬cantly thinner than usual. Figure 2 shows typical results\nof the staff-line removal. In addition to the difï¬culty inher-\nent in the staff-line removal from the manuscripts, Bachâ€™s\ndense notation and the irrelevant pixels, which are most com-\nmonly resulted from the degradation of paper, cause touch-\ning symbols. Moreover, unclear and degraded manuscripts\nare often fragmented by binarization process. These prob-\nlems make it difï¬cult to automatically decide bounding box\nof each musical symbol.\nAs it still requires further work to resolve these difï¬cul-\nties, for the present study we decided to collect C-clefs by\nmanually deciding the bounding box. Figure 3 shows the\nC-clef extracted by this method. This extraction is followed\nby both morphological operation and staff-line removal to\nprocure clear image, in order to prepare for the feature ex-\ntraction. This is shown in Figure 3(b) and (c).\n3. EXPERIMENT\nThis section explores the classiï¬cation of the C-clefs. Be-\ncause there is a controversy among Bach scholars regard-\ning both the authorship and chronology of C-clef forms, we\n2Gamera is a toolkit for building document image recognition systems\nand cross platform library for the Python programming language. It pro-\nvides a set of commonly needed functionality for document image analysis\nand allows for custom extensions as C++ or Python plugins and as toolkits.\nSee http://gamera.informatik.hsnr.de/index.html for more detail.\n41812th International Society for Music Information Retrieval Conference (ISMIR 2011)\nhave carefully selected the sample dataset from an undis-\nputed portion of Bachâ€™s fair copies that date between 1708\nand 1748. The detailed information of this is shown in Ta-\nble 1.3We prepared two classiï¬cation tasks using the same\ndataset: one is eight-class classiï¬cation using the date pro-\nposed by Kobayashi as the label; the other is two-class clas-\nsiï¬cation which only distinguishes between A B C and D E\nF G H. This corresponds to determining if a certain clef was\nwritten before Bach arrived at Leipzig (i.e. May 1723) to\nassume his role as Thomas cantor as well as the director of\nmusic for the town, or after that date.\nFeature selection is also an important factor for success-\nful classiï¬cation. For the present study, 15 features imple-\nmented in Gamera were used. Each feature is explained as\nfollows4\nâ€¢area\nThe area of the bounding box.\nâ€¢aspect ratio\nThe aspect ratio of the bounding box.\nâ€¢black area\nThe number of black pixels.\nâ€¢compactness\nThe volume to surface ratio.\nâ€¢moments\nThe centre of gravity on xandyaxis normalized by\nwidth and height.\nâ€¢ncols feature\nThe number of columns.\nâ€¢nholes\nThe averaged number of white runs not touching the\nborder. This is computed both for each row and each\ncolumn.\nâ€¢nholes extended\nDivides the image into four strips and then does a\nnholes analysis on each of those strips. This is ï¬rst\ndone vertically and then horizontally, resulting in a\ntotal of eight feature values.\nâ€¢nrows feature\nThe number of rows.\n3See [13] and [14] for detailed discussion on the chronological issue of\nJ.S.Bachâ€™s work.\n4See http://gamera.sourceforge.net/doc/html/features.html#features for\ndetailed information of the features.â€¢skelton features\nGenerates a number of features based on the skeleton\nof an image.\nâ€¢top bottom\nThe ï¬rst feature is the ï¬rst row containing a black\npixel, and the second feature is the last row containing\na black pixel.\nâ€¢volume\nThe percentage of black pixels within the rectangular\nbounding box of the image.\nâ€¢volume16regions\nDivides the image into a 4 x 4 grid of 16 regions and\ncalculates the volume within each.\nâ€¢volume64regions\nDivides the image into a 8 x 8 grid of 64 regions and\ncalculates the volume within each.\nâ€¢zenrike moments\nComputes the absolute values of the normalized zernike\nmoments [18] up to order six.\nIn the experiment, the performance of random forest (RF),\nwhich worked the best in the preliminary experiment, was\ninvestigated using 10-fold cross-validation compared with\nother methods: support vector machine (SVM), bagging,\nand boosting5. RBF kernel was used as the kernel func-\ntion of SVM, and this was automatically estimated from the\nresult of a preliminary experiment. CART Algorithm was\nused as underlying classiï¬ers for all the ensemble classiï¬ers\nand the other parameters were set as default.\nTable 2 shows the result of 10-fold cross-validation. The\nbest accuracy for two-class classiï¬cation was 89.95% ob-\ntained by random forest. This accuracy seems signiï¬cant\nconsidering that the classiï¬cation of Bachâ€™s handwriting has\nbeen attempted by only a few experts. The eight-class clas-\nsiï¬cation is far more complicated and thus extremely dif-\nï¬cult even for human experts. The best accuracy 73.82%\nwas achieved by RF for eight-class classiï¬cation, which is\nin itself remarkable. While there is much room for improve-\nment, these classiï¬cations may serve as a rough barometer\nfor musicologists.\nTables 3 and 4 indicate the confusion matrix for the two-\nclass and the eight-class classiï¬cation. Note that this confu-\nsion matrix is the result of the classiï¬cation using out of bag\ndata, and its error rate tends to be higher than that of cross\nvalidation. In Table 3 , misclassiï¬cation of B(during and\n5See [1, 2, 11, 20] for detailed explanation of each classiï¬er used in the\nexperiment. We used the implementation included in R package for all the\nclassiï¬ers. See http://www.r-project.org/ for more information about R.\n419Poster Session 3\nTable 1 . Data set used for the experiment.\nID Name of the piece BWV Name of the source Estimated date Number of the clefs extracted\nA Cantata â€œGott ist mein K Â¨onigâ€ BWV71 D-B, Mus. Ms. Bach P 45 1708 89\nB Alles mit Gott und nichts ohnâ€™ Ihn BWV1127 D-W, Ra B 24 1713 11\nC Inventions and Sinfonias BWV772â€“801 D-B, Mus.ms. Bach P 610 1723 188\nD Sanctus BWV232/III D-B, Mus. ms. Bach P 13 1724 77\nE Magniï¬cat BWV243 D-B, Mus.ms. Bach P 39 1733 221\nF St Matthew Passion BWV244 D-B, Mus.ms. Bach P 25 1736 633\nG Well-Tempered Clavier II, No. 10, 19, and 24 BWV879, 888, and 893 GB-Lbl, Add.MS. 35021 1739 69\nH Canonic Variations on V om Himmel hoch BWV769 D-B, Mus.ms. Bach P 271 1748 22\nTable 2 . Classiï¬cation accuracy evaluated by 10-fold cross\nvalidation for the best four classiï¬ers\nTwo-class Eight-class\nRandom forest 89.95% 73.82%\nSVM 85.25% 72.36%\nBagging 88.35% 73.09%\nBoosting 86.20% 60.35%\nTable 3 . Confusion matrix for two-class classiï¬cation\nA B class.error\nA 189 99 0 .34\nB 33 989 0 .032\nafter Leipzig) to A(before Leipzig) is limited . The misclas-\nsiï¬cation of A to B seems to be caused by a small sample\nsize of A compared to B. In Table 4, the misclassiï¬cation\nof classes such as A D and H into F is noticeable. This\nresult implies that classes with a low sample size tends to\nbe classiï¬ed as the class with a large sample size such as\nF. It is interesting that class G is classiï¬ed with fairly high\naccuracy although it has small sample size. This is proba-\nbly because the deviation of the shapes of the C-clefs in G\nis small enough to achieve the high classiï¬cation accuracy\neven with small sample data.\n4. CONCLUSION AND FUTURE WORK\nIn this study, we proposed a new method to work out the\nchronology of music manuscripts by classifying the shape of\nC-clefs. Applying a battery of techniques to Bachâ€™s manuscripts\nrevealed the limitation of the current image processing tech-\nniques. The method of classifying C-clefs using 15 features\nand RF produced a result of 89.95% accuracy in two-class\nclassiï¬cation and 73.82% in eight-class classiï¬cation.\nThe automatic collection of musical symbols from Bachâ€™s\nmanuscripts proves to be a challenging task, but is worth in-\nvestigating further. The accuracy of C-clef classiï¬cation canbe improved by investigating the implementation of musi-\ncal knowledge, and this should be explored in collaboration\nwith musicologists. In this study, we assumed that all the\nclefs from the same page were written in the same period.\nHowever, there is deviation in shape even in the clefs on the\nsame page and sometimes they looked as if they were added\nsubsequently or even possibly by a different hand. For this\nlevel of analysis, it requires more sophisticated image pro-\ncessing techniques that are capable of handling more subtle\nchanges in each music symbol.\nChronological identiï¬cation is not a straightforward task.\nIn contrast to OMR system, musicologists would take a com-\nplex approach, combining it with chronological, composi-\ntional, and notational information, placing them against the\nhistorical background of the source such as the situation un-\nder which the initial copying and revisions took place, the\ndiplomatic polices that might reveal the purpose for which\nthe score was made, and so on, to verify the initial hypoth-\nesis. It is hoped that quantiï¬cation and statistical analyses\nsuch as what demonstrated in this paper will be perfected in\nfuture research, and that they are adopted by future musicol-\nogists to discover many more exciting facts hidden deep in\nthe beautiful manuscripts of Johann Sebastian Bach.\n5. ACKNOWLEDGEMENTS\nThis work is ï¬nancially supported by the Nakajima founda-\ntion.\n6. REFERENCES\n[1] Leo Breiman. Bagging predictors. Machine Learning ,\n24:123â€“140, 1996.\n[2] Leo Breiman. Random forests. Machine Learning ,\n45:5â€“32, 2001.\n[3] I. Bruder. Integrating knowledge components for writer\nidentiï¬cation in a digital archive of historical music\nscores. Proceedings of the Fourth ACM/IEEE Joint Con-\nference on Digital Libraries , page 397, 2004.\n42012th International Society for Music Information Retrieval Conference (ISMIR 2011)\nTable 4 . Confusion matrix for eight-class classiï¬cation\nA B C D E F G H class.error\nA 26 0 3 2 3 55 0 0 0 .71\nB 0 7 1 0 0 3 0 0 0 .36\nC 0 0 160 1 17 5 5 0 0 .15\nD 2 0 7 8 21 39 0 0 0 .90\nE 0 0 18 1 146 56 0 0 0 .34\nF 17 0 4 5 35 572 0 0 0 .096\nG 0 0 2 0 0 0 67 0 0 .029\nH 0 0 0 0 0 22 0 0 1 .0\n[4] M. Bulacu and L. Schomaker. Text-independent writer\nidentiï¬cation and veriï¬cation using textural and allo-\ngraphic features. IEEE Transactions on Pattern Analysis\nand Machine Intelligence , 29(4):701â€“717, 2007.\n[5] J. S. Cardoso, A. Capela, A. Rebelo, and C. Guedes. A\nconnected path approach for staff detection on a music\nscore. Proceedings of the 15th IEEE International Con-\nference on Image Processing , pages 1005â€“1008, 2008.\n[6] N. P. Carter. Segmentation and preliminary recognition\nof madrigals notated in white mensural notation. Ma-\nchine Vision and Applications , 5(3):223â€“229, 1992.\n[7] C. Dalitz, M. Droettboom, B. Pranzas, and I. Fujinaga.\nA comparative study of staff removal algorithms. IEEE\nTransactions on Pattern Analysis and Machine Intelli-\ngence , 30(5):753â€“766, 2008.\n[8] Santos Cardoso dos, A. Capela, A. Rebelo, C. Guedes,\nand da Costa Pinto. Staff detection with stable paths.\nIEEE Transactions on Pattern Analysis and Machine In-\ntelligence , 31(6):1134â€“1139, 2009.\n[9] Walter Emery. The London autograph of the forty-eight.\nMusic and Letters , 34(2):106â€“123, 1953.\n[10] A. Fornes, J. Llados, G. Sanchez, and H. Bunke. Writer\nidentiï¬cation in old handwritten music scores. Proceed-\nings of the Eighth IAPR International Workshop on Doc-\nument Analysis Systems , pages 347â€“353, 2008.\n[11] Yoav Freund and Robert E. Schapire. Experiments with\na new boosting algorithm. Proc. of the Thirteenth Inter-\nnational Conference on Machine Learning , pages 148â€“\n165, 1996.\n[12] M. Y . Jaisimha. Model based restoration of document\nimages for OCR. Proceedings of the SPIE - The Interna-\ntional Society for Optical Engineering , 2660:297â€“308,\n1996.[13] Yoshitake Kobayashi. Zur Chronologie der Sp Â¨atwerke\nJohann Sebastian Bachs. Kompositions- und\nAuff Â¨uhrungst Â¨atigkeit von 1736 bis 1750. Bach-\nJahrbuch , 74:7â€“72, 1988.\n[14] Yoshitake Kobayashi. Die Kopisten Johann Sebas-\ntian Bachs: Katalog und Dokumentation . Kassel:\nBÂ¨arenreiter, 2007.\n[15] H. Miyao. Stave extraction for printed music scores. In-\ntelligent Data Engineering and Automated Learning -\nIDEAL 2002. Third International Conference (Lecture\nNotes in Computer Science Vol.2412) , pages 562â€“568,\n2002.\n[16] A. Rebelo, A. Capela, J. F. P. da Costa, C. Guedes,\nE. Carrapatoso, and J. S. Cardoso. A shortest path\napproach for staff line detection. Proceedings of\n2007.AXMEDIS â€™07.Third International Conference on\nAutomated Production of Cross Media Content for\nMulti-Channel Distribution , pages 79â€“85, 2007.\n[17] Florence Rossant. Robust and adaptive omr system in-\ncluding fuzzy modeling, fusion of musical rules, and\npossible error detection. EURASIP Journal on Applied\nSignal Processing , 2007(1), 2007.\n[18] T. Obeidi S. Belkasim, E. Hassan. Explicit invariance of\ncartesian zernike moments. Pattern Recognition Letters ,\n28:1969â€“1980, 2007.\n[19] Mariusz Szwoch. A robust detector for distorted mu-\nsic staves. Proceedings of Computer Analysis of Im-\nages and Patterns.11th International Conference, CAIP\n2005. , pages 701â€“708, 2005.\n[20] Vladimir Naumovich Vapnik. The Nature of Statistical\nLearning Theory . New York: Springer, 1995.\n[21] Georg von Dadelsen. Beitr Â¨age zur Chronologie der\nWerke Johann Sebastian Bachs . TÂ¨ubinger Bach-Studien,\nvolume 4-5. Trossingen: Hohner, 1958.\n421"
    },
    {
        "title": "Stochastic Modeling of a Musical Performance with Expressive Representations from the Musical Score.",
        "author": [
            "Kenta Okumura",
            "Shinji Sako",
            "Tadashi Kitamura"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1417093",
        "url": "https://doi.org/10.5281/zenodo.1417093",
        "ee": "https://zenodo.org/records/1417093/files/OkumuraSK11.pdf",
        "abstract": "This paper presents a method for describing the characteristics of human musical performance. We consider the problem of building models that express the ways in which deviations from a strict interpretations of the score occurs in the performance, and that cluster these deviations automatically. The clustering process is performed using expressive representations unambiguously notated on the musical score, without any arbitrariness by the human observer. The result of clustering is obtained as hierarchical tree structures for each deviational factor that occurred during the operation of the instrument. This structure represents an approximation of the performerâ€™s interpretation with information notated on the score they used during the performance. This model represents the conditions that generate the difference in the fluctuation of performance expression and the amounts of deviational factors directly from the data of real performance. Through validations of applying the method to the data measured from real performances, we show that the use of information regarding expressive representation on the musical score enables the efficient estimation of generative-model for the musical performance.",
        "zenodo_id": 1417093,
        "dblp_key": "conf/ismir/OkumuraSK11",
        "keywords": [
            "musical performance",
            "deviations from score",
            "automatic clustering",
            "expressive representations",
            "hierarchical tree structures",
            "interpretation approximation",
            "fluctuation of performance",
            "generative-model estimation",
            "real performances",
            "information regarding expressive representation"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nSTOCHASTIC MODELING OF A MUSICAL PERFORMANCE WITH\nEXPRESSIVE REPRESENTATIONS FROM THE MUSICAL SCORE\nKenta Okumura, Shinji Sako and Tadashi Kitamura\nNagoya Institute of Technology, Japan\n{k09,sako,kitamura }@mmsp.nitech.ac.jp\nABSTRACT\nThis paper presents a method for describing the characteris-\ntics of human musical performance. We consider the prob-\nlem of building models that express the ways in which devi-\nations from a strict interpretations of the score occurs in the\nperformance, and that cluster these deviations automatically.\nThe clustering process is performed using expressive rep-\nresentations unambiguously notated on the musical score,\nwithout any arbitrariness by the human observer. The result\nof clustering is obtained as hierarchical tree structures for\neach deviational factor that occurred during the operation of\nthe instrument. This structure represents an approximation\nof the performerâ€™s interpretation with information notated\non the score they used during the performance.\nThis model represents the conditions that generate the\ndifference in the ï¬‚uctuation of performance expression and\nthe amounts of deviational factors directly from the data\nof real performance. Through validations of applying the\nmethod to the data measured from real performances, we\nshow that the use of information regarding expressive repre-\nsentation on the musical score enables the efï¬cient estima-\ntion of generative-model for the musical performance.\n1. INTRODUCTION\nThe idea of having a computer perform like human musician\narose more than two decades ago. There have been various\nproposals for making a computer understand the rich expres-\nsion of a performance [2]. Historically, the mainstream ap-\nproach to capturing the nuances of performance has changed\nfrom rule-based methods to learning-based methods. One\nmodel that shows the effectiveness of the latter approach\nis represented by the generative model. Also, there is an-\nother motivation for this kind of research, that is, learning\nwhat makes a performance humanlike; however, there are\nfew initiatives based on such questions. One approach to\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proï¬t or commercial advantage and that copies\nbear this notice and the full citation on the ï¬rst page.\nc/circlecopyrt2011 International Society for Music Information Retrieval.analyze performance statistically, by capturing the trends of\nthe performance in the acoustic features, has already been\nattempted [3, 8, 10, 11]. These studies are admirable in that\ntheir veriï¬cation used a large quantity of expressive perfor-\nmance; we also essentially agree that it is desirable to per-\nform the veriï¬cation with such an approach. However, it\nis difï¬cult to observe the expressiveness of a performance\nfrom diverse perspectives by these approaches as expres-\nsiveness consists of various factors. We adopt a MIDI-based\napproach to simplify such problems, and consider a variety\nof expressive representations notated on the musical score as\nthe factor that describes how the expressive performance has\nbeen generated. In addition, our method to capture the per-\nformance is based on the idea of a generative model. There-\nfore, our method has the potential to generate an unseen per-\nformance, not merely to analyze an already known one.\nIn the following sections, we propose a method for the\nautomatic analysis of the characteristics of a performance\nbased on various combinations of expressive representations.\nAlso, we observe what kinds of representation constitute the\nhuman quality of the performance by apply them to the data\nmeasured from the real performance to evaluate the validity\nof this method.\n2. METHOD\nIn this section, we propose a method for the automatic clas-\nsiï¬cation of trends of the deviations in performance, so as to\ndescribe the dependencies between score and performance.\nOn the keyboard instrument, a performerâ€™s key operation,\nin terms of timing and intensity, causes deviations from the\nscore for the purpose of artistic expression. We believe that\nthe performerâ€™s individuality would occur in the differences\nin the trend of deviations. The occurrence tendencies of\nthese deviations in the performance are not constant, as they\nare affected by various factors such as the differences in mu-\nsical compositions. To capture the characteristics of indi-\nviduals who performed only in terms of deviation from the\naverage trend in the overall performance is difï¬cult; there-\nfore, it is necessary to handle deviations in each key action,\nspeciï¬cally and in general. Using this awareness, we have\nbeen studying a method that regards the trends in the de-\nviation as a stochastic model and acquire these trends via\nlearning and instructions on the score.\n531Poster Session 4\nartistic-deviations \nextract amount of\ndeviations for each \ndeviational factor score\nreferred by \nthe performer samples \nmeasured from \nthe performance \n-1/2 -1/4 just 1/4 1/2onset\nbeat-1/2 -1/4 just 1/4 1/2oï¬€set\nbeat\n0 0.5 1 1.5 2gate time \nra/g415o\n0 0.5 1 1.5 2dynamics \nra/g415o0 0.5 1 1.5 2tempo\nra/g415ofrequency Figure 1 . Extraction of deviational factors\nscore \n(MusicXML) melody, \nc, \n16thNote,\nmezzoForte, melody, \nf#,\n16thNote,\nmezzoForte, polyphony,\nb, \n8thNote,\nmezzoForte, polyphony,\na, \n8thNote,\nmezzoForte, Å‹\u0001Å‹\u0001Å‹\u0001etc. Å‹\u0001Å‹\u0001Å‹\nÅ‹\u0001Å‹\u0001Å‹\u0001Å‹\u0001Å‹\u0001Å‹\u0001etc. \nextract values for each \ncontextual factor and add \nthem to deviational factors for \neach note as a label to build \ncontext-dependent models Figure 2 . Extraction of contextual factors\n2.1 Context-dependent model\nIf the performance seems to be personalized, it is considered\nthat the resultant personality is caused by biases in the trends\nof performance. The trend of deviation is observed as a dis-\ntribution with some focus, according to deviations for each\nnote extracted from each note oobserved from the measured\nperformance and the corresponding score (see Figure 1). We\ncan think of the model as a Gaussian probability density\nfunction (PDF) so as to approximate the behavior of devi-\nations; this model is able to cope with complex behaviors\naccording to the Gaussian mixture model (GMM) approach.\nThe PDF Nof the observation vector ois deï¬ned by\nN(om|Âµm,Ïƒm)\n=1âˆš\n(2Ï€)DÎ D\nd=1|Ïƒmd|exp(\nâˆ’1\n2Dâˆ‘\nd=1(odâˆ’Âµmd)2\nÏƒmd)\n,\n(1)\nwhere ois observed with Ddeviational factors, odis the\ndth dimension for observation vector o,mis the mixture in-\ndex of the MGaussian component densities, Âµis the mean\nvector, and Ïƒis the diagonal covariance matrix.\nHowever, the cause of the deviating behavior is not con-\nsidered in this model. The performance of musical instru-\nments consists of playing the sequences of notes according\nto the score. Therefore, it is obvious that the qualities of\neach note have some musical signiï¬cance. As a general ex-\nample, we consider performing two notes with different rep-\nresentations in terms of dynamics. In this case, the amount\nof deviation between them may be differ not only in the dy-\nnamics, but also in the timing, because of their expressive\nrepresentations. Also, the extent to which the performer de-\nviates from the average for the note with the representation\nis considered to be under the inï¬‚uence of some individual-\nity. In the past, there were several studies that attempted to\nestimate the performersâ€™ characteristics by referring to the\namount of deviation in timing and dynamics [5â€“7]. How-\never, it is also necessary to consider what kind of representa-\ntion leads to such behavior, using some musical knowledge\nthat supersedes the mixture in the GMM.\nSeveral factors complicate the process of occurrence. We\nmake the following considerations to organize this subject:â€¢The performer obtains information from the musical\nscore, and then creates his/her own interpretation us-\ning that information, thus introducing deviations into\nthe performance.\nâ€¢The trend of deviations occurring is also inï¬‚uenced by\nunintentional factors such as the performerâ€™s physical\nlimitations.\nWe believe that the latter factor is not necessary, because it is\nconsidered likely based on relatively simple arguments, and\nthe progress of performance technology is a means to reduce\nthe interference of factors, such as unintentional representa-\ntions. Additionally, factors (such as the former) inï¬‚uence\nthe occurrence of this deviation, which is considered signif-\nicant because it is intended to expand the range of expres-\nsion in accordance with technological progress. However,\ncriteria tend to be abstract and difï¬cult to qualify, even for\nthe performers themselves. Therefore, we do not directly\naddress the interpretation of the music itself. Instead, we\nassociate the trends in the deviation with the expressive rep-\nresentations, which affects the performerâ€™s musical interpre-\ntation.\nAll the information used here is in the form of unambigu-\nous values that are available in the score, such as pitch, note\nvalue, dynamics, and so on, because we want to eliminate\nany undeï¬ned properties throughout the process. There is\nalso the musical phrase to consider, which has some rela-\ntionship that holds among surrounding notes. We introduce\nthem under the term â€context.â€ Models in which context is\napplied are called â€context-dependent,â€ because they con-\nstruct a kind of context that contributes to the interpretation.\nThe parameters of the model are the same as the model men-\ntioned above; however, each model has its own combination\nof contexts that is dealt with individually (see Figure 2). The\ndescription of the behavior for each model can be simpliï¬ed\nbecause it is deï¬ned by a number of combinations. There-\nfore, each model is trained using a single Gaussian compo-\nnent density, as shown in Equation (1) .\n2.2 Tree-based clustering\nThe purpose of introducing context is to associate a per-\nformerâ€™s interpretation of the musical composition with the\ndeviations in the performance. A more detailed representa-\ntion of the information obtained from the score has to con-\n53212th International Society for Music Information Retrieval Conference (ISMIR 2011)\nleaf Sâ‚ leaf Sâ‚‚ leaf Sâ‚ƒ leaf Sâ‚„ leaf Sâ‚… leaf Sâ‚† leaf Sâ‚‡ leaf Sâ‚ˆyes no \nyes no yes no \nyes no yes no yes no yes no Is part of \ncurrent note\nmelody? \nIs notation of \ncurrent note\nslur?Is octave of \ncurrent note\nlower than 4th?\nIs syllable of \ncurrent note\nhigher than V?Is local pos. of \npreceding note \nlater than 50%?Is type of \nsucceeding note \ndotted quarter? Is beam of \nsucceeding note \ncontinue?Sâ‚€\nSq+ Sq-qapply questions about \ncontext to the whole \ncontext-dependent \nmodels\nshare parameters in each leaf-node as model U, M=8Figure 3 . Example of a decision tree\nsider a variety of contexts. However, with increasing use of\ncontexts, the quantity of combinations of contexts increases\nexponentially. This effect is detrimental to model training,\nbecause the training data for each model will be signiï¬cantly\nreduced. On the other hand, fragmented information has lit-\ntle meaning by itself. Therefore, it is necessary to classify\na large number of combinations of contexts at a scale that\nmatches the performerâ€™s signiï¬cant interpretation. How-\never, it is beyond human power to decide appropriate cri-\nteria for each case of classiï¬cation. To address these issues,\na method is necessary to reconstruct and decompose models\nefï¬ciently, and to capture the varied expressive representa-\ntions obtained from the score. We use tree-based cluster-\ning [4] to classify the context-dependent models.\nTree-based clustering divides all possible combinations\nof context-dependent model into a countable number of clus-\nters. As a result, a decision tree (a binary tree in which\na question is attached to each node) is obtained. In this\nmethod, each of the questions relates to the contextual fac-\ntors for the preceding, current, and succeeding note. One\ntree is constructed for each deviational factor so as to cluster\nall of the corresponding behaviors of all context-dependent\nmodels. This is done because there are different trends of\nbehavior for each deviational factor. All context-dependent\nmodels in the decision tree are divided into Mnodes by\nclusters S1,Â· Â· Â·, SM, such that one model U(S1,Â· Â· Â·, SM)\nis deï¬ned for each leaf node. For example, the tree shown in\nFigure 3 will partition its behaviors into eight subsets with\nthe same number of leaf nodes. The questions and topology\nof the tree are chosen so as to maximize the likelihood of the\ntraining data, given these tied behaviors, by estimating the\nparameters of a Gaussian PDF. Once these trees have been\nconstructed, data with unseen contexts can be classiï¬ed in\nany leaf node by tracing the questions in the tree.\nInitially, all the context-dependent models to be clustered\nare placed at the root node of the tree. The log likelihood\nof the training data is calculated, supposing that all of the\nmodels in that node are tied. Then, this node is divided into\ntwo by ï¬nding a question that divides the model in the par-\nent node such that the log likelihood (maximally) increases.The log likelihood Lfor node Smis given by\nL(Sm) =âˆ’1\n2Î“m(K+Klog(2Ï€)Llog|Î£m|),(2)\nwhere Î“mis the amount of data for training at node Sm.\nThis process is then repeated by dividing the node in a way\nthat creates the maximum increase of log likelihood until\nthe minimum description length (MDL) criterion [9] is met.\nThis step is carried out to optimize the number of clusters\nwithout using external control parameters. In order to opti-\nmize the size of the tree, we use an algorithm with a prag-\nmatic cost of computation. Here, let us assume that node\nSmof model Udivides into two nodes, Smq+andSmqâˆ’,\nby answering question q. Then, let âˆ†m(q)be the difference\nbetween the description length after division and before di-\nvision, that is l(U/prime)âˆ’l(U). The description length of model\nU/primeis represented by the following equation:\nI(U/prime) =Mâˆ‘\nm/prime=1,/negationslash=m1\n2Î“m/prime(K+Klog(2Ï€) + log |Î£m/prime|)\n+1\n2Î“mq+(K+Klog(2Ï€) + log |Î£mq+|)\n+1\n2Î“mqâˆ’(K+Klog(2Ï€) + log |Î£mqâˆ’|)\n+K(M+ 1) log W+C, (3)\nwhere W=âˆ‘M\nm=1Î“m, and Cis the length of code re-\nquired to choose a model (assumed here to be a constant\nvalue). The number of nodes in U/primeisM+ 1,Î“mq+is the\noccupancy count for node Smq+, and Î“mqâˆ’is that of node\nSmqâˆ’. The difference âˆ†m(q)is given by\nâˆ†m(q) =l(U/prime)âˆ’l(U)\n=1\n2(Î“mq+log|Î£mq+|+ Î“mqâˆ’log|Î£mqâˆ’|\nâˆ’Î“mlog|Î£m|) +KlogMâˆ‘\nm=1Î“m.(4)\nWhen dividing models, we ï¬rst determine the question\nq/primethat minimizes âˆ†0q/primeand that is used at root node S0.\nIfâˆ†0(q/prime)<0, node S0is divided into two nodes, Sq+\nandSqâˆ’, and the same procedure is repeated for each of\nthese two nodes. This process of dividing nodes is car-\nried out until there are no nodes remaining to be divided.\nIfâˆ†0(q/prime)>0, then no dividing is executed.\n3. EXPERIMENTS\nIn this section, we apply the method mentioned above to\nthe real-measured performance data to verify its efï¬cacy of\nusing expressive representations from the musical score as\npriori information. This information is applied to the issue\nof classifying the trends of the deviational behavior during\nthe musical performance.\n533Poster Session 4\n3.1 Data of real-measured expressive performance\nExperiments in this paper use expressive performance data\nfrom a database ( [1] and original data we collected). These\ncontain information of musical expression on expertsâ€™ ex-\npressive piano solo performances of classical Western mu-\nsical compositions. The data of performance used in the\nexperiments are as follows:\nâ€¢performers\nPAV. D. Ashkenazy\nPGG. H. Gould\nPPM. J. Pires\nPRS. T. Richter\nPXFive anonymous semi-professional performers\nâ€¢referred scores\nSBI J. S. Bach: â€Two part Inventions BWV 772â€“\n786,â€ Henle Verlag, pp. 2â€“31.\nSBW J. S. Bach: â€The Well-Tempered Clavier\nBWV 846,â€ Wiener Urtext Edition, pp. 2â€“3.\nSCN F. F. Chopin: â€Nocturne No. 10,â€\nPaderewski Edition, pp. 54â€“55.\nSM3 W. A. Mozart: â€Sonata K. 331, the First move-\nment,â€ Wiener Urtext Edition, pp. 18â€“27.\nSM5 W. A. Mozart: â€Sonata K. 545, the First move-\nment,â€ Henle Verlag, pp. 266â€“269.\nThe actual performances also include notes do not corre-\nspond to the score. The current form of our method excludes\nthese notes from the data used to train the model.\n3.2 Design of models\nThe values of deviations and contexts are extracted by com-\nparing the performance and the score, as shown in Figure 1\nand Figure 2. The ï¬ve factors in which there could be devi-\nation (shown below) are extracted for each note; therefore,\nthe dimensionality D= 5in Equation (1).\nâ€¢Factors that depend on the note:\nonset Timing when striking the key. The amount of\ndeviation is represented relative to a beat. If the\nperformed note is struck one half beat faster, the\ndeviation of onset is âˆ’0.5.\noffset Timing when releasing the key, represented in\nthe same way as the deviation of onset.\ngate time The quotient of the time taken to depress\nthe key in the performance divided by its length\non the score. If both are exactly the same, the\ndeviation of gate time is 1.\ndynamics Strength when striking the key, obtained\nin the same way as the deviation of gate time.\nâ€¢Factor that depends on the beat:\ntempo Temporal change of BPM (current beat/average).\nThe contextual factors attached to context-dependent model\nare shown below. They are used for question to construct\ndecision trees. In this experiment, the total number of ques-\ntions used amounted to more than two thousands.â€¢Extracted for {preceding, current, succeeding }notes:\nsyllable Interval name of the note and the tonic, i.e.,\nminor third, perfect ï¬fth, etc.\nstep One of the twelve note names, from C to B.\naccidental Existence and type of accidental.\noctave Rough pitch of the note.\nchord Whether the note belongs to any chord.\ntype Note value of the note.\nstaff Clef and stage on the great staff the note is writ-\nten in.\nbeam Type of the noteâ€™s beams, i.e., begin, continue,\nend, etc.\nlocal The noteâ€™s position on the beat in the bar, rep-\nresented as a percentage.\nâ€¢Extracted for current note only:\nglobal The noteâ€™s position in elapsed time in the mu-\nsical composition, represented as a percentage.\nvoice Voice part of the note, deï¬ned by the author of\nthe database.\nnotations Noted signs for the note, such as dynam-\nics, intonation, etc.\n3.3 Efï¬cacy of tree-based clustering\nThe tree-based clustering itself is an existing method; how-\never, the effect of applying this method to a musical per-\nformance is unknown. Therefore, it is necessary to deter-\nmine whether changes in generative efï¬ciency can be seen\nin the bottom-up clustered model without additional infor-\nmation. To achieve concrete results, we tried to identify the\nperformer from the performance data using the models. The\ndata sets used in this case were SBI and SM3, both of which\nwere performed by PX. The models were trained with the\ndata of the compositions, which amounted to approximately\none quarter of the data set. The tests used each datum of the\nremaining compositions in the same set; the percentage of\nthe right choices for the performer by the trained model was\ncalculated (called the rate of identiï¬cation). Evaluation of\nresistance to the unseen data was also carried out using this\ntest, as all models were tested with data that is not used to\ntrain the models. We differentiate these methods:\nTree-based clustering The model using the proposed method.\nBottom-up clustering The model trained by GMM with\nthe same number of mixtures Mas the leaves in the\ntrees generated by tree-based clustering, and using the\nsame data set that is used to train the models.\nThe result is shown in Figure 4, and the ratio of accu-\nracy to the average of 20 ordinary human listeners for each\nmethod is also indicated in parentheses. This is a severe con-\ndition, and the most human listeners cannot tell the differ-\nence. However, proposed method can determine such subtle\ndifference with high precision, because the ratio of Tree-\nbased is about 232% for human listeners. Furthermore, the\nratio of Tree-based forBottom-up is about 111%. There-\nfore, it is conï¬rmed that the accuracy can be improved upon\nto generate models that can respond to unseen data by using\nthe clustering with the information from the score.\n53412th International Society for Music Information Retrieval Conference (ISMIR 2011)\n9.399e-02\n2.693e-01 2.668e-025.980e-01 -6.722e-02Is type of currentNote\n shorter than doubleDottedEighth?\nIs voice of currentNote\n 5?yes\nIs octave of succeedingNote\n higher than 1?no\nno\n150\nIs beam of succeedingNote\n begin?yes\nno\n62yes\n24no\n4yes\n74(b)offset for SCN by PA\n1.430e+00 4.232e-01\n1.325e+00 2.568e+00 1.302e+00 1.005e+00Is voice of currentNote\n 5?\nIs octave of succeedingNote\n higher than 3?yes\nIs type of currentNote\n shorter than Whole?no\nno\n60\nIs global of currentNote\n earlier than 3%?yesno\n36\nIs type of currentNote\n longer than Eighth?yes\nno\n13yes\n14no\n144yes\n47 (c)gate time for SCN by PA\n-5.183e-02\n1.390e-03\n5.174e-03 -3.092e-02Is type of currentNote\n shorter than doubleDottedQuarter?\nno\n43\nIs octave of currentNote\n 3?yes\nno\n166\nIs beam of currentNote\n end?yes\nno\n62yes\n43\n(a)onset for SCN by PA\n3.075e-01 2.669e-01 3.341e-01\n3.894e-01 4.521e-01Is staff of succeedingNote\n 1G2?\nIs voice of currentNote\n 4?no\nIs type of succeedingNote\n shorter than DottedQuarter?yes\nno\n90yes\n106no\n29\nIs octave of currentNote\n 5?yes\nno\n40yes\n49 (d)dynamics for SCN by PA\n1.053e+00\n8.427e-01\n9.314e-01 1.128e+00Is local of currentNote\n later than 20%?\nyes\n172\nIs local of succeedingNote\n later than 5%?no\nno\n64\nIs local of precedingNote\n later than 71%?yes\nno\n63yes\n15 (e)tempo for SCN by PA\nFigure 5 . Examples of structural and statistical differences in tree-structures for each deviational factor\n0 10 20 30 40 50 60 70 SBISM3Tree-based\nclustering\nBo/g425om-up \nclustering data set \nrate of iden/g415ï¬ca/g415on [%] random choice average rate of 20 human listeners (baseline) \n(2.48x) \n(2.28x) (2.16x) \n(1.92x) \nFigure 4 . Results of identiï¬cation test\n3.4 Observation of decision trees\nNext, we observe the decision trees obtained from the per-\nformance data to verify the kind of questions that divide the\nmodels and the statistical attributes of each model. The set\nof training data used here was SCN, performed by PA. Ex-\namples of the portion of the trees near the root are shown in\nFigure 5. Each node has the content of the question, each\nleaf gives the average deviation, and the number of models\ninvolved in each leaf is indicated by an arrow.\nThe trees of deviational factors belong to the timing ( on-\nset,offset , and gate time ) have afï¬nities in the kind of ques-\ntions. The tree of dynamics also has the sequence of ques-\ntions with the same contexts as the factors mentioned above;\nhowever, the kind of question on the root node is not seen.\nAlthough they have certain unique points, they have a simi-\nlar structure. On the other hand, the tree of tempo has very\ndifferent trends, both in terms of structure and questions.3.5 Contribution of contextual factors to decision trees\nDue to the limitations of the available data, a more efï¬cient\nanalysis is needed to understand the trends of these fac-\ntors. We therefore investigated the frequency of any ques-\ntion to ï¬nd the degree of contribution to the trend of de-\nviation caused by each contextual factor. The contribution\nCfor contextual factor Qin a tree with Mleaf nodes is\ncounted by\nCQ=Mâˆ‘\nm=1(Nm\nNallÃ—RQ)\n, (5)\nwhere Nmis the number of context-dependent models shared\nby the mth leaf node, and Ris the number of nodes related\ntoQin the path from the root node to the mth leaf node. The\ntraining data used here was SBW-by- {PG, and PR }, SCN-\nby-{PA, and PP }, and SM5-by- {PG, and PP }. The results\nfor each composition are shown in Figure 6; we propose that\nthese results show the priorities of performersâ€™ criterion to\ndifferentiate the behavior in the performance.\nThe trend of contextual factors that make a large contri-\nbution is the same in all compositions (e.g., step,octave ,\ntype,local , and syllable ). We consider the essential part of\nthe treesâ€™ construction to depend upon the selection order\nof these factors. On the other hand, the difference between\noffset and gate time is small, as mentioned above; however,\nthese result shows some differences (for example, they are\nfound in step,octave , and type). There is a possibility to\nreveal the diverging points of the deviations with expressive\nrepresentations by observing more detailed classiï¬cations.\n535Poster Session 4\n0510 15 20 25 30 35 40 45 onset oï¬€set gate /g415me dynamics tempo average\ncontextual factor rate of frequency [%] (a) average of performances for SBW by PG and PR\n0510 15 20 25 30 35 40 45 onset oï¬€set gate /g415me dynamics tempo average \ncontextual factor rate of frequency [%] \n(b) average of performances for SCN by PA and PP\n0510 15 20 25 30 35 40 45 onset oï¬€set gate /g415me dynamics tempo average\ncontextual factor rate of frequency [%] \n(c) average of performances for SM5 by PG and PP\nFigure 6 .Frequencies of contextual factors for each composition\n4. CONCLUSIONS\nIn this paper, we presented a method for describing the char-\nacteristics of human musical performance. The experimen-\ntal results of performer identiï¬cation showed the use of the\nexpressive representations from the musical score enables\nthe efï¬cient acquisition of the model of the performance.\nThe results also showed that the proposed model can cap-\nture the characteristics of the performance from any subtle\ndifferences that cannot be found by most human listeners.\nTherefore, the efï¬cacy of using expressive representations\nfrom the musical score to describe the characteristics of the\nmusical performance was shown. This method can auto-\nmatically learn the knowledge necessary to describe the tree\nstructure of the model directly from the data of the perfor-mance. We believe that the availability of such objective\nelements from the proposed model is effective for the anal-\nysis of the performance. In the future, we will make compar-\nisons based on more common and more extensive examples,\nin addition to attempting to improve the modeling method.\nFurthermore, this method can be applied to generate unseen\nperformances. We are also making efforts in that direction.\n5. ACKNOWLEDGEMENT\nThis research was partially supported by NIT presidentâ€™s\ndiscretionary expense for young researchers and a Grant-\nin-Aid for Young Scientists (B) from the Ministry of Educa-\ntion, Culture, Sports, Science, and Technology, Japan.\n6. REFERENCES\n[1]M. Hashida, T. Matsui, and H. Katayose: â€A New Music\nDatabase Describing Deviation Information of Perfor-\nmance Expressions,â€ Proceedings of the International\nSymposium on Music Information Retrieval , pp. 489â€“\n494, 2008.\n[2]A. Kirke and E. R. Miranda: â€Survey of Computer\nSystems for Expressive Music Performance,â€ Journal\nof ACM Computing Surveys , Vol. 42, No. 1, Article 3,\n2009.\n[3]J. Langner and W. Goebl: â€Visualizing expressive per-\nformance in tempo-loudness space,â€ Computer Music\nJournal , Vol. 27, No. 4, pp. 69â€“83, 2003.\n[4]J. J. Odell: â€The Use of Context in Large Vocabulary\nSpeech Recognition,â€ Ph.D thesis, Cambridge Univer-\nsity, 1995.\n[5]B. H. Repp: â€A microcosm of musical expression: I.\nQuantitative analysis of pianistsâ€™ timing in the initial\nmeasures of Chopinâ€™s Etude in E major,â€ Journal of\nthe Acoustical Society of America , Vol. 104, No. 2,\npp. 1085â€“1100, 1998.\n[6]B. H. Repp: â€A microcosm of musical expression: II.\nQuantitative analysis of pianistsâ€™ dynamics in the ini-\ntial measures of Chopinâ€™s Etude in E major,â€ Journal\nof the Acoustical Society of America , Vol. 105, No. 3,\npp. 1972â€“1988, 1999.\n[7]B. H. Repp: â€A microcosm of musical expression: III.\nContributions of timing and dynamics to the aesthetic\nimpression of pianistsâ€™ performances of the initial mea-\nsures of Chopinâ€™s Etude in E major,â€ Journal of the\nAcoustical Society of America , Vol. 106, No. 1, pp. 469â€“\n478, 1999.\n[8]C. S. Sapp: â€Comparative analysis of multiple musical\nperformances,â€ Proceedings of the International Sym-\nposium on Music Information Retrieval , pp. 497â€“500,\n2007.\n[9]K. Shinoda and T. Watanabe: â€MDL-Based context-\ndependent subword modeling for speech recognition,â€\nA. Acoustical Society Japan (E) , Vol. 21, No. 1, pp. 70â€“\n86, 2000.\n[10] G. Widmer: â€Machine discoveries: A few simple, ro-\nbust local expression principles,â€ Journal of New Music\nResearch , Vol. 31, No. 1, pp. 37â€“50, 2002.\n[11] G. Widmer, S. Dixon, W. Goebl, E. Pampalk, and A. To-\nbudic: â€In search of the Horowitz factor,â€ AI Magazine ,\nVol. 24, No. 3, pp. 110-130, 2003.\n536"
    },
    {
        "title": "MusiCLEF: a Benchmark Activity in Multimodal Music Information Retrieval.",
        "author": [
            "Nicola Orio",
            "David Rizo",
            "Riccardo Miotto",
            "Markus Schedl",
            "Nicola Montecchio",
            "Olivier Lartillot"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1416506",
        "url": "https://doi.org/10.5281/zenodo.1416506",
        "ee": "https://zenodo.org/records/1416506/files/OrioRMSML11.pdf",
        "abstract": "This work presents the rationale, tasks and procedures of MusiCLEF, a novel benchmarking activity that has been developed along with the Cross-Language Evaluation Forum (CLEF). The main goal of MusiCLEF is to promote the development of new methodologies for music access and retrieval on real public music collections, which can combine content-based information, automatically extracted from music files, with contextual information, provided by users via tags, comments, or reviews. Moreover, MusiCLEF aims at maintaining a tight connection with real application scenarios, focusing on issues on music access and retrieval that are faced by professional users. To this end, this yearâ€™s evaluation campaign focused on two main tasks: automatic categorization of music to be used as soundtrack of TV shows and automatic identification of the digitized material of a music digital library.",
        "zenodo_id": 1416506,
        "dblp_key": "conf/ismir/OrioRMSML11",
        "keywords": [
            "MusiCLEF",
            "Cross-Language Evaluation Forum",
            "music access and retrieval",
            "real public music collections",
            "content-based information",
            "contextual information",
            "professional users",
            "TV shows",
            "music digital library",
            "evaluation campaign"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nMUSICLEF: A BENCHMARK ACTIVITY IN MULTIMODAL MUSIC\nINFORMATION RETRIEV AL\nNicola Orio\nUniversity of Padova\norio@dei.unipd.itDavid Rizo\nUniversity of Alicante\ndrizo@dlsi.ua.esRiccardo Miotto, Nicola Montecchio\nUniversity of Padova\n{miottori,montecc2 }@dei.unipd.itMarkus Schedl\nJohannes Kepler University\nmarkus.schedl@jku.atOlivier Lartillot\nAcademy of Finland\nolartillot@gmail.com\nABSTRACT\nThis work presents the rationale, tasks and procedures of\nMusiCLEF, a novel benchmarking activity that has been de-\nveloped along with the Cross-Language Evaluation Forum\n(CLEF). The main goal of MusiCLEF is to promote the de-\nvelopment of new methodologies for music access and re-\ntrieval on real public music collections, which can combine\ncontent-based information, automatically extracted from mu-\nsic ï¬les, with contextual information, provided by users via\ntags, comments, or reviews. Moreover, MusiCLEF aims at\nmaintaining a tight connection with real application scenar-\nios, focusing on issues on music access and retrieval that are\nfaced by professional users. To this end, this yearâ€™s evalua-\ntion campaign focused on two main tasks: automatic catego-\nrization of music to be used as soundtrack of TV shows and\nautomatic identiï¬cation of the digitized material of a music\ndigital library.\n1. INTRODUCTION\nThe increasing availability of digital music accessible by\nend users is boosting the development of Music Informa-\ntion Retrieval (MIR), a research area devoted to the study\nof methodologies for content- and context-based music ac-\ncess. As it appears from the scientiï¬c production of the last\ndecades, research on MIR encompasses a wide variety of\ndifferent subjects that go beyond pure retrieval: the deï¬ni-\ntion of novel content descriptors and multidimensional sim-\nilarity measures to generate playlists; the extraction of high\nlevel descriptors â€“ e.g. melody, harmony, rhythm, struc-\nture â€“ from audio; the automatic identiï¬cation of artist and\ngenre. As it is well known, the possibility to evaluate the\ndifferent research results using a shared dataset has always\nplayed a central role in the development of information re-\ntrieval methodologies, as it is witnessed by the success of\ninitiatives such as TREC and CLEF, which focus on textual\ndocuments.\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proï¬t or commercial advantage and that copies\nbear this notice and the full citation on the ï¬rst page.\nc/circlecopyrt2011 International Society for Music Information Retrieval.The same need has been perceived in MIR, motivating the\ndevelopment of an important evaluation campaign, the Mu-\nsic Information Retrieval Evaluation eXchange (MIREX).\nMIREX campaigns1are organized since 2005 [4] by the In-\nternational Music Information Retrieval Systems Evaluation\nLaboratory (IMIRSEL) at the Graduate School of Library\nand Information Science, University of Illinois at Urbana-\nChampaign. Due to the many limitations posed by the mu-\nsic industry, the organizers of the MIREX chose to distribute\nonly publicly available test collections. Participants are in\ncharge to create their own collection and after local experi-\nmentation submit their software that is run by the organizers.\nThis approach has two drawbacks, which have already been\ndebated by the MIR research community: the results of pre-\nvious campaigns cannot be easily replicated and the results\ndepend on the individual training sets and not only on the\nsubmitted algorithms.\nA recent relevant initiative, that aims at overcoming the\nlimitations imposed by not sharing the datasets between re-\nsearchers, is the Million Songs Dataset (MSD). Thanks to\nMSD2, researchers can access a number of features from a\nvery large collection of songs [2]. Unfortunately, the algo-\nrithms used to extract these features are not public, limiting\nthe possibility to carry out research on content description\ntechniques. Another ongoing initiative related to the eval-\nuation of MIR approaches is the Networked Environment\nfor Music Analysis (NEMA), that aims at providing a web-\nbased architecture for the integration of music data and an-\nalytic/evaluative tools3. NEMA builds upon the achieve-\nments of MIREX campaigns regarding the evaluation of MIR\napproaches, with the additional goal of providing tools for\nresource discovery and sharing.\nWithin this scenario, MusiCLEF is an additional bench-\nmarking initiative, that has been proposed in 2011 as part\nof the activities of the Cross-Language Evaluation Forum\n(CLEF). CLEF focuses on multilingual and multimodal re-\ntrieval4and gathers researchers in different aspect of in-\nformation retrieval, ranging from plagiarism and intellectual\nproperty rights to image retrieval.\nThe goal of MusiCLEF is to promote the development of\n1http://www.music-ir.org/mirex\n2http://labrosa.ee.columbia.edu/millionsong/\n3http://www.music-ir.org/?q=nema/overview\n4http://clef-campaign.org/\n603Oral Session 6: Databases and Evaluation\nnovel methodologies for music access and retrieval, which\ncan combine content-based information, automatically ex-\ntracted from music ï¬les, with contextual information, pro-\nvided by users through tags, comments, or reviews. The\ncombination of these two sources of information is still under-\ninvestigated in MIR, although it is well known that content-\nbased information alone is not able to capture all the relevant\nfeatures of a given music piece (for instance, its usage as a\nsoundtrack or the year of release), while contextual informa-\ntion suffers from the typical limitations for new items and\nnew users (also known as cold start).\nAiming at investigating and promoting research on the\ncombination of textual and music information, MusiCLEF\nhas a strong focus on multimodality that, together with mul-\ntilingualism, is the main objective of the CLEF evaluation\nforum. Moreover, the tasks proposed for MusiCLEF 2011\nare motivated by real scenarios, discussed with private and\npublic bodies involved in music access and dissemination. In\nparticular, MIR techniques can be exploited for helping mu-\nsic professionals to describe music collections and for man-\naging a music digital library of digitized analogue record-\nings. To this end, the organizers of MusiCLEF exploited\nthe ongoing collaborations with both a company for music\nbroadcasting services (LaCosa s.r.l.) and a public music li-\nbrary (University of Alicanteâ€™s Fonoteca).\nTwo tasks are proposed within MusiCLEF 2011, and both\nare based on a test collection of thousands of songs in MP3\nformat. To completely overcome copyright issues, only low-\nlevel descriptors will be distributed to participants. Figure 1\ndepicts the tasks workï¬‚ow of MusiCLEF, which is described\nin more detail in the following sections.\nMusicCLEFnot public\nMusicCLEFpublic web site\n                      Participant                participant pc    .         \nlast.fmwebservice\nMP3 Library\nLow level features\nif(conn  SELEC  WHERE  print   features extractor\n1.read\n<script var a= var xl if(xlsParticipant algorithms\nLow level featurestags\nif(conn  SELEC  WHERE  print   last.fm data\n3. download\n4. read\nresultsResults evaluation\nCampaignParticipant Results\nif(conn  SELEC  WHERE  print   last.fm data\n4. read\n5. produce\n2. produce\n1. http request\n7. publish\n6. submit\nFigure 1 :Task workï¬‚ow in MusiCLEF.\nIt is important to note that, although the audio ï¬les can-\nnot be distributed, the goal of MusiCLEF is to grant the par-\nticipants with complete access to music features of the test\ncollection. This means that the algorithms used to extractthe music descriptors are public â€“ and in particular are based\non the set of tools provided by the MIRToolbox â€“ but also\nthat participants can submit their own original algorithm for\nfeature extraction, that will be run locally. Therefore, Musi-\nCLEF goals are to ï¬ll the gap between the other important\ninitiatives in MIR evaluation: researchers can test and com-\npare their approaches using a shared number of tasks, as in\nMIREX, while accessing a shared collection of content de-\nscriptors, as in MSD.\n2. APPLICATION SCENARIOS\nAs mentioned in the previous section, a major goal of Musi-\nCLEF is to maintain a tight connection with real application\nscenarios, in order to promote the development of techniques\nthat can be applied to solve issues in music accessing and re-\ntrieval that are faced by professional users. The choice of fo-\ncusing on professional users is motivated by the fact that they\nneed to address a number of real-life issues that are usually\nnot taken into account by music accessing systems aimed at\nthe general public. At the same time, the evaluation of the\neffectiveness of the proposed automatic solution is easier to\nassess, because professional users have a clear idea of what\nare their information needs.\nIn the following we present the two professional partners\nof MusiCLEF, and we also describe the motivations that in-\nduced us to organize the two tasks mentioned in the previous\nsection.\n2.1 LaCosa s.r.l.\nLaCosa was founded as a service provider of the major TV\nbroadcasting â€“ public and private â€“ companies in Italy with\nthe goal of managing and describing a large music collec-\ntion of songs to be used for TV programs, including jingles,\nbackground and incidental music, and music themes for TV\nshows. LaCosa has a strong cooperation with RTI, a com-\npany that, apart from buying and storing songs issued by\nthe major record companies, produces its own music cata-\nlogue. At present, RTI library contains about 320,000 songs\nof pop-rock, jazz, and classical music. Besides playing the\nrole of music consultant, being one of the biggest private\nmusic repositories in Italy, RTI offers a number of services\nto external companies of music consultants, who can browse\nremotely the repository. Audio features distributed to the\nparticipants are thus extracted remotely, without download-\ning the audio ï¬les.\nThe typical job of a music consultant is to select a list\nof songs that are suitable for a particular application, for in-\nstance a TV commercial, the â€œpromoâ€ of a new program,\nthe background music for a documentary, and so on. The\navailability of large online collections, such as Last.fm and\nYouTube, is representing an alternative to the services of a\nmusic consultant. For instance, journalists are increasingly\nselecting by themselves the music for their news stories, in-\nstead of asking to music consultants. The goal of LaCosa is\n60412th International Society for Music Information Retrieval Conference (ISMIR 2011)\nthen to provide high quality descriptions, that are tailored to\nthe particular application domain, in order to represent still\na more interesting alternative to free recommendations.\nGiven these considerations, the requirements of LaCosa\ncan be summarized as follows: How to improve the acqui-\nsition process, extracting the maximum amount of informa-\ntion about music recordings from external resources? How\nto provide good suggestion about possible usages of music\nmaterial, minimizing the amount of manual work?\nBecause of the interest on the development of automatic\nsystems for addressing these two requirements, LaCosa de-\ncided to provide at its own expenses a number of assessors\nto create the ground truth for evaluation. The involvement\nof professional users included also the deï¬nition of a vocab-\nulary of 167 terms describing music genre (terms are orga-\nnized in two levels, genre and subgenre), and of 188 terms\ndescribing the music mood. It is important to note that, in\nthis case, the concept of mood is related to the usage of a\nparticular song within a video production. As explained in\nmore detail in Section 3, only a subset of the mood tags have\nbeen used in the evaluation campaign.\n2.2 University of Alicanteâ€™s Fonoteca\nSome years ago, the local radio broadcast station Radio Al-\nicante Cadena Ser transferred its collection of vinyls to the\nLibrary of the University of Alicante. This collection con-\ntains approximately 40,000 vinyls of an important cultural\nvalue, containing a wide range of genres. The library de-\ncided to digitize the vinyls, sound and covers, to overcome\nthe preservation problems when allowing library users to ac-\ncess the discs and to enable its reproduction embedded in\nthe libraryâ€™s Online Public Access Catalog (OPAC) with the\nname Fonoteca5.\nThe process was carried out following library cataloguing\ntechniques to make the inventory of the collection. Vinyls\nwere catalogued using Universal Decimal Classiï¬cation, and\nclassiï¬ed into subjects based on the Library of Congress sub-\nject headings. Digitized covers and audio were linked to\nthe corresponding records. The cataloguing data consists of\nthe albumâ€™s title, the name of the discographic company, the\nrelease year, its physic description, several entries for gen-\nres classiï¬ed manually by the cataloguers, and ï¬nally notes\nabout the content. Regarding the sound content, each vinyl\nwas digitized in two ï¬les, one for each side. For 45 rpm\ndiscs each side usually contains only one song, while for 33\nrpm LPs, which are more common in the collection, each\nside contains several tracks.\nHaving catalogued and digitized the material, some draw-\nbacks emerge that strongly limit the browsing capabilities\nin the OPAC. The separation of tracks from a continuous\nstream could be easily solved in most cases just by ï¬nding\nsilences between tracks. However, this may not be the case\nfor live recordings or classical music tracks, where the mu-\nsic itself contains long rests. A related problem is the correct\n5http://www.ua.es/en/bibliotecas/SIBID/fonotecaentitling of the tracks. Although some catalogued albums\ncontain details of the contained tracks, there are many oth-\ners, mainly operas, where the track names are not present.\nAnother common situation is that of ï¬nding two different\nrecordings of the same work whose tracks have been la-\nbeled using two different languages or naming schemes, e.g.,\nâ€œSymphony No. 9â€ knowns as â€œNovena Sinfon Â´Ä±aâ€ as well as\nâ€œChoral Symphonyâ€. Audio ï¬ngerprinting techniques can\nhardly be applied to solve this task because of disc age, be-\nsides the fact that some of the discs may not have been reis-\nsued on CD and thus may not have been included in any\naudio ï¬ngerprint dataset.\nBesides these drawbacks, the staff of the library demands\nsome features that cannot be implemented given the current\nstructure of the data. For example, given an album, ï¬nd it\nin music sites like Last.fm orGrooveshark . Similarly, ï¬nd a\ngiven song/track and its different recordings in those music\nsites and inside the library regardless of language or naming\nschemes. In order to locate music, they want the users to be\nable to query the library given metadata not contained in the\ncatalog, like the lyrics of the songs.\n3. CATEGORIZATION OF POP/ROCK MUSIC\nThe goal of the ï¬rst task is to exploit both automatically\nextracted information about the content and user generated\ninformation about the context to carry out categorization.\nThe task is based on a real application scenario: songs of\na â€œcommercial music libraryâ€ need to be categorized ac-\ncording to their possible usage in TV and radio broadcasts\nor Web streaming (commercials, soundtracks, jingles). Ac-\ncording to experts in the ï¬eld, it is common practice to use\ndifferent sources of information to assess the relevance of a\ngiven song to a particular usage. At ï¬rst candidate songs are\nselected depending on the result of Web searches and on the\nanalysis of user-generated tags. Since these sources of infor-\nmation are usually very noisy, experts make the ï¬nal choice\ndepending on the actual music content.\nIn order to simulate this scenario, participants of Musi-\nCLEF are provided with three different sources of informa-\ntion: content descriptors, user tags, and related Web pages.\nSince CLEF campaigns aim at promoting multilingualism,\ntags and Web pages are in different languages. It was not\nmandatory, at least for MusiCLEF 2011, neither to use all\nthe different languages nor to exploit all the source of infor-\nmation. In general, participants are free to select the descrip-\ntors that better ï¬t the approach they want to test. To this end,\nthe possibility of creating a baseline of individual sources of\ninformation is considered of interest for future MusiCLEF\ncampaigns.\nThe dataset made available to participants includes mostly\nsongs of pop and rock genres, which are the more often used\nin TV broadcasts. As mentioned in Section 2.1 a number of\nmusic professionals from LaCosa s.r.l. provided the catego-\nrization for the complete dataset of 1355 songs, which has\nbeen divided in a training set of 975 song and test set of the\n605Oral Session 6: Databases and Evaluation\nremaining 380 songs. Being the ï¬rst year, the ground truth\nis available for a limited number of songs but it is envisaged\nthat the continuation of MusiCLEF over the years will create\na shared background for evaluation.\nThe participants were asked to assign to each song in the\ntest set the correct tags. Results were evaluated against the\nground truth.\n3.1 Deï¬nition of the Dataset\nThe task of music categorization can be considered an auto-\ntagging task, that is the automatic assignment of relevant de-\nscriptive semantic words to a set of songs. In the literature,\nseveral scalable approaches have been proposed for labeling\nmusic with semantics including social tagging, Web mining,\ntag propagation from similar songs, and content-based au-\ntomatic strategies [3]. Regardless of the approach used, the\noutput of a tagging system is generally a vector of tag scores,\nwhich measures the strength of the relationships tag-song for\neach tag of a semantic vocabulary (i.e. semantic weights ).\nThe dataset built to carry out the auto-tagging evaluation\ncampaign is composed of 1355 different songs, played by\n218 different artists; each song has a duration between 2 and\n6 minutes. One of the goals of the task is to have participants\nthat may exploit, beyond content-based audio features, also\nother music descriptors (e.g. social and Web mined tags).\nFor this reason we built the dataset using only well-known\nartists; this allowed us to gather a big amount of Web-based\ndescriptors (i.e. the â€œwisdom of the crowdâ€) for most of the\nsongs in the dataset. We collected the songs starting from\nthe â€œRolling Stone 500 Greatest Songs of All Timeâ€ list6,\nwhich was the cover story of a special issue of Rolling Stone\n(no. 963 of December 9 2004 â€“ updated in May 2010). The\nsong list was chosen based on votes by 172 musicians, crit-\nics, and music-industry professionals, and is almost entirely\ncomposed of English-speaking artists. Table 1 reports the\ntop 10 positions of this rank list.\nStarting from this list, we considered all the different artists\nas seeds to query a larger music database for gathering all the\nsongs associated to every artist, excluding live versions that\nare usually of little interest for TV broadcasts. From this\npool we randomly retained at most 8 songs per-artist, in or-\nder to fairly uniformly distribute songs between the different\nartist. As result, we had 161 artists associated with about 8\nsongs in the ï¬nal collection.\nEach song in the dataset has been manually annotated by\nmusic professionals from LaCosa. The vocabulary of tags\ndeï¬ned by the experts was initially composed of 355 tags\ndivided in two categories â€“ genre (167) and usage (288) â€“\nloosely inspired by the Music Genome Project7.\nAfter that, all the songs have been tagged by the human\nexperts with at least one tag for genre and ï¬ve tags for mood.\nAt the end, we discarded all the tags that were assigned to\n6http://www.metrolyrics.com/rs/ (as in May 2011)\n7http://www.pandora.comRank Title Artist\n1 Like a rolling stone Bob Dylan\n2 (I canâ€™t get no) Satisfaction Rolling Stones\n3 Imagine John Lennon\n4 Whatâ€™s going on Marvin Gaye\n5 Respect Aretha Franklin\n6 Good Vibrations Beach Boys\n7 Johnny B. Goode Chuck Berry\n8 Hey Jude Beatles\n9 Smells like teen spirit Nirvana\n10 Whatâ€™d I say Ray Charles\nTable 1 :Top 10 songs of the Rolling Stone 500 Greatest Songs\nList (updated 2010).\nless than twenty songs; this led to the ï¬nal released vocabu-\nlary of 94 tags.\n3.2 Content- and Context-based Descriptors\nSongs are also described by audio features. In particular,\nwe precomputed timbre descriptors (Mel-Frequency Cep-\nstral Coefï¬cients) that are directly available to participants.\nFeature sets have been computed using the MIRToolbox [7]\nalgorithms, which are publicly available. Moreover, partici-\npants can request the extraction of additional descriptors. In\norder to let participants perform their own feature extraction,\nwe plan to make available also more general features in fu-\nture years. In particular, we plan to provide the output of\nthe triangular ï¬lterbanks before computing the log and the\ncosine transform of MFCCs. The rhythm based descriptors\nprovided by the MIRToolbox will be precomputed as well.\nWe also provide social tags gathered from Last.fm as avail-\nable on May 2011. For each song of the corpus, we used\nthe Last.fm audio ï¬ngerprint service8and public data shar-\ning AudioScrobbler website9to associate our music ï¬les to\ntheir songs and collect social tags for each song. Therefore,\nwe release the list of social tags together with their associ-\nated score.\nCategory Tags\nGenrebossanova, country rock, hymn, orches-\ntral pop, slide blues\nMoodalarm, awards, danger, glamour, mili-\ntary, scary, trance\nTable 2 :A sample of the tags proposed to the music professionals\nfor annotating the songs of the auto-tagging dataset.\n8http://blog.last.fm/2010/07/09/\nfingerprint-api-and-app-updated /\n9http://ws.audioscrobbler.com/2.0/\n60612th International Society for Music Information Retrieval Conference (ISMIR 2011)\n3.3 Web-mining\nWeb pages covering music-related topics have been used\nsuccessfully as data source for various MIR tasks, in partic-\nular, for information extraction (e.g., band membership [5],\nartist recommendation [1], and similarity measurement [6,\n8]. The text-based features extracted from such Web pages\nare often referred to as cultural or community metadata since\nthey typically capture the knowledge or opinions of a large\nnumber of people or institutions. They therefore represent a\nkind of contextual data.\nWe ï¬rst queried Google to retrieve up to 100 URLs for\neach artist in the collection. Subsequently, we fetch the Web\ncontent available at these URLs. Since usually the resulting\npages typically contain a lot of unrelated documents, we al-\nleviate this issue by adding further keywords to the search\nquery, with an approach similar to [8]. We crawled various\nsets of Web pages in six different languages â€“ English, Ger-\nman, Swedish, French, Italian, and Spanish â€“ employing the\nfollowing query scheme:\n\"artist name\" (+music|+musik|+musique|+musica)\nFor MusiCLEF a total of 127,133pages have been fetched.\nThe resulting information enables participants who would\nlike to make use of structural information to derive corre-\nsponding features from the raw Web pages. In addition to\nthese sets of Web pages, we provide precomputed term weight\nvectors. Taking into account the ï¬ndings of a large scale\nstudy on modeling term weight vectors from artist-related\nWeb pages [6], we ï¬rst describe each artist as a virtual doc-\nument, which is the concatenation of the HTML documents\nretrieved for the artist. We then compute per virtual artist\ndocument the term frequencies (tf) in absolute numbers.\nFurther providing the inverse document frequency (id f) scores\nfor the Web page set of each language will allow participants\nto easily build a simple t fÂ·id frepresentation or apply more\nelaborate information fusion techniques. In summary, for\nthe term vector representation of the dataset, we offer the\nfollowing pieces of information:\nâ€¢t fweights per virtual document of each artist\nâ€¢global id fscores for each language\nâ€¢corresponding lists of terms for each language\nThe twofold representation of the datasets (Web pages\nand generic term weights) leaves much room for various\ndirections of experimentation. For example, Web structure\nmining and structural analysis techniques can be applied to\nthe Web pages, while the provided term weight representa-\ntion will certainly beneï¬t from term selection, length nor-\nmalization, and experimentation with different formulations\nfort fandid f.\n4. IDENTIFICATION OF CLASSICAL MUSIC\nThe task of automatically identifying an audio recording is\na typical MIR task, consisting of the clustering in the samegroup recordings of different performances of a composi-\ntion. Also in this case, a real-life application scenario has\nbeen considered: loosely labeled digital acquisition of old\nanalogue recordings of classical music should be automat-\nically annotated with metadata (composer, title, movement,\nexcerpt). Although systems for automatic music identiï¬ca-\ntion already give good results, the combination of segmen-\ntation and identiï¬cation of continuous recordings is not well\ninvestigated yet. The participants are provided by a set of\ndigital acquisitions of vinyls made by the Fonoteca, that has\nto be segmented and labeled.\nAn important aspect addressed by this task is the scalabil-\nity of the approaches. To this end, we encourage participants\nto test the performance on the same task with a reference col-\nlection of increasing size, up to about 6,700 MP3s. This is\nachieved by providing additional information on the record-\ning that can help ï¬ltering out part of the dataset. In par-\nticular, the additional information is consistent with the one\nfounded in the real LP covers â€“ author, performer, short title\nâ€“ and is the sole information that is reported by the Fonoteca\ncatalogue. For this task, relevance judgments are provided\nautomatically using available metadata and listening directly\nto the recordings.\nParticipants are provided with content descriptors of the\ncomplete dataset of 6680 single music ï¬les and with 22 ad-\nditional digital acquisitions of 11 LPs (thus a total of 22 LP\nsides is available on individual MP3s). There are two differ-\nent goal: to identify the songs belonging to the same group\n(for single ï¬les) and to match the content of the LP record-\nings with the corresponding songs.\n4.1 Deï¬nition of the Dataset\nMusic identiï¬cation usually focuses on pop music (hence\nits common designation as cover song identiï¬cation). The\nreason for that might be attributed to the disproportion in\ncommercial interests for the pop music market with respect\nto other genres. Nonetheless the need for the application of\nsuch technology to other styles is often felt by many music\nlibraries and archives that, especially in Europe, aim at the\npreservation and dissemination of classical music.\nThe collection that we propose was created starting from\nthe database of a broadcasting company consisting of about\n320,000 music recordings in MP3 format (see Section 2.1).\nOur primary aim was to extract from it the largest possible\nsub-collection of classical music in order to build a shared\ndataset for the classical music identiï¬cation task. We se-\nlected 2,671 such recordings, associated to works that are\nrepresented at least twice in the database. These recordings\nform 945 cover sets10; the distribution of the set cardinal-\nities follows a power law, and is represented in Figure 2.\nThe distribution of the recordings with respect to the worksâ€™\nauthors is depicted in Figure 3. The collection was ï¬nally\n10The phrase â€œcover setâ€ denotes a set of different recordings of the same\nunderlying piece of music.\n607Oral Session 6: Databases and Evaluation\nFigure 2 :Distribution of cover set cardinalities for the classical music cover identiï¬cation task.\n100 300\nBACH\nBEETHOVEN\nBRAHMS\nCHOPIN\nDEBUSSY\nDVORAK\nGRIEG\nHAENDEL\nHAYDN\nLISZT\nMONTEVERDI\nMOZART\nRA VEL\nROSSINI\nSCHUMANN\nSCRIABIN\nSTRAUSS J. II\nTCHAIKOVSKY\nVIV ALDI\nRACHMANINOV\nFigure 3 :Number of ï¬les for the most represented authors.\naugmented to 6680 pieces by adding recordings of classical\nmusic works by other authors.\n4.2 Content-based Descriptors\nSongs are described by audio features. In particular, we\nprecomputed audio descriptors (chroma vectors) that are di-\nrectly available to participants. Chroma vectors have been\ncomputed at different temporal and frequency resolutions.\nAlso in this case, feature sets have been computed using\nthe MIRToolbox [7] algorithms, which are publicly avail-\nable. Moreover, participants can request the extraction of\nadditional descriptors (which may include also additional\nchroma vectors computed with different algorithms). It is\nimportant to note that datasets of any size can be processed\nthanks to implicit memory management mechanisms devel-\noped in MIRtoolbox.\n5. CONCLUSIONS\nThis paper introduces MusiCLEF, a new benchmarking ac-\ntivity that aims at fostering content- and context-based anal-\nysis techniques to improve music information retrieval tasks,\nwith a special focus on multimodal approaches. A one-day\nMusiCLEF workshop is to be held in 2011 in Amsterdam aspart of the Cross-Language Evaluation Forum (CLEF) con-\nference, where participants can share their approaches and\ncontribute to the future organization of MusiCLEF.\n6. ACKNOWLEDGMENTS\nThe authors are grateful for the support of the staff of La-\nCosa s.r.l. and the University of Alicanteâ€™s Fonoteca. Mu-\nsiCLEF has been partially supported by Network of Excel-\nlence co-funded by the 7th Framework Programme of the\nEuropean Commission, grant agreement no. 258191. CLEF\nis an activity of PROMISE. This research is also supported\nby the Spanish Ministry projects DRIMS (TIN2009-14247-\nC02-02) and Consolider Ingenio MIPRCV (CSD2007-00018),\nboth partially supported by EU ERDF, and by the Austrian\nScience Funds (FWF): P22856-N23.\n7. REFERENCES\n[1] S. Baumann and O. Hummel. Using Cultural Metadata for\nArtist Recommendation. In Proc. of WEDELMUSIC , Leeds,\nUK, Sep 2003.\n[2] T. Bertin-Mahieux, D. P.W. Ellis, B. Whitman, and P. Lamere.\nThe million song dataset. In Proc. of ISMIR , 2011.\n[3] D. Turnbull et al. Five Approaches to Collecting Tags for Mu-\nsic. In Proc. of ISMIR , 2008.\n[4] J. S. Downie et al. The 2005 Music Information retrieval Eval-\nuation Exchange (MIREX 2005): Preliminary Overview. In\nProc. of ISMIR , 2005.\n[5] M. Schedl et al. Web-based Detection of Music Band Members\nand Line-Up. In Proc. of ISMIR , Vienna, Austria, Sep 2007.\n[6] M. Schedl et al. Exploring the Music Similarity Space on the\nWeb. ACM Transactions on Information Systems , 2011.\n[7] O. Lartillot and P. Toiviainen. A Matlab Toolbox for Musical\nFeature Extraction from Audio. In Proc. of DAFx , 2007.\n[8] B. Whitman and S. Lawrence. Inferring Descriptions and Simi-\nlarity for Music from Community Metadata. In Proc. of ICMC ,\nGÂ¨oteborg, Sweden, Sep 2002.\n608"
    },
    {
        "title": "Incremental Bayesian Audio-to-Score Alignment with Flexible Harmonic Structure Models.",
        "author": [
            "Takuma Otsuka",
            "Kazuhiro Nakadai",
            "Tetsuya Ogata",
            "Hiroshi G. Okuno"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1418159",
        "url": "https://doi.org/10.5281/zenodo.1418159",
        "ee": "https://zenodo.org/records/1418159/files/OtsukaNOO11.pdf",
        "abstract": "Music information retrieval, especially the audio-to-score alignment problem, often involves a matching problem between the audio and symbolic representations. We must cope with uncertainty in the audio signal generated from the score in a symbolic representation such as the variation in the timbre or temporal fluctuations. Existing audio-to-score alignment methods are sometimes vulnerable to the uncertainty in which multiple notes are simultaneously played with a variety of timbres because these methods rely on static observation models. For example, a chroma vector or a fixed harmonic structure template is used under the assumption that musical notes in a chord are all in the same volume and timbre. This paper presents a particle filterbased audio-to-score alignment method with a flexible observation model based on latent harmonic allocation. Our method adapts to the harmonic structure for the audio-toscore matching based on the observation of the audio signal through Bayesian inference. Experimental results with 20 polyphonic songs reveal that our method is effective when more number of instruments are involved in the ensemble.",
        "zenodo_id": 1418159,
        "dblp_key": "conf/ismir/OtsukaNOO11",
        "keywords": [
            "audio-to-score alignment",
            "matching problem",
            "audio signal",
            "symbolic representations",
            "timbre",
            "temporal fluctuations",
            "chroma vector",
            "fixed harmonic structure template",
            "Bayesian inference",
            "latent harmonic allocation"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nINCREMENTAL BAYESIAN AUDIO-TO-SCORE ALIGNMENT\nWITH FLEXIBLE HARMONIC STRUCTURE MODELS\nTakuma Otsukaâ€ , Kazuhiro Nakadaiâ€¡, Tetsuya Ogataâ€ , and Hiroshi G. Okunoâ€ \nâ€ Graduate School of Informatics, Kyoto Universityâ€¡Honda Research Institute Japan, Co., Ltd.\nSakyo-ku, Kyoto 606-8501 Japan Wako, Saitama 351-0114, Japan\n{ohtsuka, ogata, okuno }@kuis.kyoto-u.ac.jp nakadai@jp.honda-ri.com\nABSTRACT\nMusic information retrieval, especially the audio-to-score\nalignment problem, often involves a matching problem be-\ntween the audio and symbolic representations. We must\ncope with uncertainty in the audio signal generated from the\nscore in a symbolic representation such as the variation in\nthe timbre or temporal ï¬‚uctuations. Existing audio-to-score\nalignment methods are sometimes vulnerable to the uncer-\ntainty in which multiple notes are simultaneously played\nwith a variety of timbres because these methods rely on\nstatic observation models. For example, a chroma vector\nor a ï¬xed harmonic structure template is used under the as-\nsumption that musical notes in a chord are all in the same\nvolume and timbre. This paper presents a particle ï¬lter-\nbased audio-to-score alignment method with a ï¬‚exible ob-\nservation model based on latent harmonic allocation. Our\nmethod adapts to the harmonic structure for the audio-to-\nscore matching based on the observation of the audio signal\nthrough Bayesian inference. Experimental results with 20\npolyphonic songs reveal that our method is effective when\nmore number of instruments are involved in the ensemble.\n1. INTRODUCTION\nMusic information retrieval tasks require a robust inference\nunder the uncertainty in musical audio signals. For example,\na polyphonic or multi-instrument aspect encumbers the fun-\ndamental frequency estimation [10, 15] or instrument iden-\ntiï¬cation [9]. Overcoming the uncertainty in musical audio\nsignals is a key factor in the machine comprehension of mu-\nsical information. The audio-to-score alignment technology\nshares this uncertainty problem in that an audio signal per-\nformed by human musicians has a wide range of varieties\ngiven a symbolic score due to the musiciansâ€™ expressive-\nness. For example, the type of instruments and the temporal\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proï¬t or commercial advantage and that copies\nbear this notice and the full citation on the ï¬rst page.\nc/circlecopyrt2011 International Society for Music Information Retrieval.\nAudio: \n1. various harmonic structures \n2. unknown mixture ratio \nMismatch! Score: \n1. fixed harmonic structures \n2. equal mixture ratio Figure 1 . The issue: uncertainty in the audio and ï¬xed har-\nmonic templates from the score\nor pitch ï¬‚uctuations affect the resulting audio signals.\nIncremental audio-to-score alignment, also known as score\nfollowing , methods are essential to automatic accompani-\nment systems [5], intelligent score viewers [2], and robot\nmusicians [13] because the alignment synchronizes these\nsystems with human performances. We need a probabilis-\ntic framework for the audio-to-score alignment problem in\norder to cope with the uncertainty in the audio signal gener-\nated from the score in a symbolic representation.\nExisting methods tend to fail the alignment when mul-\ntiple musical notes are played by multiple musical instru-\nments. That is, the audio signal contains various timbres and\nthe volume ratio of each musical note is unsure. Figure 1\nillustrates this issue. The observed pitched audio signal in-\ncludes equally-spaced peaks in frequency domain called a\nharmonic structure. The observed audio is matched with\nharmonic structure templates generated from the score. Mu-\nsical notes written in the score is played with arbitrary musi-\ncal instruments. The resulting audio harmonic structures can\nvary from instrument to instrument whereas the templates of\nthe score have been set in advance using some heuristics or\na parameter learning [4]. In Figure 1, harmonic structures\nof a guitar and a violin is shown in blue and red lines, re-\nspectively. Furthermore, the mixture ratio of each note in\nthe audio is unknown until the observation while the ratio in\nthe template is ï¬xed, typically equal.\nThus, the variety of the audio signal causes a mismatch\n525Poster Session 4\nFrequency  xAmplitude Amplitude \nAmplitude \nFrequency  xProbability distribution Frequency  x lÂµlÂµ2lmÂµL1lÎ¸\n2lÎ¸\nlm Î¸1âˆ’Î›lMharmonics \nN\nLdGMMs dl Ï€Figure 2 . Audio spectrogram based on LHA\ndl Ï€nz\nlÎ›lÂµ\ndL MdNDnx\nlmÎ¸Figure 3 . Graphical model of LHA\ndl Ï€nz\nlmÎ¸lÎ›lÂµ\ndL MdNDnx\nFigure 4 . LHA with ï¬xed Î¸lmâ€™s\nbetween the observed harmonic structure and the ï¬xed one\ngenerated from the score. We need a ï¬‚exible harmonic struc-\nture model to robustly match the audio and score since the\naudio signal is almost unknown until we observe it.\nOur idea is to employ a Bayesian harmonic structure model\ncalled latent harmonic allocation (LHA) [15]. This model\nallows us to form harmonic structure templates reï¬‚ecting\nthe observed audio with the prior knowledge written in the\nscore, e.g., fundamental frequencies of musical notes.\n1.1 Related work\nTwo important aspects reside in modeling audio-to-score\nalignment: (1) a temporal model of musical notes and (2)\nan observation model of the input audio signal from the\ncorresponding score. Although improvements are made re-\npeatedly for the temporal model, misalignments are often\ncaused by static and ï¬xed audio observation models. The\naudio observation model used in the methods introduced in\nthis section uses static features such as chroma vectors or\nï¬xed harmonic structure templates based on Gaussian mix-\nture model (GMM). These features are often heuristically\ndesigned and therefore lose robustness against uncertain sit-\nuations in which many instruments are involved and the au-\ndio is polyphonic.\nMost audio-to-score alignment methods employ dynamic\ntime warping (DTW) [2,6], hidden Markov models (HMM) [4,\n12], or particle ï¬lters [7, 11, 13]. DTW or HMM-based\nmethods sometimes fails the alignment since the length of\nmusical notes is less constrained in the decoding.\nThe note length corresponds to the length of a state se-\nquence in the HMM. Contâ€™s method [3] uses a hidden semi-\nMarkov model (HSMM) to control state lengths. The HSMM\nrestricts the duration of a stay at one state so that the state\nlength is limited. While the model refrains from delayed\nstate transitions, this has no restriction on fast transitions.\nAs a result, the HSMM tends to estimate the audio signal\nfaster than it is.Some methods estimate not only the score position but\nalso the tempo, i.e., the speed of the music for the tempo-\nral accuracy. Raphaelâ€™s method includes the tempo of the\nmusic as a state [14] to accurately decode the note lengths.\nOtsuka et al. [13] propose a particle ï¬lter-based method for\ntheir simultaneous estimation. While Raphaelâ€™s method ob-\nserves only harmonic structures as pitch information, Ot-\nsuka et al.â€™s method observes the periodicity of the onsets to\ndirectly estimate the tempo.\n2. AUDIO OBSERVATION MODEL\nThis Section describes how the audio is generated in terms\nof LHA. We focus on harmonic structures to associate an\naudio signal with a symbolic score. The LHA model ï¬‚exibly\nï¬ts the shape of harmonic structures given an audio signal\nobservation using variational Bayes inference.\nThe harmonic peaks are often modeled as a Gaussian\nmixture model (GMM) by regarding each peak as a sin-\ngle Gaussian [3, 13, 14]. The black lines in Figure 1 are\nthe GMM curves. These methods use Kullback-Leibler di-\nvergence (KL-div) as a matching function between the au-\ndio harmonics and the GMM template harmonics generated\nfrom the score by regarding the harmonic structure as a prob-\nability distribution. The mean value of each Gaussian peak\nis determined by a pitch speciï¬ed in the score.\nLHA [15] is a generative model for harmonic structures\nof pitched sounds. A graphical model for LHA is depicted\nin Figure 3. In the LHA model, the amplitude of audio har-\nmonics is regarded as a histogram over the frequency bins.\nFigures 2 and 3 explain how a mixture of harmonic struc-\ntures is generated. Variables in a circle are random variables\nwhile those without a circle are parameters. Double circled\nxnmeans an observed variable. For each segment d,Nd,\nfrequencies xnare observed. The audio spectrogram is seg-\nmented into dby chords, which are sets of musical notes. To\nsample each xn, aLdM-dimensional multinomial latent vari-\nable znis sampled as follows. A harmonic structure GMM l\n52612th International Society for Music Information Retrieval Conference (ISMIR 2011)\nTÎ”\nTkÎ”(sec) \nScore position Beat \ninterval \n(a) Particle drawing (b) Weight calculation \nkA\nqW\n(c) Point estimation & \nresampling Audio buffer \n(time range:       ) \nfor observation & \nproposal dist. kT kAMost recently \nobserved \naudio signal 1âˆ’kAW\nAudio-to-score \nmatching Weighted average \ncalculation Figure 5 . Three steps in particle ï¬ltering for the audio-to-score alignment\nis selected with probability Ï€dl, where âˆ‘Ld\nl=1Ï€dl=1. Among\nMGaussian peaks, mis selected to sample xnwith probabil-\nityÎ¸lm, where âˆ‘M\nm=1Î¸lm=1. Finally, xnis sampled from\nthe Gaussian distribution of which mean and precision are\nmÂµlandÎ›l, respectively. The deï¬nitions of each variable in\nLHA in Figure 3 are summarized below:\np(X|Z,Âµ,Î›) =âˆ\ndnlmN(xdn|mÃ—Âµl,Î›l), (1)\np(Z|Ï€,Î¸) =âˆ\ndnlm(Ï€dlÎ¸lm)zdnlm, (2)\np(Ï€) =âˆ\ndDir(Ï€d|Î±0),p(Î¸) =âˆ\nlDir(Î¸l|Î²0),and (3)\np(Î›) =âˆ\nlGam (Î›l|a0,b0), (4)\nwhere N(Â·), Dir (Â·), Gam (Â·)denote the density functions of\nGaussian, Dirichlet, and gamma distribution, respectively.\nThe latent variable zdn= [zdnlm]isLdM-dimensional with\none element being 1 and the other being 0. Variables Ï€and\nÎ¸are conjugate priors for Z, and the precision of Gaussian\nharmonics Î›is a conjugate prior for X. Here, Î±0,Î²0,a0, and\nb0are hyperparameters for each distribution. Î±0= [Î±0l]Ld\nl=1\nis set as Î±0l=1, and Î²0= [Î²0m]M\nm=1is set as Î²0m=1 be-\ncause the mixture ratio of each musical note and the height\nof each harmonic are unknown. A â€œï¬‚atâ€ prior knowledge\nabout these parameters is preferred to reï¬‚ect our ignorance.\nThe hyperparameters of the gamma distribution are empir-\nically set as a0=1 and b0=2.4 by considering the width\nof harmonics determined by the window function of a short-\ntime Fourier transform (STFT).\nThe LHA is originally designed for multi-pitch analy-\nsis [15], and therefore the fundamental frequency Âµlis a\nrandom variable. However, in our audio-to-score alignment\nframework, Âµlis treated as a parameter because fundamen-\ntal frequencies are given by the score as musical notes. This\nis why Âµlis not in a circle in Figure 3.\nIn general, too ï¬‚exible model can cause an over-ï¬tting\nproblem. LHA is ï¬‚exible in terms of the mixture ratio Ï€and\nharmonic heights Î¸. To limit the model complexity, we ï¬x\nthe harmonic heights Î¸and only consider the mixture ratio\nÏ€as in Figure 4. We refer to the former model in Figure 3\nasfull LHA , and the latter in Figure 4 as mixture LHA .3. AUDIO-TO-SCORE ALIGNMENT USING\nPARTICLE FILTER\nThis section presents the problem setting and procedures of\nour method. The problem is speciï¬ed as follows:\n\u0013 \u0010\nInputs: incremental audio signal and the correspond-\ning whole score\nOutputs: the current score position and tempo\nAssumptions: (1) The score includes musical notes\nand the approximate tempos of the music. (2) Musical\nnotes are pairs of their pitch and length, e.g., a quar-\nter note, (3) Approximate tempos are speciï¬ed as the\nrange of a tempo, e.g., 90â€“110 beats per minute (bpm).\n\u0012 \u0011\nNo prior knowledge about musical instruments is assumed.\n3.1 Method overview\nLetkbe the index of ï¬ltering steps and At,fbe the ampli-\ntude of the input audio signal in the time-frequency domain.\nHere, tandfdenote the time (sec) and the frequency (Hz),\nrespectively. Our system is implemented at a sampling rate\nof 44100 (Hz), a window length of 2048 (pt), and a hop size\nof 441 (pt). Â¯At,fdenotes a quantized integer amplitude given\nbyÂ¯At,f=âŒŠAt,f/âˆ†AâŒ‹, where âˆ†Ais the quantization factor,\nandâŒŠÂ·âŒ‹is the ï¬‚ooring function. âˆ†A=3.0 in our implemen-\ntation. This value should be so small that the shape of the\nspectrum is preserved after the quantization and that sufï¬-\ncient observations are provided for the Bayesian inference\nin the LHA. Let p(beat) be the score position. The score is\ndivided into frames whose lengths are equal to 1 /12 of one\nbeat, namely, a quarter note1. Musical notes are denoted by\nÂµp= [Âµ1\np...ÂµLp\np]T, where Lpis the number of notes at p, and\nÂµis the fundamental frequency of the note.\nFigure 5 illustrates the procedures. At every âˆ†T(sec), the\nparticle ï¬ltering [1] proceeds as: (a) move particles in accor-\ndance with elapsed âˆ†T(sec) by drawing particles from the\nproposal distribution, (b) calculate the weight of each parti-\ncle, (c) report the point estimation of the score position and\nbeat interval, and resample the particles. Each particle has\n1pis discretized at 1/12 interval in (beat).\n527Poster Session 4\nscore frame p\nonset frames Figure 6 . Score position proposal. Au-\ndio frames marked by green rectangles are\naligned with score onsets.\nGMM harmonic \nstructure \ni\nkp)(~Ï„pi\nkb\nÏ„Figure 7 . Frame-by-frame alignment\nof audio using pi\nkandbi\nk\ni\nkpi\nkbSegmentation by \ndifferent chords Figure 8 . Segmentation by chords for\nLHA observation\nthe following information as a hypothesis: the score posi-\ntion pi\nk, beat interval (sec/beat), i.e., the inverse tempo, bi\nk,\nand the weight wi\nkas a ï¬tness to the model.\nIn the kth ï¬ltering step, the particle ï¬lter estimates the\nposterior distribution of the score position pkand beat in-\nterval bkgiven the latest audio spectrogram Ak= [AÏ„,f],\nwhere Ï„âˆˆTk,Tk={t|kâˆ†Tâˆ’W<tâ‰¤kâˆ†T}, and Wis\nthe window length for the audio spectrogram. The poste-\nrior distribution is approximated using many particles as in\np(sk|Ak) =âˆ‘I\ni=1wi\nkÎ´(si\nkâˆ’sk), where Iis the number of par-\nticles, and si\nk= [pi\nk,bi\nk]denotes the state of the ith particle.2\nThe weight of each particle wi\nkis calculated as:\nwi\nkâˆp(si\nk|si\nkâˆ’1)p(Ak|si\nk)\nq(si\nk|si\nkâˆ’1,Ak), (5)\nwhere p(si\nk|si\nkâˆ’1)andp(Ak|si\nk)in the numerator are the state\ntransition model and observation model, respectively. New\nscore position and beat interval values are drawn at each step\nfrom the proposal distribution q(si\nk|si\nkâˆ’1,Ak).\n3.2 Drawing particles from the proposal distribution\nParticles are drawn from the proposal distribution in Eq. (6).\nFirst, a new beat interval bi\nkis drawn, then a new score po-\nsition pi\nkis drawn depending on the drawn bi\nk. The proposal\nis designed to draw (1) a beat interval that lies in the tempo\nrange provided by the score and that matches the intervals\namong audio onsets and (2) a score position that matches the\nincrease of the audio amplitude with the score onset frame.\nsi\nkâˆ¼q(b,p|si\nkâˆ’1,Ak)\nâˆR(b;Ak)Î¨(b;Ëœb)Ã—Q(p;b,Ak,si\nkâˆ’1). (6)\nR(b;Ak)andÎ¨(b;Ëœb)denote the normalized cross correla-\ntion of the audio signal and the window function that limits\nthe range of the beat interval, respectively. Q(p;b,Ak,si\nkâˆ’1)\ndenotes the onset matching function.Detailed equations are\nexplained in [13].\nThe onset matching function Q(p;b,Ak,si\nkâˆ’1)in Eq. (6)\nrepresents how well the audio and score are aligned in terms\nof the onsets. Figure 6 explains the design. The top case in\n2Î´(x) =1 iffx=0, otherwise Î´(x) =0.which audio frames with a peak power is aligned with score\nonsets results in the larger Q, where as the bottom case Qis\na small value since the onsets are misaligned. The detailed\nmathematical expressions are presented in [13].\n3.3 Weight calculation\nThe weight for each particle is calculated with the sampled\nvalue si\nkin Eq. (5) by using the state transition model,\np(si\nk|si\nkâˆ’1) = N(pi\nk|Ë†pi\nk,Ïƒ2\np)Ã—N(bi\nk|bi\nkâˆ’1,Ïƒ2\nb),(7)\nand the observation model,\np(Ak|si\nk)âˆp(Ak|pi\nk)Ã—R(bi\nk;Ak). (8)\nThe score position transition conforms to a linear Gaussian\nmodel with the transition Ë† pi\nk=pi\nkâˆ’1+âˆ†T/bi\nkâˆ’1and the vari-\nanceÏƒ2\np(beat2). The beat interval transition is a random\nwalk model with the variance Ïƒ2\nb(sec2/beat2). The variances\nare empirically set as Ïƒ2\np=0.25 and Ïƒ2\nb=0.1, respectively.\nFor the observation of the beat interval, the normalized\ncross-correlation of the audio spectrogram, R(bi\nk;Ak), is again\nused in Eq. (8). The other factor p(Ak|pi\nk)is the likeli-\nhood corresponding to the pitch information. As explained\nin Section 2, the GMM-based harmonic structures are used\nto match the audio and score. First, the matching with KL-\ndiv is presented as a baseline where all the GMM parame-\nters, the chord mixture ratio Ï€or harmonic heights Î¸, and\nthe Gaussian width Î», are ï¬xed. Then, we explain two types\nof LHA-based audio-to-score matchings, the full LHA and\nmixture LHA, where the GMM parameters are probability\nvariables that ï¬‚exibly adapt to the observed audio harmonic\nstructure. Full LHA adapts all Ï€,Î¸, and Î»whereas mixture\nLHA adapts only Ï€andÎ»to the audio. Further discussion\nof the difference is in Section 4.\nThe KL-div matching uses a normalized amplitude spec-\ntrogram Â´Akwhile the LHA models use the quantized spec-\ntrogram Â¯Ak. To match the buffered audio, the audio spectro-\ngram AkorÂ¯Akis aligned with the score shown as Figure 7.\nAs the time kâˆ†Tis assigned to pi\nkwith the beat interval bi\nk,\nthe audio frame Ï„is linearly assigned to the score frame as\ngiven by\nËœp(Ï„) = pi\nkâˆ’(kâˆ†Tâˆ’Ï„)/bi\nk. (9)\n52812th International Society for Music Information Retrieval Conference (ISMIR 2011)\n3.3.1 Harmonic structure observation based on KL-div\nFor each score frame p, the GMM template of the harmonic\nstructure is generated from the musical notes Âµpas:\nË†Ap,f=Lp\nâˆ‘\nl=1M\nâˆ‘\nm=1CharmÏ€lÎ¸mN(f|gÂµl\np,Ïƒ2\nKL) +Cï¬‚oor,(10)\nwhere Lpis the number of notes at pand the number of har-\nmonic structures Mis 10. The ratio of each note is equally\nset as Ï€l=1/Lp. The height of the mth harmonic is set\nasÎ¸m=0.2mâˆ’1. The variance is set as Ïƒ2\nKL=2.4, derived\nfrom the window function used in STFT. Cï¬‚ooris a ï¬‚oor-\ning constant to ensure Ë†Ap,f>0 and avoid zero-divides in\nEq. (11). Charm =0.95 and Cï¬‚ooris set such that the har-\nmonic structure template is normalized as Ë†Ap,Â·=1. The\nsubscript Â·means a summation over the replaced index.\nHere, the audio likelihood using KL-div is deï¬ned as\nlogp(Ak|si\nk) = âˆ’âˆ‘\nÏ„âˆˆTkâˆ‘\nfÂ´AÏ„,flogÂ´AÏ„,f\nË†AËœp(Ï„),f,(11)\nwhere Â´AÏ„,f=AÏ„,f/AÏ„,Â·is the normalized amplitude. The\nright-hand side of Eq. (11) is a negative KL-div between the\naudio harmonic structure and the GMM harmonic template.\n3.3.2 LHA-based likelihood calculation\nWe ï¬rst explain how LHA is used as the likelihood, then\nshow the iterations for both full and mixture LHA infer-\nences. The quantized amplitudes Â¯Akare regarded as a his-\ntogram of amplitudes over frequency bins Xillustrated as\ngray bars in Figure 2. The rigorous likelihood in Eq. (5) is\np(X|si\nk) =âˆ‘\nZâˆ«âˆ«âˆ«\np(X,Z,Ï€,Î¸,Î›|pi\nk,Âµ)dÏ€dÎ¸dÎ›.(12)\nSince this analytical summation over Zis intractable3, we\ninfer the latent variables Z,Ï€,Î¸, andÎ›by variational Bayes\n(VB) method under the factorization assumption q(Z,Ï€,Î¸,Î›) =\nq(Z)q(Ï€,Î¸,Î›). We use the variational lower bound for the\nweight calculation as an approximate observation model in-\nstead of Eq. (12),\nlogp(X|si\nk)â‰ˆL(q) =EZ,Ï€,Î¸,Î›[\nlogp(X,Z,Ï€,Î¸,Î›|pi\nk,Âµ)\nq(Z,Ï€,Î¸,Î›)]\n,(13)\nwhere EZ,Ï€,Î¸,Î›[Â·]denotes an expectation over q(Z,Ï€,Î¸,Î›).\nFor the inference of LHA, the audio is segmented by the\nchord in the score as shown in Figure 8. This segmentation\ndis made on the basis of the alignment by Eq. (9).\nThe variational lower bound in Eq. (13) is maximized\nwith the following variational posteriors:\nq(Z) =âˆ\ndnlmÎ³zdnlm\ndnlm,q(Ï€) =âˆ\ndDir(Ï€d|Î±d),\nq(Î¸) =âˆ\nlDir(Î¸l|Î²l),q(Î›) =âˆ\nlGam (Î›l|al,bl),\nthe parameters of which are updated as\nÎ³dnlm =Ïdnlm/ÏdnÂ·Â·, (14)\nlogÏdnlm=Ïˆ(Î±dl)âˆ’Ïˆ(Î±dÂ·) +Ïˆ(Î²lm)âˆ’Ïˆ(Î²lÂ·)\n+Ïˆ(al)/2âˆ’(logbl)/2âˆ’(xnâˆ’mÂµl)2al/2bl,(15)\n3The integration over Ï€,Î¸, andÎ›is tractable thanks to their conjugacy.Î±dl=Î±0l+Î³dÂ·lÂ·,Î²lm=Î²0m+Î³Â·Â·lm,\nal=a0+Î³Â·Â·lÂ·\n2,bl=b0+âˆ‘dnmÎ³dnlm(xdnâˆ’mÂµl)2\n2,(16)\nwhere Ïˆ(Â·)in Eq. (15) denotes the digamma function. Eqs. (14,15)\nand Eqs. (16) are iteratively calculated until the lower bound\nin Eq. (13) converges. Note that Eq. (14) is the normaliza-\ntion of Ïover indices landm.\nMixture LHA update : In the update for the mixture LHA\nmodel, the harmonic height parameter is set as Î¸lm=0.2mâˆ’1.\nThus, the update equations are modiï¬ed as:\nlogÏdnlm=Ïˆ(Î±dl)âˆ’Ïˆ(Î±dÂ·) +logÎ¸lm\nÏˆ(al)/2âˆ’(logbl)/2âˆ’(xnâˆ’mÂµl)2al/2bl,(17)\nÎ±dl=Î±0l+Î³dÂ·lÂ·,\nal=a0+Î³Â·Â·lÂ·\n2,bl=b0+âˆ‘dnmÎ³dnlm(xdnâˆ’mÂµl)2\n2.(18)\nRelationship with the KL-div likelihood : Remember the\nnegative KL-div is used as the log-likelihood in Eq. (11).\nThe following equation always holds during the iterations:\nL(q) +KL(q||p) =logp(X|si\nk) (const wrt .q).\nThe KL-div is deï¬ned between the approximate distribu-\ntionq(Z,Ï€,Î¸,Î›)and the true posterior p(Z,Ï€,Î¸,Î›|X,pi\nk,Âµ).\nNote that maximizing L(q)is equivalent to minimizing KL-\ndiv, namely, maximizing the negative KL-div due to the\nequation above. Thus, the LHA-based likelihood is inter-\npreted as an extension of Eq. (11) in that the harmonic tem-\nplates adapt to the audio observation to minimize the KL-div\nand maximize the log-likelihood.\n3.4 Point estimation and efï¬cient computing\nAfter the weights of all particles are calculated, the point\nestimation is reported as Ë†sk=âˆ‘I\ni=1wi\nksi\nk/\nâˆ‘I\ni=1wi\nk. Particles\nare resampled after the point estimation procedure to elimi-\nnate zero-weight particles. The resampling probability is in\nproportion to the weight of each particle [1].\n4. EXPERIMENTAL RESULTS\nThis section presents the alignment error of three observa-\ntion models; conventional KL-div [13], full LHA, and mix-\nture LHA. Twenty songs from RWC Jazz music database [8]\nis used for this experiment. This test set includes various\ncompositions of musical instruments from solo performance\nto big band ensembles. Our system is implemented on Linux\nOS and a 2.4 (GHz) processor. Experiments are carried out\nwith the following parameter settings; the ï¬ltering interval\nâˆ†T=0.5 (sec), the window length for the audio processing\nW=1.5 (sec), and the number of particles I=300.\nFigure 9 shows the error percentiles of 20 songs for three\nmethods. Black, red, and blue bars represent the percentiles\nof KL-div, full LHA, and mixture LHA, respectively. The\ndarkest bars are the 50% percentiles, middle bars are the\n75%, and the lightest segments are the 100% percentiles.\nThe less values indicate the better performance. Songs with\na larger ID tend to involve more instruments. Both of the\n529Poster Session 4\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 02.5 57.5 10 12.5 15 17.5 \nSong ID Error percentile (sec) \n  \nKLâˆ’div \nfull LHA \nmixture LHA Figure 9 . Error percentiles for 20 songs\nLHA-based methods outperform the KL-div for 10 songs,\nand either full or mixture LHA shows less errors than KL-\ndiv for 3 songs. In particular, LHA-based methods tend to\nreport less errors when the song consists of a larger number\nof instruments (larger ID songs). This is what we expect\nfrom the LHA models.\nTwo major reasons are given why LHA-based observa-\ntion models still accumulate the alignment error. First, LHA\nis vulnerable to rest notes where no musical note is speci-\nï¬ed. This is because the LHA model penalizes unspeciï¬ed\nharmonic peaks. When the score provides a rest, LHA pe-\nnalizes any audio observation. The error caused by these rest\nnotes is seen in songs 2, 5, 8, and 9, where we have more\nchances to have rest notes because the number of musical\ninstruments is relatively small. The second reason is the\nnon-harmonic feature of percussions and drums. Because\ndrum sounds are loud and outstanding in the ensemble, these\nsounds interfere the harmonic structures of pitched sounds\nassumed by LHA. This case applies in songs 11, 16, and 17\nwhere drums are included in the ensemble.\nHere we discuss the difference between the full and mix-\nture LHAs. Since mixture LHA has less variables to in-\nfer, we can expect more accurate inference as long as the\nï¬xed parameters Î¸ï¬t the observation. The ï¬xed Î¸declines\nas the frequency becomes larger. This descending height is\nwell observed in stringed instruments such as guitar or piano\ndominantly used in songs 1-6; whereas wind instruments\nsuch as saxophone or ï¬‚ute or bowed instruments such as vi-\nolin show rather different peaks. When these instruments\nare dominant in a song, e.g., songs 16 and 17 which are in a\nbig band style, the full LHA will be the better choice.\n5. CONCLUSION AND FUTURE WORKS\nThe experiment has shown that LHA is especially effec-\ntive in a large-ensemble situation where more musical notes\nare simultaneously performed. However, LHA-based audio\nobservation models is disturbed by (1) rest notes and (2)\ndrum sounds. To make the best use of the LHA model, one\npromising solution is to examine the musical score in ad-\nvance of the alignment whether the expecting audio signal\nis suitable for LHA. The development of this top-level deci-sion making process will be one of the future works.\nAnother future work includes an accelerated calculation\nof LHA iterations for such real-time applications as auto-\nmatic accompaniment systems. Current implementation re-\nquires approximately 10 seconds to process one-second au-\ndio data.\n6. REFERENCES\n[1] M. Arulampalam, S. Maskell, N. Gordon, and T. Clapp. A Tutorial on\nParticle Filters for Online Nonlinear/Non-Gaussian Bayesian Tracking.\nIEEE Transactions on Signal Proc. , 50(2):174â€“189, 2002.\n[2] A. Arzt, G. Widmer, and S. Dixon. Automatic Page Turning for Mu-\nsicians via Real-Time Machine Listening. In Proc. of the European\nConference on Artiï¬cial Intelligence , pages 241â€“245, 2008.\n[3] A. Cont. A Coupled Duration-Focused Architecture for Realtime Mu-\nsic to Score Alignment. IEEE Transactions on Pattern Analysis and\nMachine Intelligence , 32(6):974â€“987, 2010.\n[4] A. Cont, D. Schwarz, and N. Schnell. Training IRCAMâ€™s score fol-\nlower. In AAAI Fall Symposium on Style and Meaning in Art, Language\nand Music , 2004.\n[5] R. Dannenberg and C. Raphael. Music Score Alignment and Computer\nAccompaniment. Comm. ACM , 49(8):38â€“43, 2006.\n[6] S. Dixon. An On-line Time Warping Algorithm for Tracking Musical\nPerformances. In Proc. of the IJCAI , pages 1727â€“1728, 2005.\n[7] Z. Duan and B. Pardo. A STATE SPACE MODEL FOR ONLINE\nPOLYPHONIC AUDIO-SCORE ALIGNMENT. In Proc. of Intâ€™l Conf.\non Acoustics, Speech and Signal Processing , pages 197â€“200, 2011.\n[8] M. Goto. AIST Annotation for RWC Music Database. In Proc. of IS-\nMIR, pages 359â€“360, 2006.\n[9] T. Kitahara, M. Goto, K. Komatani, T. Ogata, and H. G. Okuno. Instru-\nment Identiï¬cation in Polyphonic Music: Feature Weighting to Mini-\nmize Inï¬‚uence of Sound Overlaps. EURASIP Journal on Applied Sig-\nnal Processing , vol. 2007, 2007. Article ID 51979.\n[10] A. Kulapuri. Multipitch Analysis of Polyphonic Music and Speech Sig-\nnals Using an Auditory Model. IEEE Transactions on Audio, Speech\nand Language Processing , 16(2):255â€“266, 2007.\n[11] N. Montecchio and A. Cont. A UNIFIED APPROACH TO REAL\nTIME AUDIO-TO-SCORE AND AUDIO-TO-AUDIO ALIGNMENT\nUSING SEQUENTIAL MONTECARLO INFERENCE TECH-\nNIQUES. In Proc. of Intâ€™l Conf. on Acoustics, Speech and Signal Pro-\ncessing , pages 193â€“196, 2011.\n[12] N. Orio, S. Lemouton, and D. Schwarz. Score Following: State of the\nArt and New Developments. In Proc. of Intâ€™l Conf. on New Interfaces\nfor Musical Expression , pages 36â€“41, 2003.\n[13] T. Otsuka, K. Nakadai, T. Takahashi, K. Komatani, T. Ogata, and H. G.\nOkuno. Real-Time Audio-to-Score Alignment using Particle Filter for\nCo-player Music Robots. EURASIP Journal of Advances in Signal Pro-\ncessing , vol. 2011, 2011. Article ID 384651.\n[14] C. Raphael. Aligning music audio with symbolic scores using a hybrid\ngraphical model. Machine Learning , 65(2â€“3):389â€“409, 2006.\n[15] K. Yoshii and M. Goto. Inï¬nite Latent Harmonic Allocation: A Non-\nparametric Bayesian Approach to Multipich Analysis. In Proc. of IS-\nMIR, pages 309â€“314, 2010.\n530"
    },
    {
        "title": "l1-Graph Based Music Structure Analysis.",
        "author": [
            "Yannis Panagakis",
            "Constantine Kotropoulos",
            "Gonzalo R. Arce"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1417335",
        "url": "https://doi.org/10.5281/zenodo.1417335",
        "ee": "https://zenodo.org/records/1417335/files/PanagakisKA11.pdf",
        "abstract": "An unsupervised approach for automatic music structure analysis is proposed resorting to the following assumption: If the feature vectors extracted from a specific music segment are drawn from a single subspace, then the sequence of feature vectors extracted from a music recording will lie in a union of as many subspaces as the music segments in this recording are. It is well known that each feature vector stemming from a union of independent linear subspaces admits a sparse representation with respect to a dictionary formed by all other feature vectors with nonzero coefficients associated only to feature vectors that stem from its own subspace. Such sparse representation reveals the relationships among the feature vectors and it is used to construct a similarity graph, the so-called â„“1-graph. Accordingly, the segmentation of audio features is obtained by applying spectral clustering to the â„“1-graph. The performance of the just described approach is assessed by conducting experiments on the PopMusic and the UPF Beatles benchmark datasets. Promising results are reported.",
        "zenodo_id": 1417335,
        "dblp_key": "conf/ismir/PanagakisKA11",
        "keywords": [
            "unsupervised",
            "automatic",
            "music",
            "structure",
            "analysis",
            "subspace",
            "sparse",
            "representation",
            "spectral",
            "clustering"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nâ„“1-GRAPH BASED MUSIC STRUCTURE ANALYSIS\nYannis Panagakis Constantine Kotropoulos\nDept. of Informatics\nAristotle University of Thessaloniki\nBox 451 Thessaloniki, GR-54124, Greece\n{panagakis,costas }@aiia.csd.auth.grGonzalo R. Arce\nDept. of Electrical & Computer Engineering\nUniversity of Delaware\nNewark, DE 19716-3130, U.S.A.\narce@ece.udel.edu\nABSTRACT\nAn unsupervised approach for automatic music structure anal-\nysis is proposed resorting to the following assumption: If\nthe feature vectors extracted from a speciï¬c music segment\nare drawn from a single subspace, then the sequence of fea-\nture vectors extracted from a music recording will lie in a\nunion of as many subspaces as the music segments in this\nrecording are. It is well known that each feature vector stem-\nming from a union of independent linear subspaces admits\na sparse representation with respect to a dictionary formed\nby all other feature vectors with nonzero coefï¬cients associ-\nated only to feature vectors that stem from its own subspace.\nSuch sparse representation reveals the relationships among\nthe feature vectors and it is used to construct a similarity\ngraph, the so-called â„“1-graph . Accordingly, the segmenta-\ntion of audio features is obtained by applying spectral clus-\ntering to the â„“1-graph. The performance of the just described\napproach is assessed by conducting experiments on the Pop-\nMusic and the UPF Beatles benchmark datasets. Promising\nresults are reported.\n1. INTRODUCTION\nA music signal carries a highly structured information at\nseveral levels. At the lowest level, a structure is deï¬ned\nby the individual notes, their timbral characteristics, as well\nas their pitch and time intervals. At an intermediate level,\nthe notes build relatively longer structures, such as melodic\nphrases, chords, and chord progressions. At the highest\nlevel, the structural description of an entire music recording\n(i.e., its musical form) emerges at the time scale of music\nsections, such as intro, verse, chorus, bridge, and outro [16,\n17].\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proï¬t or commercial advantage and that copies\nbear this notice and the full citation on the ï¬rst page.\ncâƒ2011 International Society for Music Information Retrieval.The musical form of a recording is a high-level informa-\ntion that can be exploited in several music information re-\ntrieval (MIR) tasks, including music thumbnailing and sum-\nmarization [3], chord transcription [12], music semantics\nlearning and music annotation [1], song segment retrieval\n[1], and remixing [9]. Consequently, the interest in the au-\ntomatic music form extraction orstructure analysis has in-\ncreased as is manifested by the considerably amount of re-\nsearch that has been done so far [1,9,10,16,19]. For a com-\nprehensive review the interested reader is referred to [6, 17]\n(and the references therein). The majority of methods tested\nfor automatic music structure analysis applies a signal pro-\ncessing stage followed by a representation stage. In the ï¬rst\nstage, low-level feature sequences are extracted from the au-\ndio signal in order to model its timbral, melodic, and rhyth-\nmic content [17]. This is consistent with the ï¬ndings of\nBruderer et al. , who state that the perception of structural\nboundaries in popular music is mainly inï¬‚uenced by the\ncombination of changes in timbre, tonality, and rhythm over\nthe music piece [2]. At the representation stage, a recur-\nrence plot or a similarity matrix is analyzed in order to iden-\ntify repetitive patterns in the feature sequences by employ-\ning hidden Markov models, clustering methods, etc. [6, 17].\nIn this paper, an unsupervised approach for automatic\nmusic structure analysis is proposed. To begin with, each\naudio recording is represented by a sequence of audio fea-\ntures capturing the variations between the different music\nsegments. Since the music structure is strongly determined\nby repetition, a similarity matrix should be constructed, that\nwill be analyzed next. Here, the similarity matrix is built\nby adopting an one-to-all sparse reconstruction rather than\none-to-one (i.e., pairwise) comparisons. To this end, the\nâ„“1-graph [5] is constructed in order to capture relationships\namong the feature vectors. The segmentation of audio fea-\ntures is obtained by applying spectral clustering to the â„“1-\ngraph. Apart from the conventional mel-frequency cepstral\ncoefï¬cients andchroma features, frequently employed in mu-\nsic structure analysis, the auditory temporal modulations\nare also tested here. The performance of the proposed ap-\nproach is assessed by conducting experiments on two man-\n495Poster Session 4\nually annotated benchmark datasets, namely the PopMu-\nsic [10] and the UPF Beatles. The experimental results val-\nidate the effectiveness of the proposed approach in music\nstructure analysis reaching the performance of the state-of-\nthe-art music structure analysis methods.\nThe remainder of the paper is as follows. In Section 2,\nthe audio features employed are brieï¬‚y described. The â„“1-\ngraph based music structural analysis framework is detailed\nin Section 3. Datasets, evaluation metrics, and experimental\nresults are presented in Section 4. Conclusions are drawn\nand future research directions are indicated in Section 5.\n2. AUDIO FEATURE REPRESENTATION\nEach 22.050-Hz sampled monaural waveform is parameter-\nized by employing three audio features in order to capture\nthe variations between different music segments. The fea-\nture set includes the auditory temporal modulations (ATMs),\nthemel-frequency cepstral coefï¬cients (MFCCs), and the\nchroma features.\n1) Auditory temporal modulations: ATMs are obtained\nby modeling the path of human auditory processing. They\ncarry important time-varying information of the music sig-\nnal [15]. First, by modeling the early auditory system, the\nacoustic signal is converted into a time-frequency distribu-\ntion along a logarithmic frequency axis, the so-called au-\nditory spectrogram . In this paper, the early auditory sys-\ntem is modeled by employing the Lyonsâ€™ passive ear model\n[11]. The derived auditory spectrogram consists of 96fre-\nquency channels ranging from 62Hz to 11kHz. The audi-\ntory spectrogram is then downsampled along the time axis\nby a factor of 150ms, which allows to focus on a more\nmeaningful time-scale for music structural analysis. The\nunderlying temporal modulations of the music signal are de-\nrived by applying a wavelet ï¬lter along each temporal row\nof the auditory spectrogram for a set of 8discrete rates r\nâˆˆ {2,4,8,16,32,64,128,256}Hz ranging from slow to\nfast temporal rates [15]. Consequently, the entire auditory\nspectrogram is modeled by a three-dimensional representa-\ntion of frequency, rate, and time, which is then unfolded\nalong the time-mode in order to obtain a sequence of two-\ndimensional ATM features.\n2) Mel-frequency cepstral coefï¬cients: MFCCs param-\neterize the rough shape of spectral envelope [13] and thus\nencode the timbral properties of the music signal, which\nare closely related to the perception of music structure [2].\nFollowing [16], the MFCCs calculation employs frames of\nduration 92.9ms with a hope size of 46.45ms, and a 42-\nband ï¬lter bank. The correlation between frequency bands\nis reduced by applying the discrete cosine transform along\nthe log-energies of the bands. The lowest coefï¬cient (i.e.,\nzero-th order) is discarded and the subsequent 12coefï¬-\ncients form the feature vector that undergoes a zero-meannormalization.\n3) Chroma: Chroma features are adept in characteriz-\ning the harmonic content of the music signal by projecting\nthe entire spectrum onto 12bins representing the 12 distinct\nsemitones (or chroma) of a musical octave [13]. They are\ncalculated using 92.9ms frames with a hope size of 23.22\nms as follows. First, the salience for different fundamental\nfrequencies in the range 80âˆ’640Hz is calculated. The lin-\near frequency scale is transformed into a musical one by se-\nlecting the maximum salience value in each frequency range\ncorresponding to one semitone. Finally, the octave equiva-\nlence classes are summed over the whole pitch range to yield\na12-dimensional chroma vector.\nAll the aforementioned features are averaged over the\nbeat (i.e., the basic unit of time in music) frames by em-\nploying the beat tracking algorithm described in [8]. Thus a\nsequence of beat-synchronous feature vectors is obtained.\n3. MUSIC STRUCTURE SEGMENTATION BASED\nON THE â„“1-GRAPH\nSince repetition governs the music structure, a common strat-\negy employed is to compare each feature vector of the music\nrecording with all other vectors in order to detect similari-\nties. Let a given audio recording be represented by a feature\nsequence of Nbeat frames, i.e., {x1,x2, . . . ,xN}. The\nsimilarity between the feature vectors is frequently mea-\nsured by constructing the self-similarity matrix (SDM) Dâˆˆ\nRNÃ—Nwith elements dij=d(xi,xj),i, jâˆˆ {i,2, . . . , N },\nwhere d(Â·,Â·)is a suitable distance metric [9, 16, 17]. Com-\nmon distance metrics are the Euclidean, dE(xi,xj) =âˆ¥xiâˆ’\nxjâˆ¥2and the cosine distance, dC(xi,xj) = 0 .5(1âˆ’xT\nixj\nâˆ¥xiâˆ¥2âˆ¥xjâˆ¥2,\nwhere âˆ¥.âˆ¥2denotes the â„“2vector norm. However, the afore-\nmentioned approach suffers from two drawbacks: 1) It is\nvery sensitive to noise, since the employed distance metrics\nare not robust to noise. 2) The resulting SDM is dense and\nthus it cannot provide the locality information (i.e., to re-\nveal the relationships among neighbor feature vectors that\nbelong to the same segment class), which is valuable in the\nproblem under study.\nIn order to alleviate the aforementioned drawbacks, we\npropose to measure the similarities between the feature vec-\ntors in an one-to-all sparse reconstruction manner rather\nthan to employ the conventional one-to-one distance appro-\nach by exploiting recent ï¬ndings in sparse subspace cluster-\ning [7].\nFormally, let a given audio recording of Kmusic seg-\nments be represented by a sequence of Naudio feature vec-\ntors of size M, i.e.,X= [x1|x2|. . .|xN]âˆˆRMÃ—N. By\nassuming that the feature vectors belonging to the same mu-\nsic segment lie into the same subspace, the columns of X\nare drawn from a union of Kindependent linear subspaces\nof unknown dimensions. It has been proved that if a feature\n49612th International Society for Music Information Retrieval Conference (ISMIR 2011)\nvector stems from a union of independent linear subspaces,\nit admits a sparse representation with respect to a dictio-\nnary formed by all other feature vectors with the nonzero\ncoefï¬cients associated to vectors drawn from its own sub-\nspace [7]. Therefore, by seeking the sparsest linear combi-\nnation, the relationship with the other vectors lying in the\nsame subspace is revealed automatically. A similarity graph\nbuilt from this sparse representation, the so-called â„“1-graph\n[5] is used then in order to segment the columns of Xinto\nKclusters by applying spectral clustering.\nLetXi= [x1|x2|. . .|xiâˆ’1|xi+1|. . .|xN]âˆˆRMÃ—(Nâˆ’1).\nThe sparsest solution of xi=Xiccan be found by solving\nthe optimization problem:\nargmin\ncâˆ¥câˆ¥0subject to xi=Xic, (1)\nwhere âˆ¥.âˆ¥0is the â„“0quasi-norm returning the number of the\nnon-zero entries of a vector. Finding the solution to the opti-\nmization problem (1) is NP-hard due to the nature of the un-\nderlying combinational optimization. An approximate solu-\ntion to the problem (1) can be obtained by replacing the â„“0\nnorm with the â„“1norm as follows:\nargmin\ncâˆ¥câˆ¥1subject to xi=Xic, (2)\nwhere âˆ¥.âˆ¥1denotes the â„“1norm of a vector. It is well known\nthat if the solution is sparse enough and M << (Nâˆ’\n1), then the solution of (1) is equivalent to the solution of\n(2). The optimization problem (2) can be solved in poly-\nnomial time by standard linear programming methods [4].\nThe well-posedness of (2) relies on the condition M <<\n(Nâˆ’1), i.e., the sample size must be much larger than the\nfeature dimension. If the ATMs are used to represent au-\ndio, the sample size (i.e., the number of beats) is not much\nlarger than the feature vector dimension and thus the just-\nmentioned condition is violated, because M= 768 and\nNâ‰ˆ500on average in the experiments conducted. Ac-\ncordingly, cis no longer sparse. To alleviate this problem,\nit has been proposed to augment Xiby an MÃ—Midentity\nmatrix and to solve:\nargmin\ncâˆ¥câˆ¥1subject to xi=Bc, (3)\ninstead of (2), where B= [Xi|I]âˆˆRMÃ—((Nâˆ’1)+M)[20].\nSince the sparse coefï¬cient vector creveals the relation-\nships among xiand the feature vectors in Xi, the overall\nsparse representation of the whole feature sequence Xcan\nbe summarized by constructing the weight matrix Wusing\nAlgorithm 1. Wcan be used to deï¬ne the so-called â„“1-graph\n[5]. The â„“1-graph is a directed graph G= (V,E), where\nthe vertices of graph Vare the Naudio feature vectors and\nan edge (ui, uj)âˆˆEexists, whenever xjparticipates in\nthe sparse representation of xi. Accordingly, the adjacencyAlgorithm 1 â„“1-Graph Construction [5].\nInput: Audio feature sequence XâˆˆRMÃ—N.\nOutput: Weight matrix WâˆˆRNÃ—N.\n1:fori= 1â†’Ndo\n2:B= [Xi|I].\n3: argmincâˆ¥câˆ¥1subject to xi=Bc.\n4: forj= 1â†’Ndo\n5: ifj < i then\n6: wij=cj.\n7: else\n8: wij=cjâˆ’1.\n9: end if\n10: end for\n11:end for\nmatrix of GisW. Unlike the conventional SDM, the adja-\ncency matrix Wis robust to noise. The â„“1-graphGis an un-\nbalanced digraph. A balanced graph Ë†Gcan be built with ad-\njacency matrix Ë†Wwith elements Ë†wij= 0.5 (|wij|+|wji|),\nwhere |.|denotes the absolute value. Ë†Wis still a valid rep-\nresentation of the similarity between the features vectors,\nsince if xican be expressed as a compact linear combina-\ntion of some feature vectors including xj(all from the same\nsubspace or music segment here), then xjcan also be ex-\npressed as a compact linear combination of feature vectors\nin the same subspace including xi[7]. In Figure 1, the Ë†Wis\ndepicted for the three features tested. It can be seen that Ë†W\nhas a block structure for the ATMs, while it is unstructured\nand more dense for the MFCCs and the Chroma features.\nThis observation validates that the main assumptions made\nin the paper hold here for the ATMs, but not for the MFCCs\nand the Chroma features.\nThe segmentation of the audio feature vectors can be ob-\ntained by spectral clustering algorithms, such as the normal-\nized cuts [18] as illustrated in Algorithm 2.\nAlgorithm 2 Music Segmentation via â„“1-Graph.\nInputs: Audio feature sequence XâˆˆRMÃ—Nand number\nof segments K.\nOutput: Audio feature sequence segmentation.\n1:Obtain the adjacency matrix Wofâ„“1-graph by\nAlgorithm 1.\n2:Build the symmetric adjacency matrix of the â„“1-graph\nË†G:Ë†W= 0.5Â·(|W|+|WT|).\n3:Employ normalized cuts [18] to segment the vertices of\nË†GintoKclusters.\n497Poster Session 4\nFigure 1 . The adjacency matrix Ë†Wof the â„“1-graph for the song â€œ I saw her standing there â€ by The Beatles for (a) the ATMs,\n(b) the MFCCs, and (c) the Chroma features.\n4. EXPERIMENTAL EVALUATION\nThe performance of the proposed music structure analysis\napproach is assessed by conducting experiments on two man-\nually annotated datasets of Western popular music pieces.\nSeveral evaluation metrics are employed to assess system\nperformance from different points of view.\n4.1 Datasets\nPopMusic dataset [10]: The dataset consists of 60music\nrecordings of rock, pop, hip-hop, and jazz. Half of the\nrecordings originate from a variety of well-known artists\nappeared the past 40years, including Britney Spears, Em-\ninem, Madonna, Nirvana, etc. This subset is abbreviated as\nRecent hereafter. The remaining 30music recordings are by\nThe Beatles. The ground-truth segmentation of each song\ncontains between 2and15different segments classes. The\nnumber of classes is 6, while each recording is found to con-\ntain11segments on average [1,10]. The subset contains the\nBeatles recordings is referred ta as Beatles .\nUPF Beatles dataset :1The dataset consists of 174\nsongs by The Beatles that are annotated by the musicolo-\ngist Alan W. Pollack. Segmentation time stamps were in-\nserted at Universitat Pompeu Fabra (UPF) as well. Each\nmusic recording contains on average 10segments from 5\nunique classes [19]. Since all the recordings are from the\nsame band, there is less variation in the music style and the\ntimbral characteristics than the other datasets.\n4.2 Evaluation Metrics\nFollowing [1,9,10,16,19], the segment labels are evaluated\nby employing the pairwise F-measure, which is one of the\nstandard metrics of clustering quality. It compares pairs of\n1http://www.dtic.upf.edu/ perfe/annotations/sections/license.htmlbeats, which are assigned to the same cluster by music struc-\nture analysis against the reference segmentation. Let FAbe\nthe set of similarly labeled pairs of beats in a recording ac-\ncording to the music structure analysis algorithm and FHbe\nthe set of similarly labeled pairs in the human reference seg-\nmentation. The pairwise precision, Ppairwise , the pairwise\nrecall, Rpairwise , and the pairwise F-measure, Fpairwise ,\nare deï¬ned as follows: Ppairwise =|FAâˆ©FH|\n|FA|,Rpairwise =\n|FAâˆ©FH|\n|FH|, and Fpairwise = 2Â·Ppairwise Â·Rpairwise\nPpairwise +Rpairwise, where |.|\ndenotes the set cardinality. The average number of segments\nper song in each dataset is reported as well.\nThe segment boundary detection is evaluated separately\nby employing the standard precision, recall, and F-measure.\nFollowing [1, 10, 16], a boundary detected by the proposed\napproach is considered correct, if it falls within some ï¬xed\nsmall distance Î´away from the reference boundary. Each\nreference boundary can be retrieved by at most one out-\nput boundary. Let BAandBHdenote the sets of segment\nboundaries according to the music structure analysis algo-\nrithm and the human reference, respectively. Then, P=\n|BAâˆ©BH|\n|BA|,R=|BAâˆ©BH|\n|BH|, and F= 2Â·PÂ·R\nP+R. The parameter Î´\nis set to 3s in our experiments as was also done in [1,10,16].\n4.3 Experimental Results\nThe structural segmentation is obtained by applying the pro-\nposed approach to various feature sequences. Following the\nexperimental setup employed in [1,9,10,16,19], the number\nof clusters Kwas set to 6for the PopMusic dataset, while\nK= 4 for the UPF Beatles dataset. For comparison pur-\nposes, experiments are conducted by applying the normal-\nized cuts [18] apart from the â„“1-graph and the SDM with the\nEuclidean distance computed for the three audio features.\nThe segment-type labeling performance for the PopMusic\nand the UPF Beatles datasets is summarized in Table 1 and\n49812th International Society for Music Information Retrieval Conference (ISMIR 2011)\nTable 2, respectively.\nMethod/Reference Dataset Fpairwise Av. Number of Segments\nBeatles 0.6140 8.8333\nATM + â„“1-graph Recent 0.5885 12.6087\nbased segmentation PopMusic 0.5912 11.8679\nBeatles 0.4029 199.3667\nMFCCs + â„“1-graph Recent 0.3884 248.2826\nbased segmentation PopMusic 0.3966 239.6316\nBeatles 0.4191 153.7667\nChroma + â„“1-graph Recent 0.3520 260.3043\nbased segmentation PopMusic 0.3900 200\nBeatles 0.4243 145.7000\nATM + SDM Recent 0.3975 141.3913\nbased segmentation PopMusic 0.4027 125.5283\nBeatles 0.3664 226.3667\nMFCCs + SMD Recent 0.3663 305.9130\nbased segmentation PopMusic 0.3664 260.8868\nBeatles 0.3499 220.4333\nChroma + SDM Recent 0.3312 276.1739\nbased segmentation PopMusic 0.3418 244.6226\nMFCCs unconstrained [1] PopMusic 0.577 17.9\nMFCCs constrained [1] PopMusic 0.620 10.7\nChroma constrained [1] PopMusic 0.51 12\nBeatles 0.425 N/A\nK-means Recent 0.457 N/A\nclustering [10] PopMusic 0.441 N/A\nBeatles 0.538 N/A\nMean-ï¬eld Recent 0.560 N/A\nclustering [10] PopMusic 0.549 N/A\nBeatles 0.604 N/A\nConstrained Recent 0.605 N/A\nclustering [10] PopMusic 0.603 N/A\nTable 1 . Segment-type labeling performance on the Pop-\nMusic dataset.\nBy inspecting Tables 1 and 2, it is clear that the â„“1-graph\nbased segmentation outperforms the SDM based segmenta-\ntion in terms of pairwise F-measure for all the audio fea-\ntures employed in both datasets. Moreover, the ATMs offer\na parsimonious representation for the task of music struc-\nture analysis, especially when employed in the construction\nof the â„“1-graph.\nThe best results reported for segment-type labeling on\nthe PopMusic dataset are obtained here, when the ATMs are\nemployed for audio representation and the segmentation is\nperformed on the â„“1-graph deï¬ned by them. These results\nare comparable to the best reported results by Levy and San-\ndler [10], while inferior to those reported by Barrington et\nal.[1]. It is worth noting that the clustering is performed\nwithout any constraints in the proposed approach, which is\nnot the case for the best results reported in [1, 10]. In an\nunconstrained clustering setting, the proposed system out-\nMethod/Reference Fpairwise Av. Number of Segments\nATM + â„“1-graph based segmentation 0.5938 8.5215\nMFCCs + â„“1-graph based segmentation 0.4664 181.9950\nChroma + â„“1-graph based segmentation 0.4563 116.2989\nATM + SDM based segmentation 0.4711 81.0376\nMFCCs + SDM based segmentation 0.3985 190.5489\nChroma + SDM based segmentation 0.4066 167.9239\nMethod in [10] as evaluated in [16] 0.584 N/A\n[16] 0.599 N/A\n[19] 0.600 N/A\n[9] 0.621 N/A\nTable 2 . Segment-type labeling performance on the UPF\nBeatles dataset.Method/Reference Dataset F P R\nATM + â„“1-graph based segmentation PopMusic 0.5227 0.4737 0.6274\nMFCCs constrained [1] PopMusic 0.610 0.620 0.650\nChroma constrained [1] PopMusic 0.420 0.410 0.460\nEchoNest reported in [1] PopMusic 0.450 0.410 0.560\nK-means clustering [10] PopMuic 0.437 0.809 0.311\nMean-ï¬eld clustering [10] PopMusic 0.448 0.366 0.665\nConstrained clustering [10] PopMusic 0.590 0.648 0.567\nATM + â„“1-graph based segmentation UPF Beatles 0.5304 0.5338 0.5670\nMethod in [10] as evaluated in [16] UPF Beatles 0.612 0.600 0.646\n[16] UPF Beatles 0.55 0.521 0.612\nTimbre [9] UPF Beatles 0.586 0.581 0.619\nChroma [9] UPF Beatles 0.500 0.465 0.522\nTimbre & Chroma [9] UPF Beatles 0.536 0.49 0.55\nTable 3 . Boundary detection performance on the PopMusic\nand the UPF Beatles dataset.\nperforms the systems discussed in [1, 10].\nIn the UPF Beatles dataset, the best results for segment-\ntype labeling are obtained again when the ATMs are em-\nployed for audio representation and the segmentation is per-\nform on the â„“1-graph constructed using Ë†W. The reported\nresults are comparable to those obtained by the state-of-the-\nart music structure analysis on this dataset [16, 19]. The\nproposed approach is not directly comparable to that in [9]\ndue to the use of slightly different reference segmentations.\nThe average number of segments detected by our ap-\nproach is 11.86and8.52, when according to the ground-\ntruth the actual average number of segments is 11and10\nfor the PopMusic and the UPF Beatles dataset, respectively.\nThis result is worth noting since no constraints have been\nenforced during clustering.\nThe performance of the proposed approach deteriorates\nwhen either the MFCCs or the chroma features are employed\nfor music representation. The low pairwise F-measure and\nthe over-segmentation can be be attributed to the fact that\nthe underlying assumptions set in Section 3 do not hold for\nsuch representations.\nSince the performance of our approach is clearly inferior\nwhen MFCCs or chroma features are used for music rep-\nresentation, only the ATMs are employed in the segment-\nboundary detection task. The boundary detection results are\nsummarized in Table 3 for both the PopMusic and the UPF\nBeatles datasets. EchoNest refers to the commercial online\nmusic boundary detection service provided by The Echon-\nest and evaluated in [1]. By inspecting Table 3 the pro-\nposed approach is clearly inferior to the system proposed\nby Levy and Sandler [10] for music boundary detection on\nboth datasets. The success of the latter approach can be at-\ntributed to the constraints imposed during clustering. Con-\nsequently, the results obtained by the proposed approach in\nmusic boundary detection could be considered as accept-\nable, since the performance of our system is rated above\nthat reported for many other state-of-the-art systems with or\nwithout constraints (e.g., the EchoNest online service). It\nis worth mentioning that neither of the methods appearing\nin Table 3 reaches the accuracy of the specialized bound-\n499Poster Session 4\nary detection methods (e.g., that in [14]) which achieves a\nboundary F-measure of 0.75on a test set similar to the Bea-\ntles subset of the PopMusic dataset. However, such bound-\nary detection methods, do not model the music structure\nand provide no characterization of the segments between the\nboundaries as the proposed approach as well as the methods\nin [1, 9, 10, 16, 19] do.\n5. CONCLUSIONS\nA novel unsupervised music structure analysis approach has\nbeen proposed. This framework resorts to ATMs for mu-\nsic representation, while the segmentation is performed by\napplying spectral clustering on the â„“1-graph. The perfor-\nmance of the proposed approach is assessed by conducting\nexperiments on two benchmark datasets. The experimental\nresults on music structure analysis are comparable to those\nreported by other state-of-the-art music structure analysis\nsystems. Moreover, promising results on music boundary\ndetection are reported. It is believed that by imposing con-\nstraints during clustering in the proposed approach both the\nmusic structure analysis and the music boundary detection\nwill be considerably improved. This point will be investi-\ngated in the future. Another future research direction is to\nautomatically detect the number of music segments.\nAcknowledgements\nThis research has been co-ï¬nanced by the European Union\n(European Social Fund - ESF) and Greek national funds\nthrough the Operational Program â€œEducation and Lifelong\nLearningâ€ of the National Strategic Reference Framework\n(NSRF) - Research Funding Program: Heraclitus II. Invest-\ning in Knowledge Society through the European Social Fund.\n6. REFERENCES\n[1]L. Barrington, A. Chan, and G. Lanckriet. Modeling music as\na dynamic texture. IEEE Trans. Audio, Speech, and Language\nProcessing , 18(3):602â€“612, 2010.\n[2]M. Bruderer, M. McKinney, and A. Kohlrausch. Structural\nboundary perception in popular music. In Proc. 7th Int. Sym-\nposium Music Information Retrieval , pages 198â€“201, Victoria,\nCanada, 2006.\n[3]W. Chai and B. Vercoe. Structural analysis of musical signals\nfor indexing and thumbnailing. In Proc. ACM/IEEE Joint Conf.\nDigital Libraries , pages 27â€“34, 2003.\n[4]S. S. Chen, D. L. Donoho, and M. A. Saunders. Atomic decom-\nposition by basis pursuit. SIAM J. Sci. Comput. , 20(1):33â€“61,\n1998.\n[5]B. Cheng, J. Yang, S. Yan, Y. Fu, and T. Huang. Learning with\nl1-graph for image analysis. IEEE Trans. Image Processing ,\n19(4):858â€“866, 2010.[6]R. B. Dannenberg and M. Goto. Music structure analysis\nfrom acoustic signals. In D. Havelock, S. Kuwano, and\nM. Vorl Â¨ander, editors, Handbook of Signal Processing in\nAcoustics , pages 305â€“331. Springer, New York, N.Y., USA,\n2008.\n[7]E. Elhamifar and R. Vidal. Sparse subspace clustering. In IEEE\nInt. Conf. Computer Vision and Pattern Recognition , pages\n2790â€“2797, Miami, FL, USA, 2009.\n[8]D. Ellis. Beat tracking by dynamic programming. J. New Music\nResearch , 36(1):51â€“60, 2007.\n[9]F. Kaiser and T. Sikora. Music structure discovery in popular\nmusic using non-negative matrix factorization. In Proc. 11th\nInt. Symposium Music Information Retrieval , pages 429â€“434,\nUtrecht, Netherlands, 2010.\n[10] M. Levy and M. Sandler. Structural segmentation of musical\naudio by constrained clustering. IEEE Trans. Audio, Speech,\nand Language Processing , 16(2):318â€“326, 2008.\n[11] R. Lyon. A computational model of ï¬ltering, detection, and\ncompression in the cochlea. In IEEE Int. Conf. Acoustics,\nSpeech, and Signal Processing , pages 1282â€“1285, Paris,\nFrance, 1982.\n[12] M. Mauch, K. Noland, and S. Dixon. Using musical struc-\nture to enhance automatic chord transcription. In Proc. 10th\nInt. Symposium Music Information Retrieval , pages 231â€“236,\nKobe, Japan, 2009.\n[13] M. M Â¨uller, D. Ellis, A. Klapuri, and G. Richard. Signal pro-\ncessing for music analysis. IEEE J. Sel. Topics in Signal Pro-\ncessing (accepted for publication) , 2011.\n[14] B. Ong and P. Herrera. Semantic segmentation of music audio\ncontents. In Proc. Int. Computer Music Conference , Barcelona,\nSpain, 2005.\n[15] Y . Panagakis, C. Kotropoulos, and G. R. Arce. Non-negative\nmultilinear principal component analysis of auditory temporal\nmodulations for music genre classiï¬cation. IEEE Trans. Audio,\nSpeech, and Language Technology , 18(3):576â€“588, 2010.\n[16] J. Paulus and A. Klapuri. Music structure analysis using a prob-\nabilistic ï¬tness measure and a greedy search algorithm. IEEE\nTrans. Audio, Speech, and Language Processing , 17(6):1159â€“\n1170, 2009.\n[17] J. Paulus, M. M Â¨uller, and A. Klapuri. Audio-based music struc-\nture analysis. In Proc. 11th Int. Symposium Music Information\nRetrieval , pages 625â€“636, Utrecht, Netherlands, 2010.\n[18] J. Shi and J. Malik. Normalized cuts and image segmenta-\ntion. IEEE Trans. Pattern Analysis and Machine Intelligence ,\n22(8):888â€“905, 2000.\n[19] R. Weiss and J. Bello. Identifying repeated patterns in music\nusing sparse convolutive non-negative matrix factorization. In\nProc. 11th Int. Symposium Music Information Retrieval , pages\n123â€“128, Utrecht, Netherlands, 2010.\n[20] J. Wright and Y. Ma. Dense error correction via l1-\nminimization. IEEE Trans. Information Theory , 56(7):3540â€“\n3560, 2010.\n500"
    },
    {
        "title": "Sparse Signal Decomposition on Hybrid Dictionaries Using Musical Priors.",
        "author": [
            "HÃ©lÃ¨ne Papadopoulos",
            "Matthieu Kowalski"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1416264",
        "url": "https://doi.org/10.5281/zenodo.1416264",
        "ee": "https://zenodo.org/records/1416264/files/PapadopoulosK11.pdf",
        "abstract": "This paper investigates the use of musical priors for sparse expansion of audio signals of music on overcomplete dictionaries taken from the union of two orthonormal bases. More specifically, chord information is used to build a structured model that takes into account dependencies between coefficients of the decomposition. Evaluation on various music signals shows that our approach provides results whose quality measured by the signal-to-noise ratio corresponds to state-of-the-art approaches, and shows that our model is relevant to represent audio signals of Western tonal music and opens new perspectives.",
        "zenodo_id": 1416264,
        "dblp_key": "conf/ismir/PapadopoulosK11",
        "keywords": [
            "musical priors",
            "sparse expansion",
            "audio signals",
            "overcomplete dictionaries",
            "tonal music",
            "structured model",
            "dependencies",
            "signal-to-noise ratio",
            "evaluation",
            "approaches"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nSPARSE SIGNAL DECOMPOSITIONONHYBRIDDICTIONARIESUSING\nMUSICAL PRIORS\nHÂ´el`enePapadopoulos andMatthieu Kowalski\nLaboratoiredes Signaux etSyst` emes\nUMR8506,CNRS-SUPELEC-Univ Paris-Sud\n91172Gif-sur-YvetteCedex\nhelene.papadopoulos@lss.supelec.fr\nmatthieu.kowalski@lss.supelec.fr\nABSTRACT\nThis paper investigates the use of musical priors for\nsparse expansion of audio signals of music on overcom-\nplete dictionaries taken from the union of two orthonor-\nmal bases. More speciï¬cally, chord informationis used to\nbuildastructuredmodelthattakesintoaccountdependen-\nciesbetweencoefï¬cientsofthedecomposition. Evaluation\nonvariousmusicsignalsshowsthatourapproachprovides\nresultswhosequalitymeasuredbythesignal-to-noiserati o\ncorrespondsto state-of-the-artapproaches,and shows tha t\nourmodelisrelevanttorepresentaudiosignalsofWestern\ntonalmusicandopensnewperspectives.\n1. INTRODUCTION\nWe propose in this paper a new approach for structured\nsparsedecompositionofamusicsignalinanovercomplete\ntime-frequencydictionary. Startingfromexistingmethod s\nthat arebasedonphysicalsignalproperties,we proposeto\nincorporatemusicalpriorsinordertobuiltsignalreprese n-\ntations that are more suitable to music. For this, we take\nadvantageoftherecentworksthathavebeendoneonchord\nestimationinthe contextofmusiccontentprocessing.\nThe problem of representing an audio signal using a\ntime-frequency dictionary has been given a lot of atten-\ntion these last few years. The speciï¬city of music audio\nsignalsisthat, veryoften,severaltypesof componentsare\nsuperimposed, as for instance tonal components (the par-\ntials of the notes) and transients (the attacks of the notes) .\nThese various components may have signiï¬cantly differ-\nent behaviors. For instance fast varying transient require\nshort analysis window whereas low varyingtonals require\nlongwindows. Thus,theycannotberepresentedwithinthe\nsame basis. This is why hybridmodels allowing a simul-\ntaneous representation of different components have been\nproposed[4,12,17,22].\nPermission to make digital or hard copies of all or part of thi s work for\npersonal or classroom useis granted without fee provided th at copies are\nnotmadeordistributed forproï¬torcommercialadvantagean dthatcopies\nbear this notice and the full citation on theï¬rstpage.\nc/circlecopyrt2011 International Society for MusicInformation Retrieva l.Among the various existing transforms, the modiï¬ed\ndiscretecosinetransform(MDCT)[15]isastandardchoice\nfor the bases[6,14]. Followingthese approaches,we con-\nsider in this work a dictionary built as the union of two\nMDCTbaseswithdifferenttime-frequencyresolutions. The\nnarrow band basis - with long time resolution - is used to\nestimate the tonal parts of the signal, and the wide band\nbasis - with short time resolution - is used to estimate the\ntransient parts. Such a dictionary is chosen overcomplete,\nand thus the expansion of the signal with respect to the\ndictionary is not unique. Sparsitymay be used as a selec-\ntion criterion for ï¬nding the expansion coefï¬cients, in the\nsense that only a few coefï¬cients of the decomposition of\nthe signal on the bases are signiï¬cantly nonzero. The sig-\nnal can thus be well approximatedby a limited numberof\ncoefï¬cients. This problem is often referred to as sparse\nregression .\nA common approach to ï¬nd a sparse expansion of sig-\nnals in overcomplete dictionaries consists of minimizing\ntheâ„“1norm of the expansion, and is known as basis pur-\nsuit[1], or LASSO [21]. Various methods have been also\nproposed: they include variational approaches [13], prob-\nabilistic approaches[14], greedy methods, such as match-\ningpursuitalgorithms[2,16],orBayesianformulationsas\nforinstanceEM-basedalgorithms[9]. Intheframeworkof\nBayesianvariableselection,MCMC(MarkovchainMonte\nCarlo)typeapproachesthatconsideradictionaryconstruc ted\nastheunionoftwoorthonormalbaseshavebeenproposed\n[5,7]. One of the main advantages of the MCMC tech-\nniques is their robustness because they scan the whole of\nthe posterior distribution and thus are unlikely to fall int o\nlocalminima. However,thisisdoneattheexpanseofhigh\ncomputationalcost.\nIn order to fully exploit the dual nature of audio music\nsignals mentioned above, some approaches consider de-\npendencies between signiï¬cant coefï¬cients. In the time-\nfrequencyplane,thepartialsofthenoteswillgeneratehor -\nizontal lineslocalizedin frequency,whereasthe attackso f\nthe notes and the percussive sounds will generate vertical\nlineslocalized in time. Ideally,this structureshouldbe r e-\nï¬‚ectedinthesignaldecomposition. Thisiswhywearein-\nterested in ï¬nding a signal approximation that is not only\nsparse, but also structured. Previous approaches that use\nunstructured priors, such as Bernoulli models have shown\n687Poster Session 6\nthat they generate isolated coefï¬cients with high ampli-\ntude in both bases [7,14]. These componentsdo not have\nany musical meaning and are usually perceived as â€œmusi-\ncal artifactsâ€ or â€œmusical noiseâ€ in the reconstructed sig-\nnal. Consideringdependenciesbetweenatomscoefï¬cients\nand using structuredpriors allows reducing the numberof\nsuch undesirable components. Various approaches have\nbeen proposed for introducing dependencies between co-\nefï¬cients in the time-frequency domain. Structures can\nbemodeleddirectlyinthecoefï¬cientsthemselves,suchas\nin[13]. However,dependenciesareoftenintroducedinthe\ntime-frequency indices, rather than directly in the coefï¬-\ncients themselves. Amongexisting approaches,frequency\npersistencypropertiesofthetransientlayercanbemodele d\nusing structuredBernouilli models[14]; persistencyalon g\nthe frequency axis is favored using Markov models [17];\nin [8], structural constraints on the coefï¬cients that rely\non physical properties of the signal are imposed for both\nlayers, using two types of Markov chains. It results in a\nâ€œhorizontal structureâ€ for the tonal layer and a â€œvertical\nstructureâ€ for the transient layer. Up to now, additional\nstructure constraints that have been added rely on physi-\ncal propertiesof the signal. The originality of our work is\nthatweproposetoincorporatepriorsthatarebasedonmu-\nsical information. Relying on the model presented in [8]\nwithinaBayesianframework,webuildastructuredmodel\nfor sparse signal decomposition that incorporates musical\npriors for tonal layer modeling. Our model is particularly\nwell adapted to the tonal structure of signals and ï¬ts the\nintrinsicnatureofWestern tonalmusic.\nSparse representations of signals have recently proved\nto be useful fora wide rangeof applicationsin signal pro-\ncessing,suchasdenoising[6],codingandcompression[3,\n20]orsourceseparation[7]. Here,wefocusonthetask of\ndenoising an excerpt of musical audio. Our approachpro-\nvidesresultswhosequalityin term ofsignal-to-noiserati o\n(SNR) corresponds to state-of-the-art approaches, while\nbetterreï¬‚ectingthe natureofmusicaudiosignal.\nThestructureofthepaperisasfollows. First,inSection\n2, we present our model for sparse signal decomposition\nonhybriddictionariesthatincorporatesmusicalpriors;o ur\nmain contribution is described in part 2.3. We brieï¬‚y ad-\ndresstheproblemofparametersestimationinSection3. In\nSection4,wepresentanddiscusstheresultsofourmodel.\nConclusionsandperspectivesforfutureworksaregivenin\nSection5.\n2. SIGNAL MODEL\nThis section introduces ï¬rst the mathematical model used\nto represent the audio signal, and then deï¬nes the priors\nchoseninaBayesiancontext. Particularly,thenewmusical\npriorbasedonthe chromagram isexposedin section2.3.\n2.1 Model\nIn this part, we describe our model for signal decompo-\nsition with sparse constraint on a hybriddictionaryof ele-\nmentarywaveformsor atoms. Thedictionaryisconstructedasthe unionoftwo orthonormalbaseswithdifferenttime-\nfrequencyresolutionthataccountrespectivelyfortheton al\nand the transientparts of the signal. We rely on the model\nproposed in [8] and we consider a tree-layer signal model\noftheform: signal =tonals +transients +residual.\nLetV={vn, n= 1, . . ., N }andU={um, m=\n1, . . ., N }be two MDCT bases of RNwith respectively\nlong frame â„“tonto achieve good frequency resolution for\ntonals and short frame â„“tranto achieve good time resolu-\ntion for transients. The MDCT is a bijective linear trans-\nformandwenote nton=N\nâ„“tonandntran=N\nâ„“tranthenum-\nber of frames for each basis (see Fig. 2). Here, nandm\nare time-frequencyindexesand will be denotedin the fol-\nlowing n= (q, Î½)âˆˆ[1, â„“ton]Ã—[1, nton]orn= (q, Î½)âˆˆ\n[1, â„“tran]Ã—[1, ntran].\nWedenote D=VâˆªUthedictionarymadeastheunion\nof these two bases. Dis overcomplete in RN, and any\nxâˆˆRNadmitsinï¬nitelymanyexpansionsinthe form:\nx=X\nnâˆˆIÎ±nvn+X\nmâˆˆIÎ²mum+r (1)\nwhere I={1, . . ., N },Î±nandÎ²mare the expansion co-\nefï¬cientsand rrepresentsthenoiseterm.\nWeareinterestedinsparsesignals,i.e. signalsthatmay\nbewrittenas:\nx=X\nÎ»âˆˆÎ›Î±Î»vÎ»+X\nÎ´âˆˆâˆ†Î²Î´uÎ´+r (2)\nwhere Î›andâˆ†are small subsets of the index set I=\n{1, . . ., N }that accountforthesigniï¬cantcoefï¬cients. In\nwhatfollows,theywillbereferredtoas signiï¬cancemaps .\nWeintroducetwoindicatorrandomvariables Î³ton,nand\nÎ³tran,mcorrespondingto thesigniï¬cancemaps Î›andâˆ†:\nÎ³ton,n=ïš¾\n1ifnâˆˆÎ›\n0otherwiseÎ³tran,m =ïš¾\n1ifmâˆˆâˆ†\n0otherwise(3)\nWe canthereforerewriteEq. (2)as:\nx=X\nnâˆˆIÎ³ton,nÎ±nvn+X\nmâˆˆIÎ³tran,m Î²mum+r(4)\n2.2 Coefï¬cientPriors\nWe assumethat, conditionaluponthe signiï¬cancemaps Î›\nandâˆ†, the coefï¬cients Î±nandÎ²mare independent zero-\nmeannormalrandomvariables:\np(Î±n|Î³ton,n, Ïƒton,n) = (1 âˆ’Î³ton,n)Î´0(Î±n) +(5)\nÎ³ton,nN(Î±n|0, Ïƒ2\nton,n)\np(Î²m|Î³tran,m , Ïƒtran,m ) = (1 âˆ’Î³tran,m )Î´0(Î²m) +\nÎ³tran,m N(Î²m|0, Ïƒ2\ntran,m )\nwhere Î´0istheDiracdeltafunctionandthevariances Ïƒton,n\nandÏƒtran,mare given a conjugate inverted-Gamma prior.\nSparsity is enforced when Î³n= 0(resp. Î³m= 0). In this\ncase, thecoefï¬cients Î±n(resp. Î²m)areset to 0.\n68812th International Society for Music Information Retrieval Conference (ISMIR 2011)\n2.3 IndicatorVariablePriors\nThesigniï¬cancemaps Î›andâˆ†aregivenstructuredpriors.\nThe one correspondingto the tonal basis encodes musical\ninformation while the one corresponding to the transient\nbasisis basedonphysicalpropertiesofthesignal. Bothof\nthemareâ€œverticalâ€structures.\n2.3.1 ModelforTonals\nFor the signiï¬cance map corresponding to the tonals, we\nproposetomodeldependenciesbetweenindicatorvariables\nusing musical information. Let us assume that we know\nthe score corresponding to the musical excerpt and that,\nfor each frame qâˆˆ {1, . . ., n ton}, we know which notes\nthesignal iscomposedof.\nHere, we want to work directlyon audio. However,the\nsymbolic transcription (the score) of a piece of music is\nnotalwaysavailable,especiallyinmusicssuchasjazzmu-\nsic wherethereisa largepartdevotedto improvisation. In\naddition,algorithmsthatextractatranscriptionfromana u-\ndiosignal,suchasmulti-f0estimationalgorithms[24],ar e\nstill limitedandcostly. However,numbersofrecentworks\nhave shown that it is possible to accurately extract robust\nmid-level representation of the music, such as the chord\nprogression[18].\nWeproposetogiveamusicalpriortotheindicatorvari-\nables using musical information obtained from the chord\nprogression. The output of a chord estimation algorithm\nconsists in a progression of chords chosen among a given\nchord lexicon. Each chord may be characterized by the\nsemitone pitch classes or chroma that correspond to the\nnotes it is composed of. Since their introduction in 1999,\nPitch Class Proï¬les [10] orchroma-based representations\n[23]havebecomecommonfeaturesforestimatingchords.\nThey are traditionally 12-dimensional vectors, with each\ndimension corresponding to the intensity associated with\noneofthe12semitonepitchclasses(chroma)oftheWest-\nerntonalmusicscale,regardlessofoctave. Thesuccession\nofchromavectorsovertimeisknownas chromagram .\nIn general, the chord lexicon does not distinguish be-\ntweenanypossiblecombinationofsimultaneousnotesbut\nis typically reducedto a set of chordsof 3 or 4 notes. The\nnumberof notescomposingthe chordswill be denotedby\nNcin the following. Here, we limit our chord lexicon to\nthe24major and minor triads ( Nc= 3). The method we\nproposecouldbeextendedto largerdictionaries.\nThe chord progression does not provide an exact tran-\nscription of the music. For instance, passing notes are\nin general ignored, missing notes in the harmony may be\nadded. Moreover, the chords are estimated regardless of\noctave. However,experimentsshow that theprovidedmu-\nsical information is sufï¬cient enough to build musically\nmeaningfulpriors.\nGiven a ï¬xed frame index q, let{pc\nk}k=1,...,N cdenote\nthe semitone pitch-classes (chroma) corresponding to the\nestimated chord cq. Let also {pMDCT\nÎ½ }Î½=1,...,â„“ tondenote\nthe semitone pitch classes corresponding to each MDCT\nbin.Assumingaperfecttuningof A= 440Hz,aMDCTbin\nof frequency fÎ½is converted to a chroma pMDCT\nÎ½by the\nfollowingequation:\npMDCT\nÎ½ = (12 log2fÎ½\n440+ 69) (mod12)1(6)\nTheindicatorvariables {Î³ton,(q,Î½)}Î½=1,...,â„“ tonaregiven\nthefollowingmembershipprobabilities:\nPÎ›{Î³ton,(q,Î½)= 1} (7)\n=/braceleftbiggpton ifâˆƒkâˆˆ[1, Nc]|pMDCT\nÎ½ =pc\nk\n1âˆ’ptonotherwise\nwhere 0â‰¤ptonâ‰¤1. The signiï¬cance maps correspond-\ningtothetonallayershouldreï¬‚ectthetonalcontentofthe\naudio signal. In practice, the value ptonwill be close to\n1(in our experiments, pton= 0.9) so that atoms corre-\nsponding to the notes that are played are given high prior.\nThe signiï¬cant map for the tonal layer corresponding to\ntheGlockenspiel audio signals of our test-set is illustrated\nin Fig. 1. A set of atomsis selected at each frameaccord-\ning to the notes of the (chord) transcription, regardless of\noctave. For instance all atoms {B1, B2, . . .}correspond-\ning to the semitone B are selected when the ï¬rst B note\nof theGlockenspiel is sounded. The signiï¬cancemapsare\ngiven structures of â€œtubesâ€ that have a musical meaning.\nNotethat weprovideherea â€œverticalstructureâ€fortonals.\nStructured significance map without harmonics\nTime (frames)Frequency (frames)\n20 40 60 80 100 1201002003004005006007008009001000\nStructured significance map with harmonics\nTime (frames)Frequency (frames)\n20 40 60 80 100 1201002003004005006007008009001000\nBEBG# BEF#G# C# EBBG# BEF#G# C#\nFigure1. Structuredsigniï¬cancemapforthe Glockenspiel\nusing musical information. Left: only notes composing\nthe chords are considered. Right: higher harmonics are\nconsidered. Thetranscriptionisindicatedinthebottom.\nTwo additional components may be added to improve\nthemodel.\nâ€¢First, the instrumentsmay have been tuned accordingto\nareferencepitchdifferentfromthestandard A4 = 440Hz.\nIn this case it is necessary to estimate the tuning of the\ntrackandEq. (8)becomes:\npMDCT\nÎ½ = (12 log2fÎ½\nAest+ 69) (mod12) (8)\nwhere Aestdenotes the estimated tuning, here obtained\nwith themethodproposedin[19].\n1a(modb)denotes themathematical operator modulo, the remainder\nwhenais divided by b\n689Poster Session 6\nâ€¢Secondly, higher harmonics may be considered in the\nmodel. Each note producesa set of harmonicsthat results\ninamixtureofnon-zerovaluesinthechromavectorcorre-\nsponding to the chord. For instance a C note will produce\nthesetofharmonics {Câˆ’Câˆ’Gâˆ’Câˆ’Eâˆ’Gâˆ’. . .}. They\ncanbeconsideredinthesigniï¬cancemaps,asillustratedin\nthe right part ofFig. 1. Here we take into accountthe ï¬rst\n6 harmonicsofthenotes2.\n2.3.2 ModelforTransients\nFollowing[8],persistencyinfrequencyofthetime-freque ncy\ncoefï¬cientscorrespondingtotransientlayerismodeledgi v-\ning a vertical prior structure to the indicator variables in\nthe second basis. Given a frame index q, the sequence\n{Î³tran,(q,Î½)}Î½=1,...,â„“ tranis modeled by a two-state ï¬rst-\norderMarkovchainwithprobabilities Ptran,00andPtran,11,\nassumedequalforallframes,andwithlearnedinitialprob-\nability Ï€tran. ThemodelisillustratedinFig. 2.\nFigure2. Verticalmodelfortransients. Adaptedfrom[8].\n2.4 Residual\nTheresidualsignal rismodeledasaGaussianwhitenoise,\nwith variance Ïƒ2, which is given an inverted-Gammacon-\njugateprior.\n3. MCMC INFERENCE\nFollowing [8], the posterior distribution of the set of pa-\nrametersandhyperparametersof the model,denotedby Î¸,\nis sampled from using a Gibbs sampler [11], which is a\nstandard Markov Chain Monte Carlo (MCMC) technique\nthat simply requires to iteratively sample from the poste-\nrior distributions of each parameter upon data xand the\nremainingparameters.\nThe Minimum Mean Square Estimates (MMSE) of the\nparameters Î¸can then be computed from the Gibbs sam-\nples{Î¸(1), Î¸(2), . . . , Î¸(K)}oftheposteriordistribution p(Î¸|x):\nË†Î¸MMSE =/integraltext\nÎ¸p(Î¸|x)dÎ¸ (9)\nâ‰ˆ1\nK/summationtextK\nk=1Î¸(k)(10)\n2We limit the number of considered harmonics to 6 because many of\nthe higher harmonics, which are theoretically whole number multiples of\nthe fundamental frequency, are far from any note of the Weste rn chro-\nmatic scale. This is especially true for the 7th and the 11th h armonics.TheMAPestimatecanbecomputedbythresholdingthe\nvalues of the MMSE. In [8], all the values of the MMSE\nlower that 0.5are threshold to 0and all the values greater\nthan0.5arethresholdto 1.\nWedonotdetailheretheexpressionfortheupdatesteps\noftheparameters,detailscanbefoundin[8]. Time-domain\nsource estimates are reconstructedby inversetransformof\ntheestimatedcoefï¬cients(inverseMDCTinourcase). The\ndenoisedestimationisconstructedby Ë†x=Î±V+Î²U.\n4. RESULTS ANDDISCUSSION\nThe aim of this section is to analyze the performances of\ntheproposedapproachforthetaskofaudiodenoising. For\nthe sake of simplicity, we ï¬rst focusin details on a mono-\nphonic signal, the Glockenspiel . We also provide addi-\ntional numerical results and examples on short extracts of\npolyphonic music. The impact of the various parameters\n(tuning,harmonics,andpriorssettings)isalso studied.\n4.1 ExperimentalSetup\nIn this article, we present results assuming that the tran-\nscriptionisknown(notesforthemonophonicsignal,chords\nfor the polyphonic signals). The 5 musical excerpts of\nvarious music styles are described in Table 1. Our ap-\nproach that incorporates musical priors for modeling the\ntonallayeriscomparedwiththe onepresentedin[8].\nTable 1. Soundexcerptsused for evaluationof the model.\nSR: samplingrate.\nName SR(Hz) Duration\nGlockenspiel 44100 2s\nMisery (Beatles) 11025 11s\nLove MeDo (Beatles) 11025 5s\nBeethoven StringquartetOp.127 -1 11025 11s\nMozart PianoSonataKV310 -1 11025 11s\nParameters : The length of the two MDCT bases are\nset to 1024samples for the tonal layer and 128samples\nfor the transient layer, at a sampling rate of 44100Hz,and\nrespectively to 256and32samples at a sampling rate of\n11025Hz3. TheMMSEandMAPestimatesoftheparam-\neters are computed by averaging the last 100samples of\ntheGibbssampler,runfor 500iterations.\nEvaluation Measures : Artiï¬cial noisy signals are cre-\nated by adding Gaussian white noise to the clean signal\nwithvariousinputSNRs. Thecasewithoutadditionalnoise\nWN(without noise) correspondsto a separation into two\nlayerstransient + tonal . Partialsare expectedto berecov-\nered in the tonal layer while attacks or percussive sounds\nwillberecoveredinthetransientlayer. Theresultsinterm s\nof output SNR are summarized in Table 2 and provide an\nobjective evaluation measure. However, although widely\nusedforassessingalgorithmperformances,theSNRisnot\na completely relevant measure of distortion for audio sig-\nnals. Subjective evaluation by listening to the signals is\nalso required. The audio excerpts are available at: http:\n3Asunderlined in[8,14],better resultsareobtained usinga veryshort\nwindow length forthetransients ( â‰ˆ3ms). Thetwowindow lengths must\nbesigniï¬cantly different enough to discriminate between t onals and tran-\nsients\n69012th International Society for Music Information Retrieval Conference (ISMIR 2011)\n//webpages.lss.supelec.fr/perso/kowalski/ismir11/\nismir11.html .\nTable 2. ResultingvaluesofoutputSNRs(dB)forvarious\ninputSNRsandwithoutadditionalGaussiannoise( WN).\nProposedapproach [8]approach\nSNR WN 0 10 20 WN 0 10 20\nGlockenspiel 71.214.121.328.570.215.722.529.2\nMisery 42.37.013.020.944.47.313.321.1\nLove Me Do 28.66.812.519.329.66.912.719.4\nBeethoven 54.58.513.621.654.68.914.021.9\nMozart 62.69.315.423.460.99.815.923.9\nComputational Performances : The algorithms are im-\nplementedinMATLABandperformedonaMacBookPro\nIntel Core 2 Duoclockedat 2.4GHzwith 2GB RAM. The\ncomputationtime of the proposedmethodis similar to the\none obtained with [8], â‰ˆ447s for processing the Glock-\nenspielsignal. Theuse ofMCMCschemesgenerateshigh\ncomputationalcosts.\n4.2 ResultsandDiscussion\nConcerningthequalityofdenoising,theresultsprovidedi n\nTable2showthatourmodelprovidesresultsthatarecom-\nparable to state-of-the-artalgorithmsin termsof SNR: the\ndifferencebetweenthepresentedmethodandthe[8]arein\ngeneral lower than 1dB. However, noticeable differences\nmaybeperceivedwhilelisteningto thesoundï¬les.\nThemaininterestoftheproposedmodelliesinthemod-\neling of the tonal layer. Fig. 3 shows signiï¬cancemapsof\nthe selected atoms (MAP estimates) for the Glockenspiel\nsignal, in the WNcase. As can be seen, the use of musi-\ncalpriorsyieldstoastructurethatbetterreï¬‚ectsthemusi c\ncontent of the signal compared to the approach that uses\nphysical priors. The resolution of the tonal signiï¬cance\nmap is sharper. The partials of the notes clearly appear\nas thin horizontal lines and the beginning of the notes is\nveryclear. Onecanalsoseethatourmethodusingmusical\npriorsprovidessparserestimatesofthesigniï¬cancemap.\nIt should be noticed that, especially under low-input\nSNRs conditions, one may perceive some artifacts in the\nreconstructedsignalwiththemethodwepropose. Theyare\nprobablyduetothefactthatsomehighfrequenciesarecap-\ntured by the transient basis rather than by the tonal basis.\nFuture works should concentrate on modeling structured\npriors for the transient layer that are more adapted to the\noneproposedhereforthetonallayer. However,inspite of\nthese artifacts, one can ï¬nd by listening to the signals that\nthe sound of the reconstructed signals relying on musical\npriors is often â€œricherâ€ than the one obtained with the ap-\nproach used in [8]. Fig. 4 shows the signiï¬cance maps of\nthe selected atoms (MMSE estimates) for the Mozartsig-\nnal, inthe case SNR in= 10dB.Again,the partialsofthe\nnotes are better discriminated using musical priors, espe-\nciallyin lowfrequencies.\nIndicator Variable Prior Set-up: The value ptonin\nEq. (7)hasaneffectontheabove-mentionedartifactspro-\nduced by our model in low-input SNRs conditions. For\nTonals\nTime (frames)Frequency (frames)\n204060801001201002003004005006007008009001000\nTransients\nTime (frames)Frequency (frames)\n200400600800100020406080100120\nTonals\nTime (frames)Frequency (frames)\n204060801001201002003004005006007008009001000\nTransients\nTime (frames)Frequency (frames)\n200 400 600 800 100020406080100120\nFigure 3. Signiï¬cance maps of the tonal and transient\nbases (MAP estimates) for the Glockenspiel excerpt, case\nWN. Top: approach[8]. Bottom: proposedapproach.\ninstance, setting ptonto0.99instead of 0.9in the case\noftheGlockenspiel signalallowsreducingtheartifactsfor\nSNR in= 10dB. However,our experimentsshow that in-\ndicator variables corresponding to atoms that do not be-\nlong to the chord must not be set to 0. Setting ptonto1\nresults in reconstructedsignals of veryâ€œpoorâ€ sound, as it\ncan be assessed by listening tests. Output SNRs are also\ndegraded. Setting pton<1allowstakinginto accountim-\nperfectionsofthechromagramgivenasinputofthehybrid\nmodel(temporalimperfectionsduetowindowing,discrep-\nancybetweentheidealmodelandreality, etc.).\nImpact of Tuning: Integrating tuning information in\nthemodeldoesnotleadtoimprovementintermsofoutput\nSNRvalues,butyieldstoestimatedsigniï¬cancemapsthat\nare more coherent with our model. Indeed, the â€œtubesâ€\ndependonthetuningandthus,incaseofâ€badâ€tuning,the\natomsareselected withinthecorrectfrequencyregions.\nImpact of Harmonics: We did not ï¬nd any improve-\nment when adding harmonics in our model. This may be\npartiallyexplainedbythefactthat,inthepolyphoniccase ,\nthecontributionofalargepartoftheï¬rst6higherharmon-\nics of a note is already taken into account in the signiï¬-\ncancemapbytheothernotes. Forinstance,let usconsider\nCmajorchord(C-E-G).TheCnotegeneratesharmonicsE\nand G. E and G are thus both actual played notes and har-\nmonics. Their contribution is already partially taken into\naccount in the signiï¬cance map in the case of the model\nâ€œwithoutharmonicsâ€.\n691Poster Session 6\nTonals\nTime (frames)Frequency (frames)\n10020030040050050100150200250\n Transients \nTime (frames)Frequency (frames)\n1000 2000 3000 400051015202530\nTonals\nTime (frames)Frequency (frames)\n10020030040050050100150200250\nTransients\nTime (frames)Frequency (frames)\n1000 2000 3000 400051015202530\nFigure 4. Signiï¬cance maps of each basis (MMSE esti-\nmates)forthe Mozartexcerpt,case SNR in= 10dB.Top:\napproach[8]. Bottom: proposedapproach.\n5. CONCLUSIONAND FUTUREWORKS\nIn this article, we have presented a method for sparse de-\ncomposition of audio signals of music on overcomplete\ndictionaries made as union of two MDCT bases. We rely\nonpreviousworksthatconsiderdependenciesbetweensig-\nniï¬cant coefï¬cients of the expansion. The originality of\nour approach is that we incorporate musical priors in the\nmodel. Our approach provides results whose quality cor-\nresponds to state-of-the-art approaches for the denoising\ntask, and which show that our model that is adequate to\nfairly represent audio signals of music. The main contri-\nbutionofthearticleistoshowthatthemusicalpriorbased\non musical knowledge performs as well as more sophisti-\ncate prioras HMM and appearsto be more â€œnaturalâ€. The\nsigniï¬cancemapcorrespondingtothetonallayeriscoher-\nentwith theintrinsiccontentofmusicaudio.\nFuturework will concentrateonfully integratingin the\nmodelchordestimationinaninteractivefashion. Thechro-\nmagramcouldbeupdatedwiththeotherparametersduring\nMCMC inference in order to possibly improve the chord\nestimation. The prior we propose has a great potential of\nimprovement in the future (for example, by using a time\nsegmentation,a largerchordlexiconetc.)\nAsfarasweknow,theintroductionofmusicalpriorsin\nhybrid models for spare decomposition is novel. The use\nof mid-levelrepresentationof audio- such as the chroma-\ngram, as proposed in this paper - or scores, if available,\ncouldbeextendedtomanyapplicationssuchasdenoising,source separation, compression, coding and many others.\nUsually, only physicaland mathematical criteria are taken\ninto account. We believe that the use of musical informa-\ntionopensnewinterestingperspectives.\n6. ACKNOWLEDGMENT\nThe authorswould like to thank C. FÂ´ evotte and all the au-\nthorsof[8]forprovidingtheirsourcecode.\n7. REFERENCES\n[1] S.Chen, D.David L.Donoho, and M.Saunders. Atomicdecom posi-\ntion by basis pursuit. SIAM J. Scient. Comp. , 20,1998.\n[2] L. Daudet. Audio sparse decompositions in parallel Let t he greed be\nshared ! . IEEETrans. Sig. Proc. ,27,2010.\n[3] L.Daudet, S.Molla, andB.TorrÂ´ esani. Towardsahybrid a udio coder.\nInWAA,2004.\n[4] L. Daudet and B. TorrÂ´ esani. Hybrid representations for audiophonic\nsignal encoding. Sig. Proc.J. , 82, 2002.\n[5] M.E. Davies and L. Daudet. Sparse audio representations using the\nMCLT.Sig. Proc.J. , 86(3), 2006.\n[6] C. FÂ´ evotte, l. Daudet, S. J. Godsill, and B. TorrÂ´ esani. Sparse re-\ngression with structured priors : Application to audio deno ising. In\nICASSP,2006.\n[7] C.FÂ´ evotteandS.J.Godsill.ABayesian approachforbli ndseparation\nof sparse sources. IEEETrans. Sp.Audio Proc. ,14(6), 2006.\n[8] C. FÂ´ evotte, B. TorrÂ´ esani, L. Daudet, and S.J. Godsill. Sparse Linear\nRegression With Structured Priors and Application to Denoi sing of\nMusical Audio. IEEETrans. Sp.Audio Proc. ,16,,2008.\n[9] M.A.T. Figueiredo. Adaptive Sparseness for Supervised Learning.\nIEEETrans.Patt. Anal. Mac. Intel. , 25, 2003.\n[10] T.Fujishima.Real-timechordrecognition ofmusicals ound: asystem\nusing common lisp music. In ICMC, 1999.\n[11] S. Geman and D. Geman. Stochastic relaxation, Gibbs dis tributions,\nand the Bayesian restoration of images. IEEE Trans. Patt. Anal.\nMach. Intell. , 6,1984.\n[12] K.N. Hamdy, M. Ali, and A.H. Tewï¬k. Low Bit Rate High Qual ity\nAudio Coding With Combined Harmonic And Wavelet Representa -\ntions. InICASSP,1996.\n[13] M. Kowalski. Sparse regression using mixed norms. Appl. Comp.\nHarm.Anal. , 27,2009.\n[14] M. Kowalski and B. TorrÂ´ esani. Random models for sparse signals\nexpansion onunions ofbases with application to audio signa ls.IEEE\nTrans. Sig. Proc. ,56,,2008.\n[15] S. Mallat. A Wavelet Tour of Signal Processing . New York: Aca-\ndemic, 1998.\n[16] S.MallatandZ.Zhang.Matching PursuitWithTime-Freq uency Dic-\ntionaries. IEEETrans. Sig. Proc. ,41,1993.\n[17] S. Molla and B. TorÂ´ esani. An hybrid audio scheme using h idden\nMarkov models of waveforms. Appl.Comp. Harm. Anal. ,18,2005.\n[18] H. Papadopoulos and G. Peeters. Joint estimation of cho rds and\ndownbeats. IEEETrans. Audio,Speech, Lang. Proc. ,19(1), 2011.\n[19] G. Peeters. Musical key estimation of audio signal base d on HMM\nmodeling of chromavectors. In DAFx,2006.\n[20] E. Ravelli, G. Richard, and L. Daudet. Union of MDCT Base s for\nAudio Coding. IEEETrans.Audio, Speech, Lang. Proc. ,16, 2008.\n[21] Robert Tibshirani. Regression shrinkage and selectio n via the lasso.\nJournaloftheRoyalStatisticalSocietySerieB ,58(1):267â€“288, 1996.\n[22] T.S.VermaandT.H.Y.Meng.ExtendingSpectralModelin gSynthesis\nwith Transient Modeling Synthesis. Comp. Mus. J. , 24,2000.\n[23] G.H. Wakeï¬eld. Mathematical representation of joint t ime-chroma\ndistribution. In ASPAAI,1999.\n[24] C. Yeh, A. Roebel, and X. Rodet. Multiple Fundamental Fr equency\nEstimation and Polyphony Inference of Polyphonic Music Sig nals.\nIEEETrans.Audio, Speech, Lang. Proc. ,18-6, 2010.\n692"
    },
    {
        "title": "New Approaches to Optical Music Recognition.",
        "author": [
            "Christopher Raphael",
            "Jingya Wang"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1414856",
        "url": "https://doi.org/10.5281/zenodo.1414856",
        "ee": "https://zenodo.org/records/1414856/files/RaphaelW11.pdf",
        "abstract": "We present the beginnings of a new system for optical music recognition (OMR), aimed toward the score images of the International Music Score Library Project (IMSLP). Our system focuses on measures as the basic unit of recognition. We identify candidate composite symbols (chords and beamed groups) using grammatically-formulated top-down model-based methods, while employing template matching to find isolated rigid symbols. We reconcile these overlapping symbols by seeking non-overlapping variants of the composite symbols that best account for the pixel data. We present results on a representative score from the IMSLP.",
        "zenodo_id": 1414856,
        "dblp_key": "conf/ismir/RaphaelW11",
        "keywords": [
            "Optical Music Recognition",
            "International Music Score Library Project",
            "Measures as the basic unit",
            "Grammatically-formulated top-down model-based methods",
            "Template matching",
            "Isolated rigid symbols",
            "Reconciling overlapping symbols",
            "Non-overlapping variants",
            "Pixel data",
            "Representative score"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nNEWAPPROACHES TO OPTICAL MUSICRECOGNITION\nChristopherRaphael\nIndiana University\nSchoolofInformatics andComputingJingya Wang\nIndiana University\nSchoolof Informatics andComputing\nABSTRACT\nWe present the beginnings of a new system for optical mu-\nsic recognition (OMR), aimed toward the score images of\ntheInternationalMusicScoreLibraryProject(IMSLP).Our\nsystem focuses on measures as the basic unit of recogni-\ntion. We identify candidate composite symbols (chords and\nbeamed groups) using grammatically-formulated top-down\nmodel-based methods, while employing template matching\nto ï¬nd isolated rigid symbols. We reconcile these overlap-\nping symbols by seeking non-overlapping variants of the\ncomposite symbols that best account for the pixel data. We\npresent resultsona representative scorefrom the IMSLP.\n1. INTRODUCTION\nFor many years our community has lamented the lack of\nsymbolically-represented music. In contrast to audio, suc h\nscore-like representations allow music tobe searched, com -\npared, transformed, and analyzed in many ways, as with\ntext data. The need for these libraries is particularly acut e\nfor â€œclassicalâ€ music, where the symbolic score has been\nregarded, at least historically, as the deï¬nitive source of a\ncomposition. We believe the most promising pathway to\nlarge-scale symbolic music libraries is through optical mu -\nsicrecognition(OMR).ThepotentialforOMRhasincreased\ndramatically with the rapid rise of the International Music\nScore Library Project (IMSLP), an open library of primar-\nily scanned, public domain, machine-printed mostly classi -\ncal music scores. The IMSLP represents a potential gold\nmineof symbolic music data, virtually imploring our com-\nmunity to develop OMR technology capable of harvesting\nthesedata. AnsweringtheOMRchallengeposedbytheIM-\nSLPistheultimategoalofthenewresearcheffortdescribed\nhere.\nTheexistenceoflarge-scalesymboliclibrarieswouldtran s-\nformthemusicianâ€™sworld,allowingglobaldistribution,ï¬‚ ex-\nPermission to make digital or hard copies of all or part of this w ork for\npersonal or classroom use is granted without fee provided th at copies are\nnot made ordistributed forproï¬t orcommercial advantage and th at copies\nbearthis noticeand thefull citation ontheï¬rst page.\nc/circlecopyrt2011International Society forMusic InformationRetrieval .ible formatting, and content-based music information re-\ntrieval. Many envision future â€œdigital music standsâ€ based\non tablet computers. Fueled by symbolic music represen-\ntations, such devices could support a wide range of appli-\ncations in addition to the basic presentation of music, in-\ncludingpedagogicalsystemsofferingperformanceanalysi s,\nregistration of scores with music audio and video, musical\naccompanimentsystems,automaticï¬ngeringsystems,nota-\ntion, automatic arranging and transcription programs. Sym -\nbolic music forms the basis of many ISMIR foci, such as\nmusic information retrieval as well as harmonic, motivic,\nstructural,andSchenkerianmusicanalyses. And,ofcourse ,\nlarge-scale symbolic music collections will be transforma -\ntive for music libraries, allowing universal access to publ ic\ndomain music.\nOMR has seen various research efforts over the last sev-\neral decades, such as [2] [3], [4], [5], [6], [7], [9], [8] to\nnameonlyafew. Fujinaga[1]givesarathercompletebibli-\nographyofmorethan500differentpapers,theses,andtech-\nnical reports. Given the importance of this problem, we be-\nlieveithasbeenunderrepresentedintheISMIRcommunity,\nperhaps due to the many difï¬culties of deï¬ningthe prob-\nlem, such as stating goals, scope, and evaluation metrics\nthat are relevant to in vivorecognition situations. Our work\ndiffers from most the we know in OMR, through its ori-\nentation toward model-based top-down recognition. These\nideas have some precursors in OMR, such as [9], which in-\ntroduces Markov Source Models to OMR and performs a\nproofofconceptinasimpliï¬eddomain,and[8],whichalso\narguesformodel-drivenrecognition,evenoftheexperimen -\ntal aspect remains undeveloped. Model-based approaches\nare, of course, commonplace in the larger document recog-\nnitioncommunity,aswellasincomputervision,thoughthe\nconnectionsherearebeyondthescopeofourpresenteffort.\nThe state of OMR remains somewhat undeveloped, es-\npecially when compared to its optical character recognitio n\n(OCR) cousin, simply because OMR is much harder. The\nmost powerful ideas from the OCR literature are the one-\ndimensional modeling and processing techniques, such as\nhidden Markov models (HMM) and dynamic programming\n(DP), in recognizing lines of text. These techniques allow\nfor ï¬‚exible top-down modeling, training, and computation\nto be integrated into the same framework. DP- and HMM-\n305Oral Session 3: Symbolic Music, OMR\nbasedapproachesallow simultaneous segmentationandrecog-\nnition, in which symbols are segmented notthrough local\ntopology, but by ï¬nding divisions that allow the pieces to\nbe identiï¬ed as meaningful â€œstand-aloneâ€ quantities. It is\ndifï¬cult to apply these ideas to OMR due to the fundamen-\ntallytwo-dimensional layout ofprintedmusic. Instead, pa st\napproaches have primarily worked bottom-up , usually per-\nformingcrucialimagesegmentation beforerecognition,and\noften in peril of constructing meaningless recognition hy-\npotheses, (e.g. ï¬nding â€œorphanâ€ accidentals that do not be-\nlong tonote heads).\nOur approach compromises between our idealistic zeal\nfor top-down recognition and the computational and practi-\ncal demands of the challenging problem at hand. We be-\ngin by identifying page structure as described in Section\n2.2. Our main focus is the recognition of the individual\nmeasures identiï¬ed through the page structure decomposi-\ntion. Weemploy model-based recognition fortheimportant\nâ€œcompositesymbolâ€sub-problems: isolatedchords(Sectio n\n2.3.1) and beamed groups (Section 2.3.2). This guarantees\nthattheexampleswerecognizemakesyntacticsenseandare\nâ€œoptimalâ€ in some limited sense. We aggregate these over-\nlapping and conï¬‚icting candidates into measure hypotheses\ninSection2.3.3,throughanoptimizationproblemthatseek s\nmeaningful non-overlapping â€œversionsâ€ of the recognized\nmeasure components through constrained optimization.\n2. SCIENTIFIC APPROACH\n2.1 The Data Model\nAt a conceptual level nearly all music notation is binary,\nwith each image location, x, eitherâ€œblackâ€ (containing ink)\nor â€œwhiteâ€ (no ink). Of course this binary nature is only\napproximately captured by the actual pixel intensity values,\ng(x). Inpractice,thedistributionofintensityvaluesisnearl y\nalways bimodal, but often containing values that could be-\nlong to either category. We model these intensities proba-\nbilistically, with pBandpWthe black and white pixel dis-\ntributions.\nA recognition hypothesis, such as the identiï¬cation of a\nsingle symbol, partitions the image domain into three sub-\nsets: the locations assumed to black, B; a small â€œbufferâ€ of\npresumably white pixels surrounding the black pixels, W,\naccountingfortheseparationofsymbols;andtheremaining\nlocations which have not yet been considered, U. Suppose\nwe let pUdenote the distribution for these latter intensities\nofunknownorigin. Assumingthegraylevelsarecondition-\nally independent given the sets B,W,U, we can write the\ndata likelihood as\nP(g) =/productdisplay\nxâˆˆBpB(g(x))/productdisplay\nxâˆˆWpW(g(x))/productdisplay\nxâˆˆUpU(g(x)).\nForexample,ifourimagecontainssinglerigidisolatedsym -bol, then Bwould be the black region of that symbol, W\nwouldbeabufferaroundthisdomainaccountingforitsiso-\nlation, and Uwould bethe remainderof theimage domain.\nWhen optimizing this likelihood over various hypothe-\nses it seems pointless to require each model to account for\ntheentireimage. Instead, we optimize the above likeli-\nhoodfunctionwitheachfactordividedbyourâ€œbackgroundâ€\nmodel pU(g(x))â€” clearly not changing the ranking of hy-\npotheses. Theresultingobjectivefunction,aftertakingl ogs,\nis expressed only in terms of the pixel locations where the\nstateisknown, BandW:\nH(B,W) =/summationdisplay\nxâˆˆBlogpB(g(x))\npU(g(x))+/summationdisplay\nxâˆˆWlogpW(g(x))\npU(g(x))(1)\nFor instance, we look for a single speciï¬c rigid symbol by\nmaximizing this objective function over the location of the\nhypothesized symbol â€” essentially, this is template match-\ning. If the optimal score is less than 0, the background\nmodelgivesthehigherprobabilitythananysymbol-locatio n\npair we can identify, so we believe the symbol does not oc-\ncur in the region. Recognition in more complicated situa-\ntions willproceed analogously, byoptimizing thissame ob-\njective function over multiple symbols, subject to various\ncompositional and non-overlapping constraints.\n2.2 Finding thePage Structure\nWe represent the structure of a page of music hierarchi-\ncally, partitioning the page into systems, each system into\nsystem measures, and each system measure into individual\nstaff measures. We ï¬nd this representation by ï¬rst identify -\ning staves and then grouping the staves into systems using\nthe common bar line positions exhibited in a system. The\nsystems and measures are identiï¬ed by ï¬rst ï¬nding the best\nconï¬guration of shared bar lines for each potential system,\nandthenidentifyingthebestpartitionofstavesintosyste ms,\nboth using DP. This approach is phrased as an optimization\nof Eqn. , essentially seeking the conï¬guration of bar lines\nand systems that explains the maximal amount of black in\ntheimage. Weomitthedetailsofourapproachbecausethis\nislikely theleast challenging aspect of OMR, whileourap-\nproach has similaritieswithanumberof others.\n2.3 Measure Recognition\nMeasures are composed of two kinds of symbols we call\nrigidandcomposite . Rigidsymbols,suchasrestsandclefs,\nconsistofasingleglyphofknownscale,whosepossiblelo-\ncations may have partial constraints (e.g. the vertical po-\nsition of most clefs and rests). In contrast, the compos-\nitesymbols,mostimportantlychords(includingsingle-no te\nâ€œchordsâ€) andbeamed groups, arecomposed ofhighlycon-\nstrainedarrangementsofprimitivesymbols(noteheads,le d-\nger lines, stems, ï¬‚ags, beams, accidentals, augmentation\n30612th International Society for Music Information Retrieval Conference (ISMIR 2011)\ndotsetc.). Whentherigidandcompositesymbolscanbeor-\nderedleft-to-rightinameasure(e.g.amonophonicorhomo-\nphonic line), almost anyordering of symbols makes sense,\nas long as the time signature constraint is obeyed. As a\nconsequence, it seems that a generative model for the mea-\nsure symbols, such as a ï¬nite-state machine, is not likely\nto be powerful or useful. In contrast, chords and beamed\ngroups are natural candidates for top-down model-based,\nï¬nite-state-machine-directedrecognition. Theresultis ahy-\nbrid approach to measure recognition, combining both top-\ndown and bottom-up approaches.\nWe begin by identifying candidates for the composite\nsymbols: potentialbeamcornersforthebeamedgroupsand\npotential stem beginnings for the chords. These candidates\nareexploredthroughprincipledmodel-basedrecognitions tra-\ntegies, as described in Sections 2.3.1 and 2.3.2. We recog-\nnizetheremainingrigidsymbolswithtemplatematchingâ€”\nfor now we only consider rests and clefs at line beginnings,\nthough there are other possibilities. The result of this pro -\ncessisacollectionofmutuallyinconsistentoverlappingh y-\npotheses. Section2.3.3presentsamethodofresolvingthes e\nconï¬‚icts by seeking non-overlapping variations on the rec-\nognized symbols, perhaps completely discarding some hy-\npotheses.\n2.3.1 Isolated Chord Recognition\nWe ï¬nd candidate locations for note stems by convolving\nthe image with appropriate masks designed to â€œlight upâ€\nbothpossiblestemorientations: stem-upandstem-down. In\nï¬nding these oriented candidates we err on the side of false\npositives,sincestemsofisolatedchordsmissedatthissta ge\ncan never be recovered. We now discuss how we identify\nthe best chord beginning from one of these candidate loca-\ntions. If the score, (Eqn. 2.2), of this best chord is less tha n\n0, we donot considerthecandidate further.\nA chord arranges a collection of note heads on a stem,\ndrawing ledger lines for the notes lying off the staff, with\nthe constraint that note heads on the same side of the stem\nmust differ by at least one staff line or staff space. Figure 1\nshowsagenerativemodelforthesomewhatsimplerscenario\nin which the chord is known to be stem-up, there are no\nnotesbelowthestaff,andallnoteheadsareontherightside\nof the stem. Generalizing this situation to the full range of\npossibilitiesincreasesthecomplexityofthegraphstruct ure,\nthough the basicidea remains sound.\nA path through the ï¬gure is a recipe for drawing a par-\nticular chord from bottom to top, as follows. We start in\nthe bottom node of the ï¬gure, drawing the initial portion of\nthe stem, followed by a series of either note heads or blank\nspaces,perhapsseparatedbyanoccasionalhalf-spaceaswe\nmove between note heads centered on staff lines and those\ncenteredonstaffspaces. Thegraphendswithaï¬nalsection\ncontaining â€œself-loopsâ€ accounting for an arbitrary numbe r/0/0/0/0/0/0\n/1/1/1/1/1/1/0/0/0/0/0/0\n/1/1/1/1/1/1/0/0/0/0/0/0\n/1/1/1/1/1/1/0/0/0/0/0/0\n/1/1/1/1/1/1/0/0/0/0/0/0\n/1/1/1/1/1/1/0/0/0/0/0/0\n/1/1/1/1/1/1\n/0/0/0/0/0/0\n/1/1/1/1/1/1/0/0/0/0/0/0\n/1/1/1/1/1/1/0/0/0/0/0/0\n/1/1/1/1/1/1/0/0/0/0/0/0\n/1/1/1/1/1/1/0/0/0/0/0/0\n/1/1/1/1/1/1/0/0/0/0/0/0\n/1/1/1/1/1/1\n/0/0/0\n/1/1/1/0/0/0\n/1/1/1/0/0/0\n/1/1/1/0/0/0\n/1/1/1/0/0/0\n/1/1/1/0/0/1/1\n/0/0/0\n/1/1/1/0/0/0\n/1/1/1\n/0/0/0/0\n/1/1/1/1/0/0/0/0\n/1/1/1/1\n/0/0/0/0\n/1/1/1/1/0/0/0/1/1/1 /0/0/0/1/1/1/0/0/0/1/1/1 /0/0/0/1/1/1\n/0/0/1/1half spaces\ninitial stembetween ledgernotes/spaces\nlines\nnotes/spaces\nbetween staff lines notes/spaces\non staff linenotes/spaces\non ledger\nlines\nFigure1. Adirectedgraphrepresentingafamilyofpossible\nchords.\nof note heads above the staff with associated ledger lines.\nWhile not indicated in the ï¬gure, we can exit the model\nafter visiting (and drawing) any note head. The path that\ngenerates acmajorchord intrebleclef isshown inbold.\nAs is often the case, such a generative model can be\nturned into a recognition engine. Consider the sequence of\npixel rows beginning at the bottom of the stem, continuing\nuptothetopofthechord. Weseekapartitionofthisrowse-\nquence into consecutive intervals: I1,I2,... ,I K, and a la-\nbelingoftheseintervals, s1,s2,... ,s K,suchthatthelabel-\ning is a legal sequence of states from our graph. These two\nsequencesmustsatisfyseveralconstraints. Forinstance, the\ninitial stem must exceed some minimum length, thus con-\nstraining the associated interval. Furthermore, we knowthe\nlocation of the staff lines, so each state corresponding to a\nnote head or space on the staff must be associated with an\ninterval that spans the correct region. Similar constraint s\napply toâ€œabove staffâ€note heads and half spaces.\nForanysuchstateandintervalsequence,wecomputethe\nassociateddatalikelihood,asfollows. Each (sk,Ik)pairas-\nsumes a particular labeling of black image pixels inside Ik.\nAll states must account for the stem, thus must label the re-\ngioncorrespondingtothestemasblack. Additionally,some\noftheotherstatesaccount fornoteheads, perhaps alsowith\nledger lines. Finally we label a small band of white pixels\naround the black pixels of each labeled Ik, thus accounting\nfor our expectation that there will be some minimal separa-\ntion between the chord and othersymbols in the image. We\n307Oral Session 3: Symbolic Music, OMR\n2 beams 1 beampartial beams\nstartend\nstem\n123\n4567\n/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0\n/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1\n/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0\n/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1\n/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0\n/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1\nuRk\nv\nx0\nFigure 2.Left:Graph describing possible beamed struc-\ntures.Right:A beamed structure with an associated region\nRk.x0is the left corner of the beamed group, while uand\nvgive thebeam direction and stem orientation.\ncan then approximate Eqn. 2.2 as\nH(B,W)â‰ˆK/summationdisplay\nk=1H(Bk,Wk) (2)\nwhere BkandWkrepresent the black- and white-labeled\npixels in and around Ik. (Really, BkandWkdepend on\n(sk,Ik), though we have suppressed this in the notation).\nUsing DP, it is a simple matter to compute a global opti-\nmum of this objective function over all partitions and legal\nlabellingsofthesepartitions;thisistheessenceofourch ord\nrecognition strategy.\nA simple modiï¬cation improves this approach. Due to\nthe buffers of white pixels, the regions the {BkâˆªWk}K\nk=1\noverlap,sothatsomepixelsarecountedmultipletimes,per -\nhapsunderbothblack andwhitemodels. Weresolvethisby\nassumingthat (BkâˆªWk)âˆ©(Bk+jâˆªWk+j) =âˆ…forj >1,\nallowing us to correct this error in a pairwise manner. Thus\nwe modifyEqn. 2tobe\nH(B,W) =K/summationdisplay\nk=1H(Bk,Wk)âˆ’H(Bkâˆ’1,k,Wkâˆ’1,k)(3)\nwhere Bk,k+1=Bkâˆ©Bk+1andWk,k+1= (BkâˆªWk)âˆ©\n(Bk+1âˆªWk+1)\\Bk,k+1. In other words, when we en-\ncounter a pixel with given two different labellings, we â€œde-\nferâ€ to the black label. The modiï¬ed objective function is\nstill expressed as a sum of terms that depend on pairs con-\nsecutive states,thus isstillamenable toDP.\n2.3.2 Beamed Group Recognition\nAs with chord recognition, a candidate detection phase ï¬rst\nï¬ndspossiblelocationsfortheleftcornerofpotentialbea med\ngroups,whileclassifyingthesecandidatesâ€œstem-upâ€orâ€œs tem-\ndown,â€ and estimatingthe angle of theparallel beams.\nFigure 2 shows the graph structure we use to model a\nbeamed group (without note heads). This model â€œdrawsâ€\nthebeamsandnotestemsfromlefttoright,forcinganalter-\nnation between note stems and beams, except when partial\nbeams (as in dotted rhythms) are employed. For clarityâ€™ssake, the ï¬gure only allows one or two beams, though our\nactualmodelscanaccountforanynumberofbeams. Forex-\nample, the numbered sequence of transitions generates the\nbeam structure in the right panel of Figure 2. As with the\nchordrecognitionapproachdescribedabove,thestategrap h\nspeciï¬eswhatsequencesofstatesâ€œmakesense,â€inthisway\nlending itself naturally to a DP-based recognition strateg y,\nthistimeparsing along the horizontal dimension.\nSuppose x0gives the left hand corner of the beamed\ngroup, uis a unit vectorpointing in the beam direction, and\nvpointsinthestemdirection(upinthecaseofourFigure2).\n(x0,u,v)areestimatedwhenweidentifyabeamcandidate.\nThus,if Nisthemaximumlengthofthebeamedgroup,we\nseek a partition of {0,1,... ,N }into intervals I1,... ,I K,\nwith labels s1,... ,s Kfor the intervals, forming a legal se-\nquence fromthe stategraph of Figure 2.\nA labeled interval, (Ik,sk), corresponds to a possible la-\nbeling of the pixel data fortheregion\nRk={x: (xâˆ’x0)Â·uâˆˆIk,(xâˆ’x0)Â·v >0}\nas shown in the right panel of Figure 2. Essentially, we\nchoose a black region, Bk, that â€œï¬ts intoâ€ Rk. Forinstance,\nifskisoftypeâ€œsinglebeam,â€ Bkwouldbetheparallelogram-\nshaped of known height â€œsittingâ€ in the bottom of Rk. Orif\nskis of type â€œnote stem,â€ then Bkwould be a thin vertical\nline of known height ï¬tting into the bottom of an equally\nthinRk. By including small buffers of white pixels around\nthe black pixels, Wk, we can form an objective function as\nin Eqn. 3, with Bk,k+1andWk,k+1deï¬ned as before. As\nusual,DP leadstoaglobalmaximumofourobjectivefunc-\ntion, thus estimatingthe desired beam structure.\nAsstatedabove,theapproachonlyrecognizesthebeams\nandstems,thoughnotthenoteheadsandledgerlines. How-\never,aninterestingvariationonthisideacombinestherec og-\nnition of both beam structure and chords into a single opti-\nmization, as follows. When scoring a note stem on a par-\nticular interval, rather than only considering the stem its elf,\nwenesttheoptimizationproblemofSection2.3.1 insidethe\ncurrentoptimization,thussubstitutingthebestconï¬gura tion\nofstem,noteheadsandledgerlinesforthesinglestem. The\nresultisthemostlikelybeamedgroupconï¬guration(notyet\nconsideringnoteheadâ€œdecorationsâ€suchasaccidentalsan d\naugmentation dots), starting from the initial candidate lo ca-\ntion.\n2.3.3 Resolving Conï¬‚icts Between Hypotheses\nWhile our identiï¬cation of each chord and beamed group\nis highly constrained, their overall arrangement within th e\nmeasure is unconstrained. Thus, it is inevitable that we wil l\nï¬nd overlapping and mutually inconsistent symbols. We\nnow describe how we resolve these conï¬‚icts, producing an\nexplanationforthemeasureintermsofnon-overlappingob-\n30812th International Society for Music Information Retrieval Conference (ISMIR 2011)\n/0/0/0/0/0/0/0/0/0/0/0/0\n/1/1/1/1/1/1/1/1/1/1/1/1\n/0/0/0/0/0/0/0/0/0/0/0/0\n/1/1/1/1/1/1/1/1/1/1/1/1\n/0/0/0/0/0/0/0/0/0/0/0/0\n/1/1/1/1/1/1/1/1/1/1/1/1\n/0/0/0/0/0/0/0/0/0/0/0/0\n/1/1/1/1/1/1/1/1/1/1/1/1\n/0/0/1/1\n/0/0/1/1 /0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0\n/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1C A B\nDCE\nFigure 3.Left:Two hypotheses that both â€œclaimâ€ the re-\ngion,C.Right:A network of overlapping regions with\nvarious conï¬‚icts.\njects that still satisfy the essential grammatical constra ints\ndescribed above.\nThe simplest type of conï¬‚ict concerns two hypotheses\nthat both compete for a common subregion, C, as shown in\nthe left panel of Figure 3. Such a situation could arise, for\ninstance, when the single note on the right tries to explain\nthe rightmost note head of the beamed group as an acciden-\ntal. We resolve this conï¬‚ict by running the two recognizers\nagain, now disallowing the use of Cin their recognized re-\nsults. Such constraints are simple to incorporate into our\nrecognizers, and come with little additional cost over the\ninitial computation. Suppose that s1ands2are the uncon-\nstrained scores of the two recognizers, while sâ€²\n1andsâ€²\n2are\nthe constrained scores. Here we choose max(s1+sâ€²\n2,sâ€²\n1+\ns2)asouroptimalscore,thusallocatingthecontestedregion\ntothe betterï¬tting jointmodel.\nThis general idea applies equally well to more complex\nsituations, as in the right panel of Figure 3, showing sev-\neralregions of conï¬‚ict. Here we view the network of con-\nï¬‚icts as a graph, with the recognized regions representing\nnodesand the conï¬‚icts as edges. When this graph structure\nis a tree, we can still compute the optimal assignment of\nthe contested regions to the original hypotheses, thus pro-\nducing a non-overlapping joint hypothesis. To do this, we\nrecognize each region subject to all possible conï¬‚ict sub-\nsets. Thus, for example, the 3 conï¬‚icts involving region B\nin Figure 3 would require 8 possible constrained solutions.\nWith the constrained solutions in place, it is a simple mat-\nter to optimally allocate the regions of conï¬‚ict to the origi -\nnalhypothesesusingfamiliarâ€œmaxpropagationâ€ideasfrom\ngraphical models. In fact, this approach can be extended to\ngraphs containing cycles by an appropriate triangulation o f\nthegraph,ortosituationswherethemorethantwohypothe-\nses claim aregion.\nThis notion of conï¬‚ict resolution also plays a role in our\nrecognition of beamed groups. After having recognized a\nbeamed group in the manner of Section 2.3.2, we proceedtolookforbothaccidentalsandaugmentationdotsthatâ€œbe-\nlongâ€ to the identiï¬ed note heads. Frequently, this intro-\nduces conï¬‚icts into the result when these note head â€œdeco-\nrationsâ€ overlap each other or previously recognized parts\nof the beamed group. In such a case, it is possible foreither\nthe newly recognized decoration, or the original interpret a-\ntionoftheconï¬‚ictregiontobecorrect. Weresolvesuchsit-\nuations though pairwise conï¬‚ict resolution, performing th e\nentire recognition of beamed group and decorations subject\nto constraints that â€œallocateâ€ the region of conï¬‚ict. We re-\nsolve conï¬‚icts sequentially, moving left to right in the rec -\nognized structure. While the result is not optimal, at least i t\nprovides an interpretation that obeys the grammatical con-\nstraintsofthebeamedgroupandensuresthatallrecognized\ndecorations belong torecognized note heads.\n3. RESULTS\nWhile this research is a â€œwork in progress,â€ we present a\nsnapshotofourcurrentstateofthearthere. http://www.music.in-\nformatics.indiana.edu/papers/ismir11 showstheï¬rstï¬vepages\nof the Beethoven 2nd Romance for Violin and Orchestra ,\nop. 50, as recognized by our OMR system. Even though\nour recognition results contain important structural and a s-\nsociativeinformation,theseimagessimplycolortheregio ns\nrecognized over the original image. This coloring is done\nsothatany recognized blackregion showsupinblue, while\nany recognized white region shows up as red. Most, but not\nall,errorsareclearlyvisibleintheseimages,givinganqu ick\ninformal depiction of ourcurrent level of success.\nIn addition, we developed ground truth forthese images,\nassociating each image symbol or primitive with a hand-\nlabeledboundingbox. ThetableofFigure1givesbothfalse\npositives and false negatives for each symbol type. The ta-\nble only lists the symbols we try to recognize at present,\nthus the additional symbols in the image (not included for\nreasons of space) should be counted as a false negatives.\nInperusingtheresultsweobserve several types ofcommon\nconfusions,suchaswithopenandclosednoteheads,aswell\nas sharps and naturals. We also see a natural tendency of\nâ€œout-of-vocabularyâ€ symbols to create false positives. At\npresent, we cannot offer any comparison with other OMR\nresults â€” the evaluation problem here is a research topic in\nits own right. Though our evaluation completely misses the\nimportant interpretation of the symbols, it can be used for\nself-comparisons with future system variations. In essenc e,\nsuch a measure enables the â€œgradient descentâ€ paradigm to\nbe applied tothe overall research effort.\n4. FUTURE WORK\nAt present, we have designed the core of an OMR recog-\nnition engine, though there still remains years of work be-\ntween our current system and one that can harvest large-\n309Oral Session 3: Symbolic Music, OMR\nsymbolname False+ False-\nsolidnotehead .0474/1724 .0468/1718\nnotestem .0229/1573 .0690/1634\nledgerline .0751/701 .0643/693\n2beam .1135/312 .0413/290\n1beam .2376/331 .0823/278\naug. dot .52252/481 .1436/265\n8threst .037/242 .0410/245\n3beam .046/138 .1524/156\nsingleï¬‚agdown .00 0/92 .3651/143\nwhole rest .2128/132 .1012/116\nï¬‚at .078/107 .055/104\nquarterrest .01 1/92 .1010/101\nopen notehead .2825/88 .2926/89\nsingleï¬‚agup .02 1/50 .3425/74\nnatural .14 7/50 .3018/61\ntrebleclef .00 0/60 .00 0/60\nsharp .3621/58 .16 7/44\n16threst .04 1/24 .21 6/29\nbassclef .00 0/20 .00 0/20\ntripleï¬‚agdown .43 9/21 .20 3/15\ntripleï¬‚agup .5913/22 .10 1/10\naltoclef .00 0/10 .00 0/10\n4beam .33 1/3.00 0/2\ndouble ï¬‚agup -0/01.00 1/1\ndouble ï¬‚agdown 1.00 3/3 -0/0\nTable1. Falsepositivesandfalsenegativesforeachsymbol\nand primitive.\nscale symbolic music representations from the IMSLP. We\ncomment here on several of the tasks that must be a part of\nthisvision.\nManyOMRauthorsadvocateenablingthesystemto adapt\ntoaparticulardocument. Sincewehaveperformednotrain-\ning so far, we expect this will be a fruitful direction. Of\ncourse, this opens the door to more power data models by\nmore intricate modeling of within-symbol grey-level distr i-\nbutions. However,trainingalsoallowsustomodelaâ€œpriorâ€\ndistribution (or other regularizing notion) on the a priori\nplausibilityofvarioussymbols,aswellastheâ€œwiggleroom â€\ninthe jointsof thecomposite symbols.\nAn additional step lies between the current output of our\nsystem and the symbolic music representations we desire.\nWhile our recognition approach embeds important seman-\ntic interpretation into a recognized hypothesis, our event ual\nsystem must perform further interpretation, such as under-\nstanding rhythm and voicing. This is an active part of our\nresearcheffortstodate,thoughwedonotdiscussthemhere.\nThisinterpretationphasemayintersectwiththerecogniti on\nphase, allowing us to choose between plausible image in-\nterpretations through global constraints, such as those on a\nmeasure by thetimesignature.\nNumerous authors have also advocated the role of the\nuser interface in an OMR system. In short, the value of the\nresulting data remain suspect until corrected and â€œblessed â€by a knowledgeable person. Given that a user must be in-\nvolved at least this much, it makes sense to think creatively\nabout how the userâ€™s input can be leveraged throughout the\nrecognition process. An obvious possibility is allowing th e\nusertocorrectintermediateresultsinthechainofprocess ing\nsteps, thus avoiding the potential â€œgarbage-in garbage-ou tâ€\nscenariothatoccasionallyplaguescompletelyautomateda p-\nproaches. Anotheralternativeistoallowpartialhand-lab eling\nofmisrecognizedregions. Forinstance,theusermightiden -\ntifyasinglepixel asbelonging toabeam, thusfacilitating a\nconstrained re-recognition ofthe offending region.\n5. REFERENCES\n[1] Fujinaga I. (2000), http://www.music.mcgill.ca/ ich/ re-\nsearch/omr/omrbib.html\n[2] Fujinaga, I. (1996), â€œExemplar-Based Learning in\nAdaptiveOpticalMusicRecognitionSystem,â€ Proceed-\nings of the International Computer Music Conference ,\n55-6.\n[3] G. S. Choudhury, T. DiLauro, M. Droettboom, I. Fuji-\nnaga, B. Harrington, and K. MacMillan, (2000), â€œOp-\ntical Music Recognition System within a Large-Scale\nDigitization Project,â€ in Proceedings, International\nSymposium onMusicInformation Retrieval , 2000.\n[4] F. Rossant and I. Bloch, (2007) â€œRobust and Adaptive\nOMRSystemIncludingFuzzyModeling,FusionofMu-\nsical Rules, and Possible Error Detection,â€ EURASIP\nJournal on Applied Signal Processing , vol: 2007.\n[5] D. Bainbridge and T. Bell, (2001), â€œThe Challenge of\nOptical Music Recognition,â€ Computers and the Hu-\nmanities35: 95-121, 2001.\n[6] N. P. Carter and and R. A. Bacon, (1992), â€œAutomatic\nRecognitionofPrintedMusic,â€In StructuredDocument\nImage Analysis , ed. H. S. Baird, H. Bunke and K. Ya-\nmamoto, 456-65. Berlin: Springer-Verlag.\n[7] B. Couasnon and B. Retif, 1995, â€œUsing a Grammarfor\na Reliable Full Score Recognition System,â€ in Proceed-\ningsofInternationalComputerMusicConference,1995\n187-194, 1995.\n[8] M. Stuckelberg and D. Doermann, 1999, â€œOn Musi-\ncal Score Recognition using Probabilistic Reasoning,â€\ninProceedings of the 5th Annual International Confer-\nence on Document Analysis and Recognition (ICDAR\n99), Bangalore, India, 115-118, 1999.\n[9] G. Kopec, P. Chou, D. Maltz, 1996, â€œMarkov Source\nModel for Printed Music Decoding,â€ Journal of Elec-\ntronicImaging , 5(1), 7-14, 1996.\n310"
    },
    {
        "title": "An Empirical Study of Multi-Label Classifiers for Music Tag Annotation.",
        "author": [
            "Chris Sanden",
            "John Z. Zhang"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1416432",
        "url": "https://doi.org/10.5281/zenodo.1416432",
        "ee": "https://zenodo.org/records/1416432/files/SandenZ11.pdf",
        "abstract": "In this paper we study the problem of automatic music tag annotation. Treating tag annotation as a computational classification process, we attempt to explore the relationship between acoustic features and music tags. Toward this end, we conduct a series of empirical experiments to evaluate a set of multi-label classifiers and demonstrate which ones are more suitable for music tag annotation. Furthermore, we discuss various factors in the classification process, such as feature sets, frame sizes, etc. Experiments on two publicly available datasets show that the Calibrated Label Ranking (CLR) algorithm outperforms the other classifiers for a selection of evaluation measures.",
        "zenodo_id": 1416432,
        "dblp_key": "conf/ismir/SandenZ11",
        "keywords": [
            "automatic music tag annotation",
            "computational classification process",
            "acoustic features",
            "music tags",
            "empirical experiments",
            "multi-label classifiers",
            "Calibrated Label Ranking (CLR)",
            "evaluation measures",
            "publicly available datasets",
            "classification process factors"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nAN EMPIRICAL STUDY OF MULTI-LABEL CLASSIFIERS FOR MUSIC TAG\nANNOTATION\nChris Sanden\nMathematics and Computer Science\nUniversity of Lethbridge\nLethbridge, AB Canada\nsanden@cs.uleth.caJohn Z. Zhang\nMathematics and Computer Science\nUniversity of Lethbridge\nLethbridge, AB Canada\nzhang@cs.uleth.ca\nABSTRACT\nIn this paper we study the problem of automatic music tag\nannotation. Treating tag annotation as a computational clas-\nsiï¬cation process, we attempt to explore the relationship be-\ntween acoustic features and music tags. Toward this end, we\nconduct a series of empirical experiments to evaluate a set of\nmulti-label classiï¬ers and demonstrate which ones are more\nsuitable for music tag annotation. Furthermore, we discuss\nvarious factors in the classiï¬cation process, such as feature\nsets, frame sizes, etc. Experiments on two publicly available\ndatasets show that the Calibrated Label Ranking (CLR) al-\ngorithm outperforms the other classiï¬ers for a selection of\nevaluation measures.\n1. INTRODUCTION\nFor the past decade, digital music collections have been grow-\ning enormously in volume, due to advances in technologies,\nsuch as storage capacity, network transmission, data com-\npression, information retrieval, etc. The rapid rise in music\ndownloading has created a major shift in the music indus-\ntry away from physical media formats to electronic distri-\nbutions. Large on-line music providers now offer millions\nof music catalogs. At present, these catalogs are commonly\nclassiï¬ed and accessed through textual meta-data , such as\ngenre ,style,mood ,artist , etc. This classiï¬cation scheme\nis referred to as music tag annotation and relies on human\nexperts as well as amateurs to annotate the music [18].\nWhile this meta-data is rich and descriptive, it is difï¬cult\nto maintain and in many cases is not comprehensive, due\nto the ambiguity and subjectivity that is introduced in the\nannotation process [7]. Moreover, annotation by human ex-\nperts is an involved process, in terms of ï¬nancial and labor\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proï¬t or commercial advantage and that copies\nbear this notice and the full citation on the ï¬rst page.\nc/circlecopyrt2011 International Society for Music Information Retrieval.costs [4]. Therefore, manual annotation is insufï¬cient and\nineffective when facing large volumes of music. In Music\nInformation Retrieval (MIR), automatic music tag annota-\ntion is an emerging area that aims to help automate the anno-\ntation process. The task of music tag annotation can be de-\nï¬ned as follows [6]. Given a set of tags T={t1, t2, ..., t A}\nand a set of music pieces M={m1, m2, ..., m R}, pre-\ndict for each music piece mjâˆˆMa tag annotation vector\nA= (a1, a2, ..., a A), where ai>0if tag tihas been as-\nsociated with the piece, and ai= 0, otherwise. These ai\ndescribe the strength of the semantic associations between\ntags and the music piece and are typically referred to as se-\nmantic weights . Although these weights can be valuable in\nsome applications, we focus on the binary association where\na tag is either relevant to a music piece or not, i.e., its weight\nis mapped to{0,1}and can be interpreted as a class label. It\nis easy to see that a music piece can have multiple tags and\ntherefore music tag annotation can be modeled as a multi-\nlabel classiï¬cation process [6].\nIn our work, we study the problem of automatic mu-\nsic tag annotation by attempting to learn a relationship be-\ntween acoustic features and music tags. We conduct a se-\nries of experiments on a set of multi-label classiï¬ers which\nhave shown promising results in other application domains\nincluding document classiï¬cation, video annotation, func-\ntional genomics, etc. We demonstrate which classiï¬ers are\nmore suitable for music tag annotation using a set of eval-\nuation measures. While some of these classiï¬ers have been\nused for multi-label classiï¬cation of music into emotions [13]\nand genres [9], we believe that it would be beneï¬cial to ex-\nplore their application in music tag annotation.\n2. RELATED WORK\nAutomatic music tag annotation is an important problem in\nMIR with numerous applications, including music search,\nrecommendation, organization, etc. It has received consid-\nerable attention as of recently and many related techniques\nhave been proposed. One of the most important contribu-\n717Poster Session 6\ntions to the problem is the work of Turnbull et al. [16], who\npropose, along with a dataset called CAL500 , one of the ï¬rst\ntag annotation systems based on a generative probabilistic\nmodel. This dataset has become a de facto benchmark for\nevaluating the performance of music tag annotation systems.\nHoffman et al. [5] present another probabilistic model,\nreferred to as the Codeword Bernoulli Average , which at-\ntempts to predict the probability that a tag applies to a music\npiece. It is claimed that this model outperforms the one from\nTurnbull et al. [16] on the CAL500 dataset. In addition,\nBertin-Mahieux et al. [2] propose Autotagger , a model that\nuses advanced ensemble learning schemes to combine the\ndiscriminative power of different classiï¬ers. Ness et al. [6]\ndescribe how stacked generalization of the probabilistic out-\nputs of a Support Vector Machine (SVM ) can be used to im-\nprove the performance of automatic tag annotation.\nMore recently, Shen et al. [11] propose a framework called\nMMTagger that combines advanced feature extraction tech-\nniques and high-level semantic concept modeling for mu-\nsic tag annotation. The proposed framework uses a mul-\ntilayer architecture that gathers multiple Gaussian mixture\nmodels and SVMs. In addition, Zhao et al. [21] introduce a\nlarge-scale music tag recommender using Explicit Multiple\nAttributes based on tag semantic similarity and music con-\ntent. Experiment results in the work show that the proposed\nrecommender is more effective than existing ones and is at\nleast as effective as other SVM-based approaches.\n3. MULTI-LABEL CLASSIFICATION\nDifferent from traditional single-label classiï¬cation where\neach object belongs to only one class, multi-label classiï¬ca-\ntion deals with the problem where an object may belong to\none or multiple classes simultaneously, i.e., objects are as-\nsociated with a set of labels YâŠ†L, where L(|L|>1) is a\nset of disjoint class labels [14].\nIn our work, we evaluate the following multi-label classi-\nï¬ers for tag annotation. Random k-Labelsets (RAkEL),Cal-\nibrated Label Ranking (CLR),Multi-label k-Nearest Neigh-\nbor(MLkNN),Backpropagation for Multi-Label Learning\n(BPMLL ),Hierarchy of Multi-label Classiï¬ers (HOMER ),\nInstance Based Logistic Regression (IBLR ), and an adapta-\ntion of kNNusing Binary Relevance ( BRkNN). Moreover,\nwe use a Decision Tree (DT) and Support Vector Machine\n(SVM ) as base-level learning algorithms for CLR, RA kEL,\nand HOMER. A total of 10 multi-label classiï¬ers are eval-\nuated. For the sake of space and due to the nature of our\nwork, we will not digress into the details of these classiï¬ers.\nThe interested reader is referred to [3, 12, 14, 15, 19].\nIn order to evaluate the performance of multi-label clas-\nsiï¬ers, a variety of evaluation measures are typically em-\nployed. However, as automatic music tag classiï¬cation is\nrelatively new in MIR, the evaluation measures used varysigniï¬cantly. Furthermore, different classiï¬ers may perform\nbetter under different evaluation measures. Therefore, it is\ndesirable that multiple and contrasting evaluation measures\nare used in any multi-label classiï¬cation experiment. We\nmake use of the following measures which are commonly\nused in the multi-label classiï¬cation literature: Hamming\nLoss (HL),Average Precision (AP),Coverage (CO),Rank-\ning Loss (RL),One-Error (OE),Macro F-Measure (F1),Macro\nPrecision (Precision ), and Macro Recall (Recall ). The inter-\nested reader is referred to [14, 20] for details on them.\n4. EXPERIMENT SETUP\nIn our experiments, the Mulan1open source library for multi-\nlabel learning is used to train and evaluate each of the 10\nclassiï¬ers using default parameters, e.g., the number of neigh-\nbors is set to 10 for MLkNN andIBLR , a linear kernel is\nused to train the SVM.\n4.1 Dataset Selection\nOur experiments are conducted on two publicly available\ndatasets. The Computer Audition Lab 500 dataset ( CAL500 )\n[16] is a collection of 500 Western songs recorded by 500\ndifferent artists. Each song is manually annotated with a\nsubset of 174 tags, which are distributed across 6 attributes:\nMood, Genre, Instrument, Song, Usage, and V ocal. All tags\nare manually generated under controlled experimental con-\nditions and are therefore believed to be of high quality. For\nour experiments, we use the â€œhardâ€ annotations provided\nwith the CAL500 dataset which gives a binary value for all\nsongs and tags indicating whether a tag applies to a song.\nMagnatagatune is a collection of approximately 21,000\nclips of music, each annotated with a combination of 188\ndifferent tags. The annotations are collected through an on-\nline game, referred to as â€œTagATuneâ€, developed to collect\ntags for music and sound clips. Each clip, 29 seconds in\nlength, is an excerpt of music provided by Magnatune.com\nand FreeSound.org. All of the tags in the collection have\nbeen veriï¬ed, i.e. a tag is associated with a clip only if it is\ngenerated independently by more than two players. More-\nover, only those tags that are associated with more than 50\nclips are included in the collection. As discussed by Seyer-\nlehner et al. [10], Magnatagatune is rather difï¬cult to handle\ndue to its size and skewed tag distribution and and has not\nbeen used as widely as CAL500.\n4.2 Feature Sets and Extraction\nPrior to classiï¬cation, the music pieces must be parameter-\nized based on a set of features and their changes over time.\nHowever, it is widely known that there is no accepted cri-\nteria as which features are best for music classiï¬cation [1].\n1http://mulan.sourceforge.net.\n71812th International Society for Music Information Retrieval Conference (ISMIR 2011)\nTherefore, we experiment with three different feature sets,\nto be described below, which are commonly used for music\nclassiï¬cation. The Marsyas2audio processing framework\nis used for the computation of the features.\nTheSpectral feature set, denoted FSs, consists of spectral\nfeatures, including Spectral Flatness Measure, Spectral Cen-\ntroid, Spectral Crest Factor, Spectral Rolloff and Spectral\nFlux.\nTheTimbral feature set, denoted FSt, consists of a combi-\nnation of spectral, temporal and cepstral features. The fol-\nlowing features are included: Zero Crossing Rate, Spectral\nCentroid, Spectral Rolloff, Spectral Flux, MFCC, Chroma.\nTheBeat feature set, denoted FSb, extends FStby includ-\ning rhythmic features that are derived by extracting periodic\nchanges from a beat histogram.\nFollowing a general practice in MIR [8], we model the\naudio signal as the statistical distribution of audio features\ncomputed on individual, short segments. This process yields\na large number of feature vectors. Therefore, the feature\nvectors are then aggregated together using statistical meth-\nods. Although more elaborate representations have been\nproposed in the literature, the simplicity of using a single\nvector for classiï¬cation is appealing [6]. Frame-level fea-\ntures in our experiment are compressed into a single set of\nsong-level features by computing the mean and standard de-\nviation across the feature vectors [6]. Furthermore, we in-\nvestigate the effects of frame size on multi-label classiï¬ca-\ntion. For each <feature set ,classiï¬er >pair, we examine the\nclassiï¬cation performance as we adjust the frame size, fr,\nrepresented as the number of samples collected during a cer-\ntain time period, where frâˆˆ{256,512,1024,2048,4096}\nwith a 50% frame overlap [8].\n5. RESULTS AND DISCUSSIONS\nIn this section, we present the results from our experiments.\nFollowing the practices used in [2,5,16], 10-fold cross vali-\ndation is employed during the evaluation process.\n5.1 CAL500\nIn the ï¬rst set of experiments, we evaluate the multi-label\nclassiï¬ers using the CAL500 dataset. We ï¬nd that for all\nfeature sets, the Calibrated Label Ranking classiï¬er using\na Support Vector Machine, CLR SVM, outperforms the other\nclassiï¬ers when frâˆˆ{1024,2048,4096}. Furthermore, we\nobserve that CLR DT,BPMLL ,MLkNN andBRkNN\nalso perform well over all of the frame sizes and feature\nsets.\n2http://marsyas.sness.net.When we analyze the performance of each classiï¬er over\nthe individual frame sizes, we ï¬nd it difï¬cult to select one\nthat performs well for all of the classiï¬ers. More specif-\nically, we observe that the performance of each classiï¬ers\nis not signiï¬cantly affected by the variation in frame size.\nDespite this, we ï¬nd that CLR SVMperforms the best when\nfr= 4096 . Table 1 shows a comparison of 5 classiï¬ers,\nevaluated by HL, for the three feature sets when fr= 4096 ;\nthe value following Â±gives the standard deviation.\nNote that in the following tables, ( â†“) indicates better per-\nformance when the number is smaller while ( â†‘) indicates\nbetter performance when the number is bigger.\nFSs FSt FSb\nCLR SVM 0.125Â±0.004 0.127Â±0.004 0.128Â±0.004\nBPMLL 0.211Â±0.009 0.218Â±0.008 0.217Â±0.010\nBRkNN 0.130Â±0.004 0.134Â±0.003 0.136Â±0.004\nRAkEL DT 0.152Â±0.003 0.153Â±0.004 0.156Â±0.004\nMLkNN 0.129Â±0.004 0.133Â±0.003 0.135Â±0.003\nTable 1 . Hamming Loss ( â†“) of the classiï¬ers for the three\nfeature sets, FSs,FSt, andFSb, when fr= 4096 .\nFrom the table we can see that HL of each classiï¬er is\nbetter when FSsis used. This is also observed for the other\nevaluation measures. Table 2 presents the performance of\nCLR SVMfor each of the feature sets as evaluated by HL, OE,\nCO, and AP; the best result for each measure is shown in\nbold face. We ï¬nd that CLR SVMperforms the best, for a ma-\njority of the evaluation measures, when FSsis used. While\nspectral features have shown promising results in various\nMIR classiï¬cation tasks, the inclusion of rhythmic features\nhas been shown to increase classiï¬cation performance [17].\nFurther investigation into this result would be desirable.\nHLâ†“ OEâ†“ COâ†“ APâ†‘\nFSs0.125Â±0.004 0.102Â±0.037 116.7Â±2.814 0.586Â±0.016\nFSt0.127Â±0.004 0.094Â±0.047 119.7Â±3.659 0.576Â±0.014\nFSb0.128Â±0.004 0.088Â±0.035 121.6Â±4.018 0.567Â±0.013\nTable 2 . Classiï¬cation performance (mean Â±std) of\nCLR SVMon CAL500 for each feature set where fr= 4096 .\nWhen we analyze HL of CLR SVMfor each of the feature\nsets over all of the frame sizes, we ï¬nd it interesting that\nbothFSsandFStdemonstrate good performance when\nfrâˆˆ{1024,2048,4096}while FSbtends to perform bet-\nter when frâˆˆ{256,512,1024}. This result is discussed\nfurther in Section 5.3.\nTable 3 reports the experiment results of the top 5 multi-\nlabel classiï¬er using FSsandfr= 4096 on CAL500. To\nmake a clearer view of the relative performance between\neach classiï¬er, a partial order â€œ /followsâ€ can be deï¬ned on the\nset of all classiï¬ers for each evaluation measure, where A1\n719Poster Session 6\nHLâ†“ OEâ†“ COâ†“ RLâ†“ APâ†‘ F1â†‘ Precisionâ†‘ Recallâ†‘\nCLR SVM 0.125Â±0.004 0.102Â±0.037 116.736Â±2.814 0.140Â±0.006 0.586Â±0.016 0.497Â±0.027 0.642Â±0.059 0.124Â±0.009\nCLR DT 0.126Â±0.003 0.106Â±0.024 117.417Â±2.970 0.143Â±0.005 0.578Â±0.014 0.445Â±0.027 0.611Â±0.039 0.124Â±0.013\nBPMLL 0.211Â±0.009 0.130Â±0.051 119.878Â±3.880 0.144Â±0.007 0.570Â±0.016 0.479Â±0.016 0.294Â±0.039 0.469Â±0.026\nBRkNN 0.130Â±0.004 0.184Â±0.054 143.503Â±2.834 0.189Â±0.008 0.534Â±0.016 0.429Â±0.017 0.543Â±0.043 0.131Â±0.011\nMLkNN 0.129Â±0.004 0.132Â±0.054 126.082Â±2.994 0.159Â±0.005 0.550Â±0.011 0.476Â±0.014 0.587Â±0.047 0.118Â±0.010\nTable 3 . Classiï¬cation performance (mean Â±std) on CAL500 for FSswhere fr= 4096 .\n/followsA2 means that the performance of classiï¬er A1 is statis-\ntically better than that of classiï¬er A2 on the speciï¬ed mea-\nsure. Following the practice used by Zhang and Zhou [20],\na two-tailed paired t-test at 5% signiï¬cance level is used to\nperform the comparison.\nNote that the partial order â€œ /followsâ€ only measures the rela-\ntive performance between two classiï¬ers A1 and A2 for a\nsingle evaluation measure. It is possible that A1 performs\nbetter than A2 in terms of some measure but worse than A2\nin terms of other ones. In this case, it is hard to judge which\nclassiï¬er is superior. Therefore, in order to give an overall\nperformance assessment of a classiï¬er, a score is assigned\nto it which takes into account its relative performance with\nother classiï¬ers on all evaluation measures. For each mea-\nsure, for each possible pair of classiï¬ers A1 and A2, if A1\n/followsA2 holds, then A1 is rewarded by a positive score +1 and\nA2 is penalized by a negative score -1. Based on the accu-\nmulated score of each classiï¬er on all evaluation measures,\na total order â€œ >â€ is deï¬ned on the set of all classiï¬ers [20].\nTable 4 presents an example of this process; the accumu-\nlated score for each classiï¬er is shown in parentheses.\nMulti-label Classiï¬er\nA1-BPMLL ; A2-CLR DT; A3-CLR SVM; A4-MLkNN\nHamming Loss A2 /followsA1, A3/followsA1, A3/followsA4, A4/followsA1\nCoverage A1 /followsA4, A2/followsA4, A3/followsA4\nRanking Loss A1 /followsA4, A2/followsA4, A3/followsA4\nAverage Precision A1 /followsA4, A2/followsA4, A3/followsA1, A3/followsA4\nTotal Order A3(6) >A2(4) >A1(-1) >A4(-9)\nTable 4 . Relative performance between four multi-label\nclassiï¬cation algorithms on the CAL500 dataset.\nThe total order of all 10 multi-label classiï¬ers on CAL500\nis as follows: CLR SVM(42)> CLR DT(31)> BPMLL\n(25)> MLkNN (20)> BRkNN (-1)> RAkEL SVM(-7)\n> HOMER SVM(-9)> RAkEL DT(-13)> HOMER DT(-\n35)> IBLR (-53). It can be seen that CLR SVMoutperforms\nall the other classiï¬ers on the CAL500 dataset. Furthermore,\nCLR DT,BPMLL ,MLkNN , and BRkNN demonstrate\ngood performance and outperform the remaining classiï¬ers.\n5.2 Magnatagatune\nFor the second set of experiments we evaluate the classiï¬ers\nusing the Magnatagatune dataset. We ï¬nd that for all fea-ture sets, CLR SVMoutperforms all the other classiï¬ers when\nfrâˆˆ { 1024,2048,4096}. Furthermore, we observe that\nCLR DT,BPMLL ,MLkNN , and BRkNN , offer com-\nparable performance over all of the frame sizes and feature\nsets.\nFSs FSt FSb\nCLR SVM 0.022Â±0.002 0.021Â±0.002 0.021Â±0.001\nBPMLL 0.073Â±0.003 0.074Â±0.002 0.022Â±0.002\nBRkNN 0.021Â±0.002 0.021Â±0.002 0.022Â±0.002\nRAkEL DT 0.023Â±0.002 0.023Â±0.001 0.023Â±0.001\nMLkNN 0.021Â±0.002 0.021Â±0.002 0.022Â±0.002\nTable 5 . Hamming Loss ( â†“) of the classiï¬ers for the three\nfeature sets, FSs,FSt, andFSb, when fr= 2048 .\nOnce again, it is difï¬cult to select a frame size that works\nwell for all of the classiï¬ers. We observe that each classiï¬er\nperforms differently, for each feature set, over the differ-\nent frame sizes. In spite of this, CLR SVMperforms the best\nwhen fr= 2048 . Table 5 shows a comparison of 5 multi-\nlabel classiï¬ers, as evaluated by HL, for the three feature\nsets when fr= 2048 . From the table, we ï¬nd that HL is bet-\nter for a majority of the classiï¬ers when FStis used. It can\nbe seen that HL of CLR SVMandBPMLL is better when\nFSbis used. If we extend our analysis to include additional\nevaluation measures, we ï¬nd that, on average, performance\nimproves with the use of FStfor a majority of classiï¬ers.\nTable 6 presents the performance of CLR SVMfor each fea-\nture set.\nHLâ†“ OEâ†“ COâ†“ APâ†‘\nFSs0.022Â±0.002 0.423Â±0.028 40.6Â±4.709 0.479Â±0.017\nFSt0.021Â±0.002 0.403Â±0.050 38.9Â±4.687 0.505Â±0.027\nFSb0.021Â±0.002 0.413Â±0.037 40.7Â±5.031 0.495Â±0.024\nTable 6 . Classiï¬cation performance (mean Â±std) of\nCLR SVMon Magnatagatune for each feature set where fr=\n2048 .\nTable 7 presents the experiment results of the top 5 multi-\nlabel classiï¬ers using FStandfr= 2048 on Magnata-\ngatune. We note that, for a majority of the evaluation mea-\nsures, the performance of each classiï¬er is better on Mag-\nnatagatune than on CAL500. We will discuss more on this\n72012th International Society for Music Information Retrieval Conference (ISMIR 2011)\nHLâ†“ OEâ†“ COâ†“ RLâ†“ APâ†‘ F1â†‘ Precisionâ†‘ Recallâ†‘\nCLR SVM 0.021Â±0.002 0.403Â±0.050 38.915Â±4.687 0.076Â±0.009 0.505Â±0.027 0.350Â±0.029 0.738Â±0.088 0.018Â±0.002\nCLR DT 0.021Â±0.002 0.471Â±0.041 42.321Â±5.461 0.085Â±0.009 0.459Â±0.019 0.330Â±0.023 0.573Â±0.073 0.029Â±0.003\nBPMLL 0.074Â±0.004 0.690Â±0.037 42.081Â±4.725 0.088Â±0.012 0.360Â±0.015 0.282Â±0.015 0.118Â±0.041 0.268Â±0.030\nBRkNN 0.021Â±0.001 0.451Â±0.043 76.343Â±6.978 0.166Â±0.018 0.448Â±0.022 0.376Â±0.026 0.591Â±0.063 0.045Â±0.005\nMLkNN 0.021Â±0.002 0.443Â±0.040 51.168Â±5.082 0.102Â±0.010 0.468Â±0.024 0.390Â±0.041 0.612Â±0.068 0.045Â±0.008\nTable 7 . Classiï¬cation performance (mean Â±std) on Magnatagatune for FStwhere fr= 2048 .\nin the following section.\nSimilarly as the CAL500 dataset, the partial order â€œ /followsâ€\nand the total order â€œ >â€ are also deï¬ned on the set of all\nclassiï¬ers. The total ordering for all 10 multi-label classi-\nï¬ers on Magnatagatune is as follows (the accumulated score\nfor each classiï¬er is shown in parentheses): CLR SVM(37)>\nMLkNN (28)> CLR DT(24)> BRkNN (22)> BPMLL\n(-1)> RAkEL DT(-7)> HOMER SVM(-11)> RAkEL SVM\n(-21) > IBLR (-31) > HOMER DT(-40). It can be seen\nthatCLR SVMoutperforms all of the multi-label classiï¬ca-\ntion algorithms on the Magnatagatune dataset. Furthermore,\nMLkNN ,CLR DT,BRkNN , andBPMLL perform well\nfor a selection of evaluation measures.\n5.3 Discussions\nBase Classiï¬er : From our experiments presented above, we\nobserve that using a SVM as the base-level learning algo-\nrithm for CLR ,RAkEL , and HOMER offers improve-\nments over using a decision tree. This result is observed for\nboth of the datasets. Table 8 reports the experimental results\nofCLR ,RAkEL , and HOMER on the CAL500 dataset\nusing a SVM and DT as base classiï¬ers. It would be inter-\nesting to explore alternative base-level learning algorithms\nfor music tag annotation.\nHLâ†“ OEâ†“ APâ†‘\nCLR DT 0.126Â±0.003 0.106Â±0.024 0.578Â±0.014\nCLR SVM 0.125Â±0.004 0.102Â±0.037 0.586Â±0.016\nHOMER DT 0.196Â±0.007 0.808Â±0.061 0.355Â±0.020\nHOMER SVM 0.159Â±0.004 0.581Â±0.051 0.427Â±0.015\nRAkEL DT 0.151Â±0.003 0.283Â±0.045 0.473Â±0.010\nRAkEL SVM 0.125Â±0.004 0.239Â±0.048 0.424Â±0.013\nTable 8 . Classiï¬cation performance (mean Â±std) of CLR ,\nRAkEL , andHOMER on CAL500 using a SVM and DT\nas base classiï¬ers.\nFeature Set : We ï¬nd it interesting that, on average, classi-\nï¬cation using FSsandFSttends to demonstrate good per-\nformance when frâˆˆ{1024,2048,4096}while using FSb\nresults in better performance when frâˆˆ{256,512,1024}.\nThis might be explained by the notion that the smaller frame\ncaptures better rhythmic information over the entire music\npiece. Furthermore, a large frame may be more likely tocapture the long-term nature of the music, including melodic,\nand harmonic composition, which could lead to improved\nclassiï¬cation accuracy. While we ï¬nd small improvements\nin classiï¬cation performance using different frame sizes, we\nobserve large differences in performance between the best\nfeature set and worst feature set for a selection of evalua-\ntion measures and classiï¬ers. For example, the performance\nofBPMLL on Magnatagatune, as evaluated by AP, varies\nfrom 0.04% using FSbto 37% using FSt. In addition, we\nï¬nd that the best classiï¬cation performance is achieved on\nCAL500 and Magnatagatune using FSsandFSt, respec-\ntively. However, it is important to note that there is no ac-\ncepted criteria as which features are best for music classi-\nï¬cation [1]. Therefore, our observation in the experiments\nreported in this work may not be conclusive.\nDatasets : For a majority of the evaluation measures, it can\nbe seen that the classiï¬ers perform better on Magnatagatune,\ncompared to CAL500. For example, CLR SVMachieves a\nHamming Loss of 0.0211 on the former and 0.1247 on the\nlatter. One possible explanation for this observation is that\nthe average number of tags for each instance in Magnata-\ngatune is less than CAL500, i.e., each music piece in Mag-\nnatagatune is annotated with approximately 3 tags while each\nmusic piece in CAL500 is annotated with approximately\n26 tags. We also observe that classiï¬cation performance\nvaries for each dataset depending on individual feature sets.\nFor instance, classiï¬cation using FSsperforms the best on\nCAL500 while using FStdemonstrates the best performance\non Magnatagatune; we note that using FSsshows the worst\nclassiï¬cation performance on Magnatagatune. This leads\nus to believing that the spectral features used in our exper-\niment tend to give rise to better performance over longer\npieces of music while using timbral features performs bet-\nter on shorter music. Whether this is true in general needs\nfurther investigation.\n6. CONCLUSION\nIn this paper we present our initial attempts on automatic\nmusic tag annotation. In our work, we conduct a series of\nexperiments, on a set of multi-label classiï¬ers, exploring\nthe effects of different feature sets and frame sizes on tag\nannotation. The results offer insight into which classiï¬ers\nand features are more suitable for this task. We ï¬nd that\n721Poster Session 6\nthe Calibrated Label Ranking (CLR) classiï¬er consistently\nperforms well for a selection of evaluation measures when\nusing spectral and timbral features.\nFurther investigation is needed into the selection of clas-\nsiï¬er parameters. Recall that each classiï¬er is trained using\ndefault parameters. It would be interesting to explore the in-\nï¬‚uence of these parameters on tag annotation performance.\nIn addition, it would be interesting and beneï¬cial to com-\npare our results to existing results in the literature based on\na set of common measures.\n7. REFERENCES\n[1] J. Bergstra, N. Casagrande, D. Erhan, D. Eck, and\nB. K Â´egl. Aggregate features and AdaBoost for music\nclassiï¬cation. Machine Learning: Special Issue on Ma-\nchine Learning in Music , 65(2-3):473â€“484, 2006.\n[2] T. Bertin-Mahieux, D. Eck, F. Maillet, and P. Lamere.\nAutotagger: A model for predicting social tags from\nacoustic features on large music databases. J. New Music\nResearch , 37(2):115â€“135, 2008.\n[3] W. Cheng and E. H Â¨ullermeier. Combining instance-\nbased learning and logistic regression for multil-\nabel classiï¬cation. Machine Learning , 76(2-3):211â€“225,\n2009.\n[4] A. Eronen. Signal Processing Methods for Audio Clas-\nsiï¬cation and Music Content Analysis . PhD thesis, Tam-\npere University of Technology, Tampere, Finland, 2009.\n[5] M. Hoffman, D. Blei, and P. Cook. Easy as CBA: A\nsimple probabilistic model for tagging music. In Proc.\nIntâ€™l Conf. Music Information Retrieval , pages 369â€“374,\n2009.\n[6] S. R. Ness, A. Theocharis, G. Tzanetakis, and L. G.\nMartins. Improving automatic music tag annotation us-\ning stacked generalization of probabilistic svm outputs.\nInProc. ACM Intâ€™l Conf. Multimedia , pages 705â€“708,\n2009.\n[7] F. Pachet. Content management for electronic music dis-\ntribution. Communications of the ACM , 46(4):71â€“75,\n2003.\n[8] F. Pachet and P. Roy. Improving multi-label analysis\nof music titles: A large-scale validation of the correc-\ntion hypothesis. IEEE Trans. Audio, Speech & Language\nProcessing , 17(2):335â€“343, 2009.\n[9] C. Sanden and J. Z. Zhang. Enhancing multi-label mu-\nsic genre classiï¬cation through ensemble techniques. In\nProc. ACM Intâ€™l Conf. Research and development in In-\nformation , pages 705â€“714, 2011.[10] K. Seyerlehner, G. Widmer, M. Schedl, and P. Knees.\nAutomatic music tag classiï¬cation based on block-level\nfeatures. In Proc. Sound and Music Computing Conf. ,\n2010.\n[11] J. Shen, W. Meng, S. Yan, H. Pang, and X. Hua. Ef-\nfective music tagging through advanced statistical mod-\neling. In Proc. ACM Intâ€™l Conf. Information Retrieval ,\npages 635â€“642, 2010.\n[12] E. Spyromitros, G. Tsoumakas, and I. Vlahavas. An\nempirical study of lazy multilabel classiï¬cation algo-\nrithms. In Proc. Hellenic Conf. Artiï¬cial Intelligence ,\npages 401â€“406, 2008.\n[13] K. Trohidis, G. Tsoumakas, G. Kalliris, and I. Vla-\nhavas. Multilabel classiï¬cation of music into emotions.\nInProc. Intâ€™l Conf. Music Information Retrieval , pages\n325â€“330, 2008.\n[14] G. Tsoumakas and I. Katakis. Multi-label classiï¬cation:\nAn overview. Intâ€™l J. Data Warehousing and Mining ,\n3(3):1â€“13, 2007.\n[15] G. Tsoumakas, I. Katakis, and I. Vlahavas. Effective and\nefï¬cient multilabel classiï¬cation in domains with large\nnumber of labels. In Proc. ECML/PKDD Workshop on\nMining Multidimensional Data , 2008.\n[16] D. Turnbull, L. Barrington, D. Torres, and G. Lanckriet.\nSemantic annotation and retrieval of music and sound\neffects. IEEE Trans. Audio, Speech, and Language Pro-\ncessing , 16(2):467â€“476, 2008.\n[17] G. Tzanetakis and P. Cook. Musical genre classiï¬cation\nof audio signals. IEEE Trans. Speech and Audio Pro-\ncessing , 10(5):293â€“302, 2002.\n[18] K. West. Novel Techniques for Audio Music Classiï¬ca-\ntion and Search . PhD thesis, University of East Anglia,\nUK, 2008.\n[19] M-L. Zhang and Z-H. Zhou. Multilabel neural net-\nworks with applications to functional genomics and\ntext categorization. IEEE Trans. Knowl. and Data Eng. ,\n18:1338â€“1351, 2006.\n[20] M-L. Zhang and Z-H. Zhou. ML- kNN: A lazy learning\napproach to multi-label learning. Pattern Recognition ,\n40(7):2038â€“2048, 2007.\n[21] Z. Zhao, X. Wang, Q. Xiang, A. M. Sarroff, Z. Li, and\nY . Wang. Large-scale music tag recommendation with\nexplicit multiple attributes. In Proceedings of the in-\nternational conference on Multimedia , pages 401â€“410,\n2010.\n722"
    },
    {
        "title": "A Regularity-Constrained Viterbi Algorithm and Its Application to The Structural Segmentation of Songs.",
        "author": [
            "Gabriel Sargent",
            "FrÃ©dÃ©ric Bimbot",
            "Emmanuel Vincent 0001"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1415950",
        "url": "https://doi.org/10.5281/zenodo.1415950",
        "ee": "https://zenodo.org/records/1415950/files/SargentBV11.pdf",
        "abstract": "This paper presents a general approach for the structural segmentation of songs. It is formalized as a cost optimization problem that combines properties of the musical content and prior regularity assumption on the segment length. A versatile implementation of this approach is proposed by means of a Viterbi algorithm, and the design of the costs are discussed. We then present two systems derived from this approach, based on acoustic and symbolic features respectively. The advantages of the regularity constraint are evaluated on a database of 100 popular songs by showing a significant improvement of the segmentation performance in terms of F-measure.",
        "zenodo_id": 1415950,
        "dblp_key": "conf/ismir/SargentBV11",
        "keywords": [
            "structural segmentation",
            "cost optimization problem",
            "musical content",
            "prior regularity assumption",
            "Viterbi algorithm",
            "acoustic features",
            "symbolic features",
            "database of 100 popular songs",
            "F-measure",
            "significant improvement"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nA REGULARITY-CONSTRAINED VITERBI ALGORITHM AND ITS\nAPPLICATION TO THE STRUCTURAL SEGMENTATION OF SONGS\nGabriel Sargent\nUniversit Â´e de Rennes 1,\nIRISA (UMR 6074)\ngabriel.sargent@irisa.frFrÂ´edÂ´eric Bimbot\nCNRS,\nIRISA (UMR 6074)\nfrederic.bimbot@irisa.frEmmanuel Vincent\nINRIA Rennes\nBretagne Atlantique\nemmanuel.vincent@inria.fr\nABSTRACT\nThis paper presents a general approach for the structural\nsegmentation of songs. It is formalized as a cost optimiza-\ntion problem that combines properties of the musical con-\ntent and prior regularity assumption on the segment length.\nA versatile implementation of this approach is proposed by\nmeans of a Viterbi algorithm, and the design of the costs\nare discussed. We then present two systems derived from\nthis approach, based on acoustic and symbolic features re-\nspectively. The advantages of the regularity constraint are\nevaluated on a database of 100 popular songs by showing\na signiï¬cant improvement of the segmentation performance\nin terms of F-measure.\n1. INTRODUCTION\nMusic structure is one of the properties which contributes to\nthe characterization of a music piece. It describes its tem-\nporal organization at a high level, by means of segments\nlabeled according to their musical content and their rela-\ntionships with one another. The automatic structural seg-\nmentation of songs is generally addressed by analyzing the\nhomogeneity and the repetitiveness of the musical content\nover time (timbre, harmony, rhythm, melody).\nRecent work [2] proposes a single-level deï¬nition of the\nstructure of a music piece based on a regularity assump-\ntion. It implies the prevalence of one (or a few) typical\nsegment duration(s) within each song, i.e.structural pulsa-\ntion period(s). Indeed, a large part of western popular music\nis built on musical patterns (rhythmic cells, chord progres-\nsions, melodies...) which show cyclic behaviors and which\nare fully or partly repeated over time. This induces some\nsort of regularity in the structure of songs.\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proï¬t or commercial advantage and that copies\nbear this notice and the full citation on the ï¬rst page.\nc/circlecopyrt2011 International Society for Music Information Retrieval.The present work is based on this regularity assumption\nin music. We introduce a general segmentation framework,\nwhich consists of an optimization method to ï¬nd the best\nsegmentation combining the similarities and/or the contrasts\nin musical content and the regularity of the segments. An\nimplementation of this method is proposed by means of a\nViterbi algorithm.\nA similar segmental Viterbi algorithm was brieï¬‚y\nsketched in [10] in the context of a probabilistic model\n(segmental HMM). In this paper, we make it more explicit\nand we extend it to any type of cost function. This makes\nit possible to exploit combinations of clustering-based and\nsimilarity-matrix-based approaches and to a wider variety of\nsituations outside the probabilistic framework. We also dis-\ncuss the importance of the regularity cost in the estimation\nof the segment boundaries, and provide experimental results\nwith several choices for the two terms of the segmentation\ncost.\nThe structure of the paper is as follows. In section 2\nwe present the general music segmentation method, with-\nout considering a particular musical feature or temporal\nscale. Section 3 describes its implementation by means of a\nViterbi algorithm, and discusses the expression of segmen-\ntation costs. In section 4, after brieï¬‚y reviewing former\nwork on music structure, we apply the proposed segmen-\ntation method to this particular problem. We then present\ntwo structural segmentation systems based on the algorithm\ndeveloped above. Section 5 evaluates the effect of the incor-\nporation of regularity constraints thanks to the evaluation of\nthese systems on the RWC popular music database [6].\n2. GENERAL APPROACH\nThis section presents a general method for the temporal\nsegmentation of music pieces, when regularity assumptions\ncan be hypothesized on the segment length. It consists of\nan optimization process where the optimal segmentation is\nsearched simultaneously considering the properties of the\ndata and the regularity of the segmentation.\nA music piece Xcan be described as a sequence of N\nfeatures{xt}1â‰¤tâ‰¤Nalong a particular temporal scale ( e.g.\n483Poster Session 4\nframes, or beats...). We denote Xtj\nti={xt}tiâ‰¤t<t jthe se-\nquence of features associated to the temporal interval [ti,tj[.\nLet us deï¬ne a segmentation S={sk}1â‰¤kâ‰¤nofXas a\nsequence of nintervalssk= [tk,tk+1[, with the following\nconventions :\nâ€¢t1= 1<...<tk<...<tn<tn+1=N+ 1,\nâ€¢s0= [t0,t1[= [0,1[, for the algorithm initialization,\nâ€¢mk=tk+1âˆ’tkis the length of sk.\nWe aim at ï¬nding the optimal segmentation, by minimiz-\ning a certain cost function.\nWe assume that the cost function Ccan be written as\nC(S) =n/summationdisplay\nk=1Î“(sk) (1)\nwith\nÎ“(sk) = Î¦(sk) +Î»(Ï„)Î¨(sk) +/epsilon1 (2)\nwhere\nâ€¢Î¦(sk)is a content-based segmentation cost, which\ntakes low values when the sequence of features in sk\nis likely to correspond to a structural segment. This\ncost can be described according to different families\nof functions, like change detection functions or sim-\nilarity functions. It can also, for instance, be derived\nfrom a probabilistic function P(sk), asâˆ’logP(sk).\nâ€¢Î¨(sk)is a regularity cost. We consider that the reg-\nularity of a segmentation depends on the deviation of\nthe length of its segments to a prior reference length\nÏ„called the structural pulsation period (as a conse-\nquence, Î¨(sk)decreases as mkapproachesÏ„). Note\nthat, if the values of mkare expected to follow a par-\nticular distribution Ï€(mk)aroundÏ„,Î¨(sk)can be set\nasÎ¨(sk) =âˆ’log(Ï€(mk)).\nâ€¢Î»(Ï„)is a balance parameter between these two costs.\nâ€¢In practice, we add a small constant /epsilon1 >0to give a\nslight advantage to longer segments in the case where\nÎ¦andÎ¨would be equivalent for several segmenta-\ntions.\n3. IMPLEMENTATION\nThis section presents an implementation of the approach\npresented above, and describes possible choices of cost\nfunctions Î¦,Î¨and parameter Î».\nFigure 1 . Admissible predecessors for tand their costs\n3.1 Viterbi algorithm\nLetst,hbe the interval corresponding to Xt\ntâˆ’h= [xtâˆ’h,xt[,\nthe set of features which precede the temporal index twithin\na window of length h. We denote Has the maximal window\nlength considered1.\nâ€¢Initialization ( t= 1)\nWe setS1={[0,1[}andC1= 0.\nâ€¢Fort= 1 :Nâˆ’1\nWe consider{st,h}1â‰¤hâ‰¤H/prime, withH/prime=min(tâˆ’1,H)\nas the set of admissible predecessors for temporal in-\ndext.\nThe optimal segmentations {Stâˆ’h}1â‰¤hâ‰¤H/primeending\nat indexes{tâˆ’h}1â‰¤hâ‰¤H/primeare assumed to be\nknown, as well as their associated cumulative costs\n{Ctâˆ’h}1â‰¤hâ‰¤H/prime.\nThen, the best partial segmentation Stis built by\nchoosing the extension of the former partial segmen-\ntationStâˆ’hwith the lowest cost. We evaluate respec-\ntively :\n1.Î“(st,h)for1â‰¤hâ‰¤H/prime,\n2.b(t) =argmin1â‰¤hâ‰¤H/prime{Ctâˆ’h+ Î“(st,h)},\n3.Ct=Ctâˆ’b(t)+ Î“(st,b(t))\nWe can note that St=Stâˆ’b(t)âˆª{St,b(t)}.\nThe optimal segmentation for X, notedSoptwith cost\nCN+1, is obtained by backtracking the optimal prede-\ncessors stored in b(t). The associated temporal indexes\n{tk}1â‰¤kâ‰¤noptare then found thanks to the following recur-\nsion :\n1.tnopt+1=b(N+ 1) ,\n2.tk=b(tk+1), for1â‰¤kâ‰¤nopt.\nnoptis the number of boundaries of Sopt, obtained after this\nbacktracking process.\n1Typically,H=N, but smaller values can be used ( e.g.multiples of\nÏ„).\n48412th International Society for Music Information Retrieval Conference (ISMIR 2011)\n3.2 Design of the cost functions\n3.2.1 Content-based segmentation cost Î¦\nThe objective of the content-based segmentation cost is to\nevaluate a set of segments according to the redundancy\nof their content. Segmentations with lower costs are ex-\npected to consist of segments built on the same musical pat-\nterns. Different families of functions can be considered, like\nabrupt change detection criteria or similarity functions.\nAbrupt change detection criteria assign a low cost to\nsegments associated to probable boundaries. In automatic\nstructure inference, [3] uses for example a â€œnovelty func-\ntionâ€ based on the analysis of the local homogeneity of the\nsong over time.\nSimilarity functions aim to assign a low cost to segments\nmade of sequences of features repeated elsewhere in the\nsong. We can deï¬ne such a function as\nÎ¦(sk) =minÎ¸âˆˆZk{Ï†(Xtk+mk\ntk,XÎ¸+mk\nÎ¸)}. (3)\nThe lowest dissimilarity Ï†is taken between the sequence\nof featuresXtk+mk\ntkfromsk(of lengthmk) and any other\nsequence of the same length contained in a portion ZkofX.\nIn particular, Î¦(sk) = 0 when the sequence of features of sk\nis exactly repeated elsewhere in Zk,Î¦(sk)>0otherwise.\nZk= [1,tkâˆ’mk]âˆª[tk+mk,N]can be chosen to avoid\nintra-segment comparisons. In the case of a binary dissimi-\nlarity, where a song is described as a sequence of symbolic\nfeatures, the following function can be chosen :\nÏ†(Xtk+mk\ntk,XÎ¸+mk\nÎ¸) =mkâˆ’1/summationdisplay\np=01âˆ’Î´(xtk+p,xÎ¸+p),(4)\nwhereÎ´is Kroneckerâ€™s delta (equals 1 when arguments have\nthe same value, 0 otherwise). More generally any non-\nbinary function can be used in equation (3).\n3.2.2 Regularity cost Î¨\nThe regularity cost Î¨of a segmentation is based on the mea-\nsure of the deviation between the length of its segments from\na reference length Ï„. It can show the following properties :\n1.Î¨(Ï„) = 0 ,\n2.Î¨(mk)>0, taking higher values as the segment\nlengthmkmoves away from Ï„.\nA lot of functions can satisfy these properties. We consider\ntwo categories of functions : convex and non-convex func-\ntions. As non-convex functions verify the property :\nÎ¨(Ï„) + Î¨(Ï„+ âˆ†)<Î¨(Ï„+ âˆ† 1) + Î¨(Ï„+ âˆ† 2) (5)\nwith\nâˆ† = âˆ† 1+ âˆ† 2, (6)\nFigure 2 . Examples of regularity costs Î¨Î±for\nÎ±={0.5,1,2}andÏ„= 32\nâˆ†1>0andâˆ†2>0 (7)\nthey favor segmentations made of fewer irregular segments.\nBy contrast, convex functions tend to favor segmentations\nwith irregularities spread across several segments.\nAs an illustration, we consider the following family of\nsymmetric functions derived from the lÎ±norm :\nÎ¨Î±(mk) =|mk\nÏ„âˆ’1|Î±(8)\nmkis the length of interval sk, andÎ±controls the convexity\nof the function (we have a non-convex function if 0<Î±<\n1, and a convex one if Î±â‰¥1). Figure 2 shows Î¨Î±for\nÎ±={0.5,1,2}.\n3.3 Balance parameter Î»\nWe consider that Î»depends onÏ„as the probability of having\nirregular segments grows with the number of segments, and\ntherefore with the inverse of Ï„. We choose the linear relation\nÎ»(Ï„) =Î»Ï„, whereÎ»is a constant parameter to be tuned.\n4. APPLICATION TO THE STRUCTURAL\nSEGMENTATION OF SONGS\nThe work presented in section 3 is primarily intended to the\nstructural segmentation of songs. Automatic music structure\ninference is a difï¬cult task, because the problem to be solved\nis usually ill-posed. Moreover, it requires the analysis and\nthe complex combination of features and criteria through the\ndevelopment of sophisticated metrics and algorithms. In this\nsection, we review brieï¬‚y the main state-of-the-art methods\nfor automatic structural segmentation of songs, before de-\nscribing two structural segmentation systems implemented\nfrom the proposed method.\n4.1 State-of-the-art\nDifferent approaches have been proposed to the problem of\nautomatic structure inference. They generally use acous-\ntic features, such as Mel-Frequency Cepstral Coefï¬cients\n(MFCCs) and Chroma vectors, which characterize the in-\nstrumental timbre and the harmonic content respectively.\nOther features are described in [17], [1], and [8]. Struc-\ntural segments are assumed to show stable instrumentation\n485Poster Session 4\n(often associated to homogeneous timbre) and therefore to\nappear as blocks with speciï¬c textures in similarity matri-\nces [3,12], or sequences of similar states in Hidden Markov\nModels (HMMs) [11].\nRepeated harmonic progressions can be detected by lo-\ncalizing the sequences of high similarity coefï¬cients in sub-\ndiagonals of the chroma-based similarity matrix [4]. Other\napproaches, like HMMs [10,14], or more recently Non neg-\native Matrix Factorization [20] are also used for the recogni-\ntion of repeated harmonic patterns. Some methods use dy-\nnamic programming : Shiu et al. interpret the chroma-based\nsimilarity matrix as a time-state representation and use the\nViterbi algorithm to ï¬nd the path with the highest score in\nterms of similarity through it [15]. A constraint is set to give\npriority to the diagonal direction for the path, and implicitly\ninï¬‚uence the length of the estimated structural segments.\nSome other approaches combine these content-based\nmethods by means of optimization problems, as in [8, 12].\nA more detailed state of the art is available in [13].\nIn the following section, we present two systems that in-\nfer the structural segmentation of a song, incorporating the\nidea of â€structural pulsation periodâ€2.\n4.2 Presentation of the systems\nThese systems perform a structural segmentation of songs\ncombining content-based segmentation under a regularity\nconstraint by means of the Viterbi algorithm presented in\nsection 3.1. System 1 uses acoustic features to compute\nchange detection criteria and estimates the main structural\npulsation period Ï„from the audio. System 2 analyzes sym-\nbolic features, uses a similarity function and prior knowl-\nedge ofÏ„(ï¬xed at 32 beats). As features are considered\nat the beat scale, a beat detection system is needed. We\nevaluate for these 2 systems the impact of incorporating a\nregularity constraint on the relative performance of the seg-\nmentation.\n4.2.1 System 1 : combination of change detection criteria\non acoustic features\nThe system we consider is the one described in [16]. In\nthis paper, we consider variants of this system both with\nand without the regularity constraint in order to analyze its\nimpact on structural segmentation inference. The content-\nbased segmentation cost is based on 3 statistical criteria\nwhich measure for each temporal index the likelihood ratio\nof a structural segment boundary. This criterion combines\ninstrumental changes, short events and contrastive patterns\nover time.\nThe criteria are combined in a weighted sum to form\nwhat we name here the content-based segmentation cost. A\n2This can be seen as a way to constrain the ill-posed problem of struc-\ntural segmentation towards a well-deï¬ned solution.linear regularity cost function is used to perform the Viterbi\napproach described in section 3.1, to ï¬nd the segmentation\nwith lowest cost. The main structural pulsation period of the\nsong is estimated by a Fourier transform on the instrumental\nchange criterion.\n4.2.2 System 2 : similarity function on symbolic features\nIt is interesting to consider symbolic features for structure\ninference as other means of music description. The joint\nuse of various features in a global and versatile retrieval sys-\ntem may increase the accuracy of the estimated segmenta-\ntion [19]. The symbols can be obtained for instance from a\nscore of the piece. System 2 uses chords estimations to com-\npute the similarity function described with the equations (3)\nand (4) of section 3.2.1. Each chord class is associated to a\ndifferent symbol, to obtain a quite neutral symbolic descrip-\ntion of the song. The size of the alphabet of symbols we use\nis the number of chord classes used by the chord estimator\n(e.g.24 classes for major and minor chords). Each symbol\ncorresponds to a duration of 2 beats, in order to be consistent\nwith the temporal scale used in [2].\nThe structural pulsation period value Ï„is considered as\nprior knowledge and used in the regularity cost Î¨Î±of equa-\ntion (8), section 3.2.2. The content-based cost and the reg-\nularity cost are then combined using equations (1) and (2)\nfrom section 2, and the segmentation with lower cost is\nfound using Viterbi algorithm from section 3.1.\n5. EV ALUATION\n5.1 Evaluation database\nThe algorithms have been evaluated using the RWC popular\nmusic database [6], and the set of reference annotations pro-\nvided by [2], used in MIREX 2010. This database consists\nof 100 songs written and produced for research purposes.\n5.2 Evaluation metrics\nThe evaluation of the segmentation is done by Precision ( P),\nRecall (R) and F-measure ( F) metrics. Let sRbe the set of\nreference boundaries (annotations) and sEthe set of esti-\nmated ones, they are respectively deï¬ned as :\nP=|sEâˆ©sR|\n|sE|;R=|sEâˆ©sR|\n|sR|;F=2PR\n(P+R).(9)\nThe matching of reference and estimated boundaries is\nperformed within particular tolerance windows. We con-\nsider 0.5 s and 3 s as in MIREX 2010. Note that each bound-\nary is used only once during the matching process.\n5.3 Feature extraction and algorithm parametrization\nSystem 1 (which uses change detection criteria) uses 20\nMFCCs (including the 0th coefï¬cient), extracted from\n48612th International Society for Music Information Retrieval Conference (ISMIR 2011)\nTolerance window = 0.5 sec.\nsystemÎ±Î»P(%)R(%)F(%)\n1 - 0 10.1 53.6 17.0\n10.5 23.9 24.3 23.8\nTolerance window = 3 sec.\nsystemÎ±Î»P(%)R(%)F(%)\n1 - 0 16.8 89.3 28.2\n10.5 61.2 63.0 61.4\nTable 1 . Average Precision ( P), Recall (R), and F-measure\n(F) for two versions of System 1 on the RWC pop database.\nframes of length 23.2 ms, and a hop size of 11.6 ms (us-\ning scripts from MA toolbox by Beth Logan and Malcolm\nSlaney3). Chroma vectors (12 coefï¬cients) are extracted\nfrom frames of length 92.9 ms, and a hop size of 23.2 ms.\nChroma vectors and beats estimation are computed thanks\nto LabRosa scripts4.\nSystem 2 (based on a similarity function) inputs the\nchords transcriptions obtained by the algorithm from Ueda\net al. , described in [18], and uses the downbeat annota-\ntions available with the RWC database5. The reference\nannotations show that more than 80% of the songs have\na main structural pulsation of 32 beats. We will then use\nÏ„= 32 beats as prior knowledge for our evaluation, and\nH= 3Ï„as the maximal number of admissible predecessors\nfor each temporal unit.\nA preliminary study on a subset of RWC popular was car-\nried out to identify reasonable values of Î»which fall within\nthe interval [0, 1]. Three values of Î±are chosen to con-\nsider regularity costs functions with different convexities :\na non-convex regularity cost function ( Î±= 0.5), a convex\nregularity cost function ( Î±= 2), and the intermediate case\nÎ±= 1.\n5.4 Results\nThe values gathered in Tables 1 and 2 for System 1 and\n2 show that the overall mean F-measures increase signiï¬-\ncantly when the regularity cost is introduced.\nFigure 3 shows the average F-measure obtained with Sys-\ntem 2 for the 3 regularity costs mentioned in 5.3. The values\nofÎ»corresponding to optimal performance appear in Table\n2 for each case. The value of Î±has an impact on the accu-\nracy of the estimated boundaries : it can be seen that, for a\nsmall tolerance, a non-convex regularity cost function gives\nbetter boundary accuracy than a convex one. This can be\nexplained by the fact previously mentioned, that the convex\ncase (Î±= 2) tends to spread structural irregularities (devi-\n3http://www.ofai.at/elias.pampalk/ma/documentation.html\n4http://labrosa.ee.columbia.edu/projects/coversongs/\n5http://staff.aist.go.jp/m.goto/RWC-MDB/Tolerance window = 0.5 sec.\nsystemÎ±Î»P(%)R(%)F(%)\n2 - 0 17.9 31.9 22.0\n0.5 0.30 37.7 34.8 35.6\n1 0.30 34.7 32.3 33.0\n2 0.95 29.3 26.8 27.5\nTolerance window = 3 sec.\nsystemÎ±Î»P(%)R(%)F(%)\n2 - 0 36.1 64.7 44.5\n0.5 0.15 63.1 63.1 62.0\n1 0.15 63.4 64.1 62.7\n2 0.60 64.5 60.0 61.2\nTable 2 . Average Precision ( P), Recall (R), and F-measure\n(F) for System 2 (optimally tuned, considering Î»âˆˆ[0 : 1] ),\non the database described in 5.1.\nations from the ideal segmentation with segments of length\nÏ„) across several structural segments. On the contrary, the\nnon-convex case ( Î±= 0.5) tends to concentrate them on a\nfew segments. These results therefore show not only the ad-\nvantage of the regularity constraint but also the importance\nof the ï¬ne properties of the corresponding cost function.\nAs a point of comparison, the best system in struc-\ntural segmentation at MIREX 20106(MND1) obtained F-\nmeasures of 35.9% and 60.5% (for tolerance windows of\n0.5 s and 3 s respectively) on the same database. Note\nhowever that System 2 relies on a manual annotation of the\ndownbeats.\n6. CONCLUSION\nThe work presented in this paper has highlighted the rele-\nvance of incorporating a regularity constraint in the task of\nstructural segmentation. Even with very basic cost func-\ntions as the ones considered in the present work, the very\nexistence of the regularity constraint favors the retrieval of\na well-deï¬ned solution. The Viterbi implementation, which\nwe have detailed, allows a fast calculation of the optimal so-\nlution, and it can be applied in a generic way to any type of\ncost function.\nThe corresponding Matlab code will be made available to\nthe MIR community7for enabling further experimental in-\nvestigation within diverse structural segmentation systems\nand possibly for other tasks in MIR where the regularity\nconstraint can be meaningful.\n6http://nema.lis.illinois.edu/nema out/mirex2010/results/struct/mirex10\n/summary.html\n7http://www.irisa.fr/metiss/logiciel/\n487Poster Session 4\nFigure 3 . Evolution of the average F-measures of Sys-\ntem 2 on the database described in 5.1, as a function of\nbalance parameter Î», for 3 types of regularity cost function\n(Î¨Î±={0.5,1,2},Ï„= 32 ).\n7. ACKNOWLEDGEMENTS\nThe authors would like to thank Yushi Ueda and Nobutaka\nOno for their help in the collection of chord transcriptions\nused in this article. This work was partly supported by the\nQuaero project8funded by Oseo and by the associate team\nVERSAMUS9funded by INRIA.\n8. REFERENCES\n[1] L. Barrington, A. B. Chan, G. Lanckriet, â€œModeling music as\na dynamic textureâ€, IEEE Transactions on Audio, Speech, and\nLanguage Processing , V olume 18 Issue 3, March 2010.\n[2] F. Bimbot, O. Le Blouch, G. Sargent and E. Vincent, â€œ De-\ncomposition into autonomous and comparable blocks : a struc-\ntural description of music piecesâ€, Proceedings of the Interna-\ntional Symposium on Music Information Retrieval , pp. 189â€“\n194, 2010.\n[3] M. Cooper and J. Foote, â€œMedia segmentation using self-\nsimilarity decompositionâ€ Proceedings of the SPIE Storage\nand Retrieval for Multimedia Databases , San Jose, California,\nUSA, pp. 167â€“175, January 2003.\n[4] M. Goto, â€A chorus-section detecting method for musical au-\ndio signalsâ€ Proceedings of the IEEE International Conference\non Acoustics, Speech, and Signal Processing , Hong Kong,\nChina, pp. 437â€“440, April 2003.\n[5] M. Goto, AIST Annotation for the RWC Music Database, Pro-\nceedings of the 7th International Conference on Music Infor-\nmation Retrieval (ISMIR 2006) , pp. 359â€“360, October 2006.\n8http://www.quaero.org/\n9http://versamus.inria.fr/[6] M. Goto, H. Hashiguchi, T. Nishimura, and R. Oka, â€œRWC\nMusic Data- base : RWC music database : Popular, Classical,\nand Jazz Music Databasesâ€ Proceedings of the International\nSymposium on Music Information Retrieval , USA, pp. 287â€“\n288, October 2002.\n[7] T. Jehan, â€œHierarchical multi-class self similaritiesâ€ Proceed-\nings of the IEEE Workshop on Applications of Signal Process-\ning to Audio and Acoustics , Mohonk, New York, USA, Octo-\nber 2005.\n[8] K. Jensen, â€œMultiple scale music segmentation using rhythm,\ntimbre and harmonyâ€, EURASIP Journal on Advances in Sig-\nnal Processing , vol. 2007, 2007.\n[9] A. Klapuri, M. Davy (Editors) Signal processing methods for\nmusic transcription , Springer, New York, 2006.\n[10] M. Levy and M. Sandler, â€œNew methods in structural segmen-\ntation of musical audioâ€, Proceedings of European Signal Pro-\ncessing Conference , pp. â€“ September 2006\n[11] B. Logan and S. Chu, â€œMusic summarization using key\nphrasesâ€ Proceedings of the IEEE International Conference\non Acoustics, Speech, and Signal Processing , Istanbul, Turkey,\npp. 749â€“752, June 2000.\n[12] J. Paulus and A. Klapuri, â€œMusic structure analysis by ï¬nd-\ning repeated partsâ€, Proceedings of AMCMM , Santa Barbara,\nCalifornia, USA, pp. 59â€“68, October 2006.\n[13] J. Paulus, M. Muller, A. Klapuri, â€œAudio-based music struc-\nture analysisâ€, Proceedings of the International Symposium on\nMusic Information Retrieval , pp. 625â€“636, 2010.\n[14] G. Peeters, A. La Burthe, and X. Rodet, â€œToward automatic\nmusic audio summary generation from signal analysisâ€ Pro-\nceedings of the International Conference on Music Informa-\ntion Retrieval , Paris, France, pp. 94â€“100, October 2002.\n[15] Y . Shiu, H. Jeong, and C. C. Jay-Kuo, â€œSimilarity matrix pro-\ncessing for music structure analysisâ€ Proceedings of AMCMM ,\nSanta Barbara, California, USA, pp. 69â€“76, October 2006.\n[16] G. Sargent, F. Bimbot and E. Vincent, â€œA structural seg-\nmentation of songs using generalized likelihood ratio under\nregularity assumptions,â€ MIREX evaluation campaign , 2010.\nhttp://hal.inria.fr/inria-00551411/en\n[17] D. Turnbull, â€œA supervised approach for detecting boundaries\nin music using difference features and boostingâ€, Proceed-\nings of the International Symposium on Music Information Re-\ntrieval , pp. 057â€“060, 2007.\n[18] Y . Ueda, Y . Uchiyama, T. Nishimoto, N. Ono and S.\nSagayama, â€œHMM-based Approach for Automatic Chord De-\ntection Using Reï¬ned Acoustic Featuresâ€, Proceedings of the\nInternational Conference on Acoustics, Speech and Signal\nProcessing , pp. 5506â€“5509, March 2010.\n[19] E. Vincent, S. A. Raczynski, N. Ono and S. Sagayama â€œ A\nroadmap towards versatile MIRâ€, Proceedings of the Interna-\ntional Symposium on Music Information Retrieval , pp. 662â€“\n664, 2010.\n[20] R. Weiss, J. Bello, â€œIdentifying Repeated Patterns in Mu-\nsic Using Sparse Convolutive Non-Negative Matrix Factoriza-\ntionâ€, Proceedings of the International Symposium on Music\nInformation Retrieval , pp. 123â€“128, 2010.\n488"
    },
    {
        "title": "Investigating the Similarity Space of Music Artists on the Micro-Blogosphere.",
        "author": [
            "Markus Schedl",
            "Peter Knees",
            "Sebastian BÃ¶ck"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1414808",
        "url": "https://doi.org/10.5281/zenodo.1414808",
        "ee": "https://zenodo.org/records/1414808/files/SchedlKB11.pdf",
        "abstract": "Microblogging services such as Twitter have become an important means to share information. In this paper, we thoroughly analyze their potential for a key challenge in the field of MIR, namely the elaboration of perceptually meaningful similarity measures. To this end, comprehensiveevaluation experiments were conducted using Twitter posts gathered during a period of several months. We investigated 23,100 combinations of different term weighting strategies, normalization methods, index term sets, Twitter query schemes, and similarity measurement techniques, aiming at determining in which way they influence the similarity estimatesâ€™ quality. Evaluation was performed on the task of similar artist retrieval. Two data sets were used: one of 224 well-known artists with a uniform genre distribution, the other constituting a collection of 3,000 artists extracted from last.fm and allmusic.com.",
        "zenodo_id": 1414808,
        "dblp_key": "conf/ismir/SchedlKB11",
        "keywords": [
            "Twitter",
            "microblogging",
            "information sharing",
            "MIR",
            "perceptually meaningful similarity measures",
            "comprehensive evaluation experiments",
            "term weighting strategies",
            "normalization methods",
            "index term sets",
            "Twitter query schemes"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nINVESTIGATINGTHESIMILARITYSPACE OF\nMUSICARTISTS ONTHE MICRO-BLOGOSPHERE\nMarkus Schedl, PeterKnees, Sebastian B Â¨ock\nDepartmentofComputationalPerception\nJohannesKeplerUniversityLinz,Austria\nmarkus.schedl@jku.at, peter.knees@jku.at, sebastian.b oeck@jku.at\nABSTRACT\nMicrobloggingservices such as Twitter have becomean\nimportant means to share information. In this paper, we\nthoroughlyanalyzetheirpotentialforakeychallengeinth e\nï¬eld of MIR, namely the elaborationof perceptuallymean-\ningfulsimilaritymeasures . Tothisend,comprehensiveeval-\nuation experiments were conducted using Twitter posts\ngatheredduringaperiodofseveralmonths. Weinvestigated\n23,100combinationsof different term weightingstrategies ,\nnormalization methods ,index term sets ,Twitter query\nschemes, andsimilarity measurement techniques , aiming at\ndetermininginwhichwaytheyinï¬‚uencethesimilarityesti-\nmatesâ€™quality.\nEvaluationwasperformedonthetaskofsimilarartistre-\ntrieval. Two data sets were used: one of 224well-known\nartistswithauniformgenredistribution,theotherconsti tut-\ning a collection of 3,000artists extracted from last.fm\nandallmusic.com .\n1. MOTIVATION AND CONTEXT\nTerm weighting techniques such as TFÂ·IDFandBM25\nhavebeenusedintenselyforvarioustextretrievaltasks. A l-\nthough a wealth of approaches to model the term vector\nspace [21] on the Web has been proposed throughout the\nlast years, e.g., [6,12,20,30], IR-related research inter est\nin the relatively novel ï¬eld of microblog mining has been\nratherlimitedsofar.\nMicroblogginghasencounteredaremarkablegaininpop-\nularityduringthepast coupleofyears. Beingthemostpop-\nular microblogging service, Twitter has more than 100\nmillion registered users [31]. Millions of Twitter users\npost â€œtweetsâ€ that reveal what they are doing, what is on\ntheirmind,orwhatiscurrentlyimportantforthem. Accord-\ning to [7], the number of tweets per day surpassed 50mil-\nlions in early 2010. Twitter thus represents a rich data\nsourcefortext-basedIEandIR.\nPermission to make digital or hard copies of all or part of thi s work for\npersonal or classroom use is granted without fee provided th at copies are\nnot made or distributed for proï¬t or commercial advantage an d that copies\nbear this notice and the full citation on theï¬rstpage.\nc/circlecopyrt2011 International Society for MusicInformation Retrieva l.The work at hand was inspired by [32], where the au-\nthors thoroughly evaluate various choices related to con-\nstructingtextfeaturevectorsforIRpurposes,e.g.,termf re-\nquency (TF), term weights ( IDF), and normalization ap-\nproaches. They analyze the inï¬‚uence of these decisions on\nretrieval behavior. Similarly, we present a systematic lar ge-\nscale study on the inï¬‚uence of a multitude of decisions on\nmusicartistsimilarityestimation,usingreal-worlddata col-\nlections. To this end, we analyze several thousand com-\nbinations of the following single aspects: term frequency,\ninverse document frequency, normalization with respect to\nlength,similarityfunction,indextermset,andquerysche me.\nElaboratingmusicalsimilarity measuresthat are capable\nofcapturingaspectsrelatingtoperceivedsimilarityison eof\nthe main challenges in MIR. Such measures enable various\nmusic applications, for example, automatic playlist gener -\nators [1], music recommender systems [4], music informa-\ntion systems [23], semantic music search engines [11], and\nintelligentuserinterfaces[17]tomusiccollections.\nSimilaritymeasuresbasedontermproï¬lesextractedfrom\nartistsâ€™WebpageshavebeenstudiedinMIRforalongtime,\ne.g., [3,10,30]. In contrast, microblogs have not been har-\nvested to a large extent so far for this purpose. To the best\nofourknowledge,theonlyworkconsideringmicroblogsfor\nsimilaritymeasurementofmusicartistsis[24]. Theauthor s\nof the aforementionedpublications,however,usually sele ct\none (or a few) variant(s) of the TFÂ·IDFterm weight-\ning measure and apply it to documents retrieved for music\nartists. The individual choices involved in selecting a spe -\nciï¬cTFÂ·IDFvariant and similarity function, however,\ndo not seem to be the result of detailed assessments. In the\nwork at hand, by contrast, we present a thoroughinvestiga-\ntion of several dimensions for modeling the music-related\ntermvectorspaceonthemicro-blogosphere.\n2. MODELING THEMICROBLOG\nTERM VECTOR SPACE\nSimilarly to the large scale experiments presented in [32],\nwe aim at analyzing if speciï¬c combinations of the inves-\ntigated algorithmic choices perform considerably better o r\nworsethanothers,whereperformanceismeasuredinasim-\nilarity classiï¬cation task amongtermvectorrepresentati ons\noftweets, cf.Section3.\nTable 1 contains an overview of the denominationsused in\n323Oral Session 4: Web\nDset of documents\nNnumber of documents\nfd,tnumber of occurrences of term tindocument d\nftnumber of documents containing term t\nFttotalnumber of occurrences of tinthe collection\nTdset of distinctterms indocument d\nfm\ndlargestfd,tof all terms tind\nfmlargestftinthe collection\nrd,tterm frequency (cf.Table 3)\nwtinverse document frequency (cf.Table 4)\nWddocument length of d\nTable 1. Denominations used in term weighting functions\nandsimilaritymeasures.\nthe different term weighting formulations (Tables 3 and 4)\nandsimilaritymeasures(Table5).\n2.1 QueryScheme\nWe decided to assess two schemes to query Twitter as\npreviouswork on Web-MIR [26,30]has shown that adding\nmusic-related key terms to a search request generally im-\nproves the quality of feature vectors in terms of similarity -\nbased classiï¬cation accuracy. In Web-MIR, common terms\nused as additional key words are â€œmusic reviewâ€ or â€œmu-\nsic genre styleâ€. Taking into account the 140-character-\nlimitation of tweets, we decided to include only â€œmusicâ€\nas additional query term ( QSM) or query without any ad-\nditional key terms, i.e., use only the artist name ( QSA) as\nexactphrase.\n2.2 Index Term Set\nEarlier work in text-based music artist modeling [9,16,29]\nshows that a crucial choice in deï¬ning the representation\nof an artist is that of the used index terms. For the work\nat hand, we hence investigatedvariousterm sets, which are\nsummarizedinTable2. Set TSAcontainsalltermsfoundin\nthe corpus(after casefolding,stopping, and stemming). Se t\nTSSis the entire term dictionary of SCOWL[28], which\nis an aggregation of several spell checker dictionaries for\nvarious English languages and dialects. Set TSNencom-\npassesallartistnamespresentinthedataset. Previouswor k\nhas shown that the corresponding co-occurrence approach\nto music artist similarity estimation yields remarkable re -\nsults, cf. [26]. Term set TSDis a manually created dic-\ntionary of music-related terms that resembles the one used\nin [16]. It contains, for example, descriptors of genre, in-\nstruments, geographic locations, epochs, moods, and musi-\ncological terms. Set TSLrepresents the most popular tags\nutilized by users of last.fm . SetTSFcomprisesthe ag-\ngregated data set for the data types musical genre ,musical\ninstrument ,andemotion,extractedfrom Freebase [8].\nTo build the inverted word-level index [33], we use a\nmodiï¬ed version of the open source indexer Lucene [14],\nwhichweextendedtorepresent Twitter posts. Theexten-\nsions will be made available through our CoMIRVA frame-\nwork [5,25]. When creating the indexes for the different\nterm sets, we commonly employ casefolding and stopping,e.g. [2]. Stemming, in contrast, is only performed for the\nterm sets for which it seems reasonable, i.e., for term sets\nTSAandTSS.\n2.3 TF andIDF: Term Weighting\nEven though our experimental setting is guided by Zobel\nand Moffatâ€™s [32], we decided to extend the TFÂ·IDF\nformulationsinvestigatedbythemwith BM25-likeformula-\ntions.BM25is an alternative term weighting scheme, used\nintheOkapiframeworkfortext-basedprobabilisticretrieval\n[19]. The BM25model includes a priori class knowledge.\nSinceincorporatinggenreinformationintothetermweight -\ningfunctionwouldbiastheresultsofthegenreclassiï¬cati on\nexperiments,we includedan adaptedformulationin the ex-\nperiments, cf. variants TFGandIDFJin Tables 3 and 4,\nrespectively.\n2.4 VirtualDocumentsand Normalization\nWhencreatingatermproï¬lefromWebpagesretrievedfora\nnamedentity(amusicartistinourcase),itiscommontoag-\ngregatethepagesassociatedwith aparticularentityto for m\na â€œvirtual documentâ€, e.g. [3,10]. This procedure not only\nfacilitates handling small or empty pages, it is also more\nintuitive since the item of interest is the entity under con-\nsideration, not a Web page. Latest work [27] further shows\nthatcalculatingtermweightsonthelevelofindividualWeb\npages before aggregating the resulting feature vector per-\nforms inferior for the task of similarity calculation than u s-\ning â€œvirtual documentsâ€. It therefore seems reasonable to\naggregate all posts retrieved from Twitter for an artist\ntooneâ€œvirtualpostâ€,inparticular,takingintoconsidera tion\nthealreadystronglimitationof Twitter poststo140char-\nacters.\nSince the different length of two artistâ€™s virtual docu-\nmentsislikelytoinï¬‚uencetheperformanceofretrievaltas ks,\nwe evaluated several normalizationmethods. In addition to\napplying no normalization ( NORMNO), we analyzed sum-\nto-1normalization( NORMSUM)andnormalizingtotherange\n[0,1](NORMMAX).\n2.5 SimilarityFunction\nThesimilarity measuresanalyzedareshowninTable 5. We\nincludedallmeasuresinvestigatedbyZobelandMoffat[32]\nthatcanbeappliedtooursomewhatdifferingusagescenario\nof computingsimilarities between two equally dimensional\nterm feature vectorsthat represent two comparableentitie s.\nWe further included Euclidean similarity ( SIMEUC) and\nJeffrey divergence-based similarity [13] ( SIMJEF) in the\nset ofevaluatedsimilarityfunctions.\n2.6 Notation\nTo facilitate referring to a particular evaluationexperim ent,\nwhich is deï¬ned as a combination of the choices described\nabove,weadoptthe followingscheme:\n<Query Scheme>.<Index Term Set>.<Normalization>.\n<TF>.<IDF>.<Similarity Measure>\n32412th International Society for Music Information Retrieval Conference (ISMIR 2011)\nAbbr. / Term Set Cardinality Description\nTSA - all terms upto1,489,459All terms(stemmed) thatoccur inthe corpus of theretrieved Twitter posts.\nTSS - scowl dict 698,812All termsthat occur inthe entire SCOWLdictionary.\nTSN - artist names 224/3,000Names ofthe artists forwhich data was retrieved.\nTSD - dictionary 1,398Manually created dictionary ofmusically relevant terms.\nTSL - last.fm toptags 250Overall top-ranked tags returnedby last.fm â€™sTags.getTopTags function.\nTSF - freebase 3,628Music-related terms extractedfrom Freebase (genres, instruments, emotions).\nTable 2. Differenttermsetsusedto indexthe Twitter posts.\nAbbr. Description Formulation\nTFAFormulationusedfor binary match\nSB=brd,t=/braceleftbigg\n1 iftâˆˆ Td\n0 otherwise\nTFBStandardformulation\nSB=trd,t=fd,t\nTFCLogarithmicformulation rd,t= 1+logefd,t\nTFC2Alternativelogarithmic formulation suitedfor fd,t<1 rd,t= loge(1+fd,t)\nTFC3Alternativelogarithmic formulation as usedin ltcvariant rd,t= 1+log2fd,t\nTFDNormalizedformulation rd,t=fd,t\nfm\nd\nTFEAlternative normalized formulation. Similar to [32] we use\nK= 0.5.\nSB=nrd,t=K+(1âˆ’K)Â·fd,t\nfm\nd\nTFFOkapi formulation, according to[32]. For Wwe use the vec-\ntorspace formulation, i.e.,the Euclideanlength.rd,t=fd,t\nfd,t+Wd/avdâˆˆD(Wd)\nTFGOkapi BM25 formulation, according to[19]. rd,t=(k1+1)Â·fd,t\nfd,t+k1Â·/bracketleftbigg\n(1âˆ’b)+bÂ·Wd\navdâˆˆD(Wd)/bracketrightbigg\nk1= 1.2,b= 0.75\nTable 3. Evaluatedvariantstocalculatethetermfrequency rd,t.\nAbbr. Description Formulation\nIDFAFormulationused for binarymatch\nSB=xwt= 1\nIDFBLogarithmic formulation\nSB=fwt= loge/parenleftBig\n1+N\nft/parenrightBig\nIDFB2Logarithmic formulation usedin ltcvariant wt= loge/parenleftBig\nN\nft/parenrightBig\nIDFCHyperbolic formulation wt=1\nft\nIDFDNormalized formulation wt= loge/parenleftBig\n1+fm\nft/parenrightBig\nIDFEAnother normalized formulation\nSB=pwt= logeNâˆ’ft\nft\nThe following deï¬nitions are based on the termâ€™s noise nt\nand signal st.nt=/summationtext\ndâˆˆDt/parenleftBig\nâˆ’fd,t\nFtlog2fd,t\nFt/parenrightBig\nst= log2(Ftâˆ’nt)\nIDFFSignal wt=st\nIDFGSignal-to-Noise ratio wt=st\nnt\nIDFH wt=/parenleftbigg\nmaxntâ€²\ntâ€²âˆˆT/parenrightbigg\nâˆ’nt\nIDFIEntropy measure wt= 1âˆ’nt\nlog2N\nIDFJOkapi BM25IDFformulation, according to[18,19] wt= logNâˆ’ft+0.5\nft+0.5\nTable 4. Evaluatedvariantsto calculatethe inversedocumentfreq uencywt.\n325Oral Session 4: Web\nAbbr. Description Formulation\nSIMINNInner Product Sd1,d2=/summationtext\ntâˆˆTd1,d2(wd1,tÂ·wd2,t)\nSIMCOSCosine Measure Sd1,d2=/summationtext\ntâˆˆTd1,d2(wd1,tÂ·wd2,t)\nWd1Â·Wd2\nSIMDICDice Formulation Sd1,d2=2/summationtext\ntâˆˆTd1,d2(wd1,tÂ·wd2,t)\nW2\nd1+W2\nd2\nSIMJACJaccard Formulation Sd1,d2=/summationtext\ntâˆˆTd1,d2(wd1,tÂ·wd2,t)\nW2\nd1+W2\nd2âˆ’/summationtext\ntâˆˆTd1,d2(wd1,tÂ·wd2,t)\nSIMOVLOverlapFormulation Sd1,d2=/summationtext\ntâˆˆTd1,d2(wd1,tÂ·wd2,t)\nmin(W2\nd1,W2\nd2)\nSIMEUCEuclidean Similarity Dd1,d2=/radicalbigg/summationtext\ntâˆˆTd1,d2(wd1,tâˆ’wd2,t)2\nSd1,d2=/parenleftBig\nmaxdâ€²\n1,dâ€²\n2(Ddâ€²\n1,dâ€²\n2)/parenrightBig\nâˆ’Dd1,d2\nSIMJEFJeffreyDivergence-based Similarity Sd1,d2=/parenleftBig\nmaxdâ€²\n1,dâ€²\n2(Ddâ€²\n1,dâ€²\n2)/parenrightBig\nâˆ’Dd1,d2\nD(F,G) =/summationtext\ni/parenleftBig\nfilogfi\nmi+giloggi\nmi/parenrightBig\nmi=fi+gi\n2\nTable 5. Evaluatedsimilarityfunctions Sd1,d2.\n3. EVALUATION\nWe performed genre classiï¬cation experiments to evaluate\nthe different algorithmic choices discussed in the previou s\nsection. Although genre taxonomies are often inconsistent\nand erroneous[15], it is commonplacein MIR to use genre\nas a proxy for artist similarity. The evaluated retrieval ta sk\nconsists of determining kartists similar to a given query\nartist. Thistaskresembles knearestneighborclassiï¬cation,\nwherethe genreofa seed artist ispredictedas the mostfre-\nquentgenreamongthe seedâ€™s kmostsimilarartists.\n3.1 DataSets\nWe usedtwo datasetsforevaluation. Theï¬rst one,referred\nto asC224a, consists of 224well-known artists and has a\nuniform genre distribution ( 14genres1,16artists each). It\nhas been frequently used to evaluate Web-/text-based MIR\napproaches.\nThe second data set C3kaconsists of 3,000music artists,\nrepresentingareal-worldcollection. Thedatahasbeengat h-\nered as follows. We used last.fm â€™s API to extract the\nmost popular artists for each country of the world, which\nwe then aggregated into a single list. Since last.fm â€™s\ndata is proneto misspellingsdueto its collaborativenatur e,\nwe cleaned the data set by matching each artist name with\nthe database of the expert-based music information system\nallmusic.com , from which we also extracted genre in-\nformation. Starting this matching process from the most\npopularartistfoundby last.fm andincludingonlynames\nthatalsooccurin allmusic.com ,weeventuallyobtained\na list of20,995artists, out of which we selected the top\n1The genres in C224aare Country, Folk, Jazz, Blues, Râ€™nâ€™B/Soul,\nHeavy Metal/Hard Rock, Punk, Rap/Hip Hop, Electronica, Reg gae,\nRockâ€™nâ€™Roll, Pop,and Classical.3,000. Theseartistsarecategorizedinto 18distinctgenres2\naccordingto allmusic.com . Bothdatasetsareavailable\nfordownload.3\n3.2 Experiments\nTogathermusic-relatedposts,weuse Twitter â€™sAPI.Ac-\ncountingfor the time-varyingbehaviorof the search result s\nand to obtain a broad coverage, we queried Twitter dur-\ningFebruary/March2010andDecember2010/January2011,\nyielding a total of about six million tweets. For artist set\nC224a,weachievedacoverageof 100%;forset C3ka,we\nachievedacoverageof 96.87%.\nWeemployedatwo-stagedevaluation,similarto[22]: In\nordertoï¬lter inferioralgorithmiccombinations,weï¬rst i n-\nvestigatedeachalgorithmicsetting ondataset C224a.4In\na second set of experiments, we then evaluated the remain-\ning variants on the real-world artist set C3ka. As perfor-\nmancemeasure MeanAveragePrecision (MAP)isused.\nIntheï¬rststageoftheexperiments,onlyvariantsthatfulï¬ ll\nat least oneofthe followingtwo conditionsareretained:\nâ€¢there is a relative MAP difference of 10%or less to\nthetop-rankedvariant\nâ€¢or thet-test does not show a signiï¬cant difference to\nthetop-rankedvariant(at5% signiï¬cancelevel).\nThe top577variantshavea relativeMAP differenceof less\nthan 10% to the highest ranked combination. The pairwise\nt-testshowsasigniï¬cantdifferenceforthetop-ranked 1,809\nvariants. Forthesecondstageofexperimentation,conduct ed\n2The genres in C3kaare Avantgarde, Blues, Celtic, Classical, Coun-\ntry, Easy Listening, Electronica, Folk, Gospel, Jazz, Lati n, Newage, Rap,\nReggae, RnB, Rock, Vocal, and World.\n3http://www.cp.jku.at/people/schedl/datasets.html\n4Excluding redundant combinations, a total of 23,100single experi-\nments havebeen conducted in this stage.\n32612th International Society for Music Information Retrieval Conference (ISMIR 2011)\non collection C3ka, we therefore evaluated only these top-\nranked1,809variants.\n3.3 ResultsandDiscussion\nTable 6 shows the 10top-rankedand the 10bottom-ranked\nvariantswiththeirMAPscores(considering 15nearestneigh-\nbors) for set C224a. The MAP scoresof the 23,100evalu-\natedvariantsspanawiderangeandarequitediverse,witha\nmean ofÂµ= 37.89and a standard deviationof Ïƒ= 17.16.\nFrom Table 6 it can be seen that highest MAP scores are\nachieved when using QSA,TSA, andNORMNO. At the\nother end of the ranking we see that QSMandSIMOVL\ndominatethemostinferiorvariants.\nToobtainabetterunderstandingoftheindividualcompo-\nnents that contribute to a well-performing social similari ty\nmeasure,weanalyzedthedistributionofeachaspectamong\nthe1,809top-rankedvariants:\nRegarding the query scheme, using only the artist name\nasindicatortodeterminerelatedtweets( QSA)outperforms\nadding music-speciï¬c key words. It seems that additional\nkeywordstooheftilyprune Twitter â€™sresultset.\nAs for the term sets used for indexing, the top ranks are\ndominated by algorithmic variants that use the whole set\nof terms ( TSA). It is noteworthy, however, that the good\nperformanceof TSAandTSScomesat the price of much\nhighercomputationalcomplexity(cf.Table2). Hence,when\nperformanceis crucial, the results suggest using other ter m\nsets. A particularly good choice seems to be TSN, the list\nof artist names, as it is the set that most frequently occurs\namong the top-ranked variants ( 32.5%). Another interest-\ning ï¬nding is that the music dictionary TSD, despite its\ngood performancefor artist clustering based on Web pages ,\ncf.[16],occursï¬rst onlyat rank 1,112. Anempiricallyver-\niï¬ed reason for this may be that Twitter users tend to\nrefrain from using a comprehensivemusic-speciï¬c vocabu-\nlary,evenwhentheytwit aboutmusic-relatedissues.5\nAs for the term weighting functions ( TFandIDFvari-\nants), no clear picture regardingfavorable variants emerg es\nfrom the experiments. We found, however, that TFAonly\noccurs in 3.15%of the top-rankedvariants and should thus\nbe avoided. The most frequentlyoccurringformulationson\ntheotherhandare TFC2(15.69%)andTFE(16.80%),the\nlatterbeingparticularlypresentintheverytopranks. Ana lo-\ngoustoTF,forIDFvariantswecaneasilypointtoformu-\nlations that should be avoided, namely IDFG(0.50%oc-\ncurrence), IDFF(0.66%), andIDFA(2.54%). TheIDF\nvariants most frequently occurring within the top ranks are\nIDFB2(13.93%),IDFJ(13.71%),andIDFE(13.38%).\nAs for the similarity measure, we found no clear evidence\nthatcosinesimilarity( SIMCOS),thede-factostandardmea-\nsure inIR, generallyoutperformsthe others. Itis likelyth at\nthe key advantage of SIMCOS, the document length nor-\nmalization,playsaminorrole,becausetweetsarelimitedt o\n140characterswhichareusuallyexhausted. Furthersupport\nfor this hypothesisis given by the remarkably good perfor-\nmanceofthesimpleinnerproductmeasure( SIMINN)that\n5Only478unique terms out of the 1,398inTSDwere used, only 319\nwere used in atleast two different tweets.MAP Variant\n64.018 QSA.TSA.NORM NO.TF C2.IDF E.SIM JAC\n63.929 QSA.TSA.NORM NO.TF C2.IDF J.SIM JAC\n63.839 QSA.TSA.NORM NO.TF C.IDF E.SIM JAC\n63.810 QSA.TSA.NORM NO.TF C2.IDF E.SIM COS\n63.780 QSA.TSA.NORM NO.TF C.IDF E.SIM COS\n63.780 QSA.TSA.NORM NO.TF C2.IDF B2.SIM JAC\n63.780 QSA.TSA.NORM NO.TF C2.IDF B2.SIM DIC\n63.720 QSA.TSA.NORM NO.TF C2.IDF E.SIM DIC\n63.601 QSA.TSA.NORM NO.TF C2.IDF J.SIM COS\n63.542 QSA.TSA.NORM NO.TF C.IDF J.SIM JAC\nÂ·Â·Â·Â·Â·Â·\n3.482 QSM.TSA.NORM MAX.TF G.IDF G.SIM OVL\n3.452 QSM.TSS.NORM SUM.TF B.IDF F.SIM OVL\n3.423 QSM.TSA.NORM SUM.TF C3.IDF J.SIM OVL\n3.363 QSM.TSS.NORM MAX.TF G.IDF F.SIM OVL\n3.274 QSM.TSA.NORM SUM.TF C.IDF E.SIM OVL\n3.065 QSM.TSA.NORM SUM.TF C.IDF J.SIM OVL\n3.006 QSM.TSA.NORM MAX.TF G.IDF F.SIM OVL\n2.976 QSM.TSS.NORM MAX.TF F.IDF F.SIM OVL\n2.857 QSM.TSA.NORM MAX.TF F.IDF G.SIM OVL\n2.649 QSM.TSA.NORM MAX.TF F.IDF F.SIM OVL\nTable 6. MAP scores of the top-rankedand bottom-ranked\nvariantsonset C224a.\nMAP Variant\n72.570 QSA.TSS.NORM NO.TF G.IDF H.SIM JAC\n72.566 QSA.TSS.NORM NO.TF G.IDF H.SIM DIC\n72.553 QSA.TSS.NORM NO.TF C.IDF E.SIM COS\n72.553 QSA.TSS.NORM NO.TF C.IDF J.SIM COS\n72.536 QSA.TSS.NORM NO.TF F.IDF H.SIM DIC\nTable 7. MAP scoresofthetop5variantsonset C3ka.\ndoesnotperformanylengthnormalization. Alsoamongthe\nvirtual document normalization methods, using no normal-\nizationatall( NORMNO)outperformedtheothervariantsin-\nvestigated,accountingfor 52.24%ofthe topranks.\nOn the second data set, C3ka, the achieved results were\ncomparable. Spearmanâ€™s rank-order correlation coefï¬cien t\ncomputed on the two rankings obtained with the two artist\nsets revealeda moderate correlationof 0.37. This indicates\nthat the rankingsproducedby the same algorithmicchoices\nare not largely inï¬‚uenced by factors such as size of artist\ncollection or number of artists per genre. Table 7 contains\ntheï¬vetop-rankedvariantsforset C3ka.\n4. CONCLUSIONSAND OUTLOOK\nWe presented a large-scale evaluation of using Twitter\nposts for the purpose of artist similarity estimation. To th is\nend,weanalyzed 23,100algorithmicchoicesrelatedtoquery\nscheme, index term set, length normalization, term weight-\ning function, and similarity measure, using two data sets of\nmusic artists. The main ï¬ndingscan be summarized as fol-\nlows:\nâ€¢Restrictingthesearchbyadditionalkeywordsprunes\ntheresultingset of tweetstooheavily. Usingonlythe\nartist nameasquery( QSA)shouldbe favored.\nâ€¢Bestresultsareachievedusingalltermsinthecorpus\n(TSA), though at high computational costs. When\ncomputationalcomplexityisanissue,theresultssug-\ngestusingartist namesasindextermset ( TSN).\n327Oral Session 4: Web\nâ€¢Normalizingforlengthdoesnotsigniï¬cantlyimprove\ntheresults,neitherontermvectors,norinthesimilar-\nityfunction. Takingintoaccountthehighercomputa-\ntionalcosts,wethereforerecommendrefrainingfrom\nnormalization( NORMNO)andusing,forexample,the\ninnerproductassimilarity measure( SIMINN).\nâ€¢Thesimplebinarymatch TFformulation TFAshould\nnot be used. The most favorable variants are TFC2\nandin particular TFE.\nâ€¢Among the IDFformulations, we suggest to refrain\nfrom using IDFA,IDFF, andIDFG. Better alter-\nnatives are given by formulations IDFB2,IDFE,\nandIDFJ.\nFuture work will focuson investigatingthe performance\nof different approaches on the â€œlong tailâ€ of artists and on\nincorporatingtemporalandgeographicpropertiesoftweet s.\nThe contextual similarity measures analyzed in this work\nwillhelpdevelopmoreaccuratesocialandpersonalizedmod -\nelsofmusicalsimilarity. Combinedwithcontent-basedmod -\nels,theymightpavethewayforanewgenerationofperson-\nalizedmusicapplications,suchasintelligentrecommende rs\norplaylistgenerators.\n5. ACKNOWLEDGMENTS\nThis research is supported by the Austrian Science Funds\n(FWF): P22856-N23 and L511-N15. We further wish to\nthankTim Pohle forhiscontributions.\n6. REFERENCES\n[1] J.-J. Aucouturier and F. Pachet. Scaling Up Music\nPlaylist Generation.In Proc.IEEEICME ,Aug2002.\n[2] R. Baeza-Yates and B. Ribeiro-Neto. Modern Informa-\ntionRetrieval .AddisonWesley, 1999.\n[3] S. Baumann and O. Hummel. Using Cultural Metadata\nfor Artist Recommendation. In Proc. WEDELMUSIC ,\nSep2003.\n[4]`O. Celma. Music Recommendation and Discovery in\nthe Long Tail . PhD thesis, Universitat Pompeu Fabra,\nBarcelona,Spain,2008.\n[5] http://www.cp.jku.at/CoMIRVA(access:Jan 2011).\n[6] F.DeboleandF.Sebastiani.SupervisedTermWeighting\nforAutomatedText Categorization.In Proc.ACM SAC ,\nMar2003.\n[7] M. Evans. Twitter Enjoys Major Growth and Excellent\nStickiness. http://blog.sysomos.com(access: Jan2011).\n[8] http://www.freebase.com(access: Jan2011).\n[9] X.HuandJ.S. Downie.ExploringMoodMetadata: Re-\nlationships with Genre, Artist and Usage Metadata. In\nProc.ISMIR ,Sep2007.\n[10] P.Knees,E.Pampalk,andG.Widmer.Artist Classiï¬ca-\ntionwithWeb-basedData. In Proc.ISMIR ,Oct 2004.\n[11] P.Knees,T.Pohle,M.Schedl,andG.Widmer.AMusic\nSearch Engine Built upon Audio-based and Web-based\nSimilarityMeasures.In Proc.ACM SIGIR ,Jul2007.[12] M. Lan, C.-L. Tan, H.-B. Low, and S.-Y. Sung. A\nComprehensiveComparative Study on Term Weighting\nSchemes for Text Categorization with Support Vector\nMachines.In Proc.ACM WWW , May2005.\n[13] J. Lin. DivergenceMeasuresBased onthe ShannonEn-\ntropy.IEEETrans.InformationTheory ,37,1991.\n[14] http://lucene.apache.org(access:Jan 2011).\n[15] F.PachetandD.Cazaly.ATaxonomyofMusicalGenre.\nInProc.RIAO ,Apr2000.\n[16] E.Pampalk,A.Flexer,andG.Widmer.HierarchicalOr-\nganization and Description of Music Collections at the\nArtist Level.In Proc.ECDL , Sep2005.\n[17] E. Pampalk and M. Goto. MusicSun: A New Approach\ntoArtist Recommendation.In Proc.ISMIR ,Sep 2007.\n[18] J. PÂ´ erez-Iglesias, J. R. PÂ´ erez-AgÂ¨ uera, V. Fresno Y.\nZ., and Feinstein. Integrating the Probabilistic Models\nBM25/BM25FintoLucene. CoRR,2009.\n[19] S.E. Robertson, S. Walker, and M. Beaulieu. Okapi at\nTREC-7: Automatic Ad Hoc, Filtering, VLC and Inter-\nactiveTrack. In Proc. TREC ,1999.\n[20] G. Salton and C. Buckley. Term-weighting Approaches\nin Automatic Text Retrieval. Information Processing &\nManagement ,24(5),1988.\n[21] G. Salton, A. Wong, and C. S. Yang. A Vector Space\nModel for Automatic Indexing. Communications of the\nACM,18(11),1975.\n[22] M. Sanderson and J. Zobel. Information Retrieval Sys-\ntem Evaluation: Effort, Sensitivity, and Reliability. In\nProc.ACMSIGIR ,Aug2005.\n[23] M.Schedl. AutomaticallyExtracting,Analyzing,andVi-\nsualizing Information on Music Artists from the World\nWide Web . PhDthesis, JKU Linz,Austria,2008.\n[24] M.Schedl.OntheUseofMicrobloggingPostsforSim-\nilarity Estimation and Artist Labeling. In Proc. ISMIR ,\nAug2010.\n[25] M. Schedl, P. Knees, K. Seyerlehner, and T. Pohle. The\nCoMIRVA Toolkit for Visualizing Music-Related Data.\nInProc. of the 9th Eurographics/IEEE VGTC Sympo-\nsiumonVisualization(EuroVis2007) ,May2007.\n[26] M. Schedl, P. Knees, and G. Widmer. A Web-Based\nApproach to Assessing Artist Similarity using Co-\nOccurrences.In Proc.CBMI ,Jun 2005.\n[27] M. Schedl, T. Pohle, P. Knees, and G. Widmer. Explor-\ningtheMusicSimilaritySpaceontheWeb. ACMTrans-\nactionsonInformationSystems ,29(3),July2011.\n[28] http://wordlist.sourceforge.net(access:Jan 2011) .\n[29] D.Turnbull,L.Barrington,D. Torres,andG.Lanckriet .\nTowardsMusical Query-by-SemanticDescriptionusing\ntheCAL500Data Set. In Proc.ACM SIGIR ,Jul 2007.\n[30] B. Whitman and S. Lawrence. Inferring Descriptions\nandSimilarityforMusicfromCommunityMetadata.In\nProc.ICMC , Sep2002.\n[31] J. Yarow. Twitter Finally Reveals All Its Secret Stats.\nhttp://www.businessinsider.com(access: Jan2011).\n[32] J. Zobeland A. Moffat. Exploringthe Similarity Space.\nACMSIGIRForum ,32(1),1998.\n[33] J. Zobel and A. Moffat. Inverted Files for Text Search\nEngines.ACMComputingSurveys ,38,2006.\n328"
    },
    {
        "title": "Modeling Musical Emotion Dynamics with Conditional Random Fields.",
        "author": [
            "Erik M. Schmidt",
            "Youngmoo E. Kim"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1416606",
        "url": "https://doi.org/10.5281/zenodo.1416606",
        "ee": "https://zenodo.org/records/1416606/files/SchmidtK11.pdf",
        "abstract": "Human emotion responses to music are dynamic processes that evolve naturally over time in synchrony with the music. It is because of this dynamic nature that systems which seek to predict emotion in music must necessarily analyze such processes on short-time intervals, modeling not just the relationships between acoustic data and emotion parameters, but how those relationships evolve over time. In this work we seek to model such relationships using a conditional random field (CRF), a powerful graphical model which is trained to predict the conditional probability p(y|x) for a sequence of labels y given a sequence of features x. Treating our features as deterministic, we retain the rich local subtleties present in the data, which is especially applicable to contentbased audio analysis, given the abundance of data in these problems. We train our graphical model on the emotional responses of individual annotators in an 11Ã—11 quantized representation of the arousal-valence (A-V) space. Our model is fully connected, and can produce estimates of the conditional probability for each A-V bin, allowing us to easily model complex emotion-space distributions (e.g. multimodal) as an A-V heatmap.",
        "zenodo_id": 1416606,
        "dblp_key": "conf/ismir/SchmidtK11",
        "keywords": [
            "Dynamic processes",
            "Music emotion responses",
            "Synchrony with music",
            "Short-time intervals",
            "Conditional random field (CRF)",
            "Conditional probability",
            "Arousal-valence (A-V) space",
            "Annotators",
            "Emotion-space distributions",
            "Multimodal"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nMODELING MUSICAL EMOTION DYNAMICS WITH\nCONDITIONAL RANDOM FIELDS\nErik M. Schmidt and Youngmoo E. Kim\nMusic and Entertainment Technology Laboratory (MET-lab)\nElectrical and Computer Engineering, Drexel University\n{eschmidt,ykim }@drexel.edu\nABSTRACT\nHuman emotion responses to music are dynamic processes\nthat evolve naturally over time in synchrony with the music.\nIt is because of this dynamic nature that systems which seek\nto predict emotion in music must necessarily analyze such\nprocesses on short-time intervals, modeling not just the rela-\ntionships between acoustic data and emotion parameters, but\nhow those relationships evolve over time . In this work we\nseek to model such relationships using a conditional random\nï¬eld (CRF), a powerful graphical model which is trained\nto predict the conditional probability p(y|x)for a sequence\nof labels ygiven a sequence of features x. Treating our\nfeatures as deterministic, we retain the rich local subtleties\npresent in the data, which is especially applicable to content-\nbased audio analysis, given the abundance of data in these\nproblems. We train our graphical model on the emotional re-\nsponses of individual annotators in an 11Ã—11quantized rep-\nresentation of the arousal-valence (A-V) space. Our model\nis fully connected, and can produce estimates of the con-\nditional probability for each A-V bin, allowing us to eas-\nily model complex emotion-space distributions (e.g. multi-\nmodal) as an A-V heatmap.\n1. INTRODUCTION\nThe development of content-based systems for the predic-\ntion of emotion (mood) in music continues to be a topic of\nincreasing attention in the Music-IR community, but thus\nfar most approaches apply only a singular rating to a song\nor clip [1]. Such generalizations belie the time-varying na-\nture of music and make emotion-based recommendation dif-\nï¬cult, as it is very common for emotion to vary temporally\nthroughout a song. In this work, we investigate the applica-\ntion of conditional random ï¬elds (CRFs) to the modeling of\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proï¬t or commercial advantage and that copies\nbear this notice and the full citation on the ï¬rst page.\nc/circlecopyrt2011 International Society for Music Information Retrieval.time-varying musical emotion. CRFs are powerful graphi-\ncal models which are trained to predict the conditional prob-\nabilityp(y|x)for a sequence of labels ygiven a sequence\nof features x. Treating our features as deterministic, we re-\ntain the rich local subtleties present in the data, which is\nespecially promising in content-based audio analysis where\nthere is no shortage of rich data. Furthermore, the system\nprovides a model of both the relationships between acous-\ntic data and emotion space parameters and also how those\nrelationships evolve over time.\nHuman judgements are necessary for deriving emotion\nlabels and associations, but perceptions of the emotional\ncontent of a given song or musical excerpt are bound to vary\nand reï¬‚ect some degree of disagreement between listeners.\nFollowing from our previous work, we model human emo-\ntion responses to music in the arousal-valence (A-V) repre-\nsentation of emotion [2â€“4], where valence indicates positive\nvs. negative emotions and arousal reï¬‚ects emotional inten-\nsity [5]. In our prior approaches, we modeled our emotion\nspace distribution as a single two-dimensional Gaussian dis-\ntribution, and trained multivariate regression systems to pre-\ndict the parameters of the distribution directly from acoustic\nfeatures [3, 4]. Using that representation, we found model-\ning the dynamics of the continuous parameter space to be a\nvery challenging problem. We considered a Kalman ï¬lter-\ning approach, but while this technique provided smooth es-\ntimates over time, the limited model complexity was unable\nto cover a wide variance in emotion space dynamics [4].\nIn applying CRFs to the problem of predicting emotion\nin music, instead of modeling the ambiguity of emotion a-\npriori and representing the distribution of our emotion space\nparameters as the ground truth, we present the training algo-\nrithm with the individual user label sequences, thus allow-\ning the model to learn the range of emotion responses to a\ngiven piece. In our application of the CRF we must also as-\nsign emotion space meanings to the states of the model, and\nin doing so we discretize each label in our sequences to an\n11Ã—11grid. While this is a signiï¬cant simpliï¬cation, our\nï¬ndings indicate that it provides sufï¬cient granularity. Fur-\nthermore, our trained models are fully connected, and can\n777Oral Session 9: Emotion and Mood\nbe used to model complex distributions in emotion as an A-\nV heatmap. These heatmaps can model arbitrary modes and\ndistributions, in contrast to our previous approach, which\nconstructed uni-modal Gaussian A-V predictions.\n2. BACKGROUND\nThe general approach to implementing automatic mood de-\ntection from audio has been to use supervised machine\nlearning to train statistical models based on acoustic fea-\ntures [1]. Chan et al. recently investigated modeling emo-\ntion as a distribution [6]. Their approach investigated mod-\neling the ground truth as a Gaussian distribution as well as a\nheatmap and used support vector regression for the distribu-\ntion prediction. However, their corpus was limited to only\n60 songs, and the work only focused on applying a singular\nrating to an entire clip.\nConditional random ï¬elds have only just begun to gain\nattention as a tool for content-based audio prediction. Re-\ncently, Joder et al. successfully applied them to the task\nof audio-to-score matching, detecting more than 95% of the\nnote onset locations to within 100 ms [7].\n3. GROUND TRUTH DATA COLLECTION\nIn prior work, we developed an online collaborative annota-\ntion activity based on the two-dimensional A-V model [8].\nIn the activity, participants used a graphical interface to in-\ndicate a dynamic position within the A-V space to anno-\ntate 30-second music clips. Each subject provided a check\nagainst the other, reducing the probability of nonsense la-\nbels. The song clips used were drawn from the â€œuspop2002â€\ndatabase.1Using initial game data, we constructed a cor-\npus of 240 15-second music clips, which were selected to\napproximate an even distribution across the four primary\nquadrants of the A-V space.\nIn more recent work we have developed a Mechanical\nTurk (MTurk) activity to collect annotations on the same\ndataset [9]. The purpose of the MTurk activity was to pro-\nvide a dataset collected through more traditional means to\nassess the effectiveness of the game to determine any biases\ninduced though collaborative labeling. Overall, the datasets\nwere shown to be highly correlated, with arousal r=0.712,\nand a valence r=0.846. This new dataset has been made\navailable to the research community,2and is well anno-\ntated, containing 16.93Â±2.690ratings per song and 4,064\nlabel sequences. In this work we demonstrate the applica-\ntion of this densely annotated corpus for training our condi-\ntional random ï¬elds.\n1http://labrosa.ee.columbia.edu/projects/musicsim/uspop2002.html\n2http://music.ece.drexel.edu/research/emotion/moodswingsturk3.1 Statistical Analysis\nIn applying relational learning methods to data, we gain the\nability to model statistical dependencies from one observa-\ntion to the next. To verify that our data collection exhibits\nsuch dependencies, we compute the correlation coefï¬cients\nof our label sequences from one frame to the next and from\nthe ï¬rst frame of each sequence to the last. In these cases,\nwe treat the individual discretized user labels as variables,\nand each second as observations of those variables. Statis-\ntics of the squared correlation coefï¬cients ( r2) are provided\nfor the full dataset in Table 1.\nDimension r2Frame-Frame r2First-Last Frame\nArousal 0.944Â±0.093 0 .507Â±0.242\nValence 0.951Â±0.097 0 .524Â±0.235\nTable 1 . Statistics of ground truth squared correlation co-\nefï¬cient (r2) from one second to the next and from the ï¬rst\nsecond to the last.\nOverall, the dataset shows high correlation from one\nframe to the next, and lower correlation between the ï¬rst\nframe and last frame. In other words, the current emotion\nis highly dependent upon the emotion of the prior second,\nand on average each sequence exhibits a signiï¬cant change\nin emotion from beginning to end. As a result, the dataset is\na good match for graphical modeling techniques.\n4. ACOUSTIC FEATURE COLLECTION\nIn previous work we have found there to be no single domi-\nnant feature, but rather many that play a role (e.g., loudness,\ntimbre, harmony) in determining the emotional content of\nmusic [2,3]. Since our experiments focus on the tracking of\nemotion over time, we chose to focus solely on time-varying\nfeatures. Our collection (Table 2) consists of the two high-\nest performing features in prior work, Spectral Contrast and\nMFCCs [2, 3], as well as the Echo Nest Timbre (ENT) fea-\ntures.\nFeature Description\nSpectral Contrast\n[10]Rough representation of the harmonic\ncontent in the frequency domain.\nMel-frequency\ncepstral coefï¬cients\n(MFCCs) [11]Low-dimensional representation of\nthe spectrum warped according to the\nmel-scale. 20 dimensions used.\nEcho Nest Timbre\nfeatures (ENTs)3Proprietary 12-dimensional beat-\nsynchronous timbre feature\nTable 2 . Acoustic feature collection for music emotion pre-\ndiction.\n3http://developer.echonest.com\n77812th International Society for Music Information Retrieval Conference (ISMIR 2011)\nENTs have been receiving signiï¬cant attention lately due\nto the release of the million song dataset,4and we therefore\ninvestigate their utility in musical emotion prediction.\n5. CONDITIONAL RANDOM FIELDS\nIn this section we give a brief overview of conditional ran-\ndom ï¬elds (CRFs), mainly focused on practical considera-\ntions in implementation. The interested reader is directed\nto [12, 13] for further details.\n5.1 Overview\nTraditional approaches for graphical modeling (e.g. hid-\nden Markov models) seek to represent the joint probability\np(x,y)between sets of features xand labels y. But in forc-\ning our features into a generative model p(x)we discard\nthe rich local subtleties present in the data. Furthermore, in\ndeveloping models for audio classiï¬cation tasks, our acous-\ntic features are naturally deterministic. With CRFs, as with\nlogistic regression, we seek to model the conditional proba-\nbilityp(y|x).\nCRFs are trained on sequences, and in the process of\nlearning them we present the classiï¬cation system with the\nindividual user ratings (as opposed to statistics of all users)\nrecorded in the MTurk task. Using a fully connected model,\nwe are able to learn a set of transition probabilities from\neach class to all others. This means that at each stage in a\ntesting sequence we can display the transition probabilities\nin the form of a heatmap as shown in Figure 1.\nArousalArousalValence\nFigure 1 . Heatmap visualization of CRF transition proba-\nbilities. Actual discretization is 11Ã—11.\n5.2 Feature Functions\nCRFs require the speciï¬cation of feature functions, which\nare used to specify the degree of compatibility between the\nfeatures xand labels y. These functions are deï¬ned over\nall examples, and for a single example are non-zero only\nfor the labeled class. We train our CRFs using CRF++,5\na highly efï¬cient general purpose CRF toolkit written in\n4http://labrosa.ee.columbia.edu/millionsong/\n5http://crfpp.sourceforge.net/C++. CRF++ allows the deï¬nition of both unigram and\nbigram features, where unigram features are related to the\nprediction of a single observation in a sequence (ï¬rst order\nMarkov) and bigram features are related to the prediction\nof pairs of observations (second order Markov). Unigram\nfeatures generate a total of L Ã—N distinct features, where\nL is the number of output classes and N is the number of\nunique features. Bigram features generate L Ã—LÃ—N distinct\nfeatures.\n6. EXPERIMENTS AND RESULTS\nIn the following experiments, we investigate the use of con-\nditional random ï¬elds for the prediction of musical emo-\ntion. As a baseline for comparing performance of the CRF\nin modeling the time-dependencies of our data, we addi-\ntionally provide the performance for the CRF when trained\non independent observations as opposed to sequences. Fur-\nthermore, to provide a baseline for comparison to our prior\nwork [3, 4], we provide the prediction accuracy of multiple\nlinear regression (MLR). To compute the heatmap represen-\ntations for MLR, we ï¬rst predict the mean and covariance\nof an emotion-space Gaussian density using multivariate re-\ngression, and then integrate the probability density function\nunder each square of our heatmap.\nIn all experiments, to avoid the well-known â€œalbum-\neffect,â€ we ensure that any songs which were recorded on\nthe same album are either placed entirely in the training\nor testing set. Additionally, each experiment is subject to\n5 cross-validations, varying the distribution of training and\ntesting data sets which are split 70%/30%, respectively.\n6.1 Acoustic Feature Representation\nAll features are initially computed using short-time analysis\nwindows at a much higher rate than our 1-second emotion\nlabel windows. In order to reduce their frame rate to that\nof the labels, spectral contrast and MFCCs are simply re-\nwindowed via averaging from their original analysis rate ( âˆ¼\n23msec). The ENTs are re-windowed following their non-\nlinear analysis frame start times to take into account their\nbeat-synchronous nature.\nAdditionally, conditional random ï¬elds are highly op-\ntimized to operate on binary features, and given the high\ndimensionality of our data, we found it necessary to con-\nvert our features to such a representation. In doing so, each\nfeature dimension is quantized using 10 equal energy bins,\nwhich for the 14-dimensional case of spectral contrast yields\n140 binary features. In early experiments, we investigated\nthe use of higher discretization levels as well as combining\nrepresentations from multiple discretization levels (e.g. 5,\n10, 20), but overall found 10 levels to offer the best perfor-\nmance.\n779Oral Session 9: Emotion and Mood\n6.2 Training Sequence Label Jittering\nIn discretizing our original label sequences to the 11Ã—11\ngrid representation, our CRF models are trained on vector-\nized version of that space by assigning 121 classes. As a\nresult, the neighbor-relationship of the heatmap grid-cells is\nlost in the vector-wise representation, and we therefore in-\nvestigate how to improve the models ability to learn such\nrelationships.\nIn order to ensure that the CRF learns the spatial rela-\ntionships of each class, we train it on additional â€œjitteredâ€\nversions of each label sequence. This has two beneï¬ts: it\nincreases the overall size of our dataset, and it helps the\nmodel to learn the spatial relationships between the differ-\nent classes. In applying our jitter we increase the size of our\ndataset by a factor of 10, creating 9 additional sequences for\neach sequence in our dataset. Each jittered sequence is cre-\nated by adding a small amount of zero mean Gaussian noise,\nbiasing the whole sequence by a single point. In initial ex-\nperiments we modiï¬ed the number of jittered sequences at\nmultiple levels between 0 and 50, but found 10 to offer the\nbest performance.\n6.3 CRF Parameterization\nAs previously discussed, the training of CRFs requires the\nselection of feature functions. In our experiments, we elect\nto use three different types of features: a simple unigram\nnode feature for each acoustic feature dimension, a unigram\nedge feature that models the change in each feature dimen-\nsion between nodes, and a simple bigram (second order) fea-\nture that models the joint probability of the next two states\nfor arbitrary input. The total number of binary CRF features\nfor a selected training set is described in Table 3.\nAdditionally, in the case of the CRF trained on indepen-\ndent observations, we remove all but node features, so as\nto avoid an artiï¬cial decrease in performance. When pre-\nsenting the training algorithm with independent examples\ninstead of sequences, feature functions that encode time de-\npendencies that cannot be modeled lead to large decreases\nin performance.\nThe training of graphical models such as CRFs tends to\nhave a very high computational cost. We ran our experi-\nments on Amazonâ€™s Elastic Compute Cloud (EC2)6using\nHigh-CPU Extra Large Instances (c1.xlarge) which provide\naccess to a 64-bit platform with 8 virtual cores. Shown in\nTable 3 is the computation time for each feature domain the\nCRF was trained on as well as the number of binary features\ncreated using the speciï¬ed feature functions.\n6http://aws.amazon.com/ec2/Feature # CRF Features Compute Time (hrs)\nContrast 210,782 11 .49Â±1.245\nMFCC 300,927 11 .81Â±1.515\nENT 185,009 12 .04Â±0.461\nTable 3 . Computing time analysis for CRF training on each\ncross-validation set.\n6.4 Evaluating CRF Performance\nWe begin our analysis by attempting to predict a singular\nA-V point at each second in our sequences. These predic-\ntions are taken as the means of the CRF heatmaps, which we\ncompare to the means of the MLR Gaussian distributions. In\nthe second stage of analysis we investigate the accuracy of\nthe CRF heatmaps, which we compare to MLR Gaussian\nheatmaps.\n6.4.1 A-V Mean Prediction\nWe compute the heatmap mean as the sum of the weighted\nA-V coordinate values of each bin center. For each two-\ndimensional heatmap we compute,\nÂµa=/summationdisplay\nya,yvP(ya,yv|x)ya,\nÂµv=/summationdisplay\nya,yvP(ya,yv|x)yv. (1)\nwhereyaandyvare the arousal and valence coordinates of\neach bin center. The mean values for the ground truth distri-\nbution are computed directly in the continuous A-V space.\nThese results are available in the third column of Table 4.\nOverall we see the best performance (minimum mean /lscript2er-\nror) of 0.122using the CRF with MFCCs, which is signif-\nicantly improved over the best result with MLR, which is\nspectral contrast at 0.140.\n6.4.2 Heatmap Prediction Evaluation\nAs previously stated, because the CRF is a fully connected\nmodel, we can use the transition probabilities to construct an\nA-V heatmap. But the ground truth heatmap must be esti-\nmated empirically as a two dimensional histogram, which is\na difï¬cult task. In traditional generative estimation the goal\nis to ï¬t a probabilistic model to data, and derive a smooth\nfunction, even with a small dataset. But with histograms, a\nsmall amount of data can lead to sparse, blocky estimates,\nand a massive amount of data is needed to achieve the true\nsmooth distribution.\nAs a result of this we have chosen the earth moverâ€™s dis-\ntance (EMD) [14] to be our primary metric for comparing\nthese histograms, which can be thought of as the minimum\ncost of transforming one heatmap into the other. Using this\nmetric we can take into account the weight of adjacent bins,\nwhich overall provides a more accurate comparison of the\n78012th International Society for Music Information Retrieval Conference (ISMIR 2011)\nAcoustic Prediction A-V Mean Heatmap Earth Heatmap Error Heatmap Error Heatmap Error\nFeature Method /lscript2Error Moverâ€™s Distance Unsmoothed (Ã—10âˆ’2)Smoothed G.T. (Ã—10âˆ’2)Smoothed (Ã—10âˆ’2)\nContrast CRF 0.130Â±0.007 0 .180Â±0.007 1 .300Â±0.007 0 .539Â±0.002 0 .342Â±0.0142\nMFCC CRF 0.122Â±0.004 0 .173Â±0.004 1 .300Â±0.000 0 .541Â±0.010 0 .326Â±0.008\nENT CRF 0.130Â±0.004 0 .179Â±0.003 1 .300Â±0.009 0 .510Â±0.010 0 .337Â±0.009\nContrast CRF-I 0.138Â±0.006 0 .188Â±0.005 1 .323Â±0.007 0 .452Â±0.012 0 .355Â±0.011\nMFCC CRF-I 0.135Â±0.004 0 .186Â±0.003 1 .319Â±0.006 0 .459Â±0.007 0 .350Â±0.008\nENT CRF-I 0.144Â±0.005 0 .194Â±0.004 1 .331Â±0.005 0 .446Â±0.007 0 .367Â±0.009\nContrast MLR 0.140Â±0.005 0 .213Â±0.009 1 .082Â±0.010 0 .580Â±0.018 0 .460Â±0.018\nMFCC MLR 0.141Â±0.005 0 .208Â±0.008 1 .076Â±0.009 0 .570Â±0.021 0 .448Â±0.021\nENT MLR 0.153Â±0.005 0 .204Â±0.007 1 .068Â±0.009 0 .560Â±0.018 0 .440Â±0.018\nTable 4 . Emotion prediction results for conditional random ï¬elds (CRF) trained on sequence examples as well as independent\nexamples (CRF-I). Multiple linear regression (MLR) provided as baseline.\ntwo heatmaps. These results are in the fourth column of Ta-\nble 4, where we ï¬nd the CRF to be the best performer with\nan EMD of 0.173, which is signiï¬cantly better than the CRF\ntrained on independent samples at 0.186and MLR at 0.213.\nBut we also investigate the absolute pixel error between\nthe predicted and ground truth heatmaps. These results are\nshown in the ï¬fth column of Table 4, and we ï¬nd that MLR\nappears to be performing slightly better than the CRF. This\nresult is not surprising given the sparsity that our ground\ntruth heatmaps exhibit, which is to be expected with 121\nhistogram bins computed from an average of 16.93rat-\nings. The MLR method which predicts Gaussian distribu-\ntions guarantees a smooth distribution, which will produce a\nlower pixel error if the ground truth is sparse or blocky than\nthe CRF which takes arbitrary shapes. But it can be easily\ndemonstrated that the CRF is more accurate by applying a\nsimple smoothing function to the ground truth.\nTo smooth out the blocking artifacts from sparsity we ap-\nply a simple 2-d Gaussian ï¬lter. This process applies a light\nsmoothing without altering the mean of the data. These re-\nsults are shown in the sixth column of Table 4. Here the\nCRF performs slightly better, and the performance similar-\nity is most likely because the CRF is producing rough edges\ncompared to the smooth MLR predictions that are computed\nfrom the Gaussian PDF. An interesting result is that the in-\ndependently learned CRFs perform the best here. This is\nmost likely because they produce more uniform transition\nprobabilities due to their training method.\nTo compensate for blocking artifacts in the CRF predic-\ntions, we apply a smoothing ï¬lter to them as well. Initial\nexperiments showed applying the same ï¬lter to the MLR\nheatmaps improved performance there too, so to keep our\nanalysis consistent we apply the ï¬lter them as well. We\nexamine the differences in heatmaps using mean absolute\nerror, and these results are shown in the seventh column of\nTable 4. In these results we see again that the CRF is per-\nforming signiï¬cantly better than MLR.6.4.3 Visualizing the Results\nShown in Figure 2 are the CRF heatmap predictions for\neight seconds of the song â€œSomething About You,â€ by\nBoston. The colormap of these heatmaps assigns red to ar-\neas of high density, blue to low, and uses the color spec-\ntrum to assign colors in between. This clip was selected be-\ncause of the large change in emotion that occurs at second\n29, where the song transitions from a low-energy, negative-\nemotion introduction into a high-energy, positive-emotion\nhard-rock verse. The system tracks the transition very accu-\nrately, showing a brief amount of uncertainty at second 30\nin terms of positive or negative emotion, and ï¬nally settles\non positive emotion at second 31. Prediction videos using\nthe system are also available online.7\n7. DISCUSSION AND FUTURE WORK\nWe have demonstrated conditional random ï¬elds to be a\npowerful tool for modeling time-varying musical emotion.\nThe CRF approach is shown to be superior to MLR both\nat predicting single A-V mean values as well as full emo-\ntion space heatmaps. Overall, the best performing feature\nfor CRF prediction is MFCCs, which differs from our MLR\nmethod, where spectral contrast performs best. This perhaps\nindicates that there is more information to be gained out of\nMFCCs when modeling the temporal evolution of emotion.\nUsing the earth moverâ€™s distance we are able to better\nanalyze the similarity between heatmaps by also taking into\naccount adjacent bin densities. While the MLR method ap-\npears to perform slightly higher when the ground truth dis-\ntributions are not smoothed, this is a result of blocking ar-\ntifacts in the ground truth. The the Gaussian density is a\nsmooth function, which is much more likely to be similar\nto a sparse ground truth distribution than the CRF predic-\ntions, which take on arbitrary shapes and are not necessarily\n7http://music.ece.drexel.edu/research/emotion\n781Oral Session 9: Emotion and Mood\nEmotion Space Heatmap PredictionBoston: Something About You, 25-32 secs Figure 2 . Emotion space heatmap prediction using conditional random ï¬elds. Shown is the predicted emotion from the\nbeginning of the song â€œSomething About You,â€ by Boston. These ï¬gures demonstrate the system tracking the emotion through\nthe low-energy, negative-emotion introduction, and through the transition at second 29 into a high-energy, positive emotion\nrock verse. In these ï¬gures, red indicates the highest density and blue is the lowest.\nas smooth. Overall, the ground truth representation could\nsigniï¬cantly beneï¬t from more data.\nIn a future approach, the CRF performance could be im-\nproved by developing a model which can encapsulate the\nA-V spatial relationships between CRF nodes, which could\npotentially produce smoother estimates without any need for\nlabel jittering. In such a model, we could also limit the con-\nnections between local heatmap pixels, thus allowing us the\nability to tradeoff model complexity for the ï¬‚exibility of our\nemotion space distribution ï¬‚exibility.\n8. ACKNOWLEDGMENT\nThis work is supported by National Science Foundation\naward IIS-0644151.\n9. REFERENCES\n[1] Y . E. Kim, E. M. Schmidt, R. Migneco, B. G. Morton, P. Richardson,\nJ. Scott, J. A. Speck, and D. Turnbull, â€œMusic emotion recognition: A\nstate of the art review,â€ in ISMIR , Utrecht, Netherlands, 2010.\n[2] E. M. Schmidt, D. Turnbull, and Y . E. Kim, â€œFeature selection for\ncontent-based, time-varying musical emotion regression,â€ in ACM\nMIR, Philadelphia, PA, 2010.\n[3] E. M. Schmidt and Y . E. Kim, â€œPrediction of time-varying musical\nmood distributions from audio,â€ in ISMIR , Utrecht, Netherlands, 2010.\n[4] â€”â€”, â€œPrediction of time-varying musical mood distributions using\nKalman ï¬ltering,â€ in IEEE ICMLA , Washinton, D.C., 2010.[5] J. A. Russell, â€œA complex model of affect,â€ J. Personality Social Psy-\nchology , vol. 39, pp. 1161â€“1178, 1980.\n[6] H. Chen and Y . Yang, â€œPrediction of the distribution of perceived mu-\nsic emotions using discrete samples,â€ IEEE TASLP , no. 99, 2011.\n[7] C. Joder, S. Essid, and G. Richard, â€œA conditional random ï¬eld frame-\nwork for robust and scalable audio-to-score matching,â€ IEEE TASLP ,\nno. 99, 2011.\n[8] Y . E. Kim, E. Schmidt, and L. Emelle, â€œMoodswings: A collaborative\ngame for music mood label collection,â€ in ISMIR , Philadelphia, PA,\n2008.\n[9] J. A. Speck, E. M. Schmidt, B. G. Morton, and Y . E. Kim, â€œA com-\nparative study of collaborative vs. traditional annotation methods,â€ in\nISMIR , Miami, Florida, 2011.\n[10] D. Jiang, L. Lu, H. Zhang, J. Tao, and L. Cai, â€œMusic type classiï¬ca-\ntion by spectral contrast feature,â€ in Proc. Intl. Conf. on Multimedia\nand Expo , 2002.\n[11] S. Davis and P. Mermelstein, â€œComparison of parametric representa-\ntions for monosyllabic word recognition in continuously spoken sen-\ntences,â€ IEEE TASSP , vol. 28, no. 4, 1980.\n[12] C. Sutton and A. McCallum, â€œAn introduction to conditional random\nï¬elds for relational learning,â€ in Introduction to Statistical Relational\nLearning , L. Getoor and B. Taskar, Eds. MIT Press, 2007, ch. 4, pp.\n93â€“127.\n[13] J. Lafferty, A. McCallum, and F. Pereira, â€œConditional Random Fields:\nProbabilistic Models for Segmenting and Labeling Sequence Data,â€ in\nICML , 2001.\n[14] O. Pele and M. Werman, â€œFast and robust earth moverâ€™s distances,â€ in\nICCV , 2009.\n782"
    },
    {
        "title": "Using Mutual Proximity to Improve Content-Based Audio Similarity.",
        "author": [
            "Dominik Schnitzer",
            "Arthur Flexer",
            "Markus Schedl",
            "Gerhard Widmer"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1417979",
        "url": "https://doi.org/10.5281/zenodo.1417979",
        "ee": "https://zenodo.org/records/1417979/files/SchnitzerFSW11.pdf",
        "abstract": "This work introduces Mutual Proximity, an unsupervised method which transforms arbitrary distances to similarities computed from the shared neighborhood of two data points. This reinterpretation aims to correct inconsistencies in the original distance space, like the hub phenomenon. Hubs are objects which appear unwontedly often as nearest neighbors in predominantly high-dimensional spaces. We apply Mutual Proximity to a widely used and standard content-based audio similarity algorithm. The algorithm is known to be negatively affected by the high number of hubs it produces. We show that without a modification of the audio similarity features or inclusion of additional knowledge about the datasets, applying Mutual Proximity leads to a significant increase of retrieval quality: (1) hubs decrease and (2) the k-nearest-neighbor classification rates increase significantly. The results of this paper show that taking the mutual neighborhood of objects into account is an important aspect which should be considered for this class of content-based audio similarity algorithms.",
        "zenodo_id": 1417979,
        "dblp_key": "conf/ismir/SchnitzerFSW11",
        "keywords": [
            "Mutual Proximity",
            "unsupervised method",
            "arbitrary distances",
            "similarity computed",
            "shared neighborhood",
            "hub phenomenon",
            "high-dimensional spaces",
            "audio similarity algorithm",
            "negatively affected",
            "significant increase"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nUSING MUTUAL PROXIMITY TO\nIMPROVE CONTENT-BASED AUDIO SIMILARITY\nDominik Schnitzer1,2, Arthur Flexer1, Markus Schedl2, Gerhard Widmer1,2\n1Austrian Research Institute for Artiï¬cial Intelligence (OFAI), Vienna, Austria\n2Department of Computational Perception, Johannes Kepler University, Linz, Austria\ndominik.schnitzer@ofai.at ,arthur.flexer@ofai.at ,\nmarkus.schedl@jku.at ,gerhard.widmer@jku.at\nABSTRACT\nThis work introduces Mutual Proximity, an unsupervised\nmethod which transforms arbitrary distances to similarities\ncomputed from the shared neighborhood of two data points.\nThis reinterpretation aims to correct inconsistencies in the\noriginal distance space, like the hub phenomenon. Hubs are\nobjects which appear unwontedly often as nearest neighbors\nin predominantly high-dimensional spaces.\nWe apply Mutual Proximity to a widely used and stan-\ndard content-based audio similarity algorithm. The algo-\nrithm is known to be negatively affected by the high number\nof hubs it produces. We show that without a modiï¬cation\nof the audio similarity features or inclusion of additional\nknowledge about the datasets, applying Mutual Proximity\nleads to a signiï¬cant increase of retrieval quality: (1) hubs\ndecrease and (2) the k-nearest-neighbor classiï¬cation rates\nincrease signiï¬cantly.\nThe results of this paper show that taking the mutual\nneighborhood of objects into account is an important aspect\nwhich should be considered for this class of content-based\naudio similarity algorithms.\n1. INTRODUCTION\nA number of audio similarity algorithms which have been\npublished so far are affected by the so called â€œhub prob-\nlemâ€ [1, 4, 6, 16]. Hubs are over-popular nearest neighbors,\ni.e. the same objects are repeatedly identiï¬ed as nearest\nneighbors. The effect is particularly problematic in algo-\nrithms for similarity search, as the same â€œsimilarâ€ objects\nare found over and over again. In 2010 Radovanovi Â´c et\nal. [19] published an in-depth work about hubs, showing\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proï¬t or commercial advantage and that copies\nbear this notice and the full citation on the ï¬rst page.\nÂ© 2011 International Society for Music Information Retrieval.that they are yet another facet of the curse of dimension-\nality. Radovanovi Â´c also showed that â€œbad hubsâ€ (objects\nwhich are a bad retrieval result, in addition to being a hub)\ncan degrade the retrieval quality of algorithms signiï¬cantly.\nThe work of this paper was inspired by these problems\nand presents a straightforward method to reduce the â€œhub\nproblemâ€ signiï¬cantly. In the case of the standard audio\nsimilarity algorithm we use in this work we can show how to\nreduce its number of hubs while simultaneously increasing\nits retrieval quality.\n2. RELATED WORK\nNearest neighbor search (NNS) is a well deï¬ned task: given\nan objectxï¬nd the most similar object in a collection of\nrelated objects. In the simplest case the problem is solved\nby a linear search, computing a distance/similarity between\nxand all other objects, sorting the distances/similarities to\nreturn the top k-nearest neighbors.\nA natural aspect of nearest neighbor relations is that they\ndo not need to be symmetric: that is, object yis the nearest\nneighbor ofx, but the nearest neighbor of yis another object\na(a/negationslash=x). This behavior is problematic if xandybelong\nto the same class but adoes not, thus it is said aviolates the\npairwise cluster stability [3]. Although ais, in terms of the\ndistance measure, the correct answer to the nearest neighbor\nquery fory, it may be beneï¬cial to use a distance measure\nenforcing symmetric nearest neighbors. Thus a small dis-\ntance between two objects would be returned only if their\nnearest neighbors concur. Figure 1 illustrates this effect.\nRepairing sometimes contradicting, asymmetric nearest\nneighbor information in a similarity measure was already in-\nvestigated in a number of works. The ï¬rst publication which\nexploits common near neighbor information dates back as\nfar as 1973. Jarvis and Patrick [11] propose a â€œShared Near\nNeighborâ€ similarity measure to improve the clustering of\nnon-globular clusters. As the name may suggest the Shared\nNear Neighbor ( SNN ) similarity is based on computing the\n79Poster Session 1\n(a)(a) Original nearest neigh-\nbor relations\n(b)(b) Desired nearest neigh-\nbor relations\nFigure 1 : Schematic plot of two classes (black/white ï¬lled\ncircles). Each circle has its nearest neighbor marked with\nan arrow: (a) violates the pairwise stability clustering as-\nsumption, (b) fulï¬lls the assumption. In many applications\n(b) would be the desired nearest neighbor relation for the\ndataset.\noverlap between the knearest neighbors of two objects x,y:\nSNN k(x,y) =|NNk(x)âˆ©NNk(y)|. (1)\nShared Near Neighbor similarity was also used by Ert Â¨oz\net al. [5] to ï¬nd the most representative items in a set of\nobjects. Jin et al. [12] use the Reverse Nearest Neighbor\n(RNN) relation to deï¬ne a general measure for outlier de-\ntection.\nOther work which takes advantage of the asymmetry of\nnearest neighbors to correct the distance space was per-\nformed by Pohle et al., who propose a method named Prox-\nimity Veriï¬cation (PV) [17]. Two objects are considered\nsimilar if both objects have a low nearest neighbor rank\naccording to their counterpart. An unsupervised technique\nusing the local neighborhood of objects to improve the re-\ntrieval accuracy of cover song detection systems is proposed\nby Lagrange and Serr `a [13].\nAn effect of high dimensionality which affects particu-\nlarly NNS is the hub problem. Berenzweig [4] suspected a\nconnection between the hub problem and the high dimen-\nsionality of the feature space. Radovanovi Â´c et al. [19] were\nable to provide more insight by linking the hub problem to\nthe property of distance concentration in high dimensions.\nConcentration is the surprising characteristic of all points in\na high dimensional space to be at almost the same distance\nto all other points in that space. It is usually measured as a\nratio between spread and magnitude, e.g. the ratio between\nthe standard deviation of all distances to an arbitrary refer-\nence point and the mean of these distances. If the standard\ndeviation stays more or less constant with growing dimen-\nsionality while the mean keeps growing, the ratio converges\nto zero with dimensionality going to inï¬nity. In such a caseit is said that the distances concentrate. This has been stud-\nied for Euclidean spaces and other /lscriptp-norms. Radovanovi Â´c\npresented the argument that in the ï¬nite case, some points\nare expected to be closer to the center than other points and\nare at the same time closer, on average, to all other points.\nSuch points closer to the center have a high probability of\nbeing hubs, i.e. of appearing in nearest neighbor lists of\nmany other points.\nHubs were observed in music information retrieval [2],\nimage [9] and text retrieval [19] making this phenomenon a\ngeneral problem for information retrieval and recommenda-\ntion algorithms.\nA music similarity algorithm which is adversely affected\nby the â€œhub problemâ€ is the method published by Mandel\nand Ellis [15]. The algorithm is widely seen as a standard\nmethod for computing music similarity and its hub prob-\nlems have already been noticed and investigated (for exam-\nple by Flexer et al. [6]). The algorithm uses a timbre model\ncomputed from the audio signal for music similarity. In its\ncore the basic method stores the music similarity informa-\ntion for each music piece in a single multivariate Gaussian,\nwhich is estimated from the Mel Cepstrum Frequency Co-\nefï¬cients [14] (MFCCs) of the audio signal. To compute the\nsimilarity usually closed form solutions of Kullback-Leibler\nrelated divergences are used.\n3. AUDIO SIMILARITY\nThis work uses the basic algorithm from Mandel and El-\nlis [15] to compute audio similarity. To compute the features\nwe use 25MFCCs for each 46msof audio with a 23mshop\nsize. This corresponds to a window size of 1024 and a hop\nsize of 512audio samples at a sampling rate of 22.05kHz .\nA Gaussian model is estimated from the MFCC represen-\ntation of each song so that ï¬nally a single timbre model is\ndescribed by a 25-dimensional mean vector, and a 25Ã—25-\ndimensional covariance matrix. We use the Matlab music\nanalysis (MA) toolbox1to compute the features.\nTo compute the similarity between two timbre models\nwe use a Jensen-Shannon approximation (js), a stable sym-\nmetrized version of the Kullback-Leibler divergence from\nthe multivariate normal (MVN) toolbox2.\n4. THE METHOD\nIn this section we introduce a method that is based on: (i)\ntransforming distances between points xandyinto proba-\nbilities thatyis closest neighbor to xgiven the distribution\nof all distances to xin the data base, (ii) combining these\nprobabilistic distances from xtoyandytoxvia the prod-\nuct rule. The result is a general unsupervised method to\n1http://www.pampalk.at/ma/\n2http://www.ofai.at/~dominik.schnitzer/mvn\n8012th International Society for Music Information Retrieval Conference (ISMIR 2011)\ntransform arbitrary distance matrices to matrices of proba-\nbilistic mutual proximity (MP). The ï¬rst step of transforma-\ntion to probabilities re-scales and normalizes the distances\nlike a z-transform. The second step combines the proba-\nbilities to a mutual measure akin to shared near neighbor\napproaches. By supporting symmetric nearest neighbors the\nmethod leads to a natural decrease of asymmetric neighbor-\nhood relations and as a result, to a decrease of hubs.\n4.1 Preliminaries\nGiven a non-empty set Mwithnobjects, each object mxâˆˆ\nMassigned an index x= 1..n. We deï¬ne MP to be used for\na divergence measure d:MÃ—Mâ†’Rwith the following\nproperties:\nâ€¢ non-negativity: d(mx,my)â‰¥0,\nâ€¢ identity:d(mx,my) = 0,â‡â‡’mx=my,\nâ€¢ symmetry: d(mx,my) =d(my,mx).\nIndividual elements mxâˆˆMare referenced in the text by\ntheir indexx. The distance between two elements refer-\nenced by their index is denoted as dx,y.\n4.2 Mutual Proximity (MP)\nIn a ï¬rst step for each element xthe average distance Ë†Âµx\nand the standard deviation Ë†Ïƒxof all its distances dx,i=1..n\ninMis computed, estimating a Gaussian distance distri-\nbutionX/revsimilarN(Ë†Âµx,Ë†Ïƒx)for each element x(Equation 2).\nThis is based on the assumption that our data is normally\ndistributed due to the central limit theorem. The estimated\nnormalXthus models the spread of distances from xto all\nother elements in M:\nË†Âµx=1\nnn/summationdisplay\ni=1dx,i, Ë†Ïƒ2\nx=1\nnn/summationdisplay\ni=1(dx,iâˆ’Ë†Âµx)2.\nFigure 2a shows a schematic plot of the probability den-\nsity (pdf) function which was estimated for the distances of\nx. The mean distance ( Ë†Âµx) is in the center of the density\nfunction. Objects with a small distance to x(i.e. objects\nwith high similarity in the original space) ï¬nd their distance\non the left-side of the density function. Note that the left-\nmost distance in the Gaussian is dx,x= 0.\nBy estimating a normal distribution Xfrom the distances\ndx,i=1..n, it is possible to reinterpret the distance dx,yas the\nprobability that yis the nearest neighbor of x, given the dis-\ntancedx,yand normalX(that is the probability that a ran-\ndomly drawn element zwill have a distance dx,z>dx,y):\nP(X >d x,y) = 1âˆ’P(X/lessdblequaldx,y)\n= 1âˆ’Fx(dx,y).Fxdenotes the cumulative distribution function (cdf) of\nthe normal distribution deï¬ned by X. The probability of an\nelement being a nearest neighbor of xincreases the more\nleft its distance is on the x-axis of the pdf (cf. Figure 2a). To\nillustrate that Figure 2b plots the probability of ybeing the\nnearest neighbor of xgivendx,y(the ï¬lled area).\nTransforming all original distances into the probability\nthat a pointyis a nearest neighbor of xoffers a convenient\nway to combine this with the opposite view (the probability\nxis the nearest neighbor of y) into a single expression.\nDeï¬nition 1 Under the assumption of independence, we\ncompute the probability that yis the nearest neighbor of x\ngivenX(the Normal deï¬ned by the distances dx,i=1..n) and\nxis the nearest neighbor of ygivenY(the Normal deï¬ned\nby the distances dy,i=1..n). We call the resulting probability\nMutual Proximity (MP) :\nMP(dx,y) =P(X >d x,yâˆ©Y >d x,y)\n=P(X >d x,y)Â·P(Y >d x,y),âˆ€dx,y>0\n(2)\nClearly the assumption of independence of P(X)and\nP(Y)will be violated, still MP has, as we will show empiri-\ncally, largely positive effects especially in high dimensional\ndata spaces with high hubness.\n4.3 Properties\nMP is symmetric MP(dx,y) =MP(dy,x)and its values\nare normalized to the interval [0âˆ’1]. Note that the method\ncan therefore be easily used to linearly combine multiple\ndifferent distance measures.\nMP will only be high if both nearness probabilities are\nhigh and thus if their distance indicates a close mutual rela-\ntionship in terms of their distance distributions. If this is not\nthe case, i.e., one of the probabilities is small, their MP will\nbe small too.\n4.4 Matlab\nThe following Octave3/Matlab4code snippet demonstrates\nthe simplicity of the method. It computes DMPfor a given\nnÃ—ndistance matrix D:\nm = mean(D);\ns = std(D);\nfor i = 1:n\nfor j = (i+1):n\nD_MP(i, j) =\n(1 - normcdf(D(i, j), m(i), s(i))) *\n(1 - normcdf(D(i, j), m(j), s(j)));\nend\nend\n3http://www.gnu.org/software/octave/\n4http://www.mathworks.com/products/matlab/\n81Poster Session 1\nX ~ N( Âµx, Ïƒx)\nÂµxÂµx âˆ’ ÏƒxÂµx + Ïƒxdx,xf(dx,i)\ndx,i=1..n(a) The closer other elements are to x, the more left their distance is located\non x-axis of the density function plot. The leftmost point is the distance\ndx,x= 0.\nX ~ N( Âµx, Ïƒx)f(dx,i)\ndx,yP(X > dx,y)\ndx,i=1..ndx,x(b) The shaded area shows the probability that yis the nearest neighbor of\nxbased on the distance dx,yandX. The closer yis tox(the smaller dx,y)\nthe higher the probability.\nFigure 2 : Schematic plot of the probability density function of a normal distribution X/revsimilarN(Ë†Âµx,Ë†Ïƒx)which was estimated\nfrom the distances dx.\n5. EV ALUATION\nTo evaluate the effects of using MP for the selected au-\ndio similarity algorithm we use eight different music col-\nlections (see Table 1 for collection characteristics like col-\nlection size or numbers of genres). The collection sizes\nrange from 100 to 16 000 music pieces. Four collections\n(homburg [10], ismir2004-train5andismir2004-dev ,ball-\nroom [7]) are public benchmark sets. The other collections\n(DB-S, DB-XL, DB-RBA, DB-L) are private benchmark\ncollections. Each individual song in the collections is as-\nsigned to a music genre.\n5.1 Metrics\nThe following metrics are used to evaluate the Mutual Prox-\nimity transformation with the music similarities:\n5.1.1 Leave-One-Out, k-Nearest Neighbor Genre\nClassiï¬cation ( Ck)\nWe compute the k-nearest neighbor classiï¬cation accuracy\nusing a leave-one-out genre classiï¬cation. The k-NN classi-\nï¬cation accuracy is denoted with Ck. Higher values indicate\nmore consistent retrieval quality in terms of the class/genre.\nIt is one of the standard methods to measure the retrieval\nquality of audio similarity algorithms.\n5.1.2 Goodman-Kruskal Index ( IGK)\nTo evaluate the impact of the MP transformation, we also\ncompute the Goodman-Kruskal Index [8]. IGKis a ratio\ncomputed from the number of concordant (Qc) and discon-\ncordant (Qd) distance tuples. A distance tuple is concordant\nifdi,j<dk,land objectsi,jare from the same classes and\nk,lfrom different classes. It is is discordant if di,j>dk,l.\n5http://ismir2004.ismir.net/genre contest/index.htmIGKis bound to the interval [âˆ’1; 1]. The higher it is, the\nmore concordant distance tuples were found, thus indicating\ntighter and better clustering.\n5.1.3 Hubness ( Sk)\nWe also compute the hubness [19] for each collection. Hub-\nness is deï¬ned as the average skewness of the distribution of\nk-occurrences ( Nk):\nSk=E/bracketleftbig\n(Nkâˆ’ÂµNk)3/bracketrightbig\nÏƒ3\nNk\nPositive skewness indicates high hubness (high number\nof hub objects), skewness values around zero a more even\ndistribution of nearest neighbors.\n5.2 Results\nTable 1 displays the full evaluation results of the selected au-\ndio similarity algorithm according to the metrics introduced\nin the previous section. In the table each collection spans\ntwo rows, the ï¬rst row showing the evaluation metrics com-\nputed for the original data space and the second row listing\nthe values when using MP.\nThe collections listed in the table are sorted by their hub-\nness value in the original distance space. From the high\nhubness values ( 1.93âˆ’9.29) the hub problem of the audio\nsimilarity algorithm can be clearly seen. For example, a sin-\ngle hub song in DB-L is occurring in over 10% of all k= 5\nnearest neighbor lists in the collection. On the contrary hub-\nness is sharply decreasing when looking at the values MP\nproduces, which may indicate that MP creates a more evenly\nspread object space. The average hubness values per collec-\ntion decrease from 4.6 to 1.2; Figure 4 shows the individual\nhubness values in a plot. Another metric which increases\nfor all collections is the Goodman-Kruskal index( IGK), in-\ndicating a better separation of genres in the distance space\nafter using MP.\n8212th International Society for Music Information Retrieval Conference (ISMIR 2011)\nName, # Collection Genres nDistance Ck=1+/-Ck=5+/-Sk=5IGK\nDB-S 16 100 js 57.0%5.042.0%7.21.93 0.59\n1 MP 62.0% 49.2% 0.65 0.74\nballroom 8 698 js 54.7%4.946.3%4.82.63 0.16\n2 MP 59.6% 51.1% 1.05 0.20\nismir 2004 (tr) 6 729 js 82.9%3.473.6%3.23.61 0.35\n3 MP 86.3% 76.8% 1.15 0.41\nismir 2004 (tr+dev) 6 1458 js 86.5%3.880.6%3.34.22 0.37\n4 MP 90.3% 83.9% 1.31 0.42\nhomburg 9 1886 js 46.7%3.743.6%3.14.26 0.30\n5 MP 50.4% 46.7% 1.33 0.34\nDB-XL 21 16778 js 55.9%5.146.6%5.54.69 0.12\n6 MP 61.0% 52.1% 1.37 0.19\nDB-RBA 36 3423 js 51.4%4.741.6%4.85.77 0.26\n7 MP 56.1% 46.4% 1.69 0.31\nDB-L 22 2526 js 77.2%5.068.1%5.79.29 0.47\n8 MP 82.2% 73.8% 1.16 0.55\nTable 1 : The detailed evaluation results comparing the use of MP with a standard variant. The evaluation criteria are described\nin Section 5.\n1 2 3 4 5 6 7 80123456\nCollection #Increase (%) Ck=1\n(a) Increase of 1-NN classiï¬cation rates in %-points\n1 2 3 4 5 6 7 8012345678\nCollection #Increase (%) Ck=5\n(b) Increase of 5-NN classiï¬cation rates in %-points\nFigure 3 : Using MP increases the genre 1/5-NN classiï¬ca-\ntion rates of each music collection signiï¬cantly.\n1 2 3 4 5 6 7 80246810\n# CollectionHubness Sk=5\n  \nOriginal Divergence\nMPFigure 4 : Hubness values decrease when using the MP; a\ndesirable property for a music recommendation algorithm.\nWe also compute the Ck=1andCk=5genre classiï¬ca-\ntion rates. When comparing the two values computed for\nthe original audio similarity measure and MP, we see that\nin all collections the retrieval quality in terms of genre clas-\nsiï¬cation rates increases noticeable when MP is used. For\nk= 1 classiï¬cation increases on average by 4.5%-points,\nfork= 5on average by 4.7%-points. Figure 3 and Table 1\n(columns +/-) show the increase in 1/5-NN genre classiï¬ca-\ntion rates per collection.\n5.3 Summary\nTo summarize the evaluation we can see that all metrics we\ncomputed to evaluate the impact of MP lead to signiï¬cant\nimprovements in the retrieval quality of the basic audio sim-\nilarity measure proposed by Mandel and Ellis [15] in 2005.\n83Poster Session 1\nIn the case of the homburg andismir 2004 music genre\ncollections its performance is now very close to the reported\nperformance of the audio similarity algorithm by Pohle et\nal. [18] which ranked top in the 2009/10 MIREX (task: au-\ndio similarity and retrieval ) evaluations. Their quite sophis-\nticated algorithm uses MFCCs, Spectral Contrast features,\nâ€œHarmonicnessâ€, â€œAttacknessâ€ and a Rhythm component\n(Table 2).\nCollection Pohle [18] Mandel [15] Mandel+MP\nhomburg 50.9% 46.7% 50.4%\nismir 2004 (tr) 87.6% 82.9% 86.3%\nismir 2004 (tr+dev) 90.4% 86.5% 90.3%\nTable 2 : Nearest-neighbor ( k= 1) leave-one-out- genre\nclassiï¬cation accuracy comparison using MP. The numbers\nfrom Pohle are taken from the referenced paper [18].\n6. DISCUSSION AND FUTURE WORK\nThe authors ï¬nd it very exciting to see the potential for im-\nprovements that one of the most basic content-based audio\nsimilarity algorithms still offers without any modiï¬cation\nof its MFCC similarity features. Without using any class\ninformation and only by using a simple unsupervised trans-\nformation rewarding common neighbors, the long standing\nproblem of hub songs is alleviated and genre classiï¬cation\nrates for the algorithm can be increased signiï¬cantly.\nAs Mutual Proximity can be used with arbitrary distance\nmeasures it is also interesting to study the effects of MP on\ndatasets from different research areas. Preliminary tests in\nthat direction show that MP has in fact similar beneï¬cial\neffects on any high dimensional dataset suffering from high\nhubness in its original distance space.\nACKNOWLEDGMENTS\nThis research is supported by the Austrian Research Fund\n(FWF) (grants P22856-N23, L511-N15, Z159) and the Vi-\nenna Science and Technology Fund WWTF (Audiominer,\nproject number MA09-024). The Austrian Research Insti-\ntute for Artiï¬cial Intelligence is supported by the Austrian\nFederal Ministry for Transport, Innovation, and Technology.\n7. REFERENCES\n[1] J.J. Aucouturier and F. Pachet. Improving timbre similarity:\nHow high is the sky. Journal of Negative Results in Speech\nand Audio Sciences , 1(1):1â€“13, 2004.\n[2] J.J. Aucouturier and F. Pachet. A scale-free distribution of false\npositives for a large class of audio similarity measures. Pattern\nRecognition , 41(1):272â€“284, 2008.\n[3] K.P. Bennett, U. Fayyad, and D. Geiger. Density-based index-\ning for approximate nearest-neighbor queries. In Proceedings\nof the ï¬fth ACM SIGKDD international conference on Knowl-\nedge discovery and data mining , pages 233â€“243. ACM, 1999.[4] A. Berenzweig. Anchors and hubs in audio-based music simi-\nlarity . PhD thesis, Columbia University, 2007.\n[5] L. Ert Â¨oz, M. Steinbach, and V . Kumar. Finding clusters of dif-\nferent sizes, shapes, and densities in noisy, high dimensional\ndata. In SIAM international conference on data mining , vol-\nume 47, 2003.\n[6] A. Flexer, D. Schnitzer, M. Gasser, and T. Pohle. Combining\nFeatures Reduces Hubness in Audio Similarity. Proceedings of\nthe 11th International Society for Music Information Retrieval\nConference (ISMIRâ€™10), Utrecht, The Netherlands , 11, 2010.\n[7] F. Gouyon, S. Dixon, E. Pampalk, and G. Widmer. Evaluat-\ning rhythmic descriptors for musical genre classiï¬cation. In\nProceedings of the AES 25th International Conference , pages\n196â€“204, 2004.\n[8] S. Gunter and H. Bunke. Validation indices for graph cluster-\ning.Pattern Recognition Letters , 24(8):1107â€“1113, 2003.\n[9] A. Hicklin, B. Ulery, and C.I. Watson. The myth of goats: How\nmany people have ï¬ngerprints that are hard to match? US\nDept. of Commerce, National Institute of Standards and Tech-\nnology, 2005.\n[10] H. Homburg, I. Mierswa, B. M Â¨oller, K. Morik, and M. Wurst.\nA benchmark dataset for audio classiï¬cation and clustering. In\nProceedings of the International Conference on Music Infor-\nmation Retrieval , pages 528â€“31, 2005.\n[11] R.A. Jarvis and E.A. Patrick. Clustering using a similarity\nmeasure based on shared near neighbors. IEEE Transactions\non Computers , pages 1025â€“1034, 1973.\n[12] W. Jin, A. Tung, J. Han, and W. Wang. Ranking outliers using\nsymmetric neighborhood relationship. Advances in Knowledge\nDiscovery and Data Mining , pages 577â€“593, 2006.\n[13] M. Lagrange, J. Serr `a. Unsupervised Accuracy Improvement\nfor Cover Song Detection Using Spectral Connectivity Net-\nworks. Proceedings of the 11th International Society for Music\nInformation Retrieval Conference (ISMIRâ€™10), Utrecht, The\nNetherlands , 11, 2010.\n[14] B. Logan. Mel frequency cepstral coefï¬cients for music mod-\neling. In International Symposium on Music Information Re-\ntrieval (ISMIRâ€™00) , 2000.\n[15] M. Mandel and D. Ellis. Song-level features and support vec-\ntor machines for music classiï¬cation. In Proceedings of the\n6th International Conference on Music Information Retrieval\n(ISMIRâ€™05), London, UK , 2005.\n[16] E. Pampalk, A. Flexer, and G. Widmer. Improvements of\naudio-based music similarity and genre classiï¬cation. In Pro-\nceedings of the International Conference on Music Informa-\ntion Retrieval (ISMIRâ€™05), London, UK , 2005.\n[17] T. Pohle, P. Knees, M. Schedl, and G. Widmer. Automatically\nadapting the structure of audio similarity spaces. Proceedings\nof the 1st Workshop on Learning the Semantics of Audio Sig-\nnals (LSAS 2006) , page 66, 2006.\n[18] T. Pohle, D. Schnitzer, M. Schedl, P. Knees, and G. Widmer.\nOn rhythm and general music similarity. In Proceedings of the\n10th International Conference on Music Information Retrieval\n(ISMIRâ€™09) , 2009.\n[19] M. Radovanovi Â´c, A. Nanopoulos, and M. Ivanovi Â´c. Hubs in\nspace: Popular nearest neighbors in high-dimensional data.\nThe Journal of Machine Learning Research , 11:2487â€“2531,\n2010.\n84"
    },
    {
        "title": "A Re-ordering Strategy for Accelerating Index-based Audio Fingerprinting.",
        "author": [
            "Hendrik Schreiber 0001",
            "Peter Grosche",
            "Meinard MÃ¼ller"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1417607",
        "url": "https://doi.org/10.5281/zenodo.1417607",
        "ee": "https://zenodo.org/records/1417607/files/SchreiberGM11.pdf",
        "abstract": "The Haitsma/Kalker audio fingerprinting system [4] has been in use for years, but its search algorithmâ€™s scalability has not been researched very well. In this paper we show that by simple re-ordering of the query fingerprintâ€™s subprints in the index-based retrieval step, the overall search performance can be increased significantly. Furthermore, we show that combining longer fingerprints with re-ordering can lead to even higher performance gains, up to a factor of",
        "zenodo_id": 1417607,
        "dblp_key": "conf/ismir/SchreiberGM11",
        "keywords": [
            "Haitsma/Kalker audio fingerprinting system",
            "search algorithm scalability",
            "re-ordering subprints",
            "index-based retrieval step",
            "overall search performance",
            "increased significantly",
            "longer fingerprints",
            "re-ordering",
            "higher performance gains",
            "up to a factor"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nA RE-ORDERING STRATEGY FOR ACCELERATING INDEX-BASED\nAUDIO FINGERPRINTING\nHendrik Schreiber\ntagtraum industries\nincorporated\nhs@tagtraum.comPeter Grosche\nSaarland University\nand MPI Informatik\npgrosche@mpi-inf.mpg.deMeinard M Â¨uller\nSaarland University\nand MPI Informatik\nmeinard@mpi-inf.mpg.de\nABSTRACT\nThe Haitsma/Kalker audio ï¬ngerprinting system [4] has\nbeen in use for years, but its search algorithmâ€™s scalability\nhas not been researched very well. In this paper we show\nthat by simple re-ordering of the query ï¬ngerprintâ€™s sub-\nprints in the index-based retrieval step, the overall search\nperformance can be increased signiï¬cantly. Furthermore,\nwe show that combining longer ï¬ngerprints with re-ordering\ncan lead to even higher performance gains, up to a factor of\n9.8. The proposed re-ordering scheme is based on the ob-\nservation that sub-prints, which are elements of n-runs of\nidentical consecutive sub-prints, have a higher survival rate\nin distorted copies of a signal (e.g. after mp3 compression)\nthan other sub-prints.\n1. INTRODUCTION\nIn 2002 Jaap Haitsma and Ton Kalker proposed their au-\ndio ï¬ngerprinting system [4], which today is still in use at\nGracenote [3], competing with other commercial systems\nlike Shazam [7, 8]. In this system, identity of two songs is\nestablished by comparing so called ï¬ngerprints. These ï¬n-\ngerprints correspond to ca. 3seconds of audio and are com-\nprised of 256 sub-prints, each representing 11.6millisec-\nonds of audio with 32-bits. For a general overview of audio\nï¬ngerprinting systems, we refer to [1].\nTo identify an unknown audio fragment (the query), ï¬n-\ngerprints are extracted from the query and compared with\nï¬ngerprints stored in a database. Typically, as the fragment\nis exposed to distortions such as additive noise or compres-\nsion artifacts, one cannot assume to ï¬nd an identical ï¬nger-\nprint in the database. Therefore, the similarity of two ï¬n-\ngerprints is expressed in terms of the bit error rate (BER).\nThe lower the BER, the more likely two ï¬ngerprints belong\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proï¬t or commercial advantage and that copies\nbear this notice and the full citation on the ï¬rst page.\nc/circlecopyrt2011 International Society for Music Information Retrieval.to the same song. If the BER is below a certain threshold\n(Ï„= 0.35), both ï¬ngerprints are assumed to stem from the\nsame song.\nComparing all query ï¬ngerprints for a song with all\nreference ï¬ngerprints is only feasible for databases con-\ntaining a very limited number of recordings. There-\nfore, Haitsma/Kalker proposed an efï¬cient two-step hashing\nscheme. In the ï¬rst step, indexing techniques are employed\nto detect â€œanchor pointsâ€ in the database. The idea is, that\neven though there typically is not an exact match for a whole\nquery ï¬ngerprint in the database, at least one of the 256sub-\nprints occurs unaltered (without any bit error) in query and\nreference ï¬ngerprints. Exploiting this idea, Haitsma/Kalker\npropose to use one 32-bit long sub-print of the query ï¬nger-\nprint at a time and query the reference database for identical\nsub-prints. The positions of exact matches of sub-prints in\nthe database then serve as said anchor points. In the second\nstep, the BER for entire ï¬ngerprints (consisting of 256sub-\nprints) around these anchor points is calculated. If it turns\nout to be lower than the threshold, the search is terminated\nand the identiï¬ed song returned.\nAs the number of reference ï¬ngerprint lookups and BER\ncomputations is considerably reduced by this strategy, this\nway of searching is multiple orders of magnitude faster than\nthe na Â¨Ä±ve approach of comparing ï¬ngerprints with all refer-\nence ï¬ngerprints in the database. The lookup of potentially\nmatching songs using unaltered sub-prints is a crucial step in\nthis approach. To ï¬nd them, the system maintains a lookup\ntable with entries for each of the possible 32-bit sub-prints.\nEach entry points to a linked list of songs the given sub-\nprint occurs in and the position of the sub-print within this\nsong (Figure 1). Obviously the system is faster, if it ï¬nds a\nmatching ï¬ngerprint in as few sub-print lookups as possible.\nIn this paper, we propose an extension to the original al-\ngorithm. Our main idea is to re-order the lookup of unal-\ntered sub-prints in such a way that those sub-prints more\nlikely to survive compression distortions are looked up ï¬rst.\nIn our experiments, we show that this is the case for sub-\nprints, which are elements of n-runs of identical consecu-\ntive sub-prints (Figure 2). Exploiting this property in a sim-\n127Poster Session 1\n!\"#$%&'()*+!\"#%($%*,#+-+!\"!!!!!!!#+!\"!!!!!!!!+-++.)%+\n/0123454016+\n789:;4016<+!\"!!!!!!!!+!\"!!!!!!!#+-++++++!\"////////+=>>?85+@A9B3+\n7>12+*+!\"#.*,C///+-+++!\"!!!!!!!!+-+++!\"**D)&#.%+7>12+,+\n!\"*,C&D$#)+-+!\"!!!!!!!#+-+++!\"!!!!!!!!+-+!\"//D)&$#%+\nE.)F+\nFigure 1 . Lookup strategy for potentially matching songs\nand their reference sub-prints as suggested in [4].\n. . .\n. . .\n. . . . . ....\n3-run 2-run 2-run\nFigure 2 . Illustrative example showing one ï¬ngerprint con-\nsisting of 256sub-prints. The ï¬ngerprint exhibits runs of\nidentical sub-prints.\nple re-ordering scheme leads to signiï¬cant speed-ups of the\nsearch algorithm. In a second step, we apply the re-ordering\nscheme to ï¬ngerprints longer than 256sub-prints achieving\neven higher improvements up to a factor of 9.8.\nThe remainder of this paper is organized as follows. In\nSection 2 we motivate our re-ordering scheme by investi-\ngating the distribution of sub-prints and their likelihood of\nsurviving compression distortions. Then, in Section 3, as\nour main contribution, we introduce in detail the re-ordering\nscheme. In Section 4 we give experimental evidence for the\nspeed-up of our approach in a real-world runtime analysis.\nFinally, conclusions and outlook on future work are given in\nSection 5.\n2. SUB-PRINT PROPERTIES\nIn this section, we explain in detail the computation of the\nï¬ngerprints as proposed in [4] (Section 2.1). Then, in Sec-\ntion 2.2, we show that these ï¬ngerprints are strongly cor-\nrelated over time by analyzing audio recordings of three\ndatasets of different genres. Finally, in Section 2.3, we show\nthat the temporal correlation can be exploited for identifying\nmore robust sub-prints.\n2.1 Computation\nFollowing [4], we compute the sub-prints from a given au-\ndio signal in three steps. In the ï¬rst step, a spectrogram\nis derived from the audio. To this end, discrete Fourier\ntransforms are computed over Hann-windowed frames cor-\nRWC-P (iTunes 128kbps mp3)4-run5-run6-run7-run8-run9-run10-runrunsingle2-run3-run4-run5 or more6-run7-run8-run9-run10-run0.03276540.0448230650.0453473140.0322411540.053212850.0722891540.080321280.079317270.051204820.0641509440.067924530.0566037740.0792452840.0792452840.052830190.0389610380.090909090.1168831140.142857150.142857150.129870130.0649350660.0833333360.0833333360.1250.0833333360.0416666680.00.00.0833333360.28571430.428571430.28571430.28571430.428571430.428571430.428571430.142857150.14285715NaNNaNNaNNaNNaNNaNNaNNaNNaNNaN# of prints% of prints% of prints5931950.70236250119880.70236250119881770440.20962595211060.2096259521106517080.06122398235320.0612239823532152600.01806834475730.018068344757349800.00589648472420.008719219580115900.00188261259275390.00063819382861920.000227334350820.846776593702630.00007459408390062.540326862556\n1%\n2%\n6%\n20%\n72%Pop (mp3)\n3%\n4%\n9%\n23%\n62%Jazz (mp3)\n2%\n3%\n8%\n23%\n65%Classical (mp3)\nsingle2-run3-run4-run5 or more\n1%\n1%\n5%\n19%\n74%Pop (GSM)\n1%\n2%\n7%\n22%\n68%Jazz (GSM)\n1%\n2%\n6%\n21%\n70%Classical (GSM)Figure 3 . Percentages of sub-prints occurring as singles, or\nin higher runs in mp3 and GSM ï¬les from genre-speciï¬c\nRWC collections (ï¬rst 3 min ). The distributions for the\nWA V encoded reference ï¬les are almost identical.\nresponding to 0.37 sec of the audio. These frames overlap\nby a factor of 31/32yielding one frame for every 11.6 ms .\nIn the second step, by suitably pooling spectral coefï¬cients,\nthe frequency axis of the spectrogram is adapted to the hu-\nman auditory system. More precisely, energy values are\ncomputed for 33non-overlapping spectral bands. These\nbands are logarithmically spaced and cover the frequency\nrange from 300 Hz to2000 Hz . Finally, in the third step,\nï¬ngerprints are derived. Given the energy in frame tâˆˆ\n[0 :T] :={0,1,2,...,T}for someTâˆˆNand spectral\nbandkâˆˆ[1 : 33] denoted byE(t,k), we ï¬rst compute en-\nergy differences âˆ†(t,k)along the frequency axis âˆ†(t,k)=\nE(t,k)âˆ’E(t,k+ 1) for alltâˆˆ[0 :T]andkâˆˆ[1 : 32] .\nThen, the energy values are quantized in order to obtain a\nbinary representation Xâˆˆ{0,1}TÃ—32by determining the\nsign of energy differences along the time axis\nX(t,k) =/braceleftbigg1ifâˆ†(t,k)>âˆ†(tâˆ’1,k)\n0otherwise,\nfortâˆˆ[1 :T]. LetX[t]âˆˆ{0,1}32denote thetthcol-\numn ofX. Following [4], such a binary vector is also re-\nferred to as sub-print . Furthermore, ï¬xing a length param-\neterK(in the following we use K= 256 ), a binary block\nFâˆˆ{0,1}KÃ—32is referred to as ï¬ngerprint consisting of\nthe sub-prints F[k],kâˆˆ[1 :K]. Each of the sub-prints\nrepresents 11.6 ms of the audio with 32-bit, see Figure 2 for\na schematic illustration of a ï¬ngerprint.\n2.2 Temporal Correlation\nAs pointed out in [4], because of the high amount of overlap\nbetween adjacent frames, the sub-prints are temporally cor-\nrelated. In fact, they are correlated so strongly that often one\n12812th International Society for Music Information Retrieval Conference (ISMIR 2011)\nFigure 4 . Probability of an iTunes mp3 encoded sub-print\nat a given position min ann-run having an identical coun-\nterpart in a reference ï¬ngerprint depending on the length of\nthen-run it belongs to.\nsub-print is followed by one or more identical sub-prints.\nWe call such a sequence of identical consecutive sub-prints\nann-run (see Figure 2). For the remainder of this paper, n-\nruns withn= 1will also be called singles andn-runs with\nn>1are referred to as higher runs.\nTo better understand the temporal correlation of sub-\nprints, we analyzed a large collection of audio recordings\nof various genres with respect to the occurrence of n-\nruns. Speciï¬cally, we used the sub-collections RWC-Jazz,\nRWC-Pop, and RWC-Classical provided by the RWC Music\nDatabase [2]. Overall, there are 211recordings with a total\nduration of 16 hours . We consider each of these recordings\nin three different versions of different quality. Firstly, we re-\nfer to the CD quality versions denoted as reference versions.\nFurthermore, we consider two encoded ( distorted ) versions\nderived from the reference employing lossy audio codecs.\nAs a mildly compressed version, we use an mp3 version\nencoded with 128 kbps using iTunes. This version can be\nregarded to be of â€œstandardâ€ quality. Finally, as a heavily\ncompressed version of poor audio quality, we encode the\nreference versions using the (full rate) 13kbps GSM V oice\ncodec. Originally intended for the compression of speech\nsignals, this codec introduces severe audible distortions to\nmusic signals. This version is included in our analysis as an\nextreme case.\nFigure 3 shows the percentage of sub-prints that occur in\nann-run of identical sub-prints for versions encoded with\niTunes mp3 128kbps or GSM. As our results show, the sub-\nprints are temporally correlated. For all three datasets and\nboth encodings about 30% of all sub-prints occur in higher\nruns. For example, in the case of RWC-Jazz (mp3), 23% are\nFigure 5 . Probability of a GSM encoded sub-print at a given\nposition in an n-run having an identical counterpart in a ref-\nerence ï¬ngerprint depending on the length of the n-run it\nbelongs to.\na member of a 2-run, 9%of a3-run, 4%of a4-run, and 3%\nof a5-run or even longer run.\n2.3 Robustness\nWe hypothesize that such sub-prints occurring in n-runs are\nmore likely to have unaltered counterparts in distorted ver-\nsions of the same song than sub-prints occurring on their\nown, i.e. as singles. In other words, as the survival rate of\nsub-prints in higher runs is higher, they are more robust.\nTo test this hypothesis we measure the probability of a\nsub-print extracted from a distorted version being identical\nto its reference counterpart. This is done by computing sub-\nprints of the ï¬rst 3 min for both reference and distorted ver-\nsions. Because some distortions introduce a minor, linear\nframe-shift (Â±1), we then align both sub-print lists so that\nas many as possible sub-prints are directly opposite an iden-\ntical counterpart. This we call optimal alignment. Subse-\nquently, we categorize the distorted sub-prints as members\nofn-runs along with their position mâˆˆ[1 :n]in the run\nand record how many sub-prints of each category have un-\naltered, aligned counterparts in the reference sub-print list.\nFigures 4 and 5 show the probabilities of sub-prints that\nare members of an n-run (column group) at position m(col-\numn) having an unaltered counterpart in the reference ï¬n-\ngerprint, i.e. their survival probabilities. Our results for\niTunes encoded mp3 audio (Figure 4) clearly show that sub-\nprints belonging to higher runs are more likely to have an\nunaltered counterpart in the reference ï¬ngerprint than sub-\nprints occurring as singles. Taking the RWC-Pop values\nas example, a single sub-print has a relatively low survival\n129Poster Session 1\nprobability of 23%, while a member of a 6-run has a sur-\nvival probability between 45% and65%. Note that, because\nof the low value for singles, the relative survival rate gain\nfrom singles to higher runs is larger for the pop songs than\nfor jazz or classical music. For example, the maximal pos-\nsible gain factor from single to 6-run for RWC-Pop is Ã—2.8,\nwhile the gain factor for the RWC-Classical from the single\nwith52% to the maximal 6-run with 95% is onlyÃ—1.8.\nFor GSM encoded ï¬les the effect becomes even more sig-\nniï¬cant (Figure 5). Here, the survival probabilities are much\nlower, e.g., 0.7%for singles in the case of RWC-Classical.\nFor5-runs this probability increases to values between 5.1%\nand8.0%, a gain factor ofÃ—11.4.\nAs a second important result, we observe, that in most\ncases those members of n-runs, that take a central position\nin their run, are even more likely to have an identical ref-\nerence counterpart than n-run elements at edge positions.\nFor example, in the case of RWC-Pop (mp3), see Figure 4,\nsub-prints at edge positions of a 6-run have a survival prob-\nability of 45%. For sub-prints at center position, however,\nthis probability is signiï¬cantly higher at 65%. In the case of\nRWC-Classical (GSM), see Figure 5, sub-prints at edge po-\nsitions of a 5-runs survive with 5.7%probability, sub-prints\nat the center position, however, with 8.0%. We deï¬ne this\ncentral position in an n-run asmcentral =âŒŠn/2âŒ‹+ 1. In\nFigures 4 and 5 it is shown in black.\n3. IMPROVING THE SEARCH ALGORITHM\nIn this section, we ï¬rst explain the original Haitsma/Kalker\nlookup algorithm. Then we describe our proposed improve-\nments, which are based on the increased robustness of sub-\nprints contained in higher runs. Finally, we present an ex-\nperiment that measures actually achieved overall speedups\nvalidating our chosen approach.\n3.1 Original Algorithm\nSuppose we are given a database containing a large number\nof audio documents, which are converted into binary repre-\nsentations as described in Section 2.1. Then, given a query\nï¬ngerprintFQâˆˆ{0,1}KÃ—32, the identiï¬cation task con-\nsists of ï¬nding a document with binary representation X\nas well as a position tsuch that the ï¬ngerprint deï¬ned by\nFD:= (X[t],...,X [t+Kâˆ’1])is similar to FQ. More\nprecisely, as in [4], we require that the bit error rate (BER)\nbetweenFQandFDis below a threshold Ï„= 0.35. We then\nalso say that FDis amatch forFQ.\nTo avoid an exhaustive ï¬ngerprint search in the database,\nan index-based pre-processing step is used to cut down\nthe search space. Here, the binary representations of all\ndatabase documents are indexed by means of the 32-bit sub-\nprints using an index structure that consists of a suitable\nlookup table as illustrated by Figure 1. Then, based on theassumption that at least one sub-print FQ[k],kâˆˆ[1 :K],\nof the query appears unaltered in the document to be iden-\ntiï¬ed, a lookup is performed to ï¬rst retrieve all sub-prints\nthat coincide with FQ[k]. Each of these retrieved candidate\nsub-print consists of a document identiï¬er and a position\nparametert. LetXbe the binary representation of the cor-\nresponding document, then the BER is computed between\nFQandFD:= (X[tâˆ’k+ 1],...,X [tâˆ’k+K]). If the\nBER falls below the threshold Ï„= 0.35, the algorithm ter-\nminates and returns the associated document identiï¬er. If\nno suchFDcan be found, the algorithm terminates without\nidentifying the query.\nSince a position k, that corresponds to an unaltered sub-\nprint in the database, is not known a-priori, in [4] an outer\nloop is executed querying the index structure for sub-prints\nF[k]in the order in which they appear in FQ, i.e. with in-\ndicesk= 1,2,3,...,K . This loop is aborted as soon as\na matching ï¬ngerprint is found. Therefore, the overall run-\nning time of the algorithm crucially depends on the position\nof the index at which an unaltered sup-print of a matching\nï¬ngerprint occurs for the ï¬rst time.\n3.2 Sub-Print Re-Ordering\nTo take advantage of the observed sub-print properties, we\nchange the order in which sub-prints are looked up in the\ndatabase. Instead of simply iterating through the sub-prints\nof the query ï¬ngerprint from beginning to end, we prioritize\nthose sub-prints that are more likely to lead to matching ï¬n-\ngerprints. This means that we need to look up the central\nsub-prints of higher runs ï¬rst, ordered by the length of the\nrun they belong to in descending order. Then we look up the\nsingles and then all remaining sub-prints, again ordered by\nthe length of the run they belong to.\nFigure 6 shows an example for this re-ordering scheme.\nBecause the longest run is the 3-run, we rank its central el-\nement (3) ï¬rst. The second longest run is the 2-run, thus its\ncentral element (8) lands on rank 2. Since there are no other\nhigher runs, we then proceed to add all singles in the order\nin which they appear. And eventually, we add the remaining\nsub-prints from the two higher runs (2, 4 and 9).\nThe idea behind this is, that if a central sub-print does not\nlead to a match, it is more likely that another central sub-\nprint leads to a match (even if it is a member of a shorter\nn-run) than a non-central sub-print of an n-run we already\nknow of that its central sub-print does not match.\nMore formally, for a ï¬ngerprint FQof lengthK= 256\nwe calculate the rank (k,m,n )withk,m,nâˆˆ[1 :K]of\neach sub-print FQ[k]that is themthelement of an n-run, and\norder all sub-prints according to their rank in descending\norder. Therank function is deï¬ned as\nrank (k,m,n ) =k+Km+K2n+K3nÎ´mâˆ’1,âŒŠn/2âŒ‹(1)\n13012th International Society for Music Information Retrieval Conference (ISMIR 2011)\n!!\"\"!!\"!\"!\"\"!\"\"\"!!\"\"!!\"\"!!\"\"!!\"\"#\"#!!\"\"!!\"\"!!\"\"!!\"\"!!\"\"!!\"\"!!\"\"\"\"\"\"#$#!!\"\"!!\"\"!!\"\"!!\"\"!!\"\"!!\"\"!!\"\"\"\"\"\"#%#!!\"\"!!\"\"!!\"\"!!\"\"!!\"\"!!\"\"!!\"\"\"\"\"\"#&#!\"\"\"!!\"\"!!\"\"!!\"\"!!\"\"!!\"\"!!\"\"!\"\"\"#'#!!\"\"!\"\"\"!!\"!!!\"\"!!\"\"!!!\"!!\"\"\"!\"\"#(#!!\"\"!!\"\"!!!\"!\"\"\"!!\"\"!\"\"!!!\"\"!!\"\"#)#!!\"\"!!\"\"!!\"\"!!\"\"!!\"\"!!\"\"\"\"\"\"!!\"\"#*#!!\"\"!!\"\"!!\"\"!!\"\"!!\"\"!!\"\"\"\"\"\"!!\"\"#+#!\"\"\"!!\"\"\"!\"\"!!\"\"!!\"\"!!\"\"!!\"\"!!\"\"#\"!#%#*#\"#'#(#)#\"!#$#&#+#\n,-./012345#6247#895:45#;:/81<:1:<#895:45#,23=>:#%/1-3#$/1-3#Figure 6 . Optimization of the sub-print order.\nQuery iTunes 128kbps Lame 32kbps GSM\nOrig. 4.15 36.59 100.84\n256 1.41 (Ã—2.9) 12.82 ( Ã—2.9) 67.62 ( Ã—1.5)\n512 1.31 (Ã—3.2) 8.90 ( Ã—4.1) 60.58 ( Ã—1.7)\n1024 1.25 (Ã—3.3) 6.48 ( Ã—5.6) 43.79 ( Ã—2.3)\n2048 1.22 (Ã—3.4) 5.06 ( Ã—7.2) 27.04 ( Ã—3.7)\n4096 1.20 (Ã—3.5) 4.17 ( Ã—8.8) 18.08 ( Ã—5.6)\n8192 1.24 (Ã—3.3) 3.73 ( Ã—9.8) 14.30 ( Ã—7.0)\nTable 1 . Average number of sub-print lookups until a\nsub-print match is found, depending on query distortion\nand ï¬ngerprint length (based on 100,000 randomly selected\nqueries). Denoted in parentheses are the factors between the\noptimized and the original approach.\nand consists of four terms, each containing a weight factor\nbased onK. The last term is only /negationslash= 0if and only if the sub-\nprint is central. In that case, the term dominates the outcome\nof the function. If it is not central, the length nof the run\nbecomes the deciding factor, as K2nwill be greater than\nthe two remaining terms kandKm. Amongst sub-prints\nbelonging to the same run length n, positionmwithin the\nrun andkin the ï¬ngerprint become tie-breakers.\nAdditionally to re-ordering, in a second optimization\nstep, we also use a longer query ï¬ngerprint. This did not\nmake sense before optimizing the lookup order, as we were\nnot able to recognize more robust sub-prints. But with\nthe suggested re-ordering scheme, enlarging the ï¬ngerprint\nincreases our chances of ï¬nding more and longer n-runs,\ntherefore signiï¬cantly increasing our chances of ï¬nding a\nsurviving sub-print counterpart in the reference data.\nA side effect of this strategy is the necessity of computing\na larger query ï¬ngerprint, which puts some additional com-\nputational burden on the client and requires a longer audio\nfragment. For the BER computation we still only use 256\nsub-prints as there is nothing to be gained by using more\nsub-prints.3.3 Experimental Veriï¬cation\nTo test both approaches, sub-print re-ordering and ï¬nger-\nprint enlargement, we measured the average number of\nsub-print lookups needed to ï¬nd a matching ï¬ngerprint in\na database of 200 songs for 100,000 randomly selected\nqueries.\nNote, that in this experiment we focus on sub-print com-\nparisons, not full ï¬ngerprint comparisons. Therefore, the\nnumber of songs in the database is irrelevant. Nevertheless,\nsub-print matches lead to ï¬ngerprint comparisons. In order\nto measure the total search time, those also need to be taken\ninto account, if the number of song pointers per sub-print\nis not distributed uniformly. For the purpose of this exper-\niment we assume a uniform distribution, in particular one\nthat is independent from the used rank function.\nThe results in Table 1 show that with our optimization\nscheme between 1.5 and 9.8 times fewer lookups are neces-\nsary. Even without enlarging the query ï¬ngerprint, we were\nstill able to achieve 2.9times fewer iterations for mp3 ï¬les\nencoded at 128kbps. This equates to only 1.41sub-print\nlookups on average.\nIt also deserves to be mentioned that for audio data with\nstronger distortions (e.g. GSM, mp3 32kbps) our techniques\ntend to yield larger beneï¬ts. One reason for this is that for\nstrongly distorted audio material many more lookups are\nnecessary when no re-ordering is used ( 100.84for GSM as\nopposed to 4.15for mp3 128kbps).\n4. RUNTIME ANALYSIS\nWhy do we care so much about the sub-print lookups? Re-\nalistically, a large scale audio ï¬ngerprinting system will\nhave to be able to manage not just 10,000 songs [4], but\nrather 100 million songsâ€”perhaps even more.1Assum-\ningÂ±25,000sub-prints per song, this results in a total of\n25,000Â·108= 25Â·1011sub-prints. This means that the\nlookup table proposed by Haitsma/Kalker is not sparsely\npopulated as they claim, but on average each entry con-\ntains a list of pointers to 582 ( = 25Â·1011/232) songs.\nAssuming that each of these pointers has at least a size of\n4 bytes to reference a song, plus an offset into the songâ€™s\nreference ï¬ngerprint of 2 bytes, we must manage roughly\n232Â·(4 + 2)Â·582bytes = 15 terabytes for the pointer lists\nalone. Obviously, with current technology, we cannot sim-\nply load the data-structure into the main memory of a regular\nPC.\nInstead, just like the songsâ€™ sub-prints, the data-structure\nalso has to live in secondary storage (e.g. ï¬‚at ï¬les, a\nrelational database management system (RDBMS), a no-\nSQL database, or a simple Berkeley DB). In the case of an\n1In May 2011 MusicBrainz [6] stated on its website to have more than\n10 million tracks in its database. This number is probably going to increase\nsigniï¬cantly as the years go by.\n131Poster Session 1\n!\"#$%&'()*&'(**&+*&'(**)\"#$%&'(*),-..&'(*/0)1(**Figure 7 . Trivial table design for the reference song lookup\nwith an RDBMS.\nRDBMS, a single table containing the columns (song-) id(at\nleast 4 bytes), sub-print (4 bytes) and (sub-print-) offset (at\nleast 2 bytes) is sufï¬cient (Figure 7). One database index on\nthesub-print column and another on idandoffset ensure fast\naccess.2Assuming the aforementioned setup, the measur-\nable runtime behavior of the algorithm is governed by three\nmain factors:\n1. Number of songs in the database.\n2. Speed of lookup from secondary storage.\n3. Probability of a query sub-print having an identical\nreference counterpart.\nNote that only the number of ï¬ngerprint lookups and\nBER computations depend directly and linearly on the num-\nber of songs in the database. This means that the overall\nruntime is linear with respect to the size of the database.\nAs for the secondary storage, even though solid state\ndrives and the decreasing price of RAM slightly blur the\nlines, accessing secondary storage still takes much more\ntime than performing relatively simple arithmetic operations\nlike computing a BER. Therefore we can safely assume that\neach SQL- select operation to look up a ï¬ngerprint in the\ndatabase takes orders of magnitude longer than the associ-\nated BER computation. Besides the collectionâ€™s size, sec-\nondary storage access is therefore a determining factor for\nthe absolute runtime of the algorithm.\nFinally, how many times we have to look up complete\nï¬ngerprints and access secondary storage depends highly on\nthe probability that a given query sub-print has an identi-\ncal reference counterpart. As shown above, we can signif-\nicantly increase the probability of ï¬nding an identical sub-\nprint quickly by re-ordering the sub-prints. This is a decid-\ning factor for the runtime of this algorithm and unlike the\nother two mentioned factors it has nothing to do with avail-\nable hardware or the size of the problem.\n2For 100 million songs, this database design leads to the impressive\nstorage requirement of 25terabytes (= (4 + 4 + 2) Â·25Â·1011bytes ),\nplus additional space for the indices. Not surprisingly, Haitsma/Kalker at-\ntempted to reduce this by sub-sampling reference ï¬ngerprints [5].5. CONCLUSION\nIn this paper we presented an optimization scheme of the\nHaitsma/Kalker audio ï¬ngerprinting search algorithm. The\nsuggested approach exploits strong temporal correlations\nbetween sub-prints as an indicator for sub-print robustness.\nThis can lead to signiï¬cant savings in the number of re-\nquired lookups leading to a signiï¬cant overall speed-up for\nthe identiï¬cation task.\nFuture research may focus on applying the proposed\nstrategy on other existing algorithms or creating new ones,\nin which only reliable sub-prints are taken into account to\nbegin with, which may lead to shorter, more robust ï¬nger-\nprints and reduced overall storage requirements. Also the\ncombination of our re-ordering strategy with the reliability\nconsiderations proposed by Haitsma/Kalker is subject for\nfuture research.\nAcknowledgement. P. Grosche and M. M Â¨uller are sup-\nported by the Cluster of Excellence on Multimodal Com-\nputing and Interaction at Saarland University.\n6. REFERENCES\n[1] Pedro Cano, Eloi Batlle, Ton Kalker, and Jaap Haitsma. A re-\nview of algorithms for audio ï¬ngerprinting. In Proceedings of\nthe IEEE International Workshop on Multimedia Signal Pro-\ncessing (MMSP) , pages 169â€“173, St. Thomas, Virgin Islands,\nUSA, 2002.\n[2] Masataka Goto, Hiroki Hashiguchi, Takuichi Nishimura, and\nRyuichi Oka. RWC music database: Popular, classical and jazz\nmusic databases. In Proceedings of the International Confer-\nence on Music Information Retrieval (ISMIR) , Paris, France,\n2002.\n[3] Gracenote. http://www.gracenote.com/ .\n[4] Jaap Haitsma and Ton Kalker. A highly robust audio ï¬nger-\nprinting system. In Proceedings of the International Confer-\nence on Music Information Retrieval (ISMIR) , pages 107â€“115,\nParis, France, 2002.\n[5] Jaap Haitsma, Ton Kalker, and Steven Schimmel. Efï¬cient\nstorage of ï¬ngerprints. US Patent 7,477,739, January 2009.\n[6] MusicBrainz. http://musicbrainz.org/ .\n[7] Shazam. http://www.shazam.com/ .\n[8] Avery Wang. An industrial strength audio search algorithm. In\nProceedings of the International Conference on Music Infor-\nmation Retrieval (ISMIR) , pages 7â€“13, Baltimore, USA, 2003.\n132"
    },
    {
        "title": "Multi-Modal Non-Prototypical Music Mood Analysis in Continuous Space: Reliability and Performances.",
        "author": [
            "BjÃ¶rn W. Schuller",
            "Felix Weninger",
            "Johannes Dorfner"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1417097",
        "url": "https://doi.org/10.5281/zenodo.1417097",
        "ee": "https://zenodo.org/records/1417097/files/SchullerWD11.pdf",
        "abstract": "Music Mood Classification is frequently turned into â€˜Music Mood Regressionâ€™ by using a continuous dimensional model rather than discrete mood classes. In this paper we report on automatic analysis of performances in a mood space spanned by arousal and valence on the 2.6 k songs NTWICM corpus of popular UK chart music in full realism, i. e., by automatic web-based retrieval of lyrics and diverse acoustic features without pre-selection of prototypical cases. We discuss optimal modeling of the gold standard by introducing the evaluator weighted estimator principle, group-wise feature relevance, â€˜tuningâ€™ of the regressor, and compare early and late fusion strategies. In the result, correlation coefficients of .736 (valence) and .601 (arousal) are reached on previously unseen test data.",
        "zenodo_id": 1417097,
        "dblp_key": "conf/ismir/SchullerWD11",
        "keywords": [
            "Music Mood Classification",
            "Continuous dimensional model",
            "Discrete mood classes",
            "Automatic analysis",
            "Mood space",
            "Arousal and valence",
            "NTWICM corpus",
            "Popular UK chart music",
            "Web-based retrieval",
            "Acoustic features"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nMULTI-MODAL NON-PROTOTYPICAL MUSIC MOOD ANALYSIS IN\nCONTINUOUS SPACE: RELIABILITY AND PERFORMANCES\nBjÂ¨orn Schuller1, Felix Weninger1, Johannes Dorfner2\n1Institute for Human-Machine Communication,2Institute for Energy Economy and\nApplication Technology, Technische Universit Â¨at M Â¨unchen, Germany\nschuller@tum.de\nABSTRACT\nMusic Mood Classiï¬cation is frequently turned into â€˜Mu-\nsic Mood Regressionâ€™ by using a continuous dimensional\nmodel rather than discrete mood classes. In this paper we\nreport on automatic analysis of performances in a mood\nspace spanned by arousal and valence on the 2.6 k songs\nNTWICM corpus of popular UK chart music in full real-\nism, i. e., by automatic web-based retrieval of lyrics and di-\nverse acoustic features without pre-selection of prototypical\ncases. We discuss optimal modeling of the gold standard\nby introducing the evaluator weighted estimator principle,\ngroup-wise feature relevance, â€˜tuningâ€™ of the regressor, and\ncompare early and late fusion strategies. In the result, cor-\nrelation coefï¬cients of .736 (valence) and .601 (arousal) are\nreached on previously unseen test data.\n1. INTRODUCTION\nMusic mood analysis, i. e., automatic determination of the\nperceived mood in recorded music, has been an active ï¬eld\nof research in the last decade. For instance, it can en-\nable browsing through music collections for music with a\nspeciï¬c mood, or to automatically select music best suited\nto a personâ€™s current mood as determined manually or au-\ntomatically. In this study, we describe music mood by\nRussellâ€™s circumplex model of affect consisting of a two-\ndimensional space of valence (pleasureâ€“displeasure) and\ndegree of arousal which allows to identify emotional tags,\nsuch as the ones used for the MIREX music mood evalua-\ntions [9], as points in the â€˜mood spaceâ€™, avoiding the am-\nbiguity of categorical taxonomies [21]. Note that in re-\ncent research, e. g. [11], new models have been proposed\nspeciï¬cally for music emotion, which go beyond the tra-\nditional emotion models by including non-utilitaristic or\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proï¬t or commercial advantage and that copies\nbear this notice and the full citation on the ï¬rst page.\nc/circlecopyrt2011 International Society for Music Information Retrieval.eclectic emotions. However, the valence / arousal model\nis an emerging standard for describing human emotions in\nautomatic analysis [4]. Thus, from an application point of\nview, it is, e. g., useful for matching human emotions and\nmusic mood, such as for automatic music suggestion [16].\nFor automatic music mood recognition, a great variety of\nfeatures have been proposed, comprising low-level acous-\ntic, such as spectral, cepstral, or chromagram features [18],\nhigher-level audio features such as rhythm [14], as well as\ntextual features derived from the lyrics [12]. Early (feature-\nlevel) and late (classiï¬er-level) fusion techniques for the\nacoustic and textual modalities have been compared in [8].\nA ï¬rst major contribution of this study is to investigate\nregression in the continuous arousal / valence space by sin-\ngle modalities (spectrum, rhythm, lyrics, etc.), and by early\nas well as late fusion. To brieï¬‚y relate our work to recent\nperformance studies on music mood regression: In [18] re-\ngression in a purely acoustic feature space has been inves-\ntigated; [10] evaluates automatic feature selection and clas-\nsiï¬ers, but not various feature groups individually; [2] com-\npares prediction of dimensional and categorical annotation\nand highlights the relevance of single features without re-\nporting their actual performance. In summary, the majority\nof research still deals with classiï¬cation [8, 12, 14, 19], to\nrefer to a few recent studies. Besides, to deal with relia-\nbility issues of human music mood annotation [9], we in-\ntroduce the evaluator weighted estimator (EWE) [3] to the\nMusic Information Retrieval domain and evaluate its inï¬‚u-\nence on regression performance. The EWE has been pro-\nposed as a weighted decision taking into account reliabili-\nties of individual annotators for emotion recognition from\nspeech [3]. Furthermore, we extend late fusion approaches\nsuch as [8] by considering the regression performance of\nsingle modalities on the development set for determination\nof fusion weights, in analogy to the EWE used for reaching\na robust ground truth estimate.\nWe evaluate our system on the â€œNow Thatâ€™s What I Call\nMusic!â€ (NTWICM) database introduced in [19], contain-\ning 2 648 songs annotated by four listeners on 5-point scales\nfor perceived arousal and valence on song level. In con-\n759Poster Session 6\ntrast to some earlier work on music mood recognition such\nas [2], no instance pre-selection has been performed in or-\nder to simulate real-life conditions where an automatic sys-\ntem has to deal with non-prototypical instances, in particu-\nlar those characterized by low emotional intensity [10]. Our\nevaluation measure is the correlation coefï¬cient between the\nregression output and the estimated continuous ground truth.\nThe remainder of this contribution brieï¬‚y describes the\nevaluation database (Section 2), with a particular focus on\nannotation reliability, and the acoustic and linguistic fea-\ntures used for automatic regression (Section 3). Results of\nextensive regression runs are given in Section 4 before con-\ncluding in Section 5.\n2. NTWICM DATABASE\n2.1 Data Set\nFor building the NTWICM music database the compilation\nâ€œNow Thatâ€™s What I Call Music!â€ (U. K. series, volumes\n1â€“69) is selected. It contains 2 648 titles â€” roughly a week\nof total play time â€” and covers the time span from 1983\nto 2010. Likewise it represents very well most music styles\nwhich are popular today; that ranges from Pop and Rock\nmusic over Rap, R&B to electronic dance music as Techno\nor House. The stereo sound ï¬les are MPEG-1 Audio Layer\n3 (MP3) encoded using a sampling rate of 44.1 kHz and a\nvariable bit rate of at least 128 kBit/s as found in many typ-\nical use-cases of an automatic mood classiï¬cation system.\nFor 1 937 of 2 648 songs in the database (cf. Sec-\ntion 2.3, Table 2) lyrics can automatically be collected\nfrom two on-line databases: In a ï¬rst run lyricsDB,\n(http://lyrics.mirkforce.net/) is applied, which delivers lyrics\nfor 1 779 songs, then LyricWiki, (http://www.lyricwiki.org/)\nis searched for all remaining songs, which delivers lyrics for\n158 additional songs. The only manual post-processing car-\nried out was normalization of transcription inconsistencies,\ne. g., markers for chorus lines, among the databases.\n2.2 Annotation and Reliability\nSongs were annotated as a whole, i. e., without selection of\ncharacteristic song parts, to stick to real world use cases â€“\nsuch as music suggestion â€“ as closely as possible. Respect-\ning that mood perception is generally judged as highly sub-\njective [9], we decided for four labellers. While mood may\nwell change within a song, as change of more and less lively\npassages or change from sad to a positive resolution, anno-\ntation in such detail is particularly time-intensive. Yet, we\nare assuming the addressed music type â€“ mainstream pop-\nular and by that usually commercially oriented â€“ music to\nbe less affected by such variation as, for example, found in\nlonger arrangements of classical music. In fact, this can be\nvery practical and sufï¬cient in many application scenarios,age, g Ï CC CC-LORO\nVal Aro Val Aro Val Aro\nA 34, m .828 .749 .827 .763 .678 .456\nB 23, m .267 .623 .304 .640 -.012 .366\nC 26, m .797 .633 .800 .656 .651 .442\nD 32, f .797 .717 .819 .733 .640 .474\nTable 1 : NTWICM Database: Raters Aâ€“D by age and\ng(ender), and reliability of val(ence) and aro(usal) annota-\ntion by Spearmanâ€™s Ïand correlation coefï¬cient (CC) with\nmean (Aâ€“D), as well as CC in leave-one-rater-out (LORO)\nanalysis.\nas for automatically suggestion that ï¬ts a listenerâ€™s mood.\nDetails on the chosen raters (three male, one female, aged\nbetween 23 and 34 years; average: 29 years) and their pro-\nfessional and private relation to music are provided in Ta-\nble 1. As can be seen, they were picked to form a well-\nbalanced set spanning from rather â€˜naiveâ€™ assessors without\ninstrument knowledge and professional relation to â€˜expertâ€™\nassessors including a club disc jockey (D. J.). The latter can\nthus be expected to have a good relationship to music mood,\nand its perception by the audiences. Further, young raters\nprove a good choice, as they were very well familiar with\nall the songs of the chosen database. They were asked to\nmake a forced decision according to the two dimensions in\nthe mood plane assigning values in -2, -1, 0, 1, 2 for arousal\nand valence, respectively. They were further instructed to\nannotate according to the perceived mood, that is, the â€˜rep-\nresentedâ€™ mood, not to the induced, that is, â€˜feltâ€™ one, which\ncould have resulted in too high labelling ambiguity. The an-\nnotation procedure is described in detail in [19], and the an-\nnotation along with the employed annotation tool are made\npublicly available1.\nIn this study, we aim at music mood assessment in the\ncontinous domain as determined by the four raters. Thus,\na consensus has to be derived from the individual labellings\nfor valence and arousal. A continuous quantity as needed for\nregression is obtained as follows. As a ï¬rst step, we calcu-\nlated the agreement (reliability) of rater kâˆˆ{A,B,C,D}\nwith respect to the arithmetic mean label l(d)\nnfor each in-\nstancen,dâˆˆ{valence, arousal},\nl(d)\nn=1\n4/summationdisplay\nkl(d)\nn,k(1)\nwherel(d)\nn,kâˆˆ{âˆ’ 2,âˆ’1,0,1,2}is the label assigned by rater\nkto instance n. As a measure of reliability for each k,\nwe computed the correlation coefï¬cient CC kbetween (l(d)\nn,k)\nand(l(d)\nn). Results are shown in Table 1, where we also pro-\n1http://openaudio.eu/NTWICM-Mood-Annotation.\narff\n76012th International Society for Music Information Retrieval Conference (ISMIR 2011)\nvide the values for Spearmanâ€™s rho ( Ï) for reference: No-\ntable differences between CC and Ïcan mainly be seen for\nthe valence annotation by rater B.\nEvidently, the reliability in terms of CC kdiffers among\nthe raters â€“ especially for valence, where it ranges from .828\n(rater A, club D. J.) down to .267 (rater B). Hence, as a ro-\nbust estimate of the desired ground truth mood of each in-\nstancen, we additionally considered the EWE [3], denoted\nbyl(d)\nn, in further analyses:\nl(d)\nn=1/summationtext\nkCCk/summationdisplay\nkCCkl(d)\nn,k. (2)\nWe hypothesize that the EWE provides a robust ground truth\nestimate especially for the NTWICM database with only\nfour annotators, where a single â€˜unreliableâ€™ annotator does\nnot simply â€˜average outâ€™. Note that we refrain from report-\ning the agreement of the raters with the EWE, as in the EWE\ninformation about their reliability is already integrated. Fur-\nthermore, the CC of raters with the mean of allraters is ar-\nguably a slight overestimate of the true reliability, since the\nrating to be evaluated is included in the ground truth esti-\nmate. Thus, we additionally performed a â€˜leave-one-rater-\noutâ€™ (LORO) reliability analysis. Thereby for each rater\nkthe CC is calculated between (l(d)\nn,k)and the EWE of all\nraters except k. It turns out that human agreement is con-\nsiderably lower when measured in a LORO fashion â€“ partly,\nthis can be attributed to the fact that in the LORO analysis,\neach ground truth estimate is made up from only three raters.\nAgain, rater A exhibits the highest reliability whereas rater\nB is ranked last, both for valence and arousal (cf. Table 1).\n2.3 Partitioning\nWe partitioned the 2 648 songs into training, development,\nand test partitions through a transparent deï¬nition that al-\nlows easy reproducibility and is not optimized in any re-\nspect: Training and development are obtained by selecting\nall songs from odd years, whereby development is assigned\nby choosing every second odd year. By that, test is deï¬ned\nusing every even year. The distributions of instances per\npartition are displayed in Table 2, together with the number\nof instances for which lyrics are missing â€“ it can be seen that\ntheir proportion is roughly equal for all partitions.\nOnce development was used for optimization of classi-\nï¬er parameters, the training and development sets are united\nfor training. Note that this partitioning resembles roughly\n50 % / 50 % of overall training / test in order to favor statisti-\ncally meaningful ï¬ndings.\n3. FEATURES\nA summary of the feature groups discussed in this study is\ngiven in Table 3. They can be roughly categorized into fea-\ntures derived from the lyrics (Sections 3.1, 3.2), the songSet # songs # lyrics\nTrain 690 515 (75 %)\nDevel 686 509 (74 %)\nTrain+Devel 1 376 1 024 (74 %)\nTest 1 272 913 (72 %)\nSum 2 648 1 937 (73 %)\nTable 2 : Partitioning of the NTWICM Database, and avail-\nability of lyrics.\nmeta-information (Section 3.3), and ï¬nally the audio itself\n(Sections 3.5, 3.4, 3.6). A detailed explanation of the fea-\ntures is given in [19].\n3.1 Emotional Concepts\nSemantic features are extracted from the lyrics by the Con-\nceptNet [13] text processing toolkit, which makes use of a\nlarge semantic database automatically generated from sen-\ntences in the Open Mind Common Sense Project2. The\nsoftware is capable of estimating the most likely emotional\naffect in a raw text input, which has already been shown\nquite effective for valence prediction in movie reviews [20].\nThe underlying algorithm starts from a subset of con-\ncepts that are manually classiï¬ed into one of six emo-\ntional categories (happy, sad, angry, fearful, disgusted, sur-\nprised), and calculates the emotion of unclassiï¬ed concepts\nextracted from the songâ€™s lyrics by ï¬nding and weighting\npaths which lead to those classiï¬ed concepts. The algo-\nrithm yields six discrete features indicating a ranking of\nthe moods from highest to lowest dominance in the lyrics,\nand six continuous-valued features contain the correspond-\ning probability estimates.\n3.2 Linguistic Features: From Lyrics to Vectors\nLinguistic features are obtained from the lyrics by text pro-\ncessing methods proven efï¬cient for sentiment detection\n[20]. The raw text is ï¬rst split into words while remov-\ning all punctuation. In order to recognize different ï¬‚ex-\nions of the same word (e. g. loved ,loving ,loves should be\ncounted as love) the conjugated word has to be reduced to\nits word stem. This is done using the Porter stemming algo-\nrithm [15].\nWord occurences are converted to a vector (Bag-of-\nWords, BoW) representation where each component repre-\nsents a word stem that occurs at least 10 times. For each\nsong, the relative frequency of the stem is computed, i. e.,\nthe number of occurences is normalized by the total num-\nber of words in the songâ€™s lyrics. The dimensionality of the\nresulting feature set is 393.\n2http://openmind.media.mit.edu/\n761Poster Session 6\n3.3 Metadata\nAdditional information about the music is sparse in this\nwork because of the large size of the music collection used:\nBesides the year of release only the artist and title informa-\ntion is available for each song. While the date is directly\nused as a numeric attribute, the artist and title ï¬elds are pro-\ncessed in a similar way as the lyrics (cf. previous section):\nOnly the binary information about the occurrence of a word\nstem is retained. While the artist word list looks very spe-\nciï¬c to the collection of artists in the database, the title word\nlist seems to have more general relevance with words like\nâ€œloveâ€, â€œfeelâ€ or â€œsweetâ€. In total, the size of the metadata\nfeature set is 152.\n3.4 Chords\nFor chord extraction from the raw audio data a fully auto-\nmatic algorithm as presented by Harte and Sandler [6] is\nused. Its basic idea is to map signal energy in frequency\nsub-bands to their corresponding pitch class which leads to\na chromagram or pitch class proï¬le. Each possible chord\ntype corresponds to speciï¬c pattern of tones. By comparing\nthe chromagram with predeï¬ned chord templates, an esti-\nmate of the chord type (e. g., major, minor, diminished) can\nbe made. We recognize the nine chord types deï¬ned in [19]\nalong with the chord base tone (e. g. C, F, G /sharp). Each chord\ntype has a distinct sound which makes it possible to asso-\nciate it with a set of moods [1]: For instance, major chords\noften correspond to happiness, minor ones to a more melan-\ncholic mood, while diminished chords are frequently linked\nto fear or suspense. For each chord name and chord type, the\nrelative frequency per song is computed and augmented by\nthe total number of recognized chords (22 features in total).\n3.5 Rhythm\nThe 87 rhythm features rely on a method presented in [17].\nIt uses a bank of comb ï¬lters with different resonant fre-\nquencies covering a range from 60 to 180 bpm. The output\nof each ï¬lter corresponds to the signal energy belonging to a\ncertain tempo, devliering robust tempo estimates for a wide\nrange of music. Further processing of the ï¬lter output de-\ntermines the base meter of a song, i. e., how many beats are\nin each measure and what note value one beat has. The im-\nplementation used can recognize whether a song has duple\n(e. g., 2/4, 4/4) or triple (e. g., 3/4, 6/8) meter. A detailed\ndescription of the rhythm features is found in [19].\n3.6 Spectral\nSpectral features are straightforward and derived from the\nDiscrete Fourier Transform (DFT) of the songs, which is\nmixed down to a monophonic signal. Then, the centre ofGroup Description #\nChords rel. chord freq.; # distinct chords 22\nConcepts ConceptNetâ€™s mood from lyrics 12\nLyrics Bag-of-Words (BoW) from lyrics 393\nMeta BoW from artist, title; song date 153\nRhythm Tatum vec. (57); meter vec. (19); 87\ntatum cand.; tempo + meter estim.;\ntatum max, mean, ratio,\nslope, peak dist.\nSpec tral DFT centre of gravity, moments 2â€“4; 24\noctave band energies\nAll Union of the above 691\nNoLyr ics All\\(LyrâˆªCon ) 286\nTable 3 : Song-level feature groups and corresponding fea-\nture set sizes (#).\ngravity, and the second to fourth moment (i. e., standard de-\nviation, skewness, and kurtosis) of the spectrum are com-\nputed. Finally, band energies and energy densities for the\nfollowing seven octave based frequency intervals are added:\n0 Hzâ€“200 Hz, 200 Hzâ€“400 Hz, 400 Hzâ€“800 Hz, 800 Hzâ€“\n1.6 kHz, 1.6 kHzâ€“3.2 kHz, 3.2 kHzâ€“6.4 kHz and 6.4 kHzâ€“\n12.8 kHz, which yields a total of 24 spectral features.\n4. EXPERIMENTS AND RESULTS\n4.1 Setup\nIn our regression experiments we used ensembles of un-\npruned REPTrees with a maximum depth of 25 trained on\nrandom feature sub-spaces [7]. For straightforward repro-\nducibility, we relied on the open-source implementation in\nthe Weka toolkit [5].\nWe tuned the ensemble size (number of trees and sub-\nspace size) on the development set for each combina-\ntion of feature set and target (valence/arousal mean/EWE)\nto reï¬‚ect varying sizes and complexities of the fea-\nture sets. The number of trees was chosen from\n{10,20,50,100,200,500,1 000,2 000}and the sub-space\nsize from{.01,.02,.05,.1,.2,.5}. Results of the parame-\nter tuning for selected feature groups can be seen in Fig-\nures 1 (a)â€“(b). As expected due to different sizes of the fea-\nture space, optimal parameters vary considerably. Interest-\ningly, the best result for the Metfeature set is obtained with\n1 000 trees consisting of only 1â€“2 features, corresponding\nto a sub-space size of 1 %. Note that for the smallest fea-\nture set ( Con), the number of possible trees is bounded by/parenleftbig12\n6/parenrightbig\n= 924 , so a larger number of trees will result in dupli-\ncates by the pigeon hole principle.\n76212th International Society for Music Information Retrieval Conference (ISMIR 2011)\n10âˆ’210âˆ’1\n10102103Cross-correlation\nsub-space size#trees0.400.500.600.70(a)All\n10âˆ’210âˆ’1\n10102103\nsub-space size#trees0.300.400.500.60\n(b)Rhy\nFigure 1 : Tuning of ensemble size on CC with valence\nEWE on development set for All(a) and Rhy (b) feature\ngroups.\nCC Valence Arousal\nmean EWE mean EWE\nTrain vs. Devel .652 .680 .600 .593\nTrain+Devel vs. Test .701 .736 .613 .601\nTable 4 : Early fusion ( Allfeature set): CC of regression\non continuous valence and arousal (mean / EWE of anno-\ntators) by random sub-space learning with unpruned REP-\nTrees. Ensemble size tuned on development set (20 % sub-\nspace, 500 trees, 2 000 for mean valence).\nValence Arousal\n#tÃ—sss CC #tÃ—sss CC\nDev Test Dev Test\nCho 2kÃ—.2 .331 .409 2kÃ—.5 .299 .380\nCon 500Ã—.5 .047 .027 50Ã—.2 .079 .081\nLyr 100Ã—.1 .249 .266 200Ã—.2 .244 .312\nMet 1kÃ—.01 .209 .241 500Ã—.05 .212 .193\nRhy 100Ã—.2 .589 .620 2kÃ—.2 .520 .541\nSpe 2kÃ—.2 .518 .565 500Ã—.2 .452 .418\nNoL 2kÃ—.2 .678 .735 1kÃ—.2 .594 .602\nTable 5 : Single feature groups: CC of regression on contin-\nuous valence and arousal (EWE of annotators) by random\nsub-space learning with unpruned REPTrees. Number of\ntrees (#t) and sub-space size (sss) optimized on development\npartition.CC Valence Arousal\nALL NOLALL NOL\nTrain vs. Devel .693 .690 .599 .593\nTrain+Devel vs. Test .725 .720 .598 .588\nTable 6 : Late fusion of modalities: CC of regression on\ncontinuous valence and arousal (EWE of annotators). REP-\nTree ensembles for each modality parameterized as in Table\n5. Fusion weights corresponding to CC on development set.\n4.2 Results and Discussion\nWith the full feature set, CCs of .680 and .736 are obtained\nfor valence on the development and test sets, respectively\n(cf. Table 4)â€”this corresponds to R2statistics of .462 resp.\n.542. In that case, regression on the EWE is considerably\nmore robust than regression on the mean (absolute CC gains\nof .028 and .035 on development and test), which is proba-\nbly due to the different reliabilities of the annotators. In con-\ntrast, for arousal, where annotator reliability is more consis-\ntent, the CC with the EWE is even slightly lower (by .007\nand 0.012 absolute on development and test). In other terms,\nR2statistics of up to .36 (development) and .376 (test set)\nare obtained. For the sake of clarity, we will exclusively\nreport on CC with the EWE in the following discussion.\nAnalysis of single feature groups (Table 5) reveals that\nspectral and rhythm features contribute most to the regres-\nsion performance (CCs of .620 and .565 with the valence\nEWE on test). Chords (CC of .409) are in the mid-range\nwhile lyrics, meta information and concepts lag behind (CCs\nof .266, .241, .027). The same ranking of feature groups is\nobtained when considering the CC with the arousal EWE.\nWe conclude that the feature groups that enable robust re-\ngression can be obtained directly from the audio (chords,\nspectral and rhythm information), and thus in full realismâ€”\nthough lyrics likely contribute to the annotation since the\nannotators were not explicitly told to ignore lyrics and all of\nthem are experienced English speakers. In fact, the CC on\nthe test set by the NoLyrics feature set (.735) is only slightly\nlower than that with the full feature set (.736).\nThe noticeable differences between the reliability of dif-\nferent modalities motivate a late fusion technique where the\nfused prediction is a weighted sum of the predictions of uni-\nmodal regressors. Thereby weights correspond to the indi-\nvidual regressorsâ€™ CC on the development set, analogously\nto the EWE (Eqn. 2). Results obtained by this technique\nare shown in Table 6. On the development set, early fusion\n(cf. Table 4) is clearly outperformed for both recognition of\nvalence (CC of .693 vs. .680) and arousal (CC of .599 vs.\n.593). However, this effect is almost reversed on the test\nset, where a CC of .725 as opposed to .735 (early fusion)\nis obtained for valence; results are similar for arousal. The\nlatter result cannot be fully explained by overï¬tting fusion\n763Poster Session 6\nweights on the development set, as there is no considerable\nmismatch between the reliabilities on the development com-\npared with the test set.\n5. CONCLUSIONS\nWe analyzed regression of music mood in contiuous dimen-\nsional space. Particular emphasis was laid on realism in\nthe sense of automatically retrieving textual lyric informa-\ntion automatically from the web and by chosing a music\ndatabase that is well deï¬ned in its own: 69 consecutive dou-\nble CDs without pre-selection of high annotator agreement\ncases. As expected, the observed performances are clearly\nbelow the ones reported in studies on prototypical exam-\nples such as [2], yet in line with other studies on real-life\ndata sets [10, 21]. To establish a reliable gold standard,\ni. e., ground truth, we proposed the usage of the evaluator\nweighted estimator. The best individual feature group were\nrhythm features based on comb-ï¬lter banks. In future work\nwe will address unsupervised and semi-supervised learning\nfor music mood analysis to exploit the huge quantities of\npopular music available on the internet.\n6. REFERENCES\n[1] W. Chase. How Music REALLY Works! Roedy Black\nPublishing, Vancouver, Canada, 2nd edition, 2006.\n[2] T. Eerola, O. Lartillot, and P. Toiviainen. Prediction of\nmultidimensional emotional ratings in music from audio\nusing multivariate regression models. In Proc. of ISMIR ,\npages 621â€“626, Kobe, Japan, 2009.\n[3] M. Grimm and K. Kroschel. Evaluation of natural emo-\ntions using self assessment manikins. In Proc. of ASRU ,\npages 381â€“385, 2005.\n[4] H. Gunes, B. Schuller, M. Pantic, and R. Cowie. Emo-\ntion Representation, Analysis and Synthesis in Contin-\nuous Space: A Survey. In Proc. International Work-\nshop on Emotion Synthesis, rePresentation, and Anal-\nysis in Continuous spacE (EmoSPACE) , Santa Barbara,\nCA, 2011. IEEE.\n[5] M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-\nmann, and I. H. Witten. The WEKA data mining soft-\nware: An update. SIGKDD Explorations Newsletter ,\n11(1):10â€“18, 2009.\n[6] C. A. Harte and M. Sandler. Automatic chord identiï¬ca-\ntion using a quantised chromagram. In Proc. of the 118th\nConvention of the AES , May 2005.\n[7] T. K. Ho. The Random Subspace Method for Construct-\ning Decision Forests. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence , 20:832â€“844, 1998.\n[8] X. Hu and J. S. Downie. Improving mood classiï¬ca-\ntion in music digital libraries by combining lyrics and\naudio. In Proc. Joint Conference on Digital Libraries(JCDL) , pages 159â€“168, Gold Coast, Queensland, Aus-\ntralia, 2010.\n[9] X. Hu, J. S. Downie, C. Laurier, M. Bay, and A. F.\nEhmann. The 2007 MIREX Audio Mood Classiï¬cation\nTask: Lessons Learned. In Proc. ISMIR , pages 462â€“467,\nPhiladelphia, USA, 2008.\n[10] A. Huq, J. P. Bello, and R. Rowe. Automated Music\nEmotion Recognition: A Systematic Evaluation. Jour-\nnal of New Music Research , 39(3):227â€“244, 2010.\n[11] P. N. Juslin and J. A. Sloboda, editors. Handbook of mu-\nsic and emotion: Theory, research, applications . Oxford\nUniversity Press, New York, 2010.\n[12] C. Laurier, J. Grivolla, and P. Herrera. Multimodal music\nmood classiï¬cation using audio and lyrics. In Proc. In-\nternational Conference on Machine Learning and Appli-\ncations , pages 688â€“693, Washington, DC, USA, 2008.\n[13] H. Liu and P. Singh. ConceptNet â€” a practical com-\nmonsense reasoning tool-kit. BT Technology Journal ,\n22(4):211â€“226, 2004.\n[14] L. Lu, D. Liu, and H.-J. Zhang. Automatic mood detec-\ntion and tracking of music audio signals. IEEE Trans-\nactions on Audio, Speech and Language Processing ,\n14(1):5â€“18, 2006.\n[15] M. F. Porter. An algorithm for sufï¬x stripping. Program ,\n3(14):130â€“137, October 1980.\n[16] S. Rho, B.-J. Han, and E. Hwang. SVR-based mu-\nsic mood classiï¬cation and context-based music recom-\nmendation. In Proc. ACM Multimedia , pages 713â€“716,\nBeijing, China, 2009.\n[17] E. D. Scheirer. Tempo and beat analysis of acoustic mu-\nsical signals. Journal of the Acoustic Society of America ,\n103(1):588â€“601, January 1998.\n[18] E. M. Schmidt, D. Turnbull, and Y . E. Kim. Feature se-\nlection for content-based, time-varying musical emotion\nregression. In Proc. of MIR , pages 267â€“274, Philadel-\nphia, Pennsylvania, USA, 2010.\n[19] B. Schuller, J. Dorfner, and G. Rigoll. Determination\nof non-prototypical valence and arousal in popular mu-\nsic: Features and performances. EURASIP Journal on\nAudio, Speech, and Music Processing, Special Issue on\nScalable Audio-Content Analysis , 2010(ID 735854):19\npages, 2010.\n[20] B. Schuller and T. Knaup. Learning and Knowledge-\nbased Sentiment Analysis in Movie Review Key Ex-\ncerpts. In Toward Autonomous, Adaptive, and Context-\nAware Multimodal Interfaces: Theoretical and Practical\nIssues , volume 6456 of LNCS , pages 448â€“472. Springer,\nHeidelberg, 2010.\n[21] Y .-H. Yang, Y .-C. Lin, Y .-F. Su, and H.H. Chen. A re-\ngression approach to music emotion recognition. IEEE\nTransactions on Audio, Speech and Language Process-\ning, 16(2):448â€“457, 2008.\n764"
    },
    {
        "title": "Analysis of Acoustic Features for Automated Multi-Track Mixing.",
        "author": [
            "Jeffrey J. Scott",
            "Youngmoo E. Kim"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1416038",
        "url": "https://doi.org/10.5281/zenodo.1416038",
        "ee": "https://zenodo.org/records/1416038/files/ScottK11.pdf",
        "abstract": "The capability of the average person to generate digital music content has rapidly expanded over the past several decades. While the mechanics of creating a multi-track recording are relatively straightforward, using the available tools to create professional quality work requires substantial training and experience. We address one of the most fundamental processes to creating a finished product, namely determining the relative gain levels of each track to produce a final, mixed song. By modeling the time-varying mixing coefficients with a linear dynamical system, we train models that predict a weight vector for a given instrument using features extracted from the audio content of all of the tracks.",
        "zenodo_id": 1416038,
        "dblp_key": "conf/ismir/ScottK11",
        "keywords": [
            "digital music content",
            "multi-track recording",
            "professional quality work",
            "fundamental processes",
            "relative gain levels",
            "final mixed song",
            "time-varying mixing coefficients",
            "linear dynamical system",
            "weight vector",
            "instrument features"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nANALYSIS OF ACOUSTIC FEATURES FOR AUTOMATED\nMULTI-TRACK MIXING\nJeffrey Scott, Youngmoo E. Kim\nMusic and Entertainment Technology Laboratory (MET-lab)\nElectrical and Computer Engineering, Drexel University\n{jjscott, ykim }@drexel.edu\nABSTRACT\nThe capability of the average person to generate digital\nmusic content has rapidly expanded over the past several\ndecades. While the mechanics of creating a multi-track\nrecording are relatively straightforward, using the available\ntools to create professional quality work requires substantial\ntraining and experience. We address one of the most fun-\ndamental processes to creating a ï¬nished product, namely\ndetermining the relative gain levels of each track to produce\na ï¬nal, mixed song. By modeling the time-varying mixing\ncoefï¬cients with a linear dynamical system, we train mod-\nels that predict a weight vector for a given instrument using\nfeatures extracted from the audio content of all of the tracks.\n1. INTRODUCTION\nDigital audio production tools have revolutionized the way\nwe consume, produce and interact with music on a daily ba-\nsis. Consumers have the ability to create quality recordings\nin a home studio with a relatively limited amount of equip-\nment. Although there exists a myriad of complex software\nsuites and audio editing environments, they all perform the\nsame fundamental task of multi-track recording. This pa-\nper focuses on one of the most essential steps in music pro-\nduction: multi-track mixing. The relative levels between\nthe various instruments in a song signiï¬cantly determine the\noverall sonic quality of the piece.\nIn a previous paper we introduced a supervised machine\nlearning approach for automatically mixing a set of un-\nknown source tracks into a coherent, well-balanced instru-\nment mixture using a small number of acoustic features [1].\nWe modeled the mixing coefï¬cients as the hidden states of\na linear dynamical system and used acoustic features ex-\ntracted from the audio as the output of the model. After\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proï¬t or commercial advantage and that copies\nbear this notice and the full citation on the ï¬rst page.\nc/circlecopyrt2011 International Society for Music Information Retrieval.estimating the parameters of the model on the training data,\nwe predicted the time-varying weights of each instrument\nfor an unknown song using Kalman ï¬ltering [2].\nWe extend that approach in this paper by reducing the\nconstraints on the model and generalizing it to a larger num-\nber of instruments. One modiï¬cation to the system includes\nmodeling the weights of an individual instrument and their\nï¬rst and second derivatives instead of jointly estimating the\nweights for all of the instrument tracks at once. This re-\nmoves the restriction that the test song must contain all in-\nstrument types that the model was trained on.\nAdditionally, we explore an extended feature set within\nthis framework and analyze the performance of each indi-\nvidual feature as well as combinations of features. The fea-\ntures are chosen to contain information about the total en-\nergy of the signal, energy within various frequency bands,\nspectral shape and dynamic spectral evolution.\n2. BACKGROUND\nMuch research in the area of automatic audio signal mix-\ning is devoted to applications in the context of a live per-\nformance or event. Initial research on the subject was ori-\nented toward broadcast, live panel discussion and similar en-\nvironments dealing with the human voice as the primary au-\ndio source [3]. These systems analyze the amplitude of the\naudio signal and apply adaptive gating and thresholding to\neach input signal to create a coherent sound source mixture\nof the individual tracks in addition to preventing feedback.\nMore recent work incorporates perceptual features (e.g,\nloudness) into systems designed for live automatic gain con-\ntrol and cross-adaptive equalization [4, 5]. The implemen-\ntation of the former focuses on adapting the fader level of\neach channel with the goal of achieving the same average\nloudness per channel. The latter is designed for use in live\nsettings as a tool for inexperienced users or to reduce equip-\nment setup time. The system attempts to dynamically ï¬lter\nvarious frequency bands in each channel so that all channels\nare heard equally well.\nStructured audio is the representation of sound content\nwith semantic information or algorithmic models [6]. This\n621Oral Session 7: Structure Analysis and Mixing\nform of encoding allows for much higher data transmission\nrates as well as retrieval and manipulation of audio based\non perceptual models. Currently, professional music post-\nproduction is performed by a highly skilled engineer with\nyears of training. Using structured techniques, a parameter-\nized, generative version of this process that is applicable to\na variety of source audio is feasible.\nMore recent efforts focus on determining the parame-\nters used in common linear signal processing effects such\nas equalization and reverb as well as dynamic level com-\npression [7]. The authors also present a method for deter-\nmining static fader values for an entire song for each track\nin a multi-track recording session. An interface for assisting\nusers in creating mix-downs of user generated content from\nexamples of mixes produced by professional engineers is\npresented in [8].\nOther related work seeks to equalize an audio input based\non a set of descriptive perceptual terms such as bright or\nwarm [9]. Rather than attempt to navigate the complex net-\nwork of sliders and knobs in an audio interface, a user can\nspecify a high level term that describes the desired sound\nquality, and an appropriate equalization curve will be ap-\nplied. The system was developed through collecting user\nratings for audio examples and performing linear regres-\nsion to ï¬nd a weighting function for a particular instru-\nment/timbre pair.\n3. MODELING FRAMEWORK\nThe dataset we use in our experiments consists of 48 multi-\ntrack songs from the RockBandR/circlecopyrtvideo game. Each song\ncontains both mono and stereo tracks for a basic rock instru-\nmentation including guitar, bass, drums and vocals. Many\nsongs may also include keyboards, horns, percussion, back-\ning vocals, strings or other instruments. Often these backing\ninstruments are contained in one audio track, making model-\ning each instrument separately rather difï¬cult. To facilitate\ncomparison between the data of each song, we ï¬rst prepro-\ncess the tracks to obtain a set of ï¬ve instrument tracks â€“\nbass, drums, guitar, vocals and a backup track that contains\nall other instruments. A detailed explanation of this process\nis given in [1].\n3.1 Weight Estimation\nSince we do not have the DAW sessions used to create each\nsong, the actual fader values of the individual tracks are un-\nknown and must be estimated. To do this, the digital audio\noutput of the gaming console was recorded and aligned in a\nDAW session with the multi-track data of the corresponding\nsong. The spectrum of a frame of the output mix is assumed\nto be a linear combination of the individual input tracks ac-\ncording to\nÎ±1tU1t+Î±2tU2t+Â·Â·Â·+Î±ktUkt=Vt (1)\nFigure 1 . System diagram detailing the â€˜One Vs. Allâ€™\nmethod for mixing coefï¬cient prediction.\nwhereVtis the spectrum of the mixed track and U{1,...,k}t\nrepresents the spectra of the individual instrument tracks.\nWe vectorize the spectrogram of each frame and use non-\nnegative least squares (NNLS) to ï¬nd the mixing coefï¬-\ncients. We use NNLS as opposed to unconstrained least\nsquares estimation because multi-track mixing is an addi-\ntive process.\nThe noise in the weights is reduced through Kalman\nsmoothing [10]. It is signiï¬cant to note that while these co-\nefï¬cients produce a mix that is perceptually similar to the\noriginal track, they are not the actual ground truth weights.\nAudio examples of the original song and the reconstructed\nmix using the estimated weights are available online1.\n3.2 Weight Prediction\nWe use the weights estimated in Section 3.1 as labels in a\nsupervised machine learning task. We ï¬rst brieï¬‚y outline\nthe previous work we performed using this framework, then\nelaborate on a modiï¬ed version of the model.\nIn [1] we treat the Î±values as the hidden states of a linear\ndynamical system and our acoustic features as the output of\nthe system whose mathematical representation is\nÎ±t=AÎ±tâˆ’1+wt, (2)\nyt=CÎ±t+vt (3)\nThe dynamics matrix Acontrols the temporal evolution of\nthe hidden states and Cprojects the hidden states into our\nobservation space (feature domain). The driving and obser-\nvation noise sources, wtandvt, respectively are zero mean\nGaussian random variables with covariances QandR.\n1http://music.ece.drexel.edu/research/AutoMix\n62212th International Society for Music Information Retrieval Conference (ISMIR 2011)\nTrack All Tracks One Vs. All Best Features\nbackup 0.0126 0.0110 0.0087\nbass 0.0191 0.0163 0.0088\ndrums 0.1452 0.1283 0.0489\nguitar 0.0158 0.0151 0.0115\nvocal 0.0188 0.0160 0.0108\nTable 1 . Results for LOOCV on the database. The MSE\nfor each track across all songs is shown for the All Tracks\nmethod and the One Versus All approach.The Best Features\ncolumn is the result from sequential feature selection.\nOur state vector is the weights of each instrument at time\nstept\nÎ±t= [Î±1Î±2...Î± k]T(4)\nand the structure of the output vector is\nyt=/bracketleftBig\nF(1)\n1...F(1)\nmF(2)\n1...F(2)\nmF(k)\n1...F(k)\nm/bracketrightBigT\n(5)\nwherekindexes the instrument and mis the feature index.\nTo train the model we estimate AandCthrough con-\nstraint generation and least squares, respectively and com-\npute the covariances QandRfrom the residuals of Aand\nC[11]. In this framework, we are constrained in terms of\nthe number and type of instruments we can use the auto-\nmatic mixing system for. Since each Î±kis associated with\na speciï¬c instrument, omitting or adding tracks changes the\ndimension of the hidden state vector and in turn makes pre-\ndicting weights for a set of tracks that are not explicitly in\nthe form described in (4) and (5) intractable.\n3.3 Modiï¬ed Prediction Scheme\nInstead of modeling the time varying mixing coefï¬cients of\nall tracks as the hidden states of the LDS, we consider only\none instrument at a time. Our new state vector consists of the\nweight for the jth track and its ï¬rst and second derivatives\nÎ±t=/bracketleftbig\nÎ±jË™Î±jÂ¨Î±j/bracketrightbigT(6)\nThe derivatives of the weight vector are used to provide the\nmodel with more information about the dynamic evolution\nof the mixing coefï¬cients. Note that only the weights for\none instrument are included in the state vector. By elim-\ninating the weight values of the other instruments, we are\ntraining the model to consider only how well the current\ninstrument â€˜sitsâ€™ in the mix, not how the weights of all in-\nstruments evolve together.\nThe output vector ytis comprised of the feature set for\nthe instrument we are trying to predict stacked with the av-Feature Description\nRMS energy Root mean square energy\nSpectral ï¬‚ux Change in spectral energy\nSpectral bandwidth Range of frequencies where most energy lies\nOctave-based sub-bands Energy in octave spaced frequency bands\nMFCC Mel-Frequency Cepstral Coefï¬cients\nSpectral centroid Mean or center of gravity of the spectrum\nSpectral peaks Energy around a local sub-band maxima\nSpectral valleys Energy around a local sub-band minima\nSlope/Intercept Parameters of a line ï¬t to the spectrum of a frame\nTable 2 . Spectral and time domain features used in mixing\ncoefï¬cient prediction task.\nerage of the features from all other instruments\nyt=/bracketleftBig\nF(j)\n1...F(j)\nm1\nKâˆ’1/summationtextK\nk/negationslash=jF(k)\n2...1\nKâˆ’1/summationtextK\nk/negationslash=jF(k)\nm/bracketrightBigT\n(7)\nIfj= 1, then we are using mfeatures associated with\nthe ï¬rst track and averaging the features associated with the\ntracksk/negationslash=j, reducing the dimensionality of the feature vec-\ntor fromkmto2m. Comparing (5) to (7), we observe that in\n(7) there is no dependency on which position ( k) the features\nfor a given instrument are located. The only prior knowl-\nedge the model requires is the type of the jth instrument for\nwhich we are predicting time-varying weights. As a result,\nin this framework there is no limitation on the number or\ntype of instruments that can be mixed using the system, pro-\nvided that there exists training data for the target instrument\nj. A system diagram showing the new modeling method is\nshown in Figure 1.\nTo evaluate the efï¬cacy of this modiï¬ed estimation ap-\nproach, we perform the same experiment outlined in [1] and\ncompare the results of the two methods. Using the 48 songs\nin our dataset, we perform leave-one-out cross-validation\n(LOOCV), training an LDS on 47 tracks and predicting the\nweights for the remaining track. We repeat the process us-\ning each track as a test song only once and average the mean\nsquared error (MSE) between our estimated ground truth\nvalues and our predictions from the LDS. The results are\nshown in Table 1. We refer to the method described in Sec-\ntion 3.2 as All Tracks (AT) and the modiï¬ed approach in this\nsection as One Versus All (OV A). The OV A results are are\ncomputed using the same feature set {centroid, RMS, slope,\nintercept}that was used in the previous experiment [1].\nThe table shows an average improvement of 11.66% in\nterms of MSE for all instrument types in the dataset. The\nOV A method provides increased performance in terms of\nthe MSE of the weight predictions as well as increased ï¬‚ex-\nibility. The new topology enables the system to mix songs\nthat do not have the same number of tracks as the normal-\nized RockBand dataset we compiled.\n623Oral Session 7: Structure Analysis and Mixing\nBackup Bass Drums Guitar V ocal\nFeature Error Feature Error Feature Error Feature Error Feature Error\nBandwidth 0.0511 Flux 0.0590 Centroid 0.7322 Bandwidth 0.0756 Flux 0.1183\nFlux 0.0526 Bandwidth 0.0590 RMS 0.8415 Valley 0.0878 Centroid 0.1240\nSub-Bands 0.0580 Slope 0.0618 Slope 0.8713 Intercept 0.0908 Bandwidth 0.1251\nIntercept 0.0587 Intercept 0.0622 Bandwidth 0.8861 Slope 0.0920 Valley 0.1262\nSlope 0.0589 RMS 0.0716 Intercept 0.8932 Flux 0.0936 Peak 0.1302\nPeak 0.0607 Valley 0.0741 Peak 0.9260 Sub-Bands 0.0974 Intercept 0.1316\nRMS 0.0629 Sub-Bands 0.0743 Valley 0.9381 RMS 0.0987 Sub-Bands 0.1317\nCentroid 0.0636 Peak 0.0752 Sub-Bands 0.9649 Peak 0.1019 Slope 0.1318\nMFCC 0.0659 Centroid 0.0801 MFCC 1.1785 Centroid 0.1095 RMS 0.1320\nValley 0.0680 MFCC 0.0821 Flux 3.5767 MFCC 0.1127 MFCC 0.1373\nTable 3 . Mean squared error for all features and individual instruments. Features for each instrument are listed in order of best\nperformance to worst performance. The best combination of features for each instrument is in boldface.\n4. FEATURE ANALYSIS\nHaving shown that the OV A method outperforms the AT\nmethod, we proceed to investigate which features are the\nmost informative. We explore an extended feature set within\nthe framework described in the previous section and ana-\nlyze the performance of each individual feature as well as\n0.890.93BackupMSEx10âˆ’3\n0.961.20BassMSE\n4.995.29DrumsMSE\n1.211.40GuitarMSEError Performance for Stacked Features\n123456781.081.11VocalMSE\nNumber of Features\nFigure 2 . MSE versus the number of stacked features used\nin training an LDS for each track. Note that the scale of each\nsub-plot varies. The minimum is indicated for each track.combinations of features. Table 2 lists the array of spec-\ntral and time domain features we selected for our experi-\nment [12â€“14]. The features are chosen to contain informa-\ntion about the total energy of the signal, energy within var-\nious frequency bands, spectral shape and dynamic spectral\nevolution. All experiments are performed using LOOCV on\nthe entire dataset. In the ï¬rst experiment, we test the per-\nformance of each individual feature using the average MSE\nover all songs as our error metric. Table 3 shows the results\nfor each feature for each track type in the dataset. There\nis no single feature that appears to be dominant for mixing\ncoefï¬cient prediction.\nUsing these results, we employ sequential feature se-\nlection to increase the performance of our system [15].\nThe best performing feature for each instrument in Table\n3 is stacked with each remaining feature, and the MSE for\nLOOCV is computed for each combination. The best fea-\nture from this result is retained and the process is repeated\nuntil all features have been used. The results of this analy-\nsis are depicted in Figure 2. The best performing number of\nfeatures for each instrument is indicated with a diamond.\nSince some of our features may contain similar informa-\ntion, adding additional features eventually becomes redun-\ndant and the increase in the size of the parameter space out-\nweighs the gain in information.\n5. RESULTS\nThe overall results for using the best performing feature en-\nsemble are detailed in Table 1. The table shows that the\nOV A approach more accurately models the mixing coefï¬-\ncients and the addition of more features greatly improves the\nresults. Mean squared error does not provide any intuition\nabout where each model fails or performs well. Figure 3\nshows a comparison between the AT and OV A models. Both\n62412th International Society for Music Information Retrieval Conference (ISMIR 2011)\n012AmplitudeBackup\n012AmplitudeBass\n012AmplitudeDrums\n012AmplitudeGuitar\n0 200 400 600 800 1000 1200012AmplitudeVocal\nFrame\n012AmplitudeBackup\n012AmplitudeBass\n012AmplitudeDrums\n012AmplitudeGuitar\n01002003004005006007008009001000012AmplitudeVocal\nFrameFigure 3 . Comparison of ground truth (black) values with AT (gray) and OV A (orange) models. Left: â€˜More Than A Feelingâ€™\nby Boston. Right: â€˜Hammerheadâ€™ by The Offspring.\nmodels were trained with the feature set used in [1]. There is\nrelatively small deviation in the bass and guitar predictions\nfor each method on both songs. The most signiï¬cant differ-\nence is in the ability of the OV A model to track the vocal\nweights as evidenced by the relatively ï¬‚at predictions from\nthe AT model contrasted with the OV A model predictions\nthat follow the contour of the ground truth weights.\nIn Figure 4 we observe the effect of increasing the num-\nber of features used to train the model. The predictions us-\ning the best feature for each instrument from Table 3 are\nshown in gray and the highest performing ensemble of fea-\ntures is depicted in orange. Adding features creates the most\nimprovement in the drum track where the contour and bias\nof the predictions closely follows the ground truth for both\nsongs. Although this is only a small sample of the dataset,\nthis representation informs us of improvements that can be\nmade to the system.\n6. CONCLUSION\nOur automatic multi-track mixing system predicts a set of\nweighting coefï¬cients for an instrument given an ensem-\nble of acoustic features extracted from audio content. Weimprove upon our previous modeling framework by train-\ning a separate LDS for each instrument rather than model-\ning all weight vectors within a single system. Applying the\nOne Versus All method of training removes the restrictions\nimposed by the All Tracks model and yields better perfor-\nmance in predicting the weights for all instruments.\nMoreover, we investigate the accuracy of an array of\nspectral and time-domain features on predicting the mixing\ncoefï¬cients. The improved modeling scheme and feature\nensemble chosen through sequential feature selection illus-\ntrate marked improvement over our previous results. While\nthis approach to automatic multi-track mixing works well\nfor our small dataset, in the future we plan to develop a\nlarger and more varied corpus of songs to explore how ro-\nbust the model is.\n7. ACKNOWLEDGMENT\nThis work is supported by National Science Foundation\naward IIS-0644151.\n625Oral Session 7: Structure Analysis and Mixing\n012AmplitudeBackup\n012AmplitudeBass\n012AmplitudeDrums\n012AmplitudeGuitar\n0 200 400 600 800 1000 1200012AmplitudeVocal\nFrame\n012AmplitudeBackup\n012AmplitudeBass\n012AmplitudeDrums\n012AmplitudeGuitar\n01002003004005006007008009001000012AmplitudeVocal\nFrameFigure 4 . Comparison of ground truth (black) values with OV A model using the single best feature (gray) and using the best\ncombination of features (orange). Left: â€˜More Than A Feelingâ€™ by Boston. Right: â€˜Hammerheadâ€™ by The Offspring.\n8. REFERENCES\n[1] J. Scott, M. Prockup, E. M. Schmidt, and Y . E. Kim, â€œAuto-\nmatic multi-track mixing using linear dynamical systems,â€ in\nProceedings of the 8th Sound and Music Computing Confer-\nence, Padova, Italy, 2011.\n[2] E. M. Schmidt and Y . E. Kim, â€œPrediction of time-varying mu-\nsical mood distributions using kalman ï¬ltering,â€ in Proceed-\nings of the 2010 IEEE International Conference on Machine\nLearning and Applications , Washington D. C., USA, 2010.\n[3] D. Dugan, â€œAutomatic microphone mixing,â€ J. Audio Eng. Soc ,\nvol. 23, no. 6, pp. 442â€“449, 1975.\n[4] E. Perez Gonzalez and J. D. Reiss, â€œAutomatic gain and fader\ncontrol for live mixing,â€ in IEEE Workshop on Applications of\nSignal Processing to Audio and Acoustics , 2009, pp. 1â€“4.\n[5] â€”â€”, â€œAutomatic equalization of multichannel audio using\ncross-adaptive methods,â€ in 127th AES Convention , 2009.\n[6] B. Vercoe, W. Gardner, and E. Scheirer, â€œStructured audio:\nCreation, transmission, and rendering of parametric sound rep-\nresentations,â€ in Proceedings of the IEEE , 1998, pp. 922â€“940.\n[7] D. Barchiesi and J. Reiss, â€œReverse engineering of a mix,â€\nJournal of the Audio Engineering Society , vol. 58, no. 7, pp.\n563â€“576, 2010.\n[8] H. Katayose, A. Yatsui, and M. Goto, â€œA mix-down assistant\ninterface with reuse of examples,â€ in First International Con-\nference on Automated Production of Cross Media Content for\nMulti-Channel Distribution , Florence, Italy, 2005.[9] A. T. Sabin and B. Pardo, â€œA method for rapid personalization\nof audio equalization parameters,â€ Proceedings of ACM Multi-\nmedia , pp. 769â€“772, 2009.\n[10] R. E. Kalman, â€œA new approach to linear ï¬ltering and predic-\ntion problems,â€ Journal of basic Engineering , vol. 82, no. 1,\npp. 35â€“45, 1960.\n[11] S. Siddiqi, B. Boots, and G. Gordon, â€œA constraint generation\napproach to learning stable linear dynamical systems,â€ in Ad-\nvances in Neural Information Processing Systems 20 . Cam-\nbridge, MA: MIT Press, 2008, pp. 1329â€“1336.\n[12] D.-N. Jiang, L. Lu, H.-J. Zhang, J.-H. Tao, and L.-H. Cai, â€œMu-\nsic type classiï¬cation by spectral contrast feature,â€ in Proceed-\nings of the IEEE International Conference on Multimedia and\nExpo (ICME) , Lusanne, Switzerland, 2002, pp. 113â€“116.\n[13] S. Davis and P. Mermelstein, â€œComparison of parametric repre-\nsentations for monosyllabic word recognition in continuously\nspoken sentences,â€ IEEE Transactions on Acoustics, Speech\nand Signal Processing , vol. 28, no. 4, pp. 357 â€“ 366, aug 1980.\n[14] G. Tzanetakis and P. Cook, â€œMusical genre classiï¬cation of\naudio signals,â€ IEEE Transactions on Speech and Audio Pro-\ncessing , vol. 10, no. 5, pp. 293 â€“ 302, jul 2002.\n[15] L. Mion and G. D. Poli, â€œScore-independent audio features for\ndescription of music expression,â€ IEEE Transactions on Audio,\nSpeech & Language Processing , vol. 16, no. 2, pp. 458â€“466,\n2008.\n626"
    },
    {
        "title": "Modeling Melodic Improvisation in Turkish Folk Music Using Variable-Length Markov Models.",
        "author": [
            "Sertan SentÃ¼rk",
            "Parag Chordia"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1415570",
        "url": "https://doi.org/10.5281/zenodo.1415570",
        "ee": "https://zenodo.org/records/1415570/files/SenturkC11.pdf",
        "abstract": "The paper describes a new database, which currently consists of 64 songs encompassing approximately 6600 notes, and a system, which uses Variable-Length Markov Models (VLMM) to predict the melodies in the uzun hava (long tune) form, a melodic structure in Turkish folk music. The work shows VLMMs are highly predictive. This suggests that variable-length Markov models (VLMMs) may be applied to makam-based and non-metered musical forms, in addition to Western musical traditions. To the best of our knowledge, the work presents the first symbolic, machine readable database of uzun havas and the first application of predictive modeling in Turkish folk music.",
        "zenodo_id": 1415570,
        "dblp_key": "conf/ismir/SenturkC11",
        "keywords": [
            "database",
            "Variable-Length Markov Models (VLMM)",
            "melodies",
            "Turkish folk music",
            "makam-based",
            "non-metered musical forms",
            "Western musical traditions",
            "symbolic",
            "machine readable database",
            "first application"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nMODELING MELODIC IMPROVISATION IN TURKISH FOLK MUSIC\nUSING V ARIABLE-LENGTH MARKOV MODELS\nSertan S Â¸ent Â¨urk\nGeorgia Tech Center for Music Technology\nAtlanta, GA, USA\nsertansenturk@gatech.eduParag Chordia\nGeorgia Tech Center for Music Technology\nAtlanta, GA, USA\nppc@gatech.edu\nABSTRACT\nThe paper describes a new database, which currently con-\nsists of 64 songs encompassing approximately 6600 notes,\nand a system, which uses Variable-Length Markov Mod-\nels (VLMM) to predict the melodies in the uzun hava (long\ntune) form, a melodic structure in Turkish folk music. The\nwork shows VLMMs are highly predictive. This suggests\nthat variable-length Markov models (VLMMs) may be ap-\nplied to makam -based and non-metered musical forms, in\naddition to Western musical traditions. To the best of our\nknowledge, the work presents the ï¬rst symbolic, machine\nreadable database of uzun hava s and the ï¬rst application of\npredictive modeling in Turkish folk music.\n1. INTRODUCTION AND MOTIV ATIONS\nTo date, most computational research in music has focused\non Western music. In order to further advance the state-\nof-the-art in MIR, non-Western musics, with their unique\nchallenges should be considered [16]. Such research would\nexpand our knowledge and tools immensely, allowing us\nto adapt and improve the previous work, and would open\nup new paths for musical creativity, expressivity and inter-\naction. Computational modeling of distinct musical gen-\nres will deepen our knowledge of universal versus genre-\nspeciï¬c aspects of music and it will allow us to truly evalu-\nate the generality of various modeling strategies.\nMusical improvisation is a complex phenomenon, and\nthere have been many attempts to describe and model it\n[24]. Moreover, there is a lack of understanding the â€œmu-\nsicâ€ in the current MIR research with respect to how humans\nactually perceive the it [27]. Previous work on Western\nmelodies showed that variable-length n-gram models and\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proï¬t or commercial advantage and that copies\nbear this notice and the full citation on the ï¬rst page.\nc/circlecopyrt2011 International Society for Music Information Retrieval.human judgments of melodic continuation are highly cor-\nrelated [20]. We hope our research will give clues about\nhow we actually anticipate music [10].\n2. BACKGROUND\n2.1 Related Work\nComputational modeling of musical styles is not a new topic,\nand is a common tool in algorithmic composition [2, 7]. n-\ngram modeling have been extensively used in algorithmic\ncomposition [19], structure analysis [12], and music cogni-\ntion [21]. This work is an adaptation of our expressive tabla\nmodeling research [4], which is based on multiple viewpoint\nmodeling [6].\nAlthough information retrieval in world musics has only\nrecently started to attract attention in academia, there has\nbeen substantial amount of research in the ï¬eld [3,8,13,26].\nIn traditional Turkish music, n-gram modeling have been\npreviously used by Alpkoc Â¸ak and Gedik to classify makam s\n[1].\n2.2 Turkish Folk Music\nTurkish folk music is a profound music style that is the prod-\nuct of the emotions, thoughts, humor and social life of Turk-\nish people, and has been shaped by the historical events, ge-\nographical locations and migrations of the Turkish people.\nThe songs in Turkish folk music are typically anonymous,\nwhich have been carried from generation to generation as\nan oral tradition.\n2.2.1 Basic Concepts in Turkish Music Theory\nIn Western music, an octave is divided into 12 intervals.\nHowever, there is no theory that is completely agreed upon\nin Turkish music due to differences in theory and practice;\nthe suggested number of pitches in an octave ranges from\n17 to 79 [28]. Currently, education in makam based mu-\nsic is based on the-highly-criticized [25] Arel-Ezgi-Uzdilek\ntheory. According to the theory, a whole tone is divided\ninto intervals named koma s, which are used to discretize an\noctave into 24 consequent tones [18]. However, in Turkish\n269Poster Session 2\nFret # Note Fret # Note Fret # Note\n0A 6C/sharp 12F/sharp3\n1B/flat 7D 13F/sharp\n2B/flat28E/flat 14 G\n3B 9E/flat215A/flat\n4C 10 E 16A/flat2\n5C/sharp311 F 17 A\nTable 1 : The notes and the fret numbers in the lowest string\ngroup of ba Ë˜glama in the baË˜glama tuning ./flat2and/sharp3â€™s indicate\nthe quarter tones.\nfolk music, there are typically 17 pitches played in an octave\ndue to selection of the instruments (Section 2.2.2), and the\nmusic is indeed explainable by makam s [25].\nMakam s can be depicted as the modes of Turkish music.\nMakam s are progressions ( seyir ) used to generate melodies\n(naË˜gme).Makam s obey certain rules such as emphasizing\nthe modal centers, the use of key signatures and maintaining\ncontext-speciï¬c ascending or descending seyirs [25].\nUsul is â€the structure of musical events which are coher-\nent with respect to time.â€ Usul can be roughly translated as\nâ€meter.â€ An usul can be as short as two beats or as long\nas 128 beats, but it should always have at least one strong\nand one weak beat. Turkish music also makes a rich use of\nusuls Â¨uz(non-metered) progressions [18].\n2.2.2 Uzun Hava\nUzun hava (long tune) is a semi-improvisational melodic\nstructure in Turkish folk music. The music is usually sad;\nthe lyrics (if any) are generally about the daily struggles\nand emotions of the Anatolian people. All uzun hava s are\nusuls Â¨uz(without any meter), but there can also be usull Â¨u\n(with distinct meter) sections in between.\nThe most common instrument played in uzun hava s is\nbaË˜glama, a traditional Turkish instrument from lute family.\nIt has 17 notes in an octave [25] (Table 1). The strings are\ngrouped into three having 3, 2 and 2 strings from the highest\nto the lowest. The instrument is a transposing instrument,\nand the tuning ( dÂ¨uzen) of these strings may change for dif-\nferent songs. Moreover, the frets are tied to the fretboard\n(sap), so that microtonal adjustments in the temperament\ncan be easily made.\n3. UZUN HA V A HUMDRUM DATABASE\nFor the experiments, the authors, with the help of Prof. Erdal\nTuË˜gcular, have built the Uzun Hava Humdrum Database1.\nA Humdrum based syntax called **kern format was chosen\nfor its readability and broad search, comparison and editing\n1The Uzun Hava Humdrum database is available online at\nhttp://sertansenturk.com/uploads/uzunHavaHumdrumDatabasecapabilities [9]. In order to obtain the symbolic data, all of\ntheuzun hava s with scores (a total of 123 scores) from The\nTurkish Radio and Television Corporationâ€™s (TRT) Turkish\nFolk Music Database were chosen2. The TRT database\nconsists of the extended Western staff notations saved in .tiff\nimage format.\nIn the analysis of world musics, there are some intrinsic\nproblems of using symbolic notation such as accepting no-\ntation as an adequate means of representing improvisation\n(especially in oral traditions) and human errors in the tran-\nscriptions [17]. Therefore, it might be problematic to make\ndeductions based on symbolic notations, and audio analysis\nmight be more appropriate. Yet, audio analysis is gener-\nally not as easy and straightforward as processing symbolic\ndata. Thus, it is more suitable take the initial steps in com-\nputational modeling with human annotations, even if they\nare not perfect.\nThe scores in the TRT database were read into Finale\n2010 by using the built-in SmartScore 5 Lite, exported into\nMusicXML 2.0 format, and then converted to **kern nota-\ntion by using xml2hum [23]. After cleaning-up, grace notes,\nfermatas, quarter tone accidentals and meter changes were\nadded to the **kern ï¬les. In order to comply with the stan-\ndard humdrum notation, instead of creating our own sym-\nbols, we have chosen to indicate the quarter tones as devia-\ntions in cents in a second spine. In the TRT database, there\nare accidentals, which have different koma deviations from\nthe same tone ( B/flat2,B/flat3,B/flat4etc.) However, as the most\ncommon instrument played in uzun havas is ba Ë˜glama and it\nhas 17 notes per octave, we have chosen to map all koma\nvalues lying between semitones into a single quarter tone\nwith 50 cent deviation from the original note and match the\n17-tone scale.\nUsuls Â¨uz(non-metered) sections in uzun havas are treated\nas cadenzas such that the sections start with â€*MX/Xâ€, in-\ndicating the following notes will be played in a non-metered\nfashion and each note is proceeded by the letter â€Qâ€, which\nis used to indicate grupettos in **kern format [9]. Finally,\nthe name, region, makam , accidentals and usul are printed\nto the start of the ï¬le as comments.\nCurrently 64 songs have been encoded, with a total of\n6613 notes, in 8 makam s from different regions of Anatolia\nand Azerbaijan. However, we should note that the makam s\nof the songs are biased towards H Â¨useyni (82 songs) and Hi-\ncaz (17 songs). This is expected as uzun hava s are usually\nplayed in H Â¨useyni [11].\n2The TRT Turkish folk music database is available on-\nline at â€ TÂ¨urk M Â¨uzik K Â¨ultÂ¨urÂ¨unÂ¨un HafÄ±zasÄ±â€ Score Archive\n(http://www.sanatmuziginotalari.com/), which is freely accessible via\nhttp://devletkorosu.com/\n27012th International Society for Music Information Retrieval Conference (ISMIR 2011)\n4. COMPUTATIONAL MODELING\nParallel to Conklin and Pearceâ€™s research [6,20], the compu-\ntational framework in this work incorporates multiple view-\npoints modeling with both long-term and short-term mod-\nels. Variable-length Markov modeling (VLMM) is used to\nmodel the sequences, and the training data is stored as Pre-\ndiction Sufï¬x Trees. The evaluation of the system is done\nby entropy-based calculations. The modeling and evalua-\ntion framework was implemented in C++ as an external ob-\nject in Max/MSP along with supporting patches [4]. To the\nbest knowledge, this research is the ï¬rst attempt to model\nmelodic sequences in traditional Turkish music.\n4.1 Markov Modeling\nAnthorder Markov model is a causal, discrete random pro-\ncess where the probabilities of the next state depends only\non the probabilities of the current and the previous states.\nIf the sequences are directly observable, i.e. the states are\nvisible, most of the problems can be directly solved by deal-\ning with transition probabilities. A (nâˆ’1)thMarkov model\ncan be represented by n-grams, which are subsequences of\nlength n.n-grams are a commonly used to probabilistically\nmodel sequences of elements such as phonemes in speech,\nletters in a word, or musical notes in a phrase [15].\nIncreasing the order of the Markov model might reveal\nmore details about the data stream. However, speciï¬c pat-\nterns will get extremely uncommon as the order of the model\ngets higher, even with very big data sets. Moreover, while\nobserving speciï¬c patterns is very helpful, integrating lower\norder models to the system might also be useful to give some\nregularity. In order to capture the generality of lower or-\nder models and speciï¬city of the sequences in higher order\nmodels, we can use an ensemble of Markov models with\ndifferent orders to form a variable length Markov model\n(VLMM). The variable length of memory in contrast with\nï¬xed Markov model yields a rich and ï¬‚exible description\nof sequential data. In this work, to combine the predictions\nfrom different orders, we are using a smoothing method we\ntermed 1/N. In the 1/Nsmoothing method, weights for\nthe n-th order model are given by1\n(maxOrder âˆ’n+1), giv-\ning greater relative weight to predictions of higher orders.\nMoreover, the VLMMs are efï¬ciently stored in Prediction\nSufï¬x Trees [22] (PSTs) for performance reasons (Figure\n1).\nWhile increasing order would allow us to obtain more\nspeciï¬c patterns, it also brings the so-called zero frequency\nproblem [5]. As the order nincreases, the maximum num-\nber of possible n-grams would increase to nk, where k is\nthe number of the possible symbols. However, even in large\ndatabases, most of the sequences will not be present or seen\nvery few. This sparsity issue brings a limitation to the or-\nder of an n-gram. In order to deal with the zero frequencyViewpoint Explanation\nDuration Duration of the note\nNote Midi number corresponding to the\nnote\nNoteWCents Viewpoint denoting the â€trueâ€ sym-\nbol in Turkish folk music in note\nand cent deviation, i.e. the ï¬‚oating\nmidi number\nNoteâŠ—Dur Cross type combining note and du-\nration\nNoteWCentsâŠ—Dur Cross type combining note with\ncent deviation and duration\nTable 2 : Viewpoints used in the experiment.\nproblem, an escape probability for each level of the trie is\nreserved. The escape probability of each level is calculated\nase(n) =T1(n)\nN(n), where T1is the number of symbols that\nhave occurred exactly once and Nis the total number of ob-\nservations so far. When an event, which has never occurred\nbefore, is observed, the escape probability is returned in-\nstead of 0.\n4.2 Multiple Viewpoints\nA multiple viewpoints system [6,20] separates a musical se-\nquence to independent parallel representations such as pitch,\nrhythm, instrument, key changes. The next sequence is pre-\ndicted based on the information incorporated from these view-\npoints. Denoting music in multiple representations can be\nuseful to predict the next symbol when one of the repre-\nsentations might be suitable for that particular sequence,\nwhereas another representation is useful in other situations.\nAs an example, scale degree would be very useful if all the\nmusical context is in the same key, however melodic interval\nmight prove more suitable if the predictions are required in\na transposed key. There can also be cross-type viewpoints,\nwhich are generated by mapping the symbols in two or more\nof the parallel representations into unique tokens: for exam-\nple for NotesâŠ—Durations; a quarter C, a quarter D, a eighth\nC will all be mapped to different symbols. We use 5 view-\npoints in our experiment: Durations, Notes, NoteWCents,\nNoteâŠ—Dur and NoteWCents âŠ—Dur (Table 2).\nA common limitation of training the predictive models\nover large amount of data is that it renders the model too\ngeneral to effectively predict patterns specic to the current\nsong: if the song has a peculiar phrasing repeated through-\nout, due to the phrase having a small probability in the train-\ning database, the patterns generated might be irrelevant. In\norder to obtain predictions which are trained over a partic-\nular style and also sounds like a speciï¬c song, we use a\nlong-term-model (LTM) built on the entire training set and\n271Poster Session 2\n(a) Ending of U0368\n.152\nC\n.152\nD\n.466\nD\n12\nD\n.51\nC\n12\nBâ™­\n11\nBâ™­2\n2\nE\n11\nR\n.51\n.21\nBâ™­22\nC1\nE\nD\n11\nD\n.51\n.4 .2\nR\n.51\nD\n11\nE\n.51\nD\n.51E\n.152\n2\nD\n1\nR\n.081\n1\nE\n1\nD\n11\nD\n.21\n(b) Prediction Sufï¬x Tree\nFigure 1 : The ending of U0368 with the repeat sign taken out and the Prediction Sufï¬x Tree representing the Markov models\nof Notes-with-Cents viewpoint with a maximum order of 2, trained on these two measures. Bubbles on the top right and bottom\nright of each node denotes the count and the probability of the node respectively.\na short-term-model (STM), which is trained on the current\nsong that is being evaluated. Only symbols up to the current\ntime are used in the STM; looking ahead is not permitted\nwhen making a prediction.\nWhen a prediction is to be made at a given time-step,\nthe LTM and STM are combined and normalized to a single\npredictive distribution for each of the viewpoints. Given the\nsymbols, S={s1, s2, ..., s N}forming the probability dis-\ntribution, the probability distribution is weighted inversely\nproportional to the entropy [6]. The weight of the proba-\nbility distribution of a model is given as Ï‰m/defineslog2(N)\nHm,\nand the entropy of the probability distribution of each model\nis deï¬ned as Hm/definesâˆ’/summationtextN\nk=1Pm(sk) log2(Pm(sk)), where\nPm(sk)is the probability of the symbol, sk, at the time step,\nt.\n5. EV ALUATION\nLeave-one-out cross-validation was performed on each of\nthe 64 songs in the Uzun Hava Humdrum database . During\nthe experiment, each song is picked as the testing data, and\nLTM is trained over the other songs. STM is built while\nthe testing data is fed to the system. At each time step t,\nthe true symbol is noted. Then the predictions carried in the\nprevious step tâˆ’1are checked, and, ptthe probability of the\ntrue symbol at tis recorded. From the probabilities, cross-\nentropy [14] is calculated at the song level and through all\nexperiments.Cross-entropy is a common domain-independent approach\nused for evaluating the quality of model predictions, and it is\npreferable to symbol recognition rate in predictive systems\n[6, 20]. It is deï¬ned as: Hc=âˆ’/summationtextn\nt=1ptlog2(pt)3. If the\nprobability distribution pis unknown, under the assumption\nof uniform probability distribution ( pt=1\nn, where nis the\nnumber of predictions throughout the experiment), cross-\nentropy can be approximated by Hcâ‰ˆâˆ’1\nn/summationtextn\nt=1log2(pt).\nLater cross-entropy is converted to average perplexity, which\nis a measure of the number of choices that the model has\npicked the true symbol [14]. Perplexity is deï¬ned as P=\n2H. We also report median perplexity in addition to average\nperplexity. The prior probabilities of the symbols are used to\nobtain a baseline for evaluating perplexity results. In other\nwords, perplexity of the 0thorder model LTM is used as the\nbaseline.\n6. RESULTS\nDuring the experiments, average and median perplexities\nover the whole dataset and in the song-level are recorded for\nSTM, LTM and combined models with different orders4.\nTable 3 shows that for order 14, STM always gives the most\n3Notice that the deï¬nition of cross-entropy is very similar to the entropy\ndeï¬niton in Section 4.2. However, entropy is calculated from the proba-\nbilities of each possible symbol at a given time, whereas cross-entropy is\ncalculated from the chosen predictions at each time step.\n4The complete set of results and signiï¬cance tests is available at\nhttp://sertansenturk.com/uploads/publications/senturk2011UzunHava\n27212th International Society for Music Information Retrieval Conference (ISMIR 2011)\n0124 9 14 19 24 292.533.544.555.566.5\nOrderPerplexity\n  \nSTMCombinedLTMPriorsFigure 2 : Average perplexity for duration prediction using\nLTM, STM and combined models for orders 0-29\nconï¬dent results, while combining STM with LTM does not\nactually help predictions. STM has an average perplexity\nof 2.96, 4.13, 4.16, 6.68 and 6.69 for Duration, Note and\nNoteWCents, Note âŠ—Dur and NoteWCents âŠ—Dur respectively.\nComparing to the average baseline perplexities (6.12, 11.9,\n12.71, 171.76, 148.39), there is a remarkable decrease. The\npower of STM is even more obvious in the cross types Note âŠ—Dur\nand NoteWCentsâŠ—Dur, where the LTM gives perplexities of\n30.17 and 31.84.\nAnother interesting remark is adding the cent informa-\ntion during prediction results in a slight increase in per-\nplexity. For the 14thorder model, the average perplexi-\nties in the STM are 4.13, 4.16 for Note and NoteWCents,\nand 6.68, 6.69 for Note âŠ—Dur and NoteWCents âŠ—Dur respec-\ntively, meaning the system can effectively predict notes with\nquarter tone accidentals.\nFigure 2 shows that the perplexity decreases monoton-\nically with increasing order, as expected. STM gives the\nlowest perplexities in every order. It is also seen that there\nis only a slight change in perplexity after order 14, therefore\nchecking back more than 14 durations is unnecessary. This\noptimum order is true for all of the viewpoints.\nWhen the average perplexities are checked song by song,\nit was observed that some songs had exceptionally higher\nperplexities for LTM and Combined models. Upon inspect-\ning, it was observed that the system was not able to predict\nthe notes properly in the songs with makam s which are only\nrepresented with a few songs. Similarly the songs which\nincluded a lot of triplets, double dotted, 64thnotes were\nharder to predict. On the other hand, the latter problem\nalso affected STM, because the Duration viewpoint in these\nsongs presented a vast symbol space and thus smaller prior\nvalues, rendering the next symbol harder to predict.7. DISCUSSION\nThe results suggest that uzun hava form can be effectively\nmodeled using VLMMs. Between the perplexities obtained\nfrom the LTM, STM and Combined models with a maxi-\nmum order of 14 and viewpoint, there is a signiï¬cant5de-\ncrease in confusion, and STM outperforms both LTM and\nthe Combined model. The success of STM over LTM sug-\ngests the songs have strong local patterns. Strong patterns\nare easy to be captured and predicted by the STM; how-\never being a more general model, LTM cannot capture and\nprioritize song-related patterns as good as STM. This result\nwas expected, because in uzun hava s note and the duration\nrepetitions commonly occur during the improvised part and\nmelodies are generally repeated during vocal sections.\nOne of the most important observations is that extend-\ning the possibilities in target pitches from Western music\nto Turkish music only slightly increases perplexity values.\nWhen the quarter tones are included, i.e. the symbol in-\ndicating both the quarter tone and the neighboring tone is\ndecoupled to create two unique symbols, almost all of the\ncounts accumulate on the one note. Additionally, by in-\nspecting perplexities note-by-note, it is easily seen that the\nquarter tones such as B/flat2andF/sharp3, are easily distinguished\nfrom their neighbor tones, i.e. B/flatandF/sharp. This shows\nthat transcriptions strictly obey the key signature of their\nmakam s, and multiple viewpoint system is able to model\nthe context-speciï¬c pitches in makam s, and distinguish the\nnotes from the neighboring notes present in Western music\nvirtually without any penalty. Indeed, the selection of mul-\ntiple viewpoints might be crucial for the success. For ex-\nample, the cent deviation information cannot be used with-\nout crossing pitch related viewpoints such as absolute note\nor scale degree. For a generative system, decoupling them\nmight still give good average perplexities, however when the\nnote and the cent deviation are predicted independently from\neach other, the results might introduce notes with wrong ac-\ncidentals, disrupting the melodic intervals and the makam\nstructure.\nIn future work, we would like to include more viewpoints\nincorporating fermata, usul, scale degree, melodic interval,\ncontour and try crossing these viewpoints, to obtain better\nperplexities in prediction. We would also like to generate\nMedium Term Models (MTM), each one of which will be\ntrained on a single makam . Testing will be carried with the\nMTM of the same makam . Using this approach, we hope to\nï¬nd and predict makam based patterns with better perplex-\nity. Also, as mentioned in Section 3, extending the frame-\nwork to variable-length hidden Markov Models (VLHMMs)\nfor audio analysis is a necessary step for a more relevant as-\nsessment of uzun hava s.\n5The claim means, it is statistically signiï¬cant at the 0.01 level as deter-\nmined by a multiple comparison test using the Tukey-Cramer statistic.\n273Poster Session 2\nDuration Note NoteWCents Note âŠ—Dur NoteWCents âŠ—Dur\nAverage Median Average Median Average Median Average Median Average Median\nPriors 6.12 3.76 11.9 7.97 12.71 7.98 171.76 171.32 148.39 148.16\nLTM 3.88 2.23 5.56 4.13 5.87 4.21 30.17 20.06 31.84 21.21\nCombined 3.55 1.93 4.64 3.17 4.70 3.21 15.67 10.40 16.23 10.68\nSTM 2.96 1.94 4.13 2.96 4.16 3.00 6.68 5.30 6.69 5.30\nTable 3 : Average and median perplexities for Duration, Note, NoteWCents, Note âŠ—Dur and NoteWCents âŠ—Dur for order 14\n8. ACKNOWLEDGEMENTS\nWe would like to thank Turkish Radio and Television Cor-\nporation, TÂ¨urk M Â¨uzik K Â¨ultÂ¨urÂ¨unÂ¨un HafÄ±zasÄ± Score Archive and\nthe numerous musicians, transcribers and archivers for their\nefforts in building the TRT Turkish folk music database which\nmade this research possible. We would also like to thank\nProf. Erdal Tu Ë˜gcular for his invaluable contributions to the\nUzun Hava Humdrum Database , Avinash Sastry for his help\nin the Max/MSP framework and Prof. Nezihe S Â¸ent Â¨urk for\nher assistance in Turkish music theory. This material is\nbased upon work supported by the National Science Foun-\ndation under Grant No. IIS-0855758.\n9. REFERENCES\n[1] A Alpkoc Â¸ak and A.C Gedik. Classiï¬cation of Turkish songs according\nto makams by using n grams. In Proceedings of the 15. Turkish Sym-\nposium on Artiï¬cial Intelligence and Neural Networks (TAINN) , 2006.\n[2] C. Ames. The markov process as a compositional model: a survey and\ntutorial. Leonardo , 22(2):175â€“187, 1989.\n[3] W. Chai and B. Vercoe. Folk music classiï¬cation using hidden markov\nmodels. In Proceedings of International Conference on Artiï¬cial Intel-\nligence , volume 6, 2001.\n[4] P. Chordia, A. Albin, A. Sastry, and T. Mallikarjuna. Multiple view-\npoints modeling of tabla sequences. In Proceedings of International\nConference on Music Information Retrieval , 2010.\n[5] J.G. Cleary and W.J. Teahan. Experiments on the zero frequency prob-\nlem. In Proc. Data Compression Conference , volume 480. Citeseer,\n1995.\n[6] D. Conklin and I.H. Witten. Multiple viewpoint systems for music pre-\ndiction. Journal of New Music Research , 24(1):51â€“73, 1995.\n[7] D. Cope. Experiments in musical intelligence (EMI): non-linear\nlinguistic-based composition. Journal of New Music Research ,\n18(1):117â€“139, 1989.\n[8] A. Holzapfel. Similarity methods for computational ethnomusicology .\nPhD thesis, University of Crete, 2010.\n[9] D. Huron. Music information processing using the humdrum toolkit:\nConcepts, examples, and lessons. Computer Music Journal , 26(2):11â€“\n26, 2002.\n[10] D.B. Huron. Sweet anticipation: Music and the psychology of expecta-\ntion. The MIT Press, 2006.\n[11] K. Ë™Ilerici. Bestecilik bakÄ±mÄ±ndan T Â¨urk m Â¨uziË˜gi ve armonisi . Milli E Ë˜gitim\nBasÄ±mevi, 1981.[12] K. Lee and M. Slaney. Automatic chord recognition from audio us-\ning an hmm with supervised learning. In Proc. ISMIR , pages 133â€“137.\nCiteseer, 2006.\n[13] T. Lidy, C.N. Silla Jr, O. Cornelis, F. Gouyon, A. Rauber, C.A.A.\nKaestner, and A.L. Koerich. On the suitability of state-of-the-art mu-\nsic information retrieval methods for analyzing, categorizing and ac-\ncessing non-western and ethnic music collections. Signal Processing ,\n90(4):1032â€“1048, 2010.\n[14] C. Manning and H. Schutze. Foundations of Statistical Natural Lan-\nguage Processing , pages 60â€“78. MIT Press, 2002.\n[15] C.D. Manning, H. Sch Â¨utze, and MITCogNet. Foundations of statistical\nnatural language processing , volume 59. MIT Press, 1999.\n[16] D. Moelants, O. Cornelis, M. Leman, J. Gansemans, R. De Caluwe,\nG. De Tr Â´e, T. Matth Â´e, and A. Hallez. The problems and opportunities of\ncontent-based analysis and description of ethnic music. International\nJournal of Intangible Heritage , 2:57â€“68, 2007.\n[17] H. Myers. Ethnomusicology: an introduction , pages 110â€“164. WW\nNorton, 1992.\n[18] Ë™I.H. Â¨Ozkan. TÂ¨urk m Ë†usikÄ±si nazariyatÄ± ve us Ë†ulleri: kud Â¨um velveleleri .\nÂ¨OtÂ¨uken Nes Â¸riyat, 2006.\n[19] F. Pachet. The continuator: Musical interaction with style. Journal of\nNew Music Research , 32(3):333â€“341, 2003.\n[20] M.T. Pearce. The construction and evaluation of statistical models of\nmelodic structure in music perception and composition . PhD thesis,\nCity University, London, 2005.\n[21] M.T. Pearce, M.H. Ruiz, S. Kapasi, G.A. Wiggins, and J. Bhat-\ntacharya. Unsupervised statistical learning underpins computational,\nbehavioural, and neural manifestations of musical expectation. Neu-\nroImage , 50(1):302â€“313, 2010.\n[22] D. Ron. Automata Learning and its Applications . PhD thesis, the He-\nbrew University, 1995.\n[23] C.S. Sapp. Online database of scores in the humdrum ï¬le format. In\nInternational Society for Music Information Retrieval Conference (IS-\nMIR) , pages 664â€“665, 2005.\n[24] J.A. Sloboda. Generative processes in music . Clarendon Press, 2000.\n[25] Yalc Â¸Ä±n Tura. TÂ¨urk MusÄ±kisinin Meseleleri . Pan YayÄ±ncÄ±lÄ±k, Istanbul,\n1988.\n[26] G. Tzanetakis, A. Kapur, W.A. Schloss, and M. Wright. Computational\nethnomusicology. Journal of interdisciplinary music studies , 1(2):1â€“\n24, 2007.\n[27] G.A. Wiggins. Semantic gap?? schemantic schmap!! methodological\nconsiderations in the scientiï¬c study of music. In 2009 11th IEEE In-\nternational Symposium on Multimedia , pages 477â€“482. IEEE, 2009.\n[28] O. Yarman. 79-tone tuning & theory for Turkish maqam music . PhD\nthesis, Ë™Istanbul Teknik Â¨Universitesi Sosyal Bilimler Enstit Â¨usÂ¨u, 2008.\n274"
    },
    {
        "title": "A Multicultural Approach in Music Information Research.",
        "author": [
            "Xavier Serra"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1416592",
        "url": "https://doi.org/10.5281/zenodo.1416592",
        "ee": "https://zenodo.org/records/1416592/files/Serra11.pdf",
        "abstract": "Our information technologies do not respond to the world's multicultural reality; in fact, we are imposing the paradigms of our market-driven western culture also on IT, thus facilitating the access of a small part of the worldâ€™s information to a small part of the world's population. The current IT research efforts may even make it worse, and future IT will accentuate this information bias. Most IT research is being carried out with a western centered approach and as a result, most of our data models, cognition models, user models, interaction models, ontologies, etc., are culturally biased. This fact is quite evident in music information research, since, despite the world's richness in terms of musical culture, most research is centered on CDs and metadata of western commercial music. This is the motivation behind a large and ambitious project funded by the European Research Council entitled \"CompMusic: Computational Models for the discovery of the world's music.\" In this paper we present the ideas supporting this project, the challenges that we want to work on, and the proposed approaches to tackle these challenges.",
        "zenodo_id": 1416592,
        "dblp_key": "conf/ismir/Serra11",
        "keywords": [
            "multicultural reality",
            "paradigms of western culture",
            "IT research efforts",
            "information bias",
            "western centered approach",
            "data models",
            "cognition models",
            "user models",
            "interaction models",
            "ontologies"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\n   A MULTICULTURAL APPROACH  IN MUSIC INFORMATION RESEARCH Xavier Serra Music Technology Group Universitat Pompeu Fabra, Barcelona xavier.serra@upf.edu ABSTRACT Our information technologies do not respond to the world's multicultural reality; in fact, we are imposing the paradigms of our market-driven western culture also on IT, thus facili-tating the access of a small part of the worldâ€™s information to a small part of the world's population. The current IT re-search efforts may even make it worse, and future IT will accentuate this information bias. Most IT research is being carried out with a western centered approach and as a re-sult, most of our data models, cognition models, user mod-els, interaction models, ontologies, etc., are culturally bi-ased. This fact is quite evident in music information re-search, since, despite the world's richness in terms of musi-cal culture, most research is centered on CDs and metadata of western commercial music. This is the motivation behind a large and ambitious project funded by the European Re-search Council entitled \"CompMusic: Computational Mod-els for the discovery of the world's music.\" In this paper we present the ideas supporting this project, the challenges that we want to work on, and the proposed approaches to tackle these challenges. 1. INTRODUCTION In the last decade there has been great progress in the field of Music Information Retrieval, but the rate of improve-ment in most retrieval tasks, like the ones evaluated within the MIREX1 initiative, is c l e a r l y  s l o w i n g  d o w n .  We are starting to see the limits of the current signal processing and machine learning approaches and efforts are being made to find new ways to advance in terms of the currently identi-fied problems. However, maybe more importantly, there are many relevant problems that have not yet been looked at for which current methodologies may not work at all. For these new problems, it is even more important to concentrate on new research approximations f o r  t h e  c o m p u t a t i o n a l  p r oc-                                                1 http://www.music-ir.org/mirex  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.  Â© 2011 International Society for Music Information Retrieval  essing of music information. But, w h ich approximations should we use and what new problems should we look into?     The most general and common approaches used in music information processing a r e  b a s e d  o n d a t a  m o d e l i n g; they start from large data repositories and use signal processing and machine learning techniques to construct models. We should combine these a p p r o a c hes w i t h  o t h e r  r e s e a r c h  methodologies, like the ones coming from Musicology, Cognition or Human Computer Interaction. The approaches coming from Musicology develop models originating from music theory in which a thorough formalization contributes to an understanding of the theory itself, its predictions, and its scope. On the other hand, the approaches coming from Cognition aim at constructing theories of music cognition, formalizing the mental processes involved in listening to and performing music. The approaches than have been de-veloped by the Human-Computer Interaction research community focus on the users and bring in methodologies coming from behavioral sciences. There is a need to expand the information modeling methodologies but we should do it with a cross-fertilization approach.     Making sense of music is much more than decoding and parsing an incoming stream of sound waves into higher-level objects such as onsets, notes, melodies and harmonies. Music is embedded in a rich web of cultural, historical, commercial, technological and social contexts that influ-ence how it is produced, disseminated and perceived. For example, many qualities or categorizations attributed to a piece of music by listeners cannot solely be explained by the content of the audio signal itself. It is thus clear that high-quality automatic music understanding and description can only be achieved by taking into account information sources that are external to the actual music signals.      Despite recent efforts by the MIR research community to open up towards non-western music [1] the major focus has been on the study of a few aspects of the western commer-cial music of the past few decades. This music repertoire has conditioned the problems that we are working on and thus the solutions obtained. If we study other aspects, and especially other types of music, we will find new interest-ing problems to be solved that will require new methodolo-gies and new solutions.      For the development of music information models, it is of great advantage to work with musical repertoires, thus \n151Oral Session 2: Non-Western Music\n   musical cultures, that have a classical tradition and for which there is a relevant body o f  a c a d e m i c  s t u d i e s a v a il-able. Despite the trend towards a musical monoculture [2] there are robust music cultures with a classical tradition in places like China, India, Turkey, Indonesia, or in the Arab world, traditions that form a counterpoint to the western music context. A few of these musics have excellent musi-cological and cultural studies available; they maintain per-formance practice traditions and they exist within real so-cial contexts. Thus, some of these traditions can be an ex-cellent ground on which to build new i nf or ma t i on mode l s  and the means to challenge the dominant western-centered information paradigms. If we are able to describe and for-malize the music of these cultures we might open up the current information models in a way to better capture the richness of our world's music [3].     CompMusic, a research project funded by the European Research Council, was born from these concerns and from the realization that unless we do something, we might lose our world's cultural richness. What the European coloniza-tion of many parts of the world did not completely achieve, that is, to westernize their cultures, will now happen by the global use of our western-centric information technologies. These information technologies, and even more, the future technologies emerging from current research, will have a great impact on the way we maintain, access, and use t he world's information resources, and thus they will condition the survival of that information and of the cultures that pro-duce it. Unless the information technologies support the di-versity of cultural perspectives that exists in the world, we will slowly lose our wonderful world cultures.     It could be argued that there is no need to look outside western music to find new problems with which to improve our MIR techniques. It is completely true: there are many aspects of our classical, folk and other western music tradi-tions for which current MIR methodologies do not work. But it is also true that when we try to analyze some of the non-western classical traditions, our research limitations become even more obvious, and it might be easier to look at the problems with a fresh perspective, without being con-taminated by our traditional view on western music. At the same time, the opportunity to help bring new musical tradi-tions into the world of Music Computing is a very reward-ing task. 2. PROJECT OBJECTIVES AND APPROACHES The main objective of the CompMusic project is to pro-mote and develop multicultural perspectives in Music Computing research. We want to identify music problems coming from culture-specific contexts and work on solu-tions that might result in new computational methodologies of interest for a wide variety of music information process-ing problems.     In music computing research there is a need to advance in the description and formalization of music with the aim to make it more accessible to computational approaches. In this project we will work on finding ways to reduce the known gap between audio signal descriptions and semanti-cally meaningful music concepts. B u t  we  b e l i e v e  t hat we will only make progress if we approach this issue by com-bining academic disciplines, such as Information Process-ing, Computational Musicology, Music Cognition and Human-Computer Interaction, thus using both qualita-tive/quantitative methodologies and scientific/engineering approaches. We also need to work with a variety of infor-mation sources, s u c h  a s  a u d i o  f e a t u r e s ,  s y m b o l i c  s c o r e s ,  text commentaries, or user evaluations. Therefore, we need to open up the research methodologies being used.     CompMusic will take a cultural approach, meaning that information modeling techniques will be developed for specific music repertories with emphasis on their cultural contexts. To model a musical repertoire we need to under-stand and model the music together with the user commu-nity that creates, enjoys and supports that particular music.     An important consideration in a  c u l t u r a l-specific re-search approach is the issue of who should be involved. We need to involve different cultural perspectives. We be-lieve that is not possible to carry out such a project solely with a research group immersed in a cultural context dif-ferent to t h e  o n e  t o  b e  s t u d i e d .  C o m p M u s i c  w i l l  i n v o l v e experts, research teams and users immersed in the musical cultures being studied.      Given the availability of musicological/cultural studies and our existing collaborations with researchers and ex-perts within diverse m u s i c a l  c u l t u r e s ,  in CompMusic we will s t u d y  a r t  m u s i c  t r a d i t i o n s  i n  India ( H i n d u s t a n i  a n d  Carnatic), Turkey (Ottoman), North Africa ( A n d a l u s i a n ) ,  and China (Han). All these traditions offer large and useful information sources from which to d e v e l o p  o u r  p r o j e c t .  However, knowing that we will not be able to cover it all with the same depth, we have decided to first focus on the Hindustani, Carnatic and Ottoman traditions, and progres-sively incorporate specific aspects of the other cultures.     The Hindustani and Carnatic music traditions of the In-dian subcontinent offer the possibility to study relevant problems in all aspects of interest to this project. Their in-struments, such as the Tambura, the Veena or the Sitar, have been built to emphasize sonic characteristics that are quite different to those of the typical western musical in-struments. The concepts of Raga and Tala are completely different to the western concepts used to describe melody and rhythm, thus their understanding and computational description requires new approaches. Their particular im-provisatory nature based on the Ragas makes these reper-toires very alive and constantly evolving. The musical scores used in both Hindustani and Carnatic traditions serve a different purpose to those of western music and thus have to be studied differently. The tight musical and sonic relationship between the singing voice, the other me-lodic instruments, and the percussion accompaniment within a piece, requires going beyond the analytical and \n15212th International Society for Music Information Retrieval Conference (ISMIR 2011)\n   modular approaches commonly used in MIR. The special and participatory communication established between per-formers and audience in concerts, offers great opportunities to study issues of social cognition. The specific differences between Hindustani and Carnatic traditions in all the as-pects mentioned permit the study of musical and cultural differentiation issues. Even though we will only be able to tackle some of these problems in this project, we hope to encourage other researchers to study many of the others.     Ottoman art music is very much in the intersection of many music traditions. The Turkish culture is at the junc-tion of European and Asian cultures, and its musics reflect this geo-strategic position. The makam and its performance practice, the basis for the melodic organization of Ottoman music, offer a very rich source of microtonal studies, and there is already extensive theoretical work on its character-istic non-equal-tempered pitch organization. Its complex rhythmic patterns also challenge the audio analysis ap-proaches currently used in MIR. This music culture offers a great context in which to study the open debate in many music traditions on the relevance of theory versus practice in the understanding of a given music repertoire. The adap-tation of the western music notation to fit the microtonal aspects of their music, together with other European music influences, also offer a challenging set of musical style characterization problems. It is remarkable that despite all the cultural influences and political pressures, Ottoman art music has maintained a strong personality and an active social context. In fact, it is a music that has influenced many music traditions of both Europe and the Middle East, an aspect that also deserves a thorough study.     Andalusi music can be traced back to the time when the Arabs had a flourishing culture in Al-Andalus (now part of Spain) for many centuries. T h i s  m u s i c  t r a d i t i o n  h a d  t o  move its main center to North Africa when the Arabs were forced to leave Al-Andalus at the end of the 15th century. These historical and geographical developments give us very interesting cultural context problems to be studied. Andalusi music shares many fundamental characteristics with Ottoman tradition, for example the maqams used are very much related to the Ottoman makams. Thus, the com-parative study of these two traditions should be very fruit-ful. Also the fretless Oud, a preeminent instrument upon which most of the theoretical studies of this music tradition have been based, can give us relevant insights into the characteristics of the music. The approach to this classical tradition is more difficult for us than the others given the lack of research activity in the Arab countries of North Af-rica related to computational musicology. It will be diffi-cult to find collaborators for this project in these countries.     The other music culture that we want to study, the Han music of China, is also an amazing source of information processing problems. The relationship between language and music in a culture that has a tonal language offers in-teresting research problems. The importance of the tradi-tional philosophies and nature in all Chinese classical art forms gives a completely different context to the Han mu-sic. For example, the concept of emotion in this music has a different meaning to the one used in the west, and the so-cial function of the music is also very different. From all the cultures that we want to study, the Han culture is the one that has had the least contact with the western aca-demic world and most musicological studies have been published only in Mandarin and available only in China. Thus, collaboration with researchers and experts immersed in that culture is even more important.     Apart from the issues that can be studied on each music repertoire, it is very relevant to work on the problems that arise from comparative studies. Typically, the available comparative studies focus on the comparison between any music cultures and western classical music. But it is impor-tant to also make comparative studies between the different non-western musical cultures. For example, it is interesting to compare the Indian concept of raga with the Ottoman concept of makam, or the improvisatory strategies used in Indian music with the ones used also in Ottoman music. In fact, all the musical cultures selected in this project were part of what is known as the Silk Road. This was a network of trade routes across the Asian continent that connected East, South, and Western Asia with the Mediterranean world, as well as North, East, and Northeast Africa and Europe, for almost 3,000 years. This trade connection had a great impact on the development of all these musical cul-tures and it is thus fascinating to understand these mutual influences. In this project, we will promote a tight collabo-ration between the researchers working in the different countries and on the different culture-specific problems in order to understand these cross-cultural issues. 3. PROJECT TASKS The culture-specific problems on which we will start work-ing are not yet defined; in fact, this is now the main work in progress, together with putting together the initial research team. So far we ha ve  de f i ne d t he  ma i n general and trans-versal tasks and research approaches that we will use to tackle our ambitious aims. These tasks are: (1) to gather and organize audio recordings, metadata, descriptions, scores, plus all the needed contextual information of the selected music repertories; (2) to identify and study the re-quired musicological references in order to understand the chosen repertories within their cultural context; (3) to de-sign the ontologies needed to annotate and analyze t h e  gathered music collections; (4) to work on audio content analysis approaches to help describe the music collections chosen; (5) to work on a social-computing approach to characterize users and communities, modeling their musical preferences and behaviors; and (6) to develop systems that, by integrating the results of this project, can show the rele-vance of this research approach for the discovery of our world's music. \n153Oral Session 2: Non-Western Music\n   \n Figure1. General project tasks. 3.1 Data gathering A major issue in the gathering and organization of the rele-vant information f o r  t h i s p r o j e c t  i s  t h e  data formats used. Most databases and protocols that we have used in previous information processing projects were designed for housing western pop music. As a result, important information would be lost if we just used the same schemas to organize the types of music that this project works on. We will con-vert our current framework into a more freely structured database, based on RDF1 (Resource Description Frame-work) and graph database technologies. This will enhance the storage systemâ€™s support for heterogeneous data and allow for the flexible addition of new data as it becomes available. We also want to contribute and take advantage of the existing online data repositories that use open licenses, like Wikipedia ( g e n e r a l  e n c y c l o p e d i a ), MusicBrainz ( C D s  metadata), Mendeley (bibliographical references), or Free-sound (audi o cl i ps)2. The goal is to be active in the Open Data Movement3 promoting it within our research commu-nity.     We will also have to make some recordings and carry out fieldwork i n  order to gather specific data with which to tackle some of the identified research problems. A l l  t h i s  data will be made available through the existing open re-positories, or, i f  n o t  a p p r o p r i a t e, i t  w i l l  b e p l a c e d  o n  t h i s project's website, also using open licenses. The CompMusic website4 is u s e d  both as this p r o j e c t's d i s s e mi n a t i o n  p o r t a l  and a s  a  c o l l a b o r a t i v e  f r a m e w o r k  in w h i c h  a n y o n e  i n t er-ested in the objectives of this project can get involved and contribute information.                                                 1 http://www.w3.org/RDF 2 http://wikipedia.org, http://musicbrainz.org, http://mendeley.com, http://freesound.org 3 http://en.wikipedia.org/wiki/Open_Data 4 http://compmusic.upf.edu 3.2 Musicological studies There is no lack of musicological research on t h e  chosen musical cultures, e.g. [4], and we need to study it to under-stand the musical specificities of each repertoire, the simi-larities and differences between the selected music cultures, and what can distinguish them from classical western cul-ture. Each classical music tradition has developed its own instruments, compositional styles, performance practices, social uses, and contexts. However, we find very different points of view, sometimes opposing ones, in existing litera-ture, a n d  we will be c o llaborating wi t h  mu s i c o l o g i s t  f r o m the different music cultures in order to understand those points of view and to be able to develop appropriate musi-cological perspectives for our identified problems.      An important part of the traditional m u s i c o l o g i c a l  re-search of western music is based on the study of symbolic representations, scores, and given that our chosen reper-toires are mainly based on oral traditions, t h eir e x i s t ing symbolic representations have been studied much less. There is some work on the extension and use of digital rep-resentations like the Humdrum toolkit5 or MusicXML6 to analyze non-western music notations that we want to con-tinue. This type of analysis is very complementary to that which is audio-based and can be used to study some com-positional and semantic aspects of music.      The development of a musicological framework i s  f un-damental to properly contextualizing most  of  our r esear ch work. In particular, to identify the relevant problems to be studied, to agree on the terminology to use for the different culture-specific musical concepts, to balance the theoretical with the practical perspectives that will be involved in the different problems, and also to interpret the computational results obtained. 3.3 Ontologies design To process and share information in open environments we need a common understanding of the meaning of the data, thus we need to specify how concepts and terms of a given domain are understood, specifications that are known as ontologies. For example, Music Ontology7 [5] is being de-veloped by an open community of experts a s  a f o rmal framework for dealing with music-related information on the Semantic Web, including editorial, cultural and acoustic information. It was started with the goal of capturing t h e  musical production process, but we propose to extend it to express the variety of concepts that will be indentified in the study of the selected musical cultures. We will also ex-plore the approaches coming from what is known as emer-gent semantics [6] in order to develop ontologies from our gathered data repositories and from mining music-specific                                                 5 http://musicog.ohio-state.edu/Humdrum 6 http://www.recordare.com/musicxml 7 http://musicontology.com \n15412th International Society for Music Information Retrieval Conference (ISMIR 2011)\n   websites (e.g., music magazines, deep musical discussion forums, or reputed blogs about music).     As soon as we take a culture-specific approach in t h e  study of music, the issues of terminology and language be-come very important. We will have to develop multi-language ontologies from which to capture the musical concepts used by the different cultures, a task that will be done by musical experts. But we will have to deal with the diversity of meanings that a single term may have within a single tradition or the fact that different terms can s i g n i f y  the same or very similar meanings within that same culture.  3.4 Audio description The extraction of musically and culturally meaningful fea-tures from audio recordings of different music repertoires requires introducing new approaches, and for sure, extend-ing the existing s i g n a l  a n a l y s i s  t e c h n i q u e s  a n d  m a c h i n e learning methods. The fact that the music traditions chosen in this project have fundamental differences from western music traditions, such as different musical instruments, tun-ing systems, performance styles, or musical forms, imply that at the level of feature analysis, most of the descriptors and extraction methodologies being used to analyze com-mercial western music are not appropriate, or at least they have to be developed much further. Even at the level of acoustics there are very interesting and important d i f f er-ences between western musical instruments commonly used and the instruments used in other classical traditions. This basic consideration has profound implications at all musical levels.      Both the fact that most musical audio features are interre-lated and that their musical meaning is very much context dependent wi l l  have t o be emphasi zed i n our  pr oj ect. The recent approaches in audio analysis based on joint estima-tion of audio features [7] are especially relevant to finding more perceptually meaningful music characterizations. Also the current trend towards knowledge-based and top-down signal processing [8] is very relevant in our case since many of the music features of a given music reper-toire cannot even be analyzed without using information from the cultural tradition they are part of.     Within most cultural t r a d i t i o n s, music performance is learned by imitation and music appreciation is developed implicitly from listening. One of the consequences of this is that most people, even the experts, have a very difficult time verbalizing their musical opinions, but they are able to express those opinions t h r o u g h  e x a m p l e s .  B a s e d  o n  t h i s ,  our audio description work will use informed audio refer-ences supplied by experts as our main data sources, then we will focus on extracting musically meaningful features from them and on developing similarity measures between them.  3.5 Community profiling The understanding and characterization of the musical pref-erences and behaviors of groups of people sharing a given musical culture i s  a new and complex problem with very little prior work. Theoretically, t his profiling could be  de-veloped explicitly by asking music lovers and experts of a given community, but in this project we want to focus on the implicit characterization of online communities, that is, characterizing groups of people that leave a trace on the Web. We would like to capture the musical behavior of a given online user group by exploiting information sources such as web search statistics, musical preferences extracted from social networks, or data inferred from demographical, geographical, and psychographical data (personality, val-ues, attitudes, and interests). The actual musical profile of a group should be expressed through the audio descriptors of the music they interact with, ontologies generated by them, and from the automatically extracted information about their cultural context and behaviors. Provided that we have access to the appropriate data, with this type of information we should be able t o characterize any music community active online.     A good starting point for this work is the current research within the field of social computing [9], concerned with the intersection of social behavior and computational systems. Especially relevant is the research on the analysis, model-ing and knowledge extraction from communities of users. The work following the ideas of emergent semantics pro-poses the analysis of online communities for the extraction of ontologies that structure the concepts and terms of a par-ticular domain [6]. Peter Mika [10] proposes a unified model of social networks and semantics where social tag-ging systems can be modeled as a tripartite graph with Ac-tors, Concepts and Instances (Actor-Concept-Instance model). By analyzing the r e l a t i o n s  b e t we e n  c o n c e p t s  b o t h  on the basis of co-occurrence in instances and common us-age by actors (users), lightweight ontologies can emerge from online communities. A completely different approach to community knowledge extraction for the design of on-tologies is proposed in [11], where a Web portal with col-laborative ontology management capabilities is imple-mented. However, the field is very young and there is still a conceptual gap between this vision and its possible imple-mentation; only very simple ontologies can emerge.     A computational approach to community profiling re-quires having access to data from relevant online communi-ties. In our project we are starting with Freesound, an open collaborative sound database developed by our research group w h e r e  p e o p l e  with d i f f e r e n t  i n t e r e s t s  s h a r e  a u d i o  clips. Given its characteristics and our involvement in its development, it offers a good platform to carry out studies on social networking, specifically on community profiling, and to explore technological solutions to promote specific musical activities, which is also an important aspect of our project. There is a lot to be done in understanding the gen-eral issues of profiling of online communities. Only after we get a handle on the basic issues will we be able to face the specific profiling problems of the music communities of relevance to the CompMusic project. At this time we have \n155Oral Session 2: Non-Western Music\n   not yet identified online communities that would be appro-priate for our research objectives, but given the fast growth of online communities we should be able to find relevant music sites to study our culture-specific issues. 3.6 System integration Quite a number of musical applications could take advan-tage of the expected research results of this project. We can focus on application areas such as music education, music creation, music appreciation, music recommendation, etc., or any particular application that might benefit from an in-formation processing engine able to process our world's music, respecting its culture specificity and that of the us-ers. In this project we want to exemplify the benefits of our multicultural approach to music computing by either devel-oping new music systems or by extending the functionality of existing onesâ€”functionalities related to automatic music analysis, categorization, learning or discovery.      The development of a culture-driven system should be much more than the addition of a set of components; our system should be interactive and evolve with the users, with the context, and with the availability of new information. Even the concept of interface design becomes a critical is-sue when approached from a multicultural perspective. Can we have a single interface for a system to be used by differ-ent cultures? We think not. The interface has to adapt to the usersâ€™ cognitive/cultural structures and has to engage them using the values and attributes of their own culture. All these aspects require specific research before even consid-ering any system development.     Within CompMusic there is no particular aim of develop-ing complete systems, but we want to put together proto-types with which to demonstrate our research results. How-ever since this task will be worked on during the last part of this project, at this stage it does not make much sense to continue to describe the details or even the type of system that will be developed. 4. CONCLUSIONS CompMusic is a big and ambitious project that aims at hav-ing an impact not just within the Music Computing field but also, more generally, on the overall field o f  i n formation technology. With funding from the European Research Council we will b e a b l e  t o  s u p p o r t  the research work of quite a number of PhDs and post-doctorates from different parts of the world covering all areas described in this arti-cle. In addition, we also aim to bring in a disruptive point of view in IT by promoting, and showing the validity o f ,  a new research approach rooted in a multicultural perspec-tive.  5. ACKNOWLEDGEMENTS I want to thank the organizers of WISSAP-2010 that took place in IIT-Bombay in J a n u a r y  2 0 1 0  for inviting m e t o  give a series of tutorial lectures on Music Computing, a trip that gave me the opportunity to realize the need for this project. I would also like to thank the many musicians and researchers, working outside the well-established academic western institutions, from which I have learned alternative ways to approach and understand music. Finally I have to acknowledge the contributions of many r e s e a r c h e r s  f r o m  the MTG-UPF in the definition of this project.     The CompMusic project has received funding from the European Research Council under the European Unionâ€™s Seventh Framework Program (FP7/2007-2013) / ERC grant agreement 267583 a n d  w i l l  r u n  for five years starting on July 2011. 6. REFERENCES [1] G. Tzanetakis et al.: \"Computational Ethnomusicology,\" Journal of Interdisciplinary Music Studies, 1(2), pp. 1-24, 2007. [2] B. Nettl: The Western Impact on World Music. Schirmer Books, 1985. [3] D. Huron: \"Issues and Prospects in Studying Cognitive Cultural Diversity,\" Proc. of the 8th International Conference on Music Perception & Cognition, pp. 83-96, 2004. [4] Routledge (publisher): The Garland Encyclopedia of World Music (10 volumes), 1997-2001. [5] Y. Raimond: A Distributed Music Information System, PhD Thesis, 2008. [6] K. Aberer, et al.: \"Emergent Semantics Principles and Issues,\" Database Systems for Advanced Applications, pp. 25-38, 2004. [7] H. Papadopoulos and G. Peeters: \"Joint Estimation of Chords and Downbeats from an Audio Signal,\" IEEE Transactions on Audio, Speech, and Language Processing, 19(1), pp. 138-152, 2011. [8] A. Klapuri and M. Davy (Eds.): Signal Processing Methods for Music Transcription, Springer, 2006. [9] S. Chai e t  a l .  ( Eds.): Advances in Social Computing, Springer, 2010. [10] P. Mika: \"Ontologies are us: A Unified Model of Social Networks and Semantics,\" Journal of Web Semantics, 5(1), pp. 5-15, 2007. [11] A. V. Zhdanova: \"Community-driven Ontology Construction in Social Networking Portals,\" Web Intelligence and Agent Systems: An International Journal, vol. 6, pp. 93-121, 2008. \n156"
    },
    {
        "title": "Assessing the Tuning of Sung Indian Classical Music.",
        "author": [
            "Joan SerrÃ ",
            "Gopala K. Koduri",
            "Marius Miron",
            "Xavier Serra"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1415102",
        "url": "https://doi.org/10.5281/zenodo.1415102",
        "ee": "https://zenodo.org/records/1415102/files/SerraKMS11.pdf",
        "abstract": "The issue of tuning in Indian classical music has been, historically, a matter of theoretical debate. In this paper, we study its contemporary practice in sung performances of Carnatic and Hindustani music following an empiric and quantitative approach. To do so, we select stable fundamental frequencies, estimated via a standard algorithm, and construct interval histograms from a pool of recordings. We then compare such histograms against the ones obtained for different music sources and against the theoretical values derived from 12-note just intonation and equal temperament. Our results evidence that the tunings in Carnatic and Hindustani music differ, the former tending to a just intonation system and the latter having much equal-tempered influences. Carnatic music also presents signs of a more continuous distribution of pitches. Further subdivisions of the octave are partially investigated, finding no strong evidence of them.",
        "zenodo_id": 1415102,
        "dblp_key": "conf/ismir/SerraKMS11",
        "keywords": [
            "The issue of tuning in Indian classical music",
            "historical debate",
            "contemporary practice",
            "sung performances",
            "Carnatic and Hindustani music",
            "empiric and quantitative approach",
            "stable fundamental frequencies",
            "interval histograms",
            "music sources",
            "theoretical values"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nASSESSING THE TUNING OF SUNG INDIAN CLASSICAL MUSIC\nJoan Serr `a, Gopala K. Koduri, Marius Miron and Xavier Serra\nMusic Technology Group\nUniversitat Pompeu Fabra, Barcelona, Spain\njoan.serraj@upf.edu,gopal.iiit@gmail.com,miron.marius@gmail.com,xavier.serra@upf.edu\nABSTRACT\nThe issue of tuning in Indian classical music has been, his-\ntorically, a matter of theoretical debate. In this paper, we\nstudy its contemporary practice in sung performances of Car-\nnatic and Hindustani music following an empiric and quanti-\ntative approach. To do so, we select stable fundamental fre-\nquencies, estimated via a standard algorithm, and construct\ninterval histograms from a pool of recordings. We then com-\npare such histograms against the ones obtained for differ-\nent music sources and against the theoretical values derived\nfrom 12-note just intonation and equal temperament. Our\nresults evidence that the tunings in Carnatic and Hindustani\nmusic differ, the former tending to a just intonation system\nand the latter having much equal-tempered inï¬‚uences. Car-\nnatic music also presents signs of a more continuous dis-\ntribution of pitches. Further subdivisions of the octave are\npartially investigated, ï¬nding no strong evidence of them.\n1. INTRODUCTION\nPitch relationships are at the heart of composition and im-\nprovisation in the large majority of musical cultures [1]. The\ntuning system plays an important role in these relationships,\ndeeply inï¬‚uencing many musical qualities such as mood,\nconsonance or timbre [11, 17]. Traditionally, tuning has\nbeen a relevant subject of study from musicological, his-\ntorical and theoretical perspectives (see [11] and references\ntherein). Current technologies allow for a more empiric and\nquantitative analysis of the different tunings that enrich our\nmusical experience, specially those used by non-Western\ncultures or the ones which substantially differ from equal\ntemperament [6, 13, 17, 19].\nIn Indian classical music, both in Carnatic (South) and\nHindustani (North) musical traditions, musicologists have\ncomprehensively covered the issue of tuning (e.g. [15, 18]).\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proï¬t or commercial advantage and that copies\nbear this notice and the full citation on the ï¬rst page.\nc/circlecopyrt2011 International Society for Music Information Retrieval.Typically, one ï¬nds seven notes, called swaras, which are\ndenoted by Sa, Ri, Ga, Ma, Pa, Da and Ni1. Except for\nthe tonic and the ï¬fth, all the other swaras have two varia-\ntions each, which account for 12 notes in an octave, called\nswarasthanas. It is a well accepted notion that a swarasthana\nis aregion rather than a point [18]. Therefore, in Indian mu-\nsicology, a ï¬xed tuning for each note is not as important as\nit is, say, in Western classical music. Ornamentations are\nessential, and part of the style.\nStill, instruments are tuned using some method. For ex-\nample, Shankar [18] ï¬rst presents a set of 12 notes tuned\nin 5-limit just intonation [11] and later discusses the the-\nory of the 22 shrutis2, which ï¬xes 22 unequal subdivisions\nof the octave. Sambamoorthy [15] directly advocates for\nthis theory and states that â€œthe number 22 represents the\nbarest minimum of shrutis that has been actually used in\nIndian music from ancient timesâ€, mentioning theories of\n24, 27, 32, 48, 53 and 96 shrutis (see also [4] for a brief\noverview). Such further subdivisions of the octave are very\ndebatable [8, 9, 14], even from a purely psychoacoustical\nperspective [1]. Therefore, today, the theory of 22 shrutis\nis under controversy, with some musicologists claiming that\nthey are no more used, replaced by the basic set of 12 notes\n[14]. The tuning of these 12 notes is also debated, with au-\nthors claiming it to be either just intonation, equal-tempered\nor a mixture of both [9, 16].\nSignal processing techniques and computational resources\ncan shed light to the above discussion, providing empiric\nand quantitative evidence. Initial experiments along this line\nreported a high variability in the intonation of notes, both\nin Carnatic and Hindustani music (see [7â€“9] and references\ntherein). In [7, 8], Krishnaswamy employed different pitch\ntracking methods (based on the Fourier transform, the auto-\ncorrelation function and source separation) to analyze sev-\neral Carnatic music pieces with different instruments each\nand found only 12 distinctive intervals. Still, he did not\nprovide any clear statement whether these intervals corre-\nsponded to equal temperament or just intonation. Mahendra\net al. [12] used an autocorrelation-based method for pitch es-\n1They are analogous to European solf `egeâ€™s Do, Re, Mi, Fa, Sol, La, Si.\n2Literally, â€œthat which is heardâ€. A shruti â€œis the smallest audible dif-\nference of pitch. It is a fraction of a semitoneâ€, with no constant value [15].\n157Oral Session 2: Non-Western Music\ntimation and found clear signs of equal temperament. How-\never, their analysis was done with very few recordings and\nwithout providing much detail of them. Larger collections\nwere analyzed by Datta et al. [2â€“4], who employed a phase-\nspace analysis and a series of post-processing methods to\nstudy the shrutis in Hindustani music. Among other results,\nthey report a slightly better ï¬t for 12 interval scales com-\npared to 22 shruti scales in [2]. However, no substantial\ndifferences were found between the considered scales (in-\ncluding the 12-interval equal-tempered one). In subsequent\npapers [3, 4], the same authors provided evidence for the\nexistence of 22 shrutis, although they reported some contra-\ndictions.\nIn this paper we study the tuning of several Indian music\nrecordings. In order to do so, it makes sense to focus on\nthe singing voice, since it is not constrained in its pitch con-\ntrol, it is the reference to be followed by all the other instru-\nments and it has many solo sections, easy to analyze, in all\nperformances [15]. Using standard techniques, we estimate\nthe fundamental frequencies of sung recordings and, based\non a pool of these, build an interval histogram. Interval his-\ntograms for different data sources are then used to assess the\nplausibility of just intonation and equal temperament tun-\nings, both in Carnatic and Hindustani music. We compare\nthe interval histograms obtained from these sources against\nthe ones obtained from both synthetic and real signals, the\nlatter coming from commercial Western music recordings.\nThe possibility of 22 shrutis and the issue of tuning variabil-\nity in Indian classical music are also discussed.\nThe rest of the paper is organized as follows. First, we\npresent our methodology, both for estimating fundamental\nfrequencies and for computing interval histograms (Sec. 2).\nNext, we present and discuss our results for different sets of\nrecordings (Sec. 3). Finally, we draw our main conclusions\nand highlight future research lines (Sec. 4).\n2. METHODOLOGY\n2.1 Fundamental frequency estimation\nStrictly speaking, most Indian classical music should be con-\nsidered heterophonic. However, for the current analysis,\nwe carefully selected passages where the singerâ€™s voice was\nvery predominant and substantially louder than the rest of\nthe usual accompaniment (Sec. 3.2). Therefore, the ana-\nlyzed recordings can be roughly considered as monophonic.\nUnder this situation, we choose to estimate their fundamen-\ntal frequency with an implementation of De Cheveign Â´e &\nKawaharaâ€™s YIN algorithm [5].\nFirst, we resample the audio to 44100 Hz, downmix it\nto mono and apply a low-pass ï¬lter with a cutoff frequency\nof 1200 Hz. Next, we estimate the fundamental frequency\nin a moving window of 4096 samples with a hop size of\n133 (93 and 3 ms, respectively). To do so, the time lagÏ„minthat yields a minimum of the modiï¬ed autocorrelation\nfunctiond/prime\nn(Ï„)for each window nis selected [5]. Such\nminimumd/prime\nn(Ï„min)corresponds to a value between 0 and\n1, and provides a conï¬dence value cn= 1âˆ’d/prime\nn(Ï„min)for\nthe fundamental frequency. We set the threshold for the\nsearch of such minimum to d/prime\nTh= 0.15and discard lags be-\nlow 40 or above 882 samples (above 1100 and below 50\nHz, respectively). For a further reï¬nement of the funda-\nmental frequency, a three-point parabola interpolation and a\nâ€œbest local estimateâ€ within 20 ms is employed [5]. Finally,\neach fundamental frequency for each window is converted\nto cents, yielding what we call a pitch contour . Conï¬dence\nvaluescnare also kept.\nSince we focus on tuning, and in order to mitigate poten-\ntial errors of YIN, we consider only â€˜stableâ€™ regions of our\npitch contour. Indeed, we do not need a complete transcrip-\ntion of the sung content, but only those parts of the contour\nwhere the tuning can be assumed to be more or less constant.\nTherefore, and following the ideas of Tidhar et al. [19], we\ncan apply some principle of â€œconservative transcriptionâ€. In\nour case, we only do a kind of â€œsteady state detectionâ€ [2]\nand keep the cent and conï¬dence values of the windows cen-\ntered at a stable region of 400 ms (we consider a region to be\nstable if the standard deviation of the pitch contour elements\nof such region is lower than 20 cents).\n2.2 Interval histogram computation\nFor each recording, a weighted histogram with a resolu-\ntion of one cent is built by rounding the stable cent values\nabove. Conï¬dence values cnare used as weights. The value\nof the lowest/highest bin of the histogram corresponds to\nthe minimum/maximum value found in the stable regions\n(minus/plus an arbitrary constant /greatermuch1). This weighted his-\ntogram is mean-smoothed by taking into account the 12 near-\nest magnitudes of each bin (the 6 immediately lower and up-\nper ones) and linearly normalized between 0 and 1. Notice\nthat, in contrast to Moelants et al. [13], we have not applied\nany octave equivalence at preliminary stages. Therefore, we\nare able to discriminate intervals larger than 600 cents.\nIn Indian classical music, the reference tuning frequency\nsubstantially varies within performers [2, 15]. An intuitive,\nnatural and straightforward way to avoid the (usually not\ntrivial, see e.g. [2]) estimation of this reference frequency is\nto employ intervals, a basic perceptual concept [1]. There-\nfore, we opt to build and study an interval histogram , which\nwe denote as h. To calculate h, we select prominent peaks\nof the mean-smoothed weighted histogram and compute all\nthe possible positive subtractions between peak bins (i.e. the\nintervals). A peak bin is deï¬ned such that it has a magnitude\nhigher than the mean of all the histogramâ€™s magnitudes and\nhigher than the ones of the nearest 50 bins (the 25 lower and\nupper ones). Since in preliminary analysis we found clear\noctave equivalences for all intervals, we mapped them to\n15812th International Society for Music Information Retrieval Conference (ISMIR 2011)\nvalues between 0 and 1199 cents. For a better visualization,\nthe interval histogram is mean-smoothed by taking into ac-\ncount the 6 nearest values of each bin (the 3 lower and upper\nbin magnitudes) and linearly normalized between 0 and 1.\n3. RESULTS AND DISCUSSION\n3.1 Synthetic data\nTo conï¬rm the usefulness of our approach to detect tunings,\nwe ï¬rst test it with synthetic data. To generate this data\nwe synthesize MIDI scores using a physically-modeled pi-\nano sound on Pianoteq3. We use the ï¬rst 5 preludes and\nfugues of Bachâ€™s Well-Tempered Clavier and record them\nusing equal temperament and 5-limit just intonation (treat-\ning each MIDI channel as a different piece).\nThe interval histograms obtained for the synthetic sig-\nnals show clear peaks at the theoretical interval positions of\nequal temperament ( hE) and just intonation ( hJ) tuning sys-\ntems (Figs. 1 and 2, respectively). These and subsequent\nï¬gures show the positions of the theoretical interval values\nthat would be obtained with equal temperament, just intona-\ntion and the 22 shrutis (vertical lines; see Fig. 1â€™s caption).\nThe theoretical intervals obtained with just intonation over-\nlap with the ones obtained with the 22 shrutis (c.f. [16,18]).\nHowever, there are a few idiosyncratic locations where the\nshruti intervals do not overlap with the just intonation ones:\nat 21, 133, 337, 365, 835, 1067 and 1178 cents.\n3.2 Carnatic and Hindustani intervals\nWe now analyze and comment the interval histograms ob-\ntained with Indian classical music. We employ two mu-\nsic collections composed of Carnatic (233) and Hindustani\n(133) recordings of 30 sec to 4 min duration. The record-\nings are of both male and female singers of various schools\nand singing styles (rachanas: geetams, varnas, keertanas and\nkruti). They comprise artists with an active period from\n1930 to present such as Balamurali Krishna, Sudha Raghu-\nnathan, Maharajampuram Santhanam, John Higgins, V oleti\nVenkateswarlu, Pandit Ajoy Chakrabarty, Amir Khan, Bhim-\nsen Joshi, Vidyadhar Vyas or Girija Devi. All recordings\nwere selected such that the voice was very loud compared\nto the rest (primarily alap parts from khyal compositions).\nV oice was normally accompanied by the drone of a tambura\nand/or a sarangi, the percussion of a mridangam or the tabla\nand a violin4. In the case of Hindustani music we discarded\nrecordings that contained a harmonium since this could be\ntuned to equal-temperament.\nThe interval histogram hCobtained for Carnatic music\nsuggests that statements about the use of a just intonation\n3http://www.pianoteq.com\n4A comprehensive summary of Indian instruments can be found\nathttp://chandrakantha.com/articles/indian_music/\ninstruments.htmlLocation Nearest JI Diff. JI Nearest ET Diff. ET\n187 182 5 200 -13\n201 204 -3 200 1\n206 204 2 200 6\n281 275 6 300 -19\n302 294 8 300 2\n314 316 -2 300 14\n374 386 -8 400 -26\n397 406 -9 400 -3\n427 427 0 400 27\n496 498 -2 500 -4\n703 702 1 700 3\n766 773 -7 800 -34\n799 792 7 800 -1\n813 814 -1 800 13\n891 884 7 900 -9\n906 906 0 900 6\n923 925 -2 900 23\n985 977 8 1000 -15\n996 996 0 1000 -4\n1009 1018 -9 1000 9\nTable 1 . Exact location of prominent peaks of hC, the near-\nest locations of theoretical just intonation (JI) and equal-\ntempered (ET) proï¬les and their respective differences (in\ncents).\nsystem could be true (Fig. 3). In general, prominent peaks\nare closer to the theoretical interval positions of a just into-\nnation tuning (Table 1). In particular, clear peaks were iden-\ntiï¬ed at 314, 427, 496, 703, 813, 906, 923 and 996 cents.\nThe correlation between the Carnatic ( hC) and the just into-\nnation ( hJ) histograms yielded a value of r(hC,hJ) = 0.552,\nwhereas the correlation between hCand the equal-tempered\ninterval histogram hEyieldedr(hC,hE) = 0.448. The dif-\nference between correlations r(hC,hJ)andr(hC,hE)was\nfound to be statistically signiï¬cant at p< 10âˆ’4, according\nto Lawleyâ€™s equicorrelation test [10].\nThe interval histogram hHobtained for Hindustani mu-\nsic, however, shows a clear tendency towards equal tem-\nperament (Fig. 4). Except for few values distributed near\ntheoretical just intonation intervals (e.g. 92, 223, 792 and\n884), the major part of the prominent peaks lie near theoret-\nical equal-tempered intervals (e.g. 300, 400, 800 and 900).\nProminent peaks for hHwere found at 96, 202, 222, 299,\n279, 395, 499, 597, 702, 794, 803, 897, 906, 1000, 1104 and\n1108. All of them have an equal-tempered location as the\nnearest one except 222, 279, 702, 906 and 1108, which have\na just intonation location as the nearest one. The correlation\nbetween hHandhEyielded a value of r(hH,hE) = 0.723,\nwhereas the correlation against hJwasr(hH,hJ) = 0.641.\nThe difference between r(hH,hE)andr(hH,hJ)was found\nto be statistically signiï¬cant at p<10âˆ’4.\n159Oral Session 2: Non-Western Music\n0 100 200 300 400 500 600 700 800 900 1000 1100 120000.20.40.60.81\nInterval [cents]Relative frequencyFigure 1 . Interval histogram for synthetic equal-tempered data ( hE; bold line). Vertical black lines correspond to the theoretical\ninterval values of equal temperament (solid lines), just intonation (dash-dotted lines) and the 22 shrutis (dotted lines). The last\ntwo overlap at many places (see text).\n0 100 200 300 400 500 600 700 800 900 1000 1100 120000.20.40.60.81\nInterval [cents]Relative frequency\nFigure 2 . Interval histogram for synthetic just intonation data ( hJ; bold line).\n0 100 200 300 400 500 600 700 800 900 1000 1100 120000.20.40.60.81\nInterval [cents]Relative frequency\nFigure 3 . Interval histogram for Carnatic music ( hC; bold line).\n0 100 200 300 400 500 600 700 800 900 1000 1100 120000.20.40.60.81\nInterval [cents]Relative frequency\nFigure 4 . Interval histogram for Hindustani music ( hH; bold line).\nThis tendency towards equal temperament may be ex-\nplained by the introduction of new instruments in Hindus-\ntani music in the last centuries. Hindustani music, unlike\nCarnatic, was more open to new inï¬‚uences, as the devo-\ntional aspect lost its importance. Most instruments used\nin Indian music have ï¬‚exible tuning capabilities [15]. One\nmajor exception is the hand-pumped harmonium, a mostly\nequal-tempered instrument introduced in Hindustani music\nin the late 19th century which is used extensively to accom-pany the singer soloist. Therefore, it is logical to think that\nsuch introduction has inï¬‚uenced the way the singers adjust\ntheir pitch. Such inï¬‚uence has yielded, according to some\nmusicians, a â€œhybrid tuning systemâ€ [9], where most singers\ntry to maintain the ï¬‚at third, but only the purist ones try to\nalso maintain the ï¬‚at second and the ï¬‚at sixth.\nGoing back to the interval histogram of Carnatic music\n(hC; Fig. 3), we see that distributions around the theoreti-\ncal interval locations are less peaky than the ones obtained\n16012th International Society for Music Information Retrieval Conference (ISMIR 2011)\n0 100 200 300 400 500 600 700 800 900 1000 1100 120000.20.40.60.81\nInterval [cents]Relative frequencyFigure 5 . Interval histogram for the fretted electric bass tracks ( hB; bold line).\n0 100 200 300 400 500 600 700 800 900 1000 1100 120000.20.40.60.81\nInterval [cents]Relative frequency\nFigure 6 . Interval histogram for the vocal Motown tracks ( hM; bold line).\nwith other sources (see below). They are ï¬‚atter, showing\nless preference for speciï¬c locations, and extending up to\nthe bounds of the just intonation ones. This partially sup-\nports the idea of swarasthana regions (Sec. 1). On the other\nhand, in the analyzed Hindustani music, we see consider-\nably peaky distributions in hH(Fig. 4), which contrasts with\nthe ï¬‚atter distributions of hC. This suggests that in our Hin-\ndustani recordings the distribution of pitches is much less\ncontinuous than in the Carnatic ones.\nAs mentioned in Sec. 1, the actual use of the 22 shruti\nscale is controversial. In the light of the results presented\nhere, we must conclude that there is no strong evidence for\nthe existence of this scale in our recordings. For the Hin-\ndustani recordings, no peaks are observed at the idiosyn-\ncratic locations we mentioned in Sec. 3.1. Thus we cannot\ndifferentiate the intervals generated by the 22 shrutis from\nthe ones generated by 12 note just intonation. As for the\nCarnatic recordings, we note that there is not much support\nfor these idiosyncratic locations. If we look at, for exam-\nple, 133, 835 or 1067 cents, we do not see any contribution\ntohC. However, small contributions seem to be made at\n337, 365 and 1178 cents. Overall, claims that the 22 shrutis\nare perceivable but not actually played (e.g. [7â€“9]) acquire\nstrength. However, this phenomenon needs to be further\nstudied, also with behavioral and perceptual studies.\n3.3 Comparison with Western practice\nTo conclude, we apply our methodology to a number of\nselected Western recordings. In particular, we select 121\ntracks of electric (fretted) bass extracted from a collection of\nmultitrack recordings of popular music pieces and 142 vocaltracks extracted from a collection of multitrack Motown re-\ncordings. These recordings comprise different commercial\nartists whose active period was between 1960 and 2000. All\ntracks were monophonic and without any sound effect that\ncould change their tuning.\nThe interval histogram of the electric bass recordings hB\nshows no surprises (Fig. 5). All prominent peaks are very\nclose to (if not exactly at) the theoretical locations of equal\ntemperament intervals. We can appreciate the peakiness of\nthe distributions around, for example, 100, 200, 300, 400\nor 500 cents. We ï¬nd correlations r(hB,hE) = 0.831and\nr(hB,hJ) = 0.478, which have a statistically signiï¬cant dif-\nference atp<10âˆ’4.\nThe interval histogram of the vocal Motown recordings\nhMpresents some subtleties worth commenting (Fig. 6). In-\ntuitively, since singers perform on top of an existing equal-\ntempered background mix, one would expect hMto have\nprominent peaks at the theoretical locations of equal tem-\nperament intervals, in the same vein as hB. Although this is\ntrue for many peaks (e.g. at 200, 300, 700 or 1000 cents), we\ncan also see some other peaks closer to just intonation po-\nsitions (e.g. at 92, 112, 406, 792 or 874 cents). Indeed, the\ncorrelations r(hM,hE)andr(hM,hJ)are very similar (0.463\nand 0.477, respectively), with a difference that is only just\nstatistically signiï¬cant ( pâ‰ˆ0.01). The issue of whether\nthere exist some traces of just intonation intervals in these\nrecordings is left for further investigation.\nThe fact that even Western Motown recordings better ap-\nproach the just intonation locations than Hindustani mu-\nsic reinforces our hypothesis that the latter has dramatically\nsuffered from equal-tempered inï¬‚uences. In addition, we\n161Oral Session 2: Non-Western Music\nshould notice that the peakiness of hHseems to be slightly\nlarger than the one of hM, which indicates that the notion of\nswarasthana regions may have been lost too.\n4. CONCLUSIONS AND FUTURE WORK\nThe results in Sec. 3 shed light on some of the existing con-\ntroversies in the tuning of sung Indian classical music. First,\nwe demonstrate how a simplistic approach using standard\ntechniques allows us to assess, in an empiric and quanti-\ntative manner, the usage of different tuning systems. Sec-\nond, we provide evidence that Carnatic music does not make\nuse of an equal-tempered tuning, showing that it presents a\nstrong correlation with 5-limit just intonation. Furthermore,\nour ï¬ndings support the notion that Carnatic music may be\nless conï¬ned to strict intervals than the other recordings we\nhave analyzed (swarasthana regions). Hindustani music, on\nthe other hand, seems to be explained by a mixture of equal-\ntempered tuning and 5-limit just intonation. In addition, we\nï¬nd prominent peaks in its interval histogram, showing a\npreference for stricter, more precise intervals. In the view of\nour analysis, the theory of the 22 shrutis lacks strong quan-\ntitative evidence. However, we cannot rule out this theory,\nsince many of the intervals overlap with the just intonation\nones.\nAs future work, to resolve this ambiguity, we could em-\nploy a method for the estimation of reference tunings [2, 6].\nIn addition, more recordings should be gathered and differ-\nent categorizations than Carnatic and Hindustani should be\nstudied (e.g. different epochs [13], different ragas [3] and\ndifferent schools [2]). Furthermore, the fundamental fre-\nquency estimation procedure could be reï¬ned [7,19], as well\nas the histogramsâ€™ construction [6]. Unsupervised clustering\ntechniques could be also introduced [3, 4]. Finally, a relax-\nation and the effect of our â€˜stability conditionsâ€™ should be\nevaluated in depth.\n5. ACKNOWLEDGMENTS\nThe authors wish to thank Enric Guaus and Perfecto Herrera\nfor useful discussions and Justin Salamon for useful discus-\nsions and proofreading. The research leading to these results\nhas received funding from the European Research Coun-\ncil under the European Unionâ€™s Seventh Framework Pro-\ngramme (FP7/2007-2013) / ERC grant agreement 267583\n(CompMusic).\n6. REFERENCES\n[1] E. M. Burns. Intervals, scales and tuning. In D. Deutsch, edi-\ntor,Psychology of Music, The , chapter 7, pages 215â€“264. Aca-\ndemic Press, San Diego, USA, 2nd edition, 1999.\n[2] A. K. Datta, R. Sengupta, N. Dey, D. Nag, and A. Muker-\njee. Study of srutis in indian musical scales and relevance ofschools in recent times using signal processing techniques. In\nProc. of the Int. Symp. on Frontiers of Research on Speech and\nMusic (FRSM) , 2003.\n[3] A. K. Datta, R. Sengupta, N. Dey, D. Nag, and A. Muker-\njee. Srutis from objective analysis of the performances of hin-\ndustani music using clustering algorithm. In Proc. of the Natl.\nSymp. on Acoustics (NSA) , 2004.\n[4] A. K. Datta, R. Sengupta, N. Dey, D. Nag, and A. Mukerjee.\nEvaluation of srutis in hindustani music from recorded perfor-\nmances. In Proc. of the Int. Symp. on Frontiers of Research on\nSpeech and Music (FRSM) , 2005.\n[5] A. De Cheveigne and H. Kawahara. Yin, a fundamental fre-\nquency estimator for speech and music. Journal of the Acous-\ntical Society of America , 111(4):1917â€“1930, 2002.\n[6] A. C. Gedik and B. Bozkurt. Pitch-frequency histogram-based\nmusic information retrieval for turkish music. Signal Process-\ning, 90:1049â€“1063, 2010.\n[7] A. Krishnaswamy. Application of pitch tracking to south in-\ndian classical music. In Proc. of the IEEE Int. Conf. on Acous-\ntics, Speech and Signal Processing (ICASSP) , volume 5, pages\n557â€“560, 2003.\n[8] A. Krishnaswamy. Pitch measurements versus perception of\nsouth indian classical music. In Proc. of the Stockholm Music\nAcoustics Conference (SMAC) , pages 627â€“630, 2003.\n[9] A. Krishnaswamy. On the twelve basic intervals in south indian\nclassical music. In Proc. of the Conv. of the Audio Engineering\nSociety (AES) , page 5903, 2004.\n[10] D. N. Lawley. On testing a set of correlation coefï¬cients for\nequality. Annals of Mathematical Statistics , 34:149â€“151, 1963.\n[11] M. Lindley. Temperaments. Grove Music On-\nline. Oxford Music Online , 2011. Available online:\nhttp://www.oxfordmusiconline.com/subscriber/article/grove/\nmusic/27643.\n[12] S. R. Mahendra, H. A. Patil, and N. K. Shukla. Pitch estimation\nof notes in indian classical music. In Proc. of the IEEE Indian\nConf. , pages 1â€“4, 2009.\n[13] D. Moelants, O. Cornelis, and M. Leman. Exploring african\ntone scales. In Proc. of the Int. Soc. for Music Information Re-\ntrieval Conf. (ISMIR) , pages 489â€“494, 2009.\n[14] N. Ramanathan. Shrutis according to ancient texts. Journal of\nthe Indian Musicological Society , 12(3):31â€“37, 1981.\n[15] P. Sambamoorthy. South Indian Music . The Indian Music Pub-\nlishing House, Chennai, India, 7th edition, 2006.\n[16] C. Schmidt-Jones. Indian classical music: Tun-\ning and ragas. Connexions , 2011. Available online:\nhttp://cnx.org/content/m12459/1.11/.\n[17] W. A. Sethares. Tuning, Timbre, Spectrum, Scale . Springer,\nBerlin, Germany, 2004.\n[18] V . Shankar. Art and Science of Carnatic Music, The . Param-\npara, Chennai, India, 1999.\n[19] D. Tidhar, M. Mauch, and S. Dixon. High precision frequency\nestimation for harpsichord tuning classiï¬cation. In Proc. of the\nIEEE Int. Conf. on Acoustics, Speech and Signal Processing\n(ICASSP) , pages 61â€“64, 2010.\n162"
    },
    {
        "title": "Complexity Driven Recombination of MIDI Loops.",
        "author": [
            "George Sioros",
            "Carlos Guedes"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1415804",
        "url": "https://doi.org/10.5281/zenodo.1415804",
        "ee": "https://zenodo.org/records/1415804/files/SiorosG11.pdf",
        "abstract": "An algorithm and a software application for recombining in real time MIDI drum loops that makes use of a novel analysis of rhythmic patterns that sorts them in order of their complexity is presented. We measure rhythmic complexity by comparing each rhythmic pattern found in the loops to a metrical template characteristic of its time signature. The complexity measure is used to sort the MIDI loops prior to utilizing them in the recombination algorithm. This way, the user can effectively control the complexity and variation in the generated rhythm during performance.",
        "zenodo_id": 1415804,
        "dblp_key": "conf/ismir/SiorosG11",
        "keywords": [
            "algorithm",
            "recombining",
            "real-time",
            "MIDI",
            "drum",
            "loops",
            "novel",
            "analysis",
            "rhythmic",
            "patterns"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\n  \n \nCOMPLEXITY DRIVEN RECOMBINATION OF MIDI LOOPS     \nGeorge Sioros   Carlos Guedes  \nUniversity of Porto  (FEUP) and INESC - Porto  \nRua Dr. Roberto Frias, s/n 4200 -465 Porto \nPortugal  \ngsioros@gmail.com   University of Porto  (FEUP) and INESC - Porto \nRua Dr. Roberto F rias, s/n 4200 -465 Porto \nPortugal  \ncguedes@fe.up.pt  \nABSTRACT  \nAn algorithm  and a software application  for recombining in \nreal time MIDI drum loops that makes use of a  novel  analy-\nsis of rhythmic patterns  that sorts them in order of their \ncomplexity  is prese nted.  We measure r hythmic complexity  \nby compar ing each  rhythmic pattern  found in the loops  to a \nmetrical template  characteristic of its time signature.  The \ncomplexity measure is used to sort the MIDI  loops  prior to \nutilizing them in the recombination algor ithm. This way , the \nuser can effectively control the complexity and variation in \nthe generated  rhythm  during pe rformance.    \n1. INTRODUCTION  \nDevising different strategies for generating rhythm in real \ntime is one of the goals of the project â€œKinetic controlle r \ndriven adaptive music composition systemsâ€ . After propo s-\ning the use of Genetic Algorithms  [1], and proposing a \nmethod for generate a metrical rhythm performance st o-\nchastically  [2], we now propose a simple yet effective \nmethod for recombining MIDI drum loops of a ce rtain style \n(such as those available in Appleâ€™s GarageBand).  We d e-\nveloped a measure of rhythmic complexity in order to sort \nthe loops prior to utilizing them in the recombination pr o-\ncess.  In this Max/MSP [3] application, the user can reco m-\nbine in real time, with different degrees of complexity, a \nbatch of MIDI drum loops in order to get non -excessively \nrepetitive combinations of loops during a performance. The \nuser can control the amount of variation in recombination \nduring performance, as well as different degrees of co m-\nplexity.   \n2. THE ALGORITHM  \nRecombinance is an effective technique to generate music \naccording to a certain style [4]. The kin.recombinator appl i-\ncation generates rhythmic patterns by recombining existing ones. The recombination process consists of playing back \nMIDI drum loop files by selecting portions of these files at \nregular intervals. An analysis of the files is perf ormed prior \nto the recombination, in order to sort them according to \ntheir complexity and, in this way, better control the resul t-\ning rhythms.     \nThe algorithm can be divided in two phases. In the first \nphase a set of MIDI drum loop files input by the user  are \nanalyzed and sorted according to how complex they are, \nfrom the simple st to the most complex. This complexity \nmeasure  is based on a new method for measuring syncop a-\ntion, by comparing the patterns against a template chara c-\nteristic of  their meter . \nIn th e second phase, the patterns are played back and r e-\ncombined. A new pattern is selected for playback  on every \nbeat. Playback is performed in a cyclic way;  it restarts  when \nreaching the end of the file. When a new a file is s elected, \nplayback always continue s at the beat from where it was left \nin the previous file , always preserving the me trical position . \nThe user controls the complexity of the r esulted rhythm and \nthe amount of variation by d etermining which patterns get \nselected for playback in an easy and i ntuitive way based on \ntheir order of complex ity. \n2.1 Calculation of the complexity scores and sorting of \nMIDI drum loops  \nVarious  approaches  for measuring complexity  in rhythmic \npatterns  exist , such as pattern matching  techniques  [5], \nrhythmic  syncopation measures  [6] and analysis of  the \nmathematical or geometrical properties of the patterns  \n[6][7]. Here, we define  a new one, which is based on  the \nsame principle as  G. Toussaintâ€™s  metric complexity  [6], \nwhich  is a comparison of a rhythmic  pattern against a  tem-\nplate chara cteristic of  its meter.  Unlike Toussaintâ€™s a p-\nproach that uses the template as a way of calculating the \nmetrical accents, we use the template as the fundamental \ntool for analyzing and defining relationships between the \nevents comprising the pattern. Moreover, unlike most met h-\nods for measuring complexity, which use a binary represe n-\ntation of the pa tterns and ignore the amplitudes of the \nevents that comprise the pattern, we take into account the \nrelative amplitudes of the events in our calculation .  \nPermission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fee provided that copies are \nnot made or distributed for profit or commercial advantage and that co p-\nies bear this notice and the f ull citation on the first page.   \nÂ© 2011  International Society for Music Information Retrieval  \n381Poster Session 3\n  \n \n The user provides rhythmic patterns in the form of a co l-\nlection of MIDI files. The MIDI files are read and are qua n-\ntized according to a fixed quantization grid. This way each \nmeasure in a pattern is subdivided into pulses according to \nthe time signature and the quantization grid.  The quantiz a-\ntion grid value we use is the 32nd note. Deviations from that \ngrid are co nsidered to be  micro -timing deviations  and they \nare not treated in the current analysis.  An amplitud e value  is \nassigned to each pulse , according  to the MIDI v elocities \nfound at that time p osition  after the quantization .  \nA metrical template that defines metrical hierarchy is \nconstructed by stratifying the m eter found in the MIDI file s. \nThis template co nsists of the metrical  levels which comprise \nthe meter. A â€œmetrical accentâ€  value is calculated and a s-\nsigned to each pulse according to the metrical level it b e-\nlongs to . Each rhythmic pattern  found in the MIDI files is \ncompared  against  the metrical template  yielding a  separate \nscore for each pulse  in the pattern . The result is further fi l-\ntered by  the aforementioned values of the metrical accents . \nThe calculated  scores can be thought of as a measure of \nhow much each pulse contradicts the metrical stru cture d e-\nscribed by the template and, in this sense,  how much each \npulse contributes to the syncopation of the pattern. Finally, \na complexity score is assigne d to each MIDI file taking into \naccount , in addition to the syncopation , the density of MIDI \nevents in each file.  \n2.1.1 Constructing  rhythmic patterns  from MIDI files  \nThe user provides the MIDI drum loops as a set of MIDI \nfiles with the same bar -length and time signature. The MIDI \nnote-on events are extracted from each file  and their posi-tions are  quantized to  a 32nd note grid. The lists of ampl i-\ntudes for each pulse in the meter (i.e., their velocity value) \nmake the rhythmic patterns to be constructed. It is common \nin MIDI drum loops that each different MIDI note number \ncorrespond s to a di fferent timbre, e.g. one not e number for \na kick drum sound and another for a hit cymbal . We con-\nstruct a different rhythmic pattern for each MIDI note num-\nber according to the MIDI  velocity and time position of any \nnote-on events corresponding to that note number . The f inal \ncomplexity score calculated for the MIDI file is obtained  by \naveraging the scores of each rhythmic pattern co nstructed \nfrom the file. \nTwo more ways of translating MIDI note -on events into \nrhythmic patterns are provided: one for single -timbre MIDI \ninstruments, where a ll notes correspond to the same timbre  \nwith different pi tch, and one for a general MIDI drum l i-\nbrary. For single -timbre instruments, MIDI note nu mbers \nare ignored and a single rhythmic pattern is co nstructed by \ntaking the maximum MIDI v elocity in each puls e. For the \nGeneral MIDI drum library case,  different groups of MIDI \nnote numbers  correspond to  different patterns , e.g. all hi -hat \nnotes are grouped together to construct one rhythmic pa t-\ntern while other notes like bells are treated each one sepa-\nrately.  \n2.1.2 Constructing  the metrical te mplate  \nA metrical structure as the one described by F. Le rdahl and \nR. Jackendoff [8] can be constructed for each  meter found \nin the MIDI files. It must be noted that in order for the me t-\nrical structu re to be meaningful ly in relation to the rhythmic \npattern s extracted from a MIDI file, we must a ssume that \nthe meter of the template is in fact the meter of the pattern s; \nthat is, the time sign atures found in the MIDI files are the \nactual time signatures o f the drum loops co ntained in the \nfiles.  \nThe meter is stratified  into metrical  levels in a hiera r-\nchical manner so that each pulse belongs to a specific me t-\nrical level and all lower ones. In order to stratify the meter \nthe nu mber of pulses is decomposed into prime factors  (see \nFigure  1). Each prime factor describes how each stratific a-\ntion level is subdivided.  Different permutations of the prime \nfactors describe different metrical hierarchies . The stratif i-\ncation process is described in detail  in [9]. A simplified ve r-\nsion can also be  found in [6]. A metrical accent value is a s-\nsigned to each pulse based on the stratification level  i it be-\nlongs to, following an exponential equ ation: \ni\niM 5.0ï€½\n   (1) \nSince the number of pulses in a certain meter is dete r-\nmined by the quantization grid , the lowest stratification le v-\nel will always correspond to  that grid.  For the sake  of sim-\n \nFigure  1. Stratification of a 3/4 meter to the 16th note le vel. \nEach pulse belongs to a stratification level and all lower \nones.  \n38212th International Society for Music Information Retrieval Conference (ISMIR 2011)\n  \n \nplicity , in the examples presented  here we omitte d the 32nd \nnote metrical level.  \n2.1.3 Comparing the rhythmic  patterns to the te mplate  \nThe comparison of a rhythmic pattern to a metric al template \nis essentially  a process of spotting  the pulses in the rhythmic \npattern that contradict the prevailing meter d escribed by the \ntemplate.  In that sense, these pulses are mainly responsible \nfor any syncopation  present in the rhythmic pattern  and t he \nresult of the comparison is a mea sure of that syncopation . \nEliminating the events that occur regularly on the beat in \nsome metrical level helps distinguishing the sy ncopating \npulses.  Events  that occur regularly  on the beat do not gene r-\nate syncopation. The remaining events would be isolated \nones, that mostly occur in low metrical lev els, i.e. in â€œoff -\nbeatâ€ positions , and that contribute s to the complex ity more \nactively . For example, a cymbal hit on every quarter note \nbeat with more or less the same amplit ude contri butes less \nthan a snare  that happens only at sp ecific off -beat posi tions  \nand contributes more to a more complex  rhythm .  \nIn order to define how each pulse  contribut es to a steady \nbeat, its relation to the rest of the pulses must be exa mined.  \nWe examine the relations between the pulses of the pattern \nin light  of the metrical template, taking  advantage of its  hi-\nerarchical char acter.  Each metrical level is examined sep a-\nrately, so that each pulse in a pattern is assigned a separate \nscore for each met rical level it b elongs to. A low score in a \nmetrical level signifies that the pulse co ntributes to a steady \nbeat in that level and therefore does not contradict the m e-\nter, e.g. a quarter note surrounded by equally loud quarter \nnotes has a low score in the q uarter note metrical level, \nwhile a loud quarter note with no neighbors will have a \nhigher score. After examining all metrical levels , the min i-\nmum score for each pulse is kept.  \nThe score is calcu lated as the  average  difference of  the \nampli tude of the pulse under consideration from the ampl i-\ntudes of the neighbor pulses in each metrical level. In the \nexample of Figure 2, pulse number 5 gets three  scores, one \nfor each metric al level it belongs to, namely that of the \nquarter note  (1), the eighth  note (2) and the 16th note (3). \nEach score is the average of  the two  differences of ampl i-tudes between pulse 5 and i) pulses 1 and 9 for metrical \nlevel 1 , ii) pulses  3 and 7 for level 2 and, finally, iii) pulses \n4 and 6 for level 3. On the other hand, pulse 10 gets only \none score (metrical level 3) arising from the two differences \nfrom pulses 8 and 11.  Negative differences  are always  set \nequal to zero.  \nOne important feature of the metrical structure of the \ntemplate  is the alternation of metrical levels, in other words , \nthe highest metrical level of a pulse  is always di fferent from \nthe one s of its immediate neighbors.  As a consequence, \npulses in low metrical levels are always surrounded by \npulses in higher levels  and in most cases  their neighbors al-\nso belong  to different metrical levels  (see pulse 10 in Figure \n2). An isolated event in such a pulse pro duces a strong  syn-\ncopation feel . This syncopation  is stronger  when the diffe r-\nence of the metrical level s of the pulse s is larger . Compare, \nfor example, any of the pulses 4, 6, 8 or 10 to pulses 2 or \n12. In the absence of an event in pulse 1, pulses 2 and 12 \ncreate a stronger contradiction to the meter b ecause pulse 1 \nbelongs to the highest metrical level  (see Figure 3, A and \nB). Having this in mind, we introduced a weighting  factor in \nthe amplitude differences calculated above.  This factor is \nproportional to  the difference of the highest me trical level \nof the two pulses  that the amplitudes are taken from . This \nway the amplitude difference between pulse 2 and 1 has \nmore weight than the one between 6 and 5. Similarly, the \namplitude difference between pulse 5 and 1 has more \nweight than that between 5 and 9.  \nAn important aspect of rhythmic patterns is the direction \nin which  they are always performed. Time in music, as in \neverything else, flows in only one direction. Pulses succeed \none another in a specific order. The relation of a pulse to its \nprevious pulse is not equivalent to that to the following \npulse. Two equally loud events one after another create the \nimpression of an accent on the second event rather than on \nthe first. Let us return to our example template of Figure 2. \n \nFigure 2. Pulse number 5 belongs to metrical levels 1, 2 \nand 3. Pulse 10 belongs only to level 3.  \n \nFigure 3. Left: The contradiction of the rhythmic p attern to \nthe meter is greater in pattern B than in A since pulse 1 b e-\nlongs to a higher metrical level than pulse 5.  Right:  A loud \nevent before an a ccent (C ) enforces the accent while  a loud \nevent after an accent (D ) weakens the accent.  \n383Poster Session 3\n  \n \nConsider the relation between pulse 7 and pulses 5 or  9. A \nloud event on pulse 7 affects differently a loud event on \npulse 5 from one on pulse 10 (see  Figure 3, C and D ). In \norder to take into account this fact in our evaluat ion of how \nmuch an event contradicts the meter, the weights discussed \nin the amplitude differences calculations need to be mod i-\nfied, so that a smaller weight is to be given to the amplitude \ndiffe rence with the previous pulse than with the following \none. \nThe various pulses in a rhythmic pattern have  a diffe rent \npotential  in contradicting the prevailing meter , or syncopa t-\ning, depending on whi ch metrical levels they belong to. \nThis syncopation  potential can be thought of as the o pposite \nof the metrical accent , which essentially is the pote ntial of a \npulse to contribute to a steady beat.  A loud ev ent in a pulse \nthat belongs to a high metrical level  (high met rical a ccent),  \ndoes not have a lot of  â€œchance â€ of contradicting the meter, \neven if it is isolated without any events in its v icinity . The \nscores calculated above represent how much a pulse contr a-\ndicts the meter with respect to its relation to its neighbors  \nbut do not take into account this syncopation p otential.  We \ntherefore  multiply the scores previously calc ulated by a fac-\ntor proportional to the inverse of the metrical accent of \nequation (1). This way a pulse that belongs to metrical level \n0 could never contradict the meter, irrel evant of if an event \nexists in this or any other pulse s of the meter. Of course,  the \nabsence of an event in a high level pulse creates the poss i-\nbility for an event in some other pulse, prob ably of a low \nmetrical level, to produce  a strong syncopation , but this is \nreflected on the syncopation  potential of the low metrical \nlevel pulse.  \nThe last step taken in the calculation of our syncopation \nmeasure is to sum the scores of all pulses in the pattern and \nnormalize the result.  Normalization is performed by divi d-\ning the result by the maximum possible  sum for the meter \nand bar -length. This maximum is calculat ed by co mparing  \nagainst the metrical templ ate a pattern  in which  all pulses of \nthe lowest metrical level have maximum ampl itudes and all \nother pulse s have zero amplitude . \nAs it was described in the previous paragraphs , the \nweights used in the amplitude dif ferences are calculated a c-\ncording to the metrical levels of the pulses and to whether \nthe difference is taken from the previous or the following  \npulse. We set the exact weights empirically, by experimen t-\ning with various combinations. For differences betwee n \npulses of the same metrical level the weight was set to be \nhalf of that between pulses which belong to the two extreme \nmetrical levels. The difference from the prev ious pulse was \nset to the 80% of the weight of that from the next pulse.  2.1.4 Examples of  measu ring syncopation  \nIn this section an illustrative example of the method for \nmeasuring syncopation is given. A short evaluation of the \nmethod follows, by measuring the syncopation of six clave \nand bell rhythms of the African, Brazilian and Cuban music.  \nA sim ilar comparison of other syncopation and complexity \nmeasures based on these pa tterns can be found in [6]. \nThe two patterns  of Figure 4 are com pared against  the \nsame metrical template, namely that of a 3/4 meter. The o n-\nly difference between the two patterns is  found  in pulse 9, a \npulse that belongs to a high metrical level.  In pattern A \npulse 9 is silent while in pattern B it has maximum ampl i-\ntude. Although this difference does not cause any changes \nin the score of that pulse, it affects drastically the scores of \nthe other pulses in the patterns. The two immediate neig h-\nbors, pulse 8 and 10, both have maximum amplitudes. \nWhen no event exists in pulse 9 , both pulses have  high \nscores  since no events exist in  their vicinity, with that of \npulse 8 being a little higher. In pattern B, the ampl itude of \npulse 9 causes both scores of pulses 8 and 10 to drop. This \ndrop is larger for pulse 8, so that, pulse 8 now has a smaller \nscore than pulse 10. This inversion in th e relation of the \nscores of the two pulses is the result of the ampli tude \nweights used . In the absence of an event in pulse 9, the pr e-\nvious pulse creates a stronger syncopation. In the pre sence \nof high amplitude in pulse 9, the following pulse weakens \nthe accent, while the previous one tends to enforce it.  Alt-\nhough the difference is small, it can be of importance when \nsorting drum loops  of the  same  music style  with little diffe r-\nences. \nIn the absence of an event  in pulse 9, a small contrib u-\ntion in the total syncopation of pattern A arises in pulse 5. \nPulse 5 and 9 are related through the quarter note me trical \nlevel. The contribution is small for two reasons. On one \nhand , because of the high amplitude of pulse 1 which b e-\nlongs to a higher metrical level and the refore gets a higher \nweight than pulse 9. On the other hand, pulse 5 belongs to a \npulse with a high metrical accent, so that its syncopation \n \nFigure 4. Two patterns (grey bars) compared to the same \nmetrical template (dashed squares). The black bars repr e-\nsent the score of each separate pulse. The total syncopation \nscore  is shown above each pattern.  \n38412th International Society for Music Information Retrieval Conference (ISMIR 2011)\n  \n \npotential is  low. \nIn order to evaluate the syncopation measure described \nabove, we measured six clave and bell rhythmic patterns, \nnamely the Shiko, Son, Soukous, Rumba, Gahu and Bossa -\nNova. These patterns are all five-note patterns with 4/4 time \nsigna ture and are some of the most frequently used in the \nAfrican, Cuban and Brazilian music.  \nIn Figure 5 the six patterns are presented together with \ntheir syncopation score s. Dynamic accents were not consi d-\nered;  all pulses have eithe r maximum amplitude or are  \ncompletely  silent . The black bars represent the relative co n-\ntribution of each pulse to the total syncopation score.  The \nscores obtained by the calcul ations  seem to agree with  our \nexperience that Shiko is the easiest pattern , Rumb a is of \nmedium complexity and Gahu and Bossa -Nova are  amongst  \nthe most difficult to perform.  The order  from simple to \ncomplex is also in agreement with the cognitive complexity  \nmeasure  proposed by J. Pressing (see [5] and [6]).  \n2.1.5 Calculating the complexity of a MIDI file  \nThe complexity of a drum loop can be thought of as a ve c-\ntor in a two dimensional space, where one dimension is the \ndensity of the events and the other is th e syncopation. The \nlength of the vector is the complexity  score . The syncop a-\ntion measure is already no rmalized and can be directly used \nas one of the coordinates of the vector. The density of \nevents is calculated as the sum of all the MIDI velocities \nfound  in the MIDI file. This sum represents an effective \ndensity, since  it does not correspond to  the number of \nevents  in the pattern . This number needs to be normalized \nbefore it can be used as a c oordinate in our complexity \nplane. We normalize the density by dividing with the largest density value in the colle ction of MIDI files provided by the \nuser. The total complexity of a file is then calculated as: \n2 2n syncopatio density Complexity ï€« ï€½\n        (2) \nThis is the value used to sort the MIDI files , from the \nmost simple to th e most complex .  \n2.2 Recombining the rhythmic patterns  \nThe sorted MIDI files form  a two dimensional space, where \nthe vertical dimension represents their order of complexity \nand the horizontal represents their evolution in time (see \nFigure 6). All files share the same time signature and have \nthe same bar -length and, therefore, are perfectly aligned.  \n A global transport controls the current playback pos ition \nwhich is common to all files. The tempo of each file is i g-\nnored and the playback  follows the transport's tempo, co n-\ntrolled in real time by the user. Playback is performed in a \nloop. At every beat , a new file is randomly selected and \nplayback continues in this file at the current tr ansport's pos i-\ntion (see Figure 6), preserving always the metrical position .  \nThe d uration of the recombination beat is defined according \nto the time signature, e.g.  in a 4/4 meter , the beat would \ncorrespond to the quarter notes.  \nInstead of selecting  a file out of the whole collection  of \nthe provided MIDI files , the selection process is restricted \nto a smaller collection of files. During the pe rformance, the \nuser controls the resulted rhythm by contro lling in real time \nthe range of files, from the simplest to the most complex \nthat can be selected for playback. Increasing the range leads \nto more variation in the resulted rhythm, while moving the \nentire range vertically to more or less complex patterns co n-\ntrols the complexity of the rhythm.  \nThe out put of the recombination algorithm undoub tedly \ndepends on the provided MIDI files. In order for the output  \nto be coherent , all files should belong to the same music \nstyle. When the files have a similar structure, either a large \n \nFigure 5. The syncopation of the six fundamental 4/4 clave \nand bell p atterns is measured.  \n \nFigure 6. The rhythmic patterns , sorted from simple to \ncomplex,  are selected for playback at regular intervals.  An \nexample of three patterns ( middle)  and their recombination  \n(right) for two complexity values , simple (bottom) and \ncomplex (top)  are shown.  \n385Poster Session 3\n  \n \nscale structure consisting of several bars, or at the beat le v-\nel, thi s structure will be reflected also in the outcome of the \nrecombination. This comes about as a direct result from u s-\ning a global transport to control playback.   \n3. MAX/MSP APPLICATION  \nThe algorithm has been implemented as the \nkin.recombinator  Max/MSP a pplicat ion. A collection of \nMax/MSP externals, java classes suitable to be loaded  to \nthe mxj Max/MSP object and Max/MSP abstractions were \ndeveloped as parts of the a pplication.  \nThe user drags and drops a folder containing MIDI files \ninto the Max/MSP application . The files are automatically \nsorted and the global Max/MSP transport controls pla y-\nback. The user can graphically  control the range of files b e-\ning recombined at any one time with a range slider  like the \none in  Figure 7.  \nThe MIDI f iles are read and quantized to the 32nd note \nlevel by the java class kinMIDIFileReader . Rhythmic pa t-\nterns are constructed in the form of lists of amplitudes and \nare passed together with the r espective time signatures to a \nsubpatch where the  effective densi ty and  syncopation score \nare calculated  by the kin.OffBeatDetector  Max/MSP exte r-\nnal.  \nThe score is then stored in a collection object. After fi n-\nishing calculating the scores for all the files, the files are \nsorted according to their scores.  \nThe kin.Recombi neMIDIFiles  abstraction is performing \nthe playback.  It selects at every beat a new file for playback \nwhich is read by the kin.MIDIFileReader . \nThe kin.recombinator  Max/MSP application , as well as a \nMax/MSP patch for testing out the  syncopation measure can \nbe downloaded at the groupâ€™ s web si te: \nhttp://smc.inescporto.pt/kinetic/  \n 4. ACKNO WLEDGMENTS  \nThis research was done as part of the project â€œKinetic co n-\ntroller driven adaptive music composition systemsâ€, (ref.  \nUTAustin/CD/0052/2008), supported by the Portuguese \nFoundation for Science and Technology for the UT Austin| \nPortugal partnership in Digital Media.  \n5. REFERENCES  \n[1] G. Bernardes, C. Guedes and B. Pennycook: â€œStyle \nemulation of drum patterns by means of evolutio nary \nmethods and statistical analysis.â€ Proceedings of the \nSound and Music Computing Conference (SMC).  Bar-\ncelona, Spain, 2010  \n[2] G. Sioros and C. Guedes: â€œ Automatic Rhythmic Pe r-\nformance in Max/MSP: the kin.rhythmicator â€, Pro-\nceedings of the International Confe rence  on New In-\nterfaces  for Musical Expression , Oslo, Norway, 2011   \n[3] http://www.cycling74.com  \n[4] D. Cope: Experiments in Musical Intelligence , Middle-\nton, A -R Editions , 1996  \n[5] J. Pressing:  â€œCognitive complexity and the st ructure of \nmusical patterns â€, Proceedings of the 4th Conference \nof the Australian Cognitive Science Society , Newca s-\ntle, Au stralia , 1997  \n[6] G. T. Toussaint:  \"A mathematical analysis of African, \nBrazilian, and Cuban clave rhythms\" , Proceedings of \nBridges: Mathe matical Connections in Art, Music, and \nScience , Towson University, Baltimore, Maryland, J u-\nly 27 -29, 2002  \n[7] F. GÃ³mez, A. Melvin, D. Rappaport, and G. Tou ssaint: \nâ€œMathematical measures of syncopationâ€, In \nBRIDGES: Mathematical Connections in Art, Music \nand Sc ience, p. 73 â€“84, Jul 2005.  \n[8] F. Lerdahl & R. Jackendoff : A Generative Theory of \nTonal Music , Cambridge, The MIT Press, 1996  \n[9] C. Barlow: â€œTwo essays on theoryâ€,  Computer Music \nJournal, 11, 44 -60, 1987  \n \nFigure 7. The kin.recombinator user interface. During the \nperformance the complexity and amount of variation can be \ncontrol led graphically with a range slider.  \n386"
    },
    {
        "title": "Tarsos a Platform to Explore Pitch Scales in Non-Western and Western Music.",
        "author": [
            "Joren Six",
            "Olmo Cornelis"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1418355",
        "url": "https://doi.org/10.5281/zenodo.1418355",
        "ee": "https://zenodo.org/records/1418355/files/SixC11.pdf",
        "abstract": "This paper presents Tarsos 1 , a modular software platform to extract and analyze pitch and scale organization in music, especially geared towards the analysis of non-Western music. Tarsos aims to be a user-friendly, graphical tool to explore tone scales and pitch organization in music of the world. With Tarsos pitch annotations are extracted from an audio signal that are then processed to form musicologically meaningful representations. These representations cover more than the typical Western 12 pitch classes, since a fine-grained resolution of 1200 cents is used. Both scales with and without octave equivalence can be displayed graphically. The Tarsos API 2 creates opportunities to analyse large sets of ethnic music automatically. The graphical user interface can be used for detailed, manually adjusted analysis of specific songs. Several output modalities make Tarsos an interesting tool for musicological analysis, educational purposes and even for artistic productions.",
        "zenodo_id": 1418355,
        "dblp_key": "conf/ismir/SixC11",
        "keywords": [
            "Tarsos 1",
            "modular software platform",
            "pitch and scale organization",
            "non-Western music",
            "user-friendly",
            "graphical tool",
            "tone scales",
            "pitch organization",
            "world music",
            "Tarsos API"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nTARSOS - A PLATFORM TO EXPLORE PITCH SCALES IN NON-WESTERN\nAND WESTERN MUSIC\nJoren Six and Olmo Cornelis\nRoyal Academy of Fine Arts & Royal Conservatory,\nUniversity College Ghent\njoren.six@hogent.be -olmo.cornelis@hogent.be\nABSTRACT\nThis paper presents Tarsos1, a modular software platform\nto extract and analyze pitch and scale organization in mu-\nsic, especially geared towards the analysis of non-Western\nmusic. Tarsos aims to be a user-friendly, graphical tool to\nexplore tone scales and pitch organization in music of the\nworld. With Tarsos pitch annotations are extracted from an\naudio signal that are then processed to form musicologically\nmeaningful representations. These representations cover more\nthan the typical Western 12 pitch classes, since a ï¬ne-grained\nresolution of 1200 cents is used. Both scales with and with-\nout octave equivalence can be displayed graphically. The\nTarsos API2creates opportunities to analyse large sets of -\nethnic - music automatically. The graphical user interface\ncan be used for detailed, manually adjusted analysis of spe-\nciï¬c songs. Several output modalities make Tarsos an inter-\nesting tool for musicological analysis, educational purposes\nand even for artistic productions.\n1. INTRODUCTION\nA 2007 f(MIR) article by Cornelis et al. [3] argued that\naccess to ethnic music could become one of the next big\nchallenges for the MIR community. It gives an overview\nof the difï¬culties of working with ethnic music: i) There is\nan enormous variety of styles, timbres, moods, instruments\nfalling under â€™ethnic musicâ€™ umbrella. ii) The absence of a\ntheoretical framework and a different attitude towards mu-\nsic imply that western music-theory concepts do not always\napply. iii) A third factor that complicates access to ethnic\n1Tarsos is open source and available on http://tarsos.0110.be .\nIt runs on any recent Java Runtime and can be started using Java Web Start.\n2With the Application Programmers Interface tasks can be automated\nby programming scripts. For an application see chapter 5.\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proï¬t or commercial advantage and that copies\nbear this notice and the full citation on the ï¬rst page.\nc/circlecopyrt2011 International Society for Music Information Retrieval.\nFigure 1 . A visualization of the music archive of the Royal\nMuseum for Central Africa. The dots mark places where\nrecordings have been made.\nmusic is its distribution. Archives of ethnic music are often\nnot or not yet digitized, badly documented, and when meta-\ndata is available terminology and spelling may vary. These\nthree elements cause a lack of ground truth, which makes\n(large scale) research challenging.\nThere are difï¬culties working with ethnic music but there\nis also a lot of potential. While some archives with ethnic\nmusic are being digitized, the need for specialized MIR ap-\nplications becomes more apparent. Ethnic music offers a\nunique environment of characteristic timbres, rhythms and\ntextures which could use adapted or completely new, inno-\nvative tools. The potential of computational research within\nthe context of ethnic music has been stressed by the intro-\nduction of the term Computational Ethnomusicology [13].\nHopefully this new interdisciplinary (sub)ï¬eld can give an\nimpulse to the study and dissemination of a rich heritage of\nmusic that is now hidden in archives and aid or even stimu-\nlate new musicological ï¬eld work.\nThis research focuses on one of those unique collections\nof ethnic music: the audio archive of the Royal Museum for\n169Oral Session 2: Non-Western Music\nCentral Africa (RMCA) in Belgium. It is one of the largest\ncollections worldwide of music from mainly Central Africa.\nFigure 1 displays the geographical distribution of the audio\ncollection3that consists of about 50,000 sound recordings\n(with a total of 3,000 hours of music), dating from the early\n20th century up to now. A selection of this data set with\nAfrican music has already been used for a study on pitch\norganization and tone scales [9].\nThis paper is structured as follows: After this introduc-\ntion sketching the background for this research the follow-\ning chapter identiï¬es the need for precise pitch analysis and\nthe rationale behind the development of Tarsos. Chapter\nthree will provide a view on the method we use, and give\na brief overview of related work. Chapter four documents\nthe structure and method of the Tarsos platform. Example\napplications of Tarsos can be found in chapter ï¬ve. The ï¬nal\nchapter gives a conclusion and ponders on future work.\n2. SCALE ORGANISATION\nForWestern music pitch organization in music relies on a\nwell-deï¬ned, historically grown music theory. Nowadays\nalmost all western music relies on a division of the octave in\n12 equal parts. Only few composers have explored different\ndivisions of the octave (e.g. Darreg, Partch).\nInnon-Western classical music , tone scale organization\nleans on an, often very different, theoretical system than the\nWestern equal temperament. The most outspoken difference\nis that not all pitch intervals have an equal size. This can\nresult in an explicitly sought musical tension. An example\nis the unequal octave division of the Indonesian gamelan\nPelog scale.\nOral music traditions however, rely almost exclusively\non musical practice. Without a written musical theory the\nmaster-student relationship becomes very important, together\nwith the societal context in which people hear music. An\noral culture does not support the development towards a pol-\nished music theory but grows more organically. These fac-\ntors deï¬ne the speciï¬c characteristics of the music itself:\nless harmonic impact, instruments with varying tuning, no\nharmonic modulation and a large number of different tone\nscales. Until now, far too little attention has been paid to\nthis tone scale diversity. There is a need for a system that can\nextract pitch organisation - scales - from music in a culture-\nindependent manner.\nCurrently there is software available for pitch analysis\nbut it mainly focuses on Western music and is used for e.g.\nkey-detection in pop music. To ï¬ll the need for automated\npitch analysis of ethnic music Tarsos has been developed.\nTarsos creates opportunities to analyse pitch organization in\n3There is a website featuring complete descriptions and audio frag-\nments, it can be found at http://music.africamuseum.be0107 363 585 833 1083A100200300400500600\nPitch (cent )Number of annotations\nFigure 2 . A pitch class histogram that shows how much\npitch classes are present in a piece of music. The graph\nshows absolute pitch annotations collapsed to one octave.\nThe circles mark the most used pitch classes. For reference,\nthe dashed lines represent the Western equal temperament.\nThe pitch class Ais marked with a dotted line.\nlarge music archives, document tone scales and ï¬nd patterns\nin pitch usage.\n3. PITCH ANALYSIS\nThe basic idea behind the method we use is simple: count\nhow many times each fundamental frequency is repeated\nthroughout an audio signal and represent this data in a use-\nful way. This method has a long tradition and historically\nthis was done by hand, or, more anatomically correct, by\near. Each tone in a musical piece was compared with a large\nset of tuned bells and every match was tallied. This method\nis very labour-intensive and does not scale to large music\narchives.\nAlready in the late sixties researchers automated this pro-\ncess to study the tone scale of a Swedish folk instrument\n[11]. Since then various terms have been introduced to de-\nscribe this, or closely related ideas: Frequency Histogram\n[11], Chromavector [9], Constant-Q Proï¬le [10], Harmonic\nPitch Class Proï¬le [5] and Pitch-frequency Histogram [6].\nWorking with ethnic music, and especially African mu-\nsic, it is important that the pitch organization diversity can\nbe captured. In [9] this is done as follows. At ï¬rst the au-\ndio is analysed in blocks of 10ms and for each block a fun-\ndamental frequency estimation is made. Secondly, the fre-\nquencies are converted to the cents scale with C0set to zero\ncents while maintaining a list with the number of times each\nfrequency occurs. And ï¬nally the listed values are reduced\nto one octave. This results in a quasi-continuous pitch class\nhistogram of 1200 values as seen in Figure 2. With Tarsos\nthis method is automated in a ï¬‚exible way.\nPitch class histograms can be used for various applica-\ntions. The most straightforward application is tone scale\n17012th International Society for Music Information Retrieval Conference (ISMIR 2011)\ndetection. To extract a scale from a pitch class histogram\npeak extraction is used: i.e. ï¬nding the circles in Figure 2.\nWith the pitch classes identiï¬ed a pitch interval matrix can\nbe constructed and subsequently used for comparison and\nanalysis.\n4. TARSOS PLATFORM\nThe main contribution of this paper is Tarsos: a platform for\npitch analysis. It makes the methods described in [6, 9] eas-\nier to use and therefore accessible to a larger audience. Es-\nsentially Tarsos tries to make one-off studies of pitch usage\neasily repeatable, veriï¬able and scalable to large data sets.\nThe functions of Tarsos will be explained using the block\ndiagram in Figure 3. This should make the information ï¬‚ow\nclear and provide a feel on how Tarsos can be used.\n4.1 Input\nAs input Tarsos accepts audio in almost any format. All au-\ndio is transcoded to a standardized format. The conversion\nis done using FFMPEG4, the default format is PCM WAV\nwith all channels are downmixed to mono.\nAnother input modality are Scala ï¬les. Scala ï¬les are\nstandardized text ï¬les which contain tone scale information.\nThe ï¬le format is deï¬ned by the Scala program. Quoting\nthe Scala website: http://www.huygens-fokker.\norg/scala/\nâ€œScala is a powerful software tool for exper-\nimentation with musical tunings, such as just\nintonation scales and non-Western scales. It\nsupports scale creation, editing, comparison, anal-\nysis, storage, . . . Scala is ideal for the explo-\nration of tunings and becoming familiar with\nthe concepts involved.â€\nThe Scala program comes with a dataset of over 3900 scales\nranging from historical harpsichord temperaments over eth-\nnic scales to scales used in contemporary music. Tarsos can\nparse and export scala ï¬les. Their use should become appar-\nent in section 4.5.\n4.2 Analysis\nDuring analysis each block of audio is examined and zero,\none or more fundamental frequencies are assigned. The\nblock size and the number of extracted frequencies depend\non the underlying fundamental frequency detection algo-\nrithm. Several detection algorithm implementations are dis-\ntributed together with Tarsos and thanks to its modular de-\nsign new ones can be added. For practical purposes platform-\nindependent - pure Java - implementations of YIN [4] and\n4FFmpeg is a complete, cross-platform solution to record, convert audio\nand video. It has decoding support for a plethora of audio formats.INPUT\nANALYSIS\nREPRESENTATION\nOPTIMIZATION\nOUTPUTAnnotation\nselection\nPeak\nselection\nAuditory\nfeedbackScala\nFigure 3 . The main ï¬‚ow of information within Tarsos.\nMPM [8] are available without any conï¬guration. Currently\nthere is also support for the MAMI-detector [2] and for any\nV AMP-plugin [1] that generates frequency annotations. These\nexternal detectors are platform dependant and need some\nconï¬guration but once correctly conï¬gured their use is com-\npletely transparent: the generated annotations are transformed\nto a uniï¬ed format, cached and then used for representation.\n4.3 Representation\nThe most straightforward representation of pitch annotations\nis plotting them over time. This results in a piano-roll like\nview. In monophonic music this visualizes the melody. In\npolyphonic music it shows information about harmonic struc-\ntures and the melodic contour. The piano-roll aids transcrip-\ntion and makes repeating melodic patterns visible. Figure 4\nshows a screenshot of Tarsos, the piano roll representation is\nmarked with 3. With the interactive user interface the piano\nroll representation can be used to select an area of annota-\ntions you are interested in. This can be used to ignore anno-\ntations below a certain pitch threshold (e.g. pitched percus-\nsion) or to compare the ï¬rst part of a song with the second\npart. The selection - represented by the upwards arrow be-\ntween analysis and representation in Figure 3 - inï¬‚uences\nthe next representation.\n171Oral Session 2: Non-Western Music\nFigure 4 . A screenshot of Tarsos: 1) a pitch class histogram, 2) a pitch class interval table, 3) a piano roll like view on\nannotations, 4) a MIDI keyboard and 5) a waveform. Tarsos is available on http://tarsos.0110.be .\nWithin Tarsos the pitch histogram is constructed by as-\nsigning each annotation to a bin between 0 and 12Ã—1200 =\n14400 cents, spanning 12 octaves. The height of each peak\nrepresents the total duration of a particular detected absolute\npitch within a piece. As mentioned in section 3 to transform\nthe pitch histogram to a pitch class histogram all values are\nfolded to one octave. In the pitch class histogram a peak\nrepresents the total duration of a detected pitch class within\na piece. An example of a pitch class histogram can be seen\nin Figure 2 or the area marked with 1 in Figure 4.\nA more high level, musicologically more meaningful rep-\nresentation is the pitch interval matrix . It is constructed by\napplying automatic or manually adjusted peak detection on\nthe pitch class histogram and extracting the positions of the\npitch classes. It contains the tone scale of a song and the\nintervals between the pitch classes.An example of a pitch\ninterval matrix extracted from the pitch class histogram in\nFigure 2 can be seen in Table 1. In the screenshot, Figure 4\nit is marked as 2.\n4.4 Optimisation\nAutomatic peak extraction may yield unwanted results. There-\nfore there is a possibility to adjust this process manually.\nAdding, removing or shifting peak locations is possible with\nthe pitch class histogram user interface. Changing the posi-\ntion of a peak has an immediate effect on all other represen-tations: the pitch interval matrix is reconstructed, the refer-\nence lines in the pitch histogram and piano roll are adjusted\naccordingly.\n4.5 Output\nTarsos contains export capabilities for each representation,\nfrom the pitch annotations to the pitch class interval ma-\ntrix there are built-in functions to export the data, either as\ncomma separated text ï¬les or as image ï¬les. Since Tarsos\nhas a scriptable, documented API which can be accessed by\nany Java Virtual Machine (JVM) compatible programming\nlanguage - Groovy, Scala5, Clojure, Java - there is also a\npossibility to add new output formats based on the internal\nobject model. Scripting is also the way to go when process-\ning a large number of ï¬les.\nAs previously mentioned, for pitch class data there is a\nspecial standardized text ï¬le format deï¬ned by the Scala\nprogram: the scala ï¬le with the .scl extension. Scala ï¬les\ncan be used to compare different tone scales within Tarsos\nor with the Scala program. When used as input for Tar-\nsos, these ï¬les provide a reference for the pitch class his-\ntogram extracted from audio. A scala ï¬le e.g. extracted\nfrom Figure 2 with pitch classes (107,363,585,833,1083)\n5Do not confuse the Scala programming language with the Scala soft-\nware tool for scale analysis. Information about the programming language\ncan be found at http://scala-lang.org\n17212th International Society for Music Information Retrieval Conference (ISMIR 2011)\n249 411 639 813 1200 237 477 669 861 1200100200300400500600\nPitch (cent )Number of annotations\nFigure 5 . MR.1973.9.41-4, the second minute of the song is represented by the blue, dashed line, the seventh by the red, dotted\nline. Comparing the second with the seventh minute shows that during the performance the ï¬ddle player changed hand position.\nThe lowest, most stable pitch class is the result of the open string which lost tension during the piece and started to sound lower,\nin stark contrast with the other pitch classes.\nPitch Class (cent) Interval (cent)\n107\n255\n363 478\n222 725\n585 470 976\n248 720\n833 498\n251\n1083\nTable 1 . A pitch interval matrix with pitch classes and pitch\nclass intervals, both in cents. The peaks detected in Figure 2\nare used.\ncan be used to compare pitch use of a song recorded in the\nsame geographical area: do they both use the same absolute\npitch or the same pitch intervals?\nThe pitch annotations can also be synthesized. This re-\nsults in an audio ï¬le which can be used to check if the anno-\ntations make sense. Overlapping the original sound with the\nsynthesized annotations makes clear when no, or incorrect\nannotations were made and, conversely when annotations\nare correct. This auditory feedback can be used to decide if\nthe annotations are trustworthy (the upwards arrow starting\nfrom output in Figure 3).\nA completely different output modality is MIDI . The\nMIDI Tuning Standard deï¬nes MIDI messages to specify the\ntuning of MIDI synthesizers. Tarsos can construct Bulk\nTuning Dump -messages based on extracted pitch class data\nto tune a synthesizer enabling the user to play along with a\nsong in tune. Tarsos contains the Gervill synthesizer, one of\nthe few (software) synthesizers that offer support for tuning\nmessages.5. APPLICATIONS\nThis section illustrates how Tarsos enables or facilitates re-\nsearch on pitch use. The examples given could inspire third\nparty users - musicologists - to try Tarsos and use it to solve\ntheir own research questions.\nA ï¬rst example is an analysis on a single African ï¬ddle\npiece. In African music pentatonic scales are common but\nthis piece uses a tetratonic scale as seen in Figure 5. The\nscale is a result of a playing style with three - more or less\nequally spaced - ï¬ngers and an open string. The graphical\ninterface of Tarsos was used to compare the second minute\nof the song with the seventh, this can be accomplished by se-\nlecting the annotations in the piano roll window. This shows\nthat the open string lost tension during the performance - it\nstarted to sound lower - in stark contrast with the other pitch\nclasses. The results were exported using the L ATEX-export\nfunction and are shown in Figure 5.\nA second example illustrates what can be done with a\nscript that processes a lot of audio ï¬les in batch and the Tar-\nsos API. In an article by Bozkurt [6] pitch histograms are\nused for - amongst other tasks - makam6recognition. The\ntask is to identify which of nine makams is used in a speciï¬c\nsong. A simpliï¬ed, generalized implementation of this task\nwas scripted. With this script it is possible to correctly iden-\ntify 39% of the makams using a dataset of 800 ï¬les. Some\nmakams look very much alike: if the ï¬rst three guesses are\nevaluated the correct makam is present in 75% of the cases.\nThe example is fully documented in the Tarsos manual avail-\nable on the website http://tarsos.0110.be , also the\nsource code is available there. This method is very general\nand directly applicable to e.g. harpsicord tuning estimation\nas done, using another approach, by Tidhar et al [12].\n6A maqam deï¬nes rules for a composition or performance of classical\nTurkish music. It speciï¬es melodic shapes and pitch intervals.\n173Oral Session 2: Non-Western Music\n6. CONCLUSION, DISCUSSION AND FUTURE\nWORK\nIn this paper Tarsos was presented, a modular software plat-\nform to extract and analyze pitch organization in music. Af-\nter an introduction explaining the background and the needs\nfor precise pitch analysis, chapter two provided some con-\ntext about the method used and points to related work. Chap-\nter three gave a high level overview of the different compo-\nnents of Tarsos.\nCurrently Tarsos offers a decent foundation for research\non pitch but it also creates opportunities for future work.\nOne research idea is to reintroduce time domain informa-\ntion. By creating pitch class histograms for a sliding time-\nwindow and comparing those with each other it should be\npossible to detect sudden changes in pitch usage: modula-\ntions. Using this technique it should also be possible to de-\ntect and document pitch drift in choral or other music on a\nlarge scale. Automatic separation of speech and music could\nbe another application.\nAnother research area is to extract features on a large data\nset and use the pitch class histogram or interval data as a ba-\nsis for pattern recognition and cluster analysis. Using Tar-\nsosâ€™ scripting abilities with a timestamped and geotagged\nmusical archive it could be possible to detect geographical\nor chronological clusters of similar tone scale use.\nOn the longer term we plan to add comparable represen-\ntations of other musical parameters to Tarsos as well. In\norder to compare rhythmic and instrumental information,\ntemporal and timbral features will be included. Our ulti-\nmate goal is to develop an objective, albeit partial, view on\nmusic by combining those three parameters.\nDuring this type of research one should keep this quote\nin mind:\nâ€œAudio alone might not be sufï¬cient to un-\nderstand ethnic music. What does it mean to\ndescribe music from a culture where the word\nâ€œmusicâ€ exists only in connection to body move-\nment, smell, taste, colour. The idea of separat-\ning sound from the rest of its physical environ-\nment (movement, smell, taste, colour) may well\nbe a weird â€œinventionâ€ of the West. We can-\nnot understand ethnic music correctly without\nits social function and context [7].â€\nHowever, we do can gain interesting insights and alleviate\naccessibility problems, which is what we are aiming for.\n7. REFERENCES\n[1] Chirs Cannam. The vamp audio analysis plugin api:\nA programmerâ€™s guide. http://vamp-plugins.\norg/guide.pdf .[2] L. P. Clarisse, J. P. Martens, M. Lesaffre, B. De Baets,\nH. De Meyer, and M. Leman. An auditory model based\ntranscriber of singing sequences. In Proceedings of\nthe International Conference on Music Information Re-\ntrieval (ISMIR) , pages 116â€“123, 2002.\n[3] Olmo Cornelis, Dirk Moelants, and Marc Leman.\nGlobal access to ethnic music: the next big challenge?\nInProceedings of 9th ISMIR Conference , 2009.\n[4] Alain de Cheveign Â´e and Kawahara Hideki. Yin, a\nfundamental frequency estimator for speech and mu-\nsic.The Journal of the Acoustical Society of America ,\n111(4):1917â€“1930, 2002.\n[5] Takuya Fujishima. Realtime chord recognition of musi-\ncal sound: A system using common lisp music. In Proc.\nInt. Comput. Music Conf , pages 464â€“467, 1999.\n[6] Ali C. Gedik and BarÄ±s Â¸ Bozkurt. Pitch-frequency\nhistogram-based music information retrieval for turkish\nmusic. Signal Processing , 90(4):1049â€“1063, 2010.\n[7] Micheline Lesaffre, Olmo Cornelis, Dirk Moelants, and\nMarc Leman. Integration of music information retrieval\ntechniques into the practice of ethnic music collections.\nInProceedings Unlocking Audio 2 , 2009.\n[8] Phillip McLeod and Geoff Wyvill. A smarter way to ï¬nd\npitch. In Proceedings of International Computer Music\nConference, ICMC , 2005.\n[9] Dirk Moelants, Olmo Cornelis, and Marc Leman. Ex-\nploring african tone scales. In Proceedings of 9th ISMIR\nConference , 2009.\n[10] Hendrik Purwins, Benjamin Blankertz, and Klaus Ober-\nmayer. Constant Q proï¬les for tracking modulations in\naudio data. In International Computer Music Confer-\nence, pages 407â€“410, 2001.\n[11] J. Sundberg and P. Tjernlund. Computer measurements\nof the tone scale in performed music by means of fre-\nquency histograms. STL-QPS , 10(2-3):33â€“35, 1969.\n[12] Dan Tidhar, Matthias Mauch, and Simon Dixon. High\nprecision frequency estimation for harpsichord tuning\nclassiï¬cation. In Acoustics Speech and Signal Process-\ning (ICASSP), 2010 IEEE International Conference on ,\npages 61 â€“64, march 2010.\n[13] G. Tzanetakis, A. Kapur, W. A. Schloss, and M. Wright.\nComputational ethnomusicology. Journal of Interdisci-\nplinary Music Studies , 1(2), 2007.\n174"
    },
    {
        "title": "Design and creation of a large-scale database of structural annotations.",
        "author": [
            "Jordan Bennett Louis Smith",
            "John Ashley Burgoyne",
            "Ichiro Fujinaga",
            "David De Roure",
            "J. Stephen Downie"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1416884",
        "url": "https://doi.org/10.5281/zenodo.1416884",
        "ee": "https://zenodo.org/records/1416884/files/SmithBFRD11.pdf",
        "abstract": "This paper describes the design and creation of an unprecedentedly large database of over 2400 structural annotations of nearly 1400 musical recordings. The database is intended to be a test set for algorithms that will be used to analyze a much larger corpus of hundreds of thousands of recordings, as part of the Structural Analysis of Large Amounts of Musical Information (SALAMI) project. This paper describes the design goals of the database and the practical issues that were encountered during its creation. In particular, we discuss the selection of the recordings, the development of an annotation format and procedure that adapts work by Peeters and Deruty [10], and the management and execution of the project. We also summarize some of the properties of the resulting corpus of annotations, including average inter-annotator agreement.",
        "zenodo_id": 1416884,
        "dblp_key": "conf/ismir/SmithBFRD11",
        "keywords": [
            "database",
            "structural annotations",
            "musical recordings",
            "SALAMI project",
            "algorithm analysis",
            "corpus of recordings",
            "average inter-annotator agreement",
            "unprecedentedly large",
            "practical issues",
            "development of annotation format"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nDESIGN AND CREATION OF A LARGE-SCALE DATABASE OF \nSTRUCTURAL ANNOTATIONS\nJordan B. L. Smith1, J. Ashley Burgoyne2, Ichiro Fujinaga2,\nDavid De Roure3, and J. Stephen Downie4\n1University of Southern California, 2McGill University,\n3University of Oxford, 4University of Illinois at Urbana-Champaign\njordans@usc.edu, ashley@music.mcgill.ca, ich@music.mcgill.ca, \ndavid.deroure@oerc.ox.ac.uk, jdownie@illinois.edu\nABSTRACT\nThis paper describes the design and creation of  an unprece-\ndentedly large database of over 2400 structural annotations \nof  nearly 1400 musical recordings. The database is in-\ntended to be a test set for algorithms that will be used to analyze a much larger corpus of  hundreds of thousands of recordings, as part of  the Structural Analysis of  Large Amounts of  Musical Information (SALAMI) project. This paper describes the design goals of  the database and the practical issues that were encountered during its creation. In particular, we discuss the selection of the recordings, the \ndevelopment of  an annotation format and procedure that \nadapts work by Peeters and Deruty [10], and the manage-ment and execution of  the project. We also summarize some of  the properties of  the resulting corpus of  annota-tions, including average inter-annotator agreement.\n1.  INTRODUCTION\nThe Structural Analysis of Large Amounts of Musical In-\nformation (SALAMI) project is a musicological endeavour \nwhose goal is to produce structural analyses for a very \nlarge amount of musicâ€”over 300,000 recordings. Here structure refers to the partitioning of  a piece of  music into sections and the grouping together of similar or repeated sections. These sections usually correspond to functionally independent sections, such as the â€œverseâ€ and â€œchorusâ€ sections of a pop song, the â€œexpositionâ€ and â€œdevelopmentâ€ \nof  a sonataâ€”or, at a shorter timescale, the expositionâ€™s \nâ€œmain theme,â€ â€œtransition,â€ and â€œsecondary themeâ€ groups.\nThe recordings in the SALAMI corpus represent an \nenormous range of genres, from klezmer to top-40 pop, and a variety of sources, including professional studio record-ings and audience-recorded live sessions. The SALAMI dataset, which will be made freely available, could be of great service to music theorists, musicologists, and other music researchers, since determining the form of  an indi-\nvidual piece of  music is generally a time-consuming task. The SALAMI dataset could facilitate large-scale studies of \nform, which presently are relatively uncommon.\nBecause of the value of  knowing the structure of pieces \nof  music, the pursuit of  algorithms that produce structural descriptions automatically is an active area of  research. (For a review see [9].) The SALAMI project plans to use a selection of  these algorithms to analyze its hundreds of thousands of  recordings. However, before these algorithms can be used, it is necessary to validate their performance on \nthe vast array of genres represented. This demands the \ncreation of a human-annotated ground truth dataset. The design and creation of  a large database such as the SA-LAMI test set raises many methodological issues relating to the choice of  music, annotation format, and procedure. This paper explains the issues involved and the decisions we made to address them. \nThe next section of  this work summarizes the content \nand contributions of  several existing corpora of  structural \nannotations, as well as important recent research on the annotation process itself [1, 10]. Section 3 describes the creation of the SALAMI test set, including the corpus se-lection, the annotation format used, and the recommended workflow. Some properties of the resulting dataset are pre-sented and discussed in Section 4.\n2.  PRIOR WORK\n2.1 Existing collectionsSALAMI requires a database that includes a significant \namount of popular, jazz, classical, and world music.\n1 How-\never, most previous collections of  annotations only con-sider popular music. Three of  the largest existing databases of  annotations are TUTstructure07 [13] (557 annotations), \ncompiled at Tempere University of  Technology (TUT) and \ncontaining mainly popular music; annotations for the Beat-\nPermission to make digital or hard copies of all or  part of  this work for \npersonal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. \nÂ© 2011 International Society for Music Information Retrieval 1 These  four genre labels should be understood in their broadest sense, so \nthat together they encompass all music. Thus â€œclassicalâ€ refers to all \nWestern art music; â€œpopularâ€ refers to most modern commercial music, including The Cure and Autechre; and so forth for â€œjazzâ€ and â€œworld.â€\n555Poster Session 4\nles studio catalogue created by Alan Pollack and synchro-\nnized independently by two groups [5, 14] (180 annota-tions); and the AIST Annotation set [4] that accompanies \nthe RWC Music Database (285 annotations). The RWC set \nis approximately half  popular music, and one quarter each jazz and classical, with an additional few world music pieces, but for many of  the jazz and classical pieces only the â€œchorusâ€ sections are indicated.\n2.2 Annotation formatsNearly all previous corpora of annotations have used the \nsame straightforward annotation format. Pieces are seg-\nmented into non-overlapping sections, and every section is \ngiven a single label, such as â€œintroâ€ or â€œchorus,â€ to indicate which are similar to or repetitions of  one another. The la-bels also suggest the musical role or function of each sec-tion. In some corpora, such as the Beatles annotations [5], labels may indicate instrumentation (e.g., â€œverse_guitarâ€) or variations on a section (e.g., â€œverse_with_endingâ€).\n2.2.1 Issues with previous formats\nAs pointed out in Peeters and Deruty [10], this conflation \nof  musical similarity, function, and instrumentation is prob-lematic. For instance, a songâ€™s â€œoutroâ€ may use the same music as an earlier â€œtransition,â€ but labelling them as such fails to record their similarity. Contrariwise, a section with a single function may be musically heterogenous, as with an extended two-part introduction. Peeters and Deruty also \ncriticized the large, seemingly unconstrained vocabularies \nused in certain collections of annotations. Consider again the Isophonics Beatles annotations [5]: of  the 146 unique labels, 95 are used just once. Single-use labels may be in-formative to a human inspecting the annotation, where their meaning is understandable in context (e.g., â€œintro_redux,â€ â€œverse_(slow)â€), but having too many unique labels is less useful when the annotations are being used by a machine. \nAnother drawback of  the standard annotation format is that \nit only describes the structure at a single timescale. One of the most important attributes of  musical structure is that it is perceived hierarchically, and it would be ideal to capture some of this information in an annotation.\n2.2.2 An alternative formatPeeters and Deruty proposed an alternative annotation for-\nmat intended to resolve these problems. The format uses a \nrestricted vocabulary of 19 labels, each of  which addresses \none of  three aspects of  a pieceâ€™s structure: either musical similarity, musical role, or instrument role. In their format, musical similarity is indicated by labelling every portion of a piece as one of  five â€œConstitutive Solid Loopsâ€ (CSLoops). (If more than five are required, a sixth CSLoop is used, although the format does not imply that all sections labelled with this last label are similar.) Function labels are \noptional and are restricted to â€œintro/outro,â€ â€œtransition,â€ â€œchorus,â€ and â€œsolo.â€ Instrumentation labels indicate \nwhether a primary or supporting melodic voice is present.\nPeeters and Derutyâ€™s format also creatively incorporates \nsome hierarchical information about the structure. Two \nmarkers, â€œV1â€ and â€œV2,â€ divide CSLoops; the first indi-cates that the musical segments on either side of  the marker are similar, the second that they are dissimilar.\n2.3 Annotation proceduresUnlike pitch and, to a large extent, beat, the perception of \nstructure is a highly subjective phenomenon, and it is common for two listeners to disagree on the form of  a piece \nof  music. It is therefore challenging to develop an annota-\ntion procedure that, while perhaps not being objective, maximizes the repeatability of  the results. Note that since a structural analysis records a listenerâ€™s creative interpreta-tion as much as her perception, objectivity is arguably an impossible goal for annotations.\nOne approach is to treat the creation of annotations as a \nperceptual experiment, and simply have multiple subjects \nlisten to a piece and press a button whenever they perceive \na structural boundary. Such data were collected by [2], who noted that listeners generally agreed on the placement of boundaries that they judged most salient. These boundaries were used as a type of  â€œground truthâ€ by the authors to evaluate the success of some computational models at es-timating boundaries.\nBimbot et al. [1] managed to obtain a degree of repeat-\nability by precisely specifying an annotation procedure. They defined the musical criteria and similarity judgements an annotator should use in order to estimate boundaries. (The task of  labelling the segments remains future work.) They reported that with their procedure, annotations were very consistent across annotators and over time. An annota-torâ€™s goal is to decompose a piece into â€œautonomous and \ncomparable blocks.â€ Autonomy means that whether a block \nstands alone or is looped continuously, the result should be musically acceptable. Two blocks may be comparable if they have the same duration in beats, are interchangeable, or are similar with respect to their temporal organization.\n3.  DESCRIPTION OF THE SALAMI TEST SET\nWe developed a new corpus of  annotations using a unique \nannotation format to address the goals of the SALAMI pro-\nject. To ensure that the corpus was useful as an evaluation \ntest set for SALAMI, the main design consideration was for the corpus to cover as wide a variety of musical genres as possible. For the annotations to be musicologically useful, the design goals for the annotation format were to have musical similarity, function, and lead instrumentation de-scribed independently, and for the annotations to reflect the hierarchical nature of  musical structure. Finally, the format \nand the procedure should allow annotations to be produced \nquickly, to minimize cost, but be flexible enough to handle \n55612th International Society for Music Information Retrieval Conference (ISMIR 2011)\nworks from a wide range of  genres, all while aiming for \nhigh inter-annotator agreement. With these design consid-erations in mind, we conducted a survey of  previous cor-\npora of  annotations and existing annotation techniques. \nBased on this survey and on our own experimentation with different approaches, we settled on the corpus, format, and procedure outlined in this section.\n3.1 Contents of SALAMI test setThe first step in designing the corpus was deciding what to \nput in it. One of  SALAMIâ€™s priorities was to provide struc-tural analyses for as wide a variety of  music as possible, to \nmatch the diversity of music to be analyzed by the algo-\nrithms. In addition to popular music, the SALAMI test set should pay equal attention to classical, jazz, and non-Western music known colloquially as â€œworldâ€ music. To ensure a diversity of  recording formats, we also empha-sized the inclusion of  live recordings. The final composi-tion of the database is shown in Table 1.\nA secondary goal of  the SALAMI test set was to be able \nto compare our annotations with those of  previous data \nsets. We thus duplicated some previous work: our test set presently includes 97  and 35 recordings from the RWC and Isophonics data sets, respectively. Note that these re-cordings are all single-keyed (i.e., annotated by a single person), whereas most of  the SALAMI test-corpus is double-keyed (analyzed by two independent annotators). \nDouble-keying provides useful information but is more \nexpensive. Single-keying some entries seemed to be a rea-sonable compromise given that other groups had already annotated these pieces.\nClass Double \nkeyedSingle \nkeyedTotal Percentage\nClassical 159 66 225 16%\nJazz 225 12 237 17%\nPopular 205 117 322 23%\nWorld 186 31 217 16%\nLive music 273 109 382 28%\nTotal 1048 335 1383 100%\nTable 1.  Number of  pieces of  each class in the SALAMI \ntest set. Single and double keying refers to the number of annotators (2 or 1, respectively) who independently ana-\nlyzed each song.\nSelecting songs for the corpus by hand would be time-\nconsuming and would introduce unknown methodological bias. However, selecting songs randomly from most sources would result in a corpus heavily skewed toward popular music. To resolve this, most of  the recordings were collected from Codaich [7], a large database with carefully curated metadata, including over 50 subgenre labels. This \nenabled us to enforce good coverage of  genres while still choosing individual pieces randomly. The remainder of the \ntest set was collected randomly from the Live Music Ar-chive [6]. Unfortunately, metadata for these recordings is \ninconsistent and a distribution by genre could not be en-\nforced. The majority appears to be popular and jazz music.\n3.2 Annotation formatWe developed a new annotation format that takes after the \nformat devised by Peeters and Deruty in many important ways: we borrow their tripartite distinction between labels that indicate musical similarity, function, and instrumenta-tion, and like them we also strictly limit the vocabulary of \nfunction labels. However, we have made several modifica-\ntions to suit SALAMIâ€™s unique needs and more musi-cological focus. The labels in each of the three layers are described in the following three sections. An example an-notation is shown in Figure 1.\nFigure 1.  Example to illustrate proposed format.\n3.2.1 Musical similarity track\nThe musical similarity track includes two layers at different \ntimescales, each identifying which portions of the piece use \nsimilar musical ideas. The large-scale layer uses uppercase letters as labels (â€œA,â€ â€œB,â€ etc.) and the small-scale layer uses lowercase letters (â€œa,â€ â€œb,â€ etc.). The use of  letter la-bels mimics the familiar music-theoretical approach. Every portion of  a recording in both large- and small-scale layers must be assigned a letter label. The format specification allows any number of  lowercase or uppercase letters to be \nused (the labels â€œaa,â€ â€œab,â€ and so on may be used if  the \nalphabet is exhausted). However, for the large-scale layer, annotators were instructed to prefer to use five or fewer distinct uppercase labels per recording. This preference rule does not express an assumption that there are five or fewer distinct musical ideas in any recording. Rather, it is in-tended to guide the annotator toward a certain level of  ab-straction. This direction proved useful when annotating \nworks that are less clearly organized into distinct sections, \nsuch as through-composed pieces. It also helps when anno-tating works such as sonatas that may be organized into sections, but where these sections are not musically ho-mogenous and may include several distinct musical ideas.\nTwo additional special labels indicate silence (â€œsilenceâ€) \nand non-music, such as applause or banter in a live record-\n557Poster Session 4\ning (â€œZâ€). We also allow letter labels to be inflected by the \nprime symbol ( â€² ) to indicate a section that is evidently \nsimilar to another, but that is judged to be substantially \nvaried. Similarity judgements are inherently subjective and \nimprecise, and the prime symbol is a useful way of  ac-knowledging this. It allows the annotator to faithfully re-cord his interpretation, while allowing future users to easily adapt the labels according to their needs. For instance, de-pending on the application, a user may excise the prime markers (so that â€œaâ€ and â€œa â€²â€ are both relabelled as â€œaâ€) or \nto treat variations as distinct sections (so that â€œa â€²â€ would be \nreassigned a letter label different from â€œaâ€).\n3.2.2 Function track\nThe second track in the annotation format contains the \nmusic function labels, which all must be drawn from our strict vocabulary of  20 labels. Unlike the letter labels, it is not mandatory that every portion of  a piece receive a func-tion label. The vocabulary is listed in Table 2, separated into various relevant categories. The instrumental, transi-\ntion, and ending groups are all synonym groups. Note that \nin the ending group, the label â€œfadeoutâ€ is a special label that can occur in addition to any other label. For example, if the piece fades out over a repetition of  the chorus, then the last section may be given both labels: â€œchorusâ€ and â€œfadeout.â€ Full definitions for each term are specified in our Annotatorâ€™s Guide, available online [11].\nBasic group intro, verse, chorus, brid ge\nInstrumental instrumental, solo\nTransition transition, pre-chorus, pre-verse, \ninterlude\nGenre-specific head, main theme, (secondary) theme\nForm-specific exposition, development, recapitulation\nEnding outro, coda, fadeout\nSpecial labels silence, end\nTable 2.  List of  permitted function words in proposed an-\nnotation format.\nNote that some of the labels are genre-specific alterna-\ntives to others: for example, the â€œheadâ€ in a jazz song is analogous to a â€œchorusâ€ in a pop song or, sometimes, a â€œmain themeâ€ in a classical piece. Also, together, the terms \nâ€œexposition,â€ â€œdevelopment,â€ and â€œrecapitulationâ€ are spe-\ncific to sonata form and may in special cases be used to annotate a third level of  structural relationships at a time-scale larger than the large-scale similarity labels. However, â€œdevelopmentâ€ also has wider applicability: it may be used to indicate the function of  a contrasting middle section, which is relevant in many contexts, from various classical genres to progressive rock. Additionally, some subsets of \nthe vocabulary can function as synonym-groups that can be \ncollapsed into a single function label if  desired. For exam-ple, while our Annotatorâ€™s Guide defines a relatively subtle \ndistinction between â€œpre-chorus,â€ â€œpre-verse,â€ â€œinterlude,â€ and â€œtransitionâ€ sections, they are all synonyms of  â€œtransi-\ntion.â€ This approach allows annotators to err on the side of \nprecision, while enabling future users of  the data to ignore distinctions that are unneeded.\n3.2.3 Lead instrument trackThe final track in the annotation format indicates wherever \na single instrument or voice takes on a leading, usually melodic role. The labels in this track are simply the names of  the leading instruments, and hence the vocabulary is not \nconstrained. Also, unlike the other tracks, lead instrument \nlabels may potentially overlap, as in a duet. Note that as with the function track, there may be portions of  the re-cording with no lead instrument label, if  no instrument ful-fills a leading role.\nNote that in the written format devised for this project, \nthe boundaries delineating the small-scale similarity seg-ments are the only available boundaries when annotating \nthe function and lead instrumentation tracks. Again, this \nhelps orient annotators to an appropriate level of abstrac-tion, and relieves them of too painstakingly indicating the instrumentation changes.\n3.3 Annotation procedureThe annotators used the software Sonic Visualiser [3] to \naudition and annotate the pieces. Sonic Visualiserâ€™s key-\nboard commands allow one to insert and label boundaries \nquite quickly. We suggested the following workflow: first, listen through the song and mark a boundary whenever a structural boundary is perceived. Second, listen to the piece again, adjusting boundaries and adding lowercase labels. Third, add the uppercase and function labels, and finally add the lead instrument labels. While we found this work-flow to be efficient and straightforward, we did not demand \nthat annotators follow this or any other specific workflow.\n3.4 Project realizationThe annotation format and data collection took place over \nthe course of  10 months. First, previous annotation formats and databases of  annotations were researched. Potential annotation formats were devised and tested by the project leaders, and a tentative format was set at the end of  two months. Next, candidate annotators were trained in the an-\nnotation format and in the Sonic Visualiser environment. \nEight successful candidates were hired, all pursuing gradu-ate studies in either Music Theory or Composition, and data collection began the following week. Because the an-notation format had not been tested on a significant scale before work began in earnest, the first six weeks of  data collection were conceived as an extended trial period. Every week or two, annotators were given a new batch of \nassignments in a new genre, beginning with popular, which \n55812th International Society for Music Information Retrieval Conference (ISMIR 2011)\nwas expected to be the least problematic, and continuing in \norder with jazz, classical, and world, which were predicted to be of  increasing difficulty. At the end of  the six weeks, \nsupervision of the annotators was relaxed and any problems \naddressed on an ad hoc  basis. Data collection continued \nover the next 12 weeks, by which point the majority of assignments had been completed.\nWe collected the self-reported time it took to produce \neach annotation in order to assess productivity. The times are plotted as a function of  the date for the first 1700 anno-tations in Figure 2. It can be seen that, disregarding a num-\nber of  outliers towards the beginning of the project, annota-\ntion time decreased modestly, from a mode of  20 minutes in the first 100 days, to a mode of  15 minutes in the re-mainder, enough for 3 full listenings of  the average song, which was 4:21 long. The average annotation time also dropped from 21 to 17 minutes. Earlier analysis showed a slight correlation between a songâ€™s length and its annota-tion time.\n3.4.1 Annotation format and procedure revisions\nAfter each new assignment, we solicited feedback from the \nannotators on what weaknesses or ambiguities in the anno-tation format and procedure were revealed. Most issues were addressed and resolved at regular group meetings, where we also planned and agreed on the vocabulary. Feedback led to the introduction of  new heuristics (e.g., we \nestablished a preference to have segment boundaries fall on \ndownbeats, even in the presence of  pickups). In one case, feedback led to a major revision of  the format. We origi-nally used the â€œV1â€ and â€œV2â€ markers described by [10] to implicitly encode musical similarity at a shorter timescale. However, annotators found that explicitly describing the structure at both timescales was both conceptually simpler and quicker. Annotators were satisfied by the switch and \nthe subsequent annotations also had more information.\n4.  RESULTS\nIn this section we report certain properties of  the collected \ndata, including inter-annotator agreement.\nThe average number of  segments per annotation was \n11.3 for the large-scale analyses, with half  of  the analyses having between 8 and 14 segments. These figures were 38.4 and between 20 and 49 for the small-scale analyses. \nOn average, there were 4.0 unique large-scale labels and \n7.2 unique small-scale labels per annotation.\nFrom the variety of  measures used to compare two anno-\ntations (defined in [12], among others), we estimated the pairwise f-measure, boundary f-measure, and Rand index. \nBoundary f-measure is found by observing the precision \nand recall with which one set of boundaries matches the other set. Boundaries match if they lie within some toler-\nance window (0.5 or 3 seconds) of each other. Pairwise f-\nmeasure treats all pairs of  frames with the same label in one description as a set of  similarity relationships that the \nother description retrieves with some precision and recall. The Rand index is similar except that it also identifies how \nmany pairs of frames with different labels in one descrip-\ntion also have different labels in the other. The agreement between 974 pairs of annotations is reported in Table 3.\nFigure 2 . Plot of  annotation times over the course of the \nproject timeline.\nAnnotations\ncomparedPW fRand \nindexBound f\n(0.5 sec )Bound f\n(3 sec )\n1. Large-large 0.76 0.79 0.69 0.77\n2. Small-small 0.69 0.81 0.73 0.82\n3. Small-large and \nlarge-small (avera ge)0.60 0.70 0.38 0.44\n4. Best case 0.81 0.87 0.80 0.89\nTable 3.  Average agreement between 974 pairs of annota-\ntions, as estimated by four similarity metrics (pairwise f-\nmeasure, Rand index, and boundary f-measure with two \nthresholds) when comparing: (1) both annotatorsâ€™ large-\nscale annotations; (2) both small-scale annotations;  (3) one annotatorâ€™s large-scale annotation and the otherâ€™s small-scale one. The last row (4) takes the maximum similarity of all four possible pairings between the first and second an-notatorsâ€™ musical similarity labels.\nEach annotation describes musical similarity at two lev-\nels of  detail, both of  which should be considered valid de-\nscriptions. To compare two annotations, we may compare \nthe large-scale labels only or the small-scale labels only, but we may also find the similarity of all pairs (including small-to-large and large-to-small) and take the maximum similarity to estimate the inter-annotator agreement. This will allow us to recognize cases where the annotators have focused on different timescales. As seen in Table 3, the agreement between large-scale labels (pairwise f = 0.76, \nRand = 0.79) is comparable to that between small-scale \nlabels (pairwise f = 0.69, Rand= 0.81), and the average best \nmatch found is slightly higher than each (pairwise f = 0.81, \nRand = 0.87). For comparison, [8] reported a pairwise f of \n0.89 on a test set of  30 songs from the TUT set, and [1] 0255075100\n0 100 200 300 400Time to annotate, in minutes\nTime completed after project start date, in days\n559Poster Session 4\nreported a boundary f measure of 0.91 (using a 0.75-second \nthreshold) on a test set of 20 songs.\nThe agreement was not found to depend greatly on the \ngenre. This is reasonable since each of the broad genres \nconsidered here are each very diverse and contain some straightforward and some complex pieces. For instance, the popular genre includes both straightforward pop music and more difficult to annotate progressive rock; likewise, though much world music poses a challenge to annotators, subgenera such as klezmer and Celtic music can be struc-turally straightforward.\nWe replicated annotations for 97 recordings in the RWC \ndata set. The RWC annotations distinguish similar and identical repetitions of sections by adding letters to func-tion labels (e.g., â€œverse Aâ€, â€œverse Bâ€, etc.). We created two versions of  the RWC labels, one retaining and one ignoring the additional letter labels. These were compared to the large- and small-scale SALAMI annotations, revealing modest agreement (see Table 4). Aside from the Rand in-\ndex, the results indicate that the large-scale SALAMI \nanalyses are more similar to the RWC annotations than the small-scale analyses.\n5.  CONCLUSION\nThe SALAMI test set has over 2400 annotations describing \nthe formal structure of  almost 1400 pieces of music, from a wide variety of genres, including popular, jazz, classical, \nand world music. This set may be used for a variety of  fu-\nture studies: for example, on the connection between the surface characteristics of  music and the perception of musi-cal form, or between formal styles and musical parameters such as artist, genre, and place of  origin. The test data and the hundreds of thousands of  computed structural descrip-tions will soon be reachable from our website [11].\nWhile the worth of the corpus will ultimately depend on \nthe use researchers make of  it, the quantity and richness of \nthe information in the SALAMI test set should make it attractive to musicologists and music information retrieval researchers alike.\n6.  ACKNOWLEDGEMENTS\nThis research was funded by a Digging Into Data Chal-\nlenge award, including funds from the National Science Foundation (under Grant No. IIS 10-42727), JISC, and the \nSocial Sciences and Humanities Research Council of  Can-\nada. The authors would especially like to thank our annota-tors at McGill University and University of Southampton for their hard work.\n7.  REFERENCES\n[1] Bimbot, F., O. Le Blouch, G. Sargent, and E. Vincent. \n2010. Decomposition into autonomous and comparable Annotations\ncomparedPW fRand \nindexBound f\n(0.5 sec )Bound f\n(3 sec )\n1. RWC and lar ge 0.64 0.73 0.57 0.75\n2. RWC and small 0.45 0.77 0.38 0.52\nTable 4.  Average agreement between 97 pairs of RWC and \nSALAMI annotations when comparing: (1) SALAMIâ€™s large-scale labels with RWCâ€™s function class labels; (2) SALAMIâ€™s small-scale labels with RWCâ€™s distinct labels.\n blocks: A structural description of music pieces. Proc. \nISMIR , 189â€“94.\n[2] Bruderer, M., M. McKinney, and A. Kohlrausch. 2009. \nThe perception of structural boundaries in melody lines \nof  Western Popular music. MusicÃ¦ ScientÃ¦,  8 (2): 272â€“\n313.\n[3] Cannam, C., C. Landone, M. Sandler, and J. P. Bello. \n2006. The Sonic Visualiser: A visualisation platform for semantic descriptors from musical signals. Proc. \nISMIR,  324â€“7.\n[4] Goto, M. 2006. AIST annotation for the RWC Music \nDatabase. Proc. ISMIR,  359â€“60.\n[5] Isophonics datasets, Centre for Digital Music. http://\nwww.isophonics.net/datasets.\n[6] Live Music Archive. http://www.archive.org/details/\netree.\n[7] McKay, C., D. McEnnis, and I. Fujinaga. 2006. A large \npublicly accessible prototype audio database for music \nresearch. Proc. ISMIR,  160â€“3.\n[8] Paulus, J., and A. Klapuri. 2009. Music structure \nanalysis using a probabilistic fitness measure and a greedy search algorithm. IEEE TASLP , 17 (6): 1159â€“\n70.\n[9] Paulus, J., M. MÃ¼ller, and A. Klapuri. 2010. Audio-\nbased music structure analysis. Proc. ISMIR , 625â€“36.\n[10] Peeters, G., and E. Deruty. 2009. Is music structure \nannotation multi-dimensional? A proposal for robust \nlocal music annotation. Proc. LSAS,  75â€“90.\n[11] SALAMI. http://salami.music.mcgill.ca/.[12] Smith, J. B. L. 2010. A comparison and evaluation of  \napproaches to the automatic formal analysis of  musical audio. MA thesis, McGill University.\n[13] TUTstructure07 d ataset, Technical University of  \nTampere. http://www.cs.tut.fi/sgn/arg/paulus/\nTUTstructure07_files.html.\n[14] UPF Beatles dataset, University of  Pompeu Fabra. \nhttp://www.dtic.upf.edu/~perfe/annotations/sections/license.html.\n560"
    },
    {
        "title": "A Comparative Study of Collaborative vs. Traditional Musical Mood Annotation.",
        "author": [
            "Jacquelin A. Speck",
            "Erik M. Schmidt",
            "Brandon G. Morton",
            "Youngmoo E. Kim"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1417075",
        "url": "https://doi.org/10.5281/zenodo.1417075",
        "ee": "https://zenodo.org/records/1417075/files/SpeckSMK11.pdf",
        "abstract": "Organizing music by emotional association is a natural process for humans, but the ambiguous nature of emotion makes it a difficult task for machines. Automatic systems for music emotion recognition rely on ground truth data collected from humans, and more effective methods for collecting such data are being continuously developed. In previous work, we developed MoodSwings, an online collaborative game for crowdsourcing dynamic (persecond) mood ratings from multiple players within the twodimensional arousal-valence (A-V) representation of emotion. MoodSwings has proven effective for data collection, but potential data effects caused by collaborative labeling have not yet been analyzed. In this work, we compare the effectiveness of MoodSwings to that of a more traditional data collection method, where annotation is performed by single, paid annotators. We implement a simplified labeling task to run on Amazonâ€™s crowdsourcing engine, Mechanical Turk (MTurk), and analyze the labels collected with each method. A statistical comparison shows consistencies between MoodSwings and MTurk data, and we produce similar results using each as training data for automatic emotion production via supervised machine learning. Furthermore the new dataset collected via MTurk has been made available to the Music Information Retrieval community.",
        "zenodo_id": 1417075,
        "dblp_key": "conf/ismir/SpeckSMK11",
        "keywords": [
            "Automatic systems for music emotion recognition",
            "Ground truth data collected from humans",
            "Crowdsourcing dynamic mood ratings",
            "Two-dimensional arousal-valence representation",
            "MoodSwings online collaborative game",
            "Mechanical Turk crowdsourcing engine",
            "Comparison of data collection methods",
            "Statistical analysis of labels",
            "Similar results using each as training data",
            "New dataset collected via MTurk"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nA COMPARATIVE STUDY OF COLLABORATIVE VS. TRADITIONAL\nMUSICAL MOOD ANNOTATION\nJacquelin A. Speck, Erik M. Schmidt, Brandon G. Morton and Youngmoo E. Kim\nMusic and Entertainment Technology Laboratory (MET-Lab)\nElectrical and Computer Engineering, Drexel University\n{jspeck, eschmidt, bmorton, ykim }@drexel.edu\nABSTRACT\nOrganizing music by emotional association is a natural pro-\ncess for humans, but the ambiguous nature of emotion\nmakes it a difï¬cult task for machines. Automatic sys-\ntems for music emotion recognition rely on ground truth\ndata collected from humans, and more effective meth-\nods for collecting such data are being continuously devel-\noped. In previous work, we developed MoodSwings, an\nonline collaborative game for crowdsourcing dynamic (per-\nsecond) mood ratings from multiple players within the two-\ndimensional arousal-valence (A-V) representation of emo-\ntion. MoodSwings has proven effective for data collection,\nbut potential data effects caused by collaborative labeling\nhave not yet been analyzed. In this work, we compare the\neffectiveness of MoodSwings to that of a more traditional\ndata collection method, where annotation is performed by\nsingle, paid annotators. We implement a simpliï¬ed labeling\ntask to run on Amazonâ€™s crowdsourcing engine, Mechanical\nTurk (MTurk), and analyze the labels collected with each\nmethod. A statistical comparison shows consistencies be-\ntween MoodSwings and MTurk data, and we produce simi-\nlar results using each as training data for automatic emotion\nproduction via supervised machine learning. Furthermore\nthe new dataset collected via MTurk has been made avail-\nable to the Music Information Retrieval community.\n1. INTRODUCTION\nThe problem of automated emotion (mood) recognition\nwithin music has recently received increased attention\nwithin the music information retrieval (Music-IR) research\ncommunity [1]. The perceptual nature of emotion ne-\ncessitates that such systems be trained on ground truth\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proï¬t or commercial advantage and that copies\nbear this notice and the full citation on the ï¬rst page.\nc/circlecopyrt2011 International Society for Music Information Retrieval.data collected from humans, and the Music-IR commu-\nnity could beneï¬t from further development and evalua-\ntion of methods for collecting such data. In prior work,\nwe created MoodSwings, an online collaborative game for\ncollecting per-second labels of music, based on the two-\ndimensional arousal-valence (A-V) model of emotion [2,3].\nMoodSwings captures emotion changes in synchrony with\nmusic and collects a distribution of multiple playersâ€™ labels\nfor each moment in a song. These quantitative labels are\nwell suited to computational parameter estimation and su-\npervised machine learning [4â€“6].\nInitial studies of the gameâ€™s effectiveness found that an-\nnotators settle upon their ï¬nal ratings faster when play-\ning against a partner (as opposed to random AI, which\nsimulates a partnerâ€™s participation when an odd number\nof players are online) [7]. However, the effects of col-\nlaborative annotation on the quality of ratings have yet to\nbe established. In this work we compare MoodSwings to\na more traditional data collection method via Amazonâ€™s\nMechanical Turk (MTurk),1an online crowdsourcing en-\ngine. Through the construction of Human Intelligence Tasks\n(HITs), MTurk connects researchers with human subjects\nfrom all over the web and provides a means for payment. We\ndesign a traditional A-V labeling task, employing MTurk\nworkers to label a dataset consisting of 240, 15-second clips\npreviously annotated via MoodSwings [4]. We examine the\ncollected labels to comparatively analyze our game versus\ntraditional data collection.\nThe monetary incentives of MTurk unavoidably inject\nnoise into our labels. This becomes an issue for data quality\nas it is undesirable to pay for unsatisfactory work. MTurk\nallows us to deny workers payment if they do not properly\ncomplete the task, and we develop an outlier detection al-\ngorithm to automatically detect such workers. In an attempt\nto reduce bias, the system relies on the use of expert an-\nnotatorsâ€™ labels as a baseline when trying to validate anno-\ntations. It ï¬lters out workers who demonstrate unwilling-\nness to correctly perform the task. We compare the â€œcleanâ€\nMTurk dataset to labels from our game statistically, and with\n1http://mturk.com\n549Poster Session 4\nrespect to automatic mood prediction accuracy. The new\ndataset collected via MTurk has been made available to the\nMusic Information Retrieval community.2\n2. BACKGROUND\nThe natural language processing (NLP) [8] and machine vi-\nsion [9, 10] communities have utilized MTurk extensively,\nbut machine listening and Music-IR have been slow to adopt\nits use. Lee found crowdsourcing music similarity judg-\nments on MTurk to be less time-consuming than collecting\ndata from experts in the research community [11]. The ex-\nperiment cost $130.90 and produced 6,732 similarity judge-\nments, less than $0.02 per rating. HITs were rejected if\nworkers rated songs too quickly or failed to assign high sim-\nilarity to identical songs. While nearly half of all HITs were\nrejected, the dataset was obtained an order of magnitude\nmore quickly than in their previous attempts. Comparing the\ndatasets yields a Pearsonâ€™s correlation coefï¬cient of 0.495,\nconsistent with previous NLP work involving MTurk [8]. As\nthe previous data collection was assembled for MIREX, Lee\nreturned the submitted systems using MTurk data as ground\ntruth and found no signiï¬cant alterations to the outcome,\nscoring a 5.7% difference on the Friedman test.\nMandel et al. employed MTurk for collecting free form\ntags to study relationships between audio tags and content\n[12]. The group collected 2,100 unique tags across 925\nclips, for a reported cost of approximately $100. To en-\nsure data quality, they rejected a HIT if any tag had more\nthan 25 characters, if less than 5 tags were provided, or\nif less than half of tags were contained in a dictionary of\ncommonly applied tags (Last.fm). All HITs by a particular\nworker were rejected if the worker used too small a vocabu-\nlary, if they used more than 15% â€œstop wordsâ€ (e.g., â€œmusicâ€\nor â€œniceâ€), or if half of their individual HITs were rejected\nfor other reasons. The authors then trained a support vec-\ntor machine (SVM) classiï¬er for content-based autotagging.\nWith smoothed labels, the MTurk version increased perfor-\nmance to 63.4% versus 63.09% with MajorMiner.\n3. DATA COLLECTION METHODS\nIn previous work we designed MoodSwings, a collaborative\nonline game that leverages crowdsourcing to collect mood\nratings [2]. The game board is based on the A-V space,\nwhere the valence dimension represents positive versus neg-\native emotions and arousal represents high versus low en-\nergy [3]. Anonymously-partnered players label song clips\ntogether during each round, scoring points based on the\noverlap between their cursors, which encourages consensus.\nBonus points are awarded to a player whose partner moves\ntowards him/her, encouraging competition and discouraging\n2http://music.ece.drexel.edu/research/emotion/moodswingsturkplayers from blindly following their partners to score points.\nWe recently initiated a redesign effort, investigating game-\nplay improvements suggested by an analysis of collected la-\nbels [7]. However, we have not addressed concerns about\nthe game structure biasing annotations.\nWe designed a simpliï¬ed labeling task, shown in Figure\n1, for MTurk. Single workers provide A-V labels for clips\nfrom our dataset, consisting of 240 15-second clips, which\nare extended to 30 seconds to give workers additional an-\nnotation practice [4]. As in MoodSwings, we collect per-\nsecond labels, but no partner is present and no points are\nawarded. Workers are given detailed instructions describing\nthe A-V space. They navigate to a website which hosts the\ntask and label 11 randomly-chosen clips. The ï¬rst clip is\na practice round, omitted from our analysis. The third and\nninth are identical, randomly chosen from a set of 10 â€œveriï¬-\ncation clips,â€ which are evaluated to identify unsatisfactory\nwork. Workers are given a 6-digit veriï¬cation code to en-\nter on the MTurk website as proof of completion which, if\nsuccessful, earns workers $0.25 per HIT.\nFigure 1 . Screenshot of labeling task deployed on MTurk,\ndepicting the A-V space and a yellow orb as the annota-\ntorâ€™s cursor. A sidebar provides additional instructions, e.g.\nworkers may type â€œBâ€ if they encounter bugs in the task.\n4. FILTERING OF MECHANICAL TURK DATA\nAs previously discussed, quality control is an important is-\nsue with data collection on MTurk. In the labeling task, our\ninteractions with workers are extremely limited and workers\ncannot ask for clariï¬cation of instructions during the HIT. It\nis difï¬cult to gauge workersâ€™ understanding, and to deter-\nmine if they were blindly moving the cursor to earn $0.25\nfor entering a veriï¬cation code. Figure 2 shows examples of\nâ€œgoodâ€ and â€œbadâ€ data collected for two song clips. We ob-\ntained annotations from 272 unique workers, an average of 5\nHITs each. To determine which of the over 1,000 complete\n55012th International Society for Music Information Retrieval Conference (ISMIR 2011)\nFigure 2 . â€œBadâ€ (top) and â€œgoodâ€ (bottom) worker data for\ntwo 30-second song clips. Labels get darker to show the\nprogression of time. Both â€œbadâ€ sets of labels indicate lack\nof understanding or attention to the task.\nlabeling sessions are valid, we utilize an automatic ï¬ltering\nsystem, which is trained on expertsâ€™ annotations of our ver-\niï¬cation clips.\n4.1 Baseline for Validity: Expert Annotations\nWe evaluate workersâ€™ veriï¬cation clip labels to determine if\nthey completed the task correctly. The 10 veriï¬cation clips\nwere handpicked for their obvious mood transitions, e.g.,\nfrom low to high valence. Transitions occur near the middle\nof each clip. To provide a baseline for validity, âˆ¼10 Music-\nIR researchers labeled the veriï¬cation clips twice each, dur-\ning two sessions (one week apart). To demonstrate that the\nexpertsâ€™ labels provide a good baseline for validity, we mea-\nsure the consistency of their ratings between sessions. Con-\nsistent ratings indicate attentiveness and understanding of\nthe task, which characterize our expectations for correctly\ncompleted MTurk HITs.\nThe blue line in Figure 3 shows the normalized distances\nbetween the label distributionsâ€™ means for each veriï¬ca-\ntion clip in the two annotation sessions, averaged over time.\n(e.g., normalized mean distance between clip one in session\none and clip one in session two). As a baseline, the dashed\nline indicates the average distance over all individual clips\nfrom session 1 when they are compared to the combination\nof all remaining clips in session 2 (e.g., clip 1 from session\n1 compared to the combined labels from session 2 clips 2-\n10). For all clips, the normalized mean distances between\nsessions 1 and 2 are well below the average, demonstrating\nconsistent expert annotations over multiple trials.\nFigure 3 . Distance between labelsâ€™ means for each clip in\nexpert annotation sessions 1 and 2, with error bars indicat-\ningÂ±1 standard deviation. Dashed line indicates the aver-\nage distance between individual clips from session 1 and the\ncombination of all remaining clips from session 2.\n4.2 Automatic Filtering System\nWe wish to reject data from workers who move about the A-\nV space without paying attention or who misunderstand the\nmeanings of the A-V axes, but avoid rejecting valid ratings\nsimply because they differ from our own subjective opin-\nions. A one-class SVM for every second of each veriï¬cation\nclip is trained on the expert labels, then used to detect invalid\nworker data. The expertsâ€™ labels differ enough between indi-\nviduals to account for many valid mood ratings, but to avoid\npenalizing workers for differences in opinion we only re-\nquire that workersâ€™ veriï¬cation clip labels fall within the de-\ncision boundary of the one-class SVM on average.\nFigure 4 . One-class SVM trained on expert data for one\nsecond of a veriï¬cation clip. Expert labels (x), support vec-\ntors (0), and decision boundary are shown.\n551Poster Session 4\nValence\nArousalGame Data Distribution\nâˆ’0.5 0 0.5âˆ’0.500.5\nArousalMechanical Turk Data Distribution\nâˆ’0.5 0 0.5âˆ’0.500.5Figure 5 . A-V distribution of data shown as a contour map. MoodSwings (left) and MTurk (right).\n4.2.1 Novelty Detection and One-Class SVM\nAs our data is unlabeled, we cannot formulate the identiï¬ca-\ntion of valid labels as a traditional binary classiï¬cation prob-\nlem. The expertsâ€™ labels exemplify how such labels may be\nclustered, a â€œpositive class,â€ but we encounter an unknown\nnumber of â€œnegative classes.â€ We use outlier (novelty) de-\ntection, training a supervised machine learning system on\nonly positive examples [13]. Our system uses the one-class\nSVM implementation from the SVM-KM toolbox.3We use\na Gaussian RBF kernel, tuning parameters to include most\ntraining data and exclude outliers. For a HIT to be approved,\nboth veriï¬cation clips must lie within our decision boundary\non average. Workers must be approved for at least 60% of\nHITs completed, else all of their HITs are rejected. After\nautomatic ï¬ltering, 113 workers had all HITs approved, and\n88 had all HITs rejected.\nBecause emotions cannot be classiï¬ed by machines with\nperfect accuracy, we use human judgments to measure the\neffectiveness of our automatic system. Plots of individual\nworkersâ€™ labels for veriï¬cation clips were visually exam-\nined by the authors. Annotations were classiï¬ed â€œapprovedâ€\nif they followed a similar trajectory to that of the expertsâ€™\nlabels over time, and â€œrejectedâ€ if they rapidly jumped be-\ntween quadrants or moved in the opposite direction of ex-\npert labels. Ambiguous labels, for instance, those that did\nnot follow a smooth trajectory, but moved towards the same\nquadrant as expert data, were labeled â€œunknown.â€ Classiï¬-\ncation performance is shown in Table 1.\n5. ANALYSIS OF COLLECTED DATA\nThe system collected 4,064 label sequences after two stages\nof ï¬ltering: ï¬rst evaluating veriï¬cation clip labels, and then\nremoving labeling sessions of workers who kept the cursor\n3http://asi.insa-rouen.fr/enseignants/ âˆ¼arakotom/toolbox/index.htmlManual Annotation Number Accepted Number Rejected\nApproved 398 162\nRejected 147 527\nUnknown 89 67\nPrecision Recall F-Measure\n0.73 0.71 0.72\nTable 1 . Classiï¬cation performance of automatic ï¬ltering\nsystem for HITs labeled â€œApproved,â€ â€œRejected,â€ and â€œUn-\nknown.â€\nat the origin for too long or consistently provided the same\nrating (e.g. consistently labeled all clips as angry through-\nout a game). We analyzed only the last half of each 30-\nsecond annotation round so that the ï¬rst 15-seconds could\ngive workers time to contemplate the mood of each clip. We\nassume that the relatively small number of workers who did\nnot move after 15 seconds misunderstood the task, and thus\nï¬ltered out their data. Table 2 shows statistics of the col-\nlected per-clip annotations in the dataset, before and after\nï¬ltering.\nUnï¬ltered Veriï¬cation Stage 2\nMetric Dataset Filtering Filtering\nMean 49.79 18.20 16.93\nSt. Dev. 4.328 2.480 2.690\nMax 72 24 23\nMin 39 8 7\nTable 2 . Number of MTurk worker annotations for each clip\nbefore and after ï¬ltering.\n55212th International Society for Music Information Retrieval Conference (ISMIR 2011)\nFeature/ Average Mean Average KL Average Randomized T-test\nTopology Distance Divergence KL Divergence\nMFCC 0.143Â±0.007 1 .501Â±0.148 2 .801Â±0.294 20 .68\nChroma 0.181Â±0.008 3 .555Â±0.302 3 .897Â±0.313 21 .08\nS. Shape 0.158Â±0.007 1 .733Â±0.172 2 .501Â±0.246 23 .51\nS. Contrast 0.141Â±0.007 1 .486Â±0.158 2 .821Â±0.297 21 .17\nM.L. Combined 0.130Â±0.006 1 .308Â±0.132 2 .928Â±0.310 20 .52\nTable 3 . MLR results for short-time (one-second) A-V labels, repeating the experiments of [5].\n5.1 Correlation Between Collected Labels\nWe compute Pearsonâ€™s product-moment correlation be-\ntween the datasets from MoodSwings and MTurk for each\ndimension. Pearsonâ€™s correlation between example random\nvariablesXandYis deï¬ned as:\nÏ=cov(X,Y )\nÏƒXÏƒY(1)\nTo account for discrepancies between the number of an-\nnotations for each clip, we treat their per-second sample\nmeans as observations. We smooth both sets of labels to\nreduce noise between observations, as the mood in each\nsecond of a song clip cannot be assumed to be indepen-\ndent from that of previous seconds [6]. The results, 0.712\nfor Arousal and 0.846 for Valence, show more correla-\ntion between the two datasets than Leeâ€™s comparison of\na MTurk-collected dataset to similarly crowdsourced data\n[11]. High correlation provides evidence that annotatorsâ€™\njudgments are unaffected by collaborating with a partner\nduring MoodSwings.\n5.2 Overall Distribution Comparison\nFigure 5 shows contour maps for the datasets collected with\nMoodSwings and MTurk. Both datasets have similar den-\nsities in the quadrant centers, though the MTurk dataset has\nhigher densities along the spacesâ€™ extremities, which could\nbe attributed to a larger sample. The MTurk dataset also\ncontains small peaks throughout the distribution, whereas\nthe MoodSwings set has more consistent clusters. It is difï¬-\ncult to pinpoint a cause for this difference, but multiple small\npeaks in the MTurk distribution may suggest that workers\nremain indecisive about their mood ratings throughout the\nduration of a clip. In previous work, we showed that it takes\n7-8 seconds on average for players to reach 85% of the to-\ntal distance from the origin to their ï¬nal mood labels [7].\nBy contrast, it took MTurk workers 10-12 seconds to reach\nthe same distance percentage. Faster convergence towards\na mood decision in the game could imply that collaboration\nencourages annotators to re-evaluate their ratings earlier in\nthe clips, perhaps improving the quality of collected data.5.3 Performance in Emotion Prediction\nTo further establish correlation between the datasets, we use\neach as ground truth for the time-varying emotion predic-\ntion experiments of our previous work [5]. The predic-\ntion systems utilize supervised machine learning algorithms\nto map A-V labels to content-based audio features, e.g.,\nmel-frequency cepstral coefï¬cients (MFCCs), chroma, and\nstatistical spectrum descriptors (SSDs), including spectral\nshape and contrast. Prediction performance for each feature,\nas well as combined performance using a multi-layer regres-\nsion method for late-feature fusion, using multiple linear re-\ngression (MLR) is shown in Table 3. The results are similar:\nall features rank in the same order, and in terms of over-\nall mean distance there is only slight improvement for the\nMTurk dataset. In terms of KL-divergence, the MTurk sys-\ntem performs signiï¬cantly better. However, high KL values\nin [5] were later attributed to noisy distribution estimates at\none-second intervals, taken independently from other time\nslices [6]. Increased performance on the MTurk set can be\nsimilarly attributed to the larger per-second sample sizes.\nImprovements based on the quantity of data collected are\nunrelated to the question of whether or not collaborative la-\nbeling biases the annotatorsâ€™ judgments.\n6. DISCUSSION AND FUTURE WORK\nThe strong positive correlation between data from\nMoodSwings and MTurk provides evidence that collaborat-\ning with a partner does not bias annotatorsâ€™ mood judgments\nany more than participating in a traditional labeling task.\nWe see similar mood prediction results between the label\nsets, although the MTurk set performs better with respect to\nKL-divergence. However, we attribute this increased per-\nformance to a larger sample size and propose that similar\nKL performance could be achieved if we collected a larger\nnumber of labels from MoodSwings. In terms of annotation\nquality, some evidence suggests that the game may be a su-\nperior data collection tool because it encourages participants\nto re-evaluate their ratings earlier in a labeling round. The\nset of labels from the MTurk annotation method is available\nto the Music-IR community for future research.\nThe logistics of each method are signiï¬cant considera-\n553Poster Session 4\ntions, particularly the pace of data collection and time spent\non quality control. Monetary incentives can attract anno-\ntators very quickly, but researchers must determine how to\nseparate anonymous paid annotators, e.g. MTurk workers,\nwith good intentions from those who wish to obtain pay-\nment for as little work as possible. We advise researchers\nseeking to crowdsource subjective judgments from paid an-\nnotators to be wary of the complexity of quality control. Our\none-class SVM system must be periodically retrained to ac-\ncount for varied mood judgments. As false approvals and re-\njections of mood labels are most accurately detected by hu-\nmans, this requires manual labeling, which can be very labor\nintensive. Crowdsourcing the veriï¬cation process may be a\nmore viable solution [14]. Reliable workers may be identi-\nï¬ed through overall approval ratings available from MTurk\nand cold verify othersâ€™ work in a separate task. However,\npaying a third party to verify results introduces further un-\ncertainty to the ï¬ltering process. We prefer to deal with vol-\nunteer annotators, who are more likely to produce quality\ndata without extensive ï¬ltering. Unpaid annotators do not\nbeneï¬t from producing a large quantity of low-quality an-\nnotations in a short amount of time. Because few people\nvolunteer for tedious traditional labeling tasks, we hope that\npresenting the task as a fun game will attract annotators.\nFurther mood prediction work necessitates more data col-\nlection. In particular, some of our planned work requires an-\nnotations for a much larger, more varied song set. We have\nfound the challenges of quality control for crowd-sourced\ndata collection need to be considered when choosing a col-\nlection method. While using MTurk is a viable option, we\nplan to concentrate some of our future efforts on improving\nMoodSwings. We wish to attract annotators to our game\nas quickly as we attracted them with payment on MTurk.\nThe redesign effort initiated with [7] made considerable\nstrides towards improving the game. Continuing this effort\nby revamping the user interface, deploying the game on mo-\nbile platforms (e.g., iOS and Android) or a social network-\ning website like Facebook.com and allowing participants to\nchoose their own music will provide a more varied and en-\nhanced gameplay experience. We hope these planned im-\nprovements will attract more annotators to our game. Deal-\ning with certain paid annotatorsâ€™ attempts to earn money for\nunsatisfactory work makes a strong case for employing vol-\nunteer annotators, who we believe are less likely to â€œgame\nthe system.â€\n7. REFERENCES\n[1] Y . E. Kim, E. M. Schmidt, R. Migneco, B. G. Morton,\nP. Richardson, J. Scott, J. A. Speck, and D. Turnbull,\nâ€œMusic emotion recognition: A state of the art review,â€\ninProc. of the 11th ISMIR Conf. , Utrecht, Netherlands,\n2010.[2] Y . E. Kim, E. Schmidt, and L. Emelle, â€œMoodswings:\nA collaborative game for music mood label collection,â€\ninProc. of the 9th Intl. Conf. on Music Information Re-\ntrieval , Philadelphia, PA, September 2008.\n[3] R. E. Thayer, The Biopsychology of Mood and Arousal .\nOxford, U.K.: Oxford Univ. Press, 1989.\n[4] E. M. Schmidt, D. Turnbull, and Y . E. Kim, â€œFeature se-\nlection for content-based, time-varying musical emotion\nregression,â€ in MIR â€™10: Proc. of the Intl. Conf. on Mul-\ntimedia Information Retrieval , Philadelphia, PA, 2010,\npp. 267â€“274.\n[5] E. M. Schmidt and Y . E. Kim, â€œPrediction of time-\nvarying musical mood distributions from audio,â€ in\nProc. of the 11th ISMIR Conf. , Utrecht, Netherlands,\n2010.\n[6] â€”â€”, â€œPrediction of time-varying musical mood dis-\ntributions using Kalman ï¬ltering,â€ in Proceedings of\nthe Ninth IEEE International Conference on Machine\nLearning and Applications , Washington, D.C., Decem-\nber 2010, pp. 655â€“660.\n[7] B. G. Morton, J. A. Speck, E. M. Schmidt, and Y . E.\nKim, â€œImproving music emotion labeling using human\ncomputation,â€ in HCOMP 2010: Proc. of the ACM\nSIGKDD Workshop on Human Computation , Washing-\nton, D.C., 2010.\n[8] R. Snow, B. Oâ€™Connor, D. Jurafsky, and A. Ng, â€œCheap\nand Fast - But is it Good? Evaluating Non-Expert Anno-\ntations for Natural Language Tasks,â€ in Proc. Empirical\nMethods in NLP , 2008.\n[9] J. Whitehill, P. Ruvolo, T. Wu, J. Bergsma, and J. Movel-\nlan, â€œWhose vote should count more: Optimal inte-\ngration of labels from labelers of unknown expertise,â€\ninAdvances in neural information processing systems .\nMIT Press, 2009.\n[10] A. Sorokin and D. Forsyth, â€œUtility data annotation with\namazon mechanical turk,â€ in CVPR Workshops , 2008.\n[11] J. H. Lee, â€œCrowdsourcing music similarity judgments\nusing mechanical turk,â€ in Proceedings of the 11th IS-\nMIR Conferenceth International Society for Music In-\nformation Retrieval Conference , Utrecht, Netherlands,\nAugust 2010, pp. 183â€“188.\n[12] M. I. Mandel, D. Eck, and Y . Bengio, â€œLearning tags\nthat vary within a song,â€ in Proceedings of the 11th IS-\nMIR Conferenceth International Society for Music In-\nformation Retrieval Conference , Utrecht, Netherlands,\nAugust 2010, pp. 399â€“404.\n[13] L. M. Manevitz and M. Yousef, â€œOne-class svms for\ndocument classiï¬cation,â€ in The Journal of Machine\nLearning Research , vol. 2, 2002.\n[14] I. Sprio, G. Taylor, G. Williams, and C. Bregler, â€œHands\nby hand: Crowdsourced motion tracking for gesture an-\nnotation,â€ in IEEE CVPR Workshop on Advancing Com-\nputer Vision with Humans in the Loop , 2010.\n554"
    },
    {
        "title": "MIR in School? Lessons from Ethnographic Observation of Secondary School Music Classes.",
        "author": [
            "Dan Stowell",
            "Simon Dixon"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1416980",
        "url": "https://doi.org/10.5281/zenodo.1416980",
        "ee": "https://zenodo.org/records/1416980/files/StowellD11.pdf",
        "abstract": "To help maximise the usefulness of MIR technologies in the wider community, we conducted an ethnographic study of music lessons in secondary schools in London, UK. The purpose is to understand better how musical concepts are negotiated with and without technology, so we can understand when and how MIR tools might be useful. We report on some of the themes uncovered, both about the range of technologies deployed in schools and about the ways different musical concepts are discussed. Importantly, this rich observation elicits some of the nuances between various highand low-technologies. In particular, we discuss issues of multimodality and the role of technologies such as Youtube, as well as specific issues around musical concepts such as genre and rhythm.",
        "zenodo_id": 1416980,
        "dblp_key": "conf/ismir/StowellD11",
        "keywords": [
            "ethnographic study",
            "music lessons",
            "secondary schools",
            "MIR technologies",
            "musical concepts",
            "technologies deployed",
            "musical concepts discussed",
            "nuances between high- and low-technologies",
            "multimodality",
            "YouTube"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nMIR IN SCHOOL? LESSONS FROM ETHNOGRAPHIC OBSERVATION OF\nSECONDARY SCHOOL MUSIC CLASSES\nDan Stowell and Simon Dixon\nCentre for Digital Music, Queen Mary University of London\ndan.stowell[]eecs.qmul.ac.uk\nABSTRACT\nTo help maximise the usefulness of MIR technologies in the\nwider community, we conducted an ethnographic study of\nmusic lessons in secondary schools in London, UK. The\npurpose is to understand better how musical concepts are ne-\ngotiated with and without technology, so we can understand\nwhen and how MIR tools might be useful. We report on\nsome of the themes uncovered, both about the range of tech-\nnologies deployed in schools and about the ways different\nmusical concepts are discussed. Importantly, this rich ob-\nservation elicits some of the nuances between various high-\nand low-technologies. In particular, we discuss issues of\nmultimodality and the role of technologies such as Youtube,\nas well as speciï¬c issues around musical concepts such as\ngenre and rhythm.\n1. INTRODUCTION\nOver the past decade the ï¬eld of Music Information Re-\ntrieval (MIR) has blossomed, leading to the creation of many\nuseful analysis techniques and systems. We wish to increase\nthe beneï¬t of MIR techniques to society, and to help develop\nMIR in ways that connect with new use cases in real-world\ncontexts. This requires that we work with user groups di-\nrectly, adapting our approach and conceptual toolset to that\nof the user groups: in other words, it requires recognising\nthat MIR has its associated culture with its own assumptions\nand interests, which may differ from the assumptions and in-\nterests of a particular user group, and working to bridge any\ndivides. Connecting with user communities in this way is\nnot just a way to disseminate research outputs, but can bring\nfresh ideas and perspectives into the research process.\nThe present study was conducted in this spirit, with a spe-\nciï¬c view to investigate how new digital music technologies\nmight be developed or adapted for the school music context.\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proï¬t or commercial advantage and that copies\nbear this notice and the full citation on the ï¬rst page.\nc/circlecopyrt2011 International Society for Music Information Retrieval.This paper discusses some of the issues brought out from\nresearch conducted in London secondary schools over the\nperiod November 2010 â€“ March 2011. The full ethnographic\nanalysis cannot be represented in six pages; in this paper\nwe ï¬rst describe the setup of the study before considering a\nrange of ï¬ndings relevant for the MIR community. We dis-\ncuss the use of different musical concepts (Section 3) and\ndifferent technologies (Section 4), before ending with a dis-\ncussion reï¬‚ecting on the lessons for the use of MIR technol-\nogy in the school music-lesson context.\n2. SETTING AND METHODS\nWe chose to use an ethnographic approach, so as to elicit\na rich thick description of the way music-related ideas are\nused and relate to each other in a speciï¬c context. The sen-\nsitising questions used to guide the ethnography were:\nWhat music-related concepts do teachers and\nstudents negotiate in music classes?\nHow do they achieve this â€“ with, and without,\ntechnology?\nWe note that such â€œsensitising questionsâ€ do not serve as\nnarrow research questions to be answered speciï¬cally, but\nas a thematic core for the observations and analysis.\nThe study was conducted in music lessons at two sec-\nondary schools in London. The two schools were selected\nafter contacting a small selection of comprehensive secondary\nschools in the London area with music programmes.\nâ€¢School A was located in East London, with around\n1200 students. The school had â‰ˆ15% having special\neducational needs, and â‰ˆ50% obtaining ï¬ve or more\nA*â€“C GCSEs and equivalent (a standard UK measure\nof attainment) in 2010. The music department had six\nfull-time music teachers.\nâ€¢School B was located in West London, with around\n1000 students. The school had â‰ˆ15% having special\neducational needs, and â‰ˆ30% obtaining ï¬ve or more\nA*â€“C GCSEs and equivalent in 2010. The perform-\ning arts department had two full-time music teachers.\n347Oral Session 5: User Studies\nEach school ran a two-weekly timetable, meaning the visits\n(over the period November 2010 â€“ March 2011) typically\ncovered about six lessons of each particular class. Various\nsecondary-level lessons were included in the study (Year 7\nto Year 11, i.e. students aged approx. 11â€“16).\nObservations were conducted by one observer with note-\nbook and pen; to minimise disruption and facilitate access,\nvideo/audio recording were not used. Analysis of the ï¬eld-\nnotes was conducted as described in [4] using focused cod-\ning followed by thematic analysis. In the following, any per-\nsonal names of teachers/students that appear are pseudonyms.\n3. THEMES OF MUSICAL CONCEPT\nA high-school classroom context of course involves nego-\ntiation of various types of known and unknown concepts.\nOne can get a ï¬rst impression of the concepts that are dis-\ncussed in music lessons by looking at the curriculum speciï¬-\ncation.1However, such a document does not reveal how the\ndiscussions might play out in the actual classroom context:\nwhich concepts are more easily negotiated through which\nmodalities, how new ideas relate to prior knowledge, and\nany subtleties in the way teachers and students approach dif-\nferent concepts.\n3.1 Musical instruments are easy\nMusical instruments, familiar and unfamiliar, were the basis\nfor many discussions in the observations, but were found to\nbe amenable to negotiation through a wide range of strate-\ngies: by name, by comparing against other known instru-\nments, by describing physical characteristics, by miming,\nby showing pictures, or of course by having one in the room\nto show and/or use. The concreteness of instruments â€“ they\nare generally physical objects â€“ is of course a strong reason\nfor this, allowing access to discussion of instruments includ-\ning those from unfamiliar cultures. Indeed, the most difï¬cult\nnegotiation observed in relation to musical instrument was\nabout the more abstract idea of classiï¬cation into Western\norchestral sections, for example why an electric guitar is not\nincluded in the string section. Even here, the conceptsâ€™ an-\nchoring in the concreteness of musical instruments makes\nthem amenable to negotiation.\nMusical notes were also relatively straightforward to ne-\ngotiate, by name (â€œC sharpâ€) or number (â€œthird fretâ€ or a\nnoteâ€™s number in a sequence), or by pointing at their position\non a keyboard or fretboard. This doesnâ€™t mean notes were\neasy to recognise or memorise â€“ note the recurrent practice\nof using a felt-tip to mark the note-names on the keys of\nthe MIDI keyboard â€“ but that there were stable commonly-\nunderstood ways to refer to them. In our observations, West-\n1http://curriculum.qcda.gov.uk/\nkey-stages-3-and-4/subjects/key-stage-3/music/ ,\nhttp://www.edexcel.com/quals/gcse/gcse09/music/ern 12tet tuning was an unchallenged common ground for\nnote tunings, and we might conjecture that this supported\nthe ease of discussion; although the schools did include some\nnon-Western music in their curriculum, we did not observe\nany discussion going beyond the Western 12tet scale.\n3.2 Genre terms are contextual and useful\nGenre-type terms were observed in many conversations, used\nto navigate known and unknown music â€“ both in curriculum-\noriented conversation and more informal conversation about\nmusic that people like or dislike. In the MIR context this is\nnotable because genre has been a topic of some debate â€“ see\nSection 5 for further discussion.\nThe use of genre-type terms has an important role in\nmapping out a landscape of musical styles and exploring\nthat landscape. Note that the labels do not form a compact\nor mutually-exclusive set of categories (unlike the â€œrecord\nshopâ€ approach to genre); instead they function more like\nlandmarks, having particular traits which can be discussed\nand compared against other genres.\nThe following brief excerpt shows a function of genre in\na lesson, as one student shares music with a peer:\nPreston (American, recently new to the class) was sitting at\na computer in the corner of the room, next to Terry. He was\nlistening to something on earphones. He offered one earphone\nto Terry:\nPreston : Check this out\nTerry took the earphone and listened. Preston turned up the\nvolume loud so it was audible in the room. He nodded along\nto the slowish beat and looked around the room smiling with a\nsatisï¬ed look.\nTerry : How dyou dance to this\n[Pause.]\nPreston : This is car music bro. You just ride\naround with a fucked-up ass car.\nThey carried on listening to the music.\nHere â€œcar musicâ€ functions as a genre term, deï¬ned through\na trait not of how it is made or its sonic aspects, but of what\nlisteners do with the music.\nThe excerpt concerns social music sharing rather than a\nclass task, yet the use of genre was consistent in many ob-\nservations. When genre labels are used in a task set by the\nteacher, they function as a route in to discussing and ï¬nd-\ning out about different ways of performing and using music,\nand thus broadening studentsâ€™ awareness. The labels often\ndonâ€™t appear as entirely new concepts, rather as references\nto musical styles including ones the students may only have\na vague awareness of. Thus an important role of genre terms\nhere appears to be to provide named landmarks to navigate\nthe world of known and unknown musics.\n34812th International Society for Music Information Retrieval Conference (ISMIR 2011)\nNote also that there may be negotiation of musical genre\nterms â€“ the terms are not universal/objective, but local ne-\ngotiation is sometimes required to come to an agreed un-\nderstanding. Hence in one class task, the term â€œbhangraâ€\nwas unfamiliar to some students, leading to a discussion re-\nsolving its meaning by reference to known terms such as\nâ€œIndianâ€ and â€œBollywoodâ€. Although such a comparison\nmight seem inaccurate to some bhangra/Bollywood fans, it\nhelped resolve the term â€œbhangraâ€ as a landmark useable in\nfurther group discussion.\n3.3 Nameless rhythms\nIn contrast to the genre talk just considered, negotiation of\nrhythm generally occurred without a stable set of labels or\nways to refer to different rhythms: rhythms generally were\nincluded in discussion only by acting them out â€“ whether on\na drum, by clapping, or vocally.\nActing out rhythms is an important part of music edu-\ncation, but discussion can be impeded if there is no shared\nset of common terms used as shorthand. (Musicologists do\nhave names for many rhythms, e.g. son clave ; and note du-\nrations can be named as e.g. quaver orquarter note , though\nthese donâ€™t lead directly to shorthand names for rhythms.)\nIn our observations we found a general tendency for rhythm\ntalk to be limited by this lack of names, sometimes causing\nconfusion or difï¬culties in remembering which is which.\nThe closest to a stable terminology was the â€œone and two\nand three andâ€ approach used by some musicians, though\neven here there was ambiguity, in part because counting\ncan be done at different metrical rates, or the accents can\nbe counted rather than the underlying tactus. For example,\non one occasion a teacher talked this approach through out\nloud, saying â€œone and two and three and four andâ€ and ask-\ning the students, â€œwhich number was the â€˜andâ€™ after?â€ How-\never he became unsure himself, miming playing the drums\nwhile saying â€œone and two andâ€ and then â€œone and two and\nthree andâ€, and coming to the decision that the right answer\nwas three. This answer seemed not to affect subsequent use\nof the rhythm in class, since the rhythm pattern was subse-\nquently negotiated only by performing it, not by referring to\nany â€˜andâ€™s or numbers.\n4. THEMES OF MODALITY\nHaving contrasted the uses of some different types of mu-\nsical concept in music lessons, we next turn to consider the\nmodalities used by teachers and students.\n4.1 Multimodality\nFrom our observations we found a strong pattern in the tech-\nnological and non-technological modes that teachers and\nstudents use to negotiate music-related concepts: they use awide variety of modes, both digital and otherwise, in quick\nsuccession and often in parallel. The classroom is a rich\nenvironment in which a wide variety of resource types can\nbe called upon instantly, without necessarily planning in ad-\nvance. To give an overview â€“ teachers and students:\nâ€¢talk about musical concepts verbally, using descrip-\ntions, counting, and references to known artists/musics;\nâ€¢they demonstrate concepts by acting them out using\nphysical instruments, voice, software sequencers, or\n(surprisingly often) mime;\nâ€¢they convey concepts by talking someone else through\nacting them out;\nâ€¢they call upon resources including posters, physical\ninstruments, smartphones/MP3 players, slideshows,\nWikipedia articles, Youtube videos, and web searches;\nâ€¢and they share speciï¬c music pieces by means of head-\nphones, earphones, loudspeakers, singing, and occa-\nsionally ï¬le-transfer.\nThis list is an aggregation, but not an aggregation of dis-\nparate phases of activity: the prevailing behaviour of teach-\ners and students during music lessons involves using many\nof these in parallel, even when a task set for students might\nformally seem to revolve around one speciï¬c mode.\nOne example of a technology incorporated into the resource-\nrich classroom context is the Interactive Whiteboard (IW) â€“\ni.e. a projector screen with a touch interface, and the abil-\nity to be written on with digital pens etc. In the UK there\nwas previously special funding for IWs in schools, and they\nwere present in all classrooms observed. However, there\nwas a very strong pattern in the use of IWs, which was\nthat they were heavily used as more â€œtraditionalâ€ projection\nscreens and rarely if ever for their touchscreen or digital-\npen capabilities. The projected screen was very often used\nby the teacher to project Powerpoint slides (of task instruc-\ntions, learning objectives, descriptions of musical concepts),\nto demonstrate software use (how to ï¬ll in a form, or use a\nmusic sequencer), and to play videos. Students were often\nallowed to control what was projected, e.g. in choosing a\nmusic video. It was rare for a classroom session not to in-\nvolve the projected screen: it often served as a focal point\n(e.g. when playing a video to the class), and also very often\nas a highly visible place to leave reference information, such\nas task instructions or a musical scale or chord progression.\nThe projected screen was very commonly used in conjunc-\ntion with other resources, such as playing back a video while\nstudents played along to it using instruments.\nThere are multiple potential explanations for why the IWâ€™s\ninteractive features were not generally used. Teachers and\n349Oral Session 5: User Studies\nstudents both showed awareness of how to use those fea-\ntures, such as by tapping the screen to dismiss a screen-\nsaver; so lack of awareness was not a factor here. Rather it\nseems that the projected screen is easily incorporated along-\nside other activities such as playing an instrument or writing,\nwhile the IW-speciï¬c features make demands (such as being\nclose to the screen, and sometimes holding a special pen)\nwhich reduce their ease of integration into multimodal ac-\ntivities. Contrast this with Shannon and Cunnighamâ€™s study\nin a class of young children with special needs, in which\nthe largest effect was said to be that IW placement and other\nfactors led to symbolic â€œownershipâ€ of the IW by the teacher\n[7]. We did not observe such effects in our study, with stu-\ndents generally as comfortable as the teachers to make use\nof the IW, but both used its projected screen as part of mul-\ntimodal activities rather than using the interactive features.\n4.2 Youtube\nOne of the most-used technologies in the classes observed\nwas the youtube.com website. (There was some non-\nYoutube use of internet video, but to a very much smaller ex-\ntent.) Youtubeâ€™s breadth of coverage appears to be what sup-\nports its thorough integration into classroom practice: stu-\ndents and teachers often searched in Youtube without hav-\ning checked in advance they would ï¬nd something relevant,\nand almost always found a video which satisï¬ed them.\nYoutube was used by teachers and by students for many\npurposes, including:\nâ€¢playing a song to support a lesson topic (e.g. to demon-\nstrate a musical style);\nâ€¢playing documentaries about musical topics;\nâ€¢playing examples of live performance;\nâ€¢playing a track to work out its chords and/or instru-\nmentation;\nâ€¢playing a â€œwith-lyricsâ€ video of a track (showing ani-\nmated lyrics), to work out or sing along to the lyrics;\nâ€¢playing a track to perform along to (playing instru-\nments and/or dancing);\nâ€¢playing back old TV/radio adverts (to demonstrate the\nuse of music in them);\nâ€¢playing background music quietly;\nâ€¢ï¬nding sound effects or soundtrack elements whose\naudio could be ripped and used;\nâ€¢and music sharing (playing liked music to others).There was a strong overlap between teachersâ€™ and studentsâ€™\ninitiation of Youtube for these uses, and a strong overlap in\nwhether the projected screen or a studentâ€™s individual screen\nwas used for playback.\nContrary to the suggestions made by Webb [9], Youtube\nusage was generally not oriented around carefully-planned\nand -structured video-based activities, but as a resource ca-\nsually integrated into many multimodal activities. A re-\nsource treated in the same way was Wikipedia, a source\ncommonly turned to for factual and textual information (as\nwell as web searches more generally). Wikipedia shares\nwith Youtube the features of having a very broad coverage\nand text search, allowing teachers and students to use it at\nshort notice without having to consider in advance whether\nmaterial will be found.\n4.3 Singing\nSinging is used within music lessons, sometimes as the main\nfocus of an activity, sometimes brieï¬‚y to convey a melody\nor musical idea. However, the use of singing as a medium\nis not always straightforward: singing in UK culture can\nbe susceptible to embarrassment and concern with being\nâ€œout of tuneâ€, with speciï¬c inhibition at secondary school\nage [5]. In the following excerpt, in which students were\nplaying/singing along to Coldplayâ€™s â€œClocksâ€ on a with-\nlyrics Youtube video, we see how a reluctance to sing can\naffect the progress of a task which requires it:\nOn the screen, Amy had been searching the web and nav-\nigated to a webpage showing the lyrics to â€œClocksâ€. The\nYoutube video was still playing (in a background window or\ntab) but then it ended.\nJo to Amy/Donna : Are you guys ready to sing?\nDonna : [Pause.] No.\nAmy : We need Andrew.\nJo: Iâ€™ll play it and you sing, we need to practice\nit.\nJo played the chords, but Amy/Donna seemed unwilling to\nsing. Corinne (the teacher) came back in.\nCorinne : Right has the music ï¬nished?\nAmy : Yes\nCorinne : Right letâ€™s have a run-through. Toby\nstart with the bass.\nToby : Me?\nAfter a pause, Toby started with the bass. Jo joined in on guitar.\nThen Amy/Donna sang but very quietly.\nCorinne : Right stop. Can you guys hear them\nsinging?\nJo: No\nToby [loudly] : No!\nCorinne negotiated with Amy and Donna to try and encourage\nthem to sing more loudly. Amy protested that â€œwhen I sing\nloud it goes out of tuneâ€. Corinne got the group to do another\nplaythrough, but Amy and Donna started singing then stopped,\nsaying they didnâ€™t know where they were in the words.\n35012th International Society for Music Information Retrieval Conference (ISMIR 2011)\nBeyond the end of this excerpt, the two students offered\nfurther reasons not to sing. The multiplicity of reasons given,\nwhether or not they were the main motivations for reluc-\ntance to sing, suggest that singing can in some contexts in-\nduce conï¬dence issues which instrument-playing generally\ndoes not.\nHowever singing is not always so inhibited. In some\nlessons, students would spontaneously sing together for fun\n(not connected with a class task). Sometimes the teacher\nwould co-opt this for a learning purpose, while sometimes\nit would continue separately from the class task.\n4.4 Exploration\nA theme that cuts across all modalities is that of student ex-\nploration. Most classroom activities are unbounded, with\nstudents engaged in exploratory and/or creative tasks. This\nis in part connected to the teaching strategies currently in\nuse; here we are concerned with the implications for tech-\nnology design.\nThe casual use of various modalities and resources is\npart of this tendency towards exploration. For example, the\nsearch and browsing features of Youtube, Wikipedia and\nweb search were often used to explore available informa-\ntion, beyond the basic satisfaction of a single search objec-\ntive. Exploration was also how students engaged with mu-\nsical instruments, trying out new possibilities (such as the\nvarious sounds available on a MIDI keyboard, or what hap-\npens when you shout into a saxophone).\nIt is worth noting that the authorised/unauthorised sta-\ntus of much student activity is ambiguous, in part because\nof this exploratory mode. Studentsâ€™ actions evolve quickly\nin interaction with many things around them (socially and\nphysically), and even if one particular action is authorised/-\nunauthorised by a teacherâ€™s intervention, the studentsâ€™ ac-\ntivity very quickly moves beyond that speciï¬c action. Even\nactions which start out as speciï¬cally non-curricular (social\nor undirected) may be co-opted by the teacher.\nIt is evident that technologies which support broad ex-\nploratory activity are more likely to be generally useful, and\nthat the authorised/unauthorised status of activities can only\nbe determined in the particular context in negotiation be-\ntween teacher and students. There were occasions when\nexploratory activity caused problems for teachers â€“ such\nas when students spent more time formatting their Power-\npoint presentation than researching musical concepts for it\nâ€“ but teachers often encourage exploratory activity as part\nof lessons.\n4.5 Music sharing\nMusic sharing has been discussed in the literature most often\nin terms of social music sharing (e.g. [3]), but of course mu-\nsic lessons are a context in which people share well-knownand unfamiliar music with each other. For this reason, and\nalso because we observed non-curricular instances of mu-\nsic sharing in the classroom context, the various modes and\nmeanings of music sharing in music education emerged as a\nrecurrent theme in our analysis.\nIn the age of the Internet, developments in the music in-\ndustry have led to the idea of â€œmusic sharingâ€ becoming as-\nsociated with digital circulation of music recordings. In our\nstudy, students did occasionally share music with each other\nor with teachers by sending ï¬les electronically, but more\noften they might share their earphones to share what they\nare listening to, or sing a melody out loud, or tell someone\nhow to search for a particular artist online. We observed\nmany instances of music sharing, with the most common\nmodes being sharing headphones/earphones, playing tracks\nout loud, and singing. As noted in the previous discussion, it\nis often unclear whether speciï¬c instances of music sharing\nare authorised or unauthorised in a particular music lesson,\nand there can be conversion between the two: teachers often\nmake use of music that students like, to enhance engagement\nand to connect musical concepts to familiar music.\nIn our observations, the vast majority of students had mo-\nbile phones/MP3 players and earphones with them, so mu-\nsic sharing by sharing earphones could and did happen quite\noften. Although we were studying the music lesson context\nand not the studentsâ€™ lives more generally, the casual avail-\nability of speakers, earphones and singing seemed to make\nthem the preferred form of music sharing, rather than digital\nmeans. Compare this with Laplanteâ€™s study [6] which em-\nphasises the importance of young peopleâ€™s social networks\n(both strong and weak ties) in music discovery, though La-\nplante does not directly explore which modalities are used\nseparating out different possible modes of music sharing.\n5. DISCUSSION\n5.1 Genre and labels\nGenre has been the subject of debate in the MIR community,\nfrom foundational genre classiï¬cation experiments [8] to\nmore recent discussions problematising the â€œrecord-shopâ€\nmodel of genre and moving towards more multi-facteted ap-\nproaches such as social tagging [1] â€“ or towards the aban-\ndonment of genre labels in favour of music similarity met-\nrics. The outcomes from this study suggest that the aban-\ndonment of genre-type labels would be a mistake, as such la-\nbels function as useful landmarks in the negotiation of both\nfamiliar and unfamiliar musics. The comparison against\nrhythm talk is illustrative: the lack of stable labels for rhythms\ncan make discussion unwieldy. (MIR tools to help under-\nstand rhythm might help address this, and/or perhaps the\nuse of speciï¬c rhythm labels in teaching.)\nIn this respect the work of Craft [2] accords well with\nour observations. Craft argues that genre is not an inherent\n351Oral Session 5: User Studies\nattribute of a track, but a label that emerges from a personâ€™s\ninteraction with it and with their context: â€œmeanings of mu-\nsic, such as the categories into which an individual puts mu-\nsic, are emergent qualities of the music when given social\ncontextualization, rather than merely objective attributes of\nitâ€ (p. 167). Further, he argues that a situated approach to\ngenre is nevertheless amenable to analysis by MIR tools.\nOur research supports this position and suggests that such\nan approach would be more likely to make such analyses\nuseful to real-world contexts such as school music lessons.\n5.2 Designing for multimodality and exploration\nOur study found that teachers and students predominantly\nengaged in highly multimodal activities during music lessons.\nTeachers and students use a variety of technologies casu-\nally, often in parallel/combination and without prior plan-\nning. Also, most student activity is exploratory in nature,\ndue to both the tasks set by teachers and the studentsâ€™ inter-\nactions with their environment. Technologies designed for\nthe classroom must ï¬t with these modes of use: they must be\namenable to use in combination with other resources/tech-\nnologies, at short notice, and ideally facilitate exploration\nacross a wide range of potential topics. They should not be\ndesigned as if they will be the focus of uninterrupted atten-\ntion for long periods, but function as part of the rich class-\nroom environment, often lying latent until needed.\nDiscussion of technology and education often focuses on\nthe high-tech, but the combination of high- and low-tech\nmust be remembered. Physical musical instruments are of\ncourse used in music lessons for various purposes, but also\nsinging, mime and posters are called upon as part of negoti-\nating musical ideas. On one speciï¬c topic, we note the issue\nof studentsâ€™ potential anxiety when asked to sing, at least in\nthe UK context, while singing is an activity that music teach-\ners often want to encourage and develop. Any MIR system\nthat worked with the singing modality (such as query-by-\nsinging/humming, singing transcription) would need to be\ndesigned with sensitivity to such issues.\nReturning to the idea of open-ended exploration, it may\nbe a challenge to build a system with a breadth of coverage\non the order of that of Youtube or Wikipedia. One solution\nmight be to piggyback on larger systems such as Youtube\n(for example, offfering an MIR analysis of Youtube videos\non demand). Alternatively, linked data and the semantic web\noffer the potential to connect up with myriad large music-\nrelated resources, so might provide the infrastructure for a\nuseful resource.\n6. ACKNOWLEDGEMENTS\nWe wish to thank the schools involved for their participa-\ntion. This study was conducted as part of EPSRC projectEP/I001832/1, Musicology for the Masses .2\n7. REFERENCES\n[1] J.-J. Aucouturier and E. Pampalk. Introduction â€“ from\ngenres to tags: A little epistemology of music informa-\ntion retrieval research. Journal of New Music Research ,\n37(2):87â€“92, 2008.\n[2] A. Craft. The role of culture in Music Information Re-\ntrieval: a model of negotiated musical meaning, and\nits implications on methodology and evaluation of the\nmusic genre classiï¬cation task . PhD thesis, Goldsmiths\nCollege, University of London, 2008.\n[3] S. J. Cunningham and D. M. Nichols. Exploring social\nmusic behaviour: An investigation of music selection\nat parties. In Proceeding of the 10th International Soci-\nety for Music Information Retrieval Conference (ISMIR\n2009) , pages 26â€“30, Kobe, Japan, 2009.\n[4] R. M. Emerson, R. I. Fretz, and L. L. Shaw. Writing\nethnographic ï¬eldnotes . Chicago guides to writing, edit-\ning, and publishing. University of Chicago Press, 1995.\n[5] A. Lamont, D. J. Hargreaves, N. A. Marshall, and\nM. Tarrant. Young peopleâ€™s music in and out of school.\nBritish Journal of Music Education , 20(03):229â€“241,\n2003.\n[6] A. Laplante. Everyday life music information-seeking\nbehaviour of young adults: an exploratory study . PhD\nthesis, School of Information Studies, McGill Univer-\nsity, 2008.\n[7] G. Shannon and S. J. Cunningham. Impact of class-\nroom design on interactive whiteboard use in a special\nneeds classroom. In Proceedings of the 10th Interna-\ntional Conference NZ Chapter of the ACMâ€™s Special In-\nterest Group on Human-Computer Interaction , pages 1â€“\n4. ACM, 2009.\n[8] G. Tzanetakis, G. Essl, and P. Cook. Automatic musical\ngenre classiï¬cation of audio signals. In Proceedings of\nthe International Symposium on Music Information Re-\ntrieval (ISMIR) , pages 205â€“210, 2001.\n[9] M. Webb. Music analysis down the (You) tube? Ex-\nploring the potential of cross-media listening for the\nmusic classroom. British Journal of Music Education ,\n24(02):147â€“164, 2007.\n2http://www.elec.qmul.ac.uk/digitalmusic/m4m/\n352"
    },
    {
        "title": "OCR based post processing of OMR for the recovery of transposing instruments in complex orchestral scores.",
        "author": [
            "Verena Thomas",
            "Christian Wagner 0003",
            "Michael Clausen"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1415790",
        "url": "https://doi.org/10.5281/zenodo.1415790",
        "ee": "https://zenodo.org/records/1415790/files/ThomasWC11.pdf",
        "abstract": "Given a scanned score page, Optical Music Recognition (OMR) attempts to reconstruct all contained music information. However, the available OMR systems lack the ability to recognize transposition information contained in complex orchestral scores. 1 An additional unsolved OMR problem is the handling of orchestral scores using compressed notation. 2 Here, the information of which instrument has to play which staff is crucial for a correct interpretation of the score. But this mapping is lost along the pages of the score during the OMR process. In this paper, we present a method for retrieving the instrumentation and transposition information of orchestral scores. In our approach, we combine the results of Optical Character Recognition (OCR) and OMR to regain the information available through text annotations of the score. In addition, a method to reconstruct the instrument and transposition information for staves where text annotations were omitted or not recognized is presented. In an evaluation we analyze the impact of transposition information on the quality of score-audio synchronizations of orchestral music. The results show that the knowledge of transposing instruments improves the synchronization accuracy and that our method helps in regaining this knowledge.",
        "zenodo_id": 1415790,
        "dblp_key": "conf/ismir/ThomasWC11",
        "keywords": [
            "transposition information",
            "complex orchestral scores",
            "compressed notation",
            "instrumentation",
            "transposition information",
            "score-audio synchronizations",
            "instrument mapping",
            "staves",
            "text annotations",
            "retrieving information"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nOCR-BASED POST-PROCESSING OF OMR FOR THE RECOVERY OF\nTRANSPOSING INSTRUMENTS IN COMPLEX ORCHESTRAL SCORES\nVerena Thomas Christian Wagner Michael Clausen\nComputer Science III, University of Bonn\n{thomas,wagnerc,clausen }@iai.uni-bonn.de\nABSTRACT\nGiven a scanned score page, Optical Music Recognition\n(OMR) attempts to reconstruct all contained music informa-\ntion. However, the available OMR systems lack the ability\nto recognize transposition information contained in complex\norchestral scores.1An additional unsolved OMR problem\nis the handling of orchestral scores using compressed no-\ntation.2Here, the information of which instrument has to\nplay which staff is crucial for a correct interpretation of the\nscore. But this mapping is lost along the pages of the score\nduring the OMR process. In this paper, we present a method\nfor retrieving the instrumentation and transposition informa-\ntion of orchestral scores. In our approach, we combine the\nresults of Optical Character Recognition (OCR) and OMR\nto regain the information available through text annotations\nof the score. In addition, a method to reconstruct the in-\nstrument and transposition information for staves where text\nannotations were omitted or not recognized is presented. In\nan evaluation we analyze the impact of transposition infor-\nmation on the quality of score-audio synchronizations of\norchestral music. The results show that the knowledge of\ntransposing instruments improves the synchronization accu-\nracy and that our method helps in regaining this knowledge.\n1. INTRODUCTION\nA conductor reading an orchestral score can easily recog-\nnize which instrument is notated in which staff of a system.\nWe gratefully acknowledge support from the German Research Foun-\ndation DFG. This work has been supported by the P ROBADO project (grant\nCL 64/7-2) and the A RMADA project (grant CL 64/6-2).\n1For transposing instruments the written notes are several semitones\nhigher/lower than the sounding notes.\n2In our context, the notion of compressed score is used to describe a\nscore, where after the ï¬rst system staves of instruments not playing are\ntemporarily removed from a system.\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proï¬t or commercial advantage and that copies\nbear this notice and the full citation on the ï¬rst page.\nc/circlecopyrt2011 International Society for Music Information Retrieval.For this to be possible, a set of common conventions of type-\nsetting scores was developed. Examples are the introduction\nof all instruments playing in a piece of music by labeling the\nstaves of the ï¬rst system, a ï¬xed instrument order or the us-\nage of braces and accolades to cluster instruments [12]. In\ncase of compressed scores, in addition to the labeling of the\nï¬rst system, subsequent systems are annotated with instru-\nment text labels as well (see Figure 1). However, these are\ntypically annotated by abbreviations instead of full instru-\nment names. In several scores, labels are omitted when a\nsystem does not differ structurally from the preceding sys-\ntem.\n..................\nFigure 1 .Extracts from Franz Liszt: Eine Sinfonie nach Dantes\nDivina Commedia using compressed notation (Publisher: Breit-\nkopf & H Â¨artel ).\nThe PROBADO project3aims at developing a digital li-\nbrary system offering new presentation methods for large\ncollections of music documents (i.e., scans of sheet music\nand digitized music CDs). Similar to a conductor following\nthe score while listening to a performance, the PROBADO\nsystem highlights the measure in the score matching the cur-\nrently audible part of an audio track. One prerequisite for\nthis type of presentation is a mapping/synchronization of\npixel areas in the score scans to time intervals in the au-\ndio track (see Section 3). The ï¬rst step in calculating this\nmapping is the reconstruction of the musical information\ncontained in the score scans using OMR.4However, for\norchestral scores the existing OMR systems lack the abil-\nity to reconstruct all information given in the score. Or-\nchestral scores contain instrumentation information which\n3http://www.probado.de\n4We apply SharpEye2 (http://www.visiv.co.uk )\n411Poster Session 3\nmight be important, e.g., for extracting the score of a sin-\ngle instrument. In addition, the instrument text labels also\nmark transposing instruments in the score. Their ignorance\nresults in shifts of single voices with respect to the rest of\nthe voices in the score. In [14] the impact of typical OMR\nerrors on the results of score-audio synchronizations was an-\nalyzed. It turned out that lacking transposition information\nhas to be classiï¬ed as the most inï¬‚uential OMR error. This\nsuggests that the transposition information contained in the\nscore should be reconstructed. Unfortunately, at the cur-\nrent state no OMR system known to us offers the extraction\nof transposition information as well as a correct instrument\nlabeling5of the score. SharpEye provides some text recog-\nnition. However, the recognitions are not analyzed with re-\nspect to instrument names and are not mapped to the ac-\ncording staves, let alone propagated to the following (un-\nlabeled) systems. The OMR system PhotoScore Ultimate\n66offers instrument labeling to some extent. The included\nOCR engine recognizes the instrument texts (often includ-\ning transpositions) and maps them to the staves. In addi-\ntion, the recognized instrument text labels from the ï¬rst sys-\ntem are propagated to the following systems. However, the\nused method seems to be rather simple. PhotoScore maps\nthe instrument text labels extracted from the ï¬rst system to\nthe following systems line by line. Unfortunately, text la-\nbels from these systems and structural differences in case of\ncompressed scores are ignored. Therefore, particularly for\ncompressed scores, incorrect instrument labelings are cre-\nated. Another OMR system dealing with instrument labels\niscapella-scan .7The observed abilities of capella-scan to\ncreate and propagate instrument and transposition labels are\ncomparable to those of PhotoScore . In both OMR systems,\nthe recognized transposition text labelsâ€”even if correctly\nrecognizedâ€”do not seem to be transformed into transposi-\ntion labels that are considered during the creation of a sym-\nbolic representation, such as MIDI or MusicXML.\nIn OMR research two crucial questions exist: Firstly,\nwhich music format is processed? Each format (e.g., hand-\nwritten score, medieval score) calls for specialized recon-\nstruction methods. Secondly, what is the application sce-\nnario? The intended application strongly inï¬‚uences the re-\nquired OMR accuracy. On the one hand, there are situa-\ntions where an exact reconstruction of the score is crucial.\nIn this context, OMR systems that allow for manual cor-\nrections of the recognition results were proposed (e.g., [5]).\nLearning mechanisms integrated into those systems then use\nthe user feedback to gradually improve the OMR accuracy.\nOn the other hand, some applications demand OMR pro-\n5In contrast to the text actually placed on the scoreâ€”which we call\ninstrument text label â€”instrument labels are language independent. All\nknown abbreviations or names of the same instrument are mapped to the\nsame label. These labels are used to identify which instrument is meant to\nplay in a staff.\n6http://www.sibelius.com/products/photoscore/\nultimate.html\n7http://www.whc.de/capella-scan.cfmcesses providing a sufï¬cient quality without requiring user\ninteractions. In our scenario, we are interested in process-\ning a large data collection with as little user interaction as\npossible. The generated OMR results are only required for\nscore-audio synchronization, which is robust with respect to\nmissing notes and incorrect note durations. Therefore, in\nthis situation accuracy loss in favor of automation is desir-\nable.\nAlthough a great deal of research on OMR has been con-\nducted (see, e.g., [1]), the special challenges of orchestral\nscores have not yet been addressed. However, the extrac-\ntion of instrumentation and transposition information has to\nbe considered a crucial part of OMR for orchestral scores\nregardless of whether the goal is an exact digital reconstruc-\ntion of the score (e.g., for score-informed voice separation)\nor a rough representation intended for further processing.\nIn this contribution we present a method to reconstruct\nthe missing instrument and transposition labels in orches-\ntral scores. We combine OCR and OMR to regain informa-\ntion from text labels in the score. Subsequently, instrument\nand transposition labels for staves lacking text annotations\nare reconstructed using music-related constraints and prop-\nerties.\nIn Section 2 we will describe our instrument and trans-\nposition label reconstruction method. In Section 3 we will\ngive a short description of the applied score-audio synchro-\nnization technique. Afterwards, the results of our evaluation\nusing a set of 11 orchestral pieces are presented and dis-\ncussed. We close this paper with a summary and an outlook\non possible future work in Section 4.\n2. METHOD\nWe present our method to reconstruct the instrument and\ntransposition labels in staves of orchestral scores. Basically,\nthe algorithm can be subdivided into three parts: In the ï¬rst\npart of the process (Subsection 2.1) the text areas on the\nscore scans are identiï¬ed and processed by an OCR soft-\nware. Subsequently, the recognition results are transformed\ninto instrument labels and matched to the corresponding\nstaves. After this step, all staves, where textual informa-\ntion was given in the score and recognized by the OCR soft-\nware, possess an instrument label. But in orchestral scores,\nafter the ï¬rst system, instrument text labels are often omit-\nted. Therefore, in the second step of the algorithm (Sub-\nsection 2.2) missing labels are reconstructed by propagating\nexisting labels. Afterwards, each staff has an instrument la-\nbel associated with it. In the ï¬nal step of the algorithm the\ntransposition labels that were found in the ï¬rst system are\npropagated through the score (Subsection 2.3).\nWe impose some assumptions on the scores processed\nwith our method:\nâ€¢The ï¬rst system contains all instrument names that occur\nin the piece.\n41212th International Society for Music Information Retrieval Conference (ISMIR 2011)\nâ€¢The instrument order established in the ï¬rst system is not\nchanged in subsequent systems.\nâ€¢A maximum of two staves share a common instrument\ntext label.\nâ€¢When ï¬rst introduced, full instrument names are used.\nâ€¢For compressed scores, text labels are given if the instru-\nmentation changed compared to the preceding system.\nFor most orchestral scores these assumptions are met.\nWe will now provide a detailed account of the three steps\nof the instrument and transposition labeling algorithm. For\nan even more extensive description we refer to [15].\n2.1 OCR-based instrument labeling\nIn this part of the reconstruction, we analyze textual infor-\nmation given on the score sheets to create instrument and\ntransposition labels.\nFirst, given a scanned score image, the contained con-\nnected components (CCs) of black pixels are determined\n[11, 15]. Afterwards, CCs that deï¬nitely do not contain let-\nters are discarded. Using a sweep line algorithm [3] hori-\nzontally neighboring CCs are then merged to form words.\nSubsequently, the thereby determined image areas are used\nas input for the ABBYY FineReader 10 OCR software.8\nAt this point, we have a list of OCR recognitions and\ntheir positions on the score scans. To achieve a proper in-\nstrument labeling two additional steps are required. First,\nthe recognized text is compared to an instrument library.\nThe library contains names and abbreviations for typical or-\nchestral instruments in German, English, French, and Ital-\nian. Using the Levenshtein distance [8], the library entries\nwith the longest word count that are the most similar to the\nrecognitions are identiï¬ed and used as instrument labels in\nthe according text areas. Secondly, using the staff position\ninformation available in SharpEye , the identiï¬ed instrument\nlabels are mapped to the according staves of the score.\nIn the majority of cases, transposition information is\navailable from text labels like â€œclarinet in Aâ€ (see Figure 2).\nTo detect transpositions we therefore search for occurrences\nof text labels containing the keyword â€œinâ€ followed by a\nvalid transposition.\n2.2 Instrument label reconstruction\nThis section constitutes the main part of the proposed meth-\nod. We will use the labeling from the previous section as\ninitialization of an iterative process to reconstruct the label-\ning for all staves. Given the score of a piece of music, we\ndeï¬ne the sequence of all systems M= (M0,...,M m)and\nthe set of all instrument labels Iof systemM0that were re-\nconstructed in Section 2.1. With S= [1:N]we enumerate\nall the staves in Mand letSaâŠ‚Sdenote the staff num-\nbers corresponding to Ma. Furthermore, we create a ma-\n8http://finereader.abbyy.comtrixÏ€âˆˆ[0,1]SÃ—I, whereÏ€(i,I)will be interpreted as the\nâ€œplausibilityâ€ of staff ihaving the instrument label I. The\nsubmatrixÏ€aâˆˆ[0,1]SaÃ—Icorresponds to Ma. We initial-\nizeÏ€with the instrument labels determined in Section 2.1.\nAs plausibility values, the Levenshtein distances between\nthe instrument labels and the original instrument text on the\nscore sheets are applied. Note that due to this initialization,\nseveral instruments might be mapped to one staff (e.g., for\nthe text label â€œviola and violoncelloâ€). Afterwards, the plau-\nsibility matrix Ï€0:=Ï€is iteratively updated using an update\nmethod that can be subdivided into three steps\nÏ€k+1=IOCâ—¦IPâ—¦POP (Ï€k).\nWe will now explain these three steps of the update process\nin chronological order.\n2.2.1 Propagation of plausibilities (POP)\nIn this step we will propagate the plausibilities from system\nMato systemMb, for several a < b speciï¬ed below. To\nperform a plausibility propagation, we ï¬st calculate the set\nCa,bâ‰¡Ca,b(Ï€a,Ï€b)consisting of all triples (i,j,I )âˆˆSaÃ—\nSbÃ—Iwhose joint plausibility Ï€a(i,I)Â·Ï€b(j,I)is positive.\nWe then reduce Ca,bby removing all crossings. A crossing\nbetween two triples (i,j,I )and(k,/lscript,K )withi<k occurs\nifj > /lscript . In case of a crossing, the triple with smaller joint\nplausibility is removed. The resulting set will be denoted\nbyC/prime\na,b. By projecting the elements of C/prime\na,bonto the ï¬rst\ntwo components, (i,j,I )/mapstoâ†’(i,j), we end up with the set\nCÃ—\na,bâ‰¡CÃ—\na,b(Ï€a,Ï€b). To deal with uninitialized systems and\nfull scores, we add the pairs (0,0)and(|Sa|+1,|Sb|+1) to\nCÃ—\na,b. After sorting CÃ—\na,blexicographically, we perform the\nfollowing update process â†‘(Ï€b|Ï€a)forÏ€bgivenÏ€a:\n1. For the smallest element (i,j)âˆˆCÃ—\na,bsearch the minimal\ntâ‰¥1such that (i+t,j+t)âˆˆCÃ—\na,b.\n2. If no such texists, goto 5.\n3. Compute Pijconsisting of all (i+s,j+s)âˆˆSaÃ—Sb\\CÃ—\na,b\nsuch thatsâˆˆ[1:tâˆ’1]and staffi+sand staffj+sshare\nthe same clef label.\n4. For all (/lscript,I)âˆˆSbÃ—IupdateÏ€bas follows:\nÏ€b(/lscript,I) = max ({Ï€b(/lscript,I)}âˆª{Ï€a(k,I)|(k,/lscript)âˆˆPij}).\n5. UpdateCÃ—\na,bby removing (i,j).\n6. If|CÃ—\na,b|>1, goto 1.\nUsing this local update instruction, we deï¬ne POP (Ï€k)\nin two steps. First we calculate /tildewideÏ€k\nb:=â†‘(Ï€k\nb|Ï€k\n0)for all\nbâˆˆ[1:m]and thenPOP (Ï€k\nb) :=â†‘(/tildewideÏ€k\nb|POP (Ï€k\nbâˆ’1))is\nrecursively computed. We redeï¬ne Ï€k:=POP (Ï€k).\n2.2.2 Applying instrument properties (IP)\nIn this step, we extract knowledge from the plausibility ma-\ntrix to reconstruct missing instrument labels and to fortify\nalready existing plausibility entries. We deï¬ne some staff-\nrelated properties E1,...,E pas subsets of SwhereiâˆˆEj\n413Poster Session 3\nmeans that staff ihas property Ej(e.g., staffihas treble clef\nor staffiis the ï¬rst/last staff in the system). Similarly, we\ndeï¬ne properties F1,...,F qâŠ‚âˆªm\na=0SaÃ—Sabetween two\nstaves of the same system (e.g., staff iis in the same brace\nas staffj). We now use these staff related properties and Ï€\nto deduce instrument related properties.\nFor each instrument Iwe calculate the probability distri-\nbutionPIonE:={E1,...,E p}givenÏ€:\nPI(E|Ï€) =/summationtext\niâˆˆEwiÂ·Ï€(i,I)/summationtext\nE/primeâˆˆE/summationtext\niâˆˆE/primewiÂ·Ï€(i,I),\nwherewi=3\n4for stavesiinS0andwi=1\n4otherwise. For\n(I,F)âˆˆIÃ— FwithF:={F1,...,F q}we compute the\nprobability distribution PI,FonIgivenÏ€:9\nPI,F(J|Ï€) :=/summationtext\n(i,j)âˆˆFwi/radicalbig\nÏ€(i,I)Â·Ï€(j,J)\n/summationtext\nJ/primeâˆˆI/summationtext\n(i,j)âˆˆFwi/radicalbig\nÏ€(i,I)Â·Ï€(j,J/prime).\nUsing these global instrument properties, we now deï¬ne\nthe plausibility increase\nÏ€âˆ†(I,i) :=/summationdisplay\nEâˆˆE:iâˆˆEwEPI(E|Ï€) +\n/summationdisplay\njâˆˆS,JâˆˆI/summationdisplay\nFâˆˆF:(i,j)âˆˆFwF/radicalBig\nÏ€(j,J)PI,F(J|Ï€),\nwherewE,wFare suitable property weights. Using Ï€âˆ†, we\ndeï¬neIP(Ï€k) := N(Ï€k+Ï€k\nâˆ†), where for a non-zero matrix\nX,N(X) :=X/maxij|xij|. We redeï¬ne Ï€k:=IP(Ï€k).\n2.2.3 Exploiting the instrument order constraint (IOC)\nA common convention for score notation is that the instru-\nment order established in the ï¬rst system is not altered in\nsubsequent systems. Therefore, we use the instrument la-\nbels ofS0to penalize systems where the instrument order\nestablished by S0is violated.\nGivenM0and a system Ma,a > 0, we extract the se-\nquencesI0= (I1,...,I|S0|)andIa= (J1,...,J|Sa|)of\nmost plausible instrument labels. Afterwards we calculate\nthe setL0aof all pairs (i,j)âˆˆS0Ã—SawithIi=Jj\nfor which a pair (k,/lscript)âˆˆS0Ã—Saexists withIk=J/lscript\nsuch that (i,j,I i)and(k,/lscript,I k)constitute a crossing (Sub-\nsection 2.2.1). The plausibility decrease Ï€âˆ‡,a(j,Jj) :=\nÎ»/summationtext\ni:(i,j)âˆˆL0aÏ€a(i,Ii)with suitable parameter Î» > 0is\ncalculated for all aâˆˆ[1:m]. Finally, the plausibility\nupdate using the instrument order constraint is given by\nIOC (Ï€k) := N(Ï€kâˆ’Ï€k\nâˆ‡), whereÏ€k\nâˆ‡=/parenleftbig\nÏ€k\nâˆ‡,0,...,Ï€k\nâˆ‡,m/parenrightbig\n.\n2.3 Transposition propagation\nDuring the OCR-based reconstruction of the instrument la-\nbels, the available transposition information is also trans-\nformed into transposition labels and subsequently mapped\n9We chose two different probability distributions to account for the dif-\nferences between the two sets of properties EandF.to the according staves. After the reconstruction process de-\nscribed in the previous subsection has terminated, the trans-\nposition labels from the ï¬rst system are propagated through\nthe whole score. For each staff in S0holding a transposition\nlabel, the occurrences of its instrument label in the rest of\nthe score are determined. The concerned staves will then be\nassigned with the transposition label from S0.\nIn the context of our evaluation in Section 3 we used this\nmethod to propagate manually corrected transposition labels\nin the ï¬rst system to the whole score.\nWe are aware of the fact that some orchestral scores con-\ntain transposition information next to arbitrary staves. How-\never, extracting those short text labels (e.g., â€œin Aâ€) is a new\nchallenge and is left to be analyzed.\n3. EVALUATION\nAs the need for an algorithm that reconstructs the transposi-\ntion information contained in musical notations arose from\nour application scenario, we will evaluate the impact of our\nmethod with respect to the task of score-audio synchroniza-\ntion. In Subsection 3.1 we provide a short overview of the\ntechnique of score-audio synchronization. Afterwards, we\ngive a detailed account on the performed evaluations (Sub-\nsection 3.2).\n3.1 Score-audio synchronization\nThe goal of music synchronization in general is the calcu-\nlation of a mapping between each position in one represen-\ntation of a piece of music to the musically matching posi-\ntion in another representation of the same piece of music.\nFor score-audio synchronization tasks the given input docu-\nments are score scans and audio tracks.\nIn the ï¬rst step of the synchronization both music docu-\nments are transformed into a common representation which\nthen allows for a direct comparison. We chose to use the\nwell-established chroma-based features. For details on the\ncalculation of chroma features from audio recordings we re-\nfer to [2,9]. To extract chroma features from score scans the\ngiven sheets are ï¬rst analyzed with an OMR system to re-\nconstruct the musical information. After storing the recogni-\ntion results in a MIDI ï¬le, the chroma features are calculated\nsimilarly as for the audio recordings.\nIn the next step a similarity matrix is calculated from the\ntwo feature sequences. Finally, by applying multiscale dy-\nnamic time warping [10, 13] a minimal path through this\nmatrix is calculated. The synchronization between the mu-\nsic documents is then encoded by this path.\n3.2 Experiments\nFor our evaluation, we employ the beat annotations from the\nRWC Music Library [6] as ground truth. We extracted the\nmeasure starting points from these ï¬les to generate a ref-\nerence synchronization on the measure level. As test data\n41412th International Society for Music Information Retrieval Conference (ISMIR 2011)\nwe selected the 11 orchestral pieces which contain at least\none transposing instrument (see Table 1). In addition, the re-\nspective orchestral scores were collected and processed with\nSharpEye (data sources: IMSLP10and Bavarian State Li-\nbrary11). For four of the pieces we found scores that use a\ncompressed notation. Obviously, the labeling task is harder\nfor those scores than for scores using a full notation. To\nperform the synchronization experiments, we took audio\nexcerpts of roughly two minutes length and the according\nscore clippings.\nLabel Work Publisher\nC1 Haydn: Symphony no. 94 in G major, 1st mvmt. Kalmus\nC2 Tchaikovsky: Symphony no. 6 in B major, 4th mvmt. Dover Publications\nC3 Mozart: Le Nozze di Figaro : Overture BÂ¨arenreiter\nC4 Wagner: Tristan und Isolde : Prelude Dover Publications\nF1 Beethoven: Symphony no. 5 in C minor, 1st mvmt. Breitkopf & H Â¨artel\nF2 Brahms: Horn Trio in Eb major, 2nd mvmt. Peters\nF3 Brahms: Clarinet Quintet in B minor, 3rd mvmt. Breitkopf & H Â¨artel\nF4 Mozart: Symphony no. 40 in G minor, 1st mvmt. BÂ¨arenreiter\nF5 Mozart: Clarinet Quintet in A major, 1st mvmt. Breitkopf & H Â¨artel\nF6 Mozart: Violin Concerto no. 5 in A major, 1st mvmt. BÂ¨arenreiter\nF7 Strauss: â€œAn der sch Â¨onen Blauen Donauâ€ Dover Publications\nTable 1 .Overview of the test data. The scores of C1â€“C4 use\ncompressed and the scores of F1â€“F7 use full notation.\nBefore presenting the synchronization results, we want to\nbrieï¬‚y comment on the accuracy of the instrument labeling\nresults of the proposed method. For our test data there were\na total of 464instrument text labels given in the score. In\naddition, 87transposition text labels were found. Our eval-\nuation method could correctly reconstruct 88% of the instru-\nment and 77% of the transposition labels (see Table 2). The\nerror sources are diverse (e.g., OCR misrecognitions, un-\nconsidered instrument abbreviations) and some will be dis-\ncussed after the presentation of the synchronization results.\nInstrument labels %Transposition labels %\ntotal errors total errors\nCompressed 401 53 8775 17 77\nFull 63 1 9812 3 75\nTotal 464 54 88 87 20 77\nTable 2 .Percentage of wrongly reconstructed text labels.\nFor each piece of music we calculated four synchroniza-\ntions. In the ï¬rst case, we used the MIDI created from the\nSharpEye recognition data (OMR) to create the score-audio\nsynchronization. In the other cases we manipulated the\nOMR recognition before performing the synchronization. In\nthe second case, we manually annotated the missing trans-\nposition labels in the scores (OMRâˆ—). In the third case, we\napplied the label reconstruction method described in Sec-\ntion 2 (OMR+LR).12In the last case, we manually cor-\nrected the transposition labels in the ï¬rst system before the\ntransposition propagation is performed (OMR+LRâˆ—). Ta-\nble 3 shows the evaluation results for all of the mentioned\n10http://imslp.org/wiki/Main_Page\n11http://www.bsb-muenchen.de\n12We performed 18iterations of the process described in Section 2.2 and\nchose suitable experimentally determined parameter settings.settings. The numbers state the mean and standard devia-\ntions from the ground truth. Comparing the results of OMR\nLabel OMR OMRâˆ—OMR+LR OMR+LRâˆ—\nmean std mean std mean std mean std\nC1 456 1016 283 441 456 1016 283 441\nC2 434 502 385 378 424 505 425 503\nC3 247 349 128 178 134 183 181 247\nC4 1005 980 889 884 889 884 889 884\nAv 536 712 421 470 476 647 445 519\nF1 462 700 265 391 284 493 265 391\nF2 390 672 110 125 110 125 110 125\nF3 266 803 124 84 124 84 124 84\nF4 93 88 93 86 93 88 93 86\nF5 243 383 65 53 65 53 65 53\nF6 79 81 69 66 69 66 69 66\nF7 451 658 310 492 310 492 310 492\nAv 243 405 148 185 151 200 148 185\nTable 3 .Overview of the deviation of the different synchroniza-\ntion results from the ground truth (in ms).\nand OMRâˆ—, it becomes evident that knowing all transposi-\ntion labels results in a signiï¬cant improvement of the syn-\nchronization results.\nFor six piecesâ€”one of which has a compressed scoreâ€”\nour method could correctly reconstruct all transposition la-\nbels (C4, F2, F3 and F5â€“F7, see column OMR+LR). For\nthe remaining pieces, other than C1 and F4, the method im-\nproved the synchronization results compared to not apply-\ning any post-processing. By annotating the transposition la-\nbels in the ï¬rst system manually before propagating them\nthrough the score (OMR+LRâˆ—) the results became equal to\nOMRâˆ—for all full scores and the compressed score C1. Al-\nthough, manual interaction was still required, only annotat-\ning the ï¬rst system constitutes a signiï¬cant improvement\ncompared to annotating all systems of an orchestral piece\nmanually. For C2 and C3 a correct reconstruction of the\ntransposition labels was not possible. In addition, using the\npropagation of the transposition labels from the ï¬rst system\nresults in a degradation of the synchronization compared to\nOMR+LR (due to instrument labeling errors).\nWe will now discuss the labeling results for some scores\nin more detail. For two pieces the transposition text labels\ngiven in the score were not recognized. In C1 the score no-\ntation uses an unusual setting of the transposition text labels\n(see Figure 2). The text labeling in C1 results in the recog-\nnition of three separate text labels (â€œinâ€, â€œGâ€and â€œSolâ€) in-\nstead of one text label (e.g., â€œin Gâ€). Therefore, our method\ncould not reconstruct the transposition labeling. In F4 the\nalignment of the transposition text labels would allow for\na successful recognition but the OCR engine produced re-\nsults such as â€œi n Solâ€ or â€œinSiwâ€. In both of these examples\nthe keyword â€œinâ€ with a subsequent space was not available.\nAlthough for all other pieces the transposition labels in the\nï¬rst system were correct, some instrument labeling errors\noccurred which sometimes inï¬‚uenced the transposition la-\nbeling of subsequent systems in a negative manner. Some\nof these errors result from incorrect OCR recognitions (e.g.,\nrecognition of â€œFI.â€ instead of â€œFl.â€ (ï¬‚ute) results in a map-\n415Poster Session 3\nping to â€œFg.â€ (Fagott, German for bassoon)). Furthermore,\nsome text labels are wrongly interpreted as instrument text\nlabels and thereby produce wrong instrument labels. An in-\nteresting mix-up occurred for C3. Here, Italian text labels\nare used and both the clarinet and the trumpet are part of the\ninstrumentation. However, in Italian the trumpet is called\nâ€œclarinoâ€ which is abbreviated by â€œCl.â€. But, in English this\nabbreviation is used for the clarinet.\nFigure 2 .Examples of missed transposition text labels.\nWe also performed an evaluation of the impact of other\nOMR errors (clefs, accidentals, pitches, durations) on the\nprospective synchronization results (see Table 4). In accor-\ndance with the results in [14], correcting the OMR data al-\nmost consistently resulted in an improvement. However, the\naccuracy increase is less pronounced than for transpositions.\nLabel OMR OMRâˆ—OMR+LR OMR+LRâˆ—\nmean std mean std mean std mean std\nC4 1018 967 936 856 936 856 936 856\nAv 486 517 426 405 436 449 445 457\nF1 342 528 151 169 172 219 151 169\nAv 269 471 131 144 134 151 131 144\nTable 4 .Synchronization results for corrected OMR data. The\naverages are calculated for C1â€“C4 and F1â€“F7, respectively.\n4. CONCLUSIONS AND FUTURE WORK\nWe presented a method for the reconstruction of instrument\nand transposition labels from orchestral scores. Our method\nreconstructs instrument labels based on an OCR recognition\nand propagates those labels to staves where no instrument\ntext labels existed in the score. We tested our method in\nthe context of score-audio synchronization. The evaluation\nshowed both the need for the reconstruction of transposition\nlabels to improve the synchronization results and the ability\nof our method to achieve this.\nAt the moment our method is being integrated into the\npreprocessing workï¬‚ow of the PROBADO application (see\n[4]). We hope to thereby reduce the manual annotation effort\nrequired to administer large music databases.\nTo make the reconstruction more robustâ€”especially for\ncompressed scores and with respect to the imposed assump-\ntionsâ€”we suggest several ideas. We found that although\nABBYY FineReader produces a very high recognition rate\nfor words (>97%), the recognition of instrument abbre-\nviations was often inferior to other OCR engines. There-\nfore, a promising idea is the combination of several OCR\nengines to make the initial OCR-based instrument labelingmore reliable. Our method takes advantage of some con-\nventions for music notation while currently ignoring several\nothers. We assume that, e.g., key signatures, braces, and\ninstrument groups form powerful tools w.r.t. the task of in-\nstrument labeling. However, SharpEye does not recognize\nthose features reliably and prevents their reasonable usage.\nWe therefore suggest to reconstruct them by, e.g., combining\nseveral OMR engines as proposed in [7] and to subsequently\nintegrate them into the proposed method.\n5. REFERENCES\n[1] D. Bainbridge and T. Bell. The Challenge of Optical Music\nRecognition. Computers and the Humanities , 35(2):95â€“121,\n2001.\n[2] M.A. Bartsch and G.H. Wakeï¬eld. Audio Thumbnailing of\nPopular Music Using Chroma-Based Representations. IEEE\nTransactions on Multimedia , 7(1):96 â€“ 104, 2005.\n[3] J.L. Bentley and T.A. Ottmann. Algorithms for Reporting\nand Counting Geometric Intersections. IEEE Transactions on\nComputers , 100(9):643â€“647, 1979.\n[4] D. Damm, C. Fremerey, V . Thomas, M. Clausen, F. Kurth,\nand M. M Â¨uller. A Digital Library Framework for Hetero-\ngeneous Music Collectionsâ€”from Document Acquisition to\nCross-Modal Interaction. International Journal on Digital Li-\nbraries: Special Issue on Music Digital Libraries (to appear) ,\n2011.\n[5] M. Droettboom and I. Fujinaga. Interpreting the semantics of\nmusic notation using an extensible and object-oriented system.\nInProc. Python Conference , 2001.\n[6] M. Goto. AIST Annotation for the RWC Music Database. In\nProc. ISMIR , 2006.\n[7] I. Knopke and D. Byrd. Towards MusicDiff: A foundation for\nimproved optical music recognition using multiple recogniz-\ners. In Proc. ISMIR , 2007.\n[8] V . I. Levenshtein. Binary Codes Capable of Correcting Dele-\ntions, Insertions, and Reversals. Soviet Physics Doklady ,\n10(8):707â€“710, 1966.\n[9] M. M Â¨uller. Information Retrieval for Music and Motion .\nSpringer, Berlin, 2007.\n[10] M. M Â¨uller, H. Mattes, and F. Kurth. An Efï¬cient Multiscale\nApproach to Audio Synchronization. In Proc. ISMIR , 2006.\n[11] A. Rosenfeld and J. L. Pfaltz. Sequential Operations in Digital\nPicture Processing. Journal of the ACM , 13:471â€“494, 1966.\n[12] S. Sadie, editor. The New Grove Dictionary of Music and Mu-\nsicians (second edition) . Macmillan, London, 2001.\n[13] S. Salvadore and P. Chan. FastDTW: Toward Accurate Dy-\nnamic Time Warping in Linear Time and Space. In 3rd Work-\nshop on Mining Temporal and Sequential Data , 2004.\n[14] V . Thomas, C. Fremerey, S. Ewert, and M. Clausen.\nNotenschrift-Audio Synchronisation komplexer Orchester-\nwerke mittels Klavierauszug. In Proc. DAGA , 2010.\n[15] C. Wagner. OCR based postprocessing of OMR results in com-\nplex orchestral scores â€“ Which (transposing) instrument corre-\nsponds to which staff ? Diploma thesis, University of Bonn,\n2011.\n416"
    },
    {
        "title": "A Postprocessing Technique for Improved Harmonic/Percussion Separation for Polyphonic Music.",
        "author": [
            "Balaji Thoshkahna",
            "Ramakrishnan R. Kalpathi"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1417056",
        "url": "https://doi.org/10.5281/zenodo.1417056",
        "ee": "https://zenodo.org/records/1417056/files/ThoshkahnaK11.pdf",
        "abstract": "In this paper we propose a postprocessing technique for a spectrogram diffusion based harmonic/percussion decomposition algorithm. The proposed technique removes harmonic instrument leakages in the percussion enhanced outputs of the baseline algorithm. The technique uses median filtering and an adaptive detection of percussive segments in subbands followed by piecewise signal reconstruction using envelope properties to ensure that percussion is enhanced while harmonic leakages are suppressed. A new binary mask is created for the percussion signal which upon applying on the original signal improves harmonic versus percussion separation. We compare our algorithm with two recent techniques and show that on a database of polyphonic Indian music, the postprocessing algorithm improves the harmonic versus percussion decomposition significantly.",
        "zenodo_id": 1417056,
        "dblp_key": "conf/ismir/ThoshkahnaK11",
        "keywords": [
            "postprocessing",
            "spectrogram diffusion",
            "harmonic/percussion decomposition",
            "harmonic instrument leakages",
            "percussive segments",
            "piecewise signal reconstruction",
            "envelope properties",
            "binary mask",
            "harmonic versus percussion separation",
            "polyphonic Indian music"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nA POSTPROCESSING TECHNIQUE FORIMPROVEDHARMONIC /\nPERCUSSION SEPARATIONFORPOLYPHONIC MUSIC\nBalajiThoshkahna\nDept. ofElectricalEngineering,\nIndianInstituteofScience,\nBangalore,India\nbalajitn@ee.iisc.ernet.inK.R.Ramakrishnan\nDept. ofElectrical Engineering,\nIndianInstituteofScience,\nBangalore,India\nkrr@ee.iisc.ernet.in\nABSTRACT\nIn this paper we propose a postprocessing technique for a\nspectrogram diffusion based harmonic/percussion decom-\nposition algorithm. The proposed technique removes har-\nmonic instrument leakages in the percussion enhanced out-\nputs of the baseline algorithm. The technique uses median\nï¬lteringandanadaptivedetectionofpercussivesegmentsi n\nsubbandsfollowedbypiecewisesignalreconstructionusin g\nenvelope properties to ensure that percussion is enhanced\nwhileharmonicleakagesaresuppressed. Anewbinarymask\nis created for the percussion signal which upon applying\non the original signal improvesharmonicversuspercussion\nseparation. Wecompareouralgorithmwithtworecenttech-\nniques and show that on a database of polyphonic Indian\nmusic,thepostprocessingalgorithmimprovestheharmonic\nversuspercussiondecompositionsigniï¬cantly.\n1. INTRODUCTION\nMusic source separation has been a very important topic of\nresearch with applications in transcription [1], audio cod -\ning [2], enhancement [3] and personalization [4]. Source\nseparation involvesseparatinga polyphonicmono or stereo\nmusic into its component instrument streams. As a prelim-\ninary step towards source separation, decomposition of the\nmusic signal into separate harmonic and percussive instru-\nment streams has been a popular approach in recent years.\nThe percussive instrument stream can be used for drums\ntranscription [5], rhythm analysis [6], audio remixing [3]\namong the many applications. It has been shown that the\npercussion stream results in better drum transcription [5, 7]\nthan the original music itself. Likewise, the harmonic in-\nstruments stream can be used for multipitch estimation [1],\nPermission to make digital or hard copies of all or part of thi s work for\npersonal or classroom use is granted without fee provided th at copies are\nnot made or distributed for proï¬t or commercial advantage an d that copies\nbear this notice and the full citation on theï¬rstpage.\nc/circlecopyrt2011 International Society for MusicInformation Retrieva l.pitchmodiï¬cation[4],notetranscriptionandleadvocalse x-\ntraction[8]with greaterease.\nMcAulay et al. [9] ï¬rst used sinusoidal modeling to de-\ncomposeasignalintoharmonicandnoisecomponentspop-\nularly known as the â€œsine+noiseâ€œ model. Verma et al. [10]\nintroduced the idea of modeling transients in a signal lead-\ning to the development of â€sine+transients+noiseâ€ model.\nVarious improvementsto these models have been proposed\nin [11,12]. Gillet et al. [7] used noise subspace projection s\nto split polyphonic music into harmonic and noise compo-\nnents with the noise components predominantlyhaving the\npercussive instruments. he noise signal was used for drum\ntranscription and was found to be more effective than the\noriginal for the same task. Yoshii et al. [5] used a template\nbased approachfor harmonicinstrument suppression to ex-\ntractdrumssoundsfrompolyphonicmusicfortranscription .\nRecently Ono et al. [3,13] presented an iterative algorithm\nusing spectrogram diffusion to split music signals into the\ncomponent harmonic and percussion streams. The percus-\nsion streams were used for remixing and equalisation pur-\nposes. Fitzgerald [14] proposed a much simpler alternative\nto Onoâ€™salgorithmusingmedianï¬ltering.\nBut most ofthe abovediscussed algorithmsare aimedat\nWestern music andspeciï¬callypopmusic whichhasstrong\npercussion accompaniments. These algorithms do not per-\nformwellforIndianmusicwhichhassomewhatmutedper-\ncussion (often used just to give a basic beat to the lead in-\nstrument/vocalist) and an increased amount of vibratos in\nthe instrumental sections. This leads to a lot of leakages of\npercussionintotheharmonicstreamandviceversa.\nIn thispaperwe developa postprocessingtechniquethat\ncan applied to the output of Onoâ€™s algorithm (called the\nbaseline from here onwards) [3,13] mentioned above. In\nSection 2 we brieï¬‚y describe the baseline algorithm to es-\ntablishtheframeworkforouralgorithm. Section3describe s\nourpost processingtechnique. Thenecessaryframeworkto\ntest the algorithm, the experimentsand comparative result s\naredescribedinSection4. WeconcludethepaperinSection\n5.\n251Poster Session 2\nFigure 1 . Spectrogram of song No.151 from LSML\ndatabase. Thestrongverticalstripesarelocationsofperc us-\nsion and the horizontal stripes are the harmonicsof pitched\ninstruments. The wavy horizontal lines between 8 secs and\n10secsarethevibratosintheleadmalesinging.\n2. ONO'SALGORITHMAND SHORTCOMINGS\nThespectrogramdiffusionbasedharmonic/percussionsepa -\nrationalgorithmproposedbyOnoetal.[3,13]assumesthat\nsteady harmonic instruments show up as horizontal lines\nwhilepercussiveinstrumentsshowupasverticallinesinth e\nsignal spectrogram. This is because of the steady nature\nof harmonic instruments that play enduring discrete notes\nwhile percussive instruments have a short time burst of en-\nergy leading to a wideband spectral structure as shown in\nFigure 1. The diffusion algorithm uses a minimization of\nthe spectrogramâ€™s vertical and horizontal derivatives usi ng\nanauxiliaryfunctionapproach.\nLetx[n]be a monaural polyphonic music signal sam-\npled at 16kHz. Let X(i,j)denote its STFT (Short Time\nFourier Transform) at the ithfrequency bin and jthframe.\nLetW(i,j)be the range compressed version of the power\nspectrogramgivenby,\nW(i,j) =|X(i,j)|2Î³, (1)\nwhereÎ³= 0.3.\nSimilarlylet H(i,j)andP(i,j)representthepowerspec-\ntrograms of the component harmonic and percussion sig-\nnals.\nAcostfunction J(H,P)deï¬nedasbelowisusedtomin-\nimizethe gradientsofthespectrograms.\nJ(H,P) =1\nÏƒ2\nH/summationdisplay\ni,j(H(i,j)âˆ’H(i,jâˆ’1))2\n+1\nÏƒ2\nP/summationdisplay\ni,j(P(i,j)âˆ’P(iâˆ’1,j))2.(2)\nThen, we wish to ï¬nd HandPthat minimize the equa-\ntion(2)undertheconstraint,W=P+H. (3)\nAn iterative update method using auxiliary function ap-\nproach is used for the minimization of equation (2). This\nleads to the decomposition of the signal x[n]into its com-\nponentpercussionandharmonicspectrograms PandHre-\nspectively for various values of the diffusion coefï¬cient Î±\n(0< Î± <1).PandHare â€œbinarizedâ€œto PbinandHbinas\ninequations(4,5)toattentuatetheinterferenceofharmon ic\ninstrumentsin thepercussivestreamandviceversa.\nPbin(i,j) =/braceleftbiggX(i,j)ifP(i,j)> H(i,j),\n0 ifP(i,j)â‰¤H(i,j).(4)\nHbin(i,j) =X(i,j)âˆ’Pbin(i,j). (5)\nDependingonthevalueof Î±,eitherthepercussivestream\nwill be emphasized or the harmonic stream will be empha-\nsized. The percussive and harmonic streams p[n]andh[n]\nare reconstructedby invertingthe STFTs PbinandHbinre-\nspectivelyusingthephaseoftheoriginalsignal x[n](ateach\nframeduringinversion).\nOne of the shortcomings of this algorithm has been the\nleakage of harmonic instrument components into the Pbin\ncomponent and the leakage of low strength percussion into\ntheHbinportion. Asnotedearlierthereisahighpresenceof\nvibratos and muted percussion (tabla, mridangam1) in In-\ndianmusic. Thisleadstoa verybaddecompositionscheme\nusing baseline algorithm. A much faster algorithm using\nmedianï¬lteringhasbeenproposedin[14],buteventhat al-\ngorithmsuffersfromthesameshortcomings.\n3. THEPROPOSEDALGORITHM\nWe use only the percussion stream p[n]from the baseline\nalgorithm and the original signal x[n]for the postprocess-\ning technique we propose. Since percussion appears as a\nwidebandsignal in the spectrum and differentharmonicin-\nstrumentshavedifferentfrequencycharacteristics,nota llre-\ngions of the spectrum are equally affected by the harmonic\nleakage. Thereforewe intend to remove the leakages using\nsubband processing. The signal p[n]is passed through an\nevenstackedcosine modulatedperfectreconstructionï¬lte r-\nbank of 16 ï¬lters. The ï¬lterbank was designed using the\nLT-TOOLBOX2set of Matlab routines. The following op-\nerationsareperformedoneachsubbandsignal. Let pi[n]be\ntheoutputofthe ithsubband. Thesignalissplitintoframes\nof40ms(Framelength Nl= 640samples) with an overlap\nof20ms(Frame shift No= 320samples). Each frame is\nmultiplied with a triangular window of length Nlsamples\n1Asouth Indian classical instrument\n2http://www.students.tut.ï¬/ jalhava/lt/intro.html\n25212th International Society for Music Information Retrieval Conference (ISMIR 2011)\nFigure2. A plot of HÂµ(bluedash-dot)and M(reddotted)\nforsubband2 forNo.151fromLSMLdatabase.\nto facilitate the overlap and add at the reconstructionstag e.\nThejthframeisrepresentedas Pi(j,:).\nAsnotedbyScheirer[15],theamplitudeenvelopeismore\nimportant than the frequency content for the perception of\npercussion. Thereforeweintendtomanipulatetheenvelope\nof the subband signals. The Hilbert envelope of a signal\nhas been exploitedfor detection of transients in polyphoni c\nmusic with great success [16]. We intend to use the same\nframeworkwithaviewofincludingtemporalnoiseshaping\n(TNS)[2]foreachframein ourfuturework.\nLet the Hilbert transform for the jthframe be Ë†Pi(j,:).\nWe ï¬ndtheHilbertenvelopeofthesignal[16]as:\nHi(j,:) =/radicalBig\nPi(j,:)2+Ë†Pi(j,:)2. (6)\nWe now use the sample mean of Hi(j,:)as a represen-\ntative for the jthframe (We also tried with the energy of\neachframeasa representativeandthemethodworksjustas\nï¬ne, but since we intendto use TNS in our futurework,we\nchoosetoretaintheHilbertenvelopewithineachframe).\nHÂµ(i,j) =1\nNN/summationdisplay\nk=1Hi(j,k). (7)\nHÂµis used to detect the frames having percussion and\nharmonic instruments. In order to do this, HÂµis median\nï¬lteredwith a lpointmedianï¬lter.\nM(i,j) =median{HÂµ(i,jâˆ’k:j+k),k=lâˆ’1/2},(8)\nwhere we used l= 7. We used a value of l= 7since\na median ï¬lter whose length is greater than the duration of\nthe transient noise can suppress it [17] and most percussive\ntransientsarearound60-100mslong(3to5frameshiftsand\nhenceweusedthenextoddnumberedwindowlength).\nAsshownin Figure2, in HiandMthepresenceofhar-\nmonic instruments creates a change of shape in the usualgammafunctionenvelopesofpercussionsignals[18]. There -\nfore,thenoveltyfunction Ï,deï¬nedastheratiobetween HÂµ\nandM,\nÏ(i,j) =HÂµ(i,j)/M(i,j), (9)\nis low at places of leakage while it retains a high value if\npercussionispresent[17].\nWenowusetwopossiblemethodsofï¬ndingagoodthresh-\noldfordetectingpercussionin Ï. Intheï¬rstmethod,weï¬nd\nthe mean ( Âµ) and variance ( Ïƒ) ofÏfor each subband. The\nthresholdforthe ithsubband, T(i)is computedas,\nT(i) =min(1.75,Âµ(i)+0.5âˆ—Ïƒ(i)).(10)\nThis threshold was decided empirically after testing on a\nsmall dataset of audio clips and is similar to the one used\nin [19].\nIn the second method, we assume that we have poly-\nphonic audio with utmost 10%of the values of Ïare due\nto percussion. This is akin to the assumption that we have\n2percussionhitsof 50msdurationpersecondofthesignal.\nWe ï¬nd a threshold from the histogram of Ïsuch that 10%\nof the valuesof Ïlie to the right and the remaining 90%lie\nto theleftofthethresholdinthe histogram.\nWeusethethresholdobtainedfromtheï¬rstmethodsince\noptimization process for the second approach is still under\ndevelopment at the time of writing this paper. We now use\nthe threshold to determine the set of local maxima within\neachsubbandthatbelongtothepercussionas:\nF(i,j) =/braceleftbigg1ifHÂµ(i,j)> T(i).M(i,j),\n0otherwise .(11)\nWe locate local maxima in HÂµfor each subbandand re-\ntain only frames correspondingto them in Fwhile the rest\nof the frames are made 0. Since a percussive signal has a\ngamma function envelope, it has a minima to both sides of\nthe local envelope maxima on the time axis. Upon ï¬nding\nthe local maxima in the signal, we need to ï¬nd the local\nminima on both its sides on the time axis in order to fully\nreconstructthepercussivesignalasshownin Figure3.\nWe rebuild the exact percussion signal by using the ï¬rst\nlocal minimato the temporalleft and right of each detected\nmaxima as shown in Figure 4. This ensures that the entire\npercussion signal is preserved in the envelope. The set of\nnon-zeroframesin each subbandare consideredas the per-\ncussiveframes.\nThepercussiveframesfromeachsubbandareï¬nallyadded\nusing the overlap and add method to generate the subband\nsignal thatis percussionenhanced. Thesubbandsignalsare\nthen passed through the synthesis ï¬lterbank to generate the\nnewpercussionsignal penh[n].\n253Poster Session 2\nFigure3.Top:A percussionenvelope(solid line)andits lo-\ncal maximumand the minima (star). Bottom:Reconstructed\npercussion envelope using the piecewise reconstruction\nmethoddescribedinthispaper.\nWeusethenewlygeneratedpercussionsignaltoenhance\ntheHbinsignalgivenbythebaselinealgorithm. ASTFTof\nthesignal penh[n]iscomputedas Penh(i,j). NowtheSTFT\nisaveragedalongthefrequencyaxisasfollows:\nPavg(i,j) =1\nmi+(mâˆ’1)/2/summationdisplay\nk=iâˆ’(mâˆ’1)/2Penh(k,j),(12)\nwherem= 2.Pavgchanges from Penhby a small\namount if the frame is a percussive frame (since a percus-\nsion frame will have a wideband spectrum) while its value\nchanges signiï¬cantly if the frame has predominantly har-\nmonic components. Pavgis comparedwith the spectrumof\ntheoriginalsignal. Ifanycomponentof Pavgisgreaterthan\na threshold Î½timesX, that component is assigned to per-\ncussion otherwise it is assigned to the harmonic stream of\nthesignal.\nPfin(i,j) =/braceleftbiggX(i,j)ifPavg(i,j)> Î½.X(i,j),\n0 otherwise .(13)\nHfin(i,j) =X(i,j)âˆ’Pfin(i,j). (14)\nWe used a value of Î½= 0.45in order to enhance even\nweakpercussivesegments.\nThePfinandHfinare inverted to obtain the improved\npercussion pfin[n]and harmonic hfin[n]stream of the sig-\nnalx[n]. As can be seen in Figure5, the postprocessingre-\nduces the harmonic leakages very well. In the next section\nwecompareouroutputwithboththebaselinealgorithmand\nFitzgeraldâ€™salgorithm.\nFigure 4 .Top:Ground truth locations of percussion in\nNo.151 from LSML database. Middle:Plot ofHÂµfor sub-\nband2.Bottom:Percussionlocatedbysignalrebuildingafter\nlocalmaximadetection.\nFigure 5.Top:Ground truth locations of percussion in the\nclipâ€œRememberTheName-FortMinorâ€œfromMTG-MASS\ndatabase. Middle:PercussionstreamfrombaselineOnoâ€™sal-\ngorithm.Bottom:Percussionstreamafterpostprocessing.\n25412th International Society for Music Information Retrieval Conference (ISMIR 2011)\n4. EXPERIMENTS AND RESULTS\nSince we didnot havetheindividualinstrumentstreamsfor\nIndianmusicaswiththecasein [13]fortestingtheefï¬cacy\nof harmonic/percussion separation, we developed our own\nprocedureaselaboratedbelow.\nTocomparetheworkingofourpostprocessingtechnique,\nwe prepareda databaseof26clipsfromvariousIndianï¬lm\nsongs and also Western music songs. All songs have been\nsampledat16kHzandareanaverage10secondslong. Each\nsong was manually annotated using the gating technique\n[20] for percussive transients by two people independently .\nWe annotated drums, mridangam, tabla, shakers and bass\nguitar slaps as percussive instruments. The percussive por -\ntions common to both the annotations were retained as the\ngroundtruth. We will callthistheLSML database.\nInordertocomparetheoutputofourpostprocessingtech-\nnique with the baseline Onoâ€™s algorithm, we derive the fol-\nlowingmeasure.\nLetp[n]andh[n]betheoutputsofthebaselinealgorithm\nandpfin[n]andhfin[n]betheoutputsofourpostprocessing\ntechnique on the baseline algorithm. We now split each of\nthesesignalsintoframesof 40mswithanoverlapof 20ms.\nTheenergyin eachframeof pandharecalculatedas:\nEp(l) =(lâˆ’1).No+Nl/summationdisplay\nk=(lâˆ’1).Nop2[k], (15)\nEh(l) =(lâˆ’1).No+Nl/summationdisplay\nk=(lâˆ’1).Noh2[k]. (16)\n(17)\nSimilarlytheenergyfor pfinandhfinarecomputedand\nstoredinEpfinandEhfinrespectively.\nWe now compare the energies between EpandEpfin.\nSince both pandpfinare percussive components, we use\nthegroundtruthtoï¬ndthetotalenergyinthenon-percussiv e\nframes of both these signals. Let FPrepresent the set of\nframes marked as percussive and FHrepresent the non-\npercussiveframes. Thenweï¬ndtheenergyinthepercussive\nandnon-percussiveframesof p[n]as:\nEP\np=/summationdisplay\nlâˆˆFPEp(l), (18)\nEH\np=/summationdisplay\nlâˆˆFHEp(l). (19)\nSimilarlywecomputethesameforthe pfinasEP\npfinand\nEH\npfin.\nWe now compare the energies EH\npandEH\npfinafter nor-\nmalizingtheenergies EP\npandEP\npfin. Wecompute Î²P,where,Î²P=EP\np\nEP\npfin. (20)\nNow,\nÎ“P=EH\np\nÎ²P.EH\npfin, (21)\ncomputestheratiobetweenenergiesinthenon-percussive\nframes of pandpfinwhen the energies in the percussive\nframes are equal. A value of Î“P>1indicates that the\nsignalpfinhas lesser energy than pin the non-percussive\nsegments.\nLikewise, we compute the ratio Î“Hby normalizing the\nenergies of handhfinin the non-percussive sections and\nï¬nding the ratio of the energies in the percussive sections\nas,\nÎ“H=EP\nh\nÎ²H.EP\nhfin, (22)\nwhereÎ²His,\nÎ²H=EH\nh\nEH\nhfin. (23)\nWe formÎ“Totas,\nÎ“Tot= Î“P+Î“H, (24)\nto give us an overall measure of how well pfinandhfin\ncomparewith pandhrespectively. Î“Totattainsavalueof 2\nwhenthebaselinealgorithmis comparedwithitself.\nWe show the performance of our postprocessing algo-\nrithm (PP1) and Fitzgeraldâ€™s method against the baseline\nOnoâ€™s techniquein Figure 6. Both the postprocessingtech-\nnique and Fitzgeraldâ€™s technique are compared against the\nbaseline algorithm. As can be seen, our method performs\nbetter than both Fitzgeraldâ€™s technique and the baseline al -\ngorithm for any value of diffusion coefï¬cient Î±. Also, the\npostprocessing technique performs better for a lower diffu -\nsion coefï¬cient Î±. With increasing Î±, energyin the percus-\nsion stream pbindecreases and hence the leakage too de-\ncreases. Therefore our postprocessing algorithm performs\nbetterforlower Î±.\n5. FUTUREWORKANDCONCLUSIONS\nInthispaperwehaveproposedasimplepostprocessingtech-\nnique for Onoâ€™s harmonic/percussion decomposition algo-\nrithm using no prior information about the sources except\ntheir productionmechanismandtheenvelopestructure. We\nalso are currentlyworkingon a technique that uses the har-\nmonicstreamalongwiththepercussivestreamforimproved\nseparation.\n255Poster Session 2\nFigure 6. Performance of the postprocessing technique (\nPP1 ) against the Fitzgeraldâ€™s method and Onoâ€™s baseline\nalgorithmsforvarying Î±.\n6. REFERENCES\n[1] A. Klapuri: â€Automatic Transcription of Musicâ€, MSc\nThesis, TampereUniversity ,1998.\n[2] J. Herre: â€Temporal Noise Shaping, Quantization and\nCoding methods in Perceptual Audio Coding: A Tu-\ntorial Introductionâ€, AES 17th Intl Conference:High-\nQualityAudioCoding ,1999.\n[3] N. Ono and K. Miyamoto and H. Kameoka and S.\nSagayama: â€A Real-Time Equalizer of Harmonic and\nPercussive Components in Music Signalsâ€, Intl Society\nofMusic InformationRetrievalConference) ,2008.\n[4] M. Shashanka,P.Smaragdis, B. Raj and R.Singh: â€Sep-\narating a ForegroundSinger from BackgroundMusic.â€,\nInternational Symposium on Frontiers of Research on\nSpeechandMusic(FRSM) ,2007.\n[5] K. Yoshii, M. Goto and G. Okuno: â€Drum Sound\nRecognition for Polyphonic Audio Signals by Adap-\ntation and Matching of Spectrogram Templates with\nHarmonic Structure Suppressionâ€, IEEE Transactions\non Audio, Speech and Language Processing ,Vol-\n15,No.1,2007.\n[6] J. Paulus and A. Klapuri: â€Measuring the Similarity of\nRhythm Patternsâ€, Intl Society for Music Information\nRetrievalConference ,2002.\n[7] O.GilletandG.Richard: â€TranscriptionandSeparation\nofDrumsSignalsfromPolyphonicMusicâ€, IEEETrans-\nactionsonAudio,SpeechandLanguageProcessing ,Vol-\n16,No.3,2008.\n[8] Y. Li and D. Wang : â€Separation of Singing Voice\nfromMusicAccompanimentforMonauralRecordingsâ€,IEEE Transactions on Audio, Speech and Language\nProcessing ,Vol-15,No.4,2007.\n[9] R. McAulay and T. F. Quatieri: â€Speech Analy-\nsis/Synthesis Based on a Sinusoidal Representationâ€,\nIEEETransactionsonAcoustics,SpeechandSignalPro-\ncessing,Vol34,pp-744-754,1990.\n[10] T.S.VermaandS.N.LevineandT.H.Y.Meng: â€Tran-\nsientModelingSynthesis:AFlexibleAnalysis/Synthesis\nTool for Transient Signalsâ€, Intl Computer Music Con-\nference(ICMC ) ,1997.\n[11] X. Serra and J. O. Smith: â€Spectral Modeling Synthe-\nsis:A SoundAnalysis/SynthesisSystem Based on a De-\nterministic plus Stochastic Decompositionâ€, Computer\nMusicJournal ,Vol14(4),pp-14-24,1990.\n[12] R.Badeau, R. Boyer and B. David: â€EDS Parametric\nModeling and Tracking of Audio Signalsâ€, Intl Confer-\nenceonDigitalAudioEffects(DAFx) ,2002.\n[13] N. Ono and K. Miyamoto and J. Le Roux and H.\nKameoka and S. Sagayama: â€Separation of a Monaural\nAudioSignalintoHarmonic/PercussiveComponentsby\nComplementary Diffusion on Spectrogramâ€, European\nsignalProcessingConference(EUSIPCO)) ,2008.\n[14] D. Fitzgerald: â€Harmonic/Percussive Separation usin g\nMedianFilteringâ€, IntlConferenceonDigitalAudioEf-\nfects(DAFx) ,2010.\n[15] E. Scheirer: â€Music Listening Systemsâ€, PhD Thesis,\nMassachusettsInstituteof Technology ,2000.\n[16] F. X. Nsabimana and U. Zolzer: â€Transient Encoding\nof Audio Signals using Dyadic Approximationsâ€, Intl\nConferenceonDigitalAudioEffects(DAFx) ,2007.\n[17] I. Kauppinen: â€Methods for Detecting Impulsive Noise\ninSpeechandAudioSignalsâ€, IntlConferenceonDigi-\ntalSignalProcessing ,2002.\n[18] M.G.ChristensenandS.vandePar: â€Efï¬cientParamet-\nricCodingofTransientsâ€, IEEETransactionsonAudio,\nSpeechandLanguageProcessing ,Vol-14,No.4,2006.\n[19] J. P. Bello, L. Daudet, S. Abdallah, C. Duxbury, M.\nDavies and M. B. Sandler : â€A Tutorial on Onset De-\ntectioninMusic Signalsâ€, IEEETransactionsonAudio,\nSpeechandLanguageProcessing ,Vol-13,No.5,2005.\n[20] D. J. Hermes: â€Vowel Onset Detectionâ€, Journal\nof Acoustical. Society. of America ,Vol-87(2), 866-\n873,1990.\n256"
    },
    {
        "title": "A Transient Detection Algorithm for Audio Using Iterative Analysis of STFT.",
        "author": [
            "Balaji Thoshkahna",
            "FranÃ§ois Xavier Nsabimana",
            "Ramakrishnan R. Kalpathi"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1415630",
        "url": "https://doi.org/10.5281/zenodo.1415630",
        "ee": "https://zenodo.org/records/1415630/files/ThoshkahnaNK11.pdf",
        "abstract": "We propose an iterative algorithm to detect transient segments in audio signals. Short time Fourier transform (STFT) is used to detect rapid local changes in the audio signal. The algorithm has two steps that iteratively (a) calculate a function of the STFT and (b) build a transient signal. A dynamic thresholding scheme is used to locate the potential positions of transients in the signal. The iterative procedure ensures that genuine transients are built up while the localised spectral noise are suppressed by using an energy criterion. The extracted transient signal is later compared to a ground truth dataset. The algorithm performed well on two databases. On the EBU-SQAM database of monophonic sounds, the algorithm achieved an F-measure of 90% while on our database of polyphonic audio an F-measure of 91% was achieved. This technique is being used as a preprocessing step for a tempo analysis algorithm and a TSR (Transients + Sines + Residue) decomposition scheme.",
        "zenodo_id": 1415630,
        "dblp_key": "conf/ismir/ThoshkahnaNK11",
        "keywords": [
            "iterative algorithm",
            "detect transient segments",
            "audio signals",
            "short time Fourier transform",
            "rapid local changes",
            "localised spectral noise",
            "ground truth dataset",
            "F-measure",
            "preprocessing step",
            "tempo analysis algorithm"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nA TRANSIENT DETECTIONALGORITHM FORAUDIO USING ITERATIVE\nANALYSIS OF STFT\nBalaji Thoshkahna\nDept. of ElectricalEngineering,\nIndian Instituteof Science,\nBangalore,India\nbalajitn@ee.iisc.ernet.inFrancois XavierNsabimana\nProjectGroup Hearing,Speech andAudio Technology,\nFraunhofer Instituteof DigitalMedia Technology,\nOldenberg,Germany\nnba@fraunhofer.idmt.deK.R.Ramakrishnan\nDept. of ElectricalEngineering,\nIndian Instituteof Science,\nBangalore,India\nkrr@ee.iisc.ernet.in\nABSTRACT\nWe propose an iterative algorithm to detect transient seg-\nmentsinaudiosignals. ShorttimeFouriertransform(STFT)\nis used to detect rapid local changes in the audio signal.\nThe algorithm has two steps that iteratively - (a) calculate\na function of the STFT and (b) build a transient signal. A\ndynamic thresholding scheme is used to locate the poten-\ntial positions of transients in the signal. The iterative pr o-\ncedureensuresthatgenuinetransientsarebuiltupwhileth e\nlocalised spectral noise are suppressed by using an energy\ncriterion. The extracted transient signal is later compare d\nto a ground truth dataset. The algorithm performed well\non two databases. On the EBU-SQAM database of mono-\nphonicsounds,thealgorithmachievedanF-measureof90%\nwhile onourdatabaseofpolyphonicaudioanF-measureof\n91% was achieved. This technique is being used as a pre-\nprocessing step for a tempo analysis algorithm and a TSR\n(Transients+Sines+Residue)decompositionscheme.\n1. INTRODUCTION\nTransientsare portionsof audiosignals that evolvefast an d\nunpredictably over a short time period [1]. Transients can\nbe classiï¬ed as attack transients (sound onsets), rapid de-\ncay transients (sound offsets), fast transitions (portame n-\ntos) and noise/chaotic regimes (sounds like handclaps, rai n\netc) [2]. Percussive sounds, guitar slaps, stop consonants\n(uttered during singing) are very good examples of tran-\nsientsignals. Transientsgenerallylast for50msanddispl ay\nfast changesin amplitude and phase at variousfrequencies.\nTransients can be classiï¬ed as weak or strong based on the\nstrengthoftheenvelopewhiletheycanalsobe charaterized\nas fast or slow dependingon the rate of change of envelope\nPermission to make digital or hard copies of all or part of thi s work for\npersonal or classroom use is granted without fee provided th at copies are\nnot made or distributed for proï¬t or commercial advantage an d that copies\nbear this notice and the full citation on theï¬rstpage.\nc/circlecopyrt2011 International Society for MusicInformation Retrieva l.amplitude. Fast transients have sharp amplitude envelopes\nwhile slow transients have broad (platykurtic) envelopes.\nTransient detection is an important problem in many areas\nofmusicresearchlike-audiocoding(parametricaudiocod-\ning [3], pre-echo reduction [4] etc), onset detection [5,6] ,\ntime-scalingofaudiosignals[2,7,8],notetranscription [2],\nrhythmanalysisandpercussiontranscription[9,10].\nOne of the ï¬rst attempts to detect and model transients was\nthe TMS (Transient Modeling Synthesis) model proposed\nin [11] as an extension to the popular sinusoidal modeling\nof McAulay et al. [12] and sine + noise model [13]. The\nbasic idea of the TMS model is the time-frequency dual-\nity. The TMS model is also dual to the sinusoidal model-\ning [12]. That is, by choosing a proper linear transform,\na pure sinusoid in time domain appears impulsive in the\nfrequency domain and an impulsive like signal in time do-\nmain looks sinusoidal in the frequency domain. Discrete\nCosine Transform (DCT) was thus chosen to provide the\nmapping from the time domain to the frequencydomain so\nthat transients in the time domain become sinusoidal in the\nfrequency domain. Energy of the original signal and its\nresidue from signal modeling using DCT is used for tran-\nsient detection. Masri et al. [5] used the high frequency\ncontentfeaturetodetectattacktransientsforthepurpose sof\naudioanalysis/synthesis. Abruptphasechangesinabankof\noctave spaced ï¬lters has been employedto detect transients\nin [7]. Recently, group delay function has been used to de-\ntecttransientsinmonophonicandpitchedpercussiveinstr u-\nments[14]. In[15,16]linearpredictionfollowedbythresh -\nolding on the residual signal envelope have been used for\ntransientdetectionandmodeling. Roebelused thecenterof\ngravity (COG) of a signal to locate transients and use it for\nonset detection with good results [6]. Torresani et al. [17]\nhave used a concept of â€œtransientnessâ€œ to detect transient\nsignals. Twosetsofbasisfunctionsthathavesparse(dense )\nrepresentationsfor puresinusoidsand dense (sparse) repr e-\nsentationsfortransientssimultaneouslyarechosentodeï¬ ne\nthe transientness of audio signals. For a more exhaustive\n203Poster Session 2\nsurveyontransientdetectionwe referthe readerto[18].\nMost of the above discussed works use monophonic audio\nfor their results. Daudet et al. [18] conducted a survey of\nvarious techniques and their efï¬ciency of transient detec-\ntiononthepopularâ€œglockenspielâ€andâ€œtrumpetâ€audiosig-\nnals. Gnannet al. [14]haveusedthe EBU-SQAM database\n(monophonicsignals) to test their algorithmand we use the\nsame too.\nIn this paper, we propose to build on Ono et al. [19,20]\nbyusingamuchsimpleriterativeprocedure. Thisalgorithm\ncan be used for audio coding, rhythm analysis and percus-\nsiontranscriptionamongstthemanypossibletasks.\nThis paper is organised as follows. We describe our ap-\nproach and choice of parameters in section 2. Section 3\npresents our experimentalsetup, databases used and the re-\nsultsalongwithsomeadvantagesofourapproach. We con-\ncludeinsection4.\n2. THETRANSIENT DETECTION ALGORITHM\nWe consider percussive sounds(drums, tom-tomsetc), gui-\ntarslapsandsungconsonantsastransients. Theyshowupas\nvertical lines in spectrograms [19] . Our algorithm detects\nsuch vertical lines in the spectrogram that have sufï¬cient\nstrengthandbandwidth. Weintendtodetectreasonablyfast\npercussivetransientslikepianohits,guitarslapsandthe var-\nious drums while neglecting the slow transient signals like\ngongs.\nLetx[n]be a polyphonic audio signal. The signal is re-\nsampled at 16kHz to account for varied sampling rates and\nrecordingconditions(Thealgorithmworksat anysampling\nrate but we choose 16kHz to standardize steps for our TSR\nalgorithm). Thesignalisnormalisedsuchthatitsmaximum\nvalueis1asfollows,\nxnorm[n] =x[n]\nmax(|x[n]|). (1)\nThenormalizationstep isnotnecessaryforaudiocoding\napplications. This signal is now split into frames of 40 ms\nwith an overlapof 30ms. Each frame of the signal is multi-\nplied with a Blackman-Harris window of length, N= 640\nsamplestoreducesidelobes. A STFTofthe signalanalyses\nthe frequency content of the signal in regular periods. Let\nX(i,k)denotetheSTFT of the signalfor the ithframeand\nkthfrequencybin. Then,\nX(i,k) =Nâˆ’1/summationdisplay\nn=0x[n].w[nâˆ’iR].eâˆ’j.2Ï€.n.k/N,(2)\nwherew[n]is the windowing function, Nis the numberof samples in a window and Ris the time shift in samples\n[21].\nWenowdeï¬nefunctions Tâˆ’andT+thatarederivedfrom\nthemagnitudespectrumofthesignalasfollows:\nTâˆ’(i,k) =|X(i,k)|âˆ’|X(iâˆ’1,k)|,(3)\nT+(i,k) =|X(i,k)|âˆ’|X(i+1,k)|.(4)\nThefunctions Tâˆ’andT+actasintermediatefunctionswhich\ndetect vertical edges in the spectrogram. These derivative s\nindicate onsets and offsets respectively. Since transient s\nhave fast onsets followed by fast offsets, the Tâˆ’andT+\nfunctions should have high values at frames corresponding\nto transients. We now form a smoothened version of the\nabovefunctionsasfollows;\nF(i,j) = 0.5{j+Î½/summationdisplay\nk=jâˆ’Î½{1+sgn(Tâˆ’(i,k))}.Tâˆ’(i,k)\n+{1+sgn(T+(i,k))}.T+(i,k)},(5)\nwhere\nsgn(Î¸) =/braceleftbigg1ifÎ¸â‰¥0,\nâˆ’1ifÎ¸ <0.(6)\nF(i,j)computestemporalchangesinthemagnitudespec-\ntrum at the frame i.F(i,j)considers half wave rectiï¬ed\npositive values of T+andTâˆ’functions and adds it across\nfrequencybins jâˆ’Î½toj+Î½. Thehalfwaverectiï¬cationin\nequation(5)ensuresthatwedetectonlyonsetsfrom Tâˆ’and\noffsetsfrom T+respectively. Theparameter Î½takesintoac-\ncount the spectral spread of the transient, neglecting nois y\ninï¬‚ectionsinthe spectrogram.\nAscanbeseeninFigure1,thefunctioninred(dashes)is\nwith smoothing along the vertical direction (vertical neig h-\nboursÎ½= 3) and the function in blue (dashes and dots) is\nwithout smoothing ( Î½= 0). The smoothing operation en-\nsuresthatonlygenuineverticaledgesinthespectrogramar e\naccentuated and spurious changes (due to inï¬‚ections in vo-\ncals/instrumentation)aresuppressed.\n2.1 Proposediterationsteps\nFor the extraction of transients, we now use XandFin\nan iterative framework as described below. The main algo-\nrithm consists of 3 iterative steps. In the ï¬rst step, dynami c\nthresholdsarecomputed.Inthesecondstepthetransientsi g-\nnal updates are obtained. In the third step functionsdepen-\ndentonX(i,k)areupdated. Wenowusethe Ftodetectthe\npresenceoftransientsin theaudiosignal.\n20412th International Society for Music Information Retrieval Conference (ISMIR 2011)\nFigure1. Function Fat bin number2kHzforClaves-s sig-\nnal from EBU-SQAM database. The transient regions get\naccentuatedmorewithverticalneighbours, Î½= 3compared\ntoÎ½= 0,whilethelocalringingnoiseissuppressed.\n2.1.1 StepI:Computingdynamicthresholds\nAn adaptive threshold for the detection function F(i,j)is\nderived. Let Î»(i,j)representthedesiredthreshold. Then,\nÎ»(i,j) =Î²Ã—/summationtexti+Ï„\nl=iâˆ’Ï„F(l,j)\n2Ï„+1, (7)\nwhereÎ²is a parameter to control the strength of tran-\nsientsthataretobeextracted. Equation(7)calculatesati me\nvarying threshold for every time-frequency bin ( ithframe\nandjthfrequencybin). A ï¬‚ag is set if the value of Fat the\nbinjisgreaterthanthethreshold Î»(i,j). Thatis,\nÎ“(i,j) =/braceleftbigg1ifF(i,j)> Î»(i,j),\n0ifF(i,j)â‰¤Î»(i,j).(8)\nSumming the ï¬‚ag function Î“along the frequency bins\n(representedby Î£Î“)indicatesthenumberoffrequencybins\ninasingleframethathavemoresigniï¬cantenergythantheir\nneighboursandmayrevealthepresenceorabsenceofatran-\nsient. Thatis,\nÎ£Î“(i) =Nâˆ’1/summationdisplay\nj=0Î“(i,j). (9)\n2.1.2 StepII:Extractionofthetransientportionand\nupdateof X\nIftheÎ£Î“isgreaterthanathreshold Î»Thr,thecorresponding\nframe is declared transient frame and a small fraction Î´of\nthe magnitude spectrum is subtracted from that frame and\naddedto thefunction Ptobuildtransientsasfollows,\nP(i,j) =/braceleftbiggP(i,j) ifÎ£Î“< Î»Thr,\nP(i,j)+Î´.X(i,j)ifÎ£Î“â‰¥Î»Thr,\n(10)\nFigure 2. The function F - initial value and value after 20\niterations at 400Hz for Claves-s signal from EBU-SQAM\ndatabase.\nwherejvariesfrom 0toNâˆ’1.\nIn case ofdetected transients,the magnitudespectrumis\nmodiï¬edasfollows,\nX(i,j) =/braceleftbiggX(i,j) ifÎ£Î“< Î»Thr,\n(1âˆ’Î´).X(i,j)ifÎ£Î“â‰¥Î»Thr,(11)\nwherejvariesfrom 0toNâˆ’1.\n2.1.3 StepIII:Updateof functionsdependentonX\nThe functions F,Î»,Î“andÎ£Î“are updated using the Xob-\ntainedfrom2.1.2.\nWe iterate over steps I, II and III for Mtimes. Figure 2\nshows the changes in Fat a particular frequency bin after\nvarious iterations. As can be seen from Figure 2, Fde-\ncreases at places of transients and increases in the adjacen t\nframes. This is due to the deï¬nition of F, since it consid-\nersa contributionfrom Tâˆ’andT+onlyif theyarepositive.\nIf after a particular iteration, say Tâˆ’(i,j)becomes positive\nbecause|X(iâˆ’1,j)|reduced from the previous iteration\n(see update equations in Algorithm.1), then F(i,j)can be\ngreaterthanitsvalueinthepreviousiteration.\nThe function Pat the end of Miterationsrepresentsthe\nspectrogramofthetransientsignal. Thesamestepsare pre-\nsented as follows. From now on all variablesand functions\nused for the algorithm are superscribed with (n)(only if\ntheir values dependon the iteration) to represent the nthit-\neration.\nWe beginby initialising Pto0. The valuesfor the func-\ntionsX,F,Î“,Î»andÎ£Î“,calculatedfromtheoriginalsignal,\nareusedfortheinitial valuesofthe algorithm.\nWethushavetwoparametersthatcontrolboththestrength\nof the extractedtransient (controlledby Î²) and its spreadin\nfrequency (controlled by Î»Thr). We have used Ï„= 3and\nÎ´= 0.1inourimplementation.\n205Poster Session 2\nInput: Initialise P(1)to0,X(1)toX,F(1)toF,Î»(1)\ntoÎ»,Î“(1)toÎ“,Î£(1)\nÎ“toÎ£Î“\nOutput: TransientsignalP extractedfromX\nforeachn= 1toMdo\n(I,II) ifÎ£(n)\nÎ“(i)â‰¥Î»Thrthen\n(i)|X(n+1)(i,0 :Nâˆ’1)|=\n(1âˆ’Î´)Ã—|X(n)(i,0 :Nâˆ’1)|\n(ii)P(n+1)(i,0 :Nâˆ’1) =\nP(n)(i,0 :Nâˆ’1)+Î´Ã—|X(n)(i,0 :Nâˆ’1)|\nelse\n(i)|X(n+1)(i,0 :Nâˆ’1)|=|X(n)(i,0 :Nâˆ’1)|\n(ii)P(n+1)(i,0 :Nâˆ’1) =P(n)(i,0 :Nâˆ’1)\nend\n(III) Calculate F(n+1),Î»(n+1),Î“(n+1),and\nÎ£(n+1)\nÎ“usingX(n+1)\nend\nAlgorithm 1: Flow for updating equations of the algo-\nrithm\nThe iterative procedure is used instead of a single step\ntransient detection since this algorithm is a part of a TSR\ndecomposition algorithm we are currently developing. The\niterative procedure helps to uncover slightly masked and\nweaktransientsatlaterstepsashasbeenrevealedinourpre -\nliminary experiments. The TSR decomposition algorithm\nworks by alternatively identifying and extracting transie nts\nand sinusoids until we are left with a residue signal. Even\nwithout the sinusoidal extraction steps here, the algorith m\ndoes detect partially masked and low energy transients. As\ncanbeseenfromFigure3,thetransientsignalbuildsslowly\noveriterations.\nAfterthe Msteps,thetransientsignalthathasbeengen-\neratedisconvertedbackintotimedomainbyaninverseDis-\ncrete Fourier transform (IDFT). The phase of the original\nsignal is used for the IDFT procedure. Frames of the tran-\nsient signal that have less than 5%of the maximum energy\nare discardedto retain onlysigniï¬canttransients. Theloc a-\ntions of the transients are compared with the ground truth\ndataforevaluation.\nFigure4showstheglockenspielsignalfromEBU-SQAM\ndatabaseanditsextractedtransient. Ascanbeseen,thetra n-\nsient signaliswell extracted.\n3. EXPERIMENTS AND EVALUATIONS\nA database of 33clips averaging 10seconds each was pre-\nparedbyselectingaudiofromvariouspossiblegenres(pop,\nrock, R&B etc). Each clip was converted to â€™.wavâ€™ format\nfromCDs,andresampledat16kHz. Eachclipwasmanually\nFigure 3. The original signal and the extracted transient\nsignal after 1, 5 and 20 iterations. The strength and time\ndurationofthetransientsignalincreaseswithiterations\nannotated for percussive transients after multiple listen ing,\nusing the gating procedure [22]. Two people annotated the\ndatabase independently. The common transient segments\nfrom both the annotators were chosen for our ï¬nal ground\ntruth set. The database has a total of 1308 transient seg-\nments. The database was split into 2non-overlapping sets\nconsisting of 10clips for the training dataset having 406\ntransientsegmentsand23clipsforthetest datasetwith 902\ntransientsrespectively1.\nThe parameters Î²andÎ»Thrwere optimised using the\ntraining set consisting of 10 clips. Î²was varied in steps\nof0.1from1.25to2.5whileÎ»Thrwas varied as a frac-\ntion of frame length N(i.eN/10,N/9,N/8...). A transient\nwas declared to have been found if the extracted transient\noverlapped with the ground truth segment. We got optimal\nperformance for Î²= 2andÎ»Thr=N/6.Î»Thrparameter\nselects only signiï¬cantly long vertical lines in the spectr o-\ngram while Î²parameter evaluates the strength of the tran-\nsients. We used M= 20iterations for the algorithm. This\nway if a transient exists, approximately 90% of the magni-\ntudecanbeextractedintheiterativestepsifthe Î£Î“satisï¬es\nthethresholdconditionsforall the 20iterations.\nTheseparameterswereusedtotesttheremaining23songs\nfor their performance. The algorithm was able to correctly\ndetect (CD) 808 (90%) transients with 65 (7%)false posi-\n1Thisisdenoted astheLSMLdatabase. Weintend tomakethisaf reely\navailable database for research soon\n20612th International Society for Music Information Retrieval Conference (ISMIR 2011)\nFigure 4. Glockenspiel signal from EBU-SQAM database\nandtheextractedtransientsignal\nDatabaseName TotalCDFPFNDD\nLSML 90280865942\nEBU-SQAM 27623711390\nTable 1. Performanceofthetransientextractionalgorithm\ntives (FP). This is equivalent to a Precision (P) of 0.92and\nRecall (R) of 0.89. TheF measureisthus 0.91.\nWe have also tested our algorithm on the EBU-SQAM\npercussive monophonic database [14]. The testing proce-\ndure followedin [14]was used for testing our algorithmon\nthis database. The EBU-SQAM database has 276percus-\nsive transients in 24ï¬les. Our algorithm correctly detected\n237transients correctly while 11transients were detected\nas FP. This gives our algorithm an F-measure of 0.90. This\ncomparesvery well with the results from [14], where an F-\nmeasure of 0.92is achieved on the same dataset of EBU-\nSQAM database. While the parameters in [14] are opti-\nmized for the EBU-SQAM database, we use the same pa-\nrametersthatareoptimizedforourLSMLpolyphonicmusic\ndatabase.\nFor the EBU-SQAM database we observed that we got\nfalse positives during the slow transient portionsof the si g-\nnal orforsignalswith heavyringinginthe decaytail. Also,\na shortcomingthat we observedwith our algorithmwas the\nsensitivity to signal continuity. The EBU-SQAM database\nhas signal discontinuity in 2 ï¬les and those portions were\ndetectedastransients.\nThe performance of our algorithm is tabulated in Table\n1. Since the algorithm acts as a pre-processing stage for a\ntempo analysis algorithm and a TSR decomposition algo-\nrithm, the false positives do not harm much except when\nFigure 5 . The locations of the extracted transients are\nshown w.r.t the ground truth.The blue lines indicate the ex-\ntractedtransientlocationsandtheredlinesthe groundtru th\nParameter Numericvalue\nFramesize 640\nÎ½ 3\nÎ´ 0.1\nÏ„ 3\nÎ² 2\nM 20\nFthr 106\nTable 2. Used parametersandtheirvalues\nthey have sufï¬cient energies. Figure 5 showsthe audio sig-\nnal andextractedtransientlocationsin comparisonwith th e\ngroundtruth locations for a polyphonic piece from LSML\ndatabase.\nThe values of the parameters used by us in our imple-\nmentationisgiveninTable 2.\n4. CONCLUSIONSAND FUTUREWORK\nWehavediscussedasimpleiterativeprocedurefordetectin g\ntransients from polyphonic audio signals. The method is\nused in a TSR decomposition algorithm. This algorithm is\nalso currently acting as a pre-processing step for a tempo\nanalysisalgorithm. Wearealsolookingatusinggeneralise d\nTEF(Teagerenergyfunctions)typeoffunctionstoimprove\nourtransientdetectionaccuracy.\n5. REFERENCES\n[1] J. P. Bello and L. Daudet and S. Abdallah and C.\nDuxbury and M. Davies and M. B. Sandler: â€A Tu-\ntorial on Onset Detection in Music Signalsâ€, IEEE\nTransactions on Speech and Audio Processing, , Vol-\n13,No.5,September,2005.\n207Poster Session 2\n[2] H.Thornburg: â€Detectionandmodelingoftransientau-\ndio signals with prior informationâ€, PhD Thesis, Stan-\nfordUniversity ,2005.\n[3] B. Edler and H. Purnhagen: â€Parametric Audio Cod-\ningâ€,International Conference on Signal Processing\n(ICSP),2000.\n[4] R. Vaï¬n and R. Heusdens and S. van de Par and W.\nB. Kleijn: â€Improved modeling of audio signals by\nmodifying transient locations â€, Workshop on Applica-\ntionsofSignalProcessingtoAudioandAcoustics(WAS-\nPAA01)),2001.\n[5] P. Masri and A. Bateman: â€ImprovedModelling of At-\ntack Transients in Music Analysis-Resynthesisâ€, Proc.\nof the Intl.Computer Music Conference(ICMC) ,pg.100-\n103,1996.\n[6] A. Roebel: â€Onset detection by means of tran-\nsient peak classiï¬cationâ€, http://www.music-\nir.org/mirex/abstracts/2010/AR4.pdf ,MIREX-2010.\n[7] C.DuxburyandM.DaviesandM.Sandler: â€Separation\noftransientinformationinmusicalaudiousingmultires-\nolutionanalysistechniquesâ€, Proc.oftheConferenceon\nDigitalAudioEffect(DAFx)) ,2001.\n[8] F. X. Nsabimanaand U. Zolzer: â€AudioSignal Decom-\npositionforPitchandTimeScalingâ€, InternationalSyn-\nmposium on Communications, Control Signal Process-\ning(ISCCSP) ,2008.\n[9] J. Sillanpaa and A. Klapuri and J. Seppanen and T.\nVirtanen: â€Recognition of acoustic noise mixtures by\ncombined bottom-up and top-down processingâ€, Pro-\nceedings of the European Signal Processing Confer-\nence(EUSIPCO) ,2000.\n[10] J.Uscher: â€Extractionandremovalofpercussivesound s\nfrom musical recordingsâ€, Intl Conference on Digital\nAudioEffects(DAFx) ,2006.\n[11] T.S.VermaandS.N.LevineandT.H.Y.Meng: â€Tran-\nsient modeling synthesis:a ï¬‚exible analysis/synthesis\ntoolfortransientsignalsâ€, IntlComputerMusic Confer-\nence(ICMC) ,1997.\n[12] R. McAulay and T. F. Quatieri: â€Speech Analy-\nsis/Synthesis based on a sinusoidal representationâ€,\nIEEETransactionsonAcoustics,SpeechandSignalPro-\ncessing,Vol34,pp-744-754,1990.\n[13] X. Serra and J. O. Smith: â€Spectral Modeling Synthe-\nsis:A sound analysis/synthesissystem based on a deter-\nministicplusstochasticdecompositionâ€, ComputerMu-\nsic Journal ,Vol14(4),pp-14-24,1990.[14] V. Gnann and M. Spiertz: â€Transientdetection with ab-\nsolute discrete group delay,â€, IEEE InternationalWork-\nshop on Intelligent Signal Processing and Communica-\ntionSystems(ISPACS) ,2009.\n[15] F. X. Nsabimana and U. Zolzer: â€A tran-\nsients/sinsoids/residual approach for audio signal\ndecompositionâ€, Procof DAGA'08 ,2008.\n[16] F. X. Nsabimana and U. Zolzer: â€Transient encoding\nof audio signals using dyadic approximationsâ€, Proc of\n10thIntlConferenceonDigitalAudioEffects ,2007.\n[17] S. Molla and B. Torresani: â€Determining local tran-\nsientness in audio signalsâ€, IEEE Signal Processing\nLetters,Vol-11(7),pp.625-628,2004.\n[18] L. Daudet: â€A review of techniques for the extraction\nof transients in musical signalsâ€, Proc. of the Computer\nMusicModelingandRetrieval(CMMR) ,2005.\n[19] N. Ono and K. Miyamoto and J. Le Roux and H.\nKameoka and S. Sagayama: â€Separation of a monau-\nral audio signal into harmonic/percussive components\nbycomplementarydiffusiononspectrogramâ€, European\nsignalProcessingConference(EUSIPCO)) ,2008.\n[20] N. Ono and K. Miyamoto and H. Kameoka and S.\nSagayama: â€A real-timeequalizerof harmonicandper-\ncussive components in music signalsâ€, Intl Society of\nMusicInformationRetrievalConference) ,2008.\n[21] Lawrence Rabiner and Ronald Schafer: â€Chap\n6,Pages:251-252, Digital Processing of Speech Sig-\nnalsâ€,PearsonEducation(IndianEdition) ,1993.\n[22] D. J. Hermes: â€Vowel Onset Detectionâ€, Journal\nof Acoustical. Society. of America ,Vol-87(2), 866-\n873,1990.\n208"
    },
    {
        "title": "Factorization of Overlapping Harmonic Sounds Using Approximate Matching Pursuit.",
        "author": [
            "Steven K. Tjoa",
            "K. J. Ray Liu"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1414962",
        "url": "https://doi.org/10.5281/zenodo.1414962",
        "ee": "https://zenodo.org/records/1414962/files/TjoaL11.pdf",
        "abstract": "Factorization of polyphonic musical signals remains a difficult problem due to the presence of overlapping harmonics. Existing dictionary learning methods cannot guarantee that the learned dictionary atoms are semantically meaningful. In this paper, we explore the factorization of harmonic musical signals when a fixed dictionary of harmonic sounds is already present. We propose a method called approximate matching pursuit (AMP) that can efficiently decompose harmonic sounds by using a known predetermined dictionary. We illustrate the effectiveness of AMP by decomposing polyphonic musical spectra with respect to a large dictionary of instrumental sounds. AMP executes faster than orthogonal matching pursuit yet performs comparably based upon recall and precision.",
        "zenodo_id": 1414962,
        "dblp_key": "conf/ismir/TjoaL11",
        "keywords": [
            "Factorization",
            "polyphonic",
            "musical",
            "signals",
            "overlapping",
            "harmonics",
            "existing",
            "dictionary",
            "learning",
            "methods"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nFACTORIZATION OF OVERLAPPING HARMONIC SOUNDS USING\nAPPROXIMATE MATCHING PURSUIT\nSteven K. Tjoa\nImagine Research\nSan Francisco, CA 94114 USA\nsteve@imagine-research.comK. J. Ray Liu\nUniversity of Maryland\nCollege Park, MD 20742 USA\nkjrliu@umd.edu\nABSTRACT\nFactorization of polyphonic musical signals remains a dif-\nï¬cult problem due to the presence of overlapping harmon-\nics. Existing dictionary learning methods cannot guarantee\nthat the learned dictionary atoms are semantically meaning-\nful. In this paper, we explore the factorization of harmonic\nmusical signals when a ï¬xed dictionary of harmonic sounds\nis already present. We propose a method called approxi-\nmate matching pursuit (AMP) that can efï¬ciently decom-\npose harmonic sounds by using a known predetermined dic-\ntionary. We illustrate the effectiveness of AMP by decom-\nposing polyphonic musical spectra with respect to a large\ndictionary of instrumental sounds. AMP executes faster than\northogonal matching pursuit yet performs comparably based\nupon recall and precision.\n1. INTRODUCTION\nDictionary learning, sparse coding, and constrained factor-\nization algorithms have recently revolutionized the way we\nperform music transcription and source separation. Many\nresearchers have reported success when decomposing sim-\nple musical signals using nonnegative matrix factorization\n(NMF) [23] or methods based upon sparse coding such as\nK-SVD [1,2]. Unfortunately, problems remain for intricate,\npolyphonic musical signals. When musical notes overlap\nin time and frequency, the separation and transcription per-\nformance of these basic dictionary learning methods dimin-\nishes rapidly. In such a case, the algorithm will usually learn\na dictionary where each individual atom contains informa-\ntion from multiple musical sources, thus hindering our at-\ntempts at decomposition.\nResearchers have slowly improved upon the original dic-\ntionary learning methods by adding constraints to the learn-\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proï¬t or commercial advantage and that copies\nbear this notice and the full citation on the ï¬rst page.\nc/circlecopyrt2011 International Society for Music Information Retrieval.ing process. By restricting the dictionary atoms to reside\nwithin a predetermined feasible set, we can ensure that the\nlearned atoms will be useful at the conclusion of the learn-\ning process. For example, existing solutions include adding\nconstraints to the dictionary learning process such as har-\nmonicity [3, 25] or smoothness [3, 26].\nAnother solution is to add structure to the dictionary. For\nexample, one can construct and use a large, predeï¬ned, over-\ncomplete dictionary where each atom is already labeled and\nassumed to contain information from only one musical source.\nInstead of learning an optimal dictionary for a given musi-\ncal signal, it may sufï¬ce to match the signal to this large set\nof precomputed, labeled dictionary atoms. Then, by decom-\nposing a signal with respect to this ï¬xed dictionary, classiï¬-\ncation is easily achieved by simply reading the label of the\natom. As musical databases become more available, con-\nstruction of predeï¬ned dictionaries will become easier, thus\nreducing the need for adaptive dictionary learning.\nOf course, the performance of such an algorithm depends\nupon the breadth of the dictionary. When atoms from more\nmusical sources are added to the dictionary, the dictionaryâ€™s\nability to decompose polyphonic music will improve. How-\never, dictionary growth introduces concerns related to scal-\nability and computational complexity. While the aforemen-\ntioned algorithms have signiï¬cantly advanced the state of\nthe art, they remain slow and difï¬cult to scale as the dictio-\nnary size increases. Most of the original factorization meth-\nods such as matching pursuit (MP) [18] and NMF with mul-\ntiplicative updates [17] have complexity that is linear in the\nsize of the dictionary. As a result, when dictionary sizes\ngrow, the transcription efï¬ciency of these algorithms dimin-\nishes.\nTo summarize the problem: how can we make use of\na large, precomputed, overcomplete dictionary to factorize\noverlapping harmonic sounds accurately and efï¬ciently?\nWe address this problem by proposing a variant of MP\ncalled approximate matching pursuit (AMP). Unlike MP and\nNMF, AMP can decompose signals into a sparse combina-\ntion of atoms with complexity that is sublinear in the dictio-\nnary size while maintaining accuracy. To do this, AMP uses\n257Poster Session 2\nan approximate nearest neighbor (ANN) method to ï¬nd ap-\nproximate matches to the signal residual at each iteration.\nThe ANN method that we choose in this work is locality\nsensitive hashing (LSH), a probabilistic hash algorithm that\nplaces similar, yet not identical, observations into the same\nbin. LSH can retrieve near neighbors with a complexity that\nis sublinear in the dictionary size.\nOur experiments demonstrate that AMP is as capable as\northogonal matching pursuit (OMP) [20] for decomposing\npolyphonic musical spectra into combinations of atoms from\na large dictionary of over 17,000 labeled musical spectra.\nMeanwhile, AMP requires less computation and factorizes\nmore quickly than OMP.\n2. RELATED WORK\nComputation of sparse coefï¬cients with respect to a large,\novercomplete dictionary is often accomplished by pursuit\nalgorithms such as MP [18]. This greedy algorithm directly\naddresses the issue of sparsity by decomposing a signal, x,\ninto a linear expansion of waveforms that are selected from\na redundant dictionary of functions. When stopped after a\nfew iterations, this algorithm yields a signal approximation\nusing only a few atoms. After each iteration of the MP al-\ngorithm, the residual, r, is orthogonal to the previously se-\nlected vector, ak, but not necessarily orthogonal to the dic-\ntionary vectors selected earlier.\nPati et al. proposed OMP, an improvement over MP which\nensures that the residual is orthogonal to all previously se-\nlected dictionary vectors [20]. After dictionary atoms are\nselected for inclusion into the decomposition, an extra or-\nthogonalization step is performed by solving a least-squares\nproblem. Researchers have shown that OMP provides a dra-\nmatic improvement over MP [20]. In many cases, when an\ninput signal is known to be k-sparse, OMP converges in k\niterations, while MP will require many more iterations to\nconverge.\nPursuit algorithms have been applied to MIR in many\nways. The most popular applications are music transcription\nand source separation. Harmonic matching pursuit (HMP)\nhas been used to decompose an audio signal into Gabor or\nharmonic (i.e., sums of Gabor) atoms [15]. Dictionaries of\natoms can also be adapted and learned to ï¬t the data [9]. To\nresolve instances when harmonics from separate notes over-\nlap, some algorithms impose smoothness constraints [4].\nSimilar sparse coding methods have been used for genre\nrecognition [19]. In the neurological signal processing lit-\nerature, pursuit methods for generic acoustic signals have\nbeen applied for coding purposes [24].\nCotton and Ellis [10] also use LSH together with MP,\nhowever that work addresses a fundamentally different prob-\nlem â€“ content-based search of whole acoustic events, e.g.,\nthe sound made by a horseâ€™s hoof. There, the sparse repre-sentation produced by MP is stored using LSH. On the other\nhand, our proposed method addresses the problems of tran-\nscription and source separation. As shown later, we enhance\nMP by embedding LSH within MP to make it faster and\nmore scalable. Also, we use a massive dictionary of real-\nworld musical spectra, not synthetic Gabor atoms as in [10].\n3. PROBLEM FORMULATION\nGiven the magnitude spectrum of an input signal, xâˆˆRM,\nand a dictionary, A= [a1a2...aK]âˆˆRMÃ—K, the problem\nis to ï¬nd a vector of coefï¬cients sâˆˆRKthat minimizes\n||xâˆ’As||2.\nWhenM < K , the dictionary is called overcomplete ,\nand there are inï¬nitely many solutions for s. However, by\nimposing a sparsity constraint on s, the solution space di-\nminishes greatly, possibly to a unique solution. In particu-\nlar, if the input is truly a sparse linear combination of dic-\ntionary atoms, i.e., x=As0, where s0is a sparse vector,\nthen the problem becomes ï¬nding an optimal set of coefï¬-\ncients, Ë†s= argmins||xâˆ’As||2, that is equal to the input\ncoefï¬cients, i.e., Ë†s=s0.\nAn exhaustive search for the sparsest solution is NP-hard\n[12]. However, suboptimal greedy algorithms such as OMP\noften work well in practice. Unfortunately, OMP requires\nat leastKinner products to computed during each iteration,\nthus creating a complexity that is at least linear in K. Be-\ncause this complexity is too slow for large dictionaries, the\nproblem becomes solving for Ë†s=s0using an algorithm that\nhas complexity that is sublinear in the dictionary size, K.\nWithout loss of generality, we assume that the dictionary\nis overcomplete, M < K ; this assumption is not strictly\nnecessary for AMP to operate. We also assume that the true\nsparsity of any input signal, ||s0||0, is less than the dimen-\nsionality,M. For musical signals, this assumption usually\nholds in practice. For example, even in highly polyphonic\nmusic, the number of simultaneous sounds will likely be sig-\nniï¬cantly less than the dimensionality of our spectra, i.e., the\nnumber of frequency bins. If not, then we increase the FFT\nsize to produce longer spectra.\n4. PROPOSED ALGORITHM: APPROXIMATE\nMATCHING PURSUIT\nOne drawback of existing pursuit methods such as MP and\nOMP is their complexity. When the dictionary size, K, be-\ncomes very large (e.g., over one million), these methods\nmay require an unacceptably large amount of computation\nto ï¬nd an answer. For example, in each iteration of MP,\nKinner products must be computed between the residual\nrand every atom in the dictionary â€“ a complexity of order\nO(MK). Here, we introduce a simple variation of these\npursuit methods that uses an ANN algorithm in place of\n25812th International Society for Music Information Retrieval Conference (ISMIR 2011)\ncomputingKinner products as done in MP. As a result, we\ncan reduce the complexity to be sublinear in K.\nThe approximate matching pursuit (AMP) algorithm is\ndescribed in Algorithm 1. This algorithm is similar to OMP\nexcept that it addresses the main computational bottleneck\nfor large dictionaries â€“ nearest neighbor search â€“ by allow-\ning any adequately near neighbor to be selected as a compo-\nnent.\nAlgorithm 1 Approximate Matching Pursuit [Tjoa and Liu]\nInput: xâˆˆRM;A= [a1,a2,...,aK]âˆˆRMÃ—Ks.t.\n||ak||2= 1for allk.\nOutput: Ë†sâˆˆRK\nInitialize:Sâ†âˆ… ;sâ†0;râ†x;/epsilon1>0.\nwhile||r||>/epsilon1do\nFind anyksuch that akandrare near neighbors.\nSâ†Sâˆªk\nSolve for{sj|jâˆˆS} :minsj|jâˆˆS||xâˆ’/summationtext\njâˆˆSajsj||2\nrâ†xâˆ’As\nË†sâ†s\nAMP intentionally resembles MP and OMP. Like OMP,\nAMP is capable of providing a sparse decomposition in far\nfewer iterations than MP. If the ANN retrieval method were\ninstead changed to a nearest-neighbor (NN) method, then\nAMP would yield identical results to OMP. Also, AMP is\nï¬‚exible in the sense that any ANN method could be used as\nlong as it performs retrieval in sublinear time. Therefore,\nAMP can also be considered as a modular framework of al-\ngorithms.\nDespite its simplicity, AMP embodies a fundamentally\ndifferent philosophy to signal factorization. AMP is a data-\ndriven algorithm, not a model- or knowledge-based algo-\nrithm. With such an abundance of available musical data, we\nuse side information, not rigid mathematical models, to rep-\nresent test data. Algorithmic advances such as AMP, cou-\npled with technological advances in computing, are mak-\ning data-driven algorithms more computationally feasible\nfor problems in MIR such as transcription and source sepa-\nration.\n5. LOCALITY SENSITIVE HASHING\nAMP allows the use of any ANN algorithm that can per-\nform retrieval in sublinear time. For this work, we focus on\nlocality-sensitive hashing (LSH), a category of algorithms\nthat places nearby points in a high-dimensional space into\nthe same bin in a hash table. Because of its simplicity,\nrobustness, and low complexity, LSH has become popular\nfor solving many high-level problems beyond MIR such as\nsearch and retrieval of text and images. The robustness of\nLSH is desirable for problems in MIR where queries are of-\nten distorted due to environmental or musical variation, andtherefore, learned dictionary atoms will rarely match prede-\nï¬ned dictionary atoms exactly. Ryyn Â¨anen and Klapuri used\nLSH to perform query-by-humming (QBH) by constructing\na hash table from pitch contour vectors [21]. Yu et al. use\nLSH and order statistics to store chroma features in a hash\ntable for audio content retrieval [28]. Cotton and Ellis use\nLSH to store landmarks in audio that correspond to mean-\ningful acoustic events [10]. Casey and Slaney have used\nLSH to store features called audio shingles for computing\nvarious levels of musical similarity between songs [5â€“7].\nHowever, LSH has rarely been used for signal-level prob-\nlems like music transcription. To our knowledge, this work\nis among the ï¬rst in MIR to use LSH for low-level tasks\nsuch as sparse coding and music transcription.\nWhile other ANN algorithms can be used within AMP\ninstead of LSH, such as those that use space partitioning\nlike the kd-tree and hierarchical k-means, these algorithms\ndo not work well in high-dimensional spaces, i.e., dimen-\nsionality over 100. In fact, all current indexing techniques\nbased on space partitioning degrade to linear search for suf-\nï¬ciently high dimensions [11, 14, 27]. Therefore, we only\nconsider LSH in this work.\nIn this work, for iâˆˆ{1,2,...,k}and/lscriptâˆˆ{1,2,...,L},\nwe deï¬ne the function h/lscript\nito be\nh/lscript\ni(q) = sign/angbracketleftp/lscript\ni,q/angbracketright (1)\nwhere p/lscript\niis a zero-mean, unit variance, Gaussian random\nvector with independent elements. As illustrated later, the\nparameterskandLadjust the tradeoff between recall and\nprecision of the dictionary atoms.\nIt has been shown that this choice of distribution on p/lscript\ni\nwill hash points together whose angle,\nÎ¸(q,r) = arccos/angbracketleftq,r/angbracketright\n||q||||r||, (2)\nis small [8]. Speciï¬cally, it can be shown that, for any iand\n/lscript, the probability that h/lscript\ni(q) =h/lscript\ni(r)is equal to\nP(h/lscript\ni(q) =h/lscript\ni(r)) = 1âˆ’Î¸(q,r)\nÏ€. (3)\nWe claim that two hashes are equal, h(q) =h(r), if and\nonly if there exists an /lscriptsuch that, for all iâˆˆ{1,2,...,k},\nh/lscript\ni(q) =h/lscript\ni(r). In other words, the following events are\nequivalent:\n{h(q) =h(r)}=âˆªL\n/lscript=1âˆ©k\ni=1{h/lscript\ni(q) =h/lscript\ni(r)}.(4)\nFrom (3) and (4), it can be shown that the probability that\nh(q) =h(r)is equal to\nP(h(q) =h(r)) = 1âˆ’/parenleftBigg\n1âˆ’/parenleftbigg\n1âˆ’Î¸(q,r)\nÏ€/parenrightbiggk/parenrightBiggL\n.(5)\n259Poster Session 2\n0.2\n0.4\n0.6\n0.80.2\n0.4\n0.6\n0.80.20.40.60.8Figure 1 . LSH example with k= 2. Points on the unit\nsphere are separated into 2k= 4bins.\nTo construct the LSH table, we initialize Lempty ta-\nbles. For each atom ain the dictionary A, and for each\n/lscriptâˆˆ{1,2,...,L}, its hash is computed as a k-tuple:\nh/lscript(a) = (h/lscript\n1(a),h/lscript\n2(a),...,h/lscript\nk(a)), (6)\nandais placed into bin h/lscript(a)of table/lscript. Finally, to perform\na query for point r, for all/lscript, we retrieve all of the points\nin binh/lscript(r)of table/lscript. Among these retrieved points that\nshare a bin with r, we perform exhaustive search to ï¬nd\nthe nearest neighbor among them. As indicated by Eq. 5,\nthrough the proper choice of kandL, one can achieve any\ndesired amount of similarity between any two input vectors.\nAn example of LSH is shown in Figure 1 when k= 2.\nPoints on the unit sphere are hashed, and those points that\nreside in the same bin share the same marker. We notice that\npoints in the same bin are close together.\nThere are many theoretical results for LSH that are be-\nyond the scope of this paper. For detailed discussion and\nproofs, please see [11, 14, 22, 27].\n6. EXPERIMENTS\nTo illustrate the performance of AMP, we factorize poly-\nphonic spectra as sparse combinations of atoms from a dic-\ntionary of real piano sounds. First, we discuss how to build a\ndictionary. For this work, our data comes from the Univer-\nsity of Iowa database of musical instrument samples [13].\nEach ï¬le in the data set is labeled by pitch and loudness,\ne.g., â€œPiano C4 mfâ€, and contains a signal of an isolated\nnote sampled at 44100 Hz. We only consider the subset of\npiano sounds.\nFor each signal, we compute a short-time Fourier trans-\nform with a frame size of 92.9 milliseconds (i.e., 4096/44100)\nand a hop of 10 milliseconds. To discard silent segments, wedetect any spectrum whose power is below a threshold. The\nremaining spectra are normalized to have unit Euclidean\nnorm and are saved along with their pitch labels. These nor-\nmalized spectra constitute the dictionary, A, and the pitch\nlabels are used later to evaluate matches among dictionary\natoms. In total, we use a dictionary of 17,753 spectra of pi-\nano sounds covering the entire piano keyboard (i.e., MIDI\nvalues 21 through 108).\nFor the following experiments, the input to AMP is a\nvector xâˆˆRM, a magnitude spectrum containing overlap-\nping harmonic sounds, where x=As0.Ais the dictionary\nof sizeM-by-Kdescribed earlier, and s0is a synthetically\ngenerated sparse vector of length KcontainingÎ»ones in\nuniformly random locations. In other words, Î»determines\nthe number of overlapping sounds at any moment. We vary\nÎ»in the following experiments.\nThe LSH structure accepts parameters Landk, whereL\nis the number of LSH tables and kis the length of each key.\nThe dictionary, A, is used to populate each of the LLSH\ntables as described in Section 5. Finally, given the input\nxand the LSH tables, AMP produces a sparse coefï¬cient\nvector, Ë†s.\nGiven the output, Ë†s, we count the number of hits, misses,\nand false alarms. A hit occurs if an element in s0matches\nan element in Ë†s. A miss occurs if an element in s0does not\nmatch any element in Ë†s. A false alarm occurs if an element\ninË†sdoes not match any element in s0. A match occurs when\ntwo coefï¬cients share the same pitch label.\nAll source code is written in Python using the NumPy,\nSciPy, and Matplotlib packages [16].\nIn Figure 2, we compare AMP against another pursuit\nmethod, OMP. For each algorithm, using the number of hits,\nmisses, and false alarms, we plot the recall, precision, and\nF-measure. Recall is deï¬ned as R=hits/(hits + misses),\nprecision is deï¬ned as P=hits/(hits + false alarms), and\nF-measure is deï¬ned as F= 2PR/(P+R). We also mon-\nitor the execution time and number of M-dimensional inner\nproducts computed by each algorithm. All quantities are av-\neraged over twenty independent trials.\nFrom Figure 2, we see that the recall, precision, and F-\nmeasure are all relatively similar for both algorithms. The\nrecall for AMP is nearly as high as that of OMP. The gap\nin precision between the algorithms is slightly larger. In\npractice, the stopping criterion can affect the tradeoff be-\ntween recall and precision. When convergence occurs early,\nË†sis more sparse; therefore, recall decreases and precision\nincreases. When convergence occurs late, Ë†sis less sparse;\ntherefore, recall increases and precision decreases. For this\nwork, we simply ï¬x the stopping criterion such that the ratio\nof the residual norm to the input norm, ||r||/||x||, is equal to\n0.25. A more sophisticated stopping criterion may be able\nto improve this tradeoff.\nNext, we plot the execution time. Results show that AMP\n26012th International Society for Music Information Retrieval Conference (ISMIR 2011)\nexecutes approximately two to four times faster than OMP.\nThe parameters used in LSH, (L,k), affect execution time.\nWhen the length of the key, k, is low, then there are fewer\nkeys and more elements per bin. Therefore, the candidate set\nof spectra is larger. When kis high, there are more keys and\nfewer elements per bin resulting in a smaller candidate set.\nThe number of tables, L, has the opposite effect of k. When\nLis high, the size of the candidate set increases. When Lis\nlow, the candidate set size decreases.\nFinally, we plot the number of M-dimensional inner prod-\nucts computed by both algorithms. This measure describes\nthe primary source of computational effort. We see that\nOMP requires far more inner products than AMP. For OMP,\neach iteration requires Kinner products because the resid-\nual is matched against every dictionary atom. For AMP,\neach iteration requires far fewer than Kinner products be-\ncause LSH only retrieves those dictionary atoms that are\nlikely close to the residual vector. However, we notice that\nthe gap in the number of inner products computed by OMP\nand AMP is larger than the gap in execution time. This dis-\ncrepancy is largely caused by overhead required of LSH, for\nexample, key computation, data subset retrieval, etc. Opti-\nmizing these operations at a lower level could further widen\nthe gap in execution time between AMP and OMP.\n7. CONCLUSION\nWe have proposed AMP, a pursuit algorithm that can decom-\npose overlapping harmonic spectra as well as OMP while\nexecuting in less time and requiring fewer computations.\nWe have shown that the recall, precision, and F-measure for\nAMP is comparable with that of OMP. Unlike OMP which\nhas complexity that is linear in the size of the dictionary,\nAMP has sublinear complexity and is therefore much faster.\nThe simple modiï¬cation of using LSH in place of exhaustive\nlinear search makes previously infeasible techniques feasi-\nble once again. Previously, LSH has primarily been used to\nsolve high-level tasks such as song or document retrieval;\nhere, we use LSH for the signal-level tasks of factorization\nand separation.\nAMP, like many recently proposed machine learning al-\ngorithms, uses real data rather than contrived models and\nconstraints to describe musical spectra. We hope that this\nsimple algorithm inspires a new class of methods that intel-\nligently exploit the abundant musical data that already exists\namong public collections rather than chasing gains in fully\nunsupervised algorithms where little progress is left to be\nmade.\nThe dictionary itself has a signiï¬cant impact on the de-\ncomposition. Therefore, future work will include proper\ndictionary design, i.e., how to create dictionary atoms from\nmusical data sets for maximum accuracy and efï¬ciency. Dic-\ntionary design also affects the proper choice of LSH param-eters,Landk. A careful analysis of pairwise distances\namong dictionary atoms can reveal which set of LSH pa-\nrameters minimizes the probability of error.\n8. REFERENCES\n[1] M. Aharon, Michael Elad, and Alfred Bruckstein. K-SVD: An\nalgorithm for designing overcomplete dictionaries for sparse\nrepresentation. IEEE Trans. Signal Processing , 54(11):4311â€“\n4322, November 2006.\n[2] Michal Aharon, Michael Elad, and Alfred M. Bruckstein. K-\nSVD and its non-negative variant for dictionary design. In\nProc. SPIE Conf. Wavelets , volume 5914, pages 327â€“339, July\n2005.\n[3] N. Bertin, R. Badeau, and E. Vincent. Enforcing harmonicity\nand smoothness in bayesian non-negative matrix factorization\napplied to polyphonic music transcription. IEEE Trans. Au-\ndio, Speech, and Language Processing , 18(3):538â€“549, March\n2010.\n[4] FJ Ca Ëœnadas Quesada, P. Vera-Candeas, N. Ruiz-Reyes,\nR. Mata-Campos, and JJ Carabias-Orti. Note-event detec-\ntion in polyphonic musical signals based on harmonic match-\ning pursuit and spectral smoothness. J. New Music Research ,\n37(3):167â€“183, 2008.\n[5] M. Casey, C. Rhodes, and M. Slaney. Analysis of minimum\ndistances in high-dimensional musical spaces. IEEE Trans.\nAudio, Speech, and Language Processing , 16(5):1015â€“1028,\n2008.\n[6] M. Casey and M. Slaney. Song intersection by approximate\nnearest neighbor search. In Proc. ISMIR , volume 6, pages 144â€“\n149, 2006.\n[7] M. Casey and M. Slaney. Fast recognition of remixed music au-\ndio. In IEEE Int. Conf. Acoustics, Speech and Signal Process-\ning, volume 4, pages IVâ€“1425â€“IVâ€“1428. IEEE, April 2007.\n[8] M. S. Charikar. Similarity estimation techniques from round-\ning algorithms. In Proc. ACM Symp. Theory of Computing ,\npages 380â€“388. ACM, 2002.\n[9] N. Cho, Y . Shiu, and C. C. J. Kuo. Efï¬cient music represen-\ntation with content adaptive dictionaries. In Proc. IEEE Int.\nSymp. Circuits and Systems , pages 3254â€“3257, 2008.\n[10] C. Cotton and D.P.W. Ellis. Finding similar acoustic events us-\ning matching pursuit and locality-sensitive hashing. In IEEE\nWorkshop Appl. Signal Proc. Audio and Acoustics , pages 125â€“\n128. IEEE, 2009.\n[11] M. Datar, N. Immorlica, P. Indyk, and V .S. Mirrokni. Locality-\nsensitive hashing scheme based on p-stable distributions. In\nProc. 20th Annual Symposium on Computational Geometry ,\npages 253â€“262, 2004.\n[12] D.L. Donoho and J. Tanner. Thresholds for the recovery of\nsparse solutions via l1 minimization. In 40th Annual Conf. In-\nformation Sciences and Systems , pages 202â€“206, March 2006.\n[13] Lawrence Fritts. Musical instrument samples, 1997â€“.\n[14] A. Gionis, P. Indyk, and R. Motwani. Similarity search in high\ndimensions via hashing. In Proc. Int. Conf. Very Large Data\nBases , pages 518â€“529, 1999.\n261Poster Session 2\n2 4 6 8 10\nNumber of Overlapping Spectra0.00.20.40.60.81.0Recall\n2 4 6 8 10\nNumber of Overlapping Spectra0.00.20.40.60.81.0Precision\n2 4 6 8 10\nNumber of Overlapping Spectra0.00.20.40.60.81.0F-Measure\n2 4 6 8 10\nNumber of Overlapping Spectra0.000.050.100.150.200.250.300.350.400.45Execution Time (Sec.)\n2 4 6 8 10\nNumber of Overlapping Spectra020000400006000080000100000120000140000No. of Inner Products\nOMP\nAMP (L=9, k=10)\nAMP (L=9, k=12)\nAMP (L=11, k=12)\nAMP (L=15, k=12)Figure 2 . Recall, precision, F-measure, execution time, and number of inner products computed for OMP and AMP after\ndecomposing an input spectrum containing overlapping sounds. Dictionary contains 17,753 spectra of piano sounds. All\nquantities are averaged over twenty trials.\n[15] R. Gribonval and E. Bacry. Harmonic decomposition of audio\nsignals with matching pursuit. IEEE Trans. Signal Processing ,\n51(1):101â€“111, 2003.\n[16] Eric Jones, Travis Oliphant, Pearu Peterson, et al. Scipy: Open\nsource scientiï¬c tools for python, 2001â€“.\n[17] D. D. Lee and H. S. Seung. Learning the parts of objects by\nnon-negative matrix factorization. Nature , 401:788â€“791, 1999.\n[18] S.G. Mallat and Z. Zhang. Matching pursuits with time-\nfrequency dictionaries. IEEE Trans. Signal Processing ,\n41(12):3397â€“3415, 1993.\n[19] Pierre-Antoine Manzagol, Thierry Bertin-Mahieux, and Dou-\nglas Eck. On the use of sparse time-relative auditory codes for\nmusic. In Proc. Intl. Soc. Music Information Retrieval Conf. ,\npages 603â€“608, 2008.\n[20] Y .C. Pati, R. Rezaiifar, and P. S. Krishnaprasad. Orthogonal\nmatching pursuit: Recursive function approximation with ap-\nplications to wavelet decomposition. In Asilomar Conf. Sig-\nnals, Systems and Computers , pages 40â€“44, 1993.\n[21] M. Ryyn Â¨anen and A. Klapuri. Query by humming of midi\nand audio using locality sensitive hashing. In IEEE Int. Conf.\nAcoustics, Speech and Signal Processing , pages 2249â€“2252,\n2008.\n[22] Malcolm Slaney and Michael Casey. Locality-sensitive hash-\ning for ï¬nding nearest neighbors. IEEE Signal Processing\nMag. , 25(2):128â€“131, March 2008.\n[23] P. Smaragdis and J. C. Brown. Non-negative matrix factoriza-\ntion for polyphonic music transcription. In Proc. IEEE Work-\nshop on Appl. Signal Processing to Audio and Acoustics , pages\n177â€“180, New Paltz, NY , October 2003.[24] E. Smith and M. S. Lewicki. Efï¬cient coding of time-relative\nstructure using spikes. Neural Computation , 17(1):19â€“45,\n2005.\n[25] Steven K. Tjoa, Matthew C. Stamm, W. Sabrina Lin, and\nK. J. Ray Liu. Harmonic variable-size dictionary learning for\nmusic source separation. In Proc. IEEE Int. Conf. Acoustics,\nSpeech, and Signal Processing , pages 413â€“416, Dallas, TX,\nMarch 2010.\n[26] Tuomas Virtanen. Monaural sound source separation by non-\nnegative matrix factorization with temporal continuity and\nsparseness criteria. IEEE Trans. Audio, Speech, and Language\nProcessing , 15(3):1066â€“1074, March 2007.\n[27] R. Weber, H.J. Schek, and S. Blott. A quantitative analysis\nand performance study for similarity-search methods in high-\ndimensional spaces. In Proc. Int. Conf. Very Large Data Bases ,\npages 194â€“205, 1998.\n[28] Y . Yu, M. Crucianu, V . Oria, and E. Damiani. Combining\nmulti-probe histogram and order-statistics based lsh for scal-\nable audio content retrieval. In ACM Int. Conf. Multimedia ,\npages 381â€“390. ACM, 2010.\n262"
    },
    {
        "title": "Elementary Sources: Latent Component Analysis for Music Composition.",
        "author": [
            "Spencer S. Topel",
            "Michael A. Casey"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1417351",
        "url": "https://doi.org/10.5281/zenodo.1417351",
        "ee": "https://zenodo.org/records/1417351/files/TopelC11.pdf",
        "abstract": "Complexity of music audio signals creates an access problem to specific musical objects or structures within the source samples. Instead of employing more commonly used audio analysis or production techniques to access features, we describe extraction of sub-mixtures from real-world audio using a Probabilistic Latent Component Analysis-based decomposition tool for music composition. This is highlighted with the presentation of a prior relevant compositional approach named Spectral Music along with a discussion of five compositions extending these principles using methods more commonly associated with source separation research.",
        "zenodo_id": 1417351,
        "dblp_key": "conf/ismir/TopelC11",
        "keywords": [
            "audio analysis",
            "music composition",
            "Probabilistic Latent Component Analysis",
            "sub-mixtures",
            "source separation research",
            "Spectral Music",
            "composition principles",
            "source samples",
            "access problem",
            "specific musical objects"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nELEMENTARYSOURCES: LATENT COMPONENTANALYSIS FOR MUSIC\nCOMPOSITION\nSpencerS.Topel MichaelA.Casey\nBregman Music Audio ResearchStudio\nDartmouth College\nABSTRACT\nComplexity of music audio signals creates an access prob-\nlemtospeciï¬cmusicalobjectsorstructureswithinthesour ce\nsamples. Instead of employing more commonly used au-\ndioanalysisorproduction techniques toaccess features, w e\ndescribe extraction of sub-mixtures from real-world audio\nusing a Probabilistic Latent Component Analysis-based de-\ncompositiontoolformusiccomposition. Thisishighlighte d\nwith the presentation of a prior relevant compositional ap-\nproach named Spectral Music along with a discussion of\nï¬ve compositions extending these principles using methods\nmorecommonlyassociatedwithsourceseparationresearch.\n1. INTRODUCTION\nMusicrecordingsofalltypesconsistofmixtures;therecor ded\nsourcesaretransformedviarealorvirtualacousticproces ses\nand these are summed to make a stereo or mono track. A\nmajorchallengefacingmachineanalysisofaudioisextract -\ning information contained in mixtures for the purpose of\nisolating relevant content. Techniques based on indepen-\ndent component analysis (ICA), such as independent sub-\nspace analysis (ISA) [8] and Probabilistic Latent Compo-\nnentAnalysis(PLCA) [20],providewaysofaccessingper-\nceptuallymotivatedmusicalobjects[17],whichwedescrib e\nhere as â€œsub-mixturesâ€.\nFor compositions based upon music audio, either for in-\nformation or analysis, accessing latent features expands t he\ncreative possibilities. Speciï¬c technical innovations co n-\ntribute to this perspective: fast Fourier transform (FFT) s ig-\nnalanalysisallowedcomposersassociatedwith SpectralMu-\nsicto explore spectral proï¬les for the purpose of generating\nmaterialforpieces[10]. PLCAlikewiseextendstheconcept\nof exploring audio content for music composition by pro-\nviding the means necessary to extracting components with\nPermission to make digital or hard copies of all or part of this w ork for\npersonal or classroom use is granted without fee provided th at copies are\nnot made or distributed for proï¬t or commercial advantage and th at copies\nbear this noticeand thefull citation ontheï¬rst page.\nc/circlecopyrt2011International Society forMusic InformationRetrieval .independent properties, or what we describe here as sub-\nmixture content, i.e. distinctive streams of sound objects\ngrouped by their common frequency-amplitude statistics.\nWepresentarepertoireofworksthatusesuchtechniques.\nPieces were included based on their relevance to the tech-\nniques presented in this paper, and their speciï¬c aesthetic\ninsights or innovations with respect to component manipu-\nlation. Precedingthis,adiscussionofthehistoricallysi gnif-\nicantSpectral Music composition movement is presented,\nwhere spectrogram analysis and data-mining generated ma-\nterial fornew musical works.\n2. BACKGROUND\nAudio decomposition methods in computer-assisted music\ncomposition have a rich history in musical discourse over\nthepasttwentyyears,notablyWishartâ€™sexpansionofPierr e\nSchaefferâ€™s MusicObjets [22],andSmalleyâ€™sSpectro-mor-\nphology [19]. One approach of particular relevance, for-\nmallynamed MusiqueSpectrale ,usesextractedtimbralfea-\ntures for the purpose of creating new works, both acoustic\nand electro-acoustic in nature. Spectral Music was ï¬rst in-\ntroduced by Henry Dufourt in 1979 [10], however by this\ntime compositions using materials extracted from Fourier\ntransforms had already been written by the group, most no-\ntably GÂ´erard Griseyâ€™s â€œPartiels Pour 18 Musicienseâ€ [12],\nwhere the composer proposed macro-synthesis of analyzed\nsourcesusingcombinationsofacousticmusicalinstrument s.\nIn most of these early pieces, there was a transparent and\nstraightforwardprocessbywhichthecomposerderivedma-\nterialsfor new works, outlined inFigure 1.\nA wellspring of software development at the Institut de\nRecherche et Coordination Acoustique/Musique (IRCAM)\noccurredduringthistime,includingthevisualprogrammin g\nenvironmentMax,afterMaxMatthews,alongwithmanyof\nthewell-knownIRCAMpackagemodulesincludingâ€œMusic\nVâ€ brought to IRCAM by Jean-Claude Risset, who came to\nfurther timbre research, as well as CHANT, developed by\nXavier Rodet, and the transcription tools that now belong\ntoOpen Music . Amongst the composers belonging to the\nï¬rst-waveofSpectralcompositionwereG Â´erardGrisey,Tris-\ntanMurail,HuguesDufourt,andBritishcomposerJonathan\n579Poster Session 4\nHarvey. Their music inï¬‚uenced a younger cadre of com-\nposers, including Magnus Lindberg, Marc-Andre Dalbavie\nand Joshua Fineberg.\nThe purpose of this shift in compositional thought pos-\nsesses deeper connections to the European Zeitgeist of the\n1960â€™s and1970â€™s. The prevailing school of thought be-\ning â€œTotal Serialismâ€ was the only credible way to be a re-\nspected contemporary composer, built upon ideas ï¬rst pio-\nneeredbytheSecondVienneseSchool,consistingofArnold\nSchoenberg, and his disciples Anton Webern, and Alban\nBerg [11]. The Spectralists realized the aesthetics associ -\nated with mainstream Serialism of the time disregarded the\nï¬nal sounded musical experience. Instead, these technique s\nfavored abstraction in notation and formalism blinded, to a\ncertain extent, by the ideas that all 12-tone (half-step rel a-\ntionships) were equal, and that non-western tuning systems\nwere not relevant tomainstream Westernâ€œartâ€ music.\nThe Spectralists thus considered observation of sounded\nacoustic sources to be the startingpoint of their work. They\nfurther rejected both ideas that 12-tones were equal. Their\nearlyrepertoireemphasizedthesepoints,withpiecessuch as\nâ€œGodwanaâ€ [15], that evokes a sense of non-western tuning\nbyexploringtherelationshipbetweenasyntheticbellsour ce\nand a spectrally analyzed trombone sample and later with\nâ€œDÂ´esintÂ´egrationsâ€ [16] that utilizes the careful blending of\ntimbres between electronic sounds and acoustic sources.\n3. REVEALING LATENT STRUCTURE IN AUDIO\nAside from socio-political currents in their work, the Spec -\ntralistsâ€™ perspective emphasized uncovering hidden infor -\nmation in sound to generate new musical ideas. In the same\nway,Latentstructurereferstoidentifyingdistinctiveorsalient\nparts of recorded audio that otherwise remain hidden. For\nSpectralists thismeantidentifyingstructuralpartialsthatdis-\ntinguished one instrument from another playing the same\nperceivednote(e.g. anâ€Aâ€orâ€Bbâ€),asthesepartialsrelat e\ntosome organization ofharmonic material.\nIndependentSubspaceAnalysisextractiontechniquesof-\nferanotherwaytoaccessstructureinaudio,sincere-synth esized\nlatent components retain correlated behaviors between fre -\nquencyandamplitudeinformationineachcomponent. When\nPLCAisusedonmagnitude-onlySTFTrepresentations,the\nextractedcomponentshavesimilarcharacteristicstotheo ut-\nputofphasevocodermethodswithanimportantdistinction:\nin addition to spectrum and envelope decompositions, com-\nponents are further segmented by statistical independence\nof information content or recurrence of embedded acoustic\npatterns inthesound.\n3.1 Probabilistic Latent Component Analysis\nPLCAisageneralizationofNon-negativeMatrixFactoriza-\ntion(NMF)andamulti-variategeneralizationofHoffmanâ€™sAudio Data\nFourier-Based Transform (FFT,STFT)\nThreshold Salient Frequencies (dBCutoff)\ntranscribe or re-synthesize (Open Music)\nFigure 1. Spectralist composers used these techniques to\ngenerate novel harmonic material. The ï¬rst step would be\nto perform a Fourier Transform, that either extracted only\nfrequency information (FFT), or preserved temporal reso-\nlution of the frequencies (STFT). Then by using the result-\ning frequency information, the composers built chords with\nspeciï¬cdynamicsandorchestrationtore-createthespectr al\nproï¬les of thesource materials.\nbi-variate Probabilistic Latent Semantic Analysis (PLSA)\n[13][18][20]. Forthepurposesofmodelingatime-frequenc y\ndistribution such as the magnitude STFT, the PLCA model\nhas thefollowing form:\nP(x) =/summationdisplay\nzP(z)P(w|z)P(h|z) (1)\nwhereP(x)is the2-dimensional distribution of the ran-\ndom variable x=wh.zis a latent variable which we inter-\npret to be an additive spectrogram component of x. These\nmarginals, wandh, are frequency components and ampli-\ntude components, respectively, of independent latent mag-\nnitudespectrograms. Themarginaldistributionsthemselv es\nare dependent on a latent variable z. The objective of this\nanalysisistoï¬ndouttheunderlyingstructureofaprobabil -\nity distribution. This is done by estimating P(w|z),P(h|z)\nandP(z)from an observed P(x)using a version of the\nExpectation Maximization (EM) algorithm [9]. Following\n[18], the expectation step estimates the contribution of th e\nlatent variable z:\nR(x,z) =P(z)P(w|z)P(h|z)/summationtext\nzâ€²P(zâ€²)P(w|zâ€²)P(h|zâ€²)(2)\nandinamaximizationstepwere-estimatethemarginalsus-\ning the above weighting to obtain a new and more accurate\n58012th International Society for Music Information Retrieval Conference (ISMIR 2011)\nestimate:\nP(z) =/integraldisplay\nP(x)R(x,z)dx (3)\nwith\nP(w|z) =/integraltext\nP(x)R(x,z)dh\nP(z)(4)\nand\nP(h|z) =/integraltext\nP(x)R(x,z)dw\nP(z). (5)\nRepeatingthesestepsconvergestoasolutionforthemargin als\nandthelatentvariablepriors. Figure3illustratesthedec om-\nposition of magnitude STFT time-frequency distributions\nintoprobabilisticlatentcomponentsusingthis2-dimensi onal\nmarginal decomposition algorithm. By considering the dis-\ntributions as signal matrices, the ï¬nal form of the matrix\nfactorization using PLCA is:\nX=WZHT. (6)\nWhereWis the spectral distribution and His the tempo-\nral distribution, the product of which produces the spectro -\ngramreconstructionmatrix X. Aprior, Zisintroducedthat\nweights the relative contribution of each component within\nthespectrogramreconstructionmatrix. Forbasicprocedur es\naimedatgeneratingcompositionmaterial,the Zpriorcanbe\nomittedsincewedonotneedtopreservetherelativecontri-\nbution(orloudness)ofeachcomponenttotheoriginalmix-\nturewhenre-composingwiththem. Withthebasicmechan-\nicsofIndependentComponentAnalysismethodsdescribed,\nwe now examine applications of these algorithms for music\ncomposition.\n4. SOUNDSPLITTER: COMPONENT-WISE\nRE-SYNTHESIS\nThecompositionsdescribedbelowusedaMatlabtoolcalled\nSoundSplitter. Sounds were loaded from 44.1kHz sample-\nrate 16-bit WAV format and analyzed using the short-time\nFourier transform (STFT), yielding a sequence of vectors,\nX, with 4096 samples per frame and an overlap of 2048\nsamples. For each frame, only the ï¬rst 2049 magnitude\nFourier coefï¬cients were retained to eliminate redundancy\nduetothesymmetryoftheFouriertransformforreal-valued\nsignals. Optionally, the sequence of analysis frames was\ndivided into segments using ï¬xed-length blocks of STFT\nframes, with block-length typically between 1s and 10s in\nduration. Each block was analyzed using the PLCA2D al-\ngorithm[20]toyieldthreematricesperblockcorrespondin g\nto the frequency marginals, amplitude (probability) coefï¬ -\ncients, and time marginals respectively: W,ZandH. The\nnumber of columns in these matrices corresponded to the\nnumber of components, n, requested inthe analysis.\nComponent-wisere-synthesisproducedamagnitudespec-\ntrogramforeachmarginalcomponent, k,usingthere-synthesisequation Xk=WkZk,kHT\nkfor component spectrogram\nXk,acolumnvectorforthe kthfrequencymarginal, Wk,an\namplitudescalarfromthe kthdiagonalentryin Z,Zk,k,and\nthe transposed kth column vector from the time marginals,\nHT\nk. The compnent spectra, Ë†Xk, were re-synthesized by\nsymmetrically expanding the magnitude spectrum around\nthe Nyquist frequency and multiplying by the complex ex-\nponenentiated phase argument from the STFT of the source\nsignalXâ‹†, such that Ë†Xk=Xkejarg(Xâ‹†)forkâˆˆ {1...n}\nwhere the sum of the marginals forms the identity: Xâ‹†=\n(/summationtextn\ni=1Xi)ejarg(Xâ‹†). Thek-thcomponentsignalwascom-\nputed using overlap-add re-synthesis, via the inverse shor t-\ntimeFouriertransformof Ë†Xk,withoverlapcorrespondingto\nthehopsizeusedintheanalysisstepandeachwindowmul-\ntiplied by a raised cosine window to smooth the transition\nbetween adjacent frames. The component-wise audio sig-\nnals were controlled independently by the composers using\ndigitalaudioworkstationsoftwaretoyieldthecompositio ns\ndescribed below.\n5. LATENT COMPONENT ANALYSIS FOR MUSIC\nCOMPOSITION\nThePLCA2DalgorithmusedinthecurrentversionofSound-\nSplitterhastheattributeofextracting ï¬xedre-occurringpat-\nterns in frequency and amplitude. These components retain\na qualitatively higher level of structure of the original au -\ndio than non-pattern extraction methods, (e.g. bandpass ï¬l -\ntering, spectral frequency decomposition, or phase-vocod er\ndecomposition). Thefollowingsectiondiscussesï¬veworks\nemploying these techniques in the order of their ï¬rst per-\nformances. Each work manipulates components differently,\nbut they share acommon traitof usingcomponents toartic-\nulatespeciï¬candrecognizablecharacteristicsoftheorig inal\naudio samples at speciï¬c moments.\n5.1Strange-Charmed (1999), byMichael Casey and\nSimon Atkinson\nStrange-Charmed [7] used independent subspace analysis\n(ISA) of spectrogram data [8] to generate an expanded set\nofsoundmaterialsfromasetoftexturalandgranularsource\nsoundsconsistingofGeigercounters,insects,band-passï¬ l-\ntered water sounds, and scraped metallic objects. In con-\ntrasttoPLCA,theJADEalgorithm[2]forindependentcom-\nponent analysis was used which yields components having\nboth positive and negative values. For time-frequency dis-\ntributions with negative values, such as non-rectiï¬ed ï¬lte r-\nbankoutputs,theISAmethodiswellformed,butitisinefï¬-\ncient due to the vast quantity of data generated by the ï¬lter-\nbank. For real-time use the magnitude Fourier transform\nwas employed, and any negative values in the magnitude\nFouriertransformreconstructionhadtobetruncatedtozer o\nfor re-synthesis. A custom real-time synthesizer software\n581Poster Session 4\nFigure 2. Components were introduced one at a time, with the misalign ed components gradually aligned to bring the het-\nerophony of asynchronous components into a state of order, c learly revealing the source as Stravinskyâ€™s iconic Rite of S pring\nchord.\nFigure 3. Each horizontal row corresponds to an extracted component , where the left-hand side of the ï¬gure shows the\ntranscriptions to music notation, and the right-side shows the plots of the WandHdecompositions per-component. From\ntop to bottom: 1)A transient-laden component over the relat ive duration of a whole-note, where the peaks represent loud\narticulationsandthetroughsareequivalenttopianissimo . 2)Acleanbell-partial,3)â€œwobblingâ€ofthebell-halvese ttlingonthe\nconcrete. 4) Cement â€œclickâ€, articulated by anear-pitchle ss pizzicato near the bridge ofthe instrument.\n58212th International Society for Music Information Retrieval Conference (ISMIR 2011)\npackage was designed and implemented that used MIDI to\ncontrol the balance of amplitudes, and the relative delay, o f\neach component of each sound independently. The real-\ntime system, described in detail in [6], was used to shape\nthe sounds and the output recorded for off-line editing and\nlayered into the ï¬nal composition using digital audio work-\nstationsoftware.\n5.2Stratovinksy (2010), byPaul Osentinsky\nThe concept behind Stratovinksy is the gradual revealing of\nanimportantmusicalinstance. Namely,theiconic,jaggedl y\nrepeating, chord from the â€œOmens of Spring: Dances of the\nYouths and Maidensâ€ movement of Igor Stravinskyâ€™s â€œRite\nof Springâ€ [21] [14]. Using SoundSplitter, 128components\nwere extracted from 2seconds of audio, re-synthesized to\naudio and then normalized. The resulting characteristics o f\nthese components were micro-tonal with low-energy levels\ndistributedacross the many extracted components.\nThe entirety of the data was then imported into Ableton\nLiveasWAVï¬les,temporallymis-aligned,looped,anddis-\ntributed across sixteen virtual channels; see Figure 2. The\nlayeredcomponentsgraduallycameintoalignmentoverthe\ncourseofseveralminutes,withtheeffectofthisprocessbe -\ning equivalent to seeing a blurred object slowly come into\nfocus.\n5.3Decomposing Autumn (2010), byDavid Plans Casal\nDecomposing Autumn [3] was a live performance utilizing\ncomponent-wise decomposition and live improvised recon-\nstructionof AutumninWarsaw ,thesixthinGy Â¨orgyLigetiâ€™s\n2nd book of piano etudes. The latent component analysis\nmethodiscoupledwithreal-timecomponent-wiseaudiore-\ntrieval using SoundSpotter [4] as a foundation for a struc-\ntured improvisation. Here, the PLCA2D algorithm was em-\nployed as a decomposition technique to obtain a corpus of\nseparated sound fragments, which were then queried by a\nlive improviser performing on a custom-built acoustic gui-\ntar. Theapproachusedinthiscompositioneffectivelybrid ges\nthe ï¬elds of latent component analysis, music information\nretrieval by audio matching, and composition.\n5.4Violine(2011), bySpencer Topel\nA work with a similar aim as Stratovinksy , at highlighting\nandrevealingmusicalstructureis Violine,forsoloviolinand\nlaptop. Thesourcematerialforeachmovementconsistedof\na short12âˆ’90second audio clip of J.S. Bachâ€™s Chaconne\nin d minor from Partita No. 2 for solo violin. Doing so\npreserved not only the composed structure explicit in the\nJ.S. Bachâ€™s notation, but also the timings and articulation s\nsupplied by the performer, (e.g. rolling of chords, chord\nvoicing, and rubato).The approach in this composition was two-fold: ï¬rst to\nuseSoundSplittertoperformadecompositiontoisolatedin -\ndividualnotesorpitch-classes,andthenemploySoundSpot -\ntertomatchaliveviolinsignaldirectlyonaudiofeaturesa n-\nalyzed on the extracted component database [5]. The basic\nfunctionofSoundSpotterallowsformatchingbetweenpitch\nand timbre characteristics, both of which were utilized in\nVioline. Combined together, SoundSplitter and SoundSpot-\nter, the compositional material becomes a combination of\nthe composers intentions and the performers interpretatio n,\nsimilartothepieces discussed in[4].\nThecomponentdecompositionparameterwasagaincrit-\nical in the pre-composition phase of this piece. An eight\ncomponent-wisedecompositionprovedtobeeffectiveatex-\ntracting clear, well-formed sounding components. A musi-\ncalscorewasthenwrittenusingtheanalysisprovidedbythe\nSoundSplitterdecomposition. ArecentversionofReal-tim e\nSoundSpotterasaVSTplug-in,providedanimmediateand\ninteractive way to match sounds of the live violin, resultin g\ninanear-seamlesscounterpointbetweentheextractedcom-\nponents and thecomposition.\n5.5Elementary Sources (2011), bySpencer Topel\nElementarySources examinesSoundSplittingasameansof\ndecomposing audio into different timbral objects that con-\ntribute the identity of the original source. The movement\ndiscussedhere,ofï¬vemovementsforstringquartetandlap-\ntop, was written using a single 101second audio recording\nof brass bell-halves Case dropped on concrete and recorded\nbyartistCaseHathaway-Zepeda. SoundSplitterwasusedto\nextract components relating to different events segregate d\nby timbre, which included cement â€œclicksâ€, resulting from\nthe moment the metal hit the ground, in-harmonic partials\nfrom the metals as the bell-halves rang, and the oscillation ,\nor wobbling, of the halves as they came to rest on the ce-\nment. Different component extraction parameters were ex-\nploredandcomponentswerere-synthesizedandauditioned.\nThrough trial and error, eight components were identiï¬ed\nas the yielding the best sounding results. Additional exper -\nimentation with component re-synthesis included creating\ncomponents that had cross-spectral characteristics with t he\nthree categories described above. Speciï¬cally, spectral s ig-\nnatures and time-trajectories from SoundSplitter were re-\ncombined in different ways to extend the timbral palette\nwithout extracting new audio sources.\nA prevailing idea in Elementary Sources was to acousti-\ncallysynthesizeaspeciï¬caudiosample,likethebell-halv es,\nwith an entirely different set of sources, such as a string\nquartet. ThisisnotunlikeG Â´erardGriseyâ€™sideasforhisland-\nmark work Partiels, where he describes the concept of us-\ningtheorchestraforthepurposeofMacro-synthesis, where\neach instrument of the orchestra contributes speciï¬c time\nand frequency behaviors that culminate in an overall syn-\n583Poster Session 4\nthesis of a spectral proï¬le, rather than distinctive sectio ns\nand instrumental motives [12].\nTwo methods were explored to determine how best to\nhave the four acoustic instruments perform the different ex -\ntracted components. Firstly, a music notation relating to\neach distinctive component was devised to best articulate\nthetime-frequencybehaviors,showninFigure3. Secondly,\naudio samples were provided for the performers to audition\nthesoundsforthemselvestobestdeterminetheexecutionof\nthese components. The two methods proved to work better\nincombinationthaninisolation,sincethenotationprovid ed\na starting-point, and the playback of components were of\nhighenoughqualityastoprovideadditionalinformationfo r\neach performer that could not be captured in the music no-\ntation.\nWith basic elements of the bell-halves translated to the\nquartet, it was now possible to use a combination of live-\nelectronics(e.g. pitch-shifting,reverberation,compre ssion),\nand re-synthesized component sample playback to achieve\na fairly close relationship between the sampled sources and\nthe acoustic instruments. The result was the creation of\nan interstitial space between bell samples and their tran-\nscriptions, where relationships in the compositional mate -\nrials were a subsequent outgrowth of the timbres from the\noriginal bell-halve sources.\n6. SUMMARY\nAnewapproachtomusiccompositionusingLatentCompo-\nnent Analysis techniques is described, along with ï¬ve com-\npositions and accompanying examples that demonstrate the\nusages of these techniques, which are accessible online [1] .\nWealsoshowherethatSpectralismoverlapswiththesecon-\ncepts, since a shared theme in both repertoires is the ap-\nplication of computer analysis to uncover latent features i n\nmusic audio. Future work will examine how component es-\ntimationmoredeeplyinï¬‚uencescompositionalmaterialand\nhowrecentinnovationsonPLCAandrelatedalgorithmscan\nbe used for better extraction of compositionally relevant i n-\nformation.\n7. REFERENCES\n[1] http://digitalmusics.dartmouth.edu/ismir2011, Sep tem-\nber 2011.\n[2] Jean-Franc Â¸ois Cardoso. Source separation using highe r\norder moments. In Proc. ICASSP-89 , pages 2109â€“2112,\n1989.\n[3] D.P. Casal and M. Casey. Decomposing autumn: A\ncomponent-wise recomposition. In Proc. ICMC , 2010.\n[4] M. Casey. Soundspotting: A new kind of process. The\nOxfordHandbook of Computer Music , 2009.[5] M.A. Casey. http://soundspotter.org/.\n[6] M.A. Casey. Auditory Group Theory with Applications\ntoStatisticalBasisMethodsforStructuredAudio,Ph.D.\nThesis. MITMedia Laboratory, 1998.\n[7] M.A. Casey and S. Atkinson. Strange-Charmed: MIT\nEMS@25 CD, Track 9 . MITMedia Laboratory, 1999.\n[8] M.A. Casey and A. Westner. Separation of mixed audio\nsources by independent subspace analysis. In Proceed-\nings of the International Computer Music Conference ,\npages 154â€“161, 2000.\n[9] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maxi-\nmum likelihood from incomplete data via the EM algo-\nrithm.J. Royal Stat. Soc. B , 39(1):1â€“38, 1977.\n[10] H Dufourt. Musique spectrale. Musique, pouvoir,\nÂ´ecriture, pages 289â€“290, 1979.\n[11] M.J. Grant. Serial music, serial aesthetics: composi-\ntional theory in post-war Europe , volume 16. Cam-\nbridge Univ Press,2005.\n[12] GÂ´erard Grisey. Partiels Pour 18 Musiciense . Ricordi,\n1975.\n[13] T. Hoffman. Probabilistic latent semantic indexing. I n\nProceedings of the 22nd Annual International ACM SI-\nGIRConferenceonResearchandDevelopmentinInfor-\nmation Retrieval , pages 50â€“57. ACM Press, New York,\n1999.\n[14] L.A. Philharmonic I. Salonen. Stravinsky: Le sacre du\nprintemps, 2006.\n[15] T. Murial. Godwana . Transatlantiques, 1980.\n[16] T. Murial. DÂ´esintÂ´egrations .Â´Editions Salabert, 1989.\n[17] P.Schaeffer.Solfegedelâ€™objetsonore. INA/GRM ,1967.\n[18] Madhusudana Shashanka, Bhiksha Raj, and Paris\nSmaragdis. Probabilistic latent variable models as non-\nnegative factorizations. Technical report, Mitsubishi\nElectricResearch Laboratories, December 2007.\n[19] D. Smalley. Spectromorphology: explaining sound-\nshapes.Organised Sound , 2(2):107â€“126, 1997.\n[20] Paris Smaragdis, Bhiksha Raj, and Madhusudana V. S.\nShashanka. Sparse and shift-invariant feature extraction\nfrom non-negative data. In ICASSP, pages 2069â€“2072.\nIEEE, 2008.\n[21] I.Stravinsky. Riteof Spring . Dover Publications, 1989.\n[22] T. Wishart and S. Emmerson. On sonic art. 12, Rout-\nledge, 1996.\n584"
    },
    {
        "title": "Tempo Estimation Based on Linear Prediction and Perceptual Modelling.",
        "author": [
            "Georgina Tryfou",
            "Aki HÃ¤rmÃ¤",
            "Athanasios Mouchtaris"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1416386",
        "url": "https://doi.org/10.5281/zenodo.1416386",
        "ee": "https://zenodo.org/records/1416386/files/TryfouHM11.pdf",
        "abstract": "Many applications demand the automatic induction of the tempo of a musical excerpt. The tempo estimation systems follow a general scheme that consists of two main steps: the creation of a feature list and the detection of periodicities on this list. In this study, we propose a new method for the implementation of the first step, along with the addition of a final step that will enhance the tempo estimation procedure. The proposed method for the extraction of the feature list is based on Gammatone subspace analysis and Linear Prediction Error Filters (LPEFs). As a final step on the system, the application of a model that approximates the tempo perception by human listeners is proposed. The results of the evaluation indicate the proposed method compares favourably with other, state-of-the-art tempo estimation methods, using only one frame of the musical experts when most of the literature methods demand the processing of the whole piece.",
        "zenodo_id": 1416386,
        "dblp_key": "conf/ismir/TryfouHM11",
        "keywords": [
            "tempo estimation",
            "automatic induction",
            "musical excerpt",
            "feature list",
            "periodicities",
            "Gammatone subspace analysis",
            "Linear Prediction Error Filters (LPEFs)",
            "tempo perception",
            "human listeners",
            "evaluation"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nTEMPO ESTIMATION BASED ON\nLINEAR PREDICTION AND PERCEPTUAL MODELLING\nGeorgina Tryfou1,2, Aki H Â¨armÂ¨a3, Athanasios Mouchtaris1,2\n1Institute of Computer Science, Foundation for Research and Technology - Hellas\n(FORTH-ICS), Heraklion, Crete, Greece\n2Department of Computer Science, University of Crete, Heraklion, Crete, Greece\n3Philips Research, Eindhoven, The Netherlands\ntryfou@ics.forth.gr, aki.harma@philips.com, mouchtar@ics.forth.gr\nABSTRACT\nMany applications demand the automatic induction of the\ntempo of a musical excerpt. The tempo estimation systems\nfollow a general scheme that consists of two main steps: the\ncreation of a feature list and the detection of periodicities\non this list. In this study, we propose a new method for the\nimplementation of the ï¬rst step, along with the addition of a\nï¬nal step that will enhance the tempo estimation procedure.\nThe proposed method for the extraction of the feature list is\nbased on Gammatone subspace analysis and Linear Predic-\ntion Error Filters (LPEFs). As a ï¬nal step on the system, the\napplication of a model that approximates the tempo percep-\ntion by human listeners is proposed. The results of the eval-\nuation indicate the proposed method compares favourably\nwith other, state-of-the-art tempo estimation methods, using\nonly one frame of the musical experts when most of the lit-\nerature methods demand the processing of the whole piece.\n1. INTRODUCTION\nThe tempo is a dominant element connected to the hierarchi-\ncal structure of a music signal that can deï¬ne various aspects\nof it. Moreover, it is an intuitive music property that hu-\nman listeners, even without any musical education are able\nto perceive and understand only by listening to the ï¬rst few\nseconds of an excerpt. The tempo is deï¬ned as the rate of\nthetactus pulse, a prominent level in the hierarchical struc-\nture of music, which is also referred to as the foot-tapping\nrate.\nThe process of automatically inferring the tempo of a\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proï¬t or commercial advantage and that copies\nbear this notice and the full citation on the ï¬rst page.\nc/circlecopyrt2011 International Society for Music Information Retrieval.musical piece plays an important role among the applica-\ntions in the ï¬eld of Music Information Retrieval (MIR). Many\nof them, for example, beat tracking and music classiï¬ca-\ntion, need a preprocessing stage where tempo estimation\ntakes place. Beyond these, tempo induction is essential in\nmusic similarity and recommendation, automatic transcrip-\ntion and even audio editing. More complicated tasks such\nas meter extraction and rhythm description also demand a\ntempo estimation module. Finally, in applications with beat\nsynchronous visual and audio effects the estimation of the\ntempo is a necessary part.\nIn such applications it is desired that correct tempo esti-\nmation would be available to the system at about the same\ntime that the tempo is detected by a human listener. This\nis technically very difï¬cult because the human listeners are\nable to use higher-level context cues to conduct tempo de-\ntection. In fact, many algorithms proposed for tempo es-\ntimation in the past [7, 11] require a long signal segment\nfor producing reliable results. This is clearly a problem in\ncontents such as radio programs, where the rhythmic music\ncontent may alternate with, for example, speech segments\nTempo induction algorithms follow a general scheme [4,\n5], that consists of two main stages. In the ï¬rst stage, the\naudio signal is parsed and a set of features is created. These\nfeatures convey an initial rhythmic structure of the input mu-\nsical piece. Literature reveals two main methods to obtain\nfeatures: either from a list of the inter onset intervals (IOIs)\nof the musical signal or from the temporal evolution of the\nmusical signal.\nRepresentative algorithms that fall in the ï¬rst category,\nand use IOIs for the creation of the feature list, are presented\nin [1â€“3]. Algorithms in the second category rely on features\nextracted directly from the audio signal. These features may\nemphasize onset locations but they do not result from on-\nset lists. In [11] an amplitude envelope of the signal in six\noctave-spaced subbands is created at the ï¬rst stage of the\nsystem. This approach is expanded in [7], where a more\n197Poster Session 2\ngeneric and therefore robust accent signal is created across\nfour subbands.\nDuring the second stage of the tempo estimation scheme,\nperiodic recurrences of the features are found and the tempo\nis calculated. There are several methods to achieve this. For\nexample, the autocorrelation function (ACF) [12], comb-\nï¬lter resonators [7, 11] and phase-locking resonators [8].\nThe method proposed in this paper uses Gammatone anal-\nysis and linear prediction for extracting the feature list. Af-\nter that, the estimation of the existing periodicities takes\nplace. As a last step to the algorithm, a perceptual weighting\nmethod is proposed for enhancing the systemâ€™s accuracy.\nThe results of the system are encouraging and indicate\nthat the addition of a perceptually inspired stage at the end\nis advantageous for an algorithm that follows the tempo es-\ntimation general architecture. In addition to that, the use of\na single, 4 seconds long, frame to obtain the ï¬nal results, fa-\ncilitates the proposed algorithm to quickly adapt to possible\ntempo changes in a given music excerpt.\nThe Gammatone ï¬lterbank models the input signal using\na frequency resolution which is similar to that of the hu-\nman auditory system. Moreover, the use of LPEFs in the\nï¬rst step, enables the accentuation of points where abrupt\nchanges take place in any frequency band of the input sig-\nnal. These points in time are considered signiï¬cant for the\ntask of tempo estimation.\nThe perceptual processing, that starts with the applica-\ntion of the Gammatone ï¬lterbank on the input signal, pro-\nceeds with the weighting method that is added as a last step\non the system. This weighting method is based on a reso-\nnance model that has been found to follow the perceptual\nresponses to a variety of musical excerpts [10, 13]. The use\nof this model in order to enhance a tempo estimation system\nis novel and leads to promising results.\nThe rest of the paper is organised as follows. In Section\n2 the architecture of the system is described. Section 3 pro-\nvides results and evaluates the developed system. Finally,\nconclusions and future work are discussed in Section 4.\n2. METHOD DESCRIPTION\nThe developed system follows the general scheme of tempo\nestimation algorithms, with the addition of a last step where\nthe perceptual processing of the results takes place. The\nblock diagram of the system is shown in Figure 1. Each one\nof the three major units depicted there, is described in details\nin the following sections.\n2.1 Feature List Extraction\nWhen a listener listens to music, the musical events are re-\nlated to a regular pattern of beats, called metrical structure .\nThese patterns are organised in a metrical hierarchy that ex-\nists in every musical sound and consists of two or more lev-\nFeature List \nExtraction Tempo  \nInduction Perceptual \nModel  \nfeature \nlist  \nwinning \ntempi estimated \ntempo input \nsignal Figure 1 . The block diagram of the proposed system.\nBeats 123412341\ntatum\ntactus\nmeasure\nFigure 2 . The hierarchical structure of a piece with a 4/4\nmeter\nels. When a beat is felt stronger that the other beats of the\nsame metrical level then it is also a beat at the higher musical\nlevel. This hierarchy is depicted in Figure 2, where also the\nï¬rst three levels of it are presented. The tempo is described\nas the rate of the tactus beat, or based on the above explana-\ntion the rate at which strong beats appear at the tatum level.\nDuring this stage of the analysis the goal is to detect\nevents that are connected to the strong beat of the tatum\nlevel. To achieve this, it is assumed that any event per-\nceived as a strong beat will appear as an abrupt increase in\nthe temporal evolution of the musical signal, baring signiï¬-\ncantly more transient content than the rest of the beat-related\nevents.\nLet us consider the input music signal x[n]. The ï¬rst step\nof the processing is the application of a bank of KGamma-\ntone ï¬lters on it:\nxk[n] =hk[n]â‹†x[n]kâˆˆ[0,1,...,Kâˆ’1],(1)\nwherehk[n]the impulse response of the k-th Gammatone\nï¬lter. During the implementation the value Kwas chosen\nto be 16.\nAfter the ï¬ltering of the input signal, each subband signal\nis decimated Ktimes as follows\nxk[n] =xk[Kn]. (2)\nThexk[n]signals are then given as an input to a bank of\nadaptive LPEFs. The use of the LPEFs enables the detection\nof abrupt changes in the temporal evolution of the signal. By\nadapting the linear prediction coefï¬cients that these ï¬lters\nuse, it is possible to emphasize the events that the adaptive\nalgorithm fails to model. The strong beats that appear at the\ntatum level are connected to these events.\nThe output of the adaptive LPEFs is the prediction er-\nror of the adaptive linear predictive algorithm given in Al-\ngorithm 1. This algorithm is based on estimating the LPC\n19812th International Society for Music Information Retrieval Conference (ISMIR 2011)\ncoefï¬cients of the initial Mvalues of the Nlong frame, and\nadapting these coefï¬cients using the Least Mean Squares\n(LMS) algorithm for the remaining Nâˆ’Msamples. The\nselected values for MandNare 23 ms and 1 second re-\nspectively (converted in samples). More details on linear\nprediction and the LMS algorithm can be found in [6].\nThe output signal, dfk[n], is the detection function , a\nresidual signal that presents high values when beat related\nevents take place in the temporal evolution of the signal.\nAlgorithm 1 The implemented adaptive LPEF algorithm.\nmuâ†10âˆ’3\nwk[0]â†LPC(xk[0])\nforn= 1toNâˆ’Mdo\nËœxk[n]â†wT\nk[n]â‹†xk[n]\ndfk[n]â†xk[n]âˆ’Ëœxk[n]\nÂµâ†min/parenleftBig\nmu,1\nxT\nk[n]xk[n]/parenrightBig\nwk[n+ 1]â†wk[n] +Âµdfk[n]xk[n]\nnâ†n+ 1\nend for\nA peak picking procedure, applied on the analysis frames\n(of lengthN) of the smoothed and normalized signals dfk[n],\nproduces a time series\ntsk[n] =/braceleftBigg\n1ifdfk[n]demonstrates a peak here\n0otherwise(3)\nDuring peak picking, an adaptive threshold, calculated by\nthe sum of a predeï¬ned, static threshold and a moving me-\ndian ï¬lter is used.\nThe time series tsk[n]are then convolved with a Hanning\nwindow in order to produce the mask functions mk[n]. In\nthat way, a strongly smoothed version of the corresponding\ndetection function is created that however accentuates the\ndetected abrupt events. The above described processing for\nthe creation of the feature list combines the advantages of\nthe use of an onset list with those methods where the feature\nlists are obtained in a continuous manner.\n2.2 Tempo Induction\nIn the second part of the system, the periodicity analysis\nis carried out, in order to infer the tempo from the list of\nfeatures ( i.e.mask functions). The periodicity analysis is\ndone using a bank of comb ï¬lters.\nEach one of the mask functions ,mk[n]is given as an\ninput to a bank of comb ï¬lters. Therefore, for the analysis\nbandkthe following takes place:\nyk,Ï„[n] =aÏ„yk,Ï„[nâˆ’Ï„] + (1âˆ’aÏ„)mk[n], (4)\nfor everyÏ„âˆˆT. The intervalTranges from 42 to 242 beats\nper minute (BMP). In this interval the ï¬lterâ€™s delay Ï„takesinteger values. The term aÏ„corresponds to the ï¬lterâ€™s feed-\nback gain and it is calculated as a= 0.5Ï„\nT0. The time during\nwhich the signal should reach its half energy is T0. In this\nsystemT0is equal to 4 seconds. The selection of this time\nframe is motivated by the smallest tempi normally found in\na piece of music. With a minimum tempo of 42 BPM, this\nframe is big enough to cover at least two repetitions of the\nbeat but also small enough for the system to quickly adapt\nto any tempo changes, when more than one frames are used\nas an input.\nThe energy of each ï¬lter, in each frequency band kis\nthen calculated by\nek,Ï„[n] =1\nÏ„n/summationdisplay\ni=nâˆ’Ï„+1yk,Ï„[i]2. (5)\nA sum across all the frequency bands kwill result to a wide\nband energy signal for each tempo Ï„\neÏ„[n] =K/summationdisplay\nk=1ek,Ï„[n] (6)\nSo far, for every time index nof the input signal we ob-\ntain a vector\ne= [e42[n]e43[n]... e 242[n]]T(7)\nconsisting of the instant energies in every periodicity Ï„âˆˆT.\nTheNTmaximum components of the vector eare then\nselected in order to form a vector w. The corresponding\ntempi form the vector T. The vector Tcontains the winning\ntempi , and vector wtheir relative weights.\n2.3 Perceptual Model\nThe ambiguity in the perception of tempo has been modelled\nand tested in experiments [10, 13] where the distribution of\nresponses from several listeners to the same pieces of music\nwere studied. This analysis resulted in the following reso-\nnance model:\nAe(t) =1/radicalBig\n(t2oâˆ’t2)2+Î²t2âˆ’1/radicalbig\nt4o+t4, (8)\nwhereAeis the effective resonance amplitude, tois the res-\nonance tempo, Î²the damping constant and tis the tempo\nvariable. During experimenting, these parameters were ï¬t-\nted to the distribution of the tapped tempi. It has been found\nthat, on average, music experts produce a resonant tempo of\n138 BPM with a damping constant, Î²equal to 5.0. In Fig-\nure 3 the produced model, with the use of these parameters\nis depicted.\nIn this paper the model of equation (8) is used to weight\nthe results from the periodicity analysis so that\nw/prime\ni=Ae(Ti)wiiâˆˆ[1,2,...,NT], (9)\n199Poster Session 2\n0 100 200 300 400 500 00.02 0.04 0.06 \nTempi (BPM) Amplitude Figure 3 . The resonance model that was described in [10]\nand ï¬ts the distributions of responses to several pieces of\nmusic.\nwhereTithei-th value of vector Tandwithe corresponding\nweight. After this step, elements w/prime\niformw/primewhich contains\nthe perceptually modiï¬ed weights of the winning tempi in\nT. The tempo estimation is therefore enhanced with per-\nceptual information.\nSystems that estimate periodicity patterns in a signal stron-\ngly respond to the multiples and aliquots of any fundamen-\ntal periodicity that appears in it. Likewise, when it comes\nto human listeners, the more ambiguities in determining the\ntempo appear due to the selection of multiples and divisors\nof the same tempo. In the vector of winning tempi, T, also\nappear not only possible perceived tempi, but also multiples\nand aliquots of them.\nIn order to discard some â€œfalseâ€ estimations from Tand\ndecide which is the perceived tempo in a group of tempi\nthat have a common divisor, an extra weighting step is intro-\nduced. During experimenting, it was found that increasing\nthe weights of each tempo that appears in Twith a factor of\nthe weight of its multiples and divisors that also appear in\nT, has the following two desired effects:\na. Signiï¬cant decrease in the (normalized) weights of\ntempi whose multiples and aliquots are not present.\nb. Highly accurate decision on which is the true per-\nceived tempo within a set of tempi that have the same\ncommon divisor (as presented in Section 3).\nThis factor was chosen experimentally 0.3 for multiple pe-\nriods and 0.6 for aliquots.\n3. EVALUATION AND RESULTS\n3.1 Datasets and Evaluation Measures\nThe developed system is evaluated using the measures pro-\nposed in [5]. The two measures deï¬ned are Accuracy 1 and\nAccuracy 2 , corresponding to the percentage of tempo esti-\nmates within 4% of the ground truth data. For the calcula-\ntion of Accuracy 2 also integer multiplications and divisions\nof the ground-truth tempo are considered to be correct esti-\nmates.Winning\nTempiT1 T2 T3 T4 T5\nAccuracy 1\n(%)38.40 28.22 6.02 1.72 2.44\nAccuracy 1\nCDF (%)38.40 66.62 72.64 74.36 76.80\nTable 1 . The Accuracy 1 of the algorithm for the estimation\nof the winning tempi\nThe results are based in two different datasets, both used\nin [5] for a comparative evaluation of tempo induction algo-\nrithms. That way our results can be compared to previous\nwork. The ï¬rst dataset, Ballroom , consists of 698, (30 sec-\nonds long each) audio excerpts. The second dataset, songs ,\ncontains 465 audio excerpts, this time each one being around\n20 seconds long. The two datasets cover a wide range of\ngenres (namely Rock, Classic, Electronica, Flamenco, Jazz,\nAfroBeat, Samba, Balkan, Greek, Cha Cha, Rumba, Samba,\nJive, Quickstep, Tango and Waltz). Both datasets have been\nmade publicly available1. It is mentioned here that due to\nsome missing or bad formatted ï¬les, the following results\nhave been calculated over a subset of the above datasets,\nthat covers the 97.25% of the whole data.\n3.2 Results\nThe ï¬rst phase of the evaluation procedure was to check\nthe accuracy of the algorithm in deï¬ning the vector of the\nwinning tempi, T,i.e.before applying the perceptual mod-\nelling. The winning tempi in the vector are placed in de-\nscending order, based on their weight. In Table 1, the results\non the Ballroom dataset are illustrated. In the ï¬rst row, the\nAccuracy 1 of the algorithm in each index of the winning\ntempi is shown. The next row, presents the cumulative re-\nsults up to each index of the vector T. As depicted in this\ntable, the algorithm has a success rate of 76.8% in estimat-\ning the correct tempo in the ï¬rst 5 estimations.\nThe perceptual model at the end was inspired by this am-\nbiguity in the results. Although the algorithm is quite accu-\nrate in detecting the right periodicity from a music excerpt,\nit has a relatively low percentage (38.4%) to do so in the\nï¬rst guess ( i.e.the tempo with the higher energy). Until this\npoint, only low-level music features have been used. The\nencoding of higher level knowledge on tempo perception in\nthe model could be useful in choosing the right index of the\nwinning tempi vector as the ï¬nal estimation.\nIndeed, the last step of the system achieves this task. The\nperceptual method is applied to the output, improving sig-\nniï¬cantly the results of the algorithm. The results on both\ndatasets, for the two evaluation metrics can be seen in Table\n1http://mtg.upf.edu/ismir2004/contest/tempoContest/\n20012th International Society for Music Information Retrieval Conference (ISMIR 2011)\nBallroom Songs\nMethod A1 A2 A1 A2\nSimple 38.40 69.05 34.68 55.18\nPerceptual 57.31 80.80 51.80 69.14\nTable 2 . Resulting percentages of the algorithm\nAlonso Dixon Klapuri Uhle Schreirer 010 20 30 40 50 60 70 \nProposed algorithm \nLiterature algorithms \nFigure 4 .Accuracy 1 on the Ballroom dataset. The liter-\nature algorithms mentioned are the following: Alonso [1],\nDixon [3], Klapuri [7], Uhle [12], and Scheirer [11].\n2. In the ï¬rst row the results of the two datasets are pre-\nsented, for both measures Accuracy 1 (A1) and Accuracy\n2(A2), without the application of the perceptual modelling\ndescribed in Section 2.3. In the second row, the correspond-\ning accuracy values are shown after the application of the\nperceptual weighting.\nComparing Table 1 and Table 2, it becomes clear that\nthere is a signiï¬cant improvement of 49% in the Accuracy 1\nmeasure when the perceptual weighting is used as a last step.\nMoreover, the fact that this improvement is not followed in\nAccuracy 2 measure implies that the improvement in esti-\nmation takes places due to less multiplication and division\nerrors.\nAs mentioned above, the use of the Ballroom andSongs\ndatasets, along with the use of the Accuracy 1 andAccuracy\n2measures, enables the comparison of the results to the cur-\nrent state-of-the-art algorithms. In Figure 4 such a compar-\nison is depicted and the proposed system seems to perform\nwell. Further improvements to the proposed method are en-\nvisioned and these are discussed in the following section.\n4. CONCLUSIONS AND FUTURE WORK\nA new method to estimate the tempo of musical signals was\npresented in this paper. The evaluation of this method wasconducted using popular datasets for the tempo estimation\ntask along with previously deï¬ned evaluation measures. Al-\nthough at an early stage, the algorithm seems to operate very\nwell in comparison to the state-of-the-art, using only a sin-\ngle frame (4 seconds long) for calculating the result.\nAs mentioned, the above described results are obtained\nfrom a single frame of the input signal. An application of the\nalgorithm on the whole signal, and then the computation of\na median or average tempo estimate did not seem to yield a\nsigniï¬cant improvement. However, the implementation of a\nvoting mechanism could improve the overall tempo estimate\nof a piece. In such an extension an extra assumption has to\nbe made, i.e.that the tempo of the piece does not present\nany variations throughout the song.\nThe use of adaptive LPEFs introduced by this work, seems\nto work well in the task of extracting tempo estimation fea-\ntures. However, it was observed during experimenting, that\nthe ï¬nal results and success rates are sensitive to the the set\nof parameters used by the feature list extraction part (LPC\norder, peak picking static threshold). A detailed examina-\ntion of the results that are obtained from different parameter\nsets and the determination of an optimum set may further\nimprove the accuracy of the whole system.\nFurthermore, the use of a different set of temporal fea-\ntures that indicate the tempo can be considered in a later ver-\nsion of the algorithm as the literature reveals some promis-\ning alternatives. For example, linear prediction coefï¬cients\ninstead of the the prediction error have been successfully\nused as features for music genre classiï¬cation in [9].\nUntil now, the existing knowledge on the perceptual event\nthat leads to the well known action of foot-tapping , has not\nbeen extensively used for a systematic way of estimating\nperceived tempo. This study indicates that taking advan-\ntage of auditory modelling tools can signiï¬cantly improve\nthe performance of a tempo estimation algorithm.\n5. ACKNOWLEDGEMENT\nThis work has been funded in part by the European Commu-\nnityâ€™s Seventh Framework Programme under grant agree-\nment no230709 (PEOPLE-IAPP â€œA VID-MODEâ€ grant).\n6. REFERENCES\n[1] M. Alonso, B. David, and G. Richard. Tempo and beat\nestimation of musical signals. In Proceedings of the\n5th International Conference on Music Information Re-\ntrieval , Barcelona, Spain, 2004.\n[2] M. P. E. Davies and M. D. Plumbley. Context-dependent\nbeat tracking of musical audio. Audio, Speech, and Lan-\nguage Processing, IEEE Transactions on , 15(3):1009â€“\n1020, 2007.\n201Poster Session 2\n[3] S. Dixon, E. Pampalk, and Widmer G. Classiï¬cation of\ndance music by periodicity patterns. In Proceedings of\nthe International Conference on Music Information Re-\ntrieval , pages 159â€“165, 2003.\n[4] F. Gouyon and S. Dixon. A review of automatic\nrhythm description systems. Computer Music Journal ,\n29(1):34â€“54, 2005.\n[5] F. Gouyon, A. Klapuri, S. Dixon, M. Alonso, G. Tzane-\ntakis, C. Uhle, and P. Cano. An experimental com-\nparison of audio tempo induction algorithms. Audio,\nSpeech, and Language Processing, IEEE Transactions\non, 14(5):1832â€“1844, 2006.\n[6] S. Haykin. Adaptive ï¬lter theory . Prentice Hall, 1995.\n[7] A. P. Klapuri, A. J. Eronen, and J. T. Astola. Analysis\nof the meter of acoustic musical signals. Audio, Speech\nand Language Processing, IEEE Transactions on , 14(1),\n2006.\n[8] E. W. Large and J. F. Kolen. Resonance and the percep-\ntion of musical meter. Connection Science , 6(1):177â€“\n208, 1994.\n[9] A. Meng, P. Ahrendt, J. Larsen, and L. K. Hansen. Tem-\nporal feature integration for music genre classiï¬cation.\nAudio, Speech and Language Processing, IEEE Trans-\nactions on , 15(5):1654â€“1663, 2007.\n[10] D. Moelants and M. F. McKinney. Tempo perception\nand musical content: What makes a piece fast, slow\nor temporally ambiguous. In Proceedings of the 8th In-\nternational Conference on Music Perception and Cogni-\ntion, 2004.\n[11] E. D. Scheirer. Tempo and beat analysis of acoustic mu-\nsical signals. Acoustical Society of America , 103, 1998.\n[12] C. Uhle, J. Rohden, M. Cremer, and J. Herre. Low com-\nplexity musical meter estimation from polyphonic mu-\nsic. In Audio Engineering Society Conference: 25th In-\nternational Conference: Metadata for Audio , pages 63â€“\n68, New York, 2004.\n[13] L. Van Noorder and D. Moelants. Resonance and the\nperception of musical meter. Journal of New Music Re-\nsearch , 28(1):43â€“66, 1999.\n202"
    },
    {
        "title": "A Preplexity Based Cover Song Matching System for Short Length Queries.",
        "author": [
            "Erdem Unal",
            "Elaine Chew",
            "Panayiotis G. Georgiou",
            "Shrikanth S. Narayanan"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1415134",
        "url": "https://doi.org/10.5281/zenodo.1415134",
        "ee": "https://zenodo.org/records/1415134/files/UnalCGN11.pdf",
        "abstract": "A music retrieval system that matches a short length music query with its variations in a database is proposed. In order to avoid the negative effects of different orchestration and performance style and tempo on transcription and matching, a mid-level representation schema and a tonal modeling approach is used. The mid-level representation approach transcribes the music pieces into a sequence of music tags corresponding to major and minor triad labels. From the transcribed sequence, n-gram models are built to statistically represent the harmonic progression. For retrieval, a perplexity based similarity score is calculated between each n-gram in the database and that for the query. The retrieval performance of the system is presented for a dataset of 2000 classical music pieces modeled using ngrams of sizes 2 through 6.  We observe improvements in retrieval performance with increasing query length and ngram order. The improvement converges to a little over one for all query lengths tested when n reaches 6.",
        "zenodo_id": 1415134,
        "dblp_key": "conf/ismir/UnalCGN11",
        "keywords": [
            "music retrieval system",
            "matches music query",
            "database variations",
            "orchestration and performance style",
            "tempo effects",
            "transcription and matching",
            "mid-level representation schema",
            "tonal modeling approach",
            "sequence of music tags",
            "major and minor triad labels"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\n  \n \nA PERPLEXITY BASED COVER SONG  MATCHING SYSTEM \nFOR SHORT LENGTH QUERIES  \nErdem Unal1 Elaine Chew2 Panayiotis Georgiou3 Shrikanth S.  Narayanan3 \n1TÃœBÄ°TAK  BÄ°LGEM \n2Queen Mary, University of London \n3University of Southern California \n1unal@uekae.tubitak.gov.tr  \n2elaine.chew@eecs.qmul.ac.uk \n3{georgiou,shri}@sipi.usc.edu \n \nABSTRACT  \nA music retrieval system that matches a short length music \nquery with its variations in a database is proposed. In order \nto avoid the negative effects of different orchestration  and \nperformance s t y l e  a n d  t e m p o  o n transcription and matc h-\ning, a mid -level representation schema and a tonal mode l-\ning approach is used. The mid-level representation a p-\nproach transcribes the music pieces into a sequence of m u-\nsic tags  corresponding to major and minor triad label s. \nFrom the t ranscribed sequence, n-gram models are built to \nstatistically represent the harmonic  progression. For re-\ntrieval, a perplexity based similarity score is calculated b e-\ntween each n-gram in the database and that for the query.  \nThe retrieval performance of the system is presented for  a \ndataset of 2000 classical music pieces modeled using n -\ngrams of sizes  2 through 6 .  We observe improvement s in \nretrieval performance with increasing  query length and  n-\ngram order. T he improvement converges to a little over one  \nfor all query lengths tested when  n reaches  6. \n1. INTRODUCTION \nDue to advances in computer and network technologies, the \ndevelopment of efficient multimedia data storage and r e-\ntrieval applications have received much attention in recent \nyears. In the music do main, motivations for  such systems \ncan vary from industr y objectives  such as royalty rights \nmanagement to individual  use such as personal database \norganization, music preference list creation , etc.  Due to the \nwide range of expressive and instrumental variations poss i-\nble in music pieces, in order for such systems to have the \nnecessary performance reliability as to be useful in the i n-\ndustrial domain, m usic variation matching m u s t  b e  a d-\ndressed . A number of challeng es such as feature extraction, \nrepresentation , tempo and key varia bility ,  n e e d  t o  b e  h a n-\ndled with high precision in order to achieve reasonable per-\nformances.  \nTo eliminate the kinds of  differences caused by expre s-\nsive variations  or instrumental arrangements of the same \nmusic piece, researchers have focused on accurately e x-tracting the types of musical content in which such varia-\ntions have minimal or no effect.  \nA considerable amount of research focused on the tran-\nscription of music signal to MIDI or piano roll represent a-\ntion for accurate understandin g of the note sequence of the \nmusic. Numerous researchers have modeled sound events \nwith known machine learning techniques, in order to detect musical notes and their onset and offset times [1,2,3,4 and \n5]. Their results are promising, although  not accurat e \nenough to provide an extension to a general solution for \nmusic variations matching.   \nSince accurate transcription of multi channel audio is not \neasy, a mid level representation of music is desired. Recent \nresearch attempts in [6,7 and 8] showed that diff erent repr e-\nsentation techniques such as extracting the salient  melody \nor a chord progression from the music piece could be a feasible solution s for polyphonic representation since har-\nmonic structure tend s note  to change dramatically with e x-\npressive and ins trumental deviations .  \nOn the other hand, some researchers focused on extract-\ning fingerprints that carr y information about the acoustic \nfeature distribution of the music piece over time. [9 and 10]  \nused chroma based features to directly represent music \npieces, without labeling  and used  simple cross correlation \nof chroma vectors for measuring similarity. Kim also \nadopted delta features that represent general movement in \nthe harmonic structure of music pieces for more accurate \nrepresentation and retrieval [1 1].  \nPickens et. al [13] used existing polyphonic transcription \nsystems in the literature to abstract note features from m u-\nsic audio. The tr anscription was then mapped to the  har-\nmonic domain. A bi-gram (2 -gram) representation, namely \na 24 Ã—  24 triad (th ree-note chord ) transition matrix was \nused to represent both the query and the music pieces in the \ndatabase. A  distance metric between an input transition m a-\ntrix and the transition ma trices available in the database \nwas calcu lated to determine  similarity.  \nOur study differs from other researchers ' who use some  \nkind of mid level representation in the similarity metric we \nuse, and in that we use a sliding window approach in our \ntranscription independent of the exact locations of note o n-\nsets and offsets. While  our strate gy loses note level details \nin the audio, it makes our representation more robust  to \n43Poster Session 1\n  \n \nnote transcription errors.  In contrast to the retrieval m e-\nthods reported in  [12 and 13]  we tested our model on not \nonly bi grams but also higher order n-grams , for n up to and \nincluding 6 , and  observed a major boost in the retrieval pe r-\nformance  with increasing Markov chain order .   \nIn later studies, Lavrenko & Pickens [14] used random \nfields to model polyphonic music pieces from MIDI files. \nUsing random fields,  they automatically induced new high \nlevel features from the music pieces , such as consonant and \ndissonant chords, progressions and repetitions, to efficiently model polyphonic music information.  \nThe F-measure , Correct Retrieval Accuracy, and  Mean \nReciprocal Rank are used to measure the performance of \nthe systems available in the literature. The reported results \nvary with respect to the database selected, its size and the \ncomplexity  of the variations available. Since the algorithms \nused are ge nerally computationally expensive, the exper i-\nmental databases tend not to be larger than a couple of \nthousand songs. For a more detailed overview of the sy s-\ntems available in the  literature, please refer to [ 18].   \nMost systems, including the ones described  above,  were \ndesigned assuming the availability of the entire  query and \ntarget  song s from beginning to end. To our knowledge, no \ntests were reported when only short length queries are \npresent. In this work, a mid level tonal representation of \naudio and a s tatistical tonal modeling method for perfor m-\ning retrieval of short length audio queries is proposed.  \nIn order to ensure robust transcription against musical \nvariations, a 3 dimensional Tonal Space (TS), a toroidal  \nversion of the Spiral Array  model  [15] is used. The details \nare explained in Section 2. 12 dimensional Pitch Class Pr o-\nfile (PCP) features are mapped onto the TS and a centroid  \n(center of weight) is calculated in order to find the repr e-\nsentative position of each audio frame in 3D space. A 1 -\nnearest neighborhood classifier is used for identifying the centroid s of each frame with respect to triad chord classes. \nA key and tempo invariant time series of triad chord  label s \nare then acquired , from which we derive n-gram  represent a-\ntions  of each music piece in the database. The similarity \nbetween the extracted triad series and the n-gram models is \ncalculated using the perplexity measure. The flowchart of the proposed system can be seen in Figure 1. The paper \nconcludes with  the explanation of the experimental setup, \nthe results and the discussion on future work.\n \n \n \nFigure 1. Flowchart for the proposed system .  2. TONAL MUSIC SPACE  \nThere exists an illustrious  history  of mathematica l and m u-\nsic theor etic work on  geometric modeling of tonal  relatio n-\nships between pitches , intervals, chords , and keys. A r e-\nview of these models can be found in [ 16].  \nWe u se a toroidal version of the Spiral A rray for a nu m-\nber of reasons.  We are interested in a flexible tonal repr e-\nsentation that combines different tonal features in the same \nspace.   The Spiral Array  clusters tonal objects  that are \nharmonic ally close ; this is especially important for robust \nanalysis of audio without exact transcription.  \nThe model consists of a series of nested helices in three -\ndimensional space. The outermost spiral consists of pitch \nclasses that form  the line or circle of fifths . Pitch classes \nare placed at each quarter turn of the spiral, so that vertical-\nly aligned pitch classes are a major third apart. This net-\nwork of pitches is identical to the neo -Riemannian tonnetz  \nshown in Figure 2.  Pitch classes that are in the same triads  \nare closely clustered, as are those that are in the same key . \nChord representations are generated as weighted combin a-\ntions , a kind of centroid,  of their component pitch classes, \nand key representations are constructed from their I, IV, \nand V chords.  The details and applications of the Spiral \nArray model are explained in [15][ 17]. \n \nFigure 2.  The tonnetz. Perfect 5th, Major  3rd and Minor  \n3rd distances  \nThe Spiral Array model assumes a cylindrical form to \npreserve enharmonic spellings. In contrast, we wrap the \nmodel into a torus so as t o ignore pitch spell ing. The resul t-\ning pitch class torus is shown in Figure 3. The 24 chord \nrepresentations are then  defined by constructing the tria n-\ngle outlined by each chordâ€™s  root, fifth, and third , and cal-\nculating the centroid of these vertex points . A chord repr e-\nsentation is illustrated in Figure 3.  While the toroid m odel \nno longer has the same kinds of symmetries and invar iance \nin the cylindrical model,  the chord and key regions remain \n4412th International Society for Music Information Retrieval Conference (ISMIR 2011)\n  \n \nsufficiently distinct for geometric discrimination between \ndifferent chords.  \n \nFigure 3.  Tonal Space: positions of the 12 pitch classes and \nconstruction of the C Maj triad chord using C , G and E.  \n \n3. FEATURE EXTRACTION  \nAs discussed earlier, to overcome the effects of incorrect \ntranscription, we use a mid level transcription approach for \nthe transcription task . The goal is to accuratel y label each \nframe of music audio with major  or minor  triad chords.  For \nthis, we use the tonal  space described  in Section 2. We now \npresent our feature extraction process.  This process is ou t-\nlined in the top row of boxes in Figure 1. \nAudio Input Frames: 250 ms audio frames with 90% \noverlap is used. A large  window with a wide  margin of \noverlap is preferred because our goal is to track the general \nharmonic  movement and not instantaneous local changes  \nthat would be  expected to be sensitive  to variations  in in-\nstruments and expression  and thus pose problems for  the \nretrieval system â€™s similarity calculations.   \nPitch Class Profile: 12 dimensional Pitch Class Profile \n(PCP) features are collected  from each audio frame . The \npitch classes extracted range from  A0 (27. 5 Hz) to A7 \n(3520 Hz). From the PCP's, the note weights are mapped to \npitch class positions in the tonal space, and a centroid  is \ncalculated in 3D space as shown in Fig 4 (red star).  \nChord Labels: The centroid  derived in the fashion d e-\nscribed above  represents a kind of  tonal center of the par-\nticular frame. The system aims to captur e and record the \nmovement of centroids , after they are marked  with the most \nappropriate chord  label . First, the system classifies the  cen-\ntroid  as one of the triads located in the Tonal Space, using a  \nstraightforward 1- NN algorithm , l ik e i n  [15] . The classif i-\ncation boundaries are not calculated from training data, but deterministically defined as described in Section 2.  This \ntranscription strategy compensates for variations  in spectral \ncharacteristics and intensity levels when  the same notes and \nharmonies are played on different instruments.   \n4. N-GRAM  MODEL OF HARMONIC SEQUENCE S \nWe use  n-grams to model the harmonic progressions  of the \nmusic pieces. The output of the feature extraction process is  \nan L length chord sequence . We describe here the normal i-zation process  to make the sequence tempo and key inva-\nriant.  Such normalization is  required because the queries \nand the matching music  in the database may be in different \nkeys and temp i.  \nTo ensure key invariance , relative chord changes are ex-\ntracted from the transcribed sequence, an approach that has \nalso been used by other researchers [ 19].   \n \n \nFigure 4.  Mapping from PCP to the Tonal Space. Calcul a-\ntion of the tonal centroid and its distance to the triad chords.  \nSince the window length and overlap rate is high (250ms  \nand 90%, respectively), the transcription  of the harmonic \nprogression  contains many chord  repetitions. We remov e \nthese repetitions  so as to focus on harmonic  changes , rather \nthan harmonically stable parts  of the music sequence. By \ndoing  so, tempo variations are also  eliminated. The resul t-\ning harmony sequences thus carry more distinct inform a-\ntion about the harmonic  progression.  \nIn our experiments, n-grams were selected for modeling  \nharmonic progressions . Results for different n-grams are \nreported in S ection 6.  The audio coverage range of a 6-\ngram in our experiments is between 0.8 seconds and 2.3 \nseconds. On aver age 1. 5 seconds of music a udio is \nrepresented by  a 6-gram feature set.  \nTo enable the efficient use of this strategy, smoothing of \nthe n-gram models is required . Smoothing  is widely used to \neliminate computational problems caused  by non -existing \nn-grams in natural language processing applications . A \nUniversa l Background Model (UBM) is produced using the \nentire music database and mixed with each individual n-\ngram model using a low weight  for smoothing  (0.9 vs 0.1) . \nFinally, the collection of the smoothed  n-grams constitutes \n45Poster Session 1\n  \n \nour database.  We use the SRILM toolkit [ 20] to create the \nn-gram models, to perfor m smoothing, and to evaluate the \nmodel . \n5. RETRIEVAL  METHOD  \nWe use  the perplexity  measure to evaluate the similarity \nbetween the n-gram model of each music piece in the dat a-\nbase and that of the short -length query sequence. The pe r-\nplexity measure gives the likelihood the query was genera t-\ned by a specific probability distribution, namely one of the \nn-gram harmonic progression models in the database.  \nThe perplexity of a discret e probability distribution p can \nbe defined as:  \n, \nwhere H(p) is the entropy of the distribution. Suppose p is \nunknown. One can model the  unknown distribution p using \na training sample drawn from p. Given a proposed model q, \none can evaluate how successful ly q predicts the sample set \n{x1,x2, x3, ... x N} drawn from p using the perplexity mea s-\nure.  The perplexity of the model q  can be defined as:  \n \n. \nA model q that better predicts the unknown distribution p \ngives higher probabilities of q(x i), which  leads to lower \nperplexity.  \nOur system first builds n-gram models of the query and \nof each piece in the database. It then uses the perplexity measure to determine which of the harmonic progression \nmodels  of the pieces in the database best fits the query s e-\nquence.    The system then returns  an N -best list of the most \nlikely candidates.  \n6. EXPERIMENTS  \nA list of 1000 classical music pieces from famous compos-\ners is selected. For each piece in the list, 2 recordings are \nacquired (one termed the â€œ original â€ and  the other a  varia-\ntion). The variation can be a different instrumental ar-\nrangement of the piece  or a recording of the same piece by \nanother artist. We replace the ones for which we cannot \nfind an  additional audio  recording s (CD or mp3) with audio \nsynthesized from the  MIDI version as the variation (about \n250 such MIDI variations  are created ) .   A l l  f i l e s  a r e  c o n-\nverted to 16 kHz 16 -bit wav format. All 2000 files (1000 \noriginal s and 1000 variation s) are converted to strings of  \nchord labels  using the method explained in Section 3. The \noriginal recordings are used to  train n-gram harmonic pr o-\ngression models that constitute the database. The short \nlength test queries are extracted from random parts of each music piece. For each of the query pieces, the system aims  \nto retrieve  the original recording of the target piece in the \nN-best list.   \n  Length of the query  \n  15s 25s 35s Full \nTop-1 \nmatch  Accuracy  37.6 41.6 42.9 51.6 \nMRR  - - - - \nTop-5 \nmatch  Accuracy  55.8 57.4 59.6 63.7 \nMRR  60.4 63.6 67.4 70.1 \nTop-20 \nmatch  Accuracy  56.8 59.6 62.6 71.5 \nMRR  71.8 75.4 73.2 79.8 \nTable 2.  Retrieval results  (%) for the 6 -gram model  over \ndifferent query lengths . \n \n \n \nFigure 5 : Graph showing the effect of query length on the \ntop-N match correct retrieval accuracy for N  = 1, 5, and 20 \n(actual numbers given in Table 2).  \n \nAlongside the N -best list scores, the Mean Reciprocal \nRank (MRR) measure,  which gives  the average  rank of the \ncorrect matches in  the top -N retrieved results (by percen-\ntage) , are also calculated . Table 2 shows the retrieval r e-\nsults for the 6 -gram model as it varies with different query \nlengths and different N -best list lengths.  The numbers are \ngraphed in Figure 5.  \nOne can see from the results that one of the  main deter-\nminants of  retrieval per formance is the length of the query. âˆ‘=âˆ’xxp xp pH )( log)( )( 22 2\nâˆ‘==âˆ’N\niixqN\nqpP12 )( log1\n),( 2\n4612th International Society for Music Information Retrieval Conference (ISMIR 2011)\n  \n \nSince the system retrieves similar songs based on  the rela-\ntive frequency of n-length subsequences, the longer the \nquery, the more its n-gram model resembles that of the ta r-\nget song.  T he number of  distinct harmonic progressions  \nthat identifies the target song is  also directly increased with \nquery length.  \n \n Length of the query  \n  15s 25s 35s full \nn = 6 37.6 41.6 42.9 51.6 \nn = 5 36.4 40.2 40.9 49.7 \nn = 4 32.5 35.4 37.1 43.4 \nn = 3 28.8 35 36.2 42.3 \nn = 2 22.6 30.2 33.1 40.2 \nTable 3 . Top-1 match retrieval accuracy  (%) over different \norder n-gram models and  different query lengths . \n \nFigure 6: Graph showing t he effect of query length and  n-\ngram  size on the top-1 match correct retrieval accuracy (ac-\ntual numbers given in Table 3) . \n \nTable 3 and Figure 6 present results for different length \nn-grams. It illustrates  how the use of higher order  n-grams \n(n>2) boost s the system â€™s performance. For all query \nlengths, larger n-grams yield better results. For all n, longer \nquerie s yield higher accuracies.  \n \n \nFigure 7: Graph of retrieval accuracy ratios as n is in-\ncreased by one.  \n \nFigure 7 shows the graph of the accuracy ratios (an ind i-\ncator of performance improvement) as the n -gram order is \nincreased by one. All numbers are above one, indicating that performance improves by increasing the n-gram order. \nIt is interesting to note that t he ratio of the accuracy for n = \n6 over that for n = 5 converges between 1.03 and 1.05 for \nall query lengths. As shown by these numbers, t he perfo r-\nmance difference between 5 -grams and 6 -grams is small \nwith respect to accuracy. This may be  because 5-gram s be-\ncome sufficiently sparse for capturing  the unique harmonic \nfeatures of the music pieces . Thus,  building 6-gram and \nhigher model s will likely not have a strongly positive effect \non retrieval performance for this particular dataset. The tr a-\ndeoff between computation time and retrieval accuracy  \nshould also be a consideration  since building models and \ncalculating perplexity for larger n-grams takes more co m-\nputation al power and time.   \n7. CONCLUSION  \nIn thi s work, a perplexity based  audio music retrieval sy s-\ntem that is robust to instrumental variation is proposed. \nPCP features are ext racted from overlapping frames and \nmapped to a 3-dimension al tonal space. A1 -NN classifier \ndecides the harmonic  identity of the particular frame based \non pre -defined  positions of the 24 major  and minor  triads in \nthe tonal space. Key normalization is performed. From the classifier output, repetitions are removed so as to focus  on \nchanges in the series of harmonies . From the resulting  har-\nmonic  sequence, n-gram statistics are acquired and a  data-\nbase is constructed. Given a music query, the transcription \nis complete d using the same strategy and the similarity b e-\ntween  the transcribed input and the database models is \ncomputed using the perplexity measure.  \nThe algorithm is tested on  a database of 2000 music \npieces. While there is room for improvement, the results \nshow that, for short length queries, the perplexity -based ap-\nproach is capable of  finding the target piece. The work \ncould be strengthened by testing on a larger dataset with more versions of each song.  \nTo our knowledge, no other study in the literature r e-\nports results from short length queries. Our motivation here \nis that royalty rights management systems usually  work \nwith short length queries  and we would like to apply  our \nsystem in such scenarios.  The MRR and top -N best list \nscores suggest that a more fine-grained representation  may \nbe needed in order to more  successfully retrieve the target \npiece. Ideally, we would like a retrieval system for which \nthe target piece top s the results list, an important criterion \nfor royalty rights management  applica tions . \nFuture work includes systematically isolating comp o-\nnents  of our system for evaluation and improvements. We \nhave used a straightforward feature extraction strategy, which should be compared against other methods. We can \nsubstitute chord labeling algorithms in the literature for the  \nparticular method used to extr act harmonic labels  to ex-\namine the impact of chord labeling technique on retrieval \nsuccess.  Other f urther work includes implementing multi \n47Poster Session 1\n  \n \nstage search algorithms, in order to improve search perfo r-\nmance with respect to time and accuracy .  \n8. ACKNOWLEDGEMENTS  \nThis work was supported in part by a National Science \nFoundation (NSF)  Grant No. 0219912, and in part by a \nTÃœBÄ°TAK Career Grant 3501 -109E196. Any opinions, \nfindings and conclusions or recommendations expressed in \nthis material are those of the authors, and do not necessarily \nreflect those of the NSF or the TÃœBÄ°TAK.  \n9. REFERENCES  \n [1]  C. Raphael : â€œAutomatic Transcription of Piano M u-\nsic,â€ Proceedings of the International Conference on \nMusic Information Retrieval , 2002.  \n[2]  A. Pertusa and J. M. Inesta : â€œPolyphonic Music Tran-\nscription Through Dynamic Networks and Spectral \nPattern Identification,â€ Proceedings of the  Internatio n-\nal Workshop on Artificial Neural Networks in Pattern Recognition, 2003.  \n[3]  P. Smargadis and J. C. Brown : â€œNon -Negative Matrix \nFactorization for Polyphonic Music Transcription,â€ Proc eedings of the  IEEE Workshop on Applications of \nSignal Processing to Audio and Acoustics , 2003.  \n[4]  M. Ryyananen and A. Klapuri : â€œPolyphonic Music \nTranscription Using Note Event Modeling,â€ Proc eed-\nings of the  IEEE Workshop on Applications of Signal \nProcessing to Audio and Acoustics , 2005.  \n[5]  G. E. Poliner, and D. P. W. Ellis : â€œA Discriminative \nModel for Polyphonic Piano Transcription,â€ EURASIP \nJournal on Advances in Signal Processing, V ol. 2007.  \n[6]  W.-H. Tsai, H.- M.Yu, and H.- M. Wang : â€œQuery by \nexample technique for retrieving cover versions of popular songs with similar melodies,â€ Proc eedings of \nthe International Conference on Music Information R e-\ntrieval, 2002.  \n[7] M. Marolt : â€œA mid level melody based representation \nfor calculating audio similarity ,â€ Proc eedings of the  In-\nternational Conference on Music Information Retrie v-\nal, 2006.  \n[8]  E. Unal, P. Georgiou, E. Chew, and S. Narayanan: \nâ€œStatistical modeling and retrieval of polyphonic m u-\nsic,â€ Proc eedings of the IEEE Multimedia Signal \nProcessing Workshop, 2007.  \n[9]  J. Serra and E. Gomez : â€œAudio cover  song identific a-\ntion based on tonal sequence alignment,â€ Proc eedings \nof the  International Conference on Acoustics, Speech \nand Signal Processing, 2008.  \n[10] D. P. W. Ellis, C. V. Cotton, and M. I. Mandel : \nâ€œCross -correlation of beat -synchronous representations for music similarity,â€ Proc eedings of the  International \nConference on Acoustics, Speech and Signal \nProcessing , 2008.  \n[11] S. Kim, E. Unal, and S. Narayanan : â€œFingerprint e x-\ntraction for classical music  cover song identification,â€ \nProc eedings of the  IEEE International Conference on \nMultimedia Expo , 2008.  \n[12] â€œ S. Doraisamy and S. Ruger: â€œRobust Polyphonic R e-\ntrieval with N -grams,â€ Journal or Intelligent Inform a-\ntion Systems , Vol. 21, No. 1, pp. 53 -70, 2003.   \n[13] J. Pickens, J. B. G. Monti, M. Sandler, T. Crawford, \nM. Dovey, and D. Byrd : â€œPolyphonic Score Retrieval \nUsing Polyphonic Audio Queries: A Harmonic Mode l-\ning Approach,â€ Journal of New Music Research, V ol. \n32, 2003.  \n[14] V. Lavrenko, and J. Pickens : â€œPolyphonic Music Mo d-\neling with Random Fields,â€ Proc eedings of  ACM Mu l-\ntimedia, 2003.  \n[15] E. Chew : Towards A Mathematical Model of T onality , \nPh.D. dissertation, Massachusetts Insitute of Techno l-\nogy, Cambridge, MA, 2000.  \n[16] C. L. Krumhansl:  â€œThe geometry of musical structure: \na brief introduction and history,â€  ACM Computers in \nEntertainment , Vol. 3, No. 4 , 14 pages , 2005.  \n [17] E. Chew, Elaine:  â€œSlicing It All Ways: Mathematical \nModels for Tonal Induction, Approximation and Se g-\nmentation Using the Spiral Array ,â€ INFORMS Journal \non Computing , Vol. 18, No. 3, pp.305\nâ€“320, 2006.  \n[18] J. Serra,  E. Gomez and P. Herrera : \"Audio cover song \nidentification and similarity: background, approaches, \nevaluation, and beyond\", Studies in Computational I n-\ntelligence , 2010 . \n[19] T.E. Ahonen and K. Lemstrom : \"Identifying cover \nsongs using the normalized compression distance\", \nProceedings of the International  Workshop on M a-\nchine Learning and Music, 2008.  \n[20] A. Stolcke : â€œSrilm â€“ an Extensible Language \nModeling Toolkit,â€ Proc eedings of the  International \nConference on Spoken Language Processing, 2002.  \n48"
    },
    {
        "title": "Information Retrieval Meta-Evaluation: Challenges and Opportunities in the Music Domain.",
        "author": [
            "JuliÃ¡n Urbano"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1417897",
        "url": "https://doi.org/10.5281/zenodo.1417897",
        "ee": "https://zenodo.org/records/1417897/files/Urbano11.pdf",
        "abstract": "The Music Information Retrieval field has acknowledged the need for rigorous scientific evaluations for some time now. Several efforts were set out to develop and provide the necessary infrastructure, technology and methodologies to carry out these evaluations, out of which the annual Music Information Retrieval Evaluation eXchange emerged. The community as a whole has enormously gained from this evaluation forum, but very little attention has been paid to reliability and correctness issues. From the standpoint of the analysis of experimental validity, this paper presents a survey of past meta-evaluation work in the context of Text Information Retrieval, arguing that the music community still needs to address various issues concerning the evaluation of music systems and the IR cycle, pointing out directions for further research and proposals in this line.",
        "zenodo_id": 1417897,
        "dblp_key": "conf/ismir/Urbano11",
        "keywords": [
            "Music Information Retrieval",
            "rigorous scientific evaluations",
            "Music Information Retrieval Evaluation eXchange",
            "experimental validity",
            "Text Information Retrieval",
            "evaluation forum",
            "reliability and correctness",
            "music community",
            "evaluation of music systems",
            "IR cycle"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nINFORMATION RETRIEVAL META-EVALUATION: \nCHALLENGES AND OPPORTUNITIES IN THE MUSIC DOMAIN \nJuliÃ¡n Urbano \nUniversity Carlos III of Madrid \nDepartment of Computer Science \njurbano@inf.uc3m.es  \n \nABSTRACT \nThe Music Information Retrieval field has acknowled ged \nthe need for rigorous scientific evaluations for so me time \nnow. Several efforts were set out to develop and pr ovide the \nnecessary infrastructure, technology and methodolog ies to \ncarry out these evaluations, out of which the annua l Music \nInformation Retrieval Evaluation eXchange emerged. The \ncommunity as a whole has enormously gained from thi s \nevaluation forum, but very little attention has bee n paid to \nreliability and correctness issues. From the standp oint of the \nanalysis of experimental validity, this paper prese nts a \nsurvey of past meta-evaluation work in the context of Text \nInformation Retrieval, arguing that the music commu nity \nstill needs to address various issues concerning th e \nevaluation of music systems and the IR cycle, point ing out \ndirections for further research and proposals in th is line. \n1.  INTRODUCTION \nInformation Retrieval (IR) is a highly experimental  \ndiscipline, and IR Evaluation (IRE) experiments are  the \nmain research tool to scientifically compare IR sys tems and \nalgorithms to advance the state of the art through careful \nexamination and interpretation of their results. IR E has been \nused and studied in Text IR for over 50 years now, since the \nCranfield 2 experiments [18], with successful evalu ation \nforums such as TREC, CLEF, NTCIR or INEX. Until 200 6, \nthese evaluations were not usual at all in Music IR  (MIR), \nalthough there was general concern about specific n eeds and \nresources for a fruitful beginning of evaluation ca mpaigns in \nthe Music domain. \nThe â€œISMIR 2001 resolution on the need to create \nstandardized MIR test collections, tasks, and evalu ation \nmetrics for MIR research and developmentâ€ was draft ed and \nsigned by many members of the community as a \ndemonstration of the general concern [20]. A series  of three \nworkshops then followed between July 2002 and Augus t \n2003, were researches begun this long-needed work f or \nevaluation in Music IR [20]. There was some general  \nagreement that evaluation frameworks for Music IR w ould \nneed to follow the steps of the Text REtrieval Conf erence (TREC) [53][56], although it was clear too that spe cial care \nwas to be taken not to oversimplify the TREC evalua tion \nmodel [19], because Music IR differs greatly from T ext IR \nin many aspects that affect evaluations [21]. The g eneral \noutcome of these workshops, and many other meetings , was \nthe realization by the Music IR community that thes e \nevaluations were clearly necessary, and that a lot of effort \nand commitment was needed to establish a periodic \nevaluation forum for Music IR systems. Finally, in 2005 the \nfirst edition of the Music Information Retrieval Ev aluation \neXchange (MIREX) took place, and ever since it has \nevaluated over a thousand Music IR systems for many  \ndifferent tasks on a yearly basis [23]. \nThe impact of MIREX has been without doubt benefici al \nfor the Music IR community, not only for fostering these \nexperiments, but also for studying and establishing  specific \nevaluation frameworks for the Music domain. But now  that \nit is widely accepted, it seems that the community has \nsettled down in the belief that we finally have wha t we \nwanted. It is our belief though, that while we are on the right \npath, there is still a lot of work to do in Music I R \nEvaluation. These experiments are anything but easy  and \nstraightforward [54][26], so much that a whole area  therein \nis concerned with their reliability and correctness : \nInformation Retrieval Meta-Evaluation. The Text IR \nliterature has been flooded with meta-evaluation st udies for \nthe past two decades, showing year after year that IRE has \nits very own issues and proposing different approac hes and \ntechniques to cope with them. While the MIR communi ty \nhas inherited good evaluation practices by adopting  TREC-\nlike frameworks, some are already outdated, and oth ers still \nlack appropriate analyses. We agree that not everyt hing \nfrom the Text IR community applies to Music IR, but  a lot \nof meta-evaluation studies do. In fact, since the i nception of \nMIREX in 2005 several landmark studies have taken p lace \nin the context of TREC, specially focused on large- scale \nevaluation, robustness and reliability, none of whi ch has \neven been considered for Music IR.  \nIn this paper we approach meta-evaluation from the point \nof view of the analysis of experimental validity of  IR \nEvaluation experiments. We show different aspects o f IRE \naffected by these validity considerations, and surv ey the \nText IR literature outlining how these problems are  dealt \nwith in evaluation forums such as TREC. Finally, we  show \nthe current shortcomings in MIR evaluation and prop ose \nlines for further work, as a starting point for wha t we hope \nbegins a tradition of periodic Music IR Evaluation studies.  \nPermission to make digital or hard copies of all or  part of this work for \npersonal or classroom use is granted without fee pr ovided that copies are \nnot made or distributed for profit o r commercial advantage and that \ncopies bear this notice and the full citation on th e first page. \nÂ© 2011 International Society for Music Information Retrieval  \n609Oral Session 6: Databases and Evaluation\n2.  IR EVALUATION \nIR evaluation experiments follow the traditional Cr anfield \nparadigm conceived by Cleverdon in the late 50â€™s [1 8]. The \nmain element needed for these evaluations is a test  \ncollection, which is made up of three basic pieces:  a \ndocument collection, a set of information needs and  the \nrelevance judgments telling what documents are rele vant to \nthese information needs (the ground truth or gold s tandard). \nThese test collections are built in the context of a particular \ntask defining the intent of the information needs, and several \nmeasures are used to rank the systems following dif ferent \ncriteria, always from the point of view of a user m odel with \nassumptions and restrictions as to the potential re al users of \nthe systems being evaluated.  \nAlthough some variations exist, a typical IRE exper iment \ngoes as follows [54][26]. First, the task is identi fied and \nwell-defined, normally seeking the agreement betwee n \nseveral researchers. Depending on the task, a docum ent \ncollection is either put together or reused from an other task, \nand a set of information needs is selected, often g iven as \ndirect input queries. The systems to evaluate retur n their \nresults for the particular query set and document c ollection, \nand these results are evaluated using several measu res that \nattempt to assess how well the systems would have s atisfied \na real user. This assessment employs the relevance \njudgments in the ground truth, made before or after  running \nthe systems, depending on the task and other factor s. \n3.  IR META-EVALUATION \nExperimental validity establishes how well an exper iment \nmeets the well-grounded requirements of the scienti fic \nmethod [30][35][36]. That is, whether the results o btained \ndo fairly and actually assess what the experimenter  \nattempted to measure. Validity of experiments is us ually \nassessed from different points of view, depending o n what \naspects of the scientific method are at stake. \n \nTask  \nUser model  \nDocuments  \nQueries  \nGround truth  \nSystems  \nMeasures  \nConstruct  x x     x \nContent  x x x x  x  \nConvergent   x   x  x \nCriterion     x x  x \nInternal    x x x x x \nExternal    x x x x  \nConclusion   x  x x x x \nTable 1. The effect of Experimental Validity on Information \nRetrieval Evaluation experiments. \nInformation Retrieval Evaluation experiments, as \nscientific experiments themselves, are also subject  to \nvalidity analysis. Meta-evaluation can be viewed as  the \nanalysis of this experimental validity, highlightin g that the \nevaluation is itself being evaluated. Next, we disc uss several \ntypes of experimental validity and show how they af fect IR \nevaluation experiments (see Table 1). 3.1  Construct Validity \nConstruct validity evaluates the extent to which th e \nvariables of an experiment correspond to the theore tical \nmeaning of the concept they purport to measure. For  \nexample, an experiment to assess the quality of the  results \ngiven by a Web search engine would not have constru ct \nvalidity if quality were measured as the number of visits to \nthe site, because this actually measures its popula rity. Thus, \nan experiment acquires construct validity by thorou gh \nselection and justification of the variables used. \nIn the case of IRE, construct validity is concerned  mainly \nwith the evaluation measures and the user model con sidered \nfor the particular task [16]. For instance, in a tr aditional ad \nhoc retrieval task, binary set-based measures such as \nPrecision and Recall do not resemble a real user wh o wants \nnot only relevant documents, but highly relevant on es at the \ntop of the results list [42]. Instead, rank-based m easures \nsuch as Average Precision , graded relevance judgments \n[52][31], or the combination [29], are more appropr iate. \n3.2  Content Validity \nContent validity evaluates the extent to which the \nexperimental units reflect and represent the elemen ts of the \ndomain under study. For example, an experiment meas uring \nthe reading comprehension of students would not hav e \ncontent validity if only science-fiction stories we re \nemployed. Thus, an experiment acquires content vali dity by \ncareful selection of the experimental units include d. \nIn IR evaluation, it is imperative that the task re sembles \nas closely as possible the real-world settings it r epresents, \nand that the systems evaluated fulfill as much as p ossible the \nneeds of the real users. However, evaluating under such \nconditions would introduce a heavy user component v ery \ndifficult to manage and control, so a more system-o riented \napproach is usually followed [54][18]. As such, the  actual \nvalue of the systems in real settings is many times  \noverlooked [34], and sometimes it can be questioned  [45]. \nLikewise, the documents in the collection must rese mble \nas closely as possible the documents that would be found in \na real-world setting of the task, and have a suffic iently large \nsample so as to be representative of the domain. Al so, the \nparticular queries used should be carefully selecte d to \nrepresent a diverse and wide range of possible use cases, \nwhile being reasonable for the document collection in use \n[54][12]. Moreover, some queries are more helpful t han \nothers to differentiate between systems [25][38]. \n3.3  Convergent Validity \nConvergent validity evaluates the extent to which t he results \nof an experiment agree with other results, theoreti cal or \nexperimental, they should be related with. For exam ple, the \nresults of a study measuring the mathematical skill s of \nstudents should be correlated with other studies on  abstract \nthinking. Thus, an experiment acquires convergent v alidity \nby careful examination and confirmation of the rela tionship \nbetween its results and others supposedly related. \n61012th International Society for Music Information Retrieval Conference (ISMIR 2011)\nGround truth data is a much debated part of IR eval uation \nbecause of the subjectivity in the very concept of relevance. \nSeveral studies show that documents are judged diff erently \nby different people in terms of their relevance to some \nspecific information need, even by the same people over \ntime. As such, the validity of IRE experiments can be \nquestioned because different results are obtained d epending \non the people that make the relevance judgments. Se veral \nstudies have shown that absolute figures do indeed change, \nbut the relative differences between systems stand still for \nthe most part [51]. For very large-scale experiment s though, \nthese differences can have a large impact on the re sults [13]. \nEffectiveness measures are usually categorized as \nprecision- or recall-oriented. Therefore, it is exp ected for \nprecision-oriented measures to yield effectiveness scores \ncorrelated with other precision-oriented measures, and \nlikewise with recall-oriented ones. However, this d oes not \nalways happen [39][31], and some measures are even better \ncorrelated with others than with themselves [57], e videncing \npredictability problems. In general, all these meas ures \nshould be correlated with user satisfaction in the particular \ntask [42], so alternatives such as rank-based measu res, \ndifferent forms of ground truth data [4] or relevan ce \ndiscount functions [29] are usually considered. \n3.4  Criterion Validity \nCriterion validity evaluates the extent to which th e results of \nan experiment are correlated with those of other ex periments \nalready known to be valid. For example, a study to evaluate \nif a new product would have as good sales as an old  one \nwould lack criterion validity if subjects were just  asked \nwhether they like the new one, instead of whether t hey like \nit even more: the context changed in the second cas e. Thus, \nan experiment acquires criterion validity by carefu l \nexamination and confirmation of the correlation bet ween its \nresults and others previously established. \nAs real-world systems need to manage more and more \namounts of information, modern IR evaluation studie s have \nfocused on practical large-scale methodologies, mai nly \nthrough a technique called pooling [8]. This permit s the use \nof large collections while requiring somewhat reaso nable \neffort in relevance judging by assuming that docume nts not \nretrieved by any system are indeed not relevant. Mo re recent \nstudies analyze the use of non-experts for relevanc e judging \n[3], crowdsourcing platforms such as Amazon Mechani cal \nTurk [1][17], requiring fewer judgments to give an estimate \nof the absolute effectiveness scores of the systems  [59][60], \nselecting what judgments better tell the difference  between \nsystems [10][11], or even using no relevance judgme nts at \nall [44]. All these improvements allow for an incre ase on \ncontent validity as the effort per query diminishes . The \nresults of all these methodologies are usually comp ared with \nthe results of traditional ones, in terms of criter ion validity, \nto see whether they are really viable or not. That is, whether \nthe results they produce not only require less effo rt, but also \nagree with those of previous, accepted methodologie s. 3.5  Internal Validity \nInternal validity evaluates the extent to which the  \nconclusions of an experiment can be rigorously draw n from \nthe experimental design followed, and not from othe r factors \nunaccounted for. For example, a study on the usabil ity of \ntwo word processors would not have internal validit y if the \nsubjects were already familiar with one of the prod ucts. \nThus, an experiment acquires internal validity by c areful \nidentification and control of possible confounding variables \nand selection of experimental designs. \nIn IR evaluation, observed differences between syst ems \ncould be the result of the particular people that d o the \nrelevance judgments, as their personal notion of re levance \ncould be more beneficial for some systems than for others \n[13], let alone if the ground truth data has incons istencies. \nLikewise, if a pooling method were used, systems mo re \nalike would reinforce each other, while a system wi th a \nnovel technology would not be able to contribute th at much \nto the pool: it is more likely for the former syste ms to have \nmore of their documents included in the pool than f or the \nlatter [62]. In general, the non-relevancy assumpti on affects \nboth the measures [40] and the overall results [9].   \nThe particular queries used could also be unfair if  some \nsystems were not able to fully exploit their charac teristics. \nThis is of major importance for machine learning ta sks \nwhere systems are first tuned with a training colle ction: if \nthe query characteristics were very different betwe en the \ntraining and evaluation collections, systems could be \nmisguided. On the other hand, if the same collectio ns were \nused from year to year, an increase in performance could be \njust due to overfitting and not to a real improveme nt [54]. \nAlso, some evaluation measures could be unfair to s ome \nsystems if accounting for information they cannot p rovide. \n3.6  External Validity \nExternal validity evaluates the extent to which the  results of \nan experiment can be generalized to other populatio ns and \nexperimental settings. For example, a study on the effects of \nsome cancer treatment would not have external valid ity if \nmost patients in the sample were teenage males, as it would \nnot be clear what the effect of the drug is in, say , elder \nwomen. Thus, and experiment acquires external valid ity by \ncareful experimental design and justification of sa mpling \nand selection methods. \nThis is probably the weakest point of IR evaluation  [54]. \nAs mentioned, it is very important that the documen t \ncollection and query set is representative of the d omain \nbeing studied. On the other hand, having large coll ections \nmeans that the completeness of the ground truth is \ncompromised: it is just not feasible to judge every  query-\ndocument pair [8][62]. As mentioned, the usual solu tion is \nto pool the first k results of the participating systems and \njudge only those, assuming that all others are not relevant. \nThis is an obvious problem because the very test co llection \n(documents, queries and ground truth), which is in its own a \nproduct of the experiment, might not be reusable fo r \n611Oral Session 6: Databases and Evaluation\nsubsequent evaluations of new systems [14][15]. The  \nvalidity of the latter experiments could be comprom ised. \nLikewise, it is not justified to compare two system s \nevaluated with different test collections, because the results \nin each case are very dependent on the query set, r elevance \njudgments, measures, etc. [6][54]. Indeed, it is kn own that \ndifferent systems can perform very differently when  \nevaluated with different collections, especially if  machine \nlearning techniques are involved. This highlights t he lack of \nexternal validity in IRE experiments, and the impor tance of \nalways interpreting the results in terms of pairwis e system \ncomparisons rather than absolute performance figure s [54]. \nThat is, comparisons across collections and claims about the \nstate of the art based on a single collection, are not justified. \nNonetheless, very rough comparisons between two sys tems \nacross collections could be made if reporting the r esults of \nwell-established baseline systems for those collect ions and \ntheir relative difference with the systems of inter est [2]. \n3.7  Conclusion Validity \nConclusion validity evaluates the extent to which t he \nconclusions drawn from the results of an experiment  are \njustified. For example, a study might claim that pe ople has \nbetter access to the Internet in China than in the U.S. \nbecause there are more users connected, when in fac t the \npercentage of people connected, over the total popu lation, is \nmuch less. Thus, an experiment acquires content val idity by \ncareful selection of the measuring instruments and the \nstatistical methods used to draw de grand conclusio ns. \nTwo important characteristics of the effectiveness \nmeasures used in IR Evaluation are their stability and \nsensitivity. The results should be stable under dif ferent \nconditions, such as relevance judgments made by dif ferent \npeople or different sets of queries, so the results  do not  vary \nsignificantly and alter the conclusions as to what systems \nare better [7]. Also, they are desired to discrimin ate between \nsystems if they actually perform differently [55][3 9], and to \ndo so with the minimum effort [41]. Likewise, they are \ndesired to not discriminate between systems that ac tually \nperform very similarly. Note that these performance  \ndifferences must be considered always in the contex t of the \ntask and its underlying user model. \nGiven a set of systems and the scores they obtained  for \ndifferent queries according to some measure, they a re \nusually compared in terms of their mean effectivene ss score. \nNot until recently, statistical methods have been \nsystematically employed and analyzed to compare sys tems \nby their score distribution rather than just their mean score \n[43][58]. At this point, it is very important to in terpret \ncorrectly the results and understand the very issue s of \nhypothesis testing; and most importantly, distingui sh \nbetween statistical and practical significance: eve n if one \nsystem is found to be significantly better than ano ther one, \nthe difference might be extremely small to be notic ed by \nusers. In fact, the tiniest practical difference wi ll turn out \nstatistically significant with a sufficient number of queries.  4.  CHALLENGES IN MUSIC IR EVALUATION \nResearch in IR follows a cycle that ultimately lead s to the \ndevelopment of better systems. First, in the Develo pment \nphase researchers build a system for a particular t ask, and to \nassess how good it is, there is an Evaluation phase . Once the \nexperiments are finished, researchers then enter a phase of \nInterpretation of the results, which leads to a pha se of \nLearning why the system worked well or bad and unde r \nwhat circumstances. Finally, with the new knowledge  \ngained researchers get into an Improvement phase to  try and \nmake their system better, going back over to the Ev aluation \nphase. Unfortunately, current evaluation practices in Music \nIR seem to fall short in this cycle. \nDevelopment.  The task intent and its underlying user \nmodel are sometimes unclear or its real-world appli cability \nuncertain. For instance, is it realistic that while  the queries \nto the Query by Humming task are in audio format, t he \ndocument collection is in symbolic form? Or, in the  \nsimilarity tasks, is it realistic that the queries are actual \nitems contained in the collection? Likewise, are 30  second \nclips realistic for all tasks? \nEvaluation.  Several tasks, such as Audio Chord Detection \nor Symbolic Melodic Similarity, use document collec tions \neither too small or biased toward some genre or tim e period \n[46][48], which jeopardizes the validity of the res ults. \nMoreover, the lack of standardized and public colle ctions \nresults in research groups using their personal, pr ivate, often \nundescribed and rarely analyzed collections, which \nprecludes other researchers to compare systems or v alidate \nand replicate results, hindering the overall develo pment of \nthe field and often leading to wrong conclusions. I n this line, \nthe lack of standard evaluation software that all r esearchers \ncan use, thus minimizing the likelihood of bugs and  \nincorrect results, should be addressed too, especia lly with \nnew or undocumented measures specific of Music IR. \nInterpretation.  Some effectiveness measurers, such as \nNormalized Recall at Group Boundaries, are used wit hout \ndescription, references or source code, making them  \nimpossible to interpret or use in private evaluatio ns. Also, \nwidely-accepted baseline systems are very rarely in cluded in \nevaluations, and when they are, they use to be impl emented \nas random systems, having no useful value as a lowe r bound \nto which compare new systems. Another point that ne eds \ndiscussion is the set of statistical procedures use d, or the \nlack thereof. Given the small-scale evaluations usu ally \ncarried out in the Music IR field, it is imperative  that \nstatistical significance procedures be used, and ce rtainly that \nthe ones used are thoroughly selected and analyzed,  for \nwrong conclusions can easily be drawn from incorrec t \nprocedures or incorrect interpretation [50]. \nLearning.  When the results of an evaluation experiment \nare calculated and interpreted, the next step would  be to \nfigure out what happened and for what reasons. But there is \na great problem here: most of the times the raw mus ical \nmaterial is not available to experimenters, the act ual queries \nused are unknown, and not even their characteristic s are \n61212th International Society for Music Information Retrieval Conference (ISMIR 2011)\npublished. Researchers cannot analyze the evaluatio n results \nand improve their systems: if they had very bad res ults for \nsome queries, there is no way of knowing why. They can \nonly use their private collections over and over ag ain, \nultimately leading to overfitting and misleading re sults. \nImprovement.  There is another reason why researchers \nare forced to use their private collections all alo ng: current \ntest collections put together in collective evaluat ion forums \nare hardly reusable. As seen, the incompleteness of  ground \ntruth data depends largely on the number of partici pating \nsystems, and with the current low participation lev el, a new \nsystem would be highly penalized with the collectio n as is. \nThe reusability is of course null if these data wer e not \npublicly available, as happens with some tasks. As such, \nresearchers have no option but to blindly improve t heir \nsystems and wait for another evaluation round, with  no way \nof comparing cross-edition results due to the lack of data. \n5.  OPPORTUNITIES IN MUSIC IR EVALUATION \nAlthough not easily, these shortcomings of current \nevaluation practices in Music IR can be overcame. T o this \nend, we list several proposals to ease the way thro ugh the IR \nresearch and development cycle. \nCollections.  The document collections need to be large, \nmove beyond the handful of songs currently being us ed in \nseveral tasks; and try to include heterogeneous mat erial in \nterms of genre, time period, artist, etc. This is n ot hard to \nachieve, but when making such a collection open to other \nresearchers, copyright issues immediately arise [21 ]. A \npossibility is to publish feature vectors and metad ata, such \nas in the recent Million Song Dataset [5], although  this still \nposes problems if researchers wanted to study a new  feature \nor analyze specific items for which their system wo rked \nbetter or worse. In any case, these collections sho uld be \nstandard and used throughout the community, across tasks if \npossible, for a better comparison and understanding  of the \nimprovements between systems. \nRaw Data.  For a successful execution of the Learning \nand Improvement phases, raw musical material is nee ded. \nAn alternative is to use music free of copyright re strictions, \nsuch as that provided by services like Jamendo, but  the \npossible biases this might introduce are subject fo r further \nresearch. In this line, the use of artificial mater ial, such as \nsynthesized or error-mutated queries, should be rev ised [37]. \nEvaluation Model.  Having publicly accessible and \nstandardized collections would allow for a change i n the \ncurrent execution model employed in MIREX. Research ers \ncould be in charge of executing their systems and p roducing \nthe runs to submit back to MIREX, relieving them fr om a \ngood deal of workload and bringing researchers relu ctant to \ngive their algorithms away to third parties. This d ata-to-\nalgorithm model is used in the recent MusiCLEF foru m \n[32], and in fact it is the only viable way of movi ng to large \nscale evaluations, not only in terms of data but al so in terms \nof wider participation. The current algorithm-to-da ta model \nis in our view unsustainable in the long run, let a lone if IMIRSEL finally stops receiving funds [24], and pla tforms \nlike MIREX-DIY under NEMA [61] would still not perm it a \nfull execution of the IR cycle. \nOrganization.  The current organization of MIREX rests \nheavily on the IMIRSEL team, who plan, schedule and  run a \ngood number of tasks each year. We propose a 2 nd  tier \norganization below, for each particular task, and b y leading \nthird-party researchers. These organizers would dea l with all \nthe logistics, planning, evaluation, troubleshootin g and so \non, diminishing the workload of IMIRSEL, which woul d act \nas a sort of steering meta-organization tier provid ing the \nnecessary resources and general planning. This is t he format \nsuccessfully adopted by major Text IR forums like T REC or \nCLEF, which has helped in smoothing the process and  \ndeveloping tasks to push the state of the art in ea ch edition. \nSpecific Methodologies.  Both new methodologies \n[46][48][27][22] and effectiveness measures [47] ha ve been \nproposed for Music IR tasks, needing meta-evaluatio n \nstudies in the near future to keep improving the ev aluations. \nSome work has studied the reduction of effort neede d to \nevaluate through the use of crowdsourcing platforms  \n[49][33], and further studies should follow this li ne given \nthe usual restrictions the Music IR field has as to  availability \nof resources. Another line is the study of human ef fects on \nground truth data and evaluation results [28]. \nOverview Publications.  The organization proposal would \nalso benefit the community if by the end of each MI REX \nedition the organizers published an overview paper \nthoroughly detailing the process followed, data, re sults, and \ndiscussion to boost the Interpretation and Learning  phases. \nSuch a publication would be the perfect wrap-up to the \nparticipant-papers that describe the systems but ra rely \ninvestigate and elaborate on the results. In fact, many of \nthese participant-papers are not even drafted.  \nSoftware Standardization.  It is not rare to find incorrect \nevaluation results due to software bugs. With the \ndevelopment and acceptance of a software package to  \nevaluate systems we would gain in reliability withi n and \nbetween research groups, speeding up experiments an d \nguiding novice researchers. Also, it would further serve as \ndocumentation of the measures and processes used, f or the \nimplementation of some details is unknown or subjec t to \ndifferent interpretations; and it would call for th e \nstandardization of data formats to speed up the IR cycle.  \nBaselines.  The establishment of baseline systems to serve \nas a lower bound on effectiveness would help in ass essing \nthe overall progress in the field. With the standar dization of \nformats, public software, public collections of raw  music \nmaterial and the supervision of task-specific organ izers, the \ninclusion of baselines in these experiments would g reatly \nbenefit the execution of the IR cycle and the measu rement \nof the state of the art. \nCommitment.  In general, the current problems of Music \nIR Evaluation need to be acknowledged by researcher s. \nNow that we have a well-established evaluation foru m like \nMIREX, we need to start questioning the validity of  the \n613Oral Session 6: Databases and Evaluation\nexperiments, with the sole purpose of making them b etter \nand more striking. Current IR experiments seem to s top at \nthe Evaluation phase of the IR cycle, but the next phases are \noften ignored or impossible to engage into. \n6.  CONCLUSIONS \nWe have presented a survey of the Text IR literatur e on \nstudies tackling the problem of IR Evaluation exper iments. \nFrom the point of view of the analysis of experimen tal \nvalidity, this survey shows different aspects of IR  \nEvaluation that have been overlooked and need speci al \nattention in the Music IR domain. From the point of  view of \nthe IR research and development cycle a researcher follows \nin Music IR, we have also shown that current evalua tion \npractices force researchers to stop early in the cy cle. \nEvaluation experiments release good amounts of numb ers \nand plots, but there is a lack of proper interpreta tion and \ndiscussion due in part to the lack of public and st andardized \nresources, usually leaving researchers blind to imp rove their \nsystems. In this line, several proposals are made t o engage \nresearchers in these last phases of the cycle, whic h should \nultimately lead to a more rapid development of the field. \nWe hope this paper makes the case for MIR Meta-\nEvaluation studies and the fact that they are  actual MIR \nresearch, playing a central role in which researche rs should \nengage to begin a tradition of evaluation articles in ISMIR. \nREFERENCES \n[1] Alonso et al., Can We Get Rid of TREC assessors ? Using Mechanical Turk for \nRelevance Assessment, SIGIR Workshop on the Future of IR Evaluation , 2009. \n[2] Armstrong et al., Improvements that Donâ€™t Add U p: Ad-Hoc Retrieval Results \nsince 1998, CIKM , 2009. \n[3] Bailey et al., Relevance Assessment: Are Judges  Exchangeable and Does it \nMatter?, SIGIR , 2008. \n[4] Bennett et al., Beyond Binary Relevance: Prefer ences, Diversity and Set-Level \nJudgments, SIGIR Forum , 2008. \n[5] Bertin-Mahieux et al., The Million Song Dataset , ISMIR , 2011. \n[6] Bodoff et al., Test Theory for Assessing IR Tes t Collections, SIGIR , 2007. \n[7] Buckley et al., Evaluating Evaluation Measure S tability, SIGIR , 2000. \n[8] Buckley et al., Retrieval Evaluation with Incom plete Information, SIGIR , 2004. \n[9] Buckley et al., Bias and the Limits of Pooling for Large Collections, Journal of \nIR , 2007. \n[10] Carterette et al., Minimal Test Collections fo r Retrieval Evaluation, SIGIR , 2006. \n[11] Carterette, Robust Test Collections for Retrie val Evaluation, SIGIR , 2007. \n[12] Carterette et al., If I Had a Million Queries,  ECIR , 2009. \n[13] Carterette et al., The Effect of Assessor Erro r on IR System Evaluation, SIGIR , \n2010. \n[14] Carterette et al., Measuring the Reusability o f Test Collections, WSDM , 2010. \n[15] Carterette et al., Reusable Test Collections T hrough Experimental Design, SIGIR , \n2010. \n[16] Carterette, System Effectiveness, User Models,  and User Utility: A General \nFramework for Investigation, SIGIR , 2011. \n[17] Carvalho et al., Crowdsourcing for Search Eval uation, SIGIR Forum , 2010. \n[18] Cleverdon, The Significance of the Cranfield T ests on Index Languages, SIGIR , \n1991. \n[19] Downie, Interim Report on Establishing MIR/MDL  Evaluation Frameworks: \nCommentary on Consensus Building, ISMIR Panel on Music Information \nRetrieval Evaluation Frameworks , 2002. \n[20] Downie, The MIR/MDL Evaluation Project White P aper Collection, 3 rd ed, 2003. \n[21] Downie, The Scientific Evaluation of Music Inf ormation Retrieval Systems: \nFoundations and Future, Computer Music Journal , 2004. \n[22] Downie et al., Audio Cover Song Identification : MIREX 2006-2007 Results and \nAnalysis, ISMIR , 2008. \n[23] Downie et al., The Music Information Retrieval  Evaluation eXchange: Some Observations and Insights, in Advances in Music IR , Springer, 2010. \n[24] Downie, MIREX Next Generation, music-ir email list , 2011. Available at: \nhttp://listes.ircam.fr/wws/info/music-ir. \n[25] Guiver et al., A Few Good Topics: Experiments in Topic Set Reduction for \nRetrieval Evaluation, ACM Trans. Inf. Sys. , 2009. \n[26] Harman, Information Retrieval Evaluation, Synthesis Lectures on Information \nConcepts, Retrieval, and Services , 2011. \n[27] Hu et al., The 2007 MIREX Audio Mood Classific ation Task: Lessons Learned, \nISMIR , 2008. \n[28] Jones et al., Human Similarity Judgments: Impl ications for the Design of Formal \nEvaluations, ISMIR , 2007. \n[29] JÃ¤rvelin et al., Cumulated Gain-Based Evaluati on of IR Techniques, ACM Trans. \nInf. Sys. , 2002. \n[30] Katzer et al., Evaluating Information: A Guide  for Users of Social Science \nResearch, 4th ed., 1998. \n[31] KekÃ¤lÃ¤inen, Binary and Graded Relevance in IR Evaluations: Comparison of the \nEffects on Ranking of IR Systems, Inf. Proc. Mngt. , 2005. \n[32] Lartillot et al., MusiClef: A Benchmark Activi ty in Multimodal Music \nInformation Retrieval, ISMIR , 2011. \n[33] Lee, Crowdsourcing Music Similarity Judgments using Mechanical Turk, \nISMIR , 2010. \n[34] Marchionini, Exploratory Search: from Finding to Understanding, \nCommunications of the ACM , 2006. \n[35] Mitchell et al., Research Design Explained, 7 th ed., 2009. \n[36] Montgomery, Design and Analysis of Experiments , 7 th ed., 2009. \n[37] Niedermayer et al., On the Importance of â€˜Real â€™ Audio Data for MIR Algorithm \nEvaluation at the Note-Level: A comparative Study, ISMIR , 2011. \n[38] Robertson, On the Contributions of Topics to S ystem Evaluation, ECIR , 2011. \n[39] Sakai, On the Reliability of Information Retri eval Metrics Based on Graded \nRelevance, Inf. Proc. Mngt. , 2007. \n[40] Sakai et al., On Information Retrieval Metrics  Designed for Evaluation with \nIncomplete Relevance Assessments, Journal of IR , 2008. \n[41] Sanderson et al., Information Retrieval System  Evaluation: Effort, Sensitivity, \nand Reliability, SIGIR , 2005. \n[42] Sanderson et al., Do User Preferences and Eval uation Measures Line Up?, \nSIGIR , 2010. \n[43] Smucker et al., A Comparison of Statistical Si gnificance Tests for Information \nRetrieval Evaluation, CIKM , 2007. \n[44] Soboroff et al., Ranking Retrieval Systems Wit hout Relevance Judgments, \nSIGIR , 2001. \n[45] Turpin et al., Why Batch and User Evaluations Do Not Give the Same Results, \nSIGIR , 2001. \n[46] Typke et al., A Ground Truth for Half a Millio n Musical Incipits, Journal of \nDigital Inf. Mngt. , 2005. \n[47] Typke et al., A Measure for Evaluating Retriev al Techniques based on Partially \nOrdered Ground Truth Lists, IEEE Int. Conf. on Multimedia and Expo , 2006. \n[48] Urbano et al., Improving the Generation of Gro und Truths based on Partially \nOrdered Lists, ISMIR , 2010. \n[49] Urbano et al., Crowdsourcing Preference Judgme nts for Evaluation of Music \nSimilarity Tasks, SIGIR Workshop Crowdsourcing for Search Evaluation , 2010. \n[50] Urbano et al., Audio Music Similarity and Retr ieval: Evaluation Power and \nStability, ISMIR , 2011. \n[51] Voorhees, Variations in Relevance Judgments an d the Measurement of Retrieval \nEffectiveness, Inf. Proc. Mngt. , 2000. \n[52] Voorhees, Evaluation by Highly Relevant Docume nts, SIGIR , 2001. \n[53] Voorhees, Whither Music IR Evaluation Infrastr ucture: Lessons to be Learned \nfrom TREC, in [20], 2002. \n[54] Voorhees, The Philosophy of Information Retrie val Evaluation, CLEF , 2002. \n[55] Voorhees et al., The Effect of Topic Set Size on Retrieval Experiment Error, \nSIGIR , 2002. \n[56] Voorhees et al., TREC: Experiment & Evaluation  in Information Retrieval, 2005. \n[57] Webber et al., Precision-At-Ten Considered Red undant, SIGIR , 2008. \n[58] Webber et al., Statistical Power in Retrieval Experimentation, CIKM , 2008. \n[59] Yilmaz et al., Estimating Average Precision wi th Incomplete and Imperfect \nInformation, CIKM , 2006. \n[60] Yilmaz et al., A Simple and Efficient Sampling  Method for Estimating AP and \nNDCG, SIGIR , 2008. \n[61] Zhu et al., MIREX-DIY under NEMA, ISMIR , 2010. \n[62] Zobel, How Reliable are the Results of Large-S cale Information Retrieval \nExperiments?, SIGIR, 1998. \n614"
    },
    {
        "title": "Audio Music Similarity and Retrieval: Evaluation Power and Stability.",
        "author": [
            "JuliÃ¡n Urbano",
            "Diego MartÃ­n 0001",
            "MÃ³nica Marrero",
            "Jorge Morato"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1417269",
        "url": "https://doi.org/10.5281/zenodo.1417269",
        "ee": "https://zenodo.org/records/1417269/files/UrbanoMMM11.pdf",
        "abstract": "In this paper we analyze the reliability of the results in the evaluation of Audio Music Similarity and Retrieval systems. We focus on the power and stability of the evaluation, that is, how often a significant difference is found between systems and how often these significant differences are incorrect. We study the effect of using different effectiveness measures with different sets of relevance judgments, for varying number of queries and alternative statistical procedures. Different measures are shown to behave similarly overall, though some are much more sensitive and stable than others. The use of different statistical procedures does improve the reliability of the results, and it allows using as little as half the number of queries currently used in MIREX evaluations while still offering very similar reliability levels. We also conclude that experimenters can be very confident that if a significant difference is found between two systems, the difference is indeed real.",
        "zenodo_id": 1417269,
        "dblp_key": "conf/ismir/UrbanoMMM11",
        "keywords": [
            "reliability",
            "evaluation",
            "Audio Music Similarity",
            "Retrieval systems",
            "power",
            "stability",
            "effectiveness measures",
            "relevance judgments",
            "statistical procedures",
            "experimenters"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nAUDIO MUSIC SIMILARITY AND RETRIEVAL: \nEVALUATION POWER AND STABILITY \n \nJuliÃ¡n Urbano, Diego MartÃ­n, MÃ³nica Marrero and Jor ge Morato \nUniversity Carlos III of Madrid \nDepartment of Computer Science \n{jurbano, dmandres, mmarrero, jmorato}@inf.uc3m.es  \n \nABSTRACT \nIn this paper we analyze the reliability of the res ults in the \nevaluation of Audio Music Similarity and Retrieval systems. \nWe focus on the power and stability of the evaluati on, that \nis, how often a significant difference is found bet ween sys- \ntems and how often these significant differences ar e incor- \nrect. We study the effect of using different effect iveness \nmeasures with different sets of relevance judgments , for \nvarying number of queries and alternative statistic al proce- \ndures. Different measures are shown to behave simil arly \noverall, though some are much more sensitive and st able \nthan others. The use of different statistical proce dures does \nimprove the reliability of the results, and it allo ws using as \nlittle as half the number of queries currently used  in MIREX \nevaluations while still offering very similar relia bility levels. \nWe also conclude that experimenters can be very con fident \nthat if a significant difference is found between t wo systems, \nthe difference is indeed real.  \n1.  INTRODUCTION \nOne of the most important tasks in Music Informatio n Re- \ntrieval is Audio Music Similarity and Retrieval (AM S). \nAlong with Symbolic Melodic Similarity (SMS), AMS i s \none of the traditional tasks evaluated in the annua l Music \nInformation Retrieval Evaluation eXchange (MIREX) [ 3], \nand one of the tasks that most closely resemble a r eal-world \nmusic retrieval scenario. A music similarity retrie val system \nreturns a ranked list of music pieces deemed to be similar to \na music piece given as a query. In the case of the MIREX \nevaluation of AMS, these music pieces are 30 second  audio \nclips of music material. \nAs of the writing of this paper, a total of 41 AMS systems \nhave been evaluated in 4 editions of MIREX from 200 6 to \n2010, and it is again planned for 2011. In these ev aluations, \na set of queries is randomly selected and provided to the \nparticipating systems, which then return the corres ponding 5 \nmost similar music pieces in a music collection. To  evaluate \nthe effectiveness of the systems two things are nee ded: rele- vance judgments and effectiveness measures. The rel evance \njudgments are scores given to each query-candidate pair, \nrepresenting their similarity. Two relevance scales  are used \nin MIREX for both the AMS and SMS tasks. The Broad \nscale has three levels: not similar (NS = 0), somew hat simi- \nlar (SS = 1) and very similar (VS = 2). The Fine sc ale uses \nreal valued scores between 0.0 (not similar at all)  and 10.0 \n(identical). As to the effectiveness measures, in A MS the so-\ncalled Sum measure is used, while more complex meas ures \nwere developed for the SMS task [11]. \nThe grand results of these evaluations are pairwise  com- \nparisons between the participating systems, indicat ing which \nis better and whether the difference is statistical ly significant \nor not. When drawing such conclusions, two characte ristics \nof the evaluation must be kept in mind: power and s tability. \nPower refers to how powerful the evaluation is to e stablish a \nsignificant difference between any two systems (i.e . it is \nconcerned with Type II errors). If A is concluded to perform \nsignificantly better than B, the evaluation is considered \npowerful. If the difference were not statistically significant, \nno clear conclusion could be drawn from the experim ent: A \nand B could actually perform identically (very unlikely) , or \nthe evaluation conditions might have not been suffi cient to \nobserve a difference large enough (most likely). As suming \ntwo systems A and B are never exactly the same, an option \nto achieve significance is to increase the number o f queries, \nthough this has obvious limitations in terms of eff ort and \ncost [16][8]. The difference between practical and statisti- \ncally significant differences must be considered if  doing so. \nStability refers to how reliable a result is when c laiming a \nstatistically significant difference between two sy stems (i.e. \nit is concerned with Type I errors). If A and B were evaluat- \ned with a set of queries and the result were that A is signifi- \ncantly better than B, the expected result with a completely \ndifferent (and independent) query set would therefo re be \nthat A is again significantly better than B. If it were not, it \nwould be an indication that the evaluation is not s table when \ndifferentiating between systems. These conflicts do  appear \nin IR evaluation experiments, and if the query set used is too \nsmall, the effectiveness measures not appropriate o r the sta- \ntistical procedures not suitable, they can be frequ ent [1]; \neven when statistical significance is involved [15] . \nIn this paper we analyze the power and stability of  the \nAMS evaluation methodology when concluding that a s ys- \ntem A is significantly better than a system B. We analyze  \nPermission to make digital or hard copies of all or  part of this work for \npersonal or classroom use is granted without fee pr ovided that copies are \nnot made or distributed for profit or commercial ad vantage and that copies \nbear this notice and the full citation on the first  page.  \nÂ© 2011 International Society for Music Information Retrieval  \n597Oral Session 6: Databases and Evaluation\nthe effect that different relevance judgment sets, effective- \nness measures, query set sizes and statistical proc edures \nhave on the reliability of the AMS results. For thi s study we \ndecided to use the MIREX 2009 Audio Music Similarit y \nand Retrieval data, as it is the largest dataset av ailable to \ndate [4]. A total of 15 systems by 9 different rese arch groups \nwere evaluated with a total of 100 queries. The top  5 docu- \nments retrieved by each system were evaluated for e ach \nquery using the Broad and Fine scales, and the Sum measure \nwas used with these two sets of relevance judgments  to as- \nsess the effectiveness of the systems. The Friedman  test was \nran with a Tukeyâ€™s HSD post-hoc correction procedur e to \nlook for significant differences. The grand results  of the \nevaluation are thus 105 pairwise comparisons betwee n sys- \ntems, some of which are statistically significant. \nThe rest of the paper is organized as follows. Sect ion 2 \nreviews previous work on the analysis of power and stability \nin TREC and related studies on the evaluation of mu sic sim- \nilarity tasks. Next we discuss the effectiveness me asures \nconsidered, and Sections 4 and 5 present the result s of the \npower and stability analysis. Section 6 argues and analyzes \nthe use of different statistical procedures. Finall y, Section 7 \npresents a discussion of the results and the paper then fin- \nishes with the conclusions and lines for further wo rk.  \n2.  RELATED WORK \nThe stability of effectiveness measures has been ex tensively \nstudied in the context of the Text REtrieval Confer ence \n(TREC). Buckley and Voorhees first studied the stab ility of \nseveral measures, observing conflicts between 1% an d 14% \nof the times, depending on the measure, when compar ing \nany two systems [1]. They then studied the sensitiv ity of \nseveral measures as a function of the query set siz e, and they \nconcluded that absolute differences larger than 0.0 5 (about \n25% relative difference) are necessary for sets of 50 queries \nto assure a conflict ratio below 5% [16], confirmin g the reli- \nability of TREC evaluations for using 50 queries as  a mini- \nmum. However, none of these studies considered the effect \nof using statistical significance techniques when c omparing \ntwo systems. Sanderson and Zobel somehow filled thi s gap \nby studying the effect of several statistical proce dures on the \nsensitivity, and they concluded that virtually any relative \ndifference of 10% or more, coupled with statistical  signifi- \ncance, will not produce a conflict in other experim ents [8]. \nSakai reviewed most of this work with different dat a sets \nand with other, more recent measures [7]. With larg er query \nsets, Voorhees found that even significant differen ces could \nstill be conflictive [15]. However, the study of po st-hoc sta- \ntistical procedures was not part of any of these st udies. \nMeta-evaluation studies are very rare in Music IR [ 12], \nand to our knowledge the power and stability issues  have \nnot yet been studied for MIREX data. Nonetheless, s ome \nworks have addressed similar problems with Music IR  eval- \nuation experiments concerning the similarity tasks.  Typke et \nal. studied alternative forms of relevance judging for the SMS task [10], and they came up with a specific eff ective- \nness measure to be used with them [11]. Urbano et a l. then \nshowed how to make the evaluation more reliable whe n \nusing those relevance judgments [13]. Jones et al. studied \nthe relevance judgments made for the SMS and AMS ta sks, \nfocusing on the effect of having different people d o the \njudgments and with different scales. To reduce the cost of \njudging, the use of crowdsourcing platforms such as  Ama- \nzon Mechanical Turk has been studied by Urbano et a l. for \nthe SMS task [14], and by Lee for the AMS task [6].  In this \npaper we focus on the power and stability of the MI REX \nAMS evaluations, employing techniques similar to Bu ck- \nleyâ€™s and Voorheesâ€™, but with some modifications sp ecific \nto the AMS task and the post-hoc analysis used in M IREX. \n3.  EFFECTIVENESS MEASURES \nThe MIREX AMS evaluation campaigns use just one mea s- \nure to assess the effectiveness of the participatin g systems. \nThis is the so-called Sum measure, which is the ave rage \nrelevance of the retrieved results. When used with the Broad \njudgments, this measure is often called PSum; and w hen \nused with the Fine judgments, it is called FINE [3] . \nThe Audio community has traditionally been reluctan t to \nadopt more complex measures, even some specifically  de- \nsigned for this type of tasks [3]. In this paper we  study the \nuse of several of these measures in the Audio Music  Simi- \nlarity task, and their impact on the power and stab ility of the \nevaluation. First, we review the measures considere d. \n3.1  Average Gain \nThis measure is based on the concept of information  gain \nprovided by the retrieved documents. This informati on gain \nis usually represented by the relevance level assig ned to the \ndocument, assuming that the larger the score, the m ore in- \nformation is gained by the user. \nG@k is the Gain of the k-th document retrieved, and \n1@ @ k\niCG k G i ==âˆ‘  is the Cumulated Gain of the first k \ndocuments retrieved [5]. Thus, the Average Gain of the top-\nk documents is calculated as the mean: \n 1@ 1 @ @ k\niCG k AG k G i k k == = âˆ‘  (1) \nThis is the official measure used in the MIREX AMS task. \nWe prefer to use this definition based on informati on gain \nfor consistency with the other measures. \nThe problem of G, CG and AG is that they do not hav e a \nfixed upper bound, which causes some problems when aver- \naging the results across queries. Consider a query q1 for \nwhich there are 7 VS documents and another query q2 with \n2 VS and 5 SS documents. For q1 a perfect system can \nachieve a total CG@5 score of 10, while for q2 the maxi- \nmum possible is 7. Apparently, the system performs better \nfor q1, when in reality it returns ideal results for both  que- \nries. As with other simpler measures such as Precis ion, this \nlack of fixed upper bound makes them less stable [1 ][7]. \n59812th International Society for Music Information Retrieval Conference (ISMIR 2011)\n3.2  Normalized Discounted Cumulated Gain \nAG does not consider the rank at which documents ap pear \ndown the results list: a document at rank 3 provide s as much \ngain as if it were at ranks 1 or 5. However, a high ly relevant \ndocument is clearly more useful to the user if it a ppeared \ntoward the top of the list. To model this usefulnes s, the gain \nscores are discounted as they appear later in the r esults list. \nA logarithm function with base b is used, and so the Dis- \ncounted Cumulated Gain is defined recursively as: \n @\n@ @@( 1) log bCG k k b \nDCG k G k DCG k k b k< ï£±\nï£´=ï£²âˆ’ + â‰¥ ï£´ï£³ (2) \nAlso, to avoid the lack of fixed upper bound proble m, it \nis considered what the ideal ranking of documents w ould be: \n@ @ .. : @ @( 1) IDCG k DCG k st i k G i G i = âˆ€ < â‰¥ + . Divid- \ning the DCG@k score of the system by the ideal IDCG @k, \nthe upper bound is always 1, meaning perfect retrie val: \n @@@DCG k NDCG k IDCG k =  (3) \nThis measure is called Normalized Discounted Cumula t- \ned Gain [5], which has been shown to be particularl y stable \nand sensitive [7][17]. For this study we set the lo garithm \nbase to the standard b=2 . \n3.3  Average Normalized Discounted Cumulated Gain \nThe last measure of the information gain family we consider \nhere is the Average Normalized Discounted Cumulated  \nGain, which is calculated as the average NDCG score  \nthroughout the retrieved list: \n 11@ @ k\niANDCG k NDCG i k==âˆ‘  (4) \nANDCG provides more information about the ranking o f \nthe retrieved documents, as still quite large NDCG scores \ncould be achieved just by highly relevant documents  to- \nwards the end of the list. Like NDCG, this measure has been \nshown to be particularly stable and sensitive [7]. \n3.4  Average Dynamic Recall \nThe last measure we consider originated in the cont ext of \nthe MIREX 2005 SMS task and the evaluation with rel e- \nvance judgments in the form of partially ordered li sts \n[10][13]: Average Dynamic Recall [11]. ADR was spec ifi- \ncally designed for level-based relevance judgments without \na scale fixed beforehand, and ever since it is one of the main \nmeasures used in MIREX SMS with the Broad judgments . \nWe also define ADR in terms of information gain. Le t \n1,..., n I I I =âŒ© âŒª be the list of n judged documents ordered by \ndescending relevance level (i.e. an ideal ordering) , and let \n1,..., k R R R =âŒ© âŒª be the list of the top k â‰¤ n  retrieved docu- \nments ordered by rank. The set Ai of allowed relevant doc- \numents at rank i is defined as: \n 1{ ,..., } { : @ @ } i i j A I I I j i G j G i = âˆª > âˆ§ =  (5) that is, the union of all previous ideal documents and those \nwith lower rank but equal information gain (i.e. sa me rele- \nvance level). The final score is then calculated as : \n 1\n1{ ,..., } 1@k\ni i \niA R R ADR k k i =âˆ©=âˆ‘  (6) \nwhich is the average across ranks of the ratio of d ocuments \nretrieved that are actually in the ideal ranking. T his measure \nis widely used by the SMS community, but it has nev er been \nused in the AMS task, nor has it been analyzed in t erms of \npower or stability. In this paper we do so. \n4.  EVALUATION POWER \nTo assess the effect of different effectiveness mea sures and \nrelevance scales on the power of the evaluation, we  compute \nthe number of pairwise system comparisons that resu lt sig- \nnificant according to the Friedman-Tukeyâ€™s HSD (FT)  pro- \ncedure used in MIREX. We evaluate the original meas ure, \nAG, as well as NDCG, ANDCG and ADR; both with the \nBroad and Fine set of relevance judgments, for a to tal of 8 \ndistinct measures. \nWe study the trend for increasing query sets of siz es 5 to \n100, with increments of 5 queries each. To diminish  random \neffects when selecting a subset of queries for the 5 to 95 \nsizes, we choose 500 random samples in each case. T hus, \nthere are 52,500 system pairwise comparisons for ea ch \nmeasure and query subset size. Also, the queries in  MIREX \nwere balanced across music genres: the 100 original  queries \nwere selected from 10 different genres, with 10 que ries per \ngenre. We also reproduce this balance, using strati fied sam- \npling with equal priors when making query subsets. There- \nfore, our samples are also balanced across music ge nres, \nemulating as closely as possible a real MIREX evalu ation. \nAs Figure 1 shows, 57% of the results were signific ant \nusing AG Broad  and 54% using AG Fine  (horizontal dotted \nlines). We omitted query subset sizes below 40 for clarity: \nthe curves follow a somewhat logarithmic trend (see  the \nthumbnails for the whole plot). Indeed, it can be s een that \nthe increment in significant pairwise comparisons i s very \nsoft and quite similar for all measures but ADR Fine . \nThe right figure also shows that for larger query s ets \n(A)NDCG Fine  clearly outperform AG Fine , which seems to \nconverge. ADR Fine  performs quite poorly, following a \nsomewhat linear trend. This is expected though, as the con- \ntribution of each document retrieved is here binary : if a doc- \nument is allowed at rank i it contributes 1Â·ik + to the score, \n0 otherwise. In the (A)NDCG Fine  measures the contribution \nis discounted, but it is never binary. This makes A DR Fine  \nperform significantly worse. Nonetheless, it is imp ortant to \nnote that ADR was not intended for real valued rele vance \njudgments, which make it very difficult for two doc uments \nto have the same relevance score (right term in Equ ation 5) \nand thus it requires systems to obtain a nearly ide al ranking. \nMost importantly, it can be seen that the query set  size \ncould be significantly reduced to lower the cost of  the eval- \n599Oral Session 6: Databases and Evaluation\nuation in terms of relevance judging effort. For ex ample, \nhaving reduced the query set to 70 queries (70%) on ly 2 \nsignificant differences would have been missed if u sing \nAGBroad , none if using AG Fine . \n5.  EVALUATION STABILITY \nTo assess the effect that different effectiveness m easures \nand relevance scales have on the stability of the e valuation, \nwe need two different query sets, as if we were eva luating \nthe systems with two completely different collectio ns. Un- \nfortunately, having the same 15 systems with anothe r 100 \ncompletely different queries is not yet feasible. N onetheless, \nwe can use smaller query sets and then observe the trends to \nextrapolate to larger sets. We start with the 5,000  random \nquery subsets of sizes 5 to 50 used before. Then, f or each of \nthese we sample another query subset of the same si ze, \nagain stratified, but also without replacement. Tha t is, for \neach of the 500 trials of each of the 10 query subs ets, there \nare two query samples with no common query. Because  they \nare disjoint, we can treat them as if coming from t wo differ- \nent evaluation experiments. Note also that having a  total of \n100 queries limits the query subsets to 50 queries at most, as \nthe paired subset samples would contain the remaini ng 50 \nqueries in each case. \nWe re-evaluate the 15 systems for with each pair of  query \nsamples, and then compare the 105 system pairwise r esults \nfrom both samples. We count the number of times the re is a \nsignificant difference with one sample but not with  the other \none, again according to the original Friedman-Tukey â€™s HSD \nprocedure. These would represent stability conflict s across \ntwo real evaluations. \nAs Figure 2 shows, about 4% of the system pairwise \ncomparisons are conflicting with the Broad judgment s using \n40 queries or more, and as few as 3% with the Fine judg- \nments (dotted horizontal lines). This is consistent  with the \n5% significance level set for the statistical proce dure (see \nSection 6). Indeed, the curves tend to converge tow ard the \nend. It is noticeable again that ADR performs signi ficantly \nworse, especially for the Fine judgments, where the  increas- \ning conflict rates can be explained by the very low  sensitivi- \nty of the measure, as explained before. The other m easures \nbehave remarkably similarly. The peaks for small query subsets are explained by the \npower of the statistical procedures used: with that  few que- \nries the tests are not powerful enough to result si gnificant, \nand when they happen to do for one query sample the y still \ndo not for the other one. This increment in conflic ts starts \ndecreasing and converges because the tests get more  power- \nful with larger samples, so they are able to give s ignificance \nwith both query subsets. In fact, for AG with 50 qu eries all \nconflicts are caused by this lack of significance i n one of the \nsamples, 99.9% for NDCG and 99.7% for ANDCG; even \n99.7% for ADR Fine . Most importantly, there was no case \nwhatsoever where the two system pairwise comparison s \nwere significant but with opposite sign. As such, o ne can be \nquite confident about the difference between two sy stems \nwhen it comes up significant. \n6.  STATISTICAL ANALYSIS \nThe usual method to check whether two systems are s ignifi- \ncantly different or not is to run a statistical tes t such as the \nWilcoxon test or the t-test. Each of these has an a ssociated \nsignificance level, which is the maximum allowed pr obabil- \nity of committing a Type I error. In our case, thes e errors \noccur when the test says there is a significant dif ference but \nthere actually is none. This significance level use s to be set \nto Î±=0.05 or Î±=0.01. That is, a probability of 5% or 1% of \nincorrectly getting significant differences between  systems. \nIn the case of MIREX 2009 AMS, 105 of these pairwis e \ntests would need to be run. Unfortunately, if setti ng Î±=0.05 \nthe probability of committing a Type I error in any  of these \nwould be 1-(1-Î±)105 =0.995. This is the experiment-wide sig- \nnificance level. Thus, almost certainly we would at  least \nonce be saying that two systems are significantly d ifferent \nwhen they actually are not. In MIREX, the Friedman test is \nrun instead, with the Tukeyâ€™s HSD post-hoc procedur e for \nsignificance correction [3]. This compares all syst em pairs \nat once, with the difference that the experiment-wi de signif- \nicance level remains close to Î±=0.05. The test is thus much \nless likely to fail in one comparison, at the cost of being \nmuch more conservative and give fewer significant r esults \nin the first place [9]. Finally, we also note that while the \nFriedman test is used because it does not assume no rmality \nof the score distributions, Tukeyâ€™s HSD does assume  it. Figure 2.  Evaluation stability (lower is better) with FT , for all \nmeasures with the Broad (left) and Fine judgments ( right).  Broad judgments \nQuery subset size % Conflicting comparisons \n510 15 20 25 30 35 40 45 50 246810 12 14 16 18 20 22 AG \nNDCG \nANDCG \nADR Fine judgments \nQuery subset size % Conflicting comparisons \n510 15 20 25 30 35 40 45 50 246810 12 14 16 18 20 22 \nFigure 1.  Evaluation power (larger is better) with FT , for all \nmeasures with the Broad (left) and Fine judgments ( right).  Broad judgments \nQuery set size % Significant comparisons \n40 45 50 55 60 65 70 75 80 85 90 95 100 46 48 50 52 54 56 58 60 62 64 \nAG \nNDCG \nANDCG \nADR Fine judgments \nQuery set size % Significant comparisons \n40 45 50 55 60 65 70 75 80 85 90 95 100 46 48 50 52 54 56 58 60 62 64 \n60012th International Society for Music Information Retrieval Conference (ISMIR 2011)\nTukeyâ€™s HSD thus commits fewer Type I errors, but i n \nthe downside it is less powerful. We should at this  point \nconsider whether this is what we want. From the poi nt of \nview of the participants, what they are interested in is the \nsubexperiment  of comparing their system with the other 14, \nand the remaining 91 pairwise system comparisons ar e ra- \nther uninteresting for them. Therefore, why not per form \nthese simple 14 pairwise comparisons? The subexperi ment-\nwide significance level would be 1-(1- Î±)14 =0.512. Most \nimportantly, note that the number of pairwise syste m com- \nparisons grows quadratically in the whole experimen t but \nlinearly in the subexperiments. As such, for evalua tions with \nmany more systems the power would decrease drastica lly if \nusing Friedman-Tukeyâ€™s HSD. \n6.1  Evaluation Power \nHere we perform the same experiment as in Section 4  and \nwith the same query subsets, but instead of using F riedman-\nTukeyâ€™s HSD we perform the 105 pairwise system comp ari- \nsons using 1-tailed Wilcoxon tests at the Î±=0.01 significance \nlevel (W1). Therefore, the probability of committin g a Type \nI error for the complete experiment (in any of the 105 sys- \ntem comparisons) is 1-(1- Î±)105 =0.652, but for the subexper- \niments (14 system comparisons in each one) it is dr amatical- \nly reduced to 1-(1-Î±)14 =0.131. \nFigure 3 shows as expected that many more significa nt \ndifferences are found between systems: as much as 2 0% \nmore (the horizontal dotted lines mark the power ac hieved \nby the original evaluation). Interestingly, the dif ference be- \ntween AG and (A)NDCG is here more acute, and it get s \nlarger as more queries are used. The plots also sug gest that \nW1 with about half the queries can achieve the same  or bet- \nter power levels as the original evaluation with FT . \n6.2  Evaluation Stability \nAs expected, with simple Wilcoxon tests there are m any \nmore significant differences, but how many of them are ac- \ntually caused by mere Type I errors? We have shown that \nthe probability of having at least one incorrect re sult is very \nhigh, so next we look into stability. \nAs Figure 4 shows, the stability levels are very si milar. \nAG again converges at about 3.5% of stability confl icts, and \nit does so much earlier than in the original evalua tion. Most notably, (A)NDCG show here more stability conflicts , con- \nverging to about 6%. Note that the peaks observed f or small \nsubsets are here narrower because the statistical t ests are \nmore powerful in the first place. Again, ADR perfor ms \nworse, especially for the Fine judgments. \n7.  DISCUSSION \nTaking a close look at the power and stability resu lts, one \nmay wonder whether it is necessary to use as many a s 100 \nqueries. From a pragmatic point of view, we have ar gued \nthat simple 1-tailed Wilcoxon tests are more useful  to the \nMIREX participants than Friedman-Tukeyâ€™s HSD. Next,  we \nshow analytically that they are even more reliable and \ncheaper (see Table 1). \n 50 queries 100 queries \n Power  Conflicts  Stable Power Stable \nAG Broad (FT) 52.4%  3.6% 48.8% 57.1% 53.5% \nAG Fine (FT) 51.9%  3.2% 48.7% 54.3% 51.1% \nAG Broad (W1) 55.1%  3.7% 51.4% 59.0% 55.4% \nAG Fine (W1) 54.3%  3.3% 51.0% 60.0% 56.7% \nTable 1. Power and stability for 50 and 100 query sets when  using \nFriedman-Tukeyâ€™s HSD (FT) or 1-tailed Wilcoxon test s (W1). \nFor instance, with AG Fine  and 50 queries 51.9% of the \n105 pairwise comparisons are significant according to FT, \nbut 3.2% have a stability conflict. Thus, 48.7% of the com- \nparisons are both significant and stable. Assuming the ap- \nparent convergence of conflicting results, for 100 queries \nthere would be 51.1% significant and stable results . But also \nwith 100 queries, W1 is even more stable, and with as little \nas 50 queries it is as reliable as FT with the full  query set, \nhaving 51.0% of significant and stable results. (A) NDCG \nshow very similar results, with differences of abou t 2%. \nWe note again that very few of these conflicts are caused \nby a change in the sign of the difference between s ystems, \nand never is it found significant for both query sa mples. \nIndeed, 97.3% of the conflicts with AG were caused by \nmere lack of statistical power in one of the paired  query \nsamples, 96.7% with NDCG and 95.9% with ANDCG. \nAgain, this indicates that if significance is found , it most \nprobably is correct.  Figure 4.  Evaluation stability (lower is better) with W1, for all \nmeasures with the Broad (left) and Fine judgments ( right).  Fine judgments \nQuery subset size % Conflicting comparisons \n510 15 20 25 30 35 40 45 50 246810 12 14 16 18 20 22 Broad judgments \nQuery subset size % Conflicting comparisons \n510 15 20 25 30 35 40 45 50 246810 12 14 16 18 20 22 AG \nNDCG \nANDCG \nADR \nFigure 3.  Evaluation power (larger is better) with W1, for all \nmeasures with the Broad (left) and Fine judgments ( right).  Fine judgments \nQuery set size % Significant comparisons \n40 45 50 55 60 65 70 75 80 85 90 95 100 46 48 50 52 54 56 58 60 62 64 Broad judgments \nQuery set size % Significant comparisons \n40 45 50 55 60 65 70 75 80 85 90 95 100 46 48 50 52 54 56 58 60 62 64 \nAG \nNDCG \nANDCG \nADR \n601Oral Session 6: Databases and Evaluation\n8.  CONCLUSIONS AND FUTURE WORK \nWe have analyzed the MIREX Audio Music Similarity a nd \nRetrieval task in terms of power and stability of t he evalua- \ntions, studying four effectiveness measures (AG, ND CG, \nANDCG and ADR) with the two traditional sets of rel e- \nvance judgments employed in MIREX (Broad and Fine).  \nAbout 55% of the pairwise system comparisons come u p \nstatistically significant with current practices, w ith all \nmeasures but ADR behaving very similarly. The incre ase in \npower follows a logarithmic trend with the number o f que- \nries used, so merely using more queries to achieve signifi- \ncance does not pay off at some point. As to stabili ty, we \nobserved that about 4% of the pairwise system compa risons \nare unstable: with one test collection the differen ce would \nbe significant, but with a different collection it would not. \nHowever, less than 0.14% of these conflicts had a s wap in \nthe sign of the difference, and in no case was a si gn swap \ncoupled with significance in both query samples: at  worst, \nthey were too small to observe significance in both  evalua- \ntions. This indicates that if a significant differe nce is found \nbetween two systems, experimenters can be very conf ident \nthat the result is indeed correct and general. \nFrom the pragmatic point of view of a MIREX partici - \npant, we argue that the Friedman-Tukeyâ€™s HSD proced ure \nused to measure significance is not appropriate. In  fact, \ncomparing all system pairs with simple 1-tailed Wil coxon \ntests at the Î±=0.01 significance level we can obtain even \nmore reliability. Most importantly, we have shown t hat with \nthis procedure the query set can be cut in half, an d yet the \nreliability of the results would be as good as if u sing all 100 \nqueries and Friedman-Tukeyâ€™s HSD. This effectively reduc- \nes to 50% the effort needed for relevance judging, which is \nespecially appealing both for in-house evaluations with little \nresources and for the continuity of MIREX, given it s recent \nfunding issues [2]. Some of the spare effort could even be \ndedicated to the evaluation of more queries in the SMS task. \nFuture work will examine other test collections, us ed \nboth in audio and symbolic similarity retrieval. We  believe \nthat the similar behavior observed for AG, NDCG and  \nANDCG is due to the small evaluation depth: only th e top 5 \nresults per system are judged for relevance. Using \n(A)NDCG with the standard logarithm base 2, as we d id, \ntakes advantage of the ranking only beyond the seco nd doc- \nument retrieved. Just the top 5 documents might be too few \nto note the difference, so we also plan to study th e effect of \nevaluation depth in power and stability. The effect  of the \nnumber of systems is also subject for further resea rch, as it \naffects not only the statistical procedure but also  the evalua- \ntion of other systems through the discovery of more  relevant \nmaterial. Indeed, we expect to find different patte rns when \nevaluating systems by the same research group as op posed \nto systems by different groups. The ultimate goal o f looking \ninto these factors with more data is to come up wit h a model \nthat allows us to draw some rules of thumb to guide  experi- \nmenters in the tradeoff between reliability and cos t. REFERENCES \n[1] C. Buckley and E.M. Voorhees, â€œEvaluating Evalu a- \ntion Measure Stability,â€ ACM SIGIR , pp. 33-34, 2000. \n[2] J.S. Downie, â€œMIREX Next Generation,â€ music-ir \nemail list , 2011. Available at: \nhttp://listes.ircam.fr/wws/info/music-ir. \n[3] J.S. Downie, A.F. Ehmann, M. Bay, and M.C. Jone s, \nâ€œThe Music Information Retrieval Evaluation eX- \nchange: Some Observations and Insights,â€ Advances in \nMusic Information Retrieval , W.R. Zbigniew and A.A. \nWieczorkowska, (eds.), Springer, pp. 93-115, 2010. \n[4] IMIRSEL, â€œMIREX 2009 Audio Music Similarity and  \nRetrieval Results,â€ http://music-\nir.org/mirex/wiki/2009:Audio_Music_Similarity_and_ \nRetrieval_Results. \n[5] K. JÃ¤rvelin and J. KekÃ¤lÃ¤inen, â€œCumulated Gain- Based \nEvaluation of IR Techniques,â€ ACM Transactions on \nInformation Systems , 20:4, pp. 422-446, 2002. \n[6] J.H. Lee, â€œCrowdsourcing Music Similarity Judgm ents \nusing Mechanical Turk,â€ ISMIR , pp. 183-188, 2010. \n[7] T. Sakai, â€œOn the Reliability of Information Re trieval \nMetrics Based on Graded Relevance,â€ Information \nProcessing and Management , 43:2, pp. 531-548, 2007. \n[8] M. Sanderson and J. Zobel, â€œInformation Retriev al \nSystem Evaluation: Effort, Sensitivity, and Reliabi lity,â€ \nACM SIGIR , pp. 162-169, 2005. \n[9] M.A. Seaman, J.R. Levin, and R.C. Serlin, â€œNew De- \nvelopments in Pairwise Multiple Comparisons: Some \nPowerful and Practicable Procedures,â€ Psychological \nBulletin , 110:3, pp. 577-586, 1991. \n[10] R. Typke, M. den Hoed, J. de Nooijer, F. Wieri ng, and \nR.C. Veltkamp, â€œA Ground Truth for Half a Million \nMusical Incipits,â€ Journal of Digital Information Man- \nagement , vol. 3, no. 1, pp. 34-39, 2005. \n[11] R. Typke, R.C. Veltkamp, and F. Wiering, â€œA Me asure \nfor Evaluating Retrieval Techniques based on Partia lly \nOrdered Ground Truth Lists,â€ IEEE International Con- \nference on Multimedia and Expo , pp. 1793-1796, 2006. \n[12] J. Urbano, â€œInformation Retrieval Meta-Evaluat ion: \nChallenges and Opportunities in the Music Domain,â€ \nISMIR , 2011. \n[13] J. Urbano, M. Marrero, D. MartÃ­n, and J. LlorÃ© ns, â€œIm- \nproving the Generation of Ground Truths based on Pa r- \ntially Ordered Lists,â€ ISMIR , pp. 285-290, 2010. \n[14] J. Urbano, J. Morato, M. Marrero, and D. MartÃ­ n, \nâ€œCrowdsourcing Preference Judgments for Evaluation \nof Music Similarity Tasks,â€ ACM SIGIR Workshop on \nCrowdsourcing for Search Evaluation , pp. 9-16, 2010. \n[15] E.M. Voorhees, â€œTopic Set Size Redux,â€ ACM SIGIR , \npp. 806-807, 2009. \n[16] E.M. Voorhees and C. Buckley, â€œThe Effect of T opic \nSet Size on Retrieval Experiment Error,â€ ACM SIGIR , \npp. 316-323, 2002. \n[17] W. Webber, A. Moffat, J. Zobel, and T. Sakai, â€œPreci- \nsion-At-Ten Considered Redundant,â€ ACM SIGIR , pp. \n695-696, 2008.  \n602"
    },
    {
        "title": "Modeling Dynamic Patterns for Emotional Content in Music.",
        "author": [
            "Yonatan Vaizman",
            "Roni Y. Granot",
            "Gert R. G. Lanckriet"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1416580",
        "url": "https://doi.org/10.5281/zenodo.1416580",
        "ee": "https://zenodo.org/records/1416580/files/VaizmanGL11.pdf",
        "abstract": "Emotional content is a major component in music. It has long been a research topic of interest to discover the acoustic patterns in the music that carry that emotional information, and enable performers to communicate emotional messages to listeners. Previous works looked in the audio signal for local cues, most of which assume monophonic music, and their statistics over time. Here, we used generic audio features, that can be calculated for any audio signal, and focused on the progression of these features through time, investigating how informative the dynamics of the audio is for emotional content. Our data is comprised of piano and vocal improvisations of musically trained performers, instructed to convey 4 categorical emotions. We applied Dynamic Texture Mixture (DTM), that models both the instantaneous sound qualities and their dynamics, and demonstrated the strength of the model. We further showed that once taking the dynamics into account even highly reduced versions of the generic audio features carry a substantial amount of information about the emotional content. Finally, we demonstrate how interpreting the parameters of the trained models can yield interesting cognitive suggestions.",
        "zenodo_id": 1416580,
        "dblp_key": "conf/ismir/VaizmanGL11",
        "keywords": [
            "emotional content",
            "acoustic patterns",
            "music",
            "performers",
            "listeners",
            "generic audio features",
            "progression of features",
            "dynamics",
            "musically trained performers",
            "categorical emotions"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nMODELING DYNAMIC PATTERNS FOR EMOTIONAL CONTENT IN MUSIC\nYonatan Vaizman\nEdmond & Lily Safra Center\nfor Brain Sciences, ICNC.\nHebrew University Jerusalem Israel\nyonatan.vaizman@mail.huji.ac.ilRoni Y. Granot\nMusicology Dept.\nHebrew University\nJerusalem Israel\nrgranot@huji.013.net.ilGert Lanckriet\nElectrical & Computer\nEngineering Dept.\nUniversity of California San Diego\ngert@ece.ucsd.edu\nABSTRACT\nEmotional content is a major component in music. It has\nlong been a research topic of interest to discover the acous-\ntic patterns in the music that carry that emotional informa-\ntion, and enable performers to communicate emotional mes-\nsages to listeners. Previous works looked in the audio signal\nfor local cues, most of which assume monophonic music,\nand their statistics over time. Here, we used generic au-\ndio features, that can be calculated for any audio signal,\nand focused on the progression of these features through\ntime, investigating how informative the dynamics of the au-\ndio is for emotional content. Our data is comprised of pi-\nano and vocal improvisations of musically trained perform-\ners, instructed to convey 4 categorical emotions. We ap-\nplied Dynamic Texture Mixture (DTM), that models both\nthe instantaneous sound qualities and their dynamics, and\ndemonstrated the strength of the model. We further showed\nthat once taking the dynamics into account even highly re-\nduced versions of the generic audio features carry a substan-\ntial amount of information about the emotional content. Fi-\nnally, we demonstrate how interpreting the parameters of the\ntrained models can yield interesting cognitive suggestions.\n1. INTRODUCTION\nThere is a general agreement that music (especially instru-\nmental music) lacks clear semantic information but conveys\nrich emotional content. As a form of non semantic commu-\nnication, musical performers are able to convey emotional\nmessages through the sound and listeners are able to in-\nterpret the sound and ï¬gure out the emotional intention of\nthe performer. What are the patterns in the musical sig-\nnal itself that enable this communication? The properties\nof the musical content that are responsible for carrying this\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proï¬t or commercial advantage and that copies\nbear this notice and the full citation on the ï¬rst page.\nc/circlecopyrt2011 International Society for Music Information Retrieval.emotional information have long been the subject of inter-\nest and research. In previous computational research that\nanalyzed emotions expressed in music performance, some\nworks looked for local acoustic cues, such as notes per sec-\nond,articulation degree , etc., that are present in the sound\nand may play a signiï¬cant role in conveying the emotional\nmessage [1,2]. Statistics of these cues over time were calcu-\nlated and were usually used to train a discriminative model.\nCalculations of these local cues from raw audio data usually\nrely on intermediate signal processing algorithms to detect\nnote onsets and other events, and these intermediate calcu-\nlations may introduce assumptions, errors and bias. In ad-\ndition, such cues are often deï¬ned for monophonic music,\nand are sometimes even designed for speciï¬c instruments.\nWhile such analysis methods may be very useful for musical\ntraining and acquiring performance skills of conveying emo-\ntions, they tend to be very speciï¬c. Other works avoid this\nproblem by using generic audio features, such as MFCCs or\nother spectral features. Such generic audio features are de-\nï¬ned in a more straight forward way than sophisticated local\ncues, and donâ€™t require intermediate signal processing cal-\nculations. Although these features may not describe certain\nperceptual properties that the local cues try to capture, pre-\nsumably they will be more robust. In addition, generic audio\nfeatures donâ€™t assume anything on the signal, and can be ap-\nplied to any audio signal, even if it contains polyphonic mu-\nsic and even multiple instruments. Such audio content will\nbe a serious obstacle for the local cues approach. Several\nsystems that participated in the MIREX evaluation apply\nthe same audio features for different Music Information Re-\ntrieval (MIR) tasks [3, 4]. In those systems running average\nand standard deviations of time varying audio features were\ntaken, but same as in the local cues approach, the complete\ndynamics of the audio wasnâ€™t used. Such methods disregard\nthe order of time points and assumes theyâ€™re independent.\nIn the presented work, we suggest an approach that ad-\ndresses both the issues of speciï¬city and dynamics. We\napply generic audio features (Mel frequency spectrum) to\novercome the speciï¬city problem. The dynamics issue is re-\nsolved by using Dynamic Texture Mixture (DTM) [5]. DTM\nwas designed to model both the instantaneous properties of\n747Poster Session 6\nhappy sad angry fearful total\npiano 8 7 7 6 28\nvocal 12 12 12 12 48\ntotal 20 19 19 18 76\nTable 1 . Distribution of the recordings over emotions and\ninstrument.\na time series signal, and their dynamics. This model en-\nables us to capture important information that resides in the\ncourse of change of the audio through time, which is missed\nwhen assuming independence among time points.\nSimilar dynamic systems were used by Schmidt and Kim\n[6] to model the time-varying distribution of emotional state\n(in 1 sec intervals). Here we regard each musical instance\n(improvisation of about 30 seconds) as conveying a single\nemotional message (described simply by an emotional ad-\njective), and we apply the dynamic system on the lower level\nof the time-varying audio features themselves.\nDTMs and Gaussian Mixture Models (GMMs) have been\napplied to music information retrieval systems, including se-\nmantic tags of emotional categories annotated by listeners as\nbeing relevant for popular songs [7, 8], but not yet applied\nto audio recordings speciï¬cally created to convey emotional\ncontent. The data in the presented work has recordings of\nimprovisations by musically trained performers instructed\nto convey speciï¬c emotions.\n2. METHODS\n2.1 Data\nOur data set is comprised of 76 audio recordings of musi-\ncally trained performers (2 pianists and 2 vocalists, 1 fe-\nmale and 1 male in each category). For each recording the\nperformer was instructed to improvise a short musical seg-\nment that will convey to listeners in a clear manner a single\nemotion , one from the set of {happy ,sad,angry ,fearful}.\nThese emotional instructions were used as the ground truth\nlabels for the recordings (3 judges veriï¬ed that the appropri-\nate emotions are expressed. Future analyses will also regard\nratings from a larger group of listeners as labels). These\nimprovisations clearly rely, in part, on well entrenched cul-\ntural musical norms and even clich Â´es. Thus we obtained a\nrelatively wide variety of acoustic manifestations for each\nemotional category, which presumably capture the various\nstrategies and aspects of how these speciï¬c emotions can be\nconveyed in Western music. The distribution of recordings\nover emotions and instrument is detailed in Table 1. The\nmedian duration for a recording was 24 seconds.2.2 Audio features\nMel spectrum features were collected: for each time frame\nDiscrete Fourier Transform was calculated and the energy\nof the frequency components was integrated in overlapping\nfrequency bins, in a Mel scale, and the 10log10of the binsâ€™\nenergies were saved. Similarly to [7] we used 34 Mel fre-\nquency bins (partitioning the band from 20Hz to the Nyquist\nfrequency, 11kHz, to 34 Mel-scaled bins), and used half\noverlapping time frames of 2048 samples (after re-sampling\nthe audio data to 22,050Hz, this results in a feature vector\nevery 46msec).\n2.3 Modeling the dynamics\nIn order to model the dynamics of acoustic properties of\nthe music, we applied the Dynamic Texture Mixture (DTM)\nmodel. DTM was previously used to model dynamic tex-\ntures of video [5] and of audio [7]. A Dynamic Texture\n(DT) is a generative model for a time sequence of observed\nfeatures (e.g. the acoustic features collected for each short\ntime frame), that assumes that the observed feature vector\ny(t)was generated as a linear transformation (plus additive\nGaussian noise) over an internal state - a hidden vector vari-\nablex(t)(possibly in a much smaller dimension than the\nobserved feature vector). It also assumes the dynamics of\nthe hidden variable is a Linear Dynamic System, driven by\nadditive Gaussian zero-mean noise: the state of the hidden\nvariable at any time point x(t)depends only on its state in\nthe previous time point x(tâˆ’1), and the dependency is lin-\near./braceleftbiggxt=Axtâˆ’1+vt\nyt=Cxt+wt(1)\nWherevtandwtare both random normal variables (drawn\nindependently for each t). A DTM is a mixture of DTs,\neach having a different relative weight. The DTM models\nthe generation of an audio instance (a song) as follows: for\neach segment of the song ï¬rst select a DT out of the mixture\n(according to the weights of the DTs), and then generate the\nobserved acoustic features of the segment from the selected\nDT.\nSince this is a generative model, we can calculate the\nlikelihood of a song (or of a collection of songs) given a\nDTM. This facilitates the ranking of songs according to their\nlikelihood of being generated by a given DTM or the rank-\ning of different DTMs according to the likelihood of a song\nof being generated by them. The parameters of a DTM can\nbe learned from training data, using an iterative Expectation\nMaximization algorithm tailored for learning DTMs (EM-\nDTM) [5].\nFor each of the 4 emotions ( happy ,sad,angry andfear-\nful), sequences of 125 consecutive feature vectors were col-\nlected (in order to get many feature sequences to train on,\n74812th International Society for Music Information Retrieval Conference (ISMIR 2011)\nwe used overlapping sequences, with hop of 15 feature vec-\ntors from sequence to sequence) from all the recordings in\nthe training set that were associated with the emotion, and\na DTM to represent that emotion was trained over these se-\nquences. Since each feature vector represented a time frame\nof about 46msec, the resulting sequences represented seg-\nments of about 5.7 seconds. The median number of se-\nquences collected for a recording was 26. We used DTMs\nwith 4 components (4 DTs), and with dimension of 7 for\nthe hidden variable x(unless the observed features were in\na lower dimension).\n2.4 Performance evaluation\nIn order to evaluate the success of the acoustic features to\nrepresent the required information regarding the emotional\ncontent, and the success of the model to capture the relevant\nacoustic patterns for the emotional content, we used infor-\nmation retrieval framework and performance measures: Af-\nter training 4 emotion DTMs on the training set, a test set\nwith unseen recordings was analyzed. For each recording\nthe 4 emotions were ranked according to the likelihood of\nthat recording given the 4 emotion DTMs, and annotation\nof 1 emotion (the one with highest likelihood) was given to\nthe recording. For each emotion, the test recordings were\nranked according to their likelihood given the DTM of the\nemotion, as a retrieval task. Comparing the machineâ€™s anno-\ntation and retrieval to the ground truth emotion labels of the\ntest recording, 3 annotation measures and 2 retrieval mea-\nsures were calculated, in a similar manner to [7]: preci-\nsion (portion of the ground truth labeled instances out of the\nmachine-annotated instances), recall (portion of the machine-\nannotated instances out of the ground truth labeled instances),\nf-measure (balance measure between precision and recall),\nmean average precision - MAP (average precision over dif-\nferent thresholds of â€how many of the top-ranked instances\nto retrieveâ€) and area under ROC curve - AROC (area under\nthe tradeoff curve of true-positive rate vs. false-positive rate\nfor the retrieval task, area of 0.5 being chance and area of 1\nbeing maximum possible). Each of the 5 measures was cal-\nculated for each emotion, and then averaged over emotions.\nTo estimate these measures over general unseen data, 10-\nfold cross validation scheme was used. For each partition,\n4 emotion-DTMs were trained over 9/10 of the recordings,\nand the 5 measures were calculated over the remaining 1/10\nof the recordings. In each partition control performance\nmeasures (chance level) were approximated by repeatedly\n(400 times) generating random uniform values (instead of\nthe likelihood values actually calculated with the trained\nmodels) and feeding them to the annotation-retrieval sys-\ntem, for the test set. Mean and standard deviation over repe-\ntitions were collected as reference for assessment of quality\nof the actual performance scores. Approximated p-values\nwere then calculated to each of the 5 measures, as the prob-precision recall F MAP AROC\nscore 0.6446 0.6500 0.6000 0.8099 0.8692\nchance 0.25 0.25 0.22 0.44 0.50\np-val 0.09 0.04 0.06 0.02 0.02\nTable 2 . Annotation and retrieval results for basic features.\nability of getting a higher score under the null hypothesis,\nmeaning with random values (assuming a normal distribu-\ntion with the mean and standard deviation that we collected\nfor the random values). Finally we averaged over the 10\nfolds the 5 performance measures, as well as 5 chance level\nscores and 5 p-values for our scores. The partition to 10\nfolds was semi random, making sure each fold contained in-\nstances from all 4 emotional categories, and all experiments\nwere done using the same partitioning to 10 folds, in order\nfor the comparison to be consistent.\n3. EXPERIMENTS AND RESULTS\n3.1 Experiment 1 - basic\nThe system was applied to the basic features as described\nabove. The results of the cross validation are presented in\nTable 2. In the basic experiment, the results demonstrate\nthat the DTM model manages to capture the important acous-\ntic patterns for the communication of emotion.\n3.2 Experiment 2 - power dynamics\nIn order to investigate the role of the power dynamics, two\ncomplementary manipulations over the features were per-\nformed:\nEx2.1: ï¬‚attening the power . For each recording, all\nthe Mel spectra vectors were normalized to have the same\nconstant total power, but within each vector, the original ra-\ntios among the frequency bins were preserved. This ma-\nnipulation ï¬lters out the power dynamics (in time scales\nlarger than 46msec), and keeps all the rest of the information\nstored in the original features (melody, timbre, etc.).\nEx2.2: keeping only the power dynamics . For each\nrecording and for each time point, instead of keeping 34 co-\nefï¬cients, only 1 coefï¬cient is kept - the total power of the\ntime frame (in log scale). This manipulation preserves only\nthe power dynamics, and ï¬lters out the rest of the sound\nproperties. Since the observed features in every time frame\nwere then only 1 dimensional, the dimension of the hidden\nvariablexwas also reduced to 1, resulting in a linear dy-\nnamic system that is almost degenerate (since the transition\nmatrixAis simply a scalar), and relies more on the driving\nnoise.\nEx2.3: not modeling dynamics . As control, using the\nsame features as in Ex2.2 we applied a GMM model that\nassumes independent time frames, to see if we can still cap-\nture the remained relevant information about the emotions,\n749Poster Session 6\nprecision recall F MAP AROC\nEx2.1 score 0.6134 0.6375 0.5775 0.7627 0.8429\np-val 0.07 0.04 0.05 0.04 0.02\nEx2.2 score 0.4287 0.4625 0.3801 0.5935 0.6879\np-val 0.19 0.14 0.18 0.16 0.14\nEx2.3 score 0.2931 0.3125 0.2638 0.5536 0.6454\np-val 0.44 0.4 0.42 0.29 0.25\nTable 3 . Results for power manipulations.\nwhile disregarding the dynamics. The only dependency left\namong time frames was the 1stand 2ndtime derivatives\n(delta and acceleration) of the feature vector (of the power\nscalar, in this experiment) that were augmented, so the fea-\nture vector here was 3 dimensional (for time t:power (t),\ndelta (t) =power (t+1)âˆ’power (t)andacceleration (t) =\ndelta (t+ 1)âˆ’delta (t)). For training we used the hierar-\nchical EM algorithm for GMM (HEM-GMM), as described\nin [8]. We used 4 components (4 Gaussians) for each model\n(each GMM), and restricted to diagonal covariance matri-\nces.\nResults are presented in Table 3 (the reference chance\nlevels, which appear in Table 2, are the same in all exper-\niments). Ex2.1 demonstrates that most of the information\nabout the conveyed emotion is retained even without the\ngross dynamics of the power (keeping in mind that some\nï¬ner power dynamics can be expressed inside each time\nframe, in the lower frequency bins). Although this may\nsuggest that the gross power dynamics doesnâ€™t carry much\ninformation about the emotions, Ex2.2 shows the contrary:\nafter reducing the features to only the power dynamics, the\nscores remain fairly high (although, as expected for a 1 di-\nmensional time function, some decrease in performance is\nevident). The results show that the power dynamics does\ncarry useful information about the emotional content. The\ncontrol done in Ex2.3 shows that GMM got very poor scores\nfor the 3 annotation performance measures, and relatively\npoorer results than DTM (Ex2.2) for all measures. It is quite\nexpected that when reducing the features to only the power,\ntreating the time frames as independent will yield insufï¬-\ncient information about the emotions. The gap between the\nresults of Ex2.2 and Ex2.3 shows the added value of taking\ninto account the dynamics of the acoustical properties (when\neven 1stand 2ndtime derivatives are not enough).\n3.3 Experiment 3 - avoiding frequency correlations\nWhen acoustical instruments (or human voice) are playing,\nthe harmonic structure has correlations between the funda-\nmental frequencies and their higher harmonics, resulting in\ncorrelation between the dynamics of different frequency bins,\nand suggesting redundancy when all these frequency bins\nare speciï¬ed. The DTM model deals with this redundancy\nby trying to ï¬nd a lower representation in the hidden state\nxthat generates the observed vectors (by linear transforma-precision recall F MAP AROC\nEx3.1 score 0.5288 0.5667 0.4847 0.7331 0.7969\np-val 0.23 0.13 0.19 0.13 0.1\nEx3.2 score 0.4701 0.4375 0.3648 0.6213 0.6546\np-val 0.14 0.16 0.21 0.16 0.21\nEx3.3 score 0.1718 0.175 0.1339 0.4354 0.5425\np-val 0.67 0.65 0.69 0.51 0.4\nTable 4 . Results for keeping part of the spectrum.\ntion with the observation matrix C- the principal compo-\nnents of the observed features) [7]. We wanted to reduce the\nobserved features prior to summarizing the whole spectrum\nand, in a way, to overlook the correlations among frequen-\ncies. For this purpose we examined limiting our view to only\npart of the spectrum. We focused on two opposite extremes\nof the spectrum captured by the original features:\nEx3.1: 6400Hz-11025Hz (Nyquist frequency) . Keep-\ning only the last 6 frequency bins of each time frame. Such\nfrequency band is likely to contain resonating frequencies to\nthe fundamental frequencies of the melody being played (or\nvoiced). When calculating mean over all time frames in all\ninstances in the data set, these 6 bins carry only 0.036 of the\npower (not log power) of the spectrum.\nEx3.2: 0Hz-275Hz . Keeping only the ï¬rst 3 frequency\nbins of each time frame. For part of the time frames this\nfrequency band may be below the present fundamental fre-\nquency of the tones being played. These 3 bins carry (in\naverage) 0.25 of the power of the spectrum. For both Ex3.1\nand Ex3.2 we used dimension of 3 for the hidden variable x.\nThese extreme bands probably behave differently for piano\nand for vocal and interesting insights can later be raised by\nperforming similar experiments separately for instruments.\nEx3.3: not modeling dynamics . Similar to the control\ndone in Ex2.3, we applied the GMM model to the features\nused in Ex3.2, plus 1stand 2ndtime derivatives.\nResults are presented in Table 4. Ex3 demonstrates that\nin both extremes of the spectrum, there are small frequency\nbands that still carry a fair amount of information about the\nconveyed emotions (performance is still relatively far from\nchance level). The control in Ex3.3 that, again, shows poor\nresults with the GMM (performance being about chance level\nor worse), afï¬rms that the remained relevant information\nlies mostly in the dynamics.\n3.4 Experiment 4 - melodic structure\nNext we aimed to examine the affect of the melodic dynam-\nics on the conveyed emotions. Since it is neither simple\nnor accurate to determine the notes that were played, espe-\ncially for polyphonic music (such as our piano recordings),\nwe chose to deï¬ne a more accurate property that hopefully\nwill be more robust: the dynamics of the strongest frequency\nbin. We cannot claim to describe the perceived melody\n(or the played notes) with this property (since pitch percep-\n75012th International Society for Music Information Retrieval Conference (ISMIR 2011)\nprecision recall F MAP AROC\nscore 0.4322 0.4375 0.3852 0.58 0.6863\np-val 0.23 0.2 0.21 0.2 0.16\nTable 5 . Results for keeping only strongest frequency bin.\ntion or production is more complex than just the strongest\nfrequency present, and since the piano music has multiple\ntones played simultaneously). However this property is eas-\nily computed and can be used as a surrogate marker for\nthe melodic progression. For this experiment, in each time\nframe only the strongest bin remained active and the power\nof all the other frequency bins was nulliï¬ed. Furthermore, to\nget rid of the power dynamics, the power of all time frames\nwas set to be constant, so the only remaining information\nwas the identity of the activated bin in each time frame. The\ndimension of the hidden variable xwas set to 1. Results are\nshown in Table 5. Although the features were reduced to a\nlarge extent, a predictive ability is still present.\n3.5 Interpreting the trained models\nAfter validating that DTMs can capture important acous-\ntic patterns for emotional content, we wanted to understand\nthe differences between different trained emotion models\nthat enabled the system to discriminate. Using a genera-\ntive model is suitable to describe the process of production:\nthe performers that want to convey some emotion and apply\nan appropriate generative strategy to create their resulting\nsound. In order to get insight about the different generative\nstrategies, one needs to compare the learned parameters of\nthe trained models. For this purpose we retrained 4 emotion\nDTMs over the entire data set, for our different experiments.\nThe main component that describes the dynamics of the\nsystem in a DT is the transition matrix A. If the system\nwere a deterministic linear dynamic system, without the ad-\nditive noise, this transition matrix would tell both the des-\ntination of the state of the system xand the way it will\ntake to get there. The eigenvectors of Adescribe the dif-\nferent modes of the system - different patterns of activating\nthe observed features. The eigenvalues of A(complex num-\nbers) indicate the course of progress of the different modes\n(patterns) of the system: while having an eigenvalue with\nmagnitude larger than 1 results in the state of the system di-\nverging, having an eigenvalue with magnitude of 1 results\nin the state converging to either a stable state or stable limit\ncycle determined by the eigenvector of that value, and hav-\ning all eigenvalues with magnitudes smaller than 1 results\nin a system that strives to converge to the zero vector state\n(if there is no additive noise to reactivate the modes). The\nmagnitude of an eigenvalue indicates the intensity or stabil-\nity of this mode (how slowly this mode will decay or how\nmuch anti-mode noise needs to be added to this mode in or-\nder to silence it). The angle of the eigenvalue indicates the\nFigure 1 . Eigenvalues for using the basic features (Ex1).\nEach different shape represents 20 eigenvalues of transition\nmatrices from DTs of a different emotion DTM (5 largest\neigenvalues from 4 DTs per emotion-DTM). happy - circle,\nsad- star, angry - triangle, fearful - square.\nnormalized frequency of the mode - if an eigenvalue has a\nlarge angle its mode will oscillate and modulate its pattern\nin a fast period, returning to the original modulation pattern\n(only with smaller magnitude) after only few time frames.\nThe maximal normalized frequency will be Ï€, making the\nmode change to its exact negative in each consecutive time\nframe. We examined the eigenvalues of the different DTs\nof the different emotion DTMs, and presented their magni-\ntudes (intensity) and angles (frequency).\nIn both conditions presented in Figure 1 and Figure 2 there\nis a clear concentration of the eigenvalues of the sadmodel\n(marked with star) with relatively high intensities and low\nfrequencies (in absolute value). This can be interpreted as\na general strategy (either conscious or subliminal) of the\nperformers to convey sadness using stable and slowly mod-\nulated acoustic patterns. On the opposite, the happy and\nangry models (marked by circle and triangle, respectively)\ninclude many modes with smaller intensities and higher fre-\nquencies, suggesting strategies that include fast repetitions\nof acoustic patterns (high frequencies) and easy switching\nfrom one dominating pattern to another (the low magnitudes\nmean that little noise is sufï¬cient to shake off these modes\nand activate different modes).\nSuch conclusions should be taken with a grain of salt. We\nshould remember the system also has additive noise. In ad-\ndition, in order to adequately generalize these results, much\nlarger data sets, with many performers, should be used. How-\never, such analyses may help to focus future research on\ncertain aspects of production of music for emotional com-\nmunication.\n751Poster Session 6\nFigure 2 . Eigenvalues for keeping only higher frequency\nband (6 kHz-11 kHz. Ex3.2).\n4. DISCUSSION\nInvestigating the dynamics of generic acoustic features in\nmusical audio can reveal important components of the mu-\nsic, and speciï¬cally for emotional content. Generic acoustic\nfeatures can be informative for various melodic, harmonic,\nrhythmic and instrumental content of music, and here we\ndemonstrated their successful usage for both monophonic\nand polyphonic music. We have shown that even highly\nreduced audio features, such as the power, can still retain\nmuch of the emotional message, when taking into account\nthe time progression of the property. Interestingly, comple-\nmentary manipulations to reduce the audio features (â€ï¬‚at-\ntening the powerâ€ vs. â€keeping only the power dynamicsâ€)\nboth kept a discriminative ability, suggesting that the in-\nformation about the emotional intention carried by separate\ncomponents of the sound is not simply additive, but rather\nhaving redundancy. One should remember, though, that it\nmight require few dimensions of features to discriminate 4\nemotions, but possibly require more detailed features, when\ndiscriminating more emotions and emotional subtleties.\nFuture research using similar methods should be applied\nover more general musical data, with multiple instruments,\nto ï¬nd general dynamic patterns that convey different emo-\ntions. It may be interesting to investigate the critical time\nresolutions that show dynamics that is relevant for emo-\ntional content (perhaps taking sequences of more than 125\ntime frames will reveal slower informative patterns). Exper-\niments with larger data will enable investigating differences\nin strategies, in informative frequency bands, redundancy\npatterns and other aspects, among different emotions. An-\nother interesting direction is to use trained generative mod-\nels to synthesize new audio instances. This is not a simplechallenge, but even if the resulting sounds will not be intel-\nligible or natural sounding, they may still have an effect of\nconveying emotions, and concordance between the emotion\nof the generated audio and that of the generating model will\nbe another convincing argument that the model captures im-\nportant acoustic patterns for emotional communication.\n5. ACKNOWLEDGEMENTS\nDr. Avi Gilboa, Dr. Ehud Bodner, Liza Bekker and Nori\nJacobi took part in the collection of the data. Special thanks\nto Emanuele Coviello for guidance with DTM. Gert Lanck-\nriet acknowledges support from Yahoo! Inc. and NSF Grant\nIIS-1054960\n6. REFERENCES\n[1] L. Mion, and G. De Poli: â€œScore-Independent Audio\nFeatures for Description of Music Expression,â€ IEEE\nTransactions on Audio, Speech and Language Process-\ning, V ol.16, No. 2, pp. 458â€“466, 2008.\n[2] A. Friberg, E. Schoonderwaldt, P. Juslin, and R. Bresin:\nâ€œAutomatic Real-Time Extraction of Musical Expres-\nsion,â€ Proceedings of the International Symposium\nComputer Music Conference , pp. 365â€“367, 2000.\n[3] G. Tzanetakis: â€œMarsyas Submission to MIREX 2009,â€\nMIREX 2009 .\n[4] G. Peeters: â€œa Generic Training and Classiï¬cation Sys-\ntem for MIREX08 Classiï¬cation Tasks: Audio Mu-\nsic Mood, Audio Genre, Audio Artist and Audio Tag,â€\nMIREX 2008 .\n[5] A. B. Chan, and N. Vasconcelos: â€œModeling, Clustering\nand Segmenting Video with Mixtures of Dynamic Tex-\ntures,â€ IEEE Transactions on Pattern Analysis and Ma-\nchine Intelligence , V ol.30, No. 5, pp. 909â€“926, 2008.\n[6] E. M. Schmidt and Y . E. Kim: â€œPrediction of Time-\nVarying Musical Mood Distributions Using Kalman Fil-\ntering,â€ in Proceedings of the 2010 IEEE International\nConference on Machine Learning and Applications ,\n2010.\n[7] E. Coviello, A. B. Chan, and G. Lanckriet: â€œTime Series\nModels for Semantic Music Annotation,â€ IEEE Trans-\nactions on Audio, Speech and Language Processing ,\nV ol.19, No. 5, pp. 1343â€“1359, 2011.\n[8] D. Turnbull, L. Barrington, D. Torres, and G. Lanck-\nriet: â€œSemantic Annotation and Retrieval of Music and\nSound Effects,â€ IEEE Transactions on Audio, Speech\nand Language Processing , V ol.16, No. B2, pp. 467â€“476,\n2008.\n752"
    },
    {
        "title": "Pulse Detection in Syncopated Rhythms Using Neural Oscillators.",
        "author": [
            "Marc J. Velasco",
            "Edward W. Large"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1417519",
        "url": "https://doi.org/10.5281/zenodo.1417519",
        "ee": "https://zenodo.org/records/1417519/files/VelascoL11.pdf",
        "abstract": "Pulse and meter are remarkable in part because these perceived periodicities can arise from rhythmic stimuli that are not periodic. This phenomenon is most striking in syncopated rhythms, found in many genres of music, including music of non-Western cultures. In general, syncopated rhythms may have energy at frequencies that do not correspond to perceived pulse or meter, and perceived metrical frequencies that are weak or absent in the objective rhythmic stimulus. In this paper, we consider syncopated rhythms that contain little or no energy at the pulse frequency. We used 16 rhythms (3 simple, 13 syncopated) to test a model of pulse/meter perception based on nonlinear resonance, comparing the nonlinear resonance model with a linear analysis. Both models displayed the ability to differentiate between duple and triple meters, however, only the nonlinear model exhibited resonance at the pulse frequency for the most challenging syncopated rhythms. This result suggests that nonlinear resonance may provide a viable approach to pulse detection in syncopated rhythms.",
        "zenodo_id": 1417519,
        "dblp_key": "conf/ismir/VelascoL11",
        "keywords": [
            "syncopated rhythms",
            "perceived periodicities",
            "rhythmic stimuli",
            "energy at frequencies",
            "perceived pulse or meter",
            "metrical frequencies",
            "nonlinear resonance",
            "duple and triple meters",
            "resonance at the pulse frequency",
            "pulse detection"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\n   PULSE DETECTION IN SYNCOPATED RHYTHMS USING NEURAL OSCILLATORS Marc J. Velasco Edward W. Large Center for Complex Systems and Brain Sciences Florida Atlantic University velasco@ccs.fau.edu, large@ccs.fau.edu    ABSTRACT Pulse and meter are remarkable in part because these perceived periodicities can arise from rhythmic stimuli that are not periodic. This phenomenon is most striking in syncopated rhythms, found in many genres of music, including music of non-Western cultures. In general, syncopated rhythms may have energy at frequencies that do not correspond to perceived pulse or meter, and perceived  metrical frequencies that are weak or absent in the objective rhythmic stimulus. In this paper, we consider syncopated rhythms that contain little or no energy at the pulse frequency. We used 16 rhythms (3 simple, 13 syncopated) to test a model of pulse/meter perception based on nonlinear resonance, comparing the nonlinear resonance model with a linear analysis. Both models displayed the ability to differentiate between duple and triple meters, however, only the nonlinear model exhibited resonance at the pulse frequency for the most challenging syncopated rhythms. This result suggests that nonlinear resonance may provide a viable approach to pulse detection in syncopated rhythms. 1. INTRODUCTION Pulse is a periodicity perceived in a musical rhythm, operationally defined as the frequency at which one would most likely tap along to a rhythm [11].  People also perceive meter, a structural pattern of accents among beats of the pulse [10]. Pulse and meter can be diagrammed  using the notation of Lerdahl and Jackendoff [10], in which the metrical grid is composed of beats at multiple related frequencies, with strong beats occurring when beats at multiple frequencies overlap in time.  Thus meter organizes beats of the pulse into strong beats and weak beats. In simple rhythms (Figure 1a), note-events occur on strong beats.  Rhythms such as the 3-2 Rumba Clave (Figure 1b), although they share the same nominal metrical structure, are more complex.  In such rhythms, note-events occur on metrically weak beats, and strong metrical beats often correspond to silences. These two attributes define syncopation [3, 12]. Thus, in syncopated rhythms note events are spaced irregularly in time, yet the perceived pulse is regularly timed, and the meter, regularly structured [3, 14]. A goal of theories of pulse perception is to explain how pulse and meter are perceived for musical rhythms in general, and for syncopated rhythms in particular.   \n Figure 1. Example rhythms and metrical grid. Our approach is based on the idea that the pulse percieved in a musical rhythm is a neural resonance that arises in sensory [6, 8, 17] and motor cortices [2, 4]. The experience of meter is posited to arise from interaction of neural resonances at differenct frequencies. In this paper we put forth a neurodynamic model of pulse and meter and ask whether it can explain the perception of pulse and meter in highly syncopated rhythms.  1.1 Neural Oscillation Neural oscillation can arise from the interaction between excitatory and inhibitory neural populations. The canonical model used here was derived, using normal form theory, from the Wilson-Cowan model of the interaction between excitatory and inhibitory neural populations [7, 18]. This model is generic, however, so the responses of the model to musical rhythm are likely to be observed in many other nonlinear oscillator models of rhythm perception. !\"####$\"###$\"#$\"###Simple Rhythma)b)$\"$!!pulse12341234!\"##\"##$\"###$\"#$\"###3-2 Rumba Clave$$!!pulse12341234weakstrongweakstrong\n185Poster Session 2\n   1.2 Model Our conceptual model is a network of neural oscillators, spanning a range of natural frequencies, stimulated with an auditory rhythm. The basic concept is similar to signal processing by a bank of linear filters [15], but with the important difference that the processing units are nonlinear, rather than linear resonators. We can describe the behavior of a linear filter using a differential equation (Eq 1), where the overdot denotes differentiation with respect to time.  z is a complex-valued state variable; Ğ¢ is radian frequency. Ğ‹ < 0 is a linear damping parameter. x(t) denotes linear forcing by a time-varying external signal.   (1) Because z is a complex variable, it has both amplitude and phase.  Resonance in a linear system means that the system oscillates at the frequency of stimulation, with amplitude and phase determined by system parameters. As stimulus frequency, Ğ¢0, approaches the oscillator frequency, Ğ¢, oscillator amplitude, r = |z|, increases, providing band-pass filtering behavior. In the linear case, oscillator amplitude depends linearly on stimulus amplitude. A common model of nonlinear oscillation is based on the normal form for the Hopf bifurcation (Eq 2).     (2) Note the surface similarities between this form and the linear resonator of Equation 1.  Equation 2 can be seen as a generalization of Equation 1, and the two behave the same when ĞŒ= 0. Again Ğ¢ is radian frequency, and Ğ‹ is still a linear damping parameter.  ĞŒ < 0 is a nonlinear damping parameter, which maintains stability when Ğ‹ > 0. x(t) denotes linear forcing by an external signal. The term h.o.t. denotes higher-order terms of the nonlinear expansion that are truncated (i.e., ignored) in normal form models. When Ğ‹ = 0 and Î² < 0, the system is said to be in the critical parameter regime, poised between damped and spontaneous oscillation. The amplitude of the response depends nonlinearly on the input amplitude. Like linear resonators, nonlinear oscillators have a filtering behavior, responding maximally to stimuli near their own frequency. Differences in behavior include extreme sensitivity to weak signals and high frequency selectivity. Critical oscillators have been used to model critical oscillations of outer hair cells in the cochlea [5]. When Ğ‹ > 0 (and Î² < 0), the system exhibits a limit cycle in absence of input; thus, it can oscillate spontaneously.  Our canonical model [7] (Eq 3) is an expansion of the Hopf normal form (Eq 2), which includes higher order terms.   (3) There are again surface similarities with the previous models. The parameters, Ğ¢, Ğ‹ and ĞŒ1 correspond to the parameters of the truncated model. ĞŒ2 is an additional amplitude compression parameter, and c represents strength of coupling to the external stimulus. Ğ1 and Ğ2 are frequency detuning parameters. The parameter Ğ controls the amount of nonlinearity in the system. Most importantly, coupling to a stimulus is nonlinear and has a passive part, P(Ğ, x(t)) and an active part, A(Ğ, z), as defined in [7], which produce different higher order resonances, as described in the next section. 1.3 Properties of Nonlinear Resonance Equation 3 displays all the behavioral regimes  described above â€“  linear, critical and limit cycle â€“depending on the parameter values chosen. Additionally, Equation 3 can also exhibit a double-limit cycle bifurcation, when Ğ‹ < 0, Î²1> 0, Î²2 < 0 (and Îµ > 0). Stable states emerge at rest and at a stable limit cycle; an unstable limit cycle separates the two, functioning as a kind of threshold.  If the stimulus is strong enough, the threshold will be crossed, the system reaches the stable limit cycle, and oscillation can be maintained even after the stimulus has ceased.  Thus an oscillator operating in a double-limit cycle regime can maintain a memory of an oscillating stimulus.  Higher-order resonance means that a nonlinear oscillator with frequency f responds to harmonics (2f, 3f, ...), subharmonics (f/2, f/3, ...) and integer ratios (2f/3, 3f/4, ...) of f. If a stimulus contains multiple frequencies, a nonlinear oscillator will respond at combination frequencies (f2 - f1, 2f1 - f2, ...) as well. Higher order resonances follow orderly relationships and can be predicted given stimulus amplitudes, frequencies and phases. This has important implications for understanding the behavior of such systems. The nonlinear oscillator network does not merely transduce signals; it adds frequency information, which can be used to model pattern recognition and pattern completion, among other things. Neural pattern completion based on nonlinear resonance may explain the perception of pulse and meter in syncopated rhythmic patterns [9, 13].  \nË™z=z(Î±+iÏ‰)+x(t)\nË™z=z(Î±+iÏ‰+Î²|z|2)+x(t)+h.o.t.\n Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.  Â© 2011 International Society for Music Information Retrieval  \n18612th International Society for Music Information Retrieval Conference (ISMIR 2011)\n   Our hypothesis is that in rhythms with no energy at the pulse frequency, pulse arises due to nonlinear resonance in the brain. Significant contributions may also come from instrinsic dynamics and learned connectivity. As a first test of this hypothesis, we ask whether such resonances arise in a canonical nonlinear model. 2. EXPERIMENT 1 The first experiment compared the objective frequency content of 16 rhythms with the frequency responses of a nonlinear oscillator network. Using Fourier analysis we measured the frequency content of the rhythmic patterns, showing that in syncopated rhythms the pulse frequency is weak or absent. Next, we assessed whether nonlinear resonance could explain the perception of pulse and meter at the frequencies that are predicted by music theoretic analysis of these rhythms.  2.1 Model  Our model consisted of a single network of 289 oscillators described by Equation 3, with natural frequencies logarithmically spaced from 0.25 Hz to 16 Hz.  The model operated in a critical parameter regime (Ğ‹ = 0, ĞŒ1 = -1,  ĞŒ2 = -0.25, and Ğ = 1), poised between damped and spontaneous oscillation.   2.2 Stimuli We used 16 rhythms: one isochronous pulse train, two canonical metrical rhythms (3/4 and 4/4), three clave rhythms, and ten â€œmissing pulseâ€ rhythms that were created in our lab in the context of a previous experiment [1]. The clave rhythms were a 3-2 Son Clave, a Rumba Clave, and a clave-like rhythm we dubbed â€˜Hard Claveâ€™. The ten missing pulse rhythms were structured so as to balance strong and weak beats, with four events on strong beats and four events on weak beats. In a previous experiment we observed that most people reliably tap at the nominal pulse frequency for these rhythms. We rendered each rhythmic event as a continuous time onset â€˜bumpâ€™ with amplitude corresponding to the intensity of the event. All events were of equal intensity, except for the metrical rhythms, where intensity differences marked canonical metrical accents.  All rhythms were rendered at a tempo of 120 bpm, making the pulse frequency 2 Hz. Examples of the rhythmic stimuli are shown in Figure 2.  Figure 2. Examples of stimuli types: Isochronous, Ca-nonical 3/4, a Rumba Clave, and one of the ten missing pulse patterns.   2.3 Method Computations were performed using Matlab 7.4, on a Macintosh Mac Pro, running Mac OS X 10.5.8. In the simulations, the continuous-time pulse trains were used to drive the network model and the resulting oscillatory output behavior was examined. Network behavior was evaluated by assessing steady state amplitude of the resonating oscillators. The natural frequencies of the resonating oscillators indicate which frequencies resonate to the input stimulus. 2.4 Results Figure 3 compares a Fourier analysis (FFT) of four rhythmic input signals with the amplitude profile of the network of nonlinear oscillators. Oscillator natural frequency (Hz) runs along the x-axis, and amplitude is shown on the y-axis.  Musical notation above each panel indicates the pulse and metrical frequencies for each rhythm. For the isochronous rhythm, energy is present at the pulse frequency (2 Hz), and its harmonics. For the canonical rhythms, signal energy was observed at the pulse frequency, while the accents present in the signal contributed frequencies at metrical levels (subharmonics of the pulse). The clave rhythms all had some energy at 2 Hz;  however, this was strongly attenuated compared to the energy at other nearby frequencies. Fourier analysis of the other ten syncopated rhythms revealed no energy at the 2 Hz pulse frequency, while considerable energy was !!\"#$$\"#%%\"#&&\"#''\"###\"#((\"#))\"#*+,-./0-1-2,\n!!\"#$$\"#%%\"#&&\"#''\"###\"#((\"#))\"#*341-15.467&8'\n!!\"#$$\"#%%\"#&&\"#''\"###\"#((\"#))\"#*92:;47364<=\n!!\"#$$\"#%%\"#&&\"#''\"###\"#((\"#))\"#*>5,,51?7@26,=7$7A5:=7B,CD<=1E7F:G65E2H=\n187Poster Session 2\n   observed at non-metrical frequencies. Note that energy was present at the eighth note level of 4 Hz for all rhythms. As illustrated in Figure 3, resonant responses were observed in the oscillator network at frequencies that were not objectively present in the stimulus rhythms. Most importantly, resonances were observed at the pulse frequency for every rhythm. Resonances were also observed at subharmonics of the isochronous rhythm, and for canonical rhythms subharmonic resonances enhanced the response at the metrical frequencies. For the clave rhythms, the response at the pulse frequency (2 Hz) was also enhanced relative to the Fourier amplitude. For the missing pulse rhythms, although there was no energy at the 2 Hz pulse frequency, the nonlinear network responded at the 2 Hz pulse frequency as well as at some additional metrical frequencies.  In summary, both simple and complex rhythms contain multiple frequencies, only some of which appear to be related to the meter. Simple rhythms contain frequencies corresponding to the pulse; however, complex syncopated rhythms contain little or no energy at the pulse frequency. This feature of complex rhythms may be problematic for linear filter based methods of pulse detection.  Nonlinear oscillators can resonate at frequencies corresponding to pulse and meter even when these are not objectively present in the input. However, the simple oscillator array investigated in Experiment 1 is, by itself, likely not sufficient to induce the pulse and meter of complex rhythms. While oscillators resonate at the pulse frequency, a number of stronger resonances are observed at frequencies that do not correspond to pulse or meter. In the next experiment, we ask whether multiple networks together might provide greater frequency selectivity. 3. EXPERIMENT 2 3.1 Stimuli & Method The stimuli methods used in Experiment 2 were the same as in Experiment 1. 3.2 Model The model was based on the same oscillator equations as used in Experiment 1.  The key difference was that in Experiment 2, the model consisted of two networks interacting with each other.  Network 1 had the same parameters as used in Experiment 1. The oscillators in Network 2 were tuned to exhibit double limit cycle bifurcation behavior (Ğ‹ = 0.3, ĞŒ1 = 1, ĞŒ2 = -1, and Ğ = 1), and thus exhibited both threshold and memory properties.   !!\"#$\"!!%#\"\"!!&#\"\"!!'#\"\"!!(#\"\"\"\"#'\"#()*+,-.+/+0*\"\"#'\"#(\n!!\"#$\"!!%#\"\"!!&#\"\"!!'#\"\"!!(#\"\"\"\"#'\"#(12/+/3,24!56'\"\"#'\"#(\n!!\"#$\"!!%#\"\"!!&#\"\"!!'#\"\"!!(#\"\"\"\"#'\"#(70892!142:;\"\"#'\"#(\n!!\"#$\"!!%#\"\"!!&#\"\"!!'#\"\"!!(#\"\"\"\"#'\"#(<+/43/;2.!=*,3442>+.!?8@43>0A;B3**3/C!D04*;!%E.;F0;/,G!HIJK\"\"#'\"#(EEL!?8@43>0A;.\n  Figure 3. Experiment 1 results. A subset of the rhythms presented with both an FFT of the stimulus (black) and the  amplitudes of responding nonlinear oscillators (gray). The two networks were connected as shown in Figure 4.  Tonotopic connections between the networks allow Network 1 to drive Network 2. Next, in each network, internal connectivity coupled patches of oscillators to other patches exhibiting small integer ratio frequency relationships, 1:3, 1:2, 1:1, 2:1, 3:1.  These connections are assumed to be learned by exposure to Western rhythms, in which duple and triple meters are common. Connectivity from Network 2 to Network 1 was inhibitory. 3.3 Results Across the rhythms presented, Network 1 behaved similarly to the previous experiment, responding to frequencies present in the simulus rhythms, and also adding nonlinear resonances. Example of Network 2 responses are shown in Figure 5. Due to its thresholding properties, Network 2 responded to a subset of frequencies present in the Network 1. Importantly, Network 2 almost always responded at the pulse frequency. Moreover,  the amplitude at 2 Hz was unexpectedly strong given the relatively weak responses observed in Experiment 1.    \n18812th International Society for Music Information Retrieval Conference (ISMIR 2011)\n   \n Figure 4. Network architecture for models used in both experiments. \nFigure 5. Results for Experiment 2. Amplitude response profiles for Network 1 (gray) and Network 2 (black). Frequencies were considered â€˜activeâ€™ in Network 2 if they exceeded the threshold implicit in the double limit cycle oscillatory dynamics. Active frequencies were compared to metrical frequencies for each rhythm. For syncopated rhythms expected frequencies were the quarter note level (i.e., the pulse, 2 Hz), the eighth note level (4 Hz), as well as the half note (1 Hz) and whole note levels (0.5 Hz) for  Table 1. Summary of results for Experiment 2.  Shaded cells identify frequencies which would be expected to have a resonance for the rhythm based on meter.  Populated cells (x) show which resonant frequencies were active in Net-work 2.  most of the rhythms (the one exception was the canonical 3/4 rhythm, whose slower metrical frequency was 0.67 Hz). The results of the two-network model can be seen in Table 1. Highlighted cells show the frequencies at which response peaks would be expected based on the meter. Populated cells show whether or not response peaks were observed at given frequencies. For all but one rhythm, a response was seen at the pulse frequency of 2 Hz. For the canonical rhythms, response peaks were always found at the expected frequencies and at no others. This set of hierarchically related frequencies may correspond to a perception of meter. For the missing pulse rhythms, response peaks were found most consistently at the pulse frequency and its first harmonic at 4 Hz. At lower frequencies, the results differed from standard metrical predictions. This may explain why people sometimes have difficulty entraining periodic taps with highly syncopated stimuli. In previous experiments, level of syncopation was found to be a good predictor of pulse-finding difficulty; syncopation causes off-beat taps and some switches between on-beat and off-beat tapping [14, 16].  4. DISCUSSION Syncopated rhythms present challenges for pulse detection algorithms. Looked at in the frequency domain, some syncopated rhythms do not contain any energy at the frequency of the pulse. Yet pulse is readily perceived in !!\"!#!\"$!\"$#!\"%&'()*+,-.!!\"!#!\"$!\"$#!\"%&'()*+,-./.+01234$\n5)+.2)'6788.2.)+729:*+.9+,2.\n;1)).9+*<*+=4>'++.2)?\n@2.A,.)9=4!\"%#4!\"#4$\"!4%\"!4B\"!4C\"!$D\"!/.+01234$/.+01234%788.2.)+E88.2.)+5)+.2)'6!\"#$%&'!\"#$%&(F14/.+01234@2.A,.)9*.?\n4\n&'()*+,-.!\"!%!\"!B!\"!D!\"!C!\"$4!\"%#4!\"#4$\"!4%\"!4B\"!4C\"!$D\"!4!\"%#4!\"#4$\"!4%\"!4B\"!4C\"!$D\"!F14/.+01234@2.A,.)9*.?\n44\n!!\"!#!\"$@21G4/.+01234@2.A,.)9*.?\n@2.A,.)9=4!\"%#4!\"#4$\"!4%\"!4B\"!4C\"!$D\"!4!\"%#4!\"#4$\"!4%\"!4B\"!4C\"!$D\"!\n!\"#$\"!!%#\"\"!!&#\"\"!!'#\"\"!!(#\"\"\"\"#$%)*+,-.+/+0*\n!\"#$\"!!%#\"\"!!&#\"\"!!'#\"\"!!(#\"\"\"\"#$%12'\n!\"#$\"!!%#\"\"!!&#\"\"!!'#\"\"!!(#\"\"\"\"#$%30456!7869:\n!\"#$\"!!%#\"\"!!&#\"\"!!'#\"\"!!(#\"\"\"\"#$%;<**</=!>08*:!%?*,<886@+.!A.:B0:/,C3:*D+/*:!E4D8<@0F:\n!!G:@H+.I!%G:@H+.I!&.Stimuli Network 2 Active Frequencies (Hz) Rhythm 1/2 2/3 1 2 4 Other Isochronous x  x x x  4/4 x  x x   3/4  x  x   Son Clave x  x x  0.62 Rumba Clave x  x x  0.62, 0.88 Hard Clave   x x  0.62, 1.24 Missing Pulse 1 x   x x 0.62 Missing Pulse 2    x x 0.63, 0.75 Missing Pulse 3    x x 0.62 Missing Pulse 4    x x 0.62, 0.88, 1.12 Missing Pulse 5 x  x x x 0.62, 0.75, 1.24 Missing Pulse 6    x x 0.62, 0.75 Missing Pulse 7 x   x  0.62, 0.75 Missing Pulse 8 x   x x 0.75 Missing Pulse 9    x x 0.75 Missing Pulse 10   x  x 0.63, 0.75, 1.26 \n189Poster Session 2\n   syncopated rhythms [1, 14]. From the point of view of music perception, this observation implies that the brain adds frequency components that are not objectively present in rhythms themselves. A lack of energy at the pulse frequency may explain why pulse detection methods based on linear resonance experience problems with syncopated rhythms. For syncopated rhythms, our nonlinear model, based on fundamental principles of neurodynamics, resonates at the pulse frequency. This qualitatively matches human performance [1], and the detailed responses of this model provide novel predictions which could be tested in future experiments. Our observations support the hypothesis that pulse corresponds to a neural resonance. In simple networks, nonlinear resonance by itself is capable of restoring a missing pulse frequency. When multiple networks of nonlinear oscillators are coupled together (including internal rhythmic connectivity within networks), they can resonate at a pulse frequency and related metrical frequencies, a form of temporal pattern matching or pattern completion. In future work, we plan to construct and test other models based on nonlinear resonance. For example, the results presented here do not enable us to say whether internal network connectivity or the thresholding properties of Network 2 were primarily responsible for the observed responses. Perhaps both are necessary. Future work in this area will focus on how the connectivity patterns between networks are learned  and address developmental aspects of pulse and meter as well as differences across cultures.  5. REFERENCES [1] Chapin, H.; Zanto, T. P.; Jantzen, K. J.; Kelso, J. A. S.; Steinberg, F.; Large, E. W., Neural responses to complex auditory rhythms: The role of attending. Frontiers in Auditory Cognitive Neuroscience 2010, 1 (224), 1-18. [2] Chen, J. L.; Penhune, V. B.; Zatorre, R. J., Listening to Musical Rhythms Recruits Motor Regions of the Brain. Cereb. Cortex 2008, 18 (12), 2844-2854. [3] Fitch, W. T.; Rosenfeld, A. J., Perception and Production of Syncopated Rhythms. Music Perception 2007, 25 (1), 43-58. [4] Grahn, J. A., The role of the basal ganglia in beat perception: neuroimaging and neuropsychological investigations. Ann. N. Y. Acad. Sci. 2009, 1169, 35-45  [5] Julicher, F., Mechanical oscillations at the cellular scale. Comptes Rendus De L Academie Des Sciences Serie Iv Physique Astrophysique 2001, 2 (6), 849-860. [6] Lakatos, P.; Karmos, G.; Mehta, A. D.; Ulbert, I.; Schroeder, C. E., Entrainment of neuronal oscillations as a mechanism of attentional selection. Science 2008, 320 (5872), 110-113. [7] Large, E. W.; Almonte, F.; Velasco, M., A canonical model for gradient frequency neural networks. Physica D: Nonlinear Phenomena 2010, 239 (12), 905-911. [8] Large, E. W.; Jones, M. R., The dynamics of attending: How people track time-varying events. Psychol. Rev. 1999, 106 (1), 119-159. [9] Large, E. W.; Kolen, J. F., Resonance and the perception of musical meter. Connect. Sci. 1994, 6, 177 - 208. [10] Lerdahl, F.; Jackendoff, R., A generative theory of tonal music. MIT Press: Cambridge, 1983. [11] London, J., Hearing in Time: Psychological Aspects of Musical Meter. Oxford University Press: Oxford, 2004. [12] Longuet-Higgins, H. C.; Lee, C. S., The Rhythmic Interpretation of Monophonic Music. Music Perception: An Interdisciplinary Journal 1984, 1 (4), 424-441  [13] McAuley, J. D., Perception of time as phase: Toward an adaptive-oscillator model of rhythmic pattern processing. Unpublished doctoral dissertation, Indiana University Bloomington. 1995. [14] Patel, A. D.; Iversen, J. R.; Chen, Y. Q.; Repp, B. H., The influence of metricality and modality on synchronization with a beat. Exp. Brain Res. 2005, 163 (2), 226-238. [15] Scheirer, E. D., Pulse tracking with a pitch tracker. In Applications of Signal Processing to Audio and Acoustics, 1997. 1997 IEEE ASSP Workshop on, 1997; p 4 pp. [16] Snyder, J. S.; Krumhansl, C. L., Tapping to ragtime: Cues to pulse finding. Music Perception 2001, 18, 455-489. [17] Snyder, J. S.; Large, E. W., Gamma-band activity reflects the metric structure of rhythmic tone sequences. Cognitive Brain Research 2005, 24 (1), 117-126. [18] Wilson, H. R.; Cowan, J. D., A mathematical theory of the functional dynamics of cortical and thalamic nervous tissue. Kybernetik 1973, 13, 55-80.  \n190"
    },
    {
        "title": "Automatic Pitch Detection in Printed Square Notation.",
        "author": [
            "Gabriel Vigliensoni",
            "John Ashley Burgoyne",
            "Andrew Hankinson",
            "Ichiro Fujinaga"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1415580",
        "url": "https://doi.org/10.5281/zenodo.1415580",
        "ee": "https://zenodo.org/records/1415580/files/VigliensoniBHF11.pdf",
        "abstract": "In this paper we present our research in the development of a pitch-finding system to extract the pitches of neumesâ€”some of the oldest representations of pitch in Western musicâ€” from the Liber Usualis, a well-known compendium of plainchant as used in the Roman Catholic church. Considerations regarding the staff position, staff removal, spaceand linezones, as well as how we treat specific neume classes and modifiers are covered. This type of notation presents a challenge for traditional optical music recognition (OMR) systems because individual note pitches are indivisible from the larger ligature group that forms the neume. We have created a dataset of correctly-notated transcribed chant for comparing the performance of different variants of our pitch-finding system. The best result showed a recognition rate of 97% tested with more than 2000 neumes.",
        "zenodo_id": 1415580,
        "dblp_key": "conf/ismir/VigliensoniBHF11",
        "keywords": [
            "pitch-finding system",
            "neumes",
            "Liber Usualis",
            "plainchant",
            "Roman Catholic church",
            "staff position",
            "staff removal",
            "spaceand linezones",
            "neume classes",
            "modifiers"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nAUTOMATIC PITCH RECOGNITION\nIN PRINTED SQUARE-NOTE NOTATION\nGabriel Vigliensoni, John Ashley Burgoyne, Andrew Hankinson, and Ichiro Fujinaga\nCentre for Interdisciplinary Research in Music Media and Technology (CIRMMT)\nMcGill University, Montr Â´eal, Qu Â´ebec, Canada\n[gabriel,ashley,ich]@music.mcgill.ca, andrew.hankinson@mail.mcgill.ca\nABSTRACT\nIn this paper we present our research in the development of a\npitch-ï¬nding system to extract the pitches of neumesâ€”some\nof the oldest representations of pitch in Western musicâ€”\nfrom the Liber Usualis , a well-known compendium of plain-\nchant as used in the Roman Catholic church. Considerations\nregarding the staff position, staff removal, space- andline-\nzones , as well as how we treat speciï¬c neume classes and\nmodiï¬ers are covered. This type of notation presents a chal-\nlenge for traditional optical music recognition (OMR) sys-\ntems because individual note pitches are indivisible from the\nlarger ligature group that forms the neume. We have created\na dataset of correctly-notated transcribed chant for compar-\ning the performance of different variants of our pitch-ï¬nding\nsystem. The best result showed a recognition rate of 97%\ntested with more than 2000 neumes.\n1. INTRODUCTION\nOptical music recognition (OMR) is the process of turn-\ning musical notation represented in a digital image into a\ncomputer-manipulable symbolic notation format. Music no-\ntation, however, comes in many different forms, and so there\nis no single OMR system that can recognize all types of mu-\nsic.\nPlainchant is a large collection of monophonic melodies,\nwhich exist as one of the oldest types of notated music in Eu-\nrope. These melodies have been part of (Western) Christian\nliturgy since the Middle Ages and have formed the basis for\nmuch Western music that followed. Its unique system of no-\ntation, constructed on groups of pitches known as neumes ,\nis still in use in liturgical settings and was most famously\nre-popularized in the late 19thCentury by the monks at the\nSolesmes monastery in France.\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proï¬t or commercial advantage and that copies\nbear this notice and the full citation on the ï¬rst page.\nc/circlecopyrt2011 International Society for Music Information Retrieval.Perhaps the most used book produced by the Solesmes\ncommunity was the Liber Usualis , a 2000-page service book\nthat contains most of the texts and chants for the ofï¬ces and\nmasses of the church, designed to be a practical reference\nbook by those responsible for performing these services. As\na modern production, many qualities of older manuscript\nsourcesâ€”notably the variation in scribes handwriting and\ndegradation due to ageâ€”are not present in the Liber , but\nits sheer size and comprehensiveness provides an excellent\nfoundation for training automatic neume recognizers.\nIn this paper we present our work to date for producing a\nneume recognition system. This work concerns the recogni-\ntion of printed neume pitches, speciï¬cally in the style used\nthroughout the Liber , known as square-note notation . In\nsquare-note notation, most neumes are ligatures that repre-\nsent multiple pitches, and so accessing the individual notes\nof a neume can be problematic. We have developed a sys-\ntem whereby we recognize the position of the ï¬rst pitch of a\nneume group, and then employ a unique approach using au-\ntomatic class labels to recognize the remainder of the pitches\nin that group.\n1.1 Notation: From Neumes to MEI\nWe have chosen the Music Encoding Initiative (MEI) for-\nmat1as the basis for capturing the output of our recogni-\ntion system. This format provides an extensible approach to\nencoding different types of music documents [7]. As part\nof this project, we have developed an extension to the MEI\nformat that captures the particular qualities and nuance of\nSolesmes-style square-note notation. We convert the output\nfrom the recognition system into MEI ï¬les through the use\nof the PyMEI library2.\n1.2 Notation Systems\nPlainchant notation is a precursor to modern music notation.\nInitially it was conceived as a means of providing some in-\ndication of melodic contour to a text that was chanted during\n1http://music-encoding.org/\n2https://github.com/ahankinson/pymei\n423Poster Session 3\na liturgical service, and provided no indication of absolute\npitch values. With the invention of the musical staff between\nthe 9thand 10thCenturies, some forms of neumes made the\ntransition to being â€œheightened,â€ or given absolute pitches in\nrelation to a staff and clef. Solesmes notation features abso-\nlute pitch, and so its pitch values may be extracted directly.\nDalitz et al. [1] have presented a system for symbol recog-\nnition of Byzantine chant notation using neume forms unique\nto this repertoire. This system of notation, however, does\nnot use a staff to specify absolute pitch. The neume shapes\nfor this type of notation speciï¬ed melodic contour and rel-\native pitch direction, and the authors do not report any at-\ntempt at supplying absolute pitch information. Gezerlis and\nTheodoridis [2] have also presented a system for recogniz-\ning Byzantine chant notation, however, like Dalitz et al.,\ntheir system does not perform any pitch transcription.\nLaskov and Dimitrov [3] have presented a system for per-\nforming image segmentation for neume notation, separat-\ning the neumes as objects of interest from the background.\nAgain, however, this system is for unpitched neumes and\nthus the authors made no attempt at extracting pitches.\nFinally, Ramirez and Ohya [4] have presented a classiï¬-\ncation system used to classify eight basic types of neumes\nin pitched manuscript sources from the 14thCentury. This\nsystem classiï¬es these symbols accurately, but the authors\ndid not report any attempt at performing pitch recognition\nfrom the recognized classes. The authors also made no in-\ndication of how their system would perform outside of the\neight standard neume forms they used.\nIn our paper, we address a method of performing neume\nclassiï¬cation and pitch transcription from 11 basic types of\nneumes and its multiple variants presented on a staff.\n2. WORKFLOW\nWe are creating a fully searchable, web-based version of the\nLiber . This project will allow users to search for speciï¬c\nmelodic sequences or patterns in the book; our system will\nreturn all matching patterns, highlighting them in the origi-\nnal image. To accomplish this goal, we needed to extract all\nmusical and textual information from the Liber . Although\nit contains text and music, this paper will focus only on the\nmusic. We are using the digitized, downloadable version\nof the Liber published by Descl Â´ee & Co in 19613, which\ncomes as a PDF ï¬le with 2340 pages. Different categories\nof information and content can be found on the pages: title,\ntext, staves, lettrines , and lyrics. These were separated into\ndifferent layers during an automated preprocessing step to\nallow for easier content analysis.\n3http://musicasacra.com/2007/07/17/\nliber-usualis-online/2.1 Page Preprocessing\nWe converted the PDF ï¬le into Tagged Image File Format\n(TIFF) image ï¬les so that they could be read by Aruspix [5],\na cross-platform application devoted to optical music recog-\nnition on music printed during the European Renaissance.\nThe preprocessing capabilities of Aruspix include skew\ncorrection, resizing, cleaning, staff-position retrieval, and\nclassiï¬cation of the elements in a page in the following cat-\negories: frames and borders, lettrines , lyrics, inter-staff ele-\nments, and titles. These page elements are uniquely coloured\nto allow the separation of the page elements into discrete\nlayers for extraction. The colouring feature is also use-\nful for manual correction. Aruspix returns a container ï¬le,\nwhich includes the original binarised TIFF image, a sec-\nond coloured TIFF image with each one of the page ele-\nments in a different colour, and an XML ï¬le with infor-\nmation about the processes performed on the page. After\nautomatically preprocessing the entire TIFF images using\nAruspix, we manually corrected any misclassiï¬ed page ele-\nments. We then modiï¬ed the Gamera4Aruspix (G4A) Gam-\nera Toolkit4to extract the preprocessed layers into sepa-\nrate images, providing us with images containing only mu-\nsic staves.\n2.2 Retrieving Staff Position and Removing Staff-Lines\nWe processed the images containing only music notation\nwith the MusicStaves Gamera Toolkit5. This toolkit al-\nlowed us to extract each staff line position in the staff, store\nits location for later determination of pitches and remove all\nlines for later glyph classiï¬cation. MusicStaves is designed\nto work with an arbitrary number of lines, which was re-\nquired because the Liber features 4-line staves.\n2.2.1 Staff Detection\nThe MusicStaves toolkit has a number of different algo-\nrithms for detecting the position of staff lines in the image\nand removing them, leaving only the remaining elements\nin the resulting image. These algorithms provide different\nresults depending on the notation style and image deforma-\ntions. We tested two different staff-ï¬nding algorithms. For\nthe ï¬rst approach, we retrieved the average vertical position\nof each one of the lines horizontally across the whole page,\nand named this approach AvgLines . This approach allowed\nus to determine the gross slope of a staff line across the page\nand thus to correct for staff lines that are not straight. In the\nsecond approach, we used the Miyao algorithm, capable of\nbreaking a staff line into equidistant segments to provided\na more precise means of determining horizontal staff slope\n4https://github.com/vigliensoni/G4A\n5http://lionel.kr.hs-niederrhein.de/ âˆ¼dalitz/data/\nprojekte/stafflines/\n42412th International Society for Music Information Retrieval Conference (ISMIR 2011)\nFigure 1 . Extract of neumes in page 1045, stave 2 of\ntheLiber with the notation we developed for encoding the\nglyphs and intervals for each one of the neume variations.\nchanges in inclined, braked, or ï¬‚exed staves [8]. We pre-\nserved the position of all staff segments and named this ap-\nproach Miyao .\n2.2.2 Staff Removal\nA previous study by Dalitz et al. [6] shows that there is no\nsingle superior algorithm for performing staff removal. On\na number of different metrics, they observe that for images\nwith deformations, the performance of many algorithms for\nstaff removal is very similar, and so there is no clear best\ntechnique. We informally tested the ï¬ve algorithms pro-\nvided by MusicStaves and determined that for our reper-\ntoire, the Roach & Tatem algorithm performed the best.\n2.3 Neume Classiï¬cation and Labeling\nAfter the detection and removal of the staves, the images\nwere loaded into the Gamera interactive classiï¬er. The glyphs\nfrom 40 pages of the Liber were manually classiï¬ed to cre-\nate a training dataset for later automatic classiï¬cation. Our\npitch-ï¬nding approach would only retrieve the position of\nthe ï¬rst part of a glyph, so for different variations of neumes,\nan encoding scheme was developed that uniquely captured\nthe shape of the entire neume, grouping like neumes into the\nsame class. This encoding scheme identiï¬ed the intervals as\nwell as other auxiliary shapes in the neume and served as a\nclass identiï¬er. Those other elements that have an effect on\nthe shape of a neume, such as dots, horizontal and vertical\nepisemas , the quilismas , which are alterations of the note\nshapes, or the combination of some of those elements, are\nexplicitly declared and encoded. Figure 1 shows an excerpt\nof the Liber with the notation we developed for encoding\nthe neumes and other notational elements. It can be seen\nthat the interval between two consecutive points in a neume\nis encapsulated into the glyph class name in the Gamera in-\nteractive classiï¬er. Consequently, ï¬nding the pitch of the\nstarting note allows us to derive pitches for all notes in a\ngiven neume.2.3.1 Automated Neume Pre-classiï¬cation\nAfter manually training the classiï¬er with 40 pages of neumes,\nwe had a dataset large enough to be used as a model for au-\ntomatic classiï¬cation of elements for the remaining pages.\nA classiï¬cation and optimization script was developed to\nautomatize the process of classifying elements in the new\npages and optimize the classiï¬er. By this means, the classi-\nï¬er increased in size with new neumes or new neume varia-\ntions only.\n2.3.2 Human-supervised Neume Classiï¬cation\nThere were inevitable errors in the automatic classiï¬cation,\nand so we performed a manual correction of each page by\nmusicologists trained in the Solesmes notation system. This\nensured that all glyphs for every single page were correct.\n3. PITCH-FINDING APPROACHES\nTo calculate the pitch of the starting point for each neume,\nwe needed to know its location on the staff as well as the\nclef type and its position. To accomplish this, we created\nimaginary lines, ledger lines , and zones, line- andspace-\nzones , in the staff and calculated the placement of neumes\nin these zones.\n3.1 Ledger Lines\nWe detected the staff-line positions using the Miyao and\nAvgLines approaches described previously. However, some\nneumes, or notes inside a neume, were located on ledger\nlines above the ï¬rst (upper) or below the fourth (lower) line.\nTherefore, we virtually extend the number of staff lines by\ncreating four imaginary ledger lines, two above the stave\nand two below. The distance between two staff lines for the\nledger lines was determined by projecting the distance of the\nclosest actual staff lines as the ledger lines.\n3.2 Space- and Line-Zones\nNotes of a given neume can be located either on the staff\nlines or in the spaces between lines. We deï¬ned imaginary\nzones between the staff lines where the neumes could be lo-\ncated. We calculated these imaginary zones by segmenting\nthe space between two lines into four segments. The sec-\nond and third segments correspond to a space-zone , and the\nï¬rst and fourth were grouped with the previous fourth and\nï¬rst, respectively, to designate a line-zone . We assigned to\neach zone an unique number which corresponds to what we\nnamed the note-position of each neume on the staff. Figure\n2 shows the imaginary ledger lines and zones in a stave.\n3.3 Bounding Box\nTo determine the pitches of the neumes, we ï¬rst tested an\napproach based on the bounding boxes of each one of the\n425Poster Session 3\nFigure 2 . Stave showing imaginary upper and lower ledger\nlines as well as line- andspace-zones .\nglyphsâ€”that is, the rectangular area that deï¬nes the bounds\nof a particular glyph. These bounding boxes are generated\nby Gamera and can be easily accessed. We determined that\nmany neumes began in either the top left of the bounding\nbox (the upposition) or the bottom left of the bounding box\n(thedown position). A neume position was pre-determined\nfor its speciï¬c neume class, and the up or down position of\nthe ï¬rst note was correlated with all possible line or space\nzones in a staff. From there the pitch of the ï¬rst note of the\nneume was determined. Although this approach worked for\nmost of the neumes, some of them, especially torculus and\ncompound neumes (see Figure 1, glyph 8), do not neces-\nsarily start with a note in the upper or lower position. The\nstarting note of those neumes can be located between their\nbounding boxâ€™s left vertices, making the bounding box ap-\nproach impractical for use across the entire Liber . Because\nof this limitation we abandoned this approach.\n3.4 Horizontal Projection and Center of Mass\nA more robust approach is based on the horizontal projec-\ntion of the neumes. To ï¬nd the starting note for each one of\nthe neumes we created a sub-imageâ€”a vertically split ver-\nsion of the original neume with a width of the size of a single\npunctum . This unit was chosen because we considered the\npunctum (see Figure 1, glyph 3) as the nominal neume unit.\nBefore creating the sub-images, we calculated the average\npunctum size among all punctums on a page and used that\nsize for any sub-image creation. We then retrieved the hor-\nizontal projection of each one of the neumes and calculated\nits centre-of-mass, that is, the point around which the black\npixels of the sub-image were equally distributed. By using\nthis method, we found the mean location of the starting po-\nsition of a neume, and we determined the staff zone for this\nstarting point, allowing us to automatically derive its start-\ning pitch.3.5 Clef Type and Position for Shifting Pitch Notes\nThe pitches of the neumes in a staff depend on the clef type,\nCorF, and its line position on the staff. The Gamera clas-\nsiï¬cation process does not treat the clef in a special way,\nand so a method must be devised to allow any pitch-ï¬nding\nsystem to automatically detect the closest clef on a given\nstaff so that all notes on that staff may be correctly identi-\nï¬ed. The coordinates (relative to the staff boundaries) where\neach neume was located was stored temporarily. After all\nelements in the staff were classiï¬ed, they were then sorted\naccording to their vertical position, rounded to the nearest\nstaff boundary, and then by their position on the x-axis, left\nto right. This produced a sequence of neumes, ordered by\nstaff and then occurrence on the staff, and their pitch was\nassigned according to the clef, which is always the ï¬rst ele-\nment for each stave.\n3.6 Special Neumes and Neume Modiï¬ers\nAlthough the centre-of-mass approach provided a more ro-\nbust method of initial pitch detection, there were still some\nneume shapes that required further processing, in particu-\nlar the podatus ,epiphonus ,cephalicus ,scandicus , and their\nvariations (see Figure 1, glyphs 4, 5, 6, and 7). These shapes\nhave sub-images with two notes or elements that are verti-\ncally stacked, shifting the centre-of-mass and making our\nprojection system an inaccurate method of ï¬nding the ini-\ntial pitch position of the neume on the staff. To ï¬x this\nissue and improve accuracy, we made a number of excep-\ntions where the sub-image was determined by ï¬rst split-\nting these neumes horizontally. The centre-of-mass of the\nsub-imageâ€™s largest connected component was then consid-\nered the centre-of-mass of the neume. Similar processing\nwas needed for neumes that had horizontal episemas, ver-\ntical episemas, or dots. Their centre-of-mass was shifted\ndue to the presence of these modiï¬ers, and so we removed\nthese elements before calculating their vertical position on\nthe staff. We left this feature of treating some neumes and\nneume modiï¬ers in the described way as an option for test-\ning its performance in comparison to the standard approach.\nWe named the former Exceptions and the latter No Excep-\ntions .\n3.7 Moving the Space- and Line-Zones\nThe position of the space- and line-zones in relation to the\nstaff lines has an impact in the performance of the pitch-\nï¬nding system, and we informally tested several values for\nthis relation in order to see how its performance could be\nimproved. For the ï¬nal comparison of settings, we tried two\napproaches for the spacing: a regular spacing of the zones,\nalready described, and a shifted spacing, with the upper line\nof the space-zone shifted down by 2/16 and the lower one\nby 1/16 of the staff-space in that staff segment.\n42612th International Society for Music Information Retrieval Conference (ISMIR 2011)\nFigure 3 . Visualization of pitch ï¬nd algorithm performance in page 1242 stave 5 of the Liber Usualis using the Python Imaging\nLibrary and the original image.\n4. TESTING AND RESULTS\nTo evaluate the performance of our six pitch-ï¬nding sys-\ntems and their variants (Miyao, AvgLines, Exceptions, No\nExceptions, Regular Spacing, Shifted Spacing), we created\na ground truth dataset consisting of 20 random pages with\na total of 2219 neumes and 3114 pitches correctly labeled.\nWe used the MEI format for storing the music notation. This\nformat has the ability to correlate zones on an image with\nmusical structures encoded in XML [7]. We developed a\nscript for highlighting neumes on the page image and iden-\ntifying its pitch in text next to the note on the screen (Figure\n3). This visualization tool allowed us to quickly identify and\ncorrect the miscalculated pitches in the MEI ï¬le in order to\ncreate a ground truth dataset.\nWe tested our system using six different variants based on\nthe staff-line detection approaches we developed, the treat-\nment of glyphs with special conditions, and a shift in the\nspacing of the line- and space-zones. The nomenclature we\nused for these variants can be seen in Table 1.\nTable 1 . Nomenclature used for the different variants of\nthe experimental testing for the performance of our pitch-\nï¬nding system.\nFigure 4 shows the recognition rates achieved by the six\nmethods for the different neume classes (the compound and\nscandicus neumes are not included in the graph because they\nare relatively rare in our dataset.)\nOverall, the best variant performance was MES, i.e., the\nvariant that included the Miyao algorithm, the handling of\nspecial exceptions, and correction for vertical spacing, with\na 97% recognition rate in ï¬nding the pitch of the ï¬rst note\nof a neume only. This value was reduced to 95% when we\nretrieved and compared all notes from all neumes with the\nground-truth dataset. ANR performance was the worst at\n85% for the ï¬rst note pitch and 81% for all pitches.From the graph we can see that the cephalicus andpo-\ndatus , and to a lesser degree the epiphonus ,scandicus , and\ntorculus , share a common pattern: their best results were\nachieved with the MES, AES, and MER variants, i.e., the\nvariants that include handling of the special exceptions. These\nare the neume shapes for which the special exceptions to\nthe centre-of-mass approach were designed, and so, it is\nclear that these special exceptions were necessary. On the\nother hand, clivis ,porrectus , and punctums , and to a lesser\ndegree, the virgas , have in common that their high perfor-\nmance was accomplished using MES and MNS, i.e., the\nvariants that include both the Miyao algorithm and correc-\ntion for vertical spacing. Collectively, these neumes rep-\nresent more than 80% of all neumes in the dataset, and so\nit is clear that the use of the Miyao approach with shifted\nspacing is important for achieving the most accurate recog-\nnition possible. Conï¬rmatory testing with logistic regres-\nsion showed that overall, all three of our innovationsâ€”use\nof the Miyao algorithm, special treatment of exceptions, and\nspacing correctionâ€”produced statistically signiï¬cant ( Î±=\n0.05) improvements to the recognition rate. Furthermore, as\nFigure 4 illustrates, these improvements are large enough to\nmake an important difference in the quality of our output,\nbringing the overall recognition rate from 85% to 97%.\n5. FUTURE WORK\nFormal research should be done to determine the best po-\nsition and spacing of the line- and space-zones. We discov-\nered that this feature is an important factor to ï¬nd the correct\npitches for some neumes. Secondly, we want to calculate the\nperformance of all the automated workï¬‚ow, including the\nneume classiï¬cation and pitch recognition, to see how our\nsystem performs automatically, without human supervision.\nAs was stated, at the end of our project we will have the\nentire Liber Usualis fully transcribed and searchable. We\nhope that it will be a great source of information for mu-\nsicologists as well as church goers and musicians. At the\nsame time, however, we will have a massive ground truth\nof correctly transcribed melodies and neumes, and it could\nbe used as the starting point for digitizing and research us-\ning other books and manuscripts with Solesmes and similar\nnotation.\n427Poster Session 3\nFigure 4 . Precision and error bars for the neume classes in the ground truth dataset in ï¬nding pitches of the six variants we\ntested.\n6. ACKNOWLEDGEMENTS\nThe authors would like to thank our great development team\nfor their hard work: Remi Chiu, Mahtab Ghamsari, Jamie\nKlassen, Saining Li, Wendy Liu, Mikaela Miller, Laura Os-\nterlund, Alastair Porter, Laurent Pugin, Caylin Smith and\nJessica Thompson. This project has been funded with the\ngenerous ï¬nancial support of the Social Sciences and Hu-\nmanities Research Council (SSHRC) of Canada.\n7. REFERENCES\n[1] Dalitz, C., G. Michalakis, and C. Pranzas. 2008. Optical\nrecognition of psaltic Byzantine chant notation. Interna-\ntional Journal on Document Analysis and Recognition\n11 (3): 143â€“58.\n[2] Gezerlis, V . G., and S. Theodoridis. 2002. Optical char-\nacter recognition of the orthodox Hellenic Byzantine\nmusic notation. Pattern Recognition 35 (4): 895â€“914.\n[3] Laskov, L., and D. Dimov. 2007. Color image segmenta-\ntion for neume note recognition. Proceedings of the In-\nternational Conference on Automatics and Informatics .\nSoï¬a. 37â€“41.[4] Ramirez, C., and J. Ohya. 2010. Symbol classiï¬cation\napproach for OMR of square notation manuscripts. Pro-\nceedings of the 11th International Society for Music In-\nformation Retrieval Conference . Utrecht. 549â€“53.\n[5] L. Pugin. 2009. Editing Renaissance Music: The Arus-\npix Project. In Digitale Edition zwischen Experiment\nund Standardisierung Musik - Text - Codierung , ed.\nStadler, P., and J. Veit. 147â€“56. T Â¨ubingen: Max\nNiemeyer Verlag.\n[6] Dalitz, C., M. Droettboom, B. Czerwinski, and I. Fuji-\nnaga. 2008. A comparative study of staff removal algo-\nrithms. IEEE Transactions on Pattern Analysis and Ma-\nchine Intelligence 30 (5): 753â€“66.\n[7] Hankinson, A., L. Pugin, and I. Fujinaga. 2010. An in-\nterchange format for optical music recognition applica-\ntions. Proceedings of the 11th International Society for\nMusic Information Retrieval Conference . Utrecht. 51â€“6.\n[8] Miyao, H., and M. Okamoto. 2004. Stave extraction\nfor printed music scores using DP matching. In Journal\nof Advanced Computational Intelligence and Intelligent\nInformatics 8 (2): 208â€“15.\n428"
    },
    {
        "title": "Peachnote: Music Score Search and Analysis Platform.",
        "author": [
            "Vladimir Viro"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1417261",
        "url": "https://doi.org/10.5281/zenodo.1417261",
        "ee": "https://zenodo.org/records/1417261/files/Viro11.pdf",
        "abstract": "Hundreds of thousands of music scores are being digitized by libraries all over the world. In contrast to books, they generally remain inaccessible for content-based retrieval and algorithmic analysis. There is no analogue to Google Books for music scores, and there exist no large corpora of symbolic music data that would empower musicology in the way large text corpora are empowering computational linguistics, sociology, history, and other humanities that have printed word as their major source of evidence about their research subjects. We want to help change that. In this paper we present the first result of our work in this direction the Music Ngram Viewer and search engine, an analog of Google Books Ngram Viewer and Google Books search for music scores.",
        "zenodo_id": 1417261,
        "dblp_key": "conf/ismir/Viro11",
        "keywords": [
            "Digitization",
            "Music scores",
            "Content-based retrieval",
            "Algorithmic analysis",
            "Google Books",
            "Corpora",
            "Musicology",
            "Computational linguistics",
            "Sociology",
            "History"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nPEACHNOTE: MUSIC SCORE SEARCH AND ANALYSIS PLATFORM\nVladimir Viro\nLudwig-Maximilians-University Munich\nABSTRACT\nHundreds of thousands of music scores are being digitized\nby libraries all over the world. In contrast to books, they\ngenerally remain inaccessible for content-based retrieval and\nalgorithmic analysis. There is no analogue to Google Books\nfor music scores, and there exist no large corpora of sym-\nbolic music data that would empower musicology in the\nway large text corpora are empowering computational lin-\nguistics, sociology, history, and other humanities that have\nprinted word as their major source of evidence about their\nresearch subjects. We want to help change that. In this pa-\nper we present the ï¬rst result of our work in this direction\n- the Music Ngram Viewer and search engine, an analog of\nGoogle Books Ngram Viewer and Google Books search for\nmusic scores.\n1. INTRODUCTION\nThis project seeks to do for music scores what Google Books\nSearch does for books. We are aiming at indexing all scanned\nmusic scores and making their content available for query-\ning and algorithmic analysis. We would like to help build\nup the foundation needed for computational musicology re-\nsearch by assembling a large corpus of symbolic music data.\nWe have developed a search engine and processing pipeline\nfor scores from the Petrucci Music Library (IMSLP,\nhttp://imslp.org), the largest music score library on the Inter-\nnet. Our system takes the scores in PDF format, runs optical\nmusic recognition (OMR) software over them, indexes the\ndata and makes them accessible for querying and data min-\ning. The search engine is built upon Hadoop and HBase and\nruns on a cluster. Our system has already recognized more\nthan 250 million notes from about 650 thousand sheets, or\n45 thousand scores.\nWe chose the Petrucci library as our ï¬rst data source be-\ncause of the low entry barrier: both the scores and their\nscans at the IMSLP are free from copyright, and so we were\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proï¬t or commercial advantage and that copies\nbear this notice and the full citation on the ï¬rst page.\nc/circlecopyrt2011 International Society for Music Information Retrieval.free to use them without asking for permission. Therefore at\nthe beginning of the development it was the easiest collec-\ntion to work with. But the Petrucci Library contains only a\nsmall part of all scores digitized by the libraries worldwide.\nWe would like to help libraries not only make their score\ncollections searchable, but also to present them in novel\nways. In this paper we present one such interface - the Mu-\nsic Ngram Viewer and search engine.\nThe paper is structured as follows. First, we provide a\nshort review the related work in the areas of symbolic mu-\nsic corpora and music search engines. Then we introduce\nour search engine and analysis platform, describe its archi-\ntecture and talk about the data collected so far. The next\nsection presents the application built on top of the platform,\nthe Music Ngram Viewer and search engine. We provide\nsome statistics collected during the ï¬rst three months after\nthe public launch of the Ngram Viewer. This section is fol-\nlowed by a short conclusion.\n2. RELATED WORK\n2.1 Music data collections\nExisting corpora of symbolic music data vary in size and\nquality. Probably the largest collection is the Kunst der\nFuge collection with about 18,000 MIDI ï¬les (mostly piano\nworks or reductions) contributed by the Internet users. A\ncomparably large collection can be accessed via the search\nengine at Musipedia.com, although the data set is not avail-\nable for download or purchase. A collection from the Cen-\nter for Computer Assisted Research in the Humanities at\nStanford University is of excellent quality, containing com-\nplete orchestral scores in MusicXML format, but is com-\nparably small with 880 manually encoded compositions in\n4116 movements. It also provides a search interface for the\ncollected data, the Themeï¬nder. The online version of Bar-\nlow and Morgensternâ€™s Dictionary of Musical Themes con-\ntains 9,825 monophonic melodies of a few measures length.\n2.2 Search engines and interfaces\nTwo existing systems are most relevant for our work: the\nMusipedia search engine and the Probado project.\nMusipedia offers multiple querying interfaces: query by\nhumming, virtual keyboard, search by rhythm and by typed\n359Poster Session 3\nin melody. The database behind Musipedia is assembled\nfrom different MIDI and MusicXML collections. Most mu-\nsic is either composed or transcribed for piano, and there are\nfew orchestral scores in the system.\nThe Probado project offers a very advanced interface for\nsimultaneously browsing the scores and the audio record-\nings aligned to them (cf. [2], [3]). The scores have been\nrecognized using the SharpEye OMR software.\n3. SEARCH ENGINE AND ANALYSIS PLATFORM\n3.1 System architecture\nOur system consists of two major components: the frontend\nand the backend.\nThe backend is responsible for importing, processing and\nindexing the scores and the metadata. For importing and\npreprocessing the scores we use a cluster of Linux machines.\nThe workï¬‚ow relies on Amazonâ€™s Simple Queue Service for\npassing tasks between different processing steps.\nWe have implemented wrappers for various optical music\nrecognition systems: an open source Java-based Audiveris,\nand the proprietary Windows-based and GUI-only Sharp-\nEye, CapellaScan and Smartscore. For the GUI-only OMR\nsystems we implemented wrapper scripts that allow us to\nintegrate these systems into the recognition workï¬‚ow while\nrunning inside the VMWare virtual machines. After evaluat-\ning these OMR systems in our environment we came to the\nconclusion that Smartscore currently offers the best recog-\nnition rates among the four systems we tested, and so the\nmajority of the scores in our database are recognized using\nSmartscore 10.3.2.\nThe workï¬‚ow components responsible for indexing and\nmetadata processing are running in the Hadoop and HBase\nenvironments [8]. The frontend presenting the processed\ndata is hosted on Googleâ€™s App Engine.\nUsing HBase for data storage offers the advantage of\nbuilt-in redundancy and compression. Currently, the inverse\nindex of the ngram viewer and the search engine, which\nare described in the next section, uses 50 Gigabytes. With-\nout compression, this number would be an order of mag-\nnitude higher. Another advantage of using Hadoop in the\nprocessing backend is the ability to scale it easily with vari-\nous providers, like Amazon EC2 or supercomputing centers,\nwhich is beneï¬cial for a research project.\nUsing Google App Engine for the frontend has the ben-\neï¬t of reliability, security and ease of development and de-\nployment. In our setup we use the App Engine also as a\ncaching layer for the Hadoop backend, where the bulk of\nthe data is stored.3.2 Data\nCurrently the search engine contains the data from the Petrucci\nMusic Library. The system has already recognized more\nthan 1,000,000 sheets from more than 65,000 scores. Here\nare some occurrence counts of musical symbols recognized\nby the system. The database contains 264M notes, 45M\nmeasures, 3.7M keys, 2.8M parts, 630K staves, 52K trill\nmarks and 23 fffff signs. The following ï¬gure contains the\noccurrence counts of piano signs:\np 1808243\npp 403366\nppp 20945\npppp 1024\nppppp 10\npppppp 2\nFigure 1 . Counts of piano signs in the IMSLP scores recog-\nnized so far.\n4. MUSIC NGRAM VIEWER AND SEARCH\nENGINE\nInspired by the Google Books Ngram Viewer [1], we imple-\nmented a similar application for music scores on top of our\nplatform. We extracted the score metadata provided by the\nusers of Petrucci Music Library from the web site. For all\nscores with available date of composition or at least of ï¬rst\npublication (about two thirds of all scores), for all voices we\nextracted all melodies of up to ï¬fteen notes length. Chords\nwere represented as rising note sequences. Then, for each\nyear we stored the occurrence counts of melodies that oc-\ncurred three or more times in scores published or composed\nduring that year. We published our system at\nwww.peachnote.com. We also provided the dataset behind\nthe Ngram Viewer under the Creative Commons Attribution\nlicense. As far as we know, these are the ï¬rst publicly avail-\nable system and dataset of the kind.\n4.1 User Input\nUsers can use virtual piano keyboard implemented in Flash\nto enter their queries. In the current version query are se-\nquences of pitches. The note duration is not considered.\n4.2 Ngram Viewer\nCurrently the database contains ngrams up to the length 15,\nor melodies of up to sixteen notes. If a melody occurs in\nsome year more than two times, it is stored in the database.\nThis results in approximately 200 million ngram-year records\nin the database.\n36012th International Society for Music Information Retrieval Conference (ISMIR 2011)\nFigure 2 . Occurrences of the Ode to Joy motif\nThe above chart shows the occurrences of the Ode to Joy\nmotif from Beethovenâ€™s Ninth Symphony, encoded differ-\nentially (the numbers represent differences between conse-\nquent notes) - a 7-gram, â€0 1 2 0 -2 -1 -2â€. What the y-axis\nshows is this: of all the 7-grams contained in the OMRâ€™ed\nscores from IMSLP, the Petrucci Music Library, how many\nare identical with the ï¬rst 8 notes of Ode to Joy up to a patch\nshift? Here, you can observe a peak around 1822 - the year\nof the Ninthâ€™s composition. Apparently, the score of the\nNinth symphony contains most occurrences of this pattern.\nIt is interesting to learn what the other peaks are. Our search\nengine described in the next section provides an answer to\nthis question.\nThe next graph shows the frequency of occurrence of ma-\njor and minor chords:\nThe following graph shows the emergence of the whole-\ntone scale at the turn of the 20th century.\nThe graph below depicts the number of occurrences of\ntwelve intervals from the minor second to the octave in our\ndatabase, by year:\nThe gap between 1925 and 2000 is due to scores still\nbeing under copyright protection and hence unavailable on\nIMSLP. Modern composers, however, are free to upload\ntheir own compositions, and indeed they do so, as the bump\non the right tells.\nThe next ï¬gure shows the data for the same time frame\nand same intervals, but this time it is normalized by the total\nnumber of notes published in a given year and stored in our\ndatabase.\nThe more scores we have for any given year, the more\nreliable are the statistics.\n4.3 Search Engine\nFor each ngram which is stored in the Ngram Viewer dataset,\nwe also provide the information about the scores containing\nthe given sequence. Using the dynamic ngram chart users\ncan select the time range and get the list of scores composed\nduring this time which contain the given note sequence. The\nlist of compositions is paginated and sorted by the number\nof occurrences of the query in the scores. For each score\nwe provide a list of pages containing the query. In future\nreleases we will display the score sheets and highlight the\nlocations of the queried note sequences. Also, for queries\nreturning less than 10,000 scores we provide users the abil-\nity to ï¬lter the search results by text, using corresponding\n361Poster Session 3\ntags provided by users of the IMSLP website. This way\nusers can select pieces of particular genre (for example sym-\nphonies or quartets), participating instrument or instrument\ngroup (harp, winds), or composer.\n4.4 Usage data\nThe system has been launched on May 5-th of this year,\nwhen the Petrucci library added the â€Search by Melodyâ€\nlink on its home page. There has been a short announce-\nment on the IMSLP Journal, but apart from that we have not\npromoted the search engine in any way, since we wanted\nto test it and improve its quality ï¬rst. We installed Google\nAnalytics to gain insights into our usersâ€™ behavior. In the\nfollowing we present a few data points we collected using\nGoogle Analytics.\nIn the ï¬rst three months the system has been used by\nmore than 50,000 people from over 160 countries. On av-\nerage the search engine processed a search query every 5\nseconds.\nTo see how the system has been used by people who are\nreally interested in the insights it provides and to separate\nthem from casual users, we looked at the statistics for visits\nwith duration longer than 20 minutes. There have been 1385\nsuch visits, and the average time on site was 60 minutes,\nwhich gives a total of 1385 hours of intensive research using\nthe database. We also looked at the number of users who\nvisited the website often. More than 1500 people used the\nsystem more than 10 times, 426 users visited the site more\nthan 50 times, and 177 of them visited more than 100 times.\nThe ï¬les from the Ngram dataset have been downloaded\nmore than 800 times.\n5. CONCLUSION\nIn this paper we have presented a new music score search\nengine and analysis platform. The system opens new ways\nto explore notated music. The users can easily obtain in-\nsights that were hard to come by in the past. We also pro-\nvide a large data set that can be used in computational mu-\nsicology research. We continue digitizing score collections\nand will build additional search indexes that will allow more\nprecise and musically meaningful queries.\n6. REFERENCES\n[1] Jean-Baptiste Michel, Yuan Kui Shen, Aviva P. Aiden,\nAdrian Veres, Matthew K. Gray, The Google Books\nTeam, Joseph P. Pickett, Dale Hoiberg, Dan Clancy,\nPeter Norvig, Jon Orwant, Steven Pinker, Martin A.\nNowak, and Erez Lieberman Aiden: Quantitative Anal-\nysis of Culture Using Millions of Digitized Books. Sci-\nence 1199644 Published online 16 December 2010.[2] Juergen Diet, Christian Goehlert: Innovative Erschlies-\nsung und Bereitstellung von Musikdokumenten im\nProbado-Projekt. Zeitschrift Forum Musikbibliothek,\nV ol. 30 No 3/2009.\n[3] F. Kurth, D. Damm, C. Fremerey, M. Mueller and M.\nClausen: A Framework for Managing Multimodal Dig-\nitized Music Collections. Proceedings of 12th Euro-\npean Conference on Research und Advanced Technol-\nogy for Digital Libraries (ECDL 2008), Aarhus, Den-\nmark, September 14-19, 2008.\n[4] Akira Maezawa, Hiroshi G. Okuno, Tetsuya Ogata,\nMasataka Goto: Polyphonic Audio-to-Score Alignment\nBased on Baysian Latent Harmonic Allocation Hidden\nMarkov Model. ICASSP 2011.\n[5] M. Szwoch: Using MusicXML to evaluate accuracy of\nOMR systems. In Diagrammatic Representation and In-\nference: Proc. Diagrams 2008, volume 5223 of Lecture\nNotes in Computer Science, pages 419-422. Springer\nVerlag, Berlin. Herrsching, Germany, September 19-21,\n2008.\n[6] M. Droettboom, I. Fujinaga: Symbol-level\ngroundtruthing environment for OMR. International\nSymposium on Music Information Retrieval. Barcelona,\nSpain. 2004.\n[7] D. Byrd, W. Guerin, M. Schindele, I. Knopke, OMR\nEvaluation and Prospects for Improved OMR via Multi-\nple Recognizers. 2009.\n[8] Jeffrey Dean and Sanjay Ghemawat: MapReduce: sim-\npliï¬ed data processing on large clusters. Commun. ACM\n51, 1 (January 2008), 107-113.\n[9] Shyamala Doraisam: Polyphonic Music Retrieval: The\nN-gram Approach, PhD Thesis, Imperial College Lon-\ndon, 2004.\n[10] Shyamala Doraisamy and Stefan R Â¨uger: Robust poly-\nphonic music retrieval with n-grams. Journal of Intelli-\ngent Information Systems 21 (1): 5370. 2003.\n[11] J. Stephen Downie: Evaluating a Simple Approach to\nMusic Information Retrieval: Conceiving Melodic N-\nGrams as Text. Ph.D. Thesis, The University of Western\nOntario. 1999.\n362"
    },
    {
        "title": "Music Emotion Classification of Chinese Songs based on Lyrics Using TF*IDF and Rhyme.",
        "author": [
            "Xing Wang",
            "Xiaoou Chen",
            "Deshun Yang",
            "Yuqian Wu"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1416458",
        "url": "https://doi.org/10.5281/zenodo.1416458",
        "ee": "https://zenodo.org/records/1416458/files/WangCYW11.pdf",
        "abstract": "This paper presents the outcomes of research into an automatic classification system based on the lingual part of music. Two novel kinds of short features are extracted from lyrics using tf*idf and rhyme. Meta-learning algorithm is adapted to combine these two sets of features. Results show that our features promote the accuracy of classification and meta-learning algorithm is effective in fusing the two features.",
        "zenodo_id": 1416458,
        "dblp_key": "conf/ismir/WangCYW11",
        "keywords": [
            "automatic classification system",
            "lingual part of music",
            "novel kinds of short features",
            "tf*idf",
            "rhyme",
            "meta-learning algorithm",
            "fusion of features",
            "accuracy of classification",
            "effective in fusing",
            "two sets of features"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nMUSIC EMOTION CLASSIFICATION OF CHINESE SONGS\nBASED ON LYRICS USING TF*IDF AND RHYME\nXing Wang, Xiaoou Chen, Deshun Yang, Yuqian Wu\nInstitute of Computer Science and Technology, Peking University\n{wangxing,chenxiaoou,yangdeshun,wuyuqian }@icst.pku.edu.cn\nABSTRACT\nThis paper presents the outcomes of research into an auto-\nmatic classiï¬cation system based on the lingual part of mu-\nsic. Two novel kinds of short features are extracted from\nlyrics using tf*idf and rhyme. Meta-learning algorithm is\nadapted to combine these two sets of features. Results show\nthat our features promote the accuracy of classiï¬cation and\nmeta-learning algorithm is effective in fusing the two fea-\ntures.\n1. INTRODUCTION\nMusic itself is an expression of emotion. Music emotion\nplays an important role in music information retrieval and\nrecommendation system. Because of the explosive growth\nof music libraries, traditional emotion annotation carried out\nonly by experts can no longer satisï¬es the needs. Thus, auto-\nmatic recognition of emotions becomes the key to the prob-\nlem. Though having received increasing attention, it is still\nat the early stage. [5]\nMany methods have been applied to automatic classiï¬-\ncation of songsâ€™ emotions. Traditionally, features such as\nMFCC and chord are extracted from audio content to build\nemotion classiï¬ers. Natural language texts are the abstrac-\ntion of the human cognition, emotion included. Endowed\nwith emotion, lyrics are quite effective in predicting music\nemotion [2]. As the Internet booms, music related web doc-\numents and social tags [13] also provide valuable resources.\nWith the complementarities of features extracted from dif-\nferent modalities,more and more work [6] focus on multi-\nmodal classiï¬cation.\nHere we focus on the emotion classiï¬cation of music\nbased on lyrics only. As it is pointed out in [5], lyrics based\napproaches are particularly difï¬cult because feature extrac-\ntion and schemes for emotional labeling of lyrics are non-\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proï¬t or commercial advantage and that copies\nbear this notice and the full citation on the ï¬rst page.\nc/circlecopyrt2011 International Society for Music Information Retrieval.trivial, especially when considering the complexities involved\nwith disambiguating affect from text. In spite of those dif-\nï¬culties, the linguistic aspects of songs contains lots of e-\nmotion information. Firstly, some lexical items in lyrics are\nhighly relevant to certain emotion. Secondly, the pronun-\nciation of words must conform with the emotion, just as in\nspoken language, loudness and pitch play an important role\nin identifying the speakersâ€™ emotion [4].\nIn this work, we propose two sets of low dimensional fea-\ntures based on lyrics. We extend the work of Zaanen [11] to\nget the ï¬rst set of features based on tf*idf while the other is\nproposed based on rhymes [1]. Then classiï¬er combination\napproach is adopted to fuse these two sets of features.\nThe rest of this paper is organized as follows. We ï¬rst\npresent related work(Section 2). Then we will describe the\ntaxonomy of emotion(Section 3), features devised for emo-\ntion classiï¬cation(Section 4) and classiï¬er combination ap-\nproach(Section 5). Experimental results and analyses are\npresented in Section 6. Section 7 concludes the paper.\n2. RELATED WORK\nRelatively few research focuses on the use of lyrics as the\nsole feature for emotion classiï¬cation. Traditional methods\nsuch as the Vector Space Model(VSM) [3] are commonly\nused in text categorization,but shortcomings exist. Vector\nspace often has very high dimensionality and is noisy, re-\nsulting in huge computational cost and low accuracy. We\nhave to turn to features selection techniques.\nRecently, more information is integrated into the features,\nas in Semantic Vector Space Model(SVSM) [14] and Fuzzy\nClustering Method(FCM) [15]. In SVSM, all kinds of emo-\ntion units are extracted from Lyrics. Emotion unit is com-\nposed of an emotional word and the qualiï¬er and negative\nrelated to it. The count of emotion unit of each type is used\nas the feature. FCM analyses the emotion of each sentence\nbased on emotion dictionary ANCW. Then a fuzzy cluster-\ning method is implemented to choose the main emotion of a\nsong. Both of them use additional dictionaries and depend\ntoo much on the syntactic analysis.However these resources\nare not mature.\nWithout the use of additional resources, Zaanen proposes\n765Poster Session 6\na new approach to tf*idf feature [11]. He uses tf*idf to mea-\nsure the correlation between a term and an emotion. Lyrics\nare transformed to a feature vector, and each dimension of\nthe vector represents the correlation between the lyrics and\nan emotion. Beside, as far as we know, thereâ€™s no work fo-\ncusing on the rhyme of lyrics for classiï¬cation of emotion.\nIn this study, we focus on simple and low dimensional\nfeatures. The simple means that syntactic analysis and addi-\ntional dictionary which are not mature are not needed; low\ndimensionality means the features can be processed fast e-\nnough in practice. Two sets of features are proposed, one\nbased on the work of Zaanenâ€™s and the other based on the\nrhyme of lyrics. Then we go on to ï¬nd a way to combine\nthose features.\nThe methods to fuse these two sets of features can be\ndivided into two categories:features level fusion and clas-\nsiï¬ers level fusion. In the features level fusion, a new set\nof feature is generated by operations such as concatenating\nand features selection. A machine learning algorithm is then\nused to construct a classiï¬er. In the classiï¬ers level fusion,\none classiï¬er is built on each set of features. The ï¬nal result\nis obtained by fusing the output of each classiï¬er.\nClassiï¬er combination is an effective way to improve the\nperformance [10]. The methods to fuse classiï¬ers generated\nfrom different sets of features can be categorized into either\nbase-learning or meta-learning. Meta-learning studies how\nto choose the right bias dynamically, as opposed to base-\nlearning where the bias is ï¬xed priori, or user parameterized\n[12].\nCombinations with ï¬xed structures are base-learning meth-\nods. For example, sum of scores holds the assumption that\nthe label with the biggest sum of score is true label. On\nthe other hand, Combinations which are trained using avail-\nable training samples are meta-learning methods. Boosting\nand stacked generalization are examples of meta-learning\nmethods. Boosting algorithm is originally designed for im-\nproving the accuracy of classiï¬ers based on one set of fea-\ntures, which does not ï¬t our needs. Stack generalization\nuses the outputs of basic classiï¬ers as the inputs of the meta-\nclassiï¬er to predict the ï¬nal result.\n3. TAXONOMY\nWe adopt Thayerâ€™s arousal-valence emotion plane [9] as our\nemotion taxonomy. In this taxonomy, emotion is described\nby two dimensions:arousal(from calm to excited) and va-\nlence(from angry to happy). These two dimensions are most\nimportant and universal in expressing emotion [8]. As shown\nin ï¬gure 1, four emotion classes happy, angry, sad, and re-\nlaxing are deï¬ned according to the four quadrants of the e-\nmotion plane.\nFigure 1 . Thayerâ€™s A V model\n4. FEATURES\nZaanen proposed a new feature space based on tf*idf [11].The\nfeature vector is short and the method is robust. By tak-\ning the part of speech(POS) into consideration, we improve\nthe emotion expressive ability of Zaanenâ€™s model. Further\nmore, we make use of rhyme related cues of lyrics which\nare highly related to expression of emotion.\n4.1 pos tf*idf\nSome abbreviations are clariï¬ed here: POS is part of speech,\ntf is term frequency and idf is inverse document frequency.\nIn this section, I will describe Zaanenâ€™s work ï¬rst ,and then\nmethod for incorporating POS information will be shown.\nFirst, Zaanen merges the lyrics in the training set belong-\ning to emotion ejinto a single document docj. In this way,\ndocument set D has been produced, with each document in\nthe set corresponding to one emotion class. As shown in\nequation 1, for a term ti,tfj(ti)represents the importance\nof atiin the expression of emotion ej.id f(ti)represents\nthe ability of a word in distinguishing different emotion as\nshown in equation 2.\ntfj(ti) =ni,j/summationtext\nknk,j(1)\nwhere ni,jis the count of term tiindocj.\nid f(ti) =|D|\n|{docj:tiâˆˆdocj}|(2)\nThen lyrics lrclis represented by feature vector fvlas\nshown in equation 3. This feature vector is then used for\ntraining classiï¬er and making prediction.\nfvl= (f1, ..., f c)T(3)\nwhere cis the number of categories and each dimension\nof the vector is calculated by equation 4.\nfj=/summationdisplay\n{k|wkâˆˆlrcl}tfj(wk)âˆ—id f(wk) (4)\n76612th International Society for Music Information Retrieval Conference (ISMIR 2011)\nWe know that words of different POS are different in the\nability to express emotion. For example, verbs and adjec-\ntives are more emotional than articles. In the following of\nthis section, I will describe the method for incorporating\nPOS information.\nBased on Zaanenâ€™s feature model, we introduce a new\nfeature model which incorporates POS information in lyric-\ns. Instead of combining lyrics belonging to an emotion in-\nto one document, we combine them into several documents\nwith each document corresponding to one POS. For each\nPOS, we get four documents corresponding to the emotion\ntaxonomy just like Zaanenâ€™s. We get a feature vector of four\ncomponents for each POS as shown in equation 5. Then we\nconcatenate them to form the ï¬nal feature vector as shown\nin equation 6.\nfvl,POS = (f1,POS, ..., f c,POS )T(5)\nfvl= (f1,verb, ..., f c,verb, ..., f 1,noun, ..., f c,noun )T(6)\n4.2 rhyme\nA rhyme is a repetition of similar sounds in two or more\nwords and is most often used in poems and lyrics. Most\nChinese poems obey tail rhyme and lyrics of Chinese songs\nalso obey tail rhyme to some extent.\nRhyme is highly relevant to the emotion expression [1].\nBroad sounds such as [a] usually express happiness and ex-\ncitement while ï¬ne sounds such as [i] are related to gentle\nand sorrow. Broad sounds and ï¬ne sounds can be distin-\nguished by the level of obstruction in the vocal tract. Be-\nsides the difference between the broad and the ï¬ne, intona-\ntion also weighs a lot for the expression of emotion. Man-\ndarin has four tones:rises, falls, dips and stays.\nThere is a system of rhyme in old Chinese songs. It\nconsists of 19 main categories in terms of the broadness\nand ï¬neness, meanwhile, each main category is divided into\nthree sub-categories by the tones. Then there are totally 57\nrhyme categories.\nWe propose a rhyme frequency(rf) feature based on the\nrhyme system mentioned above as shown in equation 7.\nrfv(lrcj) = (rf1,j, ..., rf 57,j)T(7)\nwhere\nrfi,j=ni,j/summationtext\nknk,j(8)\nThis metric measures the importance of rhyme riin lyric-\nslrcj, with ni,jdenoting the number of occurrences of the\ntail rhyme riinlrcj, divided by number of all tail rhyme\noccurrences in lrcj.5. COMBINATION APPROACH\nWe fusion these two sets of features on the features level\nand classiï¬ers level. For the classiï¬ers level fusion, both\nbase-learning combination method and meta-learning com-\nbination method are tried.\n5.1 Feature Level Fusion\nFor lyrics lrcl, we simply concatenate POS tf*idf feature\nvector and rhyme feature vector to create a new feature vec-\ntor as shown in equation 9. Then a machine learning algo-\nrithm such as SMO is applied to train a classiï¬er and make\nprediction.\nfv/prime\nl= (f1,verb, ..., f c,verb, ..., rf 1,l, ..., rf 57,l)T(9)\n5.2 Classiï¬er Level Fusion\nWe use the POS tf*idf feature and rhyme feature as de-\nscribed above for song emotion classiï¬cation. For each of\nthe two kinds of features, a classiï¬cation learning algorithm\nis selected based on experimental results. SMO is chosen\nfor the POS tf*idf feature and Naive Bayes for the rhyme\nfeature.\nThe combination framework is shown in ï¬gure 2. For\neach instance, basic classiï¬ers output the conï¬dence for each\nclass label. Then combination classiï¬er output the ï¬nal class\nlabel based on the outputs of basic classiï¬ers. The base-\nlearning method and the meta-learning method differ in the\nimplementation of combination classiï¬er.\nAn instancePOS tf*idf\nSMO\nRhyme\nNBcfSMO, 1\n...\ncfNB, 1\n...Combination\nClassiï¬erclass\nlabel\nFigure 2 . Combination Framework\n5.2.1 Base-learning methods\nFor base-learning methods, combination classiï¬er is sim-\nple. Combination classiï¬er may choose the class label with\nthe largest conï¬dence value. Besides,a weighted average of\nconï¬dence value for each class label can be calculated by e-\nquation 10, then the class label with the largest cfiis chosen\nas the ï¬nal label. In our study, the latter method is used as\nthe baseline and the parameter setting is w1=w2= 0.5.\ncfi=w1âˆ—cfSMO,i +w2âˆ—cfNB,i (10)\nwhere\nw1+w2= 1 (11)\n767Poster Session 6\nClass +V ,+A +V ,-A -V ,-A -V ,+A\n# of lyrics 274 5 52 169\nTable 1 . Distribution of data set\n5.2.2 Meta-learning methods\nFor meta-learning methods, combination classiï¬er is obtained\nby learning from training set. We use stack generalization\nas the meta-learning algorithm. The training data of meta-\nlearning is obtained by the following procedure.\nGiven a training set T:{lrci, ci}m\ni=1for basic classiï¬-\ner, SMO learner and NB learner are applied to training set\nTSMO andTNBto hypothesis hSMO andhNB.\nTSMO :{(FPOS,i , ci)}m\ni=1 (12)\nTNB:{(FRhyme,i , ci)}m\ni=1 (13)\nFPOS,i andFRhyme,i are feature vectors of lyrics lrci.\nThe training data for combination classiï¬er is built on an-\nother training set T/prime:{lrc/prime\ni, c/prime\ni}n\ni=1to prevent over-ï¬tting.\nThe generated training set for combination classiï¬er is shown\nin equation 14.\nTcombiantion ={(hSMO (lrc/prime\ni), hNB(lrc/prime\ni), c/prime\ni)} (14)\nThe generation of training set for combination classiï¬er\nis done via k-fold cross validation. The whole training set\nis split into k folds. Each time, k-1 folds are used as train-\ning set Tfor basic learner and the remaining one is used as\ntraining set T/primeto build training data for combination classi-\nï¬er. Results of each fold are merged into the ï¬nal training\nset for combination classiï¬er.\nC4.5 is chosen as the learning algorithm for the combi-\nnation classiï¬er as it is similar with the arbitration process\nof human.\n6. EXPERIMENTS AND RESULTS\n6.1 Experiment Settings\n6.1.1 Data set\nThe data set we use is the same as that used by Hu [15]. It\nis made up of 500 Chinese pop songs,and the emotions of\nthe songs are labeled through a subjective test conducted by\n8 participants. The lyrics of the songs are downloaded from\nthe web by a web crawler.\nThe distribution of the songs over the four emotion class-\nes is shown in Table 1. Although the number of songs in\nclass â€™+V-Aâ€™ is small, it conforms to the distribution in real-\nity.Method Baseline POS tf*idf Fuzzy Clustering\nF-measure(av.) 0.3886 0.5942 0.547\nTable 2 . A comparison of word oriented methods\nregionZaanen POSrhyme# of\ntf*idf tf*idf song\n+V ,+A 0.7074 0.762 0.438 274\n+V ,-A 0 0 0 5\n-V ,-A 0 0 0.22 52\n-V ,+A 0 0.514 0.353 169\nav. 0.3886 0.594 0.382 500\nTable 3 . Results of single classiï¬er\n6.1.2 Machine learning algorithm\nSMO, Naive Bayes, and J48 classiï¬cation library in WEKA\n[7] are used to train classiï¬ers.\n6.1.3 Measurement\nWe choose f-measure as our metric.In each of the experi-\nments, f-measure is computed using 5 fold cross-validation.\nFor the tf*idf feature is computed on the training set, the\ntf*idf values are recomputed for each experiment.\n6.2 POS tf*idf\nThe result of the POS tf*idf feature is shown in table 2. We\nchoose Zaanenâ€™s method as our baseline. In contrast with\nthe baseline, our method which incorporates POS gets a per-\nformance increase of 53%. The POS tf*idf model even out-\nbalance Fuzzy Clustering method of Hu [15].\n6.3 Combination Approach\nIn this part, we will describe the results of combination meth-\nods.\nThe results of single classiï¬er are shown in table 3. Though\nthe result using rhyme as feature is much smaller than that\nof POS tf*idf, it is similar with result of Zaanenâ€™s tf*idf.\nRhyme frequency is an effective feature.\nregionFeatures Level Classiï¬ers Level # of\nconcatenation base meta song\nlearning learning\n+V ,+A 0.728 0.581 0.774 274\n+V ,-A 0 0 0 5\n-V ,-A 0.09 0.261 0.049 52\n-V ,+A 0.489 0.451 0.547 169\nav. 0.58 0.509 0.615 500\nTable 4 . Combination methods Analysis\n76812th International Society for Music Information Retrieval Conference (ISMIR 2011)\nThe combination of the two get a better result, though\nthere is a big difference between the two classiï¬ers. F-\nmeasures increases in all regions indicating the effectiveness\nof the meta-learning algorithm. Rhyme classiï¬er has poor\nperformance on the whole, but it is better at dealing with\ninstances in â€™-V ,-Aâ€™ region. And those misclassiï¬ed by the\nrhyme classiï¬er are corrected by the POS tf*idf classiï¬er.\nAs mentioned in section 5, fusion on features level and\nclassiï¬ers level are tried. By comparing POS tf*idf column\nin table 3 and concatenation column in table 4, we ï¬nd that\nfusion on features level fails to improve the result. For dif-\nferent features have different meanings, itâ€™s not appropriate\nto concatenate them simply.\nFor fusion on the classiï¬ers level, we try both base-learning\nand meta-learning for classiï¬er combination. We use weight-\ned average method for base-learning and stack generaliza-\ntion for meta-learning. From table 4, we ï¬nd that the meta-\nlearning outperforms the base-learning by 0.1, which proves\nthe effectiveness of meta-learning in the task of classiï¬er\ncombination. Besides, the base-learning even lowers the f-\nmeasure compared to single classiï¬er based on POS tf*idf.\nSimple strategies could not guarantee the effectiveness of\ncombination.\n7. CONCLUSION AND FUTURE WORK\nIn this paper, we present three main contributions. Firstly,\nwe get a great performance improvement in classiï¬cation of\nmusic emotion by extending the work of Zaanen. Secondly,\nwe propose to use rhyme cues in music emotion classiï¬ca-\ntion to complement traditional word based features. Final-\nly,a meta-learning algorithm is used to combine classiï¬ers\nbased on different features.\nThere are more to be explored with lyrics. New features\nsuch as the tone changes and the mental images can be ex-\ntracted from lyrics. Combining audio content,we can turn to\nthe ï¬eld of multi-modal music emotion classiï¬cation.\n8. ACKNOWLEDGMENT\nThe work is supported by Beijing Natural Science Founda-\ntion(Multimodal Chinese song emotion recognition).\n9. REFERENCES\n[1] Shen guo hui. The research into the rhyme,emotions and\ntheir association. Journal of City Polytechnic ZhuHai ,\n2009.\n[2] Xiao Hu and J. Stephen Downie. WHEN LYRICS OUT-\nPERFORM AUDIO FOR MUSIC MOOD CLASSIFI-\nCATION: A FEATURE ANALYSIS . In J. StephenDownie and Remco C. Veltkamp, editors, 11th Interna-\ntional Society for Music Information and Retrieval Con-\nference , August 2010.\n[3] Xiao Hu, J. Stephen Downie, and Andreas F. Ehmann.\nLYRIC TEXT MINING IN MUSIC MOOD CLASSIFI-\nCATION. In J. Stephen Downie and Remco C. Veltkam-\np, editors, 10th International Society for Music Informa-\ntion and Retrieval Conference , August 2009.\n[4] Petri Juslin, Patrik N.;s Laukka. Communication of e-\nmotions in vocal expression and music performance: D-\nifferent channels, same code? Psychological Bulletin ,\n129(5):770â€“814, Sep 2003.\n[5] Youngmoo E. Kim, Erik M. Schmidt, Raymond\nMigneco, Brandon G. Morton, Patrick Richardson, J-\neffrey Scott, Jacquelin A. Speck, and Douglas Urnbul-\nl. Music Emotion Recognition: a State of the Art Re-\nview. In J. Stephen Downie and Remco C. Veltkamp,\neditors, 11th International Society for Music Informa-\ntion and Retrieval Conference , August 2010.\n[6] Qi Lu, Xiaoou Chen, Deshun Yang, and Jun Wang.\nBOOSTING FOR MULTI-MODAL MUSIC EMO-\nTION . In J. Stephen Downie and Remco C. Veltkamp,\neditors, 11th International Society for Music Informa-\ntion and Retrieval Conference , August 2010.\n[7] Geoffrey Holmes Bernhard Pfahringer Peter Reutemann\nIan H. Witten Mark Hall, Eibe Frank. The weka data\nmining software: An update. SIGKDD Explorations , 11,\n2009.\n[8] J. A. Russell. A circumplex model of affect. Journal\nof Personality and Social Psychology , 39:1161â€“1178,\n1980.\n[9] Robert E. Thayer. The biopsychology of mood and\narousal . Oxford University Press, September 1989.\n[10] Sergey Tulyakov, Stefan Jaeger, Venu Govindaraju, and\nDavid S. Doermann. Review of classiï¬er combination\nmethods. In Machine Learning in Document Analysis\nand Recognition , pages 361â€“386. 2008.\n[11] Menno van Zaanen and Pieter Kanters. AUTOMATIC\nMOOD CLASSIFICATION USING TF*IDF BASED\nON LYRICS. In J. Stephen Downie and Remco C.\nVeltkamp, editors, 11th International Society for Music\nInformation and Retrieval Conference , August 2010.\n[12] Ricardo Vilalta and Youssef Drissi. A perspective view\nand survey of meta-learning. Artiï¬cial Intelligence Re-\nview, 18:77â€“95, 2002.\n769Poster Session 6\n[13] Jun Wang, Xiaoou Chen, Yajie Hu, and Tao Feng. Pre-\ndicting High-level Music Semantics using Social Tags\nvia Ontology-based Reasoning. In J. Stephen Downie\nand Remco C. Veltkamp, editors, 11th International So-\nciety for Music Information and Retrieval Conference ,\nAugust 2010.\n[14] Yunqing Xia, Linlin Wang, Kam-Fai Wong, and Mingx-\ning Xu. Sentiment vector space model for lyric-based\nsong sentiment classiï¬cation. In Proceedings of the 46th\nAnnual Meeting of the Association for Computational\nLinguistics on Human Language Technologies: Short\nPapers , HLT-Short â€™08, pages 133â€“136, Stroudsburg,\nPA, USA, 2008. Association for Computational Linguis-\ntics.\n[15] Xiaoou Chen Yajie Hu and Deshun Yang. LYRIC-\nBASED SONG EMOTION DETECTION WITH AF-\nFECTIVE LEXICON AND FUZZY CLUSTERING\nMETHOD. In J. Stephen Downie and Remco C.\nVeltkamp, editors, 10th International Society for Music\nInformation and Retrieval Conference , August 2009.\n770"
    },
    {
        "title": "Learning the Similarity of Audio Music in Bag-of-frames Representation from Tagged Music Data.",
        "author": [
            "Ju-Chiang Wang",
            "Hung-Shin Lee",
            "Hsin-Min Wang",
            "Shyh-Kang Jeng"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1417219",
        "url": "https://doi.org/10.5281/zenodo.1417219",
        "ee": "https://zenodo.org/records/1417219/files/WangLWJ11.pdf",
        "abstract": "Due to the cold-start problem, measuring the similarity between two pieces of audio music based on their low-level acoustic features is critical to many Music Information Retrieval (MIR) systems. In this paper, we apply the bag-offrames (BOF) approach to represent low-level acoustic features of a song and exploit music tags to help improve the performance of the audio-based music similarity computation. We first introduce a Gaussian mixture model (GMM) as the encoding reference for BOF modeling, then we propose a novel learning algorithm to minimize the similarity gap between low-level acoustic features and music tags with respect to the prior weights of the pre-trained GMM. The results of audio-based query-by-example MIR experiments on the MajorMiner and Magnatagatune datasets demonstrate the effectiveness of the proposed method, which gives a potential to guide MIR systems that employ BOF modeling.",
        "zenodo_id": 1417219,
        "dblp_key": "conf/ismir/WangLWJ11",
        "keywords": [
            "cold-start problem",
            "low-level acoustic features",
            "Music Information Retrieval (MIR)",
            "bag-of-frames (BOF)",
            "Music tags",
            "Gaussian mixture model (GMM)",
            "audio-based query-by-example MIR",
            "audio-based music similarity computation",
            "performance improvement",
            "prior weights"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\n  \n \nLEARNING THE SIMILARITY OF AUDIO MUSIC IN BAG-OF-\nFRAMES REPRESENTATION FROM TAGGED MUSIC DATA \nJu-Chiang Wang1,2, Hung-Shin Lee1,2, Hsin-Min Wang2 and Shyh-Kang Jeng1 \n1 Department of Electrical Engineering, Nati onal Taiwan University, Taipei, Taiwan \n2 Institute of Information Science, Academia Sinica, Taipei, Taiwan \n{asriver, hslee, whm}@iis.sinica.edu.tw, skjeng@cc.ee.ntu.edu.tw  \nABSTRACT \nDue to the cold-start problem, measuring the similarity be-\ntween two pieces of audio music based on their low-level acoustic features is critical to many Music Information Re-trieval (MIR) systems. In this paper, we apply the bag-of-frames (BOF) approach to represent low-level acoustic fea-tures of a song and exploit music tags to help improve the performance of the audio-based music similarity computa-tion. We first introduce a Gaussian mixture model (GMM) as the encoding reference for BOF modeling, then we pro-pose a novel learning algorithm to minimize the similarity gap between low-level acoustic features and music tags with respect to the prior weights of the pre-trained GMM. The results of audio-based query-by-example MIR experi-ments on the MajorMiner and Magnatagatune datasets demonstrate the effectiveness of the proposed method, which gives a potential to guide MIR systems that employ \nBOF modeling.\n   \n1. INTRODUCTION \nMeasuring the similarity between two pieces of music is a \nfundamental but difficult task in Music Information Re-trieval (MIR) research [1] since music similarity is inher-ently based on human subjective point of view and can be bias among people who have different musical tastes and prior knowledge. A piece of musi c contains a variety of \nmusical contents, including the low-level audio signal; the metadata, such as the artist, album, song name, and release year; and a number of high-level perceptive descriptions, such as timbre, instrumentation, style/genre, mood, and so-cial information (e.g., tags, blogs, and explicit or implicit user feedback). Among the musical contents, only the au-dio signal is always available while the metadata and high-level perceptive descriptions are often unavailable or ex-\n                                                                          \nThis work was supported in part by the Taiwan e-Learning and Digital Ar-\nchives Program (TELDAP) sponsored by the National Science Council of Taiwan under Grant: NSC 100-2631-H-001-013. \n \n    pensive to obtain. Owing to the cold-start problem, measur-\ning the similarity between tw o pieces of audio music based \non their low-level acoustic features is critical to many MIR systems [2, 3]. These systems are usually evaluated against \nthe objective criteria derived from the metadata and high-level perceptive descriptions, which in fact correspond to the subjective criteria that humans use to measure music \nsimilarity. The similarity gap between the acoustic features and human subjective perceptions inevitably degrades the performances of the MIR systems. The gap may come from an insufficient song-level acoustic feature extraction or rep-resentation and an ill similarity metric. Therefore, the goal of improving audio-based music similarity computation is to reduce the gap between audio features and human per-ceptions, and it can be achieved from a music feature repre-sentation perspective [3-8] or a similarity learning perspec-tive [1, 10]. \nDue to the â€œglass ceilingâ€ of  performance that the pure \naudio-based music similarity computation systems have faced, several high-level perceptive descriptions, which are considered having a smaller gap between the similarity computed on them and the subjective similarity of human, have been employed in some previous work. For example, in [6, 7], an intermediate sema ntic space (e.g. genre or text \ncaption) is used to bridge and reduce the similarity gap. During recent years, social information has been very \npopular and become a major source of contextual knowl-edge for MIR systems. The social information generated by Internet users makes the â€œwisdom of crowdsâ€ available for investigating the general criteria of human subjective music similarity. In [1], the music blogs are exploited to learn the music similarity metric of audi o features. In [8], the social \ntags are concatenated with th e audio features to represent \nmusic in a query-by-example MIR scenario. Furthermore, \nKim et al. [9] conduct explicit and implicit user feedback, which can be implemented by collaborative filtering (CF, the user-artist matrix), to measure artist similarity. Surpris-ingly, the experimental results show that CF can be a very efficient source in music similarity computation. After-wards, the CF data is used in [10] to learn the audio-based similarity metric and significant improvements in query-by-example MIR performance are ach ieved with three types of \nsong-level representations, namely, acoustic, auto-tag, and human-tag representations. \nPermission to make digital or hard copi es of all or part of this work fo r \npersonal or classroom use is granted without fee provided that copies are \nnot made or distributed for profit or  commercial advantage and that cop-\nies bear this notice and the full citation on the first page.  Â© 2011 International Societ\ny for Music Information Retrieval \n85Poster Session 1\n  \n \nIn the abovementioned work, music tags are mostly \ntreated as part of music features to represent a song [8-10]. In this paper, we adopt music tags to create a ground truth semantic space to be used to measure human subjective \nsimilarity for three reasons. First, music tags are human la-bels that represent human mu sical perceptions. According \nto previous studies [9, 10], the similarity from tags is highly relative to the subjective similarity for evaluation, i.e., the similarity gap is relatively small. Second, music tags are free-text labels that include all kinds of musical information, such as genre, mood, instrumentation, personal preference, and metadata, which are used to  objectively evaluate the ef-\nfectiveness of audio-based similarity computation in previ-ous work. Third, music tags are generally considered noisy, redundant, bias, and unstable when collected from a com-pletely non-constrained tagging environment, such as last.fm. Consequently, several web-based music tagging games have been created with a purpose of collecting reli-able and useful tags, e.g., MajorMiner.org [12] and Tag A Tune [13]. In these tagging games, music clips are ran-domly assigned to taggers in order to reduce the tagging bias. Carefully extracting tags with high term frequencies and merging equivalent tags can intuitively reduce the noisy factors. With a set of well-refined music tags, the se-mantic space which simulates the human music similarity can be established. \nIn most audio-based MIR systems, the sequence of short-\ntime frame-based or segment-based acoustic feature vectors of a song is converted into a fixed-dimensional vector so that the song-level semantic descriptions (or tags) can be incorporated into it. The bag-of-frames (BOF) or bag-of-segments approach is a popular and efficient way to repre-sent a set of frame-based acoustic vectors of a song and has been widely used in MIR applications [8,10,14]. In the tra-ditional BOF approach, a set of frame representatives (e.g., codebook, denoted as an encoding reference hereafter) are selected or learned in an unsupervised manner, then a song is represented by the histogram over the encoding reference. \nIn the BOF representation vector, each dimension repre-\nsents the effective quantity of its corresponding frame rep-resentative (e.g., codeword) within a song. Based on the ef-fective quantities, we can estimate the audio-based similar-ity of two songs. Motivated by the metric learning for au-dio-based music similarity computation in [1,  10], we could \nlearn a metric transformation over the BOF representation vector by minimizing the similarity gap between acoustic features and music tags. Since the BOF vector is generated by the encoding reference, the minimization of similarity gap can be achieved by learning the encoding reference ra-ther than learning a metric transformation on the native BOF space. This leads to a supervised method for learning the encoding reference from a tagged music dataset to im-prove the BOF representation. Hopefully, the learned en-coding reference could better generalize the BOF modeling \nthan a stacking transformation over the native metric. The remainder of this paper is organized as follows. Sec-\ntion 2 describes the audio feature extraction module and song-level BOF representation. In Section 3, we introduce the method for learning the encoding reference from the tagged music data. In Section 4, we evaluate the proposed method on the MajorMiner and Magnatagatune datasets in a query-by-example MIR scen ario. Finally, we summarize \nour conclusions in Section 5. \n2. BAG-OF-FRAMES REPRESENTATION FOR \nACOUSTIC FEATURES \n2.1 Frame-based Acoustic Feature Extraction \nWe use MIRToolbox 1.3 for acoustic feature extraction \n[14]. As shown in Table 1, we consider four types of fea-tures, namely, dynamic, spectral, timbre, and tonal features. To ensure alignment and preven t mismatch of different fea-\ntures in a vector, all the featur es are extracted with the same \nfixed-sized short-time frame. Given a song, a sequence of 70-dimensional feature vectors is extracted with a 50ms frame size and 0.5 hop shift. Then, we normalize the 70-dimensional frame-based featur e vectors in each dimension \nto mean 0 and standard deviation 1. \nTypes Feature Description Dim\ndynamic rms 1 \nspectral  centroid, spread, skewness, kurtosis, en-\ntropy, flatness, rolloff 85, rolloff 95, bright-ness, roughness, irregularity 11\ntimbre zero crossing rate, spectral flux, MFCC, \ndelta MFCC, delta-delta MFCC 41\ntonal key clarity, key mode possibility, HCDF, \nchroma , chroma peak, chroma centroid 17\nTable 1.  The music features used in the 70-dimensional frame-\nbased music feature vector. \n2.2 The Encoding Reference and BOF Representation \nThe BOF approach is argued that each frame of a song \nshould not be treated equally, and an isolated frame of low-level acoustic feature is not representative for high-level perceptive descriptions [15]. Besides, the effectiveness of BOF modeling is highly impacted by the size of encoding reference and will encounter a glass ceiling when the size is too large [16]. Our goal of improving the encoding refer-ence for BOF modeling is twofold: First, we aim at choos-ing a type of frame representative that gives better gener-alization ability and a more reliable distance measure crite-rion. Second, each frame repr esentative should not have \nequal information load during song-level encoding.  \nThe BOF modeling starts with generating the encoding \nreference from a set of available frames (denoted as F). \nThe frames are usually selected randomly and uniformly from each song in a music dataset. We use a Gaussian mix-ture model (GMM) instead of a codebook derived by the K-\nmean algorithm as the encoding reference [17]. In the \n8612th International Society for Music Information Retrieval Conference (ISMIR 2011)\n  \n \nGMM, each component Gaussian  distribution, denoted as zk, \nk=1,â€¦, K, corresponds to a frame representative. The GMM \nis trained on F by the expectation-maximization (EM) algo-\nrithm, and is expressed as follows: \n,),|( )(\n1âˆ‘\n==K\nkk k k kN p Î£Î¼v v Ï€           (1) \nwhere v is a frame-based feature vector, Nk(Â·) is the k-th \ncomponent Gaussian distribution with mean vector Î¼k and \ncovariance matrix Î£k, and Ï€k is the prior weight of the k-th \nmixture component. Given v, the posterior probability of a \nmixture component is computed by: \n.\n),|(),|() (\n1âˆ‘==K\nm m m m mk k k k\nkNNzp\nÎ£Î¼vÎ£Î¼vv\nÏ€Ï€   (2) \nGiven a song s with L frames, its BOF posterior-probability \nrepresentation (denoted as vector x) is computed by: \n,) (1) (\n1âˆ‘\n== â†L\ntt k k k zpLszp x v                 (3) \nwhere xk is the k-th element in vector x. When encoding a \nframe by GMM, the posterior probability is based on the likelihood of each component Ga ussian distribution. The \nposterior probability of each mixture component yields a \nsoft-assigned encoding criterion which enhances the mod-eling ability of the GMM-based encoding reference over the vector-quantization-based (VQ-based) one. \nOur contention is that the diversity of frame representa-\ntives in the encoding referen ce is proportional to the ability \nof the BOF modeling, i.e., the BOF modeling can involve more audio information of a s ong to be encoded. However, \nlike other encoding references (e.g., a set of randomly se-lected vectors or a trained codebook), the GMM is gener-ated in an unsupervised manne r. The factors that we can \ncontrol includes the number of components in GMM, i.e., K, the types of acoustic featur es used in the frame-based \nvector, and the construction of F. Except for K, the other \ntwo factors are fixed in the beginning. As K increases, the \nframe representatives become more diverse, but some of them are in fact redundant. This motivates us to determine the importance of each frame re presentative in a discrimi-\nnative way. The EM training for GMM provides the esti-\nmation of the data distribution over F, which is assumed to \nfollow a mixture of Gaussian distributions, by the maxi-\nmum likelihood criterion. The prior \nÏ€k of the k-th compo-\nnent Gaussian represents the corresponding effective num-\nber of frames in training set F. However, the construction \nof F implies that the estimated distribution of F actually \ndoes not have information about the song-level distribution of acoustic feature vectors. In other words, it may not re-flect the importance of each mixture component when en-\ncoding a song. In fact, as will be discussed later in Sec. 4, our experimental results show that setting the trained priors to a uniform distribution improves the MIR performance. In light of the observations described above and the ben-\neficial characteristics of music tags, we readily incorporate \nthe tagged music data as a supervision guide to determine the importance of each mixture component in the GMM.   \n3. LEARNING THE AUDIO-BASED SIMILARITY \nIn this work, learning the similarity of audio music from \ntagged music data is achieved by learning the encoding ref-erence to minimize the similarity gap between low-level acoustic features and high-level music tags. To this end, we conduct learning with respect to the parameters of the \nGMM trained from F. In this paper, we only consider the \nrelearning of the prior probabilities, i.e., the pre-learned pa-\nrameters Î¼\nk and Î£k, k=1,â€¦, K, are fixed. The proposed itera-\ntive learning algorithm has two steps, namely, encoding \nsongs into BOF vectors and minimizing the similarity gap with respect to the prior probabilities of the GMM.  \n3.1 Preliminary \nSuppose there is a tagged music corpus D with N songs. \nGiven a song s\ni in D, we have its BOF vector xiâˆˆRKÃ—1, \nwhich is encoded by the GMM to represent the acoustic \nfeatures, and its tag vector yiâˆˆ{0,1}MÃ—1, in which each tag \nis binary  labeled (multi-label case) from a pre-defined tag \nset with M tags. Two similarity matrices are defined: SX is \ncomputed on the N BOF vectors, and SY is computed on the \nN tag vectors. We estimate the acoustic similarity between \nsi and sj in D by computing the inner product of xi and xj. \nTherefore, the acoustic similarity matrix SX of D can be ex-\npressed as: \n,XXT\nXS=              (4) \nwhere X is a K-by-N matrix with xi as its i-th column. The \ntag similarity matrix SY of D is expressed as:   \n,YYT\nYS=             (5) \nwhere Y is an M-by-N matrix with yiâˆ•||yi|| as its i-th col-\numn. Since each song may have different numbers of tags, \nto ensure that the tag-based similarity of a song itself is al-ways the largest, we compute the cosine similarity between \ny\ni and yj in Eq. (5) to estimate the tag-based similarity to \nsimulate the human similarity between si and sj. \nThe methods for audio-based similarity computation can \nbe evaluated by a query-by- example MIR system, i.e., \ngiven a query song with the audio signal only, the system ranks all the songs in the da tabase based on audio-based \nsimilarity computation only. To evaluate the effectiveness of S\nX, we perform leave-one-out  MIR tests to evaluate the \nnormalized discounted cumulative gain  (NDCG) [18] with \nrespect to the ground truth relevance derived by SY. That is, \neach song si in D is taken as a query song in turn, the out-\nput ranked list for si is generated by sorting the elements in \nthe i-th row of SX in descending order, and the correspond-\ning ground truth relevance is the i-th row of SY. The \n87Poster Session 1\n  \n \nNDCG i@P, which represents the quality of ranking of the \ntop P retrieved songs for query si, is formulated as follows: \n,log)()1(1@ NDCG\n2 2â­â¬â«\nâ©â¨â§\n+ =âˆ‘\n=P\nji\ni\nPijjRRQP       (6) \nwhere Ri(j) is the ground truth relevance (obtained from the \ni-th row of SY) of the j-th song on the ranked list, and QP is \nthe normalization term representing the ideal ranking of the P songs [18]. Intuitively, if more songs with large ground \ntruth relevance are ranked higher, a larger NDCG will be \nobtained. The query-by-example MIR performance on D \nbased on S\nX with respect to SY is evaluated by  \n.@1)@ NDCG(\n1âˆ‘\n==N\niiP NDCGNPD   (7) \nThe larger NDCG in Eq. (7) is, the more effective the audio \nsimilarity computation for SX is. \n3.2 Minimizing the Similarity Gap \nWe define a K-by-K symmetric transformation matrix W \nfor the BOF vector space. The transformed BOF vector for \nsi is expressed by Wx i, and the new acoustic similarity ma-\ntrix ST of D can be obtained by: \n, ) () ( TXX WX WXT T\nTS = =           (8) \nwhere T=WTW. Therefore, minimizing the similarity gap \nbetween the transformed BOF vector space and human tag vector space is equivalent to  minimizing the distance or \nmaximizing the correlation [19] between the two kernel \nmatrices S\nT and SY with respect to W. In this paper, moti-\nvated by the work in [20], we express the N songs in D as \ntwo random vectors, ZxâˆˆRNÃ—1 for the transformed acoustic \nfeature and ZyâˆˆRNÃ—1 for the tag label, which follow two \nmultivariate Gaussian distributions Nx and Ny, respectively. \nThere exists a simple bijection between the two multivari-ate Gaussians. Without loss of generality, we assume N\nx \nand Ny have an equal mean and are parameterized by ( Î¼, ST) \nand ( Î¼, SY), respectively. Then, the â€œclosenessâ€ between Nx \nand Ny can be measured by the relative entropy KL( Nx || Ny) \n(i.e., the KL-divergence), which is equivalent to d(ST || SY):   \n{} ,  | |log) (  21)||(1 1N SS SStr S SdYT YT Y T âˆ’ âˆ’ =âˆ’ âˆ’     (9) \nwhere tr(Â·) and |Â·| are the trace and determinant of a matrix, \nrespectively. The minimization of d(ST || SY) can be solved \nby setting the derivative of d(ST || SY) with respect to T to \nzero. The solution that minimizes d(ST || SY) is as follows: \n () . )(11 *âˆ’âˆ’=T\nYS X X T    (10) \nSince W is symmetric, the optimal matrix W is derived by \n.)(21* *T W=            (11) \nTo prevent singularity, a small value 0.001 is added to each \ndiagonal element of the matrices  that are inversed in solv-ing W. If we restrict W in Eq. (8) to be diagonal, i.e., we \nignore the correlation among diffe rent dimensions in the \nBOF vector, and define vector w â‰¡ diag( W), the optimal w* \nis the diagonal of W*: \n),(diag* *W w=              (12) \nwhere each element in w* must be greater than zero. The \nderivations of Eqs. (10) and (12) are skipped due to the \nspace limitation. \nIn the testing phase, each song is first encoded into a \nBOF vector by the GMM using Eq. (3). Then, the audio-based similarity between any two songs s\ni and sj is com-\nputed as xiTT*xj, where T* can be a full or diagonal matrix \naccording to the initial setting of W in Eq. (8). In the ex-\nperiments, this method with full transformation and diago-\nnal transformation is denoted as FullTrans and DiagTrans respectively, while the method without transformation is denoted as OrigGMM (i.e., the native GMM). \n3.3\n Relearning the Priors of the GMM \nInstead of learning a transfor mation matrix, we can also \nminimize the similarity gap by relearning the prior weights of the GMM. We propose a two-step iterative learning method, which iteratively updates the prior weights of the GMM until convergence. The NDCG in Eq. (7) can be used as the criterion for ch ecking the convergence of the \nlearning procedure. The minimization of similarity gap im-plies the improvement in NDCG since the learned S\nT tries \nto preserve the structure of SY, which is used as the ground \ntruth relevance in computing NDCG. If NDCG is no longer improved, the learning algorithm stops. The learning method is summarized in Algorithm 1. \nAccording to Algorithm 1, ther e are two steps in an itera-\ntion. Line 05 corresponds to th e first step, which encodes \nall songs into their BOF vectors; and lines 11 and 13 corre-spond to the second step, which minimizes the similarity gap with respect to the prio r weights of the GMM. Since \nencoding all songs in \nD is a complicated procedure, di-\nrectly optimizing NDCG with respect to the parameters of the GMM with Eqs. (1), (2) and (3) is infeasible. Therefore, \nwe turn to find an indirect so lution that minimizes the simi-\nlarity gap with respect to th e priors of the GMM. We ex-\nploit the property of w\n* to derive Eq. (13), which serves as \nan indirect optimizer for maximizing the NDCG( D)@N by \nreweighting the prior weights of the GMM. In tuitively, the \nvector w* derived in line 11 plays a role to select mixture \ncomponents in the GMM.  \nIn the testing phase, each song is encoded into a BOF \nvector by the GMM with the relearned prior weights using Eq. (3). Then, the audio-base d similarity between any two \nsongs s\ni and sj is computed as the inner product of xi and xj , \nwithout the need to apply an y stacking transformation in \nthe BOF space. In the experiments, the proposed method implemented in this way is denoted as DiagGMM. \n8812th International Society for Music Information Retrieval Conference (ISMIR 2011)\n  \n \n4. EVALUATIONS \n4.1 Datasets \nWe evaluate the proposed method on the MajorMiner and \nMagnatagatune datasets in a query-by-example MIR sce-nario. Both datasets are generated from social tagging \ngames with a purpose (GWAP) [11, 12] to collect reliable and useful tag labels. The MajorMiner dataset has been a well-known benchmark in MIREX since 2008. The one used in this paper is crawled from the MajorMiner website in March 2011. It contains 2,472 10-second music clips and \n1,031 raw tags. After exacting the high frequency tags and \nmerging the redundant tags, 76 tags are left. The Magnata-\ngatune dataset [12], which contains 25,860 30-second au-\ndio clips and 188 pre-processe d tags, is downloaded from \n[21]. To construct \nF, we randomly select 25% and 2% of \nframes from the two datasets, respectively. For MajorMiner, \nF contains 235,000 frames, while for Magnatagatune, F \ncontains 535,800 frames. The F constructed in this way is \nblind to song-level information.  To prevent bias in the tag-\nbased similarity computation of SY, we ignore the clips la-\nbeled with fewer tags. For the MajorMiner dataset, 1,200 clips having at least 5 tags ar e left. For the Magnatagatune \ndataset, 3,764 clips having at  least 7 tags are left. \n4.2\n Experimental Results and Discussions \nIn the experiments, we repeat  three-fold cross-validation 10 \ntimes on the MajorMiner dataset, which is divided into three folds at random. In each run, two folds are used for \ntraining the transformation matr ix of the FullTrans and Di-\nagTrans methods or relearning the prior weights of the GMM for the DiagGMM method, while the remaining fold, which serves as both the test queries and the target data-\nbase to retrieve , is used for the leave-one-out  audio-based \nMIR outside test. For the Magnatagatune dataset, all clips have been divided into 16 folds to prevent that two or more \nclips originated from the same song occur in different folds. \nWe merge the 16 folds into 4 folds and perform four-fold cross-validation. The NDCG@ P in Eq. (7) is used as the \nevaluation metric in both in side and outside tests.  \nFirst, we examine the lear ning process of DiagGMM on \nthe MajorMiner dataset. Figure 1 shows an example learn-ing curve in terms of NDCG for one of the three-fold cross-validation runs. The curve is equivalent to the inside test  \nperformance evaluated on the training data. We can see that the learning curve of DiagGMM ( K=256) increases mono-\ntonically till convergence, although DiagGMM can only improve the NDCG of the training data indirectly as dis-cussed in Section 3.3. Diag GMM gains an absolute in-\ncrease of 0.04 in NDCG@10 and 0.002 in NDCG@800. The NDCG of FullTrans can be considered an upper bound for DiagGMM since it adopts a di rect optimization strategy.  \nNext, we evaluate OrigG MM and the VQ-based method \non the MajorMiner dataset. Ther e is no need to divide the \ndata into three folds since no supervised learning is in-\nvolved in the methods.  From the MIR results shown in Ta-ble 2, we observe that replacing the priors of the GMM \ntrained from \nF with a uniform distribution enhances the \nperformance. We also observe  that, even with a large K, \nOrigGMM outperforms VQ-based BOF modeling. The re-sults demonstrate the better modeling ability of the GMM over the K-means derived codebook. \nFinally, we compare DiagGMM with three baselines, i.e., \nFullTrans, DiagTrans, and Orig GMM. The results of three-\nfold cross-validation on the Ma jorMiner dataset are shown \nin Figure 2, while the results of four-fold cross-validation \non the Magnatagatune dataset are shown in Figure 3. From Figures 2 and 3, it is obvious that the proposed DiagGMM outperforms all other methods in most cases. The conven-\ntional BOF approach does face a glass ceiling when K is Algorithm 1.  The learning algorithm \nInput:  Initial GMM parameters { Î¼k, Î£k }, k=1,â€¦, K;  \nA tagged music corpus D: a set of frames Vi for si, \ni=1,â€¦, N, and tag similarity matrix  SY  from Eq. (5);\nOutput:  Learned GMM prior {kÏ€Ë†}; \n01: Initialize )0(\nkÏ€ to be 1/ K; \n02: Iteration index  t â† 0;  \n03: L(t) â† 0; \n04: while   t â‰¥ 0 do  \n05:    Encode Vi into xi with Eq. (3) using {k kt\nkÎ£Î¼,,)(Ï€ };\n06:    Compute SX with Eq. (4); \n07:    t â† t + 1; \n08:    L(t) â† NDCG( D)@N with Eq. (7) using SX and SY; \n09:    If  (L(t)-L(t-1))/L(t) < 0 then  \n10:         Return )1(Ë†âˆ’â†t\nk kÏ€ Ï€ and break ; \n11:    Compute w* with Eq. (12) using  SX and SY ; \n12:    for k=1,â€¦, K, do \n13:         ;\n1)1()1(\n)(\nâˆ‘=âˆ’âˆ’\nâ†K\nqt\nqqt\nkk t\nkww\nÏ€Ï€Ï€                                     (13)\n(where wk is the k-th element in w*) \n14:    end for  \n15: end while  0 5 10 15 200.8750.880.8850.890.8950.90.9050.910.9150.920.925\nIterationNDCG@10The Convergence of DiagGMM (K=256)\n  \nDiagGMM\nFullTrans\n0 5 10 150.88850.8890.88950.890.89050.8907\nIterationNDCG@800The Convergence of DiagGMM (K=256)\n  \nDiagGMM\nFullTrans\n(a)            (b) \nFigure 1. The learning curve in terms of (a) NDCG@10 and (b) \nNDCG@800 evaluated on the MajorMiner training data.  \n89Poster Session 1\n  \n \ntoo large, as evidenced by th e observation that the perform-\nance of OrigGMM saturates at around K=1,024 for Ma-\njorMiner (10-second clips) and K=2,048 for Magna-\ntagatune (30-second clips). The proposed DiagGMM en-hances the performance over the glass ceiling of OrigGMM with a smaller \nK, e.g., DiagGMM with K=512 outperforms \nOrigGMM with K=2,048 on the MajorMiner dataset. Full-\nTrans outperforms DiagTrans and DiagGMM only when K \nis small. However, FullTrans tends to saturate early since it has more parameters to train and thus requires more train-\ning data, compared with Diag Trans and DiagGMM. In Fig-\nure 1, the performance of FullTrans shows an upper bound of DiagGMM in inside test; however, in outside test, DiagGMM outperforms FullTrans except when \nK is small. \nThe experimental results in Fi gures 2 and 3 demonstrate the \nexcellent generalization ability of DiagGMM, which learns the similarity of audio music by relearning the priors of the GMM instead of a transformati on in the BOF vector space. \n5. CONCLUSIONS  \nIn this paper, we have addr essed a novel research direction \nthat the audio-based music similarity computation can be learned by minimizing the similarity gap or maximizing the NDCG measure with respect to the parameters of the en-coding reference in BOF repr esentation. We have imple-\nmented the idea by learning the prior weights of the GMM from tagged music data. The e xperimental results demon-\nstrate the effectiveness of the proposed method, which gives a potential to guide MIR systems that employ BOF representation, e.g., the Di agGMM can be directly com-\nbined with the codeword Be rnoulli average (CBA) method \n[13], a well-known automatic music tagging method.  \n6. REFERENCES \n[1] M. Slaney, K. Weinberger, and W. White: â€œLearning a met-\nric for music similarity,â€ ISMIR , 2008. \n[2] J.-J. Aucouturier and F. Pachet: â€œMusic similarity measures: Whatâ€™s the use?,â€\n ISMIR , 2002. \n[3] M. Mandel and D. Ellis: â€œSong-level features and SVMs for music classification,â€\n ISMIR , 2005. \n[4] E. Pampalk: â€œAudio- based music similarity and retrieval: \nCombining a spectral similarity model with information ex-tracted from fluctuation patterns,â€ \nISMIR , 2006. \n[5] M. Hoffman, D. Blei and P. Cook: â€œContent-based musical \nsimilarity computation using the hierarchical Dirichlet proc-ess,â€ \nISMIR , 2008. \n[6] K. West and P. Lamere: â€œA model-based approach to con-\nstructing music similarity functions,â€ EURASIP Journal on \nAdvances in Signal Processing , 2007(1), 1â€“10, 2007. \n[7] L. Barrington, A. Chan, D. Tu rnbull, and G. Lanckriet: â€œAu-\ndio information retrieval using semantic similarity,â€ ICASSP , \n2007. \n[8] M. Levy and M. Sandler: â€œMusic  information retrieval using \nsocial tags and audio,â€ IEEE TMM , 11(3), 383-395, 2009. \n[9] J. H. Kim, B. Tomasik, and D. Turnbull: â€œUsing artist simi-larity to propagate semantic information,â€ \nISMIR , 2009. \n[10] B. McFee, L. Barri ngton and G. Lanckriet: â€œLearning simi-larity from collaborative filters,â€ ISMIR , 2010. \n[11] M. Mandel and D. Ellis: â€œA web-based game for collecting music metadata,â€ \nJ. New Mus. Res. , 37(2), 151â€“165, 2008. \n[12] E. Law and L. von Ahn: â€œI nput-agreement: A new mecha-\nnism for data collection usi ng human computation games,â€ \nACM CHI , 2009. \n[13] M. Hoffman, D. Blei and P. Cook: â€œEasy as CBA: A simple \nprobabilistic model for tagging music,â€ ISMIR , 2009. \n[14] O. Lartillot and P. Toiviainen: â€œA Matlab toolbox for musi-cal feature extraction from audio,â€ \nDAFx , 2007. \n[15] G. Marques, et al.:  â€œAdditional evidence that common low-level features of individual a udio frames are not representa-\ntive of music genre,â€ \nSMC Conference , 2010. \n[16] J.-J. Aucouturier, B. Defreville and F. Pachet: â€œThe bag-of-frames approach to audio patte rn recognition: A sufficient \nmodel for urban soundscapes but not for polyphonic music,â€ \nJ. Acoust. Soc. Am. , 122(2), 881â€“91, 2007. \n[17] K. Yoshii, M. Goto, K. Komatani, T. Ogata, and H. G. Okuno: â€œAn efficient hybrid music recommender system us-ing an incrementally trainable probabilistic generative mod-el,â€ \nIEEE TASLP , 16(2), 435â€“447, 2008. \n[18] K. Jarvelin and J. Kekalainen: â€œCumulated gain-based eval-uation of IR techniques,â€\n ACM Trans. on Information Sys-\ntems, 20(4), 422â€“446, 2002. \n[19] Y. Zhang and Z.-H. Zhou: â€œMulti-label dimensionality re-duction via dependence maximization,â€ \nAAAI , 2008. \n[20] J. V. Davis, B. Kulis, P. Jain, S. S. and I. S.  Dhillon: â€œInfor-\nmation-theoretic metric learning,â€ ICML , 2007. \n[21] http://tagatune.org/Magnatagatune.html NDCG @5 @10 @20 @30 \nOrigGMM ( K=2,048) w/o Prior 0.9382 0.9015 0.8753 0.8674\nOrigGMM ( K=2,048) w Prior 0.9322 0.8992 0.8743 0.8669\nVQ-based ( K=2,048) Histogram 0.9297 0.8930 0.8721 0.8650\nTable 2. The results of OrigGMM and the VQ-based method  on \nthe complete MajorMiner dataset (1,200 clips).  \n32 64 128 256 512 1024 20480.840.860.880.90.920.940.96\nNumber of Component KNDCGMajorMiner P=5\n  \nDiagGMM\nFullTrans\nOrigGMM\nDiagTrans\n32 64 128 256 512 1024 20480.760.780.80.820.840.860.880.90.92\nNumber of Component KNDCGMajorMiner P=10\n  \nDiagGMM\nFullTrans\nOrigGMM\nDiagTrans\nFigure 2.  The results in terms of NDCG@5 and NDCG@10 on \nthe MajorMiner dataset with different K. \n128 256 512 1024 2048 25600.860.880.90.920.940.95\nNumber of Component KNDCGMagnatagatune P=5\n  \nDiagGMM\nFullTrans\nOrigGMM\nDiagTrans\n128 256 512 1024 2048 25600.790.810.830.850.870.890.910.92\nNumber of Component KNDCGMagnatagatune P=10\n  \nDiagGMM\nFullTrans\nOrigGMM\nDiagTrans\nFigure 3.  The results in terms of NDCG@5 and NDCG@10 on \nthe Magnatagatune dataset with different K. \n90"
    },
    {
        "title": "Potential Relationship Discovery in Tag-Aware Music Style Clustering and Artist Social Networks.",
        "author": [
            "Dingding Wang 0001",
            "Mitsunori Ogihara"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1418221",
        "url": "https://doi.org/10.5281/zenodo.1418221",
        "ee": "https://zenodo.org/records/1418221/files/WangO11.pdf",
        "abstract": "With the rapid growth of music information and data in todayâ€™s ever changing world, exploring and analyzing music style has become more and more difficult. Traditional content-based methods for music style analysis and newly emerged tag-based methods usually assume music items are independent of each other. However, in real world applications, do there exist some relationships among them. In this paper, we construct the social relation graph among different music artists by extracting the friendship information from social media such as Twitter, and incorporate the generated social networking graph into tag-based music style clustering. Experiments on real data show the effectiveness of this novel integration of different information sources.",
        "zenodo_id": 1418221,
        "dblp_key": "conf/ismir/WangO11",
        "keywords": [
            "music",
            "information",
            "analysis",
            "social",
            "media",
            "friendship",
            "graph",
            "tag-based",
            "music",
            "style"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nPOTENTIAL RELATIONSHIP DISCOVERY IN TAG-AWARE MUSIC\nSTYLE CLUSTERING AND ARTIST SOCIAL NETWORKS\nDingding Wang\nCenter for Computational Science\nUniversity of Miami\nCoral Gables, FL USA\nd.wang1@miami.eduMitsunori Ogihara\nDepartment of Computer Science\nUniversity of Miami\nCoral Gables, FL USA\nogihara@cs.miami.edu\nABSTRACT\nWith the rapid growth of music information and data in\ntodayâ€™s ever changing world, exploring and analyzing mu-\nsic style has become more and more difï¬cult. Traditional\ncontent-based methods for music style analysis and newly\nemerged tag-based methods usually assume music items\nare independent of each other. However, in real world ap-\nplications, do there exist some relationships among them.\nIn this paper, we construct the social relation graph among\ndifferent music artists by extracting the friendship infor-\nmation from social media such as Twitter, and incorporate\nthe generated social networking graph into tag-based mu-\nsic style clustering. Experiments on real data show the\neffectiveness of this novel integration of different infor-\nmation sources.\n1. INTRODUCTION\nAs the rapid growth of music items on the Internet, music\nstyle analysis such as music classiï¬cation and clustering\nhas become increasingly prevalent in music information\nretrieval research. Traditional methods usually focus on\naudio feature extraction and acoustic content analysis. For\nexample, Pampalk et al. [19] integrate different similari-\nty sources based on ï¬‚uctuation patterns and use a nearest\nneighbor classiï¬er to categorize music items. Chen and\nChen [3] apply both long-term and short-term features and\nuses support vector machines to classify music genres.\nMore recently, methods utilizing music social tags have\nemerged and have been receiving more and more atten-\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies\nare not made or distributed for proï¬t or commercial advantage and that\ncopies bear this notice and the full citation on the ï¬rst page.\nc/circlecopyrt2011 International Society for Music Information Retrieval.tion. Social tags are free-text descriptions added by user-\ns to express their personal views and interests in music\nitems such as songs, artists, albums, and playlists. The\ntags provide direct insights into user behavior and opin-\nions and the retrieval methods using tags have been shown\nto be more effective than the traditional methods solely\nbased on music content analysis [14,24,27]. For example,\nBischoff et al. [2] demonstrate that different types of so-\ncial tags can improve music search. Symeonidis et al. [23]\npropose a music recommendation algorithm using a user-\ntag-item tensor. Wang et al. [26] show the effectiveness of\ntag features by way of joint analysis of tags and contents.\nAlthough the content-based and tag-aware methods are\nsuccessful in many music information retrieval applica-\ntions, they make a somewhat curious assumption that mu-\nsic items are independent of each other, which is not al-\nways true. In this paper, we assume that music items are\nrelated to each other and try to establish relations among\nthem by discovering relationships among artists. To do\nthis we look for the â€œfollowingâ€ information on Twitter\nand construct a linked graph to represent the artist so-\ncial network. We then propose a novel tag-aware music\nstyle clustering system utilizing this network by way of\nmatrix factorization. By assuming that the â€œfollowerâ€ re-\nlationship as represented in the social network thus build\nis transitive, we can capture indirect relationships among\nthe artists, which are usually ignored in the existing music\nstyle clustering methods.\nThe rest of this paper is organized as follows. Section 2\ndiscusses the related work. Section 3 introduces our pro-\nposed approaches for constructing the artist social graph,\ngenerating artist relation matrix, and clustering using rela-\ntion matrix based factorization. We conduct experiments\non a real world data set and Section 4 presents the experi-\nmental results. Section 5 gives an conclusion and discuss-\nes the future work.\n435Poster Session 3\n2. RELATED WORK\nAutomatic music analysis such as music item clustering,\nclassiï¬cation, and similarity search has been playing a\ncentral role in music information retrieval. Traditional au-\ntomatic music analysis methods usually focus only on au-\ndio content analysis via audio feature extraction. Timbral\ntexture features [25] are the most widely used features,\nwhich usually consist of Short-Term Fourier Transform\n(STFT) and Mel-Frequency Cepstral Coefï¬cients (MFC-\nC) [21]. Various data mining and statistical methods have\nbeen applied to such features for classiï¬cation and clus-\ntering of music items, such as artists, songs, and album-\ns [3, 4, 6, 12, 20, 25].\nAnalysis of music social tags is a subject quickly gain-\ning popularity in music information retrieval research.\nMusic social tags are free-text descriptions of any length\n(though in practice there sometimes is a limit in terms of\nnumber of characters) with no restriction on the words to\nbe used. Because they are free texts, they are thought of\nas representing feelings of listeners on the music items\n(artists, songs, etc.) for which they leave tags. Also, be-\ncause they are free texts, they range from a single character\n(e.g., â€œ!â€) to a full sentence (e.g., â€œI love you baby, can I\nhave some more?â€). However, in many cases, they are one\nor two words, such as â€œSadâ€, â€œHappyâ€, â€œBlack Metalâ€,\nâ€œLoved itâ€, and â€œIndie Popâ€. As can be easily seen social\ntags include words that do not necessarily appear as labels\nexperts such as musicologists provide. Their amateurism\nnotwithstanding, by collecting a large number of tags for\none single piece of music item, an understanding can be\nobtained on how the general listeners appreciate the item.\nWith that idea, work has been done to show the promise of\nusing tags for music data analysis. For example, Lamere\nand Pampalk [15] use tags to enhance simple search, sim-\nilarity analysis, and clustering of music items. Lehwark\net al. [17] generate visual clustering of tagged music da-\nta. Karydis et al. [13] propose a tensor-based algorithm to\ncluster music items using 3-way relational data involving\nsong, users, and tags. The effectiveness of tags may come\nfrom the fact that the distance between the original data\nsource and the tag in terms of informativeness appears to\nbe much smaller. There also exist a few efforts in combin-\ning content-based and tag-based analysis. For example,\nF. Wang et al. [27] attempts to integrate audio contents and\ntags for multi-label classiï¬cation of music styles. D. Wang\net al. [26] explores the integration of music content and\ntags in the problem of artist style clustering.\nIn addition to social tags, much more social informa-\ntion has become available on the Internet. For instance,\nsocial networking sites, such as Facebook and MySpace,and a social medium Twitter can provide the friendship\ninformation among users by adding a friend on Facebook\nor following a tweet page on Twitter. Recent work by\nAnglade et al. [1] uses complex network theoretic analysis\nto group similar listeners. Jacobson et al. [11] and Fields\net al. [8] study the inï¬‚uence of social networks for the mu-\nsic community detection and playlists generation. In this\npaper, we explore the effectiveness of the joint use of the\nanalysis of the social networking graph and the tag-based\nmusic style clustering.\n3. METHODOLOGY\n3.1 Framework\nFigure 3.1 shows the framework of our proposed music\nstyle clustering system that integrates tag and social graph\nanalysis. Given a collection of representative music pieces\nfrom different artists, we ï¬rst obtain the tags describing\nthese music pieces to construct a music-tag matrix and\ngenerate the social networking graph among the artists,\nfrom which the artist relation matrix is created. We then\nperform matrix factorization on the music-tag matrix us-\ning artist relation matrix as the base. Upon the conver-\ngence of the factorization, we can obtain the music style\nindicator matrix and ï¬nally partition the music pieces into\ndifferent style groups.\nmusicÍ²tag\u0003representationartist\u0003social\u0003graphartist\u0003relation\u0003matrix\nfactorization\u0003using\u0003artist\u0003relation matrix as bases\nrelation\u0003matrix\u0003as\u0003basesartistÍ²style\u0003cluster\u0003tagÍ²style\u0003cluster\u0003indicatorindicator\nFigure 1 . The framework of the proposed method.\n3.2 Artist Social Graph Construction\nIn order to construct an artist social graph, we select 327\nartists that are active users of Twitter. The genres cov-\nered these artists are Pop, Jazz, Rock, Hip Hop, and Coun-\n43612th International Society for Music Information Retrieval Conference (ISMIR 2011)\ntry. Each node of the graph represents an artist. For these\nartists we extract the â€œfollowingâ€ information among these\nartists using the API provided by Twitter. If artist Aiis\nâ€œfollowingâ€ the tweets of artist Aj, there will be a directed\nlink from node Aito nodeAj. An example of the gener-\nated social graph is shown in Figure 2.\nRihannaNicki Minaj\nKaty\u0003Perry\nPink\nLady\u0003Gaga\nBruno\u0003Mars\nTaylor\u0003Swift\nBritney\u0003Spears\nFigure 2 . An Example Social Graph Generated from\nTwitter.\n3.3 Artist Relation Matrix Generation\nBased on the artist social graph, we can generate the artist\nrelation matrix which considers both the direct and indi-\nrect relationships using the method proposed in [9]. Sup-\npose that artist Aiis followed by a set of artists Fi, a\nmatrixSto represent the direct relationships among the\nartists can be computed in this way:\nSji=/braceleftBigg\n1/|Fi|ifAjâˆˆFi\n0otherwise,\nwhere|Fi|is the size of set Fi. To capture the indirect\nrelationships, we perform a random walk on the directed\ngraph denoted by S. An artist can be identiï¬ed as a relat-\ned one if the random walk stops at the node representing\nhim/her. A parameter Î±is used to specify the probability\nthat the random walk stops at the current node which is set\nto 0.99 in the experiments, and based on the properties of\nrandom walk, the relation matrix can be computed as\nB= (1âˆ’Î±)(1âˆ’Î±S)âˆ’1.\n3.4 Factorization with Artist Relation Base Matrix\n3.4.1 the Model\nIn order to obtain the music style clusters, we perform ma-\ntrix tri-factorization [7] using the artist relation matrix asthe base matrix. The problem can be treated as an opti-\nmization problem with the following objective:\nmin\nBâ‰¥0,Uâ‰¥0,Vâ‰¥0||Xâˆ’BUVT||,s.t.UTU=I,VTV=I,\nwhereXdenotes the artist-tag matrix, and Bis the gen-\nerated artist relation matrix as described in Section 3.3.\nFromU, we can obtain the artist-style clusters, and from\nVwe can get the tag-style clusters. To solve this opti-\nmization problem, we use an algorithm similar to the tri-\nfactorization [7] and nonnegative matrix factorization (N-\nMF) [16] to iteratively update UandVas follows:\nUasâ†Uas[CBTV]as\nVtsâ†Vts[BDTU]ts,\nwhereCij =Xij/[UVTB]ij, andDij =\nXij/[BUVT]ij. Different with the traditional tri-\nfactorization approach, here we use the social relation\nmatrix as the base matrix to incorporate social networking\ninformation among the artists, and the base matrix is ï¬xed\nduring the updates of the other two matrices. The beneï¬t\nof using the base matrix is that the artist relations obtained\nfrom the social media can be naturally incorporated to\nguide the factorization procedures.\n3.4.2 Computational Algorithm\nIn the algorithm derivation, we follow the Expectation-\nMaximization (EM) procedure to maximize the marginal-\nized likelihood of observations by iteratively updating the\nartist-style and tag-style matrices until convergence. The\ncomputational algorithm is described in Algorithm 1.\n3.4.3 Algorithm Correctness\nNow we prove the loss /lscript(U,V)is nonincreasing under the\nupdate rules.\nProof LetÎ±iklj=Bik/tildewideUkl/tildewideVjl/[B/tildewideU/tildewideVT]ij. Applying\nJensenâ€™s inequality, we obtain\n/lscript(U,V) =X\nij(X\nklBikUklVjlâˆ’Xijln(X\nklBikUklVjl))\nâ‰¤X\nijX\nkl(BikUklVjlâˆ’XijlnBikUklVjl\nÎ±iklj)\n=âˆ’X\nijklCijBikeUkleVjlln(UklVjl)\ndef=Q(U,V;eU,eV).\n(1)\nThe equality holds when U=/tildewideUandV=/tildewideV. Instead\nof minimizing /lscript, we minimizeQwithout the non-negative\n437Poster Session 3\nAlgorithm 1 Factorization given an artist relation base.\nInput:X: artist-tag matrix.\nB: artist-artist matrix;\nOutput:U: artist-style matrix;\nV: tag-style matrix.\nbegin\n1.Initialization:\nRandomly initialize UandV.\n2.Iteration:\nrepeat\n2.1 Compute Cij=Xij/[UVTB]ij;\n2.2 AssignUasâ†Uas/bracketleftbig\nBTCV/bracketrightbig\nst,\nand normalize each column to 1;\n2.3 Compute Dij=Xij/[BUVT]ij;\n2.4 AssignVtsâ†Vts/bracketleftbig\nDTBU/bracketrightbig\ndt,\nand normalize each row to 1;\nuntil convergence\n3.ReturnU,V\nend\nconstraints. Later on, we ï¬nd that the update rules satisfy\nthe non-negative constraints. The Lagrangian of Qis\nL(U,V;\u0018) =Q(U,V;eU,eV) +\u0018T(UT1âˆ’1).+\u0010T(V1âˆ’1).\n(2)\nThe Karush-Kuhn-Tucker (KKT) conditions are\nâˆ‚LUkl=âˆ’1\nUkl/tildewideUkl/bracketleftBig\nBTC/tildewideV/bracketrightBig\nkl+Î¾l= 0, (3)\nâˆ‚LVjl=âˆ’1\nVjl/tildewideVjl/bracketleftBig\nDTB/tildewideU/bracketrightBig\njl+Î¶j= 0, (4)\nâˆ‚LÎ¾l=/summationdisplay\nkUklâˆ’1 = 0, (5)\nâˆ‚LÎ¶j=/summationdisplay\nlVjlâˆ’1 = 0 (6)\nWe derive the update rule from the KKT conditions. We\ncan verify that the update rules keep UandVnon-\nnegative.\n4. EXPERIMENTS\n4.1 Data Collection\nFor experimental purpose, we select 327 most popular\nartists of the following 5 styles: Pop (91 artists), Rock (67\nartists), Country (55 artists), Jazz (48 artists), and Hip Hop\n(66 artists). We use the API provided by Twitter to check\nif there is a â€œfollowingâ€ relationship among these artists.The style information and tags of the artists are collected\nfrom Last.fm (http://www.last.fm).\n4.2 Implemented Baselines\nWe implement the following baselines to compare them\nwith our proposed method which integrating the social\ntags and the social networking graph.\nâ€¢K-means - performs standard K-means clustering\non the artist-tag matrix.\nâ€¢Normalized Cuts (Ncut) [28] - conducts graph-\nbased spectral clustering using normalized cuts.\nâ€¢Nonnegative Matrix Factorization (NMF) [16] - per-\nforms nonnegative matrix factorization on the artist-\ntag matrix to obtain the artist-style matrix from\nwhich the artist cluster assignments can be obtained.\nâ€¢Tri-factorization (Tri-fac) [7] - performs tri-\nfactorization on the artist-tag matrix.\nâ€¢Probabilistic Latent Semantic Indexing (PLSI) [10]\n- performs PLSI on the artist-tag matrix.\nâ€¢PLSI+PHITS [5] - combines the tag-based analy-\nsis with social graph using PLSI plus Probabilistic\nHyperlink-Induced Topic Search (PHITS).\nThese baseline methods that we use in the experiments\nare most widely used clustering algorithms and some new\nemerged methods combing content and link analysis in da-\nta mining, information retrieval, and social network anal-\nysis areas. We aim to compare our proposed models with\nthe state-of-the-art methods for artist clustering.\n4.3 Evaluation Methods\nTo measure the artist style clustering performance, we use\naccuracy and normalized mutual information (NMI) as\nperformance measures.\nâ€¢Accuracy measures the relationship between each\ncluster and the ground truth class. It sums up the\ntotal matching degree between all pairs of clusters\nand classes. Accuracy can be represented as:\nAccuracy =Max (/summationdisplay\nCk,LmT(Ck,Lm))/N,\nwhereCkdenotes the k-th cluster, and Lmis the\nm-th class. T(Ck,Lm)is the number of entities\nwhich belong to class m and are assigned to cluster\nk. Accuracy computes the maximum sum of\n43812th International Society for Music Information Retrieval Conference (ISMIR 2011)\nT(Ck,Lm)for all pairs of clusters and classes, and\nthere is no overlap among these pairs. It is obvious\nthat the greater accuracy, the better clustering\nperformance.\nâ€¢NMI [22] measures the amount of statistical infor-\nmation shared by two random variables represent-\ning cluster assignment and underlying class label.\nSuppose entry nijdenotes the amount of data item-\ns belonging to cluster iand classj. NMI is then\ncomputed as:\nNMI =/summationtextc\ni=1/summationtextk\nj=1nij\nnlognijn\nni.n.j/radicalBig\n(/summationtextc\ni=1âˆ’ni.\nnlogni.\nn)(/summationtextk\nj=1âˆ’n.j\nnlogn.j\nn),\nwhereni.=/summationtextk\nj=1nij,n.j=/summationtextc\ni=1nij,n,c,k\ndenote the total number of data objects, the number\nof clusters, and the number of classes, respectively.\nBased on our prior knowledge of the number of\nclasses, we set the number of clusters equal to the\ntrue number of classes, i.e., c=k.\n4.4 Experimental Results\nFigure 3 and Figure 4 show the accuracy and NMI results\nof different clustering methods respectively.\nKÃ¯means Ncuts NMF TriÃ¯fac PLSI PLSI+PHITS Ours00.050.10.150.20.250.30.350.40.45Accuracy\nFigure 3 . The accuracy results of different clustering\nmethods.\nThe clustering results of our proposed method outper-\nforms the state-of-the-art methods signiï¬cantly. From the\nresults, we have the following observations.\nKÃ¯means Ncuts NMF TriÃ¯fac PLSI PLSI+PHITS Ours00.020.040.060.080.10.120.140.160.180.2NMIFigure 4 . The NMI results of different clustering method-\ns.\nâ€¢(1) Graph-based and factorization based tag-aware\nclustering methods outperform traditional cluster-\ning methods such as K-means.\nâ€¢(2) Methods incorporating social networking graph\nanalysis (such as PLSI+PHITS and Ours) demon-\nstrate more promising performance than the meth-\nods using only social tag information, which shows\nthe effectiveness of the integration of the different\ninformation sources.\nâ€¢(3) Our factorization with given artist relation bases\noutperforms PLSI+PHITS which is one of the\nmost widely used combination methods because our\nmethod takes the indirect relationships into consid-\neration and naturally incorporates it into the algo-\nrithm.\n5. CONCLUSION AND FUTURE WORK\nIn this paper, we explore the potential beneï¬ts of integrat-\ning tags and social networking graphs in music style clus-\ntering. Given a collection of artists and their representative\nmusic pieces, social tags of free languages are extracted to\ndescribe the music pieces. The direct and indirect rela-\ntionships among the artists are also discovered from the\nartist social networking graph, which is generated from\npopular social media sites, such as Twitter. Then a fac-\ntorization based algorithm is derived to make use of both\nthe two types of information. Experimental results on real\nworld data demonstrate the effectiveness of the proposed\nmethod.\n439Poster Session 3\nThis is a pilot study of incorporating social networking\nanalysis into music style clustering, and the initial results\nshow the promising future of research in this direction.\nIn the future work, large-scale data sets will be collected\nand further experiments will be performed on them. We\nwill also discover other meaningful and useful types of\ninformation and examine if they can facilitate the task of\nmusic style analysis.\n6. ACKNOWLEDGMENTS\nThis work is in part supported by NSF Grant CCF-\n0948590.\n7. REFERENCES\n[1] A. Anglade, M. Tiemann, and F. Vignoli: â€œComplex-\nnetwork theoretic clustering for identifying groups of similar\nlisteners in p2p systems,â€ Proceedings of RecSys , 2007.\n[2] K. Bischoff, C. Firan, W. Nejdl, and R. Paiu: â€œCan all tags\nbe used for search?,â€ Proceedings of CIKM , 2008.\n[3] S. Chen and S. Chen: â€œContent-based music genre classiï¬-\ncation Using timbral feature vectors and support vector ma-\nchine,â€ Proceedings of ICIS , 2009.\n[4] R. Cilibrasi, P. Vit Â´anyi, and R. Wolf: â€œAlgorithmic cluster-\ning of music Based on string compression,â€ Computer Music\nJournal 28:4, 2004.\n[5] D. Cohn and T. Hofmann: â€œThe missing link - a probabilis-\ntic model of document content and hypertext connectivity,â€\nNIPS , 2000.\n[6] H. Deshpande, R. Singh, and U. Nam: â€œClassiï¬cation of\nmusic signals in the visual domain,â€ Proceedings of the the\nCOST-G6 Conference on Digital Audio Effects , 2001.\n[7] C. Ding, T. Li, W. Peng, and H. Park. â€œOrthogonal nonnega-\ntive matrix tri-factorizations for clustering,â€ SIGKDD , 2006.\n[8] B. Fields, K. Jacobson, C. Rhodes, and M. Casey: â€œSocial\nPlaylists and Bottleneck Measurements: Exploiting Musi-\ncian Social Graphs Using Content-Based Dissimilarity and\nPairwise Maximum Flow Valuesâ€, Proceedings of ISMIR ,\n2008.\n[9] Z. Guo, S. Zhu, Y . Chi, Z. Zhang, and Y . Gong. â€œA latent\ntopic model for linked documents,â€ Proceedings of SIGIR ,\n2009.\n[10] T. Hofmann: â€œProbabilistic latent semantic indexing,â€ SI-\nGIR, 1999.\n[11] K. Jacobson, B. Fields, and M. Sandler: â€œUsing Audio Anal-\nysis and Network Structure to Identify Communities of On-\nLine Social Networks of Artists,â€ Proceedings of ISMIR ,\n2008.[12] Y . Liu, Y . Wang, A. Shenoy, W. Tsai, and L. Cai: â€œClustering\nmusic recordings by their keys,â€ ISMIR , 2009.\n[13] I. Karydis, A. Nanopoulos, H. Gabriel, and M. Spiliopoulou:\nâ€œTag-aware spectral clustering of music items,â€ ISMIR , p-\np. 159â€“164, 2009.\n[14] P. Knees, T. Pohle, M. Schedl, D. Schnitzer, K. Seyerlehn-\ner, and G. Widmer: â€œAugmenting text-based music retrieval\nwith audio similarity,â€ ISMIR , 2009. (Tutorial)\n[15] P. Lamere and E. Pampalk: â€œSocial tags and music informa-\ntion Retrieval,â€ ISMIR , 2008.\n[16] D. Lee and H. Seung: â€œAlgorithms for non-negative matrix\nfactorization,â€ NIPS , 2001.\n[17] P. Lehwark, S. Risi, and A. Ultsch: â€œVisualization and Clus-\ntering of Tagged Music Data,â€ in GFKL , 2007.\n[18] M. Levy and M. Sandler: â€œLearning latent semantic models\nfor music from social tagsâ€ Journal of New Music Research ,\n37:137â€“150, 2008.\n[19] E. Pampalk, A. Flexer, and G. Widmer: â€œImprovements of\naudio-based music similarity and genre classiï¬cation,â€ IS-\nMIR, 2005.\n[20] D. Pye: â€œContent-based methods for managing electronic\nmusic,â€ ISCASSP , 2000.\n[21] L. Rabiner and B. Juang: Fundamentals of Speech Recogni-\ntion, Prentice-Hall, NJ, 1993.\n[22] A. Strehl and J. Ghosh: â€œClustering ensembles - a knowl-\nedge reuse framework for combining multiple partitions,â€\nJournal of Machine Learning Research , 3:583-617, 2003.\n[23] P. Symeonidis, M. Ruxanda, A. Nanopoulos, and Y .\nManolopoulos: â€œTernary semantic analysis of social tags for\npersonalized music Recommendation,â€ ISMIR , 2008.\n[24] D. Turnbull, L. Barrington, M. Yazdani, and G. Lanckriet:\nâ€œCombining audio content and social context for semantic\nmusic discovery,â€ SIGIR , 2009.\n[25] G. Tzanetakis and P. Cook: â€œMusical Genre Classiï¬cation\nof Audio Signals,â€ IEEE Transactions on Speech and Audio\nProcessing , 10:5, 2002.\n[26] D. Wang, T. Li, and M. Ogihara: â€œAre tags better than audio\nfeatures? The effect of Joint use of tags and audio content\nfeatures for artistic style clustering,â€ ISMIR , 2010.\n[27] F. Wang, X. Wang, B. Shao, T. Li, and M. Ogihara: â€œTag\nintegrated multi-label music style classiï¬cation with hyper-\ngraph,â€ in ISMIR , pp. 363â€“368, 2008.\n[28] J. Shi and J. Malik: â€œNormalized Cuts and Image Segmen-\ntation,â€ IEEE Trans. Pattern Anal. Mach. Intell. , 22(8):888â€“\n905, 2002.\n440"
    },
    {
        "title": "User studies in the Music Information Retrieval Literature.",
        "author": [
            "David M. Weigl",
            "Catherine Guastavino"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1417811",
        "url": "https://doi.org/10.5281/zenodo.1417811",
        "ee": "https://zenodo.org/records/1417811/files/WeiglG11.pdf",
        "abstract": "This paper presents an overview of user studies in the Music Information Retrieval (MIR) literature. A focus on the user has repeatedly been identified as a key requirement for future MIR research; yet empirical user studies have been relatively sparse in the literature, the overwhelming research attention in MIR remaining systems-focused. We present research topics, methodologies, and design implications covered in the user studies conducted thus far.",
        "zenodo_id": 1417811,
        "dblp_key": "conf/ismir/WeiglG11",
        "keywords": [
            "user studies",
            "Music Information Retrieval",
            "key requirement",
            "future MIR research",
            "empirical user studies",
            "sparse in the literature",
            "systems-focused",
            "research topics",
            "methodologies",
            "design implications"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nUSER STUDIES IN THE MUSIC INFORMATION RETRIEVAL LITERATURE\nDavid M. Weigl and Catherine Guastavino\nSchool of Information Studies, McGill University\nMontreal, QC, Canada\ndavid.weigl@mail.mcgill.ca\nABSTRACT\nThis paper presents an overview of user studies in the Mu-\nsic Information Retrieval (MIR) literature. A focus on the\nuser has repeatedly been identiï¬ed as a key requirement for\nfuture MIR research; yet empirical user studies have been\nrelatively sparse in the literature, the overwhelming research\nattention in MIR remaining systems-focused. We present re-\nsearch topics, methodologies, and design implications cov-\nered in the user studies conducted thus far.\n1. INTRODUCTION\nDespite recurring calls for a greater focus on user-centric\nresearch, work in the ï¬eld of Music Information Retrieval\n(MIR) has been largely systems-focused. This paper reports\non the limited but growing body of user studies in the ï¬eld.\nA broad deï¬nition of â€˜user studyâ€™ is employed in the article\nselection: qualifying documents report on empirical inves-\ntigations of user requirements or interactions with systems\nprimarily aimed at providing access to musical information,\nincluding musical recordings, scores, lyrics, photography\nand artwork, and other associated metadata.\nThe goals of this review are threefold: to survey the dis-\ntinct topics that have been investigated by user studies in the\nï¬eld; to provide an overview of the research methodologies\nemployed in these studies; and to report on implications for\nMIR systems design offered by the works covered.\n2. SYSTEMS-CENTRIC FOCUS IN MIR\nResearch activity in MIR has been motivated to some extent\nby textual Information Retrieval (IR)â€”a ï¬eld of research\ndating back to the 1950â€™s. Plans for an evaluation platform\ninspired by TREC (Text REtrieval Conference) [37] were\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proï¬t or commercial advantage and that copies\nbear this notice and the full citation on the ï¬rst page.\nc/circlecopyrt2011 International Society for Music Information Retrieval.under discussion from ISMIRâ€™s early days [13], and even-\ntually led to the creation of MIREX, the Music Information\nRetrieval Evaluation eXchange [14]. Given this emulation\nof early developments in the ï¬eld of textual lR, it is perhaps\nunsurprising that the primary emphasis of research in MIR\nhas been placed on systems development. Formal consider-\nation of user information needs and information behaviour\nhas been sparse in comparison. This imbalance is problem-\natic: a lack of grounding in user requirements makes the\nreal-world applicability of developed MIR systems a matter\nof speculation [2]. The situation reï¬‚ects the early state of\nresearch in the ï¬eld of textual IR, where similar early em-\nphasis on information systems gradually gave way to a more\nuser-centric paradigm [10, 38].\nArticles reï¬‚ecting on the state of MIR have repeatedly\ncalled for a greater focus on the potential users of MIR sys-\ntems [13]. In his wide-ranging summary of the early state\nof the ï¬eld, Downie identiï¬es the â€˜multiexperiential chal-\nlengeâ€™ to MIR [11]: subjective musical experience varies\nnot only between, but also within individuals, depending on\naffective and cultural context, associations between the mu-\nsic and events from episodic memory, and a host of other\nfactors.\nUsersâ€™ information needs vary accordingly; an ethno-\nmusicologistâ€™s analytical requirements are likely served by\nqueries of a different nature to those used by a party host\ncompiling a playlist. Core IR concepts such as â€˜similarityâ€™\nand â€˜relevanceâ€™ may also be variably deï¬ned: â€˜similarityâ€™\nmight, for instance, refer to song structure, or to mood con-\nveyed; â€˜relevanceâ€™ to a tuneâ€™s bibliographical ï¬t to a key-\nword query, or to its applicability to a given use case (e.g.,\nâ€˜driving,â€™ â€˜housework,â€™ or â€˜exerciseâ€™).\nDesign decisions have typically been based on â€œintuitive\nfeelings for user information seeking behaviour,â€ [8] â€œanec-\ntodal evidence and a priori assumptions of typical usage sce-\nnariosâ€ [25] when facing such issues. User studies, con-\nducted with the same empirical rigour and research excel-\nlence we have come to expect from systems-based research,\ncan provide valuable insights for MIR researchers and de-\nvelopers, resulting in more useful systems for MIR users\nand greater ecological validity in research ï¬ndings.\n335Oral Session 5: User Studies\n3. REVIEW OF USER STUDIES IN THE MIR\nLITERATURE\n3.1 Selection Strategy\nThe criteria employed in article selection for this review\nemploy a broad deï¬nition of the term â€˜user studyâ€™, as de-\nscribed above. Articles primarily reporting the results of\nsuch user studies were targeted for inclusion. The ISMIR\nCloud Browser [16] served as a starting point for article dis-\ncovery; this textual information retrieval tool is capable of\ngenerating visualisations and ranked result lists based on a\nuser query, using a TF Â·IDF-based metric [40] to match the\nquery to a set of 719 articles representing the output of the\nï¬rst decade of ISMIR-related activity. Results from the fol-\nlowing query strings were used: â€œhuman responsesâ€; â€œinfor-\nmation behaviourâ€; â€œinformation useâ€; â€œinformation needâ€;\nâ€œparticipantsâ€; â€œperceptual evaluationâ€; â€œrespondentsâ€; â€œus-\nabilityâ€; â€œuser studyâ€; and â€œuser testingâ€.\nAdditional articles were identiï¬ed through a search on\nthe ISI Web of Knowledge database using the query string\nâ€œâ€˜music information retrievalâ€™ AND userâ€; by following ci-\ntations in the resultant documents; and by searching for ar-\nticles citing the original documents using Google Scholar.\n3.2 Research Topics\nA number of different aspects of music information be-\nhaviour have been investigated. The topics have been for-\nmulated here by reference to explicit research questions,\nwhere provided, or by the implied aims of the research:\nâ€¢User requirements and information needs [1, 30];\nâ€¢The information needs of speciï¬c groups [9, 17â€“19]\nand in speciï¬c contexts [7];\nâ€¢Insights into speciï¬c aspects of music perception and\npreference, such as the factors that cause listeners to\ndislike certain songs [5], the impact of social relations\non music acquisition and taste [23], and the effects of\ndemographic factors and musical background on the\nsemantic descriptions of music [26, 27];\nâ€¢Analyses of textual MIR queriesâ€”symbolic represen-\ntation of the melody sought [34], and natural language\nexpressions of music information needs [1, 25];\nâ€¢Employment of user studies to generate ground-truth\ndata for use in training and evaluation corpora [31â€“\n33].\nâ€¢The organisation of digital music information [6, 17];\nâ€¢Search strategies and relevance criteria used when ac-\ntively seeking new music [22, 24];\nâ€¢Information behaviour in passive or serendipitous en-\ncounters with new music [4];3.3 Methodologies\nThe research methodologies employed in the user studies\nare predominantly qualitative in nature. Approaches range\nfrom situated-researcher methodologies, such as ethno-\ngraphic observation of information behaviour, face-to-face\nuser interviews, and participatory design panels, to more re-\nmote methodologies such as diary studies, online surveys,\nand query log analyses.\nThe emphasis on qualitative methodology reï¬‚ects the\nlargely exploratory nature of existing research; only a few\nstudies take quantitative or mixed approaches, by quantita-\ntive analysis of natural language user queries [25], by ap-\nplying measures from usability engineering [34], by use of\nbehavioural studies [21, 36], and by systematic analyses of\ndemographic factors and musical background [26]. A fur-\nther group of studies employs quantitative approaches to-\nwards the systems-centric goal of corpus generation, crowd-\nsourcing annotations from large quantities of users com-\npetiting in music-related online games [31â€“33].\nThe relatively small number of user studies is reï¬‚ected\nin the equally small number of researchers involved. Conse-\nquently, many studies have used somewhat uniform partici-\npant pools, consisting predominantly of male subjects from\nsimilar backgrounds. Several studies do take precautions to\nensure more representative sampling: for instance, Taheri-\nPanah and MacFarlane [30] recruit participants from 3 dis-\ntinct age-bands, balancing gender; and Lesaffre et al. [26]\nmake the effect of demographic context on the perception\nand description of music a research priority in a large scale,\ncross-sectional study.\nThe limited number of researchers has also resulted in\na somewhat homogeneous use of research methodologies;\nthe majority of the user studies in the ï¬eld have been qual-\nitative in nature, usually making use of Grounded Theory\n(GT) in the analysis phase [15]. GT is an approach in which\nobservations are coded with no prior assumptions, allowing\ntheory to emerge from the data. GT is relied upon exclu-\nsively in the data analysis phases of many of the articles\ncovered [1, 4â€“8, 22, 24].\nGT is an appropriate tool in exploratory research, where\nno conceptual models have been established to aid data anal-\nysis. As such, these studies represent valuable work; how-\never, there is a clear opportunity for further research to build\na conceptual framework informed by the existing results, by\nconducting further qualitative research to pin down the re-\nquired concepts, or by pursuing quantitative work to identify\nwhether existing results can be generalized.\nA notable exception is presented by Inskip, Butterworth,\n& MacFarlaneâ€™s study into the information needs of users of\na folk music library (2008) [17]; here, qualitative, face-to-\nface interviews are analysed in line with Nicholasâ€™ frame-\nwork for evaluating information need (2000) [29]. The re-\nsearchers are thus able to base their results on an established\n33612th International Society for Music Information Retrieval Conference (ISMIR 2011)\nanalytical tool, while at the same time validating the appli-\ncability of the tool in a new context.\n3.4 Recommendations for MIR System Design\nWhile the studies presented in this review are concerned\nwith user requirements and information behaviour, a pri-\nmary goal of such research is to inform the development of\ninformation systems to better meet such requirements and\nsupport such behaviours. Over the last decade, researchers\nhave built an arsenal of algorithms and components to tackle\nvarious aspects of MIR; however, the ï¬eld has yet to pro-\nduce an integrated, full-featured system, tying together these\nvarious capabilities. Accomplishing this has been described\nas the â€œGrand Challengeâ€ of ISMIRâ€™s second decade [13]\n(p. 18). By conducting user-centric research and applying\nï¬ndings to the design of such a system and its components,\nwe can â€œimprove the quality of the communityâ€™s research\noutputâ€ and help create â€œtruly useful music-IR systemsâ€ (p.\n17).\nThe recommendations and implications for MIR systems\nconcluded by the studies covered in the review originate\nfrom a number of different contexts, e.g., digital libraries\nversus personal collections. Thus, not all of the recom-\nmendations are necessarily applicable to the same system;\nrather, provided here is an overview of the recommendations\navailable, in order to guide future development efforts.\n3.4.1 Undirected Browsing\nUsers spend much of their time seeking new music updating\nand expanding their musical knowledge, without a speciï¬c\ngoal in mind; they are often more motivated by the plea-\nsure of this activity in itself, than by an actual information\nneed [24]. Emphasis should be placed on such serendipi-\ntous â€˜discoveryâ€™ processes in the context of MIR systems\ndevelopment by supporting various different browsing ap-\nproaches.\nOne such approach is the provision of â€œentry pointsâ€ to\nthe catalogue, to aid users navigating through collections of\npotentially unfamiliar music [17]; this allows users to situate\nthemselves, encouraging subsequent browsing and discov-\nery. Audio previewing can be a useful tool in the browsing\nprocess, allowing users to quickly sample a piece of music\nto determine whether further attention is warranted; here,\nMIR systems could usefully identify representative portions\nof the music to sample, for instance by offering a skip-to-\nchorus feature [7].\nOther approaches might make use of visual elements; one\nstudy proposes a shifting collage of CD covers accompa-\nnied by snippets of songs from each album as it is given\nprominence in the collage [8]. Musical content could also\nbe visualized symbolically, by generating map displays that\ntranslate sound or rhythm similarity into visual proximity tobetter support genre browsing [8], or by generating graph-\nics that translate audio similarity into visual similarity more\nexplicitly [21].\n3.4.2 Goal-Directed Search\nAs when browsing, individuals employ different approaches\nto the goal-directed search for new music. Inskip et al.\n(2008) give examples of different strategies employed by\nusers of a folk music library, noting that strategies sig-\nniï¬cantly vary with research experience of the individual;\nthus, variable search techniques should be supported [17].\nSearching by similarity (to a particular song or artist) is a\npopular feature among MIR system users [36]; Isikhan et al.\n(2010) [20] evaluate a melody similarity metric in a percep-\ntual study, aiming to improve result rankings of MIR sys-\ntems. Another user study evaluates the suitability of sup-\nporting textual queries for melodic content by symbolic en-\ncoding of the sought melodic contour; results indicate that\nsuch queries are too difï¬cult to be used successfully by or-\ndinary users, and require considerable musical training to\nconstruct [34]. A different approach to textual queries re-\ntrieves musical recommendations based on semantic quali-\nties of music through affective, structural, and kinaesthetic\ndescriptors [27].\nCertain search strategies may be of value for use in spe-\nciï¬c contexts; for instance, a search function matching video\nfeatures to music features would have potential applications\nin ï¬lm making, advertising, and other domains requiring\nsynchronisation [18]. Casey et al. (2008) provide a far-\nranging overview of other available content-based search\napproaches, outlining different use cases and query types\n[3].\n3.4.3 Recommendations on Metadata\nDescriptive elements, stored as metadata, are used to search,\nï¬lter, and organize music collections. If the metadata in\na userâ€™s collection is to remain cohesive and up-to-date as\nnew items are added, simplicity of use is paramount; adding\nvalid metadata to a track should be a task requiring no\nmore than a few clicks [6]. Beyond bibliographic infor-\nmation such as artist, album, and song name, user studies\nfrequently identify the potential value of including lyrics in\nmetadata [1, 6, 7, 18]. Relational information between cat-\nalogue items, such as inter-artist links, should be provided\nto aid the user in his or her selection [22]. While metadata\nshould be accurate, â€˜fuzzyâ€™ querying should be supported;\ne.g., date queries should be treated ï¬‚exibily, allowing re-\ntrieval by decade or more blurry categories such as â€˜recentâ€™\nand â€˜oldâ€™, instead of requiring a year to be speciï¬ed [1].\nBeyond describing musical content, metadata may de-\nscribe context; â€œuse taggingâ€ can prove valuable to users\nby encoding information on different scenarios in which a\ngiven piece of music might be relevant [6, 17]. Allowing\n337Oral Session 5: User Studies\nusers to provide arbitrary metadata would allow for ï¬‚exi-\nbility in this regard, and cater to a number of use cases;\nfor instance, attendees might seek to justify the inclusion\nof a song in a party playlist at a social gathering, to the\npartyâ€™s host [7]. Related to â€œuse taggingâ€ is the provision\nof user proï¬les, or â€œmusic personalitiesâ€; these allow a rec-\nommender system to cater to different user contexts and\nmoods [22]. Demographics and musical background, and\nfamiliarity with a particular piece, have been shown to im-\npact on usersâ€™ semantic descriptions of music [26], further\nsuggesting the usefulness of distinguishing between differ-\nent categories of users. Chai and Vercoe (2000) propose\nan XML-like mark-up language which would encode such\ncontextual tagging for efï¬cient sharing and re-use between\ndifferent MIR systems [35].\n3.4.4 Social Aspects\nChanges in musical taste are invariably inï¬‚uenced by so-\ncial factors [23]; in one study, 96% of participants discussed\nmusic with their friends [30]. To incorporate social as-\npects, researchers have suggested support for collaborative\nplaylist creation [7] among users in social settings; further\nstudies discuss collaborative browsing and search [8], anno-\ntation [17], and collaborative ï¬ltering, taking into account\nboth preferences and dislikes [5].\nBeyond collaborative access of an external catalogue,\nusers enjoy browsing through other usersâ€™ music libraries.\nThis allows them to target users with compatible tastes, and\nthus discover new music [30]. Cunningham et al. (2004) [6]\ndiscuss such sharing of personal collections, emphasising\nthe requirement that a collectionâ€™s public appearance must\nbe customisable, e.g., to hide â€˜guilty pleasuresâ€™ that might\nnegatively affect the image the user wishes to convey of his\nor her musical tastes.\nSocial networking techniques could create trusted recom-\nmendations among users, mirroring the way that trust is built\nup in musical tastes among peers [24]. An online forum\ncould fulï¬ll a similar role, encouraging networking between\nusers [17].\n3.4.5 Organization of Music Information\nA study examining personal music collections reveals orga-\nnization principles based on intended use: people organize\nmusic on the basis of the situation in which they intend to\nlisten to a particular set of music (e.g. â€œwork musicâ€, â€œdriv-\ning musicâ€) [6]. The same study calls for functionality en-\nabling links between songs or song collections and online\nresources; furthermore, an archival function is suggested,\nwhich both removes neglected tracks from the standard li-\nbrary, and provides a mechanism to rediscover old music.\nAnother study [7] recommends that media interfaces sup-\nport and seamlessly integrate different ï¬le formats and me-\ndia (e.g., music downloaded to the hard drive, USB sticks,CDs, etc) into a single collection without loss of metadata.\n3.4.6 User Interface Appearance\nMusic playback systems should feature simple, clean inter-\nface designs featuring large, clearly labeled controls. Inter-\nfaces should be attractive and playful, avoiding the clinical\nand â€œsomewhat darkâ€ appearance of most currently avail-\nable media players [7]. Existing visual representations of\nmusical content, such as â€œlandscapeâ€ representations pro-\nviding a geographic view of a musical collection, have cer-\ntain disadvantages [21]; one solution is a procedural algo-\nrithm to generate icons to be applied to the music ï¬les of\nthe content they represent; this allows visual data mining of\nmusic collections from within the ï¬le listings of a standard\ncomputer operating system.\nSpecial considerations must be taken into account when\ndeveloping interfaces aimed at young users. A compre-\nhensive review of relevant guidelines has been established,\nmaking use of a participatory design panel in order to create\na novel music organizer for children [9].\n3.4.7 User Support\nGraduated access (â€œtraining wheelsâ€) can help inexperi-\nenced users to overcome the learning curve of an unfamiliar\nsystem. Online support should be available; in a digital li-\nbrary context, users should be able to contact librarians for\nhelp [17]. Certain metadata such as genre or record label are\nuseless to people lacking the required knowledge to interpret\nthem; thus, supporting descriptions should be provided [22].\nUser studies are useful in shedding light on the â€œinforma-\ntion problemâ€ of the users of MIR systems, but ultimately,\na cognitive framework will be required to better understand\nthe music seeking behaviour of MIR users [30].\n3.4.8 Hardware/Portable MIR Device\nCunningham et al. (2007) outline plans for a portable MIR\nplatform. This device would be equipped with a microphone\nthat constantly records surrounding sounds, identifying mu-\nsical extracts and saving them for later analysis by audio-\nï¬ngerprinting against a database. Such a device would be\nuseful in tracking down information on music encountered\nserendipitously during everyday activities [4]. This direc-\ntion of research seems especially relevant given the capabil-\nities and increasingly widespread adoption of smartphone\nplatforms [28].\n4. CONCLUSIONS\nUser studies have been identiï¬ed as key components of\nmusic information research. A number of studies have\nbeen conducted in this direction; however, the dominant\nparadigm in the ï¬eld is ï¬rmly systems-oriented.\n33812th International Society for Music Information Retrieval Conference (ISMIR 2011)\nWhile the existing work has provided valuable ï¬ndings\nand recommendations for future MIR development, ex-\npanded research attention will be required to provide a com-\nprehensive, generalizable picture of music information use.\nFuture research might include the more widespread adop-\ntion of quantitative methods; this would provide a route\ntowards testing the generalisability of developerâ€™s assump-\ntions and of the initial ï¬ndings thus far. Crowd-sourcing\nmethodologies, previously applied to corpus-generation\n[31â€“33], provide an intriguing direction for future quantita-\ntive work. Furthermore, a greater emphasis on demographic\ndiversity and cross-sectional research will broaden the ap-\nplicability of future research ï¬ndings towards the listening\npublic at large.\nIf the â€œGrand Challengeâ€ of the ï¬eld is to provide a fully-\nintegrated system providing all manners of MIR access [13],\na ï¬rm focus on user requirements is important; otherwise,\nconvincing listeners to actually use such a system in the real\nworld may prove to be a Grander Challenge Still.\n5. REFERENCES\n[1] D. Bainbridge, S. J. Cunningham, and J. S. Downie:\nâ€œHow People Describe their Music Information Needs:\nA Grounded Theory Analysis of Music Queries,â€ Pro-\nceedings of the 4th International Conference on Music\nInformation Retrieval , pp. 221â€“222, 2003.\n[2] D. Byrd and T. Crawford: â€œProblems of Music Informa-\ntion Retrieval in the Real World,â€ Information Process-\ning & Management , V ol. 38, No. 2, pp. 249â€“272, 2002.\n[3] M. A. Casey, R. Veltkamp, M. Goto, M. Leman, C.\nRhodes, and M. Slaney: â€œContent-Based Music Infor-\nmation Retrieval: Current Directions and Future Chal-\nlenges,â€ Proceedings of the IEEE , V ol. 96, No. 4, pp.\n668â€“696, 2008.\n[4] S. J. Cunningham, D. Bainbridge, and D. McKay:\nâ€œFinding New Music: A Diary Study of Everyday En-\ncounter with Novel Songs,â€ Proceedings of the 8th In-\nternational Conference on Music Information Retrieval ,\npp. 83â€“88, 2007.\n[5] S. J. Cunningham, J. S. Downie, and D. Bainbridge:\nâ€œâ€˜The Pain, The Pain:â€™ Modelling Music Information\nBehavior and the Songs We Hate,â€ Proceedings of the\n6th International Conference on Music Information Re-\ntrieval , pp. 474â€“477, 2005.\n[6] S. J. Cunningham, M. Jones, and S. Jones: â€œOrganiz-\ning Digital Music for Use: An Examination of Personal\nMusic Collections,â€ Proceedings of the 5th International\nConference on Music Information Retrieval , pp. 447â€“\n454, 2004.[7] S. J. Cunningham and D. M. Nichols: â€œExploring So-\ncial Music Behaviour: An Investigation of Music Selec-\ntion at Parties,â€ Proceeding of 10th International Society\nfor Music Information Retrieval Conference , pp. 26â€“30,\n2009.\n[8] S. J. Cunningham, N. Reeves, and M. Britland: â€œAn\nEthnographic Study of Music Information Seeking: Im-\nplications for the Design of a Music Digital Library,â€\nProceedings of the ACM/IEEE Joint Conference on Dig-\nital Libraries , pp. 5â€“16, 2003.\n[9] S. J. Cunningham and Y . E. Zhang: â€œDevelopment of\na Music Organizer for Children,â€ Proceedings of the\n9th International Conference on Music Information Re-\ntrieval , pp. 185â€“190, 2008.\n[10] B. Dervin and M. Nilan: â€œInformation Needs and Uses,â€\nAnnual Review of Information Science and Technology ,\nV ol. 21, No. 3, pp. 3â€“33, 1986.\n[11] J. S. Downie: â€œMusic Information Retrieval, Annual Re-\nview of Information Science and Technology , V ol. 37,\nNo. 1, pp 295â€“340, 2003.\n[12] J. S. Downie: â€œThe Music Information Retrieval Evalua-\ntion EXchange (2005â€“2007): A Window into Music In-\nformation Retrieval Research,â€ Acoustical Science and\nTechnology , V ol. 29, Num. 4, pp. 247â€“255, 2008.\n[13] J. S. Downie, D. Byrd, and T. Crawford: â€œTen Years of\nISMIR: Reï¬‚ections on Challenges and Opportunities,â€\nProceedings of the 10th International Society for Music\nInformation Retrieval Conference , pp. 13â€“18, 2009.\n[14] J. S. Downie, K. West, K., A. Ehmann, and E. Vincent:\nâ€œThe 2005 Music Information Retrieval Evaluation EX-\nchange (MIREX 2005): Preliminary Overview,â€ Pro-\nceedings of the 6th International Conference on Music\nInformation Retrieval , pp. 320â€“323, 2005.\n[15] B. G. Glaser, A. L. Strauss, and E. Strutzel: â€œThe Dis-\ncovery of Grounded Theory; Strategies for Qualitative\nResearch,â€ Nursing Journal , V ol. 17, No. 4, pp. 364,\n1968.\n[16] M. Grachten, M. Schedl, T. Pohle, and G. Widmer: â€œThe\nISMIR Cloud: A Decade of ISMIR Conferences at Your\nFingertips,â€ Proceedings of the 10th International Soci-\nety for Music Information Retrieval Conference , 2009.\n[17] C. Inskip, R. Butterworth, and A. MacFarlane: â€œA Study\nof the Information Needs of the Users of a Folk Music\nLibrary and the Implications for the Design of a Dig-\nital Library System,â€ Information Processing & Man-\nagement , V ol. 44, No. 2, pp. 647â€“662, 2008.\n339Oral Session 5: User Studies\n[18] C. Inskip, A. MacFarlane, and P. Rafferty: â€œMusic,\nMovies and Meaning: Communication in Film-makers\nSearch for Pre-existing Music, and the Implications for\nMusic Information Retrieval,â€ Proceedings of the 9th In-\nternational Conference of Music Information Retrieval ,\npp. 477â€“482, 2008.\n[19] C. Inskip, A. MacFarlane., and P. Rafferty: â€œCreative\nProfessional Users Musical Relevance Criteria,â€ Jour-\nnal of Information Science , V ol. 36, No. 4, pp. 517â€“529,\n2010.\n[20] C. Isikhan, A. Alpkocak, and Y . Ozer: â€œInterval Distinc-\ntion on Melody Perception for Music Information Re-\ntrieval,â€ Computer Music Modeling and Retrieval. Gen-\nesis of Meaning in Sound and Music , Springer Berlin /\nHeidelberg, pp. 196â€“206, 2009.\n[21] P. Kolhoff, J. PreuÃŸ, and J. Loviscach: â€œContent-based\nIcons for Music Files,â€ Computers & Graphics , V ol. 32,\nNo. 5, pp. 550â€“560, 2008.\n[22] A. Laplante: â€œUsers Relevance Criteria in Music Re-\ntrieval in Everyday Life: An Exploratory Study,â€ Pro-\nceedings of the 11th International Society for Music In-\nformation Retrieval Conference , pp. 601â€“606, 2010.\n[23] A. Laplante: â€œThe Role People Play in Adolescents Mu-\nsic Information Acquisition,â€ Workshop on Music Rec-\nommendation and Discovery , 2010.\n[24] A. Laplante and J. S. Downie: â€œEveryday Life Mu-\nsic Information-Seeking Behaviour of Young Adults,â€\nProceedings of the Seventh International Conference on\nMusic Information Retrieval , pp. 381â€“382, 2006.\n[25] J. H. Lee: â€œAnalysis of User Needs and Information Fea-\ntures in Natural Language Queries Seeking Music Infor-\nmation,â€ Journal of the American Society for Informa-\ntion Science and Technology , V ol. 61, No. 5, pp. 1025â€“\n1045, 2010.\n[26] M. Lesaffre, B. De Baets, H. De Meyer, and J.-P.\nMartens: â€œHow Potential Users of Music Search and Re-\ntrieval Systems Describe the Semantic Quality of Mu-\nsic,â€ Journal of the American Society for Information\nScience and Technology , V ol. 59, No. 5, pp. 695â€“707,\n2008.\n[27] M. Lesaffre, M. Leman, and J.-P. Martens: â€œA User-\nOriented Approach to Music Information Retrieval,â€\nDagstuhl Seminar Proceedings , 2006.\n[28] L. A. Mutchler, J.P. Shim, and D. Ormond: â€œExploratory\nStudy on Users Behavior: Smartphone Usage,â€ AMCIS\n2011 Proceedings - All Submissions . Paper 418. 2011.[29] D. Nicholas: â€œAssessing Information Needs: Tools,\nTechniques and Concepts for the Internet Age,â€ London:\nASLIB , 2000.\n[30] S. Taheri-Panah and A. MacFarlane: â€œMusic Informa-\ntion Retrieval Systems: Why Do Individuals Use Them\nand What Are Their Needs?â€ Procedings of the 5th In-\nternational Conference on Music Information Retrieval ,\npp. 455â€“460, 2004.\n[31] D. Torres, D. Turnbull, L. Barrington, and G. Lanckriet:\nâ€œIdentifying words that are musically meaningful,â€ Pro-\nceedings of the 8th International Conference on Music\nInformation Retrieval , pp. 405â€“410, 2007.\n[32] D. Turnbull, L. Barrington, and G. Lanckriet: â€œFive Ap-\nproaches to Collecting Tags For Music,â€ Proceedings of\nthe 9th International Conference on Music Information\nRetrieval , pp. 225â€“230, 2008.\n[33] D. Turnbull, L. Barrington, D. Torres, and G. Lanck-\nriet: â€œSemantic Annotation and Retrieval of Music and\nSound Effects,â€ IEEE Transactions on Audio, Speech,\nand Language Processing , V ol. 16, No. 2, pp. 467â€“476,\n2008.\n[34] A. L. Uitdenbogerd and Y . W. Yap: â€œWas Parsons Right?\nAn Experiment in Usability of Music Representations\nfor Melody-based Music Retrieval,â€ Proceedings of the\n4th International Conference on Music Information Re-\ntrieval , pp. 75â€“79, 2003.\n[35] W. Chai and B. Vercoe: â€œUsing User Models in Music\nInformation Retrieval Systems,â€ Proceedings of the 1st\nInternational Symposium on Musical Information Re-\ntrieval , 2000.\n[36] F. Vignoli: â€œDigital Music Interaction Concepts: a User\nStudy,â€ Proceedings of the 5th International Conference\non Music Information Retrieval , 2004.\n[37] E. M. V oorhees and D. K. Harman: TREC: Experiment\nand Evaluation in Information Retrieval , MIT Press,\nCambridge, US-MA, 2005.\n[38] T. D. Wilson: â€œOn User Studies and Information Needs,â€\nJournal of Documentation , V ol. 37, No. 1, pp. 3â€“15,\n1981.\n[39] T. D. Wilson: â€œInformation Behaviour: An Interdisci-\nplinary Perspective*,â€ Information Processing & Man-\nagement , V ol. 33, No. 4, pp. 551â€“572, 1997.\n[40] J. Zobel and A. Moffat: â€œExploring the Similarity\nSpace,â€ ACM SIGIR Forum , V ol. 32, pp. 18â€“34, 1998.\n340"
    },
    {
        "title": "Automatic Assessment of Singer Traits in Popular Music: Gender, Age, Height and Race.",
        "author": [
            "Felix Weninger",
            "Martin WÃ¶llmer",
            "BjÃ¶rn W. Schuller"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1417821",
        "url": "https://doi.org/10.5281/zenodo.1417821",
        "ee": "https://zenodo.org/records/1417821/files/WeningerWS11.pdf",
        "abstract": "We investigate fully automatic recognition of singer traits, i. e., gender, age, height and â€˜raceâ€™ of the main performing artist(s) in recorded popular music. Monaural source separation techniques are combined to simultaneously enhance harmonic parts and extract the leading voice. For evaluation the UltraStar database of 581 pop music songs with 516 distinct singers is chosen. Extensive test runs with Long Short-Term Memory sequence classification reveal that binary classification of gender, height, race and age reaches up to 89.6, 72.1, 63.3 and 57.6 % unweighted accuracy on beat level in unseen test data.",
        "zenodo_id": 1417821,
        "dblp_key": "conf/ismir/WeningerWS11",
        "keywords": [
            "fully automatic",
            "singer traits",
            "gender",
            "age",
            "height",
            "race",
            "pop music",
            "UltraStar database",
            "Long Short-Term Memory",
            "sequence classification"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nAUTOMATIC ASSESSMENT OF SINGER TRAITS IN POPULAR MUSIC:\nGENDER, AGE, HEIGHT AND RACE\nFelix Weninger, Martin W Â¨ollmer, Bj Â¨orn Schuller\nInstitute for Human-Machine Communication, Technische Universit Â¨at M Â¨unchen, Germany\n(weninger|woellmer|schuller)@tum.de\nABSTRACT\nWe investigate fully automatic recognition of singer traits,\ni. e., gender, age, height and â€˜raceâ€™ of the main performing\nartist(s) in recorded popular music. Monaural source sepa-\nration techniques are combined to simultaneously enhance\nharmonic parts and extract the leading voice. For evalu-\nation the UltraStar database of 581 pop music songs with\n516 distinct singers is chosen. Extensive test runs with Long\nShort-Term Memory sequence classiï¬cation reveal that bi-\nnary classiï¬cation of gender, height, race and age reaches\nup to 89.6, 72.1, 63.3 and 57.6 % unweighted accuracy on\nbeat level in unseen test data.\n1. INTRODUCTION\nSinger trait classiï¬cation, that is, automatically recogniz-\ning meta data such as age and gender of the performing\nvocalist(s) in recorded music, is currently still an under-\nresearched topic in music information retrieval, in contrast\nto the increasing efforts devoted to that area in paralinguistic\nspeech processing. Speaker trait recognition is often used in\ndialog systems to improve service quality [1], yet another\nimportant area of application is forensics where it can de-\nliver cues on the identities of unknown speakers [9]. Like-\nwise, applications in music processing can be found in cate-\ngorization and query of large databases with potentially un-\nknown artists â€“ that is, artists for whom not enough reliable\ntraining data is available for building singer identiï¬cation\nmodels as, e. g., in [12]. Robustly extracting a variety of\nmeta information can then allow the artist to be identiï¬ed\nin a large collection of artist meta data, such as the Inter-\nnet Movie Database (IMDB). In addition, exploiting gender\ninformation is known to be very useful for building models\nfor other music information retrieval tasks such as automatic\nlyrics transcription [11].\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proï¬t or commercial advantage and that copies\nbear this notice and the full citation on the ï¬rst page.\nc/circlecopyrt2011 International Society for Music Information Retrieval.Current research in speech processing suggests that the\nautomatic determination of age in full realism is challeng-\ning even in clean, spoken language [16]. On the other hand,\nit is well known that age as well as body shape (height and\nweight) have acoustic correlates [4,10] that can be exploited\nfor automatic classiï¬cation [13]; additionally, it has been\nshown that demographic traits including ethnicity can be de-\nrived from spoken language [6]. In comparison to speech,\nrecognition of singer traits is expected to be an even more\nchallenging task due to pitch variability, inï¬‚uence of voice\ntraining, and presence of multiple vocalists as well as instru-\nmental accompaniment. Previous research dealt with gen-\nder identiï¬cation of unseen artists in recorded popular mu-\nsic [17], which could be performed with over 90 % accuracy\nin full realism by extracting the leading voice through an ex-\ntension of non-negative matrix factorization (NMF) [3].\nStill, to our knowledge, few, if any, studies exist on\nrecognition of other singer traits in music. Hence, we intro-\nduce three new dimensions to be investigated: age, height\nand race. Our annotation scheme is inspired by the TIMIT\ncorpus commonly used in speech processing, which pro-\nvides rich speaker trait information. As such, we adopt\nthe term â€˜raceâ€™ from the corpusâ€™ meta-informationâ€”though\nmodern biology often neither classiï¬es the homo sapiens\nsapiens by race nor sub-categories for collective differen-\ntiation in both physical and behavioral traits. While current\nmolecular biologic and population genetic research argues\nthat a systematic categorization may not sufï¬ce the enor-\nmous diversity and ï¬‚uent differences between geographic\npopulation, it can be argued that when aiming at an end-\nuser information retrieval application, a classiï¬cation into\nillustrative, archetypal categories is desirable.\nFor evaluation of automatic singer-independent classiï¬-\ncation, we extended the UltraStar database [15] with de-\ntailed annotation of singer traits (Section 2). Furthermore,\nwe improve extraction of the leading voice by ï¬ltering of\ndrum accompaniment (Section 3). The classiï¬cation by\nBidirectional Long Short-Term Memory Recurrent Neural\nNetworks (BLSTM-RNN) is brieï¬‚y outlined in Section 4.\nComprehensive evaluation results are presented in Section 5\nbefore conclusions are drawn in Section 6.\n37Oral Session 1: Melody and Singing\n2. ULTRASTAR SINGER TRAITS DATABASE\nOur experiments build on the UltraStar database proposed\nin [17] for singer-independent evaluation of vocalist gender\nrecognition, containing 581 songs commonly used for the\nâ€˜UltraStarâ€™ karaoke game, corresponding to over 37 h to-\ntal play time. Note that using highly popular songs is no\ncontradiction to the goal of recognizing unknown artists,\nbut rather a requirement for establishment of solid ground\ntruth. The database is split according to the ï¬rst letter of\nthe name of the performer into training (A, D, G, . . . ), de-\nvelopment (B, E, H, . . . ) and test partitions (0-9, C, F, . . . ).\nThe ground truth tempo is provided and lyrics are aligned to\n(quarter) beats. The annotation of the database was substan-\ntially extended beyond gender information: The identity of\nthe singer(s) was determined at beat level wherever possible.\nThis is particularly challenging in case of formations such\nas â€˜boy-â€™ or â€˜girl-groupsâ€™, in which case the â€˜singer diariza-\ntionâ€™ (alignment of the singer identity to the music) was de-\ntermined from publicly available music videos. Then, infor-\nmation on gender, height, birth year and race of the 516 dis-\ntinct singers present in the database was collected and multi-\nply veriï¬ed from on-line textual and audiovisual knowledge\nsources, including IMDB, Wikipedia and YouTube. All an-\nnotation was performed by two male experts for popular mu-\nsic (24 and 28 years old).\nIn a multitude of cases, two or more singers are singing\nsimultaneously. In [17], which only dealt with gender recog-\nnition, the case that male and female vocalists are singing in\nâ€˜duetâ€™ was treated as a special case, where the corresponding\nbeats were excluded from further analysis. To extend this\nparadigm to the now multi-dimensional annotation, we de-\nrived the following scheme: For nominal traits (gender and\nrace), the beats were marked as â€˜unknownâ€™ unless all simul-\ntaneously present artists share the same attribute value. For\ncontinuous-valued traits (age and height), the average value\nwas calculated, since in formations the individual artistsâ€™\ntraits are usually similar. This procedure was also followed\nto treat performances of formations where an exact singer\ndiarization could not be retrieved, by assuming presence of\nan â€˜average singerâ€™ throughout. In case that the desired at-\ntribute is missing for at least one of the performing vocalists,\nthe corresponding beats were marked as â€˜unknownâ€™.\nThe distribution of gender and race among the 516\nsingers are shown in Figures 1a and 1b. Age (Figure 1c)\nand height (Figure 1d) are shown as box-and-whisker plots\nwhere boxes range from the ï¬rst to the third quartile and\nall values that exceed that range by more than 1.5 times the\nwidth of the box are considered outliers, depicted by cir-\ncles. Unlike gender, height, and race, the age distribution\ncan only be given on beat level since age is not well deï¬ned\nper artist (due to different recording dates) nor per song (due\nto potentially multiple singers per song). The continuous-\nvalued attributes height and age were discretized to â€˜shortâ€™# beats train devel test Î£\nno voice (0) 90 076 75 741 48 948 214 765\ngender\nfemale (f) 32 308 23 071 9 739 65 118\nmale (m) 55 505 49 497 37 686 142 688\n? 86 253 771 1 110\nrace\nwhite (w) 67 525 62 003 40 479 170 007\nb/h/a 16 378 9 465 7 136 32 979\n? 3 996 1 353 581 5 930\nage\nyoung (y) 48 510 42 056 25 682 116 248\nold (o) 34 074 24 596 18 712 77 382\n? 5 315 6 169 3 802 15 286\nheight\nshort (s) 29 638 24 946 8 562 63 146\ntall (t) 30 177 30 146 23 452 83 775\n? 28 084 17 729 16 182 61 995\nÎ£ 177 975 148 562 97 144 423 681\nTable 1 : Number of beats per trait, class and set (train /\ndevel /test) in the UltraStar singer trait database. â€˜b/h/aâ€™:\nblack / hispanic / asian. â€˜Unknownâ€™ (?) includes simultane-\nous performance of artists of different gender / race, as well\nas those with unknown ground truth.\n(s,<175cm) and â€˜tallâ€™ (t, â‰¥175cm), respectively â€˜youngâ€™\n(y,<30years) and â€˜oldâ€™ (o, â‰¥30years); the thresholds\nare motivated by the medians of the traits (175 cm resp.\n28 years) to avoid sparsity of either class. For race, the pro-\ntotypical classes â€˜Whiteâ€™, â€˜Blackâ€™, â€˜Hispanicâ€™ and â€˜Asianâ€™\nwere annotated. The smaller classes â€˜Blackâ€™, â€˜Hispanicâ€™ and\nâ€˜Asianâ€™ were subsumed due to great sparsity of â€˜Hispanicâ€™\nand â€˜Asianâ€™ singers: Our goal is to evaluate our system on all\ndata for which a ground truth is available. â€˜Unknownâ€™ beats\nare excluded from further analysis. From the manual singer\ndiarization and collection of singer meta data, the beat level\nannotation is generated automatically, resulting in the num-\nber of beats and according classiï¬cation tasks shown in Ta-\nble 1. To foster further research on the challenging topics\nintroduced in this paper, the annotation (singer meta-data,\nvoice alignments, song list with recording dates and parti-\ntioning) is made publicly available for research purposes at\nhttp://www.openaudio.eu.\n3. MONAURAL SOURCE SEPARATION METHODS\nA major part of our experiments is devoted to ï¬nding the\noptimal preprocessing by source separation for recognition\nof vocalist gender, age, height and race. To this end, we in-\nvestigate harmonic enhancement as in [8,17] and extraction\nof the leading voice as in [3], as well as a combination of\nboth.\n3812th International Society for Music Information Retrieval Conference (ISMIR 2011)\nFemaleMale\n0100200300400(a) gender\nAsianHispanicBlackWhite\n0100200300400 (b) race\nâ—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â— â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â— â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â— â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â— â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â— â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â— â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—\n10 20 30 40 50 60\n(c) age [years]\n150 160 170 180 190 (d) height [cm]\nFigure 1 : Distribution of gender, race, and height among 516 singers in the UltraStar Singer Trait Database. Distribution of\nage is shown on beat level, since it is dependent on recording date.\n3.1 Enhancement of Harmonic Parts\nEnhancement of harmonic parts is performed following\n[17]. It is based on a non-negative factorization of the mag-\nnitude spectrogram obtained by Short-Time Fourier trans-\nform (STFT) that is computed using a multiplicative up-\ndate algorithm for NMF minimizing the Kullback-Leibler\ndivergence. We then use a Support Vector Machine (SVM)\nto discriminate between components (spectra and their\ntime-varying gains) corresponding to percussive or non-\npercussive signal parts. The classiï¬er is trained on a man-\nually labeled set of NMF components extracted from popu-\nlar music as described in [15]. The features for discrimina-\ntion of drum and harmonic components exactly correspond\nto those used in [15]. For straightforward reproducibility\nof our experiments, we used the default parameters of the\npublicly available1drum beat separation demonstrator of\nthe source separation toolkit openBliSSART [18]: frame\nrate 30 ms, window size 60 ms, and 100 iterations. 50 NMF\ncomponents are used; for 20 components thereof, the spec-\ntral shape wis pre-initialized from typical drum spectra de-\nlivered with the openBliSSART demonstrator. To allow the\nalgorithm to use different sets of components for the indi-\nvidual sections of a song, chunking into frame-synchronous\nnon-overlapping chunks is performed as in [17].\n3.2 Leading Voice Separation\nThe second method used to facilitate singer trait identiï¬-\ncation is the leading voice separation approach described\nin [2, 3]. In this model, the STFT of the observed signal\nat each frame is expressed as the sum of STFTs of vocal and\nbackground music signals. These are estimated by an unsu-\npervised approach: The voice STFT is modelled as product\nof source (periodic glottal pulse) and ï¬lter STFTs while no\n1http://openblissart.github.com/openBliSSARTspeciï¬c constraints are set for the background music signal\nbecause of its wide possible variability. The estimation of\nthe various model parameters is then conducted by iterative\napproaches based on NMF techniques following a two step\nstrategy. The ï¬rst step provides an initial estimate of the pa-\nrameters while the second step is a constrained re-estimation\nstage which reï¬nes the leading melody estimation and in\nparticular limits sudden octave jumps that may remain after\nthe ï¬rst estimation stage. To ensure best reproducibility of\nour results, we used an open-source implementation2of the\nalgorithm with default parameters. Chunking was applied as\nin [17].\n3.3 Combined Source Separation Approaches\nWhen the algorithm described in the last section is applied\nto popular music, it turns out that part of the drum track\nmay remain after separation. Hence, for this study, we con-\nsidered cascading of both separation techniques: harmonic\nenhancement after leading voice separation (LV-HE), and\nvice versa (HE-LV). Thereby time domain signals are syn-\nthesized inbetween the two separation stages, in order to be\nable to use different NMF parameterizations for both algo-\nrithms.\n4. EXPERIMENTAL SETUP\n4.1 Acoustic Features\nThe features exactly correspond to those used in [15] and\nwere extracted for each beat using the open-source toolkit\nopenSMILE [5]. We consider the short-time energy, zero-,\nand mean-crossing rate known to indicate vocal presence. In\naddition we extract values from the normalized autocorrela-\ntion sequence of the DFT coefï¬cients, namely voicing prob-\nability, F-zero and harmonics-to-noise ratio (HNR). F-zero\n2Software available at http://www.durrieu.ch/phd/software.html\n39Oral Session 1: Melody and Singing\nis the location of the highest peak of the autocorrelation se-\nquence aside from the maximum at zero. HNR is computed\nby the value of this peak. Pitch and voice quality parame-\nters have been successfully used in paralinguistic informa-\ntion assessment from speech [16]. We further calculate Mel\nfrequency cepstral coefï¬cients (MFCC) 0â€“12 and their re-\nspective ï¬rst-order delta regression coefï¬cients which are\nknown to capture the characteristic qualities of individual\nvoices for singer identiï¬cation [12]. Thus, altogether we\nemploy a set of 46 time-varying features. The employed\nconï¬guration of the openSMILE toolkit is provided for fur-\nther reproducibility at http://www.openaudio.eu.\n4.2 Classiï¬cation by BLSTM-RNN\nAs in [17], sequence classiï¬cation with Bidirectional Long\nShort-Term Memory (BLSTM) recurrent neural networks\n(RNNs) has been observed greatly superior to beat-wise\nstatic classiï¬cation by SVMs or Hidden Naive Bayes on the\nvocalist gender recognition task (90.77 % beat level accu-\nracy on original signals vs. 72.78 % resp. 76.17 %), we opt\nfor this classiï¬er for our study. BLSTM-RNNs unite the\nconcept of bidirectional RNNs (BRNNs) with Long Short-\nTerm Memory (LSTM) [7]. BRNNs use two separate hid-\nden layers instead of one, both connected to the same input\nand output layers, of which the ï¬rst processes the input se-\nquence forwards and the second backwards. The network\ntherefore always has access to the complete past and the fu-\nture context in a symmetrical way. Consequently, it must\nhave the complete input sequence at hand before it can be\nprocessed; however, this is not a restriction in the context of\nour application. In short, the LSTM concept allows the net-\nwork to access potentially unlimited range of context, and\nto learn when to store, use, or discard information acquired\nfrom previous inputs or outputs. This makes (B)LSTM-\nRNNs useful for sequence classiï¬cation tasks where the re-\nquired amount of context is unknown a priori.\n4.3 BLSTM Topology and Training\nWe trained individual BLSTM networks for each classiï¬-\ncation task. As in [17], the networks had one hidden layer\nwith 80 LSTM memory cells for each direction. The size\nof the input layer was equal to the number of features (46),\nwhile the size of the output layer was equal to the number\nof classes to discriminate (2â€“3). Its output activations were\nrestricted to the interval [0; 1] and their sum was forced to\nunity by normalizing with the softmax function. Thus, the\nnormalized outputs represent the posterior class probabil-\nities. The songs in the test set were presented frame by\nframe (in correct temporal order) to the input layer, and each\nframe was assigned to the class with the highest probability\nas indicated by the output layer. For network training, su-\npervised learning with early stopping was used as follows:We initialized the network weights randomly from a Gaus-\nsian distribution ( Âµ= 0,Ïƒ= 0.1). Then, each sequence\n(song) in the UltraStar training set was presented frame by\nframe to the network. To improve generalization, the or-\nder of the input sequences was determined randomly, and\nGaussian noise ( Âµ= 0,Ïƒ= 0.3) was added to the input\nactivations. The network weights were iteratively updated\nusing resilient propagation [14]. To prevent over-ï¬tting, the\nperformance (in terms of classiï¬cation error) on the valida-\ntion set was evaluated after each training iteration (epoch).\nOnce no improvement over 20 epochs had been observed,\nthe training was stopped and the network with the best per-\nformance on the validation set was used as the ï¬nal network.\nAs the race recognition problem is particularly unbalanced,\nslight modiï¬cations were employed for the training proce-\ndure: A ï¬xed number of 20 epochs was run to avoid over-\nï¬tting to the validation set, and the standard deviation of\nthe Gaussian noise on the input activations was increased to\nÏƒ= 0.9.\n5. RESULTS\nOur primary measure for evaluating performance of au-\ntomatic singer trait recognition is unweighted accuracy\n(UA)â€”i. e., the average recall of the classesâ€”on beat level.\nDue to class imbalance (Table 1) it represents the discrim-\nination power of the classiï¬er more closely than â€˜conven-\ntionalâ€™ weighted accuracy (WA) where recalls of the classes\nare weighted with their a-priori probabilities. Note that both,\nrandom guessing or always picking the majority class would\nachieve a UA of 33.33 % in ternary and 50.00 % in binary\nclassiï¬cation tasks.\n5.1 Results on Beat Level\nIn order to highlight the difï¬culty of the evaluated singer\ntrait recognition tasks in full realism, we ï¬rst evaluated the\nBLSTM-RNN on the task to recognize the presence of a\nsinger. It turns out that this can be done with over 75 %\nUA when using the leading voice extraction â€“ note that this\nalgorithm usually extracts the leading instrument when no\nvoice is present, hence the task remains non-trivial. Best re-\nsults on the 2-class gender recognition task are obtained by\nthe proposed combination of source separation algorithms\n(LV-HE, 89.61 % UA) while in the 3-class task, best UA is\nachieved by the LV algorithm alone (69.29 % UA). Notably,\nthis is higher than it would be expected if accuraries of voice\nactivity and 2-class gender recognition were independent. 2-\nclass recognition of race delivers up to 63.30 % UA when\nincluding HE preprocessing, while LV alone downgrades\nperformance compared to the original. Furthermore, we ob-\nserve that height recognition can be robustly performed at\nup to 72.07 % UA when using HE-LV preprocessing, which\nboosts the UA by over 7 % absolute compared to no pre-\n4012th International Society for Music Information Retrieval Conference (ISMIR 2011)\nPreprocessing\n[%] â€“ HE LV LV-HE HE-LV\ntask classes UA WA UA WA UA WA UA WA UA WA\nvoice 0/1 74.55 74.50 73.82 73.84 75.77 75.81 75.40 75.41 75.09 75.11\ngender 0/m/f 63.75 68.54 65.65 68.91 69.29 71.31 67.90 70.41 68.52 70.44\nm/f 86.67 91.09 88.45 91.91 86.93 91.12 89.61 93.60 87.76 92.50\nrace 0/w/b+h+a 48.17 63.84 47.46 63.02 49.37 65.46 49.23 63.63 48.40 63.77\nw/b+h+a 60.44 65.82 63.30 76.98 55.05 76.18 62.57 78.67 62.78 75.16\nage 0/y/o 51.02 57.61 50.00 57.14 53.50 59.85 51.26 58.86 50.01 57.72\ny/o 55.30 55.60 57.55 56.56 53.93 53.63 55.97 54.89 54.69 54.17\nheight 0/s/t 53.94 66.79 52.35 66.57 58.15 69.30 57.67 68.41 58.91 69.53\ns/t 64.70 72.73 62.31 70.67 66.54 73.00 69.65 77.49 72.07 78.26\nTable 2 : Beat-wise BLSTM-RNN classiï¬cation of UltraStar test set on 2- and 3-class tasks. Preprocessing: HE = harmonic\nenhancement (Section 3.1); LV = leading voice extraction (Section 3.2); LV-HE: HE after LV; HE-LV: LV after HE.\nPreprocessing\n[%] â€“ HE LV LV-HE HE-LV\ntask vote on UA WA UA WA UA WA UA WA UA WA\ngender 0/m/f 80.9 87.0 81.7 85.6 87.7 90.9 91.3 92.4 87.7 90.9\nm/f 86.9 90.1 89.0 90.9 87.7 90.9 89.6 93.9 89.6 93.9\nrace 0/w/b+h+a 49.8 78.8 53.5 79.7 51.0 78.2 54.0 75.2 48.9 72.2\nw/b+h+a 52.8 59.8 62.6 75.9 54.7 73.7 64.4 78.9 61.7 74.4\nage 0/y/o 55.2 54.5 54.6 54.1 56.0 54.1 56.9 57.4 50.9 51.6\ny/o 54.5 54.5 57.0 55.7 52.2 51.6 53.4 52.5 58.9 58.2\nTable 3 : Song-wise BLSTM-RNN predictions on UltraStar test set by beat-wise majority vote. V ote among 3-class tasks\n(ignoring beats not classiï¬ed as 0) or 2-class tasks. Height is not included due to the low number of songs (88) with known\nground truth. Preprocessing as in Table 2.\nprocessing. Finally, up to 57.55 % UA are achieved in age\nrecognition when using HE; while this is clearly below typ-\nical results on spoken language, it is signiï¬cantly above\nchance level (50 % UA) according to a z-test ( p<. 001).\n5.2 Results on Song Level\nAs a performance estimate for â€˜taggingâ€™ entire songs, we\ncalculated for each scenario the accuracies of majority vote\non beat level compared with the most frequent ground truth\nclass on beat level. Note that such measurements are more\nheuristic in nature, since a song level ground truth cannot\nalways be established due to typical phenomena in real-life\nmusic such as alternating male / female vocalists. To brieï¬‚y\ncomment on the results, song level gender can be recognized\nwith up to 91.3 % UA, race with 64.4 % UA and age with\n58.9 % UA. For gender, estimation from the vote on all(not\nonly voiced) beats seems to be even more robust than votes\non the 2-class beat level task. LV-HE preprocessing delivers\noverall best results.5.3 Discussion and Outlook\nForrace, an interdependency with genre could be assumed;\nhowever, the fact that source separation generally improves\nthe result over the original music suggests that genre is not\nthe primary information learned by the classiï¬er. Further-\nmore, genres typically associated with non-white singers\nsuch as hip hop are very sparsely represented in the Ul-\ntraStar database, which is originally intended for karaoke\napplications. Still, the very robust recognition of height is\nclearly correlated with robust gender identiï¬cation, as tall\nfemale singers are sparse in the considered data set.\nCompared to â€˜usualâ€™ results obtained on spoken lan-\nguage, accuracies of agerecognition are rather low; the task\nseems to be especially challenging on the considered type of\nâ€˜chartâ€™ popular music with a prevalence of singers in their\ntwenties. At least, when using gender-dependent models for\nage, 61.63 % UA could be achieved for males; for females\nthere is not enough training data.\nA promising direction for further research may be to in-\nvestigate different units of analysis, such as longer-term sta-\ntistical functionals that are commonly used in paralinguistic\ninformation retrieval from speech [16], instead of recogni-\n41Oral Session 1: Melody and Singing\ntion at the beat level. Still, this is not fully straightforward\ndue to the feature variation, especially for pitch, which will\nnecessitate methods for robust pitch estimation and transfor-\nmation.\n6. CONCLUSIONS\nInspired by previous successful studies on vocalist gender\nrecognition, we introduced fully automatic assessment of\nnew paralinguistic traits (age, height and race) in a large col-\nlection of recorded popular music. While we could also im-\nprove gender recognition close to perfection even on beat\nlevel (up to 93.60 % WA on unseen test data), foremost\nwe have shown feasibility of race and height classiï¬cation\nin full realism. Even in chart music with a prevalence of\nsingers from 20â€“30 years, age recognition could be per-\nformed signiï¬cantly above chance level; still, when aiming\nat real-life applications new directions in research must be\ntaken.\nFuture work should primarily focus on more variation in\ndata (particularly concerning age and race) by not only in-\ncluding chart music, but also jazz and non-Western music.\nFurthermore, we will investigate multi-task learning to ex-\nploit singer trait interdependencies in learning.\n7. ACKNOWLEGMENT\nThe research leading to these results has been partly funded\nby the German Research Foundation through grant no.\nSCHU 2580/2-1. The authors would like to thank Ga Â¨el\nRichard and Jean-Louis Durrieu for their highly valuable\ncontributions.\n8. REFERENCES\n[1] F. Burkhardt, R. Huber, and A. Batliner. Application\nof speaker classiï¬cation in human machine dialog sys-\ntems. In Christian M Â¨uller, editor, Speaker Classiï¬cation\nI: Fundamentals, Features, and Methods , pages 174â€“\n179. Springer, 2007.\n[2] J.-L. Durrieu, G. Richard, and B. David. An iterative ap-\nproach to monaural musical mixture de-soloing. In Proc.\nof ICASSP , pages 105â€“108, Taipei, Taiwan, 2009.\n[3] J.-L. Durrieu, G. Richard, B. David, and C. F Â´evotte.\nSource/ï¬lter model for unsupervised main melody ex-\ntraction from polyphonic audio signals. IEEE Trans-\nactions on Audio, Speech, and Language Processing ,\n18(3):564â€“575, 2010.\n[4] S. Evans, N. Neave, and D. Wakelin. Relationships be-\ntween vocal characteristics and body size and shape\nin human males: An evolutionary explanation for a\ndeep male voice. Biological Psychology , 72(2):160â€“\n163, 2006.[5] F. Eyben, M. W Â¨ollmer, and B. Schuller. openSMILE â€“\nthe Munich versatile and fast open-source audio feature\nextractor. In Proc. of ACM Multimedia , pages 1459â€“\n1462, Florence, Italy, October 2010. ACM.\n[6] D. Gillick. Can conversational word usage be used to\npredict speaker demographics? In Proc. of Interspeech ,\npages 1381â€“1384, Makuhari, Japan, 2010.\n[7] A. Graves. Supervised sequence labelling with recur-\nrent neural networks . PhD thesis, Technische Univer-\nsitÂ¨at M Â¨unchen, 2008.\n[8] M. Hel Â´en and T. Virtanen. Separation of drums from\npolyphonic music using non-negative matrix factoriza-\ntion and support vector machine. In Proc. of EUSIPCO ,\nAntalya, Turkey, 2005.\n[9] M. Jessen. Speaker classiï¬cation in forensic phonetics\nand acoustics. In C. M Â¨uller, editor, Speaker Classiï¬ca-\ntion I , volume 4343, pages 180â€“204. Springer Berlin /\nHeidelberg, 2007.\n[10] R. M. Krauss, R. Freyberg, and E. Morsella. Inferring\nspeakers physical attributes from their voices. Journal of\nExperimental Social Psychology , 38(6):618â€“625, 2002.\n[11] A. Mesaros and T. Virtanen. Automatic recognition of\nlyrics in singing. EURASIP Journal on Audio, Speech,\nand Music Processing , 2009. Article ID 546047.\n[12] A. Mesaros, T. Virtanen, and A. Klapuri. Singer identi-\nï¬cation in polyphonic music using vocal separation and\npattern recognition methods. In Proc. of ISMIR , pages\n375â€“378, 2007.\n[13] I. Mporas and T. Ganchev. Estimation of unknown\nspeakers height from speech. International Journal of\nSpeech Technology , 12(4):149â€“160, 2009.\n[14] M. Riedmiller and H. Braun. A direct adaptive method\nfor faster backpropagation learning: the RPROP algo-\nrithm. In Proc. of IEEE International Conference on\nNeural Networks , pages 586â€“591, 1993.\n[15] B. Schuller, C. Kozielski, F. Weninger, F. Eyben, and\nG. Rigoll. V ocalist gender recognition in recorded pop-\nular music. In Proc. of ISMIR , pages 613â€“618, Utrecht,\nNetherlands, August 2010.\n[16] B. Schuller, S. Steidl, A. Batliner, F. Burkhardt, L. Dev-\nillers, C. M Â¨uller, and S. Narayanan. The INTER-\nSPEECH 2010 Paralinguistic Challenge. In Proc. of\nINTERSPEECH , pages 2794â€“2797, Makuhari, Japan,\nSeptember 2010. ISCA.\n[17] F. Weninger, J.-L. Durrieu, F. Eyben, G. Richard,\nand B. Schuller. Combining Monoaural Source Separa-\ntion With Long Short-Term Memory for Increased Ro-\nbustness in V ocalist Gender Recognition. In Proc. of\nICASSP , Prague, Czech Republic, 2011.\n[18] F. Weninger, A. Lehmann, and B. Schuller. openBliS-\nSART: Design and Evaluation of a Research Toolkit for\nBlind Source Separation in Audio Recognition Tasks. In\nProc. of ICASSP , Prague, Czech Republic, 2011.\n42"
    },
    {
        "title": "Adapting Metrics for Music Similarity Using Comparative Ratings.",
        "author": [
            "Daniel Wolff",
            "Tillman Weyde"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1416440",
        "url": "https://doi.org/10.5281/zenodo.1416440",
        "ee": "https://zenodo.org/records/1416440/files/WolffW11.pdf",
        "abstract": "Understanding how we relate and compare pieces of music has been a topic of great interest in musicology as well as for business applications, such as music recommender systems. The way music is compared seems to vary among both individuals and cultures. Adapting a generic model to user ratings is useful for personalisation and can help to better understand such differences. This paper presents an approach to use machine learning techniques for analysing user data that specifies song similarity. We explore the potential for learning generalisable similarity measures with two stateof-the-art algorithms for learning metrics. We use the audio clips and user ratings in the MagnaTagATune dataset, enriched with genre annotations from the Magnatune label.",
        "zenodo_id": 1416440,
        "dblp_key": "conf/ismir/WolffW11",
        "keywords": [
            "musicology",
            "business applications",
            "music recommender systems",
            "individuals",
            "cultures",
            "generic model",
            "user ratings",
            "personalisation",
            "understanding differences",
            "machine learning techniques"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nADAPTING METRICS FOR MUSIC SIMILARITY\nUSING COMPARATIVE RATINGS\nDaniel Wolff and Tillman Weyde\nDepartment of Computing\nCity University London\n{daniel.wolff.1, t.e.weyde }@soi.city.ac.uk\nABSTRACT\nUnderstanding how we relate and compare pieces of music\nhas been a topic of great interest in musicology as well as for\nbusiness applications, such as music recommender systems.\nThe way music is compared seems to vary among both indi-\nviduals and cultures. Adapting a generic model to user rat-\nings is useful for personalisation and can help to better un-\nderstand such differences. This paper presents an approach\nto use machine learning techniques for analysing user data\nthat speciï¬es song similarity. We explore the potential for\nlearning generalisable similarity measures with two state-\nof-the-art algorithms for learning metrics. We use the audio\nclips and user ratings in the MagnaTagATune dataset, en-\nriched with genre annotations from the Magnatune label.\n1. MOTIVATION\nIn the recent years, increased efforts have been made to\nadapt MIR techniques, especially for music recommenda-\ntion, to speciï¬c contexts or user groups. This is encouraged\nby developments in machine learning that make more algo-\nrithms applicable to accumulated user data, like user pref-\nerences or click-trough data for ranked search results, and\nenable the involvement of crowd wisdom into general clas-\nsiï¬cation and distance learning tasks. Moreover, the combi-\nnation of different information sources has been proven suc-\ncessful for improving music recommendation and for clas-\nsiï¬cation into cultural categories such as musical genres.\nThis paper shows the results of some experiments on learn-\ning a musical distance metric from user similarity compar-\nisons. Similarity models of mixed acoustic and tag features\nare trained using comparative user judgent data on song sim-\nilarities. We derive information of the form â€Song A is more\nsimilar to Song B than to Song Câ€, represented by binary\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proï¬t or commercial advantage and that copies\nbear this notice and the full citation on the ï¬rst page.\nc/circlecopyrt2011 International Society for Music Information Retrieval.rankings, which allows for the application of more generic\nalgorithms designed for learning from such data.\nAlthough the above type of rating data is not as readily\naccessible as customer preference or social network data, it\nprovides a valuable change of focus from general classiï¬ca-\ntion and recommendation success towards modelling musi-\ncal similarity and the usersâ€™ perception of it when engaged\nin a comparison task. Thus, instead of targeting a general\nrelevance criterion, the optimisation task tackled in the fol-\nlowing experiments addresses reported perceived similarity,\nwhich only constitutes one of the many variable aspects of\nrelevance. As distance measures we use Mahalanobis dis-\ntance metrics, which allow for a direct analysis as well as\nthe easy comparison of learning results [5], and therefore\nencourage evaluation from a musicological perspective.\n2. RELATED WORK\nThe distance metrics learning in this paper can be seen as\nan extension of feature selection techniques developed ear-\nlier in the MIR ï¬eld, regarding feature selection as a binary\nweighting of features. E.g., Dash and Liu [4] assembled a\ncomprehensive survey of general techniques for feature se-\nlection in classiï¬cation tasks. They pointed out attributes\nrelevant for diverse application scenarios, e.g. compabil-\nity considering dataset size, number of classes or robustness\nagainst noise. These attributes enable a systematic compar-\nison of the various approaches when given the parameters\nof a speciï¬c application. Pickens [13] categorised selection\ntechniques for music retrieval using symbolic data, calling\nfor special attention to featuresâ€™ musicological properties.\nA set-based method for learning a feature weightings was\napplied by Allan et al. [1]. Users could specify their per-\nceived similarity using two example song sets: one contain-\ning similar and one dissimilar songs. A detailed discussion\non how to generate a successful stimulus partitioning for\na survey involving comparison within triplets of clips sup-\nported the design of their Balanced Complete Block Parti-\ntioning.\n73Poster Session 1\n2.1 Optimising Recommendation via Metadata and\nUser Information\nOut of the many data sources available for music descrip-\ntion, genre annotations provide particularly valuable data for\nindexing and presenting music in recommendation settings.\nMusical genre has been used for the general evaluation of\nsimilarity measures, using the correlation of songsâ€™ genres\nand data clusters derived from the learned similarity [11,12].\nBarrington et al. showed a training of linear combina-\ntions of SVM kernels relating to similarity measurements on\nacoustic, tagging and web-mined annotation data, for build-\ning classiï¬ers for automatic annotation [3]. They also pro-\nvide relevance levels of the different feature types for differ-\nent tag classiï¬ers.\nA user-data based similarity measure for multimedia ob-\njects was introduced by Slaney [15]. Here, similarity of ob-\njects was based on users votings for them. Songs which fea-\nture the same grade of likeability by the same group of users\nwere considered similar. The resulting similarity measure\nwas evaluated via analysing artist consistency in rankings.\nInferring similarity from similar metadata sources as well as\nmusic blog titles, Slaney et al. evaluated the performance of\nseveral methods for learning a Mahalanobis distance met-\nric for music in [16]. McFee et al. [10] used the MLR al-\ngorithm (see below) for parametrising a content-based mu-\nsic similarity metric. A Mahalanobis metric was trained on\ncollected crowd data in form of rankings. This approach is\nvery similar to ours, but their emphasis has been on the need\nfor reliable content-based classiï¬ers for music discovery in\nsparsely annotated data.\nBade et al. [2] train a set of song-adaptive music similar-\nity msasures for folksongs, inferring training data from ex-\npert classiï¬cations: Several known similarity measures for\nthe symboloc music data and metadata are combined lin-\nearly via a weighted sum speciï¬c to the measured songs,\nits corresponding clusters or database. For optimisation,\nthe expert classiï¬cation information is transferred into rela-\ntive distance statements enforcing the class members to be\nnearer than songs from foreign classes.\n2.2 Metric Learning from Comparative Ratings\nMany common algorithms for metric learning use class an-\nnotations and nearest neighbour classiï¬cations for optimis-\ning and evaluating metrics [18]. As we intend to learn music\nsimilarity from relative comparisons, such approaches are\ndifï¬cult to apply considering the missing ground truth data\nfor clusters of perceptually similar music pieces or equiva-\nlents.\nBased on a framework for Support Vector Machines,\nSchultz and Joachims [14] presented an optimisation using\nrelative constraints we apply on the task of music similarity\nlearning. Davis et al. formulated a metric learning prob-lem as an LogDet optimisation task [5]. In this case, a\nfully parametrised Mahalanobis metric was learned, allow-\ning for a regularisation towards another predeï¬ned Maha-\nlanobis metric.\nMcFee et al. have designed an algorithm for learning a\nMahalanobis metric to rankings (MLR) [9]. In our experi-\nments, MLR is applied to learning a distance metric on mu-\nsic, using the implementation provided by the authors. In\ntheir publication mentioned above [10], this algorithm has\nbeen adapted to enable learning from collaborative ï¬ltering\ndata.\n3. THE MAGNATAGATUNE DATABASE\nThe MagnaTagATune database combines the results of a\nweb-based game called â€TagATuneâ€ together with the mu-\nsic clips used therein and extracted audio features [7]. These\nroughly 30-second long clips are provided by the Magnatune\nonline music label on a creative commons license. Mag-\nnatune has labelled the clips in this database with 44 genre-\ntags, which are not mutually exclusive. The majority of the\ndata can be divided into four disjoint main groups using the\ngenre tags â€classicalâ€, â€electronicaâ€, â€worldâ€ and â€rockâ€,\neach containing more than 17% of the total number of clips.\nThe MagnaTagATune game is a collaborative online game\nwith two modes: a regular mode for collecting tags and a\nbonus mode for collecting similarity ratings.\n3.1 Captured Similarity Ratings\nWe extract relative similarity information from data collected\nduring the â€bonusâ€ mode of the â€TagATuneâ€ game. In that\nmode, two players earn points if they vote the same clip as\nthe outlier out of three clips provided [8]. All votes made\n(matching or not) are saved into a histogram hi={ha,hb,hc}\nâˆˆHfor that triplet of songs. 533 such histograms are in-\ncluded in the MagnaTagATune database, describing the vote\ndistribution (between 1 and 153 votes per triplet, 14 on av-\nerage). Not counting permutations of triplets, there are 346\nunique triplets comprising 1019 unique clips. Many his-\ntograms do not show a clear agreement on one outlier. This\nmay be caused by the diverse nature of the clips, causing\ntriplets normally to range over various genres, as discussed\nin [11]. However, many other variables like usersâ€™ cultural\nbackgrounds can equally affect their decisions. Content is\nhomogeneously distributed throughout the complete 25863-\nclip database, but the small number of triplets available and\nthe varying number of permutations do not allow for choos-\ning a suitable subset featuring a Balanced Block Partition-\ning. This has been pointet out as important in [1] to obtain a\nrelatively unbiased survey data set.\nThe above data was transferred into a ranking represen-\ntation like in [9]. Treating the histograms as votings on the\nsimilarity between the outlier and the other clips, for each\n7412th International Society for Music Information Retrieval Conference (ISMIR 2011)\nclipCa, a setrs\naof similar and, respectively, dissimilar clips\nrd\nawas calculated.\nrs\na={b|âˆƒhiâˆˆH:ha<hcâˆ§hb<hc} (1)\nrd\na={c|âˆƒhiâˆˆH:ha<hcâˆ§hb<hc} (2)\nThe complete set of derived rankings is then given by\nO=/braceleftBig\n(rs\na,rd\na)|âˆƒCaâˆ§rs\na,rd\na/negationslash=âˆ…âˆ§rd\naâˆ©rs\na=âˆ…/bracerightBig\n.(3)\nInconsistent rankings with rd\naâˆ©rs\na/negationslash=âˆ…were excluded to\nenable the following training process. In order to use the\ndata with other algorithms, we removed further triplets1.\nAll but 12 of the resuting rankings contain a single clip on\neach side:|rd\na|=|rs\na|= 1. This resulted in 533 rankings.\n3.2 Feature Generation\nThe MagnaTagATune dataset comes with precalculated fea-\ntures for all clips extracted by the â€The Echo Nestâ€ API\n1.0, via the â€analyseâ€ interface. These features are also in-\ncluded in other online databases such as the Million Song\nDataset2. This also allows for a wider application of the\nfeature extraction procedure detailed below and facilitates\ncomparability with other studies. Of the wide feature range\nprovided3, we only use the chroma and timbre informa-\ntion. The chroma and timbre features are sampled on a non-\nuniform time scale. In order to aggregate to the clip level,\nwe use a k-means based algorithm to extract n= 4 cluster\ncentres for both of these features. In order to keep the fea-\ntures invariant to key, whilst preserving the harmonic and\nstructural information, the chroma features are then trans-\nposed to ï¬t the main key as estimated in the provided fea-\ntures, in the ï¬rst chroma bin. This is achieved using a cir-\ncular shift on the nchroma mean vectors. The resulting\nshifted chroma mean vectors are now separately normalised\nto a maximum value of 1.\nThe timbre features provided within the dataset very much\nresemble the output of a 2-dimensional convolution with\n12 different ï¬lters, corresponding to characteristic spectral\nshapes. After clustering the timbre data to n= 4mean vec-\ntors, these are scaled and clipped to retain 85% of the data\nwithin the interval of [0, 1] for the set of the 1019 clips.\nAdditionally, the cluster weights for each of the included\nchroma and timbre cluster centroids are included in the fea-\ntures.\n3.2.1 Genre Features\nThese acoustic features are enriched using the genre tags as-\nsigned by the Magnatune label. This way, up to four genre\n1Two histograms{ha, hb, hc}and{ha, hb, hd}were removed if they\ndid not agree on the outlier, except if the outliers were candd.\n2http://labrosa.ee.columbia.edu/millionsong/\n3http://developer.echonest.com/docs/v4/ static/AnalyzeDocumentation 2.2.pdftags are assigned to each of the clips. For each clip, a binary\n44-dimensional vector indicates the annotation according to\nthe tags found for all of the clips in the dataset. The combi-\nnation results in one feature vector xiâˆˆ(Râˆ©[0,1])148per\nclipCi,iâˆˆ{1,Â·Â·Â·,1019}.\n4. LEARNING SIMILARITY FROM\nCOMPARISONS\nThe distance measure d(xi,xj)we intend to optimise using\nthe following algorithms is deï¬ned on the clip level. Gen-\nerally, our approach and the corresponding features are in-\ntended to model a perceived distance, assumed to resemble\nthe inverse similarity of two songs.\nThe ranking data in the following experiments has been\napproximated as a consensus from decisive triplet histograms,\nand is therefore simpler, e.g. contains fewer contradictory\nelements than the original data. Concerning the gathering of\nthe histograms themselves, the authors of [1] emphasise that\nboth the representation and especially the selection of com-\nbinations of the rated stimuli, in this case the clips, presented\nto the users, affect the balance of the resulting ratings. They\nonly accept a set containing all possible triplet combinations\nof a set of stimuli for an unbiased test. Unfortunately, the\ntriplets contained in the MagnaTagATune comparison data\nand the resulting ratings riare unbalanced. This may well\ninclude a bias caused by the speciï¬c constellation of graph-\nical and acoustical presentations.\nUsing a metric for modelling song similarity implies sev-\neral assumptions. These assumptions have already been ques-\ntioned by Tversky [17], arguing that perceived similarity\nis not necessarily a linear, positive deï¬nite and symmet-\nric function, which satisï¬es the triangle inequality. Instead,\nperceived similarity, in many circumstances, is assumed be\ndirectional, considering speciï¬c functions of the objects in\ncomparison, e.g. prototype and referent.\nHowever, the properties of a metric support efï¬cient and\nrobust learning algorithms for dealing with the highly sparse\nand often contradictory data involved in learning the song\nsimilarity. Also, metrics have a straightforward geomet-\nric interpretation. Thus, besides the comparison of songs,\nframeworks are available for comparing the metrics them-\nselves. We now give a quick overview of the family of met-\nrics used in our experiments before we focus on the way\nthey are used in Section 5.\n4.1 Mahalanobis distances\nThe two algorithms summarised below are designed to learn\nparametrised distance functions. These functions are special\ncases of Mahalanobis distances, which are deï¬ned as\ndW(xi,xj) =/radicalBig\n(xiâˆ’xj)TW(xiâˆ’xj), (4)\n75Poster Session 1\nwherexi,xjâˆˆRNandWâˆˆRNÃ—N.\nTo qualify as a metric, Whas to be positive deï¬nite\n[19]. The algorithms we use only guarantee Wto be posi-\ntive semideï¬nite. The corresponding distance functions still\nsatisfy the conditions of symmetry, non-negativity and the\ntriangle inequality, but allow for dW(xi,xj) = 0 whilst\nxi/negationslash=xjand therefore are called pseudometrics. This func-\ntion is the Euclidean metric if Wis the unit matrix. As\ndetailed below, a Mahalanobis distance can be described as\na weighted Euclidean distance applied to previously linearly\ntransformed features.\n4.2 SC03\nIn [14], Matthew Schultz and Thorsten Joachims present an\nSVM approach to learning a distance metric. The function\nlearned here is parametrised by two matrices, a linear trans-\nformationAand the positive semideï¬nite W. For our exper-\niments,A=Icontains the identity transformation and W\nis constrained to be a diagonal matrix. Thus dWdescribes a\nweighted Euclidean distance metric.\nIn order to use the usersâ€™ similarity data rd\niandrs\ni, the\nrankings are converted into singular similarity statements of\nthe form (a,b,c), where the clip Cais more similar to Cbthan\ntoCc. This leads to the following set of triplet constraints:\nQ=/braceleftBig\n(a,b,c )|âˆƒ(rs\na,rd\na)âˆˆO:bâˆˆrs\naâˆ§câˆˆrd\na/bracerightBig\n(5)\nFor each training triplet (a,b,c ), Schultz et al. consider\nthe squared pointwise difference âˆ†xi,xjof the transformed\nclipsâ€™ features, which in this application case reduces to\nâˆ†xi,xj= (xiâˆ’xj)Â·(xiâˆ’xj)(note the point-wise product).\nThe weighted differences of\nâˆ†âˆ†\n(a,b,c)= (âˆ†xa,xcâˆ’âˆ†xa,xb) (6)\nare then used as constraints for the following optimisation\nproblem (with w=diag(W)):\nmin\nw,Î¾1\n2wTw+cSC03Â·/summationdisplay\nabcÎ¾abc (7)\ns.t.âˆ€(a,b,c )âˆˆQtrain :wTâˆ†âˆ†\n(a,b,c)â‰¥1âˆ’Î¾abc\nwi,jâ‰¥0, Î¾abcâ‰¥0.\nThis minimises the loss deï¬ned by the sum of the slack\nvariablesÎ¾abc, whilst regularising Wusing the Frobenius\nnorm with1\n2/bardblW/bardbl2\nF. We used the SVMlightC++ implemen-\ntation4to minimise the above term. The software returns\nwin form of its support vector expansion, containing the\nsupport (difference) vectors âˆ†âˆ†\niof the corresponding hy-\nperplane and their weights Î±iyi.wcan be easily retrieved\nusingw=/summationtextn\ni=1Î±iyiâˆ†âˆ†\ni.\n4http://svmlight.joachims.org/4.3 Metric Learning to Rank\nIn [9], McFee et al. describe an algorithm for learning a\nfullly parametrised Mahalanobis distance (see Equation (4))\nusing ranking information. Presenting an algorithm based\non Structural SVM, they compute Wwhilst assuring the\nmargin between the given training rankings and possible dif-\nferent rankings of the training data [10]. This method uses\nbinary rankings and evaluates results by the relative posi-\ntioning of clips marked as relevant or irrelevant. A fully cor-\nrect ranking positions the relevant clips rs\nabefore the ones\ninrd\na. The calculation of the associated loss involves stan-\ndard IR measures for estimating the ranking loss, e.g. the\narea under ROC curve. For selecting the most effective con-\nstraints, a cutting-planes method [6] is used. Note that clips\nnot named in the rankings stay neutral and have no effect on\nthe loss.\nThe MATLAB R/circlecopyrtimplementation of the MLR framework,\navailable online5, provides several options for choosing the\ncutting-planes method and loss function. In the experiments\nbelow, we selected the AUC-related methods for simplicity.\nIn the literature, Wis regularised by its trace tr(W), but\nthe implementation provided by McFee also allows to use a\nsquared Frobenius norm, similar to the quadratic regularisa-\ntion in (7).\n5. EXPERIMENTS\nAll experiments were performed using ï¬ve-fold cross-vali-\ndation on the rankings. The ranking set Owas divided into\nï¬ve disjoint batches of 106 or 107 rankings, respectively.\nEach batch was used once as a test set against the remaining\nfour batches combined as training set. For smaller sized\ntraining sets, subsets were picked randomly from each of the\ntraining batches. The size of the test sets was kept constant\nfor all training set sizes.\nWe tested three different variations of learning metrics:\nSC03 for learning a weighted Euclidean distance, MLR for\ncalculating a full Mahalanobis matrix, and MLR with W\nconstrained to be diagonal. The slack-loss / regularisation\ntrade-off factors cwere set to cmlr= 10000 for both the\ndiagonal and the full- WMLR, andcSC03= 100 for the\nSC03 algorithm (Section 4.2). The squared Frobenius norm\nwas used for regularising Win all experiments. These pa-\nrameters were determined in earlier experiments using the\npresent dataset with non-reduced training sets.\nFor evaluation, we compare the rankings in the ground\ntruth with rankings induced by the learned distance func-\ntions. We also tested an unweighted Euclidean distance met-\nric as a baseline. As we deal with binary rankings as de-\nscribed in Section 3.1, any ranking featuring the clips in rs\na\nbefore the ones in rd\nafor a query clip aqualiï¬es as correct,\n5http://cseweb.ucsd.edu/Ëœbmcfee/code/mlr/\n7612th International Society for Music Information Retrieval Conference (ISMIR 2011)\nthe absolute ranking positions were not taken into account.\n5.1 Results\nFigure 1 shows the results for running the above conï¬gu-\nration on the features described in Section 3.2. The upper\nplot (a) shows the percentage of correctly induced rankings\nfor the three metric learning approaches as well as the re-\nsults for an unmodiï¬ed Euclidean metric, serving as base-\nline. With 81.81% correctly reproduced test rankings and a\nstandard deviation of 4.78% over the ï¬ve test sets, the fully\nparametrised MLR-trained distance produces the best re-\nsults, followed by the diagonal-MLR (71.85%, 2.69%) and\nSC03 (69.61%, 4.27%), barely superceeding the baseline of\n67.74%. Both of the diagonal- Wmethods score rather low\ncompared to the MLR-trained metric. Although the number\nof variables to determine is rather high, given the feature\ndimensionality, MLR proves successful in ï¬nding the best\nsolution, except for the training with less than 50 rankings.\n(a)\n0 100 200 300 4006065707580% test cnstr. satisfied\n(b)\n0 100 200 300 4008090100% train cnstr. sat.\nnumber of training constraints\nFigure 1 . Results for increasing training set size. Plotted are\nthe mean percentages of fulï¬lled rankings. MLR algorithm\n(â—¦), MLR with diagonal W(/triangleleft), and SC03 (+). The perfor-\nmance of the Euclidean metric is represented by a straight\nline.\nSC03 performs worst in this comparison, even dropping\nbelow the baseline during the medium-sized test-sets. As\ncan be seen in Figure 1(b), SC03 performs much better than\nthe diagonal MLR on the training set. This suggests an over-\nï¬tting of SC03 and possibly insufï¬cient inï¬‚uence of the reg-\nularisation loss. Overï¬tting depends strongly on the choice\nofcSC03. The fact that the more ï¬‚exible fully parametrised\nMLR-trained distance metric shows more ï¬‚exibility towards\nthe satisfaction of training constraints appears intuitive\n(Figure 1(b)). Lesser so, the better generalisation, which\nmight be explained by the ability to spread the necessary ad-\njustments in the metric across many parameters compared tothe diagonally parametrised metrics.\n(a)\n0 100 200 300 400657075% test cnstr. sat.\n(b)\n0 100 200 300 4006080100% train cnstr. sat.\nnumber of training constraints\nFigure 2 . Results for increasing training set size using PCA\nfeatures. Labels are as above.\n5.1.1 PCA features\nFigure 2 shows the results of applying the metric learning to\na feature set that was reduced to 20 dimensions using Prin-\ncipal Component Analysis (PCA). As in the earlier experi-\nment, MLR scores best, with (76.94%, 3.1%). The degrada-\ntion may be attributed to the smaller number of parameters\n(WâˆˆR20Ã—20) available for adapting the metric. However,\nwhen analysing the weights for the single feature dimen-\nsions, the ordering (by absolute value of the eigenvalues)\nused for determining the relevant pca dimensions does not\ncorrespond to their inï¬‚uence on the rated similarity. Thus,\ninformation relevant for similarity is lost in these PCA re-\nduced features, which has been validated by the training of\nmetrics using more PCA coefï¬cients. In this experiment\nwith 20 coefï¬cients we compare the ranking of PCA coefï¬-\ncients, as determined by PCA data variance, with the rank-\ning of PCA coefï¬cients derived from the SC03 weighting.\nThey differ on average by more than 52% of the index range.\nWith the PCA features, the SC03 algorithm greatly im-\nproves in performance, 75.42% indicating a higher suitabil-\nity of the low-dimensional vector space. This time, a less\neffective enforcement of training constraints apparently en-\nables a better generalisation. In contrast, the diagonal MLR\nis less able to cope with the data. Especially for the train-\ning sets involving around 300 rankings, the decrease in per-\nformance on the test set can be explained by less consis-\ntent training sets leading to badly generalising metrics. The\nbaseline Euclidean metric achieves 66.97% of correct rat-\nings.\n77Poster Session 1\n6. DISCUSSION\nIn the present paper, we apply general algorithms for met-\nric learning to a music similarity modelling task Using sim-\nple and widely available features and comparative similar-\nity ratings, we demonstrated that a considerable proportion\nof the ratings can be effectively learned and reproduced us-\ning Mahalanobis distances. This corroborates the initial hy-\npothesis that the ratings sharing some concordant informa-\ntion. Whilst with both the original features and the low-\ndimensional PCA features the MLR algorithm shows supe-\nrior results, the diagonal matrix algorithms show compara-\nble generalisation abilities for the PCA features. However,\nPCA seems not suitable for reducing feature dimensionality\nin a musical similarity context. Instead, the metric leaning\ntechniques may hint on the necessary transformations and\non which features may be ommitted.\n6.1 Future Work\nDespite the sparse and sometimes contradictory nature of\nthe rankings derived from MagnaTagATune, we ï¬nd the our\nresults encouraging to develop more elaborate data sets for\nfurther experiments. Special attention will be given to the\nvariation of learned metrics when observing different cul-\nturally deï¬ned user groups. More research has to be done in\nthe development of specialised regularisation terms for met-\nric learning algorithms, e.g. allowing for a customised W\nas a regularisation target [5].\n6.2 Acknowledgements\nWe would like to thank Brian McFee and Thorsten Joachims\nfor their help with using their implementations.\n7. REFERENCES\n[1] H. Allan, D. M Â¨ullensiefen, and G. Wiggins. Method-\nological considerations in studies of musical similarity.\nIn8th International Conference on Music Information\nRetrieval , 2007.\n[2] Korinna Bade, J Â¨org Garbers, Sebastian Stober, Frans\nWiering, and Andreas N Â¨urnberger. Supporting folk-song\nresearch by automatic metric learning and ranking. In\nISMIR , pages 741â€“746, Kobe, Japan, October 2009.\n[3] L. Barrington, M. Yazdani, D. Turnbull, and G. Lanck-\nriet. Combining feature kernels for semantic music re-\ntrieval. In ISMIR , pages 614â€“619, 2008.\n[4] M. Dash and H. Liu. Feature selection for classiï¬cation.\nIntelligent Data Analysis , 1(1-4):131â€“156, 1997.\n[5] J. V . Davis, B. Kulis, P. Jain, S. Sra, and I. S. Dhillon.\nInformation-theoretic metric learning. In ICML , ICML\nâ€™07, pages 209â€“216, New York, NY , USA, 2007. ACM.[6] T. Joachims, T. Finley, and C.-N. J. Yu. Cutting-plane\ntraining of structural svms. Machine Learning , 77:27â€“\n59, October 2009.\n[7] E. Law, K. West, M. Mandel, M. Bay, and J. S. Downie.\nEvaluation of algorithms using games: the case of music\nannotation. In ISMIR , pages 387â€“392, October 2009.\n[8] M.I. Mandel and D. P. W. Ellis. A web-based game for\ncollecting music metadata. Journal of New Music Re-\nsearch , 37(2):151â€“165, 2008.\n[9] B. Mcfee and G. Lanckriet. Metric learning to rank. In\nICML , 2010.\n[10] L. McFee, B.and Barrington and G. Lanckriet. Learn-\ning similarity from collaborative ï¬lters. In ISMIR , pages\n345â€“350, 2010.\n[11] A. Novello, M. F. Mckinney, and A. Kohlrausch. Per-\nceptual evaluation of music similarity. In ISMIR , 2006.\n[12] E. Pampalk. Computational Models of Music Similarity\nand their Application in Music Information Retrieval .\nPhD thesis, Vienna University of Technology, Vienna,\nAustria, March 2006.\n[13] Jeremy Pickens. A survey of feature selection tech-\nniques for music information retrieval. In ISMIR , 2001.\n[14] M. Schultz and T. Joachims. Learning a distance metric\nfrom relative comparisons. In Advances in Neural Infor-\nmation Processing Systems (NIPS) . MIT Press, 2003.\n[15] M. Slaney. Similarity based on rating data. In ISMIR ,\n2007.\n[16] Malcolm Slaney, Kilian Q. Weinberger, and William\nWhite. Learning a metric for music similarity. In\nJuan Pablo Bello, Elaine Chew, and Douglas Turnbull,\neditors, ISMIR , pages 313â€“318, 2008.\n[17] A. Tversky. Features of similarity. Psychological Re-\nview, 84:327â€“352, 1977.\n[18] K. Q. Weinberger and L. K. Saul. Distance metric learn-\ning for large margin nearest neighbor classiï¬cation. J.\nMach. Learn. Res. , 10:207â€“244, 2009.\n[19] E. P. Xing, A. Y . Ng, M. I. Jordan, and S. Russell.\nDistance metric learning, with application to clustering\nwith side-information. In Advances in Neural Informa-\ntion Processing Systems 15 , pages 505â€“512. MIT Press,\n2002.\n78"
    },
    {
        "title": "A Two-Fold Dynamic Programming Approach to Beat Tracking for Audio Music with Time-Varying Tempo.",
        "author": [
            "Fu-Hai Frank Wu",
            "Tsung-Chi Lee",
            "Jyh-Shing Roger Jang",
            "Kaichun K. Chang",
            "Chun-Hung Lu",
            "Wen-Nan Wang"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1416782",
        "url": "https://doi.org/10.5281/zenodo.1416782",
        "ee": "https://zenodo.org/records/1416782/files/WuLJCLW11.pdf",
        "abstract": "Automatic beat tracking and tempo estimation challenging tasks, especially for audio music with timevarying tempo. This paper proposes a two-fold dynamic programming (DP) approach to deal with beat tracking with time-varying tempo. In particular, the first DP computes the tempo curve from the tempogram. The second DP identifies the optimum beat positions from the novelty and tempo curves. Experimental results demonstrate satisfactory performance for music with significant tempo variations. The proposed approach was submitted to the task of audio beat tracking in MIREX 2010 and was ranked no. 1 for 6 performance indices out of 10, for the dataset with variable tempo.",
        "zenodo_id": 1416782,
        "dblp_key": "conf/ismir/WuLJCLW11",
        "keywords": [
            "Automatic",
            "beat",
            "tracking",
            "tempo",
            "estimation",
            "challenging",
            "tasks",
            "especially",
            "audio",
            "music"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nA TWO-FOLD DYNAMIC PROGRAMMING APPROACH  \nTO BEAT TRACKING  FOR AUDIO MUSIC WITH TIME -VARYING  TEMPO  \n \nFu-Hai Frank Wu ,  Tsung -Chi Lee,  Jyh-Shing Roger Jang                    Kaichun K. Chang                            Chun Hung Lu,  W en Nan Wang \nDepartment of Computer Science                                     D epartment  of Computer Science                Institute  For Information  Industry  \nNational Tsing Hua University                     King â€™s College London                                            (IDEAS)  \n   Hsinchu, Taiwan                    London, United Kingdom                                     Taipei,Taiwan  \n{frankwu,  leetc,  jang}@mirlab.org                                  {ken.chang }@kcl.ac.uk                   {enricoghlu,  wennen}@iii.org.tw  \n \n \nABSTRACT  \n \nAutomatic beat tracking and tempo estimation  are \nchallenging tasks, especially for audio music with time -\nvarying tempo. This paper proposes a two -fold dynamic \nprogramming (DP) approach to deal with beat tracking with \ntime-varying tempo. In particular, the first DP computes the \ntempo curve from the t empogram. The second DP identifies \nthe optimum beat positions from the novelty and tempo \ncurves. E xperimental results demonstrate satisfactory \nperformance for music with significant tempo variations. \nThe proposed approach was submitted to the task of audio  \nbeat tracking in MIREX 2010 and was ranked no. 1 for 6 performance indices out of 10, for the dataset with variable \ntempo.  \n \nIndex Terms  â€“ Beat tracking, T empogram , Time -varying \ntempo, Dynamic programming, Viterbi search \n \n1. INTRODUCTION  \n \nTempo and beat are two essential elements in music.  Such  \ninformation is useful  in several applications such as query \nby tempo  (querying a large database based on tempo) , beat \nslicing [17] (segmentation into bas ic music units separated \nby beats ), and beat synchronous mixing . However, \nautomatic beat tracking  and tempo estimation are still \nchallenging tasks when  the music has  time-varying tempo s.  \nConventional  beat tracking schemes  [1] rely on certain \nassumptions about  music contents such as stable  tempo over \ntime, periodical percussions /onsets, and four beats per \nmeasure.  Under  these assumptions , most approaches of beat \ntracking are accomplished by two phases. In the first phase, \nthe onset  strength of music  along time , called novelty curve,  \nis esti mated to indicate the possible positions of note onsets . \nIn the second phase, the quasi -periodic patter ns in novelty \ncurve are analyzed to discover the possible tempo value and the corresponding beat positions . Here, tempo is assumed to \nbe stable  throughout the whole piece of music .  \nHowever,  the above -mentioned assumptions do not hold \ntrue universally, especially for music of classical and jazz. \nMusic of these genre s often has  significant tempo variations , \nmaking it difficult to detect the periodical patterns. In order \nto detect  the variations in tempo,  Frequency Mapped Auto-\nCorrela tion Function (FM-ACF) and S hort-Time Fourier \nTransform  (STFT)  [2] are frequently used to derive a time -\nfrequency representation of the novelty curve , called \ntempogram  [3]. The tempo information is embedded in \ntempogram. We can then apply dynamic programming (DP) \nto the tempogram to deriv e the  so-called tempo curve, which \nrepresents  the most likely  tempo at each time frame.  \nA number of  beat tracking algorithms have been \nproposed in the literature under different methodologies, \nincluding beat-template  training [2], neural network s [4], an \nagent -based method  [5], and so on . Among them , DP is still \nconsidered an efficient and effective  way for  determining \nbeat positions . The use of DP  for beat tracking  has been \nproposed in [1] with good performance, but it is based on a \npre-estimated stable  tempo  which is estimated by time -\ndomain autocorrelation with window weighting . \nThere are several important previous studies that \nattempted to deal with time -varying tempo s. Klapuri et al.  \n[18] used the bandwise time -frequency method to obtain  \naccentuation information, then use d comb filter resonator s \nand probabilistic  models to estimate pulse  width and phase \nof different metrical  level s, including tatum, tactus, and \nmeasure. Davi es and Plumbley  [19] proposed the  use of \ncomplex spectral difference onset function [15] to obtain \nmiddle level representation. Their  algorithm employs  two-\nstate switching model , including general state and context -\ndependent state, to obtain  final b eat positions.  Groshe and \nMuller  [16] used the novelty curve to generate predominant  \nlocal pulse  (PLP) for estimating time -varying tempos.  \nIn this study, we follow the three -phase framework [ 2, 6] \nof beat tracking and attempt  to remove the stable -tempo \nrestriction  by develop ing a two-fold DP approach  for robust \nbeat tracking with  time-varying tempos.  To this en d, the \nfirst DP estimates the time -varying tempo curve from the \ntempogram (which is  obtained from the novelty curve ). \nThen the second DP uses the time -varying tempo  curve to \nidentify the optimum beat positions on the novelty curve.   \nPermission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fee provided that copies are \nnot made or distributed for profit or commercial advantage and th at copies \nbear this notice and the full citation on the first page.   \nÂ© 2011 International Society for Music Information Retrieval  \n191Poster Session 2\n(In fact, we have proposed similar concepts for speech \nanalysis, including DP -based robust pitch determination [13] \nfor Mandarin tone recognition, and DP -based pitch marking \n[14] for TD -PSOLA  synthesis .) In addition, we also propose \npartial- FFT-based tempo curve estimation and peak picking \nin tempogram  for DP, which enhance the overall efficiency \nwith almost no accuracy  loss. The proposed approach was \nranked no. 1 for 6 performance indices out of 10, for the dataset of time -varying temp in the audio beat tracking task \nof MIR EX 2010.  \nThe remainder of this paper is organized as follows . \nSection  2 describes the details of the proposed framework . \nPerformance evaluation is given  in Section 3. Section 4 \nconcludes this work with potential future work . \n \n2. SYSTE M DESCRIPTION  \n \nThe proposed beat tracking system  is shown in Figure  1.  \n \n \nFigure 1. Flowchart  of the proposed beat tracking system  \n \nThe first block compute s the  novelty curve based on [1 , 6]. \nThe second block generates  the tempogram and estimates \nthe tempo curve from the novelty curve. In the third block, \nbeat positions are estimated by using the information from \nprevious two blocks. Details of each block will be explained \nin the following subsections.  \n 2.1 Novelty Curve Estimation  \n \nFigure  2 shows typical  outputs of various steps in \nnovelty curve estimation. A  power  spectrogram  is first \nobtained by applying STFT to  the source audio with a frame \nsize 31.6 milliseconds and 87.5% overlap. The frequency \ncomponents of spectrogram are then mapped into Mel -scale \nin Figure  2(a) for conforming to the characteristics of \nhuman perception  [1]. Then we apply spectral flux (S PF) \n[12] to obtain the raw novelty curve, as shown in Figure  2 (b)  \nTo be more specific, we have 40 bands in t he Mel -scale \nspectrogram , where each band has a n equal width in the \nMel-scale frequency. In other words, each frame is \ntransformed into a vector of 40 elements of mean energy \nwithin the bands. Moreover, the Mel -scale  spectral flux  can \nbe defined as follows:  \n  \n ğ‘€ğ‘’ğ‘™ğ¹ğ‘™ğ‘¢ğ‘¥ (ğ‘¡ğ‘–)=1\nğ‘âˆ‘ğ»ğ‘…ğ¹ï¿½ğ‘€ğ‘’ğ‘™ğ‘†ğ‘ğ‘’ğ‘ğ‘¡ğ‘Ÿğ‘œ ï¿½ğ‘¡ğ‘–+1,ğ‘ğ‘—ï¿½âˆ’ğ‘\nğ‘—=1\n                                                          ğ‘€ğ‘’ğ‘™ğ‘†ğ‘ğ‘’ğ‘ğ‘¡ğ‘Ÿğ‘œ ï¿½ğ‘¡ğ‘–,ğ‘ğ‘—ï¿½ï¿½      (1) \n \n wher eğ‘¡ğ‘– is the time for frame i , ğ‘ğ‘— is Mel -band  j,  \nğ‘€ğ‘’ğ‘™ğ‘†ğ‘ğ‘’ğ‘ğ‘¡ğ‘Ÿğ‘œ ï¿½ğ‘¡ğ‘–,ğ‘ğ‘—ï¿½ is the Mel spectrogram at frame i  and \nMel-band   j, and  ğ»ğ‘…ğ¹ ï¿½ï¼ï¿½ is the  half-wave rectifier.  \n \n \nIn general, we neglect the locally periodical information \nabove 500 BPM (beats per minute) due to the limitation of \nhuman perception [7]. Thus we use Gaussian smoothing \n(which acts as a low -pass filter) to filter out the redun dant \nhigh-frequency parts in raw novelty curve, as shown in \nFigure  2 (c). The Gaussian filter has a cutoff frequency \nequal to the sampling frequency divided by 5 . At last, we \nsubtract the local mean ( dotted curve in Figure  2 (c) ) to \nobtain the final novelt y curve, as shown in Figure  2 (d).  The \nlocal mean is derived from Gaussian smoothed raw novelty curve  filtered by another Gaussian filter with a cutoff \nfrequency equal to the sampling frequency divided by 125.  \n \n2.2 Tempo Curve Estimation  \n \nIn this block, we estimate the tempo curve by analy zing \nlocally periodical patterns in novelty curve. Generally speaking, local periodicity estimation is usually \naccomplished by STFT, FM- ACF or a combined method [2]. \nHowever, the autocorrelation -based meth od generates  non-\nuniform tempo grid s in tempogram,  since  the temp o is the \ninverse of the beat time difference. More specially , the \nlower the temp o is, th e finer resolution (via interpolation, \nfor instance) is requi red to achieve a high precision. To \navoid such extra work for maintaining the precision, here we \nuse STFT to obtain the tempo curve in our study . Tempo  Curve Estimation  \nDP Initialization:  \nïƒ˜ Anchor Candidate Searching  \nïƒ˜ Probability weighting window  \nïƒ˜ Number of beat candidate  Mel-scale spectrogram  \nCalculate difference between  \nadjacent frames  \nHalf-wave rectification  \nSum over all frequency bands  \nGaussian smoothing  \nLocal-mean subtraction  Novelty Curve Generation  \nShort -Time Fourier Transform  Monaural audio  \nPartial  FFT \nTempogram construction  \nDynamic Programming  DP Initialization:  \nïƒ˜ Tempo change penalty  \nïƒ˜ Peak picking in tempogram  \nBeat tracking  \nForward/Backward   \nDynamic Programming \n  \nBeat positions  SPF Calc.  \nNovelty Curve  Tempo Curve  \n19212th International Society for Music Information Retrieval Conference (ISMIR 2011)\n         \n         \nFigure 2. (a) Power  spectrogram . (b) Raw novelty curve . (c) \nSmoothed novelty curve with local mean curve ( the dash \ncurve) . (d) Novelty curve after local mean sub traction  \n \nAs mentioned above, we do not have to analyze all \nfrequency components in the novelty  curve. Therefore, a \npartial FFT method  is employed to eliminate  high-frequency \ncomputation  in STFT . Furthermore , the selection of \nanalyz ing window length  significantly influence s the \ncapability  for tracking tempo variation. In  our \nimplementation, the frame size is set to be  4 second s with \n99.6%  overlap . The resulting tempogram is  shown in Figure  \n3(a). \nIn order to stri ke a balance between tempo continuity \nand novelty curve strength, a  DP-based approach is used to \nobtain the tempo curve . Given the magnitude  ğ‘€ğ‘–,ğ‘— of a point  \nin the tempogram with time index ğ‘– (1â‰¤ğ‘–â‰¤ğ‘›), we want to \nfind a tempo path   ğ= ï¿½ğ’‘ğŸ,â‹¯,ğ’‘ğ’Šâ‹¯,ğ’‘ğ’ï¿½, with ğ‘ğ‘–  is tempo \nvalue , such that the over -all objective  utility function  is \nmaximized : \n \n \nJ(ğ,ğœƒ)  = âˆ‘ğ‘€ğ‘–,ğ‘ğ‘–ğ‘›\nğ‘–=1âˆ’ Î¸Ã—âˆ‘ |ğ‘ğ‘–âˆ’ğ‘ğ‘–+1|ğ‘›âˆ’1\nğ‘–=1  , (2) \n \nwhere ğœƒ is the transition penalty factor incurred by the \ndiffer ence of the tempo path within two  consecutive frames.  \nThe first term in the utility  function is the magnitude values along the path over the tempogra m, while the second term \ncontrols  the smoothness of the path (thus the computed \ntempo curve). If Î¸ is larger, then the tempo curve will be \nsmoother. In particular , if ğœƒ = 0 in the extreme case, then \nmaximizing  the utility  function is equivalent to maximum -\npicking of each column  (or equivalently, each frame ) of the \ntempogram . \n     For efficiency, we shall employ DP to find the maximum \nof the utility  function, where the optimum -valued function \nğ·(ğ‘–,ğ‘—) is defined as the maximum utility  starting from \nframe 1 to ğ‘–, with  the frequency/ tempo index ending at  ğ‘— \n(1â‰¤ğ‘—â‰¤ğ‘š). Then the recurrent equation for DP can be \nformulated as follows:  \n \nğ·(ğ‘–,ğ‘—)=ğ‘€ğ‘–,ğ‘—+ğ‘šğ‘ğ‘¥ğ‘˜,ğ‘—âˆˆ[1,ğ‘š]{ğ·(ğ‘–âˆ’1,ğ‘˜)âˆ’ğœƒÃ—ğ‘¡ğ‘‘(ğ‘˜,ğ‘—)}          (3) \nwhere  ğ‘–âˆˆ[2,ğ‘›],ğ‘˜ ğ‘ğ‘›ğ‘‘  ğ‘— are tempo  index  \n     td (ï¼) is tempo difference function  \n      \nThe initial conditions are  \n \nğ·(1,ğ‘—)=ğ‘€1,ğ‘—,ğ‘—âˆˆ[1,ğ‘š]                                       ( 4) \n \nAnd the maximum utility  is equal to ğ‘€ğ´ğ‘‹ğ‘—âˆˆ[1,ğ‘š]ğ·(ğ‘›,ğ‘—). A \nsimilar DP -based pitch tracking method has been proposed \nfor tone recognition in our previous work [ 13].  \nIn prac tice, we can replace ğ‘¡ğ‘‘(ğ‘˜,ğ‘—) in the recurrent \nequation with ğ‘¡ğ‘‘(ğ‘˜,ğ‘—)=|ğ‘ğ‘˜âˆ’ğ‘ğ‘—|, which represents the \ntempo difference between tempo indices  ğ‘˜ and ğ‘—. This is \nadopted in our implementation.  Figure 4 demonstrates \ntypical results of DP over a tempogram, with (a) and (b) \nbeing the tempogram ğ‘€ and the DP table ğ·, respectively, \ntogether with the optimum path obtained via DP. Figure 4 (c) \nand (d) shows the same plots using a 3D surface for easy \nvisualization.  \nAs a common  practice in DP, after  the maximum utility  is \nfound, we can backtrack to  find the optimum path  together  \nwith the mos t likely tempo curve , as shown in Figure  3(b). \nThe transition penalty factor ğœƒ controls tempo variations, \nthat is, it determines the smoothness of tempo curve, as \nshown in Figure  3(c), where a larger value of   ğœƒ leads to a \nsmoother tempo curve.  In our experiment, the transition \npenalty factor ğœƒ is set to 0.01 empirically  in order to track \nthe correct tempos . \n       \nTime(sec)Mel Band(a)\n  \n0.5 1 1.5 2 2.5 3 3.5 4 4.5 50102030\n-60-40-20\n0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 500.0050.010.0150.02(b)\nTime(sec)Amplitude\n0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5246810x 10-3 (c)\nTime(sec)Amplitude\n0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 502468x 10-3 (d)\nTime(sec)Amplitude\nTime(sec)Beats Per Minute(BPM)(a)\n  \n0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5100200300400\n0.050.10.15\n0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 50100200300400500(b)\nTime(sec)Tempo(BPM)\n193Poster Session 2\n \nFigure 3. (a) The tempogram o btained from the novelty \ncurve (b) Local maxima of each column of the tempogram \nand the final optimum tempo path (solid line) with ğœƒ = 0.01  \n(c) Tempo curves obtained with ğœƒ= 0.01 and 0.03, \nrespectively.  \n \nFigure 4. (a) Tempogram (as a contour map) and the \noptimum path . (b) DP table (as a contour map) and the \noptimum path . (c) Tempogram (as a 3D surface) and the \noptimum path. (d) DP table (as a 3D surface) and the \noptimum path.  \n \nWhen ğ‘›  is big, the computational complexity is still too \nhigh to compute the recurrent equation  over all  states. To \nreduce the computation, we can simply pick the ğ¿ largest \nlocal maxima within each column of the tempogram as the \ncandi date states for DP, as shown in Figure  3 (b). In our \nexperiment, this simplified algorithm with L equal to 10 can \nachieve almost the same  performance as the original DP.  \n \n2.3 Beat Tracking  \n               \nThis block utilizes  both the tempo curve and the novelty \ncurve  to find a sequence of beat positions that fits the tempo \ncurve and the novelty strengths  as much as possible. To \nachieve  this task, we apply another DP -based method in a \nprobabilistic  framework (just like Viterbi search in speech \nrecognition) to perform forward and back ward  beat tracking , \nstarting  from the anchor beat position  (the position of the \nmost prominent  peak) of the novelty curve.  We have proposed such a probability -based DP framework for pitch \nmark identification [14]. Another  DP-based approach has \nbeen proposed for stable -tempo beat tracking [ 1], though not \nin a probabilistic  framework . \nHere  we use Figure  5 to explain the weighting -based DP \nmethod  for beat position identification . First of all, we find \nthe maximum of the novelty curve as the first beat position, \nwhich is referred to as the anchor candidate. Starting from \nthe anchor candidate, we search on both  sides, one side at a \ntime, to obtain all beat positions. The search regio n is \ngener ally defined as a range from 0.2  to 2.2 times ğ‘‡, the \nbeat period at the anchor candidate. We use a log -time \nGaussian function over the search region as a weighting window for approximat ing the transition probability. Note \nthat the maximum of the  log-time Gaussian window is \nlocated at ğ‘‡ from the anchor candidate.  \nIn practice, only the largest  ğ‘ peaks of the novelty curve \nwithin the next search region are selected as the candidates \nfor the next beat posi tions. As a result, we need to perform \nnormalization to guaran tee that the transition probabilities \nsum to 1 within the search region. Similarly, the state \nprobabilities of these ğ‘ candidates are obtained based on \ntheir heights within the novelty curve.  \n \n \n \nFigure 5.  Backward beat search  with ğ‘ =2  \n \nOnce the state and transition probabilities are defined, we \ncan apply DP just like Viterbi search for the optimum beat \npositions. The search  is performed  twice for both forward \nand backward directions from the anchor candidate, and the \nresults of them are merged to obtain the complete beat \npositions. In our experiment, we set N  to 2. Figure  6 shows a \ntypical result with ğœƒ = 0.01 and ğ‘=2. \n   \nFigure 6. (a) Typical  beat tracking results with ğœƒ = 0.01 and \nğ‘=2 (beat positions  indicated by circles) .  (b) The tempo \ncurve used to obtain  the beat positions in (a).  0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5120140160180200220240260280(c)\nTime(sec)Tempo(BPM)\n  \nÎ˜ = 0.01\nÎ˜ = 0.03\n0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 502468x 10-3 (a)\nTime(sec)Amplitude\n0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5100150200250300(b)\nTime(sec)Tempo(BPM)Beat  \ncandidates  Search  \nregion  Succ essive \nanchor  \ncandidate  Weighting window  \nSearch  \nregion  Anchor  \ncandidate  \n19412th International Society for Music Information Retrieval Conference (ISMIR 2011)\n3. PERFORMANCE EVALUATIO N \n \nIn this section, we present the performance of the proposed  \nalgorithm by using the results of the Audio Beat Tracking \ncontest  in Music Information Retrieval Evaluation \neXchange (MIREX) 2010  [8]. \n \n3.1 Performance Indices  \n \nThere are a number of performance indice s proposed for the \naudio beat tracking task in MIREX 2010 [8]. For simplicity, \nhere w e explain  two performance indice s which are \ngenerally adopted in beat tracking evaluation among all . The \nfirst one is F -measure [9] which  considers the  estimated beat \nas correct if it is within a tolerance window (Â±70ms in \nMIREX 2010)  around the ground truth.  The second one is \nP-score [10] which  measures  beat tracking  accuracy by the \nsummation  of the cross -correlation between  impulse trains  \nof the estimated  beats and the ground truth.  \n \n3.2 Data sets \n \nTwo music data sets are used to evaluate the performance of \nthe proposed system with  stable  and time -varying tempo, \nrespectively.  \n \n- MCK  dataset:  \nïƒ˜ Collected by Martin F. McKinney and Dirk Moelants . \nïƒ˜ Contains  160 30- second excerpts . \nïƒ˜ Ground truth is annotated as stable  tempo . \nïƒ˜ A large variety of instrumentation and musical styles.  \n \n- MAZ dataset:  \nïƒ˜ Collected by Craig Sapp . \nïƒ˜ A subset of 367 Chopin Mazurka pieces  [10]. \nïƒ˜ Ground truth is annotated as time -varying tempo .  \n \n3.3 Performance and discussion \n \nTable s 1 and 2 show the performance of participating  team s \nin MIREX 2010 a udio beat tracking task on stable  and time -\nvarying tempo respectively. Only the best method s from \neach team  are listed here.  Algorithm TL2 uses the proposed \nmethod in this paper.  LGG2 and MRVCC1 accomplish this \ntask based on BeatRoot system  proposed by Simon Dixon  \n[5]. NW1 is based on Predominant  Local Pulse curves  (PLP) \n[6]. GP3 estimates beat and downbeat position s [11 ] \nsimult aneously  via an inverse Viterbi formulation  and LDA -\ntrained  beat-template  [3]. ZTC1 tracks beat with  a global \nstable  tempo value . BES4 is based on bidirectional Long \nShort -Term Memory (BLSTM) recurrent neural networks .  \n \nAlgorithm  \nID TL2 LGG2  MRVCC1 NW1  GP3  ZTC1  BES4  \nF-\nMeasure  42.0 50.0 25.7 35.6 50.3 1.2 54.5 \nP-Score  50.6 55.0 38.4 45.7 56.5 0.9 59.2 \nTable  1. Performance on MCK dataset  (stable  tempos)   As shown in Table 1, the proposed algorithm (TL2) only \nperforms moderately well on MCK dataset which has stable  \ntempos. The performance in this dataset indicates we might have put too much emphasis on tracking tempo variations \ninstead of identifying stable  tempos. In other words, we \nmight want to increase the value of the transition p enalty \nfactor  ğœƒ  such that the tempo variations can be kept small for \nthis dataset.  \n \nAlgorithm  \nID TL2 LGG2  MRVCC1 NW1  GP3  ZTC1  BES4  \nF-\nMeasure  68.5 41.5 49.2 27.6 47.1 24.6 58.7 \nP-Score  72.2 43.5 51.0 31.4 48.7 26.1 57.9 \nTable  2. Performance o n MAZ dataset (time -varying tempos)  \n \n \nFigure 7. The performance on MAZ dataset, including all \nsubmitted algorithms and 7 performance measures.  \n \nOn the other hand , in Table 2, the proposed algorithm  \n(TL2)  outperforms all the other teams based on the \nperformance indices  of F -measure and P -score. More \nspecifically, if we consider all the submitted algorithms and \nall the performance measures,  the proposed algorithm \noutperforms other 12 submitted algorithms on 6 \nperformance indice s out of 9, as shown in Figure. 7 . (Note \nthat in the Figure, we only show  7 performance indi ces for \nclarity. Moreover, the performance measure  by Goto is not \ncounted since it is close to zero for all submitted algorithms.) \nThis clearly demonstrates the feasibility of the proposed \ntwo-fold DP  strategy for dealing with music of time -varying \ntempos.  \n \n4. CONCLUSION S \n \nIn this paper, we have propose d a two-fold DP approach to \nbeat tracking, especially for time -varying tempo musi c. The \nfirst DP is applied to estimate the tempo curve from the \ntempogram, and the second DP is used to find the optimum \nbeat positions with maximum likelihood. The proposed \nmethod is very similar to our previous work on speech \nanalysis, where the first DP is used for  robust pitch \ndetermination [13] and the second DP for robust  pitch \nmarking [14] . Based on the results of the a udio beat tracking \ncontest of MIREX 2010, the proposed method performs ex-\ntremely well for music with time -varying tempos, but only \nmoderately well for music with stable  tempos.  To improve  0 10 20 30 40 50 60 70 80 F-Measure  \nCemgil  \nMcKinney P -score  \nCMLc  \nCMLt  \nAMLc  \nAMLt  \n195Poster Session 2\nthe proposed algorithm, our immediate work is to use a \ntraining based method to select the transition penalty factor \nğœƒ such that it can deal with music with both stable  and time -\nvarying tempos.  Moreover, we would like to develop a more \nsystematic way of defining the state and transition \nprobabilities used for the second- fold DP for finding the \noptimum  beat positions . We will also investigate the \npossibility of incorporating more acoustic features, either \ntime- or fr equency- domain, to define the more robus t \nnovelty curve that can deal with music with no percussions.   \n \n5.  ACKNOWLEDGMENTS  \n \nThe third  author would like to acknowle dge the sponsor of \nthis work by \" III Innovative and Prospective  Technologies   \nProject\" of the Institute for Information Industry which is \nsubsidized by the Ministry of Economy Affairs of the \nRepublic of China.  \n \n6.  REFERENCES  \n \n[1] D.P.W. Ellis, â€œBeat Tracking by Dynamic Programming,â€ Journal of New Music Research,  Vol. \n36(1), 51 â€“60, 2007.  \n[2] G. Peeters, â€œTemplate -based Estimation of T ime-\nVarying Tempo,â€ EURASIP Journal on Advances in \nSignal Processing, Vol. 2007, 158â€“ 171, 2007.  \n[3] G. Peeters, â€œBeat -marker Location Using a \nProbabilistic Framework and L inear Discriminant \nAnalysis,â€ in Proc. DAFX, Como, Italy, 2009.  \n[4] A.T. Cemgil, B. Kappen, P. Desain, and H. Honing, \nâ€œOn Tempo Tracking: Tempogram Representation \nand Kalman Filtering â€ Journal of New Music \nResearch , Vol. 28(4), 259- 273, 2001.  \n[5] S. Dixon,  â€œAutomatic Extraction of Tempo and Beat \nfrom Expressive Performances,â€ Journal of New \nMusic Research , 30(1):39â€“ 58, 2001.  \n[6] P. Grosche and M. MÃ¼ller, â€œA M id-level \nRepresentation for C apturing Dominant Tempo and \nPulse Information in M usic Recordings,â€  in Proc.   \nISMIR , pages 189 â€“194, Kobe, Japan, 2009.  \n[7] J. Bilmes, â€œA Model for Musical Rhythm,â€ in Proc.  \nICMC, San Francisco, USA, 1992.  \n[8] MIREX 2010 Audio Beat Tracking Contest Results. \nhttp://www.music -\nir.org/mirex/wiki/2010 :MIREX2010_Results  \n[9] M.F. McKinney, D. Moelants , M.E.P. Davies and A. \nKlapuri, \"Evaluation of audio beat tracking and music \ntempo extraction algorithms,\" Journal of New Music \nResearch , Vol. 36, no. 1, pp. 1 -16, 2007.  \n[10] The Mazurka Project.  http://www.mazurka.org.uk, \n2010.  \n[11] G. Peeters and H. Papadopoulos, â€œSimultaneous Beat \nand Downbeat -tracking Using a P robabilistic \nFramework: Theory and L arge-scale Evaluation,â€ submitted to IEEE. Trans. on Audio, Speech and \nLanguage  Processing , 2010.  \n[12] S. Dixon, â€œO nset Detection  Revisited â€ in Proc. the 9th \nInt. Conference o n Digital Audio Effects (DAFx -06), \nMontreal, Canada, September 18 -20, 2006  \n[13] Jiang -Chun Chen, J. -S. Roger Jang, â€œTRUES: Tone \nRecognition Using Extended Segmentsâ€œ, ACM Transactions on Asian Language Information \nProcessing, Vol. 7, No. 10,Aug 2008.  \n[14] Cheng- Yuan Lin, J. -S. Roger Jang, â€œA Two -Phase \nPitch Marking Method for TD -PSOLA Synthesisâ€œ,  In \nProc . Interspeech  2004 - 8th International Conference \non Spoken Language Processing (ICSLP), pp. 1189-1192, Korea, Oct 2004.  \n[15] Juan Pablo Bello, Laurent Daudet, Samer Abdall ah, \nChris Duxbury, Mike Davies, Mark B. Sandler , â€œ A \nTutorial on Onset Detection in Music Signals â€œ IEEE \nTransactions  on Speech  and Audio Processing , Vol. \n13, N o. 5, S eptember  2005  \n[16] Peter Grosche, Meinard MÂ¨uller , â€œComputing  \nPredominant  Local Periodicity  Information  in Music \nRecordings â€œ  IEEE Workshop on Applications of \nSignal Processing to Audio and Acoustics , 2009  \n[17] Adam M. Stark, Matthew E. P. Davies , Mark D. \nPlumbley , â€œReal-Time  Beat-Synchronous  Analysis  of \nMusical  Audioâ€œ in Proc. the 12th Int. Conference  on \nDigital Audio Effects (DAFx -09) \n[18] Anssi P . Klapuri, Antti J. Eronen, Jaakko T. Astola , \nâ€œAnalysis of the Meter of Acoustic Musical \nSignals â€œ IEEE T ransactions  on Audio, Speech , and \nLanguage  Processing , Vol. 14, N o. 1, J anuary  2006  \n[19] Matthew E. P. Davies , Mark D. Plumbley,  â€œContext -\nDependent Beat Tracking of Musical Audio \" IEEE \nTransactions  on Audio, S peech , and Language  \nProcessing , Vol. 15, N o. 3, March  2007  \n \n  \n \n196"
    },
    {
        "title": "Segmentation, Clustering, and Display in a Personal Audio Database for Musicians.",
        "author": [
            "Guangyu Xia",
            "Dawen Liang",
            "Roger B. Dannenberg",
            "Mark Harvilla"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1418185",
        "url": "https://doi.org/10.5281/zenodo.1418185",
        "ee": "https://zenodo.org/records/1418185/files/XiaLDH11.pdf",
        "abstract": "Managing music audio databases for practicing musicians presents new and interesting challenges. We describe a systematic investigation to provide useful capabilities to musicians both in rehearsal and when practicing alone. Our goal is to allow musicians to automatically record, organize, and retrieve rehearsal (and other) audio to facilitate review and practice (for example, playing along with difficult passages). We introduce a novel music classification system based on Eigenmusic and Adaboost to separate rehearsal recordings into segments, an unsupervised clustering and alignment process to organize segments, and a digital music display interface that provides both graphical input and output in terms of conventional music notation.",
        "zenodo_id": 1418185,
        "dblp_key": "conf/ismir/XiaLDH11",
        "keywords": [
            "music classification system",
            "Eigenmusic",
            "Adaboost",
            "separation",
            "organizing",
            "alignment",
            "digital music display interface",
            "graphical input",
            "output",
            "conventional music notation"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\n   SEGMENTATION, CLUSTERING, AND DISPLAY IN A PERSONAL AUDIO DATABASE FOR MUSICIANS Guangyu Xia      Dawen Liang      Roger B. Dannenberg      Mark J. Harvilla Carnegie Mellon University  {gxia, dawenl, rbd, mharvill}@andrew.cmu.edu  ABSTRACT Managing music audio databases for practicing musicians presents new and interesting challenges. We describe a sys-tematic investigation to provide useful capabilities to musi-cians both in rehearsal and when practicing alone. Our goal is to allow musicians to automatically record, organize, and retrieve rehearsal (and other) audio to facilitate review and practice (for example, playing along with difficult pas-sages). W e  i n t r o d u c e  a  n o v e l  m u s i c  c l a s s i f i cation system based on Eigenmusic and Adaboost to separate rehearsal recordings into segments, an unsupervised clustering and alignment process to organize segments, and a digital music display interface that provides both graphical input and out-put in terms of conventional music notation. 1. INTRODUCTION Music Information Retrieval promises new capabilities and new applications in the domain of music. Consider a per-sonal music database composed of rehearsal recordings. Music is captured by continuously recording a series of re-hearsals, where the music is often played in fragments and may be played by different subsets of the full ensemble. These recordings can become a valuable resource for musi-cians, but accessing and organizing recordings by hand is time consuming. To make rehearsal recordings more useful, there are three main processing tasks that can be automated. ( S e e  Figure 1.) The first is to separate the sound into music and non-music segments. T h e  m u s i c  s e g m e n t s  w i l l  c o n s i s t  o f  many repetitions of the same material. Many if not most of the segments will be fragments of an entire composition. We want to organize the segments, clustering them by composition, and aligning them to one another (and possi-bly to other recordings of the music). Finally, we want to coordinate the clustered and aligned music with an interface to allow convenient access. We see these capabilities as the foundation for an inte-grated system in which musicians can practice and compare their intonation, tempo, and phrasing to existing recordings or to rehearsal data from others. By performing alignment in real time, the display could also turn pages automati-cally. The next section presents a novel method for mu-sic/non-music classification and segmentation. Section 3 describes how to organize the segments. Section 4 de-scribes a two-way interface to the audio. \n Figure 1. System diagram for a musician's personal audio database. Rehearsal recordings are automatically processed for simple search, analysis, and playback using a music no-tation-based user interface.\u00012. CLASSIFICATION AND SEGMENTATION 2.1 Related Work Much work has been done in the area of classification and segmentation on speech and music. For different tasks, people extract different features. Some f o c u s  o n  b a ck-ground music detection [6], while others detect s p e e c h  o r  music sections in TV programs or broadcast radio. Many features have been tested in the realm of speech/music clas-sification [ 8 ,  1 7 ]. Two frequently used ones are Spectral-Centroid and Zero-Crossing Rate. Also, different statistical models have been used. Two of them, long window sam-pling [7] a n d  the HMM segmentation framework [ 1 ,  1 4 ], are especially relevant to our work. Other approaches in-clude using decision trees [16] and Bayesian networks [5]. However, the particular problem of variations i n  the sound source seems to be largely ignored. In reality, sound is not standardized in volume or bandwidth and may even contain different kinds of noise. In these cases, more robust features and methods are needed. This section will concen-trate on new feature extraction and model design methods  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.  Â© 2011 International Society for Music Information Retrieval  \n139Poster Session 1\n   to achieve music/non-music classification and segmentation on realistic rehearsal audio.  2.2 Eigenmusic Feature Extraction  The concept of Eigenmusic is derived from the well-known representation of images in terms of Eigenfaces [12]. The process of generating Eigenmusic can be performed in both the time and frequency domains, and in either case, simply refers to the result of the application of Principal Component Analysis (PCA) to the audio data [3]. Therefore, Eigenmusic refers to t h e  eigenvectors of an empirical covariance matrix associated with an array of music data. The array of music data is structured as a spectrogram and hence contains the spectral information of the audio in those time intervals. When expressing non-music d a t a  i n terms of Eigenmusic, the coefficients are g e n e r a l l y  expected to be outlying based o n the fundamentally different characteristics of music and non-music.  In practice, we use about 2.5 hours of pure music in the training data collection to extract the Eigenmusic in the fre-quency domain. First, let X = [x1, x2, â€¦ , xT] be a spectro-gram, a matrix consisting of, in its columns, m a g n i t u d e spectra corresponding to 1.25 second non-overlapping win-dows of the incoming music data. Second, the correspond-ing empirical covariance matrix, Cx, a n d  i t s  E i g e n v e c t o r s  are computed. Ultimately, we retain the first 10 eigenvec-tors corresponding to the largest eigenvalues. If P i s  t h e matrix of column-wise eigenvectors of Cx, given a new magnitude spectrum column vector x, we can represent its Eigenmusic coefficients b y  PTx, which will be a 10-dimensional vector. 2.3 Adaboost Classifier  Adaboost [18] is a very interesting classification algorithm, which follows a simple idea: to develop a sequence of hy-potheses for classification and combine the classification results to make the final decision. Each simple hypothesis is individually considered a weak classifier, h(PTx), a n d  t h e  combined complex hypothesis is considered to be the strong classifier. In the training step, each weak classifier focuses on instances where the previous classifier failed. Then it will obtain a weight, Î±t, and update the weight of individual training data based on its performance. In the decoding step, the strong classifier is taken to be the sign of the weighted sum of weak classifiers: H(x)=sign(Î±tht(PTx))tâˆ‘                            (1) By training a sequence of linear classifiers ht, each one of which merely compares a n  i n d ividual Eigenmusic coeffi-cient against a threshold that minimizes the weighted error, Adaboost is able to implement a  n o n-linear classification surface in the 10-dimensional Eigenmusic space. 2.3.1 Data Collection and Representation The Adaboost training data is a collection of about 5 hours of rehearsal and performance recordings of western music; while the testing data is a collection of 2.5 hours of Chinese music. For the music parts, each data c o l l e c t i o n  contains different combinations of w i n d  i n s t r u m e n t s ,  s t r i n g  i n s t ru-ments, and singing. For the non-music parts, each data col-lection contains speech, silence, applause, noise, etc. Both data collections are labeled as music or non-music at the frame level (1.25 seconds). From Section 2.2, we know that each frame is a point in the 10-dimensional Eigenmusic space. Therefore, we have about 5 (hours) Ã— 3600 (s/hour) / 1.25 (s/frame) = 14,400 frames for training and 7,200 frames for testing. 2.3.2 Implementation and Evaluation  We train 100 weak classifiers to construct the final strong classifier. The testing accuracy is shown in Figure 2. The results were obtained in terms of the percentage of error at the frame level. Two different statistics h a v e  b e e n  c a l cu-lated: the percentage of true music identified as non-music, shown as the solid line, and the percentage of true non-music identified as music, shown as the dotted line. \n02040608010000.050.10.150.20.250.30.35\nnumber of weak classifierserror rateerror rate of testing data\n  musicnonÃ¯music Figure 2. The testing error of music and non-music. From Figure 2, it can be seen that the proposed Adaboost classifier in the Eigenmusic space is capable of achieving a low error rate (about 5.5%) on both music and non-music data, even when the testing data comes from a completely different sound source from the training data. 2.3.3 Probabilistic Interpretation  We can improve the frame level classification by consider-ing that state changes between music and non-music do not occur rapidly. We can model rehearsals as a two-state hid-den Markov model (HMM) [13]. Formally, given a vector x, let y âˆˆ {-1,1} represent its true label. Here, -1 stands for non-music and 1 stands for music. And let w(x) represent the weighted sum of weak classifiers: \n14012th International Society for Music Information Retrieval Conference (ISMIR 2011)\n   w(x)=Î±tht(PTx)tâˆ‘                             (2) In Equation (1), we took the sign of w(x) as the decision, but we can modify this approach to compute the a posteri-ori probability of y = 1, given the weighted sum, which we denote as the function F: F(w(x))=P(y=1|w(x))                        (3) According to the discussion in [15], F(w(x)) is a  l o g i s t i c  function, as shown in Equation 4: F(w(x))=11+exp(âˆ’2â‹…w(x))                     (4) In Figure 3, the small circles show P(y = 1 | w(x)) estimated from training data sorted into bins according to w(x). The logistic function is shown as the solid curve. It can be seen that our empirical data matches the theoretical probability quite well. \nÃ¯3Ã¯2Ã¯10123400.20.40.60.81\nw(x)p(y=1|w(x))logistic estimation\n Figure 3. The logistic function estimation on training data. We note that the idea of linking Adaboost with HMMs is not new, but very little work has been done to implement it [4, 19]. As far as we know, this is the first attempt of a probabilistic i n t e r p r e t a t i o n of A d a b o o s t  w h e n  l i n k e d  w i t h  HMMs.  2.4 HMM Smoothing for Segmentation  The significance of smoothing is that even a very low error rate at t h e  f r a m e  l e v e l  c a n n o t  guarantee a  s a t i s f y i n g  s eg-mentation result overall (i.e. at the piece level). For exam-ple, suppose a relatively low 5% error rate is obtained at the frame level. If the segmentation rule is to separate the target audio at every non-music frame, a 10 minute long pure mu-sic piece would be cut into about 25 pieces in this case. Ul-timately, this is an undesirable result.  Based on typical characteristics of rehearsal audio data, we assume that: (1) music and non-music frames cannot alternate frequently, and (2) short duration music and non-music intervals are less likely than longer ones. By utilizing these assumptions in conjunction with the HMM, low (but possibly deleterious) frame-level error rates can be further reduced. We use a fully-connected HMM with o n l y  t w o  states, representing music and non-music. The H M M ob-servation corresponding to every frame x is a real number w(x), as in Equation (2), given by the Adaboost classifier. 2.4.1 HMM Training The training data collection m e n t i o n e d i n S e c t i o n 2 . 3 . 1  is used to estimate the HMM parameters. Formally, let S = [S1, S2,â€¦,ST] b e  t h e  s t a t e  s e q u e n c e  a n d  l e t  O =  [O1, O2,â€¦,OT] be the observation sequence. Since it is a super-vised learning problem, we do Maximum Likelihood Esti-mation (MLE) by counting or just manually setting the pa-rameters f o r  i n i t i a l  s t a t e  p r o b a b i l i t i e s  a n d  t r a n s i t i o n  p r ob-abilities. For emission probabilities, we use Bayesâ€™ rule: P(Ot|St=1)=P(St=1|Ot)â‹…P(Ot)P(St=1)          (5) Remember that in our model Ot = w(xt) and P(Ot) is a con-stant. Therefore, if we plug in function F a c c o r d i n g  t o  Equation (3), we obtain the estimate of the emission prob-ability of music w h e r e  C d e n o t e s  a  c o n s t a n t  s c a l a r  m u l ti-plier: P(Ot|St=1)=Câ‹…F(w(xt))P(St=1)                 (6)\u0001Using the same method, we obtain the estimate of the emis-sion probability of non-music: P(Ot|St=âˆ’1)=Câ‹…1âˆ’F(w(xt))P(St=âˆ’1)              (7) Here, we set the a priori probability of both music and non-music to 0.5 and then apply the Viterbi algorithm [13] t o  efficiently find the best possible state sequence for a given observation sequence. 2.4.2 Implementation and Evaluation At the frame level, HMM smoothing reduced the error rate from about 5.5% to 1.8% on music and to 2.2% on non-music. This is the same as the best claimed result [17] i n  the references [6, 7, 8, 17], where classifiers were tested on cleaner data sets not related to our application. Since the piece level evaluation has been largely ignored in previous works o n music/non-music segmentation, we adopt an evaluation method from s p e e c h  s e g m e n t a t i o n  [20] called Fuzzy Recall and Precision. This method pays more atten-tion to insertion and deletion than boundary precision. We get a Fuzzy Precision of 89.5% and Fuzzy Recall of 97%. The high Fuzzy Recall reflects that all true boundaries are well detected with only some imprecision a r o u n d  t h e  boundaries. The lower Fuzzy Precision reflects that about 10% of the detected boundaries are not true ones.  \n141Poster Session 1\n   3. CLUSTERING OF MUSIC SEGMENTS Assuming perfect classification results from the previous step, the clustering task is a distinct problem. Our goal is to cluster the musical segments belonging to the same piece. 3.1 Feature Extraction Chroma vectors [2] have been widely used as a robust har-monic feature in all kinds of MIR tasks. The chroma vector represents the spectral energy distribution in each of the 12 pitch classes (C, C#, D,â€¦ A#, B). Such features strongly correlate to the harmonic progression of the audio.  Considering the objective that our system should be ro-bust to external factors (e.g. a u d i e n c e  cheering and ap-plause), the feature cannot be too sensitive to minor varia-tions. Therefore, as suggested by MÃ¼ller, we first calculate 12-dimensional chroma vectors using 200ms windows with 50% overlap, then compute a  l o n g e r-term summary by windowing over 41 consecutive short-term vectors and normalizing, with a 10-vector (1s) hop-size. These long-term feature vectors are described as CENS features (Chroma Energy distribution Normalized Statistics) [10, 11]. The length of the long-term window and hop size can be changed to take global tempo differences into account. 3.2 Audio Matching and Clustering  Given the CENS features, audio matching can be achieved by simply correlating the query clip Q = (q1, q2, â€¦ qM) with the subsequences of musical segments P = (p1, p2, â€¦ pN) in the database (assume N >  M). Here, a l l  l o w e r  c a s e  l e t t e r s  (e.g. qi, pi) represent 12-dimensional CENS vectors. Thus, Q and P are both sequences of CENS vectors over time. As in [11], the distance between the query clip Q and the sub-sequence P(i) = (pi, pi+1,â€¦ pi+M-1) is: dist(Q, P(i)) = 1-1Mqk,pi+kâˆ’1k=1Mâˆ‘              (8) Here <qk, pi+k-1> denotes the dot product between these two CENS vectors. All of the distances for i\u0001= 1, 2, ... Nâˆ’M+1 together can be considered a distance function âˆ† b e t w e e n  query clip Q and each of the musical segments P in the da-tabase. If the minimum distance is less than a preset thresh-old Î³, then Q can be clustered with P.  One problem with this decision scheme is that, unlike a traditional song retrieval system which has a large reference database in advance, our system has no prior information about the rehearsal audio stream. We are only given a stream o f  potentially unordered and unlabeled audio t h a t  needs to be clustered. To solve this problem, we construct the database from the input audio dynamically. The inputs are all the music segments obtained from Section 2, and the algorithm is: 1. Sort all the music segments according to their length. 2. Take out the longest segment S. i) If database D is empty, put S into D as a cluster.  ii) Otherwise match S w i t h  e v e r y  s e g m e n t  i n  D b y  calculating distance function âˆ†. Let Dm be the segment in D with the best match. (1) If the distance function âˆ† of Dm with S has a minimum less than Î³, cluster S with Dm. (2) Otherwise make S a new cluster in D. iii) Repeat step 2 until all segments are clustered. Here we made a critical assumption: the longest segment is m o s t  l i k e l y  t o  b e  a  w h o l e  p i e c e  o r  a t  l e a s t  t h e  l o n g e s t  segment for this distinct piece, so it is reasonable to let it represent a new cluster. At every step of the iteration, we take out a new segment S which is guaranteed to be shorter than any of the segments in database D. This implies it can either be part of an existing piece in the database (in which case we will cluster it with a matching segment) or it is a segment for a new piece which does not yet exist in the da-tabase (in which case we will make it a new cluster). We also need to consider the possibility that tempo dif-ferences cause misalignment between sequences. We can obtain different versions of CENS features ( f o r  example, from 10% slower to 10% faster) for the same segment to represent the possible tempos. This is achieved by adjusting the length of the long-term w indow and the hop size as mentioned in Section 3.1. During matching, the version of the segment w i t h  t h e  l o w e s t  distance function minimum will be chosen.  3.2.1 Segment Length vs. Threshold Value While t i m e scaling compensates for global tempo differ-ences, it does not account for local variation within seg-ments. It is interesting to consider the length of the query clip that is used to correlate with the segments in the data-base. Intuitively, longer clips will be more selective, reduc-ing spurious matches. However, if the length is too large, e.g. two segments both longer than 5 minutes, sequence misalignments due to tempo variation will decrease the cor-relation and increase the distance. If longer segments lead to greater distance, one might compensate w i t h  l a r g e r  threshold values (Î³). However, larger Î³ values may not prove strict enough to filter out noise, leading to clustering errors. We will compare two pairs of configurations: longer segments with larger Î³ and shorter segments with smaller Î³. 3.2.2 Experiments and Evaluation We have two parameters to control: Î³, which determines if the two segments are close enough to be clustered together, and t, the length of the segments. We use hours of rehearsal recordings as test data, with styles that include c l a ssical, \n14212th International Society for Music Information Retrieval Conference (ISMIR 2011)\n   rock, and jazz. We also use live performance recordings, which are typically even longer. To evaluate the clustering results, we use the F-measure as discussed in [9]:  P=TPTP+FP, R=TPTP+FN                          (9) \u0001\u0001FÎ²=(Î²2+1)PRÎ²2P+R                                   (10) Here, P (precision) and R (recall) are determined by 4 dif-ferent variables: TP ( t r u e  p o s i t i v e ) w h i c h corresponds to assigning two similar segments to the same cluster, TN (true negative) corresponding to a s s i g n i n g  t w o  d i s s i m i l a r  segments to the different clusters, FP (false positive) corre-sponding to assigning two dissimilar segments to the same cluster, and FN ( f a l s e  n e g a t i v e )  which corresponds to as-signing two similar segments to different clusters. Î² is the tuning parameter used to adjust the emphasis on precision or recall. In our case, it is more important to avoid cluster-ing segments from different pieces into one cluster than it is to avoid â€œoversegmentingâ€ by creating too many clusters. The latter case is more easily rectified manually. Thus, we would like to penalize more on false positives, which leads to choosi ng Î² < 1.  Her e, we use Î² = 0. 9.  Considering the possible noise near the beginning and the end of the record-ings, we choose the middle t s e c o n d s  if the segment is shorter than the original recording. \u0001As seen in Figure 4, for segments longer than 3 minutes, the relatively larger Î³ = 0.25 outperforms others, while for shorter segments around 20s to 60s, the smaller Î³ = 0.15 has the best performance. It is also shown that if Î³ i s  s e t  t o o  large (0.35), the performance drops drastically. Overall, shorter segments and smaller Î³ g i v e  u s  b e t t e r  results than longer segments and larger Î³. Finally, since calculating cor-relation has O(n2) complexity, shorter segment lengths can also save significant computation. Thus, our current system uses a segment length t = 40s and Î³ = 0.15. K-means clus-tering was also tested but did not work as well as our algo-rithm because of the non-uniform segment length and un-known number of clusters (details omitted for reasons of space). 4. USER INTERFACE Ultimately, we plan to integrate our rehearsal audio into a digital music display and practice support system (see Fig-ure 5.). While listening to a performance, the user can tap on music locations to establish a correspondence between music audio and music notation. Once the music has been annotated in this manner, audio-to-audio alignment (a by-product of clustering) can be used to align other audio automatically. The user can then point to a music passage in order to call up a menu of matching audio sorted by date, length, tempo, or other attributes. The user can then prac-tice with the recording in order to work on tempo, phrasing, or intonation, or the user might simply review a recent re-hearsal, checking on known trouble spots. One of the excit-ing elements of this interface is that we can make useful audio available quickly through a natural, intuitive interface (music notation). It is easy to import scanned images of no-tation into the system and create these interfaces. \n05010015020025030035040000.10.20.30.40.50.60.70.80.91\nSegment Length (s)FÃ¯measure\n  \na = 0.15a = 0.20a = 0.25a = 0.30a = 0.35 Figure 4. Experimental results with different segments of length t and matching threshold Î³. \n Figure 5. Audio database is accessed through a common music notation interface. The user has selected the begin-ning of system 3 as a starting point for audio playback, and the current audio playback location is shown by the thick vertical bar at the beginning of system 4. \n143Poster Session 1\n   5. CONCLUSIONS We have presented a system for automated management of a personal audio database for practicing musicians. The system segments recordings and organizes them through unsupervised clustering and alignment. An interface based on common music notation allows the user to quickly re-trieve music audio for practice or review. Our work intro-duces Eigenmusic as a music detection feature, a probabil-istic connection between Adaboost and HMMs, an unsu-pervised clustering algorithm for music audio organization, and a notation-based interface that takes advantage of audio-to-audio alignment. In the future, we will fully inte-grate these components and test them with actual users. 6. ACKNOWLEDGEMENTS This work is supported by the National Science Foundation under Grant No. 0855958. We wish to thank Bhiksha Raj for suggestions and comments on this work, and the Chi-nese Music Institute of Peking University for providing recordings of rehearsal for analysis. 7. REFERENCES [1] J. Ajmera, I. McCowan and H. Bourlard: â€œSpeech/Music Segmentation Using Entropy and Dynamism Features in a HMM Classification Framework,â€ Speech Communi-cation 40 (3), pp. 351-363, 2003. [2] M. Bartsch and G. Wakefield: â€œTo Catch a Chorus: Using Chroma-Based Representations for Audio Thumbnailing,â€ IEEE Workshop on Applications of Signal Processing to Audio and Acoustics, pp. 15-18, 2001. [3] D. Beyerbach, H. Nawab: â€œ P r i n c i p a l  C o m p o n e n t s  Analysis of Short-Time Fourier Transform,â€ International Conference on Acoustics, Speech, and Signal Processing, 1991. [4] C.  Dimitrakakis and S. Bengio: â€œBoosting HMMs with an Application to Speech Recognition,â€ International Conference on Acoustics, Speech, and Signal Processing, Montreal, Canada, 2004. [5] T. Giannakopoulos, A. Pikrakis and S. Theodoridis: â€œA Speech/Music Discriminator for R a d i o  R e c o r d i n g s  Using Bayesian Networks,â€ International Conference on Acoustics, Speech, and Signal Processing, 2006. [6] T. Izumitani, R. Mukai, and K. Kashino: â€œA Background Music Detection Method Based on Robust Feature Extraction,â€ International Conference on Acoustics, Speech, and Signal Processing, 2008. [7] K. Lee and D. Ellis: â€œDetecting Music in Ambient Audio by Long-Window Autocorrelation,â€ International Conference on Acoustics, Speech, and Signal Processing, Las Vegas, USA, 2008. [8] G. Lu and T. Hankinson: â€œA Technique Towards Automatic Audio Classification and Retrieval,â€ Proceedings of ICSP, Beijing, China, 1998. [9] C. D. Manning, P. Raghavan, and H. SchÃ¼tze: Introduction to Information Retrieval, Cambridge University Press, 2008.  [10] M. MÃ¼ller, S. Ewert, and S. Kreuzer: â€œMaking Chroma Features More Robust to Timbre Changes,â€ International Conference on Acoustics, Speech, and Signal Processing, pp. 1869-1872, Taipei, Taiwan, 2009. [11] M. MÃ¼ller, F. Kurth, and M. Clausen: â€œAudio Matching via Chroma-Based Statistical Features,â€ in Proceedings of the 6th International Conference on Music Information Retrieval, pp. 288-295, 2005. [12] D. Pissarenko: â€œEigenface-based facial recognition,â€ 2002. [13] L. Rabiner: â€œA Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition,â€ Proceedings of the IEEE, 77(2), pp. 257-287, 1989. [14] C. Rhodes, M. Casey, S. Abdallah, and M. Sandler: â€œA Markov-chain Monte-Carlo approach to musical audio segmentation,â€ International Conference on Acoustics, Speech, and Signal Processing, 2006. [15] J. Riedman, T. Hastie, and R. Tibshirani: â€œAdditive Logistic Regression: A Statistical View of Boosting,â€ The Annals of Statistics, vol 208, No.2, pp. 337-407, 2000. [16] A. Samouelian, J. Robert-Ribes, and M. Plumpe:  â€œSpeech, Silence, Music and Noise Classification of TV Broadcast Material,â€ Proceedings of International Conference on Spoken Language Processing, vol. 3, pp. 1099-1102, Sydney, Australia, 1998. [17] J. Saunders: â€œReal Time Discrimination of Broadcast Speech/Music,â€ International Conference on Acoustics, Speech, and Signal Processing, pp. 993-996, 1996. [18] R. E. Schapire: â€œA Brief Introduction to Boosting,â€ Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence, 1999. [19] H. Schwenk: â€œUsing Boosting to Improve a Hybrid HMM/Neural Network Speech Recognizer,â€ International Conference on Acoustics, Speech, and Signal Processing, pp. 1009-1012, 1999. [20] B. Z i Ã³ Å‚ ,  S .  M a n a n d h a r ,  a n d  R .  C .  W i l s o n: â€œ F u z z y  Recall and Precision for Speech Segmentation Evaluation,â€ Proceedings o f  3 r d  L a n g u a g e  &  Technology Conference, Poznan, Poland, 2007.  \n144"
    },
    {
        "title": "Fast Hamming Space Search for Audio Fingerprinting Systems.",
        "author": [
            "Qingmei Xiao",
            "Motoyuki Suzuki",
            "Kenji Kita"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1418143",
        "url": "https://doi.org/10.5281/zenodo.1418143",
        "ee": "https://zenodo.org/records/1418143/files/XiaoSK11.pdf",
        "abstract": "In music information retrieval, a huge search space has to be explored because a query audio clip can start at any position of any music in the database, and also a query is often corrupted by significant noise and distortion. Audio fingerprints have recently attracted much attention in music information retrieval, for they provide a compact representation of the perceptually relevant parts of audio signals. In this paper, we propose an extremely fast method of exploring a huge Hamming space for audio fingerprinting systems. The effectiveness of the proposed method has been evaluated by experiments using a database of 8,740 songs.",
        "zenodo_id": 1418143,
        "dblp_key": "conf/ismir/XiaoSK11",
        "keywords": [
            "music information retrieval",
            "huge search space",
            "query audio clip",
            "database",
            "query corrupted",
            "audio fingerprints",
            "compact representation",
            "Hamming space",
            "audio fingerprinting systems",
            "efficacy evaluation"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\n  \n \nFAST  HAMMING  SPACE  SEARCH  FOR  AUDIO  \nFINGERPRINTING  SYSTEMS  \nQingmei Xiao  Motoyuki Suzuki  Kenji Kita  \nFaculty of Engineering, The University of Tokushima  \nTokushima 770 -8506, Japan  \nxiaoqingmei@is s.tokushima -u.ac.jp, moto@m.ieice.org , \nkita@is.tokushima -u.ac.jp  \nABSTRACT  \nIn music information retrieval, a huge search space has to \nbe explored because  a query audio clip can start at any pos i-\ntion of any music in the database,  and also a query is often \ncorrupted by significant noise and distortion.  Audio finge r-\nprints have recently attracted much attention in music i n-\nformation  retrieval,  for they provide a compact represent a-\ntion of the perceptually  relevant parts of audio signals. In \nthis paper, we propose an extrem ely fast  method of explo r-\ning a huge Hamming space for audio fingerprinting  systems. \nThe effectiveness of the proposed method has been eva-\nluated  by experiments using a database of 8,740 songs.  \n1. INTRODUCTION  \nJust as fingerprint s are  used for identifying huma n being s, \naudio fingerprint s can be used to identify  music . Audio fi n-\ngerprints , together with a music information database , can \nbe used to derive  information about an unknown aud io clip \nautomatically, such as the  names of the song , artist and a l-\nbum. Gracenote [1] and Mido mi [2] are two  well-known \ncommercial service s. They  retrieve a song by using a few \nseconds of music clip  caught by such as a PC or mobile \nphone , display  the title of the song and other information, \nand also enable the user to download the song from a web -\nbased music store.  In recent years, audio finger print s have  \nalso attracted attention as a technique for copyright prote c-\ntion of music, such as detecting  the distribution of copy-\nright -infringing songs  on the Internet.   \n    In general, an audio  clip is given as a search query , and it \ndoes not necessarily start at the beginning of the song. \nTherefore, a retrieval method should consider any time as a \nstarting position , but this requires a long c omputation time. \nIn order to solve this problem, fast and effe ctive retrieval \nmethods are necessary . Some e fficient retrieval  method s based on audio fingerprin ts have been proposed, including  \na method using a hash table [3][4] and a tree-structure d re-\npresentation of fingerprints  [5]. \n    A query is an excerpt of a song , but it may be â€œcorrupted â€ \nby being mixed wit h environmental noise, or it may have \nbeen  modified by a low -pass filter. As a result, retrieval \nmethods  should be able to handle  queries which are similar \nto, but not exactly the same as a song in the database. Lo-\ncality -Sensitive Hashing (LSH) is an emerging technique  \nfor solving large -scale similarity re trieval in high -\ndimensional spaces, and has been applied  in extensive r e-\nsearch fields  [6-8]. \n   In this paper, w e propose a fast method for exploring a \nhuge Hamming space which  is suitable for audio finge r-\nprinting  systems  building on the ideas of LSH . There have \nbeen s everal previous proposals on H amming space retrie v-\nal methods based on  LSH,  however , our method uses less \nmemory . The effectiveness of the proposed method is dem-\nonstrated  by evaluation experiments using a database of \n8,740 songs.  The paper is organized as follows: Section 2 \noutlines  music retrieval based on audio fingerprint s. We \npropose a fast music retrieval method  particu larly suitable \nfor audio fingerprinting sys tems in Section 3, and evaluate \nthe method in Section 4. Finally, w e conclude  the paper  in \nSection 5.  \n2. OVERVIEW O F MUSIC RETRIEVAL BASE D ON \nAUDIO FINGERPRINT S \nAudio fingerprinting  is a kind of message digest ( one-way \nhash function), and it  converts an audio signal into a rela-\ntively compact representation by using acoustical and pe r-\nceptional characteristics of the audio  signals . For message \ndigesting methods used for authentication and digital sign a-\ntures (e.g. MD5), slight difference in the original objects \nresults in totally different hash values . This means that two \nhash values mapp ed from an  original  audio si gnal and a \ncorrupted one are completely diffe rent, which  drastically \ndecreases the retrieval performance for â€œcorrupted â€ queries. \nHowever, in audio fingerprint ing, similar inputs are hashed \nto similar hash values.   \nPermission to make digital or hard copies of all or part of this work for \npersonal or classroom use  is granted without fee provided that copies are \nnot made or distributed for profit or commercial advantage and that co p-\nies bear this notice and the full citation on the first page.   \nÂ© 2011  International Society for Music Information Retrieval  \n133Poster Session 1\n  \n \n    Music retrieval based on audio fingerprinting involves \nsome key problems: (1) which  type of audio fingerprints  to \nuse, (2) how to define the distance between two fingerprints, \nand (3) how to retrieve from a huge data base. We review \nthese problems  next. \n2.1 Audio Fingerprint  Extraction   \nA variety of  audio fingerprint extraction algorithm s have \nbeen proposed  based on different acoustic feature s, such as \nFourier coefficients  [9], Mel frequency cepstral  coefficients  \n[10], spectral flatne ss [11] and so on . In particular, the fin-\ngerprint extraction algorithm  by Haitsma and Kalker  [3] \nuses a feature of the energy difference betwe en frequ ency \nband s as follows .   \n    First, an input audio signal  is segmented into frames , and \nthen 32 -bit sub -fingerprints are extracted from each ove r-\nlapping frame . Sub-fingerprints  are actually calculated in \nthe frequency domain . Each frame  is first converted into a \nfrequency domain  by using FFT, and then segmented into \n33 non-overlapping frequency bands . Next , a sub-\nfingerprint  is calculated by checking the sign (plus or minus)  \nof the energy difference  between two successive  frequency \nband s. Haitsma and Kalker  [3] used a frame length of 0.37 \nsecond  with an overlap factor of 31/32 , so a sub-fingerprint  \nwas extract ed for every 11.6 millisecond s. \n    The sub-fingerprint s are calculated as follows : let E(n, m) \nbe the power  of frequency band m of frame n, then the m-th \nbit of f rame n, F(n, m), is determined as:  \n                                \n                        ï¼Œ                                                                                                                                                                      \nwhere \n                                    \n                                           (2)            \nHaitsma and Kalker  [3] demonstrated that the sign of \npower  differences between successive frequency bands was \neffective  for identifying music , and was also robust against \nvarious â€œcorrupted â€ input s such as comp ressed or delayed \nmusic . The Haitsma and Kalker  algorithm can be impl e-\nmented  by simple arithmetic,  while maintaining  compact  \nrepresentation for generated  audio fingerprint s. \n2.2 Distance between Audio Fingerprint s \nThe sub-fingerprint  is a 32 -bit feature extracted  from a \nframe in an input  audio , and one sub -fingerprint  does not \nhave  enough  information to identify the audio . To obtain \nsufficient  information, a fingerprint block , which is a s e-\nquence of sub -fingerprints, is used for matching audio sub -\nfingerprint s. A fingerprint bl ock con sisting  of 256 sub -\nfingerprints was used in the experiments in [3]. \n     Bit error rate is used as the distance between two finge r-\nprint  blocks.  Let FA(n, m), FB(n, m) be the sub -fingerprints \nextracted from audio  clips A and B respectively . The bit e r-ror rate of fingerprint block BER(A, B) of length N is for-\nmally defined as:  \n                               \n      \n   \n              \nThe operator ^ denotes  bitwise  operation  XOR ( exclusive \nor). The n umerator of Equation  (3) calculates  the Hamming \ndistance between two fingerprint blocks, which is divided \nby the bit length of fingerprint block s (32N). BER(A,B) is \nthe error rate per bit.  \n2.3 Audio Fingerprint  Search  \nMost music retrieval methods based on audio fingerprinting \nhave the following stages.  First, fingerprint blocks are e x-\ntracted from each song in the database.  Because of the u n-\nknown position of the query , all variations of starting point \nshould be considered. Therefore, each song allows extrac t-\ning quite a number of fingerprint blocks  by shifting all the \nframes to fingerprint blocks one by one . When a query is \ngiven,  many fingerprint blocks are also extracted from the \nquery. Thus, m usic retrieval involves finding the fingerprint \nblock in the database that is most similar to the fingerp rint \nblock derived from the query . \n     The search space of audio fingerprinting is huge . For \nexample , a fingerprint database containing  10,000 songs \neach with an average length of 5 minu tes would result in \napproximately 250 million fingerprint blocks in total using \nthe algorithm in [3].  The number of distance calculation s \nwould be  several to several dozen  times as large as 250 mi l-\nlion by brute -force search taking account of matching the \nfingerprint blocks . Many ways of reducing the number of \ncalculatio ns have been proposed , such as using a hash table \n(lookup table) for sub -fingerprints [3 ], a tree -structur ed re-\npresentation of sub-fingerprints [ 5], and a hash table co n-\nsisting of peak values in the frequency doma in and duration  \nbetween the two peak s [4]. However, with these methods \nthe size of the hash table grows  rapidly with the bit error \nrates between the query and songs in the  database  increa s-\ning. \n3. FAST HAMMING SPACE SEARCH  FOR  AUDIO \nFINGERPRINTS  \nIn this section, we propose a fast retrieval method for audio \nfingerprint ing systems . Suppose that audio fingerprints are \nrepresented by binary bit vector s, and the Hamming di s-\ntance is used for the distance between two audio finge r-\nprints.  We first outline the search methods for Hamming  \nspace based on LSH in Section 3.1, and then propose a new \nretrieval method in Section 3.2.  \n13412th International Society for Music Information Retrieval Conference (ISMIR 2011)\n  \n \n3.1 Locality -Sensitive Hashing  \nLocality -Sensitive Hashing  (LSH)  is a hashing scheme for \nprobabilistic search es of  large -scale high-dimension al data, \nrather than a specific algo rithm . It includes the  hashing  me-\nthod for Hamming  distance  using bit sampling  [6], the me-\nthod for Jaccard distance using min -wise independent pe r-\nmutation  [12], the method based on random projection  for \ncosine distance  [13], and the method using p -stable di stri-\nbution  for Lp distance  [14]. The concept of LSH is to map \nthe high-dimension al vector data into hash values  so that \nsimilar data are mapped to the same hash values with high \nprobability . Gener ally, we cannot find a hash function \nwhich gives the same hash values for similar hi gh-\ndimensional data. LSH  can maintain certain retrieval acc u-\nracy by using multiple hash functions.  \n    There are a few Locality -Sensitive Hashing  schemes \nproposed to reduce the problem in the Hamming space. In-\ndyk and Motwani propo sed an  LSH algorithm for Ha m-\nming space based on the Point Location in Equal Balls  \n(PLEB)  problem  [15], and Charikar [13] and Ravicha ndran \n[16] improv ed the algorithm by u sing random permutation s \nof binary  vectors . \n    The concept of random permutation s is as follows : given \na set of n vectors D = {d1, d2, â€¦, dn}, where each vector \nconsists  of k binary bits, permutation    is defined as a bije c-\ntion on {1, 2, â€¦, k}, and then  we can define that the bit \nvector                      is a permutation of            . \nThe number of permutation s for k bits is k!, hence  a random \npermutation is a random selection from these k! permut a-\ntions. \n   We can now create the data set     by permut ing all bits \nby using    for all elemen ts in the  data set D, and also calc u-\nlate the new query vector     from the query vector q in the \nsame way . The most similar vector to     can be found  in \nthe data set    by doing the following  steps : Sort    in lex-\nicographic order,  and then perform the binary search . The \nbinary search is carried out from the first bit to the last bit , \nso if a different bit is located in the upper side (near the first \nbit), then the search makes a mistake. On the other hand, if \na different bit is located in the lower side, the search c an \nfind the nearest vector. We expect to find the most similar \nvector by making a number of random permutations    \nand corresponding data set    , and searching for all data \nsets. This is an overview of the LSH for Hamming space \nproposed by Charikar [13] an d Ravicha ndran [16] ; the de-\ntails of the theories and experimental analysis  of this m e-\nthod are discus sed in [13] and  [17]. \n3.2 Fast Hamming Space  Search  Method for Audio \nFingerprints  \nThe principle of the Hamming space search based on ra n-dom permutation s is simple. The binary search can certai n-\nly find the exact vector if there exists one vector the same \nas the query.  A similar vector which has a few differen t bits \nin the lower side can be found, too , but the problem  is that \nsometimes it cannot find a simi lar v ector which has a few \ndifferent  bits in the upper side. To address  this problem,  \nrandom permutations are used. In general, LSH -based m e-\nthods use multiple hash functions . In the Hamming space \nsearch based on the random permutation  method, multiple \nrandom permutations can be regarded as multiple hash \nfunctions .  \n   The greatest disadvantage of the retrieval method based \non random permutation s is the require ment for a huge \namount of memory  in order to perform  many random pe r-\nmutations on the original database in advance.  This i n-\ncreases the size of  database required to at least several to \nseveral dozen times  larger than the size  of the original  data-\nbase.  \nIf we c ould only multiplex the quer y vectors without \nmultiplexing the database vectors , then Hamming space \nsearch ing would require little memory . Based on  this a s-\nsumption, w e propose  a new search method by modifying  \nthe query vector into many  similar vector s. \n     The scheme of the search method based on random pe r-\nmutation s is shown in Figure 1, and that of the proposed \nmethod is  shown in Figure 2.  In the random  permutation \nmethod, m ultiple random permutations  (  ,           in \nFigure 1) are applied to b oth the original data base and \nquery  vector  in order to solve the problem  of search omi s-\nsions . On the other hand, in the proposed method, only the \nquery is multiplexed  through  the functions (   ,           \nin Fig ure 2). The definition of functions    is necessarily \napplication -dependent.  \n    The proposed method  is based on the sub-fingerprint \nmatching  scheme , and functions    create the multiplexed \nsearch queries of sub -fingerprint s equences from the query \naudio clip . Many sub -fingerprints are extracted  by shifting \nthe query  into frame s. Moreover, there exists a great sim i-\nlarity  between the overlapping sub -fingerprints in the s e-\nquence of sub -fingerprint , so that m ultiplexed  sub-\nfingerprints with slight differences can be obtained as star t-\ning time of frame moving down . These sub -fingerprints  are \nused for queries  multiplexing , whic h make it possible to \nsearch for a song without modifying the original database \nby using random permutations . \n   The flow of the proposed method is as follows: first, es-\ntimate  several candidate s of starting position  in the database \nthose  using sub -fingerprint s obtain ed from the query.  Then , \ncalculate the Hamming distance (bit error rate)  for the fin-\ngerprint block s of query mu sic data and estimated cand i-\ndates . Usually one sub -fingerprint does not contain suff i-\ncient information for music identification, so a sequence of   \n135Poster Session 1\n  \n \n \n     \nFigure 1. Schematic diagram of search based on random \npermutation s \n \n \n \n \nFigure 2. Schematic diagram of search based on query \nmultiplexing  \n \nsub-fingerprints (SSF) is  used. In this paper, the length of \nthe SSF is set to 3 ( 3 sub -fingerprints, containing  96 bits in \ntotal).  \nA schematic diagram of the SSF search is shown in \nFigure 3. The sub-fingerprint s obtained from all the songs \nof the database are denoted by FP = (FP 1, FP 2, â€¦, FP n). \nAs stated above, we can get many SSFs  in certain length by \nchanging the starting position of  fingerprint . Let m be the \nlength of each SSF, and the SSFs are constructed  from \n   in such a  way that SSF 1 = (FP1, FP 2,â€¦, FP m), SSF 2 = \n(FP 2, FP 3,â€¦, FP m+1) and the i-th sub-fingerprint sequence \nSSF i = (FP i, FP i+1,â€¦, FP i+m-1).  All the SSFs are sorted by \nvalue and the sorted position s of SSFs are stored in a one -\ndimension al array S = S1, S2, â€¦, Sn-m+1. Array S, similar to \nthe suffix array  [18],  contains the indexes to    and satis-         \nFigure 3. Schematic diagram of SSF search  \n \nfies the following : \nSj = i    iff    SSF i = (FP i, FP i+1,â€¦, FP i+m-1)  is the     \n   j-th SSF in sorted o rder.                      (4) \n \nIn the search step, a binary search is performed on array  \nS for all the SSFs extracted from the query  audio clip . Most \nsimilar SSF can be found by checking the neighborhood \npositions of the searched block in array S. \nArray  S is used as  an index for music retrieval. The size \nof the i ndex is proportion al to the length of the sub-\nfingerprint  sequence  in the database , so it requires much \nless memory/storage compared with the method based on \nrandom  permutation s. \nThe proposed method can be summarized as follows:  \n(1) Extract the sub-fingerprint  sequenc e FP from query \nmusic . \n(2) For all SSFs, find candidate positions by performing a \nbinary search on array S. \n(3) Set the start position of the FP to the candidate pos i-\ntion, and calculate the Hamming distance (bit error \nrate) between FP and the fingerprint  block corre s-\nponding to the SSF.  \n(4) Output the top n songs as the final result s. \n \n4. EVALUATION EXPERIMEN TS \nTo evaluate  the effectiveness of the proposed me thod, real \nmusic data were  used for evaluation experiments.  In these \nexperiments , the algorithm proposed by Haitsma and Kal k-\ner [3] was  used for extracting audio fingerprint s in different \nacoustical  analysis settings . \n4.1 Music Data  \nThe database had 8,740 songs  in mp3 format from  CDs or \nthe Internet.  The compression ratio was different for each           First song \nfingerprint s Second  song \nfingerprint s â€¦â€¦â€¦â€¦  \n \nSSF 2 SSF 3 FP1 FP2  FP3 FP4  FP5   â€¦â€¦â€¦  FPi FPi+1 FPi+2 â€¦â€¦ FPn \nâ€¦â€¦â€¦ \n â€¦â€¦ \nâ€¦ \nMultiplexed \nqueries  \ngeneration  \nQuery  Retrieval data  \nsort \nâ€¦\nâ€¦\nâ€¦ 00â€¦0 11â€¦0 â€¦\nâ€¦\nâ€¦ 01â€¦0 11â€¦0 \n11â€¦0 Binary search  \n11â€¦0    \n    01â€¦0 10â€¦0 \n   \n       \n11â€¦0 sort \nBinary search  \nQuery  11â€¦0 sort \n      \n   â€¦\nâ€¦\nâ€¦ 10â€¦0 10â€¦1 â€¦\nâ€¦\nâ€¦ 01â€¦0 11â€¦1 â€¦\nâ€¦\nâ€¦ 01â€¦0 11â€¦1 â€¦\nâ€¦\nâ€¦ 10â€¦1 10â€¦0 â€¦\nâ€¦\nâ€¦ 01â€¦0 11â€¦1 â€¦\nâ€¦\nâ€¦ 11â€¦1 01â€¦0 â€¦\nâ€¦\nâ€¦ 01â€¦0 11â€¦0 \nsort \n01â€¦1 11â€¦1 \n   Random  \npermutations  \nRandom  \npermutations  FP â€¦â€¦ \n3 2 n-2 i \ni\ni â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ \nâ€¦ â€¦ â€¦ \nQuery clip  010 â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ 110 \n96 bits ... ... Binary search  SSF 1 SSF i SSF n-2 \nSorted  positions  \nS Last song \nfingerprint s \n13612th International Society for Music Information Retrieval Conference (ISMIR 2011)\n  \n \nsong. There were ma ny genres in the database such as  pop, \nclassica l, and folk music.  \n       \nCategor y Notes  Audio  Accuracy  \n \nOriginal \nmusic  \n Non-\nnoise  PV music faithful to the \noriginal music with little \nnoise  if any .  \n104  \n96.2%  \nWith \nnoise  Declared  to be original \nbut with obvious noise .  \n22  \n100%  \n \nLive data  Live audio , most of which \ncontain  voice s, chee r-\ning and applause , and \nother noise.   \n142  \n83.1%  \n \nTable 1. Results on evaluation data  \n \nMusic clips uploaded to YouTube  were used for the \nqueries. Audio data were  extracted from various types  of \nvideo s, such as promotional video and live video. Many of \nthe music data were of poor quality , including music fo l-\nlowing and followed by long silences, and music with var i-\nous types  of noise such as hand -clapping, cries of excit e-\nment, and other environmental noise . 268 songs were used \nfor evaluation  data, which  are roughly classified by hand.  \nDetails of the evaluation data  are shown  in Table 1.  \n4.2 Acoustical Analysis Settings  \nAfter down -sampling to 4,000 Hz, the music data were  \nsegmented into frames by using a Hamming window. The  \nframe length was set to 1.024 seconds and the frame shift to  \n32 milliseconds. All frames were converted into the fr e-\nquency  domain by FFT. The frequency doma in was divided  \ninto 33 frequency bands, and 32 -bit sub -fingerprint features  \nwere extracted. The length of the fingerprint block was set  \nto 128.  \nAlthough these settings seem rough c ompared with those \ngiven by Haitsma and Kalker [3], these parameters  were \ndetermined by many preliminary experiments  and the r e-\nsulting proposed  algorithm gave a high accuracy . The total \nnumber of sub-fingerprints  was about  70 million . \n \n4.3 Experimental Results  \nExperiments were  carried out  on a PC (DELL  Precision  \nM6500 ) with an Intel Core  i7 CPU (1.73  GHz) (8 cores)  \nand 4 GB of memory. Retrie val times varied  as the query \nmusic , and each song was retriev ed in approx imately 0.4 to \n0.6 second s. \n   The length of the sub-fingerprint  sequence for one query  \nmusic was approximately  from 6,000 to 8,000 fingerprints . \nThe proposed algorithm search es candidate positions by a \nbinary search for all SSFs  (length was 3) extracted  from the \nquery music before calculating the bit error rate of finge r-\nprint block s. Therefore, we believe that the algorithm is competent and fast since the retr ieval time per  SSF did not \nexceed 0.1 milliseconds . \n    The top-1 retrieval accuracy  is shown in the right column \nof Table 1 . The retrieval rate for â€œoriginal music â€ was \n98.6%, and accura cy for â€œlive musicâ€ was 83.1%. The dif-\nference was due to the different melody of the live clip \nfrom that of the original music.  The evaluation data of \nâ€œoriginal music â€ can be divided into two classes with r e-\ngard to noise, but the results did not show any influence of \nnoise.  \n5. CONCLUTION S \nIn this paper , we have proposed  a fast  Hamming space \nsearch  method for audio fingerprinting  systems . Our me-\nthod is inspired by Locality -Sensitive Hashing (LSH) , a \nprobabilistic  algorithm for solving the n earest neighbor  \nsearch  in high-dimensional spaces . LSH  uses multiple hash \nfunctions to maintain high retrieval accuracy  and therefore \nrequires a large amount of memory/storage for saving hash \ntable s. For the Hamming space search, LSH must maintain \nmultiple database set s created by random permutations. On \nthe other hand, the proposed m ethod created multiplexed \nsearch quer ies of sub -fingerprint sequence  with different \nstarting time , and does not require expansion of the dat a-\nbase. As a result, a large amount of memory/stora ge is not \nneeded. Experimental results showed that the proposed m e-\nthod delivers accurate, fast retrieval.  \n6. REFERENCES  \n[1] Gracenote: available from http://www.gracenote.com/ . \n[2] Midomi: available from http://www.midomi.co m/. \n[3] Jaap Haitsma and Ton Kalker : â€œHighly Robust Audio \nFingerprinting System ,â€ Proceedings of the 3rd \nInternational Conference on Music Information \nRetrieval (ISMIR  2002 ), pp.107â€“115, 2002.  \n[4] Avery Li -Chun Wang: â€œAn Industrial -Strength Audio \nSearch Algorithm ,â€ Proceedings of the 4th \nInternational Conference on Music Information \nRetrieval (ISMIR  2003 ), pp.7â€“13, 2003.  \n[5] Matthew L. Miller, Manuel Acevedo Rodriguez, and \nIngemar J. Cox : â€œAudio Fingerprinting:  Nearest  \nNeighbor Search in High Dimensional Binary Spaces ,â€ \nJournal  of VLSI Signal Processing , Vol. 41, No.  3, \npp.285 -291, 2005 . \n[6] Aristides Gionis, Piotr Indyk, and Rajeev Motwani : \nâ€œSimilarity Search in High Dimensions  via Hashing, â€ \n25th International Conference on Very Large Data \nBases  (VLDB 1999 ), pp.518â€“529, 1999 . \n137Poster Session 1\n  \n \n[7] Brian Kulis  and Trevor Darrell: â€œLearning to Hash \nwith Binary Reconstructive  Embeddings, â€ \nProceedings of the 23rd Annual Conference on Neural \nInformation  Processing Systems (NIPS 2009 ), \npp.1042â€“1050 , 2009.  \n[8] Brian Kulis and Kristen Grauman: â€œKernelized LSH  \nfor Scalable  Image Search, â€ Proceedings of the 12th \nIEEE International Conference on  Computer Vision \n(ICCV 2009 ), pp.2130â€“2137 , 2009.  \n[9] Dimitrios Fragoulis, George Rousopoulos, Thanasis \nPanagopoulos, Constantin  Alexiou, and Constantin \nPapaodysseus:  â€œOn the Automated Recognition of \nSeriously  Distorted Musical Recordings, â€ IEEE \nTransactions on Signal Processing , Vol. 49,  No. 4, \npp.898â€“908, 2001.  \n[10] Beth Logan: â€œMel Frequency Cepstral Coefficients for \nMusic Modeling, â€ Proceedings  of the International \nSymposium on Music Info rmation Retrieval (ISMIR \n2000 ), pp.11â€“23, 2000.  \n[11] Eric Allamanche et al. : â€œAudioID: Towards Content -\nbased Identification of Audio  Material,â€ Proceedings  \nof  the 1 10th AES Convention , 2001.  \n[12] Andrei Z. Broder, Moses Charikar, Alan M.  Frieze, \nand Michael Mitzenmacher:  â€œMin-wise Independent \nPermutations, â€ Proceedings of the 30th Annual ACM \nSymposium on Theory of Computing , pp.327â€“336, \n1998.  \n[13] Moses S. Charikar: â€œSimilarity Estimation Techniques \nfrom Rounding Algorithms, â€ Proceedings of the 34th \nAnnual ACM Sym posium on Theory of Computing , \npp.380â€“388, 2002.  \n[14] Mayur Datar, Nicole Immorlica, Piotr Indyk, and \nVahab S.  Mirrokni: â€œLocality -Sensitive Hashing \nScheme Based on p -Stable Distributions, â€ \nProceedings of the 20th Annual Symposium on \nComputational Geometry , pp.253â€“262, 2004.  \n[15] Piotr Indyk and Rajeev Motwani: â€œApproximate \nNearest Neighbors: Towards Removing  the Curse of \nDimensionality, â€ Proceedings of the 30th Annual \nACM Symposium on Theory of Computing , pp.604â€“\n613, 1998 . \n[16] Deepak Ravichandran, Patrick Pantel , and Eduard \nHovy: â€œRandomized Algorithms  and NLP: Using \nLocality Sensitive Hash Functions for High Speed \nNoun Clustering, â€ Proceedings of ACL , pp.622â€“629, \n2005.  [17] Gurmeet Singh Manku, Arvind Jain, and Anish Das \nSarma: â€œDetecting Near -Duplicates for Web Crawling, â€ \nProceedings of the 16th international conference on  \nWorld Wide Web , pp.141â€“149, 2007 . \n[18] Udi Manber and Gene Myers: â€œSuffix Arrays: A New \nMethod for On -line String  Searches, â€ SIAM Journal \non Computing , Vol.  22, No.  5, pp.935 â€“948, 1993 . \n \n \n \n \n \n \n \n \n138"
    },
    {
        "title": "Music Tagging with Regularized Logistic Regression.",
        "author": [
            "Bo Xie 0002",
            "Wei Bian",
            "Dacheng Tao",
            "Parag Chordia"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1415258",
        "url": "https://doi.org/10.5281/zenodo.1415258",
        "ee": "https://zenodo.org/records/1415258/files/XieBTC11.pdf",
        "abstract": "In this paper, we present a set of simple and efficient regularized logistic regression algorithms to predict tags of music. We first vector-quantize the delta MFCC features using k-means and construct â€œbag-of-wordsâ€ representation for each song. We then learn the parameters of these logistic regression algorithms from the â€œbag-of-wordsâ€ vectors and ground truth labels in the training set. At test time, the prediction confidence by the linear classifiers can be used to rank the songs for music annotation and retrieval tasks. Thanks to the convex property of the objective functions, we adopt an efficient and scalable generalized gradient method to learn the parameters, with global optimum guaranteed. And we show that these efficient algorithms achieve stateof-the-art performance in annotation and retrieval tasks evaluated on CAL-500.",
        "zenodo_id": 1415258,
        "dblp_key": "conf/ismir/XieBTC11",
        "keywords": [
            "delta MFCC features",
            "vector-quantization",
            "k-means",
            "bag-of-words representation",
            "logistic regression",
            "training set",
            "ground truth labels",
            "linear classifiers",
            "prediction confidence",
            "music annotation and retrieval tasks"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nMUSICTAGGING WITH REGULARIZED LOGISTIC REGRESSION\nBoXie\nGTCMT\nGeorgia Tech\nAtlanta, GA,USA\nbo.xie\n@gatech.eduWeiBian\nQCIS\nUniv ofTechSydney\nSydney,NSW,Australia\nbrian.weibian\n@gmail.comDachengTao\nQCIS\nUniv of TechSydney\nSydney,NSW,Australia\ndacheng.tao\n@uts.edu.auParagChordia\nGTCMT\nGeorgia Tech\nAtlanta, GA, USA\nppc\n@gatech.edu\nABSTRACT\nIn this paper, we present a set of simple and efï¬cient regu-\nlarized logistic regression algorithms to predict tags of m u-\nsic. We ï¬rst vector-quantize the delta MFCC features us-\ning k-means and construct â€œbag-of-wordsâ€ representation\nfor each song. We then learn the parameters of these logis-\ntic regression algorithms from the â€œbag-of-wordsâ€ vectors\nand ground truth labels in the training set. At test time, the\nprediction conï¬dence by the linear classiï¬ers can be used\nto rank the songs for music annotation and retrieval tasks.\nThankstotheconvexpropertyoftheobjectivefunctions,we\nadopt an efï¬cient and scalable generalized gradient method\nto learn the parameters, with global optimum guaranteed.\nAnd we show that these efï¬cient algorithms achieve state-\nof-the-artperformanceinannotationandretrievaltaskse val-\nuated onCAL-500.\n1. INTRODUCTION\nAutomatictaggingofmusicisapopulartopicinrecentyears ,\nwithapplicationsinmusicinformationretrieval,descrip tion\nof music, etc. The task is to associate a song with a few rel-\nevant labels (or tags), e.g. pop, male vocal and happy. We\nwant to predict conï¬dence values that accurately estimate\nthe strength of the association between the labels and audio\ncontents. Givenasong,theseconï¬dencevaluescanbeused\nto rank the tags by relevance, and this is the music anno-\ntation task. In the music retrieval task, we rank the songs\naccording totheirrelevance toa speciï¬c query tag.\nThe challenge mainly lies in two parts. One is how to\nrepresent a song or a song segment that best summarizes\nits content. The most popular audio feature is the Mel-\nFrequencyCepstralCoefï¬cient(MFCC)thatonlydescribes\nPermission to make digital or hard copies of all or part of this w ork for\npersonal or classroom use is granted without fee provided th at copies are\nnot made or distributed for proï¬t or commercial advantage and th at copies\nbear this noticeand thefull citation ontheï¬rst page.\nc/circlecopyrt2011International Society forMusic InformationRetrieval .a 23ms time window. While these very short â€œframesâ€ can-\nnot be used directly as features for songs, they make up\nthe building blocks for more advanced features. [7] sum-\nmarized the frame-level features over a segment by means\nand covariances and other features were combined by Ad-\naBoost. Spectral covariances over a segment were also pro-\nposed and achieved better results than means and covari-\nances of MFCC [6]. Other methods tried to estimate the\nprobability distribution of the MFCC feature space and use\nthis as song-level features [1,3]. At the same time, time se-\nries model [5] attempted to incorporate the temporal infor-\nmation but the complex structures in music are difï¬cult to\ncapture because of therichpatterns ofmultiple timescales .\nThe other difï¬culty is the multitude of the labels. The\nlarge number of tags and relatively few tags per song result\nin severe label imbalance, presenting a challenging prob-\nlem for most discriminative methods such SVM and Ad-\naBoost [7,13]. These methods tend to score high in classi-\nï¬cation by predicting most new test songs as negative sam-\nples. However, we found, with empirical evaluation, that\nlogistic regression appears to be more robust in such situa-\ntions in that it tries to maximize the conditional probabili ty\nrather than tominimize theclassiï¬cation errordirectly.\nCurrently, most state-of-the-art methods are probabilis-\ntic models. Gaussian Mixture Models (GMM) [3] approxi-\nmate the probability distribution of features conditioned on\neach tag with a mixture of Gaussian distributions. Then the\nBayesian rule is applied to calculate the posterior probabi l-\nity of a tag given a new song. One shortcoming of the gen-\nerative model is that it does not fully utilize the label info r-\nmation compared with discriminative methods. Recently, a\nmore â€œdiscriminative-ï¬‚avoredâ€ probabilistic model, Code -\nword Bernoulli Average (CBA) [1], was proposed and it\nachieved state-of-the-art performance on annotation and r e-\ntrieval tasks. Although CBA is efï¬cient and effective, the\nEM algorithm used in estimating its parameters only con-\nverges to a local optimum and as a result the learnt parame-\nterswilldepend on different initializations.\nWe propose to use regularized logistic regression to ad-\n711Poster Session 6\ndress the music tagging problem. First, song-level statist ics\nare summarized in the â€œbag-of-wordsâ€ of quantized delta\nMFCC features. Then, we apply logistic regression to learn\nthe correlations of tags and music content by exploiting the\nlabel information. Different regularization terms are inc or-\nporated in logistic regression to reduce overï¬tting and im-\nprove generalization. Our approach enjoys the beneï¬t of\nconvex optimization with global optimum guarantee. Also,\nby using ï¬rst-order methods, the proposed model can be\nlearnt in a short time and it scales linearly to large dataset .\nMoreover, experiments demonstrate that our regularized lo -\ngisticregressioncanachievestate-of-the-artperforman cein\nCAL-500 dataset [2].\n2. SONG-LEVEL FEATURE REPRESENTATION\nWechooseasimpleâ€œbag-of-wordsâ€representation,thesame\nas in [1,11] and many other image classiï¬cation algorithms\n[10], as our song-level feature. This simple representatio n\nfacilitates efï¬cient and scalable prediction of musictags for\nalarge set ofdata.\nOur primary features are the 39 dimension delta MFCC\nfeaturesover23mstime-window. EachdeltaMFCCfeature\nis concatenated from one MFCC feature, its ï¬rst derivative\nand its second derivative. As a preprocessing step, we ï¬rst\nnormalize all the delta MFCC features to have zero mean\nand unit variance in each dimension. We then apply k-\nmeans to learn Kcluster centroids as â€œaudio dictionaryâ€\nD= [d1,d2,Â· Â· Â·,dK]âˆˆRpÃ—Kin the pdimensional fea-\nture space, where p= 39. The centroids act as â€œrepresenta-\ntivesâ€ of typical audio frames.\nLet{vi,1,vi,2,Â· Â· Â·,vi,Ni}denotethesetofdeltaMFCC\nvectors for song i. We count the number of feature vectors\nforsong ithatarenearesttodictionaryitem djinEuclidean\ndistance\nni,j=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/braceleftbigg\nk:j= arg min\nt/baâˆ‡dblvi,kâˆ’dt/baâˆ‡dbl2\n2/bracerightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle.(1)\nThe counts ni,jcan be considered as a discrete approx-\nimation to the probability distribution on the feature spac e.\nComparedwiththeparametricmodel[3],ournon-parametric\nrepresentation ismoreï¬‚exible and easier toimplement.\nWe then normalize the counts to cancel out the effect\nof varying song lengths. The frequency of the j-th â€œaudio\nwordâ€ inthe i-thsong iscalculated as\nri,j=ni,j/summationtextK\nk=1ni,k. (2)\nFinally, the i-th song is represented as x(i)whose j-th\nelement is x(i)\nj=ri,j.\nThe most time consuming part of song-level feature rep-\nresentationisk-meansclustering. However,thisisdoneof f-\nline and can be speeded up by using a subset of samples orusing hierarchical clustering. When a new song arrives, we\njustneedtoassigneachofitsdeltaMFCCfeaturestooneof\nthecentroids andconstruct thehistogram,whosetimecom-\nplexity is linear inthenumber ofdelta MFCC features.\n3. LOGISTIC REGRESSION WITH\nREGULARIZATION\nGiven the â€œbag-of-wordsâ€ representation of each song, we\ntrain a linear classiï¬er to predict the labels. We choose lo-\ngistic regression because its loss function is less sensiti ve\nto noise and label imbalance compared with others, such as\nhinge lossinSVM or exponential lossinAdaBoost.\n3.1 Multi-label LogisticRegression\nIn the automatic music tagging problem, there are mla-\nbels/tags, and we want to learn a vector-valued prediction\nfunction f(x) = [f1(x),f2(x),Â· Â· Â·,fm(x)]T:X /mapstoâ†’ Y,\nwhere the input space Xis theKdimensional vector space\nofâ€œbag-of-wordsâ€andthelabelspace Yis{1,âˆ’1}m. Here,\nwe are interested in the family of linear classiï¬ers and f(x)\ncan be writtenas\nf(x) = sgn( Bx+c), (3)\nwhereB= [b1,b2,Â· Â· Â·,bm]TâˆˆRmÃ—Kis the coefï¬cient\nmatrix for the prediction function and câˆˆRmis the bias\nvector. Note that row l,bT\nl, is the classiï¬er coefï¬cients for\nthel-thlabel.\nWithlogisticregressionmodel,theconditionallikelihoo d\nPr(yl|x;bl,cl)is give by\nPr (yl|x;B,c) =1\n1 + exp/parenleftbig\nâˆ’yl/parenleftbig\nbT\nlx+cl/parenrightbig/parenrightbig.(4)\nAndthelearningofoptimalparameters (Bâˆ—,câˆ—)basedona\ntraining dataset D={(x(i),y(i)|i= 1,2,Â· Â· Â·,n)}can be\nperformed by minimizing the negative log likelihood plus a\nregularization term R(B),\n(Bâˆ—,câˆ—) = arg min\nB,câˆ’1\nnn/summationdisplay\ni=1m/summationdisplay\nl=1log Pr/parenleftBig\ny(i)\nl|x(i);B,c/parenrightBig\n+Î»R(B),\n(5)\nwhere Î»isaweighting parameter forthe regularization.\nTo predict the labels of a new song /hatwidex, we compute the\nconditional likelihood Pr (yl|/hatwidex;Bâˆ—,câˆ—)with Eq. 4, which\nshows theconï¬dence of thelabel yl.\n3.2 Different Regularizations\nRegularizationplaysanimportantroleinincorporatingpr ior\ninformation and reducing model complexity to avoid over-\nï¬tting. Adopting different regularization terms will lead to\nmodels withdifferent interpretations and performance.\n71212th International Society for Music Information Retrieval Conference (ISMIR 2011)\nAcommonchoiceisthe l2termthatcontainsmodelcom-\nplexity, i.e.\nR(B) =/baâˆ‡dblB/baâˆ‡dbl2\n2=K/summationdisplay\nj=1m/summationdisplay\ni=1B2\nij. (6)\nRecently, sparsity inducing norms are very popular and\nhave wide applications in machine learning and music in-\nformation retrieval [8,14]. So, we also consider l1norm\nregularizationthatencouragessparsityofmodelparamete rs.\nTechnically, the regularizer is\nR(B) =/baâˆ‡dblB/baâˆ‡dbl1=K/summationdisplay\nj=1m/summationdisplay\ni=1|Bij|. (7)\n4. FIRST-ORDER OPTIMIZATION METHOD\nWe adopt efï¬cient ï¬rst-order methods to learn the parame-\nters. Thanks to convexity, the convergence of our algorithm\ntoaglobal minimum isguaranteed.\n4.1 Gradient Descent for l2\nSince the original objective function with l2regularization\nissmooth, wecan update the parameter by gradient descent\nBt+1=Btâˆ’Î·(âˆ‡Ln(Bt) + 2Î»Bt),(8)\nct+1=ctâˆ’Î·âˆ‡Ln(ct), (9)\nwhere âˆ‡Ln(Â·)is the derivative of the loss function and Î·is\nthe stepsize.\n4.2 Generalized Gradient Descent for l1\nDue to the non-smoothness of l1norm, at iteration step t,\nwe update Bby\nBt+1= arg min\nZ/an}bâˆ‡acketle{tâˆ‡L n(Bt),Zâˆ’Bt/an}bâˆ‡acketâˆ‡i}ht\n+1\n2Î·/baâˆ‡dblZâˆ’Bt/baâˆ‡dbl2+Î»/baâˆ‡dblZ/baâˆ‡dbl1,(10)\nwhere Î· >0and1/Î·issetlargerthantheLipschitzconstant\nofâˆ‡Ln[9]. Hereweomit cbecause itisnot inthe l1norm\nand can besolved bystandard gradient descent (Eq. 9).\nThe above procedure is the generalized gradient descent\nschemebecausewhen Î»= 0,itiseasytoseeEq.10reduces\ntoBt+1=Btâˆ’Î·âˆ‡Ln(Bt).\nDenote Bt+1= [bâˆ—\n1,bâˆ—\n2,Â· Â· Â·,bâˆ—\np],Bt= [b1,b2,Â· Â· Â·,bp]\nandâˆ‡Ln(Bt) = [h1,h2,Â· Â· Â·,hp]andEq.10canbesolved\nbypseparate sub-problems. According to [9], each sub-\nproblem issolved by\nbâˆ—\nj=TÎ»Î·(bjâˆ’Î·hj), (11)where TÎ±(Â·)is the soft thresholding operator. And it is de-\nï¬ned by\nTÎ±(x)i= (|xi| âˆ’Î±)+sgn(xi), (12)\nwhere (x)+=xifx >0and(x)+= 0otherwise.\nThedetailedprocedureofgeneralizedgradientdescentis\nillustratedinAlg. 1.\nAlgorithm 1 Generalized Gradient Descent Algorithm\nInput: Training set D={(x(i),y(i)|i= 1,2,Â· Â· Â·,n)}\nOutput: Model parameters Bâˆ—âˆˆRmÃ—p,câˆ—âˆˆRm\nInitialize t= 0,Î·,B0,c0\nRepeat until convergence:\n1. Compute thepartial gradient âˆ‡BLn(Bt,ct).\n2. For j= 1top\n2.1 Calculate w=bjâˆ’Î·hj.\n2.2 Calculate the j-thcolumn of Bt+1byTÎ»Î·(w).\n3. Compute thepartial gradient âˆ‡cLn(Bt,ct).\n4. Update ct+1=ctâˆ’Î·âˆ‡cLn(Bt,ct).\n5. EXPERIMENTS ON ANNOTATION AND\nRETRIEVAL\nWeevaluatedourthreeversionsoflogisticregressionontw o\ntasks: music annotation and retrieval. Compared with bi-\nnary classiï¬cation tasks, these two tasks are more closely\nrelated withreal scenarios.\nThemusicdatacomesfromCAL-500Dataset[2]. There\nare500Westernpolyphonicsongsandtheannotationswere\ncollected from more than three human subjects per song.\nWhen training the classiï¬er, we only use the binary anno-\ntations with {0,1}(transformed to {âˆ’1,1}for learning) to\nindicate whether thetag isrelevant tothe song.\nWe are more interested in predicting more â€œusefulâ€ tags\nrather than very obscure ones. Following the same setting\nin [4,5], we only evaluate on the 78 tags that have at least\n50 examples and 97 toppopular tags.\n5.1 Annotation and Retrieval\nUsing similar experimental setting as in [4,5], we used 5-\nfold cross validation. In each round, we ï¬rst learned our\nmodelparameters Bâˆ—,câˆ—withthe400-songtrainingsetand\npredictedconï¬denceratingsontheremaining100-songtest\nset. The conditional probability (conï¬dence rating) of a ta g\nbeing assigned to a song was then calculated using Eq. 4.\nTo compensate for non-uniform label prior, we adopted the\nsameheuristicusedin[1]byintroducingaâ€œdiversityfacto râ€\n713Poster Session 6\nModel Precision Recall F-score P3 P5 P10 MAP AROC\nCBA 0.361 0.212 0.267 0.463 0.458 0.440 0.425 0.691\nGMM 0.405 0.202 0.269 0.456 0.455 0.441 0.433 0.698\nContext-SVM 0.380 0.230 0.286 0.512 0.487 0.449 0.434 0.687\nDirMix 0.441 0.232 0.303 0.519 0.501 0.470 0.443 0.697\nLogRegr 0.396 0.196 0.262 0.407 0.428 0.424 0.404 0.671\nl1LogRegr 0.416 0.202 0.272 0.414 0.413 0.417 0.411 0.673\nl2LogRegr 0.446 0.227 0.301 0.515 0.512 0.485 0.459 0.719\nTable 1. Experimental results for top 97 popular tags. The results o f Codeword Bernoulli Average (CBA), Gaussian Mixture\nModels(GMM),Context-SVMandDirichletMixture(DirMix)a rereportedin[4]. Ourresultsarenon-regularized(LogReg r),\nl1regularized ( l1LogRegr) and l2regularized ( l2LogRegr) logisticregressions, respectively.\nModel P R F-score AROC MAP P10\nCBA 0.41 0.24 0.29 0.69 0.47 0.49\nHEM-GMM 0.490.23 0.26 0.66 0.45 0.47\nHEM-DTM 0.47 0.25 0.30 0.69 0.48 0.53\nLogRegr 0.44 0.23 0.30 0.67 0.45 0.48\nl1LogRegr 0.46 0.23 0.31 0.68 0.46 0.49\nl2LogRegr 0.48 0.26 0.34 0.72 0.50 0.54\nTable 2. Experimental results for top 78 popular tags. The\nresults of Codeword Bernoulli Average (CBA), hierarchical\nEMGaussianMixtureModels(HEM-GMM)andhierarchi-\ncal EM Dynamic Texture Model (HEM-DTM) are reported\nin [5]. Our results are non-regularized (LogRegr), l1regu-\nlarized( l1LogRegr)and l2regularized( l2LogRegr)logistic\nregressions, respectively.\nd= 1.25. For each predicted conï¬dence rating, we sub-\ntracted dtimes the mean conï¬dence for that tag. We then\nassigned each song withthetop10 most conï¬dent tags.\nAnnotation was evaluated by mean precision and recall\nover the tags. Given the 10 annotations per song in the test\nset, we calculated precision and recall for each tag and then\naveraged across all considered tags. The ï¬nal result was\naveraged over 5 rounds of cross validation. In addition, F-\nscore, the harmonic mean of precision and recall, was com-\nputed tosummarize thetwoaspects of precisionand recall.\nFor retrieval, we ï¬rst ranked the songs in the descend-\ning order according to conï¬dence ratings for a speciï¬c tag.\nBetter retrieval result corresponds to cases that more rele -\nvant songs appear at the top of the ranking list. Then, we\ncalculated precision at every position down the ranking lis t\nvia dividing the number of true positives found so far by\nthe total number of songs so far. Evaluation was conducted\nthroughaveragedprecisionand precisionat k(k= 3,5,10)\nas in [4]. Averaged precision was computed by taking the\naverageofallthepositionsdowntherankinglistwherenew\ntrue positives were found. Precision at kwask-th precisionthat we calculated on theranking list.\n5.2 Experiment Results and Dicussions\n5.2.1 Comparison withState-of-the-art\nWecompareourresultswithstate-of-the-artperformanceo n\nthe CAL-500 dataset. For the 97 tags setting, we compare\nwith CBA [1], GMM [3], Context-SVM [12] and Dirich-\nlet Mixtures (DirMix) [4]. Their results were originally re -\nported in [4] and are copied in Table 1 for more convenient\ncomparison. Forthe78tagssetting,CBA,HEM-GMM(the\nsame as GMM) and HEM-DTM [5] were compared. Their\noriginal resultsreported in[5]and copied inTable 2.\nTheresultsofourthreevariantsoflogisticregressionun-\nder the 97 tags setting are also reported in Table 1. All our\nmethods were based on K= 2000dictionary size â€œbag-\nof-wordsâ€ representation, with the cluster centroids trai ned\non a random subset of 100,000 samples from all the delta\nMFCCfeaturesprovidedinthedataset. Non-regularizedlo-\ngistic regression was equivalent to setting Î»= 0. The pa-\nrameter Î»in the two regularized algorithms were set to the\noptimum. For l1logistic regression, it was set to 0.001 and\nforl2logisticregression, it was setto0.01.\nFrom Table 1, we can see that non-regularized logistic\nregression performed the worst but still had reasonable re-\nsults.l1regularizationimprovedtheperformanceby0.01or\n0.02forsomemeasures. l2regularizationintroducedgreater\nimprovement over the l1regularized variant, achieving best\nperformance in retrieval even than the state-of-the-art. A nd\nitwascomparablewiththeDirichletMixturemodelinanno-\ntation task. Note that theDirichlet Mixture model exploite d\nthe label correlations explicitly while our method incorpo -\nrated nosuch schemes toutilizecontext information.\nFor the 78 tags case illustrated in Table 2, the simple lo-\ngistic regression performed better than CBA in the annota-\ntion task. l1regularization consistently improved the per-\nformance by 0.01 for most measures. Again, l2regular-\nized logistic regression outperformed other approaches in\n71412th International Society for Music Information Retrieval Conference (ISMIR 2011)\nall measures except for precision. However, by comparing\nthe F-score which summarizes the overall annotation score,\nall three variants performed better than or on par with the\nstate-of-the-art. Considering thefact that HEM-DTM bene-\nï¬ted from information over the 23ms time window, our al-\ngorithmsâ€™ performance areeven moreencouraging.\nThe performance of non-regularized logistic regression\nwas limited because of the overï¬tting effect. l1regulariza-\ntionslightlyimprovedthesituationbyconstrainingtheco m-\nplexityoftheparameters. However,itappearsthattheâ€œbag -\nof-wordsâ€ representation does not have the hidden sparse\nstructurewhich l1normregularizationcanhelpreveal. Rather,\nthe classiï¬er coefï¬cients should be dense to fully take into\naccount all the details in the distribution. The l2norm was\nthussuitableforsuchsituationwhereitconstrainedthemo del\ncomplexity ingeneral and produced non-zero coefï¬cients.\n5.2.2 Effect of Changing Dictionary Size K\nWe also explored the effect of different dictionary sizes K.\nIntheexperiments,weran l2regularizedlogisticregression\nwithÎ»set to 0.01 and under different K(10, 20, 50, 100,\n200, 500, 800, 1000, 2000 and 5000). Fig. 1 illustrates the\nperformance on the two tag number settings for annotation\nand retrieval tasks.\nFrom Fig. 1, we can see that as Kincreases, the algo-\nrithmbeneï¬tsfrommoreaccurateapproximationtothedis-\ntribution and achieves better performance. The biggest im-\nprovement occurs from 10 to 100 dictionary sizes. It ap-\npearsthatwhen Kincreasesoverthisrange,themajorstruc-\nture in the distribution has been captured by the â€œbag-of-\nwordsâ€representation. Aswegoontomodeltheï¬nerscales\nwith even larger K, the performance continues to climb up\nuntil it gradually levels off when Kexceeds 2000. From\nK= 2000toK= 5000, the improvement is less than 0.01\nfor retrieval while the computational cost is multiplied by\n2.5 times. Therefore, we choose K= 2000as our optimal\ndictionary sizeintheCAL-500 dataset.\n5.2.3 Effect of Different Regularization Parameter Î»\nThe regularization parameter Î»affects the performance by\nbalancing the loss function and the regularization. Smalle r\nÎ»leads to more focus on the empirical error while larger Î»\nplaces more priorityon keeping the model complexity low.\nWevaried Î»from10âˆ’5to10withequalstepsizeinloga-\nrithmscalefor l2regularized logisticregressionunder K=\n2000. The effect is demonstrated in Fig. 2. For l2regular-\nized logistic regression, the optimal Î»is 0.01. And we can\nsee that the algorithm is relatively robust to the parameter\nchangefrom 10âˆ’4to10âˆ’1. Notethatsincethevaluesinthe\noriginal normalized â€œbag-of-wordsâ€ representation are to o\nsmall, making them badly scaled compared with the bias,\nwemultiplytheâ€œbag-of-wordsâ€by100andtheparameter Î»\nisreported after such preprocessing.10 20 50 100 200 500 800 1000 2000 5000 0.35 0.4 0.45 0.5 \nDictionary Size K Precision \n  \n97 tags \n78 tags \n10 20 50 100 200 500 800 1000 2000 5000 0.66 0.67 0.68 0.69 0.7 0.71 0.72 0.73 \nDictionary Size K AROC \n  \n97 tags \n78 tags 10 20 50 100 200 500 800 1000 2000 5000 0.18 0.2 0.22 0.24 0.26 0.27 \nDictionary Size K Recall\n  \n97 tags \n78 tags \n10 20 50 100 200 500 800 1000 2000 5000 0.4 0.42 0.44 0.46 0.48 0.5 0.52 \nDictionary Size K MAP \n  \n97 tags \n78 tags (a) (b) \n(c) (d) \nFigure 1. Effect of varying dictionary size K. The perfor-\nmance is evaluated on l2LogRegr with optimal parameter\nsetting. (a)Annotationperformance: precision;(b)Annot a-\ntion performance: recall; (c) Retrieval performance: mean\naveraged precision; (d) Retrieval performance: area under\nthe receiver operating characteristic curve.\n6. CONCLUSIONS\nWeproposedtouseregularizedlogisticregressionalgorit hms\ntoautomaticallytagmusic. Ourapproachenjoysconvexfor-\nmulations and can be solved efï¬ciently by ï¬rst-order meth-\nods. The convergence of our algorithm is guaranteed and it\nis scalable to large dataset. Empirical evaluation for musi c\nannotation and retrieval on the CAL-500 dataset has shown\nthatl2regularized version with â€œbag-of-wordsâ€ representa-\ntionofquantizeddeltaMFCCfeaturesachievesstate-of-th e-\nart performance.\nCurrently,nolabelcorrelationsareconsideredinourfram e-\nwork and learning is done independently for each label. In\nfuture work, we are interested in modeling such correla-\ntions by using structure inducing norms for regularization .\nAlso, instead of k-means clustering, dictionary learning a p-\nproachesarepromisinginthatmoreadaptiveâ€œaudiowordsâ€\ncan be learnt from data.\n7. REFERENCES\n[1] M. Hoffman, D. Blei and P. Cook: â€œEasy as CBA: A\nSimple Probabilistic Model for Tagging Music,â€ Pro-\nceedings of the 10th International Conference on Music\nInformationRetrieval (ISMIR) ,2009.\n[2] D. Turnbull, L. Barrington, D. Torres and G. Lanckriet:\nâ€œTowards Musical Query-by-Semantic Description us-\n715Poster Session 6\n10 -4 10 -2 10 00.05 0.15 0.25 0.35 0.45 0.5 \nParameterAnnotation Performance \n  \nPrecision \nRecall\n10 -4 10 -2 10 000.1 0.2 0.3 0.4 0.45 \nParameterAnnotation Performance \n  \nPrecision \nRecall\n10 -4 10 -2 10 00.3 0.4 0.5 0.6 0.7 0.75 \nParameterRetrieval Performance \n  \nMAP \nAROC \n10 -4 10 -2 10 00.25 0.35 0.45 0.55 0.65 \nParameterRetrieval Performance \n  \nMAP \nAROC \n(a) (c) (d) (b) \nFigure 2. Effect of varying regularizaiton parameter Î». The performance is evaluated on l2LogRegr with K= 2000. (a)\nAnnnotationperformancefor78tagssetting;(b)Retrieval performancefor78tagssetting;(c)Annnotationperforman cefor97\ntags setting; (d)Retrieval performance for97 tags setting .\ningtheCAL500DataSet,â€ ACMSpecialInterestGroup\nonInformationRetrievalConference(SIGIR'07) ,2007.\n[3] D. Turnbull, L. Barrington, D. Torres and G. Lanck-\nriet: â€œSemantic Annotation and Retrieval of Music and\nSound Effects,â€ IEEE Transactions on Audio, Speech,\nand Language Processing , 2008.\n[4] R. Miotto, L. Barrington and G. Lanckriet: â€œImproving\nAuto-Tagging by Modeling Semantic Co-Occurrences,â€\nProceedings of the 11th International Conference on\nMusicInformation Retrieval , 2010.\n[5] E. Coviello, A. Chan, L. Barrington and G. Lanckriet:\nâ€œAutomatic Music Tagging With Time Series Models,â€\nProceedings of the 11th International Conference on\nMusicInformation Retrieval , 2010.\n[6] J.Bergstra,M.MandelandD.Eck: â€œScalablegenreand\ntag prediction with spectral covariance,â€ Proceedings of\nthe11thInternationalConferenceonMusicInformation\nRetrieval, 2010.\n[7] J. Bergstra, N. Casagrande, D. Erhan, D. Eck, and B.\nKegl: â€œAggregatefeaturesandAdaBoostformusicclas-\nsiï¬cation,â€ Machine Learning , 2006.\n[8] Y. Panagakis, C. Kotropoulos and G. Arce.: â€œMu-\nsic genre classiï¬cation using locality preserving non-\nnegative tensor factorization and sparse representa-\ntions,â€Proceedingsofthe10thInternationalConference\nonMusicInformation Retrieval (ISMIR) ,2009.\n[9] A. Beck and M. Teboulle: â€œA Fast Iterative Shrinkage-\nThresholding Algorithm for Linear Inverse Problems,â€\nSIAM Journal onImaging Sciences , 2009.\n[10] G. Csurka, C. Dance, L.X. Fan, J. Willamowski and C.\nBray: â€œVisual categorization with bags of keypoints,â€\nProc. of ECCV International Workshop on Statistical\nLearning inComputer Vision , 2004.[11] M. Hoffman, D. Blei, and P. Cook: â€œContent-based\nmusical similarity computation using the hierarchical\nDirichletprocess,â€ InProc.InternationalConferenceon\nMusicInformation Retrieval , 2008.\n[12] S.R. Ness, A. Theocharis, G. Tzanetakis and L.G. Mar-\ntins: â€œImproving automatic music tag annotation using\nstackedgeneralizationofprobabilisticSVMoutputs,â€ In\nProceedings of ACM Multimedia , 2009.\n[13] D. Turnbull and C. Elkan: â€œFast recognition of musi-\ncal genres using RBF networks,â€ IEEE Transactions on\nKnowledge and DataEngineering , 2005.\n[14] K. Koh, S.J. Kim and S. Boyd: â€œAn Interior-Point\nMethodforLarge-Scale l1-RegularizedLogisticRegres-\nsion,â€Journal of Machine Learning Research , 2007.\n716"
    },
    {
        "title": "A Vocabulary-Free Infinity-Gram Model for Nonparametric Bayesian Chord Progression Analysis.",
        "author": [
            "Kazuyoshi Yoshii",
            "Masataka Goto"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1417389",
        "url": "https://doi.org/10.5281/zenodo.1417389",
        "ee": "https://zenodo.org/records/1417389/files/YoshiiG11.pdf",
        "abstract": "This paper presents probabilistic n-gram models for symbolic chord sequences. To overcome the fundamental limitations in conventional modelsâ€”that the model optimality is not guaranteed, that the value of n is fixed uniquely, and that a vocabulary of chord types (e.g., major, minor, Â· Â· Â· ) is defined in an arbitrary wayâ€”we propose a vocabulary-free infinity-gram model based on Bayesian nonparametrics. It accepts any combinations of notes as chord types and allows each chord appearing in a sequence to have an unbounded and variable-length context. All possibilities of n are taken into account when calculating the predictive probability of a next chord given a particular context, and when an unseen chord type emerges we can avoid out-of-vocabulary error by adaptively evaluating the 0-gram probability, i.e., the combinatorial probability of note components. Our experiments using Beatles songs showed that the predictive performance of the proposed model is better than that of the state-of-theart models and that we could find stochastically-coherent chord patterns by sorting variable-length n-grams in a line according to their generative probabilities.",
        "zenodo_id": 1417389,
        "dblp_key": "conf/ismir/YoshiiG11",
        "keywords": [
            "probabilistic n-gram models",
            "symbolic chord sequences",
            "Bayesian nonparametrics",
            "vocabulary-free",
            "infinity-gram model",
            "conventional models limitations",
            "combinatorial probability",
            "stochastically-coherent chord patterns",
            "predictive probability",
            "unseen chord type handling"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nA VOCABULARY-FREE INFINITY-GRAM MODEL\nFOR NONPARAMETRIC BAYESIAN CHORD PROGRESSION ANALYSIS\nKazuyoshi Yoshii Masataka Goto\nNational Institute of Advanced Industria l Science and Technology (AIST), Japan\n{k.yoshii, m.goto }@aist.go.jp\nABSTRACT\nThis paper presents probabilistic n-gram models for sym-\nbolic chord sequences. To overcome the fundamental lim-itations in conventional modelsâ€”that the model optimality\nis not guaranteed, that the value of nis ï¬xed uniquely, and\nthat a vocabulary of chord types (e.g., major, minor, Â·Â·Â·)i s\ndeï¬ned in an arbitrary wayâ€”we propose a vocabulary-free\ninï¬nity-gram model based on Bayesian nonparametrics. It\naccepts any combinations of notes as chord types and allows\neach chord appearing in a sequence to have an unboundedand variable-length context. All possibilities of nare taken\ninto account when calculating the predictive probability of\na next chord given a particular context, and when an unseen\nchord type emerges we can avoid out-of-vocabulary error by\nadaptively evaluating the 0-gram probability, i.e., the com-\nbinatorial probability of note components. Our experiments\nusing Beatles songs showed that the predictive performanceof the proposed model is better than that of the state-of-the-\nart models and that we could ï¬nd stochastically-coherent\nchord patterns by sorting variable-length n-grams in a line\naccording to their generative probabilities.\n1. INTRODUCTION\nChord progression analysis is an important task for content-\nbased music information retrieval (MIR) [1,2]. Because the\nchord patterns used in musical pieces are closely related to\nthe composer styles [3] and musical genres [4], it is usefulto build statistical models of chord patterns from symbolic\nchord sequences. In addition, accurate models of chord se-\nquences (called language models in analogy with automatic\nspeech recognition) could improve the accuracy of auto-\nmatic chord recognition for music audio signals [5, 6].\nSo far,n-gram models have often been used as language\nmodels of chord sequences [2â€“6]. An n-gram is a subse-\nquence of nchords in a given chord sequence, and n-gram\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted w ithout fee provided that copies are\nnot made or distributed for proï¬t or c ommercial advantage and that copies\nbear this notice and the full citation on the ï¬rst page.\nc/circlecopyrt2011 International Society for Music Information Retrieval.F:maj G:7 C:majG:7 C:majC:maj1-gram probability\n2-gram probability\n3-gram probabilityP(C:maj| Ï†)\nP(C:maj|G:7)\nP(C:maj|F:maj G:7)0-gram probability (base measure)\nP(C:maj)Bayesian\nsmoothing\nContext Next chordBayesian\nsmoothing\nBayesian\nsmoothing\nFigure 1 . A hierarchical nonparametric Bayesian model for\naccurately smoothing n-gram probabilities.\nmodels are based on ( nâˆ’1)-order Markovian assumption be-\ncause chords exhibit strong short-term dependency. In other\nwords, each chord in a given sequence is assumed to de-\npend on its nâˆ’1previous chords called a context .U s i n g\na limited amount of observed data, the goal is to make a\nstatistical model that can calculate the predictive probability\nof a next chord ( n-gram probability), given any context of\nlengthnâˆ’1. However, the observed n-grams are generally a\nlimited subset of all kinds of n-grams, and the number of all\nkinds ofn-grams increases exponentially with increasing n.\nTherefore, the naive estimates of the probabilities of unob-servedn-grams are zero. To avoid such overï¬tting, various\nheuristic smoothing methods have been developed [7].\nIn this paper we focus on three fundamental limitations\nof conventional n-gram models: 1) n-gram models based on\nheuristic smoothing methods have no solid theoretical foun-\ndation, 2) the value of nshould be speciï¬ed uniquely in ad-\nvance even though each chord depends on a variable-length\ncontext, 3) A limited set of chord labels (e.g., major, minor,\naugmented, diminished, seventh, Â·Â·Â·, and their derivations)\nshould be deï¬ned as a vocabulary in advance. Especially,\nthe last limitation has not been discussed so far.\nTo overcome these limitations, we propose a vocabulary-\nfree inï¬nity-gram model by extending modern nonparamet-\nric Bayesian n-gram models [8â€“10]. Our model is formu-\nlated in a hierarchical Bayesian manner (Figure 1) and has\nthe following merits: 1) The predictive distribution of a next\nchord can be naturally formalized by providing the proba-\nbilistic generative model of chord sequences. 2) Each chord\nin a sequence is allowed to have an unbounded and variable-length context. A posterior distribution of the context length\n645Oral Session 8: Chord Analysis\ncan be estimated. 3) Any combinations of notes can be ac-\ncepted as chord types. A chord vocabulary is incrementally\nexpanded as needed. These metits enable our model to not\nonly attain the best performance but also ï¬nd â€œstochastically-\ncoherentâ€ variable-length chord patterns that are not always\nsimply the ones used most frequently (cf. [11]).\nThe innovative models of symbolic chord sequences (an\ninï¬nity-gram model and its vocabulary-free extension) are\nuseful for probabilistic modeling of music audio signals. A\ntypical application is automatic chord recognition, where a\nvocabulary of chord labels is given. For example, an inï¬nity-\ngram model could be fused with a joint probabilistic model\nof keys, chords, and bass notes [12]. Another novel applica-\ntion is automatic music transcription, where a vocabulary isnotgiven. We plan to use a vocabulary-free model as a prior\ndistribution on a probabilistic acoustic model for multipitch\nestimation [13], and jointly optimize the both models. This\nmeans that chords and their progressions (now â€œchordsâ€ are\ncombinations of notes, not text labels) are self-organized in\nan unsupervised manner and are used as a constraint on si-\nmultaneous and temporal pitch distributions.\nThe rest of this paper is organized as follows: Section 2\ndescribes the chord notations used in this study. Section 3\nintroduces related work on nonparametric Bayesian n-gram\nmodels and Section 4 explains our model. Section 5 reports\nour experiments and Section 6 concludes this paper.\n2. CHORD NOTATIONS\nWe introduce label-based and component-based notations to\nrepresent chord sequences (Table 1).\n2.1 Label-based Notation\nThe conventional label-based notation is based on intuitive\nshorthand labels deï¬ned by Harte et al. [14]. There are 17\nchord labels with an attached root note, which is one of 12pitch classes.\n1In this paper we do not distinguish C# from\nDb because they are in the same pitch class. This is a stan-\ndard treatment used in [2, 3]. For example, C major and\nGb diminished seventh chords are respectively represented\nas C:maj and F#:dim7. The symbol â€œNâ€ is used to indicate\nâ€œno chordâ€ (e.g., silence or untuned sounds). The resulting\nvocabulary size is 205 ( 17Ã—1 2+1 ).\n2.2 Component-based Notation\nThe component-based notation is based on degrees of note\ncomponents (relative displacements against a root note). Each\nchord is represented as a combination of a root note and a\n12-dimensional binary vector whose elements indicate the\nexistences of the corresponding degrees. For example, C\nmajor chords are written as C:100010010000 and D major\nchords as D:100010010000, not as D:001000100100. Note\nthat any combinations of notes can be represented even if\n1The pitch classes are deï¬ned as 12 different scales within an octave,\ni.e.,{C ,C # ,D ,D # ,E ,F ,F # ,G ,G # ,A ,A # ,B }.\nChord type\n Label Components\nMajor\n maj 100010010000\nMinor\n min 100100010000\nDiminished\n dim 100100100000\nAugmented\n aug 100010001000\nMajor Seventh\n maj7 100010010001\nMinor Seventh\n min7 100100010010\nSeventh\n 7 100010010010\nDim. Seventh\n dim7 100100100100\nHalf Dim. Seventh\n hdim7 100100100010\nMin. (Maj. Seventh)\n minmaj7 100100010001\nMajor Sixth\n maj6 100010010100\nMinor Sixth\n min6 100100010100\nNinth\n 9 101010010010\nMajor Ninth\n maj9 101010010001\nMinor Ninth\n min9 101100010010\nSuspended Second\n sus2 101000010000\nSuspended Fourth\n sus4 100001010000\nTable 1 . Shorthand labels and pitch-class components\nthey are not deï¬ned in Table 1. For example, C major chords\nwith an added fourth are written as C:100011010000. Such\ninformation is available in Harteâ€™s chord annotations [14].\nWith the additional symbol â€œNâ€, the resulting vocabularysize is 49153 ( 2\n12Ã—12 + 1 ). This is ï¬nite because we fo-\ncus on note existences in individual pitch classes. Note that\na truly vocabulary-free (inï¬nite-vocabulary) notation can be\ndeï¬ned by focusing on note counts based on musical scores,\ni.e., by representing note components of each chord as a 12-\ndimensional nonnegative-integer vector.\n3. PROBABILISTIC LANGUAGE MODELS\nThis section introduces related work on n-gram models. We\nï¬rst identify the purpose of n-gram modeling and then ex-\nplain several state-of-the-art models based on the probability\ntheory of Bayesian nonparametrics.\n3.1 Problem Speciï¬cation\nSuppose we have a chord vocabulary Ww h o s es i z ei s V\n(in this paper, 205 or 49153). Let wâˆˆWbe a chord and\nuâˆˆWnâˆ’1,w h e r encan be anypositive integer, be a con-\ntext consisting of a sequence of nâˆ’1chords. We have a\nlimited amount of observed data X, which is a sequence of\nMchords,x1x2Â·Â·Â·xM,w h e r exmâˆˆW(1â‰¤mâ‰¤M).\nWe assume for simplicity that we have only one chord se-\nquence. In n-gram modeling, each chord xmis assumed to\ndepend on the past nâˆ’1chords (context).\nGiven observed data X, the goal is to estimate Pu(w|X),\ni.e., the predictive probability of chord wfollowing context\nu.L e tcuwbe the number of occurrences of chord wfol-\nlowing context uin training data X. The naive maximum\nlikelihood (ML) estimate is given by\nPML\nu(w|X)=cuw\ncuÂ·(1)\nwhere the dot ( Â·) means the sum over that index, i.e., cuÂ·=/summationtext\nw/primecuw/prime.H o w e v e r , i f n-gramuwis not observed in X\n64612th International Society for Music Information Retrieval Conference (ISMIR 2011)\n(cuw=0), its probability is estimated to be zero. This is\ncalled the zero-probability problem.\nTo solve this problem various smoothing methods have\nbeen proposed. The family of Kneser-Ney (KN) smoothing\nis empirically known as one of the most accurate smoothing\ntechniques [7]. A method called interpolated KN (IKN) es-\ntimatesPu(w|X)by discounting the actual count cuwby\na ï¬xed amount d|u|depending on the context length |u|\nifcuw>0(otherwise the count remains 0). Furthermore,\nthe discounted n-gram probability of chord wis interpo-\nlated with the ( nâˆ’1)-gram probability of chord w. Another\nimportant variant is called modiï¬ed KN (MKN), where the\namount of discount is allowed to vary according to the value\nofcuw. MKN is known to slightly outperform IKN.\n3.2 Hierarchical Pitman-Yor Language Model\nTeh [8] proposed a nonparametric Bayesian n-gram model\ncalled a hierarchical Pitman-Yor language model (HPYLM).\nInterestingly, IKN was proven to be a deterministic approx-\nimation of the HPYLM, which can be optimized in a princi-\npled way and performs better than IKN.\n3.2.1 Pitman-Yor Process and Hierarchical Formulation\nWe brieï¬‚y explain the Pitman-Yor process (PY) [15], which\nis a building block of nonparametric Bayesian models. The\nPY is a distribution over distributions (e.g., n-gram distribu-\ntions) over a sample space (e.g., vocabulary W). Letdand\nÎ¸be positive real numbers and G0be a distribution over a\nsample space. The PY is written as\nGâˆ¼PY(d,Î¸,G 0) (2)\nwheredis called a discount parameter, Î¸a strength parame-\nter, andG0a base measure. Gis a random distribution over\nthe sample space. When the value of Î¸becomes larger, Gis\nmore likely to be similar to G0.\nThe HPYLM is formulated by layering PYs in a hierar-\nchical Bayesian manner. Suppose we have a unigram distri-butionG\nÏ†overW,w h e r eÏ†is the empty context and GÏ†(w)\nis the unigram probability of chord w. A bigram distribution\nGugiven the last chord udiffers from but is somewhat sim-\nilar toGÏ†.H e r eGuis assumed to be drawn from a PY with\nbase measure GÏ†asGuâˆ¼PY(d1,Î¸1,GÏ†),w h e r ed1andÎ¸1\nare discount and strength parameters that are shared among\ncontexts of length 1. Generally speaking, an n-gram distri-\nbutionGugiven a context uof length nâˆ’1is drawn from a\nPY with base measure GÏ€(u)as follows:\nGuâˆ¼PY(d|u|,Î¸|u|,GÏ€(u)) (3)\nwhereÏ€(u)is a shortened context obtained by removing\nthe earliest chord from u,a n dd|u|andÎ¸|u|are discount and\nstrength parameters depending on the length |u|. Since the\n(nâˆ’1)-gram distribution GÏ€(u)is unknown, a PY prior with\nparameters d|Ï€(u)|andÎ¸|Ï€(u)|and base measure GÏ€(Ï€(u))\nis recursively put on GÏ€(u). Finally, the unigram distribu-\ntionGÏ†is given by GÏ†âˆ¼PY(d0,Î¸0,G0)whereG0is aF:maj\nC:maj D:minC:majDepth 0\nDepth 1\nDepth 2Original customer\nOriginal customer \nwho sends a proxy\nProxy customer\nProxy customer \nwho sends a proxyâ€¦â€¦ Back-track\nthe context\nSend \na proxy\nSend \na proxyÏ†\nâ€¦\nFigure 2 . Hierarchical Pitman-Yor language model.\nglobal base measure (0-gram distribution), which is usually\nassumed to be uniform, i.e., G0(w)=1/V.\nConsequently, the hierarchical structure of the HPYLM\ncan be represented as a sufï¬x tree of depth nâˆ’1,a ss h o w ni n\nFigure 2 where the case of n=3is illustrated. Each node is\nidentiï¬ed as a context, i.e., descending the tree from the root\nnode to the target node means back-tracking the context.\n3.2.2 Stochastic Process for Data Generation\nOnce the HPYLM is deï¬ned, observed data Xis generated\naccording to a stochastic process called the Chinese restau-\nrant franchise (CRF), which can be explained by using a\nmetaphor in which contexts are likened to restaurants ,M\nobserved variables in Xare likened to customers ,a n dV\nchord types in Ware likened to dishes . Each restaurant is\nallowed to have an unbounded number of tables and each ta-\nble is served a dish. Each customer enters a restaurant, sits\nat a table, and eats a dish served at that table.\nWe suppose that x1,Â·Â·Â·,xMare generated sequentially,\nand consider how the m-th customer xmbehaves, given a\nseating arrangement of the past customers {x1,Â·Â·Â·,xmâˆ’1}.\nThe customer xmenters restaurant u=xmâˆ’(nâˆ’1)Â·Â·Â·xmâˆ’1\nof depthnâˆ’1.L e ttuwbe the number of tables serving dish\nwin restaurant u.T h e r ea r e tuÂ·tables in total. Let cuwkbe\nthe number of customers sitting at table kand eating dish w\n(cuwk=0if tablekdoes not serve dish w). The customer\nxmthen sits (i) at an existing table k(1â‰¤kâ‰¤tuÂ·)and eats\nad i s hwserved at the table with probability proportional to\ncuwkâˆ’d|u|or (ii) at a new table k=tuÂ·+1with probability\nproportional to d|u|tuÂ·+Î¸|u|. In the case (i), the value of xm\nis set towandcuwkis incremented. In the case (ii), to order\na dish served at the new table k, a proxy customer is sent to\nthe parent restaurant Ï€(u), where he behaves in a recursive\nmanner. If he eventually eats a dish win restaurant Ï€(u),\nthe dishwis also served at the new table kin restaurant u\nand the customer xmeats the dish w. Consequently, tuwis\nincremented, the value of xmis set tow,a n dcuwkis incre-\nmented. Note that when the proxy customer sits at a new\ntable in restaurant Ï€(u), a new proxy customer is further\nsent to the restaurant Ï€(Ï€(u)). Finally, a proxy customer\nmay be sent to the root restaurant Ï†.W h e nh es i t sa tan e w\ntable in the root restaurant Ï†, a dish served at the new table\nis chosen according to the global base measure G0.\n647Oral Session 8: Chord Analysis\nMore speciï¬cally, given a particular seating arrangement\n(denoted by S), a next chord wfollowing context uis gen-\nerated according to the following predictive distribution:\nPHPY\nu(w|S)=cuwÂ·âˆ’d|u|tuw\ncuÂ·Â·+Î¸|u|+d|u|tuÂ·+Î¸|u|\ncuÂ·Â·+Î¸|u|PHPY\nÏ€(u)(w|S)(4)\nwhere Eqn. (4) is a recursive deï¬nition with respect to con-\ntextuof any length, e.g., PHPY\nÏ€(u)(w|S)is given by substi-\ntutingÏ€(u)intouin Eqn. (4). Starting with an empty tree\n(cuwk=0 andtuw=0), a seating arrangement for Xis\nobtained by adding Mcustomers one by one. The IKN was\nfound to be an approximation of the HPYLM (the HPYLM\nreduces to the IKN when Î¸|u|=0andtuw=1).\n3.2.3 Predictive Distribution and Bayesian Inference\nThe goal is to estimate the predictive distribution Pu(w|X)\nin a Bayesian manner. Since a true seating arrangement for\nXis unknown, the expected value of PHPY\nu(w|S)is calcu-\nlated under the CRF P(S|X)as follows:\nPHPY\nu(w|X)=/summationdisplay\nSPHPY\nu(w|S)P(S|X) (5)\nBecause this sum is analytically intractable, Gibbs sampling\nis used for approximation. More speciï¬cally, we get\nPHPY\nu(w|X)â‰ˆ1\nLL/summationdisplay\nl=1PHPY\nu(w|Sl) (6)\nwhereLis the number of many i.i.d. seating arrangements\nsampled from p(S|X)andlis a sample index.\nThe Gibbs sampling algorithm is shown in Figure 3. First,\na seating arrangement is initialized by adding all customers\none by one according to the posterior CRF, where each cus-\ntomerxm=wsits at an existing or new table serving dish w\nwith probability given by the ï¬rst or second term of Eqn. (4).\nThen a customer xmis selected randomly and removed from\nthe tree, from which the related proxy customers and tables\nthat become empty are also removed. Given a seating ar-\nrangement of the other customers, the customer xmis added\nto the tree again according to the posterior CRF. By iterat-\ning this operation, Lseating arrangements are sampled with\na certain interval. Since the parameters d0,Â·Â·Â·,dnâˆ’1and\nÎ¸0,Â·Â·Â·,Î¸nâˆ’1are unknown, beta and gamma prior distribu-\ntions are put on them and the values of the parameters are\nsampled from posterior distributions (see details in [8]).\n3.3 Variable-Order Pitman-Yor Language Model\nA problem of the HPYLM is that all Mcustomers are forced\nto enter restaurants of ï¬xed depth nâˆ’1. To solve the prob-\nlem, Mochihashi and Sumita [9] proposed a variable-order\nPY language model (VPYLM) that allows each customer to\nenter a restaurant of variable depth. Each chord xmis asso-\nciated with a latent variable zmthat indicates the value of n\n(depth +1). Since a true value of zmis unknown, all possi-\nble values of zmare considered ( nis marginalized out) for\nmaking predictions, resulting in the inï¬nity -gram model.for  m = 1 :Min random  order\nAdd customer         to the tree at depth  n-1mx\nfor  i= 1 :âˆ\nfor  m = 1 :Min random  order\nRemo ve customer          f rom  the tree\nAdd customer          to the tree at depth  n-1mx\nmxCreate  an empty  tree\nFigure 3 . Gibbs sampling algorithm for HPYLM.\n3.3.1 Stochastic Process for Data Generation\nWe consider how the value of n-gram length zmis stochas-\ntically determined. The customer xmdescends the tree by\nfollowing a path Ï†â†’xmâˆ’1â†’xmâˆ’2â†’Â·Â·Â· , i.e., by back-\ntracking the context u. When he arrives at restaurant uiof\ndepthi(0â‰¤iâ‰¤âˆ ), he stops there with probability Î·uior\npasses through with probability 1âˆ’Î·ui. The probability of\nzm=n(1â‰¤nâ‰¤âˆ)is therefore given by\nPu(n|Î·)=Î·unâˆ’1nâˆ’2/productdisplay\ni=0(1âˆ’Î·ui) (7)\nSinceÎ·(a set of parameters) is unknown, beta prior distribu-\ntions with hyperparameters Î±andÎ²are put on Î·as follows:\np(Î·)=/productdisplay\nuâˆˆtreeBeta(Î·u|Î±,Î²) (8)\nGiven the value of zm,t h ev a l u eo f xmis stochastically\ndetermined according to the CRF described in Section 3.2.2.\nNote that there are not only proxy customers but also origi-\nnal customers in restaurants other than leaf nodes.\nMore speciï¬cally, given a particular seating arrangement\ndenoted by S, a next chord wfollowing context uis gener-\nated according to the following predictive distribution:\nPVPY\nu(w|S)=/summationdisplay\nnPVPY\nu(w|n,S)Pu(n|S) (9)\nwherePVPY\nu(w|n,S)is obtained in the same way as Eqn. (4)\nandPu(n|S)=/integraltext\nPu(n|Î·)p(Î·|S)dÎ·is easily calculated by\nusing the conjugacy between Eqns. (7) and (8) (see [9]).\n3.3.2 Predictive Distribution and Bayesian Inference\nThe predictive distribution of a next chord wis obtained in\nthe same way as the HPYLM (Section 3.2.3). The only dif-\nference with respect to Gibbs sampling is that the VPYLM\nneeds to sample the value of zmfrom its posterior distribu-\ntion before adding customer xmto the tree. When xm=w,\nthe posterior probability of zm=nis given by\nPu(n|S,w)âˆPu(w,n|S)=PVPY\nu(w|n,S)Pu(n|S)(10)\n3.4 Nested Pitman-Yor Language Model\nAn essential problem of standard n-gram models is that we\nneed to deï¬ne a ï¬nite vocabulary even though in the real\nworld the vocabulary is growing steadily. To solve this prob-\nlem in the context of word sequence modeling, Mochihashi\net al. [10] proposed a nested PY language model (NPYLM)\nby formulating a global base measure G0over a countably\n64812th International Society for Music Information Retrieval Conference (ISMIR 2011)\ninï¬nite number of variable-length words. Note that the con-\nventional base measure G0(w)=1/Vcannot be used be-\ncauseG0(w)â†’0whenVâ†’âˆ . Instead, a spelling model\nbased on a letter-level VPYLM is given as a global base\nmeasureG0of aword-level VPYLM. More speciï¬cally, each\nword is regarded as a sequence of letters, which are assumed\nto follow a letter-level CRF. The word length (the numberof letters) is assumed to follow a Poisson distribution. Thus,\nthe0-gram probability of any word w,G\n0(w),i sg i v e nb y\nthe product of the probabilities of the letters and their num-\nber, resulting in the inï¬nite -vocabulary model.\n4. VOCABULARY-FREE INFINITY-GRAM MODEL\nForchord sequence modeling we propose a novel vocabulary-\nfree inï¬nity-gram model similar in spirit to the NPYLM.\n4.1 Mathematical Formulation\nA critical problem is that we cannot apply the NPYLM to\nchord sequence modeling. Because words are temporal se-\nquences of letters and chords are simultaneous combinationsof notes, we need a different base measure G\n0.\nTo solve this problem, we formulate a probabilistic model\nbased on the component-based notation (Section 2.2) as a\nglobal base measure G0of a chord-level VPYLM. The base\nmeasureG0is based on a conjugate model. In general, a\nchordwcan be written as w0:w1Â·Â·Â·w12,w h e r ew0is a\nroot note and the other variables take binary values. When\nw=N,w0=N and other variables are not used. We assume\nw0to follow a 13-dimensional discrete distribution and the\nothers to follow Bernoulli distributions as follows:\nG0(w)=p(w|Ï€,Ï„)=Ï€w012/productdisplay\ni=1Ï„wi\ni(1âˆ’Ï„i)1âˆ’wi(11)\nwhereÏ€={Ï€C,Ï€C#,Â·Â·Â·,Ï€B,Ï€N}indicates the probabilities\nof the respective pitch classes and â€œNâ€ and Ï„={Ï„1,Â·Â·Â·,Ï„12}\nindicates the existence probabilities of the respective de-\ngrees. Ifw=N,G0(w)=Ï€N. Since the values of Ï€andÏ„\nare unknown, we put prior distributions as follows:\np(Ï€,Ï„)=Dir(Ï€|a0)12/productdisplay\ni=1Beta(Ï„i|b0,c0) (12)\nwherea0,b0,a n dc0are hyperparameters (set to 0.5).\n4.2 Bayesian Inference\nGiven a seating arrangement S, the posterior distribution of\nÏ€andÏ„can be easily calculated as follows:\np(Ï€,Ï„|S)=Dir(Ï€|a0+n)12/productdisplay\ni=1Beta(Ï„i|b0+ni,c0+Â¯ni)(13)\nwherenv(vis one of the pitch classes or â€œNâ€) is the number\nof tables serving dishes with root note v(w0=v) in the root\nrestaurant Ï†,niis the number of tables serving dishes with\nthei-th note ( wi=1)i nÏ†,a n d Â¯niis the number of tables\nserving dishes without the i-th note (wi=0)i nÏ†.The predictive distribution of a next chord wcan be cal-\nculated in the same way as the VPYLM (Section 3.3.2). The\nGibbs sampling algorithm of the VPYLM is modiï¬ed as fol-\nlows: When a (proxy) customer sits at a new table (a new\ntable is added) in the root restaurant Ï†, the values of nvand\nniorÂ¯niare incremented according to the components of the\ntarget chord (a dish served at that table). When a table is re-moved from the root restaurant Ï†, the values of n\nvandni\norÂ¯niare decremented. The values of Ï€andÏ„are sampled\nfrom the posterior distribution given by Eqn. (13).\n5. EXPERIMENTS\nThis section reports our comparative experiments.\n5.1 Experimental Conditions\nWe used a standard dataset of chord sequences for 180 Bea-\ntles songs collected from 12 albums (13 CDs) [14]. Be-\ncause the choice of chords depends on the musical key, we\nselected 137 major-scale non-transposition songs and trans-\nposed them to C major. The total number of chords was\n10,761, where 103 chord types were observed in the label-based notation (the vocabulary size was 205) and 149 chord\ntypes were observed in the component-based notation (the\nvocabulary size was 49153). The entropies of both data were\n3.79 [bits] and 3.92 [bits], respectively.\nIn the ï¬rst experiment using the label-based notation, the\neffectiveness of inï¬nity-gram modeling was evaluated by\ncomparing six existing methods: Good-Turing (GT), Witten-Bell (WB), IKN, MKN, HPYLM, and VPYLM, where GT\nand WB are classical smoothing methods [7]. In the second\nexperiment using the component-based notation, the effec-\ntiveness of vocabulary-free modeling was evaluated. In ad-\ndition to the existing methods, we tested our models that in-\ncorporate the vocabulary-free base measure G\n0into HPYLM\nand VPYLM (denoted by preï¬x â€œVF-â€). To evaluate the pre-dictive performance, we conducted 10-fold cross validation\nand measured perplexity , which indicates the average num-\nber of next-chord candidates (a degree of uncertainty), given\na context. A lower perplexity means better performance.\n5.2 Experimental Results\nWe found in the ï¬rst experiment that VPYLM yielded the\nlowest perplexity (Table 2) and that, as shown in Figure 4,\na posterior distribution over ncan be estimated for each\nchord. To obtain better predictive performance, it is impor-\ntant to marginalize out n(take all possibilities into account)\nrather than use a maximum-a-posteriori (MAP) estimate of\nn. The training time and memory usage of the VPYLM were\ntwo times shorter and ï¬ve times smaller than those of the 10-\ngram HPYLM because unnecessarily-longer contexts (deep\nnodes) do not need to be considered (expanded). We could\ndiscover stochastically-coherent chord patterns (Table 3) by\ncalculating P\nu(w,n|X)=/summationtext\nSPu(w,n|S)P(S|X),w h i c h\nindicates how likely chord wis to follow context uof length\n649Oral Session 8: Chord Analysis\nn\n GT WB IKN MKN HPYLM\n VPYLM\n1\n 16.8 15.6 16.0 15.7 15.8 ( Â±0.03)\n2\n 20.3 14.2 15.2 15.8 14.5 ( Â±0.10)\nn: posterior sample\n3\n 23.5 15.4 16.0 16.3 16.0 ( Â±0.18)\n 13.4 (Â±0.33)\n4\n 25.5 16.8 17.7 15.5 13.9 ( Â±0.25)\n5\n 26.3 17.5 16.2 14.1 13.7 ( Â±0.23)\n n: MAP estimate\n6\n 27.0 17.8 15.1 13.5 13.6 ( Â±0.23)\n 12.9 (Â±0.35)\n7\n 27.3 18.0 14.5 13.3 13.6 ( Â±0.23)\n8\n 27.3 18.0 14.2 13.2 13.6 ( Â±0.22)\nn: marginalized out\n9\n 27.3 18.0 14.1 13.1 13.5 ( Â±0.23)\n 11.9 (Â±0.22)\n10\n 27.3 18.0 14.0 13.1 13.5 ( Â±0.23)\nTable 2 . Perplexities in label-based notation.\nIntro\nVerse AVerse B\nChorusC:maj \nG:maj \nA:min F:maj7 F:maj6 C:maj \nG:maj \nF:maj C:maj \nC:maj \nA:min \nE:min \nF:maj \nC:majC:maj G:maj \nF:maj \nC:maj C:maj \nG:maj\nA:min \nA:min F:maj7 F:maj6 \nC:maj \nG:maj F:maj C:maj \nC:maj \nG:maj \nA:min \nA:min \nF:maj7 F:maj6 \nC:maj \nG:maj F:maj C:maj n=  1  2  3  4  5  6  7  8  9  10 n=  1  2  3  4  5  6  7  8  9  10\nFigure 4 . Hinton-diagram representation of posterior distri-\nbutions over nat the beginning of the Beatlesâ€™ â€œLet It Be.â€\nnâˆ’1. For example, C:7 F:7 C:7 is a typical blues-rock pat-\ntern that was popularized by the Beatles. We can see that the\nBeatles liked to use chord patterns including (major/minor)\nseventh chords, which were not so common at that time.\nIn the second experiment, VF-VPYLM, the vocabulary-\nfree inï¬nity-gram model, yielded a perplexity signiï¬cantly\nlower than the other models did (Table 4). The performance\nadvantage was larger than that in the ï¬rst experiment. Thisproves that our model is robust to the data sparseness (large-\nor inï¬nite-vocabulary situation).\n6. CONCLUSION\nWe presented a nonparametric Bayesian n-gram model for\nchord sequences that requires neither a vocabulary of chordtypes nor a predeï¬nition of n. We showed that it performed\nsigniï¬cantly better than the state-of-the-art models.\nThis study opens up a new research direction. We plan to\nlet computers acquire the concept of â€œchordsâ€ in an unsuper-vised manner from a large amount of music scores and, ul-\ntimately, from a large amount of musical audio signals. We\nknow that certain combinations of notes can form chords. Is\nthis learned from experience? How reasonable is a deï¬ni-\ntion of chords? To explore ways to answer this question we\nneed to consider an inï¬nite number of note combinations as\nchord candidates. Bayesian nonparametrics is a promisinggenerative approach to such kinds of meta-level problems.\nPu(w,n|X)\nStochastically-cohe rent chord pattern ( nâ‰¥3)\n0.701\n n=3:C : 7F : 7C : 7\n0.682\n n=3: B:maj F:maj G:maj\n0.656\n n=3: A:min C:7 F:maj\n0.647\n n=3: F:min G:maj C:maj\n0.645\n n=4: F:maj F:maj G:maj C:maj\n0.632\n n=3: E:min C:7 F:maj\n0.630\n n=3: C:maj7 D:min7 E:min7\n0.627\n n=4: B:maj F:maj G:maj C:maj\n0.622\n n=3: D:min7 G:sus4 G:maj\n0.620\n n=5: D:min G:maj C:maj F:maj C:maj\nTable 3 . Stochastically-coherent chord patterns.\nn\n GT WB IKN MKN\n10\n 38.3 24.4 18.5 17.5\nn\nHPYLM VF-HPYLM\n10\n 18.0 (Â±0.29) 16.5 ( Â±0.60)\nn\nVPYLM VF-VPYLM\nâˆ\n 15.8 (Â±0.29) 14.6 (Â±0.55)\nTable 4 . Perplexities in component-based notation.\nAcknowledgment : This study was partially supported by KAK-\nENHI 23700184. We thank Dr. Daichi Mochihashi (ISM).\n7. REFERENCES\n[1] J.-F. Paiement, D. Eck, and S. Bengio. A probabilistic model\nfor chord progressions. ISMIR , pp.312â€“319, 2005.\n[2] R. Scholz, E. Vincent, and F. Bimbot. Robust modeling of mu-\nsical chord sequences using probabilistic N-grams. ICASSP ,\npp.53â€“56, 2009.\n[3] M. Ogihara and T. Li. N-gram chord proï¬les for composer\nstyle representation. ISMIR , pp.671â€“676, 2008.\n[4] C. P Â´erez-Sancho, D. Rizo, and J. M. I Ëœnesta. Genre classiï¬ca-\ntion using chords and sto chastic language models. Connection\nScience , vol.21, no.2-3, pp.145â€“159, 2009.\n[5] H.-T. Cheng, Y.-H. Yang, Y.-C. Lin, I-B. Liao, and H. H. Chen.\nAutomatic chord recognition for music classiï¬cation and re-trieval. ICME , pp.1505â€“1508, 2008.\n[6] M. Khadkevich and M. Omologo. Use of hidden Markov mod-\nels and factored language models for automatic chord recogni-\ntion. ISMIR , pp.561â€“566, 2009.\n[7] S. Chen and J. Goodman. An empirical study of smooth-\ning techniques for language mode ling. Tech. Repo. TR-10-98,\nComputer Science Group, Harvard University, 1998.\n[8] Y. W. Teh. A Bayesian interpretation of interpolated Kneser-\nNey. Tech. Repo. TRA2/06, NUS School of Computing, 2006.\n[9] D. Mochihashi and E. Sum ita. The inï¬nite Markov model.\nNIPS , pp.1017â€“1024, 2007.\n[10] D. Mochihashi, T. Yamada, and N. Ueda. Bayesian unsuper-\nvised word segmentation with nested Pitman-Yor language\nmodeling. ACL-IJCNLP , pp.100â€“1008, 2009.\n[11] M. Mauch, S. Dixon, C. Harte, B. Fields, and M. Casey. Dis-\ncovering chord idioms through Beatles and real book songs,\nISMIR , pp.255â€“258, 2007.\n[12] M. Mauch and S. Dixon. Simu ltaneous estimation of chords\nand musical context from audio. IEEE Trans. ASLP , vol.18,\nno.6, pp.1280â€“1289, 2010.\n[13] K. Yoshii and M. Goto. Inï¬nite latent harmonic allocation: A\nnonparametric Bayesian appr oach to multipitch analysis. IS-\nMIR, pp.309â€“314, 2010.\n[14] C. Harte, M. Sandler, S. Abdallah, and E. G Â´omez. Symbolic\nrepresentation of musical chords: A proposed syntax for textannotations. ISMIR , pp.66â€“71, 2005.\n[15] J. Pitman and M. Yor. The two- parameter Poisson-Dirichlet\ndistribution derived from a stable subordinator. Annals of\nProbability , vol.25, no.2, pp.855â€“900, 1997.\n650"
    },
    {
        "title": "An Investigation of Musical Timbre: Uncovering Salient Semantic Descriptors and Perceptual Dimensions.",
        "author": [
            "Asteris I. Zacharakis",
            "Konstantinos Pastiadis",
            "Georgios Papadelis",
            "Joshua D. Reiss"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1414964",
        "url": "https://doi.org/10.5281/zenodo.1414964",
        "ee": "https://zenodo.org/records/1414964/files/ZacharakisPPR11.pdf",
        "abstract": "A study on the verbal attributes of musical timbre was conducted in an effort to identify the most significant semantic descriptors and to quantify the association between prominent timbral aspects and several categorical properties of environmental entities. A verbal attribute magnitude estimation (VAME) type of listening test in which participants were asked to describe 23 musical sounds using 30 Greek adjectives together with verbal terms of their own choice was designed and conducted for this purpose. Factor and Cluster Analysis were performed on the subjective evaluation data in order to shed some light on the relationships between the adjectives that were proposed and to conclude to the number and quality of the salient perceptual dimensions required for the description of this set of sounds.",
        "zenodo_id": 1414964,
        "dblp_key": "conf/ismir/ZacharakisPPR11",
        "keywords": [
            "verbal attributes",
            "musical timbre",
            "semantic descriptors",
            "quantify association",
            "environmental entities",
            "verbal attribute magnitude estimation",
            "adjectives",
            "categorical properties",
            "listening test",
            "factor and cluster analysis"
        ],
        "content": "12th International Society for Music Information Retrieval Conference (ISMIR 2011)\nAN INVESTIGATION OF MUSICAL TIMBRE: UNCOVERING SALIENT\nSEMANTIC DESCRIPTORS AND PERCEPTUAL DIMENSIONS.\nAsteris Zacharakis\nQueen Mary University of London,\nCentre for Digital Music,\nLondon, UK.\nasteriosz@eecs.qmul.ac.ukKostantinos Pastiadis, Georgios Papadelis\nAristotle University of Thessaloniki,\nSchool of Music Studies,\nThessaloniki, Greece\npastiadi@mus.auth.grJoshua D. Reiss\nQueen Mary University of London,\nCentre for Digital Music,\nLondon, UK.\njosh.reiss@eecs.qmul.ac.uk\nABSTRACT\nA study on the verbal attributes of musical timbre was con-\nducted in an effort to identify the most signiï¬cant semantic\ndescriptors and to quantify the association between promi-\nnent timbral aspects and several categorical properties of\nenvironmental entities. A verbal attribute magnitude esti-\nmation (V AME) type of listening test in which participants\nwere asked to describe 23 musical sounds using 30 Greek\nadjectives together with verbal terms of their own choice\nwas designed and conducted for this purpose. Factor and\nCluster Analysis were performed on the subjective evalua-\ntion data in order to shed some light on the relationships be-\ntween the adjectives that were proposed and to conclude to\nthe number and quality of the salient perceptual dimensions\nrequired for the description of this set of sounds.\n1. INTRODUCTION\nMusical timbre perception and its acoustical correlates have\nbeen a subject of research since the late 19th century [15].\nDuring the last decades numerous studies on musical timbre\nhave tried to uncover the number of signiï¬cant perceptual\ndimensions and their semantic associations. Having applied\ndifferent techniques most of these studies have concluded\nto either 3 or 4 major perceptual dimensions for modelling\ntimbres of monophonic acoustic instruments and have also\nproposed a wide range of verbal attributes to label them.\nGrey in his state-of-the-art study in 1977 proposed a 3-D\nspace for musical timbre representation by applying Mul-\ntidimensional Scaling techniques to pairwise dissimilarity\nrating data [3]. Krumhansl and McAdams have also pro-\nposed a 3-D space [8], [9] whose physical correlates vary\ncompared to the ones proposed by Grey.\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proï¬t or commercial advantage and that copies\nbear this notice and the full citation on the ï¬rst page.\nc/circlecopyrt2011 International Society for Music Information Retrieval.von Bismarck conducted a semantic differential listen-\ning test featuring 30 verbal scales in order to rate 35 speech\nsounds [14]. According to this study timbre would have\nfour orthogonal dimensions. One of the four von Bismarckâ€™s\ndimensions is associated with volume ( full-empty ), another\none is a blend of vision and texture ( dull-sharp ), the third\nis labelled colourful-colourless and the last one is labelled\ncompact-diffused . Other related studies also revealed three\nor four perceptual axes. Pratt and Doak, working with sim-\nple synthetic tones have proposed a 3-D space featuring a vi-\nsion ( bright-dull ), a temperature ( warm-cold ) and a wealth\n(rich-pure ) axis [11]. Ë˜StË˜epÂ´anekâ€™s study in the Czech lan-\nguage [13] reveals one dimension associated with vision\n(gloomy-clear ), another one with texture ( harsh-delicate ),\na third one with volume ( full-narrow ) and a last one with\nhearing ( noisy/rustle-â€˜undeï¬nedâ€™ ). Moravecâ€™s work again in\nCzech language has also resulted to four perceptual axes re-\nlated to vision ( bright/clear-gloomy/dark ), texture ( hard/sharp-\ndelicate/soft ), volume ( wide-narrow ) and temperature ( hot/\nhearty - â€˜undeï¬nedâ€™ ) [10]. Finally, Howardâ€™s study in the\nEnglish language [6] has uncovered four salient dimensions\nthe ï¬rst of which is a mixture of vision, texture, volume and\ntemperature ( bright/thin/harsh-dull/warm/gentle ). The sec-\nond one is labelled pure/percussive-nasal , the third is as-\nsociated with the material of the sound source ( metallic-\nwooden ) and the fourth is related to the evolution in time\n(evolving ).\nAlthough there seems to be some agreement concerning\nthe number and attributes of the timbre dimensions, some\ndifferences between studies do exist. Such inconsistencies\ncould be due to the different experimental protocol used\neach time and also due to generalization of the ï¬ndings that\nresulted from a particular â€˜samplingâ€™ of the vast timbre space.\nThus, the selection of an appropriate set of sounds that will\nrepresent as much of the variance of the existing musical\ntimbres as possible and at the same time will keep the dura-\ntion of a listening test relatively short is crucial. This work\naddressed this issue by including a wide range of musical\ntimbres with high ecological validity drawn from acoustic\n807Oral Session 10\ninstruments, electric instruments and synthesisers.\nAll of the cited studies have applied Factor Analysis and\nCluster Analysis techniques in order to achieve dimension\nreduction of their multidimensional perceptual data. Factor\nanalysis is a multivariate statistical technique that is used to\nuncover the latent structure of a set of inter-correlated vari-\nables [4]. It is widely applied in musical timbre research in\norder to reduce a large number of semantic descriptions to a\nsmaller number of interpretable factors. Cluster Analysis is\nanother statistical technique that seeks to identify homoge-\nneous subgroups within a larger set of observations [12]. In\nthe research on timbre perception it can indicate groups of\nsemantically related verbal descriptors.\nThe current work has also made use of these data analysis\ntechniques seeking for more deï¬nitive conclusions concern-\ning the nature of the signiï¬cant verbal descriptors of musical\ntimbre. Overall, it aims at yielding a content analysis frame-\nwork based on extramusical semantics.\n2. METHOD\nFor the purpose of this study a listening test exploiting a\nvariation of the Verbal Attribute Magnitude Estimation (V AME)\n[7] method was designed and conducted. The subjects were\nprovided with a pool of 30 Greek verbal descriptors and\nwere asked to describe timbral attributes of 23 sound stim-\nuli by choosing the adjectives they believed that were more\nappropriate for each case. Once a subject chose a descriptor\nhe was further asked to insert its amount of relevance on a\nscale anchored by the verbal attribute and its negation, such\nas â€œ not brilliant - very brilliantâ€. This rating was performed\nby a horizontal slider with a hidden continuous scale rang-\ning from 0 to 100. The verbal descriptors used, were En-\nglish language equivalents that are commonly found in tim-\nbre perception literature [1], [14], [2], [5] and are depicted\nin Table 1. The subjects were also free to insert up to three\nadjectives of their own choice for describing each stimuli in\ncase they felt that the provided terms were inadequate.\n2.1 Stimuli - Material\nA set of 23 sounds of high ecological validity (acoustic in-\nstruments, electric instruments and state-of-the-art synthe-\nsisers) was selected. The following 14 instrument tones\ncome from the MUMS (McGill University Master Samples)\nlibrary: violin, sitar, trumpet, clarinet, piano at A3 (220 Hz),\ndouble bass pizzicato, Les Paul Gibson guitar, baritone sax-\nophone B ï¬‚at at A2 (110 Hz), oboe at A4 (440 Hz), Gib-\nson guitar, pipe organ, marimba, harpsichord at G3 (196\nHz) and french horn at A3# (233 Hz). A ï¬‚ute recording at\nA4 was also used along with a set of 8 synthesiser sounds:\nAcid, Hammond, Moog, Rhodes piano at A2, electric piano\n(rhodes), Wurltitzer, Farï¬sa at A3 and Bowedpad at A4. The\nsamples were loudness equalised with an informal listeningtest within the research team. The playback level was set\nbetween 65 and 75 A weighted dB SPL rms. 83% of the\nsubjects found that level comfortable and 78% reported that\nloudness was perceived as being constant across stimuli.\nThe listening test was conducted in an acoustically iso-\nlated listening room. Sound stimuli were presented through\nthe use of a desktop computer (Intel pentium 2.8 GHz, 1 GB\nRam, WinXP(SP3)), with an M-Audio (Firewire 410) exter-\nnal audio interface, and a pair of Sennheiser HD60 ovation\ncircumaural headphones. The interface of the experiment\nwas built in Max/MSP.\n2.2 Listening Panel\nForty one subjects (aged 19-55, mean age 23.3, 13 male)\nparticipated in the listening test. None of them reported any\nhearing loss and all of them were critical listeners and had\nbeen practising music for 13.5 years on average (ranging\nfrom 5 to 35). The majority of subjects were students at the\nDepartment of Music Studies of the Aristotle University of\nThessaloniki. Course credit was offered as a reward for their\nparticipation.\n2.3 Procedure\nInitially the listeners were presented with a familiarisation\nstage which consisted of the random presentation of the stim-\nuli set in order for them to get a feel of the timbral range\nof the experiment. For the main part of the experiment\nthe playback of each sound was allowed as many times as\nneeded prior to submitting a rating. The sounds were pre-\nsented in a random order for each listener in order to min-\nimize bias to the responses. Subjects were advised to use\nas many of the terms as they felt were necessary for an ac-\ncurate description of each different timbre and also to take\na break in case they felt signs of fatigue. They were also\nfree to withdraw at any point. The overall listening test pro-\ncedure, including instructions, lasted around 40 minutes for\nthe majority of the subjects. The wide majority of subjects\nrated the above procedure as easy to follow, clear and mean-\ningful.\n2.4 Factor Analysis\nAlthough the choice between Exploratory Factor Analysis\n(FA) or Principal Components Analysis (PCA) for data re-\nduction has long been debated, we believe that FA is the\nappropriate choice for our investigation, as we focus on the\nidentiï¬cation of potential underlying structures that shall de-\nscribe and justify the semantic representation of listenersâ€™\ntimbral experiences and judgements, across different musi-\ncal sounds.\nThe basic FA model is described as:\n80812th International Society for Music Information Retrieval Conference (ISMIR 2011)\nzj=aj1F1+aj2F2+. . .+ajnFn+Uj=n/summationdisplay\ni=1ajiFi+Uj\n(1)\nwhere j= 1. . . m or in matrix notation,\nZ = AÂ·F + U (2)\nwhere\nZT=/bracketleftbig\nz1Â·Â·Â·zm/bracketrightbig\nis the array of manalysed variables\nA=ï£®\nï£¯ï£°a11Â·Â·Â· a1n\n.........\nam1Â·Â·Â·amnï£¹\nï£ºï£»\nis the matrix of factor loadings to be estimated from the\ndata,\nFT=/bracketleftbig\nF1Â·Â·Â·Fn/bracketrightbig\nis the array of n Common Factors , and\nUT=/bracketleftbig\nU1Â·Â·Â·Um/bracketrightbig\nis the array of m Unique Factors .\nActually, the problem and methodology of FA is to try\nto create, from a set of original variables, a new set of con-\nstructs (the common factors, with n <m) that will com-\npactly describe the correlations between the original vari-\nables. Unique factors add to the versatility of the solution,\nas they account for that part of the original variance that\ncannot be attributed or modelled by the common factors.\n3. RESULTS\nThe listenersâ€™ responses were analysed employing Cluster\nAnalysis and Factor Analysis (FA). For this reason the quan-\ntity estimations on each verbal descriptor and each musical\ntimbre were averaged over the 41 subjects of the test. Basic\nstatistics for each descriptor are shown in Table 1.\nOnly 37% of the subjects inserted at least one extra ver-\nbal descriptor thus providing 36 additional terms. However,\nonly 9 of them where mentioned more than once and only 4\nwere mentioned by more than one subject. This sparsity and\ninconsistency of the ï¬ndings implies that our proposed set\nof 30 adjectives was adequate for describing this particular\nset of musical timbres.\nAs the distributions for most descriptors showed exces-\nsive positive skewness, a square root monotonic transforma-\ntion was applied. Initially, the terms empty, distinct, nasal\nwere removed following a bivariate correlation analysis over\nthe 30 descriptors that was employed to identify and removeTable 1 . Basic statistics for each verbal descriptor.\nDescriptor Range Mean Descriptor Range Mean\nBrilliant 25.68 8.63 Deep 59.93 10.82\nHollow 17.43 6.08 Distinct 34.34 11.65\nClear 48.39 8.76 Dry 24.00 8.13\nRough 33.45 8.47 Light 25.54 4.76\nMetallic 39.17 14.02 Messy 39.73 4.90\nWarm 23.66 9.01 Empty 36.80 6.93\nSmooth 19.24 5.05 Dirty 41.51 8.60\nThick 47.32 8.26 Compact 17.22 7.91\nRounded 26.10 11.22 Dark 23.95 7.81\nHarsh 25.88 9.48 Soft 34.32 6.14\nDull 30.41 10.93 Nasal 33.07 9.30\nThin 18.76 5.61 Full 35.90 13.50\nShrill 55.37 17.90 Dense 20.07 8.89\nCold 13.33 6.59 Bright 16.95 5.44\nSharp 36.31 10.96 Rich 20.49 6.68\nthose with several instances of low correlation coefï¬cients\n(absolute value <0.2), which could potentially reduce the\nvalidity of further dimensionality reduction analysis. A cen-\ntroid Hierarchical Cluster Analysis based on squared Eu-\nclidean distances over the remaining 27 descriptors (Figure\n1) identiï¬ed 3 major clusters of descriptors, namely Clus-\nter 1: soft, light, warm, smooth, rounded, dull, rich, full,\nthick, deep, dense, dark, compact, hollow, Cluster 2: bright,\nbrilliant, thin, clear, Cluster 3: shrill, sharp, rough, harsh,\ndirty, messy, dry, cold, metallic. In order to further reduce\nthe number of verbal descriptors, a preliminary Factor Anal-\nysis was performed within each cluster and those with abso-\nlute factor loadings1>0.7 were selected for the subsequent\nï¬nal Factor Analysis.\nFor each cluster FA, Maximum Likelihood (ML) factor\nextraction with Oblimin rotation was employed. Maximum\nLikelihood estimation of factor loadings allows for sufï¬-\ncient, consistent and efï¬cient representation of the FAâ€™s pat-\ntern matrix, under the provision of multivariate normality of\nthe data, a condition for which special steps have been taken\nin this work (e.g. variable transformation).Traditionally, FA\nresults in a reduced size description of correlations between\nthe subjected variables using new â€˜combinedâ€™ variables (the\nfactors) which are designed and computed as mutually or-\nthogonal. However, in several cases, orthogonality of fac-\ntors could impede the interpretability of results by consti-\ntuting an unexpectedly strict and excluding possibility. We\nbelieve that in this work we should relax the factorsâ€™ or-\n1Factor loadings are the correlation coefï¬cients between variables and\nfactors. The values of the factor loadings indicate how well a certain vari-\nable is represented by a particular factor and are crucial for the labelling\nand interpretation of the factors.\n809Oral Session 10\nFigure 1 . Dendrogram of the Hierarchical Cluster Analysis\nover the 27 descriptors.\nthogonality requirement, and follow a conceptually â€˜widerâ€™\napproach, by employing a non-orthogonal (oblique) rotation\nof the initial orthogonal solution. Later on, as it is usually\npreferred, it will be possible to check and justify the neces-\nsity for such a divergence from orthogonality requirements,\nby considering inter-factor correlations. The Direct Oblimin\nmethod (among others) is considered as a viable approach to\nthe problem of oblique factor rotation.\nPrincipal components extraction was used prior to fac-\ntor extraction in order to determine the number of factors\nand ensure absence of multicollinearity. The Kaiser-Meyer-\nOlkin (KMO)2measure of sampling adequacy was for all\nthree clusters bigger than 0.6 (Cluster 1: 0.672, Cluster2:\n0.69, Cluster 3: 0.76), and the Bartlettâ€™s test of spheric-\nity3also showed statistical signiï¬cance. For each clus-\nter, the ï¬rst 3 factors were decided to be retained from the\ninitial eigenvalues and the scree plots, accounting for more\nthan 79% of cumulative variance. After factor extraction,\nthe selected factors based on communalities4bigger than\n2The KMO assesses the sample size (i.e. cases/variables) and predicts\nif data are likely to factor well based on correlation and partial correlation.\nThe KMO can be calculated for individual and multiple variables. KMO\nvaries from 0 to 1.0 and KMO overall should be .60 or higher to proceed\nwith factor analysis.\n3Bartlettâ€™s test concerns whether correlations between variables are\noverall signiï¬cantly different from zero.\n4The communality measures the percent of variance in a given variable0.6 were: Cluster 1: soft, light, warm, smooth, rounded,\nrich, full, thick, deep, dense , Cluster 3: shrill, sharp, rough,\nharsh, dirty, messy, dry . However, for the second cluster, a\n3-factor solution could not be obtained and we decided to\nreduce the number of factors to 1, leading to retained de-\nscriptors as Cluster 2: bright, brilliant . In all 3 cases all\neigenvalues were >0.014, avoiding singularity.\nThe descriptors selected in the preliminary stage were\nthen subjected to a ï¬nal FA, again using ML and Oblimin\nrotation. The KMO measure was 0.654 and the Bartletts test\nof sphericity also showed statistical signiï¬cance. Although\nsingularity was again avoided, extreme multicollinearity was\npresent leading to removal of â€˜culpritâ€™ descriptors. Next, the\nFA was repeated with a reduced set of 15 remaining descrip-\ntors. Again, 3 factors were extracted, accounting for more\nthan 85% of initial variance. Although only messy and dirty\nhad extracted communality <0.6, for reasons of parsimony\nwe additionally posed a criterion of absolute factor loading\n>0.75 as a ï¬nal step to data reduction. Maximum corre-\nlation between rotated factors was 0.249. The prominent\ndescriptors over the three factors are shown in Table 2. Fac-\ntor scores coefï¬cients are given in Table 3. Multiplied by a\nsampleâ€™s standardized measured score on the corresponding\nvariables, these coefï¬cients will sum to the score of a given\nsample on a given factor.\nTable 2 . Factor Loadings.\nFactor\n1 2 3\nBrilliant -0.885\nDeep 0.824\nSoft 0.881\nFull 0.851\nBright -0.946\nRich 0.993\nHarsh -0.861\nRounded 0.904\nThick 0.798\nWarm 0.787\nSharp -0.779\nFactor loading values are the basis for inputting a label\nto each of the different factors. A high factor loading in-\ndicates that a particular variable is expressed strongly by a\ncertain factor. Based on Table 2, the three factors could be\nidentiï¬ed as Factor 1 volume/wealth , Factor 2 brightness\nanddensity , and Factor 3 texture andtemperature(warmth) .\nThus, it would seem possible to address musical timbre with\nsemantic associations to material objects properties. It also\nseems, based on indications from the extracted variances,\nand since the oblique rotation results in relatively low levels\nexplained by all the factors jointly.\n81012th International Society for Music Information Retrieval Conference (ISMIR 2011)\nTable 3 . Factor Scores Coefï¬cients.\nFactor\n1 2 3\nBrilliant -0.17 -0.121 0.020\nDeep -0.057 0.266 0.079\nSoft -0.035 0.098 0.160\nFull 0.065 0.103 -0.022\nBright -0.051 -0.286 0.079\nRich 0.898 -0.186 -0.099\nHarsh 0.003 0.006 -0.106\nRounded 0.011 0.006 0.588\nThick 0.076 0.258 0.009\nWarm -0.000 0.006 0.065\nDense 0.18 0.052 0.003\nDry -0.005 0.018 -0.057\nSharp 0.003 -0.043 -0.095\nof correlation between factors, that all factors share some\ncommon and balanced portion ( 23%, 34% and 24% corre-\nspondingly) of the total explained variance ( âˆ¼82%), which\nby turn reveals a relatively equal importance of descriptors\nupon the timbral targets.\nThe low correlation between factors implies the existence\nof a nearly orthogonal perceptual space, thus a positioning\nof the 23 sound stimuli into a euclidean 3-D space seems\njustiï¬ed and is shown in Figures 2, 3 and 4. Figures 3 and\n4 reveal a noticeable inï¬‚uence of fundamental frequency on\nthe brightness axis, as higher pitched sounds tend to be rated\nas brighter than lower pitched ones. A potential similar in-\nï¬‚uence on the other two axis cannot be supported by these\ndepictions.\n4. DISCUSSION\nThe above ï¬ndings share many things in common with re-\nsults of previous studies -as presented in the introduction-\nboth on the number and on the attributes of the uncovered\ntimbre space dimensions. Indeed, volume ,wealth ,texture ,\ntemperature andvision related terms have also been attributed\nas labels to timbre space dimensions from previous research.\nFurthermore, most of the past studies result in perceptual\nspaces of either three or four dimensions for musical timbre\nrepresentation. This agreement is present even among stud-\nies that apply different experimental protocols and methods\nfor the creation of timbre spaces such as Multidimensional\nScaling on data from pairwise dissimilarity listening tests\nor Principal Component Analysis for dimension reduction\namong perceptual variables. It is important, however, to\nemphasize the fact that the Factor Analysis applied on the\nvariables (i.e adjectives) of this experiment was based on\nstrictly mathematical criteria avoiding any bias from paststudies results.\nOne other important outcome of the current work is that\ninter-dimension correlation is low. Consequently, even though\nthe orthogonality requirement was not initially followed, as\nin most previous works, the result is still a nearly orthogonal\nspace with independent dimensions.\nA conï¬rmatory study for examining the adequacy of the\nextracted perceptual dimensions regarding timbre descrip-\ntion will be the next step for reaching the desired content\nanalysis framework. The deï¬nition of such a framework\nwill contribute towards a better understanding of musical\ntimbre and can be used for the development of perceptual\ndriven applications on musical sound modiï¬cation and syn-\nthesis.\nFinally, this study also positively adds to the concept of\ninter-linguistic agreement regarding musical timbre verbal-\nization and proposes a certain rationale for the interpretation\nof the salient musical timbre space dimensions. The notion\nof timbre perception as being projected on other less abstract\nsenses in order to facilitate expression and communication\ncould in a sense justify the inter-linguistic agreement. The\norientation of the human mind towards decoding and cate-\ngorizing all incoming information to familiar entities could\nbe responsible for the semantic associations to material ob-\njects that were revealed in this study.\nFigure 2 . V olume/Wealth vs Texture/Temperature\n5. CONCLUSION\nIn this paper, we have conducted an initial exploration of the\npossible underlying semantic structure of adjective timbral\ndescriptors for musical sounds. Factor and Cluster Analy-\nsis applied on the subjective evaluation responses revealed\n811Oral Session 10\nFigure 3 . Brightness-Density vs V olume/Wealth.\nthree perceptual dimensions with high degree of indepen-\ndence that explained over 80% of the total variance. These\ndimensions are associated with material object properties\nsuch as volume ,brightness-density andtexture-temperature\nand constitute a framework for the semantic description of\nthis particular set of sound stimuli. A further challenging\nissue is the conduction of conï¬rmatory structural analysis\n(e.g. Conï¬rmatory Factor Analysis) along different groups\nof sounds and/or different groups of listeners, since all aes-\nthetic, stylistic and cultural factors could possibly affect the\nvalidity of the hereby developed semantic model. Subse-\nquently, such a developed semantic framework could be de-\nployed in a semantically driven framework of audio signal\nprocessing with application in musical sound synthesis, au-\ndio post-production or other similar ï¬elds.\n6. REFERENCES\n[1] R. Ethington and B. Punch. Seawave: A system for musi-\ncal timbre description. Computer Music Journal , 18(1):30â€“39,\n1994.\n[2] A. Faure, S. McAdams, and V . Nosulenko. Verbal correlates of\nperceptual dimensions of timbre. In Proc. 1996 Int. Conf. on\nMusic Perception and Cognition , pages 79â€“84, 1996.\n[3] J.M. Grey. Multidimensional perceptual scaling of musi-\ncal timbres. Journal of the Acoustical Society of America ,\n61:1270â€“1277, 1977.\n[4] H. H. Harman. Modern Factor Analysis . University of Chicago\nPress, 3 edition, 1976.\n[5] D. Howard, A. Disley, and A. Hunt. Timbral adjectives for the\ncontrol of a music synthesizer. In 19th International Congress\non Acoustics , Madrid, 2-7 September 2007.\nFigure 4 . Brightness-Density vs Texture/Temperature.\n[6] D. Howard and A. Tyrrell. Psychoacoustically informed spec-\ntrography and timbre. Organised Sound , 2(2):65â€“76, 1997.\n[7] R. A. Kendall and E. C. Carterette. Verbal attributes of simul-\ntaneous wind instrument timbres: I. von bismarckâ€™s adjectives.\nMusic Perception , 4(10):445â€“468, 1993a.\n[8] C. L. Krumhansl. Why is musical timbre so hard to under-\nstand? In S. Nielz Â´en and O. Olsson, editors, Structure and\nPerception of Electroacoustic Sound and Music: Proc. Marcus\nWallenberg Symposium , pages 43â€“53, Lund, Sweden, August\n1988. Excerpta Medica, Amsterdam.\n[9] S. McAdams, S. Winsberg, S. Donnadieu, G. De Soete, and\nJ. Krimphoff. Perceptual scaling of synthesized musical tim-\nbres : Common dimensions, speciï¬cities, and latent subject\nclasses. Psychol. Res. , 58:177â€“192, 1995.\n[10] O. Moravec and J. Ë˜StË˜epÂ´anek. Verbal description of musical\nsound timbre in czech language. In Proceedings of the Stock-\nholm Music Acoustics Conference (SMAC03) , pages 643â€“645,\nStockholm, Sweden, 4-5 September 2003.\n[11] R.L Pratt and P.E. Doak. A subjective rating scale for timbre.\nJournal of Sound and Vibration , 45, 1976.\n[12] C. Romesburg. Cluster Analysis for Researchers . Lulu.com,\n2004.\n[13] J. Ë˜StË˜epÂ´anek. Musical sound timbre: Verbal descriptions and di-\nmensions. In Proc. of the 9th Int. Conference on Digital Audio\nEffects (DAFx-06) , Monteral, Canada, 18-20 September 2006.\n[14] G. von Bismarck. Timbre of steady tones: A factorial investi-\ngation of its verbal attributes. Acustica , 30:146â€“159, 1974.\n[15] H. L. F. von Helmholtz. On the Sensations of Tone as a Phys-\niological Basis for the Theory of Music . New York: Dover\n(1954), 4 edition, 1877.\n812"
    },
    {
        "title": "Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011, Miami, Florida, USA, October 24-28, 2011",
        "author": [
            "Anssi Klapuri",
            "Colby Leider"
        ],
        "year": "2011",
        "doi": "10.5281/zenodo.1285647",
        "url": "https://doi.org/10.5281/zenodo.1285647",
        "ee": "http://ismir2011.ismir.net/",
        "abstract": "The Annotated Jingju Arias Dataset is a collection of 34 jingju arias manually segmented in various levels using the software Praat v5.3.53. The selected arias contain samples of the two main shengqiang in jingju, name xipi and erhuang, and the five main role types in terms of singing, namely, dan, jing, laodan, laosheng and xiaosheng.\n\nThe dataset includes a Praat TextGrid file for each aria with the following tiers (all the annotations are in Chinese):\n\n\n\taria: name of the work (one segment for the whole aria)\n\tMBID: MusicBrainz ID of the audioi recording(one segment for the whole aria)\n\tartist: name of the singing performer (one segment for the whole aria)\n\tschool: related performing school (one segment for the whole aria)\n\trole-type: role type of the singing character(one segment for the whole aria)\n\tshengqiang:boundaries and label of theshengqiangperformed in the aria (including accompaniment)\n\tbanshi: boundaries and label of the banshi performed in the aria (including accompaniment)\n\tlyrics-lines: boundaries and annotation of each line of lyrics\n\tlyrics-syllables: boundaries and annotation of each syllable\n\tluogu: boundaries and label of each of the performed percussion patterns in the aria\n\n\nThe ariasInfo.txt file contains a summary of the contents per aira of the whole dataset.\n\nA subset of this dataset comprising 20 arias has been used for the study of the relationship between linguistic tones and melody in the following papers:\n\n\nShuoZhang, Rafael Caro Repetto, and Xavier Serra (2014) Study of the Similarity between Linguistic Tones and Melodic Pitch Contours in Beijing Opera Singing. In Proceedings of the 15th International Society for Music Information Retrieval Conference (ISMIR 2014), Taipei, Taiwan, October 2731, pp. 343348.\n\n\n\n______ (2015) Predicting Pairwise Pitch Contour Relations Based on Linguistic Tone Information in Beijing Opera Singing. In Proceedings of the 16th International Society for Music Information Retrieval Conference (ISMIR 2015), Mlaga, Spain, October 2630, pp. 107113.\n\n\nHere is the list of the arias from the dataset used in these papers.\n\nThe whole dataset has been used for the automatic analysis of the structure of jingju arias and their automatic segmentation in the following master&#39;s thesis:\n\n\nYile Yang(2016) Structure Analysis of Beijing Opera Arias. Masters thesis, Universitat Pompeu Fabra, Barcelona.\n\n\nUsing this dataset\n\nIf you use this dataset in a publication, please cite the above publications.\n\nWe are interested in knowing if you find our datasets useful! If you use our dataset please email us at mtg-info@upf.edu and tell us about your research.\n\nContact\n\nThe audio recordings used for these annotations are available for research purposes. Please contact Rafael Caro Repetto\n\nrafael.caro@upf.edu\n\n\n\nhttp://compmusic.upf.edu/node/349",
        "zenodo_id": 1285647,
        "dblp_key": "conf/ismir/2011"
    }
]