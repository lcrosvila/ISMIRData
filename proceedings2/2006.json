[
    {
        "title": "Optimal filtering of dynamics in short-time features for music organization.",
        "author": [
            "Jerónimo Arenas-García",
            "Jan Larsen",
            "Lars Kai Hansen",
            "Anders Meng"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1415078",
        "url": "https://doi.org/10.5281/zenodo.1415078",
        "ee": "https://zenodo.org/records/1415078/files/Arenas-GarciaLHM06.pdf",
        "abstract": "There is an increasing interest in customizable methods for organizing music collections. Relevant music characteriza- tion can be obtained from short-time features, but it is not obvious how to combine them to get useful information. In this work, a novel method, denoted as the Positive Con- strained Orthonormalized Partial Least Squares (POPLS), is proposed. Working on the periodograms of MFCCs time series, this supervised method finds optimal filters which pick up the most discriminative temporal information for any music organization task. Two examples are presented in the paper, the first being a simple proof-of-concept, where an altosax with and without vibrato is modelled. A more complex 11 music genre classification setup is also inves- tigated to illustrate the robustness and validity of the pro- posed method on larger datasets. Both experiments showed the good properties of our method, as well as superior per- formance when compared to a fixed filter bank approach suggested previously in the MIR literature. We think that the proposed method is a natural step towards a customized MIR application that generalizes well to a wide range of dif- ferent music organization tasks. Keywords: Music organization, filter bank model, positive constrained OPLS",
        "zenodo_id": 1415078,
        "dblp_key": "conf/ismir/Arenas-GarciaLHM06",
        "keywords": [
            "customizable methods",
            "organizing music collections",
            "music characterization",
            "short-time features",
            "supervised method",
            "periodograms of MFCCs",
            "discriminative temporal information",
            "music organization task",
            "music genre classification",
            "fixed filter bank approach"
        ],
        "content": "Optimal ﬁltering of dynamics in short-time features for music organization\nJer´onimo Arenas-Garc ´ıa, Jan Larsen, Lars Kai Hansen and Anders Meng\nInformatics and Mathematical Modelling\nTechnical University of Denmark\n2800 Kgs. Lyngby, Denmark\n{jag,jl,lkh,am }@imm.dtu.dk\nAbstract\nThere is an increasing interest in customizable methods for\norganizing music collections. Relevant music characteriza-\ntion can be obtained from short-time features, but it is not\nobvious how to combine them to get useful information.\nIn this work, a novel method, denoted as the Positive Con-\nstrained Orthonormalized Partial Least Squares (POPLS), isproposed. Working on the periodograms of MFCCs time\nseries, this supervised method ﬁnds optimal ﬁlters which\npick up the most discriminative temporal information forany music organization task. Two examples are presented in\nthe paper, the ﬁrst being a simple proof-of-concept, where\nan altosax with and without vibrato is modelled. A morecomplex 11music genre classiﬁcation setup is also inves-\ntigated to illustrate the robustness and validity of the pro-\nposed method on larger datasets. Both experiments showed\nthe good properties of our method, as well as superior per-\nformance when compared to a ﬁxed ﬁlter bank approachsuggested previously in the MIR literature. We think that\nthe proposed method is a natural step towards a customized\nMIR application that generalizes well to a wide range of dif-ferent music organization tasks.\nKeywords: Music organization, ﬁlter bank model, positive\nconstrained OPLS\n1. Introduction\nThe interest in automated methods for organizing music isincreasing, which is primarily due to the large digitalizationof music. Music distribution is no longer limited to physi-\ncal media, but users can download music titles directly from\nInternet services such as e.g. iTunes orNapster\n1. Portable\nplayers easily store most users personal collections and al-\nlow the user to bring the music anywhere. The problem of\nnavigating these seemingly endless streams of music appar-ently seems dubious with current technologies. However,\nthe increased research conducted in ﬁelds of music infor-\n1www.itunes.com andwww.napster.com .\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies\nare not made or distributed for proﬁt or commercial advantage and that\ncopies bear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2006 University of Victoria\nmation retrieval (MIR) will aid users in organizing and nav-\nigating their music collections. Furthermore, there has been\nan increasing interest in customization when organizing the\nmusic, see e.g. [1, 2], which provides a better control of theusers individual collections. The problems that researchers\nface when working with customization, especially in MIR,\nare many and indeed require robust machine learning algo-\nrithms for handling the large amount of data available for an\naverage user. User interaction could be in the sense of or-ganizing the music collection in speciﬁc taxonomies. This\ncould be a simple ﬂat genre taxonomy that is frequently used\nin portable players, or taxonomies based on instrumenta-tion, artist or theme, see e.g. www.allmusic.com and\n[1]. Customization in terms of predicting users personal\nmusic taste was investigated in [3], where a support vectormachine was applied in connection with active retrieval.\nThe present work introduces a method for learning im-\nportant dynamical structure in the short-time features\n2ex-\ntracted from the music, in such a way that this information is\nas relevant as possible for a given music organization task.The basic idea stems from the work of [4], where the au-\nthors investigated an audio classiﬁcation task using differ-\nent perceptual (and non-perceptual) short-time features atlarger time-scales. A periodogram was computed for each\nshort-time feature dimension over a frame corresponding\nto∼768 ms , followed by a summarization of the power\nin4predeﬁned frequency bands using a ﬁlter bank. This\nmethod was investigated in greater detail in [5], where dif-ferent methods for handling dynamics of short-time features,\ndenoted as temporal feature integration\n3, were investigated.\nThe ﬁxed ﬁlter bank applied in [4], was selected from the as-sumed importance of the dynamics in the short-time features\nfor the given learning task. The method, however, is not gen-\neral enough, since for a custom music organization task, the\ndynamics in the short-time features are context dependent\n(i.e., the relevant pattern of temporal changes in short-timefeatures is expected to be different for, e.g., vibrato/non vi-\nbrato detection, or for genre classiﬁcation tasks), which is\nthe reason for suggesting a method where an optimal ﬁlterbank is learned for a particular music organization task.\n2Short-time features are usually extracted from music at time-levels\naround 5−100 ms .\n3Temporal feature integration is the process of combining all the feature\nvectors in a time-frame into a single new feature vector in order to capturethe relevant temporal information in the frame.The content of this paper has been structured as follows.\nSection 2 presents the short-time features used and shortly\ndescribes the method in [4] for capturing the dynamic struc-ture in the short-time features. Section 3 introduces the posi-\ntive constrained OPLS, which can be used to ﬁnd an optimal\nﬁlter bank for any given music organization task. In Sec-tion 4, two experiments are described: the ﬁrst experiment\nis a proof-of-concept illustrating the goodness of the ﬁlters\nobtained using the proposed method, when discriminating\nbetween vibrato/non-vibrato playing of music instruments;\nin the second experiment, we compare the ﬁlter banks de-rived from our method with those proposed in [4], within an\n11ﬂat taxonomy music genre classiﬁcation task. Section 5\nprovides the conclusion and suggestions for future work.\n2. Music Feature Extraction\nThe complete system considered in this work has been il-\nlustrated in Figure 1. The purpose of the overall system is\nto classify music data according to some criterion, such asgenre, or presence vibrato, so we are assuming that some la-\nbelled data is available for the design. From the raw digital\naudio signal, an initial step towards an automated organiza-tion of music is feature extraction. This is the process of ex-\ntracting relevant information from the audio signal that can\nbe used in a sub-sequential learning algorithm. A music sig-\nnal is typically stationary in periods ranging from 5-100 ms ,\nsee e.g. [6], and features extracted at this time-scale are de-noted short-time features.\n2.1. Short-time features\nThe Mel Frequency Cepstral Coefﬁcients (MFCC) have been\nselected as short-time features in this work. These coefﬁ-\ncients were originally developed for automatic speech recog-\nnition, aiming at deconvolving the effects of the vocal tractshape and the vocal cord excitation. However, they have\nbeen applied with great success in various ﬁelds of MIR,\nsee e.g. [7, 4, 3]. The features are perceptually inspired,meaning that they resemble the auditory system of humans.\nThe MFCCs are ranked in such a manner that the lower or-\nder MFCCs contain information about the slow variations in\nthe spectral envelope. Hence, including the higher MFCCs\na richer representation of the spectral envelope will be ob-tained.\nFor this investigation, the 6initial MFCCs have been\nused, including the ﬁrst coefﬁcient, which is correlated withthe perceptual dimension of loudness. In the investigations,\neach music snippet is power normalized prior to the MFCC\nextraction stage. A frame-size of 30 ms and a hop-size of\n7.5m s have been applied in all experiments to minimize\naliasing in the MFCCs.\n2.2. Temporal feature integration\nTemporal feature integration is the process of combining all\nthe feature vectors in a time-frame into a single new fea-\nture vector in order to capture the relevant information inthe frame. Formally, this amounts to the following (see also\nFig. 1):\nz\nk=f(xk·hsx,xk·hsx+1,...,xk·hsx+fsx−1),(1)\nwherexrepresents the short-time features (MFCCs), fsxis\nthe frame-size, and hsxthe hop-size, both deﬁned in a num-\nber of sample manner. Function f(·)maps the sequence of\nshort-time features into a single vector zk, fork=0,1,...,\nK−1.\nIn [4] it was proposed to perform temporal feature in-\ntegration by estimating the power spectrum of the MFCCsusing the periodogram method [8]. In addition to this, the\nauthors propose to summarize the energy in different fre-\nquency bands using a predeﬁned ﬁlter bank:\n˜z\n(i)\nk=WTz(i)\nk(2)\nwhere z(i)\nkis a periodogram of dimension Dzof the i-th\nMFCC coefﬁcient over some frame fsx,k=0,...,K −1\nis the index at the larger time-scale, and Wcomprises the\nfrequency magnitude response of the ﬁlter bank. Finally,\nthe feature vector ˜z(i), which has as many components as\nthe number of ﬁlters in the bank, is used as an input to the\nsubsequent classiﬁcation process.\nIn other words, the temporal feature extraction stage con-\nsists of estimating the periodogram of each MFCC dimen-\nsion independently over some time-frame fsx, after which\na ﬁlter bank Wis applied. In the coming sections we have\nremoved the superscript i, meaning that each short-time fea-\nture dimension is processed independently, using the same\nﬁlter bank for all MFCCs.\nThe ﬁlter bank Wis a matrix of dimension Dz×4, where\nDz=fsx\n2+1(throughout this paper we will use fsx= 256 ,\nso that Dz= 129 ), which simply summarizes the power\ncomponents in four frequency bands:\n1.0H z (DC value)\n2.1−2H z (beat rates)\n3.3−15 Hz (modulation energy, e.g. vibrato)\n4.20−sx\n2Hz(perceptual roughness)\nwhere the sampling rate sxis related to the hop-size ( hsx).\nThis ﬁlter bank ( W) has been suggested for general audio\nclassiﬁcation and is inherently positive, since it is applieddirectly on the estimated power spectrum (periodogram).\nThe ﬁlter bank, however, can easily become sub-optimal for\na speciﬁc music organization task, which is the reason forsuggesting a method for ﬁnding an optimal ﬁlter bank in a\nsupervised manner. The proposed method for the optimal\ndesign of the ﬁlter bank is the topic of the next section.MFCCAudio\nDx(= 6)\nDz\nD˜zx\nz\n˜zTemporal Feature IntegrationFilter BankPeriodogram\nfsxfs=3 0m s\n(W&U)\nClassiﬁer\nPostprocessing\nDecision\nFigure 1. The ﬁgure illustrates the ﬂow-chart of the complete\nprocess. After MFCC extraction, periodograms are computed\nfor each MFCC. The output of the “periodogram” box is a\nDz= 129 dimensional vector for each MFCC, correspond-\ning to the power in the different frequency bands. The ﬁlter\nbank (WorU) summarizes the power in predeﬁned frequency\nbands. The dimension of ˜z, denoted by D˜zwill depend on the\nnumber of MFCCs, the selected frame-size fsxand the num-\nber of ﬁlters in the ﬁlter bank W(ﬁxed to 4)o rU(nf).\n3. Supervised Design of Filter Banks\nAs can be understood from our previous discussion, and\nsince the goal is to optimize the classiﬁcation performance\nof the whole system, a better behavior can be obtained if the\nﬁlter bank is designed in a supervised manner, i.e., to opti-mize the performance in some training dataset whose labels\nare used during the design process.\nThen, we will assume that we are given a set of Ntrain-\ning pairs {z\nk,yk}N\nk=1, withykbeing the label vector asso-\nciated to zk. The Cdimensional vector yk, where Cis the\nnumber of classes, contains a one in the position of the true\nlabel for pattern zk, and zeros elsewhere. In this section,\nwe address the issue of how one can use the training datato design a ﬁlter bank U=[u\n1,u2,...,unf], where umis\nthe frequency amplitude of the m-th ﬁlter and nfis the total\nnumber of ﬁlters in the bank, in such a way that the outputs\nof the ﬁlters,\n˜zk=UTzk, (3)\nare as relevant as possible for the classiﬁcation task at hand4.\n4Note that we have opted to use Uto denote the ﬁlter bank obtained\nwith our method, to differentiate it from the ﬁlter bank from [4] that weFrom its deﬁnition, and given that this matrix operates on the\npower spectrum of the different MFCCs, it should be clear\nthat all elements in Ushould be non-negative numbers (i.e.,\nuij≥0), so that ˜zkcan be effectively interpreted as the out-\nput energies of a ﬁlter bank. Note that a negative uijwould\ncorrespond to the subtraction of the energy in a certain fre-quency band.\nThe procedure we present lies within the framework of\nMultivariate Analysis methods [9], and, in particular, it is\na variant of Orthonormalized Partial Least Squares (OPLS).\nNext, we will brieﬂy review OPLS, and explain how it can\nbe solved under the additional constraints u\nij≥0, resulting\nin a method that we have called Positive constrained OPLS\n(POPLS). Readers that prefer to skip the implementation de-\ntails of the method, can go directly to the experiments sec-tion.\n3.1. Multi-regression model for Feature Extraction and\nData Classiﬁcation\nFor the classiﬁcation process (see Fig. 1) we will consider\na multi-regression model. Although other models are pos-\nsible, we will see that the regression approach results in a\nvery convenient method for computing the ﬁlter bank. The\nmulti-regression model can be written as\nˆy\nk=BUTzk+b=B˜zk+b, (4)\nwhere ˆykis the predicted output, and {B,b}are the free\nparameters of the model. In particular, bis a bias that com-\npensates for the different means of the input and output vari-\nables. Note that, since BisC×nfandUisDz×nf, the\nﬁlter bank is effectively imposing a bottleneck in the sys-\ntem, in the sense that the ˜zkvectors given to the classiﬁer\nare lower dimensional than the original zk. This dimension-\nality reduction is very useful to simplify the design of theclassiﬁer and to improve generalization, and is an unavoid-\nable step when the training dataset is small. However, in\norder to not degrade the performance of the classiﬁer, it is\ncrucial that ˜z\nkretains the most discriminative information\ninzk, what can only be achieved with a good design of the\nﬁlter bank.\nOur aim is to adjust all parameters in the model, as well\nas the ﬁlter bank, to minimize the sum-of-squares of the dif-\nferences between the real and estimated labels, i.e.,\n[Uo,Bo,bo]=a r gm i n\nU,B,b/bardblY−B˜Z−b1T/bardbl2\nF (5)\nwhere Y=[y1,...,yN],1is an all-ones vector of appro-\npriate dimensions, and ˜Z=UTZwithZ=[z1,...,zN].\nSubscript ‘ F’ refers to the Frobenius norm of a matrix.\nIt is known that Bocan be obtained as the solution of the\nwill denote with Wthroughout the paper.following modiﬁed problem:\nBo=a r gm i n\nB/bardblYc−B˜Zc/bardbl2\nF\n=Yc˜ZT\nc(˜Zc˜ZT\nc)−1(6)\nwhere ˜ZcandYcare centered versions of ˜ZandY, respec-\ntively. Then, the bias is simply given by\nb0=1\nN(Y−B0˜Z)1. (7)\nOnce we have derived a closed form expression for Bo\nandbo, we are ready to present our POPLS method for\nthe selection of the optimal ﬁlter bank which minimizes (5)[10], subject to the constraint that all entries in Uare posi-\ntive.\n3.2. Positive Constrained OPLS\nTo start with, let us introduce the optimal regression matrix,\nB\n0, into (5). Taking also into account that ˜Zc=UTZc, the\nminimization problem can be rewritten as\nUo=a r gm i n\nU/bardblYc−Bo˜Zc/bardbl2\nF\n=a r gm i n\nU/bardblYc[I−ZT\ncU(UTZcZT\ncU)−1UTZc]/bardbl2\nF\nwithIbeing the Ndimensional identity matrix.\nNow, using the fact that /bardblA/bardbl2\nF=Tr{AAT}, and after\nsome algebra, we arrive to the following optimization prob-\nlem\nmaximize: Tr {(UTCzzU)−1UTCzyCyzU}(8)\nsubject to: UTU=I (9)\nuij≥0 (10)\nwhere we have deﬁned the covariance matrices Czz=ZcZT\nc,\nCzy=ZcYT\ncandCyz=CT\nzy, and where we have made\nexplicit the positivity constraint. The additional constraint(9) is needed to make the solution unique.\nThere are a number of ways to solve the above problem.\nWe will use a procedure consisting on iteratively calculatingthe best ﬁlter, so that we are not only guaranteeing that U\no\nis the optimal bank with nfﬁlters, but also that any subbank\nconsisting of some of the ﬁrst columns of Uois also opti-\nmal with respect to the number of ﬁlters used. In brief, the\nprocess consists of the following two differentiated stages:\n1) Solve the “one ﬁlter” optimization problem given by:\nmaximize:uTCzyCyzu\nuTCzzu(11)\nsubject to: uTu=1 (12)\nui≥0 (13)\n2) Remove from Ycthe prediction obtained from the cur-\nrent ﬁlter bank.\nInputs: Z,Y,nf\n1 - Calculate centered data matrices ZcandYc\n2-Czz=ZcZT\nc,Y(1)\nc=Yc\n3- F o r m=1,...,n f\n3.1 -C(m)\nyz=Y(m)\ncZcT;C(m)\nzy=C(m)\nyzT\n3.2 - Solve (11)-(13) to obtain um\n3.3 -Y(m+1)=Y(m)/bracketleftBig\nI−ZT\ncumuTmZc\nuTmCzzum/bracketrightBig\n4 - Output ﬁlter bank: Uo=[u1,...,unf]\nTable 1. POPLS pseudocode.\nTable 1 summarizes our POPLS algorithm for the super-\nvised design of ﬁlter banks. It is also worth mentioning that,in our implementation, the maximization problem (11)-(13)\nwas solved with the fmincon matlab function. However, in\nmost occasions, the convergence of this routine was not sat-isfactory, making it necessary to recur to an alternative rep-\nresentation of ubased on hyperspherical coordinates. The\nadvantage of this representation is that restriction (12) is di-\nrectly incorporated into the representation, what simpliﬁes\nthe application of any optimization algorithm.\n4. Experiments\nThis section considers two different experiments. The ex-periment described in Subsection 4.1 is a proof-of-concept\nexperiment that illustrates the basic idea of POPLS for dis-\ncriminating between an instrument played with and without\nvibrato. The second line of experiments described in Sub-\nsection 4.2 considers an 11music genre dataset, investigated\nusing the ﬁlter Wfrom [4] and the ﬁlter obtained from the\nPOPLS, U.\n4.1. Experiment 1: Instrument vibrato/non-vibrato de-\ntection\nThis experiment considers the problem of detecting vibrato\nor non-vibrato of a single instrument and is only intended as\na proof-of-concept example.\nA small dataset has been created consisting of music snip-\npets consisting of an alto saxophone with notes ranging from\nD\nb3to A b5(138.59−830.61 Hz ), with and without vibrato,\nresulting in a total of 64(32train / 32test) small music clips\neach of 3−4s. The music samples were extracted from the\nMIS (Music Instrument Samples) database developed by the\nuniversity of Iowa [11]. This database has been applied in\nconnection with automated instrument classiﬁcation in e.g.[12].\nOnly the ﬁrst MFCC has been used in this experiment,\nwhich is known to be correlated with the perceptual dimen-sion of loudness. A frame-size ( f\nsx) corresponding to 960 ms\nand a corresponding hop-size of 240 ms were selected. The\nframe-size was selected to ensure a few periods of the mod-Frequency [Hz]AmplitudeFilter No. 11\n0.9\n0.8\n0.7\n0.6\n0.5\n0.4\n0.3\n0.2\n0.1\n0\n0 10.420.731.041.351.762.0\nFrequency [Hz]AmplitudeFilter No. 21\n0.9\n0.8\n0.7\n0.6\n0.5\n0.4\n0.3\n0.2\n0.1\n0\n0 10.420.731.041.351.762.0\nFigure 2. The left and right ﬁgure illustrates the ﬁrst and sec-\nond most discriminative ﬁlters extracted by the POPLS proce-dure, respectively.\nulation frequency of 4−5H z, hence, obtaining a better spec-\ntral estimate for the instruments played with vibrato.\nLeave-one-out cross-validation (LOO-CV) [13] was ap-\nplied to access the test-accuracy. In each fold, the optimalﬁlters were calculated using the POPLS method described in\nSubsection 3.2, and the resulting error was obtained using a\nlinear classiﬁer. The LOO-CV classiﬁcation error obtained\nusing n\nf=2 5 ﬁlters was 19% at the 960 ms time-scale,\ngetting as low as 9.4%when performing weigted voting5\nacross the frames in each music sample to achieve a single\ndecision of each music sample. It is noted that, when us-\ning the ﬁxed ﬁlter bank W, close to random performance\n(48.3%, where random performance is 50%) is obtained,\nwhich is ascribed to a smearing of the relevant frequency\ncomponents, since the ﬁlter is summarizing the frequencies\nbetween 3−15 Hz .\nThe two ﬁlters with largest discriminative performance\nprovided by POPLS have been illustrated in Figure 2. The\nleft ﬁgure, which illustrates the ﬁlter with largest discrimi-\nnative performance, clearly indicates that the most relevantinformation concerning the modulation (vibrato ∼4−6H z)\nof the instrument is learned by the POPLS. Using only these\ntwo ﬁlters a classiﬁcation error of 20% is obtained using\nweighted voting to obtain a single decision per music sam-\nple.\n4.2. Experiment 2: Music genre classiﬁcation\nThe experiment described in this subsection considers the\nﬁxed ﬁlter bank Wand the POPLS method for determining\na ﬁlter bank Uin an 11music genre classiﬁcation setup.\n4.2.1. Dataset\nThe dataset has previously been investigated in [5, 14], and\nconsists of 1317 music snippets each of 30 s.distributed\nevenly among the 11music genres: alternative, country,\neasy listening, electronica, jazz, latin, pop&dance, rap&hip-\nhop, r&b and soul, reggae and rock, except for latin, which\nonly has 117music samples. The labels have been obtained\nfrom an external reference. The music snippets consist of\n5Weighted voting is the process of selecting class membership by sum-\nming across the output vectors of the classiﬁer, ˆyk, corresponding to all\nfeature vectors ˜zkbelonging to the same clip. The class that obtains the\nlargest sum is the “voted” class.\nCross-validation test error [%]\nFilter banks - nf12346062646668707274767880\nFilter banks - nf0 5 10 15 20 256062646668707274767880\nFigure 3. The left ﬁgure illustrates the mean cross-validation\nerror for the ﬁxed ﬁlter bank ( W, solid line) and the ﬁrst 4\nﬁlters of the POPLS procedure ( U, broken line). The right\nﬁgure illustrates the mean cross-validation error for the nf=\n25ﬁlters obtained by the POPLS procedure. The error-bars\non both plots are ±the standard deviation of the mean.\nMP3 (MPEG1-layer3) encoded music with a bitrate of 128\nkbps or higher, downsampled to 22050 Hz . This dataset is\nrather complex having on the average 1.83songs per artist.\nPrevious results show that this is a difﬁcult dataset for genre\nclassiﬁcation (see, for instance, [14]).\n4.2.2. Initial investigations\nPrevious investigations of the frame-size conducted in [5]\nshowed that a frame-size ( fsx) of approximately 2swas op-\ntimal for the method in [4]. Since the aim is to illustrate\nthat a supervised determination of the ﬁlter bank is supe-\nrior to the ﬁxed ﬁlter bank, the same frame-size has beenused for POPLS. With a hop-size on the short-time features\nof7.5m s, this frame-size corresponds to approximately 256\nsamples. Due to the symmetry in the periodogram, the re-\nsulting dimensions of the ﬁlter banks become 129×n\nffor\nUand129×4forW. It was observed that the mean clas-\nsiﬁcation test error did not improve for nf>25, hence,\nnf=2 5 was the largest amount of ﬁlters investigated.\n4.2.3. Results & Discussion\nTo access the classiﬁcation accuracy of the two methods,\n10-fold cross-validation has been applied. In each fold, the\noptimal ﬁlters were estimated from the training set as de-\nscribed in Section 3, and the performance of the system was\nsubsequently evaluated on the corresponding test fold.\nFigure 3 shows the 10-fold cross-validation error as a\nfunction of the number of ﬁlters in the banks. The left ﬁg-\nure shows the cross-validation error obtained using only theﬁrst4ﬁlters of the POPLS, and using the ﬁxed ﬁlter bank\nW. It is observed that the ﬁlters obtained by the POPLS\nprocedure are on the average 2%better than the ﬁxed ﬁlter\nbank, shown in solid line. Furthermore, using only the ﬁrst\n3ﬁlter banks obtained by POPLS the cross-validation test\nerror is similar to the performance obtained using the ﬁxed\nﬁlter bank W. Although most of the important dynamical\nstructure of the MFCCs is captured by the ﬁrst few ﬁlters ofU, the right plot of Figure 3 shows that a signiﬁcant error\nreduction can be obtained when considering a larger number\nof ﬁlters, achieving error rates around 61% forn\nf>15.AmplitudeFilter No. 11\n0.9\n0.8\n0.7\n0.6\n0.5\n0.4\n0.3\n0.2\n0.1\n0\n0 10.420.731.041.351.762.0Filter No. 21\n0.9\n0.8\n0.7\n0.6\n0.5\n0.4\n0.3\n0.2\n0.1\n0\n0 10.420.731.041.351.762.0\nFrequency [Hz]AmplitudeFilter No. 31\n0.9\n0.8\n0.7\n0.6\n0.5\n0.4\n0.3\n0.2\n0.1\n0\n0 10.42 0 .731.041.351.762.0\nFrequency [Hz]Filter No. 41\n0.9\n0.8\n0.7\n0.6\n0.5\n0.4\n0.3\n0.2\n0.1\n0\n0 10.42 0 .731.041.351.762.0\nFigure 4. The four most discriminative ﬁlters, where the up-\nper left ﬁgure illustrates the most relevant ﬁlter for the speciﬁc\nmusic genre classiﬁcation setup.\nFigure 4 shows the ﬁrst 4ﬁlters obtained on a single\nfold using the POPLS. Filter 1includes the most important\nfrequencies of the MFCCs periodograms, which basically\ncover the modulation frequencies of instruments. Filters 2\nand3provide attention to the lower modulation frequencies,\nwith ﬁlter 2having frequency components at the beat-scale.\nFilter 4spans the higher modulation frequencies, which are\nrelated to the perceptual roughness. The difference between\nthe ﬁlters obtained for each training data fold is small, which\npartly illustrates that the proposed method is robust to noiseand, further, that the speciﬁc underlying temporal structure\nof the MFCCs is relevant for discriminating between the dif-\nferent genres.\n5. Conclusions and Future work\nIn this paper we have presented a method for designing ﬁl-\nter banks that are able to learn the important dynamics in\nshort-time features for a given classiﬁcation task. The pro-\nposed method is very versatile, in the sense that it can be\napplied to any discrimination task, as we have illustrated inour experiments section, where we tackled two very differ-\nent classiﬁcation problems, namely, the detection of vibrato\nin instrument music clips, and music genre classiﬁcation.The advantage of our approach over other feature extraction\nmethods, is that it provides an elegant physical interpreta-\ntion of the extracted features, in terms of the dynamical be-\nhavior of the MFCCs time series.\nAlthough here we limited the method to provide an unique\nﬁlter bank, it is straightforward to allow for different ﬁlters\nfor each of the MFCCs. Exploiting the cross-correlation\namong the different ﬁlters in the bank, could also be usedto improve the accuracy of the whole system. These lines,\nas well as the application of the method to other MIR prob-\nlems, constitute logical directions for future research.6. Acknowledgments\nThis work is partly supported by the Danish Technical Re-search Council, through the framework project ‘Intelligent\nSound’, www.intelligentsound.org(STVF No. 26-04-0092),\nand by the European Commission through the sixth frame-work IST Network of Excellence: Pattern Analysis, Sta-\ntistical Modelling and Computational Learning (PASCAL),\ncontract no. 506778. The work of J. Arenas-Garc´ ıa was also\nsupported by the Spanish Ministry of Education and Science\nwith a postdoctoral fellowship.\nReferences\n[1] H. Homburg, I. Mierswa, B. M¨ oller, K. Morik, and M. Wurst.\nA benchmark dataset for audio classiﬁcation and clustering.\nInInternational Symposium on Music Information Retrieval ,\npages 528–531, 2005.\n[2] J. Bergstra, N. Casagrande, D. Erhan, D. Eck, and B. Kgl.\nMeta-features and adaboost for music classiﬁcation. Ma-\nchine Learning Journal : Special Issue on Machine Learning\nin Music , 2006.\n[3] M. Mandel, G. Poliner, and D. Ellis. Support vector machine\nactive learning for music retrieval. Accepted for publication\nin ACM Multimedia Systems Journal , 2006.\n[4] M. F. McKinney and J. Breebart. Features for audio and\nmusic classiﬁcation. In International Symposium on Music\nInformation Retrieval , pages 151–158, 2003.\n[5] A. Meng, P. Ahrendt, J. Larsen, and L. K. Hansen. Temporal\nfeature integration for music genre classiﬁcation. Submitted,\n2006.\n[6] J.-J. Aucouturier, F. Pachet, and M. Sandler. The way it\nsounds : Timbre models for analysis and retrieval of poly-phonic music signals. IEEE Trans. on Multimedia , 7(6):8,\nDecember 2005. (in press).\n[7] P. Ahrendt, A. Meng, and J. Larsen. Decision time horizon\nfor music genre classiﬁcation using short time features. InEUSIPCO , pages 1293–1296, Vienna, Austria, sept. 2004.\n[8] M. H. Hayes, Statistical Digital Signal Processing and Mod-\neling , N.Y .: Wiley, 1996.\n[9] T. W. Anderson, An Introduction to Multivariate Statistical\nAnalysis , 3rd Ed., N.Y .: Wiley-Interscience, 2003.\n[10] S. Roweis and C. Brody. Linear Heteroencoders. Gatsby\nUnit Technical Report GCNU-TR-1999-02 , Gatsby Compu-\ntational Neuroscience Unit, London, 1999.\n[11] University of Iowa musical instrument sample database,\nhttp://theremin.music.uiowa.edu/index.\nhtml .\n[12] E. Benetos, M. Kotti, C. Kotropoulos, J. J. Burred, G. Eisen-\nberg, M. Haller, and T. Sikora. Comparison of subspace\nanalysis-based and statistical model-based algorithms for\nmusical instrument classiﬁcation. In Proc. of 2nd Work-\nshop On Immersive Communication And Broadcast Systems\n(ICOB) , October 2005.\n[13] C. M. Bishop. Neural Networks for Pattern Recognition . Ox-\nford University Press, 1995.\n[14] A. Meng and J. Shawe-Taylor. An investigation of feature\nmodels for music genre classiﬁcation using the Support Vec-\ntor Classiﬁer. In International Conference on Music Infor-\nmation Retrieval , pages 604–609, 2005."
    },
    {
        "title": "Identifying music documents in a collection of images.",
        "author": [
            "David Bainbridge 0001",
            "Tim Bell 0001"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1416424",
        "url": "https://doi.org/10.5281/zenodo.1416424",
        "ee": "https://zenodo.org/records/1416424/files/BainbridgeB06.pdf",
        "abstract": "Digital libraries and search engines are now well-equipped to find images of documents based on queries. Many images of music scores are now available, often mixed up with tex- tual documents and images. For example, using the Google “images” search feature, a search for “Beethoven” will re- turn a number of scores and manuscripts as well as pictures of the composer. In this paper we report on an investiga- tion into methods to mechanically determine if a particular document is indeed a score, so that the user can specify that only musical scores should be returned. The goal is to find a minimal set of features that can be used as a quick test that will be applied to large numbers of documents. A variety of filters were considered, and two promising ones (run-length ratios and Hough transform) were evalu- ated. We found that a method based around run-lengths in vertical scans (RL) that out-performs a comparable al- gorithm using the Hough transform (HT). On a test set of 1030 images, RL achieved recall and precision of 97.8% and 88.4% respectively while HT achieved 97.8% and 73.5%. In terms of processor time, RL was more than five times as fast as HT. Keywords: Optical Music Recognition (OMR), Score Clas- sification, Music Image",
        "zenodo_id": 1416424,
        "dblp_key": "conf/ismir/BainbridgeB06",
        "keywords": [
            "digital libraries",
            "search engines",
            "images of documents",
            "music scores",
            "mixed up with text",
            "Google images",
            "investigation methods",
            "mechanically determine",
            "document is a score",
            "minimal set of features"
        ],
        "content": "Identifying music documents ina collection ofimages\nDavidBainbridge\nDepartmentofComputerScience\nUniversityofWaikato\ndavidb@cs.waikato.ac.nzTim Bell\nDepartmentofComputerScienceandSoftwareEngineering\nUniversityofCanterbury\ntim.bell@canterbury.ac.nz\nAbstract\nDigital libraries and search engines are now well-equipped\ntoﬁndimagesofdocumentsbasedonqueries. Manyimages\nofmusic scoresare nowavailable,oftenmixedupwithtex-\ntual documentsandimages. For example,using the Google\n“images” search feature, a search for “Beethoven” will re-\nturna numberofscoresandmanuscriptsas well aspictures\nof the composer. In this paper we report on an investiga-\ntion into methods to mechanically determine if a particular\ndocumentisindeedascore,sothattheusercanspecifythat\nonlymusicalscoresshouldbereturned. Thegoalistoﬁnda\nminimal set of features that can be used as a quick test that\nwill beappliedto largenumbersofdocuments.\nA variety of ﬁlters were considered, and two promising\nones (run-length ratios and Hough transform) were evalu-\nated. We found that a method based around run-lengths\nin vertical scans (RL) that out-performs a comparable al-\ngorithm using the Hough transform (HT). On a test set of\n1030images,RLachievedrecallandprecisionof97.8%and\n88.4%respectivelywhileHTachieved97.8%and73.5%. In\ntermsofprocessortime,RLwasmorethanﬁvetimesasfast\nasHT.\nKeywords: OpticalMusicRecognition(OMR),ScoreClas-\nsiﬁcation,Music Image\n1. Introduction\nAgrowingamountofpublic-domainscannedandphotographed\nmusic is becoming available on the web. Some of this mu-\nsic is easily accessed throughwell-indexedlibrary system s,\nwhile other music is part of ad-hocprivate collections. It i s\npossible to search for these images of music using a search\nengine, but the results are rather haphazard. For example,\nFigure 1 shows the results of a Google image search for\n“Clair de Lune”. Three of the ﬁrst 16 images displayed are\nscanned sheet music, while the others are clearly irrelevan t\nif oneis lookingforamusicscore.\nThe goal of this research is to provide an automatic ﬁl-\nter that distinguishes documents that are likely to be sheet\nmusic from other images, which may be photographsor art\nPermission to makedigital orhardcopies ofallorpartofthi s workfor\npersonal or classroom use is granted without fee provided th at copies\narenotmadeordistributed forproﬁtorcommercial advantag e andthat\ncopies bear this notice and thefull citation on the ﬁrstpage .\nc/circlecopyrt2006 University of Victoria\nFigure1. AGoogleimagessearchfor“ClairdeLune”,withno\nﬁltering.\nrelatingtothetitle, ormoreoften,albumcoversandposter s\nrelating to recordings and performances of the piece being\nsearchedfor.\nSuch a ﬁlter might be used by a search engine (either\nlocal or external) to pre-classify documents at the indexin g\nstage,orasapost-ﬁltertoeliminatequeryresultsjustbef ore\nthey are presented to the user. It might also be used as a\npreliminary phase to OMR so that the recognition is only\nperformed on music pages, avoiding wasted time on title\npages and textual commentaries. It could also be used as\na desktop search for a personal ﬁle space that contains a\nmixtureofmusicandotherimages.\nSucha systemneedstobefast, particularlyifbeingused\nas a post-ﬁlter, to provide good response times. Inevitably\nspeed will be correlated with accuracy, and a fast system\nmay not be completely accurate. Ideally the ﬁlter should\nfavourfalsepositivesoverfalsenegatives(thatis,highr ecall\nover high precision), since a false positive will just creat e\nmoreworkfortheuser,whereasafalsenegativewillprevent\ntheuser fromﬁndingavalidmusicimagealtogether.\nNote that the kind of material to be ﬁltered will depend\non how the initial query is formulated. Unfortunately theuser cannot simply add phrases like “music” or “score” to\ntheir search, as these may not have been used in conjunc-\ntion with a music image. Some composer’snames are well\nrecognised (e.g. Beethoven) and will generally be a good\nstartingpointfora search,althoughothersdonothavesuch\ndistinct names (e.g. “John Williams”), while others share a\nname with other famous events or people (e.g. the Britten\nmotorcycle). The titles of musical pieces can also be over-\nloaded, such as the “Clair de Lune” example earlier, or any\nother title that evokes imagery. These issues make a simple\nﬁlter all themoreimportantforsiftingoutrelevantimages .\nWe have focussed on detecting CMN (common music\nnotation), although have tried to be fairly broad in what\nwe acceptas music to allow for applicationswhere the user\nmight be interested in viewing images even if they are low\nquality or incomplete. Most CMN music is written on ﬁve-\nlinestaves,anditistheseregularlinesthatarethemainfe a-\nture that we use to detect music. However, the techniques\ndiscussed do not require the lines to be in sets of ﬁve, so\nthey should detect, for example, 6-line guitar tablature, o r\n4-linepneumenotation.\n2. Simpleclassiﬁcationmethods\nThere are some straightforward ways to ﬁlter music based\non simple features of the image ﬁle. For example, the size\nof the image is important; an image that has been captured\nat less than 100dpi is unlikelyto be veryuseful, and at this\nlowresolutionevenjusttwolinesofmusiconapagewould\nneed to be approximately 60,000 pixels, while a full sheet\nis morelikelyto be well over500,000pixels. Thiscriterion\nalonewilleliminatealotofphotographsandimagesthatare\nusedonwebpages.\nFor example, a Google image search for “Bach fugue”\nreturns about 1,270 images. However, limiting the search\ntoGoogle’s“large”images(thethresholdseemstobethose\nwith more than about 400,000pixels) producesjust 137 re-\nsults. While this excludes a number of smaller images that\nwe would recognise as sheet music, the smaller images are\ngenerally quite low resolution, mainly used as thumbnails\nand samples. Of the 137 large images, some 106 are rele-\nvant sheet music, which can be expressed as a precision of\n106/137,or77%.\nOf course, smaller images of music may be of interest\ntoo. These can be incipits or sample ﬁles that lead the user\nto sourcesofthemusic suchason-linemusicstores.\nAnothersimplefeaturemightseemtobetheuseofcolour,\nsince sheet music is invariably black and white. However,\nmany historical score collections use full colour images to\ncapture the detail on the page, and even a black and white\nimagemightbestoredinaformatthatincludessomecolour\n(suchasina framearoundthe image).\nAs an example of using colour as a ﬁltering criterion,\nTable 1 shows the breakdown of the pages returned in the\nGooglesearchfor“Bach fugue”(largeimagesonly). ThreeFilter Number Number Precision Recall\nfound relevant\nNone 137 106 77% 100%\nGrayscale 65 63 97% 59%\nBi-level 18 18 100% 17%\nGrayor 83 81 98% 76%\nbi-level\nTable1. PrecisionandrecallusingforGoogleimagesearchf or\n“Bach fugue”,large images only.\nlevels of ﬁltering were used: no ﬁltering (“any colours”),\ngray-scale only, and bi-level (“black and white”) only. The\nprecision column shows what percentage of the images re-\ntrievedwasrelevant(i.e.representedimagesofscores),w hile\ntherecallcolumn shows what proportion of the relevant\nimages were retrieved (assuming that the unﬁltered search\nfound all relevant documents). Without any restriction on\nthe colour of the images, 22.6% of the images were false\npositives, whereas the grey-scale only restriction produc ed\njust 2 false positives out of 65 images, and all of the bi-\nlevel imageswere relevant. Unfortunatelythe latter two ﬁl -\nters also produces a lot of false negatives; the grey-scale\nﬁlter rejected 43 (40.6%) of the relevant images, while the\nbi-level ﬁlter rejected 88 (83%). Moderately good results\nareachievedinthiscasebycombiningthegrayandbi-level\ndocuments, giving 98% precision and 76% recall, but bear\ninmindthattheuserislikelytofavourrecalloverprecisio n.\nAnother problem with using colour as a ﬁlter is that for\nlater composers who lived in the period of black and white\nphotography(late 19th and early 20thcentury),manygray-\nlevel images exist of the individual. For example, a search\nfor“Bartokconcerto”ﬁlteredbygray-levelproducesonly5\nmusic images out the 47 returned, a precision of just 11%\ncomparedwith 97%forthe“Bach fugue”example.1\nThus ﬁltering for colour is more likely to produce false\nnegatives, which is not desirable for this application, al-\nthough would be useful if the user requires high precision\nratherthanrecall.\nAnothersimple approachis simply to run Optical Music\nRecognition (e.g. [1], [2], [3]) on each candidate page and\nmeasure how many valid objects are found, or how many\nerrorsoccur. However,a full OMR system wouldtake con-\nsiderablylongertoprocessapagethanthesimpleﬁlterswe\npropose below. The most time-consuming stage is classi-\nfying the primitive shapes—note heads, stems, tails, clefs ,\naccidentals and so forth—that constitute a work. This pro-\ncessingcostcanbeasmuchas80timeshigherthanthetime\nspent locatingstave lines[1]. Furthermore,mostOMR sys-\ntemsworkonbi-levelimagesonly,somostofthecandidate\nimages would need to be convertedbefore being processed\nandit is notclear howwell these existingalgorithmswould\n1Itisironicthatcomposers fromearlier periods could bedep icted more\noften using colour images.workwhenappliedtoimagesthathavebeenconvertedusing\nstandardbinarythresholdingtechniques.\nA simpler approachis just to performa small part of the\nOMR process, such as stave detection [4], and determine\nhowsuccessfulithasbeen. Thisiseffectivelywhatwehave\ndone—generalisedtohandlegrayscaleandcolourimages—\nwith theﬁltersproposedbelow.\n3. Improved classiﬁcationmethods\nWe have explored a number of properties that can be eval-\nuated quickly and might detect the main features of an im-\nageofmusic,whichisprimarilythepresenceofstavelines.\nMany of these techniques are typical of OMR systems, but\nin an OMR system one starts with the assumption that the\nimageismusic,andhencewhiletheyareeffectiveforrecog-\nnisingmusic,theyarenotnecessarilyeffectiveforreject ing\nimagesthat arenotmusic.\nThefollowingmethodswerenotsuccessfulinthemselves,\nalthough may possibly be used in combination with other\nfeaturesfora reﬁnedclassiﬁcation.\nHorizontalprojections : Horizontal projections of a mu-\nsic image usually show peaks where the stave lines\nare. However,todetectmusicweneedtodetectregu-\nlarlyspacedprojections,andalsothismethodreliesof\nthe image being reasonably straight (the cost of cor-\nrectingtheimagewouldbetoohighgiventhatweare\nlooking for a simple ﬁlter). More importantly, many\nnon-musicimages(especiallytext)sharetheproperty\nofhavingregularlyspacedpeaksinhorizontalprojec-\ntions.\nPixeldensity : Thisverysimplemeasureisofsomeuse as\nmusic images tend to have approximately 10 to 20%\nblackpixels,comparedwithtextwhichisaround5%.\nHowever,otherimagescanhaveaverywiderangeof\nratios,andmusicwithwidemarginscanhaveanarbi-\ntrarily low density, while heavy stavelines and dense\nwritingcangiveadensityuptoabout30%.\nThefollowingmethodshaveshownpromise,andareeval-\nuatedmorecarefullyinthe experimentsbelow.\nHoughtransform(HT) : TheHoughtransform,initsmost\ngeneralform,isamethodforevaluatingthethelikeli-\nhoodthataparameterisedfeatureexistswithinanim-\nage [5]. Finding straight lines (at arbitrary angles) is\naparticularinstantiationofthetransformandonethat\nsees it often used in computer vision work. For our\npurposes the formulation for straight lines is of par-\nticular relevance to our task at hand—locating stave\nlines.\nWe use its normalised form for our implementation\nwhere a straight line is described using polar coordi-\nnates: randθdescribe a line that is perpendiculartotheangle θatdistance rfromtheorigin. Valuesinthe\n[θ, r]transformed space with high values correspond\ntostrongevidenceoftherebeingalineintheoriginal\nimageat thatposition.\nThe Houghtransform can be computationallyexpen-\nsive, and since in practice we were only interested in\nhorizontal and near horizontal lines the full range of\ntheta was not calculated. Through trial and error we\nfoundtherangerestrictedto ±3◦workedwell.\nRun-lengthratios(RL) : This method involves following\nvertical scan lines down the page and measuring the\nratioofthelengthofadjacentrunsofblackandwhite\npixels. As the scan line passes throughan emptypart\nof a music stave, the alternating runs of black and\nwhitepixelswillbethestavelines(black)followedby\nthe gap between the lines (white). Typically the gap\n(runsofwhitepixels)isaround5to6timestheheight\nof the lines (run of black pixels). Of course, the scan\nlineswill alsoencountermusicalsymbolsandtexton\nthe page, but typically about 25% of the area of the\nstave lines have no symbols superimposed on them,\nandso thedominantratiois that ofthe heightof gaps\ntotheheightofthelines. Wesimplylookforthemost\nfrequentratio.\nIn practice, the ratio for music images can become\nas small as one to one (for very small score images,\nwhich of course are practically unreadable), and as\nlarge as 11 to one (where the manuscript lines are\nvery light). In the experiments we allowed for this\nfairly large range, as even with extreme values it of-\nferedgoodsensitivityforﬁndingmusicimages.\nA goodfeatureof this methodis that (like the Hough\ntransform) it is not sensitive to small rotations in the\nimage. A disadvantage is that is is not able to detect\nsingle-line staves, such as a page of percussion mu-\nsic. Also, it assumes a black and white image, which\nrequires colour and gray-scale images to be thresh-\nolded;thisisdiscussedfurtherintheexperimentalde-\nscription.\nFor further discrimination, we included a measure of\nhow common the most frequent ratio was. Typically\nat least 10%, and more usually 20%, of the samples\nformusicwouldhavethemostcommonratio,whereas\nnon-music images that had a promising ratio usually\ndidn’t have them so dominant. The more sparse the\nmanuscript is, the more dominant the ratio is. Mu-\nsic that makes heavy use of beams seems to have the\nworst effect on this ratio, but even in this case there\nwill bea numberofuntouchedstave lines.\nHybrid: This method uses run-length ratios (RL) as the\ninitial ﬁlter,andthenthe Houghtransform(HT)ﬁlteris applied to the images accepted as music as a sec-\nondaryﬁlter toeliminatefalsepositives.\nAn important way to accelerate almost any method that\nisclassifyingimagesistousesamplesofeachimage,rather\nthan working with the entire image. For example, the test\ncould be applied to a number of randomlychoses small re-\ngions within the page, or for a single, slightly larger re-\ngion from the centre of the page. This idea was used by\nG¨ ocke[6],whowasperformingmoredetailedclassiﬁcation\nofmusicimagesto identifywhothe writerwas.\nFortheHoughtransform,restrictingtheangle θis effec-\ntivelyaformofsampling,butinadditiontheactualareasof\nthe source image that were transformed was also limited.\nVarious selection strategies were trialled: for example, a\nrandompattern and regionsstaggereddiagonallyacross the\npage. Takingpairsofrectangularregions,onelocatedonth e\nleftandtheotherlocatedontheright—amethodthatechoes\nthat of OMR systems that use horizontal projects to locate\nstave lines—was found to work reliably across a range of\nsituations. Regions were 120 ×60 pixels, except where the\nsource image dimension was so small it could not support\ntwo side by side. In such cases, the available width was\ndivided in two. Where the source image was tall enough,\nregionswheretakenatthetop,middleandbottom. Thiscol-\nlapseddowntotwoandthenonewhenspacewasrestricted.\nForimagesgreaterthan500pixelshigh,the topmiddleand\nbottom approach was extended to two at the top, three in\nthe middleandtwo at the bottom. The extralineswere also\nindentedby20%.\nFor the run-length ratio method, the natural sample to\ntake is a small number of single-pixel wide vertical slices\ndownthepage. Becausea randomlyselectedareaofa stave\nis very likely to be unpopulated with notation, the prob-\nability of even a single slice providing useful samples is\nveryhigh,withthemainproblembeingthatthereisasmall\nchancethat aparticularslice will encounterbarlinesorno te\nstems all the way down the page (particularly because it is\nnotunusualforbarlinestobealignedeveniftheyareindif-\nferentsystems).\nA useful side-effect of performing a constant number of\nsamples for each image is that it avoids spending too much\ntimeonlargeimages;someofthelargerimagescontainsev-\neral megapixels, and it simply isn’t worth processing every\npixel.\n4. Experimental evaluation\nThe three methods proposed above (HT, RL and hybrid)\nwereevaluatedonasetofmusicthatwasobtainedusingthe\nGoogle image search (all sizes and colours). Five queries\nwereused: “beethoven”,“sonatasheet”,“sheetmusic”,“mo zart”,\nand “overture sheet”. Note that inverted commas were not\nused in the queries. To generate a reasonable cross-section\nof monochrome, grayscale and colour samples, each querywas repeated three times with the appropriate colour-ﬁlter\non, and the results aggregated. For each query, the images\npresentedontheﬁrst ﬁvepagesweredownloaded.\nGooglepresented1218thumbnailsofrelevantimagesin\ntotal for the above queries, although when we downloaded\nthe (source) images, only 1030were available, that is, 15%\noftheimageshadbeenremovedfromtheirwebpagessince\nGoogle indexed them, or else their web server was not re-\nsponding at the time we ran our sweep. There were only\n8 images that were returned by more than one of the ﬁve\nqueriesmade,andtheseduplicateswereremoved.\nWethenmanuallyclassiﬁedtheimagestoproduceaground-\ntruth database. We needed to be clear on what we mean by\nimages of music. We have focussed on detecting Common\nMusic Notation (CMN) images, excluding other notations\nsuch as “piano roll” or textual formats. Also, our goal is\nsimplyto detect thatthe documentis likelyto be music;we\ndo notattempt toclassify it further(forexample,G¨ ocke[6 ]\nhasdoneworkinvestigatingclassifyingmusicimagestode-\ntermine who the authoris, or one mightwant to distinguish\nstylessuchastablatureorearlymusic).\nEvenwithin CMN notation,imagesencounteredinclude\nscreen-shotsofmusicprocessingprograms,andevenaphoto\nofamusicstandwithmusiconit! Wehaveusedthedistinc-\ntion that we will accept images where the reader can see\nsufﬁcient CMN music to identify more than three notes or\nsome other substantive musicological information (such as\nthe number of staves); this therefore includes incipits and\nsmall samples, as well as originalmanuscriptsand the soft-\nwarescreenshots,butexcludesaphotographthathappensto\nhavesomeunreadablesheetmusicaspartofthescene.\nOf the 1030 available images, the manual classiﬁcation\nto establish the ground truth found 367 images that were\nclassiﬁed as music, and 663 examples that were not. The\nimagesclassiﬁedasmusicwerethosewhereit waspossible\nin some way to read the noteson the page. For some of the\nmoreborderlinelow-resolutionimagesthiswouldinvolvea\nreasonable amount of guesswork to interpret the music, but\nwe triedtobeasinclusiveaswe reasonablycould.\nAnindependentsetof336imageswereusedtoﬁne-tune\ntheﬁlterssothatwewerenottrainingtheparametersonthe\nﬁlesthat theevaluationwasperformedon.\n4.1. Test sequence\nFactoring in the earlier observation about size and use of\ncolour in music (simple classiﬁcation methods) along with\nthe morespeciﬁcprocessingmethodsthat targetsthedetec-\ntionofstaves, thefollowingtest sequencewas developed:\n1. Istheimagebigenough?\n2. Doesit makelimiteduseofcolour?\n3. Istherea dominantbackgroundcolour?\n4. Test forstave lines(RL, HTorhybrid).Filter True False True False Precision Recall Total Process ing\nPositives Positives Negatives Negatives Time\nHoughTransform(HT) 357 129 536 8 73.5% 97.8% 205secs\nRun-Length(RL) 357 47 618 8 88.4% 97.8% 36secs\nHybrid 352 30 635 13 91.0% 96.4% 165secs\nTable2. Precisionandrecall andtimingresultsfor process ingthe 1030 test setof images.\nFigure2. Examplesofsomeofthemostborderlineimagesthat wereclassiﬁedinthegroundtruthasreadablemusic;thesea retypical\nof theimages thatcaused false negatives.\nIf an image fails at any of these steps it is immediately\nrejected. For image size, we set the limit at 120x60 pixels\nfor the smallest image considered since even at low reso-\nlutions this is barely large enough to represent more than a\nfew notes. Forlimiteduse ofcolourwe used theHSV (Hue\nSaturation Value) model and calculated a histogram based\nonthefrequencyofhuevalues. Ignoringfrequenciescounts\noflessthan10,iftheimageusedmorethan100huesit was\nrejected since even a colour image of music with illustra-\ntions does not exhibit this range of colour. Monochrome\nandgrayscaleimagestriviallymeetthisrequirementasthe y\nuse onlyonehue.\nFor the dominant background colour, the value compo-\nnent of HSV was used. Again a frequency histogram was\ncalculated and if less than 50% of the image values fell\nwithin20readingsofthemostfrequentvaluetheimagewas\nrejected. For sheet music to be readable there has to be a\ndecentcontrastbetweenthebackground(usuallywhite)and\nforegroundpixels. The essence of this test is that, allowin g\nfor some discolouration, the majority of the pixels present\nin the imageshould be the same. The test was posedin this\nway in an attempt to handle colour images of vintage sheet\nmusic where the paper has often turned a yellowy-brown\nover time. It also has the advantage that it works equally\nwellfor“reverse-video”sheetmusic(wherethebackground\nis black and the notation is represented in white), or a blue\nbackground for that matter (as occurred in one of the test\nsamples).\nIf an image passed all these tests, then it was tested for\nthepresenceofstavelinesusingoneofthethreealgorithms\nunder investigation: Hough transform, Run-Length and the\nhybrid.\nHaving performed the basic Hough transform, the deci-\nsion as to whether to classify an image as sheet music ornot is based on the number of high values that are found in\nthe transformed space. For each region pair, the maximum\nvalueinthetransformisretrieved(calculatedasasideeff ect\nofthebasictransform)anditiscomparedwiththecombined\nwidthofthetworegions. Ifthevalueislessthan25%ofthe\ncombinedwidth, then it is rejected outright. If not, then th e\nnumberofvaluesthatarewithin70%ofthemaximumvalue\niscomputedandthisistestedtoseeifitlieswithintherang e\nof1%-25%(exclusive)ofthe heightoftheregion.\nThe rationale for these limits is that we are looking for\nstrongevidenceofstraightlines(withintherange ±3◦)that\nare a modest percentage of the y-dimension. There has to\nbe somethingthere(hencethe1% lowerlimit)but we don’t\nwant it to get too high as this means there are more black\nlinesaroundthanwhitespace.\nThe RL method was implemented by categorising white\nto blackratiosinto a histogramwiththe ranges0to 0.5,0.5\nto 1.5, 1.5 to 2.5 etc., that is, we rounded the ratios to the\nnearestinteger. Mostmusicshowedastrongpeakataround\n5 or 6, but we accepted images where the histogram peak\nwasbetween1and11inclusive. Afurtherﬁlterwasapplied\niftheratiowas1or2,sincetheseratioswouldonlyreﬂecta\nthumbnail image, but might also correspondto effects such\nasditheringinanon-musicimage. Ifthedominantratiowas\n2thenifthepagewasmorethan160pixelshighwerequired\nthe ratio to occur at least 30 times; if it was 1 we simply\nrequired the ratio to occur at least 100 times. In addition,\nthe RL method evaluated used just 9 vertical scan samples\n(see section4.3).\nIn line with other steps in the decision-making process,\nthegenerationoftheHSVhistogramsisbasedacoarsesam-\npleratherthanvisitingeverypixelintheimage. Usingagri d\nformedfrom120pixelsevenlyspacedouthorizontally,and\n60 vertically,wasfoundto speedupprocessingtime signif-icantly,withoutcompromisingaccuracy.\n4.2. Results\nTable 2 summarises the results of processing the test set of\nimageswiththethreealgorithmsdevised,whichwasrunon\na 1.7 GHz Pentium processor with 1 GByte of RAM. Both\nrecallandprecisionarerelativelyhighforallthreemetho ds.\nThehybridmethodhasmoreprecision,butintheprocessof\nimprovingthe precision, it hasdecreased recall because th e\nextra step removes candidates, but does not add any. Both\nof the methods involving the Hough transform are signiﬁ-\ncantly slower then the RL method. Giventhat we are likely\nto favour recall over precision, the RL method gives excel-\nlent performanceand takes a fraction of the time to run the\ntests, andthusisthe recommendedmethod.\nManyofthefalsenegativeresultswerecausedbyimages\nthat were very low quality, generally with very low resolu-\ntion and/or faint colours. Figure 2 shows enlargements of\nsamples from images that caused these sorts of problems.\nThe low resolutionones are generally fromthumbnailsthat\nmight be of interest in that they could lead to other images\norlinksthatarerelevanttothequery,evenifthemusicisn’ t\nusefulin itself. Theimageswith faintcolourswouldproba-\nblycausefewerproblemsifanimprovedthresholdingalgo-\nrithmwereused.\n4.3. SamplingfortheRLmethod\nAn important trade-off for the RL method is the number of\nvertical scans to make; this affects both the precision/rec all\nandthetimetaken. Figure3showshowtheseareaffectedas\nthe number of samples increases. The main concern is the\nrecall,whichreaches97.8%if9samplescansaretaken;just\n8 of the 1030 images were incorrectly rejected. The preci-\nsion starts dropping beyond this point, despite the increas e\nin samples. This is because the extra samples are likely to\npick up other patterns in an image that aren’t spread uni-\nformly across the page. The speed is also shown in Fig-\nure3 asa percentageofthe test with 25samplesper image.\nOverall, about 9 samples per image provides an excellent\ntrade-offbetween the speed, precision and recall. Scannin g\nevery vertical line in the images takes about 20 times as\nlong as taking 9 samples per image, doesn’t offer any pre-\ncision/recall beneﬁt, and makes the evaluation time much\nmoredependentonthe sizeofthe image.\n5. Conclusions\nThe experiments have shown that it is feasible to provide a\nfast ﬁlter on an image search that distinguishes music im-\nages. The run-length ratio (RL) ﬁlter is particularly fast,\nand is able to work with relatively small samples of an im-\nage. We favoured false positives over false negatives, and\nwereabletochooseparametersforthesystemthatprovided\ngood rates for both of these types of errors, giving 97.8%\nrecalland88.4%precisionin ourexperiments.\nFigure 3. The precision/recall curves and time trade-off fo r\nsamplingintheRLmethod.\nThe Hough transform ﬁlter (HT) was almost an order of\nmagnitudeslowerthantherun-length(RL)ﬁlter, anddidn’t\nshowanadvantageinthequalityoftheﬁltering. Combining\nboth ﬁlters into a hybrid did improve the ﬁltering, at the\nexpense of a compromise to the speed of the system. In\npracticetheRL methodonits ownislikelyto befavoured.\nFurtherinvestigationscouldincludeusingamachinelearn -\ning system to evaluate a large numberof attributes (such as\npixel density, image size, colour range and run-length rati o\ndistribution) to ﬁnd more accurate ways to classify the im-\nages. Better methods for thresholding might also improve\nspeedandaccuracy.\nReferences\n[1] D.Bainbridge. ExtensibleOpticalMusicRecognition . Ph.D.\nthesis,Department ofComputer Science,UniversityofCan-\nterbury, NZ,1997.\n[2] K. MacMillan, D. Droettboom, and I. Fujinaga. Gamera:\nOptical music recognition in a new shell. In Proceedings of\nthe International Computer Music Conference , pages 482–\n485, 2002.\n[3] Kia Ng. Optical music recognition for printed music scor e\nand handwritten music manuscript. In Susan George, edi-\ntor,Visual perception of music notation: on-line and off-line\nrecognition , pages 108–127. IRMPress,2005.\n[4] Ichiro Fujinaga. Staff detection and removal. In Susan\nGeorge, editor, Visual perception of music notation: on-line\nand off-linerecognition , pages 1–39. IRMPress,2005.\n[5] R.O.Duda andP.E.Hart. Use ofthe Houghtransformation\ntodetectlinesandcurvesinpictures. Communicationsofthe\nACM,pages 11–15, January 1972.\n[6] Roland G¨ ocke. Building as system for writer identiﬁca-\ntion on handwritten music scores. In M.H. Hamza, edi-\ntor,Proceedings of the IASTED International Conference\nonSignalProcessing,PatternRecognitionandApplication s,\npages 250–255, Rhodes, Greece, 2003. Acta Press, Ana-\nheim, USA."
    },
    {
        "title": "Predicting genre labels for artist using FreeDB.",
        "author": [
            "James Bergstra",
            "Alexandre Lacoste",
            "Douglas Eck"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1416448",
        "url": "https://doi.org/10.5281/zenodo.1416448",
        "ee": "https://zenodo.org/records/1416448/files/BergstraLE06.pdf",
        "abstract": "This paper explores the value of FreeDB as a source of genre and music similarity information. FreeDB is a public, dy- namic, uncurated database for identifying and labelling CDs with album, song, artist and genre information. One qual- ity of FreeDB is that there is high variance in, e.g., the genre labels assigned to a particular disc. We investigate here the ability to use these genre labels to predict a more constrained set of “canonical” genres as decided by the cu- rated but private database AllMusic (i.e. multi-class learn- ing). This work is relevant for study in music similarity: we present an automatic, data-driven method for embedding artists in a continuous space that corresponds to genre simi- larity judgements over a large population of music fans. At the same time, we observe that FreeDB is a valuable re- source to researchers developing music classification algo- rithms; it serves as a reference for what music is popular over a large population, and provides relevant targets for su- pervised learning algorithms. Keywords: Music Similarity, Music Classification, Genre Recognition, FreeDB",
        "zenodo_id": 1416448,
        "dblp_key": "conf/ismir/BergstraLE06",
        "keywords": [
            "genre",
            "music",
            "similarity",
            "information",
            "FreeDB",
            "genre",
            "labels",
            "album",
            "song",
            "artist"
        ],
        "content": "Predicting genre labels for artists using FreeDB\nJames Bergstra, Alexandre Lacoste, and Douglas Eck\nDept. of Computer Science\nUniversit ´e de Montr ´eal\nCP 6128 succ Centre-Ville\nMontreal, QC\nH3C 3J7, Canada\nbergstrj,lacostea,eckdoug@iro.umontreal.ca\nAbstract\nThis paper explores the value of FreeDB as a source of genre\nand music similarity information. FreeDB is a public, dy-\nnamic, uncurated database for identifying and labelling CDs\nwith album, song, artist and genre information. One qual-\nity of FreeDB is that there is high variance in, e.g., the\ngenre labels assigned to a particular disc. We investigate\nhere the ability to use these genre labels to predict a more\nconstrained set of “canonical” genres as decided by the cu-\nrated but private database AllMusic (i.e. multi-class learn-\ning). This work is relevant for study in music similarity:\nwe present an automatic, data-driven method for embedding\nartists in a continuous space that corresponds to genre simi-\nlarity judgements over a large population of music fans. At\nthe same time, we observe that FreeDB is a valuable re-\nsource to researchers developing music classiﬁcation algo-\nrithms; it serves as a reference for what music is popular\nover a large population, and provides relevant targets for su-\npervised learning algorithms.\nKeywords: Music Similarity, Music Classiﬁcation, Genre\nRecognition, FreeDB\n1. Introduction\nThe importance of musical genre in music information re-\ntrieval is reﬂected in recent research efforts to create objec-\ntive and canonical genre hierarchies. [1] introduced a genre\nhierarchy that uses instrumentation and historical descrip-\ntors. Other researchers such as [2] have deﬁned relatively\ncomplete taxonomies for the purposes of comparing the per-\nformance of different classiﬁers. Genre has also been the\nfocus of MIR computing contests, the most recent being the\nMIREX 2005 [3] Symbolic Genre Classiﬁcation and Audio\nGenre Classiﬁcation contests.\nSeveral commercial websites offer genre labels as part\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies\nare not made or distributed for proﬁt or commercial advantage and that\ncopies bear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2006 University of Victoriaof their music descriptions. AllMusic1is perhaps the most\npopular, offering a wide range of information about bands,\nsuch as “Genre”, “Style”,“Similar Artists” and “Followers”.\nGraceNote (formerly CDDB)2, and FreeDB3provide mu-\nsic meta-data such as album title, song titles, group and\ngenre, indexed by a unique compact-disc identiﬁer.\nThese approaches treat genre as both canonical (i.e. gen-\nerated by well-established rules) and tied to the musical qual-\nity of a song. Unfortunately in reality this is not always the\ncase. Traditionally, the music industry has used the concept\nof genre to orient consumers in record stores and internet\nsites. This leads to situations in which genre is decided not\nby the type of music found on a CD but rather by market\npressures [4]. For example, a traditional “Blues” disc might\nbe placed in the “Rhythm & Blues” if that is where it is\nlikely to generate more sales.\nMore generally, [1] outline a number of problems asso-\nciated with musical genre as commonly used in the music\nindustry; [5] summarize:4\n1. They are designed for albums, not tracks.\n2. There is considerable disagreement among different\ntaxonomies on how to classify individual albums.\n3. Within these taxonomies, taxons do not bear ﬁxed se-\nmantics, leading to ambiguity and redundancy. (eg.\nGenres ’Jazz’ and ’Christmas’ overlap.)\n4. They are sometimes culture-speciﬁc, and often not re-\nlated to actual musical content. (eg. A genre such as\n’Christian’.)\n1.1. Two views of genre\nThis study compares two approaches to genre. The ﬁrst is\nthe traditional canonical view of genre, described above. As\nan example of this approach we use the AllMusic service\nwhich offers a clean and authoritative set of genre for ev-\nery album and artist in its database. AllMusic is curated and\nrelatively complete, and has shown itself to be very useful\nas evidenced by its immense popularity. However in addi-\ntion to suffering from the general weaknesses outlined above\n1http://allmusic.com\n2http://gracenote.com\n3http://freedb.com\n4The examples have been added by the authors, we hope that they are\nconsistent with the original intent.concerning canonical genre systems, AllMusic is also pro-\nhibitively expensive to license for unlimited access, and is\nthus of limited value to the MIR research community.\nThe second view of genre allows for multiple comple-\nmentary opinions about the genre of an album. As an ex-\nample of this approach we use the FreeDB system. In con-\ntrast to AllMusic, FreeDB is user-maintained and uncurated.\nConsequently, the genre labels in FreeDB have high vari-\nance and are drawn from a large set. While it is more com-\nmon for users to tag discs with the 125 genres from the id3v2\nstandard, FreeDB users are free to add their own genre tags\nif they wish.5To compare, there are 32 popular and classical\ngenres in AllMusic versus more than 500 in FreeDB6.\n2. Correlating AllMusic and FreeDB\nSince FreeDB permits multiple genre tags for a single al-\nbum, we can build a histogram over genres for an artist\nby summing over all the albums in his or her discography.\nWe observe that the probability distribution described by\nthe histogram could be highly correlated with the canoni-\ncal genre from a source such as AllMusic and also, more\ngenerally, be a useful descriptor for music similarity. In this\nwork we investigate the former point by performing a set\nof prediction experiments with the FreeDB database. We\ntest the ability of a simple machine-learning model to pre-\ndict the more canonical AllMusic genre information from\nthe FreeDB histogram. The latter point, regarding music\nsimilarity, is left for future work.\nTo quantify the extent to which AllMusic genre can be\npredicted from FreeDB, we collected genre histograms for a\nsmall but representative sample of artists in FreeDB, and\nbuilt a logistic regression model to predict the AllMusic\ngenre from the FreeDB genre.\n2.1. Data Collection\nThe entire FreeDB database is available from the FreeDB\nwebsite. Although the database is large (roughly 8GB in\nthe unix distribution) we distilled it to a manageable size by\ngrouping entries with the same artist, according to approxi-\nmate string-matching of artist ﬁelds. After trimming artists\nwith fewer than 10 disc entries, 20 470 artists remained. Af-\nter trimming artists with fewer than 50 disc entries, 2 388\nartists remained. We chose our dataset artists randomly from\namong this set of the most popular 2 388. We selected 500\nartists at random from the top 2 388 as the subjects of our\nexperiment. There are 639 genres in FreeDB that occur in at\nleast 10 album labels. The most popular of these are Rock,\nClassical, and Pop, while the less-frequently used genre la-\nbels explore a wide space of possibilities (eg. weihnacht-\nslieder, hungarian folk, viking metal). Among albums of\nthe 500 artists that we chose, 408 genres were mentioned.\n5Id3v2: http://www.id3.org/id3v2.3.0.html\n6Because FreeDB is dynamic, uncurated and prone to typographical\nerrors, it is difﬁcult to offer a deﬁnitive count.\n1e-040.0010.010.11Freq\nFreeDB Genre\nFigure 1. FreeDB genre histogram for 2Pac (log-scaled y-axis).\nThe histogram is dominated by Rap and HipHop, but many\ngenres are present.\nWe collected data from AllMusic by hand, from their\nweb interface at ( http://allmusic.com ). We recorded\nthe genre associated with each artist, and when multiple\ngenres were associated with an artist, we recorded all of\nthem. Out of the 500 artists we chose from FreeDB, 30 of\nthem were either repeated names (artist strings not identi-\nﬁed by the approximate string match, such as “Mozart” and\n“Mozart, Wolfgang Amadeus”), or else were not found in\nAllMusic, so our effective dataset had 463 examples. All-\nMusic deﬁnes 32 popular and classical genres. The most\npopular genre by far is Rock, which includes about half\nof the popular artists from FreeDB. In contrast to FreeDB,\nClassical is not a genre in AllMusic, but instead, classical\nmusic is divided among 13 genres such as Ballet, Choral\nMusic, Keyboard Music, Opera, and Symphony.\n2.2. Experiment\nWe used a simple neural network to build a predictive model\nof AllMusic’s genre, given FreeDB’s genre information. We\ndeﬁned the input vector for a given artist to be the histogram\nof the genres assigned to their albums in FreeDB. Similarly,\nwe deﬁned the target vector for an artist to be the histogram\nof their genres in the AllMusic database. In many cases,\nespecially among pop artists, the target histogram has only\none genre. In contrast, FreeDB typically associates each\nartist with a large number of genres. For example, Figures 1\nand 2 show the histograms associated with rapper 2Pac and\nclassical composer Wolfgang Amadeus Mozart.\nAs a learning problem, we cast this as logistic regres-\nsion. We assumed that the map between the different genre\nhistograms would be relatively simple, so we chose a very\nsimple neural network architecture to learn the map. Our\nneural network had no ’hidden layer’; it was a linear trans-\nform from the 563-dimensional input to the 32-dimensional\noutput, followed by a softmax transform that ensured the\nnetwork’s output was a valid histogram (all numbers posi-\ntive, sum to one). Thus we had 563×32 = 17248 parame-\nters. We set them by gradient-descent of a cost function, us-\ning the Kullback-Leibler divergence of the prediction with1e-040.0010.010.11Freq\nFreeDB Genre\nFigure 2. FreeDB genre histogram for Mozart, W.A. (log-\nscaled y-axis). The distribution takes a peak at Classical, but\nmany genres are present.\nTable 1. Predictive power of FreeDB genre histograms.\nAlgorithm Measure Mean ±95%\nrandom guess KL 3.665 0.0219\nrock guess KL 2.269 0.0374\nlinsoft KL 0.922 0.0539\nrandom guess 0-1 3.00% 0.000742\nrock guess 0-1 47.09% 0.002921\nlinsoft 0-1 74.10% 0.002003\nAllMusic genre distribution as a cost function.\nWe evaluated the performance of our function approx-\nimator by 10-fold cross-validation, giving us 423training\nexamples for each fold. Even our simple network had far\nmore capacity than was required for this task, so we regu-\nlarized learning by starting the gradient descent with small\nweights, and using an early-stopping heuristic (learning is\naborted once 50 iterations fail to improve performance on a\nheld-out validation set).\nFor more information on function approximation by neu-\nral networks, as well as evaluation methods such as cross-\nvalidation, see [6].\n2.3. Results and Discussion\nThe results of our experiment are posted in Table 1 and in\nFigure 3. In the table, three algorithms appear, and two scor-\ning measures. The ﬁrst algorithm random guess outputs a\nrandom histogram without making reference to the data in\nany way. The second algorithm, rock guess , guesses each\noutput label according to the frequency in the training data,\nand represents the best classiﬁer possible that does not use\nthe input data. The third algorithm linsoft is the classiﬁer\ndescribed above, trained by gradient descent. The KLmea-\nsure is the mean KL-divergence between test predictions\nand test targets. This is one metric for measuring the dis-\ntance between two probability distributions. The 0−1mea-\nsure (classiﬁcation accuracy) is the fraction of test examples\nsuch that the index of the maximum predicted value was\nequal to the index of an arbitrarily chosen maximum value\nin the target.It was our impression that the artist names in AllMusic\nare correct, in keeping with our assumption that it is a well-\nmanaged database. However, a number of artists that ap-\npeared to be more popular outside of North America were\nmissing from AllMusic (eg. Eri Esittajia). Furthermore, of\nthe more exotic artists that did appear in AllMusic, many\nentries were relatively incomplete; there was no background\ndata on the artist, often nothing but a list of their albums and\na genre (eg. Die Artze). In contrast, most popular artists\n(eg. Radiohead ) have biographical sketches, a list of styles\nthat are more precise than the genre, and a list of moods to\nwhich their music corresponds.\n2.3.1. Classiﬁcation Rates\nWhile this task is not strictly classiﬁcation, 92%(435 /463)\nof the artists in our dataset have a single genre, so the 0−1\nmeasure is relevant. The expected performance of the ran-\ndom guess algorithm is1\n32= 3% , in agreement with ob-\nservation. The performance of the rock guess is more in-\nteresting, it correctly labelled 47% of the data. This is a\nconsequence of the enormous class imbalance in AllMusic’s\ngenre labels. The linsoft classiﬁer correctly labelled 74% of\ntest examples. This demonstrates a large degree of correla-\ntion between FreeDB and AllMusic. Still, there are a large\nnumber of mistakes, in view of the fact that the input and\noutput are both genre histograms.\nOne shortcoming of the 0−1measure is that it does\nnot take into account how close the model is to getting the\nanswer right. Another way to compare model performance\nis by looking at the relative ranking of correct labels. As\nis seen in Figure 3 the linsoft classiﬁer converges towards\n100% classiﬁcation more quickly than the other two, indi-\ncating that its highly-ranked choices are often the right ones.\nFor example, 89% of correct answers are found in the top 5\nof 32 choices.\n2.3.2. KL Divergence\nTheKL divergence between the target and predicted his-\ntograms provides another perspective on the performance of\nthe algorithms. When there is just one correct genre for an\nartist, the KL divergence is the negative logarithm of the\nprobability mass that the prediction put on the right genre.\nWhen there are multiple correct genres, the KLdivergence\nis the average negative logarithm of the masses on the cor-\nrect genres.\nKL(P||Q) =/summationdisplay\niPilog/parenleftbiggPi\nQi/parenrightbigg\n(1)\nAKL score of zmeans that the model got an average\nfraction of e−zof the density that it was supposed to. In\nthis way we can see that the KL score of random guess\ncorresponds to the fraction 0.025, the score of rock guess\ncorresponds to 0.10, while the score of linsoft corresponds\nto0.40. Recalling that there are 32 genres in the target his-\ntogram, this represents a signiﬁcant degree of learning.0 5 10 15 20 25 30 350102030405060708090100Percent correct\nBest−of−k scorelinsoft\nrock\nrandomFigure 3. Cumulative sums showing the relative contributions\nin rank order. Faster convergence to 100% represents bet-\nter performance in multi-task classiﬁcation because it indicates\nthat correct answers, even when not the ﬁrst choice, are highly\nranked by the classiﬁer.\nIf the scores seem low, remember that in terms of a learn-\ning problem, this would normally be a very difﬁcult prob-\nlem. Our model has over 17 000 parameters, and our dataset\nhas only 463 examples. Only 422 of those are used during\ntraining. Of those 422, 338 are used for ﬁtting the param-\neters, and 84 are held out to guard against over-ﬁtting. At\nthe same time consider that each example is described by\na histogram over 540 genres, and we would like to assign\na histogram over 32 genres. Also consider that half of the\ntarget labels are rock, severely limiting the value of the little\ntraining data that we have.\n3. Conclusions and Future Work\nOur results suggest that a linear model explains much of the\nvariation in AllMusic given FreeDB input. Given the suc-\ncess of our simple model in what would usually be a difﬁ-\ncult machine learning problem, and given that AllMusic is a\ngold-standard for stylistic music classiﬁcation, we conclude\nthat FreeDB is a useful resource for labelling music for the\npurpose of music classiﬁcation.\nThis conclusion has particular relevance for the MIR com-\nmunity. We would like to see FreeDB used as a reference for\nlabelling music for the purpose of comparing algorithms.\nGathering music that is representative of current tastes, and\nlabelling that music in order to test automatic labelling meth-\nods are both difﬁcult time-consuming tasks. FreeDB can\nhelp in both. First, FreeDB can be used as a reference of\nwhat is popular music (there are charts posted on the web-\nsite, and each album entry is dated). Second, FreeDB can be\nused in the manner described here to associate a vector with\nan artist, such that the prediction of that vector is tantamountto classifying the genre of the artist.\nIn this work we have not addressed the question of what\nkind of music similarity is induced by our notions of his-\ntogram similarity, such as the [exponential of the negative]\nKL divergence. Future work will consider whether near-\nness relations correspond to acoustic similarity or stylistic\nsimilarity, or whether they are more determined by factors\noutside the content of the music. Another question we have\nnot addressed is the effect of artist popularity on our la-\nbelling scheme; the histograms of popular artists are more\nclearly deﬁned, but lesser-known artists may be more care-\nfully and accurately labelled by their fans.\nAt the same time, we have not demonstrated that his-\ntograms related to FreeDB genres are any easier to predict\nfrom symbolic or recorded audio than are the canonical gen-\nres offered by AllMusic. Indeed, since there are more FreeDB\ngenres, they should presumably be harder to predict. The\nreason that FreeDB genres are attractive compared to canon-\nical genres for machine learning methods, is that that ma-\nchine learning methods work best when there is an enor-\nmous amount of training data. It is difﬁcult to obtain large\namounts of training data for canonical genres, while FreeDB\ncan be used to automatically label any large collection of\nmusic. Future work will explore machine learning methods\nfor predicting FreeDB genres from symbolic and recorded\naudio using training databases of hundreds of thousands, or\nmillions of songs.\n4. Acknowledgements\nThe authors would like to thank NSERC and FQRNT for\ntheir generous support, as well as the reviewers for their\nhelpful comments.\nReferences\n[1] F. Pachet and D. Cazaly, “A taxonomy of musical gen-\nres,” in Proc. Content-Based Multimedia Information Access\n(RIAO) , 2000.\n[2] C. McKay and I. Fujinaga, “Automatic music classiﬁcation\nand the importance of instrument identiﬁcation,” in Pro-\nceedings of the Conference on Interdisciplinary Musicology\n(CIM05), Montreal, Canada , 2005.\n[3] J. S. Downie, K. West, A. Ehmann, and E. Vincent, “The\n2005 music information retrieval evaluation exchange (mirex\n2005): Preliminary overview,” in Proceedings of the Sixth In-\nternational Conference on Music Information Retrieval: IS-\nMIR 2005 (J. D. Reiss and G. A. Wiggins, eds.), pp. 320–\n323, Sept 2005.\n[4] D. Perrott and R. Gjerdingen, “Scanning the dial,” in Soci-\nety for Music Perception and Cognition Conference (SMPC),\nEvanston, IL , 2005.\n[5] J. Aucouturier and F. Pachet, “Representing musical genre:\nA state of the art,” Journal of New Music Research , vol. 32,\nno. 1, pp. 1–12, 2003.\n[6] C. M. Bishop, Neural networks for pattern recognition . Ox-\nford, UK: Oxford University Press, 1996."
    },
    {
        "title": "Muugle: A Modular Music Information Retrieval Framework.",
        "author": [
            "Martijn Bosma",
            "Remco C. Veltkamp",
            "Frans Wiering"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1415916",
        "url": "https://doi.org/10.5281/zenodo.1415916",
        "ee": "https://zenodo.org/records/1415916/files/BosmaVW06.pdf",
        "abstract": "Muugle (Musical Utrecht University Global Lookup En- gine) is a modular framework that allows the comparison of different MIR techniques and usability studies. A system overview and a discussion of a pilot usability experiment are given. A demo version of the framework can be found on http://give-lab.cs.uu.nl/muugle. Keywords: Music Information Retrieval, Framework",
        "zenodo_id": 1415916,
        "dblp_key": "conf/ismir/BosmaVW06",
        "keywords": [
            "Music Information Retrieval",
            "Framework",
            "Comparison",
            "Modular",
            "Comparison",
            "Usability",
            "Studies",
            "Pilot",
            "Experiment",
            "Demo"
        ],
        "content": "Muugle: A Modular Music Information Retrieval Framework\nMartijn Bosma, Remco C. Veltkamp, Frans Wiering\nUtrecht University\nPadualaan 14, De Uithof\n3584 CH Utrecht, The Netherlands\n{mbosma, remco.veltkamp, frans.wiering }@cs.uu.nl\nAbstract\nMuugle (Musical Utrecht University Global Lookup En-\ngine) is a modular framework that allows the comparison of\ndifferent MIR techniques and usability studies. A system\noverview and a discussion of a pilot usability experiment\nare given. A demo version of the framework can be found\nonhttp://give-lab.cs.uu.nl/muugle .\nKeywords: Music Information Retrieval, Framework\n1. Introduction\nRecently, many MIR systems have been developed with many\ndifferences between them [7]. For example, the representa-\ntion of the music data on which they operate varies and the\ncollections on which they are tested are different. Not all\nsystems handle polyphonic data and some ignore note dura-\ntions. They thus operate under different circumstances. To\nmake a methodical comparison of their performance pos-\nsible, the underlying techniques of these systems need to\noperate in the same framework.\nFrameworks such as RUBATO [5] and M2K [3] are com-\nponent based, which in principle makes them suitable for the\ncomparison of different techniques. However, RUBATO is\ndesigned for the generation of performance data from score\nand not for MIR purposes. Therefore it contains no match-\ning components. M2K is mainly a development environ-\nments for the rapid prototyping of MIR systems. In [4] an\nexperimentation framework for comparing similarity algo-\nrithms is described. However, this system and M2K are not\ndesigned as frameworks for usability experiments.\nTo our knowledge, there is still no framework that allows\nthe comparison of different feature extraction and similarity\nmethods. Our aim with Muugle (Musical Utrecht University\nGlobal Lookup Engine) is to provide such a framework.\n2. Architecture\nA retrieval process starts with the query formulation in the\nuser interface and it leads to the presentation of the most\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies\nare not made or distributed for proﬁt or commercial advantage and that\ncopies bear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2006 University of Victoria\nFigure 1. Overview of the Muugle-system architecture.\nsimilar music present in the database. Figure 1 shows how\nthis process is modelled in the architecture of the Muugle\nframework. The arrows in the diagram represent ﬂows, the\nellipses processes and the boxes data.\nThe data set can be a local music collection on hard-disk,\nor music found online by a web-crawler. As a ﬁrst instan-\ntiation we have ﬁlled a database with 2 music collections,\nnamely 815 ringtones, and 476.621 incipits of the RISM\ncollection [2].\nFeatures are extracted and stored in the database’s tables.\nEach matching component may require a speciﬁc feature set\nto be extracted from the data. We extracted chunks, each\nconsisting of six consecutive notes in the ringtones and ﬁve\nin the RISM collection. Chunks overlap resulting in 28.483\nchunks in the ringtone and 4.664.702 in the RISM collec-\ntion. For each note the onset, pitch and duration are stored.\nComparing the features of the query with those of all\ntunes in the database can be a time-consuming task. There-\nfore an index data structure is constructed. We have indexed\nthe chunks with the vantage indexing method [9]. The dis-\ntances between all database tunes and some vantage objects\nare pre-calculated. The set of tunes that have approximately\nthe same distance to the vantage objects as the query has to\nthese vantage objects, contains also those objects that have\nabout the same distance to the query object. At query time a\nmatching component only has to calculate the possibly com-\nplex distance between the query and the vantage objects, af-\nter which an efﬁcient range search among the pre-computed\ndistances can be done.Figure 2. Muugle interface with piano-roll editor.\nGenerally, queries are in an audio or symbolic format.\nMuugle currently provides four different interfaces designed\nfor query formulation by playing a software keyboard, up-\nloading a MIDI ﬁle, playing an external MIDI device, or\nby Query By Humming. Figure 2 depicts the software key-\nboard interface which can be played with the mouse. The\nquery can be modiﬁed (or generated from scratch) by means\nof a piano-roll editor.\nAppropriate features must be extracted from the query\nfor matching. The user-selected matching component uses\nthe features of the query and those of the tunes in the database\nto arrive at a similarity judgment. Currently, four matching\ncomponents are implemented in Muugle, which all operate\non the note level. Three of these components use trans-\nportation distances, which are the Earth Mover’s Distance\n(EMD), the Proportional Transportation Distance (PTD) [6]\nand the Combined EMD-PTD . The combined EMD-PTD\ncomponent calculates the EMD and the PTD distance and\nreturns the minimum of the two. The fourth component uses\ntheMaximum Overlap which is one of the geometrical algo-\nrithms developed for the C-Brahms project [8].\nThe output of the matching component is a list of tune\nids, ordered according to the tunes’ similarity. The fetching\nmodule receives this list and retrieves the data of the corre-\nsponding tunes. Finally, an ordered list of tune references,\nwith for every tune a notation of its ﬁrst few bars is presented\nto the user.\nThe programming languages used for this instantiation\nof Muugle are MySQL (database), Java (user interfaces), C\nand C++ (matching and indexing), Perl (feature extraction)\nand PHP (fetching and result presentation).3. Discussion and Future Work\nMuugle is a framework that integrates the study of several\naspects of MIR. Its modular architecture is designed for test-\ning and comparing input methods, result presentations and\nMIR algorithms. Currently the focus is on symbolic repre-\nsentations of music, but it is possible to incorporate audio\ncomponents.\nWe performed a pilot experiment to compare the differ-\nent input methods [1]. The subjects had to formulate a query\nthat was similar to a melody they just had heard. For each\nquery they used one of the input methods described above.\nThe results indicated that the Query by Humming method\nwas slightly better, and that subjects with prior knowledge\nof MIDI beneﬁted from the piano-roll editor. A larger ex-\nperiment is needed to give support to these indications.\nWe believe that the performance of a MIR system will\nimprove if it operates on higher-level features that are rel-\nevant to human music perception and cognition. Although\nthese features, such as key and metric structure, are impor-\ntant, extracting them from the raw query does not seem to\nbe feasible. It seems that people ﬁrst of all try to capture the\nbasic contour of the melody they are searching. Therefore\nwe have started investigating the possibility of relevance-\nfeedback by the user. The idea is that feedback is given on\nthe results of a rough contour based search. In a second\nsearch, key and metric structure are used.\nAcknowledgements: This work is partially supported by the\nDutch ICES/KIS III bsik project MultimediaN. We thank Kjell\nLemstr ¨om and the other researchers of the C-Brahms project for\nproviding us their implementation of the Maximum Overlap algo-\nrithm. We thank Robbert Krijger, Bart van Andel, Bart Kuipers\nand Niels Gorisse for implementing Muugle components.\nReferences\n[1] M. Bosma, R.C. Veltkamp, and F. Wiering. Muugle: A mu-\nsic retrieval experimentation framework. In ICMPC , 2006.\n[2] Repertoire International des Sources Musicales RISM. 2002.\nhttp://rism.stub.uni-frankfurt.de.\n[3] J.S. Downie, J. Futrelle, and D. Tcheng. The international\nmusic information retrieval systems evaluation laboratory:\nGovernance, access and security. In ISMIR , 2004.\n[4] J. Garbers. An environment for testing and comparing music\nanalysis algorithms. In ISMIR , 2006.\n[5] G. Mazzola and O. Zahorka. The RUBATO performance\nworkstation on NEXTSTEP. In ICMC , 1994.\n[6] R. Typke, P. Giannopoulos, R. C. Veltkamp, F. Wiering, and\nR. van Oostrum. Using transportation distances for measur-\ning melodic similarity. In ISMIR , 2003.\n[7] R. Typke, F. Wiering, and R.C. Veltkamp. A survey of music\ninformation retrieval systems. In ISMIR , 2005.\n[8] E. Ukkonen, K. Lemstr ¨om, and V . M ¨akinen. Geometric al-\ngorithms for transposition invariant content-based music re-\ntrieval. In ISMIR , 2003.\n[9] J. Vleugels and R. C. Veltkamp. Efﬁcient image retrieval\nthrough vantage objects. Pattern Recognition , 35(1), 2002."
    },
    {
        "title": "Structural boundary perception in popular music.",
        "author": [
            "Michael J. Bruderer",
            "Martin F. McKinney",
            "Armin Kohlrausch"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1418339",
        "url": "https://doi.org/10.5281/zenodo.1418339",
        "ee": "https://zenodo.org/records/1418339/files/BrudererMK06.pdf",
        "abstract": "The automatic extraction of musical structure from audio is an important aspect for many music information retrieval (MIR) systems. The criteria on which structural elements in music are defined in MIR systems is often not clearly stated but typically stem from (music) theoretical or signal-based properties. In many cases, however, perceptual-based crite- ria are the most relevant and systems need to be trained on or modeled after the perception of structural elements in music. Here, we investigate the perception of structural boundaries to Western popular music and examine the musical cues re- sponsible for their perception. We make links to music the- oretical descriptions of structural boundaries and to compu- tational methods for extracting structure. The methods and data presented here are useful for developing and training systems for the automatic extraction of musical structure as it is perceived by listeners. Keywords: Music cognition, music structure, music percep- tion, music segmentation.",
        "zenodo_id": 1418339,
        "dblp_key": "conf/ismir/BrudererMK06",
        "keywords": [
            "automatic extraction",
            "music information retrieval",
            "structural elements",
            "music theoretical properties",
            "perceptual-based criteria",
            "music theoretical descriptions",
            "computational methods",
            "music segmentation",
            "music cognition",
            "music perception"
        ],
        "content": "Structural boundary perception in popular music\nMichael J. Bruderer1, Martin McKinney2, Armin Kohlrausch1,2\n1Technische Universiteit Eindhoven, Postbus 513, 5600 Eindhoven, The Netherlands\n2Philips Research Laboratories, Prof. Holstlaan 4 (WO-02), 5656 AA Eindhoven, The Netherlands\nm.j.bruderer@tm.tue.nl, {armin.kohlrausch, martin.mckinney }@philips.com\nAbstract\nThe automatic extraction of musical structure from audio\nis an important aspect for many music information retrieval\n(MIR) systems. The criteria on which structural elements in\nmusic are deﬁned in MIR systems is often not clearly stated\nbut typically stem from (music) theoretical or signal-based\nproperties. In many cases, however, perceptual-based crite-\nria are the most relevant and systems need to be trained on or\nmodeled after the perception of structural elements in music.\nHere, we investigate the perception of structural boundaries\nto Western popular music and examine the musical cues re-\nsponsible for their perception. We make links to music the-\noretical descriptions of structural boundaries and to compu-\ntational methods for extracting structure. The methods and\ndata presented here are useful for developing and training\nsystems for the automatic extraction of musical structure as\nit is perceived by listeners.\nKeywords: Music cognition, music structure, music percep-\ntion, music segmentation.\n1. Introduction\nAutomatic music structure analysis is an important com-\nponent of music information retrieval. The ability to au-\ntomatically identify or extract various structural elements\nfrom musical audio would be a boon for automatic meta-\ndata generators and would beneﬁt music consumers, retail-\ners and libraries. Several recent studies, employing differ-\nent algorithms, have shown that one can extract elements\nof structure from music audio with modest success. The\nalgorithms rely on signal based features, such as MFCC,\nchroma, linear prediction coefﬁcients or spectral-based con-\ntrasts [1, 2]. These features are used in various ways: as\ninput to neural nets to predict the likelihood of a musical\nboundary [2]; as the basis for a similarity matrix from which\nrepeated patterns are identiﬁed [3, 4]; or as input to algo-\nrithms for “saliency” to identify the most representative ex-\ncerpt of a musical piece [5]. A more recent study employs\na hierarchical approach in an effort to incorporate musical\nknowledge into the system [6]. This system ﬁrst analyzes\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies\nare not made or distributed for proﬁt or commercial advantage and that\ncopies bear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2006 University of Victoriathe rhythm and uses it to extract music-theoretical features\nincluding chord information, singing voice detection, and\nsong structural elements. The structural elements are based\non repetition as in the other studies, where repetition here\nrefers to melody or chords. While all of these algorithms\ncan, to some degree, extract or identify elements of the mu-\nsical structure, it is unclear how well those elements relate\ntoperceived elements of structure or to those deﬁned clearly\nby rules of music theory.\nIn developing algorithms for the extraction of musical\nstructural boundaries, deﬁned as perceptual points in time\nbetween consecutive segments, we can turn to music theo-\nretical studies on structure to learn which cues are impor-\ntant. Lerdahl and Jackendoff [7], in their seminal book on\nstructure in Western classical music, propose, among others,\nthe following cues for segmenting monophonic melodies:\nPauses; longer notes in between short notes; changes in reg-\nister, dynamics, articulation, and length; and repetition (par-\nallelism). Using these cues they propose rules on how the\nexperienced listener segments music. If several cues oc-\ncur together at the same time, they are added and a stronger\nboundary is perceived. One missing element of this model\nis that there is no quantiﬁcation of the salience of the differ-\nent cues. It is also unclear how well their rules relate to the\nperception of structure in music.\nDeli`ege [8] tested some of the rules from Lerdahl and\nJackendoff for their perceptual relevance and in doing so\ngave an order of importance of the different rules. She found\nthat the most important rules were register change, attack-\npoint (a long note in between two short notes), and rest. She\nalso proposed an additional important rule: timbre change.\nA recent study by Frankland and Cohen [9] quantiﬁed\nsome of the rules of Lerdahl and Jackendoff [7] and tested\nthe quantiﬁed rules for their perceptual validity. They found\nthat only two of these rules were used by subjects for seg-\nmenting monophonic melodies: attack-point and rest.\nAnother quantitative model based on music theory uses\nthe change in intervals to segment monophonic melodies [10].\nAn interval is deﬁned as a difference in pitch, in intensity,\nand in the IOI (inter-onset-intervals). Preliminary results on\nfour pieces show reasonable performance but this model still\nneeds further testing.\nAll the models above were mainly conceived and tested\non monophonic Western classical music. A more prevalent\nform of music, however, is Western popular music. Thisstudy extends the perceptual validation of the models for\npopular music.\nFor many applications it is desirable to have a system\nthat can automatically segment music similar to the way in\nwhich humans do. While the models based on musicology\ndescribed above can serve as a starting point, a model for the\nperception of music structure or a set of perceptually-based\nground-truth data is required. We present here a method\nfor investigating the perception of structural boundaries in\nmusic and for assigning perceptual relevance to various seg-\nmenting cues based on musicology. We show results and\nanalysis for six songs from Western popular music. This\nmethod and data can be used as a training ground for sys-\ntems to automatically extract structural boundaries from mu-\nsic.\n2. Method\n2.1. Material\nFrom a pool of twenty songs we chose six to cover a range of\npopular music styles and to have time-distributed boundary\ncues based on musicology. We used MIDI ﬁles correspond-\ning to the audio tracks and the models of Cambouropou-\nlos [10] and Frankland and Cohen [9] to analyze the salience\nand timing of various segmentation cues. We also marked\nthree other cues: the ending of harmonic cycles and the in-\ntroduction and ending of instrumental voices. If the vari-\nous cues all occurred at the same time (high temporal cor-\nrelation), it would be difﬁcult to assign relative perceptual\nsalience to speciﬁc cues. Thus, if a song had low temporal\ncorrelation between these different cues we deemed it suit-\nable as a candidate for the experiment.\nThe six chosen songs were: the song “Heart to hurt”\nby Kousuke Morimoto taken from the RWC database [11],\n“Moondance” by Van Morrison, “Live and let die” by Paul\nMcCartney, “And when I die” by Blood Sweat and Tears,\n“Body and soul” performed by Billy Holiday (vocal) and\n“Body and soul” by Coleman Hawkins (instrumental). The\nsongs had a duration of between 3 and 5 minutes.\n2.2. Procedure\nThe experiment consisted of two parts; ﬁrst subjects listened\nto music and pressed a key whenever they encountered a\nphrase or segment boundary; second, subjects were asked to\nrate the salience of a selected number of boundaries.\nIn the ﬁrst part subjects listened to the song four times,\nthe ﬁrst time to familiarize, and three times to record the key\npresses, without any symbolic representation of the song.\nThe data from the ﬁrst part were analyzed and the bound-\naries where 90% of the subjects tapped within a time-window\nof 2.4-s (empirically calculated as being the optimal win-\ndowsize) were taken for the second part. Additionally, two\nto three medium and weak boundaries per song were se-\nlected close to 80% and 40% agreement respectively, yield-\ning 98 boundaries in total (14-21 per song).In the second part of the experiment subjects rated the\nsalience of the selected boundaries on a scale from 0 to 6.\nSubjects were presented a horizontal line representing the\ntimespan of the song. Vertical lines indicated the boundaries\nand a moving cursor indicated the momentary playing posi-\ntion. Subjects also gave a free description of the cues – what\nin the music contributed to the perception of the boundary.\nIn this part of the experiment, subjects could listen to the\nwhole song as well as parts of it as many times as they liked.\nThe order of the songs was randomized for both parts of the\nexperiment.\n2.3. Subjects\nEighteen subjects (14 male, 4 female) participated in the\nﬁrst part of the experiment. From these, ﬁfteen also partici-\npated in the second part. Musical experience differed widely\namong subjects, with a mean of 6.8 years (SD of 7.0) for the\nﬁrst part and a mean of 6.7 years (SD of 7.3) for the second\npart, and ranging from 0 to 21 years of musical training. The\naverage subject age was 26.5 years, ranging from 21 to 37\nyears.\n3. Results and Analysis\nAll the notated boundaries were collapsed into one vector\nand quantized to one millisecond resolution. In order to esti-\nmate a density function of notated boundaries, the quantized\nvector was convolved with a Gaussian window. The peaks\nin the convolved vector indicated a boundary detected by\nseveral subjects. At each peak all notated boundaries within\na 2.4-s window were summed and this sum was taken to\nrepresent the salience rating of the boundary, here called\nthe number of notated boundaries. A few times subjects\npressed a key more then once within a 2.4-s window and\nthus some boundaries exceeded the theoretical maximum of\n54. Two examples of the distribution of the notated bound-\naries within a 2.4-s window are shown in Figure 1. The\ndistribution for “Live and Let Die” (top panel) shows sev-\neral boundaries indicated by almost all subjects, as well as\nmany boundaries indicated by fewer subjects. The distribu-\ntion for “Body and Soul” shows much less agreement across\nsubjects, with only a single boundary getting more than 40\nindications.\nFigure 2 shows a result from the second part of the exper-\niment: the relation between the salience rating and the num-\nber of indicated boundaries. The ratings of the boundaries\ngiven by the subjects in the second part of the experiment\nwere signiﬁcantly correlated with the number of subjects\nthat pressed a key at the boundary within a 2.4-s window\n(R= 0.88,p <0.001). In a previous study with a similar\nmethod [12] it had been assumed that the number of notated\nboundaries was an indication of the strength of a boundary\nbut it had never been shown. It is possible that the salience\nrating of a boundary for a given subject is not related to the\nnumber of subjects that perceive the boundary at a particular0 50 100 1500102030405060Live And Let Die\nTime in sNumber of notated boundaries\n0 50 100 1500102030405060Body And Soul (instrumental)\nTime in sNumber of notated boundariesFigure 1. Two examples of the distribution of the notated\nboundaries within 2.4-s windows. The ﬁgure above shows a\nhigh consistency over the strongest boundaries, the ﬁgure be-\nlow shows less consistency.\n●●\n●●\n●●\n●●●\n●\n●●\n●●\n●●\n●●●\n●●\n●●\n●●\n●●\n●●\n●●●\n●●\n●●\n●\n●●●\n●\n●●\n●●●\n●●●\n●\n●●●\n●●\n●\n●●●\n●\n●●\n●●\n●●●\n●●\n●\n●●●●\n●\n●●\n●●\n●●\n●\n●●●\n●●\n●●●●●\n●●\n●●●\n●\n010 20 30 40 50 600123456\nNumber of notated boundariesMean salience ratingFigure 2. The correlation between the number of notated\nboundaries and the respective rating of the boundary. The line\nrepresents the linear regression. The correlation between the\ntwo is 0.884 (p <0.001).\npoint in time. Also, some boundaries may be perceptually\nmore diffuse over time than others. However, here we found\na very high correlation between the number of subjects that\nindicate a boundary within 2.4-s window and the respective\nsalience rating.\nIn order to see which cues contributed to the subjects’\nperception of boundaries, we analyzed their descriptions\nof contributing cues. The descriptions were classiﬁed\ninto twelve different classes: level change, change in tim-\nbre (drums, voice, other), tonality (harmonic progression,\nmelody change), rhythm (a change in the strength of the\nrhythm, tempo change, rhythm change), and structural de-\nscriptions (global structure, repetition, break). Using these\nclasses the descriptions of each of the boundaries were then\nclassiﬁed. The terms mentioned most often were ‘global\nstructure’ and ‘change in timbre: other’, both being men-\ntioned more than 300 times, followed by ‘change in level’,\n‘repetition’, and ‘break/pause’, all three mentioned about\n160 times.\nWe then computed the mean-term-rating , i.e., the mean\nsalience value association with a speciﬁc term, to get an in-\ndication of the relative importance of each term type. Using\nthis measure the strongest cues seem to be the change in\nthe strength of the rhythm and change in drums (timbre).\nIt must be mentioned, however, that these cues were only\nmentioned a few times (5 and 32 times), thus it is question-\nable if they are the overall most salient cues. When lookingat the cues that were mentioned more then a hundred times\nthe strongest cue is a change in rhythm.\nThe mean-term-rating is also dependent on the song. For\nthe two songs “Live and Let Die” and “And When I Die”,\nchange in tempo has a high mean-term-rating, but for the\nother songs this cue is less relevant.\nThe cue associated with the maximum mean-term-rating\nfor each song is: change in strength of rhythm (6.0) for\n“Heart to hurt”; progression (5.8) for “Moondance”; repeti-\ntion (5.4) for “Body and soul (vocal)”; repetition (5.10) for\n“And when I die”; drums (6.0) for “Body and Soul (instru-\nmental)”; no clear strong mean-term-rating for “Live and\nLet die”.\n4. Discussion\nThe experiment shows that there are structural boundaries\nin music that are perceived by almost all listeners. A novel\nﬁnding of this study is the strong correlation between the\nnumber of times a boundary was indicated by subjects and\nthe reported salience rating of that boundary. The high\ncorrelation shows that a ground-truth database of boundary\nsalience can be generated in two manners: Either subjects\nsegment a piece of music by pressing a key and then all no-\ntated boundaries within a certain time-window are summed\nup, or a set of boundaries are given to subjects with the task\nof rating the salience of each boundary.\nWhen comparing the different cues with which subjects\ndescribed a boundary, it can be seen that a change in timbre\nwas often mentioned, which is in agreement with the study\nof Deli `ege [8]. Repetition and change in dynamics, rules\nused by Lerdahl and Jackendoff [7], are also often found in\nthe description of the boundaries. Frankland and Cohen [9]\nfound that an important cue is rest, which corresponds to\nour often-mentioned break/pause class. These results extend\nthe ﬁndings of previous studies and show that there are cues\nthat contribute to the perception of musical structure across\nmusic styles.\nAlgorithms that segment music are often binary in na-\nture: a boundary either exists or does not. Our study shows\nthat perceptually, there is a wide range of salience across dif-\nferent boundaries. Thus, the perception of boundaries is not\nbinary. Algorithms for automatic segmentation that intend\nto extract perceptually relevant structural elements should\naccount for this range of salience in structural boundaries.\n5. Summary and Conclusions\nWe have described an experimental method for examining\nthe perception of structural boundaries in music. We have\nshown experimental data on how subjects segment Western\npopular pieces and what cues they used in describing the\nsegment boundaries. These results are important for mu-\nsic information retrieval because there exist few perceptual\nstudies that explore the perception of structural boundaries,\nespecially for popular music. For automatic segmentationalgorithms, however, it is crucial to have a perceptually rel-\nevant ground truth, if the algorithm should segment the same\nway humans do.\nOur analysis shows that for Popular music there are sev-\neral cues that strongly contribute to the perception of struc-\ntural boundaries in music: changes in timbre, changes in\nlevel, repetition, and breaks/pauses. These results, however,\nare based only on the subjects’ description of the bound-\naries and it is possible that there are other contributing fac-\ntors. Our next step is to examine correlations between music\ntheoretical boundaries and strengths (based on analysis of\ncorresponding MIDI data) and the perceptual data collected\nhere.\nReferences\n[1] G. Peeters, A. L. Burthe, and X. Rodet, “Toward automatic\nmusic audio summary generation from signal analysis.” in\nProceedings of the 3rd International Conference on Music\nInformation Retrieval (ISMIR 2002) , 2002.\n[2] N. Hu and R. B. Dannenberg, “A bootstrap method for train-\ning an accurate audio segmenter.” in Proceedings of the 6th\nInternational Conference on Music Information Retrieval\n(ISMIR 2005) , 2005, pp. 223–229.\n[3] R. B. Dannenberg and N. Hu, “Discovering musical structure\nin audio recordings,” in Music and Artiﬁcial Intelligence,\nSecond International Conference, ICMAI 2002 , ser. Lecture\nNotes in Computer Science, vol. 2445. Springer, Sept 2002,\npp. 43–57.\n[4] J. Foote, “Visualizing music and audio using self-similarity,”\ninMULTIMEDIA ’99: Proceedings of the seventh ACM in-\nternational conference on Multimedia (Part 1) . New York,\nNY , USA: ACM Press, 1999, pp. 77–80.\n[5] L. Lu and H.-J. Zhang, “Automated extraction of music snip-\npets,” in MULTIMEDIA ’03: Proceedings of the eleventh\nACM international conference on Multimedia . New York,\nNY , USA: ACM Press, 2003, pp. 140–147.\n[6] N. C. Maddage, “Automatic structure detection for popular\nmusic,” IEEE MultiMedia , vol. 13, no. 1, pp. 65–77, 2006.\n[7] F. Lerdahl and R. Jackendoff, A Generative Theory of Tonal\nMusic . Cambridge, Mass.: MIT Press, 1983.\n[8] I. Deli `ege, “Grouping conditions in listening to music,” Mu-\nsic Perception , vol. 4, no. 4, pp. 325–360, 1987.\n[9] B. W. Frankland and A. J. Cohen, “Parsing of melody: Quan-\ntiﬁcation and testing of the local grouping rules of “Lerdahl\nand Jackendoff’s a Generative Theory of Tonal Music”,”\nMusic Perception , vol. 21, no. 4, pp. 499–543, 2004.\n[10] E. Cambouropoulos, “Towards a general computational the-\nory of musical structure,” Ph.D. dissertation, University of\nEdinburgh, faculty of music and department of artiﬁcial in-\ntelligence, 1998.\n[11] M. Goto, H. Hashiguchi, T. Nishimura, and R. Oka,\n“RWC music database: Popular, classical, and jazz music\ndatabases,” in Proceedings of the 3rd International Confer-\nence on Music Information Retrieval (ISMIR 2002) , October\n2002, pp. 287–288.\n[12] E. F. Clarke and C. L. Krumhansl, “Perceiving musical time,”\nMusic Perception , vol. 7, no. 3, pp. 213–252, 1990."
    },
    {
        "title": "Prospects for Improving OMR with Multiple Recognizers.",
        "author": [
            "Donald Byrd",
            "Megan Schindele"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1418307",
        "url": "https://doi.org/10.5281/zenodo.1418307",
        "ee": "https://zenodo.org/records/1418307/files/ByrdS06.pdf",
        "abstract": "OMR (Optical Music Recognition) programs have been available for years, but they still leave much to be desired in terms of accuracy. We studied the feasibility of achieving substantially better accuracy by using the output of several programs to “triangulate” and get better results than any of the individual programs; this multiple- recognizer approach has had some success with other media but, to our knowledge, has never been tried for music. A major obstacle is that the complexity of music notation is such that evaluating OMR accuracy is difficult for any but the simplest music. Nonetheless, existing programs have serious enough limitations that the multiple- recognizer approach is promising. Keywords: Optical Music Recognition, OMR, classifier, recognizer",
        "zenodo_id": 1418307,
        "dblp_key": "conf/ismir/ByrdS06",
        "keywords": [
            "Optical Music Recognition",
            "accuracy",
            "multiple recognizer approach",
            "music notation",
            "triangulate",
            "classifier",
            "recognizer",
            "difficulty",
            "existing programs",
            "promising"
        ],
        "content": "Prospects for Improving OMR with Multiple Recognize rs \nDonald Byrd \nSchool of Informatics and School of Music \nIndiana University, Bloomington \ndonbyrd@indiana.edu Megan Schindele \nSchool of Music \nIndiana University, Bloomington \nmhschind@indiana.edu \n  \nAbstract \nOMR (Optical Music Recognition) programs have been \navailable for years, but they still leave much to b e desired \nin terms of accuracy. We studied the feasibility of  \nachieving substantially better accuracy by using th e output \nof several programs to “triangulate” and get better  results \nthan any of the individual programs; this multiple-\nrecognizer approach has had some success with other  \nmedia but, to our knowledge, has never been tried f or \nmusic. A major obstacle is that the complexity of m usic \nnotation is such that evaluating OMR accuracy is di fficult \nfor any but the simplest music. Nonetheless, existi ng \nprograms have serious enough limitations that the m ultiple-\nrecognizer approach is promising. \nKeywords : Optical Music Recognition, OMR, classifier, \nrecognizer \n1.  Recognizers and Multiple Recognizers in \nText and Music \nThis report describes research on an approach to im proved \nOMR (Optical Music Recognition) that, to our knowle dge, \nhas never been tried with music before, though it h as been \nin use for some time in other domains, in particula r OCR \n(Optical Character Recognition).  \nThe basis of an optical symbol-recognition system o f \nany type is a recognizer, an algorithm that takes an image \nthat the system suspects represents one or more sym bols \nand decides which, if any, of the possible symbols to be \nrecognized the image contains. The recognizer works  by \nfirst segmenting  the image into subimages, then applying a \nclassifier, which decides for each subimage on a single \nsymbol or none. The fundamental idea of a multiple-\nrecognizer  system is to take advantage of several pre-\nexisting but imperfect systems by comparing their r esults to \n“triangulate” and get substantially higher accuracy  than any \nof the individual systems. This has been done for O CR by \nPrime Recognition. Its creators reported a very sub stantial \nincrease in accuracy (Prime Recognition, 2005); the y gave \nno supporting evidence, but the idea of improving a ccuracy this way is certainly plausible. The goal of multip le-\nrecognizer OMR (henceforth “MR” OMR) is to do the \nsame with music, and the basic question for such a system \nis how to merge the results of the constituent sing le-\nrecognizer (henceforth “SR” OMR) systems, i.e., how  to \nresolve disagreements among them in the way that \nincreases accuracy the most. \nThe simplest merging algorithm for a MR system is t o \ntake a “vote” on each symbol or sequence of symbols  and \nassume that the one that gets the most votes is cor rect. \n(Under ordinary circumstances—at least with text—a \nunanimous vote is likely on most symbols.) This app ears to \nbe what the Prime Recognition system does, with thr ee to \nsix SR systems voting on a character-by-character b asis. A \nslightly more sophisticated approach is to test in advance \nfor the possibility that the SR systems are of vary ing \naccuracy, and, if so, to weight the votes to reflec t that. \nBut music is so much more complex than text that su ch \nsimple approaches appear doomed to failure. To clar ify the \npoint, consider an extreme example. Imagine that sy stem A \nalways recognizes notehead shapes and flags on note s \ncorrectly; system B always recognizes beams correct ly; and \nsystem C always recognizes augmentation dots correc tly. \nAlso say that each does a poor job of identifying t he \nsymbols the others do well on, and hence a poor job  of \nfinding note durations. Even so, a MROMR system bui lt on \ntop of them and smart enough to know which does wel l on \nwhich symbols would get every duration right! Syste m A \nmight find a certain note—in reality, a dotted-16th  note \nthat is part of a beamed group—to have a solid note head \nwith no flags, beams, or augmentation dots; B, two beams \nconnected (unreasonably) to a half-note head with t wo \ndots; C, an “x” head with two flags and one augment ation \ndot. Taking A’s notehead shape and (absence of) fla gs, B’s \nbeams, and C’s augmentation dots gives the correct \nduration.  See Figure 1. \n \nFigure 1. Permission to make digital or hard copies of all or  part of this work for \npersonal or classroom use is granted without fee pr ovided that copies \nare not made or distributed for profit or commercia l advantage and that \ncopies bear this notice and the full citation on th e first page. \n© 2006 University of Victoria For music, then, it seems clear that one should beg in by \nstudying the pre-existing systems in depth, not jus t \nmeasuring their overall accuracy, and looking for s pecific \nrules describing their relative strengths and weakn esses that \nan MROMR system can exploit. \nIt should also be noted that fewer high-quality SR \nsystems exist for music, so it is important to get as much \ninformation as possible from each. \n1.1  Alignment \nMusic as compared to text presents a difficulty of an \nentirely different sort. With any type of material,  before \nyou can even think of comparing the symbolic output  of \nseveral systems, you must know which symbols output  by \neach system correspond, i.e., you must align  the systems’ \noutput. (Note that we use the verb “to align” in th e \ncomputer scientist’s symbol-matching sense, not the  usual \ngeometric sense.) Aligning two versions of the same  text \nthat differ only in the limited ways to be expected  of OCR \nis straightforward. But with music, even monophonic  \nmusic, the plethora of symbols and of relationships  among \nthem (articulation marks and other symbols “belong”  to \nnotes; slurs, beams, etc., group notes horizontally , and \nchords group them vertically) makes it much harder.  And, \nof course, most music of interest to most potential  users is \nnot monophonic. Relatively little research has been  done to \ndate on aligning music in symbolic form; see Kilian  & \nHoos (2004). \n2.  MROMR Evaluation and Related Work \nObviously, the only way to really demonstrate the v alue of \na MROMR system would be to implement one, test it, and \nobtain results showing its superiority. However, \nimplementing any system at all, on top of conductin g the \nnecessary research to design it, was out of the que stion in \nthe time available for the study described here. Eq ually \nimportant, the evaluation of OMR systems in general  is in a \nprimitive state. Not much progress has been made in  the \nten years since the groundbreaking study by Nick Ca rter \nand others (Selfridge-Field, Carter, et al, 1994). A paper by \nDroettboom & Fujinaga (2004) makes clear the diffic ulty \nof evaluating OMR, pointing out that “a true evalua tion of \nan OMR system requires a high-level analysis, the \nautomation of which is a largely unsolved problem.”  This \nis particularly true for the “black box” commercial  \nSROMR programs, offering no access to their interna l \nworkings, against which we would probably be evalua ting \nthe MROMR. And the manual techniques available with out \nautomation are, as always, costly and error-prone. Two \ninteresting recent attempts to make OMR evaluation more \nsystematic are Bellini et al (2004) and Ng et al (2 004). \nBellini et al propose metrics based on weights assi gned by \nexperts to different types of errors. Ng  et al des cribe \nmethodologies for OMR evaluation in different situa tions. Automation aside, it is not clear whether an evalua tion \nshould consider the number of errors or the amount of \nwork necessary to correct them. The latter is more relevant \nfor many purposes, but it is very dependent on the tools \navailable, e.g., for correcting the pitches of note s resulting \nfrom a wrong clef. Also (and closely related), shou ld \n“secondary errors” clearly resulting from an error earlier in \nthe OMR process be counted or only primary ones? Fo r \nexample, programs sometimes fail to recognize part or all \nof a staff, and as a result miss all the symbols on  that staff. \nFinally, with media like text, it is reasonable to assume that \nall symbols are equally important; with music, that  is not \neven remotely the case. \nUnder the circumstances, all we can do is to descri be the \nbasis for an MROMR system and comment on how \neffective it would likely be. \n2.1  Methodology \n2.1.1  Programs Tested \nWe studied three of the leading commercial programs  \navailable as of spring 2005: PhotoScore 3.10, Smart Score \n3.3 Pro, and SharpEye 2.63. All are distributed mor e-or-\nless as conventional “shrink wrap” programs, effect ively \n“black boxes” as the term is defined above. We also  \nconsidered Gamut/Gamera, one of the leading academi c-\nresearch systems (MacMillan, Droettboom, & Fujinaga , \n2002). But Gamut/Gamera’s great flexibility—it is f ully \nuser-trainable for the “typography” of any desired corpus \nof music—meant that it would have taken too long to  get \nstarted. \n2.1.2  Test Data and Procedures \nWhile the “largely unsolved problem” of “a true \nevaluation” is a serious obstacle for building a wo rking \nMROMR system, determining rules for designing  one is \nrather different from the standard evaluation situa tion. The \nidea here is not to say how well a system performs by some \nkind of absolute measures, but to say in detail how  the \nSROMR systems compare to each other.  \nWhat are the appropriate metrics for such a detaile d \ncomparison? We considered three questions, two of t hem \nalready mentioned under Evaluation. (a) Do we care more \nabout minimizing number of errors, or about minimiz ing \ntime to correct? Also (and closely related), (b) sh ould we \ncount “secondary errors” or only primary ones? Fina lly, (c) \nhow detailed a breakdown of symbols do we want? In \norder to come up with a good multiple-recognizer \nalgorithm, we need a precise description of errors.  \nTherefore the answer to (a) is  minimizing number of \nerrors;  to (b) is,  to the greatest extent possible, we should \nnot count secondary errors.  For item (c), consider the \n“extreme example” of three programs attempting to f ind \nnote durations we discussed before, where all the \ninformation needed to reconstruct the original note s is available, but distributed among the programs. So, we want \nas detailed a breakdown as possible.  \nWe adopted a strategy of using as many approaches a s \npossible to gathering the data. We assembled a test  \ndatabase of about five full pages of “artificial” e xamples, \nincluding the well-known “OMR Quick-Test” (Ng & Jon es, \n2003), and 20 pages of music from published edition s. The \ndatabase has versions of all pages at 300 and 600 d pi, with \neight bits of grayscale; we chose these parameters based on \ninformation from Fujinaga and Riley (2002, plus per sonal \ncommunication from both, July 2005). In most cases,  we \nscanned the pages ourselves; in a few, we used page  image \nfiles produced directly via notation programs or se nt to us. \nWith this database, we planned to compare fully-aut omatic \nand therefore objective (though necessarily very li mited) \nmeasures with semi-objective hand error counts, plu s a \ncompletely subjective “feel” evaluation.  \nWe also considered documentation for the programs a nd \nstatements by expert users. We collected advice fro m \nskilled, regular users of each program as to the op timal \nsettings of their parameters, among other things, i ntending \nto rely on their advice for our experiments. Howeve r, it \ntook much longer than expected to locate experts on  each \nprogram and to get useful advice from them; as a re sult, \nsome of the settings we used probably were not idea l, at \nleast for our repertoire. \n \n2.1.3  Automatic Comparison and Its Challenges \nThe fully-automatic measures were to be implemented  via \napproximate string matching of MusicXML files gener ated \nfrom the OMR programs. With this automation, we \nexpected to be able to test a large amount of music . \nHowever, the fully-automatic part ran into a series  of \nunexpected problems that delayed its implementation . For \nexample, with PhotoScore, comparing the MusicXML \ngenerated to what the program displays in its built -in editor \noften showed serious discrepancies. SharpEye did no t have \nthis problem, but—since it has no “Print” command—w e \nopened its MusicXML files and printed them with Fin ale. \nFinale sometimes misrepresented SharpEye's results,  \ncausing much confusion until we realized was happen ing. \n2.1.4  Hand Error Count \nThe hand count of errors, in three pages of artific ial \nexamples and eight of published music, was relative ly \ncoarse-grained in terms of error types. We distingu ished \nonly seven types of errors, namely: \n• Wrong pitch of note (even if due to extra or missin g \naccidentals) \n• Wrong duration of note (even if due to extra or \nmissing augmentation dots) \n• Misinterpretation (symbols for notes, notes for \nsymbols, misspelled text, slurs beginning/ending on  \nwrong notes, etc.) • Missing note ( not  rest or grace note) \n• Missing symbol other than notes (and accidentals an d \naugmentation dots) \n• Extra symbol (other than accidentals and augmentati on \ndots) \n• Gross misinterpretation (e.g., missing staff) \nFor consistency, we evolved a fairly complex set of  \nguidelines to be used in the process of scanning an d \nexamining the music. All the hand counting was done  by \nthe creators of the guidelines and nearly all by a single \nperson (Schindele), so we are confident our results  have a \nreasonably high degree of consistency. \n2.1.5  “Feel” Evaluation \nWe also did a completely subjective “feel” evaluati on of a \nsubset of the music used in the hand error count, p artly as a \nso-called reality check on the other results, and p artly in \nthe hope that some unexpected insight would arise t hat \nway. The eight subjects were music librarians and \ngraduate-student performers affiliated with a large  \nuniversity music school. We gave them pairs of page s of \nmusic—an original, and a version printed from the o utput \nof each OMR program of a 300-dpi scan—to compare. \nThere were two originals: a monophonic page of Bach , and \na page of Mozart piano music. Thus, with the three OMR \nversions of each original, each subject saw a total  of six \npairs. The Mozart page is the same one used by Sapp  \n(2005); in fact, we used his scans of the page. \nBased on results of the above tests, we also create d and \ntested another page of artificial examples, “Questi onable \nSymbols”, intended to highlight differences between  the \nprograms: we will say more about this later. \n2.1.6  Intellectual Property Rights and Publication Dates \nAn important consideration for any serious music-en coding \nproject, at least for academic purposes, is the int ellectual-\nproperty status of the publications to be encoded. \nObviously the safest approach, short of potentially  \nspending large amounts of time and money to clear t he \nrights, is to use only editions that are clearly in  the public \ndomain. In general, to cover both Europe and the U. S., this \nmeans that the composer must have been dead for mor e \nthan 70 years, and the edition must have been publi shed \nbefore 1923. This restriction is somewhat problemat ic \nbecause music-engraving practice has changed gradua lly \nover the years, and perhaps less gradually since th e advent \nof computer-set music, and the commercial OMR progr ams \nare undoubtedly optimized for relatively-recent edi tions. \nFor this study, we used a mixture of editions from the \npublic-domain and non-public-domain periods, plus s ome \nvery recent computer-set examples. \n3.  Results \nAs a result of the difficulties with the fully-auto matic \nsystem, we ended up relying almost entirely on the hand error counts, plus expert opinions and documentatio n. The \nfollowing results attempt to reveal the strengths a nd \nweaknesses of each program, with an eye toward \nintegrating that data into a future MROMR system. \n3.1  Hand Error Count Results \nResults of the hand error counts may be seen in Fig ures 2 \nthrough 5. The tables and graphs demonstrate some o f the \nmain points: \n• With our test pages, the higher resolution did not \nreally help. This was as expected, given that none of \nthe test pages had very small staves. In fact, it h ad \nlittle effect on SharpEye and SmartScore. PhotoScor e \nwas much less predictable: sometimes it did better at \nthe higher resolution, sometimes worse. \n• The programs generally had more trouble with more \ncomplex and more crowded pages. This was also as \nexpected. \n• Despite its generally good accuracy and its high ra ting \nin the “feel” evaluation (see below), SharpEye made  \nmany errors on note durations. Most of these are \nbecause of its problems recognizing beams. \n• By the guidelines of Byrd (2004), which seem \nreasonable for high-quality encodings, all three \nprograms were well above acceptable limits in terms  \nof note pitch and duration for multiple pages. For \nexample, SharpEye had error rates for note duration  \nover 1% on several of our test pages, and over 4% o n \none (see Figure 5). Byrd’s guidelines say 0.5% is t he \nhighest acceptable rate. For note pitch, SharpEye h ad \nan error rate over 1% for one test page and nearly 1% \nfor another (Figure 4), while Byrd’s limit for pitc h \nerror is only 0.2%. \nWe also did a more detailed count of errors in a fe w \nspecific symbols other than notes: C clefs, text, h airpin \ndynamics, pedal-down marks, and 1 st  and 2 nd  endings. \nThese are included in the Total Errors graphs, figu res 2 and \n3. \n \n050 100 150 \nTest Page No. and Errors by ProgramNo. of Errors \nPhotoScore 31 22 27 22 25 68 40 93 24 36 81 \nSharpEye 4 2 17 7 8 55 22 55 17 42 25 \nSmartScore 9517 14 20 48 13 76 731 114 TP \n1TP \n2TP \n3TP \n7TP \n8TP \n12 TP \n13 TP \n15 TP \n16 TP \n19 TP \n21 \n \nFigure 2. Total Errors at 300 dpi 020 40 60 80 100 120 140 \nTest Page No. and Errors by ProgramNo. of Errors \nPhotoScore 22 5 19 29 63 64 28 123 27 42 106 \nSharpEye 4 2 18 9 9 45 13 66 14 40 29 \nSmartScore 9418 39 30 53 19 96 13 30 119 TP \n1TP \n2TP \n3TP \n7TP \n8TP \n12 TP \n13 TP \n15 TP \n16 TP \n19 TP \n21 \n \n Figure 3. Total Errors at 600 dpi \n0.0 5.0 10.0 15.0 20.0 25.0 \nTest Page No. and Percent Wrong Pitch by ProgramPercent Wrong \nPhotoScore 20.6 14.4 5.6 0.6 0.3 0.6 0.4 3.3 0.0 0.6 1.2 4.35 \nSharpEye 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.9 1.2 0.3 0.5 0.26 \nSmartScore 1.5 0.0 0.0 0.0 0.0 0.0 0.0 0.2 0.0 0.3 1.2 0.30 TP 1 TP 2 TP 3 TP 7 TP 8 TP \n12 TP \n13 TP \n15 TP \n16 TP \n19 TP \n21 Mean \n \n Figure 4. Note Pitch Errors at 300 dpi  \n0.0 1.0 2.0 3.0 4.0 5.0 \nTest Page No. and Percent Wrong Duration by ProgramPercent Wrong \nPhotoScore 0.0 0.0 0.0 1.3 1.6 3.4 3.1 1.6 0.0 2.5 0.5 1.27 \nSharpEye 0.0 0.0 0.0 1.3 0.3 4.0 2.7 3.8 0.0 3.8 1.0 1.53 \nSmartScore 0.0 0.0 0.0 1.6 0.3 0.3 0.0 2.4 0.0 1.6 2.0 0.74 TP 1 TP 2 TP 3 TP 7 TP 8 TP \n12 TP \n13 TP \n15 TP \n16 TP \n19 TP \n21 Mean \n \n Figure 5. Note Duration Errors at 300 dpi \n3.2  “Feel” Evaluation Results \nAs a “reality check”, this confirmed the hand error  count \nresults showing that all of the programs did worse on more \ncomplex music, and, in general, that SharpEye was t he \nmost accurate of the three programs. One surprise w as that \nthe subjects considered SmartScore the least accura te, but \nnot by much: its rating and PhotoScore’s were very close. \nThe gap from SharpEye to PhotoScore was not great e ither, \nbut it was considerably larger. \nThe rating scale of 1 to 7 was presented to the sub jects \nas follows: 1 = very poor, 2 = poor, 3 = fair, 4 = fairly \ngood, 5 = good, 6 = very good, and 7 = excellent. For OMR accuracy on this scale, SharpEye averaged 3 .41 \n(halfway from “fair” to “fairly good”) across both pages; \nSmartScore, 2.88 (a bit below “fair”), and PhotoSco re, \n3.03 (barely above “fair”); see Table 2. Several of  the \nsubjects offered interesting comments, though no gr eat \ninsights.  \nTable 2. Subjective OMR Accuracy \n Test Page \n8 (Bach) Test Page \n21 (Mozart) Average \nPhotoScore 3.94 2.13 3.03 \nSharpEye 4.44 2.38 3.41 \nSmartScore 3.88 1.88 2.88 \n3.3  Integrating Information from All Sources \nWe created and tested another page of artificial ex amples, \n“Questionable Symbols”, intended to demonstrate \nsignificant differences among the programs. This br ief \nexample of musical notation, including fingering ma rkings, \ndifferently shaped noteheads, tremolo markings, and  \nseveral other common music notations, was created f rom \nour testing and other information sources on the pr ograms’ \ncapabilities: their manuals, statements on the boxe s they \ncame in, and comments by experts. Of course claims in the \nmanuals and especially on the boxes should be taken  with \nseveral grains of salt, and we did that. While this  data is not \nincluded in the figures in 3.1, it did provide some  insight \nabout various quirks of the three programs. \n3.4  Rules for MROMR \nBased on our research, we developed a set of 17 pos sible \nrules for an MROMR system. Here are four, stripped of \njustification: \n1. In general, SharpEye is the most accurate. \n2. PhotoScore often misses C clefs, at least in one -staff \nsystems. \n3. For text (excluding lyrics and standard dynamic- level \nabbreviations like “pp”, “mf”, etc.), PhotoScore is  the most \naccurate. \n4. SharpEye is the worst at recognizing beams, i.e. , the \nmost likely to miss them completely. \nThese rules are, of course, far too vague to be use d as \nthey stand, and they apply to only a very limited s ubset of \nsituations encountered in music notation; 50 or eve n 100 \nrules would be a more reasonable number. Both the \nvagueness and the limited situations covered are di rect \nresults of the fact that we inferred the rules by i nspection of \njust a few pages of music and OMR versions of those  \npages, and from manually-generated statistics for t he OMR \nversions. Finding a sufficient number of truly usef ul rules \nis not likely to happen without examining a much la rger \namount of music—say, ten times the 15 or so pages i n our \ncollection that we gave more than a passing glance,  if not more.  This would be a very arduous task to perform  \nmanually. \n4.  Conclusions: Prospects for Building a \nUseful System \nIt seems clear that by far the best way of examinin g \nenough music to infer an adequate set of rules woul d be \nwith automatic processes. This is especially true b ecause \nbuilding MROMR on top of independent SROMR systems \ninvolves a moving target. As the rules above sugges t, \nSharpEye was more accurate than the other programs on \nmost of the features we tested, so, with these syst ems, the \nquestion reduces to one of whether it is practical to \nimprove on SharpEye’s results by much. One way in i t \nclearly is  practical is for note durations. A more precise \nstatement of our rule 4 (above) is “when SharpEye f inds no \nbeams but the other programs agree on a nonzero num ber \nof beams, use the other programs”. This rule would cut \nSharpEye’s note-duration errors by a factor of 3, i .e., to a \nmean of 0.51%, improving its performance in this re spect \nfrom the worst of the three programs to by far the best. \nHowever, since the conclusion of our study, major \nupgrades to both PhotoScore and SmartScore have bee n \nreleased. Other upgrades are very likely forthcomin g, and, \nof course, a new program might do well enough to be  \nuseful. In any case, the moving-target aspect shoul d be \nconsidered in the design process. In particular, de voting \nmore effort to almost any aspect of evaluation is p robably \nworthwhile, as is a convenient knowledge representa tion, \ni.e., way of expressing whatever rules one wants to  \nimplement.  \nOne factor we did not consider seriously enough at the \nbeginning of this study is the fact that all of the  commercial \nsystems have built in “split-screen” editors to let  users \ncorrect the programs’ inevitable mistakes: these ed itors \nshow the OMR results aligned with the original scan ned-in \nimage. As general-purpose music-notation editors, i t is \ndoubtful they can compete with programs like Sibeli us and \nFinale, but for this purpose, being able to make co rrections \nin a view that makes it easy to compare the origina l image \nand the OMR results is a huge advantage. The ideal way \nmight be to write a new split-screen editor; that w ould be a \nmajor undertaking, though it could be extremely use ful for \nother purposes, e.g., an interactive music version-\ncomparing program. Otherwise, it could probably be done \nby feeding “enhanced” output in some OMR program’s \ninternal format back into it. Documentation on Shar pEye’s \nformat is available, so the problem should not be t oo \ndifficult. \n4.1  Chances for Significant Gains and a Different \nKind of MROMR \nLeaving aside the vagueness and limitations of our rules, \nand the question of alignment before they could be applied, more work will be needed to make it clear how much rules \nlike these could actually help. But it is clear tha t help is  \nneeded, even with notes: as we have said, in our te sts, all of \nthe programs repeatedly went well beyond reasonable  \nlimits for error rates for note pitch and note dura tion. \nThis suggests using a very different MROMR method, \ndespite the fact that it could help only with note pitches and \ndurations: we refer to using MIDI files as a source  of \ninformation. This is particularly appealing because  it \nshould be relatively easy to implement a system tha t uses a \nMIDI file to correct output from an OMR program, an d \nbecause there are already collections like the Clas sical \nArchives ( www.classicalarchives.com ), with its tens of \nthousands of files potentially available at very lo w cost. \nHowever, the word “potentially” is a big one; these  files \nare available for “personal use” for almost nothing , but the \nprice for our application might be very different. Besides, \nmany MIDI files are really records of performances,  not \nscores, so the number that would be useful for this  purpose \nis smaller—probably much smaller—than might appear.  \n4.2  Advancing the State of the Art \nThe results described above are encouraging enough that \nwe plan to continue working along the same lines to wards \nan implementation of a MROMR system in the near fut ure. \nTo help advance the state of the art of OMR, we als o plan \nto make our test collection (the files of scanning images, \ngroundtruth files, and OMR-program output files) an d \nfindings available to the community in an informal way. \nFor those interested in learning more about our wor k, an \nextended version of this report is available, toget her with \nseveral supporting documents (exact results of the hand \nerror counts, “Questionable Symbols”, the full list  of \npossible rules, etc.) on the World Wide Web at \nhttp://www.informatics.indiana.edu/donbyrd/MROMRPap . \n5.  Acknowledgements \nIt is a pleasure to acknowledge the assistance of M ichael \nDroettboom and Ichiro Fujinaga (information on OMR \ntechnology, especially Gamut/Gamera); Bill Clemmons , \nKelly Demoline, Andy Glick, and Craig Sapp (advice as \nexperienced users of the programs we tested); Graha m \nJones (SharpEye); Don Anthony, Andy Glick, Craig Sa pp, \nand Eleanor Selfridge-Field (obtaining test data); Tim \nCrawford and Geraint Wiggins (project planning, amo ng \nother things); Anastasiya Chagrova (the automatic-\ncomparison program, etc.); and, Don Anthony, Mary \nWallace Davidson, Phil Ponella, Jenn Riley, and Rya n \nScherle (advice and/or support of various kinds). \nThis project was made possible by the support of th e \nAndrew W. Mellon Foundation; we are particularly grateful to Suzanne Lodato and Don Waters of the \nFoundation for their interest in our work. \n \nReferences \n \n[1]  Bellini, P., Bruno, I, and Nesi, P. (2004) Assessin g Optical \nMusic Recognition Tools. Available at \nhttp://www.interactiveMUSICNETWORK.org/wg_imaging \n/upload/assessingopticalmusicrecognition_v1.0.doc \n[2]  Byrd, Donald (2004). Variations2 Guidelines For Enc oded \nScore Quality. Available at \nhttp://variations2.indiana.edu/system_design.html \n[3]  Byrd, Donald, and Schindele, Megan (2006). MROMR \n(Multiple-Recognizer Optical Music Recognition) Web  \npage. http://mypage.iu.edu/~donbyrd/MROMRPaper \n[4]  Droettboom, Michael, & Fujinaga, Ichiro (2004). Mic ro-\nlevel groundtruthing environment for OMR. In \nProceedings of the 5th International Conference on Music \nInformation Retrieval (ISMIR 2004),  Barcelona, Spain, pp. \n497–500. \n[5]  Fujinaga, Ichiro, & Riley, Jenn (2002). Digital Ima ge \nCapture of Musical Scores. In Proceedings of the 3rd \nInternational Symposium on Music Information Retrie val \n(ISMIR 2002), pp. 261–262. \n[6]  Kilian, Jürgen, & Hoos, Holger (2004). MusicBLAST—\nGapped Sequence Alignment for MIR. In Proceedings of \nthe 5th International Conference on Music Informati on \nRetrieval (ISMIR 2004), pp. 38–41. \n[7]  MacMillan, Karl, Droettboom, Michael, & Fujinaga, I chiro \n(2002). Gamera: Optical music recognition in a new shell. \nIn Proceedings of the International Computer Music \nConference,  pp. 482–485. \n[8]  Ng, Kia C., & Jones, A. (2003). A Quick-Test for Op tical \nMusic Recognition Systems. 2nd MUSICNETWORK \nOpen Workshop, Workshop on Optical Music Recognitio n \nSystem, Leeds, September 2003. \n[9]  Ng, Kia, et al. (2004) CIMS: Coding Images of Music  \nSheets. Interactive MusicNetwork working paper. \nAvailable at \nwww.interactivemusicnetwork.org/documenti/view_docu m \nent.php?file_id=1194 \n[10]  Prime Recognition (2005). \nhttp://www.primerecognition.com \n[11]  Sapp, Craig (2005). SharpEye Example: Mozart Piano \nSonata No. 13 in B-flat major, K 333. \nhttp://craig.sapp.org/omr/sharpeye/mozson13/ \n[12]  Selfridge-Field, Eleanor, Carter, Nicholas, et al. (1994). \nOptical Recognition: A Survey of Current Work; An  \nInteractive System; Recognition Problems; The Issue  of \nPracticality. In Hewlett, W., & Selfridge-Field, E.  (Eds.), \n(Computing in Musicology,  vol. 9, pp. 107–166. Menlo \nPark, California: Center for Computer-Assisted Rese arch in \nthe Humanities (CCARH)."
    },
    {
        "title": "Assessing the Performance of Melodic Similarity Algorithms Using Human Judgments of Similarity.",
        "author": [
            "Margaret Cahill",
            "Donncha O&apos;Maidín"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1417997",
        "url": "https://doi.org/10.5281/zenodo.1417997",
        "ee": "https://zenodo.org/records/1417997/files/CahillO06.pdf",
        "abstract": "This paper outlines a project to identify reliable algorithms for measuring melodic similarity by using melodies extracted from a piece of music in Theme and Variations form, for which human judgements of similarity have been gathered. Keywords: melodic similarity, human similarity judgments, scores",
        "zenodo_id": 1417997,
        "dblp_key": "conf/ismir/CahillO06",
        "keywords": [
            "melodic similarity",
            "human similarity judgments",
            "scores",
            "Theme and Variations form",
            "algorithms",
            "measuring",
            "project",
            "music",
            "extracted melodies",
            "humans"
        ],
        "content": "Assessing the Performance of Melodic Similarity Algo rithms Using Human \nJudgments of Similarity \nMargaret Cahill Donncha Ó Maidín \nCentre for Computational Musicology and Computer Mu sic \nDepartment of Computer Science and Information Syst ems \nUniversity of Limerick \nIreland \nmargaret.cahill, donncha.omaidin@ul.ie \nAbstract \nThis paper outlines a project to identify reliable algorithms \nfor measuring melodic similarity by using melodies \nextracted from a piece of music in Theme and Variat ions \nform, for which human judgements of similarity have  been \ngathered.  \nKeywords : melodic similarity, human similarity \njudgments, scores \n1. Background Work and Context \n This research is concerned with identifying reliab le \nalgorithms for measuring melodic similarity in scor es. \nOften, pitch alone or pitch with duration, are the only \nmusical features used to measure similarity. A wide  range \nof further musical information is available from a score. \nThe approach taken here is to study music perceptio n \nresearch, especially melody identification and melo dic \nmemory research for indications of which musical fe atures \nof a score would be most useful for measuring simil arity. \nAlong with the question of which musical features t o use in \nan algorithm, there is the issue of how to mathemat ically \nand proportionally combine these features and what values \nto assign to internal algorithm parameters and weig htings.  \nThe aim of this research is to use a test-bed of mu sical \nmaterial for which we have gathered reliable human \njudgments of similarity to evaluate the performance  of \nalgorithms, use of particular features, and for imp roving \nand tweaking internal weights and parameter values.  This \npaper gives an overview of a listening experiment, an \ninitial algorithm with variations, and some discuss ion of \nissues in comparing the human and algorithmic measu res \nof similarity. \n2. Overview of the Experiment and Results \nA listening experiment was carried out to gather hu man \njudgments of similarity for the algorithmic test-be d. Real melodies from a piece of monophonic music in Theme and \nVariations style were chosen as the Variations demo nstrate \nvarying degrees of similarity to the Theme. The pie ce of \nmusic chosen was a set of nine variations on  “Twin kle, \nTwinkle, Little Star” for recorder [1]. The piece c ontains \namong other things, rhythmic and elaborate melodic \nvariations, different time and key signatures, augm entation \nand diminution in the time domain, triplets and oct ave \nreplacements of notes. The Theme and Variations eas ily \nsegments into distinct four bar sections (ABA) and two \nseparate melodies for the experiment were created u sing \nthe first and second four bars of the Theme and eac h \nVariation. It is important for the success of this research \nthat the human judgments of similarity are as accur ate as \npossible for comparison with the algorithm results.  The use \nof similar melodies helps the user make a judgment on the \nrelative similarity of each. The fact that the melo dies are \nshort and well-known means the subjects do not have  \nproblems remembering the main melody and can \nconcentrate on assessing the similarity of the Vari ation \nmelody. The short melodies, comprehensive introduct ion \nand demonstration phase that makes use of a similar  \nmelody with variations, and the short listening tim e (c. 15 \nmins) also contribute to the collection of accurate  similarity \njudgments. The experiment involved playing subjects  pairs \nof the Theme and a Variation melody, and asking the m to \ngive a rating on a scale of 1 to 7 indicating the d egree of \nsimilarity between the two melodies. Each melody \nextracted from the piece generated 9 pairs of melod ies for \ncomparison. Each pair was repeated in random order so \nthat subject consistency could be checked.  \nThe correlation coefficient (Spearman’s non-paramet ric) \nwas calculated for each individual subject using th eir \nratings for the first and repeated playing of each pair of \nmelodies. Only the data of the most highly consiste nt \nsubjects was kept (those showing significance at th e .01 \nlevel). There was quite a high level of agreement b etween \nsubjects with a mean and median inter-subject corre lation \nof .78 and .84 respectively for the first melody an d .71 and \n.75 for the second. The ratings are pooled for futu re use, by \nusing the median ratings as a measure of central te ndency.  Permission to make digital or hard copies of all or  part of this work for \npersonal or classroom use is granted without fee pr ovided that copies \nare not made or distributed for profit or commercia l advantage and that \ncopies bear this notice and the full citation on th e first page. \n© 2006 University of Victoria 3. The Algorithms \nThe first algorithm used to compare to these human \nsimilarity judgments is listed as Equation 1. [2]  \n \n            \n(1) \n \n \nk = the time windows of the score \np1, p 2 = pitch values of the first and second melodies in  \na window \nwk = the weight associated with that time window \ntotaldur = the duration processed \n \nThis algorithm processes the score in time windows.  A \ntime window unit in this case is the duration of th e shortest \nfull note at that particular point in the score. Th e basic \nform of the algorithm multiplies a weight value by the pitch \ndifference in each time window.  Various ways of \nincorporating duration (possibly separately to wind ow \nwidth) so that longer notes are weighted as being m ore \nimportant than short notes, using pitch difference values \nbased on tonality as Mongeau and Sankoff did [3], u sing \nmetrical stress weights, so that notes on stronger beats of \nthe bar are weighted more than notes on weak beats,  and \nusing weights according to the melodic accents in a  melody \n[4], are currently being explored. If using metrica l accents, \nthere is the question of how to choose appropriate values \nfor different beats of a bar. Further treatments fo r \ncomparing melodies in different time signatures and  \nidentifying transpositions are also implemented. Th e \nperformance of a number of variations of this algor ithm are \nbeing evaluated, with implementations of other algo rithms \nplanned so that comparison of performance can be ma de.  \n4. Issues in Comparing the Algorithm Output \nto Human Ratings  \nOne of the considerations when comparing the human \nratings to the output of the algorithms is the diff erence in \nscale and range of both. In this case, the human ra tings \nranged from 1 (least similar) to 7 (most similar), while the \nalgorithm ranges from 0 (the same) to some higher v alue \nthat represents a degree of difference. This is a s imilar case \nin many such algorithms, where each difference foun d \nbetween melodies cumulatively contributes to an ove rall \nmeasure of similarity.  \nCorrelation can be a useful metric here, as the dir ection \nand difference in scale do not affect the calculati on. The \ncorrelation between the human ratings and each \nalgorithmic similarity measure for all nine variati ons was \nused as a rough metric to identify the most success ful \nalgorithms. Correlation can only be used to assess the \nperformance of the algorithm across all nine variat ions. It may also be useful to be able to assess the perform ance of \nparticular algorithms for particular variations. Ea ch \nvariation varies the original theme in a different way, and \nan algorithm that performs well with a particular v ariation, \nmay not perform well with others. It would also be useful \nto isolate the results for those variations with di fferent time \nsignatures to the theme to assess the way these hav e been \nhandled. \nIn order to compare the algorithmic and human \njudgments for individual variations, the scales of both need \nto be normalized in some way first.  The human rati ng \nresults can be inverted by subtracting each from th e \nmaximum rating. This results in a range of values f rom 0 \n(most similar) to 6 (least similar), and both sets of results \nuse the same direction of scale. There are a number  of \nways in which the sets of values can be normalized.  \nStandard score, such as z-score, can be used to exp ress the \nvalues in terms of the mean and standard deviation.  These \ncan be problematic, however, when outliers occur in  the \ndata set, as was found in these algorithmic measure s. The \nmin-max normalization was chosen, as this allows bo th sets \nof values to be represented in a chosen range, such  as 1-10, \nor 1-100. The human ratings and algorithm output ca n then \nbe compared for individual variations. A metric suc h as the \nsum of the absolute differences between both for ea ch \nvariation, or the Euclidean distance for the variat ions, can \nthen be calculated and used as a measure of how tha t \nalgorithm performed overall. Although it is importa nt to \nthoroughly examine the closeness of results between  the \nalgorithmic and human judgments, it is also importa nt to be \nwary of over-fitting algorithms to the particular m elodies \nchosen here.  \nFurther work involves detailed analysis of the \nperformance of algorithms, along with the implement ation \nand comparison of further algorithms, and the intro duction \nof further musical test-beds for this early evaluat ion phase.  \nReferences \n[1] M. Duschenes, “Variations on Twinkle, Twinkle, \nLittle Star,” in Method for the Recorder – Tunes and \nExercises . Berandol Music Limited, Ontario, Canada, \n1962. \n[2] D. Ó Maidín. “A Geometrical Algorithm for Melod ic \nDifference,” Melodic Similarity - Concepts, \nProcedures and Applications, Computing in \nMusicology II, MIT Press. 1998. \n[3] M. Mongeau and D. Sankoff. “Comparison of \nMusical Sequences,” Computers and the Humanities , \n24, pp.  161-175, 1998. \n[4] M. Jones. “Dynamic Pattern Structure in Music: \nRecent Theory and Research,” Perception & \nPsychophysics , 41, 6, pp. 621-634, 1987. totaldur wp p\ndifference n\nkk k k∑\n=−\n=12 1 | |"
    },
    {
        "title": "The Sonic Visualiser: A Visualisation Platform for Semantic Descriptors from Musical Signals.",
        "author": [
            "Chris Cannam",
            "Christian Landone",
            "Mark B. Sandler",
            "Juan Pablo Bello"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1416388",
        "url": "https://doi.org/10.5281/zenodo.1416388",
        "ee": "https://zenodo.org/records/1416388/files/CannamLSB06.pdf",
        "abstract": "Sonic Visualiser is the name for an implementation of a system to assist study and comprehension of the contents of audio data, particularly of musical recordings. It is a C++  application  with  a Qt4 GUI that  runs  on Windows,  Mac,  and  Linux.  It  embodies  a  number  of concepts which are intended to improve interaction with audio data and features, most notably with respect to the representation  of  time-synchronous  information.  The architecture of the application allows for easy integration of third  party algorithms for the extraction of low and mid-level features from musical audio data. This paper describes  some  basic  principles  and  functionalities  of Sonic Visualiser. Keywords:  Visualisation,  Musical  Feature,  Semantic Descriptor.",
        "zenodo_id": 1416388,
        "dblp_key": "conf/ismir/CannamLSB06",
        "keywords": [
            "Sonic Visualiser",
            "audio data",
            "musical recordings",
            "C++ application",
            "Qt4 GUI",
            "Windows",
            "Mac",
            "Linux",
            "time-synchronous information",
            "third party algorithms"
        ],
        "content": "The Sonic Visualiser: A Visualisation Platform for Semantic Descriptors from \nMusical Signals\nChris Cannam, Christian Landone, Mark Sandler, Juan Pablo Bello\nCentre for Digital Music, Queen Mary University of London\nMile End Road, London, UK\nchris.cannam@elec.qmul.ac.uk\nAbstract\nSonic Visualiser is the name for an implementation of a \nsystem to assist study and comprehension of the contents \nof audio data, particularly of musical recordings.\nIt is a C++ application with a Qt4 GUI that runs on \nWindows, Mac, and Linux. It embodies a number of \nconcepts which are intended to improve interaction with \naudio data and features, most notably with respect to the \nrepresentation of time-synchronous  information. The \narchitecture of the application allows for easy integration \nof third party algorithms for the extraction of low and \nmid-level features from musical audio data. This paper \ndescribes some basic principles and functionalities of \nSonic Visualiser.\nKeywords: Visualisation, Musical Feature, Semantic \nDescriptor.\n1. Introduction\nThe Sonic Visualiser project originated as an integrated \nframework to aid researchers at the Centre for Digital \nMusic at Queen Mary University of London in the \ndevelopment of algorithms for the extraction of low and \nmid-level features from musical audio signals. \nThe internal requirements called for a cross-platform \napplication that could enable the user to browse and edit \ntime-synchronous  musical features overlaid and aligned to \nwaveform and spectrogram representations of the audio \nsignal under analysis.  A further requirement was to allow \nresearchers to integrate feature extraction algorithms and \nuse the application as a test bed to evaluate their \nperformance.\nAlthough tools for audio analysis and annotation are \nreadily available in the open source space, these \napplications are mostly inherited from the linguistics and \nspeech analysis communities and enhancements have been \nintroduced in order to extend their usability to the music \ninformation retrieval community. A modified version of WaveSurfer [1] has been adopted \nas the annotation client for the MUCOSA environment [2] \nand various automatic feature extraction plugins have \nbeen developed for the same application [3].  \nAlso ad-hoc tools for the visualisation of musical \nfeatures have emerged in recent times; the CLAM \nAnnotator [4], for instance, natively allows the user to \nbrowse and edit automatically extracted descriptors, as \nwell as to manually enter “ground truth” annotations.\nWhilst both WaveSurfer and the CLAM Annotator \nassociate each data set to a separate canvas, the Sonic \nVisualiser project was commenced in order to provide the \nusers with an innovative approach to the visualisation of \nmusical features, where each set of low and mid-level \ndescriptors is presented as a logical layer, in a fashion \nsimilar to an image processing application.\nThe management of machine-extracted descriptors in \nthe Sonic Visualiser differs substantially from that \nemployed by the CLAM Annotator, that requires an \nexternal executable following specific command line and \noutput conventions. The Sonic Visualiser, instead, \nsupports a specialised type of plugin named “Vamp”, that \nhas been developed in parallel at QMUL’s  Centre for \nDigital Music. \nThis choice was driven by the need to introduce a \ncommon C/C++ API for the development of feature \nextraction algorithms in order to speed up development \ntime, increase the potential for dissemination and \nharmonise legacy code.\n2. The Application\nThe Sonic Visualiser has been developed in C++ using the \nopen source distribution of QT4 [5] for the user interface \nand is licensed under the terms of the GNU General \nPublic License, with the hope of promoting further and \nconstant community-driven enhancements.\nThe application uses stackable panes that can be added \nor removed at the user’s request and can show a number \nof different pieces of data overlaid on one another and \naligned according to time. The overlay approach assists \ncomparisons among extracted and annotated data, as well \nas aiding understanding of the original audio data by \nallowing it to be viewed in more than one form “at once”.Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.\n© 2006 University of Victorialayer  visual \nproperties\nlayer playback and \nsynthesis controlsestimated beats\nonset detection function \nFigure 1. Waveform with beat detection function and estimated  beatswaveform overview displaylayers\nTime scaling\ncontrol\nFigure 2. Waveforms and spectrograms at different zoom levelsThis facility may be used with various sorts of data, \nincluding points in time, values on a discrete time graph, \nas well as waveforms and spectra. Figure 1 shows a detail \nfrom a pane with four layers:\n1 - The pane scroll/zoom properties layer.\n2 - A mono waveform.\n3 - The output of a beat detection function: a series of \nuser-editable values shown as a curve.\n4 - Estimated beats: a series of instants in time shown \nby a  vertical bar at each beat location\nEach layer has a set of visual properties (for example \ncolour or plot type in layer 4); these are available in a \nstack of property boxes shown to the right of figure 1. A \nlayer may be visually raised above the other layers by \nselecting its property box from the stack.\nThe y axes of separate layers on the same pane do not \nnecessarily conform with one another; for example, there \nis no meaningful common y axis between a waveform and \nspectrogram on the same pane.\nIn contrast to the y axes, x axes of separate layers on \nthe same pane always align exactly by audio frame. For \nexample, data in a particular spectrogram window covers \nthe correct x extent corresponding to the source audio \nframes for that window. Similarly, beats detected using a \nfunction that operates on a particular window size are \nshown using vertical lines of width corresponding to that \nof the window.\nIn addition to the user-definable panes, an overview of \nthe time domain waveform under analysis is shown in an \narea at the bottom of the application’s user interface \n(figure 1); this is used also to display the position of the \nplayback cursor (vertical line) as well as the extent of the \ncurrent view (rectangular box).   \n2.1 Multi-resolution representation\nWhen more than one pane is used in the Sonic Visualiser, \nthese are usually aligned to the same sample frame in the \nmiddle of the pane. The user can scroll through the audio \nby dragging the pane left and right with the mouse.\nHowever, the separate panes do not have to be shown at \nthe same time resolution and different zoom levels can be \napplied to each pane independently.\nFigure 2 shows the same source in two waveform panes \nand a spectrogram pane, with the top pane zoomed to a \nsignificantly higher resolution than the second and the \nthird ones, this is also reflected in the waveform overview \ndisplay, where multiple rectangular outlines show the \ndifferent zoom extents (pointed out by the solid arrows in \nfigure 2) \nThe top pane covers a period of approximately one \nsecond in time, while the second and third panes cover \napproximately six seconds . The top pane is therefore effectively a detailed view of the centre one-second of data \nin the bottom panes. If the user were to scroll through the \nfile around this point, the two frames would scroll \ntogether, with the top pane appearing to scroll more \nrapidly than the bottom one, whilst maintaining the same \nsample frame at their centre.\n2.2 Annotation Layers \nIn its current implementation, the Sonic Visualiser also \nprovides the user with four different types of manual \nannotation layers that can be exported to a file or used to \nimport and visualise comma and space separated, or MIDI \ndata files.\nThe “Time Instants Layer”, shown at the top of figure \n3, allows the insertion of  vertical lines at points in time \nthat may represent onsets or beats, while the “Time \nValues Layer” (middle of fig. 3) is an editable sequence of \npoints in time, where each point has a value that \ndetermines its position on the Y axis. Each point may also \nhave a label.\nThe notes layer (bottom of figure 3) consists of an \neditable sequence of notes with an associated label where \neach note has a start time, a duration and a value that \ndetermines its position on the Y axis.\nA text layer is also available for placing freely around \nthe pane informal or descriptive annotations that are not \nspecifically attached to other features.\nThe Sonic Visualiser allows entire work sessions to be \nsaved to file, enabling the instant recall of the audio file \nunder analysis, visualisation styles and settings, \nannotations and data layers.\nFigure 3:Editable layers\n2.3 Playback of audio and features\nAudio data can be played by the Sonic Visualiser and a \nfull set of transport commands is provided. \nTo aid analysis and annotation, looping and time \nscaling during playback are provided. The amount of time \nscaling can be varied in real time using a rotary control in \nthe user interface, shown in figure 2.\nFeatures can also be “listened to” using the playback \nand synthesis controls shown in figure 1; each of the data \nlayers displayed in the Visualiser has a set of performance \ncharacteristics associated with it, including auralisation \nmethod (e.g. play a particular sound each time a data \npoint is hit during playback), output level, and stereo pan. \nIt is therefore possible to compare aurally a set of \ncalculated or annotated features by panning them \nseparately or playing them with different sounds.\n3. Vamp plugins\nAn external API is used by the Sonic Visualiser for the \nautomatic extraction of low and mid-level descriptors \nfrom musical audio data.\nVamp plugins are dynamic link libraries that process \nsampled audio data, returning complex multidimensional \ndata with labels representing semantic observations that \nare automatically loaded by the Sonic Visualiser and \nshown on a new layer.\nThe plugins receive their data block-by-block but are \nnot required to return output immediately on receiving the \ninput, therefore a Vamp plugin may be non-causal, \npreferring to store up data based on its input until the end \nof a processing run and then return all results at once. \nThe feature extractor can indicate to the host the \npreferred processing block and step sizes, and may ask to \nreceive data in the frequency domain instead of the time \ndomain. The host takes the responsibility for converting \nthe input data using an FFT of windowed frames or, \nalternatively, cache frequency-domain data when possible, \nallowing the plugins to do straightforward frequency-\ndomain processing. \nA Vamp plugin is configured once before each \nprocessing run, and receives no further parameter changes \nduring use – unlike real time plugin APIs in which the \ninput parameters may change at any time. This also \nmeans that fundamental properties such as the number of \nvalues per output or the preferred processing block size \nmay depend on the input parameters. \nThe cross-platform plugin SDK is distributed under the \nterms of a BSD-style license, allowing for closed-source \nfeature extraction modules to be used by the Sonic Visualiser and thus permitting the safe dissemination of \nmusical analysis algorithms that are yet unpublished \nand/or not protected by patents.\nVamp plugins are already available: implementations \nof  algorithms for onset detection [6] and beat tracking [7] \nare available from the Centre for Digital Music website \n[8] while other functionalities are available from the \nMazurka project [9]. \n4. Conclusions\nA framework for the automatic extraction, browsing and \nediting of musical feature descriptors has been presented \nin this paper. The Sonic Visualiser runs on Linux, \nWindows and MacOS and proposes a novel “layered” \napproach to musical features visualisation. The binaries \nand source code, along with the Vamp plugin SDK are \navailable for download from the project’s web site:\nwww.sonicvisualiser.org.\n5. Acknowledgments\nPart of this work has been done in the context of the \nSIMAC IST-5071 42 and EASAIER EU-FP6-IST-0339 02 \nprojects. The authors would like to thank Daniel Leech-\nWilkinson and Craig Sapp for their valuable sugge stions.\nReferences\n[1]K. Sjölander, J. Beskow, “WaveSurfer - An Open Source \nSpeech Tool”, in Proceedings of the International \nConference on Spoken Language Processing, 2000.\n[2]P. Herrera et al, “MUCO SA: A Music Content Semantic \nAnnotator”, in Proceedings of the 6th International \nConference on Music Information Retrieval, London  2005.\n[3]F. Gouyon, N. Wack, S. Dixon, “An Open Source Tool for \nSemi-Automatic Rhythmic Annotation”, in Proceedings of \nthe 7th International Conference on Digital Audio Effects, \nNaples 2004 \n[4]X. Amatriain et al, “The CLAM annotator: A Cross-\nPlatform Audio Descriptors Editing Tool”, in Proceedings \nof the 6th International Conference on Music Information \nRetrieval, London 2005.\n[5]QT4 open source edition: www.trolltech.com \n[6]J.P. Bello, C. Duxbury, M.E. Davies, M.B. Sandler, “On \nthe use of Phase and Energy for Musical Onset Detection in \nthe Complex Domain”, IEEE Signal Processing Letters, \nVol 11, No. 6, pp 553-556, June 2004\n[7]M.E. Davies, M.D. Plumbley, “Beat Tracking with a Two \nState Model”, in Proceedings of the International \nConference on Acoustics, Speech and Signal Processing, \n2005\n[8]http://www .elec.qmul.ac.uk/digitalmusic/down loads/\n[9]http://sv.mazurka.org.uk"
    },
    {
        "title": "Song Intersection by Approximate Nearest Neighbor Search.",
        "author": [
            "Michael A. Casey",
            "Malcolm Slaney"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1417827",
        "url": "https://doi.org/10.5281/zenodo.1417827",
        "ee": "https://zenodo.org/records/1417827/files/CaseyS06.pdf",
        "abstract": "We present new methods for computing inter-song similari- ties using intersections between multiple audio pieces. The intersection contains portions that are similar, when one song is a derivative work of the other for example, in two differ- ent musical recordings. To scale our search to large song databases we have developed an algorithm based on locality- sensitive hashing (LSH) of sequences of audio features called audio shingles. LSH provides an efficient means to identify approximate nearest neighbors in a high-dimensional fea- ture space. We combine these nearest neighbor estimates, each a match from a very large database of audio to a small portion of the query song, to form a measure of the approx- imate similarity. We demonstrate the utility of our methods on a derivative works retrieval experiment using both ex- act and approximate (LSH) methods. The results show that LSH is at least an order of magnitude faster than the exact nearest neighbor method and that accuracy is not impacted by the approximate method. Keywords: Music similarity, audio shingling, nearest neigh- bors, high dimensions",
        "zenodo_id": 1417827,
        "dblp_key": "conf/ismir/CaseyS06",
        "keywords": [
            "inter-song similarities",
            "audio shingles",
            "locality-sensitive hashing (LSH)",
            "approximate nearest neighbors",
            "derivative works retrieval",
            "exact nearest neighbor method",
            "high-dimensional feature space",
            "approximate similarity",
            "searching large song databases",
            "accuracy impact"
        ],
        "content": "Song Intersectionby Approximate Nearest Neighbor Search\nMichael Casey\nGoldsmithsCollege\nUniversityofLondon\nm.casey@gold.ac.ukMalcolmSlaney\nYahoo! ResearchInc.\nSunnyvale,CA\nmalcolm@ieee.org\nAbstract\nWe presentnewmethodsforcomputinginter-songsimilari-\nties using intersections between multiple audio pieces. Th e\nintersectioncontainsportionsthataresimilar,whenones ong\nis a derivative work of the other for example, in two differ-\nent musical recordings. To scale our search to large song\ndatabaseswehavedevelopedanalgorithmbasedonlocality-\nsensitivehashing(LSH)ofsequencesofaudiofeaturescall ed\naudioshingles. LSH providesan efﬁcientmeansto identify\napproximate nearest neighbors in a high-dimensional fea-\nture space. We combine these nearest neighbor estimates,\neach a match froma verylargedatabaseof audioto a small\nportionof the querysong,to forma measureofthe approx-\nimate similarity. We demonstratethe utility of our methods\non a derivative works retrieval experiment using both ex-\nact and approximate(LSH) methods. The results show that\nLSH is at least an order of magnitude faster than the exact\nnearest neighbor method and that accuracy is not impacted\nbytheapproximatemethod.\nKeywords: Musicsimilarity,audioshingling,nearestneigh-\nbors,highdimensions\n1. Introduction\nThispaperexploresameanstocomputetheintersectionbe-\ntweenmultipleaudiopieces. Wewanttoﬁndtheportionsof\na piece that are similar, perhaps because one is a derivative\noftheother,in twodifferentmusicalrecordings.\nWeareinterestedinapproximatemethods,wheretheap-\nproximation can be as good as necessary, because we now\nhave access to million-song databases. Exact algorithms\nbasedonbrute-forceaudiosimilaritymeasuresareprohibi tively\nexpensive. The key to our work is a new type of algo-\nrithmcalledlocality-sensitivehashing(LSH).LSHprovid es\naveryefﬁcientmeanstoidentify(approximate)nearestnei gh-\nborsinahigh-dimensionalfeaturespace. Wecombinethese\nnearest neighbor estimates, each a match from a very large\ndatabase of audio to a small portion of the query song, to\nform a measureof the approximatesimilarity of two songs.\nPermission to makedigital orhardcopies ofallorpartofthi s workfor\npersonal or classroom use is granted without fee provided th at copies\narenotmadeordistributed forproﬁtorcommercial advantag e andthat\ncopies bear this notice and thefull citation on the ﬁrstpage .\nc/circlecopyrt2006 University of VictoriaIn our application two songs are similar if one portion is\napproximatelycontainedin anothersong.\nThere are two practical needs driving this work. First,\nusers often have a playlist they want to move to a new sys-\ntem. Wewanttobeabletooffertheuseraclosematchifwe\ndon’t have the exact song title. Second, and perhaps more\nimportantly,commercial success in these days of large mu-\nsiccatalogsisbasedonﬁndingthemusicthatpeoplewantto\nlistento. Thisisdrivenbyarecommendationsystem,which\ndepends on users’ rating data. A recommendation system\nwillperformmuchbetterifwecanpropagateauser’srating\ntootherrecordingsofthesamesong. Theproblemisanalo-\ngousto near-duplicateeliminationin text document[4] and\nimage archives [13] and has many interesting analogues in\ntheaudiodomain.\n1.1. AudioSimilarity\nIt is difﬁcult to deﬁne similarity and even more difﬁcult to\nscore results. For the purposes of this work, we say two\nsongsaresimilarifoneisaderivativeofanother. Derivati ve\nworks do not simply contain “samples” of the signal of an\noriginalwork,butinsteadusepartofavocaltrackandremix\nit with newpercussionandbasstracks. Furthermore,onlya\nsmallpartofthesourceworkisusedforthederivativework,\nsoanymethodusedtoidentifyderivativeworksmustbeable\nto identify a small amount of material in a completely new\ncontext; this is called partial containment. Hence identiﬁ -\ncation of derivativeworksrequiresdeterminingpartial co n-\ntainmentofapproximatelymatchingaudio. Forpurposesof\nevaluation,ourgroundtruthisidentiﬁedassongswithover -\nlappingtitlestemswhichisdiscussedinSection3.4. Table 1\nillustratestherelatedtitlesforMadonna’s NothingFails .\nOursimilaritydeﬁnitionmeansthatourworkisdifferent\nfrom the work that has been done on audio ﬁngerprinting\n[15][11][5][21]. With ﬁngerprinting users want to ﬁnd the\nnameofarecordinggivenasampleoftheaudio. Thesecret\nsauce that makes ﬁngerprinting work is based on deﬁning\nrobustfeaturesofthesignalthatlendthesongitsdistinct ive\ncharacter, and are not harmed by difﬁcult communications\nchannels (i.e. a noisy bar or a cell phone). These systems\nassume that some portion of the audio is an exact match—\nthisisnecessarysotheycanreducethesearchspace. Wedo\nnotexpecttoseeaexactmatchinsongintersectionretrieva l\nandweareinterestedinrankingthesongsthataresimilarto\neachother.Table 1. Derivative works of the Madonna title Nothing Fails\ninacommercial database.\nDuration Title\n4m49s Nothing Fails\n3m55s Nothing Fails(Nevins Mix)\n7m27s Nothing Fails(Jackie’s InLove InThe ClubMix)\n7m48s Nothing Fails(Nevins Global Dub)\n7m32s Nothing Fails(TracyYoung’s Underground Mix)\n6m49s Nothing Fails(Nevins BigRoom Rock Mix)\n8m28s Nothing Fails(PeterRauhofer’s Classic House Mix)\n3m48s Nothing Fails(RadioEdit)\n4m0s Nothing Fails(RadioRemix)\n1.2. LocalitySensitive Hashing\nOuraudioworkisbasedonanimportantnewwebalgorithm\nknown as shingles and a randomized algorithm known as\nlocality-sensitivehashing(LSH)[4]. Shinglesarea popul ar\nway to detectduplicateweb pagesand to lookforcopiesof\nimages. Shingles are one way to determine if a new web\npage discovered by a web crawl is already in the database.\nText shingles use a feature vector consisting of word his-\ntogramstorepresentdifferentportionsofadocument. Shin -\ngling’s efﬁciency at solving the duplicate problemis due to\nan algorithm known as a locality-sensitive hash (LSH). In\na normal hash, one set of bits (e.g. a string) is transformed\nintoanother. Anormalhashisdesignedsothatinputstrings\nthatareclosetogetheraremappedtoverydifferentlocatio ns\nin the output space. This allows the string-matching prob-\nlem to be greatly sped up because it’s rare that two strings\nwill havethesamehash.\nLSH, instead, does exactly the opposite; two patterns\nthat are close togetherare hashedto locationsthat are clos e\ntogether. Each hash produces an approximate result since\nthere is always a chance that two nearby points will end up\nintwodifferenthashbuckets. Thus,wegainarbitrarily-hi gh\nprecisionbyperformingmultipleLSHmappings,eachfrom\nadifferentrandomdirection,andnotingwhichdatabasefra mes\nappearmultipletimesin thesame hashbucketasourquery.\nEach hash can be as simple as a random projection of the\noriginalhigh-dimensionaldataontoasubspaceoftheorigi -\nnaldimensions.\n1.3. Contributions\nThis paper discusses our approach to song-similarity using\napproximate matches. Our earlier work [6] showed that\nmatchedﬁlters,andthusEuclideandistanceinfeaturespac e,\nare an effective way to measure song similarity. We intro-\nducetheideaofaudioshinglesanddescribehowwecanuse\nthem to effectively search a large database of songs by ap-\nproximate matching using nearest neighbor methods. Each\nnearestneighbormatchisweakevidencethatthetwosongsFingerprinting Derivative Works\nGenericSpecificGenre\nFigure 1. Speciﬁcity of derivative works identiﬁcation. Th e\nmost speciﬁc queries are on the left of the ﬁgure and the most\ngeneric on the right. Derivative works identiﬁcation, as de -\nscribedinthiswork, fallsinbetween.\nshare a common musical motif or passage. By combining\nthese simple and fast distance measures, we can effectively\ncomputetheintersectionandsimilaritybetweennearbyson gs.\n2. PreviousWork\nTo date, a rangeof feature-basedtechniqueshave beenpro-\nposed for describing and ﬁnding musical matches from a\ncollection of audio. Figure 1 shows the range of options.\nFingerprinting [12] ﬁnds the most salient portions of the\nmusical signal and uses detailed models of the signal to\nlook for exact matches. At the other end of the speciﬁcity\nscale, genre-recognition [20], global song similarity [17 ],\nartist recognition [9], musical key identiﬁcation [18], an d\nspeaker identiﬁcation [19] use much more general models\nsuch as probability densities of acoustic features approxi -\nmated by Gaussian Mixture Models. These so-called bag-\nof-feature models ignore the temporal ordering inherent in\nthe signal and, therefore, are not able to identify speciﬁc\ncontent within a musical work such as a given melody or\nsectionofa song.\nOurapplicationrequiresalgorithmsthatarerobusttodif-\nferencesinthelyrics,instrumentation,tempo,rhythm,ch ord\nvoicing and so forth, so we explore features that are invari-\nantto variouscombinationsofthese[2][3].\nInherentin ourproblemis the needto measuredistances\nin a perceptually relevant fashion and quickly ﬁnd similar\nmatcheswithoutanexhaustivesearchthroughtheentiredat abase.\nExisting Gaussian Mixture Model methods for computing\naudio similarity do not scale to large databases of millions\nof songsdue to the computationrequiredin pair-wise com-\nparison of models using a suitable distance function such\nas Earth Movers Distance (EMD) [14]. Likewise, high-\ndimensional feature representations are susceptible to th e\ncurseofdimensionalitythatleadstoinefﬁcient(linearti me)\nsearchalgorithms. Wewillfailinlargedatabasesifweneed\nto lookat everysignaltodecidewhichareclosest.\nRecent work showsthat audiofeaturesare efﬁcientlyre-\ntrievedusinglocality-speciﬁchashes(LSH)whichhavesub -\nlinear time complexity in the size of the database. This\nis a key requirement for audio retrieval systems to scale\nto searching in catalogues consisting of many millions of\nentries. These methods have already found applicability in\nimage-retrievalproblems[4]. LSHsolvesapproximatenear -\nest neighborretrievalinhighdimensionsbyeliminatingth e\ncurseofdimensionality[10][8][7].\nThefeaturesusedtodescribethesignalarecritical. LSHis only appropriate when the signal can be represented by\na point in a ﬁxed-dimensional metric space with a simple\nnorm (such as L2). For example, methodsthat comparese-\nquencesofdifferentlengths,suchasdynamictimewarping,\narenoteasytoimplementusingLSH.Othermodelsfailthis\nmetric because the distance measure is not simple. These\nincludeGaussian mixturemodelsandhiddenMarkovmod-\nels. Earlier work [6] shows that LSH is theoretically able\nto solve the audio sequence search problem accurately, and\nin sub-linear time, when the similarity measure is a convo-\nlution of sequencesof audio featureswhich providesan L2\nnorm.\nOur previous work we showed that matched ﬁlters, and\ntherefore Euclidean distance, using chromagram and cep-\nstral features performs well for measuring the similarity o f\npassages within songs [6]. The current work applies these\nmethods to a new problem, grouping of derived works and\nsource works in a large commercial database using an efﬁ-\ncientimplementationbasedonLSH.\n3. Song Intersection\nWenowdescribethestepsforretrievingsongsfromadatabas e\nwith contentthatpartiallyintersectswitha querysong.\n3.1. FeatureExtraction\nUncompressed 44.1kHz PCM audio signals are ﬁrst seg-\nmented into length 372ms frames overlapped with a hop\nsize of 100ms. Thehopsize waschosentotradeofftempo-\nral acuity against time and space complexityfor the search.\nPrevious work indicates that, even at the signal level, the\nspectrum is sufﬁciently correlated in time that small shift s\nin frame alignment lead to small changes in feature values\n[15].\nWe derivetwofeaturesusingconstant-Qspectrumtrans-\nform. Log-frequency cepstral coefﬁcients (LFCC) are ex-\ntracted using a 16th-octaveﬁlterbank and chromagramfea-\ntures are extracted with a 12th-octave ﬁlterbank. In both\ncasestheﬁlterbankextendedfrom 62.5Hzto8kHz. Theﬁl-\nterbankwasnormalizedsuchthatthesumofthelogarithmic\nbandpowersequalledthetotalpower.\nTo extract the LFCC coefﬁcients we used a discrete co-\nsine transform (DCT) retaining the ﬁrst 20 coefﬁcients. To\nextractCHROMfeatureswesummedtheenergyinlogarith-\nmic bands at octave multiples of 12 reference pitch classes\ncorrespondingtothe set {C, C#, D, ..., A #, B}.\n3.2. AudioShingles\nWecreateashinglebyconcatenating30framesof12-dimensi onal\nchromagram features into a single 360 dimensional vector.\nMuch like the original work on shingles [4], we advance a\npointerbyoneframetime,100ms,andthencalculatea new\nshingle. Unlike text shingles, which are word histograms,\nourshinglesaretime-varyingvectors. Tomaketheshingles\ninvariant to energy level we normalized the shingle vectors\nto unitlength.We use the vector dot product to compute the similarity\nbetweenapairofshingles. Thiscanbecomputedefﬁciently\nfor audio shingles using convolution which is proportional\nto theL2(Euclidean)distancebetweenthem[16][6].\n3.3. SimilarityMeasurement\nForthispaper,weuseanewversionofLSHbasedonp-sta-\nble distributions [8][1]. With a p-stable distribution, ve ctor\nsums of random variables from a p-stable distribution still\nhave the original probability distribution. We form a num-\nberofdotproductsbetweenthedatabaseentriesandrandom\nvariables from the p-stable distribution. Each of these dot\nproductsforms a projectiononto the real axis, and helps us\nestimate thetruedistance.\nWecanthendivideuptherealaxisintobucketsandform\na hash that is locality speciﬁc points that are close togethe r\nintheinputspacewillbeclosetogetherafterprojectionon to\nthereal axis.\nOur similarity measurement is performed in two stages.\nWeﬁrstsearchforthe Naudioshinglesinourdatabasethat\nareclosesttoeachquerysong. Giventhesenearestneighbor\nmatches, found using brute force or LSH, we look at the\ntopNshingle matches for a pair of songs and compute the\nsimilarity by averaging these smallest Ndistance scores to\nﬁndthesimilaritybetweenthetwosongs. Thusashortfrag-\nment that is contained in another song will cause the simi-\nlaritymeasuretobesmall andindicatea closematch.\nOur use of LSH is different from its use when ﬁnding\nnearest neighbor matches. Normally, the points found by\nLSH are checked with an exact distance calculation to en-\nsurethattheyaretruenearestneighbors,andnottheresult of\na hash conﬂict. In our case, we skip this ﬁlter. We are only\ninterested in the average distance, so we use all the close\npointsreturnedbyLSHtoformourestimate. Inessencewe\nare using LSH to estimate the matched ﬁlter between two\nshingles.\nInaddition,ourdataismorerandomlydistributedthanin\nnormalusesofLSH.Oftenthenearestmatcheswhenﬁnding\ntextduplicatesaretrulyclosetothequery,perhapsdiffer ing\nin a few discrete directions. In our case, we see that the\ndata is randomly distributed in our 360 dimensional space.\nWeexpectaGaussiannoisemodelsthedistancebetweenan\naudioshingleandit’sclosest neighbor.\nFigure2showsaplotoftheinter-pointdistancesbetween\nchromagramshinglesandrandomGaussian-distributedvec-\ntors. Thedistancehistograms,afterscaling,arenearlyid en-\ntical. Thisequidistancebehavior,andtheexponentialgro wth\nof the distance histogram means that it is hard to pick the\nright radiusfor the nearest neighborcalculation for this a p-\nplicationofLSH.\n3.4. DataSet\nWe performed our experiments on the complete recordings\nof two artists, Madonna and Miles Davis. These two artists\nwerechosenbecausetheybothhaveextensivebackcatalogs1 1.5 2 2.5 3 3.5 4 4.5 5 5.5\nx 10−300.511.522.5x 105\nVector DistanceNumber of vectors with this distanceIntra−song Vector−Distance Histogram\n  \nSong Chromagram Distances\nRandom vector distances\nFigure 2. Intra-song Vector Distance Histograms. Compar-\ning the distance between 100-frame chromagram shingles and\n(solidline)andtwoGaussian random vectors (dashedline.)\nTable 2. Distributionof derivative works in a2018 songsubs et\nof thedatabase.\nArtist Tracks StemsSources Derivatives\nMadonna 306 14282 164\nMilesDavis 1712 540348 1172\nand their music is available electronically. Each recordin g\nhas a unique 20-digituniqueidentiﬁer(UID) that is used to\nlocate metadata such as artist, title, album and song length .\nWe obtained exact copies of each commercially distributed\nrecording in a losslessformat from the Yahoo YMU ware-\nhouse(80GBytesofdata)andperformedourfeatureextrac-\ntion directly on the 44.1kHz PCM representation. Our ex-\nperimentcatalogueconsistsof 306separateMadonnarecord-\nings and 1712separate Miles Davis recordings. The total\ndurationofaudiowas 222hours26minutesand 14seconds.\nOn inspecting the catalogue, it is immediately apparent\nthat many recordingsshare all, or part, of their title strin gs.\nTo stem the titles, we ﬁrst removed any puncuation, such\nas quotation marks, and truncated each title up to the ﬁrst\nparenthesisifpresent,elsenotruncationoccured. Anylea d-\ning or trailing whitespace after these transformations was\nalso removed. For example, all of the titles in the Table 1\nwere transformed by the stemming to the string “Nothing\nFails.”\nOnce the titles in the database were stemmed, we gath-\nered statistics on title use within each artist’s collectio n of\nsongs,whicharesummarizedin Table2.\nTherewere 306differentMadonnarecordingsinthedatabase\nwith142uniquetitle stems, 82ofwhich hadderivativever-\nsions(58%), giving a total of 164derivative works. Simi-\nlarly,therewere1712differentMilesDavisrecordings,wi th\n540unique title stems, of these 348 had derivative versions(64%)givingatotal of1172derivativeworks.\nWe used 20 Madonna songs with deriative works as our\ntest set. From the set of songs with the same title stem, a\n“source”songwasselected asbeingthe historicallyearlie st\nversionof the songin the database. Thenumberof relevant\nmatchesfortheset of20suchsourcequeries(notincluding\nthequeriesthemselves)is76songsofthe2018.\n4. Results\nIn this section we describe the details and evaluation of re-\ntrieving derivative works by nearest neighbor audio shin-\ngles. The similarity measure is a measure of the degree of\nintersection between the songs in the database. In our ex-\nperiments, reported here, silence was ﬁrst removed using\nanabsolutethresholdandthenlow-energyshingleswerere-\nmovediftheywere belowthemeanenergyforthesong.\n4.1. LSHExperiment\nIntheﬁrstexperimentweextracted 30-frameshinglesof12-\ndimensionalCHROM featureswith a hopsize ofoneframe\n(0.1s). Thisyieldeda360dimensionvectorevery 0.1s. For\neach song in the detabase we found 10 nearest neighbors\nfor pairs of query and database song shingles. The average\nof the 10nearest distances for each song was taken to be\nthe measure of intersection between the query song and the\ndatabasesong. Sortingthedistancesyieldeda rankedlist o f\ndatabasesongsforthegivenquerysong. Thisoperationwas\nperformedforall of 20querysongs.\nWe used textual title stem matches to identify ground\ntruth derivative works, see Table 1. We recorded true posi-\ntives and false positives at each level of recall standardiz ed\ninto 10th-percentiles. Conﬁdence intervals were estimate d\nusing the standard deviation of the precisions at each 10th-\npercentile interval and dividing by the square root of the\nnumberofquerysongs.\nFigure 3 shows the results of retrieval of song intersec-\ntionsusingthe LSH algorithmvaryingthe searchradiusfor\nnearest neighbors. The dotted line shows the result for ex-\nactnearestneighborretrieval. Theremaininglinesshowth e\nperformance of LSH retrieval using raidii 0.04≤r≤0.2.\nAt70%recallthealgorithmachieves 70%precisionfor r=\n0.2, dropping to 51%precision for 100%recall. We note\nthe the LSH approximation did not introduce any signiﬁ-\ncant error in the derivative works retrieval task for a radiu s\nofr= 0.2, but for lower radii the precision decreased sig-\nniﬁcantlywhencomparedwiththeexactalgorithm’sperfor-\nmance. Thisillustratestheneedtochoosethecorrectsearc h\nradiusforthe task.\n4.2. FeatureVariation\nIn the next experiment we varied the features to test which\nfeaturecombinationperformedbest inourtask. We alsoin-\ncreased the shingle size to 100frames, thus yielding 12000.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10.10.20.30.40.50.60.70.80.91\nRecallPrecisionLSH 10−Nearest Neighbour Song Intersection Performance\nr=0.04 0.1 0.14 0.16 0.18 r=0.2 Exact Nearest Neighbor \nFigure 3. Performance of LSH nearest neighbor retrieval for\nestimating song intersections (derivative works). Audio s hin-\ngles were 30 frames and the search radius varied between\nr= 0.04andr= 0.2. The dotted line shows the exact nearest\nneighborresultfor searchradius r= 0.2.\ndimensional vectors for the CHROM features and 2000 di-\nmensional vectors for LFCC. For comparison to the Chro-\nmagram extraction method of Bartsch [2] we tried a varia-\ntion on CHROM features with a cutoff frequency of 2kHz\ninsteadofthe8kHzcutoffusedforthe rest.\nWealsotriedajointfeaturespaceconsistingofbothCHROM\nand LFCC features. Here, the song similarity measure is a\nweighted averageof the chromagramandlfcc features. Re-\nsults are shown for 0.9*CHROM + 0.1*LFCC; an empiri-\ncallydeterminedmixtureofthedistances.\nFigure4showstheresultsforthefeaturevariationexper-\niment. The worst performing features were CHROM, with\n2kHz cutoff, and LFCC both returning a precision of 65%\nat70%recall. For the CHROM features, with 8kHz cutoff,\nthe performance is much better and almost identical to that\nshown in Figure 3. From this we conclude that increasing\ntheshinglesize from 3sto10shadnosigniﬁcantimpacton\nthe results. We also note that CHROM features performed\nsigniﬁcantly better than LFCC but signiﬁcantly worse than\nthe joint CHROM+LFCC feature space. The improvement\nmight be accounted for by the false negative rate being re-\nduced but not the false positive rate using the joint feature\nspace. CHROMandLFCCencodequalitativelydifferentas-\npects of the songs– CHROM features encode the harmony\nand pitch content, and LFCC features encode the timbral\ncontent. However, we were surprised that the joint features\nperformedbetterandwe areinvestigatingthereason.\nTo see how retrieval performance scaled, we compared\nperformanceusing the 306-songsubset with the 2018-song\ndatabase, Figure 5. There was a 10% drop in precision for\nthe larger database at recall rates greater than 40%. The\nprecisionwas63%at a recallof70%forthelargerdataset.\nFigure 4. Performance of exact audio shingle retrieval for d if-\nferentfeaturesandafeaturecombination. Heretheaudiosh in-\ngles are 10sinlength.\nFigure 5. Comparative performance for database of 306 songs\nand2018 songs.4.3. TimecomplexityofExactvs. LSH Algorithms\nThe time complexity of the exact approach is |Q| × |¯S| ×\nd×w×O(N)in the numberof songs in the database, N,\nthe number of query shingles, |Q|, the average number of\nshinglesper song,¯|S|, the featuredimensionality, d, and the\nlength of the shingles, w. For the twenty queries matched\nagainsta306-songdatabaseusingchromagramfeatures,thi s\nresultsinapproximately 306×20×3000×3000×12×30 =\n19.8×1012multiply-accumulate operations. Computation\nfortheexactalgorithmapproximately7hoursusinga3GHz\nPPC processor. For the 2018-song database, computation\ntimeincreasedtoapprox. 150hoursfortheexactalgorithm.\nThe LSH algorithm’s performance depends on the size\nofthehashbucketsandthedegreeofapproximationusedin\nthe nearestneighborsearch. Forourchosenparameters,the\nLSH program completed the task in approximately 1 hour\nfor the 306-song dataset. However, more than half of the\ntime was spent self-tuning the parameters and building the\nhash tables, both of these are operations that only need to\nbeperformedonceforeachradius. Weobservedthatthere-\ntrievalpartoftheexecutioncycletooklessthan30minutes ,\ntherefore running at least 14times faster than exact nearest\nneighborretrieval.\n5. Conclusions\nWe introduced audio shingles for measuring musical simi-\nlarity. We employed them as a means for identifying mu-\nsical works that approximately match, or intersect, over a\npart of their content. We described the features used and\nthe similarity methods employed as well as two algorithms\nforimplementingthesimilarity-basedretrievalusingnea rest\nneighborsearch.\nThe exact method gives good results, but it takes a long\ntime to compute the answer, scaling linearly in the size of\nthe database. The approximate algorithm based on LSH is\ngreaterthananorderofmagnitudefasterandyieldsaccurat e\nresultsonourchosentask.\nOur conclusion is that hashing for low-level audio fea-\ntures is accurate and speeds up complex retrieval tasks sig-\nniﬁcantly.\n6. Acknowledgement\nMalcolmSlaneydedicatesthispapertothememoryof\nGloria Levitt(Hejna). She loveddancingandsingingto the\nmusicofMadonnawithheryoungsons,JoeyandJoshua.\nReferences\n[1] Alex Andoni and Piotr Indyk. E2LSH0.1 User Manual ,\nMIT, 2005. http://web.mit.edu/andoni/www/LSH .\n[2] Mark A. Bartsch and Gregory H. Wakeﬁeld. To Catch a\nChorus: Using Chroma-Based Representations for Audio\nThumbnailing. in Proc.WASPAA ,2001.\n[3] J.P.Bello,andJ.A.Pickens. ARobustMid-level Represe n-\ntation for Harmonic Content in Music Signals. Proceedingsof the 6th International Conference on Music Information\nRetrieval (ISMIR-05), London, UK. September 2005.\n[4] A.Z.Broder,S.C.Glassman,M.S.Manasse,andG.Zweig.\nSyntactic clustering of the web. In Proceedings of WWW6 ,\npages 391–404, Elsevier Science,April 1997.\n[5] P. Cano and E. Batlle and T. Kalker and J. Haitsma. A re-\nview of algorithms for audio ﬁngerprinting. In International\nWorkshop on Multimedia Signal Processing , US Virgin Is-\nlands, December 2002.\n[6] Michael Casey and Malcolm Slaney. The Importance of\nSequences for Music Similarity. In proc. IEEE ICASSP ,\nToulouse, May2006.\n[7] T.Darrell,P.IndykandG.Shakhnarovich.Locality-sen sitive\nhashing using stable distributions, in Nearest Neighbor\nMethods inLearning and Vision: Theory and Practice ,MIT\nPress,2006.\n[8] M. Datar, P.Indyk, N. Immorlica and V. Mirrokni. Localit y-\nSensitive Hashing Scheme Based on p-Stable Distributions,\nInProceedings of the Symposium on Computational Geom-\netry, 2004.\n[9] D. Ellis, B. Whitman, A. Berenzweig, S. Lawrence. The\nQuest for Ground Truth in Musical Artist Similarity. Proc.\nISMIR-02 , pp. 170–177, Paris,October 2002.\n[10] Aristides Gionis, Piotr Indyk and Rajeev Motwani. Simi lar-\nitySearchinHighDimensionsviaHashing. TheVLDBJour-\nnal,pp. 518–529, 1999.\n[11] Jaap Haitsma, Ton Kalker. A Highly Robust Audio Finger-\nprinting System,in Proc.ISMIR ,Paris,2002.\n[12] J. Herre, E. Allamanche, O. Hellmuth, T. Kastner. Robus t\nidentiﬁcation/ﬁngerprinting of audio signals using spect ral\nﬂatness features. Journal of the Acoustical Society of Amer-\nica, Volume 111, Issue 5, pp. 2417–2417, 2002.\n[13] Yan Ke, Rahul Sukthankar, LarryHuston. An efﬁcient nea r-\nduplicate andsub-image retrievalsystem. ACMMultimedia,\n2004: 869–876.\n[14] B. Logan and S. Chu. Music Summarization Using Key\nPhrases.InProc.IEEEICASSP ,Turkey, 2000.\n[15] MatthewMiller,ManuelRodriguezandIngemarCox.Audi o\nFingerprinting: Nearest Neighbour Search in High Dimen-\nsionalBinarySpaces. IEEEWorkshop onMultimediaSignal\nProcessing , 2002.\n[16] Meinard Muller, Frank Kurth and Michael Clausen. Audio\nMatching via Chroma-Based Statistical Features. In Proc.\nISMIR, London, Sept. 2005\n[17] E.Pampalk,A.FlexerandG.Widmer.ImprovementsofAu-\ndio Based Music Similarity and Genre Classiﬁcation. In\nproc. ISMIR , London, Sept. 2005.\n[18] S. Pauws. Musical Key Extraction from Audio. In\nProc.ISMIR ,Barcelona, 2004.\n[19] DouglasA.Reynolds.Speakeridentiﬁcationandveriﬁc ation\nusingGaussianmimxturespeakermodels. SpeechCommun.,\n17 (1–2):91–108, 1995.\n[20] G. Tzanetakis and P. Cook. Musical genre classiﬁcation of\naudiosignals. IEEETransactions onSpeechandAudioPro-\ncessing,10(5):293–302, 2002.\n[21] AveryLi-ChunWang,JuliusO.Smith,III.Systemandmet h-\nods for recognizing sound and music signals in high noise\nand distortion. UnitedStatesPatent6990453, 2006."
    },
    {
        "title": "BDB-MUS: a project for the preservation of Brazilian musical heritage.",
        "author": [
            "Beatriz Magalhães Castro",
            "Luiza Beth Nunes Alonso",
            "Edilson Ferneda",
            "Murilo Bastos da Cunha",
            "Fernando William Cruz",
            "Márcio da Costa P. Brandão"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1414998",
        "url": "https://doi.org/10.5281/zenodo.1414998",
        "ee": "https://zenodo.org/records/1414998/files/CastroAFCCB06.pdf",
        "abstract": "This poster proposes a discussion on concepts evolving from the role of digital libraries on the preservation of tan- gible and intangible cultural inheritance, including con- cepts developed in 2003 by UNESCO and the World Summit on the Information Society. It further describes the construction and design process leading to the develop- ment of BDB-MUS – Brazilian Digital Music Library, which aims to establish national recommendations on metadata attributions, and to develop means for appropria- tion and retrieval of musical sources. The poster further explores the concept of digital music or culture within the aims and objectives of the project. Keywords: Digital Libraries; Cultural Preservation; In- digenous Music; Popular Music; Erudite Music; globaliza- tion; Brazil.",
        "zenodo_id": 1414998,
        "dblp_key": "conf/ismir/CastroAFCCB06",
        "keywords": [
            "Digital Libraries",
            "Cultural Preservation",
            "UNESCO",
            "World Summit on the Information Society",
            "BDB-MUS",
            "Brazilian Digital Music Library",
            "metadata attributions",
            "musical sources",
            "digital music or culture",
            "globalization"
        ],
        "content": "BDB-MUS: a project for the preservation of Brazilian musical heritage \n \nBeatriz Magalhães Castro \nUniversity of Brasilia \nMusic Department \nBrasília, BRAZIL \nbeatriz@unb.br Luiza Beth Nunes Alonso \nCatholic University of Brasilia \nKnowledge Management & IT  \nBrasília, BRAZIL \nlualonso@pos.ucb.br  Edilson Ferneda \nCatholic University of Brasilia \nKnowledge Management & IT  \nBrasília, BRAZIL \neferneda@pos.ucb.br\nMurilo Bastos da Cunha \nUniversity of Brasilia \nInformation Science Department \nBrasília, BRAZIL \nmurilobc@unb.br Fernando William Cruz \nUniversity of Brasilia \nInformation Science Department \nBrasília, BRAZIL \nfwcruz@ucb.br  Márcio da Costa P. Brandão \nUniversity of Brasilia \nComputer Science Department \nBrasília, BRAZIL \nbrandao@unb.br\nAbstract \nThis poster proposes a discussion on concepts evolving \nfrom the role of digital libraries on the preservation of tan-gible and intangible cultural inheritance, including con-cepts developed in 2003 by UNESCO and the World Summit on the Information Societ y. It further describes the \nconstruction and design process leading to the develop-ment of BDB-MUS – Brazilian Digital Music Library, which aims to establish national recommendations on metadata attributions, and to develop means for appropria-\ntion and retrieval of musical sources. The poster further \nexplores the concept of digital music or culture within the \naims and objectives of the project. \nKeywords: Digital Libraries; Cultural Preservation; In-\ndigenous Music; Popular Music; Erudite Music; globaliza-\ntion; Brazil. \n1. Introduction \nThe term globalization has been commonly perceived as an economic movement that  accelerated production proc-\nesses, market exchanges, decision making and interven-tions into the real world. These were enabled through global transformations, especially those related to informa-tion and communication technologies (ICT). Yet, such technological innovations also brought the prospect of identifying and distinguishing new cultures and differenti-ated social structures th roughout the possibilities offered \nby interconnected virtual spaces. This enabled the contact with other world interpretations, rooting the phenomenon \nof regionalism or localization [\n1]. \nToday we dwell with technology-based industries that can act upon information transforming data into new logi-cal processes coherent to the complex interactions between \nhuman thought and economic development. Nevertheless, \nin spite of a seemingly final overthrown, local cultural \nexpressions, and especially those from economically frag-ile groups, are able to find through ICT its subsistence and developmental continuity. Unlike past economically glob-alized movements such as European mercantilist and colo-nialist action in Latin America and Asia, today there is the possibility of rejection and di sposal of acculturation proc-\nesses. This may be referred to as transculturation  or \ntransculturality , a concept that it wi ns prominence with \nAugustí Coll’s works [2]. \nIn Coll's transculturality  it is necessary to avoid three \ntypes of reductionism: culture solely as folklore, and sometimes associated to gr oups or communities marginal-\nized from hegemonic production processes; culture as a scholarly expression in which only abstraction is valued; and culture as sacred soil of values and faiths that cannot be questioned or, when done so, be considered as politi-cally incorrect. Thus, the intent is to understand cultural contexts inserted into different realities - individual, col-lective, tangible, intangible, in interaction with other social groups and cultural forms.  \nThe speed and easiness of access of information re-\ntrieval, favored by ICT may foster the creation of new cul-tural codes within the so-called culture of virtual reality \n[3]. Anchored on socially organized webs disentailed from economic status and political  empowerment, and based on \nthe use of the human mind as a creative and productive force and not just as an elem ent utilized by a given produc-\ntion system, information technologies within the context of digital music and culture emerge as a resisting factor against the process of cultural homogenization inherent to the globalized order. \nPermission to make digital or hard copi es of all or part of this work for \npersonal or classroom use is grante d without fee provided that copies \nare not made or distributed for profit or commercial advantage and that \ncopies bear this notice and the full citation on the first page. \n© 2006 University of Victoria Based on the concept of culture of virtual reality and \nbranching from the informational paradigm, is envisioned a framework in which cultural expressions interweave \nfrom and within a global electronic hypertext, the Internet, configured as a common reference for the processing of cultural expressions and of human creativity, independ-ently from excluding and limitative parameters such as geographical location and linear time. What matters are the capacity and the actual experience as communicational beings. The symbolic is not just a group of metaphors that guarantee a group’s identity, but ra ther translates itself in \nactual experience, in which vi rtual space, past and future, \ndialogue in an extemporal frame within computer webs and electronic media. \n2. The project \nThe BDB-MUs group aims to develop and act on three applied areas of ITCs: (i) the unearthing of what is still \nignored, including tangible and \"intangible heritage\" [4], musical and cultural expressions of several Brazilian in-digenous groups; (ii) in the dissemination and preservation \nof known but not treated musical archives, such as Brazil-ian composer Claudio Santoro’s complete archives (ca. 20.000 items); (iii) the construction of a digital library on \nthe popular instrumental music gender Choro  [5]; (iv)  the \ndevelopment of metadata pattern attributions to systema-tized information, as found in archival sources in libraries throughout the country. One should point to other projects \ndeveloped in Brazil such as  the Biblioteca Nacional [6] \nand the CDMC at Unicamp Univ ersity [7]. These topics \nare also being discussed within the Brazilian scientific \ncommunity in Music [8] and in Computer Science [9]. \nHowever, these initiatives are not aimed to the study of \nmusical object patterns, metadata retrieval, preservation strategies, among others important aspects that may guar-antee longevity and interoperab ility of existent or develop-\ning databases. The IBICT Institute (Instituto Brasileiro de Informação em Ciência e Tecnologia ) [10], a federal a-\ngency with several atributions in Information Science, seeks to develop and recommend normative directives for the development of digital archives in the expanded spec-\ntrum of Brazilian music heritage. \nThe BDB-MUS project, supported by IBICT, integrates \nresearchers from several areas including music, ICT, \nKnowledge Management and Information Science and its objectives are: (i) identification of ICT resources that can \nbe applied to composition of musical archives within the concept of culture of virtual reality, (ii) accomplishment \nand promotion of national debates with the intent of adju-dication and endorsement of the results of technological research, (iii) discussion of normative procedures to be \napplied on a broader national scope, and (iv) elaboration of \naccompaniment routines and certification of the imple-\nmented actions. The certification will be based on two pillars: the application of Information Technologies and \nthe theoretical conceptualization supported by Knowledge Management tools. From the point of view of ICT, the following tasks are \nforeseen: (i) definition of the elements that compose a mu-\nsical digital object and its format, (ii) definition of relevant \nmetadata (content and bibliographical entries), (iii) identi-\nfication of alternatives for content retrieval, (iv) identifica-\ntion of possible user interface and interaction, (v) defini-\ntion of strategies for the preservation of musical objects, and (vi) integration with RISM and others international \nmusic databases.  The project seeks th e conceptualization \nof a framework for the applied use of approved and en-\ndorsed normatives.  \nIn the perspective of Knowledge Management, the in-\nteractivity between ICT and the creation of a “Culture of \nvirtual reality” will be chiefly approached. Aspects such as transculturality  and group identity  will be differentiated \nand particularly explored in the qualitative analysis of the collected material. \nFinally, from the musicological point of view, one aims \nto the construction of interactive digital musical archives containing Brazilian musical forms and genres, regardless of style and source. \n3. Perspectives \nDue to its interdisciplinary nature, this project, will work closely with Brazilian and international communities within the areas of Musicology, ICT, Knowledge Man-agement, and Information Science. A partnership with the community of Music Information Retrieval will be at this point opportune and profitable. \nReferences \n[1] Castells, M. “Globalization, Technology, Work, Employ-\nment, and Enterprise”, La factoria , n. 7, October 1998. \nAvailable: http://lafactoriaweb.com/articulos/castells7.htm . \n(In Spanish) \n[2] Coll, A. N. “The Cultures Are Not Disciplines: Does The Transcultural Exist?” In: Educação e Transdiciplinaridade \nII. São Paulo : Editora Triom, 2000. (In Portuguese) \n[3] Castells, M. The Information Age, Vol. 1: The Rise of the \nNetwork Society . Cambridge MA, Oxford (UK): Blackwell \nPublishers, 2nd Edition, 2000. \n[4] UNESCO. “Convention for the Sa feguarding of the Intangi-\nble Cultural Heritage”. MISC/2003/CLT/CH/14. Paris, Oc-tober 2003. Available: \nunescdoc.unesco.org/images/0013/  \n001325/132540e.pdf .  \n[5] Cruz, F. W. et al. \"A Brazilian Popular Music Oriented Digital Library For Musical Harmony E-Learning\", Pro-\nceedings of the 5\nth International Conference on Music In-\nformation Retrieval  (ISMIR'04), pp. 429-432, Barcelona \n(Spain), 2004. Available: ismir2004.ismir.net/proceedings/  \np078-page-429-paper198.pdf . \n[6] Brazilian Nacional Library [Web Site] Available: http:// www.bn.br/fbn/musica/ \n[7] Centro de Documentação de Música Contemporânea. Uni-camp [Web Site] Available:  http://www.unicamp.br/cdmc/ [8] ANPPOM. Associação Nacional de Pesquisa e Pós-\nGraduação em Música. [Web site] Available: http:// www.anppom2006.unb.br/ \n[9] SBCM. Brazilian Symposium on Computer Music [Web site] Available: http://www.cefala.org/sbcm2005/ [10] IBICT. [Web site] Available: http://www.ibict.br"
    },
    {
        "title": "Search Sounds: An audio crawler focused on weblogs.",
        "author": [
            "Òscar Celma",
            "Pedro Cano",
            "Perfecto Herrera"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1415142",
        "url": "https://doi.org/10.5281/zenodo.1415142",
        "ee": "https://zenodo.org/records/1415142/files/CelmaCH06.pdf",
        "abstract": "In this paper we present a focused audio crawler that mines audio weblogs (MP3 blogs). This source of semi-structured information contains links to audio files, plus some textual information that is referring to the media file. A retrieval system —that exploits the mined data— fetches relevant au- dio files related to user’s text query. Based on these results, the user can navigate and discover new music by means of content-based audio similarity. The system is available at: http://www.searchsounds.net. Keywords: focused audio crawler, weblogs, music recom- mendation, content-based similarity.",
        "zenodo_id": 1415142,
        "dblp_key": "conf/ismir/CelmaCH06",
        "keywords": [
            "focused audio crawler",
            "audio weblogs",
            "semi-structured information",
            "audio retrieval system",
            "user text query",
            "content-based audio similarity",
            "music recommendation",
            "weblogs",
            "content-based similarity",
            "audio files"
        ],
        "content": "Sear chSounds: Anaudio crawler focused onweblogs\n\u001eOscar Celma\nMusic Technology Group\nUniversitat Pompeu Fabra\nPg.Circumv al.laci ´o8,08003\nBarcelona, SPAIN\nocelma@iua.upf.eduPedroCano\nMusic Technology Group\nUniversitat Pompeu Fabra\nPg.Circumv al.laci ´o8,08003\nBarcelona, SPAIN\npcano@iua.upf.eduPerfecto Herr era\nMusic Technology Group\nUniversitat Pompeu Fabra\nPg.Circumv al.laci ´o8,08003\nBarcelona, SPAIN\npherrera@iua.upf.edu\nAbstract\nInthispaper wepresent afocused audio crawler thatmines\naudio weblogs (MP3 blogs). This source ofsemi-structured\ninformation contains links toaudio \u0002les, plus some textual\ninformation thatisreferring tothemedia \u0002le. Aretrie val\nsystem that exploits themined data fetches relevantau-\ndio\u0002les related touser' stextquery .Based onthese results,\ntheuser cannavigateanddisco vernewmusic bymeans of\ncontent-based audio similarity .Thesystem isavailable at:\nhttp://www .sear chsounds.net .\nKeywords: focused audio crawler,weblogs, music recom-\nmendation, content-based similarity .\n1.Introduction\nInrecent years thetypical music consumption behaviour\nhaschanged dramatically .Personal music collections have\ngrownfavoured bytechnological impro vements innetw orks,\nstorage, portability ofdevices andInternet services. Inthe\nconte xtoftheWorld WideWeb,theincreasing amount of\navailable music makesverydif\u0002cult, totheuser,to\u0002ndmu-\nsiche/she would liketolisten to.Toovercome thisproblem,\nthere aresome audio search engines1thatcan\u0002ttheuser' s\nneeds [1].Some ofthese search engines arenevertheless not\nfully exploited because their companies would havetodeal\nwith copyright infringing material.\n2.Syndication ofWebContent\nDuring thelastyears, syndication ofweb content asec-\ntionofawebsite made available forother sites touse has\nbecome acommon practice forwebsites. This originated\nwith newsandweblog sites, butnowadays isincreasingly\n1Tomention afew(accessed onJune, 1st,2006):\nhttp://search.singing\u0002sh.com/,\nhttp://audio.search.yahoo.com/,\nhttp://www .audio crawler.com/,\nhttp://www .allthe web.com/?cat=mp3 and\nhttp://www .altavista.com/audio/\nPermission tomakedigital orhard copies ofallorpartofthisworkfor\npersonal orclassroom useisgranted without feeprovided thatcopies\narenotmade ordistrib uted forpro\u0002t orcommercial advantage andthat\ncopies bear thisnotice andthefullcitation onthe\u0002rstpage.\nc\r2006 University ofVictoriaused tosyndicate anykind ofinformation. Since thebe-\nginning of2003, aspecial type ofweblog, named audio\nweblogs (orMP3 blogs), hasbecome verypopular .These\nblogs makemusic titles available fordownload. Themusic\nposted isexplained bytheblog author ,andusually ithas\nlinks thatallowtobuythecomplete albumorwork. Some-\ntimes, themusic posted ishard to\u0002nd orhasnotbeen is-\nsued inmanyyears, andmanyMP3 blogs link strictly to\nmusic thatisauthorized forfreedistrib ution. Inother cases,\nMP3 blogs include adisclaimer stating thattheyarewilling\ntoremo vemusic ifthecopyright owner objects. Anyway,\nthissource ofsemi-structured information isajewelforweb\ncrawlers, asitcontains theuser' sobject ofdesire e.g. an\naudio \u0002le, andsome textual information thatisreferring\ntotheobject.\n2.1. Fileformats\nThe\u0002leformat used tosyndicate web content isXML. The\nXML description touseisde\u0002ned intheRSS (and Atom)\nfamily .The RSS abbre viation isvariously torefer tothe\nfollowing standards: Really Simple Syndication (RSS 2.0),\nRich SiteSummary (RSS 0.91 and1.0) orRDF SiteSum-\nmary (1.0).\n2.2. Multimedia syndication\nOfspecial interest arethefeeds thatsyndicate multimedia\ncontent. These feeds publish audio visual information thatis\navailable onthenet. Aninteresting example istheMedia\nRSS (mRSS) speci\u0002cation2,lead byYahoo! andthemul-\ntimedia RSS community .mRSS allowstosyndicating mul-\ntimedia \u0002les (audio, video, image) inRSS feeds, andadds\nseveralenhancements toRSS enclosures. Although mRSS\nisnotyetwidely used onthenet,there aresome websites\nthatsyndicates their multimedia content following thespec-\ni\u0002cation3.\n3.Crawling RSS feeds\nThe implemented audio crawler starts theprocess from a\nmanually selected listofRSS links (MP3 blogs). Each RSS\n\u0002lecontains alistofentries (oritems ).The crawler seeks\nfornewincoming items using thepubDateitem value\n2http://search.yahoo.com/mrss/\n3One ofthemost important ones ishttp://www .ourmedia.or gandcomparing with thelatest entry inthedatabase and\nstores thenewinformation intothedatabase. Thus, theau-\ndiocrawler system hasanhistoric information ofallthe\nitems thatappeared inafeed.\n4.Audio Retrie valSystem\nThelogical viewofafeed item canbedescribed bythebag-\nof-w ords approach: adocument isrepresented asanumber\nofunique words, with aweight assigned toeach word[2].\nSpecial weights areassigned tothemusic related terms, as\nwell asthemetadata (e.gID3 tags) extracted from theau-\ndio\u0002le. Similar toourapproach, [3]presents aproposal of\nmodifying theweights oftheterms pertaining tothemusical\ndomain. Amore sophisticated method based onunsuper -\nvised learning oftextpro\u0002les formusic (from unstructured\ndata crawled from theweb) isexplained in[4]\nMoreo ver,basic natural language processing methods are\napplied toreduce thesize ofthedocument (elimination of\nstopw ords, andapply Porter' sstemming algorithm [5]). The\ninformation retrie val(IR) model used istheclassic vector\nmodel approach, where agivendocument isrepresented as\navector inamultidimensional space ofwords (each wordof\nthevocabulary isacoordinate inthespace).\n4.1. Full textsearch\nThesimilarity function, sim(dj;q),between aquery (q)and\nadocument (dj)isbased ontheTF=IDFweightning func-\ntion, where TF(term frequenc y)denotes thefrequenc yof\nthewordtinthedocument dj,andIDF(inversedocument\nfrequenc y)measures thegeneral importance oftintheover-\nallcollection [2]:\nsim(dj;q)\u0018X\nt2qTFt;j=j~djj\u0001IDFt (1)\nThis IRmodel iswell suited notonly forquerying via\nartists' orsongs' names, butformore comple xtextqueries\nsuch as:funk yguitar riffsortraditional Irish tunes.\nTheretrie valsystem outputs thedocuments (i.e.feed en-\ntries) thatarerelevanttotheuser' squery ,rankedbythesim-\nilarity function. Alltheaudio links posted intheentry are\ndisplayed too,sotheuser canlisten totheaudio \u0002les asso-\nciated tothatentry .\n4.2. Content based similarity\nBased ontheresults obtained from theuser' stextual query ,\nthesystem allowsto\u0002nd similar audio \u0002les bymeans of\ncontent-based audio similarity .Each link toanaudio \u0002le\nhasaSimilar sounds button thatretrie vesthemost simi-\nlaraudio \u0002les, based onasetofmid-le velaudio descriptors.\nThese descriptors areextracted from theaudio andrepresent\nproperties such as:rhythm, harmon y,timbre andinstrumen-\ntation, intensity ,structure andcomple xity[6].\nThis exploration (orbrowsing) mode allowstotheuser\ntodisco vernewmusic, related toheroriginal (text-based)query ,thatwould bemore dif\u0002cult todisco verbyusing tex-\ntualqueries only.Toourknowledge, nowadays, thisisthe\nonly web-based audio search engine thatallowsthistype of\ncontent-based navigation. There isanaanalogy between\nthis type ofnavigation and, forexample, Google' s\u0002nd\nweb pages that aresimilar toagivenHTML page. In\nourcase, similarity among items arebased onaudio simi-\nlarity ,whereas Google approach isbased onthecontent of\ntheHTML page. Both browsing approaches are,then, based\nonthecontent analysis oftheretrie vedobject.\n5.Conclusions andPending Work\nWehavepresented anaudio crawler focused onweblogs that\npublish music related information. Outofthecrawling pro-\ncess, each feed item isrepresented asatextdocument, con-\ntaining theitem content, aswell asthelinks totheaudio\n\u0002les. Then, aclassic textretrie valsystem outputs relevant\nitems related totheuser' squery .Moreo ver,acontent-based\nnavigation allowstobrowseamong theretrie veditems and\ndisco vernewmusic andartists bymeans ofaudio similarity .\nAninteresting remaining task could beasetofexperi-\nments addressing theinteraction between content-based and\ntext-based querying, andhowusers differently emplo ythem.\nFinally ,arelevance feedback method totune thesystem and\ngetmore accurate results (specially forthecontent-based\nnavigation) should betakenintoaccount.\n6.Ackno wledgements\nWeareverygreatful forthehelp ofMarkus Koppi Koppen-\nberger,Nicolas Falquet andXavierOliverinthedesign and\nimplementation ofthesystem.\nRefer ences\n[1]I.Knopk e.AROOOGA: AnAudio SearchEngine forthe\nWorld WideWeb,inProceedings oftheInternational Com-\nputer Music Conference, 2004, Barcelona, Spain.\n[2]R.Baeza-Y ates, Modern Information Retrie val,Addison-\nWesley,1999.\n[3]S.VembuandS.Baumann, ASelf-Or ganizing Map Based\nKnowledg eDisco very forMusic Recommendation Systems ,\nComputer Music Modeling and Retrie val,2004, Esbjer g,\nDenmark.\n[4]B.Whitman andS.Lawrence. Inferring Descriptions and\nSimilarity forMusic fromCommunity Metadata ,inProceed-\nings ofthe2002 International Computer Music Conference,\n2002, Gotebor g,Sweden.\n[5]M.F.Porter ,(1980). AnAlgorithm forSuf\u0002xStripping .Pro-\ngram 14,130137.\n[6]P.Herrera etal.SIMA C:Semantic Inter action with Music\nAudio Content ,inProceedings ofthe2ndEuropean Work-\nshop ontheIntegration ofKnowledge, Semantic andDigital\nMedia Technologies, 2005, SavoyPlace, London."
    },
    {
        "title": "A Fast, Randomised, Maximal Subset Matching Algorithm for Document-Level Music Retrieval.",
        "author": [
            "Raphaël Clifford",
            "Manolis Christodoulakis",
            "Tim Crawford",
            "David Meredith 0001",
            "Geraint A. Wiggins"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1414878",
        "url": "https://doi.org/10.5281/zenodo.1414878",
        "ee": "https://zenodo.org/records/1414878/files/CliffordCCMW06.pdf",
        "abstract": "We present MSM, a new maximal subset matching algo- rithm, for MIR at score level with polyphonic texts and pat- terns. First, we argue that the problem MSM and its an- cestors, the SIA family of algorithms, solve is 3SUM-hard and, therefore, subquadratic solutions must involve approx- imation. MSM is such a solution; we describe it, and argue that, at O(n log n) time with no large constants, it is orders of magnitude more time-efficient than its closest competi- tor. We also evaluate MSM’s performance on a retrieval problem addressed by the OMRAS project, and show that it outperforms OMRAS on this task by a considerable margin. Keywords: Pattern matching, point set representation.",
        "zenodo_id": 1414878,
        "dblp_key": "conf/ismir/CliffordCCMW06",
        "keywords": [
            "MSM",
            "maximal subset matching algorithm",
            "MIR",
            "polyphonic texts",
            "patterns",
            "3SUM-hard",
            "subquadratic solutions",
            "approximation",
            "O(n log n)",
            "performance evaluation"
        ],
        "content": "A Fast, Randomised, Maximal Subset Matching Algorithm\nfor Document-Level Music Retrieval\nRapha¨el Clifford\nUniversity of Bristol, Merchant Venturers’ Building\nWoodland Road, Bristol BS81UB, UK\nclifford@compsci.bristol.ac.ukManolis Christodoulakis\nKing’s College, London\nThe Strand, London WC2R 2LS, UK\nmanolis@dcs.kcl.ac.uk\nTim Crawford, David Meredith, Geraint Wiggins\nGoldsmiths College, University of London\nNew Cross, London SE14 6NW, UK\n{t.crawford,d.meredith,g.wiggins }@gold.ac.uk\nAbstract\nWe present MSM, a new maximal subset matching algo-\nrithm,forMIRatscorelevelwithpolyphonictextsandpat-\nterns. First, we argue that the problem MSMand its an-\ncestors, the SIA family of algorithms, solve is 3SUM-hard\nand, therefore, subquadratic solutions must involve approx-\nimation.MSMis such a solution; we describe it, and argue\nthat, at O(nlogn)time with no large constants, it is orders\nof magnitude more time-efﬁcient than its closest competi-\ntor. We also evaluate MSM’s performance on a retrieval\nproblemaddressedbytheOMRASproject,andshowthatit\noutperformsOMRASonthistaskbyaconsiderablemargin.\nKeywords: Pattern matching, point set representation.\n1. Introduction\n1.1. Overview\nWe present MSM, a new fast, randomised maximal subset\nmatching algorithm which is applicable to MIR at the level\nof(quantised)scoreencodings. Thisisausefullevelofrep-\nresentationbecausethereisavastamountofmusicstoredin\nthisform(thoughnotyetdigitally),andbecausesuch“mid-\nlevel” representations constitute a helpful abstraction in un-\nderstanding perceived musical structure (Bello and Pick-\nens, 2005), and arise as output from, for example, MIDI\nrecorders and transcription systems. We currently focus on\nidealisedrepresentationscontainingonlyrestrictedformsof\nnoise; we will generalise this later.\nOuralgorithmﬁndsthelargestsubsetofapatternappear-\ning in a text at an arbitrary offset in any of the dimensions\nrepresented. In our representation, this means transposition\ninvariant and time offset tolerant matching.\nThe rest of this section explains why point-set represen-\ntationsarebetterfornote-levelMIRthanstring-basedones.\nSection 2 summarises related work in point-set matching\nand relevant aspects of the OMRAS project, against which\nPermissiontomakedigitalorhardcopiesofallorpartofthisworkfor\npersonal or classroom use is granted without fee provided that copies\narenotmadeordistributedforproﬁtorcommercialadvantageandthat\ncopies bear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2006 University of Victoriawe evaluate our results. Section 3 presents ﬁrst a proof that\nthe problem we solve is 3SUM-hard, and so is unlikely to\nhave an exact solution better than O(n2)in time, and then\na randomised, approximate O(nlogn)time solution. Sec-\ntion4presentscomparisons,withabaselineoftheOMRAS\nmatching method, and Ukkonen et al.’s (2003) P3, applied\nto query-by-example (i.e., large queries). Section 5 con-\ncludes and suggests future work.\n1.2. Point-set matching in music\nMost approaches to music pattern matching proposed to\ndate presuppose that the music is represented as a string of\nsymbolsorasetofsuchstrings. Typically,eachvoiceisrep-\nresentedasastringandeachsymbolwithineachstringrep-\nresents either a note or an interval between two consecutive\nnotes in a voice. Usually, the similarity between two pat-\nternsismeasuredby editdistance ,calculatedusingdynamic\nprogramming(Bellman,1957). Authorsusingthisapproach\ninclude Crawford et al. (1998), Lemstr ¨om (2000), Guo and\nSiegelmann (2004) and Cambouropoulos et al. (2005).\nFigure 1: Pattern B is perceived as an elaboration of pattern\nA,butthemusicaleditdistancebetweenthemis9,whichis\nlarge, being more than twicethe number of notes in A.\nHowever, there are several problems with this approach.\nConsider the patterns, A and B, in Figure 1. B is clearly\nperceived to be an embellished version of A. So we would\nwant a musical pattern matching algorithm to identify these\ntwo patterns as versions of the same motif. However, if the\ntwo patterns in Figure 1 are represented as strings in which\neachsymbolisanote,thentheeditdistancebetweenthemis\n9 because 9 notes need to be inserted into A to get B. Since\nAonlycontains4notes,aneditdistanceof9islarge. Butif\nwe allow A to match with patterns that differ from it by an\nedit distance of 9, we obtainmany spurious matches.\nAnother problem with string-based approaches to musi-\ncal pattern matching arises when we search in polyphonic\nmusic. Speciﬁcally, if we do not know the voice to whicheach note belongs or if we are interested in patterns con-\ntaining notes from two or more voices, then the number\nof passage-length strings required to represent the passage\nfully is exponential in the number of distinct onsets; if we\ndon’tsearch allofthesestrings,weriskmissingoccurrences\nof the query. This is illustrated in Figure 2, which shows a\ngraph of pitch against time representing the beginning of\nFr`ere Jacques . Each point in the ﬁgure represents a note\nPitchTime?????????????????????????1st part entry3rd part entry2nd part entry?????????\nFigure 2: Fully representing a passage of unvoiced poly-\nphonic music produces a combinatorial explosion.\nandallpairsofnotesthatcouldbeconsecutivewithinasin-\ngle string are linked by an arrow. The number of strings\nrequired is multiplied by keach time a note is followed by\nknotes that start simultaneously.\nThese problems are avoidable by representing the mu-\nsicgeometrically , as a set of points (or line segments) in\na multi-dimensional Euclidean space. A two-dimensional\nrepresentation has been used, in which one dimension rep-\nresents time and the other pitch. We follow this approach.\n2. Background\n2.1. Point-set pattern matching in music\nWe now review basic concepts of the geometrical approach\nand existing work on point-setpattern matching in music.\nLetvbe a vector and pa point in a k-dimensional Eu-\nclidean space. Then p+vdenotes the point that results\nwhen pistranslatedby v. Similarly,let Pbeasetofpoints\nandvbeavectorina k-dimensionalEuclideanspace. Then\nP+vdenotesthepointsetthatresultswhenallpointsin P\nare translated by v. That is, P+v={(p+v)|p∈P}.\nLetPandTbek-dimensional point sets and vbe a k-\ndimensional vector. Then Pistranslatable in Tbyviff\nP⊆TandP+v⊆T. Twopointsets, PandQ,aretrans-\nlationally equivalent iff there exists a vector, v, such that\nP+v=Q.Pisthe maximal translatable pattern (MTP)\ninTfor the vector, v, iffPis the largest point set that is\ntranslatable in Tbyv. In other words, the MTP in Tfor a\nvector vis{p|(p∈T)∧((p+v)∈T)}.\nThemaximal match for a pattern point set, P, in a text\npointset, T,foravector, v,istheorderedpair, /angbracketleftv, S/angbracketright,such\nthatScontains all the points in Pthat can be translated by\nthevector vtogivepointsin T. Thatis,themaximalmatch\nfor a query set, P, in a text set, T, for a vector, v, is/angbracketleftv, S/angbracketright\nsuch that S={p|(p∈P)∧((p+v)∈T)}.\nMeredith et al. (2002a) present SIA, an algorithm that\nﬁnds all non-empty MTPs in a k-dimensional point set of\nsizeninO(kn2log2n)time and O(kn2)space. Wig-\ngins et al. (2002) generalise SIA to a general patternmatching algorithm, SIA(M), which ﬁnds all maximal\nmatches of a k-dimensional pattern point set, P, of size\nm, in a k-dimensional text point set, T, of size n, in\nO(knm log2n)time and O(knm )space. They implement\nversionSIA(M)Ex,whichﬁndsallthe complete occurrences\nofapatternpointsetofsize minatextpointsetofsize nin\nO(knm )time, and version SIA(M)E which ﬁnds all com-\npleteand partial matches in O(knm log(nm))time.\nThe problem of ﬁnding the largest subset of Pwhich\noccurs in the data set under translation has also been stud-\nied. A na ¨ıveO(nmlogn)algorithm takes the differences\nbetween each point in the pattern and each in the data set,\nand sorts them (Wiggins et al., 2002); the algorithm can be\nreduced straightforwardly to O(nm)time, by using hashed\nstorage, but the bottleneck of space usage remains (Mered-\nith et al., 2002b). By observing that the differences from\nanygivenpointinthepatternarealreadyinsortedorder,the\nworking space overhead was reduced to O(m)by Ukkonen\netal.(2003). Althoughthetimecomplexityofthisapproach\nisO(nmlogm)it is the only practical method for solving\nlarge problems of which we are aware. Lubiw and Tanur\n(2004) also consider the matching problem in sets of hori-\nzontallinesegments. Theyuseweightfunctionstomeasure\nthe quality of a match rather than, e.g., the degree of over-\nlap between line segments in pattern and text as in Ukko-\nnen et al.’s (2003) problem P3. For example, Lubiw and\nTanur’s(2004)techniqueallowsforaconsonantintervalbe-\ntween a pattern note and a text note to be matched more\nstronglythanadissonantone. Theygiveanalgorithmwhich\nruns in O(nmlogm)time ( nandmas before) and show\nthat the problem is 3SUM-hard and so unlikely to have a\nsubquadratic solution.\nOtherimportantrelatedcontributionsarebyClausenand\nKurth (2002), who present a general framework and imple-\nmentations for searching for patterns in polyphonic sym-\nbolic and audio music data, and Typke et al. (2004), whose\nmethod searches for a polyphonic pattern in a database of\npolyphonicmusicrepresentedintheformofweightedpoint\nsets,similaritybetweenwhichismeasuredintermsoftrans-\nportation distances such asthe Earth Mover’s Distance.\n2.2. OMRAS\nTheOMRASproject( http://www.omras.org )designed\na polyphonic MIR method (Pickens et al., 2003) to capture\nthe general harmonic similarity between a musical query\nandmusicaldocumentssoughtinacollection. Thesimplest,\n0th-order OMRAS model is built by estimating the relative\nstrength of the members of a lexicon of chords, given the\nobservable notes in a window traversing the music, and by\nconsolidating the set of these partial observations (one for\neach window position) into an overall harmonic signature\nfor the piece, which, like each of the partial observations, is\na probability distribution over the lexical chords rather than\na single value. This process is applied to all the ﬁles in the\ncollection to be searched and the signature of each ﬁle is\nstored. Then, at search time, the signature of the query is\ncomputed along with a measure of the divergence betweenthe query and each ﬁle in the database. A list of ﬁle names\nis then returned with the ﬁles ranked by decreasing order of\ndivergence from the query.\nThe chord-lexicon used in most of the OMRAS work is\nthe24majorandminortriads. Toadmitapproximatematch-\ning between harmonic descriptions, it was necessary to rep-\nresenttheharmonicunfoldingofthemusicsothatnotesina\nquerydifferentfromthoseintheoriginaldocumentlendap-\npropriate weight to a matching function according to their\nharmonic distance from it. There is no music-theoretical\nway to judge the distance between all possible chords aris-\ning in the course of a piece of music, but there is such a\nmeasure for the set of 24 triads: Krumhansl and Shepard\n(1979) present the relative positions of the triads in a four-\ndimensional space derived from rigorous empirical percep-\ntual studies. The Euclidean distance between the triads in\nthis space forms the basis for constructing the model.\nAs well as this simple harmonic model, OMRAS uses\nhigher-order models to capture not just the local harmonic\ndescription of each window on the music, but also the har-\nmonic transitions between windows. The model grows ex-\nponentially with order, which severely curtails efﬁciency,\nthough it adds considerable richness to the model.\nThe OMRAS results reported in this paper use a param-\neter set that has been found to perform robustly: the data is\ngathered from MIDI ﬁles using a window of size 4 events\nand the OMRAS models are of the second order. We em-\nphasise that this method essentially performs approximate\nmatching for document-level retrieval, and was principally\ndesigned as a step towards bridging the gap between audio\nand symbolic representations of music (Bello and Pickens,\n2005).\n3. MSM\nWe now show that the problem of ﬁnding the largest sub-\nset match is 3SUM-hard. A problem is 3SUM-hard if it is\nat least as hard as the problem of determining if any three\nintegers in a set sum to 0(Gajentaan and Overmars, 1995,\nwhose notation we use). A quadratic time lower bound is\nconjectured for the time complexity of this class of prob-\nlems if exact solutions are required. We then present a ran-\ndomised algorithm which takes O(nlogn)time and so is\nable to beat the conjectured bound by giving approximately\ncorrect answers with high probability.\n3.1. Largest subset match is3SUM-hard\nFortwoproblems PR1 and PR2, PR1≪f(n)PR2means\nthatanyinstanceofPR1canbesolvedusingaconstantnum-\nber of instances of PR2 plus O(f(n))time. A problem PR\nis3SUM-hardif 3SUM≪f(n)PRwhere f(n)∈o(n2).\nOur proof is by reduction from E QDIST, a known 3SUM-\nhard problem (Barequet and Har-Peled, 2001).\nProblem 1. EQDIST(EQUALDISTANCE)1: Given two\nsetsAandBofnandm=O(n)integers, respectively,\n1This problem deﬁnition has been adapted from the work of Barequet\nand Har-Peled (2001), since the original considered real values.is there a pair a1,a2∈Aand a pair b1,b2∈Bsuch that\na1−a2=b1−b2?\nProblem 2. MAXSUBSET(MAXIMUM SUBSETMATCH):\nConsider two sets PandQofmandn=O(m)integers\nand an integer bound d. Consider also the largest subsets\nP/prime⊆PandQ/prime⊆Qsuch that for some vector v,P/prime+\nv=Q/prime. TheMAXSUBSETproblemistodeterminewhether\n|P/prime| ≥d.\nTheorem 1. EQDIST≪f(n)MAXSUBSET\nProof.Let(A, B )beaninstanceofE QDIST. Weclaimthat\n(A, B )is also an instance of M AXSUBSETwith the bound\nd= 2. Assume there is a solution to E QDIST. Therefore,\nthere exist a1,a2∈Aandb1,b2∈Bsuch that a1−a2=\nb1−b2. Letv=b1−a1, so now we have v+a1=b1and\nv+a2=b2and therefore the maximum subset match of A\nandBis at least 2.\nNow assume there is a solution to M AXSUBSET. It fol-\nlows that there exists a vector vsuch that for two subsets\nA/prime⊆AandB/prime⊆B,A/prime+v=B/primeand|A/prime| ≥2. There-\nfore, there exist two points a1, a2∈A/primeand two points\nb1, b2∈B/primesothat a1+v=b1anda2+v=b2. Therefore\nb1−b2=a1−a2as required.\nEQDISTis known to be 3SUM-hard (Barequet and Har-\nPeled, 2001) and so it follows immediately that M AXSUB-\nSETis also 3SUM-hard.\n3.2. How to overcome the complexity limit\nWenowintroducean O(nlogn)timerandomisedalgorithm\nthat calculates approximately the largest subset match. The\nidea is to reduce the problem of point set matching in ndi-\nmensions to binary wildcard matching in 1 dimension.\nDeﬁnition 1. Given k-dimensional point sets PandT, the\nsubset matching problem is to ﬁnd the largest subset of P\nwhich is a subset match in T.\nFurther, we deﬁne the hash functions g(x) =axmod q,\nh(x) =g(x) mod sandh2(x) = ( g(x) +q) mod sfor\nsome preselected values of a, qands.\nThe ﬁrst two steps ‘project and reduce length’ (Cardoze\nand Schulman, 1998; Cole and Hariharan, 2002):\n1. Random Projection in 1D To project PandTto one\ndimension, pick dintegers biuniformly at random from a\nlarge space. For each point, xinP∪Tcalculate/summationtextbixi,\nwhere xiisthe ithcoordinateof x. Calltheresultingsetsof\npoints in one dimension, P/primeandT/prime(Fig. 3(b)).\n2. Length Reduction (by hashing) Use universal hashing\nto reduce the sparsity of the data. Choose qto be a random\nprime in [2N, . . . , 4N], where Nis the maximum of the\nprojectedvaluesof PandT;auniformlyfrom [1, . . . , q −1];\nandsto be kn, where k > 1is a constant. Each non-zero\nlocation in P/primeis mapped to position h(P/prime\ni). Each non-zero\nlocationin T/primeismappedtofourpositions: h(T/prime\nj),h(T/prime\nj)+s,\nh2(T/prime\nj)andh2(T/prime\nj) +s. Calltheresultantarraysoflength sand2sresp.,pandt(Fig. 3(c)). The values at the mapped\npositions in pandtare all set to 1.\n3. BinaryWildcardMatching(usingFFTs) Setanyunde-\nﬁnedpositionin pto‘wildcard’. Setanyundeﬁnedposition\nintto0. Performbinarywildcardmatchingon pandt. The\nresult is the number of 1s that pandthave in common at\neach alignment position.\nLemma 1 justiﬁes using these hash functions for match-\ning. Its signiﬁcance is that if some subset of the pattern P/prime\nmatches in the text so that P/prime\ni+c=T/prime\njfor some cthen\nh(P/prime\ni) +h(c)matchesoneof h(T/prime\nj),h(T/prime\nj) +s,h2(T/prime\nj)and\nh2(T/prime\nj) +s.\nLemma 1.\nh(x) +h(y) =\n\nh(x+y)ifg(x) +g(y)< qand\nh(x) +h(y)< s,\nh(x+y) +sifg(x) +g(y)< qand\nh(x) +h(y)≥s,\nh2(x+y)ifg(x) +g(y)≥qand\nh(x) +h(y)< s\nh2(x+y) +sotherwise.\nProof.Itiseasytoshowthat h(x) +h(y) =h(x+y)when\ng(x) +g(y)< qandh(x) +h(y)< s. Now consider the\nsecond case, g(x) +g(y)< qandh(x) +h(y)≥s. Then\nh(x) +h(y) = g(x) mod s+g(y) mod s\n= (g(x) +g(y)) mod s+s\n= (axmod q+aymod q) mod s+s\n= (a(x+y) mod q) mod s+s\n=h(x+y) +s.\nWe can follow similar stepsto prove the other cases.\n606770\n1 2pitchonset time123\n4\n(a)2D (b)Random Projection to 1D182\n203\n212\n2051 2 4 3(1,60)=1.2+60.3=182\n(1,67)=1.2+67.3=203\n(1,70)=1.2+70.3=212\n(2,67)=2.2+67.3=205b1=2,b2=3\n(,)=.+. xx xbxb 12 11 22\n1\n2\n3\n4\nN =212,\n(200*182mod431)mod16=4\n(200*203mod431)mod16=61\n2(200*212mod431)mod16=2\n(200*205mod431)mod16=734\n(c)Length Reduction (by hashing)11 2 4 3\n2 3 4 5 6 7 8q     [424, 848] =  431 (prime), a     [1, 430] = 200, s = kn = 8 2 = 16.\nFigure 3: Illustration of themain steps of the procedure\nNow we estimate the size of the subset matches. At\neach alignment of pandtwe estimate the number of true\nmatches in the original data. By making some simple as-\nsumptions,wederiveanunbiasedestimateofthenumberoftrue matches at a given offset. However, this estimate may\nhave unacceptably large variance, so we make a further ob-\nservation, giving us a biased estimate which is much more\naccurateinpractice,forthetypeofmusicaldataweanalyse.\nOur improvement works because we can, with some ef-\nfort, ﬁnd at which offset in the original 2-dimensional data\nthe best match in the hashed and length reduced data ap-\npears. Havingfoundit,wecountthetruenumberofmatches\nat that point, thus entirely removing one source of error.\nThe initial, possibly inaccurate, estimate of the number of\nmatches at a given offset is not reported; only the order of\nthe estimates is used. To achieve a perfect ranking of the\ndocuments we now only require that the initial estimates be\nmonotonic in the true number of matches. Indeed, we need\neven less than this as only the largest subset match is con-\nsidered for any given pair of pieces.\nThe relative probability of introducing false positives\nduringthedimensionreductionphaseissmallenoughtouse\nP/primeandT/primeas the original data from here on. We can add a\nfurther stage of lookup to ﬁnd the true original points in P\nandTif need be. The outline algorithm runs thus:\n1. Findtheoffsetgivingthebestmatchof pandtandcallit\nh(c)(cis the currently unknown offsetin P/primeandT/prime).\n2. Findwhichpointsin pmatch tatoffset h(c)(O(m)time).\n3. Look up where the points that match in the hashed pat-\ntern,pwere mapped from. We must have stored a reverse\nmap from pitoP/prime\niwhen doing the original hashing.\n4. We want to avoid calculating h(c)for all c. We know\nP/prime\nifor each match pi, and that tj=h(P/prime\ni+c); so we can\nlook up T/prime\njin another reverse lookup table. Note that the\nfour images (Lemma 1) created by hashing T/primeare distinct.\nNowwehave c=T/prime\nj−P/prime\niforeachpairofpointsthatmatch\nin the hashed strings at a particular offset. If there are no\ncollisions in the hashing step of P/primeandT/primethen we have\nsolved the problem.\n5. However, collisions are possible, so look at all pjand\nmake an array of candidate offsets c. Choose the offset that\noccurs most frequently as the estimate of the true offset.\nThe running time is dominated by the time taken to do\nthe binary wildcard matching of pandt, which are both of\nlength s∈O(n). So the total running time is O(nlogn).\n4. Results and Evaluation\n4.1. Evaluation Method\nWe compared MSMwith the OMRAS system on a\ndocument-level retrieval task. OMRAS uses an approxi-\nmate matching method, designed to work with noisy data\n(in both pitch and time dimensions), whereas MSMis an\nexact-matching method suitable for score-like representa-\ntions; therefore the comparison is not exact, but we believe\nit convincingly shows the validity of our new approach.\nTo compare the two systems, we ran them on a test set\nof 2338 documents, of which 480 were used as queries.\nEach query was deﬁned prior to the experiment to be rel-\nevant to 39 other queries (and itself), thus partitioning the480 queries into 12 sets of 40 mutually relevant documents.\nEach of these sets was generated by artiﬁcially corrupting\none of Bach’s four-part chorale harmonizations by (1) in-\nserting 10, 20, 30, 40 or 50% extra notes at random into\nthe chorale (5 documents); (2) inserting 10, 20, 30, 40 or\n50%extranotessothateachinsertednotewasaperfectﬁfth\nawayfromoneofthenotesintheoriginal(5documents);(3)\ntransposing 10, 20, 30, 40 or 50% of the notes by a random\ninterval smaller than an octave (5 documents); (4) transpos-\ning 10, 20, 30, 40 or 50% of the notes by a perfect ﬁfth or\na perfect fourth (5 documents); (5) deleting 10, 20, 30, 40\nor 50% of the notes (5 documents); and (6) removing some\ncombination of voices (15 documents).\nApartfromtheseartiﬁciallycorrupteddocuments,which\ntestrobustness,alltheotherdocumentsinthedatabasewere\nderived automatically from the Musedata score encodings\n(Hewlett, 1997)2. The documents were normalised with\nrespecttotactus;theyweregeneratedfromscoreencodings,\nso they contained no expressivetiming.\nMSMand OMRAS were used to compute the similar-\nity between each query document and each database docu-\nment. Thedocumentswerethenrankedbydecreasingorder\nof similarity to the query. The rankings of the relevant doc-\numents for each query were then combined together to pro-\nduce an 11-point precision-recall curve for each algorithm.\nIn this task, the relevant documents for each query were\ncorrupted versions of the query, as above. This models a\nreal-world retrieval task of seeking possibly degraded ver-\nsions of a symbolic query in a database of symbolic mu-\nsic encodings (i.e., scores). The noise introduced into the\nrelevant documents here is typical of the pitch noise that\nmightbefoundinMIDIﬁlesderivedfromaudiorecordings\nor performed input. However, timing information was not\ndistorted at all, so robustness in this respect was not tested.\n4.2. Test Results\nFigure 4 shows the results of the tests with noisy queries,\ntwo tests per graph, comparing MSMresults and OMRAS\nresults for each test; in 4(a) and 4(b), the MSMresults for\neachpairoftasksarealmostidentical. Theoverallresultsof\nthe tests are shown in Figure 5. MSMconsistently outper-\nforms OMRAS on these tasks by a considerable margin.\n4.3. Execution speed of MSM\nFigure 6 shows the execution times ( y-axis) of Ukkonen\net al.’s (2003) P3 algorithm and MSM, applied to preﬁxes\nofvarioussizes( x-axis)ofBeethoven’s3rdSymphony,each\npreﬁxbeingcomparedwithitself;thisﬁndsthemaximalre-\npeated subset of the preﬁx. MSM’s subquadratic behaviour\nis clearly visible, and the timings show that no large con-\nstants are hidden in our estimates. Further, they show that\nMSMisfasterbybetweenoneandtwoordersofmagnitude,\nimproving with data size—the graph is plotted on log scale\nto allow the comparison to ﬁt on the page. The code was in\nC++, compiled under g++ 3.4 on an AMD 3000+ machine\n2http://www.musedata.org ; thanks to CCARH for their use.(a)\n11-pt Precision-Recall curve for Tasks 1 & 2\n00.10.20.30.40.50.60.70.80.91\n00.20.40.60.81RecallPrecision\nTask 1, OMRASTask 1, MSMTask 2, OMRASTask 2, MSM\n(b)\n11-pt Precision-Recall graph for Tasks 3 & 4\n00.10.20.30.40.50.60.70.80.91\n00.20.40.60.81RecallPrecisionTest 3, OMRASTest 3, MSMTest 4, OMRASTest 4, MSM\n(c)\n11-pt Precision-Recall Curve for Tasks 5 & 6\n00.10.20.30.40.50.60.70.80.91\n00.20.40.60.81RecallPrecision\nTest 5, OMRASTest 5, MSMTest 6, OMRASTest 6, MSM\nFigure 4: Precision/Recall graphs for the noisy queries; in\n(a) and (b), MSMresults for each pair are almost identical.\nOverall 11-pt Precision-Recall Curve\n00.10.20.30.40.50.60.70.80.91\n00.20.40.60.81RecallPrecision\nOMRASMSM\nFigure 5: Overall precision/recall graph for all tests.\nusing the fast Fourier library fftw3.1.1 (Frigo and Johnson,\n2005). No attempt at optimisation was made.Comparison between Ukkonen et al (2003) and MSM23.3842.62\n0.070.140.1972.442.019.770.310.040.010.1110100\n500010000150002000025000Number of notesRun time (seconds; log scale)Ukkonen et al (2003)MSMFigure 6: P3 vs.MSMlog-scale CPU time used to detect\nmaximalstructurerepetitioninBeethoven’s3rdSymphony.\n4.4. Discussion\nMSMis much faster than SIA(M)E, particularly for large\nqueries;themethodispracticablewhereSIA(M)Ewasgen-\nerally not. It outperforms Ukkonen et al.’s (2003) P3 al-\ngorithm, without loss in accuracy, though we note that this\nalgorithm was intended for small queries. On well-behaved\nscore data, its precision/recall results approach perfection,\nthough we concede that this is a specialist task in which\nsmall-scale noise is not an issue, which is not usually the\ncase in real-world data. In this special case, MSMsigniﬁ-\ncantly outperforms OMRAS.\n5. Conclusions and Future Work\nWe believe the new method is promising, but there is more\nto do, in rendering the method robust to the kind of noise\nthat is encountered in real-world music data. The technique\nis also applicable to line segment representations of music;\nthis requires further empirical work. Most important, this\nalgorithm is fast enough to admit non-indexed searching of\nlarge score databases for theﬁrst time.\nReferences\nG. Barequet and S. Har-Peled. Polygon containment and transla-\ntional min-hausdorff-distance between segment sets are 3sum-\nhard.Int. J. Comput. Geometry Appl. , 11(4):465–474, 2001.\nR. Bellman. Dynamic Programming . Princeton University Press,\n1957.\nJ. P. Bello and J. Pickens. A robust mid-level representation for\nharmonic content in music signals. In Proceedings of the 7th\nInternational Conference on Music Information Retrieval, IS-\nMIR 2005 . Queen Mary College, University of London, 2005.\nE.Cambouropoulos,M.Crochemore,C.Iliopoulos,M.Mohamed,\nand M.-F. Sagot. A pattern extraction algorithm for abstract\nmelodic representations that allow partial overlapping of inter-\nvalliccategories. In ProceedingsoftheSixthInternationalCon-\nference on Music Information Retrieval (ISMIR 2005, 11–15\nSeptember 2005, London) , 2005.\nD. E. Cardoze and L. J. Schulman. Pattern matching for spatial\npoint sets. In IEEE Symposium on Foundations of Computer\nScience, pages 156–165, 1998.\nM.ClausenandF.Kurth. Auniﬁedapproachtocontentbasedand\nfault tolerant music identiﬁcation. In International Conference\non Web Delivering of Music , Darmstadt, Germany, 2002.R. Cole and R. Hariharan. Verifying candidate matches in sparse\nand wildcard matching. In Proceedings of the Annual ACM\nSymposium on Theory of Computing , pages 592–601, 2002.\nT.Crawford,C.S.Iliopoulos,andR.Raman.Stringmatchingtech-\nniquesformusicalsimilarityandmelodicrecognition. Comput-\ning in Musicology , 11:73–100, 1998.\nM. Frigo and S. G. Johnson. The design and implementation of\nFFTW3. Proceedings of the IEEE , 93(2):216–231, 2005. spe-\ncial issue on ”Program Generation, Optimization, and Platform\nAdaptation”.\nA.GajentaanandM.H.Overmars.Onaclassof o(n2)problemsin\ncomputationalgeometry. ComputationalGeometry ,5:165–185,\n1995.\nA. Guo and H. Siegelmann. Time-warped longest common sub-\nsequence algorithm for music retrieval. In Proceedings of the\nFifth International Conference on Music Information Retrieval\n(ISMIR 2004, 10–15 October 2004, Barcelona) , 2004.\nW. B. Hewlett. MuseData : Multipurpose representation. In\nE. Selfridge-Field, editor, Beyond MIDI: The Handbook of Mu-\nsical Codes , pages 402–447. MIT Press, Cambridge, MA.,\n1997.\nC. L. Krumhansl and R. N. Shepard. Quantiﬁcation of the hierar-\nchy of tonal functions within a diatonic context. Journal of Ex-\nperimental Psychology: Human Perception and Performance ,\n5:579–594, 1979.\nK. Lemstr ¨om.String Matching Techniques for Music Retrieval .\nPhD thesis, University of Helsinki, Faculty of Science, Depart-\nment of Computer Science, 2000. Report A-2000-4.\nA. Lubiw and L. Tanur. Pattern matching in polyphonic music\nas a weighted geometric translation problem. In Fifth Interna-\ntional Conference on Music Information Retrieval, Barcelona\n(ISMIR 2004, 10–15 October 2004) , 2004.\nD.Meredith,K.Lemstr ¨om,andG.A.Wiggins. Algorithmsfordis-\ncovering repeated patterns in multidimensional representations\nof polyphonic music. Journal of New Music Research , 31(4):\n321–345, 2002a.\nD. Meredith, G. A. Wiggins, and K. Lemstr ¨om. Method of\npattern discovery, 2002b. PCT patent application number\nPCT/GB02/02430, UK patent application number 0211914.7.\nApplied for by City University, London and ﬁled on 23 May\n2002. (Priority date: 23 May 2001, draft available online at\nhttp://www.titanmusic.com/papers.html ).\nJ. Pickens, J. P. Bello, G. Monti, T. Crawford, M. Dovey, M. San-\ndler, and D. Byrd. Polyphonic score retrieval using polyphonic\naudio queries: A harmonic modeling approach. Journal of New\nMusic Research , 32(2):223–226, 2003.\nR.Typke,R.C.Veltkamp,andF.Wiering. Searchingnotatedpoly-\nphonic music using transportation distances. In Proceedings\nof the ACM Multimedia Conference (New York, October 2004) ,\npages 128–135, 2004.\nE. Ukkonen, K. Lemstr ¨om, and V. M ¨akinen. Geometric algo-\nrithms for transposition invariant content-based music retrieval.\nInProceedingsoftheFourthInternationalConferenceonMusic\nInformation Retrieval, Baltimore (ISMIR 2003, 26–30 October\n2003), 2003.\nG. A. Wiggins, K. Lemstr ¨om, and D. Meredith. SIA(M)ESE: An\nalgorithmfortranspositioninvariant,polyphonic,content-based\nmusicretrieval. In 3rdInternationalSymposiumonMusicInfor-\nmation Retrieval (ISMIR 2002), 13–17 September 2002 , pages\n283–284, IRCAM, Centre Pompidou, Paris, France., 2002."
    },
    {
        "title": "Realtime Multiple Pitch Observation using Sparse Non-negative Constraints.",
        "author": [
            "Arshia Cont"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1416770",
        "url": "https://doi.org/10.5281/zenodo.1416770",
        "ee": "https://zenodo.org/records/1416770/files/Cont06.pdf",
        "abstract": "In this paper we introduce a new approach for realtime mul- tiple pitch observation of musical instruments. The propo- sed algorithm is quite different from others in the literature both in its purpose and approach. It is destined not for conti- nuous multiple f0 recognition but rather for projection of the ongoing spectrum to learned pitch templates. The de- composition algorithm on the other hand, does not compro- mise signal processing models for pitches and consists of an algorithm for efficient decomposition of a spectrum using known pitch structures and based on sparse non-negative constraints. After introducing the algorithm along with eva- luations, a real-time implementation of the algorithm is pro- vided for free download for the MaxMSP realtime program- ming environment. Keywords: Multiple-pitch observation, Non-negative Ma- trix Factorization, Sparseness constraints, Machine Learning.",
        "zenodo_id": 1416770,
        "dblp_key": "conf/ismir/Cont06",
        "keywords": [
            "realtime",
            "multiple pitch observation",
            "musical instruments",
            "algorithm",
            "projection",
            "ongoing spectrum",
            "pitch templates",
            "decomposition algorithm",
            "signal processing models",
            "sparseness constraints"
        ],
        "content": "Realtime Multiple Pitch Observation using Sparse Non-negative Constraints\nArshia Cont\nIrcam, Realtime Applications Team, Paris. and\nCenter for Research in Computing and the Arts, UCSD, San Diego.\ncont@ircam.fr\nAbstract\nIn this paper we introduce a new approach for realtime mul-\ntiple pitch observation of musical instruments. The propo-\nsed algorithm is quite different from others in the literature\nboth in its purpose and approach. It is destined not for conti-\nnuous multiple f0recognition but rather for projection of\nthe ongoing spectrum to learned pitch templates. The de-\ncomposition algorithm on the other hand, does not compro-\nmise signal processing models for pitches and consists of an\nalgorithm for efﬁcient decomposition of a spectrum using\nknown pitch structures and based on sparse non-negative\nconstraints. After introducing the algorithm along with eva-\nluations, a real-time implementation of the algorithm is pro-\nvided for free download for the MaxMSP realtime program-\nming environment.\nKeywords: Multiple-pitch observation, Non-negative Ma-\ntrix Factorization, Sparseness constraints, Machine Learning.\n1. Introduction\nThe task of estimating multiple fundamental frequencies\nof audio and speech signals has attained substantial effort\nfrom the research community in the recent years. More in-\nterestingly, proposed algorithms in the literature undergo a\nwide variety of methods spanning from pure signal proces-\nsing models to machine learning methods. For an excellent\noverview of different methods for multiple- f0estimation,\nwe refer the curious reader to [1].\nIn this paper, we present an algorithm for realtime obser-\nvation of multiple pitches of polyphonic instruments. The\nproposed method uses machine learning techniques in its\ncore for realtime decomposition of ongoing audio using known\npitch templates of an instrument. It is thus different from\nmany algorithms in the sense that it does not provide the user\nwithf0computations but tells which of many (previously\nlearned) pitch templates are currently active for reconstruc-\ntion of the ongoing audio. During (one-time) learning, the\nsystem browses a library of instrumental sounds and learns\nspectral structures of the pitches that can be produced by the\ninstrument. In this sense, one can say, the algorithm learns\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies\nare not made or distributed for proﬁt or commercial advantage and that\ncopies bear this notice and the full citation on the ﬁrst page.\nc°2006 University of Victoriathe instrument pitch model that will be used during realtime\nobservation.\nThe algorithm presented here is based on a modiﬁed Non-\nnegative Matrix Factorization (NMF) algorithm introduced\noriginally by Lee and Seung [2]. The ﬁrst musical appli-\ncations of NMF are reported in [3, 4] for polyphonic mu-\nsic transcription and [5] for source separation. In all ap-\nproaches, the algorithm learns parts representations of the\naudio signal which correspond to music events. Despite their\nsigniﬁcant results, the algorithms are heavy in computation\nand non-realtime in nature. Recently, Sha and Saul have pro-\nposed a real-time pitch determination algorithm using NMF\nfor speech signals [6]. Our approach is quite similar in ar-\nchitecture to Sha and Saul but different by adding sparse-\nness constraints to a regular NMF (and thus changing the\nalgorithm used). Despite their success on speech databases,\ntheir algorithm would be far from success for music signals\nwhich undergo wider spectral characteristics than speech si-\ngnals.\nThe paper is organized as follows. In Section 2 we present\nthe general architecture for training and realtime observa-\ntions. This general architecture will be detailed in the fol-\nlowing sections on learning and multiple-pitch observation\nwith the proposed NMF. Speciﬁcally in Section 4 we detail\noursparse NMF algorithm which is used during realtime\nobservation followed by results and discussions of the algo-\nrithm.\n2. General Architecture\nThe proposed method relies on unsupervised learning al-\ngorithms that are used for knowledge representation and dis-\ncovery. During realtime observation, the algorithm tries to\nreconstruct the ongoing audio using previously learned pitch\nstructures of an instrument, as a linear combination with\nnon-negative weights. This implies an ofﬂine learning of\npitch structures of all the pitches of an instrument which will\nbe used as templates during learning. We will give details of\nthis learning phase in Section 3.\nThis architecture is similar to the system proposed in\n[6] with a crucial difference for music signals. Instead of\nusing a regular NMF algorithm for real-time determination\nof pitch, we use a modiﬁed NMF algorithm with sparseness\nconstraints as outlined casually below and detailed in Sec-\ntion 4. We compare results of the proposed algorithm with\nthat of [6] in section 5.2.1. Non-negative Matrix Factorization\nThe learning algorithm used in both learning and realtime\nobservation is based on Non-negative Matrix Factorization\n(NMF). Both algorithms will be presented mathematically\nin Sections 3 and 4 but like any machine learning algorithm,\nthis choice will bring advantages and limitations which are\npart of the general philosophy of the proposed method. Non-\nnegative matrix factorization is an unsupervised algorithm\nfor decomposition and learning for multivariate data [2].\nSpeaking generally, unsupervised learning algorithms such\nas principal component analysis and vector quantization can\nbe understood as factorizing a data matrix subject to dif-\nferent constraints. NMF in this respect is another factoriza-\ntion algorithm that uses nonnegativity constraint. Nonnega-\ntivity in this sense, means that an original matrix Vis com-\nposed of the desired number of templates (stored as columns\ninW) which can reconstruct the original by being added li-\nnearly with nonnegative weights (stored in H) orV¼WH .\nThe idea behind NMF algorithms is that the signal can be re-\nconstructed using its parts (W) through addition of the parts\nwith different weights, and thus the parts are not necessarily\nindependent. This fact seems to be consistent with how we\nhear and transcribe music chords through addition of single\npitches or identities we know a priori. However, this implies\ncareful considerations for signal representation and obser-\nvation as discussed below.\nIn our formulation of the problem of multiple pitch ob-\nservation, Vwould be the ongoing audio representation or\nthe result of the signal processing frontend of the system, W\nwould represent the pitch templates of an instrument, and H\nwould represent nonnegative weights corresponding to pre-\nsence of pitch templates in V.\n2.2. Signal Processing Frontend\nThe additive characteristic of NMF is an essential fac-\ntor for any kind of representation used for Vwhich, in the\ncase of multiple pitch observation, implies that the spectral\nrepresentation used for Vshould demonstrate a harmonic\nstack of pitch templates added together for a given chord.\nThe signal processing front end used for this observation\nis the result of a ﬁxed point analysis of frequency to instan-\ntaneous frequency mapping of the ongoing audio spectrum\n[7]. The short-time Fourier transform (STFT) is an efﬁcient\ntool for instantaneous frequency (IF) estimation [8]. Given\nz(!; t)as the analytical form of the STFT, the mapping\n¸(!; t) =@\n@targ[z(!; t)] (1)\ncan be computed efﬁciently and in real-time using STFTs\n[8] and the ﬁxed points of this mapping can be extracted\nusing the following criteria [7, 6] :\n¸(!¤; t) =!¤and@¸\n@!j!=!¤<1 (2)\n&\n?c\nc□....jœœœœbbb Œ Ó\n□.Jœ Œ Ó□.....jœœœœœb#bŒ Ó\n□ÓŒ ‰jœœœœb#b\nÓŒ ‰Jœwwww\nw\n.œ j\nFrame numberFrequency (Hz)Fixed−point Instantaneous Frequency Representation\n100 200 300 400 500 600 7005502\n4964\n4426\n3887\n3349\n2811\n2272\n1734\n1196\n657\n119FIG.1. Fixed Point Instantaneous Frequency Representa-\ntion of audio (bottom) corresponding to three chords (above)\nplayed on a piano\nAs a result, vector Vwould be non-negative amplitudes of\nthe ﬁxed-point instantaneous frequency representing harmo-\nnic stacks at each analysis frame with the rest of the spec-\ntrum zeroed out. Figure 1 shows a snapshot of this represen-\ntation on three given chords played on a real piano.\n2.3. Sparsity of the solution\nDespite perceptual advantages of an NMF approach over\nICA algorithms for multiple-pitch detection, since pitch tem-\nplates are not mathematically independent, for a given spec-\ntrum (in V) there may exist many possible solutions ( H)\nusing templates in W. More speciﬁcally for our problem, a\ngiven piano chord can be reconstructed by the templates of\nits original pitches as well as octaves, dominant and other\npitches with harmonic relations to the original ones.\nTo overcome this problem, we use the strong assumption\nthat the correct solution for a given spectrum (in V) uses a\nminimum of templates in W, or in other words, the solu-\ntion has the minimum number of non-zero elements in H.\nThis assumption is hard to be proofed for every music ins-\ntrument and highly depends on the template presentations\ninW, but is easily imaginable as harmonic structure of a\nmusic note can be minimally expressed (in the mean squa-\nred sense) using the original note than a combination of its\noctaves and dominant.\nFortunately, this assumption has been heavily studied in\nthe ﬁeld of sparse coding . The concept of ‘sparse coding’ re-\nfers to a representational scheme where only a few units out\nof a large population are effectively used to represent typicaldata vectors [9]. One of the useful properties of NMF is that\nit usually produces a sparse representation of the data. Ho-\nwever this sparseness is more of a side-effect than a goal and\none can not control the degree to which the representation is\nsparse.\n3. Learning Pitch Templates\nAs explained earlier, the system knows the pitch struc-\ntures of all pitches of an instrument for use during real-\ntime observation. In Section 4 we introduce the decompo-\nsition process or how to obtain Hand here we show how\nwe learn different pitch templates for an instrument. As a re-\nminder, Wcontains pitch structures of all pitches of a given\ninstrument. For example, for an acoustic piano, matrix W\nwould contain all 88pitches as 88different columns. To this\nend, training is done using databases of instrumental sounds\n[10, 11] and an off-line training learns different pitch struc-\ntures of an instrument by browsing all sounds produced by\nthe given instrument in the database and stores them in ma-\ntrixWfor future use.\nFor each audio ﬁle in the database, training is an itera-\ntive NMF algorithm with a symmetric kullback-leibler di-\nvergence for reconstruction error as shown in Equation 3,\nwhere ­is an element by element multiplication. In this\noff-line training Vwould be the short-time ﬁxed-point ins-\ntantaneous frequency spectrum of the whole audio ﬁle as\ndescribed in Section 2.2 and the learning algorithm facto-\nrizesVasV¼WH . In order to obtain precise and discri-\nminative templates, we put some constraints on Wvectors\nlearned during each NMF iteration. For each sound in the\ndatabase (or each pitch) we force the algorithm to decom-\npose Vinto two vectors ( Whas two columns) where we\nonly learn one vector and have the other ﬁxed as white non-\nnegative noise, where only the ﬁrst one would be stored for\nthe global W. This criteria helps the algorithm focus more\non the harmonic structure of V. Furthermore, we constrain\neach iteration by an envelope ( Env in equation 3). This en-\nvelope is constructed from the pitch information of the audio\nﬁle (usually taken from the name of the ﬁle in the database)\nand emphasizes frequencies around the fundamental with a\ndecreasing envelope towards the end and close to zero for\nfrequencies less than the fundamental. While this assump-\ntion does not hold for many instruments (such as violin and\npiano), it enforces the most important characteristic of the\nspectrum for pitch classiﬁcation. This constraint improves\ncommon octave and harmonic errors that can be introduced\nin pitch determination during realtime observation.\nHa¹Ã¡Env­Ha¹P\niWiaVi¹=(WH)i¹P\nkWka\nWiaÃ¡Env­WiaP\niHa¹Vi¹=(WH)i¹P\nºHaº(3)\nWhen the training reaches an acceptable stopping crite-ria, the harmonic spectra in the local Wwill be saved in the\nglobal Wand the algorithm continues to the next audio ﬁle\nin the database until it constructs Wfor all pitches of the\ninstrument.\n4. NMF for Multiple-pitch Observation\nAs stated in Section 2.3, in order to decompose the spec-\ntrum using learned pitch templates, the solution needs to be\nsparse. In this section, we introduce a modiﬁed sparse non-\nnegative decomposition algorithm useful for realtime pitch\nobservation.\nNumerous sparseness measures have been proposed and\nused in the literature. In general, these measures are map-\npings fromRntoRwhich quantify how much energy of a\nvector is packed into a few components. As argued in [12],\nthe choice of sparseness measure is not a minor detail but\nmay have far reaching implications on the structure of a\nsolution. Very recently, Hoyer has proposed an NMF with\nsparseness constraints by projecting results into `1and`2\nnorm-spaces [13]. Due to real-time considerations and the\nnature of sparseness in audio signals for pitch determina-\ntion we propose a modiﬁed version of NMF with sparseness\nconstraint of that in [13].\nThe deﬁnition commonly given for sparseness is based\non the `0norm deﬁned as the number of non-zero elements\nkXk0=#fj; xj6= 0g\nN\nwhere Nis the dimension of vector X. It is characteristic for\nthe`0norm that the magnitude of non-zero elements is igno-\nred. Moreover, this measure is only good for noiseless cases\nand adding a very small measurement noise makes comple-\ntely sparse data completely non-sparse. A common way to\ntake the noise into account is to use the `²norm deﬁned as\nfollows :\nkXk0;²=#fj;jxjj ¸²g\nN\nwhere parameter ²depends on the noise variance. In prac-\ntice, there is no known way of determining this noise va-\nriance which is independent of the variance in x. Another\nproblem of this norm is that it is non-differentiable and thus\ncan not be optimized with gradient methods. A solution is\nto approximate the `²norm by tanh function,\ng(x) =tanh(jaxjb)\nwhere aandbare positive constants. In order to imitate `²\nnorm, the value of bmust be greater than 1.\nIn addition to the tanh norm, we force an `2constraint\non the signal. This second constraint is crucial for the nor-\nmalization of the results and emphasis on signiﬁcance of\nfactorization during note events in contrary to silent states.\nIn summary, the sparseness measure proposed is based\non the relationship between the `²norm and the `2norm as\ndemonstrated in Equation 4.sparseness(x) =p\nN¡Ptanh(jxij2)=pPx2\nip\nN¡1(4)\nFor NMF with sparseness constraint, we use gradient des-\ncent updates instead of the original NMF multiplicative up-\ndates (Equation 3) and project each vector in real-time to be\nnon-negative and have desired `2and`²norms. This pro-\njected gradient descent, adapted from [13], is outlined be-\nlow. Once again this algorithm shows the factorization for\nHwhen templates are known.\nGiven VandW, ﬁnd the non-negative vector Hwith a given\n`²norm and `2norm :\n1.Initialize Hto random positive matrices\n2.Iterate\n(a)SetH=H¡¹HWT(WH¡V)\n(b)Setsi=hi+ (`²¡Ptanh(h2\ni))=N\nandmi=`²=N\n(c)Sets=m+®(s¡m)where\n®=¡(s¡m)Tm+p\n((s¡m)Tm)2¡P(s¡m)2(Pm2¡`2\n2)P(s¡m)2\n(d)Set negative components of sto zero\nand set H=s\nAlgorithm 1. Sparse Non-Negative Matrix Decomposition\nWhere (a) is a negative gradient descent and (b) through (d)\nare the projection process on the `²and`2space. In (b) we\nare projecting the vector to the `²hyperplane and (c) solves\na quadratic equation ensuring that the projection has the de-\nsired `2norm.\nFor realtime pitch observation, the `2norm is provided\nby the spectrum energy of the realtime signal and the `²\ntakes values between 0and1, is user-speciﬁed and can be\ncontrolled dynamically. The higher the `², the more sparse\nis the solution in H.Vwould be a vector of size nwhere\nhere we use n= 512 for an FFT window of 93msec to cap-\nture harmonic structure up to about 6KHz . Equivalently,\nWwould be a matrix of n£mwithmas the number of\ntemplates and Hwould be a vector of size m.\n5. Results and Evaluation\nIn this section we evaluate sparsity of the solution and\npitch observation of the proposed algorithm.\n5.1. Sparsity\nWe start demonstrating the results by emphasizing on the\nsparsity factor of the proposed algorithm. Figure 2 com-\npares two instances of NMF pitch observation on three piano\nchords represented in Figure 2(a). Results on Figure 2(b)\npurport to a regular NMF algorithm (learning only Hwith\nknown Wor pitch templates) as proposed in [6] using regu-\nlar NMF for speech signals and Figure 2(c) corresponds toresults obtained using the algorithm presented in the pre-\nvious section. Both ﬁgures show time (or analysis frame\nnumber) on the x-axis and the contributions of each pitch\ntemplate (indexed on y-axis by their names) are shown in\nthe ﬁgure by a colormap representing an interval between\n0and1. The analysis is done by using a window size of\n93msec , overlap of 12msec and ﬁxed `²of0:8.\n&\n?c\nc□....jœœœœbbb Œ Ó\n□.Jœ Œ Ó□.....jœœœœœb#bŒ Ó\n□ÓŒ ‰jœœœœb#b\nÓŒ ‰Jœwwww\nw\n.œ j\n(a) Piano Score being played\nchord test without sparseness constraint\nframe numberPitch Templates\n100 200 300 400 500 600 700 800B0Db1Eb1F1G1A1B1Db2Eb2F2G2A2B2Db3Eb3F3G3Bb3C4D4E4Gb4Ab4Bb4C5D5E5Gb5Ab5Bb5C6D6E6Gb6Ab6Bb6C7D7E7G7A7B7noise\n(b) Decomposition using regular NMF\nchord test with sparseness constraint\nframe numberPitch Templates\n100200300400500600700800B0Db1Eb1F1G1A1B1Db2Eb2F2G2A2B2Db3Eb3.F3G3Bb3C4D4E4Gb4Ab4Bb4C5D5E5Gb5Ab5Bb5C6D6E6Gb6Ab6Bb6C7D7E7G7A7B7noise\n(c) Decomposition using sparse NMF\nFIG.2. Comparing sparse and non-sparse non-negative de-\ncomposition for pitch observation\nAn important remark is the presence of an additional noise\ntemplate in W(bottom rows of Figure 2(b) and 2(c)). This\ntemplate is responsible for absorbing eventual noise, rever-\nberation and non-harmonic structures such as transients thatcan not be decomposed using pitch templates of the instru-\nment. As is seen in Figure 2(c), this noise template has im-\nportant presence (specially during transitions) in the sparse\nsolution and assures generalization and robustness of the al-\ngorithm.\nComparing Figures 2(b) and 2(c), the sparsity of the se-\ncond is quite evident. The sparse NMF learns a solution\nthat emphasizes fewtemplates where the regular NMF (Fi-\ngure 2(b)) uses a large number of templates (typically tem-\nplates of harmonic relationship to the original pitch depicted\nin the score of Figure 2(a)) for the solution.\nFinally, it is worthy to note that the example shown in\nFigure 2 is the result of playing the score in Figure 2(a) on a\n(real) Piano different than the one used for learning the pitch\ntemplates. Moreover, the sustain pedal of the piano has been\npressed down during the whole performance of the score,\nadding sustained resonance throughout the whole spectrum.\n5.2. Evaluating the observation\nA close examination of results in Figure 2(c) along with\nthe score in Figure 2(a) reveals that the corresponding tem-\nplates of the notes in the score are along the most active\nones at the appropriate time. However, we prefer evaluating\nthe algorithm and its robustness on a larger corpus of mu-\nsic. For this purpose, we run the algorithm on three classical\nmusic pieces which are aligned to their MIDI scores using\nan external application described in [14]. The audio is taken\nfrom the RWC database [15] and the pieces and their speci-\nﬁcations are given in Table 1. Pieces 1and2were performed\non a Piano and piece 3is performed on a Harpsichord. Ac-\ncordingly, pitch templates of appropriate instruments will\nbe used during evaluation. Note that by doing this, we are\nevaluating the system in a transcription framework, even-\nthough the proposed system does no undertake transcription\nbecause of lack of temporal smoothing. In previous state-\nof-the-art evaluation, authors in [16] construct the reference\nby aligning the MIDI score to their results using dynamic\nprogramming. In our evaluation as mentioned, we use an\nexternally aligned score to the audio.\nTAB.1. Speciﬁcation of Audio and Midi used for evaluation\n# Piece Name Duration Events\n1 Mozart’s Piano Sonata in A major,K.331 9 :55 4268\n2 Chopin’s Nocturne no.2 in Eb major, opus 9 3 :57 1291\n3 Bach’s Fugue no.2 in C minor BWV 847 1 :53 752\nFor this evaluation, we compare results of the proposed\nalgorithm on the audio with the aligned MIDI score. Speciﬁ-\ncally, for each note event in the aligned score, we look at the\ncorresponding frames of the sparse NMF observation and\ncheck if the corresponding template has high activity and\nif it is among the top Ntemplates, where Nspeciﬁes the\nnumber of pitches active at the event frame time taken out\nof the reference MIDI. This way, for each event in the score\nwe can have a precision percentage and the overall meanof these can represent the algorithm’s precision ( precision\n1). Moreover, since we do not have any speciﬁc temporal\nmodel (for note-offs for example) we can consider (subjec-\ntively) positive detection during atleast 80% of a note life\nto be acceptable and recalculate the precision ( precision 2).\nThe results of this evaluation are shown in Table 2 for both\nthesparse algorithm proposed in this paper and the regular\n(non-sparse ) NMF proposed in [6].\nTAB.2. Multiple-pitch observation evaluation results\nPiece No. Precision 1 Precision 2\nSparse Non-sparse Sparse Non-Sparse\n1 78:1% 49 :6% 88 :0% 68 :1%\n2 71:0% 32 :7% 81 :2% 51 :2%\n3 74:9% 43 :1% 87 :1% 59 :4%\nBesides the evident gain of the sparse over non-sparse\nalgorithm, there is a downward shift in the results for the\nsecond piece in Table 2. This is mostly due to the fact that\nfor the performance of (Chopin’s Nocturne ), pianists always\nuse the sustain pedal of the piano excessively, thus adding\nmore and more sustained resonance of previous pitches in\nthe spectrum. One reason for providing precision 2 is that in\na multiple-pitch situation (and specially in the presence of\nthe sustain pedal for piano) the duration of each note event\nbecomes intractable or inexact. It should be mentioned that\nthe references used are also erroneous especially with the ti-\nming of events and further evaluations need hand-correction\nof these references to the audio.\nFinally, note that the templates were trained on a different\npiano than the one used for evaluation. The sounds used are\nprofessional recordings and ofcourse, in a realtime situation,\nthe recording microphone would be placed in a closer loca-\ntion to the piano. However, the obtained results reveal the\nsomehow surprisingly robust and generalized result of lear-\nning.\n6. Realtime Implementation\nThe proposed algorithm has been developed and tested\nfor MaxMSP1realtime programming environment and using\nthe FTM library2and is available for free download at :\nhttp://crca.ucsd.edu/arshia/ismir06/\nFigure 3 shows a screenshot of this implementation.\n7. Conclusion\nIn this paper, we proposed a realtime multiple pitch ob-\nservation algorithm based on sparse non-negative constraints.\nThe algorithm is different from most algorithms in the sense\nthat it knows the pitch templates of the instrument in ad-\nvance and through an unsupervised learning process as des-\ncribed. Thanks to sparseness constraints it correctly observes\n1http://www.cycling74.com/\n2http://www.ircam.fr/ftm/FIG.3. Sparse Non-negative Multiple-Pitch Observation in ac-\ntion, in MaxMSP environment and using FTM libraries\nongoing pitches. We evaluated the algorithm using (exter-\nnally) aligned audio to score as reference. The evaluation re-\nsults in Table 2 are close enough to state-of-the-art multiple\npitch algorithms with the huge difference that the proposed\nalgorithm in this paper is destined to work in realtime. We\nare currently evaluating the algorithm on larger databases\nand more complicated musical situations.\nPerhaps a major drawback of the described model des-\ncribed is the stationary pitch template model rather than a\nmoving spectrum with instrument envelopes. One should\nnote that by this outcome, we gain better generalization and\nrealtime capabilities. Finally, the proposed algorithm can be\nused along in various applications as a multiple-pitch obser-\nvation module. One instance of such application is reported\nin [17] where the proposed algorithm is used in the context\nof realtime polyphonic score following.\n8. Acknowledgments\nThe realtime implementation of the proposed algorithm\nwould never see the light of day without great help and effort\nof the following individuals : R ´emy Muller for his help with\nMaxMSP and FTM externals, Olivier Pasquet and Norbert\nSchnell for their help and availability for MaxMSP patching.\nAlso, great thanks to Chungsin Yeh for providing the aligned\ndatabase and useful discussions during evaluations.References\n[1]A. de Cheveign ´e, “Multiple f0 estimation,” in Computa-\ntional Auditory Scene Analysis : Principles, Algorithms and\nApplications , D.-L. Wang and G.J. Brown, Eds. IEEE Press\n/ Wiley, 2006 (in press).\n[2]Daniel D. Lee and H. Sebastian Seung, “Algorithms for non-\nnegative matrix factorization,” in Advances in Neural Infor-\nmation Processing Systems 13 , Todd K. Leen, Thomas G.\nDietterich, and V olker Tresp, Eds. 2001, pp. 556–562, MIT\nPress.\n[3]P. Smaragdis and J. Brown, “Non-negative matrix factoriza-\ntion for polyphonic music transcription,” 2003.\n[4]Samer M. Abdallah and Mark D. Plumbley, “Polyphonic\ntranscription by non-negative sparse coding of power spec-\ntra.,” in ISMIR , 2004.\n[5]Tuomas Virtanen, “Separation of sound sources by convolu-\ntive sparse coding,” 2004.\n[6]Fei Sha and Lawrence Saul, “Real-time pitch determination\nof one or more voices by nonnegative matrix factorization,”\ninAdvances in Neural Information Processing Systems 17 ,\nLawrence K. Saul, Yair Weiss, and L ´eon Bottou, Eds. MIT\nPress, Cambridge, MA, 2005.\n[7]Hideki Kawahara, H. Katayose, Alain de Cheveign ´e, and\nR.D. Patterson, “Fixed point analysis of frequency to instan-\ntaneous frequency mapping for accurate estimation of f0 and\nperiodicity,” in Eurospeech , 1999, vol. 6, pp. 2781–2784.\n[8]Toshihiko Abe, Takao Kobayashi, and Satoshi Imai, “Har-\nmonic tracking and pitch extraction based on instantaneous\nfrequency,” in IEEE ICASSP . 1995, pp. 756–759, Tokyo.\n[9]D. J. Field, Neural Computation , vol. 6, chapter What is the\ngoal of sensory coding ?, pp. 559–601, 1994.\n[10] Lawrence Fritts, “Musical instrument samples from the uni-\nversity of iowa electronic music studios,” Webpage : http:\n//theremin.music.uiowa.edu/ , 1997.\n[11] Guillaume Ballet, Riccardo Borghesi, Peter Hoffmann, and\nFabien L ´evy, “Studio online 3.0 : An internet ”killer appli-\ncation” for remote access to ircam sounds and processing\ntools,” in Journ ´ee d’Informatique Musicale (JIM) , paris,\n1999.\n[12] Juha Karvanen and Andrzej Cichocki, “Measuring sparse-\nness of noisy signals,” in ICA2003 , 2003.\n[13] Patrik O. Hoyer, “Non-negative matrix factorization with\nsparseness constraints.,” Journal of Machine Learning Re-\nsearch , vol. 5, pp. 1457–1469, 2004.\n[14] Hagen Kaprykowsky and Xavier Rodet, “Globally optimal\nshort-time dynamic time warping application to score to au-\ndio alignment,” in IEEE ICASSP . May 2006, Toulouse.\n[15] Masataka Goto, Hiroki Hashiguchi, Takuichi Nishimura, and\nRyuichi Oka, “Rwc music database : Popular, classical and\njazz music databases.,” in ISMIR , 2002.\n[16] Hirokazu Kameoka, Takuya Nishimoto, and Shigeki Sa-\ngayama, “Separation of harmonic structures based on\ntied gaussian mixture model and information criterion for\nconcurrent sounds,” in IEEE ICASSP . March 2005, Phila-\ndelphia.\n[17] Arshia Cont, “Realtime audio to score alignment for\npolyphonic music instruments using sparse non-negative\nconstraints and hierarchical hmms,” in IEEE ICASSP . May\n2006, Toulouse."
    },
    {
        "title": "Moody Tunes: The Rockanango Project.",
        "author": [
            "Nik Corthaut",
            "Sten Govaerts",
            "Erik Duval"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1418211",
        "url": "https://doi.org/10.5281/zenodo.1418211",
        "ee": "https://zenodo.org/records/1418211/files/CorthautGD06.pdf",
        "abstract": "Wouldn\u0001t it be nice if we had a tool that could offer peo- ple the right music for a specific time and place? For HORECA (hotel, restaurants and cafés) businesses, provid- ing appropriate music is often not just nice, but essential. Typically this boils down to music that matches a certain situation on desired atmospheres, this will be defined as a musical context (MC). The developed tool, a music player, meeting the specific needs of HORECA, allows creation and management of those contexts. The user creates a mu- sical context by selecting a number of appropriate atmos- pheres and can fine-tune the context with additional musi- cal properties. The atmospheres are defined by a group of music experts, composed of DJ\u0001s, music teachers, musi- cians, etc., who also manually annotate the properties of all musical content. To assist the music experts, a specially developed tool allows them to categorise and annotate the songs and evaluate their results. We provide insight on how we constructed and implemented our metadata schema and look at some existing schemas. The evaluation shows the economic value of such a system in the specific context of a HORECA business. Keywords: Multimedia Systems, Music Information Re- trieval, Context, Metadata.",
        "zenodo_id": 1418211,
        "dblp_key": "conf/ismir/CorthautGD06",
        "keywords": [
            "tool",
            "music",
            "HORECA",
            "appropriate music",
            "musical context",
            "music player",
            "musical properties",
            "atmospheres",
            "music experts",
            "metadata schema"
        ],
        "content": "Moody Tunes: The Rockanango Project \nNik Corthaut, Sten Govaerts, Erik Duval \nK.U. Leuven \nCelestijnenlaan 200A \nB-3001 Heverlee \n{Nik.Corthaut,Sten.Govaerts,Erik.Duval}@cs.kuleuven.be \nAbstract \nWouldn\u0001t it be nice if we had a tool that could offer peo-\nple the right music for a specific time and place? For \nHORECA (hotel, restaurants and cafés) businesses, provid-ing appropriate music is often not just nice, but essential. Typically this boils down to music that matches a certain situation on desired atmospheres, this will be defined as a \nmusical context (MC). The developed tool, a music player, \nmeeting the specific needs of HORECA, allows creation and management of those contexts. The user creates a mu-sical context by selecting a number of appropriate atmos-pheres and can fine-tune the context with additional musi-cal properties. The atmospheres are defined by a group of \nmusic experts, composed of DJ \u0001s, music teachers, musi-\ncians, etc., who also manually annotate the properties of all \nmusical content. To assist the music experts, a specially developed tool allows them to categorise and annotate the songs and evaluate their results. We provide insight on how we constructed and implemented our metadata schema and look at some existing schemas. The evaluation shows the economic value of such a system in the specific context of a HORECA business. \nKeywords : Multimedia Systems, Music Information Re-\ntrieval, Context, Metadata. \n1. Introduction \nWouldn\u0001t it be nice if we had a tool that could offer peo-\nple the right music for a specific time and place? For \nHORECA businesses, providing appropriate music is often not just nice, but essential. Automating this process re-quires means to describe music and to select music that corresponds to a given description that mirrors the context. \nMoreover, individual pieces of music must be combined \ninto a playlist and automatically rendered in a nice mix. This way the user can focus on his core task: bartending, rather than acting as a DJ. This is the focus of the Rocka-nango project (rockanango.cs.kuleuven.be), an 18 month cooperation between the K.U.Leuven (www.kuleuven.be), a Belgian music content provider for HORECA, Aristo Music BVBA (www.aristomusic.com) and with the finan-\ncial support of IWT-Vlaanderen (www.iwt.be). \nFor the music recommendation part, legacy systems are \nroughly based on two strategies. In collaborative filtering, the musical taste of a specific user is matched against that of all other users. The assumption is that the usage patterns of similar users lead to useful recommendations. This ap-\nproach requires some degree of feedback. This can be very \nsimple, in a 'do you like this?' style or more indirect, by tracking user behaviour. Audioscrobbler (www.audioscrobbler.net) as used by Last.FM (www.last.fm) and Amarok (amarok.kde.org) rely on this technique. \nA more common strategy for music recommendation is \ncontent based filtering. In this approach, the actual music and metadata are processed to determine patterns and cate-gorizations. MusicLens (www.musiclens.de), MusicMiner (musicminer.sourceforge.net) and the Music Genome Pro-\nject (www.pandora.com) rely on this approach. The meta-\ndata is provided by feature extraction and manual annota-tion of the music, either done by professional experts or by a community of users. \nConsumer applications like Pandora or Last.FM rely on \ncontinuously evolving personal music profiles. Bootstrap-ping proceeds in a similar way in both systems. First, the user is prompted for an artist or song he likes and then an audio stream with similar songs is created. The user cannot intervene directly in the music selection. However, the user can indicate whether he likes the music or not, which is used to fine-tune the profile and thus taken into account for \nfuture recommendations. The emphasis is on finding more \nmusic that the user likes. In this way, he can expand and explore his musical boundaries. \nUnlike the abovementioned approaches, we focus on \ncommercial use of music in public places, like pubs, res-taurants, hotels, etc. In our context, the customer popula-tion changes throughout the day and the week. As an ex-ample, customers may want to dance in the evening. At other times, a thematic music selection is more appropri-ate, for instance when there is a big sports manifestation, a Latin night, a disco evening, etc. The bartender needs to \nadapt the music to certain types of clientele and situations, \nimmediately, without the converging process associated with profile fine-tuning.  \nIn our approach, songs are annotated with roughly \ntwenty metadata fields and MCs are created to define the \nPermission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fee provided that copies \nare not made or distributed for profit or commercial advantage and that \ncopies bear this notice and the full citation on the first page. \n© 2006 University of Victoria relevant music, by selecting values for the different meta-\ndata fields. In this way, full control is in the hands of the context creator. Music experts have created a set of default contexts for different occasions. Bartenders can fine-tune these defaults or create contexts of their own. After the context creation, a playlist can be generated that covers a \nspecific timespan. The user preserves control over the \nplaylist. He can add or remove tracks, change the order, ... After all, it might not be a good idea to play the soccer hymn of the adversary in the club's canteen.  \nThis paper is structured as follows: In the second sec-\ntion we define and explain the concept of a MC. In the next two sections we determine what information is useful to model MCs and what we used. We compare our ap-proach with the work of others. The fifth section deals with the implementation of the system and the final 2 we try to validate the system and give a road map for the future. \n2. What is a Musical Context? \nA personal music collection typically contains a few thou-\nsands songs. Manually picking songs from such a collec-tion in order to create a MC is a tedious task that demands \nexpertise and skill. In most tools, the user must know the \nsongs by e.g. artist to find the song. Moreover, this manual approach limits the ability to discover new music, since the selector has to know his music collection pretty well. \nWith a description of the music we want for a certain \ncontext, we make it possible for the computer to select the appropriate music and propose a playlist. Different musical parameters, like genre, rhythm, mood, occasion, year, dance style, etc., describe the music selection for a context. For the dance evening we want for example: \n• Genre:  Dance \n• Mood: Cheerful, Exuberant, Delirious, Stimulating.  \n• Popularity: medium to very high \n• Party level:  medium to very high. \n• Loudness:  medium to high \nBy selecting the music that satisfies the description multi-ple playlists with a specified time span can be generated without having to know the entire music collection. In or-der to be able to select songs for a MC, we need a detailed description of each song in the collection. Through experi-ence and knowledge, a musical expert knows in which situation he can play a song. The expert has the skills to \nuse appropriate songs for a situation with a certain atmos-\nphere. They actually assign possible atmospheres to the different songs. An example: the description of the atmos-phere for a song used for a slow dance between amorous couples could be: romantic, emotional, intimate, etc.  \nThese atmospheres can be recalled if the end user thinks \nthey are appropriate for his specific situation. Basically, selected atmospheres are reused in a different (musical) context and through that, they evoke the atmospheres of the situation they were derived from. These MCs can then be further refined for personal taste with other musical properties, like genre, rhythm, instrument, etc. In conclu-sion, a musical context is a musical description for situa-\ntions based on atmospheres and musical properties. \n3. Modeling of Metadata \nAs discussed before, we use metadata to describe a MC. \nThese include the author and the title of the song, the group members of the band, the record label and the genre of the song. However, such basic information is not suffi-cient when we want to generalise and determine whether the music is good for partying, for a candlelight dinner, for \nchildren or for a special occasion. \n3.1 What metadata? \n3.1.1 Objective and subjective metadata \nMany of the metadata elements define characteristics \nwhich are objective, such as the song author and title, art-ists, the release year, the version, ...  \nOf course, there are a lot more musical characteristics, \nwhich tend to be based on personal perspective or differing points of view, hence more subjective [1]. A few examples are popularity, mood, genre, hardness (metal vs. lounge), etc.  \n3.1.2 An extraction based categorisation \nBased on the way the metadata is extracted, we can catego-\nrise musical metadata in different groups [2][3]: \n• Editorial metadata:  authoritative experts provide the \nmetadata manually.  \n• Acoustic metadata:  the information (usually purely \nobjective) is obtained by analysis of the music file, \nwithout any reference to textual or prescribed infor-mation. \n• Cultural metadata:  the metadata results from an \nanalysis of emerging patterns, categories or associa-tions from a source of documents produced by the environment or culture. \n3.2 Other metadata schema’s \nSongs can be categorized in many ways, however com-\nparison of different metadata schemas of existing systems learns that they all incorporate the same basic editorial metadata for a song. We will discard this metadata for now. We are interested in the metadata that describes the musical features of a song and subjective metadata like mood and purpose. \nIn table 1, we collected these metadata elements for 8 \nsystems. We weren't always able to obtain detailed infor-\nmation about the systems mentioned here, and basically \nrelied on [4] and [5]. All of the systems are used for music recommendation, although most people use the All Music Guide mainly as a music encyclopedia. The PATS system [6] is focused on Jazz Music, so it contains a lot of specific metadata for this kind of music, e.g. the ensemble strength, the soloists and the geographical roots. We don't really need this for HORECA. People seem to use genre \u0001 as the main categorizer. All \nsystems have it; some use only that apart from the basic \nmetadata, e.g. Magnatune. We need it too for MCs. \nAnother interesting data element, which reoccurs in the \nother systems, is mood \u0002. AllMusic, Moodlogic and \nStreamMan use a value space for the mood description. \nMusicLens uses an interval between positive feelings and negative, as does MoodLogic on top of a mood descrip-tion. As abovementioned, this is what we will use too to describe musical contexts. StreamMan and MusicLens take \nthis a bit further by enabling to describe the purpose \u0003 \n(ranging from listening to dance). We embedded this in our \nsystem in the dancability data element. This is an essential factor in the HORECA. \nMusicLens also takes the listener into account by select-\ning music based on the listener's sex and age. When there are a lot of people present, describing the listeners be-comes very hard. We put the stress on describing the mu-sic, not the people. \nThere are more similar elements, like tempo \u0004, instru-\nments \u0006, similar artists \u0005, ... MoodLogic features other \nelements that can be very useful to describe the desired \nmusic, e.g. language, the energy of a song, the topic of the lyrics, ... All Music Guide recommends music based on similar artists and an influence network between artists, we however believe that we should work on song level for recommendations. \n3.3 Incremental approach \n3.3.1 Why? \nA lot of the objective metadata can be retrieved from exist-\ning databases, like FreeDB (www.freedb.org). Other objec-tive metadata fields, like for example the duration, the beats per minute (BPM), the intro end and the beginning of the outro, can be calculated by the computer (www.un4seen.com/bass.html) [7]. Subjective metadata is \ntypically hard to determine by computer. Collaborative filtering techniques are an alternative, but require feedback and a large user population. A pub owner is typically too pre-occupied to give feedback to the system. \nBecause we don't have the critical mass to converge to \nvalid metadata, we decided to annotate the music manually \na priori. Later on, when we have a solid metadatabase, we can automate annotation with feature extraction techniques for data member estimation. \nIn the beginning, we had a very long list of possible \nmetadata elements, but it was not clear what metadata ele-ments were needed or useful to describe a MC. Some mu-sical properties that are subject to personal taste can be hard to reach consensus on. The multitude of musical metadata elements and songs often implies that it is hard to let the categorization converge to a uniform tagging over \nthe whole music collection. Experience learned that this is \nmainly due to the fact that many people work together on the same task and during a long time period, in which they find new insights and change their minds. \n3.4 Construction of the metadata schema \nWe opted a flat structure for the metadata elements with \nindependent value spaces, allowing the structure to grow, \nadapt and change. This allows flexibility in the structure and favours the modeling capabilities for a MC.  \nBy working closely together with the music annotators, \nwe built a software metadata input tool that allowed them to experiment with many musical parameters and play with the parameters’ modeling powers to create a MC. The card sorting idea from the usability-testing world lies at the ba-sis of the metadata input tool (http://www.iawiki.net/CardSorting).  \nOn the left side of Figure 1 they have a working set of \nsongs available to annotate and listen to. They can then \nselect a data element (2nd column) to work on, e.g. genre. Table 1: comparison of existing metadata schemas \nThen they will be presented with the value space of this \ndata element and add songs to these values (cards), e.g. pop, rock, metal, … The experts can also add data ele-ments and edit their value spaces. A visualisation tool [8] is used for evaluation purposes, which clusters the songs based on musical parameter constraints and let them com-\npare their work with that of the other team members. \n \nFigure 1: paper mock-up of metadata input tool. \n3.5 Who works on the metadata? \nThe choice of the people who work on the metadata will \ndetermine the categorisation and hence also the usefulness of the fields to describe a MC with. The success or failure of a playlist generated from a MC depends highly on the quality of the metadata.  \nIt is very hard to gather such a team. When we use for \nexample a group of musicologists, we will probably get historically correct data, e.g. a classical reactionary work can historically have an aggressive and dark mood, but we can perceive it today as vibrant and enthusiastic instead. \nOn the other hand, if we just let anyone annotate music, \nwe get a lot of data from people with different back-\ngrounds and musical tastes. The major disadvantages are possible divergence in the selection of appropriate values from the value space and the need for a substantially large user base to support this. This would require a lot of time to bootstrap and due to the relatively short duration of this project (18 months) we chose another option. The idea to create a community collaborative system has not been abandoned and might be embedded in a follow-up project. \nWe decided to work with a diverse group of music ex-\nperts, including DJ's (from different areas: clubs, discos, \nparties, weddings, fashion shows, etc.), music teachers, \ndance teachers, musicologists, musicians, producers, etc. These music experts have expertise in their own musical sub-domain. Through their diversity, experience in the field and their work as a team they can unveil the correla-tion between musical properties and atmospheres without being all too rigorous or fundamentalistic. For most people reggae is associated with the typical backbeat guitar riff though not all reggae songs have this peculiar pattern. The end goal is to fulfill the expectations of the users of our system, which are not music experts. The old saying 'Equality is fitness for purpose' is still valid (http://www.qualityresearchinternational.com/glossary/fitn\nessforpurpose.htm). \n3.6 Evolution of the metadata schema \nIn the beginning we presented the music experts a selec-\ntion from the metadata fields as seen in Table 1 and asked them for their opinion and additions. This is how we came up with the left column of Table 2 (this is also a reduced \nschema like in Table 1). \nAs shown in the first column we took a lot of data ele-\nments from the existing systems, e.g. genre, mood, dance level, ... The experts added rhythm style and party level \nbecause they f ound these important to select dance music \nand are not present in the other systems. The geographical location was important for the music distribution, because some regions have very local music needs, carnival music for example is very region bound. \nTable 2: evolution of the metadata schema. \nWe compared the original schema with the present one  \nin Table 2 by underlining the identical data elements and putting them next to each other. It is clear that the biggest part of the original metadata schema is still there. We \nadded a few extra data elements, like the occasion to a play a song (Christmas, Halloween, …), the loudness of a song (to be able to select some quiet music for dinner) and the dance style for more control on party music. On the other \nhand we merged timelessness and music chart actuality \ntogether. The new data element describes whether it’s a hit (in the music charts) or whether it’s new, recent or from the archive. This is especially handy for clients to check the latest music on their system. \nGenerally speaking, a few data elements were added to \nthe set of the existing systems, which are typical for usage \nin the HORECA. \n3.7 Problematic cases \n3.7.1 Genres \nThere are many music genres available \n(http://en.wikipedia.org/wiki/Category:Musical_genres) making it hard to decide which is the most appropriate one. Most of the time this selection reflects the personal opinion \nof the music expert. A song by U2 for example could be \ncategorised in Rock or in Pop. Neither case is ideal. There-fore the experts decided to create a genre and a subgenre categorisation. The former contains main genres, like Pop, Rock, Jazz, Rhythm & Blues, Classical, etc. The subgenre can contain songs from different genres and thus take care of the border cases between main genres. For the U2 case, all their songs are categorised in genre Rock and the harder songs in the subgenres Rock Café and the ones closer to Pop in Pop Café. \n3.7.2 Popularity of a song \nThe relevance of the popularity depends highly on the user \ngroup. Each genre has its 'anthems', how many people know the main hit from the gabber scene, let alone appre-ciate it? To cope partially with this problem we came up with the idea of global popularity made for the mainstream user. \n4. Modeling of a Musical Context \n4.1 The idea \nSay all the songs in the collection have values for all the \nmetadata elements in the metadata schema. Now we want to create a MC description. \nWe can use the data members and their values to de-\nscribe the MC. If you select values of musical parameters, you will get music that satisfies these parameter values. E.g. if we select genre: jazz and soul and mood: relax and \nintimate, we can get a playlist that has jazz or soul music \nthat is relax or intimate.  \nWe can take this one step further, say we want romantic \nmusic for a candlelight dinner and we select genre: pop and chansons, language: English and French and mood: romantic, of course. Now we get a playlist with English and French romantic songs. But we like the English songs more then the French and we want to be able to alter the ratio of French and English music. So you could create two MC descriptions, one for the English music and one for the French music. We call these subcontexts. Each subcontext \nhas a certain weight ensuring for instance 20% French and \n80% English music. This makes it also easier to create a complex MC (divide-and-conquer). \n4.2 More abstract \nConsider a set of musical parameters. Each parameter has a \ndomain. Note that the domains are not necessarily com-\npatible. \nEach song has values for the musical parameters within \nthe proper domain. A subcontext is defined as a subset of parameters with selected values within their domain and a \nweight. This weight allows modeling things such as 20% French and 80% English songs. A musical context is de-fined as a collection of subcontexts. A song belongs to a subcontext if and only if the song contains at least one value that also occurs in the selected values of the subcon-\ntext, for each parameter in the subcontext. A s ong is part of \na context if it belongs to the union of songs belonging to the subcontexts. When generating playlists from a context, the weight has to be taken into account. \nSay for example we create a subcontext by selecting: \ngenre: Pop & Rock, year: 2004 till 2006, mood: happy, summer & exuberant. To get all the songs contained by this subcontext description, we select the songs that have as genre Pop OR Rock AND the songs with mood happy OR summer OR exuberant AND the songs from the year 2004 OR 2005 OR 2006. \n5. Implementation of the HORECA System \n \nFigure 2: architecture of the HORECA player. \nFor HORECA businesses we developed a stand-alone sys-\ntem (Figure 2) consisting of a music player, a database system, an encrypted music archive and a set of services for querying and context retrieval. To distribute the right music to the right place, the customers are clustered in groups with similar music styles by doing an inquiry on their preferred musical taste. Monthly music, metadata, playlist and context updates are sent on a CD by mail. Downloading updates is a future project. \nIn the player the user can search in a Google-like fash-\nion for artists, albums, titles, etc., get suggestions for simi-\nlar music and can add the found results to playlists and manage them. To create a context the user can select the wanted values from the different data fields and generate a playlist or he can use a context provided by the experts. \nOn top of a PostgreSQL database (www.postgres.org) \nlies a Hibernate object-relational mapping (www.hibernate.org). Hibernate enables us to transparantly communicate with the database through objects and offers different object-oriented query languages. One of these, the Criteria language, is used to dynamically build the query for the context creation and search service. \nAn XML-RPC layer takes care of the communication \nbetween the query part in Java and the player part in .Net. By placing the database and services on a server and let-ting users connect through XML-RPC over a network we can easily create a distributed system. 6. Evaluation \n6.1 Why contexts are a great idea in HORECA busi-\nnesses \nExperience learned that the most important factors that the \nHORECA wants to offer their clientele are continuity of adequate music and the possibility to comply with cus-tomer requests. From the customer's perspective this is enough and this was more or less feasible with the old sys-tem. The pub or restaurant had a music player, the PCDJ Red (www.pcdj.com). Aristo Music provided the music and manually created playlists. User profiles were created dependent on the preferred style: rock, general pop, lounge, lounge/dance, restaurant, dance, schlager, youth house, background music, a general profile and custom profiles. Each of those profiles had variants depending the \npossible location of the business: Flemish or Wallon part \nof Belgium, the Netherlands or Luxemburg. The playlists and music are matched with the profiles in Aristo Music's ERP system. \nSwitching contexts is as simple as changing a playlist, \none push of a button. Typically visiting a HORECA busi-ness takes between a few minutes up to a few hours. Bar-tenders stay through the day in the same working place, leaving them bored with the same playlists over and over. The use of contexts and a newly developed player offers an improvement on those matters, hence providing a direct \nperceived incremental benefit [9]. Music selection is better \nand more varied over time. The use of contexts is as easy as using playlists. In fact, a context can be seen as a vari-able playlist and is presented this way in the GUI. The HORECA workers can edit and create contexts on the spot, taking into account the given circumstances. \nThe new approach seems to hold. Since the introduction \nat a HORECA fair in November 2005 (www.horecaexpo.be), already 200 installations have been set up. Owners of the old system who are confronted with the new system indicate the willingness to migrate. Users of the new system do not want the old system back. The \nmain reasons given for this behaviour are the diversity in \nmusic, the simplicity and ease of use. The main reason why the new product is not purchased is the lack of DJ functionality. This is currently under development. \n7. The Future is Bright \nWith a version of the tool running in a few hundred \nHORECA businesses and a metadatabase of about 25000 songs, the foundations have been built for truly interesting points of improvement. \nPlaylist generation and rendering can be taken to a \nhigher level. Gradual shifting from one MC to another could be achieved with transition tracks. Also, the default \ncross-fade between tracks should be replaced by automatic mixing techniques. \nSince June 2006, we have started the next phase of this \nproject, currently we are focusing on the following topics. We are evaluating actual usage and the quality of the \nmetadata. This includes determining how to deal with \ndemographics and psychological matters regarding musical taste [10]. What parameters are really important for de-scribing mood?  \nThe painstaking task of entering metadata can be re-\nlaxed with music information retrieval techniques (http://www.iua.upf.es/mtg/clam/). These can be embedded to automate the process of annotating music. This automa-tion can combine suggestions for values, to assist the mu-sic experts, and calculating objective meta-data. Another way to get and evaluate metadata would be through setting up a community to gather metadata and to compare the \nresults of collaborative filtering with the existing database. \nBoth topics will be researched in the new phase. \nReferences \n[1] E. Duval, W. Hodgins, S. Sutton, S.L. Weibel, “Metadata \nPrinciples and Practicalities” in D-Lib Magazine,  April \n2002, vol. 8, no. 4. \n[2] F. Pachet, A. La Burthe, J-J. Aucouturier, A. Beurivé, “Edi-\ntorial Metadata in Electronic Music Distribution Systems: Between Universalism and Isolationism”, published in Journal of New Music Research 2005, vol. 34, no. 2. \n[3] F. Pachet, “Knowledge Management and Musical Meta-\ndata”, in Encyclopaedia of Knowledge Management , \nSchwartz, D. Ed. Idea Group, 2005. \n[4] S. Hansen, “Verzoeknummer – Hoe werken muziekadvi-\nessystemen?” in C’T, Magazine voor computer techniek , \nApril 2006,  Dutch, pp. 104-107. \n[5] The media metadata workspace, \nhttp://www.eu.socialtext.net/mediametadata/index.cgi \n[6] S. Pauws, B. Eggen, PATS: Realization and User Evalua-\ntion of an Automatic Playlist Generator , ISMIR 2002 3rd \nInt. Conf. on Music Inf. Retr. \n[7] X. Amariain, J. Massaguer, D. Garcia, I. Mosquera, The \nCLAM Annotator: A Cross-platform Audio Descriptors Ed-iting Tool , ISMIR 2005 6\nth Int. Conf. on Music Inf. Retr. \n[8] J .  Kle r kx,  M .  Me ir e,  S.  Te r nie r ,  K.  Ve r be r t,  E.  Duva l ,  In-\nformation Visualisation: Towards an Extensible Frame-work for Accessing Learning Object Repositories , World \nConference on Educational Multimedia, Hypermedia & \nTelecommunications, ED-MEDIA 2005. \n[9] E.D. Scheier, About this Business of Metadata , ISMIR \n2002 3rd Int. Conf. on Music Inf. Retr. \n[10] A. Uitdenbogerd, R. van Schyndel, A review of Factors \nAffecting Music Recommender Success, ISMIR 2002 3rd \nInt. Conf. on Music Inf. Retr."
    },
    {
        "title": "&apos;More of an Art than a Science&apos;: Supporting the Creation of Playlists and Mixes.",
        "author": [
            "Sally Jo Cunningham",
            "David Bainbridge 0001",
            "Annette Falconer"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1415662",
        "url": "https://doi.org/10.5281/zenodo.1415662",
        "ee": "https://zenodo.org/records/1415662/files/CunninghamBF06.pdf",
        "abstract": "This paper presents an analysis of how people construct playlists and mixes.  Interviews with practitioners and postings made to a web site are analyzed using a grounded theory approach to extract themes and categorizations. The information sought is often encapsulated as music information retrieval tasks, albeit not as the traditional “known item search” paradigm.  The collated data is analyzed and trends identified and discussed in relation to music information retrieval algorithms that could help support such activity.",
        "zenodo_id": 1415662,
        "dblp_key": "conf/ismir/CunninghamBF06",
        "keywords": [
            "grounded theory approach",
            "music information retrieval tasks",
            "practitioners",
            "web site",
            "trends",
            "traditional known item search",
            "collated data",
            "activity",
            "algorithm",
            "analysis"
        ],
        "content": "‘More of an Art than a Science’:  Supporting the Creation of Playlists and Mixes Sally Jo Cunningham Dept. of Computer Science University of Waikato Hamilton, New Zealand sallyjo@cs.waikato.ac.nz David Bainbridge Dept. of Computer Science University of Waikato Hamilton, New Zealand davidb@cs.waikato.ac.nz Annette Falconer Independent Scholar 17 Ernest Road Hamilton, New Zealand   Abstract This paper presents an analysis of how people construct playlists and mixes.  Interviews with practitioners and postings made to a web site are analyzed using a grounded theory approach to extract themes and categorizations.  The information sought is often encapsulated as music information retrieval tasks, albeit not as the traditional “known item search” paradigm.  The collated data is analyzed and trends identified and discussed in relation to music information retrieval algorithms that could help support such activity.  Keywords: playlists, mix CD, music information behavior. 1. Introduction I’m trying to put together a mix where one song responds to another song.  The first example that comes to mind is Neil Young’s “Southern Man,” to which Lynyrd Skynyrd replied with “Sweet Home Alabama.”  This is a real-world music information retrieval task, taken from a posting to the Art of the Mix web site, a site dedicated to playlists and mixes.  Other examples from the site mention as motivations for playlist creation an event such as a wedding, a mood they want to create, or a particular beat-rate for the gym.  Queries like these do not conform to the “known item search” paradigm which has seen extensive research in the music information retrieval community over the past few years, but rather taps into more lateral notions of association.  What motivates such requests?  Are there any established rules for forming mixes and playlists?  Can existing MIR algorithms augment existing software tools to help people satisfy such queries?   In this paper we consider these and related issues.  We start by describing the data that was gathered for analysis.  Several sources were used: face to face and web interviews with people who regularly create playlists, discussion threads about playlists, and as postings seeking help with mixes.  While conversationally the terms playlist and mix are often used interchangeably, here we are more careful in distinguishing between them.  A mix is usually of a set length, enough music to fill a CD or (less commonly these days) a tape,  usually has a strongly defined theme, and the order of the songs can be significant.  It is often a gift for someone else.  Playlists, in comparison, are typically for personal use, have varying lengths and a less strictly defined theme.  In Section 3 we highlight identifiable patterns to playlist generation that emerged from our collated data (primarily the interviews).  In Section 4 we study postings about formal mixes and categorize the set of organizing principles that are often the motivation for a mix.  We conclude with a discussion of how, based on our findings, software features and music processing techniques such as those already developed for MIR can assist people in creating playlists and mixes. 2. Data Gathering Data was collected on how individuals create playlists or mix CDs, the organizing principles behind the lists, the factors that make a given list ‘good’ or ‘bad’, and the purposes for which lists are created.  Specifically, we conducted six face-to-face interviews, and seven web-based interviews. We also collected six ‘threads’ (a total of 24 postings) exploring those questions from The Art of the Mix website (www.artofthemix.org). In the discussion below, interview participants are identified by a letter of the alphabet (Participant A, B, etc.), and contributors to the ‘threads’ are identified by their Art of the Mix usernames (e.g., bazoomy). Art of the Mix is dedicated to mix tapes and CDs.  It includes an extensive database of tens of thousands of playlists (text lists of song titles and artists) submitted by mix aficionados worldwide, and also includes forums and blogs that serve as a community center for discussion of playlist and mix creation. A set of 29,000 playlists from the Art of the Mix database have been used in earlier music retrieval studies to construct a graph of artist relationships, based on artist co-occurrence in playlists ([3], [4], [11]).  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. © 2006 University of Victoria In this study, we also analyze 115 requests posted to the Art of the Mix Forums, requesting help in completing a mix. These requests are essentially music information queries, with the goal of the information request being an ordered set of songs that conform to the organizing principles of the mix.   We analyzed these interviews and playlist/mix construction help requests using a grounded theory approach [7]. With this technique researchers attempt to approach the data without prior assumptions, and to generate theory from the data.  3. Personal Use Playlists Some playlists are created for personal use by oneself or a few close friends—primarily as background for another activity, for example to listen to while traveling, studying, or exercising at the gym. A playlists may be created to reflect a particular mood or emotion in the creator, such as depression, angst, or cheerfulness (“They reflect how I’m feeling at the time, they reinforce how I’m feeling”; Participant C). A playlist might also be shared as ‘party music’, in this context mainly as background rather than as dance music or the center of focus for the party.  The length of a playlist should match the length of its associated activity. If the activity is brief or isn’t repeated, then it hardly seems worth the effort to create a special playlist, rather than just putting existing ‘Favorites’ on shuffle.   The longest personal use playlist reported by a participant was eight hours of ‘thesis proposal writing songs’: It’s called Plan, it was made for writing my research plan for my PhD. I did not enjoy that process. One of the things I did to make it easier was to select 8 hours of music I liked well enough that it wouldn’t be annoying, so I could just leave it playing 8 hours and not have to mess around with my music. [Participant B] Personal playlists can be re-used: for example, songs to listen to while walking can also keep one awake while driving, a playlist for one long plane journey can be useful for the next trip, and ‘study music’ serves as background for more than one subject or assignment. Participants reported being more likely to invest effort into creating a playlists that they felt would be used several times. Because these lists are listened to on more than one occasion, or perhaps repeated during extra-long journey, participants report that they usually set these playlists on shuffle; this keeps the playlist fresh.  If the songs all conform to the organizing style or theme of the playlist then they are more or less compatible with each other, and there is little risk of an abrasive transition between songs (“so it’s not a big shock going from punk to world music or something”; Participant A).  Indeed, the serendipitous juxtaposition of songs may inspire new insights into the songs or spark an idea for a new playlists. Three interview participants did see a need for being able to sequence certain songs in a given playlist—a ‘chain’ of two or perhaps three songs that they felt should be played in a certain order. For example, “if there are two parts to that song, like with We Will Rock You by Queen, the second half of that song is We Are the Champions.  I usually play those in that order” (Participant E). Popular mp3 players do not as yet support the idea of ordered sequences within a shuffle. 4. Formal Mixes This section discusses types of formal mixes, and issues in their creation.  4.1 Organizing Principles for Mixes A good mix has a central theme or organizing principle:  it tells a story, shares a mood, “gives a perspective into the individual songs that you wouldn’t have had without seeing them in that idea” (Interview B). Table 1 presents a categorization of the organizing principles for 115 mix help requests posted to the Art of the Mix Forums; these categories emerged from a grounded theory analysis of the requests [7], and the assignment of a request to a category was confirmed by discussion among the researchers. As many proposed mixes had more than one theme, the percentages total to more than 100%. Table 1. Categorization of Organizing Principles for Mix Help Requests Category No. (%) Artist/Genre/Style  29 (25.2%) Event or Activity (further breakdown in Table 2) 29 (25.2%) Romance 22 (19.1%) Message or Story 19 (16.5%) Mood 19 (16.5%) Challenge or Puzzle 12 (10.4%) Orchestration 8  (7%) Characteristics of Mix Recipient 7 (6.1%) Cultural References 7 (6.1%) Other 3 (2.6%)  One quarter of mixes focused on specific Artists, Genres, or Styles—for example, “Best of Prince”, “acoustic-country-folk type stuff”, “Hawaiian music”.  Genre and style categories are notoriously difficult to define [13], and the mix help requests frequently desire songs that straddle one or more genres and styles (“acoustic-country-folk”, “kinda Psychedelic/British Invasion/Mod”).  On the other hand, if the goal is to support browsing then a lack of crisp categories may not be problematic. Browsing structures could help the user to narrow down the choices to a reasonably sized set of candidates if combined with display facilities that support the user in efficiently scanning that set.  Relationships can be complicated. The Romance category includes conventional love themes (for example, for a mix as a gift for a “little sister just…married to the man of her dreams”, who wants a “romantic masterpiece”; “great love songs”). Other requests are less straightforward:  a mix to mark ‘the end of the affair’ (“…that expresses dissapointment [sic] and longing…kind of like my farewell... but I don’t want the entire thing to be unlistenably depressing. I also don’t want all of them to do with obsessive love, ’cause I would hate to leave this man thinking I’m going to pull a \"Fatal Attraction\" on him”); a mix with the title “‘quit being a douche’, ’cause I’m in love with you. The title is somewhat self explainatory [sic]”; expressing bitterness towards a former lover (“Yeah, so I just got my heart broken. Any break up song that has a sad/angry vibe.”). Some mixes are intended to be listened to, as part of an Event or Activity (Table 2). Travel mixes are mainly intended as gifts for a friend or lover who is making a significant journey.  Mixes often celebrate holidays in idiosyncratic ways (“anti-Valentine mix”; “a very homosexual holiday mix”); mixes can set the vibe for a party (“creepy songs”) and may also be given as gifts to attendees; and assorted other activities (working out at the gym, enjoying “sparkling afternoons”).  We distinguish (perhaps arbitrarily) between mixes that are listened to as background music for an Event or Activity, and those that are constructed to tell a Story by describing an experience (real or imaginary, past or future) in music: for example, the planned activities for an upcoming bachelor party (“gambling, golf, a trip to the ballet, some drinking, a tailgate party and an NFL game”).  A mix can also be intended to send a Message to the recipient (a “mix for a cute stalker” that sends the message, “Yeah, I see you checking me out”).  Messages can be ambiguous:  There was a situation where I was getting to know this guy. We both gave each other playlists, it was bad because a lot of songs are about love and relationships, you just didn’t know [how the other person was interpreting the mix]. [Participant P, interviews] A range of Moods were specified: for example, “feel good happy”, “mellow”, “aggressive, violent or angry”. Some mood descriptions are more complex and difficult to capture in words (or song): “ever just sit alone in the dark while it's raining out - you feel kind of lonely and sad, but it's that sweet, sensual sort of sorrow that just feels good and, in a way, comforting?”  With a Challenge or Puzzle mix, the goal is to create an acceptably listenable mix that meets artificial criteria:  for example, a mix of songs with “eye” in the title, or more elaborately, a Frankenstein mix of songs conforming to rules such as: a song by an artist from, or somehow relating to where you live; a cover you like more than the original; a one hit wonder; the last song you downloaded; a song whose title is a question?; and so forth. These mixes allow the creator to show off their deep knowledge of music, explore and display their own collection and musical tastes (sometimes a fraught process, if the collection includes songs that might destroy one’s hip credibility), and offer the enjoyment and mental stimulation of a crossword puzzle or other mental game.  The Orchestration category was construed broadly, to include instrumentation (“songs that feature cello…preferably solo”) and other sound-related facets of the performance (“songs where the singer humms for a little bit”).  Seven mix help requests focused on Characteristics of the Mix Recipient as defining the types of songs required to complete a mix for that person:  for example, songs for “my mom…She’s tiny, ... 100 lbs about, harley chick, fun with broad music tastes.” These descriptions tap into stereotypes or profiles of the sorts of music that people of a particular age group, sexual orientation, personality, subculture, etc. might be expected to enjoy. Cultural References are a mixed bag: for example, “songs about superheroes”, “Viva Las Vegas”, “40 oz malt liquor”.  Other requests fall outside any of the categories: for example, a mix “devoted ENTIRELY to listening pleasure”.   Table 2. Events and Activities Category No.  Party  8 Travel 6 Holiday 5 Other 8 4.2 Additional Descriptive Features Tempo is mentioned as a secondary criteria for three mix requests:  songs that are approximately 140 beats per minute (for exercising in a gym), songs that are not “terribly sloooww”, and songs to appropriately pace various segments of a 5½ hour dance party. Eleven queries (9.6%) referenced the preferred date of first release for a candidate song for a mix. This date is expressed as a time period (1970s, 1980s, “older”) rather than as a specific year. The intention may be to identify songs that have the ‘feel’ of that period, without necessarily actually originating then: “Basically I'm trying to make a mix that's got the feel of a collection of lost 60's classics, but is actually composed entirely of songs from the past 20 years or so.” Over half of the requests (64) include at least one example of a song that could or should be included in the mix. A further 10 requests give examples of artists, rather than specific songs. These examples are intended to be interpreted in the light of the description of the mix theme—which explain what features of these songs are significant to the mix. Music similarity algorithms may be useful in suggesting ‘more songs like these’, particularly if the system interface allows the user to specify a weighting for similarity features. Existing similarity approaches include collaborative filtering (e.g., a variant of CF based on requests to an Internet radio station [10]) and metadata-based approaches (using, for example, the metadata available in the All Music Guide database  [16]). 4.3 The Mix Creation Process The creation of a mix can be precipitated in many ways:  the desire to give someone a special gift, an upcoming event that requires background music, a wish to listen to songs reflecting a mood, or even simply because there are “a few songs that I feel just need to be together” (zaxxon25) because “I think [they] would segue together really well” (concubine). A clear idea of the organizing principle is crucial. DJ Usurp also suggests picking a ‘general sound’ for the mix—“something like ‘loud/feedbacky’ or ‘sad’ or ‘piano’ or something”.  It will be easier to create a ‘listenable’ mix if the possibility of grating transitions is reduced by selecting candidate songs with a similar ‘sound’, broadly construed. At this point, usually one or more songs pop into the head of the mix creator as possibilities for inclusion in the mix. These songs can serve as anchors, around which the rest of the playlist is organized. The creator identifies more candidate songs by browsing his/her personal collection. If the mix is intended as a gift, then if possible that person’s collection should be browsed as well, “…for two reasons: a) no redundancies (if possible) and b) to get a feel for what they dig.” (DJ Usurp). More rarely, the creator might search an online database, either looking for specific songs that don’t happen to be in their personal collection, or trying out new songs: “I usually download a bunch of songs, some I’ve heard before and some I haven’t” (Shiloh). At this point, the creator is trying to open up creative avenues to explore: “My key to a great mix is overpicking … I always pick twice as much music as I need to fill the 80/90 minutes, then force myself to start winnowing things down” (zaxxon25).  The candidates are then arranged, rearranged, and mulled over until a satisfactory playlist emerges. This process can take varying lengths of time: “anywhere from an hour to a week” (bezoomny); “I usually spend several weeks or sometimes months playing songs over in my head and trying to figure out how to link A to B to C” (Mesh). The playlist might be manipulated as a paper list: “I end up with a couple of sheets filled with song titles, arrows, comments like \"fade this in halfway through…awesome!\" and lots and lots of crossings out”; (Concubine); or pulled together on a PC or portable mp3 player: “right now I have a 90 minute mix on mp3 that I’ve been listening to for a week, trying to get it down to 80” (zaxxon25).  4.4 Mix Song Order For mix CDs, song order is usually significant. Newbie requests for an explanation of the rules for good ordering elicited responses to the effect that “It’s been said that there’s only one rule… There are no rules” (FLWB).  This type of statement was then followed up with a set of loose suggestions: that there be no more than two songs from the same artist or genre in a row, unless there is a ‘special link’ between the songs (no borders); consecutive songs should have complementary styles or sounds so that the mix does “not clash one song up against another” (FLWB); the first song should be good, but not the best on the CD: “[good] enough to be like an introduction to the tape, and will make the listener stay and hear the who [sic] thing out, but not so good that they turn it off after they get what they want in the first song” (bezoomy); particular care should be taken in selecting the final song, as it “has to leave a pleasant memory…they’ll remember the last song easily, if it’s good” (bezoomy). But the Art of the Mix contributors remind us that rules are made to be broken: for example, “sometimes an abrupt change-up can be like an alarm clock going off in a mix tooo and that’s not necessarily a bad thing” (FLWA). Some creators even smooth over song transitions by crafting segues, crossfades, or special effects with music manipulation software (John Olson).  Participant B (interviews) sums up her song ordering technique as trying to avoid boring repetition and excessive change: I try to make the playlists so that there’s not too many slow songs, hard rock, sad songs together. I try to mix them up a little but not so much it sound random. A couple of upbeat ones, then a slower one, then a fast but maybe sad one, then a real hard rock one, then some slower ones again…I try to make it not too samey but not so random it’s completely un-listenable. Making a playlist is more of an art than a science. 4.5 Mix Length The length of a mix—the number of songs included, and the time in minutes—is usually tied to the physical recording medium. The original mix cassette tapes have been superceded by mix CDs (with a brief foray into mix 8-tracks in the 1980s). Tapes were more complicated to manage, as the two sides of the tape were essentially mini-mixes and had to have an internal coherency as well as supporting a sense of ‘flow’ from side A to side B. The goal is to avoid wasting space on the tape or CD by coming as close as possible to filling it, without cropping any song. Although the occasional mix is considerably longer or shorter than what would fit on a CD—and remember, there are no rules in mix construction—the majority of mixes discussed in the Art of the Mix Forum queries conform to this convention. 4.6 Mix Covers A formal mix often is accompanied by a CD cover design. Some mix creators consider the cover art “an intrinsical [sic] art of a mix” (yohan luxbroden), a final step in the process of crafting a mix. A good cover reflects the theme of the mix or the style of the songs. The cover might include images associated with the artists featured in the mix, but only if the artist is the principal organizing theme for the mix.  The cover is particularly important for mix CDs intended as gifts—and 36 of the mix requests (31.3%) were intended to be given as presents.  A gift must be both tangible and attractive—where a personal playlist or mix can be used or shared on an mp3 player, a gift should be burned to CD and have a proper front and back cover.  The back usually lists the songs on the CD, and probably the artists as well, but generally “not the lengths and so on” (Participant D). While no one reported creating full liner notes, it seems likely that givers would be more inclined to include them if those details were easily accessible.  If the mix CD focuses on one, two, or three artists, then the cover might simply include their photos or logos. Mix CDs with a more complex theme require more creativity to express “whatever undertone you’re trying to convey with the music” (yohan luxbroden).  For example, Participant B created a cover collage for a gift mix that described shared experiences with her friend: “there was cookies, cups of teas, the whole INXS rock star phenomenon, photos of some jewelry I made for her, Brian Mulco became a standing joke I don’t know how.”  Currently Google Images is a common source for digital images [5], and indeed Google Images is mentioned as the source for cover images in the interviews and on the Forums. Other sources include sites with copyright free images, stick figures custom-drawn from requests submitted to www.explodingdog.com, and of course one could “just go out and snap some pictures with a camera” (yohan luxbroden).  Less frequently, covers might be decorated with other physical media:  I have used all kinds of art supplies from crayons to markers to glittery glue to sequins to felt for decorate [sic] a mix cd's packaging and liner notes. I once did a fall mix which included Autumn Sweater by Yo La Tengo and I titled the mix The Autumn Sweater Mix and drew a little stick figure on the cover and cut out a piece of fuzzy sweater material from some scrap fabric I had and cut it into a sweater shape and glued that onto the stick figure guy. [dchipster] 5. Discussion and Conclusions Creating a playlist or mix can be fun—the creator engages with their personal collection, browsing through it, sorting and ordering the songs, viewing the songs in light of a new idea (the organizing principle of the mix). From the point of view of software support, applications with more interactive browsing facilities and more extensive browsing structures would both make it easier to locate candidate songs and add to the individual’s pleasure of possession of the personal collection. The general wisdom for information retrieval system design is that users wish to achieve their information goals efficiently and with a minimum of interaction with the system [12]; with one’s personal collection, the interaction may be more enjoyable if it can also include exploration and review while still maintaining a sense of purpose. Creating a playlist or a mix often begins with a linear search of one’s personal collection, or perhaps of a subset of the collection such as a folder dedicated to a particular genre, artist, or ‘Recent favorites’. Within these categories, usually the songs are ordered simply by filename or song title. When scanning the list, the creator must imagine how each song might fit the theme of the mix or list: is the song of the same or a compatible genre as others already in the list?  Do the lyrics fit with the message or story?  Is the emotional tenor of the song in keeping with the mix/list theme? Is the tempo appropriate?  The simple browsing structures and limited visible metadata for most music collection management software are too impoverished to provide support for these decisions—the creator must recall, for each song browsed, a host of details relevant to the selection decision.  Again, for a software application designed to assist someone in creating a playlist, richer browsing structures and more extensive search facilities are indicated, to aid in developing a shortlist of candidate songs. Additional metadata for individual songs should be easily accessible—for example, the lyrics are particularly useful in determining whether a given song conveys the precise shade of meaning required.  Creating a formal mix CD can require significant experimentation in song selection and ordering. Tools to annotate intermediate efforts would be welcomed by mix aficionados, who currently must rely on paper notes, digital notes created outside the music collection software, or their own memories. Collection visualization and interaction software such as the Artist Map [9], PlaySom [6] or Musicream [8] are promising; Musicream is particularly appealing because it includes features to support rearranging groups of songs.  Twelve (10.4%) of the mix help requests posted to Art of the Forum included negative constraints on the mix songs—that is, information about what they did not wish to include in the mix.  These details included limits on the tempo (“preferably no terribly sloooww songs”), message (“no bitchy \"I hate the world!\" bullshit”), artist (“As long as it is not Kid Rock, I'm game”), genre (“i wanted non-nashville sound type \"country\" music”), mood (“no serious songs”), orchestration (“trying to stay away from acoustic guitar”), specific songs (for an Anti-Valentine’s Day mix, “First person to say \"Love Stinks\" is placed on ignore”)—essentially, the same range of music features identified in this study that have been referenced above in a positive capacity.  Few current music retrieval systems offer a Boolean NOT feature for queries, or allow users to apply a NOT to filter entries in a browsing structure, yet this would be useful for fulfilling an important category of the posted mix help requests.  Creators of personal use playlists noted shortcomings in the interfaces to support playlist development, particularly on mobile mp3 players. It can be awkward to remove a song from a playlist: “One thing that irritates me about on the go: you can’t delete a song if you put it in there by mistake; you can delete an entire playlist but not an individual song. It annoys me because there’s one song I thought was another song and I can’t get rid of it [without deleting the entire playlist and starting again] (Participant B).   Earlier music retrieval work regarding playlists has focused on ‘automatic playlist generation’ (e.g., [1], [2], [11]), and to a lesser extent on supporting users in more easily constructing their own playlists [14]. The structures of the automatically generated playlists sit indecisively between the two types of lists identified in this paper—the informal personal use playlists and the formally organized mixes. Systems to automatically generate playlists may allow users to specify the length of a mix (e.g., 12 songs and 76 minutes [2]) and define an ordering for songs, but have the much more loosely defined theme of a personal use playlist (e.g., similarity to one or more seed (example) songs [11]).  Perhaps the automatic playlist generation techniques should be relaxed to provide support for users in creating and listening to their personal use playlists: for example, music similarity techniques could be used to suggest, rather than automatically select candidate songs similar to a few user-specified seeds; the user could specify a desired length for the playlist, so that the list could be more closely tailored to the circumstances under which it is used; and song ordering constraints could provide an improved shuffle facility.  This last point deserves further discussion. Proposed ordering constraints are intended to avoid abrupt transitions between songs by selecting consecutive songs that are closely related, for example by genre [2] or mood [11]; a list should have a sense of progression, for example with the songs increasing tempo or having a brighter mood [2]; and avoiding ‘same-iness’ by ensuring that there are limits on consecutive songs by the same artist [15]. A fixed ordering, however, is quickly perceived as boring for playlists that see frequent use, while a random shuffle may produce the occasion infelicity in song ordering. Both problems might be overcome by using shuffle to randomize the songs, and then a probabilistic application of the ordering constraints to make minor smoothing adjustments. References [1] M. Alghoniemy and A.H. Twefik. “A networked flow model for playlist generation,” in IEEE In. Conf. Multimedia and Expo. Proc., 2001. [2] J-J Aucouturier and F. Pachet. “Scaling up music playlist generation,” in IEEE In. Conf. Multimedia and Expo. Proc, 2001. [3] A. Berenzweig, B. Logan, D.P.W. Ellis, and B. Whitman. “A large-scale evaluation of acoustic and subjective music similarity measures,” Computer Music Journal, vol. 28, no. 2, pp. 63-76. [4] P. Cano and M. Koppenberger. “The emergence of complex network patterns in music artist networks,” in ISMIR 2004 Fifth In. Conf. on Music Inf. Retr. Proc, 2004, pp. 466-469. [5] S.J. Cunningham and M. Masoodian. “Looking for a picture: an analysis of everyday image information searching,” in JCDL06 Proc Joint ACM/IEEE Conf. on Digital Libraries, 2006. [6] M. Dittenbach, R. Neumayer, and A. Rauber. “PlaySOM: An alternative approach to track selection and playlist generation in large music collections,” [7] B. Glaser and A. Strauss. The Discovery of Grounded Theory:  Strategies for Qualitative Research. Chicago, 1967. [8] M. Goto and T. Goto. “Musicream: New music playback interface for streaming, sticking, sorting, and recalling musical pieces,” in ISMIR 2005 Sixth In. Conf. on Music Inf. Retr. Proc., 2005, pp. 404-411. [9] R. van Gulik and F. Vignoli. “Visual playlist generation on the artist map,” in ISMIR 2005 5th In. Conf. on Music Inf. Retr. Proc., 2005, pp. 520-523. [10] D.B, Hauver and J.C. French. “Flycasting: using collaborative filtering to generate a playlist for online radio,” Proc. Int. Conf. Web Delivery of Music, 2001. [11] B. Logan. “Content-based playlist generation: exploratory experiments,” in ISMIR 2002 Third In. Conf. on Music Inf. Retr. Proc, 2002. [12] G. Marchionini. “Interfaces for end-user information seeking,” JASIST, vol. 43, no. 2, 1992, pp. 156-163. [13] F. Pachet and D. Cazaly. “A taxonomy of musical genres,” in RIAO ’00 Proc Content-based Multimedia Info. Access Conf., vol. 2, pp. 1238-1245. [14] E. Pampalk, T. Pohle, and G. Widmer. “Dynamic playlist generation based on skipping behavior,” in ISMIR 2005 Sixth In. Conf. on Music Inf. Retr. Proc, 2005, pp. 634-637. [15] T. Pohle, E. Pampalk, G. Widmer. “Generating similarity-based playlists using traveling salesman algorithms,” in DAFx’05 8th In. Conf. on Digital Audio Effects, 2005. [16] S. Pauws and B. Eggen. “PATS: Realization and Evaluation of an Automatic Playlist Generator,” in ISMIR 2002 Third In. Conf. on Music Inf. Retr. Proc, 2002."
    },
    {
        "title": "Efficient Genre Classification using Qualitative Representations.",
        "author": [
            "Morteza Dehghani",
            "Andrew M. Lovett"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1416964",
        "url": "https://doi.org/10.5281/zenodo.1416964",
        "ee": "https://zenodo.org/records/1416964/files/DehghaniL06.pdf",
        "abstract": "We have constructed a system that can compute a qualitative representation of music from high-level features extracted from MusicXML files. We use two cognitively motivated computational models called SME and SEQL to build generalizations of musical genres from these representations. We then categorize novel music pieces according to the generalizations. We demonstrate the feasibility of the system with training sets much smaller than those used in previous systems. Keywords: Genre Classification, Symbolic Representation of Music.",
        "zenodo_id": 1416964,
        "dblp_key": "conf/ismir/DehghaniL06",
        "keywords": [
            "symbolic",
            "representation",
            "music",
            "genre",
            "classification",
            "MusicXML",
            "cognitively",
            "motivated",
            "computational",
            "models"
        ],
        "content": "Efficient Genre Classification using Qualitative Representations  Morteza Dehghani Northwestern University EECS Dept 2145 Sheridan Rd  Evanston, IL 60208-0834 morteza@cs.northwestern.edu Andrew M. Lovett Northwestern University EECS Dept 2145 Sheridan Rd  Evanston, IL 60208-0834 andrew@cs.northwestern.edu Abstract We have constructed a system that can compute a qualitative representation of music from high-level features extracted from MusicXML files. We use two cognitively motivated computational models called SME and SEQL to build generalizations of musical genres from these representations. We then categorize novel music pieces according to the generalizations. We demonstrate the feasibility of the system with training sets much smaller than those used in previous systems. Keywords: Genre Classification, Symbolic Representation of Music. 1. Introduction The problem of automatic genre classification has received a lot of attention in recent years, as it plays an important role in the field of music information retrieval [5]. The main approach to this problem has been focused on using low-level features extracted from audio files as a starting point for classification. At present, there is no precise way to extract many high-level features from polyphonic audio [5,6].   With only a small amount of computation, music features can be easily extracted from symbolic formats such as MusicXML. There have been several attempts to perform genre classification using high-level features extracted from symbolic formats [1,5,7]. McKay and Fujinaga [5] used a corpus of 950 MIDI files and achieved a success rate of 98% for 3-way classification. Chai and Vercoe [1] trained hidden Markov models on a corpus of 491 pieces and achieved the performance of 65%-77% for 2-way classification. Shan and Kuo [7] achieved an accuracy of 70%-84% for 2-way classification. One disadvantage of these systems is that they require a large database of files for training.  This requirement is a result of the machine learning algorithms used by their systems.    In this paper, we suggest an alternative method for representing, comparing, and classifying pieces of music.  Our system builds qualitative representations of musical pieces based on high-level features in symbolic MusicXML files.  It compares two pieces through a process called structural alignment, which is based on a psychological model of analogy and similarity in humans [3].  It learns to classify pieces by building generalizations for each genre containing the structure found in most or all the examples of that genre. 2. System Description The music conversion pipeline for our system starts with MIDI files. After converting MIDI’s to MusicXML’s, the following information is extracted from the files: the instrument name, part name, time signatures, and key signatures for each track; and the chord names. 2.1 Qualitative Representation (QR)  We use the data extracted from the MusicXML files to produce a qualitative representation of each musical piece.  The representation contains an entity for each track in the musical piece.  The entity receives attributes representing the instrument, time signature, and key signature for that track. The system also encodes the chord intervals in the piece. These intervals represent the distance from key name to root pitch of the chord. 2.2 Comparison and Generalization We compare representations using the Structure-Mapping Engine (SME) [2].  SME is a computational model of similarity and analogy based on Gentner’s [3] structure-mapping theory of analogy in humans.  SME takes as input two cases.  It finds all possible correspondences between entities, attributes, and relations in the two cases.  It combines consistent correspondences to produce mappings between the cases.  Our system learns categories of objects using SEQL, a model of generalization built on SME [4].  The idea behind SEQL is that humans form a representation of a category by abstracting out the common structure in all the exemplars of that category.  In its default mode, SEQL works in the following way: when it encounters a new case, it uses SME to compare that case to the known generalizations.  If the new case aligns with a sufficient amount of the structure in one of the generalizations, the case is added to that generalization.  Any part of the generalization’s structure that does not align with the new case is removed, so that the generalization continues to represent only the structure found in all of its exemplars. A recent update to SEQL associates a probability with each fact in the generalization.  When a new case is added to a generalization, those parts of the generalization that do not align with the case are not automatically removed, but instead have their probability decreased. 3. Experimental Section Our dataset includes 85 MIDI files. These files are from two root genres: pop and classical.  There are two leaf genres under the pop root genre: Rock and Bluegrass; and three leaf genres under the classical root genre: Baroque, Classical, and Romantic. There are 17 pieces for all leaf genres. 3.1 Evaluation Method To evaluate the system, we randomly divided the files into a test and a training set 20 different times.  The training sets contained 9 pieces from each genre, and the test sets contained the remaining 8.  In each run, our system used SEQL to produce a single generalization for all the pieces in each leaf genre’s training set.  Pieces in the test set were classified by using SME to compare each piece’s representation to all of the genre generalizations and returning the genre that matched most closely.  Matches were scored according to what percentage of expressions in the generalization were matched by SME to expressions in the test piece’s representation. Two results were measured: successful classification into the correct leaf genre, and classification into any genre falling under the correct root genre.  The results were averaged over all 20 trials.   3.2 Results Our best results were achieved when only instrument names and chord intervals were encoded.  With this representation scheme, our system was able to classify a musical piece into the correct root genre 88% of the time and into the correct leaf genre 58% of the time. Performance using only instrument names was identical for root genre classification but slightly lower for leaf genre classification, suggesting that chord intervals provided some additional information that helped to distinguish between leaf genres within a root genre Finally, because our focus was on learning from a minimal amount of data, we evaluated the performance when the size of the training set was decreased.  Table 1 shows the results when the training set size varied from 2 pieces per genre to 9 pieces per genre.  Interestingly, performance was well above chance even with only 2 pieces for each genre.  Performance increased as the training set size increased.  However, it appeared to level off at about 8 pieces per genre.  This may indicate that, at least for our dataset, there is no significant advantage to training on more than half of the 17 pieces in each genre.  4. Conclusions and Future Wor k Our system was able to efficiently learn generalizations based on training sets up to 1/10 smaller than training sets  \n Table 1. Performance with different training sets  used in systems which utilize other machine learning techniques [1,5], and still achieve an accuracy of 88% and 58%, respectively, for 2-way and 5-way classification. Even when we further limited the training set size to as few as 2 pieces per genre, we were still able to achieve accuracy rates of 79% and 45%, levels comparable to other systems.  We believe our system can be useful for classifying genres in situations where there is a constraint on the number of music pieces in the training set. 5.  References [1] Chai, W. and B. Vercoe. 2001. Folk music Classification using Hidden Markov Models. In Proceedings of the International Conference on Artificial Intelligence.  [2]  Falkenhainer, B., Forbus, K. and Gentner, D. 1989. The Structure-Mapping Engine: Algorithms and Examples. Artificial Intelligence 41: 1-63  [3] Gentner, D. 1983. Structure-Mapping: A Theoretical Framework for Analogy. Cognitive Science 7: 155-170.  [4]  Kuehne, S., Forbus, K., Gentner, D. and Quinn, B. 2000. SEQL: Category Learning as Progressive Abstraction Using Structure Mapping. In Proceedings of the 22nd Annual Conference of the Cognitive Science Society, 770-775. Philadelphia, PA.  [5]  McKay, C., & Fujinaga, I. 2004. Automatic Genre Classification Using Large High-Level Musical Feature Sets. In Proceedings of the 5th International Conference on Music Information Retrieval. Barcelona, Spain.  [6] Pampalk, E., Flexer, A., & Widmer, G. 2005.     Improvements of Audio-Based Music Similarity and Genre Classification. In Proceedings of the 6th International Conference on Music Information Retrieval. London, UK.  [7] Shan, M. K., and F. F. Kuo. 2003. Music style Mining and Classification by Melody. IEICE Transactions on Information and Systems E86-D (3): 655–69."
    },
    {
        "title": "Towards a MIR System for Malaysian Music.",
        "author": [
            "Shyamala Doraisamy",
            "Hamdan Adnan",
            "Noris Mohd. Norowi"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1414744",
        "url": "https://doi.org/10.5281/zenodo.1414744",
        "ee": "https://zenodo.org/records/1414744/files/DoraisamyAN06.pdf",
        "abstract": "Systems for the archival of musical documents digitally and development of digital music libraries are currently being researched and developed extensively.  However, adapting these systems for the archival and retrieval of Malaysian music materials might not be as straightforward due to the distinct differences in musical structure and modes of non-Western music.  This paper covers the motivations for the creation of a MIR system for Malaysian Music and outlines the plans for its development. Keywords: Malaysian Music, Digital Libraries, Music IR, Genre Classification, N-grams.",
        "zenodo_id": 1414744,
        "dblp_key": "conf/ismir/DoraisamyAN06",
        "keywords": [
            "digital archiving",
            "digital music libraries",
            "musical documents",
            "musical structure",
            "non-Western music",
            "MIR system",
            "Malaysian Music",
            "Digital Libraries",
            "Music IR",
            "Genre Classification"
        ],
        "content": "Towards a MIR System for Malaysian Music \nShyamala Doraisamy \nDept. of Multimedia \nFaculty of Comp. Sc. & IT \nUniversity Putra Malaysia \nshyamala@fsktm.upm.edu.my Hamdan Adnan \nDept. of Music \nNational Arts Academy \nMinistry of Culture, Arts and Heritage \nMalaysia \nhamdan_adnan@hotmail.com Noris Mohd. Norowi \nDept. of Multimedia \nFaculty of Comp. Sc. & IT \nUniversity Putra Malaysia \nnoris@fsktm.upm.edu.my \nAbstract \nSystems for the archival of musical documents digit ally \nand development of digital music libraries are curr ently \nbeing researched and developed extensively.  Howeve r, \nadapting these systems for the archival and retriev al of \nMalaysian music materials might not be as straightf orward \ndue to the distinct differences in musical structur e and \nmodes of non-Western music.  This paper covers the \nmotivations for the creation of a MIR system for Ma laysian \nMusic and outlines the plans for its development. \nKeywords : Malaysian Music, Digital Libraries, Music IR, \nGenre Classification, N-grams. \n1.  Introduction \nThis rapid development in MIR systems research and \ndevelopment is quite evident.  However, almost all of the \nsystems developed are largely based on Western musi c.  \nInterest in the incorporation of non-Western music data \nsuch as Korean, Indonesian and Malaysian is growing  \n[1,2,5].   \nThe study by Norowi et.al [1] investigates automate d \ngenre classification of Traditional Malaysian Music . A \nconsensus on the definition of Malaysian music can be \ndifficult to achieve.  Based on a very broad and ge neral \nview, it encompasses music from various cultures --  \nMalays, Indian, Chinese and native music of those f rom \nSabah and Sarawak (East Malaysian states on the Isl and of \nBorneo), as the Malaysian community is multi-racial .  The \nmodern music (contemporary, pop, etc.) of these var ious \ncultures further expands the genre classes of Malay sian \nmusic.  However, we define the scope of traditional  \nMalaysian music based on the study by Nasuruddin [4 ], \nand this is discussed in the following section. \nIn general, traditional Malaysian music is no longe r \nlistened to very widely, and the Malaysian society in \ngeneral simply associate it as music played for tra ditional \nceremonies.  Most are ignorant of its forms, genres  and also the possibility of this rich heritage disappea ring.  \nApart from it not usually being the favourite choic e for \nlistening when compared to pop music (either Wester n, \nMalay, Chinese, Indian, etc.), there are also very few \nmusical documents available for several of the genr es.  For \nthe genres with relatively large collections, eithe r in private \ncollections or with government agencies, these may not \nalways be easily accessible.  There are also genres  where \nthere are no notation systems, and the music is pas sed on \norally from generation to generation. \nThis paper discusses the need to use state-of-the a rt MIR \ntechnologies for the archival and retrieval of trad itional \nMalaysian music digitally and a review of technolog ies \ntowards its development.   Section 2 presents an ov erview \nof Malaysian Music.  A review of MIR technologies a nd \nthe related problems of its use with Malaysian musi c are \npresented in the next section. \n2.  Traditional Malaysian Music  \nThis section discusses briefly the genres and music al \nstructures of traditional Malaysian music and the a vailable \ncollections. \n2.1  Genres and musical structures \n     Using the term Malaysian Traditional Music wou ld \nencompass all of the traditional music in Malaysia \nincluding Sabah and Sarawak.  Nasuruddin [4] catego rises \nthese into six broad categories: shadow puppet musi c, \ndance theatre, music with Indonesian influence, per cussion \nmusic and nobat , syncreatic Malaysian music, and music \nfrom Sabah and Sarawak.  He also attempts to notate  the \nmicrotonal instruments using standard Western notat ion. \nHowever it is just an approximation and does not re flect \nthe actual pitches. \n     For the purpose of developing the MIR, these g enres \nwould have to be reclassified as tonal, microtonal and un-\npitched, based on the instruments used and the voca l styles \ninvolved during the actual performance.  \n     A number of genres from the syncreatic forms, and \nseveral based on the Indonesian influence such as Gamelan  \nand Caklempung  have been notated using Western notation \nor tablature. These can be loosely classified as to nal, and \nother genres such as dance theatre, shadow puppet t heatre \nand other related genres would be classified as mic rotonal. Permission to make digital or hard copies of all or  part of this work for \npersonal or classroom use is granted without fee pr ovided that copies \nare not made or distributed for profit or commercia l advantage and that \ncopies bear this notice and the full citation on th e first page. \n© 2006 University of Victoria 2.2  Musical Document Collections \nMost of the Malaysian music archives are in the for m of \nvinyl, tapes, manuscripts and scores.  A few collec tions of \nvinyl recordings are in mint condition, and the res t simply \nnot well preserved.  A number of rare recordings ex ist in \nthe cassette format.  However, due to the way these  were \nrecorded, most are not very clear audibly and requi res \naudio cleanup and enhancements to be useful for pur poses \nof MIR.  As for music manuscripts, the collections in \ncertain agencies have been through fire and floods.   \nThe military has its own musical tradition inherite d from \nthe British army who once colonised the country.  R ecently \na collection of approximately 400 titles from the B oosey \nmilitary band library was discovered in neglected s tate in a \nstorage facility.  Restoration of these is being pl anned and \nthe first phase includes fumigation and storage in air-\nconditioned facilities. Follow up processes are vac uuming \nand scanning of the scores forming digital versions .  \nFor genres that are lacking in documents, the Minis try of \nCulture, Arts and Heritage under its various agenci es have \nbeen documenting various traditional art forms via video \nand audio.  For music passed on aurally from genera tion to \ngeneration, some of the singings of the ‘masters’ w ould \nhave to be recorded and transcribed manually.  All these \nwould be converted into the appropriate digital for mat (if \nnot already in the suitable format) for MIR purpose s. \n3.  System Integration  \nIn general, the development of MIR systems requires  \nintegration of many technologies, most of which are  multi-\ndisciplinary.  Computing technologies include Infor mation \nRetrieval (IR), Digital Libraries (DLs), DBMS (Data base \nManagement Systems), MMIRSs (Multimedia Indexing \nand Retrieval Systems).  This paper looks closely a t three \ncomputing technologies and presents the issues that  would \nhave to be addressed towards the integration and \ndevelopment of the MIR system for Malaysian Music. \n3.1  Information Retrieval \nThe study by Doraisamy [3] investigated the use of text IR \nsystems for MIR.  Musical sequences are converted a s text \nstrings.  These strings are then subdivided into ov erlapping \nsub-strings of n-grams and encoded with text alphabets.  \nThe musical n-grams are then used as terms for the \nindexing and retrieval of the musical document.  Th e data-\ndriven encoding formula based on tonal music in thi s study \nwould have to be reformulated.  Micro-tonality woul d have \nto be addressed and one approach suggested for comp uter \nrepresentations of Microtonal Music by Suyoto [2] i s to \nalter or approximate the micro-tonality using the M IDI \npitch bend events.  This would have to be tested. 3.2  Feature Extraction and Data Mining \nAutomatic genre classification has been investigate d by [2] \nusing Traditional Malaysian Music genres. This woul d be \nimportant where there may be problems in manual \nclassification of these genres for retrieval system s due to it \nbeing only recognized by a small group of people in  the \nfuture.  Testing the system against manual classifi cation is \ncurrently being conducted. \n3.3  Digital library \nAmongst the large music digital library projects is  Meldex \n[7].  This was developed using Greenstone, a suite of \nsoftware for building and distributing digital libr ary \ncollections available at http://www.greenstone.org/ .  It has \nalso been shown to be useful for the retrieval of o nline \nscores available at http://chopin.lib.uchicago.edu/  [6].  This \napproach would be adapted for the retrieval of imag es of \nour scores as most of it would have to remain in th e \narchives due to the delicate conditions of the sour ce and \nalso ownership and copyright issues.  \n4.  Conclusion \nBased on the enormous amount of data on Malaysian \nmusic available through government agencies such as  the \nNational Archives, Radio Television Malaysia (the \nMalaysian broadcasting agency) and private collecti ons, \nthere is a need the retrieval or cataloguing system  to be \nupdated based on state-of-the-art MIR technologies.   \nOngoing work includes outlining the system architec ture \nand an early prototype for a MIR system for Malaysi an \nMusic. \nReferences \n[1]  N.M. Norowi, S. Doraisamy and R.W.O.K. Rahmat, \n“Factors Affecting Automatic Genre Classification: An \nInvestigation Incorporating Non-Western Musical For ms”,  \nin ISMIR 2005 Sixth Int. Conf. on Music Inf. Retr. Pro c ., \n2005, pp. 13-20. \n[2]  I. S.H. Suyoto, and A.L. Uitdenbogerd. “Exploring \nMicrotonal Matching,” in ISMIR 2004 Fifth Int. Conf. on \nMusic Inf. Retr. Proc ., 2004, pp. 158-163. \n[3]  S. Doraisamy, “Polyphonic Music Retrieval: The N-Gr am \nApproach”, PhD Thesis, Imperial College London , 2004. \n[4]  M.G. Nasuruddin, “Muzik Tradisional Malaysia”, Dewa n \nBahasa dan Pustaka, Kuala Lumpur, 2003. \n[5]  J.H. Lee, J.S. Downie and S.J. Cunningham, “Challen ges in \nCross-Cultural/Multilingual Music Information Seeki ng”, \nin ISMIR 2005 Sixth Int. Conf. on Music Inf. Retr. Pro c ., \n2005, pp. 153-160. \n[6]  T.A. Olson and S.J. Downie, “Chopin early editions:  \nConstruction and usage of online digital scores, in  ISMIR \n2005 Sixth Int. Conf. on Music Inf. Retr. Proc ., 2003, pp. \n247-248. \n[7]  R. McNab, L.A. Smith, D. Bainbridge and I.H. Witten , \n“The New Zealand digital library MELody index, D-Li b \nMagazine, May 1997."
    },
    {
        "title": "Instrument classification using Hidden Markov Models.",
        "author": [
            "Matthias Eichner",
            "Matthias Wolff",
            "Rüdiger Hoffmann"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1414960",
        "url": "https://doi.org/10.5281/zenodo.1414960",
        "ee": "https://zenodo.org/records/1414960/files/EichnerWH06.pdf",
        "abstract": "In this paper we present first results on musical instrument classification using an HMM based recognizer. The final goal of our work is to automatically evaluate instruments and to classify them according to their characteristics. The first step in this direction was to train a system that is able to recognize a particular instrument among others of the same kind (e.g. guitars). The recognition is based on solo music pieces played on the instrument under various conditions. For this purpose a database was designed and is currently being recorded that comprises four instrument types: classi- cal guitar, violin, trumpet and clarinet. We briefly describe the classifier and give first experimental results on the clas- sification of acoustic guitars. Keywords: Automatic musical instrument recognition, mu- sic content processing, multimedia content description",
        "zenodo_id": 1414960,
        "dblp_key": "conf/ismir/EichnerWH06",
        "keywords": [
            "Automatic musical instrument recognition",
            "Music content processing",
            "Multimedia content description",
            "Classification of acoustic guitars",
            "Solo music pieces",
            "Database design",
            "Recognizer based on HMM",
            "Training system",
            "Recognition based on various conditions",
            "Classification of instruments"
        ],
        "content": "Instrument classiﬁcation using Hidden MarkovModels\nMatthiasEichner, MatthiasWolff,R ¨udiger Hoffmann\nTechnischeUniversit¨ atDresden\nLaboratoryofAcousticsandSpeechCommunication\n{eichner,wolff,hoffmann }@ias.et.tu-dresden.de\nAbstract\nIn this paper we present ﬁrst results on musical instrument\nclassiﬁcation using an HMM based recognizer. The ﬁnal\ngoal of our work is to automatically evaluate instruments\nand to classify them according to their characteristics. The\nﬁrststepinthisdirectionwastotrainasystemthatisabletorecognizea particularinstrument amongothers of the same\nkind (e.g. guitars). The recognition is based on solo music\npieces played on the instrument under various conditions.For this purpose a database was designed and is currently\nbeingrecordedthatcomprisesfourinstrumenttypes: classi-\ncal guitar, violin, trumpet and clarinet. We brieﬂy describe\nthe classiﬁer and give ﬁrst experimentalresults on the clas-\nsiﬁcationofacousticguitars.\nKeywords: Automaticmusicalinstrumentrecognition,mu-\nsic contentprocessing,multimediacontentdescription\n1. Introduction\nRobust musical instrument recognition could lead to a va-\nriety of applications in the ﬁeld of music content analysis\nincluding automatic annotationof musical signals, retrieval\nof music from a database or quality assessment of instru-ments. Different approaches have been investigated in the\npastthatdifferinthefeaturesusedtodescribetheimportant\nspectralandtemporalpropertiesofthesignal,theclassiﬁca-tion strategy and the musical pieces that were used to train\nthesystem [1, 2, 3].\nInthispaperweapplyformerworkonageneralstructure\ndiscovering technique for speech signals [4] to music sig-nals. The method infers Hidden Markov Models (HMMs)\nof arbitrary topology in an entirely data-driven way from a\nset of training signals. This is particularly useful if thereis few or no prior knowledge about the temporal structure\nof the signals to model. We successfully applied this tech-\nnique to supersonic signals used in process integrated non-\ndestructivetesting(PINT)tasks[5].\nThepaperis organizedas follows. Sectiontwodescribes\nthedatabaseusedfortheexperiments. Theclassiﬁerisbrief-\nly described in section three. Finally, ﬁrst experimental re-\nPermissiontomakedigitalorhardcopiesofallorpartofthisworkfor\npersonal or classroom use is granted without fee provided that copies\narenotmadeordistributedforproﬁtorcommercialadvantageandthat\ncopies bearthisnoticeandthefullcitationontheﬁrst page.\nc/circlecopyrt2006University ofVictoria\nFigure 1. HMM automaton graph for one individual guitar.\nThe transitions are annotated with the index of a GaussianPDF deﬁning one state in the feature space and a weight (neg.\nlog. transition probability).\nsults onnoterecognitionandontheidentiﬁcationofguitars\narepresentedanddiscussed.\n2. Database\nFor our experimentswe designeda databasethat comprises\nfour instrument types: classical guitar, violin, trumpet and\nclarinet. Those instruments were chosen because (a) theycover a broad range of instrument groups and (b) we have\nseveral instruments permanently available for each group.\nThisallowsustoaddnewrecordingsinthefuture. Forevery\ninstrumenttypewedo600recordingsvaryingthefollowing\nconditions: (a) 10 instruments, (b) 2 rooms (unechoic andconference room), (c) 3 solo pieces, (d) 5 players and (e) 2\nrepetitiveplayings.\nSofar,weﬁnishedtherecordingsfortheclassicalguitars\nand are currently working on the violins. The selected gui-\ntars cover a wide spectrum in construction type and soundcharacteristics. We usedanartiﬁcialheadwhichwasplaced\n2 meters in front of the musician for the recordings. There\nwerethreesolopieces(ascale,abluesandanetude)playedon the guitars. The pieces are all approximately 30 s long\nandcontainonlyfewpolyphonicparts. Welabeledallrecord-\ningsonnotelevelina semi-automaticprocedure.\n3. Training and Classiﬁcation\nWe use our standard speech recognizer for the experiments\nand trained acoustic models for (1) single notes and (2) in-dividual instruments. First we pass the recordings through\na 31 channel mel–scaled ﬁlter bank (which slightly outper-\nforms MFCCs in our speech recognizer) and compute theTable 1. Experiment 1 - Note recognition correctness depend-\ning onthenumber of Gaussian PDFs.\n#GMs\n Correctness\n Density\n532\n 78%\n 2.97\n1061\n 76%\n 3.17\nﬁrst and second order differences which results in a 63-di-\nmensional primary feature vector. Then we apply a statis-\ntical principal component analysis and reduce the feature\nspaceto 25dimensions.\nWetestedtwodifferentsetsofHMMs. Foraﬁrstexperi-\nmentwetrainedHMMsforallnotesoccurringintherecord-\nings independently of the instrument and all other condi-\ntions described in section 2. The recognition task was to\nﬁnd the most likely sequences of notes for unseen record-\nings using the Viterbi algorithm. The correctness of the re-sult was assessed by the standard DTW string alignment to\nsemi-automaticallygeneratedreferencelabels.\nIn a second experiment we trained one HMM for each\ninstrument independently of all other conditions. Here the\nrecognition task was to identify one out of the ten instru-ments of one type by an unseen recording. This was done\ncomputingthemostlikelystatesequenceoftherecordingin\nallinstrumentHMMsandselectingtheonewiththehighestlikelihoodscore.\nIn both experiments we applied an HMM inference pro-\ncedurewhichis abletodiscoverthestructureofsignalsand\n– in contrast to [3] – to model it symbolically by inferring\nnot onlythe GaussianPDFs andthe transitionmatrixof theHMMs by also an arbitrary automaton graph [5]. Figure 1\nshowsaverysimpleexampleforanHMMtopologymodel-\ninga particularguitar(secondexperiment).\n4. Experiments\nThe experiments discussed in this section were carried out\nusing the 600 recordings of guitar solo pieces as describedin section2.\nFor the ﬁrst experiment we used the 120 recordings of\none guitar player. We randomly picked 110 recordings as\ntraining set and 10 as test set and trained 177 note HMMs\nand one silence model. Table 1 shows the note recogni-tion correctness for two different sets of HMMs. Despite\nthe high label insertion rate of about 300 % the experiment\nproovedthat ourrecognizeris suitable forprocessingmusi-cal signals.\nInthesecondexperimentwetrainedoneHMMperguitar\nandonesilencemodel. Thetaskwastoﬁndoutonwhichof\nthe ten guitars test recordingsnot seen by the training were\nplayed. Table 2 shows the guitar identiﬁcation results formodels trained with data from either a single or two musi-\ncians. The classiﬁcation margin was calculated by averag-\ning the differences between best and the second-best scoreTable 2. Experiment 2 - Guitar identiﬁcation correctness de-\npending on the number of Gaussian PDFs per guitar modeland the number ofmusicians in training set.\nSingle Musician\n Two Musicians\n#GMs\n Correct\n Margin\n Correct\n Margin\n15\n 60%\n 0.54\n 65%\n 0.26\n30\n 70%\n 0.82\n 75%\n 0.34\n60\n 100%\n 1.02\n 85%\n 0.48\n120\n 100%\n 1.40\n 95%\n 0.62\n238\n 100%\n 1.66\n 85%\n 0.69\nof all correct classiﬁed guitars. For recordings of a single\nmusician the system is able to correctlyindentifyall instru-\nments. If recordings of another musician are added to thetrainingset theperformancedecreases.\n5. Conclusion\nTheexperimentsdescribedarepreliminaryandworkinpro-\ngress, but they show the suitability of the chosen approach.\nFurther research will include experiments using more data\nandbuildingbettermodelsforinstrumentrecognitionusing\nlabel information. We will also try to develop a strategy toevaluate unseen instruments and to assign them descriptive\nattributes. The used dataset has a very strong inﬂuence on\ntheobtainedresults. Therefore,weplantoverifyourexperi-mentsto publiclyaccessibledatabases.\n6. Acknowledgments\nThis research is supported by the BMBF grant 03i4745A\nandis conductedincooperationwiththeInstitutf¨ urMusik-\ninstrumentenbauZwota,Germany(IfM).\nReferences\n[1] P.Herrera-Boyer, X. Amatriain,E.Batlle andX.Serra,“To-\nwards InstrumentSegmentationfor Music Content Descrip-\ntion: a Critical Review of Instrument Classiﬁcation Tech-niques”, 1st InternationalConferenceonMusic Information\nRetrieval, 2000.\n[2] A.G. Krishna and T.V. Sreenivas, “Music instrument recog-\nnition: from isolated notes to solo phrases”, ProceedingsICASSP-04 (IEEE International Conference on Acoustics,\nSpeechandSignalProcessing),2004.\n[3] M. Casey, “GeneralizedSoundClassiﬁcation and Similarity\nin MPEG-7”, Organized Sound, 6:2, Cambridge UniversityPress, 2002.\n[4] M.Eichner,M.WolffandR.Hoffmann,“Auniﬁedapproach\nforspeechsynthesisandspeechrecognitionusingstochastic\nMarkov graphs,” in Proc. 6th Int. Conf. Spoken LanguageProcessing(ICSLP),vol.1,pp.701–704,Beijing,PRChina,\n2000.\n[5] C. Tsch¨ ope, D. Hentschel, M. Wolff, M. Eichner and R.\nHoffmann, “Classiﬁcation of non-speech acoustic signalsusing structure models”, Proc. IEEE Intl. Conference on\nAcoustics,SpeechandSignalProcessing(ICASSP), 2004."
    },
    {
        "title": "Feature Selection Pitfalls and Music Classification.",
        "author": [
            "Rebecca Fiebrink",
            "Ichiro Fujinaga"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1415144",
        "url": "https://doi.org/10.5281/zenodo.1415144",
        "ee": "https://zenodo.org/records/1415144/files/FiebrinkF06.pdf",
        "abstract": "Previous work has employed an approach to the evaluation of wrapper feature selection methods that may overstate their ability to improve classification accuracy, because of a phenomenon akin to overfitting. This paper discusses this phenomenon in the context of recent work in machine learning, demonstrates that previous work in MIR has indeed exaggerated the efficacy of feature selection for music classification, and presents new testing providing a more realistic analysis of feature selection’s impact on music classification accuracy. Keywords: Feature selection, classification.",
        "zenodo_id": 1415144,
        "dblp_key": "conf/ismir/FiebrinkF06",
        "keywords": [
            "feature selection",
            "classification accuracy",
            "wrapper feature selection methods",
            "overfitting",
            "machine learning",
            "music classification",
            "new testing",
            "more realistic analysis",
            "previous work",
            "evident exaggeration"
        ],
        "content": "Feature Selection Pitfalls and Music Classification Rebecca Fiebrink and Ichiro Fujinaga Music Technology, McGill University Montreal, Canada rfiebrink@acm.org, ich@music.mcgill.ca  Abstract Previous work has employed an approach to the evaluation of wrapper feature selection methods that may overstate their ability to improve classification accuracy, because of a phenomenon akin to overfitting. This paper discusses this phenomenon in the context of recent work in machine learning, demonstrates that previous work in MIR has indeed exaggerated the efficacy of feature selection for music classification, and presents new testing providing a more realistic analysis of feature selection’s impact on music classification accuracy.  Keywords: Feature selection, classification. 1. Introduction Music classification can employ a palette of hundreds of low-level features (e.g., zero-crossing rate, MFCCs, LPC coefficients) and higher-order variations on these (e.g., standard deviation, first-order difference) [1]. Extracting hundreds of features from a large music collection, however, is costly in terms of both time and space. Furthermore, ideally, the size of a classifier’s training set should grow exponentially with the number of features [2]. However, it is not necessarily intuitive which of the possible features will be most relevant to a high-level music classification task, such as genre or artist identification, so it is sensible to look for an automated way of selecting a good subset of the available features.  Unfortunately, some proponents of one such technique, wrapper feature selection, have employed poor evaluation methods that may lead to an exaggeration of its benefits. We discuss recent work re-examining feature selection’s efficacy, and we temper our previous claims in [3] with results of recent, better-designed testing on the same data.  2. Wrapper Feature Selection Wrapper feature selection refers to a subset of feature selection techniques wherein each candidate feature subset visited in the algorithm’s search is evaluated by training and testing an induction algorithm (a classifier) using only that feature subset [4]. The classification accuracy for the visited candidate subsets is used to guide the search to new subsets, and the output of the algorithm at termination is the visited candidate subset with the best accuracy. Many wrapper algorithms have been proposed (e.g., forward selection, genetic algorithms), and there exists a body of literature claiming that particular methods work well, or that certain methods are superior to others. Reunanen [5,6] points out a problem with several such studies (e.g., [7]): they do not use an independent evaluation set to evaluate feature selection’s efficacy. In such studies, candidate feature subsets are evaluated using n-fold cross-validation accuracy on the entire dataset, computed by a classifier using the feature subset. Unfortunately, the cross-validation score of the best subset is typically not representative of the classification accuracy one can expect for new data. Moore and Lee [8] provide further insight: “… a naïve intensive use of cross validation, perhaps over many thousands of models, may produce a deceptively good lowest-error model, in a manner similar to overfitting of data.” Reunanen [5,6] recommends avoiding this pitfall by partitioning the dataset into mutually exclusive “outer” training and testing sets. Feature selection uses cross-validation accuracy on the outer training set to find the “best” feature subset. The efficacy of selection is evaluated by training a classifier on the outer training set, using the chosen feature subset, and examining classification accuracy on the independent outer testing set. Reunanen [5] uses this evaluation method to defend the use of simple feature selection methods such as forward selection over more intense search methods, whose efficacy may be more greatly exaggerated by poor evaluation techniques. Reunanen [6] further demonstrates that, when evaluated properly, feature selection is often actually ineffective at improving classification accuracy. These findings run contrary to claims of previously published studies not using independent test sets. 3. Feature Selection in MIR 3.1 Previous Work Previous work in MIR, namely [3], has used the aforementioned poor evaluation methodology for feature selection. That work used the evaluation methodology of [7] to show that feature selection was effective in improving classification accuracy on both musical and non-musical data. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. © 2006 University of Victoria 3.2 Re-evaluating Feature Selection in MIR We re-evaluated the claims of [3] using the recommendations of [5] and [6]. We ran forward selection on several datasets used in [3]. All tests were done using a nearest-neighbor classifier. Table 1 shows classification accuracy on the training and testing sets for the best feature subset. It illustrates that feature selection offers little or no benefit when measured using an independent test set, for these datasets and this classifier.  Table 1. Classification accuracy on testing set with no selection, and accuracy on training and testing sets using forward selection, for timbre recognition tasks from [3] Forward selection Timbre recognition task No selection (Testing set) Training Testing All attack 94.9 100 93.8 All 512 92.5 98.9 92.1 Time attack 88.9 96.5 89.4  S n a r e Time 512 91.0 95.1 86.2 Beat-box 91.6 98.3 91.1  We wondered whether feature selection would still offer any benefit for other problems in music involving many features, such as audio genre classification. We extracted 74 low-level features from the Magnatune database (4476 songs, 24 genres) [9] using jAudio [1]. We split the data into training and testing sets of equal size, stratified by album. Results appear in Table 2.  Perhaps unsurprisingly, forward selection does boost test set accuracy, though not to the extent suggested by the training set performance. It is reasonable to assume that many of the original low-level features contained redundant or irrelevant information. Additionally, the training and testing sets contained near-identical numbers of songs from each album, so the “album effect” ([10]) suggests that classification accuracy would tend to correlate well. We compared the above results to those obtained using principle components analysis (PCA) for dimensionality reduction, which re-mapped the data into a new 36-dimensional feature space expressed as a linear combination of the original features. The new features accounted for 95% of the variance of the original features. Results show that, using the same classifier and partitioning of the data as for forward selection, PCA provides a similar increase in accuracy in a fraction of the time (Table 2).  Table 2. Classification accuracy for no selection, forward selection, and PCA on Magnatune genre classification Method Time Training Accuracy Testing Accuracy No selection — 61.9 61.2 Forward 6.3 days 97.7 69.8 PCA 1 minute 70.6 71.0  4. Conclusions Our testing supports the conclusions of [6]: namely, the efficacy of feature selection in improving classification accuracy has been overstated. Our work in [3] fell into a common pitfall of poor evaluation methodology. New tests suggest that feature selection is in fact unable to significantly improve accuracy on the musical problems used in that work. Further testing on a musical genre classification problem suggests that feature selection can still improve classification accuracy under some circumstances, but PCA results in the same increase in accuracy, in a fraction of the time. Our results do not preclude the possibility that feature selection will work well for other problems and classifiers in MIR. Also, feature selection can provide other benefits, such as reduced feature extraction time with comparable (or only slightly worse) classification accuracy. In any case, one must employ sound evaluation methods to obtain a clear picture of the impact of feature selection on classification accuracy for a particular problem and classifier. 5. Acknowledgments We gratefully acknowledge support from the McGill University Max Stern Fellowship in Music and the Canada Foundation for Innovation. References [1] D. McEnnis, C. McKay, I. Fujinaga, and P. Depalle, “jAudio: A Feature Extraction Library,” in Int. Conf. on Music Inf. Retr. Proc., 2005. [2] R. O. Duda, P. Hart, and D. Stork, Pattern Classification, 2nd ed. New York: Wiley & Sons, Inc., 2001, pp. 169–170. [3] R. Fiebrink, C. McKay, and I. Fujinaga, “Combining D2K and JGAP for Efficient Feature Weighting for Classification Tasks in Music Information Retrieval,” in Int. Conf. on Music Inf. Retr. Proc., 2005. [4] R. Kohavi and G. John, “Wrappers for Feature Subset Selection,” Artificial Intelligence, vol. 97, pp. 273–324, 1997. [5] J. Reunanen, “Overfitting in Making Comparisons Between Variable Selection Methods,” Journal of Machine Learning Research, vol. 3, pp. 1371–1382, 2003. [6] J. Reunanen, “A Pitfall in Determining the Optimal Feature Subset Size,” in Int. Workshop on Pattern Recognition in Information Systems Proc., 2004. [7] M. Kudo and J. Sklansky, “Comparison of Algorithms that Select Features for Pattern Classifiers,” Pattern Recognition, vol. 33, pp. 25–41, 2000. [8] A. Moore and M. Lee, “Efficient Algorithms for Minimizing Cross Validation Error,” in Int. Conf. on Machine Learning Proc., 1994. [9] Magnatune, “Magnatune: MP3 Music and Music Licensing (Royalty Free Music and License Music),” [Web site] 2006, Available: http://www.magnatune.com  [10] B. Whitman, G. Flake, and S. Lawrence, “Artist Detection in Music with Minnowmatch,” in IEEE Workshop on Neural Networks for Signal Processing Proc., 2001."
    },
    {
        "title": "Probabilistic Combination of Features for Music Classification.",
        "author": [
            "Arthur Flexer",
            "Fabien Gouyon",
            "Simon Dixon",
            "Gerhard Widmer"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1415524",
        "url": "https://doi.org/10.5281/zenodo.1415524",
        "ee": "https://zenodo.org/records/1415524/files/FlexerGDW06.pdf",
        "abstract": "We describe an approach to the combination of music sim- ilarity feature spaces in the context of music classification. The approach is based on taking the product of posterior probabilities obtained from separate classifiers for the dif- ferent feature spaces. This allows for a different influence of the classifiers per song and an overall classification accuracy improving those resulting from individual feature spaces alone. This is demonstrated by combining spectral and rhythmic similarity for classification of ballroom dance music. Keywords: music classification, combination",
        "zenodo_id": 1415524,
        "dblp_key": "conf/ismir/FlexerGDW06",
        "keywords": [
            "music classification",
            "combination",
            "spectral",
            "rhythmic similarity",
            "ballroom dance music",
            "posterior probabilities",
            "classifier",
            "feature spaces",
            "overall classification accuracy",
            "different influence"
        ],
        "content": "Probabilistic Combination ofFeatures forMusic Classiﬁca tion\nArthur Flexer1, FabienGouyon2, SimonDixon2, Gerhard Widmer2,3\n1InstituteofMedicalCybernetics and ArtiﬁcialIntelligen ce\nCenter forBrain Research, MedicalUniversityofVienna,Au stria\n2AustrianResearch InstituteforArtiﬁcial Intelligence(O FAI), Vienna, Austria\n3DepartmentofComputationalPerception\nJohannesKeplerUniversity,Linz, Austria\narthur.flexer@meduniwien.ac.at, fabien.gouyon@ofai.a t,\nsimon.dixon@ofai.at, gerhard.widmer@jku.at\nAbstract\nWe describe an approach to the combination of music sim-\nilarity feature spaces in the context of music classiﬁcatio n.\nThe approach is based on taking the product of posterior\nprobabilities obtained from separate classiﬁers for the di f-\nferentfeaturespaces. Thisallowsforadifferentinﬂuence of\ntheclassiﬁerspersongandanoverallclassiﬁcationaccura cy\nimprovingthoseresultingfromindividualfeaturespacesa lone.\nThis is demonstrated by combining spectral and rhythmic\nsimilarityforclassiﬁcationofballroomdancemusic.\nKeywords: music classiﬁcation,combination\n1. Introduction\nSincetheperceivedsimilaritybetweenpiecesofmusicisde -\nﬁnedbya wholerangeofdifferentaspects(timbre,rhythm,\nharmony, melody, socio-cultural, etc) it is only logical th at\nany attempt at music classiﬁcation should be based on a\ncombination of these different dimensions of similarity. I n\nthe scientiﬁc ﬁeld of statistical pattern recognition, the re\nexist clear results as to how to achieve such a combina-\ntion (see e.g. [7]). Bayesian theory tells us that all avail-\nableinformation(i.e.featuresderivedfromdifferentasp ects\nof music similarity) should be considered simultaneously\nby using one overall classiﬁer. However, this is very often\nnot practicalor evenpossible (dueto exponentialgrowthof\nthe number of the parameters, different time scales or gen-\neral incomparabilityof feature spaces, etc). The alternat ive\nthenistocombineinformationfromdifferentsourceswhich\nleadstothequestionofhowtoweighinformationfromthese\nsources to reach an overall decision. In a probabilistic set -\nting, the preferred approach is to train separate classiﬁer s\nfor the different feature spaces and then to combine poste-\nriorprobabilitiestoobtainajointdecision. Thisallowsf ora\ndifferent inﬂuence of the classiﬁers per song based on their\nPermission to makedigital orhardcopies ofallorpartofthi s workfor\npersonal or classroom use is granted without fee provided th at copies\narenotmadeordistributed forproﬁtorcommercial advantag e andthat\ncopies bear this notice and thefull citation on the ﬁrstpage .\nc/circlecopyrt2006 University of Victoriaposterior probabilities. Thereby different aspects of mus ic\nsimilarity achieve different weights in the joint classiﬁc a-\ntiondecisionforeverysong.\nAlthough this combination approach is well known in\nstatistical pattern recognition, something related has ba rely\nbeen used within the Music Information Retrieval commu-\nnity so far [16, 14]. We therefore think it is beneﬁcial to\nfurther explore this probabilistic approach to the combina -\ntion of featuresfor music classiﬁcation by (i) reviewingth e\nnecessarytheory,(ii) presentingexperimentalresults on the\ncombinationofspectralandrhythmicsimilarityforclassi ﬁ-\ncationofballroomdancemusic.\n2. Data\nThe musical data set used for training and testing contains\nexcerpts from S=698 pieces of music, around 30 seconds\nlong,amountingtoaround20940secondsofdata. Thisdata\nwas originally downloaded from a web site1providing di-\nverseresourcesrelatedtoballroomdancing(onlinelesson s,\nvideos, books, etc.). Some characteristic excerpts of many\ndance styles are provided in the low quality Real Audio\nsound format (with a compression factor of almost 22 with\nrespect to the common 44.1 kHz 16 bits mono WAV for-\nmat), labelled with a speciﬁc dance style ( G=8 music gen-\nres: Cha Cha Cha (111 pieces), Jive (60), Quickstep (82),\nRumba (98),Samba (86), Tango(86), Viennese Waltz (65),\nSlow Waltz (110)). Thedata wassubsequentlyconvertedto\nWAV formatforexperiments.\nThis data was ﬁrst used in [6, 4]. It was also used in\ntheTempo induction andRhythm classiﬁcation contests or-\nganised during ISMIR 2004. The audio data and style and\ntempoannotationsarepubliclyavailable.2\n3. Musicsimilarity\nOne conceptofcentralimportancein musicinformationre-\ntrievalisthenotionofmusicalsimilarity. Similaritymet rics\ndeﬁne the inherent structure of a music collection, and the\nacceptanceofamusicretrievalsystemcruciallydependson\n1http://www.ballroomdancers.com/\n2http://ismir2004.ismir.net/whethertheusercan recognisesomesimilaritybetweenthe\nqueryandtheretrievedsoundﬁles. Sinceusuallynoground\ntruthwithrespecttomusicsimilarityexists,genreclassi ﬁca-\ntion is widely used for evaluationof music similarity. Each\nsongislabelledasbelongingtoamusicgenreusinge.g.mu-\nsic expert advice. High genre classiﬁcation results indica te\ngoodsimilaritymeasures. Genreclassiﬁcationisalsoagoa l\nin its own right: it allows labelling of a user’s collection o f\nmusic based on a subset of trainingsongswith user deﬁned\ngenrelabels.\nSome of the most successful approaches to genre clas-\nsiﬁcation are based on the use of spectral features (see e.g.\n[10, 1, 15] and many more), rhythm [15, 6, 13] or cultural\nfeatures [12, 16, 8]. It seems clear that combinationof dif-\nferentaspectsofmusicsimilarityisabletoimprovetheper -\nformanceachievedso far. Our experimentsare onthe com-\nbinationofspectralandrhythmicsimilarity.\n3.1. Spectralsimilarity\nThe following approach to music similarity based on spec-\ntral similarity (see [10, 1] for early references) is now see n\nasoneofthestandardapproachesintheﬁeldofmusicinfor-\nmation retrieval. For a given music collection of Ssongs,\neach belonging to one of Gmusic genres, it consists of\nthe following steps: (i) for each song, compute Mel Fre-\nquency Cepstrum Coefﬁcients (MFCCs) for short overlap-\nping frames; (ii) train a Gaussian Mixture Model (GMM)\nfor each of the songs; (iii) compute an S×Sdistance ma-\ntrixbetweenallsongsusingthelikelihoodofasonggivena\nGMM.\nWe divide the raw audio data into overlapping frames\nof short duration and use Mel Frequency Cepstrum Coefﬁ-\ncients(MFCC)torepresentthespectrumofeachframe(see\ne.g.[9]). TheframesizeforcomputationofMFCCs forour\nexperimentswas 23.3ms(1024samples),withahop-sizeof\n11.6ms(512 samples) for the overlap of frames. We used\ntheﬁrst 8 MFCCsforall ourexperiments.\nA Gaussian Mixture Model (GMM) models the density\noftheinputdatabyamixturemodeloftheform\np(x) =M/summationdisplay\nm=1PmN[x, µm, Um] (1)\nwhere Pmisthemixturecoefﬁcientforthe m-thcomponent,\nNisthe Normaldensityand µmandUmarethe meanvec-\ntorandcovariancematrixofthe m-thmixture. Foradataset\nXicontaining Tdata points given a GMM trained on song\nj, thenegativelog-likelihoodfunctionisgivenby\nL(Xi|GMM j) =−1\nTT/summationdisplay\nt=1log(pj(xi\nt))(2)\nFor learning a GMM for a song i,L(Xi|GMM i)is min-\nimised both with respect to the mixing coefﬁcients Pmandwith respect to the parameters of the Gaussian basis func-\ntionsusing Expectation-Maximisation(see e.g.[2]). Fora ll\nourexperimentsweused M= 10componentsanddiagonal\ncovariances. Computation of L(Xi|GMM j)for all possi-\nble combinations of songs iand GMMs jgives an S×S\ndistancematrix DS.\n3.2. Rhythmicsimilarity\nThere are many ways to compute rhythmic similarity, e.g.\n[15, 6, 13]. In thispaper,the deﬁnitionofrhythmicsimilar -\nity focusesona singlerhythmicdimension: thetempo.\nTempoisamusicalattributeofprimeimportanceandre-\ncent research showed, on the data used here, the high rele-\nvance of tempo for ballroom dance music classiﬁcation [6,\n4]. Common musical knowledge (e.g. instructional books,\ndance class websites) suggests that tempo is a fundamen-\ntal feature in the deﬁnition of musical styles, and on the\notherhand,[11]showsonalargeamountofdata(morethan\n90000 instances) that different dance music styles (“tranc e,\nafro-american,houseandfast”)showclearlydifferenttem po\ndistributions,centredarounddifferent“typical”tempi.\nThe tempo induction algorithm used is one of the algo-\nrithms that entered the MIREX 2005 competition on Per-\nceptual tempo induction . It is referred to as Algorithm1\nin [5] and consists of the following processing steps: (i)\nframing of the signal; (ii) computation of the magnitude-\nnormalised derivative of the energy in 8 frequency bands;\n(iii) computation of the autocorrelation in each band; (iv)\nparsingofperiodicityfunctionpeaksandglobaltempocom-\nputation as in [3]. This algorithm yields one single tempo\nfeature tifor each song i. The distance dR(i, j)between\ntwo songs iandjis computed as the Euclidean distance\n(ti−tj)2. Computation of all possible combinations of\nsongs iandjgivesan S×Sdistancematrix DR.\n4. Combination\nFollowingearlierworkoncombinationofclassiﬁers[7],le t\nus consider a pattern recognition problem where a pattern\nZis to be assigned to one of Gclasses (ω1, ..., ω G). Let us\nfurtherassumewehave Rclassiﬁerseachreceivingdistinct\nmeasurementvectors xi(e.g.one classiﬁer trained onspec-\ntral similarity, anotheroneon rhythmicsimilarity). In me a-\nsurement space xieach class ωkis modelled by the proba-\nbility density function p(xi|ωk), with P(ωk)being the cor-\nrespondingpriorprobability. Accordingto Bayesian theor y\na pattern Zwith measurements xi, i= 1, ..., R, should be\nassigned to class ωjprovidedthat the correspondingposte-\nriorprobabilityismaximal:\nP(ωj|x1, ..., x R) = max\nkP(ωk|x1, ..., x R)(3)\nThismeansthattoutiliseallavailableinformation,allav ail-\nablemeasurementsshouldbeconsideredsimultaneously.\nHowever, very often this is not practical or even not feasi-\nble. Simultaneoususe of all measurementscan lead to verylargefeaturespaceswhicharehardtomodelduetothecurse\nof dimensionality(i.e. exponentialgrowthof the numberof\nmodelparameterswithnumberoffeatures,seee.g.[2]). Of-\nten subsetsof themeasurementsarehardtocomparedueto\ntheir differentorigin and it is unclear how to weight or nor-\nmalise them. Sometimes they exist on different time scales\n(e.g. one spectral feature vector every 11.6mscompared to\none single rhythmic feature for a whole song) and cannot\nbe concatenated at all. Therefore a promising approach is\ntouseindividualclassiﬁersforsubsetsofthemeasurement s\nandtocombinetheclassiﬁer outcomesinstead.\nAssumingthatthemeasurements xi, i= 1, ..., Rarecon-\nditionallystatistically independent,we canrewritethed eci-\nsionrulegivenbyEqn.3to:\nP(ωj|x1, ..., x R) = max\nkR/productdisplay\ni=1P(ωk|xi)(4)\nThis means we can express the posterior probability given\nthejointmeasurementsastheproductoftheposteriorscom-\nputed in the individualmeasurementspaces. This is known\nas the product rule [7] and for many applicationsthe above\nindependence assumption provides an adequate and practi-\ncable approximation to a reality that might be more com-\nplex. It is important to note that the product rule provides\na different inﬂuence of the classiﬁers per song since the\nweightgiventoeachoftheclassiﬁerschangeswiththepos-\nteriors obtained for each song. If a classiﬁer is very inse-\ncure about its decision all posterior probabilities will be at\nthe same level and not inﬂuence the other classiﬁer poste-\nriors at all. If the decision of a classiﬁer is very clear, the\ncorresponding posterior probability will be very high and\ndominatetheproductoftheposteriors.\nA simple way to directly estimate posteriorprobabilities\nis to use K-nearest neighbour classiﬁcation [2]. The num-\nbers of songs belonging to genres (ω1, ..., ω G)in the set of\ntheKnearest neighbours are an approximation of the true\nposteriorprobabilities. Toavoidzeroprobabilitieswead ded\napseudo-counttoallnumbersofsongsbelongingtogenres.\nA related approach to combination of music similarity\nfeatures has been reported in [16]. Contrary to the above\ndescribed approach no proper posterior probabilities were\nused and therefore, followinga suggestion by [7], the aver-\nage instead of the product was computed for combinations\nofestimatedposteriors. Thedatasetinthisstudywasrathe r\nsmall (25 artists) and the goal was classiﬁcation of artists ,\nnot songs,into ﬁvegenres. Combinationof audioand com-\nmunity meta-data improved the results achieved on the in-\ndividual feature spaces. Not directly related is an approac h\nat thesymbolicleveloncombiningpredictivemodelsofse-\nquentialpitchstructureinmelodicmusic[14].\nOur own previous efforts on combination [13] relied on\na linear combinationof distance matrices which were com-\nputed for the individualfeaturespaces. This new combineddistancematrixwasthenusedasinputforoneoverallclassi -\nﬁer. Contraryto our new approachthis requiresone overall\nweighting for all songs in a training set. It is also not clear\nhow to obtain the speciﬁc weighting which is optimal for\na speciﬁc data set. This would require a meta-search strat-\negy evaluating all different possible weights for the linea r\ncombinationand a ﬁnal evaluation of the winner using pre-\nviouslyunseentestdatatoallowforfairperformanceevalu -\nation. Nevertheless,linearcombinationofdistancematri ces\nisalsoapplicabletoproblemsoutsideofmusicclassiﬁcati on\nsincenoclasslabelsareneededtocomputethecombination\nweights.\n5. Results\nWecomputeddistancematrices DSandDRforall S= 698\npiecesofmusicinourdatabaseasdescribedinSecs3.1and\n3.2. We also computeda combineddistance matrix DCus-\ning a linear combination: DC= (DS+DR)/2. Since we\nhave noinformationas to which weightingto preferwe de-\ncided to use this simple average. All distances in DSand\nDRwere normalised to zero mean and unit standard devia-\ntion before this combination to guarantee comparability as\nsuggestedin [13].\nIn a 10-foldcross validationwe did the followingexper-\niments:\n1. doK=10-nearestneighbourclassiﬁcationforallsongs\nin the test fold using distance matrices DS,DRand\nDCto compute respectiveposteriorprobabilities; as-\nsign each song to the class with maximal posterior\nprobability\n2. usetheproductruletocombineposteriorprobabilities\nobtainedfrom K=10-nearestneighbourclassiﬁcation\nusingDSandDRseparately;assign eachsongto the\nclasswithmaximalposteriorprobability\nAverageclassiﬁcationaccuraciesplusstandarddeviation s\nfor the four different methods are given in Table 1. As can\nbe clearly seen, the combination based on the product rule\nisabletoenhancetheperformanceconsiderablywhencom-\nparedwithclassiﬁcation basedonspectralorrhythmicsim-\nilarity alone. Both results are statistically signiﬁcant w hen\nusing paired t-tests: spectral vs. product t=| −16.10|,\nrhythm vs. product t=| −4.61|> t(99,d f=9)= 3.25.\nThe product rule also outperforms the linear combination\n(t=|−5.55|> t(99,d f=9)= 3.25),whichevenfallsbehind\ntheclassiﬁcation basedonrhythmicsimilarity alone.\nspectral rhythm linear product\n33.39±6.1658.59±5.3448.67±7.7766.89±5.26\nTable 1. Mean accuracies andstandarddeviations for thefou r\nmethods.1234020406080100ChaChaCha\n1234020406080100Jive\n1234020406080100Quickstep\n1234020406080100Rumba\n1234020406080100Samba\n1234020406080100Tango\n1234020406080100VienneseWaltz\n1234020406080100Slow Waltz\nFigure 1. Mean percentages of correctly classiﬁed songs per\nGenre given for the four methods: (1) spectral, (2) rhythm, ( 3)\nlinear combination,(4)posterior productcombination.\nThe mean percentages of correctly classiﬁed songs per\ngenrearegiveninFig.1. Whereascombinationbasedonthe\nproduct rule improves performance for every genre except\n’Viennese Waltz’ and ’Jive’, linear combination increases\nperformanceforonly two genres(’ChaChaCha’,’Jive’)but\ndecreasesit forﬁveofthe remainingsix.\nWe also tried different weights for the linear combina-\ntion of DSandDR. Changing the weights in steps of 0.1\ngavebest performanceof 59.97±9.63forthe combination\nDC= (0.1∗DS+0.9∗DR). Althoughthispost-hocchoice\nof an optimal classiﬁer is not statistically sound and shoul d\nlead to over-optimistic performance estimates, the result is\nstilljustatthelevelofclassiﬁcationbasedonrhythmicsi m-\nilarity alone. Combination of posterior probabilities usi ng\nthe average instead of the product as done by [16] yielded\nanaverageaccuracyof 63.17±5.77. Thisisbetterthanthe\nresultsbasedonspectralorrhythmicsimilarityaloneaswe ll\nasbasedonthelinearcombination. Butitdoesnotreachthe\nproduct rule’s performance(product vs. average t=|5.01|\n> t(99,d f=9)= 3.25).\n6. Conclusion\nWepresentedageneralframeworkforcombinationofmusic\nsimilarity feature spaces in the context of music classiﬁca -\ntion. Combinationofseparateclassiﬁerstrainedontheind i-\nvidualfeaturespacesallowsfora differentweightingof th e\nseparate classiﬁers per song which results in a considerabl e\nincrease in genre classiﬁcation accuracy. This probabilis -\ntic approach to combination of classiﬁers is well known in\nstatistical pattern recognition and our results obtained f or a\ncombination of spectral and rhythmic similarity applied to\nballroom dance music conﬁrm its applicability in the ﬁeld\nofMusicInformationRetrieval.7. Acknowledgments\nParts of the MA Toolbox3and the Netlab Toolbox4have been\nused for this work. Thisresearch was supported bythe EU proj ect\nS2S2,5and the WWTF project CI010 Interfaces to Music . OFAI\nacknowledges support from the ministries BMBWKandBMVIT.\nReferences\n[1] Aucouturier J.-J., Pachet F.: Music Similarity Measure s:\nWhat’s the Use?, in Proc. of the 3rd Int. Conf. on Music\nInformation Retrieval,pp. 157-163, 2002.\n[2] Bishop C.M.: Neural Networks for Pattern Recognition,\nClarendon Press,Oxford, 1995.\n[3] Dixon S., Pampalk E., Widmer G.: Classiﬁcation of dance\nmusic by periodicitypatterns, inProc.4thInt. Conf.on Mu-\nsic Information Retrieval,2003.\n[4] Gouyon F., Dixon S.: Dance music classiﬁcation: A tempo-\nbased approach, in Proc. of the 5th Int. Conf. on Music In-\nformation Retrieval,2004.\n[5] Gouyon F., Dixon, S.: Inﬂuence of input features in perce p-\ntual tempo induction, in 2nd Annual Music Information Re-\ntrieval eXchange (MIREX),2005.\n[6] Gouyon F., Dixon S., Pampalk E., Widmer G.: Evaluat-\ning rhythmic descriptors for musical genre classiﬁcation, in\nProc.25thInternationalAESConference,pp.196-204,2004 .\n[7] Kittler J., Hatef M., Duin R.P.W., Matas J.: On combining\nclassiﬁers, IEEE Transactions on Pattern Analysis and Ma-\nchine Intelligence, 20(3), 1998.\n[8] Knees P.,PampalkE.,WidmerG.: ArtistClassiﬁcationwi th\nWeb-based Data, Proc. of the 5th Int. Conf. on Music Infor-\nmation Retrieval,2004.\n[9] Logan B.: Mel Frequency Cepstral Coefﬁcients for Music\nModeling, Proc. of the International Symposium on Music\nInformation Retrieval,2000.\n[10] LoganB.,SalomonA.: Amusicsimilarityfunctionbased on\nsignalanalysis,IEEEInternationalConf.onMultimediaan d\nExpo, Tokyo, Japan, 2001.\n[11] Moelants D.: Dance music, movement and tempo prefer-\nences, inProc. 5thTriennal ESCOMConference, 2003.\n[12] Pachet F., Westermann G., Laigre D.: Musical Data Minin g\nfor Electronic Music Distribution, Proc. of the ﬁrst Wedel-\nMusic conference, 2001.\n[13] PampalkE.,FlexerA.,WidmerG.: ImprovementsofAudio -\nBased Music Similarity and Genre Classiﬁcation, Proc. of\nthe 6thInt. Conf. onMusic Information Retrieval, 2005.\n[14] Pearce M.T., Conklin D., Wiggins G.A.: Methods for com-\nbining statistical models of music, inWiil U.K. (Ed.), Com-\nputer music modelling and retrieval, (pp. 295-312), Heidel -\nberg, Germany, Springer Verlag, 2005.\n[15] TzanetakisG.,CookP.: Musicalgenreclassiﬁcationof audio\nsignals,IEEETransactionsonSpeechandAudioProcessing,\nVol. 10, Issue 5, 293-302, 2002.\n[16] WhitmanB.,SmaragdisP.: CombiningMusicalandCultur al\nFeatures for Intelligent Style Detection, Proc. of the 3rd I nt.\nConf. onMusic Information Retrieval,2002.\n3http://www.ofai.at/˜elias.pampalk/ma\n4http://www.ncrg.aston.ac.uk/netlab\n5http://www.s2s2.org"
    },
    {
        "title": "An Integrated MIR Programming and Testing Environment.",
        "author": [
            "Jörg Garbers"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1414864",
        "url": "https://doi.org/10.5281/zenodo.1414864",
        "ee": "https://zenodo.org/records/1414864/files/Garbers06.pdf",
        "abstract": "The process of shaping a music information retrieval algo- rithm is highly connected with implementing it and test- ing suitable parameterizations. Often music information re- trieval scientists do not have a programmer at hand and must implement their experimental setup themselves. This paper describes an integrated tool setup OHR consisting of the mu- sic (analysis) systems OpenMusic, Humdrum and Rubato and a system for form based parametrization and compari- son of algorithms. These packages and their programming environments provide the scientist with frameworks and ex- isting libraries for implementing and testing algorithms. They differ in the programming languages that they support and in the type of testing user interfaces that they allow the sci- entist to build easily. The systems and their components are integrated by using their scripting languages. We sketch an example of the integrated use of these systems. Keywords: Computational music analysis, development en- vironments, scientific programming, software integration",
        "zenodo_id": 1414864,
        "dblp_key": "conf/ismir/Garbers06",
        "keywords": [
            "music information retrieval",
            "algorithm implementation",
            "parameterization",
            "OpenMusic",
            "Humdrum",
            "Rubato",
            "form-based",
            "comparative testing",
            "scientific programming",
            "software integration"
        ],
        "content": "An Integrated MIR Programming and Testing Environment\nJ¨org Garbers\nDepartment of Computer and Information Sciences\nUtrecht University\ngarbers@cs.uu.nl\nAbstract\nThe process of shaping a music information retrieval algo-\nrithm is highly connected with implementing it and test-\ning suitable parameterizations. Often music information re-\ntrievalscientistsdonothaveaprogrammerathandandmust\nimplement their experimental setup themselves. This paper\ndescribesanintegratedtoolsetup OHRconsistingofthemu-\nsic (analysis) systems OpenMusic ,Humdrum andRubato\nand a system for form based parametrization and compari-\nson of algorithms. These packages and their programming\nenvironmentsprovidethescientistwithframeworksandex-\nistinglibrariesforimplementingandtestingalgorithms. They\ndiffer in the programming languages that they support and\nin the type of testing user interfaces that they allow the sci-\nentisttobuildeasily. Thesystemsandtheircomponentsare\nintegrated by using their scripting languages. We sketch an\nexample of the integrated useof these systems.\nKeywords: Computationalmusicanalysis,developmenten-\nvironments, scientiﬁc programming, software integration\n1. Introduction\nThe process of shaping a music information retrieval algo-\nrithm is highly connected with implementing it and test-\ning it. Algorithm implementation is usually done in one\nor more general purpose programming language(s) with or\nwithoutthehelpofanintegratedprogrammingenvironment\nandwithrespecttoexistinglibraries. Effectivetestinghow-\never requires the scientist him/herself to build a time con-\nsuminggraphicalortextbasedinterfaceortoembedhis/her\nalgorithmintoexistingtestingsystems. Thatiswhereexper-\nimentingenvironmentscomeinthatprovideuswithwaysof\nparameterizing algorithms, of applying them to data and of\nvisualizing results.\nWe give a short overview of the systems OpenMusic ,\nHumdrum ,RubatoR/circlecopyrtandtheirintegration OHR.Thesystems\nhave their speciﬁc strengths and weaknesses, technical bar-\nriersandprogrammingskillsrequirements. Weshowhowto\nintegrate them, so that one can use both the systems’ inter-\nactive parameterization front ends and their functional back\nPermissiontomakedigitalorhardcopiesofallorpartofthisworkfor\npersonal or classroom use is granted without fee provided that copies\narenotmadeordistributedforproﬁtorcommercialadvantageandthat\ncopies bear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2006 University of Victoriaends. Then we describe the general purpose resource com-\nparisonframeworkofthe Reisewissen projectanditsability\nto call into scriptable systems. Finally, we give an example\nof using the resulting system as an experimentation frame-\nwork for comparing similarity algorithms.\n2. OHR\nTheOHRsystem was an effort to integrate OpenMusic [1],\nHumdrum [2] andRubato[3], and to make each system’s al-\ngorithmsreusablefromeachoftheothersystems. Whyand\nhow this was done is described in detail in [4, 5].\nEssentially the approach uses the scripting facilities of\neach of the systems, which make available expression eval-\nuatorsthat are close or identical to the components imple-\nmentation languages:\n•TheHumdrum toolscanbeinvokedby(remotely)eval-\nuating shell scripts. This allows users as well as the\nother systems to access and ﬁlter a large database of\nmusicalscores. Humdrumuserscanhelpotherscien-\ntists to construct such scripts.\n•Rubato’s algorithmsandcomponentscanbeaccessed\nviaFScript,whichprovideswrappersfor Apple’sCo-\ncoalibraries and user deﬁned Objective-C classes.\nBesides simple evaluation of scripts, external mod-\nules can also change the state of the running Rubato\napplication. Scientists who have a Macintosh com-\nputer can build their testing graphical user interfaces\nwith Apple’s free XCodeIDE andInterface Builder .\n•Functions that are implemented by the user in Open-\nMusicasgraphical Patchesorordinary Lispfunctions\ncan be called via the underlying Lispsystem.\nInitsextendedversionin OHROpenMusic comeswithrou-\ntines for generating scripts and for sending them to Hum-\ndrumorRubato. This allows users to graphically deﬁne\nfunctions that use arbitrary functionality of the three sys-\ntems. So scientist are not required to have other program-\nming environments but OpenMusic . From there they can\nuse the other systems’ components in ways that are often\nnot anticipated by their developers.\n3. The Reisewissen testing system\nTheReisewissen testing system is described in detail in [6].\nIn short its graphical user interface allows users to comparethe performance of parameterizable evaluators and combi-\nnations of their results with respect to a list of resources\n(e.g. scores). The results are presented in an ( evaluator x\nresource)table. Eachevaluatorinstancecomeswithitsown\nwindowinwhichtheuserenterstheparametersandinspects\ndetails of the results. This setup is not only appropriate for\ntesting within the hotel booking domain [7], where the tool\nwas developed, but can also be used to do music similarity\nstudies: An evaluator implements a melodic, harmonic or\nrhythmic distance between two scores. The query score is\ngivenasaparameterandcomparedwiththedatabasescores.\nThe distances are presented in the table.\nWriting an evaluator with a new form based front end is\nrelativelysimpleandrequiresonlybasicknowledgeof Java.\nSwingcallsorHTMLcodeareautomaticallygeneratedfrom\nthe programmer’s form description, so scientists can con-\ncentrate on the algorithm. However, to use the system the\nscientistisnotrequiredtoknow Java. Insteadtherearegen-\neral purpose scripting connectors for SWI-Prolog andLisp\nbased systems. The latter allows us to call as a function any\nPatchfromOpenMusic , including Patchesthat callRubato,\nHumdrum or one of the IRCAMsynthesis applications that\nare callable from OpenMusic .\n4. Example\nTo build and test a new complex similarity deﬁnition we\nmight proceed as follows:\n•To exclude ornaments from comparison, we apply a\nmetrical ﬁlter. Therefore we use Humdrum’s metpos\ncommandor Rubato’sMetroRubette [8]. Notesonon-\nsets with low metrical proﬁle are omitted.\n•Humdrum’s yank andextractare used to create a re-\nduction of the score segments to be compared.\n•Rubato’s HarmoRubette [9] is used to compute a\nfunctional harmonical analysis of the reduced scores.\nWe play with parameters in the interactive user in-\nterfaceandstoredifferentharmonictheoryconﬁgura-\ntions.\n•OpenMusic is used as a front end to deﬁne distance\nfunctionsandbindeverything together: Weplaywith\nseveral deﬁnitions for the distance between two har-\nmonic sequences. A distance Patch takes two named\nscore ﬁles and produces a real number. To test this\npatch quickly, some score ﬁles are named directly in-\nside atesting Patch and thedistance Patch is applied\nto them via mapcar, with one bound parameter.\n•We run our hybrid algorithm from the Reisewissen\ntestingsystem: Tosimplifythevisualcomparison,we\nmay preorder the scores according to our similarity\nexpectations. We use the system to compare our re-\nsults with the results of existing similarity implemen-\ntations, such as [10].5. Outlook\nThe described systems will be used and improved within\ntheWITCHCRAFT projectatthe UtrechtUniversity andthe\nMeertens Instituut . We will integrate them into the MUU-\nGLE project (see poster by Martijn Bosma). We will evalu-\natetheintegratedsystemanditscomponentswithrespectto\nscientiﬁc programming needs and its usability for end users\nandpowerusers. Onthebasisofcomputationalexperiments\non music we hope to stimulate the discussion between mu-\nsictheoristsandthemusicinformationretrievalcommunity.\nSee http://www.cs.uu.nl/research/projects/witchcraft.\n6. Acknowledgments\nWe want to thank those who have developed software that\nis used directly in this work: Guerino Mazzola, Oliver Za-\nhorka, Carlos Agon, Philippe Mougin, David Huron and\nMagnus Niemann. Thanks also to my new colleagues in\nUtrecht for feedback.\nReferences\n[1] C.Agon,G.Assayag,M.Laurson,andC.Rueda,“Computer\nassisted composition at Ircam: Patchwork & OpenMusic,”\nComputer Music Journal , 1998. [Online]. Available:\nhttp://www.ircam.fr/equipes/repmus/RMPapers/CMJ98/\n[2] D. Huron, “Music information processing using the Hum-\ndrum toolkit: Concepts, examples, and lessons,” Computer\nMusic Journal , vol. 26, no. 2, pp. 11–26, 2002.\n[3] G. Mazzola and O. Zahorka, “The RUBATO performance\nworkstation on NeXTSTEP,” ICMA (ed.): Proceedings of\nthe ICMC 94, S. Francisco , 1994.\n[4] J. Garbers, “Integration von Bedien- und Programmier-\nsprachen am Beispiel von OpenMusic, Humdrum und Ru-\nbato,” Ph.D. dissertation, Fakult ¨at IV – Elektrotechnik und\nInformatik der Technischen Universit ¨at Berlin, 2004.\n[5] ——, “User participation in software conﬁguration and in-\ntegration of OpenMusic, Humdrum and Rubato,” Lluis-\nPuebla, Emilio, Guerino Mazzola und Thomas Noll (eds.):\nPerspectives in Mathematical and Computer-Aided Music\nTheory, Verlag epOs-Music, Osnabr ¨uck, 2003.\n[6] J. Garbers and M. Niemann, “The Reisewissen testing sys-\ntem,” FU-Berlin, Tech. Rep., 2006, to be published at\nhttp://reisewissen.ag-nbi.de/en/.\n[7] J. Garbers, M. Niemann, and M. Mochol, “A personalized\nhotel selection engine,” Proceedings of the European\nSemantic Web Conference 2006 . [Online]. Available:\nhttp://www.eswc2006.org/poster-papers/FP15-Mochol.pdf\n[8] A.Fleischer,“DieanalytischeInterpretation,SchrittezurEr-\nschließung eines Forschungsfeldes am Beispiel der Metrik,”\nPh.D. dissertation, Philosophische Fakult ¨at der Humbold-\nUniversit¨at zu Berlin, 2002.\n[9] J. Garbers and T. Noll, “Harmonic path analysis,” Lluis-\nPuebla, Emilio, Guerino Mazzola und Thomas Noll (eds.):\nPerspectives in Mathematical and Computer-Aided Music\nTheory, Verlag epOs-Music, Osnabr ¨uck, 2003.\n[10] F. Wiering, R. Typke, and R. Veltkamp, “Transportation dis-\ntancesinmusicnotationretrieval,” ComputinginMusicology\n13, 113-128 , 2004."
    },
    {
        "title": "Web-Based Artist Categorization.",
        "author": [
            "Gijs Geleijnse",
            "Jan H. M. Korst"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1417579",
        "url": "https://doi.org/10.5281/zenodo.1417579",
        "ee": "https://zenodo.org/records/1417579/files/GeleijnseK06.pdf",
        "abstract": "We present a novel approach in categorizing artists into sub- jective categories such as genre. We base our method on co-occurrences on the web, found with the Google search engine. A direct mapping between artists and categories proved to be unreliable. We use the categories mapped to closely related artists to obtain a more reliable mapping. The method is tested on a genre classification test set with con- vincing results. Moreover, mood categorization is explored using the same techniques. Keywords: Artist categorization, web, Google.",
        "zenodo_id": 1417579,
        "dblp_key": "conf/ismir/GeleijnseK06",
        "keywords": [
            "novel",
            "approach",
            "categorizing",
            "artists",
            "sub-jective",
            "categories",
            "genre",
            "co-occurrences",
            "web",
            "Google"
        ],
        "content": "Web-Based Artist Categorization\nGijs Geleijnse JanKorst\nPhilips Research\nProf Holstlaan 4\n5656 AAEindho ven(theNetherlands)\nfgijs.geleijnse,jan.korst g@philips.com\nAbstract\nWepresent anovelapproach incategorizing artists intosub-\njectivecategories such asgenre. Webase ourmethod on\nco-occurrences ontheweb, found with theGoogle search\nengine. Adirect mapping between artists andcategories\nprovedtobeunreliable. Weusethecategories mapped to\nclosely related artists toobtain amore reliable mapping. The\nmethod istested onagenre classi\u0002cation testsetwith con-\nvincing results. Moreo ver,mood categorization isexplored\nusing thesame techniques.\nKeywords: Artist categorization, web, Google.\n1.Introduction\nWebservices inthemusic domain provide allkinds ofmeta-\ndata onmusic. Some meta-data isobjecti veandveri\u0002able,\nliketheyear ofrelease ofanalbumorthenationality ofan\nartist. Other meta-data concerns thecategorization ofmusic,\nsuch astheassignment ofagenre toanalbumoramood to\nasong. Although such information may bedebatable, itcan\nbehelpful forauser inselecting music.\nTheWorldWideWebprovides anoverwhelming amount\nofinformation onthemusic domain. Home pages ofartists,\nfanpages andalbumreviewsgiveinformation toidentify\ncategories (genres, moods) formusic. Evenifsome ofthe\nsources contradict each other ,weareoften able toselect the\nmost appropriate category.\nInthispaper ,wepresent amethod toautomatically cat-\negorize artists andtheir music intomoods andgenres. The\nmethod isbased onco-occurrences ontheeb.Weareinter-\nested in\u0002nding themost appropriate mapping from agiven\nsetofcategories.\nInearlier workonartist classi\u0002cation with web data the\nnumber ofGoogle hitsforqueries with twoartists wasused\n[12,15].Inthisworkwecompare such anapproach with\nmore ef\u0002cient methods with respect tothenumber ofGoogle\nqueries. Contrary toapproaches in[8,12],themethods in-\ntroduced here aresimple andunsupervised.\nPermission tomakedigital orhard copies ofallorpartofthisworkfor\npersonal orclassroom useisgranted without feeprovided thatcopies\narenotmade ordistrib uted forpro\u0002t orcommercial advantage andthat\ncopies bear thisnotice andthefullcitation onthe\u0002rstpage.\nc\r2006 University ofVictoriaThis paper isorganized asfollows. Inthenextsection\ntheproblem statement isgiven.Section 3handles three al-\nternati vemethods inacquiring amapping between artists\nandcategories. Since such amapping showed tobeunre-\nliable, weacquire additional information andusethisina\n\u0002nal mapping inSection 4.Twoexperiments arediscussed\ninSection 5:thecategorization ofartists into genres and\nintomoods. Related workcanbefound inSection 6andwe\nconclude inSection 7.\n2.Problem Description\nGivenaretwosetsAandL.ThesetAconsists ofnames of\nartists, andthesetLcontains categories e.g.genres. Also a\nmapping m:A!Lisgiven.Weassume thesetLtobe\ncomplete andconsequently weassume thateachuinAcan\nbemapped toatleast onevinL.\nDe\u0002nition. Wecallanelement m(u)2Lthemost ap-\npropriate category foru2Aifadomain expert would se-\nlectm(u)from thesetLasthecategory bestapplicable tou.\nProblem. Givenasetofartists Aandasetofcate-\ngories L,\u0002ndforeachu2Athemost appropriate mapping\nm(u)2L.\nWeusetheweb toextract such information andassume\nthatanartist categorization canbededuced from it.\n3.ThreeMethods forCategorizing Artists\nInthissection, wepresent three methods tocategorize artists\nusing web data. The\u0002rstmethod isbased onanalyzing the\nnumbers ofco-occurrences ofartists inAandcategories in\nLontheweb.Toretrie vethisdata weusetheGoogle search\nengine [4].\nTheestimated numbers ofGoogle hitscan\u0003uctuate which\nmay lead tounexpected results [14]. Another drawback of\nthismethod is,thatitrequires manyqueries toasearch en-\ngine. Forlargesets ofinstances thiscanbeproblematic,\nsince search engines currently allowonly alimited amount\nofautomated queries perday.\nInSections 3.2and3.3wepresent twoalternati vemeth-\nodsthatdonotsufferfrom these drawbacks.\n3.1. Page-count-based mapping (PCM)\nWeareinterested inapreliminary mapping m0.Toobtain\nsuch amapping weperform aGoogle query\"a\",\"g\"foreach pair(a;g)2A\u0002L.Perquery ,weextract theestimated\nnumber ofhitsco(a;b).\nco(a;b)=`thenumber ofhitsforquery a,b'\nWeassume thattheorder ofaandbinthequery does not\neffectthenumber ofhits, thus co(a;b)equals co(b;a).\nThis Page-Count-based Mapping (PCM)issimple andin-\ntuitive.Ifweareforexample interested incategorizing\nmusic artist into genres, weanalyze thenumber ofhitsto\nqueries forcombinations ofthenames oftheartist andeach\ngenre. Assuming Johnn yCash tobeacountry artist, weex-\npect thatmore documents contain both theterms Country\nandJohnn yCash than ReggaeandJohnn yCash .\nPera2Awecould map theg2Lwith themost hits.\nHowever,weobserv ethat frequently occurring categories\ninLhavealargerprobability tobemapped toanyartist in\nA.Forexample, thequery `Pop'results in8times more\nhitsthan thequery 'Disco '.Although weconsider Bone y\nMasadisco-act, thequery Bone yM,popgivestwice the\namount ofhitsasBone yM,disco .This observ ation leads\ntoanormalized approach, inspired bythepointwise mutual\ninformation theory [9].\nFora2Aandg2L,wede\u0002ne ascoring function\nS(a;g)between thetwoasfollows.\nS(a;g)=co(a;g)\n1+P\nb2Aco(b;g)(1)\nInthedenominator weadd1tothesum ofallco-occur-\nrences withgtoavoiddividing by0.\nNowwehavecomputed thescores forallpairs, weselect\napreliminary mapping m0foralla2A.Perartist weselect\nthecategoryg2Lwith thehighest scoreS.\nm0(a)=argmaxh2LS(a;h) (2)\nUsing PCMwethus need toperform jAj\u0001jLjqueries.\n3.2. Patter n-based mapping (PM)\nThis mapping (PM)isbased onoccurrences ofterms inphra-\nsesontheweb.Weobserv ecombinations ofterms inphrases\nthat express therelation weareinterested in.Forexam-\nple,ifweareinterested intherelation between artists and\ntheir genres, anappropriate phrase thatlinks terms ofthe\ntwocould be`(artist) isoneofthebiggest (genre) artists' .\nThe method works asfollows. Wecompose anumber\nofpatterns thatexpress thecategorization oftheartists inA\nintocategories ofL.Wecaneither compose these patterns\nmanually ,oruseatraining settoautomatically \u0002ndpatterns\n[7].\nWecombine thepatterns with aninstance ofeither oneof\nthearguments oftherelation itre\u0003ects. Weusethese combi-\nnations ofapattern andaninstance asaquery tothesearch\nengine [7]. Forexample, ifwehavethepattern (genre)artist such as(artist) ,weuseitinqueries incombinations\nwith allnames ofgenres andartists. Forexample, weuse\nthispattern both forthequery Country artists such asand\nforthequery artists such asPrince .Intheresults forthe\n\u0002rstquery ,weidentify artists inA,while intheresults for\nthesecond query wesearch forgenres inLrelated toPrince .\nThese queries provide access torelevantdata. From the\nexcerpts returned bythesearch engine, wethus identify the\nelements ofeither AorLtomeasure thenumber ofco-oc-\ncurrences ofthepairs. Hence, using PMco(a;b)isde\u0002ned\nasfollows.\nco(a;b)= `number ofoccurrences ofa\nwhen querying patterns containing b'+\n`number ofoccurrences ofb\nwhen querying patterns containing a'\nUsing PMweonly needm\u0001(jAj+jLj)queries, withmthe\nnumber ofpatterns expressing themapping m.Weusethe\nsame scoring function (1)toobtain apreliminary mapping\n(2).\n3.3. Document-based mapping (DM)\nIntheDocument-based Mapping (DM)wecollect the\u0002rst\nkURLsofthedocuments returned bythesearch engine for\nsome query .These kURLsarethemost relevantforthe\nquery submitted based ontheranking used bythesearch\nengine [4].\nInthe\u0002rstphase ofthealgorithm, wequery allelements\ninbothAandLandcollect thetopkdocuments foreach\nofthequeries. Fortheartists inA,weretrie veeach docu-\nment using the URLsfound bythesearch engine. Wecount\ntheoccurrences ofthecategories inLintheretrie veddoc-\numents fortheintermediate mapping m0.From thedocu-\nments retrie vedwith aninstance g2L,wesimilarly extract\ntheoccurrences ofartists inA.\nThedocuments obtained using DMarethemost relevant\nforeach element a2A.Fortheartists queried weex-\npect fanpages, thehome page oftheartist, entries inmusic\ndatabase sites andsoon.Thegenres orstyles mentioned in\nthese pages willmost probably re\u0003ect theartist queried.\nTheco-occurrences function ishere thus de\u0002ned asfol-\nlows.\nco(a;b)= `number ofoccurrences ofa\nindocuments found with b'+\n`number ofoccurrences ofb\nindocuments found with a'\nTheco-occurrences between artists andgenres againare\nused foranintermediate mapping using thesame scoring\nfunction.\nThis method requires onlyjAj+jLjqueries. However,\nadditional data communication isrequired since each ofthe\ndocuments hastobedownloaded instead ofusing only the\ndata provided bythesearch engine.4.Impr oving Precision using Additional\nInformation\nSince thepreliminary mapping showed tobeunreliable (see\nSection 5),weobserv etheneed foradditional data. Weuse\ntheassumption thatrelated artists often share thesame cat-\negory.Theother wayaround, iftwoartists areboth known\nforthesame category (e.g. romantic music), weexpect them\ntooccur often inthesame conte xt.Wethus usetheworking\nhypothesis thatstrongly related artists ingeneral arecatego-\nrized with thesame element inL.\nWeacquire co-occurrences between artists (Section 4.1)\nusing PCM,PMorDMasdescribed intheprevious section.\nWeusethisinformation ina\u0002nal mapping m(Section 4.2).\n4.1. Finding co-occur ences between artists\nWeconsider artists toberelated, when theyfrequently occur\ninthesame conte xt.Themethods PCM,PMand DMcanbe\nused togather numbers ofco-occurrences co(a;b)between\nartists aandb.\nPerpair(a;b)2A\u0002Awecompute thescoreT,similar\ntothescoreSin(1).\nT(a;b)=co(a;b)\n1+P\ny;y6=aco(a;y)\u0001P\nx;x6=bco(x;b)(3)\nThe scoring function Tissymmetric initsarguments.\nAgain,wedonotuseamajority voting topreventfrequently\noccurring instances tobestrongly related tomanyother in-\nstances. Forexample, anartist likeMadonna isexpected to\nco-occur alotwith manyother artists, duetolargenumber\nofweb pages mentioning Madonna (26million hits).\nInPCMwecombine thenames oftwoartists intoaquery\nandextract thenumber ofhits.Using thismethod thisphase\nrequires jAj2queries. Thetotal number ofGoogle queries\nforthePCMmethod isthusjAj\u0001(jAj+jLj).\nIfweuse PMtoobtain thenumbers ofco-occurrences be-\ntween artists, wecanspecify therelatedness between artists.\nForexample, wecanbesolely interested inartists who have\nplayed together .Apattern such as(artist) recorded aduet\nwith (artist) could besuitable forthispurpose. This phase\nofthemethod consists ofk\u0001jAjqueries (withkthenumber\nofpatterns), leading toatotal Google comple xityofjAj+jLj\nInthedocuments obtained with the DMmethod weonly\nexpect names ofother artists that arestrongly connected\nwith theartist queried. Wereuse thedocuments obtained\nbyquerying theartists inthe\u0002rstphase. This method thus\nrequires jAj+jLjqueries intotal.\n4.2. Combine results in\u0002nal mapping\nWecombine thescores Twith thepreliminary mapping m0\nasfollows.Perartista,weinspect m0todetermine thecate-\ngory thatisassigned most often toaanditsnclosest related\nartists. Wethus expect thatthemost appropriate categoryvforaismost often mapped bym0among thenearest neigh-\nbors toa.\nPerinstance a2A,weconstruct anordered listwitha\nanditsnnearest neighbors, A=(a0;a1;:::;an)witha=\na0asits\u0002rst element. ForeachaiinAwithi>0holds\nT(a;ai)\u0015T(a;ai+1).\nFora\u0002nal mapping mofartists atoacategory inL,we\ninspect themost occurring category mapped bym0toaand\nitsnnearest neighbors.\nm(a)=argmaxh2L(X\n0\u0014i\u0014nI(ai;h))\nwith\nI(ai;h)=\u001a1ifm0(ai)=h\n0otherwise.\nIftwocategories haveanequal score, weselect the\u0002rst\noccurring one. That is,thecategory thatismapped bym0to\naortotheartist most related toa.\n5.Experiments\nWepresent twoexperiments ofourmethod inthemusic do-\nmain.\nInthe\u0002rst, wecategorize alistof224artist intogenres\n[8].Contrary toprior work[8,12],wedonotcluster related\nartists, butareinterested inamapping ofartists togenres.\nWethus explicitly label each artist with agenre.\nInthesecond experiment wemap artists tothemood as-\nsociated with their music using webdata. Toourbestknowl-\nedge, noprevious workaddresses thisissue. Based ondata\nofMoodLogic [10],weassign moods toalistofartists using\nthemethod presented.\n5.1. Genr ecategorization\nWeadded allnames ofartists inthelistcomposed byKnees\netal.[8]tothesetA.This listconsists of14genres, each\nwith 16artists.\nThe genres mentioned inthelistarenotallsuitable for\n\u0002nding co-occurrences. Forexample, theterm classical is\nambiguous andAlternati veRock/Indie isnotaterm fre-\nquently used. Wetherefore manually rewrote thenames of\nthegenres intounambiguous ones (such asclassical music )\nandadded some synon yms. After collecting thenumbers of\nco-occurrences between artists andgenres, wesummed up\nthescores oftheco-occurrences forsynon yms. Thus, for\neach artistathenumber ofco-occurrences with theterms\nIndie andAlternati verock areadded totheco-occurrences\nofawith thegenre Alternati veRock/Indie .\nWeperformed theexperiment three times, using each of\nthemethods described toobtain theco-occurrences.\nMoti vated bytheresults in[12],for PCMweused the\nallintitle option. Wealso added theextraterm music\nfor\u0002nding co-occurrences between theartists. Forexample\ntheterms Bush andInner Circle co-occurred alotonthe\"#1(artists ORbandsOR\nactsORmusicians) like#0\"\n\"#1(artists ORbandsOR\nactsORmusicians) suchas#0\"\n\"#1(artists ORbandsORacts\nORmusicians) forexample #0\"\n\"#0andother#1(artists\nORbandsORactsORmusicians)\"\nTable 1.The fourpatter nsforartist (#0) -genr e(#1) relation.\n\"like#0and#1\"\n\"suchas#0and#1\"\n\"including #0and#1\"\n\"forexample #0and#1\"\n\"namely #0and#1\"\n\"#0and#1\"\n\"#0#1andother\"\nTable 2.patter nsforartist -artist relation.\nweb, duetoAmerican politics. Byadding theterm music\nwerestrict ourselv estodocuments handling music.\nForPMweselected forthegenre-artist relations thepat-\nterns inTable 1from alistofpatterns found expressing\nthisrelation [7]. Weconsidered twoartists related, ifthe\ntwoco-occur insome enumeration. Since wearenotin-\nterested inthenature oftherelatedness between artists, we\nselected general enumeration patterns (Table2)toobtain co-\noccurrences.\nInTable 3theperformance ofthepreliminary mapping\ncanbefound forthethree methods (n=0).Wewere able\ntomap allartists toagenre. Co-occurrences between genres\nandartists thuscould befound using PCM,PMaswell asDM.\nThe latter performs best. Withrespect tothepreliminary\nmapping, themethod with thesmallest amount ofGoogle\nqueries performs best. The data found onthebest ranked\ndocuments isthus reliable.\nUsing DMonly fewrelated artists canbefound onthe\ndocuments visited. This leads toastable performance for\nthe\u0002nal mapping when expanding thelistofrelated artists\n(Figure 1).That is,weonly consider artists thatco-occur at\nleast once. Contrary toespecially PCM,largenumbers ofn\ndonotdeteriorate theprecision.\nThe performance ofthepattern-based method strongly\nmethod n=0 best (corresponding n)\nPCM 71.4 81.3 (13)\nPM 72.2 88.8 (8)\nDM 83.9 87.1 (5)\nTable 3.Precision (%) without related artists and best preci-\nsion permethod.impro vesbyconsidering related artists, thebestperformance\nisobtained forn=8.Allmethods perform best forvalues\nofnbetween 5and13.TheRock n'Roll artists provedtobe\nthemost problematic tocategorize. Theartists inthegenres\nclassical ,blues andjazz were allcorrectly categorized with\nthebestscoring settings.\nWiththesupervised method discussed in[8]aprecision\nof87% wasobtained using comple xmachine learning tech-\nniques andarelati velylargetraining set.In[12]aprecision\nofupto85% precision wasobtained using O(jAj2)Google\nqueries. Wecanconclude thatoursimple andunsupervised\nmethod produces similar results. Aimportant observ ation\nis,that thebest scoring methods (document- andpattern-\nbased) aretheones performing onlyO(jAj+jGj)Google\nqueries. Our approach isthus well suited forcategorizing\nlargersetsofartists.\n5.2. Mood categorization\nApart from agenre, music meta-data providers often asso-\nciate amood with anartist, song oralbum.MoogLogic [10]\ndistinguishes sevenmoods: upbeat ,happ y,sad,brooding ,\naggressi ve,sentimental andromantic .Contrary toe.g.AMG\n[1],with each song only onemood isassociated. Inthissec-\nondexperiment, weinvestig atewhether thesame methods\ncanbeused without adaptations toidentify themood ofthe\nmusic ofsome artist. Wehere usetheassumption thatfrom\nthelistofsevenmoods amost appropriate onecanbeas-\nsigned toeach artist.\nSince noprior workisknowntousinthis\u0002eld, wehave\ncomposed twotestsetsourselv es1,based ondatafrom Mood-\nLogic. Weconducted theexperiment onasetof230artists.\nThesetup wasequal tothe\u0002rstexperiment. Weonly used\ndifferent patterns fortheidenti\u0002cation ofco-occurrences be-\ntween moods andartists (Table 4).\nWeevaluated theperformance using twosetsets. The\n\u0002rstsetcontains foreach ofthesevenmoods thetwoartists\nthatcorresponded bestwith amood. Forthesecond testset,\n1Seehttp://gijsg.dse.nl/ismir06\n 70 72 74 76 78 80 82 84 86 88 90\n 0  5  10  15  20  25  30  35  40precision\nnumber of neighbors consideredartist-genre categorization\ndm\npm\npcm\nFigur e1.Precision forgenr ecategorization.\"#1(moodORtunesORstyle)by#0\"\n\"#1(moodORtunesORstyle)like#0\"\n\"#1(moodORtunesORstyle)of#0\"\n\"#0's#1\"\nTable 4.Patter nsforthemood (#1) -artist (#0) relation.\nweused alessstrict criterion. Ifatleast 70% ofthesongs of\nanartist wasassociated with onemood, weadded ittothe\ntestset.This second testsetcontains 47artists.\nFigures 2and3showtheperformance ofthethree meth-\nodsinmood categorization. These tests indicate that DM\nagainisthemost constant andreliable. Itseems that PCMis\nunsuited forcategorizing artists into moods. Although the\nresults arenotasconvincing asthegenre categorization, es-\npecially DMperforms signi\u0002cantly better than thebaseline\nof14%, or1outof7correct.\nEventhough thedata collected byDMissparse (e.g. no\nmood could beassigned toR.E.M.), thistestshowsthatthis\nmethod isthemost reliable inmood categorization. The\nperformance ofPMcanbeexplained bytheobserv ation that\nanartist andmoods from thislistoccur rarely within asen-\ntence. Thehypothesis thattaking related artists intoaccount\nwill reduce errors inthecategorization only holds for DM.\nThis technique compensates fortheartists forwhich nopre-\nliminary mapping could befound.\nThe overall performance could beimpro vedbyadding\nsynon yms forthemoods inthelist. Ontheonehand the\nterms brooding andupbeat arerare andontheother hand\nhapp yandsaddonotalwaysre\u0003ect themood ofmusic. Ap-\nparently ,therelati vescoring function does notcompensate\nforthis.\nUnlik ethemodel with sevendistinct moods used inMood-\nLogic, multiple moods seem tobeapplicable toasingle\nartist oreventoasingle song. Especially thedistinction be-\ntween upbeat andhapp ysometimes seems arbitrary .How-\never,weexpect asong nottobeboth aggressi veandroman-\ntic.AMG assigns multiple moods toartists, forexample 21\nofthese moods apply toMadonna .The document-based\nmethod givesrisetoresearch theassignment ofmultiple\nmoods tosongs orartists.\n6.Related Work\nWeobserv etwoareas ofrelated work: research related to\nweb-based relation identi\u0002cation andworkonartist classi\u0002-\ncation with web data.\nEarly workonrelation identi\u0002cation from theweb canbe\nfound in[3].Brin describes awebsite-dependent approach\ntoidentify hyperte xtpatterns thatexpress some relation. For\neach web site, such patterns arelearned andexplored to\nidentify instances thataresimilarly related. Theidea ofus-\ningpatterns forrelation extraction issimilar toPM,although\ntheextracted relation arenotevaluated. 0 10 20 30 40 50 60 70 80\n 0  5  10  15  20precision\nnumber of neighbors consideredartist-mood categorization small set\ndm\npm\npcm\nbaseline\nFigur e2.Precision formood categorization with small set.\n 8 10 12 14 16 18 20 22 24 26\n 0  5  10  15  20precision\nnumber of neighbors consideredartist-mood categorization large set\ndm\npm\npcm\nbaseline\nFigur e3.Precision formood categorization with largerset.\nCimiano andStaab descibe amethod touseasearch en-\ngine toverify ahypothesis relation [6].Forexample, ifwe\nareinterested inthe`isa'orhypon ymrelation andwehave\nacandidate instance pair (Nile,river)forthisrelation, we\ncanuseasearch engine toquery phrases expressing thisre-\nlation (e.g. riverssuch astheNile ).The number ofhits\ntosuch queries isused todetermine thevalidity ofthehy-\npothesis. Contrary toourmethod, amajority voting isused.\nIn[11] various techniques arediscussed toidentify relations\nbetween concepts from theweb foraquestion answering\nsystem.\nIn[2]anumber ofdocuments onartstyles arecollected.\nNames ofpainters areidenti\u0002ed within these documents.\nThe documents areevaluated bycounting thenumber of\npainters inatraining set(ofe.g.expressionists )thatappear\ninthedocument. Painters appearing onthebestrankeddoc-\numents then aremapped tothestyle. This methods differs\nfrom DMintwoaspects. First wedonotcollect aconstant\namount ofweb pages, butwecollect web pages forallel-\nements inthesetsAandL.Secondly ,DeBoer etal.use\natraining setandpage evaluation, where wejustobserv e\nco-occurrences.\nThenumber ofGoogle hitsforpairs ofterms canbeusedtocompute asemantic distance between terms [5].Thena-\nture oftherelation isnotidenti\u0002ed, butthetechnique can\nforexample beused tocluster painters. In[15]asimilar\nmethod isused tocluster artists using search engine counts.\nIn[12], thenumber ofGoogle hitsofcombinations of\nartists isused inclassifying artists. InPCMweusethe\nsame techniques toobtain these \u0002gures, butdonotusema-\nchine learning techniques tointerpret them. Moreo ver,we\nmap artists tocategories instead ofclustering them. Co-\noccurrences between artists using search engine hitscanalso\nbeused todisco verprototypical artists pergenre [13].\nAdocument based technique inartist classi\u0002cation isde-\nscribed in[8]. Forallartists, anumber ofdocuments is\ncollected using asearch engine. Forsetsofrelated artists\nanumber ofdiscriminati veterms islearned. These terms\nareused toclassify theother artists using support vector\nmachines. The documents areobtained inasimilar way\ninourdocument-based method. However,werestrict our-\nselvestoidentifying names ofartists andcategories onthe\ndocuments.\n7.Conclusions andFutur eWork\nWehavediscussed three alternati vemethods toobtain co-\noccurrences between terms using asearch engine. These\nmethods areapplied togainapreliminary mapping between\nartists andcategories such asgenre. When related artists\nshare thesame category,themutual distance between artists\ncanbeused toobtain amore reliable mapping.\nThe three alternati vesused haveadifferent comple xity\nwith respect tothenumber ofqueries toasearch engine.\nThemethod using patterns andtheoneusing complete doc-\numents arelinear tothenumber ofitems inthesetsofartists,\nwhere thepage-count-based mapping isquadratic. This dis-\ntinction isimportant forcategorizing largesets ofartists,\nsince search engines allowonly alimited amount ofauto-\nmated queries perday.\nInthe\u0002rstexperiment weshowed thatwecanprecisely\ncategorize artists togenres, where themost ef\u0002cient meth-\nodswith respect totheGoogle comple xityperform best.\nThesecond experiment consisted ofthemapping ofartists\ntothemoods associated with their music. This novelap-\nproach inartist categorization hadencouraging results, but\nisopen toimpro vement. Thedocument-based method seems\nbestsuited tocategorize artists intomoods.\nThetwoexperiments showed thatthemethods with least\namount ofqueries givethebest results. Weshowed that\nsimple andunsupervised methods canbeused forareliable\ncategorization. Therefore, these methods arewell suited to\nbeapplied tolargerdata-sets.\nInfuture work, wewanttofurther explore the\u0002eld of\nautomatically categorizing music into moods. Wewill in-\nvestig atemethods tocategorize songs oralbums rather than\nartists. Moreo ver,themoods identi\u0002ed intheexperiment\narelesssuited, since theterms areambiguous. Automaticidenti\u0002cation ofterms associated with moods (analogous to\nworkin[8])isaninteresting option toexplore. Finally ,we\nplan totestthemethods using aricher categorization, since\nwecurrently only identi\u0002ed afewbroad genres andmoods.\nThemeta-data onmoods provided byAMG canbeused as\nabenchmark.\nRefer ences\n[1]All Music Guide (AMG). website.\nhttp://www .allmusic.com.\n[2]V.d.Boer ,M.v.Someren, andB.J.Wieling a.Extracting in-\nstances ofrelations from web documents using redundanc y.\nInProceedings oftheThirdEuropean Semantic WebConfer -\nence (ESWC'06) ,Budv a,Montene gro,June 2006.\n[3]S.Brin. Extracting patterns andrelations from theworld\nwide web.InWebDB Workshop atsixth International\nConfer ence onExtending Database Technolo gy(EDBT'98) ,\n1998.\n[4]S.Brin andL.Page. The anatomy ofalarge-scale hyper-\ntextual web search engine. Computer Networks andISDN\nSystems ,30(17):107117, 1998.\n[5]R.Cilibrasi andP.Vitanyi.Automatic meaning disco very\nusing Google. http://www .cwi.nl/paulv/papers/amdug.pdf.\n[6]P.Cimiano andS.Staab .Learning byGoogling. SIGKDD\nExplor ations Newsletter ,6(2):2433, 2004.\n[7]G.Geleijnse andJ.Korst. Learning effectivesurfacetext\npatterns forinformation extraction. InProceedings ofthe\nEACL2006 workshop onAdaptive TextExtraction andMin-\ning(ATEM 2006) ,pages 18, April 2006.\n[8]P.Knees, E.Pampalk, andG.Widmer .Artist classi\u0002ca-\ntion with web-based data. InProceedings of5thInter -\nnational Confer ence onMusic Information Retrie val(IS-\nMIR'04) ,pages 517524, Barcelona, Spain, October 2004.\n[9]C.D.Manning andH.Sch¨utze. Foundations ofStatistical\nNatur alLangua geProcessing .TheMIT Press, Cambridge,\nMassachusetts, 1999.\n[10] MoodLogic. website. http://www .moodlogic.com.\n[11] D.Ravichandran. Terascale Knowledg eAcquisition .PhD\nthesis, University ofSouthern California, 2005.\n[12] M.Schedl, P.Knees, andG.Widmer .AWeb-Based Ap-\nproach toAssessing Artist Similarity using Co-Occurrences.\nInProceedings ofthe Fourth International Workshop\nonContent-Based Multimedia Indexing (CBMI'05) ,Riga,\nLatvia, June 2005.\n[13] M.Schedl, P.Knees, andG.Widmer .Disco vering andVi-\nsualizing Prototypical Artists byWeb-based Co-Occurrence\nAnalysis. InProceedings oftheSixth International Confer -\nence onMusic Information Retrie val(ISMIR'05) ,London,\nUK, September 2005.\n[14] J.V´eronis. Weblog, 2006. http://aixtal.blogspot.com.\n[15] M.Zadel andI.Fujinag a.Webservices formusic informa-\ntion retrie val.InProceedings of5thInternational Confer -\nence onMusic Information Retrie val(ISMIR'04) ,Barcelona,\nSpain, October 2004."
    },
    {
        "title": "Efficient Lyrics Extraction from the Web.",
        "author": [
            "Gijs Geleijnse",
            "Jan H. M. Korst"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1415982",
        "url": "https://doi.org/10.5281/zenodo.1415982",
        "ee": "https://zenodo.org/records/1415982/files/GeleijnseK06a.pdf",
        "abstract": "We present a novel method to extract lyrics from the Web. The aim is to extract a set of multiple versions of the lyrics to a song. Lyrics can be identified within a text by a regular expression. We use a projection of a document to efficiently identify lyrics within the document by mapping it to a regu- lar expression. We describe a method to cluster the multiple versions of the lyrics by filtering out erroneous texts such as lyrics to other songs. For reasons of efficiency, we do this by comparing fingerprints instead of the texts themselves. Keywords: Lyrics, Web, Google, Regular Expressions.",
        "zenodo_id": 1415982,
        "dblp_key": "conf/ismir/GeleijnseK06a",
        "keywords": [
            "novel",
            "method",
            "extract",
            "lyrics",
            "Web",
            "identify",
            "text",
            "regular",
            "expression",
            "cluster"
        ],
        "content": "Ef\u0002cient Lyrics Extraction fromtheWeb\nGijs Geleijnse JanKorst\nPhilips Research\nProf Holstlaan 4\n5656 AAEindho ven(theNetherlands)\nfgijs.geleijnse,jan.korst g@philips.com\nAbstract\nWepresent anovelmethod toextract lyrics from theWeb.\nTheaimistoextract asetofmultiple versions ofthelyrics\ntoasong. Lyrics canbeidenti\u0002ed within atextbyaregular\nexpression. Weuseaprojection ofadocument toef\u0002ciently\nidentify lyrics within thedocument bymapping ittoaregu-\nlarexpression. Wedescribe amethod tocluster themultiple\nversions ofthelyrics by\u0002ltering outerroneous textssuch as\nlyrics toother songs. Forreasons ofef\u0002cienc y,wedothis\nbycomparing \u0002ngerprints instead ofthetextsthemselv es.\nKeywords: Lyrics, Web,Google, Regular Expressions.\n1.Introduction\nPeople areinterested inthelyrics ofthesongs theylisten to.\nInstead ofsearching forlyrics ontheweb ourselv es,weare\ninterested whether wecangather lyrics automatically .This\ncanbeanattracti vefeature within anaudio device. Ifthe\nextraction isdone fast,thelyrics ofasong canbedisplayed\nwhile playing.\nEvilL yrics1automatically extracts lyrics from anumber\noflargelyrics depot sites. Unlik eourmethod, EvilL yrics\nmakesuseofthehomogeneous structure ofthepages within\nthese websites [1].\nIn[3]amethod isintroduced toautomatically extract\nlyrics from theweb.This method isthe\u0002rsttoextract lyrics\nfrom heterogeneous sources. Lyrics areidenti\u0002ed within\nweb pages using multiple string alignment. The authors\nshowed thatthisisaneffectivemethod toacquire correct\nversions, howeverthemethod itself hasahigh time and\nspace comple xity.Another drawback isthatthemethod as-\nsumes multiple distinct documents containing thelyrics.\nContrary to[3]wearenotinterested inidentifying acor-\nrectversion ofthelyrics, butinanumber ofpossible dif-\nferent versions ofthelyrics tothesong asoccurring onthe\nWeb.Differences occur duetypo' sandmisheard words, the\nomission ofarepeating chorus, theinsertion ofline and\n1http://www.evillabs.sk/evillyrics/\nPermission tomakedigital orhard copies ofallorpartofthisworkfor\npersonal orclassroom useisgranted without feeprovided thatcopies\narenotmade ordistrib uted forpro\u0002t orcommercial advantage andthat\ncopies bear thisnotice andthefullcitation onthe\u0002rstpage.\nc\r2006 University ofVictoriafragment breaks, etc. Inthispaper wepresent anef\u0002cient\nmethod toextract anumber ofversions ofthelyrics ofa\nsong from theWeb.Givensuch asetofversions ofthe\nlyrics, auser canselect (orconstruct) asatis\u0002able version.\n2.Method\nOur method consists ofthree phases. Inthe\u0002rst phase,\nwegather documents (potentially containing lyrics) using\nGoogle. Then weidentify thelyrics from these pages. When\nwehaveextracted asetofpotential lyrics, inthelastphase\nwe\u0002lter outirrele vantelements from thisset,e.g.lyrics to\nother songs.\n2.1. Collecting documents\nWeuseGoogle toretrie veanumber ofpages thatarelikely\ntocontain thelyrics ofthesong ofinterest. Tothisendwe\nquery allintitle: song title, artist name, lyrics .Weex-\ntract the URLsofthenbest scoring documents, weused\nn=40.Perweb site, weonly store thebest rankedURL.\nMoreo ver,wedonotstore URLsofdocuments inother than\nhyperte xtformat, e.g.with theextensions .doc or.pdf.\nIfthisapproach does notyieldnURLs,wesend broader\nqueries toGoogle (allintitle: song title, artist name and\nsong title, artist name )andextract theresulting URLs.\nAfter gathering the URLswecollect thecorresponding\ndocuments.\n2.2. Extracting lyrics fromthedocuments\nAfter gathering thedocuments corresponding tothe URLs\nidenti\u0002ed, weextract thelyrics from these documents.\nWemakeuseofthestructural representation oflyrics.\nLikepoetry ,lyrics consist ofstanzas separated byblank lines.\nEach stanza consists ofoneormore lines, where alineisa\nsequence thatends with anend-of-line mark er.\nWeusethese characteristics toidentify lyrics inahyper-\ntext.Anadditional observ ation used isthatthelyrics ingen-\neralhaveauniform lay-out. This implies thatwithin lyrics\nnohtml-tags occur other than theendoflinetags<br> .\nLyrics within ahyperte xtcanthus bedescribed byareg-\nular expression. Ifweconsider ahyperte xtdocument asa\nstring, wecanthusextract the\u0002rstsubstring thatisdescribed\nbysuch aregular expression.\nForreasons ofef\u0002cienc yhowever,we\u0002rstmap each sub-\nstring inthehyperte xtdocument separated byanend-of-linetagtooneofthecharacters l,bandnaccording tothefol-\nlowing rules.\n-bifthesubstring isempty orthestring only consists of\nwhite space characters (blank).\n-lifthesubstring does notcontain anyhtml-tags andcon-\ntains between 3and80characters (lyrics line).\n-notherwise (non lyrics).\nWenowhaveamapping ofthedocument toasequence S\noftheform(ljbjn)+.Wematch thissequence totheregular\nexpression describing lyrics givenby\nR=(l1\u000020\u0001b)1\u000012\u0001l1\u000020(1)\nwhereai\u0000jdenotes asequence ofasofalength be-\ntween iandj.\nNowwematch thesequence StoRandextract the\u0002rst\nsubstring thatmatches. The small size ofthesequence S\nassures anef\u0002cient mapping with atime comple xityupper -\nbound ofO(jSj)[2].\nWeusethecorresponding substring inthedocument to\nobtain thelyrics within it.This leads toasetLofextracted\ndata with amaximal sizeofn.\n2.3. Lyrics identi\u0002cation byclustering\nWeobserv edthatweindeed obtain anumber ofversions of\nthelyrics ofsome songa.However,notallextracted texts\narelyrics ofa.The main problem isthat weextract the\nwrong lyrics from apage containing multiple lyrics. Also\nlists such asthetracks ofanalbumthatmatch theregular\nexpression areextracted.\nWecould usemultiple string alignment onthestrings\ninLtocompute some `correct' version ofthelyrics ofa.\nHoweverweareinterested inef\u0002ciently obtaining asetof\nversions ofthelyrics astheyoccur ontheweb.Wethere-\nfore usea\u0002ngerprinting technique to\u0002lter outerroneously\nextracted data.\nFingerprints ofthelyrics ofsongashould ontheone\nhand bediscriminati vefrom lyrics ofother songs orother\ntexts.Ontheother hand, wewould likemultiple versions of\nlyrics tohavethesame \u0002ngerprints.\nWetherefore choose toselect themlongest words inthe\nlyrics asthe\u0002ngerprints. Ingeneral, thelongest words are\nthemost discriminati veinatext.Ifthelongest words intwo\ntextshavenooverlap, wecanassume thatthetextshandle a\ndifferent topic.\nPerstring inLweidentify the\u0002ngerprint, weusedm=\n5.Weusetheassumption thattwostrings areboth versions\nofthesame lyrics, iftheyshare atleastk\u0002ngerprints. In\nourexperiments weusedk=3asathreshold fortwotexts\nbeing both versions ofthesame lyrics.\nHaving alistLofpotential lyrics each withm\u0002nger -\nprints, wesortthislistbased onthesum oflength ofthe\n\u0002ngerprints.The \u0002rst elements intheLhavethelongest \u0002ngerprint\nwords andthusweexpect thecomplete versions ofthelyrics\nisthe\u0002rstpartofthelist. Thetopelement inthelistisse-\nlected asthereference element. Wecompare each ofthe\n\u0002ngerprints inLtothisreference element. Ifatextshares\natleastk\u0002ngerprints itisadded tothecluster with theref-\nerence element. Else, anewsetiscreated andthistextis\nselected asreference element forthisset.\n3.Experiments\nWetested ourmethod onthesetofsong titles used in[3].\nFor239ofthe258songs, theselected cluster relates tothe\nlyrics oftheintended song (93%). For13songs nolyrics\nwere found because thedifferent versions thatwere found\nwere toodiverse. One ofthese 13songs consisted outof\nonefragment with toomanylines (Absolute Beginner by\nDasBoot ).Fortheother 12songs, 10songs consist outofa\nsingle fragment. Since ourregular expression only accepts\nlyrics with atleast twofragments, these 10lyrics were not\nrecognized.\nInaddition, for6songs a\u0002nal clustering wasfound that\nturns outnottocontain thelyrics ofthesong. For4ofthese\n6songs, thesong title oftheintended song appears inthe\nlyrics ofanother song bythesame artist. Theintended song\nconsists ofonefragment while thefound song consists of\nmultiple fragments.\n4.Conclusions andfutur ework\nWehavedeveloped anmethod toextract lyrics from hetero-\ngeneous sources from theWebusing regular expressions.\nAfter having extracted asetoftexts,wegenerate acluster\nof(possibly multiple) versions ofthelyrics weareinterested\nin.The methods presented here areef\u0002cient, considering\nthepossibility tousethem inreal-time applications such as\nMP3-players.\nInfuture work, wecanimpro veourresults by\u0002ltering\noutannotations from extracted lyrics. Ifnames ofartists,\ncomposers andwebsites areremo ved,the\u0002ngerprinting phase\ncanbecome more effective.Wecanconstruct orsearch for\na`correct' version ofthelyrics among thesetofversions\nusing multiple string alignment methods.\nRefer ences\n[1]V.Crescenzi andG.Mecca. Automatic information extrac-\ntion from largewebsites. Journal oftheACM,51(5):731\n779, 2004.\n[2]D.Gus\u0002eld. Algorithms onstrings, trees, andsequences:\ncomputer science and computational biolo gy.Cambridge\nUniversity Press, Cambridge, UK, 1997.\n[3]P.Knees, M.Schedl, andG.Widmer .Multiple Lyrics Align-\nment: Automatic Retrie valofSong Lyrics. InProceedings\nof6thInternational Confer ence onMusic Information Re-\ntrieval(ISMIR'05) ,pages 564569, London, UK, September\n2005."
    },
    {
        "title": "ENST-Drums: an extensive audio-visual database for drum signals processing.",
        "author": [
            "Olivier Gillet",
            "Gaël Richard"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.7432188",
        "url": "https://doi.org/10.5281/zenodo.7432188",
        "ee": "https://zenodo.org/records/7432188/files/ENST-drums-audio.tar.bz2",
        "abstract": "The ENST-Drums database is a large and varied research database for automatic drum transcription and processing:\n\n\n\tThree professional drummers specialized in different music genres were recorded.\n\tTotal duration of audio material recorded per drummer is around 75 minutes.\n\tEach drummer played his own drum kit.\n\tEach sequence used either sticks, rods, brushes or mallets to increase the diversity of drum sounds.\n\tThe drum kits themselves are varied, ranging from a small, portable, kit with two toms and 2 cymbals, suitable for jazz and latin music ; to a larger rock drum set with 4 toms and 5 cymbals.\n\n\nEach sequence is recorded on 8 individual audio channels, is filmed from two angles, and is fully annotated\n\nA large part of ENST-Drums is publicly available under some conditions. These conditions include:\n\n\n\tThe use and exploitation of the database should be limited to research purposes. No commercial use is possible.\n\tThe database is distributed under the licence Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)\n\tAny document describing a research work where ENST-Drums was used should include a reference to ENST-Drums and to the paper Olivier Gillet and Gal Richard. ENST-Drums: an extensive audio-visual database for drum signals processing, In Proc of ISMIR&#39;06, Victoria, Canada, 2006.\n\n\n\n\nAcknowledgements\n\nWe would like to thank:\n\n\n\tThe 3 drummers: Louis Cav, Bertrand Clouard and Frdric Rottier.\n\tE. Thivon (author) and Play Music Publishing (publisher) for the background accompaniment sequences.\n\n\nThe authors wish to acknowledge the support of the French ministry of research (ACI-MusicDiscover project) and of the European Commission under the FP6-027026-K-SPACE contract.",
        "zenodo_id": 7432188,
        "dblp_key": "conf/ismir/GilletR06",
        "keywords": [
            "ENST-Drums",
            "large and varied research database",
            "automatic drum transcription and processing",
            "three professional drummers",
            "different music genres",
            "around 75 minutes",
            "audio material recorded per drummer",
            "varied drum kits",
            "8 individual audio channels",
            "fully annotated"
        ]
    },
    {
        "title": "The song remains the same: identifying versions of the same piece using tonal descriptors.",
        "author": [
            "Emilia Gómez",
            "Perfecto Herrera"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1417273",
        "url": "https://doi.org/10.5281/zenodo.1417273",
        "ee": "https://zenodo.org/records/1417273/files/GomezH06.pdf",
        "abstract": "Identifying versions of the same song by means of automa- tically extracted audio features is a complex task for a music information retrieval system, even though it may seem very simple for a human listener. The design of a system to per- form this task gives the opportunity to analyze which fea- tures are relevant for music similarity. This paper focu- ses on the analysis of tonal similarity and its application to the identification of different versions of the same piece. This work formulates the situations where a song is ver- sioned and several musical aspects are transformed with res- pect to the canonical version. A quantitative evaluation is made using tonal descriptors, including chroma representa- tions and tonality. A simple similarity measure, based on Dynamic Time Warping over transposed chroma features, yields around 55% accuracy, which exceeds by far the expec- ted random baseline rate. Keywords: version identification, cover versions, tonality, pitch class profile, chroma, audio description.",
        "zenodo_id": 1417273,
        "dblp_key": "conf/ismir/GomezH06",
        "keywords": [
            "version identification",
            "cover versions",
            "tonality",
            "pitch class profile",
            "chroma",
            "audio description",
            "Dynamic Time Warping",
            "transposed chroma features",
            "quantitative evaluation",
            "simple similarity measure"
        ],
        "content": "The song remains thesame: identifying versions ofthesame piece\nusing tonal descriptors\nEmilia G´omez\nMusic Technology Group, Universitat Pompeu Fabra\nOcata, 1\n08003, Barcelona\nemilia.gomez@iua.upf.eduPerfecto Herr era\nMusic Technology Group, Universitat Pompeu Fabra\nOcata, 1\n08003, Barcelona\nperfecto.herrera@iua.upf.edu\nAbstract\nIdentifying versions ofthesame song bymeans ofautoma-\ntically extracted audio features isacomple xtaskforamusic\ninformation retrie valsystem, eventhough itmay seem very\nsimple forahuman listener .Thedesign ofasystem toper-\nform thistask givestheopportunity toanalyze which fea-\ntures arerelevantformusic similarity .This paper focu-\nsesontheanalysis oftonal similarity anditsapplication\ntotheidenti\u0002cation ofdifferent versions ofthesame piece.\nThis workformulates thesituations where asong isver-\nsioned andseveralmusical aspects aretransformed with res-\npect tothecanonical version. Aquantitati veevaluation is\nmade using tonal descriptors, including chroma representa-\ntions andtonality .Asimple similarity measure, based on\nDynamic TimeWarping overtransposed chroma features,\nyields around 55% accurac y,which exceeds byfartheexpec-\ntedrandom baseline rate.\nKeywords: version identi\u0002cation, coverversions, tonality ,\npitch class pro\u0002le, chroma, audio description.\n1.Introduction\n1.1. Tonality andmusic similarity\nThepossibility of\u0002nding similar pieces isoneofthemost\nattracti vefeatures that asystem dealing with largemusic\ncollections canprovide. Similarity isaambiguous term, and\nmusic similarity issurely oneofthemost comple xproblems\ninthe\u0002eld ofMIR. Music similarity may depend ondiffe-\nrentmusical, cultural andpersonal aspects. Manystudies in\ntheMIR literature trytode\u0002ne andevaluate theconcept of\nsimilarity ,i.e.,when twopieces aresimilar .There aremany\nfactors involvedinthisproblem, andsome ofthem (maybe\nthemost relevantones) aredif\u0002cult tomeasure.\nSome studies intend tocompute similarity between audio\n\u0002les. Manyapproaches arebased ontimbre similarity using\nlow-levelfeatures [1,2].Other studies focus onrhythmic\nsimilarity .Foote proposes some similarity measures based\nPermission tomakedigital orhard copies ofallorpartofthisworkfor\npersonal orclassroom useisgranted without feeprovided thatcopies\narenotmade ordistrib uted forpro\u0002t orcommercial advantage andthat\ncopies bear thisnotice andthefullcitation onthe\u0002rstpage.\n©2006 University ofVictoriaonthebeat spectrum, including Euclidean distance, aco-\nsinemetric orinner product [3].Tempo isalsoused tomea-\nsure similarity in[4].Theevaluation ofsimilarity measures\nisahard task, giventhedif\u0002culty ofgathering ground truth\ndata foralargequantity ofmaterial. Some researchers as-\nsume thatsongs from thesame style, bythesame artist oron\nthesame albumaresimilar [5,6,7].Adirect waytomea-\nsure thesimilarity between songs isalso togather ratings\nfrom users (see[4]),which isadif\u0002cult andtime-consuming\ntask.\nTonality hasnotbeen much applied tomusic similarity ,\nasitmight benotsoclear forpeople nothaving amusical\nbackground. Wefocus here onanalyzing howtonal descrip-\ntorscanbeused tomeasure similarity between pieces.\nWeconsider thattwopieces aretonally similar ifthey\nshare asimilar tonal structure, related totheevolution of\nchords (harmon y)andkey.Wewillassume thattwopieces\naresimilar iftheyshare thesame tonal contour .Forsong\nsimilarity ,tonal contour could beasrelevantasmelodic\ncontour isformelody recognition[8 ].Wefocus then onthe\nproblem ofidentifying different versions ofthesame song,\nandstudy theuseoftonal descriptors forthistask.\n1.2. Version identi\u0002cation\nWhen dealing with huge music collections, version identi-\n\u0002cation isarelevantproblem, because itiscommon to\u0002nd\nmore than oneversion oftheagivensong. Wecanidentify\ndifferent situations forthisinmainstream popular music, as\nforexample re-mastered, recorded live,acoustic, extended\nordisco tracks, karaok eversions, covers(played bydifferent\nartists) orremix es.One example oftherelevance ofcover\nsongs isfound intheSecond Hand Songs database1,which\nalready contains around 37000 coversongs.\nAsong canbeversioned indifferent ways, yielding dif-\nferent degree ofdissimilarity between theoriginal andthe\nversioned tune. Themusical facets thataremodi\u0002ed canbe\ninstrumentation (e.g. leading voice oradded drum track),\nstructure (e.g. newinstrumental part, intro orrepetition),\nkey(i.e. transposition) andharmon y(e.g. jazz harmoniza-\ntion). These modi\u0002cations usually happen together inver-\nsions from popular music pieces. Thedegree ofdisparity on\nthedifferent aspects establishes avague boundary between\n1http://www .secondhandsongs.comwhat isconsidered aversion orwhat isreally adifferent\ncomposition. This frontier isdif\u0002cult tode\u0002ne, anditis\nanattracti vetopic ofresearch from theperspecti veofin-\ntellectual property rights andplagiarism. Theproblem has\nconceptual links with theproblem ofanalogy inhuman cog-\nnition, which isalsoanintriguing andfarfrom being under -\nstood topic. This istheproblem alsowhen developing com-\nputational models toautomatically identify these versions\nwith absolute effectiveness.\nThere isfewliterature dealing with theproblem ofiden-\ntifying versions ofthesame piece byanalyzing audio. Yang\nproposed analgorithm based onspectral features toretrie ve\nsimilar music pieces from anaudio database [9].This me-\nthod considers thattwopieces aresimilar iftheyarefully or\npartially based onthesame score. Afeature matrix wasex-\ntracted using spectral features anddynamic programming.\nYangevaluated thisapproach using adatabase ofclassical\nandmodern music, with classical music being thefocus of\nhisstudy .30to60second clips of120music pieces were\nused. Hede\u0002ned \u0002vedifferent types ofsimilar music\npairs, with increasing levelsofdif\u0002culty .Theproposed al-\ngorithm performed verywell (90% accurac y)insituations\nwhere thescore isthesame andthere aresome tempo mo-\ndi\u0002cations, which istheworst case \u0002gure. Onthesame\nidea, Purwins etal.calculate thecorrelation ofconstant\nQ-pro\u0002les fordifferent versions ofthesame piece played\nbydifferent performers andinstruments (piano andharpsi-\nchord) [10].\n2.Tonal featur eextraction\nThetonal features used forthisstudy arederivedfrom the\nHarmonic PitchClass Pro\u0002le (HPCP). TheHPCP isapitch\nclass distrib ution (orchroma) feature computed inaframe\nbasis using only thelocal maxima ofthespectrum within\nacertain frequenc yband. Itconsiders thepresence ofhar-\nmonic frequencies, asitisnormalized toeliminate thein-\n\u0003uence ofdynamics andinstrument timbre (represented by\nitsspectral envelope). From theinstantaneous evolution of\nHPCP ,wecompute thetransposed version ofthispro\u0002le\n(THPCP), which isobtained bynormalizing theHPCP vec-\ntorwith respect totheglobal key.The THPCP represents\natonal pro\u0002le which isinvariant totransposition. Forthese\ntwofeatures, weconsider both theinstantaneous evolution\nandtheglobal average. Werefer to[11,12]forfurther ex-\nplanation ontheprocedure forfeature extraction.\nInorder tomeasure similarity between global features,\nweusethecorrelation coef\u0002cient. Asanexample, thecorre-\nlation between HPCP average vectors fortwodistant pieces\nisequal to0.0069. This small value indicates thedissimi-\nlarity between thepro\u0002les, andcanbeconsidered asabase-\nline. Forinstantaneous features, weuseaDynamic Time\nWarping (DTW) algorithm. Ourapproach isbased in[13].\nTheDTW algorithm estimates theminimum costrequired to\nalign onepiece totheother onebyusing asimilarity matrix.3.Case study\nWeanalyze here theexample offour different versions of\nthesong Imagine,written byJohn Lennon. Themain differ-\nences between each oftheversions andtheoriginal song is\nsummarized inTable 2.\nWe\u0002rst analyze howglobal tonal descriptors aresimi-\nlarforthese different pieces. Inorder toneglect structural\nchanges, we\u0002rstconsider only the\u0002rstphrase ofthesong,\nwhich ismanually detected. Forthelastversion, performed\nbytwodifferent singers, weselect twophrases, each one\nsung byoneofthem, sothatthere isatotal of6different au-\ndiophrases. HPCP average vectors areshowninFigure 1.\n00.51Type VI − Transposition − Average HPCP\n00.51\n00.51\n00.51\n00.51\nA # B C # D # E F # G # A00.51\nFigur e1.HPCP average for6differ entversions ofthe\u0002rst\nphrase ofImagine.1.John Lennon, 2.Instrumental, guitar\nsolo, 3.Diana Ross, 4.Tania Maria, 5.Khaled and6.Noa.\nThecorrelation matrix Rphrasebetween theaverage HPCP\nvectors forthedifferent versions isequal to:\nRphrase=0\nBBBBBB@10:970:820:940:330:48\n0:97 10:860:950:310:45\n0:820:86 10:750:590:69\n0:940:950:75 10:180:32\n0:330:310:590:18 10:95\n0:480:450:690:320:95 11\nCCCCCCA\n(1)\nTable 1.Classi\u0002cation oftonal featur esused forsimilarity .\nFeatur e Pitch-class\nrepresenta-\ntionTemporal scope\nHPCP Absolute Instantaneous\nTHPCP Relati ve Instantaneous\nAverage HPCP Absolute Global\nAverage THPCP Relati ve GlobalTable 2.Details onversions ofthesong Imagine.\nID Artist Modi\u0002ed musical facets Key\n1 John\nLennonOriginal CMajor\n2 Instrumental Instrumentation (solo\nguitar instead ofleading\nvoice)CMajor\n3 Diana Ross Instrumentation, tempo,\nkeyandstructureFMajor\n4 Tania\nMariaInstrumentation, tempo,\nharmonization (jazz) and\nstructureCMajor\n5 Khaled and\nNoaInstrumentation, tempo,\nkeyandstructureEbMajor\nWecanseethatthere aresome lowvalues ofcorrelation be-\ntween versions, mainly fortheones which aretransposed to\nEbmajor (5and6),asthistonality isnotclose toCma-\njorasFmajor is(3).THPCP average vectors areshownin\nFigure 2.\n00.51Type VI − Transposition − Average THPCP\n00.51\n00.51\n00.51\n00.51\nA # B C # D # E F # G # A00.51\nFigur e2.THPCP average for6differ entversions ofthe\u0002rst\nphrase ofImagine.1.John Lennon, 2.Instrumental, guitar\nsolo, 3.Diana Ross, 4.Tania Maria, 5.Khaled and6.Noa.\nThecorrelation matrix Rt;phr asebetween theTHPCP ave-\nrage vectors forthedifferent versions isequal to:\nRt;phr ase=0\nBBBBBB@10:970:970:940:940:97\n0:97 10:980:950:910:98\n0:970:98 10:920:950:99\n0:940:950:92 10:860:94\n0:940:910:950:86 10:95\n0:970:980:990:940:95 11\nCCCCCCA\n(2)\nThis correlation matrix showhigh values forallthedifferent\nversions, with aminimum correlation value of0.86. Whencomparing complete songs inpopular music, most ofthe\nversions haveadifferent structure than theoriginal piece,\nadding repetitions, newinstrumental sections, etc.Welook\nnowatthecomplete 5versions ofthesong Imagine,byJohn\nLennon, presented inTable 2.Thecorrelation matrix Rbe-\ntween theaverage HPCP vectors forthedifferent versions is\nequal to:\nR=0\nBBBB@10:990:830:960:45\n0:99 10:860:950:45\n0:830:86 10:790:65\n0:960:960:79 10:35\n0:450:450:650:35 11\nCCCCA(3)\nWeobserv ethatthecorrelation values arelowerforthepiece\ninadistant key,which, inthecase ofversion 5,isEbmajor .\nWecanagainnormalize theHPCP vector with respect tothe\nkey.THPCP average vectors areshowninFigure 3.\n00.51Type 7 − Different structure − Average THPCP\n00.51\n00.51\n00.51\nA # B C # D # E F # G # A00.51\nFigur e3.THPCP average for5differ entversions ofImagine.\nThe correlation matrix Rtbetween theaverage THPCP\nvectors forthedifferent versions isequal to:\nRt=0\nBBBB@10:990:980:960:98\n0:99 10:990:950:98\n0:980:99 10:950:99\n0:960:950:95 10:95\n0:980:980:990:95 11\nCCCCA(4)\nWeobserv ethatthecorrelation values increase forversion\n5.Inthissituation, itbecomes necessary tolook atthestruc-\ntureofthepiece. When thepieces under study havedifferent\nstructures, westudy thetemporal evolution oftonal features,\ninorder tolocate similar sections. Structural description is\nadif\u0002cult problem, andsome studies havebeen devoted to\nthisissue (see, forinstance [14]and[15]).Foote [16]pro-\nposed theuseofself-similarity matrices tovisualize music.\nSimilarity matrices were builtbycomparing Mel-frequenc yFigur e4.Similarity matrix between version 5andtheoriginal\nversion ofImagine.\ncepstral coef\u0002cients (MFCCs), representing low-leveltim-\nbrefeatures. Weextend thisapproach tothementioned low-\nleveltonal features. Figure 5(atthetopandleftside) rep-\nresents theself-similarity matrix fortheoriginal version of\nImagine,using instantaneous THPCP .Thesimilarity matrix\nisobtained using distance between THPCP pro\u0002les statistics\noverasliding windo w.\nInthisself-similarity matrix wecanidentify thestruc-\nture ofthepiece bylocating side diagonals (verse-ver se-\nchorus-ver se-chorus ).Wealsoobserv ethatthere isachord\nsequence which isrepeating along theverse (C-F), sothat\nthere isahigh self-similarity inside each verse. Instead\nofcomputing aself-similarity matrix, wecompute nowthe\nsimilarity matrix between twodifferent pieces. Figure 5\nshowsthesimilarity matrix between theoriginal song (1)\nandtheinstrumental version (2).\nInthis\u0002gure, wealsoidentify thesame song structure as\nbefore, which ispreserv edinversion 2.Wealsoseethatthe\ntempo ispreserv ed,asthediagonal islocated sothatthetime\nindexremains thesame inxandyaxis. Now,weanalyze\nwhat happens ifthestructure ismodi\u0002ed. Figure 4shows\nthesimilarity matrix between theoriginal song andversion\n5.Here, theoriginal overall tempo ismore orlesskept,but\nwecanidentity some modi\u0002cations inthestructure ofthe\npiece. Withrespect totheoriginal song, version 5intro-\nduces anewinstrumental section plus anadditional chorus\nattheendofthepiece. Figure 5represents thesimilarity ma-\ntrixforeach ofthe5coverversions andtheself-similarity\nmatrix oftheoriginal song. Wecanseethatversion 4(Tania\nMaria) isthemost dissimilar one, sothatwecannotdistin-\nguish clearly adiagonal inthesimilarity matrix. Ifwelistentoboth pieces, wecanhear some changes inharmon y(jazz),\naswell aschanges inthemain melody .These changes affect\ntheTHPCP features. Inthissituation, itbecomes dif\u0002cult to\ndecide ifthisisadifferent piece oraversion ofthesame\npiece. InFigure 5,wealso present thesimilarity matrix\nwith adifferent song, Besame MuchobyDiana Krall, inor-\ndertoillustrate thatitisnotpossible to\u0002ndadiagonal for\ndifferent pieces iftheydonotshare similar chord progres-\nsions. Asaconclusion totheexample presented here and\ntotheobserv ation of90versions ofdifferent pieces, wead-\nvance thehypothesis thattheinstantaneous tonal similarity\nbetween pieces isrepresented bydiagonals inthesimilarity\nmatrix from tonal descriptors. Theslope ofthediagonal rep-\nresents tempo differences between pieces. Inorder totrack\nthese diagonals, weuseasimple Dynamic TimeWarping,\nfound in[13].This algorithm estimates theminimum cost\nfrom onepiece totheother oneusing thesimilarity matrix.\nWestudy innextsection howthisminimum costcanbeused\ntomeasure similarity between pieces.\n4.Evaluation\n4.1. Methodology\nInthisevaluation experiment, wecompare theaccurac yof\nfour different similarity measures:\n1.Correlation ofglobal HPCP ,computed astheaverage\nofHPCP overthewhole musical piece.\n2.Correlation ofglobal THPCP ,computed byshifting\ntheglobal HPCP vector with respect tothekeyofthe\npiece, obtained automatically asexplained in[11].\n3.Minimum costcomputed using DTW andasimilarity\nmatrix from HPCP values.\n4.Minimum costcomputed using DTW andasimilarity\nmatrix from THPCP values.\nTheestimation accurac yismeasured using average pre-\ncision andrecall forallsongs inthedatabase. Foreach one,\nthequery isremo vedfrom thedatabase, i.e.itdoes notap-\npear intheresult list. Inorder toestablish abaseline, we\ncompute theprecision thatwould beobtained byrandomly\nselecting pieces from themusic collection. Let'sconsider\nthat, givenaquery ifrom thecollection (i=1:::N),we\nrandomly chose agivenpiecej6=i(j=1:::N)from the\nevaluation collection asmost similar toaquery .Theproba-\nbility ofchoosing apiece with thesame version Idisequal\nthen to:\nRandomP recision i=nId(i)\u00001\nN\u00001(5)\nTheaverage forallthepossible queries isequal to:\nRandomP recision =1\nN\u0001NX\ni=1RandomP recision i(6)Figur e5.Similarity matrix for5differ entversions ofImagine.\nFortheconsidered evaluation collection, thebaseline would\nbeRandomP recision =3:196% ,with amaximum value\noftheFmeasure equal to0.0619. This isaverylowvalue\nthatourproposed approach should impro ve.\n4.2. Material\nThe material used inthisevaluation are90versions from\n30different songs takenfrom amusic collection ofpopular\nmusic. Theversions include different levelsofsimilarity to\ntheoriginal piece, which arefound inpopular music: noise,\nmodi\u0002cations oftempo, instrumentation, transpositions and\nmodi\u0002cations ofmain melody andharmonization. Theav-\nerage number ofversions foreach song isequal to3.07, and\nitsvariance is2.71. Most oftheversions include modi\u0002ca-\ntions intempo, instrumentation, keyandstructure, andsome\nofthem include variations inharmonization2.Wearethen\ndealing with themost dif\u0002cult examples, sothattheevalua-\ntioncanberepresentati veofarealsituation when organizing\ndigital music collections.\n4.3. Results\nFigure 6showstheaverage precision andrecall forallthe\nevaluated collection forthedifferent con\u0002gurations. When\nusing thecorrelation ofglobal average HPCP asasimilarity\n2Thelistofsongs inthemusic collection andsome additional material\ntothisworkispresented inhttp://www .iua.upf.edu/~e gomez/v ersionidmeasure between pieces, theobtained precision isverylow,\n20% with arecall levelof8%andaFmeasure of0.145.\nWhen using global features normalized with respect tothe\nkey(THPCP), theprecision increases to35:56%,around\n15% higher than using HPCP .Therecall levelalsoincreases\nfrom8%to17:6%,andtheFmeasure to0.322. Using ins-\ntantaneous HPCP andDTW minimum cost, theprecision is\nequal to23:35%,which ishigher than using aglobal mea-\nsure ofHPCP .The recall levelisslightly higher ,equal to\n10:37% andtheFvalue isequal to0.159. Finally ,ifweuse\nDTW minimum cost computed from instantaneous THPCP\nassimilarity measure, weobserv ethatthemaximum pre-\ncision increases upto54:5%,andtherecall levelisequal\nto30:8%,obtaining aFmeasure of0.393. This evalua-\ntion showsthatrelati vedescriptors (THPCP) seem toper-\nform better than absolute chroma features, which iscoher -\nentwith theinvariability ofmelodic andharmonic percep-\ntion totransposition. Also, itseems thatitisimportant to\nconsider thetemporal evolution oftonality ,which issome-\ntimes neglected. The best accurac yisthen obtained when\nusing asimple DTW minimum costcomputed from THPCP\ndescriptors, anditisaround 55% precision (recall levelof\n30%,Fmeasure equal to0.393).0 10 20 30 40 50 60 70 80 90 1000102030405060\nAverage RecallAverage PrecisionPrecision vs Recall\nAv. HPCP\nAv. THPCP\nHPCP DTW\nTHPCP DTW\nFigur e6.Precision vsrecall values forthediffer entcon\u0002gura-\ntions.\n5.Conclusions andfutur ework\nWehavefocused inthispaper ontheanalysis oftonal simi-\nlarity anditsapplication totheidenti\u0002cation ofdifferent ver-\nsions ofthesame piece. Wehavepresented asmall experi-\nment showing thattonal descriptors byitself canbehelpful\nforthistask.\nThere aresome conclusions tothisstudy .First, itisne-\ncessary toconsider invariance totransposition when com-\nputing tonal descriptors forsimilarity tasks. Second, we\nshould look atthestructure ofthepiece toyield relevant\nresults. Looking atthetonal structure ofthepiece yields\nverygood results thatmay probably exceed those attainable\nusing other types ofdescriptors (i.e.timbre orrhythm).\nVersion identi\u0002cation isadif\u0002cult problem requiring a\nmultif aceted andmultile veldescription. Aswementioned\nbefore, ourevaluation database represents areal situation\nofadatabase including coverversions, where eventhehar-\nmonyandthemain melody ismodi\u0002ed. This factaffects the\npitch class distrib ution descriptors. Eveninthissituation,\nweseethatonly using low-leveltonal descriptors andavery\nsimple similarity measure, wecandetect until55% ofthe\nversions with arecall levelof30% (Fmeasure of0.393).\nThese results overcome thebaseline (Fmeasure of0.0619)\nandshowthattonal descriptors arerelevantformusic simi-\nlarity .\nFurther experiments will bedevoted toinclude higher\nlevelstructural analysis (determining themost representa-\ntivesegments), toimpro vethesimilarity measure, andto\ninclude other relevantaspects asrhythmic description (ex-\ntracting characteristics rhythmic patterns) andpredominant\nmelody estimation.6.Ackno wledgments\nThis research hasbeen partially supported byEU-FP6-IST -\n507142 project SIMA C3ande-Content HARMOS4project,\nfunded bytheEuropean Comission. Theauthors would like\ntothank Anssi Klapuri, FlavioLazzareto andpeople from\nMTG rooms 316-324 fortheir help andsuggestions.\nRefer ences\n[1]Elias Pampalk. Amatlab toolbox tocompute music similar -\nityfrom audio. InISMIR ,Barcelona, Spain, 2004.\n[2]Jean-Julien Aucouturier andFranc ¸oisPachet. Tools andar-\nchitecture fortheevaluation ofsimilarity measures: case\nstudy oftimbre similarity .InISMIR ,Barcelona, Spain, 2004.\n[3]Jonathan T.Foote, Matthe wCooper ,andUnjung Nam. Au-\ndioretrie valbyrhythmic similarity .InISMIR ,Paris, France,\n2002.\n[4]Fabio Vignoli andSteffenPauws. Amusic retrie valsystem\nbased onuser-drivensimilarity anditsevaluation. InISMIR ,\nLondon, UK, 2005.\n[5]Beth LoganandAriel Salomon. Amusic similarity func-\ntion based onsignal analysis. InInternational Confer ence\nonMultimedia andExpo ,Tokyo,Japan, 2001.\n[6]Elias Pampalk, Simon Dixon, andGerhard Widmer .Onthe\nevaluation ofperceptual similarity measures formusic. In\nInternational Confer ence onDigital Audio Effects,London,\nUK, 2003.\n[7]Adam Berenzweig, Beth Logan,Daniel P.W.Ellis, andBrian\nWhitman. Alarge-scale evalutation ofacoustic andsubjec-\ntivemusic similarity measures. InInternational Confer ence\nonMusic Information Retrie val,Baltimore, USA, 2003.\n[8]W.JayDowling. Scale andcontour: twocomponents of\natheory ofmemory formelodies. Psychological Review,\n85(4):341354, 1978.\n[9]Cheng Yang. Music database retrie valbased onspectral sim-\nilarity .InISMIR ,2001.\n[10] Hendrik Purwins, Benjamin Blank ertz, and Klaus Ober -\nmayer .Anewmethod fortracking modulations intonal mu-\nsicinaudio data format. Neur alNetworks -IJCNN, IEEE\nComputer Society ,6:270275, 2000.\n[11] Emilia G´omez. Tonal description ofpolyphonic audio for\nmusic content processing. INFORMS Journal onComputing ,\nSpecial Cluster onComputation inMusic ,18(3), 2006.\n[12] Emilia G´omez. Tonal description ofmusic audio sig-\nnals.Phddissertation, Universitat Pompeu Fabra, July 2006.\nhttp://www .iua.upf.es/~e gomez/thesis.\n[13] Dan Ellis. Dynamic Time Warp (DTW) inMat-\nlab. Online resource, last accessed onMay 2006.\nhttp://www .ee.columbia.edu/~dpwe/resources/matlab/dtw .\n[14] WeiChai. Automated analysis ofmusical structur e.Phd\nthesis, MIT,August 2005.\n[15] Beesuan Ong andPerfecto Herrera. Semantic segmentation\nofmusic audio contents. InICMC ,Barcelona, 2005.\n[16] Jonathan T.Foote. Visualizing music andaudio using self-\nsimilarity .InACM Multimedia ,pages 7784, Orlando,\nFlorida, USA, 1999.\n3http://www .semanticaudio.or g\n4http://www .harmosproject.com"
    },
    {
        "title": "AIST Annotation for the RWC Music Database.",
        "author": [
            "Masataka Goto"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1418125",
        "url": "https://doi.org/10.5281/zenodo.1418125",
        "ee": "https://zenodo.org/records/1418125/files/Goto06.pdf",
        "abstract": "In this paper, we introduce our activities regarding the man- ual annotation of the musical pieces of the RWC Music Database. Although the RWC Music Database is widely used, its annotated descriptions are not widely available. We therefore annotated a set of music-scene descriptions con- sisting of the beat structure, melody line, and chorus sec- tions. We call this AIST Annotation. We also manually syn- chronized standard MIDI files with the corresponding audio signals at the beat level. We hope that the AIST Annota- tion will contribute to further advances in the field of music information processing.",
        "zenodo_id": 1418125,
        "dblp_key": "conf/ismir/Goto06",
        "keywords": [
            "manual annotation",
            "RWC Music Database",
            "annotated descriptions",
            "music-scene descriptions",
            "beat structure",
            "melody line",
            "chorus sections",
            "AIST Annotation",
            "standard MIDI files",
            "audio signals"
        ],
        "content": "AIST Annotation for the RWC Music Database\nMasataka Goto\nNational Institute of Advanced Industrial Science and Technology (AIST)\nIT, AIST, 1-1-1 Umezono, Tsukuba, Ibaraki 305-8568, Japan\nm.goto@aist.go.jp\nAbstract\nIn this paper, we introduce our activities regarding the man-\nual annotation of the musical pieces of the RWC MusicDatabase. Although the RWC Music Database is widelyused, its annotated descriptions are not widely available. We\ntherefore annotated a set of music-scene descriptions con-\nsisting of the beat structure, melody line, and chorus sec-tions. We call this AIST Annotation . We also manually syn-\nchronized standard MIDI ﬁles with the corresponding audio\nsignals at the beat level. We hope that the AIST Annota-tion will contribute to further advances in the ﬁeld of music\ninformation processing.\n1. Introduction\nManual music annotation for musical pieces in a music col-lection is a laborious task, but it is important because anno-\ntated descriptions are useful for training statistical modelsand for evaluating or benchmarking various systems includ-ing automatic music annotation systems. In this paper, mu-\nsic annotation is deﬁned as a process to label events and\nscenes in musical pieces with correct descriptions regardingthe beat structure, melody line, and chorus sections. Since\nmanual annotation is so important, various studies have been\nreported, including efforts to build special editors for an-notating musical pieces by hand [1, 2, 3], work on man-\nual annotation of music collections [4], and discussions of\nmethodologies and ﬁle formats for annotation [5, 6].\nThe RWC Music Database [7], a copyright-cleared music\ndatabase (DB) available to researchers as a common foun-\ndation for research, is already widely used worldwide, but\nannotated descriptions for its musical pieces are not widelyavailable. The database is distributed as 27 music compact\ndiscs (CDs) containing 315 musical pieces and 12 DVD-\nROM discs containing 29.1 Gbytes of monaural sound ﬁlesof 50 instruments. For all 315 musical pieces, standard\nMIDI ﬁles (SMFs) and text ﬁles of lyrics (for songs) are also\nprovided on the WWW, but the SMFs are not synchronizedwith the original audio signals.\nTo enhance the usefulness of the RWC Music Database,\nwe have made a continuous effort to manually annotate its\nmusical pieces since August, 2001. The following sections\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copiesare not made or distributed for proﬁt or commercial advantage and thatcopies bear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2006 University of Victoria\ndescribe our annotated descriptions and issues regarding the\nannotation and distribution of those descriptions.\n2. AIST Annotation\nTable 1 lists music scene descriptions [8] for musical pieces\nwe have already annotated in the RWC Music Database. Us-\ning our multipurpose music-scene labeling editor (describedin [8]), a music college graduate with absolute pitch anno-tated the pieces with the following descriptions.\n2.1. Beat Structure\nThe hierarchical beat structure consists of the quarter-note\nlevel represented as the temporal position of each beat and\nthe measure level annotated by labeling the beginning of\neach measure on the corresponding beat.\n1\nTwo techniques facilitated this annotation. First, when\nthe audio signal of a track before mixdown2for a musical\npiece included metronome clicks that were given to musi-cians to keep the tempo in recordings, its track was analyzed\nby using a simple amplitude-based event detection method.\nBeat positions were thus initialized with the detected eventsand each position was then manually checked and adjusted\non the editor while watching the waveform and listening to\naudio playback with clicks at beat positions as well as shortplayback excerpts before or after a beat position. Second,\ngiven the annotated beat positions and a time-signature as-\nsumption, the beginning of all measures after the currentcursor position of the editor was automatically labeled.\n2.2. Melody Line\nThe melody line is represented as the temporal trajectory\nof the fundamental frequency (F0). The F0 is measured inhertz and the discrete time step is 10 ms. For time steps\nwhere the melody line is absent, the F0 is set to 0 Hz. Note\nthat the melody line is not represented as a series of eithermusical notes or MIDI note numbers.\nAs we did for the beat annotation, the melody line was\nalso initialized with the F0 estimated on a melody track be-fore mixdown when available. The F0 values were graphi-\ncally set and adjusted on the editor while watching the spec-\ntrogram with the melody line and listening to the melodyplayback generated using the amplitude of harmonics of the\n1When the time signature is 4/4, for example, the beginning of measures\nis labeled on every four beats.\n2Audio signals of tracks before mixdown were stored in the music pro-\nduction system used for the database development [7]. Those signals areusually not available for commercially distributed copyrighted music.Table 1. List of music scene descriptions annotated in ﬁve of six component databases (DBs) in the RWC Music Database.\nComponent DB in the RWC Music Database\n Beat structure\n Melody line\n Chorus sections\n Audio-synchronized SMF\nPopular Music DB (RWC-MDB-P-2001 Nos. 1–100)\n V\n V\n V\n V\nRoyalty-Free Music DB (RWC-MDB-R-2001 Nos. 1–15)\n V\n V\nClassical Music DB (RWC-MDB-C-2001 Nos. 1–50)\n V\n V\nJazz Music DB (RWC-MDB-J-2001 Nos. 1–50)\n V\n V\nMusic Genre DB (RWC-MDB-G-2001 Nos. 1–100)\n V\n V\ncurrently labeled F0 as well as the melody-cancelled back-\nground playback.\n2.3. Chorus Sections\nThe chorus (refrain) sections, which are the most represen-\ntative thematic sections of a musical piece, are represented\nas a list of the beginning and end points of every chorus sec-\ntion. When the music structure is obvious, a musical pieceis manually segmented into sections and every section is la-\nbeled with a section name of the music structure, such as\nintro,verse A ,verse B ,pre-chorus ,chorus A ,chorus B ,post-\nchorus ,bridge A ,bridge B , and ending .\nBy making the most of the beat-structure annotation, the\nbeginning and end points of each section were easily speci-\nﬁed on beat positions while moving the cursor only on beatpositions, watching both global and local views of labeled\nsections, and listening to the audio playback in units of mea-\nsure or section. It was also useful to highlight each sectionwith a color corresponding to the labeled section name, es-\npecially when showing the entire piece in the global view.\n2.4. Audio-Synchronized Standard MIDI File (SMF)\nWe have worked on synchronizing each SMF with the au-\ndio signal of the corresponding musical piece. Althoughthe SMFs in the RWC Music Database were transcribed by\near and might not correspond to original scores, they can\nstill be considered a potential source of informative anno-tated descriptions.\n3Using the annotated beat positions of\naudio signals, it is not difﬁcult to synchronize those posi-\ntions with beat positions in an SMF and generate a synchro-nized tempo track for the SMF. But since the beat positionsaround the introduction and ending of a piece sometimes\ndo not match straightforwardly, the editor had to include a\nfunction to edit their positions on a wave or MIDI-piano-rolldisplay. The editor also supported interactive and synchro-\nnized audio/MIDI playback during editing.\n3. Issues When Sharing Annotated Descriptions\nTo make annotated descriptions for sound ﬁles ripped from\nCDs available for researchers around the world, an impor-\ntant issue is how to synchronize their temporal axes because\ndifferent CD drives and ripping software have different tem-poral offsets or gaps at the beginning of sound ﬁles ripped\nfrom the same CD. We solve this issue by measuring a gap\n3For example, the onset times of drum sounds were extracted from the\nsynchronized SMFs of RWC-MDB-P-2001 and used as the ground-truthannotation for the Audio Drum Detection contest in the Music InformationRetrieval Evaluation eXchange (MIREX) 2005.— a silent period where absolute waveform values are be-\nlow a threshold — at the beginning of a sound ﬁle and stor-ing a very short excerpt of the waveform right after the gap\nas a signature. We can share its length and the signature, to-\ngether with annotated descriptions, for the synchronization.This technique can also be used for sharing the annotation\nfor copyrighted music without sharing the music itself.\nAll descriptions are stored in text ﬁles and can easily be\nconverted to any ﬁle format such as XML and CSV . Eachtime step or section (temporal region) is represented, in a\nseparate text ﬁle line, as a pair consisting of its absolute time\n(with temporal resolution of 10 ms) and values/words.\n4. Conclusion\nWe have described AIST Annotation , the annotation for the\nRWC Music Database, which is done for the purposes of\nstatistical learning, evaluation, comparison, shared bench-marking, systematic technology improvement, and so on.\nWe plan to distribute the annotated descriptions on the RWC\nMusic Database home page. We sincerely hope that thesedescriptions will be widely used\n4and will contribute to our\nresearch ﬁeld. Although our annotation is not perfect and\nmay include errors, we hope that researchers around theworld will also contribute by adding and improving anno-tated descriptions in various ways and will share their ad-\nditions and improvements, thus expediting progress in this\nﬁeld of research.\nReferences\n[1] P. Herrera, et al. , “MUCOSA: A music content semantic an-\nnotator,” in ISMIR , 2005, pp. 77–83.\n[2] X. Amatriain, et al. , “The CLAM annotator: A cross-platform\naudio descriptors editing tool,” in ISMIR , 2005, pp. 426–429.\n[3] “Sonic visualiser,” http://www.sonicvisualiser.org/.[4] K. Tanghe, et al. , “Collecting ground truth annotations for\ndrum detection in polyphonic music,” in ISMIR , 2005, pp. 50–\n57.\n[5] M. Lesaffre, et al. , “Methodological considerations concern-\ning manual annotation of musical audio in function of algo-rithm development,” in ISMIR , 2004, pp. 64–71.\n[6] C. Harte, et al. , “Symbolic representation of musical chords:\nA proposed syntax for text annotations,” in ISMIR , 2005, pp.\n66–71.\n[7] M. Goto, “Development of the RWC Music Database,” in ICA\n, 2004, pp. I–553–556.\n[8] ——, “Music scene description project: Toward audio-based\nreal-time music understanding,” in ISMIR , 2003, pp. 231–232.\n4The AIST Annotation can also be used for development (training) data\nsets for future MIREX contests."
    },
    {
        "title": "A Philosophical Wish List for Research in Music Information Retrieval.",
        "author": [
            "Cynthia M. Grund"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1415190",
        "url": "https://doi.org/10.5281/zenodo.1415190",
        "ee": "https://zenodo.org/records/1415190/files/Grund06.pdf",
        "abstract": "Within a framework provided by the traditional trio consisting of metaphysics, epistemology and ethics, a first stab is made at a wish list for MIR-research from a philosophical point of view. Since the tools of MIR are equipped to study language and its use from a purely sonic standpoint, MIR research could result in another revealing revolution within the linguistic turn in philosophy. Keywords: Philosophy and MIR, language as spoken, memory",
        "zenodo_id": 1415190,
        "dblp_key": "conf/ismir/Grund06",
        "keywords": [
            "metaphysics",
            "epistemology",
            "ethics",
            "wish list",
            "MIR-research",
            "philosophical point of view",
            "language and its use",
            "linguistic turn",
            "revolution",
            "memory"
        ],
        "content": "A Philosophical Wish List for Resear ch in Music Information Retrieval \nCynthia M. Grund \nInstitute of Philosophy, Education and the Study of Religions - Philosophy \nUniversity of Southern Denmark, Odense \ncmgrund@ifpr.sdu.dk \n \nAbstract \nWithin a framework provided by the traditional trio \nconsisting of metaphysics, epistemology and ethics, a first \nstab is made at a wish list for MIR-research from a philosophical point of view. Since the tools of MIR are \nequipped to study language and its use from a purely sonic \nstandpoint, MIR research could result in another revealing revolution within the linguistic turn in philosophy.  \nKeywords : Philosophy and MIR, language as spoken, \nmemory \n1. Introduction and Brief Setting of the Stage \nPhilosophy wrestles with questions regarding \nmetaphysics , epistemology  and ethics .  \nMetaphysics  can usefully be characterized as that field \nwhich grapples with the question: What is the ultimate \nsubstrate of the universe, that which is most aptly regarded \nas basic? A related question is: What is there in the \nuniverse for which our chosen metaphysics is the \nsubstrate? This area of inquiry may be labeled as the rightful domain of ontology.   \nEpistemology may be briefly characterized as the area \nwithin philosophy which struggles with the query: What criteria do we have for knowing rather than simply \nbelieving that something is the case?  \nEthics seeks insight into the how we should answer the \nquestion: How ought we to behave/what is right? \nThe foregoing questions are daunting ones and the way \none might go about trying to answer them is far from \nobvious. The previous century has many times been \ncelebrated as that in which philosophy took a “linguistic turn.” A candidate for a brutal formulation of the attitudes \nand convictions reflected by “the linguistic turn” is this: It \nis difficult, if not impossible, to imagine having proper access to the kind of high-level of abstraction required for \ndealing with basic, philosophical questions without \nlanguage as the shaper of this conceptual realm, indeed as that which gives us the only means at our disposal for \ndefining what it is we believe this realm to consist of. By \ntrying to gain insight into the way language mediates our \nunderstanding, we are afforded methods for dealing with what might otherwise seem utterly intractable questions. \nWhat insights could MIR-research provide into the \ngreat questions of philosophy? The answer lies in the \nparticular facility which the tools of MIR possess for dealing with language as a sonic phenomenon, thus \nproviding yet another revolution in the linguistic turn. \n1 A \nsearch through the reams of literature produced regarding the role of language in philosophical endeavor reveals that \nthe language framework is virtually always a written one. \nLittle or no attention has been paid to the mechanisms at work in thinking, learning and communicating in a context \nwhich is virtually of an exclusively oral, sonic.\n2 Since the \noverwhelming majority of time during which our thinking, \nlearning and communicative skills developed was \ncharacterized by speech activity which was unsupported by writing, it is of great relevance to our understanding of \nourselves as linguistic agents – and thus thinking agents – \nto understand what sort of influence this has had on the development of our brains, our strategies for memorizing \nand what semantic contributions tonal aspects of language \nmay supply.\n3 \n2. First Stab at a Wish List \n(1) First on the list is a wish for more study of the ways \nin which meaning is evoked, borne and created in \nlanguage thanks to sonic characteristics.4 The study of \n                                                           \n1 As a philosopher, it has been a privilege for me to gain some \nfamiliarity with these tools at ISMIR 2004, ISMIR 2005, CMMR 2004 and CMMR 2005, as well as through NTMSB and JMM . See [1], [2], [3] and [4]. \n2 Writers interested in the evolution of speech and music are one \ninteresting exception. See, for example [5] and [6]. \n3 An anonymous reviewer suggested integrating some discussion \nof Bayesian statistical modeling as well as some references to automatic speech recognition and computational linguistics here in order to “allow MIR practitioners to more easily understand the role they could play in contributing to \ninterdisciplinary work.” Although this is an excellent \nsuggestion, space limitations preclude implementing it here. \n4 It could be relevant to study to what extent stroke patients \nwhose native languages are of the markedly melodic, tonal sort recover spontaneously (viz., make unaided progress in the first six months or so) after a stroke. Work by Gottfried Schlaug [7] \nand collaborators at the Dept. of Neurology, Music and \nNeuroimaging Laboratory, Beth Israel Deaconess Medical Center and Harvard Medical School, indicates that melodic intonation therapy is a highly effective method for helping Permission to make digital or hard copi es of all or part of this work for \npersonal or classroom use is granted without fee provided that copies \nare not made or distributed for profit or commercial advantage and that \ncopies bear this notice and th e full citation on the first page. \n© 2006 University of Victoria adult illiterate subjects is very important in this respect, \nsince all of their usage of speech is predicated upon \nremembering small bits of sonic material and rules of composition for these. More work is needed on the ways in \nwhich the comprehension of segmentation of speech and \nof prosody in speech is or is not related to the way in which we experience interference in the comprehension of \nmusical gestalts when performance is slowed down, time \nintervals between parts of a melody are lengthened or \nshortened, etc. ([8], [9] and [10]).  It would be of immense \ninterest to find out how illiterate agents living in a basically reading/writing free context (if such a context \ncan, indeed, still be found) co mpare with illiterate agents \nembedded in a literate milieu. In such a context, that which must be remembered – if any appreciable detail is involved \n– requires that at least one member of the group actively \nand intentionally can retrieve information from his or her own intentionally stored memories (we are regarding \nsituations such as the reactivation of hypnotically \nconditioned agents á la The Manchurian Candidate  [11] to \nbe sufficiently pathological so as not to require inclusion \nin the analysis at hand). MIR researchers asking the right sort of questions could begin to examine the relations \nholding among the following: What mnemonic roles are \nplayed by the tonal features of language? To what extent are musical preferences a function of memorability? What \nis the relationship between knowledge, belief – and \nmemory?  In order for an agent to have any grasp of what sort of recallable things and/or processes there are at all – \nand thus commit to at least some sort of ontology – \nmemory is indispensable. \n(2) Since memory on both a group and individual level \nis also crucial for the sustenance of culture, the relationships between these two sorts of memory are \nimportant. The tools of MIR increasingly exhibit \nunprecedented ability to analyze huge databases of sonic data at different scales,  viz. on individual and on \nstatistically determined levels. As the contents of \naccessible databases becomes more varied and ever more widely available, it should be possible to obtain \nquantifiable evidence with regard to what the \ncharacteristics of music regarded as memorable within the \nframework of an entire culture might be. Such insights into \nways in which culturally robust sonic artifacts are constructed and maintained are philosophically relevant \nfrom the perspective of “practical ontology.” \n(3) Since the description of musical pieces in terms of \nemotional content is widely employed by listeners, \nresearch that can show correlations between brain states \nand at least certain emotional ones in a wide variety of listeners listening to the same musical selections could \nhelp to cut across discussions of cultural relativity and \n                                                                                               \n \npatients with aphasia after strokes to regain their ability to \nspeak. emotion and provide indices which are reliable on a global, \nmulticultural scale.5 \n(4) This brings us, finally, to the questions of ethics. As \nmore and more music becomes digitally available and \nassociated with various moods, emotional states, and the \nlike due to various phenomenological properties, to what degree should users be sensitive to  impropriety, such as, \nsay, using musical material from one culture’s repertoire of \nsacred music as dance and party music within the context \nof another culture? \nReferences \n[1] C.M. Grund. “Music Information Retrieval, Memory and \nCulture: Some Philosophical Remarks,” in ISMIR 2005: \nProceedings of the Sixth International Conference on \nMusic Information Retrieval, Joshua D. Reiss and Geraint \nA. Wiggins, Eds., 2005, pp. 8-12.   \n[2] C.M. Grund. “Interdisciplinarity and Computer Music \nModeling and Information Retrieval: When Will the \nHumanities Get into the Act?” in Computer Music \nModeling and Retrieval Third International Symposium, \nCMMR 2005, Pisa, Italy, September 2005, Revised Papers, Springer Verlag, 2006, pp. 265-273. \n[3] NTSMB, www.ntmsb.dk. See programs for Nature, \nCulture and Musical Meaning  and Music, Logic and \nTechnology.  \n[4] JMM: The Journal of Music and Meaning, \nwww.musicandmeaning.net \n[5] Christensen-Dalsgaard, Jakob.  “Music and the Origin of \nSpeeches.” JMM 2 , www.musicandmeaning.net, section 2.  \n[6] Iegor Reznikoff, “On Primitive Elements of Musical \nMeaning”, JMM 2 , www.musicandmeaning.net, section 2. \n[7] “Using Music and Musicians to Explore Brain Plasticity,” \ntalk given by Gottfried Schlaug at the conference Music in \nthe Brain – Experience and Learning  held April 21-22, \n2006 at the Royal Academy of Music, Århus, Denmark (hereafter referred to as MIB). \n[8] Cyrille Magne, Mitsuko Aramaki, Corine Astesano, Reyna \nLeigh Gordon, Sølvi Ystad, Snorre Farner, Richard Kronland-Martinet & Mireille Besson, “Comparison of Rhythmic Processing in Language and Music: An Interdisciplinary Approach” JMM 3  \nwww.musicandmeaning.net, section 5.  \n[9] “The Nature of Music from a Biological Perspective,” talk \ngiven by Isabelle Peretz at MIB (see [7]). \n[10] “Influence of Music training on Language Processing: \nElectrophysical Studies in Both Adults and Children,” talk given by Mireille Besson at MIB (see [7]). \n[11] The Manchurian Candidate, dir. John Frankenheimer, \nscreenplay by George Axelrod, based upon a novel by Richard Condon. MGM 1962. MGM DVD 2004. \n[12] “Emotions in Music Listening are Mainly Learned,” talk \ngiven by Eckart Altenmüller at MIB (see [7]). \n[13] “Emotions in Music are Partly Innate,” talk given by Karen \nJohanne Pallesen at MIB (see [7]). \n                                                           \n5 Their seemingly contradictory titles notwithstanding, [12] and \n[13] contained findings which pointed in this direction."
    },
    {
        "title": "Music Scope Headphones: Natural User Interface for Selection of Music.",
        "author": [
            "Masatoshi Hamanaka"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1416566",
        "url": "https://doi.org/10.5281/zenodo.1416566",
        "ee": "https://zenodo.org/records/1416566/files/Hamanaka06.pdf",
        "abstract": "This paper describes a novel audio only interface for selecting music which enables us to select songs without having to click a mouse. Using previous music players with normal headphones, we can hear only one song at a time and we thus have to play pieces individually to select the one we want to hear from numerous new music files, which involves a large number of mouse operations. The main advantage of our headphones is that they detect natural movements, such as the head or hand moving when users are listening to music and they can focus on a particular musical source that they want to hear. By moving their head left or right, listeners can hear the source from a frontal position as the digital compass detects the change in the direction they are facing. By looking up or down, the tilt sensor will detect the change in the face’s angle of elevation; they can better hear the source that is allocated to a more distant or closer position. By putting their hand behind their ear, listeners can adjust the focus sensor on the headphones to focus on a particular musical source that they want to hear. Keywords: Headphones, music interface, digital compass, tilt sensor, infrared distance sensor.",
        "zenodo_id": 1416566,
        "dblp_key": "conf/ismir/Hamanaka06",
        "keywords": [
            "audio only interface",
            "selecting music",
            "without clicking mouse",
            "natural movements detection",
            "headphones",
            "digital compass",
            "tilt sensor",
            "focus sensor",
            "music players",
            "new music files"
        ],
        "content": "Music Scope Headphones: Natural User Interface for Selection of Music \nMasatoshi Hamanaka \nPresto, Japan Science and Technology Agency \nA.I.S.T. Mbox 604 1-1-1 Umezono, \nTsukuba, Ibaraki, 305-8568 Japan \nm.hamanaka@aist.go.jp Seunghee  Lee \nUniversity of Tsukuba  \nTennoudai 1-1-1, \nTsukuba, Ibaraki, 305-8574, Japan \nlee@kansei.tsukuba.ac.jp \nAbstract \nThis paper describes a novel audio only interface for \nselecting music which enables us to select songs without having to click a mouse. Using previous music players with normal headphones, we can hear only one song at a time and we thus have to play  pieces individually to select \nthe one we want to hear from numerous new music files, which involves a large number of mouse operations. The main advantage of our headphones is that they detect natural movements, such as the head or hand moving when users are listening to music and they can focus on a particular musical source that  they want to hear. By \nmoving their head left or right, listeners can hear the source from a frontal position as the digital compass detects the change in the di rection they are facing. By \nlooking up or down, the tilt sensor will detect the change in the face’s angle of elevati on; they can better hear the \nsource that is allocated to a mo re distant or closer position. \nBy putting their hand behind their ear, listeners can adjust the focus sensor on the headphones to focus on a particular musical source that they want to hear. \nKeywords : Headphones, music interface, digital compass, \ntilt sensor, infrared distance sensor. \n1. Introduction \nAlthough we have recently been  able to download a huge \nnumber of songs through Internet music delivery services, \nusers are only listening to a small number because opportunities to find unfamiliar musical pieces in the collection are limited. Our goal was to construct a system that would enable people to easily select musical sources \nthat they had an affinity for from many unknown ones. \nPrevious music retrieval methods that use queries such \nas similarity-based [1-3] sear ching, text-based searching \n[4], or collaborative filtering based searching [5, 6] are useful for narrowing the num ber of musical pieces, but \nafter the list of rankings has been provided we have to listen to songs one by one because  no consideration has been given to finding songs one has an affinity for from the list. \nMusicream [7], on the other hand, make it possible to \ninteract with many music collections by applying operations and providing functions for the order of play. Papipuun [8] and SmartMusicKIOSK [9] provide a music summary and allow quick listening in a manner similar to a stylus skipping on a scratched record. All these systems [7-9] enable us to save time by previewing songs from a list of rankings acquired from the results of music retrieval. However, these systems also force us to listen to songs one by one and involve many mouse operations. \nIn contrast, our system, called Music Scope Headphones, \nmake it possible to select a musical source from the many available without the need for mouse clicks or other visual manipulations by detecting natural movements when users are listening to music and focusing on the particular musical source that they want  to hear. The Music Scope \nHeadphones provide a novel musi c selection interface that \nenables the following three functions to be applied. 1. Scoping\n function : enable us to scope many musical \nsources allocated in 2-dimensional space by moving our heads left or right or by looking up or down. The function enables us to landscape songs and save time in previewing them. \n2. Focusing function : highlights a particular musical \nsource that users want to hear by them placing their hand behind an ear. This function enables us to narrow the area in which sources are audible in 2-dimensional space as if controlling the directivity of a microphone. \n3. Switching function : seamlessly changes musical \nsources in 2-dimensional space through users’ gestures such as them nodding or shaking their heads, turning them around, or leaning them to one side. For example, when users are turning their head around, the next 10 musical sources in the order on the list acquired from a music retrieval system will be allocated in 2-dimensional space. \nWe mounted three sensors to the headphones, i.e., a \ndigital compass, a tilt sensor, and a focus sensor, which detect natural movements, such as that of the head or the \nplacement of a hand behind an ear, and this allowed us to use these three functions without the need for a display or a computer mouse. Users are freed from mouse operations and can select music much more actively. \nPermission to make digital or hard copi es of all or part of this work for \npersonal or classroom use is grante d without fee provided that copies \nare not made or distributed for profit or commercial advantage and that \ncopies bear this notice and the full citation on the first page. \n© 2006 University of Victoria Previously reported headphones with sensors to detect \nthe direction users were facing or the location of the head \ncould improve the sense of musical presence and create a realistic impression, but could not highlight parts according to their wishes [10-12] . It was difficult to clearly \nhear a particular musical source from many other sources with these headphones, including some that users may have preferred not to hear. There are music spatialization systems [13, 14] that allow users to control the localization of each part in real time through a graphical interface. \nHowever, it is difficult to control each musical source’s location through this interface. \nThis paper is organized as follows. Section 2 explains \nthe functions of the headphones. Section 3 describes system processing, and Section 4 discusses the implementation. Sections 6 and 7 present the experimental results and the conclusion.  \n2. Music Scope Headphones  \nWe constructed the Music Scope Headphones that enabled us to save time in previewing songs based on the following three policies. \nReduced mouse operations  When selecting music with a \ncomputer, we generally have to play songs individually \nwith many mouse operations and these interrupts break the process of listening just as if a telephone were ringing. To \nsolve this, we propose operations without mouse clicks \nachieved by detecting natural movements when listening to music and using these to control the computer.  \nEasy preview of many songs  We wanted to increase the \nnumber of opportunities for encountering unfamiliar \nmusical pieces in collections . However, the number of \nsongs that can be previewed is limited within a fixed amount of time. This is becau se the larger the number of \nsongs to be previewed, the s horter the time to listen to each \nsong. To solve this, we propose a novel way of selecting music by playing many musical  sources at the same time. \nNo computer display  We wanted the system to be used \nanywhere and at any time such as  at work or when riding or \nwalking without the need to see a computer display.  We \nattempted to construct a system to investigate whether it were possible to select and manipulate songs without a display.  \nThe Music Scope Headphones let users control an audio \nmixer through natural movements, and thus enable them to select a musical source that they want to listen to from numerous sound sources. We will now explain the problems and solutions we encountered with the Music \nScope Headphones based on these policies. \n2.1 How musical sources are scoped  \nA particular musical source that the user temporarily wants to \nhear must be differentiated from other musical sources to scope \nit. We automatically adjusted each musical source's volume and panpot so that it could be distinguished from other musical sources. That is, we increased that source's volume and turned its panpot to the center, while decreasing the volume of \nthe other sources and turning their panpots left or right. In \nthis way, a user can easily scope a particular musical source. \n2.2 How motion is detected \nNatural movements must be detected while the user is \nlistening to music to control the audio mixer through them. To enable this, we mounted the digital compass and the tilt \nsensor on top of the headband to detect the direction the user was facing and to detect the face's angle of elevation. \nWe also mounted the focus sensor on the outside of the right speaker to detect the di stance from the hand to the ear \n(Figure 1). We prepared three focus sensor prototypes and evaluated how practical they  were in an experiment. \n \nFigure 1. Three sensors mounted to headphone. \n2.3 How function and motion are linked  \nHow usable the Music Scope Headphones are depends on the quality of the links between the functions and the users’ natural movements while they are listening to music. Let us imagine the following scenario. • We receive several session recordings from a childhood friend. \n• It sounds like the friend is playing a saxophone on one of these recordings. \nIn such a case, we would ordinarily search for songs with a saxophone part, and we then might want to hear the saxophone playing more clearly.  We used the three links \nthat follow to achieve this. \nLink for scoping function  When users move their head \nleft (right), the musical sour ce normally heard from the left \n(right) side can be heard from the frontal position as the \ndigital compass detects the cha nge in the direction they are \nfacing. This allows users, through natural movements, to scope the musical source they want to hear most clearly and hear it from the front. When  there are several musical \nsources at the front, users might not be able to hear the desired source clearly even afte r turning their head left or \nright to hear it from the front. In such a case, they can change the mix by moving their head up or down; the tilt sensor will detect the cha nge in the face's angle of \nelevation. By looking up or down, users can increase the volume of sources so that instruments appear farther away or nearer. Here, we change d each source's position in 2-\ndimensional space, as can be seen in the graphical user \ninterface in Figure 2. The circle  at the center indicates the \n \n(b)Tilt sensor \n(a) Digital Compass (c) Focus sensor \n(Infrared dist ance sensor) position of the user's avatar and his/her head direction, and \nthe circled numbers around the avatar indicate the positions of the sources.  We also had several preset \nallocations for musical sources and these were easy to change by putting one's head to one side (Figure 3).   \n \nFigure 2. GUI for locating positions of parts. \n \nFigure 3. Presets for allocation. \nLink for focusing function  The focus sensor is used to detect \nthe motion of users putting their hand behind an ear while they are listening to sound coming from the frontal position. The distance between the hand and ear determines the area in which \nsources are audible. For exampl e, when users place their hand \nclose to their ear, they can only hear the sources from the \nfrontal position. When they removes their hand, they can hear all the sources except those behi nd them. When they put their \nhand in the middle position, they can  hear the sources located in \nthe front half position. By adjusting the distance between their hand and ear in this way, they can control the focus level and highlight the source of interest. \nLink for switching function  The system has two modes, a \nsong selecting mode  and a part scoping mode . We can \nscope and preview 10 songs in the song selecting mode \nfrom those listed in order by a music retrieval system allocated in 2-dimensional space. When converging on several songs using the focusing function in the song selecting mode, users can leave focused songs and delete unfocused songs by nodding their head (Figure 4(a)). If they want more convergence, they only need to adjust the focus level and nod their head again. Conversely, users can defocus by shaking their head and return to the previous scenario (Figure 4 (b)). When they only select one song and have a sound source where the tracks for each part have been recorded separately , the system changes to the \npart scoping mode. The Music Scope Headphones provide novel entertainment with this mode through which users \ncan \"scope\" onto the part they want to hear more clearly. \nThey can return to the song selection mode by shaking their head. Users can change  the preset allocation by \nputting their head to one side (Figure 4 (c)) during the song selecting mode or the part scoping mode. By turning their head around in the song selection mode, the songs \nallocated in 2-dimensional space change to the next 10 songs from the list (Figure 4 (d)). \n(a) Select the focusing songs(d) Changes to the next 10 songs\n(c) Change the allocation of the\nmusical source by presets allocation(b) Return to the previous mode of situation\n \nFigure 4. Link for switching function. \n3. Processing \nThis section describes the processing flow for the system. We mainly describe sound processing and have omitted \nexplanations for detecting gestures, nodding, shaking, putting the head to one side, and turning it around because of word limitations. In the following, we use θ (-π \n≤ θ < π) \nas the facing direction detect ed by the digital compass, φ \n(-π  ≤ φ < π) as the face's angle of elevation detected by \nthe tilt sensor, and δ (0 ≤ δ ≤ 1) as the distance between the \nhand and the ear detected by the focus sensor (Figure 5). We use radians as angle units and set the starting direction and angle of elevation to zero. We normalized δ from 0 to 1, and \nthe focus sensor could detect a distance from 0 to 3 cm. When the distance was 0 cm, δ was output as 0, and when \nthe distance was 3 cm, δ was output as 1. When the distance \nwas between 0 and 3 cm, δ ranged from 0 to 1. \n \nFigure 5. Three sensors mounted to headphone. \nPretreatment  We prepared sound source Sn by recording a \nseparate track for each part and allocating a position on the \ngraphical user interface to each part (Figure 2). Here, ln (0  ≤ \nln ≤ 1) indicates the distance fro m the avatar to each part \nand θn indicates the direction of  each part. We normalized \nln  so that the most distant part would have a value of 1. \nStep 1  h nφ(0 ≤ h nφ ≤ 1) was calculated as the amplification \nrate for each part, n , which changes depending on the angle \nof elevation, φ. We used the following formula so that when \nusers looked up (down), the volumes of parts located far \nfrom (near to) their position would increase. \n⎪\n⎩⎪\n⎨⎧\n<<≤<\n=\n,  ~1             11~0         0~               0\nφφ φφ\nφ\nnn nn\nn\nhh hh\nh                               (1) \n Position of musical sources \n     Avatar \n(currently looking forward) \n① ② ③ ④ ⑤ ⑥ \n⑦ \n⑧ \n⑨ \n⑩ \n(a) Circle (c) Band \nθφ \nδ \nx  y z(d) Orchestra (b) Star \nwhere ∑−+=\nmm n n lml h φ φφsin1sin 1~ \nm: number of parts      . \nWhen we allocated the positions for all parts as in Figure \n6 (a), the mixing console was as in Figure 6 (b) when φ was \nzero. When φ was negative, the mixing console was as in \nFigure 6 (c), indicating that the volume of parts located \nnear to (far from) the avatar  was increased (decreased). \n \nFigure 6. Angle of elevation φ and mixing console. \nStep 2    h nδ was calculated as the amplification rate for all \nparts n, which changes according to the distance between \nthe hand and ear δ. Here, | a| indicates the absolute value \nof a, andθn’(－π  ≤ θn’< π) indicates the angle \nbetween θn and θ. \n⎪⎩⎪⎨⎧\n′<⋅′≥⋅\n=\nnn\nnh\nθδπθδπ\nδ\n         0          1                                  (2) \nFor example, h nδ= 0 corresponds to the parts located behind \nthe user and h nδ=1 corresponds to the parts in front of the \nuser when θ = π/3 and δ = 0.5 (Figure 7). In this way, we \ncan eliminate parts the user does not want to hear. \nh ＝1 nδ\nh ＝0 nδ①②③④⑤⑥⑦\n⑧\n⑨\n⑩\n⑪\n \nFigure 7. Distance from hand to ear δ and h nδ. \nStep 3   h nθ (0  ≤ h nθ ≤ 1) is calculated as the \namplification rate for all parts n, which changes according \nto the direction. The h nθ output has a large value when \nthe part is located in front of  the user and becomes smaller \nwhen the part is located  in another direction. \n⎩⎨⎧\n≤<=\n,~0                     ~0~                      0\nθ θθ\nθ\nn nn\nnh hhh                                 (3) \n \n⎪⎩⎪⎨⎧\n>⋅′⋅−=\n=. 0           10                 0      \nδδπθαδ\nθ\nn nh When we allocated the positions of all parts as in Figure 2, \nthe mixing console was as in Figure 8(a) when users were \nlooking left, as in Figure 8(b) when they were looking straight ahead, and as in Figure 8(c) when they were looking right. \n(a)Looking left       (b) Looking at  center      (c) Looking right \n \nFigure 8. Direction θ and mixing console. \nWe used an adjustable parameter, α (0 ≤ α < 1), to \ndecrease the amplification ra te when users placed their \nhand on their ear and δ < 1. When we allocated positions \nfor all parts as in Figure 2, the mixing console was as in \nFigure 9 (a) when users moved their hand away from their ear, and as in Figure 9 (b) when they moved their hand toward their ear. \n (a)Removing hand from ear ( δ = 0)     (b) Hand approaching ear ( δ > 0) \n  \n    \n  \n \nFigure 9. Decreasing amplification rate while α > 0. \nStep 4   ｐn (0  ≤ ｐn < 1)is calculated as the left/right \nvolume ratio depending on direction θ. Here, ｐn = 0 \nindicates that the ratio is 0:1 and ｐn = 0.5 indicates that it \nis 1:1. We used an adjustable parameter, β, to change the \nleft/right ratio when the users put their hand to their ear \nand δ < 1. When β> 0 and δ < 1, the panpots of the parts \nmove to the back except for th e part in the frontal position, \nand users can hear music as if focusing on the front part. \nδπθβ\n⋅′⋅+=n\nnp21                                               (4) \nStep 5    The amplification rates acquired in Steps 1 to 4 \nare multiplied and then the sound is output by  summing \nup the sounds of all parts. \nRight-side output: \n∑ ⋅⋅⋅⋅=\nnn n n n n Right p hhhS Sθδφ   and                        (5) \nLeft-side output: \n() ∑ −⋅⋅⋅⋅=\nnn n n n n Left p hhhS S 1θδφ  .                      (6) \n4. Implementation \nWe presented the processing flow for the software in the \nprevious section. It worked on the Max/MSP [16]. Here, we describe the implementation of the hardware. We implemented the headphones with the two policies that follow so that everyone  can easily use them.  \n② ③ ④ ⑤ ⑥ ⑦ ⑧ ⑨ ⑩ \n①  (c) Looking down (b) Looking  \nhorizontally  \nSmooth distribution Sharp distribution (a) Position of  \nmusical sources \nwhere • Lightweight yet strong.  \n• Easily connected to computer. \nHeadphones  We selected headphones (Zenhizer: \nHD212Pro) that had adjusters inside the headband, \nbecause we could mount the fo cus sensor outside the right \nheadband and this would therefore work stably even if the \nspeakers were moved (Figure 1). \nSensors    We mounted the attitude detection module \n(Aichi Micro Intelligent: AMI 302-ATD) to the top of the \nheadband, which consisted of the digital compass (MI \nsensor) and tilt sensor (Figure 1 (a), and (b)). Generally, the larger the angles of eleva tion, the larger the margin for \nerror in the digital compass, because it detect the direction \nof the magnetic line of force. The main advantage of using the module was that the tilt sensor could correct the output of the digital compass. The detection resolution for the module was 2 degrees. \nWe also mounted the focus sensor to the right of the \nheadband. We compared three focus-sensor prototypes in the experiments with musical novices, which are described below, and we selected the infrared distance sensor (Sharp: GP2S40J) (Figure 1 (c)). The infrared distance sensor consists of illuminant and acceptance of infrared and measures the distance between  the sensor to objects by \naccepting the reflecting infrared. We prepared a circuit for \nmounting the infrared distance sensor and we mounted a semi-variable resistor so that the sensor could detect from 0 to 3 cm. \nProtectors   We made protectors for the sensor out of \nacrylic resin (Figure 1). We tested several colors for the \nresin and selected a light pi nk because this was affected \nleast by sunlight from the windows and it widened the detection range of the sensor. \nCircuit  We integrated the information from the sensors by \nusing a microcomputer (Renesas: R8C/15) mounted inside the \nheadphone speaker housing and it output a serial signal. We could therefore reduce the numbe r of cores in the cable from \nthe headphone to the computer. The microcomputer sent a signal with output information from the sensors every 120 ms. \nUSB conversion   We used a USB converter (Silicon Labs.: \nCP2102), which converted the serial signal to USB. It was \nmounted in the middle of the cable from the headphones to enable easy connection to the computer. We mounted LEDs on all sensors to indicate whethe r they were connected to the \ncomputer. If there was a connection they blinked quickly and if there was no connection they blinked slowly and we had to re-connect the USB cable. \nPower supply  All the sensors and the microcomputer \nworked on the bus current of the USB, which simplified \nthe connection of the headphones. All we needed were the headphones and the computer. \n5. Experimental Results  \nWe designed the Music Scope headphones to enable not only \na particular song to be selected from an ordered list acquired from music retrievals but also to highlight a particular \ninstrument in the selected song that a user may want to hear more clearly. The system allows both audio files and MIDI files. In the experiments, we used RWC music database, which contains raw audio data before mix-down [15].  \n5.1 Evaluation of usability of focus sensors \nHere, we discuss our evaluation of how usable the three \nfocus-sensor prototypes were. They were (a) a variable \nresistor, (b) a bend sensor on a plastic lever, and (c) an infrared distance sensor (Figure 10). All headphones sets used the same digital compass and tilt sensor. We asked three musical novices to find a particular instrument, which we specified randomly, while listening to a song using the headphones.  We used the song RWC-MDB-J-\n2001 No. 38 [15], which was played by 10 instruments located around the avatar as in Figure 2. The subjects already knew the sound of each instrument and were allowed to use all headphones several times before the experiment to familiarize themselves with their operation. The adjustable parameters α and β described in Section 3 \nwere tuned by the subjects as they wanted. The following describes one trial of the experiment. (1) Before the song was started we specified an instrument to subjects. \n(2) We started the song at a midpoint randomly selected for the specified instrument.   \n(3) We measured the time the s ubjects needed to find the \ninstrument.  \nThe location of all instruments were randomly changed at \nevery trial. The musical novices changed their headphones after every 10 trials. \n \n(a)Variable resistor  (b) Bend sens or   (c) Infrared distance sensor \n \nFigure 10. Three types of focus sensors .  \nTable 1 lists the average results from 100 trials. While the \nbend sensor was no less accurate th an the variable resistor or \nthe infrared sensor, it was attached to a plastic lever, which made it difficult to precisely control. Subjects A and C could find an instrument more quickly when using the infrared focus sensor. Subject B, on the other hand, could find an instrument more quickly when using the variable resistor. We selected the infrared distan ce sensor because the average \ntime for the three subjects was the shortest. \n \nTable 1. Comparison of three kinds of focus sensors.  \n Variable resistance Bend sensor  Infrared distance sensor \nSubject A 1.84 sec. 1.28 sec. 1.12 sec. \nSubject B 0.72 sec. 1.04 sec. 0.84 sec. \nSubject C 1.02 sec. 2.01 sec. 0.74 sec. \nAverage 1.19 sec. 1.44 sec. 0.90 sec. 5.2 Evaluations of usability for selecting songs  \nWe evaluated whether users could select a song by using the \nMusic Scope Headphones. We asked three musical novices to find a song with a soprano saxophone from the RWC-MDB-J-2001 database [15]. It  had fifty jazz songs and only one song \nhad a soprano saxophone part. We measured the time the subject needed to find the soprano saxophone. The subjects had not heard the songs on the database before the experiment except for RWC-MDB-J-2001 No. 38, which we had used in the experiment in Section 5.1. After measuring the time using the Music Scope Headphones, we measured the time for same trial using Windows Mediaplayer, which is a standard music player pre-installed in Window s XP. The time for each subject \nto find the musical instrument was only measured one for the \nMusic Scope Headphones and Windows Mediaplayer. \nTable 2 lists the results obtained with the Musical Scope \nHeadphones and Windows Mediaplayer. All the subjects could \nfind the song more quickly when using our Music Scope Headphones, but Windows Mediaplayer was handicapped because the subjects may have me morized the songs in the first \ntrial with the Music Scope Headphones. As a result, our experiment revealed that the Music Scope Headphones were superior for previewing songs from an ordered list. \nTable 2. Comparison of our system and standard music player.  \n Music Scope Headphones Windows Media Player \nSubject A 224 sec. 845 sec. \nSubject B 423 sec. 1145 sec. \nSubject C 642 sec. 751 sec. \nAverage 429 sec. 914 sec. \n6. Conclusion  \nThe Music Scope Headphones enabled wearers to control an audio mixer through natural movements that enabled them not only to select a song from an ordered list acquired from music retrievals but also to highlight a particular instrument in the selected song that they wanted to hear more clearly. Three sensors were mounted to the headphones: a digital compass, a tilt sensor, and a focus sensor for detecting natural movements. This freed users from mouse operations so they could select music much more actively. We tested how usable three kinds \nof focus sensors were and found that an infrared distance sensor was better than either a variable resistor or a bend sensor from the average time it took three subjects to locate an instrument. We also tested how efficiently the headphones were in selecting songs and the results revealed that they performed better than the standard Windows Mediaplayer by being able to select a particular song from fifty others.  \nWe are now developing other applications for the \nheadphones. Figure 11 shows where the light’s brightness has been controlled according to the sound level at the music stands. This allows the user to experience all sound levels visually as well as aurally. This should help musical novices who do not know what individual instruments sound like to learn the relationship between these and the entire piece. The \nvideo is available at http://staff.aist.go.jp/m.hamanaka/video/. We plan to use these headphones with music retrieval \nbased on voice recognition to construct a system in which \na display and a mouse are unnecessary. \n \nFigure 11. Lighting depending on sound levels at music stands.  \nReferences \n[1] G. Tzanetakis and P. Cook. Musi cal genre classification of audio \nsignals. IEEE Trans. on Speech and Audio Proc., 10(5): \n293–302, 2002. \n[2] F. Vignoli and S. Pauws. A mu sic retrieval system based on \nuser-driven similarity and its ev aluation. In Proc. of ISMIR 2005, \npp. 272–279, 2005. \n[3] E. Pampalk. A MATLAB toolbox to compute music \nsimilarity from audio. In Proc. of ISMIR2004, pp. 254–257, \n2004. \n[4] T. Soding and A. F. Smeaton. Evaluating a music information \nretrieval system - TREC style. In Proc. of ISMIR2002, pp. 71–\n78, 2002. \n[5] W. W. Cohen and W. Fan, “Web-collaborative filtering: \nRecommending music by crawling the Web. ” \nWWW9/Computer Networks, 33 (1-6): 685 –698, 2000. \n[6] A. Uitdenbogerd and R. van Sc hyndel. A review of factors \naffecting music recommender su ccess. In Proc. ISMIR2002, \npp. 204–208, 2002. \n[7] M. Goto and T. Goto. Musicream: New Music Playback \nInterface for Streaming, Sticking, Sorting, and Recalling Musical Pieces, In Proc. of ISMIR 2005, pp. 404–411, 2005. \n[8] K. Hirata and S. Matsuda. In teractive Music Summarization \nBased on GTTM. In Proc. of ISMIR 2002, pp. 86–93, 2002. \n[9] M. Goto: SmartMusicKIOSK: Mu sic Listening Station with \nChorus-search Function, In Proc. of UIST 2003, pp. 31–40, 2003.  \n[10] Warusfel, O. and Eckel, G. LISTEN - Augmenting Everyday Environments through Interac tive Soundscapes. In Proc. IEEE \nVR2004, pp. 268–275, 2004. \n[11]Wu, J., Duh, C., Ouhyoung, M., and Wu, J. 1997. Head \nMotion and Latency Compensation on Localization of 3D \nSound in Virtual Reality. In Proc. ACM VRCIA1997, pp. 15–20, 1997. \n[12]Goudeseune, C., and Kaczmarski, H.. Composing Outdoor \nAugmented-reality Sound Environments. In Proc. of ICMC2001, pp. 83–86, 2001 \n[13]Pachet, F. and Delerue, O. A Mixed 2D/3D Interface for Music \nSpatialization. In Proc. of ICVW1998, pp. 298–307, 1998. \n[14]Pachet, F. and Delerue, O. On-the-Fly Multi-track Mixing.. \nIn Proc. of AES2000, 2000. \n[15]Goto, M., Hashiguchi, H., Nishimura, T., and Oka, R. RWC \nMusic Database: Popular, Classical, and Jazz Music Databases. In Proc. of  ISMIR2002, pp. 287–288, 2002. \n[16]cycling74. http://www.cycli ng74.com/products/maxmsp/, 2006."
    },
    {
        "title": "Evolving Performance Models by Performance Similarity: Beyond Note-to-note Transformations.",
        "author": [
            "Amaury Hazan",
            "Maarten Grachten",
            "Rafael Ramírez 0001"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1417691",
        "url": "https://doi.org/10.5281/zenodo.1417691",
        "ee": "https://zenodo.org/records/1417691/files/HazanGR06.pdf",
        "abstract": "This paper focuses on expressive music performance mod- eling. We induce a population of score-driven performance models using a database of annotated performances extracted from saxophone acoustic recordings of jazz standards. In addition to note-to-note timing transformations that are in- variably introduced in human renditions, more extensive al- terations that lead to insertions and deletions of notes are usual in jazz performance. In spite of this, inductive ap- proaches usually treat these latter alterations as artifacts. As a first step, we integrate part of the alterations occurring in jazz performances in an evolutionary regression tree model based on strongly typed genetic programming (STGP). This is made possible (i) by creating a new regression data type that includes a range of melodic alterations and (ii) by using a similarity measurement based on an edit-distance fit to hu- man performance similarity judgments. Finally, we present the results of both learning and generalization experiments using a set of standards from the Real Book. Keywords: Evolutionary Modeling, Expressive Music Per- formance, Melodic Similarity",
        "zenodo_id": 1417691,
        "dblp_key": "conf/ismir/HazanGR06",
        "keywords": [
            "Expressive music performance modeling",
            "inducing score-driven models",
            "database of annotated performances",
            "jazz standards",
            "note-to-note timing transformations",
            "melodic alterations",
            "evolutionary regression tree model",
            "strongly typed genetic programming",
            "edit-distance similarity",
            "Real Book standards"
        ],
        "content": "Evolving Performance Models by Performance Similarity: Beyond Note-to-note\nTransformations\nAmaury Hazan\nMusic Technology Group\nPompeu Fabra University\nOcata 1\n08001 Barcelona, Spain\nahazan@iua.upf.eduMaarten Grachten\nArtiﬁcial Intelligence Research Institute\nSpanish Council for ScientiﬁcResearch\n( IIIA - CSIC )\nCampus UAB, 08193 Bellaterra, Spain\nmaarten@iiia.csic.esRafael Ramirez\nMusic Technology Group\nPompeu Fabra University\nOcata 1\n08001 Barcelona, Spain\nrramirez@iua.upf.edu\nAbstract\nThis paper focuses on expressive music performance mod-\neling. We induce a population of score-driven performance\nmodelsusingadatabaseofannotatedperformancesextracted\nfrom saxophone acoustic recordings of jazz standards. In\naddition to note-to-note timing transformations that are in-\nvariablyintroducedinhumanrenditions,moreextensiveal-\nterations that lead to insertions and deletions of notes are\nusual in jazz performance. In spite of this, inductive ap-\nproachesusuallytreattheselatteralterationsasartifacts. As\na ﬁrst step, we integrate part of the alterations occurring in\njazz performances in an evolutionary regression tree model\nbasedonstronglytypedgeneticprogramming(STGP).This\nis made possible (i) by creating a new regression data type\nthatincludesarangeofmelodicalterationsand(ii)byusing\nasimilaritymeasurementbasedonanedit-distanceﬁttohu-\nman performance similarity judgments. Finally, we present\nthe results of both learning and generalization experiments\nusing a set of standards from the Real Book .\nKeywords: Evolutionary Modeling, Expressive Music Per-\nformance, Melodic Similarity\n1. Introduction\nModelingexpressivemusicperformanceisachallengingas-\npectforseveralresearchareasrangingfromcomputermusic\nto artiﬁcial intelligence and behavioral psychology. The fo-\ncus of this paper is the study of how skilled musicians (sax-\nophonejazzplayersinparticular)expressandcommunicate\ntheir view of the musical and emotional content of musi-\ncal pieces by introducing deviations and changes of various\nparameters. Traditionally, the deviations introduced by the\nperformerarestudiedonanote-to-notebasis. However,jazz\nperformance is characterized by a stronger manifestation of\nexpressivity. A set of common deviations, which we call\nperformance events , is listed below.\nInsertion The occurrence of a performed note that is not in the\nscore\nPermissiontomakedigitalorhardcopiesofallorpartofthisworkfor\npersonal or classroom use is granted without fee provided that copies\narenotmadeordistributedforproﬁtorcommercialadvantageandthat\ncopies bear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2006 University of VictoriaDeletion The non-occurrence of a score note in the performance\nConsolidation The agglomeration of multiple score notes into a\nsingle performed note\nFragmentation The performance of a single score note as multi-\nple notes\nTransformation The change of nominal note features like onset\ntime, duration, pitch, and dynamics\nOrnamentation Theinsertionofoneorseveralshortnotestoan-\nticipate another performed note\nThe listed performance events tend to occur persistently\nthroughoutdifferentperformancesofthesamephrase. More-\nover, performances including such events sound perfectly\nnatural,somuchthatitissometimeshardtorecognizethem\nasdeviatingfromthenotatedscore. Thissupportsourclaim\nthat even the more extensive deviations are actually a com-\nmon aspect of (jazz) performance. As a starting point, we\ndeal in this paper with the performance events that occur\nmost commonly in our training database, namely transfor-\nmations, consolidations and ornamentations. Finally, more\nsubtle alterations such as intra-note modulations (e.g. vi-\nbrato or loudness shape, see [10]) play an important role in\na saxophone rendition, however such alterations are out of\nthescopeofthiswork. Wefocushereontheinﬂuenceofthe\nmelodiccontextontheexpressivegesturespresentedabove.\nThe problem can be stated as follows: how do underlying\npatternsinthescoreaccountforexpressivegesturesinaper-\nformance? Our approach is the following: we extract a set\nof features describing the melodic context of a given set of\nmusical fragments, e.g. the melodic and rhythmic intervals\nbetweennotes,theirmetricalpositionwithinthebar,orhow\nthey are perceptually grouped. On the other hand, we ana-\nlyze the recordings of the corresponding pieces performed\nby a saxophone player. Each performance transcription can\nbestoredandconsideredastrainingperformance. Basedon\nthe melodic context features, we used each model to pre-\ndict a set of performance events. A similarity assessment\nbetween training and predicted performances is then com-\nputed (see Section. 3) and used to guide the evolutionary\nprocess. Figure 1 summarizes this approach.\nThe modeling task lies in ﬁnding an appropriate map-\nping from melodic descriptors to expressive features thatFigure 1. General framework for evolving a performance\nmodel. For each training fragments, we provide an acous-\ntic recording of the performance and the corresponding score.\nThetranscriptionoftherecordingisconsideredastargetandis\ncompared with the melody generated by the model. The com-\nputedsimilarityinusedtocharacterizetheﬁtnessofthemodel\nin the evolutionary process\nreﬂects patterns appearing in a training set. Moreover, we\nare interested in a mapping that shows good generalization\ncapabilities i.e. that produces accurate predictions when\nprocessing unseen data. Despite of the challenge it rep-\nresents for multidisciplinary research, inductive expressive\nperformance modeling has motivated relatively few works.\nIn [12, 14, 13] the authors investigate loudness and tempo\nvariationsofclassicalpianoperformancesandinducemulti-\nlevel performance models. In [4] the authors use a Case-\nBase Reasoning system to model expressivity-aware tempo\ntransformations and introduce a framework to describe ex-\ntensive melody alterations. In [9], we use Inductive Logic\nProgramming to build a set of independent greedy regres-\nsion tree models that can generate and explain expressive\nperformancesregardingtiming,ornamentationsandconsol-\nidations. A major drawback of this approach is that the dif-\nferent models are induced separately. As a consequence,\nthe set of tree models may produce predictions that are not\nconsistent. Here, we present a way to integrate genera-\ntivepredictions for a range of performance events into self-\ncontained performance tree models. This is made possi-\nble by using an evolutionary regression tree approach ([5]).\nThis technique is ﬂexible enough (i) to produce structured\npredictions that are needed to integrate a number of possi-\nble performance events, and (ii) to let us deﬁne an accuracy\nmeasurement that goes beyondpair-wise comparisons.\n1.1. Why use evolutionary computation techniques?\nWe present in this section the arguments that led us to con-\nsider evolutionary computation (EC), and more speciﬁcally\nstrongly typed genetic programming (STGP, [8]), as conve-\nnient methods for building expressive performance models.\nEveniftheaspectswepresenthereareofinterestforapply-\ning EC to a whole range of domains [6], we focus on how\nthese techniques can beneﬁt our particular problem.\n1.1.1. Population of performance models\nHumanperformanceisaninexactphenomenon: evenifper-\nformers learn through rehearsal to produce globally similarperformances,theycouldhardlyreproduceexactlyidentical\nrenditions. This is particularly true in jazz music, where\nthe improvisational contribution to a performance is fun-\ndamental. At each new rendition, small alterations in the\ntiming and other expressive features of the musical piece\nareintroduced. Expressiveperformancemodelsshouldtake\ninto account this aspect. In contrast, traditional decision\ntree models are essentially deterministic. That is, once in-\nduced a feature-based decision tree model will produce a\nconstant prediction for transforming a given fragment. EC\nprovideanelegantwaytocopewiththislimitationbyevolv-\ningapopulationofmodels,withbeneﬁtsforbothend-users\nand researchers. The former have access to a population of\nmodels,eachonewhichproducedistinctsetofperformance\nevents. The latter can reﬁne speciﬁc design decisions by\nanalyzing their consequenceon the whole population.\n1.1.2. Customdatatypesforstructuredmodeling/prediction\nAlthoughitispossibletouseevolutionarycomputationtech-\nniques in a straightforward way i.e. using templates and\nwell-deﬁned data types, the EC programmer may design\ncustom data types for the model inputs and outputs. In the\ncaseofSTGP,itispossibletoreﬁnetheinput/outputspeciﬁ-\ncations for each primitive of the genetic program. Here, we\nmakeuseofsuchﬂexibilitytodeﬁneastructuredprediction\ndata type that takes into account features of note timing, or-\nnamentations, consolidations. This is a ﬁrst step towards a\nmore complete prediction of performance events. We will\npresent this prediction typein detail in the next section.\n1.1.3. Integration of domain-speciﬁc knowledge\nWhen modeling a given problem, it is sometimes crucial to\nintegrateelementsofdomain-speciﬁcknowledgeinthesys-\ntem. In the case of feature-based tree modeling, one can\ndeﬁne this knowledge at several levels. First, the tree struc-\nture has to be deﬁned so that it matches a regression tree\nstructure. Also, one can easily handle the prior distribution\nof inputs and outputs by deﬁning the initialization operator\nforeachprimitiveinuseinthegeneticprogram. Inourcase,\nwe refer to constant generation : on the one hand, features\nare compared to constants across the successive tests, these\nconstants have to respect the statistical distribution of the\nfeature space. On the other hand, single or even structured\nconstant predictions are generated independently to where\nthey will appear in a tree. Consequently, the role of the ini-\ntialization operator is to provide a ﬁrst guess for both fea-\ntures and prediction values. Finally, the most central aspect\nof EC is that the user has the freedom (and the responsi-\nbility) to deﬁne a ﬁtness function that is suitable for a given\nproblem. Thisenablesustoincludeelaborateaccuracymea-\nsurements such as performance similarity in the evolution\nprocess.\nThe rest of the paper is organized as follows: Section\n2 presents the types, primitives, and operators of the tree\nmodel. We deﬁne the evolutionary constraints needed forproducing consistent regression trees from both typing and\nlogical point of view, and deﬁne a prediction type for our\nproblem. Section 3 presents a performance-similarity crite-\nrionbasedonedit-distance. InSection4weshowthelearn-\ningandgeneralizationaccuracyoftheevolvedmodelsusing\nan expressive performance database and discuss the results.\nFinally,Section5presentsourconclusionsaswellasfuture\nextensions to this work.\n2. Evolutionary regression trees\nRegression tree models are a speciﬁc instance of tree pro-\ngrams. Their general structure is the following: Each node\nis a test comparing an input with a numerical constant (typ-\nically alower than inequality). The outcome of a test is\neither a new test or a prediction. Furthermore, successive\ntests must be arranged to form consistent rules. We inte-\ngrate these constraints in the evolutionary process. First,\nit is possible to ensure that the models will be structurally\nconsistent by deﬁning appropriate types and primitives. We\nthen address the issue of logical consistency and deﬁne a\nmechanismtoensureitbyoverridingtheevolutionaryoper-\nators.\n2.1. Types and primitives\nType consistency refers to how each primitive in the tree\nis connected to its neighbors. We use this STGP feature\nto ensure the desired regression tree structure. We deﬁne\nfour different types: InputValue ,FeatValue ,BoolandReg-\nValue. The ﬁrst two types represent ﬂoating-point values,\nthe ﬁrst being used as terminal for the inputs of the model,\nthesecondoneencapsulatingconstantstobecomparedwith\nthe inputs. The third type represents boolean values used as\noutcome of these tests.\nThelasttypeisusedforencodingthemodelpredictions.\nWe give in table 1 details of the RegValue data type. The\nﬁrst three RegValue members, namely Duration Ratio, On-\nset Deviation and Relative Loudness, are ﬂoat-valued and\nthey refer to a transformation event. The fourth member,\nnamely Ornamentation Event, is a boolean value indicat-\ning whether an ornamentation will be generated. If it is\nfalse, no ornamentation will be generated, and members 5\nto8(namelyOrnamentationRelativeOnset,Ornamentation\nAbsolute Duration, Ornamentation Relative Pitch and Or-\nnamentation Relative Loudness) are ignored. Otherwise,\nwe use these ﬂoat-valued members to generate the orna-\nmentation. The ninth RegValue member, namely Consoli-\ndation Event, is a boolean indicating whether a consolida-\ntion will be generated. Otherwise, the last Consolidation\nRelative Duration member is used along with the three ﬁrst\nmembers to produce a consolidation. Note that this design\nallow the system to predict the following events combina-\ntions: transformation, ornamentation+transformation, con-\nsolidation, ornamentation+consolidation.\nCorresponding to these types, we present in table 2 theTable 1.RegValue data type description\nMember Name Type\nDuration Ratio ﬂoat\nOnset Deviation ﬂoat\nRelative Loudness ﬂoat\nOrnamentation Event bool\nOrnamentation Relative Onset ﬂoat\nOrnamentation Absolute Duration ﬂoat\nOrnamentation Relative Pitch ﬂoat\nOrnamentation Relative Loudness ﬂoat\nConsolidation Event bool\nConsolidation Relative Duration ﬂoat\nTable 2. STGP primitives in use for this work. Each primitive\nis presented along with the number of arguments it handles,\nthetypeofeachargument,anditsreturntype;Notethatprim-\nitives EFV and ERV, which are used for constant generation,\ntake no argument as input\nNameNb args Arg. Type Return Type\nLT2 1st:InputValue , 2nd\nFeatValueBool\nIF3 1st:Bool, 2nd and\n3rd:RegValueRegValue\nEFV0 - FeatValue\nERV0 - RegValue\nneededprimitivesandshowhowtheycorrespondtothestruc-\nture of a regression tree. First, LT primitive tests whether\nan input of type InputValue is lower than a FeatValue typed\nconstantgeneratedbytheEFVprimitive. ALTprimitivere-\nturnsaBoolthatisusedasﬁrstargumentoftheIFprimitive.\nThe latter performs a test on this boolean value and returns\naRegValue. If the test succeeds, the second argument is re-\nturned, otherwise, the third argument is returned. Both of\nthese arguments are RegValue typed. Finally ERV primi-\ntive generates an RegValue prediction constant. As a result,\nthesecondandthirdargumentsofanIFprimitivecaneither\nbe linked to an ERV primitive, or to another IF primitive,\nchaining successive tests until an ERV primitive is reached.\n2.2. Logical consistency\nBased on these ideas we show in Figure 2 two prototypical\nmodels that are type consistent. Both models perform two\nsuccessive tests involving the InputValue primitive labeled\nIN0. The root of both trees, indicated by an arrow, ﬁrst\ntests whether IN0is lower than a constant equal to 0. If\nthe test succeeds, an ERV primitive connected to the sec-\nond argument of the root is returned. Otherwise, a new\ntest is performed via the right-most IF primitive, checking\nwhether IN0is either lower than 1 (upper model), or -1Figure2. Twotype-consistentregressiontrees. (Top)Logically\nconsistent tree, (Bottom) Logically inconsistent tree\n(lowermodel). Wecanseethattheuppermodelislogically\nconsistent: its tests are plausibly structured and can possi-\nblyrepresentthefeaturespace. Oppositely,thelowermodel\nis logically inconsistent: because the right-most test is not\nlogicallyconsistentwiththeleft-mostone,alltheprimitives\nmarkedingrayturntobeuseless. Thisleadstoasituationof\ncode bloat [11] where the evolved tree may contain useless\nbranches, called introns. The inﬂuence of code bloat in the\nresults of an evolutionary system is still debated and may\ndepend of the model application domain. However, in this\npaper,becauseourworkispreliminary,wedecidetodiscard\nindividuals containing such introns by introducing a logical\nconsistency check mechanism. As a future extension, we\nplantostudythoroughlytheinﬂuenceofsuchintronsinthe\nevolutionary process for our particular domain.\n2.3. Genetic Operators\nIn order to implement the logical consistency check pre-\nsented above, we slightly modiﬁed the operators provided\nin [1], which are a reﬁnement of the strongly typed oper-\nators proposed in [8]. We modiﬁed the base class of each\noftheseoperatorstoimplementalogicalconsistencycheck.\nThemainoperatorsareTreeCrossover,wheretwoindividu-\nalscanswapsubtreeandStandardMutation,whereasubtree\nis replaced by a newly generated one. Additionally, Shrink\nmutation replaces a branch with one of its child nodes, and\nSwap mutation swaps two subtrees of an individual.\n2.4. Constant Generation\nFinally we use the ERV (respectively EFV) primitive for\ngenerating constants that are used for prediction (respec-\ntively comparison with inputs). ERV and EFV initializationoperatorsareusedtointegratethedistributionofbothinputs\nand outputs in the training data. We approximate these dis-\ntributionswithgaussians,andusethelatterwhengenerating\nrandom constants. This is a ﬁrst step towards using a larger\nrepertoire of statistical distributions (e.g. gamma distribu-\ntions, mixture of gaussians).\nWe deﬁned in this section a framework for evolving re-\ngressiontreesfromthetypingandlogicalpointofview. Ad-\nditionally, we use a mechanism for constant generation that\nreﬂects both input and output distributions of the training\ndata. What we need now is a ﬁtness measure in the context\nof performance modeling.\n3. Accuracy for performance modeling:\nﬁtness evaluation\nIn this section, we devise a ﬁtness function to guide the\nmodelsearchintherightdirection. Theﬁtnessfunctionper-\nformsaperformancesimilaritymeasurementusingtheedit-\ndistance. The formulas we introduce apply to the ﬁtness\ncomputation for a given performance fragment. The aver-\nagemodelﬁtnessiscomputedoverthedistinctfragmentsof\nthe training database.\n3.1. Performance similaritybased on edit-distance\nHowaccuratelycanagivenmodellearntogenerateexpres-\nsive fragments? We want to measure how similar is a pre-\ndicted performance with the training one. To achieve this,\nwe use a edit-distance that was ﬁt to human performance\nsimilarity judgments, as shown below. The edit-distance\n[7] is used to determine the similarity between two per-\nformances of the same melody. It is deﬁned as the min-\nimal total cost of a sequence of editions needed to trans-\nform one performance into the other, given edit-operations\nlike deletion, insertion, and replacement. The edit-distance\nis ﬂexible enough to accommodate for comparison of per-\nformances of different lengths (in case of e.g. consolida-\ntion/fragmentation)anditallowsforcustomizationtoapar-\nticularusebyadjustingparametervaluesoftheedit-operation\ncost functions. The cost of a particular edit-operation is de-\nﬁned through a cost function wthat computes the cost of\napplying that operation to zero or more notes of one perfor-\nmance and zero or more of the other, given as parameters\ntow. We deﬁned the following cost functions for 1-0, 0-1,\n1-1, N-1, and 1-N mapping edit-operations, which we write\nrespectively w(si,∅),w(∅,tj),w(si,tj),w(si−K:i,tj)and\nw(si,tj−L:j):\nw(si,∅) = α1(δD(si) + /epsilon1E(si) ) + β1\nw(∅,tj) = α1(δD(tj) + /epsilon1E(tj) ) + β1\nw(si,tj) = α2\nπ|P(si)− P(tj)|+\nδ|D(si)− D(tj)|+\no|O(si)− O(tj)|+\n/epsilon1|E(si)− E(tj)|\n+β2w(si−K:i,tj) = α3\nπ/summationtextK\nk=0|P(si−k)− P(tj)|+\nδ|D(tj)−/summationtextK\nk=0D(si−k)|+\no|O(si−K)− O(tj)|+\n/epsilon1/summationtextK\nk=0|E(si−k)− E(tj)|\n+β3\nw(si,tj−L:j) = α3\nπ/summationtextL\nl=0|P(si)− P(tj−l)|+\nδ|D(si)−/summationtextL\nl=0D(tj−l)|+\no|O(si)− O(tj−L)|+\n/epsilon1/summationtextL\nl=0|E(si)− E(tj−l)|\n+β3\nwhere sand tare the performances as sequences of per-\nformed notes. Subsequences are denoted si:j= (si, ..., s j),\nand si= (si), andP(si),D(si),O(si), andE(si)respec-\ntively represent the pitch, duration, onset, and dynamics at-\ntributes of a note si. Each attribute has a corresponding pa-\nrameter( π,δ,o,and /epsilon1,respectively),thatcontrolstheimpact\nof that attribute on operation costs. The βparameters con-\ntrol the absolute cost of the operations. The αparameters\ncontrol the partial cost of the operation due to (differences\nin) attribute values of the notes.\n3.1.1. Fitting the edit-distanceto human judgments\nThe cost parameters presented above were optimized in [4]\nto ﬁt human performance similarity judgments based on a\nwebsurvey. Inthissurvey,92subjectswerepresentedques-\ntions containing target performance A(the nominal perfor-\nmance, without expressive deviations) of a short musical\nfragment, and two different performances BandCof the\nsame fragment. The task was to indicate which of the two\nalternative performances was perceived as most similar to\nthe target performance. The edit-distance cost parameters\nwere optimized using a genetic algorithm to ﬁt the survey\nresults. We show in table 3 theﬁnal parameters values.\nThe optimized edit-distance is used to compute the ﬁt-\nness of the model, by comparing the model output perfor-\nmance to the training performance. We obtain the value of\nthe error-driven ﬁtness component by applying the follow-\ning formula:\nferror =1\n1 +editdistance(1)\n4. Experimental results and discussion\n4.1. Data\nInthispaperweuseanexpressiveperformancedatabasethat\ncomes from annotations of acoustic saxophone recordings.\nWe consider four jazz from standards the Real Book . The\nexcerpts are the following: Body and Soul ,Once I Loved ,\nUp Jumped Spring ,Like Someone In Love . We character-\nize the score melodic context of this data using, for each\nnote,thefollowingfeatures: noteduration,metricalstrength\nwithin the bar, previous and next note relative duration and\npreviousandnextnoterelativeinterval. Weanalyzethecor-\nresponding acoustic recordings to obtain the performance\ntranscriptions. See [3] for detailed description of the tran-\nscription process.\n0 50 100 150 200\nGeneration Number212223242526Average Edit-DistanceFigure 3. Average edit-distance of best-of-run model during\nthe evolution. Plain: training data, dashed: test data\n4.2. Method\nThetrainingandgeneralizationexperimentswepresenthere\nare completed as follows: we include in the training data\n(usedforcalculatingtheﬁtness)themusicaldatathatcomes\nfrom the ﬁrst two thirds of each of the four musical frag-\nments presented above. The material of the last third of\neach fragment is used as test data. That is, we train the\nmodeltopredicttheperformanceeventsthattakeplacedur-\ningtheﬁrstpartofeachfragment. Duringthetestphase,we\nevaluatehowaccuratelythemodelisabletopredicttheper-\nformance events from the end of these fragments. The test\nfragments were carefully chosen so that they do not contain\nmelodic information present in the training set.\n4.3. Evolutionary settings and Implementation\nWe used the following evolutionary settings for this experi-\nment. The population size was set to 1000 individuals, and\nthe maximum number of generations was set to 200. We\nused the following probabilities for our evolutionary oper-\nators: Crossover probability is set to 0.9, with a branch-\ncrossover probability of 0.2. The Standard mutation prob-\nability is set to 0.01, while the Swap mutation has been set\nto 0.1. Finally, Shrink mutation probability is set 0.05. The\nmaximum tree depth has been set to 20.\n4.4. Results\nFigure 3 shows a plot of the average edit distance over the\ntraining fragments (solid) and the test fragments (dashed)\nfor the best-of-run model during the evolution. The x-axis\nindicates the generation number while the y-axis shows the\naverageeditdistancefortrainingandtestset. Wecannotice\na ﬁrst period in which the overall distance to the actually\nperformedfragmentsisgloballydecreasinguntilgeneration\n20. After this period, we can observe a clear tendency: the\ntraining distance decreases while the test distance does notTable 3. Optimized values of edit-operation cost parameters\nα1 α2 α3 β1 β2 β3 π δ o /epsilon1\n0.031 0.875 0.243 0.040 0.3300.380 0.452 1.000 0.120 0.545\ndecrease anymore. This is a situation of overﬁtting, which\nmight be a consequence of the limited amount of training\ndata. To conﬁrm this, we can see that the maximum depth\nof the evolved models is 11, while the tree programs were\nallowed to grow to a depth of 20. This means that the train-\ningdataarenotsufﬁcientforthemodelstogrowenoughand\nﬁnd a system of rules that is suitable for both training and\ntest set.\n5. Conclusion\nWe have presented an approach to derive evolutionary re-\ngression trees for modeling expressive music performance.\nWe ﬁrst presented the beneﬁts of evolutionary computation\nandstronglytypedgeneticprogramming,whichcanbesum-\nmarized as follows: evolution of a population of models,\nprior knowledge integration, and ﬂexibility regarding the\ndata types, structures, and model evaluation. In continua-\ntion, we deﬁned the basis of a structured-prediction regres-\nsion tree for modeling expressive music performance. We\nspeciﬁedthetypes,primitives,operatorsandﬁtnessfunction\nusedintheSTGPframework. Wehavesetthebasisofaper-\nformancemodelingevolutionaryframeworkbasedonaper-\nformancesimilaritymeasurethatwasﬁttohumansimilarity\njudgments. Weﬁnallypresentedpreliminaryresultsforboth\nlearning and generalization experiments, using a small ex-\npressive performance database annotated from monophonic\nrecordings of jazz standards. We compared the training and\ntestperformancesimilarities,toidentifyanoverﬁttingpoint.\nWe plan to extend our work in the following directions:\nﬁrst we want to increase substantially the size of the anno-\ntated performance database we are using in order to more\nrobustly assess the training and generalization abilities of\ntheevolvedmodels. Wewilltakeadvantageofrecentworks\nsuch as [2] that highlight the importance of a validation set\nfor selecting robust best-of-run models. At this point, it is\nworth mentioning that the incorporation of an edit-distance\nbased ﬁtness computation into the evolutionary framework\nhas led to a massive use of computation time: increasing\nsubstantiallythetrainingdatabasewillrequireustosetadis-\ntributedframework,whichisfeaturedinOpenBeagle. Also,\nwe will integrate an extended set of performance events in\norder to reﬂect all the possible manifestations of expressiv-\nitythatcharacterizejazzperformance. Finally,inouractual\napproach we achieve score-driven performance prediction.\nInthecontextofsequentialprocessing,anexcitingworkdi-\nrectionisstudyingthecontributionofbothscorecontextand\npast expressive gestures in music performance.References\n[1] C. Gagn ´e and M. Parizeau. Open beagle manual. Technical\nReport RT-LVSN-2003-01-V300-R1, Laboratoire de Vision\net Syst`emes num ´erique, Universit ´e de Laval, 2005.\n[2] C. Gagn ´e, M. Schoenauer, M. Parizeau, and M. Tomassini.\nGenetic programming, validation sets, and parsimony pres-\nsure. In P.Collet et al., editor, EuroGP 2006, LNCS 3905 ,\npages 109–120, 2006.\n[3] E. G ´omez, M. Grachten, X. Amatriain, and J. Arcos.\nMelodic characterization of monophonic recordings for ex-\npressive tempo transformations. In Proceedings of Stock-\nholm Music Acoustics Conference 2003 , Stockholm, Swe-\nden, 2003.\n[4] M. Grachten, J.L. Arcos, and R. L ´opez de Mantaras. A case\nbasedapproachtoexpressivity-awaretempotransformation.\nMachine Learning (in press) , 2006.\n[5] A.Hazan,R.Ramirez,E.Maestre,A.Perez,andA.Pertusa.\nModelling expressive music performance: a regression tree\napproach based on strongly-typed genetic programming. In\nForth European Workshop on Evolutionary Music and Art ,\npages 676–687, Budapest, Hungary, 2006.\n[6] J.R. Koza. Genetic Programming: On the programming of\nComputers by means of natural Selection . MIT Press, Cam-\nbridge, MA, USA, 1992.\n[7] V. I. Levenshtein. Binary codes capable of correcting dele-\ntions, insertions and reversals. Soviet Physics Doklady ,\n10:707–710, 1966.\n[8] D.J.Montana. Stronglytypedgeneticprogramming. Techni-\ncalReport7866,10MoultonStreet,Cambridge,MA02138,\nUSA, 7 1993.\n[9] R. Ramirez and A. Hazan. A tool for generating and ex-\nplainingexpressivemusicperformancesofmonophonicjazz\nmelodies. In International Journal on Artiﬁcial Intelligence\nTools (to appear) , 2006.\n[10] R. Ramirez, A. Hazan, and E. Maestre. Intra-note features\nprediction model for jazz saxophone performance. In In-\nternational Computer Music Conference , Barcelona, Spain,\n2005.\n[11] Walter Alden Tackett. Greedy recombination and genetic\nsearch on the space of computer programs. In Foundations\nof Genetic Algorithms 3 . Morgan Kaufmann, San Francisco,\nCA, 1995.\n[12] A. Tobudic and G. Widmer. Relational IBL in music with\na new structural similarity measure. In Proceedings of the\nInternational Conference on Inductive Logic Programming ,\npages 365–382, 2003.\n[13] G. Widmer. In search of the horowitz factor: Interim re-\nport on a musical discovery project. In Proceedings of the\n5thInternationalConferenceonDiscoveryScience(DS’02) ,\nLbeck, Germany., 2002.\n[14] G. Widmer. Machine discoveries: A few simple, robust lo-\ncal expression principles. Journal of New Music Research ,\n31(1):37–50, 2002."
    },
    {
        "title": "Feature-Based Synthesis: A Tool for Evaluating, Designing, and Interacting with Music IR Systems.",
        "author": [
            "Matthew D. Hoffman",
            "Perry R. Cook"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1417515",
        "url": "https://doi.org/10.5281/zenodo.1417515",
        "ee": "https://zenodo.org/records/1417515/files/HoffmanC06.pdf",
        "abstract": "We present a general framework for performing feature- based synthesis – that is, for producing audio characterized by arbitrarily specified sets of perceptually motivated, quantifiable acoustic features of the sort used in many music information retrieval systems.",
        "zenodo_id": 1417515,
        "dblp_key": "conf/ismir/HoffmanC06",
        "keywords": [
            "general",
            "framework",
            "feature-based",
            "synthesis",
            "audio",
            "acoustic",
            "features",
            "music",
            "information",
            "retrieval"
        ],
        "content": "Feature-Based Synthesis: A Tool for Evaluating, Designing, and Interactingwith Music IR SystemsMatt HoffmanPrinceton UniversityComputer Science Department35 Olden StreetPrinceton, NJ 08544mdhoffma@cs.princeton.eduPerry R. CookPrinceton UniversityComputer Science & Music Departments35 Olden StreetPrinceton, NJ 08544prc@cs.princeton.eduAbstractWe present a general framework for performing feature-based synthesis – that is, for producing audio characterizedby arbitrarily specified sets of perceptually motivated,quantifiable acoustic features of the sort used in manymusic information retrieval systems.1. IntroductionWe have implemented a general framework for performingfeature-based synthesis, which attempts to synthesize,given any set of feature values, audio that matches thosefeature values as closely as possible. Depending on howone chooses the values to synthesize, feature-basedsynthesis can be used to evaluate the usefulness of a givenset of features for a particular audio IR domain, todiagnose why a system is not performing as well asexpected, as a tool for gaining insight into whatinformation a set of features is encoding, and to generatestimuli for use in studies of human perception.We frame the problem in terms of minimizing thedistance between a target feature vector and the featurevector describing the synthesized sound over the set ofunderlying synthesis parameters. The mapping betweenfeature space and parameter space can be highly nonlinear,complicating optimization. Our framework separates thetasks of feature extraction, feature comparison, soundsynthesis, and parameter optimization, making it possibleto combine various techniques in the search for an efficientand accurate solution to the problem of synthesizingsounds manifesting arbitrary perceptual features.2. Motivation2.1 Feature Evaluation and SelectionFeature-based synthesis can be used to address thisquestion of what relevant qualities, if they were encodedby one’s feature set, might enable better performance on aproblem. As Lidy, Pölzlbauer, and Rauber [1] observe, oneway of qualitatively evaluating the meaningfulness of afeature set is through an analysis-by-synthesis processwhere one extracts the features in question from multiplesounds from the target domain, synthesizes new soundsmatching the extracted features, and compares the originaland resynthesized versions. If the resynthesized version ofa sound file lacks some quality relevant to the problem athand, then it is likely that adding a feature representingthat quality to the feature set will improve performance.2.2 Feature ExplorationOur system provides an interface for synthesizing audiomanifesting feature values specified in real-time, which canbe used to gain a more intuitive understanding of how thevarious features one is using map to actual sounds.Attempting to generate sounds with specific perceptualcharacteristics in this way can stimulate insights into howmuch descriptive power a feature set has.2.3 Perceptual Study Stimulus GenerationStudies such as [2] [3] [4] have investigated the humanability to perceive various physical attributes of soundsources. We suggest that feature-based synthesis could beof use in studying the low-level acoustical properties thathuman listeners use to deduce the more complex physicalattributes of a sound’s source. We can generate soundsdefined over a set of features we expect to correlate withlisteners’ perceptions of, e.g., size, material, or shape, andthen use techniques like those described in [5] to determinehow those sounds map to the ecological features we wishto study. From the data points obtained in this way, wemay be able to discover consistent relationships betweenacoustical and human-generated features that can be used topredict how a sound manifesting certain acoustic featurevalues will be perceived.2.4 Classification System EvaluationWe can also treat the confidence outputs of entireclassification systems as features to match, enabling us togain insights into what sorts of audio a system stronglybelieves fit into one category or another, as well as whatsorts of audio it finds difficult to classify.Permission to make digital or hard copies of all or part of this work forpersonal or classroom use is granted without fee provided that copiesare not made or distributed for profit or commercial advantage andthat copies bear this notice and the full citation on the first page.© 2006 University of Victoria3. Related WorkOur system’s approach of synthesizing audio to fitquantifiable targets is not entirely without precedent. Forexample, a presentation at the 2004 ISMIR graduate school[6] mirrors many of the ideas behind our system. Anotherapproach seeks synthesis parameter values that willproduce a sound closely matching the spectrum of anexisting sound (e.g. [7, 8, 9]). Some work has been doneon synthesizing audio manifesting a limited number ofspecific feature values (e.g. [1]). Finally, relatively recentwork in concatenative synthesis and audio mosaicing,although sometimes working to somewhat different ends,faces some of the same challenges as feature-basedsynthesis. An overview of current work in concatenativesynthesis and audio mosaicing can be found in [10].4. Implementation\nFigure 1. Overview of the architecture of the framework.Given a target feature vector v, P searches through theparameter space of S to find a set of synthesis parameters u’that will minimize D(v, F(S(u’))).Our architecture focuses on four main modularcomponents: feature evaluators, parametric synthesizers,distance metrics, and parameter optimizers. Featureevaluators take a frame of audio as input and output an n-dimensional vector of real-valued features. Parametricsynthesizers take an m-dimensional vector of real-valuedinputs and output a frame of audio. Distance metrics definesome arbitrarily complex function that compares how“similar” two n-dimensional feature vectors are. Finally,parameter optimizers take as input a feature evaluator F, aparametric synthesizer S, a distance metric D, and an n-dimensional feature vector v generated by F (which D cancompare to another such feature vector). The parameteroptimizer P outputs a new m-dimensional synthesisparameter vector u’, a new n-dimensional feature vector v’,and a frame of audio representing the output of S whengiven u’. This frame of audio produces v’ when given asinput to F. v’ represents the feature vector as close to v(where distance is defined by D) as P was able to find inthe parameter space of S.These four components together make up a completesystem for synthesizing frames of audio characterized byarbitrary feature vectors. Any implementation of one ofthese components is valid, so long as it adheres to theappropriate interface.5. Future WorkThe next phase of the project will involve implementingmore feature evaluators, synthesizers, optimizers, anddistance metrics, and evaluating the system’s performancemore rigorously in a variety of domains. Additionally, wewill extend the framework to more directly handle time-domain features and interpolate between parameterssmoothly to produce smoother, more natural output.References[1] T. Lidy, G. Pölzlbauer, and A. Rauber, “Sound re-synthesis from rhythm pattern features – audible insightinto a music feature extraction process,” in Proceedingsof the International Computer Music Conference 2005,pp. 93-96.[2] S. Lakatos, P. Cook, and G. Scavone, “Selective attentionto the parameters of a physically informed sonic model,”Acoustics Research Letters Online, Acoustical Societyof America, March 2000.[3] P. Cook and S. Lakatos, “Using DSP-based parametricsynthesis models to study human perception,” inProceedings of the IEEE Workshop on Applications ofSignal Processing to Audio and Acoustics, 2003.[4] D. Rocchesso, “Acoustic cues for 3-D shapeinformation,” in Proceedings of the 2001 InternationalConference on Auditory Display, 2001.[5] Scavone, G., Lakatos, S., Cook, P., and Harbke, C.,“Perceptual spaces for sound effects obtained with aninteractive similarity rating program,” in Proceedings ofthe International Symposium on Musical Acoustics,2001[6] S. Le Groux, “Extraction of Relevant Controllers for the‘Analysis by Synthesis’ of Musical Sounds,” researchproposal presentation given at ISMIR 2004 Fifth Int.Conf. On Music Inf. Retr., graduate school, available athttp://www.iua.upf.es/mtg/ismir2004/graduateschool/[7] A. Horner, J. Beauchamp, and L. Haken, “MachineTongues XVI: Genetic algorithms and their applicationto FM matching synthesis,” Computer Music Journal17(3), pp. 17-29, 1993.[8] A. Horner, N. Cheung, and J. Beauchamp, “Geneticalgorithm optimization of additive synthesis envelopebreakpoints and group synthesis parameters,” inProceedings of the International Computer MusicConference 1995, pp. 215-222.[9] S. Wun, A. Horner, and L. Ayers, “A comparison betweenlocal search and genetic algorithm methods forwavetable matching,” in Proceedings of theInternational Computer Music Conference, 2004, pp.386-389.[10] D. Schwarz, “Current Research in Concatenative SoundSynthesis,” in Proceedings of the InternationalComputer Music Conference, 2005, pp. 802-805.05, pp.802-805."
    },
    {
        "title": "Exploiting Recommended Usage Metadata: Exploratory Analyses.",
        "author": [
            "Xiao Hu 0001",
            "J. Stephen Downie",
            "Andreas F. Ehmann"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1415998",
        "url": "https://doi.org/10.5281/zenodo.1415998",
        "ee": "https://zenodo.org/records/1415998/files/HuDE06.pdf",
        "abstract": "In this paper, we conduct a series of exploratory analyses on the user-recommended usages of music as generated by 1,042 reviewers who have posted to www.epinions.com. Using hierarchical clustering methods on data derived from the co-occurrence analyses of usage and genre, usage and artist, and usage and album, we are able to conclude that further investigation of user-recommended usage metadata is warranted, especially with regard to its implications for future iterations of the Music Information Retrieval Evaluation eXchange (MIREX). Keywords: user-recommended usage, music reviews, hierarchical clustering, MIREX,  HUMIRS, user study",
        "zenodo_id": 1415998,
        "dblp_key": "conf/ismir/HuDE06",
        "keywords": [
            "user-recommended usage",
            "music reviews",
            "hierarchical clustering",
            "MIREX",
            "HUMIRS"
        ],
        "content": "Exploiting Recommended Usage Metadata: Exploratory Analyses \nXiao Hu \nGraduate School of Library and \nInformation Science \nUniversity of Illinois at Urbana-\nChampaign  \nxiaohu@uiuc.edu J. Stephen Downie \nGraduate School of Library and \nInformation Science \nUniversity of Illinois at Urbana-\nChampaign \njdownie@uiuc.edu Andreas F. Ehmann \nDepartment of Electrical and  \nComputer Engineering \nUniversity of Illinois at Urbana-\nChampaign \naehmann@uiuc.edu \n \nAbstract \nIn this paper, we conduct a se ries of exploratory analyses \non the user-recommended usages of music as generated by \n1,042 reviewers who have posted to www.epinions.com .  \nUsing hierarchical clustering methods on data derived \nfrom the co-occurrence analyses of usage and genre, usage \nand artist, and usage and album, we are able to conclude \nthat further investigation of user-recommended usage \nmetadata is warranted, especially with regard to its \nimplications for future iterations of the Music Information \nRetrieval Evaluation eXchange (MIREX). \nKeywords : user-recommended usage, music reviews, \nhierarchical clustering, MI REX,  HUMIRS, user study \n1. Introduction \nQuestions involving how music information is used have \ndrawn growing attention in the MIR community. Studies \nhave explored music use behaviors from different angles \nand by various approaches. For example, Bainbridge et al. \n[1] employed qualitative ethnographic methods to examine \nmusic seeking behaviors and goals as presented on the \nGoogle Answers website. Lee and Downie [2] applied \nsurvey methods to develop a set of descriptive statistics on \ninformation needs, uses and seeking behaviors amongst a \ngroup of 427 users. Data mining and natural language \nprocessing (NLP) techniques have also been applied on a \nvariety of music related textual data sources to analyze \nhow users organize and access music information. For \nexample, Whitman and Lawrence [3], Baumann and \nHummel [4], and Knees et al. [5] retrieved top ranked web \npages related to a list of artists from search engines, \napplied text processing techniques to build vector space \nmodels representing the artists  and then calculated artist \nsimilarities based on similarities of the vectors. Pachet et \nal. [6] used playlists from radio stations and compilation \nCD databases to conduct cluster analyses on music titles \nand artists. User-generated playlists have also been used to \ncompute artist similarities which were subsequently validated against ground truth taken from the expert-\nderived artist lists compiled by the editors of \nwww.allmusic.com  [3]. \nOnline music reviews, such as those found on \nwww.epinions.com , because they are written by music \nusers themselves, provide both users’ opinions and an \nexcellent source for ground truth data with regard to such \nthings as genre labels, artist na mes, quality ratings, etc. For \nexample, Hu et al. [7] developed a set of Naïve Bayesian \nmodels that could successfully predict both the user-\nassigned genre labels (78.9%) and the user-generated \nquality ratings (81.3%) based upon the examination of \n2,160 epinions.com review texts. More recently Downie \nand Hu [8] undertook a frequent pattern analysis of the \nepinions.com review data and discovered a set of three-\nterm descriptive patterns that were helpful in \ndifferentiating positive and negative reviews.  \nIn this paper, we extend the previous work on the \nanalysis of user-generated mu sic reviews but now shift the \nanalytic focus to explore th e user-generated recommended \nusages  of the music being reviewed. In each of the reviews \npresented on epinions.com, there is a field called “Great \nMusic to Play While” where the reviewer provides a usage \nsuggestion for the reviewed piece. Each review can be \nassociated with at most one recommended usage. \nEpinions.com provides a ready-made list of 13 \nrecommended usages prepared by the editors. By \nanalyzing the application of these recommended usage \nlabels using a range of statistical analyses, we aim to \nanswer the following questions:  \n1. What are the relationships between usages and music \ngenres?  \n2. What are the relationships between usages and music \nartists?  \n3. How are the usages related to each other based on \ntheir co-occurrences with genre, artist, and album \ntitles? Can the usage classes be meaningfully \ngrouped into broader categories (i.e., superclasses)?   \n1.1 Motivation \nThe present exploratory study is being conducted under the \nauspices of the Human Use of Music Information Retrieval \nSystems (HUMIRS) project. The goal of the HUMIRS \nproject is the creation of standardized evaluation tasks and \n“query” documents, grounded in real-world user behaviors, Permission to make digital or hard copi es of all or part of this work for \npersonal or classroom use is grante d without fee provided that copies \nare not made or distributed for profit  or commercial advantage and that \ncopies bear this notice and the full citation on the first page. \n© 2006 University of Victoria for use in the Music Information Retrieval Evaluation \neXchange (MIREX). The overarching goal of this study is \nthe determination of whether (or not) user-generated \n“usage” classes warrant further examination as \ncontributory elements in: (a) the definition of future \nMIREX contests; and/or, (b) the basic structure of the \nMIREX evaluation “query” documents.  \n2. Data Set \nTable 1 shows the recommended usage categories applied \nto 5,772 review texts randomly downloaded from \nepionions.com in April 2005 along with the number of \nreviews suggesting a particular usage. Our analyses in this \npaper focus on the first 11 categories and ignore the last 2 \ncategories where the data were  too sparse for meaningful \nanalyses (i.e., “With Family” and “Sleeping”). 180 reviews \nfrom each of the 11 usage catego ries were selected to form \na data set of 1,980 reviews. Such even distribution across \nusage categories was made to eliminate possible analytic \nbiases. \nTable 1.  Suggested usage  categories and counts \nUsage Count Usage Count\nDriving 1,349 Waking Up 271\nHanging With Friends 1,215 Going to Sleep 269\nListening 592 Cleaning the House 230\nRomancing 492 At Work 188\nReading or Studying 447 With Family 35\nGetting Ready to Go Out 378 Sleeping 15\nExercising 291 TOTAL 5,772\n3. Co-occurrence Analyses \nThe review dataset we are studying contains information \nabout the genre of the album, the artist who performs the \nmusic, the album title under review and the usage \nsuggested by the reviewer. In this section we examine the \nrelationship between (a) music genre and usage; (b) artist \nand usage; and, (c) album and usage. \n3.1 Genre and Usage  \n3.1.1  Dependency Analysis \nThere are 12 genres covered by the 1,980 reviews in the \ndataset used in our experiments. These were the same 12 \ngenres we examined in our previous work [7]. To \ndetermine the relationships between different genres and \ndifferent usages, a vector, consisting of 23 binary variables \n(representing the 12 genre and 11 usage classes), was \nconstructed for each review. These 1,980 vectors formed \nthe input for a Pearson’s chi-square dependency test [9] for \nall possible pairs of genres and usages. Table 2 reports the \nχ2 and p values of only those tests that indicated strong \ndependencies between the corresponding genres and \nusages (statistically significant at the p < 0.01 level). Some \nof the results validate “common sense” about possible \ngenre and usage relationships, such as “Jazz Instrument” \nmusic being significantly related to “Romancing.” Some others, however, are interesting findings that might not \nhave been well known beforehand, such as “Electronic” \nmusic being recommended for “Going to Sleep,” and \n“Country” music for “C leaning the House.” \nTable 2.  Key dependencies between genres and usages \nGenre Usage Pearson’s χ2 p value\nClassical  Listening  37.61  < 0.001\nCountry Cleaning the House 70.78 < 0.001\nElectronic Going to Sleep 29.13 < 0.001\nHard Core Punk Waking Up 12.54 < 0.001\nJazz Instrument Romancing 123.45 < 0.001\nPop Vocal Romancing 49.88 < 0.001\n3.1.2 Usage Clustering on Genre-Usage Co-occurrences \nA thoughtful examination of the usages listed in Table 1 \nsuggests that some of the usages are semantically related. \nFor example, “Exercising” and “Cleaning the House” both \ndenote some kind of physical activity. While it is tempting \nto have the authors themselves manually group the \ncategories into superclasses, this approach is obviously not \npreferable because the interpretation of the semantics of \nthe category labels can be subj ective. To overcome author-\nbased biases, we instead chose to take advantage of the \nassociations between music genre and usage to see if it \ncould assist us in generating meaningful superclasses upon \nwhich to base further explorations. Therefore, we clustered \nusages based on their co-occurrences with genres. A co-\noccurrence matrix was formed such that each cell of the \nmatrix was the number of reviews with the genre and \nusage specified by the coordinates of the cell. This co-\noccurrence matrix then underwen t a hierarchical clustering \nprocedure using Euclidean di stance and the complete link \nalgorithm  [10]. Figure 1 shows the clustering results.  \nWe note here the grouping of the “Read/Study” and “Go \nto Sleep” usages that suggests a kind of Passive  or \nRelaxing  engagement with the music. This contrasts well \nwith the more Active  or Stimulating  interactions evoked by \nthe grouping of the “Ready to  Go” and “Exercise” usages. \n \nFigure 1. Usage clusters from genre-usage co-occurrences  3.2 Artist and Usage \n3.2.1 Dependency Analysis \nThere are 897 unique artists in the dataset of 1,980 \nreviews. It is interesting to see how specific artists tend to \nbe recommended for certain usage occasions. We explored \nthe data to discover the strongest relationships between \nspecific usage classes and specific  artists. In order to verify \nthat our observations were not purely random, a binomial \nexact test [11] for each of the artists and usages was \nconducted on those ar tists that were represented by at least \n10 reviews. Table 3 presents the artist usage pairs whose \nrelations tested to be significant at p < 0.05. For example, \nBlack Sabbath’s music is r ecommended most often for use \n“At Work.” Similarly, a signif icant plurality of reviewers \nrecommended listening to Nirvana when “Going to Sleep.” \nTable 3.  Significant relati ons between artists and usages \nArtist Usage p value \nAFI Waking Up 0.034\nBlack Sabbath At Work 0.000\nCeline Dion Romancing 0.025\nDream Theater Listening 0.019\nMetallica Waking Up 0.033\nNirvana Going to Sleep 0.019\nWe suggest that artist-usage relationships warrant \nfurther examination on two fronts. First, work needs to be \ndone to see if a case could be made that the music \nproduced by these artists, in some way “exemplifies” or \nrepresents the “archetype” of the associated usage classes. \nFor example, constructing an audio-based feature set of \n“Metallica” and “AFI” pieces for exploratory similarity \nexperiments based on the identification of other “Waking \nUp” (or more generally, Active/Stimulating ) music could \nprove fruitful. Second, notice in Figure 2 how each artist \nappears to have a unique recommended usage profile (i.e., \nvarious proportions of recommend usages). If one were \nable to garner enough usage data for each artist to \nconstruct his/her own usage profile, such data could \ncontribute significantly to th e building of artist-to-artist \nsimilarity analyses and recommender systems. \n Figure 2. Usage distributions  of 10 most-reviewed artists 3.2.2 Usage Clustering on Artist-Usage Co-occurrences \nThe facets of artist and usage can also form a co-\noccurrence matrix. Using the same clustering techniques \ndiscussed in 3.1.2, usage clusters were created from the \nartist-usage co-occurre nces (Figure 3).  \n  \nFigure 3. Usage clusters from artist-usage co-occurrences \nAgain we note in Figure 3 that the close proximity of \nthe “Go to Sleep” and “Read/Study” usages suggests a \nPassive/Relaxing  grouping. Similarly, the close proximity \nof the “Exercise” and “Waking Up” usages suggests an \nActive/Stimulating  relationship with the music. \n3.3 Album and Usage  \n3.3.1 Usage Clustering on Album-Usage Co-occurrences \nAlbum titles are another facet that can be utilized as the \nhidden variable that connects different usages. There are \n1,372 unique albums in this dataset, among which 366 \nalbums have 2 to 7 reviews. A co-occurrence matrix of \nalbums and recommended usages was constructed as \nbefore and was subjected to the aforementioned clustering \nprocedures. The resulting cluster tree is shown in Figure 4. \n \nFigure 4. Usage clusters from album-usage co-occurrences  \n4. Discussion \nComparing Figures 1, 3, and 4, it is obvious that there are \nconsistent general patterns to the clusters derived from \nthree completely different data facets. Again, we note that \n“Go to Sleep” and “Read/Study” are always grouped \ntogether (i.e., Passive/Relaxing ), while “Ready to Go,” \n“Exercise,” “Wake up,” “Drive,” “Hang out with friends,” \nand “At Work” (i.e., Active/Stimulating ) are consistently \ngrouped together at various levels of proximity. This \ngeneral consistency of the clustering methods’ results \nacross different data facets (genre, artist, and album) serves \nto reinforce the notion that the usage groups of \nActive/Stimulating  and Passive/Relaxing  are reasonable \nand that the groupings do indeed seem to suggest the \npresence of at least two usage superclasses.  \nHowever, there are still individual recommended \nusages that show discrepancies in groupings across \nexperiments. One problematic example is “Listening”: it is \ngrouped together with different usages across the various \nexperiments. This disagreement  is not surprising because \nfrom a semantic point of view, “Listening” is an \nambiguous usage, as all music activities involve listening. \nAlthough there should be a r eason for a user to pick \n“Listening” among all the given choices, it is unclear \nwhether the reason relates more to Active/Stimulating  or \nPassive/Relaxing  activities.  \nThere are several limitations to the dataset that could \nplay a role in masking the presence of other readily \ndiscernible and informative su perclasses. First, the usage \nlabels were created by the editors of epinons.com and thus \nend users did not have complete freedom to specify their \nusage recommendations but rather had to choose from the \n13 options provided. Second, some of the options are \nsemantically ambiguous such as “Listening” mentioned \nabove. Third, it is reasonable to  say that most of the usages \nmay not have been interpreted consistently by different \nusers. Fourth, and finally, the users were limited to only \none usage choice, which thus prevents us from examining \nhow individual users would have brought together related \nusage classes to form possible superclasses. \n5. Conclusions and Future Work \nIn this exploratory study we set out to determine whether \nuser-recommended usage information warranted further \nexamination in the construction of MIREX tasks and/or \n“query” documents. The consistent manifestation, under a \nrange of experimental conditions, of groupings of usage \ncategories that form the two plausible superclasses of \nPassive/Relaxing  and Active/Simulating  leads us to \nconclude that the recommended usages specified by users \nreflect a meaningful source of user-generated metadata. As \nsuch, we firmly believe that further investigation is \nwarranted so that we may better understand this particular type of user-generated metadata in order to better model  \nreal-world user behavior for MIREX. \nIn future work, further analysis on the relationships \namong genre, artist and usage is desirable, as is the \nexploration of new, large-scale, data sets. We are \nespecially interested in determining whether other (and \npossibly more informative) usage superclasses can be \nuncovered. We also want to move toward examining the \nrelationships between the recommended usages and the \naudio features of the music under review. \n6. Acknowledgments \nWe thank the Andrew W. Mellon Foundation for their \nsupport. This project is also supported by the National \nScience Foundation under Grant No. NSF IIS-0327371.  \nReferences \n[1] D. Bainbridge, S. J. Cunningh am and J. S. Downie. “How \nPeople Describe Their Music Information Needs: A Grounded Theory Analysis of Music Queries. In ISMIR \n2003 Fourth Int. Conf. on Music Inf. Retr. Proc ., 2003, pp. \n221–222. \n[2] J. Lee, and J. S. Downie. “Survey of music information \nneeds, uses, and seeking behaviours: Preliminary findings,” \nISMIR 2004 Fifth Int. Conf. on Music Inf. Retr. Proc ., \n2004, pp 441– 446. \n[3] B. Whitman, and S. Lawrence. “Inferring Descriptions and \nSimilarity for Music from Community Metadata,” In \nProceedings of the 2002 International Computer Music Conference,  2002, pp. 591–598. \n[4] S. Baumann and O. Hummel. “Using Cultural Metadata for \nArtist Recommendation,” in Proceedings of the \nWedelMusic Conference , 2003, pp. 138–141. \n[5] P. Knees, E. Pampalk, and G. Widmer. “Artist \nClassification with Web-based Data,” in ISMIR 2004 Fifth \nInt. Conf. on Music Inf. Retr. Proc ., 2004, pp. 517–524. \n[6] F. Pachet, G. Westerman, and D. Laigre. “Musical Data \nMining for Electronic Music Distribution,” In Proceedings \nof the WedelMusic Conference,  2001. \n[7] X. Hu, J. S. Downie, K. We st, and A. Ehmann. “Mining \nMusic Reviews: Promising Preliminary Results.” in ISMIR \n2005 Sixth Int. Conf. on Music Inf. Retr. Proc ., 2005, pp. \n536-539. \n[8] J. S. Downie and X. Hu. “R eview Mining for Music Digital \nLibraries: Phrase II,” In Proceedings of the Sixth Joint \nConference on Digital Libraries, 2006, pp.196-197. \n[9] A. Agresti. An Introduction to Categorical Data Analysis , \nWiley, New York 1996. \n[10] A. Jain, M. Murty and P. Flynn. “Data Clustering: A \nReview,” ACM Computing Surveys , Vol 31, No. 3, pp.264-\n323, 1999. \n[11] M. Buntinas, and G. M. Funk.  Statistics for the Sciences , \nBrooks/Cole/Duxbury, 2005."
    },
    {
        "title": "Automatic Feature Weighting in Automatic Transcription of Specified Part in Polyphonic Music.",
        "author": [
            "Katsutoshi Itoyama",
            "Tetsuro Kitahara",
            "Kazunori Komatani",
            "Tetsuya Ogata",
            "Hiroshi G. Okuno"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1417887",
        "url": "https://doi.org/10.5281/zenodo.1417887",
        "ee": "https://zenodo.org/records/1417887/files/ItoyamaKKOO06.pdf",
        "abstract": "We studied the problem of automatic music transcription (AMT) for polyphonic music. AMT is an important task for music information retrieval because AMT results enable retrieving musical pieces, high-level annotation, demixing, etc. We attempted to transcribe a part played by an instru- ment specified by users (specified part tracking). Only two timbre models are required in the specified part tracking to identify the specified musical instrument even when the number of instruments increases. This transcription is for- mulated into a time-series classification problem with mul- tiple features. We furthermore attempted to automatically estimate weights of the features, because the importance of these features varies for each musical signal. We esti- mated quasi-optimal weights of the features using a genetic algorithm for each musical signal. We tested our AMT sys- tem using trio stereo musical signals. Accuracies with our feature weighting method were 69.8% on average, whereas those without feature weighting were 66.0%. Keywords: automatic music transcription, specified part track- ing, feature weighting, genetic algorithm",
        "zenodo_id": 1417887,
        "dblp_key": "conf/ismir/ItoyamaKKOO06",
        "keywords": [
            "automatic music transcription",
            "polyphonic music",
            "music information retrieval",
            "specified part tracking",
            "timbre models",
            "instrument identification",
            "time-series classification",
            "musical signals",
            "feature weighting",
            "genetic algorithm"
        ],
        "content": "Automatic Feature Weighting in Automatic Transcription of\nSpeciﬁed Part in Polyphonic Music\nKatsutoshi Itoyama, Tetsuro Kitahara, Kazunori Komatani, Tetsuya Ogata and Hiroshi G. Okuno\nDept. of Intelligence Science and Technology, Graduate School of Informatics, Kyoto University\nSakyo-ku, Kyoto 606-8501, Japan\n{itoyama, kitahara, komatani, ogata, okuno }@kuis.kyoto-u.ac.jp\nAbstract\nWe studied the problem of automatic music transcription\n(AMT) for polyphonic music. AMT is an important taskfor music information retrieval because AMT results enable\nretrieving musical pieces, high-level annotation, demixing,\netc. We attempted to transcribe a part played by an instru-ment speciﬁed by users ( speciﬁed part tracking ). Only two\ntimbre models are required in the speciﬁed part tracking\nto identify the speciﬁed musical instrument even when the\nnumber of instruments increases. This transcription is for-\nmulated into a time-series classiﬁcation problem with mul-tiple features. We furthermore attempted to automatically\nestimate weights of the features, because the importance\nof these features varies for each musical signal. We esti-mated quasi-optimal weights of the features using a genetic\nalgorithm for each musical signal. We tested our AMT sys-\ntem using trio stereo musical signals. Accuracies with our\nfeature weighting method were 69.8% on average, whereas\nthose without feature weighting were 66.0%.\nKeywords: automatic music transcription, speciﬁed part track-\ning, feature weighting, genetic algorithm\n1. Introduction\nRecently, because of the growth of the digital music indus-try, demand for music information retrieval (MIR) and man-\nagement of musical data has been increasing. Automatic\nmusic transcription (AMT) is needed to improve MIR be-cause musical scores enable MIR by melody or musical in-\nstrument, etc. AMT for polyphonic music generally con-\nsists of two successive processes: note formation , which es-\ntimates the onset time and pitch of each note, and stream\nformation , which classiﬁes the formed notes by their in-\nstruments (parts). The latter problem has not been studied\nenough, which made the AMT incomplete. Therefore, a\nmethod to form streams is strongly required to realize anAMT for polyphonic music.\nPrevious studies of stream formation were classiﬁed broadly\ninto two approaches. One identiﬁes the musical instruments\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies\nare not made or distributed for proﬁt or commercial advantage and that\ncopies bear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2006 University of Victoria\nMusical Signal\nStream Formation\nNotes of\nSpecified InstrumentNote Formation\nFeature Extraction\nSpecified Part\nTrackingCalculation of\nFitness FunctionIteration\nwith GA\nAutomatic Feature Weighting\nFigure 1. Overview of Speciﬁed Part Tracking\nof all parts and labels all the instruments given [1, 2, 3]. In\nthis approach, training data for all instruments which couldbe contained in musical pieces is required to separate all in-\nstruments exactly. Another forms streams without informa-\ntion about musical instruments contained in musical pieces.This approach does not require training data [4]. However,\nusers cannot extract streams they wanted because the ob-\ntained streams have no label of the target instrument.\nWe developed a new approach in which the AMT system\nis given one of the musical instruments included in the mu-\nsical pieces and transcribes that part of the musical pieces.\nBy focusing on only one musical instrument a user speciﬁed,\nwe require only two timbre models to identify the speciﬁed\nmusical instrument. Speciﬁed part tracking is the stream\nformation based on our approach.\nWe also developed a method for automatically estimat-\ning weights of features which are used in the speciﬁed part\ntracking. The importance of these features depends on musi-cal signals. For example, directional reliability by alignment\nof instruments, distortion of timbre features by noises. We\ndevelop a method for estimating quasi-optimal weights foreach musical signal using a genetic algorithm.\n2. Problem Speciﬁcation\nSpeciﬁed part tracking classiﬁes musical notes into the setof notes of a speciﬁed instrument Nand that of other in-\nstruments ¯N. We deﬁned a pair of them H=(N,¯N)as a\nhypothesis of the speciﬁed part tracking. The speciﬁed part\ntracking is performed as follows:1. Generate two initial hypotheses ({n1},φ),(φ,{n1})\nfor the ﬁrst note n1.\n2. Expand each hypothesis H=(N,¯N)on notes n1···nk\ninto two new hypotheses H0=(N∪{nk+1},¯N)and\nH1=(N,¯N∪{nk+1}), and calculate the reliability\n(corresponding likelihood) of each hypothesis.\n3. If the number of hypotheses exceeds a constant K,\ndelete all hypotheses except those reliabilities are in\nthe top K.\n4. Iterate 2 and 3 for all notes.\n5. After expanding hypotheses and calculating their reli-\nability through a note list, output a hypothesis which\nhas the maximum reliability as the result of the spec-\niﬁed part tracking.\n3. Implementation\nWe implemented the speciﬁed part tracking with four fea-tures. The features were classiﬁed into two: we used “Tim-\nbre Similarities to the Model” to evaluate the similarity be-tween note nand the speciﬁed instrument; “Timbre Simi-\nlarities to the Speciﬁed Part,” “Proximity of Localization,”\nand “Pitch Transition Frequency” to evaluate the similaritybetween nand the speciﬁed part. The latter features were\ndesigned based on Sakuraba et al. [4].\nTimbre Similarity to the Model ( f\nI)This feature represents\nthe timbre similarity between a note nand the spec-\niﬁed instrument. The timbre of note nis described\nby vector x(n)proposed by Kitahara et al. [5]. The\ndistance of nto the model of the speciﬁed instrument\nMrepresents the similarity between nand the instru-\nment, but features extracted from mixed sounds are\nfrequently distorted. We used global model G, which\ndoes not depend on any musical instruments, and weused the distance of ntoM(d(n,M)) divided by the\ndistance ntoG(d(n,G)) to evaluate the similarity.\nf\nI(n)is described as the statistical probability calcu-\nlated by an F-test:\nfI(n)=/integraldisplay∞\ndI(n)ξm/2−1\nB(m/2,m /2)(ξ+1 )mdξ,\nwhere dI(n)=d(n,M)/d(n,G),m=d i m ( x(n)),\nandB(m1,m2)is the Beta function.\nTimbre Similarity to the Speciﬁed Part ( fS)This feature\nrepresents the timbre similarity of a note nand the\npartN. The timbre features are described as the same\nas above. We used the distance nto the distribution\nof the timbre features of ˜n∈N(dS(n)) to evaluate\nthe similarity. fS(n)is calculated by a χ2-test:\nfS(n)=/integraldisplay∞\ndS(n)ξm/2−1e−ξ/2\n2m/2Γ(m/2)dξ,\nwhere Γ(m)is the Gamma function.Localizational Proximity ( fL)This feature represents the\nlocalizational proximity of ntoN. The localization\nof the note is the mode value of the interaural phasedifference (IPD) of every frame. We used the distance\nofnto the distribution of the localization of ˜n∈N\n(d\nL(n)) to evaluate the proximity. fL(n)is calculated\nby aχ2-test:\nfL(n)=/integraldisplay∞\ndL(n)1\n√\n2πξ−1/2e−x/2dξ.\nPitch Transition Frequency ( fT)This feature represents the\nfrequency of pitch transitions. This is the trigramprobability that nfollows N. We used the model of\nthe pitch transition as a trigram model in which the\npitch occurrence probability depends on the pitch ofthe adjacent two notes ( pitch(n\nc−1)andpitch(nc)).\nfT(n)is described as a posterior probability under N:\nfT(n)=p(pitch(n)|pitch(nc−1),pit c h (nc)).\nWe used two different timbre features fIandfS. There\nis the risk that the timbre features of each musical piece andtraining data and then the reliability of f\nIbecomes lower.\nEven if they are not similar, the reliability of fSkeeps up\nbecause fScompares the timbre features between the musi-\ncal notes in the same musical piece.\nThe reliability of a hypothesis f(H)is calculated based\non multiple features as:\nf(H)=fI(H)×⎛\n⎝/summationdisplay\ni∈S,L,Twifi(H)⎞\n⎠,\nfi(H)=/summationdisplay\nn∈Nfi(n)−/summationdisplay\nn∈¯Nfi(n).\nfIis not given the weight and previleged, because our aim\nis tracking the part that the user speciﬁed and fIis the only\nfeature that evaluates the timbre similarity to the instrument\nthat the user speciﬁed.\n4. Automatic Weighting of Multiple Features\nAfter evaluation of hypotheses, optimal weights of featuresdiffer depending on the recording conditions of acoustic sig-\nnals, etc. Therefore, these weights must be automatically\nestimated from acoustic signals. To do this, we designed a\nﬁtness of the speciﬁed part tracking. Optimal weights can\nbe estimated by searching for weights that maximize thisﬁtness. We deﬁned the two following conditions to estimate\nthe ﬁtness of H=(N,¯N)and designed quantitative mea-\nsures.\n1. The number of notes derived from the speciﬁed in-\nstrument in Nis greater than in ¯N. We designed\nthe difference between Nand¯Nof the feature on the\nsimilarities of timbre to the model as:\nE[f\nI(N)]−E[fI(¯N)].2. The majority of notes included in Nare derived from\nthe same sound source. We designed the summation\nof the ratio of within-class variance to between-classvariance as:\n/summationdisplay\ni∈I,S,L,T/parenleftbig\nE[fi(N)]−E[fi(¯N)]/parenrightbig2\nVar[fi(N)] +Var[fi(¯N)].\nWe deﬁned the product of these two values as the ﬁtness of\nthe speciﬁed part tracking. We used a genetic algorithm to\nsearch for quasi-optimal weights because theoretical calcu-lation of the weights that maximize the ﬁtness is difﬁcult.\nThe procedure of automatic weight estimation is as follows:\n1. Generate initial genes randomly.\n2. Track a speciﬁed part with the weights of each gene.3. Calculate the ﬁtness of each gene from the results of\nthe speciﬁed part tracking.\n4. Select genes by elite and roulette wheel selection.5. Crossover between two randomly selected parents and\ngenerate a new gene which has a weight that is themean of the weights of parents.\n6. Mutate randomly selected genes into randomly calcu-\nlated weights.\n7. Output the weights of the gene with the highest ﬁtness\nwhen above steps repeated Ltimes ( Lis a constant.)\n5. Experiment\nWe conducted three experiments on AMT for polyphonic\nmusic to show the effectiveness of our method:\n1. We evaluated the effectiveness of automatic feature\nweighting. We used a trio musical signal including\nviolin, ﬂute and piano and tracked each instrument\npart.\n2. We evaluated the robustness of the speciﬁed part track-\ning with automatic feature weighting to errors derived\nfrom automatic note formation. We used HTC [6] as\na baseline method of note formation.\n3. We evaluated whether automatic feature weighting can\nestimate appropriate weights: the estimated weights\nreﬂect the importance of the features. We tested the\nspeciﬁed part tracking and automatic feature weight-ing, to see whether the weight for proximity of local-\nization decreases according to the reliability of local-\nization. We created the musical pieces with severalreliabilities of localization by adding following devi-\nation to the localization of each note:\n50×X×(Variance Rate of Localization) ,\nwhere Xis a random variable derived from N(0,1).We evaluated the accuracy Fusing the F-measure, which is\ndeﬁned as\nP=# of notes which are correctly tracked\n# of notes the system outputs,\nR=# of notes which are correctly tracked\n# of notes which is on the scoreand,\nF=2×P×R\nP+R.\nIn experiment 2, correctly tracked notes mean that the notes\nhave correct pitch and their onset time deviation is at most\n10ms. We compared three feature weightings: even weights;\nweights estimated by our method; weights estimated by ourmethod using the accuracy as the ﬁtness (upper limit).\n5.1. Data for Experiments\nThe polyphonic musical signal we used was “Auld lang syne,”\nplayed for about 1 minute, which included 242 notes. This\nmusical signal was generated by mixing audio data takenfrom RWC-MDB-I-2001 [7] according to a standard MIDI\nﬁle (SMF) on a computer. To create the timbre model and\nthe global model, we used mixed sound templates [5]. Weused duo and trio musical pieces for mixed sound templates\nwhich were generated according to the SMFs from RWC-\nMDB-C-2001 (Piece Nos. 13, 16 and 17) [8]. We also used\nSMFs from RWC-MDB-C-2001 (Piece Nos. 1–50) to create\nthe trigram model of pitch transition.\n5.2. Experimental Results\nThe results of experiments 1 and 2 are listed in Tables 1 and\n2, respectively. Using automatic feature weighting, we im-\nproved the accuracies from 66.0% to 69.8% in experiment\n1 on average. This shows that the introduction of weightsavoided incorrect part tracking (e.g., tracking the violin part\neven though the ﬂute part was speciﬁed). We improved the\naccuracies from 44.5% to 55.3% in experiment 2 on average.This shows the robustness of our feature weighting method\nto errors derived from automatic note formation. The results\nof experiment 3 are listed in Table 3. This shows that the\nmore a musical signal has variance of localization, the more\nthe weight w\nLdecreases (i.e., appropriate weights were es-\ntimated according to the importance of features). The accu-\nracies were also improved by feature weighting.\nIn experiment 1, the accuracy of the piano part decreased\nfrom 98.3% to 93.1%. This shows our feature weighting\nmethod cannot always estimate better weights than even weights.\nHowever, the results also show the number of false alarms\ndecreased by feature weighting. This means the estimated\nweights can reject the notes of other part and noises derivedfrom note formation.\nIt was notable that the accuracies of the ﬂute part in ex-\nperiment 1 and 2 were reversal toward the accuracies ofthe violin and piano parts. We assumed this as follows.\nThe timbre features of the ﬂute notes were often distorted\nin polyphonic music, because the power of the ﬂute notesTable 1. Results of Experiment 1\nTracking\n Fwith Weights Upper\nPart\n Even Estimated Limit of F\nVn\n 85.7% 91.9% 92.5%\nFl\n 14.2% 24.6% 24.6%\nPf\n 98.3% 93.1% 99.1%\ntotal\n 66.0% 69.8% 72.1%\nTable 2. Results of Experiment 2\nTracking\n Fwith Weights Upper\nPart\n Even Estimated Limit of F\nVn\n 30.6% 48.7% 50.0%\nFl\n 36.9% 40.7% 43.2%\nPf\n 66.0% 76.6% 77.6%\ntotal\n 44.5% 55.3% 56.9%\nTable 3. Results of Experiment 3\nVariance Rate\n Estimated Weights\n Fwith Weights\nof Localization\n wSwLwT\n Even Est.\n0\n 0.40 0.55 0.05\n 92.6% 97.4%\n1\n 0.59 0.20 0.21\n 85.7% 91.9%\n2\n 0.28 0.12 0.60\n 74.4% 81.5%\nat their onset time is smaller than the power of other in-struments. However, the timbre features of the ﬂute notes\nwere hardly distorted if its onset time varies slightly be-\ncause the ﬂute notes have gradual power envelope at on-set time. The note formation detects a strong attack as the\nonset time, and the onset time of ﬂute notes was estimated\nslightly late. The distortion of the timbre features of the ﬂutenotes caused by the onset time deviation was smaller than by\nmixed sounds. Therefore, the transcription was more correct\nwith automatic note formation.\n6. Conclusion\nWe developed the speciﬁed part tracking and automatic fea-ture weighting, and showed that our method can estimate\nbetter weights than even weights in many cases. We also\nconﬁrmed the robustness to the error derived from automaticnote formation. We need to improve our feature weight-\ning to bring the estimated weights close to optimal weights,\nspeciﬁcally by investigating the ﬁtness in the GA.\nWe did not refer to conventional methods of note for-\nmation. Since accuracies of note formation were different\namong the parts, the results of experiment 2 were affected\nby note formation. Many studies have been done on note\nformation, and we need to examine several note formationmethods. We are also planning to evaluate more complex\nmusical pieces (e.g., including drums and commercial CD\nmusic).\nWe designed four features for the speciﬁed part track-\ning. Speciﬁcally, we used two different features about tim-\nbre similarity because humans can often distinguish instru-ment sounds by previous contents of musical pieces if they\nhave not listened to the instruments. In addition, we used\nonly two timbre models to identify musical instruments: themodel of the speciﬁed instrument and the global model. Al-\nthough conventional studies on musical instrument identiﬁ-\ncation have been using models of the all instruments that amusical piece contains, our method is a new approach.\nMany studies on musical instrument identiﬁcation require\nthat all instruments are known. However, this approach has\nseveral weak points: when the number of instruments in-\ncreases, new data of those instruments must be created, etc.By contrast, the speciﬁed part tracking is scalable on the\nnumber of instruments that the system needs to prepare the\ndata of instruments that users want to track. Because we didnot evaluate the number of instruments of musical pieces,\nthis is part of our future work.\n7. Acknowledgements\nThis research was partially supported by the Ministry of Ed-\nucation, Culture, Sports, Science and Technology (MEXT),\nGrant-in-Aid for Scientiﬁc Research and Informatics Re-\nsearch Center for Development of Knowledge Society In-frastructure (COE program of MEXT, Japan). This research\nused the RWC Music Database (Classic, Musical Instrument\nSound) [7, 8], and we thank everyone who contributed thisdatabase.\nReferences\n[1] K. Kashino and H. Murase, “A Sound Source Identiﬁcation\nSystem for Ensemble Music Based on Template Adaptation\nand Music Stream Extraction,” Speech Communication , vol.\n27, pp. 337–349, Mar. 1999.\n[2] T. Kinoshita, S. Sakai and H. Tanaka, “Musical Sound\nSource Identiﬁcation Based on Frequency Component Adap-tation,” Proc. IJCAI CASA Workshop , pp. 18–24, Aug. 1999.\n[3] E. Vincent and X. Rodet, “Instrument Identiﬁcation in Solo\nand Ensamble Music Using Independent Subspace Analy-\nsis,” in Proc. ISMIR , pp. 576–581, 2004.\n[4] Y . Sakuraba, T. Kitahara and H. G. Okuno, “Comparing Fea-\ntures for Forming Music Streams in Automatic Music Tran-\nscription,” in Proc. ICASSP , vol. IV , pp. 273–276, 2004.\n[5] T. Kitahara, M. Goto, K. Komatani, T. Ogata and H. G.\nOkuno, “Instrument Identiﬁcation in Polyphonic Music:Feature Weighting with Mixed Sounds, Pitch-DependentTimbre Modeling and Use of Musical Context,” in Proc. IS-\nMIR, pp. 558–563, 2005.\n[6] H. Kameoka, T. Nishimoto and S. Sagayama. “Harmonic-\nTemporal Structured Clustering via Deterministic Annealing\nEM Algorithm for Audio Feature Extraction,” in Proc. IS-\nMIR, pp. 115–122, 2005.\n[7] M. Goto, H. Hashiguchi, T. Nishimura and R. Oka, “RWC\nMusic Database: Music Genre Database and Musical Instru-\nment Sound Database,” in Proc. ISMIR , pp. 229–230, 2002.\n[8] M. Goto, H. Hashiguchi, T. Nishimura and R. Oka,\n“RWC Music Database: Popular, Classical, and Jazz Music\nDatabases,” in Proc. ISMIR , pp. 287–288, 2002."
    },
    {
        "title": "Audio Key Finding Using Low-Dimensional Spaces.",
        "author": [
            "Özgür Izmirli"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1415858",
        "url": "https://doi.org/10.5281/zenodo.1415858",
        "ee": "https://zenodo.org/records/1415858/files/Izmirli06.pdf",
        "abstract": "This paper presents two models of audio key finding: a template based correlational model and a template based model that uses a low-dimensional tonal representation. The first model uses a confidence weighted correlation to find the most probable key. The second model is distance based and employs dimensionality reduction to the tonal representation before generating a key estimate. Experiments to determine the dependence of key finding accuracy on dimensionality are presented. Results show that low dimensional representations, compared to commonly used 12 dimensions, may be utilized for key finding without sacrificing accuracy. The first model’s independently verified performance enabled it to be used as a benchmark for evaluation of the second model. Key finding accuracies for both models are given together with detailed results of the second model’s performance as a function of the number of dimensions used. Keywords: Key finding, chroma based representations, dimensionality reduction.",
        "zenodo_id": 1415858,
        "dblp_key": "conf/ismir/Izmirli06",
        "keywords": [
            "audio key finding",
            "template based correlational model",
            "template based model",
            "low-dimensional tonal representation",
            "confidence weighted correlation",
            "distance based",
            "dimensionality reduction",
            "key finding accuracy",
            "dimensionality",
            "benchmark evaluation"
        ],
        "content": "Audio Key Finding Using Low-Dimensional Spaces \nÖzgür İzmirli \nCenter for Arts and Technology \nComputer Science \nConnecticut College \nNew London, Connecticut, 06320 \noizm@conncoll.edu  \nAbstract \nThis paper presents two models of audio key finding: a \ntemplate based correlational model and a template based model that uses a low-dimensional tonal representation. The first model uses a confidence weighted correlation to find the most probable key. The second model is distance based and employs dimensionality reduction to the tonal representation before generating a key estimate. Experiments to determine the dependence of key finding accuracy on dimensionality are presented. Results show that low dimensional representations, compared to commonly used 12 dimensions, may be utilized for key \nfinding without sacrificing accuracy. The first model’s \nindependently verified performance enabled it to be used \nas a benchmark for evaluation of the second model. Key finding accuracies for both models  are given together with \ndetailed results of the second model’s performance as a function of the number of dimensions used.   \nKeywords : Key finding, chroma based representations, \ndimensionality reduction. \n1. Introduction \nAudio key finding is the problem of estimating the key of a musical piece in terms of the most stable pitch and the mode of the musical scale used. Finding the key of a piece using general polyphonic audio as input is one of the important problems in content analysis for music information retrieval. A robust solution to this problem is essential to higher levels of processing and analysis, and therefore, study of models that attempt to solve this problem is of great interest. For example, in tonal music, most high level music analyses require the determination of key as the first step. Other applications include musical style modeling, modulation detection and similarity \nmodeling by tonal evolution.     \nThis paper describes two key finding models and \npresents a comparative evaluati on of the two. Both models \nwork on recorded polyphonic audio input and produce a \nkey estimate for each file. They are designed to operate on short fragments of audio taken from the beginnings of musical works. The first model participated in the Music Information Retrieval Evaluation Exchange in 2005 (MIREX 2005). This model scored the best composite score among the 7 participating models on the unreleased \naudio data set. Therefore, this model serves as a good \nreference to evaluate the second model. The second model explores the effect of dimensionality of tonal representation in the context of the key finding problem.  \nThe remainder of the paper is organized as follows: \nSection 2 refers to related work. Section 3 describes the first model in detail and summarizes the results of the independent evaluation carried out in MIREX 2005. Section 4 describes the second model. Evaluation results for both models are given in Section 5. Section 6 concludes the paper.  \n2. Related Work \nMany audio key finding models use a chroma based \nrepresentation. A chroma based representation is a compact form of spectral representation obtained by a many-to-one mapping from the short-time spectrum of audio. The most commonly used mapping is the Pitch Class Profile (PCP) originally proposed by Fujishima \n[1] \nfor recognizing chords. Izmirli [2] compared pure spectral \nand chroma representations for key finding and reported significantly higher accuracy  with chroma based \nrepresentations weighted by pitch distribution profiles. Gomez \n[3] used a chroma based representation called the \nHarmonic Pitch Class Profile, which used the peaks in the spectrum. Cabral et al. \n[4] studied the effects of weighting \nthe contribution of FFT bins by their distance to the closest note. Pauws \n[5] used a chromagram that models chroma as \na decaying spectral impulse train and arrived at a \ncollection of chroma likelihoods in a single octave. \nModels for key finding and others that use some form \nof tonal description have utilized classification and machine learning techniques to learn from existing data and to achieve higher rates of accuracy. Purwins et al. \n[6] \nused various classifiers to classify composers using Constant Q profiles and reported on a method for finding the degree of major/minor ambiguity. G όmez and Herrera \n[7] studied machine learning methods for key finding \nusing the Harmonic Pitch Class Profile. Izmirli [8] studied \nthe dimensionality reduction of spectra for major and Permission to make digital or hard copi es of all or part of this work for \npersonal or classroom use is grante d without fee provided that copies \nare not made or distributed for profit or commercial advantage and that \ncopies bear this notice and the full citation on the first page. \n© 2006 University of Victoria  \n \nminor pitch sets. Sheh and Ellis [9] used a chroma based \nrepresentation together with Hidden Markov Models for \nfinding chord boundaries. Chai and Vercoe [10] proposed \nan HMM-based model to segment musical pieces according to points of key change.  \nIn general, regarding the solutions to the audio key \nfinding problem, many models have been reported in the literature (e.g.\n[2] [3] [5][11] [12] [13][14].) \n3. Model I \nThis section describes the key finding model that participated in MIREX 2005. The model is designed with the following assumptions: The input to the algorithm is a sound file that contains the musical work for which the key is to be estimated. The algor ithm analyzes fragments of \npolyphonic audio taken from the beginnings of musical works. It is assumed that pieces input to this algorithm \nstart in the same key as the one designated by the composer. The output consists of a single key estimate that \nis one of 24 possibilities – 12 for major and 12 for minor. \nThe model has three stages: chroma template calculation \nusing monophonic instrument sounds, chroma summary \ncalculation from the input audio file and overall key estimation using the chroma templates and chroma summary information. Every file is processed independently – there is no learning across files. \nIn the first stage, templates are formed using \nmonophonic instrument sounds spanning several octaves.  For this, initially, average sp ectra are calculated for each \nmonophonic note. Next, spectral templates are formed as a weighted sum of the aver age spectra obtained from \nindividual instrument sounds. Two types of weighting are performed. The first is done according to a pitch distribution profile. In general, the profile can be chosen to be one of Krumhansl’s probe tone ratings \n[15], \nTemperley’s profiles [16], flat diatonic profiles or \ncombinations of these. The second is a weighting that is a function of the contributing note’s (MIDI pitch) value. The first weighting is used to model the pitch class distribution and the second weighting is used to account for the registral distribution of notes. The resulting spectral templates are then collapsed into chroma templates. This process comprises a many-to-one frequency mapping for each chroma in order to form a 12-element chroma \ntemplate. As a result 24 chroma templates are formed. \nThese templates act as protot ypes of chroma vectors for \nmajor and minor keys. It should be mentioned that the 12 \nelement chroma representation is most common but other divisions are also possible.   \nIn the second stage, spectra are calculated from the \ninput audio file and then mapped to chroma vectors. A summary chroma vector is obtained by averaging the chroma vectors in a window of fixed length. Windows of different lengths are used to obtain a range of localities. All windows start from the beginning of the piece and therefore longer windows contain information in the \nshorter windows as well as the new information in the later \nparts of their span. The lengths of the windows start from a single frame and progressively increase up to a maximum time into the piece.  \nThe key is estimated in the third stage using the \nprecalculated chroma templates and the summary chroma vectors calculated from the input file. For each window, correlation coefficients ar e calculated between the \nsummary chroma vector and all chroma templates. The \ntemplate index with the highest correlation is regarded as the estimate of the key for that window. In order to find the most prevalent key estimat e for a piece, the confidence \nof the estimate for each window is also found. Next, the total confidence over all windows is calculated for each plausible key. The key with the maximum total confidence \nis reported as the overall key estimate. \n3.1 Template Calculation \nTemplates act as prototypes to which information obtained \nfrom the audio input is compared. The purpose of constructing templates is to have an ideal set of reference chroma patterns for all possible keys. This section outlines the calculation of templates and the following section describes how the input audio is processed in order to perform the key estimation.  \n3.1.1 Instrument Sounds \nIn this algorithm, templates are obtained from recordings of real instruments, but, they could equivalently be obtained from synthetically generated harmonic spectra. A collection of sound files is used. Each file contains a single note and is appropriately labelled to reflect its note content. For this algorithm, piano sounds from the University of Iowa Musical Instrument Samples online database have been used. The sounds were converted to mono from stereo and down sampled to a sampling rate of 11025 Hz. The frequency analysis is carried out using 50% overlapping 4096-point FFTs with Hann windows. The \nanalysis frequency range for this algorithm is fixed at 55 \nHz on the low end and 2000 Hz on the high end. In general, depending on the spectral content of the input a wider frequency range may be used. \n3.1.2 Pitch Distribution Profiles \nPitch distribution profiles may be used to represent tonal hierarchies in music. Krumhansl \n[15] suggested that tonal \nhierarchies for Western tonal music could be represented by probe tone profiles. Her method of key finding is based on the assumption that a pattern matching mechanism between the tonal hierarchies and the distribution of pitches in a musical piece models the way listeners arrive at a sense of key. Although she formulated this key finding method on symbolic data, many key finding models, symbolic and audio, rely on this assumption and several \nextensions have been proposed. In one such extension,  \n \nbeside other additions, Temperley [16] proposed a \nmodification to this pitch distribution profile. We utilize \nthis profile in combination with a diatonic profile as this combination results in the be st performance of the current \nmodel. The diatonic profile can be viewed as a flat profile which responds to presence or absence of pitches but is not sensitive to the relative importance of pitches. Application of this profile alone would resemble earlier approaches to key finding in which pattern matching approaches had been used. Figure 1 shows the normalized composite profile used in this model together with Temperley’s and the diatonic profiles. \nProfiles are incorporated into the calculation of \ntemplates to approximate the distribution of pitches in the spectrum and the resulting chroma representation. The base profile for a reference key (A in this case) has 12 elements, represents weights of individual chroma values and is used to model pitch distribution for that key. Given that this distribution is i nvariant under transposition, the \nprofiles for all other keys are obtained by rotating this base profile. \n0 1 2 3 4 5 6 7 8 910 1100.51\nChromaMajor Key Profile Weights\n0 1 2 3 4 5 6 7 8 910 1100.20.40.60.81\nChromaMinor Key Profile WeightsTemperley\nDiatonic\nComposite\n \nFigure 1. Pitch distribution profiles used in Model I: Diatonic \nmajor (D M), Temperley major (T M), composite major (P M) \n(top); harmonic minor (D m), Temperley minor (T m), and \ncomposite minor (P m) (bottom).  \n3.2 Chroma Templates \nThe average spectrum of an individual monophonic sound with index i, X\ni, is computed by averaging the spectra, \nobtained from windows that have significant energy, over the duration of the sound. The average spectrum is then scaled by its mean value. Here,  i=0 refers to the note A in \nthe lowest octave, i=1 refers to Bb a semitone higher etc. R \nis the total number of notes within the instrument’s pitch range used in the calculation of the templates. For this algorithm R is chosen to be 51. The lowest note is A1 and \nthe highest is B5. \nTemplates are obtained by weighted sums of the \naverage spectra calculated for individual notes. A template \nfor a certain scale type and chroma value is the sum of Xi \nweighted by the profile element for the corresponding chroma and by the second weighting that is a function of the note index ( i). A template is calculated for each scale \ntype and chroma pair resulting in a total of 24 templates as \ngiven in Equation (1). The first 12 are major, starting from \nreference chroma ‘A’, and last 12 are minor.   \n⎪⎪⎪⎪⎪\n⎩⎪⎪⎪⎪⎪\n⎨⎧\n≤ ≤⎥⎥\n⎦⎤\n⎢⎢\n⎣⎡\n+ − Ψ≤ ≤⎥⎥\n⎦⎤\n⎢⎢\n⎣⎡\n+ − Ψ\n=∑∑\n−\n=−\n=\n23 12) 12 mod ) 24 ( ( ) (11 0) 12 mod ) 12 ( ( ) (\n1\n01\n0\nn ifn i P i f Xn ifn i P i f X\nC\nR\nim iR\niM i\nn  (1) \nPe(k) is the profile weight as shown in Figure 1, where e \ndenotes the scale type (M:major or m:minor) and k denotes \nthe chroma. In this work, the profile is given by the \nelementwise product of the diatonic and Temperley profiles: P\ne(k)=D e(k)T e(k). f(i) is the secondary weighting \nfunction that accounts for the registral distribution of notes. Here, it is chosen to be a simple decreasing function: f(i)= 1-0.14 i\n0.5. Ψ is a function that maps the \nspectrum into chroma bins. The mapping is performed by dividing the analysis frequency range into 1/12th octave regions with respect to the reference A=440 Hz. Each chroma element in the template is found by a summation of the magnitudes of the FFT bins over all regions that have the same chroma value. \n3.3 Summary Calculation \nOnce the profiles are calculated they become part of the \nmodel and are used to determine the key estimates for all audio input. i.e. one set of templates is used for all audio files in a dataset. The second stage of the method involves calculation of chroma summary vectors.  \nInitially, a chroma vector is calculated for each FFT \nframe from the audio input with the same analysis parameters used for calculating the templates. Next, the actual starting point of the music is found by comparing the signal energy to a threshold. This frame is made the pivot point for the remainder of the analysis. A summary chroma vector is defined to be the average of individual chroma vectors within a window of given length. All windows start from the pivot frame with the first window \ncontaining a single frame. Window length is progressively \nincreased in succeeding windows until the maximum \nanalysis duration is reached. The maximum length of the \naudio to be analyzed is chosen to be approximately 30 seconds in this particular implementation. This results in a sequence of summary chroma vectors where each summary vector corresponds to a window of specific length. \n3.4 Estimation of Key \nThe key estimate for an input sound file is determined from the individual key estimates corresponding to the  \n \nvarious size windows and their associated confidence \nvalues. These two entities are determined as follows: For each window a key estimate is produced by computing correlation coefficients be tween the summary chroma \nvector and the 24 precalculated chroma templates and then picking the one with the ma ximum value. The confidence \nfor an individual key estimate is given by the difference between the highest and second highest correlation value divided by the highest value.  At this point each window \nhas a key estimate and an a ssociated confidence value. \nFinally, the total confidence for each plausible key is \nfound by summing confidence values over all windows. A key is plausible if it has a ppeared at least once as an \nindividual estimate in one of the windows. The key with the maximum total confidence is selected as the key \nestimate.  \n3.5 MIREX Evaluation \nMIREX 2005 provided the opportunity for empirical evaluation and comparison of algorithms in many areas related to music information retrieval. Algorithms participating in MIREX 2005 were submitted directly to the MIREX committee and the evaluations were run without intervention of the participants. The results of the MIREX 2005 evaluations were reported for all participating algorithms \n[17]. Beside other contests that \ntook place during the exchange, a closely related category \nwas symbolic key finding. The MIREX evaluation framework for audio key finding and symbolic key finding used the same dataset containing 1252 pieces. The symbolic key finding algorithms used data directly from MIDI note files whereas sound files were synthesized from the same set of MIDI files for use in audio key finding. This enabled, for the first time, a performance comparison \nof symbolic key finding and audio key finding methods. It should be stressed, however , that because the audio \nmaterial in the dataset was synthesized, the results of the audio key finding cannot be ge neralized to actual audio \nrecordings containing the same pieces. \nPrior to evaluation, a test set of 96 pieces were made \navailable to the participants for testing and calibrating their algorithms. The performance evaluation criteria were established before the actual evaluation started. According to these the performance of an algorithm was determined \nby the percentage of correctly identified keys as well as closely related keys. In order not to severely penalize closely related key estimates the following fractional allocations were used: correct key, 1 point; perfect fifth, \n0.5; relative major/minor, 0.3; parallel major/minor, 0.2 \npoints. This was determined by the proposers of the \ncontest at an early stage of the audio key finding contest \nproposal. \nThe audio dataset was reporte d to have two versions. \nDifferent synthesizers were used  to generate the different \nversions - Winamp and Timidity. A percentage score was calculated for each version of the dataset taking into \naccount the fractional allocations mentioned above. The \ncomposite percentage score was the average performance \nof the algorithms on the two datasets. \nThe algorithm explained in this paper performed as \nfollows: Using the Winamp database, 1086 pieces were \nestimated correctly. Furthermore, an additional 36 estimates were perfect fifths of the correct key, 38 were \nrelative major/minors and 17 were parallel major/minors. 75 of the estimated keys were  considered unrelated. The \npercentage score for this database was 89.4 percent. Using the Timidity database, the algorithm found the correct key for 1089 pieces. For this database, an additional 42 estimates were perfect fifths of the correct key, 31 were \nrelative major/minors and 18 were parallel major/minors. \n72 of the estimated keys were  considered unrelated. The \npercentage score for this database was 89.7 percent. The \nresulting composite percentage score was 89.55 percent. \nThis algorithm performed slightly better than the other \nalgorithms in this evaluation exchange for the given dataset. The performances of the 7 participating algorithms ranged from 79.1 percent to 89.55 percent in their composite percentage scores. \n4. Model II \nThis section describes a second model for key finding that operates on a representation with fewer dimensions compared to Model I. The motivation behind this is to find the optimal number of dimensions for a specific problem -key finding in this case - ra ther than to  decrease the \ncomputational cost. Dimensionality reduction is performed on the data prior to a distance based calculation to determine the most probable key. The model takes in a parameter indicating the number of dimensions to be used \nin the process of key finding. \n4.1 Dimensionality Reduction \nChroma based representations have been used extensively \nwith great success in models that deal with tonal content analyses such as chord recognition, major/minor key detection and key finding. These chroma based representations are obtained by a fairly straightforward many-to-one mapping from the spectrum to a low-dimensional vector. This vector often has 12 elements - hence the name chroma mapping or chromagram - although the same calculation can be carried out for different vector sizes. Performing this mapping from many bins in the FFT to 12 bins can be viewed as a significant reduction in dimensionality but the question remains as to \nwhether further reduction is possible. We explore the \nrelationship between the number of dimensions used in the representation of tonal content and recognition performance in key finding using Model II.  \n   In order to find the relationship between the number \nof dimensions and key findi ng accuracy, one could start  \n \nfrom a spectral representation and perform dimensionality \nreduction. This leads to a compact tonal representation by preserving the cognitive distances between keys. Izmirli \n[8] demonstrated that cyclic distance patterns can be \nobtained through dimensionality reduction of raw spectral data pertaining to diatonic sets. In \n[2] a comparison of \nspectral and chroma representations of tonal content showed that chroma representa tions were more effective in \ncorrelation based key recognition. Therefore, we have chosen to start with the 12-dimensional PCP chroma representation and explore the effects of dimensionality reduction on the performance of key finding. \nThis model uses Principle Components Analysis (PCA) \nfor performing dimensionality reduction. In PCA, the eigenvectors and eigenvalues of the covariance matrix give the rotation and scaling of the axes. This is based on  maximization of the variance for each principle component. PCA automatically orders the principle components in order of importance and the variance of the data projected onto each principle component monotonically decreases by index. The first has the highest variance. As PCA performs rotation and scaling on the original data to obtain the transformed data it maintains all linear relationships of the data points. \n4.2 Distance-Based Key Finding \nIn this model, the precomputed chroma templates and the summary chroma vectors calculated for Model I are used. For each summary chroma vector , PCA is applied to that \nvector and the 24 chroma templates. The 25 data points are then projected onto the new ax es. A new PCA is calculated \nfor every summary chroma point (instead of applying the same projection repeatedly) to ensure that the new data point contributes to the process. For a given number of dimensions, m, the chroma template point with the \nminimum Euclidean distance to  the summary chroma point \nis found in the m-dimensional space. The key label of the \nnearest point is regarded as the key estimate. Here, m is the \nparameter to the model that determines how many dimensions of the transformed data will be used. The remaining components are ignored. \nAs in Model I, a confidence value is calculated for each \nchroma summary vector. In th is case, the confidence is \ngiven by the proximity of the estimate to a template point compared to its proximity to the next nearest template point. The confidence value is given by the difference between the two distances divi ded by the nearest distance. \nSimilar to Model I, the conf idence values are used as \nweights in determining the key estimate. \n5. Evaluation \nThe key finding accuracies of the two models were evaluated using an audio collection consisting of 152 classical pieces recorded from the naxos collection \n[18]. \nThe first 30 seconds of each piece was processed. Pieces were chosen randomly among those with key information in their labels. The collection had music in all keys but the \ndistribution was not uniform. The number of files in the \nsame key ranged from 3 to 11. Works by the following \ncomposers were used: Albinoni, Albrechtsberger, Alkan, \nBach, C.P.E. Bach, Beethoven, Bella, Brahms, Chopin, Clementi, Corelli, Dvorak, Grieg, Handel, Haydn, Hofmann, Kraus, Liszt, Mendelssohn, Mozart, Pachelbel, Paganini, Prokofiev, Rachma ninov, Scarlatti, Schubert, \nScriabin, Telemann, Tchaikovsky, Vivaldi. \nTable 1. Raw and composite scores for Model I and Model II. \n \nModel Raw \nScore \n(%) Composite \n Score \n (%)  \nVariance \n(%) \nModel I 86.2 88.9 - \nModel II – 12 components 85.5 88.4 100.0 \nModel II – 8 components 84.2 87.2 98.7 \nModel II – 6 components 85.5 88.7 94.8 \nModel II – 4 components 78.9 83.7 87.6 \nModel II – 3 components 76.3 81.9 79.6 \nModel II – 2 components 32.9 44.9 71.1 \n \nModel I was run on this audio set to obtain a reference \nfor key finding performance. Model II was run on the same audio set with different parameters. Table 1 gives the results of the evaluation. The raw score is the percentage \nof the correctly identified keys. The composite score reflects the weighted contributions due to closely related keys as those used in the MIREX 2005 audio key finding evaluation explained in Section 3.5.  \n0 1 2 3 4 5 6 7 8 9 10 11 12020406080100\nNumber of DimensionsAccuracy (Percent)\n0 1 2 3 4 5 6 7 8 9 10 11 12020406080100\nNumber of DimensionsVariance (Percent)Model I Score\nModel II Score\n \nFigure 2. Key finding accuracy (top) and the percentage of \nvariance accounted for in the projected data (bottom) versus \nnumber of dimensions.  \n \nThe top plot in Figure 2 s hows the key finding accuracy \nversus the number of dimensions. This is indicated with \nthe ‘*’ symbols and the continuous line. The dashed line shows the performance of Model I for comparison. The bottom plot shows the percentage of total variance accounted for in the portion of the projected data used as a function of the number of dimensions. This percentage can be viewed as the explanatory power of the data being used with the selected number of components. It should be \nnoted that although there is not much difference in the \npercentage variance between 2 and 3 components, the \naccuracy is drastically poorer with 2 components. Again, \nfor accuracy, it can be seen that the performance remains \nalmost constant from 12 dimensions down to 6. This \nsuggests that a tonal representation with at least 6 dimensions can be used to capture the essential portion of information. It should be noted that accuracy plunges \ngoing from 3 dimensions down to 2 dimensions. This may be interpreted as a corroboration of toroidal models of tonal space over planar models (see for example \n[19]).  \n6. Conclusion \nTwo audio key finding models that produce successful results are presented and a co mparative evaluation of the \ntwo is given. Model I that performed well in the MIREX 2005 audio key finding evaluation is used as reference for performance evaluation of Model II. The second model implements key finding using a low-dimensional space. This model is run with a range of parameters in order to determine the effect of dimensionality for tonal representation in key finding. It was found that the key finding performance did not significantly change from 12 dimensions to 6 dimensions, dropped slighted between 5 to 3 dimensions, and dropped significantly using 2 \ndimensions. It can be concluded that excellent \nperformance is obtained with 6 and higher dimensions and 3 to 5 dimensions provi de acceptable performance. \nFuture work will concentrate on further analysis of \nproperties of the new low-dimensional space and explore mappings from spectral and chroma representations to low-dimensional features. Segmentation with respect to modulation points constitutes a di rect application of these \nmodels. Chord boundary detection combined with key context may be used for tonal analysis. A front-end for tuning adjustment will prove useful for these models to cater to arbitrary reference frequencies. \nReferences \n[1] T. Fujishima, “Realtime Chord Recognition of Musical \nSound: A System Using Common Lisp Music,” Proceedings of the Inter national Computer Music \nConference , Beijing, China, pp. 464–467, 1999. \n[2] Ö. İzmirli, “Template Based Key Finding From Audio,” \nProceedings of the Inter national Computer Music Conference  (ICMC2005), pp. 211-214, Barcelona, Spain, \n2005.  \n[3] E. Gómez, “Tonal Descrip tion of Polyphonic Audio for \nMusic Content Processing,” INFORMS Journal on \nComputing, Special Cluster on Computation in Music,  \nVol.18 .3, 2006. \n[4] G. Cabral, J.-P. Briot, F. Pachet, “Impact of Distance in Pitch Class Profile Computation,”  Proceedings of the 10th \nBrazilian Symposium on Computer Music  (SBCM2005), \nBelo Horizonte, Brazil, 2005. \n[5] S. Pauws, “Musical Ke y Extraction from Audio,\" \nProceedings of the Fifth International Conference on \nMusic Information Retrieval , pp. 96-99, Barcelona, Spain, \n2004. \n[6] H. Purwins, B. Blankertz, G.  Dornhege and K. Obermayer, \n“Scale Degree Profiles from Audio Investigated with Machine Learning Techniques,” Audio Engineering Society \n116th Convention , Berlin, 2004. \n[7] E. Gόmez and P. Herrera, “Estimating the Tonality of \nPolyphonic Audio Files: C ognitive versus Machine \nLearning Modelling Strategies\", Proceedings of the Fifth \nInternational Conference on Music Information Retrieval, Barcelona, Spain, 2004. \n[8] Ö. İzmirli, “The Emergence of Cyclic Patterns among \nSpectra of Diatonic Sets,” to appear in Computing in \nMusicology, MIT Press, Vol. 15. \n[9] A. Sheh and D. P. W. Ellis, “Chord Segmentation and Recognition using EM-Trained Hidden Markov Models,” Proceedings of the Inter national Conference on Music \nInformation Retrieval, Baltimore, Maryland, USA, 2003. \n[10] W. Chai and B. Vercoe, “D etection of Key Change in \nClassical Piano Music,” Proceedings of the International Conference on Music Information Retrieval (ISMIR2005), \npp. 468-474, London, UK, 2005 \n[11] C.-H. Chuan and E. Chew, “Fuzzy Analysis in Pitch Class Determination for Polyphonic Audio Key Finding,” Proc. \nof the International Conference on Music Information Retrieval (ISMIR2005), pp. 296-303, London, UK, 2005. \n[12] Ö. İzmirli and S. Bilgen, “A Model for Tonal Context Time \nCourse Calculation fro m Acoustical Input,” Journal of New \nMusic Research,  Vol.25, No. 3, pp. 276-288, 1996. \n[13] H. Purwins, B. Blankertz, and K. Obermayer, “Constant Q Profiles for Tracking Modulations in Audio Data,” Proceedings of the Inter national Computer Music \nConference , Havana, Cuba, 2001. \n[14] Y. Zhu, M. S. Kankanhalli, and S. Gao, “Music Key \nDetection for Musical Audio,” Proceedings of the 11th \nInternational Multimedia Modelling Conference , \nMelbourne, Australia, 2005.  \n[15] C. Krumhansl, Cognitive Foundations of Musical Pitch , \nOxford University Press, New York, 1990. \n[16] D. Temperley, The Cognition of Basic Musical Structures , \nCambridge, MA: MIT Press, 2001. \n[17] http://www.music-ir.org/evaluation/mirex-results/ \n[18] http://www.naxos.com. \n[19] F. Lerdahl, Tonal Pitch Space , New York: Oxford \nUniversity Press, 2001."
    },
    {
        "title": "A Multifaceted Approach to Music Similarity.",
        "author": [
            "Kurt Jacobson"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1417016",
        "url": "https://doi.org/10.5281/zenodo.1417016",
        "ee": "https://zenodo.org/records/1417016/files/Jacobson06.pdf",
        "abstract": "Previous work has explored the concept of music similarity measures and a variety of methods have been proposed for calculating such measures.  This paper describes a system for music similarity which attempts to model and compare some of the more musically salient features of a set of audio signals.  A model for timbre and a model for rhythm are implemented directly from previous work, and a model for song structure is developed. The different models are weighted and combined to provide an overall music similarity measure.  The system is tested on a small set of popular music files spanning eleven different genres.  The system is tuned to estimate genre boundaries using multidimensional scaling – a technique that allows for quick visualization of similarity data.  An “automatic DJ” application, that generates playlists based on the music similarity models, serves as a subjective evaluation for the system. Keywords: music similarity, automatic DJ, playlist generation, multidimensional scaling, song structure.",
        "zenodo_id": 1417016,
        "dblp_key": "conf/ismir/Jacobson06",
        "keywords": [
            "music similarity measures",
            "audio signals",
            "timbre model",
            "rhythm model",
            "song structure model",
            "multidimensional scaling",
            "genre boundaries",
            "playlist generation",
            "automatic DJ",
            "objective evaluation"
        ],
        "content": "A Multifaceted Approach to Music Similarity Kurt Jacobson University of Miami  Coral Gables, FL USA kurtj@miami.edu  Abstract Previous work has explored the concept of music similarity measures and a variety of methods have been proposed for calculating such measures.  This paper describes a system for music similarity which attempts to model and compare some of the more musically salient features of a set of audio signals.  A model for timbre and a model for rhythm are implemented directly from previous work, and a model for song structure is developed. The different models are weighted and combined to provide an overall music similarity measure.  The system is tested on a small set of popular music files spanning eleven different genres.  The system is tuned to estimate genre boundaries using multidimensional scaling – a technique that allows for quick visualization of similarity data.  An “automatic DJ” application, that generates playlists based on the music similarity models, serves as a subjective evaluation for the system. Keywords: music similarity, automatic DJ, playlist generation, multidimensional scaling, song structure. 1. Introduction From purchase to playback, digital audio files are becoming a ubiquitous part of the music consumption process.  At all levels, improved methods for navigating collections of digital music files are desired.  Audio-based music similarity measures could be applied to this retrieval problem in a number of ways including playlist generation, recommendation of unknown pieces or artists, organization and visualization of music collections, and retrieval by example. Recent work suggests that the limits of what can be achieved with audio-based music similarity measures are close at hand.  Some of the most advanced work in music similarity suggests there exists a “glass ceiling” limiting the effectiveness of this approach [1].  Combining additional dimensions of similarity, such as the temporal loudness descriptor used in [2], provides only a small benefit.  Music similarity is undeniably complex, and modeling it is likely to require a high-dimensional space.  Perhaps using half-a-dozen descriptors as opposed to two or three descriptors will provide some improvement.  Perhaps the number is much higher or even non existent. The system described in this work uses prior art to model the rhythm [5] and timbre of music signals [3].  A song structure model is also developed, although its effectiveness is questionable. The system is tested on a set of popular songs from the iTunes music store. Multidimensional scaling (MDS) is used to visualize the songs in the test set and to estimate how well the music similarity measures identify genre boundaries. 2. Related Wor k There has been a significant amount of research on music similarity and even more research on audio-based genre classification [1-3, 9, 11]. Both areas of research use some type of content-based descriptors extracted from audio signals. Mel frequency cepstral coefficients (MFCC) have been used in previous work to determine spectral similarity [1-4].  The MFCC frames for a given music signal are grouped into clusters and then some statistical technique is used to compare cluster models between songs.  This technique is most closely associated with the timbral attributes of a music signal and is used here as the timbre model. A variety of methods have also been developed for describing and comparing the rhythmic similarity for a set of music [5, 9].  The approach developed in [5] is used here.  It provides information about both rhythm and tempo, making it more appropriate for an automatic DJ application. MDS is applied to the similarity results as a means to quickly estimate system performance.  This approach has been applied to musical timbre perception research in [8], but it is usually not applied to music similarity. 3. System Design A timbre, rhythm, and song structure model are extracted for a given music file and stored in XML format.  The XML files associated with a given set of music files are then used to calculate inter-song similarities.  MDS is then applied to visualize a “music space,” providing estimation of how well the similarity data follows genre boundaries.  The similarity data is also applied to playlist generation. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. © 2006 University of Victoria 3.1 Timbre Model The timbre model is based on [1-4] and uses the open source MA Matlab toolbox.  The timbre model is based on the k-means clustering of MFCC, following the approach outlined in [3].  Although [2] suggests Gaussian mixture models provide improved results, the Earth Mover’s Distance is used to compare timbre models.   3.2 Rhythm Model The rhythm model uses the approach described in [5].  A self-similarity matrix is calculated for a portion of the audio signal.  The same MFCC frames are used as in the timbre model.  Summing across the super-diagonals of the self-similarity matrix generates a “beat spectrum” vector, which is stored in XML.  The vectors are compared using a cosine distance. 3.3 Song Structure Model To extract the song structure model a low-resolution version of the self-similarity matrix is calculated.  By correlating a Gaussian-tapered checkerboard kernel with the main diagonal of the self-similarity matrix, a “novelty index” is calculated.  This process has been applied to automatic audio segmentation and is described in detail in [6]. A threshold is set for the novelty index.  Every positive cross of the threshold is considered to indicate a significant change in the music.  For every positive cross, a counter increments and the position of the change relative to the song length is recorded.  This thought process is described in pseudo code in equation (1). \n! if(Nvp(i)>Nvthreshold&Nvp(i\"1)#Nvthreshold){Cp=Cp+1rlp(j)=ilength(Nvp)j=j+1}     (1)  Where \n! Nvthreshold is some constant and \n! Nvp(i\"1) refers to the previous value of the novelty index for song \n! p.  This way, only the positive crossings of the \n! Nvthreshold increment \n! Cp.  This is a fairly accurate method for finding changes in an audio signal.  Note that \n! rlp records the normalized locations of the changes.  Should \n! rlp=0.5, this would indicate a change in the middle of the song.  The mean of \n! rlp is taken to get \n! µlp. To compare the structure model of song \n! p with that of song \n! q, \n! Cp and \n! Cq are considered magnitudes, while \n! µlp and \n! µlq are taken to be the corresponding angles.  The song structure similarity between songs \n! p and \n! q is calculated as a Euclidean distance between the resulting vectors: \n! SMstructure(p,q)=Cpcos(\"#µlp)$Cqcos(\"#µlq)()2+Cpsin(\"#µlp)$Cqsin(\"#µlq)()2 (2) Here, \n! \" represents some maximum angle.  The lower \n! \" is set, the less impact the relative location of changes has on structure model similarity.  Preliminary tests indicate that \n! \"=#4 is an appropriate value.  This allows songs with different change distributions, but the identical numbers of changes to still be similar.  \n! SMstructure is normalized to one by dividing by the maximum value of \n! SMstructure.  This normalization conforms to the other similarity measures.  3.4 Combined Similarity To derive one matrix of similarity distances \n! SMtotal that combines all the similarity models the following equation is used: \n! SMtotal=wtimbreSMtimbre+wrhythmSMrhythm+wstructureSMstructure (3) The weights reflect the relative importance of each model to overall music similarity.  The weight values should sum to one.  This is a very simple method for combining these measures, and a more advanced method maybe called for in future testing. 4. Testing the System 4.1 iTunes top tens To create a test pool of digital music files, the top ten rated songs in eleven different genres were purchased from the iTunes music store.  The genres were selected arbitrarily and included Hip-hop/Rap, Classical, Pop, World, Jazz, Dance, Electronic, Country, Blues, Alternative, and R&B/Soul.  The iTunes top ten ratings are based on route sales data.  To date, over 980 million songs have been purchased since the service first launched on April 28, 2003.  There are currently iTunes stores available in 21 countries.  Given the popularity and broad scope of the iTunes music store, the genre distinctions applied to the test songs can hardly be ignored, even if they are questionable.  A useful system should at least loosely identify genre boundaries defined by the iTunes music store. This sampling of the iTunes content creates a test pool with distinct files that represent the same song title.  This type of duplication occurs when a song title is in the top ten lists for two genres, when a song title has both radio edit and explicit versions in a top ten list, or when a song title has both album and single versions in a top ten list. 4.2 Specific genre sets Selections from the digital music collections of three professional DJs in the Miami area were also tested.  Each collection represents a specific genre.  These genres include drum and bass, hip hop, and reggae-dancehall.   4.3 Multidimensional Scaling As a means to quickly calibrate the weights applied to each similarity model, MDS solutions were calculated and visualized. MDS is a set of statistical techniques that allow for the visualization of data based on distances or dissimilarity data.  The details of MDS are given in [7].  A two-dimensional metric MDS is applied to the music similarity data.  In the visualization, songs are color-coded by genre to estimate genre classification.  The visualization of the MDS analysis seemed to indicate the best genre separation in the iTunes test set with weight values of \n6.0=timbrew, \n3.0=rhythmw, and \n1.0=structurew Using MDS to estimate the performance of a music similarity system is not a common practice.  However, the technique has been applied to timbre perception research [8].  It also provides a quick estimate of system performance without formally applying the genre classification problem. 4.4 The Auto DJ As a subjective evaluation of the system, an automatic DJ application generates playlists based on the music similarity data generated by the system.  Because the system is sensitive to tempo, no time-stretching or pitch shifting is applied to the music signals.  Instead, quick fades are applied at musical changes indicated by the stored novelty indexes associated with the music signals.  The user can alternatively prompt the auto DJ to mix to the next song immediately.  More sophisticated mixing approaches are also implemented. 5. Results While a rigorous quantitative evaluation of this system’s performance is not provided, there are some encouraging results.  As described in section 4.1, in the iTunes test set, there exist several song titles that appear twice.  The system identifies all eight pairs of doubles as being most similar to the alternate version.  The auto DJ application also consistently plays these double songs back-to-back. A similar result was found when applying the system to the reggae-dancehall set.  In this genre, it is common for several different vocalists to use identical backing tracks to create distinct songs.  Songs with the same backing track are said to be on the same “riddim” (from rhythm).  The auto DJ consistently plays songs on the same riddim back-to-back. 6. Future work A more rigorous testing of this system is required.  Using the ISMIR contest music collections as a test set and applying the system to automatic genre classification would allow for a quantitative comparison to other music similarity systems. Additional models should be added to the system as well.  Some model for melodic or harmonic similarity could improve system performance. Also, the models currently in the system could be improved.  The current rhythm model has no significant psychoacoustic basis.  Different methods for the characterization of music based on rhythmic patterns have been developed such as [9] and should also be explored in the context of this system. Perhaps most importantly, the relevance of the song structure model should be rigorously evaluated.  Currently, the model is only justified by a few test cases and the MDS estimations.  Closely examining the structure models of several test songs seems to indicate that the current method is good at identifying break-down sections in dance music or hip hop, but less effective at identifying more subtle changes, like those found in jazz.   7. Acknowledgments This work makes use of the open source MA Toolbox by E. Pampalk [13].  References [1] E. Pampalk, A. Flexer, and G. Widmer.  “Improvements of Audio-Based Music Similarity and Genre Classification,” Proc ISMIR, 2005. [2] J.-J. Aucouturier and F. Pachet.  “Improving Timbre Similarity: How high is the sky?”  JNRSAS, 2004. [3] B. Logan, “A Content-Based Music Similarity Function,” Cambridge Research Laboratory Technical Report Series, 2001. [4] A. Berenzweig, B. Logan, D. Ellis, and B. Whitman.  “A Large-Scale Evaluation of Acoustic and Subjective Music Similarity Measures.” Johns Hopkins University, 2003. [5] J. Foote, M. Cooper, and U. Nam.  “Audio Retrieval by Rhythmic Similarity.”  Proc ISMIR, 2002. [6] J. Foote and M. Cooper.  “Media Segmentation using Self-Similarity Decomposition.”  FX Palo Alto Laboratory, 2002. [7] J. Kruskal and M. Wish.  “Multidimensional Scaling.”  Sage University, Quantitative Applications in the Social Sciences, 1976. [8] J. Grey. “Multidimensional Perceptual Scaling of Musical Timbres.” Journal of the Acoustical Society Of America, 1977. [9] S. Dixon, F. Gouyon, and G. Widmer.  “Towards Characterization of Music Via Rhythmic Patterns.”  Proc ISMIR, 2004. [10] G. Tzanetakis, G. Essl, and P. Cook.  “Automatic Musical Genre Classification of Audio Signals.”  Proc ISMIR, 2001. [11] T. Lidy and A. Rauber.  “Evaluation of Feature Extractors and Psychoacoustic Transformations for Music Genre Classification.”  Proc ISMIR, 2005. [12] D. Ellis, B. Whitman, A. Berenzweig, and S. Lawrence.  “The Quest for Ground Truth in Musical Artisit Similarity.”  Proc ISMIR, 2002. [13] E. Pampalk.  “A Matlab Toolbox to Compute Music Similarity from Audio.”  Proc ISMIR, 2004."
    },
    {
        "title": "A Retrieval Approach for Human/Robotic Musical Performance.",
        "author": [
            "Ajay Kapur",
            "Eric Singer"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1414894",
        "url": "https://doi.org/10.5281/zenodo.1414894",
        "ee": "https://zenodo.org/records/1414894/files/KapurS06.pdf",
        "abstract": "We present a general framework for performing feature- based synthesis – that is, for producing audio characterized by arbitrarily specified sets of perceptually motivated, quantifiable acoustic features of the sort used in many music information retrieval systems.",
        "zenodo_id": 1414894,
        "dblp_key": "conf/ismir/KapurS06",
        "keywords": [
            "general",
            "framework",
            "feature-based",
            "synthesis",
            "audio",
            "acoustic",
            "features",
            "music",
            "information",
            "retrieval"
        ],
        "content": "Feature-Based Synthesis: A Tool for Evaluating, Designing, and Interactingwith Music IR SystemsMatt HoffmanPrinceton UniversityComputer Science Department35 Olden StreetPrinceton, NJ 08544mdhoffma@cs.princeton.eduPerry R. CookPrinceton UniversityComputer Science & Music Departments35 Olden StreetPrinceton, NJ 08544prc@cs.princeton.eduAbstractWe present a general framework for performing feature-based synthesis – that is, for producing audio characterizedby arbitrarily specified sets of perceptually motivated,quantifiable acoustic features of the sort used in manymusic information retrieval systems.1. IntroductionWe have implemented a general framework for performingfeature-based synthesis, which attempts to synthesize,given any set of feature values, audio that matches thosefeature values as closely as possible. Depending on howone chooses the values to synthesize, feature-basedsynthesis can be used to evaluate the usefulness of a givenset of features for a particular audio IR domain, todiagnose why a system is not performing as well asexpected, as a tool for gaining insight into whatinformation a set of features is encoding, and to generatestimuli for use in studies of human perception.We frame the problem in terms of minimizing thedistance between a target feature vector and the featurevector describing the synthesized sound over the set ofunderlying synthesis parameters. The mapping betweenfeature space and parameter space can be highly nonlinear,complicating optimization. Our framework separates thetasks of feature extraction, feature comparison, soundsynthesis, and parameter optimization, making it possibleto combine various techniques in the search for an efficientand accurate solution to the problem of synthesizingsounds manifesting arbitrary perceptual features.2. Motivation2.1 Feature Evaluation and SelectionFeature-based synthesis can be used to address thisquestion of what relevant qualities, if they were encodedby one’s feature set, might enable better performance on aproblem. As Lidy, Pölzlbauer, and Rauber [1] observe, oneway of qualitatively evaluating the meaningfulness of afeature set is through an analysis-by-synthesis processwhere one extracts the features in question from multiplesounds from the target domain, synthesizes new soundsmatching the extracted features, and compares the originaland resynthesized versions. If the resynthesized version ofa sound file lacks some quality relevant to the problem athand, then it is likely that adding a feature representingthat quality to the feature set will improve performance.2.2 Feature ExplorationOur system provides an interface for synthesizing audiomanifesting feature values specified in real-time, which canbe used to gain a more intuitive understanding of how thevarious features one is using map to actual sounds.Attempting to generate sounds with specific perceptualcharacteristics in this way can stimulate insights into howmuch descriptive power a feature set has.2.3 Perceptual Study Stimulus GenerationStudies such as [2] [3] [4] have investigated the humanability to perceive various physical attributes of soundsources. We suggest that feature-based synthesis could beof use in studying the low-level acoustical properties thathuman listeners use to deduce the more complex physicalattributes of a sound’s source. We can generate soundsdefined over a set of features we expect to correlate withlisteners’ perceptions of, e.g., size, material, or shape, andthen use techniques like those described in [5] to determinehow those sounds map to the ecological features we wishto study. From the data points obtained in this way, wemay be able to discover consistent relationships betweenacoustical and human-generated features that can be used topredict how a sound manifesting certain acoustic featurevalues will be perceived.2.4 Classification System EvaluationWe can also treat the confidence outputs of entireclassification systems as features to match, enabling us togain insights into what sorts of audio a system stronglybelieves fit into one category or another, as well as whatsorts of audio it finds difficult to classify.Permission to make digital or hard copies of all or part of this work forpersonal or classroom use is granted without fee provided that copiesare not made or distributed for profit or commercial advantage andthat copies bear this notice and the full citation on the first page.© 2006 University of Victoria3. Related WorkOur system’s approach of synthesizing audio to fitquantifiable targets is not entirely without precedent. Forexample, a presentation at the 2004 ISMIR graduate school[6] mirrors many of the ideas behind our system. Anotherapproach seeks synthesis parameter values that willproduce a sound closely matching the spectrum of anexisting sound (e.g. [7, 8, 9]). Some work has been doneon synthesizing audio manifesting a limited number ofspecific feature values (e.g. [1]). Finally, relatively recentwork in concatenative synthesis and audio mosaicing,although sometimes working to somewhat different ends,faces some of the same challenges as feature-basedsynthesis. An overview of current work in concatenativesynthesis and audio mosaicing can be found in [10].4. Implementation\nFigure 1. Overview of the architecture of the framework.Given a target feature vector v, P searches through theparameter space of S to find a set of synthesis parameters u’that will minimize D(v, F(S(u’))).Our architecture focuses on four main modularcomponents: feature evaluators, parametric synthesizers,distance metrics, and parameter optimizers. Featureevaluators take a frame of audio as input and output an n-dimensional vector of real-valued features. Parametricsynthesizers take an m-dimensional vector of real-valuedinputs and output a frame of audio. Distance metrics definesome arbitrarily complex function that compares how“similar” two n-dimensional feature vectors are. Finally,parameter optimizers take as input a feature evaluator F, aparametric synthesizer S, a distance metric D, and an n-dimensional feature vector v generated by F (which D cancompare to another such feature vector). The parameteroptimizer P outputs a new m-dimensional synthesisparameter vector u’, a new n-dimensional feature vector v’,and a frame of audio representing the output of S whengiven u’. This frame of audio produces v’ when given asinput to F. v’ represents the feature vector as close to v(where distance is defined by D) as P was able to find inthe parameter space of S.These four components together make up a completesystem for synthesizing frames of audio characterized byarbitrary feature vectors. Any implementation of one ofthese components is valid, so long as it adheres to theappropriate interface.5. Future WorkThe next phase of the project will involve implementingmore feature evaluators, synthesizers, optimizers, anddistance metrics, and evaluating the system’s performancemore rigorously in a variety of domains. Additionally, wewill extend the framework to more directly handle time-domain features and interpolate between parameterssmoothly to produce smoother, more natural output.References[1] T. Lidy, G. Pölzlbauer, and A. Rauber, “Sound re-synthesis from rhythm pattern features – audible insightinto a music feature extraction process,” in Proceedingsof the International Computer Music Conference 2005,pp. 93-96.[2] S. Lakatos, P. Cook, and G. Scavone, “Selective attentionto the parameters of a physically informed sonic model,”Acoustics Research Letters Online, Acoustical Societyof America, March 2000.[3] P. Cook and S. Lakatos, “Using DSP-based parametricsynthesis models to study human perception,” inProceedings of the IEEE Workshop on Applications ofSignal Processing to Audio and Acoustics, 2003.[4] D. Rocchesso, “Acoustic cues for 3-D shapeinformation,” in Proceedings of the 2001 InternationalConference on Auditory Display, 2001.[5] Scavone, G., Lakatos, S., Cook, P., and Harbke, C.,“Perceptual spaces for sound effects obtained with aninteractive similarity rating program,” in Proceedings ofthe International Symposium on Musical Acoustics,2001[6] S. Le Groux, “Extraction of Relevant Controllers for the‘Analysis by Synthesis’ of Musical Sounds,” researchproposal presentation given at ISMIR 2004 Fifth Int.Conf. On Music Inf. Retr., graduate school, available athttp://www.iua.upf.es/mtg/ismir2004/graduateschool/[7] A. Horner, J. Beauchamp, and L. Haken, “MachineTongues XVI: Genetic algorithms and their applicationto FM matching synthesis,” Computer Music Journal17(3), pp. 17-29, 1993.[8] A. Horner, N. Cheung, and J. Beauchamp, “Geneticalgorithm optimization of additive synthesis envelopebreakpoints and group synthesis parameters,” inProceedings of the International Computer MusicConference 1995, pp. 215-222.[9] S. Wun, A. Horner, and L. Ayers, “A comparison betweenlocal search and genetic algorithm methods forwavetable matching,” in Proceedings of theInternational Computer Music Conference, 2004, pp.386-389.[10] D. Schwarz, “Current Research in Concatenative SoundSynthesis,” in Proceedings of the InternationalComputer Music Conference, 2005, pp. 802-805.05, pp.802-805."
    },
    {
        "title": "Towards Quantifying the &quot;Album Effect&quot; in Artist Identification.",
        "author": [
            "Youngmoo E. Kim",
            "Donald S. Williamson",
            "Sridhar Pilli"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1415722",
        "url": "https://doi.org/10.5281/zenodo.1415722",
        "ee": "https://zenodo.org/records/1415722/files/KimWP06.pdf",
        "abstract": "Recent systems for automatically identifying the perform- ing artist from the acoustic signal of music have demon- strated reasonably high accuracy when discriminating be- tween hundreds of known artists. A well-documented issue, however, is that the performance of these systems degrades when music from different albums is used for training and evaluation. Conversely, accuracy improves when systems are trained and evaluated using music from the same album. This performance characteristic has been labeled the “album effect”. The unfortunate corollary to this result is that the classification results of these systems are based not entirely on the music itself, but on other audio features common to the album that may be unrelated to the underlying music. We hypothesize that one of the primary reasons for this phe- nomenon is the production process of commercial record- ings, specifically, post-production. Understanding the pri- mary aspects of post-production, we can attempt to model its effect on the acoustic features used for classification. By quantifying and accounting for this transformation, we hope to improve future systems for automatic artist identification. Keywords: Artist identification, song classification, album effect, music production",
        "zenodo_id": 1415722,
        "dblp_key": "conf/ismir/KimWP06",
        "keywords": [
            "album effect",
            "artist identification",
            "classification",
            "commercial recordings",
            "post-production",
            "performance characteristic",
            "audio features",
            "acoustic signal",
            "systems",
            "accuracy"
        ],
        "content": "Towards Quantifying the “Album Effect” in Artist Identiﬁcation\nYoungmoo E. Kim, Donald S. Williamson, and Sridhar Pilli\nDrexel University\nElectrical and Computer Engineering\n{ykim,dsw36,sp375 }@drexel.edu\nAbstract\nRecent systems for automatically identifying the perform-\ning artist from the acoustic signal of music have demon-\nstrated reasonably high accuracy when discriminating be-\ntween hundreds of known artists. A well-documented issue,\nhowever, is that the performance of these systems degrades\nwhen music from different albums is used for training and\nevaluation. Conversely, accuracy improves when systems\nare trained and evaluated using music from the same album.\nThis performance characteristic has been labeled the “album\neffect”. The unfortunate corollary to this result is that the\nclassiﬁcation results of these systems are based not entirely\non the music itself, but on other audio features common to\nthe album that may be unrelated to the underlying music.\nWe hypothesize that one of the primary reasons for this phe-\nnomenon is the production process of commercial record-\nings, speciﬁcally, post-production. Understanding the pri-\nmary aspects of post-production, we can attempt to model\nits effect on the acoustic features used for classiﬁcation. By\nquantifying and accounting for this transformation, we hope\nto improve future systems for automatic artist identiﬁcation.\nKeywords: Artist identiﬁcation, song classiﬁcation, album\neffect, music production\n1. Introduction\nThe explosion of digital music has led to unprecedented ac-\ncess to large and varied music collections, and now we are\nfaced with the difﬁculty of organizing, labeling, and search-\ning these libraries. This problem is exacerbated by the unre-\nliability of music metadata, which is primarily contributed\nby the community at large and is susceptible to input errors\nand the preferences of individual users (due to the lack of\nglobally adopted labeling standards). A large amount of re-\nsearch in music information retrieval has been focused on\ntasks which attempt to automatically extract relevant infor-\nmation about the music from the acoustic signal itself. Artist\nidentiﬁcation is one such problem that has been the subject\nof myriad papers as well as a competitive task in several of\nthe recent MIREX events.\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies\nare not made or distributed for proﬁt or commercial advantage and that\ncopies bear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2006 University of VictoriaIn early work in this area, it became apparent that artist\nclassiﬁcation scores using supervised learning systems im-\nproved when songs from the same album were used for both\ntraining and evaluation [1]. Conversely, it has been shown\nthat the performance of most systems degrades substantially\nwhen the albums used in training and testing are mutually\nexclusive [2]. This performance characteristic was ﬁrst\ncoined by Whitman et al as the “album effect”.\nThe majority of artist identiﬁcation systems employ a-\ncoustic features modeling the spectral characteristics of au-\ndio (such as Mel-frequency cepstral coefﬁcients and linear\nprediction coefﬁcients). It follows that the album effect is\nprimarily the result of frequency-domain features common\nto the songs on a given album. Although there are notable\nexceptions, albums tend to be recorded within a relatively\nshort consecutive time period, so it makes sense that an\nartist’s choice of instrumentation is fairly consistent within\nan album while it varies somewhat more between albums.\nPerhaps even more important, however, are the aesthetic\nsensibilities of the producer(s), who has overall creative con-\ntrol of the album. The producer’s choices in equipment,\norchestration, and particularly audio effects and post-\nproduction will impart a spectral imprint on the overall al-\nbum. Hence, the album effect has since come to also be\nknown as the “producer effect”.\n2. Commercial music production\nProduction of an album consists of multiple stages, each of\nwhich contributes to the overall album effect. The choice of\nrecording studio and equipment will play a signiﬁcant role\nsince they will be consistent across all songs of an album\n(and will likely change between albums of a given artist).\nDuring recording, microphones will impart characteristic fre-\nquency responses that will affect the spectrum of the record-\ned sound. Studio consoles employ different electronics and\nﬁlters, which will also impart their signatures to the sound.\nAnd during mixing the choice of sound monitors, used by\nthe producer as a reference for the output, will also have a\ndirect effect on the ﬁnal recording.\nIt is likely that the producer’s aesthetic choices during\nmixing also contribute to the album effect. For example,\nthe producer may employ a consistent equalization , ﬁlter-\ning to boost or attenuate particular frequency ranges, to the\nlead vocalist as well as other instruments. Originally im-\nplemented using analog ﬁlters, equalization (or EQ) is nowsystematically applied to most sound elements in a song us-\ning digital ﬁlters. Across the album there is usually consis-\ntency in the placement of instruments in a stereo ﬁeld (pan-\nning). Additionally, the wideness (or lack of wideness) of\nthe stereo image is generally the same for all of the songs. A\nproducer’s continued use of the same digital audio effects,\nsuch as reverberation, chorus, ﬂanging, etc., will also pro-\nvide the songs with more commonalities.\nPost-production consists of processing that is applied to\nall of the individual songs of an album after they have been\nmixed to provide a consistent sound quality across the al-\nbum. This can include the application of additional equal-\nization, dynamics, and effects. Dynamics refer to dynamic\ncompression or expansion, i.e. manipulation of the dynamic\nrange of the audio. This is done during post-production to\nensure that songs on an album do not dramatically change\nin volume so that one can listen to the entire album without\nchanging the level. In popular music, dynamic compression\nis usually applied so that playback on lower-quality audio\nspeakers will not result in distortion.\n3. Acoustic Features used in Classiﬁcation\nThe most common acoustic features used in automatic clas-\nsiﬁcation systems are mel-frequency cepstal coefﬁcients.\nThese employ the well-known cepstrum (inverse-log-Fourier\ntransform), but weighted according to the mel-frequency\nscale to approximate the frequency resolution of human per-\nception. A relatively small number of MFCCs approximate\nthe spectral envelope of a short-time segment of a signal.\nMFCCs are ordered in terms of the bandwidth of energy\nthey contribute to the signal, from wideband to narrowband.\nThus, the lowest coefﬁcient is correlated with the full-band\nenergy contained within the signal (approximating the am-\nplitude envelope). Higher-order coefﬁcients add more detail\nto the spectral envelope approximation. The representation\ncan be inverse transformed to a spectral representation, so\nit is easy to gauge the relative contributions of individual\ncoefﬁcients.\nEqualization is the most obvious post-production feature\nreﬂected in the MFCCs. A post-production equalization ﬁl-\nter applied to all songs within an album will bias the MFCCs\nin a consistent manner. The dynamic range of the music (in-\ndicating the amount of dynamic compression applied) will\nalso be indicated by the ﬁrst mel-cepstrum coefﬁcient.\n4. Analysis Examples\nA particularly interesting data set is formed from original\nrecordings of hit songs and re-releases of those songs as part\nof “Greatest Hits” albums. In this speciﬁc case, the exact\nrecording of the original hit is almost always used for the\nre-release (the intent, after all, is to capitalize on the popu-\nlarity of the earlier release). These collections are usually\nremastered , meaning that the group of hit songs (from mul-\ntiple albums) undergoes additional post-production to createa consistent sound quality across the new collection. With\nolder songs, the recordings may also be re-digitized from\nanalog tape with more modern digital-to-analog converters\nwith improved characteristics. Using this data set, we can\ncompare the characteristics of two different masterings of\nthe same recording.\nIf we examine the MFCCs across an entire song and av-\nerage the resulting time-varying spectral envelope we ob-\nserve distinct differences between an original recording and\nits remastering (Figure 4), which appears to be the result of\nequalization.\n0 5 10 15 20707580859095\nFrequency (kHz)dB SPL\n  \nOriginal\nRemastered\nFigure 1. Average MFCC spectra of original and remastered\nrecordings of The Unforgettable Fire by U2.\nThe distribution of power per analysis frame for each ver-\nsion of the song reveals that the remastered version has a\nhigher overall dynamic level, as well as a wider dynamic\nrange indicating another post-production alteration.\n50100150200250300050100150\nPower (per frame)Occurrences\n  \noriginal\nremastered\nFigure 2. Distribution of signal power per frames of original\nand remastered recordings of The Unforgettable Fire by U2.\n5. Future Directions\nWe intend to use these results to model the within-class vari-\nance for each artist, in order to identify features that are\nmore indicative of the production as opposed to the music\nof the artist. In this way, we hope to improve the overall\nperformance of artist classiﬁcation systems.\nReferences\n[1] B. Whitman, G. Flake, and S. Lawrence. “Artist detection\nin music with Minnowmatch,” in Proc. IEEE Workshop on\nNeural Networks for Signal Processing , 2001, pp. 559-568.\n[2] M.I. Mandel and D.P.W. Ellis. “Song-level features and sup-\nport vector machines for music classiﬁcation,” in Proc. Int’l\nSym. on Music Information Retrieval 2005 , pp. 594-599."
    },
    {
        "title": "Multiple Fundamental Frequency Estimation by Summing Harmonic Amplitudes.",
        "author": [
            "Anssi Klapuri"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1416740",
        "url": "https://doi.org/10.5281/zenodo.1416740",
        "ee": "https://zenodo.org/records/1416740/files/Klapuri06.pdf",
        "abstract": "This paper proposes a conceptually simple and computa- tionally efficient fundamental frequency (F0) estimator for polyphonic music signals. The studied class of estimators calculate the salience, or strength, of a F0 candidate as a weighted sum of the amplitudes of its harmonic partials. A mapping from the Fourier spectrum to a “F0 salience spec- trum” is found by optimization using generated training ma- terial. Based on the resulting function, three different esti- mators are proposed: a “direct” method, an iterative estima- tion and cancellation method, and a method that estimates multiple F0s jointly. The latter two performed as well as a considerably more complex reference method. The number of concurrent sounds is estimated along with their F0s. Keywords: F0 estimation, pitch, music transcription.",
        "zenodo_id": 1416740,
        "dblp_key": "conf/ismir/Klapuri06",
        "keywords": [
            "fundamental frequency (F0) estimator",
            "polyphonic music signals",
            "salience",
            "F0 candidate",
            "harmonic partials",
            "Fourier spectrum",
            "F0 salience spectrum",
            "mapping",
            "generated training material",
            "multiple F0s"
        ],
        "content": "MultipleFundamental FrequencyEstimation bySumming Harmonic Amplitudes\nAnssiKlapuri\nInstitute ofSignal Processing, Tampere University ofTechnology\nKorkeakoulunkatu 1,33720 Tampere, Finland\nanssi.klapuri@tut.fi\nAbstract\nThis paper proposes aconceptually simple andcomputa-\ntionally efﬁcient fundamental frequenc y(F0) estimator for\npolyphonic music signals. The studied class ofestimators\ncalculate thesalience, orstrength, ofaF0candidate asa\nweighted sum oftheamplitudes ofitsharmonic partials. A\nmapping from theFourier spectrum toa“F0salience spec-\ntrum” isfound byoptimization using generated training ma-\nterial. Based ontheresulting function, three different esti-\nmators areproposed: a“direct” method, aniterati veestima-\ntion andcancellation method, andamethod thatestimates\nmultiple F0sjointly .Thelatter twoperformed aswell asa\nconsiderably more comple xreference method. Thenumber\nofconcurrent sounds isestimated along with their F0s.\nKeywords: F0estimation, pitch, music transcription.\n1.Introduction\nPitch information isanessential partofalmost allWestern\nmusic, butextracting thepitch content automatically from\nrecorded audio signals isdifﬁcult. Whereas thespectrogram\nofamusic signal canbecalculated straightforw ardly using\ntheshort-time Fourier transform, computing a“piano roll”-\nrepresentation which showspolyphonic pitch content asa\nfunction oftime isnon-tri vial, andsystems trying todothis\ntend tobeverycomple x.\nF0estimation inpolyphonic music hasbeen addressed\nbymanyauthors. Kashino extracted sinusoid tracks from a\nmusic signal andgrouped them tosound sources based on\nacoustic features andmusical information [1].DeCheveign´e\n[2],Tolonen andKarjalainen [3],andKlapuri [5]proposed\nmethods based onmodeling thehuman auditory system. Goto\n[6]andDavyandGodsill [7]emplo yedaparametric signal\nmodel andstatistical methods. Smaragdis [8]andAbdallah\n[9],proposed unsupervised learning techniques toresolv e\nsound mixtures, andPoliner andEllis [10]introduced aclas-\nsiﬁcation approach totheproblem.\nHere westudy acertain type ofF0estimators, where an\ninput signal isﬁrstspectrally ﬂattened (“whitened”) inorder\ntosuppress timbral information, andthen thesalience ,or\nPermission tomakedigitalorhardcopiesofallorpartofthisworkfor\npersonalorclassroom useisgrantedwithoutfeeprovidedthatcopies\narenotmadeordistributedforpro®torcommercial advantageandthat\ncopiesbearthisnoticeandthefullcitationonthe®rstpage.\nc°2006UniversityofVictoriastrength, ofaF0candidate iscalculated asaweighted sum\noftheamplitudes ofitsharmonic partials. More exactly ,the\nsalience s(¿)ofaperiod candidate ¿iscalculated as\ns(¿)=MX\nm=1g(¿;m)jY(f¿;m)j; (1)\nwhere f¿;m=mfs=¿isthefrequenc yofthem:thharmonic\npartial ofaF0candidate fs=¿,fsisthesampling rate, and\nfunction g(¿;m)deﬁnes theweight ofpartial mofperiod\n¿inthesum.Y(f)istheshort-time Fourier transform of\nthewhitened time-domain signal.1Thespectral whitening\nisastraightforw ardpre-processing operation explained in\nSect. 2.1. Forconvenience, wewrites(¿)asafunction of\nthefundamental period ¿instead oftheF0(=fs=¿).\nThebasic idea of(1)isintuiti velyappealing since pitch\nperception isclosely related tothetime-domain periodicity\nofsounds, andtheFourier theorem states thataperiodic sig-\nnalcanberepresented with spectral components atinteger\nmultiples oftheinverseoftheperiod. Indeed, formulas and\nprinciples resembling (1)havebeen used forF0estimation\nbyanumber ofauthors, under different names andindif-\nferent variants. Already in1960s and70s, Schroeder intro-\nduced thefrequencyhistogramandNoll theharmonic sum\nspectrum (see[11,p.414]). DeCheveign´e[2]discusses har-\nmonicselection methods and, more recently ,Walmsle y[12]\nuses thenameharmonic transform forasimilar technique.\nThequestion ofanoptimal mapping oftheFourier spec-\ntrum toa“F0salience spectrum” isclosely related tothese\nmethods. Here, thefunction g(¿;m)islearned byabrute-\nforce optimization using alargeamount oftraining material.\nBased onthis, aparametric form isproposed forg(¿;m).\nInthispaper ,three different methods based on(1)are\nproposed. Theﬁrstandsimplest isa“direct” method based\nonevaluating s(¿)forarange ofvalues of¿andpicking the\ndesired number ofhighest local maxima init.The second\nmethod represents aniterati veestimation andcancellation\napproach, where themaximum ofs(¿)isused toestimate\noneF0which isthen cancelled from themixture before es-\ntimating thenextone. The third method estimates allF0s\njointly .Forthelatter twomethods, atechnique isproposed\nforestimating thenumber ofsounds inthemixture. The\niterati vemethod admits averyfastimplementation which\n1De®ning s(¿)intermsofthepowerspectrum insteadofthemagnitude\nspectrum wouldhavecertainanalytical advantages,butthisledtoclearly\ninferiorF0estimation resultsdespiteofextensiveinvestigation.0100020003000400050006000700000.51\nFrequency (Hz)Power\nFigure1.Responses Hb(k)appliedinspectralwhitening .\ndoes notrequire evaluating s(¿)forallperiod candidates ¿.\nThethree methods areevaluated using mixtures ofmusical\ninstrument sounds, andtheresults arecompared with three\nreference methods [3],[4]and[5].\n2.Proposedmethods\nThis section describes theproposed methods indetail.\n2.1.Spectralwhitening\nOne ofthebigchallenges inF0estimation istomakesys-\ntems robustfordifferent sound sources. Awaytoachie ve\nthisistotrytosuppress timbral information prior totheac-\ntualF0estimation. This canbedone byestimating therough\nspectral energydistrib ution (which largely deﬁnes thetim-\nbreofasound) andthen ﬂattening itentirely orpartly by\ninverse ﬁltering. This process iscalled spectral whitening\nandthere areseveralwaysofdoing it(see e.g. [3]). Here\nafrequenc y-domain technique isemplo yedwhich iseasy to\nimplement andleads togood results inpractice.\nFirst, thediscrete Fourier transform X(k)oftheinput\nsignal x(n)iscalculated inananalysis frame thatisHanning-\nwindo wed andzero-padded totwice itslength before the\ntransform. Then abandpass ﬁlterbank issimulated inthe\nfrequenc ydomain. Center frequencies cb[Hz] ofthesub-\nbands aredistrib uted uniformly onthecritical-band scale,\ncb=229£(10(b+1)=21:4¡1);andeach subband b=\n1;:::;30hasatriangular power response Hb(k)that ex-\ntends fromcb¡1tocb+1andiszero elsewhere (seeFig.1).\nStandard deviations ¾bwithin thesubbands barecalcu-\nlated byapplying theresponses Hb(k)inthefrequenc ydo-\nmain:\n¾b=Ã\n1\nKX\nkHb(k)jX(k)j2!1=2\n; (2)\nwhere Kisthelength oftheFourier transform. Next,band-\nwise compression coefﬁcients °b=¾º¡1\nbarecalculated,\nwhere º=0:33isaparameter determining theamount of\nspectral whitening applied. Thecoefﬁents°barelinearly in-\nterpolated between thecenter frequencies cbtoobtain com-\npression coefﬁcients °(k)forallfrequenc ybinsk.\nFinally ,awhitened spectrum Y(k)isobtained byweight-\ningthespectrum oftheinput signal bythecompression co-\nefﬁcents, Y(k)=°(k)X(k):\n2.2.Calculation ofthesaliencefunctioninpractice\nCalculation ofs(¿)using (1)directly requires evaluating\nY(f)forarbitrary frequencies fwhich iscomputationallyinefﬁcient. UseofthefastFourier transform becomes pos-\nsible byreplacing Y(f)in(1)byitsdiscrete version Y(k)\nandbyapproximating s(¿)by\n^s(¿)=MX\nm=1g(¿;m)max\nk2·¿;mjY(k)j; (3)\nwhere theset·¿;mdeﬁnes arange offrequenc ybins inthe\nvicinity ofthem:thovertone partial oftheF0candidate\nfs=¿.More exactly ,\n·¿;m=[hmK=(¿+¢¿=2)i;:::;hmK=(¿¡¢¿=2)i];(4)\nwhereh¢idenotes rounding tothenearest integer.Itisclear\nthat^s(¿)¼s(¿)when¢¿!0.Inpractice, however,itis\nuseful toset¢¿according tothespacing between succes-\nsiveperiod candidates ¿inorder toensure thatallspectral\ncomponents kbelong totherange ·¿;mofatleast onepe-\nriod candidate ¿when misﬁxed.Here weusethevalue\n¢¿=0:5,thatis,thespacing between fundamental period\ncandidates ¿ishalfthesampling interv al.2\n2.3.Optimization oftheweightfunction\nAremaining taskistooptimize thefunction g(¿;m)soasto\nminimize theF0estimation error rateofthesystem. Forthis\npurpose, wegenerated training material consisting ofran-\ndom mixtures ofmusical instrument sounds with their ref-\nerence F0data. Thedatabase from which thesamples were\ndrawnisdescribed inmore detail inSect. 3.Themixtures\nwere generated byﬁrst allotting aninstrument andthen a\nrandom sound from itsplaying range, limiting F0sbetween\n40Hzand2100 Hz. This two-stage randomizing wasre-\npeated until thedesired number ofsounds wasobtained, and\nthesounds were then mixedwith equal mean-square levels.\nOne thousand mixtures ofone, two,four,andsixsounds\nwere generated, totalling 4000 training instances.\nF0estimation wasperformed simply bypicking Phigh-\nestlocal maxima inthefunction ^s(¿).Thenumber ofF0s\nineach mixture, P,wasgiventotheestimator along with a\n93msanalysis frame.Multiple-F0 estimation error rateis\ndeﬁned astheproportion ofreference F0sthatwere notcor-\nrectly found. Inpredominant-F0 estimation ,thetask isto\nﬁndonly oneF0ineach mixture. Inthiscase, themaximum\nof^s(¿)wastakenandjudged correct ifitmatched anyof\nthereference F0sinthemixture. Acorrect F0estimate was\ndeﬁned todeviate lessthan 3%from thereference. Thecri-\nterion tobeminimized intheoptimization wastheaverage\nofmultiple-F0 andpredominant-F0 estimation error rates in\ndifferent polyphonies.\nTwodifferent factorized forms ofg(¿;m)were studied:\ng(¿;m)=g1(¿)g2(m); (5)\ng(¿;m)=g1(¿)g3(f¿;m): (6)\n2InSect.2.4wherethefastalgorithm ispresented, thesampling of¿\nhasonlyminoreffectoncomputational ef®ciency,andtherefore verydense\nsampling canbeimplemented. Inpractice, ¢¿=0:5suf®ces.Reducing thetwo-parameter function g(¿;m)toaproduct\noftwomarginal functions makestheoptimization task eas-\nierandislikelytolead toaresult thatgeneralizes better to\npreviously unseen testcases.\nLetusﬁrstconsider theform givenby(5).Thefunction\ng1(¿)wasparametrized byinterpolating between ten“an-\nchor points” which were distrib uted roughly asageomet-\nricseries between thefundamental frequencies 30Hzand\n2500 Hz. Similarly ,thefunction g2(m)wasparametrized\nbydistrib uting tenanchor points asageometric series be-\ntween the1stand21st harmonic, andthefunction g2(m)\nwasthen linearly interpolated between these. Theoptimiza-\ntion wasdone byinitializing theamplitudes oftheanchor\npoints tounity values andthen updating them cyclically ,one\natthetime, soastominimize theF0estimation error rate.\nFigure 2showsthelearned functions g1(¿)andg2(m),\ntogether with theresulting F0estimation error rates (forthe\ntraining data). Thefound shape ofg1(¿)ismore orlessa\nlinear function ofF0(that is,fs=¿),whereas g2(m)con-\nverged roughly tothe1=mshape, howeverwith smaller\nweights forthelowest even-numbered harmonics (seeFig.2).\nThepredominant-F0 estimation accurac yisgood, butmulti-\nple-F0 estimation leavesroom forimpro vement.\nOptimization forthefactorization in(6)wasdone ina\nsimilar manner .Thefunction g3(f¿;m)wasparametrized by\ninterpolating itlinearly between 13anchor points thatwere\ndistrib uted roughly asageometric series between 30Hzand\n7kHz. Theoptimization wasagaincarried outbyupdating\ntheamplitudes oftheanchor points cyclically ,oneatatime,\nsoastominimize theF0estimation error rate. The best\nresult wasobtained bystarting theoptimization from con-\nﬁguration g(¿;m)=1=m,which isobtained byinitializing\ntheanchor points asg1(¿)=fs=¿andg3(f¿;m)=1=f¿;m.\nFigure 3showsthelearned functions g1(¿)andg3(f¿;m),\nandtheresulting F0estimation error rates. Ascanbeseen,\nthefunctions g1(¿)andg3(f¿;m)donotdrift veryfarfrom\ntheir initial shape, andtheerror rates areabout thesame as\nthose achie vedwith theprevious factorization. The latter\nform (6)isinteresting, because itallowsasimple imple-\nmentation where thespectrum Y(k)isﬁrst ﬁltered using\ntheresponse g3(f¿;m),then^s(¿)iscomputed without any\nweights, andintheend^s(¿)isweighted withg1(¿).\nThelatter factorization (6)wastakenintouse. Togetrid\nofthelargenumber freeparameters (theanchor points), the\nfunction g1(¿)ismodeled asalinear function offundamen-\ntalfrequenc y,g1(¿)=fs=¿+®;andthefunction g3(f¿;m)\nismodeled asaninverse ofthefrequenc yf¿;mandamod-\neration term¯,g3(f¿;m)=1=(f¿;m+¯).Thedashed lines\ninFigure 3showthemodeled functions. Asaresult, the\nfunction g(¿;m)canbeﬁnally written as\ng(¿;m)=fs=¿+®\nmfs=¿+¯; (7)\nwhere theparameters ®and¯aregiveninSect. 3.00.511.5200.511.52\nF0 (kHz)g1(τ)\n151015200123\nHarmonic indexg2(m)\n1246010203040\nPolyphonyError rate (%)\nFigure2.Thelearnedfunctions g1(¿)andg2(m)areshownin\ntheleftandthemiddlepanels,respectively.Forclarity,g1(¿)\nisdrawnasafunctionofF0(fs=¿)insteadof¿.Theright\npanelshowstheresultingerrorratesformultiple-F0 estima-\ntion(black)andpredominant-F0 estimation (white).\n00.511.520500100015002000\nF0 (kHz)g1(τ)\n12345670123x 10−3\nFrequency (kHz)g3(f)\n1246010203040\nPolyphonyError rate (%)\nFigure3.Thelearnedfunctions g1(¿)andg3(f¿;m)aredrawn\nwithasolidlineintheleftandthemiddlepanels,respectively.\nThedashedlinesshowthecorresponding parametric models.\nTherightpanelshowsF0estimation errorratesbeforethe\nparametric modeling.\n2.4.Iterativeestimation andcancellation\nThe“direct” F0estimator described abovesuffersfrom the\nproblem thatasingle F0inasound mixture produces several\npeaks to^s(¿),andalthough themaximum of^s(¿)isarobust\nindicator ofoneofthetrueF0s, thesecond orthird-highest\npeak isoften duetothesame sound andlocated at¿thatis\nhalfortwice theposition ofthehighest peak.\nMultiple-F0 estimation accurac ycanbeimpro vedbyan\niterati veestimation andcancellation scheme where each de-\ntected sound iscancelled from themixture and^s(¿)isup-\ndated accordingly before estimating thenextF0.Thebasic\ncancellation mechanism described here issimilar tothatpre-\nsented in[5],except thathere afastalgorithm isdescribed\nforﬁnding themaximum of^s(¿)andatechnique ispro-\nposed forestimating thenumber ofsounds inthemixture.\nLetusﬁrstlook atanefﬁcient wayofﬁnding themaxi-\nmum of^s(¿).Some what surprisingly ,theglobal maximum\nof^s(¿)andthecorresponding value of¿canbefound with\nafastalgorithm thatdoes notrequire evaluating ^s(¿)forall\n¿.This isanother motivation fortheiterati veestimation and\ncancellation approach where only themaximum of^s(¿)is\nneeded ateach iteration.\nLetusdenote theminimum andmaximum fundamental\nperiod ofinterest by¿minand¿max,respecti vely,andthe\nrequired precision ofsampling ¿by¿prec.Afastsearch\nofthemaximum of^s(¿)canbeimplemented byrepeatedly\nsplitting therange [¿min;¿max]intosmaller “blocks”, com-\nputing anupper bound forthesalience within each block\nq,smax(q),andcontinuing bysplitting theblock with theAlgorithm 1:Fastsearch ofthemaximum of^s(¿)\nQÃ1;¿low(1)Ã¿min;¿up(1)Ã¿max;qbestÃ1; 1\nwhile ¿up(qbest)¡¿low(qbest)>¿precdo 2\n#Split thebestblock andcompute newlimits 3\nQÃQ+1 4\n¿low(Q)Ã(¿low(qbest)+¿up(qbest))=2 5\n¿up(Q)Ã¿up(qbest) 6\n¿up(qbest)Ã¿low(Q) 7\n#Compute newsaliences forthetwoblock-halv es 8\nforq2fqbest;Qgdo 9\nCalculate smax(q)using Equations (3)-(4) 10\nwithg(¿;m)=fs=¿low(q)+®\nmfs=¿up(q)+¯\n¿=(¿low(q)+¿up(q))=2\n¢¿=¿up(q)¡¿low(q) 11\nend12\n#Search againthebestblock 13\nqbestÃargmax q2[1;Q]smax(q) 14\nend15\nReturn ^¿=(¿low(qbest)+¿up(qbest))=2\n^s(^¿)=smax(qbest) 16\nhighest smax(q).Letusdenote thenumber ofblocks byQ\nandtheupper andlowerlimits ofblock qby¿low(q)and\n¿up(q),respecti vely.Indexofthehighest-salience block is\ndenoted byqbest.Thealgorithm starts with only oneblock\nwith upper andlowerlimits at¿minand¿max,andthen re-\npeatedly splits thebest block intotwohalves,asdetailed in\nAlgorithm 1.3Asaresult, itgivesthemaximum of^s(¿)\nandthecorresponding value of¿.\nOnlines 13–14 ofthealgorithm, inorder toobtain anup-\nperbound forthesalience ^s(¿)within range[¿low(q);¿up(q)],\nEquation (3)isevaluated using thegivenvalues forg(¿;m),\n¿,and¢¿.Splitting ablock later oncanonly decrease the\nvalue ofsmax(q)when computed forthenewblock-halv es.\nAlgorithm 1isimportant fortworeasons. First, itallows\nsearching themaximum of^s(¿)efﬁciently evenwhen the\nrequired sampling density of¿isveryhigh. Secondly ,in-\ncreasing thesampling density of¿hastheconsequence that\nallthesets·¿;min(3)contain exactly onefrequenc ybin,\ninwhich case thenon-linear maximization operation van-\nishes and^s(¿)becomes alinear function ofthemagnitude\nspectrumjY(k)j,making itanalytically more tractable.\nTheiterati veestimation andcancellation goes asfollows:\n1.Aresidual spectrum YR(k)isinitialized toequalY(k),\nandaspectrum ofdetected sounds YD(k)tozero.\n2.Afundamental period ^¿isestimated usingYR(k)and\nAlgorithm 1.Themaximum of^s(¿)determines ^¿.\n3.Harmonic partials of^¿arelocated inYR(k)atbin\nhmK=¿i.Thefrequenc yandamplitude ofeach par-\ntialisestimated andused tocalculate itsmagnitude\n3Itisevenmoreef®cienttostartwithp\n(¿max¡¿min)=¿precblocks.spectrum atthefewsurrounding frequenc ybins. The\nmagnitude spectrum ofthem:thpartial isweighted\nbyg(^¿;m)andadded tothecorresponding position\nofthespectrum ofdetected sounds, YD(k).\n4.Theresidual spectrum isrecalculated as\nYR(k)Ãmax(0;Y(k)¡dYD(k));\nwhere dcontrols theamount ofthesubtraction.\n5.Ifthere aresounds remaining inYR(k),return toStep 2.\nNote thatthepurpose ofthecancellation istosuppress har-\nmonic andsubharmonic peaks of^¿in^s(¿).This should\nbedone insuch awaythattheresidual spectrum YR(k)is\nnotcorrupted toomuch todetect remaining sounds atthe\ncoming iterations. These conﬂicting requirements areeffec-\ntivelymetbyweighting thepartials ofadetected sound by\ng(¿;m)inStep 3before adding them toYD(k).Inpractice\nthismeans thatthehigher partials arenotentirely cancelled\nfrom themixture sinceg(¿;m)¼1=m.Parameter d¼1\ntogether withg(¿;m)deﬁnes theamount ofsubtraction.\nThe function g(¿;m)wasre-optimized fortheiterati ve\nmethod using asimilar optimization scheme asdescribed\nabove.Despite thedouble role ofg(¿;m)here (affecting\nboth thesalience andthecancellation), theobtained func-\ntionsg1(¿),g2(m),andg3(f¿;m)were verysimilar tothose\nshowninFigs. 2–3, andthemodel (7)issuitable.\nWhen thenumber ofsounds inthemixture isnotgiven,it\nhastobeestimated. This task,polyphony estimation ,isac-\ncomplished bystopping theiteration when anewly-detected\nsound ^¿jatiteration jnolonger increases thequantity\nS(j)=Pj\ni=1^s(^¿i)\nj°; (8)\nwhere °=0:70wasfound empirically .4The value ofj\nmaximizing (8)istakenastheestimated polyphon y^P.\n2.5.Jointestimation ofmultipleF0s\nThedescribed iterati vemultiple-F0 estimator isefﬁcient and\nproduces good results, butitalsoleavesuswith acouple of\nopen questions. Howmuch does theiterati vesearch algo-\nrithm affecttheresult? Isitpossible tocompute saliences of\nthefound sounds sothattheorder ofdetecting them would\nnotaffect? This section describes ajoint estimator which\ncananswer these questions.\nFirst, thesalience function ^s(¿)iscalculated according\nto(3).Then, Ihighest local maxima of^s(¿)arechosen as\ncandidate fundamental period values¿i,i=1;:::;I.For\neach candidate i,thefollowing quantities arecomputed:\n²Frequenc ybins ofharmonic partials ki;m,where mis\ntheharmonic indexandki;mcorresponds tothemax-\nimum ofjY(k)jintherange·m;¿ i(see(4)).\n4NotethatS(j)wouldbemonotonically decreasing for°=1(average\nof^s(^¿i):s)andmonotonically increasing for°=0(sum).²Candidate spectrum Zi(k)isanestimate ofthespec-\ntrum ofcandidate i,andiscalculated bytranslating\nthespectrum ofthewindo wfunction (Hanning) tothe\npositions ki;mandadding them toZi(k)after scaling\nby(d=2)g(¿i;m),where disthecancellation param-\neterfrom Step 4oftheiterati vemethod.\nLetusdenote byPthenumber ofsimultaneous F0sto\nestimate andbyIasetofPdifferent candidate indices i\n(there are¡I\nP¢\ndifferent possibilities). Then thejoint estima-\ntionconsists ofﬁnding such anindexsetIthatmaximizes\nG(I)=X\ni2IX\nmg(¿i;m)jY(ki;m)jY\nj2Ini(1¡Zj(ki;m));\n(9)\nwhere Zj(k)·1because (d=2)g(¿;m)·1.Bycompari-\nsonwith (1),itcanbeseen thattheabovegoodness measure\nimplements asimilar harmonic summing model butwith the\ndifference that thesalience contrib ution ofsound iisre-\nduced by“inhibition” (cancellation) from other sounds jin\nI,asdetermined bytheir estimated spectrum Zj(k).Infact,\ntheabovemodel isaveryclose equivalent totheiterati ve\nmethod presented above,thedifference being thathere the\nestimation isperformed jointly instead ofiterati vely.The\nreason whytheparameter dishalvedwhen calculating Zj(k)\nisthathere allsounds inhibit allothers, whereas intheitera-\ntivemethod only sounds detected atearlier iterations inhibit\n(through cancellation) those detected later.\nAproblem with (9)isthatthecomputational comple xity\nofevaluating G(I)forall¡I\nP¢\ndifferent indexcombinations\nIiscomputationally impractical. Areasonably efﬁcient im-\nplementation ispossible bymaking useofthelowerbound\n~G(I)ofG(I).Bywriting outtheproduct in(9),itiseasy\ntoseethatG(I)¸~G(I)where\n~G(I)=X\ni2IX\nmg(¿i;m)jY(ki;m)j[1¡X\nj2IniZj(ki;m)]\n=X\ni2I^s(¿i)¡X\ni2IX\nj2IniInh(i;j) (10)\nwhere the“inhibition” Inh(i;j)isanon-symmetric function\nInh(i;j)=X\nmg(¿i;m)jY(ki;m)jZj(ki;m): (11)\nFrom thecomputational viewpoint, theadvantage of(10)\nisthatthevalues^s(¿i)andInh(i;j)canbeprecomputed,\nmaking theevaluation of(10) aneasy operation fordifferent\nindexcombinationsI.Another crucial factor isthat, dueto\nthesparseness ofZj(k),thelowerbound ~G(Ic)isactually\nanaccurate estimate ofG(Ic)sothatG(Ic)¼~G(Ic).The\nalgorithm forﬁnding asetIwhich maximizes (9)is:\n1.Initialize Idifferent setsIwith theindividual candi-\ndates¿i,i=1;:::;I,sothatsetnumber cisinitial-\nized withIcÃfcgand~G(Ic)Ã^s(¿c).2.Generate I£Inewcombinations byextending all\ntheexisting combinations with alltheindividual can-\ndidates ¿i.Thegoodness measures ofthese extended\ncombinations canbecomputed recursi velyas\n~G(Ic[i)=~G(Ic)+^s(¿i)¡X\nj2Ic(Inh(i;j)+Inh (j;i)):\n3.Sort theI£Iextended setsindescending goodness\norder andretain onlyIbest combinations with dif-\nferent goodness measures. The latter prevents from\nchoosing different permutations ofasame set.\n4.Ifpolyphon yPwasnotreached, return toStep 2.\n5.Evaluate theexact goodness measure (9)fortheI\nbest combinations andchoose thecombination with\nthehighest value tooutput.\nIfthepolyphon yPisnotgiven,itcanbeestimated byeval-\nuating theexact goodness measure (9)fortheIbest com-\nbination alwaysbetween theSteps 3and4,andbystoring\nthebestj-size combinationIbest(j).Extending thesetsis\ncontinued aslong asthefollowing measure increases:\nS(j)=G(Ibest(j))=j°; (12)\nwhere theparameter °=0:73wasfound empirically .\nAnadvantage ofthejoint estimation method isthat, con-\ntrary totheiterati vesystem, here theorder ofdetecting the\nsounds does notaffecttheresult. Adrawback isthatthe\njoint estimator iscomputationally lessefﬁcient asitrequires\nevaluating thefunction ^s(¿)forall¿andtherefore Algo-\nrithm 1cannot beused. Finding theoptimal combinationI\nintheabovealgorithm isstillquite efﬁcient when 50–100\ncandidates ¿iareselected from^s(¿),which isroughly the\namount needed toretain thetrue periods among them. In\nthesimulations, weusedI=100.\n3.Evaluation\nSimulation experiments were carried outtoevaluate thepro-\nposed estimators. These were compared with thereference\nmethods [3]and[5]based onauditory models, andtheref-\nerence method [4]based onspectral techniques. Testdata\nconsisted ofrandom mixtures ofmusical instrument sam-\nples with F0sbetween 40and2100 Hz,generated inthe\nsame wayasinSect. 2.3butofcourse randomizing newtest\ncases here.5Theacoustic database, however,wasthesame,\nandconsisted ofsamples from theMcGill University Master\nSamples collection, theUniversity ofIowawebsite, IRCAM\nStudio Online, andofrecordings fortheacoustic guitar .In\ntotal, there were 2842 samples from 32musical instruments.\nAsestimation ofthenumber ofconcurrent sounds isvery\ndifﬁcult initself, weevaluate F0estimation andpolyphon y\nestimation separately .Theparameter values®,¯,anddwere\nthesame forallthethree proposed methods andwere 27Hz,\n5Forthereference method[3],F0swererestricted between40Hzand\n530Hz,sincethemethodisnotveryrobustforF0shigherthanthis.12460102030405060\ndij[3][4]\n[5]Multiple−F0 estim.\n46 ms frame\nPolyphonyError rate (%)\n124601020304050\ndij[3][4]\n[5]Multiple−F0 estim.\n93 ms frame\nPolyphonyError rate (%)\n124601020304050\ndij[3][4]\n[5]Predominant−F0 estim.\n46 ms frame\nPolyphonyError rate (%)\n1246051015202530\ndij[3]\n[4]\n[5]Predominant−F0 estim.\n93 ms frame\nPolyphonyError rate (%)\nFigure4.Multiple-F0 estimation andpredominant-F0 estima-\ntionresultsin46msand93msanalysisframes.Readingleftto\nright,eachstackofsixthinbarscorrespondstotheerrorrates\nofthedirect(d),iterative(i),joint(j),andreferencemethods\n[3],[4],and[5]inacertainpolyphony .\n123456789*\nPolyphony123456789*\nPolyphony123456789*\nPolyphony123456789*\nPolyphony\nFigure5.Histograms ofpolyphony estimates fortheiterative\nmethodanda93msanalysisframe.Theasterisksindicatethe\ntruepolyphony (1,2,4,and6,fromlefttoright).\n320Hz,and1.0,respecti vely,for46msanalysis frame, and\n52Hz,320Hz,and0.89, respecti vely,for93msframe.\nFigure 4showstheF0estimation results oftheproposed\nandthereference methods. Here thenumber ofconcur -\nrent sounds (polyphon y)wasgivenasaside-information\ntotheestimators. The error rates arepractically thesame\nfortheproposed iterati veandjoint methods andtherefer-\nence method [5],andthese three outperform themethods\n[3]and[4]. This isaverynice result, since thebest refer-\nence method [5]involvescomputation ofanauditory model,\nincluding e.g.Fourier transforms at70subbands. Thepro-\nposed methods areconsiderably simpler andcomputation-\nallymore efﬁcient. Inmonophonic cases (polyph. 1),about\n50% oftheerrors arecaused byF0sbetween 40and65Hz.\nThe lowerpanels ofFigure 4showpredominant-F0 es-\ntimation accuracies. Here theerror rates arepractically the\nsame fortheproposed direct andtheiterati vemethod and\nforthereference method. Theaccurac yofthejoint method,\nhowever,isclearly better inhigh polyphonies.\nFigure 5illustrates theresults ofpolyphon yestimation\nfortheiterati vemethod anda93msanalysis frame. Results\nforthejoint method were verysimilar andarenotshown.\nTheasterisk indicates truepolyphon yineach panel, andbars\nshowahistogram oftheestimates. Theresults arenotfully\nsatisf actory ,anditseems thatrobustestimation ofthenum-\nberofsounds requires more than oneanalysis frame.4.Conclusions\nTheprinciple ofsumming harmonic amplitudes asgivenby\n(1)isverysimple, yetitsufﬁces forpredominant-F0 estima-\ntioninpolyphonic signals provided thattheweights g(¿;m)\nofdifferent partials andperiods areappropriate. Inmultiple-\nF0estimation, both theiterati veandthejoint estimator were\nsuccessful, buttheiterati vemethod admits afastimplemen-\ntation andistherefore more appealing. Thejoint estimator ,\ninturn, achie vesbetter predominant-F0 estimation. Both\nmethods canbeseen toimplement themodel embodied in\nthegoodness measure (9),which isverysimplistic consider -\ningthewide range ofinstruments andF0values addressed.\nReferences\n[1]K.KashinoandH.Tanaka,ªAsoundsourceseparation sys-\ntemwiththeabilityofautomatic tonemodeling, ºinInterna-\ntionalComputer MusicConf.,(Tokyo,Japan),1993.\n[2]A.deCheveignÂe,ªSeparation ofconcurrent harmonic\nsounds: Fundamental frequencyestimation andatime-\ndomaincancellation modelforauditoryprocessing, ºJournal\noftheAcoust.Soc.Am.,vol.93,no.6,1993.\n[3]T.TolonenandM.Karjalainen, ªAcomputationally ef®cient\nmultipitch analysismodel,ºIEEETrans.SpeechandAudio\nProcessing,vol.8,no.6,pp.708±716, 2000.\n[4]A.P.Klapuri,ªMultiple fundamental frequencyestimation\nbasedonharmonicity andspectralsmoothness, ºIEEETrans.\nSpeechandAudioProcessing,vol.11,no.6,2003.\n[5]A.P.Klapuri,ªAperceptually motivatedmultiple-F0 estima-\ntionmethodforpolyphonic musicsignals,ºinIEEEWork-\nshoponApplications ofSignalProcessing toAudioand\nAcoustics ,(NewPaltz,NY),2005.\n[6]M.Goto,ªAreal-time musicscenedescription system:\nPredominant-F0 estimation fordetecting melodyandbass\nlinesinreal-worldaudiosignals,ºSpeechCommunication ,\nvol.43,no.4,pp.311±329, 2004.\n[7]M.Davy,S.Godsill,andJ.Idier,ªBayesian analysisofWest-\nerntonalmusic,ºJournaloftheAcoustical SocietyofAmer-\nica,vol.119,no.4,pp.2498±2517, 2006.\n[8]P.Smaragdis andJ.C.Brown,ªNon-negativematrixfactor-\nizationforpolyphonic musictranscription, ºinIEEEWork-\nshoponApplications ofSignalProcessing toAudioand\nAcoustics ,(NewPaltz,NY),2003.\n[9]S.A.Abdallah andM.D.Plumbley,ªPolyphonic transcrip-\ntionbynon-negativesparsecodingofpowerspectra,ºin\nInternational ConferenceonMusicInformation Retrieval,\n(Barcelona, Spain),pp.318±325, Oct.2004.\n[10]G.E.PolinerandD.P.W.Ellis,ªAclassi®cation approach\ntomelodytranscription, ºin6thInternational Conferenceon\nMusicInformation Retrieval,(London, UK),2005.\n[11]W.J.Hess,PitchDetermination ofSpeechSignals.Berlin\nHeidelber g:Springer,1983.\n[12]P.Walmsley,SignalSeparationofMusical Instruments.\nSimulation-based methodsformusicalsignaldecomposition\nandtranscription .PhDthesis,Department ofEngineering,\nUniversityofCambridge, Sept.2000."
    },
    {
        "title": "Composer attribution by quantifying compositional strategies.",
        "author": [
            "Peter van Kranenburg"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1415062",
        "url": "https://doi.org/10.5281/zenodo.1415062",
        "ee": "https://zenodo.org/records/1415062/files/Kranenburg06.pdf",
        "abstract": "Taking a theory of musical style, developed by Leonard B. Meyer, as a starting point, an experiment is described in which statistical pattern recognition algorithms are used to characterize a particular musical style with respect to other styles. The resulting description can be used in authorship discussions. In the current study, a number of disputed or- gan works from the Bach catalog is used to illustrate the possibilities of this approach. Keywords: Musical Style, Pattern Recognition, Classical Music, Composer Attribution, Johann Sebastian Bach.",
        "zenodo_id": 1415062,
        "dblp_key": "conf/ismir/Kranenburg06",
        "keywords": [
            "Musical Style",
            "Pattern Recognition",
            "Classical Music",
            "Composer Attribution",
            "Johann Sebastian Bach",
            "Statistical Algorithms",
            "Stylistic Analysis",
            "Compositional Strategies",
            "Authorship Discussion",
            "Style Constraints"
        ],
        "content": "Composer attribution by quantifying compositional strategies\nPeter van Kranenburg\nDepartment of Information and Computing Sciences\nUtrecht University\npetervk@cs.uu.nl\nAbstract\nTaking a theory of musical style, developed by Leonard B.\nMeyer, as a starting point, an experiment is described in\nwhich statistical pattern recognition algorithms are used to\ncharacterize a particular musical style with respect to other\nstyles. The resulting description can be used in authorship\ndiscussions. In the current study, a number of disputed or-\ngan works from the Bach catalog is used to illustrate the\npossibilities of this approach.\nKeywords: Musical Style, Pattern Recognition, Classical\nMusic, Composer Attribution, Johann Sebastian Bach.\n1. Introduction\nIn order to describe a musical style, or differences between\nstyles, or the historical development of certain styles, a the-\nory of style is necessary. This applies to “traditional” de-\nscriptions of musical style as well as studies in which tools\nand algorithms from information technology are used.\nIn[5],LeonardMeyerdevelopsatheoryofmusicalstyle\nthat can be used as starting point for studies in which sta-\ntistical pattern recognition algorithms are used to study and\ncompare musical styles. Meyer deﬁnes (musical) style as\nfollows:Style is a replication of patterning, whether in hu-\nmanbehaviororintheartifactsproducedbyhumanbehav-\nior, that results from a series of choices made within some\nset of constraints.\nWithout repeating patterns, there would be no style at\nall. The constraints are important for they shape a musi-\ncal style by allowing certain patterns and disallowing oth-\ners. Meyer distinguishes three levels in these constraints:\nlaws, rules and strategies. Laws are universal constraints,\ne.g., one cannot ask a piccolo to play a contra G. The sec-\nond level, the rulesare intracultural constraints. It is in the\nrules that music from the Renaissance differs from music\nfrom the Baroque. The third level, the strategies are con-\nstraintsthecomposersubjectshimselfto,withintherulesof\na certain cultural established style. Thus it is in the strate-\ngiesthatthemusicofG.F.Handeldiffersfromthemusicof\nG.Ph. Telemann. Strategies reside on conscious as well as\nPermissiontomakedigitalorhardcopiesofallorpartofthisworkfor\npersonal or classroom use is granted without fee provided that copies\narenotmadeordistributedforproﬁtorcommercialadvantageandthat\ncopies bear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2006 University of Victoriaon unconscious levels. Certain patterns are ingrained dur-\ning the training and development of a composer and are not\nreplicated consciously.\nIn the second part of his book, Meyer applies his theory\ntonineteenthcenturywesternclassicalmusic. Headdresses\nsomegeneralpatternsthatrecurinmanycompositionsfrom\nthat age and connects these patterns to the underlying ro-\nmantic esthetic and ideology. In doing so, he is forced to\nlimit himself to proof by example. For a more profound\nevaluation of musical styles, it would be necessary to make\nextensive use of all available data (everything in all consid-\nered scores). For achieving this, statistical pattern recogni-\ntionalgorithmscanbeofgreatuse. AsMeyerhimselfstates:\n“Since all classiﬁcation and all generalization about stylis-\ntic traits are based on some estimate of relative frequency,\nstatistics are inescapable.” [5], p. 64.\n2. A Pattern Recognition Approach\nMeyer’s theory offers a theoretical background for the de-\nsignofexperimentsinwhichalgorithmsfromstatisticalpat-\ntern recognition are used. The features that will represent\n(partsof)compositionscanbealliedwiththereplicatedpat-\nterns that are mentioned in Meyer’s deﬁnition. Assuming\nthatforacertainmusicologicalproblemthescoresinvolved\nare electronically available, a major task will be the extrac-\ntion of the feature values from those scores. From the per-\nspective of “traditional” style analysis, large-scale features\nare more interesting than small-scale features, e.g., in order\ntodeterminethewayinwhichacertaincompositionresem-\nblesasonata-form,aglobaloverviewoftheentirecomposi-\ntion is necessary. These, indeed, are the kind of features\nMeyer uses. From the perspective of algorithmic extrac-\ntion, small-scale features are more interesting, because the\nalgorithms to extract them are less complicated and the re-\nsults less ambiguous. It is, for example, not clear how to\nquantify the extent to which a composition resembles a cer-\ntain sonata-form, but it is much less difﬁcult to determine\nthe proportion of parallel thirds with respect to all inter-\nval successions in the composition. So we need small-scale\npatterns, which can be easily detected and counted, and of\nwhich we have many.\nWith this in mind, a set of twenty features is designed.\nThesmallestscaleinascoreisthatoftherelationofasingle\nnote to the other notes around it. Most features quantify as-\npects of local (note-level) relations between voices in poly-phonic compositions. Because we will use this represen-\ntation for studying authorship of organ fugues, the restric-\ntiontopolyphoniccompositionsisnotaproblem. Thereare\nalsosomeotherfeaturesintheset,thatdescribemoreglobal\ncharacteristics. Thefeaturesaredescribedin[1]. Herealist\nof them is provided:\n1. StabTimeslice 6. PitchEntropy 11. PartAugFourths 16. PartOctaves\n2. DissPart 7. VoiceDensity 12. PartDimFifths 17. ParThirds\n3. BeginBarDiss 8. PartSeconds 13. PartFifths 18. ParFourths\n4. SonorityEntropy 9. PartThirds 14. PartSixths 19. ParSixths\n5. HarmonyEntropy 10. PartFourths 15. PartSevenths 20. StepSuspension\nBy measuring all these features, (parts of) compositions\nare represented as vectors in a 20-dimensional space. To\nsuch a data set various kinds of pattern recognition algo-\nrithms can be applied.\n3. Organ Fugues ascribed to J.S. Bach\nAsapilotexperiment,adatasetisassembledwith16fugues\nfor organ that are listed in the catalog of compositions of\nJohann Sebastian Bach [7]. Of six of these fugues the au-\nthorship has been disputed. Also ﬁve fugues of his eldest\nson, Wilhelm Friedemann Bach, and eight of his most im-\nportantstudent,JohannLudwigKrebs,areincorporated. So\nwe have a three-class data set.1Each composition is seg-\nmentedusingasegmentingmethoddescribedin[1],soeach\ncomposition is represented by a “cloud” of points.\nTheFisher-transformation,describedin[8],p.145ff,can\nbe used to project the data points onto a two-dimensional\nspace in such a way that the classes are optimally sepa-\nrated. This projection, depicted in the background of ﬁg-\nure 1, shows that the compositions of each composer do\nform a cluster.\nFigure 1 indicates where the data points of the disputed\nfugues are projected. Some interesting observations can be\nmade. The F minor fugue BWV 534.ii, is projected among\nthe fugues of J.L. Krebs. This fugue has been ascribed to\nW.F. Bach [3]. With the current result, that ascription can\nbe rejected. An ascription to J.L. Krebs seems more likely.\nA suggested composer for BWV 536.ii is J.P. Kellner [4].\nIf this is true, Kellners style resembles more the style of\nJ.S.Bachthanthatoftheothertwocomposers. BWV537.ii\nis said to be composed partly by J.S. Bach (bar 1–40) and\npartly by J.L. Krebs [6]. The ﬁrst part is projected among\nthe works of J.S. Bach indeed. The second part however, is\noutside of both the Bach-region and the Krebs-region. The\nending of the fugue is in the region between J.S. Bach and\nKrebs. This does not fully support the hypothesis, but it\nshows that a large part of the fugue is not Bach-like. Also\nBach’sauthorshipofthefugueinCminor,BWV546.ii,has\nbeendoubted[2]. Thecurrentevaluationshowsusthat,with\nrespecttothestylesofW.F.BachandJ.L.Krebs,thisfugue\nhasthecharacteristicsofthestyleofJ.S.Bach. Thefuguein\n1The data set and the scores in humdrum encoding, are available from:\nhttp://www.musical-style-recognition.org .\nFigure 1. Projection of disputed fugues on top of the the com-\npositions of J.S. Bach (+), W.F. Bach (o) and J.L. Krebs (*).\nD minor, BWV 565.ii, the second part of the most famous\norgan work in existence, is not projected among the other\ncompositions of Bach. This conﬁrms the doubts expressed\nin [9].\n4. Conclusion\nBecause not all candidate composers are represented in the\ndata set, the current results don’t offer enough evidence to\ndrawconclusionsabouttheauthorshipoftheinvolvedcom-\npositions. It is, however, clear that the proposed method is\nvery helpful in ﬁnding hypotheses about differences in per-\nsonal styles and thus for studying authorship problems.\nReferences\n[1] E.BackerandP.vanKranenburg,“Onmusicalstylometry– –\na pattern recognition approach”, in Pattern Recognition Let-\nters, 26 (2005), 299–309.\n[2] W.Breig,“VersucheinerTheoriederBachschenOrgelfuge”,\ninDie Musikforschung 48 (1995), 14–52.\n[3] P. Dirksen, “Het auteurschap van Praeludium en fuga in f\n(BWV 534)”, in Het Orgel 96 (2000), nr. 5, 5–14.\n[4] D. Humphreys, “A Bach Polyglot– –the A major Prelude &\nFugue BWV 536”, in The Organ Yearbook XX (1989), 72–\n87.\n[5] L.B. Meyer, Style and Music– –Theory, History, and Ideol-\nogy, Chicago, 1989.\n[6] J. O’Donnell, “Mattheson, Krebs and the Fantasia & Fugue\nin C minor BWV 537”, in The Organ Yearbook XX (1989),\n88–95.\n[7] W. Schmieder, Thematisch-systematisches Verzeichnis der\nmusikalischen Werke von Johann Sebastian Bach. Bach-\nWerke-Verzeichnis– –2. ¨uberarbeitete und erweiterte Aus-\ngabe, Wiesbaden,21990.\n[8] A.Webb, StatisticalPatternRecognition ,Chichester,22002.\n[9] P. Williams, “BWV 565: a toccata in D minor for organ by\nJ. S. Bach?”, Early Music 9 (1981), 330–337."
    },
    {
        "title": "The Cyclic Beat Spectrum: Tempo-Related Audio Features for Time-Scale Invariant Audio Identification.",
        "author": [
            "Frank Kurth",
            "Thorsten Gehrmann",
            "Meinard Müller"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1414980",
        "url": "https://doi.org/10.5281/zenodo.1414980",
        "ee": "https://zenodo.org/records/1414980/files/KurthGM06.pdf",
        "abstract": "In this paper, we present a novel set of tempo-related au- dio features for applications in audio retrieval. As opposed to existing feature sets commonly used in the retrieval do- main which mainly focus on local spectral characteristics of the audio signal, our features capture its local temporal behaviour w.r.t. tempo, rhythm, and meter. As a key compo- nent to obtaining a high level of feature robustness we intro- duce the cyclic beat spectrum (CBS) consisting of residual tempo classes which are constructed similarly to the well- known pitch chroma classes. We illustrate the use of the newly constructed features by applying them to robust time- scale invariant audio identification. Keywords: Cyclic beat spectrum, tempo-related audio fea- tures, time-scale invariant audio identification",
        "zenodo_id": 1414980,
        "dblp_key": "conf/ismir/KurthGM06",
        "keywords": [
            "tempo-related audio features",
            "audio retrieval",
            "local temporal behaviour",
            "tempo",
            "rhythm",
            "meter",
            "cyclic beat spectrum",
            "pitch chroma classes",
            "robust time-scale invariant audio identification",
            "audio identification"
        ],
        "content": "The Cyclic Beat Spectrum: Tempo-Related Audio Features for Time-S cale\nInvariant Audio Identiﬁcation\nFrank Kurth Thorsten Gehrmann Meinard M ¨uller\nDepartment of Computer Science, University of Bonn\nR¨omerstraße 164, 53117 Bonn, Germany\n{frank,gehrmann,meinard }@cs.uni-bonn.de\nAbstract\nIn this paper, we present a novel set of tempo-related au-\ndio features for applications in audio retrieval. As oppose d\nto existing feature sets commonly used in the retrieval do-\nmain which mainly focus on local spectral characteristics\nof the audio signal, our features capture its local temporal\nbehaviour w.r.t. tempo, rhythm, and meter. As a key compo-\nnent to obtaining a high level of feature robustness we intro -\nduce the cyclic beat spectrum (CBS) consisting of residual\ntempo classes which are constructed similarly to the well-\nknown pitch chroma classes. We illustrate the use of the\nnewly constructed features by applying them to robust time-\nscale invariant audio identiﬁcation.\nKeywords: Cyclic beat spectrum, tempo-related audio fea-\ntures, time-scale invariant audio identiﬁcation\n1. Introduction\nRecent progress in the ﬁeld of audio retrieval has led to suc-\ncessful methods for solving retrieval tasks such as audio\nidentiﬁcation [1] and audio matching [2]. Consider an audio\ndatabase containing a collection of CD recordings. Whereas\naudio identiﬁcation aims at identifying a short excerpt (le t’s\nsay of about 10-30 seconds of duration) of audio as being\npart of a particular audio recording taken from a particu-\nlar CD, audio matching aims at automatically retrieving all\nmusically similar excerpts in all interpretations of the un der-\nlying pieces of music, which are contained in the database.\nThus, audio matching may in a sense be considered as a se-\nmantically advanced retrieval problem.\nExisting audio features used for audio retrieval are mainly\nspectral features (e.g., based on spectral ﬂatness, short- time\nFourier analysis, chroma analysis) capturing local spectr al\nor harmonic behaviour of a signal [3]. In some cases the\ntemporal progression of spectral features is incorporated ,\ne.g., by considering feature sequences [2, 4]. However, not\nall excerpts of music audio are suitably characterized by\ntheir harmonic contents only. Examples are excerpts with\nmonotonous harmonies or only slowly changing harmonic\nPermission to make digital or hard copies of all or part of this w ork for\npersonal or classroom use is granted without fee provided th at copies\nare not made or distributed for proﬁt or commercial advantage an d that\ncopies bear this notice and the full citation on the ﬁrst page .\nc/circlecopyrt2006 University of Victoriaprogressions, as well as non-harmonic excerpts such as per-\ncussive parts. As a consequence, the above cited retrieval\nmethods are not equally successful for all kinds of music.\nIn this paper, we suggest a novel set of audio features\nwhich are inspired by the musical parameters of tempo, me-\nter, and rhythm. The focus on those parameters is moti-\nvated by the fact that fragments of audio which do not ex-\nhibit a salient harmonic progression may be frequently bet-\nter characterized using local beat patterns. In our approac h,\nwe ﬁrst describe a method for extracting robust local tempo\nfeatures. As a key idea to achieve robustness we propose\nto combine techniques for tempo extraction with a modu-\nlar technique which reduces the estimated tempo in beats\nper minute (BPM) to an equivalence class of tempi. From\nthose tempo classes we construct the cyclic beat spectrum\n(CBS) which serves as a basis for subsequently designed lo-\ncal rhythm- and meter-related features.\nAs an application in the domain of audio retrieval, we\nconsider the robust identiﬁcation of time-scaled music au-\ndio. Such kind of time-scaling frequently occurs in broad-\ncast scenarios where the audio playback speed is varied in\norder to attract the listeners attention. It turns out that u sing\nthe proposed features, a robust audio identiﬁcation is stil l\npossible for scaling factors of ±20% and for severe addi-\ntional distortions of the audio signal such as lossy compres -\nsion, analog transmission, and noise addition. Hence, the\nproposed features can be used to substantially improve ex-\nisting methods for time-scale invariant audio identiﬁcati on.\nFurthermore, because of their semantic expressiveness, th e\nfeatures may be used in combination with existing harmony-\nbased feature sets to improve the performance in MIR tasks\nsuch as audio matching [2] and audio structure analysis [5].\nThe paper is organized as follows. Section 2 brieﬂy re-\nviews related work. In Section 3 we propose the CBS as\na robust tempo-related audio feature. Subsequently, the fo -\ncus of Section 4 is on the extraction of rhythm- and meter-\nrelated features. The application of the proposed features to\nthe robust identiﬁcation of time-scaled audio is described in\nSection 5.\n2. Related Work\nThere has been a signiﬁcant amount of research on extract-\ning the musical parameters of tempo, rhythm, and meter,\nsee [6] for an overview. Instead of focussing on extractingstrictly musically meaningful features, in this research w e\nfollow an approach previously proposed by Scheirer [7] to\nﬁrst derive basic tempo-related features. Those features a re\nsubsequently used to construct more robust features which\nare motivated by the notions of musical tempo, rhythm, and\nmeter, although they do not exactly correspond to their mu-\nsical relatives. In contrast to existing approaches, our me ter-\nand rhythm-related features are invariant w.r.t. time scaling\nof the underlying audio signal, which makes them particu-\nlarly useful for time-scale invariant audio identiﬁcation .\nA general overview on audio identiﬁcation techniques is\ngiven in [3]. The problem of audio identiﬁcation for broad-\ncast scenarios including time-scaled audio identiﬁcation for\nrelatively small scaling factors is investigated in [4]. Th e\nbasic approach to efﬁcient audio identiﬁcation used in this\npaper is described [8]. An extension of this approach to\ntime-scale invariant audio identiﬁcation is discussed in [ 9]\nwhere, however, the proposed features lack some robustness\nagainst signal distortions. In the Beat-ID system, ﬁnger-\nprints derived from a beat analysis are used for audio iden-\ntiﬁcation [10]. However, time-scaled audio material is not\nconsidered.\nRhythmic features have previously been proposed as a\nmeasure of musical similarity [11]. The approach is based\non using a global beat spectrum to compare two musical\npieces as a whole. In contrast, in this paper we use a kind of\nlocal beat spectrum to derive our CBS features.\nThe principle of making features robust by using resid-\nuals, which will be exploited later on, has been previously\nused to construct chroma features. Those pitch-related fea -\ntures are constructed by replacing all pitches of the well-\ntempered scale by 12 chroma classes each corresponding to\none of the 12 notes C,C#,... ,B [12]. Due to the identi-\nﬁcation of octaves, chroma features are robust to, e.g., var i-\nations in harmonics and timbre. Correspondingly, the pro-\nposed CBS features robustly represent certain tempo classe s.\n3. Robust Tempo-Related Features\nThe extraction of tempo-related features proceeds in two\nsteps. First, a tempo analysis of the music audio is per-\nformed using a comb ﬁlter bank. Then some post process-\ning results in a so called beat spectrogram which may be\ninterpreted as a time-tempo representation of the input sig -\nnal. Subsequently, for each time instant we calculate a CBS\nfrom which we extract local tempo classes.\n3.1. Tempo Analysis\nIn a prepocessing step, a lowpass ﬁlter with 7350 Hz cut-\noff frequency and downsampling to 14.7 kHz is applied to\nan input signal xin order to restrict the signal contents to\na frequency range covering the fundamental frequencies of\nwestern musical notes and to eliminate timbre information.\nA short time Fourier transform of step size 4.4 ms and a\nwindow size of M= 1024 samples is applied to generate0100200300400500Novelty\n Tempo [BPM]\nTime [Samples]0 1000 2000 3000 4000 5000 6000320 \n40  \n21.3\n14.5\n11  \n40 57 96 32022.533.54Beat spektrum for range 40 BPM − 320 BPMIntensity\nFigure 1. From top to bottom: Novelty curve of ﬁrst 30 seconds\ntaken from Baby one more time by Britney Spears, correspond-\ning beat spectrogram, and excerpt of the local beat spectrum\nfor sample position 3000 (corresponding to the boxed region).\na sequence X(1),X(2),...ofM-dimensional spectral vec-\ntors, i.e., X(t) = (X(t,0),... ,X (t,M−1)). To extract\nspectral changes, ﬁrst the positive novelty\nN[x](t) :=M/2−1/summationdisplay\nk=0max( |X(t+ 1,k)| − |X(t,k)|,0)\nis calculated. Using an approach similar to Scheirer [7], we\nthen apply a comb ﬁlter bank to N[x], where the ouput\nyp(t) := (1 −α)N[x](t) +αyp(t−p)\nof each recursive ﬁlter ypis parametrized by the resonance\nperiod pand a ﬁxed resonance factor α, which will be cho-\nsen as α= 0.5for the purpose of this paper. For a partic-\nular underlying sampling rate, each resonance period p(in\nsamples of the novelty curve) corresponds to a (reciprocal)\ntempo valueb(p)in beats per minute (BPM). Omitting tech-\nnical details, we choose the resonance periods to cover a\ntempo range of 40–320 BPM for the experiments discussed\nlater on. To emphasize resonance frequencies, we perform\na smoothing operation on each of the resonator ﬁlter bands.\nThis results in the beat spectrogram B=B[x], with\nB(t,p) :=r/summationdisplay\nτ=−r|yp(t+τ)|2\nrepresenting the amount of resonance for a resonance period\nofpsamples contained in a neighborhood of 2r+1samples\naround time position t. Here we choose r= 2300 such that\nsmoothing is performed in a window corresponding to 20\nseconds of duration. In what follows we will be interested \nG fundamental frequency (high)\nH harmonic\nS subharmonic\ng fundamental frequency (low)\nh harmonic\ns subharmonicG/h2g/S2\nh1\nh3s1\ns3 s2S1\nS3\nH1\nH3H2\nT octave T octave T octave T octave T octave T octave T octave T octave\ntempo\ntempo classesG\ng\n          \nFigure 2. Calculation of CBS: input signal (time domain) con-\nsisting of two periodic clicks (top), tempo analysis covering 8\ntempo octaves (center), summed CBS (bottom).\nSum of all tempo octaves\nG g \nSum \nT−Octave 8 T−Octave 1 \nFigure 3. The CBS (bold curve) is obtained by summing up\ntempo intensities of all eight tempo octaves.\nin values of the beat spectrogram for a ﬁxed time position\nt. Correspondingly, the columns B(t,·)ofBwill be called\n(local) beat spectrum at position t.\nFig. 1, from top to bottom, shows the novelty curve of\nthe ﬁrst 30 seconds taken from Baby one more time by Brit-\nney Spears, the beat spectrogram, and an excerpt of the lo-\ncal beat spectrum at position 3000. Note that the tempi are\ngiven in BPM. The sequence of beats in the underlying mu-\nsic may be observed as sequence of pulses in the novelty\ncurve, inducing resonances in the outputs of the comb ﬁlters\nwhich occur as bright rows in the beat spectrogram (second\ngraphic) and as peaks in the local beat spectrum (bottom\ngraphic).\n3.2. Cyclic Beat Spectrum\nA canonical approach to extract local tempi of a signal x\nat position tnow consists of determining the peak positions\nof the beat spectrum vector B(t,·)(see bottom graphic of\nFig. 1). Unfortunately, the beat spectrum not only empha-sizes the fundamental tempo but also the corresponding har-\nmonics, i.e., the 2-, 3-, 4-,. . . fold tempo, and subharmonic s,\ni.e., the 1/2-, 1/3-, 1/4-,. . . fold tempo, and therefore tem po\nconfusions are likely to occur. As an illustration, conside r\nFig. 2, showing an excerpt of a local beat spectrum of an\ninput signal consisting of two superimposed sequences of\nperiodic clicks with a ratio of 1:3 of the corresponding clic k-\nfrequencies Gandg. LetH1,H2,...andS1,S2,...denote\nthe harmonics and subharmonics of G, whereas h1,h2,...\nands1,s2,...denote the harmonics and subharmonics of g.\nClearly, in the local beat spectrum the fundamental of Gis\nsuperimposed with a harmonic of gwhereas the fundamen-\ntal of gcoincides with a subharmonic of G. Furthermore,\nthe subharmonics constitute some dominant peaks with am-\nplitudes close to those of the fundamentals. Hence, a simple\npeak picking approach is likely to confuse the real (funda-\nmental) tempi with the (sub-) harmonics’ tempi.\nTo avoid such kind of tempo confusion and hence make\nthe extracted local tempi more robust, we propose to iden-\ntifyfundamental tempo frequencies and their 2k-fold (sub-)\nharmonics, kan integer, using a concept similar to that of\nchroma classes in the pitch domain [12]. For this, we ﬁrst\nsubdivide the local beat spectrum into tempo octaves . In\nanalogy to musical octaves, where two notes are assigned\nthe same chromatic pitch if their frequencies f1,f2are re-\nlated by f1= 2kf2for an integer k, we partition the beat\nspectrum into tempo octaves by assigning two tempi ν1and\nν2the same tempo class , ifν1= 2kν2for an integer k.\nChoosing 10 BPM as a lower tempo limit, tempo octave\n1 covers the range of [10,20)BPM, tempo octave 2 cov-\ners[20,40)BPM, etc. For sake of illustration, we chose the\nbeat spectrum in Fig. 2 to cover the ﬁrst eight tempo octaves.\nNote that in this ﬁgure, the tempo axis is spaced logarithmi-\ncally, hence all of the octaves are of equal size.\nIn a second step, we add up all tempi corresponding to\nthe same tempo class. To be more precise, for a ﬁxed posi-\ntiont, assume that we start with a beat spectrum B(t,p1),\n. . . ,B(t,pA)calculated for Alogarithmically spaced tempi\nν1,... ,ν A, where for each k,pk=b−1(νk)is the reso-\nnance period corresponding to tempo νk. In particular, we\nﬁxν1at a certain tempo (in BPM) and let νi:= 2(i−1)/Lν1\nfor an integer LwithA=LK. Hence each tempo octave is\nsampled at Ltempi. The CBS /vector c(t) = (c(t,i))L\ni=1at position\ntis then calculated by summing over all tempo octaves:\nc(t,i) :=K−1/summationdisplay\nk=0B(t,pi+kL).\nIn our experiments, we selected ν1:= 40 ,L= 30 , andA=\n90, resulting in the above range of 40–320 BPM covering\nK= 3tempo octaves.\nAs for any tempo ν, all tempi of the form 2kνare iden-\ntiﬁed, the resulting spectrum may indeed by considered as\ncyclic. A visual representation accounting for this fact is1000 1200 1400 1600 1800 2000 2200 2400 2600 2800 TimeNovelty (i)\n1000 1200 1400 1600 1800 2000 2200 2400 2600 2800 TimeNovelty (ii)\n1000 1200 1400 1600 1800 2000 2200 2400 2600 2800 TimeNovelty (iii)\n1000 1200 1400 1600 1800 2000 2200 2400 2600 2800 TimeNovelty (iv)\nFigure 4. Criteria for local (i), global (ii), windowed (iii) signif-\nicance are combined to yield signiﬁcant maxima (iv).\nshown in Fig. 3 where the eight tempo octaves are plotted\nas closed curves which are stacked over each other. The\nCBS resulting from summing over all octaves is depicted as\na bold curve.\nA (cyclic) tempo estimate at position tof the novelty\ncurve may be then obtained from the CBS /vector c(t)by choos-\ningν′(t) := argmaxℓc(t,ℓ). Note that due to the modular\napproach, ν′(t)only corresponds to a class of tempi rather\nthan to a concrete tempo. To obtain explicit tempo values\nfor a position t, we choose the tempo octave in the range\nof[80,160) BPM as a representative and deﬁne ν(t) :=\n80·2ν′(t)/L. This choice is motivated by the observation\nthat the true musical tempo of many pieces lies in this par-\nticular BPM-range. The local beat period P(t)at position t\nmay then be deﬁned as the reciprocal value P(t) := 1 /ν(t).\nWe ﬁnally note that several local tempi may be estimated\nby considering the ﬁrst few signiﬁcant local maxima of the\nCBS.\n4. Rhythm and Meter Features\nWhereas tempo classes may be estimated for each sample\nposition of the novelty curve, rhythm- and meter-related fe a-\ntures are only estimated with respect to beat positions with in\nthe underlying audio. In this section, we ﬁrst describe a\nnovel method for the time-scale invariant extraction of bea t\npositions. We subsequently describe how time-scale invari -\nant rhythm- and meter-related features are extracted based\non those beat positions and the estimated local beat periods .\n4.1. Detection of Beat Positions\nSeveral methods for extracting beat positions from a sig-\nnals novelty curve Nhave been described in the literature.\nWe propose to combine the following three criteria to de-\ntectsigniﬁcant maxima ofNwhich we will then assume\nto correspond to beat positions. In particular, for each lo-\ncal maximum at position tofNwe calculate the maximum\nleft-sided and right-sided intervals\n1.[t−kℓ:t]and[t:t+kr], such that Nis strictly\nNoveltybeat period P(t)\n sig. sig.\n sig. sig. sig. sig.\n sig.\n sig.\n sig.\n sig. sig.\nt τ Time \nFigure 5. Extraction of rhythm feature R(t) =|τ−t|/P(t)at\nbeat position t.\nincreasing and decreasing resp. (local signiﬁcance),\n2.[t−κℓ:t]and[t:t+κr], such that N(t)is the global\nmaximum on both intervals (global signiﬁcance),\n3.[t−kℓ:t]and[t:t+kr], such that N(t)is the global\nmaximum of the novelty curve windowed by a trian-\ngular window centered at tand extending kℓsamples\nto the left and krsamples to the right (windowed sig-\nniﬁcance),\nresulting in six signiﬁcance values (kℓ,kr,κℓ,κr,kℓ,kr).\nFor a local maximum to be signiﬁcant, we require that each\nof its signiﬁcance values exceeds a particular fraction of a\nbeat period: First, to eliminate small local maxima resulti ng\nfrom noisy signal parts, we require the local signiﬁcance to\nexceed θ1:= 1/16of a beat period. To achieve a minimum\ninter onset interval (IOI), we furthermore require the glob al\nsigniﬁcance to exceed θ2:= 1/2of a beat period. Finally,\nto avoid signiﬁcance assignments to smaller peaks in noise-\nlike passages, we require the windowed signiﬁcance values.\nto exceed θ3:= 3/2of a beat period. The signiﬁcance val-\nues assigned to a maximum at position thence depend on\nthe beat period at t. In particular,\nm(t) := min/parenleftbiggkℓ\nθ1,kr\nθ1,κℓ\nθ2,κr\nθ2,kℓ\nθ3,kr\nθ3/parenrightbigg\nis the maximum beat period such that all three signiﬁcance\nrequirements are satisﬁed, and a maximum at tis considered\nas signiﬁcant if an only if m(t)exceeds the beat period P(t).\nThe upper three curves of Fig. 4 illustrate each of the\nthree individual criteria used for extracting signiﬁcant m ax-\nima. Extracted maxima according to those criteria are plot-\nted as vertical lines. The bottom plot shows the combined\ncriterion, where all of the three requirements are combined .\n4.2. Rhythm Features\nThe features proposed in the following are motivated by the\nmusical notion of rhythm, i.e., the relative durations of su b-\nsequent notes and pauses within a local neighborhood of a\npiece of music. As we do not extract pauses, the following\nfeature class is constructed using note- (or, more precisel y,\nbeat-) information only. Furthermore, as we use the posi-\ntive novelty curve, the detected signiﬁcant peaks do more\nlikely represent onsets of actual notes rather then note end s.Noveltybeat period P(t)  sig.\nt t+P(t)/4 t+P(t)/2 t+3P(t)/4 t + P(t) t+5P(t)/4 Time \nFigure 6. Extraction of a 6D meter feature at beat position t.\nThe basic idea behind constructing rhythm-based features\nnow consists of considering ratios of subsequent signiﬁcan t\nmaxima and local beat periods. Fig. 5 shows a novelty curve\nwith signiﬁcant maxima indicated by the label sig.To assign\na rhythm-like feature to a position tcontaining a signiﬁcant\nmaximum, we ﬁrst determine the position τof the next sig-\nniﬁcant maximum and let R(t) :=|τ−t|/P(t).\n4.3. Meter Features\nThe musical meter encodes the accentuation of successive\nnotes resp. beat positions. Although the succession of ac-\ncentuated and unaccentuated beats musically is of periodic\nnature, the local meter of an actual performance is generall y\nonly pseudo-periodic. To measure the local accentuation in\na neighborhood of a particular beat position t, we sample the\nnovelty curve around tusing a sampling interval of a quarter\nbeat period P(t):\nM(t) := ( N(t+round(kP(t)/4)))5\nk=0\ndeﬁnes a local 6D meter feature at position t, see Fig. 6,\nwhere the sampling positions kP(t)/4are rounded to the\nnext sample position of the novelty curve. Note that al-\nthough this choice of sampling positions seemingly favors\nrhythms related to quarters, our experiments show that the\nresulting features are meaningful also for other rhythm typ es.\nTo conclude this section we note that a ﬁnal postprocess-\ning step, where P(t),R(t), andM(t)are quantized to some\nsuitable sets of feature classes is performed as a preparatory\nstep for the subsequent index-based audio identiﬁcation. F or\nexample, instead of allowing a continuous range of meter\nfeatures, M(t)is quantized to a set of 32 meter classes.\n5. Robust Identiﬁcation of Time-Scaled Audio\nTo apply the proposed features to robust audio identiﬁcatio n\nwe ﬁrst summarize a previously proposed method for efﬁ-\ncient index-based audio identiﬁcation and its adaptation t o\ntime-scale invariant audio identiﬁcation [9]. We then appl y\nthe proposed features in an analogous fashion. Finally, we\ngive some test results of the resulting retrieval method.\n5.1. Robust Audio Identiﬁcation\nWe consider a database Dofnaudio signals (x1,... ,x n).\nUsing a suitable feature extractor F, each signal xiis pro-\ncessed to yield a feature set F[xi]consisting of pairs [t,f],Table 1. Identiﬁcation rates for differently time-scaled queries\nand top-5 rates (correct match is among the top 5 matches).\nScaling [%] 79 84 89 94 97\nID rate [%] 87 95 98 98 98\nTop-5 rate [%] 90 97 99 99 99\nScaling [%] 103 106 112 119 126\nID rate [%] 99 98 98 94 90\nTop-5 rate [%] 99 99 99 96 93\nwhere fdenotes a feature class and t∈Za sample po-\nsition. Then, [t,f]∈F[xi]means that a feature of class\nfis assigned to position tofxi. Note that we do not re-\nquire the assigned features to be spaced regularly on the\ntime axis. After feature extraction, we obtain the feature\nsetsF[D] := (F[x1],... ,F [xn]).\nTo identify a query signal q, the feature set F[q]is ex-\ntracted and the actual audio identiﬁcation is performed bas ed\non the feature sets F[D]. In particular, a match to a query\nqis given by a document ID iand a shift parameter Tsuch\nthatF[q] +T⊆F[xi], i.e., the T-shifted query features\nF[q] +T:={[t+T,f]|[t,f]∈F[q]}\ncoincide with features extracted from signal xi[8].\nAs time-scaled audio signals result in time-scaled feature\nsets, this approach is not suitable to identify time-scaled au-\ndio signals. To extend the approach to facilitate the iden-\ntiﬁcation of time-scaled audio, we introduce an additional\nfeature component sreﬂecting the time-scale of a particular\nfeature. Then, features are of the form [t,s,f]and a feature-\nbased match now is a document ID i, a shift parameter T,\nand a scaling parameter Ssuch that S·F[q] +T⊆F[xi],\nwhere S·F[q]+T:={[St+T,Ss+T,f]|[t,s,f]∈F[q]}\ndeﬁnes the set of time-scaled query features. Details on thi s\napproach and the resulting indexing techique for fast audio\nidentiﬁcation are beyond the scope of this paper, see [9].\n5.2. Audio Features\nTo apply the technique for audio identiﬁcation with last sec -\ntion’s features, we note that the beat period P(t)at posi-\ntiontactually changes linearly when the underlying signal\nis time-scaled and may be hence used as the local time-scale\nfeature component s. The rhythm and meter features R(t)\nandM(t)are time-scale invariant by construction and are\nthus used as the local feature class f. In summary, for each\nsignal xiofD, we construct the set of features\nF[xi] :={(t,P(t),[R(t),M(t)])|tbeat pos. of N[xi]}.\nUsing the same procedure for a query signal q, it is straight\nforward to use the above audio identiﬁcation technique.\n5.3. Test Results\nFor our tests we used a database of 100 audio pieces of var-\nious genres with a total duration of 7 hours of music, result-Table 2. ID rates for simultaneos time-scaling, lossy MPEG-\ncompression, and addition of noise.\nScaling [%] 84 89 94 100 106 112 119\nCoding [kbps] 32 64 128 - 128 64 32\nSNR [dB] 6 12 18 ∞ 18 12 6\nID rate [%] 81 92 97 100 95 91 82\nTop-5 rate [%] 87 95 98 100 97 94 87\nTable 3. Identiﬁcation rates for different signal degradations.\nType of Degradation ID rate [%]\nBackground Noise (SNR=18dB) 98\nBackground Noise (SNR=6dB) 92\nMPEG@128 kBit/s 100\nMPEG@32 kBit/s 96\nMicrophone recording (at 30cm) 91\nMicrophone recording (30cm, query len. 60s) 97\ning in about 50.000 features. To test the identiﬁcation capa -\nbilities, we generated 300 queries from those audio pieces.\nFor this, we chose three random excerpt of 30 seconds of\nduration from each audio, one from the beginning, one from\nthe middle, and one from the end of the audio. To test the\nrobustness of the audio identiﬁcation, the queries were ﬁrs t\nprocessed by various signal transformations and then used\nas an input to the above audio identiﬁcation method.\nTable 1 shows the robustness w.r.t. time scaling where\nthe audio signals are scaled from 79% – 126% of their orig-\ninal lengths. The ID rate indicates percentage of queries\nwhich are correctly identiﬁed by the ﬁrst (top-1-) match. In\nthis, retrieval results are ranked according to the percent age\nof query features matching a particular feature document.\nBelow, the percentage of correct matches among the top-5\nmatches are given. Note that the ratio of correct identiﬁca-\ntions is very high even for high scaling factors.\nTo test the identiﬁcation robustness for a combination of\ntime-scaling and signal degradations, we conducted exten-\nsive experiments where we considered degradations result-\ning from lossy compression, noise addition, time-stretchi ng,\nA/D conversion, and processing with various studio effects .\nTable 2 shows ID rates for varying scaling factors, varying\nMPEG-compression ratios and varying amounts of added\nnoise. Note that while the center colum refers to ID rates for\nthe undistorted audio, the degrees of signal degradation fo r\nall three types of distortion is choosen to increase simulta -\nneously. For example, the leftmost colum refers to ID rates\nfor a time-scaling to 84%, lossy compression with a bitrate\nof 32 kbit/sec, and additive noise at a SNR of 6 dB.\nTable 3 illustrates the robustness of the proposed method\nw.r.t. several signal distortions for the case of unscaled sig-\nnals. Although ID ratios are rather high, existing meth-\nods for audio identiﬁcation using local spectral features a re\nknown to provide better results for some of those cases.\nWe conclude that while the proposed features work well for\nidentifying even severely time-scaled audio, a suitable co m-\nbination with existing spectral feature sets should be used toaccount for both the scaled and unscaled cases.\n6. Conclusions\nIn this paper we proposed a new set of tempo-related au-\ndio features that capture short-time tempo-, rhythm- und\nmeter-characteristics of a piece of music audio. Robustnes s\nof the tempo features is obtained by reducing tempo esti-\nmates to certain modular tempo classes which are invariant\nw.r.t. tempo doubling, resulting in the concept of a cyclic\nbeat spectrum (CBS). We demonstrated how the proposed\nfeatures may be successfully applied to robust time-scale\ninvariant audio identiﬁcation. In this, we obtain a substan -\ntially improved identiﬁcation performance for highly time -\nscaled and distorted audio material. Future work will consi st\nof investigating how the proposed class of features may be\ncombined with chroma-based harmonic features [2] in or-\nder to extend existing audio matching techniques to broader\nclasses of music. Furthermore, we will investigate how the\nproposed feature types are related to their musical relativ es.\nReferences\n[1] Eric Allamanche, J ¨urgen Herre, Bernhard Fr ¨oba, and\nMarkus Cremer. AudioID: Towards Content-Based Identi-\nﬁcation of Audio Material. In Proc. 110th AES Convention,\nAmsterdam, NL , 2001.\n[2] Meinard M ¨uller, Frank Kurth, and Michael Clausen. Audio\nMatching via Chroma-based Statistical Features. In ISMIR,\nLondon, GB , 2005.\n[3] Pedro Cano, Eloi Battle, Ton Kalker, and Jaap Haitsma. A\nReview of Audio Fingerprinting. In Proc. 5. IEEE Workshop\non MMSP , St. Thomas, Virgin Islands, USA , 2002.\n[4] Pedro Cano, Eloi Battle, Harald Mayer, and Helmut\nNeuschmied. Robust Sound Modeling for Sound Identiﬁ-\ncation in Broadcast Audio. In Proc. 112th AES Convention,\nMunich, Germany , 2002.\n[5] Masataka Goto. A chorus-section detecting method for mu-\nsical audio signals. In Proc. ICASSP , pages 437–440, 2003.\n[6] M. Alonso, B. David, and G. Richard. A study of tempo\ntracking algorithms from polyphonic music signals. In 4-th\nCOST 276 Workshop, Bordeaux, France , 2003.\n[7] Eric D. Scheirer. Tempo and beat analysis of acoustic musi-\ncal signals. JASA , 103(1):588–601, 1998.\n[8] Michael Clausen and Frank Kurth. A Uniﬁed Approach to\nContent-Based and Fault Tolerant Music Recognition. IEEE\nTransactions on Multimedia , 6(5), October 2004.\n[9] Rolf Bardeli and Frank Kurth. Robust Identiﬁcation of Time-\nScaled Audio, 2004. Proceedings of the AES 25th Interna-\ntional Conference on Metadata for Audio, London, UK.\n[10] D. Kirovski and H. Attias. Beat-ID: Identifying Music via\nBeat Analysis. In Proc. 5. IEEE Workshop on MMSP , St.\nThomas, Virgin Islands, USA , 2002.\n[11] Jonathan Foote, Matthew D. Cooper, and Unjung Nam. Au-\ndio Retrieval by Rhythmic Similarity. In Proc. ISMIR, Paris ,\n2002.\n[12] Mark A. Bartsch and Gregory H. Wakeﬁeld. Audio thumb-\nnailing of popular music using chroma-based representa-\ntions. IEEE Trans. on Multimedia , 7(1):96–104, Feb. 2005."
    },
    {
        "title": "Data Dictionary: Metadata for Phonograph Records.",
        "author": [
            "Catherine Lai",
            "Ichiro Fujinaga"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1417423",
        "url": "https://doi.org/10.5281/zenodo.1417423",
        "ee": "https://zenodo.org/records/1417423/files/LaiF06.pdf",
        "abstract": "The creation and maintenance of a metadata data dictionary is essential to large-scale digital repositories. It assists the process of data entry, ensures consistency of records, facilitates semantic compatibility and interoperability between systems, and, most importantly, forms the foundation for efficient and effective information retrieval infrastructure. In this paper we explain in detail the necessity of metadata data dictionaries to digitization projects and digital library retrieval services. We also describe the development process of our Data Dictionary for phonograph records. We then present the underlying data model of our Data Dictionary and provide information about the meaning and use of semantic units defined in the Data Dictionary. We stress the usefulness of the generation and maintenance of our Data Dictionary for MIR as it provides a means to ensure accurate, consistent, and comprehensive metadata annotation. For maximum interoperability between systems, digital repositories not only need to agree on the same metadata fields, but also the meanings of the fields. To this end, we believe our Data Dictionary is the cornerstone of optimal retrieval of music information about phonograph records. Keywords: Metadata, Data Dictionary, Phonograph Records, Digitization, Standardization, Management",
        "zenodo_id": 1417423,
        "dblp_key": "conf/ismir/LaiF06",
        "keywords": [
            "Metadata",
            "Data Dictionary",
            "phonograph records",
            "digitization",
            "standardization",
            "management",
            "interoperability",
            "information retrieval",
            "semantic compatibility",
            "digital repositories"
        ],
        "content": "Data Dictionary: Metadata for Phonograph Records Catherine Lai Schulich School of Music McGill University Montreal, QC Canada H3A 1E3 lai@music.mcgill.ca Ichiro Fujinaga Schulich School of Music McGill University Montreal, QC Canada H3A 1E3 ich@music.mcgill.ca Abstract The creation and maintenance of a metadata data dictionary is essential to large-scale digital repositories. It assists the process of data entry, ensures consistency of records, facilitates semantic compatibility and interoperability between systems, and, most importantly, forms the foundation for efficient and effective information retrieval infrastructure.  In this paper we explain in detail the necessity of metadata data dictionaries to digitization projects and digital library retrieval services. We also describe the development process of our Data Dictionary for phonograph records. We then present the underlying data model of our Data Dictionary and provide information about the meaning and use of semantic units defined in the Data Dictionary. We stress the usefulness of the generation and maintenance of our Data Dictionary for MIR as it provides a means to ensure accurate, consistent, and comprehensive metadata annotation. For maximum interoperability between systems, digital repositories not only need to agree on the same metadata fields, but also the meanings of the fields. To this end, we believe our Data Dictionary is the cornerstone of optimal retrieval of music information about phonograph records. Keywords: Metadata, Data Dictionary, Phonograph Records, Digitization, Standardization, Management 1. Introduction Libraries have a history of managing large collections of information and using technology to carry out associated services such as delivering bibliographic information in the form of electronic records to library patrons. With the advent of the Internet, patrons’ expectations for access to information have increased dramatically. Libraries, archives, and cultural institutions worldwide are thus reinventing their roles in today’s networked society to meet patrons’ new demands.  As digitization technology, metadata standards, data management techniques, and digital preservation awareness have evolved and advanced, libraries, archives, cultural institutions, and other organizations have initiated digitization programs for preservation of their unique and precious analogue holdings, including analogue sound recordings such as 78 rpm and long-playing phonograph (LP) records. Development of digitization programs lowers the barriers to physical access. With the availability of digital collections, users can gain direct access to the digitized versions of artifacts as well as retrieve the metadata stored with the digital objects through automated library services such as online catalogues.  Metadata issues are central to discussions about the evolution of digital library information retrieval services. There are many definitions of metadata. In fact, the Task Force on Metadata compiled more than 25 different definitions of the term in the appendix section of its final report. The formal working definition of the term “metadata” by the Task Force is “metadata are structured, encoded data that describe characteristics of information-bearing entities to aid in the identification, discovery, assessment, and management of the described entities” [1].  Another phrase that is often heard in discussions of metadata is the term “metadata schema.” There are also many definitions of the term. The definition by Murtha Baca in her Introduction to Metadata edited for the Getty Research Institute defines metadata schema as “a set of rules for encoding information that supports specific communities of users” [2]. Metadata is indeed important in this digital era where digital objects need to have information attached to them simply in order to be found. However, if digital collections are to be effectively retrieved and shared among digital repositories, an agreement on adoption and use of standards needs to be established. Metadata that is randomly or arbitrarily added to a digital object without any overarching principles or established framework will lack interoperability with other resources. It will consequently be difficult to locate, and therefore be underused. The ultimate purpose of the implementation of this metadata Data Dictionary is to facilitate retrieval of digital collections and promote interoperability between different systems by defining the semantics (i.e., what each metadata element means) of the comprehensive list Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. © 2006 University of Victoria  \n of metadata elements developed for digitized representations of phonograph records. Metadata permits efficient and effective information retrieval only if digital repositories agree not only on the same metadata fields, but also the meanings of the fields. Our Data Dictionary clarifies the scope and type of metadata associated with the digital representations of the original recordings (e.g., label issue number vs. matrix number). It helps to prevent duplicate handling of data, inconsistencies, and lack of integrity during metadata entry (e.g., as part of the digitization process) and data management.  The next section describes the development of metadata in the context of the history of cataloguing, and shows why metadata and data dictionary have become so important in recent years for retrieval of digital objects. Section 3 then explains the need for a data dictionary in order to assist projects digitizing phonograph records and digital library information retrieval services. Section 4 describes the development process of our Data Dictionary. The paper then presents the underlying data model of our Data Dictionary and provides information about the meaning and use of semantic units defined in the Data Dictionary. 2. Historical Background 2.1 Library Catalogues The concepts and techniques of metadata creation have been around since the first library catalogue was created more than 2000 years ago. The first appearance of the term metadata dates back to the 1960s and became established in the context of database management systems in the 1970s. The first national cataloguing code, arranged by author entry, was traced back to the French code of 1791, which used catalogue cards and rules of accessioning and cataloguing. Various cataloguing rules were developed and published in different countries, for example, Sir Anthony Panizzi’s cataloguing rules for the British Museum Library in 1841 and Charles A. Cutter’s Rules of a Dictionary Catalog in the U.S.A. in 1876. Library associations in the two countries continuously worked to develop and improve cataloguing rules. In 1904, the American Library Association and the Library Association in the UK co-operated to produce an international cataloging code [3].  At the International Conference on Cataloguing Principles in Paris in 1961, international participants drafted twelve “Paris Principles” to draw a common basis for the assignment and form of access points (e.g., subject headings). The American and the British library associations cooperated again and published in 1967 the first edition of the Anglo-American Cataloguing Rules (AACR), based on the “Paris Principles.” As a means for the international exchange and sharing of bibliographic information, in 1974, the International Federation of Library Associations issued the International Standard Bibliographic Description (ISBD). The second edition of AACR, AACR2, published in 1978, incorporated ISBD and brought cataloguing of non-book materials into the mainstream [4]. Libraries worldwide have gradually come to adopt and use the interpretation and implementation of AACR2 for cataloguing of bibliographic records. The adoption of a common standard has enabled an authority control of bibliographic records and facilitated transparency in the exchange and sharing of catalogue records. The most recent revised edition of AACR2 was published in 2002 (AACR2r) [5]. 2.2 Electronic Catalogues Since the late 1960s, the sharing of catalogue records has become computerized. The fundamental tool that enables the exchanges of catalogues in computer-based systems is the MARC (Machine-Readable Cataloging) format developed by the Library of Congress. The availability of MARC records has benefited library patrons to a wider access of searchable catalogues. The implementations of AACR2 and MARC have made universal bibliographic control and interoperability between different systems possible and have since then provided an efficient method of retrieving items from library holdings. 2.3 Metadata Initiatives The idea of cataloguing the resources on the World Wide Web bourgeoned in the mid-1990s in response to the tremendous growth of the Internet. The idea became known as the Dublin Core Metadata Initiative (DCMI), which provides a minimum set of 15 metadata elements designed to describe document-like web objects to facilitate resource discovery [6]. DCMI is an ongoing initiative. Its goal of annotating metadata to help manage and retrieve electronic resources has led to a widespread use and application of metadata across different disciplines. Some of the most well-known metadata standards are EAD (Encoded Archival Description), TEI (Text Encoding Initiative), VRA (Visual Resource Association) Core, and MPEG-7 (Motion Picture Expert Group).  3. Rationale for the Data Dictionary A new set of metadata standards is necessary for describing digitized representations of phonograph records because traditional formats and standards for cataloguing sound recordings such as described in [7, 8, 9] are inadequate for the encoding and retrieval of digital representations of phonograph records. Specifically, the traditional practices of cataloguing sound recordings are generally limited to bibliographic description of relatively few elements [10]. Information about artwork or photographs in the album packaging of a LP, for example, is usually not included. A few recognized authorities have begun to contribute and expand the utility of metadata to sounds. IASA (International Association of Sound and  \n Audiovisual Archives), an association working to support the professional information exchange between audiovisual archives in all fields, provides cataloguing rules and guidelines on the production and preservation of digital audio objects. However, the cataloguing rules only cover descriptions of sound recordings in general [11]. The rules are not refined enough to provide a foundation for information retrieval of digital representations of phonograph records. MPEG-7, a formal system for describing multimedia content, defines elements for description of audio and video content [12]. However, MPEG-7 does not have characteristics tailored to the structural complexity that is necessary for the full description of sound recordings. For example, significant types of important information at the individual track or song level are missing. Unspecified information such as recording location, recording session date, recording engineers, or recording equipment used, if available, are potentially useful and convenient retrieval points. MPEG-7, instead, places most emphasis on low-level perceptual features of multimedia data which are meant to be extracted automatically and the data is typically not human-readable. We therefore, created a set of metadata elements specifically designed for phonograph records for the encoding of metadata as part of a large phonograph records digitization management system [13].  A metadata data dictionary, which provides semantic meanings to metadata elements, is perhaps even more important than the metadata element set. Unlike in the library community where cataloguers have well-established rules for data content description in the form of the AACR, the lack of a data dictionary can cause problems within and across digital collections. Digitizers may encode the same data element using different metadata elements, or they may encode different data elements using the same metadata element. As a result, digital repositories may not be able to combine or map data across systems because the definitions used are not consistent or identical. A data dictionary (similar to AACR2) that systematically defines the semantic meaning, descriptive requirement, and formatting principles of metadata is therefore necessary to create semantic compatibility (i.e., consistent assignment and form of content data to metadata entries), facilitate efficient interoperability between systems, and provide effective searching mechanism. 4. Development Process 4.1 Methodology Taking the set of metadata elements previously developed for phonograph records, we conducted a comparative study of existing data dictionaries of well-established metadata standards in closely related fields and used it as a guideline to assist in building our Data Dictionary. The data dictionaries studied included: • AACR2r • California Digital Library (CDL) for  administrative and structural metadata • CDWA for arts metadata • Dublin Core Metadata Initiative • EAD • IASA cataloging rules for digital audio objects  • MARC 21 • NISO Z39.87 for technical metadata for digital  still images • PREMIS for preservation metadata • Variations2 for musical work metadata • VRA for cultural work and images metadata  4.2 Implementation Considerations Depending on practicability and suitability, our Data Dictionary partially incorporates features from these established standards. Types of metadata and their functions sometimes overlap because different types of metadata do not always have well-defined boundaries. For example, the metadata “image bit depth” is categorized as administrative metadata in CDL and technical metadata in NISO. In our Data Dictionary, types and functions of metadata are classified based on practicality that best matches the structural complexity of digitized representations of phonograph records (i.e., the data model). We used the following guidelines in defining our Data Dictionary to maximize interoperability with the existing systems. 4.2.1 Flexibility The Data Dictionary should be constructed to handle fine metadata granularity and directly use or modify existing establishments whenever possible. 4.2.2 Extensibility The Data Dictionary should allow extensions to be developed later as necessary. The design should be flexible to support changing user needs and new technologies. 4.2.3 Effectiveness The creation and maintenance of the Data Dictionary should be as economical and efficient as possible without sacrificing the usability. 4.2.4 Openness The creation and ongoing management of the Data Dictionary should be as cooperative and inclusive as practicably possible. 4.2.5 Unambiguity The Data Dictionary should provide for consistent and unique interpretation and employ controlled vocabularies to increase accuracy, precision and recall of records.  \n 4.2.6 User Needs The Data Dictionary should be designed to meet the needs of users, and not based on ease of implementation. 5. Limits to the Scope of the Data Dictionary Metadata elements in this Data Dictionary focus solely on digitized representations of phonograph records of music. Sound recordings in other formats such as CDs, tapes, cylinders or phonograph records of speech or animal sound, which may include metadata elements such as species name, habitat, weather conditions, are beyond the scope of this dictionary.  Our Data Dictionary currently includes five types of metadata: description metadata to enable discovery and identification of resources; administration metadata to support management of resources; structure metadata to describe font and layout characteristics of texts and images; legal rights metadata to protect intellectual property rights; and technical information metadata to record the capture process and technical characteristics of digital objects. The metadata pertains to the recording itself as well as to the content of the recording.  6. Fields Reference Guide Each entry in our Data Dictionary offers these attributes of a semantic unit: 6.1 Field Name The field name is devised to be descriptive and unique within the Data Dictionary. This name is different from the display name (see the Label field below) presented on a computer display interface. 6.2 Definition The definition field provides semantic meaning of metadata elements with the intention of disambiguating metadata terms, thereby clarifying the conceptual intent of each metadata term. 6.3 Multiplicity The multiplicity field [14] indicates the possible number of data entries for each metadata element. This field combines two commonly used fields in other data dictionaries: requirement and repeatability. For example, the value “0…many” indicates that the metadata is not required and may be repeated, the value “0…1” indicates that the metadata is not required and may not be repeated, and the value “1” indicates that the metadata is required and may not be repeated. 6.4 Data Type The data type field indicates the format in which the data is to be entered into the system and indicates any vocabulary controls (e.g., Library of Congress Subject Headings) enforced. 6.5 Label Label refers to the contextual instance of a metadata element. This field dictates the way that a metadata element is presented on a computer display interface. The field provides an easy mechanism to change its display name to a variant name while still pertaining to the same semantic meaning. Although the goal of the data dictionary is standardization, this flexibility allows terminologies to be renamed so vocabularies that are special to the field appear natural and logical to users. 6.6 Provenance The provenance field pertains to the derivation history of a metadata element from its original source(s), including its unique identification. This information can help to validate and determine quality of the Data Dictionary. 6.7 Examples or Notes The examples or notes field suggests encoding guidelines by providing examples of usage and describing the nature and properties of the metadata element. 6.8 Data Constraints The data constraint field indicates the scope of metadata elements. Examples include constraints applied to content data at the collection level, artwork level, or track level. 6.9 Version Tracking The version-tracking field supports changes in data management, semantic evolution, and new technical requirements. The issued date, modified dates, replacement, or the deprecation of fields are provided when applicable. 7. Data Model In addition to describing individual metadata elements, we also defined the relationships among metadata elements in our Data Dictionary. The data model adopted in this design facilitates the management of the wide variety of objects (e.g., tracks, discs, performers) that comprise phonograph records. The metadata elements belong to one or more hierarchical classes: Collection, Album, Image, Disc, and Track [13].  8. The Data Dictionary Our Data Dictionary currently contains 103 description metadata, 4 administration metadata, 3 structure metadata, 11 legal rights metadata, and 52 technical information metadata, totaling more than 170 metadata elements. Due to space constraints, we show in this paper the metadata we created as well as a few examples of metadata that we incorporated from existing well-established standards.  8.1 Descriptive Metadata (examples) Field Name grooveCharacteristics Definition Give the groove characteristic of a  \n phonograph record if it is not standard Multiplicity 0…1 Data type String Label Groove Characteristics Provenance AACR2r Example or notes microgroove Data constraints Disc level Version tracking Issued on 2004-07-13  Field Name trackDuration Definition Give the duration of a track Multiplicity 0…1 Data type Record in the representations of ISO 8601: hh:mm:ss Label Track Duration Example or notes 00:02:59 Data constraints Track level Version tracking Issued on 2004-07-13  Field Name dateRecording Definition Give the date the recording was made Multiplicity 0…1 Data type Record in the representations of ISO 8601: YYYY-MM-DD, YYYY-MM, or YYYY Label Date of Recording Example or notes 1938-06-24 Data constraints Track level Version tracking Issued on 2004-07-13 Modified on 2005-02-03  Field Name trackPeculiarityNote Definition Make notes of any peculiarities associated with the track Multiplicity 0…M any Data type String Label Track Peculiarity Note Example or notes Track number typo: this track should be track 4, following track 3, but the track number is printed track 3 again on the disc label. Data constraints Track level Version tracking Issued on 2004-07-13 8.2 Administrative Metadata (examples) Field Name provenance Definition Specify the source the data is derived, for example, the disc label, a discography website, or legacy metadata from the library’s primary catalogue Multiplicity 0…M any Data type String Label Provenance of Data Example or notes MARC record from McGill Library’s MUSE. Data constraints  Version tracking Issued on 2005-02-03  Field Name provenanceComment Definition Give comments about the provenance of data Multiplicity 0…M any Data type String Label Comment about the Provenance of Data Example or notes The date information provided here may not necessarily be the date of recording. The event associated with this date was not explicitly stated on the MARC record. Version tracking Issued on 2005-02-03 8.3 Technical Information Metadata (examples) Field Name stylusDimension Definition Give the radius of the tip of the stylus Multiplicity 0…1 Data type String Label Stylus Dimension Example or notes 2.5 mil Data constraints Disc level Version tracking Issued on 2005-02-03  Field Name stylusShape Definition Give the shape of the stylus Multiplicity 0…1 Data type String Label Stylus Shape Example or notes elliptical Data constraints Disc level Version tracking Issued on 2005-02-03  Field Name trackingForce Definition Give the total force holding the stylus in place in the record groove Multiplicity 0…1 Data type String Label Tracking Force Example or notes 1.5 g Data constraints Disc level Version tracking Issued on 2005-02-03  Field Name Turnover Definition Give the equalizer settings used to accomplish the playback adjustment for bass turnover Multiplicity 0…1  \n Data type String Label Turnover Example or notes 500 Hz RIAA Data constraints Disc level Version tracking Issued on 2005-02-03  Field Name playbackSpeed Definition Give the playing speed of an analog disc in revolutions per minute (rpm). Multiplicity 1 Data type String Label Playback Speed Provenance AACR2r Example or notes 76.6-rpm Data constraints Disc level Version tracking Issued on 2004-07-13  Field Name imageProducer Definition Identify the organization-level producer(s) of the image. Multiplicity 1…M any Data type String Label Image Producer Provenance ANSI/NISO Z39.87 proposed draft, 2005 Example or notes Music Technology Area, Schulich School of Music of McGill University Version tracking Issued on 2004-07-13 Modified on 2005-02-03 9. Conclusion and Future Wor k The creation and maintenance of our Data Dictionary is important to MIR as it provides a means to ensure accurate, consistent, and comprehensive metadata annotation and promotes interoperability between different systems. Specifically, the Data Dictionary assists the process of data entry, ensures consistency of records, facilitates semantic compatibility and interoperability between systems and, most importantly, forms the foundation for an efficient and effective information retrieval infrastructure.  The Data Dictionary has been used and tested for McGill University’s audio preservation digitization pilot projects [15]. Our Data Dictionary, however, still needs to be used and evaluated in future phonograph records digitization projects. It is not intended to be fixed or final, but rather provides a starting point for improvements and enhancements to digital library services based on community experience and feedback.  The most immediate future research to be performed involves the implementation of the Data Dictionary in an XML-based format that facilitates the exchange of metadata information between digital repositories. References [1] Association for Library Collections & Technical Services, Committee on Cataloging: Description and Access, Task Force on Metadata, “Final Report,” pp. 16, [Web site] 2000, [2006 April 4], Available: http://www.libraries.psu.edu/tas/jca/ccda/tf-meta6.html [2] Bacca, M., ed., Introduction to Metadata, Los Angeles: Getty Research Institute, [Web site] 1998, [2006 April 4], Available: http://www.getty.edu/research/conducting_research/standards/intrometadata/ [3] D. Haynes, Metadata for information management and retrieval, London: Facet publishing, 2004.  [4] A. Taylor, The organization of information, Englewood, Colo.: Libraries Unlimited, 1999. [5] Joint Steering Committee for Revision of AACR, American Library Association, et al., Anglo-American cataloging rules, 2nd ed., 2002 revision, 2005 update. Chicago:  American Library Association; Ottawa: Canadian Library Association; London: Chartered Institute of Library and Information Professionals, 2005. [6] Dublin Core Metadata Initiative, [Web site] 2006, [2006 April 4], Available: http://dublincore.org/ [7] S. Mudge, and D. Hoek, “Describing jazz, blues, and popular 78 rpm sound recordings: Guidelines and suggestions,” Cataloging & Classification Quarterly, vol. 29, no. 3, pp. 21–48. 2000. [8] T. Simpkins, “Cataloging popular music recordings,” Cataloging & Classification Quarterly, vol. 31, no. 2, pp. 1–35. 2001. [9] R. Smiraglia, Describing music material, 3rd ed., Lake Crystal, MN.: Soldier Creek Press, 1997. [10] H. Hemmasi. “Why not MARC?” in Proceedings of the International Conference on Music Information Retrieval, 2002, pp. 242–248. [11] M. Miliano, and IASA. The IASA cataloguing rules: a manual for the description of sound recordings and related audiovisual media, IASA Cataloguing Rules Editorial Group, Eds. Stockholm: International Association of Sound and Audiovisual Archives, 1999. [12] R. Koenen, and F. Pereira, “MPEG-7: A standardised description of audiovisual content,” Signal Processing: Image Communication, vol. 16, no. 1, pp. 5–13, 2000. [13] C. Lai, I. Fujinaga, and C. Leive, “Metadata for phonograph records: Facilitating new forms of use and access to analog sound recordings,” in Proceedings of the Joint Conference on Digital Libraries, 2005, pp. 385. [14] Variations2 Version 1 Data Dictionary. [Web site] 2003, [2006 April 19], Available http://variations2.indiana.edu /pdf/DML-metadata-elements-v1.pdf [15] C. Lai, B. Li, and I. Fujinaga. “Preservation digitization of David Edelberg’s Handel LP collection: A pilot project,” in Proceedings of the International Conference on Music Information Retrieval, 2005, pp. 570–575."
    },
    {
        "title": "Everyday Life Music Information-Seeking Behaviour of Young Adults.",
        "author": [
            "Audrey Laplante",
            "J. Stephen Downie"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1417957",
        "url": "https://doi.org/10.5281/zenodo.1417957",
        "ee": "https://zenodo.org/records/1417957/files/LaplanteD06.pdf",
        "abstract": "This poster presents the preliminary results of an ongoing qualitative study on the everyday-life music information- seeking behaviour of young adults. The data were collected through in-depth interviews and analyzed following a grounded theory approach. The analysis showed a strong penchant for informal channels (e.g., friends, relative) and, conversely, a distrust of experts. It also emerged that music seeking was mostly motivated by curiosity rather than by actual information needs, which in turn explains why browsing is such a popular strategy. Keywords: user studies, music information behaviour",
        "zenodo_id": 1417957,
        "dblp_key": "conf/ismir/LaplanteD06",
        "keywords": [
            "qualitative study",
            "everyday-life music information-seeking",
            "young adults",
            "in-depth interviews",
            "grounded theory approach",
            "informal channels",
            "distrust of experts",
            "curiosity",
            "browsing",
            "music seeking"
        ],
        "content": "Everyday Life Music Information-Seeking Behaviour of Young Adults\nAudrey Laplante J. Stephen Downie \nMcGill University University of Illinois at Urbana-Champaign \nGraduate School of Library and Information Science Graduate School of Library and  Information Studies \naudrey.laplante@mcgill.ca jdownie@uiuc.edu \n \nAbstract \nThis poster presents the preliminary results of an ongoing \nqualitative study on the everyday-life music information-seeking behaviour of young adults. The data were collected through in-depth interviews and analyzed following a grounded theory approach. The analysis showed a strong penchant for informal channels (e.g., friends, relative) and, conversely, a distrust of experts. It also emerged that music seeking was mostly motivated by curiosity rather than by actual information needs, which in turn explains why browsing is such a popular strategy.   \nKeywords : user studies, music information behaviour \n1. Introduction \nThis poster presents the preliminary results of a grounded theory \n[1] qualitative study on the everyday-life music \ninformation-seeking behaviour of young adults (18-29 years old) of the French-speaking Montreal Metropolitan community. The aim of the study is to contribute to a better understanding of real-life music needs by questioning people about the strategies they currently employ to discover new music. Unlike Lee and Downie \n[2] \nwho carried out a large scale su rvey that allowed them to \nidentify what  people do to find music, the objective of this \nstudy is to understand how and why they use various music \nseeking strategies. This study is in line with the HUMIRS project \n[3] whose aim is to provide a rich understanding of \nreal-world music queries. \n2. Data Collection and Method \nThe data were collected through face-to-face in-depth \ninterviews. The interviewing a pproach was chosen because \nit allows us to capture things that cannot be observed directly such as the perceptions, feelings and motives participants have in relation to their music information behaviour. A guide, based on a revised version of \nWilson’s 1996 conceptual model of information behaviour \n[4], was developed to give structure of the interviews and \nallow comparison between participants. It is composed of questions on music information behaviour in the context of everyday life, which means that work- or school-related experiences are not considered. \nThis study is designed to run from March 2006 to July \n2006 with the expectation of 20 interviews being conducted. This is a reasonable sample size for a qualitative research which typi cally “produce[s] a wealth \nof detailed data about a much smaller number of people and cases” \n[5] than does quantitative research. The \nmaximum variation sampling strategy as defined in [6] \nwas used to optimize the diversity in the sample. The participants were recruited by distributing flyers in the hallway of the Grande Bibliothèque, a large public library located in downtown Montréal.  \nThe present paper represents the preliminary findings \nderived from 242 minutes (38,150 words) of the interviews that have been transcribed a nd then analyzed with N6 by \nQSR, a software package de signed specifically for the \nencoding and analyses of qualitative textual data.  \n3. Emergent Themes and Implications \nThe grounded theory [1] approach was adopted to analyze \nthe data. According to this inductive approach, the researcher analyzes the data without any preconceptions, attempting to generate theory from  the data. To achieve \nthat, the constant comparative method \n[7] was used. \nNames have been altered to protect confidentiality. \n3.1 Informal Channels/Distrust of Experts  \nIn all but one case, participants reported that informal channels such as friends, colleagues or relatives were important sources of information. The main reason mentioned for this was two fold: 1) those people know their tastes so they can provide personalized and relevant recommendations; and, 2) as they know the tastes of those people, it is possible for them to tell if they can trust their recommendations or not. Hence, comparing a formal source to his friends, Antoine says: “[On allmusic ,] they \ndon’t write for you, knowing your tastes. It is intended for everyone. Hence, it is a cons iderably less reliable source.” \nSimilar critiques were addressed to library or record store \nstaff, although the possibility to  see and discuss with the \nperson seemed to help assess the reliability of this person \nas a source of music information. Eric mentioned that if a staff member “looks like” someone who could be a fan of the type of music he is looking for, he might ask for \nPermission to make digital or hard copi es of all or part of this work for \npersonal or classroom use is grante d without fee provided that copies \nare not made or distributed for profit or commercial advantage and that \ncopies bear this notice and the full citation on the first page. \n© 2006 University of Victoria advice, whereas Juliette sometimes has a little chat with a \nstaff member to “test” his/her level of knowledge and see if it is worth asking for advice. \nThis finding corresponds to what [2] found, which is \nthat most people prefer to turn to friends or relatives than to formal sources when seeking everyday music information. However, all participants also acknowledged that their acquaintances are limited sources of information and that formal sources are sometimes indispensable, for \ninstance when they are looking for music none of their friends/family is interested in or are more knowledgeable then they are about. In those cases, Internet is the most \npopular source, more specifically music-related websites \nsuch as allmusic , the music section of MySpace.com , \nartists and labels official websites.    \nImplications: Considering the popularity of persons \nwith close ties as sources of music information and what \nparticipants had to say about the advantages and limits of such persons as sources, MIR systems that enhance social-network/peer-to-peer recommendation exchanges in conjunction with access to outsi de web resources are best \npositioned to find broad-based acceptance and usage. \n3.2 Music Information Seeking as a Non-Goal \nOriented Activity/Browsing over Searching \nInterestingly, when asked to recall a recent situation in which they had searched for music with a specific goal in \nmind, most participants could not recall any and those who could said that the situation they had described was not typical for them. The reason they all gave was that searching for music is something they do regularly (on a daily or weekly basis), just because it is an activity they \nenjoy doing. For example, Julie tte confessed that she feels \n“excited” when going to her favorite second-hand record store. Indeed, it is mostly the pleasure they take in the activity itself that motivates them to seek for music rather than an actual information need. This is similar to what \nToms found in a study on electronic newspaper readers: \n“There was no ‘need,’ no anomalous state of knowledge and no knowledge gap evident. This was simply an information gathering experience without expectations or predicted outcome.” \n[8] As a logical consequence, all \nparticipants reported spending “a lot of” or “too much” time searching for music. Most admitted that they are sometimes so absorbed when they browse for music that \nthey have problem stopping. François compared it to playing “video games” and Antoine confessed that he was spending so much time on that at some point that he was afraid he would never finish his master degree. \nRelated to that, browsing, which facilitates \nserendipitous discoveries and thus seems best suited for non-goal oriented information seeking, was a very common and well-liked strategy among the participants. It can take several forms depending on the medium used. In public libraries or record stores, it consists of browsing the stacks, sometimes systematically, sometimes in a chaotic way, looking at CD covers −which appears to be an \nimportant factor in music selection −and hoping to discover \na completely unknown artist that will become a favourite. The appeal of the unexpected discovery is so strong for Eric that from time to time he simply closes his eyes and picks any (or “almost any”) CD at hand.  \nOn the Web, browsing is done through hyperlinks, \nwhich allow one to navigate from one artist/album to another in different ways. Hence the popularity of music sites, such as allmusic , who link each artist with similar \nartists, artists that have influenced this artist, artists with whom this artist has worked with, etc. Most participants reported that they had discovered one or several of their favourite artists this way.  \nImplications: The capacity of a MI R system to capture \nand maintain the attention of its users is an important factor in its success. Encouraging serendipitous discoveries, for instance by offering a variety  of browsing \nfacilities, could be one way to achieve this. Knowing that the mere pleasure of discovery or information gathering is a primary motivation for users, suggests that future research efforts might pay bigger dividends by putting less effort into building “perfect  search” algorithms and more \neffort into developing “dis covery” or “novelty-biased” \nMIR systems. \n4. Acknowledgments \nThis material is based upon work supported by the National Science Foundation under Grant No. 0328471. \nReferences \n[1] B. G. Glaser and A. L. Stra uss, The discovery of grounded \ntheory: strategies for qualitative research. Chicago: Aldine \nPublishing Company, 1967. \n[2] J. H. Lee and J. S. Downie, \"Survey of music information \nneeds, uses, and seeking beha viours: preliminary findings,\" \npresented at 5th International Conference on Music Information Retrieval, Barcelona, Spain, 2004. \n[3] J. S. Downie, \"The creati on of music query documents: \nframework and implications of the HUMIRS Project,\" presented at Proceedings of ACH/ALLC 2004., Göteborg, Sweden, 2004. \n[4] T. D. Wilson and C. Walsh,  \"Information behaviour: an \ninter-disciplinary perspective,\" British Library Research & Innovation Centre, London British Library Research and Innovation Report 10, 1996. \n[5] M. Q. Patton, Qualitative research and evaluation methods, 3rd ed. Thousand Oaks, CA: Sage Publications, 2002, p. 227. \n[6] Y. S. Lincoln and E. G. Guba , Naturalistic inquiry. Beverly \nHills, CA: Sage Publications, 1985, p. 201. \n[7] P. Maykut and R. More house, Beginning qualitative \nresearch: a philosophic and practical guide. London; Philadelphia: Routledge, Flamer, 1994. \n[8] E. G. Toms, \"What motivates the browser?,\" in Exploring \nthe contexts of information behaviour. London: Taylor Graham Publishing, 1999, p. 202"
    },
    {
        "title": "Factors Affecting Response Rates for Real-Life MIR Queries.",
        "author": [
            "Jin Ha Lee 0001",
            "M. Cameron Jones",
            "J. Stephen Downie"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1416840",
        "url": "https://doi.org/10.5281/zenodo.1416840",
        "ee": "https://zenodo.org/records/1416840/files/LeeJD06.pdf",
        "abstract": "In this poster we present preliminary findings of an exploratory study of natural language music information queries posted to the Google Answers web site. We discuss the proportion of queries answered as a function of time and attempt to identify factors which affect the probability of a query being answered. Keywords: HUMIRS, Google Answers, queries, users.",
        "zenodo_id": 1416840,
        "dblp_key": "conf/ismir/LeeJD06",
        "keywords": [
            "natural language music",
            "exploratory study",
            "Google Answers",
            "proportion of queries",
            "time",
            "factors affecting",
            "probability of a query",
            "Google Answers",
            "queries",
            "users"
        ],
        "content": "Factors Affecting Response Rates for Real-Life MIR Queries \nJin Ha Lee M. Cameron Jones J. Stephen Downie \nGraduate School of Library and Information Science \nUniversity of Illinois at Urbana-Champaign \n{jinlee1,mjones2,jdownie}@uiuc.edu \n \nAbstract \nIn this poster we present preliminary findings of an \nexploratory study of natural language music information \nqueries posted to the Google Answers web site. We discuss the proportion of queries answered as a function of time \nand attempt to identify factors which affect the probability \nof a query being answered. \nKeywords : HUMIRS, Google Answers, queries, users. \n1. Introduction \nThis poster reports on research conducted as part of the \nHuman Use of Music Information Retrieval Systems (HUMIRS) project [1]. The primary goal of the HUMIRS \nproject is the acquisition and analysis of real-life user data \nso that an empirically justifiable framework can be developed for future Music Information Retrieval \nEvaluation eXchange (MIREX) tasks. \nWe examined a collection of real-life music-related \nqueries from the Google Answers website. Prior studies of queries in the Google Answers system focused on \nidentifying information needs and information features \nused in the queries [2], [3]. The goal of this study is to improve our understanding of the factors related to a query \nbeing answered. First, we examine how the proportion of \nqueries answered varies over time. Then we compare selected features of answered and unanswered queries, \nnamely the price offered for the answer and the length of \nthe query, in order to understa nd if these variables affect \nthe probability of a query being answered. \n2. Data and Analysis \n2,208 queries were collected from Google Answers’ music \ncategory on April 27, 2005. The following features were extracted from the queries: (1) if the query was answered \nor not, (2) the elapsed time in minutes between when the \nquery was posted and when it was answered, (3) the number of content words in the query (i.e., stop words \nremoved), and (4) the price offered for the answer. \nIn addition to descriptive st atistics, we use logistic \nregression to measure the effects of price and query length on the probability of a query being answered. A logistic \nregression model describes how the probability of a query \nbeing answered depends on the explanatory variables [4]. \n2.1 Response Rate and Time: If Not Now… Never! \nOf the 2,208 queries examined in this study, 1,062 were \nanswered and 1,146 were unanswered. The overall probability of a query being answ ered in this collection is \napproximately 0.481. Figures 1 and 2 show the proportion \nof queries answered over time. The increase in the \nproportion drops rapidly over time. More than half of all answered queries (51.69%) are answered within two hours \nof being posted and 83.71% w ithin the first 24 hours.  \n \n \nFigure 1.  The cumulative proportion of queries answered \nover one month. \n \nFigure 2. The cumulative proportion of queries answered \nover the first 24 hours. Permission to make digital or hard co pies of all or part of this work for \npersonal or classroom us e is granted without fee provided that copies \nare not made or distributed for profit  or commercial advantage and that \ncopies bear this notice and the full citation on the first page. \n© 2006 University of Victoria 2.2 Can an Answer be Bought? \nThe difference in the mean price offered between answered \nand unanswered queries is very small ( Δµ = 0.097), \nsuggesting that there may be no significant relationship between price and a query’s probability of being answered. \nEquation (1) gives the logistic regression model for price. \n \nlogit X000.0 079.0 )( +−=π  (1) \nWe test if the probability of a query being answered is \nindependent of X (price) by the likelihood-ratio test [4]. \nOur H 0 is that β=0 (price has no effect on the probability of \na query being answered). A well-fitting model is significant at the .05 level or better, meaning the model is \nsignificantly different from the one with the constant only. \nThe likelihood-ratio test confirms that the query price has no effect on the probability  of getting an answer \n(likelihood-ratio test statistic = 0.013; df = 1; p = 0.908). \nFurthermore, there is a very weak correlation between the amount of money offered for a question and the amount of \ntime taken to produce an answer (Pearson’s r = 0.125). \n2.3 Is More Information Better? \nThe mean query length (number of content words) of \nanswered queries was 4.553 words shorter than that of \nunanswered queries, suggesting that query length may \nhave a negative effect on the probability of a query being answered. We test our H\n0: β=0 (query length has no effect \non the probability of a query be ing answered). Equation (2) \ngives a logistic regression model for query length.  \n logit X006.0 099.0)( −=π  (2) \nThe likelihood-ratio test shows that the word count has \na statistically significant effect  on the probability of getting \nan answer (likelihood-ratio test statistic = 14.342; df = 1;  \np = 0.000). However, the substantive difference is very \nsmall as can be visually confirmed in Figure 3. Adding a single content word to a query decreases the odds of the query being answered by a multiplicative factor of 0.994, \nwith lower and upper confidence bounds of 0.991 and \n0.997, respectively. \n3. Discussion \nThe reason for price not having a significant effect may be \nthat some questions are either impossible or difficult to \nanswer given the query statement, regardless of the price. \nUsers may also need more words to adequately describe their information needs for difficult questions. This may \nexplain the weak but negative effect of query length on the \nprobability of a query being answered. Another possible explanation is that as users provide more information they \nmay also increase the chance that they are providing \nincorrect information. This can be problematic in formulating search statements, especially for artist or work \nidentification queries which comprise a majority of the \nqueries in the system [3], where a single incorrect feature used in a Boolean search statement can result in search failure [5]. \n4. Conclusion and Future Work \nContrary to our intuitions that offering more money for an \nanswer or providing more information in a query will \npositively affect the probab ility of that query being \nanswered, our results show that price has no effect and adding more words has a negative effect, although \nminimal, on the likelihood of a query being answered. \nIn future research, we will conduct a qualitative analysis \nof the queries to determine which of the answered queries were answered correctly and identify reasons why some \nqueries were never answered. We will also assess and \ncompare the level of accuracy of user provided information features for answered queries. \n5. Acknowledgements \nWe thank the Andrew W. Mellon Foundation and the \nNational Science Foundation (Grant No. NSF IIS-\n0327371) for their support. \nReferences \n[1] J. S. Downie, “The scientific evaluation of music \ninformation retrieval systems: Foundations and future,” \nComputer Music Journal , vol. 28, no. 2, 2004, pp. 12-23. \n[2] D. Bainbridge, S. J. Cunningham, and J. S. Downie. “How \npeople describe their music information needs: A grounded \ntheory analysis of music queries,” in ISMIR 2003 4th Int. \nConf. on Music Inf. Retr. Proc ., 2003, pp. 221-222. \n[3] J. H. Lee, J. S. Downie, and S. J. Cunningham. “Challenges \nin cross-cultural/multilingual music information seeking,” \nin ISMIR 2005 6th Int. Conf. on Music Inf. Retr. Proc ., \n2005, pp. 1-7. \n[4] A. Agresti, Categorical Data Analysis , 2nd ed., New York: \nWiley, 2002. \n[5] B. Allen, “Recall cues in known-item retrieval,” Journal of \nAmerican Society for Information Science , vol. 40, no. 4, \n1989, pp. 246-252.             \n \nFigure 3. Frequency distribution of query length for \nanswered and unanswered queries."
    },
    {
        "title": "Automatic Chord Recognition from Audio Using a HMM with Supervised Learning.",
        "author": [
            "Kyogu Lee",
            "Malcolm Slaney"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1415158",
        "url": "https://doi.org/10.5281/zenodo.1415158",
        "ee": "https://zenodo.org/records/1415158/files/LeeS06.pdf",
        "abstract": "In this paper, we propose a novel method for obtaining la- beled training data to estimate the parameters in a super- vised learning model for automatic chord recognition. To this end, we perform harmonic analysis on symbolic data to generate label files. In parallel, we generate audio data from the same symbolic data, which are then provided to a machine learning algorithm along with label files to esti- mate model parameters. Experimental results show higher performance in frame-level chord recognition than the pre- vious approaches. Keywords: Chord recognition, hidden Markov model, su- pervised learning",
        "zenodo_id": 1415158,
        "dblp_key": "conf/ismir/LeeS06",
        "keywords": [
            "harmonic analysis",
            "symbolic data",
            "label files",
            "machine learning algorithm",
            "supervised learning",
            "frame-level chord recognition",
            "hidden Markov model",
            "experimental results",
            "higher performance",
            "previous approaches"
        ],
        "content": "Automatic Chord RecognitionfromAudio Using anHMM withSup ervised\nLearning\nKyoguLee\nCenterforComputerResearchinMusicandAcoustics\nDepartmentofMusic,StanfordUniversity\nkglee@ccrma.stanford.eduMalcolmSlaney\nYahoo! Research\nSunnyvale,CA94089\nmalcolm@ieee.org\nAbstract\nIn this paper, we propose a novel method for obtaining la-\nbeled training data to estimate the parameters in a super-\nvised learning model for automatic chord recognition. To\nthis end, we perform harmonic analysis on symbolic data\nto generate label ﬁles. In parallel, we generate audio data\nfrom the same symbolic data, which are then provided to\na machine learning algorithm along with label ﬁles to esti-\nmate model parameters. Experimental results show higher\nperformance in frame-level chord recognition than the pre-\nviousapproaches.\nKeywords: Chord recognition, hidden Markov model, su-\npervisedlearning\n1. Introduction\nA musical chord is a set of simultaneoustones. Succession\nof chordsover time, or chord progression, form the core of\nharmony in a piece of music. Hence analyzing the overall\nharmonic structure of a musical piece often starts with la-\nbelingeverychord. Automaticchordlabelingisveryuseful\nforthosewhowant todoharmonicanalysisofmusic. Once\nthe harmonic content of a piece is known, a sequence of\nchordscan be used forfurtherhigher-levelstructuralanal y-\nsiswherephrasesorformscanbedeﬁned. Chordsequences\nare also a good mid-level representation of musical signals\nfor such applications as music search, music segmentation,\nmusicsimilarityidentiﬁcation,andaudiothumbnailing. F or\nthese reasons and others, automatic chord recognition has\nrecently attracted a number of researchers in the Music In-\nformationRetrievalﬁeld.\nHiddenMarkovmodels(HMMs) are verysuccessful for\nspeechrecognition,andgiganticdatabaseswithlabelsacc u-\nmulated over decades play an important role in estimating\nthe model parameters appropriately. However, there is no\nsuch database available for music. Furthermore, the acous-\ntical variance in a piece of music is even greater than that\nin speech in terms of its frequency range, instrumentation,\nPermission to makedigital orhardcopies ofallorpartofthi s workfor\npersonal or classroom use is granted without fee provided th at copies\narenotmadeordistributed forproﬁtorcommercial advantag e andthat\ncopies bear this notice and thefull citation on the ﬁrstpage .\nc/circlecopyrt2006 University of Victoriadynamics,orduration,andthusalot moredataisneededto\ntrainthemodelsforgeneralization.\nHand-labelingthechordboundariesinanumberofrecord-\nings is an extremely time consuming and tedious task. In\nthis paper, we propose a method of automating this daunt-\ningtasktoprovidethemodelswithlabeledtrainingdata. To\nthis end, we use symbolic data such as MIDI data to gener-\nate chord names and boundaries as well as to create audio.\nAudio and chord boundary information generated this way\nare in perfect sync, and we can use them to estimate the\nmodelparameters.\nThere are several advantages to this approach. First, we\ndo not need to manually annotate chord boundaries with\nchordnamesto obtaintrainingdata. Second,we can gener-\nateasmuchdataasneededwiththesamenotesbutdifferent\nmusical attributes by just changinginstrumentation,temp o,\nor dynamics when synthesizing audio. This helps avoid\noverﬁttingthemodelstoaspeciﬁctypeofmusic. Third,suf-\nﬁcient training data enable us to include more chord types\nsuchas7th,augmented,ordiminished.\nThis paper continues with a review of related work in\nSection2;inSection3,wedescribehowweextractthefea-\nture vectors, and explain the model and the method of ob-\ntaining the labeled training data; in Section 4, we present\nempiricalresultsfollowedbydiscussions,anddrawconclu -\nsionsinSection5.\n2. RelatedWork\nSheh and Ellis proposed a statistical learning method for\nchordsegmentationandrecognition[1]. Theyused the hid-\nden Markov models (HMMs) trained by the Expectation\nMaximization (EM) algorithm, and treated the chord labels\nas hidden values within the EM framework. In training the\nmodels,theyusedonlythechordsequenceasaninputtothe\nmodels, and applied the forward-backwardor Baum-Welch\nalgorithm to estimate the model parameters. The frame ac-\ncuracyinpercenttheyobtainedwasabout76%forsegmen-\ntationandabout22%forrecognition,respectively. Thepoo r\nperformancefor recognitionmay be due to insufﬁcient and\nunlabeledtrainingdata comparedwitha largeset of classes\n(20songsfor147chordtypes).\nBello and Pickens also used HMMs with the EM algo-\nrithm [2]. They incorporated musical knowledge into the\nmodels by deﬁning a state transition matrix based on thekey distance in a circle of ﬁfths, and avoided random ini-\ntialization of a mean vector and a covariance matrix of ob-\nservation distribution, which was modeled by a single mul-\ntivariate Gaussian [2]. In addition, in training the model’ s\nparameter, they selectively update the parameters of inter -\nest on the assumption that a chord template or distribution\nisalmostuniversal,thusdisallowingadjustmentofdistri bu-\ntionparameters. Theaccuracythusobtainedwasabout75%\nusing beat-synchronous segmentation with a smaller set of\nchordtypes(24major/minortriadsonly). Inparticular,th ey\narguedthattheaccuracyincreasedbyasmuchas32%when\nthe adjustment of the observationdistribution parameters is\nprohibited. Again, this may be because they not only used\nan unsupervised model, but also the training data were in-\nsufﬁcienttoappropriatelyestimatethemodelparameters.\nOur approach is based on the work of Sheh and Ellis or\nBello and Pickens in that the states in the HMM represent\nchord types, and try to ﬁnd the optimal path, i.e., chord se-\nquenceinamaximumlikelihoodsense. Themostprominent\ndifference in our approach is, however, that we use labeled\ntrainingdatabywhichmodelparameterscanbedirectlyes-\ntimated. Furthermore, we propose a method of automati-\ncallyobtainingthelabeledtrainingdata,whichremovesth e\nproblematicandtimeconsumingtaskofmanualannotation.\n3. System\nOur system starts with extracting suitable feature vectors\nfromtherawaudio. Likemostchordrecognitionsystems,a\nchromavectorora PCP vectorisusedasthefeaturevector.\n3.1. ChromaFeatures\nA chromagram or a Pitch Class Proﬁle (PCP) is the choice\nof the feature set in automatic chord recognitionor key ex-\ntraction since introduced by Fujishima [3]. Perception of\nmusicalpitchinvolvestwodimensions– heightandchroma.\nPitchheightmovesverticallyinoctavestellingwhichocta ve\na note belongs to. On the other hand, chroma tells where it\nstandsinrelationtootherswithinanoctave. Achromagram\nor a pitch class proﬁle is a 12-dimensionalvector represen-\ntation of a chroma, which represents the relative intensity\nin each of twelve semitones in a chromatic scale. Since a\nchordiscomposedofaset oftones,anditslabelisonlyde-\nterminedbythepositionofthosetonesinachroma,regard-\nless of their heights, chroma vectors appear to be an ideal\nfeatureto representamusical chordora musicalkey.\nFujishima developed a realtime chord recognition sys-\ntem, where he derived a 12-dimensional pitch class proﬁle\nfrom the DFT of the audio signal, and performed pattern\nmatchingusingthebinarychordtypetemplates[3]. Gomez\nand Herrera proposed a system that automatically extracts\nfrom audio recordings tonal metadata such as chord, key,\nscale and cadence information [4]. They used as the fea-\nture vector, a Harmonic Pitch Class Proﬁle (HPCP), which\nis based on Fujishima’s PCP, and correlated it with a chordorkeymodeladaptedfromKrumhansl’scognitivestudy[5].\nSimilarly,Pauwsusedthe maximum-keyproﬁlecorrelation\nalgorithm to extract key from the raw audio data, where\nhe averaged the chromagram features over variable-length\nfragments at various locations, and correlate them with the\n24 major/minor key proﬁle vectors derived by Krumhansl\nand Kessler [6]. Harte and Sandler used a 36-bin chroma-\ngram to ﬁnd the tuning value of the input audio using the\ndistribution of peak positions, and then derived a 12-bin,\nsemitone-quantized chromagram to be correlated with the\nbinarychordtemplates[7].\nTherearesomevariationswhencomputinga12-binchro-\nmagram,butitusuallyfollowsthefollowingsteps. First,t he\nDFToftheinputsignal X(k)iscomputed,andtheconstant-\nQ transform XCQis calculated from X(k), using a loga-\nrithmically spaced frequencies to reﬂect the frequency res -\nolution of the human ear [8]. The frequency resolution of\ntheconstant-Qtransformfollowsthatoftheequal-tempere d\nscale,whichisalsologarithmicallybased,andthe kthspec-\ntral componentis deﬁnedas\nfk= (21/B)kfmin, (1)\nwhere fkvaries from fminto an upper frequency, both\nof which are set by the user, and Bis the number of bins\nin an octave in the constant Q transform. Once XCQ(k)is\ncomputed,achromagramvector CHcanbeeasilyobtained\nas:\nCH(b) =M−1/summationdisplay\nm=0/vextendsingle/vextendsingleXCQ(b+mB)/vextendsingle/vextendsingle, (2)\nwhere b= 1,2,· · ·, Bis the chromagrambin index,and\nMisthenumberofoctavesspannedintheconstantQspec-\ntrum. For chord recognition, only B= 12is needed, but\nB= 24orB= 36is alsoused.\nIn our system, we used as feature vectors 12-bin Quan-\ntizedchromagramproposedbyHarteandSandler[7],which\ncompensatesa possible mistuning present in the recordings\nbyreallocatingthepeaksbasedonthe peakdistribution.\n3.2. HiddenMarkovModel\nA hidden Markov model [9] is an extension of a discrete\nMarkov model, in which the states are hiddenin the sense\nthat an underlyingstochastic processis not directlyobser v-\nable,butcanonlybeobservedthroughanothersetofstochas -\ntic processes.\nWe recognize chords using a 36-state HMM. Each state\nrepresents a single chord, and the observation distributio n\nis modeled by a single multivariate Gaussian in 12 dimen-\nsions deﬁned by its mean vector µiand covariance matrix\nΣi, where idenotes ith state. We assume the features are\nuncorrelated with each other, and thus use diagonal covari-\nance matrix. State transitions obey the ﬁrst-order Markovproperty; i.e.,thefutureisindependentofthe pastgiventhe\npresentstate. Inaddition,weuseanergodicmodelsincewe\nalloweverypossibletransitionfromchordtochord,andyet\nthetransitionprobabilitiesarelearned.\nOnce the model parameters – initial state probabilities,\nstate transition probabilities, and mean vector and covari -\nance matrix for each state – are learned, the Viterbi algo-\nrithm is applied to the model to ﬁnd the optimal path, i.e.,\nchord sequence, in a maximum likelihood sense given an\ninputsignal.\nIn our model, we have used 36 classes or chord types –\nmajor,minor,anddiminishedtriadsforeachpitchclass. We\ntreated major and dominant seventh chords as belonging to\nmajortriads,minorseventhstominortriads,anddiminishe d\nsevenths to diminished triads. We found this class size ap-\npropriateinasensethatitliesbetweenoverﬁttingandover -\nsimpliﬁcation.\n3.3. LabeledTraining Data\nInordertotrainasupervisedmodel,weneedlabelﬁleswith\nannotatedchordboundaries. Toautomatethislaboriouspro -\ncess, we use symbolicdata to generate label ﬁles as well as\naudio data. To this end, we ﬁrst convert a symbolic ﬁle to\na format which can be used as an input to a chord analysis\ntool. Chord analyzer then performs harmonic analysis and\noutputs a ﬁle with root information and note names from\nwhich complete chord information (i.e., root and its sonor-\nity -major,minor,ordiminishedtriad/seventh)isextract ed.\nSequenceofchordsareusedasground-truthorlabelswhen\ntraining the HMM. In parallel, we use the same symbolic\nﬁles to generate audio ﬁles using a sample-based synthe-\nsizer. Audio data generatedthis way are in sync with chord\nlabelﬁlesobtainedabove,andareenharmonicallyrichasin\nrealacousticrecordings. Figure1illustratestheovervie wof\nthesystem.\n4. Implementationand Experiments\nAs shown in Figure 1, our system for generating labeled\ntrainingdata hastwo main blocksrunningin parallel. First ,\nharmonic analysis is performedon symbolic data. We used\nsymbolic ﬁles in Humdrum data format. Humdrum is a\ngeneral-purposesoftwaresystemintendedtohelpmusicre-\nsearchers encode, manipulate, and output a wide variety of\nmusically-pertinent representations.1For harmonic anal-\nysis , we used the Melisma Music Analyzer developed by\nSleatorandTemperley.2TheMelismaMusicAnalyzertakes\na piece of music represented by an event list, and extracts\nmusical information from it such as meter, phrase struc-\nture, harmony, pitch-spelling, and key. By combining har-\nmony and key information extracted by the analysis pro-\ngram, a complete Roman-numeral analysis is performed,\n1http://dactyl.som.ohio-state.edu/Humdrum/\n2http://www.link.cs.cmu.edu/music-analysis/(.lab)Label(MIDI)\nChord analysis MIDI synthesis\ntime:             0   1.5  3.2   6.0 ...chord name: C   G   D7   Em ...Symbolic data\nAudio\n(.wav)\nChroma analysis\n12−bin chroma features\nHMMTraining\nFigure1. Overview of thesystem.\nfrom which we can generate label ﬁles with sequence of\nchordnames.\nThe analysisprogramwas tested on a corpusof excerpts\nand the 48 fugue subjects from the Well-Tempered Clavier ,\nand the harmony analysis and the key extraction yield the\naccuracyof83.7%and87.4%,respectively[10].\nIn the near feature extractionblock in our system, MIDI\nﬁlesare synthesizedusingTimidity++usinga GUS (Glavis\nUltraSound)instrumentpatch. Timidity++isafreesoftwar e\nsynthesizer, and converts MIDI ﬁles into audio ﬁles in a\nWAVEformat.3Itusesasample-basedsynthesistechnique\ntogenerateenharmonicallyrichaudiowithupperpartialsa s\nin real acoustic recordings. The raw audio is downsampled\nto11025Hz,and12-binchromafeaturesareextractedfrom\nit with the frame size of 8192 samples and the hop size of\n2048samples. Thechromavectorsarethenusedasinputto\ntheHMM alongwiththe labelﬁlesobtainedabove.\nAsatrainingdataset,weused175ﬁlesofHaydn’sString\nQuartets in a Humdrumdata format at the Center for Com-\nputer Assisted Research in the Humanitiesat StanfordUni-\nversity.4These ﬁles were convertedto a format which can\nbeusedintheMelismaMusicAnalyzeraswellastoaMIDI\nformatusing the toolsdevelopedby Craig Sapp.5We used\nacoustic piano samples to generate audio. The audio data\nsynthesized from these MIDI ﬁles is about 7.5 hours long,\nandcontainsabout145,000framesintotal.\nFigure2showsatransitionprobabilitiesmatrixandtran-\n3http://timidity.sourceforge.net/\n4http://www.ccarh.org/\n5http://extras.humdrum.net/sition probabilities for C major chord estimated from the\ntrainingdataset. Itcanbeobservedthatthetransitionmat rix\nis strongly diagonal since chord duration is usually longer\nthantheframelength,andthusthestatedoesnotchangefor\nseveralframes. However,chordprogressionbasedonmusic\ntheory can also be found in transition probabilities, for ex -\nample, in the case of C major chord. As mentioned, it has\nthe largest probability of staying within the same state, i.e.,\nwithin C major chord, but has comparablyhigher probabil-\nities for making a transition to speciﬁc chordslike F major,\nG major, or D minor chord than to others, as shown in the\nright ﬁgure. F major and G major have ﬁfth-tonic relation-\nships with C major, and transitions between them happen\nvery often in Western tonal music. C major chord is also\na dominant chord of F minor, and therefore a C major to F\nminortransitionisfrequentaswell.\nTransition probabilities matrix\n510 15 20 25 30 35CC#DD#EFF#GG#AA#BCmC#mDmD#mEmFmF#mGmG#mAmA#mBmCdimC#dimDdimD#dimEdimFdimF#dimGdimG#dimAdimA#dimBdim\n0 510 15 20 25 30 3500.10.20.30.40.50.60.70.8Transition probabilities for C major chord\nF maj\nG maj F min\nFigure 2. 36x36 transition probabilities matrix and transi tion\nprobabilities for C major chord. Axes are numbered in the\norder of major, minor,and diminishedchords.\nFigure 3 exempliﬁes the observation distribution param-\netersestimatedfromthetrainingdataforCmajorchord. On\nthe left is the mean chroma vector for C major chord. It is\nobvious that it has three largest peaks at chord tones or at\nC, E, and G, as expected. In addition, we can also see rel-\natively large peaks at D and B, which come from the third\nharmonics of G and E, respectively. Covariance matrix for\nCmajorchordisalsoconsistentwithwhatisexpectedfrom\nthe music theoretical knowledge. Chord tones or C, E, and\nGarestronglycorrelatedwiththemselveswhereasverylow\ncorrelationwasfoundwith D#,F#, orG#.\nCC#DD#EFF#GG#AA#B0.040.060.080.10.120.140.160.18Mean chroma vector for C major chord\nPitch classCovariance matrix for C major chord\nPitch classPitch class\nCC#DD#EFF#GG#AA#BC\nC#\nD\nD#\nE\nF\nF#\nG\nG#\nA\nA#\nB\nFigure 3. Estimated mean chroma vector and covariance ma-\ntrixfor C major chord\n4.1. Empirical Results\nWe tested our model on the actual recordingof Bach’s Pre-\nlude in C major performed by Glenn Gould. It is approx-\nimately 140 seconds long, and contains 753 frames. Testdata ﬁrst goes through the chroma analysis which outputs\n12-binquantizedchromafeaturevectors. Thesefeaturevec -\ntors are then fed into the trained HMM. Recognition is ac-\ncomplished as the Viterbi algorithm ﬁnds the optimal path\ngiven the model parameters and the input observation vec-\ntors. We compared the output of the model, which is a se-\nquence of frame-level chord names, with the hand-marked\nground-truthtomakescoresforframerateaccuracy.\nIn computing scores, we only counted exact matches as\ncorrect recognition. We tolerated the errors at the chord\nboundaries by having some time margins of a few frames\naround the boundaries. This assumption is fair since the\nground-truthwasgeneratedbyhumanbylisteningtoapiece,\nwhichcan’tberazorsharp. Figure4showsarecognitionex-\namplefromthetest data.\nAs can be seen in Figure 4, estimated chord boundaries\nare very closely aligned with the ground-truth boundaries.\nFurthermore,allchordnamesarealso correctlyrecognized .\nAsmentionedinSection3.2,dominantseventhchordswere\nrecognized as their root triads, which we treated as correct\nrecognition. The overall frame-level accuracy was about\n93.35%.\nExceptforsomesporadicerrors,mostconsistenterrorsin\nthetestdatacamefromtheconfusionbetweenAminorsev-\nenthchordandCmajorchord. Aminorseventhiscomposed\nof four notes – A, C, E, and G – in which C, E, and G are\nalsochordtonesofCmajortriad. SincewetreatedAminor\ntriad and A minor seventh as one class, it is highly likely\nthat A minor seventh is misrecognized as C major triad in\nthepresenceofaG note,whichwasthecase.\nFurthermore,theprecedingchordwasCmajortriad,and\nthus it is a most likely decision for the system to stay in\nthe same state of C major chord rather than jumping to an-\nother state unless there is a large change in the observation\nvectors. We expect that the system will be less sensitive to\nthissortofconfusionifweincreasetheclasssizetoinclud e\nseventhchordsandtrainourmodelonmoredata.\nIt is hardto directlycompareperformanceof oursystem\nwithpreviousworksinceweareusingdifferenttypeofmu-\nsic for testing as well as for training. But we believe our\nhigh performance, when training on synthetic pieces and\ntesting on live recordings, will only get better as we add\nmorepiecestoourtrainingcollectionandaddadditionalin -\nstrumentations.\n5. Conclusion\nThe main contribution of this work is the automatic gener-\nation of labeled training data for a machine learning model\nfor automatic chord recognition. By using the chord labels\nwithexplicitsegmentationinformation,wedirectlyestim ate\nthemodelparametersinanHMM.\nIn order to accomplish this goal, we have used symbolic\ndata to generate label ﬁles as well as to create audio ﬁles.\nThe rationale behind this idea was that it is far easier andPitch Class12−bin Chromagram\n0 5 10 15 20 25C C#D D#E F F#G G#A A#B \n0 5 10 15 20 25C:maj D:min7 G:dom7 C:maj A:min D:dom7 G:majtrue\n0 5 10 15 20 25C:maj D:min G:dom C:maj A:min D:dom G:maj\nTime (s)recog0.050.10.150.20.250.30.350.4\nFigure 4. 12-bin chromagram of an excerpt from Bach’s Prelude in C Major (BWV 846) performed by Glenn Gould. At the bottom\nchord labels with boundaries can be observed: “true” corres ponds to the ground-truth annotation, and “recog” correspo nds to the\nsystem output.\nmore robust to perform harmonic analysis on the symbolic\ndata than on the raw audio data since symbolic ﬁles such\nas MIDI ﬁles contain noise-free pitch information. In addi-\ntion, by using a sample-based synthesizer, we could create\naudio ﬁles which have enharmonically rich spectrum as in\nrealrecordings.\nAs feature vectors, we used 12-bin tuned chroma vec-\ntors which have been successfully used by others for the\nchord recognition application. We have deﬁned 36 classes\nor chord types in our model, which include for each pitch\nclass three distinct sonorities – major, minor, and dimin-\nished. Wetreatedseventhchordsastheircorrespondingroo t\ntriads, and disregarded augmented chords since they very\nrarelyappearintonalmusic.\nAfterthemodelparameterswereestimatedfromthetrain-\ning data, unseen test input of real recording was fed to the\nmodel,andtheViterbialgorithmwasappliedtoﬁndthebest\nprobable state path, i.e., chord sequence, at the frame rate.\nExperimentsshowedverypromisingresults.\nIn this paper, we trained our model only on piano mu-\nsic, and tested with piano music. In the near futurewe plan\nto includemore trainingdatawith differentinstrumentati on\nand genre to make our system more general to all kinds of\nmusic.\n6. Acknowledgments\nThe authors would like to thank Craig Sapp and Jonathan\nBergerforfruitfuldiscussionsandsuggestionsregarding this\nresearch.References\n[1] A. Sheh and D. P. Ellis, “Chord segmentation and recogni-\ntion using EM-trained hidden Markov models,” in Proceed-\nings of the International Symposium on Music Information\nRetrieval,Baltimore, MD,2003.\n[2] J. P. Bello and J. Pickens, “A robust mid-level represent a-\ntion for harmonic content in music signals,” in Proceedings\nof the International Symposium on Music Information Re-\ntrieval, London, UK, 2005.\n[3] T.Fujishima,“Realtimechordrecognitionofmusicalso und:\nAsystemusingCommonLispMusic,”in Proceedings ofthe\nInternational Computer Music Conference . Beijing: Inter-\nnational Computer Music Association, 1999.\n[4] E. Gomez and P. Herrera, “Automatic extraction of tonal\nmetadatafrompolyphonicaudiorecordings,”in Proceedings\nof the Audio Engineering Society . London: Audio Engi-\nneering Society, 2004.\n[5] C. L. Krumhansl, Cognitive Foundations of Musical Pitch .\nOxford UniversityPress,1990.\n[6] S. Pauws, “Musical key extraction from audio,” in Proceed-\nings of the International Symposium on Music Information\nRetrieval,Barcelona, Spain,2004.\n[7] C. A. Harte and M. B.Sandler, “Automatic chord identiﬁca -\ntion using a quantised chromagram,” in Proceedings of the\nAudioEngineering Society . Spain: Audio EngineeringSo-\nciety, 2005.\n[8] J. C. Brown, “Calculation of a constant-Q spectral trans -\nform,”Journal oftheAcousticalSociety ofAmerica ,vol.89,\nno. 1, pp. 425–434, 1990.\n[9] L. R. Rabiner, “A tutorial on hidden Markov models and se-\nlected applications in speech recognition,” Proceedings of\nthe IEEE,vol. 77, no. 2, pp. 257–286, 1989.\n[10] D. Temperley, The cognition of basic musical structures .\nThe MIT Press,2001."
    },
    {
        "title": "A Genre Classification Plug-in for Data Collection.",
        "author": [
            "Tue Lehn-Schiøler",
            "Jerónimo Arenas-García",
            "Kaare Brandt Petersen",
            "Lars Kai Hansen"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1416202",
        "url": "https://doi.org/10.5281/zenodo.1416202",
        "ee": "https://zenodo.org/records/1416202/files/Lehn-SchiolerAPH06.pdf",
        "abstract": "This demonstration illustrates how the methods developed in the MIR community can be used to provide real-time feedback to music users. By creating a genre classifier plug- in for a popular media player we present users with rele- vant information as they play their songs. The plug-in can furthermore be used as a data collection platform. After informed consent from a selected set of users the plug-in will report on music consumption behavior back to a central server. Keywords: Genre classification, media player, plug-in, data collection",
        "zenodo_id": 1416202,
        "dblp_key": "conf/ismir/Lehn-SchiolerAPH06",
        "keywords": [
            "Genre classification",
            "media player",
            "plug-in",
            "data collection",
            "real-time feedback",
            "music users",
            "relevance",
            "informed consent",
            "central server",
            "music consumption behavior"
        ],
        "content": "A Genre Classiﬁcation Plug-in for Data Collection\nTue Lehn-Schiøler, Jer ´onimo Arenas-Garc ´ıa, Kaare Brandt Petersen, Lars Kai Hansen\nThe Technical University of Denmark\nInformatics and Mathematical Modelling\nRichard Petersens Plads Bld 321\nKgs. Lyngby DK 2800\n{tls,jag,kbp,lkh }@imm.dtu.dk\nAbstract\nThis demonstration illustrates how the methods developed\nin the MIR community can be used to provide real-time\nfeedback to music users. By creating a genre classiﬁer plug-\nin for a popular media player we present users with rele-\nvant information as they play their songs. The plug-in can\nfurthermore be used as a data collection platform. After\ninformed consent from a selected set of users the plug-in\nwill report on music consumption behavior back to a central\nserver.\nKeywords: Genre classiﬁcation, media player, plug-in, data\ncollection\n1. Media Player Plugins\nTo create a plug-in that gathers data from its users it is im-\nportant to obtain user trust and collaboration. Hence, the\nplug-in needs to be seamlessly integrated with the music\nplayer, and furthermore the user should ﬁnd the plug-in at-\ntractive to use. In this demonstration we propose on-line\ngenre classiﬁcation and later advanced music based search\nfunctionality as added value. The perspective is that if our\nusers like the player and the plug-in we can collect informa-\ntion about their music related behaviors. This way, a large\ncollection of songs and related data can be collected; this i n-\ncludes meta data, usage information and content based fea-\ntures. Once sufﬁcient data has been collected, the plug-in\ncan be extended to serve as a search engine; along with the\nonline genre classiﬁcation new songs can be recommended\ngiving further incentive to use the plug-in.\nMany existing media players support external plug-ins,\ne.g., for programming of visualizations of the music, most\nclassical visualizations for plug-ins are based on a short t ime\nFourier transform of the signal. However, in some media\nplayers it is possible to get access to the original sound str eam\nthus making it possible to exploit the content based tech-\nniques developed in the MIR community to create visual-\nizations. Such methods have been described among others\nPermission to make digital or hard copies of all or part of this w ork for\npersonal or classroom use is granted without fee provided th at copies\nare not made or distributed for proﬁt or commercial advantage an d that\ncopies bear this notice and the full citation on the ﬁrst page .\nc/circlecopyrt2006 University of Victoria\nFigure 1. Screen-shot of the real-time genre classiﬁer.\nby [2, 4, 7, 8].\n2. Genre Prediction System\nIn this application we make use of aggregated features as\ndescribed in [5]. On a 20 ms scale with 10 ms overlap seven\nMFCC’s are extracted and the ﬁrst one discarded. These\nfeatures are collected during a frame of one second thus\ncreating a six dimensional time series with 100 samples.\nFor each second the time series is modeled by a multivari-\nate autoregressive (MAR) process with three lags: xk=/summationtext3\nl=1Alxk−l+ek. The values of the three A-matrixes, the\nmean and the covariance of the residuals ek(3·62+ 6 +\n(62+ 6)/2 = 135 ) are ﬁnally concatenated in a vector zk\nrepresenting each second of the song.\n2.1. Predicting the genre from AR coefﬁcients\nThe machine learning component of the system used in the\ngenre prediction task, is implemented with a simple linear\nregression model with a softmax activation function [3]:\nˆyk= softmax [ Bzk+b], (1)\nwhere ˆykis aCdimensional vector containing the predic-\ntions for the different genres ( C= 15 in our system), andzkare the parameters of the MAR model characterizing one\nsecond of the song. The softmax activation allows to inter-\npretˆykas estimations of the a posteriori probabilities of all\ngenres given some input vector.\nTo adapt the parameters in the model, namely Bandb,\nwe used a set of 10000 training songs whose genre labels\nwere known in advance (it was possible for a song to belong\nto more than one label). These data were used to produce\na set of training pairs {zk,yk}N\nk=1, withykbeing the true\nlabels associated to zk. Then, Bandbwere optimized by\ngradient descent of the cost function\nE=N/summationdisplay\nk=1/bardblyk−ˆyk/bardbl2\n2. (2)\nTo improve the performance of this simple classiﬁer, we\nadded the following improvements:\n•Dimensionality reduction: Instead of using the orig-\ninal MAR coefﬁcients, we only retain the most rele-\nvant projections, so that the regression model equa-\ntion (1) operates on ˜zk=UTzk. The projection ma-\ntrixUis also optimized to minimize equation (2) us-\ning so-called Orthonormalized Partial Least Squares\n(OPLS) [1]. In the reported experiments, ˜zkwas 14-\ndimensional, the maximum number of projections that\nOPLS can provide in this situation (with C=15 genre\nlabels).\n•Non-linear projection: To increase the expressive power\nof the overall classiﬁer, we used a kernel implementa-\ntion [6] of the dimensionality reduction method, i.e.,\ndata was ﬁrst projected to a high dimensional space,\nwhere features with improved discrimination capabil-\nities can be obtained.\n2.2. Data Postprocessing\nThe genre is predicted every second but for visual purposes\nthe results are smoothed before they are sent to the display.\nThe genre vector presented is a running average over the\nprevious four predictions. A screen-shot of the plug-in is\nshown in ﬁgure 1.\n3. Evaluation\nBefore the online implementation the genre predictor was\ntested on a set of unseen data and error an rate of 62.5 %\nwas achieved. In the test, a song was classiﬁed according to\nthe sum of the one-second predictions in the song, i.e. only\nthe overall genre of the song was used. Figure 2 shows the\nconfusion matrix. The confusion is consistent with known\nsimilarity between genre. When the predictor fails for blues\nit is most often because it predicts jazz, country or rock. Th e\nconfusion matrix also illustrates that some genres are hard er\nto predict than others; Christian music is for example oftenClassification labelTrue Label\nABChClCoDFJLNOPR&BRaRoAlt Rock\nBlues\nChristian\nClassical\nCountry\nDance\nFolk\nJazz\nLatin\nNew Age\nOpera\nPop\nR&B\nRap\nRock102030405060708090\nFigure 2. The confusion matrix illustrates that the majority of\nerrors are due to ’reasonable’ confusions such as ’Alternative\nrock’ - ’Rock’ or ’Folk’ - ’Country’.\nmiss-classiﬁed. This reﬂects the fact that the ’Christian’ la-\nbel is not directly related to the musical content, but rathe r\nderived from the context and possibly the lyrics.\nWhether people will ﬁnd it interesting to use the plug-in\nremains to be seen; this in fact is the evaluation that really\nmatters. The plan is to make the plug-in publicly available\nbefore the conference at\nhttp://www.intelligentsound.org/\nReferences\n[1] T. W. Anderson. An Introduction to Multivariate Statistical\nAnalysis . Wiley-Interscience, NY , 3rd edition, 2003.\n[2] Jean-Julien Aucouturier and Francois Pachet. Representing\nmusical genre: A state of the art. Journal of New Music\nResearch , 32:83–93, 2003.\n[3] C. M. Bishop. Neural Networks for Pattern Recognition . Ox-\nford University Press, 1995.\n[4] Martin F. McKinney and Jeroen Breebart. Features for audio\nand music classiﬁcation. In Proceedings of the International\nConference on Music Information Retrieval (ISMIR) , 2003.\n[5] A. Meng, P. Ahrendt, and J. Larsen. Improving music genre\nclassiﬁcation by short-time feature integration. In IEEE In-\nternational Conference on Acoustics, Speech, and Signal\nProcessing , volume V , pages 497–500, mar 2005.\n[6] B Schlkopf and A. J. Smola. Learning with Kernels . MIT\nPress, 2001.\n[7] George Tzanetakis and Perry Cook. Music genre classiﬁ-\ncation of audio signals. IEEE Transactions on Speech and\nAudio Processing , 10(5):293–302, July 2002.\n[8] Erling Wold, Thom Blum, Douglas Keislar, and James\nWheaton. Content-based classiﬁcation, search, and retrieval\nof audio. IEEE Multimedia , 3(3):27–36, 1996."
    },
    {
        "title": "On the Requirement of Automatic Tuning Frequency Estimation.",
        "author": [
            "Alexander Lerch 0001"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1414812",
        "url": "https://doi.org/10.5281/zenodo.1414812",
        "ee": "https://zenodo.org/records/1414812/files/Lerch06.pdf",
        "abstract": "The deviation of the tuning frequency from the standard tun- ing frequency 440 Hz is evaluated for a database of classical music. It is discussed if and under what circumstances such a deviation may affect the robustness of pitch-based systems for musical content analysis. Keywords: concert pitch, tuning frequency, detuning.",
        "zenodo_id": 1414812,
        "dblp_key": "conf/ismir/Lerch06",
        "keywords": [
            "deviation",
            "tuning frequency",
            "standard tuning frequency",
            "database",
            "classical music",
            "pitch-based systems",
            "musical content analysis",
            "robustness",
            "detuning",
            "concert pitch"
        ],
        "content": "On the Requirement of Automatic Tuning Frequency Estimatio n\nAlexander Lerch\nzplane.development\nBerlin, Germany\nlerch@zplane.de\nAbstract\nThe deviation of the tuning frequency from the standard tun-\ning frequency 440Hzis evaluated for a database of classical\nmusic. It is discussed if and under what circumstances such\na deviation may affect the robustness of pitch-based system s\nfor musical content analysis.\nKeywords: concert pitch, tuning frequency, detuning.\n1. Introduction\nPitch extraction from musical audio signals is an important\ntask in the ﬁeld of musical content analysis of monophonic\nas well as polyphonic input data. It is a required processing\nstep for automatic transcription, melody ﬁnding, harmony\nand key detection and other algorithms for audio content\nanalysis.\nFor these applications, various approaches to fundamen-\ntal frequency detection have been proposed, but the mapping\nof frequencies to pitches is frequently regarded to be trivi al,\nassuming the mid frequencies of the pitches to be tuned with\nreference to a standardized tuning frequency of 440Hzfor\nthe pitch A4.\nOn the other hand, there exist a few publications that ad-\ndress the issue of possible deviations of the real tuning fre -\nquency from 440Hzand propose algorithms for the auto-\nmatic detection of this tuning frequency (see section 4.1).\nThis raises the question if an automatic tuning frequency\ndetection could possibly improve the pitch tracking result s\nor if its inﬂuence is negligible. To verify the hypothesis th at\nit might improve the results, a pre-test with a simple auto-\nmatic key detection has been executed on a small database\n(65 tracks) of key labeled jazz recordings. The correct clas -\nsiﬁcation rate increased from 70.8%at a ﬁxed tuning fre-\nquency of 440Hzto76.9%with adaptive tuning frequency\nestimation as described below. Although this result is stat is-\ntically not signiﬁcant due to the small test database, it ind i-\ncates that pitch-based analysis systems may beneﬁt from an\nautomatic detection of tuning frequency.\nPermission to make digital or hard copies of all or part of thi s work for\npersonal or classroom use is granted without fee provided th at copies\nare not made or distributed for proﬁt or commercial advantag e and that\ncopies bear this notice and the full citation on the ﬁrst page .\nc/circlecopyrt2006 University of Victoria2. Tuning Frequency\nThe concert pitch or standard (musical) pitch is used for tun -\ning one or more musical instruments and is deﬁned to be the\npitch A4. Its frequency, the tuning frequency, is standard-\nized internationally to 440Hz[1], but the exact frequency\nused by musicians can vary due to various reasons, e.g. the\nusage of historic instruments or timbre preferences, etc.\nOver time, the variation of the tuning frequency de-\ncreased. Briner [2] mentions some typical frequency ranges\nfor the tuning frequency from the past three centuries, dis-\nplayed in Table 1 as deviation from 440Hz:\nTable 1. Typical Deviation of the tuning frequency from 440Hz\nover three centuries.\nYear lower deviation upper deviation\n∼1750 −50Hz +30Hz\n∼1850 −20Hz +20Hz\n∼1950 −5Hz +10Hz\nNowadays, while for the majority of electronic music\nproductions the default tuning frequency of 440Hzcan be\nassumed, the tuning frequencies of orchestras may show de-\nviations from 440Hz. For example, the Chicago Symphony\nOrchestra and the New York Philharmonic tune at 442Hz,\nwhile the Berlin and Vienna Philharmonic orchestras have\na tuning frequency of 443Hz1. At least in the case of\nboth European orchestras, the tuning frequency was higher\nin previous decades. The frequencies 442Hzand443Hz\ncorrespond to deviations from the standard tuning frequenc y\nof7.85cent and11.76cent, respectively. Such deviations\nmay also occur in other music styles, especially when acous-\ntic instruments are used.\nBesides this intended shift of the tuning frequency, there\nmay be unintended variations over the time of a concert or a\nrecording session. For example, the tuning frequency could\nbe slowly decreasing, as it can be sometimes recognized\nwith choirs without accompaniment; contrarily, a rising in -\nvolvement of the musicians during the concert could lead to\nan increasing tuning frequency. In the case of professional\nmusicians, the maximum variation can probably be assumed\nto be three to ﬁve cent.\n1personal communication with the orchestra’s archivists, M arch and\nApril 20063. Other Frequency Deviations\nIn the context of pitch analysis, there are also other possi-\nble reasons for detection inaccuracies that may add togethe r\nwith deviations of the tuning frequency. Partly, these are\nunder control of the developer like the system’s frequency\ndetection accuracy. On other deviations, the developer has\nno or only limited inﬂuence.\n3.1. Deviation of harmonics from equal tempered scale\nIn several applications, e.g. when creating a simple pitch\nchroma [3], the “pitch detection” is not only based on the\ndetected fundamental frequency, but on all spectral maxima .\nIn this case, the deviation of harmonics from the equal tem-\npered scale (the scale they are mapped to) has to be taken\ninto account. Table 2 shows mean and maximum deviation\nof the harmonic series from the closest equal tempered pitch\nfrequency with respect to the number of harmonics. The\nfundamental is in tune with the scale.\nTable 2. Deviation of harmonics from equal tempered scale\n#Harm max. deviation mean abs deviation\n3 2.0cent 0.7cent\n5 14.7cent 3.1cent\n7 31.2cent 7.0cent\nWhile a maximum deviation of 31.2cent sounds alarm-\ning, its inﬂuence should not be overrated since the seventh\nharmonic usually has a small level compared to the level of\nlower harmonics, dependent on the instrument playing.\nFurthermore, the deviation does not matter in many cases\nat all, since many systems for frequency tracking take the\nharmonic structure into account.\n3.2. Deviation due to non-equal temperament\nIn the equal tempered scale, the frequency ratios of all in-\ntervals are multiples of12√\n2. In an analysis context, equal\ntemperament is usually assumed, which is the only possible\nassumption since the key of the piece is in most cases un-\nknown. However, a musician not restricted to the equal tem-\npered scale by his or her instrument or accompaniment will\nmost likely perform on a non-equal tempered scale, since\nthe equal tempered scale is only a mathematical construct\nto make interval ratios independent from position and key.\nFor example, two string instruments playing a ﬁfth will most\nlikely play a perfect ﬁfth rather than an equal tempered one,\njust because it sounds more “natural”.\nThe Pythagorean temperament ( PT) and the Meantone\ntemperament ( MT) are used as examples to illustrate devia-\ntions from the equal tempered scale. Basically, PTis con-\nstructed with perfect ﬁfths, while MTis constructed with\nperfect thirds. Table 3 shows the maximum deviations of\nthePTandMTscale from the equal tempered scale in cent\nwith an A4 tuning frequency of 440Hzfor different keys.Table 3. Max. deviation of PTand MTfrom equal tempered\nscale\nKey max. deviation ( PT) max. deviation ( MT)\nC 9.8cent 17.1cent\nD 5.9cent 10.3cent\nE 9.8cent 17.1cent\nF 11.7cent 20.5cent\nG 7.8cent 13.7cent\nA 7.8cent 13.7cent\nB 11.7cent 20.5cent\n4. Evaluation of Real World Signals\nThe mentioned deviations are within an assumed tolerance\nrange of ±50cent, but they can add together. To be able\nto draw conclusions if an algorithms performance may be\ninﬂuenced by an incorrect tuning frequency assumption, the\namount of tuning frequency deviation in real world signals\nhas to be investigated. To get results for a large amount of\ntest ﬁles, this analysis has to be done in an automated way.\n4.1. Automatic Tuning Frequency Detection\nThe following systems have been proposed to ﬁnd the best\ntuning frequency match automatically:\nScheirer [4] used a set of narrow bandpass ﬁlters with\ntheir mid frequencies at particular bands that have been\nhandpicked to match pitches from the analyzed score. These\nﬁlters are swept over a small frequency range. The estimated\ntuning frequency is then determined by the frequency of the\nmaximum ﬁlter output sum.\nDixon [5] proposed to use a peak detection algorithm in\nthe FFT domain, calculating the instantaneous frequency of\nthe detected peaks, and adapting the equal tempered refer-\nence frequencies iteratively until the distance between de -\ntected and reference frequencies is minimal. The adaptatio n\namount is calcucated by the lowpass ﬁltered geometric mean\nof previous and current reference frequency estimate.\nZhu et al. [6] computed a constant Q transform (CQT)\nwith the frequency spacing of 10cent over a range of 7 oc-\ntaves. The detected peaks in the CQT spectrum are grouped\nbased on the modulus distance against the concert pitch. If\nthe maximum energy of the resulting 10-dimensional vector\nis above a certain energy threshold, it is used for later pro-\ncessing. For the results of all processing blocks (if not dis -\ncarded), a 10-dimensional so-called tuning pitch histogra m\nis computed, and the tuning frequency is chosen correspond-\ning to the bin with the maximal count.\nUsing a CQT with 33cent frequency spacing, Harte and\nSandler [7] estimate the exact peak positions by interpola-\ntion. A histogram of the peak positions based on the mod-\nulus distance against the concert pitch is computed over the\nlength of the audio ﬁle, and the tuning frequency is set ac-\ncording to its maximum.0 1 2 3 4 5 6 7 8 9 10440442444446448450452454\nTime (s) →Tuning Frequency (Hz)Adaptation Speed\nFigure 1. Adaptation of tuning frequency from initial setti ng\nof440Hzto target 452Hz\nIn the context of single-voiced input signals, Ryyn¨ anen\n[8] added the modulus distance of detected base frequencies\nto a 10-dimensional histogram that is lowpass-ﬁltered over\ntime. Then, a ‘histogram mass centre’ is computed and the\ntuning frequency is adjusted according to this mass centre.\n4.2. Algorithm used\nThe method for the automatic detection of the tuning fre-\nquency used in this paper is described below. A previous\nversion of this algorithm has been published in [9].\n4.2.1. Description\nThe input audio samples are processed by a ﬁlter bank of\nsteep resonance ﬁlters. In the range of 2 octaves around A4,\nthere are 24 groups of ﬁlters in (equal tempered) halftone\ndistance, with each group consisting of 3 ﬁlters. The mid\nfrequencies of each group are spaced with 12cent and the\nmid frequency of the centered ﬁlter is selected based on the\ncurrent tuning frequency assumption. All ﬁlters have con-\nstant Q. The ﬁlter output energy per processing block of\nlength 20msis then grouped based on the modulus distance\nagainst the concert pitch, resulting in a 3-dimensional vec tor\nEfor each block n.\nThe symmetry of the distribution of the three accumu-\nlated energies gives an estimate on the deviation from the\ncurrent tuning frequency compared to the assumption. If the\ndistribution is symmetric, e.g. E(0, n)equals E(2, n), the\nassumption was correct. In the other case, all ﬁlter mid fre-\nquencies are adjusted with the objective to symmetrize the\nenergy distribution in the following processing blocks. Th e\nRPROP-algorithm [10] is used as adaptation rule because it\nallows fast and robust adaptation without the requirement o f\nspeciﬁcally controlling the adaption step size. The adapti on\nrule for the adjustment of the assumed tuning frequency fA4\nof the following processing block n+ 1is:\nfA4(n+ 1) =/parenleftBig\n1 +η·sign/parenleftBig\nE(2, n)−E(0, n)/parenrightBig/parenrightBig\n·fA4(n)(1)\nwithηbeing scaled up if sign returns the same result\nas for the previous block, and scaled down otherwise. To\nensure high accuracy, ηis initialized with a small value.440 441 442 443 444 445 446 447 448 449 4500246810121416\nTuning Frequency (Hz) →OccuranceMIDI Evaluation\nFigure 2. Distribution of results for the MIDI-generated te st\nset tuned at 446Hz\nFigure 1 shows the adaptation from the initial tuning fre-\nquency 440Hzto the real frequency 452Hz. Adaptation is\nparametrized for accuracy rather than speed in this case, so\nit takes the algorithm more than 3sto converge to the target\nfrequency.\nWhile this approach allows real-time processing and per-\nmanent adaptation to possibly varying tuning frequencies,\nin the current context the overall tuning frequency is com-\nputed by ﬁnding the maximum count in a histogram contain-\ning the estimates of all processing blocks. The histogram\nclasses are spaced by one Hertz; while this is not completely\nconsistent since, on the pitch scale, the width of the classe s\ndecreases slightly with increasing tuning frequency, it ne v-\nertheless was chosen considering that on the one hand the\ndeviations are small compared to the expected accuracy, on\nthe other hand these class labels are the most transparent fo r\nthe user when interpreting the result.\n4.2.2. Evaluation\nTo verify the algorithm’s accuracy, a test with a small\ndatabase of 29 input ﬁles generated from MIDI content was\nperformed. The ﬁles were generated with equal tempera-\nment and pitched to a tuning frequency of 446Hzand were\nsigniﬁcantly longer than 10s.\nFigure 2 shows the result for this test set. The result\nis correct in a range of ±1Hzaround the reference. Co-\nincidently, this range roughly corresponds to the just no-\nticeable frequency difference humans are able to recognize\n(2−4cent) [11].\nThe algorithm is expected to give slightly less accurate\nresults when alternative temperaments are used.\n4.3. Analysis\nProcessing a small database of 60 pop and 12 classical\npieces, Zhu et al. [6] found that the majority of pieces\nare tuned to the standard tuning frequency ±10cent, while\nthree pieces of this database had about 50cent deviation\nfrom the standard tuning frequency.\nHere, a larger database consisting of classical music is\nevaluated to allow quantitative statements about tuning fr e-\nquency deviations.410 420 430 440 450 460 47000.050.10.150.20.25\nTuning Frequency (Hz) →rel. occuranceTuning Frequency Distribution\nFigure 3. Distribution of results for the complete data base\n4.3.1. Test sequences\nThe test database is a private collection of classical mu-\nsic, where the term classical is interpreted as “non-popular”\nmusic. It consists of about 300 CDs with overall 3336\ntracks, and has an overall playing time of approximately\n291 hours. It includes various instrumentations and ensem-\nble sizes from solo chamber music to oratorio and integrates\nmusic from different eras of the western music history with a\nfocus on the classic and romantic periods. The signals have\nCD-quality and the average track length is around 314s.\n4.3.2. Results\nFigure 3 shows the distribution of the detected tuning fre-\nquency per track for the whole database. While the maxi-\nmum of the detected tuning frequencies can be found at fre-\nquency 440Hz; the maximum itself consists of about 21%\nof the test database. The result’s mean value is at frequency\n442.38Hzwith a standard deviation of 2.75Hz. 95% of the\nresults are in the range from 439−448Hzand only 50% of\nthe results have tuning frequencies between 440−443Hz.\nThe percentage of ﬁles below 439Hzis about 3.3%.\nWhen the results are sorted into classes roughly corre-\nsponding to the date of composition, there are no signiﬁ-\ncant differences from the overall result, although the maxi -\nmum of three classes can be found at higher frequencies than\n440Hz. It is basically not surprising that the result is sim-\nilar between the classes, since many of the recordings were\nmade at the end of the twentieth century with contemporary\ninstruments. In further evaluations, it might be interesti ng\nto see if there are differences between classes if sorted by\ninstrumentation and/or recording date.\nThe workload produced by the software is, scaled to a\nx86 CPU frequency of 1GHz , about 6%.\n5. Conclusions\nWhile the maximum of the distribution of tuning frequen-\ncies for the test database is indeed at the standard tuning\nfrequency 440Hz, the results indicate a relatively wide fre-\nquency interval of tuning frequencies from 439−448Hz,\ncorresponding to a deviation from the standard tuning fre-\nquency of −3.9cent to31.2cent.Such a deviation is well within a detection range of\n±50cent per pitch; however, in addition to other de-\nviations that cannot be inﬂuenced by the developer like\ntemperament-based pitch frequency deviations, it may lead\nto (avoidable) pitch detection errors.\nThus, at least in the context of classical music, the ro-\nbustness of pitch-based systems for music content analysis\ncould most likely be improved by the usage of an automatic\ntuning frequency detection. Probably, similar results can be\nfound for other musical genres that are played with acoustic\ninstruments.\nFor result veriﬁcation, the used software for automatic\ntuning frequency estimation is available online as a FEAPI\nplugin [12] at http://www.zplane.de/FEAPI.\nReferences\n[1] ISO, “Acoustics – standard tuning frequency (standard m u-\nsical pitch),” 16:1975, ISO, 1975.\n[2] Ermanno Briner, Musikinstrumentenf¨ uhrer , Philipp Reclam\njun., Stuttgart, 3 edition, 1998.\n[3] Mark A. Bartsch and Gregory H. Wakeﬁeld, “To Catch a\nChorus: Using Chroma-Based Representations for Audio\nThumbnailing,” in Proc. of the IEEE Workshop on Appli-\ncations of Signal Processing to Audio and Acoustics (WAS-\nPAA) , New Paltz, October 2001.\n[4] Eric D. Scheirer, “Extracting Expressive Performance I n-\nformation from Recorded Music,” Master’s thesis, Mas-\nsachusetts Institute of Technology, September 1995.\n[5] Simon Dixon, “A Dynamic Modelling Approach to Music\nRecognition,” in Proc. of the International Computer Music\nConference (ICMC) , Hong Kong, August 1996.\n[6] Yongweil Zhu, Mohan S. Kankanhalli, and Sheng Gao,\n“Music Key Detection for Musical Audio,” in Proc. of the\n11th International Multimedia Modelling Conference , Mel-\nbourne, January 2005.\n[7] Christopher A. Harte and Mark B. Sandler, “Automatic\nChord Identiﬁcation Using a Quantised Chromagram,” in\nProc. of the 118th AES Convention , Barcelona, May 2005,\nnumber 6412.\n[8] Matti Ryyn¨ anen, “Probabilistic Modelling of Note Even ts in\nthe Transcription of Monophonic Melodies,” Master’s thesi s,\nTampere University of Technology, March 2004.\n[9] Alexander Lerch, “Ein Ansatz zur automatischen Erkennu ng\nder Tonart in Musikdateien,” in Proc. of the VDT Inter-\nnational Audio Convention (23. Tonmeistertagung) , Leipzig,\nNovember 2004.\n[10] Martin Riedmiller and Heinrich Braun, “A Direct Adapti ve\nMethod for Faster Backpropagation Learning: The RPROP\nAlgorithm,” in Proc. of the IEEE International Conference\non Neural Networks , San Francisco, March/April 1993.\n[11] Eberhard Zwicker and Hugo Fastl, Psychoacoustics. Facts\nand Models , Springer, 2 edition, 1999.\n[12] Alexander Lerch, Gunnar Eisenberg, and Koen Tanghe,\n“FEAPI: A Low Level Feature Extraction Plugin API,”\ninProc. of 8th Int Conference on Digital Audio Effects\n(DAFx’05) , Madrid, September 2005."
    },
    {
        "title": "Tempo Tracking With a Periodicity Comb Kernel.",
        "author": [
            "Ian Leue",
            "Özgür Izmirli"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1416266",
        "url": "https://doi.org/10.5281/zenodo.1416266",
        "ee": "https://zenodo.org/records/1416266/files/LeueI06.pdf",
        "abstract": "Automatic tempo extraction and beat tracking from audio is an important ability, with many applications in music information retrieval. This paper describes a method for tempo tracking which builds on current research in the field. In this algorithm, an autocorrelation surface is calculated from the output of a spectral energy flux onset novelty function. The most salient repetition rate is calculated by cross-correlating dilations of a comb-like prototype spanning multiple frames and the autocorrelation surface. The method addresses tempo tracking through time to account for pieces with variable tempos. In order to compare the performance of our method on music with strong and weak percussive onsets we have evaluated it on both classical music with and without percussion and popular music with percussion. Additionally, beats are phase-aligned and superimposed on the signal for aural evaluation. Results show the comb kernel to be a useful feature in determining the correct beat level. Keywords: beat, tempo tracking, onset detection.",
        "zenodo_id": 1416266,
        "dblp_key": "conf/ismir/LeueI06",
        "keywords": [
            "tempo tracking",
            "tempo extraction",
            "beat tracking",
            "spectral energy flux",
            "comb-like prototype",
            "autocorrelation surface",
            "cross-correlation",
            "phase alignment",
            "audio analysis",
            "music information retrieval"
        ],
        "content": "Tempo Tracking With a Periodicity Comb Kernel\n Ian Leue      Ozgur Izmirli \nCenter for Arts and Technology \nConnecticut College \n270 Mohegan Ave, New London, CT. USA. \nipleu,oizm@conncoll.edu \nAbstract \nAutomatic tempo extraction and beat tracking from audio \nis an important ability, with many applications in music information retrieval. This paper describes a method for tempo tracking which builds on current research in the field. In this algorithm, an autocorrelation surface is \ncalculated from the output of a spectral energy flux onset novelty function. The most salient repetition rate is calculated by cross-correlating dilations of a comb-like prototype spanning multiple frames and the autocorrelation \nsurface. The method addresses tempo tracking through \ntime to account for pieces with variable tempos. In order to \ncompare the performance of our method on music with \nstrong and weak percussive onsets we have evaluated it on both classical music with and without percussion and popular music with percussi on. Additionally, beats are \nphase-aligned and superimpos ed on the signal for aural \nevaluation. Results show the comb kernel to be a useful feature in determining the correct beat level. \nKeywords : beat, tempo tracking, onset detection. \n1. Introduction \nMuch work has been done in onset detection, tempo extraction and beat tracking. However, the problem of metrical hierarchy identification has proven to be difficult.  It is non-trivial to find whic h multiple of the tatum is \nperceived as the beat. This paper describes a tempo tracking algorithm that emphasizes the most salient multiple of the tatum. It works locally, tracking both static and changing tempos. We have evaluated the algorithm on both popular and classical music, which is harder to track. \nTempo is a fundamental aspect of western music, and \nits recognition is considered imperative for computer understanding of music \n[1]. Advances have been made in \nrecent years towards onset detection and effective tempo tracking from audio [2,3]. See [1,4] for overviews. \nOur algorithm works directly from uncompressed audio \nand creates a time-variable tempo curve. It performs periodicity estimation on spectral onset features reported in previous work [1,5,6,7]. Specifically, we detect onsets \nusing a spectral energy flux f eature [1,7,8], perform an \nautocorrelation on the onsets, th en cross-correlate a range \nof dilations of a comb-like prototype spanning multiple frames with the autocorrelation for an estimate of tempo. \n2. Method \n2.1 Onset Detection and Periodicity Estimation \nWe separate the audio into 7 logarithmic frequency bands \nand perform a differentiation on energy in each band. The \nsum of the differentiated energy outputs across all bands is a signal representing the level of onset versus time. \nAfter detecting onsets, we calculate the autocorrelation \nsurface A(τ, n) from the onset signal to find the repetition \npattern over time. The autocorrelation is performed on 8 second windows and is hopped in 1 second increments.  \nHere, \nτ represents the lag and n is the time index. We \ncalculate autocorrelation by sliding windows over the \noriginal full signal. The re sult is a “trend-corrected” \nautocorrelation surface (see Figure 1) that favors all lag times equally, and is primed for the comb prototype. \n2.2 Beat-level Estimation \nWhile autocorrelation can effectively find self-similarity at various lag times, it still leaves open the question of beat-level. Past algorithms picked the highest non-zero lag in the autocorrelation [9] or the highest lag with a multiplicity relationship with other high lags [1]. We sought to build on this groundwork by utilizing a tented comb-like prototype. \n    \n  \nFigure 1.  Left: Autocorrelation surface A(τ, n). Right: \nTented Comb-like Prototype C(τ, j), graphed at a specific \ndilation in the lag-domain. He ight indicates prong weight.  Permission to make digital or hard copi es of all or part of this work for \npersonal or classroom use is grante d without fee provided that copies \nare not made or distributed for profit or commercial advantage and that \ncopies bear this notice and the full citation on the first page. \n© 2006 University of Victoria We construct a comb prototype corresponding to the slowest allowable tempo, T\ns. Then, for each time index, we cross-correlate between dilations of this prototype and the \nautocorrelation surface. Prototypes C(τ, j) are designed to \nhighlight the salient beat level (see Figure 1). Each comb \nhas M prongs (typically 4) at multiples of its current lag \nvalue. This is expected to reveal the highest similarity to the beat-level lag when cross-correlated with the \nautocorrelation surface. These pr ongs are tented in the lag-\ndomain to allow for irregularities in the performance and time-quantization, and index j extends through N time \nframes to ensure that the repetition found in that frame is not temporary. Consecutive prongs  have decaying weights.  \nEach comb dilation is cross-correlated with the \nautocorrelation surface to calcula te the most salient lag:  \n∑∑− +\n=τ\nτ = τ− τ τ =1 N n\nn kmax\nmin) n k , r ( C ) k , ( A ) n , r ( B            (1) \nwhere r is the dilation factor of the prototype (1.. Rmax).  \nRmax is the ratio of the fastest tempo to the slowest tempo. \nτmin and τmax are limits on the lag axis corresponding to the \nspan of the comb for the fastest and slowest tempos. n is \nthe time index of the tempo estimate, N is the number of \ntime frames used in the cross-correlation.  \nStudies have shown that listeners prefer a beat centered \naround 120 beats per minute (BPM)[10]. To model this, we apply a preference curve using the Parncutt function, \nw(r), as formulated in [10] but with a \nβ value of 0.25:  \n          (2) ()()([ r R T r ws − + − = 1 / 120 log exp ) (max2\n2β )]\nAs formulated, the spontaneous tempo is 120 BPM and \nTs is the slowest tempo of interest. The tempo estimate for \neach time index, n, is found by first determining the value \nof r that maximizes w(r)B(r,n) . The tempo estimate is \ngiven by rTs in BPM. This operation, performed on all \nframes, results in a time-variable tempo curve with \nestimates 1 second apart. \n3. Results and Evaluation  \nIn order to compare the performance of our method on music with strong and weak percussive onsets we constructed two test sets: one containing percussive and non-percussive classical music, and one containing percussive popular music. These test sets contained 45 and 30 tracks respectively and were picked randomly. \nWe annotated each piece and compared that to the \nlongest-lasting tempo in the tempo curve. We then calculated a performance score for each test set based on the evaluation criteria used for MIREX 2005. Within an 8% tolerance zone, 1 point was awarded to the correct tempo, .8 for twice or half, and .6 for thrice or one third.  These scores were averaged over the entire test set to create a single performance score between 0 and 1. For comparison purposes, we also implemented a very simple beat-level estimation scheme in which we picked the peak non-zero lag from the autocorre lation surface for each time frame. Our method found 80% (score = 0.96) of the tempos correctly for popular music and 63% (score = 0.81) of the tempos correctly for classical music. The simple method performed poorly with 16% (score = 0.22) on the popular set and 7% (score = 0.12) on classical set. \nIn addition, we phase-a ligned and superimposed \nsynthetic beats onto the original signal for aural evaluation.  This consisted of constructing a local 4-second beat train from the tempo curve,  cross-correlating that beat \ntrain with the onset output, picking the cross-correlation’s peak as the time of a beat a nd repeating this for the entire \npiece. This made it easier to aurally evaluate the algorithm’s performance with variable tempos. The piece “Alphabet Aerobics” by Blacka licious for instance, has a \nsteadily increasing tempo, and we were able to listen to the algorithm successfully track the changing tempo.  \n4. Conclusion \nThe results are promising with MIREX-based scores of 0.96 and 0.81 for the two test sets. They also indicate clearly that utilizing a comb kernel as formulated in this paper can be more effective than other (admittedly primitive) beat-level estimation schemes. We tested and evaluated our method on both popular and classical music in order to have a baseline benchmark for both genres.  Results show that classical music remains an area with \nroom for improvement, and future work will focus on new onset features targeted towards detecting tonal onsets. \nReferences \n[1] M. Alonso, B. David, and G. Richard. “Tempo and Beat Estimation \nof Musical Signals,” in ISMIR 2004 Fifth Int. Conf. on Music Inf. \nRetr. Proc ., 2004, pp. 158-163. \n[2] F. Gouyon, “A Computational Appr oach to Rhythm Description,” \nPh.D. Thesis, Univ. Pompeu Fabra , 2005. \n[3] J.P. Bello, L. Daudet, S. Abdallah, C. Duxbury, M.E. Davies and \nM.B. Sandler, “A Tutorial on Onset Detection in Musical Signals,” IEEE Tran..on Speech and Audio Processing , Feb. 2004. \n[4] F. Gouyon, A. Klapuri, S. Dixon, M. Alonso, G. Tzanetakis, C. \nUhle and P. Cano. \"An Experimental Comparison of Audio Tempo Induction Algorithms,\" IEEE Tran. on Speech and Audio Proc. , \nvol. 14, No. 5, 2006. \n[5] Goto, M, ”An Audio-based Real-time Beat Tracking System for Music With or Without Drum-sounds,” J. New Music Research , \nvol. 30, No. 2, pp. 159–171, 2001. \n[6] A. Klapuri, ”Musical Meter Estimation and Music Transcription,” Cambridge Music Processing Colloquium , Cambridge University, \nMarch 2003.  \n[7] C. Uhle and J. Herre.  “Estimation of Tempo, Micro Time and Time Signature From Percussive Music,” in DAFx-03 6th Int. \nConf. Of Digital Audio Effects, 2003. \n[8] J. Laroche, ”Efficient Tempo and Beat Tracking in Audio \nRecordings,” J. Audio. Eng. Soc. , vol. 51, No. 4, pp. 226–233, \nApril 2003. \n[9] M. McKinney and D. Moelants. “Extracting The Perceptual Tempo \nFrom Music,” in ISMIR 2004 Fifth Int. Conf. on Music Inf. Retr. \nProc., 2004. \n[10] K. Frieler, “Beat and Meter Extr action Using Gaussified Onsets,” \nin ISMIR 2004 Fifth Int. Conf. on Music Inf. Retr. Proc ., 2004."
    },
    {
        "title": "Extending Audacity for Audio Annotation.",
        "author": [
            "Beinan Li",
            "John Ashley Burgoyne",
            "Ichiro Fujinaga"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1417963",
        "url": "https://doi.org/10.5281/zenodo.1417963",
        "ee": "https://zenodo.org/records/1417963/files/LiBF06.pdf",
        "abstract": "By implementing a cached region selection scheme and automatic label completion, we extended an open-source audio editor to become a more convenient audio annotation tool for tasks such as ground-truth annotation for audio and music classification. A usability experiment was conducted with encouraging preliminary results. Keywords: audio annotation, classification, usability.",
        "zenodo_id": 1417963,
        "dblp_key": "conf/ismir/LiBF06",
        "keywords": [
            "audio annotation",
            "classification",
            "usability experiment",
            "ground-truth annotation",
            "open-source audio editor",
            "automatic label completion",
            "more convenient audio annotation tool",
            "tasks such as",
            "audio and music classification",
            "key aspects"
        ],
        "content": "Extending Audacity for Audio Annotation Beinan Li, John Ashley Burgoyne, and Ichiro Fujinaga Music Technology Area, Schulich School of Music, McGill University, Montreal, Quebec beinan.li@mail.mcgill.ca {ashley, ich}@ music.mcgill.ca   Abstract By implementing a cached region selection scheme and automatic label completion, we extended an open-source audio editor to become a more conve nient audio annotation tool for tasks such as ground-truth annotation for audio and music classification. A usability experiment was conduc ted with encouraging preliminary results. Keywords: audio annotation, classification, usability. 1. Introduction Providing training data and suppor ting objective evaluation, ground  truth annotation is both an essential and time-consuming procedure in building general pattern classification systems. In audio segmentation and classification field such as music information retrieval (MIR), manual annotation is increasingly cumbersome for the rapidly growing databases of real-world data.  A few efforts have been contributed to creating specialized annotation tools. Some of them focus on annotating low-level musical events such as onsets [1] and drum patterns [2, 3]. Other publicly available tools work well for everyday audio but lack analytical features such as waveform visualization [4, 5]. In our projects such as automatic track segmentation for digitized phonogr aph records [6] and chorus detection for popul ar music, the goal of the ground-truth annotation has been to mark the semantic passages in a complete audio stream with time-stamped textual taxonom ic labels and to export them in a human- and machine-readable format. No existing tools meet these requirements exactly. We thus started developing our own annotation software based on the open-source audio editor Audac ity [7] 2. The Choice of Bas ic Softw are Framework A few pieces of existing software that have potential to become audio annotation tools are introduc ed in [3]. They are either commercial and not customizable [8] or difficult to adapt to music annotation [9]. In contrast, the open-source audio editor Audac ity is designed for music-oriented audio applications, is cross-platform, suppor ts MIDI and other major audio formats, and is fully customizable. The current version has a built-in function for creating label tracks that are integrated with an active audio track. Users can create one or more time-stamped labels for a selected region of audio data, and the labels can be exported to a text file. These features makes Audac ity a suitable basic framework for our tool. 3. Exte nding Audacity Our starting point is Audac ity 1.3 Beta (the latest version),  in which the label track function has been improved: the labeled region can be adjusted by mouse dragging. However, a few desired features are still missing and therefore we made several extensions. 3.1 Region Selection For ground-truth-oriented audio annotation in which semantic bounda ries should be placed accurately, human annotators must listen repeatedly to the candidate opening and closing areas of a semantic region before making a selection. Like traditional audio editors, Audac ity allows users to select an audio region by dragging the mouse, to expand selection with the Shift key plus a mouse click and to input time bounda ries manually. However, these features share a common drawback of causing an annotator to lose track of any previously located bounda ries—for example, the beginning of a pop-music chorus. As a result, an annotator has to memorize at least one of the bounda ry positions, which wastes annotation time. Our extension introduc es an intermediate cache for the user to store any opening or closing bounda ry positions so that adjustments are easy to make and little short-term memory is required. To facilitate annotating while listening, a playback position is also cacheable as either bounda ry (start or end) of the target region. 3.2 Label Organization and Automatic Completion Audac ity considers labels to be independent symbols with arbitrary titles that should be manually typed, which is cumbersome and error-prone for large-scale annotation tasks. Allowing heterogeneous labels to reside in the same label track adds to the difficulty in performing batch operations such as renaming or region adjustment for a specific category of labels. With our extension, multi-class annotation is performed by using parallel multiple label tracks, each correspondi ng to a single category. In the case of a complementary binary classification, an annotator only needs to annotate one category (such as “Chorus” in our tests) through a single label track, and the annotations Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on t he first page. © 2006 U niversity of Victoria for the other category are automatically completed. An additional conve rter was also implemented to export the labeling data into the ACE XML format [10].  \n Figure 1. The GUI screen-shot of the extended Audacity. a) A text label in an independent label track. b) Toolbar butto ns for the selection. c) Automatic completion. 4. Experiment and Results A usability experiment was conduc ted. During the experiment, six annotators with solid musical background (4 years of professional music training) were asked to annotate the chorus and non-chorus passages of six pop songs ranging in length from 3’54” to 4’18” by using both Audac ity 1.3 Beta (AB) and our extension (AE). The annotation time for each song by each subject was recorded and then analyzed by employing a statistical model. Half the songs  are annotated with AB and the other half AE, with the compositions shuffled for each subject. The mean labeling times for a test song for AE and AB were 605 sec. (σ = 189 sec.) and 698 sec. (σ = 221 sec.). In our model, the usability performance is represented by a normal linear model for log labeling time: \n[]log()ETASV=++                        (1) The factors on time consumption include the subject annotator (A), the selected song (S), and the annotation tool in use (V). Errors are assumed to be normal and additive, and a Kolmogorov-Smirnov test on the model residuals shows no significant departures from normality. We also investigated the inclusion of other factors, such as the presumed level of annotation difficulty, mean and standard deviation of the length of the choruses, and the total length of the song; none  of these additional factors proved to be significant. Under the chosen model, the use of our version of the software was significant (p = 0.0411)  and showed an average reduction in labeling time of 17.1 percent when AE was in use (with the 95-percent confidence interval ranging from 7.9 to 25.6 percent). Note that during the experiment, AB was reported to have crashed during annotations four times in total and two subjects had to restart their annotation of the affected songs . In these cases, we used the time spent on the second attempt as the resulting time to avoid the performance of AB being artificially degraded. 5. Conclusion and Future Wor k  An audio annotation tool that is capable of ground-truth  annotation for audio and music segmentation and classification is introduc ed by extending the open-source audio editor Audac ity. A usability experiment shows that users can perform annotation faster with our extension than with the current version of Audac ity.  Traditional audio editors offer only limited means of audio visualization, typically waveform and spectrogram. More helpful visual cues derived from audio features that have proven to be reliable semantic indicators might facilitate annotation further. By displaying these data, pre-processed with a tool like ACE [10], the annotator could be aided by a larger number of visual cues.  6. Acknowledgments We would like to thank CIRMMT, the Canada Founda tion for Innova tion, and Schulich Scholarship program at McGill University for their generous financial support. References [1] P. Leveau P, L. Daudet, and G. Richard. “Met hodology and Tools for the Evaluation of Automatic Onset Detection Algorithms in Music,” in ISMIR 2004 Fifth Int. Conf. on Music Inf. Retr. Proc., 2004, pp. 72–75. [2] F. Gouyon, N. Wack, and S. Dixon. “An Open Source Tool for Semi-Automatic Rhythmic Annotation,” in DAFx 2004 Seventh Int. Conf. on Digital Audio Effects Proc., 2004, pp. 193–196. [3] K. Tanghe, M. Lesaffre,  S. Degroeve, M. Leman , B. De Baets, and J.-P. Mart ens. “Collecting Ground Truth Annotations  for Drum Detection in Polyphonic Music,” in ISMIR 2005 Sixth Int. Conf. on Music Inf. Retr. Proc., 2005, pp. 50–57.   [4] Academic Technologies. “Project Pad – Documentation.” [Web site] 2006, [2006 July 8], Available: http://dewey.at.northwestern.edu/ppad2/documents/help/audio.html [5] The Center for Humane Arts, Letters, and Social Sciences Online. “[Med iaMat rix].” [Web site] 2006, [2006 July 8], Available: http://www.matrix.msu.edu/~mmat rix/ [6] Marv in Duchow Music Library. “David Edelberg Handel LPs.” [Web site] 2006, [2006 April 22], Available: http://coltrane.music.mcgill.ca/handel/lp/search.php [7] Audacity Development Team.  “Audacity: Free Audio Editor and Recorder.” [Web site] 2006, [2006 April 22], Available: http://audacity.sourcefo rge.net/ [8] Twelve Tone Systems, Inc. “SONAR 5” [Web site] 2006,  [2006 April 22], Available: http://www.cakewalk.com/ Products/SONAR/default.asp [9] P. Boersma and D. Weenink. “Praat : Doing Phonetics by Computer” [Web site] 2006, [2006 April 22], Available: http://www.fon.hum.uva.nl/praat/ [10] C. McK ay, R. Fiebrink, D. McE nnis, B. Li, and I. Fujinaga. “ACE: A Framew ork for Optimizing Music Classification,” in ISMIR 2005 Sixth Int. Conf. on Music Inf. Retr. Proc., 2005, pp. 42–49.  \n \n a b c"
    },
    {
        "title": "Singing Voice Separation from Monaural Recordings.",
        "author": [
            "Yipeng Li",
            "DeLiang Wang"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1416006",
        "url": "https://doi.org/10.5281/zenodo.1416006",
        "ee": "https://zenodo.org/records/1416006/files/LiW06.pdf",
        "abstract": "Separating singing voice from music accompaniment has wide applications in areas such as automatic lyrics recog- nition and alignment, singer identification, and music in- formation retrieval. Compared to the extensive studies of speech separation, singing voice separation has been little explored. We propose a system to separate singing voice from music accompaniment from monaural recordings. The system has three stages. The singing voice detection stage partitions and classifies an input into vocal and non-vocal portions. Then the predominant pitch detection stage detects the pitch contour of the singing voice for vocal portions. Fi- nally the separation stage uses the detected pitch contour to group the time-frequency segments of the singing voice. Quantitative results show that the system performs well in singing voice separation. Keywords: Singing voice detection, predominant pitch de- tection, singing voice separation",
        "zenodo_id": 1416006,
        "dblp_key": "conf/ismir/LiW06",
        "keywords": [
            "singing voice detection",
            "predominant pitch detection",
            "singing voice separation",
            "quantitative results",
            "speech separation",
            "automatic lyrics recognition",
            "singer identification",
            "music information retrieval",
            "monaural recordings",
            "time-frequency segments"
        ],
        "content": "Singing VoiceSeparation fromMonaural Recordings\nYipengLi\nDepartment of Computer Science and Engineering\nThe Ohio State University\nliyip@cse.ohio-state.eduDeLiangWang\nDepartmentof Computer Science and Engineering\nand the Center for CognitiveScience\nThe Ohio State University\ndwang@cse.ohio-state.edu\nAbstract\nSeparating singing voice from music accompaniment has\nwide applications in areas such as automatic lyrics recog-\nnition and alignment, singer identiﬁcation, and music in-\nformation retrieval. Compared to the extensive studies of\nspeech separation, singing voice separation has been little\nexplored. We propose a system to separate singing voice\nfrommusicaccompanimentfrommonauralrecordings. The\nsystem has three stages. The singing voice detection stage\npartitions and classiﬁes an input into vocal and non-vocal\nportions. Thenthepredominantpitchdetectionstagedetects\nthepitchcontourofthesingingvoiceforvocalportions. Fi-\nnally the separation stage uses the detected pitch contour\nto group the time-frequency segments of the singing voice.\nQuantitative results show that the system performs well in\nsingingvoiceseparation.\nKeywords: Singing voice detection, predominant pitch de-\ntection,singing voiceseparation\n1. Introduction\nA successful singing voice separation system is useful in\nmany areas such as automatic lyrics recognition and align-\nment, singer identiﬁcation, and music information retrieval.\nIn this paper, we focus on singing voice separation from\nmonaural recordings. A monaural solution is indispensable\ninmanycases,suchasseparatingthesingingforliverecord-\nings(non-studiorecordings). Thedevelopmentofasuccess-\nfulmonauralsingingvoiceseparationsystemcouldalsoen-\nhanceourunderstandingofhowthehumanauditorysystem\nperformssuch tasks.\nAlthoughsingingvoiceisproducedbythespeechorgan,\nspeech separation systems might not be directly applicable\nto singing voice separation. This is mainly because of the\nnature of other concurrent sounds. In a real acoustic envi-\nronment, interfering sounds in most cases are uncorrelated\nwith speech. For recorded songs, however, music accompa-\nniment is correlated with singing voice since they are com-\nposed to be a coherent whole. This difference makes the\nseparation of singing voice from music accompaniment po-\ntentiallymore challenging.\nThe perceptual work by Bregman [1] and others has in-\nspired researchers to study computational auditory scene\nanalysis(CASA). Compared to other sound separation ap-\nproaches, such as spectral subtraction, CASA makes fewer\nassumptionsaboutconcurrentsoundsthereforeshowsgreater\nPermissiontomakedigitalorhardcopiesofallorpartofthisworkfor\npersonal or classroom use is granted without fee provided that copies\narenotmadeordistributedforproﬁtorcommercialadvantageandthat\ncopies bear this notice and the full citation on the ﬁrst page.\nc°2006 Universityof Victoria\nSinging\r\nVoice\r\nDetection\rPredominant\r\nPitch\r\nDetection\rSinging\r\nVoice\r\nSeparation\r Mixture\r Vocal\r\nPortions\rSeparated\r\nSinging\rFigure1. Schematic diagram of the proposedsystem\npromise in singing voice separation from monaural record-\nings. Mellinger [2] proposed a CASA system which ex-\ntractsonsetandcommonfrequencyvariationandusesthem\nto group frequency partials from the same musical instru-\nment. Godsmark and Brown [3] developed a CASA system\nwhich uses harmonicity and other auditory scene analysis\nprinciplesinablackboardarchitectureformusicsoundsep-\naration. Recently a speech separation system developed by\nHu and Wang [4] exploits pitch and amplitude modulation\ntoseparatevoicedspeechfromvariouskindofinterference.\nSystematic evaluation shows that the system performs sig-\nniﬁcantly better than previoussystems.\nSince the Hu–Wang system allows the interference to be\nharmonic,itispossibletoapplythesystemtosingingvoice\nseparation. The accuracy of pitch detection is critical for\nthe Hu–Wang system. However, as shown in [5] their pitch\nestimation is unreliable when singing voice is accompanied\nby music. This problem can be alleviated by a predomi-\nnantpitchdetectionalgorithmweproposedin[5],whichde-\ntectsmoreaccuratelythepitchesofsingingvoicefordiffer-\nent musical genres. Because their system works for voiced\nspeech, it is necessary to have a mechanism to distinguish\nportions where singing voice is present from those where\nit is not. On the other hand, although their system is lim-\nited to voiced speech separation, this limitation is less se-\nvere for singing voice separation because unvoiced singing\ncomprisesasmallerpercentageintermsoftimeanditscon-\ntribution to the intelligibility of singing is less than that to\nthe intelligibility of speech.\nIn this paper we propose a singing voice separation sys-\ntem which consists of three stages, as shown in Fig. 1. The\nﬁrst stage performs singing voice detection in which the in-\nput is partitioned and classiﬁed into vocal and non-vocal\nportions. Thenvocalportionsareusedasinputtothesecond\nstage for predominant pitch detection. In the last stage, de-\ntected pitch contours are used for singing voice separation\nwhere we extend the Hu–Wang system [4]. The output of\nthe system is separated singing voice.\nTheremainderofthispaperisorganizedasfollows. Sec-\ntion2describeseachstageofthesystem. Section3presents\nthe evaluationand the last section concludes the paper.\n2. System description\n2.1. Singing voicedetection\nThe goal of this stage is to partition the input into vocal\nportions in which singing voice is present and non-vocal\nportions in which singing voice is absent. Our strategy isbased on the observation that, when a new sound enters a\nmixture, it usually introduces signiﬁcant spectral changes.\nTherefore the possible instance when a sound enters can be\ndeterminedbyidentifyingsigniﬁcantspectralchanges. This\nidea is more suitable for singing voice detection since, in\norder to conform with the rhythmic structure of a song, a\nvoiceismorelikelytojointheaccompanimentatbeattimes\nwhen strong spectral perturbation occurs. Therefore in this\nstage we ﬁrst use a spectral change detector to partition the\ninput into spectrally homogeneous portions and then pool\ntheinformation within a portion for classiﬁcation.\nThe spectral change detector used in this stage is pro-\nposed by Duxbury et al. [6]. It calculates the Euclidian\ndistanceinthecomplexdomainbetweentheexpectedspec-\ntral value and the observed one in each frame. Signiﬁcant\nspectral changes are indicated as local peaks in the distance\nvalues. After the input is partitioned into portions, each\nportion is classiﬁed into vocal or non-vocal according to\nthe overall likelihood. Formally let fX1;X2; :::;XMgbe\na set of feature vectors of a portion with Mframes. Let\nlogp(Xjcv)andlogp(Xjcnv)represent the log likelihood\nof an observed feature vector Xbeing in the vocal class cv\nand the non-vocal class cnv, respectively. Then a portion is\nclassiﬁedas vocalif:\nMX\nj=1logp(Xjjcv)>MX\nj=1logp(Xjjcnv)(1)\nSincemel-frequencyspectralcoefﬁcients(MFCC)andGaus-\nsianmixturemodels(GMM)arewidelyusedinaudioclassi-\nﬁcationtasks,wechooseMFCCsasthefeaturesandGMMs\nasthe classiﬁers for likelihoodevaluation.\n2.2. Predominantpitch detection\nIn this stage, a predominant pitch detection algorithm pro-\nposed in [5] is used to detect the pitch contour of singing\nvoice for vocal portions. The algorithm ﬁrst decomposes\na vocal portion into its frequency components with a 128-\nchannel gammatone ﬁlterbank. A normalized correlogram\nis then computed for each channel and each frame to ob-\ntain periodicity information. The peaks in the normalized\ncorrelogram contain the periodicity information of the in-\nput. However,duetothepresenceofmusicaccompaniment,\nsome peaks may give misleading information. To allevi-\nate the problem, channel and peak selection are applied to\nall channels to extract reliable periodicity information. The\nalgorithm uses Hidden Markov Model (HMM) to describe\nthe pitch generation process. In each frame the observation\nprobabilityofapitchhypothesisiscalculatedbyintegrating\nthe periodicity information across all frequency channels.\nThe transition probability between two consecutive frames\nis determined by training. In order to reduce the interfer-\nence of other harmonic sounds from accompaniment, the\nHMM tracks up to 2 predominant pitch contours simulta-\nneously. Finally the Viterbi algorithm is used to ﬁnd the\nmost likely sequence of pitch hypotheses and the ﬁrst pitch\ncontour of this optimal sequence is considered as the pitch\ncontour of the singing voice. More details of the algorithm\ncanbe found in [5, 7].\n2.3. Singing voiceseparation\nIn this stage, the voiced speech separation algorithm devel-\nopedbyHuandWang[4]isextendedforsingingvoicesep-aration for vocal portions. The singing voice separation al-\ngorithm ﬁrst passes the input, i.e., a vocal portion, through\nan auditory periphery which is a 128-channel gammatone\nﬁlterbank. The output of each channel is further divided\ninto 16–ms time frames with 50% overlap. In this way, the\ninput is decomposed into a time-frequency (T-F) map, each\nelement of which is called a T-F unit. For each T-F unit,\nthe following features are extracted: energy, autocorrela-\ntion, cross-channel correlation, and cross-channel envelope\ncorrelation.\nNext,thealgorithmformssegmentsbymergingcontigu-\nousT-Funitsbasedontemporalcontinuityandcross-channel\ncorrelation. Only those T-F units whose energy and cross-\nchannel correlation both high are considered. Neighboring\nunits, either in time or frequency, are merged into segments\niteratively.\nBycomparingthelocalperiodicityinformationindicated\nin the autocorrelation of a T-F unit to the estimated period-\nicity of the singing voice in the same frame, the T-F unit\nis labeled as either singing voice dominant or accompani-\nment dominant. We use the pitch contour of the singing\nvoice obtained in the second stage to label each T-F unit.\nMorespeciﬁcally,aT-Funitinfrequencychannel candtime\nframe mis labeled as singing dominant if:\nA(c; m; ¿ S(m))\nA(c; m; ¿ P(c; m))> µT (2)\nwhere A(c; m; ¿ )is the autocorrelation of the unit with the\ntime lag indicated by ¿, and µTis a threshold. ¿S(m)is\nthe time lag corresponding to the estimated pitch period in\nframe mwhile ¿P(c; m)is the time lag corresponding to\ntheglobalmaximumof A(c; m; ¿ )withintheplausiblepitch\nrange from 80 to 500 Hz. This periodicity criterion works\nwell for T-F units where harmonics are resolved — a har-\nmonic is resolved if it activates a dedicated auditory ﬁlter.\nForﬁltersrespondingtomultipleharmonics,theirresponses\nare amplitude-modulated. As a result, the time lag of the\nglobal maximum of A(c; m; ¿ )of those ﬁlters within the\npitch range might not correspond to the pitch period.\nTo deal with the problem of unresolved harmonics, the\nalgorithm extracts the amplitude modulation (AM) rate for\neachunitandcomparestheAMratewiththeestimatedpitch\nperiod. More speciﬁcally, a normalized envelope of a T-F\nunit is ﬁrst extracted. Then a single sinusoid with the same\nperiodastheestimatedpitchperiodisconstructed. Tocom-\nparethesinusoidwiththenormalizedenvelope,thephaseof\nthe sinusoid is adjusted such that the square error between\nthese two signals is minimized. After the phase is deter-\nmined,theT-Funitislabeledassingingdominantiftheen-\nvelopecanbewelldescribedbytheobtainedsinusoid. This\nAM criterion is formally deﬁned as:\nPN¡1\nn=0[^r(c; n)¡cos(2¼n\n¿S(m)fS+Ácm)]2\nPN¡1\nn=0^r2(c; n)< µAM(3)\nwhere ^r(c; n)is the normalized envelope. Ácmrepresents\nthe phase minimizing the square error and fSis the sam-\npling frequency of the input. nis the time index and Nis\nthe length of the envelope. µAMis a threshold. For units\nlabeledassingingdominantbytheAMcriterion,additional\nsegments are generated based on temporal continuity and\ncross-channel envelopecorrelation.Table 1. Classiﬁcation accuracy for different methods (%\nframes)\n-5 dB 0 dB 5 dB 10 dB\nproposed method 80.3 85.0 90.2 91.1\nframe-levelclassiﬁcation 71.3 77.4 81.7 83.8\nHMM 79.0 83.5 87.5 88.8\nTable 2. Predominant pitch detection error rates with actual\nclassiﬁcation(%)\n-5 dB 0 dB 5 dB 10 dB\nProposed 44.2 31.7 24.3 21.6\nKlapuri 55.5 41.7 31.7 26.5\nWuet al. 55.1 39.0 29.0 22.4\nIn the ﬁnal step of the separation algorithm, segments\nwhereamajorityofT-Funitsislabeledassingingdominant\nare grouped to form the foreground stream, which corre-\nsponds to the singing voice. From the segments in the fore-\nground stream the singing voice can be obtained by resyn-\nthesis. Formore details of the Hu–Wangsystem, see [4].\n3. Evaluation\nWeextracted10songssampledat16kHzfromkaraokeCDs\nfor singing voice detection. These CDs are recorded with\nmultiplex technology. With proper de-multiplexing soft-\nware, clean singing voice and accompaniment can be ex-\ntracted. We further extracted 25 clips from the 10 songs for\nsingingvoicepitchdetectionandseparation. Theseclipsin-\nclude rock and country music. We refer to the energy ratio\nof singing voice to accompaniment as signal to noise ratio\n(SNR)as in speech separation studies.\nFig. 2 shows the output of each stage of the proposed\nsystem for a clip of rock music. Fig. 2(a) is the waveform\nof the clean singing voice. The thick lines above the wave-\nformindicatereferencevocalportionsobtainedbyapplying\nanenergy-basedsilencedetectoronthecleansingingvoice.\nThe mixture in which the singing voice and the accompa-\nniment are mixed in 0 dB is shown in Fig. 2(b). Fig. 2(c)\ngives the result of singing voice detection on the mixture.\nA high value indicates the frame is classiﬁed as vocal and a\nlowvalueasnon-vocal. Fig. 2(d)givestheresultofpredom-\ninantpitchdetectiononthedetectedvocalportions. Thede-\ntected pitches are plotted as dots against the reference pitch\ncontours which are plotted as solid lines. The output of the\nsinging voice separation stage is plotted in Fig. 2(e). As\ncan be seen, the separated singing voice matches the clean\nsingingvoicewell.\nForsingingvoicedetection,wetrainedtheclassiﬁerswith\nsamplesmixedin0and10dBandtestedthemin4different\nSNRs using 10-fold cross validation. The average classiﬁ-\ncation accuracies (percentage of frames) are shown in the\nﬁrst row of Table 1. For comparison purposes, the results\nof frame-level classiﬁcation (each frame is a portion) and\nHMM similar to the one used in [8] are also shown in Ta-\nble 1. As can be seen, the proposed singing voice detection\nmethodperforms better for all SNRs.\nFor predominant pitch detection, we calculate the error\nrates (at the frame level) of pitch detection for the ﬁrst two\nstages. Note that in this case the singing voice detection\n00.4 0.8 1.2 1.6 22.4 2.8−101(a)Amplitude\n00.4 0.8 1.2 1.6 22.4 2.8−101(b)Amplitude\n00.4 0.8 1.2 1.6 22.4 2.8(c)\n00.4 0.8 1.2 1.6 22.4 2.8234567(d)Pitch Period (ms)\n00.4 0.8 1.2 1.6 22.4 2.8−101(e)\nTime(s)AmplitudeFigure2. Outputofeachstageforaclipofrockmusic. (a)The\nsinging voice. (b) The mixture. (c) The output of the singing\nvoicedetectionstage. Vocalportionsareindicatedbyhighval-\nuesandnon-vocalportionsbylowvalues. (d)Theoutputofthe\npredominant pitch detection stage. Dots indicate the detected\npitches and the solid lines indicate the reference pitches. (e)\nThe output of the singing voiceseparation stage.\nstage also contributes to the pitch detection errors. The ref-\nerence pitches are calculated using Praat [9]. An error oc-\ncurs if a detected pitch is not within 10% of the reference\npitch. The error rates of the proposed method as well as\nthose of two other methods are shown in Table 2. Kla-\npuri’salgorithm[10]performsmultipitchdetection. Weim-\nplemented the algorithm and chose the ﬁrst detected pitch\nas the predominant one. The obtained pitch sequence was\nsmoothed to improve the pitch detection accuracy. The per-\nformance of the original algorithm developed by Wu et al.\n[7] is also listed in Table 2. As can be seen, for all SNRs,\nour method has lowerpitch detection error rates.\nAn important aspect of evaluating sound separation sys-\ntems is the criterion, which is directly related to the com-\nputational goal of a system. For musical applications, the\nperceptual quality of the separated sound is emphasized in\nsome cases. However, perceptual quality is subjective and\nhard to quantify. Here we adopt the notion of ideal binary\nmask proposed in [4]: a T-F unit in the mask is assigned\n1 if the energy of the target source in the unit is stronger\nthan that of other concurrent sounds, and 0 otherwise. This\nnotion is grounded on the well-established auditory mask-\ning phenomenon [11]. For more discussion of the ideal bi-\nnary mask, the interested reader is referred to [12]. With\nclean singing voice and accompaniment available, the ideal\nbinary mask can be readily constructed. Our informal lis-\ntening experiments show that the quality of singing voice\nresynthesized from the ideal binary mask is close to that of\ntheoriginalonewhenSNRishighanditdegradesgradually\nwithdecreasingSNR.Thereforewesuggesttousetheideal\nbinarymaskasthecomputationalgoalforsingingvoicesep-−5 0 5 10−202468101214\nSNR of Input Signal (dB)SNR Gain (dB)Ideal Pitch\nIdeal Classification + Pitch Detection\nActual Classification + Pitch Detection\nComb FilteringFigure3. SNR gain comparison.\naration.\nThe performance of the system can be quantiﬁed by cal-\nculating the SNR before and after the separation using the\nsinging voice resynthesized from the ideal binary mask as\ntheground truth [4]:\nSNR = 10 log 10[P\nnI2(n)P\nn(I(n)¡O(n))2](4)\nwhere I(n)isthegroundtruth. IncalculatingtheSNRafter\nseparation, O(n)is the output of the system. In calculat-\ning the SNR before separation, O(n)is the mixture resyn-\nthesized from an all-one mask, which compensates for the\ndistortionintroduced in the resynthesis.\nWe evaluate the performance of the proposed system for\n4 different SNRs: -5, 0, 5, and 10 dB. Fig. 3 shows the\nSNR gains after separation for different cases. The SNR\ngains using the ideal pitch as input to the separation stage\nare shown as the line on the top. This gives the ceiling per-\nformance of our pitch-based separation system. The second\nline from the top gives the SNR gains using reference vo-\ncal portions (ideal classiﬁcation) and pitch detection. As\nthe predominant pitch detection stage introduces errors to\nthe system, the gains are lower than that using ideal pitch.\nThe third line from the top shows the SNR gains of the\nsystem, i.e., using actual classiﬁcation and pitch detection.\nAs the singing voice detection stage also makes errors, the\nperformance is further decreased. Although the SNR af-\nter separation of the proposed system for the 10 dB case\nis not improved, the system achieves SNR improvements\nof 7.1, 5.5, and 3.7 dB for the input SNR of -5, 0, and 5\ndB, respectively. This demonstrates that the proposed sys-\ntem works better for low SNR situations. We also compare\nthe proposed separation system with a standard comb ﬁl-\ntering method [13], which extracts the spectral components\nat the multiples of a given pitch. As shown in the bot-\ntom line in Fig. 3, the performance of the comb ﬁltering\nmethod is consistently worse than that of the proposed sys-\ntem. Since the classiﬁcation stage rejects energy from the\naccompaniment, this stage alone is expected to contribute\nto the SNR gains. Quantitatively the SNR gains from the\nclassiﬁcation stage alone are 1.4, 1.0, 1.1, and 0.2 dB for-5, 0, 5, and 10 dB cases, respectively. Therefore the SNR\ngains are mainly contributed from the pitch-based separa-\ntion. Demos of singing voice separation can be found at\nhttp://www.cse.ohio-state.edu/˜liyip/Research/Publication/2\n006/singing demo.htm.\n4. Conclusion\nIn this paper, we have proposed a monaural system to sep-\narate singing voice from music accompaniment. Our sys-\ntem ﬁrst detects vocal portions and then applies predomi-\nnantpitchdetectiontoeachvocalportiontoobtainthepitch\ncontour of singing voice. Finally the system uses detected\npitch contours to separate the singing voice from music ac-\ncompaniment by extending a voiced speech separation sys-\ntem. Quantitativeevaluationofthesystemshowsthatitper-\nforms well for singing voice separation, especially in low\nSNR conditions.\n5. Acknowledgments\nThis research was supported in part by an AFOSR grant\n(F49620-04-1-0027) and an NSF grant (IIS-0534707). We\nthank G. Hu for manyuseful discussions.\nReferences\n[1]A. S. Bregman, Auditory Scene Analysis , MIT Press, Cam-\nbridge,MA, 1990.\n[2]DavidK.Mellinger, EventFormationandSeparationinMu-\nsical Sound , Ph.D. thesis, Stanford University, Department\nofComputer Science, 1991.\n[3]D. Godsmark and G. J. Brown, “A blackborad architecture\nforcomputationalauditorysceneanalysis,” SpeechCommu-\nnication, vol.27, no. 4, pp. 351–366, 1999.\n[4]Guoning Hu and DeLiang Wang, “Monaural speech segre-\ngation based on pitch tracking and amplitude modulation,”\nIEEE Transactions on Neural Networks , vol. 15, pp. 1135–\n1150,2004.\n[5]Yipeng Li and DeLiang Wang, “Detecting pitch of singing\nvoice in polyphonic audio,” in Proc. IEEE ICASSP , 2005,\nvol.3, pp. 17–20.\n[6]Chris Duxbury, Juan Pablo Bello, Mike Davies, and Mark\nSandler, “Complex domain onset detection for musical sig-\nnals,” inProc.ofthe6thConferenceonDigitalAudioEffect\n(DAFx-03) ,London, U.K., 2003.\n[7]Mingyang Wu, DeLiang Wang, and Guy J. Brown, “A mul-\ntipitch tracking algorithm for noisy speech,” IEEE Trans-\nactions on Speech Audio Processing , vol. 11, pp. 229–241,\n2003.\n[8]Adam L. Berenzweig and Daniel P. W. Ellis, “Locating\nsingingvoicesegmentswithinmusicsignals,” in Proc.IEEE\nWASPAA,2001, pp. 119–122.\n[9]P. Boersma and D. Weenink, “Praat: Do-\ning phonetics by computer, version 4.0.26,”\n(http://www.fon.hum.uva.nl/praat),2002.\n[10]A.P. Klapuri, “Multiple fundamental frequency estima-\ntion based on harmonicity and spectral smoothness,” IEEE\nTransactions on Speech Audio Processing , vol. 11, pp. 204–\n816,2003.\n[11]Brian C. J. Moore, An Introduction to the Psychology of\nHearing, ﬁfthedition,AcademicPress,London,U.K.,2003.\n[12]DeLiang Wang, “On ideal binary mask as the computa-\ntional goal of auditory scene analysis,” in Speech Separa-\ntionbyHumansandMachines ,P.Divenyi,Ed.,pp.181–197.\nKluwerAcademic, Boston, MA, 2005.\n[13]J. R. Deller, J. G. Proakis, and J. H. L. Hansen, Discrete-\nTime Processing of Speech Signals , Macmillan, New York,\n1993."
    },
    {
        "title": "Visually Profiling Radio Stations.",
        "author": [
            "Thomas Lidy",
            "Andreas Rauber"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1418079",
        "url": "https://doi.org/10.5281/zenodo.1418079",
        "ee": "https://zenodo.org/records/1418079/files/LidyR06.pdf",
        "abstract": "The overwhelming number of radio stations, both online and over the air, makes the choice of an appropriate program difficult. By profiling the program content of radio stations using Self-Organizing Maps we provide a reflection of a sta- tion’s program type and give potential listeners a visual clue for selecting radio stations. Profiles of current broadcasts indicate which program type a station is currently playing. By creating radio station maps it is possible to directly pick a specific program type instead of having to search for a suitable radio station. Keywords: radio stations, online streams, broadcast, au- dio feature extraction, genre discrimination, profiles, Self- Organizing Map",
        "zenodo_id": 1418079,
        "dblp_key": "conf/ismir/LidyR06",
        "keywords": [
            "radio stations",
            "online streams",
            "broadcast",
            "audio feature extraction",
            "genre discrimination",
            "profiles",
            "Self-Organizing Map",
            "program type",
            "program content",
            "potential listeners"
        ],
        "content": "Visually Proﬁling Radio Stations\nThomasLidy AndreasRauber\nVienna University of Technology\nDepartment of Software Technology and Interactive Systems\nFavoritenstrasse 9-11/188, A-1040 Vienna, Austria\nlidy,rauber@ifs.tuwien.ac.at\nAbstract\nTheoverwhelmingnumberofradiostations,bothonlineand\nover the air, makes the choice of an appropriate program\ndifﬁcult. By proﬁling the program content of radio stations\nusingSelf-OrganizingMapsweprovideareﬂectionofasta-\ntion’sprogramtypeandgivepotentiallistenersavisualcl ue\nfor selecting radio stations. Proﬁles of current broadcast s\nindicate which program type a station is currently playing.\nBy creating radio station maps it is possible to directly pic k\na speciﬁc program type instead of having to search for a\nsuitable radiostation.\nKeywords: radio stations, online streams, broadcast, au-\ndio feature extraction, genre discrimination, proﬁles, Se lf-\nOrganizing Map\n1. Introduction\nMusic information retrieval is a fast growing research area .\nTo date many issues for intelligent retrieval of music have\nbeen addressed. Music can be retrieved by similarity, in\ntermsofmelody,rhythm,pitch,brightness,etc. Artistssi m-\nilar to a given one can be found either by acoustic analysis\nor data mining on the web, or also combined approaches.\nSongscanberetrievedbyhumming,byaudioexamples,etc.\nMany more techniques have been presented in recent years,\nand many of them deal with efﬁcient retrieval of pieces of\nmusicfrom large repositories.\nYet, there is a plethora of audio streams emerging: new\non-line radio stations are created, and the number of sta-\ntions using traditional broadcast over the air is growing as\nwell. Also, the trend to podcasts increases the problem of\n“channel overﬂow”. Choosing a station with a program\nthat matches one’s personal taste thus is not an easy task,\na situation that one might know if one travels to another\ncountry. Only from a station’s name (e.g. “Soundportal”)\nit is very hard to guess what music it is playing. While\nlistening a while to one or another station and switching\nthrough several stations might be the traditional solution ,\nPermissiontomakedigitalorhardcopiesofallorpartofthisw orkfor\npersonal or classroom use is granted without fee provided th at copies\narenotmadeordistributedforproﬁtorcommercialadvantagean dthat\ncopies bearthis notice andthefull citation ontheﬁrst page .\nc/circlecopyrt2006University ofVictoriathis will hardly be feasible given the multitude of new sta-\ntions. TheEuropeanBroadcastingUnionhasaddressedthis\nproblem within the Radio Data System (RDS; RDBS in the\nUSA) [1], which provides additional information to a tradi-\ntionalbroadcast. Besidesbroadcastingthestationnamean d\nother textual information, automatically switching to tra f-\nﬁc broadcast or alternative frequencies, it also broadcast s\nmeta-data about the program type, allowing one to pick ra-\ndio stations playing classical music or news. However, this\ninformationishardlyusedbytheconsumers,andalsomany\nstations donot even broadcast thismeta-information.\nIn this paper we present a method for proﬁling radio sta-\ntions, both traditional ones and online radios, which assis ts\ninsolvingthisproblembylettingthecomputerindexthera-\ndio stations and create proﬁles, or “ﬁngerprints”, of them.\nWiththeseproﬁlesitispossibletogetaquickimpressionof\na station’s focus. We use the technique of Self-Organizing\nMaps to organize the program coverage of radio stations on\na two-dimensional map. From the radio station map we de-\nrive visual proﬁles of speciﬁc radio stations by considerin g\ntheir distribution within speciﬁc areas of the musical hori -\nzon the music map spans. This approach allows to proﬁle\nthecompleteprogramofaradiostation,gettinganoverview\nofthestation’sprogram,ortoviewanactivityproﬁleofe.g .\nthe last 10 minutes of the broadcast, in order to see where\nthe station is currently active. Moreover, the “now playing ”\nfunction can show immediately the kind of music a station\nisplaying bymarkingthelocationontheradiostationmap.\nThis enables completely novel access to the plethora of ra-\ndio stations, allowing to select always the program or kind\nof musiconelikes insteadofhaving tohave afavoriteradio\nstation.\nIn Section 2 we discuss related work. Section 3 presents\nthe method of proﬁling radio stations. Section 4 discusses\ndifferent application scenarios for our technique. Sectio n 5\ngives a summaryand outlines futurework.\n2. Related Work\nThe two techniques underneath our proﬁling approach are\naudio descriptors for genre classiﬁcation and Self-Organi -\nzing Maps.\nFeature extraction for genre discrimination is a research\nareathathasseentremendousattentionwithinthelastyear s.\nWhile a state-of-the-art comparison is done in the annualMIREX evaluation [2], a broad overview upon existing au-\ndio descriptors for music classiﬁcation is given in [3, 4].\nIn this work we employ Rhythm Histograms and Statistical\nSpectrumDescriptors[5],bothbecausetheyperformedvery\nwell in the MIREX2005 evaluation [6] and because they\nare sufﬁciently efﬁcient for our approach. A feature set for\ngenre discrimination seems to be well-suited to distinguis h\ndifferentradiostations,nevertheless,wewillinvestiga tethe\nuse ofother feature setsforthisnovel kindof application.\nA Self-Organizing Map (SOM) is an unsupervised neu-\nral network providing a topology-preserving mapping from\nahigh-dimensionalinputspaceontoatwo-dimensionalout-\nputspace[7]. Theinputdata,inourcasetheaudiofeatures,\nare arranged on a two-dimensional grid whose units are it-\neratively activated, and thus “trained” by the input featur es.\nThisresultsinanorganizationwheresimilardataismapped\nclose toeach other, building clusters.\nThe earliest works that use Self-Organizing Maps to or-\nganize sounds, based on pitch, duration and loudness, date\nbackto[8,9]. In[10]MFCCsareusedforretrievalofsound\nevents from aSOM.\nAutomatic organization of music collections on SOMs\nhasbeenﬁrstdemonstratedin[11],andlaterin[12,13,14].\nAnother work on exploring music collections [15] uses\nAligned-SOMs, which allow for interactively changing the\nfocusoforganizationamongdifferentaspects,likee.g. ti m-\nbre or rhythm. In [16, 17] SOMs are applied to organize\nmusic at the artist level using artist information mined fro m\nthe web.\nIn[18]so-calledEmergentSOMsareemployedforvisu-\nalizationofmusiccollectionswhichareparticularlysuit able\nforcreatinglargemaps. Theusageofverylargemaps,how-\never,isnotappropriateinourcontext,asthiswouldpossib ly\nresultinatoodetaileddiscriminationofaudiofeatures,d is-\ncriminating parts of songs rather than radiostations.\nAnotherinterfacetoaudiocollectionshasbeenpresented\nwithMARSYAS3D[19],aframeworkthatcontains,among\nother visualizations, a 3D space with audio ﬁles mapped\naccording to timbre. Among audio from different sources\nalso clips recorded from FM radio have been visualized in\nMARSYAS3D. In [20] applications of the FastMap algo-\nrithmforvisualizingaudiosimilarityandimprovingbrows -\ning of music archives are discussed. Torrens et. al [21]\npresentnewinterfacesforexploringpersonalmusiclibrar ies\ninform of disc-and tree-map-based visualizations.\nIn [22] a semantic web application is described, that in-\ndexes(mines)websitesandretrievescollectionsofdataus e-\nful for a variety of applications. One of them is “Now Play-\ning!”, which shows the current song playing on a number\nof radio stations, based on the information found on the ra-\ndiostations’websites. Withourradiostationproﬁlingtec h-\nnique we can provide a “Now Playing” function based on\nthe content of the audio stream.3. GeneratingRadio Station Proﬁles\nThe program content of a selected set of radio stations is\norganized on a two-dimensional Self-Organizing Map, ac-\ncordingtosimilarityordissimilarityofaudiothatisplay ed.\nTo achieve this, we extract features from the audio signal\nreceived from the radio station, in order to be able torecog-\nnize the content of a radio station’s program. Research on\nMusic Information Retrieval has generated a wealth of de-\nscriptors (feature sets) for computing audio similarity. F or\nproﬁling radio stations we apply Rhythm Histograms and\nStatistical Spectrum Descriptors [5]. The feature extract ion\nalgorithm tries to replicate the human perception by incor-\nporatingpsycho-acoustics,andthusshouldbeclosetowhat\na human listener perceives while listening to radio station s.\nIt then computes descriptors containing information about\nthe audio spectrum and rhythmics contained in it. One fea-\nturevectoriscalculatedforevery6secondsreceivedfroma\nradio station’sbroadcast.\nA Self-Organizing Map (SOM) is an unsupervised neu-\nralnetworkthatmapsahigh-dimensionalfeaturespaceonto\na two-dimensional output space, in our case a rectangular\nmap. The neural network is trained by the feature vectors\nextracted from the audio. We collect an appropriate num-\nber of feature vectors (between 1000 and 4000) from each\nradio station and put them together as the training database\nfor the Self-Organizing Map. No information about which\nfeature vector belongs to which radio station is provided to\ntheSOMalgorithm. Weobtainamaporganizedbyacoustic\nsimilarity,whereeachfeaturevectorhasbeenmappedtothe\nmap unit best representing it.\nA radio station’s proﬁle is derived from the organization\nof its program content on the two-dimensional map. Nearly\nevery radio station serves speciﬁc user groups and thus has\na focus on a speciﬁc type of program, being classical mu-\nsic, pop music, rock music, report-based, etc. As a conse-\nquence,theradiostationwillalsohaveafocusonaspeciﬁc\nregion of the music map. The more focused a radio station\nis, the more concentrated it appears on the map. We com-\npute the frequency each radio station hits each unit on the\nmap. The graphical representation of this “hit histogram”\nreveals the areas in which a radio station is active. The vi-\nsual proﬁle is enhanced by taking the logarithm of the hit\nhistogram, in order to reduce elevated concentrations, and\nby subsequent smoothing. Such a representation of a radio\nstation’sprogramrecordedforseveralhourscontainsaver y\ngood reﬂection of the station’s focused music area. Many\nradio stations build clearly visible clusters, which immed i-\nately give an idea of the radio station’s concentration on a\ncertain music style (or show whether the station is mainly\nspeech-based). By comparing the visual proﬁles of several\nstations, the user gets an immediate overview on the cover-\nage of a radio station’s program, and if he or she will like it\nor not.Figure1. Overviewoftheradiostationmapcontaining18,600segme nts: thecoloredtessellationshowsthedistributionofthe8radio\nstations, the labels are explaining the acoustic organization (indepe ndently from radio station assignments); dashed lines indicate\nsmooth transitionsbetween thegenres.\nIfwereassignastation’snameasclasslabeltoeachfea-\nture vector, we can obtain a colored visualization, which\nshows the distribution of the radio stations on the map (c.f.\nFigure 1). Through acoustically exploring the map, one can\nﬁndoutratherquicklythelocationsofaggregationsofaspe -\nciﬁctypeofmusic. Thus,itispossibletoexplorethecover-\nage, or “horizon”, of aradio station’sprogram.\n4. ApplicationScenarios\nWe recorded the program of 8 Austrian radio stations and\nextracted audio features of every 6-second segment of the\nrecorded audio. We recorded a total of 31 hours program\nand consequently extracted about 18,600 feature vectors.\nThe radio station map (or RadioSOM) we created contains\n28x26units,whichmeansthateachunitonaveragecontains\n25 segments (which is about 2.5 minutes of program). We\ndidnotwanttocreatealargermapinthiscase,asthiswould\nresult in a much more detailed discrimination of audio fea-\ntures, which means that a single song (usually roughly 4\nminutes long) would be discriminated and distributed to a\nmuch larger extent.\n4.1. The RadioSOM\nTheRadioSOMwearepresentinginFigure1showsthemu-\nsical horizon of 8 Austrian radio stations. There are radio\nstationsthatareclearlyrepresentedbytwoorthreecluste rs,\nwhile others are more interweaved. The former are thosewith a strong focus of their program, e.g. on news reports,\nclassical music or rock music. The latter kind of radio sta-\ntionsaremostlythosebroadcasting“pop”music,a“genre”,\nwhichisinfactnotagenrebyitself,butrepresentsthebroa d\nspectrum of popular music from different decades. On the\nonehand westillcan makeoutdifferences intheareas cov-\neredby“pop”music. Ontheotherhand,mostpeoplewhose\nfavorite music is “pop” music are used to the broad variety\nthat it represents.\nIn order to provide a more detailed description of the ra-\ndiostationmapgiveninFigure1wehavemanuallylabeled\nthe image with areas of clearly distinctive sound character -\nistics. The image shows a Voronoi-like tessellation of the\nRadioSOM color-coding each radio station, in order to give\nan overview of the distribution of the 8 radio stations. Ev-\nidently, the goal is not to get a uniform class (color) sepa-\nration in this application, as many of the stations overlap i n\ntheir program (clearly visible from Figure 1). Note that in\ncontrast to this ﬁgure the visual proﬁles given in Figure 2\nalsoincludethefrequencyofmappingsperstationtoaunit.\nA rather large part of the map consists of recordings\ncontaining speech, which is not really surprising, as all of\nthe radio stations broadcast news, and some of them addi-\ntionally bring reports, sports, telephone games, and so on.\nAmong the radio stations with a clearly distinctive area is\nOE1, whose program consists of about 50 % classical mu-\nsic and 50 % spoken contributions, reports and news. This(a) RadioOE1: Classical music andspeech.\n (b) RadioOE3: popsongs.\n (c) RadioSoundportal: hiphopandrock.\nFigure2. Proﬁles of three of theeight stationson theRadioSOM.\nFigure 3. Proﬁle of a yet unknown radio station, Radio\nStephansdom,mapped on theRadioSOM.\nis clearly reﬂected on the map with one out of two clus-\nters containing all recordings with speech and the other one\ncontaining classical music. Note that the classical cluste r is\nactuallylocatedwithinthehugespeechcluster,neverthel ess\nit is clearly separated by strong borders (no smooth transi-\ntions).\nThelargearealabeled“speech”iscoveredeffectivelyby\nallradiostations. Thereasonwhythestationsareratherwe ll\nseparated and not completely mixed up is that with the fea-\nturesextractedfromtheaudiosignalthecomputerisableto\ndistinguishthevoiceofdifferentspeakers. Thus,thevari ous\nsub-areas of the speech cluster contain different pitches o f\nvoices, and besides distinguishing male and female speak-\ners, the computer also ﬁnds differences in the type of spo-\nken program: news, reports, weather, telephone calls, and\nothers. Commercials are located alongside the borderline\nbetween speech and music as the majority of them contain\nboth speech and music. A cluster with a particularly large\nnumberofcommercialshasbeenlabeledonthemapinFig-\nure 1.\nThepopclusteriscoveredby5oftheradiostations,pre-\ndominantly by OE3, but also by radio TIROL, partly by\nFigure 4. “Now Playing” proﬁle of 10 minutes of radio FM4.\nTheproﬁle showsactivity inthe hiphop/reggae/r’n’b area.\nFM4, by radio NOE and radio WIEN, with smooth tran-\nsitionsbothtowardstherockand“slowoldies”cluster. Two\nof our recorded stations - radio NOE and WIEN - have a\nstrong focus on oldies. “Schlager” music is an Austrian /\nGerman specialty: a kind of simply structured pop music\nwith a catchy harmonic melody and modest humorous or\nsentimental lyrics1. Many Austrian regional radio stations\nliketobroadcastthiskindofmusic,andradioBGLDseems\nto focus fully on it. Also radio NOE plays this kind of mu-\nsic,buttendsmoretooldies,similartoradioTIROL,whose\nmusic we ﬁnd in the middle of pop, rock and oldies. Radio\nFM4deﬁnesitselftobetheradiostationfor“alternativemu -\nsic” and combines elements from alternative rock, electron -\nics and also hip hop and reggae music. Radio Soundportal\nhastwoclearlyvisibleareas: hiphop(plusalittleelectro nic\nmusic) and rock (morespeciﬁcally “new metal”).\nMusicmapshavebeenpreviouslypresentedmainlywith\nthe intention to organize music archives (see Section 2).\nTheRadioSOMisfacingsomenewchallenges,oneofthem\nbeing the high number of situations where speech plays a\nrole. We have reports with moderately spoken text, while\n1seehttp://[de/en].wikipedia.org/wiki/Schlagernews are usually spoken much faster. There is text accom-\npanied by background music as in commercials, program\nannouncements, etc. There is even a cluster with jingles\non its own. There are phone games and interviews with\nphone callers, which means that the frequency bandwidth\nof the signal is narrowed under these circumstances. The\nRadioSOM also distinguishes segments with transitions in\nthe ﬂow of the program, i.e. a piece of music ends and the\nstation’s speaker starts to talk, or vice versa, and has aggr e-\ngated them in the lower right corner. This is especially con-\nvenientwhenusingthe“StickingToFavoriteMusic”feature\ndescribedinsub-section4.5,becausetheseparationoftho se\nsegments enables avoiding discontinuities.\n4.2. Obtaining Radio Station Proﬁles\nAs described in Section 3 we compute hit histograms as the\nbasis for a radio station’s proﬁle. Based on the RadioSOM\ndescribedinSection4.1wenowdescribethevisualproﬁles\nof three of the stations, depicted in Figure 2. The proﬁle of\nradioOE1showsthatthisstationisalmostequallyactivein\ntwo areas: The upper cluster belongs to the speech area of\nthe map whilethelower cluster represents classical music.\nRadio OE3 is thepop station among our radio stations\nand its proﬁle is thus almost entirely active in the upper\n(rather left) area. A few outlying peaks show that OE3\nmakes a few exceptions like playing for instance a hip hop\nsong. The small cluster on the lower center corresponds to\nasports report.\nTheproﬁleofradioSoundportalreﬂectsveryniceitstwo\nprogram focuses: The cluster on the right border belongs to\nthe rock area, the other cluster is the center of the hip hop\narea.\nInthisexamplewehaveshownthreeverydistinctivepro-\nﬁles. In the next sub-section we will see that similar radio\nstations willresultinsimilarproﬁles.\n4.3. Proﬁling Unknown Radio Stations\nLetussupposeweﬁndoutaboutanewradiostation,butdo\nnot yet know what kind of program it is broadcasting. With\nourtechniquewecanrecordtheradiostationforawhileand\nmap its characteristics on the already existing radio stati on\nmap. In this application scenario we do not train a new Ra-\ndioSOM, rather we want to see which kind of programs of\nthe spectrum given by the map we already know is covered\nby the new station. Put in short, we want to ﬁnd out the\nfocus(es) of thenew station.\nWe recorded 3 hours from Radio Stephansdom, seg-\nmented the recordings and extracted audio features, as we\ndid with the other radio stations before. Now, for each seg-\nment, we ﬁnd the optimal mapping on the RadioSOM, i.e.\nthe unit with the closest distance to the segment’s feature\nvector. Doing this with the 1800 segments we extracted\nfrom Radio Stephansdom, we get the proﬁle visualized in\nFigure 3. Immediately we recognize a strong overlap be-\ntweenthisproﬁleandtheoneofRadioOE1,depictedinFig-ure 2(a). Indeed, Radio Stephansdom also consists mainly\nof classical music and spoken contributions, both reﬂected\ninthe visual proﬁle.\nApplying this technique one can get a good impression\nof previously unknown radiostations.\n4.4. Now Playing\nProﬁling only the last e.g. 10 minutes of a radio station\nshows us the area of the RadioSOM where a certain station\nhas been recently active, and thus indicates which program\ntype the station is currently broadcasting. If we reduce the\ntime recorded for creating the proﬁle to 3 minutes or even\nonesingle6secondsegmentwegetkindofa“NowPlaying”\nfunction, showing exactly what a station is broadcasting at\nthe moment. This would constitute an audio-based equiva-\nlent to the “Now Playing” feature offered by semantic web\napplicationsmonitoringandintegratingthecurrentprogr am\nwebsiteofseveralradiostations[22]. Figure4showsa“ﬁn-\ngerprint” of what FM4 broadcast within the 10 minutes on\n2006-02-13, between 20:08 and 20:18 (CET). This activ-\nity proﬁle reveals that the station was mainly active in the\nhiphop/reggae/r’n’b area, with a few speech sections in be-\ntween. If this feature is performed in real-time the “Now\nPlaying”functioncreatesananimation-likelive-viewoft he\nradio station’scurrent type ofprogram.\n4.5. Sticking To Favorite Music\nWe can use the RadioSOM to narrow the music or program\nthat we want to listen to by selecting a speciﬁc area of the\nmap and telling the computer to play only stations that are\ncurrently in the focus of the selected area. This allows a\ncompletely novel access to choosing radio stations, always\nlistening to the favorite type of program or music instead of\nhaving to search for a radio station. Furthermore, with this\ntechnique we can not only limit the scope of music to listen\nto, it also may enable functions like skipping commercials\nand other parts of the program that one dislikes, by switch-\ning to another program with similar music. The “Sticking\nToFavoriteMusic”featureisnotyetimplementedasanau-\ntomaticreal-timefeatureinourapplication,butisoneoft he\nmain features tobe integrated next.\n5. Conclusions\nWe presented a method for creating proﬁles of radio sta-\ntions as well as creating radio station maps, which give an\noverview of the horizon of a group of radio stations. The\nvisual proﬁles allow quick discovery of the program diver-\nsity of new/unknown radio stations. Moreover, this method\nallows for identiﬁcation of the program that is currently\nbroadcastonaradiostationandprovidesanovelwayofse-\nlecting radio stations according to one’s preferences. The\napproach is useful for discovering new radio stations for\none’s personal taste,both online and traditional ones.We will investigate the selection of appropriate features\nto be extracted from radio stations improving the discrimi-\nnation of program types of different radio stations and ad-\ndressing a better distinction of the large number of station s\nbroadcasting “pop” music.\nWe would like to integrate on-the-ﬂy feature extraction\nand visualization into one application which then supports\nthe “now playing” function in real-time and allows ad-hoc\nselection of a favorite program area having the computer to\nselect automatically the station that best covers one’s per -\nsonal taste.\n6. Acknowledgments\nPart of this work was supported by the European Union in\nthe6. FrameworkProgram,IST,throughtheMUSCLENoE\non Multimedia Understanding through Semantics, Compu-\ntation and Learning, contract 507752.\nReferences\n[1] D.KopitzandB.Marks, RDS:RadioDataSystem . Artech\nHouse, November 30 1998.\n[2] “Annual Music Information Retrieval Evaluation eX-\nchange (MIREX),” Website, 2005, http://www.music-ir.org/\nmirexwiki/index.php/Main Page.\n[3] J. Downie, Annual Review of Information Science and Tech-\nnology. Medford,NJ:InformationToday,2003,vol.37,ch.\nMusicinformationretrieval, pp. 295–340.\n[4] R.Typke,F.Wiering,andR.C.Veltkamp,“Asurveyofmu-\nsic information retrieval systems,” in Proceedings of 6th In-\nternational Conference on Music Information Retrieval (IS-\nMIR 2005) ,London, UK,September 11-15 2005.\n[5] T.LidyandA.Rauber,“Evaluationoffeatureextractorsand\npsycho-acoustic transformations for music genre classiﬁca-\ntion,” inProceedings of the 6th International Conference on\nMusic Information Retrieval (ISMIR 2005) , London, UK,\nSeptember 11-15 2005,pp. 34–41.\n[6] “Music Information Retrieval Evaluation eXchange - audio\ngenre classiﬁcation,” Website, 2005, http://www.music-ir.\norg/evaluation/mirex-results/audio-genre/index.html.\n[7] T. Kohonen, Self-Organizing Maps , 3rd ed., ser. Springer\nSeries in Information Sciences. Berlin: Springer, 2001,\nvol. 30.\n[8] P. Cosi, G. D. Poli, and G. Lauzzana, “Auditory modelling\nand self-organizing neural networks for timbre classiﬁca-\ntion,”Journal of New Music Research , vol. 23, no. 1, pp.\n71–98, March1994.\n[9] B. Feiten and S. G ¨unzel, “Automatic indexing of a sound\ndatabaseusingself-organizingneuralnets,” ComputerMusic\nJournal, vol.18, no.3,pp. 53–65, 1994.\n[10] C. Spevak and R. Polfreman, “Sound spotting - a frame-\nbased approach,” in Proceedings of the 2nd International\nSymposium on Music Information Retrieval (ISMIR 2001) .\nIndiana University, Bloomington, IN, USA: Indiana Univer-\nsity, October 15-17 2001, pp. 35–36. [Online]. Available:\nhttp://ismir2001.indiana.edu/posters/spevak.pdf[11] A. Rauber and M. Fr ¨uhwirth, “Automatically analyzing and\norganizing music archives,” in Proceedings of the 5th Eu-\nropean Conference on Research and Advanced Technology\nfor Digital Libraries (ECDL 2001) , Darmstadt, Germany,\nSeptember 4-82001.\n[12] A. Rauber, E. Pampalk, and D. Merkl, “Using psycho-\nacoustic models and self-organizing maps to create a hier-\narchicalstructuringofmusicbymusicalstyles,”in Proceed-\nings of the 3rd International Conference on Music Informa-\ntion Retrieval (ISMIR 2002) , Paris, France, October 13-17\n2002, pp.71–80.\n[13] E. Pampalk, A. Rauber, and D. Merkl, “Content-based orga-\nnizationandvisualizationofmusicarchives,”in Proceedings\nof ACM Multimedia 2002 . Juan-les-Pins, France: ACM,\nDecember 1-62002, pp.570–579.\n[14] R. Neumayer, M. Dittenbach, and A. Rauber, “Playsom and\npocketsomplayer, alternative interfaces to large music col-\nlections,”in Proceedingsofthe6thInternationalConference\nonMusicInformationRetrieval(ISMIR2005) ,London,UK,\nSeptember 11-15 2005,pp. 618–623.\n[15] E. Pampalk, S. Dixon, and G. Widmer, “Exploring music\ncollections by browsing different views,” Computer Music\nJournal, vol.28, no.2,pp. 49–62,2004.\n[16] P. Knees, E. Pampalk, and G. Widmer, “Artist classiﬁction\nwith web-based data,” in Proceedings of the 5th Interna-\ntional Conference on Music Information Retrieval (ISMIR\n2004), Barcelona, Spain,October 10-14 2004.\n[17] E. Pampalk, A. Flexer, and G. Widmer, “Hierarchical or-\nganization and description of music collections at the artist\nlevel,” in Proceedings of the 9th European Conference on\nResearch and Advanced Technology for Digital Libraries\n(ECDL2005) , Vienna, Austria, September 18-232005.\n[18] F. M ¨orchen, A. Ultsch, M. N ¨ocker, and C. Stamm,\n“Databionic visualization of music collections according to\nperceptualdistance,”in Proceedingsofthe6thInternational\nConference on Music Information Retrieval (ISMIR 2005) ,\nLondon, UK,September 11-152005.\n[19] G.TzanetakisandP.Cook,“MARSYAS3D:Aprototypeau-\ndio browser-editor using a large-sclae immersive visual and\naudio display,” in Proceedings of the International Confer-\nenceonAuditoryDisplay(ICAD2001) ,Espoo,Finland,July\n29 -August 12001.\n[20] P. Cano, M. Kaltenbrunner, F. Gouyon, and E. Battle, “On\nthe use of fastmap for audio retrieval and browsing,” in Pro-\nceedings of the 3rd International Conference on Music In-\nformation Retrieval(ISMIR2002) ,Paris,France, 2002.\n[21] M.Torrens,P.Hertzog,andJ.L.Arcos,“Visualizingandex-\nploring personal music libraries,” in Proceedings of the 5th\nInternational Conference on Music Information Retrieval\n(ISMIR2004) , Barcelona, Spain,October 2004.\n[22] G. Gottlob, C. Koch, R. Baumgartner, M. Herzog, and\nS. Flesca, “The lixto data extraction project - back and forth\nbetween theory and practice.” in Proceedings of the 23rd\nACM SIGMOD-SIGACT-SIGART Symposium on Principles\nof Database Systems (PODS'04) . New York, NY, USA:\nACM Press,2004, pp.1–12."
    },
    {
        "title": "The Significance of the Non-Harmonic &quot;Noise&quot; Versis the Harmonic Series for Musical Instrument Recognition.",
        "author": [
            "Arie Livshin",
            "Xavier Rodet"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1416538",
        "url": "https://doi.org/10.5281/zenodo.1416538",
        "ee": "https://zenodo.org/records/1416538/files/LivshinR06.pdf",
        "abstract": "Sound produced by Musical instruments with definite pitch consists of the Harmonic Series and the non- harmonic Residual. It is common to treat the Harmonic Series as the main characteristic of the timbre of pitched musical instruments. But does the Harmonic Series indeed contain the complete information required for discriminating among different musical instruments? Could the non-harmonic Residual, the “noise”, be used all by itself for instrument recognition? The paper begins by performing musical instrument recognition with an extensive sound collection using a large set of feature descriptors, achieving a high instrument recognition rate. Next, using Additive Analysis/Synthesis, each sound sample is resynthesized using solely its Harmonic Series. These “Harmonic” samples are then subtracted from the original samples to retrieve the non-harmonic Residuals. Instrument recognition is performed on the resynthesized and the “Residual” sound sets. The paper shows that the Harmonic Series by itself is indeed enough for achieving a high instrument recognition rate; however, the non- harmonic Residuals by themselves can also be used for distinguishing among musical instruments, although with lesser success. Using feature selection, the best 10 feature descriptors for instrument recognition out of our extensive feature set are presented for the Original, Harmonic and Residual sound sets. Keywords: instrument recognition, musical instruments, residual, noise, harmonic series, pitch",
        "zenodo_id": 1416538,
        "dblp_key": "conf/ismir/LivshinR06",
        "keywords": [
            "Harmonic Series",
            "Non-harmonic Residual",
            "Instrument recognition",
            "Pitch",
            "Additive Analysis/Synthesis",
            "Resynthesized",
            "Feature descriptors",
            "Feature selection",
            "Musical instruments",
            "Noise"
        ],
        "content": "The Significance of the Non-Harmonic “Noise” Versus the Harmonic Series for \nMusical Instrument Recognition \nArie Livshin Xavier Rodet\nIRCAM Centre Pompidou \n1 place Stravinsky, Paris 75004, FRANCE \narie.livshin,xavier.rodet@ircam.fr \nAbstract \nSound produced by Musical instruments with definite \npitch consists of the Harmonic Series and the non-harmonic Residual. It is common to treat the Harmonic Series as the main characteris tic of the timbre of pitched \nmusical instruments. But does the Harmonic Series indeed contain the complete information required for discriminating among different musical instruments? Could the non-harmonic Residual, the “noise”, be used all by itself for instrument recognition? The paper begins by performing musical instrument recognition with an extensive sound collection using a large set of feature descriptors, achieving a high instrument recognition rate. Next, using Additive Analysis/Synthesis, each sound sample is resynthesized using solely its Harmonic Series. These “Harmonic” samples are then subtracted from the \noriginal samples to retrieve the non-harmonic Residuals. Instrument recognition is performed on the resynthesized and the “Residual” sound sets. The paper shows that the Harmonic Series by itself is indeed enough for achieving a high instrument recognition rate; however, the non-harmonic Residuals by themselves can also be used for distinguishing among musical instruments, although with lesser success. Using feature se lection, the best 10 feature \ndescriptors for instrument recognition out of our extensive feature set are pr esented for the Original, \nHarmonic and Residual sound sets. \nKeywords : instrument recognition, musical instruments, \nresidual, noise, harmonic series, pitch \n1. Introduction \nMusical instruments with definite pitch (“pitched \ninstruments”) are usually based on a periodic oscillator such as a string or a column of air with non-linear excitation. In consequence, their sound is mostly composed of a Harmonic Series of sinusoidal partials, i.e. frequencies which are integer multiples of the fundamental frequency ( f0). While the relation between the energy levels of the different harmonics is widely \nconsidered as the main characteristic of pitched \ninstruments’ timbre (e.g. [1]), if we subtract this Harmonic Series from the original sound there is a non-harmonic Residual left. This Residual is far from being 'white noise'; it is heavily filtered by the nature of the instrument itself as well as the playing technique, and may contain inharmonic sinusoidal partials as well as non-sinusoidal ‘noise’, such as the breathing sounds in the flute or the scraping noises in the guitar.  \nDoes the Harmonic Series i ndeed encapsulate all the \ndistinguishing information of the sounds of pitched musical instruments? If so, about the same instrument recognition rate should be achieved by using only the Harmonic Series as by using all the information in the signal, with the same feature descriptor set used for classification. This is a prac tical question for the field of \ninstrument recognition; when performing instrument recognition in multi-instrument al, polyphonic music, it is \ndifficult as well as computationally expensive to perform full source separation [2] and restore the original sounds out of the polyphonic mixture in order to recognize each \nsource separately. On the other hand, estimating the Harmonic Series of the different notes in the mixture is a relatively easier task [3]. Fo r example, in [4] Harmonic \nSeries estimation is used for performing “Source Reduction”, reducing the volume of all instruments except one and then recognizing it. In [1], instrument recognition is performed using only features based on the Harmonic Series, estimated from pre-given notes. There \nis also research attempting to perform instrument recognition by recognizing directly instrument mixtures, instead of trying to separate them into individual instruments, see for example [5]. \nAnother interesting question comes from the opposite \ndirection: is the non-harmonic Residual, the “noise” a musical instrument produces, so distinct as to allow \ndistinguishing between different instrument types, e.g. can we actually distinguish between different wind instruments just by the sound of their airflow hiss? \nIn order to answer these questions, the paper explores \nhow instrument recognition rates using signals resynthesized solely from th e Harmonic Series of the \nsound, and signals containing solely the non-harmonic \nPermission to make digital or hard copi es of all or part of this work for \npersonal or classroom use is grante d without fee provided that copies \nare not made or distributed for profit or commercial advantage and that \ncopies bear this notice and the full citation on the first page. \n© 2006 University of Victoria Residuals, compare with the recognition rates when using \nthe complete signals. In order to perform this comparison as directly as possible, the first step is to achieve a high instrument recognition rate. Th is is accomplished here by \ncomputing an extensive set of feature descriptors on a large and diverse set of pitched musical instrument sound samples, reducing the feature dimensions with Linear Discriminant Analysis (LDA) and then classifying the sounds with K-nearest neighbours (KNN).  \nNext, the Harmonic Series of each sample in the sound \nset is estimated, including the f0s, harmonic partials and \ncorresponding energy leve ls, and using Additive \nSynthesis all the signals are resynthesized using only their Harmonic Series, thus creating synthesized ‘images’ of the original signals which lack any non-harmonic information. These resynthe sized sounds will be referred \nto in the paper as “Harmonic” signals, while the original sounds from the sound set will be called, the “Original” signals. As the phase information of the Original signals is kept in the Harmonic signals, by subtracting the Harmonic signals from the Original signals we remain with the non-harmonic, “noisy”, part of the signals, referred to shortly as the “Residuals”. \nAfter that, the same set of feature descriptors is \ncomputed on each sample group: the Original, Harmonic and Residual Signals. These three groups are then divided separately into training and test sets and instrument recognition is performed on each group independently.  \nThe instrument recognition results are presented and compared in Section 7.  \nUsing the Correlation-based Feature Selection (CFS) \nalgorithm with a greedy stepwise forward search method, the 10 most important feature descriptors for each of the three groups of samples are estimated and presented. \n2. Original Sound Set \nThe sound set consists of 5006 samples of single notes \nof 10 “musical instruments”: bassoon, clarinet, flute, trombone, trumpet, contrabass, contrabass pizzicato, violin, violin pizzicato and piano. As the violin and bass pizzicato sounds are very different from the bowed sounds they are treated here as separate instruments. \nThe sound samples were collected from 13 different \ncommercial and research sound databases, all “well recorded” and practically l acking noise. The databases \ncontain sounds recorded in different recording environments, using different individual instruments (e.g. using different violins in  each sound database). The \nsound set spans the entire pitch range of each of the 10 instrument types and includes vibrato and non-vibrato sounds where applicable.  \nThe collection of all the samples of a specific \ninstrument taken from a single database (e.g. all the violin samples from database #1), is referred to in the paper as an “instrument Instance”. The total number of instrument \nInstances in the sound set is 77.  \nAll the sounds are sampled in 44 KHz, 16 bit, mono. \n3. Harmonic Sounds and Residuals \nAdditive analysis/synthesis is based on Fourier's theorem, \nwhich states that any physi cal function that varies \nperiodically with time with a frequency f can be \nexpressed as a superposition of sinusoidal components of frequencies: f, 2f, 3f, 4f, etc.  Additive synthesis applies \nthis theorem to the synthesis of sound [6]. For a review of supplementary Additive Synt hesis techniques see [7]. \nIn order to separate the sound samples into their \nharmonic and non-harmonic components, the samples are analyzed and then selectiv ely resynthesized using the \nAdditive analysis/synthesis program, “Additive” [8], which considers also inharmonic deviations of the partials (e.g. found in the piano sounds ).  Very precise Additive \nanalysis was performed by supplying the Additive program with specifically tailored parameters for each sound sample using its note name and octave, known in advance, for estimating its f0. For example, the Additive \nanalysis/synthesis window size was set to 4*(1/ f0), FFT \nsize to 4*nextpow2\n1(sampleRate * windowSize), etc.  \n0 2 4 6 8\nx 104-0.1-0.0500.050.10.15 original\n0 2 4 6 8\nx 104-0.1-0.0500.050.10.15resynthesized\n0 2 4 6 8\nx 104-0.1-0.0500.050.10.15residual\n \nFigure 1. Left to right: original  Clarinet sample (A3), the \nsample resynthesized from the Harmonic Series, the \nResidual (subtraction). \nFigure 1 shows an example of  an Original clarinet \nsample (the note A3), the sound we resynthesized from the Harmonic Series of the Original, and the non-harmonic Residual. We can see that the Original and Harmonic sound envelopes are similar and that the Residual energy, resulting from subtracting the resynthesized Harmonic sound from the Original, is comparatively very low.  \nWhile the sounds resynthesized from the Harmonic \nSeries sound very similar to the Original samples, the non-harmonic Residuals sound very differently from them while sounding quite similar to each other for the same instrument. For example, the clarinet Residual of the note \nA3 sounds like a steady airflow while the trombone Residual of the same pitch sounds mellower and with \n                                                          \n \n1 nextpow2(N) is the first P such that 2^P >= abs(N) addition of “static-electric ity” crackle. The bass pizzicato \nResidual of A3 sounds like a wooden barrel being hit with a hammer, while the Residual of the violin pizzicato of exactly the same note sounds much higher “pitched” due to the considerably smaller size of its wooden resonator, and includes some tremolo. To learn how the physical structure of musical  instruments shapes the \nsound, see [9]. Note that the Attacks are far from being the only parts of the Residuals influencing the descriptors; The Sustained parts of the Residuals of the clarinet, flute, trombone and trumpet contain energy levels as high as or higher th an their Attack Transients. \n4. Feature Descriptors \nThe same feature set2  is computed on the Original \nsamples, the Harmonic samples and the Residuals. \nIn order to encapsulate va rious characteristics of the \nsignals, the feature set is quite large and includes 62 different feature types. Many of these features include several variations using di fferent parameter types, \nresulting in a total of 513 different feature descriptor “flavors”. For example, Spectral Kurtosis “flavors” include Kurtosis computed on the linear spectrum, the \nlog-spectrum, the harmonics envelope, etc. After computation, the feature desc riptors are normalized to the \nrange of [0 - 1] using Min-Max Normalization. Except the features computed on the whole signal, most of the features are computed on the Short-Time Fourier Transform (STFT) of the signal, using a sliding frame of 60 ms with a 66% overlap. For each sample, the average and standard deviation of thes e frames are used as feature \ndescriptors. \n \nThe different feature types are: \n4.1 Temporal Features  \nFeatures computed on the whole signal (without \ndivision into frames), such as Log Attack Time, Temporal Decrease, Effective Duration, etc. \n4.2 Energy Features \nFeatures referring to the ener gy content of the signal, \nlike Total Energy, Harmonic En ergy, Noise-Part Energy, \netc. \n4.3 Spectral Features  \nFeatures computed from the Short Time Fourier \nTransform (STFT) of the signal, including the Spectral Centroid, Spectral Spread, Spectral Skewness, etc. \n4.4 Harmonic Features  \nFeatures computed from the Sinusoidal Harmonic \nmodeling of the signal, like f0, Inharmonicity, Odd to \nEven Ratio, etc. \n                                                          \n \n2 The feature computation routines were written by Geoffroy \nPeeters of IRCAM. Full feature list can be found in [10]. 4.5 Perceptual Features  \nFeatures computed using a model of the human \nhearing process, including Mel Frequency Cepstral Coefficients (MFCC), Loudness, Sharpness, etc. \n5. Feature Selection \nIn order to provide the 10 best features out of our \nextensive feature set for each group of samples (the Original samples, the Harmonic samples and the Residuals) the Correlation-based Feature Selection (CFS) evaluator is used with a gr eedy stepwise forward search \nmethod. The entropy-based CFS algorithm scores and ranks the “worth” of subsets of features by considering the individual predictive ability  of each feature along with \nthe degree of redundancy between them. Subsets of features that are highly corre lated with the class while \nhaving low intercorrelation are preferred. As the feature space is very large and checking all the feature combinations is not practical , CFS starts with an empty \nset and adds features using a stepwise forward search \nmethod, searching the space of feature subsets by greedy hillclimbing augmented with a backtracking facility. For further reading on CFS see [11]. In this paper, we use the WEKA data-mining software [12] implementation of the CFS algorithm. \n6. Classification and Evaluation \nInstrument recognition is performed on the Original, \nResynthesized and Residual sets  of samples separately.  \n6.1 Minus-1 Instance Evaluation \nIn order to get meaningful instrument recognition results \nit is necessary not to use sounds recorded by the same instrument and the same recording conditions both in the learning and test sets [13]. For this purpose, we introduce the ‘Minus-1 Instance’ cr oss-validation evaluation \nmethod: each instrument Instance is removed in its turn from the sound set\n3 and classified by all the remaining \nsamples. The recognition rate is computed per instrument type and is the average of th e grades of its Instances.  \n6.2 Classification \nEach classification phase of the Minus-1 Instance \nEvaluation begins by computing a Linear Discriminant Analysis (LDA) transformation matrix using the learning set. LDA [14] reduces the dimensionality of data with C classes down to C-1 dimensions while maximizing the distance between the means of  the different classes and \nminimizing the variance inside  each class (the Fisher \ncriterion). After dimension reduction, the test set is \n                                                          \n \n3 Reminder: our sound set is jo ined from samples originating \nfrom 13 different sound databases.  At each classification step, \nall the samples of one instrume nt from one of these databases \nare removed. classified by the learni ng set using the K-Nearest-\nNeighbors (KNN) algorithm. K values in the range of [1 - 80] are tested at each cla ssification phase. After the \nMinus-1 Instance evaluation process completes and all the Instances are classified , the best K for the whole \nclassification process is reported. \n7. Results \n7.1 Instrument Recognition \nThe confusion matrices in this section show the Minus-1 \nInstance recognition rates for the Original samples, the Harmonic samples and the Residuals. These matrices show the percentage of samples (rounded to integers) of the instruments in the first column which were classified as the instruments in the firs t row. For example in Table \n1, 8% of the clarinet samples are misclassified as flute. The instrument abbreviations are: bsn = Bassoon, cl = Clarinet, fl = Flute, tbn = Trombone, tr = Trumpet, cb = Contrabass, cbp = Contrabass Pizzicato, vl = Violin, vlp = Violin Pizzicato, pno = Piano. \n7.1.1 Original Samples \nTable 1. Confusion matrix of the Original samples \n bsn cl fl tbn tr cb cbp vl vlp pno \nbsn  95 0 1 3 0 0 0 0 0 0 \ncl 0 89 8 0 1 1 0 0 0 0 \nfl 0 4 94 0 0 0 0 1 0 0 \ntbn 2 0 0 94 2 0 0 1 0 0 \ntr 0 2 2 1 95 0 0 0 0 0 \ncb 0 0 0 0 0 98 0 1 0 0 \ncbp 0 0 0 0 0 0 98 0 0 2 \nvl 0 0 2 0 0 0 0 97 0 0 \nvlp 0 0 0 0 0 0 1 0 94 5 \npno 1 0 0 1 0 2 0 0 1 94 \n \nThe average Minus-1 Instance recognition rate per \ninstrument for the Original samples is 94.89% (using K=18 with KNN). It is rather hard to compare recognition rates with other papers as each paper attempts to recognize its own instrument set, uses different sound databases and different evalua tion techniques. In addition, \nmost papers on instrument recognition of separate tones are unfortunately using sounds from the same instrument Instances both in the learning and test sets, a fact which \nraises a strong doubt regardi ng the applicability of their \nresults, which are often unrealistically high [13]. Even so, while our results are obtained by Minus-1 Instance evaluation, they are still higher or comparable to most instrument recognition rates reported by papers on instrument recognition of separate tones, regardless of their evaluation techniques. In [13] for example, an average Minus-1 DB recognition rate of 83.17% for 7 instruments is achieved. It is interesting to note, that the main difference between the classification performed in \nthis section and the one we used in [13] is that our current sound set is much larger and more diverse. This \nexemplifies well an intuitive claim from [13], which states that enriching a learning set with sound samples from different databases im proves its generalization \npower. \n7.1.2 Harmonic Samples \nTable 2. Confusion matrix of the Harmonic samples \n bsn cl fl tbn tr cb cbp vl vlp pno \nbsn 93 2 0 1 1 2 0 0 0 0 \ncl 2 86 7 0 5 1 0 1 0 0 \nfl 0 7 89 0 1 0 0 3 0 0 \ntbn 5 0 0 84 8 2 0 0 1 0 \ntr 1 5 6 3 84 1 0 1 0 0 \ncb 1 0 0 0 0 96 0 2 0 0 \ncbp 0 0 0 0 0 0 98 0 1 1 \nvl 0 1 3 0 0 2 0 93 0 0 \nvlp 0 0 0 0 0 0 3 0 90 6 \npno 1 0 0 0 0 1 2 0 3 92 \n \nThe average Minus-1 Instance recognition rate per \ninstrument for the resynthesized samples is 90.53% (using K=4 with KNN). This recognition rate is only 4.36% lower than the rate achieved using the Original samples, and is still quite high. This rate shows that the information in the Harmonic Series of the signal is quite enough for achieving a high average instrument recognition rate which is rather close to the rate obtained using the complete signals. Comparing the confusion matrices of the Harmonic samples (Table 2) to the Originals (Table 1), we can see that the recognition rate \nof all the instruments has worsened somewhat, which \nconsistently indicates that some instrument-discriminating information was lost. The most  noticeable declines are the \ntrumpet (-11.25%) and the trombone (-9.42%).  \n7.1.3 The Residuals \nTable 3. Confusion matrix of the Residuals \n bsn cl fl tbn tr cb cbp vl vlp pno \nbsn 89 2 2 1 0 0 0 1 0 4 \ncl 1 53 24 3 9 3 0 4 0 4 \nfl 0 23 65 1 0 2 0 9 0 0 \ntbn 2 4 1 77 5 0 0 0 1 10 \ntr 2 29 12 0 56 1 0 0 0 0 \ncb 0 2 1 0 0 97 0 0 0 1 \ncbp 0 0 0 0 0 0 95 0 0 5 \nvl 0 9 13 0 0 0 0 76 0 1 \nvlp 1 1 1 0 0 1 0 1 86 8 \npno 3 0 0 1 0 3 2 2 3 85 \n \nThe average Minus-1 Instan ce recognition rate for the \nResiduals is 77.94% (using K=21 with KNN), which is 16.95% lower than the rate achieved with the Original samples. While this is a considerable difference, these results do indicate, perhaps surprisingly, that the Residuals by themselves (yes, these “airflow” and “click” \nsounds) contain considerable distinguishing instrument information. As this experiment did not involve any descriptors “tailored” specifically for the Residuals, it seems reasonable to expect th at the recognition rate could \nbe improved further. \nThe instruments with recognition rates reduced by \nmore than 25% compared with  the Original samples are \nthe clarinet (-36.56%), flute (-29.24%), confused mainly with each other, and the trumpet (-38.44%), also confused mostly with the clarinet and flute. \n7.2 Best 10 Feature Descriptors \nUsing CFS with a greedy stepwise forward search \nmethod, the best 10 feature descriptors were selected for each of the three sample groups out of the total 513 different feature descriptors in our feature set.  \nTable 4. 10 best features fo r the Original, Harmonic and \nResidual sample groups, selected using CFS. \nFeature Type  Descriptor Flavor ST O H R \nRel. Specific \nLoudness            2’nd  \nMel-Band m 1 1 1 \nTemporal  \nIncrease   2 2 X \nSpec. Kurtosis    log freq., \nnorm. db ampl. m 3 X 3  \nTemporal  \nCentroid   4 3 2 \nMFCC                 2’nd coefficient m 5 X X \nDelta-Delta  \nMFCC                 1’st m 6 X 9 \nSpec. Spread       lin. freq., \nnorm. db ampl. m 7 4 X \nTemporal  \nDecrease   8 6 X \nRoughness mean (ERBs)  9 X X \nBark-Band  \nTristimulus         lin. ampl,  \nbands(2+3+4)/sum(all) m 10 10 X \nBark-Band  \nTristimulus         norm. db ampl.,  \nband(1)/sum(all)  m X 5 X \nSpec. \nSkewness            lin. freq.,  \nnorm. db ampl. m X 7 X \nHarmonic  \nSpec. Roll-Off  m X 8 X \nFluctuation  \nStrength 7’th ERB m X 9 X \nSpec. Variation   norm. db ampl. s X X 4 \nMFCC                 4’th coefficient m X X 5 \nPerceptual  \nSpec. Kurtosis    orig. bands,  \norig. ampl. s X X 6 \nFluctuation  \nStrength mean (ERBs)  X X 7 \nBark-Band  \nTristimulus         quad. freq., \nsum(5:end)/sum(all) m X X 8 \nSpec. Kurtosis    lin. freq., \nnorm. db ampl. m X X 10The “Feature Type” column in Table 4 shows the \nfeature type, while the “Descr iptor Flavor” column shows \nthe parameter types used w ith this feature. “Feature \nType” column abbreviations: Rel. = Relative. spec. = spectral. “Descriptor Flavor” column abbreviations: freq. \n= frequency scale, ampl. = amplitudes, lin. = linear, orig. = original, quad. = quadratic, norm. = normalized. For a comprehensive description of all these features, see [10]. \nMost features are computed on each STFT frame of the \nsignal separately and then either the mean (‘m’) or the standard deviation (‘s’) of these frames is used. For such features, the “Frames” column  specifies which of these \nstatistics was used. The “O”, “S” and “R” columns indicate the Original sample group, the Harmonic samples (Synthesized) and the Residuals, and show which feature descriptors were selected for these sample groups and in \nwhich order of importance, from 1 to 10. An  X indicates \nthat a feature was not selected. \nThe recognition rates using only these sets of 10 \nselected feature descriptors are 71.18% for the Original \nsamples, 71.43% for the Harmonic samples and 64.48% for the Residuals.  \nThe Original samples “share” 6 descriptors with the \nHarmonic samples and 4 with the Residuals out of the selected 10. The Harmonic samples and the Residuals share among themselves only 2 descriptors: the Temporal Centroid and the Relative Specific Loudness in the 2’nd Mel-Band; these descriptors are also shared with the \nOriginal samples.  It is interesting to note that the Relative Specific Loudness in the 2’nd Mel-Band is the most prominent feature descriptor (#1) selected for all three \ngroups, and thus seems to be very appropriate for instrument recognition in general.  \nAlthough Table 2 shows that the Harmonic signals \ncontain most of the distinguishing instrument information (resulting in a recognition loss of only 4.36% compared with the Original signals), Table 4 shows that besides this small decrease in average r ecognition rate, removing the \nnon-harmonic residuals has also caused a somewhat different set of features to be selected by the feature \nselection algorithm. This indicates that the non-harmonic Residuals present in the Original signals do influence the instrument recognition process. \n8. Conclusions \nThe paper shows that using only information present in \nthe Harmonic Series of the signal is enough for achieving a high average musical inst rument recognition rate – \n90.53% for 10 instruments using Minus-1 Instance evaluation. This is only 4.36% less than the recognition rate obtained by using the complete, Original signals.  \nOn the other hand, Table 3 shows that there is a lot of \ndistinguishing instrument information present in the non-harmonic Residuals which by themselves produced an average instrument recognition rate of 77.94%. It was \nalso shown that the information present in the non-harmonic Residuals is not completely redundant to the information present in the Harmonic Series; Table 2 shows that although the averag e recognition rate of the \nHarmonic signals is high, some of the instruments have suffered noticeably from removing the non-harmonic \nResiduals, especially the trumpet and trombone. In addition, Table 4 shows that the 10 best feature descriptors selected for the Original sample set differ from the ones selected for the Harmonic samples. These results show that the sound of pitched musical instruments should not be treated as containing only the Harmonic Series, although most of the energy and distinguishing instrument information of the signal is indeed present in the Harmonic Series.  \n9. Future Work \nIt was shown that using only the harmonic series does not \nconsiderably lower the aver age instrument recognition \nrate although some instruments “suffer” more than others. This means that instrument recognition in polyphonic, multi-instrumental music could indeed be performed with rather high results without performing full source-separation; Using multiple f0 estimation algorithms (such \nas [2]), estimated harmonic partials could be used solely to classify musical instruments without losing too much distinguishing information.  \nIt might be possible to increase the instrument \nrecognition rate of the Residuals by specifically tailoring special feature descriptor s for them. Instrument \nrecognition of pitched instruments could then be improved by splitting the classi fied sounds into harmonic \nand non-harmonic components (when applicable) and computing special feature descriptors on the Residuals in addition to the feature descriptors computed on the original signal.  The splitting of the signal makes it easier to deal with the non-harmonic Residuals, due to their relatively low energy. \n10. Acknowledgments \nThis work is partly supported by the \"ACI Masse de \ndonnées\" project “Music Discover”. Thanks to Geoffroy Peeters for using his feature computation routines. References \n[1] J. Eggink and G. J. Brown.  “Instrument recognition in \naccompanied sonatas and concertos,” in ICASSP 2004 \nIEEE Int. Conf. on Acoustics, Speech, and Signal Processing Proc. , 2004, pp. 217-220. \n[2] E. Vincent and X. Rodet. “Instrument identification in \nsolo and ensemble music using Independent Subspace Analysis,” in ISMIR 2004 Fifth Int. Conf. on Music Inf. \nRetr. Proc ., 2004. \n[3] C. Yeh, A. Röbel and X. R odet. “Multiple fundamental \nfrequency estimation of polyphonic music signals,” in ICASSP 2005 IEEE Int. Conf. on Acoustics, Speech and Signal Processing Proc. , 2005. \n[4] A. Livshin and X. Rodet.  “Musical Instrument \nIdentification in Continuous Recordings,” in DAFx 2004 \n7’th international conference on Digital Audio Effects Proc.,  2004, pp. 222-227. \n[5] S. Essid, G. Richard a nd B. David. “Instrument \nrecognition in polyphonic music based on automatic taxonomies,” IEEE Transactions on Speech and Audio \nProcessing , Jan. 2006. \n[6] J. C. Risset. “Computer Music Experiments, 1964-…,\"  \nComputer Music Journal , vol. 9, no. 1, pp. 11-18, 1985.  \n[7] X. Serra and J. O. Smith. “S pectral Modeling Synthesis: A \nSound Analysis Synthesis System Based on a Deterministic plus Stochastic Decomposition,\" Computer \nMusic Journal , vol. 14, no. 4, pp. 12-24, 1990. \n[8] “IRCAM Real time applications - Additive and HMM,”  \nAvailable:http://www.ircam.fr/58.html?L=1&tx_ircam_pi  2%5BshowUid%5D=35&ext=2 \n[9] N. Fletcher and T. D. Rossing. The Physics of Musical \nInstruments, second edition . Springer, 1998. \n[10] G. Peeters, \"A large set of audio features for sound \ndescription (similarity and classification) in the CUIDADO project,\" 2003, Ava ilable:http://www.ircam.fr/ \nanasyn/peeters/ARTICLES/Peeters_2003_cuidadoaudiofeatures.pdf \n[11] M. A. Hall, Correlation-based feature selection machine \nlearning , Ph.D. Thesis, Department  of Computer Science, \nUniversity of Waikato, Ha milton, New Zealand, 1998. \n[12] I. H. Witten and E. Frank. Data Mining – Practical \nMachine Learning Tools and Techniques, second edition . \nMorgan Kaufmann Publishers (i mprint of Elsevier), 2005. \n[13] A. Livshin and X. Rodet. \"The Importance of Cross \nDatabase Evaluation in Musical Instrument Sound \nClassification,\" in ISMIR 2003 Fourth Int. Conf. on Music \nInf. Retr. Proc ., 2003. \n[14] G. J. McLachlan, Discriminant Analysis and Statistical \nPattern Recognition . New York, NY: Wiley Interscience, \n1992."
    },
    {
        "title": "Low Level Descriptors for Automatic Violin Transcription.",
        "author": [
            "Alex Loscos",
            "Ye Wang 0007",
            "Wei Jie Jonathan Boo"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1416020",
        "url": "https://doi.org/10.5281/zenodo.1416020",
        "ee": "https://zenodo.org/records/1416020/files/LoscosWB06.pdf",
        "abstract": "On top of previous work in automatic violin transcription we present a set of straight forward low level descriptors for assisting the transcription techniques and saving computational cost. Proposed descriptors have been tested against a database of 1500 violin notes and double stops. Keywords: Violin, Automatic Transcription.",
        "zenodo_id": 1416020,
        "dblp_key": "conf/ismir/LoscosWB06",
        "keywords": [
            "Automatic",
            "Violin",
            "Transcription",
            "Low",
            "Level",
            "Descriptors",
            "Assisting",
            "Computational",
            "Cost",
            "Database"
        ],
        "content": "Low Level Descriptors for Automatic Violin Transcription\nAlex Loscos \nMTG at Universitat Pompeu Fabra \nSoC at National University of Singapore \naloscos@iua.upf.edu \nloscos@comp.nus.edu.sg  Ye Wang \nNational University of Singapore \nSchool of Computing (SoC) \nSingapore 117543 \nwangye@comp.nus.edu.sg Wei Jie Jonathan Boo \nNational University of Singapore \nSchool of Computing (SoC) \nSingapore 117543 \nweijie.boo@gmail.com \nAbstract \nOn top of previous work in automatic violin transcription \nwe present a set of straight forward low level descriptors for assisting the transcription techniques and saving computational cost. Proposed descriptors have been tested against a database of 1500 violin notes and double stops. \nKeywords : Violin, Automatic Transcription. \n1. Introduction \nAutomatic transcription is a problem that has been \naddressed using many different approaches [ 1]. Most of \nthese tackle the problem  from an instrument-free \nperspective such as ‘music transcription’ or ‘monophonic and polyphonic transcription’. And among the ones that specialize on specific sections (drums and percussion) or instruments (piano) very few focus on violin [2][3][4].  \nIn this context, this paper aims a step forward in the \nimplementation of a clear-cut violin transcription system first described in [5], originally thought to be used for distant education, self-learning and evaluation. We introduce a set of low level descriptors by which the system can improve its performance and adapt the complexity of the analysis  algorithms that are being \napplied. The general block diagram of the system is represented in Figure 1.  \nFollowing sections introduce improvements in the pitch \nestimation; present first derivative zero crossing descriptor and modulation descriptor for upper octave polyphony \ndetection; and inharmonic descriptor for any other duo-phony detection. So once the p itch analysis receive a note, \nstability descriptor decides which frames to use for the pitch estimation, and inharmonic descriptor decides whether pitch estimation algorithm deals with a single note or a double-stop region. For those cases in which monophonic pitch analysis has been applied, the upper octave descriptors, one of them using the already estimated pitch, decide whether the upper octave note has to be given as transcription output as well.  \n \nFigure 1. General block diagram  of the automatic violin \ntranscription system where dotted lines represent descriptors, \ntriangles represent controls and rounded-end lines represent \noutputs  \n2. Note Level Segmentation \nNote Level segmentation uses implementation from \nprevious system [5] based on the autocorrelation of the Note Spectrogram. While this approach is efficient sorting out monophonic pitch changes, it sometimes lacks of resolution for detecting tim bre modulations, amplitude \nmodulations, and pitch modulations. This, far from being a drawback, allows the note level segmentation to work free from such kind of modulations, which enrich the sound but do not define the note itself. However, specific cases such \nPermission to make digital or hard copi es of all or part of this work for \npersonal or classroom use is grante d without fee provided that copies \nare not made or distributed for profit or commercial advantage and that \ncopies bear this notice and the full citation on the first page. \n© 2006 University of Victoria as note repetitions, fast perform ance, or deep modulations \nrequire additional processing.  \n3. Pitch Analysis \nPrevious system [5] already confronted the problem of \noctave errors by means of adding a compression term ruled by parameter α (set to 5 in [5]) in the summation of the \nSemitone Band Spectrum (SBS) p(w) as formulated in \nequation:  \n() ( ) () ()∑\n=⋅⋅ =5\n1, min\nkwp wkp wA α                (1) \nThis compressed addition has been proven useful for \ngetting rid of lower octave errors. When w is set to be F0/2 \n(being F0 the pitch), we add the real fundamental and \nharmonics up to a certain proportional value. If no peak is at F\n0/2 no relevant energy will is added to A(w) . However, \nas we discuss in section 3.1.2, this technique does not get rid of upper octave errors  \n3.1 Improved Pitch Estimation \nA couple of generic modifications (both for monophonic \nand polyphonic pitch estimation) are introduced: steady state detection for reducing computational cost and avoiding transients, and octave error pruning. \n3.1.1 Stability Descriptor \nFor each of the note regions  given by the note-level \nsegmentation, a stability descriptor is computed so that the pitch estimation is performed using only the most stable sub-segment of the note. This selection is done by means of the inverse correlation descriptor [5].  \n3.1.2 Upper Octave Pitch Error \nPitch estimation of monophonic signals is a problem that \ncan be considered solved. However, upper octave errors can occur when analyzing the monophonic note samples using the formulation presented in [5]. These errors take offset values of 12 or 19 semitones which correspond to the second and the third harmonic respectively. \nIn order to solve such errors, a new term is added in the \nsummation of the harmonics in the SBS. This term punishes the energy in the sub-harmonics frequencies using the following expression: \n() ( ) () ()\n⎟⎟\n⎠⎞\n⎜⎜\n⎝⎛⎟\n⎠⎞⎜\n⎝⎛ ⋅−−⋅+⎟\n⎠⎞⎜\n⎝⎛−⋅⋅+⋅⋅ =\n∑ ∑∑\n= ==\n3\n14\n15\n1\n32\n3 2, min\nk kk\nw wwkpwwkpwp wkp wA\nβα\n       (2) \nAfter running several experiments, β was set to 3. With \nsuch value we have achieved error-free pitch estimation for monophonic notes. \n \n \nFigure 2. Local view of spectrums for typical pitch errors: \nB3 second harmonic confusion error (upper figure), and F4 \nthird harmonic confusion error (lower figure). Black dots \nshow mistaken fundamental and harmonics, empty dots show \nthe rest of original harmonic peaks.  \n3.2 One Octave Duo-phony Detection \nOur earlier system did not consider polyphonies in which \none of the notes was an octave higher or lower from the other. The difficulty of pitch detection in such cases is that there is 100% overlap of harmonics, i.e., the harmonic spectra of the higher octave note hides under the lower octave note harmonics. In order to take such cases into consideration, we propose to include a detector in charge of resolving whether the note being considered was played together with its upper octave note or not.    \nOur octave duo-phony detector is based on the analysis \nof the magnitude spectrum modulation around the even harmonics ( 2*k*F\n0 for k=1, 2, 3,.. ), and the zero-crossing \nfactor of the first derivative of the low-pass filtered waveform. \n3.2.1 Modulation Descriptor \nSince the violin is not a perfect tuned instrument [1], the \nassumption is that whenever we have an octave distance \nduo-phony, even harmonics will suffer from amplitude and frequency modulations because of the frequency juxtaposition. The modulation descriptor is formulated as the mean value: \n∑∑ ⋅ ∆=ram AvSpectrogram AvSpectrog am FSpectrogrMdt     (3) \nwhere AvSpectogram  is the spectrogram averaged along \ntime and FSpectrogram  is the spectrogram of the input \nsignal filtered by an adaptive FIR comb filter which has zeros placed over the funda mental frequency and odd \nharmonics, as shown in Figure 3.  \n()()⎟⎟\n⎠⎞\n⎜⎜\n⎝⎛\n⎟⎟\n⎠⎞\n⎜⎜\n⎝⎛\n⎥⎦⎤\n⎢⎣⎡\n⋅−+⋅=\n02 21\nFFnxnx nys          (4) \nwhere Fs is the sampling frequency and F0 is the \nestimated pitch.  \nFigure 3. Local view (lower spectra) of the F0 dependent \ncomb filter (dotted line) and the resulting filtered violin \naverage spectra (log magnitude versus frequency index) \n \nFigure 4. Amplitude/Frequency modulation along notes \n(A#3, A#4, A#5, A3, A4, A5, B3, B4, B5, C#4, C#5, C4, C5, \nD#4, D#5, D4, D5, E4, E5, F#4, F#5, F4, F5, G#4, G#5, G3, \nG3, G4, G5) for duo-phonic (+), and solo lower note (.)  \n \nFigure 4 shows the values of our modulation descriptor \nalong our octave duo-phony recordings. In average terms the modulation descriptor obtained in the octave duo-phonic notes (~1.3e-4) are six times the values obtained for the solo notes (~2.2e-5). Note that the modulation parameter has a strong impact on performance. Indices 26 and 27 refer to the same note (\nG3) but performed in a \ncompletely different way; for index 26 the upper octave is extremely subtle and very well-tuned.  \n3.2.2 First Derivative Zero -Crossing Descriptor \nThe zero-crossing factor of the first derivative of the \nwaveform counts the number of times a signal changes from decreasing to increasing and vice versa. After removing some high frequency components using a low-pass pre-filtering, the descript or will be proportional to the \nhigher pitch, assuming the signal is a summation of two components an octave apart.  \n \nFigure 5. Two sets of parame ters obtained from 29 files \n(A#3, A#4, A#5, A3, A4, A5, B3, B4, B5, C#4, C#5, C4, C5, \nD#4, D#5, D4, D5, E4, E5, F#4, F#5, F4, F5, G#4, G#5, G3, \nG3, G4, G5) for duo-phonic (+), and solo notes(.) \n \nResults shown in Figure 5 use a 10 point average filter. \nThe mean descriptor value obtained in the octave duo-phonic notes (~0.15) is around twice the mean value obtained for the solo notes (~0.08). Lowest values such as the one obtained for note 10 (C#4) most of the times are \ndue to an extremely soft upper octave note, being this note only distinguishable at the release or transition.  \n3.3 Duo-phony Detection for Double Pitch Estimation \nThe harmonic descriptor is for guiding the pitch estimation \nprocess by telling whether the input note is monophonic or polyphonic. Based on this discrimination, simple or more demanding pitch estimation techniques are applied accordingly. With the harmonic descriptor, octave distance \nduo-phonies should be detected as monophonic. \nThe inharmonic descriptor is based on the spectrum \npeaks distribution along frequency axis. While most existing harmonic descriptors assume a prior knowledge of the pitch, our method is “blind”. The descriptor picks first eight most prominent magnitude spectral peaks and measures the divisibility among frequency distances defined by all possible different pairs of them. \nThe spectral peak detecti on uses a modification of \nPickPeaks  procedure from [6]. Only those peaks above a \nlower frequency boundary, which is set to the lowest possible violin pitch, are considered, and all peaks below an adaptive noise threshold are discarded. Being PeakFreq  \nthe vector containing sorted frequency positions of the peak, the descriptor can be formulated as: \n() { }\n() { }∑∑\n==⎟⎟\n⎠⎞\n⎜⎜\n⎝⎛\n∆∆\n=8\n18\nii j ff\nj PeakFreqi PeakFreq\nres InHd             (5) \nwhere  ()[]xyxy yxres −⋅= . \nMost non-duo-phony notes with high inharmonic score \noccur with the highest violin notes, where the bow noise becomes more significant and spectral peaks become more \ndistant among them. Consequently, some parasite peaks are gathered mistakenly. \n \nFigure 6. Inharmonic descript or obtained from 478 files \nfor duo-phonic (+), and monophonic (.) notes.  \n \nPitch estimation technique presented in section 3.1 does \nnot fit into duo-phonic pitch analysis well. Formulations such as (1) and specially (2) do not make sense anymore since F\n0/2 and F0/3 might confuse with harmonics of the \nsecond note. Better results can be achieved by trying to get two pitches at one step rath er than two step analyses \n(analysis, subtraction, and analysis) similar to [7]. 4. Vibrato Analysis \nViolin vibrato is produced by a periodic rolling motion of \na finger on the string, which results in a frequency modulation. Due to instrument body resonance, the frequency modulation (FM) ge nerates also amplitude \nmodulations (AM) in each par tial [8]. FM and AM nearly \nalways coexist in all musical instruments in a way that it is impossible to have frequency vibrato without amplitude \nvibrato but not vice versa [8]. In the case of violin vibrato, AM seems to be perceptually more relevant than FM [9]. One may then assume AM to be the natural feature for \nvibrato detection. However, experiments show there is strong correlation between fundamental’s FM and partials’ FM while no such correlation appears for AM. \nBecause of above considera tions, the vibrato analysis \nproposed for our automatic transcription uses FM to determine vibrato presence while employing both FM and AM to characterize its manife stations. Our current vibrato \nanalyzer is implemented based on [10], where the so-called Time-Varying Amplitude (TVA) and the Time-Varying Frequency (TVF) for the j\nth harmonic at the mth time frame \nis calculated by formulas: \n ()2∑=\njjm m\nn fS TVA                   (6) \n() ()2 2∑ ∑⋅=\njjm\njjm\njm\nn fS fSf TVF        (7) \nwhere in our adaptation, j takes values between 0 and \n4, Sm(f) is the spectrum of frame m, and fj (being the \nfundamental) covers, for every index j, a range of 100 \ncents centered around  the jth harmonic frequency.  \n \n \n \nFigure 7. TVF  (left column) and TVA  (right column) for the \nfirst four harmonics of an A4, performed with vibrato. \nDashed lines indicate bounds of vibrato regions.  \n5. Concluding Remark and Future Work \nThis paper has presented the current state of an ongoing \nresearch project. For our intended application, we have studied the problem of note-level segmentation, pitch estimation, and vibrato analysis. Experience tells us reliable note-level segmentation based solely on audio signal is a very challenging goal to achieve. Preliminary results show that it is possible to improve note-level segmentation with the help of visual cues from associated video clips. This approach is currently under progress. We \nalso plan to include dynamics attribute extraction in future releases since its importance in the educational context, \nespecially for advanced students. Regarding pitch estimation, although accuracy has been improved due to our modifications on previous system. We believe it should allow a non-fixed scale analysis, due to the fact that amateur students can perform significantly out of tune. \nReferences \n[1] Klapuri, A, “Automatic music transcription as we \nknow it today,” Journal of New Music Research, Vol. 33, No. 3, pp. 269-282, Sep. 2004. \n[2] Yin, J., Wang, Y., and Hsu, D., “Digital Violin \nTutor: An Integrated System for Beginning Violin Learners” ACM Multimedia, Hilton, Singapore, 2005. \n[3] Wilson, R. S., “First Steps Towards Violin \nPerformance Extraction using Genetic Programming”, In John R. Koza editor, Genetic Algorithms and Genetic Programming at Stanford 2002, pages 253-262, Stanford, California, 2002. \n[4] Krishnaswamy, A. and Smith, J. O., “Inferring \ncontrol inputs to an acoustic violin from audio spectra”, in Proceedings  of the International \nConference on Multimedia Engineering, New York,2003, IEEE Press. \n[5] Boo, J. Wang, Y., Loscos, A., “A Violin Music \nTranscriber for Personalized Learning”, in Proceedings of the ICME06 Conference, Toronto, Canada, 2006. \n[6] Zölzer, U., et al. “DAFX - Digital Audio Effects’, \nISBN: 0-471-49078-4, John Wiley & Sons, 2002, Chapter 11. Availabl e: http://www2.hsu-\nhh.de/ant/dafx2002/DAFX_Book_Page/matlab.html \n[7] Fujishama, T., “Real-time chord recognition of \nmusical sound: A system using Common Lisp Mu-sic”, in Proceedings of th e International Computer \nMusic Conference, Beijing, 1999 \n[8] Rossing, T.D., Fletcher, N. H., “The Physics of \nMusical Instruments”, Springer, 2nd edition, 1998 \n[9] Järveläinen, H., “Perception-based control of vibrato \nparameters in string instrument synthesis”, in Proc. International Computer Mu sic Conference, Sweden, \n2002 \n[10] Liu, N., “Vibrato analysis and synthesis for violin: an \napproach by using energy density spectrum analysis”, internal report, National University of Singapore, 2005"
    },
    {
        "title": "Separating voices in MIDI.",
        "author": [
            "Søren Tjagvad Madsen",
            "Gerhard Widmer"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1414752",
        "url": "https://doi.org/10.5281/zenodo.1414752",
        "ee": "https://zenodo.org/records/1414752/files/MadsenW06.pdf",
        "abstract": "This paper presents an algorithm for converting midi events into logical voices. The algorithm is fundamentally based on the pitch proximity principle. New heuristics are intro- duced and evaluated in order to handle unsolved situations. The algorithm is tested on ground truth data: inventions and fugues by J. S. Bach. Due to its left to right processing it also runs on real time input. Keywords: Voice separation, Stream separation",
        "zenodo_id": 1414752,
        "dblp_key": "conf/ismir/MadsenW06",
        "keywords": [
            "algorithm",
            "midi events",
            "logical voices",
            "pitch proximity principle",
            "heuristics",
            "unsolved situations",
            "ground truth data",
            "J. S. Bach",
            "left to right processing",
            "real time input"
        ],
        "content": "Separating voices in MIDI\nSøren Tjagvad Madsen\nAustrian Research Institute for Artiﬁcial Intelligence\nFreyung 6/6\nA-1010 Vienna, Austria\nsoren.madsen@ofai.atGerhard Widmer\nDepartment of Computational Perception\nJohannes Kepler University, Altenbergerstraße 69\nA-4040 Linz, Austria\ngerhard.widmer@jku.at\nAbstract\nThis paper presents an algorithm for converting midi events\ninto logical voices. The algorithm is fundamentally based\non the pitch proximity principle. New heuristics are intro-\nduced and evaluated in order to handle unsolved situations.\nThe algorithm is tested on ground truth data: inventions and\nfugues by J. S. Bach. Due to its left to right processing it\nalso runs on real time input.\nKeywords: V oice separation, Stream separation\n1. Introduction\nV oice separation or stream separation is the task of dividin g\na set of notes into most likely voices or auditory streams as\nit was coined by Bregman [1]. Adding voice information\nto notes is essentially adding structure to complex data. An\nobvious application that can beneﬁt from a voice separation\nalgorithm is music transcription, but a reliable stream sep a-\nration algorithm could also be useful in music analysis sys-\ntems. For MIR systems based on monophonic techniques –\nsuch MIR applications could be query by humming or theme\nﬁnding – voice information is a prerequisite. This particul ar\nseparation algorithm is intended to be used for online analy -\nsis of performed music.\n2. Related work\nA handful of existing approaches for separating voices has\nbeen published.\nCambouropoulos [2] and Kilian and Hoos [3] propose\nalgorithms aiming at transcription applications. The latt er\nmethod uses a stochastic local search based on a paramet-\nric cost function. The main feature of this approach is the\nability to assign chords to a single voice.\nOther algorithms aim at being able to reproduce the voices\nas they are perceived or exactly as they were written in the\nscore. Kirlin and Utgoff [4] propose a data driven approach\nwhere a same-voice predicate is learned and later used to\nseparate new music.\nPermission to make digital or hard copies of all or part of this w ork for\npersonal or classroom use is granted without fee provided th at copies\nare not made or distributed for proﬁt or commercial advantage an d that\ncopies bear this notice and the full citation on the ﬁrst page .\nc/circlecopyrt2006 University of VictoriaChew and Wu [5] attack the problem by splitting the\npiece into ‘contigs’ where the number of voices is constant.\nContigs are then combined using pitch proximity.\nTemperley [6] lists well-formedness rules and preference\nrules (GTTM style [7]) and uses dynamic programming to\nminimize the cost function balancing the preferences. Our\nmethod is highly inspired by this approach as we shall see\nin Section 4.\nThe algorithms differ in the way they determine the num-\nber of voices they will produce. The methods described by\nCambouropoulos, and Chew and Wu use as many voices as\nthere are notes in the largest chord in the input ﬁle. Kilian\nand Hoos require an input value of desired maximum num-\nber of voices voices, whereas Kirlin and Utgoff and Tem-\nperley decide this dynamically.\nIn order to be able to do real time voice separation, we\nwill pursue the approach of ‘left to right’ processing, as\nwell as dynamically opening and closing voices as they are\nneeded.\n2.1. Gestalt principle of proximity\nAll methods are in some way or another dependent on the\nprinciple of pitch proximity when grouping notes into se-\nquences.\nIf two note sequences are played simultaneously but sep-\narated somewhat in pitch, we tend to perceive them as two\nseparate voices – the notes sequentially group together in\ntwo melodies. This is even true when the notes are played\nin alternation (see Figure 1). Pseudo-polyphony is the term\nfor this phenomenon when one sequence of notes gives the\nimpression of two melodies. The impression weakens when\nthe sequence is played slowly (longer time between note on-\nsets). The impression increases when the pitch separation i s\ngreater. Huron reports limit values for when events in a se-\nquence will separate or fuse [8].\n3. Quantizing the MIDI events\nA MIDI ﬁle contains events with information about note on-\nsets and offsets – we need to construct notes from that infor-\nmation. The algorithm processes events in groups of notes\napproximately beginning at the same time. MIDI events are\nprocessed in their time stamp order. Notes having onsets\nseparated by no more than 35 ms are assumed to onset at the\nsame time and are treated as a chord (onset group).\u0000 \u0001\u0002\n\u0001\u0002\n\u0001\u0002\n\u0001\u0002\n\u0001\u0002\n\u0001\u0002\n\u0001\u0002\n\u0001\u0002\n\u0001\u0002\n\u0001\u0002\n\u0001\u0002\n\u0001\u0002\n\u0001\u0002\n\u0001\u0002\n\u0001\u0002\n\u0001a)\n\u0000\u0002\u0003 \u0004\u0005\n\u0004\u0005\n\u0004\u0005\n\u0004\u0005\n\u0004\u0005\n\u0004\u0005\n\u0004\u0005\n\u0004\u0005\n\u0004\u0005\n\u0004\u0005\n\u0004\u0005\n\u0004\u0005\n\u0004\u0005\n\u0004\u0005\n\u0004\u0005\n\u0004b)\n\u0003\u0005\nFigure 1. When two pitch sequences are interleaved, the two\nsequences will seem to fuse into a single stream if they are close\ntogether in pitch (b); otherwise they will seem to form two in-\ndependent streams (a) [6].\nThe algorithm processes the groups of onsetting notes in\ntime order. Note that only notes onsetting (approximately)\nsimultaneously are member of the same group. Notes sus-\ntained from past onset groups are not included. Such notes\nare said to be busy at group time tg(tgis the start time of the\nﬁrst onsetting note in the group) when they have not ended\nbefore 80 ms after tg. We shall use this later to determine if\na voice is occupied by a note at any given group time.\nWhen the input is a ﬁle, all notes are constructed at the\nbeginning and afterwards the groups are determined and the\nseparation begins. From real time input notes and groups\nare created on the ﬂy.\n4. The algorithm\nInspired by Temperley’s well-formedness rules we will stat e\nsome requirements the algorithm must assure:\n•Each note must be assigned to exactly one voice\n•A voice must not contain overlapping notes (must con-\nsist of temporally contiguous notes)\nV oices are not allowed to contain notes sounding at the\nsame time – we want the resulting voices to be entirely\nmonophonic melodies. Furthermore the following will be\npreferred:\n•Minimize leaps between notes in all voices\n•Minimize the number of voices\n•Minimize the number of rests within a voice (end a\nvoice instead of introducing long rests)\nIn contrast to most of the existing algorithms voice cross-\ning is not prohibited. Making voices cross from one onset\ngroup to the next is always more costly than making them\ncontinue the shortest path to the next notes (when the notes\nin each group have distinct pitches) and will thus not be a\npreferred choice. We shall later return to the issue of han-\ndling groups containing equal pitched notes.\nThe algorithm is implemented with a voice conﬁguration\nunit generating valid (well formed) solutions and a note as-\nsignment unit calculating the preferred-ness of a solution us-\ning a parameterized cost function. There is a cost related tostarting and ending a voice, inserting a rest (not assigning\na note to an open voice) and there is a cost function of the\nleap size (for example the number of semitones). The hope\nis that the cheapest solution – given appropriate parameter s\n– now corresponds to the correct one.\nThe optimal voice conﬁguration and note assignment of\na group giis highly dependent on the surrounding groups.\nWe cannot consider the entire search space at once. Instead\nthe separation happens iteratively with a small lookahead.\nWhen determining the best conﬁguration and assignment\n(ca) forgiwe consider for example all possible conﬁgura-\ntions and assignments for gi,gi+1, andgi+2. Each cahas a\ncost so the total cost of the lookahead is ca(gi)+ca(gi+1)+\nca(gi+2). The caforgileading to the cheapest solution\nwhen including the lookahead is ﬁnally applied.\nIn a given iteration, the well-formedness part of the algo-\nrithm makes sure that enough voices will be open (at least\nas many voices as there are notes in the group have to be\nopen). Different conﬁgurations of incrementally opening\nmore voices (up to the size of the group) and closing voices\nthat are not needed are all evaluated.\nThe notes in gthen have to be assigned to the open voices.\nRests are added to open voices that have not been assigned\nany note (in the case there are more voices than notes to be\nassigned). All possibilities of assigning notes to the voic es\nare evaluated. The cost of assigning a note to a voice is\nthe pitch difference (or a function of the pitch difference)\nbetween that note and the previous note in the voice. The\ncost of assigning a rest is proportional to the duration of th e\ngroup; the duration of a group is the time span to the follow-\ning group.\nThe search algorithm uses a ‘branch and bound’ heuristic\nto prune the search space. When a solution is found, its\ncost is stored. Other possible solutions are not evaluated t o\nthe end if their cost-so-far already exceed the best solutio n’s\ncost.\n5. Evaluation methods\nWe have tested the algorithm mainly on the 15 two and 15\nthree part inventions by J. S. Bach (BWV 772-801) as well\nas the 48 fugues from the Well Tempered Clavier (BWV\n846-893) from the same composer – the ‘chewBach’ data\nset also used by Chew and Wu in [5]. The inventions were\nobtained from www.bachcentral.com and the fugues from\nwww.musedata.org . V oices are stored on different tracks in\nthe midi ﬁle, providing the ground truth.\nThe separation algorithm assigns notes to voices. The\nquestion is now how well the voices correlate to the actual\nvoices in the input data. Two evaluation procedures will\nbe used: soundness andcompleteness as suggested by Kir-\nlin [4].\nSoundness is calculated by running through the notes in\nthe ground truth streams, and for each adjacent pair deter-\nmine if the notes were really assigned to the same voice.Table 1. Evaluation in soundness and completeness\nExp. Soundness (errors) Completeness\n1 94.25 % (4077) 66.70 %\n2 94.34 % (4013) 69.47 %\n3 94.79 % (3692) 73.76 %\n4 97.21 % (1979) 67.83 %\n5 97.44 % (1812) 71.31 %\n6 97.58 % (1717) 71.58 %\nA percentage will be returned. Soundness thus counts the\npercentage of ‘right transitions’ between notes.\nCompleteness on the other hand indicates how well the\nalgorithm actually assigns notes from the same voice to the\nsame stream. To give an example: consider two ground\ntruth melodies consisting of 50 notes each. They do not\ncross. If a separation makes the streams cross exactly af-\nter the 25th note in each stream (let’s assume that’s pos-\nsible), and otherwise makes no errors, soundness will be\n97.96 % (96/98) and completeness 50%. Completeness is\ncomparable to Chew and Wu’s average voice consistency\nmeasure [5].\nAs in [5] we also end the separation where a ground truth\nvoice becomes non-monophonic.\n6. Experiments and results\nTo reduce calculation time for the experiments in this sec-\ntion, we adopted the policy of only opening up to one more\nstream than needed (instead of op to the number of the notes\nin the group) in the voice conﬁguration unit. Thus if two\nstreams are present, and two notes are to be assigned, we\ncheck the possibilities of opening none and opening one ad-\nditional stream – not two more, which in theory would also\nbe possible, since any note can start a stream. When more\nstreams are present than notes to be assigned, we do not try\nopening new streams (but only closing).\nTo run the algorithm we need to specify cost values for\nopening and closing voices together with a cost value for\nrests (the value is per second). In the ﬁrst experiment we\nsetcopen,cclose, andcrestto 50 and lookahead to 3 groups.\nThe leap cost is simply the number of semitones. The re-\nsult of Experiment 1 is shown in Table 1. A total of 70874\nnote transitions were examined in the soundness evaluation .\nThe algorithm seems to perform better when evaluated for\nsoundness than completeness. One factor inﬂuencing this is\nthat the number of voices found necessary to separate the\nﬁles (from 2 to 24) are higher than the actual number of\nvoices in the inventions and fugues (2-5).\nTo compensate for this we introduce a small post process-\ning step, trying to recombine ended voices with voices be-\nginning later on. The assumption that closed streams will\nlater reappear seems to be somewhat plausible in the chew-\nBach data set, but it might not hold in all types of music.\nThe concatenation of streams has a greater impact on com-pleteness than on soundness, see Experiment 2 in the table.\nThis approach is used in all experiments below.\nA side effect of allowing voices to cross is the following:\nwhen two voices jump in the same direction, and the lower\nvoice ends above the previous pitch of the upper voice, the\nsum of the leaps will be the same whether the voices cross\nor not. Adding a small non-linear term will make the algo-\nrithm prefer two intermediate-sized leaps over a large and a\nsmall, and thus prefer the shortest path for both voices. Ex-\nperiment 3 was done with the leap cost function: cost (d) =\nd+ 0.05d2. This function will be used in the remaining ex-\nperiments. Again the improvement primarily concerns com-\npleteness.\nOther cost parameters might be a better choice. In Exper-\niment 4 we have chosen the following parameters: copen=\n100,cclose= 20 , and crest= 50 . An improvement in\nsoundness is introduced, but completeness drops. These val -\nues will be used in the remaining experiments.\nNotes sharing both pitch and duration (group) are unsep-\narable. We can only hope that they will be assigned to cor-\nrect voices. There are 78 occurrences of this phenomenon\nin the data. We can expect our separation to handle these\nsituations correctly in half of the occurrences. In 273 situ -\nations notes from different voices begin on the same pitch,\nand in 135 situations voices end on the same pitch – in both\ncases the durations are not equal. V oices coming together in\nunison are furthermore likely to cross, which result in erro rs\nprimarily inﬂuencing the completeness measured. A way to\navoid this could be by preferring a voice to continue close\nto the previous pitch it possessed. So when two voices jump\ninto unison, they will try to part again without crossing. By\nadding 1/20 of the leap cost between this note and the sec-\nond last note, we notice an improvement in completeness –\nsee Experiment 5 in Table 1. This strategy is also used in\nthe following experiment.\nIn Experiment 6 a new strategy is introduced: pattern\nmatching. The idea is that previously heard sequences will\nbe perceived in that way again. When a solution is consid-\nered, the solution is preferred, if the intervals between th e\nlast 5 notes in the voice(s) are already present somewhere\nin existing voices. We simply perform a search in all ex-\nisting voices at the given time to see if the sequence previ-\nously occurred. Exact matching of pitches is used and no\nduration information is taken into account in this simple ap -\nproach. On some ﬁles the effect is more positive than on\nother. By letting pattern matching compete with or overrule\nthe pitch proximity rule in this way, new errors are naturall y\nintroduced. Overall only a small improvement is obtained\nin both soundness and completeness.\nChanging the cost parameters in Experiment 4 caused a\nlarge impact on both soundness and completeness. With\nthe danger of overﬁtting, we plan to do a stochastic search,\noptimizing soundness and completeness respectively in or-\nder to see how far we can go on this data set by using the‘right’ parameters. Many pieces can be quite nicely sepa-\nrated with the tools at hand, but by using ﬁle speciﬁc pa-\nrameters. Preliminary experiments show that when mini-\nmizing the total number of errors (optimizing soundness)\nusing an evolutionary algorithm, we are not likely to get\nmuch further. Using the settings from Experiment 5, but\nwith the values (copen,cclose,crest)= (96.4, 9.2, 95.2) we\nget a soundness of 97.60% and a completeness of 71.13%.\nHowever, when optimizing completeness, it is possible to\nreach at least 79.68% by using the values (64.0, 98.5, 21.3)\n(soundness is then 94.62%).\nWe did a ﬁnal attempt to improve the performance with\nrespect to completeness. It can be noted that voices that\ncross tend to cross back again after a (short) while. We have\nrun a few experiments preferring voices that cross each othe r\nan even number of times. Experiments showed it possible to\ngain some completeness (81.70%) while losing some sound-\nness (94.73%).\n6.1. Error analysis\nThe principle of pitch proximity is not enough to solve all\nproblems in voice separation completely. However some\nof the mistakes committed by the algorithms making use of\nthe principle are not clear to the perceiver. A common sit-\nuation is when a voice ends, and one of the other voices\npresent immediately jumps to a pitch close to the one the\nending voice was playing. This situation makes the algo-\nrithm prefer to end the leaping voice and continue the end-\ning. Pattern matching was expected to be able to handle\nsome of these situations, by preferring that reoccurring ma -\nterial should not be divided into different streams. The eff ect\nof pattern matching on voice separation deserves a study on\nits own – we expect to do more research along these lines.\nIn many cases the principle of ‘good continuation’ could\nbe a helping hand: prefer voices to have a logical continua-\ntion of what they recently were ‘doing’: prefer to continue\nin a way that reinforces any kind of musical ‘idea’ in pitch\nand/or rhythm. Since the percept of parallelism in music can\nbe created by many compositional means, this heuristic will\nbe hard to implement. However in cases where a musical\nidea is transferred from voice to voice, this approach would\nfail.\nA systematic approach of locating errors and categoriz-\ning them could be useful but has not yet been pursued. This\ncould give statistical evidence of which problems are most\nfruitful to solve.\n7. Conclusion\nA stream separation algorithm has been presented. The al-\ngorithm performs well on highly polyphonic music by Bach\n– most convincing when measuring the performance in sound-\nness. Regarding completeness Chew and Wu’s algorithm is\nclearly ahead (they report 88.98%, but provide no evaluatio n\ndirectly comparable to our measure of soundness).However our method has other advantages: the real time\nprocessing ability including the dynamic opening and clos-\ning of streams. When connecting a MIDI instrument the in-\nput can almost immediately be viewed as separated voices.\nThe border between single melody and pseudo-polyphony\n(the fusion of alternating notes) can be demonstrated.\nIn performed music, onset and offset of notes are most\noften not as strictly proportional to the notated values in\nthe scores, as is the case with our Bach ﬁles rendered from\nscores. More intelligent means of quantizing the data are\nrequired – for example beat tracking could be useful when\ndetermining the onset groups (see [9]).\nWe are planning to use the separation algorithm for on-\nline analysis of performed music.\n8. Acknowledgments\nThis research was supported by the Viennese Science and\nTechnology Fund (WWTF, project CI010). The Austrian\nResearch Institute for AI acknowledges basic ﬁnancial sup-\nport from the Austrian Federal Ministries of Education, Sci -\nence and Culture and of Transport, Innovation and Technol-\nogy.\nReferences\n[1] Albert S. Bregman. Auditory Scene Analysis. The Perceptual\nOrganization of Sound . The MIT Press, Cambridge, Massa-\nchusetts, 1990.\n[2] Emilios Cambouropoulos. From midi to traditional musi-\ncal notation. In In Proceedings of the AAAI’2000 Workshop\non Articial Intelligence and Music , Austin, TX, 2000. AAAI\nPress.\n[3] J ¨urgen Kilian and Holger H. Hoos. V oice separation: A\nlocal optimisation approach. In Michael Fingerhut, editor,\nProceedings of the Third International Conference on Music\nInformation Retrieval: ISMIR 2002 , pages 39–46, 2002.\n[4] Phillip B. Kirlin and Paul E. Utgoff. V oiSe: Learning to seg-\nregate voices in explicit and implicit polyphony. In Joshua D.\nReiss and Geraint A. Wiggins, editors, Proceeding of the\nSixth International Conference on Music Information Re-\ntrieval: ISMIR’05 , pages 552–557, 2005.\n[5] Elaine Chew and Xiaodan Wu. Separating voices in poly-\nphonic music: A contig mapping approach. In Proceedings\nof the Second International Symposium on Computer Music\nModeling and Retrieval (CMMR’04)) , volume 3310 of Lec-\nture Notes in Computer Science , pages 1–20. Springer, 2004.\n[6] David Temperley. The Cognition of Basic Musical Struc-\ntures . MIT Press, Cambridge, MA, 2001.\n[7] Fred Lerdahl and Ray Jackendoff. A Generative Theory of\nTonal Music . MIT Press, 1983.\n[8] David B. Huron. Tone and voice: A derivation of the rules of\nvoice-leading from perceptual principles. Music Perception ,\n19(1):1–64, 2001.\n[9] Fabien Gouyon and Simon Dixon. A review of auto-\nmatic rhythm description systems. Computer Music Journal ,\n29(1):34–54, 2005."
    },
    {
        "title": "Music Summarization Via Key Distributions: Analyses of Similarity Assessment Across Variations.",
        "author": [
            "Arpi Mardirossian",
            "Elaine Chew"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1418295",
        "url": "https://doi.org/10.5281/zenodo.1418295",
        "ee": "https://zenodo.org/records/1418295/files/MardirossianC06.pdf",
        "abstract": "This paper presents a computationally efficient method for quantifying the degree of tonal similarity between two pieces of music. The properties we examine are key frequencies and average time in key, and we propose two metrics, based on the L 1 and L 2 norms, for quantifying similarity using these descriptors. The methods are applied to 711 classical themes and variations over 71 variation sets by 10 composers of different genres. Quantile-quantile plots and the Kolmogorov-Smirnov measure show that the proposed metrics exhibit strongly distinct behaviour when assessing pieces from the same variation set, and those that are not. Comparisons across variation sets by the same composer, and comparisons of pieces by different composers although result in similar distributions, are derived from fundamentally different underlying distributions, according to the K-S measure. We present probabilistic analyses of the two methods based on the distributions derived empirically. When the discrimination threshold is set at 55, the probabilities of Type I and Type II errors are 18.41% and 20.56% respectively for Method 1, and 15.72% and 22.94% respectively for Method 2. Method 1 has a success rate of 99.48% when labeling pieces as dissimilar (not from the same variation set), while the corresponding rate for Method 2 is 99.45%. Keywords: Music similarity, similarity assessment, music representation, music summarization, key distribution, pitch, music information retrieval.",
        "zenodo_id": 1418295,
        "dblp_key": "conf/ismir/MardirossianC06",
        "keywords": [
            "quantitative",
            "tonal similarity",
            "music",
            "L 1 norm",
            "L 2 norm",
            "key frequencies",
            "average time in key",
            "711 classical themes",
            "71 variation sets",
            "10 composers"
        ],
        "content": "Music Summarization Via Key Distributions:Analyses of Similarity Assessment Across VariationsArpi Mardirossian and Elaine ChewUniversity of Southern California Viterbi School of EngineeringEpstein Department of Industrial & Systems EngineeringIntegrated Media Systems CenterLos Angeles, CA 90089, USA{mardiros, echew}@usc.eduAbstractThis paper presents a computationally efficient methodfor quantifying the degree of tonal similarity between twopieces of music. The properties we examine are keyfrequencies and average time in key, and we propose twometrics, based on the L1 and L2 norms, for quantifyingsimilarity using these descriptors. The methods are appliedto 711 classical themes and variations over 71 variationsets by 10 composers of different genres. Quantile-quantileplots and the Kolmogorov-Smirnov measure show that theproposed metrics exhibit strongly distinct behaviour whenassessing pieces from the same variation set, and those thatare not. Comparisons across variation sets by the samecomposer, and comparisons of pieces by differentcomposers although result in similar distributions, arederived from fundamentally different underlyingdistributions, according to the K-S measure. We presentprobabilistic analyses of the two methods based on thedistributions derived empirically. When the discriminationthreshold is set at 55, the probabilities of Type I and TypeII errors are 18.41% and 20.56% respectively for Method1, and 15.72% and 22.94% respectively for Method 2.Method 1 has a success rate of 99.48% when labelingpieces as dissimilar (not from the same variation set),while the corresponding rate for Method 2 is 99.45%.Keywords: Music similarity, similarity assessment, musicrepresentation, music summarization, key distribution,pitch, music information retrieval.1.IntroductionThis paper presents a computationally efficient methodfor determining tonal similarity between two pieces ofmusic. The information required by the system is pitchinformation for any time segment, which can be derivedfrom MIDI or audio. We focus on the properties of keyfrequencies and the average time in each key, and proposesimilarity metrics based on these descriptors.Music similarity is a complex problem because thedefinition of similarity can be widely divergent and highlysubjective. Music similarity has been viewed from manyangles with different assumptions. Some aspects ofsimilarity include: instrumentation, timbre, melody,harmony, rhythm, tempo, mood, lyrics, socio-culturalbackgrounds, structure, and complexity [1].Subsequently, a challenge in music similarity researchis the determining of appropriate ground truth data. In thispaper, we have chosen variation sets as our ground truthinformation on which to verify our proposed metrics forsimilarity assessment. The theme and variations genreconsists of music in which an initial melody, the theme, isfirst presented in an introductory section; it is then alteredas variations to the original theme in subsequent sections.We will refer to each set of theme and variations as the“Variation Set.”2.Related WorkAny study of music similarity must first define its subjectof focus, whether it be low- or high-level, melodic orrhythmic, or in linear or vertical time. We present heresome recent work that spans several representativedomains. Our work differs from these approaches in that itfocuses on pitch structure at a relatively high level,allowing for more general classification based on vectorsdescribing key frequency and average time-in-keyinformation.One domain of music similarity research is melody.The melody is often the “star” of a piece. It is what weoften remember about a song. Hu, Dannenberg and Lewis[2] used dynamic programming algorithms to compute an‘edit distance’ as a measure of melodic dissimilarity.Typke et al [3] developed a method where notes weremapped to weighted points in two-dimensional space, andmelodic similarity was measured using the Earth Mover’sDistance and the Proportional Transportation Distance.Another domain in music similarity research is rhythm,the pattern of proportional durations of notes. Paulus andKlapuri [4] developed a system that measures thesimilarity of two rhythmic patterns by using aprobabilistic musical meter estimation process. Hofmann-Engl [5] represent durations as chains based on atomicbeats.  They derived rhythmic similarity from how muchtwo rhythms deviate in shape. Chew, Volk and Lee [6]used the method of Inner Metric Analysis to compute aPermission to make digital or hard copies of all or part of this work forpersonal or classroom use is granted without fee provided that copiesare not made or distributed for profit or commercial advantage andthat copies bear this notice and the full citation on the first page.© 2006 University of Victoriametric, and Dixon, Pampalk and Widmer [7] usedperiodicity patterns, to assess rhythmic similarity for dancemusic classification.Work that is most closely related to the present onewith regards to the domain focus and methods used is thatby Tzanetakis, Ermolinskyi and Cook [8]. Their methodcreates pitch histograms, and then extracts several featuresfrom them for genre classification. They mention that pitchhistograms may be utilized in the determining of musicsimilarity. Our work goes one step further to consider keyhistograms, and their behaviour under different testsituations. Since each key can be summarized as a pitchdistribution, our approach essentially considers thedistribution of pitch distributions.In [9], we introduced the use of key distributions inmeasuring similarity, and a sum-of-squared-differencemetric for quantifying similarity, and tested it on a limitedset of Mozart variations, showing the results in a self-similarity matrix. In the current paper, we use an L1 metricfor key distribution similarity assessment, and provide in-depth probabilistic and statistical analyses of the outcomesof this method. We consider the additional statistic – themean time in key – and use the L2 norm for quantifyingsimilarity for (key distribution, mean-time-in-key) pairs.The present test data set is also vastly increased from theprevious one, and now contains 711 variations from 71Variation Sets by 10 composers.Work that is related to ours with regards to the use ofVariation Sets is that of Pickens [10]. This work developsa music information retrieval system where given a pieceof music as input, it returns an ordered list of similarpieces.3.System DescriptionThis section describes the techniques we use to assessmusic similarity. We begin by slicing a piece of musicinto uniform segments and determining the key for eachslice using a key-finding algorithm. We then generate akey histogram as well as a mean-time-in-key histogram forthe piece. These histograms summarize the tonal patternsin the piece. The process repeated for the comparison piece,we compare result from the two pieces. We present twomethods of this comparison. This section concludes with ashort example.3.1SegmentationEach piece, say of length n, is segmented into a givennumber of segments, m, of uniform length. It follows thatthe length of each segment is n/m. We refer to a completesample of music as a “piece”. In our experiments, thistypically refers to either the theme section of a VariationSet, or one of its variations. When comparing pieces ofdiffering lengths, m, remains constant while the length ofeach segment depends on n. m is constant so that thesummary description of different performances of the samepiece will be approximately the same. In our experience,the choice of m has some effect on the final result, but isreasonably stable over a range of m values. If m is verysmall, then each segment will be too large to providereasonable discriminatory information. If m is very large,then each segment will be too small to produce anymeaningful high-level pitch structure information. Theselection of m will be further discussed in Section 4.3.2Key DeterminationThe key of each segment must be determined in order togenerate the key distribution of each piece. Any keyfinding algorithm may be invoked at this stage (see [11]for references to key finding algorithms). We use theCenter of Effect Generator algorithm [12,13] based on theSpiral Array, a mathematical model for tonality that usesnested helixes to represent tonal elements, such as pitchclasses and keys. The pitches in each segment of a pieceare mapped to pitch class positions on the helix using apitch spelling algorithm [14]. An aggregate position ofthese positions is obtained by weighting each pitch classrepresentation by its proportional duration in the segment.The key is then determined through a nearest neighborsearch for the nearest key representation on the major andminor key helixes. This key finding algorithm can be usedfor both MIDI and audio input [11],[13],[15]. Even thoughwe focus here on MIDI input, it is easy to see how ourapproach may be extended and used for audio input.3.3Key HistogramsWe use the sequence of keys calculated for the segments togenerate the key histograms. We represent the sequence ofkeys as an m-dimensional vector K = {k1, k2, …, km}. Eachki is a key identified by the key finding algorithm forsegment i. The bins of the key histogram are the 55possible major and minor keys from Cbb to C##, shownas a vector of pitch names, P = {p1, p2, …, p55}. P has 55elements because the Spiral Array does not assumeenharmonic equivalence. The key histogram values arestored in the vector F = {f1, f2, …, f55} where fi representsthe number of times an element of K is equal to the i–thelement of P.Let us consider a simple example. If there were onlytwo possible keys (A and B), we would have P = {A, B}.Assume that m = 5 and the sequence of key segments is K= {A, A, B, B, A}. Then F = {3, 2}.3.4Mean-Time-In-Key HistogramsWe use the vector K to generate the Mean-Time-In-Keyhistograms. Let O = {o1, o2, …, o55} be a vector such thatoi is the number of times a continuous sequence ofelements corresponding to pi occurs in the vector K. Themean-time-in-key histogram is stored in the vector M ={m1, m2, …, m55}, where mi = fi/ oi. Continuing with ourprevious example, O = {2, 1} and M = {1.5, 2}.3.5Comparing Two PiecesThis section details the methods we propose for obtaininga similarity measure. We present two methods and willlater compare the results obtained for both. Our firstmethod uses vector F, and computes a distance betweenthem as the measure of similarity. The second method usesboth vectors F and M, and gets the distance between pairsof values of F and M as the measure of similarity.The selected features measure the degree of tonalstability in a piece. A piece with an F vector containingpeaks is more stable than a piece that has a uniformlydistributed F vector. For an F with peaks, consider itscorresponding M vector. If the values of M correspondingto the peaks of F are large, then the piece is more stablethan if these values were small. We consider both the one-and two-vector methods to see if including the additionalinformation in M gives better results.3.5.1Comparing Two Key DistributionsWe use the distance between two probability massfunctions (p.m.f.’s) as our first measure of similarity,which we will refer to as “Method 1”. Consider twopieces, Piece A and Piece B, with key histograms, F = {f1,f2, …, f55} and F' = {f '1, f '2, …, f '55} respectively. Wetreat F and F' as p.m.f.’s, and measure the distancebetween them using the L1 norm, shown in (1).€ fi−fi'i=155∑                                 (1)3.5.2Comparing Pairs of Key and Mean-Time-In-KeyDistributionsWe use the Euclidean distance between two (F, M) pairs asour second measure of similarity, which we will refer to as“Method 2”. Let M = {m1, m2, …, m55} and M' = {m'1,m'2, …, m'55} be the respective mean-time-in-keyhistograms for Piece A and Piece B. The measure ofsimilarity is based on the L2 norm, shown in (2).€ (fi−fi')2+(mi−mi')2i=155∑                 (2)3.6ExampleAt this point, we present a real example to illustrate themethods. We use three pieces for this example. Piece A isthe theme section from Beethoven’s La Molinara, Piece Bis the third variation of the same piece, and Piece C is thesecond variation of Schumann’s Symphonische Etüden.Since Piece B is a variation of Piece A, they should bemore similar than Pieces A and C. Note that m = 50.Consider the plots of F shown in Figure 1. Ourassumption that Pieces A and B are similar and that PiecesA and C are different is supported by direct inspection ofthese plots. Using Method 1, we calculate a distance of 10for Pieces A and B, and 98 for Pieces A and C.The plots of M are shown in Figure 2. Using Method2, we calculate a distance of 11.82 for Pieces A and B and103.55 for Pieces A and C. These findings further supportour initial assumptions. Method 1 performs better whencomparing pieces A and B, while Method 2 performsbetter when comparing pieces B and C.4.ResultsWe present and discuss our results in this section. We alsodiscuss our motivation for choosing our particular data set.We state our expected results and then present our actualfindings.We briefly mentioned the selection of the segmentationvariable m. The average length of a piece in our data set is61.3 seconds. The value of m must be such that each ki inK is small enough to provide insight yet large enough soas not to contain insignificant fluctuations. Initialsensitivity analysis experiments were conducted to see thebehavior of our system as the value of m changed. We setm to a number in the range [50, 300]. At m = 50, eachsegment has, on average, a length of 1.26 seconds. Oursystem is robust within the range of values tested. Wefound that at m = 50 there was less noise than at m = 300.This led us to choose m = 50 for further experiments.4.1Data: Variation SetsWe have chosen to use Variation Sets as our data set sincethe similarity of pieces in the same set is objectively pre-defined by the composer, and thus less subject to dispute.We have amassed a collection of Variation Sets from [16]spanning ten composers and periods ranging from Baroqueand Classical, to Romantic. Table 1 summarizes thestatistics on our data set, consisting of 711 theme andvariations over 71 Variation Sets.Figure 1. Plot of vector F for Pieces A, B, and C.\n  Figure 2. Plot of vector M for Pieces A, B, and C.0510152025A MajorA# MinorAbb MajorB MinorBb MajorBbb MinorC# MajorC##Cbb MajorD# MajorDb MinorE MinorEb MajorEbb MinorF# MajorF## MinorG MajorG# MinorGbb MajorPiece APiece BPiece C\n00.511.522.533.54A MajorA# MinorAbb MajorB MinorBb MajorBbb MinorC# MajorC##Cbb MajorD# MajorDb MinorE MinorEb MajorEbb MinorF# MajorF## MinorG MajorG# MinorGbb MajorPiece APiece BPiece CTable 1. Summary of the pieces in the data set.ComposerNo. ofVariation SetsNo. ofPiecesAvg. PieceLength (secs)Bach348107.48Beethoven2020550.80Brahms812857.41Chopin42156.90Handel54032.30Haydn129353.32Liszt32236.86Mozart109960.95Schubert43469.56Schumann22187.38TOTAL7171161.304.2Expected ResultsWe assume that pieces within the same Variation Set aresimilar one to another. Such an assumption was also madein [10]. Note that the converse may not necessarily be true.Even though we expect pieces from different Variation Setsto be less similar than pieces from the same, we cannotassume that they will not be similar. Even though wedistinguish between composers in our analysis, it isunclear if the methods would be able to distinguishbetween composer’s styles. Because the methods sostrongly favor pieces with the same tonal patterns, acharacteristic of variations, we expect pieces acrossdifferent Variation Sets to be less similar one to another,even if they are by the same composer. We postulate thatour methods are better at capturing similarities at the mid-level of theme and variations than at the composer level.4.3Comparing All Pieces by All ComposersWe used both Methods 1 and 2 to compare all the piecesin our data set. We compared all 711 pieces to one anotherand obtained a total of 505,521 comparisons for eachmethod of similarity assessment. Note that we thendiscarded repeated comparisons. Like the work in [10], wehave chosen to include the comparisons of pieces tothemselves in our analysis as they provide a good check ofour system. When comparing N pieces, each segmentedinto m sections, the computational complexity forcalculating all pairwise comparisons is O(mN + N2).For each method, we extracted three groups from thetotal measurements. Group 1 contains all comparisons ofpieces from the same Variation Set. Group 2 contains allcomparisons of pieces from the same composer, butdifferent Variation Sets. Group 3 contains all comparisonsof pieces from different composers. Since the number ofcomparisons in each group differs greatly, we normalizedthe results so that the distributions sum to one.4.3.1Analysis of Results Using Method 1In this Section, we analyze the distributions of the threeGroups of data for Method 1. Based on our expectationsstated in Section 4.2, we would expect that thedistribution of Group 1 would differ from the distributionsof Groups 2 and 3. Also, we would expect thedistributions of Groups 2 and 3 to be similar. Weconstruct empirical quantile-quantile plots in order to makethese comparisons (see Figure 3), which consist of plottingthe quantiles of one empirical distribution against thecorresponding one in the other. If the distributions comefrom the same underlying distribution, then the plot willbe close to the line x=y.\nFigure 3. Quantile-quantile plots comparing Groups 1 & 2,1 & 3, and 2 & 3, respectively, using Method 1.It is clear from Figure 3 that Group 1 does not comefrom the same underlying distribution as Groups 2 and 3.Also, Groups 2 and 3 appear to be derived from the sameunderlying distribution. These observations support ourinitial assumptions and verify that Method 1 is successfulat distinguishing between pieces from the same anddifferent Variation Sets.Table 2. Results of K-S test for Method 1.ComparisonGroupsK-Sstatp-valueReject H0?1 and 20.5880.00Yes1 and 30.6120.00Yes2 and 30.0511.957x10–10YesWe also conduct a Kolmogorov-Smirnov (K-S) test tocompare the distributions of the three groups. The nullhypothesis, H0, for this test is that two sets come from thesame underlying continuous distribution. We present theresults of this test in Table 2. This test verifies that thedistribution of Group 1 is indeed significantly differentfrom the distributions of Groups 2 and 3. Even though thedistributions for Groups 2 and 3 appear to be similar, andthe K-S statistic is correspondingly smaller, the testreveals that they come from different underlyingdistributions.The distributions of Groups 1, 2 and 3 are shown inFigure 4. By inspection, we can see that the plot for Group1 is significantly different from that for Groups 2 and 3,while Groups 2 and 3 appear much more similar.Next, we perform some probabilistic analyses ofclassification errors should Method 1 be used for musiccategorization. Recall that Method 1 returns a single valuefor every comparison made between two pieces. If twopieces are exactly the same, this value is equal to zero. Asthe degree of difference between the pieces increases, sodoes this measure. In a rudimentary categorization scheme,we could select a cutoff point for determining if the twocan be considered variations one of another. If the value isless than this cutoff point, we conclude that the pieces aresimilar. If it is greater than or equal to the cutoff point, weconclude that the pieces are dissimilar.0.00%2.00%4.00%6.00%8.00%10.00%12.00%14.00%16.00%18.00%\n05101520253035404550556065707580859095100105110115Group 1Group 2Group 3\nFigure 4. Distributions of similarity measures, obtainedusing Method 1, divided into Groups 1, 2 and 3.Suppose the cutoff point is 55, the point at which theoutlines of the three distributions cross. LetA = “Two pieces are from the same Variation Set,” andB = “Their similarity value is less than 55.”Next we compute Type I (false positive) and Type II (falsenegative) probabilities for this strategy.  The probability ofa Type I error, P(B | A') = 18.41%. The probability of aType II error, P(B' | A) = 20.56%.Consider the question: if we pick a data point atrandom, and its value is less than 55, what is theprobability that this data point belongs to Group 1? Wecan restate this question as P(A | B). Also consider theconverse of this question: if we pick a data point atrandom, and its value is greater than or equal to 55, whatis the probability that this data point does not belong toGroup 1? This can be restated as P(A' | B'). We considerthree possible scenarios: (1) when we consider all VariationSet comparisons for pieces by only one composer (i.e. A'consists of members of Group 2 only); (2) when weconsider single-composer and different-composer VariationSet comparisons (A' = Group 3); and, (3) when weconsider all comparisons (A' = Groups 2 and 3). Theresults are summarized in Table 3.Table 3. Bayesian reasoning for Method 1.Groups Considered as A'P(A | B)P(A' | B')Group 234.90%96.66%Group 39.61%99.39%Groups 2 and 38.15%99.48%Since Groups 2 and 3 have far more data points thanGroup 1, a randomly selected data point is more likely tobe from one of these groups than Group 1. Consider theprobabilities of Groups 1, 2 and 3 in Table 3.  When werestrict ourselves to only the comparisons of Variation Setsby the same composer (scenario 1), P(A | B) is 34.90%and P(A' | B') is 96.66%. When we consider sameVariation Set and different composers comparisons(scenario 2), P(A | B) is 9.61% and P(A' | B') is 99.39%.When considering all comparisons (scenario 3), P(A | B) is8.15% and P(A' | B') is 99.48%. Hence, in all cases, theprobability of a false negative is far higher than that of atrue positive. This is to be expected since the number ofpieces in the same Variation Set is far exceeded by thosethat are not.4.3.2Analysis of Results Using Method 2In this Section, we carry out the same type of analysis forMethod 2. As before, we construct empirical quantile-quantile plots for comparisons of Groups 1 and 2, 1 and 3,and 2 and 3. The resulting plots are shown in Figure 5. Byinspection, it is clear from Figure 5 that the data points ofGroup 1 are not derived from the same underlyingdistributions as Groups 2 and 3. And, the data points inGroups 2 and 3 seem to come from similar underlyingdistributions. These observations support our initialassumptions, and verify that Method 2 is also successful atdistinguishing between pieces from the same VariationSet.\nFigure 5. Quantile-quantile plots comparing Groups 1 & 2,1 & 3, and 2 & 3, respectively, using Method 2.We again conduct a K-S test to compare thedistributions of the groups, this time using Method 2.Recall that the null hypothesis is that two sets come fromthe same underlying continuous distribution. We presentthe results of this test in Table 4.Table 4. Results of K-S test for Method 2.Comparison GroupsK-S statp-valueReject H0?1 vs. 20.5880.00Yes1 vs. 30.5980.00Yes2 vs. 30.0415.623x10–7YesThis test verifies that the distribution of Group 1 issignificantly different from those of Groups 2 and 3. Theresults of the comparisons of Groups 2 and 3 show thatthey come from different underlying distributions.However, note that the K-S statistic for this comparison issignificantly smaller. We may conclude, as we did withMethod 1, that even though Groups 2 and 3 do not comefrom the same underlying distributions, they are muchmore similar one with another than with Group 1.Figure 6 is a visualization of the distributions ofGroups 1, 2 and 3. Refer to this figure for furtherconfirmation that the distribution of Group 1 is visiblydifferent from the distributions of Group 2 and 3, whilethe distributions of Group 2 and 3 are more similar.Again, we calculate the error rates and probabilities forMethod 2. As before, the outlines of all three plotsconverge at the distance value 55, which we set as thecutoff point. In this case, the probability of a Type I erroris P(B | A') = 15.72%, and the probability of a Type IIerror is P(B' | A) = 22.94%. Comparing these numberswith those for Method 1, we find that Method 1 has alower Type II error (false negative) probability, whileMethod 2 has a lower Type I error (false positive)probability. We also compute P(A | B) and P(A' | B') for Method 2.We consider the same three scenarios as in Method 1. Theresults are presented in Table 5. When considering only theVariation Sets by the same composer (scenario 1), P(A | B)is 37.98% and P(A' | B') is 96.45%. When we considersame Variation Set and different composers comparisons(scenario 2), P(A | B) is 10.55% and P(A' | B') is 99.36%.When considering all possible pairs of pieces (scenario 3),P(A | B) is 9.00% and P(A' | B') is 99.45%.Table 5. Bayesian reasoning for Method 2.Groups Considered as A'P(A | B)P(A' | B')Group 237.98%96.45%Group 310.55%99.36%Groups 2 and 39.00%99.45%5.ConclusionWe have shown that key histograms can be used todevelop musically accurate summarization of pieces. Ourselection of a data set has helped us establish ground truthin an area of research that often lacks it. We have providedtwo efficient methods for determining the level ofsimilarity between two pieces. Both methods are rathersuccessful in identifying similarity at the level of VariationSets. Both methods are comparable in their level ofsuccess. When the cutoff statistic is set at 55, Method 1has a success rate of 99.48% in labeling pieces asdissimilar, while the corresponding rate for Method 2 is99.45%. Future work will consider comparisons of datasets normalized to the same key. This will allow us tobetter compare transposed variations and fluctuations inkey. We will also consider variations on the same themeby several composers.6.AcknowledgmentsWe thank Sheldon Ross for suggesting the Mean-Time-In-Key statistic and Kurt Palmer for suggesting someBayesian analysis. This work was supported in part by theNSF under ITR Grant No. 0219912, and the sharedfacilities at, the Integrated Media Systems Center, an NSFERC, under Cooperative Agreement No. EEC-9529152.Any opinions, findings, and conclusions orrecommendations expressed in this material are those ofthe authors, and do not necessarily reflect those of NSF.References[1] E. Pampalk, Computational Models of Music Similarityand Their Application in Music Information Retrieval.Ph.D. thesis, Vienna University of Technology, 2006.[2] N. Hu, R. Dannenberg, and A. Lewis, “A ProbabilisticModel of Melodic Similarity,” in Proceedings of theInternational Computer Music Conference, 2002.[3] R. Typke, P. Giannopoulos, R. Veltkamp, F. Wiering, andR. van Oostrum, “Using Transportation Distances forMeasuring Melodic Similarity,” in InternationalSymposium on Music Information Retrieval, 2003.[4] J. Paulus, and A. Klapuri, “Measuring the Similarity ofRhythmic Patterns,” in Proceedings of the InternationalSymposium on Music Information Retrieval, 2002.[5] L. Hofmann-Engl, “Rhythmic Similarity: A Theoreticaland Empirical Approach,” in Proceedings of the 7thInternational Conference on Music Perception andCognition, 2002.[6] E. Chew, A. Volk, and C.-Y. Lee, “Dance MusicClassification Using Inner Metric Analysis – acomputational approach and case study using 101 LatinAmerican Dances and National Anthems,” In BruceGolden, S. Raghavan, Edward Wasil (eds.): The NextWave in Computing, Optimization and DecisionTechnologies: Springer OR/CS Interfaces Series No.29,pp.355-370.[7] S. Dixon, E. Pampalk, and G. Widmer, “Classification ofDance Music by Periodicity Patterns,” in Proceedings ofthe International Conference on Music InformationRetrieval, 2003.[8] G. Tzanetakis, A. Ermolinskyi, and P. Cook, “PitchHistograms in Audio and Symbolic Music InformationRetrieval,” Journal of New Music Research, 2003.[9] A. Mardirossian and E. Chew “Key Distributions asMusical Fingerprints for Similarity Assessment,” 1stIEEE International Workshop on MultimediaInformation Processing and Retrieval, 2005.[10] J. Pickens, Harmonic Modeling for Polyphonic MusicRetrieval. Ph.D. thesis, University of MassachusettsAmherst, 2004.[11] 1st Annual Music Information Retrieval EvaluationeXchange,” [Web site] 2006,url: http://www.music-ir.org/mirex2005/index.php[12] E. Chew, Towards a Mathematical Model of Tonality.Ph.D. thesis, Massachusetts Institute of Technology,2000.[13] E. Chew, “Modeling Tonality: Applications to MusicCognition”, in Proceedings of the 23rd Annual Meetingof the Cognitive Science Society, 2001.[14] E. Chew and Y.-C. Chen, “Real-Time Pitch SpellingUsing the Spiral Array”, Computer Music Journal, 2005.[15] C.-H. Chuan, and E. Chew, “Fuzzy Reasoning in PitchClass Determination for Polyphonic Audio KeyFinding”, 6th International Conference for MusicInformation Retrieval, 2005.[16]  “Classical Music Archives,” [Web site] 2006,Available: http://www.classicalarchives.comFigure 6. Distributions of similarity measures, obtainedusing Method 2, divided into Groups 1, 2 and 3.0.00%2.00%4.00%6.00%8.00%10.00%12.00%14.00%16.00%\n05101520253035404550556065707580859095100105110115Group 1Group 2Group 3"
    },
    {
        "title": "A Mid-level Melody-based Representation for Calculating Audio Similarity.",
        "author": [
            "Matija Marolt"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1416252",
        "url": "https://doi.org/10.5281/zenodo.1416252",
        "ee": "https://zenodo.org/records/1416252/files/Marolt06.pdf",
        "abstract": "We propose a mid-level melody-based representation that incorporates melodic, rhythmic and structural aspects of a music signal and is useful for calculating audio similarity measures. Most current approaches to music similarity use either low-level signal features, such as MFCCs that mostly capture timbral characteristics of music and contain little semantic information, or require symbolic representa- tions, which are difficult to obtain from audio signals. The proposed mid-level representation is our attempt to bridge the gap between audio and symbolic domains by providing an integrated melodic, rhythmic and structural representa- tion of music signals. The representation is based on a set of melodic fragments extracted from prominent melodic lines, it is beat-synchronous, which makes it independent of tempo variations and contains information on repeti- tions of short melodic phrases within the analyzed piece. We show how it can be calculated automatically from polyphonic audio signals and demonstrate its use for dis- covering melodic similarities between songs. We present results obtained by using the representation for finding different interpretations of songs in a music collection. Keywords: music similarity, searching audio, melody- based representation, mid-level representation",
        "zenodo_id": 1416252,
        "dblp_key": "conf/ismir/Marolt06",
        "keywords": [
            "mid-level melody-based representation",
            "melodic",
            "rhythmic and structural aspects",
            "audio similarity measures",
            "audio signals",
            "symbolic representations",
            "gap between audio and symbolic domains",
            "automatic calculation",
            "discovering melodic similarities",
            "music collection"
        ],
        "content": "A Mid-level Melody-based Representation for Calculating Audio Similarity \nMatija Marolt \nUniversity of Ljubljana \nTrzaska 25 \n1000 Ljubljana, Slovenia \nmatija.marolt@fri.uni-lj.si  \nAbstract \nWe propose a mid-level melody-based representation that \nincorporates melodic, rhythmic  and structural aspects of a \nmusic signal and is useful for calculating audio similarity measures. Most current approach es to music similarity use \neither low-level signal feat ures, such as MFCCs that \nmostly capture timbral characteristics of music and contain little semantic information, or require symbolic representa-tions, which are difficult to obtain from audio signals. The proposed mid-level representation is our attempt to bridge the gap between audio and symbolic domains by providing an integrated melodic, rhythm ic and structural representa-\ntion of music signals. The representation is based on a set of melodic fragments extracted from prominent melodic lines, it is beat-synchronous, which makes it independent of tempo variations and c ontains information on repeti-\ntions of short melodic phrases  within the analyzed piece. \nWe show how it can be calculated automatically from polyphonic audio signals and demonstrate its use for dis-covering melodic similarities between songs. We present results obtained by using the representation for finding different interpretations of songs in a music collection. \nKeywords : music similarity, searching audio, melody-\nbased representation, mid-level representation \n1. Introduction \nCalculating music similarity is one of the key areas in \nmusic information retrieval, as it enables searching and organization of music collections. Although melody is a very important descriptor of (Western) music [1], querying audio collections by melody is still an elusive goal. Most current approaches to audio similarity, such as audio fingerprinting [2] or genre classification techniques [3] are based on low-level audio features. Audio fingerprinting techniques typically rely on spectral representations, which are processed to be resistant to various types of noise and are unique for each piece of music; a query results in a match only if the exact same piece of music resides in the queried database. Genre or mood classification techniques \nmostly rely on MFCC coefficients and other low-level descriptors, leading to timbre-based similarity measures.  \nQuery by melody is possible, if symbolic data are avail-\nable [4]; for most recorded mu sic this is not the case. Tran-\nscription and melody extraction techniques are improving, but are still unreliable - the most successful MIREX’05 melody extractor achieved ~70%  accuracy [5]. Shwartz et \nal. [6] presented a system for querying audio collections by melody, but it requires a symbolic representation of the query and does not account for audio to audio matching. \nMid-level representations are an attempt to reduce the \nsemantic gap between low-level and symbolic representations by extracting some higher-level semantic features from music signals, while still avoiding symbols. Dixon et al. [7] introduced rhythmic templates that represent typical rhythmic patterns of a piece and may be used for calculating rhythmic similarity. Bello and Pickens [8] introduced a mid-level harmonic representation, based on chroma features and showed its usability for segmentation. \nMelody is an important desc riptor of a piece of music \nand therefore very desirable for querying a music collection. For this purpose, we propose a mid-level melody-based representation, demonstrate how it can be used for calculating inter-song similarities and present results obtained on the task of finding different interpretations of a song in a music collection.  \n2. Mid-level Melodic Representation \nIn our proposed mid-level representation, we seek to \ncombine melodic, rhythmic a nd structural aspects of a \npiece of music.  \n2.1 Melody \nThe melodic aspects of the representation stem from our \napproach to melody extraction. The approach is briefly summarised as follows (for full description, see [9]). First, spectral modelling synthesis (SMS) is used to extract partials from audio signal, which are then subjected to a psychoacoustic masking model. Predominant pitches are extracted from partials with  an EM approach, which \nestimates the most likely pitches to have generated the observed series of partials. Using SMS partial tracking information, the found pitches are linked in time, resulting \nPermission to make digital or hard copi es of all or part of this work for \npersonal or classroom use is grante d without fee provided that copies \nare not made or distributed for profit or commercial advantage and that \ncopies bear this notice and the full citation on the first page. \n© 2006 University of Victoria in a series of pitch tracks, which are then filtered by \nremoving short and muted tracks. We call this final set of pitch tracks melodic fragments , because they represent \ndifferent parts of melodic lin es (lead and accompaniment) \nin the analyzed piece. Each fragment has its start and end time, time-varying loudness and time-varying pitch. Due to the EM approach taken, in regions of audio where only one pitch is dominant, only a single melodic fragment will be found, while in regions where several pitches are competing for attention, several fragments will appear simultaneously. The resulting representation contains most parts of the main melodic line (~90% on our test set consisting of parts of MIREX’04 and ’05 melody extraction datasets) with additional fragments of other melodic lines, especially when lead is not present.  \n2.2 Rhythm  \nEvents in a piece of music are not perceived in direct \nrelation to time, but in rela tion to their place within a \nmetric hierarchy, whose basic elements are beats. These in \nturn relate to time according to tempo and its variation within a piece. Calculati ng intra- and inter-piece \nsimilarities is difficult when tempo varies; dynamic programming approaches can be used to alleviate this problem [10]. Instead, we prefer to make our representation tempo-independent by using a beat tracker [11] to perform beat detection and then aligning the representation to the beat grid, thus making it beat-synchronous. Beat boundaries are used to resample the representation with an averaging filter to 6 frames per beat, leading to a tempo-invariant melodic representation. The resampling rate was chosen experimentally; smaller values had a negative effect on performance, while larger values did not lead to improvements. In the process, we also resample the frequency axis to a half-tone scale, resulting in 24 values per octave. The coarse scale has been selected to reduce effects of vibrato or similar pitch fluctuations on the representation. We also wr ap the frequency axis to the \nrange of one octave, resulting in a pitch-class type of representation, thus sidestepping octave errors, which are quite common in the melody extraction procedure used. \n \nFigure 1. Mid-level melody-bas ed representation of 12 beats \nof song Love is in the Air The resulting mid-level representation contains most \nparts of the main melodic line, together with some fragments of competing lines and is octave and tempo invariant. An example is given in Figure 1, which shows an excerpt from “Love is in the Air” (sung by J.P. Young). Melodic fragments belonging to lead vocals are visible in the beginning and middle sections , as well as several other \nfragments that mostly occur between vocal parts. \n2.3 Structure \nWe infer the structure of a piece by calculating the self-\nsimilarity matrix S\n(l) of the beat-synchronous melodic \nrepresentation. Each element i,j of matrix S(l) is defined as: \n ()\n.. ..( , ) 1... , 1..l\nij i i l j j lsd b b i n jn++== = . (1)                      \nbi represents a vertical slice of the beat-synchronous \nrepresentation at beat position i (24 frequency bins by 6 \nframes per beat). bi..i+l represents a sequence of slices b \nstarting at beat i and ending at beat i+l; values for beat \npositions beyond the total number of beats n (i>n) are set \nto 0. d is a similarity measure;  after some experiments, we \ndecided to use the cosine similarity measure [14], which performed roughly the same as symmetric KL distance or correlation. Since silence matches silence well, we also add a small amount of random noise to slices b\ni to avoid \nhigh similarity scores for regions of silence. S(l) is thus a \nsquare nxn matrix ( n is the total number of beats) and \ncontains similarities between all pairs of excerpts of length l beats taken from the representation.  \nParameter l controls the length of excerpts to be \ncompared. l is estimated by calculating the self-similarity \nmatrix S\n(l) with a sequence length of l=12 beats, \naccounting for 3/4 and 4/4 bars. Elements of the matrix above the main diagonal are then averaged across diagonals:  \n \n,\n111.. 1nk\nki i k\nids k nnk−\n+\n== =−−∑  (2) \nand autocorrelation of resulting vector d calculated. Peaks \nof the autocorrelation function correspond to typical gaps between repetitions of melodic patterns in the piece. The highest autocorrelation peak above 9 is taken as length l, \nso that compared sequences are at least 10 beats long. We chose this threshold experimentally, as we found that lengths smaller than 10 beats may contain too little information to be compared reliably. Typical values of parameter l are 16 beats for pieces in 4/4 and 12 or 24 \nbeats for 3/4 pieces.  \nWe also found that multiplying (element-wise) matrices \ncalculated at different lengths l results in a significant \nnoise reduction. We therefore calculate the final self-\nsimilarity matrix S by combining matrices calculated with \nthree different values of l (eq. 3). In addition, we apply \nfilter F\n(s) to the resulting product. The filter is a square sxs \nmatrix with diagonal elements equal to 1 and other elements equal to –1/( s-1), thus emphasizing diagonals and \nsuppressing off-diagonal repetitions. This reduces the effect of long notes that produce square blocks in the matrix. The self-similarity matrix is calculated as: \n \n()( ) (0.75 ) (0.5 ) (0.25 )ll l l=∗SS S S Fii  (3) \nwhere i denotes element-wise matrix multiplication and \n∗ the convolution operator. The left side of Figure 2 \nshows the matrix calculated for song “Love is in the Air”. Due to beat-synchronous melodic representation and long comparison sequence ( l=16), the figure clearly reflects the \nstructure of the piece. Diagonals  indicate repeated sections \nand reveal the structure of the piece to be aaBaaBCaaBaaBCC. The structure can be derived from the top part of the matrix, as indicated by overlaid dotted lines. If we compare the resulting matrix to self-similarity matrices calculated from MFCC or chroma features (i.e. [13]), the amount of noise is greatly reduced and structure revealed in places that may be  ignored by these approaches \ndue to different timbres or harmonies. We contribute this difference to a more musically  meaningful melody-based \nrepresentation and longer sequences used for self-similarity calculation. Although explicit segmentation is not our goal in this research, obtained self-similarity matrices show promise that the mid-level representation could also be useful for segmentation. We use self-similarity information for extraction of melodic patterns as described in the next section. \n3. Calculating Similarity \nOur goal is to obtain a representation that would emphasize melodic aspects of a given piece and would be \nuseful for calculating melodic similarity of different pieces, giving high scores for pieces with similar melody \neven if they have different tempi, different instrumentation or different arrangements. To assess whether the proposed \nmid-level representation is suitable for such tasks, we tested it within a simple music retrieval system, based on comparisons of melodic patterns extracted from each song.  \n3.1 Finding Melodic Patterns \nWe define melodic patterns as parts of melody that are \nrepeated several times in a song. They may be parts of a chorus or verse or entire chorus/verse segments. We use these patterns as a summarized description of a song and use them for calculating similarity. The idea of using patterns to characterize musi c is not new; Paulus and \nKlapuri [12] used rhythmic patterns to measure similarity and Dixon et al. [7] to characterize ballroom dances.  \nThe algorithm for extracting melodic patterns follows \nan approach similar to segmentation and chorus extraction methods, such as Goto’s RefraiD algorithm [13]. It is based on the self-similarity matrix, calculated as described previously. Patterns are extr acted from the matrix by a \nsimple greedy approach, which can be outlined as:  \n1. find the most repeated melodic pattern. We first sum \nthe self-similarity matrix across one dimension, resulting in a vector with high values at beat positions that are often repeated. We th en smooth this vector with \na Gaussian filter. Position of the highest peak in the resulting smoothed vector is taken to lie within the most repeated pattern. The first a nd last beat of the pattern \nare then searched for by s earching the vector from the \nchosen peak forward and backward until the first local minima before and after the peak are met. Beat positions of the minima are taken to represent the start and end position of the most repeated pattern; \n2. find all salient repetitions of this pattern in the matrix. \n         \n  \nFigure 2. Self-similarity matrix calculated  from the beat-synchronous representation of Love is in the Air with structure  \nindicated (left). For comparison: self-similarity  matrix calculated fr om chroma features 3. remove the pattern and all found repetitions from the \nself-similarity matrix and add them to the list of found patterns, if more than one repetition was found. \n4. repeat steps 1-3 until most of the initial matrix is \nremoved (we use a fixed threshold of 90%).  \nThe result is a set of melodic patterns (typically 2-4 on our \ndatabase consisting of mostly pop/rock songs). Each pattern is characterized by its  start and end beat locations \nand a list of repetitions. We also calculate the median mid-level melodic representation of each pattern and all of its repetitions, which keeps only the most salient features (melody) and removes some background accompaniment that may vary within the piece. \n3.2 Compensating for Difference in Keys \nIf we calculate melodic patterns of all songs in a song \ncollection, we can use the obtained patterns to calculate each song’s similarity to all ot her songs in the collection. \nThis can be done by comparing the median mid-level representation of each pattern of a song to representations of each melodic pattern of the compared song. To perform the comparison, we first need to account for possible differences in keys of both songs.  \nA key profile of each song is first calculated. We start \nby summing the entire mid-level representation of a song across time, resulting in the song’s pitch profile. We calculate the dot product of the obtained pitch profile with Bayesian key profiles [15] for all 24 major and minor keys, resulting in the song’s key profile. When we compare two songs, key profiles of both songs are correlated in all shifted positions and the best match is taken to represent the difference in key between the two songs. This difference is then compensated for by a circular shift of melodic patterns of one of the songs. If we assume that two compared songs have similar melody, such procedure leads to corr ect key compensation most of \nthe time. \n3.3 Calculating Similarity \nAfter key compensation, comparison of a pair of melodic \npatterns is straightforward. Since patterns will not usually be time-aligned and of equal size, similarity is calculated by shifting the shorter pattern beat-by-beat over the length of the longer pattern and cal culating similarity of each \nshifted position.  We again use the cosine similarity measure, the same as used for the self-similarity matrix calculation. The highest similarity of all shifted positions is taken as the pattern similarity score. After similarities of all pairs of patterns of two songs have been calculated, the mean similarity of n best matching patterns is taken as \nsimilarity of the two songs. Best value for n was \nexperimentally determined to be 2.  4. Experiment and Discussion \nThe described approach to calculating similarity was tested for retrieval of different performances of a song from a larger song collection. For this task, we first collected a set of different performances of 8 songs, totaling 36 songs. Each song had at least four different versions in this set, either by the same or by different performers. Beat tracking of songs was manually checked for errors and corrected. The list of songs can be found at http://lgm.fri.uni-lj.si/~matic/similarity. We injected the 36 songs into a larger collection of 1820 songs of similar, mostly pop and rock genres. The task was to retrieve the different performances of a given song from the collection.  \nIn the experiment, we compared two different \nrepresentations of music: the proposed melody-based representation and a representation based on chroma feature vectors [13]. With both, we used the same locations of melodic patterns and the same key compensation technique, only the actual representations of melodic patterns varied. Evaluation was performed by considering each of the known 36 pieces as a query and calculating its similarity to all other pieces in the collection. Table 1 lists 11 point precision averages and percentages of hits in top 5 returned songs for all 36 queries and separately for each of the songs used in queries. 11 point precision averages are often used for MIR system evaluation [4] and are calculated as averages \nof precision at recall levels 0%, 10%,...,100%. Results for both compared representations are given in columns 3 and 4. Column 5 lists retrieval results obtained with a similarity measure calculated as a sum of similarity measures of both approaches, resulting in a new combined melody+chroma similarity measure. Number of song versions and performers is given in column 2 (n.v./n.p. - see also the list of songs at http://lgm.fri.uni lj.si/~matic/similarity). Note that the queried song was always returned first in the list of hits and was therefore excluded when calculating 11 point precision averages and top 5 scores. \nAlthough we used a very simple method for calculating \nsimilarity, we achieved solid retrieval results with a number of songs (1,2,3,7 in Table 1), even though performers and arrangements differed (i.e. Mamas and \nPapas, Dean Martin, Beautiful South and Ella Fitzgerald for song 3). This shows the robustness of the presented representation to changes in instrumentation.  \nIf songs differ a lot in melody or rhythm, the presented \napproach fails completely (songs 4 and 8). This has less to do with the mid-level representation than with the simple approach taken to extracting melodic patterns and comparing songs; a dynamic programming approach that would allow for rhythmic variations might lead to better results. Other examples of false negatives are found in songs with several concurrent melodic lines (such as the baroque interpretation of song 1), where the mid-level \nrepresentation either includes too many melodic lines, or picks out the wrong ones. The segmentation procedure is another source of errors (i.e. song 2 by Celine Dion or interpretations of song 6). When the found melodic patterns are not at least approximately time-aligned between interpretations, songs are not found to be similar, even though their melodies and rhythms (and consequently the beat-synchronous melodic representation) are similar. A somewhat more elaborate segment extraction procedure could be the solution for this type of errors. \nWe have also found the existence of the so-called hubs \n[16], which are defined as songs that occur frequently as a false positive according to a given similarity measure. Most of these songs turned out to be a problem for either our melody extraction or beat detection algorithms. This led to poor mid-level representations and unrepresentative segments (few segments, short segments or segments full of melodic lines) that appear to match well with segments of most songs. Otherwise, false positives were mostly due to either errors in extraction of melodic lines (too many lines, incorrect lines) or beat  tracking errors (incorrect \nresampling and segment extraction).  \nResults obtained by using chroma feature vectors \nindicate that these work we ll when performances are by \nthe same performer or have very similar arrangements, such as with song 1, where three of four performances are by The Beatles and also for song 6, where arrangements are very similar. In such cas es, results are comparable or \nbetter than by using the melody-based representation. With different performers and arrangements, however, chroma features lag behind due to their greater dependence on arrangements in comparison with the proposed representation. We have also made an experiment by summing similarity scores of both representations into a combined similarity score, which additionally improved \nretrieval accuracy (Table 1, column 5).  \nWe should also comment on the possible bias \nintroduced by errors of the beat tracking algorithm that \nwas used to automatically extract beat positions in songs from the database (for the queried songs, beat positions were manually checked for errors and corrected). Although it is hard to estimate how errors in beat tracking affect our results, inspection of false positives shows that these are often caused by inco rrect beat tracking, so we \ncould speculate that beat tr acking errors affect false \npositives as well as true negatives and do not introduce a large positive or negative bias into the presented results. \n5. Conclusion and Further Work \nWe described a novel mid-level representation that integrates melodic, rhythmic and structural aspects of a \nmusic signal. We described its use for calculating melodic similarity in audio collections. Results show that the proposed representation provides a good basis for tasks such as retrieval of different interpretations of a song from a song collection even if interpretations are by different artists and have different arrangements. We acknowledge \nthat our tests were made on a small database, but the retrieval method used was also very primitive and we argue that better techniques, such as dynamic programming or HMM modeling should lead to good results also on large databases. We also plan to augment the melodic representation with harmonic information [8]; a combined approach should lead to additional improvements, as is indicated by better results achieved with the combined melody+chroma similarity measure. The proposed representation also seems to hold promise for segmentation and summarization, which is another set of tasks we plan to pursue further. Table 1. 11 point precision averages and percentages of hits in top 5 returned songs for  \nmelody-based representation, chroma representation and a combined approach \n  melody chroma combined \n n.v ./ n.p.11pt. \nprec % \n top 5  11pt. \nprec % \n top 5  11pt. \nprec %  \n top 5  \nMean precision and % of hits  \nin top 5 for all queries  0.20 25 0.15 19 0.22 27 \n1. A hard day’s night 4 / 2 0.35 33 0.39 5 0.40 42 \n2. All by myself 4 / 4 0.33 42 0.12 17 0.33 42 \n3. Dream a little dream  5 / 4 0.44 55 0.28 35 0.51 5 \n4. Georgia on my mind 4 / 4 0 0 0 0 0 0 \n5. Goodnight Irene 4 / 4 0.13 25 0 0 0.03 8 \n6. Knockin’ on heaven’s door 5 / 5 0.02 0 0.20 35 0.20  35 \n7. Love is in the air 5 / 4 0.33 4 0.18 15 0.32  35 \n8. Summertime 5 / 5 0.01 0 0  0 0 0 6. Acknowledgments \nThis work was supported by an International Short Visit \ngrant from the Royal Society th at enabled the author to \nspend six weeks at the Centre  for Digital Music, Queen \nMary, University of London. I would like to thank Dr. Juan Bello and Dr. Mark Plumbley for invitation, their ideas and support during the visit. \nReferences \n[1] E. Selfridge-Field. “Conceptual and Representational \nIssues in Melodic Comparison,” in Melodic Similarity: Concepts, Procedures, and A pplications, MIT Press, MA, \n1998. \n[2] J. Haitsma, T. Kalker. “A Highly Robust Audio \nFingerprinting System,” in ISMIR 2002 , Proc ., 2002. \n[3] E. Pampalk, S. Dixon, and G. Widmer. “Exploring Music \nCollections by Browsing Different Views,” CMJ , Vol. 28, \nNo. 2, pp 49-62, 2004. \n[4] A.L. Uitdenbogerd. “Music In formation Retrieval Techn-\nology,” Ph.D. Thesis , RMIT, 2002. \n[5] “MIREX 2005 - 1st Annual Music Information Retrieval \nEvaluation eXchange,”, [Web site] 2005, Available: http://www.music-ir.org/mirex2005/index.php/Main_Page  \n[6] S. Shwartz, S. Dubnov, N. Friedman, Y. Singer. “Robust \nTemporal and Spectral Modeling for Query By Melody,”     Proc. ACM SIGIR*02 , Tampere, Finland, 2002.  [7] S. Dixon, F. Gouyon, G. Widmer, “Towards \nCharacterization of Music via Rhythmic Patterns,” in ISMIR 2004 Proc., 2004. \n[8] J.P. Bello, J. Pickens. “A Robust Mid-level Representation \nfor Harmonic Content in Music Signals,” in ISMIR 2005, \nProc. London, UK. September 2005.   \n[9] M. Marolt. “Audio Melody Extraction Based on Timbral \nSimilarity of Melodic Fragments,\" in Proceedings Eurocon \n2005 , Belgrade, 2005.  \n[10] R.J. McNab, L.A. Smith, I.H. Witten, C.L. Henderson,  S.J. \nCunningham. “Towards the digital music library: Tune retrieval from acoustic input,” Proceedings of Digital Libraries '96. ACM, 1996. \n[11] M. E. P. Davies and M. D. Plumbley. “Beat tracking with a \ntwo state model,” in IEEE ICASSP  Proc. , Philadelphia, \nPenn., USA, 2005. \n[12] J. Paulus, A. Klapuri. “M easuring the similarity of \nrhythmic patterns,” in ISMIR 2002 Proc., 2002.  \n[13] M. Goto. “A chorus-section detecting method for musical \naudio signals,” ICASSP 2003 Proc., 2003. \n[14] J. Foote. “Visualizing mu sic and audio using self-\nsimilarity,” Proc.  ACM  international conference on \nMultimedia , 1999. \n[15] D. Temperley. “A Bayesian  Key-Finding Algorithm,” in \nMusic and Artificial Intelligence , Springer, 2002. \n[16] J.J. Aucouturier. “Ten E xperiments on the Modelling of \nPolyphonic Timbre,” Ph.D. Thesis, L’Universite Paris 6, 2006."
    },
    {
        "title": "The Map of Mozart.",
        "author": [
            "Rudolf Mayer",
            "Thomas Lidy",
            "Andreas Rauber"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1416060",
        "url": "https://doi.org/10.5281/zenodo.1416060",
        "ee": "https://zenodo.org/records/1416060/files/MayerLR06.pdf",
        "abstract": "We present a study on using a Mnemonic Self-Organizing Map for clustering a very homogeneous collection of mu- sic. In particular, we create a map containing the complete works of Wolfgang Amadeus Mozart. We study and analyze the clustering capabilities of the SOM on this very focused collection. We furthermore present a web-based application for exploring the map and accessing the music it represents. Keywords: Self-Organizing Map, Clustering, Explorative Search",
        "zenodo_id": 1416060,
        "dblp_key": "conf/ismir/MayerLR06",
        "keywords": [
            "Self-Organizing Map",
            "Clustering",
            "Explorative Search",
            "Mnemonic Self-Organizing Map",
            "Wolfgang Amadeus Mozart",
            "Music",
            "Homogeneous Collection",
            "Web-based Application",
            "Complete Works",
            "Study"
        ],
        "content": "The Mapof Mozart\nRudolf Mayer,Thomas Lidy,Andreas Rauber\nViennaUniversityofTechnology\nDepartmentofSoftwareTechnologyandInteractiveSystems\n{mayer, lidy, rauber }@ifs.tuwien.ac.at\nAbstract\nWe present a study on using a Mnemonic Self-Organizing\nMap for clustering a very homogeneous collection of mu-\nsic. In particular, we create a map containing the complete\nworksofWolfgangAmadeusMozart. Westudyandanalyze\nthe clustering capabilities of the SOM on this very focused\ncollection. We furthermorepresentaweb-basedapplicatio n\nforexploringthemapandaccessingthemusicitrepresents.\nKeywords: Self-Organizing Map, Clustering, Explorative\nSearch\n1. Introduction\nThe Self-OrganizingMap (SOM) [1] has been successfully\napplied in the ﬁeld of InformationRetrieval in general, and\nspeciﬁcally also inthe areaof Music InformationRetrieval ,\nas for example in the SOM-enhanced JukeBox (SOMeJB)\n[5]. In this paper, we present our work with a speciﬁc col-\nlection of music, which is characterized by being very ho-\nmogeneous. We took the 250th anniversary of Wolfgang\nAmadeusMozartasmotivationtocreateanintuitivevisual-\nizationandinterfacetohiscompleteworks.\n2. Self-Organizing Maps\nThe Self-Organizing Map (SOM) [1] and related architec-\ntures enjoy considerable popularity for data mining appli-\ncations. This is due to their ability to generate a topology-\npreservingmappingfromahigh-dimensionalinputspaceto\na lower dimensional output space. The data thus structured\nand organized enables an easier interpretation of complex\ninherentstructuresandcorrelationsinthedata. Inmanyap -\nplications the output space is constituted by a two-dimen-\nsional rectangularorhexagonalgrid. Thisprovidesa repre -\nsentation which is easy to read and interpret. A number of\ntechniquesfor visualizing the maps have been suggested to\nfurtherassist theuser ininterpretingthe maps. Inourwork ,\nwe are speciﬁcally using the Smoothed Data Histograms\n(SDH)[4]forrepresentingtheclusterstructure.\nPermission to makedigital orhardcopies ofallorpartofthi s workfor\npersonal or classroom use is granted without fee provided th at copies\narenotmadeordistributed forproﬁtorcommercial advantag e andthat\ncopies bear this notice and thefull citation on the ﬁrstpage .\nc/circlecopyrt2006 University of Victoria2.1. PlaySOMand PocketSOMPlayer\nSOMsmaybeusedtoofferexplorativeaccesstomusiccol-\nlections. They are used in the PlaySOM and PocketSOM\napplications [3], where the user can interactively browse\nand zoom trough the data space, and select playlists on the\nmap on desktop and mobile devices. For our studies, we\nwill use a slightly modiﬁed version of this method, namely\nMnemonicSOMs.\n2.2. Mnemonic SOMs\nFor memorizing the orientation of a map, i.e. the location\nof different genres of music on the map, and for explain-\ningtothe userwherethosegenreclustersarelocatedonthe\nmap,arectangularmapmightbesub-optimal. Therefore,in\nthisstudywe usedamodiﬁcationoftheSOM, theso called\nMnemonic SOM [2]. This modiﬁcation is based on using\nnot rectangular maps, but recognizable shapes in the form\nof countries, geometrical ﬁgures, and similar. In our spe-\nciﬁc case,weusea mapbasedonthesilhouetteofMozart’s\nhead.\n2.3. RhythmPatternsFeatures\nIn order to be able to arrange the pieces of music themati-\ncallyontheSOM,theircontentﬁrstneedstoberepresented\nin the form of a vector of characteristic features. For our\nexperiments, we use a set of features that is based only on\nthe audio content of the music, the Rhythm Patterns (RP)\n[5]. This feature set includes psycho-acoustic processing\nanddescribestheloudnessamplitudemodulationforarange\nofmodulationfrequencieson24criticalfrequencybands. A\nRhythmPatternfeaturevectorconsistsof1440dimensions.\nCapturing ﬂuctuations on all human audible frequency re-\ngions, it is capable not only to discover rhythmics, but also\ntimbral features and thus recognizes different instrument a-\ntionin music.\n3. MusicCollection\nThe music collection we used in our experiments are the\ncompleteworksofthecomposerWolfgangAmadeusMozart.\nThiscollectioncontains2.442tracks,consistingofdiffe rent\ngenres such as Operas, Symphonies and Sonatas. The data\nhas been manually classiﬁed into 17 different classes. We\nuse this class information to enhance the different visual-\nizations we provide. An overview of these classes and the\nnumberofpiecesforeachclassisgivenin Table1.Figure1. Mapof Mozart withclass information.\nTable1. Classes ofMozart's Music\nClassname #Pieces\nSymphonies 144\nString Ensembles 130\nSacred Works 324\nSerenades 77\nDances 207\nSongs 33\nChurch Sonatas 17\nHorn,Oboe,Clarinet Ensembles 10\nPiano Ensembles 30\nConcertos 159\nKeyboardWorks 146\nOperas 768\nDivertimenti 169\nCanons 41\nConcertArias 53\nFlute Quartets& Sonatas 27\nViolin Sonatas 107\n4. MapofMozart\nInourstudy,wemappedthemusiccollectionontoaMnemonic\nSOM in the shape of the silhouette of its composer. As\nvisualizations, the user can choose between three differen t\nvariants: (1) the map with the image of Mozart as back-\nground,asdepictedinFigure1;(2)onlytheshapeofMozart\nwith the SDH visualization; and (3) a combinationof both,\nasemi-transparentSDHontheimage,asshowninFigure2.\nAdditionally,theusercanchoosetoshowthedistributiono f\ntheclasses thepiecesofmusicbelongto.\nTo make the PlaySOM application more easily accessible,\nan HTML version has been created. Even though this ver-\nsion allowsless waysof interaction,the user can still easi ly\nnavigate through all the pieces of music on the map, and\nselect music to listen to. The Map of Mozart can be ex-\nplored with limited amounts of music available on-line at\nhttp://www.ifs.tuwien.ac.at/mir/mozart .\nOne can ﬁnd, for example, the string ensembles in the\nregion of Mozart’s right ear, while the dances are arranged\nFigure2. Mapof Mozart withimage andSDHvisualization.\nleft-above of the string ensembles in the region of the back\nof the head. Almost all operascomposedby Mozart are lo-\ncatedinthelowerpartofthemap. Theoperasarefurtherdi-\nvidedintodifferentregions,forexamplerecitatives,loc ated\nintheregionofMozart’sneck. Aclusterofpianomusiccan\nbefoundonthetopedgecontainingpianosonatasandpiano\nconcerts. It becomesapparentthatthe clusteringabilitie sof\nthe SOM and the features extracted by the Rhythm Pattern\nalgorithm are still working on an as homogeneous dataset\nas this speciﬁc music collection by Mozart. Moreover, the\nMnemonicMapofMozartoffersattractiveandeye-catching\nvisualizations to the user, and a playful alternative to the\nK¨ ochel-VerzeichnisforexploringMozart’smusic.\nReferences\n[1] T. Kohonen. Self-Organizing Maps Springer, Berlin, Ger-\nmany, 2001.\n[2] R.Mayer,D.Merkl,andA.Rauber. MnemonicSOMs: Rec-\nognizable shapes for self-organizing maps. In Proc Work-\nshop onSelf-Organizing Maps , Paris,France, 2005.\n[3] R. Neumayer, M. Dittenbach, and A.Rauber. PlaySOMand\nPocketSOMPlayer: Alternativeinterfacestolargemusicco l-\nlections. In Proc Intl Conf on Music Information Retrieval ,\nLondon, UK,2005.\n[4] E.Pampalk, A. Rauber, andD. Merkl. Usingsmoothed data\nhistograms for cluster visualization inself-organizing m aps.\nInProc Intl Conf on Artiﬁcial Neural Networks , Madrid,\nSpain, 2002.\n[5] A. Rauber, E. Pampalk, and D. Merkl. Using psycho-\nacoustic models and self-organizing maps to create a hier-\narchical structuring of music by musical styles. In Proc Intl\nConf on Music Information Retrieval ,Paris,France, 2002."
    },
    {
        "title": "Overview of OMEN.",
        "author": [
            "Daniel McEnnis",
            "Cory McKay",
            "Ichiro Fujinaga"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1418171",
        "url": "https://doi.org/10.5281/zenodo.1418171",
        "ee": "https://zenodo.org/records/1418171/files/McEnnisMF06.pdf",
        "abstract": "This paper introduces OMEN (On-demand Metadata Extraction Network), which addresses a fundamental problem in MIR: the lack of universal access to a large dataset containing significant amounts of copyrighted music. This is accomplished by utilizing the large collections of digitized music available at many libraries. Using OMEN, libraries will be able to perform on-demand feature extraction on site, returning feature values to researchers instead of providing direct access to the recordings themselves. This avoids copyright difficulties, since the underlying music never leaves the library that owns it. The analysis is performed using grid-style computation on library machines that are otherwise under- used (e.g., devoted to patron web and catalogue use). Keywords: Music database, datasets, feature extraction, distributed computing",
        "zenodo_id": 1418171,
        "dblp_key": "conf/ismir/McEnnisMF06",
        "keywords": [
            "Music database",
            "datasets",
            "feature extraction",
            "distributed computing",
            "libraries",
            "on-demand feature extraction",
            "copyright difficulties",
            "digitized music",
            "grid-style computation",
            "under-used machines"
        ],
        "content": "Overview of OMEN Daniel McEnnis, Cory McKay, and Ichiro Fujinaga Music Technology, Schulich School of Music, McGill University 555 Sherbrooke Street West Montreal, QC H3A 1E3 {daniel.mcennis, cory.mckay}@mail.mcgill.ca, ich@music.mcgill.ca Abstract This paper introduces OMEN (On-demand Metadata Extraction Network), which addresses a fundamental problem in MIR: the lack of universal access to a large dataset containing significant amounts of copyrighted music. This is accomplished by utilizing the large collections of digitized music available at many libraries. Using OMEN, libraries will be able to perform on-demand feature extraction on site, returning feature values to researchers instead of providing direct access to the recordings themselves. This avoids copyright difficulties, since the underlying music never leaves the library that owns it. The analysis is performed using grid-style computation on library machines that are otherwise under-used (e.g., devoted to patron web and catalogue use). Keywords: Music database, datasets, feature extraction, distributed computing 1. Introduction It is becoming increasingly important for music information retrieval (MIR) researchers to have access to a large, varied dataset of music with which to test their algorithms. Researchers also need access to the same datasets in order to meaningfully compare results. From the 2005 MIREX abstract [1]:  … there remain several serious challenges that must be overcome in order to conduct future MIREX contests that consistently provide meaningful and fair scientific evaluations. These challenges include: 1. The continued near impossibility of establishing a common set of evaluation databases or the sharing of databases among researchers due primarily to intellectual property restrictions and the financial implications of those restrictions; …  The size of such a universal dataset is an obstacle. In order to provide a solid foundation for comparing algorithms, a dataset consisting of many gigabytes is needed to perform analysis. Even disregarding copyright issues, a dataset that is sufficiently large to be useful would be prohibitively expensive to transfer between researchers.  Furthermore, researchers often want to make sure that the results obtained during their research are applicable to music in use by the general public. This music is almost exclusively copyrighted music. Transferring this kind of music between researchers can involve insurmountable legal obstacles, even for a relatively small dataset.  These problems make directly copying the database for each researcher infeasible. One solution is to centralize all aspects of the analysis process from feature extraction to output. Alternatively, one can take advantage of the nature of MIR research by splitting tasks into two stages: feature extraction and analysis of these features. Issues relating to the feature extraction stage are addressed by OMEN, and analysis of the features is left to individual researchers. Although centralizing all aspects of the feature extraction and analysis is the simplest approach, this requires that a central site acquire all the music in the dataset. This is prohibitively expensive. Furthermore, this site must also provide all of the computing resources for all of the analyses—also an expensive proposition. If one provides access to the underlying features instead of performing all processing locally, then the most important information relating to the dataset can be distributed. This does not violate copyright law provided that the features extracted cannot be used to reconstruct the original recording.  One way of achieving this is to pre-compute features from the dataset and then publish the results. However, there are a number of different parameters involved in feature extraction. Accounting for all possible variations of these parameters would result in a combinatorial explosion of variations to be computed. In addition to being infeasible computationally, the storage requirements for this extraction would be prohibitively large. Furthermore, adding a new feature immediately precipitates additional computation. In order to circumvent these problems, OMEN calculates features as needed, on-demand. Only those calculations requested by researchers are computed, reducing the computation to a reasonable level. Since calculations can be repeated, if necessary, the amount of Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. © 2006 University of Victori disk space available is not an issue. Furthermore, new features can be added without requiring the additional computation involved with pre-computing features. OMEN was designed with the following priorities in mind: • The dataset may be distributed among multiple physically remote computers. • Libraries should be the primary providers of music material. • A library’s costs (e.g., new hardware) to join OMEN should be minimal. • Libraries should be able to easily add and modify materials made available through OMEN. • Libraries should be able to control the use of their own resources. • Adding new features to OMEN should be easy. • Extracted feature values should be stored in standard data formats. • Feature extraction should occur only when requested. • Researchers should not need to know where datasets are located. • It should be easy to formulate a feature extraction request. • There must be a central access point for MIR researchers to make feature extraction requests and otherwise interact with OMEN. • Researchers should be able to monitor the progress of their requests. • Space permitting, OMEN should cache extracted features. There are multiple ways to describe a distributed system such as OMEN. This paper utilizes three overlapping approaches. Firstly, the structure of OMEN is described by the activities that occur at each computer. Secondly, the interfaces provided by OMEN are described by how different users of OMEN will interact with the system. Finally, certain fundamental tasks performed by OMEN are described from start to finish. 2. Related Wor k There are a number of projects that are either used by OMEN or implement a subset of the functionality OMEN provides. jAudio is the feature extraction system utilized by OMEN. MIREX is a competition that could greatly benefit from a universal music dataset made available through OMEN. There are also a number of alternative datasets intended for MIR research. Finally, there exist several other systems that provide distributed feature extraction. 2.1 jAudio jAudio [2,3] provides an interface and engine for extracting features from audio files. It supplies features, metafeatures, and aggregators to accomplish this task. jAudio also has the important advantage that it can add new features during execution when used as a library. jAudio serves as OMEN’s feature extraction library. 2.2 MIREX The first MIREX competition [1], held in 2005 in conjunction with ISMIR 2005, provided a forum where MIR researchers could evaluate their algorithms for various tasks using common datasets. As a result, the organizers of MIREX faced the problem associated with building datasets for each of the different MIR tasks.  MIREX chose to use a centralized approach to address these problems, relying on donations of music from participants to construct their datasets. Some of these donations were of questionable legality since they involved copyrighted music. This resulted in two problems: the datasets were small, and researchers could not legally test their algorithms against a subset of the final dataset before submission. 2.3 Music Databases There have been previous attempts to generate datasets intended for MIR research. These efforts have centered on creating copyright-free datasets that could be distributed to  researchers. 2.3.1 RWC The RWC database [4] is a small dataset consisting of original recordings of jazz, classical, and pop music commissioned by the Real World Computing Partnership in Japan. This dataset is small enough that transmission is not an issue, and it is free from copyright restrictions. Unfortunately, there is not enough music present in the dataset to effectively evaluate most MIR algorithms and it does not include the popular music that many researchers are interested in. 2.3.2 Music Audio Benchmark Dataset This dataset [5] is derived from music freely available from the GarageBand website [6]. Unfortunately, the dataset is still relatively small and also does not cover popular (copyrighted) music. 2.3.3 Magnatune The Magnatune [7] dataset is a collection of music that can be licensed for research purposes. This dataset is relatively small and consists entirely of music under the Creative Commons license, which effectively excludes most popular music. 2.3.4 Classical Music Archive The Classical Archive [8] is an online repository of over 38,000 classical music files. This excludes both popular music and any classical music not in the public domain. Furthermore, access to high quality recordings is restricted to those who have paid for a subscription, and limits are placed on the number of recordings that can be downloaded each day 2.3.5 Variations2 Variations 2 [9] provides access to the digital music repositories of Indiana University’s music library. Access to the music within this archive is restricted to Indiana University students and employees.  2.4 Distributed Feature Extraction A number of other music analysis packages support distributed feature extraction. 2.4.1 M2K M2K [10] provides an environment where new features can be loaded at startup (by adding class files to its plugins folder), but not during execution. The way in which distributed computation is implemented in M2K/D2K is also somewhat inflexible, and does not incorporate  distributed computation according to resource allocation policies. M2K is also built upon D2K, which, unfortunately, is a commercial product.  2.4.2 Marsyas 0.2 Version 0.2 of Marsyas [11] introduces both distributed computing and runtime configuration of features. However, adding new features to Marsyas requires recompilation. 3. Structure OMEN constructs its dataset by utilizing existing libraries’ digitized music collections in order to provide a large and diverse collection of music for analysis. Several requirements influence the overall structure.  As discussed in Section 1, OMEN requires both a central access point for researchers and distributed control nodes for librarians. This mandates that there exist a central server with decentralized computers controlling library resources.  Libraries have computers that are typically idle during off-peak hours and unused when libraries are closed. In order to minimize the computational costs associated with on-demand feature extraction, OMEN utilizes grid computing to take advantage of these under-utilized library computers for the computationally intensive task of feature extraction. OMEN is split into three distinct kinds of participating computers—a Master Node that coordinates all tasks, Library Nodes that coordinate all tasks for a given digital library, and Worker Nodes that perform feature extraction. Each type of Node communicates with the other types via a collection of web services. 3.1 Master Node The Master Node is the central point coordinating all aspects of OMEN. It maintains metadata on all the music files currently present in the dataset (across all libraries) and presents this dataset as a single entity to researchers via an interface. The Master Node also hosts the Administrator Interface. In order to accommodate these needs, the Master Node utilizes Tomcat [12] to provide a Servlet/JSP container, MySQL for database support, and Axis [13] to provide web services support. The Master Node provides the following web services to Library Nodes: • PublishResults: allows Library Nodes to return the results of a feature extraction request to the Master Node. • NotifyAnalysisFailure: method for Library Nodes to notify the Master Node if feature extraction failed for a given request.  • FileChange.loadFileRecord: method for adding new files to the dataset. • FileChange.changeFileRecord: method for allowing libraries to change metadata for a file already in the dataset. • FileChange.deleteFileRecord: method for removing a file from the dataset. • FileChange.listFileRecord: method for listing all files that a library has contributed to the OMEN dataset. 3.2 Library Nodes Each digital library that participates in OMEN has one Library Node. Each Library Node maintains on its hard drive the portion of the dataset that this library makes available. When the Master Node delivers a feature extraction request, the Library Node divides the request into pieces, each consisting of one file, and oversees distribution of these pieces to its Worker Nodes. Furthermore, each Library Node includes the Librarian Interface so that librarians can manage each library’s participation in OMEN. In order to accommodate these needs, each Library Node utilizes Tomcat to provide a Servlet/JSP container, MySQL for database support, and Axis to provide web services support. The Library Node also provides a collection of web services for both the Master Node and its Worker Nodes: • ExecuteBatch: this service provides a means for the Master Node to request feature extraction on a number of files located at the library. The list of features to extract and their parameters are passed as arguments. • AddFeature: the Master Node utilizes this service to disseminate to the Library Nodes new features that have been approved for use by the administrator. • PublishResults: this service allows Worker Nodes to return to the Library Node the results of their analysis. The results, in either ARFF [14] or ACE XML [15] format, are passed as parameters. • Music: Worker Nodes utilize this service to retrieve files to be analyzed from the Library Node. Files may be in any of a number of formats, including mp3, wav, aiff, or au. While only the distribution of files stored on the local hard drive is implemented, it is possible to implement a stand in system that instead pulls music files from an existing system provided it implements the same web services as the basic implementation. • NotifyAnalysisFailure: Worker Nodes utilize this service to notify the Library Node if analysis could not be completed. 3.3 Worker Nodes Library computers that are provided to patrons for Internet access and searching of bibliographic records are used as Worker Nodes. OMEN utilizes grid computing in order to exploit this under-utilized resource for feature extraction. In order to prevent library patrons from experiencing degraded service, Worker Nodes provide a number of settings that assist in minimizing the impact on patrons’ tasks. In order to accommodate these needs, each Worker Node utilizes Axis, within Tomcat, to provide web services support. Feature extraction is performed by jAudio. jAudio was chosen for this task because it is written in Java, is easy to embed in other applications, implements a wide variety of features, and allows OMEN to add new features during execution. Worker Nodes provide the following web services to communicate to their Library Node: • AddFeature: method for Library Nodes to add new features to jAudio. • ExecuteBatch: method used to initiate feature extraction on a file. • ApplySettings: method for configuring the policies for minimizing impact on the Worker Nodes’ primary task of serving patrons. 4. Interfaces In order to be useful to the MIR community, OMEN must provide user-friendly interfaces to each of its different types of users. The Librarian Interface provides a mechanism for librarians to each control their respective library’s contributions. The Researcher Interface allows MIR researchers to view the dataset’s metadata and extract features from it. Finally, the Administrator Interface permits the overall administrator to maintain and police OMEN. 4.1 Librarian Interface In order for OMEN to function, librarians must have an easy mechanism for both making their digitized music collection available to the MIR community and controlling OMEN’s use of computing resources within the library. The library interface, hosted at each Library Node, accomplishes these tasks. OMEN provides an interface for allowing librarians to modify the recordings whose features are made available to the public. There are several methods available. One mechanism to do this is to use iTunes to rip CDs into an archive. The GraceNote CDDB [16] service provides metadata. Ideally, this metadata should be cleaned using software such as jMetaManager [17]. A librarian then uploads the location of the files and the associated metadata into OMEN by uploading the XML document created by iTunes that describes the ripped music. Alternatively, librarians can use their own favorite program for ripping CDs and enter the metadata in a web-based form. This web form can also be used to modify or delete files from the database. While the legality of ripping CDs has not been tested in U.S. court, an attorney of the Recording Industry Association of America (the most likely plaintiff) has testified in the U.S. Supreme Court that they believe that ripping CDs is not a copyright infringement [18]. Library Nodes must also control how their Worker Nodes perform the feature extraction. As it is desirable to prevent OMEN from interfering with these computers’ primary purpose, librarians are presented with options for accomplishing this goal. One approach is to set the priority of the computation sufficiently low that it does not interfere with other activities. A second approach is to restrict the time periods during which Worker Nodes can extract features so that they only operate during the hours a library is closed. Since computations generally take at most minutes, Worker Nodes effectively stop computation when the Library Portal stops distributing tasks. Any changes made in these settings are communicated to all Worker Nodes in the library via the ApplySettings web service. 4.2 Researcher Interface The Researcher Interface is hosted by the Master Node and provides researchers with access to the dataset that is spread across all participating libraries as if it were a single entity. Researchers can choose what subset of the entire dataset to utilize, which features to extract, and the parameters to use for these features. Additionally, researchers can submit new features for use in OMEN. OMEN provides a search mechanism that allows researchers to search the metadata of all the files in the dataset. Returned results can be refined either by additional searches or by manually pruning the result set. Once the desired dataset has been constructed, the researcher can save this result for future use.  Researchers have two different mechanisms for establishing the parameters to be used in feature extraction. One method is to save a settings file in jAudio and then upload this file in the Researcher Interface. The other is to manually specify the settings via a web interface. Whichever method is used, researchers can store these settings for later use. Once a researcher has chosen the query set and feature parameters, an extraction request can then be executed. The Master Node parses the query set, divides the set according to the libraries holding the requested data, and uses the ExecuteBatch service provided by the Library Nodes to begin extraction.  OMEN also serves as a repository for feature extraction algorithms. Researchers are encouraged to submit new features to OMEN so that they can be used by other researchers. In the current version of OMEN, features must be implemented in the Java programming language by sub-classing either the FeatureExtractor or Aggregator abstract classes of jAudio.  4.3 Administrator Interface The Administrator Interface (located at the Master Node) provides an administrator with the tools needed to maintain OMEN. The administrator adds and removes library accounts using this interface. In addition, the administrator can delete researchers or reset their passwords. Furthermore, this interface allows the administrator to view submitted new features and either approve or reject them. 5. Implementation Here we describe OMEN by following the execution path of several example tasks. 5.1 Adding Files to OMEN A librarian adds music files to the system via one of two methods: by uploading an iTunes XML file or by adding the files manually and then using the Librarian Interface to add the metadata. In either case, the Library Node uploads the metadata about the files to the Master Node. Upon receiving the metadata, the Master Node adds this metadata to its database. During this process, the Master Node generates a list of unique IDs for the files added, uniquely identifying them in OMEN.  This list of IDs is returned to the Library Node that added the files. These IDs are then linked to the location of the file and stored in the database locally. 5.2 Feature Extraction Feature extraction is initiated in the Researcher Interface at the Master Node. The researcher provides the list of files to be analyzed and the feature settings to be used. First, the request is analyzed and altered as necessary to avoid violating copyright law. The request is then split into a separate request for each library that has files in the dataset, identifying each music file by the unique ID it was given when it was added to OMEN. The request is added to the list of outstanding requests for this researcher. The outstanding request is then added to the Researcher Interface. Then the request is sent to each Library Node via the ExecuteBatch web service. The Library Node parses the requests into numerous small requests of one file each. These requests are queued internally. As Worker Nodes become available, the requests are dispatched using each Worker Node’s ExecuteBatch web service. Each Worker Node that receives a request first checks to see if it has the file requested in its internal cache. If not, the Worker Node calls the Music web service at the Library Node to download it. Feature extraction is performed using jAudio and the results are returned to the Library Node via its PublishResults web service. The Library Node collects all the results from the Worker Nodes. Once the results are all collected, it relays the extracted features back to the Master Node via its PublishResults web service. As Library Nodes return results, the Researcher Interface is updated to reflect ongoing progress. Once all results are returned, they are made available through the Researcher Interface as either Weka ARFF [14] or ACE XML files [15]. 5.3 Adding a New Feature A researcher can add a new feature by submitting Java source code through the Researcher Interface. The code is then made available to the administrator through the Administrator Interface. The administrator then checks the code for security. Once the code is cleared, the administrator compiles the code and submits the compiled code for distribution. The Master Node distributes this code to each Library Node via the AddFeature web service and updates its internal list of features. Each Library Node takes the incoming compiled code and then redistributes this code to its Worker Nodes via their AddFeature web service. Each Worker Node updates its list of features with the new feature and adds the compiled code to the directory that jAudio automatically searches for new features. 6. Example Dataset The functionality of OMEN has been successfully tested with two different music libraries, namely the Codaich database [17] (16 803 songs totaling 80 GB) and a private CD collection (577 songs totaling 3 GB). 7. Conclusions and Future Wor k OMEN provides a mechanism for legally providing all MIR researchers with access to a large dataset of music. This will permit researchers to compare results of MIR algorithms using the same music at any time, not just during specific competitions. It will also save researchers significant amounts of time by removing the need to construct their own datasets or resort to non-representative music in the public domain. However, there is still work to be done. In particular, the system requires additional security to prevent unauthorized access of OMEN resources. This includes functionality such as digital signatures for authenticating communications between nodes. Another area for improvement is the scheduling algorithm for the Library Nodes. The required bandwidth could be greatly reduced if more care were taken to distribute feature extraction requests to Worker Nodes that already have needed files in their caches.  8. Acknowledgments We would like to thank the Social Sciences and Humanities Research Council of Canada for their financial support and the Canada Foundation for Innovation for supplying the equipment used in this project. References [1] J. S. Downie, K. West, A. Ehmann, and E. Vincent, “The 2005 Music information retrieval evaluation exchange (MIREX 2005): Preliminary overview,” in Proceedings of the International Conference on Music Information Retrieval, 2005, pp. 320–323. [2] D. McEnnis, C. McKay, I. Fujinaga, and P. Depalle, “jAudio: A feature extraction library,” in Proceedings of the International Conference on Music Information Retrieval, 2005, pp. 600–603. [3] D. McEnnis and I. Fujinaga. “jAudio: Additions and Improvements,” in Proceedings of the International Conference on Music Information Retrieval, 2006 (in press). [4] M. Goto, H. Hashiguchi, T. Nishimura, and R. Oka, “RWC music database: Popular, classical and jazz music databases,” in Proceedings of the International Conference on Music Information Retrieval, 2002, pp. 287–288. [5] H. Homburg, I. Mierswa, B. Möller, K. Morik, and M. Wurst. “A benchmark dataset for audio classification and clustering,” in Proceedings of the International Conference on Music Information Retreival, 2005, pp. 528–531. [6] “GarageBand.com,” [Web site] 2006, [2006 March 27], Available: http://www.garageband.com [7] “Magnatune: MP3 music and music licensing,” [Web Site] 2006, [2006 March 27], Available: http://magnatune.com [8] “Classical Music Archives,” [Web Site] 2006, [2006 July 6], Available: http://www.classicalarchives.com [9] J. W. Dunn and M. Notess. “Variations 2: The Indiana University Digital Music Library Project” in Proceedings of Digital Library Federation Fall Forum, 2002. [10] J. S. Downie, J. Futrelle, and D. Cheng. “The international music information retrieval systems evaluation laboratory,” in Proceedings of the International Conference on Music Information Retreival, 2004, pp. 9–14. [11] S. Bray and G. Tzanetakis. “Distributed audio feature extraction for music,” in Proceedings of the International Conference on Music Information Retrieval, 2005, pp. 434–437. [12] “Apache Tomcat,” [Web site] 2006, [2006 April 14], Available: http://tomcat.apache.org [13] “Web Services – Axis,” [Web site] 2006, [2006 April 14], Available: http://ws.apache.org/axis [14] I. Witten and E. Frank. Data Mining: Practical Machine Learning Tools and Techniques, 2nd ed., New York: Morgan Kaufman, 2005. [15] C. McKay, D. McEnnis, R. Fiebrink, and I. Fujinaga. “ACE: A general purpose classification ensemble optimization framework,” in Proceedings of the International Conference on Music Information Retrieval, 2005, pp. 161–164. [16]  “Gracenote,” [Web site] 2006, [2006 March 29], Available: http://www.gracenote.com [17] C. McKay, D. McEnnis, and I. Fujinaga. “A large publicly accessible annotated audio music research database,” in  Proceedings of the International Conference on Music Information Retrieval, 2006 (in press). [18] “Supreme Court of the United States: Argument Transcripts” [Web site] 2006, [2006 July 1], Available: http://www.supremecourtus.gov/oral_arguments/argument_transcripts/04-480.pdf"
    },
    {
        "title": "jAudio: Additions and Improvements.",
        "author": [
            "Daniel McEnnis",
            "Cory McKay",
            "Ichiro Fujinaga"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.5793205",
        "url": "https://doi.org/10.5281/zenodo.5793205",
        "ee": null,
        "abstract": "Supplementary files to Genome-wide Bioinformatics Analysis Reveals the Deduced Evolutionary Origin of BnGRAS genes in Brassica Genus.",
        "zenodo_id": 5793205,
        "dblp_key": "conf/ismir/McEnnisMF06a",
        "keywords": [
            "Genome-wide",
            "Bioinformatics",
            "Analysis",
            "Deduced",
            "Evolutionary",
            "Origin",
            "BnGRAS",
            "genes",
            "Brassica",
            "Genus"
        ]
    },
    {
        "title": "Musical genre classification: Is it worth pursuing and how can it be improved?",
        "author": [
            "Cory McKay",
            "Ichiro Fujinaga"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1417417",
        "url": "https://doi.org/10.5281/zenodo.1417417",
        "ee": "https://zenodo.org/records/1417417/files/McKayF06.pdf",
        "abstract": "Research in automatic genre classification has been pro- ducing increasingly small performance gains in recent years, with the result that some have suggested that such research should be abandoned in favor of more general similarity research. It has been further argued that genre classification is of limited utility as a goal in itself because of the ambiguities and subjectivity inherent to genre. This paper presents a number of counterarguments that emphasize the importance of continuing research in auto- matic genre classification. Specific strategies for overcom- ing current performance limitations are discussed, and a brief review of background research in musicology and psychology relating to genre is presented. Insights from these highly relevant fields are generally absent from dis- course within the MIR community, and it is hoped that this will help to encourage a more multi-disciplinary approach to automatic genre classification in the future. Keywords: Genre, classification, music, improvements.",
        "zenodo_id": 1417417,
        "dblp_key": "conf/ismir/McKayF06",
        "keywords": [
            "performance gains",
            "abandonment",
            "utility",
            "ambiguities",
            "subjectivity",
            "counterarguments",
            "strategies",
            "current limitations",
            "review",
            "insights"
        ],
        "content": "Musical genre classification: Is it worth pursuing and how can it be improved? \nCory McKay and Ichiro Fujinaga \nMusic Technology, Schulich School of Music, McGill University \nMontreal, Quebec, Canada \ncory.mckay@mail.mcgill.ca, ich@music.mcgill.ca \nAbstract \nResearch in automatic genre classification has been pro-\nducing increasingly small performance gains in recent \nyears, with the result that some have suggested that such \nresearch should be abandoned in favor of more general \nsimilarity research. It has been further argued that genre \nclassification is of limited utility as a goal in itself because \nof the ambiguities and subjectivity inherent to genre.  \nThis paper presents a number of counterarguments that \nemphasize the importance of continuing research in auto-\nmatic genre classification. Specific strategies for overcom-\ning current performance limitations are discussed, and a \nbrief review of background research in musicology and \npsychology relating to genre is presented. Insights from \nthese highly relevant fields are generally absent from dis-\ncourse within the MIR community, and it is hoped that this \nwill help to encourage a more multi-disciplinary approach \nto automatic genre classification in the future. \nKeywords: Genre, classification, music, improvements. \n1. Introduction \nAutomatic genre and style classification have been popular \ntopics in MIR research in the past. The ground-breaking \nwork of Dannenberg, Thom and Watson [1] and of \nTzanetakis and Cook [2] is particularly well-known, and \nmore recent work includes the MIREX 2005 winning work \nof both Bergstra et al. [3] and of McKay and Fujinaga [4]. \nThe many other exciting approaches applied to these prob-\nlems are too numerous to include here, but a survey of the \nISMIR proceedings from the past several years and their \nreferences will reveal the many different approaches used. \nDespite this popularity, the view that further research in \nautomatic genre classification will offer little of value has \nincreasingly been expressed in informal discussions among \nresearchers and on the MIREX and Music-IR mailing lists. \nIt has been suggested that it would be more profitable to \npursue research on more general music similarity instead, \nsuch as playlist generation, similarity-based browsing inter-\nfaces and recommendation systems.  This paper seeks to consider the arguments for and \nagainst further research in automatic genre classification, \nand a variety of major changes are proposed regarding how \ngenre classification should be approached. This paper addi-\ntionally includes a brief review of musicological and psy-\nchological research on genre and human classification.  \nBefore proceeding, it is useful to briefly discuss the dif-\nference between “style” and “genre,” as some disagreement \nhas been expressed regarding these terms. Although there \nare no universally accepted definitions, Franco Fabbri has \nusefully defined genre as “a kind of music, as it is ac-\nknowledged by a community for any reason or purpose or \ncriteria, i.e., a set of musical events whose course is gov-\nerned by rules (of any kind) accepted by a community” and \nstyle as “a recurring arrangement of features in musical \nevents which is typical of an individual (composer, per-\nformer), a group of musicians, a genre, a place, a period of \ntime” [5]. It might be added that style is primarily related \nto individuals or groups of people involved in music pro-\nduction and that genre is related to more general groups of \nmusic and their audiences. Genre can thus be broader and \nmore nebulous than style from a content-based perspective, \nand may be more strongly characterized by cultural fea-\ntures. The differences between genre and style are dis-\ncussed in more detail in many of the references referred to \nin Section 2. \nDespite these differences, many systems designed for \ngenre classification could easily be applied to style classifi-\ncation, and vice versa, so strictly differentiating between \nthe two is not necessarily an issue of primary importance \nfrom an MIR perspective. Although this paper targets is-\nsues relating to genre specifically, many of the points made \nhere apply to style classification as well. \n2. Insights From Other Disciplines \nGenre is an area of inquiry that has been given significant \nattention in a variety of academic fields, with a particular \nemphasis found in literary (e.g., [6]) and film (e.g., [7]) \nstudies. This research attempts to address issues such as \nhow genres are created, how they can be defined, how they \nare perceived and identified, how they are disseminated, \nhow they change, how they are interrelated and how we \nmake use of them. \nA number of musicologists have adapted this work to \nmusic and expanded upon it. Much of their research has Permission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fee provided that copies \nare not made or distributed for profit or commercial advantage and that \ncopies bear this notice and the full citation on the first page. \n© 2006 University of Victoria emphasized the role of cultural factors in genre. Fabbri, for \nexample, suggests that musical genres can be characterized \nusing the following types of rules, of which only the first is \nrelated strictly to musical content [8]: \n· Formal and technical: Content-based practices. \n· Semiotic: Abstract concepts that are communicated (e.g., \nemotions or political messages). \n· Behavior: How composers, performers and audiences appear \nand behave. \n· Social and ideological: The links between genres and demo-\ngraphics such as age, race, sex and political viewpoints. \n· Economical and juridical: The laws and economic systems \nsupporting a genre, such as record contracts or performance \nlocales (e.g., cafés or auditoriums). \nFabbri has also contributed many other ideas on diverse \naspects of musical genre [5, 8, 9]. Frith offers insights on \nhow musical genres are formed and what they mean [10]. \nToynbee discusses how genres inform musicians and how \nthey are influenced by identification with different commu-\nnities and by the music industry [11]. Brackett has pro-\nvided useful ideas on how genres can be characterized and \non how genres are constructed, how they can be grouped \nand how they change [12, 13]. Important research has also \nbeen published on how genres can be organized from a \ntechnological perspective [14, 15]. \nA better understanding of the psychological processes \ninvolved in human music classification can also prove use-\nful to MIR genre researchers. Not only does it help one \nmodel human classification behavior, but it can also be \nuseful in designing interfaces that better meet human needs. \nEarly psychological models assumed that humans form \ncategories by specifying necessary and sufficient condi-\ntions for each category. The diverse work of Eleanor Rosch \nhas been influential in experimentally demonstrating the \nshortcomings of this approach and in promoting alternative \nmodels where categories are hierarchically organized and \nare defined by prototypical exemplars. Lakoff has written \nan excellent overview of these developments in the psy-\nchology of classification [16], and many papers have since \nbeen published proposing variations of exemplar theory. \nA number of psychologists have applied these ideas to \nmusic. Deliege, for example, has suggested that humans \nabstract useful features contained in music into “cues,” and \nuse these to segment music, judge musical similarity and \nform “imprints” that help us to perceive musical structure, \nevaluate similarity and perform classifications [17]. There \nis also an extensive literature on the perception and cogni-\ntion of musical similarity that is too extensive to cite here, \nbut is certainly relevant to MIR-related similarity research. \nImportant research has also been performed by popular \nand ethnomusicologists on developing features that can be \napplicable to categorizing a diverse range of musics. Al-\nthough there is insufficient room to present details here, a \nreview has been previously written [18].  3. Problematic Aspects of Genre \nThe acquisition of reliable ground truth is a key require-\nment of training effective genre classifiers. It has been sug-\ngested that only limited agreement can be achieved among \nhuman annotators when classifying music by genre, and \nthat such limits impose an unavoidable ceiling on auto-\nmatic genre classification performance. Not only can indi-\nviduals differ on how they classify a given recording, but \nthey can also differ in terms of the pool of genre labels \nfrom which they choose. Very few genres have clear defini-\ntions, and what information is available is often ambiguous \nand inconsistent from source to source. There is often sig-\nnificant overlap between genres, and individual recordings \ncan belong to multiple genres to varying degrees. There are \noften complex relationships between genres, and some gen-\nres are broad while others are narrow. Furthermore, genres \noften encapsulate multiple discrete clusters (e.g., Baroque \nmusic could include both a Monteverdi opera and a Scar-\nlatti harpsichord sonata).  \nOnly a small amount of experimental psychological re-\nsearch has been performed on human genre classification. \nAn often-cited preliminary study found that a group of un-\ndergraduate students made classifications agreeing with \nthose of record companies only 72% of the time when clas-\nsifying among ten genres [19]. Listeners in these experi-\nments were only exposed to 300 ms of audio per recording, \nhowever, and higher agreement rates could quite possibly \nhave been attained had longer listening intervals been used. \nAnother study involving longer thirty second listening in-\ntervals found inter-participant genre agreement rates of \nonly 76% [20]. However, one of the six categories used in \nthis study was “Other,” an ambiguity that could lead to \nsubstantial disagreement due to degree of membership and \ncategory coarseness rather than entirely different classifica-\ntions. So, although these two studies do provide useful in-\nsights, there is clearly a need for more experimental evi-\ndence before definitive conclusions can be drawn regarding \nupper bounds on software performance due to limits in \nhuman genre classification.  \nIn any case, although some good sources of ground truth \ndo exist, such as the AllMusic Guide [21], they are few in \nnumber and often contain too much or too little informa-\ntion. Furthermore, genre classifications tend to be by artist \nor album rather than by individual recording. Those \nsources that do provide classifications of individual re-\ncordings—such as Gracenote CDDB [22] or the metadata \nfound in MP3 ID3 tags—tend to have unreliable annota-\ntions. There is also usually little or no documentation on \nhow classifications were performed, and it is often doubtful \nwhether serious effort was put into thoughtful, methodical \nand consistent annotations. \nThe expertise and time needed to manually clasify re-\ncordings pose serious obstacles to the production of quality \nground truth. This is particularly true when large datasets are needed to avoid overtraining and to effectively learn \nmodels that incorporate the ambiguities and inconsistencies \nthat one finds with genre. \nTo further complicate matters, not only are new genres \nintroduced regularly, but the understanding of existing gen-\nres can also change with time, which can necessitate re-\ntraining of systems and re-annotation of ground truth. The \nneed for a large training set also has implications in terms \nof machine learning. Powerful learning algorithms such as \nsupport vector machines or AdaBoost are needed to model \nhighly complex genre spaces effectively, but many of the \nmost powerful algorithms do not scale well. \nIn terms of actual software performance to date, no sys-\ntem has yet achieved sufficiently high success rates to \nmake it usable in realistic situations. For example, the \nhighest success rates in the MIREX 2005 audio genre clas-\nsification contest were 75% and 87% when classifying \namong ten and six genres, respectively, and the highest \nrates were 46% and 86% in the symbolic classification \ncontest for 38 and 9 genres, respectively [23]. Furthermore, \nit has been observed that recent systems that assess audio \nsimilarity in general using timbre-based features have \nfailed to achieve major performance gains over earlier sys-\ntems [24]. It is clear that fundamentally new approaches \nare needed if automatic genre classification is to become \npractically viable. \n4. Arguments in Favor of Using Genre \nBefore proposing ways of overcoming the serious issues \ndescribed in the previous section, it is appropriate to first \nemphasize the usefulness of automatic genre classification. \nIt has been suggested in a number of discussions that genre \nis a hopelessly ambiguous and inconsistent way to organize \nand explore music, and that users’ needs would be better \naddressed by abandoning it in favor of more general simi-\nlarity-based approaches. Those adhering to this perspective \ngenerally hold that genre is only a subset of broader simi-\nlarity research and has only been worth pursuing as an ini-\ntial limited stage of research where features and learning \nalgorithms can be developed, compared and refined. \nAlthough it is true that genre is in some ways a subset of \nthe more general similarity problem, genre involves a spe-\ncial emphasis on culturally predetermined classes that \nmakes it worthy of separate attention. Even similarity \nmeasurements that involve cultural features such as playlist \nco-occurrence tend to be based on individual preferences \nrather than genre’s more formal sociocultural agreements \n(see Section 2). In essence, the query “find me something \nlike this (relatively small) set of recordings” is intrinsically \ndifferent from “find me something in this generally under-\nstood genre category,” which could encompass a poten-\ntially huge set of recordings and which is based on cultur-\nally determined categories rather than more content-\noriented or  individually defined similarity.  This highlights the importance of cultural features and \nthe ever increasing variety and scale of metadata that can \nbe mined from the web. Relatively little attention has been \ngiven to these types of features to date, yet they could well \nhold the potential to surpass current limitations on classifi-\ncation performance. The potential of such features will \ncontinue to increase as more metadata becomes available \non the web and in recordings themselves. \nThe question remains whether genre classification is \nuseful to end users, or simply an awkward and obsolete \nlabeling system. Although browsing and searching by genre \nis certainly not perfect—and alternatives are always worth \nresearching—end users are nonetheless already accustomed \nto browsing both physical and on-line music collections by \ngenre, and this approach is proven to be at least reasonably \neffective. A recent survey, for example, found that end \nusers are more likely to browse and search by genre than \nby recommendation, artist similarity or music similarity, \nalthough these alternatives were each popular as well [25]. \nResources such as the AllMusic Guide [21], which use \nlabeled fields such as genre, mood and style are commonly \nused, while alternative similarity-based interfaces have yet \nto be widely adopted by the public, despite the increasing \nmedia attention that they have been receiving.  \nLabels such as genre and mood have the important ad-\nvantage that they provide one with a vocabulary that can be \nused to discuss musical categories. Conversations concern-\ning more general notions of similarity quickly become \nbogged down due to the necessity of making frequent ref-\nerences to musical examples. Moreover, such discussions \ncan be unclear in terms of which dimensions of similarity \nare being considered.  \nMIR researchers should avoid adopting a patronizing \napproach where they insist that end users abandon a form \nof music retrieval for which they have a demonstrated at-\ntachment and which they find useful. A better approach is \nto recognize and utilize genre in MIR systems while at the \nsame time also presenting alternatives utilizing more gen-\neral similarity that can also be useful, potentially in entirely \ndifferent user scenarios. \nOnce one accepts the usefulness of genre for end users, \nthe utility of automatically classifying the genres of re-\ncordings stored in music databases becomes clear. Al-\nthough the time needed to label training data and the noisi-\nness of existing annotations have already been presented as \nserious problems when training automatic genre classifiers, \nthe difficulty of manually labeling the entirety of huge and \nrapidly growing databases is much greater. \nAlso, similarity research has many of its own problems \nrelated to ground truth ambiguity and subjectivity, particu-\nlarly when it comes to evaluating systems and comparing \ntheir performance. It is therefore inconsistent to suggest \nsimilarity as an alternative to genre classification specifi-\ncally because of problems relating to ground truth. Musical genre also has significant importance beyond \nsimply its utility in organizing and exploring music, and \nshould not be evaluated solely in terms of commercial ap-\nplicability. Many individuals actively identify culturally \nwith certain genres of music, as can easily be observed in \nthe differences in the ways that many fans of death metal or \nrap dress and speak, for example. Genre is so important to \nlisteners, in fact, that psychological research has found that \nthe style of a piece can influence listeners’ liking for it \nmore than the piece itself [26]. Additional psychological \nresearch has found that categorization in general plays an \nessential role in music appreciation and cognition [27].  \nResearch in automatic genre classification can also pro-\nvide valuable empirical contributions to the fields of musi-\ncology and music theory (e.g., [28]). Genre research that \nforms correlations between particular cultural and content-\nbased characteristics or that involves ontological structures \nthat can successfully map genre interrelationships can also \nhave important musicological significance. \n5. Improving Automatic Genre Classification \nThere is truth to the criticisms that genre classifiers appear \nto have reached a maximum in performance, but there is no \nevidence that this is a ceiling that cannot be surpassed. Al-\nthough continuing minor refinements are not likely to ac-\ncomplish much, there are a number of major changes to \nhow automatic genre classification is approached that \ncould result in significant improvements. \nMost genre classification systems to date have utilized \nprimarily low-level features relating to timbre. It is not \nsurprising that the performance of such systems has been \nlimited, as timbre represents only a relatively small part of \nwhat humans consider when they classify music. High-level \nfeatures based on musical abstractions are central to com-\nposers and performers and, as discussed in Section 2, many \nmusicologists hold that cultural information beyond the \nscope of musical content is of paramount importance.  \nEach of these three types of features can encompass sig-\nnificantly different information, and combining features \nfrom the different groups could significantly improve suc-\ncess rates. Promising results have already been attained by \ncombining high-level features with automatic feature selec-\ntion [4, 18], and it has been experimentally demonstrated \nthat combining cultural features mined from the web with \nlow-level features can significantly improve performance \nover low-level features alone [29]. Although cultural and \nhigh-level features can be more difficult to extract than \nlow-level features, extensive existing research in text min-\ning can be taken advantage of to extract cultural features \nfrom the web, and improvements in transcription technol-\nogy are making high-level features increasingly accessible \nin audio recordings as well as symbolic recordings.  \nAn additional important issue that has rarely been ad-\ndressed by published systems is that it should be possible \nto assign multiple genres to individual recordings, both in terms of classifier output and ground truth. Research in \nfuzzy logic should also be considered, and class member-\nships should be weighted, even if only casually. Although it \ncan be argued that this puts an even greater load on annota-\ntors, weights do not have to be perfectly precise, as the \npoint is simply to allow one to express some general sense \nof the relative importance of different genres. Allowing \nannotators to assign multiple labels could actually make \ntheir work easier, as they could express their real views \nwithout being confined to awkward artificial classification \nschemes. Most importantly, this approach would signifi-\ncantly improve the quality of ground truth, and would make \nthe evaluation of systems more realistic.  \nGround truth collection and labeling should be consid-\nered high priority goals in and of themselves. The construc-\ntion of custom training and testing databases have tradi-\ntionally comprised only a small part of larger projects, and \nas such have not received the attention that they warrant. \nAlthough some researchers have used existing collections \nsuch as Magnatune or Epitonic rather than constructing \ntheir own databases, they have generally not made serious \nefforts to refine and correct the provided metadata, which \ncan be inconsistent or even incorrect, and have often failed \nto ensure that the music in such collections is representa-\ntive of the commercial music that most users are actually \ninterested in. \nIn general, training and testing databases tend to be too \nsmall to be sufficiently diverse or to average out annotation \nnoise. Annotations also tend to be error-prone due to the \nlimited expertise of individual annotators, insufficient time \nfor thoughtful annotations and biases due to the needs of \nparticular systems. Trained models and evaluation metrics \ncan ultimately only be as good as the ground truth that they \nare built upon, and the need for high-quality ground truth \nmust be addressed before truly successful systems can be \nproduced. Research should therefore be performed on dif-\nferent ways of constructing and maintaining research data-\nbases, including comparisons of methodologies such as \nusing panels of experts, general surveys and automated \nweb-based mining of ground truth labels.  \nAnother issue is that recordings are often annotated as \ngroups based on artist or album. Although this can be ef-\nfective in some limited cases, it is almost always problem-\natic if sufficiently fine genres are being considered. For \nexample, although one might label Christina Aguilera in \ngeneral as pop, a thoughtful annotator might label some \nsongs as pop ballads, some as dance pop and some as \nR&B. Furthermore, some artists (e.g., Neil Young or Miles \nDavis) have had such musically diverse careers that at-\ntempting to label all of their work with even a fairly broad \ngenre is unrealistic. Serious efforts must therefore be made \nto annotate databases on a song by song basis. \nIt is also important to pay careful attention to the palette \nof genre labels that can be assigned to recordings. Expert \nopinion, general surveys and web mining could once again prove useful. Genre labels must be chosen that actually \nrepresent categories that users are interested in and that do \nnot force annotators to make artificial decisions. Also, \nthere should be many different candidate genres, including \nboth coarse and broad categories. The common practice of \nusing only ten or so categories is very unrealistic. Elec-\ntronic dance music alone can easily be broken into twenty \nor thirty sub-genres, for example. \nIncorporating some sort of ontological structure that \nmaps the interrelationships and intersections between genre \ncategories could also be highly beneficial. This would not \nonly provide annotators with a helpful framework that \ncould aid in synchronizing differences of opinion, but \nwould also allow the use of structured classification strate-\ngies. Research has already found that even a simple hierar-\nchical classification strategy that allows individual catego-\nries to appear in multiple branches can result in improved \nsuccess rates [18]. The use of ontological genre structures \nwould also provide end users with a structure to use when \nbrowsing music collections, and would allow exploration at \ndifferent levels of granularity. A user only mildly interested \nin electronic dance music might be happy with an overall \nclassification such as techno, for example, while other us-\ners would require many finer categories that might confuse \nthe first user. The “emergent” approach proposed by Au-\ncouturier and Pachet [14] could prove useful in construct-\ning such ontologies.  \nAn additional issue is that some forms of misclassifica-\ntion are far more significant than others. Misclassifying \nhard rock as heavy metal, for example, is less serious than \nlabeling it ragtime. Failure to consider this during training \nand evaluation could limit the quality of a learned model. \nAn ontological structuring as discussed above has the addi-\ntional advantage that it could help to implement realistic \npenalization schemes during training and evaluation. \nYet another issue is that not only can different parts of a \nsingle recording belong to different genres, but different \nsections of a recording might be representative of the same \ngenre in different ways. The verse and chorus of a pop \nsong, for example, will be different from each other but \nwill still both be characteristic of pop songs. Alternatively, \ndifferent sections of a recording can belong to different \ngenres, which might in itself be indicative of a broader \ngenre (e.g., rap metal). In essence, different sections of a \nrecording can correspond to separate clusters that may or \nmay not belong to the same class, which means that indi-\nvidual recordings should ideally include segmented labels. \nThis also means that averaging features over long windows \nor over an entire recording in order to make a single classi-\nfication can be a limiting approach. \nThe structure of a piece and how it evolves over time \ncan also be highly indicative of a genre. Examples include \nsonata form or twelve-bar blues form. This means that even \nclassifying small windows of a recording independently \ncan potentially ignore important information. Utilizing fea-tures that encapsulate changes over time and/or classifiers \nwith memory (e.g., hidden Markov models or recurrent \nneural networks) is a potentially effective approach that has \nbeen largely neglected to date. A pre-processing system \nthat segments recordings based on form could also help, \nnot only by allowing musically meaningful window sizes to \nbe set dynamically, but also by generating new features \ndelineating form. \nAn additional issue, from a musicological perspective, is \nthat researchers often use statistical techniques such as \nprincipal component analysis to reduce feature dimension-\nality. Although fine when considered purely in terms of \nsuccess rates, this limits the quality of results from a theo-\nretical perspective, as one loses potentially meaningful \ninformation about which musical qualities are most useful \nin different contexts. A more profitable approach, from a \nmusicological perspective, might be to use feature selection \ntechniques that reduce dimensionality while maintaining \nthe original identity of the features, such as forward-\nbackward selection or genetic algorithm-based selection. \nThere is also an important need to perform further psy-\nchological research on human genre classification. Studies \nshould compare the classification differences between ex-\nperts and non-experts, as well between individuals of dif-\nferent ages, cultures and musical backgrounds. This could \nprove beneficial not only in learning how to improve \nground truth, but also in developing different systems that \nmeet different user needs. A musicologist, a record indus-\ntry scout, a teenaged consumer and a librarian, for exam-\nple, will all have very different needs, and successful sys-\ntems should be able to address such varied needs.  \n6. Conclusions \nAutomatic genre classification is a difficult and problem-\natic task that nonetheless has important value in terms of \nboth pure research and commercial application. Continuing \nresearch in automatic genre classification has much to of-\nfer, as does parallel research involving other aspects of \nmusical similarity. \nAutomatic genre classification performance appears to \nhave fallen into a local maximum recently, and serious \nmodifications to the approaches used are needed in order to \nrealize further improvements. The highlights of the sugges-\ntions offered in this paper are as follows: \n· Information from low-level, high-level and cultural features \nshould be combined. \n· Each recording should be permitted to have more than one \ngenre label, and labels should be weighted. \n· A large number of realistic and diverse candidate genre labels \nof varying breadth should be used, and these genres should \nbe organized into ontological structures. \n· Misclassification penalties in training and evaluation should \nreflect the varying similarities between different genres. \n· It should be possible to label different sections of a recording \ndifferently, and windows should be classified individually. · Sequential classifiers and features that can encapsulate how \nrecordings change over time should be experimented with. \n· Dimensionality reduction techniques that preserve the origi-\nnal meaning of features should be used. \n· Existing psychological, musicological and music theoretical \nknowledge should be taken advantage of by MIR researchers, \nand new empirical research in these domains should be per-\nformed to help fill the gaps in our understanding of musical \ngenre and of how humans classify music. \nMost importantly, all areas of MIR research could bene-\nfit from a concerted effort to develop carefully annotated \nmusic datasets that include varied metadata such as genre, \nmood, groove, composer, performer, lyrics, meter, chord \nprogressions, instruments present, etc. Limited and/or \npoorly annotated ground truth is a problem that has an ef-\nfect well beyond the scope of genre classification in limit-\ning MIR research, and the benefits of a large-scale effort to \nconstruct high-quality ground truth would more than justify \nthe extensive work needed to do so. \n7. Acknowledgments \nWe would like to thank the researchers with whom we have \ninformally discussed these issues, as well as those who \nhave made insightful comments on the Music-IR and \nMIREX mailing lists. This includes J. Stephen Downie, \nDouglas Eck, Dan Ellis, Joe Futrelle, Paul Lamere, Elias \nPampalk, George Tzanetakis, Kris West and the research-\ners at the McGill University Music Technology labs. We \nwould also like to thank the Social Sciences and Humani-\nties Research Council of Canada and the Canada Founda-\ntion for Innovation for their generous financial support. \nReferences \n[1] R. B. Dannenberg, B. Thom, and D. Watson. “A Machine \nLearning Approach to Musical Style Recognition,” in Pro-\nceedings of the International Computer Music Conference, \n1997, pp. 344–7. \n[2] G. Tzanetakis., and P. Cook. “Musical Genre Classification \nof Audio Signals,” IEEE Transactions on Speech and Audio \nProcessing, vol. 10, no. 5, pp. 293–302, 2002. \n[3] J. Bergstra, N. Casagrande, D. Erhan, D. Eck, and B. Kegl. \n“Aggregate Features and AdaBoost for Music Classifica-\ntion,” Machine Learning, 2006 (in press).  \n[4] C. McKay, and I. Fujinaga. “Automatic Genre Classification \nUsing Large High-Level Musical Feature Sets,” in Proceed-\nings of the International Conference on Music Information \nRetrieval, 2004, pp. 525–30. \n[5] F. Fabbri. “Browsing Music Spaces: Categories and the \nMusical Mind,” in Proceedings of the IASPM Conference, \n1999. \n[6] D. Duff, Ed., Modern Genre Theory, New York: Longman, \n2000. \n[7] B. K. Grant, Ed., Film Genre Reader III, Austin, TX: Uni-\nversity of Texas Press, 2003. \n[8] F. Fabbri. “A Theory of Musical Genres: Two Applica-\ntions,” in Popular Music Perspectives, D. Horn and P. Tagg, \nEds. Göteborg: IASPM, 1981. [9] F. Fabbri. “What Kind of Music?,” Popular Music, no. 2, \npp. 131–43, 1982. \n[10] S. Frith, Performing Rites: On the Value of Popular Music, \nCambridge, MA: Harvard University Press, 1996. \n[11] J. Toynbee, Making Popular Music: Musicians, Creativity \nand Institutions, London: Arnold, 2000. \n[12] D. Brackett, Interpreting Popular Music, New York: Cam-\nbridge University Press, 1995. \n[13] D. Brackett, (In Search Of) Musical Meaning: Genres, \nCategories and Crossover, London: Arnold, 2002. \n[14] J. J. Aucouturier, and F. Pachet. “Representing Musical \nGenre: A State of the Art,” Journal of New Music Research, \nvol. 32, no. 1, pp. 1–12, 2003. \n[15] F. Pachet, and D. Cazaly. “A Taxonomy of Musical Gen-\nres,” in Proceedings of the Content-Based Multimedia In-\nformation Access Conference, 2000. \n[16] G. Lakoff, Women, Fire, and Dangerous Things: What \nCategories Reveal About the Mind, Chicago: University of \nChicago Press, 1987. \n[17] I. Deliege. “Prototype Effects in Music Listening: An Em-\npirical Approach to the Notion of Imprint,” Music Percep-\ntion, vol. 18, no. 33, pp. 371–407, 2001. \n[18] C. McKay. “Automatic Genre Classification of MIDI Re-\ncordings,” M.A. Thesis, McGill University, Canada, 2004. \n[19]  D. Perrot, and R. Gjerdigen. “Scanning the Dial: An Explo-\nration of Factors in the Identification of Musical Style,” in \nProceedings of the Society for Music Perception and Cogni-\ntion, 1999, p. 88 (abstract). \n[20] S. Lippens, J. P. Martens, M. Leman, B. Baets, H. Meyer, \nand G. Tzanetakis. “A Comparison of Human and Auto-\nmatic Musical Genre Classification,” in Proceedings of the \nIEEE International Conference on Audio, Speech and Sig-\nnal Processing, 2004. \n[21] “AllMusic Guide,” [Web site] 2006, [2006 March 29], \nAvailable: http://www.allmusic.com \n[22] “Gracenote,” [Web site] 2006, [2006 March 29], Available: \nhttp://www.gracenote.com \n[23]  “MIREX 2005 Contest Results,” [Web site] 2005, [2006 \nApril 18], Available: http://www.music-ir.org/evaluation/ \nmirex-results. \n[24] J. J. Aucouturier, and F. Pachet. “Improving Timbre Similar-\nity: How High is the Sky?,” Journal of Negative Results in \nSpeech and Audio Sciences, vol. 1, no. 1. 2004. \n[25] J. H. Lee, and J. S. Downie. “Survey of Music Information \nNeeds, Uses, and Seeking Behaviours: Preliminary Find-\nings,” in Proceedings of the International Conference on \nMusic Information Retrieval, 2004. \n[26] A. C. North, and D. J. Hargreaves. “Liking for Musical \nStyles,” Music Scientae, vol. 1, no. 1, pp. 109–28, 1997. \n[27] H. G. Tekman, and N. Hortacsu. “Aspects of Stylistic \nKnowledge: What are Different Styles Like and Why Do We \nListen to Them?,” Psychology of Music, vol. 30, no. 1, pp. \n28–47, 2002. \n[28] C. McKay, and I. Fujinaga. “Automatic Music Classification \nand the Importance of Instrument Identification,” in Pro-\nceedings of the Conference on Interdisciplinary Musicology, \n2005. \n[29] B. Whitman, and P. Smaragdis. “Combining Musical and \nCultural Features For Intelligent Style Detection,” in Pro-\nceedings of the International Conference on Music Informa-\ntion Retrieval, 2002, pp. 47–52."
    },
    {
        "title": "A Large Publicly Accassible Prototype Audio Database for Music Research.",
        "author": [
            "Cory McKay",
            "Daniel McEnnis",
            "Ichiro Fujinaga"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1416652",
        "url": "https://doi.org/10.5281/zenodo.1416652",
        "ee": "https://zenodo.org/records/1416652/files/McKayMF06.pdf",
        "abstract": "This paper introduces Codaich, a large and diverse publicly accessible database of musical recordings for use in music information retrieval (MIR) research. The issues that must be dealt with when constructing such a database are dis- cussed, as are ways of addressing these problems. It is sug- gested that copyright restrictions may be overcome by al- lowing users to make customized feature extraction queries rather than allowing direct access to recordings themselves. The jMusicMetaManager software is introduced as a tool for improving metadata associated with recordings by auto- matically detecting inconsistencies and redundancies. Keywords: Music database, MP3s, features, metadata.",
        "zenodo_id": 1416652,
        "dblp_key": "conf/ismir/McKayMF06",
        "keywords": [
            "Music database",
            "MP3s",
            "features",
            "metadata",
            "Music information retrieval (MIR)",
            "copyright restrictions",
            "customized feature extraction queries",
            "jMusicMetaManager",
            "auto-detect inconsistencies",
            "auto-detect redundancies"
        ],
        "content": "A Large Publicly Accessible Prototype Audio Database for Music Research \nCory McKay \nMcGill University \nMontreal, Quebec, Canada \ncory.mckay@mail.mcgill.ca Daniel McEnnis \nMcGill University \nMontreal, Quebec, Canada \ndaniel.mcennis@mail.mcgill.ca Ichiro Fujinaga \nMcGill University \nMontreal, Quebec, Canada \nich@music.mcgill.ca \n \nAbstract \nThis paper introduces Codaich, a large and diverse publicly \naccessible database of musical recordings for use in music \ninformation retrieval (MIR) research. The issues that must \nbe dealt with when constructing such a database are dis-\ncussed, as are ways of addressing these problems. It is sug-\ngested that copyright restrictions may be overcome by al-\nlowing users to make customized feature extraction queries \nrather than allowing direct access to recordings themselves. \nThe jMusicMetaManager software is introduced as a tool \nfor improving metadata associated with recordings by auto-\nmatically detecting inconsistencies and redundancies. \nKeywords: Music database, MP3s, features, metadata. \n1. Introduction \nThe maturation of the MIR field is increasingly requiring \nresearchers to move beyond work involving simplistic mu-\nsical datasets to more expansive studies that require much \nlarger, more varied and carefully annotated (i.e., labelled \nwith metadata) collections. This is necessary in order to \ndevelop and validate MIR tools that can be applied to the \nvast and varied universe of music that exists outside of the \nlab. Developing a framework for building and maintaining \nlarge music databases is also becoming increasingly impor-\ntant as libraries digitize their collections and on-line com-\nmercial databases continue to grow. In addition, high qual-\nity datasets are needed to provide ground truth that can be \nused to compare the performance of different systems in \ncompetitions such as MIREX [1]. Unfortunately, problems \nrelating to copyright laws and poor metadata annotations \nhave hindered efforts by researchers to form, share and use \nhigh quality collections. \nThis paper discusses approaches to overcoming these \nlimitations and presents a prototype database of recordings \nwith these ideas in mind. In particular, the following list is \nproposed as a basic set of requirements that should be met \nby any music database that is to effectively meet the broad \nneeds of current MIR research: · Data should be freely and legally distributable to researchers. \n· The database should contain many different types of music. \n· The database should include many thousands of recordings. \nThis is important not only to allow sufficient variety, but also to \navoid research overuse of a relatively small number of re-\ncordings, which can result in overtraining. Furthermore, even \ngood quality annotations will inevitably contain some errors, \nand a large database helps to average out such noise. \n· The database should include a significant amount of commer-\ncial music, although independent music can certainly play a \nrole as well. The vast majority of end users are interested pri-\nmarily in professionally produced music, so MIR systems must \ndemonstrate that they are able to deal with such music. \n· Each recording should be annotated with as diverse a range of \nmetadata fields as possible in order to make the database usable \nas ground truth for as wide a range of research as possible. \n· It should ideally be possible to label segments of recordings as \nwell as recordings as a whole.  \n· Annotations of subjective fields such as genre or mood should \ninclude a wide range of candidate categories, as simply allow-\ning ten or so coarse categories is unrealistic. \n· Annotations should be correct, complete and consistent.  \n· It should be possible to assign multiple independent values to a \nsingle field so that, for example, a recording could be classified \nas both swing and blues. \n· The ability to construct ontological structures between fields \ncould be useful. \n· Metadata should be made available to users in formats that are \neasy to both manually browse and automatically parse. \n· Automatic tools should be available to validate metadata and \ngenerate profiles of the database. \n· Entire recordings should be accessible, at least indirectly, not \njust short excerpts. Each researcher should be able to choose \nhow much and what parts of recordings are to be studied.  \n· Given that different compression methods can influence ex-\ntracted feature values, the audio format(s) most commonly used \nby the public should be adopted in order to reflect realistic \nconditions. This is important for many types of end user ori-\nented research, although uncompressed audio is also useful for \nsome theoretical research. A variety of encoders and bit rates \nshould be used for similar reasons. \n· It should be easy to add new recordings and their metadata. \n2. Existing Databases \nMIR researchers have traditionally assembled their own \nmusical datasets for training and/or testing, an understand-\nable approach given the lack of alternatives. This approach \nis ultimately flawed, however, as collections assembled in Permission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fee provided that copies \nare not made or distributed for profit or commercial advantage and that \ncopies bear this notice and the full citation on the first page. \n© 2006 University of Victoria this manner tend to be restricted in size and variety, and \ncan also be biased due to the characteristics of the system \nbeing developed and the musical background of the indi-\nviduals assembling the database. Furthermore, such data-\nbases cannot be legally distributed for refinement, expan-\nsion and comparison if they contain copyrighted material. \nAlthough such datasets have sometimes been “unofficially” \ndistributed among researchers in the past, this quasi-legal \nsharing is not viable in the long term. \nAn alternative approach is to make use of recordings \nthat are in the public domain, such as through sites like \nGarageBand [2]. Homburg et al. [3] have explored such an \napproach, and have also incorporated multiple views and \nhigh quality annotations. Unfortunately, this approach gen-\nerally restricts one to a limited number of recordings that \nare usually either very old, are by amateur musicians or \nconsist of only short low quality extracts. These limitations \nare demonstrated by Homburg et al.’s database, which, \ndespite its many advantages, contains only 1886 files, of \nwhich only 10-second segments are available. \nSome improvement can be achieved by utilizing music \nprotected under more limiting but still fairly lenient frame-\nworks such as Creative Commons. Web sites such as Mag-\nnatune [4] and Epitonic [5], for example, allow one to pre-\nview recordings for free, and entire music collections can \nsometimes be licensed for research purposes at little or no \ncost. Unfortunately, size is still an issue. For example, at \npress time Magnatune had only 210 artists and 5662 songs. \nAlso, the music available on such sites usually excludes \nartists contracted to major record labels, which is the music \nthat most users are interested in. In addition, the metadata \ncontained in ID3 tags is not entirely reliable, with problems \nsuch as multiple spellings of a genre names or strange treat-\nment of special characters. Finally, there can still be legal \nrestrictions on distributing downloaded music to other re-\nsearchers, even if one is permitted to access and store en-\ntire recordings for free oneself.  \nAn additional possibility is to contract arrangers and \nmusicians to produce original recordings for use in re-\nsearch, as was done in constructing the RWC database [6]. \nAlthough this approach has the important advantage that it \novercomes copyright protection, costs prevent it from scal-\ning to any reasonably large number of recordings. There \ncan also be doubts as to how well such original music \nsimulates what one encounters in the real world. \n3. Overcoming Copyright Limitations \nThe copyright laws of many countries make it difficult to \nacquire access to a sufficient range of music to meet the \nneeds of MIR research. This puts pressure on each re-\nsearcher to independently collect and annotate his or her \nown dataset, a process that, in addition to the problems \ndiscussed in Section 2, also wastes significant amounts of \ntime in duplicated effort. The motivation behind copyright laws is not to hinder \nresearch, of course, but to protect intellectual property and \nprevent pirating. Discussions with legal experts at past \nISMIR conferences seem to indicate that there is likely no \nlegal obstacle to the distribution of information extracted \nfrom recordings, as long as such information cannot be \nused to reconstruct the music itself. This means that many \nof the features that are typically extracted form music in \nMIR research can in fact be publicly distributed, even \nwhen the music itself cannot. Even features such as \nMFCCs can only reconstruct a very poor facsimile of the \noriginal music under certain parameterizations. \nSince features and metadata are essentially what many \nMIR researchers are interested in, publicly distributing this \ndata rather than the music itself is a useful way of circum-\nventing copyright limitations. Features extracted from legal \nlocal music collections can thus be combined into data-\nbases so that researchers can effectively share their music. \nSimply extracting and publicly posting stock features is \ninsufficient, however, as an important part of MIR research \ninvolves developing new and specialized features. Fur-\nthermore, different researchers will want features extracted \nwith different parameters (e.g., window size and overlap, \ndownsampling, amplitude normalization, etc.). The jAudio \nsoftware [7], an open-source Java-based feature extraction \npackage, presents a solution to these problems by allowing \nspecification of extraction parameters and by making it a \nrelatively simple matter for researchers to design their own \nfeatures and add them to the jAudio framework.  \nPart of the On-demand Metadata Extraction Network \n(OMEN) project [8] has involved integrating OMEN ser-\nvices with jAudio so that it can receive and execute param-\neterized feature extraction requests as well as deploy new \nfeature classes while it is running on a network. This means \nthat a database of musical recordings can be stored pri-\nvately on one or more servers, which can receive and proc-\ness customized feature extraction requests. Researchers can \nthus access needed features without ever violating copy-\nright laws by accessing the music itself. The only interven-\ntion needed at the server is examination of requests to en-\nsure that they will not provide sufficient information to \nreconstruct the music. \nA good way to provide the computing power needed to \nservice feature extraction requests is to make use of dis-\ntributed computing. Libraries offer a particularly suitable \nresource in this respect, as they have large networks of \ncomputers around the world that go unused outside of \nopening hours. They also often have large music collec-\ntions from which features could be automatically extracted \nand made publicly available with jAudio and OMEN.  \n4. Achieving High Quality Annotations \nThere can be difficulty involved in properly annotating the \nmetadata associated with music recordings. Both signifi-\ncant amounts of time and extensive musicological knowl-edge are often needed, both of which can be lacking when \nperforming technical MIR research where dataset collec-\ntion is only part of a larger project. Accurate and consistent \nannotation is nonetheless essential, as the metadata found \nin annotations will often serve as the ground truth for train-\ning and evaluating systems. \nSubjective fields such as genre or mood are particularly \nproblematic, as multiple interpretations may be valid. Such \nfields are nonetheless important when constructing a data-\nbase, as many researchers and end users are interested in \nthem. Extra care must be taken when annotating such \nfields, in terms of both which candidate categories are used \nand how they are assigned. A number of associated issues \nare well-discussed elsewhere [9]. \nIt is necessary to consider alternatives to manual meta-\ndata entry when constructing a database, as this would be \nvery time consuming for large databases. This is typically \ndone by extracting metadata from the ID3 tags of MP3s or \nfrom metadata management services such as Gracenote \nCDDB [10]. Unfortunately, although these sources do save \nsignificant amounts of time, they tend to be at best noisy \nand inconsistent, and at worst entirely incorrect. \nIt is necessary to correct the metadata derived from such \nsources if they are to be used. The Java-based jMu-\nsicMetaManager software was therefore built to automati-\ncally error check the metadata of music databases.  \nOne of the important problems that jMusicMetaManager \ndeals with are the inconsistencies and redundancies caused \nby multiple spellings that are often found for entries that \nshould be identical. For example, uncorrected occurrences \nof both “Lynyrd Skynyrd” and “Leonard Skinard,” or of \nthe multiple valid spellings of composers such as Stravin-\nsky, would be problematic for an artist identification sys-\ntem that would perceive them as different artists.  \nAt its simplest level, jMusicMetaManager calculates the \ncase-insensitive Levenshtein (edit) distance between each \npair of entries for a given field. A threshold is then used to \ndetermine whether two entries are likely to in fact corre-\nspond to the same true value. This threshold is dynamically \nweighted by the length of the strings and whether their \nother fields are similar. This is done separately once each \nfor the artist, composer, title, album and genre fields. In the \ncase of titles, recording length is also considered, as two \nrecordings might correctly have the same title but be per-\nformed entirely differently (e.g., an original Led Zeppelin \nsong compared with a Dread Zeppelin cover, or live and \nstudio versions of the same song both by Led Zeppelin). \nThis approach, while helpful, is too simplistic to detect \nthe full range of problems that one finds in practice. Addi-\ntional processing was therefore implemented and additional \npost-modification distances were calculated. For example: \n· Instances of “The ” were removed (e.g., “The Police” should \nmatch “Police”). \n· Occurrences of “ and ” were replaced with “ & ” (e.g., “Simon \nand Garfunkel” should match “Simon & Garfunkel”). · Personal titles were converted to abbreviations (e.g., “Doctor \nJohn” should mach “Dr. John”). \n· Instances of “in’” were replaced with “ing” (e.g., “Breakin’ \nDown” should match “Breaking Down”). \n· Punctuation and brackets were removed (e.g., “REM” should \nmatch “R.E.M.”). \n· Spaces were removed, as their omission is a common typo. \n· Numbers were removed from the beginnings of titles, as track \nnumbers are sometimes encoded in titles. \n· Word orders were rearranged (e.g., “Ella Fitzgerald” should \nmatch “Fitzgerald, Ella,” and “Django Reinhardt & Stéphane \nGrappelli” should match “Stéphane Grappelli & Django \nReinhardt”). \njMusicMetaManager also automatically generates a va-\nriety of HTML-formatted statistical reports about music \ncollections, including multiple data summary views and \nbreakdowns of co-occurrences between artists, composers, \nalbums and genres. This allows one to easily acquire and \npublish HTML database profiles.  \nUsers often need a graphical interface for viewing and \nediting a database’s metadata. It was therefore decided to \nlink jMusicMetaManager to the Apple iTunes software, \nwhich is not only free, well-designed, and commonly used, \nbut also includes an easily parsed XML-based file format. \niTunes, in addition, has the important advantage that it \nsaves metadata modifications directly to the ID3 tags of \nMP3s as well as to its own files, which means that the re-\ncordings can easily be disassociated from iTunes if needed. \niTunes can also access Gracenote’s metadata automatically, \nwhich can then be cleaned with jMusicMetaManager. \njMusicMetaManager can extract metadata from iTunes \nXML files as well as directly from MP3 ID3 tags. Since \nMIR systems do not typically read these formats, jMu-\nsicMetaManager can also be used to generate ground-truth \ndata formatted in ACE XML [11] or Weka ARFF [12] \nformats. This is also important because iTunes XML has a \nlimited number of fields, only allows one genre per re-\ncording, does not allow ontological structuring and does \nnot allow segmented annotations of recordings. More \nflexible file formats such as ACE XML allow access to \nexpanded expressiveness when required.  \n5. Details of the Codaich Database \nA prototype database named Codaich (Gaelic for “share”) \nwas constructed using the majority of the guidelines and \ntools discussed in this paper. It currently consists of 20,849 \nMP3 recordings from 1941 artists. Details on the database \nmay be access via iTunes XML, ACE XML, Weka ARFF \nor jMusicMetadata HTML files. Codaich is intended to be \nintegrated with OMEN so that features values may be pub-\nlicly accessed.  \nMetadata fields (including Title, Performer, Composer, \nAlbum, Track Number, Disc Number, Year, Genre, Bit \nRate and Track Duration) were originally extracted from \nGracenote CDDB and pre-existing ID3 tags. These were then cleaned using the jMusicMetaManager software, and \nfinal manual improvements were made with iTunes when \nnecessary, in consultation with the AllMusic Guide [13]. \nThe recordings are annotated using a total of 53 candidate \ngenres, which are distributed among the coarse categories \nof popular, world, classical and jazz. Efforts were made to \nachieve as stylistically diverse a collection as possible. \nThe MP3 audio format was chosen because it is by far \nthe most popular format. Some MP3s were ripped from \nCDs, some were recorded from tapes and LPs and some \nwere found as pre-encoded MP3s. A variety of encoders \nand bit rates were used. This diversity simulates what one \nfinds in the real world.   \nThe recordings were acquired from the Marvin Duchow \nMusic Library, from contributions from the personal col-\nlections of members of the McGill Music Technology Area \nand from the in-house database of Douglas Eck’s lab at the \nUniversité de Montréal.  \nThe jMusicMetaManager software and information on \naccessing the Codaich database may be found at \nhttp://sourceforge.net/projects/jmir. \n6. Conclusions and Future Research \nThis paper has emphasized the importance of building \nlarge publicly accessible databases of musical recordings \nfor use in MIR research, and has discussed important issues \nto consider when constructing such a database. The Co-\ndaich prototype database was presented, and the OMEN \nframework was suggested as a means of distributing fea-\ntures extracted from this database without violating copy-\nright laws. The jMusicMetaManager software was intro-\nduced as a tool for generating profiles of music collections, \nas well as for cleaning recordings’ metadata by detecting \ninconsistencies and redundancies. \nThe Codaich database is still growing rapidly, and the \nauthors will continue to add many more recordings to it. It \nis hoped that in the future other researchers will also con-\ntribute their collections using the OMEN framework. \nA priority for future research is the development of im-\nproved data mining software to automatically mine multi-\nple sources on the web to improve entries for fields such as \ngenre. Additional fields such as mood will also be added in \norder to make the annotations suitable for a wider range of \nMIR research, and functionality for incorporating sophisti-\ncated ontologies and multiple entries per field will be de-\nveloped. In addition, there are plans to integrate audio fin-\ngerprinting technology into jMusicMetaManager to fill in \nmissing fields and detect incorrect labels. Also, automated \nname authority control using a standardized reference such \nas the U.S. Library of Congress will be incorporated [14]. \nFinally, the framework discussed here will be adapted to \nconstruct a database of symbolic recordings in formats \nsuch as MIDI and Humdrum kern. 7. Acknowledgments \nWe would like to thank the helpful and patient librarians at \nthe Marvin Duchow Music Library for accommodating our \nmany requests for CDs. We would also like to thank Doug-\nlas Eck for giving us access to his lab’s database as well as \nthe other individuals who contributed recordings. Finally, \nwe are grateful to the Social Sciences and Humanities Re-\nsearch Council of Canada and the Canada Foundation for \nInnovation for their financial support. \nReferences \n[1] J. S. Downie, K. West, A. Ehman, and E. Vincent, “The \n2005 Music Information retrieval Evaluation Exchange \n(MIREX 2005): Preliminary Overview,” in Proceedings of \nthe International Computer Music Conference, 2005, pp. \n320–3. \n[2]  “GarageBand.com,” [Web site] 2006, [2006 March 27], \nAvailable: http://www.garageband.com \n[3] H. Homburg, I. Mierswa, B. Moller, K. Morik, and M. \nWurst, “A benchmark dataset for audio classification and \nclustering,” in Proceedings of the International Conference \non Music Information Retrieval, 2005, pp. 528–31. \n[4]  “Magnatune: MP3 music and music licensing,” [Web site] \n2006, [2006 March 27], Available: http://magnatune.com \n[5]  “Epitonic.com: Hi quality free and legal MP3 music,” \n[Web site] 2006, [2006 March 27], Available: \nhttp://www.epitonic.com \n[6] M. Goto, H. Hashiguchi, T. Nishimura, and R. Oka, “RWC \nmusic database: Popular, classical and jazz music data-\nbases,” in Proceedings of the International Conference on \nMusic Information Retrieval, 2002, pp. 287–8. \n[7] D. McEnnis, C. McKay, I. Fujinaga, and P. Depalle, \n“jAudio: A feature extraction library,” in Proceedings of \nthe International Conference on Music Information Re-\ntrieval, 2005, pp. 600–3. \n[8] I. Fujinaga, and D. McEnnis. “On-demand Metadata Ex-\ntraction Network,” in Proceedings of the Joint Conference \non Digital Libraries, 2006. \n[9] J. J. Aucouturier, and F. Pachet, “Representing musical \ngenre: A state of the art,” Journal of New Music Research, \nvol. 32, no. 1, pp. 1–12, 2003. \n[10] “Gracenote,” [Web site] 2006, [2006 March 29], Available: \nhttp://www.gracenote.com \n[11] C. McKay, D. McEnnis, R. Fiebrink, and I. Fujinaga. \n“ACE: A general-purpose classification ensemble optimiza-\ntion framework,” in Proceedings of the International Com-\nputer Music Conference, 2005, pp. 161–4. \n[12] I. H. Witten, and E. Frank. Data Mining: Practical Ma-\nchine Learning Tools and Techniques, 2nd ed., New York: \nMorgan Kaufman, 2005. \n[13] “AllMusic Guide,” [Web site] 2006, [2006 March 29], \nAvailable: http://www.allmusic.com \n[14] T. DiLauro, G. S. Choudhury, M. Patton, and J. W. War-\nner, “Automated Name Authority Control and Enhanced \nSearching,” D-Lib Magazine, vol. 7, no. 4, 2001."
    },
    {
        "title": "Problems and Opportunities of Applying Data- &amp; Audio-Mining Techniques to Ethnic Music.",
        "author": [
            "Dirk Moelants",
            "Olmo Cornelis",
            "Marc Leman",
            "Jos Gansemans",
            "Rita M. M. De Caluwe",
            "Guy De Tré",
            "Tom Matthé",
            "Axel Hallez"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1417787",
        "url": "https://doi.org/10.5281/zenodo.1417787",
        "ee": "https://zenodo.org/records/1417787/files/MoelantsCLGCTMH06.pdf",
        "abstract": "Current research in music information retrieval focuses on Western music. In music from other cultures, both musical structures and thinking about music can be very different. This creates problems for both the analysis of musical features and the construction of databases. On the other hand, a well-documented digitization offers interesting opportunities for the study and spread of ‘endangered’ music. Here, some general problems regarding the digital indexation of ethnic music are given, illustrated with a method for describing pitch structure, comparing Western standards with African music found in the digitization of the archives of the Royal Museum of Central-Africa in Tervuren (Brussels). Keywords: ethnic music, Africa, pitch, archiving, databases, cultural heritage",
        "zenodo_id": 1417787,
        "dblp_key": "conf/ismir/MoelantsCLGCTMH06",
        "keywords": [
            "ethnic music",
            "African music",
            "pitch structure",
            "digitization",
            "archives",
            "cultural heritage",
            "Western music",
            "database",
            "research",
            "analysis"
        ],
        "content": "Problems and Opportunities of Applying Data- & Audio-Mining Techniques \nto Ethnic Music \n \nDirk Moelants, Olmo Cornelis, \nMarc Leman \nIPEM-Dept. of Musicology, Ghent \nUniversity \nBlandijnberg 2 \nB-9000 Ghent, Belgium \nDirk.Moelants@UGent.be Jos Gansemans \nDept. of Cultural Anthropology, Royal \nMuseum of Central-Africa \nLeuvensesteenweg 13 \nB-3080 Tervuren \njgansemans@africamuseum.be  Rita De Caluwe, Guy De Tré, \nTom Matthé, Axel Hallez \nTELIN, Ghent University \nSint-Pietersnieuwstraat 41 \nB-9000 Ghent, Belgium \nRita.DeCaluwe@UGent.be \nAbstract \nCurrent research in music in formation retrieval focuses on \nWestern music. In music from other cultures, both musical structures and thinking about music can be very different. This creates problems for both the analysis of musical features and the construction of databases. On the other hand, a well-documented digitization offers interesting opportunities for the study a nd spread of ‘endangered’ \nmusic. Here, some general problems regarding the digital indexation of ethnic music are given, illustrated with a method for describing pitch structure, comparing Western standards with African music found in the digitization of \nthe archives of th e Royal Museum of Central-Africa in \nTervuren (Brussels). \nKeywords : ethnic music, Africa, pitch, archiving, \ndatabases, cultural heritage \n1. Introduction \nAt the moment most MIR applications are aimed \nat popular and classical musi c in the Western tradition. \nWith the increasing use of digital search methods, users are guided to music that is  ‘searchable’. Music that \noccupies a more marginal position risks to be omitted from databases, or be much less accessible (e.g. because popular search fields can not be filled out or similarity measures with popular styles are very low). Thus the combination of digitization and commercialization tends to push ‘vulnerable’ music into oblivion. However, if we develop methods aimed at indexing non-Western music in an appropriate way, the opposite might be true. The use of music databases can bring people in contact with music they would normally never have heard of, and thus give the community a broader view on music. \nSimply integrating more ethnic music, and non-\nwestern music in general, into the existing databases and indexes, can not entirely so lve this problem. Musical \nstructures can be fundamentally different and also the relative importance of structur al elements in the musical \nexperiences can be different. In  the west, music theory and \nresearch traditionally focus on pitch, but in African music a fixed tuning does not exist and relative pitch (higher-\nlower) is more important than absolute pitch.  \nAnother difficulty of integrating ethnic music in \ndigital music libraries lies in the construction of the meta-data. In the description of field recordings, some information that is ‘compulsory’ in the description of Western music is lacking, while information that seems irrelevant in the description of Western music is very important. Names of composers are usually not known, performers could be named, but some music is seen as performed by ‘the community’ and the names of the participants are not considered very important. On the other hand, location and date of the recording are important as documentation for researchers. Not all recordings are well-documented,  in some, even the most \nbasic information is lacking, this while the knowledge about traditional music within  the cultures is vanishing \nunder pressure of urbanization and Westernization. This combination makes it harder and harder to identify and label the music correctly. \nFinally there is also a terminology problem. \nDifferent local names for the same concept can exist, and then different researchers can use different transcriptions \nfor these concepts. At this mo ment the American Folklore \nSociety and the American Folklif e Center at the Library of \nCongress are constructing an “Ethnographic Thesaurus”, a comprehensive, controlled list of subject terms to be used in describing ethnographic and ethnological research collections (cf. http://www.et project.org). But even a \nstandardized list can not solv e all problems. We can give \nthe example of the ‘thumb piano’ (lamellophone). This instrument type is referred to  with diverse names (mbira, \nlikembe, sanza, kalimba, etc.). Someone looking for one of these search terms should also be directed to pieces in which one of the other denominations is used (Matthé et al., 2006) To make it even more complicated, one denomination does not necessarily point at a specific subtype: size, material, number of pitches and tuning can widely vary. Therefore the user should be given the means to refine his search by looking for more specific instrument characteristics, or for instruments with a similar tuning.  2. Digitization of the audio collection of the \nBelgian Royal Museum of Central-Africa \nThe ideas described in this paper are a result of \nwork on the digitization of the ethnomusicological archives of the Royal Museum  of Central-Africa (RMCA) \nin Tervuren (Brussels).  With its 50,000 sound recordings, with a total of 3,000 hours of music, the audio archive is one of the biggest archives in world for the region of Central-Africa, an area wher e the cultural heritage is \nendangered by modernization and political instability (Cornelis et al., 2005). The goal of the project is to conserve the unique collection of sound recordings and to make the archives accessible for a broad group of users. This implies the digitization of different types of sound recordings (wax cylinders, sonofil, vinyl recordings and magnetic tapes) each with their own problems, the construction of a database structure that is suitable to describe the music (Matthé et al., 2005) and develop some tools that allow a further an alysis and classification. \nMusicological texts describing the music and instruments of different areas, sound examples and photos from the archives and the collection of the museum are added as background information. The progress can be followed on the website http://music.africamuseum.be/.   \nAn additional challenge is the differentiation \nbetween user groups. We can broadly distinguish three groups. The first and largest group is that of people who are just interested in African music, but don’t have a good knowledge of it. These people want to find e.g ‘drumming’, ‘trance music’ or ‘some song from Rwanda’, which needs rather vague, general labeling. A second group consists of people from within the culture. They might have a very good knowledge of certain repertoires and functions of the music, and thus ask very specific questions like. music played  by a specific performer, \nmusic from one particular village, lyrics, genres, instruments (in local terminology). The third group consists of researchers, using the database in their study. For them a fully indexed electr onic database is an excellent \ntool to study elements like the geographical spread of \ncertain instrument types or the relative importance of certain musical structures in different regions. \nInteresting applications for the first group could \nbe query by example, or searching by affective parameters. Latter two groups can add valuable information to the database. For this reason a forum is set up, when the database will be fully available, users will be invited to discuss on certain topics in this forum and hopefully this will allow us to fill some gaps, come in contact with \nresearchers how to use the database and might give ideas for the development of additional search tools and extend the database with (copies of) recordings kept at other institutions or by private persons. \n3. Pitch structures in African Music \nModels for pitch detection generally assume that \nthere are certain fixed elemen ts like 12 chromatic tones, a \ntonic and a dominant and octave relationships. These concepts allow describing, comparing and classifying the pitch structure in different pieces. When studying African \nMusic we can not hold these concepts as basis of our study. This is a problem with which ethnomusicologists already have to deal with for many years in making transcriptions, usually solving it by adding additional marks for deviations from regular tuning, sometimes adding precise pitches.  \nThe music of Central-Africa uses many different \ntunings and scales. In fact, me lody is closely connected to \nthe tones of Bantu languages. The music thus follows the melody of speech. Therefore it is more important to have high and low pitches as such than to have specific harmonic relations between them. Furthermore in this region instrumental sounds with a very broad, percussive spectrum are preferred, which also reduces the importance \nof ‘correct’ intervals. Nevertheless there instruments with a fixed tuning like flutes, citers, (wooden) trumpets or thumb piano’s allow to study the scales used.  \nIn order to study the scales and tunings, a system \nwas developed that allows representing scales without reference to Western notes or s cales. Instead of an a priori \nthinking in pitch categories, we chose to start from a continuous representation of pitch. In a first stage the music is analyzed by a melody  extractor (De Mulder et al., \n2004). This system was originally devised for vocal queries, and is thus optimized for monophonic music and the normal voice range. Yet, testing the model on different types of music reveals that it can give a very good image of the distribution of pitches in music with a more complex texture. The melody extractor currently used gives a frequency for every time frame of 10 ms. In order to give an image of the scale, we tr ansform these values to cent \nvalues (taking the low A, 55Hz as 0 cents) and then make a frequency table by cent values between 1 and 6000.  \nThe method is illustrated bye the analysis of a \nsong “Ingendo y'inka” with ikembe (thumb piano) accompaniment recorded on 14 Jan 1973 by Jos Gansemans in the village Karengera, Cyangugu province, Rwanda. The singing is mainly in parlando style, following the pitches of the instrument. The text describes the elegance of the local co ws. If we extract the most \nprominent peaks (table 1) we see more or less equidistant intervals, with an average size of 236 cents. The intervals betweens successive tones are somewhat larger than a \nPermission to make digital or hard copi es of all or part of this work for \npersonal or classroom use is grante d without fee provided that copies \nare not made or distributed for profit or commercial advantage and that \ncopies bear this notice and the full citation on the first page. \n© 2006 University of Victoria whole tone but the size is rather variable (202-286 cents), \nbut the octave relationships are generally quite well preserved, which is also the case for the fifths. However, we see that the highest octave is much (60 cents) too small, while the lowest is 25 cents too wide. This causes a problem when we want to reduce the representation to one single octave (see Figure 1). Three tones keep a very narrow peak, indicating near perfect octave relationships, but the middle peak is clearly widened and in the left peak, \nthe one corresponding to the extreme pitches we clearly see a combination of three smaller peaks.  \nTable 1: analysis of the most  prominent peaks in “Ingendo \ny'inka”, left the pitch in cents  above 55 Hz, next the distance \nin cents between successive pitches and in the right column \nthe size of the octave relationships. \npitch interval octave \n1042 236 1225 \n1278 264 1199 1542 202 1185 1744 237 1214 1981 286 1198 2267 210 1140 2477 250  2727 231  2958 221  3179 228  3407   \n \nFigure 1: pitch analysis of “Ing endo y'inka”, reduced to one \noctave, on the x-axis the pitch in  cents (0 = a), on the y-axis \nthe smoothed number of occurrences for every 1 cent. \n \nThis example illustrates the difference between \nAfrican and Western tone syst ems very well. The Western \nsystem uses a chromatic scale in which interval sizes are \nmultipliers of 100 cents and there is a strict adherence to a 1200 cents octave. In African music there is no standard tuning, in this case there is a more or less equidistant pentatonic scale, but many different scale types are found. The octave exists, but it’s role is not absolute. In fact it is not unusual that players deliberate tune some of the octaves to wide or to narrow in order to create an extra \nrich timbre. \nThe graphic representation can be the basis of \ncomparison of tone systems. This can allow to use the \nsystem for musicological app lications (e.g. to map the \nspread of certain tone systems), but also to let a user of the \ndatabase look for similar music. \n4. Conclusions \nConstructing MIR applications that can deal with \nthe world’s music is difficult, but necessary to protect the \nworld’s cultural heritage. It is important to bring knowledge about the music toge ther and make it accessible \nto a broad audience. The fundamentally different use of pitch in African music co mpared to Western music \nillustrates the difficulty of simply applying existing methods to other bodies of music in order to index it. In this phase of the project of digitizing the archives of the RMCA we started to develop some methods that should be able to deal with all types of musical content. This was illustrated by a method of extr acting the scale of a musical \npiece and representing it in a graphic notation that can \nserve as basis of comparison with scales used in other pieces, regardless of their cultural background and tone \nsystem \nReferences \nCornelis, O., De Caluwe, R., De Tré, G., Hallez, A., Leman, M., Matthé, T., Moelants, D., & Gansemans, J. (2005). Digitisation of th e Ethnomusicological Sound \nArchive of the Royal Museum for Central Africa (Belgium). Iasa journal , 26, 35-43. \n De Mulder, T., Martens, J.P., Lesaffre, M., Leman, M., De Baets, B., & De Meyer, H. (2004). Recent improvements of an auditory model based front-end for the transcription of vocal queries, in: Proceedings of the IEEE International \nConference on Acoustics, Sp eech and Signal Processing  \n(pp. 257-260). Montreal. \nMatthé, T., De Tré, G., Hallez, A., De Caluwe, R., Leman, \nM., Cornelis, O., Moelants, D., & Gansemans, J. (2005). A Framework for Flexible Querying and Mining of Musical Audio Archives. In Proceedings of the 16th International \nConference on Database and Expert Systems Applications  \n(pp. 1041-1045). Copenhagen. \nMatthé, T., De Caluwe, R., De Tré, G., Hallez, A. \nVerstraete, J., Leman, M., Cornelis, O. Moelants, D., &. Gansemans, J. (2006). Simila rity Between Multi-valued \nThesaurus Attributes: Theory and Application in Multimedia Systems. Lecture Notes in Computer Science  \n4027 , 331–342 0 20 40 60 80 100 120 140 \n0 100 200 300 400 500 600 700 800 900 1000 1100 1200"
    },
    {
        "title": "Name that mood! Describe that tune! Invitation to the IMP.",
        "author": [
            "Rosemary Mountain"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1418187",
        "url": "https://doi.org/10.5281/zenodo.1418187",
        "ee": "https://zenodo.org/records/1418187/files/Mountain06.pdf",
        "abstract": "The ongoing research project The Interactive Multimedia Playroom (IMP) was established to stimulate discourse about issues relating to our perception and description of sounds in artistic and multimedia contexts. Although it was originally conceived to help develop better analytical tools for music, the unique and playful design is well-adapted to helping establish common references for potential collaborators in media arts.  As the team working on the project development includes experts in psychology as well as creative artists and theorists, the format of the project is being designed to maximize its transfer to psychological studies.  Unlike most psychological studies, however, we are particularly interested in the reactions of those intimately involved in the arts, and ask participants to comment on the suitability of the terminology, perceived relevance of the questions, etc.  It is believed that the issues being addressed by the project are fundamental ones which could have high relevance for MIR research, including descriptors, sound-image associations, and the recognition of salient characteristics of a musical excerpt.",
        "zenodo_id": 1418187,
        "dblp_key": "conf/ismir/Mountain06",
        "keywords": [
            "The Interactive Multimedia Playroom (IMP)",
            "stimulate discourse",
            "perception and description of sounds",
            "analytical tools for music",
            "media arts",
            "format design",
            "psychological studies",
            "arts reactions",
            "fundamental issues",
            "MIR research"
        ],
        "content": "Name that mood!  Describe that tune!  Invitation to  the IMP  \n Rosemary Mountain \nConcordia University \n7141 Sherbrooke St. W \nMontreal, Qc, Canada H4A 3C5 \nmountain@vax2.concordia.ca  \nAbstract \nThe ongoing research project The Interactive Multimedia \nPlayroom (IMP) was established to stimulate discourse \nabout issues relating to our perception and descrip tion of \nsounds in artistic and multimedia contexts. Althoug h it was \noriginally conceived to help develop better analyti cal tools \nfor music, the unique and playful design is well-ad apted to \nhelping establish common references for potential \ncollaborators in media arts.  As the team working o n the \nproject development includes experts in psychology as well \nas creative artists and theorists, the format of th e project is \nbeing designed to maximize its transfer to psycholo gical \nstudies.  Unlike most psychological studies, howeve r, we \nare particularly interested in the reactions of tho se \nintimately involved in the arts, and ask participan ts to \ncomment on the suitability of the terminology, perc eived \nrelevance of the questions, etc.  It is believed th at the issues \nbeing addressed by the project are fundamental ones  which \ncould have high relevance for MIR research, includi ng \ndescriptors, sound-image associations, and the reco gnition \nof salient characteristics of a musical excerpt. \n \nKeywords : descriptors, multimedia, non-verbal, play \n1.  Introduction \nThe Interactive Multimedia Playroom (IMP) is an \nexpansion of the earlier project The Multimedia Thesaurus \n(MMT) 1. The project was conceived to help stimulate \ndiscourse on the elusive questions relating to our \ndescription and identification of sounds and music.   It has \nalso been designed to help us understand the variou s ways \nin which music and sound can be naturally associate d with \nother sensory information (image, colour, light, an d \nmovement); and to what extent these associations ma y be \nshared within communities or cross-culturally.   Th e two \nmajor motivations for the project development were:  the \nperceived need for a greater range of tools for mus ic \nanalysis, and a need for a richer and more shared \nvocabulary among potential collaborators in the art s and \ntheir associates (performers, critics, teachers, et c.) Subsidiary motivations emerged from working in the field \nof electroacoustic music.  The  absence of notation  for \nelectroacoustics has created an urgency for creatin g new \nstrategies for composition as well as analysis.  Ev en basic \nsteps such as how to name and organize digital soun d files \nto facilitate subsequent retrieval are not yet clea r. \nThe IMP  project was not initially conceived of with any \nspecific  reference to MIR research.  However, it \ninvestigates several aspects that seem to overlap w ith that \nfield.  The aim of our project is to further our \nunderstanding about how people think about music an d \nsonic art, and to encourage the refinement of appro priate \nvocabulary and methods for describing music.  There fore, \nthe strategies which we are designing to stimulate discourse \non these issues, as well as the actual responses we  are \nbeginning to collect, may offer unusual perspective s to the \nquestions posed by MIR.   \n2.  Project description \nThe format of the Interactive Multimedia Playroom  is \nnormally that of a one-room installation.  A main v isual \nfocus of the installation is composed of a series o f plastic \nchains which hang in an even grid, about 0.5m apart , \nthereby describing a large cube through which parti cipants \ncan walk.  Participants are presented with a collec tion of \nobjects linked by barcode to numerous short (10 sec .) \nsound and image clips.  They are invited to scan an d then \n“sort” these coded objects in any of a variety of w ays: by \nsonic characteristics, genre, mood, colour, similar ity, \npairings, etc. A preliminary sorting can benefit fr om card \nlabels and diagrams, placed by bins and racks. The grid can \nthen be used as a major sorting cube, with appropri ate \nlabels being chosen for each of the three axes.  Pa rticipants \nare encouraged to explore the installation in group s, and \ndiscuss their choices.  It is these discussions tha t are at \npresent the most valuable aspect of the project. \nThe Playroom  houses not only the sound and image \nclips with their sorting grids and labels, but also  other \nresources which are designed to encourage involveme nt:  \nrelevant books and journals; sound-producing object s of \nboth electronic and acoustic types; video cameras, a \nPermission to make digital or hard copies of all or  part of this work for \npersonal or classroom use is granted without fee pr ovided that copies \nare not made or distributed for profit or commercia l advantage and that \ncopies bear this notice and the full citation on th e first page. \n© 2006 University of Victoria ______________________________________________  \n1 Both of these projects were funded by Hexagram, the  Montreal \ninteruniversity Institute of Research/C reation in Media Arts and \nTechnologies. SmartBoard, and drawing materials to record  partic ipants’ \nnon-verbal descriptions of sounds; computers linked  to \nwebsites of cognate projects.  Future plans include  a virtual \nversion to complement (but never replace!) the phys ical \nspace, and we are open to any means to link other r elevant \nprojects to ours, as the success of the project is directly \nrelated to the number of minds working in it. \n3.  Categories and descriptors \nTo date, we have experimented with several differen t labels \nfor the grid axes, used in a variety of combination s.  These \nhave been drawn mainly from psychology, musicology,  and \ncomputer music studies, and supplemented by suggest ions \nfrom team members and visitors.  One of the interes ting \naspects of the Thesaurus  (core element of the Playroom  \nand that most closely linked with possible categori zations \nof music and sounds) is that any participant can su ggest a \nlabel, and then we can experiment to see how useful  it \nseems over the range of musical and sonic examples.   \nClearly, some are not appropriate for the grid form at \n(which suggests a continuum between poles on each a xis): \ngenre and colour being two good examples. 2  We have \nbeen grouping the other (non-genre) labels loosely into two \ngroups: sonic characteristics, and qualities of ass ociation or \ncharacter (including mood). For sonic characteristi cs we \nhave been using categories such as melody, texture,  \ngesture, chordal, rhythmic, etc. These can be used in a \npreliminary classification, to refer to what seems the most \nstriking characteristic of the sound or its best de scription; \nsome of them can also be used as the basis for axis  labels \nto encourage sorting according to the nature of the  \ncharacteristic: “sparse/dense” or “less/more grainy ” for \ntexture; “smooth-angular” or “simple/complex” for \nmelody, “regular/irregular” for rhythm, etc. Of cou rse, with \nthe amount of borrowed vocabulary in music, these \ndescriptions quickly become associative too.  This is not \nseen as a problem, but rather as an interesting way  to probe \nthe ideas of researchers such as Rolf Inge Godøy – one of \nthe IMP  collaborators – who suggest that our appreciation \nof aural information is often if not always linked to some \nform of visual imagery. \n                                                           \n2 Although genre is naturally a  significant and oft en the first \nclassifier for many people, we have found it someti mes \nunhelpful, for several reasons. Many genre descript ions are so \nbroad that they beg further discrimination, but oft en only those \nwho listen to that broad category are familiar with  the sub-\ncategories, which can be carried by the specialist into an almost \ninfinite  number of subheadings.  Also, a considera ble bulk of \nthe past century’s musical repertoire exhibits a di versity of \nstyles but has not yet received adequate genre labe ls, remaining \nunder such unmanageable categories as “avant-garde classical”. \nIn general, a focus on genre seems to deflect from a focus on \nthe components which may contribute to its being re cognized \nas that genre. 4.  Team members \nProbably the most valuable aspect of the project is  the \ndiverse expertise of those who have volunteered to \nparticipate in the project development, whether as \nconsultants or experimenters.   Dr. John Sloboda, D r. \nAnnabel Cohen, and Dr. Stephen McAdams all have vas t \nknowledge of psychological studies (in the areas of  \nemotion & music, film music, and auditory perceptio n \nrespectively);  Dr. Louise Poissant and Dr. Leigh L andy \nare both leaders of projects which aim to define \nterminology ( Encyclopédie des Arts Médiatiques  and EARS  \nrespectively);  the other team members are all acti ve and \nnoted artists / researchers in design, dance, sculp ture, film, \nand music. 3    \n5.  Summary \nThe essential concepts and design of the Interactive  \nMultimedia Playroom  are being continually reviewed and \nupdated.  The project design improves itself natura lly as \nthose of us working on it further our collective \nunderstanding of the issues and factors involved.  \nMeanwhile, the nature of the project is proving suc cessful \nin encouraging participants, whether team members o r \nvisitors, to spend extensive time reflecting on the  latent \nassociations we have regarding sound and image, and  ways \nto articulate them. It was considered essential to have such \ndiversity of disciplines represented on the team an d \nthrough selection of installation locales, to ensur e that we \nare not too hasty to jump to any conclusions about how \n“people” listen, articulate, associate, and communi cate.  \nWe are not as interested in the “average” or majori ty \nresponse as we are in our own and our colleagues’ \nresponses to music and image.  I suggest that this priority, \nalong with the emphasis on collaborative discussion  about \ncategorization,  may provide insights to research b eing \nundertaken by members of the MIR community. In \naddition, the design of the Playroom – a “flexible \nframework” – can easily function as an adaptable te sting-\nground for work-in-progress. Meanwhile, we are alre ady \ndisseminating some of the MIR results to a communit y who \nmight otherwise be less likely to hear of them, and  thus \nhoping to ensure that the vocabulary and concepts a re \nmutually consistent and enriching. \n                                                           \n3 Space limitations precludes an exhaustive list of all team \nmembers and their respective research; please visit  the website \nhttp://www.armchair-researcher.com  for up-to.date information, \nmore details, pictures, upcoming installations, etc ."
    },
    {
        "title": "An Efficient Multiscale Approach to Audio Synchronization.",
        "author": [
            "Meinard Müller",
            "Henning Mattes",
            "Frank Kurth"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1417409",
        "url": "https://doi.org/10.5281/zenodo.1417409",
        "ee": "https://zenodo.org/records/1417409/files/MullerMK06.pdf",
        "abstract": "We present an efficient and robust multiscale DTW (Ms- DTW) approach to music synchronization for time-aligning CD recordings of different interpretations of the same piece. The general strategy is to recursively project an alignment path computed at a coarse resolution level to the next higher level and then to refine the projected path. As main contribu- tions, we address several crucial issues including the design and specification of robust and scalable audio features, suit- able local cost measures, MsDTW levels, constraint regions, as well as sampling rate adaptation and structural enhance- ment strategies. Extensive experiments on Western classi- cal music show that our MsDTW-based algorithm yields the same alignment result as the classical DTW-based strategy while significantly reducing the running time and memory requirements. Even for pieces of a duration of 10 to 15 min- utes, the alignment (based on previously extracted feature sequences) can be computed in less than a second. Keywords: audio synchronization, alignment, multiscale, chroma feature",
        "zenodo_id": 1417409,
        "dblp_key": "conf/ismir/MullerMK06",
        "keywords": [
            "audio synchronization",
            "alignment",
            "multiscale DTW",
            "chroma feature",
            "audio features",
            "local cost measures",
            "sampling rate adaptation",
            "structural enhancement",
            "running time",
            "memory requirements"
        ],
        "content": "An Efﬁcient Multiscale Approach to Audio Synchronization\nMeinard M ¨uller Henning Mattes Frank Kurth\nDepartment of Computer Science, University of Bonn\nR¨omerstraße 164, 53117 Bonn, Germany\nmeinard@cs.uni-bonn.de, henning1000@gmx.de, frank@cs. uni-bonn.de\nAbstract\nWe present an efﬁcient and robust multiscale DTW (Ms-\nDTW) approach to music synchronization for time-aligning\nCD recordings of different interpretations of the same piec e.\nThe general strategy is to recursively project an alignment\npath computed at a coarse resolution level to the next higher\nlevel and then to reﬁne the projected path. As main contribu-\ntions, we address several crucial issues including the desi gn\nand speciﬁcation of robust and scalable audio features, sui t-\nable local cost measures, MsDTW levels, constraint regions ,\nas well as sampling rate adaptation and structural enhance-\nment strategies. Extensive experiments on Western classi-\ncal music show that our MsDTW-based algorithm yields the\nsame alignment result as the classical DTW-based strategy\nwhile signiﬁcantly reducing the running time and memory\nrequirements. Even for pieces of a duration of 10to15min-\nutes, the alignment (based on previously extracted feature\nsequences) can be computed in less than a second.\nKeywords: audio synchronization, alignment, multiscale,\nchroma feature\n1. Introduction\nFor one and the same piece of music, there often exists a\nlarge number of CD recordings representing different inter -\npretations by various musicians. In particular for Western\nclassical music, these interpretations may exhibit consid er-\nable deviations in tempo, note realizations, dynamics, and\ninstrumentation. For example, a music database may con-\ntain for Beethoven’s Fifth Symphony some interpretations\nby Karajan and Bernstein, some historical recordings by\nFurthw ¨angler and Toscanini, Liszt’s piano transcription of\nBeethoven’s Fifth played by Sherbakov and Glenn Gould,\nor some synthesized version of a corresponding MIDI ﬁle.\nThen, the goal of audio synchronization is to automatically\ngenerate alignments between corresponding note events be-\ntween different interpretations. These alignments can be\nused to jump freely between different audio recordings, thu s\naffording efﬁcient and convenient music browsing.\nPermission to make digital or hard copies of all or part of this w ork for\npersonal or classroom use is granted without fee provided th at copies\nare not made or distributed for proﬁt or commercial advantage an d that\ncopies bear this notice and the full citation on the ﬁrst page .\nc/circlecopyrt2006 University of VictoriaIn the last few years, several alignment strategies have\nbeen proposed, see, e. g., [1, 2, 3, 4, 5, 6] and the refer-\nences therein. Most of these approaches rely on some vari-\nant of dynamic time warping (DTW). However, due to the\nquadratic time and space complexity, DTW-based strategies\nbecome infeasible for long pieces. To reduce the computa-\ntional cost, Salvador et al. [7] propose for general time se-\nries a multiscale DTW (MsDTW) approach that recursively\nprojects an alignment path computed at a coarse resolution\nlevel (using coarse features, e.g., obtained by averaging a nd\ndownsampling) to the next higher level and then reﬁnes the\nprojected path. One hazard with this approach is that an\nincorrect alignment on a low resolution level propagates to\nhigher levels resulting in erroneous alignment results. Th is\nhazard is fostered by the fact that coarsening the features\ncan lead to heavily deteriorated cost matrices, as is also il -\nlustrated by Fig. 4 (a)-(c). Dixon et al. [1] describe a lin-\near time DTW approach based on forward path estimation.\nFurther related work will be discussed in the respective sec -\ntions.\nIn this paper, we describe an MsDTW approach to efﬁ-\ncient as well as robust audio synchronization. In Sect. 2,\nthe general ideas of MsDTW are summarized. In Sect. 3,\nwe then propose solutions to several crucial issues of Ms-\nDTW including the design of robust and scalable audio fea-\ntures, speciﬁcation of suitable local cost measures, adapt a-\ntion strategies for the feature sampling rate, determinati on of\nMsDTW resolution levels and constraint regions, as well as\nenhancement strategies of DTW cost matrices. We then re-\nport on our extensive experiments based on a wide range of\nclassical music demonstrating the practicability of our al go-\nrithm. Furthermore, the synchronization results have been\nintegrated in our audio player [8] and soniﬁed for evaluatio n,\nsee Sect. 4. For some audio demos we refer to www-mmdb.\niai.uni-bonn.de/projects/MsDTWsync .\n2. General Multiscale Approach\nIn this section, we summarize the main ideas of classical\nDTW and MsDTW. Let X:= (x1,x2,... ,x N)andY:=\n(y1,y2,... ,y M)be two feature sequences with xn,ym∈\nF,n∈[1 :N],m∈[1 :M], where Fdenotes a suitable\nfeature space. Furthermore, let c:F × F → Rdenote\nalocal cost measure onF. The resulting (N×M)-cost\nmatrix Cis deﬁned by C(n,m) :=c(xn,ym).2.1. Classical DTW\nDTW is a well-known technique to align XandYwith re-\nspect to the cost measure c. Recall that a warping path is\na sequence p= (p1,... ,p L)withpℓ= (nℓ,mℓ)∈[1 :\nN]×[1 :M]forℓ∈[1 :L]satisfying 1 =n1≤n2≤...≤\nnL=Nand1 =m1≤m2≤...≤mL=M(bound-\nary and monotonicity condition), as well as pℓ+1−pℓ∈\n{(1,0),(0,1),(1,1)}(step size condition). The total cost of\npis deﬁned as/summationtextL\nℓ=1c(xnℓ,ymℓ). Then, an optimal warping\npath between XandYis given by a warping path p∗hav-\ning minimal total cost among all possible warping paths. To\ndetermine such an optimal path, one recursively computes\nan(N×M)-matrix D, where the matrix entry D(n,m)is\nthe total cost of an optimal path between (x1,... ,x n)and\n(y1,... ,y m). In the following, we denote a matrix entry\nD(n,m)ascell. Introducing an additional weight vector\n(wd,wh,wv)∈R3yields the recursive deﬁnition\nD(n,m) := min\n\nD(n−1,m−1) +wd·c(xn,ym),\nD(n−1,m) +wh·c(xn,ym),\nD(n,m−1) +wv·c(xn,ym),\nforn,m > 1. Furthermore, D(n,1) :=/summationtextn\nk=1wh·c(xk,y1)\nforn >1,D(1,m) =/summationtextm\nk=1wv·c(x1,yk)form > 1,\nandD(1,1) := c(x1,y1). Note that in the unweighted case\n(wd,wh,wv) = (1 ,1,1)one has a preference of the diag-\nonal alignment direction, since one diagonal step (cost of\none cell) corresponds to the combination of one horizontal\nand one vertical step (cost of two cells). To counterbalance\nthis preference, we chose (wd,wh,wv) = (2 ,1.5,1.5)(still\nslightly favoring the diagonal direction). Now, p∗can be de-\nrived from Din some linear fashion, see [9] for details. The\noverall computational cost is proportional to the number of\ncellsD(n,m)to be computed during this process leading to\na time and space complexity of O(NM).\n2.2. Multiscale DTW (MsDTW)\nTo speed up DTW computations, one common strategy is\nto impose a global constraint region R⊆[1 :N]×[1 :\nM]on the possible warping paths (e. g., Itakura parallelo-\ngram, Sakoe-Chiba band), thus limiting the number of cells\nneeded to be computed, see [9]. However, the usage of\nglobal constraint regions is problematic, since the optima l\nwarping path may leave the speciﬁed region. In other words,\nletp∗\nRdenote the optimal warping path with respect to R,\nthenp∗\nRmay differ from p∗, see Fig. 1. A second strategy\nis to reduce the feature sampling rate (also referred to as di -\nmensionality reduction or data abstraction), thus reducin g\nthe lengths NandMof the sequences to be synchronized.\nHowever, the resulting optimal warping path becomes in-\ncreasingly inaccurate or even completely useless as the res -\nolution decreases, see Fig. 4 for an illustration. MsDTW\nemploys these two strategies in some iterative fashion to\ngenerate data-dependent constraint regions. We summarize(a) (b) (c)\nFigure 1. (a) Optimal warping path p∗\n2on Level 2. (b) Opti-\nmal warping path p∗\nRwith respect to the constraint region R\nobtained by projecting path p∗\n2to Level 1. (Here, p∗\nRdoes not\ncoincide with the (unconstrained) optimal warping path p∗.)\n(c) Optimal warping path p∗\nRδusing an increased constraint\nregion Rδ⊃Rwithδ= 2. Here, p∗\nRδ=p∗.\nthe main ideas and refer to [7] for details. For a similar ap-\nproach, applied to melody alignment we refer to [10].\nLetX1:=XandY1:=Ybe the sequences to be syn-\nchronized having lengths N1:=NandM1=M, respec-\ntively. It is the objective to compute an optimal warping\npathp∗between X1andY1. The highest resolution level\nwill also referred to as Level 1. By reducing the feature\nsampling rate by a factor f2∈N, one obtains sequences X2\nof length N2:=N1/f2andY2of length M2:=M1/f2.\n(Here, we assume that f2divides N1andM1, which can be\nachieved, e. g., by suitably padding X1andY1.) Next, one\ncomputes an optimal warping path p∗\n2between X2andY2\non the resulting resolution level (Level 2). This path is pro -\njected onto Level 1 and there deﬁnes a constraint region R.\nNote that Rconsists of L2×f2\n2cells, where L2denotes the\nlength of p∗\n2. Finally, an optimal alignment path p∗\nRrelative\ntoRis computed. We say that this procedure is successful ,\nifp∗=p∗\nR. The overall number of cells to be computed in\nthis procedure is N2M2+L2·f2\n2, which is generally much\nsmaller than the total number N1M1of cells on Level 1.\nIn an obvious fashion, this procedure can be recursively ap-\nplied by introducing further levels of decreasing resoluti on.\nFor a complexity analysis, we refer to [7].\nThe constraint path p∗\nRmay not coincide with the optimal\npathp∗. To alleviate this problem, one can increase the con-\nstraint region R—at the expense of efﬁciency—by adding\nδcells to the left, right, top, and bottom of every cell in R\nfor some parameter δ∈N. The resulting region Rδwill be\nreferred to as δ-neighborhood of R, see Fig. 1.\n3. MsDTW Audio Synchronization\nThe multiscale approach to DTW constitutes a general frame-\nwork to speed up computations. In view of robustness, ef-\nﬁciency, and practicability of the resulting synchronizat ion\nprocedure, however, one has to specify several important pa -\nrameters. In this section, we describe and discuss the desig n\nchoices for our audio synchronization algorithm and report\non our experiments. To avoid incorrect alignments even in\nextreme situations we sketch a strategy for structural en-\nhancement of the cost matrix, which works even at very low\nresolution levels, see Sect. 3.5.01C \n01C#\n01D \n01D#\n01E \n01F \n01F#\n01G \n01G#\n01A \n01A#\n0 2 4 6 8 10 12 14 16 18 2001B \nFigure 2. The ﬁrst 21seconds of Bernstein’s interpretation of\nBeethoven’s Fifth Symphony. The light curves represent the lo-\ncal chroma energy distributions (10 features per second). The\ndark bars represent the CENS features (1 feature per second) .\n3.1. Audio Features\nPrevious studies have shown that chroma-based audio fea-\ntures are well suited to characterize harmony-based music,\nsee [11, 2, 12]. Representing the spectral energy of each of\nthe12traditional pitch classes of the equal-tempered scale,\nchroma features do not only account for the close octave re-\nlationship in both melody and harmony as it is prominent\nin Western music, but also introduce a high degree of ro-\nbustness to variations in timbre and articulation [11]. Sin ce\ndifferent interpretations typically exhibit signiﬁcant v aria-\ntions in such parameters, chroma-based features are well-\nsuited for audio synchronization leading to robust and accu -\nrate alignments, see [2].\nThere are various ways to compute chroma features, e. g.,\nby suitably pooling spectral coefﬁcients obtained from som e\nshort-time Fourier transform [11] or by suitably summing\nup pitch subbands obtained as output after applying some\npitch-based ﬁlter bank [12]. For details, we refer to the lit -\nerature. In our implementation, we convert an audio signal\ninto a sequence X= (x1,x2,... ,x N)of normalized 12-\ndimensional feature vectors xn∈[0,1]12,1≤n≤N,\nwhich express the local energy distribution in the 12 chroma\nclasses. Here, we use a feature sampling rate of 10Hz,\nwhere each feature vector xncovers 200ms of audio with\nan overlap of 100ms. This rate, which will constitute the\nﬁnest resolution level, turns out to be sufﬁcient in view of\nour intended applications.\nFor our multiscale approach, we need a computationally\ninexpensive way to adjust the feature resolution. Instead\nof simply modifying the analysis window in the chroma\ncomputation, we introduce a second, much larger statistics\nwindow (covering wconsecutive chroma vectors) and con-\nsider short-time statistics of the chroma energy distribution\n12345678924681012\n00.51\n12345678924681012\n11.52\nFigure 3. Optimal warping path based on the cost measure cα\nwithα= 0(left) and α= 1(right).\nover this window. This again results in a sequence of 12-\ndimensional vectors, which is then downsampled by a fac-\ntor ofqand renormalized with respect to the Euclidean norm\n(hence being invariant towards variations in dynamics). Fo r\nexample, w= 41 andq= 10 yields a feature sampling rate\nof1Hz, where each feature vector represents information\nof the audio signal within a window of 4200 ms. The re-\nsulting feature sequence will be referred to as CENS (w,q)\nsequence ( Chroma Energy Normalized Statistics). Similar\nfeatures have been applied in the audio matching scenario\nand are described in detail in [12]. Note that by modi-\nfying the parameters wandq, one can adjust the feature\ngranularity and sampling rate without repeating the cost-\nintensive chroma computations. Fig. 2 shows the result-\ning chroma and CENS (41,10)features sequences extracted\nfrom a Bernstein interpretation of Beethoven’s Fifth.\n3.2. Local Cost Measure\nThe normalized chroma and CENS features are elements in\nF:= [0,1]12. To compare two features x,y∈ F, we use the\ncost measure cα:F×F → [0,1]+αdeﬁned by cα(x,y) :=\n1− /an}bracketle{tx,y/an}bracketri}ht+αfor some offset α∈R≥0. (Note that /an}bracketle{tx,y/an}bracketri}ht\nis the cosine of the angle between xandy, since xandyare\nnormalized.) The offset αis introduced for the following\nreason. Audio recordings often contain long segments of\nlittle variance such as pauses or sustained chords. This lea ds\nto rectangular regions in the cost matrix, also referred to a s\nplateaus . Ifα= 0, all cells within a plateau reveal some\ncost close to zero. Being close to zero, there may be large,\nmore or less random relative differences among the costs\nof these cells (e. g., one cell has cost 0.01and another one\n0.001). As a result, one obtains an uncontrollable run of\nthe optimal warping path within a plateau. By increasing\nα, the relative differences decrease, whereas the absolute\ndifferences are retained unchanged (e. g., for α= 1, one cell\nhas cost 1.01and another one 1.001). As a consequence,\nthe effect of the weight vector (wd,wh,wv)as introduced\nin Sect. 2.1 becomes more dominant. For the parameters\n(wd,wh,wv) = (2 ,1.5,1.5)andα= 1, which have turned\nout to be suitable in our experiments, the diagonal directio n\nreceives a slight but stable preference in plateau regions, see\nFig. 3. In the following, we set c:=c1.Table 1. Speciﬁcation of the features used in our MsDTW au-\ndio synchronization system.\nLevel Feature Resolution Factor\n1 Chroma 10Hz -\n2 CENS(41,10) 1Hz 10\n3 CENS(121,30) 1/3Hz 3\n4 CENS(271,90) 1/9Hz 3\nTable 2. Total running time (for 363 synchronization pairs)\nagainst the number of levels used in the MsDTW algorithm\nbased on the features of Table 1.\nRun time \\Levels 11−21−31−4\ntCells [s] 1434.078.567.667.2\ntCENS [s] 0.016.524.729.8\ntMsDTW (absolute) [s] 1434.095.092.397.0\ntMsDTW (relative) [ %]100 6.626.446.76\n3.3. Resolution Levels and δ-Neighborhood\nAs described in Sect. 3.1, the chroma features at a time res-\nolution of 10Hz constitute the basic resolution (Level 1) of\nour audio synchronization. The CENS features, which can\nbe efﬁciently derived from the chroma features, are used at\nthe lower resolution levels. To determine a suitable number\nof levels as well as the resolutions at each level used in the\nMsDTW, we conducted comprehensive experiments. We re-\nport on some experiments that are based on the features in-\ndicated by Table 1. For our tests, we used 363pairs of CD\nrecordings, where each pair corresponds to two different in -\nterpretations of the same piece. The recordings have a dura-\ntion of 3to20minutes and cover a wide range of classical\nmusic, see Table 5 for examples. In a preprocessing step, we\ncomputed and stored the chroma features of all recordings.\nFor each test series, we performed an audio synchronization\nfor all 363pairs. The algorithms have been implemented in\nC/C++ and tests were run on an Intel Pentium M, 1.7 GHz,\n1 GByte RAM, under Windows XP.\nIn one test series, we used classical DTW on Level 1\nand MsDTW based on the ﬁrst two, three, and four lev-\nels. In all these cases we used δ= 30 , see the discus-\nsion below. The resulting running times are shown in Ta-\nble 2. The DTW-based strategy required 1434.0seconds to\nsynchronize all of the 363pairs. In contrast, the two-level\nMsDTW-based strategy required tCENS = 16.5seconds to\ncompute the CENS (41,10)-features used for Level 2, and\ntCells= 78.5seconds to compute all required cells on the\ntwo levels, amounting to a total running time of tMsDTW =\n95.0seconds—only 6.62% of the running time of classical\nDTW. Similarly, it took 92.3and97.0seconds when us-\ning the three-level and four-level MsDTW-based strategy,\nrespectively. In particular, we obtained the lowest total r un-\nning time for three levels. Using a fourth level indeed fur-\nther decreased the total number of cells to be evaluated,\nbut the computational overhead due to the additional CENSTable 3. Running time, absolute and relative error against the\nsize of the δ-neighborhood and adaptive neighborhood based\non a three-level MsDTW.\nRun time \\δ 010 20 30\ntCells [s] 29.341.654.667.6\ntCENS [s] 25.024.724.624.7\ntMsDTW (absolute) [s] 54.366.379.292.3\ntMsDTW (relative) [ %]3.794.625.526.44\nError (absolute) 92 27 6 0\nError (relative) [ %] 25.34 7.44 1.65 0\nfeatures needed at Level 4 deteriorated the overall result.\nAlso introducing additional intermediate levels had only a\nmarginal effect on the overall running time. We therefore\nuse three multiscale levels as default setting in our audio\nsynchronization system.\nFor our evaluation, we use the DTW-alignment (uncon-\nstrained warping path) as ground truth and check whether\nthe MsDTW-alignment (constrained warping path) entirely\ncoincides with the DTW-alignment (then MsDTW is called\nsuccessful, see Sect. 2.2) or not (then MsDTW produces\nan error). In another test series, we evaluated different δ-\nneighborhoods for a three-level MsDTW. Table 3 shows the\nperformance for δ= 0,10,20,30. For δ= 0, in92of\nthe363cases the MsDTW-based strategy was not success-\nful (corresponding to an error rate of 25.34%). Increasing\nδleads to an increase of the running time and a decrease of\nthe error rate. For δ= 30 , all363pairs have been success-\nfully aligned by the MsDTW strategy. The running time to\nevaluate the cells has increased from tCells= 29.3seconds\n(δ= 0) totCells= 67.6seconds ( δ= 30 )— a moderate\nincrease with regard to the total running time. For our audio\nsynchronization system, we therefore use δ= 30 as default.\n3.4. Experimental Results\nIn this section, we discuss some representative experimen-\ntal results based on the three-level MsDTW with δ= 30 .\nFor these parameters, as was reported above, the audio syn-\nchronization has been successful for all of the 363 pairs\nof audio recordings. Table 5 shows a selection of these\nrecordings including complex orchestral pieces having a du -\nration of 3to20minutes. Some of the interpretations sig-\nniﬁcantly differ in tempo, instrumentation, and articulat ion.\nFor example, Sacchi’s interpretation of Schubert’s Unﬁn-\nished is much faster ( 817seconds) than Solti’s interpreta-\ntion (951 seconds). Or, there is an orchestral as well as\na piano version (piano transcription) of Beethoven’s Fifth\nand Wagner’s Prelude, respectively. Furthermore, the Mae\ninterpretation of Vivaldi’s spring includes many addition al\nornamentations, which can not be found in the Zukerman\ninterpretation. In view of such signiﬁcant variations, the\nCENS (41,10)features as used on Level 2 constitute a goodTable 4. Performance of the implementation of our MsDTW-based au dio synchronization algorithm for a representative selection\nof recordings using three-levels and δ= 30 . For each level, the total number of cells (DTW) as well as the numbe r of cells to be\nevaluated by MsDTW are indicated. The last three columns show a com parison of the DTW and MsDTW running times.\nSynchronization Number of cells to be evaluated by DTW and MsDTW in each level Total run time[s]\nRecording 1 Recording 2 Level 1 Level 2 Level 3 Levels 1-3\nId 1 Length [s] Id 2 Length [s] DTW MsDTW [%] DTW MsDTW [%] DTW DTW MsDTW [%]\nBeet9Bern 1144.9Beet9Kar 1054.8 120808050 2117929 1.75 1209030 17657 1.46 134464 31.18 1.08 3.46\nRavBolAbb 862.5 RavBolOza 901.0 77737897 1694610 2.18 778426 14121 1.81 86688 20.04 0.80 3.99\nSchub8Sac 817.3 Schub8Sol 950.8 77736075 1704150 2.19 777918 14322 1.84 86541 20.01 0.80 3.99\nDvo9Maaz 704.2 Dvo9Franc 710.7 50068752 1356308 2.71 501255 11295 2.25 55695 12.85 0.60 4.67\nWagPreArm 595.0 WagPreGould 576.9 34337270 1124029 3.27 343892 9387 2.73 38407 8.85 0.48 5.42\nBeet5Bern 519.0 BeLi5Sher 444.1 23068056 923444 4.00 231400 7721 3.34 25926 5.84 0.37 6.34\nBeet5Bern 519.0 Beet5Kar 443.9 23052480 923028 4.00 230880 7716 3.34 25752 5.87 0.38 6.47\nSchosJazzCha 223.6 SchosJazzYab 193.6 4335306 394259 9.09 43456 3291 7.57 4875 1.11 0.15 13.51\nVivSpringMae 192.5 VivSpringZuk 218.9 4217940 389034 9.22 42267 3261 7.72 4745 1.07 0.13 12.15\nElgEnigDel 91.8 ElgEnigSino 93.1 856508 167699 19.58 8648 1404 16.23 992 0.22 0.06 27.27\nTable 5. Some audio recordings (with identiﬁer) contained in\nour test database comprising 33hours of audio.\nComposer / piece / interpreter Identiﬁer\nBeethoven / Symph. 5, Op. 67, 1st mov. / Bernstein Beet5Bern\nBeethoven / Symph. 5, Op. 67, 1st mov. / Karajan Beet5Kar\nBeeth. (Liszt) / Symph. 5, Op. 67, 1st mov. (piano) / Sherbakov BeLi5Sher\nBeethoven / Symph. 9, Op. 125, 4th mov. / Bernstein Beet9Bern\nBeethoven / Symph. 9, Op. 125, 4th mov. / Karajan Beet9Kar\nDvorak / Symph. 9, Op. 95, 1st mov. / Francis Dvo9Franc\nDvorak / Symph. 9, Op. 95, 1st mov. / Maazel Dvo9Maaz\nElgar / Op. 36, Andante (Enigma) / Del Mar ElgEnigDel\nElgar / Op. 36, Andante (Enigma) / Sinopoli ElgEnigSino\nRavel / Bolero / Abbado RavBolAbb\nRavel / Bolero / Ozawa RavBolOza\nSchubert / Symph. 8, D759, 1st mov. (Unﬁnished) / Sacchi Schub8Sac\nSchubert / Symph. 8, D759, 1st mov. (Unﬁnished)/ Solti Schub8Sol\nShostakovich / Jazz Suite No. 2, 6th mov. (Waltz) / Chailly SchosJazzCha\nShostakovich / Jazz Suite No. 2, 6th mov. (Waltz) / Yablonsky SchosJazzYab\nVivaldi / RV 269 (Spring), 1st mov. / Mae VivSpringMae\nVivaldi / RV 269 (Spring), 1st mov. / Zukerman VivSpringZuk\nWagner / Meistersinger Prelude / Armstrong WagPreArm\nWagner / Meistersinger Prelude (piano) / Gould WagPreGould\ncompromise between reasonable feature resolution, compu-\ntational efﬁciency and robustness of the alignment result.\n(Actually, in situations where one has signiﬁcant differen ces\nin note realizations—e. g., one interpreter plays a sustaine d\nchord whereas another interpreter plays an ornamentation—\nthe alignment on a ﬁner resolution level may not even be\nsemantically meaningful.)\nThe increase in performance of our MsDTW-based audio\nsynchronization in comparison to a classical DTW-based\napproach is illustrated by Table 4. For example, to synchro-\nnize ‘Beet9Bern’ and ‘Beet9Kar’ only 1.75% of the cells\non Level 1 have to be evaluated by MsDTW, decreasing the\nmemory requirements roughly by a factor of 57. Here, note\nthat the overall memory requirement is proportional to the\nmaximal number of cells needed to be evaluated at some\nlevel. The number of additional cells needed to be com-\nputed at Levels 2 and 3 is comparatively small. The overall\nrunning time (including the CENS feature computation) to\nsynchronize the two recordings (each version having a dura-\ntion of almost 20minutes) was 1.08seconds—nearly thirtytimes faster than classical DTW. Obviously, the relative sa v-\nings increase with the lengths of the pieces.\n3.5. Enhancing Cost Matrices\nWe have also conducted experiments with manually distorted\nand highly repetitive audio material, which constitutes an\nextreme scenario for MsDTW. As one example, we used\nthe ﬁrst 22seconds of an Cabrera interpretation of Bach’s\nToccata BWV 565, where the theme is repeated three times\nat three different octaves. We generated an audio ﬁle, re-\nferred to as ‘Bach12’, by concatenating four copies of this\nsegment, resulting in 12repetitions of the theme. We then\ngenerated a time-warped version of ‘Bach12’, referred to\nas ‘Bach12Warp’, by locally increasing and decreasing the\ntempo up to 50percent. The cost matrix using CENS (41,10)\nand the resulting optimal warping between ‘Bach12’ and\n‘Bach12Warp’ is shown in Fig. 4 (a). Now, reducing the\nfeature sampling rate leads to a heavily deteriorated cost\nmatrix, where the repetitions cannot be resolved any longer .\nThis, in turn, results in absurd optimal warping paths on the\nlower resolution levels, see Fig. 4 (b) and (c).\nTo alleviate this problem, we improve the structural prop-\nerties of the cost matrix by incorporating contextual infor -\nmation into the local cost measure, see [13]. Intuitively, t he\nidea is to enhance the diagonal path structure of the 2D cost\nmatrix by applying a local 1D low-pass ﬁlter along the di-\nagonals (having gradient (1,1)). Note that this process only\nworks if corresponding segments of the two audio record-\nings reveal the same tempo progression. To account for\ntempo differences in the two interpretations, the idea is to\nsimultaneously ﬁlter along different directions (in our im -\nplementation we used eight different gradients in a neigh-\nborhood of (1,1), which cover tempo variations of roughly\n−30to+40 percent) and then to take the minimum over\nthe ﬁlter outputs. The technical details of this approach ar e\ndescribed in [13]. The effect of this enhancement strategy,\nwhich allows to reduce the feature sampling rate without\ncompletely destroying the structural properties of the cos t20 40 6090\n80\n70\n60\n50\n40\n30\n20\n10\n10 20 3045\n40\n35\n30\n25\n20\n15\n10\n5 \n5 10 1522\n20\n18\n16\n14\n12\n10\n8 \n6 \n4 \n2 \n20 40 6090\n80\n70\n60\n50\n40\n30\n20\n10\n10 20 3045\n40\n35\n30\n25\n20\n15\n10\n5 \n5 10 1522\n20\n18\n16\n14\n12\n10\n8 \n6 \n4 \n2 (a) (b) (c)\n(d) (e) (f)\nFigure 4. Optimal warping path between ‘Bach12’ and\n‘Bach12Warp’ using CENS (41,10)(a), CENS (81,20)(b), and\nCENS (161,40)(c). The paths are semantically incorrect for\nthe two lower resolution levels. Locally ﬁltering the cost matrix\nbased on CENS (41,10)(using eight different gradients) and\ndownsampling by a factor 1(d),2(e), and 4(f) leads to correct\noptimal warping paths even on the lower resolution levels.\nmatrix, is illustrated by Fig. 4 (d)-(f). Even at the lowest r es-\nolution level (obtained by ﬁltering with an averaging ﬁlter\nof length 8and downsampling by a factor of 4), the optimal\nwarping path leads to the ‘correct’ alignment. In practice,\none has to assess the trade-off between increased computa-\ntional complexity caused by the additional ﬁltering step an d\nthe boost of robustness and conﬁdence due to the structural\nenhancement.\n4. Conclusions and Future Work\nIn this paper, we presented an efﬁcient MsDTW-based ap-\nproach to audio synchronization yielding stable alignment s\neven in the presence of signiﬁcant variations. One main ap-\nplication of audio synchronization is to allow for efﬁcient\nmusic browsing. To this end, we integrated the alignment\nresults into the SyncPlayer framwork [8] (an audio player\nwith additional retrieval and browsing functionality). Du r-\ning playback of a CD recording, SyncPlayer allows the user\nto directly jump to the corresponding position of any other\ninterpretation of the same piece. In view of this applicatio n,\nan alignment at a resolution of 10Hz is more than sufﬁcient.\nTo evaluate the absolute alignment quality achieved by our\nsystem, we conducted the following experiment. Based on\nthe alignment result of two recordings, we time-warped the\nsecond recording to run synchronously to the ﬁrst record-\ning. For the warping, we used an overlap-add technique\nbased on waveform similarity (WSOLA) as described in\n[14]. We then produced a stereo audio ﬁle containing the\nmono version of the original ﬁrst recordings in one channel\nand a mono version of the time-warped second recording\nin the other channel. Listening to this stereo audio ﬁle ex-\nhibits even small temporal deviations of less than 100ms be-tween corresponding note events. Some representative au-\ndio examples can be found at www-mmdb.iai.uni-bonn.\nde/projects/MsDTWsync . Based on the synchronization\nresult, it is possible to continuously blend from one inter-\npretation to another one. It would be an interesting task to\nemploy synchronization techniques for mixing and morph-\ning different interpretations. As another task, based on ou r\nchroma-based alignment, one can apply more reﬁned tech-\nniques and features to further enhance the alignment. Fi-\nnally, we plan to develop strategies to automatically detec t\ncritical audio segments (e. g., segments leading to plateau s),\nwhere one can then locally switch between various synchro-\nnization strategies (e. g., locally activating and deactiv ating\nthe enhancement strategy).\nReferences\n[1] Simon Dixon and Gerhard Widmer. Match: A music align-\nment tool chest. In Proc. ISMIR, London, GB , 2005.\n[2] Ning Hu, Roger Dannenberg, and George Tzanetakis. Poly-\nphonic audio matching and alignment for music retrieval. In\nProc. IEEE WASPAA, New Paltz, NY , October 2003.\n[3] Meinard M ¨uller, Frank Kurth, and Tido R ¨oder. Towards\nan efﬁcient algorithm for automatic score-to-audio synchro-\nnization. In Proc. ISMIR, Barcelona, Spain , 2004.\n[4] Christopher Raphael. A hybrid graphical model for align-\ning polyphonic audio with musical scores. In Proc. ISMIR,\nBarcelona, Spain , 2004.\n[5] Ferr ´eol Soulez, Xavier Rodet, and Diemo Schwarz. Improv-\ning polyphonic and poly-instrumental music to score align-\nment. In Proc. ISMIR, Baltimore, USA , 2003.\n[6] Robert J. Turetsky and Daniel P.W. Ellis. Force-Aligning\nMIDI Syntheses for Polyphonic Music Transcription Gener-\nation. In Proc. ISMIR, Baltimore, USA , 2003.\n[7] S. Salvador and P. Chan. FastDTW: Toward accurate dy-\nnamic time warping in linear time and space. In Proc. KDD\nWorkshop on Mining Temporal and Sequential Data , 2004.\n[8] Frank Kurth, Meinard M ¨uller, David Damm, Christian Fre-\nmerey, Andreas Ribbrock, and Michael Clausen. Syncplayer\n- an advanced system for content-based audio access. In\nProc. ISMIR, London, GB , 2005.\n[9] Lawrence Rabiner and Biing-Hwang Juang. Fundamentals\nOf Speech Recognition . Prentice Hall, 1993.\n[10] Norman Adams, Daniela Marquez, and Gregory H. Wake-\nﬁeld. Iterative deepening for melody alignment and retrieval.\nInProc. ISMIR, London, GB , 2005.\n[11] Mark A. Bartsch and Gregory H. Wakeﬁeld. Audio thumb-\nnailing of popular music using chroma-based representa-\ntions. IEEE Trans. on Multimedia , 7(1):96–104, 2005.\n[12] Meinard M ¨uller, Frank Kurth, and Michael Clausen. Audio\nMatching via Chroma-based Statistical Features. In Proc.\nISMIR, London, GB , 2005.\n[13] Meinard M ¨uller and Frank Kurth. Enhancing similarity ma-\ntrices for music audio analysis. In Proc. ICASSP , 2006.\n[14] W. Verhelst and M. Roelands. An overlap-add technique\nbased on waveform similarity (WSOLA) for high quality\ntime-scale modiﬁcation of speech. In Proc. ICASSP , 1993."
    },
    {
        "title": "Globally Optimal Audio Partitioning.",
        "author": [
            "Eric Nichols",
            "Christofer Raphael"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1416846",
        "url": "https://doi.org/10.5281/zenodo.1416846",
        "ee": "https://zenodo.org/records/1416846/files/NicholsR06.pdf",
        "abstract": "We present a technique for partitioning an audio file into maximally-sized segments having nearly uniform spec- tral content, ideally corresponding to notes or chords. Our method uses dynamic programming to globally optimize a measure of simplicity or homogeneity of the intervals in the partition. Here we have focused on an entropy-like mea- sure, though there is considerable flexibility in choosing this measure. Experiments are presented for several musical scenarios. 1 Keywords: audio partitioning, dynamic programming",
        "zenodo_id": 1416846,
        "dblp_key": "conf/ismir/NicholsR06",
        "keywords": [
            "audio partitioning",
            "dynamic programming",
            "entropy-like measure",
            "spectral content",
            "global optimization",
            "musical scenarios",
            "partitioning technique",
            "uniform spectral content",
            "notes or chords",
            "measure of simplicity"
        ],
        "content": "Globally Optimal Audio Partitioning\nEric Nichols\nDept. ofComputer Science, Indiana Univ.\nepnichol@indiana.eduChristopher Raphael\nSchool ofInformatics, Indiana Univ.\ncraphael@indiana.edu\nAbstract\nWepresent atechnique forpartitioning anaudio \u0002le\nintomaximally-sized segments having nearly uniform spec-\ntralcontent, ideally corresponding tonotes orchords. Our\nmethod uses dynamic programming toglobally optimize a\nmeasure ofsimplicity orhomogeneity oftheinterv alsinthe\npartition. Here wehavefocused onanentrop y-lik emea-\nsure, though there isconsiderable \u0003exibility inchoosing\nthismeasure. Experiments arepresented forseveralmusical\nscenarios.1\nKeywords: audio partitioning, dynamic programming\n1.Introduction\nThesignal-to-score problem seeks torepresent symbolically\nthecontent ofamusic audio \u0002le. While there aremany\nwaysinwhich theproblem canbeposed, perhaps themost\nambitious seeks arepresentation thatcaptures themost im-\nportant elements ofWestern music notation including pitch,\nrhythm, andvoice. Giventhesigni\u0002cant dif\u0002culty ofthis\ngoal, several simpler problems havebeen considered in-\ncluding signal-to-midi, which does notattempt toattrib ute\nrhythm orvoice totherecognized pitches; monophonic; or\npolyphonic single-instrument transcription [1],[2],[3],[4].\nWhile thechoice ofproblem isadif\u0002cult one, therelevant\ntradeof fsseem clear: simpler problem statements reduce the\ncomple xityofwhat weseek toestimate andareusually eas-\nier;ontheother hand onehopes thatwith aricher interpre-\ntation ofthemusical content, some elements which areless\nambiguous willhelp toreinforce acorrect interpretation of\nthose thataremore ambiguous.\nWeexplore here arelati velysimple problem statement\nwhich could aseasily beviewed asapreprocessing tech-\nnique asrecognition. Weseek topartition theaudio into\nsegments having nearly constant spectral properties andthus\nlikelytorepresent single notes orchords.\nOurgoal ismotivated, inpart, bythewell-kno wntrade-\noffsbetween time and frequenc yresolution faced when\n1This material isbased upon worksupported bytheNational\nScience Foundation under Grant No. IIS-0534694 andtheCenter\nforResearch onConcepts andCognition atIndiana University .\nPermission tomakedigital orhard copies ofallorpartofthisworkfor\npersonal orclassroom useisgranted without feeprovided thatcopies\narenotmade ordistrib uted forpro\u0002t orcommercial advantage andthat\ncopies bear thisnotice andthefullcitation onthe\u0002rstpage.\nc\r2006 University ofVictoriachoosing thelength ofaFourier transform: tomakepre-\ncise estimates ofunderlying frequencies, andhence pitch,\nonewants touselonger analysis segments. However,ifa\nsegment overlaps note boundaries, ourinterpretation ofthe\nspectral content isconfounded. Thus inthisanalysis we\nseek thelongest variable-length segments possible thatdo\nnotoverlap note boundaries. Inexploring tradeof fswithin\nthiseffortwehope tooversegment rather than underse g-\nment thedata, sothat each recognized segment does not\ncross note boundaries. While such asegmentation could be\nused toprovide useful pitch andrhythmic analysis ofthe\naudio data, itcould also beused simply asameans oflo-\ncating reasonable frames formore sophisticated analysis.\nSuch frames would provide better frequenc yresolution due\ntotheir maximally long nature.\n2.Partitioning through Optimization\nOurbasic approach istochoose apartition ofthedata that\noptimizes some objecti vefunction measuring theoverall ap-\npropriateness ofthepartition. Inparticular ,ourapproach is\nbased ondynamic programming, andrequires ourobjecti ve\nfunction tobeexpressed asasum ofcontrib utions, each de-\npending only onasingle interv alofthepartition. Tobe\nspeci\u0002c, suppose weviewouraudio data asacollection of\nMframes, each oflength Nsamples with actual samples\ns0;:::;sNM\u00001.Letb0<b1<:::<bnbeasequence of\nbreakpoints partitioning theaudio data into acollection of\nsegments. Wewrite(bk\u00001;bk)forthesequence ofsamples\nsNbk\u00001;:::;sNbk\u00001andweassume b0=0andbn=M.If\nH(bk\u00001;bk)isourmeasure ofthequality oftheparticular\ninterv al,then theoverall measure ofthepartition isgivenby\nH(b0;:::;bn)=nX\nk=1H(bk\u00001;bk)\nAsimple dynamic programming argument showsthatwe\ncancompute theoptimal partition recursi vely.LetH\u0003(b)be\nthescore oftheoptimal partition oftheaudio data(0;b).\nThe optimal partition of(0;b)must either betheunparti-\ntioned interv aloritmust becomposed oftheoptimal parti-\ntionofof(0;a)with theadditional interv al(a;b)appended\nforsomea<b.Theusual idea ofdynamic programming\nthen leads to\nH\u0003(b)=b\u00001\nmin\na=0H\u0003(a)+H(a;b) (1)\nwhere thecase ofasingle interv alisaccounted forbythe\na=0case (wede\u0002ne H\u0003(0)=0).The recursion canbemodi\u0002ed to\u0002nd thebest partition\nintointerv als(a;b)where lmin\u0014b\u0000a\u0014lmaxbyrede\u0002n-\ningH(a;b)=1when b\u0000a<lminorb\u0000a>lmax.\nRestricting theminimum sizeofinterv alshasthebene\u0002t of\nexcluding interv alstooshort toreasonably beanote, while\nrestricting themaximum sizedecreases thenumber ofinter-\nvalsonwhich wemust compute H(a;b)overwhelmingly\nthemain costinimplementing thealgorithm.\nAsimple modi\u0002cation ofthealgorithm instead mini-\nmizes\nH(b0;:::;bn)=n\u00001X\nk=0H(bk;bk+1)+n\u0015\nwhere wehaveadded thepenalty \u0015foreach interv alin\nthepartition, thereby encouraging explanations involving as\nfewinterv alsaspossible. Thepenalized version oftheob-\njectivefunction isminimized with therecursion\nH\u0003(b)=b\u00001\nmin\na=0H\u0003(a)+H(a;b)+\u00151a6=0 (2)\nwhere thelatter term is1only whena6=0.\nOur algorithm for\u0002nding optimal partitions iswell-\nknownintheliterature [5]andisalmost acanonical example\nofdynamic programming. Thetime comple xityofthealgo-\nrithm isclearly O(M2)since each possible interv almust be\nconsidered once inEqn.1. However,ifwerestrict themaxi-\nmalinterv alsize, then thealgorithm isO(M).\n3.Minimizing Entr opy\nOurmeasure ofthequality ofapartition isde\u0002ned interms\nofFourier transforms. Suppose oursequence ofMframes\nispartitioned into severalcontiguous sections f1;:::fnof\nlengths l1;:::;lnwithl1+:::ln=M.Here thefkarethe\nactual sample vectors corresponding totheinterv als.Sup-\npose wetakethe\u0002nite Fourier transform ofeachfkyielding\nktransforms F1;:::;Fk.The Parsevalrelation assures us\nthatthetotal energyinispreserv edbetween each(fk;Fk)\npair:X\nif2\nk(i)=X\njjFk(j)j2(3)\nConsequently ,ifwesum theenergyoverallcells ofthepar-\ntition wesee\nX\ni;kf2\nk(i)=X\nj;kjFk(j)j2(4)\nSince thelefthand sideissimply aconstant thetotal sum\nofsquares oftheaudio data, which isindependent ofchoice\nofpartition theright hand sidemust alsobeindependent\nofthechoice ofpartition.\nOurgoal istochoose apartition sothatFkcluster their\nenergyinascompact awayaspossible. Forthispurpose weFigur e1.Top:Merging similar frames results inreduced en-\ntropy.Bot: Merging dissimilar frames results inincreased or\nequivalent entropy.\nchoose tominimize theentropyoverallpartitions de\u0002ned as\nH=\u0000X\nj;kjFk(j)j2log2(jFk(j)j2)\nWhile itmight seem more natural tode\u0002ne theentrop yon\ntheversion ofjFk(j)j2normalized tosum to1,astheusual\nde\u0002nition does, theresult ofminimizing either entrop yover\nthepartition choice willbethesame duetoEqn. 4.\nFigure 1givesanintuiti veexplanation ofwhy minimiz-\ningentrop ymight lead toreasonable partitions. Inthetop\npanel ofthis\u0002gure, thelefthand member showsacartoon-\nlikepicture ofjFk(j)j2overaseries ofseveralconsecuti ve\noriginal (unconcatenated) frames, allcontained within asin-\nglemusical note. While there may well bemanyharmonics\ninthespectra, forsimplicity' ssakewefocus onasingle one,\nthehorizontal strip intheleft-hand-side ofthe\u0002gure. Now\nsuppose weconcatenate these frames into onelargeframe\nasintheright-hand member ofthe\u0002gure. Clearly theen-\ntropywill besmaller fortheright-hand member since we\nhaveconcentrated alltheenergyonasingle pixel.Note\nthatthenumber ofpixelsisthesame inboth cases. Thus the\nenergyminimizing partition tends tomergeframes corre-\nsponding tothesame note orchord, since thisleads tolower\nentrop y.\nInthebottom panel ofFigure 1wecontemplate the\nmergeofneighboring frames corresponding todifferent\nnotes orchords. While each ofthese arerepresented hor-\nizontally asone-pix el-wide spectra, each frame might be\ntheresult ofseveralmergesatsome earlier stage. The re-\nsultofmerging these twoframes results inthespectrum on\ntheright. Note thatboth cases concentrate theenergyonthesame number ofpixels,suggesting thattheentrop ymeasure\nisindif ferent towhether ornotwemergehere. However,\namore lifelik eversion ofthissituation would revealthat\npeaks intheright-hand spectrum will bemore spread out\nthan intheleft-hand side, duetothe\u0002ner resolution ofthe\nFourier transform. Forthisreason theentrop ycriterion will\ntend tofavorthesituation ontheleft, asisconsistent with\nourgoal.\nWhile theexperiments presented herein usetheentrop y\nmeasure astheobjecti vefunction tobeminimized, other\nreasonable choices arepossible andworthy offurther study .\nOne oftheother objecti vefunctions wehaveconsidered is\nbased onconditional entrop ythepartition thatminimizes\nG(b0;:::;bn)=nX\nk=1H0(bk\u00001;bk)E(bk\u00001;bk)+n\u0015\nwhere H0(bk\u00001;bk)istheentrop yofthesquared frequenc y\nenergyofsegment (bk\u00001;bk),normalized tosumtoone, and\nE(bk\u00001;bk)isthetotal squared energyinthesegment.\nThe other measure wehaveconsidered isbased onau-\ntoregression andisde\u0002ned by\nG(b0;:::;bn)=nX\nk=1A(bk\u00001;bk)+n\u0015\nwhere A(bk\u00001;bk)isthetotal residual squared error when\nanautore gressi vemodel of\u0002xedorder is\u0002ttothedata in\n(bk\u00001;bk).\n4.Experimental Results\nWeimplemented thedynamic programming algorithm de-\nscribed aboveintheClanguage. Allexperiments were per-\nformed ona2.1GHz Linux PC.Audio inputs were record-\nings ofmusical performances subsequently sampled down\nto8kHz mono audio. We\u0002xedtheframe sizeNto256\nsamples forallexperiments. The FastFourier Transform\n(FFT) wasused tospeed upentrop ycalculations.\nBecause theFFT algorithm used requires thenumber of\nsample points tobeapoweroftwo,whereas ourpartition\nsegments arenotsoconstrained, weusethestandard tech-\nnique ofzero-padding thedata totherequired size. Adding\nthese additional data points hastheeffectofinterpolating\ntheFFT overtheentire setoffrequenc ybins aswell as\nshrinking themagnitudes sothatthesquared norm (Eqn. 3)\nispreserv ed.Entrop yisnotpreserv edduring zero-padding;\nhowever,wecanapproximate thedesired non-zero-padded\nentrop yby\nH=X\nj;kjFk(j)j2log2\u0012\njFk(j)j2J0\nJ\u0013\n(5)\nwhere Jisthenumber ofsamples andJ0=2dlog2Jeisthe\nsizeofthezero-padded data.Tospeed upexperimentation, theprogram precomputes\nH(a;b)forallpossible pairs ofbreakpoints subject toa<b\nandb\u0000a\u0014lmaxandstores theresults ondisk. Fortwomin-\nutes ofaudio, thiscomputation would takeapproximately\n18minutes withlmax setto100. Ifdesired theperformance\ncould beimpro vedbyapproximating longer FFTs based on\nshorter FFT results. Once theentrop yvalues were recorded,\ntheywere used inmultiple experiments bythedynamic pro-\ngramming algorithm forarange ofvalues forparameters \u0015\nandlmin.Each computation ofanoptimal partition viathe\ndynamic programming algorithm took anegligible amount\noftime when using theprecomputed H(a;b)values.\nWeused theprogram tocompute optimal partitions for\nseveralpolyphonic audio examples, including recordings of\ntwoChopin Preludes (Op. 28,Nos. 7and20),andMvt. 1of\ntheSchostak ovich String Quartet No.3,Op.73,henceforth\nchopin7 ,chopin20 ,andschostakovich .Figures 2,\n3,and4showpartitions generated bythealgorithm foreach\nofthese examples.2Each \u0002gure displays aspectrogram of\nthedata with vertical lines indicating breakpoints found by\nthealgorithm. Particular values of\u0015selected foreach \u0002leto\nobtain these results were 0,0.1,and0.01, respecti vely.\nFigur e2.Section ofChopin Prelude No.7,Op. 28\nFigure 2compares thealgorithm results (partitions in\nthetophalf ofthe\u0002gure) with thetrue note onset times\n(bottom half ofthe\u0002gure). Manyofthenote onsets cor-\nrespond exactly with thegenerated partitions. Repeated\nchords areacommon omission inourresults, asarenote\nonsets where previous notes aresustained overthenewen-\ntry.Inthechopin20 results we\u0002ndthatoverpartitioning\noften breaks chords upintoasegment during theattack por-\ntionandasegment after thechord hasdecayed substantially .\nFortheschostakovich partition weseethatsome qui-\neternotes inthestrings aregrouped together intounderpar -\ntitioned segments.\n2Audio \u0002les ofthese results in.wavformat areavailable\nonline athttp://xavier.informatics.indiana.edu /\ncraphael/ismir06/ .Thepartitions generated bythealgo-\nrithm arerepresented byaudible clicks inthese \u0002les.Figur e3.Section ofChopin Prelude No.20,Op. 28\nFigur e4.Section ofSchostak ovich String Quartet No.3\nForeach experiment wehaveanimportant choice ofthe\npenalty parameter \u0015aswell asselection oftheallowedrange\nofpartition sizes speci\u0002ed bylminandlmax.Typical values\noflmax forourexperiments were 100 or200 frames, se-\nlected toreduce computation time while ensuring thatthe\nlongest notes ineach experiment would still\u0002twithin asin-\ngleinterv alofapartition. lmin,ontheother hand, wassetto\n4frames foreach experiment, corresponding totheshortest\nsixteenth notes inthechopin7 example.\nDifferent audio recordings yield different characteristic\nentrop yvalues. Tochoose agood \u0015foraparticular input,\nwewould search foravalue thatresulted inneither serious\noverpartitioning (suggested byaprevalence ofbreakpoints\nseparated bylmin)orunderpartitioning (indicated bybreak-\npoints separated bylmax).Wewould aimforavalue of\u0015\nthattended toslightly overpartition thedata. Serious over-\npartitioning wasnotalwaysapossibility; forexample, the\nbest \u0015turned outtobe0forchopin7 .Typically we\nwould \u0002nd agood result with\u0015<1.The problem of\u0015\nselection merits further study ,although weknowthat the\nrelationship between \u0015andthenumber ofinterv alsinthe\n0.0000 0.0002 0.0004Square Magnitude\nFigur e5.Spectrum offrame 240.0.000 0.001 0.002 0.003Square Magnitude\nFigur e6.Spectrum offrames 227-259.\npartition may beunderstood bythinking of\u0015asspecifying\nageometric prior onthenumber ofbreakpoints [5].\n5.Conclusion\nThegoal ofentrop y-dri venaudio partitioning istosegment\naudio input inamanner thatmaximizes theamount ofcon-\nsistent information within asegment. Fourier transforms of\nlargersegments should provide amore precise characteriza-\ntion offrequenc ydistrib ution. Asanexample oftheutil-\nityofourtechnique, consider thepartial spectrum from one\nframe oftheschostakovich data inFigure 5.This sin-\ngleframe isincluded ina33-frame long segment inthepar-\ntition computed above;Figure 6showsthespectrum com-\nputed from thislonger segment. Thepeaks aremuch more\nwell-de\u0002ned duetotheimpro vedfrequenc yresolution, and\nassuch may bemore amenable toanalysis byother audio\nprocessing algorithms.\nRefer ences\n[1]Saito S.,Kameoka H.,Nishimoto T.,Sagayama S.,Spec-\nmurt Analysis ofMulti-Pitch Music Signals with Adap-\ntiveEstimation ofCommon Harmonic Structure, Proc.Int.\nSymp. Music Info. Retrie val,2005 ,pp.8491, London UK.\n[2]Klapuri A.,Multiple Fundamental Frequenc yEstimation by\nHarmonicity andSpectral Smoothness, IEEE Trans.Speec h\nandAudio Processing ,11(6), 804-816, 2003.\n[3]Cemgil A.T.,Kappen H.J.,Barber D.AGenerati ve\nModel forMusic Transcription, IEEE Transactions onAu-\ndio,Speec handLangua geProcessing 14(2), March 2006.\n[4]Kapanci E.,PfefferA.,Signal-to-Score Music Transcrip-\ntionUsing Graphical Models, Proc.19th Int.JointConf .on\nArtif.Intel (IJCAI), 2005 ,Edinb urgh,UK.\n[5]B.Jackson, J.D.Scargle,D.Barnes, etal.,Analgorithm\nforoptimal partitioning ofdata onaninterv al,IEEE Signal\nProcessing Letter s,vol12,no.2,pp.105108, Feb.2005."
    },
    {
        "title": "Key Estimation Using a Hidden Markov Model.",
        "author": [
            "Katy C. Noland",
            "Mark B. Sandler"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1415800",
        "url": "https://doi.org/10.5281/zenodo.1415800",
        "ee": "https://zenodo.org/records/1415800/files/NolandS06.pdf",
        "abstract": "A novel technique to estimate the predominant key in a mu- sical excerpt is proposed. The key space is modelled by a 24-state Hidden Markov Model (HMM), where each state represents one of the 24 major and minor keys, and each observation represents a chord transition, or pair of consec- utive chords. The use of chord transitions as the observa- tions models a greater temporal dependency between con- secutive chords than would observations of single chords. The key transition and chord emission probabilities are ini- tialised using the results of perceptual tests in order to reflect the human expectation of harmonic relationships. HMM pa- rameters are then trained on a per-song basis using hand- annotated chord symbols, before the model for each song is decoded to give the likelihood of each key at each time frame. Examples of the algorithm as a segmentation tech- nique are given, and its capability to estimate the overall key of a song is evaluated using a data set of 110 Beatles songs, of which 91% were correctly classified. An extension to in- clude operation from audio data instead of chord symbols is planned, which will enable application to general music retrieval purposes. Keywords: Key estimation, chords, harmony, HMM.",
        "zenodo_id": 1415800,
        "dblp_key": "conf/ismir/NolandS06",
        "keywords": [
            "key estimation",
            "chords",
            "harmony",
            "HMM",
            "musical excerpt",
            "major and minor keys",
            "temporal dependency",
            "perceptual tests",
            "human expectation",
            "song classification"
        ],
        "content": "KeyEstimationUsingaHiddenMarkovModel\nKatyNoland,MarkSandler\nCentreforDigitalMusic,\nQueenMary,UniversityofLondon,\nMileEndRoad,\nLondon,E14NS.\nkaty.noland,mark.sandler@elec.qmul.ac.uk\nAbstract\nAnoveltechniquetoestimatethepredominantkeyinamu-\nsicalexcerptisproposed.Thekeyspaceismodelledbya\n24-stateHiddenMarkovModel(HMM),whereeachstate\nrepresentsoneofthe24majorandminorkeys,andeach\nobservationrepresentsachordtransition,orpairofconsec-\nutivechords.Theuseofchordtransitionsastheobserva-\ntionsmodelsagreatertemporaldependencybetweencon-\nsecutivechordsthanwouldobservationsofsinglechords.\nThekeytransitionandchordemissionprobabilitiesareini-\ntialisedusingtheresultsofperceptualtestsinordertoreﬂect\nthehumanexpectationofharmonicrelationships.HMMpa-\nrametersarethentrainedonaper-songbasisusinghand-\nannotatedchordsymbols,beforethemodelforeachsong\nisdecodedtogivethelikelihoodofeachkeyateachtime\nframe.Examplesofthealgorithmasasegmentationtech-\nniquearegiven,anditscapabilitytoestimatetheoverallkey\nofasongisevaluatedusingadatasetof110Beatlessongs,\nofwhich91%werecorrectlyclassiﬁed.Anextensiontoin-\ncludeoperationfromaudiodatainsteadofchordsymbols\nisplanned,whichwillenableapplicationtogeneralmusic\nretrievalpurposes.\nKeywords:Keyestimation,chords,harmony,HMM.\n1.Introduction\nThemainkeyofamusicalwork,andthesequenceofkeys\nthroughwhichthemusicpasses,arefundamentaltomusic\nanalysis.Thehomekeyservesasananchor:thechordse-\nquencesinthemusicmayliewithinthehomekey,ormay\ncontainnotesthatarenotpartofthehomekeyandtherefore\npullawayfromtheanchor,suggestingotherkeys.Itisthe\ninterplaybetweenkeysthatgivesharmonicinteresttothe\nmusic.\nThispaperdescribesanoveltechniqueforestimatingthe\nkeyofamusicalrecordingfromchordsymbolsonbotha\nframe-by-frameandaper-trackbasis.Anestimateofthe\nPermissiontomakedigitalorhardcopiesofallorpartofthisworkfor\npersonalorclassroomuseisgrantedwithoutfeeprovidedthatcopies\narenotmadeordistributedforproﬁtorcommercialadvantageandthat\ncopiesbearthisnoticeandthefullcitationontheﬁrstpage.\nc/circlecopyrt2006UniversityofVictoria\noverallkeyofatrackcanbedirectlyappliedtomusicre-\ntrievalsystems,ascanaharmonicstructureanalysisderived\nfromaframe-by-framekeyestimate.\nOurapproachmakesuseofperceptualtestscarriedout\nbyKrumhansl,describedin[1],andoutlinedinBox1.She\nusestheresultsinakey-ﬁndingalgorithm([1],Chapter4),\nforwhichsheusedtheprobetoneproﬁles(seeBox1).The\ndurationsofeachofthetwelvepossiblepitchesintheex-\ncerptweresummedtogiveatoneproﬁleofthemusic.The\ncorrelationsbetweentheproﬁleofthemusicandthetwelve\nkeyproﬁleswasthencalculated,andthekeywiththehigh-\nestcorrelationwastakentobecorrect.Performancewasex-\ncellentonBach,butthealgorithmwaslesswellabletocope\nwiththeextrachromaticismsofShostakovichandChopin.\nKrumhansl’sapproachtokeyﬁndingrequiresthemusic\ntoberepresentedinsymbolicformwithnotepitchesand\ndurationsexplicitlyspeciﬁed.Pauws[2]enablescalcula-\ntionsfromaudiodatabycalculatinga12-binchromagram\nforeachframe,givinganenergymeasureforeachofthe\n12pitchclasses.Thechromavalueswerethenusedinthe\ncorrelationcalculations,inplaceoftheproﬁleofthemusic\nthatKrumhanslderivedfrompitchdurations.G´omezand\nHerrera[3]suggestasimilarapproach,butmodifytheprobe\ntoneproﬁlestoemphasisepitchesinthetonic,dominantand\nsubdominantchords,andtotakeintoaccountharmonicsof\npitchesthatwillappearintheaudiosignal.Bothpapers\nsuggestthatanalysisofthetemporalstructureofthemusic\ncouldimproveaccuracy.\nHiddenMarkovModels(HMMs)incorporateadegreeof\ntemporaldependency,andhavebeenusedtoestimatetonal-\nity.ChaiandVercoe[4]estimatekeychangesinamusical\nexcerptusingchromagramdataastheobservationsfortwo\nHMMs.Intheﬁrstmodel,eachstaterepresentsakeypair\n(majoranditsrelativeminor);inthesecond,eachstaterep-\nresentsamode(majororminor),anddecodingbothmodels\ngivesboththerootandthemodeofthekey.Notrainingwas\ncarriedoutontheHMMparameters,andnopreferencewas\ngiventoanykey.Theyfoundthatvaryingtheprobabilityof\nstayinginthesamekeygaveatrade-offbetweenthepreci-\nsionofthekeysextracted,andtherecallandkeyaccuracy.\nBurgoyneandSaul[5]trainaDirichlet-basedHMMon\napitchclassproﬁle(similartoa12-binchromagram)for\neachaudioframe.Onlytheobservationprobabilitiesaretrained;thekeytransitionprobabilitiesaresetaccordingto\nameasureofrelatednessderivedfrommusictheory.Their\ntechniquehasbeensuccessfulforextractingsinglechords,\nbutinordertoaccuratelyextractthekeyaneedforamore\nadvancedharmonicmodelisidentiﬁed.\nOtherrelatedworkincludesthatofShehandEllis[6]and\nBelloandPickens[7],whouseHMMstoextractchordin-\nformationfromaudio,andTemperley’sBayesianapproach\ntosymbolickey-ﬁnding,availableat[8].\nThealgorithmsdescribedinthissectionhaveallsuc-\ncessfullyusedpriormusicalknowledgetoaidextractionof\nharmonicinformation,butallbasetheiranalysisonsingle\nchords.Thispaperintroducesfurthertemporaldependency\nintothetaskofkey-ﬁndingbytraininganHMMonchord\ntransitionsratherthansinglechords,aswellasmakinguse\noflisteningtestresultsforinitialisation,inordertorepresent\nexpectedrelationshipsbetweenchordsandkeys.\nSection2introducesthemodelandexplainstheinitiali-\nsation,traininganddecoding,Section3givesexampleanal-\nysesoftwosongs,Section4explainstheoverallkey-ﬁnding\nmethodandgivestheresults,whicharediscussedinSection\n5,andSection6concludesthepaper.\n2.ModelofKeySpace\nThemusic-theoreticnotionthatasoundedchordsequence\ncanstronglyimplyanunderlyingkey,oralludetomorethan\nonekey,ﬁtswellintotheHMMstructure,whichconsistsof\nasetofunderlying,unobservablestatesthatemitobservable\ndata.Analysisoftheobservabledata(chords)cangivethe\nmostlikelysequenceofunderlyingstates(keys),orthelike-\nlihoodofeachstateateachtimeframe(relativeimportance\nofallkeysovertime).ForanintroductiontoHMMs,to-\ngetherwithadescriptionofstandardtechniquesfortraining\nanddecoding,see[9]and[10].Foranintroductiontomusic\ntheorysee[11]and[12].\nSo,wemodeltonalityusingadiscreteHMM.Figure1\nshowsasimpliﬁeddiagramofthemodel.Only3keysare\nshownintheﬁgureforclarity,butthe24possiblemajor\nandminorkeysareincludedintheactualcalculations.Each\nstaterepresentsakey,andthemodelisfullyconnectedso\nthatanykeycanmovetoanyotherkey,orstaythesame.At\neachtimestepthekeygeneratesanobservablechordtransi-\ntion,forexampleCmajortoAminor.\nItwasdecidedthatapairofconsecutivechordsshouldbe\nusedforeachobservation,insteadofasinglechord,inorder\ntoextendthetemporaldependencyacrossagreaternumber\nofframes.Thetwochordsthatmakeupeachchordtran-\nsitioncanbeanymajor,minor,augmentedordiminished\ntriad,ornochord,whichoccursduringsilenceorentirely\npercussivesections.Morecomplexchordshavebeenex-\ncludedfromthemodelsincechordsthatarenotbasedon\ntriadsareveryrareintheWesternmusicrepertoireforwhich\nthisanalysisisintended,anditwasconsideredthatthese-\nquenceofunderlyingtriads,excludingextensions,issufﬁ-\nBOX1:PERCEPTUALTESTS(KRUMHANSL)\nMoredetailsofthefollowingtestsaregivenin[1],and\nnumericalresultsarealsogivenintheAppendix.\nProbetoneproﬁles\nTenlistenerswereaskedtojudgehowwelleachsemi-\ntoneﬁtwithinagivenmajororminorkeycontext,ona\nscaleof1(verygood)to7(verybad).Averageratings\nwerethencalculatedtorepresenttheimportanceofeach\nprobetonewithinamajorandminorkeycontext,giving\naprobetoneproﬁleforeachkey.\nCorrelationsbetweenkeyproﬁles\nThecorrelationbetweeneachpairofprobetoneproﬁles\nwascalculatedtogiveameasureofhowcloselyeach\npairofkeysisrelated.TheresultsaregiveninTable2\nintheAppendix.\nChordtransitionratings\nThechordtransitionratingtestwasusedtogivenumer-\nicalvaluestohowwellachordtransition,orpairof\nchords,ﬁtswithinagivenkeycontext.Listenerswere\naskedtojudgetheﬁtonascaleof1to7,andthemean\nratingwascalculated.Ratingsweregivenforallpossi-\nblediatonicchordtransitionswithinamajorkey,except-\ningthecasewherenotransitionismade,e.g.dominant-\ndominantortonic-tonic.Table3intheAppendixshows\ntheratings.\nSinglechordratings\nForthesinglechordratingtestlistenerswereaskedto\njudgehowwellsinglechordsﬁtwithinakey,usingthe\nsamescaleof0to7.Thistestincludedallmajor,minor\nanddiminishedtriadsinbothmajorandminorkeys,and\ntheaverageratingsareshowninTable4intheAppendix.\ncienttodeﬁnethekey.Allinversionsofthesamechordare\ntreatedidentically,whichmeansthatthechoiceofbassnote\ndoesnotaffecttheestimatedkey.\n2.1.Initialisation\nTherearethreeHMMparametersthatrequireinitialisation.\n2.1.1.Initialstateprobabilities\nTheinitialstateprobabilitiesreﬂectanypriorinformation\nthatwemayhaveaboutthemostlikelykey,beforeanyof\nthemusichasbeenheard.However,thereisnoreasonto\npreferanykeyaboveanyother,sotheinitialprobabilities\nforallstatesaresetequally,to1\n24.\n2.1.2.StateTransitionProbabilities\nTheinitialtransitionmatrixshouldexpresshowlikelyitis\nthatwheninaparticularkey,themusicmovestoanotherFigure1.Simpleﬁeddiagramoftheharmonicmodel.\nkeyatthenexttimestep.Intuitivelyitismostlikelythat\nthemusicwillstayinthesamekey,andifitdoeschange\nkeyitismostlikelytomovetoonethatiscloselyrelated\nandcontainsmanyofthesamechords.Theinitialkeytran-\nsitionmatrixwascreatedusingthekeyproﬁlecorrelations\ninTable2intheAppendix,whichgivenumericalvaluesto\nourintuitions.Thevalueswerecircularlyshiftedtogivethe\ntransitionprobabilitiesforkeysotherthanCmajorandC\nminor;anoperationthatassumesGistoGmajorasCisto\nCmajor,etc..Thevalueswereallmadepositivebyadding\n1,thentheywerenormalisedtosumto1foreachkey.This\ngavetheﬁnal24×24transitionmatrix.\n2.1.3.ObservationProbabilities\nTheinitialobservationprobabilitiesshouldreﬂectthehu-\nmanexpectationofthekey(s)impliedbyacertainchord\ntransition.Weareassumingthatthereisastrongcorrelation\nbetweenthekeyimpliedbyachordtransition,andthelike-\nlihoodofthetransitionoccurringinthatkey.Thisassump-\ntionissupportedbyKrumhansl[1]p.195,andbyﬁnding\nthecorrelationbetweenthechordtransitionratingsandthe\ncorrespondingnumberoftransitionspresentinourtestdata.\nCorrelationsof0.39formajorkeysand0.22forminorkeys\nwerefound,bothhighlysigniﬁcantgiventherespective40\nand154degreesoffreedom.\nSo,thechordtransitionBmajortoEmajorstronglyim-\npliesthekeyofEmajor,sinceitformsaperfectcadence,\nsotheprobabilityofstateEmajoremittingB-Ewillbevery\nhigh.However,bothchordsarealsocontainedinthekey\nofBmajor,sotheprobabilityofstateBmajoremittingB-E\nwillbealmostashigh.NeitherchordiscontainedinB/flatma-\njor,sotheprobabilityofstateB/flatmajoremittingB-Ewillbe\nverylow.\nTheratingsforchordtransitionswithinakey,givenin\nTable3intheAppendix,wereusedtoprovidenumerical\nvaluesfortheemissionmatrix.Theseonlycoverthedia-\ntonicchordsofmajorkeys,butthemodelincludesallma-\njor,minor,augmentedanddiminishedtriadsaswellasthepossibilityoftherebeingnochord,sosomeadditionalnu-\nmericalvalueswererequired.\nThepre-normalisedprobabilitiesforstayingonthesame\nchord,fordiatonicchords,weretakenfromtheratingsof\nindividualchordswithinakey,giveninTable4inthe\nAppendix,andartiﬁciallyboostedbecauserepeatedchords\noneitheraframe-by-frameorbeat-by-beatlevelarevery\nlikely.Theapproximateoptimalincreasewasexperimen-\ntallyfoundtobe2.Forexample,thepre-normalisedﬁgure\nforemittingatransitionfromAminortoAminorinthekey\nofCmajorwas3.62+2=5.62.Pre-normalisedvalues\nfortransitionsinvolvingoneormorenon-diatonicchordare\nsetuniformlylow,to1.Forminorkeystheratingsforma-\njorkeyscorrespondingtothesamescaledegreeswereused.\nTheemissionmatrixwasthennormalisedsothattheobser-\nvationprobabilitiessummedto1foreachkey.Theﬁnal\nemissionmatrix,then,haddimensions(48+1)2×24=\n2401×24,sincethereare48possiblechordsandthepossi-\nbilityofnochordtoformthechordtransitions,in24possi-\nblekeys.\n2.2.Training\nTheexpectationmaximisation(E-M)algorithm,described\nin[9],wasusedtolearntheHMMparametersforeachin-\ndividualsong.Iftheobservationprobabilities,whichmodel\ntherelationshipofeachchordtransitiontoeachkey,were\nsubjecttotraining,wecouldnolongerbecertainthatthe\nhiddenstatesrepresentkeys.Toverifythis,experiments\nwereconductedwithvariouscombinationsofHMMparam-\neterstrained.\nThetrainingdatawasasequenceofchordtransitions,so\nforachordsequenceDm-Bdim-Ctheﬁrstchordtransition\nwouldbeDm-Bdim,andthesecondBdim-C.Foreachkey,\neachchordtransitionwasgivenanumericalindexfrom1to\n2401.Thesewerecircularlyshiftedtogivethevaluesfor\notherkeys,withtheexceptionoftransitionsinvolvingano\nchord,whichhasthesamefunctionineverykeyandsowas\nkeptattheendofthesequence.\n2.3.Decoding\nTheViterbialgorithmwasusedtoﬁndthemostlikelyse-\nquenceofkeys,andstandardHMMdecoding[9]wasused\ntocalculatetheposteriorstateprobabilities,givingthelike-\nlihoodofbeinginanykeyateachtimeframe.\n3.SampleSegmentationResults\nThealgorithmwastestedonhandannotationsofthestart\nandendtimesofeverychordinalloftheﬁrst8Beatles\nalbums,providedbyHarte(see[13],[14]).Onlysimple\ntriadswereused:triadextensionswereignored,andnon-\ntriadicchordsweremappedtotheclosesttriadtypeaccord-\ningtotheircorrelationwith4chordtemplates,formajor,\nminor,augmentedanddiminishedchords.Tosimulatethe\nkindofsignalthatwouldbeobtainedfromaudiodata,withaviewtofutureextensionofthealgorithmtoworkfromau-\ndio,thechordsequencewassampledatequaltimeintervals\nof100ms,suchthatsampletimesthatfellbetweenachord\nstartandendtimeweregiventhecorrespondingchordlabel,\nandanyotherswerelabelledN,fornochord.\nFigures2and3showsomeexamplesoftheresults.The\nupperplotsshowtheposteriorstateprobabilities,andthe\nlowerplotsshowthemostlikelykeysequenceforthewhole\nsong.GroundtruthforthekeychangesintheBeatles’songs\nisnotavailabletoourknowledge,buttheﬁguresshowthat\nthealgorithmiscapableofextractingmeaningfulstructure.\nInI’llCryInstead(seeFigure2,bottom)thetwobridgepas-\nsagesinDmajor,at42sto52sand72sto82s,havebeen\nclearlyseparated.Similarly,inI’mHappyJusttoDance\nWithYou(seeFigure3,bottom)thechoruses(C/sharpminor)and\nverses(Emajor)havebeenextracted.TheBeatlesmodiﬁed\ntheﬁnalchorussuchthatthechordsformingthetransition\nbacktoEmajorareheardsoonerthaninpreviouschoruses,\ntheninterruptedwithC/sharpminoragain.Thistransitionap-\npearsasashortEmajorsectionatabout103to105s.This\ndemonstratesoneoftheweaknessesofourapproach,thatal-\nthoughthechordsweremostcloselyrelatedtoEmajor,the\nkeyofEmajorwasnotﬁrmlyestablished.Itisexpectedthat\nthistypeoferrorwouldoccurlessfrequentlyifthechords’\npositionrelativetothemusicalphrasesweretakenintoac-\ncount.\nInspectionoftheupperplotsofFigures2and3reveals\nfurtherstructurethatisnotapparentinthehardkeyclas-\nsiﬁcation.Forexample,intheﬁrstEmajorsectionofI’m\nHappyJusttoDanceWithYou,twoconsecutiveversesare\nplayed.Thehardclassiﬁcationofkeyshowninthelower\nplotcannotshowtherepetition,butthesameperiodinthe\nupperplotshowsapatternintheposteriorstateprobabilities\nofapproximately16sindurationthatisrepeatedonce.\nInvestigationofthealgorithmasamusicalstructureex-\ntractiontechniquewillbethesubjectoffurtherresearch.\n4.EvaluationTechnique\nInordertoproduceaquantitativeevaluationofthekeyanal-\nysisalgorithm,anexperimenttotestitsabilitytoextractthe\noverallkeyofasongwasdevised.Asubjectively-assessed\ngroundtruthisavailableat[15],whichgivesamusicologi-\ncalanalysisoftheBeatles’songs.Todeterminetheoverall\nkey,theoutputmatrixcontainingthelikelihoodofeachkey\nateachtimeframe,suchasthoseintheupperplotsofFig-\nures2and3,wassummedacrossthetimedomain,giving\nanoveralllikelihoodforeachkey.Thekeywiththelargest\nlikelihoodvaluewastakentobethekeyofthesong.\nItshouldbenotedthatthegroundtruthoftenmentioned\nmorethanonekey,inwhichcasetheﬁrstwastakentobe\nthemostimportant.Alsoseveralofthesongsaremodal,\nanddonotdirectlyﬁtthismodelofmajorandminorkeys.\nLydianandMixolydianmodesweretreatedasmajor,and\nDorianandAeolianasminor.\nTime (s)KeyRelative probabilities of all keys\n0 10 20 30 40 50 60 70 80 90 100Bm\nB\nBbm\nBb\nAm\nA\nG#m\nAb\nGm\nG\nF#m\nF#\nFm\nF\nEm\nE\nEbm\nEb\nDm\nD\nC#m\nDb\nCm\nC\n0 10 20 30 40 50 60 70 80 90 100C#mDDmEbEbmEEmFFmF#F#mGGm\nTime (s)KeyMost likely keysFigure2.Probabilitiesofeachkey(top),andmostlikelykey\n(bottom)foreachframefortheBeatlesI’llCryInstead.Black\nindicatesahighprobability.\nTime (s)KeyRelative probabilities of all keys\n0 20 40 60 80 100Bm\nB\nBbm\nBb\nAm\nA\nG#m\nAb\nGm\nG\nF#m\nF#\nFm\nF\nEm\nE\nEbm\nEb\nDm\nD\nC#m\nDb\nCm\nC\n0 20 40 60 80 100DbC#mDDmEbEbmEEm\nTime (s)KeyMost likely keysFigure3.Probabilitiesofeachkey(top),andmostlikelykey\n(bottom)foreachframefortheBeatlesI’mHappyJustto\nDanceWithYou.Blackindicatesahighprobability.\nTable1showsthepercentageofcorrectlyassignedover-\nallkeysforthe110songsintheﬁrst8Beatlesalbums,with\ndifferentHMMparameterstrained.\nFigure4showstheconfusionmatrixforthecasewhere\nthetransitionandpriorprobabilitiesweretrained,butthe\nobservationprobabilitieswerenot.Onlytheincorrectesti-\nmatesareshownintheﬁgure.\n5.Discussion\nTheresultsinTable1verifythepropositionthatﬁxingthe\nemissionprobabilitiesgivesthemostaccuraterepresenta-\ntionofthesong,sinceallowingadjustmentaltersthemean-\ningofthehiddenstates.Trainingthepriorstateprobabilities\nhadlittleeffectonthenumberofsongscorrectlyclassiﬁed.\nThisismostlikelyduetothestepwherethekeyprobabili-\ntiesacrossthewholesongweresummed:thepriorprobabil-Table1.Percentageofsongscorrectlyclassiﬁedwithvarying\ntraining.\nProbabilitiessubjecttotraining\n Percent\nPrior Transition Emission\n correct\nyes yes yes\n 27\nyes yes no\n 91\nyes no yes\n 18\nyes no no\n 87\nno yes yes\n 28\nno yes no\n 91\nno no yes\n 18\nno no no\n 87\nExpectedvalueforrandomchoiceofkey\n 4\nFigure4.Confusionmatrixforthecasewherepriorandtran-\nsitionprobabilitiesweretrained.Onlytheincorrectestimates\nareshown.Minorkeysfollowtheirparallelmajoralongthe\nhorizontalaxis.\nitieswillonlyaffecttheﬁrstfewframesandwilltherefore\nhavelimitedeffectontheoverallkeyestimation.Thesuit-\nabilityoftheperception-basedinitialisationwasconﬁrmed\nbythecasewithnotraining,where87%ofsongswerecor-\nrectlyclassiﬁed.Trainingthetransitionprobabilitiesfor\neachsonggavetheoptimumresultof91%ofsongscor-\nrectlyclassiﬁed.\nThe91%accuracyforﬁndingtheoverallkeyofBeatles\nsongsisveryencouraging.Closerinspectionoftheground\ntruthandconfusionmatrixrevealthatalloftheincorrect\nestimatescanbeexplained,andnoneisunreasonablyfar\nfromthegroundtruth.\nThreeofthemodalsongswereincorrectlyclassiﬁed:two\nMixolydiansongsweremistakenforthemajorkeyontheir\nfourthdegree,duetotheirﬂattened7th,andonesongwith\nDorianinﬂexionswasmistakenforthemajorkeyonits7th\ndegree,duetoitsﬂattened3rdand7th.Theseerrorsare\ncomparabletoerrorsbetweenrelativemajorandminorkeys.\nOnesonginAmajorwasmistakenforitsdominant,E\nmajor.However,thechordsthatmakeupthesongareB,\nE,AandDmajors,whichimplybothkeysequallywhen\nthereisnocontext.OnesonginAmajorwasmistakenforitssubdominant,explainedbytheparticularstressonthe\nﬂattened7thdegreeofthescale,usedheretogiveablues\nfeelratherthanamovetothesubdominantkey.Thesetwo\ncaseswouldbeneﬁtfromlongertemporaldependenciesin\nthemodel,basedonphraselengths,sinceitisusuallythe\nchordorcadenceattheendofaphrasethatdeﬁnesthekey.\nTheremainingﬁveincorrectkeyestimatesareforsongs\nwheremorethanonekeyismentionedinthegroundtruth\nforthehomekey,anditisoneofthesealternativekeysthat\nhasbeenselectedbythealgorithm.\n6.Conclusion\nAnHMMinitialisedwithresultsoflisteningtestshas\nprovedverysuccessfulforkeyestimationfromchordsym-\nbols,andshownpotentialasamusicalstructureextraction\ntechnique.Workingfromchordsymbolsisintendedasa\nmeansoftestingtheharmonicmodelwithouttheproblems\nassociatedwithaudioanalysis.However,formostinfor-\nmationretrievalpurposesitisnecessarytoworkwithaudio\ndatatoeliminatethepainstakingtaskofhandannotation,\nsoitisplannedtoextendthemodeltoincludeaudio-to-\nchordfunctionality.The91%accuracyreportedherewill\nalmostcertainlynotholdwhenworkingwithaudio,butwe\nwillhaveanunderstandingofhowtheaudio-to-chordand\nchord-to-keyerrorsdiffer.Amoredetailedexplorationof\nsegmentationpossibilitiesisalsoplanned.\nReferences\n[1]CarolL.Krumhansl,CognitiveFoundationsofMusical\nPitch,OxfordUniversityPress,1990.\n[2]SteffenPauws,“Musicalkeyextractionfromaudio,”in\nProceedingsofthe5thInternationalConferenceonMusic\nInformationRetrieval,Barcelona,2004.\n[3]EmiliaG´omezandPerfectoHerrera,“Estimatingthetonal-\nityofpolyphonicaudioﬁles:Cognitiveversusmachine\nlearningmodellingstrategies,”inProceedingsofthe5th\nInternationalConferenceonMusicInformationRetrieval,\nBarcelona,2004.\n[4]WeiChaiandBarryVercoe,“Detectionofkeychangein\nclassicalpianomusic,”inProceedingsofthe6thInterna-\ntionalConferenceonMusicInformationRetrieval,London,\n2005.\n[5]J.AshleyBurgoyneandLawrenceK.Saul,“Learninghar-\nmonicrelationshipsindigitalaudiowithdirichlet-basedhid-\ndenMarkovmodels,”inProceedingsofthe6thInternational\nConferenceonMusicInformationRetrieval,London,2005.\n[6]AlexanderShehandDanielP.W.Ellis,“Chordsegmentation\nandrecognitionusingem-trainedhiddenMarkovmodels,”\ninProceedingsofthe4thInternationalConferenceonMusic\nInformationRetrieval,Baltimore,Maryland,USA,2003.\n[7]JuanP.BelloandJeremyPickens,“Arobustmid-levelrep-\nresentationforharmoniccontentinmusicalsignals,”inPro-\nceedingsofthe6thInternationalConferenceonMusicInfor-\nmationRetrieval,London,2005.\n[8]DavidTemperley,“ABayesiankey-ﬁndingmodel,”MIREX\nSymbolicKey-Findingentry,2005,[website],[2006Jul04],Available:http://www.music-ir.org/evaluation/mirex-results/\narticles/key\nsymbolic/temperley.pdf.\n[9]LawrenceR.Rabiner,“AtutorialonhiddenMarkovmodels\nandselectedapplicationsinspeechrecognition,”inProceed-\ningsoftheIEEE,February1989,vol.77,no.2.\n[10]S.J.Cox,“HiddenMarkovmodelsforautomaticspeech\nrecognition:Theoryandapplication,”SpeechandLanguage\nProcessing,1990.\n[11]EricTaylor,TheABGuidetoMusicTheory,PartI,TheAs-\nsociatedBoardoftheRoyalSchoolsofMusic(Publishing)\nLtd.,1989.\n[12]EricTaylor,TheABGuidetoMusicTheory,PartII,TheAs-\nsociatedBoardoftheRoyalSchoolsofMusic(Publishing)\nLtd.,1991.\n[13]ChristopherHarte,“Chordtools&transcriptions,”Centre\nforDigitalMusicSoftware,2005,[website],[2006Jun27],\nAvailable:http://www.elec.qmul.ac.uk/digitalmusic/\ndownloads/index.html#chordtools.\n[14]ChristopherHarte,MarkSandler,SamerAbdallah,and\nEmiliaG´omez,“Symbolicrepresentationofmusicalchords:\nAproposedsyntaxfortextannotations,”inProceedingsof\nthe6thInternationalConferenceonMusicInformationRe-\ntrieval,London,2005.\n[15]AlanW.Pollack,“Noteson...series,”sound-\nscapes.info,2000,[website],[2006Jun27],Available:\nhttp://www.icce.rug.nl/∼soundscapes/DATABASES/AWP/\nawp-notes−on.shtml.\nAppendix:Perceptualtestresults\nTable2.Krumhansl’scorrelationsbetweenkeyproﬁles(see\n[1],p.38).\nCMajorCMinor\nCmajor\n 1.000 0.511\nC/sharp/D/flatmajor\n−0.500 −0.158\nDmajor\n 0.040 −0.402\nD/sharp/E/flatmajor\n−0.105 0.651\nEmajor\n −0.185 −0.508\nFmajor\n 0.591 0.241\nF/sharp/G/flatmajor\n−0.683 −0.369\nGmajor\n 0.591 0.215\nG/sharp/A/flatmajor\n−0.185 0.536\nAmajor\n −0.105 −0.654\nA/sharp/B/flatmajor\n 0.040 0.237\nBmajor\n −0.500 −0.298\nCminor\n 0.511 1.000\nC/sharp/D/flatminor\n−0.298 −0.394\nDminor\n 0.237 −0.160\nD/sharp/E/flatminor\n−0.654 0.055\nEminor\n 0.536 −0.003\nFminor\n 0.215 0.339\nF/sharp/G/flatminor\n−0.369 −0.673\nGminor\n 0.241 0.339\nG/sharp/A/flatminor\n−0.508 −0.003\nAminor\n 0.651 0.055\nA/sharp/B/flatminor\n−0.402 −0.160\nBminor\n −0.158 −0.394\nTable3.Krumhansl’schordtransitionratings(see[1],p.193).\nFirst\n SecondChord\nChord\nI ii iiiIVV viviiAve\nI\n 5.104.785.915.945.264.575.26\nii\n 5.69 4.004.766.104.975.415.16\niii\n 5.384.47 4.635.034.604.474.76\nIV\n 5.945.004.22 6.004.354.795.05\nV\n 6.194.794.475.51 5.194.855.17\nvi\n 5.045.444.725.075.56 4.505.06\nvii\n 5.854.164.164.535.164.19 4.68\nAve\n 5.684.834.395.075.634.764.76\nTable4.Krumhansl’sharmonichierarchyratingsformajor,\nminoranddiminishedchords(see[1],p.171).\nContext\nChord\n CMajorCMinor\nCMajor\n 6.66 5.30\nC/sharp/D/flatMajor\n 4.71 4.11\nDMajor\n 4.60 3.83\nD/sharp/E/flatMajor\n 4.31 4.14\nEMajor\n 4.64 3.99\nFMajor\n 5.59 4.41\nF/sharp/G/flatMajor\n 4.36 3.92\nGMajor\n 5.33 4.38\nG/sharp/A/flatMajor\n 5.01 4.45\nAMajor\n 4.64 3.69\nA/sharp/B/flatMajor\n 4.73 4.22\nBMajor\n 4.67 3.85\nCMinor\n 3.75 5.90\nC/sharp/D/flatMinor\n 2.59 3.08\nDMinor\n 3.12 3.25\nD/sharp/E/flatMinor\n 2.18 3.50\nEMinor\n 2.76 3.33\nFMinor\n 3.19 4.60\nF/sharp/G/flatMinor\n 2.13 2.98\nGMinor\n 2.68 3.48\nG/sharp/A/flatMinor\n 2.61 3.53\nAMinor\n 3.62 3.78\nA/sharp/B/flatMinor\n 2.56 3.13\nBMinor\n 2.76 3.14\nCDim\n 3.27 3.93\nC/sharp/D/flatDim\n 2.70 2.84\nDMinor\n 2.59 3.43\nD/sharp/E/flatDim\n 2.79 3.42\nEDim\n 2.64 3.51\nFDim\n 2.54 3.41\nF/sharp/G/flatDim\n 3.25 3.91\nGDim\n 2.58 3.16\nG/sharp/A/flatDim\n 2.36 3.17\nADim\n 3.35 4.10\nA/sharp/B/flatDim\n 2.38 3.10\nBDim\n 2.64 3.18"
    },
    {
        "title": "Perceptual evaluation of music similarity.",
        "author": [
            "Alberto Novello",
            "Martin F. McKinney",
            "Armin Kohlrausch"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1416700",
        "url": "https://doi.org/10.5281/zenodo.1416700",
        "ee": "https://zenodo.org/records/1416700/files/NovelloMK06.pdf",
        "abstract": "This paper presents an empirical method for assessing mu- sic similarity on a set of stimuli using triadic comparisons in a balanced incomplete block design. We first evaluated the consistency of subjects in their rankings and then the con- cordance across subjects. The concordance was also evalu- ated for different subject populations to assess the influence of experience of the subject with the musical material. We finally analysed subjects’ ranking by the means of multidi- mensional scaling. Similarity judgments were found to be rather concordant across subjects. Significant differences between musicians and non-musicians and between subjects being familiar or non-familiar with the music were found for a small number of cases. Multidimensional scaling reveals a proximity of songs belonging to the same genre, congruent with the idea of genre being a perceptual dimension in subjects’ similarity ranking. Keywords: Perception, Music, Music Similarity.",
        "zenodo_id": 1416700,
        "dblp_key": "conf/ismir/NovelloMK06",
        "keywords": [
            "empirical",
            "music similarity",
            "triadic comparisons",
            "balanced incomplete block design",
            "subject rankings",
            "subject populations",
            "multidimensional scaling",
            "genre perception",
            "genre similarity",
            "musicians vs non-musicians"
        ],
        "content": "Perceptual evaluation of music similarity\nAlberto Novello1, Martin F. McKinney1, Armin Kohlrausch1,2\n1Philips Research Laboratories, Eindhoven, The Netherlands\n2Human Technology Interaction, Technische Universiteit Eindhoven, Eindhoven, The Netherlands\n{alberto.novello, martin.mckinney, armin.kohlrausch }@philips.com\nAbstract\nThis paper presents an empirical method for assessing mu-\nsic similarity on a set of stimuli using triadic comparisons in\na balanced incomplete block design. We ﬁrst evaluated the\nconsistency of subjects in their rankings and then the con-\ncordance across subjects. The concordance was also evalu-\nated for different subject populations to assess the inﬂuence\nof experience of the subject with the musical material. We\nﬁnally analysed subjects’ ranking by the means of multidi-\nmensional scaling.\nSimilarity judgments were found to be rather concordant\nacross subjects. Signiﬁcant differences between musicians\nand non-musicians and between subjects being familiar or\nnon-familiar with the music were found for a small number\nof cases.\nMultidimensional scaling reveals a proximity of songs\nbelonging to the same genre, congruent with the idea of\ngenre being a perceptual dimension in subjects’ similarity\nranking.\nKeywords: Perception, Music, Music Similarity.\n1. Introduction\nIn the domain of Music Information Retrieval there has re-\ncently been an increasing interest in the automatic evalua-\ntion of music similarity. Although the task of identifying\nsimilar music often seems quite simple for a human listener,\nit is rather difﬁcult to assess algorithmically or to represent\nit perceptually because of the multidimensionality involved\nin the human cognitive process.\nThe proposed theoretical models underline the multidi-\nmensional nature of perceived music similarity and stress\nthe importance of the perceptual weighting of the various\nmusical dimensions in this respect. Deliege’s approach is\nan extension of the Gestalt theory applied to music [1]. The\nlistener uses his/her prior experience to segment the musi-\ncal piece and extracts features from every part. A weighted\ncomparison between the features extracted from different\nparts can tell whether two parts are similar and in which re-\nspect. Okelford’s zygonic theory is a more musicology ori-\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies\nare not made or distributed for proﬁt or commercial advantage and that\ncopies bear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2006 University of Victoriaented [2] alternative that tries to describe the feeling of mu-\nsical derivation. This approach focuses on the relationships\nbetween notes (pitches, tempi, intervals, etc.) within a song,\nthus it seems more suitable for MIDI data than for raw audio\nsignals. Cambouropulos’ [3] unscramble algorithm/model\nalso incorporates an important part of dimensional weight-\ning but the nature of the weights is purely theoretical and its\nveriﬁcation has been performed using just one song.\nThe multidimensionality of music similarity has mainly\nbeen explored through perception experiments [4]. Several\nstudies [5, 6, 7] on this topic show the primary importance\nof the songs’ tempo, genre and timbre in subjects’ similarity\nratings. In Chupchik’s [5] experiments, the dominant di-\nmensions used by subjects in similarity ratings were tempo,\ndominant instrument and articulation in a ﬁrst experiment\nand tempo and genre in a second experiment. Dibben and\nLamont [6] using a similar paradigm found that subjects pri-\nmarily listened to ”surface” features such as tempo, dynam-\nics and articulation.\nConsidering the experimental framework in this domain,\nthe research published so far appears very fragmented or too\nspeciﬁc for general applicative interest based on a global\nrepresentation of similarity [7-12]. As for the applications,\nfew algorithms developed for assessment of music similarity\nbetween songs are linked to human perception.\nFrom examining the literature in the domain of music\nsimilarity it appears that there is a lack of experimental veri-\nﬁcation of the theoretical perceptual models proposed [1, 2].\nExamples of open experimental questions are: do subjects\nhave a common perception of music similarity? What are\nthe principal perceptual features used by subjects in rating\nsimilarity? Which features are most relevant? Is there a\nsigniﬁcant inﬂuence of musical experience (musicians/non-\nmusicians, familiar/non-familiar musical material)?\n2. Method\nWe use the method of triadic comparisons because it is a\nrather simple ranking procedure for subjects (compared to\nrating) and an efﬁcient method to extract maximum infor-\nmation from a small set of stimuli. In the experimental ses-\nsion the subject is provided with three stimuli A, B and C\nand is asked to choose which pair is the most similar and\nwhich is the most dissimilar.\nIn the method of triadic comparisons, a complete block\ndesign is made with all possible comparisons (without sym-metric repetitions). Having nstimuli, the number of triads\nis given by the formula:\nb=n(n−1)(n−2)\n6(1)\nA valid alternative is the balanced incomplete block de-\nsign(BIBD) in which all possible pair-wise comparisons of\nstimuli occur λtimes. The BIBD reduces the experimental\ntime by a factor λ/(n−2)compared to the complete design.\n2.1. Experiment\nSubjects were asked to listen to a set of three song-excerpts\n(triads) and choose for each comparison the most similar\nand most dissimilar of the three possible pairs. A question-\nnaire was also presented to each subject to evaluate subject\nfamiliarity with each stimulus, musical training, and general\ninformation (age, gender, etc.).\nOur primary interest in the analysis of the whole com-\nplexity of popular music genres together with the limitations\nof experimental time lead to the choice of 18 audio excerpts\nstimuli spanning 9 genres: pop, rock, country, blues, jazz,\nheavy metal, hip hop, classical and funk.\nOne fast ( ≥140 BPM) and one slow song ( ≤100 BPM)\nwere chosen for each genre. For each song, a representative\nsection of 10 seconds was extracted from the chorus.\nIn the present experimental design, λ= 2was chosen as\na good compromise between experimental time and stim-\nulus repetition. With 18 stimuli this factor gives a total\nof 102 triads in the BIBD. In addition, 10 triads were re-\npeated in the design to examine subject consistency. The\n10 repeated triads were chosen to span a range of difﬁculty\nas determined from a pilot experiment. The total number\nof triads was thus 112 and took about 50 minutes to com-\nplete. The obtained BIBD was randomly permutated to form\n6 experimental designs to examine possible order effects.\nThirty-six subjects participated: 18 musicians with at least\n5 years of musical training and 18 non-musicians with no\nmusical training (except compulsory low level school music\ncourses).\n2.2. Analysis\nThe data analysis can be separated in three parts: subject\nconsistency, across-subject concordance and multidimen-\nsional scaling. In the ﬁrst two parts Kendall’s coefﬁcient of\nconcordance (KCC) is used to assess subject’s concordance\n[13]. Kendall’s Wis deﬁned as:\nW=12S\nm2(n3−n)(2)\nwhere Sis the variance of the sum (across mjudges) of\nranks for nstimuli:\nS=n/summationdisplay\nj=1(Rj−¯R)2(3)where Rjis the sum of judges’ rankings for the j-th stim-\nulus, and ¯Ris the average value for Rj(always equal to\n1\n2m(n+ 1)).\n2.2.1. Within-Subject Analysis\nThe 10 repeated triads were used to assess subjects’ consis-\ntency. To do this, we calculated the KCC on the data from\nrepeated trials for individual subjects. We used this measure\nto examine subject consistency in general and to compare\nsubject consistency between musicians and non-musicians\nand between subjects familiar and unfamiliar with the mu-\nsic.\n2.2.2. Across-Subject Analysis\nThe across-subject analysis was performed using only the\n102 non-repeated triads in the BIBD. We ﬁrst calculated\nKCC, across all the 36 subjects in the experiment for each\nof the 102 triads to evaluate general concordance of differ-\nent subjects in their ranking and particular triad difﬁculty.\nFurther analysis was conducted to ﬁnd subjects whose pair\nranking signiﬁcantly reduced the overall concordance.\nA third analysis divided subjects in two populations: mu-\nsicians and non-musicians to assess across-subject concor-\ndance in each group. The bootstrap technique was applied\nto compare the two populations.\nThe ﬁnal step of this analysis followed the same proce-\ndure. Subjects were divided depending on their familiarity\nwith each triad into three groups: non-familiar, neutral and\nfamiliar. As not all triads had a sufﬁcient number of sub-\njects in both groups, familiarity and non-familiarity thresh-\nolds were adjusted to allow this veriﬁcation on about half of\nthe triads.\n2.2.3. Multidimensional Scaling\nAfter rejecting 8 outliers due to subject inconsistency (see\nbelow), we built a dissimilarity matrix of the remaining sub-\njects’, assigning the value 2 to the least similar pair, 1 to the\nmiddle pair and 0 for the most similar pair. We used the\ndissimilarity matrix as input to a multidimensional scaling\n(MDS) program calculation [14]. Given an input matrix of\ndistances between points, the algorithm calculates the opti-\nmal positions of the points in an n-dimensional space.\nIn order to determine the optimal number of dimensions\nin the algorithmic calculation and plotting, we performed\nthe MDS calculation with increasing number of dimensions\n(from 1 to 10) and recorded the stress value [14].\n3. Results\n3.1. Subject Consistency\nFor each subject, we calculated 10 concordance values W\n(one for each repeated triad), to measure subject consis-\ntency. The mean Wvalues across 10 triads are plotted in\nFig. 1.\nWe obtained the 5% signiﬁcance level from the normal\ndistribution of possible KCC values with 2 judges. The0 510 15 20 25 30 350.50.550.60.650.70.750.80.850.90.951\nSubject numberMean concordance (W) value.05 significance levelFigure 1. Mean concordance values for subjects on the 10 re-\npeated triads to evaluate subject consistency\nplot shows that on average 8 subjects (4 musicians and 4\nnon-musicians) are not signiﬁcantly consistent within them-\nselves on repeated triads’. These subjects were considered\noutliers and were rejected from the following part of the\nanalysis. No signiﬁcant differences in subject consistency\nwere found between musicians/non-musicians or between\nsubjects being familiar/non-familiar with the music.\n3.2. Across-Subject Concordance\nTheW1, .., W 102values of KCC for each triad across all\nsubjects are displayed in the Fig. 2. The data show gen-\neral agreement across subjects: 97 triads from a total of 102\nshow rankings with signiﬁcant concordance. Four of the ﬁve\ntriads on which subjects show no concordance are formed\nby stimuli belonging to three different musical genres. The\nremaining triad is composed of two rock-excerpts and one\nheavy metal excerpt.\n0 20 40 60 80 10000.10.20.30.40.50.60.70.80.91\n .05 significance level\nTriad numberSubject concordance (W)\nFigure 2. Across-subject concordance (W) per triad\nFurther analysis focused on the differences in distribu-\ntions of the KCC values for musicians ( Wm) and for non-\nmusicians ( Wnm). In this case, only 6 triads out of 102 showsigniﬁcant differences in concordance between musicians\nand non-musicians and on all these triads musicians show\nhigher concordance than non-musicians. These triads (dif-\nferent from the triads shown in Fig. 2 on which subjects tend\nnot to be concordant) contained stimuli belonging to three\ndifferent genres. The difference between the distributions\nof the concordance for “non-familiar” subjects and “very-\nfamiliar” subjects has also been calculated for the triads that\nhad a sufﬁcient number of subjects in both groups. In this\ncase again, only 5 triads out of the total non-rejected triads\n(about half of the original 102) show signiﬁcant differences\nin concordance between familiar and non-familiar subjects\nand on all these triads the subjects non-familiar with the mu-\nsic perform more consistent than very-familiar subjects. All\nthese triads again show three genres in their composition.\n3.3. Multidimensional scaling\nWe used the MDS technique to represent subjects’ rankings\nin a multidimensional metrical space. We computed the de-\npendence of the stress factor (goodness of ﬁnal plot to the\noriginal matrix) on the number of dimensions. It’s common\npractice in literature [6] to consider a stress value between\n0.15 and 0.1 as an acceptable limit value providing a reliable\nﬁt. In the present experiment, the optimum compromise was\nchosen to be 3 dimensions with a stress value of 0.157.\n−2−1.5−1−0.5 00.511.52−2−1.5−1−0.500.511.52\nBlsF\nBlsS\nClsFClsSCntrF\nCntrSFnkFFnkS\nHMF\nHMSHHFHHSJzFJzSPpF\nPpS\nRckFRckS\nFigure 3. Excerpt positions in a 2-dimensional space, con-\nstructed with the ALSCAL algorithm to ﬁt perceptual dis-\ntances between excerpts (stress=0.244). Arrows and ellipses\nconnect stimuli from same genre. Genre labels: Blues (Bls);\nClassical (Cls); Country (Cntr); Funk (Fnk); Hip Hop (HH);\nHeavy Metal (HM); Jazz (Jz); Pop (Pp); Rock (Rck). Sub-\nscripts: Slow Tempo (S); Fast Tempo (F).\nAlthough the stress value in the case of 2D is too high for\na good ﬁt, we decided to show in Fig. 3 the results of the bi-dimensional MDS because of the difﬁculty of showing and\ninterpreting a 3D plot in a paper format.\nWith only 2 dimensions it is already possible to see the\neffect of genre proximity for some genres (classical, funk,\njazz, rock, hip-hop) while other genres (pop, blues, heavy\nmetal, country) appear more dispersed.\nIn order to measure the existence of signiﬁcant proxim-\nity in the stimuli depending on genre or tempo, we calcu-\nlated the Euclidean distances of the three-dimensional posi-\ntions determined as output by ALSCAL. The measure was\nconducted twice, comparing inter-tempo (songs classiﬁed\nin opposite tempo categories) with intra-tempo (songs in\nthe same tempo category) distances, and inter-genre (songs\nbelonging to different genres) with intra-genre distances\n(songs in the same genre). Signiﬁcant differences were\nfound only in the case of genre. Intra-genre distances were\nfound to be signiﬁcantly smaller than inter-genre distances.\n4. Discussion\nThe experimental results show that despite 8 outliers, sub-\njects show a common concordance on a large set of triads.\nSigniﬁcant differences in consistency between musicians\nand non-musicians occurred only in a few triads, as was\nthe case between subjects familiar and non-familiar with the\nmusic material. Nevertheless, the trend of the results on the\ntriads that show signiﬁcant differences needs some atten-\ntion. In particular it seems interesting to understand which\nintrinsic characteristic of the 5 triads in Fig. 2 make the sub-\njects’ ranking less concordant. Nearly all these triads show\nthree different genre stimuli in their composition. We think\nthis characteristic might underlie an equal spacing of stim-\nuli in a perceptual similarity space making it difﬁcult for\nsubjects to choose the most and least similar pair. The re-\nmaining triad appears consistent in this interpretative frame.\nIt is composed of two rock excerpts and one heavy metal ex-\ncerpt. If genre is one possible important dimension, in this\ncase it also would impair the subject’s ranking presenting\ntwo identical genre stimuli and one excerpt that belongs to\na closely related genre.\nWe advance the hypothesis that musicians, who per-\nform more concordantly on a fewer set of triads than non-\nmusicians, have a more common approach to music inter-\npretation than do non-trained listeners. For the case of mu-\nsic familiarity, a possible conclusion might be that subjects\nnot familiar with the music (who show better concordance\non few triads) make similarity judgments based more on the\nsurface musical audio signal rather than on associated expe-\nrience factors.\nThrough the use of a BIBD similarity ranking experiment\nand MDS of the resulting data, we have been able to repre-\nsent subjects’ “perceptual genre space”: in 3-dimensions.\nFuture work will extend this study to include a wider range\nof music.5. Acknowledgments\nWe thank Jan Engel for the statistical support and Greg\nDunn for his help running the experiment. This work is per-\nformed as part of a Marie Curie Early Stage Training grant\n(MEST-CT-2004-8201).\nReferences\n[1] Deliege I.,“Introduction, Similarity Perception - Categoriza-\ntion - Cue Abstraction”, Music Percept. , 18(3), 233-43,\n(2001).\n[2] Ockelford A., “On similarity, derivation and the cognition\nof musical structure”, Psychology of Music , 32(1), 23-74,\n(2004).\n[3] Cambouropoulos E. “Melodic Cue Abstraction, Similarity\nand Category Formation: A Formal Model”, Music Percept. ,\n18(3), 347-370, (2001).\n[4] McAdams, S. “Similarity, Invariance and Variation”, Annals\nNew York Academy of Sciences , (2001).\n[5] Chupchik G. C., “Similarity and preference judgements of\nmusical stimuli”, Scand. J. Psychol. , 23, 273-282, (1982).\n[6] Lamont A., Dibben, N., “Motivic Structure and the Percep-\ntion of Similarity”, Music Percept. , 18(3), 245-74, (2001).\n[7] Eerola T., Jarvinen T., Louhivuori J., Toiviainen P., “Statis-\ntical Features and Perceived Similarity of Folk Melodies”,\nMusic Perception , 18(3), 275-96, (2001).\n[8] Cahill M., “Melodic Similarity Algorithms - Using Similar-\nity Ratings For Development And Early Evaluation”, ISMIR\n2005.\n[9] Foote J., Cooper M., Nam U., “Audio Retrieval by Rhythmic\nSimilarity”, 265-266, ISMIR 2002.\n[10] Aloupis G., “Algorithms for Computing Measures of\nMelodic Similarity”, Proc. 15thCanadian Conference on\nComputational Geometry , Halifax, 81-84, 2003.\n[11] Allamanche E., Herre J., Hellmuth O., Kastner T., Ertel C.,\n“A multiple Feature Model for Musical Similarity Retrieval”,\n265-266, ISMIR 2003.\n[12] Berenzweig A., Logan B., Ellis D.P.W., Whitman B., “A\nLarge-Scale Evaluation of Acoustic and Subjective Music\nSimilarity Measures”, ISMIR 2003.\n[13] Kendall M., “Rank Correlation Methods”, Charles Grifﬁn,\nLondon (1975).\n[14] Young F. W., Lewyckyj R., “ALSCAL User’s Guide ( 5th\nEd.)”, L.L. Thurstone Psychometric Laboratory, University\nof North Carolina, Chapel Hill, NC (1996).\n[15] Kruskall J. B., “Multidimensional Scaling, Quantitative Ap-\nplications in the Social Sciences” (Sage Publ. 1983)"
    },
    {
        "title": "PAPA: Physiology and Purpose-Aware Automatic Playlist Generation.",
        "author": [
            "Nuria Oliver",
            "Lucas Kreger-Stickles"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1416794",
        "url": "https://doi.org/10.5281/zenodo.1416794",
        "ee": "https://zenodo.org/records/1416794/files/OliverK06.pdf",
        "abstract": "In this paper we present PAPA, a novel approach for au- tomatically generating playlists. The proposed framework utilizes the user’s physiological response to music, together with traditional song meta-data to generate a playlist the user will not only enjoy, but which will assist him or her in achieving various user-defined goals (“purpose”). In ad- dition to outlining the generic framework, we present an ex- emplary application named MPTrain that (1) creates a playlist in real-time to assist users in achieving specific exercise goals; and (2) incorporates the user’s physiological response to the music to determine the next song to play. Keywords: Automatic Playlist Generation, Physiological Mon- itoring, User Modeling",
        "zenodo_id": 1416794,
        "dblp_key": "conf/ismir/OliverK06",
        "keywords": [
            "Automatic Playlist Generation",
            "Physiological Monitoring",
            "User Modeling",
            "Exercise Goals",
            "Real-time Playlist Creation",
            "Song Meta-Data",
            "Users Physiological Response",
            "PAPA Framework",
            "MPTrain Application",
            "Music-Assisted Exercise"
        ],
        "content": "PAPA:Physiology and Purpose-AwareAutomaticPlaylist Generation\nNuriaOliver\nMicrosoft Research\nOne MicrosoftWay\nRedmond, WA 98052, USA\nnuria@microsoft.comLucasKreger-Stickles\nMicrosoft Research\nOne Microsoft Way\nRedmond, WA 98052\nlucasks@cs.washington.edu\nAbstract\nIn this paper we present PAPA, a novel approach for au-\ntomatically generating playlists. The proposed framework\nutilizes the user’s physiological response to music, toget her\nwith traditional song meta-data to generate a playlist the\nuser will not only enjoy, but which will assist him or her\nin achieving various user-deﬁned goals (“purpose”). In ad-\nditiontooutliningthegenericframework,wepresentanex-\nemplaryapplicationnamedMPTrainthat(1)createsaplayli st\ninreal-timetoassistusersinachievingspeciﬁcexerciseg oals;\nand(2)incorporatestheuser’sphysiologicalresponsetot he\nmusictodetermine thenext songtoplay.\nKeywords: AutomaticPlaylistGeneration,PhysiologicalMon-\nitoring, User Modeling\n1. Introductionand Related Work\nIn recent years, there has been increasing interest in the\nautomatic generation of playlists, partly due to the broad\nadoptionofdigitalmusicandpersonaldigitalmusicplayer s.\nCommon approaches to creating playlists range from ran-\ndomly shufﬂing a collection ( e.g.iPod shufﬂe) to manually\ncreatingtheplaylistbyselectingtheorderofthesongs. Th e\nﬁrst approach does not give any control to the user in terms\nof which songs will be selected. The second approach, on\nthe other hand, gives the user complete control over the se-\nlection, but is typically time consuming, tedious, static a nd\npotentially hard to implement when the music collection is\nlarge (e.g.tens of thousands of songs). Moreover, as it en-\ntirelyreliesontheuser’sknowledgeofhisorhercollectio n,\nit does not allow for the discovery of “forgotten” songs in\nthe collection.\nConsequently, there have been multiple efforts in the re-\nsearchcommunitytoassistuserscreatingplaylists. Mosto f\nthepreviousworktodatehasfocusedonefﬁcientalgorithms\nto automatically create a playlist that satisﬁes some given\nconstraints [2, 3, 1], and on collaborative ﬁltering [5, 6] a l-\ngorithms. Typically, the user provides the system with a\nseedsongfortheplaylisttobegeneratedandthealgorithms\nPermissiontomakedigitalorhardcopiesofallorpartofthisw orkfor\npersonal or classroom use is granted without fee provided th at copies\narenotmadeordistributedforproﬁtorcommercialadvantagean dthat\ncopies bearthis notice andthefull citation ontheﬁrst page .\nc/circlecopyrt2006University ofVictoriaﬁnd similarsounding songs.\nTherearetwoprojectsthatareparticularlyrelevanttothe\napproach described in this paper: (1) The PATS system [8],\nwhere the authors introduce the concept of context-of-use\nas an important input variable when automatically creating\na playlist. The context-of-use is deﬁned in [8] as the real-\nworld environment in which the music is heard, being it a\nparty, a romantic evening or traveling in the car or train ;\nand (2) the HPDJ system [4], which uses sensors to deter-\nminephysicalandphysiologicalresponsesofacrowdtothe\nmusicandusesthisfeedbacktoautomaticallysequenceand\nmix the music in nightclubs. The PATS system is similar to\nthe approach proposed in this paper in that it takes into ac-\ncount the context-of-use as an input variable when creating\na playlist. The HPDJ system shares with our approach the\naddition of real-time biofeedback to inform the song selec-\ntionalgorithms.\nIn this paper we combine information about the user’s\nphysiological response and purpose in order to automati-\ncallygenerateplaylists. WerefertothisframeworkasPAPA ,\nbecause our approach takes into account both the purpose\nand the physiological response of the user to the music.\nWhereas other systems employ context in song selection so\nastobetterﬁndanastheticallyappropriatesong,webeliev e\nwearetheﬁrsttoproposeutilizingthemusic’spotentialim -\npact not only on the user’s enjoyment, but also on helping\nhim/her achieve aparticular goal.\nWe illustrate our approach by means of an application\nnamed MPTrain that (1) selects the music to assist users\nin achieving speciﬁc exercising goals; (2) incorporates th e\nuser’s physiological response to the music to determine the\nnext songtoplay.\nThepaperisorganizedasfollows: InSection2wepresent\nPAPA,theframeworkforautomaticallygeneratingplaylist s.\nNext, Section 3 is devoted to describing the MPTrain sys-\ntem, an exemplary application of the proposed framework.\nFinally,Section4containssomeconclusionsandfuturelin es\nof work.\n2. Automatic Generation of Playlists from\nPurposeand PhysiologicalResponse\nFigure 1illustratesthebasic components ofPAPA.\nTypically, the user (1) is listening to music from his per-\nsonaldigitalmusiclibrary(DML)(2)bymeansofaportable\ndigital music player. Let the DML consist of a collection ofNsongs, {S1,S2,...,S N}. In the Figure, the portable de-\nvice is a mobile phone with MP3 playback capabilities (3).\nThesystemhasaccessto: (a)theuser’sproﬁle(10)contain-\ningpersonalinformation, Pu,suchastheuser’sage,weight,\ngender, and musical preferences, together with (b) histori c\ndata in the form of logs of previous interactions with the\nsystem, Hu.\nThe user also wears a set of Kphysiological and envi-\nronmental sensors (2), such as sensors for determining the\nuser’s heart-rate, galvanic skin response, respiration ra te,\nmovement,position,etc. Let Fu={fu\n1,fu\n2,...,fu\nK}denote\nthe user’s physiology and environmental data, where fu\niis\nthefeaturevectorforsensor i,withi= 1,...,K. Thesesen-\nsorscontinuously senddata wirelesslytothemobile phone.\nThe system takes into account the context in which the\nuser is listening to the music ( e.gat work, in the car, at\nhome, with friends, etc) together with the purpose of lis-\nteningtothemusic( e.g.relaxing,concentrating,exercising,\ndriving,etc.) (4). Thecontextandpurposecanbeexpressed\nasafunction Goftheuser’s: (a)real-timephysiologicaland\ncontextual data, Fu; (b) proﬁle, Puand (c) historic data,\nHu. The user’s DML is augmented with relevant metadata\nMs(5),suchasthesongs’tempo,averageenergy,duration,\ngenre,etc. Let Msi={msi\n1,msi\n2,...,msi\nL}denotethesetof\nLfeatures extracted fromsong i, with i= 1...N.\nThe system utilizes the user’s biofeedback and explicit\nfeedback (7) to learn a model of the set of features in the\nmusic, Ms–e.gtempo,averageenergy,etc–andtheuser’s\nresponsetoit, Fu–e.g.increasedheart-rate,decreasedres-\npiration rate, etc. The system will select the next song to\nplay based on the deviation between the desired goal Gand\nthe user’scurrent state {Fu,Pu,Hu}.\nForexample,letusassumethattheuser’sgoalistorelax.\nThisgoalcorrespondstocertainvaluesintheuser’sphysio l-\nogy,e.g.low heart-rate, galvanic skin response, respiration\nrateandmovement. Theuserstartslisteningtotheﬁrstmu-\nsicalpieceasselectedbythesystem. Astimeprogresses,th e\nsystemconstantlymonitorstheuser’sresponsetothemusic\nand uses this information to select the next “optimal” song\nto play. If, for example, the user’s current physiology does\nnot correspond to the system’s model of a “relaxed” state,\nthe system will select a more “relaxing” song to play next.\nThe model of the user’sstateislearned from data.\nNote that there are two different mappings that need to\nbeestimatedfromdata: (1)Themapping Map MUbetween\nmusicalfeatures Msandtheuser’sstate (Fu,Pu,Hu);and\n(2)themapping Map GUbetween thegoal Gand theuser’s\nstate(Fu,Pu,Hu). Thosetwomappings,togetherwiththe\nuser’sproﬁle( Pu)andhistoricdata( Hu)arepartoftheuser\nmodel (6).\nFinally,therearetwoimportantaspectsinevaluatingthis\nparadigm: (1) How well the music helps users in achieving\ntheirdesiredgoal,and(2)howenjoyablethemusicselectio n\nistothe user,given the context.\n\u0000\u0001 \u0002\n\u0003\u0004\u0005\n\u0003\u0001 \u0006 \u0007\b\n\u0006 \t \u0001 \n \u0004 \u0001\n\u000b\f \r\n\u0003\u0004 \u0002\n\r\u0002 \u000e\n\u0003\u0001\n\u0003 \u000f\u0004 \u0010\b\n\n\r\u0011\u0012\u0013\n\u0014\u0015\u0016\n\u0017 \u0018\u0019 \u001a\u0015\n\u0018\u001b\n\u0017\u0018\u0019\u001c\u0016\n\u0013 \u001d \u001b\n\u0017\u001e\n\u0017 \u0018\u0019\u001f \n\u0018\u001b \u0013\n\u0018!\u001d \u0015\n!\n\u0017\u0018\u0019\n\u001a\u001d\n\u0017\"\n\u0017 \u0018\u0019\n# \r\u000e\n\r\u0003$\n%\u0010\b\n\n\r\u0011\f \r &\u0006 $ \u0006' (\n#\u0010\n\f )*+ , - . /\n01 2 1 0 1\nUser \n\u0007\n\u000f'\n\n\r\u0001\n%\u0001 \u000e\n\r\u0011 $\n%$ \u0002\n34\u00025\n\r\u0006 \u0001 6 \u0004 \u0002\n\u0003$\n%7\u0004 \u0002 \n \u0001 \u0006 \n8 \r\u0001 $ \u0002\n3 9\n \u0004 \u0006:\u0004 \u0004\n3 &$ \u0011\n;\n7\u0004 $ \u0006 \u0011\n\u000f\n\u000b\u0001 \u0006<\t\n\u0003\n\r6 $\n%\n7\u0001 \u0002 \u000e\n\u0003\u0001\u0007\n%$' =\n\u0004\u0005\n\u0003=\n\u0004\u0005\n\u0003\n7\u0001 \u0002 \u000e\n\u0003\u0001\u0007\n%$'\n9\n \u0004 \u0006 \u0010 \u0001\n3\u0004\n%\u0010 \u0001\n& \r %\u0004 \u0007\n\u000f\u0001 \u0002 \u0004>\n\r\u0003\u000f\u0010 \u0007\n?\t\n%$'\n&$ \u0011\n;@A B\n@ C B\n@ D B\n@E\nB@F\nB@G\nB\n@ H B\n@ I B@ J B\n9\n \u0004 \u0006\nK\n \u0007 \u0006 \u0001\n\u000b\r %\u0004L\u0013 \u001d \u001e  \n\u0018\u0015\n\u0014M \u0018 N \u001d O \u0015\n!\n\u0017 \n\u0018 P\n\u0017\u001e\n! \u001d\n\u0017\u001b\u001a\u0015\n!\u0015\n@ A Q B\nFigure 1. PAPA: Physiology and Purpose-Aware Playlist Gen-\neration.\nWe shall illustrate the proposed approach with a proto-\ntypenamedMPTrainthatcreatesaplaylisttoassistusersin\nachieving speciﬁc exercise goals.\n3. MPTrain: Automatic PlaylistGeneration\nfor Optimal ExercisePerformance\nMPTrain is a mobile phone based system that takes advan-\ntage of the inﬂuence of music in exercise performance en-\nabling users tomoreeasily achieve theirexercise goals.\nMPTrain is designed as a mobile and personal system\n(hardware and software) that users wear while exercising\n(walking,joggingorrunning). MPTrain’shardwareinclude s\na continuous heart-rate and acceleration monitor wireless ly\nconnectedtoamobilephonecarriedbytheuser. MPTrain’s\nsoftware allows the user to enter a desired workout in terms\nofdesiredheart-ratestressovertime. Itthenassiststheu ser\nin achieving the desired exercising goals by: (1) constantl y\nmonitoringhis/herphysiology(heart-rateinnumberofbea ts\nper minute) and movement (speed in number of steps per\nminute); and (2) selecting and playing music (MP3s) with\nspeciﬁc features that will guide him/her towards achieving\nthe desiredexercising goals.\nMPTrainusesalgorithmsthatlearnthemappingbetween\nmusicalfeatures( e.g.beat),theuser’scurrentexerciselevel\n(e.g.runningspeedorgait)andtheuser’scurrentphysiolog-\nical response ( e.g.heart-rate). The goal is to automaticallychoose and play the “right” music to encourage the user to\nspeed up, slow down or maintain their pace while keeping\nhim/her ontrack withthedesired workout.\nFigure 2 illustrates MPTrain’s data ﬂow. The user is lis-\ntening to digital music on his/her mobile phone while jog-\nging. At the same time, the user’s hear-rate and speed are\nmonitored and stored on the mobile phone. A few sec-\nonds (typically 10s) before the conclusion of the song cur-\nrently being played, MPTrain compares the user’s current\nheart-rate with the desired one according to the current pre -\nselectedworkout. Theuser’smodeliscomposedoftwoele-\nments: (1) The next action module , which determines if the\nuserneedstospeedup,slowdownorkeeptheirpaceofjog-\nging, based on whether his/her heart-rate needs to increase ,\ndecrease or stay the same. With this information, (2) the\nmusic ﬁnding module identiﬁes the next song to be played\nfromthemusicdatabase. Thecurrentimplementationofthe\nmusic ﬁnding algorithm determines the next song by iden-\ntifying a song (1) that hasn’t been played yet; and (2) has a\ntempo (in beats per minute) that is similar to the user’s cur-\nrent gait, but increasing or decreasing an amount inversely\nrelatedtothedeviationbetweentheactualheart-rateandt he\ndesired heart-ratefrom thepreset workout.\nAtanyinstantoftime,theusercancheckhowwell(s)he\nis doing with respect to the desired exercise level, mod-\nify the exercising goals or change the music track from the\none automatically selected by MPTrain. We are currently\nworkingonincludingadditionaluseradaptationtoMPTrain\nby incorporating information about that user’s past sessio ns\nandbyconstantlymonitoringtheuser’sactionsandlearnin g\nfrom them. As the user interacts with the MPTrain system,\nwe would like for its music selection algorithm to become\nprogressively better suitedfor that particular user.\nWereferthereaderto[7]foradetaileddescriptionofthe\nMPTrain system.\n3.1. AutomaticPlaylist Generation\nMPTrainactsasapersonaltrainerthatusesmusictoencour-\nage the user to accelerate, decelerate or keep the running\nspeed. The key element is that music improves gait regu-\nlarity due to the use of the beat, which helps individuals to\nanticipate the desired rate of movement [9]. The rhythmic\nstructure of the music and the rhythmic actions performed\nby the body are believed to combine and sychronize. We\nshall describe next MPTrain’s algorithm for selecting the\nmusictoplay.\nInitscurrentimplementation,MPTraindoesnottakeany\naction until one of these threeconditions aretrue:\n1. Thereareafewsecondsleft( e.g.10)beforetheendof\nthecurrentsong : Inthiscase,MPTraindetermineswhether\nthe user needs to increase, decrease or keep their running\npace,bycomparingtheuser’scurrentheart-ratewiththede -\nsired heart-rate from the desired workout for that day. Once\nit has determined the action to take, it searches the user’s\nRS T\nUV S\nW XY V\nZY[\n\\] ^_ `\na b c^ d e fg h i g j\nd dk lm n\n^ lo\nlmp p\nk cg\nq eg m\nf d fm\ndg\nrs tu v\nwx y z { | } ~\n~\n\nx\n{ {\n\n \nx x }\n~ \n\nz\n\n| ~ y z  x\n\nx |\n{\n  x \n{ y x\nwx yz { | z  \n    \n  \n\n \n \n  \n    \n¡¢ £  \n¡\n¤  \n   \n¡   \n ¥\n    \n  \n ¦ §\n¡ \n¨   \n  \n©\nª«\nª\n¬\nª\n­ ®¯ ° ± ¯ °² ³ ´ vµ¶v v· ´ ¸u v ´ · v v ±­· v v ± ³ · ³ ´ v µ\n¹ º»\nº \n¨  ¢\nº¼¹\n\n½ ¢ \n§\n¡ \n¨   \n  ¾¿À\nÁ ÂÃ Ä Å\nÁÃ Æ ¿ Ç\nÈÉÊ\n¿\nË\nÌSÍ\n\\ ÎÏ\n\\UY Ð Ñ Y\nW[\nÒS\nÓt² ± ¯ ·\nstu ¸\n®´\n®¯ ° vµ ´ ¯ ² Ôt² Õ v ¸\ns´ · v µ u\nt² ³\nsvÓt² ± ¯·\nstu ¸\n®´\ntu\nt\n®¸µ ´ ¯² Ôt² Õ v ¸\ns´ · vµ u\nt² ³\nsvÓt² ± ¯ ·\ns tu ¸\n® Ö¸ ´\nsvµ ´ ¯² Ôt² Õ v ¸\ns´ · vµ u\nt² ³\nsvÑ[\nT\nUÏ ×\nUÐ\nW UÐ ØÑ Y\nW[\nÒS\n\nÙÚ Û\nÜÝÞ\nÜÚ ß à Ú á\nâãä Ý ß å Ú\næÝ\nâãä Ý ß\nçäèÝ Ý\næ éá ê\në\nFigure2. MPTrain’s dataﬂow.\ndigital music library (DML) for the optimal song to play.\nNotethattheDMLcontainsnotonlytheuser’spersonalcol-\nlection of MP3 songs, but also additional information about\neachsong,suchasitsbeat1andaverageenergy. Depending\nonthesituation,MPTrainwilllookforasongwhosebeatis\nsimilar,higherorlowerthanthatofthesongcurrentlybein g\nplayed, according to the difference between the actual and\ndesired heart-rates. In the event that the workout target is\nabout to change, MPTrain selects a song appropriate to the\nnextworkout target.\nFor example, if the user’s current heart-rate is at 55%of\nthemaximumheart-ratereserve2,butthedesiredheart-rate\nat that point is at 65%, MPTrain will ﬁnd a song that has\nfasterbeatthantheonecurrentlybeingplayed. Theincreas e\nin beat in the song is proportional to the percentage of error\nbetween the user’sactual and desired heart-rate.\n2. There is a discontinuity in the desired workout pat-\ntern, such as moving from a warming-up (about 60%of\nmaximum heart-rate reserve) to a weight management sec-\ntion (about 70%of maximum heart-rate reserve) in the de-\nsiredworkout. Inthiscase,MPTraininterruptsthesongtha t\nis currently playing, unless the song has been playing for a\n1Determined automatically ormanually.\n2Heart-rate reserve is deﬁned as the user’s maximum heart-rate minusthe user’s resting heart-rate.very short time ( e.g.less than 20s)3. MPTrain then selects\nthe next song toplay asdescribed incase (1)above.\n3. The user explicitely requests a change of song . In this\ncase,MPTrainselectsadifferentsongfromtheDMLwhose\nfeatures stillsatisfytheconstraints given the situation .\nMPTrain’s current implementation uses an empirically\nlearned function to map the physiological response to the\nmusic’s beat. This model is used to make a statistically ac-\ncuratetrackselection. Furtherversionswillalsoincorpo rate\ninformation about the user’s past performance and speciﬁc\nresponse toeach song.\n3.2. Evaluation\nWe are currently carrying out a comprehensive user study\nwith20runners. Thestudyconsistsofthreetofourrunning\nsessions. In each of the sessions the runners know what is\nthe desired exercise pattern for the session and their heart -\nrate and speed are monitored. The ﬁrst running session is\nwithout music ( mutecondition); the second one with ran-\ndom music from the digital music library that is stored in\nthe phone ( randomcondition); and the third one with the\nmusic as it is selected by the MPTrain system ( MPTrain\ncondition)andthefourthandoptionalﬁnalsessioniswitha\nMetronome as selected by thesystem.\nAll runners ﬁll out a pre-run questionnaire and a post-\nrun questionnaire after each of the sessions. With the study\nwe plan to evaluate, for each of the sessions: (1) how well\neachrunnerachieves thepre-deﬁned exercisegoal;(2)thei r\nperception of the workout; (3) their level of enjoyment of\nthe music selected by MPTrain, especially when compared\ntothe restof theconditions.\nBeforedeployingthecomprehensiveuserstudy,wehave\nextensively tested MPTrain with two runners. The results\nhavebeenverypositiveandencouraging. Thefocusofthese\nsuiteoftestshas been toevaluate and reﬁneMPTrain’s mu-\nsic selection algorithms. In our tests, the Digital Music Li -\nbrary contained up to 57songs with durations and tempos\nranging from 2:02 to 5:55 minutes and 65to185beats per\nminute,respectively. Thesongsbelongedtoavarietyofmu-\nsic genres ( e.g.pop, rock, soul, hip-hop, etc), both instru-\nmentalandvocal. NotethatMPTrain’smetadataabouteach\nsongincludesthetempoandenergyofthesongin 20swin-\ndowintervalsandtheaveragetempoandenergy. Themusic\nselection algorithm described in MPTrain uses the song’s\ntempo.\nIn the outdoor running experiments that we have carried\nout so far, the runners were able to achieve a workout sig-\nniﬁcantly more similar to the desired workout when using\nMPTrain than in the muteandrandomconditions. We have\nalso observed a learning curve phenomenon. It typically\ntook the runners a few sessions to get used to both the mu-\nsicintheDMLandtoMPTrain’sstyleof“coaching”. Inall\ncases,theusersbecamebetteratachievingthedesiredwork -\n3Inthis case asong hasalready beenselected based onthenew t arget.out as their experience with the system was increased. Fi-\nnally,therunnersreportedthat(1)runningwithmusicmade\ntheir workout more enjoyable than running without it; (2)\nrunning with MPTrain’s music selection was not only fun,\nbut alsoanefﬁcient way toachieve theirworkout goal.\n4. Conclusionsand FutureWork\nWe have presented a novel approach to the automatic gen-\neration of playlists that incorporates the user’s physiolo gi-\ncal response and purpose in the music selection algorithms.\nWe have illustrated our approach with MPTrain, a mobile\nphoned based system that utilizes the user’s biofeedback to\nselectthe“right”musictoplaytoassisttheuserinachievi ng\naspeciﬁc exercise goal.\nIn addition to ﬁnishing the user study and analyzing the\ndatafromthestudy,thereareseverallinesoffutureresear ch\nthat we would like to pursue with the proposed approach,\nincluding (1) exploring other domains where our approach\ncould be applied; (2) enriching the user’s model by adding\ninformation about the user’s past interactions with the sys -\ntem to enhance the automatic playlist generation algorithm ;\n(3) incorporating new musical features to the music selec-\ntion algorithm, such as the song’s average energy and the\nvolume at which is being played; (4) altering the speed and\nvolume with which each song is being played, to better di-\nrectuserstowardsachievingtheirgoal;(5)incorporating ad-\nditionalcontextualinformation,suchasGPSdata,bodyand\nexternal temperature, barometric pressure to determine in -\ncline, etc.; and (6) working on different user interfaces to\nallowuserstoratetheplaylist,reviewtheirpastinteract ions\nwith the system, and identify trends and deviations from\nthose trends. We would also like to include lifestyle vari-\nables, such as diet, overall mood, stress levels, etc, and ﬁn d\ncorrelations between them and the efﬁcacy of the system,\nboth in terms of how well it helps users achieve their goal\nand how enjoyable the musicselection is.\nReferences\n[1] http://www.gracenote.com/gn products/playlists.html.\n[2] M. Alghoniemy and A. Tewﬁk. A network ﬂow model for\nplaylistgeneration. In Proc.ICME’01 ,2001.\n[3] M.AucouturierandF.Pachet. Scalingupmusicplaylistgen-\neration. In Proc. ICME’02 ,2002.\n[4] D. Grahan-Rowe. Computer dj uses biofeedback to pick\ntracks. http://www.newscientist.com/article.ns?id=dn1563,\n2001.[5] D. Hauver and J. French. Flycasting: using collaborative\nﬁltering to generate a playlist for online radio. In Proc. Int.\nConf.onWebDeliveringofMusic ,pagespp.123–130,2001.\n[6] C. Hayes and P. Cunningham. Smart radio: Building music\nradio on the ﬂy. In Proc. SGES Conf. on Knowledge based\nSystems and AppliedArtiﬁcialIntelligence , 2000.\n[7] N. Oliver and F. Flores-Mangas. Mptrain: A mobile, mu-\nsic and physiology-based personal trainer. In Proc. Mobile-\nHCI’06(ToAppear) ,2006.\n[8] S. Pauws and B. Eggen. Pats: Realization and user evalua-\ntion of an automatic playlist generator. In Proc. ISMIR’02 ,\n2002.[9] M. Staum. Music and rhythmic stimuli in the rehabilitation\nofgaitdisorders. JournalofMusicTherapy ,20:69–87,1983."
    },
    {
        "title": "MusicRainbow: A New User Interface to Discover Artists Using Audio-based Similarity and Web-based Labeling.",
        "author": [
            "Elias Pampalk",
            "Masataka Goto"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1417313",
        "url": "https://doi.org/10.5281/zenodo.1417313",
        "ee": "https://zenodo.org/records/1417313/files/PampalkG06.pdf",
        "abstract": "In this paper we present MusicRainbow which is a simple in- terface for discovering artists where colors encode different types of music. MusicRainbow is based on a new audio- based approach to compute artist similarity. This approach scores 15 percentage points higher in a genre classification task than the similarity computed on track level. Using a traveling salesman algorithm, similar artists are mapped near each other on a circular rainbow. Furthermore, we present a new approach of combining this audio-based information with information from the web. In particular, we label the rainbow and summarize the artists with words extracted from web pages related to the artists. We use different vocabular- ies for different hierarchical levels and heuristics to select the most descriptive labels. We conclude with a discussion of the results. The first impressions are very promising.",
        "zenodo_id": 1417313,
        "dblp_key": "conf/ismir/PampalkG06",
        "keywords": [
            "MusicRainbow",
            "simple interface",
            "discovering artists",
            "colors encode different types of music",
            "audio-based approach",
            "genre classification task",
            "traveling salesman algorithm",
            "web information",
            "labeling rainbow",
            "summarizing artists"
        ],
        "content": "MusicRainbow: A New User Interfaceto DiscoverArtists\nUsing Audio-based Similarity and Web-based Labeling\nEliasPampalk and MasatakaGoto\nNationalInstituteofAdvancedIndustrialScienceandTechnology(AIST)\nIT,AIST,1-1-1Umezono,Tsukuba,Ibaraki305-8568,Japan\nAbstract\nInthispaperwepresent MusicRainbow whichisasimplein-\nterface fordiscoveringartists where colorsencodedifferenttypes of music. MusicRainbow is based on a new audio-\nbased approach to compute artist similarity. This approach\nscores 15 percentage points higher in a genre classiﬁcationtask than the similarity computed on track level. Using a\ntravelingsalesmanalgorithm,similarartistsaremappednear\neach other on a circular rainbow. Furthermore, we presenta new approachof combining this audio-based information\nwith information from the web. In particular, we label the\nrainbowandsummarizetheartistswithwordsextractedfromwebpagesrelatedtotheartists. We usedifferentvocabular-\nies for different hierarchical levels and heuristics to select\nthe most descriptive labels. We conclude with a discussionoftheresults. Theﬁrst impressionsareverypromising.\n1. Introduction\nDeclining production costs have made it much easier for\nartists to produce their own music without the support of\nrecord labels. Furthermore, the Internet enables artists toeasily reach out to their audience. However, at the same\ntime the competition for attention has become harder. Mu-\nsic listeners are confronted with an abundance of choices.The work presented in this paper aims at supporting listen-\ners indiscoveringartists theymightnotdiscoverotherwise.\nThere are many ways to discover artists, including, for\nexample, reading reviews and recommendations, browsing\nlists of similar artists (e.g. amazon.com or allmusic.com),\nparticipating in community networks (e.g. myspace.com),orlisteningtopersonalizedInternetradio(e.g. last.fm).\nIn contrast to these existing approaches,we use content-\nbasedanalysisto computeartistsimilarity. Inparticular,wedonotusecollaborativeﬁlteringormanuallyannotateddata.\nContent-based approaches have the advantage that they are\nnot biased by popularity (unlike collaborative ﬁltering) andare much cheaper than manual annotations. Furthermore,\nwedonotassumethattheusershavespeciﬁcartistsinmind\nwhen they start their exploration. This is in contrast to sys-\nPermission to makedigital orhardcopies ofall orpart ofthis workfor\npersonal or classroom use is granted without fee provided that copies\nare notmadeordistributed forproﬁtorcommercial advantage andthat\ncopies bear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2006 University of Victoria\ntemssuchasliveplasma.comwhichrequiretheusertoenter\nanartist’s namebeforetheyrecommendsimilarartists.\nOur primary objective is to support users in discovering\nnew artists. Given a large music collection with many dif-\nferent styles of music, the users should be able to quickly\nget an overview of the contents. The secondary objectiveis to keep the interface as simple as possible. Usage of theinterface should be intuitive and require no training. The\ninterface should be implementable on mobile devices with\nlimitedscreensizes includingphonesandmusicplayers.\nInthispaperwepresentanewmusicinterfacetodiscover\nartists. Artists are arranged on a circular rainbow (with nobeginning or end, see Figure 1). Similar artists are located\nclose to each other on the rainbow. The similarity is com-\nputedbyanalyzingtheaudiosignals. Therainbowislabeledautomaticallywithwordsoccurringonwebpagesrelatedto\nthe artists. The web pages are found using Google and ﬁl-\nteredusingspeciﬁcvocabularies. Therainbowconsists of8concentricringshavingdifferentcolorsfrompurple(inside)to red (outside). Each color represents a different style of\nmusic. Ifacolorshinesbrighterinasegmentoftherainbow\nthen this indicates that the respective style of music can befound there. Furthermore, we apply audio summarization\ntechniques so that the users can quickly get an overview of\neachartist.\nAs input device we use a Grifﬁn Powermate knob (see\nFigure1). Alternatively,similarinterfaces(suchasthewheelused for iPods) could be used. The user interacts with the\nvisualization by rotating the rainbow (by turning the knob)\nandselectinganartist tolisten to(bypushingtheknob).\n2. Related Work\nIn [1] an approach was presented with the same objectivesand using similar techniques to describe artists and groupsof artists with words. In contrast to this paper, the artists\nwere organized using a hierarchical tree structure, and nei-\ntheraudio-basedanalysisnorvisualizationswereused.\nIn[2]asimilarapproachwas usedtomapmusictoacir-\ncle and a wheel was proposed as input device. One of thedifferences is that we do not aim at playlist generation. In\nparticular,insteadofmappingtracksindividually,we group\nthem by artists. Furthermore, we use a new visualizationmetaphor, describe artists using words and audio thumb-\nnails, and label the sections of the circle to help users ﬁnd\ninterestingsectionsmoreeasily.Figure 1. The MusicRainbow interface. The input device (Grifﬁn Powermate knob) is shown in the upper left. On the right\nside is a screenshot where Outkast is selected. The picture in the lower left shows how the colors and labels change when the\nrainbow(shownontheright)is rotatedcounter-clockwiseby17degrees.\nOverall, there are a number of related approaches. Re-\nlated user interfaces include, for example, the work pre-\nsented in [7] which helps users discover and enjoy musicusing audio-based similarity. Approaches with a stronger\nfocus on visualizations(e.g. using maps to displaythe con-\ntents of a collection) include, for example, the work pre-sented in [3–5]. Approaches without a particular focus on\nvisualization include,for example, work using user proﬁles\nand information on the web to make recommendations [6].Furthermore, related work includes automatic playlist gen-eration(e.g. [8–12]).\n3. User Interface\nFigure 1 shows the MusicRainbowinterface. Insidethe cir-cularrainbowarehigh-levelgenretermswhichdescribethedifferent sections (e.g. Rap, Jazz, Soul). More speciﬁc\nterms(e.g. Female,Guitar,Beats)arelocatedoutside. Each\nringoftherainbow(andthuscolor)correspondstoonehigh-level term. In sections where the terms are frequently used\nto describe the artists, the colors are brighter. For example,\ninthescreenshotoneoftheyellow-greencolorscorrespondsto Rap and red corresponds to Rock. (However, this infor-mationis notgivenexplicitlytotheuser.)\nByturningtheknob,theuserrotatestherainbow(andla-\nbels). There is no linear mapping between positions on the\nknob and the rainbow. If the knob is turnedfaster, the rota-tion step size is larger, allowing the user to quickly reach adifferentregionontherainbow. Iftheknobisturnedslowly,\nthestepsizeis smallallowingtheusertomovebetweenad-\njacentartists ontherainbow.\nInthecenteroftherightsideisthenameofthecurrentlyselected artist. In the screenshot(Figure 1, right side), Out-\nkast is selected. Artists in the neighborhood include, for\nexample,Eminem,Jay-Z,BustaRhymes,andWarrenG.Atthe bottom right, the selected artist is described with some\nwords. ForOutkast thefollowing(automaticallygenerated)\ndescriptionisdisplayed: “rap,singing,southern,duo,beats”.(NotethatOutkastis aduofromAtlanta.)\nFurthermore, below the description, 8 color bars (corre-\nspondingto the colors ofthe rainbow)are displayed. These\ncolorbarshelptheuserunderstandwhichcolorscorrespond\ntowhichtypeofmusic. Forexample,forOutkasttheyellow-greencolorhasthelargestbar. Thisindicatesthatotherseg-mentsoftherainbowwherethiscolorishighlightedcontain\nsimilar artists. (In contrast to the rainbowcolors, which are\nsmoothedacrosssegments,thesecolorbarsarenotsmoothed.They only describe the respective artists, and thus do not\nnecessarilycorrespondtothecolorsofthecurrentsegment.)\nThe labels assigned to the sections of the rainbow are\nmisleading in some cases. For example, a section labeled\n“Female” can include a large number of male artists. Toavoid user frustration,we haveimplementedtwo strategies.\nFirst, the labels ﬂoat alongside the rainbow making it less\nobvious to which segments they apply exactly. Second, therelevant labels for the currently selected artist are always\nhighlighted. Thisisintendedtohelptheuserunderstandthat\nthe labels which are closest to an artist do not necessarilyapply. In the case of Outkast, the labels “Rap” and “Beats”arehighlighted.\nBy pushing the knob, a song from the currently selected\nartist is played. Ifthe userpushes thebuttonagain,the next\nsongisplayed. Fromeachsong,onlyasegmentofabout20secondslengthisplayed. Toidentifythesegmentsthatsum-\nmarizethesongaswellaspossible,weuseachorus-sectiondetection method (RefraiD [13]). The song continues un-\ntil thesectionis overortheuserselectsa differentartist (by\nturningtheknobandpushingit). Pressingtheknobformorethan2secondsstopsthe music.\nAll inputs from the user can be given using, for exam-\nple, a Grifﬁn Powermate knob. There are basically 3 inputactions. (1) By turning the knob quickly the user can jumpto differentregionson therainbow. (2)By turningtheknob\nslowly the user stays in the same region and can explore\nsimilar artists. (3) By pushing the knob the user can selectanartisttolistento. Thecurrentlyplayingsongisdisplayed\nintheupperright. Byturningtheknobduringplayback,the\nusercanreadthesummariesofotherartists.\n3.1. ImplementationandData\nMusicRainbowisbuiltwithProcessing(processing.org).The\ncomputationsdescribedinthenextsectionwereruninMat-\nlab. For the experiments we used a collection of 15336tracks from 558 artists. This collection is a subset of the\nDB-XL collectiondescribedin[14].\n4. Method\nFirst,wecomputethesimilaritiesfortheartistandmapthem\nto the circular rainbow. Second, we mine the web to ﬁnd\nwords to label the rainbow. Finally, based on these words\nwe computethecolorsoftherainbow.\n4.1. MappingArtists totheRainbow\nTo compute the similarity of artists, we ﬁrst compute the\nsimilarity of tracks using the approach described in [14] asG1C which is based low-level audio statistics. G1C is acombinationofspectralsimilarityandinformationextracted\nfromﬂuctuationpatterns.\nGiventhesimilarity of tracks,we computethe similarity\nofartistsasfollows. First,thesimilaritiesarenormalizedso\nthat for each song the sum of similarities to all other songs\nequals1. Second,let s(t,B)bethesimilarityoftrack ttothe\nmost similar track from artist B. We then compute the sim-ilarity s(A,B)of artists A to B as the average of all s(t,B)\nover all tbelongingto A. Finally, we enforce symmetry by\nsetting s(A,B)totheminimumof s(A,B)and s(B,A).\nTo evaluate the performance of this approach, we use a\ngenre classiﬁcation scenario. The assumption is that very\nsimilarartists belongtothesamegenre. We usetheDB-XLcollectionwhere eachartist is assigned to oneof 16 genres.(Theseassignmentsareonlyusedforthisevaluationandnot\nfor the interface.) We compute the classiﬁcation accuracies\nusingleave-one-outcross-validationwithanearestneighborclassiﬁer. On track level, the classiﬁcation accuracyis 31%\n(usinganartist ﬁlter sothat trainingandtest set donotcon-\ntain pieces from the same artist [15]). On artist level, weobtaina signiﬁcantlybettervalueof46%. Usingwebpages\n(as described below) and the tf ×idf similarity implementa-\ntiondescribedin[1],weobtainabout47%accuracy.First subjective impressions conﬁrm that the quality of\nthe audio-basedsimilarity is much better at artist level thanat tracklevel. We assumethereasonforthis is thatcomput-\ningsimilaritiesforsetsoftracksinsteadofindividualtracks\nis statistically morerobust.\nGiventhedistancesbetweenartists,wemaptheartiststo\ntherainbowusingaone-dimensionalcircularself-organizing\nmap [16]. Alternatively any other traveling salesman algo-rithmcouldbeused(see e.g.[2]).\n4.2. Labelingthe Rainbow\nWe query Google via its SOAP interface using the artist’s\nname as exact phrase and “music” and “review” as con-\nstraints [17]. We retrieve the top 50 ranked pages per artistand parse them using special vocabularies. For each artistwe counthowofteneachwordoccurs.\nWe use 3 different vocabularies for labeling and sum-\nmarizing the artists. One for the labels inside the rainbow,one for those outside, and one for short descriptions of the\nartists. All vocabularies are subsets of the vocabulary used\nin [1]. As recommendedin [1], some words such as “song”or “group” were removed. Furthermore, the vocabularies\ncontain lists of similar words. For example, “rap”, “hip-\nhop”,“hiphop”,and“hiphop”aretreatedas oneterm.\nFor the labels on the inside of rainbow, we use a vo-\ncabulary of over 50 high-level terms such as “rock”. For\nthe labels on the outside, we use terms describing variousstylesofmusic(e.g. “synthpunk”),variousinstruments(e.g.\n“ﬂute”), and various words commonly associated with mu-\nsic (e.g. “contemporary”). Not includedarewordsfromthehigh-level vocabulary, nor words which we consider to begenerally unrelated to the acoustical characteristics of the\nmusic (e.g. names of cities or countries). The thirdvocabu-\nlary contains about 1400 words. It contains all words fromtheothertwoplusadditionalonessuchas “Berlin”.\nWeuseﬁlterstosmooththetermfrequenciesontherain-\nbow and remove words that occur very frequently (for ex-ample, “singing” describes almost half of the rainbow). As\na scoring function, we use the technique presented in [18].\nIn addition, we use simple heuristics to ﬁnd labels whichdescribelargerregionsofthe rainbow.\n4.3. ColoringtheRainbow\nThecolorsoftherainbowencodethetermfrequenciesofthe\nhigh-levellabelsusedinsidetherainbow. Eachofthe8rings\noftherainbowcorrespondstoalabel. Theringsareordered\nsuchthattheringrepresentingthemostfrequentterminthecollection is on the outside (red), and the ring representing\ntheleastfrequenttermisontheinside(purple). InFigure1,\nthe outermost ring correspondsto “rock” which is the mostfrequenthigh-levelterminthecollection. The(orange)ring\nnexttoit correspondsto“jazz”.\nThe rainbow is segmented into equally large arcs (note\nthat this segementation is hard to see sometimes because\nneighboringsegmentstendtohavesimilarcolors). Thebright-\nness of each of the 8 colors in a segment is deﬁned by thescoringfunctionusedtoselectthehigh-levellabels. Forex-\nample, if “rock”has a highaveragescore in a segmentthenthebrightnessofthe redcoloris increased.\n5. Discussion\nWe have not conducted a formal user study. However, theﬁrst impressions of the interface are promising. The labels\non the inside describe the rough structure of the rainbow\nwell. Thelabelsontheoutsideareusefulbutnotas goodasthe ones inside. A positive example in Figure 1 is the seg-\nment labeled with “female” between the segments labeled\nwith “beats” and “club”. The Spice Girls are located in thissegmentand9oftheir10closest neighborsarefemales(in-cluding Jennifer Lopez and Kylie Minogue). However, the\nnumber of males in the second “female” segment between\n“guitar” and “club” is much higher. In particular in the re-gion between “female” and “club”, there are less than 50%\nfemales.\nIngeneral,thequalityoftheartistsummariesseemsgood.\nFor example, the Spice Girls are described with “pop, fe-\nmale, singing,rock,British”. GilbertoGil is describedwith\n“Brazilian, singing, world music, bossa nova”. Queen isdescribed with “rock, singing, classic, guitar, opera”. The\nterm “classic rock” should have been added to the vocabu-\nlarytoavoidoverlapwith“classicmusic”. Theterm“opera”wasprobablyselectedbecauseoftheiralbum“Anightatthe\nopera”.\nErrors in the summaries occur most frequently in cases\nwheretheartistnameisambiguous(e.g. inthecaseof“Elis-\nabeth”). Anexampleforauniqueartistnamethatresultedin\nasuboptimalsummaryisJamiroquai. Thebandisdescribedwith “funk, soul, female, light, cowboy”. The reason why\n“female”was selectedis probablytheirsong“Cosmicgirl”.\n(Female and Girl are treated as the same term.) The term“cowboy”shouldberemovedfromthevocabulary. Therea-son why it has been selected is probably their album “The\nreturnofthespace cowboy”.\nThe impressionsof using the GrifﬁnPowermateknobas\ninput device are good. Turning the rainbow appears to be\nan intuitive way to navigate in the collection. However, aforce feedback system that would give the user additionalinformation when the selection has been changed from one\nartisttothenextwouldmaketheinterfaceevenmoreenjoy-\nable. Alternatively,we havebeenexperimentingwith a softclicking sound to inform the user when the selection was\nchanged(andbyhowmuchithas changed).\n6. Conclusions\nIn this paper we presented MusicRainbow, a new interface\nto explore music collections at the artist level. The colors\nof the rainbow encode different styles of music. Similar\nartists are located close to each other on the rainbow. Wepresented a new approach to compute artist similarities us-\ningtracklevelinformation,anda newapproachtocombine\naudio-based similarity and information extracted from webpages(usingdifferentvocabulariesfordifferenthierarchical\nlevels). We believe MusicRainbow can help users make in-teresting discoveries amidst the rapidly growing number of\nartists onthemarket. Futureworkwill includeauserstudy.\nAcknowledgments\nThis researchwas supportedbyCrestMuse,CREST, JST.\nReferences\n[1] E. Pampalk, A. Flexer, and G. Widmer, “Hierarchical Orga-\nnization and Description of Music Collections at the ArtistLevel,” in ECDL,2005.\n[2] T. Pohle, E. Pampalk, and G. Widmer, “Generating\nSimilarity-Based Playlists Using Traveling Salesman Algo-rithms,” in DAFx,2005.\n[3] E. Pampalk, A. Rauber, and D. Merkl, “Content-Based Or-\nganization and Visualization of Music Archives,” in ACM\nMultimedia , 2002.\n[4] R. van Gulik, F. Vignoli, and H. van de Wetering, “Mapping\nMusic In The Palm Of Your Hand, Explore And Discover\nYour Collection,” in ISMIR,2004.\n[5] M. Schedl, P. Knees, and G. Widmer, “Discovering and Vi-\nsualizing Prototypical Artists by Web-Based Co-Occurrence\nAnalysis,” in ISMIR,2005.\n[6] O.Celma,M.Ram´ ırez,andP.Herrera,“Foaﬁngthemusic: A\nmusic recommendation system based on RSS feeds and userpreferences,” in ISMIR, 2005.\n[7] M.Goto and T.Goto,“Musicream: NewMusic Playback In-\nterfaceforStreaming,Sticking,Sorting,andRecallingMusi-\ncal Pieces,”in ISMIR,2005.\n[8] B. Logan, “Content-Based Playlist Generation: Exploratory\nExperiments,” in ISMIR,2002.\n[9] J.-J.Aucouturier and F.Pachet,“Music SimilarityMeasures:\nWhat’s the Use?” in ISMIR,2002.\n[10] E. Pampalk, T. Pohle, and G. Widmer, “Dynamic Playlist\nGeneration Based on Skipping Behaviour,” in ISMIR,2005.\n[11] F. Vignoli and S. Pauws, “A Music Retrieval System Based\non User-Driven Similarity and its Evaluation,” in ISMIR,\n2005.\n[12] M. Mandel, G. Poliner, and D. Ellis, “Support Vector Ma-\nchine ActiveLearning forMusic Retrieval,” Multimedia Sys-\ntems, Springer,2006.\n[13] M. Goto, “A Chorus-Section Detecting Method for Musical\nAudio Signals,”in ICASSP,2003.\n[14] E.Pampalk,“ComputationalModelsofMusicSimilarityand\ntheir Application in Music Information Retrieval,” Doctoraldissertation, Vienna University of Technology, 2006.\n[15] E. Pampalk, A. Flexer, and G. Widmer, “Improvements of\nAudio-Based Music Similarity and Genre Classiﬁcation,” in\nISMIR, 2005.\n[16] T. Kohonen, Self-Organizing Maps , Springer, 2001.\n[17] B. Whitman and S. Lawrence, “Inferring Descriptions and\nSimilarityfor Music from Community Metadata,” in ICMC,\n2002.\n[18] K. Lagus and S. Kaski, “Keyword Selection Method for\nCharacterizing TextDocument Maps,” in ICANN,1999."
    },
    {
        "title": "An Implementation of a Simple Playlist Generator Based on Audio Similarity Measures and User Feedback.",
        "author": [
            "Elias Pampalk",
            "Martin Gasser"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1415130",
        "url": "https://doi.org/10.5281/zenodo.1415130",
        "ee": "https://zenodo.org/records/1415130/files/PampalkG06a.pdf",
        "abstract": "This paper presents an implementation of a simple playlist generator. An audio-based music similarity measure and simple heuristics are used to create playlists given minimum user input. The ultimate goal of this work is to conduct a field study, i.e., to run the system on the users’ personal col- lection and study the usage behavior over a longer period of time. The functions include, for example, allowing the user to control the variance of the playlists in terms of how often the same song or songs from the same artists are repeated.",
        "zenodo_id": 1415130,
        "dblp_key": "conf/ismir/PampalkG06a",
        "keywords": [
            "playlist generator",
            "audio-based music similarity",
            "field study",
            "personal collection",
            "usage behavior",
            "longer period of time",
            "user control",
            "variance of playlists",
            "songs from the same artists",
            "simple heuristics"
        ],
        "content": "An Implementation ofa Simple PlaylistGenerator\nBased on Audio Similarity Measuresand User Feedback\nElias Pampalk*\nNationalInstituteofAdvanced\nIndustrialScienceandTechnology(AIST)\nIT,AIST,1-1-1Umezono\nTsukuba,Ibaraki305-8568,JapanMartin Gasser\nAustrianResearchInsitute\nforArtiﬁcial Intelligence(OFAI)\nFreyung6/6\nA-1010Vienna,Austria\nAbstract\nThis paper presents an implementation of a simple playlist\ngenerator. An audio-based music similarity measure and\nsimpleheuristicsareusedtocreateplaylistsgivenminimum\nuser input. The ultimate goal of this work is to conduct a\nﬁeldstudy,i.e.,torunthesystemontheusers’personalcol-\nlectionandstudytheusagebehavioroveralongerperiodoftime. The functionsinclude, forexample,allowing the user\ntocontrolthevarianceoftheplaylistsintermsofhowoften\nthesame songorsongsfromthe sameartists arerepeated.\n1. Introduction\nMobile audio players can store personal music collections\nof20,000andmoretracks. However,thevalueofsuchlarge\ncollections is limited by how easy it is, for example,to cre-\nate an interesting playlist. In the MIR research community\nseveralapproacheshavebeenpresentedaddressingthis(seee.g.[1, 2, 3, 4, 5]).\nA number of commercial systems exist which create in-\nteresting playlists. For example, iTunes allows the user tocreate “smart playlists” which use simple rules based on\nmetadata and song ratings. Last.fm uses collaborative ﬁl-\ntering to create playlists of similar songs and pandora.com\nuses manually annotated data. In contrast to these systems\nour implementation does not require any additional infor-mation other than the audio signals. In particular, it uses\ncomputationalmodelsofaudio-basedmusic similarity.\nIn this paper we present an implementation of a simple\nplaylistgenerator(SPG)basedontheworkpresentedin[6].\nThe idea is to use simple heuristics to constantly improve\nplaylists by adjusting them to user feedback. The mini-\nmum interaction scenario is that the user selects one song\nandpressestheplaybutton. SPG wouldrespondbyplayingsimilar songs. If the user rates one or more of these songs\n*)PartofthisworkwasdonewhiletheauthorwasworkingattheAustrian\nResearch Institute for Artiﬁcial Intelligence (OFAI).\nPermission to makedigital orhardcopies ofall orpart ofthis workforpersonal or classroom use is granted without fee provided that copies\nare notmadeordistributed forproﬁtorcommercial advantage andthat\ncopies bear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2006 University of Victoria\nthis information will be used to improve future recommen-\ndations.\nOur implementation serves two purposes. First, it has\nhelped clarify the requirements of a simple playlist gener-\nator. Including, for example, the requirements of adjusting\nthevarianceinthe playlistsmentionedbelow. Second,it al-lows us to conduct user studies where the users can use the\ntool on their own music at home. In particular, our goal is\ntoasktheuserstokeepdailynotesoftheirexperiences,andfollowupthese descriptionswithstructuredinterviews.\n2. Functionality\nSPG uses an audio-basedsimilaritymeasure, feedbackpro-\nvided by the user, and simple heuristics. These heuristics\nsearch for songs similar to preferredsongs and avoid songssimilar to rejected songs. The usage scenario is that the\nuserswanttoquicklycreateaplaylistfromtheirownmusic\ncollection.\nThecorefunctionsofSPG are:\nA. Verysimilar to pandora.comthe users can managetheir\nownradiostations. (However,inthiscasearadiostation\nonlyplayssongsfromtheuser’s collection.) Eachradiostationisdeﬁnedbyfavoriteandbannedsongsorartists.\nDependingontheratingstheusersmaketheycandeﬁne\na radiostation to play what they consider to be, for ex-\nample,“wake-upmusic”or“Saturdaydaynightmusic”.\nTheminimumrequirementtodeﬁne a newradiostationis to select one favoritesong orartist. All the user’s rat-\ningscanbeeasily accessedandmodiﬁedanytime.\nB. Akeyissueistocontrolthevarianceofthesongsplayed\non the radio station. If there is too little variance the\nsame songs will be repeated frequently, if there is too\nmuch variance the songs on the playlist might not be\nsimilar to each other. The user can control the variance\nby setting the frequency with which songs and artistsare repeated. For each there are three options: rarely,\nsometimes,andfrequently.\nC. In contrast to the work presented in [6] skipped songs,\nandsongswhichareplayed,arenotautomaticallyrated.\nInsteadtheusercaneasilyadjusttheratingofeachsong\nintheplaylist orintheeditradiostationpanel.Figure1. Screenshotof SPGrunningon MacOSX.\n3. User Interface\nFigure1shows ascreenshotofSPG (whichis implemented\nin Java). The list on the right side is the current playlistwhich is generated by the selected radio station. The user\ncan rate songs by clicking on the icons to the right of each\nsong. (Thisonlyhasanimpactonthecurrentradiostation.)\nTheplaylist is updatedbyclickingon“updateplaylist”. On\nthe left side (top to bottom) are the radio station selector,audio player controls, and controls to adjust the variance\nof the playlist. The export button underneath the playlist\nselector allows the users to export the current playlist to anM3U ﬁle (andthus allows them to use their preferredaudio\nplayer).\nThe controls to adjust the variance allow the user to set\nthe number of times artists and songs are repeated to either\n“rarely”, “sometimes”, or “frequently”. If, for example,the\nuser chooses to repeat songs rarely then SPG would ratherplay a not so similar song instead of playing a previously\nplayed song again. If the user chooses to repeat artists fre-\nquently then SPG (if appropiate) might play a number of\nsongs by the same artist within an hour. The user can only\nset the repetition frequencyof artists to the maximum leveltheuserchoseforthesongs(e.g. repeatingsongsfrequently,\nbutartists rarelyis nota validsetting).\nNot shownis the edit radiostation panelwherethe users\ncanview theirratingsandchangethe nameof theradiosta-\ntion. Onelistisshownforallratedsongs,andonelistforall\nratedartists. New items canbe addedusinga simplesearchfunction. Ratings in the lists can be changed in the same\nway thattheyarechangedintheplaylist.\n4. Techniques\nWe usethe audio-basedmusicsimilarity measuredescribed\nasG1Cin[7]. Itcombinesspectralsimilarity[8]withinfor-\nmation from ﬂuctuation patterns [9, 10]. To create playlists\nwe use the heuristic D described in [6]. Basically, songswhich are close to any of the user’s favorite songs, and far\naway from bannedsongs are recommended. If the user dis-\nlikes or likes an artist then all songs from this artist (unlesstheyareratedindividually)aretreatedasfavoriteorbanned\nsongs.\n5. Conclusions\nInthispaperwepresentedaJavaimplementationofasimple\nplaylist generator. The player implements minimum func-tionality to supportevaluationof MIR technologies(in par-\nticular audio-based music similarity measures and playlist\ngenerationheuristics) in everydaymusic consumption. Thefunctionalityincludesmanagementofradiostationsandsim-\nple controlof the variancein the play-lists. Future workin-\ncludes conducting a user study where users install the tool\nontheirprivatecollections.\n6. Acknowledgments\nThis work was supported by the EU project FP6-507142\n(SIMAC)andtheWWTFprojectInterfacestoMusic(I2M).\nReferences\n[1] J.-J. Aucouturier and F. Pachet, “Scaling up music playlist\ngeneration,” in Proc. of the IEEE Intl. Conf. on Multimedia\nExpo, 2002.\n[2] S. Pauws and B. Eggen, “PATS:Realization and User Eval-\nuation of an Automatic Playlist Generator,” in Proc. of the\nISMIR Intl.Conf. on Music Information Retrieval ,2002.\n[3] T. Pohle, E. Pampalk, and G. Widmer, “Generating\nSimilarity-Based Playlists Using Traveling Salesman Algo-\nrithms,” in Proc. of the Intl. Conf. on Digital Audio Effects ,\n2005.\n[4] R. van Gulik and F. Vignoli, “Visual Playlist Generation on\nthe Artist Map,” in Proc. of the ISMIR Intl. Conf. on Music\nInformation Retrieval ,2005.\n[5] M.GotoandT.Goto,“Musicream: NewMusicPlaybackIn-\nterface for Streaming, Sticking, Sorting, and Recalling Mu-\nsical Pieces,” in Proc. of the ISMIR Intl. Conf. on Music In-\nformation Retrieval ,2005.\n[6] E. Pampalk, T. Pohle, and G. Widmer, “Dynamic Playlist\nGeneration Based on Skipping Behaviour,” in Proc. of the\nISMIR Intl.Conf. on Music Information Retrieval ,2005.\n[7] E. Pampalk, “Computational Models of Music Similarity\nandtheirApplicationinMusicInformationRetrieval,”Ph.D.dissertation, Vienna University of Technology, 2006.\n[8] M. Mandel and D. Ellis, “Song-Level Features and Support\nVector Machines for Music Classiﬁcation,” in Proc. of the\nISMIR Intl.Conf. on Music Information Retrieval ,2005.\n[9] E. Pampalk, “Islands of Music: Analysis, Organization, and\nVisualization of Music Archives,” Master’s thesis, Vienna\nUniversity of Technology, 2001.\n[10] E. Pampalk, A. Flexer, and G. Widmer, “Improvements of\nAudio-Based Music Similarityand Genre Classiﬁcation,” inProc. of the ISMIR Intl. Conf. on Music Information Re-trieval, 2005."
    },
    {
        "title": "Musical Key Extraction from Audio Using Profile Training.",
        "author": [
            "Steven van de Par",
            "Martin F. McKinney",
            "André Redert"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1417879",
        "url": "https://doi.org/10.5281/zenodo.1417879",
        "ee": "https://zenodo.org/records/1417879/files/ParMR06.pdf",
        "abstract": "A new method is presented for extracting the musical key from raw audio data. The method is based on the extrac- tion of chromagrams using a new approach for tonal com- ponent selection taking into account auditory masking. The extracted chromagrams were used to train three key profiles for major and three key profiles for minor keys. The three trained key profiles differ in their temporal weighting of in- formation across the duration of the song. One profile is based on uniform weighting while the other two apply em- phasis on the beginning and ending of the song, respectively. The actual key extraction is based on comparing the key pro- files with three average chromagrams that were extracted from a particular piece of music using the same temporal weighting functions as used for the key profile training. A correct key classification of 98% was achieved using non- overlapping test and training sets drawn from a larger set of 237 CD recordings of classical piano sonatas. Keywords: Key Extraction, Chromagram, Audio, Music.",
        "zenodo_id": 1417879,
        "dblp_key": "conf/ismir/ParMR06",
        "keywords": [
            "chromagrams",
            "tonal component selection",
            "auditory masking",
            "key profiles",
            "major keys",
            "minor keys",
            "temporal weighting",
            "average chromagrams",
            "key extraction",
            "non-overlapping test and training sets"
        ],
        "content": "Musical KeyExtraction fromAudio Using Pro\u0002le Training\nStevenvandePar,Martin McKinney ,Andr ´eRedert\nPhilips Research Laboratories Eindho ven\nHigh TechCampus 36\n5656 AEEindho ven,theNetherlands\u0000Steven.van.de.Par,Martin.McKinney, Andre.Redert \u0001@philips.com\nAbstract\nAnewmethod ispresented forextracting themusical key\nfrom rawaudio data. The method isbased ontheextrac-\ntion ofchromagrams using anewapproach fortonal com-\nponent selection taking intoaccount auditory masking. The\nextracted chromagrams were used totrain three keypro\u0002les\nformajor andthree keypro\u0002les forminor keys.Thethree\ntrained keypro\u0002les differintheir temporal weighting ofin-\nformation across theduration ofthesong. One pro\u0002le is\nbased onuniform weighting while theother twoapply em-\nphasis onthebeginning andending ofthesong, respecti vely.\nTheactual keyextraction isbased oncomparing thekeypro-\n\u0002les with three average chromagrams that were extracted\nfrom aparticular piece ofmusic using thesame temporal\nweighting functions asused forthekeypro\u0002le training. A\ncorrect keyclassi\u0002cation of98% wasachie vedusing non-\noverlapping testandtraining setsdrawnfrom alargersetof\n237CDrecordings ofclassical piano sonatas.\nKeywords: KeyExtraction, Chromagram, Audio, Music.\n1.Introduction\nMusical keyextraction isimportant foranumber ofreasons;\nthemajor/minor distinction hasbeen linkedtotheemotional\nconnotation ofmusic [1],thekeycanbeused forfurther au-\ntomatic music analyses, andalsoDJing andmixing requires\nkey.\nAutomatic keyextraction from rawaudio data poses two\nchallenges. First, tonal components havetobeextracted\nfrom therawaudio data toobtain some kind ofharmonic\nrepresentation oftheactual notes thatarebeing played. Sec-\nond, based ontheharmonic representation, aninterpreta-\ntion hastobeperformed thatleads toakeyclassi\u0002cation.\nSystems thatworkwith symbolic data, e.g. MIDI, areable\ntoskip the\u0002rst step. Here weaddress themore dif\u0002cult\nproblem ofalso extracting aharmonic representation from\nrawaudio data such asdone inanumber ofrecent studies\n[2,3,4].\nPermission tomakedigital orhard copies ofallorpartofthisworkfor\npersonal orclassroom useisgranted without feeprovided thatcopies\narenotmade ordistrib uted forpro\u0002t orcommercial advantage andthat\ncopies bear thisnotice andthefullcitation onthe\u0002rstpage.\nc\n\u00022006 University ofVictoriaThesecond stephasbeen addressed invarious ways,e.g.\nbymapping theharmonic representation toaso-called spi-\nralarray which isatopology thatallowsabetter interpre-\ntation ofharmonic relatedness ofnotes [4],orbyextracting\nchromagrams (e.g. [2])andcomparing these tokeypro\u0002les\nbased onperceptual data [5].\nThemethod presented here isanextension oftheworkof\nPauws [2]andisbased onchromagram extraction from raw\naudio data andcomparison ofthechromagrams toasetof\nkeypro\u0002les thatarebased ontraining data.\n2.Method\nInthetraining phase ofthealgorithm, chromagrams areex-\ntracted onasegment-by-se gment basis from theinput au-\ndio(1024 sample segments with asampling frequenc yof16\nkHz). Each chromagram consists of12numbers thateach\nrepresent theprevalence ofaparticular note value inaseg-\nment. Chromagrams areaveraged across time with three\ndifferent weighting functions. Because thekeyisknownin\nthetraining phase, allchromagrams canbenormalized to\nC-major orc-minor keysandaveraged across alltraining\npieces such thattwosetsofkeypro\u0002les result (for major\nandminor key)each consisting ofthree keypro\u0002les with\ndifferent temporal emphasis.\n2.1. Chromagram extraction\nAsa\u0002rststep forthechromagram calculation, tonal (sinu-\nsoidal) components need tobeextracted. The initial tonal\ncomponent selection isbased onapeak-picking method in\ntheDFT (discrete Fourier transform) domain thatselects lo-\ncalmaxima. Inorder tore\u0002ne thefrequenc yresolution, the\nmethod ofDesainte-Catherine andMarchand [6]isapplied\nwhere thespectrum ofthewindo weddifferentiated signal\nisdivided bythespectrum ofthewindo wedoriginal sig-\nnal. Using thepeak positions intheoriginal spectrum, the\ndivided spectra allowforaveryaccurate estimate offre-\nquenc y.\nAlowcomple xitymasking model isapplied which cal-\nculates theenergywithin three consecuti veauditory \u0002lter\nbands (cf.[7])ofthesignal spectrum using aspectral weight-\ningfunction yielding aso-called excitation value. When\natonal component within that band hasanenergybelow\ntheexcitation value byacertain margin,itisassumed tobe\nmask ed,andthatcomponent isdiscarded. Inthiswayonlylower-order harmonics willgenerally beused forthechro-\nmagram calculation. This leads toimpro vements inperfor -\nmance ofthealgorithm.\nFortheselected components, note values arecalculated\nassuming thatAhasafrequenc yof440Hz.Amplitudes of\neach component aretransformed tothedBdomain toobtain\nmore equal weighting ofcomponents with different ampli-\ntudes. Based onthese components ahistogram ismade for\neach note value which constitutes thechromagram.\nBecause pieces ofmusic typically start andendinthe\ntonic, weweight these sections more inthekeycalculation.\nThetemporal weighting functions used forthispurpose are\nexponentially damped andramped functions, respecti vely.\nAdamping of90% isreached after 15seconds from thebe-\nginning orending ofthepiece.\n2.2. Keyextraction\nOnce thekeypro\u0002les havebeen trained according tothe\nmethod described above,akeycanbeextracted forapar-\nticular piece ofmusic. Three chromagrams areextracted\nusing thethree temporal weighting functions.\nEach ofthese extracted chromagrams iscorrelated totheir\nrespecti vekeypro\u0002les, yielding three correlation values that\nrelate tothebeginning ( \u0000\u0002\u0001\n\u0003\u0005\u0004\u0007\u0006\t\b\u000b\n),ending ( \u0000\r\f\n\u0003\u0005\u0004\u0007\u0006\t\b\u000b\n),andto-\ntality ofthesong ( \u0000\u000f\u000e\n\u0003\u0005\u0004\u0007\u0006\t\b\u000b\n).Here\n\u0004denotes thekeyindex\n(\n\u0004\u0011\u0010\u0013\u0012forC,\n\u0004\u0011\u0010\u0015\u0014forC-sharp, etc.) and\n\bdenotes thekey\ndenominator (major/minor).\nThecorrelation values arecombined intoa\u0002nal correla-\ntion value according to \u0000\u0002\u0016\n\u0003\u0005\u0004\u0007\u0006\t\b\u000b\n\u0017\u0010\u0019\u0018\u0000\u001a\u000e\n\u0003\u0005\u0004\u0007\u0006\t\b\u000b\n\u001c\u001b\u0000\r\u0001\n\u0003\u0005\u0004\u0007\u0006\t\b\u000b\n\u001d\u001b\u0000\r\f\n\u0003\u0005\u0004\u0007\u0006\t\b\u000b\n,where\n\u0018adjusts therelati vecontrib ution of \u0000\r\u000ever-\nsus \u0000\r\u0001and \u0000\r\f.Thevalues of\n\u0004and\n\bthatresult inthemaxi-\nmum value of \u0000\u0002\u0016\n\u0003\u0005\u0004\u0007\u0006\t\b\u000b\nindicate thekeythatwasdetected by\nthealgorithm.\n3.Evaluation\nThekeyextraction algorithm wasevaluated using adatabase\nof237classical piano pieces including pieces ofBach, Sho-\nstakovich, Brahms, and Chopin identical tothedatabase\nused byPauws [2].\nThis database wasrandomly split intoadisjunct training\nandtestset,where thetraining setcovered 98% ofthetotal\ndatabase. This procedure wasrepeated 5000 times toobtain\nanaccurate estimate oftheaverage performance ofthekey\nextraction algorithm onthisdatabase.\nInTable I,keyclassi\u0002cation results areshownforanum-\nberofconditions. Thestandard andbestcondition (\u0002rst col-\numn) waswith\n\u0018\u001e\u0010\u001f\u0014,masking module on,anddBcon-\nversion ofthetonal components before accumulation intoa\nchromagram. Theother columns showseveralvariations of\nthebestalgorithm where thealgorithm wasretrained forthe\nparticular variation. When\n\u0018 \u0010\"!itmeans thatonly the\nchromagram from thewhole thesong isused.Table 1.Correct keyclassi\u0002cations forvarious modi\u0002ca-\ntions ofparameters andsettings. The\u0002rstcolumn showsthe\nbest condition with\n\u0018#\u0010 \u0014,including dBtransformation on\ntonal components andmasking. Thenextcolumns showdif-\nferent values of\n\u0018,andthelasttwocolumns showtheresults\nwith\n\u0018$\u0010\u0015\u0014butwithout using adBtransformation andwith-\noutusing themasking model. Standard errors ofthemean\npercentages inthistable arealllessorequal to0.2.\u0018%\u0010&\u0014 \u0018%\u0010(' \u0018$\u0010)\u0012 \u0018$\u0010(* \u0018%\u0010No No!dB Mask\n98.1 95.6 96.8 97.6 83.7 91.8 88.7\n4.Conclusions\nThe method forextracting musical keythatwaspresented\nhere achie ves98% correct keyclassi\u0002cation onasetofclas-\nsical piano pieces. This method isbased ondifferent key\npro\u0002les forbeginning, ending, andtotality ofthesong. When\ninstead ofdifferent pro\u0002les only onekeypro\u0002le isused, only\n75.1% (nopro\u0002le training) to83.7% (with pro\u0002le training)\nperformance isachie ved(see [2]andTable Iwith\n\u0018+\u0010,!).\nAnadvantage ofthismethod isthatitisbased onthetrain-\ningofkeypro\u0002les from rawaudio reference data which al-\nlowsforagood match between training andtestdata. In\naddition, itisbene\u0002cial tohaveacompressi ve(dB) trans-\nformation ofamplitudes oftonal components togetamore\nequal contrib ution ofcomponents ofdifferent levelsandto\nuseamasking model toremo venon-salient components.\n5.Ackno wledgments\nWethank Armin Kohlrausch andJanto Skowronek fortheir\nhelpful comments.\nRefer ences\n[1]M.P.Kastner and R.G. Crowder ,Perception ofthema-\njor/minor distinction: IV.Emotional connotations inyoung\nchildren. Music Perception, 1990, Vol.8,No.2, pp189-202.\n[2]S.Pauws, Musical keyextraction from audio, inISMIR\n2004FifthInt.Conf.onMusicInf.Retr.Proc.,2004, paper\n142.\n[3]W.Chai andB.Vercoe, Detection ofkeychange inclassical\npiano music, inISMIR2005SixthInt.Conf.onMusicInf.\nRetr.Proc.,2005, pp468-473.\n[4]C-H. Chuan andE.Chew,Fuzzy analysis inpitch class\ndetermination forpolyphonic audio key\u0002nding, inISMIR\n2005SixthInt.Conf.onMusicInf.Retr.Proc.,2005, pp468-\n473.\n[5]C.L. Krumhansl, Congnitive Foundations ofMusicPitch,,\nOxford University Press, NewYork,1990.\n[6]M.Desainte-Catherine andS.Marchand, High-precision\nFourier analysis ofsounds using signal derivativesJ.Au-\ndioEng. Soc.,v ol.48,no.7/8,July/Aug. 2000, pp.654-667\n[7]B.R. Glasber gandB.C.J. Moore, Deri vation ofauditory\n\u0002lter shapes from notched-noise data,Hearing Research,\n1990, Vol.47,pp.103-138"
    },
    {
        "title": "Fast Generation of Optimal Music Playlists using Local Search.",
        "author": [
            "Steffen Pauws",
            "Wim Verhaegh",
            "Mark Vossen"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1417050",
        "url": "https://doi.org/10.5281/zenodo.1417050",
        "ee": "https://zenodo.org/records/1417050/files/PauwsVV06.pdf",
        "abstract": "We present an algorithm for use in an interactive music sys- tem that automatically generates music playlists that fit the music preferences given by a user. To this end, we introduce a formal model, define the problem of automatic playlist generation (APG) and indicate its NP-hardness. We use a local search (LS) procedure based on simulated annealing (SA) to solve the APG problem. In order to employ this LS procedure, we introduce an optimization variant of the APG problem, which includes the definition of penalty functions and a neighborhood structure. To improve upon the per- formance of the standard SA algorithm, we incorporated three heuristics referred to as song domain reduction, par- tial constraint voting, and two-level neighborhood structure. In tests, LS performed better than a constraint satisfaction (CS) solution in terms of run time, scalability and playlist quality. Keywords: local search, simulated annealing, music playlist generation, music retrieval.",
        "zenodo_id": 1417050,
        "dblp_key": "conf/ismir/PauwsVV06",
        "keywords": [
            "algorithm",
            "interactive music system",
            "automatic music playlist generation",
            "formal model",
            "problem of automatic playlist generation",
            "NP-hardness",
            "local search procedure",
            "simulated annealing",
            "song domain reduction",
            "partial constraint voting"
        ],
        "content": "FastGeneration of Optimal Music Playlists using Local Search\nSteffenPauws,WimVerhaegh,Mark Vossen\nPhilips Research Europe\nHigh TechCampus 34\n5656 AE Eindhoven\nTheNetherlands\nsteffen.pauws/wim.verhaegh@philips.com\nAbstract\nWepresentanalgorithmforuseinaninteractivemusicsys-\ntem that automatically generates music playlists that ﬁt the\nmusicpreferencesgivenbyauser. Tothisend,weintroduce\na formal model, deﬁne the problem of automatic playlist\ngeneration (APG) and indicate its NP-hardness. We use a\nlocal search (LS) procedure based on simulated annealing\n(SA) to solve the APG problem. In order to employ this LS\nprocedure,weintroduceanoptimizationvariantoftheAPG\nproblem, which includes the deﬁnition of penalty functions\nand a neighborhood structure. To improve upon the per-\nformance of the standard SA algorithm, we incorporated\nthree heuristics referred to as song domain reduction, par-\ntialconstraintvoting,andtwo-levelneighborhoodstructure.\nIn tests, LS performed better than a constraint satisfaction\n(CS) solution in terms of run time, scalability and playlist\nquality.\nKeywords: local search, simulated annealing, music\nplaylistgeneration, music retrieval.\n1. Introduction\nTo realize personalized assistance in music choice, we re-\nsearchtheautomaticgenerationofmusicplaylistsbymeans\nof mathematical programming and combinatorial optimiza-\ntion. As a ﬁrst prerequisite, we need to be able to reason\nabout songs. Therefore, we think of songs as a list of at-\ntributes that are deemed to be relevant for music choice. As\nshown in Table 1, song attributes can be nominalsuch as\nthe song/album title or the performing artist, allowing only\nreasoningintermsof equivalence andsetmembership (e.g.,\nthese two songs are by the same artist). Attributes can also\nbenumerical suchasthedurationandthetempoofthesong;\nnumerical attributes allow the computation of a difference\nbetween attribute values. Data extracted from musical au-\ndio such as a chroma spectrum for key/chord information\nor timbre features devised for audio similarity purposes can\nPermissiontomakedigitalorhardcopiesofallorpartofthisworkfor\npersonal or classroom use is granted without fee provided that copies\narenotmadeordistributedforproﬁtorcommercialadvantageandthat\ncopies bear this notice and the full citation on the ﬁrst page.\nc°2006 Universityof Victoriabe represented as vectors of numericals. Other types of at-\ntributes(e.g., categorical ,ordinal) are also possible.\nTable1. Song attributesand types.\nkattribute type example\n1 song ID nominal 101\n2 title nominal All Blues\n3 artist nominal Miles Davis\n4 album nominal Kind of Blue\n5 genre nominal Jazz\n6 duration numerical 696 (secs)\n7 year numerical 1959\n8 tempo numerical 137 (bpm)\n9 chroma spectrum numericals\n10 audio features numericals\nInformally,aplaylistisasequenceofthe‘right’songsat\nthe‘right’positionsthatcanbeplayedbackinonego. What\nis ‘right’ in this respect depends on the prevailing purposes\nof everyday music listening. In our work, we model these\ndesired playlist properties as formal constraints that are de-\nﬁned over the playlist positions in terms of song attributes.\nWedistinguishthreetypesofconstraints. A unaryconstraint\nposesarestrictionforasingleplaylistposition(e.g,theﬁrst\nsong should be a ‘jazz-song’). A binaryconstraint declares\na desired relation between songs at two different positions.\nOrder or similarity of songs at two positions are examples\n(e.g., both songs should have the similar tempo/timbre). Fi-\nnally, aglobalconstraint is deﬁned on any number of po-\nsitions. For instance, they can express restrictions on car-\ndinality for the entire playlist (e.g., there should be at most\ntwo different genres in a playlist) or group a set of unary\nor binary constraints all together (e.g., all songs should be\n‘jazz-songs’).\nSince the algorithms will be used in an interactive mu-\nsic system, demands on efﬁciency ,scalability , andplaylist\nquality(i.e., optimality) are pressing. Time to compute a\nplaylist should run in a few seconds, since there is a user\nwaiting for the result. In some applications, we can afford\nourselves to present a partial (non-optimal) playlist, while\ncomputing the rest of the playlist. The algorithms should\nscale towards playlists of any length and music collections\nof any size and any variety. The returned playlist should be\noptimal and reﬂect the music preferences given by the user,even if these preferences result into conﬂicting constraints\nandno playlist existsthat meets all preferences expressed.\nAfterdiscussingrelatedwork,wepresentaformalmodel\nand the computational difﬁculties of the automatic playlist\ngenerationprobleminSection2. Anoptimizationvariantof\ntheproblemwillbeintroducedinSection3,whichallowsus\ntouseaclassofgenericapproximationalgorithmsknownas\nlocalsearch. Wewillexplaintheuseofsimulatedannealing,\nas a special case of local search, for our optimization prob-\nleminSection4. Astudyoftheproblemstructureprovided\nus heuristics to improve simulated annealing for our prob-\nlem. In Section 5, we show the evaluation of the algorithm\nina comparison test.\n1.1. Related work\nPlaylist generation is an active ﬁeld within MIR. Here, we\nonly review approaches that use problem formulations sim-\nilarto ours.\nAlghoniemy and Tewﬁk [1] present a network ﬂow ap-\nproach to playlist generation and a branch and bound algo-\nrithm to solve it as a binary linear program. Unfortunately,\nbranch and bound is an exponential algorithm in the worst\ncase.\nPachet, Roy and Cazaly [2] use a constraint satisfaction\nformulation in which desired properties of the playlist are\ndeclared by constraints to be met. Aucouturier and Pa-\nchet [3] later re-formulate the problem to allow the use of\napproximatingalgorithmsbasedonlocalsearchtoscalethe\napproach towards very large music collections. Costs are\nassociated with playlists: the more constraints are violated,\nthe higher the cost. The use of this solution method is also\nthe subject of this paper. A more detailed exposition of the\nmethodand its evaluationcan be found elsewhere[4].\n2. A formalmodel\nFormally, a song is given by a vector s= (v1; : : : ; v K)\nof attribute values, denoting that the k-th attribute of song\nshas value vk2Dk. For an example of attributes, see\nTable 1. Next, a music collection is given by a set M=\nfs1; : : : ; s mgofmsongs.\nAplaylistis formally deﬁned by a vector p=\n(p1; : : : ; p n)of length n, where pi2Mdenotes the song\nat the i-th position, for all i= 1; : : : ; n. Each song piis\nagain a vector of length K, so we can denote attribute kof\nsong pibypik. Although the length nis not speciﬁed be-\nforehand, we assume that a lower bound nminand an upper\nbound nmaxare given.\nAplaylisthastomeetasetofdeclaredunary,binaryand\nglobal constraints. A unary constraint restricts the choice\nof songs for one speciﬁc position. In its general form, it is\ngiven by a triple (i; k; V ), for a position i2 f1; : : : ; n ming,\nattribute k2 f1; : : : ; K g, and value set VµDk, and it\nimplies that pik2Vhas to hold. For instance, to specify\nthat the ﬁrst song of the playlist should be of genre Rock orJazz, we choose i= 1,k= 5, and V=fRock;Jazzg. Note\nthat we do not allow i > n min, as the resulting playlist may\nnot be long enough to havesuch a position.1\nTo enable a more efﬁcient speciﬁcation of unary con-\nstraints, we introduce the followingthree speciﬁc forms.\n²Inanexclude-unary constraint,wespecifyaset Wµ\nDkof forbidden attribute values, meaning that V=\nDknW.\n²In arange-unary constraint, the set of desired values\nVis given by an interval [v; w], that is, V=fx2\nDkjv·x·wg. This constraint requires a (partial)\norder on the attributeinvolved.\n²In asimilar-unary constraint, the set Vis given indi-\nrectly by V=fx2Dkjl·f(v; x)·ug, using\na similarity function f:Dk£Dk![0;1], attribute\nvalue v2Dk, and bounds l; u2[0;1]on the desired\nsimilarity.\nAbinary constraint enforces a relation between songs\nat two speciﬁc playlist positions. In its general form, it\nis given by a four-tuple (i; j; k; d ), for positions i; j2\nf1; : : : ; n ming, attribute k2 f1; : : : ; K g, and function d:\nDk!2Dk, and it implies that pik2d(pjk)has to hold.\nThe function dis generally not given explicitly, but implic-\nitly as in the following ﬁve speciﬁc forms of binary con-\nstraints.\n²In anequal-binary constraint, the function dis given\nbyd(v) =fvgforall v2Dk. Thisimpliesthat pik=\npjkhas to hold. For an inequal-binary constraint, we\ntakethe complement givenby d(v) =Dkn fvg.\n²In asmaller-equal-binary constraint, dis given by\nd(v) =fx2Dkjx·vg, implying that pik·pjk\nhas to hold. Note that this constraint again requires\na (partial) order on the attribute. A greater-equal-\nbinaryconstraintis quite similar in its deﬁnition.\n²Finally, in a similar-binary constraint, the function d\nis given by d(v) =fx2Dkjl·f(x; v)·ug,\nagain using a similarity function f:Dk£Dk!\n[0;1], and bounds l; u2[0;1]on the desired similar-\nity. So, this constraint implies l·f(pik; pjk)·u.\nGlobal constraints pose restrictions on songs at a num-\nber of positions. The set of positions is denoted by\naninterval [i; j], with i2 f1; : : : ; n mingandj2\nf1; : : : ; n min; : : : ; n maxg, which is formally deﬁned as the\nsetfl2Nji·l·j^l·ng. Notethatif j=nmax,then\nthissetdependsonthelength noftheplaylist,andcontains\nat least all positions from nminonwards in the tail of the\nplaylist.\n1Or,conversely,ifonewantstorestrictthesong onacertain position i,\none has to choose nmin¸i.There is no general form of a global constraint, except\nthat it always contains an interval [i; j]as described above,\nand an attribute k2 f1; : : : ; K gon which it applies. Some\nof the global constraints that we consider are deﬁned here-\nunder.\n²Acardinality-global constraintisgivenbyaﬁve-tuple\n(i; j; k; a; b ), where apart from the interval [i; j]and\nattribute ka lower bound aand upper bound bare\ngiven on the number of different attribute values that\nareallowed. Morespeciﬁcally,thisconstraintimplies\nthata· jfplkjl2[i; j]gj ·bhas to hold.\n²Acount-global constraint is given by a six-tuple\n(i; j; k; V; a; b ), with VµDkanda; b2N, imply-\ning that a· jfl2[i; j]jplk2Vgj · bhas to hold.\nIn other words, the number of songs in the interval\nwith an attribute value from Vshould be between a\nandb.\n²Asum-global constraint is given by a ﬁve-tuple\n(i; j; k; a; b ), with bounds a; b2R, and it denotes\nthata·P\nl2[i;j]plk·b. Note that it is only deﬁned\nfornumerical attributes.\nIn addition, we use global constraints that imply a unary\nconstraintoneachpositioninaninterval(i.e., each-global ),\nthat poses a binary constraint on each two successive posi-\ntions in an interval (i.e., chain-global ), and that poses a bi-\nnaryconstraintoneverypairofpositionsinaninterval(i.e.,\npairs-global ).\nHaving everything in place now, we can give a formal\ndeﬁnitionof the playlist generation problem, as follows.\nDeﬁnition 1. (Automatic playlist generation problem\n(APG)) Given a music collection M, a set of constraints\nC, and length bounds nminandnmax, ﬁnd a playlist pof\nn2 fnmin; : : : ; n maxgsongs from Msuch that psatisﬁes\nallconstraints in C.\nWithout going into details, we indicate that APG is NP-\nhard. This is caused by aspects corresponding to four dif-\nferent NP-complete problems [5]. For instance, ﬁnding a\nplaylist in which each two consecutive songs are similar is\ncomparable to the Hamiltonian path problem. Next, ﬁnd-\ning a playlist in which for each attribute the occurring val-\nuesaredifferentcorrespondstothe3-dimensionalmatching\nproblem. Finding a playlist with a total duration of a cer-\ntain length corresponds to the subset sum problem. Finally,\nﬁnding a playlist in which each two songs are sufﬁciently\ndifferentis comparable to the independent set problem.\n3. An optimization variant\nAs APG is NP-hard due to several reasons, we opt for a\ngeneric approximation method. To this end, we convert\nAPG into an optimization variant APG-O by introducing anon-negative penalty function that represents the amount of\nviolationoftheconstraints. Then,insteadofsearchingfora\nplaylist that meets all constraints, we search for a playlist\nthat minimizes the penalty by a local search method. If\nthe penalty is zero, all constraints are met. Introducing a\npenalty also overcomes the issue of over-constrained prob-\nlem instances. In that case, no solution exists that meets all\nconstraints, but a playlist is generated that meets the con-\nstraints as well as possible.\nThe penalty function for a playlist is as follows. First,\nwe deﬁne for each constraint a penalty function that returns\na value from [0;1]. Next, the penalty of a playlist is given\nby a weighed average of each of the constraint penalties.\nThe weights can be used to give more importance to one\nconstraint over the other. In case of an over-constrained in-\nstance, this allowsto trade-offdifferentconstraints.\nForthepenaltyfunctiondeﬁnitions,weuseaspecialnor-\nmalized differencefunction, ª,between attributevalues.\n²For a nominal attribute k, we determine (in)equality\nbetweenvalues,thatis,foreachtwovalues a; b2Dk,\naªb= 0, ifa=b,and aªb= 1, otherwise.\n²For a numerical attribute k, we use the difference as\ndeﬁned by aªb=ja¡bj\nmaxDk¡minDk.\nAlsoforothertypesofattributes,asuitabledifferencefunc-\ntion can be deﬁned.\nNow, we can deﬁne the constraint penalties. For a unary\nconstraint (i; k; V ), we deﬁne the penalty as the minimum\ndifference to any element from V, that is, as minfpikª\nvjv2Vg. We however make an exception for two of the\nthree speciﬁc forms.\n²Foranexclude-unary constraint,theset Visindicated\nby its complement W=DknV, with Wtypically\nvery small. To prevent very small penalty values due\nto normalization, we deﬁne the penalty for this con-\nstraint as 0ifpik62Wand1otherwise.\n²Forasimilar-unary constraint,therequirementisthat\nf(v; pik)should lie in the interval [l; u]. Hence, we\ndeﬁne the penalty to be equal to the distance to this\ninterval, that is, the penalty is given by minfjx¡\nf(v; pik)j jx2[l; u]g. This is comparable to the\ngeneral deﬁnition of unary-constraint penalties, ex-\ncept that we deﬁned it on the co-domain of f, instead\nof on attributevaluesdirectly.\nForabinaryconstraint (i; j; k; d ),therequirementisthat\npik2d(pjk). The corresponding penalty is therefore de-\nﬁned as minfpikªvjv2d(pjk)g, comparable to unary\nconstraints. Again, we make the following two exceptions\nto this deﬁnition.\n²For aninequal-binary constraint, we deﬁne a penalty\nof0ifpik6=pjkand1otherwise.²For asimilar-binary constraint, we again use the\nsimilarity function fand bounds l; uin the deﬁni-\ntion of the penalty, resulting in a penalty minfjx¡\nf(pik; pjk)j jx2[l; u]g.\nThe penalties for global constraints are deﬁned as fol-\nlows.\n²For acardinality-global constraint (i; j; k; a; b ), the\nnumber °=jfplkjl2[i; j]gof different attribute\nvaluesisrequiredtoliebetween aandb,hencewede-\nﬁnethepenaltyby1\n±¢minfjx¡°j jx2 fa; : : : ; b gg,\nwhere ±is given by maxfa;j[i; j]j ¡bgfor normal-\nization.\n²For acount-global constraint (i; j; k; V; a; b ), the\nnumber of songs ¹=jfl2[i; j]jplk2Vgwith\nan attribute value from Vshould lie between aandb,\nso we deﬁne the penalty by1\n±¢minfjx¡¹j jx2\nfa; : : : ; b gg,with again ±= max fa;j[i; j]j ¡bg.\n²For asum-global constraint (i; j; k; a; b ), where the\nsum¾=P\nl2[i;j]plkshould lie in [a; b], the penalty\nisgivenby1\n±0¢minfjx¡¾j jx2[a; b]g. Asthemini-\nmum possible sum equals v=j[i; j]jminDkand the\nmaximum possible sum equals w=j[i; j]jmaxDk,\nwe choose the normalization constant ±0= max fa¡\nv; w¡bg.\nIn addition, the penalties for the each-global ,chain-\nglobalandpairs-global are deﬁned by the normalized sum\nof the penalties of their constituent unary or binary con-\nstraints.\n4. Local search\nThe deﬁnition of the optimization variant APG-O allows us\nto solve it with a generic approximation method such as lo-\ncal search (LS) [6]. The key feature of local search is that\nit searches the solution space by iteratively stepping from\none solution to a neighboring solution, and comparing their\nquality. A neighborhood structure deﬁnes which solutions\nareneighborstoagivensolution,whichareusuallyobtained\nbymakingsmallalterationstothegivensolution. ForAPG-\nO, solutions are given by playlists, and its neighborhood\nstructureisgiveninSection4.2,consistingofreplacements,\ninsertions, deletions, and swaps of songs. The cost function\nisobviouslygivenbythetotalweighedpenaltyofaplaylist,\nwhichwe denote by f(p).\nA solution is called locally optimal if there is no neigh-\nboringsolutionwithbettercost. Asolutioniscalled globally\noptimalif there is no solution in the whole solution space\nwith better cost. The objective of APG-O is to ﬁnd such a\nglobaloptimum, that is, a playlist with minimal penalty.4.1. Simulated annealing\nBasic LS algorithms like iterative (ﬁrst and best) improve-\nmentwere found notto be well equipped to solve our prob-\nlem as they fell into local optima. Therefore, we consider\nsimulatedannealing (SA),whichincorporatesamechanism\ntoescapefromlocaloptimawithoutaneedforrestarting[7].\nIn contrast to the basic LS algorithms, SA replaces the\ndeterministic (strict improving) acceptance criterion by a\nstochastic criterion. More speciﬁcally, a control variable t\nisintroduced,andthechanceofacceptinganeighboringso-\nlution p0to a given solution pis deﬁned by the acceptance\nprobability\nPr(p0jp) =(\n1 iff(p0)·f(p), and\nexp³\nf(p)¡f(p0)\nt´\notherwise.\nAswecansee,thechanceofacceptingadeterioratingsolu-\ntion depends on the amount of deterioration, as well as the\ncontrol parameter t. For each value of t, sequences of so-\nlutions are generated and evaluated, after which the control\nvariableisloweredbyadecrementfunction. Asaresult,the\nchance of accepting deteriorating solutions decreases dur-\ning the course of the algorithms. For further explanation of\nSA, we make a forward reference to Figure 1 for our ﬁnal\nalgorithm.\nSAalgorithmsmakeuseofaso-called coolingschedule ,\nwhich consists of the sequence length of solutions Lh, the\ninitialvalueofthecontrolparameter t0,thedecrementfunc-\ntionusedfordecreasing t,andastopcriterion. Weusea ge-\nometric cooling schedule that has been successfully applied\nto many problems described in literature. For APG-O, this\nresults in a choice of Lh= 10,t0= 1, decrement function\nth+1= 0:9¢th,andstopcriterion (f(p)< ²jjh > H ),that\nis, we stop if all constraints are ‘sufﬁciently’ satisﬁed or we\ndid a pre-deﬁned number of iterations.\n4.2. Neighborhood deﬁnition\nFor the neighborhood, we deﬁned the following four types\nof moves.\nAreplace move chooses a playlist position and a new\nsongfromthemusiccollectionandreplacesthesongthatis\nat that position by the newsong.\nAninsert move chooses a position in the playlist (if n <\nnmax)andanewsongfromthemusiccollectionandinserts\nthat song into the playlist at the chosen position.\nAdelete move chooses a position in the playlist (if n >\nnmin)and removesthe song at that position.\nFinally,a swapmove choosestwopositionsintheplaylist\nand swapsthe songs that appear at these positions.\nEach of the above four types of moves deﬁnes a neigh-\nborhood. The complete neighborhood is given by the union\nofthesefourneighborhoods. Tobalancetheselectionofthe\nfour individual neighborhoods for generating a new solu-\ntion in our SA algorithm, we introduce probability weightswreplace,winsert, and wdelete, which determine the proba-\nbility of performing a particular type of move. In tests [4],\nwe found 1=3to be a good performing value for all weights\nforacollectionof2,248songs. Swapmovesaretreatedina\nseparateneighborhood (see Section 4.3.2).\nAs the moves described above make small modiﬁcations\nto a playlist, the changes in penalty function can be calcu-\nlatedincrementally,and thus more efﬁciently.\n4.3. Heuristic improvements\nIn order to increase the performance of SA, we propose\nthree heuristic improvements based on the various types of\nconstraints in APG-O: song domain reduction, a two-level\nneighborhood,and partial constraint voting.\n4.3.1. Song domain reduction\nSongdomainreduction resemblesaformofconstraintprop-\nagation to guarantee node consistency for unary constraints\nused in constraint satisfaction methods [8]. To this end, we\ndenoteasongdomain Miofaposition iasthesubsetofthe\nmusic collection, MiµM, that deﬁnes the possible songs\nthat are allowed at that position; for a playlist p, it should\nhold that pi2Mi. By reducing the song domains Mi, we\ncan in this way trim the search space for our LS. If a posi-\ntion is not over-constrained, the reduction is established by\nremoving all songs from a given song domain that do not\nmeet all unary constraints that are declared for the position\nunderconsideration. Wehavedevelopeddifferentreduction\nmechanismsforallfourindividualneighborhoodstructures.\n4.3.2. Two-levelneighborhood structure\nThe penalties of global constraints such as cardinality-\nglobalandsum-global constraints are not affected by swap\nmoves; they do not depend on song order. In contrast, most\nunary and binary constraints and their combinations into\nglobalconstraints do depend on song order.\nBased on this observation, we introduce a two-level\nneighborhood , splitting the search into twoprocedures, that\nare applied alternatingly. The ﬁrst procedure consists of a\nsequence of ¯replace, insert, and delete moves, for meet-\ning constraints that do not depend on song order. Next, a\nsequence of swap moves is applied to put the songs in the\nright order. For the latter, we employ a simple procedure,\ncallednon-deteriorating reordering (NDR), which applies\niterative improvement with a maximum of °swap moves.\nIntests[4],wefound100tobeagoodperformingvaluefor\nboth¯and°fora collection of 2,248 songs.\n4.3.3. Partialconstraintvoting\nSimplyapplyingrandommovesatrandomlychosenplaylist\npositions leads to an inadequate coverage of the restrictions\nasposedbysomeglobalconstraints,notablythe cardinality-\nglobaland thecount-global constraints. These constraints\nneed speciﬁc types of songs at speciﬁc positions. To this\nend, we apply a partial constraint voting mechanism inwhichconstraintsvotefororagainstaplaylistpositionanda\nsonginamove. Everyconstraintcancastapositivevotefor\na position with a song in a given solution, if it contributes\nto its violation. On the other hand, a constraint can cast a\nnegative vote for a position with a song, it it helps in its\nsatisfaction. The votes from all constraints are tallied, and\na playlist position is chosen biased by these votes. So, a\nposition with many positive votes and a few negative votes\nis more likely to be chosen than a position with fewer pos-\nitive votes and more negative votes. If the replace or the\ninsertneighborhoodwasselected,wealsohavetovotefora\nspeciﬁc song to be added to the playlist. For efﬁciency rea-\nsons, only the above-mentioned global constraints can vote\nfor songs. Again, the song votes are tallied and one song is\nchosen, biased by the collected votes.\nThough this voting mechanism is effective in direct-\ning constraint satisfaction, it is computationally intensive\nin comparison to random selection of positions and songs.\nTherefore, we limit its use to a fraction ±of the reselect\nmoves. Intests[4],wefound ±= 0:3tobeagoodperform-\ning valuefor a collection of 2,248 songs.\n4.3.4. Finalalgorithm\nThe adapted SA algorithm is depicted in Figure 1.\nINITIALIZE p,t0,L0;\nh:= 0;\nr:= 0;\nrepeat\nforl:= 1toLhdo\nbegin\nifr < ¯then\nbegin\nif± >random[0 ;1)then\nGENERATE RANDOM p02Nreselect (p)\nelse\nGENERATE p02Nreselect (p)BY VOTING ;\niff(p0)·f(p)orexp(f(p)¡f(p0)\nt)>random[0 ;1)\nthenp:=p0;\nr:=r+ 1\nend\nelse begin\np:=NDR(p; °);\nr:= 0\nend\nend;\nh:=h+ 1;\nCALCULATE LENGTH Lh;\nCALCULATE CONTROL th\nuntilSTOP CRITERION\nFigure1. The resultingalgorithm forAPG-O.\n5. Evaluation\nIn performance tests, the algorithm was shown to be an im-\nprovementoverapreviouslydesignedconstraintsatisfaction\n(CS)algorithmonefﬁciency,scalabilityandplaylistquality\n(optimality). We used constraint sets that were inspired by101418222630 0.1   1  10 1001000\nCS\nLS\nplaylist lengthmean run time (secs)\n0 2 4 6 8\nx 104051015202530\nLS\nCS\nmusic collection size10 2002.557.510\nLS\nCS\nplaylist lengthmean rating scoreFigure 2. (a) Mean run time over 10 runs as a function of playlist length for LS (local search) and CS (constraint satisfaction) using\na set of 15 global constraints. (b) Mean run time over 10 runs as a function of music collection size for LS and CS using a set of 15\nglobalconstraints. (c)Mean ratingscore acrossLSandCSanddifferent playlistlengths . Cross-barsrepresentstandarderrorsofthe\nmean.\nprevioususerstudies[9]andmusiccollectionsrangingfrom\n2,248songsto71,194songs. InFigure2(a),weseethatLS\nruns shorter and less erratic in run time than CS does for a\nsetof15globalconstraintsfordifferentplaylistlengthsand\na collection of 2,248 songs. Ten runs of the test were per-\nformed to arrive at a mean run time. The typical run time\nof the algorithm is about 2 seconds on a PC platform for\nplaylists of at most 14 songs, a collection of about 2,000\nsongs, and various constraint sets. In Figure 2 (b), we see\na linear increase on run time for larger music collections\nfor LS generating playlists of 10 songs using the same set\nof constraints. Its run time on a large music collection is\ntoo high for particular interactive applications. CS did not\ngenerateplaylistsforlargemusiccollectionsduetomemory\ninsufﬁency.\nIn a user evaluation, eighteen participants (22-41 years;\n13 men, 4 women) were asked to rate on a scale of 0to10\n(extremelybad-good)48playlistsintotalofvaryinglengths\nthat were generated either by LS or CS using various con-\nstraintsets. AsshowninFigure2(c),playlistsgeneratedby\nLS were rated signiﬁcantly higher than playlists generated\nby CS (mean rating score for LS playlists: 7.7; mean rating\nscore for CS playlists: 6.5; F(1;17) = 56 :6; p < 0:001).\nAlso,weseethatlongerCSplaylistswereratedsigniﬁcantly\nlower than smaller CS playlists, whereas this is not true for\nLSplaylists ( F(1;17) = 7 :6; p < 0:05).\nFor a detailed description of the evaluation, we refer\nto[4].\n6. Conclusion\nThealgorithmhasalreadybeenembeddedininteractivemu-\nsic prototype systems and services designed for consumer\nelectronic devices [9]. These systems open up completely\nnew methods for users to experience music by the art of\nre-combining songs in various ways. Playlist generation is\nalso useful for automatic DJ-ing applications that requirethat songs are ﬁrst ordered on meter, tempo and key before\nthey are mixed one after the other. Online music sales ap-\nplications can be easily augmented with a service to auto-\nmatically compile and download a personal album. Music\nstreaming and broadcasting can excel using on-the-ﬂy gen-\nerationofmusicprogramsallowingtrulypersonalandinter-\nactive(Internet) radio and podcasting.\nReferences\n[1]M. Alghoniemy and A.H. Tewﬁk, “A Network Flow Model\nfor Playlist Generation”, In: Proceedings of the IEEE\nInternational Conference on Multimedia and Expo 2001\n(ICME2001) , August 22 - 25, 2001, Tokyo,Japan.\n[2]F.Pachet,P.RoyandD.Cazaly,“ACombinatorialApproach\nto Content-based Music Selection”, IEEE Multimedia, 7, 1 ,\n2000,44-51.\n[3]J.-J. Aucouturier and F. Pachet, “Scaling up Music Playlist\nGeneration”,In: ProceedingsoftheIEEEInternationalCon-\nference on Multimedia and Expo 2002 (ICME2002) , August\n26- 29, 2002, Lausanne, Switzerland.\n[4]S. Pauws, W. Verhaegh and M. Vossen, “Playlist Generation\nby Adapted Simulated Annealing”, In: Vasilakos, A. (Ed.),\nInformation Science: Special Issue on Ambient Intelligence ,\n2006.\n[5]M.R. Garey and D.S. Johnson, Computers and Intractabil-\nity: A Guide to the Theory of NP-Completeness , 1979, W.H.\nFreemanand Company,NewYork.\n[6]E.H.L. Aarts and J.K. Lenstra, Local Search in Combinato-\nrialOptimization ,1997, Wiley.\n[7]S. Kirkpatrick, C.D. Gelatt and M.P. Vecchi, “Optimization\nby Simulated Annealing”, Science, 220, 4598 , 1983, 671–\n680.\n[8]E.P.K. Tsang, Foundations of Constraint Satisfaction , Aca-\ndemicPress, 1993.\n[9]S. Pauws and S. van de Wijdeven. “User Evaluation of a\nNew Interactive Playlist Generation Concept.” In: Proc.\nSixth International Conference on Music Information Re-\ntrieval(ISMIR2005) ,Reiss,J.D.&G.A.Wiggins(Eds.). 11-\n15September 2005, 638–643."
    },
    {
        "title": "Chroma-based estimation of musical key from audio-signal analysis.",
        "author": [
            "Geoffroy Peeters"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1416420",
        "url": "https://doi.org/10.5281/zenodo.1416420",
        "ee": "https://zenodo.org/records/1416420/files/Peeters06.pdf",
        "abstract": "This paper deals with the automatic estimation of key (key- note and mode) of a music track from the analysis of its audio signal. Such a system usually relies on a succes- sion of processes, each one making hypotheses about either the signal content or the music content: spectral representa- tion, mapping to chroma, decision about the global key of the music piece. We review here the underlying hypothe- ses, compare them and propose improvements over current state of the art. In particular, we propose the use of a Har- monic Peak Subtraction algorithm as a front-end of the sys- tem and evaluate the performance of an approach based on hidden Markov models. We then compare our approach with other approaches in an evaluation using a database of 302 baroque, classical and romantic music tracks. Keywords: key estimation, pitch representation, Harmonic Peak Subtraction, hidden Markov model",
        "zenodo_id": 1416420,
        "dblp_key": "conf/ismir/Peeters06",
        "keywords": [
            "key estimation",
            "pitch representation",
            "Harmonic Peak Subtraction",
            "hidden Markov model",
            "audio signal analysis",
            "automatic system",
            "music track",
            "spectral representation",
            "chroma mapping",
            "global key determination"
        ],
        "content": "Chroma-based estimation of musical key from audio-signal analysis\nGeoffroy Peeters\nIrcam - Sound Analysis/Synthesis Team, CNRS - STMS\n1, pl. Igor Stravinsky\nF-75004 Paris - France\npeeters@ircam.fr\nAbstract\nThis paper deals with the automatic estimation of key (key-\nnote and mode) of a music track from the analysis of its\naudio signal. Such a system usually relies on a succes-\nsion of processes, each one making hypotheses about either\nthe signal content or the music content: spectral representa-\ntion, mapping to chroma, decision about the global key of\nthe music piece. We review here the underlying hypothe-\nses, compare them and propose improvements over current\nstate of the art. In particular, we propose the use of a Har-\nmonic Peak Subtraction algorithm as a front-end of the sys-\ntem and evaluate the performance of an approach based on\nhidden Markov models. We then compare our approach\nwith other approaches in an evaluation using a database of\n302 baroque, classical and romantic music tracks.\nKeywords: key estimation, pitch representation, Harmonic\nPeak Subtraction, hidden Markov model\n1. Introduction\nIn the ﬁeld of Music Information Retrieval, automatic es-\ntimation of musical key or of chord progression over time\nfor a music track has received much attention in the re-\ncent years. This comes from the numerous applications it\nallows (search/ query music databases, automatic playlists\ngeneration and automatic accompaniment) and from the fact\nthat recent studies have shown that reasonable results could\nbe achieved without the need of a symbolic transcription\n(which is not always available) and without the necessity\nto extract such a transcription from the audio signal (audio\nto symbolic notes algorithms are still limited and costly).\nIn order to do that, most existing algorithms start by a\nfront-end which converts the signal frames to the frequency\ndomain (DFT or CQT [1] [13]) and then map it to the chroma\ndomain [17] (or Pitch Class Proﬁle [4]). Chroma/ PCP vec-\ntors represent the intensities of the twelve semitones of the\npitch classes over time. Algorithms then try to ﬁnd the key\nor chord progression that best explains the succession of\nchroma-vectors over time. For this, Chew [2] proposes the\nSpiral Array Model/ Center of Effect Generator, many oth-\ners ([5] [6] [3] [10]) use key-chroma proﬁles derived from\nthe probe-tone experiment of Krumhansl & Schmukler [8]\nor from the modiﬁed version proposed by Temperley [15].\nThese experiments were aimed at describing the perceptual\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies\nare not made or distributed for proﬁt or commercial advantage and that\ncopies bear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2006 University of Victoriaimportance of each semitone in a key resulting in a pitch-\ndistribution proﬁle.\nUsually, key estimation systems rely on a succession of\nprocesses, each one making underlying hypotheses about ei-\nther the signal content or the music content. The paper is or-\nganized according to these various processes: 1) extraction\nof information about periodicity or pitches from the audio\nsignal, 2) mapping of this information to the chroma/ PCP\ndomain, 3) decision of a global key for the music piece from\nthe succession of chroma-vectors over time.\nThe ﬁrst problem of current systems comes from the fact\nthat we do not observe directly the various pitches in a spec-\ntral representation (DFT or CQT) but a mixture of their har-\nmonics. This problem can be solved either by extracting\nthe pitches (or removing the harmonics), or by considering\nthe presence of these harmonics during the creation of the\nkey-chroma proﬁles. The ﬁrst choice is taken for example\nby Pauws [10] (using a model taking into account simulta-\nneously the perceptual pitch and the musical background),\nChuan [2] (using a fuzzy analysis system) or Cremer [3]\n(using an overtones removal process). The second choice is\ntaken for example by Gomez [5] (extending the PCP to Har-\nmonic Pitch Class Proﬁles by considering a theoretical am-\nplitude contribution of the ﬁrst 4 harmonics of each pitch of\nthe three main triads in a given key) or Izmirli [6] (the con-\ntribution of the harmonics is there measured on a database of\npiano notes). While the solution in [5] provides a too rough\napproximation (a large part of musical instrument sounds\ndoes not behave as the proposed theoretical spectral enve-\nlope), the solution in [6] would require different spectral en-\nvelope measures for each speciﬁc instrument. In this paper\nwe propose the use a Harmonic Peak Subtraction function,\nwhich allows reducing the inﬂuence of the higher harmonics\nof each pitch.\nA second problem comes from the method used in or-\nder to decide on the global key. A hypothesis is often made\nabout the existence of the key in the beginning of the track.\nTherefore, only the ﬁrst part of the track is considered (ﬁrst\n20s of the ﬁrst movement). Several approaches are taken\nto estimate the global key from the chroma-vectors over\ntime. Gomez [5] chooses the key corresponding to the key-\nchroma proﬁle which has the highest correlation with a global\naverage chroma-vector. Izmirli [6] estimates at each time\nthe key-chroma proﬁle which is the most correlated with\na cumulated over-time-chroma-vector, assigns to it a score\nand ﬁnally takes the key with the maximum average score.\nIn this paper, we will test both methods. We will also test\na proposed decision method based on modeling the succes-\nsion of chroma-vectors over time by a set of hidden Markov\nmodels representing the various keys.Sound\r\nSpectrum (DFT)\r\nChroma\rHarmonic Peak\r\nSubtraction\r\nCognitive\r\nKey-chroma\r\nprofiles\rHMM\r\ndecoding\r\nKey\r\n(key-note/ mode)\rKrumhansl\r\nTemperley\r\nDiatonic\rHarmo.\r\ncontrib.\r\nMain triads\rPre-processing\r\nSilence detection, Tuning\r\nScale (lin, ener, sone)\r\nHMM for CM\r\nHMM for DbM\r\nHMM for Cm\r...\r\nDecision\r\nmethod\rHMM for C#m\r\n...\rFigure 1. Global ﬂowchart of the key estimation system.\nThe paper is organized as follows. In section 2.2, we\npropose our Harmonic Peak Subtraction function for spec-\ntral observation of periodicities. In section 2.3, we propose\nthe mapping of it to the chroma domain. We show the im-\nportance of the scale used for the mapping and propose the\nuse of a sone scale. In section 2.4, we present the various\nkey decision methods and propose a method based on hid-\nden Markov modeling of the keys. In section 3, we evaluate\nthe performances of our system in comparison with various\nother systems. The evaluation is performed using a database\nof 302 baroque, classical and romantic music tracks.\n2. Key estimation system\nThe global ﬂowchart of the key estimation system is indi-\ncated in Figure 1. We detail it in the following.\n2.1. Pre-processing stages\nA set of pre-processing algorithms are ﬁrst applied to the\nsignal. The signal is ﬁrst down-sampled to 11025Hz and\nconverted to mono by mixing both channels. The exact\nstarting time of the music piece in the sound ﬁle is estimated\nby a method based on loudness and spectral ﬂatness mea-\nsure. The tuning of the track is then found using the method\nwe have proposed in [12]. In short, we test a set of candidate\ntunings between 427Hz and 452Hz (the quarter-tones below\nand above A4). For each candidate tuning, we estimate the\namount of energy of the spectrum explained by the frequen-\ncies corresponding to the semitones based on this candidate\ntuning. For the database we will use in section 3, we have\nfound tunings ranging from 438 to 447Hz with concentra-\ntion at 440Hz and 443Hz. Using this estimation, the signal\nis re-sampled (using a polyphase ﬁlter implementation) in\norder to bring its tuning back to 440Hz. The rest of the sys-\ntem is based on a tuning of 440Hz.\n2.2. Spectral observation: Harmonic Peak Subtraction\nThe front-end of most key estimation systems extracts a\nspectral representation from the signal. Since this represen-\ntation will be mapped to the chroma domain, it is important\nthat it represents only information about the pitches and notall their harmonics. Indeed, the presence of the harmon-\nics of the pitches will distort the chroma representation (for\nexample the harmonics h= 3,6will strengthen the pres-\nence of the ﬁfth note and h= 5 the presence of the third)\nand induce error in the key estimation (especially the ﬁfth\nup/down confusion). In this paper we propose the use of a\nHarmonic Peak Subtraction function, which allows reducing\nthe inﬂuence of the higher harmonics of each pitch.\nIn the case of mono-pitch signals , we have proposed in\n[11] a function which combines a frequency representation\nS(fk)(the DFT or the Auto-Correlation of the DFT) with\na temporal representation r(τl)(the Auto-Correlation of the\nsignal or the Real-Cepstrum function) mapped to the fre-\nquency domain. The mapping consists in considering that\nthe value of r(τl)is a measure of the periodicity at lag τl\nor at the frequency 1/τl. We interpolate the values of r(τl)\nin order to obtain the values of r(τ)at the same frequency\nas the DFT τ= 1/fk. Only the positive values of r(1/fk)\nare considered (Half Wave Rectiﬁcation). We now have two\nmeasures of the periodicity at the same frequencies fkand\nthe ﬁnal function is obtained by computing the product of\nboth: h(fk) = S(fk)·r(1/fk). This function has been\ntested in [11] for a task of pitch estimation. For this, we sim-\nply take the frequency corresponding to the maximum peak\nofh(fk)as the pitch estimation. This process has achieved\n97% correct recognition over a large database.\nThe underlying process of this method is that the ACF\n(or Real-Cepstrum) r(τ)can be considered as the decom-\nposition of the power spectrum (log-amplitude spectrum),\nA(fk), on a cosine function gτ(fk) = cos(2 πfkτ)and there-\nfore measures the periodicity of the peaks of A(fk). This is\nillustrated in Figure 2 where we superimposed gτ(fk)on\nA(fk)for various lags: τ=T0/5,τ=T0andτ= 2T0.\nWe decompose gτ(fk)into its positive and negative part:\ngτ(fk) =g+\nτ(fk)−g−\nτ(fk).Positive values ofr(τ)occur\nonly when the contribution of the projection of A(fk)on\ng+\nτ(fk)is greater than the one on g−\nτ(fk)(this is the case\nfor the sub-harmonics of f0,τ=k/f0, k∈N+).Non-\npositive values occur when the contribution of g−\nτ(fk)is\nlarger than or equal to the one of g+\nτ(fk)(this is the case for\nthe higher harmonics of f0,τ= 1/(kf0), k > 1, k∈N+).\nOn the other side, energy in the spectrum S(fk)only exist\nforf=f0,2f0, ... so that when multiplying S(fk)and\nr(1/fk)only the peak at f=f0remains.\nThis function is not a pitch detection algorithm but a rep-\nresentation that strengthens the energy at the pitch frequency\nand reduces the energy at the other harmonics. Because of\nthat we would like to use this method as a front-end for key\nestimation which would therefore avoid the effect we have\nmentioned above about the presence of higher harmonics.\nHowever in the case of multi-pitch signals , the above-\nmentioned function cannot be applied directly. For multi-\npitch signal, the relationship between r(τ)and the period-\nicity of the various pitches becomes intricate. We therefore\nuse the same underlying process but without the use of the\nprojection on cosine functions. This process can be summa-\nrized as testing the hypothesis that fkis a pitch (value given\nby the projection of A(fk)ong+\nτ(fk)) against the hypothe-\nsis that fkis a higher harmonic (projection on g−\nτ(fk)) or a\nlower harmonic of another pitch (multiplication by S(fk))1.\n1It should be noted that this method does not allow to solve the missing0 5 10 15−1−0.500.51\nFrequency [Hz]Amplitude\n00.5 11.5 22.5 33.5 44.5 5−1−0.500.51\nLag [sec]AmplitudeDFT\ncosine at τ=T0/5\nf=5f0\ncosine at τ=T0\nf=f0\ncosine at τ=2T0\nf=f0/2\nACF\nτ=T0/5\nτ=T0\nτ=2T0Figure 2. [Top] Magnitude of the DFT of the signal; super-\nimposed: cosine at τ=T0/5, T0,2T0andf= 5f0, f0, f0/2;\n[bottom] ACF function; superimposed: τ=T0/5,τ=T0,\nτ= 2T0positions; on a periodic signal at f0=1/T0=2Hz.\nWe ﬁrst compute at each frame the energy spectrum (or\nthe log-amplitude spectrum) A(k). For each frame and each\nfrequency fkwe then compute a score2deﬁned as\nˆr(fk) =HX\nh=1A(hfk)−max{α(fk), β(fk), γ(fk)}(1)\nα(fk) =H−1X\nh=0Aµµ\nh+1\n2¶\nfk¶\nβ(fk) = min\nh∈{1\n3,2\n3,4\n3,5\n3}A(hfk)\nγ(fk) = min\nh∈{1\n5,2\n5,3\n5,4\n5}A(hfk)(2)\nˆr(fk)is the sum of the energy (log-amplitude) of the spec-\ntrum explained by the hypothesis that fkis a pitch, penalized\nby the hypothesis that fkis an even ( α), third ( β) or ﬁfth ( γ)\nharmonic of a lower pitch.\n•The ﬁrst term of (1) is equivalent to the projection of\nA(fk)ong+\nτ(fk)but using a narrower and constant-\nover-τfrequency bandwidth basis3. It is the sum of\nthe harmonic of the current frequency fk.\n•αpenalizes frequencies which are even harmonics of\na lower pitch (the current frequency is potentially the\nsecond, fourth, sixth, ... harmonic of a lower pitch).\n•βpenalizes frequencies which have third harmonic\nrelationship with a lower frequency. We make the un-\nderlying assumption of spectral envelope continuity\nby taking the minimum over the considered harmon-\nics: if fkwas the 3rd harmonic of a pitch fk/3then\nenergy should be present at1\n3,2\n3,4\n3,5\n3.\n•γis the same as βbut for the ﬁfth harmonic.\nfundamental problem.\n2This score plays the same role as r(fk)in the previous method; how-\neverˆr(fk)is directly computed at the frequencies fkof the DFT bins.\nTherefore no interpolation is required.\n3The frequency bandwidth corresponding to the positive-valued part of\ngτ(f)varies with τ.\n0 500 1000 1500 2000 2500 30000501001502002503001\n21\n21\n21\n2\n33 33\n5555\n0 500 1000 1500 2000 2500 30000246810x 104Figure 3. [Top] Magnitude spectrum A(fk); [Bottom] Har-\nmonic Peak Subtraction function ˆh(fk)(see text).\nTime [sec]Frequency [Hz]\n0 2 4 6 8 10050010001500\nTime [sec]Frequency [Hz]\n0 2 4 6 8 10050010001500\nFigure 4. [Top] Spectrogram S(fk, t); [Bottom] Harmonic\nPeak Subtraction function over time ˆh(fk, t)(see text).\nWe ﬁnally take the maximum over the three penalties.\nAs for the mono-pitch case, we then compute the product of\nboth measures: ˆh(fk) =S(fk)·ˆr(fk).\nWe illustrate the computation of the Harmonic Peak Sub-\ntraction function in Figure 3 for a multi-pitch signal which is\nthe superposition of C4 (261.6Hz), C#4 (277.2Hz), and F5\n(698.5Hz) viola sounds. The upper part represents A(fk).\nThe frequency being analyzed with ˆr(fk)is 698Hz. We su-\nperimposed the various possible interpretation of this fre-\nquency: - 1stharmonic of a F5 note (1), - 2ndharmonic of\na F4 (2) - 3rdharmonic of a A#3 (3) or - 5thharmonic of a\nC#3 (5). The resulting function ˆh(fk)is represented in the\nlower part of the ﬁgure showing emphasis on the C4, C#4,\nand F5 frequencies. In Figure 4, we illustrate the results of\nthe Harmonic Peak Subtraction function over time in com-\nparison with the spectrogram on the ﬁrst 10s of J.S. Bach,\nWell-Tempered Clavier, 02 Fugue in CM.\nIn the rest of the paper, the results are presented for A(fk)\ncorresponding to the log-amplitude spectrum and S(k)to\nthe magnitude spectrum. The use of a log-amplitude scale\nallows reducing the inﬂuence of the spectral envelope of\neach instrument in the computation of ˆh(fk).2.3. Mapping to chroma scale\nShepard [14] proposes to represent the pitch as a two di-\nmensional structure: the tone height (octave number) and\nthe chroma (pitch class). Based on that, the chroma spec-\ntrum or Pitch Class Proﬁle (PCP) has been proposed in order\nto map the values of the Fourier transform (or Constant-Q\ntransform) frequencies to the 12 semi-tones pitch classes C.\nIn our system, we ﬁrst map the values of the Fourier\ntransform to a semi-tone pitch spectrum , smooth the cor-\nresponding channels over time and then map the results to\nthesemi-tone pitch class spectrum (chroma spectrum).\nSemi-tone pitch spectrum: The mapping function be-\ntween the frequencies fkof the Fourier transform and the\nsemi-tone pitch scale n(expressed in a midi-note scale) is\ndeﬁned as:\nn(fk) = 12 log2µfk\n440¶\n+ 69 n∈R+(3)\nThe computation of the semi-tone pitch spectrum is made\nusing a set of ﬁlters Hn/primecentered on the semi-tone pitch fre-\nquencies n/prime∈[43,44, . . . , 95](corresponding to the notes\nG2 to B6 or the frequencies 98Hz to 1975 Hz). In order to\nincrease the “pitch resolution”, we deﬁne a factor R∈N+\nwhich ﬁxes the number of ﬁlters used to represent one semi-\ntone. The center of the ﬁlters are now deﬁned by n/prime∈\n[43,43 +1\nR,43 +2\nR, . . . , 95]. Each ﬁlter is deﬁned by the\nfunction\nHn/prime(fk) =1\n2tanh ( π(1−2x)) +1\n2(4)\nwhere xis the relative distance between the center of\nthe ﬁlter n/primeand the frequencies of the Fourier transform:\nx=R|n/prime−n(fk)|. The ﬁlters are equally spaced and sym-\nmetric in the logarithmic semi-tone pitch scale, extend from\nn/prime−1ton/prime+ 1with a maximum value at n/prime.\nThe values of the semi-tone pitch spectrum N(n/prime)are\nthen obtained by multiplying the Fourier transform values\nA(fk)by the set of ﬁlters Hn/prime:\nN(n/prime) =X\nfkHn/prime(fk)A(fk) (5)\nSmoothing: The semi-tone pitch spectrum N(n/prime)is\ncomputed for each frame t. The output signal of each ﬁl-\nterN(n/prime, t)is then smoothed over time using median ﬁl-\ntering. This provides a reduction of transients and noise in\nthese signals. Also, for the rest of the process, only the ﬁl-\nters centered on the exact semi-tone pitches are considered\n(i.e. among the Rﬁlters representing one semi-tone, we\nonly consider the middle one; for example if R= 3, we\nonly keep n’=69 but not n’=68.666 and n’=69.333). We can\ndo this because the tuning is guaranteed to be 440 Hz. This\nprocess also allows a reduction of the inﬂuence of noise in\nthe computation of the chroma spectrum.\nSemi-tone pitch class spectrum (chroma spectrum):\nThe mapping function between the semi-tone pitches nand\nthe semi-tone pitch classes (chroma) cis deﬁned as c(n) =\nmod(n,12). The mapping to the 12-chroma scale vector\nC(l)(pitch classes) is achieved by adding the equivalent\npitch classes\nC(l) =X\nn/primeso that c(n/prime)=lN(n/prime)l∈[0,12[ (6)Parameters: The analysis is performed using Short Time\nFourier Transform with a window of type Blackman, length\n371.5ms and 50% overlap. Because of frequency resolution\nlimits (the frequency distance between adjacent semi-tone\npitches becomes small in low frequency), we only consider\nfrequencies above 100Hz. The upper limit is set to 2000Hz.\nThe value of Ris set to 3.\nChoice of the amplitude scale: The choice of A(fk)in\n(5) plays a crucial role. In section 3, we will compare the use\nof the DFT and HPS for A(fk). We will also test for each\ncase, the inﬂuence of the scale of A(fk): amplitude scale,\nenergy scale but also a sone-converted value scale. The sone\nvalues are obtained in a similar way as proposed in [9]:\nAs(fk) = 21\n10(Adb(fk)−40)ifAdb(fk)>40\n= 1/40Adb(fk)2.642else(7)\nwhere Adb(fk) = 10 log10(A(fk)1096/20)is the spectrum\nafter scaling to the maximum signal resolution and conver-\nsion to decibel scale.\n2.4. Key estimation from the chroma-vectors\nThe key is estimated from the succession of chroma-vectors\nover time C(t). This can be done in several ways.\n2.4.1. Key estimation based on cognitive models\nKey-chroma proﬁles: This approach is close to the one\nproposed in [5]. The key proﬁles are created by extending\nthe Temperley-Diatonic pitch-distribution-proﬁles4to the\npolyphonic case (several pitches) by considering the con-\ntribution of the three main triads (tonic, dominant and sub-\ndominant) in each key. The key proﬁles can be further ex-\ntended to the audio case (harmonics of each pitch) by con-\nsidering a contribution of the hharmonic of each pitch with\nan amplitude of 0.6h−1. In [5], the ﬁrst H=4 harmonics are\nconsidered. In section 3, we will test the simple polyphonic\nmodel (H=1) and the extended-to-audio model (H=4). In\nboth cases, the result is a 12-dimensions key-chroma proﬁle\nfor each of the 24 keys: ˆCii∈[1,24].\nKey decision method: In the following we note C(t)\nthe 12-dimension chroma-vector extracted from the signal\nat time t. [5] proposes to estimate the global key of a piece\nas the key-chroma proﬁle Ciwhich has the highest correla-\ntion with an averaged over time chroma-vector: max{ˆCi·\nµ(C(t))}We have found better results using the maximum\nof the average correlation between key-chroma proﬁles and\ninstantaneous chroma-vectors: max{µ(ˆCi·C(t))}. We call\nthis method MeanInstCorrel . We also test the decision method\nproposed in [6]. At each time t, we estimate the Cithat has\nthe highest correlation with a cumulated-over-time chroma-\nvector5. We attribute a score to this key proportional to\nthe distance between its correlation value and the correla-\ntion value of the second most likely key. This score acts as\na reliability coefﬁcient. The ﬁnal key is chosen as the one\nwith the maximum score cumulated over time. We call this\nmethod ScoreCorrelCumul .\n4During our experiment, we have found better results using the\nTemperley-Diatonic proﬁles proposed in [6] than the Krumhansl ones.\n5At time t, the cumulated-over-time chroma-vector is computed by\naveraging the chroma-vector C(τ)since the beginning of the track:\n1/tPt\nτ=0C(τ).2.4.2. Key estimation based on HMM\nIn [12], we have proposed a method for key estimation based\non training a set of hidden Markov models on the chroma\nrepresentation corresponding to the various keys. No a pri-\nori musical knowledge at all is introduced in this method.\nThe characteristics (signal and musical) of the keys are learned\ndirectly from the training. In this case the chroma-vectors\nwere derived from the DFT expressed in sone scale. 24\nmodels corresponding to each possible key need to be trained.\nHowever, because the number of instances in our database\nstrongly differs among the 24 keys, training directly the HMMs\non the set of items belonging to a speciﬁc key could lead to\nover-ﬁtting (learning the track characteristics instead of the\nkey characteristics). We therefore start by training only two\nmodels, a Major and minor mode model, and then map the\ntwo trained models to the various possible keys. For this the\nchroma-vectors of all the tracks are mapped to a key-note of\nC (by using circular permutation of chroma-vectors). Two\nHMMs are trained corresponding to C Major and C minor.\nThe training is done using the Baum-Welsh algorithm. The\nparameters of the two models are then used to construct the\n24 HMMs corresponding to the various keys. This is done\nby circular permutation of the mean vectors and covariance\nmatrices of the state observation probability. For a song with\nunknown key, we evaluate the log-likelihood (using the for-\nward algorithm) that its chroma-vector sequence has been\nproduced by each of the 24 HMMs. The model with the\nmaximum log-likelihood gives the key. In a 10-fold cross-\nvalidation, we have obtained the best results using the fol-\nlowing conﬁguration: 3 hidden states / 3 Gaussians per state\n(GMM) with diagonal matrices.\n3. Evaluation\nSo far, we have proposed several alternatives for each stage\nof the key estimation system. We would like now to com-\npare the effect of each of them on the global recognition rate.\nWe would especially like to test the inﬂuence of: •the peri-\nodicity observation (DFT or HPS), •the scale used to repre-\nsent the value before the mapping to chroma (amplitude, en-\nergy or sone scale), •the value of Hin the key-chroma pro-\nﬁles (H=1 means ignoring the harmonic contribution, H=4\nmeans considering the ﬁrst four harmonics), •the key de-\ncision method (MeanInstCorrel or ScoreCorrelCumul). We\nalso test the performances of the HMM-based approach [12]\napplied directly to the chroma-vectors derived from the DFT\nin sone scale.\nFor each track of the database, only the ﬁrst 20s are an-\nalyzed. We therefore make the underlying hypothesis that\nthe main key is used in the beginning of the track but not\nnecessarily right at the beginning of the track (as it is often\nthe case in romantic music).\n3.1. Test set\nA database of 302 European baroque, classical and roman-\ntic music extracts have been created. This includes pieces\nby Bach (48), Corelli (12), Handel (16), Telleman (17), Vi-\nvaldi(6), Beethoven (33), Haydn (23), Mozart (33), Brahms\n(32), Chopin (29), Dvorak (18), Schubert (23), Schuman (7).\nThe pieces are for solo keyboard (piano, harpichord), cham-\nber and orchestra music. It should be noted that no opera orTable 1. Distribution of the test set.\nTable 2. MIREX score (MI), recognition rate of key (KE), key-\nnote (KN) and mode (MO) for various conﬁgurations.\nchoir music was considered in the present study. The distri-\nbution of the test set is indicated in Table 1. As in [6], the\ndatabase was derived from the NAXOS web radio service.\nThe ground-truth key (key-note and mode) was derived from\nthe title of the piece. Only the ﬁrst movement of each piece,\nsupposed to correspond to the provided key, was used. Note\nthat we had to manually correct the annotation of part of\nthe baroque pieces, since they were based on a tuning of\nA4=415Hz.\n3.2. Results\nIn Table 2, we indicate the recognition rate of key (KE),\nkey-note alone (KN), and mode alone (MO). We also in-\ndicates the score used for the MIREX-2005 key estimation\ncontest (MI)6. According to this evaluation the best recog-\nnition rate of key (KE), as well as the highest MIREX score\n(MI) are obtained using the HPS using a sone scale and the\nScoreCorrelCumul decision method (MI=89.1%, KE=84.8%).\nWe also see that changing only one of the processes (scale,\nH or decision method) can change drastically the results.\nWhat conclusion can we draw from this evaluation?\nConcerning the choice of a speciﬁc decision method: there\nis no clear trend in the results, the highest value of MI is not\nnecessarily obtained with the same decision method as the\nhighest value of KE.\n6This score uses the following weights: - 1 for correct key estimation,\n- 0.5 for perfect ﬁfth relationship between estimated and ground-truth key,\n- 0.3 if detection of relative major/minor key, - 0.2 if detection of parallel\nmajor/minor key.Table 3. Recognition rate of key by music genre and instru-\nmentation type (HPS, sone scale, ScoreCorrelCumul).\nConcerning the choice of a scale: the choice of the en-\nergy scale systematically decrease both MI and KE. The am-\nplitude and sone scale give very close results in the case of\nthe DFT, but the sone scale surpasses the amplitude in the\ncase of the HPS.\nConcerning the value of H:in the case of the DFT, H=4\nallows increasing MI and KE (this can be understand by\nthe fact that the DFT does not remove the higher harmon-\nics contribution therefore it is necessary to include it in the\nkey-chroma proﬁles), while in the case of the HPS choosing\nH=4 decreases the results (for the opposed reason).\nConcerning the choice of the periodicity observation (DFT\nor HPS) : in the case of the sone scale, the HPS surpasses the\nDFT, however this is not the case for the amplitude scale.\nWhy are the results better in sone scale for the HPS ?:\nThis can be explained considering Figure 3 where we see\nthat the HPS allows to emphasize the existing pitch frequen-\ncies but does not provides an accurate estimation of their\namplitude. Because the sone scale performs a compression\nofA(fk)it provides a reduction in the amplitude discrep-\nancy.\nThe last row of Table 2, indicates the recognition rate\nobtained using the HMM-based approach. This result has\nbeen obtained using a ten-fold cross-validation. It is inter-\nesting to consider that this method without any introduction\nof musical knowledge achieves quiet reasonable results (the\nKN value is very close to our winning algorithm).\nTo conclude we indicate in Table 3, the MI score of the\nwinning algorithm by music genre and instrumentation type.\nThis table emphasizes the fact that the results strongly de-\npend on the considered music genre. The lowest recognition\nrate is obtained for the romantic period (81.8%). Part of the\ntracks of this period (Brahms, Schuman) actually contains\nmainly a neighboring key in the ﬁrst 20s.\n4. Conclusion and Future Work\nIn this paper, we have presented a system for the automatic\nestimation of key based on chroma representation. The main\ncontribution of this paper is the Harmonic Peak Subtraction\nfunction expressed in a sone scale to be used as front-end\nfor spectral representation of the signal. We have tested\nvarious way of estimating the key from the succession of\nchroma-vector over time including a proposed HMM-based\napproach. In an evaluation using a database of 302 baroque,\nclassical, romantic music tracks, the best results (89.1% MIREX\nscore) were obtained using our HPS function in sone scale\nwith Gomez polyphonic key-chroma proﬁles and Izmirli score-\nbased key decision method. It is however worth mentioning\nthat the results obtained during the evaluation strongly de-\npends on the music period. The more harmonically com-\nplex romantic music has a lower recognition rate than thebaroque and classical music. This could indicate the limita-\ntion of such a straightforward approach for key estimation.\nOn the signal side, future works will concentrate on im-\nproving the amplitude associated to the peaks of the HPS\nfunction. We would also like to test the performance of\na multi-pitch detection algorithm ([7][18]) mapped to the\nchroma domain in order to know the limits of the chroma-\nbased approach. On the music analysis side, the key deci-\nsion method should certainly be improved in order to take\ninto account potential modulation over time, this was in fact\nthe prime reason for testing the HMM approach.\nAcknowledgments\nPart of this work was conducted in the context of the Euro-\npean I.S.T. project Semantic HIFI [16]7. To my father.\nReferences\n[1] J. Brown. Calculation of a constant Q spectral transform.\nJASA , 89(1):425–434, 1991.\n[2] C.-H. Chuan and E. Chew. Fuzzy analysis in pitch class\ndetermination for polyphonic audio key ﬁnding. In ISMIR ,\npages 296–303, London, UK, 2005.\n[3] M. Cremer and C. Derboven. A system for harmonic analysis\nof polyphonic music. In AES 25th Int. Conf , pages 115–120,\nLondon, UK, 2004.\n[4] T. Fujishima. Realtime chord recognition of musical sound:\na system using common lisp music. In ICMC , pages 464–\n467, Bejing, China, 1999.\n[5] E. Gomez. Tonal description of polyphonic audio for mu-\nsic content processing. INFORMS Journal on Computing,\nSpecial Cluster on Computation in Music , 18(3), 2006.\n[6] O. Izmirli. Template based key ﬁnding from audio. In ICMC ,\npages 211–214, Barcelona, Spain, 2005.\n[7] A. Klapuri. A perceptually motivated multiple-f0 estimation\nmethod. In WASPAA , New Paltz, New York, 2005.\n[8] C.-L. Krumhansl. Cognitive foundations of musical pitch .\nOxford University Press, New-York, 1999.\n[9] E. Pampalk, S. Dixon, and G. Widmer. Exploring music\ncollections by browsing different views. In ISMIR , pages\n201–208, Baltimore, USA, 2003.\n[10] S. Pauws. Musical key extraction from audio. In ISMIR ,\npages 96–99, Barcelona, Spain, 2004.\n[11] G. Peeters. Music pitch representation by periodicity mea-\nsures based on combined temporal and spectral representa-\ntions. In ICASSP , Toulouse, France, 2006.\n[12] G. Peeters. Musical key estimation of audio signal based\non HMM modeling of chroma vectors. In DAFX , McGill,\nMontreal, Canada, 2006.\n[13] H. Purwins, B. Blankertz, and K. Obermayer. A new method\nfor tracking modulations in tonal music in audio data format.\nInNeural Networks, IJCNN, IEEE Computer Society , 2000.\n[14] R. Shepard. Circularity in judgements of relative pitch.\nJASA , 36:2346–2353, 1964.\n[15] D. Temperley. What’s key for key? the Krumhansl-\nSchmuckler key ﬁnding algorithm reconsidered. Music Per-\nception. , 17(1):65–100, 1999.\n[16] H. Vinet. The Semantic Hiﬁ project. In ICMC , pages 503–\n506, Barcelona, Spain, 2005.\n[17] G. Wakeﬁeld. Mathematical representation of joint time-\nchroma distributions. In SPIE , Denver, USA, 1999.\n[18] C. Yeh, A. Roebel, and X. Rodet. Multiple fundamental fre-\nquency estimation of polyphonic music signals. In ICASSP ,\nvolume 3, pages 225–228, Philadelphia, PA, USA, 2005.\n7http://shf.ircam.fr"
    },
    {
        "title": "A computationally efficient speech/music discriminator for radio recordings.",
        "author": [
            "Aggelos Pikrakis",
            "Theodoros Giannakopoulos",
            "Sergios Theodoridis"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1414862",
        "url": "https://doi.org/10.5281/zenodo.1414862",
        "ee": "https://zenodo.org/records/1414862/files/PikrakisGT06.pdf",
        "abstract": "This paper presents a speech/music discriminator for radio recordings, based on a new and computationally efficient re- gion growing technique, that bears its origins in the field of image segmentation. The proposed scheme operates on a single feature, a variant of the spectral entropy, which is ex- tracted from the audio recording by means of a short-term processing technique. The proposed method has been tested on recordings from radio stations broadcasting over the In- ternet and, despite its simplicity, has proved to yield perfor- mance results comparable to more sophisticated approaches. Keywords: Speech/music discrimination, spectral-entropy, region growing techniques",
        "zenodo_id": 1414862,
        "dblp_key": "conf/ismir/PikrakisGT06",
        "keywords": [
            "speech/music discriminator",
            "region growing technique",
            "audio recordings",
            "spectral entropy",
            "Internet radio stations",
            "performance results",
            "sophisticated approaches",
            "short-term processing",
            "radio recordings",
            "field of image segmentation"
        ],
        "content": "A computationally efﬁcientspeech/musicdiscriminatorforradio re cordings\nAggelos Pikrakis,TheodorosGiannakopoulos andSergiosTheo doridis\nUniversity of Athens\nDepartment of Informaticsand Telecommunications\nPanepistimioupolis, 15784, Athens, Greece\n{pikrakis, tyiannak, stheodor }@di.uoa.gr\nAbstract\nThis paper presents a speech/music discriminator for radio\nrecordings,basedonanewandcomputationallyefﬁcientre-\ngion growing technique, that bears its origins in the ﬁeld of\nimage segmentation. The proposed scheme operates on a\nsinglefeature,avariantofthespectralentropy,whichise x-\ntracted from the audio recording by means of a short-term\nprocessingtechnique. Theproposedmethodhasbeentested\non recordings from radio stations broadcasting over the In-\nternet and, despite its simplicity, has proved to yield perf or-\nmanceresultscomparabletomoresophisticatedapproaches .\nKeywords: Speech/music discrimination, spectral-entropy,\nregion growing techniques\n1. Introduction\nThe problem of speech/music discrimination is important\nin a number of audio content characterization applications .\nSince the ﬁrst attempts in the mid 90’s, a number of algo-\nrithms have been implemented in various application ﬁelds.\nThemajorityoftheproposedmethodsdealwiththeproblem\nin two separate steps: ﬁrstly, the audio signal is split into\nsegmentsbydetectingabruptchangesinthesignalstatisti cs\nand at asecond steptheextracted segments areclassiﬁedas\nspeech or musicby usingstandard classiﬁcation schemes.\nOne of the ﬁrst methods focused on the real-time, auto-\nmatic monitoring of radio channels, using energy and zero-\ncrossing rate (ZCR) as features [1]. In [2], thirteen audio\nfeatures were used to train different types of multidimen-\nsional classiﬁers, including a Gaussian MAP estimator and\na nearest neighbor classiﬁer. In [3], energy, ZCR and fun-\ndamental frequency were used as features and segmenta-\ntion/classiﬁcation was achieved by means of a procedure\nbased on heuristic rules. A similar approach was proposed\nin[4]. FrameworksbasedoncombinationsofstandardHid-\nden Markov Models, Multilayer Perceptrons and Bayesian\nNetworks were used in [5] and [6]. An Adaboost-based al-\ngorithm, applied on the spectrogram of the audio samples,\nPermissiontomakedigitalorhardcopiesofallorpartofthisw orkfor\npersonal or classroom use is granted without fee provided th at copies\narenotmadeordistributedforproﬁtorcommercialadvantagean dthat\ncopies bearthis notice andthefull citation ontheﬁrst page .\nc/circlecopyrt2006University ofVictoriawasusedin[7]. Theauthorsin[8]haveusedGaussianMix-\ntureModelingonasinglefeature,calledWarpedLPC-based\nspectralcentroid,fortheclassiﬁcationofpre-segmented au-\ndio data tospeech and music.\nInthispaper,adifferentphilosophyisadopted,thatbears\nitsoriginsintheﬁeldofimagesegmentation. Themainidea\nis that, if speech/music discrimination is treated as a seg-\nmentationproblem(whereeachsegmentislabeledaseither\nspeechormusic),theneachofthesegmentscanbetheresult\nof a segment (region) growing technique, where one starts\nfrom small regions (segments) and keeps expanding them\nas long as certain criteria are fulﬁlled. This approach has\nbeen used in the past in the context of image segmentation,\nwhere a number of pixels are usually selected as candidates\n(seeds) for region growing. In image segmentation, regions\ngrow by attaching neighboring pixels, provided that certai n\ncriteria are fulﬁlled. These criteria usually examine the r e-\nlationship between statistics drawn from the region and the\npixel values tobeattached.\nFollowing this philosophy, a feature sequence is ﬁrst ex-\ntracted from the audio recording, by means of a short-term\nprocessing technique. To this end, a variant of the spectral\nentropy is extracted per short-term frame. Once the feature\nsequence is generated, a number of frames are selected as\ncandidates for region expansion. Starting from these seeds ,\nsegments grow and keep expanding as long as the standard\ndeviationofthefeaturevaluesineachregionremainsbelow\na pre-deﬁned threshold. In the end, adjacent segments are\nmerged and short segments are eliminated. All segments\nthat have survived are labeled as music, whereas the rest\nof the feature sequence is tagged as speech. The novelty\nof our approach lies in the fact that ideas from the ﬁeld of\nimage segmentation are applied in the context of off-line\nspeech/musicdiscrimination,yieldingacomputationally ef-\nﬁcientalgorithmthatachieveshighdiscriminationaccura cy.\nThe paper is organized as follows: the next Section fo-\ncuses on feature extraction, Section 3 presents the region\ngrowing technique, the performance of the proposed algo-\nrithm is discussed in Section 4 and ﬁnally conclusions are\ndrawn inSection 5.\n2. Feature extraction\nAtaﬁrststep,theaudiorecordingisbrokenintoasequence\nof non-overlapping short-term frames ( 46.4ms long). Fromeach frame, a variant of the spectral entropy [9] is extracte d\nasfollows,bytakingintoaccountthefrequencyrangeupto\napproximately2KHz(bydeﬁnition,entropyisameasureof\nthe uncertainty ordisorder inagiven distribution[10]):\n•All computations are carried on a mel-scale, i.e., the\nfrequency axis iswarped according tothe equation\nf= 1127 .01048 ∗log(fl/700 + 1)\nwhere flisthe frequency value on alinear scale.\n•The mel-scaled spectrum of the short-term frame is\ndivided into Lsub-bands (bins). The center frequen-\ncies of the sub-bands are chosen to coincide with the\nfrequencies of semitones ofthe chromatic scale, i.e.,\nfk= 1127 .01048 ∗log(f0∗2k\n12\n700+1),k= 0,... ,L −1\nwhere f0is the center frequency of the lowest sub-\nband of interest (onalinear scale).\n•Theenergy Xiofthe i-thsub-band, i= 0,... ,L −1,\nis then normalized by the total energy of all the sub-\nbands, yielding ni=Xi/summationtextL−1\ni=0Xi,i= 0,... ,L −1.\nTheentropyofthenormalized spectralenergy isthen\ncomputed by theequation:\nH=−L−1/summationdisplay\ni=0ni·log2(ni) (1)\nIn the sequel we will also refer to this feature by the term\n“chromatic entropy”. At the end of the feature extraction\nstage, the audio recording is thus represented by the featur e\nsequence F, i.e.,F={O1,O2,... ,OT}, where Tis the\nnumber of short-term frames. Figure 1 presents the feature\nsequence that has been extracted from a BBC radio record-\ning, the ﬁrst half of which corresponds to speech and the\nsecond half corresponds to music. It can be observed that\nthe standard deviation of chromatic entropy is signiﬁcantl y\nlower forthe caseof music.\n3. SegmentationAlgorithm\nOncethefeaturesequencehasbeenextracted,speech/music\ndiscrimination is achieved by means of a region growing\ntechnique. The main idea behind this approach is that, at\nan initialization stage, a number of frames are selected as\n“seeds”, i.e., as candidates, that will serve as the basis to\nform regions (segments). Subsequently, by means of an it-\nerative procedure, these regions will grow (expand) while a\ncriterion related to the standard deviation of the chromati c\nentropyisfulﬁlled. Theprocedureisrepeateduntilnomore\nregion growing takes place. At a ﬁnal step, neighboring re-\ngions are merged and after merging, regions that do not ex-\nceed a pre-speciﬁed length are eliminated. At the end of0 5 10 15 20 25 300.511.522.533.544.555.5\nTime (secs)Chromatic Entropy\nFigure 1. Chromatic entropy for 26seconds of a BBC radio\nrecording.\nthis procedure, all segments that have survived correspond\ntomusic,whereasallframesthatdonotbelongtosuchseg-\nmentsareconsideredtobespeechsegments. Theprocedure\nisdescribed indetail as follows:\nInitialization step - Seed generation : IfTis the length\nofthefeaturesequence,a“seed”ischosenevery Mframes,\nMbeing a pre-deﬁned constant. If Kis the total number\nof seeds and ikis the frame index of the k-th seed, then the\nframeindexes ofthe seeds form theset\n{i1,i2,... ,i K}\nThek-th seed is considered to form a region, Rkconsisting\nofasingleframe,i.e., Rk={Oik}where Oikisthefeature\nvalue of therespective frame.\nIteration: In this step, every region, Rk, is expanded by\nexamining the feature values of the two frames that are ad-\njacent to the boundaries of Rk. To this end, let lkandrk\nbetheindexesthatcorrespondtotheleftmostandrightmost\nframesof Rkrespectively. Clearly,if Rkconsistsofasingle\nframe, then lk=rk=ik. Following this notation, lk−1\nandrk+1aretheindexesofthetwoframeswhichareadja-\ncent to the left and right boundary of Rk, respectively. Our\nalgorithm decides to expand Rkto include Olk−1, ifOlk−1\nis not already part of any other region and if the standard\ndeviationofthefeaturevaluesofthisexpandedregionisbe -\nlow a pre-deﬁned threshold Th, common to all regions. In\nother words, if the standard deviation of feature values for\nOlk−1∪Rkis less than Th, then, at the end of this step Rk\nwillhavegrownoneframetoleft. Similarly,if Ork+1isnot\nalreadypartofanyotherregionandifthestandarddeviatio n\nofthefeaturevaluesin Rk∪Ork+1islessthan Th,thenRk\nwill also grow by one frame to its right. At the end of this\nstep,each Rkwillhavegrownbyatmosttwoframes. Ithas\ntobenotedthat,certainregionsmaynotgrowatall,because\nbothframesthatareadjacenttotheirboundaries,alreadyb e-\nlong to other regions. At the end of the step, it is examinedwhetheratleastoneregionhasgrownbyatleastoneframe.\nIf this is the case, this step is repeated until no more region\ngrowing takes place.\nTermination : After region growing has been completed,\nadjacentregions(ifany)aremergedtoformlargersegments .\nFinally, after merging is complete, short regions are elimi -\nnated by comparing their length with a pre-deﬁned thresh-\nold, say Tmin.\nIdeally, all segments (regions) that survive at the end of\nthe algorithm should correspond to music and any frame\noutside these regions should correspond to speech. This is\nbecause the proposed scheme relies on the assumption that\nmusic segments exhibit low standard deviation in terms of\nthe adopted feature (see Figure 1). As will be explained\nin the next section, our approach, despite its simplicity, e x-\nhibits ahigh discriminationaccuracy.\nFinally it has to be noted that the proposed algorithm\nis dependent on three parameters, namely M, the distance\n(measuredinframes)betweensuccessiveseeds, Th,thethres-\nholdforstandarddeviation(basedonwhichregiongrowing\ntakes place) and Tmin, the minimum segment length (used\nintheﬁnal stageofthealgorithm). Thechoice ofvalues for\nthese parameters isexplained inthenext section.\n4. Experiments\nWe carried out two sets of experiments, each of which on a\nseparate dataset, inorder to:\na)Determine the values of the parameters of the method,\nsubject totwodifferent maximization criteria,namely ove r-\nall discrimination accuracy and music precision. The latte r\nrefers to the proportion of audio data in the recording that\ncorresponds to music and was also classiﬁed as music (see\nalsosubsections4.2and4.3). Ithastobenotedthatbymax-\nimizing music precision, overall discrimination accuracy is\nlikely to decrease. Although this is undesirable if the pro-\nposed method is used as a standalone discriminator, it may\nnot be a restriction if it is used as a low-complexity pre-\nprocessingstepformusicdetection. Inthiscase,highmusi c\nprecision(closeto 100%)ensuresthatalldetectedsegments\nare correctly classiﬁed as music, and all remaining parts of\nthe audio recording can be subsequently fed to other, more\nsophisticated discrimination schemes, for further proces s-\ning.\nb)Assessthealgorithm’sperformance,usingtheparameters\nextracted from maximizing thedesired criteria.\n4.1. Datasets\nThe audio data used for the above purposes was collected\nfrom seven different BBC Internet radio stations, covering\na wide range of music genres and speakers. Obviously, the\ndataset used for testing system performance was different\nfrom the dataset used for parameter tuning. More specif-\nically, 30minutes of audio recordings (dataset D1), were\nused for estimating parameter values. To this end, an ex-haustive approach was adopted, i.e., each parameter was al-\nlowed to vary within a predeﬁned range of values. For sys-\ntem testing, a different dataset, D2, was created, consisting\nof audio recordings of a total duration of 160minutes. A\n16KHzsampling rate was used in all cases. All record-\nings (of both datasets) were manually segmented and la-\nbeledasspeechormusic. Ithastobenotedthat“silent”seg-\nments, i.e. segments with almost zero energy) were treated\nasspeech,undertheobservationthatsuchsegmentsusually\noccur between speech segments. This manual segmentation\nprocedure revealed that 69.77% of the data was music and\n30.23% was speech.\n4.2. Segmentationresultsformaximizingtheoverallac-\ncuracy\nThe parameter estimation process, subject to maximizing\ndiscrimination accuracy for dataset D1, led to the values of\nTable 1.\nThreshold Min. Duration Seed Dist\n0.50 3.0 sec 2.0 sec\nTable 1. Parameter values subject to maximizing discrimina-\ntionaccuracy over D1\nUsing the above parameter values, our method was then\ntested on D2. The proposed scheme classiﬁed 75.10%of\nthe data in D2as music and 24.90% as speech. Table 2\npresents the average confusion matrix C, of the discrimina-\ntion results. Each element Ci,jof the matrix corresponds to\nMusic Speech\nMusic 69.13%0.65%\nSpeech 5.97%24.25%\nTable2. Averageconfusionmatrixfor D2usingtheparameter\nvalues inTable 1\nthe percentage of data whose true class label was iand was\nclassiﬁed to class j. From Cone can directly extract the\nfollowing measures foreach class:\n1.Recall(Ri).Riis the proportion of data with true\nclasslabel i,thatwerecorrectlyclassiﬁedinthatclass.\nForexample,therecallofmusiciscalculatedas R1=\nC1,1\nC1,1+C1,2.\n2.Precision (Pi).Piis the proportion of data classiﬁed\nas class i, whose true class label is indeed i. There-\nfore,music precisionis P1=C1,1\nC1,1+C2,1.\nAccording to the confusion matrix, the overall discrimi-\nnation accuracy of our system is equal to 93.38% (C1,1+\nC2,2). Table 3 presents recall and precision values for both\nspeechandmusic. Fromthistable,itcanbeseenthat,more\nthan99%of the “true” music data was detected, while the\n”falsealarm” forthe musicclass was below 8%.MusicRecall 99.07%\nSpeech Recall 80.25%\nMusicPrecision 92.05%\nSpeech Precision 97.39%\nTable3. RecallandPrecisionofbothclassesfortheparameter\nvalues inTable 1\n4.3. Segmentation results for maximizing music preci-\nsion\nAs explained above, we have also estimated the parameter\nvaluessubjecttomaximizingmusicprecision. Theresultin g\nvalues arepresented inTable 4.\nThreshold Min. Duration Seed Dist\n0.30 5.0 sec 2.0 sec\nTable 4. Parameter values subject to maximizing music preci-\nsion\nTheaverage confusion matrixinthiscaseispresented in\nTable5. Itcanbeseenthat 58.09%ofthedatawasclassiﬁed\nas music and 41.91%as speech and the overall accuracy of\nthesystemwas 87.58%,almost 6%lowerthantheaccuracy\npresentedinsection4.2. However,ascanbeseeninTable6,\nmusicprecision isnowequalto 99.36%. Thisleadsustothe\nconclusion, that, when the proposed algorithm is fed with\nthis second set of parameter values, it can be used as a low-\ncomplexitypreprocessingstepinamoresophisticatedaudi o\ncharacterizationsystem(e.g. [6]),fortheinitialdetect ionof\na smaller proportion of the music segments (smaller music\nrecall), but withan almost zero“false alarm”( 0.4%).\nMusic Speech\nMusic 57.72%12.05%\nSpeech 0.37%29.86%\nTable5. Averageconfusionmatrixfortheresultsobtainedwith\ntheparameter values inTable4\n5. Conclusions\nThis paper presented a computationally efﬁcient, off-line\nspeech/musicdiscriminator,basedonaregiongrowingtech -\nnique operating on a single feature that we call chromatic\nentropy. The system was tested on recorded Internet ra-\ndiobroadcasts(ofalmost3hoursduration)andachievedan\naverage discrimination accuracy of 93.38%. This is com-\nparable to the performance obtained with other computa-\ntionally more complex methods. It is worth noticing that,\nif the method’s parameters are tuned to maximize music\nprecision, although the system’s overall accuracy drops to\n87.58%,musicprecisionalmostreaches 100%(99.3%),i.e.,MusicRecall 82.73%\nSpeech Recall 98.78%\nMusicPrecision 99.36%\nSpeech Precision 71.25%\nTable6. RecallandPrecisionofbothclassesfortheparameter\nvalues inTable 4\nallsurvivingsegmentscorrespondtomusic. Takingintoac-\ncount these results, it can be concluded that the proposed\nalgorithm iscapable of working both:\n1. As a standalone speech/music discriminator of high\nperformance.\n2. Asacomputationallyefﬁcientpreprocessingstagefor\nmusic detection in audio streams (when the parame-\nters are tuned to maximize music precision). In this\nlattercase,non-musicsegmentscanbefurtherprocessed\nbymore complex discrimination schemes.\nReferences\n[1] J. Saunders, “Real-time discrimination of broadcast\nspeech /music”, in Proc. of ICASSP 1996 , Vol. 2, pp.\n993-996, Atlanta, USA, May1996.\n[2] E.ScheirerandM.Slaney,“ConstructionandEvaluationofa\nRobustMultifeatureSpeech /MusicDiscriminator”,in Proc.\nICASSP1997 , pp.1331-1334, Munich, Germany.\n[3] TongZhangandC.-C.JayKuo,“AudioContentAnalysisfor\nOnline Audiovisual Data Segmentation and Classiﬁcation”,\ninIEEE Transactions on Speech and Audio Processing , Vol.\n9, No.4,pp. 441-457,May 2001.\n[4] C. Panagiotakis and G. Tziritas, “A Speech /Music Discrim-\ninator Based on RMS and Zero-Crossings”, IEEE Transac-\ntions onMultimedia , vol. 7(1),pp.155-166, Feb.2005.\n[5] Jitendra Ajmera, Iain McCowan and Herve Bourlard,\n“Speech/music segmentation using entropy and dynamism\nfeatures in a HMM classiﬁcation framework”, Speech Com-\nmunication , vol.40,pp. 351-363,2003.\n[6] Aggelos Pikrakis, Theodoros Giannakopoulos and Sergios\nTheodoridis,“Speech/MusicDiscriminationforradiobroad-\ncasts using a hybrid HMM-Bayesian Network architecture”,\ninProc.ofthe14thEuropeanSignalProcessingConference\n(EUSIPCO-06) ,September 4-8,2006, Florence, Italy.\n[7] N.Casagrande,D.Eck,andB.Kigl.,“Frame-levelaudiofea-\ntureextractionusingAdaBoost”,in Proc.ofISMIR2005 ,pp.\n345-350, London,UK, 2005.\n[8] J.E. Munoz-Exposito et al, “Speech/Music discrimination\nusing a single Warped LPC-based feature”, Proc. of ISMIR\n2005, pp.614-617, London,UK, 2005.\n[9] Hemant Misra, Shajith Ikbal, Herve Bourlard, and Hynek\nHermansky, “Spectral entropy based feature for robust\nASR”, in Proc. of ICASSP 2004 , Vol. 1, pp. 193-196, Mon-\ntreal, Canada, 2004.\n[10] A.PapoulisandS.UnnikrishnaPillai,“Probability,Random\nVariables and Stohastic Processes, 4th edition”, McGraw-\nHill, NY, 2001."
    },
    {
        "title": "Independent Component Analysis for Music Similarity Computation.",
        "author": [
            "Tim Pohle",
            "Peter Knees",
            "Markus Schedl",
            "Gerhard Widmer"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1417877",
        "url": "https://doi.org/10.5281/zenodo.1417877",
        "ee": "https://zenodo.org/records/1417877/files/PohleKSW06.pdf",
        "abstract": "In the recent years, a number of publications have ap- peared that deal with automatically calculating the similar- ity of music tracks. Most of them are based on features that are not intuitively understandable to humans, as they do not have a musically meaningful counterpart, but are merely measures of basic physical properties of the audio signal. Furthermore, most of these algorithms do not take into ac- count the temporal development of the audio signal, which certainly is an important aspect of music. All of them con- sider the musical signal as a whole, not trying to reconstruct the listening process of dividing the signal into a number of sources. In this work, we present a novel approach to fill this gap by combining a number of existing ideas. At the heart of our approach, Independent Component Analysis (ICA) de- composes an audio signal into individual parts that appear maximally independent from each other. We present one basic algorithm to use these components for similarity com- putations, and evaluate a number of modifications to it with respect to genre classification accuracy. Our results indicate that this approach is at least of similar quality as many ex- isting feature extraction routines. Keywords: audio feature extraction, music similarity com- putation",
        "zenodo_id": 1417877,
        "dblp_key": "conf/ismir/PohleKSW06",
        "keywords": [
            "audio feature extraction",
            "music similarity computation",
            "independent component analysis",
            "genre classification",
            "temporal development",
            "basic physical properties",
            "musically meaningful counterpart",
            "novel approach",
            "splitting audio signal",
            "reconstructing listening process"
        ],
        "content": "Independent Component Analysis forMusic Similarity Compu tation\nTim Pohle1, Peter Knees1, Markus Schedl1,Gerhard Widmer1,2\n1JohannesKeplerUniversityLinz,Austria\n2AustrianResearchInstituteforArtiﬁcialIntelligence(O FAI),Vienna,Austria\nmusic@jku.at\nAbstract\nIn the recent years, a number of publications have ap-\npeared that deal with automatically calculating the simila r-\nity of music tracks. Most of them are based on features\nthatarenotintuitivelyunderstandabletohumans,astheyd o\nnothaveamusicallymeaningfulcounterpart,butaremerely\nmeasures of basic physical properties of the audio signal.\nFurthermore, most of these algorithms do not take into ac-\ncount the temporal development of the audio signal, which\ncertainly is an important aspect of music. All of them con-\nsiderthemusicalsignalasawhole,nottryingtoreconstruc t\nthe listeningprocessofdividingthe signalintoa numberof\nsources.\nIn this work, we present a novelapproachto ﬁll this gap\nby combining a number of existing ideas. At the heart of\nour approach, Independent Component Analysis (ICA) de-\ncomposes an audio signal into individual parts that appear\nmaximally independent from each other. We present one\nbasicalgorithmtousethesecomponentsforsimilaritycom-\nputations,andevaluatea numberof modiﬁcationsto it with\nrespecttogenreclassiﬁcationaccuracy. Ourresultsindic ate\nthat this approach is at least of similar quality as many ex-\nisting featureextractionroutines.\nKeywords: audio feature extraction, music similarity com-\nputation\n1. Introduction\nAlthough “music similarity” is an ill-deﬁned concept, most\npeople have an intuitive idea of which music is similar, and\nwhichisnot. Forexample,mostpeoplewouldconsidertwo\nheavymetal piecesasbeingmoresimilartoeachotherthan\nto a classical choir piece. It is obvious that such common-\nsense judgementsaresomehowreﬂectedin themusicalsig-\nnal. In recent years, some research has been performed to\nextract these aspects algorithmically, and to automatical ly\ncomputethemusicsimilarity. Reviewsofanumberofthese\napproaches can be found in [1, 2]. Among the algorithms\nthat seem to perform best are those that are based on the\nwell-knownMelFrequencyCepstralCoefﬁcients(MFCCS)\nPermission to makedigital orhardcopies ofallorpartofthi s workfor\npersonal or classroom use is granted without fee provided th at copies\narenotmadeordistributed forproﬁtorcommercial advantag e andthat\ncopies bear this notice and thefull citation on the ﬁrstpage .\nc/circlecopyrt2006 University of Victoriawhich were invented for speech recognition [3, 4, 5, 6].\nMFCCsonlydescribethecoarseshapeofthespectrum,and\ndiscard the harmonic structure. Also, in these algorithms\nthe temporal structure is discarded. Approaches to model\nthe development of MFCCs in time have not signiﬁcantly\nimproved the performance (cf. [7]). Most of the other al-\ngorithmsthat aim todescribecertainaspects(“features”) of\ntheaudiosignal(cf. also[8])alsoonlyoperateonveryshor t\nsegmentsoftheaudiosignal(“frames”)anddonotconsider\ntheirtemporalorder.\nThere are only few audio features that take into account\nthe temporal structure of the signal, which certainly is of\ngreat importance when regarding music (e.g. beat\nhistograms [9], meter and tempo descriptors[10], and ﬂuc-\ntuationpatterns[11,12]).\nOne thingthat iscommonto these audiosimilarity algo-\nrithms is that they are based on features that only look at\nthemixedaudiosignalasawhole,althoughsplittingtheau-\ndiosignal intoa numberofsubbandsgivessomeseparation\nas usually in each bandonly few instrumentsare dominant.\nOne approach that has not been tried yet is to simulate the\nhearing process to a certain extent, and separate the audio\nsignal into various sources before doing a similarity analy -\nsis. The purposeof this paper is to take a ﬁrst step into this\ndirection.\n2. Stateofthe Art\nInthissection,wedescribethebasictechniqueweuse,Inde -\npendent Component Analysis (ICA), and give an overview\nof how it has already been applied to the ﬁeld of music in-\nformationretrieval. Also,we motivatewhywethinkICA is\nusefulforthe purposeofthiswork.\n2.1. Independent ComponentAnalysis(ICA)\nIndependent Component Analysis (ICA) is an approach to\nsolve the following problem: given a number of observed\nsignals that are a linear mixture of a number of unknown\nsignals, ﬁnd these unknown signals andthe amount each\nof them contributes to the observed signals. An example\nforthisscenarioistwo peoplespeakingsimultaneouslyina\nroom,whichisrecordedwithtwo microphones.\n2.2. ICAintheﬁeldofAcousticsandMusicInformation\nRetrieval\nIn [13], the authors apply ICA to short frames of time-\ndomain audio signals, so that each time-domain frame canbe represented by a mixture of independent components.\nThis work was motivated by prior work in image analysis\nthat found independent components to be related to low-\nlevelaspectsofhumanvision. Theauthorsﬁndsomeindica-\ntion that for certain kinds of audio signals, the resulting i n-\ndependentcomponentsmaybesimilartolow-levelprocess-\ningstepsinthehumanauditorysystem.\nIn[14],ICAwasappliedtoseparatethesourcesofmusi-\ncal signals, includingvariousdrum soundsand vocals. The\nstepofisolatingvocalsfromtheotherinstrumentsinasong\nwas improved and automated in [15]. The promising re-\nsults from these works indicate that ICA might be useful\nto separate the various musical instruments similar to hu-\nman perception. Thus, ICA seems to be valuable for our\nexperimentspresentedhere. Fortheﬁeld ofpitchdetection ,\nmethodsrelatedtoICA wereusedin[16].\n3. Approach\nThe starting point of our experimentsis the combinationof\nanumberofexistingideas. Weadaptthebasicapproachfor\nimage similarity computation by independent components\nfrom[17] to the domainof audioprocessing. The approach\nanditsadaptationisdescribedinthefollowingsections.\n3.1. OriginalApproachfrom[17]\nFor convenience and a better understanding, here ﬁrst the\noutlineoftheoriginalapproachisrepeated.\n3.1.1. FeaturesandFeatureExtraction\nIn the original work [17], feature extraction is done in the\nfollowingway: aftera preprocessingstep, a numberof ran-\ndomlychosenexcerptsof 12×12pixelsaretakenfromeach\nimage. The authorscall these excerpts patches[18]. ICA is\nrunonagreatnumberofpatches. Eachindividualpatchcan\nbe thought of as a linear combination of independent com-\nponents. To describe a whole image, this image is divided\ninto patches, and each patch is described by the degree of\nactivationofeachindependentcomponent. Thefeaturesfor\nthewholeimagearetheactivationhistogramsforeachinde-\npendent component over all patches from the image. Only\nthose histograms are retained whose average activation per\npicture has the highest varianceovera largenumberof pic-\ntures. Two images are compared by comparing their (inde-\npendentcomponents’activation)histograms,asdescribed in\nthenextsection.\n3.1.2. Determiningthe SimilarityBetweenImages\nAstheactivationsoftheindependentcomponentsaresparse\nandhighlyuncorrelated,interdependenciesbetweentheva r-\nious activation histograms of the independent components\ncan be disregarded. Thus, the comparison of two images\nreduces to a comparison of the histogramsbelongingto the\nsame component. The individual similarities then can be\ncombinedtoobtainoneoverallsimilarityvalue. Theauthor s\nproposeanumberofapproachestocomparethehistograms.\nFigure 1. Example of activation histograms for a choir piece\nand a piece for prepared piano. The two independent compo-\nnentswhosehistogramsareshownaregivenintheleftcolumn .\nIt can be seen that the upper component is more often close to\nzero in the choir piece than in the piano piece, while the com-\nponent depicted below is more frequently activated in the in\nthe choirpiece thaninthepianopiece.\n•The simplest is to take the mean of all activations of\na component over all patches of the image. Thus,\nthe feature data of an image consists of one vector of\nlength n,where nisthenumberofindependentcom-\nponents. The similarity of two images is determined\nbytheEuclediandistancebetweenthesevectors.\n•A better description of the histogram shapes is\nachieved by modelling them as a Gaussian distribu-\ntion,whichresultsintwovaluesperhistogram(mean\nand variance). These values are compared by apply-\ning the KL Distance. As the activation histograms\nof the independent components are known to have a\nhighkurtosis,theauthorsalsointroduceanadditional\nﬂavour of this Gauss model: the activationhistogram\nis mirrored at x= 0, and the Gauss model is calcu-\nlatedovertheresultingdistribution. Obviously,inthis\ncasethe meaniszero.\n•The most exact comparison between histograms was\ndone by modelling each histogram with a B spline,\nand calculating the common area under the spline\nmodels.\nThesimilaritymeasurewasevaluatedonasetof 540im-\nages from four categories. Lacking a reliable pairwise sim-\nilarity judgement, evaluation of the similarity measure wa s\ndone by nearest neighbour classiﬁcation, and the classiﬁ-\ncation accuracy was taken as an indicator of the similarity\nmeasure’s quality. The authors report classiﬁcation accur a-\nciesofupto 87percent.\n3.2. AdoptiontoMusical Signals\nWhen adopting this approach to music signals, the crucial\nstep is to ﬁnd an analogue for the image patches (i.e. the\n12×12pixelsamplesfromtheimages),includingasuitabledata representation for their extraction. Some authors hav e\nchosen a single time domain or frequencydomain frame as\none “patch”. Althoughwe also investigatethisdeﬁnitionof\na patch in the course of our experiments, we found it more\ninterestingtoalsotakeintoaccountthetemporalchangeso f\nthe signal, because the developmentin time is an important\naspect of music, and furthermore already a great number\nof audio descriptorsexist that operate on a per-framebasis .\nThus, in our initial experimental setup, we opted to regard\na number of Nconsecutive (frequency-domain) frames as\nonepatch,withtheexactvalueof Nbeingoneparameterin\nour experiments. Each patch coversall frequencies.In[19] ,\nthis deﬁnition of a “patch” was already proposed. Also, it\nis mentioned there that the great number of frequency bins\nin a usual spectrum produces problems, as it causes the in-\nput vector to the ICA algorithm to be very large (ie. num-\nber of frequency bins ×number of frames in patch). Be-\ncause of this, and due to the more perceptually motivated\nrepresentation,we transformedthe spectrumto a mel-sone-\nrepresentation, which has only 18frequency bands instead\nof e.g. 256. These 18frequency bands roughly correspond\nto criticalbandsin thehumanauditorysystem.\n3.3. Filtersastemplatesforeventdetection\nOf courseit would be desirableto obtainindependentcom-\nponents(“ﬁlters”)thatrepresentmeaningfulaspectsofhe ar-\ning. Examples of such dedicated ﬁlters include “percus-\nsive sound”, “high-frequency noise”, “sustained sound in\nthe lower frequencies”. Such ﬁlters then might serve as\ntemplates to scan a given piece of music for these kinds\nof events. The only publication we are aware of that uses\na template-based process for music analysis in the acoustic\ndomainis[20],whereitwasusedfordrumsounddetection.\n4. Experimental Setup\nWe carried out our experiments with the tracks from the\nISMIR’04 Genre Classiﬁcation Contest training set. This\nset consists of 724 tracks1from the six genres classical\n(43.7%),electronic( 15.9%),jazz/blues( 3.6%), metal/punk\n(6.0%), rock/pop ( 14.0%), world ( 16.9%).2Feature ex-\ntraction was done on 30seconds from the middle of each\nﬁle. The overall algorithm was as described above: after\ntransformingeachtrackintothemel/sonerepresentationu s-\ning the MA Toolbox [11], we calculated the independent\ncomponents on a subset of the collection. In the next step,\nthe componentswere used to extract the featuresfromeach\nsong by determining how strong each component is acti-\nvated duringthe song. The ﬁnal similarity computationde-\npendsonthe particularhistogramcomparisonmethod.\n1Weleft out ﬁvetracks dueto ﬁle naming inconsistencies.\n2The full set is available for download at\nhttp://ismir2004.ismir.net/genre contest/index.htm.4.1. Obtainingthe Independent Components\nWe used the FastICA3algorithm to compute the indepen-\ndent components. The data on which we computed ICA\nwere small fragments from the mel/sone representation of\ntheaudiotracks. Westartedtheevaluationwiththreediffe r-\nent fragment lengths: 0.15sec,0.3sec, and 0.6sec. Note\nthat0.6sec is the duration of one quarter note when play-\ning a4/4thmetre at 100 bpm, and similarly 0.3sec and\n0.15sec correspondto the durations of 1/8and1/16note.\nBasedontheoutcomeoftheexperiments,wethenalsotried\nother lengths. As the resulting patches contained up to 576\nvalues, calculating ICA on them also produced up to 576\ncomponents. To reduce this large number of components,\nwealsoappliedadimensionalityreductionoftheinputdata\nby PCA before calculating ICA (the PCA compression is\nthen invertedon the – fewer – individualcomponentsafter-\nwards to expand them again to e.g. 576values per compo-\nnent). This way, we reduced the number of componentsby\n0%(i.e. no PCA), 50%and75%. We evaluated three ways\nto obtainICA components:\n1. Asaﬁrstapproach,wecreatedarepresentativesubset\nof100songsfromourcollection. Onthese 100songs,\nwecalculatedICAonrandomlychosenfragments. In\nFigure 2, an example of the resulting components is\ngiven.\n2. Thesecondapproachwasliketheﬁrstapproach,with\nthe difference that the patches were not extracted at\nfully randomly chosen points, but rather starting on\nthoseframesthat arelikelytocontainbeat onsets.\n3. Finally, as an interesting try, we deﬁned the compo-\nnents manually, according to what we subjectively\nthought to be meaningful entities such as high / mid\n/ low frequency content, beat onsets in various fre-\nquencies, periodicities at various levels. An example\nofthesecomponentisgiveninFigure3.\nThe quality of these approaches was evaluated as de-\nscribedinthenextsection.\n4.2. Evaluation\nLackinghumanjudgementsaboutthesimilarityofeachpair\noftracksinthecollection,weevaluatedthealgorithm’spe r-\nformance in a similar manner as in the original paper. We\nassume that tracks that belong to the same genre are more\nsimilar to each other than tracks that are labelled to belong\nto different genres. After calculating the similarity of ea ch\npair of tracks, we do a leave-one-out 1-Nearest-Neighbour\nclassiﬁcation,andtaketheclassiﬁcationaccuracyasanin di-\ncatorofthealgorithm’scapabilitytocalculatethepercei ved\nsimilarityofmusictracks.\n3http://www.cis.hut.ﬁ/projects/ica/fastica/Mel/SonePatchLength 0.075sec 0.15sec 0.3sec\nPCA compression 1 0.5 0.25 1 0.5 0.25 1 0.5 0.25\nMean 63.8% 62.4% 61.8% 64.2% 63.5% 61.7% 64.2% 62.8% 61.5%\nKL 68.5% 66.7% 65.7% 67.4% 68.2% 65.4% 64.5% 64.7% 65.6%\nKLzero 67.4% 66.0% 65.7% 67.2% 66.3% 65.4% 63.6% 64.9% 64.3%\nTable 1.A patch deﬁned as a short excerpt of the mel/sone representat ion of the song. Average classiﬁcation accuracy for a number of\npatch sizes, for various histogram comparison methods and w ith varying PCA compression factor. Histogram comparison m ethods\nwere the Euclidean distance between the means of each histog ram (Mean), Kullback-Leiber (KL) divergence based on mean and\nstandard deviation of the histograms ( KL), and KL divergence based on the standard deviation of the hi stograms, mirrored at the\nzero-point, which produces a mean of zero ( KLzero). The overall maximum accuracy found with this setup was 68.5%at a patch\nlengthof 0.075sec, belowwhichpatchlengthall accuracies tendedtodecre ase.\nFigure 2. Independent components calculated on patches of\nlength 0.15sec with randomly chosen starting times, and 50%\nPCAcompression.\n5. Results andDiscussion\nIn this section, we give the results of the experiments de-\nscribed above, and give a short interpretation of these re-\nsults.\n5.1. ICscalculatedon randomlychosenpatches\nWhen calculating the independent components on patches\nthat are randomly chosen, in many cases the components\nseem not to correspond to meaningful acoustical entities.\nButinsomeconﬁgurationsinterestingresultsappeared. Fo r\nexample, in Figure 2 the components for patches of length\n0.15sec with 50%PCA compression are given. It can be\nseen that some components are primarily located in time,\nwhich can be assumed to be activated on percussive mu-\nsic events. Other components are mainly located in fre-\nquency, with a horizontal shape. Components with a hori-\nzontal shape might be preferably activated on sustained\nsounds. We examined the calculated components for such\npropertiesbyinvestigatingtracksforwhichcomponentsar e\nactivated most frequently, and by visualizing the activati on\nof the components over time. Unfortunately, we found no\nstrongindicationthatthesecomponentsarerelatedtomean -ingful musical entities in such a way. But at least compar-\ning certain activation histograms of clearly different pie ces\ngiveaweakindicationthattheremightbesuchrelationship s\n(cf.Figure1).\nTheclassiﬁcationaccuraciesfor 1NNleave-one-outeval-\nuation are given in Table 1. The values shown are the re-\nsult of considering all available components. The accura-\ncies are constantly above 60%, with a maximum value of\n68.5%. Although this value is still below the classiﬁcation\naccuracy achieved with the algorithm and proposed para-\nmeter set from [5], which is at 72%, it is still above the\nvalues we achieved with other audio descriptors for simple\nnearest-neighbor classiﬁcation [1], and within the range o f\ntheaccuraciesachievedwithsophisticatedmachinelearni ng\nalgorithms applied on a set of many commonly used audio\nfeatures[1].\nFollowing the practice in [17], we also tried to apply a\nHammingwindowtoeachpatchbeforecalculatingtheinde-\npendent components or determining the activation strength\nof each component, respectively. This additional step did\nnot lead to increased classiﬁcation accuracy in our experi-\nments.\nAnother observationwe made was that only considering\nthosencomponentswhose average activationper song had\nthe highestvarianceovera largenumberof songsproduced\nloweraccuraciesthanconsidering allcomponents. For this,\nwe tried n={1,2,5,10,15,20}. However, it should be\nnoted that for n= 20, the achieved classiﬁcation accuracy\nwasonlyafewpercentagepointsbelowtheresultsobtained\nwhenusingallcomponents.\n5.1.1. PatchesDeﬁnedasOneFrame\nAs the highest results were achieved with patches of short\nlength ( ≤8Frames), we also investigated the aforemen-\ntionedalternativewaytodeﬁnepatchestouseonlyonesin-\ngle frame, but with a higher frequency resolution. Except\nfor the differences in the frequency representation, the pa -\nrametersof the experimentstayed the same. The resultsfor\nnumFreq = 129which are given in Table 2 indicate that\nthis alternativeway to deﬁne patchesdoes not contribute to\na higherclassiﬁcationaccuracy.1 0.5 0.25 0.125 0.0625\nMean 50.0% 53.3% 53.5% 50.5% 49.1%\nKL 58.4% 57.8% 62.8% 58.8% 55.2%\nKLzero 58.1% 58.4% 61.1% 59.6% 51.7%\nTable 2. A patch deﬁned as one FFT frame with 129 frequen-\ncies:Average classiﬁcation accuracy for the various methods\nwith varying PCA compression factor. Same abbreviations as\ninTable 1.\n5.2. PatchesatLikely Onsets\nInsteadoftryingtomodeleachpossiblepatchwitharbitrar y\nstartpositionbyindependentcomponents,itmightbebene-\nﬁcial to onlyconsiderpatchesthat start at onsettimes. Thi s\nmightcontributetomakingvariouspatchesbettercompara-\nble to each other, as in this case it is knownthat each patch\nstarts at an onset. We evaluatedthis by ﬁnding possible on-\nset times with a simple onset-detection algorithm. This al-\ngorithmwasappliedtoselectthepatchesforcalculatingth e\nindependentcomponents,andalsoduringfeatureextractio n\nforeachparticularsong.\n0.075s0.15s0.3s0.6s\nMean 63.2% 62.5% 62.2% 54.8%\nKL 65.8% 65.1% 63.1% 56.8%\nKLzero 66.4% 62.9% 56.8% 48.0%\nTable3.Componentsextractedatlikelyonsetpositions: Average\nclassiﬁcation accuracy for the various methods with varyin g\ncomponent length. Same abbreviationsas inTable 1.\nAscanbeseeninTable3,thisapproachdidnotimprove\ntheclassiﬁcationaccuracies. Alternativereasonsforthe fail-\nuremightbethatthechosenonsetdetectionalgorithmmight\nnot be the best for our purpose, or that too few patches are\nextracted from each song, resulting in ill-deﬁned activati on\nhistograms. The latter point in particular might be a reason\nforthelowaccuraciesforpatchesoflength 0.6sec.\n5.3. Self-deﬁnedComponents\nAs described in Section 5.1, the independent components\ncomputed in this experiment are not clearly related to in-\ntuitively understandable musical properties. We were also\ninterestedinﬁndingoutifitisbeneﬁcialtointentionally de-\nﬁne the componentsso that they might describe such prop-\nerties. Therefore, we created a number of componentsour-\nselves (cf. Fig. 3). These components included averaging\nﬁlters for three frequency regions, high-pass like element s\nin four frequency bands, components aimed to detect sus-\ntainedsoundsattheindividualfrequencies,andcomponent s\nthat should be triggered by the presence of certain periodic\nevents at various frequencies. The latter resemble the ﬂuc-\ntuationpatterns[11].\nWith the self-deﬁned components, the independent ac-\ntivation of the components can not be assumed any more.Figure3. Examplefor self-deﬁnedcomponents.\nThus, we expandthe KL distancemeasurebetween the his-\ntogramstoalsotakeintoaccountthecovariancebetweenthe\nactivationsof differentcomponentsduringeachsong. Con-\nsequently, for this comparison method the feature data for\na song consists of the mean value and the full covariance\nmatrix. This additional comparison method is denoted as\nKLfullin Table4.\n0.075s0.15s0.3s0.6s\nMean 59.6% 58.1% 57.0% 55.7%\nKL 64.0% 62.0% 61.0% 58.5%\nKLzero 64.4% 63.3% 61.0% 59.5%\nKLfull 67.6% 68.7% 68.2% 67.0%\nTable 4.Self-deﬁned Components: Average classiﬁcation accu-\nracy for the various methods with varying component length.\nSame abbreviations as in Table 1, with the additional method\nusingthefullcovariance matrix( KLfull). Intentionallyreduc-\ning the number of components did not lead to increased accu-\nracies.\nFrom Table 4 it can be seen that KLfullproduces the\nhighestclassiﬁcationaccuraciesfortheself-deﬁnedcomp o-\nnents. These accuracies are also slightly higher than those\nachievedwith the otherapproacheswe evaluatedin thispa-\nper. However, we still were not able to ﬁnd that the activa-\ntions correspond to musical aspects as perceived by human\nlisteners. Also, a linear combination of the distances pro-\nduced by these algorithms with the distances produced by\na MFCC-based algorithm (which produces an accuracy of\n72%onthisdata)didnotyielda relevantimprovement.\n6. ConclusionandFuture Work\nWe evaluated a number of ways to apply what we consider\naninterestingapproachtomusicsimilaritycomputation. T he\napproach is based on Independent Component Analysis\n(ICA) and was originally developed as an image similaritymeasure. We chose the classiﬁcation accuracy as a quality\nmeasure, whichyields promisingresults. However,we also\nhadhopedtoextractmusicallymeaningfulinformationfrom\ntheaudiodatawiththisapproach;wehavenotsucceededin\nthisso far.\nPossible improvements of the algorithm include the use\nofothersparsecodingtechniques(e.g. Non-negativeMatri x\nFactorization) instead of ICA, and the use of B-splines for\nhistogram comparison. Using B-splines would not be fea-\nsible for self-deﬁned components, as no interdependencies\nbetween the componentscan be modelledwith this. Also it\nmightbepossibletomodelthetemporalorderofcomponent\nactivations,e.g. viaHiddenMarkovModels(HMMs).\n7. Acknowledgements\nThis research is supported by the Austrian Fonds zur\nF¨ orderung der Wissenschaftlichen Forschung (FWF) under\nproject number L112-N04, and by the EU 6th FP project\nS2S2(“SoundtoSense,SensetoSound”,IST-2004-03773).\nThe Austrian Research Institute for Artiﬁcial Intelligenc e\nalsoacknowledgesﬁnancialsupportbytheAustrianFederal\nMinistriesBMBWK andBMVIT.\nReferences\n[1] Tim Pohle, “Extraction of Audio Descriptors and their\nEvaluation in Music Classiﬁcation Tasks,” M.S. the-\nsis, TU Kaiserslautern, ¨OFAI, DFKI, 2005, available at\nhttp://kluedo.ub.uni-kl.de/volltexte/2005/1881/.\n[2] Elias Pampalk, Computational Models of Music Similarity\nand their ApplicationinMusic Information Retrieval , Ph.D.\nthesis, Technische UniversitatWien, 2006.\n[3] Beth Logan, “Mel frequency cepstral coefﬁcients for mu-\nsic modeling,” Read at the ﬁrst International Symposium on\nMusic InformationRetrieval, 2000.\n[4] BethLoganandArielSalomon, “Amusicsimilarityfuncti on\nbased on signal analysis,” in Proceedings of the IEEEInter-\nnational Conference on Multimedia and Expo (ICME’01) ,\nTokyo, Japan, August 22-25 2001.\n[5] Jean-Julien Aucouturier and Francois Pachet, “Improvi ng\ntimbresimilarity: Howhighisthesky?,” JournalofNegative\nResults inSpeech and AudioSciences , vol.1, no. 1, 2004.\n[6] Michael Mandel and Dan Ellis, “Song-Level Features and\nSupport VectorMachines forMusicClassiﬁcation,” in Proc.\nInternational Symposium on Music Information Retrieval\n(ISMIR’05) , London, UK,2005.\n[7] Arthur Flexer, Elias Pampalk, and Gerhard Widmer, “Hid-\nden markovmodels for spectral similarityofsongs,” in Pro-\nceedings of the 8th International Conference on Digital Au-\ndioEffects(DAFx’05) , 2005.\n[8] Michael Casey, “Mpeg-7 sound-recognition tools,” IEEE\nTransactions on Circuits and Systems for Video Technology ,\nvol. 11, no. 6, pp. 737–747, June 2001.\n[9] G. Tzanetakis and P. Cook, “Musical genre classiﬁcation\nof audio signals,” IEEE Transactions on Speech and Audio\nProcessing , vol. 10, no. 5, pp. 293–302, 2002.[10] S. Dixon, E. Pampalk, and G. Widmer, “Classiﬁcation of\ndance music by periodicity patterns,” in Proceedings of the\nFourth International Conference on Music Information Re-\ntrieval (ISMIR’03) , Baltimore, MD, USA, October 26-30\n2003, pp. 159–166, John Hopkins University.\n[11] Elias Pampalk, “A matlab toolbox to compute music sim-\nilarity from audio,” in Proceedings of the Fifth Inter-\nnational Conference on Music Information Retrieval (IS-\nMIR’04), Barcelona, Spain, October 10-14 2004.\n[12] Elias Pampalk, Arthur Flexer, and Gerhard Widmer, “Im-\nprovements of audio-based music similarity and genre clas-\nsiﬁcaton,” in Proceedings of the Sixth International Confer-\nence on Music Information Retrieval (ISMIR’05) , London,\nUK, October 10-14 2005.\n[13] Samer A. Abdallah and Mark D. Plumbley, “If the indepen-\ndent components of natural images are edges, what are the\nindependent components of natural sounds?,” 2001.\n[14] Paris Smaragdis, Redundancy reduction for computational\naudition. A unifying approach. , Ph.D. thesis, Massachusetts\nInstitute of Technology, Media Laboratory, 2001.\n[15] Shankar Vembu and Stephan Baumann, “Separation of vo-\ncals from polyphonic audio recordings,” in Proceedings of\ntheSixthInternationalConferenceonMusicInformationRe -\ntrieval(ISMIR’05) ,London,UK,September11-152005,pp.\n337–344.\n[16] Samer Abdallah and Mark Plumbley, “Polyphonic music\ntranscription by non-negative sparse coding of power spec-\ntra,” inProceedingsoftheFifthInternational Conference on\nMusic Information Retrieval (ISMIR’04) , Barcelona, Spain,\nOctober 10-14 2004.\n[17] Herv´ e Le Borgne, Gu´ erin-Dugu´ e Anne, and Anestis Ant o-\nniadis, “Representation of images for classiﬁcation with i n-\ndependent features,” Pattern Recognition Letters , vol. 25,\npp. 141–154, jan2004.\n[18] Anthony J. Bell and Terrence J. Sejnowski, “The ‘indepe n-\ndent components’ of natural scenes are edge ﬁlters,” Vision\nResearch, vol.37, no. 23, pp. 3327–3338, 1997.\n[19] Samer A. Abdallah, Towards music perception by redun-\ndancy reduction and unsupervised learning in probabilisti c\nmodels, Ph.D.thesis,DepartmentofElectronicEngineering,\nKing’s College London, 2002.\n[20] Kazuyoshi Yoshii, Masataka Goto, and Hiroshi G. Okuno,\n“Automaticdrumsounddescriptionforreal-worldmusicus-\ning template adaption and matching methods,” in Proceed-\ningsoftheFifthInternationalConferenceonMusicInforma -\ntion Retrieval (ISMIR’04) , Barcelona, Spain, October 10-14\n2004."
    },
    {
        "title": "Optical Music Recognitoin of Early Typographic Prints using Hidden Markov Models.",
        "author": [
            "Laurent Pugin"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1416974",
        "url": "https://doi.org/10.5281/zenodo.1416974",
        "ee": "https://zenodo.org/records/1416974/files/Pugin06.pdf",
        "abstract": "Music printed with movable type (typographic music) from the 16th and 17th centuries contains specific graphic fea- tures. In this paper, we present a technique and associ- ated experiments for performing optical music recognition on such music prints using Hidden Markov Models (HMM). Our original approach avoids the difficult and unreliable re- moval of staff lines usually required before processing. The modeling of symbols on the staff is based on low-level sim- ple features. We show that, using our technique, these fea- tures are robust enough to obtain good recognition rates even with poor quality images scanned from microfilm of origi- nals. The music content retrieved by the optical recognition process can be put to significant use in, for example, the creation of searchable digital music libraries. Keywords: OMR, Early typographic prints, HMM",
        "zenodo_id": 1416974,
        "dblp_key": "conf/ismir/Pugin06",
        "keywords": [
            "Hidden Markov Models",
            "Optical Music Recognition",
            "Staff Lines",
            "Low-Level Features",
            "Microfilm",
            "Digital Music Libraries",
            "Searchable Libraries",
            "Early Typography",
            "Movable Type",
            "Music Recognition"
        ],
        "content": "OpticalMusic Recognition of EarlyTypographic Prints\nusing Hidden Markov Models\nLaurent Pugin\nMusicTechnologyArea,SchulichSchoolofMusic\nMcGillUniversity,Montreal,Canada\nlaurent@music.mcgill.ca\nAbstract\nMusic printedwith movabletype (typographicmusic) from\nthe 16th and 17th centuries contains speciﬁc graphic fea-\ntures. In this paper, we present a technique and associ-\nated experiments for performing optical music recognition\nonsuchmusicprintsusingHiddenMarkovModels(HMM).\nOuroriginalapproachavoidsthedifﬁcultandunreliablere -\nmovalofstafflinesusuallyrequiredbeforeprocessing. Th e\nmodelingofsymbolsonthestaffisbasedonlow-levelsim-\nple features. We show that, using our technique, these fea-\nturesarerobustenoughtoobtaingoodrecognitionrateseve n\nwith poor quality images scanned from microﬁlm of origi-\nnals. Themusic contentretrievedbytheopticalrecognitio n\nprocess can be put to signiﬁcant use in, for example, the\ncreationofsearchabledigitalmusiclibraries.\nKeywords: OMR, Earlytypographicprints,HMM\n1. Introduction\nTypographicscoresfromthe16thand17thcenturiesarethe\nmostcommonmusicprintsofthattime[1]. Havinganopti-\ncalrecognitionsystemforthiskindofdocumentistherefor e\nimportant, for example to build searchable digital librari es\n[2]ortoassistmusicologistsintherealizationofcritica lmu-\nsic editions[3]. But optical recognitionof these document s\nis difﬁcultbecausetheyareold andoftenin poorcondition.\nFurthermore, they cannot be treated with conventional op-\ntical music recognition (OMR) techniques because of the\nnumerous printing irregularities they present and their pa r-\nticular layout. One importantpointisthat staff linesaren ot\nalwayscontinuousandtheirwidthcanvary(ﬁgure1).\nThe following section looks at the background of pre-\nvious work carried out both in OMR of similar prints and\nin using HMM for OMR. In section 3 we explain how the\nresearchwasorganizedandhowoursolutionworks. Insec-\ntion 4, we present the results we obtained on different data\nsets and with different parameters. Finally, section 5 dis-\ncussestheresultsandfurtherpossibledevelopments.\nPermission to makedigital orhardcopies ofallorpartofthi s workfor\npersonal or classroom use is granted without fee provided th at copies\narenotmadeordistributed forproﬁtorcommercial advantag e andthat\ncopies bear this notice and thefull citation on the ﬁrstpage .\nc/circlecopyrt2006 University of Victoria\nFigure1. Example of staves printedwithmovable type\n2. Background\n2.1. Opticalrecognitionofearlytypographicprints\nOMR is a research ﬁeld that has been covered by several\nstudies [4]. Usually it is common music notation (CMN)\nthat is treated, with solutions speciﬁc to its characterist ics\n[5]. Opticalrecognitionofearlytypographicprintshasne ver\nbeen the subject of a speciﬁc study. Only Carter in the\nnineties once mentioned early typographic prints in his re-\nsearch on OMR [6]. The solution proposed by Bainbridge\nandBell [7]isdesignedtobeextensible,buttheparticular i-\nties of typographyare not consideredand nevermentioned.\nThe research by Pinto et al. [8] concerns more speciﬁcally\nearly music but in manuscript form. The closest research\nis undoubtedlyby Dalitz and Karsten [9] on recognition of\nprints of the same period but in lute tablature, proposing a\nsolutionspeciﬁctothismusicnotation.\n2.2. HiddenMarkovModelsin OMR\nHidden Markov Models (HMM) have almost never been\nused in OMR except for experiments by Kopec and Chou\n[10]. Our work was partly inspired by their use in opti-\ncal character recognition, and more particularly in cursiv e\nscript recognition. Two approaches are used to extract fea-\ntures from images, one segmentation-based and the other\nsegmentation-free [11]. In the ﬁrst, a segmentation algo-\nrithm is used to separate letters or letter elements. In the\nsecond,thesystem worksexactlylike most speechrecogni-\ntionsystemsbasedonHMM witha slidingwindow.\n3. Experiments\nOur approach deals with each staff globally without seg-\nmenting the symbols and exploits the fact that a staff is\nformed by a juxtaposition of typographic types from left\nto right. Our solution is model discriminant and uses thetypefaceastherecognitionunit. Theideaistobuildadirec t\ncorrespondencebetweenthemusicfontusedintheprintand\nthe set of HMM used for recognition. In consequence, the\nnumber of HMM, i.e.the system vocabulary, corresponds\nto the numberof typefacesin the music font. Thereforethe\nsize of this vocabulary is about 200 different symbols. In\ntheend,theHMMset willconstituteatypographicalmodel\nofa speciﬁcmusicfont.\n3.1. Trainingdata\nWe built our training data from three different music prints\n[12, 13, 14] realized with two music fonts made up of very\ndifferent graphical forms. To obtain a ground-truthof each\nstaff, the musical content was entered via a MIDI keyboard\nin a music notation application. In all, 240 pages were en-\ntered,giving1,478stavesand52,178characterscorrespon d-\ningto175differentsymbols.\nWe also developed a pre-treatment system for scanned\npages,inwhichweuseascanningresolutionof400dpiwith\ngrayscale images. The image automatically extracted for\neach music stave is resized so that the distance between the\nbottom and the top staff lines is 100 pixels [15]. As well\nas saving time, the main advantage of this automatic pre-\ntreatmentsolution(skew correction,size normalization, de-\ntectionofstavesonthepages,ﬁltering,etc.) isthatitall ows\nourapproachtobeevaluatedonoriginaldataandnotondata\npreparedbyhand.\n3.2. Featureextraction\nWe use a segmentation-free approach and the feature ex-\ntraction is performed with a sliding window as in speech\nrecognition. We did not consider the segmentation-based\napproach because one of our goals was precisely to avoid\nsegmentation problems. The features are extracted directl y\nfrom the image without erasing the staff lines ﬁrst. Our so-\nlution therefore treats the staff lines implicitly in the fe a-\nture extraction. It is importantto note that this movesaway\nfromotherstudiesin OMRwhereremovalofstaff linesisa\nubiquitousandcomplicatedoperationperformedbeforethe\nrecognitiontaskitself[16]. Withourapproachwecanavoid\nthis task, which would have been much more complicated\nwith the prints considered here because of the irregulariti es\nofthestaff lines.\nWe tried different experiments with various numbers of\nfeatures – from 4 to over 40. Finally, we selected a solu-\ntionwithonly6valueschosenjudiciouslybasedonthestaff\nconﬁguration. Our tests showed their strongly discrimina-\ntivenatureandinanycaseadditionalvaluesdidnotimprove\nresultssigniﬁcantly.\nThe values are calculated as follows: for each window,\nwe determine the ndistinct connected black zones with h\nandwtheheightandthewidthofthewindow, Sitsareaand\nAthe total area of the black pixels. The ﬁrst value depends\non the number of zonesand correspondsto1\n1+n(this value\nis 1 if the window is empty). The second and third valuesare functions of the gravity centers cxandcy. The gravity\ncentersarecalculatedaccordingtotheequations(1)and(2 )\nwhere ci\nxcorresponds to the gravity center x(respectively\ny) of a connected zone and aito its area. The used values\nare1\ncxand1\ncyand0.5forbothifthewindowisempty.\ncx=/summationtextn\ni=1ci\nx·ai\nA·w(1)\ncy=/summationtextn\ni=1ci\ny·ai\nA·h(2)\nThe fourth feature corresponds toa(ni)\nSwhere a(ni)is\nthe area of the largest black element. The ﬁfth value corre-\nspondstoa(nj)\nSwhere a(nj)istheareaofthesmallestwhite\nelement. If the window is empty, these values are 0 and 1\nrespectively.\nThesixthvaluedependsonthetotalareaoftheblackele-\nmentsinthewindow. Thisvalueiscalculatedwithaweight-\ning mask. The idea isto givemoreimportanceto the pixels\nthatarebetweenthestafflinesthantothoseonthelines. Th e\nmaskisdesignedinsuchawaythatthenearerthepixelsare\nto the center of a line space, the higher the weight factor is\ninthemask. Inonesense,thewindowisstretchedvertically ,\nbut the stretchingis moreimportantnearerthe centerof the\nline spacethanthe line.\nFinally, if Iis the window and Mthe weighting mask,\nA′, the total area of the elements after weighting, is given\nby equation(3)and the value used is givenby equation(4).\nThevalueis0if thewindowisempty.\nA′=w/summationdisplay\nk=1h/summationdisplay\nl=1I(k, l)·M(k, l) (3)\nFeature =A′\n/summationtextw\nk=1/summationtexth\nl=1M(k, l)(4)\n3.3. Implicit treatmentofstaffcurvature\nIn some cases, staves can be curved on the image, for ex-\nampleifthepagewasnotﬂatwhentheimagewasacquired.\nWe usea two-stageprocessto considerthiscurvaturein the\nfeature extraction phase. The ﬁrst stage enables the detec-\ntion of the exact staff height. It is performed with a sliding\nwindowof90pixels. Foreachwindow,wemakeahorizon-\ntal projection from which we can deduce the staff height at\nthat position. With this sliding window, the staff height de -\ntection performs well even if the staff is curved or leaning.\nForeachstaffwekeepthemedianvalue,andforeachpage,\nthemedianofthevaluesobtainedforeachstaff.\nInthesecondoperation,wedetectlocallytheverticalpo-\nsition of the staff. The principle is again to use a sliding\nwindow and to ﬁnd for each window the staff position by\ncorrelating the horizontal projection of the window with a\ncorrelation mask representing the staff lines. The mask is\nbuilt on the staff height detected (1 for the staff line and 0elsewhere) with a line width of 5 pixels. A maximumof 20\npixels above and below a reference position is considered.\nThe curvaturedetected is used in the feature extraction, fo r\nexamplewhenapplyingtheweightingmask(ﬁgure2).\nFigure2. Staffcurvature infeature extraction\n3.4. HMMtopology\nWe use left-right HMM because they model the sequential\nandunidirectionalnatureofthesequencewetreatherewell .\nConcerning the number of states, different experiments we\nmade showed that the best results are obtained if the num-\nber of states of each HMM matches as closely as possible\nthe width in pixelsofthe correspondingsymbol. Thischar-\nacteristichasalreadybeennoticedbystudiesinhandwriti ng\nrecognition[17].\nOnthebasisofthetypesfoundinthefontsofourtraining\ndata, we deﬁned three classes of topologies based on the\ntypewidths(ﬁgure3). Alltypesofourfontsﬁttheseclasses ,\nbut it is clear that with a font with types of another width\n(with oblique ligatures for example) it would be necessary\nto modifythenumberofclasses.\n(A) Class I\n (B) Class II\n(C) Class III\nFigure3. Types of differenttopological classes\nFor one font, with wthe size of the window and rthe\nnumberofoverlappingpixelseachtimethewindowismoved,\nthenumberofstatesofanHMMforeachclass Scisdeﬁned\nbyequation(5)where P(c)isthewidthinpixelsofthetypes\nofthisclassforthisspeciﬁc font.\nSc=P(c)\nw−r(5)\n3.5. Spacebetweensymbols\nSpace between symbols can vary from one print to another\nbut also within the same print or the same staff. Sometimes\nsymbolsare juxtaposedor spaced out by one or more spac-\ning types (ﬁgure 4). To manage these different situations,\nwe use a special modelused for silencesin speech recogni-\ntion [18]. This model is considered neither in ground-truth\nsequencesnorinrecognitionoutput.\nFigure4. Spacebetweensymbols\n3.6. Distribution,initializationand learning\nWe used HTK [19] for our experiments with a continuous\napproach where for each state in the HMM output proba-\nbility distribution is given by 5 gaussians. Usually, bette r\nresultsareobtainedifonlyonegaussianisconsideredatth e\ninitialization and the numberof gaussiansis increased lat er\n[17]. Initialization of the models is done without ground-\ntruth specifying the position of each symbol in the image.\nThe training is then performed with the embedded version\nof the Baum-Welch algorithm. For every training iteration,\neachstaffisusedoncetoadaptthemodelscorrespondingto\nthesymbolsofwhichthestaffismade.\n4. Results\n4.1. Evaluationsets\nToevaluateourresults,weusedaset FMIXwhichcontains\ndata regardless of the font used in the print, and two sets\nbased on one print [13] to compare results obtained on the\nsamedatabutinonecaseextractedfromacleanedfacsimile\nedition ( WFS) and in another from a microﬁlm of the orig-\ninal (WMF). This comparison enabled us to evaluate how\nrobustoursolutionistonoise.\nTable1. Evaluationsets\nData Staves Characters Symbols\nFMIX1,478 52,170 175\nWFSandWMF 491 22,290 152\nEvaluationoftheresultsisbasedontheeditdistancebe-\ntween the recognized sequence of symbols and the correct\none. Each time, we calculated the effectiverecognitionrat e\n(REC)andwhatwecallthemusicalrecognitionrate(MUS)\nwhich does not consider height shift errors on symbols for\nwhich this has no musical consequence. For example, if a\nrestiscorrectlyrecognizedbutonelinebeloworabove,thi s\nisarecognitionerrorwhichhasnoconsequencefromamu-\nsical point of view. For each evaluation, we performed a\nk-fold cross-validation with k=10. Because the ideal num-\nberoftrainingiterationscannotbedeterminedtheoretica lly,\nweusedthemeanvalueoftheresultsobtainedfromtheﬁnal\nﬁve training iteration models. We used 2 as window width\nwith nooverlap.\nIf we compare results obtained on WFSandWMF, we\ncan see that the difference is small with less than 1.5% of\nlosswith themicroﬁlmimageswithmost noise.Table 2. Recognition rates\nFMIX WFSWMF\nREC96.8297.1695.77\nMUS 97.1197.4296.22\n5. Conclusion\nWhereas traditional OMR acts at symbol level using com-\nplex pattern recognition processes, we advocate the use of\nHMM for OMR of early typographic prints to operate di-\nrectlyatstafflevel. Weshowthatbyusingwell-chosenfea-\ntures and mono-dimensional models, accurate recognition\ncan be achieved directly from the original microﬁlm with-\nout the need for unreliable pre-processing ( e.g.staff-line\nremoval). In particular, our recognition process integrat es\nthestaffcurvatureaswell asthestafflineswithoutanyseg -\nmentationoftheindividualelements. Wealsoshowthatour\nsolution performswell even with image noise, which is es-\nsential whendealingwitholddocuments.\nWith the topological classes proposed, the learning pro-\ncess is initiated without the need to specify the position of\nthesymbolsintheground-truth,whichgreatlysimpliﬁesth e\ncreationoftrainingdata.\nOneinnovativeaspectofourrecognitiontechniqueisthat\nthe staves are considered as sequences of symbols. This\nmeans that we can imagine building models of these se-\nquences that can be integrated into the recognition process ,\nenablingustoevaluatethevalidityorprobabilityofa give n\nmusicalsequence. Themodelcouldbeasetofgrammatical\nrules,e.g.for counterpoint, or a stochastic model, such as\na class-based n-grammodeladaptedto music. Thisinnova-\ntive approach to early music recognition opens up a whole\nrangeofnewpossibilitiesforfutureresearch.\nThe next stage should be to work on much more data in\norder to better evaluate whether it is more efﬁcient to sepa-\nrate typographicalmodelsfor each typeface or to use a sin-\nglemodelforoneormoretypefaces.\nAcknowledgments\nThis work was supported by the Swiss National Science\nFoundationthrougharesearchprojectatGenevaUniversity ,\nSwitzerland,underthesupervisionofProf. E. Darbellay.\nReferences\n[1] H. E. Poole, “Music Printing,” in Music Printing and Pub-\nlishing, D. W. Krummel and S. Sadie, Eds. New York:\nNorton, 1990, pp. 3–78, part one.\n[2] G. S. Choudhury, T. DiLauro, M. Droettboom, I. Fujinaga,\nB. Harrington, and K. MacMillan, “Optical Music Recog-\nnition System within a Large-Scale Digitization Project,” in\nProceedingofthe1stInternational Conference onMusicIn-formation Retrieval (ISMIR’00) , Plymouth Massachusetts,\n2000, online presentation.\n[3] L. Pugin, “Computer Software for Early Music Editions: A\nNew Approach,” in Music, Poetry, and Patronage in Late\nRenaissance Italy: Luca Marenzio and the Madrigal. Inter-\nnational Conference, Harvard University, April 7-82006 , to\nappear.\n[4] D. Blostein and H. S. Baird, “A Critical Survey of Music\nImage Analysis,” in Structured Document Image Analysis ,\nH. Bunke andK.Yamakato, Eds. Springer, Berlin,1992.\n[5] D. Bainbridge and T. Bell, “The Challenge of Optical Mu-\nsic Recognition,” in Computer and the Humanities , vol. 35,\n2001, pp. 95–121.\n[6] N. Carter, “Automatic Recognition and Related Topics: R e-\nport for Research at Guildford, University of Surrey,” Com-\nputinginMusicology: ADirectoryofResearch ,vol.7,1991.\n[7] D. Bainbridge and T. Bell, “A Music Notation Constructio n\nEngine forOptical Music Recognition,” Software –Practice\nand Experience , vol. 33, no. 2, pp. 173–200, 2003.\n[8] J.C.Pinto,P.Vieira,andJ.M.Sousa,“ANewGraphic-Lik e\nClassiﬁcation Method Applied to Ancient Handwritten Mu-\nsicalSymbols,” InternationalJournalonDocumentAnalysis\nand Recognition (IJDAR) ,vol.6, no. 1, pp. 10–22, 2003.\n[9] C.DalitzandT.Karsten, “Usingthe GameraFrameworkfor\nBuildingaLute TablatureRecognition System,”in Proceed-\ning of the 6th International Conference on Music Informa-\ntionRetrieval (ISMIR’05) ,London, 2005, pp. 478–81.\n[10] G. E. Kopec and P. A. Chou, “Markov Source Model for\nPrinted Music Decoding,” Journal of Electronic Imaging ,\nvol. 5,no. 1, 1996.\n[11] T. Steinherz, E. Rivlin, and N. Intrator, “Ofﬂine Cursi ve\nScript Word Recognition - a Survey,” International Journal\non Document Analysis and Recognition (IJDAR) , vol. 2, no.\n2-3, pp. 90–110, 1999.\n[12] E.Ghibel, Ilprimolibrodemadrigaliatrevocianotenegre .\nVenezia: A. Gardano, 1552, RISM G-1773, facsimile (Peer:\nAlamire,1984).\n[13] A. Willaert, Fantasie recercari contrapunti a tre voci .\nVenezia: A. Gardano, 1559, RISM W-1121, D-Mbs, micro-\nﬁlmand facsimile (Peer: Alamire, 1986).\n[14] L. Marenzio, Il quinto libro de madrigali a sei voci .\nVenezia: A.Gardano,1595,RISMM-0516,I-Bc,microﬁlm.\n[15] L. Pugin, “Lecture et traitement informatique de typog ra-\nphies musicales anciennes. Unlogicielde reconnaissance d e\npartitions par mod` eles de Markov cach´ es,” Ph.D. Disserta -\ntion, Universityof Geneva, 2006.\n[16] I.Fujinaga,“StaffDetectionandRemoval,”in VisualPercep-\ntion of Music Notation: On-Line and Off-Line Recognition ,\nS.E.George, Ed. Hershey: IRMPress,2005, ch. 1.\n[17] S.G¨ unter andH. Bunke, “Optimizingthe Number of State s,\nTraining Iterations and Gaussians in an HMM-based Hand-\nwritten Word Recognizer,” in Proceedings of the 7th Inter-\nnationalConference onDocumentAnalysisandRecognition\n(ICDAR’03) ,vol. 1,Edinburgh, 2003, pp. 472–6.\n[18] S. Young et al.,The HTK Book (for HTK Version 3.2) , De-\ncember 2002, PDFversion, <http://htk.eng.cam.ac.uk >.\n[19] Cambridge University Engineering Department. Hidden\nMarkov Model Toolkit. <http://htk.eng.cam.ac.uk >."
    },
    {
        "title": "A Study on Music Genre Classification Based on Universal Acoustic Models.",
        "author": [
            "Jeremy Reed",
            "Chin-Hui Lee"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1417255",
        "url": "https://doi.org/10.5281/zenodo.1417255",
        "ee": "https://zenodo.org/records/1417255/files/ReedL06.pdf",
        "abstract": "Classification of musical genres gives a useful measure of similarity and is often the most useful descriptor of a musical piece.  Previous techniques to use hidden Markov models (HMMs) for automatic genre classification have used a single HMM to model an entire song or genre.  This paper provides a framework to give finer segmentation of HMMs through acoustic segment modeling.  Modeling each of these acoustic segments with an HMM builds a timbral dictionary in the same fashion that one would create a phonetic dictionary for speech.  A symbolic transcription is created by finding the most likely sequence of symbols.  These transcriptions then serve as inputs into an efficient text classifier utilized to provide a solution to the genre classification problem.  This paper demonstrates that language-ignorant approaches provide results that are consistent with the current state-of-the-art for the genre classification problem.  However, the finer segmentation potentially allows for “musical language”-based syntactic rules to enhance performance. Keywords: musical genres, acoustic segment models, hidden Markov models, latent-semantic indexing",
        "zenodo_id": 1417255,
        "dblp_key": "conf/ismir/ReedL06",
        "keywords": [
            "musical genres",
            "acoustic segment modeling",
            "timbral dictionary",
            "symbolic transcription",
            "text classifier",
            "genre classification problem",
            "language-ignorant approaches",
            "musical language",
            "syntactic rules",
            "performance"
        ],
        "content": "A Study on Music Genre Classification Based on Universal Acoustic Models\nJeremy Reed \nSchool of Electrical and Computer Engineering  \nGeorgia Institute of Technology \nAtlanta, GA 30332 \njeremy.reed@gatech.edu Chin-Hui Lee \nSchool of Electrical and Computer Engineering  \nGeorgia Institute of Technology \nAtlanta, GA 30332 \nchl@ece.gatech.edu \nAbstract \nClassification of musical genres gives a useful measure of \nsimilarity and is often the most useful descriptor of a \nmusical piece.  Previous techniques to use hidden Markov \nmodels (HMMs) for automatic genre classification have \nused a single HMM to model an entire song or genre.  This \npaper provides a framework to give finer segmentation of \nHMMs through acoustic segment modeling.  Modeling \neach of these acoustic segments with an HMM builds a \ntimbral dictionary in the same fashion that one would \ncreate a phonetic dictionary for speech.  A symbolic \ntranscription is created by finding the most likely sequence \nof symbols.  These transcriptions then serve as inputs into \nan efficient text classifier utilized to provide a solution to \nthe genre classification problem.  This paper demonstrates \nthat language-ignorant approaches provide results that are \nconsistent with the current state-of-the-art for the genre \nclassification problem.  However, the finer segmentation \npotentially allows for “musical language”-based syntactic \nrules to enhance performance. \nKeywords: musical genres, acoustic segment models, \nhidden Markov models, latent-semantic indexing \n1. Introduction \nWith the advent of MP3 and other audio coding schemes, \nmusic content analysis has become a growing research \narea.  Genre provides a very useful description of a musical \npiece.  However, a lack of label consistency exists in the \nmusic community [1].  This leads to difficulties in \ncomparing the performance of genre classification \nalgorithms across databases. \nMusic genre classification is composed of two basic \nsteps: feature extraction and classification.  In the first \nstage, various features are extracted from the waveform.  In \nthe second stage, a classifier is built using the features \nextracted from the training data.  Li and Sleep [2] vector \nquantized Mel-frequency cepstral coefficients (MFCC), \nand then used the codebook assignments for each frame as a textual representation of the song.  A Lempel-Ziv-type \ncoding algorithm was then utilized to build a modified \nsupport vector machine (SVM).  In [3], spectral features \nare extracted and classification is performed using a binary \nclassification tree with each node containing a linear \ndiscriminant function (LDF) or single Gaussian classifier.  \nMeng and Shawe-Taylor [4] integrated MFCCs into an \nautoregressive model to build long-term features, which \nwere placed into a linear neural network and a SVM \nclassifier. \nIt has been suggested that music genre classification \nparallels the spoken language identification problem [5].  \nJust as language governs the syntax of phonemes and \nwords, a musical genre’s theoretical structure governs the \nsyntactic order of sounds.  For example, the basic 12-bar \nblues form specifies an ordering of I, IV, and V chords.  In \nother words, music genre imposes syntactic constraints that \ninfluence transition probabilities between fundamental \nacoustic units (notes and chords), which is similar to how \nlanguage imposes probabilistic constraints on phone and \nword transitions.  In addition, these fundamental units vary \nin both observational feature values and in duration.  In \nspeech, variable-length acoustic units are modeled using \nhidden Markov models (HMMs) [6].   The variable-length \nsegments is the key difference between this proposed \napproach and the one found in [2].  In addition, these \nvariable-length segments are treated as fundamental \nacoustic units in this study, upon which higher-cognitive \ntype approaches may be built to improve results.  There \nhave been attempts to incorporate HMMs into the design of \ngenre classifiers.  Scaringella and Zoia [7] modeled each \ngenre by a single 4-state HMM, with each state \ncharacterized by a Gaussian mixture model (GMM) with 3 \nmixture components.  Aucouturier and Pachet [8]  use a \nsingle HMM for each song in the training database and \nassociate each test song to the genre model that scores \nhighest.  The equivalence to speech would be to model \nspeech at the language (genre) or utterance (song) level.  \nAlmost all speech HMMs model much smaller units, e.g. \nphonemes and words, which parallel musical notes and \nchords. \nA problem in modeling at the note level in music lies in \nthe fact that there are no transcribed databases for music \nand automatic transcription is not yet a solved problem.  \nMany difficulties exist in creating these corpora, including Permission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fee provided that copies \nare not made or distributed for profit or commercial advantage and that \ncopies bear this notice and the full citation on the first page. \n© 2006 University of Victoria time, money, and copyright regulations.  Even should \ntranscribed corpora exist one day, many additional \nproblems need to be addressed.  For instance, music is not \nmonophonic, but is often composed of multiple instruments \nplaying many notes simultaneously.  In order to have a \nsingle HMM for each note, source separation would have \nto become a realizable possibility.  If HMMs are \nconstructed for observations of multiple notes being played \nat once, studies need to be conducted to determine whether \nthe number of states in a note is instrument dependent, if \ndifferent HMMs need to be constructed for every possible \nnote in a chord or whether key notes such as the root note \ncould serve as an anchor note in identifying the HMM to \nuse for the chord, etc. \nThis paper argues that a smaller representation based on \nacoustic segment models (ASMs) [9] is a possible solution \nuntil transcribed databases become a realistic alternative.  \nIn fact, much of this paper is based on a language \nidentification approach by Ma, et al. [10].  A textual \ntranscription of each song is created by finding the most \nprobable sequence of ASMs.  Therefore, each song can \nserve as the musical equivalent of a document that is \ncomposed of a vocabulary of symbolic units.  These \ntranscriptions allow for more robust text retrieval \nalgorithms, which this paper accomplishes through latent \nsemantic indexing (LSI).  This algorithm transforms word \ncounts into multidimensional vectors, which are then used \nto build the final classifiers.  The SVM [11] was used for \nthis study.  This approach provides an initial foundation for \nfuture improvements through the use of syntax and \n“musical language”-based rules. \nThe rest of this paper is organized in accordance with \nthe flow of the algorithm.  In Section 2, ASMs are \ndiscussed for a musical framework.  A musical text-based \nSVM classifier design is discussed in Section 3.  The \nresults, including an analysis across new standard \ndatabases is given in Section 4.  Finally, our conclusions \nare given in Section 5. \n2. Universal Acoustic Models \nUniversal acoustic models are based on the idea that an \nacoustic utterance can be described by a sequence of \nsmaller units, e.g. phones build up to words and sentences.  \nReal signal observations can be considered as noisy \nrepresentations of these basic units.  The models \ncorresponding to these units form a standard set capable of \nrepresenting every possible combination of sounds.  If a \nlabeled training corpus exists, it can be used to train \nHMMs, as is done in speech.  However, no such corpus \nexists for music.  Therefore, an unsupervised approach is \nutilized. \nAssuming that the features extracted from the audio \nsignal accurately describe the various sounds encountered, \none would expect that the real, noisy observations of the \nsame fundamental unit to be close by some metric and observations of different units to be far apart.  Therefore, \nthe basic units can be found by vector quantizing (VQ) \n[12] the acoustic space.  The resulting clusters are then \nrepresented with symbols that serve as entries in an \nacoustic codebook.  Each song is then represented as a \nsequence of symbolic observations based on distance \nmeasures between song segments and codebook entries.  \nThis idea of breaking an acoustic utterance into segments \nand assigning each segment to an entry of a global acoustic \ncodebook is known as tokenization [9]. \n2.1 Initial Segmentation and transcription \nAs described in [6], training HMMs requires labeled \ntraining data, but since no such data exists for music \ncurrently, the ASM approach is used to build initial \ntranscripts.  The individual ASMs are a global set that is \nfound by finding clusters of observations in the training \ndata.  Because the HMM training process is an iterative \nprocess, only a rough initial transcription is needed.  \nPotentially, a better segmentation scheme based on musical \nanalysis and theory can provide better results.  However, \nthe focus of this paper is to demonstrate that the ASM \napproach to segmentation provides results consistent with \ncurrent solutions.  More advanced domain specific-\nknowledge principles will be investigated in later research. \nTo find an initial set of ASMs and transcripts, each \naudio file is first divided into 25 ms, non-overlapping \nframes that are weighted by a Hamming window.  The \nwindows are chosen to be non-overlapping to decrease \ncomputation time as this is the most time consuming step in \nthe algorithm.  Because later HMM training stages will \nredefine better boundary locations, it was felt that \nsacrificing a finer segmentation at this stage for speed was \na fair tradeoff.  For each audio frame, 8 MFCCs are \nextracted.  This number was chosen empirically to balance \nbetween segments that were too short, e.g. every individual \nframe being labeled as a segment, and segments that were \ntoo long, e.g. multiple notes being grouped to form a \nsegment.  Intuitively, this makes sense because these low-\norder MFCCs describe the slowly changing spectral shape \n[4].  For each song, cepstral mean subtraction [13] and \nvariance normalization [14] have been applied, such that \nthe mean and variance of each coefficient are zero and one, \nrespectively.  Successive frames are then grouped into \nclusters such that they minimize the following distortion \nfunction \n() ( )\u0001\u0001\n= + =-=Q\nqb\nbtq tq\nqod QOD\n1 11, , m  (1) \nwhere O = (o1, o2, …, oT) are the observation vectors, \u0001q is \nthe centroid of the q-th segment which ends at bq (b0=0), \nand d(ot, \u0001q) is a distortion metric between ot and \u0001q.  This \npaper uses a simple Euclidean distance metric.  The \ndistortion is taken across the Q segments for each song.  The segmentation that minimizes this distortion function \ncan be found using the dynamic time-warping procedure \ndescribed in [15].  An example of segmented audio for two \nnotes from a song in the RWC Classical Music Database \n[16] is given in Figure 1. \n \nFigure 1.  Example of segmentation algorithm output. \nThe segmentation is very efficient in not only describing \nthe starting and endpoints of the audio, but also is able to \ndescribe the rough locations of the transitional parts, such \nas the attack, sustain, and release. \nEvery segment is then summarized by the means of the \nframes that compose the segment.  The means from each \nsegment in every training file is used to build a global VQ \ncodebook.  A transcript for each training file is then built \nby identifying the closest codebook entry for each segment. \nAn example of how these transcripts might look is given in \nFigure 2. \n \n x123 \nx54 \nx32 \nx3 \n… x54 \nx78 \nx93 \nx13 \n… \nSong 1 Song 2 … \n \nFigure 2.  Initial segment model transcripts. \nEach line in a file represents a symbolic codebook entry.  \nFor example, “x123” is the first “word” in Song 1, “x54” is \nthe second, etc.  In this way, each song is sequence of \nsymbols in the same way that a text document or speech \ntranscription is a sequence of words. \n2.2 ASM/HMM Training \nThe transcripts obtained in the previous step provide a \nstarting point for an iterative HMM training process.  \nWhile the first 8 MFCCs accomplish the task of finding the \ninitial segmentation, it has been found that using higher-\norder coefficients, energy, and their derivatives and \naccelerations yield better results for audio and speech \ncontent.  With this idea in mind, each training and testing audio file is divided using a sliding window of 25 ms taken \nevery 10 ms.  Each frame is weighted by a Hamming \nwindow to limit edge effects.  For each frame, the first 12 \nMFCCs energy, derivatives, and acceleration coefficients \nare found to build a 39-dimension feature vector for each \nframe.  Again, cepstral mean subtraction and variance \nnormalization is applied to both the training and testing \ndata.  The training data and associated initial transcripts are \nused to train a set of HMMs (equal to the number of \nASMs), with each HMM having 3 states.  This number was \nchosen based on current trends in speech research, but may \nnot be the most ideal choice.  More experimentation will be \nnecessary to determine an appropriate number and whether \nthis number is dependent on instruments, style of play, etc.  \nEach state is characterized by a Gaussian mixture model, \nwith the number of mixtures found by increasing the \nnumber until no noticeable improvement is found in the \nperformance.  For this study, a total of 16 mixtures per \nstate was adequate.  For a detailed description of the HMM \ntraining process the reader is referred to [6].  After training \nthe HMMs, they are used to re-estimate the transcription of \nthe training files.  These transcripts will be different from \nthe original transcripts and are used to further train the \nHMMs.  This process is repeated until only a small amount \nof improvement is noticed with the training data. \n3. Text-Based Classification \nThe ASM transcription process creates a string of HMM \nsymbols for each training song.  These final transcripts can \nbe thought of as a timbral score (as in a “score of music”).  \nThis symbolic format allows for the use of proven text \nclassification techniques commonly used in the information \nretrieval community. \n3.1 Acoustic Language \nAs stated in Section 1, music is structured in its creation, \nwith deviations being used to incite senses of novelty and \nto prevent boredom.  The ASMs produced in Section 2 can \nbe viewed as terms or even as an alphabet of an acoustic \nlanguage.  Their co-occurrences could be seen as syntax, \neven if on a rough level.  While the authors want to caution \nagainst the belief that the brain processes music and \nlanguage in the same fashion, there does seem to be some \nsimilarity.  This can be seen with music theory, which \ndictates syntactical usage.  One common phenomena in \nlanguage processing is Zipf’s Law [17], which says if one \nranks the terms in order of their frequency, f, in a large \ncorpus of any language, then the relationship between f and \nthe rank, r, will be \nrf1µ              (2) A surprising result was found when this applied to the \nMagnatune1 database.  While the unigrams (appearance of \na single symbol) did not show this result, the bigrams \n(appearance of two symbols in specific order) did, as \ndemonstrated in Figure 3. \n \n \nFigure 3.  Bigram frequency versus rank for the songs in the \nMagnatunes training set. \nThe authors suggest that a possible reason that bigrams \nexhibit this behavior and why unigrams do not is that \nmusic’s information is not carried in the individual tones, \nbut in the difference between pairs of tones.  That is, it is \nnot the individual sounds which are the basic building \nblocks, but the pairs of tones which develop the concept of \nmelody.    For instance, transposing a melody to another \nkey changes the note names and individual sound, but the \nsense of melody remains the same.  \n3.2 Latent Semantic Indexing \nOne such approach that has proven successful is latent \nsemantic indexing (LSI) [18], which represents a training \ncorpus by a term-document matrix with the rows \ncorresponding to the individual terms and the columns \nrepresenting the documents.  In addition to the unigram \ncounts, bigram counts can also be considered for each \ndocument.  Therefore, if there are J=128 terms, then each \ncolumn (in this case, song) is a vector of size M = \nJ+J*J=16512, with J unigrams, which are the ASMs \ndescribed in Section 2, and J*J bigrams.  Specifically, each \nelement in the matrix, W, is given by \n\u0002\u0002\n\u0003\u0004\n\u0005\u0005\n\u0006\u0007\n-=\njji\ni jinc\nw,\n, 1e         (2) \nwhere ci,j is the number of times that word i appears in \ndocument j and nj is the number of words in document j.  \nThe term \u0002i is the normalized entropy of word i and is given \nby \n\u0001\n=-=N\nji\niji\ni ttc\nT1,loglog1e         (3) \n                                                           \n1 http://www.magnatune.com \n where ti is the total number of times that word i appears in \nthe training documents and T is the number of training \ndocuments.  The word entropy gives a measure of the \nindexing power.  Specifically, values close to zero indicate \nthat the word has more indexing power than words with \nvalues close to one because the former appears in fewer \ndocuments.  Typical examples of maximal indexing words \nare proper names, while values very close to one are often \nin function words (e.g. “the” and “a”). \nEven for large databases, many bigrams will never \nappear in the training data.  Leaving these values as zero \ncan lead to undesirable results.  Therefore, they are often \nassigned a very small constant or smoothed by some other \nmethods.  In general, one implements feature reduction \nthrough singular value decomposition [18], however, \nbecause current music research databases do not contain a \nlarge number of songs, this was seen as unnecessary for \nthis study. \n3.3 Evaluation Measures \nThree measures of performance are often used: precision, \nrecall, and accuracy.   Precision and recall are useful \nmeasures for comparing individual genres because they are \nindependent to the number of examples that may exist in \neach genre.  The formulas for these two performance \nmeasures are \nfp tptpec precision+= =Pr   (4) \nfn tptpc recall+= =Re                    (5) \ntNtpAcc accuracy = =                    (6) \n \nwhere tp, fp, fn, and Nt are the true positives, false \npositives, false negatives, and total number of test queries, \nrespectively. \n3.4 Support Vector Machines \nSupport vector machines (SVM) [11] have proven to be \neffective in a variety of classification problems.  The idea \nbehind SVMs is to project data onto a higher dimensional \nspace in order to separate classes with a LDF, which \nmaximizes the margin between competing classes.  The \nsoftware package SVMlight [19] was used for training and \nclassification. \nThe inputs into the SVMs are the LSI vectors created in \nthe previous step (one vector per song).  The output of this \nclassifier is a score, with a positive value indicating one \nclass and a negative value indicating the other class.  \nHowever, the song genre problem is multi-category \nproblem where each song is assigned to the most likely genre.  In order to incorporate SVM into a multi-category \nproblem, the “one-against-one” voting scheme [20] was \nemployed.   \n4. Experimental Results \n4.1 Dataset \nFor this study, the training and testing files were obtained \nfrom Magnatunes, which was used for the 2004 ISMIR \nContest.  However, HMMs require a lot of data during \ntraining; therefore, the RWC [16] and Dortmund [21] \ndatabases were added to train the global HMMs.  However, \ndifferent people will label the same song differently.  To \nprevent such labeling inaccuracies from influencing the \nresults, only the final transcripts arising from Magnatunes \nfiles were utilized in the creation of the term-document \nmatrix.  To create this matrix, each training song was \ndivided into 30-second, non-overlapping segments, and \nthese segments served as the documents with their \nsymbolic unigram and bigram counts serving as the terms.  \nThe test songs were divided in a similar fashion to create \nthe test queries.  The authors realize that ideally, each song \nin its entirety would be represented by a single document.  \nHowever, with the size of current research databases, some \ngenres are underrepresented.  In addition, diversity in \nartists is also needed to adequately describe a genre.  \nTherefore, the files were divided because of the large data \ndemands of SVM classifiers.  An artist filter [22] was used \nso that songs from a single artist were used for either \ntraining or testing, but not both.  The breakdown is \ndemonstrated in Table 1 and is given in full detail on the \nfirst author’s website2  For each genre, the number of \nindividual songs and the total number of 30-second \nsegments for each genre is illustrated. \nTable 1.  Training and testing databases used for each genre. \n Training Testing \nGenre Full Segment Full Segment \nClassical 109 580 30 287 \nElectronic 115 580 30 316 \nRock 92 560 30 223 \nJazz/Blues 53 430 21 180 \nAmbient 50 571 28 297 \n4.2 Behavior of ASM Training \nAs stated in Section 2, the training of HMMs is an iterative \nprocess of finding the ASMs and then creating new \ntranscripts.  To view how the testing data responds to this \nprocess, the results for the first four iteration rates are \nshown in Table 2 using 128 ASMs. \nTable 2.  Accuracy versus iteration number. \nIteration 1 2 3 4 \nAcc (%) 67.87 69.32 72.14 72.86 \n                                                           \n2 http://users.ece.gatech.edu/~jreed/ The accuracy rates increase each time a new set of \ntranscripts for the training data are created and the HMMs \nare retrained with the new transcripts.  There does appear \nto an asymptotic value close to 73%.  This is consistent \nwith previous solutions to this problem and is often cited as \nthe “glass ceiling” of performance which cannot be \nsurpassed without taking higher level cognitive processing \ninto account [8]. \nTo get an accurate view of the SVM training process, \nthe number of support vectors (SV) for the 128-ASM \nclassifiers are listed in Table 3. \nTable 3.  Number of support vectors for the 128 ASM classifiers. \nClassifier type Num. SV \nClassical versus Electronica 488 \nClassical versus Rock 478 \nClassical versus Jazz 550 \nClassical versus Ambient 677 \nElectronica versus Rock 672 \nElectronica versus Jazz 558 \nElectronica versus Ambient 716 \nRock verus Jazz 561 \nRock versus Ambient 574 \nJazz versus Ambient 641 \n4.3 Genre Confusion \nThe final confusion matrix is displayed in Table 4 for the \nSVM maximum vote classifier, where the rows represent \nthe ground truth as labeled in the metadata from \nMagnatunes and the columns represent how the algorithm \nclassified the test songs.  Recall and precision rates are \nshown as defined in Section 3.3 as well. \nTable 4.  Final confusion matrix for SVM classifier with C = \nclassical, E = electronic, R = rock, J/B = jazz and blues, and A = \nambient \nGenre C E R J/B A Rec \nC 26 0 1 1 2 86.7 \nE 0 19 9 0 2 63.3 \nR 0 5 24 0 1 80.0 \nJ/B 1 2 5 12 1 57.1 \nA 1 4 2 1 21 72.4 \nPrec 92.9 63.3 58.5 85.7 77.8  \nMost errors occur in just one other class and can be \nexplained by the fact that many songs are not necessarily \n“strictly jazz”, “strictly electronic”, etc.  For instance, some \nof the files in the Magnatunes corpus are described as \n“electronic rock with a pop edge.”  This may indicate that \nmany of the proposed genre classification schemes need to \nbe extended to allow for multi-topic categorization.  \nAdditionally, heuristic clues based on perception and \ncognition may help in discrimination. \n4.4 ASM Size Performance \nAn important variable is the number of ASMs that are used \nas unigram terms.  If the number of ASMs is too small, then there will not be enough acoustic coverage.  However, \ntoo many ASMs will lead to a large dimensionality and \nrequires more training data and computation time.  \nAccuracies, as defined in Section 3.3, using 64 and 128 \nASMs after 2 iterations are shown in Table 5. \nTable 5.  Genre accuracies vs. number of ASMs \n64 ASM 128 ASM \n55.41% 69.32% \nA significant increase in performance (13.91%) can be \nseen as the number of ASMs increases from 64 to 128.   \n5. Conclusion \nThe algorithm we have presented provides comparable \nresults to past solutions of the genre classification problem.  \nHowever, efficient segmentation of HMM modeling is \nprovided with this approach.  Previous use of HMMs for \nthis problem modeled an entire song or genre with a single \nHMM.  If genre classification is comparable to language \nrecognition, then modeling HMMs in this way would \nequate to having a single HMM for an entire spoken \ndocument or entire language.  Most speech applications use \nHMMs on the phonetic level and are therefore able to use \nsyntactic rules to improve classification performance.  This \nstudy demonstrates that a similar approach may be possible \nfor music, even though labeled training corpora are not in \nexistence. \nUsing the acoustic segment model idea on music allows \nfor a “timbre dictionary” to be created, which is then used \nto train HMMs that represent the entire acoustic space.  \nThe resulting transcriptions allow for a conversion into a \ntextual transcript so that efficient text retrieval algorithms \ncan then be utilized. \nReferences \n[1] F. Pachet and D. Cazaly.  “A Taxonomy of Musical \nGenres,” in Proc. Of Content-Based Multimedia Conf. on \nInformation Access Conf, April 2000. \n[2] M. Li and R. Sleep.  “Genre Classification via an LZ78-\nBased String Kernel,” in ISMIR 2005 Sixth Conf. on Music \nInf. Retr. Proc., 2005, pp. 252-259.  \n[3] K. West and S. Cox.  “Features and Classifiers for the \nAutomatic Classification of Musical Audio Signals,” in \nISMIR 2004 Fifth Conf. on Music Inf. Retr. Proc., 2004. \n[4] A. Meng and J. Shawe-Taylor.  “An Investigation of \nFeature Models for Music Genre Classification Using the \nSupport Vector Classifier,” in ISMIR 2005 Sixth Conf. on \nMusic Inf. Retr. Proc., 2005, pp. 604-609.  \n[5] A.G. Krishna and T.V. Sreenivas. “Music Instrument \nRecognition: From Isolated Notes to Solo Phrases,” in \nICASSP ‘04, vol. 4, pp. 265-268, May 2004. \n[6] L.R. Rabiner.  “A Tutorial on Hidden Markov Models and \nSelected Application in Speech Recognition,” in Proc. Of \nIEEE, Iss. 2, vol. 77, pp. 257-286, Feb. 1989. [7] N. Scaringella and G. Zoia.  “On the Modeling of Time \nInformation for Automatic Genre Recognition Systems in \nAudio Signals,” in ISMIR 2005 Sixth Conf. on Music Inf. \nRetr. Proc., 2005, pp. 666-671. \n[8] J.-J. Aucouturier and F. Pachet.  “Improving Timbre \nSimilarity: How High’s the Sky?” in J. of Negative Results \nin Speech and Audio Sciences, vol. 1, 2004. \n[9] C.-H. Lee, F.K. Soong, and B.-H. Juang.  “A Segment \nModel Based Approach to Speech Recognition,” in \nICASSP ’88, vol. 1, pp. 501-541, 1998. \n[10] B. Ma, H. Li, and C.-H. Lee.  “An Acoustic Segment \nModeling Approach to Automatic Language \nIdentification,” in Interspeech 2005 Eurospeech – 9th \nEuropean Conf. on Speech Comm. and Technology, \nSeptember 4-8, 2005. \n[11] C. Burges. “A Tutorial on Support Vector Machines for \nPattern Recognition,” in Data Mining and Knowledge \nDiscovery, vol. 2, 121-167, 1998. \n[12] Y. Linde, A. Buzo, and R. M. Gray.  “An Algorithm for \nVector Quantizer Design,” in IEEE Trans. On Comm., vol. \n28, iss. 1, pp. 85-95, Jan. 1980. \n[13] A. Anastasakos, et. al.  “Adaptation to new microphones \nusing tied-mixture normalization,” in ICASSP ’94, vol. I, \npp. 19-22, April 1994. \n[14] C.-P. Chen, J. Bilmes, and K. Kirchhoff.  “Low-Resource \nNoise-Robust Feature Post-Processing on the Aurora \n2.0/3.0 Databases,” in ICSLP ’02, Denver, Col., 2002. \n[15] T. Svendsen and F. Soong.  “On the Automatic \nSegmentation of Speech Signals,” in ICASSP ’87, vol. 12, \npp. 77-80, April 1987. \n[16] M. Goto, H. Hashiguchi, T. Nishimura, and R. Oka. “RWC \nMusic Database: Popular, Classical, and Jazz Music \nDatabases”, ISMIR 2002, pp.287-288, October 2002. \n[17] G. K. Zipf.  Human Behavior and the Principle of Least \nEffort.  Cambridge, MA: Addison-Wesley. \n[18] J.R. Bellegarda.  “Exploiting Latent Semantic Information \nin Statisical Language Modeling,” in Proc. IEEE, vol. 88, \nno. 8, pp. 1279-1296, 2000. \n[19] T. Joachims.  Making large-Scale SVM Learning Practical.  \nAdvances in Kernal Methods – Support Vector Learning, \nB. Schölkopf and C. Burges and A. Smola (ed.), MIT-\nPress, 1999. \n[20] C.-W. Hsu and C.-J. Lin.  “A comparison of methods for \nmulti-class support vector machines,” in IEEE Trans. On \nNeural Networks, vol. 13, pp. 415-425, 2002. \n[21] H. Helge, I. Mierswa, B. Möller, K. Morik, and M. Wurst.  \n“A Benchmark Dataset for Audio Classification and \nClustering,” in ISMIR 2005Sixth Conf. on Music Inf. Retr. \nProc., pp. 528-531, Sept. 2005. \n[22] E. Pampalk, A. Flexer, and G. Widner.  “Improvements of \nAudio-Based Music Similarity and Genre Classification,” \nin ISMIR 2005Sixth Conf. on Music Inf. Retr. Proc., pp. \n623-633, Sept. 2005."
    },
    {
        "title": "Ask a Librarian: The Role of Librarians in the Music Information Retrieval Community.",
        "author": [
            "Jenn Riley",
            "Constance A. Mayer"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1414986",
        "url": "https://doi.org/10.5281/zenodo.1414986",
        "ee": "https://zenodo.org/records/1414986/files/RileyM06.pdf",
        "abstract": "Participation from music librarians has been sparse in the first six ISMIR conferences, despite many potential areas of common interest. This paper makes an argument for the benefit to both the library and Music IR communities of increased representation of librarians at ISMIR. An analysis of conference programs and primary publications of two music library organizations to determine topics from the library literature relevant to Music IR research is presented. A discussion follows of expertise music librarians could potentially contribute to Music IR research and the ways in which Music IR research could further the work of music librarians, in each of the topics represented in the library literature. Keywords: Music librarians, ISMIR.",
        "zenodo_id": 1414986,
        "dblp_key": "conf/ismir/RileyM06",
        "keywords": [
            "Music librarians",
            "ISMIR conferences",
            "Music IR communities",
            "Increased representation",
            "Library literature",
            "Music IR research",
            "Expertise contribution",
            "Music librarians work",
            "Music IR research",
            "Music librarians work"
        ],
        "content": "Ask a Librarian: The Role of Librarians in the Music Information Retrieval \nCommunity \nJenn Riley \nIndiana University \n1320 E. 10th St. E170 \nBloomington, IN 47405 \njenlrile@indiana.edu Constance A. Mayer \nUniversity of Maryland \n2511 Clarice Smith Performing Arts Center  \nCollege Park, MD 20742 \nmayer@umd.edu \nAbstract \nParticipation from music librarians has been sparse in the \nfirst six ISMIR conferences, despite many potential areas of common interest. This paper makes an argument for the benefit to both the library and Music IR communities of increased representation of librarians at ISMIR. An \nanalysis of conference programs and primary publications of two music library organizations to determine topics from the library literature relevant to Music IR research is presented. A discussion follows of expertise music \nlibrarians could potentially cont ribute to Music IR research \nand the ways in which Music IR  research could further the \nwork of music librarians, in each of the topics represented \nin the library literature. \nKeywords : Music librarians, ISMIR. \n1. Introduction \nSince its inaugural conference in October 2000, ISMIR has \nprovided a forum for exploring the topic of music information retrieval from a variety of perspectives. In a 2003 review of the proceedings of the 2000 and 2001 ISMIR conferences, Futrelle and Downie \n[1] outline \nseveral Music IR research communities—computer science and information retrieval, audio engineering and digital signal processing, musicology and music theory, library science, cognitive sc ience, psychology, philosophy, \nand law—and point out that dialogue among the communities is both enhanced and impeded by the fact that each community has its own methodologies, jargon, and \nphilosophies.  \nAlthough common sense suggests that practicing music \nlibrarians, who have historically managed music collections and assisted users in finding music information within them, would participat e actively in the conferences, \na review of the ISMIR proceedings from 2000 through 2005 reveals few papers or posters authored by members of this group.  Contributions include overviews of digital library projects \n[2][3][4], standards for digital image \ncapture of musical scores [5] and metadata [6].  \nAdmittedly, we could increase the perception of librarian participation by including pa pers and posters on library-\nrelated issues that were presented by technology staff in digital library programs or faculty and graduate students in schools of information science.  We could also argue that more music librarians have attended ISMIR conferences than have presented papers or posters. But we decided to focus on librarian participati on as measured by conference \npresentation partly because pr esentations are central to \ndiscourse at ISMIR conferences, but also because \ninstitutional support for attendan ce tends to be greater for \npresenters. The first task seemed to be to attempt to \nidentify topics and styles of presentation that resonate with music librarians but also fit in to the research agenda of \nISMIR. \nBuilding on the model employed by Futrelle and \nDownie, we surveyed the conference programs as well as the primary publications of two organizations that represent music librarian ship—The Music Library \nAssociation \n[7] and the International Association of Music \nLibraries Archives and Documentation Centres [8]—\nhoping to identify topics that both engage music librarians and intersect with the research  agendas of other Music IR \nscholars. Full-length articles from Notes: Quarterly \nJournal of the Music Library Association  \n[9], and Fontes \nartis musicae, Journal of the International Association of Music Libraries, Archives and Documentation Centres  \n[10] as well as public presentations from the Music \nLibrary Association Annual Meetings \n[11][12][13][14][15][16] and the IAML Annual \nConferences [17][18][19][20][21][22] covering the years \n2000-2005 were examined and broadly classified according to the topics listed in Table 1. \nPermission to make digital or hard copi es of all or part of this work for \npersonal or classroom use is grante d without fee provided that copies \nare not made or distributed for profit or commercial advantage and that \ncopies bear this notice and the full citation on the first page. \n© 2006 University of Victoria Table 1. Distribution of major topics identified  in MLA and IAML conferences and publications 2000-2005 \nTopics Descriptions MLA \nAnnual \nMeetings Notes IAML Conferences Fontes \nartis musicae \nMusic history and \nliterature Research in the historical  and cultural aspects of \nmusic including repertoire 17% 8% 9% 8% \nPrint collections Descriptions of print collections and historical \nprinting and publishing methodologies 18% 41% 24% 49% \nReference and user education Questions users ask; how we teach users to find information; usability of standard music databases 16% 12% 13% 0 \nDigital libraries Descriptions of digital libraries, digital collections, \nand digital preservation 9% 5% 15% 0 \nCataloging and metadata Various schemas for organizi ng information 10% 5% 7% 0 \nCopyright How does copyright affect our ability to create and \nprovide access to digital collections? 1% 1% 4% 0 \nLibrarianship Professional and methodological issues related to \nmusic librarianship 18% 25% 19% 40% \nOther Miscellaneous topics 11% 3% 9% 3% \nTotal \npercentages  100% 100% 100% 100% \n \nThe results, also shown in Table 1, present a snapshot \nof the kinds of issues that interest librarians enough to \ninspire them to write articles or prepare conference papers. \nNot surprisingly, music librarians, as practitioners, tend to focus much of their energy on performing the day-to-day duties of their jobs as effectively as possible. “Librarianship” topics involving library management, library facilities, and “how we do it well in our library,” provide core information and a sense of community for the practicing music librarian a nd represent an average of \n25.5% of the conference presentations and articles examined. While these topics , in themselves, are not \nappropriate for ISMIR, some understanding of the difference between those whose primary focus is research \nand those whose primary focus is practice would undoubtedly help to build n ecessary bridges between \nlibrarians and those from other disciplines.  \nMany of the other subject  areas suggest opportunities \nfor collaboration and dialogue. In the remainder of this paper, we will briefly explore these topics and make some practical suggestions about ways in which each community—music librarians and Music IR researchers—\ncan benefit future work. \n2. Music history and literature \n2.1 Librarians’ potential contributions to MIR \nMany academic music librarians hold graduate degrees in music and possess the skills to contribute scholarly research to the corpus of writings on music history and literature. Our brief survey of MLA and IAML conferences and articles reveal s that an average of 10.5% \nof the submissions fell into this category. Specific titles frequently reflected either the au thor’s research interest or \nthe locale chosen for the conference and focused on a broad range of musical styles including classical, world, jazz, bluegrass, and musical theatre. Music librarians could \nhelp to add to the general Music IR knowledge about types of music, notation and its interpretation across cultures and historical periods, and musical style.  They could also help Music IR researchers identify core repertoire for study. \n2.2 MIR benefits to librarians \nSome of the tools being developed by Music IR researchers to automate musi cal analysis, classification, \ntranscription, and theme extraction will provide invaluable assistance to musical scholar s, including those who are \npracticing music librarians.  These tools, when well designed, will allow the scholar to easily perform tasks that required hours of tedious work when performed manually. This is an area where dialogue between developers and scholars could be particularly productive. \nDevelopers can help music librarians stretch their imaginations while librarians can help developers define real problems to solve. \n3. Print collections \n3.1 Librarians’ potential contributions to MIR \nMusic librarians have historically acquired, collected, managed, preserved, and provi ded access to collections of \nphysical items related to the study of music: books, manuscripts, printed musical scores and, more recently, \nsound and video recordings. An average of 33% of the presentations and articles found in our survey shared information about unique, interesting, or special collections in music. These collections, as well as the depth of knowledge about them, can provide a corpus of material with which Music IR  researchers can work. Music \nlibrarians could ostensibly add value to ISMIR conferences with presentations about unique and special collections. \n3.2 MIR benefits to librarians \nAs libraries strive to transform physical collections into digital ones, Music IR resear ch can provide the means.  \nRecent ISMIR presentations have investigated standards for scanning and processing of digital scores, optical music recognition, and music transcription. All of these issues address the nontrivial problem of transforming print music collections—including hard-to-read manuscripts and a variety of notational systems—into true digital collections that can be searched, manipulated, and read. \n4. Reference and user education \n4.1 Librarians’ potential contributions to MIR \nWhile only about 10% of the articles and presentations reviewed in our study were di rectly related to reference \nand user education, most librarians spend at least some of their time on these tasks and can offer their perspectives on usability of databases, wishlists for automated reference functions, and ideas for further study. This is another area where close communication betw een librarians, with their \nday-to-day experience with users, and Music IR researchers, with their knowledge of the possibilities \noffered by technol ogy, could result in exciting new \nservices for library patrons. \n4.2 MIR benefits to librarians \nAlthough many of these concepts will be discussed in more detail in the following sections on digital libraries and metadata, we want to emphasize that reference and user education continue to be nefit greatly from advances in \nMusic IR research. Among the possibilities for enhancement to current services  are 1) advanced searching \ncapabilities to include genre, instrumentation, similarity, mood, tempo, rhythmic  and melodic patterns, and performer; 2) more sophisticated document retrieval that return results in ways that are meaningful to  individual \nusers; 3) audio retrieval-by-example; 4) audio matching where all instances of the same  piece can be located via an \naudio search; 5) improved cross-database searching capabilities; 6) more personalized user interfaces, including those designed for persons with special needs; and 7) automatic extraction of metadata. 5. Digital libraries \n5.1 Librarians’ potential contributions to MIR \nLibrarians possess skills that transform a simple pool of \ncontent into a true library in the online environment as well as in the physical one, and they have played key roles in the implementation of a number of digital music library systems. Many of these syst ems serve to increase access to \nnotable library collections beyond a library’s local users, as for example the numerous online sheet music sites developed by libraries, including those from the Library of Congress \n[23], Johns Hopkins University [24], and Indiana \nUniversity [25] and the recently-launched Cylinder \nPreservation and Digitization Project from the University of California, Santa Barbara \n[26]. Many librarians in \nacademic environments have made their audio course \nreserves available to students online. Other librarians have served as valuable contributors to the design of systems that take one step further and use content from the library as the core of digital music library systems with more advanced technologies layered above. The Harmonica project, funded by the European Commission’s TAP Libraries Programme, produced a large number of documents outlining potential areas of implementation and further study for fosteri ng networked access to music \nlibrary materials \n[27]. The Variations2 system from \nIndiana University, well represented at ISMIR \nconferences, is one example of a fully implemented system, intended to support mu sic teaching, learning, and \nresearch \n[28]. The success of these projects indicates \nlibrarians’ skills can provide similar benefits to new technologies being developed in the Music IR community. \nIn addition to functionality in digital library systems \nalready implemented, librarian s can contribute many ideas \nfor how to improve these syst ems in the future. Specific \nand appropriate genre, style, and instrumentation terms chosen and maintained by librarians could be used to provide high-level browsing of library collections, a method of access that has hi storically received little \nattention. Knowledge of musical works and their relationships to one another could be used to improve \nsearch results \n[29]. Librarians’ understanding of music \nliterature and how users move between secondary and primary sources could be used in the high-level design of digital library systems. As systems evolve, librarians can \nhelp to continually push functionality to new levels. \n5.2 MIR benefits to librarians \nInnovations in the Music IR community provide many opportunities for digital music library systems to improve \nend-user search and discovery beyond what librarians alone have envisioned. Deve loping technologies promise \nto allow easier integration of controlled vocabularies for names, titles, and subjects into  search interfaces, shifting \nthe responsibility for knowing the \"right\" version of a term in this situation from the user to the search system. Optical \nmusic recognition could greatly increase the amount of encoded score content available for our users. Audio identification algorithms could improve the speed in which libraries can connect appropriate metadata to digital audio. Content-based, by-example, and similarity searching algorithms could be added to systems that previously only used metadata-based searching to produce more robust and flexible discovery for our users. The possibilities for integration of Music IR research techniques into digital music library systems are nearly endless. \nFinding music is not an end unto itself. Discovery is the \nfirst and perhaps even least inte resting (to him) step a user \ntakes in his path towards his final goal–putting that music to use. This use can take a variety of forms. In academic music libraries, the primary uses of materials from library collections are classroom assignments, research, and performance. Tools from the Music IR community for harmonic and formal analysis, for use with audio, score images, and encoded scores, can help students complete \nclassroom assignments and more fully understand \ncompositional styles. These same tools provide music \nresearchers with unprecedented methods for performing \nquantitative analysis. Automated audio and score alignment methods, instrument artist identification algorithms, and many other Music IR techniques can simplify many music history and theory classroom assignments.  \nNo matter what use a library patron makes of the music \nhe finds, he must have some method for managing the materials in which he is interested. Music IR tools frequently present innovative methods for user interaction with materials that could supplement existing tools in libraries.  \n6. Cataloging and metadata \n6.1 Librarians’ potential contributions to MIR \nMusic IR research has hist orically focused heavily on \ncontent-based retrieval methods. These methods show great promise for improving access to music; however, they cannot by themselves meet every user's searching need. Instead, it is likely that  a combination of content \nsearching and \"traditional\" metadata searching can be used to provide improve access to music; neither method must operate in isolation. The aforementioned Variations2 project is one proof-of-concept example of how the two methods might operate in concert \n[30]. \nLibrarians have developed a number of techniques to \naid metadata-based access to their collections, many of which date from the days of printed catalog cards. Most of these methods focus around the idea of \"collocation\"—grouping together like materials either on the shelf or in the catalog. One such technique is called \"authority control,\" which seeks to describe people, corporations, titles, and subjects with the same label every time they appear in the library catalog.  A user can then find every \nitem by a given person, all instances of a given work, or all works on a given subject with a single search, rather than \nhaving to dream up all possible versions of a name or synonyms for a subject. Variant names, titles, and synonyms for subjects are then cross-referenced through the library catalog to the prefe rred form that appears in the \ncatalog records. Subjects, wh ich in library cataloging, \ninclude such important user access points as genre and instrumentation, are pre-arranged into a hierarchical structure so that users may broaden or narrow their search as desired. These goals, if not the specific mechanisms librarians currently use to achieve them, are essential for next-generation music retrieval systems. \nThe library community has recently developed new \nmodels aimed at more clearly structuring bibliographic information. The most influential of these models is known as \"FRBR\"–a report from the International Federation of Library Associations and Institutions entitled Functional Requirements for Bibliographic Records , \nwhich applies entity-relationship modeling to bibliographic information. \n[31] The FRBR structure and \ncurrent technologies represent a great deal of potential for search systems based on high-quality, structured metadata. FRBR-like metadata models have been implemented in such systems as Variations 2 from Indiana University \n[32], \nthe British Library Sound Archive [32], and \nMusicAustralia [34]. \nLibrarians who work with these techniques on a daily \nbasis can and should provide expert input to leverage them in today's environment. Music librarians possess the in-depth understanding of metadata-based searching needed to effectively integrate these technologies and data sources into our search systems. The data itself generated from the traditional library approach can also be of use to the Music \nIR community. The current Amadeus  (Authority \nMulticultural Archive Description Effective Universal Search) project, a collaborativ e effort centered in the \nItalian Castalia music center, is one example of how library data might be used to enhance retrieval systems \n[35].  \nIn addition to thinking about how library approaches to \nmetadata can be expanded in the networked environment, librarians and library organizations have embarked on a number of studies to analyze the music metadata environment and make recommendations for the role of libraries in a collaborative metadata future. MLA has provided an organizational home for two working groups studying music metadata issues \n[36][37], and the IAML \nInformation Technology Committ ee has sponsored at least \ntwo conference sessions devoted to examining the potential application of emerging technologies to the library music metadata environment \n[38][39]. 6.2 MIR benefits to librarians \nInnovative research coming out of the Music IR \ncommunity offers unprecedented opportunity for the implementation of systems that shift the burden of vocabulary control from the cat aloger and the user to the \nsystem, to integrate authority lists and subject thesauri into cataloging and search systems,  transparently assisting \ncatalogers in assigning appropriate names, titles, and subjects and users in finding material described with these terms.  \nMetadata-based searching has been represented at \nISMIR, most notably through a special session on metadata at ISMIR 2002, which featured one librarian and four representatives from various aspects of the music industry. Librarians can learn from Music IR’s example that valuable and useful metadata is not just available from libraries. Demonstration of how differently-structured and unstructured metadata can be used for searching will broaden librarians’ perspectives of how search systems can \nwork. Similarly, taxonomies and ontologies developed \noutside of libraries can show how different approaches can \nbe effective for retrieval. Using innovative Music IR \ntechniques together with industry data sources and library data using traditional authority control mechanisms can vastly improve access to music materials. \nThe lessons Music IR has to teach librarians about the \nvalue of content-based searching also cannot be underestimated. While metadata-based searching has a solid role in Music IR syst ems, librarians can learn from \nthe demonstrations in the Music IR community of the effectiveness of content-based searching, and better understand how the two approaches can work together in systems. \n7. Copyright \nWith regards to copyright i ssues, librarians and Music IR \nresearchers share a common co re of issues and needs. \nNeither has special knowledge unknown to the other; rather, an understanding of copyright laws affecting the ability to copy, use, and distribute musical works, scores, and recordings is necessary to  the work of each of these \ndomains. Individual experts in copyright matters in a variety of countries are active in the library and Music IR communities, for example Charles Cronin at Columbia Law School \n[40]. Each community can benefit from the \nexperience and expertise of the other, especially in the application of creative solutions for meeting user needs for access to music in legal and ethical ways. \n8. Conclusion \nWe have suggested some of the ways in which the expertise, style, and interests of music librarians will be able to contribute to the adva ncement of research in music \ninformation retrieval. We have also acknowledged the many contributions Music IR research can bring to the practical realities of daily life in the music library.  \nHopefully both communities will be able to discover new \nways of communicating with each other, seek to merge the theoretical with the practi cal, and enhance the musical \ndiscovery process for all types of users. \nReferences \n[1] J. Futrelle and J. S. Downie, “Interdisciplinary research \nissues in music information retrieval: ISMIR 2000-2002,” J.  New Music Res. , vol. 32, no. 2, pp. 121-131, 2003. \n[2] S. Davison, C. Requardt, and K. Brancolini, “A Specialized Open Archives Initiative Harvester for Sheet Music: A Project Report and Examination of Issues,” in Proc. 4th \nInternational Conference on Mu sic Information Retrieval, \n2003. Available: http://ismir2003.ismir.net/papers/Davison.PDF \n[3] C. C. Pedersen, “The Distribu tion of Digital Sheet Music in \nDanish Libraries,” in Proc. 4th International Conference \non Music Information Retrieval, 2003. \n[4] R. Holmes and M-L Ayres, “MusicAustralia: Towards a National Music Information Infrastructure,” in Proc. 5th \nInternational Conference on Mu sic Information Retrieval, \n2004. Available: http://www.iua.upf.es/mtg/ismir2004/review/CRFILES/paper125-29819929bd1180cc585bd7c232804de8.pdf \n[5] I. Fujinaga and J. Riley, “Digital Image Capture of Musical Scores,” in Proc. 3rd International Conference on Music \nInformation Retrieval, 2002. Available: http://ismir2002.ismir.net/proceedings%5C03-SP01-3.pdf \n[6] H. Hemmasi, “Why Not MARC?” in Proc. 3rd \nInternational Conference on Mu sic Information Retrieval, \n2002. Available: http://variations2.indiana.edu/pdf/hemmasi-ismir2002.pdf \n[7]  “Music Library Association,” [Web site] 2006 [2006 Apr 22]. Available: http://www.musiclibraryassoc.org/   \n[8] “IAML: International Association of Music Libraries Archives and Documentation Centres,” [Web site] 2006 [2006 Apr 22].  Available: http://www.iaml.info \n[9] Notes: Quarterly Journal of the Music Library Association , \n(Vol. 56, No. 3, March 2000 through Volume 62, No. 2, December 2005) \n[10] Fontes artis musicae, Jour nal of the International \nAssociation of Music Libraries, Archives and Documentation Centres  (Vol. 47/1, January-March 2000 \nthrough the latest available volume, Vol. 50/2-4, April-December 2003) \n[11] “MLA 2000 Program,” [Web site] 2000 [2006 Apr 23].  Available: http://www3.baylor.edu/MLA/program2000.html \n[12] “MLA Annual Meeting New York City, 2001: Program.” \n[13] “Music Library Association 71st Annual Meeting: Program,” [Web site] 2002 [2006 Apr 23] Available: http://unitproj.library.ucla.edu/music/mla/chron.cfm \n[14] “Music Library Association 72nd Annual Meeting: Program,” [Web site] 2003 [2006 Apr 23]. Available: http://orpheus.ucsd.edu/music/program.htm \n[15] “MLA 2004 Annual Meeting: Sc hedule of Events,” [Web \nsite] 2004 [2006 Apr 23] Available: http://www.lib.jmu.edu/org/ mla2004/schedule.html [16] “Music Library Association 74th Annual Conference: \nProgram, [Web site] 2005 [2006 Apr 23] Available: http://www.musiclibraryasso c.org/2005_conference/schedu\nle.html \n[17] “IAML Annual Conference Edinburgh August 6-11 2000: Final Programme,” [Web site] 2000 [2006 Apr 23] Available:http://www.iaml-uk-irl.org/edinburgh_2000/conf2000.htm \n[18] “IAML Annual Conference–Perigueux 8-13 July 2001: Preliminary Programme,” [Web site] 2001 [2006 Apr 23] Available:http://www.aibm-france.org/congres_inter nationaux/perigueux_2001/ci01_pr\nogramme_1.htm  \n[19] “IAML 2002 Conference—Berkeley, CA: Program.” \n[20] “IAML Annual Conference Tallinn, Estonia, July 6-11, 2003: Programme,” [Web site] 2003 [2006 Apr 23] Available: http://www.utlib.ee/fonoteek/IAML2003/ \n[21] “IAML-IASA Congress 8-13 August 2004, Oslo, Norway: Preliminary Programme.” \n[22] “IAML Annual Conference Warsaw, Poland 10-15 July 2005: Conference Programme,” [Web site] 2005 [2006 Apr 23] Available: http://www.iaml.pl/index-e.htm \n[23] “Library of Congress American Memory Sheet music Collections,” [Web site] 2006 [2006 Jul 1]. Available: http://memory.loc.gov/ammem/browse/ListSome.php?format=Sheet+Music \n[24] “The Lester S. Levy Collection of Sheet Music,” [Web site] 2006 [2006 Jul 1]. Available: http://levysheetmusic.mse.jhu.edu/ \n[25] “Indiana University Sheet Music,” [Web site] 2006 [2006 Jul 1]. Available: http://www.dlib.indiana.edu/collections/sheetmusic \n[26] “Cylinder Preservation and Digitization Project, Department of Special Coll ections, Donald C. Davidson \nLibrary, University of California, Santa Barbara,” [Web site] 2006 [2006 Jul 1]. Available: http://cylinders.library.ucsb.edu/ \n[27] “Harmonica,” [Web site] 2006 [2006 Jul 4]. Available: http://projects.fnb.nl/harmonica/default.htm \n[28] “Variations2: Indiana University Digital Music Library Project,” [Web site] 2006, [2006 Apr 22], Available: http://variations2.indiana.edu/research/ \n[29] J. Riley, “Exploiting Musical Connections: A Proposal for Support of Work Relationships in a Digital Music Library,” in Proc. 6th International Conference on Music \nInformation Retrieval, 2005.  Available: http://ismir2005.ismir.net/proceedings/1108.pdf \n[30] W. Birmingham, K. O'Malley, J. Dunn, and R. Scherle, \n“V2V: A Second Variation on Query-by-Humming,” in Proc. 3rd ACM/IEEE-CS Jo int Conference on Digital \nLibraries , 2003. Available: \nhttp://csdl2.computer.org/comp/proceedings/jcdl/2003/1939/00/19390380.pdf \n[31] IFLA Study Group on the Functional Requirements for Bibliographic Records, Functional Requirements for \nBibliographic Records , Munich: K.G. Saur, 1998. \nAvailable: http://www.ifla.org/VII/s13/frbr/frbr.pdf \n[32] M. Notess, J. Riley, and H. Hemmasi. “From Abstract to Virtual Entities: Implementation of Work-Based Searching in a Multimedia Digital Library,”  In Research and \nAdvanced Technology for Digital Libraries: Proc. 8th European Conf. Dig. Lib,ECDL 2004  2005, pp. 157-167. \nAvailable: http://mypage.iu.edu/~mnotess/ECDL/ecdl-04-reprint.pdf \n[33] “British Library Sound Archiv e Catalogue,” [Web site] \n2006, [2006 Apr 22], Available: http://www.bl.uk/collecti ons/sound-archive/cat.html \n[34] “Australia's Music Online, in Time, ” [Web site] 2006, \n[2006 Apr 22], Available: http://www.musicaustralia.org/ \n[35] “Progetto Amadeus,” [Web  site] 2006, [2006 Jul 4], \nAvailable: http://www.consorzioglossa.it/ opencms/opencms/glossa/am\nadeus.htm \n[36]  “International Music Metada ta Projects Working Group,” \n[Web site] 2006 [2006 Jul 4]. Available: http://www.musiclibraryassoc.org/BCC/BCC-Historical/BCC2002/BCC2002IMP1.html \n[37] “Music Library Association Bibliographic Control Committee Metadata Standa rds Working Group,” [Web \nsite] 2006 [2006 Jul 4]. Available: http://unitproj.library.ucla.edu/music/metadata/docs.cfm \n[38] M. Gentili-Tedeschi, “Semantic Web and the Librarian: XML Schemas as a Future Way of Cataloging?” Paper \ndelivered at 2001 IAML Conference, Périgueux,\n France,  8-\n13 July 2001. \n[39] “A Web ontology for music?” Panel discussion at 2006 \nIAML Conference, Göteborg, Sweden, 18-23 June 2006. \n[40] “Columbia Law School Arthur  W. Diamond Law Library \nMusic Plagiarism Project,” [Web site] 2006 [2006 Jul 8]. \nAvailable: http://ccnmtl.columbia.edu/pr ojects/law/library/introductio\nn.html"
    },
    {
        "title": "A Pattern Recognition Approach for Melody Track Selection in MIDI Files.",
        "author": [
            "David Rizo",
            "Pedro J. Ponce de León",
            "Carlos Pérez-Sancho",
            "Antonio Pertusa",
            "José Manuel Iñesta Quereda"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1417419",
        "url": "https://doi.org/10.5281/zenodo.1417419",
        "ee": "https://zenodo.org/records/1417419/files/RizoLPPI06.pdf",
        "abstract": "Standard MIDI files contain data that can be considered as a symbolic representation of music (a digital score), and most of them are structured as a number of tracks. One of them usually contains the melodic line of the piece, while the other tracks contain accompaniment music. The goal of this work is to identify the track that contains the melody us- ing statistical properties of the musical content and pattern recognition techniques. Finding that track is very useful for a number of applications, like speeding up melody match- ing when searching in MIDI databases or motif extraction, among others. First, a set of descriptors from each track of the target file are extracted. These descriptors are the input to a random forest classifier that assigns the probability of being a melodic line to each track. The track with the high- est probability is selected as the one containing the melodic line of that MIDI file. Promising results have been obtained testing a number of databases of different music styles. Keywords: Melody finding, musical analysis, symbolic rep- resentation, music perception.",
        "zenodo_id": 1417419,
        "dblp_key": "conf/ismir/RizoLPPI06",
        "keywords": [
            "Standard MIDI files",
            "symbolic representation of music",
            "melodic line identification",
            "pattern recognition techniques",
            "motif extraction",
            "MIDI databases",
            "music styles",
            "random forest classifier",
            "probability assignment",
            "high probability track"
        ],
        "content": "A PatternRecognition ApproachforMelody TrackSelection in MIDI Files\nDavidRizo, PedroJ.Poncede Le ´on,Carlos P ´erez-Sancho,Antonio Pertusa,Jos ´e M. I˜nesta\nDepartamentode Lenguajes y Sistemas Inform ´aticos\nUniversidadde Alicante, Spain\ninesta@dlsi.ua.es\nAbstract\nStandard MIDI ﬁles contain data that can be considered as\na symbolic representation of music (a digital score), and\nmost of them are structured as a number of tracks. One of\nthem usually contains the melodic line of the piece, while\nthe other tracks contain accompaniment music. The goal of\nthisworkistoidentifythetrackthatcontainsthemelodyus-\ning statistical properties of the musical content and pattern\nrecognition techniques. Finding that track is very useful for\na number of applications, like speeding up melody match-\ning when searching in MIDI databases or motif extraction,\namong others. First, a set of descriptors from each track of\nthe target ﬁle are extracted. These descriptors are the input\nto a random forest classiﬁer that assigns the probability of\nbeing a melodic line to each track. The track with the high-\nest probability is selected as the one containing the melodic\nlineof thatMIDIﬁle. Promisingresultshavebeen obtained\ntestinga number of databases of differentmusic styles.\nKeywords: Melodyﬁnding,musicalanalysis,symbolicrep-\nresentation,music perception.\n1. Introduction\nA huge number of digital music score can be found on the\nInternet or in multimedia digital libraries. These scores are\nstored in ﬁles conforming to a proprietary or open format,\nlikeMIDIorthevariousXMLmusicformatsavailable. Most\noftheseﬁlescontainmusicorganizedinawaysuchthatthe\nleadingpartofthemusic,themelody,isinsomewaystored\nseparately from the rest of the musical content, which is of-\ntentheaccompanimentforthemelody. Inparticular,astan-\ndard MIDI ﬁle is usually structured as a number of tracks,\none for each voice in a music piece. One of them usually\ncontains a melodic line, specially in the case for modern\npopular music. The goal of this work is to automatically\nﬁnd this melody track in a MIDI ﬁle using statistical prop-\nerties of the musical content and pattern recognition tech-\nniques. The proposed methodology can be applied to other\nsymbolic music ﬁle formats, because the information used\nto take decisions is based solely on how the notes are ar-\nPermissiontomakedigitalorhardcopiesofallorpartofthisworkfor\npersonal or classroom use is granted without fee provided that copies\narenotmadeordistributedforproﬁtorcommercialadvantageandthat\ncopies bear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2006 Universityof Victoriarangedwithineachvoiceofadigitalscore. Onlythefeature\nextraction front-end would need to be adapted for dealing\nwith other formats.\nTheidentiﬁcationofthemelodytrackisveryusefulfora\nnumber of applications. For example, in melody matching,\nwhen the query is either in symbolic format [1] or in audio\nformat [2], the process can be sped up if the melody track\nis known or a way to know which tracks are most likely to\ncontain the melody, because the query is almost always a\nmelody fragment. Another useful application can be help-\ning motif extraction systems to build music thumbnails of\ndigital scores for music collection indexing.\nThe literature about melody voice identiﬁcation is quite\npoor. In the digital sound domain, several papers aim to\nextract the melodic line from audio ﬁles [3, 4]. In the sym-\nbolicdomain,Ghiasetal.[2]builtasystemtoprocessMIDI\nﬁlesextractingasortofmelodiclineusingsimpleheuristics.\nTang et al. [5] present a work where the aim is to propose\ncandidate melody tracks, given a MIDI ﬁle, much like in\nour work. They do so by taking decisions based on sin-\ngle features derived from informal assumptions about what\na melody track may be. They conclude that using the track\nname is the best criteria for selecting a melody track from a\nMIDIﬁle. Intheworkpresentedhere,theuseoftrackname\n(or similar metadata) is explicitly avoided both for labeling\ntracks and as part of the extracted features, as it has proven\nto be unreliable information.\nIn a different approach to melody identiﬁcation, Uitden-\nbogerd and Zobel [6] developed four algorithms for detect-\ningthemelodiclineinpolyphonicMIDIﬁles,assumingthat\na melodic line is a monophonic sequence of notes. These\nalgorithms are based mainly on note pitches; for example,\nkeeping at every time the note of highest pitch from those\nthat sound at that time (skylinealgorithm).\nAnother line of work focuses on how to split a poly-\nphonic source into a number of monophonic sequences by\npartitioning it into a set of melodies [7] or selecting at most\none note at every time step [1]. In general, these works are\ncalled monophonic reduction techniques [8]. Different ap-\nproaches,likeusingvoiceinformation(whenavailable),av-\nerage pitch, and entropy measures have been proposed in\nthese works.\nOther approaches, related to motif extraction, focus on\nthe development of techniques for identifying patterns as\nrepetitions that are able to capture the most representativenotesin a music piece [9, 10, 11].\nNevertheless, in this work the aim is not to extract a\nmonophonic line from a polyphonic score, but to decide\nwhich of the tracks contains the main melody in a multi-\ntrack standard MIDI ﬁle. For this, we need to assume that\nthe melody is indeed contained in a single track. This is\noftenthe case of popular music.\nThefeaturesthatshouldcharacterizemelodyandaccom-\npaniment voices must be deﬁned in order to be able to se-\nlect the melodic track. There are some features in a melody\ntrackthat,atﬁrstsight,seemtobeenoughforidentifyingit,\nlike the presence of higher pitches (see [6]) or being mono-\nphonic. Unfortunately,anyempiricalanalysiswillshowthat\nthese hypotheses do not hold in general, and more sophis-\nticated criteria need to be devised in order to take accurate\ndecisions.\nToovercometheseproblems,aclassiﬁerensemblethatis\nable to learn what is a melodic track, in a supervised man-\nner based on note distribution statistics, has been used in\nthis work. In order to setup and test the classiﬁer, a num-\nber of data sets based on different music styles and consist-\ningofmultitrackstandardMIDIﬁleshavebeenconstructed.\nAll tracks in such ﬁles are labeled either as melody or non-\nmelody.\nThe rest of the paper is organized as follows: ﬁrst the\nmethodologyisdescribed,boththewayatrackischaracter-\nized and how the classiﬁer is built. Then, the data used and\nthe experiments designed to test the method are discussed,\nand ﬁnally, some conclusions about the results obtained are\npresented,and future workto be done is discussed.\n2. Methodology\nThe usual methodology in the pattern recognition ﬁeld has\nbeen used in this work. A vector of numeric descriptors\nis extracted from each track of a target midiﬁle, and these\ndescriptors are the input to a classiﬁer that assigns to each\ntrack its probability of being a melody. The random forest\nclassiﬁer [12] –an ensemble of decision trees– was chosen\nasthepatternrecognitiontoolforthistask. TheWEKA[13]\ntoolkitwasused to implement the system.\n2.1. MIDI Trackcharacterization\nThecontentofeachnon-emptytrack1ischaracterizedbya\nvector of descriptors based on descriptive statistics of note\npitches and note durations that summarize track content in-\nformation. This type of statistical description of musical\ncontent is sometimes referred to as shallow structure de-\nscription [14, 15].\nA set of descriptors has been deﬁned, based on several\ncategoriesoffeaturesthatassessmelodicandrhythmicprop-\nerties of a music sequence, as well as track related proper-\nties. This set of descriptors is presented in Table 1. The\nleft column indicates the category being analyzed, and the\n1trackscontaining at least one note event. Empty tracks are discarded.Table1. Extracted descriptors\nCategory Descriptors\nTrackinformation Normalizedduration\nNumberof notes\nOccupationrate\nPolyphonyrate\nPitch Highest\nLowest\nMean\nStandarddeviation\nPitchintervals Numberof differentintv.\nLargest\nSmallest\nMean\nMode\nStandarddeviation\nNotedurations Longest\nShortest\nMean\nStandarddeviation\nSyncopation Numberof Syncopated notes\nrightoneshowsthestatisticsdescribingpropertiesfromthat\ncategory.\nFour features were designed to describe the track as a\nwhole and ﬁfteen to describe particular aspects of its con-\ntent. Fortheseﬁfteendescriptors,bothnormalizedandnon-\nnormalized versions have been computed. The former were\ncalculatedusingtheformula (value i−min)/(max−min),\nwhere value iisthedescriptortobenormalizedcorrespond-\ning to the i-th track, and minandmaxare, respectively,\nthe minimum and maximum values for this descriptor for\nall the tracks of the target midiﬁle. This makes it possible\nto know these properties proportionally to the other tracks\nin the same ﬁle, using non-dimensional values. This way, a\ntotal number of 4 + 15 ×2 = 34descriptors were initially\ncomputed for each track.\nThe track information descriptors are its normalized du-\nration (using the same scheme as above), number of notes,\noccupation rate (proportion of the track length occupied by\nnotes), and the polyphony rate, deﬁned as the ratio between\nthe number of ticks in the track where two or more notes\nareactivesimultaneouslyandthetrackdurationinticks(the\nMIDIﬁleresolutionestablisheshowmanyticksformabeat).\nPitch descriptors are measured using MIDI pitch values.\nThemaximumpossibleMIDIpitchis127(note G8)andthe\nminimum is 0 (note C−2).\nTheintervaldescriptorssummarizeinformationaboutthe\ndifferenceinpitchbetweenconsecutivenotes. Theabsolute\npitch intervalvalueswere computed.\nFinally,notedurationdescriptorswerecomputedinterms\nof beats, so they are independent from the MIDI ﬁle reso-\nlution.\nA view to the graphs in Figure 1 provides some hints on 0 2000 4000 6000 8000 10000 12000 14000 16000\nTrue NoTrackNumNotes\nIs melody?Melody vs. TrackNumNotes\n 20 30 40 50 60 70 80 90 100 110 120\nTrue NoAvgPitch\nIs melody?Melody vs. AvgPitch\n 0 10 20 30 40 50 60 70\nTrue NoAvgAbsInterval\nIs melody?Melody vs. AvgAbsInterval\n 0 0.2 0.4 0.6 0.8 1\nTrue NoAvgNormalizedDuration\nIs melody?Melody vs. AvgNormalizedDurationFigure1. Distributionofvaluesforsomedescriptors: (top-left)\nnumber of notes, (top-right) mean pitch, (bottom-left) mean\nabsoluteinterval,and (bottom-right) mean relativeduration.\nwhat a melody track could look like using these descrip-\ntors. Thisway,amelodytrackseemstohavelessnotesthan\nothernon-melody tracks, an averagemean pitch, it contains\nsmallerintervals,andhasnottoolongnotes. Whenthissort\nof hints are combined by the classiﬁer, a decision about the\ntrack“melodicity”istaken. Thistypeofreasoninghasbeen\nused by other authors, like in [5], in order to build a series\nofrulesabletotakeadecisiononwhichisthemelodytrack\nina symbolic music ﬁle.\n2.2. The random forestclassiﬁer\nAnumberofclassiﬁersweretestedinaninitialstageofthis\nresearch and the random forest classiﬁer yielded the best\nresults among them, so it was chosen for the experiments\npresentedin the nextsection.\nRandom forests [12] are weighed combinations of deci-\nsiontreesthatusearandomselectionoffeaturestobuildthe\ndecision taken at each node. This classiﬁer has shown good\nperformance compared to other classiﬁer ensembles and it\nis robust with respect to noise. One forest consists of K\ntrees. Each tree is built to maximum size using CART [16]\nmethodology without pruning. Therefore, each leaf on the\ntree corresponds to a single class. The number Fof ran-\ndomly selected features to split on the training set at each\nnode is ﬁxed for all the trees. After the trees have grown,\nnew samples are classiﬁed by each tree and their results are\ncombined, giving as a result a membership probability for\neachclass.\nIn our case, the membership for class “melody” can be\ninterpreted as the probability that a track will contain a me-\nlodicline.\n2.3. Trackselection procedure\nThereareMIDIﬁlesthatcontainmorethanonetrackwhich\nis suitable to be classiﬁed as melody: singing voice, instru-\nmentsolos,melodicintroductions,etc. Ontheotherhand,asusually happens in classical music, some songs do not have\nan obvious melody, like in complex symphonies or single-\ntrack piano sequences. The algorithm proposed here can\ndeal with the ﬁrst case. For the second case, there are more\nsuitable methods [6] that perform melody extraction from\npolyphonic data.\nIn some of the experiments in the next section, at most\none melody track per MIDI ﬁle is selected. However, a ﬁle\ncan contain more than one melody track. Therefore, given\naﬁle,allitsnon-emptytracksareclassiﬁedandtheirproba-\nbilities of being a melody are obtained. Then the track with\nthehighestprobabilityisselectedasthemelodytrack. Ifall\ntracks have near-zero probability (actually less than 0.01),\nno melody track is selected –that is, all tracks are consid-\nered as not melody tracks.\nIn the ﬁrst stages of this work, a probability threshold\naround 0.5 was established in order to discard tracks whose\nprobability of being a melody was below that value. This\nresulted in some ﬁles in our test datasets being tagged as\nmelody-less. However most of those ﬁles actually have a\nmelody. In general, this produced systems with lower esti-\nmated accuracythan systems with no probability threshold.\n3. Results\n3.1. Datasets\nSix corpora (see Table 2) were created, due to the lack of\nexisting databases for this task. The ﬁles were downloaded\nfromanumberoffreelyaccessibleInternetsites. First,three\ncorpora(namedJZ200,CL200,andKR200)werecreatedto\nset up the system and to tune the parameter values. JZ200\ncontains jazz music ﬁles, CL200 has classical music pieces\nwhere there was an evident melody track, and KR200 con-\ntains popular music songs with a part to be sung (karaoke\n(.kar) format). All of them are made up of 200 ﬁles. Then,\nthree other corpora (named JAZ, CLA, and KAR) from the\nsame music genres were compiled from a number of differ-\nent sources to validate our method. This dataset is available\nfor research purposes on request to the authors.\nTable 2. Corpora used in the experiments, with identiﬁer, mu-\nsic genre, number of ﬁles, total number of tracks, and total\nnumber of melody tracks.\nCorpusID Genre FilesTracks Melody tracks\nCL200 Classical 200 687 197\nJZ200 Jazz 200 769 197\nKR200 Popular 2001370 179\nCLA Classical 131 581 131\nJAZ Jazz 1023 4208 1037\nKAR Popular 1360 9253 1288\nThemaindifﬁcultyforbuildingthedatasetswastolabelthe tracks in the MIDI ﬁles. Text tagging of MIDI tracks\nbased on metadata such as the track name, is unreliable.\nThus, a manual labeling approach was carried out. A mu-\nsician listened to each one of the MIDI ﬁles playing all the\ntracks simultaneously with a sequencer. For each ﬁle, the\ntrack(s)containingtheperceivedmelodywereidentiﬁedand\ntagged as melody. The rest of tracks in the same ﬁle were\ntagged as non-melody . In particular, introduction passages,\nsecondvoicesorinstrumentalsolopartsweretaggedas non-\nmelody,aimingtocharacterisewhatisclearlyaleadmelody\npart.\nSome songs had no tracks tagged as melody because ei-\nther it was absent, or the song contained just a melody-\nless accompaniment, or the melody constantly moves from\none track to another. Other songs contained more than one\nmelody track (e.g. duplicates, often with a different timbre)\nandall those tracks were tagged as melody.\n3.2. Experiments\nThe WEKA package was used to carry out the experiments\ndescribedhere,anditwasextendedtocomputetheproposed\ntrackdescriptors directly from MIDI ﬁles.\nFour experiments were carried out. The ﬁrst one tried to\nassess the capability of random forests to classify melodic\nand non-melody tracks properly. In the second experiment,\nthe aim was to evaluate how accurate the system was for\nidentifying the melody track in a MIDI ﬁle. Finally, the\nspeciﬁcityofthesystemwithrespecttoboththemusicgenre\nandthe corpora utilized were tested.\n3.2.1. Melody versusnon-melody classiﬁcation\nTherandomforestclassiﬁerassignsaclassmembershipprob-\nability to each test sample, so in this experiment a test track\nis assigned to the class with the highest membership proba-\nbility.\nThreeindependentsub-experimentswerecarriedout,us-\ning the three 200-ﬁle corpora (CL200, JZ200, and KR200).\nThisway,2826tracksprovidedbytheseﬁleswereclassiﬁed\nin two classes: melody/non-melody . A 10-folded cross-\nvalidation scheme was used to estimate the accuracy of the\nmethod. The results are shown in Table 3. The remarkable\nsuccesspercentagesobtainedareduetothefactthattheclas-\nsiﬁer was able to successfully map the input feature vector\nspaceto the class space.\nAlso, precision, recall and the F-measure are shown for\nmelody tracks. These standard information retrieval mea-\nsures are based on the so-called true-positive (TP),false-\npositive(FP) and false-negative (FN) counts. For this ex-\nperiment, TP is the number of melody tracks successfully\nclassiﬁed, FP is the number of misclassiﬁed non-melody\ntracks,andﬁnally,FNisthenumberofmisclassiﬁedmelody\ntracks. Theprecision,recallandF-measurearecalculatedas\nfollows:\nPrecision =TP\nTP+FPTable3. Melody versusnon-melody classiﬁcation results.\nCorpus Success Precision Recall F-measure\nCL200 98.9% 0.99 0.98 0.98\nJZ200 96.8% 0.95 0.92 0.94\nKR200 96.8% 0.92 0.80 0.86\nRecall =TP\nTP+FN\nFmeasure =2×Recall ×Precision\nRecall +Precision\nThere was a somewhat low recall of melody tracks ( 0.8)\nfor the KR200 corpus. This indicates that most errors are\ndue to FNs, i.e., the classiﬁer missed to detect a signiﬁcant\nnumber of melody tracks. The causes of this confusion are\nratherobscure,butthereasonseemstobetheheterogeneous\nwayinwhichmusicisorganizedwithinkaraokeMIDIﬁles.\nThese results have been obtained using K= 10trees\nandF= 5randomlyselectedfeaturesfortherandomforest\ntrees. Duetotherelativelygoodresultsobtainedinthethree\nsub-experiments, the same classiﬁer structure was used in\nthe rest of experimentspresented in the nextsections.\n3.2.2. Melodic trackselection experiment\nNow, the goal is to know how many times does the method\nselect as melody track the proper track from a ﬁle. For this\nexperiment, the system was trained the same way as in the\nlatter one, but now a test sample is not a single track but a\nMIDI ﬁle. Due to the limited number of samples available\n(200 per corpus), this experiment was performed using a\nleave-one-out scheme at the MIDI ﬁle level to estimate the\nclassiﬁcationaccuracy. Allthetracksfromaﬁlehaveaclass\nmembership probability. For each ﬁle, the system outputs\nthe track number that gets a higher membership probability\nfor the class melody, except when all these probabilities are\nnear-zero, in which case the system concludes that the ﬁle\nhas no melody track.\nThe classiﬁer answer is considered as a success if\n1. theﬁlehasatleastonetracktaggedas melodyandthe\nselected track is one of them.\n2. The ﬁle has no melody tracks and the classiﬁer out-\nputs no melody track number.\nTheobtainedresultsareshowninTable4. Theprecision,\nrecall, and F-measure values have been calculated. In order\nto obtain these values, songs without melody tracks were\nnot considered. The TP value is computed as the number\nof times that the algorithm returns an actual melody track.\nNote that this value differs from the success rate because\nhere the songs without any melody track are not included.The FP value is calculated as the number of times that the\nsystem selects a track that is not tagged as melody. The FN\nare those cases in which the classiﬁer does not select any\ntrack, but the song actually has one or more melody tagged\ntracks.\nTable4. Melody track selection results.\nCorpus Success Precision Recall F-measure\nCL200 100.0% 1 1 1\nJZ200 96.5% 0.97 0.99 0.98\nKR200 72.3% 0.72 0.99 0.84\nNotethehighqualityoftheresultsforCL200andJZ200.\nHowever, a lower success rate has been obtained for the\nkaraoke ﬁles. This is due to the fact that 31 out of 200 ﬁles\nin this corpus were tagged by a human expert as having no\nactual melody track, but they have some portions of tracks\nthat could be considered as melody (like short instrument\nsoloparts),thusconfusingtheclassiﬁerasFPhits,therefore\nloweringthe classiﬁer precision for this corpus.\n3.2.3. Style speciﬁcity\nThis experiment was designed in order to evaluate the sys-\ntem robustness against different corpora. In other words, it\nis interesting to know how speciﬁc the classiﬁer’s inferred\nrules are with respect to the music genre of the ﬁles con-\nsidered for training. For it, two melody track selection sub-\nexperiments were performed: in the ﬁrst one, the classiﬁer\nwastrainedwitha200-ﬁlecorpusofagivenstyle,andtested\nwith a different corpus of the same style (see Table 5). For\nthe second sub-experiment, the classiﬁer was trained using\nthedataoftwostylesandthentestedwiththeﬁlesofanother\nstyle(see Table6).\nTable5. Melody track selection within style.\nTrain. TestSuccess Precision Recall F\nCL200 CLA60.6% 0.62 0.890.73\nJZ200 JAZ96.5% 0.97 0.990.98\nKR200 KAR73.9% 0.86 0.800.83\nThe results in Table 5 show that the performance of the\nsystem degrades when more complex ﬁles are tested. The\n200-ﬁle corpora are datasets that include MIDI ﬁles that\nwereselectedamongmanyothersforhavingan’easily’(for\na human) identiﬁable melody track. This holds also for the\nJAZcorpus,asmostjazzmusicMIDIﬁleshavealeadvoice\n(or instrument) track plus some accompaniment tracks like\npiano, bass and drums. However, it does not hold in gen-\neral for the other two corpora. Classical music MIDI ﬁles\n(CLA corpus) come in very different structural layouts, due\nto both the way that the original score is organized and the\nidiosyncrasy of the MIDI ﬁle authors. This is also mostlytrue for the KAR corpus. Moreover, karaoke ﬁles tend to\nmake intensive use of duplicate voices and dense pop ar-\nrangements with lots of tracks containing many ornamen-\ntation motifs. In addition, we have veriﬁed the presence\nof very short sequences for the CLA corpus, causing less\nquality in the statistics that also degrades the classiﬁcation\nresults.\nAs both the training and test corpus contain samples of\nthe same music genre, better results were expected. How-\never, the CLA and KAR corpora are deﬁnitively harder to\ndeal with, as it became clear in the second experiment pre-\nsented in this section. So, it can be said that the difﬁculty\nof the task resides more on the particular internal organiza-\ntion of tracks in the MIDI ﬁles than on the ﬁle music genre,\nalthough the results in Table 5 seem to point out that genre\nmakesadifference. ThesecondexperimentpresentedinTa-\nble 6 showedsome evidencein this direction.\nAn interesting point is that, despite low success and pre-\ncision, recall can be considered high for the CLA corpus.\nThisisduetothefactthatmosterrorsareproducedbecause\na non-melody track is selected as melody (false-positives).\nTheKARcorpussuffersfrombothsomewhatlowprecision\nand low recall: errors come from both false-positives and\nfalse-negatives (the classiﬁer does not get a melody from\nmelody tagged tracks).\nTable6. Melody track selection acrossstyles.\nTrain. TestSuccess Prec.Recall F\nKAR+JAZ CLA71.7% 0.730.920.81\nCLA+KAR JAZ92.6% 0.960.970.96\nCLA+JAZ KAR64.9% 0.770.780.78\nTheresultsinTable6showthattheperformanceispoorer\n(with respect to values in Table 5) when no data from the\nstyle tested were used for training. This does not happen in\nclassical music, probably due to effects related to the prob-\nlems expressedabove.\n3.2.4. Trainingset speciﬁcity\nTo see how conditioned are these results by the particular\ntraining sets utilized, a generalization study was carried out\nbuilding a new training set merging the three 200-ﬁles cor-\npora (named ALL200), and then using the other corpora for\ntest. The results are detailed in Table7.\nThis shows that, when using a multi-style dataset, the\nperformance of the system is somewhat improved (now the\ntraining set contains samples from the same style as the test\ndataset). Note that the results are better despite that the size\nof the training set is smaller than the size of those used in\nthe second experimentof Section 3.2.3 (Table6).\nWhen combining all the success results, taking into ac-\ncount the different cardinalities of the test sets, the average\nsuccessfulmelodytrackidentiﬁcationpercentageis81.2%.Table 7. Melody track selection by styles when training with\ndatafromall the styles.\nTraining TestSuccess Precision Recall F\nALL200 CLA73.8% 0.75 0.920.82\nALL200 JAZ97.0% 0.97 0.990.98\nALL200 KAR70.2% 0.84 0.790.82\n4. Conclusions and futurework\nA method to identify the voice containing the melody in a\nmultitrack digital score has been proposed here. It has been\napplied to standard MIDI ﬁles in which music is stored in\nseveral tracks, so the system determines whether a track is\na melodic line or not. The track with the highest probabi-\nlity among the melodic tracks is ﬁnally labeled as the one\ncontainingthe melody of that song.\nThe decisions are taken by a pattern recognition algo-\nrithm based on statistical descriptors (pitches, intervals, du-\nrations and lengths), extracted from each track of the target\nﬁle. The classiﬁer used for the experiments was a decision\ntreeensembleclassiﬁernamedrandomforest. Itwastrained\nusingMIDItrackswiththemelodytrackpreviouslylabeled\nbya human expert.\nTheexperimentsyieldedpromisingresultsusingdatabases\nfrom different music styles, like jazz, classical, and popular\nmusic. Unfortunately, the results could not be compared to\nothersystems because of the lack of similar works.\nThe results show that enough training data of each style\nare needed in order to successfully characterize the melody\ntrack,duetothespeciﬁcitiesofmelodyandaccompaniment\nin each style. Classical music is particularly hard for this\ntask,becauseofthelackofasingletrackthatcorrespondsto\nthemelodicline. Instead,themelodyisconstantlychanging\namong different tracks along the piece. To overcome this\nproblem, more sophisticated schemes oriented to melodic\nsegmentationare needed.\nAlso,theuseofinformationaboutthelayoutofthetracks\nwithinaMIDIﬁleisbeinginvestigated. Wehopethiswould\nhelptoimprovetheperformanceofthesystemwhendealing\nwithparticularlyhardinstanceslikepopularmusicﬁles. Fi-\nnally, the extraction of human-readable rules from the trees\nin the random forest that help characterize melody tracks is\nanothertopic under research now.\n5. Acknowledgments\nThisworkwassupportedbytheprojects: GeneralitatValen-\ncianaGV06/166andSpanishCICyTTIC2003–08496–C04,\npartiallysupported by EU ERDF.References\n[1] A. Uitdenbogerd and J. Zobel, “Melodic matching tech-\nniques for large music databases,” in Proceedings of the\nseventhACMInternationalMultimediaConference(Part1) .\nACMPress, 1999, pp. 57–66.\n[2] A.Ghias,J.Logan,D.Chamberlin,andB.C.Smith,“Query\nby humming: Musical information retrieval in an audio\ndatabase,”in Proc.of3rdACMInt.Conf.Multimedia ,1995,\npp.231–236.\n[3] A. Berenzweig and D. Ellis, “Locating singing voice seg-\nments within music signals,” in Proceedings of the IEEE\nworkshoponApplicationsonSignalProcessingtoAudioand\nAcoustics(WASPAA) ,October 2001.\n[4] J. Eggink and G. J. Brown, “Extracting melody lines from\ncomplex audio,” in 5th International Conference on Music\nInformationRetrieval(ISMIR) , 2004, pp. 84–91.\n[5] M. Tang, C. L. Yip, and B. Kao, “Selection of melody lines\nfor music databases.” in Proceedings of Annual Int. Com-\nputerSoftwareandApplicationsConf.COMPSAC ,2000,pp.\n243–248.\n[6] A. L. Uitdenbogerd and J. Zobel, “Manipulation of music\nfor melody matching,” in Proceedings of the sixth ACM In-\nternationalMultimediaConference . ACMPress,1998,pp.\n235–240.\n[7] A. Marsden, “Modelling the perception of musical voices:\na case study in rule-based systems,” in Computer Represen-\ntations and Models in Music . Academic Press, 1992, pp.\n239–263.\n[8] K.LemstromandJ.Tarhio,“Searchingmonophonicpatterns\nwithin polyphonic sources,” in Proceedings of the RIAO\nConference, volume 2 , 2000, pp. 1261–1278. [Online].\nAvailable: citeseer.ist.psu.edu/lemstrom00searching.html\n[9] E. Cambouropoulos, “Towards a general computational the-\nory of musical structure,” Ph.D. dissertation, Faculty of mu-\nsic and Department of Artiﬁcial Intelligence, University of\nEdinburgh,1998.\n[10] O. Lartillot, “Perception-based advanced description of ab-\nstractmusicalcontent,”in DigitalMediaProcessingforMul-\ntimedia Interactive Services , E. Izquierdo, Ed., 2003, pp.\n320–323.\n[11] B. Meudic, “Automatic pattern extraction from polyphonic\nMIDI ﬁles,” in Proc. of Computer Music Modeling and Re-\ntrievalConf. ,2003, pp. 124–142.\n[12] L. Breiman, “Random forests,” Machine Learning , vol. 45,\nno.1, pp. 5–32, October 2001.\n[13] I. H. Witten and E. Frank, Data Mining: Practical Machine\nLearning Tools and Techniques , ser. Morgan Kaufmann Se-\nries in Data Management Systems. Morgan Kaufmann,\n1999.\n[14] J.Pickens,“Asurveyoffeatureselectiontechniquesformu-\nsic information retrieval,” Center for Intelligent Information\nRetrieval, Departament of Computer Science, University of\nMassachussetts,Tech.Rep., 2001.\n[15] P. J. Ponce de Le ´on, J. M. I ˜nesta, and C. P ´erez-Sancho,\n“A shallow description framework for musical style recog-\nnition,”Lecture Notes in Computer Science , vol. 3138, pp.\n871–879,2004.\n[16] R. O. Duda, P. E. Hart, and D. G. Stork, Pattern Classiﬁca-\ntion. John Wileyand Sons, 2000."
    },
    {
        "title": "Evaluation of the Technical Leval of Saxophone Performers by Considering the Evolution of Spectral Parameters of the Sound.",
        "author": [
            "Matthias Robine",
            "Mathieu Lagrange"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1416620",
        "url": "https://doi.org/10.5281/zenodo.1416620",
        "ee": "https://zenodo.org/records/1416620/files/RobineL06.pdf",
        "abstract": "We introduce in this paper a new method to evaluate the technical level of a musical performer, by considering only the evolutions of the spectral parameters during one tone. The proposed protocol may be considered as front end for music pedagogy related softwares that intend to provide feedback to the performer. Although this study only consid- ers alto saxophone recordings, the evaluation protocol in- tends to be as generic as possible and may surely be consid- ered for wider range of classical instruments from winds to bowed strings. Keywords: music education, performer skills evaluation, si- nusoidal modeling.",
        "zenodo_id": 1416620,
        "dblp_key": "conf/ismir/RobineL06",
        "keywords": [
            "musical performer",
            "spectral parameters",
            "technical level",
            "evolutions",
            "pedagogy",
            "feedback",
            "music education",
            "softwares",
            "evaluation protocol",
            "music instruments"
        ],
        "content": "Evaluationof the TechnicalLevelof Saxophone Performers\nbyConsidering the Evolutionof Spectral Parametersof the Sound\nMatthias Robine and Mathieu Lagrange\nSCRIME – LaBRI, Universit ´eBordeaux 1\n351 cours de la Lib ´eration, F-33405 Talencecedex,France\nfirstname.name@labri.fr\nAbstract\nWe introduce in this paper a new method to evaluate the\ntechnical level of a musical performer, by considering only\ntheevolutionsof the spectral parameters during one tone.\nThe proposed protocol may be considered as front end\nfor music pedagogy related softwares that intend to provide\nfeedbacktotheperformer. Althoughthisstudyonlyconsid-\ners alto saxophone recordings, the evaluation protocol in-\ntendstobeasgenericaspossibleandmaysurelybeconsid-\nered for wider range of classical instruments from winds to\nbowedstrings.\nKeywords: musiceducation,performerskillsevaluation,si-\nnusoidalmodeling.\n1. Introduction\nSeveral parameters could be extracted from a musical per-\nformance. TheworksofLangner&al[1]orScheirer[2]ex-\nplainhowtodifferentiatepianoperformancesusingvelocity\nand loudness parameters for example. Studies presented by\nStamatatos&al[3,4]usedifferencesfoundinpianoperfor-\nmancestorecognizeperformers. Weproposehereamethod\ntoevaluatethetechnicallevelofamusicalperformerbyan-\nalyzing non expressive performances, as scales. Our results\nare based on the analysis of alto saxophone performances,\nhowever the same approach can be used with other instru-\nments.\nBefore us, Fuks [5] explains how the exhaled air of the\nperformer can inﬂuence the saxophone performance, and\nHaas [6] propose with the SALTO system to reproduce the\nphysicinﬂuenceofthesaxophoneinstrumentontheperfor-\nmance. Herewedonotwanttoconsiderthephysicbehavior\noftheinstrument,orwhatisinﬂuencedbythephysiologyof\nthe performer. Since the spectral envelop is strongly depen-\ndent to the physics of the couple instrument / instrumental-\nist,this kind of observationscan not be considered.\nOn contrary, the long-term evolution of the spectral pa-\nrameters over time reﬂects the ability of the performer to\ncontrolits sound production. Evenif this ability is only one\nPermissiontomakedigitalorhardcopiesofallorpartofthisworkfor\npersonal or classroom use is granted without fee provided that copies\narenotmadeordistributedforproﬁtorcommercialadvantageandthat\ncopies bear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2006 Universityof Victoriaaspectofsaxophonetechnique,itappearstobestronglycor-\nrelatedtotheoveralltechnicallevelinanacademiccontext.\nMoreover, we will show in this paper that considering this\nevolutionisrelevanttoevaluateperformersoverwiderange,\nfrom beginnersto experts.\nWethinkthisworkcouldbeusefulinmusiceducation. A\nsoftware that can automatically evaluate the technical level\nof a performer would be a good feedback of his progress,\nespecially when the teacher is not present. This kind of\nsoftwarewillsurelybewelcomeinmusicschoolssincemu-\nsic teachers we met during the recordings were very excit-\ning with this idea. Other projects are already on the same\nway, as IMUTUS [7, 8], the Piano Tutor [9], and the I-\nMAESTRO[10] projects.\nAfter presenting the sinusoidal model for sound analysis\nin Section 2, we explain in Section 3 the experiment proto-\ncol we used to record the 30 alto saxophonists playing long\ntones,exercisesusuallyplayedbyinstrumentalistsasscales.\nWe also detail how and why the music exercises have been\nchosen,andhowthedatabasehasbeenbuiltfromtherecord-\nings.\nConclusions of our study are based on the analysis of\nthis database, using metrics to evaluate the musical perfor-\nmance. These metrics proposed in Section 4 are deﬁned to\ncorrespond to the perceptive criteria of technical level com-\nmonly used by music teachers to evaluate the quality of the\nproduced sound. The results presented in Section 5 show\nhow these metrics are well-suited to evaluate the technical\nlevelof a performer.\n2. Sinusoidal Model\nAdditive synthesis is the original spectrum modeling tech-\nnique. ItisrootedinFourier’stheorem,whichstatesthatany\nperiodic function can be modeled as a sum of sinusoids at\nvarious amplitudes and harmonic frequencies. For station-\nary pseudo-periodic sounds such as saxophone tones, these\namplitudesandfrequenciescontinuouslyevolveslowlywith\ntime,controllingasetofpseudo-sinusoidaloscillatorscom-\nmonly called partials. This representation is used in many\nanalysis/synthesisprogramssuchasAudioSculpt[11],SMS\n[12], or InSpect [13].\nFormally, a partial is composed of three vectors that are\nrespectivelythetimeseriesoftheevolutionofthefrequency,linearamplitude, and phase of the partial overtime:\nPk={Fk(m), Ak(m),Φk(m)},∀m∈[bk,···, bk+lk−1]\nwhere Pkis the partial number k, of length lk, and that ap-\npearedat frame index bk.\nTo evaluate the technical level of a performer, its perfor-\nmance is recorded following a protocol detailed in the next\nsection. From these recordings, the partials are extracted\nusingtrackingalgorithms[14]. Sincetheprotocolproposed\nin this article is intended to evaluate the technical level of\nwind and bowed string instrument performer, the frequen-\ncies of the partials that compose the analyzed tones are in\nharmonic relation and the evolution of the parameters of\nthese partials are correlated [15]. Consequently, only the\nfundamentalpartialisconsideredforthecomputationofthe\nmetricsproposed in Section 4.\n3. Experiment Protocol\nToevaluatethetechnicallevelofsaxophonists,weaskthem\nto play long tones, exercises they use frequently to warm\nup, as scales. These exercises are commonly used in mu-\nsic education to improve and evaluate the technical level of\na performer, either by the teacher or by the instrumentalist\nhimself. It consists in controlling music parameters as nu-\nance,pitch and vibrato.\nRecordings took place in the music conservatory of Bor-\ndeaux and in the music school of Talence, France. More\nthan 30 alto saxophonists have been recorded, from begin-\nnerstoteachersincludinghightechnicallevelstudents. They\nplayed long tones without directive about duration on 6 dif-\nferent notes: low B, low F, C, high G, high D, and high A\naltissimo. For each note, they executed 5 exercises: ﬁrst\na straight tone with nuance piano, a straight tone mezzo\nforteand a straight tone forte, respectively corresponding\nto a sound with low, medium and high amplitude. Then\nthey played a long tone crescendo / decrescendo , frompi-\nanotofortethenfortetopiano, corresponding to an ampli-\ntude evolving linearly from silence to a high value, then to\nsilence again. They ended the exercises by a long tone with\nvibrato. An example of these exercises with the note C is\ngivenby Figure 1.\nThesoundﬁleswererecordedusingamicrophoneSONY\nECM-MS907 linked to a standard PC sound card. The cho-\nsen format was PCM sampled at 44100 kHz, and quantized\non 16 bits. A database containing about 900 ﬁles (5 long\ntones per saxophonist) has been built from the recordings.\nThe fundamental partial has been extracted for each ﬁle us-\ningcommon sinusoidal techniques referenced in Section 2.\nWhilecomparingtheperformancesofseveralsaxophon-\nists, an important factor to consider is the multiplier coefﬁ-\ncient of amplitude from pianostraight tone to fortestraight\ntone, noted α. Its value depends on the control of the air\npressure. A pianotone is much more difﬁcult to perform at\na very low amplitude. The technical effort to differentiate\nnuances can therefore affect the results, but increases the α\n\u0000\u0002\u0001\u0003\n\u0004 \u0005\u0007\u0006\u0003\n\u0004 \u0006\u0003\n\u0004 \u0001\b\u0001\u0003\n\u0004 \u0006 \u0006 \u0001\b\u0001\n\t\f\u000b \r\f\u000e \u000f \u0010 \u0011\u0005\u0012\u0006\u0003\n\u0004Figure 1. Musical exercises performed by saxophonists during\nrecordings. Here is an example with the note C. It consists in ﬁrst\nplaying a straight tone piano, then a straight tone mezzo forte and\na straight tone forte. Then a long tone crescendo / decrescendo\nmust be played before ending with a long tone with vibrato. There\nis no directiveabout duration.\ncoefﬁcient. In the results presented in Section 5, αis com-\nputed as the ratio between the sum of the amplitudes of all\nthe partials extracted from the fortetone versus the ampli-\ntudes of all the partials extractedfrom the pianotone.\n4. EvaluationMetrics\nForeachexercisepresentedinthelastsection,weintroduce\nthe metrics that are computed to evaluate the quality of the\nperformance. These metrics consider the evolution of the\nfrequencyandtheamplitudeparametersofthepartial Pkas\ndeﬁnedinSection2. Forthesakeofclarity,theindex kwill\nbe removedin the remaining of the presentation.\n4.1. The WeightedDeviation\nWhen performing a straight tone, the instrumentalist is re-\nquestedtoproduceasoundwithconstantfrequencyandam-\nplitude. It is therefore natural to consider the standard devi-\nation to evaluatethe quality of its performance.\nd(X) =vuut1\nNN−1X\ni=0(X(i)−¯X)2 (1)\nwhere Xis the vector under consideration of length N, and\nthe mean of Xis:\n¯X=1\nNN−1X\ni=0X(i) (2)\nHowever,iftheamplitudeisveryhigh,aslightdeviation\nof the frequency parameter will be perceptively important.\nOn contrary, if the amplitude is very low, a major devia-\ntion of the frequency parameter will not be perceptible. To\nperform this kind of “perceptual” weighting, we consider a\nstandard deviationweighted by the amplitude vector A:\nwd(X) =vuut1\nN¯AN−1X\ni=0A(i) (X(i)−¯X)2(3)\nThis weighting operation is also useful to minimize the\ninﬂuence of sinusoidal modeling errors. Due to time / fre-\nquency resolution problems, a partial extracted with com-\nmon partials tracking algorithms is often initiated with a\nvery low amplitude and a noisy frequency evolution before\nthe attack, see Figure 2.0 100 200 300 400 500 600467468469470471472473Frequency (Hz)\nTime (frames)(a)\n0 100 200 300 400 500 60000.020.040.060.080.1(b)Amplitude\nTime (frames)Figure 2. Frequency and amplitude vectors of a partial corre-\nsponding to the ﬁrst harmonic extracted using common sinusoidal\nanalysis techniques. Before the attack, the frequency evolution is\nblurreddue to the low amplitude.\nThis unwanted part could be automatically discarded by\nconsidering an amplitude threshold. However, the attack is\nimportant to evaluate the performance of an instrumentalist\nand could be damaged by such a removal. By considering\nthe amplitude weighted version of the standard deviation,\nwecansafelyconsidertheentireevolutionoftheparameters\nofthe partial.\n4.2. Sliding Computation\nAs presented in Section 3, no particular directive about the\ndurationofthetonehasbeengiventoperformers. Thus,the\nlength of the partial may be different for each instrumental-\nist. To compare the deviations of multiple performers on a\nsametimeinterval,weconsideraslidingcomputationofthe\nweighteddeviation:\nswd(X) =1\nKK−1X\ni=0wd(X[i∆, . . . , i ∆ + 2∆] (4)\nwhere ∆isthe hop size and K=⌊N/∆⌋.\nThisslidingcomputationisalsousefultoconsideramean\nvaluecomputedonalocalbasiswhichleadstoalessbiased\nestimationof the deviation.\nThe choice of the window length is therefore critical. If\nthe length is too small, we will consider very local devi-\nations which are probably non perceptible. On the other\nhand,ifthewindowistoolong,themeanvaluewillbevery\nbiased and we will consider global variations. Although\nthese slow variations are not perceived as annoying, they\nwill be penalized. For example in Figure 3, the evolution\nof the parameters plotted in double solid line would be pe-\nnalized, however it reﬂects a good control of the exercise.\nIn the experiments reported in Section 5, we use a window\nlengthof 80 ms.\n0200 400 600 800 1000 1200 1400 1600 1800 2000465470475480Frequency (Hz)\nTime (frames)(a)\n0200 400 600 800 1000 1200 1400 1600 1800 200000.050.10.150.20.25(b)Amplitude\nTime (frames)Figure 3. Frequency and amplitude vectors of the partials cor-\nresponding to the ﬁrst harmonic of a long tone crescendo / de-\ncrescendo played by two performers. In double solid line, the per-\nformer is an expert and in solid line, the performer is a mid-level\nstudent.\n4.3. Metrics forthe Straight Tones\nThe instrumentalist performing a straight tone is asked to\nstart at a given frequency and amplitude and ideally these\nparametersshouldremainconstantuntiltheendofthetone.\nThe sliding and weighted deviation can then be considered\ndirectly. Since the pitch and the loudness differ between\ndifferent exercises, we apply a normalization to obtain the\nfollowingmetrics:\ndf(P) =1\n¯Fswd(F) (5)\nda(P) =1\n¯Aswd(A) (6)\n4.4. Metric forthe Long Tones crescendo/ decrescendo\nWhen the instrumentalist performs a long tone crescendo /\ndecrescendo , the amplitude should start from an amplitude\ncloseto0,linearlyincreasestoreachamaximumvalue Mat\nindex m,andlinearlydecreasestoreachtheamplitudeclose\nto 0. From the evolution of the amplitude of a partial A, we\ncan compute the piecewiselinear evolution Lasfollows:\nL(i) =½\ns1(i−b) +A(b) ifi < m\ns2(l−b−i) +A(b+l)otherwise(7)\nwhere bandlare respectively the beginning index and the\nlength of the partial P. The coefﬁcients s1ands2are re-\nspectivelythe slopes of the linear increase and decrease:\ns1=M−A(b)\nm−b\ns2=M−A(b+l)\nl−m+b0200 400 600 800 1000 1200 1400 1600 1800 2000−0.0200.020.040.060.080.10.120.14Amplitude\nTime (frames)\n0 500 1000 1500−0.1−0.0500.050.10.150.20.25Amplitude\nTime (frames)Figure 4. Amplitude vector Aand piecewise linear vector Lof a\npartialfortwolongtonescrescendo/decrescendo. Thedifference\nbetween the two vectors is plotted with a dashed line. On top, the\nperformer is an expert and at bottom, the performer has a mid-\nlevel.\nTwoexamplesofthedifferencebetween Aanditspiecewise\nlinearversion Lareshownin Figure 4.\nAs a metric, we consider the sliding weighted deviation\nofthedifferencebetweentheamplitudeofthepartial Aand\na piecewise linear evolution ( L). Since the objective of the\nexercise is to reach a high amplitude from a low amplitude,\nwepropose to weight the deviationas follows:\nd<>(P) =1\n(M−min(A))swd(A−L)(8)\n4.5. Metrics forthe VibratoTones\nWhen performing a vibrato tone, the frequency should be\nmodulated in a sinusoidal manner. The evolution of the fre-\nquency during the vibrato is plotted on Figure 5. As the\nclassical saxophone vibrato is commonly taught using 4 vi-\nbrationsbyquarternotewith72beatsperminute,weﬁxthat\nthe frequency of the sinusoidal modulation should be close\nto4.8Hertz. The amplitude of the vibrato should remain\nconstant for all the tone duration. We therefore consider\nthese two criteria to evaluate the performance of an instru-\nmentalistin the case of a vibrato tone.\nWe estimate the evolution of the frequency and the am-\nplitudebyperformingaslidingtimespectralanalysisofthe\nfrequency vector F. For each spectral analysis, we con-\nsideratimeintervalequivalenttofourvibratoperiodsat 4.8\nHertz, a Hanning window and a zero-padded fast Fourier\ntransformof 4096 points.\nAtagivenframe i,themagnitudeandthelocationofthe\nmaximal value of the power spectra respectively estimate\n0 200 400 600 800 1000 1200469470471472473Frequency (Hz)\nTime (frames)(a)\n0 1 2 3 4 5 6 7 8 90246810x 10−3Magnitude\nFrequency (Hz)(b)Figure 5. Frequency vector Fof a vibrato tone. At bottom, the\nspectrum of the vector Fis plotted in solid line and the vertical\ndashed line is located at 4.8 Hz. The difference between the fre-\nquency location of the maximal value of the spectrum and this fre-\nquency is one of the metrics consideredfor the vibratotones.\nthe amplitudes VA (i)and the frequencies VF (i)of the vi-\nbratoofthepartial P. Weseekforthismaximalvalueinthe\nfollowingfrequencyregion: [3.2, 6.4] Hz.\nThe ﬁrst metric dvffor vibrated tones is deﬁned as the\ndifference between the mean value of VF and the reference\nfrequency 4.8 Hz, see Figure 5. The second one, dva, is\ndeﬁned as the standard deviation of the amplitude of the vi-\nbrato overtime:\ndvf(P) = /bardbl4.8−VF/bardbl (9)\ndva(P) = d(VA) (10)\n5. Results\nFor each sound, the metrics presented in the last section are\ncomputedfromtheevolutionovertimeoftheparametersof\nthe fundamental partial. For convenience, the values com-\nputed using these metrics are convertedin marks.\n5.1. ConversionfromMetrics to Marks\nThe technique of an instrumentalist is principally evaluated\naccordingtothebestperformersinhisclassormusicschool.\nIt is what explains that technical marks are here dependent\non the best performances. Indeed, this dependence respects\nthe technical difﬁculties of the instrument. Even for an ex-\npertsaxophonist,playingalowB pianoisverydifﬁcult,be-\ncause of the physic of the instrument. Relative evaluation,\ninstead of absolute one, allows to evaluate the performance\nwithout being inﬂuenced by the instrument itself. We have\nchosen the conﬁrmed class as mark reference (mark 100).\nIt groups high level students and teachers, and contains 7\nelements. Although the expertsclass could be a better ref-\nerence due to the better marks obtained by its elements, it\ndoes not contain enough elements (3).Amplituderesults\nαpmff <>vibrato\nexperts 1755105122126114\n(3) (8)(40)(67)(24)(150)\nconﬁrmed 11100100100100100\n(7) (22)(33)(10)(28)(98)\nmid 74589888041\n(6) (12)(22)(36)(21)(42)\nelementary 447657644100\n(8) (22)(22)(19)(7)(90)\nbeginners 432496547 -\n(6) (16)(22)(39)(17) (-)\nFrequencyresults\nαpmff <>vibrato\nexperts 1710611510016992\n(3) (49)(54)(47)(68)(18)\nconﬁrmed 11100100100100100\n(7) (37)(36)(32)(24)(26)\nmid 75471747561\n(6) (19)(19)(9)(12)(34)\nelementary 45059606545\n(8) (15)(19)(10)(15)(7)\nbeginners 442696473 -\n(6) (11)(23)(28)(27) (-)\nTable 1. Results for low note F . 5 level classes of performers\nare represented (with the number of performers by class within\nparentheses), where the conﬁrmed class is the reference 100 to\ngive marks to individual performances. The results are marks\ngiven by class with standard deviation within parentheses. We\ncan notice that the level classes are homogeneous, with reason-\nable standard deviations, and that the technical marks corre-\nspond to the supposed technical level, illustrated for example by\nthevalues of the amplitude results forthe straight tone forte.\nWe distinguish amplitude results and frequency results.\nFortheamplituderesultsweusethemetricsdeﬁnedinSec-\ntion 4: da,d<>anddvato compute the technical marks for\nrespectivelythestraighttones,thelongtone crescendo/de-\ncrescendo and the vibrato tone. The marks given as fre-\nquencyresultsarecomputedusingmetrics df,again df,and\ndvfrespectively for straight tones, long tone crescendo / de-\ncrescendo andvibrato tone.\nSincethesevaluescomputedusingthemetricsintroduced\nin the previous section are errors, we consider as marks the\ninverse of the values multiplied by 100. These marks are\nthen divided by the mean of the marks obtained by instru-\nmentalistsof the conﬁrmed class.\n5.2. Presentationof Results\nSaxophonistsplayedlongtonesandonlyafewsucceedwith\naltissimo high note A. Table 1 shows results for the note F,\nwhere αis the multiplier coefﬁcient of amplitude from pi-anostraight tone to fortestraight tone. p,mf, andfcor-\nrespond to the straight tones played respectively with low\n(piano), medium ( mezzo forte ) and high ( forte) amplitude.\nThe tone <>correspond to the long tone crescendo / de-\ncrescendo , andvibratotothe long vibrated tone.\nSaxophonists were clustered in ﬁve classes ( beginners ,\nelementary ,mid,conﬁrmed ,experts)accordingtotheiraca-\ndemic level validated by school teachers. Marks obtained\nwith the proposed metrics reﬂect fairly this ranking since\nlevelclassesarehomogeneous,withreasonablestandardde-\nviations.\nFor example, with the long tone mezzo forte ,expertsgot\n105asamplitudemark, conﬁrmed got100,mid89,elemen-\ntary65 andbeginners 49. We can notice that levels under\nconﬁrmed class have big difﬁculties with the constance of\nfrequency for the pianoandmezzo forte tones. The fre-\nquencyresult for the vibrato seems to be a good criterion to\ndifferentiateperformersunder conﬁrmed class,butnotover.\nThe amplitude results for the vibrato do not exactly corre-\nspond to the supposed technical level of performers. Surely\nthe metrics used to evaluate the quality of the vibrato could\nbe improvedin future work.\nResultsofMarionandPaularepresentedinTables2and\n3. Marion is a conﬁrmed performer of the music conserva-\ntory of Bordeaux, and Paul is a mid-level performer from\nthe music school of Talence. We can infer technical infor-\nmation from marks they got. The results for Marion, given\nby Table 2, show for example that she respects better the\namplitude constraints than the frequency ones. She must be\ncareful with the pitch, especially with low note F and note\nC.\nPaul must work to increase his αcoefﬁcient for the ex-\ntreme notes of the saxophone, since alto saxophonists can\nplay notes from low B ﬂat to high F sharp, without consid-\nering the altissimo notes. He only got a 2 for the αof high\nD as shown in Table 3. The same problem appears with the\nfrequency results of his vibrato, that decrease for high note\nD and lownote B.\nThus, with few exercises and the metrics we propose in\nSection4,itispossibletoevaluateaperformeraccordingto\nconﬁrmed performers, to identify his technical facilities or\ndefaults. It is a good way to increase the technical progress\nof a performer.\n6. Conclusion\nWe have proposed a protocol to evaluate the technical level\nofsaxophoneperformers. Wehaveshownthattheevolution\nof the spectral parameters of the sound during the perfor-\nmanceofonlyonetonecanbeconsideredtoachievesucha\ntask.\nWe introduced metrics that consider this evolution and\nappear to reﬂect important technical aspects of the perfor-\nmance. It allows us to automatically sort performers of the\nevaluation database with a strong correlation with the rank-\ning givenby professional saxophonist teachers.Amplituderesults\nnote αpmff<>vibrato\nlowB 5126100818824\nlowF 4959811210995\nC 91471085983150\nG 4117811025885\nhighD 61417712610639\nFrequencyresults\nnote αpmff<>vibrato\nlowB 59465909364\nlowF 46073719251\nC 910464675365\nG 4114130989847\nhighD 613711018914250\nTable 2.Results for Marion, a conﬁrmed performer. The ampli-\ntude results of Marion are high, with a good αcoefﬁcient. But\nshe must improve the control of the pitch of notes, regarding to\nherlowfrequency results.\nThis new protocol may be considered as a front end for\nmusiceducationrelatedsoftwaresthatintendtoprovidefeed-\nback to the performer of a wide range of classical instru-\nmentsfrom winds to bowedstrings.\nAdditionally, the use of pitch estimation techniques in-\nstead of considering the fundamental partial in a sinusoidal\nmodel may lead to a better robustness. This issue will be\nconsideredforfurtherresearches,astheproblemofgivinga\nsingle technical mark to a performer by combining the pro-\nposedmetrics.\nReferences\n[1]J¨org Langner and Werner Goebl, “Visualizing Expressive\nPerformance in Tempo-loudness Space,” Computer Music\nJournal, vol.27, no. 4, pp. 69–83, 2003.\n[2]Eric D. Scheirer, Computational Auditory Scene Analy-\nsis,chapterUsingMusicalKnowledgetoExtractExpressive\nPerformance Information from Audio Recordings, pp. 361–\n380, LawrenceErlbaum, 1998.\n[3]Efstathios Stamatatos, “A Computational Model for Dis-\ncriminating Music Performers,” in Proceedings of the\nMOSART Workshop on Current Research Directions in\nComputer Music , Barcelona, 2001, pp. 65–69.\n[4]Efstathios Stamatatos and Gerhard Widmer, “Music Per-\nformer Recognition Using an Ensemble of Simple Classi-\nﬁers,” in Proceedings of the 15th European Conference on\nArtiﬁcial Intelligence(ECAI) , 2002, pp. 335–339.\n[5]Leonardo Fuks, “Prediction and Measurements of Exhaled\nAirEffectsinthePitchofWindInstruments,”in Proceedings\nof the Institute of Acoustics ,1997, vol.19, pp. 373–378.\n[6]Joachim Haas, “SALTO - A Spectral Domain Saxophone\nSynthesizer,” in ProceedingsofMOSARTWorkshoponCur-\nrent Research Directions in Computer Music , Barcelona,\n2001.Amplitude results\nnote αpmff<>vibrato\nlowB 37589861729\nlowF 665949857111\nC 428466822147\nG 56549763652\nhighD 249658942260\nFrequencyresults\nnote αpmff<>vibrato\nlowB 38883818286\nlowF 64765696991\nC 453526846119\nG 56259823663\nhighD 23047746138\nTable 3.Results for Paul, a mid-level performer. It appears that\nPaul must improve his control of pitch and loudness specially\nplaying the lowest and the highest notes of the saxophone. For\nthesenotes(herelowBandhighD),histechnicalmarksdecrease\nand the αcoefﬁcientis low.\n[7]Erwin Schoonderwaldt, Kjetil Hansen, and Anders Asken-\nfeld, “IMUTUS - an interactive system for learning to play\na musical instrument,” in Proceedings of the International\nConference of Interactive Computer Aided Learning (ICL) ,\nAuer,Ed., Villach,Austria, 2004, pp. 143–150.\n[8]Dominique Fober, St ´ephane Letz, Yann Orlarey, Anders\nAskenfeld, Kjetil Hansen, and Erwin Schoonderwaldt,\n“IMUTUS - an Interactive Music Tuition System,” in Pro-\nceedings of the Sound and Music Computing conference\n(SMC), Paris,2004, pp. 97–103.\n[9]Roger B. Dannenberg, Marta Sanchez, Annabelle Joseph,\nRobertJoseph,RonaldSaul,andPeterCapell, “Resultsfrom\nthe Piano Tutor Project,” in Proceedings of the Fourth Bien-\nnial Arts and Technology Symposium , Connecticut College,\n1993,pp. 143–150.\n[10]“I-MAESTROproject ,”Online.\nURL:http://www.i-maestro.org .\n[11]IRCAM, Paris, AudioSculpt User’s Manual , second edition,\nApril1996.\n[12]Xavier Serra, Musical Signal Processing , chapter Musi-\ncal Sound Modeling with Sinusoids plus Noise, pp. 91–122,\nStudies on New Music Research. Swets & Zeitlinger, Lisse,\ntheNetherlands, 1997.\n[13]Sylvain Marchand and Robert Strandh, “InSpect and Re-\nSpect: Spectral Modeling, Analysis and Real-Time Synthe-\nsisSoftwareToolsforResearchersandComposers,” in Proc.\nICMC,Beijing, China, October 1999, ICMA, pp. 341–344.\n[14]Robert J. McAulay and Thomas F. Quatieri, “Speech Anal-\nysis/SynthesisBasedonaSinusoidalRepresentation,” IEEE\nTransactions on Acoustics, Speech and Signal Processing ,\nvol.34, no. 4, pp. 744–754, 1986.\n[15]Mathieu Lagrange, “A New Dissimilarity Metric For The\nClustering Of Partials Using The Common Variation Cue,”\ninProc.ICMC ,Barcelona, Spain, September 2005, ICMA."
    },
    {
        "title": "MIDI Music Genre Classification by Invariant Features.",
        "author": [
            "Adi Ruppin",
            "Hezy Yeshurun"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1415744",
        "url": "https://doi.org/10.5281/zenodo.1415744",
        "ee": "https://zenodo.org/records/1415744/files/RuppinY06.pdf",
        "abstract": "MIDI music genre classification methods are largely based on generic text classification techniques. We attempt to leverage music domain knowledge in order to improve classification results. We combine techniques of selection and extraction of musically invariant features with classification using compression distance similarity metric, which is an approximation of the theoretical, yet computationally intractable, Kolmogorov complexity. We introduce several methods for extracting features which are invariant under certain transformations commonly found in music. These methods, combined with data compression, generate a lossy compressed representation which attempts to preserve feature invariance. We analyze the performance of each method, thus gaining insight into the features that are significant to the human perception of music. Keywords: Genre classification, Kolmogorov complexity",
        "zenodo_id": 1415744,
        "dblp_key": "conf/ismir/RuppinY06",
        "keywords": [
            "MIDI music genre classification",
            "music domain knowledge",
            "musically invariant features",
            "compression distance similarity",
            "data compression",
            "feature extraction",
            "genre classification",
            "Kolmogorov complexity",
            "human perception",
            "lossy compressed representation"
        ],
        "content": "MIDI Music Genre Classification by Invariant Featur es \nAdi Ruppin         Hezy Yeshurun \nSchool of Computer Science \nTel Aviv University, Israel \nruppin@att.biz, hezy@math.tau.ac.il \nAbstract \nMIDI music genre classification methods are largely  \nbased on generic text classification techniques. We  \nattempt to leverage music domain knowledge in order  to \nimprove classification results. \nWe combine techniques of selection and extraction o f \nmusically invariant features with classification us ing \ncompression distance  similarity metric, which is an \napproximation of the theoretical, yet computational ly \nintractable, Kolmogorov complexity.  \nWe introduce several methods for extracting feature s \nwhich are invariant under certain transformations \ncommonly found in music. These methods, combined \nwith data compression, generate a lossy compressed \nrepresentation which attempts to preserve feature \ninvariance. We analyze the performance of each meth od, \nthus gaining insight into the features that are sig nificant to \nthe human perception of music. \nKeywords : Genre classification, Kolmogorov complexity \n1. Introduction \nWe seek to extract features which are the basic mus ical \nbuilding blocks, and widely reoccur within a musica l \npiece or genre, often undergoing certain transforma tions. \nComposers, psychologists and researchers place grea t \nimportance on such features. Lehrdal and Schenker [ 7, \n13] have identified significant repetition as essen tial to \nthe interpretation of music. We aim to process our corpus \nin such a way as to preserve features invariance un der \nsuch transformations. \nFor classification we use compression distance [4] to \nmeasure similarity between the musical pieces. In \naddition to this method’s power, compression effect ively \ncaptures repeating patterns. \n2. Related work \nCommon classification methods include Baysean \nclassifiers, Decisions Trees, Neural Networks and H idden \nMarkov Chains [8]. These have primarily been used i n text classification. \nWorks pertaining specifically to music mostly deal \nwith audio signals [14]. MIDI classification works \ninclude statistical methods, neural networks techni ques \n[5], pitch class methods [2], multi-resolution view s [6], \nself organizing networks [1], clustering according to \ncompression distance [4] as well as other approache s. \n3. Method \nWe represent music as a time function expressing n-\ndimensional pitch and duration vectors (chords): \n)(: )(nntf ℜ×Ζ→Ζ                          (1) \n3.1 The transformations \nWe define the transformations commonly used in musi c: \n3.1.1 Transposition transformation \nTransposition occurs frequently in music and involv es a \ntheme or segment being played at a constant pitch o ffset: \n],[,] 0 ,[)()( batctftgT∈Ι ⋅+=rr\n               (2) \nWe would like extracted pitch features to be invari ant to \nthis transformation, as two musical pieces or segme nts \ncan be considered equivalent when played at a diffe rent \npitch. \n \nFigure 1: Folk song transposition sample \n3.1.2 Augmentation/diminution transformation \nA musical theme or segment is often played or repea ted at \na different speed, or tempo: \n],[,],[)()( bat tftgT∈ΙΙ⋅=rr\nλ                 (3) \nTwo such segments which differ only in tempo or by a \nfixed note length ratio are considered an augmentation  or \ndiminution  and can be considered equivalent. \n \nFigure 2: Folk song diminution sample \nPermission to make digital or hard copies of all or  part of this work for \npersonal or classroom use is granted without fee pr ovided that copies \nare not made or distributed for profit or commercia l advantage and that \ncopies bear this notice and the full citation on th e first page. \n© 2006 University of Victoria 3.1.3 Sequential modulation transformation \nSequential modulation , or “inexact” modulation, does not \npreserve exact pitch distances, typically introduci ng no \nmore than a small error (1/2 tone): \n}21,21, 0 {,] 0 , [)()(,−∈ ++=i tT\ntCCCtftgrrr        (4) \n   \n  \nFigure 3: Sequential modulation sample (excerpt fro m \nBrahms Symphony No. IV) \n3.1.4 Crab transformation (“crab form”): \nCrab form  inverts the pitch for a melodic segment: \n],[,],[)(] 0 ,[)( bat tfCtgT∈Ι −Ι⋅−=rr rr\n        (5) \n \nFigure 4: Crab sample (excerpts from Bach Prelude # 1) \n3.2 The method, step-by-step \n3.2.1 Step 1 \nFirst, we take the quantized melody contour, ignori ng \nMIDI note-off and other events and accepting only n ote-\non events, thus disposing of performance-sensitive data. \nTo ensure invariance to transformations 3.1.1 and \n3.1.2, we take the derivatives of the pitch and dur ation \n)(tfdt d (we look at the pitch and time differences and \ndispose of the absolute pitch and time). Note durat ion \nchange is more effectively treated by calculating t he time \nratios, so we derive the logarithm of the duration. \nFor inexact sequentials described in 3.1.3 an optio nal \npreprocessing step truncates the exact pitch interv als and \ndenotes only pitch direction (up or down) and wheth er it \nis a step (1/2 or 1 tone) or a jump (1.5 tones or h igher). \nCrab form described in 3.1.4 can be treated by \npreserving only the change of the pitch direction i nstead \nof the absolute pitch direction (the second derivat ive). \nFor the purpose of comparison with earlier works, w e \nalso experiment with extracting the pitch normalize d to its \ndifference from the musical piece’s average pitch. \nFinally, we observe that musical pieces may include  \npatterns repeating at different hierarchies. For ex ample, a \nmusically significant repetition may occur at f(kt)  a<t<b. \nTo uncover such underlying musical structure, multi -\nresolution methods are applied. In our case, we app lied a \nsimple low-pass filter. \n3.2.2 Step 2 \nThe compression distance is a metric [4] which does  not \nrely only on a small set of features, but rather at tempts to \ncalculate an ideal information distance . According to this principle, two objects are considered close if each  can be \nwell compressed using the information provided by t he \nother object: \n              \n)} (), (max{( )} |(), |(max{ ),(yKxKxyKyxKyxd=                  (7) \nWhere K represents Kolmogorov complexity. \n \nCompression may be viewed as modeling the more \nideal Kolmogorov complexity 1. The caveat to the \nKolmogorov distance approach is that it is not \ncomputable. For this reason we can only attempt to \napproximate it by a standard compression algorithm such \nas Lempel Ziv [15]. We remove from the compression \nshort patterns that are likely to reoccur and skew the \nresults, typically sequences shorter than 4 or 5 sy mbols. \n3.2.3 Step 3  \nWe perform classification using the k-NN algorithm,  \nwhich takes into account k musical pieces to determ ine \nwhich category is closest. We typically take k=3. \n4. Experiments \nThe test collection was comprised of 50 musical pie ces in \nMIDI format, from 3 main categories: classical musi c, \npop music and traditional Japanese music. The first  two \ngenres are further divided by composer: Mozart, Bra hms, \nVivaldi and the Beatles, Abba and Britney Spears \nrespectively. We perform the following experiments \nusing Leave-one-out cross validation: (1) We take t he \npitch/time derivative; (2) We take the pitch/time \nderivative and truncate exact intervals; (3) The pi tch/time \nderivative after a low-pass filter; (4) The pitch/t ime \nsecond derivative; and (5) The average pitch method . \nSome confusion matrices for the above experiments \nare shown below. For each category, scores indicate  the \nnumber of matches out of the total elements in clas ses. \n  \n  \nFigure 5: Sample confusion matrices from experiment s \n                                                           \n1 Kolmogorov complexity K(x) is defined as the lengt h of the \nshortest compressed binary version from which x can  be fully \nreproduced Overall, in selecting invariant features we see a \ntradeoff between the quality of an exact composer m atch \nand the quality of a more general genre match. The \ndegree of “lossiness” influences classification acc uracy \nfor different corpora. For example, Vivaldi is reco gnized \nalmost perfectly by all methods as it is very struc tured, \nhowever Brahms is not classified correctly by metho d (1) \nbut is recognized by (2). On closer inspection, the se \npieces include many sequential modulations (inexact  \nrepetitions), that are largely missed by methods wh ich \nrely on exact intervals. On the other hand, method (2) \nfailed in other genres, as it is more error prone.  \nTable 1: Results summary \n 1 2 3 4 5 \nComposer match 51% 51% 58% 42% 27% \nMatch or 2 nd  label 73% 57% 62% 62% 45% \nGenre match 80% 75% 72% 85% 58% \n5. Comparison with other methods \nBelow is a performance summary of different methods . MIDI/ \nAudio  \nBy Date Method Genres Corpus size Success M Chai & Vercoe 2001 HMM 3 491 63% \nM Ponce de Leon 2002 SOM 2 N/A 77% \nM Shan & Kuo 2003 Associative \nclassification 2 70-\n100 84% \nM McKay 2004 NN 3 255 84% \nM Cilibrasi 2004 Compression 3 36 80% \nM Lin 2004 Repetitions 7 500 49% \nM Pollastri 2001 HMM 5 100 49% \nA Li & Tzanetakis 2003 LDA 10 1000 71% \nA Tzanetakis & \nEssl 2001 Gaussian 6 300 62% \nA Burred 2003 GMM 13 850 60% \nFigure 6: Comparison with other methods \n6. Conclusion and future work \nEven with simple compression such as LZW good \nresults were obtained, possibly rivaling those achi eved by \nhumans [11]. Since LZW essentially eliminates \ncontinuous repetitions, we can conclude that repeti tion in \nmusic occurs more often than the human ear might \nrecognize and is instrumental for its classificatio n. This is \nconsistent with music theory notion of an underlyin g \nrepetitive structure. \nOur invariant features approach produces better \nclassification results than most existing methods, such as \nthe pitch-averaging method. The performance of the \ndifferent methods highlights what features are appl icable \nto various corpora. Still, for best results some ma nual \nfine-tuning is required, as one method may be more \nfitting a specific corpus than another. Future work may include experiments with additional  \ncompression algorithms, possibly capable of handlin g the \nmulti-resolution nature of music, such as DCT. \nAdditionally, additional clustering techniques shou ld be \nexplored. \n7. References  \n[1] C. Anagnostopoulou, G., Westermann, “Classification  in \nMusic: A Computational Model for Paradigmatic \nAnalysis”, in Proc. of the International Computer Music \nConference , 1997. \n[2] S, Blackburn, and D. De Roure, “Musical Part \nClassification in Content Based Systems”, in Proc. of \nACM Multimedia ’98 , 1998, pp. 361-368. \n[3] W. Chai, B. Vercoe, Folk music classification using  \nhidden Markov models. In Proc. of International \nConference on Artificial Intelligence , 2001. \n[4] R. Cilibrasi, P. Vitanyi, R. De Wolf, “Algorithmic \nClustering of Music”, [Web site] Available: \nhttp://xxx.lanl.gov/abs/cs.SD/0303025. \n[5] R. Dannenberg, B. Thom, D. Watson, ``A Machine \nLearning Approach to Musical Style Recognition\" in 1997 \nInternational Computer Music Conference , International \nComputer Music Association, pp. 344-347.  \n[6] E.W. Large, C. Palmer, J.B. Pollack, “Reduced Memor y \nRepresentations for Music”, Cognitive Science , 1995, pp. \n53-96. \n[7] F. Lerhdal, R. Jackendoff, “A Generative Theory of  \nTonal Music”, Cambridge MIT press, 1983. \n[8] Y.H. Li., A.K. Kain, Classification of Text Documen ts, \nComputation Journal . 41, 8, pp. 537-546. \n[9] C.R. Lin, N.H. Liu, Y.H. Wu, L.P. Chen, “Music \nClassification Using Significant Repeating Patterns ”, \nDASFAA , 2004: pp. 506-518 \n[10] C. McKay, I. Fujinaga, “Automatic Genre Classificat ion \nUsing Large High-Level Musical Feature Sets”, in 5th \nInternational Conference on Music Information Retri eval , \n2004. \n[11] E. Pollastri, G. Simoncelli, “Classification of Mel odies by \nComposer with Hidden Markov Models”, in Proc. of  the  \ninternational conference on Web delivering of music , \n2001. \n[12] P.J. Ponce-de-León, Musical style identification us ing \nself-organizing maps, in Proc. of International \nConference on Web Delivering of Music , Wedelmusic \n2002. IEEE Computer Society Press, pp. 82-92, 2002.  \n[13] H. Schenker, “Harmony”, Edited by Oswald Jonas, \ntranslated by Elisabeth Mann Borgese. Cambridge: M. I.T. \nPress, 1954. \n[14] G. Tzanetakis, P. Cook, “Automatic Music Genre \nClassification of Audio Signals”, in Proc. Of Int’l \nSymposium on Music Information Retrieval , 2001. \n[15] J. Ziv, A. Lempel, “A Universal Algorithm for Seque ntial \nData Compression”, IEEE transactions on information \ntheory , 1997."
    },
    {
        "title": "Transcription of the Singing Melody in Polyphonic Music.",
        "author": [
            "Matti Ryynänen",
            "Anssi Klapuri"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1418291",
        "url": "https://doi.org/10.5281/zenodo.1418291",
        "ee": "https://zenodo.org/records/1418291/files/RyynanenK06.pdf",
        "abstract": "This paper proposes a method for the automatic transcrip- tion of singing melodies in polyphonic music. The method is based on multiple-F0 estimation followed by acoustic and musicological modeling. The acoustic model consists of separate models for singing notes and for no-melody seg- ments. The musicological model uses key estimation and note bigrams to determine the transition probabilities be- tween notes. Viterbi decoding produces a sequence of notes and rests as a transcription of the singing melody. The per- formance of the method is evaluated using the RWC popular music database for which the recall rate was 63% and pre- cision rate 46%. A significant improvement was achieved compared to a baseline method from MIREX05 evaluations. Keywords: singing transcription, acoustic modeling, musi- cological modeling, key estimation, HMM",
        "zenodo_id": 1418291,
        "dblp_key": "conf/ismir/RyynanenK06",
        "keywords": [
            "automatic transcription",
            "polyphonic music",
            "multiple-F0 estimation",
            "acoustic modeling",
            "musicological modeling",
            "key estimation",
            "note bigrams",
            "Viterbi decoding",
            "recall rate",
            "precision rate"
        ],
        "content": "Transcription of the Singing Melody in Polyphonic Music\nMatti Ryyn ¨anen and Anssi Klapuri\nInstitute of Signal Processing, Tampere University Of Tech nology\nP.O.Box 553, FI-33101 Tampere, Finland\n{matti.ryynanen, anssi.klapuri }@tut.fi\nAbstract\nThis paper proposes a method for the automatic transcrip-\ntion of singing melodies in polyphonic music. The method\nis based on multiple-F0 estimation followed by acoustic and\nmusicological modeling. The acoustic model consists of\nseparate models for singing notes and for no-melody seg-\nments. The musicological model uses key estimation and\nnote bigrams to determine the transition probabilities be-\ntween notes. Viterbi decoding produces a sequence of notes\nand rests as a transcription of the singing melody. The per-\nformance of the method is evaluated using the RWC popular\nmusic database for which the recall rate was 63% and pre-\ncision rate 46%. A signiﬁcant improvement was achieved\ncompared to a baseline method from MIREX05 evaluations.\nKeywords: singing transcription, acoustic modeling, musi-\ncological modeling, key estimation, HMM\n1. Introduction\nSinging melody transcription refers to the automatic extra c-\ntion of a parametric representation (e.g., a MIDI ﬁle) of the\nsinging performance within a polyphonic music excerpt. A\nmelody is an organized sequence of consecutive notes and\nrests, where a note has a single pitch (a note name), a begin-\nning (onset) time, and an ending (offset) time. Automatic\ntranscription of singing melodies provides an important to ol\nfor MIR applications, since a compact MIDI ﬁle of a singing\nmelody can be efﬁciently used to identify the song.\nRecently, melody transcription has become an active re-\nsearch topic. The conventional approach is to estimate the\nfundamental frequency (F0) trajectory of the melody within\npolyphonic music, such as in [1], [2], [3], [4]. Another clas s\nof transcribers produce discrete notes as a representation of\nthe melody [5], [6]. The proposed method belongs to the\nlatter category.\nThe proposed method resembles our polyphonic music\ntranscription method [7] but here it has been tailored for\nsinging melody transcription and includes improvements,\nsuch as an acoustic model for rest segments in singing. As\na baseline in our simulations, we use an early version of\nPermission to make digital or hard copies of all or part of thi s work for\npersonal or classroom use is granted without fee provided th at copies\nare not made or distributed for proﬁt or commercial advantag e and that\ncopies bear this notice and the full citation on the ﬁrst page .\nc/circlecopyrt2006 University of VictoriaFIND OPTIMAL PATH\nTHROUGH THE MODELSEXTRACT\nFEATURES\nSELECT SINGING\nNOTE RANGE\nESTIMATE\nMUSICAL KEYCHOOSE\nBETWEEN−NOTE\nTRANSITION\nPROBABILITIESCOMPUTE OBSERVATION\nLIKELIHOODS FOR MODELSREST MODEL\nGMM\nSEQUENCE OF\nNOTES AND RESTSOUTPUT:\nMUSICOLOGICAL MODELINPUT:\nAUDIO SIGNALACOUSTIC MODEL\nHMMNOTE MODEL\nFigure 1. The block diagram of the transcription method.\nthe proposed method which was evaluated second best in\nthe Music Information Retrieval Evaluation eXchange 2005\n(MIREX05)1audio-melody extraction contest. Ten state-\nof-the-art melody transcription methods were evaluated in\nthis contest where the goal was to estimate the F0 trajectory\nof the melody within polyphonic music. Our related work\nincludes monophonic singing transcription [8].\nFigure 1 shows a block diagram of the proposed method.\nFirst, an audio signal is frame-wise processed with two fea-\nture extractors, including a multiple-F0 estimator and an a c-\ncent estimator. The acoustic modeling uses these features\nto derive a hidden Markov model (HMM) for note events\nand a Gaussian mixture model (GMM) for singing rest seg-\nments. The musicological model uses the F0s to determine\nthe note range of the melody, to estimate the musical key,\nand to choose between-note transition probabilities. A sta n-\ndard Viterbi decoding ﬁnds the optimal path through the\nmodels, thus producing the transcribed sequence of notes\nand rests. The decoding simultaneously resolves the note\nonsets, the note offsets, and the note pitch labels.\nFor training and testing our transcription system, we use\nthe RWC (Real World Computing) Popular Music Database\nwhich consists of 100 acoustic recordings of typical pop\nsongs [9]. For each recording, the database includes a ref-\nerence MIDI ﬁle which contains a manual annotation of the\nsinging-melody notes. The annotated melody notes are here\nreferred to as the reference notes. Since there exist slight\n1The evaluation results and extended abstracts are availabl e at\nwww.music-ir.org/evaluation/mirex-results/audio-mel odytime deviations between the recordings and the reference\nnotes, all the notes within one reference ﬁle are collective ly\ntime-scaled to synchronize them with the acoustic signal.\nThe synchronization could be performed reliably for 96 of\nthe songs and the ﬁrst 60 seconds of each song are used. On\nthe average, each song excerpt contains approximately 84\nreference melody notes.\n2. Feature Extraction\nThe front-end of the method consists of two frame-wise fea-\nture extractors: a multiple-F0 estimator and an accent esti -\nmator. The input for the extractors is a monophonic audio\nsignal. For stereophonic input audio, the two channels are\nsummed together and divided by two, prior to the feature\nextraction.\n2.1. Multiple-F0 Estimation\nWe use the multiple-F0 estimator proposed in [10] in a fash-\nion similar to [7]. The estimator applies an auditory model\nwhere an input signal is passed through a 70-channel band-\npass ﬁlterbank and the subband signals are compressed, half -\nwave rectiﬁed, and lowpass ﬁltered. STFTs are computed\nwithin the bands and the magnitude spectra are summed\nacross channels to obtain a summary spectrum for subse-\nquent processing. Periodicity analysis is then carried out by\nsimulating a bank of comb ﬁlters in the frequency domain.\nF0s are estimated one at a time, the found sounds are can-\nceled from the mixture, and the estimation is repeated for\nthe residual.\nWe use the estimator to analyze audio signal in overlap-\nping92.9ms frames with 23.2ms interval between the be-\nginnings of successive frames. As an output, the estima-\ntor produces four feature matrices X,S,Y, andDof size\n6×tmax(tmaxis the number of analysis frames):\n•F0 estimates in matrix Xand their salience values in\nmatrix S. For a F0 estimate xit= [X]it, the salience\nvaluesit= [S]itroughly expresses how prominent\nxitis in the analysis frame t.\n•Onsetting F0 estimates in matrix Yand their onset\nstrengths in matrix D. If a sound with F0 estimate\nyit= [Y]itsets on in frame t, the onset strength value\ndit= [D]itis high.\nThe F0 values in both XandYare expressed as unrounded\nMIDI note numbers by 69+12 log2(F0/440) . Logarithm is\ntaken from the elements of SandDto compress their dy-\nnamic range, and the values in these matrices are normalized\nover all elements to zero mean and unit variance.\n2.2. Accent Signal for Note Onsets\nThe accent signal atindicates the degree of phenomenal ac-\ncent in frame t, and it is here used to indicate the poten-\ntial note onsets. There was room for improvement in the\nnote-onset transcription of [7], and the task is even more626466687072MIDI notesRef. notes\n626466687072MIDI notesRef. notes\n48 50 52 54 56−1012\ntime (s)Ref. note onsetsa) F0 estimates xit(intensity by sit)\nb) Onsetting F0 estimates yit(intensity by dit)\nc) Accent signal at\nFigure 2. The features extracted from a segment of song RWC-\nMDB-P-2001 No. 14. See text for details.\nchallenging for singing voice. Therefore, we add the accent\nsignal feature which has been successfully used in singing\ntranscription [8]. The accent estimation method proposed\nin [11] is used to produce accent signals at four frequency\nchannels. The bandwise accent signals are then summed\nacross the channels to obtain the accent signal atwhich is\ndecimated by factor 4 to match the frame rate of the multiple-\nF0 estimator. Again, logarithm is applied to the accent sig-\nnal and the signal is normalized to zero mean and unit vari-\nance.\nFigure 2 shows an example of the features compared to\nreference notes. Panels a) and b) show the F0 estimates xit\nand the onsetting F0s yitwith the reference notes, respec-\ntively. The gray level indicates the salience values sitin\npanel a) and the onset strengths ditin panel b). Panel c)\nshows the accent signal atand the note onsets in the refer-\nence melody.\n3. Acoustic and Musicological Modeling\nOur method uses two different abstraction levels to model\nsinging melodies: low-level acoustic modeling and high-\nlevel musicological modeling. The acoustic modeling aims\nat capturing the acoustic content of singing whereas the mu-\nsicological model employs information about typical me-\nlodic intervals. This approach is analogous to speech recog -\nnition systems where the acoustic model corresponds to a\nword model and the musicological model to a “language\nmodel”, for example.3.1. Note Event Model\nNote events are modeled with a 3-state left-to-right HMM.\nThe note HMM state qi,1≤i≤3,represents the typical\nvalues of the features in the i:th temporal segment of note\nevents. The model allocates one note HMM for each MIDI\nnote in the estimated note range (explained in Section 3.3).\nGiven the extracted features X,S,Y,D, andat, the ob-\nservation vector on,t∈R5is deﬁned for a note HMM with\nnominal pitch nin frame tas\non,t= (∆xn,t, sjt,∆yn,t, dkt, at), (1)\nwhere\n∆xn,t=xjt−n , (2)\n∆yn,t=ykt−n . (3)\nIndex jis obtained using\nm= arg max\ni{sit}, (4)\nj=/braceleftbigg\nm , if|xmt−n| ≤1\narg min i{|xit−n|},otherwise.(5)\nIndex kis chosen similarly to (4)–(5) by substituting k,yit,\nandditin place of j,xit, andsit, respectively.\nAn observation vector thus consists of ﬁve features: (i)\nthe F0 difference ∆xn,tbetween the measured F0 and the\nnominal pitch nof the modeled note and (ii) its correspond-\ning salience value sjt; (iii) the onsetting F0 difference ∆yn,t\nand (iv) its strength dkt; and (v) the accent signal value at.\nFor a note model n, the maximum-salience F0 estimate and\nits salience value are associated with the note if the absolu te\nF0 difference is less or equal to one semitone (see (4)–(5)),\notherwise the nearest F0 estimate is used. A similar selec-\ntion is performed to choose index kfor the onsetting F0s.\nWe use the F0 difference as a feature instead of the abso-\nlute F0 value so that only one set of note-HMM parameters\nneeds to be trained. In other words, we have a distinct note\nHMM for each nominal pitch nbut they all share the same\ntrained parameters. This can be done since the observation\nvector (1) is tailored to be different for each note model n.\nSince the F0 difference varies a lot for singing voice, we\nuse the maximum-salience F0 in contrast to the nearest F0\nused in [7]. For the same reason, the onset strength values\nare slightly increased during singing notes, and therefore ,\nwe decided to use the onsetting F0s and their strengths sim-\nilarly to normal F0 measurements.\nThe note model is trained as follows. For the time region\nof a reference note with nominal pitch n, the observation\nvectors (1) constitute a training sequence. Since for some\nreference notes there are no reliable F0 measurements avail -\nable, the observation sequence is accepted for the training\nonly if the median of the absolute F0 differences |∆xn,t|\nduring the note is less than one semitone. The note HMMparameters are then obtained using the Baum-Welch algo-\nrithm. The observation likelihood distributions are model ed\nwith a four-component GMM.\n3.2. Rest Model\nWe use a GMM for modeling the time segments where no\nsinging-melody notes are sounding, that is, rests. Rests ar e\nclearly deﬁned for monophonic singing melodies, and there-\nfore, we can now train an acoustic rest model instead of us-\ning artiﬁcial rest-state probabilities derived from note- model\nprobabilities as in [7]. The observation vector or,tfor rest\nconsists of the maximum salience and onset strength in each\nframe t, i.e.,\nor,t= (max\ni{sit},max\nj{djt}). (6)\nThe model itself is a four-component GMM (analogous to a\n1-state HMM) trained on the frames of the no-melody seg-\nments. The logarithmic observation likelihoods of the rest\nmodel are scaled to the same dynamic range with those of\nthe note model by multiplying with an experimentally-found\nconstant.\n3.3. Note Range Estimation\nThe note range estimation aims at constraining the possible\npitch range of the transcribed notes. Since singing melodie s\nusually lie within narrow note ranges, the selection makes\nthe system more robust against spurious too-high notes and\nthe interference of prominent bass line notes. This also re-\nduces the computational load due to the smaller amount of\nnote models that need to be evaluated. If the note range esti-\nmation is disabled, we use a note range from MIDI note 44\nto 84.\nThe proposed procedure takes the maximum-salience F0\nestimate in each frame. If an estimate is on MIDI note\nrange 50–74 and its salience value is above a threshold 1.0,\nthe estimate is considered as valid. Then we calculate the\nsalience-weighted mean of the valid F0s to obtain the note-\nrange mean, i.e., nmean=/an}bracketle{t(/summationtext\nixisi)/(/summationtext\nisi)/an}bracketri}ht, where op-\nerator /an}bracketle{t·/an}bracketri}htis the nearest integer function, xiis a valid F0\nestimate, and siits salience. The note range is then set to\nbenmean±12, i.e., a two octave range centered around the\nmean.\nIn 95% of the songs, all reference notes are covered by\nthe estimated ranges, and even in the worst case over 80%\nof notes are covered. Averaged over all songs, the estimated\nnote ranges cover over 99.5% of the reference notes.\n3.4. Key Estimation and Note Bigrams\nThe musicological model controls transitions between the\nnote models and the rest model in a manner similar to that\nused in [7]. The musicological model is based on the fact\nthat some note sequences are more common than others in a\ncertain musical key. A musical key is roughly deﬁned by the\nbasic note scale used in a song. A major key and a minor keyFrom note to note\nC4  C#4 D4  Eb4 E4  F4  F#4 G4  G#4 A4  Bb4 B4  C5  \nC4  \nC#4 \nD4  \nEb4 \nE4  \nF4  \nF#4 \nG4  \nG#4 \nA4  \nBb4 \nB4  \nC5  \nC4  C#4 D4  Eb4 E4  F4  F#4 G4  G#4 A4  Bb4 B4  C5  00.050.10.15Note −> rest −> note transition probabilitiesNote-transition prob-\nabilities P(nt|nt−1)\nFigure 3. Musicological transition probabilities over one octave\nfor the relative-key pair C major / A minor.\nare called a relative-key pair if they consist of scales with the\nsame notes (e.g., the C major and the A minor).\nThe musicological model ﬁrst ﬁnds the most probable\nrelative-key pair using a musical key estimation method [8] .\nThe method produces likelihoods for different major and\nminor keys from those F0 estimates xit(rounded to the near-\nest MIDI note numbers) for which salience value is larger\nthan a ﬁxed threshold (here we use zero). The most prob-\nable relative-key pair is estimated for the whole recording\nand this key pair is then used to choose transition probabil-\nities between note models and the rest model. The current\nmethod assumes that the key is not changed during the mu-\nsic excerpt. In general, this is an unrealistic assumption,\nhowever, acceptable for short excerpts of popular music.\nTime-adaptive key estimation is left for future work.\nThe transition probabilities between note HMMs are de-\nﬁned by note bigrams which were estimated from a large\ndatabase of monophonic melodies, as reported in [12]. As a\nresult, given the previous note and the most probable relati ve-\nkey pair r, the note bigram probability P(nt=j|nt−1=\ni, r)gives a transition probability to move from note ito\nnotej.\nThe musicological model assumes that it is more prob-\nable both to start and to end a note sequence with a note\nwhich is frequently occurring in the musical key. A rest-MUSICOLOGICAL MODEL\nNOTE MODEL\nTIMEMIDI NOTES\nREST MODEL\nFigure 4. The network of note models and the rest model.\nto-note transition corresponds to starting a note sequence\nand a note-to-rest transition corresponds to ending a note\nsequence. Krumhansl reported the occurrence probabilitie s\nof different notes with respect to the musical key, estimate d\nfrom a large amount of classical music [13, p. 67]. The\nmusicological model applies these distributions as proba-\nbilities for the note-to-rest and the rest-to-note transit ions\nso that the most probable relative-key pair is taken into ac-\ncount. Figure 3 shows the musicological transition proba-\nbilities for between-note, note-to-rest, and rest-to-not e tran-\nsitions in the relative-key pair C major / A minor. If the\nmusicological model is disabled, uniform distributions ov er\nall transitions are used.\n3.5. Finding the Optimal Path and Post-processing\nThe note event models and the rest model form a network of\nmodels where the note and rest transitions are controlled by\nthe musicological model. This is illustrated in Figure 4. We\nuse the Viterbi algorithm to ﬁnd the optimal path through\nthe network to produce a sequence of notes and rests, i.e.,\nthe transcribed melody. Notice that this simultaneously pr o-\nduces the note pitch labels, the note onsets, and the note\noffsets. A note sets on when the optimal path enters the\nﬁrst state of a model and sets off when the path exits the\nlast state. The note pitch label is determined by the MIDI\nnote number of the note model. Figure 5 shows an example\ntranscription after ﬁnding the path.\nAs an optional post-processing step, we may use a simple\nrule-based glissando correction. The term glissando refer s\nto a fundamental-frequency slide to the nominal note pitch.\nGlissando is usually employed at the beginning of long notes\nwhich often begin ﬂat (too low) and the fundamental fre-\nquency is matched to the note pitch during the ﬁrst 200 ms\nof a note [14, p. 203].\nIf a transcribed note shorter than 180 ms is immediately\nfollowed by a longer note with +1or+2interval between\nthe notes, these two notes are merged as one which starts at48 50 52 54 56C#4 D4  E4  F#4 G4  A4  B4  \ntime (s)Note names (in D major)\nRef. notes\nTrans. notes\nFigure 5. The transcription of the melody from song RWC-\nMDB-P-2001 No. 14. Figure 2 shows the features for this time\nsegment.\nTable 1. Simulation results summary (%).\nMethod R P F M\nMIREX05 method (baseline) 56 28 37 51\nAcoustic models (notes, rest) 60 42 48 54\n+ Note-range estimation 61 43 49 54\n+ Key estimation and note bigrams 63 45 51 53\n+ Glissando correction 63 48 53 54\nthe ﬁrst note onset, and has the MIDI note number and the\noffset of the latter note.\n4. Simulation Results\nThe melody transcription method was evaluated using three-\nfold cross-validation on the 96 songs in RWC popular mu-\nsic database. We used the performance criteria proposed\nin [7], including the recall rate ( R), the precision rate ( P),\nand mean overlap ratio ( M). The recall rate denotes how\nmany of the reference notes were correctly transcribed and\nthe precision rate how many of the transcribed notes are cor-\nrect. A reference note is correctly transcribed by a note in\nthe transcription if their MIDI note numbers are equal, the\nabsolute difference between their onset times is less than\n150 ms, and the transcribed note is not already associated\nwith another reference note. The mean overlap ratio mea-\nsures the average temporal overlap between transcribed and\nreference notes. In addition, we report the f-measure F=\n2RP/(R+P)to give an overall measure of performance.\nThe recall rate, the precision rate, the f-measure, and the\nmean overlap ratio are calculated separately for the tran-\nscriptions of each recording, and the average over all the\ntranscriptions for each criterion are reported.\n4.1. Transcription Results\nTable 1 summarizes the melody transcription results for dif -\nferent simulation setups. As a baseline method, we use ourTable 2. Results with perfect note range, perfect key, and wo rst\ncase key (%).\nMethod R P F M\nPerfect note range estimation 64 47 53 53\nPerfect key estimation 63 45 51 53\nWorst-case key estimation 37 29 32 57\nmelody-transcription method in the MIREX05 evaluations.\nThe baseline method is a slight modiﬁcation of the poly-\nphonic music transcription method proposed in [7], and it\nuses multiple-F0 estimation (two F0s per frame), note event\nmodeling, and note bigrams with key estimation.\nThe proposed transcription method reached recall rate\n63%, precision rate 48%, f-measure 53%, and mean over-\nlap ratio 54% when for the baseline method the correspond-\ning results were 56%, 28%, 37%, and 51%. The rest model\nsigniﬁcantly improves the precision compared to the base-\nline method. By adding note-range estimation, the recall\nand precision rates are slightly increased. Using key esti-\nmation with note bigrams further improves both recall and\nprecision rates. Finally, using simple post-processing to cor-\nrect glissandi, precision rate is increased, since it reduc es the\nnumber of incorrectly transcribed notes. The balance of re-\ncall and precision rates can be adjusted with the weighting\nof the rest model.\nWe studied the inﬂuence of imperfections in the note\nrange estimation and in the key estimation to the overall\nperformance of the method. The results are summarized\nin Table 2. We used the method with all the other com-\nponents but the post processing (the results on the second\nlast line in Table 1). By using this method but setting the\nnote range limits according to the minimum and maximum\nof the reference notes, the recall and precision rates incre ase\nby one and two percentage units, respectively. However,\nno improvement is obtained from using manually annotated\nkey signatures instead of the estimated keys (see key esti-\nmation results in Sec. 4.2). This suggests that small errors\nin key-estimation are not crucial to the overall performanc e.\nWe also simulated the worst-case scenario of key estimation\nby converting every reference key into a worst-case key by\nshifting its tonic by a tritone (e.g., C major key is shifted\nto F♯major). This dropped the recall and precision rates\nto 37% and 29%, respectively, thus indicating that the key\nestimation plays a major role in the method.\nThe perceived quality of the transcriptions is rather good.\nDue to the expressive nature of singing, the transcriptions\ninclude additional notes resulting from glissandi and vibr ato.\nThe additional notes sound rather natural although they are\nerroneous according to the evaluation criteria. Demonstra -\ntions of the singing melody transcriptions done with the pro -\nposed method are available at http://www.cs.tut.fi/sgn/\narg/matti/demos/melofrompoly .Table 3. Key estimation results.\nDistance on the\ncircle of ﬁfths0 1 2 3 ≥3\n% of songs 76.6 12.8 4.26 4.26 2.13\n4.2. Key Estimation Results\nWe also evaluated the performance of the key estimation\nmethod. We manually annotated key signatures for 94 songs\nof the dataset (for two songs, the key was considered too am-\nbiguous). As an evaluation criterion, we use the key signa-\nture distance on the circle of ﬁfths between the reference ke y\nand the estimated relative-key pair. This distance is equal to\nthe absolute difference in the number of accidentals (sharp s\n♯and ﬂats ♭). For example, if the reference key is A ma-\njor and the key estimator correctly produces a relative-key\npair A major / F ♯minor, the distance is zero (three sharps\nfor both keys). If the reference key is E minor (one sharp)\nand the estimated relative-key pair is F major / D minor (one\nﬂat), the distance is two.\nTable 3 shows the evaluation results for the key estima-\ntion method by using the introduced distance. The method\ncorrectly estimates the relative-key pair (distance zero) for\nover 76% of the songs. For approximately 90% of the songs,\nthe key estimation method produces correct or a perfect ﬁfth\nkey (i.e., distance one).\n5. Conclusions and Future Work\nThis paper described a method for the automatic transcrip-\ntion of singing melodies in polyphonic music. The method\nwas evaluated with realistic popular music and showed a sig-\nniﬁcant improvement in transcription accuracy compared to\nour previous method. This was mainly due to the acoustic\nmodeling of no-melody (i.e., rest) segments.\nThere is still room for improvement. One possible ap-\nproach to enhance the transcription accuracy would be to\nelaborate timbre information to discriminate singing note s\nfrom notes played with other instruments. We did some pre-\nliminary tests to include sound source separation in our tra n-\nscription system. Brieﬂy, we ﬁrst generated a large set of\nnote candidates by iteratively decoding several possible n ote\npaths. The note candidates covered approximately 80% of\nthe reference notes. We then run a sound-source separation\nalgorithm on these notes, calculate MFCCs on the separated\nnotes, model the MFCCs of the correctly transcribed candi-\ndates with a GMM to derive a timbre model, and then run the\nViterbi decoding again with the timbre model. Yet this ap-\nproach did not perform any better than the proposed system\nin the preliminary simulations. However, we believe that us -\ning timbre in singing melody transcription from polyphonic\nmusic is worth further study and has the potential of improv-\ning the results in instrument speciﬁc transcription tasks.References\n[1] J. Eggink and G. J. Brown, “Extracting melody lines from\ncomplex audio,” in Proc. 5th International Conference on\nMusic Information Retrieval , Oct. 2004.\n[2] M. Goto, “A real-time music-scene-description system:\nPredominant-F0 estimation for detecting melody and bass\nlines in real-world audio signals,” Speech Communication ,\nvol. 43, no. 4, pp. 311–329, 2004.\n[3] M. Marolt, “Audio melody extraction based on timbral sim i-\nlarity of melodic fragments,” in Proc. EUROCON 2005 , Nov.\n2005.\n[4] K. Dressler, “Extraction of the melody pitch contour fro m\npolyphonic audio,” in Proc. 6th International Conference\non Music Information Retrieval , Sept. 2005. MIREX05\nextended abstract, available online http://www.music-\nir.org/evaluation/mirex-results/articles/melody/dre ssler.pdf.\n[5] G. E. Poliner and D. P. W. Ellis, “A classiﬁcation approac h to\nmelody transcription,” in Proc. 6th International Conference\non Music Information Retrieval , pp. 161–166, Sept. 2005.\n[6] R. P. Paiva, T. Mendes, and A. Cardoso, “On the detection\nof melody notes in polyphonic audio,” in Proc. 6th Interna-\ntional Conference on Music Information Retrieval , pp. 175–\n182, Sept. 2005.\n[7] M. P. Ryyn ¨anen and A. Klapuri, “Polyphonic music tran-\nscription using note event modeling,” in Proc. 2005 IEEE\nWorkshop on Applications of Signal Processing to Audio and\nAcoustics , pp. 319–322, Oct. 2005.\n[8] M. Ryyn ¨anen, “Singing transcription,” in Signal Processing\nMethods for Music Transcription (A. Klapuri and M. Davy,\neds.), pp. 361–390, Springer Science + Business Media LLC,\n2006.\n[9] M. Goto, H. Hashiguchi, T. Nishimura, and R. Oka,\n“RWC music database: Popular, classical, and jazz music\ndatabases,” in Proc. 3rd International Conference on Music\nInformation Retrieval , Oct. 2002.\n[10] A. Klapuri, “A perceptually motivated multiple-F0 est ima-\ntion method,” in Proc. IEEE Workshop on Applications of\nSignal Processing to Audio and Acoustics , pp. 291–294, Oct.\n2005.\n[11] A. P. Klapuri, A. J. Eronen, and J. T. Astola, “Analysis o f\nthe meter of acoustic musical signals,” IEEE Transactions on\nAudio, Speech, and Language Processing , vol. 14, pp. 342–\n355, Jan. 2006.\n[12] M. P. Ryyn ¨anen and A. Klapuri, “Modelling of note events\nfor singing transcription,” in Proc. ISCA Tutorial and Re-\nsearch Workshop on Statistical and Perceptual Audio , Oct.\n2004.\n[13] C. Krumhansl, Cognitive Foundations of Musical Pitch . Ox-\nford University Press, 1990.\n[14] J. Sundberg, “The perception of singing,” in The Psychology\nof Music (D. Deutsch, ed.), ch. 6, pp. 171–214, Academic\nPress, second ed., 1999."
    },
    {
        "title": "Good Vibrations: Music Discovery through Personal Musical Concepts.",
        "author": [
            "Vegard Sandvold",
            "Thomas Aussenac",
            "Òscar Celma",
            "Perfecto Herrera"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1418275",
        "url": "https://doi.org/10.5281/zenodo.1418275",
        "ee": "https://zenodo.org/records/1418275/files/SandvoldACH06.pdf",
        "abstract": "We present here Good Vibrations, a tool for music tagging, exploration and discovery, shaped as a media player plugin, and intended for home users. The plugin allows the quick ”invention” of concepts and properties that can be tagged to songs. After some hours of active tagging, the plugin starts automatically proposing the proper tags to the user, who is also allowed to correct them. The plugin generates playlists according to the user-defined concepts, and recom- mends related music either from the user’s personal collec- tion or from the Internet (through it’s connection to Foafing the Music). The plugin runs, for the moment, in Nullsoft Winamp on Windows XP systems. Keywords: Playlist generation, music recommendation, mu- sic tagging, software tools.",
        "zenodo_id": 1418275,
        "dblp_key": "conf/ismir/SandvoldACH06",
        "keywords": [
            "Good Vibrations",
            "media player plugin",
            "home users",
            "quick invention",
            "song tagging",
            "concept properties",
            "automatic tagging",
            "user-defined playlists",
            "related music recommendation",
            "Nullsoft Winamp"
        ],
        "content": "Good Vibrations: Music Disco verythrough Personal Musical Concepts\nVegard Sandv old,Thomas Aussenac, \u001eOscar Celma, Perfecto Herr era\nUniversitat Pompeu Fabra\nPg.Circumv al\u0001laci´o8\nBarcelona, 08003 Spain\nvsandvold, taussenac, ocelma, pherrera@iua.upf.es\nAbstract\nWepresent here Good Vibrations, atoolformusic tagging,\nexploration anddisco very,shaped asamedia player plugin,\nandintended forhome users. The plugin allowsthequick\ninvention ofconcepts andproperties thatcanbetagged\ntosongs. After some hours ofactivetagging, theplugin\nstarts automatically proposing theproper tags totheuser,\nwho isalso allowed tocorrect them. Theplugin generates\nplaylists according totheuser-de\u0002ned concepts, andrecom-\nmends related music either from theuser' spersonal collec-\ntionorfrom theInternet (through it'sconnection toFoa\u0002ng\ntheMusic). The plugin runs, forthemoment, inNullsoft\nWinamp onWindowsXPsystems.\nKeywords: Playlist generation, music recommendation, mu-\nsictagging, softw aretools.\n1.Introduction\nMusic taste isformanystrongly connected toasense of\nidentity .This connection isreinforced whene verwebuya\nnewalbumforourCDcollection, participate inamusic-\nrelated discussion forum, publish playlists ontheInternet, or\nletsoftw aretrack ourlistening habits. Music creates com-\nmunities andbonds between individuals with similar taste,\nandwerely oncredible personalities andprovenentities,\ntastemak ers,toexpose ustonewandrelevantmusic.\nAnumber ofmusic recommendation systems areavail-\nable today Last.fm, AllMusic Guides, MusicStrands, Mo-\nodLogic, MusicIP andMusicmobs, justtoname afew.With\nafewexceptions, these systems arebased oncollaborati ve\n\u0002ltering, manual annotation oracombination ofboth. Foaf-\ningtheMusic[1] isanexample ofmore novelapproaches,\nwhere user pro\u0002les arecombined with conte xt-based web\ninformation andcontent-based audio descriptors.\nInthisdemonstration paper wepresent Good Vibrations,\namusic exploration anddisco verytoolwith powerful playlist\ngeneration capabilities. Itisimplemented asaplugg able\nprogram module (plugin) formedia players (currently avail-\nable forWinamp), operating mainly onpersonal music col-\nPermission tomakedigital orhard copies ofallorpartofthisworkfor\npersonal orclassroom useisgranted without feeprovided thatcopies\narenotmade ordistrib uted forpro\u0002t orcommercial advantage andthat\ncopies bear thisnotice andthefullcitation onthe\u0002rstpage.\nc\r2006 University ofVictorialections. Good Vibrations drawsit'sstrength from anovel\nuser interaction process, where theuser tags songs with as-\nsociations tosemantic concepts such asmood, intensity or\ngenre. Thesystem quickly adapts totheuser' sstyle oftag-\nging (orpersonal preference), andstarts toautomatically\npropose theproper tags.1The automatic tagging isdone\nbypredicti vemodels, builtfrom content-based audio de-\nscriptors andtheannotations already provided bytheuser.\nPlaylists aregenerated according totheuser-de\u0002ned con-\ncepts, either from theuser' spersonal collection orfrom the\nInternet. Online recommendation isprovided byFoa\u0002ng the\nMusic[1 ].\nThis conceptual categorization process canalso beseen\nasaprocess ofmacro-annotation. Byassigning semantic\nlabels tosongs onaglobal level,adatasetiscollected thatin\nturnisused totrain predicti vemodels. Good Vibrations can\nthus beseen asacontinuation oftheMUCOSA Collection\nTagger[2 ],intended forthehome user.\n2.Developing Musical Concepts\nPeople develop personal musical concepts bymaking asso-\nciations between songs andconceptual categories. Listen-\ningtoLikeaVirgin byMadonna may activateconcept\ncategories likeup-beat, carefree, sexy,female vocalist, bass\nguitar etc.Some ofthese concepts aredirectly related tothe\ncontent ofthemusic, some totheartist oraparticular lis-\ntening experience, while others arerelated tothevalues a\ngivensocial group assigns tothem. Good Vibrations letsthe\nuser record associations tomusical concepts, much likethe\nconceptual categorization process ofourcogniti vesystem.\nTheassociations form thebasis forpredicti vemodels used\ntoorganize, disco verandrecommend music.\nTheconcept tagging dialog inFigure 1offersthefollow-\ningfunctionality:\n\u000fArtist, albumandtitle information forthecurrently\nplaying song.\n\u000fLinks toadditional information onMP3.com2\n1Theparticular brand oftagging used inGood Vibrations isnotidentical\ntofreetexttagging asweknowitfrom \u0003ickr ,del.icio.us andlast.fm, butit\nbrings manyofthesame positi veeffects (\u0003exibility ,playfulness andself-\nrevelation) intothecategorization process. Anynumber ofconcepts canbe\ntagged toasong, andconcepts areeasily created ormodi\u0002ed.\n2http://www .mp3.com/Figur e1.Good Vibrations concept tagging dialog .\n\u000fAlbumcoverartdownloaded from aweb service pro-\nvided byFoa\u0002ng theMusic[1 ].\n\u000fAssociated concept categories, formated likeatextin\nnatural language. FortheMadonna example, thetext\nmay read This song isup-beat, moderate andcare-\nfree, with afemale vocalist. Thecolor ofacategory\nlabel indicates iftheassociations were tagged bythe\nuser orautomatically bythesystem.\n\u000fAccessible tagging andconcept manipulation opera-\ntions (extend, modify andcreate newconcepts).\nInorder tojump-start thetagging process, Good Vibra-\ntions willfrom theverybeginning propose tags totheuser,\nusing asetofpre-de\u0002ned predicti veconcept models built\nfrom ageneral collection ofmanually annotated music. In\nthebeginning theuser may disagree with automatically pro-\nposed tags, asconceptual categories often areidiosyncratic,\nwith differences depending onaperson' sindividual history .\nAstheuser corrects proposed tags, thepredicti vemodels\ngradually convergetowards theuser' strue concept de\u0002ni-\ntions, andthesystem getsincreasingly better attagging.\n3.Creating Playlists\nTheaimofGood Vibrations istoprovide playlists andrec-\nommendations based onmusic similarity .Asatisf actory\nuniversal measure ofsimilarity formusic canbedif\u0002cult\ntoestablish because therelevantvariables forsimilarity will\ndepend onaperson' sconceptual categories. Byusing per-\nsonal musical concepts asbasis formusic similarity com-\nputations, recommendations with ahigher levelofpersonal\nrelevance canbeprovided.\nAllplaylists canbesavedto\u0002le,orloaded directly into\nthemedia player .Theplaylist entries display asong' sartist,\nalbum, title andlength, andallowsbookmarking forlater\nretrie val,short previewsandenqueueing andplaying songs\ninthemedia player .Playlists canbegenerated according to\nthefollowing modes andconstraints:\u000fSongs similar tothecurrently playing song, sorted ac-\ncording tosimilarity ,using adistance measure devel-\noped fortheUPF MusicSurfer[4 ].\n\u000fSongs with similar tagstothecurrently playing song,\norsongs going from onecategory ofaconcept toan-\nother (e.g. from sadtohapp y).Since everysong in\ntheuser' scollection canbetheseed ofaplaylist, the\npotential forvariation isproportional tothesizeofthe\ncollection.\n\u000fAspecial playlist listing allbookmark edsongs.\n\u000fRecommendations forfreeonline music, provided by\nFoa\u0002ng theMusic[1].\nFormaximum convenience, Good Vibrations scans the\nlibrary ofthemedia player foraudio \u0002les, which arecon-\nsequently analyzed andclassi\u0002ed using thepredicti vecon-\ncept models. Descriptors areextracted from theaudio rep-\nresenting properties ofrhythm, harmon y,timbre andinstru-\nmentation, intensity ,structure andcomple xity[3 ],running\n15times faster than real-time ona2.4GHz Pentium proces-\nsor.Automatic tagging isperformed byaclassi\u0002er imple-\nmenting nearest neighbor heuristics.\n4.Pending work\nGood Vibrations willsoon beported toother platforms and\nmedia players. The plugin isusable andwell-functioning,\nbutitremains toconduct asystematic evaluation oftheus-\nability anduser satisf action. Webelie veaweb siteforshar-\ningpersonal musical concepts willbeameaningful supple-\nment toplaylist sharing. This musical culture sitewill\nallowusers toupload, rate, comment anddownload each\nothers predicti veconcept models.\n5.Ackno wledgments\nThe research anddevelopment reported here waspartially\nfunded bytheEU-FP6-IST -507142 SIMA C(Semantic In-\nteraction with Music Audio Content) project. Thecontrib u-\ntions made byBram deJong, Nicolas Wack,Amaury Hazan,\nJose Pedro Garcia andPedro Cano arehighly appreciated.\nRefer ences\n[1] \u001eO.Celma, M.Ram\u001e\u0011rezandP.Herrera, Foa\u0002ng theMusic,\nAMusic Reommendation System Based onRSS Feeds and\nUser Preferences, inProceedings ofthe6thInternational\nConfer ence onMusic Information Retrie val,London, 2005.\n[2]P.Herrera etal.MUCOSA: AMusic Content Semantic An-\nnotator ,inProceedings ofthe6thInternational Confer ence\nonMusic Information Retrie val,London, 2005.\n[3]P.Herrera etal.SIMA C:Semantic Interaction with Music\nAudio Content, inProceedings ofthe2ndEuropean Work-\nshop ontheIntegration ofKnowledg e,Semantic andDigital\nMedia Technolo gies,SavoyPlace, London.\n[4]P.Cano etal.Content-based Music Audio Recommenda-\ntion,inProceedings ofACMMultimedia 2005 ,Singapore,\nSingapore."
    },
    {
        "title": "Assigning and Visualizing Music Genres by Web-based Co-Occurrence Analysis.",
        "author": [
            "Markus Schedl",
            "Tim Pohle",
            "Peter Knees",
            "Gerhard Widmer"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1415176",
        "url": "https://doi.org/10.5281/zenodo.1415176",
        "ee": "https://zenodo.org/records/1415176/files/SchedlPKW06.pdf",
        "abstract": "We explore a simple, web-based method for predicting the genre of a given artist based on co-occurrence analysis, i.e. analyzing co-occurrences of artist and genre names on mu- sic-related web pages. To this end, we use the page counts provided by Google to estimate the relatedness of an arbi- trary artist to each of a set of genres. We investigate four dif- ferent query schemes for obtaining the page counts and two different probabilistic approaches for predicting the genre of a given artist. Evaluation is performed on two test collec- tions, a large one with a quite general genre taxonomy and a quite small one with rather specific genres. Since our approach yields estimates for the relatedness of an artist to every genre of a given genre set, we can de- rive genre distributions which incorporate information about artists that cannot be assigned a single genre. This allows us to overcome the inflexible artist-genre assignment usu- ally used in music information systems. We present a sim- ple method to visualize such genre distributions with our Traveller’s Sound Player. Finally, we briefly outline how to adapt the presented approach to extract other properties of music artists from the web. Keywords: Web Mining, Co-Occurrence Analysis, Genre Classification, Evaluation, User Interface",
        "zenodo_id": 1415176,
        "dblp_key": "conf/ismir/SchedlPKW06",
        "keywords": [
            "web-based method",
            "co-occurrence analysis",
            "genre prediction",
            "music-related web pages",
            "Google page counts",
            "probabilistic approaches",
            "genre distributions",
            "artist-genre assignment",
            "genre taxonomy",
            "user interface"
        ],
        "content": "Assigning and Visualizing Music Genres by Web-based Co-Occ urrence Analysis\nMarkus Schedl1, Tim Pohle1, Peter Knees1, Gerhard Widmer1,2\n1Department of Computational Perception, Johannes Kepler U niversity, Linz, Austria\n2Austrian Research Institute for Artiﬁcial Intelligence, V ienna, Austria\nmarkus.schedl@jku.at\nAbstract\nWe explore a simple, web-based method for predicting the\ngenre of a given artist based on co-occurrence analysis, i.e .\nanalyzing co-occurrences of artist and genre names on mu-\nsic-related web pages. To this end, we use the page counts\nprovided by Google to estimate the relatedness of an arbi-\ntrary artist to each of a set of genres. We investigate four di f-\nferent query schemes for obtaining the page counts and two\ndifferent probabilistic approaches for predicting the gen re\nof a given artist. Evaluation is performed on two test collec -\ntions, a large one with a quite general genre taxonomy and\na quite small one with rather speciﬁc genres.\nSince our approach yields estimates for the relatedness of\nan artist to every genre of a given genre set, we can de-\nrive genre distributions which incorporate information ab out\nartists that cannot be assigned a single genre. This allows\nus to overcome the inﬂexible artist-genre assignment usu-\nally used in music information systems. We present a sim-\nple method to visualize such genre distributions with our\nTraveller’s Sound Player . Finally, we brieﬂy outline how to\nadapt the presented approach to extract other properties of\nmusic artists from the web.\nKeywords: Web Mining, Co-Occurrence Analysis, Genre\nClassiﬁcation, Evaluation, User Interface\n1. Introduction and Motivation\nThe continuous growth of electronic music distribution in-\ncreases the interest in automatic retrieval of meta-data fo r\nmusic. Today, meta-data like genre, instrumentation, or ba nd\nmembers is usually provided by the music distributor who\nhas to annotate the music. Unfortunately, this method has\nseveral drawbacks. First, for the distributor, it is a very l a-\nbor intensive task. Second, even if annotation is performed\nby experts, it is usually inﬂuenced by subjective opinions\nand different local deﬁnitions, e.g. in Northern America th e\ngenre Rock/Pop is used in a broader sense than in Europe.\nIntelligent methods for automatic music annotation that re ly\non global “knowledge” as encoded in the World Wide Web\nare therefore getting more and more important. To this end,\nPermission to make digital or hard copies of all or part of thi s work for\npersonal or classroom use is granted without fee provided th at copies\nare not made or distributed for proﬁt or commercial advantag e and that\ncopies bear this notice and the full citation on the ﬁrst page .\nc/circlecopyrt2006 University of Victoriawe propose a very simple approach that automatically gath-\ners descriptive information about an arbitrary artist from the\nweb and, hence, incorporate opinions and knowledge of a\nhuge number of people from all over the world.\nIn the following section, a brief overview of web mining ap-\nproaches and co-occurrence analysis for tasks related to mu -\nsic information retrieval is given. In Section 3, we present\nour approach to inferring descriptive properties for music\nartists. We evaluate the approach on a genre assignment\nproblem using two test collections and four query schemes.\nIn Section 4, we show how to incorporate the extracted genre\ninformation in a music player, namely our Traveller’s Sound\nPlayer , to facilitate browsing. Finally, Section 5 looks into\nthe possibilities of inferring properties other than genre . We\nillustrate that with the property tempo .\n2. Related Work\nFirst experiments with co-occurrence analysis for tasks re -\nlated to MIR can be found in [5], where playlists of ra-\ndio stations and compilation CDs are used to ﬁnd co-occur-\nrences between titles and between artists. In [11, 2], ﬁrst a t-\ntempts to exploit the cultural knowledge offered by the web\ncan be found. User collections taken from the music shar-\ning service OpenNap1are analyzed, artist co-occurrences\nare extracted, and eventually, a similarity measure based o n\ncommunity meta-data is elaborated. This measure is eval-\nuated by comparison with direct subjective similarity judg -\nments obtained via a web-based survey. In contrast to this\nsurvey of non-professionals, in [1], expert opinions taken\nfrom the All Music Guide2and co-occurrences on playlists\nfrom The Art of the Mix3are used to create a similarity net-\nwork of music artists.\nFurthermore, co-occurrences of artist names on web pages\nhave been successfully applied to the task of genre classi-\nﬁcation, e.g. in [8]. The approach presented in [8] uses\nthe page counts returned by Google in reply to queries con-\ntaining artist names. Based on these page counts, complete\nsimilarity matrices are determined, i.e. a similarity valu e\nis calculated for every pair of artists. However, this ap-\nproach is computationally complex and hardly applicable\nfor large music collections. An alternative that does not pr o-\nduce complete similarity matrices is proposed in [12]. Here ,\n1http://opennap.sourceforge.net\n2http://www.allmusic.com\n3http://www.artofthemix.orgthe aim is to ﬁnd similar artists to a given seed artist us-\ning Amazon’s and Google’s web services. A list of artists\npotentially related to the seed artist is used to calculate c o-\noccurrences. Based on the number of web pages on which\nthe seed artist and the potentially related artists co-occu r, a\n“relatedness” is deﬁned for every potentially related arti st,\nand the artists are presented to the user in the order of their\nrelatedness.\nIn contrast, the approach presented in [3] considers the con -\ntent of artist-related web pages rather than only their page\ncounts. The common text mining technique TF ·IDF is ap-\nplied to weight each of a set of words extracted from the\nparticular web pages. The resulting term proﬁles are used\nfor artist-to-genre classiﬁcation.\nBesides similarity measurement and genre classiﬁcation, c o-\noccurrence analysis has also been applied to the task of de-\ntecting prototypical artists for a given genre. In [9], we\nused a technique based on a similar idea as Google’s Page-\nRank Citation Ranking (cf. [6]) on page count estimates\nto derive the prototypicality of each of a set of artists for a\ngiven genre. In [10], this approach is extended to avoid dis-\ntractions caused by artist names that equal common speech\nwords.\nThe approaches to genre classiﬁcation presented so far usu-\nally predict the genre of an unknown artist on the basis of\nsimilarities to already classiﬁed artists using, for examp le,\nSupport Vector Machines ork-Nearest Neighbor classiﬁca-\ntion. In contrast, our approach does not depend on an a\npriori assignment of artists to genres. In other words, we\nrequire no labeled training set. Indeed, lists of artist and\ngenre names are sufﬁcient since we directly investigate oc-\ncurrences of artist and genre names on music-related web\npages instead of deriving similarities between artists.\n3. Genre Assignment by Co-Occurrence\nAnalysis\nOur approach to infer genre information about an arbitrary\nartist relies on the automatic analysis of results to speciﬁ c\nqueries raised to an arbitrary search engine. We use Google\nsince it is the most popular search engine and provides a\nWeb API4. Since we do not have access to artist collections\nthat are annotated with meta-data other than genre, we must\nrestrict evaluation to genre classiﬁcation. As a result, we\nexplain the approach for gathering genre information. How-\never, we will show how to adapt the approach for extracting\narbitrary properties in Section 5.\n3.1. Methodology\nThe basic approach that we propose is very simple. Given\ntwo lists, one of artist names and one of genre names, we\nﬁrst query Google to estimate the total number of pages on\n4http://www.google.at/apiswhich each single name of the two lists is mentioned. We\ndenote the returned page counts as pcaandpcg, where ais\nthe artist name and gis the genre name. We further investi-\ngate for every combination of artist and genre name, on how\nmany web pages both can be found (denoted as pca,g). For\nthe task of genre classiﬁcation, we are indifferent of the or -\nder of the respective terms.5\nTo determine the genre of an artist, we investigate two dif-\nferent probabilistic approaches. Both use relative freque n-\ncies based on page counts. The ﬁrst one estimates the con-\nditional probability for the artist name to be found on a web\npage that mentions the genre name, more formally, p(a|g) =\npca,g\npcg. The second one estimates the probability for the genre\nname to be found on a page that contains the artist name,\nformally, p(g|a) =pca,g\npca. Both approaches yield, for every\nartist, a probability distribution for its relatedness to e ach\ngenre and should therefore be able to deal with artists that\ncannot be assigned a single genre, for example, artists that\nproduce music of very different styles. Having calculated\np(a|g)orp(g|a)for the artist ato be classiﬁed and all po-\ntential genres g, we simply predict the most probable genre.\nCompared to the approach which we proposed in [8], the\napproach presented here usually has a much lower compu-\ntational complexity since it only needs a·gqueries and cal-\nculations ( abeing the number of artists, gthe number of\ngenres, which is usually much lower than a). The approach\npresented in [8] has complexity quadratic in a.\n3.2. Experiments and Evaluation\nWe evaluated four different query schemes to obtain the\npage counts. They vary in regard to additional keywords\nadded to the artist or genre name.\n•M:“artist/genre name”+music\n•MG:“artist/genre name”+music+genre\n•MS:“artist/genre name”+music+style\n•MGS :“artist/genre name”+music+genre+style\nSince we aim at restricting the search results to web pages\nrelated to music , we use this keyword in all schemes. Addi-\ntionally, we add the terms genre and/or style to describe the\nproperties which we intend to capture.\nFor evaluation, two test collections were used. The ﬁrst one\ncomprises 1995 artists from 9very general genres that were\ntaken from the All Music Guide .6We abbreviate this col-\nlection as C1995a in the following. C1995a is used to test\nour approach on popular and mostly well-known artists. A\nlist of the artists together with their assigned genres can b e\ndownloaded from http://www.cp.jku.at/people/schedl/music/\nC1995a artists genres.txt . Since we aimed at enriching our\n5For predicting general properties, it may be better to take t he order of\nthe search terms into account, e.g. search for exact phrase “ loud volume”.\n6The collection C1995a contains artists from the genres Blues (9.4%),\nCountry (12.3%), Electronica (4.8%), Folk (4.1%), Heavy Metal (13.6%),\nJazz (40.7%), Rap(2.1%), Reggae (3.0%), and RnB (10.1%).Table 1. Accuracies in percent for the genre prediction task on\nthe 1995-artist-collection for the different query scheme s. The\nupper part of the table shows the accuracies obtained using\npca,g\npcg, the lower one those obtained withpca,g\npca. The last row\nshows the results obtained with the modiﬁed genre names (for\npca,g\npca).\npredictions 1 2 3 4 5\npca,g/pcg\nM 42.01 65.87 76.09 82.21 87.37\nMG 57.10 70.43 77.20 80.50 83.41\nMS 36.89 65.36 73.13 79.55 84.86\nMGS 23.96 39.35 50.63 61.86 72.48\npca,g/pca\nM 57.24 68.07 72.18 75.39 78.40\nMG 62.31 68.07 72.78 77.04 79.80\nMS 63.31 68.37 71.48 74.94 77.19\nMGS 43.56 58.85 68.67 73.73 78.50\npca,g/pcawith modiﬁed genre names\nMS 71.33 81.75 86.27 93.13 95.14\nTraveller’s Sound Player with genre information extracted\nfrom the web, we needed a second collection that not only\ncontains artist names, but real music tracks. To this end, we\ncompiled an in-house collection containing 2545 tracks by\n103(partially quite unknown) artists that are clustered in 13\nmuch more speciﬁc genres than in C1995a . Artist and genre\nnames are available at http://www.cp.jku.at/people/schedl/\nmusic/C103a artists genres.txt . We denote this second col-\nlection C103a .7\nWe ran the evaluation experiments using each combination\nof query scheme, prediction approach, and test collection.\nSince genre is an ill-deﬁned concept, it is often impossible\nto assign an artist to one particular genre. This issue to-\ngether with the fact that our approach yields probabilities\nrather than boolean values for the relatedness of an artist t o\neach genre permits us to predict more than one genre for\nan artist. However, our test collections only show a 1 :n\nassignment between genre and artist. Thus, we try to ac-\ncount for the probabilistic output of our genre classiﬁer in\nthe evaluation by investigating not only the most probable\ngenre of an artist but up to 5genres (those with maximum\nprobability). Hence, if the correct genre with respect to ou r\nground truth is within the 5most probable genres predicted\nby our approach, we rate the classiﬁcation result as correct .\nOf course, we also show the results when allowing only 1,\n2,3, and4genre(s) to be predicted.\n3.3. Results and Discussion\nIn Table 1, the evaluation results for the collection C1995a\nare shown. It can be seen that the prediction approach that\n7The collection C103a contains tracks from the genres A Cappella\n(4.4%), Acid Jazz (2.7%), Blues (2.5%), Bossa Nova (2.8%), Celtic (5.2%),\nElectronica (21.1%), Folk Rock (9.4%), Italian (5.6%), Jazz (5.3%), Metal\n(16.2%), Punk Rock (10.2%), Rap(13.0%), and Reggae (1.9%).Table 2. Accuracies in percent for the genre prediction task on\nthe 103-artist-collection for the different query schemes . The\nupper part of the table shows the accuracies obtained using\npca,g\npcg, the lower one those obtained withpca,g\npca.\npredictions 1 2 3 4 5\npca,g/pcg\nM 29.13 45.63 61.17 71.85 79.61\nMG 44.66 57.28 64.08 71.85 78.64\nMS 30.l0 47.57 61.17 69.90 72.82\nMGS 30.10 44.66 58.25 66.02 73.79\npca,g/pca\nM 36.89 41.75 48.54 58.25 67.96\nMG 33.98 42.72 48.54 52.43 58.25\nMS 35.92 40.78 48.54 51.46 65.05\nMGS 33.98 37.86 48.54 53.40 62.14\nrelates the combined page counts to the page counts of the\nweb pages containing artist information (pca,g\npca) yields better\nresults thanpca,g\npcgfor this collection, at least when looking\nat only the 1or2top-ranked predictions (columns 1and2).\nAn explanation for this may be that the artists of C1995a are\ngrouped in very general genres for which a disproportionall y\nlarge number of web pages (with respect to the genre clas-\nsiﬁcation task) exists. Therefore, the occurrence of a genr e\nname on a web page that mentions the artist under consid-\neration is more likely to indicate a correct artist-genre as -\nsignment than vice versa. Furthermore, we can state that the\nquery schemes MGandMSperform better than the simple\nMand the complex MGS schemes.\nTable 2 shows the classiﬁcation results for the collection\nC103a . These are obviously worse since the genre taxon-\nomy used for this collection clusters the artists according to\nmuch more speciﬁc and partially overlapping genres. An-\nother interesting fact is that, overall, the prediction app roach\npca,g\npcgyields better results thanpca,g\npcafor this collection. The\nreason for this is contrary to the explanation given above fo r\nthe collection C1995a . The best results are obtained when\nusing the query scheme MG with the prediction approach\npca,g\npcg.\nSince we also wanted to investigate which genres are of-\nten confused, we draw confusion matrices that can be found\nfor the best performing settings (query scheme and predic-\ntion approach) in Figure 1 for the collection C1995a and\nin Figure 2 for the collection C103a . A closer look at Fig-\nure 1 reveals that the genres Blues ,Country ,Jazz,Rap, and\nReggae are usually classiﬁed correctly, whereas the perfor-\nmance of Electronica ,Heavy Metal , and RnB is very bad.\nSince we suspected this to be the result of ambiguous genre\nnames (e.g. instead of Electronica ,Electronic may be used\nto denote the same genre), we performed evaluation again\nwith slightly modiﬁed genre names. More precisely, instead\nofElectronica , we used Electronic , instead of Heavy Metal ,\nwe used Metal , and instead of RnB, we used R&B , which ispredicted genrescorrect genres89.9\n0.8\n16\n19.9\n3.7\n5\n42.65.3\n96.3\n23.2\n42\n73.8\n3.3\n4.9\n13.3\n34.20.5\n0.4\n2.5\n0.4\n0.1\n1.50.5\n1.2\n34.6\n0.4\n0.40.5\n1.2\n2.6\n0.52.1\n0.8\n71.6\n2.5\n2.6\n90.8\n2.4\n1.7\n19.33.2\n1.2\n0.6\n92.7\n21.1\n0.4\n1.1\n0.2\n80\n0.51.1\n0.4\n0.4\nBl Cou Ele Folk HM Jazz Rap Reg RnBBl\nCou\nEle\nFolk\nHM\nJazz\nRap\nReg\nRnB\nFigure 1. Confusions for the genre prediction task performe d\non the 1995-artist-collection using the settings MSandpca,g\npca.\na more common abbreviation. The accuracies obtained with\nthese modiﬁed genre names can be found in the last row of\nTable 1, the confusion matrix is depicted in Figure 3. It can\nbe seen that the slight modiﬁcations considerably improve\nperformance (by more than 8% overall), especially for the\ngenres Electronic andMetal .R&B still seems to be too spe-\nciﬁc an expression.\nHowever, this modiﬁcation cannot improve the following\ndistortion that becomes obvious when inspecting the second\ncolumn of Figure 1 or 3. The genre Country is incorrectly\npredicted for a large number of artists. This can be explaine d\nby the fact that many web pages contain the term “country”,\nbut not to denote a genre name but to describe the country\nof origin of an artist. Moreover, Electronica is often mis-\nclassiﬁed as Jazz. This is not very surprising since the genre\nElectronica contains many artists that may also be classiﬁed\nasAcid Jazz . Finally, RnB is often misclassiﬁed as Blues\nbecause of the similar genre names.\n4. Visualizing Genre Distributions\nIn the following, we show how to integrate the gathered\ngenre meta-data into an existing music player. First, we\npresent our Traveller’s Sound Player . Then, we elaborate\non how we extended it to visualize the genre distribution of\narbitrary music collections. We demonstrate it on the colle c-\ntionC103a , which we already used for evaluating our genre\nprediction approach.\n4.1. The Traveller’s Sound Player\nOurTraveller’s Sound Player (TSP) was originally presented\nin [7]. The basic idea of the TSP is to arrange the trackspredicted genrescorrect genres75\n7.1 7.110025\n100\n3.6\n20\n44.4\n41.725\n100\n40\n11.1\n28.6\n16.7\n8.325 3.6\n20\n33.340\n64.3\n16.77.1\n20\n11.1\n7.1\n83.3\n8.325\n7.1\n2550\n39.3\n60\n100\nAC AJ Bl BN Ce El FR Ita Jaz Met PR Rap RegAC\nAJ\nBl\nBN\nCe\nEl\nFR\nIta\nJaz\nMet\nPR\nRap\nReg\nFigure 2. Confusions for the genre prediction task performe d\non the 103-artist-collection using the settings MGandpcag\npcg.\npredicted genrescorrect genres89.9\n1.2\n16\n4.1\n4.1\n5\n42.65.3\n95.5\n14.7\n43.2\n45.8\n3.3\n4.9\n13.3\n34.70.5\n0.8\n40\n1.2\n0.70.5\n1.2\n33.3\n0.4\n0.40.4\n1.1\n46.9\n0.1\n1.7\n0.52.1\n0.8\n41.1\n2.5\n1.8\n91.2\n2.4\n1.7\n19.32.1\n1.2\n0.5\n92.7\n1.51.1\n1.1\n1.2\n0.2\n78.3\n0.50.5\n1.2\n0.4\n0.1\n1\nBl Cou Ele Folk Met Jazz Rap Reg R&BBl\nCou\nEle\nFolk\nMet\nJazz\nRap\nReg\nR&B\nFigure 3. Confusions for the genre prediction task performe d\non the 1995-artist-collection using the modiﬁed genre name s.\nThe settings MSandpca,g\npcawere applied.\nof a music collection around a wheel that serves as a track\nselector (cf. Figure 4) such that consecutive tracks are max -\nimally similar. For this purpose, a large circular playlist is\ncreated by applying a Traveling Salesman algorithm on au-\ndio similarities. Provided that the heuristic used to solve the\nTraveling Salesman Problem ﬁnds a good tour, stylistically\ncoherent areas emerge around the wheel. A more detailed\nelaboration on the used similarity measure and evaluations\nof different TSP algorithms can be found in [7].Figure 4. Our Traveller’s Sound Player extended with the visu-\nalization of genre distributions.\n4.2. Visualization Technique\nA drawback of the existing version of the TSP is that it does\nnot guide the user in ﬁnding certain styles of music. In-\ndeed, the user has to explore different regions of the playli st\nby randomly selecting different angular positions with the\nwheel.\nTo overcome this problem, we extended the TSPby visualiz-\ning distributions of meta-data, genre in our case, to facili tate\nbrowsing the collection. For this purpose, we use the genre\ndistributions obtained by the approach which we presented\nin Section 3. We cluster the tracks of the collection in 360\nbins, one for each degree. For every bin, we then calculate\nthe mean of the probability values of the contained tracks.\nPerforming this for every genre gives a smoothed distribu-\ntion of each genre along the playlist. The values of the genre\ndistributions are mapped to gray values and made available\nto the user via a ring which is visualized around the wheel.\nTo switch between the visualizations of the particular genr e\ndistributions, the user is offered a choice box. In Figure 4, a\nscreenshot of the extended TSP is depicted. In this example,\nthe user has chosen to visualize the distribution of the genr e\nA Cappella and can easily ﬁnd music of that style.\n5. Inferring General Properties\nWe also tried to apply our approach to inferring descriptive\nattributes for artists, e.g. period of activity/popularit y, ge-\nographical origin, or the preferred tempo of their music.\nHowever, since most of the attribute values are mutually\nexclusive (e.g. tempo can be slow or fast), we found that\ncalculating and visualizing probability distributions (l ike in\nthe case of genres) did not yield good results in regard to\nthe discriminability of the attribute values. We therefore\nadopt an alternative approach that assigns every artist the\nFigure 5. Our Traveller’s Sound Player extended with the visu-\nalization of tempo distribution.\nmost probable value of the attribute under consideration.\nThis produces only discrete values 0and1for the attribute\ndistribution of an artist. Following this approach for deri v-\ning the distribution of the tempo values slow andfastusing\nthe query scheme “artist name”+music+tempo+ [slow /fast]\nand the prediction methodpca,tempo =slow/fast\npcaon the collec-\ntionC103a produces visualizations like the one depicted in\nFigure 5. Comparing this screenshot with Figure 4 reveals\nthat areas predicted to contain music of the genre A Cap-\npella also show high values for the property slow tempo .\nLikewise, Bossa Nova ,Blues , and Jazz correspond to slow\ntempo, whereas the distribution of the attribute value fast\ncorrespond to the genres Metal andPunk Rock . Indeed,\nPearson’s linear correlation coefﬁcient between the distr i-\nbution of the genre Metal and that of fast tempo is0.51. For\nPunk Rock , this correlation equals 0.36.\n6. Conclusions and Future Work\nWe have presented a web-based artist-to-genre classiﬁcati on\napproach with computational complexity a·g, where ais\nthe number of artists to be classiﬁed, and gis the number of\nclasses (genres). The approach investigates co-occurrenc es\nof artist and genre names on music-related web pages and\nuses a probabilistic model to predict the genre of an arbi-\ntrary artist.\nWe evaluated the approach on two test collections using\nfour different query schemes for obtaining the page counts\nand two different probabilistic approaches for predicting the\ngenre (pca,g\npcaandpca,g\npcg). We found thatpca,g\npcaseems to be\nbetter suited for genre taxonomies comprising general gen-\nres (like collection C1995a ), whereaspca,g\npcgis better for tax-\nonomies of speciﬁc genres (like C103a ). As for the differ-\nent query schemes, we can state that overall MG andMSperform better than the simple Mand the complex MGS\nschemes.\nTaking into account the simplicity of our approach, it per-\nforms quite well. However, we found that it depends strong-\nly on proper genre names. Indeed, using different names for\nthe same genre, e.g. Electronica vs.Electronic , may consid-\nerably change accuracy. On the whole, we can state that our\napproach is successfully applicable for genre classiﬁcati on\nas long as the used genre taxonomy is not too speciﬁc and\ngenre names are reasonably unambiguous.\nMoreover, we brieﬂy described ﬁrst steps to adapt the ap-\nproach for predicting artist properties other than genre, a nd\nshowed how to use the extracted meta-data, i.e. distributio ns\nof genres or other properties, to enrich our Traveller’s Sound\nPlayer .\nAs for future work, we will investigate other visualization\ntechniques for the obtained property distributions. For ex -\nample, we plan to incorporate the meta-data into our SOM-\nbased three-dimensional user interface for navigating in m u-\nsic collections (cf. [4]). Furthermore, methods should be\ninvestigated for dealing with synonymous genre names in\norder to overcome problems like the Electronica vs.Elec-\ntronic case. Finally, we will intensify our efforts in auto-\nmatically extracting arbitrary properties like those used , for\nexample, in the music search engine musiclens8. Our ul-\ntimate aim is to automatically annotate music at the track\nlevel according to an arbitrary ontology.\n7. Acknowledgments\nThis research is supported by the Austrian Fonds zur F¨ order -\nung der Wissenschaftlichen Forschung (FWF) under project\nnumber L112-N04 and by the Vienna Science and Technol-\nogy Fund (WWTF) under project number CI010 (Interfaces\nto Music). The Austrian Research Institute for Artiﬁcial In -\ntelligence acknowledges ﬁnancial support by the Austrian\nministries BMBWK and BMVIT.\nReferences\n[1] P. Cano and M. Koppenberger. The Emergence of Complex\nNetwork Patterns in Music Artist Networks. In Proceedings\nof the 5th International Symposium on Music Information\nRetrieval (ISMIR’04) , Barcelona, Spain, October 2004.\n[2] D. P. W. Ellis, B. Withman, A. Berenzweig, and S. Lawrence .\nThe Quest for Ground Truth in Musical Artist Similarity. In\nProceedings of the 3rd International Symposium on Music\nInformation Retrieval (ISMIR’02) , Paris, France, 2002.\n[3] P. Knees, E. Pampalk, and G. Widmer. Artist Classiﬁca-\ntion with Web-based Data. In Proceedings of the 5th In-\nternational Symposium on Music Information Retrieval (IS-\nMIR’04) , pages 517–524, Barcelona, Spain, October 2004.\n[4] P. Knees, M. Schedl, T. Pohle, and G. Widmer. An Innova-\ntive Three-Dimensional User Interface for Exploring Music\nCollections Enriched with Meta-Information from the Web.\n8http://www.musiclens.deInProceedings of the 14th ACM Conference on Multimedia\n2006 , Santa Barbara, CA, USA, October 2006. submitted.\n[5] F. Pachet, G. Westerman, and D. Laigre. Musical Data Min-\ning for Electronic Music Distribution. In Proceedings of the\n1st WedelMusic Conference , 2001.\n[6] L. Page, S. Brin, R. Motwani, and T. Winograd. The PageR-\nank Citation Ranking: Bringing Order to the Web. In Pro-\nceedings of the Annual Meeting of the American Society\nfor Information Science (ASIS’98) , pages 161–172, January\n1998.\n[7] T. Pohle, E. Pampalk, and G. Widmer. Generating\nSimilarity-based Playlists Using Traveling Salesman Algo -\nrithms. In Proceedings of the 8th International Conference\non Digital Audio Effects (DAFx-05) , pages 220–225, Madrid,\nSpain, September 20-22 2005.\n[8] M. Schedl, P. Knees, and G. Widmer. A Web-Based Ap-\nproach to Assessing Artist Similarity using Co-Occurrence s.\nInProceedings of the Fourth International Workshop\non Content-Based Multimedia Indexing (CBMI’05) , Riga,\nLatvia, June 2005.\n[9] M. Schedl, P. Knees, and G. Widmer. Discovering and Vi-\nsualizing Prototypical Artists by Web-based Co-Occurrenc e\nAnalysis. In Proceedings of the Sixth International Confer-\nence on Music Information Retrieval (ISMIR’05) , London,\nUK, September 2005.\n[10] M. Schedl, P. Knees, and G. Widmer. Improving Prototypi -\ncal Artist Detection by Penalizing Exorbitant Popularity. In\nProceedings of the Third International Symposium on Com-\nputer Music Modeling and Retrieval (CMMR’05) , Pisa, Italy,\nSeptember 2005.\n[11] B. Whitman and S. Lawrence. Inferring Descriptions and\nSimilarity for Music from Community Metadata. In Pro-\nceedings of the 2002 International Computer Music Confer-\nence, pages 591–598, Goeteborg, Sweden, September 2002.\n[12] M. Zadel and I. Fujinaga. Web Services for Music Infor-\nmation Retrieval. In Proceedings of the 5th International\nSymposium on Music Information Retrieval (ISMIR’04) ,\nBarcelona, Spain, October 2004."
    },
    {
        "title": "Language Identification in Vocal Music.",
        "author": [
            "Jochen Schwenninger",
            "Raymond Brueckner",
            "Daniel Willett",
            "Marcus E. Hennecke"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1416574",
        "url": "https://doi.org/10.5281/zenodo.1416574",
        "ee": "https://zenodo.org/records/1416574/files/SchwenningerBWH06.pdf",
        "abstract": "Language identification is an important field in spoken lan- guage processing. The identification of the language sung or spoken in music, however, has attracted only minor attention so far. This, however, is an important task when it comes to categorizing, classifying and labelling of music data. In this paper, we review our efforts of transferring well- established techniques from spoken language identification to the area of language identification in music. We present results of distinguishing German and English sung modern music and propose and evaluate techniques designed for im- proving the classification performance. These techniques involve limiting the classification on song segments that ap- pear to have vocals and on frames that are not distorted by heavy beat onsets.",
        "zenodo_id": 1416574,
        "dblp_key": "conf/ismir/SchwenningerBWH06",
        "keywords": [
            "Language identification",
            "Music language identification",
            "Spoken language processing",
            "Minor attention",
            "Music data categorization",
            "Song segments",
            "Frames",
            "Vocals",
            "Heavy beat onsets",
            "Techniques evaluation"
        ],
        "content": "Language Identiﬁcation in Vocal Music\nJochen Schwenninger\nDepartment of Electrical Engineering\nUniversity of Ulm\nUlm, Germany\njochen.schwenninger@uni-ulm.deRaymond Brueckner, Daniel Willett, Marcus Hennecke\nHarman/Becker Automotive Systems\nSpeech Dialog Systems\nUlm, Germany\n{rbrueckner,dwillett,mhennecke }@harmanbecker.com\nAbstract\nLanguage identiﬁcation is an important ﬁeld in spoken lan-\nguage processing. The identiﬁcation of the language sung or\nspoken in music, however, has attracted only minor attentio n\nso far. This, however, is an important task when it comes to\ncategorizing, classifying and labelling of music data.\nIn this paper, we review our efforts of transferring well-\nestablished techniques from spoken language identiﬁcatio n\nto the area of language identiﬁcation in music. We present\nresults of distinguishing German and English sung modern\nmusic and propose and evaluate techniques designed for im-\nproving the classiﬁcation performance. These techniques\ninvolve limiting the classiﬁcation on song segments that ap -\npear to have vocals and on frames that are not distorted by\nheavy beat onsets.\n1. Introduction\nA lot of effort has been put into the categorization of audio\ndata. However, only in recent times the lyrics have attracte d\nsome interest [1].\nAlthough the text caries the major part of the artist’s mes-\nsage, automatically extracting it is difﬁcult, as most spee ch\nrecognizers are established for spoken speech in quiet or\nwith only little random noise, not for sung language convo-\nluted with instrumental music. Besides, an automatic lyric\ntranscription is difﬁcult because of the fact that differen t\nspeech recognizers have to be used for different languages.\nThis difﬁculty is partly caused by the fact that different\nspeech recognizers have to be used for different languages.\nTo avoid using them in parallel, one possibility is to use a\nsimple scheme to determine the most probable language and\nuse only the corresponding recognizer.\nThe rest of the paper describes such a scheme for lan-\nguage identiﬁcation (LID) that is based on ideas from the\narea of LID in spoken language. Section 2 deals with the\ntechniques devised to improve the performance for music\nﬁles, compared to spoken utterances. In Section 3 the ex-\nperimental setup and in Section 4 the results are presented\nwhich lead to the conclusions in Section 5.\nPermission to make digital or hard copies of all or part of this w ork for\npersonal or classroom use is granted without fee provided th at copies\nare not made or distributed for proﬁt or commercial advantage an d that\ncopies bear this notice and the full citation on the ﬁrst page .\nc/circlecopyrt2006 University of Victoria2. Language identiﬁcation\nTraditional language identiﬁcation for spoken language is a\ntwo-stage process: First different features are extracted from\nknown utterances and their distribution is approximated us -\ning statistical models. After this training phase, unknown\nutterances can be classiﬁed by the system by extracting the\nfeatures, modeling their distribution and then comparing t his\nmodel to the already learned ones. Most probably the ut-\nterance in question belongs to the language with the most\nsimilar distribution.\nBut since the identiﬁcation accuracy is only at 64 % when\nused on modern polyphonic music, different adaptations are\ntested. These approaches aim at separating the background\nmusic from the singing voice.\n2.1. Segmentation\nA typical music track consists of passages where singing is\npresent (chorus, verse) as well as purely instrumental part s\n(intro, soli etc.). The latter degrade the accuracy of the mo d-\neling, since these additional sounds have to be included in\nthe model as well. Since we are only interested in a fraction\nof the frames, those with the highest probability of singing ,\nno perfect discrimination is needed. According to Nwe and\nWang [2], a good indicator for the presence of singing voice\nis the energy in high frequency regions.\nSo a simple discrimination system can be constructed by\nthresholding the log energy in high freqency bands. Tests\nproved that for a 512 point FFT and a sample rate of 22 kHz\nthe last 50 frequency bins yield good results.\nIn order to maintain contextual information between suc-\ncessive frames, the stream of frames was divided into 300\nwindows of about 1s duration. This length ensures that some\ncontext for the statistical modelling is available. Then al l\nframes were sorted by their energy at high frequencies. Even -\ntually a window was considered to contain singing voice if\nmore than half of its frames showed a feature value among\nthe top 30 % of all frames. This resulted in approximately\n20 - 30 seconds of audio data when applied to whole songs.\nIn the case of pure instrumental pieces, the frames with top\nvalues are taken and considered as singing voice.\n2.2. Distortion reduction\nIn modern music most of the rhythm is generated by ei-\nther the drums or the bass guitar. Therefore, most of the\ntime they are very prominent and furthermore they are oftenfound in the center of the stereo mix. In the case of a typical\nonset, they dominate the whole spectrum, effectively mask-\ning most of the voice. Hence, the assumption is that the clas-\nsiﬁcation rate can be improved by ignoring the frames dur-\ning such an onset. To ﬁnd the onsets, a Mel-based approach\nﬁrst described by West and Cox [4] was used. The detected\nframes are ignored in the following processing steps.\n2.3. Azimuth discrimination\nEven if the segments which contain singing are identiﬁed\nperfectly, still the background music detoriates the mod-\nels. One possible solution for stereophonic signals is a kin d\nof beamforming. Since the vocalist is often panned to the\ncenter of the virtual stage, a concentration in this directi on\nshould increase the overall performance.\nBarry proposes an algorithm for this purpose: Azimuth\ndiscrimination and resynthesis. He ﬁrst determines the fre -\nquency bands that originate in a speciﬁc direction and then\nreconstructs them. For details see [3]. In the experiments a\nFFT-length of 512 was used to match the preprocessing for\nthe MFCCs. The auditory scene was divided into 21 distinct\ndirections and a reconstruction window of size 4 was used.\n3. Experimental Setup\nDuring the experiments three different databases were used .\nThe ﬁrst, denoted by “SPE”, consisted of digit and short\nfree speech utterances, totaling approximately 1:45 hours of\nspeech in German (752 ﬁles) and English (776 ﬁles) each.\nThe average utterance’s length is 8s. This approach was\nused to deﬁne an upper limit for language identiﬁcation in\nthe case of perfect separation of singing and music.\nThe second database “MUS” was build from 205 mod-\nern pop and rock songs, 103 German and 102 English ones.\nThey were manually selected to cover the same musical gen-\nres, in order not to differentiate between languages by mean s\nof different instrumentation. The last dataset “ACAP” con-\nsisted entirely of A Capella pieces, 40 German und 39 En-\nglish. They were used to test the inﬂuence of the back-\nground instruments. Unfortunately, not enough solo pieces\nwere available, so that in most songs background singing\nis present, with probably the same negative effect as back-\nground instrumentation.\nThe features used for modeling are MFCC extracted from\nstereophonic audio ﬁles sampled at 22050 Hz. To keep com-\nputation time low, only 30 seconds after the ﬁrst minute\nwere selected. A 512 point FFT was performed on hamming-\nwindowed frames with 50 % overlap. A DCT for decorrela-\ntion reduced the 32 Mel-ﬁlterbank coefﬁcients to 10 values\nafter discarding the energy term. Finally a cepstral mean\nnormalization was performed for each track.\n4. Results\nThe results for the baseline system for the different datase ts\nare listed in Table 1. But in spite of the expectations, noneof the signal processing techniques described in Section 2\nimproved the poor accuracy on the “MUS” dataset.\nThe concentration on passages with high probability of\nsinging yielded comparable accuracy. Ignoring the major\nonsets to avoid distortion of speech caused a slow decrease\nof performance. The azimuth discrimination appeared highl y\nunstable and resulted in almost all tracks being either clas -\nsiﬁed as German or English, depending on the parameter\nsettings. Even ﬁltering the spectrum to the human speech\nrange (200-2500 Hz) did not improve accuracy.\nTable 1. LID accuracy\nDataset Acc [%]\nSPE 84\nACAP 68\nMUS Baseline 64\nMUS with Segmentation 63\nMUS with Distortion Reduction 61\nMUS with Azimuth Discrimination 51\n5. Conclusion and Outlook\nIn comparison to Tsai and Wang [6] who devised a system\ndiscriminating between Mandarin and English, the setup pre -\nsented in this paper is much simpler. The results achieved in\nidentifying spoken language looked very promising, but the\napplication to music was not as successful with recognition\naccuracy at only 64%. Unfortunately, none of the imple-\nmented preprocessing steps helped to improve this perfor-\nmance.\nTo increase the performance on the examined task, dif-\nferent aspects have to be improved: The discrimination be-\ntween singing and pure instrumental passages, the extrac-\ntion of the voice from the background music and ﬁnally the\nmodeling of language together with the extraction of fea-\ntures. Such a system, although computationally much more\ndemanding, might yield satisfactory results in the task of\nlanguage identiﬁcation in vocal music.\nReferences\n[1] J. P. G. Mahedro, A. Martinez, P. Cano, M. Koppenberger\nand F. Gouyon, “Natural language processing of lyrics,” in\nACM Int. Conf. on Multimedia Proc. , pp. 475-478, 2005.\n[2] T. L. Nwe and Y . Wang, “Automatic Detection of V ocal Seg-\nments in Popular Songs,” in Int. Conf. on Music Information\nRetrieval Proc. , 2004, pp. 138-145.\n[3] D. Barry, B. Lawlor and E. Coyle, “Sound Source Separa-\ntion: Azimuth Discrimination and Resynthesis,” in Int. Conf.\non Digital Audio Effects , October 2004.\n[4] K. West and S. Cox, “Finding an Optimal Segmentation for\nAudio Genre Classiﬁcation,” in Int. Conf. on Music Informa-\ntion Retrieval Proc. , 2005, pp. 680-685.\n[5] http://music-ir.org/evaluation/m2k\n[6] W. H. Tsai and H. M. Wang, “Towards Automatic Identiﬁ-\ncation of Singing Language in Popular Music Recordings,”\ninInt. Conf. on Music Information Retrieval Proc. , 2004,\npp. 568-576."
    },
    {
        "title": "Tempo Induction by Stream-Based Evaluation of Musical Events.",
        "author": [
            "Frank Seifert 0001",
            "Katharina Rasch",
            "Michael Rentzsch"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1418181",
        "url": "https://doi.org/10.5281/zenodo.1418181",
        "ee": "https://zenodo.org/records/1418181/files/SeifertRR06.pdf",
        "abstract": "We present an approach for tempo induction that is based on a more perception-oriented analysis of inter-onset intervals. Therefore we utilize auditory grouping concepts and define some rules for their formation. Finally, we show preliminary results that confirm our aim of improving the quality of tempo induction by reducing the amount of perceptually irrelevant data. Keywords: tempo induction, stream segregation.",
        "zenodo_id": 1418181,
        "dblp_key": "conf/ismir/SeifertRR06",
        "keywords": [
            "tempo induction",
            "auditory grouping",
            "inter-onset intervals",
            "stream segregation",
            "perceptually irrelevant data",
            "tempo quality",
            "rule formation",
            "perception-oriented analysis",
            "tempo induction approach",
            "tempo induction method"
        ],
        "content": "Tempo Induction by Stream-Based Evaluation of Musical EventsFrank Seifert Department of Computer Science University of Technology Chemnitz, 09107 Germany fsei@cs.tu-chemnitz.de Katharina Rasch Department of Computer Science University of Technology Chemnitz, 09107 Germany Michael Rentzsch Department of Computer Science University of Technology Chemnitz, 09107 Germany mren@cs.tu-chemnitz.de Abstract We present an approach for tempo induction that is based on a more perception-oriented analysis of inter-onset intervals. Therefore we utilize auditory grouping concepts and define some rules for their formation. Finally, we show preliminary results that confirm our aim of improving the quality of tempo induction by reducing the amount of perceptually irrelevant data.  Keywords: tempo induction, stream segregation. 1. Introduction Most beat detection algorithms of symbolical music such as MIDI rely on either a stochastic evaluation of inter-onset intervals (IOIs), e.g. [1], or oscillatory models, e.g. [2]. To improve both approaches sometimes several methods like beam search [3] and weighting of IOIs are used in order to select more consistent beat hypotheses. Weighting of IOIs [4] is based on the assumption that listeners place long notes on strong beats. However, although we also suggest a more perception-oriented IOI-analysis we would like to do this in a more generic way by evaluating only IOIs. Thus we are able to handle both life-performed symbolical music without restrictions (e.g. staccato) and prepare the foundations for an audio analysis.  Foremost, let us illustrate drawbacks of a pure IOI-analysis. Although time between onsets is essential for estimating beat, not every possible IOI contributes to the generation of tempo-hypotheses to the same degree: \n!!\"\"\"\"#$$$$$$$$$$$$%$&3333'\"\"\"\"$$$$$$$$$$$$$$&$$ Figure 1: Triplets versus eights Figure 1 shows a fragment of Debussy’s Arabesque in E-Major, presenting parallel eighths and triplets. The resulting distribution of IOIs within a real performance is given in Figure 2. For simplification only distances that are shorter than a quarter note are presented. Quarter notes are the supposed beat, thus we denote them by a distance value of 1, triplets by 1/3 and eights by 1/2. \n   Figure 2. Inter-Onset Intervals Which intervals contribute mostly to the perceived beat? Using the maximum does not seem very satisfying due to the existence of similar dominant neighbors. Additionally, more complex rhythms, tuplets, parallelism and performance deviations can cause even further complications. 2. A perceptual approach Obviously not each interval influences the perceived beat to the same degree. Intuitively, we should restrict irrelevant hypotheses by an individual evaluation of melody and bass line. Basis of a separate analysis is the human auditory system, which groups musical events [5]. Grouped musical events form streams, which are perceived independently. We hypothesize that evaluating only IOIs of streams respective distances between connected tones should improve the quality of beat-detection-algorithms. 2.1 Stream segregation We consider a stream to be a path through successive events. Whether two tones are interconnected within a stream depends on their presentation rate PR and pitch difference Δp. The inverse PR-1 describes the average time between two events. It typically ranges from 0.1 to 0.8s. The greater PR is the more streams are likely to be segregated [6]. Van Noorden [7] formalized the maximal pitch and volume difference Δpmax respective Δvmax (in dB): \n! \"pmax=not definedPR#1<0.1s$PR#1>0.8s10.1s%PR#1%0.4s10PR#1#30.4s<PR#1%0.8s& ' ( ) (  \n! \"vmax=not definedPR#1<0.1s$PR#1>0.8s30.1s%PR#1%0.4s25PR#1#50.4s<PR#1%0.8s& ' ( ) (  To find an optimal allocation of all tones N(e) to all possible streams S for each event e requires a rating value Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. © 2006 University of Victoria \n! 16 \n! 13 \n! 12 \n! 23 \n! 56 R(s,n), which describes how good n fits into s for each n∈N and each s∈S. Non-allocatable tones form a new stream. Finally, by evaluating all ratings we can determine the ideal allocation of tones to streams. R(s,n) delivers values between 0 and 100 or less than 0. Is R(s,n) negative, no allocation happens. Otherwise, R(s,n) describes how good n fits into s. To determine R(s,n), we have to compute single ratings of component parameters first. For the current event ei at timestamp t(ei) the presentation rate PR-1 consists of the average distance of k predecessors: \n! PR\"1(t(ei))=k\"1(t(ei)\"t(ei\"1))+...+(t(ei\"k+1)\"t(ei\"k)) Δp(s,n) denotes the pitch difference and Δv(s,n) the volume difference between n and the last tone of s. Both are rated by Rp(s,n) respective Rv(s,n): \n! Rp(s,n)=\"1#p(s,n)>2#pmax\"50#pmax#p(s,n)+100otherwise$ % & ' &  \n! Rv(s,n)=\"1if #v(s,n)>2#vmax\"50#vmax#v(s,n)+100otherwise$ % & ' &  Rd(s,n) describes whether n continues an ascending or descending pitch sequence. Rc(s,n) describes whether n continues a sequence of tones with similar IOIs. In both cases count() denotes the length of a found sequence. \n! Rd(s,n)=min(100,20count(d)) \n! Rc(s,n)=min(100,20count(d)) Rt(s,n) rates the temporal distance from n to last tone of s: \n! Rt(s,n)=0.1\"t(s,n)\"t(s,n)#2$1\"t(s,n)>2% & '  All single ratings have to be integrated into R(s,n). However, pitch is the most important criterion for stream segregation. The other criteria affect stream segregation only if at least two of them indicate the same result. Therefore we combine the additional criteria first:  \n! Rv,d,c(s,n)=0.3Rv(s,n)+0.55Rd(s,n)+0.15Rc(s,n) These coefficients have been determined empirically. Rv,d,c is useful to confirm or attenuate a pitch-based rating. We have to check, if Rv,d,c indicates a different stream allocation than Rp. If Rp<50 and Rv,d,c≥50 or Rp≥50 and Rv,d,c<50 then influence of pitch is diminished: Thus \n! R(s,n)=Rt(s,n)(0.3Rp+0.7Rv,d,c(s,n)), otherwise  \n! R(s,n)=Rt(s,n)(0.7Rp+0.3Rv,d,c(s,n)) 2.2 Beat estimation For a perceptually plausible time window of a few seconds we construct a histogram to show the frequency of directly connected IOIs of each stream within the window. Then, we attempt to generate a beat hypothesis that is consistent with the most frequent intervals and which lies within a plausible time frame from 60 to 240 beats per minute. By stepwise moving of this window, we can evaluate the beat-evolution over an entire song and are able to discover typical performance deviations, such as ritartando and rubato. 3. Results Based on a real performance of Debussy’s Arabesque Figure 3 shows the resulting stream segregation of the first time window:    \n Figure 3. Stream segregation of the Debussy-fragment One can recognize that both left and right hand score form their individual streams.  \n   Figure 4. Stream-based IOI-distribution Figure 4 contrasts the results of stream-based interval estimation to the pure IOI-distribution of figure 2. As predicted, the stream-based algorithm only finds eights and triplets. Integrating these IOIs in a consistent hypothesis results in the supposed quarter note beat. A short subjective study confirmed our findings especially for romantic and impressionistic music, which shows a high parallelism of complex tuplets. 4. Conclusions & further work We have presented a stream-based approach for improving IOI-based symbolic tempo detection systems. By implementing a perception-oriented evaluation of events we could reduce the amount of perceptually irrelevant data considerably and improve the quality of beat estimation substantially. Furthermore, our approach should enable a much more reliable detection of difficult rhythmic situations, such as rubato or ritartando. Our future research will focus on a quantitative and qualitative comparison of our system with existing ones. References [1] S. Dixon. “A lightweight multi-agent musical beat tracking system,” In PRICAI 2000 Proc. Of the Pacific Rim Int. Conf. on Artificial Intelligence, Springer, 2000. [2] B. Pardo: “Tempo Tracking with a Single Oscillator,” in ISMIR2004 Fifth Int. Conf. on Music Inf. Retr. Proc., 2004. [3] P. Allen, R. Dannenberg: “Tracking Musical Beats in Real Time,” in ICMC 1990 Int. Comp. Music Conf. Proc., 1990. [4] J.C. Brown, “Determination of the meter of musical scores by autocorrelation,” J. Acoust. Soc. Am. 94 (4), Oct. 1993. [5] A.S. Bregman. Auditory Scene Analysis. Cambridge, MA: Bradford/MIT Press, 1990. [6] S. Handel. Listening. MIT Press, 1989. [7] Van Noorden. “Temporal coherence in the perception of tone sequences, Institute of Perception Research, Diploma thesis, 1975. \n! 16 \n! 13 \n! 12 \n! 23 \n! 56"
    },
    {
        "title": "Social Cognition and Melodic Persistence: Where Metadata and Content Diverge.",
        "author": [
            "Eleanor Selfridge-Field"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1417699",
        "url": "https://doi.org/10.5281/zenodo.1417699",
        "ee": "https://zenodo.org/records/1417699/files/Selfridge-Field06.pdf",
        "abstract": "The automatic retrieval of members of a tune family from a database of melodies is potentially complicated by well documented divergences between textual metadata and mu- sical content. We examine recently reported cases of such divergences in search of musical features which persist even when titles change or the melodies themselves vary. We find that apart from meter and mode, the rate of pres- ervation of searchable musical features is low. Social and gestural factors appear to play a varying role in establish- ing the “melodic” identity of widely transmitted songs. The rapid growth of social computing bring urgency to better understanding the different ways in which “same” or “simi- lar” can be defined. Keywords: melodic similarity, musical features, social cognition, tune families.",
        "zenodo_id": 1417699,
        "dblp_key": "conf/ismir/Selfridge-Field06",
        "keywords": [
            "Automatic retrieval",
            "Melodic features preservation",
            "Social cognition",
            "Tune families",
            "Musical content divergence",
            "Search urgency",
            "Social computing growth",
            "Similarity definition",
            "Social and gestural factors",
            "Divergences between textual metadata"
        ],
        "content": "Social Cognition and Melodic Persistence:  \nWhere Metadata and Content Diverge \n Eleanor Selfridge-Field \nCCARH, Braun Music Center #129 \nStanford University \nStanford, CA 94305-3076 \nesfield@stanford.edu \nAbstract \nThe automatic retrieval of members of a tune family  from a \ndatabase of melodies is potentially complicated by well \ndocumented divergences between textual metadata and  mu- \nsical content. We examine recently reported cases o f such \ndivergences in search of musical features which per sist \neven when titles change or the melodies themselves vary. \nWe find that apart from meter and mode, the rate of  pres- \nervation of searchable musical features is low. Soc ial and \ngestural factors appear to play a varying role in e stablish- \ning the “melodic” identity of widely transmitted so ngs. The \nrapid growth of social computing bring urgency to b etter \nunderstanding the different ways in which “same” or  “simi- \nlar” can be defined. \nKeywords : melodic similarity, musical features, social \ncognition, tune families. \n1.  Introduction \nThe durability of specific musical features in memb ers of \ntune families seems to vary from case to case. Some  fami- \nlies cohere by title but not content, others by con tent but \nnot title. Among tune-family studies of the past ha lf century \nin anthropology, folk music research, ethnomusicolo gy, \nand historical musicology, text-based research has empha- \nsized the stability of meter and scansion in Britis h and Ca- \nnadian folksong. Group members in folksong research  have \nusually been identified by title (a principal compo nent of \nmetadata in music information retrieval). Ethnomusi cology \nhas evaluated social function in relation to conten t and sta- \nbility. In pre-industrialized communities such of t he Hopi \nof the Western U.S., cases in which musical “simila rity” is \nequated with commonality of social function (List 1 975), to \nthe apparent exclusion of musical content, raises a  pro- \nfound question as to whether the concept of musical  simi- \nlarity is itself so culture-bound as to have no rel evance in \nlarge parts of the world. (We do not address this q uestion \nhere but urge readers to consider its provocative n ature.)   Tune-family research has included delineating proc - \nesses of song transmission and musical borrowing, l ocating \nthe geographical and/or temporal margins of a persi stent \nmelody, establishing the range of uses to which a s ingle \nmelody has been put, and exploring the diverse ways  in \nwhich it can be elaborated. The study of similarity  and its \nperceptual correlates has obvious value in the exam ination \nof music plagiarism claims (Cronin, 1998). \n In historical musicology tune-family members may b e \nrelated by title, as well as place and period of or igin, per- \nforming medium, seasonal association, and other cri teria \nwhich may be present in music files as metadata. Se veral \nrecently described families consist of members with  diver- \ngent titles and origins which have been culled on t he basis \nof musical content. These diametrically opposed ap-\nproaches to similarity merit close attention by the  music-\nquery community because the sudden rise of interest  in so- \ncial computing portends the likelihood of conflicti ng paths \nto social agreement on identity of musical works. W hether \nthe level of agreement in collective perceptions of  music \nwill be greater or lesser for communities nurtured online is \ncurrently unknown. \n2.  Tune-Family Identification \nAn exploration of tune families attests to a surpri sing di- \nversity within what, in a field such as “title,” mi ght appear \nto be identical works. It also documents cases in w hich \nwhat is arguably the same melody turns up under doz ens of \ndifferent titles and many associated parameters (pl ace of \norigin, date of publication, lyrics). In the first case, meta- \ndata is eminently useable but potentially misleadin g. Iden- \ntical names do not produce identical music. In the second, \nmetadata searches may be useless but to investigate  all the \ncorresponding pieces, they first need to be encoded  sym- \nbolically. While title and social identity do not f ully con- \nstrain musical content, superficial changes to appa rent \nidentity (e.g., by lyrics added to an instrumental piece, or \nby lyrics replaced to make a hymn or patriotic song  out of a \nfolksong) do not necessarily liberate it.  \n Most current applications in music query seek to p re- \nsent probability-of-match rankings to internet user s or to \nrecommend “similar” pieces (often as suggested by a rtist or \npopular-music genre, less often on the basis of sim ilarities Permission to make digital or hard copies of all or  part of this work for \npersonal or classroom use is granted without fee pr ovided that copies \nare not made or distributed for profit or commercia l advantage and that \ncopies bear this notice and the full citation on th e first page. \n© 2006 University of Victoria of timbre, tempo, or “mood”) in a list of available  re- \ncordings. As quantities of data grow and as electro nically \navailable repertories become more diverse, procedur es \nwhich search by musical parameters will be required . It is \nin this eventuality that the lessons offered by tun e-family \nresearch  provide pertinent points for consideratio n. \nIn relation to the large effort already devoted to tagging \n“moods” and “genres” for works held in audio databa ses, \nthe quantity of tools to identify works by social f unction \n(e.g., wedding music, funeral music) is infinitesim ally \nsmall. This may simply reflect the fact that audio databases \nhold little other than popular music of the past th irty years. \nMusic with a social role distinct from pure enterta inment \nmay be conveyed by other means (including memory). Mu- \nsical memory is notoriously prone to error with res pect to \ndetail but robust for contour and meter.  Musical m emory \nwould have been the chief means of preservation for  most \nof the repertories mentioned here, although the stu dies ex- \namined confine themselves to printed exemplars. \n3. Findings from Tune-Family Meta-analysis \nIn terms of the preservation of musical features am ong \nmembers of a single tune family, systematic examina tions \n(Selfridge-Field, 2004, 2006) yield low scores for collec- \ntions formed by cultural agreement over time. The u nderly- \ning surveys include collections assembled by musico lo- \ngists, ethnomusicologists, folksong and dance resea rchers, \nhymnologists, and anthropologists. The results sugg est that \nsocial definitions of musical similarity mask a ran ge of as- \nsociations that do not necessarily privilege musical  content. \nSocial perception seems to make a significant contr ibution \nto group definitions of similarity. The families re presented \nhere are shown in Table 1 and are referenced subseq uently \nby letter. \nTable 1. Tune families examined. \nCode Title Earliest known use \nA The Morris Tune Dance (duple meter) \nB The Folìa  Dance (triple meter) \nC The Dance of Mantua Dance (duple meter) \nD Go Tell Aunt Rhody Gavotte in operetta (?) \nE Danny Boy Folksongs (2) \nWhat we can learn from tune families falls into thr ee \ncategories of information: (1) features obvious fro m the \nlisting itself, (2) features of the content which v ary from \ncase to case, and (3) deductions which can be made from \nthis combination of findings.  Using the codes give n in Ta- \nble 1, A and B are title-based collections, while C -E are \ncontent-based collections. \nFINDING #1:  Among members of both title-based and \ncontent-based collections, four families (A-D) have  some \nassociation with dancing . three distinct categories \nemerged. The Morris tune (A; a collection of tunes associ- ated with Morris dancing) offers an impressive exam ple of \npersistence, although the degree of melodic preserv ation is \ngreat only in terms of metrical stability and gener al con- \ntour. Ward (1986) traced this melody over four cent uries \nand three continents, showing its drifts toward ton ality and \nmetrical regularity but also the independence of va riations \nin its two strains from one another. An early insta nce of A \nis shown in Figure 1, a later one in Figure 2.  \n \nFigure 1. The Morris tune (A), Strains 1 and 2, as given by \nThomas Weelkes (1608).  \n1. 21. 2\n \nFigure 2. The Morris tune (A), both strains as give n by Ed- \nward Jones (1802).  \nFINDING #2:  The next most frequent feature of tune-\nfamily collections was a nationalistic  (or other ethnic) as- \nsociation. This suggests that if a song has particu lar social \nvalue, its degree of preservation is high,  despite  its wide- \nspread dissemination. \nThe “Dance of Mantua” (B; Tagliavini 1994), with 68  \nprinted instantiations (Figure 3), is essentially t he same as \nthe Israeli national anthem (“Hatikva” or “The Hope ”; Fig- \nure 4). Its adoption as a national anthem (1948) po stdated \ncenturies of oral transmission, particularly among commu- \nnities of instrumentalists whose ranks are now beli eved to \nhave included many Jewish exiles.  \n \nFigure 3. The anonymous Dance of Mantua (C; “Ballo di \nMantova”), early seventeenth-century . \n \n \nFigure 4. “Hatikva” (C 1) the Israeli national anthem (1948). \nThe “Dance of Mantua” is multiply nationalistic in that \nit also occurs in Smetana’s symphonic poem Má Vlast  ( The \nFatherland ; Figure 5), in which it represents the River \nMoldau (or Vltava in Czech). This theme has anteced ents \nin Bohemian folksong not included in Tagliavini (19 94). \nFigure 5. The River Moldau theme (C 2, from the first move- \nment of Smetana’s Má Vlast . \nThe second family, with weaker coherence among the \ncamdidates, is available in the case of the “London derry \nAire”/”Danny Boy” (Audley 2000). As the “Londonderr y \nAire” it is considered a folksong, but as “Danny Bo y” it \nhas a designated author and formal title. Audley’s study diminuendoprovides a different genealogy for the verse part o f the \nsong (shown in Figure 6) than for the chorus (Figur e 7). \nThe first genealogy is also longer by about a gener ation.  \n \nFigure 6. Beginning of the verse of the Londonderry  Aire/ \nDanny Boy (E).  \n \n \nFigure 7. Early example of what became the chorus o f the \nLondonderry Aire/Danny Boy (E 1). \nWhat formed the song as we know it today was a nine - \nteenth-century concatenation of two then unrelated songs. \nThe number of titles turned up in Audley’s search i s exten- \nsive. Londonderry (the town) was initially a fortre ss guard- \ning Scottish settlers around a port on the north co ast of \n(Northern) Ireland. Londonderry’s ethnic, social, a nd po- \nlitical identities have been contested repeatedly. As “Danny \nBoy,” the same song has been associated in recent d ecades \nwith the Republic of Ireland (Eire) and with Irish immi- \ngrants in the U.S. (likening its social function to  that of \n“Hatikva” of “Moldau” in Israel and Bohemia). In su ch \ncases particular melodies seem to have a totemic role , in \nthat they give mnemonic assistance to cultural memo ry. \nFINDING #3:  Cases in which a composed melody passes \ninto common usage are fewer than those of persisten t reuse \nof a tune of unknown origin. Among the studies revi ewed, \nan unusual one is that of Sickbert (1999), who find s in a \ngavotte in Rousseau’s opera Le Devin du village  (1752; \nFigure 7) the origins of the American folk song “Go  Tell \nAunt Rhody” (Figure 8) .  \nFigure 7. Rousseau: Gavotte (D) from  Le Devin du village.  \n Figure 8. “Go Tell Aunt Rhody (D 1).” \nThis pair of “matches” is somewhat disputable. Rous - \nseau’s piece has a melodic range of a perfect fourt h but in- \ncludes four different note durations (plus that of the grace \nnote). “Aunt Rhody” spans a perfect fifth but inclu des only \ntwo durational values. To judge from studies of mus ic per- \nception, the difference of a third between the firs t notes of \nBar 3 is so significant (because of its occurrence at the start \nof the second phrase) that these melodies should pr obably \nnot be considered to belong to the same melodic fam ily.  \nCollections formed on the basis of contour and mode  \nmay not be sufficiently persuasive to satisfy eithe r cogni- \ntive or social selection criteria. These two melodi es are cer- tainly more similar to each other than some of the pieces \nsaid by List’s Hopi subjects to be “the same.” Most  of \nSickbert’s items conform well to the melody in Figu re 8 \nbut less well to the melody in Figure 7. Some of Wa rd’s \n“Morris tune” members are so shapeless as to barely  qual- \nify as any particular melody. In contrast, there is  very little \nin Tagliavini’s collection that is open to dispute.   \nFINDING #4:  Melodies may be preserved in parallel , in \nwhich case coexistence may be a necessary condition  of a \nmatch. The technique is pervasive in dance music of  the \nsixteenth through eighteenth centuries. Variations of the so-\ncalled folie d’Espagne (Spanish follies) set by numerous \ncomposers including Corelli and Vivaldi were intend ed to \nevoke a mental state—madness. The corresponding dan ce \nbecame faster and faster to mimic frenzy, as in Sic ily’s tar- \nantella . This points to a mimetic role.   \nAs a subject for query, the folia  required both a treble \nand a bass line corresponding in meter, mode, and c ontour \nto Figures 9 and 10.  \n \nFigure 9. The Folìa treble (B 1), all iterations but last. \n \nFigure 10. The Folìa bass (B 2), last iteration. \nArtistic settings of La Folìa may lack any explicit  state- \nment of either the treble or the bass line and yet in their \nvarious incarnations they remained inseparable. As in much \nother music called “classical” today, the artistry was one of \nconcealment. Figure 11 (the start of a keyboard set ting by \nAlessandro Scarlatti) offers only one of myriad set tings.  \n \nFigure 11. Start of a keyboard variation by A. Scar latti on \nB1,2  (“La Folìa di Spagna”).  \nFigure 11 raises a further question about the cogni tive \nlimits of perceived similarity.  \n4.  Common Features of Family Members \nWhat would be useful to extrapolate from tune colle ctions \nsuch as those cited here is a set of musical featur es which \nconsistently sustain a singular melodic identity ac ross time \nand place. Apart from more subtle questions (e.g., How \nmuch deviation is too much? Under what circumstance s \ndoes the answer depend purely on features internal to the GotellAuntRhody,GotellAuntRhody,GotellAunt Rho dy the oldgraygooseisdead.music? How often does it depend on social agreement ?), \nwhat can be learned from large families of tunes? T able 2 \ngives an overview of feature sustenance in the five  families \nconsidered above. Pitch-contour assessments concern  sur- \nface activity. Implied contours are almost always p reserved \nbut they are difficult to search. The tune-families  are iden- \ntified as in Table 1. \nTable 2. Persistence of specific features within tu ne families. \n Family \nFeature pre- \nserved \n A B \n \n C \n \n D E \n \n To- \ntals \nTitle 1 2 4 4 3 14 \nComposer at- \ntribution 4 4 3 3 3 17 \nSocial function 1 3 4 4 4 16 \nMeter 1 1 2 1 3 8 \nMode 1 1 2 1 2 7 \nPitch contour 1 3 2 2 2 10 \nPitches on ac- \ncented beats 3 3 2 3 2 13 \nPitches initiat- \ning and termi- \nnating phrases 2 3 1 3 3 12 \nTotals 14 20 20 21 22  \nKey       \n always  1     \n usually  2     \n sometimes  3     \n Rarely or never  4     \n \nRecalling that A and B are title-driven, while the other \nthree families are content-driven, we note that met er and \nmode vary less in de facto  social collections (A, B) than in \nthose constituted by individual selection. The over all pres- \nervation of mode (overall score = 7) is the most st riking \nfeature consistently present among all the families . We \nshould expect meter (score = 8) to be persistent in  melodies \nassociated with dancing and this is borne out (only  E has \nno connection to dancing). Meter and mode do not by  \nthemselves cull short lists of match candidates in large da- \ntabases (those with > 1000 items); see Sapp et al. (2004) \nbecause they are too general. Metadata offers littl e help, \nsince titles (score = 14) and composer attributions  (score = \n17) vary more than the musical content! So too does  social \nfunction (insofar as it is recorded) except in one family. \nThe main implication is clear: neither via metadata - nor via \nmusical-content searches can capture the same clust ers of \npotential match candidates as those identified in r ecent \ntune-family research. Agreement on contour (score=1 0) \ncomes at the price of excessive generality. As folk song re- searchers have long held, terminal notes of phrases  (score \n= 12) seem to offer one of the more promising param eters \nfor content searches.  \nFuture work might fruitfully address correspondence s in \ninitial, metrically accented, and terminal notes of  parallel \nphrases within tune-family members (the “Aunt Rhody ” \nquestion) and their cognitive correlates. How much can \ncomponent parts of a song vary without asserting a new \nidentity (social or individual) on the song itself?  Ahlbäck \n(2004) touches on some aspects of this question, pa rticu- \nlarly for repertories with complex and irregular me ters, in \nhis wide-ranging enquiry. More studies in the socia l per- \nception and cognition of melody are essential to th e future \nof music query in the boundless terrain of “simple”  song.  \nAcknowledgments \nThe musical examples were typeset by Don Anthony an d \nDon Simons.  \nReferences \n[1]  Ahlbäck, Sven. Melody without Notes: Towards a Theory \nof Melody Cognition . Gothenburg, 2004.   \n[2]  Audley, Brian.  “The Provenance of the Londonderry Air,” \nJournal of the Royal Musical Association  125 (2000), 205-\n247.  \n[3]  Cronin, Charles. “”Concepts of Melodic Similarity i n Mu- \nsic-Copyright Infringement Suits,” Melodic Similarity: \nConcepts, Procedures, and Applications  ( Computing in \nMusicology  11, 1998), 187-209. \n[4]  List, George. “Hopi Melodic Concepts,” Journal of the \nAmerican Musicological Society , XXXVIII (1985), 143-\n152. \n[5]  Sapp, Craig Stuart, Yi-Wen Liu, and Eleanor Selfrid ge-\nField, “Search Effectiveness Measures for Symbolic Music \nQueries in Very Large Databases,” ISMIR2004 . \n[6]  Selfridge-Field, Eleanor. “Social Dimensions of Mel odic \nIdentity, Cognition, and Association,” Musicae Scientiae: \nThe Journal of the European Society for the Cogniti ve Sci- \nences of Music , vol. 6, no. 2 (Autumn 2006), forthcoming. \n[7]  Selfridge-Field, Eleanor. “Towards a Measure of Cog nitive \nDistance in Melodic Similarity,”  Music Query: Methods, \nModels, and User Studies  (Computing in Musicology 13, \n2004), pp. 93-112. \n[8]  Sickbert, Murl. “Go Tell Aunt Rhody She’s Rousseau’ s \nDream” in Vistas of American Music: Essays and Composi- \ntions in Honor of William K. Kearns (Warren: Harmonie \nPark Press, 1999), pp. 125-150. \n[9]  Tagliavini, Luigi Ferdinando. “ ‘Il ballo di Mantov a”, ov- \nvero, ‘Fuggi, fuggi, da questo cielo’, ovvero, ‘Cec ilia’, ov- \nvero…” in Max Lutolf zum 60. Geburtstag: Festschrift \n(Basel: Wiese, 1994), 135-175. \n[10]  Ward, John.  “The Morris Tune,” Journal of the American \nMusicological Society , XXXIX (1986), 294-331."
    },
    {
        "title": "Joint Beat &amp; Tatum Tracking from Music Signals.",
        "author": [
            "Jarno Seppänen",
            "Antti J. Eronen",
            "Jarmo Hiipakka"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1417319",
        "url": "https://doi.org/10.5281/zenodo.1417319",
        "ee": "https://zenodo.org/records/1417319/files/SeppanenEH06.pdf",
        "abstract": "This paper presents a method for extracting two key met- rical properties, the beat and the tatum, from acoustic sig- nals of popular music. The method is computationally very efficient while performing comparably to earlier methods. High efficiency is achieved through multirate accent analy- sis, discrete cosine transform periodicity analysis, and phase estimation by adaptive comb filtering. During analysis, the music signals are first represented in terms of accentuation on four frequency subbands, and then the accent signals are transformed into periodicity domain. Beat and tatum peri- ods and phases are estimated in a probabilistic setting, incor- porating primitive musicological knowledge of beat–tatum relations, the prior distributions, and the temporal continu- ities of beats and tatums. In an evaluation with 192 songs, the beat tracking accuracy of the proposed method was found comparable to the state of the art. Complexity evaluation showed that the computational cost is less than 1% of earlier",
        "zenodo_id": 1417319,
        "dblp_key": "conf/ismir/SeppanenEH06",
        "keywords": [
            "beat",
            "tatum",
            "acoustic signals",
            "popular music",
            "computational efficiency",
            "multirate accent analysis",
            "discrete cosine transform periodicity analysis",
            "phase estimation",
            "adaptive comb filtering",
            "musicological knowledge"
        ],
        "content": "Joint Beat & Tatum Tracking from Music Signals\nJarno Sepp ¨anen Antti Eronen Jarmo Hiipakka\nNokia Research Center\nP.O.Box 407, FI-00045 NOKIA GROUP, Finland\n{jarno.seppanen, antti.eronen, jarmo.hiipakka }@nokia.com\nAbstract\nThis paper presents a method for extracting two key met-\nrical properties, the beat and the tatum, from acoustic sig-\nnals of popular music. The method is computationally very\nefﬁcient while performing comparably to earlier methods.\nHigh efﬁciency is achieved through multirate accent analy-\nsis, discrete cosine transform periodicity analysis, and phase\nestimation by adaptive comb ﬁltering. During analysis, the\nmusic signals are ﬁrst represented in terms of accentuation\non four frequency subbands, and then the accent signals are\ntransformed into periodicity domain. Beat and tatum peri-\nods and phases are estimated in a probabilistic setting, incor-\nporating primitive musicological knowledge of beat–tatum\nrelations, the prior distributions, and the temporal continu-\nities of beats and tatums. In an evaluation with 192 songs,\nthe beat tracking accuracy of the proposed method was found\ncomparable to the state of the art. Complexity evaluation\nshowed that the computational cost is less than 1% of earlier\nmethods. The authors have written a real-time implementa-\ntion of the method for the S60 smartphone platform.\nKeywords: Beat tracking, music meter estimation, rhythm\nanalysis.\n1. Introduction\nRecent years have brought signiﬁcant advances in the ﬁeld\nof automatic music signal analysis, and music meter estima-\ntion is no exception. In general, the music meter contains\na nested grouping of pulses called metrical levels , where\npulses on higher levels are subsets of the lower level pulses;\nthe most salient level is known as the beat, and the lowest\nlevel is termed the tatum [1, p. 21].\nMetrical analysis of music signals has many applications\nranging from browsing and visualization to classiﬁcation\nand recommendation of music. The state of the art has ad-\nvanced high in performance, but the computational require-\nments have also remained restrictively high. The proposed\nmethod signiﬁcantly improves computational efﬁciency while\nmaintaining satisfactory performance.\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies\nare not made or distributed for proﬁt or commercial advantage and that\ncopies bear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2006 University of VictoriaThe technical approaches for meter estimation are vari-\nous, including e.g.autocorrelation based methods [6], inter-\nonset interval histogramming [5], or banks of comb ﬁlter\nresonators [4], possibly followed by a probabilistic model [3].\nSee [2] for a review on rhythm analysis systems.\n2. Algorithm Description\nThe algorithm overview is presented in Fig. 1: the input is\naudio signals of polyphonic music, and the output consists\nof the times of beats and tatums. The implementation of the\nbeat and tatum tracker has been done in C++ programming\nlanguage in the S60 smartphone platform. The algorithm\ndesign is causal and the implementation works in real time.\nThe operation of the system can be described in six stages\n(see Fig. 1):\n1. Resampling stage,\n2. Accent ﬁlter bank stage,\n3. Buffering stage,\n4. Periodicity estimation stage,\n5. Period estimation stage, and\n6. Phase estimation stage.\nFirst, the signal is resampled to a ﬁxed sample rate, to\nsupport arbitrary input sample rates. Second, the accent\nﬁlter bank transforms the acoustic signal of music into a\nform that is suitable for beat and tatum analysis. In this\nstage, subband accent signals are generated, which consti-\ntute an estimate of the perceived accentuation on each sub-\nband. The accent ﬁlter bank stage signiﬁcantly reduces the\namount of data.\nThen, the accent signals are accumulated into four-second\nframes. Periodicity estimation looks for repeating accents\non each subband. The subband periodicities are then com-\nbined, and summary periodicity is computed.\nNext, the most likely beat and tatum periods are esti-\nmated from each periodicity frame. This uses a probabilistic\nformulation of primitive musicological knowledge, includ-\ning the relation, the prior distribution, and the temporal con-\ntinuity of beats and tatums. Finally, the beat phase is found\nand beat and tatum times are positioned. The accent signal\nis ﬁltered with a pair of comb ﬁlters, which adapt to different\nbeat period estimates.Phase \nestimationPeriod \nestimationAccent\nfilter bankBufferingPeriodicity \nestimationAudio \nsignalSubband\naccent \nsignalsAccent \nframesSummary \nperiodicityBeat and \ntatum \nperiodsBeat and \ntatum \ntimes\nResamplingAudio \nsignal\nFigure 1. Beat and tatum analyzer.\nx2LPF(a)\nDiff Comp Rect 0.8\n0.2(b)\nM1 QMF\n26 kHz\nQMFΣ\nQMF\n2 1,5 kHz\nQMFΣ\nQMF\n2\nQMFΣ375 Hz24 kHz\n375 Hz(c)\n125 Hz 125 Hz\na2a1\na3\na4xΣ\n125 Hz\n125 Hz\n125 Hz(b) (c)(b) (c)(b) (c)\nFigure 2. Accent ﬁlter bank overview. (a) The audio signal is ﬁrst divided into subbands, then (b) power estimates on each subband\nare calculated, and (c) accent computation is performed on the subband power signals.\n2.1. Resampling\nBefore any audio analysis takes place, the signal is con-\nverted to a 24 kHz sample rate. This is required because\nthe ﬁlter bank uses ﬁxed frequency regions. The resampling\ncan be done with a relatively low-quality algorithm, linear\ninterpolation, because high ﬁdelity is not required for suc-\ncessful beat and tatum analysis.\n2.2. Accent Filter Bank\nFigure 2 presents an overview of the accent ﬁlter bank. The in-\ncoming audio signal x[n]is (a) ﬁrst divided into subband au-\ndio signals, and (b) a power estimate signal is calculated for\neach band separately. Last, (c) an accent signal is computed\nfor each subband.\nThe ﬁlter bank divides the acoustic signal into seven fre-\nquency bands by means of six cascaded decimating quadra-\nture mirror ﬁlters (QMF). The QMF subband signals are\ncombined pairwise into three two-octave subband signals,\nas shown in Fig. 2(a). When combining two consecutive\nbranches, the signal from the higher branch is decimated\nwithout ﬁltering. However, the error caused by the alias-\ning produced in this operation is negligible for the proposed\nmethod. The sampling rate decreases by four between suc-\ncessive bands due to the two QMF analysis stages and the\nextra decimation step. As a result, the frequency bands are\nlocated at 0–190 Hz, 190–750 Hz, 750–3000 Hz, and 3–\n12 kHz, when the ﬁlter bank input is at 24 kHz.\nThere is a very efﬁcient structure that can be used to im-plement the downsampling QMF analysis with just two all-\npass ﬁlters, an addition, and a subtraction. This structure is\ndepicted in Fig. 5.2-5 in [7, p. 203]. The allpass ﬁlters for\nthis application can be ﬁrst-order ﬁlters, because only mod-\nest separation is required between bands.\nThe subband power computation is shown Fig. 2(b). The\naudio signal is squared, low-pass ﬁltered (LPF), and dec-\nimated by subband speciﬁc factor Mito get the subband\npower signal. The low-pass ﬁlter is a digital ﬁlter having\n10 Hz cutoff frequency. The subband decimation ratios Mi=\n{48,12,3,3}have been chosen so that the power signal sam-\nple rate is 125 Hz on all subbands.\nThe subband accent signal computation in Fig. 2(c) is\nmodelled according to Klapuri et al. [3, p. 344–345]. In the\nprocess, the power signal ﬁrst is mapped with a nonlinear\nlevel compression function labeled Comp in Fig. 2(c),\nf(x) =/braceleftbigg5.213 ln(1 + 10√x), x > 0.0001\n5.213 ln 1 .1 otherwise.(1)\nFollowing compression, the ﬁrst-order difference signal is\ncomputed ( Diff) and half-wave rectiﬁed ( Rect). In accor-\ndance with Eq. (3) in [3], the rectiﬁed signal is summed to\nthe power signal after constant weighting, see Fig. 2(c). The\nhigh computational efﬁciency of the proposed method lies\nmostly in the accent ﬁlter bank design. In addition to efﬁ-\nciency, the resulting accent signals are comparable to those\nof Klapuri et al. , see e.g.Fig. 3 in [3].00.511.522.533.5400.010.02BT(a)\nTime lag [s]\n1234567891000.51B T(b)\nFrequency [Hz]Figure 3. (a) Normalized autocorrelation and (b) summary pe-\nriodicity, with beat (B) and tatum (T) periods shown.\n2.3. Buffering\nThe buffering stage implements a ring buffer which accu-\nmulates the signal into ﬁxed-length frames. The incoming\nsignal is split into consecutive accent signal frames of a ﬁxed\nlength N= 512 (4.1 seconds). The value of Ncan be mod-\niﬁed to choose a different performance–latency tradeoff.\n2.4. Accent Periodicity Estimation\nThe accent signals are analyzed for intrinsic repetitions. Here,\nperiodicity is deﬁned as the combined strength of accents\nthat repeat with a given period. For all subband accent sig-\nnals, a joint summary periodicity vector is computed.\nAutocorrelation ρ[/lscript] =/summationtextN−1\nn=0a[n]a[n−/lscript],0≤/lscript≤\nN−1, is ﬁrst computed from each N-length subband accent\nframe a[n]. The accent signal reaches peak values whenever\nthere are high accents in the music and remains low other-\nwise. Computing autocorrelation from an impulsive accent\nsignal is comparable to computing the inter-onset interval\n(IOI) histogram as described by Sepp ¨anen [5], with addi-\ntional robustness due to not having to discretize the accent\nsignal into onsets.\nThe accent frame power ρ[0]is stored for later weight-\ning of subband periodicities. Offset and scale variations are\neliminated from autocorrelation frames by normalization,\n¯ρ[/lscript] =ρ[/lscript]−minnρ[n]/summationtextN−1\nn=0ρ[n]−Nminnρ[n]. (2)\nSee Fig. 3(a) for an example normalized autocorrelation frame.\nThe ﬁgure shows also the correct beat period B, 0.5 seconds,\nand tatum period T, 0.25 seconds, as vertical lines.\nNext, accent periodicity is estimated by means of the N-\npoint discrete cosine transform (DCT)\nR[k] = ckN−1/summationdisplay\nn=0¯ρ[n] cosπ(2n+ 1)k\n2N(3)\nc0=/radicalbig\n1/N (4)\nck=/radicalbig\n2/N, 1≤k≤N−1. (5)\nCalculate \nfinal \nweight \nmatrixSummary \nperiodicity \nstrength\nUpdate \nbeat and \ntatum \nweights\nPrevious  \nperiod \nestimatesPriors\nBeat & \ntatum \nweightsPeriod \nrelation \nmatrix\nWeight \nmatrixCalculate \nweighted \nperiodicityFind \nmaxima\nWeighted \nperiodicityBeat \nand \ntatum \nperiodsFigure 4. The period estimator.\nSimilarly to an IOI histogram [5], accent peaks with a period\npcause high responses in the autocorrelation function at lags\n/lscript= 0,/lscript=p(nearest peaks), /lscript= 2p(second-nearest peaks),\n/lscript= 3p(third-nearest peaks), and so on. Such response is ex-\nploited in DCT-based periodicity estimation, which matches\nthe autocorrelation response with zero-phase cosine func-\ntions; see dashed lines in Fig. 3(a).\nOnly a speciﬁc periodicity window, 0.1 s≤p≤2 s, is\nutilized from the DCT vector R[k]. This window speciﬁes\nthe range of beat and tatum periods for estimation. The sub-\nband periodicities Ri[k]are combined into an M-point sum-\nmary periodicity vector, M= 128 ,\nS[k] =4/summationdisplay\ni=1ρi[0]γ/tildewideRi[k] 0≤k≤M−1, (6)\nwhere/tildewideRi[k]has interpolated values of Ri[k]from 0.5 Hz\nto10 Hz , and the parameter γ= 1.2controls weighting.\nFigure 3(b) shows an example summary periodicity vector.\n2.5. Beat and Tatum Period Estimation\nThe period estimation stage ﬁnds the most likely beat pe-\nriod ˆτB\nnand tatum period ˆτA\nnfor the current frame at time n\nbased on the observed periodicity S[k]and primitive mu-\nsicological knowledge. Likelihood functions are used for\nmodeling primitive musicological knowledge as proposed\nby Klapuri et al. in [3, p. 344–345], although the actual\ncalculations of the model are different. An overview of the\nperiod estimator are depicted in Fig. 4.\nFirst, weights fi(τi\nn)for the different beat and tatum pe-\nriod candidates are calculated as a product of prior distribu-\ntions pi(τi)and “continuity functions”:\nfi\nC/parenleftbiggτi\nn\nτi\nn−1/parenrightbigg\n=1\nσ1√\n2πexp/bracketleftBigg\n−1\n2σ2\n1/parenleftbigg\nlnτi\nn\nτi\nn−1/parenrightbigg2/bracketrightBigg\n,(7)\nas deﬁned in Eq. (21) in [3, p. 348]. Here, i=Adenotes\nthe tatum and i=Bdenotes the beat. The value σ1= 0.63\nis used. The continuity function describes the tendency that\nthe periods are slowly varying, thus taking care of “tying”\nthe successive period estimates together. τi\nn−1is deﬁned as\nthe median of three previous period estimates. This is found\nto be slightly more robust than just using the estimate from00.511.52\n00.511.5200.20.40.60.8\nBeat period [s ] Tatum period [s]LikelihoodFigure 5. Likelihood of different beat and tatum periods to\noccur jointly.\nthe previous frame. The priors are lognormal distributions\nas described in Eq. (22) in [3, p. 348].\nThe output of the Update beat and tatum weights step in\nFig. 4 are two weighting vectors containing the evaluated\nvalues of the functions fB(τB\nn)andfA(τA\nn). The values\nare obtained by evaluating the continuity functions for the\nset of possible periods given the previous beat and tatum\nestimates, and multiplying with the priors.\nThe next step, Calculate ﬁnal weight matrix , adds in the\nmodelling of the most likely relations between simultaneous\nbeat and tatum periods. For example, the beat and tatum are\nmore likely to occur at ratios of 2, 4, 6, and 8 than in ratios\nof 1, 3, 5, and 7. The likelihood of possible beat and tatum\nperiod combinations τB,τAis modelled with a Gaussian\nmixture density, as described in Eq. (25) in [3, p. 348]:\ng(τB, τA) =9/summationdisplay\nl=1wlN(τB\nτA;l, σ2) (8)\nwhere lare the component means and σ2is the common\nvariance. Eq. (8) is evaluated for the set of M×Mperiod\ncombinations. The weights wlwere hand adjusted to give\ngood performance on a small set of test data. Fig. 5 de-\npicts the resulting likelihood surface g(τB, τA). The ﬁnal\nweighting function is\nh(τB\nn, τA\nn) =/radicalbigg\nfB(τBn)/radicalBig\ng(τBn, τAn)fA(τAn).(9)\nTaking the square root spreads the function such that the\npeaks do not become too narrow. The result is a ﬁnal M×M\nlikelihood weighting matrix Hwith values of h(τB\nn, τA\nn)for\nall beat and tatum period combinations.\nTheCalculate weighted periodicity step weights the sum-\nmary periodicity observation with the obtained likelihood\nweighting matrix H. We assume that the likelihood of ob-\nserving a certain beat and tatum combination is proportional\nto a sum of the corresponding values of the summary peri-\nodicity, and deﬁne the observation O(τB\nn, τA\nn) = ( S[kB] +\nComb filter \n1Time of last \nbeat in the \nprevious frameBeat \nperiod\nPhase \npredictionWeighted \naccentuationPhase \nprediction\nComb filter \n2Previous \nbeat period\nCalculate \nscores for \nphase \ncandidatesCalculate \nscores for \nphase \ncandidates\nPrevious \nbeat \nperiodBeat \nperiodSelect winning \nphase and refine \nbeat period. Store \nwinning Comb \nfilter state.Beat period \nand phasePredicted \nphasePredicted \nphaseFigure 6. The phase estimation stage ﬁnds the phase of the beat\nand tatum pulses, and may also reﬁne the beat period estimate.\nS[kA])/2, where the indices kBandkAcorrespond to the\nperiods τB\nnandτA\nn, respectively. This gives an observation\nmatrix of the same size as our weighting matrix. The ob-\nservation matrix is multiplied pointwise with the weighting\nmatrix, giving the weighted M×Mperiodicity matrix P\nwith values P(τB\nn, τA\nn) = h(τB\nn, τA\nn)O(τB\nn, τA\nn). The ﬁ-\nnal step is to Find the maximum fromP. The indices of\nthe maximum correspond to the beat and tatum period es-\ntimates ˆτB\nn,ˆτA\nn. The period estimates are passed on to the\nphase estimator stage.\n2.6. Beat Phase Estimation\nThe phase estimation stage is depicted in Fig. 6. The tatum\nphase is the same as the beat phase and, thus, only the beat\nphase is estimated. Phase estimation is based on a weighted\nsumv[n] =/summationtext4\ni=1(6−i)ai[n]of the observed subband ac-\ncent signals ai[n],0≤n≤N−1. Compared to Eq. (27) in\n[3, p. 350], the summation is done directly across the accent\nsubbands, instead of resonator outputs.\nA bank of comb ﬁlters with constant half time T0and de-\nlays corresponding to different period candidates have been\nfound to be a robust way of measuring the periodicity in ac-\ncentuation signals [3] [4]. Another beneﬁt of comb ﬁlters\nis that an estimate of the phase of the beat pulse is read-\nily obtained by examining the comb ﬁlter states [4, p. 593].\nHowever, implementing a bank of comb ﬁlters across the\nrange of possible beat and tatum periods is computationally\nvery expensive. The proposed method utilizes the beneﬁts\nof comb ﬁlters with a fraction of the computational cost of\nthe earlier methods. The phase estimator implements two\ncomb ﬁlters. The output of a comb ﬁlter with delay τandgainατfor the input v[n]is given by\nr[n] =ατr[n−τ] + (1 −ατ)v[n]. (10)\nThe parameter τof the two comb ﬁlters is continuously\nadapted to match the current ( ˆτB\nn) and the previous ( ˆτB\nn−1)\nperiod estimates. The feedback gain ατ= 0.5τ/T 0, where\nthe half time T0corresponds to three seconds in samples.\nThe phase estimation starts by ﬁnding a prediction ˆφnfor\nthe beat phase φnin this frame, the step Phase prediction in\nFig. 6. The prediction is calculated by adding the current\nbeat period estimate to the time of the last beat in the previ-\nous frame. Another source of phase prediction is the comb\nﬁlter state, however, this is not always available since the\nﬁlter states may be reset between frames.\nThe accent signal is passed through the Comb ﬁlter 1,\ngiving the output r1[n]. If there are peaks in the accent sig-\nnal corresponding to the comb ﬁlter delay, the output level\nof the comb ﬁlter will be large due to a resonance.\nWe then calculate a score for the different phase candi-\ndates l= 0, . . . , ˆτB−1in this frame. The score is\np[l] =1\n|Il|/summationdisplay\nj∈Ilr1[j] (11)\nwhere Ilis the set of indices {l, l+ ˆτB, l+ 2ˆτB, . . .}be-\nlonging to the current frame, ∀i∈Il: 0≤i≤N−1.\nThe scores are weighted by a function which depends on the\ndeviation of the phase candidate from the predicted phase\nvalue. More precisely, the weight is calculated according to\nEq. (33) in [3, p. 350]:\nw[l] =1\nσ3√\n2πexp/parenleftbigg\n−d[l]2\n2σ2\n3/parenrightbigg\n, (12)\nbut the distance is calculated in a simpler way: d[l] = ( l−\nˆφn)/ˆτB. The phase estimate is the value of lmaximizing\np[l]w[l].\nIf there are at least three beat period predictions avail-\nable and the beat period estimate has changed since the last\nframe, the above steps are mirrored using the previous beat\nperiod as the delay of comb ﬁlter 2. This is depicted by the\nright hand side branch in Fig. 6. The motivation for this is\nthat if the prediction for the beat period in the current frame\nis erroneous, the comb ﬁlter tuned to the previous beat pe-\nriod may indicate this by remaining locked to the previous\nbeat period and phase, and producing a more energetic out-\nput and thus larger score than the ﬁlter tuned to the erro-\nneous current period.\nIn the ﬁnal step, the best scores delivered by both branches\nare compared, and the one giving the largest score deter-\nmines the ﬁnal beat period and phase. Thus, if the comb\nﬁlter branch tuned to the previous beat period gives a larger\nscore, the beat period estimate is adjusted equal to the pre-\nvious beat period. The state of the winning comb ﬁlter is\nstored to be used in the next frame as comb ﬁlter 2.After the beat period and phase are obtained, the beat\nand tatum locations for the current audio frame are inter-\npolated. Although this reduces the ability of the system to\nfollow rapid tempo changes, it reduces the computational\nload since the back end processing is done only once for\neach audio frame.\n3. Implementation\nThe authors have written a real-time implementation of the\nproposed method for the S60 smartphone platform. The im-\nplementation uses ﬁxed-point arithmetic, where all signals\nare represented as 32-bit integers and coefﬁcients as 16-\nbit integers. The power estimation low-pass ﬁlter is imple-\nmented simply as a ﬁrst-order IIR due to the arithmetic used.\nIncreasing the ﬁlter order would have a positive impact on\nperformance, but the given ﬁlter design causes that the co-\nefﬁcients exceed 16-bit dynamic scale. Naturally, the accent\npower compression is realized by a 200-point lookup table.\nTables are used also in the period and phase estimation for\nefﬁciently computing weight function values. The continu-\nity function, the priors, and the likelihood surface shown in\nFig. 5 are stored into lookup tables. Lookup tables are also\nutilized for storing precalculated feedback gain values for\nthe comb ﬁlters. For efﬁciency, both the autocorrelation and\ndiscrete cosine transform processes are implemented on top\nof a fast Fourier transform (FFT).\nFor low-latency real-time implementation, the algorithm\nis split into two execution threads. Referring to Fig. 1, a\nhigh-priority “front-end” thread runs the resampling andac-\ncent ﬁlter bank stages, feeding their results into a memory\nbuffer. The front-end runs synchronously with other au-\ndio signal processing. Periodicity estimation and following\nstages are run in a low-priority “back-end” thread, which is\nsignaled when a new accent frame is available from buffer-\ningstage. The lower priority allows the back-end processing\nto take a longer time without interrupting the audio process-\ning, unlinking audio frame length and accent frame length.\n4. Evaluation\nThe proposed algorithm is evaluated in two aspects, beat\ntracking performance and computational complexity. The\nmethods of Klapuri et al. [3] and Scheirer [4] are used as a\ncomparison, using the original authors’ implementations.1\n4.1. Performance\nThe performance was evaluated by analyzing 192 songs in\nCD audio quality. Songs with a steady beat were selected\nfrom various genres. The majority of songs were rock/pop\n(43%), soul/R&B/funk (18%), jazz/blues (16%), and elec-\ntronic/dance (11%) music, and all except two songs were in\n4/4 meter. The beats of approximately one minute long song\n1We wish to thank Anssi Klapuri and Eric Scheirer for making\ntheir algorithm implementations available for the comparison.Table 1. Beat tracking accuracy scores.\nContinuity required Individual estimates\nMethod Correct Accept d/h Period Correct Accept d/h Period\nProposed 60% 70% 76% 64% 76% 79%\nKlapuri 66% 76% 73% 72% 85% 81%\nScheirer 29% 34% 30% 53% 65% 59%\nexcerpts were annotated by tapping along with the song play-\ning. The evaluation methodology followed the one proposed\nin [3], assessing both the period and phase estimation accu-\nracy of the proposed method. A correct period estimate is\ndeﬁned to deviate less than 17.5% from the annotated refer-\nence, and the correct phase to deviate less than 0.175 times\nthe annotated beat time. The following scores were calcu-\nlated and averaged over the duration of the excerpts and over\nall 192 songs:\n•Correct: Beat estimate with correct period and phase.\n•Accept d/h: Beat estimate with period matching either\n0.5, 1.0, or 2.0 times the correct value, and correct\nphase.\n•Period: Beat estimate with correct period, phase is\nignored.\nWe calculated the scores for both the longest continuous\ncorrectly analyzed segment and individual estimates without\ncontinuity requirement. For comparison, the methods pro-\nposed in [3] and [4] were run on the same data. The results\nare shown in Table 1. In summary, the proposed method ap-\nproaches the Klapuri et al. method performance in all of the\ncases. The biggest deviations are in the Scheirer method\nscores with continuity requirement, reﬂecting the lack of\nbeat period smoothing in the Scheirer method.\n4.2. Complexity\nWe compared the computational complexity of the three al-\ngorithms on a PC having 1.86 GHz Pentium M processor\nand 1 GB of memory. The proposed and Scheirer methods\nwere implemented in C++ in ﬂoating point and compiled\nwith the same compiler settings; function inlining intrinsics\nwere added into Scheirer’s original algorithm. The Klapuri\nmethod is a combination of MATLAB and C++ code.\nA 300-second audio clip was processed ﬁve times with\neach of the three methods and the algorithm CPU time was\nmeasured (excluding ﬁle access and decoding). The median\nCPU cycles of the ﬁve runs are shown in Table 2, divided by\n106(Mcycles), and normalized with audio clip length (Mcy-\ncles/s). The Klapuri method is not strictly comparable to the\nothers because it is mostly MATLAB processing: 61% of\nthe CPU is used in MATLAB code. The Scheirer method\ncycles break down into 82% for comb ﬁltering and 13% forTable 2. Processor usage proﬁles.\nMethod Mcycles Mcycles/s\nProposed 678 2.3\nKlapuri (MATLAB) 125000 420\nScheirer 136000 450\nScheirer without malloc etc. 119000 390\nruntime functions (e.g. malloc ). A second Scheirer pro-\nﬁle in Table 2 has the runtime functions subtracted. The\nproposed algorithm is found over 170 times more efﬁcient.\nWe also evaluated the computational complexity of the\nproposed method on a Nokia 6630 smartphone having a\n220 MHz ARM9 processor. An instruction proﬁler was\nconﬁgured to sample the processor program counter on a\n1 kHz rate, yielding 302500 data points in total. During\nplayback, 13% of processor time was spent in the beat and\ntatum tracker implementation and 8% in MP3 format de-\ncoding. The proﬁle shows the algorithm to perform very ef-\nﬁciently, comparable to the complexity of the MP3 decoder.\n5. Conclusion\nA beat and tatum tracker algorithm can be made computa-\ntionally very efﬁcient without compromising beat tracking\nperformance. We introduced a novel beat and tatum tracker\nfor music signals, consisting of multirate accent analysis,\ndiscrete cosine transform periodicity analysis, and phase es-\ntimation by adaptive comb ﬁltering. The complexity of the\nproposed method is less than 1% of Scheirer’s method, and\nits beat tracking accuracy approaches Klapuri’s method. The\nauthors have created a real-time implementation of the pro-\nposed method for the S60 smartphone platform.\nReferences\n[1] J.A. Bilmes. “Timing is of the Essence: Perceptual and Com-\nputational Techniques for Representing, Learning, and Re-\nproducing Expressive Timing in Percussive Rhythm.” M.Sc.\nThesis, Massachusetts Institute of Tech., Sep. 1993.\n[2] F. Gouyon and S. Dixon. “A review of automatic rhythm de-\nscription systems.” Comp. Music J. , 29(1):34–54, 2005.\n[3] A.P. Klapuri, A.J. Eronen, and J.T. Astola. “Analysis of\nthe meter of acoustic musical signals.” IEEE Trans. Audio,\nSpeech, and Lang. Proc. , 14(1):342–355, Jan. 2006.\n[4] E.D. Scheirer. “Tempo and beat analysis of acoustic musical\nsignals.” J. Acoust. Soc. Am. , 103(1):588–601, Jan. 1998.\n[5] J. Sepp ¨anen. “Tatum grid analysis of musical signals.” In\nProc. IEEE Workshop on Applic. of Signal Proc. to Audio\nand Acoust. (WASPAA) , pp. 131–134, New Paltz, NY, USA,\nOct. 2001.\n[6] C. Uhle, J. Rohden, M. Cremer, and J. Herre. “Low com-\nplexity musical meter estimation from polyphonic music.”\nInProc. AES 25th Int. Conf. , London, UK, 2004.\n[7] P.P. Vaidyanathan. Multirate Systems and Filter Banks.\nPrentice-Hall, Upper Saddle River, NJ, USA, 1993."
    },
    {
        "title": "Mel Frequency Cepstral Coefficients: An Evaluation of Robustness of MP3 Encoded Music.",
        "author": [
            "Sigurdur Sigurdsson",
            "Kaare Brandt Petersen",
            "Tue Lehn-Schiøler"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1417149",
        "url": "https://doi.org/10.5281/zenodo.1417149",
        "ee": "https://zenodo.org/records/1417149/files/SigurdssonPL06.pdf",
        "abstract": "In large MP3 databases, files are typically generated with different parameter settings, i.e., bit rate and sampling rates. This is of concern for MIR applications, as encoding dif- ference can potentially confound meta-data estimation and similarity evaluation. In this paper we will discuss the in- fluence of MP3 coding for the Mel frequency cepstral coe- ficients (MFCCs). The main result is that the widely used subset of the MFCCs is robust at bit rates equal or higher than 128 kbits/s, for the implementations we have investi- gated. However, for lower bit rates, e.g., 64 kbits/s, the im- plementation of the Mel filter bank becomes an issue. Keywords: Mel frequency cepstral coefficients, MFCC, ro- bustness, MP3.",
        "zenodo_id": 1417149,
        "dblp_key": "conf/ismir/SigurdssonPL06",
        "keywords": [
            "MP3 databases",
            "different parameter settings",
            "MIR applications",
            "encoding difference",
            "meta-data estimation",
            "similarity evaluation",
            "Mel frequency cepstral coefficients (MFCCs)",
            "robustness",
            "bit rates",
            "lower bit rates"
        ],
        "content": "Mel Frequency Cepstral Coefﬁcients: An Evaluation of Robustness of MP3\nEncoded Music\nSigurdur Sigurdsson, Kaare Brandt Petersen and Tue Lehn-Schi øler\nInformatics and Mathematical Modelling\nTechnical University of Denmark\nRichard Petersens Plads - Building 321\nDK-2800 Kgs. Lyngby - Denmark\n{siggi,kbp }@imm.dtu.dk\nAbstract\nIn large MP3 databases, ﬁles are typically generated with\ndifferent parameter settings, i.e., bit rate and sampling r ates.\nThis is of concern for MIR applications, as encoding dif-\nference can potentially confound meta-data estimation and\nsimilarity evaluation. In this paper we will discuss the in-\nﬂuence of MP3 coding for the Mel frequency cepstral coe-\nﬁcients (MFCCs). The main result is that the widely used\nsubset of the MFCCs is robust at bit rates equal or higher\nthan 128 kbits/s, for the implementations we have investi-\ngated. However, for lower bit rates, e.g., 64 kbits/s, the im -\nplementation of the Mel ﬁlter bank becomes an issue.\nKeywords: Mel frequency cepstral coefﬁcients, MFCC, ro-\nbustness, MP3.\n1. Introduction\nThe use of Mel frequency cepstral coefﬁcients (MFCCs) for\nmusic information retrieval has become standard since the\nseminal paper [4] in 1997. But only little effort has been put\ninto investigating the applicability of the MFCC’s as fea-\ntures for music, with [6] as a rare exception. In this paper\nwe investigate how MP3 encoding of music ﬁles is inﬂuenc-\ning the signal information content of the MFCC’s.\n2. Mel Frequency Cepstral Coefﬁcients\nWe will use the Intelligent sound implementation (ISP) to\nexplain the computation of MFCCs. First the music signal\nis divided into short time windows, where we compute the\ndiscrete Fourier transform (DFT) of each time window for\nthe discrete-time signal x(n)with length N, given by\nX(k) =N−1/summationdisplay\nn=0w(n)x(n)exp( −j2πkn/N ) (1)\nfork= 0,1,... ,N −1, where kcorresponds to the fre-\nquency f(k) =kfs/N,fsis the sampling frequency in\nPermission to make digital or hard copies of all or part of this w ork for\npersonal or classroom use is granted without fee provided th at copies\nare not made or distributed for proﬁt or commercial advantage an d that\ncopies bear this notice and the full citation on the ﬁrst page .\nc/circlecopyrt2006 University of VictoriaHertz and w(n)is a time-window. Here, we chose the popu-\nlar Hamming window as a time window, given by\nw(n) = 0.54−0.46cos( πn/N ), due to computational sim-\nplicity.\nThe magnitude spectrum |X(k)|is now scaled in both\nfrequency and magnitude. First, the frequency is scaled log -\narithmically using the so-called Mel ﬁlter bank H(k,m)and\nthen the logarithm is taken, giving\nX′(m) = ln/parenleftBiggN−1/summationdisplay\nk=0|X(k)| ·H(k,m)/parenrightBigg\n(2)\nform= 1,2,... ,M , where Mis the number of ﬁlter banks\nandM≪N. The Mel ﬁlter bank is a collection of triangu-\nlar ﬁlters deﬁned by the center frequencies fc(m), written\nas\nH(k, m) =\n/BK/BQ/BQ/BQ/BO/BQ/BQ/BQ/BM0 for f(k)< fc(m−1)\nf(k)−fc(m−1)\nfc(m)−fc(m−1)forfc(m−1)≤f(k)< fc(m)\nf(k)−fc(m+1)\nfc(m)−fc(m+1)forfc(m)≤f(k)< fc(m+ 1)\n0 for f(k)≥fc(m+ 1).\n(3)\nThe center frequencies of the ﬁlter bank are computed by\napproximating the Mel scale with\nφ= 2595log10(f\n700+ 1), (4)\nwhich is a common approximation. Note that this equation\nis non-linear for all frequencies. Then a ﬁxed frequency res -\nolution in the Mel scale is computed, corresponding to a log-\narithmic scaling of the repetition frequency, using\n∆φ= (φmax−φmin)/(M+ 1) where φmaxis the highest\nfrequency of the ﬁlter bank on the Mel scale, computed from\nfmax using equation (4), φminis the lowest frequency in\nMel scale, having a corresponding fmin, andMis the num-\nber of ﬁlter banks. The values for the ISP implementation is\nfmax= 11.025kHz,fmin= 0Hz, and M= 30 . The center\nfrequencies on the Mel scale are given by φc(m) =m·∆φ\nform= 1,2,... ,M . To obtain the center frequencies\nin Hertz, we apply the inverse of equation (4), given by\nfc(m) = 700(10φc(m)/2595−1), which are inserted into\nequation (3) to give the Mel ﬁlter bank. Finally, the MFCCs\nare obtained by computing the DCT of X′(m)using\nc(l) =M/summationdisplay\nm=1X′(m)cos(lπ\nM(m−1\n2)) (5)0 2000 4000 6000 8000 10000 1200000.20.40.60.81\nf (Hz)AmplitudeISP0 2000 4000 6000 800000.20.40.60.81\nf (Hz)AmplitudeHTK\n0 1000 2000 3000 4000 500000.20.40.60.81\nf (Hz)AmplitudeDavis\n0 2000 4000 6000 800000.0050.010.015\nf (Hz)AmplitudeAuditory toolbox\nFigure 1. The ﬁgure shows 4 different implementations of the\nMel ﬁlter bank. Note the different scaling of the frequency axes\nin the plots.\nforl= 1,2,... ,M , where c(l)is thelth MFCC.\nIn this paper we will focus on 4 different implementa-\ntions of the MFCCs; the algorithm due to Davis [2], the Au-\nditory toolbox [8], the hidden Markov model toolkit (HTK)\n[9], and the ISP implementation given above. The imple-\nmentations have different Mel ﬁlter banks, shown in Fig-\nure 1. Note the different characteristics of the ﬁlter banks .\nDavis’ implementation has linear spacing up to 1 kHz and\nthen logarithmic spacing, where the ﬁlter amplitude is con-\nstant. HTK has logarithmic spacing and constant amplitude.\nThe Auditory toolbox suppresses frequencies below approx-\nimately 133 Hz, has linear spacing up to 1 kHz and then\nlogarithmic spacing, where the energy in all ﬁlters is ﬁxed\nto unity. The ISP implementation is similar to HTK, us-\ning the same deﬁnition of the Mel ﬁlter bank with different\nnumber of ﬁlters and ﬁlter center frequencies. Also, the ISP\nimplementation does not use liftering.\n3. MP3 Encoding\nThe compression used for MP3 ﬁles is based on perceptual\nencoding, where the goal is to apply efﬁcient coding while,\nat the same time, obtaining a perceptually good coding of\nthe signal. The main building blocks of an MP3 encoder\nare: An analysis ﬁlter bank which decomposes the signal\ninto subsampled spectral bands, a perceptual model which\ncontrols the quantization and coding scheme for the decom-\nposed signal, and ﬁnally a bitstream coding. It is the per-\nceptual model that determines the quality of the signal, as\ncompression is obtained by adapting the amount of quanti-\nzation noise, based on the amplitude and frequency content\nof the signal. Despite of this advanced scheme for coding\nthe music signals, some artifacts are encountered. The most\ncommon is pre-echo where a noise signal is observed be-fore the music signal that actually causes the noise. This\nis due to the temporal resolution of the decoder, given by\nthe synthesis window length, where the quantization error\nis distributed over the full window. Thus, a sudden signal\nattack increases the quantization error, which includes th e\nmusic signal before the attack. Another artifact is the loss\nof signal bandwidth when the encoder runs out of bits for\na given quality of the signal. For an introduction to MP3\ncoding, see e.g. [1].\nIn this paper we have used the LAME 3.96.1 encoder,\nwhich is very popular and often acclaimed being the best en-\ncoder for bit rates at 128 kbit/s or higher. We have used the\npopular Madplay 0.15.0 (beta) for decoding the MP3 ﬁles.\nThe choice of encoder/decoder were based on their popular-\nity and that they are freely available. The encoder speciﬁ-\ncations for the experiments were; stereo mode, variable bit\nrates at 64, 128 and 320 kbit/s, sampling rate of 44.1 and\n22.05 kHz. The most commonly used bit rate is 128 kbit/s,\nwhere both good compression and reasonable sound quality\nmay be obtained. The 64 and 320 kbit/s are used to show\nresults at very low and good quality. The reason to use a\nlower sampling rate than 44.1 kHz is to show improvement\nin quality at low bit rate.\n4. Evaluating Robustness with Correlation\nIn order to evaluate the effect of different MFCC approaches\nand different MP3 encodings, we need a measure of dif-\nference. We have chosen the so-called Pearson’s correla-\ntion coefﬁcient to compare MFCCs. By using this sim-\nple scheme, we avoid selecting a classiﬁer for a speciﬁc\nMIR task and choosing a temporal coding scheme for the\nMFCCs, e.g. Gaussian mixture model.\nThe Pearson’s correlation coefﬁcient rxyfor two vari-\nablesxandy, is a measure of the correlation between them\ngiven a linear model and Gaussian noise [3]. Here we will\nuse the squared correlation r2\nxy, which indicates the percent-\nage of variation in the data that can be explained with the\nlinear model. For r2\nxy= 1the relation is exact, and as r2\nxy\nbecomes smaller, the relation becomes weaker.\nIt is well known that Pearson’s correlation coefﬁcient\nshould be used as a measure of regression rather than corre-\nlation, and in the case of the MFCCs we are doing exactly\nthat: Estimating the noise variance under the linear assump -\ntion. To be sure that the assumption about the linear rela-\ntion and Gaussian noise is not too restrictive, we conduct a\nKolmogorov-Smirnov test (KS-test) on the noise residuals,\nsee e.g. [7] for details.\n5. Experiments\nAll experiments were conducted using a data set of 46 songs\nfrom 46 different rock and pop artists. WA V ﬁles were gen-\nerated from compact disks using CDex 1.51. MP3 ﬁles were\ngenerated from the WA V ﬁles using the LAME encoder. To\navoid noise due to time difference between the WA V andDavis\nHTK5 10 15 20 255\n10\n15\n20Davis\nISP5 10 15 20 25 305\n10\n15\n20\nHTK\nAuditory toolbox10 20 30 405\n10\n15\n20\n25HTK\nISP5 10 15 20 25 305\n10\n15\n20\n25\nAuditory toolbox\nISP5 10 15 20 25 3010\n20\n30\n40Davis\nAuditory toolbox10 20 30 405\n10\n15\n2000.20.40.60.81\nFigure 2. The ﬁgure shows the squared Pearson’s correlation\ncoefﬁcient ( r2) between single MFCCs for the 4 selected im-\nplementations, where the values on the axes indicates MFCC\nnumber. Note that the images are different in size, due to dif-\nferent number of MFCCs for each implementation.\nMP3 ﬁles, the signals were aligned in time prior to MFCC\ncomputation. Various window sizes are suggested to com-\npute MFCCs, ranging from 5-100 ms and often around 20\nms, with overlap 30-50 %. On the basis of this, the MFCCs\nfor the songs were computed using a ﬁxed window size of 20\nms with 50 % overlap. As the music ﬁles contain stereo mu-\nsic, we generate a single channel signal by averaging over\nboth channels prior to MFCC computation.\n5.1. MFCC Implementations\nThe implementation comparison used only WA V ﬁles for\nevaluation. MFCCs were computed for each song for all 4\nimplementations. The squared Pearson’s correlation coefﬁ -\ncientr2was computed between all MFCCs for all methods\nand for each song. The result shown in ﬁgure 2 is the aver-\nage over all songs. From the ﬁgure we observe that approxi-\nmately the ﬁrst 15 MFCCs are quite correlated between im-\nplementations. This varies somewhat between implemen-\ntations, e.g. the HTK and ISP are very correlated as they\nare based on the same implementation of the Mel ﬁlter bank\nwith different speciﬁcations. In practical applications o nly\nthe ﬁrst 5-15 MFCCs are in general used, which could ex-\nplain similar performances using different implementatio ns.\nFor instance, investigations of different MFCC implementa -0 10 20 30 4000.20.40.60.81\nMFCC numberr2Davis\nHTK\nAuditory toolbox\nISP\n0 10 20 30 4000.20.40.60.81\nMFCC numberr2HTK\nDavis\nAuditory toolbox\nISP\n0 10 20 30 4000.20.40.60.81\nMFCC numberr2Auditory toolbox\nDavis\nHTK\nISP\n0 10 20 30 4000.20.40.60.81\nMFCC numberr2ISP\nDavis\nHTK\nAuditory toolbox\nFigure 3. The ﬁgure shows the squared Pearson’s correlation\ncoefﬁcient ( r2) where each MFCC of one implementation (title\nof plot) is conditioned on all the MFCCs for the other imple-\nmentations (legend of plot).\ntion schemes for speaker veriﬁcation have shown very sim-\nilar results [5]. The MFCCs above approximately 15, have\nlower r2and become more diffused, as information spreads\nout to neighboring MFCCs.\nIt should be noted that the assumption of the relation be-\ntween MFCCs from different implementations are modeled\nlinearly with Gaussian noise is highly unlikely. This is due\nto the fact that each MFCC implementation is a highly non-\nlinear process. On the other hand, high r2means that much\nof relation may be explained with the linear model, while\nthe noise is not Gaussian distributed. This was conﬁrmed\nwith the KS-test.\nThe results shown in ﬁgure 2 may be conﬁrmed by com-\nputing the r2between a single MFCC conditioned on all\nMFCCs from other implementations. Figure 3 shows the re-\nsults for all implementations. The ﬁgure shows that the r2\nis approximately 0.8 or higher for MFCCs up to 15 for all\nimplementations. Again it should be noted that the KS-test\nrejects in many cases the hypothesis of a linear model with\nGaussian noise, although the r2is high.\n5.2. MFCC Robustness to MP3 Coding\nThe inﬂuence of MP3 coding was evaluated by computing\nthe MFCCs for WA V and MP3 ﬁles at different bit rates and\nsample rates, and then evaluating the squared Pearson’s cor -\nrelation coefﬁcient r2between the WA V generated MFCCs\nand the MP3 generated MFCCs. The KS-test accepted in al-\nmost all cases the hypothesis of a linear relation with Gauss ian\nnoise. The results are shown in ﬁgure 4. At a ﬁxed sam-\npling rate of 44.1 kHz and bit rate of 320 kbits/s the r2\nbetween WA V and MP3 MFCCs are approximately 1, in-\ndicating little or no loss. At 128 kbits/s, r2drops similarly0 10 20 30 400.70.750.80.850.90.951\nMFCC numberr244100 samp/s | 320 kbits/s\nDavis\nHTK\nAuditory toolbox\nISP\n0 10 20 30 400.70.750.80.850.90.951\nMFCC number44100 samp/s | 128 kbits/sr2\n0 10 20 30 400.70.750.80.850.90.951\nMFCC numberr244100 samp/s | 64 kbits/s\n0 10 20 30 400.70.750.80.850.90.951\nMFCC number22050 samp/s | 64 kbits/sr2\nFigure 4. The squared Pearson’s correlation coefﬁcient ( r2) as\na function of MFCC number for the 4 MFCC implementations,\nusing different sampling rate and bit rate.\nfor all implementations, but is higher than approximately\n0.95 for the ﬁrst 15 MFCCs. Interestingly, r2is dependent\non the MFCC number, showing that higher MFCCs have\nlower sample correlation, indicating that they are less rob ust\nto MP3 encoding of music. At 64 kbits/s the sample corre-\nlation has decreased signiﬁcantly and is now dependent on\nimplementations. The largest single factor is the highest f re-\nquency included in the Mel ﬁlter bank. The most robust im-\nplementation is Davis’ with the highest frequency 4.6 kHz,\nwhile the least robust is the ISP implementation with high-\nest frequency 11.025 kHz. The HTK and Auditory toolbox\nimplementations are in between the other two, having the\nhighest included frequency of 8 kHz and 6.9 kHz.\nFigure 4 shows also that it is possible to improve the ro-\nbustness by reducing the sample rate from 44.1 kHz to 22.05\nkHz. This is due to the MP3 encoding, where higher fre-\nquencies are more expensive to code and deviate more from\nthe original. Thus, by disregarding higher frequencies, bo th\nby removing higher frequencies in the Mel ﬁlter bank im-\nplementation and reducing the sampling rate, more robust\nMFCCs are obtained.\n6. Conclusion\nIn this paper we have evaluated the robustness of MFCCs\nwith the squared Pearson’s correlation coefﬁcient. The re-\nsults show that the different MFCC implementations are very\ncorrelated for approximately the ﬁrst 15 MFCCs. This sup-\nports experiments for speaker veriﬁcation [5], showing sim -\nilar performance for different MFCC implementations and\nsettings.\nMFCCs were shown to be very robust at bit rates of 320\nand 128 kbit/s for all implementations at a ﬁxed sampling\nrate of 44.1 kHz. At 64 kbits/s, using the same samplingrate, the implementations are less robust and the robustnes s\nis dependent on implementation. The robustness decayed\nmore rapidly for implementations that included higher fre-\nquencies in the Mel ﬁlter bank. Also, we showed that the ro-\nbustness at lower bit rates, e.g. 64 kbits/s, may be improved\nby reducing the sampling rate, especially for implementa-\ntions that included higher frequencies in the Mel ﬁlter bank .\nFinally, we illustrated that higher order MFCCs are less ro-\nbust than lower order for MP3 encoding.\nThis paper shows that MFCC features are very robust to\nMP3 encoding and thus applicable in MIR tasks. However,\nthe MFCC implementation should take into account the en-\ncoding distortion in MP3 ﬁles at low bit rates.\n7. Acknowledgements\nThis work is supported by the Danish Technical Research\nCouncil, through the framework project ’Intelligent Sound ’,\nwww.intelligentsound.org (STVF No. 26-04-0092). We\nthank Anders Meng, Jan Larsen and Lars Kai Hansen for\ndiscussions and comments.\nReferences\n[1] Karlheinz Brandenburg. MP3 and AAC explained. In AES\n17th International Conference on High Quality Audio Cod-\ning, 1999.\n[2] Steven B. Davis and Paul Mermelstein. Comparison of\nparametric representations for monosyllabic word recogni-\ntion in continuously spoken sentences. IEEE Transactions\non Acoustics, Speech and Signal Processing , 28(4):357–366,\n1980.\n[3] Allen L. Edwards. An introduction to linear regression and\ncorrelation . W. H. Freeman and Company, 1976.\n[4] J. Foote. Content-based retrieval of music and audio. In\nMultimedia Storage and Archiving Systems II, Proc. of SPIE ,\nvolume 3229, pages 138–147, 1997.\n[5] Todor Ganchev, Nikos Fakotakis, and George Kokkinakis.\nComparative evaluation of various MFCC implementations\non the speaker veriﬁcation task. In Proceedings of the\n10th International Conference on Speech and Computer\n(SPECOM 2005) , volume 1, pages 191–194, 2005.\n[6] Beth Logan. Mel frequency cepstral coefﬁcients for music\nmodeling. In Proceedings of International Symposium on\nMusic Information Retrieval (ISMIR) , 2000.\n[7] William H. Press, Brian P. Flannery, Saul A. Teukolsky, and\nWilliam T. Vetterling. Numerical Recipes in C : The Art of\nScientiﬁc Computing . Cambridge University Press, Chapter\n14, pp. 623–628, 2nd edition, 2002.\n[8] Malcolm Slaney. Auditory toolbox, version 2. Technical\nReport #1998-010, Interval Research Corporation, 1998.\n[9] Steve Young, Gunnar Evermann, Dan Kershaw, Gareth\nMoore, Julian Odell, Dave Ollason, Dan Povey, Valtcho\nValtchev, and Phil Woodland. The HTK book (for version\n3.2). Cambridge University Engineering Department, De-\ncember 2002."
    },
    {
        "title": "Lilypond for pyScore: Approaching a universal translator for music notation.",
        "author": [
            "Stephen Sinclair",
            "Michael Droettboom",
            "Ichiro Fujinaga"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1418245",
        "url": "https://doi.org/10.5281/zenodo.1418245",
        "ee": "https://zenodo.org/records/1418245/files/SinclairDF06.pdf",
        "abstract": "Several languages for music notation have been defined in recent years. pyScore, a framework for translating between notation formats, and new module for it which can generate input for the LilyPond music engraving system are described. This shows the potential for developing pyScore into a “universal translator” for musical scores. Keywords: Notation, score, engraving, translation, representation.",
        "zenodo_id": 1418245,
        "dblp_key": "conf/ismir/SinclairDF06",
        "keywords": [
            "Music Notation",
            "pyScore",
            "LilyPond",
            "Music Engraving",
            "Translation Framework",
            "Digital Archiving",
            "Score Editing",
            "MusicXML",
            "GUIDO",
            "Notation Representation"
        ],
        "content": "Lilypond for pyScore: Approaching a universal translator for music notation\nStephen Sinclair\nSchulich School of Music\nMcGill University\nMontreal, Quebec\nsinclair@music.mcgill.caMichael Droettboom\nformerly of Johns Hopkins University\n3400 North Charles Street\nBaltimore, Maryland, USA 21218\nmike@droettboom.comIchiro Fujinaga\nSchulich School of Music\nMcGill University\nMontreal, Quebec\nich@music.mcgill.ca\nAbstract\nSeveral languages for music notation have been deﬁned\nin recent years. pyScore, a framework for translating\nbetween notation formats, and new module for it which can\ngenerate input for the LilyPond music engraving system are\ndescribed. This shows the potential for developing pyScore\ninto a “universal translator” for musical scores.\nKeywords: Notation, score, engraving, translation,\nrepresentation.\n1. Introduction\nRecent years have seen a large increase in the use\nof computers for representing musical notation. This\nis due to an increased interest in digital archiving of\nmusical scores, computer typesetting, score editing, optical\nmusic recognition, web-based distribution of music, and\ninterchange between the variety of available software\nprograms.\nTwo forerunners have most recently been touted as\nsolutions for distribution and interchange. These are\nRecordare’s MusicXML [3], an “Internet-friendly” XML-\nbased format, and GUIDO, a “representationally adequate”\nand “human readable” text format [4]. They are both\ndesigned and built on previous work, including, but not\nlimited to, the binary NIFF format, SMDL, and Humdrum’s\n**kern [5].\nAnother contender is a free and open-source music\nengraving system called LilyPond [6]. Its input format is\nintended more as a description of visual layout than for data\ninterchange, but due to the professional quality of its output,\nsome have begun to use its input format to store musical\ndata.\nOne reason for the encouraging support it has obtained\nis that its input format is similar in many ways to\nthe popular L ATEX document typesetting software. The\ngrammar is highly ﬂexible and quite easy to read and\nwrite. For example, musical expressions can be deﬁned\nas “commands,” to be later be recalled in one or more\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies\nare not made or distributed for proﬁt or commercial advantage and that\ncopies bear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2006 University of Victoriacontexts. Different instrument voices can then be deﬁned\nseparately and speciﬁed in a more speciﬁc layout context\nlater in the document. Also, any variable used for control\nof its Scheme-based layout system can be overridden from\nwithin the input script. Finally, many special cases are\nsupported for instruments with exceptional notation, such\nas bagpipes or percussion, as well as ancient notation and\nspecial constructs for contemporary music.\nBecause it is free and very extensible, it can be used as a\ngood solution for displaying results from MIR transactions.\nAn easy way to generate its input format for automated\nsystems would be highly desirable. In this paper, we discuss\na software framework for translating musical scores. In\naddition to its previous support for MusicXML and GUIDO,\nwe have created a new module for generating input to\nLilyPond.\n2. Translating notation with pyScore\nBecause of the multiplicity of formats that exist, it is\nimpossible to expect every software package to know how\nto parse them all. Instead, it is important to be able to\nindependently translate foreign formats to familiar ones,\nbridging otherwise incompatible programs. Additionally,\nstoring musical information in open formats ensures data\nlongevity. Translation can allow for this, while ensuring\ncompatibility with a wide array of software tools.\nWhile many programs exist for speciﬁc translations\nbetween particular formats, a more global approach to this\nproblem is taken by pyScore, a framework developed in the\nPython programming language [1]. Currently, it can work\nwith MusicXML and GUIDO. It is able to read them into\ninternal tree structures which can then be translated from\none to the other, or into MidiXML, which can in turn output\nMIDI data as a ﬁle or a stream. It can also render GUIDO\nto image ﬁles by calling the online NoteServer.\nThe framework is structured so that it is quite simple to\nadd support for new formats. When a module is added,\npyScore is able to ﬁnd a path from whatever input format is\ngiven to the new module. Thus, given the ability to translate\na GUIDO tree into a LilyPond tree, it is automatically able\nto also perform the internal conversion from MusicXML\nto GUIDO if necessary. This feature, in addition to the\nclear and modular programming framework, and combined\nwith the advantages of using Python (such as cross-platformFigure 1. Conversion graph for pyScore, with LilyPond\nmodule added.\ncompatibility, and readability of the source), make pyScore\na particularly well-adapted platform for building a universal\nscore converter.\nWe have created a LilyPond module for pyScore. The\nnew conversion graph is shown in Figure 1. Currently it\nsupports conversion toLilyPond. It can handle the complete\ndeﬁnition of the “Basic” subset of GUIDO, meaning that\nit supports most important musical features, but currently\nlacks much of the layout information that is possible to\nspecify using GUIDO’s “Advanced” counterpart.\n3. Representational differences\nThere are inevitably some choices that must be made when\nformalizing an abstract concept like music. Some of these\nchoices are human considerations, to increase clarity and\nto ease manual editing, while others make fundamental\ndifferences in how the computer must make assumptions\nor numerically handle information. Different systems will\ninvariably choose different ways of encoding structural\nrelationships that exist within music at every level of\nabstraction [2].\nAs a short example, consider the seemingly simple\nconstruct of a tuplet. In a triplet, each note has a duration\nof2\n3of its normal duration. GUIDO represents this by\nspecifying each note with its absolute duration, multiplying\nby a fraction if necessary:\n[c*1/3 c *1/3 c *1/3] % Half-notes, 1 bar\n[c*1/6 c *1/6 c *1/6] % Quarter-notes,1\n2bars\nThis is representationally simple, but does not explicitly\ndeﬁne the relationship between the notes. In contrast,\nLilyPond chooses to specify normal durations, and multiply\nthe group of notes by a fraction:\n\\times 2/3 {c’2 c’ c’ }% Half-notes, 1 bar\n\\times 2/3 {c’4 c’ c’ }% Quarter-notes,1\n2bars\nThis difference implies that when reading GUIDO, the\ntranslator must detect groups of notes to be considered\ntuplets, since they are not already grouped in the input ﬁle.This is done by checking each note duration to see if it is an\neven power of two, and if not, collecting them until the total\nduration of the group matches this criteria.\nDesign choices such as these make it often impossible\nto derive a universal solution for translating symbolic\ninformation. pyScore approaches this by performing\nspeciﬁc translations from one language to another in series,\nrather than trying to deﬁne a one-size-ﬁts-all internal\nrepresentation. While long chains of translations may be\nlossier than necessary, this means individual translations can\nbe optimized to reduce loss of information.\n4. Conclusion\npyScore is a useful framework for building a notation\ntranslator. By using it to convert from GUIDO to LilyPond,\nwe were able to take advantage of a ready-made GUIDO\nparser. As a bonus, the software is automatically able\nto translate MusicXML into LilyPond, by discovering the\nappropriate intermediate conversion. This allowed us to\nconcentrate solely on LilyPond’s representation, without\nhaving to track differences between three grammars.\nFuture work will support more of the Advanced GUIDO\nfeatures. Additionally, a parser for the LilyPond format\nshould be added, to enable conversion from LilyPond. This\nmay present a more difﬁcult task, because of the ﬂexibility\nof the LilyPond language, and its escape mechanism for the\nsoftware’s internal Scheme interpreter.\nIt would be beneﬁcial to add yet more format support\nto pyScore, to eventually achieve an open-source “universal\ntranslator” for music.\n5. Acknowledgments\nWe are grateful to National Science Foundation, Institute for\nMuseum and Library Services, the Lester S. Levy Family,\nand Canadian Foundation for Innovation for their ﬁnancial\nsupport.\nReferences\n[1] pyScore [Software], 2006 Apr 24; available from:\nhttp://pyscore.sf.net/ .\n[2] R. Dannenberg, “Music representation: Issues, techniques,\nand systems,” in Computer Music Journal , vol. 17, no. 3, pp.\n20–30, 1993.\n[3] M. Good and G. Actor, “Using MusicXML for\nFile Interchange,” in Proceedings Third International\nConference on WEB Delivering of Music , 2003, p. 153.\n[4] H. Hoos, K. Hamel, and K. Renz, “Using Advanced GUIDO\nas a Notation Interchange Format,” in Proceedings of the\nInternational Computer Music Conference , 1999.\n[5] D. Huron, “Music information processing using the\nHumdrum toolkit: Concepts, examples, and lessons,” in\nComputer Music Journal , vol. 26, no. 2, pp. 11–26, 2002.\n[6] H.-W. Nienhuys and J. Nieuwenhuizen, “Lilypond, a system\nfor automated music engraving,” in Proceedings of the XIV\nColloquium on Musical Informatics , 2003."
    },
    {
        "title": "Ground truth for automatic music mood classification.",
        "author": [
            "Janto Skowronek",
            "Martin F. McKinney",
            "Steven van de Par"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1416696",
        "url": "https://doi.org/10.5281/zenodo.1416696",
        "ee": "https://zenodo.org/records/1416696/files/SkowronekMP06.pdf",
        "abstract": "Automatic music classification based on audio signals pro- vides a core technology for tools that help users to manage and browse their music collections. Since “mood” is also used as a browsing criterium, automatic mood classification could support the creation of the necessary metadata. We have developed a method to obtain a reliable “ground truth” database for automatic music mood classification. Our re- sults confirm that excerpt selection is a non-trivial issue and that there are some mood labels that are relatively consistent across subjects. Keywords: music mood classification, ground truth.",
        "zenodo_id": 1416696,
        "dblp_key": "conf/ismir/SkowronekMP06",
        "keywords": [
            "Automatic music classification",
            "core technology",
            "user management",
            "browse music collections",
            "mood classification",
            "metadata creation",
            "ground truth database",
            "excerpt selection",
            "non-trivial issue",
            "consistent mood labels"
        ],
        "content": "Ground truth forautomatic music mood classi\u0002cation\nJanto Skowronek, Martin F.McKinney ,StevenvandePar\nPhilips Research Laboratories\nHightech Campus 36,5656 AEEindho ven,TheNetherlands\u0000janto.skowronek,martin.mckinney,ste ven.van.de.par \u0001@philips.com\nAbstract\nAutomatic music classi\u0002cation based onaudio signals pro-\nvides acore technology fortools thathelp users tomanage\nandbrowse their music collections. Since mood isalso\nused asabrowsing criterium, automatic mood classi\u0002cation\ncould support thecreation ofthenecessary metadata. We\nhavedeveloped amethod toobtain areliable ground truth\ndatabase forautomatic music mood classi\u0002cation. Our re-\nsults con\u0002rm thatexcerpt selection isanon-tri vialissue and\nthatthere aresome mood labels thatarerelati velyconsistent\nacross subjects.\nKeywords: music mood classi\u0002cation, ground truth.\n1.Introduction\nLuetal.[1]discussed that automatic mood classi\u0002cation\ncanbecriticized because theemotional meaning inmusic is\nhighly subjecti ve.However,theyalsostressed thatthere isa\ncertain agreement onthemusic' smood andtheyshowedthat\nmood classi\u0002cation ispossible. Inaddition, there isastrong\napplication-oriented interest inmood classi\u0002cation: music\ndownload services [2]oraudio players [3]allowmusic col-\nlection browsing using mood asonesearch criterium. Au-\ntomatic mood classi\u0002cation could decrease theeffortinpro-\nviding thenecessary metadata.\nWhen developing aclassi\u0002cation system, thede\u0002nition\nandcollection ofaproper ground truth iscrucial. From a\nclassi\u0002cation point ofview,thede\u0002ned classes should be\ninternally consistent. From anapplication point ofview,\nusers should haveaclear andcommon understanding ofthe\nclasses andthedata intheclasses should re\u0003ect theusers'\nopinions. Both perspecti veshavetwoissues incommon:\nclass de\u0002nition andmaterial selection. Inastudy focussing\nonmood classi\u0002cation weaddressed, therefore, twoques-\ntions: Howdoweselect music tracks thatprovokeorar-\nticulate emotions well enough emotions such listeners can\nassign amood label tothetrack? Howdoweselect labels\nthatlisteners caneasily usefordescribing themusic' smood\nandonwhich subjects agree when assessing individual mu-\nsicpieces?\nPermission tomakedigital orhard copies ofallorpartofthisworkfor\npersonal orclassroom useisgranted without feeprovided thatcopies\narenotmade ordistrib uted forpro\u0002t orcommercial advantage andthat\ncopies bear thisnotice andthefullcitation onthe\u0002rstpage.\nc\n\u00022006 University ofVictoria2.Backgr ound\nLuetal.[1]setupamood classi\u0002cation system which de-\n\u0002ned four mood categories, which were derivedfrom atwo-\ndimensional model ofaffect[4].Forthetrack selection, Lu\netal.follo wed anexpert-based approach: Amusic excerpt\n(western classical music) wasappended totheground truth\nonly ifthree experts agreed onthemood.\nInanother study ,Leman etal.[5]used 15bipolar adjec-\ntivepairs asmood descriptions selected byliterature scan\nandtrial experiments. Using afactor analysis onthegath-\nered subjecti vedata, theyidenti\u0002ed anunderlying three di-\nmensional space. Then theyprojected subjecti vemood as-\nsessments ofmusic tracks onto thatspace andused linear\nregression inorder topredict these projections with audio\nfeatures computed from thecorresponding music excerpts.\nLeman etal.used alargersetofmood labels than Luet\nal.buttheydidnottrytodirectly predict themood labels,\nbutrather their projections inthefound three-dimensional\nspace. Withrespect totrack selection, Leman' sapproach\nwasuser based: 20people were askedtopropose music in\nwhich theyrecognize anemotional affectandtodescribe it,\ngivennoconstraints about musical style.\nBoth studies used different mood labels aswell asdif-\nferent methods fortrack selection. Regarding the\u0002nal goal\nofdeveloping amood classi\u0002cation system andduetoour\nexperience from twopilot experiments, wesawadvantages\nanddisadv antages inboth approaches; thus wedecided to\ncombine them intoourownexperimental method.\n3.Experiment\nInlinewith ourresearch questions, theexperiment design\nfocussed onacareful selection ofmusic excerpts andabroad\nsearch forproper mood labels.\nExcer ptselection: Since complete music pieces cancon-\ntainsections with different moods [1],wechose 20-second\nlong excerpts from around themiddle of470music tracks\nfrom 12music genres. The selection wasdone manually\nsuch thatthemood waslikelytobeconstant within theex-\ncerpt byavoiding drastic changes inthemusical character -\nistics (structure, loudness, instrumentation /timbre, tempo\netc.) Then the \u0003\u0005\u0004\u0007\u0006author evaluated these excerpts using\nnine bipolar mood scales, which comprised thehighest fac-\ntorloadings from [5],thetwoaxesoftheaffectmodel [4]\nandsome additional considerations. Foreach ofthe12mu-\nsicgenres wethen chose thetracks with the\u0002vemost ex-treme ratings, aiming toobtain aselection ofexcerpts with\neasy toevaluate moods.\nLabel de\u0002nition: When de\u0002ning themood scales used\nfortheexperiment, weaddressed twoissues: (1)Ina\u0002rst\npilot experiment subjects reported dif\u0002culties inassessing\nthemood with thetwoaffect model axesused byLuet\nal.[1].Another pilot experiment showed lowagreement\nacross subjects formost ofthenine bipolar mood scales\nmentioned intheprevious subsection. Since wewere in-\nterested inmood labels thatareeasy touse aswell ascon-\nsistent across subjects, wedecided forabroad search for\nproper direct mood labels andnottorestrict totheaxes\nofanunderlying mood space. Our collection of33adjec-\ntivescovered themain directions ofLeman' smood space\n(highest factor loadings in[5])aswell astheaxesandthe\nwith 45degrees rotated axesoftheaffectmodel [1,4,6],\naugmented bylabels already used inapplications ([2,3]and\nsome additional considerations. (2)Inthepilot experiments\nweexperienced thattheexact interpretation ofthelabels in-\n\u0003uenced thejudgments. Inorder tocon\u0002ne themeaning\nofthescales, weprovided uptothree synon yms ofadjec-\ntivesforeach scale, which inseveralcases coincided with a\nmerging ofsimilar scales from different sources ([1]-[6]).\nSince most ofthepotential subjects were notnativeEnglish\nspeak ers,four native-speak ersofother languages (NL, D,F,\nI)provided translations ofthelabels, andthechosen sub-\njects hadtohaveoneofthese languages asmother tongue.\nPractical experiment design: Wecollected data on33\nscales (7points: strongly disagree tostrongly agree) for60\ntracks with tworepetitions ( \b3960 individual evaluations).\nToreduce experiment time, wedistrib uted thetracks across\n10subjects such thata)each track isevaluated byfour sub-\njects, b)each subject hasadifferent setof24tracks andc)\nthe12music genres areequally represented intheset. In\naddition subjects were polled with questionnaires ontheir\npreferences with respect tomusic mood andontheease-of-\nuseoftheprovided labels.\n4.Data analysis\nWelookedatvarious aspects such asin\u0003uence ofexternal\nfactors andglobal mood, perceptual space spanned bythe\nlabels, similarity oflabels, consistenc ywithin subjects etc.\nHowever,wefocussed primarily onthose results regarding\nthetwomain questions ofthisstudy: theproper selection of\nmusic tracks andthecollection ofuseful mood labels.\nExcer ptselection: Inorder tocheck howwell these-\nlected excerpts were easy tojudge, weaveraged pertrack\nandlabel theratings across subjects andsessions andcom-\nputed a)thenumber ofexcerpts thatgotastrongjudgment\nforatleast onelabel, andb)theaverage number ofstrong\njudgments pertrack. De\u0002ning strongjudgments asagree\n&strongly agree, 31tracks gotatleast onestrong rating\nandonaverage everytrack gotstrong ratings in1.47 labels.\nConsistency acrosssubjects: Due totheexperiment de-sign, blocks of12excerpts havebeen evaluated bythesame\nfour subjects. Foreach ofthese blocks (5intotal) wecom-\nputed theCronbach coef\u0002cient \t[7]perlabel asaconsis-\ntencymeasure across subjects. Then weaveraged the \t's\nacross theblocks andidenti\u0002ed 10labels thathadvalues be-\ntween 0.7and0.84, which were intherange oftheminimum\nvalue indicating acceptable consistenc y[7].\nImportance and easiness ofmood labels: Analyzing\ntheexperiment questionnaires (mean value across subjects,\nrounded toclosest category), 20mood labels were atleast\nimportant forsubjects; 16ofthem were atleast easy to\nuse.\nBest labels: Interesting candidates foramood classi\u0002ca-\ntionsystem arethose which belong tothemost consistent\ngroup andwhich were assessed atleast asimportant andat\nleast aseasy touse. Wefound thatsuch labels exist, they\nwere: tender/soft, powerful/strong, loving/romantic, care-\nfree/lighthearted, emotional/passionate, touching/mo ving,\nangry/furious/aggressi ve,sad.\n5.Conclusions\nWeinvestigated howwecanobtain aproper ground truth for\namusic mood classi\u0002cation system.\nWithrespect toexcerpt selection, only halfofallexcerpts\nrecei vedstrong judgements. That means aproper choice of\neasy tojudge excerpts isanon-tri vialtask, evenforan\nexperienced listener .Possibilities foroptimization include\nincreasing thenumber subjects intheselection process and\nextending thenumber ofcandidate tracks.\nWithrespect toselection ofmood labels, wewere able\ntoidentify eight labels thatwere relati velyconsistent across\nsubjects, percei vedaseasy touseandregarded asimportant.\nThese aregood candidates foraground truth ofanautomatic\nmood classi\u0002cation system.\nRefer ences\n[1]L.Lu,D.Liu, H.-J. Zhang, Automatic Mood Detection and\nTracking ofMusic Audio Signals ,IEEE transactions onau-\ndio,speech, andlanguage processing, Vol.14(1), 5-18, 2006.\n[2]http://www.allmusic.com\n[3]http://www.moodlogic.com\n[4]J.A. Russell, Acircumple xmodel ofaffect,J.Personality &\nSocial Psychology ,Vol.39,1161-1178, 1980.\n[5]M.Leman, V.Vermeulen, L.DeVoogdt, D.Moelants, M.\nLesaf fre,Prediction ofMusical AffectUsing aCombination\nofAcoustic Structur alCues ,J.ofNewMusic Research, Vol.\n34(1), 39-67, 2005.\n[6]D.A. Ritossa, N.S. Rikkard, Therelative utility of'pleasant-\nness' and'liking' dimensions inpredicting theemotions ex-\npressed bymusic ,Psychology ofMusic, Vol.31(1), 5-22,\n2004.\n[7]J.M. Bland, D.G. Altman, Statistics notes: Cronbac h'sAl-\npha,BMJ, Vol.314, 572, 1997."
    },
    {
        "title": "Music Information Retrieval from a Singing Voice Based on Verification of Recognized Hypotheses.",
        "author": [
            "Motoyuki Suzuki",
            "Toru Hosoya",
            "Akinori Ito",
            "Shozo Makino"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1414786",
        "url": "https://doi.org/10.5281/zenodo.1414786",
        "ee": "https://zenodo.org/records/1414786/files/SuzukiHIM06.pdf",
        "abstract": "Several music information retrieval (MIR) systems have been developed which retrieve musical pieces by the user’s singing voice. All of these systems use only melody information for retrieval, although lyrics information is also useful for re- trieval. In this paper, we propose an MIR system that uses both melody and lyrics information in the singing voice. The MIR system verifies hypotheses output by a lyrics recognizer from a melodic point of view. Each hypothesis has time alignment information between the singing voice and recognized text, and the boundaries of each note can be estimated using the information. As a result, melody in- formation is extracted from the singing voice. On the other hand, the melody information can be calculated from the musical score of the song because the recognized text must be a part of the lyrics of the song. The hypothesis is veri- fied by calculating the similarity between the two types of melody information. From the experimental results, the verification method increased the retrieval accuracy. Especially, it was very ef- fective when the number of words in the user’s singing voice was small. The proposed method increased the retrieval ac- curacy from 81.3% to 87.4% when the number of words was only three. Keywords: MIR from singing voice, verification of recog- nized hypotheses, lyrics recognition.",
        "zenodo_id": 1414786,
        "dblp_key": "conf/ismir/SuzukiHIM06",
        "keywords": [
            "music information retrieval (MIR)",
            "singing voice",
            "melody information",
            "lyrics information",
            "verification method",
            "similarity calculation",
            "musical score",
            "experimental results",
            "retrieval accuracy",
            "number of words"
        ],
        "content": "Music Information Retrieval from a Singing Voice\nBased on Veriﬁcation of Recognized Hypotheses\nMotoyuki Suzuki, Toru Hosoya, Akinori Ito and Shozo Makino\nGraduate School of Engineering, Tohoku University\n6-6-05, Aramaki-Aza-Aoba, Aoba-ku, Sendai, 980-8579, JAPAN\n{moto, thosoya, aito, makino }@makino.ecei.tohoku.ac.jp\nAbstractSeveral music information retrieval (MIR) systems have been\ndeveloped which retrieve musical pieces by the user’s singing\nvoice. All of these systems use only melody information for\nretrieval, although lyrics information is also useful for re-trieval. In this paper, we propose an MIR system that uses\nboth melody and lyrics information in the singing voice.\nThe MIR system veriﬁes hypotheses output by a lyrics\nrecognizer from a melodic point of view. Each hypothesis\nhas time alignment information between the singing voice\nand recognized text, and the boundaries of each note can\nbe estimated using the information. As a result, melody in-\nformation is extracted from the singing voice. On the otherhand, the melody information can be calculated from the\nmusical score of the song because the recognized text must\nbe a part of the lyrics of the song. The hypothesis is veri-ﬁed by calculating the similarity between the two types of\nmelody information.\nFrom the experimental results, the veriﬁcation method\nincreased the retrieval accuracy. Especially, it was very ef-\nfective when the number of words in the user’s singing voicewas small. The proposed method increased the retrieval ac-\ncuracy from 81.3% to 87.4% when the number of words was\nonly three.\nKeywords: MIR from singing voice, veriﬁcation of recog-\nnized hypotheses, lyrics recognition.\n1. Introduction\nRecently, several music information retrieval (MIR) systemsthat use a user’s singing voice as a retrieval key have been\nresearched (for example, MIRACLE[1], SoundCompass[2],\nand our proposed method[3, 4]). These systems use melodyinformation in the user’s singing voice, however, the lyrics\ninformation is not taken into consideration.\nLyrics information is very useful for MIR systems. In a\npreliminary experiment, a retrieval key consisting of three\nJapanese letters narrowed hypotheses into ﬁve songs on av-\nerage, and the average number of retrieved songs was 1.3\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies\nare not made or distributed for proﬁt or commercial advantage and that\ncopies bear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2006 University of Victoria\nwhen ﬁve Japanese letters were used as a retrieval key. Note\nthat 161 Japanese songs were used as the database, and apart of the correct lyrics was used as the retrieval key in this\nexperiment.\nIn order to develop an MIR system that uses melody and\nlyrics information, the lyrics recognition method from a singing\nvoice has been proposed[5]. This method uses a ﬁnite state\nautomaton as a language model. It gave 77.4% word accu-\nracy, and the retrieval accuracy given by the system achieved\n85.9%.\nConventional MIR systems cannot use a singing voice\nas a retrieval key. One of the biggest problems is how to\nsplit the input singing voice into musical notes. TraditionalMIR systems[6, 2, 7] assume that a user hums with plo-\nsive phonemes, such as phonemes /ta/ or /da/, because the\nhummed voice can be split into notes only using power in-\nformation. On the other hand, recent MIR systems can split\nthe input singing voice into musical notes, however, they of-ten give inaccurate information. It is difﬁcult for them to\ndiscriminate between one note with long duration and two\nnotes with the same pitch because these systems split the in-put singing voice using pitch and power information. It is\nvery hard to split the singing voice into musical notes with-\nout linguistic information.\nThe lyrics recognizer outputs several hypotheses as a recog-\nnition result. Each hypothesis has time alignment informa-tion between the singing voice and recognized text. A hy-\npothesis can be veriﬁed using the time alignment informa-\ntion from a melodic point of view. In this paper, we pro-pose a new MIR system using lyrics and melody informa-\ntion based on veriﬁcation of recognized hypotheses.\n2. Veriﬁcation Method of Recognized\nHypotheses\n2.1. Overview of the System\nFigure 1 shows an outline of the proposed MIR system.\nFirst, a user’s singing voice is input to the lyrics recognizer,\nand the top Nhypotheses with higher recognition score are\noutput.\nEach hypothesis hhas the following information:\n•Song name S(h)\n•Recognized text W(h)\nIt must be a part of the lyrics of the song S(h).MelodyLyrics\nrecognition\nhypothese sSinging voice\nby melody\nRetrieval resultVerificationLyrics\ndatabase\ndatabase\nFigure 1. Outline of the MIR system using lyrics and melody\n•Recognition score R(h)\n•Time alignment information F(h)\nFor all phonemes in the recognized text, frame num-\nbers of the start frame and the end frame are output.\nFor a hypothesis h, the tune corresponding to the recognized\ntextW(h)can be obtained from the database because W(h)\nmust be a part of the lyrics of S(h)[5]. The melody infor-\nmation, which is deﬁned as a relative pitch and relative IOI\n(Inter-Onset Interval) of each note, can be calculated fromthe tune. On the other hand, the melody information can be\nextracted using the estimated pitch sequence of the singing\nvoice and F(h). If the hypothesis his correct, the two types\nof information should be similar. The veriﬁcation score is\ndeﬁned as the similarity between the two types of informa-\ntion.\nFinally, the total score is calculated from the recognition\nscore and the veriﬁcation score, and the hypothesis with thehighest total score is output as a retrieval result.\n2.2. Extraction of Melody Information from a Singing\nVoice\nRelative pitch Δf\nnand relative IOI Δtnof a note nare\nextracted from the singing voice. In order to extract this\ninformation, boundaries between notes are estimated fromtime alignment information F(h).\nFigure 2 shows an example of the estimation procedure.\nFor each song in the database, a correspondence table is\nmade from the musical score of the song in advance. This ta-\nble describes all of the correspondences between phonemesin the lyrics and notes in the musical score (for example, the\ni-th note of the song corresponds to phonemes from jtok).\nWhen the singing voice and the hypothesis hare given,\nboundaries between notes are estimated from the time align-\nment information F(h)and the correspondence table. The\nTime alignme ntCorrespondence\nboundariesinformationtableMusic score\nWord sequence\nSinging\nvoice Estimatedarosi ao\nFigure 2. Example of estimation of boundaries between notes\nphoneme sequence corresponding to the note ncan be ob-\ntained from the correspondence table, and the start frame of\nnis obtained as the start frame of the ﬁrst phoneme from\nF(h). In the same way, the end frame of nis obtained as\nthe end frame of the last phoneme.\nAfter estimation of boundaries, pitch sequence is calcu-\nlated by the praat[8] system frame-by-frame, and the pitch\nof the note is deﬁned as the median of the pitch sequence\ncorresponding to the note. IOI of the note is obtained as theduration between boundaries.\nFinally, the pitch and IOI of the note nare translated into\nrelative pitch Δf\nnand relative IOI Δtnusing the two equa-\ntions:\nΔfn=l o g2fn+1\nfn(1)\nΔtn=l o g2tn+1\ntn(2)\nwhere, fnandtnare pitch and IOI of the n-th note respec-\ntively.\nNote that estimated boundaries using the hypothesis are\ndifferent from that using another hypothesis. Therefore, dif-ferent melody information will be extracted using another\nhypothesis from the same singing voice.\n2.3. Calculation of Veriﬁcation Score\nThe veriﬁcation score V(h)corresponding to a hypothesis\nhis deﬁned as the similarity between melody information\nextracted from the singing voice and the tune.\nAt ﬁrst, relative pitch Δˆf\nnand relative IOI Δˆtnare cal-\nculated from the tune corresponding to the recognized text\nW(h), and the veriﬁcation score V(h)is calculated by\nV(h)=1\nN−1N−1/summationdisplay\nn=1/braceleftbig\nw1(|Δˆtn−Δtn|)\n+(1−w1)(|Δˆfn−Δfn|)/bracerightBig(3)\nwhere, Ndenotes the number of notes in the tune, and w1\ndenotes a pre-deﬁned weighting factor.The total score T(h)is calculated by Eq. (4) for each\nhypothesis h, and the ﬁnal result His selected by Eq. (5):\nT(h)= w2R(h)−(1−w2)V(h) (4)\nH=a r g m a x\nhT(h) (5)\n3. Experiments\nIn order to investigate the effectiveness of the proposed MIR\nsystem, several experiments were carried out.\nHTK[9] was used as the recognizer, and a monophone\nHMM was used as an acoustic model. Note that a “mono-\nphone HMM” is a simple acoustic model, and it means thata phoneme is modeled by an HMM. The HMM was trained\nusing a large amount of normally read speech, and the singing\nvoice adaptation method[5] was carried out using 127 singingvoice data sung by 6 male university students.\nOne hundred and ten singing voice data sung by another\n6 male students were used as test data, and all of the test data\nwere automatically segmented into words using the Viterbi\nalgorithm[10]. It is assumed that a user sings a few wordswhen the MIR system is used. Therefore, the number of\nwords in a test sample can be controlled. There were 156\nJapanese children’s songs in the database.\nThe average word accuracy of the test data was 81.0%,\nand the lyrics recognizer output 1,000 hypotheses per test\nsample. In this hypotheses list, some similar hypotheses\nwere output as another hypothesis. For example, both hy-\npotheses hand¯hare in the hypotheses list as another hy-\npothesis because W(h)is a slightly different from W(¯h),\neven though S(h)is exactly the same as S(¯h). The correct\nhypothesis was not included in the hypotheses list for 2.6%of test samples. This means that the maximum retri eval ac-\ncuracy was limited to 97.4%.\n 84 86 88 90 92 94 96 98 100\ntop 1 top 5 top 10Retrieval accuracy (%)\nNumber of retrieved results\nFigure 3. Retrieval accuracy using ﬁve wordsTable 1. Relationship between the rank of the correct hypoth-\nesis and veriﬁcation method\nAfter veriﬁcation\nTop 1\n Others\nBefore\n Top 1\n 753\n 8\nveriﬁcation\n Others\n 37\n 52\n3.1. Retrieval Accuracy for Fixed-length Input\nIn this section, the number of words in a test sample was\nﬁxed to ﬁve, and weighting factors w1andw2were set to\noptimum values a posteriori .\nFigure 3 shows retrieval accuracy given by the before and\nafter veriﬁcation. In this ﬁgure, the left side of each num-\nber of retrieved results denotes the retrieval accuracy given\nby before veriﬁcation, which is the same as the system pro-\nposed in [5], and the right side denotes that given by the\nproposed MIR system. The horizontal line denotes the up-per limit of the retrieval accuracy.\nThis ﬁgure shows that the veriﬁcation method was very\neffective in increasing retrieval accuracy. Especially, the re-trieval accuracy of top 1 increased by 3.4 points, from 89.5%\nto 92.9%. However, the retrieval accuracy of top 10 was\nslightly improved. This result means that the hypotheseswith higher (but not ﬁrst-ranked) recognition score can be\ncorrected.\nTable 1 shows the relationship between the rank of the\ncorrect hypothesis and veriﬁcation method. The numbers in\nthis table indicate a number of samples, and the total number\nof test samples was 850.\nIn 753 test samples, which is 88.6% of the test samples,\nthe correct hypothesis was ranked ﬁrst before and after veri-\nﬁcation. The correct hypothesis became the ﬁrst-rank by theveriﬁcation in 37 samples. On the other hand, only 8 sam-\nples were corrupted by the veriﬁcation method. This result\nshowed that the veriﬁcation method does not decrease the\nperformance of lyrics recognition results for any samples,\nand several samples can be improved by the method.\n3.2. Retrieval Accuracy for Variable Length Input\nIn this section, we investigate the relationship between the\nnumber of words in a singing voice and retrieval accuracy.\nThe number of words was increased from 3 to 10. In this ex-periment, 152 singing voice data sung by 6 new males were\nadded to the test samples in order to increase the statistical\nreliability of the experimental result. Other experimentalconditions were the same as in the previous experiments.\nFigure 4 shows the relationship between the number of\nwords and retrieval accuracy. In this ﬁgure, the left side ofeach number of words denotes the retrieval accuracy given\nby before veriﬁcation, and the right side denotes that given\nby the proposed MIR system. 80 85 90 95 100\n3 5 7 10Retrieval accuracy (%)\nNumber of words in the singing voice\nFigure 4. Retrieval accuracy using various number of words\nThis ﬁgure shows that the proposed MIR system gave\nhigher accuracy for all conditions. Especially, the veriﬁca-\ntion method was very effective when the number of words\nwas small. There are many songs which have partially the\nsame lyrics. If the number of words in the retrieval key\nis small, a lot of hypotheses are ranked at the same rank,and cannot be distinguished only using lyrics information.\nMelody information is very powerful in these situations. The\nχ\n2-test showed that the difference between before and after\nveriﬁcation is statistically signiﬁcant when the number of\nwords was set to 3 and 5.\n3.3. Discussion: system performance when the lyrics are\nonly partially known by a user\nThe proposed system assumes that the input singing voice\nconsists of a part of the correct lyrics. If it includes a wrongword, the retrieval may fail.\nThis issue need to be addressed in future work, however,\nit is not fatal for the system. If a user knows several correct\nwords in the lyrics, retrieval can still succeed because the\nproposed system gave about 87% retrieval accuracy with the\nquery consisting of only three words. Moreover, the lyricsrecognizer can correctly recognize a long query even if it\nincludes several wrong words because of the grammatical\nrestriction of FSA.\n4. Conclusion\nIn this paper, we propose an MIR system that uses bothmelody and lyrics information in the singing voice.\nThe MIR system veriﬁes hypotheses output by a lyrics\nrecognizer from a melodic point of view. Each hypothesis\nhas time alignment information between the singing voice\nand recognized text, and the boundaries of each note canbe estimated using the information. As a result, melody in-\nformation is extracted from the singing voice. On the other\nhand, the melody information can be calculated from themusical score of the song because the recognized text must\nbe a part of the lyrics of the song. The hypothesis is veri-\nﬁed by calculating the similarity between the two types ofmelody information.\nFrom the experimental results, the veriﬁcation method\nincreased the retrieval accuracy. Especially, it was very ef-fective when the number of words in the user’s singing voice\nwas small. The proposed method increased the retrieval ac-\ncuracy from 81.3% to 87.4% when the number of words was\nonly three.\nReferences\n[1] J. S. R. Jang, J. Chun, and M.-Y . Kao, “MIRACLE: A Mu-\nsic Information Retrieval System with Clustered Computing\nEngines,” in International Symposium on Music Information\nRetrieval , 2001.\n[2] N. Kosugi, Y . Nishihara, T. Sakata, M. Yamamoto, and\nK. Kushima, “A Practical Query-By-Humming System for\na Large Music Database,” in ACM Multimedia 2000 , 2000,\npp. 333–342.\n[3] S.-P. Heo, M. Suzuki, A. Ito, and S. Makino, “Three di-\nmensional continuous DP algorithm for multiple pitch candi-\ndates in music information retrieval system,” in Proc. ISMIR ,\n2003, pp. 235–236.\n[4] S.-P. Heo, M. Suzuki, A. Ito, S. Makino, and H.-Y . Chung,\n“Multiple pitch candidates based music information retrieval\nmethod for query-by-humming,” in Proc. AMR , 2003, pp.\n189–200.\n[5] T. Hosoya, M. Suzuki, A. Ito, and S. Makino, “Lyrics recog-\nnition from a singing voice based on ﬁnite state automaton\nfor music information retrieval,” in Proc. ISMIR , 2005, pp.\n532–535.\n[6] A. Ghias, J. Logan, D. Chamberlin, and B. C. Smith, “Query\nby humming: Musical information retrieval in an audio\ndatabase,” in Proc. ACM Multimedia , 1995, pp. 231–236.\n[7] B. Liu, Y . Wu, and Y . Li, “A Linear Hidden Markov Model\nfor Music Information Retrieval Based on Humming,” inProc. ICASSP 2003 , 2003, vol. V ol. V , pp. 533–536.\n[8] P. Boersma and D. Weenink, “praat,” University of Amster-\ndam, http://www.fon.hum.uva.nl/praat/.\n[9] Cambridge University Engineering Department, “Hidden\nMarkov Model Toolkit,” http://htk.eng.cam.ac.uk/.\n[10] A. Viterbi, “Error bounds for convolutional codes and an\nasymptotically optimal decoding algorithm,” IEEE Trans.\nInformation Theory , vol. 13, no. 2, pp. 260–269, 4 1967."
    },
    {
        "title": "A Probabilistic Model of Melody Perception.",
        "author": [
            "David Temperley"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1414988",
        "url": "https://doi.org/10.5281/zenodo.1414988",
        "ee": "https://zenodo.org/records/1414988/files/Temperley06.pdf",
        "abstract": "This study presents a probabilistic model of melody perception, which infers the key of a melody and also judges the probability of the melody itself. (A “melody” is defined here as a sequence of pitches, without rhythmic information.) The model uses Bayesian reasoning. A generative probabilistic model is proposed, based on three principles: 1) melodies tend to remain within a narrow pitch range; 2) note-to-note intervals within a melody tend to be small; 3) notes tend to conform to a distribution (or “key-profile”) that depends on the key. The model is tested in three ways: on a key-finding task, on a melodic expectation task, and on an error-detection task. Keywords: Music cognition, key identification, probabilistic modeling, expectation, error detection",
        "zenodo_id": 1414988,
        "dblp_key": "conf/ismir/Temperley06",
        "keywords": [
            "Bayesian reasoning",
            "melody perception",
            "key identification",
            "probabilistic modeling",
            "generative model",
            "pitch range",
            "note intervals",
            "melodic expectation",
            "key-profile",
            "error detection"
        ],
        "content": "A Probabilistic Model of Melody Perception David Temperley Eastman School of Music 26 Gibbs St. Rochester, NY 14604 dtemperley@esm.rochester.edu   Abstract This study presents a probabilistic model of melody perception, which infers the key of a melody and also judges the probability of the melody itself. (A “melody” is defined here as a sequence of pitches, without rhythmic information.) The model uses Bayesian reasoning. A generative probabilistic model is proposed, based on three principles: 1) melodies tend to remain within a narrow pitch range; 2) note-to-note intervals within a melody tend to be small; 3) notes tend to conform to a distribution (or “key-profile”) that depends on the key. The model is tested in three ways: on a key-finding task, on a melodic expectation task, and on an error-detection task.  Keywords: Music cognition, key identification, probabilistic modeling, expectation, error detection  1. Introduction In recent years, methods of Bayesian probabilistic modeling have been widely used in a variety of areas in information processing and cognitive modeling, such as natural language processing, vision, and knowledge representation; they have also been applied to musical problems such as transcription [1] and metrical analysis [2, 3]. In general, we can frame the Bayesian approach as a way of recovering some kind of underlying structure from some kind of surface representation. Bayesian logic tells us that   P(structure | surface)  ∝ P(surface | structure)P(structure)     (1)  Thus we may determine the most probable structure given a surface if we know, for all structures, the probability of the surface given the structure and the prior probability of the structure. A further consequence of the Bayesian approach is the possibility of calculating the probability of the surface pattern itself:  P(surface)  = ∑ P(structure, surface)                     = ∑ P(surface | structure) P(structure)    (2)  It can be seen, then, that calculating the most probable structure given a surface and calculating the probability of a surface are very closely related problems.  In the current case, we define a surface as a sequence of pitches (without rhythmic information); the structure is a key. The problem, then, is to determine the most probable key given a note sequence—this is the familiar and well-studied “key-finding” problem. The probability of a surface is then the probability of a sequence of pitches. I will argue that this concept of “surface probability” is of relevance to a variety of processes in music information processing, such as expectation, error detection, and transcription. (More information about the model presented here can be found in [4]). Like most Bayesian models, our approach begins with a generative model, which generates a key and then a series of pitches. The model also considers two other important factors in melodic construction, range and pitch proximity. To set the model’s parameters, we use a corpus of over 6,000 computationally-encoded European folk melodies, the Essen Folksong Collection [5]. We will then show how this model can be used in Bayesian fashion to perform key identification as well as “surface-level” processes such as expectation and error detection.  2. The Generative Model We begin with a very basic question: What kind of pitch sequence makes a likely melody? Perhaps the first principle that comes to mind is that a melody tends to be confined to a fairly limited range of pitches. In the Essen corpus, the average range of a melody (from the highest to the lowest pitch, inclusive) is 13.6 semitones. We can model this situation in a generative way by first choosing a central pitch c for the melody; this is randomly chosen from a normal distribution, which we call the central pitch profile. We then create a second normal distribution centered around c, the range profile, which is used to actually generate the notes. A melody can then be constructed as a series of notes generated from the range profile.  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. © 2006 University of Victoria structure structure A further important principle in melodic construction is that intervals between adjacent notes in a melody tend to be small [6]. In the Essen corpus, more than half of all melodic intervals are 2 semitones or less. We can approximate this distribution of melodic intervals with a proximity profile—a normal distribution centered around a given pitch, indicating the pitch probabilities for the following note. We then create a new distribution which is the product of the proximity profile and the range profile. In effect, this “range × proximity” profile favors melodies which maintain small note-to-note intervals, but also remain within a fairly narrow global range.  Our third and final principle of melodic construction is that melodies (at least in the Western tradition) tend to adhere to the scale of a particular key. We incorporate this using the concept of key-profiles. A key-profile is a twelve-valued vector, representing the stability or appropriateness of pitch-classes in relation to a key [8]. In this case, the key-profiles are based on the actual distribution of scale-degrees (pitch-classes in relation to the key) in the Essen corpus. We count the occurrences of each scale-degree in each song; we sum these counts over all songs (grouping major-key and minor-key songs separately), and express the totals as proportions. The resulting key-profiles are shown in figure 1. The profiles show that, for example, 18.4% of notes in major-key melodies are scale-degree 1. The profiles reflect conventional musical wisdom, in that pitches belonging to the major or minor scale of the key have higher values than other pitches, and pitches of the tonic chord (the 1, 3, and 5 degrees in major or the 1, b3, and 5 degrees in minor) have higher values than other scalar ones.   \n \n Figure 1. Key-profiles for major keys (above) and minor keys (below). To combine all three of our principles together, we duplicate the key-profiles over many octaves; we can then multiply a key-profile together with the range and proximity profiles. We will call the resulting distribution the RPK profile. (To be interpretable as probabilities, the RPK profile values must be normalized to sum to 1.) In generating a melody, we must construct the RPK profile anew at each point, since the proximity profile depends on the previous pitch. (For the first note, since there is no previous pitch, we simply use the product of the range and key profiles.)  Figure 2 shows an RPK profile, assuming a key of C major, a central pitch of 68 (Ab4), and a previous note of C4. One can discern a roughly bell-shaped curve to this profile (though with smaller peaks and valleys). The proximity profile pulls the center of the curve towards C4, but the range profile pulls it towards Ab4; the result is that the actual center is in between the two. The key-profile gives higher values to pitches that are within the C major scale, thus accounting for the local peaks and valleys.  \n Figure 2. An RPK profile, assuming a key of C major, a central pitch of Ab4, and a previous pitch of C4. The generative process thus operates by choosing a key and a central pitch, and then generating a series of pitches. (The process does not decide how many pitches to generate; this is assumed to be given.) The probability of a pitch occurring at any point is given by its RPK profile value: the normalized product of its range-profile value (given the central pitch), its proximity-profile value (given the previous pitch), and its key-profile value (given the chosen key). We express the joint probability of a pitch sequence of n notes with a key k and a central pitch c as follows:    P(pitch sequence, k,  c) = P(k) P(c) Π RPKn      (3)     where P(k) is the probability of a key being chosen, P(c) is the probability of a central pitch being chosen, and RPKn are the RPK-profile values for the pitches of the melody given the key, central pitch, and previous pitch. We assume that all keys of the same mode are equal in prior probability, since most listeners—lacking “absolute pitch”—are incapable of identifying keys in absolute terms. However, we assign major keys a higher probability than minor keys, reflecting the higher proportion of major-key melodies in the Essen collection.  How do we calculate the overall probability of a pitch sequence? For the moment, let us think of the structure as the combination of a key and a central pitch; the surface is a sequence of pitches. From equations 2 and 3 above:  n P(pitch sequence) = ∑ (P(k) P(c) Π RPKn)              (4)     This can be calculated quite easily by considering each (k, c) pair and calculating the joint probability of the pitch sequence with that pair. 3. Testing the Model on Key-Finding We now consider how the generative process described above might be incorporated into a key-finding model. The task is simply to choose a single key for a given melody. (We do not allow the possibility of modulations—changes of key.) Using Bayesian logic, the most probable key given the melody will be the one maximizing P(k, pitch sequence). Using equations 2 and 4 above:     P(k, pitch sequence) = ∑ (pitch sequence, k, c)         = ∑ (P(k) P(c) Π RPKn)        (5)  Thus the most probable key given a pitch sequence is the one maximizing this expression. Our key-finding process thus proceeds as follows. For each key, we calculate the joint probability of the melody with that key and each central pitch, and sum this over all central pitches. The probability of a pitch at a given point in the melody depends only on its value in the RPK profile at that point; the RPK profile can be recreated at each note, just as it was in the generative process. We perform this process for all keys, and choose the key yielding the highest value; this is the most probable key given the melody.  To test the model’s key-finding ability, we use a sample of 65 songs from the Essen collection. (This sample was not included in the corpus used for setting the model’s parameters.) Each song in the corpus is annotated with a single key label; these labels provide a set of “correct” judgments against which the model can be evaluated. The model judged the key correctly for 57 of the 65 melodies (87.7%). By way of comparison, two other well-known key-finding algorithms were also tested on the corpus (using my own implementations). The model of Longuet-Higgins and Steedman [7] guessed the correct key on 46 out of 65 melodies, or 70.8% correct; the model of Krumhansl and Schmuckler [8] guessed the correct key on 49 out of 65, or 75.4% correct. 4. Expectation and Error Detection It is well known that in listening to a melody, listeners form expectations as to what note will occur next. Melodic expectation has been the subject of a great deal of research in music psychology and music theory. Of particular interest here is an experimental study by Cuddy and Lunney [9]. In this study, subjects were played a context of two notes played in sequence (the “implicative interval”), followed by a third note (the “continuation tone”), and were asked to judge the third note given the first two on a scale of 1 (“extremely bad continuation”) to 7 (“extremely good continuation”). Eight different contexts were used: ascending and descending major second, ascending and descending minor third, ascending and descending major sixth, and ascending and descending minor seventh. Each two-note context was followed by 25 different continuation tones, representing all tones within an octave above or below the second tone of the context (which was always either C4 or F#4). For each condition (context plus continuation tone), Cuddy and Lunney reported the average rating, thus yielding 200 data points in all.  We tested the current model’s ability to predict melodic expectation, using Cuddy and Lunney’s data. To do this, it was necessary to interpret their data probabilistically. Specifically, each rating was taken to indicate the log probability of the continuation tone given the previous two-tone context. Under the current model, the probability of a pitch pn given a previous context (p0…pn–1) can be expressed as   P(pn | p0…pn–1) = P(p0…pn) / P(p0…pn–1)                   (6)     where P(p0…pn) is the overall probability of the context plus the continuation tone, and P(p0…pn–1) is the probability of just the context. An expression indicating the probability of a sequence of tones was given in equation 4 above; this can be used here to calculate both P(p0…pn–1) and P(p0…pn). For example, given a context of (Bb4, C4) and a continuation tone of D4, the model’s expectation judgment would be log(P(Bb4, C4, D4) / P(Bb4, C4)) = –1.955.  The model was run on the 200 test items in Cuddy and Lunney’s data, and its outputs were compared with the experimental ratings for each item. Using the parameters gathered from the Essen Folksong Collection, the model yielded the correlation r = 0.664. It seemed reasonable, however, to adjust the parameters to achieve a better fit to the data. (This is analogous to what is done in most other tests of expectation models—such as those in [9] and [10]—in which multiple regression is used to fit a set of factors to the data in an optimal way.) With adjusted parameters, the model achieved a score of r = .822. This score is slightly better than that of Cuddy and Lunney’s own model (.80), though not quite as good as that of Schellenberg’s model [10] on the same data (.851).  Another kind of phenomenon that is illuminated by the current model could be described as “pitch error detection.” It seems uncontroversial that most human listeners have some ability to detect errors—“wrong notes”—even in an unfamiliar melody. The ability of the current model to detect errors was tested using the 65-song Essen test set described above. The model was given the original melodies as well as randomly distorted versions of the same melodies; the question was whether it could reliably assign a higher probability to the correct versions n  c  c n k, c as opposed to the distorted versions. The deformed version of a melody was produced by randomly choosing one note and replacing it by a random pitch within the range of the melody (between the lowest and highest pitch). The process was repeated 10 times for each of the 65 melodies, yielding a total of 650 trials. In each trial, the model’s analyses for the correct version and the deformed version were compared simply with regard to the total probability given to each melody (as defined in equation 4), to see which version was assigned higher probability. In effect, then, the model simply judged which of a pair of melodies was more likely to contain an error, without expressing any opinion as to exactly where the error was. The model assigned the correct version of the melody higher probability than the deformed version in 573 out of 650 trials (88.2%). This level of performance seems promising. Probably, not all random “errors” of this type would be identifiable as errors even by humans; whether the model’s ability is comparable to that of human listeners remains to be tested.  5. Further Issues We have presented a probabilistic model which performs key identification as well as the surface-level tasks of expectation and error detection. On balance, where comparison is possible, the model is at least competitive with other models in its level of performance. Beyond the issue of performance, however, the current model has important advantages over others that have been proposed. In particular, the current model is able to perform both the structural task of key identification and the surface-level tasks of expectation and error detection within a single framework. This sets it apart from prior models, which have addressed these problems separately. The connection between expectation and key-finding is indirectly reflected in some earlier work—notably in the fact both expectation models [9, 10] and key-finding models [8] have made use of key-profiles. But this connection is brought out much more clearly in the current approach. The current model also provides a natural way of calculating the overall probability of a melody, which earlier key-finding and expectation models do not. A further aspect of melody perception deserving brief mention here is the actual identification of notes. The extraction of note information from an auditory signal is a complex process, involving the grouping of partials (individual frequencies) into complex tones, and the correct categorization of these complex tones into pitch categories. (See [11] for a review of recent research on this problem.) It seems likely that the model proposed above could contribute to this task, by evaluating the probability of different possible note patterns (as in the error-detection task above). These judgments could then be used in a “top-down” fashion—in effect, bringing to bear musical considerations such as key, pitch proximity, and range on the transcription process. Some efforts have been made to apply Bayesian methods to transcription [1, 12], but much more could be done in this area. One obvious question that arises here is whether the current model could be extended to handle polyphonic music. I have argued elsewhere [13] that a rather different approach to key identification is required in polyphonic music. Briefly, there is so much use of doubled and repeated pitch-classes in polyphonic music that counting every event gives too much weight to such pitch-classes; a better approach is to simply judge each pitch-class as “present” or “absent” within a small segment of music. (See [13] for a polyphonic key-finding model based on this idea.) However, this model cannot be used to calculate the probability of a pitch pattern, and is therefore not well-suited to modeling expectation and error detection. The way these problems might be addressed in polyphonic music remains an open question.  References [1] K. Kashino, K. Nakadai, T. Kinoshita, and H. Tanaka, “Application of Bayesian probability networks to musical scene analysis,” in Computational Auditory Scene Analysis, D.F. Rosenthal and H.G. Okuno, Eds. Mahwah, NJ: Lawrence Erlbaum, 1998, pp. 115-137. [2] A. T. Cemgil, B. Kappen, P. Desain, and H. Honing, “On Tempo tracking: Tempogram representation and Kalman filtering,” Journal of New Music Research, vol. 29, pp. 259–273, 2000.  [3] C. Raphael, “A hybrid graphical model for rhythmic parsing,” Artificial Intelligence, vol. 137, pp. 217–238, 2002.  [4] D. Temperley, Music and Probability, Cambridge: MIT Press, forthcoming.  [5] H. Schaffrath, The Essen Folksong Collection, David Huron, Ed. Stanford, CA: Center for Computer–Assisted Research in the Humanities, 1995. [6] P. von Hippel and D. Huron, “Why do skips precede reversals? The effect of tessitura on melodic structure,” Music Perception, vol. 18, pp. 59–85, 2000. [7] H. C. Longuet–Higgins and M. J. Steedman, “On interpreting Bach,” Machine Intelligence, vol. 6, pp. 221–241, 1971. [8] C. L. Krumhansl, Cognitive Foundations of Musical Pitch, New York: Oxford University Press, 1990. [9] L. L. Cuddy and C. A. Lunney, “Expectancies generated by melodic intervals: Perceptual judgments of melodic continuity,” Perception & Psychophysics, vol. 57, pp. 451–62, 1995. [10] E. G. Schellenberg, “Simplifying the implication-realization model of melodic expectancy,” Music Perception,  vol. 14, pp. 295–318, 1997. [11] A. P. Klapuri, “Automatic music transcription as we know it today,” JNMR, vol. 33, pp. 269–282, 2004. [12] S. A. Abdallah, and M. D. Plumbley, “Polyphonic transcription by non-negative sparse coding of power spectra,” in ISMIR 2004. [13] Temperley, D, “Bayesian models of musical structure and cognition,” Musicae Scientiae, vol. 8, pp. 175–205, 2004."
    },
    {
        "title": "Modeling music and words using a multi-class naïve Bayes approach.",
        "author": [
            "Douglas Turnbull",
            "Luke Barrington",
            "Gert R. G. Lanckriet"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1415782",
        "url": "https://doi.org/10.5281/zenodo.1415782",
        "ee": "https://zenodo.org/records/1415782/files/TurnbullBL06.pdf",
        "abstract": "We propose a query-by-text system for modeling a het- erogeneous data set of music and words. We quantitatively show that our system can both annotate a novel song with semantically meaningful words and retrieve relevant unla- beled songs from a database given a text-based query. We explain two feature extraction methods useful for summa- rizing the audio content of a song. We describe a supervised multi-class na¨ıve Bayes model and compare two parameter estimation techniques. Our approach is influenced by recent computer vision research on the related tasks of image an- notation and retrieval. Keywords: music annotation, music retrieval, query-by-text, heterogeneous data",
        "zenodo_id": 1415782,
        "dblp_key": "conf/ismir/TurnbullBL06",
        "keywords": [
            "query-by-text",
            "heterogeneous data set",
            "music annotation",
            "music retrieval",
            "semantically meaningful words",
            "audio content",
            "supervised multi-class naive Bayes model",
            "parameter estimation techniques",
            "image annotation",
            "retrieval"
        ],
        "content": "Modeling music and words using a multi-classna ¨ıve Bayesapproach\nDouglasTurnbull\nUCSanDiego\nLa Jolla,CA 92093\ndturnbul@cs.ucsd.eduLuke Barrington\nUC SanDiego\nLaJolla, CA 92093\nlbarring@ucsd.eduGert Lanckriet\nUCSan Diego\nLaJolla,CA 92093\ngert@ece.ucsd.edu\nAbstract\nWe propose a query-by-text system for modeling a het-\nerogeneousdata set of music and words. We quantitatively\nshow that our system can both annotate a novel song with\nsemantically meaningful words and retrieverelevant unla-\nbeled songs from a database given a text-based query. We\nexplain two feature extraction methods useful for summa-\nrizingtheaudiocontentofasong. Wedescribeasupervised\nmulti-class na¨ ıve Bayes model and compare two parameter\nestimationtechniques. Ourapproachisinﬂuencedbyrecent\ncomputer vision research on the related tasks of image an-\nnotationandretrieval.\nKeywords: musicannotation,musicretrieval,query-by-text,\nheterogeneousdata\n1. MusicAnnotationand Retrieval\nMusicisaformofcommunicationthatcanrepresenthuman\nemotions,personalstyle,geographicorigins,spiritualf oun-\ndations, social conditions, and other aspects of humanity.\nListeners naturally use words in an attempt describe what\ntheyhear,thoughtwo listenersmayusedrasticallydiffere nt\nwords when describing the same piece of music. However,\nwords related to some aspects of the audio content, such as\ninstrumentationandgenre,maybeagreeduponbya major-\nity of listeners. This agreement suggests that it is possibl e\ntocreateacomputerauditionsystemthatcanlearntherela-\ntionshipbetweenaudiocontentandwords. By jointlymod-\neling these two representations, we create a model that can\nbeusedtoboth retrievesoundsgivenatext-basedqueryand\ntoannotatea soundwithtextgiventheaudiocontent.\nA central goal of the music information retrieval com-\nmunityistocreatesystemsthatefﬁcientlystoreandretrie ve\nsongs from large databases of musical content [1]. The\nmostcommonwaytostoreandretrievemusicusesmetadata\nsuch as the name of the composer or artist, the name of the\nsong or the release date of the album. We consider a more\ngeneral deﬁnition of musical metadata as any non-acoustic\nrepresentation of a song. This includes genre and instru-\nment labels, song reviews, ratings according bipolar adjec -\ntives (e.g., happy/sad), and purchase sales records. These\nPermission to makedigital orhardcopies ofallorpartofthi s workfor\npersonal or classroom use is granted without fee provided th at copies\narenotmadeordistributed forproﬁtorcommercial advantag e andthat\ncopies bear this notice and thefull citation on the ﬁrstpage .\nc/circlecopyrt2006 University of Victoriarepresentations can be used as input to collaborative ﬁlter -\ningsystemsthat helpuserssearchformusic. Thedrawback\nof all these systems is that they require a novel song to be\nmanually annotatedbeforeit canberetrieved.\nAnother approach, called query-by-similarity , takes an\naudio-based query and measures the similarity between the\nquery and all of the songs in a database [1]. One draw-\nback of query-by-similarityis that it requiresa user to hav e\na useful audio exemplar in order to specify a query. For\ncases in which no such exemplar is available, researchers\nhavedeveloped query-by-humming [2],-beatboxing [3],and\n-tapping[4]. However, it can be hard, especially for an un-\ntrained user, to emulate the tempo, pitch, melody, and tim-\nbre well enough to make these systems viable [2]. A nat-\nural alternative is to describe music using words. A good\ndeal of research has focused on content-based classiﬁca-\ntionofmusicbygenre[5],emotion[6],andinstrumentation\n[7]. These classiﬁcation systems effectively ‘annotate’ m u-\nsic with class labels (e.g., ‘blues’, ‘sad’, ‘guitar’). The as-\nsumptionofapredeﬁnedtaxonomyandtheexplicitlabeling\nof songs into classes can give rise to a number of problems\n[8] dueto thefactthat musicisinherentlysubjective.\nWeproposeacontent-based query-by-text musicretrieval\nsystem that learns a relationship between acoustic feature s\nandwordsusingaheterogeneousdatasetofsongsandsong\nreviews. Our goal is to create a more general system that\ndirectly models the relationship between audio content and\na vocabulary that is less constrained than existing content -\nbased classiﬁcation systems. The query-by-text paradigm\nhas been largely inﬂuenced by work on the similar task of\nimageannotation. We speciﬁcallyadapta supervisedmulti-\nclassna¨ıveBayes [9] modelsince it hasperformedwell on\nthetaskofimageannotation. Thisapproachviewssemantic\nannotation as one M-class problem rather than Mbinary\none-vs-all problems where Mis the number of words in\na predeﬁned vocabulary. A comparative summary of alter-\nnative supervisedone-vs-all[10] and unsupervised[11, 12 ]\nmodelsforimageannotationispresentedin [9].\nDespite interest within the computer vision community,\nthere has been relatively little work on developing ’query-\nby-text’foraudio(andspeciﬁcallymusic)data. Oneexcep-\ntion is the work of Whitman [13, 14, 15]. Our approach\ndiffers from his in number of ways. First, he uses a set\nof web-documentsassociated with an artist whereaswe use\nsongreviewscreatedbyexperts. Second,hetakesaone-vs-\nallapproachandlearnsadiscriminativeclassiﬁer(asuppo rt\nvector machine or a regularizedleast-squares classiﬁer) f oreachterminthevocabulary. Weuseagenerativemulti-class\napproachbyestimatingaprobabilitydistributionoverafe a-\nture space for each term in our vocabulary. An advantage\nof our approachis that, for annotation, our model outputsa\nnatural ranking of words [9]. Other query-by-text audition\nsystems[16,17]havebeendevelopedforannotationandre-\ntrievalofsoundeffects.\n2. FeatureExtraction\nInordertomodeltherelationshipbetweensongsandwords,\neach song is represented by both the words extracted from\nthe text review and the audio features extracted from the\nacousticwaveform.\n2.1. TextFeatureExtraction\nA song review often contains semantic information about\ntheaudiocontent(e.g.,genre,instrumentation,emotion, style,\nrhythm)ofasong. Whilemuchofthisinformationcanonly\nbe extracted though sentence- or document-level compre-\nhension,modelingthesehigh-levelaspectsofproserequir es\nacomplexlanguagemodel. Instead,wefocusonword-level\ncomprehension using the bag-of-words representation. We\nreduceasongreviewtothesetofwords Wthatarefoundin\nboththereviewandourmusicalvocabulary V. Anexample\nof a song review and the associated bag-of-wordsrepresen-\ntationcanbefoundintheﬁrst two columnsofTable1.\nOur musical vocabulary consists of 317 musically infor-\nmativewordsthattheauthorshavehandpickedfromthelist\nof the 1200most commonwords foundin a corpusof song\nreviews. “Musically informative” means that the word\nmaydescribesomethingabouttheaudiocontent,asopposed\nto words whose meaning is historical, cultural, syntactica l\netc. We do not include common stop words (‘the’, ‘into’,\n‘a’),vaguewords(‘meaningful’,‘across’),orgeneralwor ds\n(‘song’,‘genre’). In addition,we preprocessthe text with a\ncustomstemmingalgorithmthatalterssufﬁxessothatsome\nwords, such as ‘guitar’ and ‘guitars’, are considered ident i-\ncal,whileothers,suchas‘blue’and‘blues’,remaindistin ct.\n2.2. AudioFeatureExtraction\nOur musical data set consists of MP3 audio ﬁles which we\nconverttosinglechannelaudiodatawith asamplingrateof\n22,050Hz. We examine two feature extraction techniques\nthat havebeenusefulforclassifyingmusicbygenre[5].\n2.2.1. DynamicMel-FrequencyCepstral Coefﬁcients\nMel-FrequencyCepstralCoefﬁcients(MFCCs)describethe\nspectral shape of a short-time audio frame in a concise and\nperceptually meaningful way and are popular features for\nspeech recognition and music classiﬁcation (e.g., [18, 19,\n16]). We calculate 13 MFCC coefﬁcients for each short-\ntime frameof512samples(23ms)ofaudio.\nIn an attempt to capture musically-relevantdetails about\nchanges between frames (e.g., beat onsets, rhythmic pulse,\npitch transitions), we collect a series of MFCC vectors and\nuse them to calculate dynamic MFCC (dMFCC) featuresFigure1. dMFCCfeatureextraction schematic\n(see Figure 1). We consider a texture window of 16640\nsamples (755ms) comprised of 64 half-overlapping short-\ntime frames. For each of the 13 MFCCs, we take a dis-\ncrete Fourier transform (DFT) over the texture window of\n64 points, normalize by the DC value (to remove the effect\nof volume) and summarize the resulting spectrum by inte-\ngrating across 4 bins: (unnormalized) DC, 1-2Hz, 3-15Hz\nand 20-43Hz. The resulting52 features(4 featuresforeach\nof the 13 MFCCs) describe a texture window. We further\nreduce the dimensionality by performing Principal Com-\nponents Analysis (PCA) [20] in the 52-dimensional vector\nspace to ﬁnd a projection into a 12-dimensional subspace.\nThese12principalcomponentsaccountfor99%ofthetrain-\ningdatavariance.\n2.2.2. AuditoryFilter-bankTemporalEnvelope\nTheauditoryﬁlter-banktemporalenvelope(AFTE)features\nextract information about the temporal and spectral char-\nacteristics of music (see Figure 2) [5]. In our implemen-\ntation, 743ms analysis windows of a sound waveform are\npassed through a biologically-inspired 18 channel gamma-\ntone ﬁlter-bank [21]. We examine the positive tempo-\nral envelope of these gammatone ﬁlter responses by recti-\nfying (squaring) each time series. We retain only the low-\nfrequency,slowly-modulatingenvelope by taking the abso-\nlute value of the DFT of the rectiﬁed, Hamming-windowed\nsignal and ignoringall of spectral componentsabove1kHz.\nTo summarize the spectrum of this temporal envelope in a\nconcise form that still retains much of its analytical capac -\nity, we look at the data in 4 chunks of the spectrum; DC,\n3-15Hz, 20-150Hz and 150-100Hz. With 18 gammatone\nﬁlters, this results in a total of 72 features describing eac h\n743msanalysiswindow. Againwe usePCAfordimension-\nalityreductionandrepresenteachfeaturevectorwith12co -\nefﬁcients. Thisprojectionaccountsfor95% of the variance\nin thetrainingset.\n3. Modeling MusicandWords\nConsider a vocabulary Vconsisting of Munique words.\nEach word wi∈ Vmay be a unigram, such as ‘happy’ or\n‘blues’,orabigram,suchas‘electric;guitar’or‘bob;dyl an’.\nThe goal in annotation is to ﬁnd a set W={w1, ..., w A}\nofAsemantically meaningful words that describe a query\nsongsq. Retrieval involves rank ordering a set of RsongsFigure2. AFTEfeatureextraction schematic\nS={s1, ..., s R}given a query Wq. It will be convenient\nto represent each annotation Was a binary vector y=\n{y1, ..., y M}where yi= 1ifwi∈ W, and0otherwise.\nWe representa song sas a set X={x1, ...,xT}ofTreal-\nvalued feature vectors where each vector, xt, is extracted\nfromashortsegment(e.g.,3/4seconds)oftheaudiocontent\nandTdependsonthelengthofthesong. Ourdataset Dwill\nthenberepresentedasa set {(X1,y1), ...,(XD,yD)}.\nAnnotation can be thought of as a multi-class classiﬁca-\ntionprobleminwhicheachword wi∈ Vrepresentsaclass.\nOur approach involves modeling a class-conditional distri -\nbution P(x|i), i∈ {1, ..., M }foreachword wi∈ V. Given\na query song represented by X={x1, ...,xT}, the Bayes\ndecisionruleforselectingtheindividualwordwiththemin -\nimumprobabilityoferrorisgivenby:\ni∗= arg max\niP(i|Xq) = argmax\niP(Xq|i)P(i)\nP(Xq),\nwhere P(i)isthepriorprobabilitythatword wiwill appear\nin an annotation. If we assume that xaandxb(∀a, b≤\nT, a/negationslash=b)areconditionallyindependentgivenword wi,then\ni∗= argmax\ni[T/productdisplay\nt=1P(xt|i)]·P(i).(1)\nWe assume a uniform prior ( P(i) = 1/Mfori= 1, .., M)\nsince the Tfactors in the product dominate the word prior.\nTakinglogarithmsresultsinourﬁnal annotation equation:\ni∗= arg max\niT/summationdisplay\nt=1logP(xt|i), (2)\nWhile the na¨ ıve Bayes assumption introduced in (1) is un-\nrealistic, modeling the interaction between feature vecto rs\nmaybeinfeasibleduetocomputationalcomplexityanddata\nsparsity. Computing (2) for each word creates an ordering\nfor all words in the vocabulary. To annotate a song, we se-\nlect the Awordsthatindividuallymaximizethisequation.\nForretrieval,wewanttorankallsongsinatestsetbased\non their conditional probability given a single-word query\nwq. We ﬁnd empirically that using the posterior P(X|q)\nalways returns the same ranking under every trained word\nmodel since some songs are much more likely than others.\nTheﬁrst reasonforthis isthat longersongs(with morefea-\nture vectors) have lower log likelihoods resulting from thesum of additional log probability terms. It has been argued\nthat the underestimation of the log likelihood is due to the\npoor conditional independence assumption in (1) between\nthe audio feature vectors [22]. The standard solution is to\ncalculate the averagelog posterior for each track (where T\nisproportionaltothelengthofthe song):\nX∗= argmax\nX1\nTT/summationdisplay\nt=1logP(xt|q).(3)\nThe second, more subtle source of bias is that the class\nconditional density functions P(x|q)for most feature vec-\ntors take on values very similar to the song prior density\nfunction P(x). This creates a song bias where songs that\nhave high likelihood under the prior distribution will have\nhighlikelihoodundermostoftheclassconditionaldistrib u-\ntions. We normalize for this song bias, P(X), and use the\nlikelihood P(q|X)insteadoftheposteriorfor retrieval:\nX∗= arg max\nXP(q|X)\n= arg max\nXP(X|q)P(q)\nP(X)\n= arg max\nX[/producttextT\nt=1P(xt|q)]·P(q)\n/summationtextM\ni=1[/producttextT\nt=1P(xt|i)]·P(i)\n= arg max\nX/summationtextT\nt=1logP(xt|q)\n/summationtextM\ni=1/summationtextT\nt=1logP(xt|i).(4)\nAgain,weassumeauniformwordpriorandtakelogarithms\nfor computational simplicity. Normalizing with the song\nbias effectively allows each song to place more weight on\nthewordsthathavehighest relativeposterior. Weranksongs\nby the weight that each song in the database places on the\nquery word. Note that the factor 1/Tintroduced in (3) to\naccountforthesonglengthcancelsoutin(4).\n4. ParameterEstimation\nForeachword wi,we learnthe parametersofthe classcon-\nditional density, P(x|i)using audio featuresfrom all songs\nwhich have wiin their associated annotations. The training\nsetTiforword wiconsistsofonlythe positiveexamples:\nTi={Xd: [yd]i= 1} (5)\nNote that the alternative supervised one-vs-all framework\nlearns a classiﬁer for each word in the vocabulary using\nboth the positive and negativeexamples, explicitly creating\na negative-class model [9]. This approach is problematic\nwhenusing a dataset thatis weakly labeled : the absenceof\na word from the annotation does not necessarily mean that\nthe song could not be correctly labeled with that word. In\nthis case, the negative-classmodel can not be learned from\ndata points that could have been positively labeled. Our\nmulti-classframeworkfocusesonlearningthepositive-cl ass\nmodelusingonlydatathatisknowntobepositivelylabeled.Figure3. (a)Directand(b)NaiveAveragingparameterestim a-\ntion. Arrows indicatethatparameters are learnedusingEM.\nWe learn a set of Mword-level conditionaldistributions\nP(x|i)fori= 1, ..., M, where each distribution is a C-\ncomponentmixtureofGaussiansdistributionparameterize d\nby{πc, µc,Σc}forc= 1, ..., C. The word-level distribu-\ntionforword wiisgivenby:\nP(x|i) =C/summationdisplay\nc=1πcN(x|µc,Σc)\nwhere N(·, µ,Σ)isamultivariateGaussiandistributionwith\nmeanµand covariance matrix Σ. We consider only diago-\nnalcovariancematricessinceusingfullcovariancematric es\ncan cause models to overﬁt the training data while scalar\ncovariancesdonotprovideadequategeneralization.\nWe consider two parameter estimation techniques: di-\nrect estimation and naive averaging [9]. Both techniques\nare similar in that, for each word wi∈ V, they use the\nExpectation-Maximization(EM)algorithmforﬁttingamix-\ntureofGaussians[20]distributiontothetrainingdataset Ti\ndescribed in (5). They differ in how they break down the\nparameter estimation problem into subproblems and merge\nthese resultstoproduceaﬁnal densityestimate.\n4.1. DirectEstimation\nDirect estimationtrainsa modelforeachword wiusingthe\nsuperset of feature vectors for all the songs that have word\nwiin the associated human annotation:/uniontextXd∀dsuch that\nXd∈ Ti. Usingthistrainingset,wedirectlylearntheword-\nlevel mixture of Gaussian distribution using the EM algo-\nrithm(Figure3a).\nThedrawbackofusingthismethodisthatcomputational\ncomplexityincreases with training set size. For example, i f\nTicontains200songsandthereareonaverageT=600fea-\nture vectorsper song, we must train each word-levelmodel\nusing 120,000 feature vectors. To learn a mixture of Gaus-\nsians distribution (with C= 32Gaussian components), it\ncan take many hours to train a single word-level model. A\nmoreseriousdeﬁciencyofthisestimationmethodisthatthe\nEM algorithm can converge to a bad local optimum since\ntheset of120,000featurevectorscancontainanyandall of\ntheoutliersthat existforthat wordclass.\n4.2. NaiveAveraging\nInstead of directly estimating a word-level distribution f or\nwi, we can ﬁrst learn song-level distributions: P(x|i, j),j∈1, ...,|Ti|where the variable jindicates a song. We\nuse EM to train a song-level distribution from the feature\nvectors extracted from that song. We then create a word-\nlevel distribution by averaging the song-level distributi ons\nofeachsongreviewedwith wi.Naiveaveraging givesequal\nweight to each song-level distribution, which results in th e\nfollowingdistribution:\nP(x|i) =1\n|Ti||Ti|/summationdisplay\nj=1K/summationdisplay\nk=1π(j)\nkN(x|µ(j)\nk,Σ(j)\nk)\nwhere Kis the number of song-level mixture components\n(Figure3b).\nTraining a model for each song in the training set and\nsummingthemisrelativelyefﬁcientbutthedrawbackofthis\nestimation technique is that the size of word-level models\ngrows with the size of the training database since there are\n|Ti|·Kcomponents. Usingtheexampleabove,if Ticontains\n200 songs and we model each song-level distribution with\nK= 8components then to evaluate the word-level model\nforafeaturevector x,weneedtoevaluatetheprobabilityof\nxunder1,600multivariateGaussiandistributions.\n5. Experimental Setup and Results\nIn this section, we quantitatively demonstrate that our sys -\ntemcanbothannotatesongswithanumberofrelevantwords\nand retrieve songs from database given a text query. We\nadopt similar evaluation methods to those used for image\nannotation [9, 12]. It should be noted that it is difﬁcult for\nus to directly compare our results with Whitman’s related\nwork [13] since much of his research focuseson evaluation\nofvocabularyselectionratherthanretrievalperformance .\nWe collect a set of 2,131 songs in MP3 format from our\npersonal collections and their associated song reviews. Re -\nviewsarenaturallanguagedocumentsdescribingindividua l\nsongscreatedbyhumanexpertsatAMGAllmusic[23](see\nTable 1). Reviews are parsed, stemmed and converted to\nbinary document vectors. Each review contains, on aver-\nage, 19 of the 317 words in our vocabulary. For each 2 to\n12 minute song, we extract one dMFCC and AFTE feature\nvectorfromhalf-overlapping3/4secondwindows. Afterap-\nplyingPCA,theresultingrepresentationisabagofbetween\n320and192012-dimensionalfeaturevectors.\nWerandomlypartitionourdataofsong-reviewpairsinto\na training set (80%) and a test set (20%). The training set\nis used for learning the PCA projection matrix and the pa-\nrameters for our each of our Mword-level distributions.\nThe test set is used for model evaluation. We consider two\nparameter estimation methods (direct with C= 32, naive\naveraging with K= 8) and two audio feature extraction\ntechniques(dMFCC, AFTE) for a total of four models. We\ncompare these models against three random baselines: ran-\ndom sample, prior stochastic, and prior deterministic. For\neach song, random sample picks words at random (with-\nout replacement) from our vocabulary to annotate a song.Table 1. Original review plusthebagof words, directmodel a ndrandom baselineannotationsfor theMonkees’“I’m aBelie ver”.\nHuman Review Bag ofWords ModelAnnotation Random Stochastic Prior Deterministic Prior\nThe best of the ’60s good-timepop songs and one of the most infec-\ntioussingleseverrecorded,“I’maBeliever”bythe Monkees grooves\nalong with a ragged accompaniment featuring handclaps and t am-\nbourine, fab electric piano solos , and a few well-timed, rushing-up-\nto-the-brink pauses just before the contagious, fervent chorus. For a\nsong so incredibly catchy, it’s hardly surprising that songwriter Neil\nDiamond(beforehistransformation intoa sexbombformiddle-aged\nfemalesacrossthe nation) and producer Jeff Barry wereresp onsible;\nthey were two of the most talented hires on the assembly line a t the\npop-song factoryknownastheBrillBuilding,responsiblefor dozens\nofhitsduring the’60s. “I’maBeliever”itselfranksasthethird mo st\npopular rock song of the ’60s, behind only the Beatles’“Hey Jude”\nand“IWanttoHoldYourHand.”Itspentsevenweeksatnumbero ne\ninAmerica,hitthetopof the chartsin Britain aswell,andcharted in\nover adozen countries...american monkees lyric debut;album lyric\nbeatles bouncy soundtrack bass guitar\ncatchy beatles perfect catchy band\nchorus witty subtle vocal vocal\ncomplex call;response electronic instrumental rock\nelectric descending led;zeppelin late pop\ngood john;lennon tone studio hit\nhigh pop;song contemporary love love\nhit beat roots blues melody\ninfectious british emotion chorus chorus\nlove\nmonkees\nmotown\npiano\n...\nPrior-stochastic sampleswords(withoutreplacement)from\na multinomial distribution parameterized by the word prior\ndistribution, P(i)fori= 1, ...,317, that are estimated us-\ning the word counts observed in the training set. Prior-\ndeterministic rankswordsaccordingtothewordpriors P(i)\nthusalwaysselectingthesame wordsforeveryannotation.\n5.1. Annotation\nUsing each model, we annotate all test set songs with the\n10 most likely words using (2). Annotation performanceis\nmeasured using mean per-word precision and recall. For\neach word w,|wH|is the number of songs that have win\nthe “human” song review. |wA|is the number of songs the\nmodel “automatically” annotates with w.|wC|is the num-\nberof“correct”wordsusedinboththesongreviewsandby\nthe model. Per-wordrecall is |wC|/|wH|andper-wordpre-\ncision is |wC|/|wA|. Mean per-word recall/precision is the\naverageoftheseratiosoverall317wordsinourvocabulary.\nSince precision is undeﬁned for words that the model\nneveruses,weactuallycompute smoothed precisionbyplac-\ningasmallnon-negativeweight /epsilon1/307oneachwordthatthe\nmodel did not use to annotate a test song ( /epsilon1= 10−4). The\nweight of a word that is used by the model is corrected to\n1−(/epsilon1/10)sothatthetotalweightdistributedacrossanyone\ntest song is 10. The smoothed estimate for words that are\nnot used by a model is approximatelythe word prior, P(i).\nWithout smoothing and deﬁning precision ≡0 for words\nwhere |wA|= 0, the precision of the deterministic prior\n(whichalways choosesthe same 10 words)is reducedfrom\n0.060 to 0.010 while mean precisions for all other models\nremainroughlyunaffected.\nQuantitative annotation results for the four models and\nthree random baselines are in Table 3. Models using dM-\nFCCfeaturesperformbestandsigniﬁcantlybeattherandom\nbaselinesby3timesinmeanrecalland2timesinmeanpre-\ncision. Table 1 shows example annotations created by one\nmodel(dMFCCs,directestimation)andrandombaselines.\n5.2. Retrieval\nFor each word wq, we rank the test songs in Saccording\nto (4) and calculate the mean averageprecision (mAP) [12]\nand the mean area under the receiver operating characteris-\ntic (ROC) curve (mAROC). Average precision is found by\nmovingdownourrankedlistoftestsongsandaveragingtheprecisions at everypoint where we correctly identify a new\nsong. The ROC curve plots true positive rate as a function\nof the false positive rate as we move down our ranked list\nof songs. The area under the ROC is found by integrating\nthe ROC curve. (Random guessing producesan area of 0.5\nasshownempiricallyinTable3). Columns4and5ofTable\n3 show mAP and mAROC found by averaging each metric\noverall thewordsinourvocabulary.\nSimilar to the annotation results, we see that our best\nmodels (direct and naive averaging with dMFCC) perform\nsigniﬁcantlybetterthanrandominbothmAP andmAROC.\nAlso,weseethatmodelstrainedusingdMFCCfeaturesout-\nperform those that use AFTE features. Qualitative retrieva l\nresultsforonesongareshowninTable 2.\n6. Discussion\nWhileourmodelssigniﬁcantlyoutperformtherandombase-\nlines, our best annotation results (recall = 0.09, precisio n =\n0.12) leave much room for improvement. State-of-the-art\ncontent-based image annotation systems report mean per-\nword recall and precisionscoresof about 0.25[9]. How-\never,the relativeobjectivityofthe tasks in the two domain s\nas well as the vocabulary, the quality of annotations, the\nfeatures, and the amount of data differ greatly between our\nmusicannotationsystemandexistingimageannotationsys-\ntemsmakinganydirectcomparisonssomewhatmisleading.\nIt should be noted that our “ground truth” human re-\nviewsrepresent noisyversionsofidealannotations. Amusic\nreviewer creating a document to describe a song does not\nmake explicit decisions about whether speciﬁc words that\nweincludeinourvocabularyarerelevantornot. Thus,rele-\nvantwordsareoftenomitted(weaklabeling)anderroneous\nwords can be included by our representation of the reviews\n(e.g.,“thissongdoesnotrock”). We expecttoimproveper-\nformanceinfutureworkbyreplacingnaturallanguagesong\nreviews with cleanannotations from a manually labeled a\ndata set. We also expect performance to improve with bet-\nter,automaticvocabularyselectionasin [14].\nOur system has explicitly been designed to be modular\nso that we can incorporate new training data, test different\nfeature extraction techniques and use alternative heterog e-\nneous data models. Numerous short and medium-time fea-\nture extraction techniqueshave been proposedmy the MIRTable 2. Test set songs retrieved by our model usingthe query\nword“punk;rock”andsongsinwhich“punk;rock”appearsin\nthe associated songreview.\nAutomatically Retrieved Manually Reviewed\nsmashing pumpkins-cherub rock clash-the guns of brixton\nramones-pinhead r.e.m.-radio free europe\nguns n roses-you could bemine ramones-cretin hop\nneutral milk hotel-holland 1945 replacements-answering machine\nbuilt to spill-you were right stooges-t.v. eye\nreplacements-answering machine television-see no evil\ncheap trick-dream police\noasis-supersonic\ngerms-manimal\nweezer-buddy holly\nTable 3. Annotation and retrieval results. All models perfo rm\nsigniﬁcantlybetterthanthe‘randomsample’baselineinao ne-\nsided, pairedt-test ( α= 0.01). Recall = mean per-word recall,\nPrec =mean per-word smoothed precision,mAP = mean aver-\nage precision,mAROC =mean area underthe ROCcurve.\nModel Annotation Retrieval\nRecall Prec mAP mAROC\nRandom Baselines\nRandom Sample 0.030 0.060 0.071 0.49\nPrior (Stochastic) 0.032 0.060 0.072 0.50\nPrior (Deterministic) 0.032 0.060 0.068 0.50\ndMFCC Features\nDirect 0.087 0.108 0.105 0.60\nNaive Averaging 0.072 0.119 0.109 0.61\nAFTEFeatures\nDirect 0.067 0.089 0.092 0.58\nNaive Averaging 0.055 0.110 0.097 0.59\ncommunity, such as those based on psychoacoustic models\n[5] and autoregression[24]. We plan to explorethese exist-\ningtechniquesaswellastodesignnovelmethodsspeciﬁcto\nthe “query-by-text” annotation/retrieval tasks. We are al so\ninterested in a music model that takes account of the tem-\nporal relationshipsbetween features(e.g., a hiddenMarko v\nmodel[25])asanalternativetoour“bag-of-feature-vecto rs”\nrepresentation. We plan to implementalternativeparamete r\nestimationtechniques,suchasMixtureHierarchiesEM[9],\nandexperimentwithunsupervisedmodels[11, 12].\nOne topic not addressed by this paper is retrieval with\nmulti-wordqueries. Wecanimagineoneapproachthatcom-\nbines rankings output from individual word models or an-\nother approach that merges word-level distributions (e.g. ,\nusingnaiveaveraging)tocreatea“query-level”distribut ion.\nAcknowledgments\nWe would like to thank N. Vasconcelos, A. Chan, and R.\nManmatha and the anonymous reviewers for their helpful\ncomments. WeusedM.Slaney’sAuditoryToolboxforGam-\nmatone ﬁltering and MFCC computation [26], A. McCal-\nlum’s Rainbow [27] for text parsing and I. Nabney’s NET-\nLAB[28]formixturemodeltraining.\nReferences\n[1] M.GotoandK.Hirata. Recentstudiesonmusicinformatio n\nprocessing. Acoustical Science and Technology , 25(4):419–\n425, 2004.[2] R. B. Dannenberg and N. Hu. Understanding search perfor-\nmance inquery-by-humming systems. ISMIR,2004.\n[3] A. Kapur, M. Benning, and G. Tzanetakis. Query by beat-\nboxing: Musicinformationretrievalforthedj. ISMIR,2004.\n[4] Gunnar Eisenberg, Jan-Mark Batke, and Thomas Sikora.\nBeatbank - an mpeg-7 compliant query by tapping system.\nAudioEngineering Society Convention , 2004.\n[5] M. F. McKinney and J. Breebaart. Features for audio and\nmusic classiﬁcation. ISMIR, 2003.\n[6] T. Li and G. Tzanetakis. Factors inautomatic musical gen re\nclassiﬁcationof audio signals. IEEEWASPAA ,2003.\n[7] S. Essid, G. Richard, and B. David. Inferring efﬁcient hi -\nerarchical taxonomies for mir tasks: Application to musica l\ninstruments. ISMIR,2005.\n[8] F. Pachet and D. Cazaly. A taxonomy of musical genres.\nRIAO,2000.\n[9] G. Carneiro and N. Vasconcelos. Formulating semantic im -\nage annotation as a supervised learning problem. IEEE\nCVPR,2005.\n[10] D. Forsythand M. Fleck. Body plans. IEEECVPR ,1997.\n[11] D.M.BleiandM.I.Jordan. Modelingannotateddata. ACM\nSIGIR,2003.\n[12] S. L. Feng, R. Manmatha, and V. Lavrenko. Multiple\nbernoulli relevance models for image and video annotation.\nIEEECVPR ,2004.\n[13] B. Whitman. Learning the meaning of music . PhD thesis,\nMassachusetts Instituteof Technology, 2005.\n[14] B.WhitmanandD.Ellis. Automaticrecordreviews. ISMIR,\n2004.\n[15] B.WhitmanandR.Rifkin. Musical query-by-descriptio n as\na multiclass learningproblem. IEEEMMSP .\n[16] M. Slaney. Semantic-audio retrieval. IEEEICASSP ,2002.\n[17] P.CanoandM.Koppenberger. Automaticsoundannotatio n.\nInIEEEworkshoponMachine LearningforSignalProcess-\ning, 2004.\n[18] L. Rabiner and Biing-Hwang Juang. Fundamentals of\nSpeech Recognition . Prentice Hall,1993.\n[19] B. Logan. Mel frequency cepstral coefﬁcients for music\nmodeling. ISMIR, 2000.\n[20] T. Hastie, R. Tibshirani, and J. Friedman. The elements of\nstatistical learning: Data mining, inference, and predict ion.\nSpringer, 2001.\n[21] W. M. Hartmann. Signals, Sound, and Sensation . Springer-\nVerlag, 1998.\n[22] D.Reynolds,T.F.Quatieri,andR.B.Dunn. Speakerveri ﬁca-\ntion using adapted gaussian mixture models. Digital Signal\nProcessing , 10:19–41, 2000.\n[23] AMG. Allmusic guide. http://www.allmusic.com.\n[24] A. Meng, P. Ahrendt, and J. Larsen. Improving music\ngenre classiﬁcation by short-time feature integration. IEEE\nICASSP,2005.\n[25] A. Flexer, E. Pampalk, and G. Widmer. Novelty detection\nfor spectral similarityof songs. ISMIR,2005.\n[26] M. Slaney. Auditory toolbox. Interval Re-\nsearch Corporation Technical Report 1998-010 .\nhttp://rvl4.ecn.purdue.edu/ malcolm/interval/1998-01 0/.\n[27] A. K. McCallum. Bow: A toolkit for statistical lan-\nguage modeling, text retrieval, classiﬁcation and cluster ing.\nhttp://www.cs.cmu.edu/ mccallum/bow, 1996.\n[28] I. Nabney and C. Bishop. Netlab toolbox.\nhttp://www.ncrg.aston.ac.uk/netlab/."
    },
    {
        "title": "Bayesian Modelling of Temporal Structure in Musical Audio.",
        "author": [
            "Nick Whiteley",
            "Ali Taylan Cemgil",
            "Simon J. Godsill"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1415138",
        "url": "https://doi.org/10.5281/zenodo.1415138",
        "ee": "https://zenodo.org/records/1415138/files/WhiteleyCG06.pdf",
        "abstract": "This paper presents a probabilistic model of temporal struc- ture in music which allows joint inference of tempo, meter and rhythmic pattern. The framework of the model natu- rally quantifies these three musical concepts in terms of hid- den state-variables, allowing resolution of otherwise appar- ent ambiguities in musical structure. At the heart of the sys- tem is a probabilistic model of a hypothetical ‘bar-pointer’ which maps an input signal to one cycle of a latent, periodic rhythmical pattern. The system flexibly accommodates dif- ferent input signals via two observation models: a Poisson points model for use with MIDI onset data and a Gaussian process model for use with raw audio signals. The discrete state-space permits exact computation of posterior proba- bility distributions for the quantities of interest. Results are presented for both observation models, demonstrating the ability of the system to correctly detect changes in rhythmic pattern and meter, whilst tracking tempo. Keywords: tempo tracking, rhythm recognition, meter recog- nition, Bayesian inference",
        "zenodo_id": 1415138,
        "dblp_key": "conf/ismir/WhiteleyCG06",
        "keywords": [
            "probabilistic model",
            "temporal structure",
            "tempo",
            "meter",
            "rhythmic pattern",
            "hidden state-variables",
            "latent periodic rhythmical pattern",
            "observation models",
            "discrete state-space",
            "posterior probability distributions"
        ],
        "content": "BayesianModelling of TemporalStructure in MusicalAudio\nNickWhiteley,A.TaylanCemgil andSimonGodsill\nSignalProcessingGroup,UniversityofCambridge\nDepartmentofEngineering,TrumpingtonStreet,Cambridge ,CB2 1PZ,UK\n{npw24, atc27, sjg }@eng.cam.ac.uk\nAbstract\nThispaperpresentsa probabilisticmodeloftemporalstruc -\nture in music which allows joint inference of tempo, meter\nand rhythmic pattern. The framework of the model natu-\nrallyquantiﬁesthesethreemusicalconceptsintermsofhid -\nden state-variables, allowing resolution of otherwise app ar-\nentambiguitiesinmusicalstructure. Attheheartofthesys -\ntem is a probabilistic model of a hypothetical ‘bar-pointer ’\nwhichmapsaninputsignaltoonecycleofalatent,periodic\nrhythmicalpattern. The system ﬂexibly accommodatesdif-\nferent input signals via two observation models: a Poisson\npoints model for use with MIDI onset data and a Gaussian\nprocess model for use with raw audio signals. The discrete\nstate-space permits exact computation of posterior proba-\nbility distributionsfor the quantitiesof interest. Resul ts are\npresented for both observation models, demonstrating the\nabilityofthesystemtocorrectlydetectchangesinrhythmi c\npatternandmeter,whilst trackingtempo.\nKeywords: tempotracking,rhythmrecognition,meterrecog-\nnition,Bayesianinference\n1. Introduction\nIn construction of intelligent music systems, an important\nperceptualtask ishowto inferattributesrelatedto tempor al\nstructure. These attributes may include musicological con -\nstructssuch asmeterandrhythmicpattern. Therecognition\nof these characteristics forms a sub-task of automatic mu-\nsictranscription-theunsupervisedgenerationofascore, or\ndescriptionofanaudiosignalin termsofmusicalconcepts.\nForinteractiveperformancesystems, especiallywhenan\nexact score is a-prioriunknown, it is crucial to construct\nrobust algorithms that can correctly operate under rhyth-\nmic ﬂuctuations, ritardando/accelarando (systematic slo w-\ningdownorspeedingup),metricmodulations,etc. Formu-\nsic categorisation systems, tempo and rhythmic pattern are\ndeﬁning features of genre. It is therefore apparent that a\ncomplete system should be able to recognise all these fea-\ntures.\nPermission to makedigital orhardcopies ofallorpartofthi s workfor\npersonal or classroom use is granted without fee provided th at copies\narenotmadeordistributed forproﬁtorcommercial advantag e andthat\ncopies bear this notice and thefull citation on the ﬁrstpage .\nc/circlecopyrt2006 University of VictoriaMuch work has been done on detecting the ‘pulse’ or\nfoot-tapping rate of musical audio signals. Existing algo-\nrithmic approachesdealingwith raw audiosignals focuson\nthe extraction of some frequency variable representing thi s\nrate [1],[2]. However these approaches do not detect more\ncomplex temporal characteristics, such as meter and rhyth-\nmic pattern, which are needed for complete transcription.\nGoto and Muraoka detail a multiagent hypothesis system\nwhich recognises beats in terms of the ‘reliability’ of hy-\npotheses for different rhythmic patterns, given a ﬁxed 4/4\nmeter [3].This system was later extended to deal with non-\npercussivemusic[4].\nCemgil and Kappen also introduce musical concepts by\nmodelling MIDI onset events in terms of a tempo process\nandswitchesbetweenquantisedscorelocations[5]. Raphae l\nindependently proposed a similar system [6]. Hainsworth\nand Macleod build on some elementsof this work inferring\nbeatsfromrawaudiosignalsusinganonsetdetector[7],but\nin these works meter and rhythmic pattern are still not ex-\nplicitlymodelled.\nApproaches to meter detection include ‘Gaussiﬁcation’\nof onsets [8], similar to the Tempogram representation of\nCemgil et. al. [9], autocorrelation methods, [10],[11] and\npreference-rulebasedsystems, [12],[13]. Pikrakiset al. ex-\ntractmeterandtempo,assumingthatmeterisconstant[14].\nTakeda et al. perform tempo and rhythm recognition from\na MIDI recordingby analogywith speech-recognition[15].\nKlapuri et. al. deﬁne metrical structure in terms of pulse\nsensationsondifferenttimescales[16]. Thissystem issuc -\ncessfulindetectingperiodicityonthesetimescales,butd oes\nnot yield an explicit estimate of musical meter in terms of\nstandardmusicalnotation: 3/4,4/4etc.\nInthispaperwefocusonthreemusicalconcepts: tempo,\nmeter and rhythmic pattern. The strength of the system\npresented here, compared to the existing works described\nabove, is that it explicitly models these three concepts in\na consistent framework which resolves otherwise apparent\nambiguities in musical structure, for example switches in\nrhythmic pattern or meter. Furthermore, the probabilistic\napproachpermitsrobust inferenceofthe quantitiesofinte r-\nest, ina principledmanner.\nIn the Bayesian paradigmone views tempo trackingand\nrecognitionofmeterandrhythmicpatternasalatentstatei n-\nference problem, where given a sequence of observed dataY1:K, we wish to identify the most probable hidden state\ntrajectory X0:K. Firstly we need to postulate prior distri-\nbutions over X0:K. Secondly, an observation model is de-\nﬁnedwhichrelatestheobservationstothehiddenvariables .\nThe posterior distribution over hidden variables is given b y\nBayes’ theorem:\np(X0:K|Y1:K) =p(Y1:K|X0:K)p(X0:K)\np(Y1:K)(1)\nThisadmitsarecursionforonline(‘causal’),potentially real-\ntime computationofﬁlteringdistributionsoftheform\np(Xk|Y1:k),fromwhichestimatesofthecurrenttempo,rhyth-\nmic pattern and meter can be made, given observations up\nto thepresenttime. Deﬁning αk≡p(Xk|Y1:k)p(Y1:k),\nαk=p(Yk|Xk)/integraldisplay\ndXk−1p(Xk|Xk−1)αk−1(2)\nFor off-lineinference, smoothing may be performed,which\nconditions on future as well as present and past observa-\ntions. Intuitively, smoothing is the retrospective improv e-\nmentofestimates. Deﬁning βk≡p(Yk+1:K|Xk),\nβk=/integraldisplay\ndXk+1p(Xk+1|Xk)p(Yk+1|Xk+1)βk+1(3)\nAsmoothingdistributionforasingletimeindexmaybeob-\ntainedintermsofthe correspondingﬁlteringdistribution :\np(Xk|Y1:K)∝αkβk (4)\n2. DynamicBarPointerModel\nWeheredeﬁnethe‘barpointer’asbeingahypothetical,hid-\nden object located in a space consisting of one period of a\nlatent rhythmical pattern, i.e. one bar. The velocity of the\nbarpointerisdeﬁnedtobeproportionaltotempo,measured\nin quarter notes per minute. Note that we therefore avoid\nexplicitly modelling hierarchical periodicity, as descri bed\nfor example in [16] in terms of measure, tactus and tatum\nperiods, but such quantities could be be extracted from the\nmodelifrequired.\nIn qualitative terms, for a given rhythmicalpattern there\nare locations in the bar at which onsets will occur with rel-\natively high probability. This concept is quantiﬁed and re-\nlated to observed signals in section 3 below. In this section\nwe deﬁnethe priormodelforthebarpointerdynamics.\nWe choose to deﬁne a discrete ‘position’ space in terms\nofMuniformally spaced points in the interval [0,1). De-\nnote by mk∈ {1,2, ..., M }the index of the location of\nthe bar-pointer in this space at time k∆, where ∆andk∈\n{1,2, ..., K }are respectively the discrete time period and\nindex. Next deﬁne a discrete ‘velocity’ space with Nel-\nements and denote by nk∈ {1,2, ..., N }the index of the\nvelocity of the bar pointer at time index k. A similar con-\nstruction called a ‘score positionpointer’is deﬁned in [17 ],butforthismodelexactinferenceisintractableandswitch es\nin rhythmicpatternandmeterare notexplicitlymodelled.\nOver time the position and velocity indices of the bar\npointerevolveaccordingto:\nmk+1= (mk+nk−1)mod(Mθk) + 1 (5)\nfor1< nk< N,\np(nk+1|nk) =\n\npn\n2, n k+1=nk±1\n1−pn, n k+1=nk\n0, otherwise(6)\nwhere pnis the probability of a change in velocity. At a\nboundary, nk= 1ornk=N, the velocity either remains\nconstantwith probability 1−pn, ortransitionsrespectively\ntonk+1= 2ornk+1=N−1withprobability pn. Amod-\nulo operationis implied in a similar context in [6], to allow\ncalculationofnotelengthsin termsofquantisedscoreloca -\ntions. In the dynamic bar pointer model, this modulo oper-\nation is made explicit and exploited to allow representatio n\nofswitchesinmeter. Themeterindicatorvariable, θk,takes\none valuein a ﬁnite set, forexample θk∈T={3/4,4/4},\nat each time index k. Thisfacilitates modellingof switches\nbetween 3/4and4/4meters during a single musical pas-\nsage and is illustrated in ﬁgure 1. The advantage of this\n| | |\n3 • • • • • • • •\nnk 2 • • • • • • • •\n1 • • • • • • • •\n1 2 3 4 5 6 7 8\nmk3/4 time4/4 time\nFigure 1: Toy example of the position and velocity state sub-\nspace for M= 8,N= 3. Solid lines indicate examples of\npossible state transitions and dotted lines indicate the ef fect of\nthe modulo operation for different meters. The implication is\nthat, for a given tempo, one bar in 3/4 meter is simply 3/4 the\nlengthofonebarin4/4meter. Theconceptgeneralisestooth er\nmeters, eg bars in 2/4, 3/4, and 4/4 can all be represented as\nsubsetsofabarin5/4meter. Notethatinpractice Mwouldbe\nchosen muchlarger.\napproach compared to the existing resonator-based system\nof Scheirer, [1], is that the bar pointer continues to move\nwhether or not onsets are observed. This explicitly models\nthe concept that tempo is a latent process and provides ro-\nbustness against rests in the music which might otherwise\nbe wrongly interpreted as local variations in tempo. Large\nand Kolen formulatea phase lockingresonator,but it is not\ngivena fullprobabilistictreatment[18].Switches in meter are modelled as occurring when the\nbar-pointerpassestheendofthebar:\nformk< m k−1,\np(θk|θk−1, mk, mk−1) =/braceleftbiggpθ, θ k∝ne}ationslash=θk−1\n1−pθ, θk=θk−1(7)\notherwise,\np(θk|θk−1, mk, mk−1) =/braceleftbigg\n0, θk∝ne}ationslash=θk−1\n1, θk=θk−1(8)\nwhere pθis the probability of a change in meter at the end\nofthebar.\nThelaststate variableisarhythmicpatternindicator, rk,\nwhich takes onevalue in a ﬁnite set, forexample rk∈S=\n{0,1},at each time index k. The elements of the set Scor-\nrespond to different rhythmic patterns, described in secti on\n3. For now we deal with the simple case in which there are\nonly two such patterns, andswitching betweenvaluesof rk\nismodelledasoccurringat theendofthe bar:\nformk< m k−1,\np(rk|rk−1, mk, mk−1, θk−1) =/braceleftbiggpr, r k∝ne}ationslash=rk−1\n1−pr, rk=rk−1\n(9)\notherwise,\np(rk|rk−1, mk, mk−1, θk−1) =/braceleftbigg\n0, rk∝ne}ationslash=rk−1\n1, rk=rk−1(10)\nwhere pris the probability of a change in rhythmic pattern\nat theendofabar.\nInsummary, Xk≡[mknkθkrk]Tspeciﬁesthestate of\nthe system at time index k. For computation, the set of all\npossiblestatesmaybearrangedintoavector xandthestate\nof the system at time kmay then be represented by Xk=\nx(i), ie the ith element of this vector. Using equations 5 -\n10,atransitionmatrix Amaythenbeconstructedsuchthat:\nA(i, j) =p(Xk+1=x(j)|Xk=x(i))(11)\nFor a value of Mwhich is high enoughto give useful reso-\nlution, eg. 1000, and a suitable value of N, eg20, this ma-\ntrix is large, but extremely sparse, making exact inference\nviable.\n3. ObservationModels\n3.1. PoissonPoints\nDenote by ykthe numberof MIDI onseteventsobservedin\nthekthnon-overlappingframeoflength ∆. All otherMIDI\ninformation,(eg. key,velocity,duration)isdisregarded .The\nnumber ykismodelledasbeingPoissondistributed:\np(yk|λk) =λyk\nkexp(−λk)\nyk!(12)A gamma distribution is placed on the intensity parameter\nλk. This provides robustness against variation in the data.\nThe shape and rate parameters of the gamma distribution,\ndenotedby akandbkrespectively,arefunctionsofthevalue\nof a rhythmicpatternfunction, µ(mk, rk). Themean of the\ngamma distribution is deﬁned to be the value of this rhyth-\nmic patternfunction,which quantiﬁesknowledgeabout the\nprobable locations of note onsets for a given rhythmic pat-\ntern. This formalises the onset time heuristics given in [4] .\nExamples of rhythmic pattern functions are given in ﬁgure\n2. Forbrevitydenote µk≡µ(mk, rk).\np(λk|mk, rk) =bak\nkexp(−bkλk)\nΓ(ak)λak−1\nk(13)\nak=µ2\nk/Qλ (14)\nbk=µk/Qλ (15)\nwhere Qλis the variance of the Gamma distribution, the\nvalueofwhichischosentobeconstant.\nInferenceoftheintensityparameter λkisnotrequiredso\nit isintegratedout. Thismaybe doneanalytically,yieldin g:\np(yk|mk, rk) =bak\nkΓ(ak+yk)\nyk!Γ(ak)(bk+ 1)ak+yk(16)\nFigure3givesagraphicalrepresentationofthecombinatio n\nof the Poisson Points observation model and the Dynamic\nBar-Pointermodel.\n01002003004005006007008009001000024\nmkµkTriplet Rhythm\n01002003004005006007008009001000024\nmkµkDuplet Rhythm\nFigure2: Twoexamplerhythmicpatternfunctionsforusewit h\nthe Poisson points observation model, each correspondingt o a\ndifferent value of rk. Top - a bar of triplets in 4/4 meter, Bot-\ntom-abarofdupletsin4/4meter. Thelocationsofthepeaksi n\nthe functioncorrespond tothe locations of probableonsets for\na given rhythmic pattern. The heights of the peaks imply the\naverage numberof onset events at thecorrespondingbar loca -\ntions,viaagammadistribution. Thewidthsofthepeaksmode l\narpeggiationofchordsandexpressiveperformance. Constr uc-\ntionintermsofsplinespermitsﬂatregionsbetweenpeaks,c or-\nrespondingtoanonset event ‘noiseﬂoor’.n0 n1 n2 n3\nθ0 θ1 θ2 θ3\nm0 m1 m2 m3\nr0 r1 r2 r3\nλ1 λ2 λ3\ny1 y2 y3\nFigure3: Graphicalrepresentationofthebar-pointermode lin\nconjunction with the Poisson points observation model. Eac h\nnode corresponds to a random variable. Directed links denot e\nstatisticaldependence: eachnodeisconditionallyindepe ndent,\ngiven the values of its‘parent’ nodes. The graph for theGaus -\nsian process modelis exactly equivalent.\n3.2. GaussianProcess\nThemotivationforspecifyingthissecondobservationmode l\nis to demonstrate that the dynamic bar pointer framework\ncanbeemployedwithrawaudioaswellasMIDIonsetdata.\nTheGaussianprocessmodelpresentedhereisthereforesim-\nple and is suitable for percussive sounds only. However, it\nshould be noted that this observationmodel could easily be\nmodiﬁed to operate on other feature streams, such as those\nin deﬁned in [2], taking into account changes in spectral\ncontent.\nDenote by zka vector of νsamples constituting the kth\nnon-overlappingframeofarawaudiosignal. Thetimeinter-\nval∆is thengivenby ∆ =ν/fs, where fsis the sampling\nrate. The samples are modelled as independentwith a zero\nmeanGaussiandistribution:\np(zk|σ2\nk) =1\n(2πσ2\nk)ν/2exp/parenleftbigg\n−zT\nkzk\n2σ2\nk/parenrightbigg\n(17)\nAninverse-gammadistributionisplacedonthevariance σ2\nk.\nTheshapeandscaleparametersofthisdistribution,denote d\nbyckanddkrespectively,aredeterminedbythelocationof\nthe bar pointer, mkand the rhythmicpattern indicator vari-\nablerk, again via a rhythmic pattern function, µk. An ex-\nampleofarhythmicpatternfunctionforusewiththismodel\nisshowninﬁgure4.\np(σ2\nk|mk, rk) =dck\nkexp(−dk/σ2\nz)\nΓ(ck)σ−2(ck+1)\nk(18)\nck=µ2\nk/Qs+ 2 (19)\ndk=µk/parenleftbiggµ2\nk\nQs+ 1/parenrightbigg\n(20)where Qsis the variance of the inverse-gammadistribution\nandischosento beconstant.\nThe variance of the Gaussian distribution, σ2\nk, may be\nintegratedoutanalyticallytoyield:\np(zk|mk, rk) =dck\nkΓ(ck+ν/2)\n(2π)ν/2Γ(ck)/parenleftbiggzT\nkzk\n2+dk/parenrightbigg−(ck+ν/2)\n(21)\n0100200300400500600700800900100000.10.20.30.4\nmkµkDuplet Rhythm\nFigure 4: A duplet rhythmic pattern function for use with\ntheGaussianProcessobservation model,correspondingtoo ne\nvalue of rk. The function is piece-wise exponential: a simple\nmodelof energytransientsfor percussiveonsets. Thelocat ions\nof the peaks correspond to temporal locations of probable on -\nsets for a given rhythmical pattern. The heights of the peaks\nimply the average signal power at the corresponding bar loca -\ntions,viaan inverse-gamma distribution.\n4. Inference Algorithm\nComputation of posterior marginal ﬁltering and smoothing\ndistributions can be performed exactly using the forwards-\nbackwards algorithm, see [19] and references therein for\ndetails. At each time step, the forward pass of the algo-\nrithmrecursivelycomputesaso-called‘alphamessage’vec -\ntor,αk. Each element of this vector, αk(i), is proportional\ntop(Xk=x(i)|y1:k)- the corresponding element of the\nﬁlteringdistribution1. Therecursionis givenby:\nαk+1=OkATαk (22)\nα0=p(X0) (23)\nwhereAis the state transition matrix as previouslydeﬁned\nandOkisa diagonalmatrix:\nOk(j, j) =p(yk|Xk=x(j)) (24)\nTheforwardpasscanpotentiallybecarriedoutinrealtime.\nThebackwardpassofthealgorithmcomputesthebetames-\nsages, βk,whicharethencombinedwiththealphamessages\nto givethecorrespondingsmoothingdistribution:\nβk=Ok+1Aβk+1 (25)\nβK=1 (26)\n1Notation for the Poisson points observation model. For the G aussian\nprocess model, replace y1:kwithz1:k.p(Xk|y1:K)∝αk∗βk (27)\nwhere 1is a vector of ones and ∗represents element-wise\nproduct.\n5. Results\n5.1. MIDIOnset Events\nFigure5showsresultsforanexcerptofaMIDIperformance\nof‘Michelle’bytheBeatles,demonstratingthejointtempo -\ntracking and rhythm recognition capability of the system.\nThe performance, by a professional pianist, was recorded\nusing a Yamaha Disklavier C3 Pro Grand Piano. The Pois-\nson points observation model was employed with the two\nrhythmic patterns in ﬁgure 2 and a single value of θk= 1,\nie 4/4 meter. Uniform initial prior distributions were set o n\nmk,nkandrk, with M= 1000andN= 20. The time\nframe length was set to ∆ = 0 .02s, corresponding to the\nrange of tempi: 12−240quarter notes per minute. The\nprobability of a change in velocity from one frame to the\nnext was set to pn= 0.01and the probability of a change\ninrhythmicpatternwassetto pr= 0.1. Thevarianceofthe\nGammadistributionwasset Qλ= 10.\nThis section of ‘Michelle’ is potentially problematic for\ntempotrackersbecauseofthetriplets,eachofwhichbydef-\ninitionhasadurationof3/2quarternotes. Aperformanceof\nthis excerpt could be wrongly interpreted as having a local\nchange in tempo in the second bar, when really the rate of\nquarter notes remains constant; the bar of triplets is just a\nchangeinrhythm.\nInﬁgure5,thestrongdiagonalstripesintheimageofthe\nposterior smoothing distributions for mkcorrespond to the\nmaximum a-posteriori (MAP) trajectory of the bar pointer.\nThesystemcorrectlyidentiﬁedthechangetoatripletrhyth m\nin the second bar and the subsequent reversion to duplet\nrhythm. The MAP tempo is given by the darkest stripe in\nthe image for the velocity log-smoothing distribution - it i s\nroughlyconstantthroughout.\n5.2. RawAudio\nA percussive pattern with a switch in meter (score given in\nﬁgure 6) was performed and recorded in mono .wav for-\nmat at fs= 11.025kHz. The system was then run on this\nraw audio signal using the Gaussian Process observation\nmodel,todemonstratejointtempotrackingandmeterrecog-\nnition. The single rhythmic pattern function given in ﬁg-\nure4wasemployedinconjunctionwithtwometersettings:\nθk∈ {3/4,4/4}. The probability of a change in velocity\nwas set pn= 0.01and the probability of a change in me-\nter was set pθ= 0.1. The variance of the inverse-gamma\ndistribution was set Qs= 10. The frame length in samples\nwas set ν= 256withM= 1000andN= 20, corre-\nspondingtotherangeoftempi: 10.3−208quarternotesper\nminute. Uniform initial prior distributions were set on all\nhiddenvariables.050100150200250300350400450012ykObserved Data\nmklog p(mk|y1:K)\n  \n50100150200250300350400450800\n600\n400\n200 −10−50\nQuarter notes per min.log p(nk|y1:K)\n  \n50100150200250300350400450180\n120\n60\n−10−50\np(rk|y1:K)\nFrame Index, k  \n50100150200250300350400450Triplets\nDuplets0.20.40.60.8\nFigure 5: Results for joint tempo tracking and rhythmic pat-\ntern recognition on a MIDI performance of ‘Michelle’ by the\nBeatles. The topﬁgureis thescore whichthepianistwas give n\nto play. Each image consists of smoothing marginal distribu -\ntions for each frame index.\nWhilsteachsectionofthispercussivepatternmayappear\nsimple at ﬁrst glance, it poses two potential challenges for\ntempotrackers. Firstly,thelackofeighthnotesinthemidd le\ntwo bars could incorrectly be interpreted as a reduction in\ntempo by a factor of two. However, the tempo measured\nas the rate of quarter notes remains constant. Secondly, the\nmeterswitchinthesametwobarsdisruptstheaccentpattern\nestablishedinbars1,2,5and6.\nFigure 6 shows that the system correctly identiﬁed the\nmetrical modulation. Note that the system is robust to the\nlack of eighth notes in the third and fourth bars; the MAP\ntempoisroughlyconstantthroughout.\nAudio ﬁles and supporting materials for further results\nmay be found on the web at http://www-sigproc.\neng.cam.ac.uk/ ˜npw24/ismir06/\n6. Discussion\nA model of temporal characteristics of music has been pre-\nsented, based around the probabilistic dynamics of a hypo-\nthetical bar pointer. Two observationmodels accommodate\nMIDI onset eventsor percussiveraw audio,relatingthe ob-\nserveddatatothelocationofthebarpointer. Exactinferen ce0 2 4 6 8 10 12−101sample value\ntime, sObserved Data\nmklog p(mk|z1:K)\n  \n100 200 300 400 500800\n600\n400\n200 −10−50\nQuarter notes per min.log p(nk|z1:K)\n  \n100 200 300 400 500155\n103\n52\n−10−50\np(θk|z1:K)\nFrame Index, k  \n100 200 300 400 5004/4\n3/40.20.40.60.8\nFigure 6: Results for joint tempo tracking and meter recog-\nnition from raw audio. The top most ﬁgure is the percussion\nscore for thepiece.\noftempo,rhythmicpatternandmetermaybeperformedon-\nline(andpotentiallyrealtime)fromposteriorﬁlteringdi stri-\nbutions,suitableforautomaticaccompanimentapplicatio ns.\nForanalysisandinformationretrievalpurposes,off-line op-\neration yields smoothing posterior distributions. Demon-\nstrationsofthecapabilitiesofthesystemwerepresentedf or\ntwo pieces, one involvinga change in rhythmic pattern and\nthe other a switch in meter. The results show that the sys-\ntem robustly handlessuch temporal variationswhich might\ndefeatsimpletempotrackers.\nWe are currently investigating how this model could be\nextended to use MIDI volume information, via a marked\nPoisson process. Higher tempo resolution, larger numbers\nofmetersandrhythmicpatternswouldrequireanevenlarger\ntransition matrix, ultimately exceeding practical comput a-\ntional limits. Future work will therefore investigate the a p-\nplication of faster, approximate inference schemes, such a s\nparticle ﬁltering, and the treatment of the position and ve-\nlocityofthebarpointerascontinuousvariables.\nReferences\n[1] E. Scheirer, “Tempo and Beat Analysis of Acoustic Music\nSignals,” in J. Acoust. Soc. Am. , vol 103, no. 1, pp 588-601,\nJan. 1998.\n[2] W.A.Sethares,R.D.MorrisandJ.C.Sethares.“BeatTrac k-\ning of Musical Performances Using Low-Level Audio Fea-tures.”in IEEETrans.SpeechandAudioProcessing ,vol.13,\nno. 2, Mar. 2005.\n[3] M. GotoandY. Muraoka, “Music Understanding attheBeat\nLevel - Real-Time Beat Tracking of Audio Signals,” in Pro-\nceedings of IJCAI-95 Workshop on Computational Auditory\nScene Analysis , pp. 68-75, 1995.\n[4] M. Goto “An Audio-based Real-time Beat Tracking System\nforMusicWithorWithoutDrum-sounds”,in JournalofNew\nMusic Research , vol.30, no. 2,pp. 159-171, 2001.\n[5] A. T. Cemgil and H. J. Kappen, “Monte Carlo methods for\nTempo Tracking and Rhythm Quantization.” in Journal of\nArtiﬁcial Intelligence Research , vol.18, pp 45-81, 2003.\n[6] C. Raphael, “Automated Rhythm Transcription” in Proc. of\nthe2ndAnn.Int.Symp.onMusicInfo.Retrieval. ,pp.99-107,\nStephen Downie and DavidBainbridge eds, 2001.\n[7] S.HainsworthandM. Macleod. “Beat trackingwithpartic le\nﬁltering algorithms.” in Proceedings of IEEE Workshop on\nApplications of Signal Proc. to Audio and Acoustics , New\nPaltz,New York, 2003.\n[8] K.Frieler,“BeatandMeterExtractionUsingGaussiﬁedO n-\nsets”, inProc. of the 5th Ann. Int. Symp. on Music Info. Re-\ntrieval.,2004.\n[9] A. T. Cemgil, H. J. Kappen, P. Desain, and H. Honing. “On\ntempotracking: TempogramRepresentationandKalmanﬁl-\ntering.”in Journal ofNewMusicResearch , vol.28no.4,pp.\n259-273, 2001.\n[10] J. Brown, “Determination of the meter of musical scores\nby autocorrelation”, in Journal of the Acoustical Society of\nAmerica, vol.94, no. 4, pp.1953-1957, 1993.\n[11] D. Eck and N. Casagrande, “Finding Meter in Music Using\nan Auto-correlation Phase Matrixand Shannon Entropy”, in\nProc. of the 6th Ann. Int. Symp. on Music Info. Retrieval. ,\n2005.\n[12] F.LerdahlandR.Jackendoff,“AGenerativeTheoryofTo nal\nMusic.” MIT Press,Cambridge, Massachusetts, 1983.\n[13] D.TemperleyandD.Sleator,“Modelingmeterandharmon y:\nA preference-rule approach.” in Computer Music Journal ,\nvol. 23, no. 1, pp.1027, 1999.\n[14] A.Pikrakis,I.AntonopoulisandS.Theodoridis,“Musi cMe-\nter and Tempo Tracking from Raw Polyphonic Audio” in\nProc. of the 5th Ann. Int. Symp. on Music Info. Retrieval. ,\n2004.\n[15] H. Takeda, T. Nishimoto and S. Sagayama, “Rhythm and\nTempo Recognition of Music Performance from a Proba-\nbilisticApproach”in Proc.ofthe5thAnn.Int.Symp.onMu-\nsic Info. Retrieval. ,2004.\n[16] A. Klapuri, A. Eronen, and J. Astola, “Analysis of the me ter\nof acoustic musical signals” in IEEE Trans. Audio, Speech,\nand Language Processing , vol.14 no. 1, 2006.\n[17] A. T. Cemgil, H. J. Kappen, and D. Barber, “Generative\nModel based Polyphonic Music Transcription” in Proc. of\nIEEEWASPAA ,2003\n[18] E. W. Large and J. F. Kolen, “Resonance and the perceptio n\nof musical meter,” in Connection Science , vol. 6, no. 1, pp.\n177-208, 1994.\n[19] K. P. Murphy, “Dynamic Bayesian Networks: Representa-\ntion, Inference and Learning.” PhD Thesis, University of\nCalifornia, Berkeley , 2002."
    },
    {
        "title": "Heroic Frogs Save the Bow: Performing Musician&apos;s Annotation and Interaction Behavior with Written Music.",
        "author": [
            "Megan A. Winget"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1417709",
        "url": "https://doi.org/10.5281/zenodo.1417709",
        "ee": "https://zenodo.org/records/1417709/files/Winget06.pdf",
        "abstract": "Although there have been a number of fairly recent studies in which researchers have explored the information seeking and management behaviors of people interacting with musical retrieval systems, there have been very few published studies of the interaction and use behaviors of musicians themselves. The qualitative research study reported here seeks to correct this deficiency in the literature. Drawing on data collected from nearly 300 annotated parts representing 15 unique works, and 20 musician interviews, we make a number of functionality recommendations for constructive music digital library tool development. For example, all musicians annotate their written music, although this action seems to become more important as the musician becomes more skilled. Musicians’ annotations are comprehensible to anyone who can read music, and are valuable as records of interpretation, interaction, and performance. Musicians annotate at the note (rather than at the phrase or movement) level, their annotations are standardized and formal, and are largely non-text. Music digital libraries that cater to musicians should attempt to provide annotation tools that work at the micro level, and extend the symbolic language of the primary document. Furthermore, preserving the annotations for future use would prove valuable for performance students, professionals, and historians alike. Keywords: annotation, musician, performance, interaction.",
        "zenodo_id": 1417709,
        "dblp_key": "conf/ismir/Winget06",
        "keywords": [
            "annotation",
            "musician",
            "performance",
            "interaction",
            "preserving annotations",
            "symbolic language",
            "digital library",
            "performance students",
            "professionals",
            "historians"
        ],
        "content": "Heroic Frogs Save the Bow:Performing Musician’s Annotation and Interaction Behavior with WrittenMusicMegan WingetiSchool, University of Texas at Austin1 University Station, D7000Austin, TX 78712-0390winget@ischool.utexas.eduAbstractAlthough there have been a number of fairly recent studiesin which researchers have explored the information seekingand management behaviors of people interacting withmusical retrieval systems, there have been very fewpublished studies of the interaction and use behaviors ofmusicians themselves. The qualitative research studyreported here seeks to correct this deficiency in theliterature. Drawing on data collected from nearly 300annotated parts representing 15 unique works, and 20musician interviews, we make a number of functionalityrecommendations for constructive music digital librarytool development. For example, all musicians annotatetheir written music, although this action seems to becomemore important as the musician becomes more skilled.Musicians’ annotations are comprehensible to anyone whocan read music, and are valuable as records ofinterpretation, interaction, and performance. Musiciansannotate at the note (rather than at the phrase or movement)level, their annotations are standardized and formal, and arelargely non-text. Music digital libraries that cater tomusicians should attempt to provide annotation tools thatwork at the micro level, and extend the symbolic languageof the primary document. Furthermore, preserving theannotations for future use would prove valuable forperformance students, professionals, and historians alike.Keywords: annotation, musician, performance, interaction.1. IntroductionAlthough there have been a number of fairly recent studiesin which researchers have explored the information seeking[5, 6, 8, 12] and management behaviors [1, 11, 13] ofpeople interacting with musical retrieval systems, therehave been very few published studies of the interaction anduse behaviors of musicians themselves. Bellini’s work ondigital music stands [2] and information management inorchestras [3] mentions musician interviews andobservations, but in-depth, published reports of musicianbehavior and information use do not exist.This is unfortunate, because the information with whichmusicians interact, the musical score, has special propertiesthat make its study profitable for general theories ofinformation behavior, interaction and use. In addition toproviding valuable insight into the annotative behaviors ofusers interacting with notational, symbolic data, thisresearch also clearly benefits the music information sciencecommunity, providing a user study of musician’sinteraction with their written music. Understandingmusicians’ annotation behaviors will hopefully influencemusic digital library tool development, and may lead tobetter interfaces, more contextually relevant retrievalsystems, and modified digitization and digitized scorepreservation policies.The goal of this research project was to find out moreabout how musicians interact with their written music.This paper will review our findings regarding musicians’annotation behaviors and provide recommendations forsuccessful tool development in the performative context.2. MethodologyThis project used an ethnographic model with three datacollection points: informal rehearsal observation, semi-structured musician interviews, and content analysis ofannotated scores.Because we wanted to focus on exploring the ways thatmusicians interact with and annotate their written musicfor the purpose of performance, we wanted both the abilityto read music, and the fact that the music was formallywritten to be the de facto means of representation andinteraction. Therefore, the user group was limited toclassically trained musicians playing classical music.Interesting future work could be done on less formallyrepresented music styles, like jazz, folk and rock.Table 1 illustrates the six groups of musicians observedin this study.Permission to make digital or hard copies of all or part of this work forpersonal or classroom use is granted without fee provided that copiesare not made or distributed for profit or commercial advantage andthat copies bear this notice and the full citation on the first page.© 2006 University of VictoriaTable 1. Data Collection Grid: Musician Types. Shadedrows represent the number of parts collected; un-shadedrows represent the number of interviews conducted.ProSemi-ProfessionalAmateur10510512Orchestral22101654Chamber434We divided score collection into orchestral and chambermusicians to investigate whether the presence of aconductor or interpretative leader made any difference inthe quality and quantity of annotations. Furthermore, wewanted to explore the differences between amateur, semi-professional, and professional musicians’ interaction,collaboration, and annotation styles.2.1 Score Annotations: Content AnalysisThe field of information and library science (ILS), has onlyrecently discovered the utility of studying annotationbehavior, and there are two main approaches to annotationstudies in this field. In those technical strands of the fieldfocused on artificial intelligence or knowledgerepresentation, “annotation” seems to be synonymous withautomatically generated metadata or machine learningapplications [7]. At the more sociological end of thespectrum, annotations are becoming widely recognized asvaluable indicators of user interaction with a primaryobject or text. Essentially regarding annotations as a“reflection of a reader’s engagement with a text,” [10],these annotation studies focus on studying different user’sannotation styles and methods in order to develop newsystems for reading, writing, or interacting with digitaldata. We took the more sociological approach, althoughthere could be some machine learning applications for thisresearch. Table 2 shows the content analysis griddeveloped for this project.Table 2. Example Content Analysis Grid. “SpecificPurpose” and “Transcript Example” columns are notcomplete but provide examples.ModeGeneralPurposeSpecificPurposeTranscriptExampleTechnicalBowingV (up) or η(down)SymbolContextualDynamics< (cresc.) or> (decresc.)TechnicalFingering1-5NumberContextualTempo2/2, 4/4TechnicalBowing“Save!”“Frog!”TextContextualDynamics“MAX”“CRESC”The content analysis process consisted of transcribingeach annotation on each collected part, recording the barnumber and whether the annotation modified publishedinformation; and characterizing each annotation with threelevels of description: 1) the annotation method: text,symbol, or number; 2) the general annotation purpose:technical-physical, or contextual; and 3) the specificannotation purpose: like bowing, tempo, dynamics, andcues (among others).Technical annotations focus on the physicality ofplaying a specific instrument, exemplified by cues,bowing, breathing, and fingering instructions.Context-based annotations are specific to theperformance context and the players’ intentions,interpretations and skill. Dynamics, timing, and phrasinginstructions are typical of this group.Figure 1 shows an example of a marked up annotatedscore.\nFigure 1. Example of annotated part. From musician inamateur chamber group. [1. text: context: rhythm; 2. text:context: dynamics; 3. text: context: rhythm; 4. symbol:context: dynamics; 5. text: technical: attentive; 6. text:technical: bowing; 7. text: context: rhythm; 8. text:technical: bowing]2.2 Musician InterviewsWe conducted semi-structured interviews with all of thechamber musicians from whom we collected scores. Theparticipants in orchestras were self-selecting, but wesucceeded in interviewing the concertmaster and conductorfor each group. The interview questions were based onBuckland’s typology of information [4], and modified forannotation studies by MacMullen [9]. The three interviewsections focus on 1) the process of creating annotations; 2)the physicality of those annotations; 3) and the knowledgenecessary to create, utilize, and understand the annotations.2.3 Rehearsal ObservationWe attended rehearsals to become acclimated to therehearsal /performance lifecycle rather than to gather dataupon which to build theories. For each group, we observedrehearsals at the beginning of the rehearsal process, towardsthe middle, and right before performance. This entirerehearsal/performance cycle would sometimes take a week,and sometimes months.The information gathered from this phase of datacollection was very informal and general. While it isrelatively straightforward to collect observational data froma chamber group like a quartet or quintet; it would benearly impossible to consistently and unobtrusively collectdata from a 100-member orchestra.3. Annotation CharacterizationMost of the annotations that musicians make are technicalor physical in nature, showing bowings, fingerings, whichstring to play on, breathing, breaks and articulation. Theirannotations are largely  non-textual in nature, withsymbolic annotations being more prevalent than numericor textual ones.3.1 PurposeThe technical / physical annotations represent specificinstructions regarding how to play the piece of music. Forexample, articulation annotations are “specific instructionson how to begin and end the note.” Bowing instructions(V or ∏) relate whether the bow should be moving up ordown, or what position the bow should be at thebeginning of a note. Breathing and break annotations(denoted by a comma) tell the musician when to take abreath or a break. Fingering instructions (numeric, 1 – 5)define which finger plays which note. Of course all ofthese technical decisions have aesthetic consequences: a“down bow” for example is louder and its definition ismore pronounced than an “up bow,” and hence theaesthetic response might be “power” or “force,” rather than“sweetness” or “calm.”The prevalence of these physical/technical annotationsillustrates the importance of physicality in performance.Musicians are focused on correctly and reliably performinga piece, and that involves physically hitting the right noteswith the right fingers, and drawing the bow across thestrings in a specific and particular way.Contextual annotations are those related to specificperformance contexts, like cues and characterization ofdifficult parts, dynamics and tempo; and are based on theperformance space, the expectations on the group, and theskill level of the musicians. It is conceivable that many ofthese notes could be considered technical in nature: tempoin particular has connotations of specific technicalprocedures. However, the annotations are general to theensemble, not specific to a particular instrument ormusician.3.2 ModeThe majority of annotations are symbolic in nature,followed by numeric and finally by text. Symbolicannotations cover the whole gamut of annotation purpose.There are symbols for attentive notes, like the eyeglasses;for contextual notes, like dynamics marks; and fortechnical notes, like articulation and bowing instructions.That most of the annotations are symbolic may be becausethe musicians are simply extending the symbolic languageof musical notation for their own use. The mostcommonly annotated elements (bowing, articulation, anddynamics) are annotated using the common symbolsassociated with those musical procedures. Further, amusician is likely to use the same representation system asused in the written score at that point in the score: whenthe published work uses a “<” to denote a “crescendo,” themusician is more likely to extend the lines of thecrescendo mark to make it longer, or start it earlier, ratherthan writing “cresc.,” or “ff.” However, there are instanceswhere the musician reinforces a textual note (“cresc.,” or“ff”) with a symbol and vice versa.Numeric annotations are used for fingering, timing,reminders regarding which string to play on, and bar orphrase numbers for navigation. Numeric annotations arealso very prevalent, and are used equally by musicians ofall skill levels. This democracy of use might be due to thedifficulty in conveying many of these concepts using anyother mode of communication. Although there are somesymbolic representations of tempo, those symbols wereonly used as an annotation once in all of the collectedparts, and that was by a professional chamber player.Likewise, fingering could be communicated by writing“pinky” or “thumb,” but is much easier to simply write“5” and “1” respectively. While a very few musicians preferto write finger names using text, the vast majority usefinger numbers.Musicians may also use textual annotations, althoughthis is the least common method of musical annotation.There are examples of musicians using words to denoteany musical procedure, but as their skill level increases,the likelihood of their using words diminishes. There areonly a few examples of natively textual musicalannotations, and those are related to attentive cuesspecifically.  When one needs to listen to the cello forwhatever reason, there is not a symbol or number for“cello,” so it is most natural to write the word “cello,” or“viola,” or whatever instrument that needs attention.Interestingly, the more skilled a musician becomes, themore symbol-like these annotations become. “Cello”becomes “VCL” (for violincello); “viola” becomes “VLA;”the first violin is “V1” or sometimes even “I” and thesecond violin is “V2” or “II.” Less serious musiciansmight even be more likely to write the names of themusicians instead of the instrument they play.4. Musician CharacterizationDifferent musicians annotate differently. We divided thesedifferences based on musician skill, instrument played, andmode, whether the musician plays in a chamber group ororchestra.4.1 SkillThere are a number of distinctions between amateur andprofessional musicians’ annotations. First, professionalmusicians tend to use musical symbols more regularly tocommunicate with themselves about musical matters. Theyhave internalized the symbolic language of music to thedegree that they easily communicate using it. Symbolsseem to be a very succinct and efficient way for musiciansto communicate necessary information back to themselves.A second, more subtle difference is in the quality of theprofessional’s versus the amateur’s annotations.Professional musicians are more distanced from the musicthey play, while being more invested in its reliableperformance. The professionals who participated in thisstudy are more likely to distance themselves personallyfrom the music they perform and this is borne out by theirsymbolic, highly technical annotations. Music for them isa profession where they practice and collaborate effectively,and their musical ability and commitment make successpossible. For the amateur and less invested semi-professional, music is an avocation. It is a hobby. Becauserehearsal attendance is not practically mandatory, amateursdo things like rehearse because they want to. Their notesand methods of communicating are therefore more personalas well. They use words instead of musical symbols. Theyrefer to their friends rather than the instruments theirfriends play. For example, if a professional needed toremind himself to listen for the cello, he would write“cello” or “VCL.” The amateur is more likely to write thename of the cellist, “Matt.” It infers that the amateurmusician isn’t listening to the cello, but to Matt, andinfers a more personal relationship to the performance thanthe professional has.One of this study’s less intuitive findings is that thehigher the musician’s skill level, the more annotations heis likely to have. Professional musicians have by far thegreatest number of annotations, and this is true acrossorchestral and chamber musicians. There are a number ofexplanations for this: first, amateur musicians mentionedthat they liked to try to remember different instructions,and saw the performance process as something of a game.They were the group least likely to have a pencil withthem during rehearsal. Most professional musicians,however, said that they are “100%” likely to have a pencilat rehearsal, and many mentioned that they would be “veryembarrassed” to have to borrow one.Professional musicians know that their individual skillsand methods must merge seamlessly with others in thegroup. The group’s earning power, which is dependent ofthe quality of their performance, is likewise dependent onindividual members performing the piece correctly andreliably. Because “getting it right” is so important forsuccess (both individually and as a group), professionalmusicians tend to not leave very much to chance, and seetheir annotations as an effective method to ensure success.4.2 InstrumentAnnotations are a reflection of a musician’s engagementwith a piece of music. Often this engagement representschallenging elements for either the individual or the group.Although there tend to be an average of one or twoannotations per bar of music (a piece with 350 bars ofmusic will typically have around 350 annotations), thoseannotations are not clustered evenly. There will sometimesbe forty or fifty measures of un-annotated music, and fouror five measures where every element has a mark.Musicians indicated that these sections were indeed themost difficult parts of the piece, although they were notthe sections commonly understood to be the mostdifficult. For example, Shoskatovich’s String Quartet,#11, Op. 122, played by the professional chamber group,has virtuoso parts for both the first violin and the cello.However, this piece was the least annotated piece for theprofessional group generally, and the particularly difficultphrases were totally un-annotated by both the cello and thefirst violin. When asked about this, these two participantsmentioned that 1) they’d need to concentrate during thesedifficult passages and annotations would have beendistracting; and 2) during these virtuoso phrases orsections, these instruments are often playing alone, so theneed to “get it right,” is purely individual. This suggeststhat annotations, in this context, have a specificallycollaborative character.Also, the presence of numerous annotations suggeststhat some instruments have more responsibility for thesmooth functioning of the group as a whole. In thechamber music groups, across skill level, the second violinoften had twice as many annotations as anyone else, andsometimes had four times as many annotations, while thefirst violin had the smallest number of annotations. Whenasked about this, the professional first violin player hadtwo explanations: 1) the second violin is often responsiblefor thematically and functionally tying together all of theother instruments. Because the second violin both backsup the tempo set by the cello and supports the melodyplayed by the first violin, the second violin has moreinformation to keep track of, and more responsibility for“getting everything right.” They have to interact witheverybody in the group, and use annotations to keep trackof those interactions. 2) The first violin typically plays themelody, which is easier to memorize, and less dependenton successful interaction with other members of the group.Everyone else has a responsibility to follow the firstviolin’s lead. The need to annotate is therefore less urgentfor the first violin than it is for everyone else in the group,but especially the second violin, whose “job” is to manageinteraction among the different instruments.4.3 Mode (Orchestra  versus Chamber)Chamber musicians are much more prolific annotators thanorchestral musicians; and their annotations’ purpose rangebetween technical, attentive, and contextual; whereas thepurpose of orchestral annotations are almost purelytechnical.The disparity in the amount of annotations orchestraland chamber musicians produce might be due to thedifferent power hierarchies in the two groups. If we acceptthat annotations represent interactions that the musicianhas with the written music, and those interactionsthemselves represent performative decisions made by themusician, then it is only natural that chamber musicianswould make more performative decisions and hence makemore annotations than orchestral musicians, who aremaking relatively few performative decisions on their own.There are many consequences of this chain of command inchamber versus orchestral music. As discussed earlier, thefact that the first violin, who is often considered the“leader” of a quartet, carries the melody which everyonefollows, has the result of his having fewer annotations thaneveryone else in the group because his responsibilitywithin the group is to lead rather than to manage thecollaboration. The musician tasked with managing thecollaboration, the second violin, often has twice as manyannotations as anyone else.In an orchestral setting none of the musicians haveindividual responsibility for either carrying the melody(except in the case of solos, which are generally notannotated) or for managing collaboration amonginstruments. The conductor has those responsibilities, andthe musicians look to him for technical, attentive, orcontextual cues – they don’t need to make the notes thatchamber musicians must, because they are operating in amuch less egalitarian society. They make no performativedecisions; they make very few personal annotations ontheir music.As mentioned before the most prevalent type ofannotation on an orchestral piece is technical: the fingeringand bowing instructions for the strings (violin, viola,cello, double bass, and plucked harp), the fingering andbreathing instructions for the winds (flute, oboe, Englishhorn, clarinet, and bassoon; French horn, trumpet,trombone, and tuba), and the pitch changes for thepercussion section (usually one person who playseverything: kettledrums or timpani, snare and bass drums,cymbals, triangle, and xylophone).Orchestral musicians serve the intellectual will of theconductor. The feeling that their annotations give is thatorchestral musicians are following orders. In many casestheir annotations are given to them from the section chair,who has received orders from the concertmaster (first chairviolin), who has often received information, if not outrightdirection, from the conductor.As mentioned earlier, most of the orchestral annotationsare technical. When asked why orchestral musicians writeso few non-technical notes, particularly emotive ones, theprofessional chamber musicians said that an orchestralmusician would be “laughed off the stage” if they startedmaking those sorts of personal annotations. So there aresome cultural norms at work, which don’t allow fororchestra members to make emotive or contextualdecisions or even allow those decisions to see the light ofday in performance.5. Recommendations for Music DigitalLibraries DevelopmentAll performing musicians make annotations on theirwritten music. Their annotations are largely non-text, andare formal and standardized. The annotations arecommonly understood by anyone who is able to readmusic, and musicians value certain annotations, like thoseof highly skilled musicians, or their mentors; and do notvalue others. Musicians annotate for a number of reasons:precise technical or physical direction on how to playspecific notes or phrases, general contextual clues on thefeeling and tone of the piece, and as reminder notes tolisten for a cue, or turn the page. These findings have anumber of applications for the Library and InformationScience community.5.1 Collection Development  & PreservationAlthough it is difficult to make recommendations for toolsthat do not yet exist, we know that musicians highly valueannotated parts from their mentors or respected colleagues.All of the more invested players had a story to tell about“saving” (i.e., copying) some annotated parts from belovedteachers; or how “precious” their old annotated parts wereto them. Currently, annotated parts are erased when theyare either returned to the library or the rental agency and allthat information is lost forever. Unless the musical partcomes from a particularly famous musician or composer,annotations are erased for digitization as well. Althoughthis practice is understandable for physical objects –annotations can be distracting – if annotation tools are everdeveloped for music digital libraries, that informationcould be saved for future students learning the piece, orscholars studying a piece’s particular performance history.5.2 Annotation ToolsIf a music digital library caters to performance students orworking musicians, the development of annotation tools isvitally important. All musicians annotate, although thequality and quantity of those annotations varies acrossskill level, instrument, and mode of play.Most of the annotations are at the note or elementallevel, and are dependent on context. Although annotationscan be symbolic, numeric or textual, they are mostlysymbolic or numeric, extending the symbolic languageemployed by music notation. Any annotation tooldeveloped for music digital libraries should providefunctionality for annotating notes and musical elementsusing the language of the primary document itself.Music annotation, like musical notation, is highlystructured and standardized. Instead of providing stylusfunctionality where the musician simply “writes” on ascreen whatever they want to write, it might be interestingto develop a system architecture that would preserve theannotations in a structured and standardized way for futureuse. 6. ConclusionWhether they are used for technical or contextualpurposes; whether they are symbolic, numeric or textual;or whether there are a lot or few of them; musicalannotations are an almost ubiquitous process forperforming musicians.This research can best be used to define tools and needsthat must be met for development of a fully functionaldigital library for performing musicians. These needsinclude the ability to annotate at the micro level usingsymbols, numbers or text, and to allow for different modesof annotation dependent on musician performance mode.7. AcknowledgmentsThis work was partially funded by an unrestricted researchgift from Microsoft Research to the Annotation ofStructured Data research team in the School of Informationand Library Science at the University of North Carolina atChapel Hill, whose members contributed to this work:Gary Marchionini, Paul Solomon, and Catherine Blake,co-PIs; with team members Tom Ciszek, Xin Fu, LiliLuo, W. John MacMullen, Cathy Marshall, Mary Ruvane,and Davis West. The website is available at:http://ils.unc.edu/annotation/.My doctoral committee also assisted with thisproposal’s development. They are: Helen Tibbo (co-chair),Gary Marchionini (co-chair), Deborah Barreau, and PaulSolomon at the School of Information and Library Scienceat the University of North Carolina at Chapel Hill;Catherine C. Marshall from Microsoft Corp., and J.Stephen Downie at the Graduate School of Library andInformation Science at the University of Illinois at Urbana-Champaign.References[1] Bainbridge, D., Cunningham, S.J. and Downie, J.S.,Visual Collaging of Music in a Digital Library. inInternational Symposium on Music InformationRetrieval ISMIR, (Barcelona, Spain, 2004).[2] Bellini, P., Fioravanti, F. and Nesi, P. Managing Music inOrchestras. Computer (September). 26-34.[3] Bellini, P., Nesi, P. and Spinu, N.B. Cooperative visualmanipulation of music notation. ACM Transactions ofComputer-Human Interaction, 9 (3). 194-237.[4] Buckland, M.K. Information as thing. Journal of theAmerican Society for Information Science, 42 (2). 358?[5] Cunningham, S.J., Jones, M. and Jones, S., OrganizingDigital Music for Use: An examination of personalmusic collections. in International Symposium onMusic Information Retrieval, (Barcelona, Spain, 2004),[6] Cunningham, S.J., Reeves, N. and Britland, M. Anethnographic study of music information seeking:Implications for the design of a music digital library.IEEE 2002. 5-16.[7] Heggland, J., OntoLog: Temporal Annotation Using AdHoc Ontologies and Application Profiles. in ECDL '02:Proceedings of the 6th European Conference onResearch and Advanced Technology for DigitalLibraries, (2002), 128.[8] Lee, J.H. and Downie, J.S., Survey of Music InformationNeeds, Uses, and Seeking Behaviors: PreliminaryFindings. in The 2004 International Symposium onMusic Information Retrieval, (Barcelona, Spain, 2004).[9] MacMullen, J. Annotation as process, thing, andknowledge: Multi-domain studies of structured dataannotation Technical Report Series, University of NorthCarolina, School of Information and Library Science,Chapel Hill, NC, 2005.[10] Marshall, C.C. Toward and ecology of hypertextannotation. HyperText 98. 40-49.[11] van Gulik, R., Vignoli, F. and van de Wetering, E.,Mapping Music in the Palm of your Hand, Explore andDiscover Your Collection. in International Symposumon Music Information Retrieval (ISMIR), (BarcelonaSpain, 2004).[12] Vignoli, F., Digital Music Interaction Concepts: A UserStudy. in International Symposium on MusicInformation Retrieval, (Barcelona, Spain, 2004).[13] Voida, A., Grinter, R.E., Ducheneaut, N., Edwards, W.K.and Newman, M.W., Listening in: practices surroundingiTunes music sharing. in SIGCHI conference on Humanfactors in computing systems, (Portland, Oregon, 2005),ACM Press."
    },
    {
        "title": "Remixing Stereo Music with Score-Informed Source Separation.",
        "author": [
            "John F. Woodruff",
            "Bryan Pardo",
            "Roger B. Dannenberg"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1414898",
        "url": "https://doi.org/10.5281/zenodo.1414898",
        "ee": "https://zenodo.org/records/1414898/files/WoodruffPD06.pdf",
        "abstract": "Musicians and recording engineers are often interested in manipulating and processing individual instrumental parts within an existing recording to create a remix of the recording. When individual source tracks for a stereo mixture are unavailable, remixing is typically difficult or impossible, since one cannot isolate the individual parts. We describe a method of informed source separation that uses knowledge of the written score and spatial information from an anechoic, stereo mixture to isolate individual sound sources, allowing remixing of stereo mixtures without access to the original source tracks. This method is tested on a corpus of string quartet performances, artificially created using Bach four-part chorale harmonizations and sample violin, viola and cello recordings. System performance is compared in cases where the algorithm has knowledge of the score and those in which it operates blindly.  The results show that source separation performance is markedly improved when the algorithm has access to a well-aligned score. Keywords: source separation, score alignment, music.",
        "zenodo_id": 1414898,
        "dblp_key": "conf/ismir/WoodruffPD06",
        "keywords": [
            "source separation",
            "score alignment",
            "music",
            "remixing",
            "instrumental parts",
            "knowledge of score",
            "analogous corpus",
            "sample recordings",
            "algorithm performance",
            "system performance"
        ],
        "content": "Remixing Stereo Music with Score-Informed Source Separation John Woodruff Music Technology,  School of Music  Northwestern University Evanston, IL 60208, USA j-woodruff@northwestern.edu  Bryan Pardo Electrical Engineering and  Computer Science  Northwestern University Evanston, IL 60208, USA pardo@northwestern.eduRoger Dannenberg School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213 USA rbd@cs.cmu.edu Abstract Musicians and recording engineers are often interested in manipulating and processing individual instrumental parts within an existing recording to create a remix of the recording. When individual source tracks for a stereo mixture are unavailable, remixing is typically difficult or impossible, since one cannot isolate the individual parts. We describe a method of informed source separation that uses knowledge of the written score and spatial information from an anechoic, stereo mixture to isolate individual sound sources, allowing remixing of stereo mixtures without access to the original source tracks. This method is tested on a corpus of string quartet performances, artificially created using Bach four-part chorale harmonizations and sample violin, viola and cello recordings. System performance is compared in cases where the algorithm has knowledge of the score and those in which it operates blindly.  The results show that source separation performance is markedly improved when the algorithm has access to a well-aligned score. Keywords: source separation, score alignment, music. 1. Introduction Musical remixing can be broadly defined as the process of manipulating and processing individual instrument parts within an existing recording.  This could mean simply raising the level of a single instrument in a poorly balanced mixture, or completely reworking a piece of music through editing and applying effects to individual instruments.  When individual source tracks for a stereo mixture are unavailable, remixing is typically difficult or impossible. In order to remix existing recordings, one must perform source separation—separation of the audio mixture into its component sound sources.  While perfect reconstruction of individual sources from a musical mixture is not currently possible in the general case, even imperfect isolation is useful for a number of purposes, including improved instrument identification and analysis within polyphonic recordings, structured audio coding and both the creative and restorative remixing applications described above. In this paper we describe a method that performs source separation using information from the written score and spatial cues present in a stereo recording. The combination of these lets our method isolate individual musical parts in a corpus of four-part Bach chorale recordings so that audio effects, equalization and volumes can be altered on an instrument-by-instrument basis.  The remaining sections of this paper describe current research in source separation, our existing source separation method, our score alignment method, how we combine score information and spatial information to improve source separation, and experimental results. We also provide links to example remixes created using our approach at http://bryanpardo.com/papers/ismir2006. 2. Current Wor k in Source Separation The difficulty of the source separation problem depends on the number of sources (instruments) in the recording and the number of sensors (microphones) used to make the recording. When the number of available audio channels (mixtures) equals or exceeds the number of individual sources (a quadraphonic recording of a trio, for example), one may use Independent component analysis (ICA) [7]. The source separation problem is considered degenerate, or under determined, when the number of sources exceeds the number of mixtures. Standard ICA algorithms are not effective in the degenerate case.  Since millions of audio recordings exist in a stereo format (two-mixtures), but typically consist of more than two source signals, it should be clear why solving the degenerate source separation problem is of considerable interest to researchers. Recent approaches to degenerate source separation of speech mixtures have exploited the sparsity of speech signals in the time-frequency domain. Speech is considered sparse because the vast majority of time-frequency frames in a speech signal have magnitude near zero. This is used to justify the assumption that at most one source signal (talker) has significant energy in any given time-frequency Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. © 2006 University of Victoria frame (the signals are time-frequency disjoint). Given this assumption, a time-frequency masking approach can be used that exploits spatial cues from an anechoic, stereo recording to separate sources from a mixture [14]. Tonal music makes extensive use of multiple simultaneous instruments, playing consonant intervals (such as unisons, octaves and perfect fifths). When two harmonic sources form a consonant interval, their fundamental frequencies are related by a ratio that results in significant overlap between the harmonics (regions of high-energy at integer multiples of the fundamental frequency) of one source and those of another.  Non-harmonic instruments, such as percussion instruments, further complicate the problem due to their wide-band (noisy) spectral characteristics. Thus, instrument signals frequently overlap in both time and frequency, rendering approaches that assume time-frequency disjoint sources ineffective. To deal more effectively with overlapping source signals, researchers have introduced assumptions about the structure of the sound sources. In the single-mixture (monophonic) domain, Virtanen and Klapuri [9, 10] assume source signals are harmonic, allowing multi-pitch estimation of the polyphonic mixture to determine frequency regions in which source signals overlap.  By assuming that the signals have a smoothly decaying overtone series as a function of frequency, source amplitudes in the overlapping frequency regions can be estimated. Every and Szymanski [1] use a single mixture and prior knowledge of instrument pitches to determine regions of source signal overlap.  They linearly interpolate between known harmonics in cases where multiple sources overlap and achieve separation through spectral-filtering of the mixture. If the number of audio channels equals or exceeds the number of sound sources, Viste and Evangelista [11] show they can perform iterative source separation by minimizing the variance of the temporal envelopes of each source’s individual harmonics. While this method does very well in situations where two sources overlap and can potentially deal with reverberant recordings, it cannot be applied in the degenerate case. Vincent [8] approaches demixing stereo recordings with two or more instruments by incorporating grouping rules from computational auditory scene analysis [6], spatial cues and time-frequency source signal priors to cast the demixing problem into a Bayesian estimation framework. This is done to let the system handle reverberant recordings, but requires significant prior knowledge of each source signal in the mixture and is not suited to the remixing applications described in the introduction. While remixing is possible when source separation can be achieved, researchers have also approached remixing without attempting to fully isolate each sound source.  Methods for isolating percussion instruments from the rest of a stereo recording for remixing purposes are proposed in [2, 15].   While the overarching goal of this work is related to our own, our effort has been focused on isolating and remixing harmonic instruments in music recordings, requiring distinctly different processing techniques.   We previously introduced the ASE method to perform separation of stereo, anechoic mixtures of any number of harmonic, monophonic sources [13].  This approach requires no prior information about the sources, but can deal effectively with mixtures that contain significant source overlap. ASE can accurately resolve situations where two sources overlap, and uses this information to resolve regions of recordings where three or more sources are simultaneously active. We describe the details of this method in the next section. 3. ASE Source Separation The Active Source Estimation (ASE) source separation approach assumes an anechoic, stereo (two-channel) mixture of harmonic sound sources.  The two mixture channels are modeled as follows, \n!==NnnSX11),(),(\"#\"# (1) \n!=\"=NnnjnSeaXn12),(),(#$#$#% (2) where X1(τ,ω) and X2(τ,ω) represent the left and right mixtures in the time-frequency domain, with time frame τ and frequency bin ω.  Here, Sn(τ,ω) is the nth source signal, an is the cross-channel amplitude scaling and δn is the cross-channel time-shift associated with source n.  We call an and δn the mixing parameters of source n.  ASE takes a three-step approach to source separation.  In the first stage, common amplitude and phase differences between the two mixtures are assumed to result from the differing spatial locations of the individual sources.  The most common cross-channel scaling and time-shift factors are thus associated with the individual sources as the mixing parameters an and δn, and used to identify the time-frequency frames of the mixture that result from only one source [5, 13, 14].   The energy from these single-source frames is distributed to create initial source estimates while time-frequency frames that do not match the mixing parameters of any of the sources are left in the mixtures for later processing.  In the second step, ASE estimates the number of sources that are active in each remaining time-frequency frame of the mixtures.  It does this by pitch-tracking the partial source estimates from the first step. These fundamental frequency estimates, combined with simple harmonic models (estimated from the first stage), let the system identify which sources are likely to contribute energy to the remaining time-frequency frames in the stereo mixture. It is during this stage that the pitch information from the score can be utilized.  We discuss the implementation of score knowledge into this stage of the algorithm in section 4. Given our mixture models, the problem when two sources are active in a given time-frequency frame is even determined, allowing us to solve for the appropriate source energies in these frames.  This requires solving the system of equations provided by (1) and (2) under the assumption that only two sources are active.  If we denote the active sources in a particular time-frequency frame, (τ,ω), by Sg(τ,ω) and Sk(τ,ω), the following equations result, \n),(),(),(1!\"!\"!\"gkSXS#= (3) \nkgkjkjgjkgeaeaXeaXS!\"!\"!\"!#!#!#$$$$$=),(),(),(12 (4) By determining the appropriate source energy in each of these frames, more complete source estimates are created, leaving only those time-frequency frames that contain energy from three or more sources.  In mixture frames that have energy from three or more sources, the problem is under determined and the relative energy contribution of each source cannot be solved for directly.  In this case, a third step is taken.  The system models the amplitude variation of the harmonics in each source estimate. These models are used in conjunction with the mixture energy to predict the relative strength of the sources in the remaining time-frequency frames. This approach lets ASE separate mixtures containing time-frequency frames in which multiple harmonic sources are active without prior knowledge of source characteristics. This method is, however, susceptible to errors in pitch tracking. Inaccurate estimates of fundamental frequency will result in the system making mistakes about which sources contributed energy to individual time-frequency frames, causing source separation to fail.  In order to improve the reliability of the fundamental frequency estimates, it is helpful to use information from the musical score, when available. 4. Score Alignment A key ingredient in this approach to source separation is the labeling of audio with symbolic pitches. Labeling could be done manually, but it is much easier to start with a symbolic, machine-readable score, e.g. MIDI. In practice, MIDI files can often be found on the Web. While a MIDI file encodes basic rhythmic timing, it lacks any information about expressive timing and tempo in the audio recording. Alignment can recover this information. Polyphonic alignment is performed by converting the MIDI file and the audio file into a chromagram representation [12]. A chromagram is a sequence of chroma vectors, 12-element vectors representing the total spectral energy corresponding to each of the 12 pitch classes (C, C#, D, …, B). The chroma vector is chosen because it captures harmonic and melodic information, which is shared by audio and MIDI, and it tends to be insensitive to amplitude and timbral differences, which often do not to match very well between audio and MIDI [4]. Audio data is divided into 125 ms frames, each of which is converted to a chroma vector. For MIDI data, chroma vectors are estimated by summing all the matching pitch classes sounding during that frame, weighted by the key velocity and duration (0.125, or less if the note begins or ends during the frame). Audio and MIDI chroma vectors are normalized to have a mean of 0 and a standard deviation of 1. The next step uses dynamic time warping to find the best time alignment, using Euclidean distance between chroma vectors. Finally, the rough alignment, which is a path quantized to points along a 125 ms grid, is smoothed at each point by finding the best fit to the nearest 7 points, using linear regression. The resulting points define a sampled function that can be linearly interpolated to map between MIDI file time and audio file time.  \n Figure 1: Illustration of the combined Score Alignment and ASE Source Separation algorithm. Score alignment is carried out prior to ASE separation.  Score knowledge is incorporated during stage 2 of ASE. 5. The Combined System To incorporate knowledge of the score into the source separation algorithm, we must accomplish three primary tasks.  First, we must correctly associate individual voices (instrument parts) contained in the score with the mixing parameters of each source. Second, the pitches in a score give only a rough estimate of the actual fundamental frequencies present in the mixture. The system must refine the pitch estimates provided by the score in order to achieve accurate separation. Third, further timing alignment between performance and score must be performed within the combined system, since the score-alignment can be expected to make note onset timing errors on the order of 60 ms. To account for these issues we incorporate the information in the score after the first stage of the ASE algorithm.  This allows us to use fundamental frequency and amplitude envelope characteristics of the initial signals to solve all three problems.  The next two sections will describe the techniques employed in more detail. An illustration of the overall system is provided in Figure 1. 5.1 Associating Scored Voices with Initial Source Estimates The first stage of the ASE algorithm distributes time-frequency frames of the mixture that contain energy from a single source.  For the score to be of use in the second and third stages of energy distribution, we must determine which voice in the score is associated with a particular source estimate.  Since ASE is already estimating the fundamental frequency of each source, we take a simple approach to establishing this relationship.  We compare the fundamental frequency estimates of each source to each pitch track provided by the score.   We calculate the number of time frames in which the fundamental frequency is at most half a semitone, or within roughly ±3%, from the frequency associated with the pitch in the score.  For each source estimate established in stage one of ASE, we store the pitch track from the score that has the most time frames in common with the source’s fundamental frequency estimate.  We take care to ensure that each source is associated with a different voice in the score by giving priority to sources that have higher similarity ratings to a pitch track in the score. Using this simple method, the system correctly associated 99% of the score voices with source estimates on our testing corpus.   5.2 Refinement of Pitch and Timing Information Provided by the Score The pitch-tracks provided by the score are useful to ASE in the determination of where sources are likely to have high amounts of energy in time-frequency space.  However, using the score without frequency refinement would often cause the algorithm to miss the true fundamental frequencies of the sources in the mixture, since intonation variation and vibrato are not typically represented in scores. Also, even the time-aligned score can have errors of up to 60 ms, depending on the amount of expressive timing variation and asynchrony between performers. Without timing refinement, perceptually important signal features like note onsets can be lost.  To refine the frequency of the pitch tracks provided by the score, we again turn to the sources’ fundamental frequency estimates calculated in stage two of ASE.  Since we have determined which pitch track is most similar to each source’s fundamental frequency estimate, we simply use the fundamental frequency estimate provided by ASE in all time frames in which it is within half a semitone of the pitch track frequency from the score.  To refine the timing information, we calculate the amplitude envelopes of the sources from the stage one estimates, set an amplitude threshold to determine when the source is active, and record all frames that transition from below to above the threshold as possible note onsets.  We then allow the note onsets provided by the score to be altered to the possible onsets calculated above if the estimated onsets are within 3 time frames of the onset in the score.  Again, we chose 3 frames because the difference between the centers of consecutive time windows is 23 ms and the score alignment had a maximal error of roughly 60 ms on this corpus. 6. Experimental Results In previous work we tested the efficacy of the ASE source separation algorithm in isolation [13]. In this work, we were interested in measuring the possible improvement of source separation when score information is available.  To this end, we created a corpus of 100 stereo mixtures of four-part Bach chorales and tested our source separation under four conditions: blind (no score), an un-aligned initial score, a machine-aligned score and ideal score. This section describes the result in detail. 6.1 Corpus of Scores and Audio Mixtures Our test corpus consisted of 100 typical Bach soprano-alto-tenor-bass four-part chorale harmonizations. For each harmonization, we randomly chose a four second segment, typically equating to about one or two measures in the music.  For each segment of the harmonization chosen, we created three MIDI versions. The first version was an unaltered representation of the selected segment of the harmonization.  We call this the original score.  From each original score, we created the second MIDI version by randomly altering the tempo of each piece between 71% and 140% of the original tempo, with the average deviation being roughly 20%.  This version, the ideal score, was used to generate the audio mixture.  Although a typical interpretive performance of a piece of music would likely include tempo variation throughout the duration of the piece, our scored segments were only a measure or two long, so we felt that a simple tempo scaling was a reasonable simulation of a performance of the harmonization segment.  For each notated instrument part in the ideal score we created an audio file using recorded samples of violin (soprano and alto part), viola (tenor) and cello (bass). The samples used were from a commercial instrument sample library, Xsample Professional Sound Libraries, Volume 41: Solo Strings.  These individual audio recordings (one for each instrument part in the score) were then combined to create a stereo audio mixture of each chorale harmonization.  We created mixtures in this way in order to measure the difference between the ideal (the pre-mix individual signals) and the source estimates extracted from each mixture.   We then performed score following on each audio mixture, aligning the original score to the mixture. The output of the score follower was a MIDI file that had been time-altered to match the timing of the audio mixture. This is the aligned score. 6.2 The Experiment For each audio mixture we performed source separation four times: once with no score (the standard ASE algorithm), once with the ideal score, once with the aligned score, and once with the original score. For this experiment, we used a window length of 186 ms and a 163 ms overlap between time frames in the time-frequency analysis of the mixture.  Since we were interested in testing the system improvement when incorporating score knowledge, we must note that one key aspect of the separation algorithm was not tested in the presented data.  During the first stage, ASE uses the approach presented in [14] to determine each source’s mixing parameters (aj,δj). Since the experiment in this paper was designed to measure how score knowledge improves the second stage of demixing (separation of overlapped harmonics), we wanted all variation in results to be due to the use of score information and used known values for the mixing parameters. 6.3 The Error Measure The performance of the algorithm was measured by its ability to achieve complete isolation of the individual sources.  For each source estimate created from the mixture, we calculate the Signal-to-Distortion Ratio (SDR) as shown in Equation 5 [3]. Here, \ns is the original source signal, and \nsˆ is the source estimate provided by the algorithm.  \n!!\"#$$%&'=22210,ˆˆ,ˆ,ˆlog10ssssssSDR (5) A high SDR represents a strong correlation between the estimated and original signal, with little noticeable distortion.  Through informal listening tests, we feel that an SDR under 3 or 4 dB results from estimated signals that are similar to the original sources, but with very noticeable interference or artifacts due to demixing errors.  Signals with an SDR above 6 dB are better and can be sufficient for many remixing applications.  Signals with an SDR above 8 or 9 dB may still contain audible artifacts when isolated, but these artifacts are easily masked when recombined with other instruments from the original recording.  6.4 Results We found that using knowledge of the score greatly improved the performance of the source separation algorithm. Without score knowledge, the fundamental frequency estimation in stage 2 of ASE was accurate (within half a semitone) in an average of 69.4% of a source signal’s time frames.  Using the aligned and refined scores increased this accuracy to 92.9%.  The increased accuracy of the fundamental frequency estimates resulted in improved separation performance in 78.25% of the separated signals.  The SDR improvement between the median blind and median aligned score performance was 1.7 dB.   Figure 2 shows notched box-plots of the SDR over all trials for the three score knowledge scenarios. Each box represents the performance on 400 signals, four for each chorale harmonization. The lower and upper lines of each box show 25th and 75th percentiles of the sample. The line in the middle of each box is the sample median. The lines extending above and below the box show the extent of the rest of the sample, excluding outliers. Outliers are defined as points further from the sample median than 1.5 times the interquartile range (the overall height of the box) and are indicated by plus signs. The notches in each box show the 95% confidence interval around the median. Since the notches in the box-plot for the blind case and the aligned score do not overlap, we conclude, with 95% confidence, that use of the aligned score provides significant performance improvement.  While knowledge of the score can improve the algorithm’s performance, a misaligned score can actually degrade separation.  In comparing the blind algorithm performance to the performance with the original score (the non-aligned score), the median SDR decreased by 3.51 dB with 79.25% of the cases performing worse when the algorithm had knowledge of the misaligned score.  This result emphasizes the necessity of score alignment if one is to incorporate score knowledge into a signal separation algorithm. \n Figure 2: Performance results over all mixtures, compared between score knowledge conditions. Direct comparisons between this system and other musical separation or remixing systems are difficult because of the lack of commonality between source signal assumptions, mixture assumptions, and testing data.  Considering the variety of approaches to the problem, the most suitable algorithm for a given mixture depends primarily on how well suited the algorithm’s assumptions and required a priori knowledge are to the given source signals and mixing process. 6.5 Example Remixes To illustrate the effectiveness of the combined algorithm for score-informed source, we created a number of example remixes that are accessible here: http://bryanpardo.com/papers/ismir2006 We have included examples of amplification and attenuation of individual instruments in the mixture, and also some in which we have applied reverberation and other effects processing to individual instruments.  A final example mimics a complete reworking of a piece of music by applying editing, looping and effects to the individual instrument parts.  We also provide audio examples of isolated source estimates.  One of the benefits of using source separation for musical remixing is that although isolated source estimates may contain audible artifacts or interference, these distortions are due to other sources in the recording and are often masked when the signal is recombined into a remix.  Effective manipulation of level and instrument timbre is possible even at relatively low SDR levels.  To illustrate this, we provide examples of remixes using source estimates at various SDR levels.  7. Conclusions and Future Wor k Musicians, recording engineers and composers often desire remixing of fully notated musical pieces.  We have presented a method combining score alignment and source separation to achieve such a task for anechoic, stereo recordings.  We have discussed the implementation of a musical source separation algorithm incorporating score knowledge and found this yields a notable improvement in separation performance. Our results make it clear that the fundamental frequency stage of the ASE algorithm can be inaccurate.  In future work, we plan to explore more robust methods of fundamental frequency estimation, which our results show will improve overall performance. The approach of the ASE method is to first create initial signal estimates, which can be analyzed to assist with demixing more difficult mixture regions.  Our future work will examine more sophisticated analysis methods and signal modelling to leverage learned structural information concerning the sources in conjunction with spatial information present in stereo recordings.  We feel that the development of an effective separation algorithm requires the exploitation of simultaneous signal features, and the ability to assess the reliability of these features at a given time.  Relying heavily on cross-channel amplitude and timing differences is unrealistic in reverberant or studio-produced recordings. We believe, however, that additional source characteristics can be learned from corrupted or mixed signals, which will allow systems such as ASE to degrade more gracefully as the recording process or environment becomes more challenging. References [1] M. Every and J. Szymanski. “A Spectral-Filtering Approach to Music Signal Separation”, in DAFx 04 Seventh Int. Conf. on Digital Audio Effects Proc., 2004, pp. 197-200. [2] O. Gillet and G. Richard. “Extraction and Remixing of Drum Tracks from Polyphonic Music Signals”, in WASPAA 05 IEEE Workshop on App. of Signal Proc. and to Audio and Acoustics Proc., 2005, pp. 315-318. [3] R. Gribonval, L. Benaroya, E. Vincent, C. Févotte.  “Proposals for Performance Measurement in Source Separation”, in ICA 03 Fourth Int. Symp. on Ind. Comp. Analysis and Blind Signal Sep. Proc., 2003, pp. 763-768. [4] N. Hu, R. Dannenberg, G. Tzanetakis. “Polyphonic Audio Matching and Alignment for Music Retrieval”, in WASPAA 03 IEEE Workshop on App. of Signal Proc. to Audio and Acoustics Proc., 2003, pp. 185-188. [5] A. Master. “Sound Source Separation of N Sources from Stereo Signals via Fitting to N Models Each Lacking One Source”, Stanford University, CCRMA Technical Report, 2002. [6] D.F. Rosenthal, and H.G. Okuno. Computational Auditory Scene Analysis.  Lawrence Eribaum Associates, 1998. [7] J.V. Stone. Independent Component Analysis: A Tutorial Introduction, Cambridge, Mass.: MIT Press, 2004. [8] E. Vincent.  “Musical Source Separation Using Time-Frequency Priors”, IEEE Trans. on Audio, Speech and Language Proc., vol. 14, no. 1, pp. 91-98, 2006. [9] T. Virtanen, and A. Klapuri. “Separation of Harmonic Sounds using Multipitch Analysis and Iterative Parameter Estimation”, in WASPAA 01 IEEE Workshop on App. of Signal Proc. to Audio and Acoustics Proc., 2001, pp. 83-86. [10] T. Virtanen, and A. Klapuri. “Separation of Harmonic Sounds using Linear Models for the Overtone Series”, in ICASSP 02 IEEE Int. Conf. on Acoustics, Speech and Signal Processing Proc., 2002, pp. 1757-1760. [11] H. Viste and G. Evangelista.  “A Method for Separation of Overlapping Partials Based on Similarity of Temporal Envelopes in Multi-Channel Mixtures”, IEEE Trans. on Audio, Speech and Language Proc., in press. [12] G.H. Wakefield. “Mathematical Representation of Joint Time-Chroma Distributions”, in SPIE 99 Int. Symp. on Opt. Sci., Eng., and Instr. Proc., 1999, pp. 637-645. [13] J. Woodruff and B. Pardo. “Active Source Estimation for Improved Source Separation”, Northwestern University, EECS Dept. Technical Report, NWU-EECS-06-01, 2006 [14] O. Yilmaz and S. Rickard. “Blind Separation of Speech Mixtures via Time-Frequency Masking”, IEEE Transactions on Signal Processing, vol. 52, no. 7, 2004, pp. 1830-1847. [15] K. Yoshii, M. Goto, H. Okuno. “Inter:D: A Drum Sound Equalizer for Controlling Volume and Timbre of Drums”, in EWIMT 05 2nd Euro. Workshop on the Int. of Knowledge, Semantic and Digital Media Tech. Proc., 2005, pp. 205-212."
    },
    {
        "title": "Hybrid Collaborative and Content-based Music Recommendation Using Probabilistic Model with Latent User Preferences.",
        "author": [
            "Kazuyoshi Yoshii",
            "Masataka Goto",
            "Kazunori Komatani",
            "Tetsuya Ogata",
            "Hiroshi G. Okuno"
        ],
        "year": "2006",
        "doi": "10.5281/zenodo.1416826",
        "url": "https://doi.org/10.5281/zenodo.1416826",
        "ee": "https://zenodo.org/records/1416826/files/YoshiiGKOO06.pdf",
        "abstract": "This paper presents a hybrid music recommendation method that solves problems of two prominent conventional meth- ods: collaborative filtering and content-based recommen- dation. The former cannot recommend musical pieces that have no ratings because recommendations are based on ac- tual user ratings. In addition, artist variety in recommended pieces tends to be poor. The latter, which recommends mu- sical pieces that are similar to users’ favorites in terms of music content, has not been fully investigated. This induces unreliability in modeling of user preferences; the content similarity does not completely reflect the preferences. Our method integrates both rating and content data by using a Bayesian network called an aspect model. Unobservable user preferences are directly represented by introducing la- tent variables, which are statistically estimated. To verify our method, we conducted experiments by using actual au- dio signals of Japanese songs and the corresponding rating data collected from Amazon. The results showed that our method outperforms the two conventional methods in terms of recommendation accuracy and artist variety and can rea- sonably recommend pieces even if they have no ratings. Keywords: hybrid method, probabilistic model, collabora- tive filtering, content-based recommendation.",
        "zenodo_id": 1416826,
        "dblp_key": "conf/ismir/YoshiiGKOO06",
        "keywords": [
            "hybrid music recommendation method",
            "collaborative filtering",
            "content-based recommendation",
            "Bayesian network",
            "latent variables",
            "probabilistic model",
            "unobservable user preferences",
            "artist variety",
            "reliable modeling",
            "unreliability in modeling"
        ],
        "content": "Hybrid Collaborative and Content-based Music Recommendation\nUsing Probabilistic Model with Latent User Preferences\nKazuyoshi Yoshii †∗Masataka Goto ‡ Kazunori Komatani †\nTetsuya Ogata † Hiroshi G. Okuno †\n†Graduate School of Informatics, Kyoto University∗JSPS Research Fellow (DC1)\n‡National Institute of Advanced Industrial Science and Technology (AIST)\n{yoshii,komatani,ogata,okuno }@kuis.kyoto-u.ac.jp m.goto@aist.go.jp\nAbstract\nThis paper presents a hybrid music recommendation method\nthat solves problems of two prominent conventional meth-ods: collaborative ﬁltering and content-based recommen-dation. The former cannot recommend musical pieces that\nhave no ratings because recommendations are based on ac-\ntual user ratings. In addition, artist variety in recommendedpieces tends to be poor. The latter, which recommends mu-sical pieces that are similar to users’ favorites in terms of\nmusic content, has not been fully investigated. This induces\nunreliability in modeling of user preferences; the contentsimilarity does not completely reﬂect the preferences. Ourmethod integrates both rating and content data by using a\nBayesian network called an aspect model. Unobservable\nuser preferences are directly represented by introducing la-tent variables, which are statistically estimated. To verifyour method, we conducted experiments by using actual au-\ndio signals of Japanese songs and the corresponding rating\ndata collected from Amazon. The results showed that ourmethod outperforms the two conventional methods in termsof recommendation accuracy and artist variety and can rea-\nsonably recommend pieces even if they have no ratings.\nKeywords: hybrid method, probabilistic model, collabora-\ntive ﬁltering, content-based recommendation.\n1. Introduction\nNeeds for music recommendation emerge today [1] becausewe can access various music databases through the Internet.\nTo ﬁnd favorite musical pieces by using retrieval systems,\nwe have to execute queries repeatedly by ourselves. There-fore, we are often at a loss as to what queries are appropriate.To solve this problem, it is desirable that recommender sys-\ntems select probably-preferred pieces from the database by\nestimating our preferences. So far, two major recommen-dation techniques have been proposed: collaborative ﬁlter-ing and content-based recommendation, which have com-\nplementary advantages as described below.\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies\nare not made or distributed for proﬁt or commercial advantage and thatcopies bear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2006 University of VictoriaCollaborative methods [2–4] recommend pieces to a user\nby considering someone else’s ratings of those pieces. For\nexample, suppose that there is a target user who likes pieces\nA and B. If there are many other users who like A, B, andC, C will probably be recommended to the target user. Thistechnique is widely utilized in practical web-shopping ser-\nvices (e.g., Amazon and iTunes music store) and has been\ndemonstrated to be rather effective. However, there are twoproblems. The ﬁrst problem is that pieces that have not beenrated (e.g., newly-released CDs and minor songs) cannot be\nrecommended. Therefore, there are not many chances of en-\ncountering unexpected favorite pieces. The second problemis that artists of the recommended pieces tend to be the sameand are often well-known to a target user. Such recommen-\ndations may be unsatisfactory or meaningless.\nContent-based methods [5–7] recommend pieces that are\nsimilar to users’ favorites in terms of music content such\nas moods and rhythms. This leads to a rich artist variety;\nvarious pieces including unrated ones can be recommended.To achieve this, it is necessary to associate user preferenceswith the music content by using a practical database where\nmost users tend to rate few pieces as favorites. However,\nreliable methods for doing this have not been established.This is because a lot of attention has been paid to developingmusic retrieval systems [8] in which queries that represent\nuser preferences are prepared by users. Although Hoashi et\nal.[5] tried to model user preferences, their method was\nonly veriﬁed in an impractical database with 12 subjectswhere the balance in ratios of positive and negative ratings\nwas kept to some extent. Logan [6] did not address actual\nuser ratings by assuming a set of pieces in an album-CD asa set of favorite pieces of a particular user. To use Celma’s\nWEB-based recommender system [7], which was not evalu-\nated, users should list their favorite artists used as queries.\nTo solve the problems in the two methods, we propose a\nhybrid method that integrates both rating and content data.\nThis enables more accurate recommendations with a richvariety. Many ratings by other users aid the reliable mod-eling of preferences by compensating for the insufﬁciency\nof pieces rated by a target user. A possible way of imple-\nmenting a hybrid method is to use collaborative and content-based methods in parallel or in cascade [9, 10]. However,substantial user preferences cannot be still captured because\nobservable data (ratings and contents) do not completely re-ﬂect the preferences. To solve this, our method is based\non an extended version of a three-way aspect model pro-posed by Popescul el al. [11] that directly represents un-\nobservable user preferences as a set of latent variables. In\nour method, the distribution of mel-frequency cepstral co-\nefﬁcients (MFCCs) is modeled as music content. To ourknowledge, our method is the ﬁrst to apply the aspect modelto content extracted not from additional documents (e.g., re-\nview comments) but from actual media ﬁles.\nThe rest of this paper is organized as follows. Section 2\nspeciﬁes a recommendation task, and Section 3 introducesthe conventional recommendation methods. Section 4 ex-\nplains our hybrid recommendation method. Section 5 re-\nports on our experiments using practical rating data fromAmazon, and Section 6 summarizes the key points.\n2. Speciﬁcation of Music Recommendation\nWe ﬁrstly describe three requirements for designing recom-mender systems and then deﬁne a recommendation task.\n2.1. Our Goal\nWe aim to satisfy the following requirements:\n1.High recommendation accuracy A better system\nwill recommend more favorite pieces and fewer dis-liked ones from a practical database in which the num-ber of ratings given by each user is not sufﬁcient.\n2.Rich artist variety If the recommended pieces\nwere by various artists unfamiliar to a target user, the\nchances of ﬁnding new artists who play music match-ing the likes of the user would increase.\n3.Solving new-item problem This enables users\nto ﬁnd appropriate selections that have unfortunately\nbeen given few/no ratings. In addition, the variety inrecommended pieces is enhanced.\nCollaborative systems and content-based systems cannot\nsatisfy all the three requirements, as discussed in Section 1.We believe that their merits will be combined by reﬂecting\nboth collaborative data (ratings of other users) and content-\nbased data (acoustic features) in the recommendation.\n2.2. Recommendation Task\nAn objective of music recommendation is to rank musical\npieces that have not been rated by a target user. Let indexesof users and those of pieces be U={u|1,···,N\nU}and\nM={m|1,···,NM}, respectively. Here, NUandNMare\nthe number of users and of pieces. We assume that Uand\nMare registered in a system in advance. Additional meta-\ndata (e.g., titles, artist names, and genres) are not necessary.\nRating data should be also reserved in the system. In this\npaper, we focus on scores on a 0-to-4scale as rating data;\nletru,mbe a rating score given to piece mby user u,w h e r e\nru,m is an integer between 0and4(4being the best). By\ncollecting all the ratings, rating matrix Ris obtained by\nR={ru,m|1≤u≤NU,1≤m≤NM}. (1)When user uhas not rated piece m,φis substituted for ru.m\nas a symbol of representing an “empty” score for conve-\nnience. Note that most scores in Rare empty in practical\ndata because each user has rated a few pieces in M. Collab-\norative methods use only Rfor the recommendation.\nTo use content-based methods, content data is required.\nWe assume that audio signals of the pieces represented byMare available. The content of each piece is represented\nas a single vector of several acoustic features extracted from\nthe corresponding audio signal. Let the indexes of thosefeatures be T={t|1,···,N\nT},w h e r e NTis the number of\nfeatures (a dimension of the feature vector). Here, cm,tis\ndeﬁned as the t-th feature value of piece m. By collecting\nall the feature vectors, content matrix Cis obtained by\nC={cm,t|1≤m≤NM,1≤t≤NT}. (2)\nGiven a target user u∈U, content-based methods use Cand\nnotRbut{ru,m|1≤m≤NM}for the recommendation.\nThat is, they do not use scores given by other users in R.\n3. Conventional Recommendation Methods\nWe review typical recommendation methods. which were\nused for comparison experiments in Section 5.\n3.1. Collaborative Filtering\nCollaborative methods try to predict unknown rating scores\nof a target user for musical pieces that have not been rated bythe user, considering someone else’s scores of those pieces.Given a target user u,l e t/tildewider\nu,mbe a predicted rating score of\nuserufor piece m, which is given by\n/tildewideru,m=¯ru+k/summationdisplay\n{u/prime|u/prime/negationslash=u,u/prime∈U}wu,u/prime(ru/prime,m−¯ru/prime), (3)\nwhere ¯ruand¯ru/primeare the average rating score of user uand\nthat of user u/prime, respectively. The value wu,u/primeis a weight that\nreﬂects the preference similarity between users uandu/prime,a n d\nkis a normalizing factor such that the absolute values of the\nweights sum to unity. That is,/summationtext\nu/prime|wu,u/prime|=1.A f t e r t h e\nscore prediction, pieces are ranked according to /tildewideru,m.\nThere are several measures for calculating the similarity.\nThe most popular one may be the Pearson correlation coef-\nﬁcient, which shows stable performance in many tasks [3].By using this measure, the similarity is deﬁned as\nw\nu,u/prime=/summationtext\nm(ru,m−¯ru)/summationtext\nm(ru/prime,m−¯ru/prime)/radicalbig/summationtext\nm(ru,m−¯ru)2/summationtext\nm(ru/prime,m−¯ru/prime)2, (4)\nwhere the summations over mare for the pieces rated by\nboth users uandu/prime. However, there are usually very few of\nthose pieces when rating matrix Ris sparse. Therefore, this\nbasic calculation method often fails.\nTo solve this problem, empty scores in rating matrix R\nare replaced with a default score rD. We empirically set rD\nto2.5, which is a biased score (c.f., a neutral score is 2on a\n0-to-4scale), because most users tend to provide high scores\n(3and4) more often than low ones ( 0and1).maximum similaritymusical piece\nuser’s favorite \nmusical pieces',mus\n4\nuM'm\nFigure 1. Similarity calculation between user preference and\nmusical pieces in content-based recommendation method.\n3.2. Content-based Recommendation\nContent-based methods try to rank musical pieces on the\nbasis of music-content similarity by representing user pref-\nerences in the music-content space. Let a content vector ofpiecembec\nm=(cm,1,···,cm,NT). Let a set of pieces\nwhich were given r-scores ( 0≤r≤4)b yu s e r ubeMr\nu=\n{m|ru,m=r}. Given a target user u, Logan’s method [6],\nwhich may be the most basic one, is applied as follows:\n1. IfM4\nuis not empty (i.e., user uhas provided 4-scores),\nwe focus on M4\nu. Otherwise, we then focus on M3\nu\nifM3\nuis not empty. After this, we explain the algo-\nrithm in the case of using M4\nu. A set of content vectors\n{cm|m∈M4\nu}represents a preference of user u.W e\ncall those vectors preference vectors .\n2. By using a similarity measure, the similarity between\neach preference vector cm(m∈M4\nu)and content\nvector cm/prime(m/prime∈M,r u,m/prime=φ)is calculated, as\nshown in Figure 1. Let the maximum similarity bes\nu,m/prime, which indicates how likely user uwill like piece\nm/prime. Then, su,m/primeis calculated for each piece m/prime.\n3. Pieces {m/prime|ru,m/prime=φ}that have not been rated by\nuseruare ranked according to the similarity su,m/prime.\nThe cosine measure is often used to calculate the vector sim-\nilarity [5], a method also used in this paper.\nIn our recommendation task, we should deal with the\ncase where user uhas provided only scores of 0and/or 1\n(i.e., the user has only rated disliked pieces). In this case, we\nmodify the method so that the similarity between the contentof the disliked pieces and that of the recommended pieces isminimized. Note that if user uprovides only neutral scores\n(2-scores), random pieces are recommended.\n4. Hybrid Recommendation Method\nTo meet the three requirements described in Section 2.1, we\npropose a hybrid method that integrates rating and contentdata. First, we discuss a problem for modeling user prefer-\nences. Next, we explain an uniﬁed probabilistic model.\n4.1. Problem\nTo achieve the hybrid recommendation, it is necessary to re-\nﬂect both rating and content data in modeling of user pref-erences. However, a problem is that representations of user\npreferences are different between collaborative methods andcontent-based methods. The former represents a preference\nof user uas aN\nM-dimensional vector that contains rating\nscores of all pieces (ru,1,···,ru,NM)(see Section 3.1). The\nlatter represents the preference as a set of NT-dimensional\nfeature vectors of favorite pieces (see Section 3.2).\nIn those representations, user preferences are only indi-\nrectly represented; observable data such as ratings and fea-\ntures do not completely reﬂect the preferences. In addition,\nto build a hybrid recommender system, ad-hoc rules may beused to forcibly merge the two different representations.\n4.2. Solution\nTo solve this problem, we associate rating and content data\nwith newly-introduced variables that represents user pref-\nerences. In this paper, we use a Bayesian network calleda three-way aspect model proposed by Popescul et al. [11].\nThis model has a set of latent variables that directly describe\nsubstantial preferences that cannot be observed. Those pref-\nerences are statistically estimated with theoretical proof. Thiswill contribute to reliable recommendation.\nUnfortunately, Popescul’s method cannot be directly ap-\nplied to our task because it was designed for document rec-ommendation. The document content is represented on the\nbasis of a “bag-of-words” model originally proposed in the\nﬁeld of language processing — the contents of a documentare represented as a set of frequencies of words.\nTo apply the three-way aspect model to music recom-\nmendation, the contents of a musical piece should be rep-resented as a single vector in which all dimensions are se-mantically equivalent. For example, each dimension always\nrepresents a word frequency in Popescul’s method. In addi-\ntion, all dimensions of each vector should sum to unity.\nAfter this, we ﬁrstly discuss how to apply the three-way\naspect model to music recommendation. Then, we explain\nan implementation of our aspect model.\n4.2.1. Three-way Aspect Model with Bag of Timbres\nTo meet the above-mentioned requirements, we propose a\n“bag-of-timbres” model that represents the contents of a\nmusical piece as a set of weights of polyphonic timbres .A n\nidea of polyphonic timbres was proposed by Aucouturier et\nal.[12]. The polyphonic timbres represent the perceptual\n“sounds” not of individual instruments but of their combina-\ntions. Those timbres are important features that character-\nize textures of musical pieces. In addition, there is a meritthat modeling of polyphonic timbres can be applied to vari-ous signals because separation of instrument parts, which is\nvery difﬁcult, is not required.\nIn the three-way aspect model, observation data is asso-\nciated with one of the latent variables Z={z|z\n1,···,zNz},\nwhere Nzis the number of latent variables, as shown in Fig-\nure 2. The latent variables represent user preferences; eachlatent variable conceptually corresponds to a genre ,a n da\nset of proportions of the genres reﬂects a musical taste of\neach user. A possible explanation of this model is that a userstochastically chooses a genre according to his or her pref-\nerence, and then the genre stochastically “generates” piecesand polyphonic timbres. In this model, the conditional inde-pendence of users, pieces, and polyphonic timbres through\nthe latent genres is assumed. Note that the aspect model al-\nlows multiple genres per user, unlike most clustering meth-ods that assign each user to a single genre class.\n4.2.2. Modeling of Polyphonic Timbres\nTo model timbres of audio signals, mel-frequency cepstral\ncoefﬁcients (MFCCs) have often been used in many studiesof genre classiﬁcation [13]. Aucouturier et al. [12] proposed\na method of modeling polyphonic timbres for similarity-\nbased audio clustering. Their method applies a Gaussianmixture model (GMM) to MFCCs of each musical piece.The similarity between two pieces is measured as the re-\nciprocal of the distance between the corresponding GMMs,\nwhich is obtained by using a sampling method.\nTo obtain a “bag-of-timbres” of each piece, we also build\na GMM by using MFCCs of that piece. We assume that each\nGaussian represents MFCC-distribution of a particular poly-\nphonic timbre; mixture weights of Gaussians correspond toweights of timbres. However, Aucouturier’s method cannotbe applied because Gaussians in a GMM are different from\nthose in another GMM. In other words, each GMM repre-\nsents a different combination of polyphonic timbres.\nOur unique idea to solve this problem is that “bags-of-\ntimbres” of all pieces share the same combination of Gaus-\nsians. Means and covariances of the Gaussians are estimated\nby using numerous MFCCs extracted not from each piece\nbut from all the pieces , and mixture weights of the Gaussians\nare discarded in this estimation. Weights of polyphonic tim-\nbres in each piece are obtained as mixture weights of the\nﬁxed Gaussians in that piece; only the mixture weights are\nre-estimated by using MFCCs of the single piece.\nFirst,13-dimensional MFCCs are extracted from audio\nsignals sampled at 16.0 kHz by applying short-time Fourier\ntransformation (STFT) with a Hanning window of 200 [ms].The shifting interval is 100 [ms]. Then, 28-dimensional fea-\nture vectors are obtained (MFCCs, energy, and their delta\ncomponents). Let feature vectors extracted from piece mbe\n{f\nm,i|1≤i≤Im},w h e r e Imis the number of feature vec-\ntors. Next, the parameters of the Gaussians are estimated forall the pieces by using the Expectation-Maximization (EM)\nalgorithm [14], where the number of Gaussians is set to 10.\nThat is, N\nT=1 0 .\nLet the estimated mean vector and covariance matrix of\nthet-th Gaussian be µtandΣt, respectively. The value cm,t\nis a weight of timbre tin piece m, which is obtained by\ncm,t=kmIm/summationdisplay\ni=11\n(2π)28\n2|Σt|1\n2exp/parenleftbigg\n−1\n2D2(fm,i,µt)/parenrightbigg\n,(5)\nwhere D2is the squared Mahalanobis distance given by\n(fm,i−µt)TΣ−1\nt(fm,i−µt)andkmis a normalizing factor\nsuch that/summationtext\ntcm,t=1(m∈M).MZ\nT)|(ztp )| (zmpU\npolyphonic \ntimbremusical \npieceuser)| (umprecommended piece\nwith highm\nlatent variable \n(conceptual genre))|(zup)(up\nFigure 2. Asymmetric representation of our proposed aspect\nmodel using polyphonic timbre weights as music content.\n4.2.3. Estimation of Model Parameters\nAn asymmetric speciﬁcation of the joint probability distri-\nbution p(u, m, t, z )overU,M,T,a n dZis given by\np(u, m, t, z )=p(u)p(z|u)p(m|z)p(t|z). (6)\nAn equivalent symmetric speciﬁcation is obtained by\np(u, m, t, z )=p(z)p(u|z)p(m|z)p(t|z). (7)\nMarginalizing out z, we obtain the joint probability distri-\nbution p(u, m, t )overU,M,a n dTas follows:\np(u, m, t )=/summationdisplay\nzp(z)p(u|z)p(m|z)p(t|z). (8)\nModel parameters p(z),p(u|z),p(m|z),a n dp(t|z)are\ndetermined using the EM algorithm [14] to ﬁnd a local max-imum of the log-likelihood of the training observation data.Letn(u, m, t )be an indicator of how much user ulikes\npolyphonic timbre tin piece m, which is given by\nn(u, m, t )=r\nu,m×cm,t, (9)\nwhere ru,mandcm,tare deﬁned as follows:\n•ru,m is a rating score of user ufor piece m. In our\nmethod, a default rating score ( 2.5) is substituted for\nempty scores, as described in Section 3.1.\n•cm,tis a weight of polyphonic timbre tin piece m.\nGiven training data in this form, the log likelihood Lis\nL=/summationdisplay\nu,m,tn(u, m, t )l o gp(u, m, t ). (10)\nTherefore, the corresponding EM algorithm is given by\nEs t e p\np(z|u, m, t )=p(z)p(u|z)p(m|z)p(t|z)/summationtext\nz/primep(z/prime)p(u|z/prime)p(m|z/prime)p(t|z/prime),(11)\nMs t e p\np(u|z)∝/summationdisplay\nm,tn(u, m, t )p(z|u, m, t ),(12)\np(m|z)∝/summationdisplay\nu,tn(u, m, t )p(z|u, m, t ),(13)\np(t|z)∝/summationdisplay\nu,mn(u, m, t )p(z|u, m, t ),(14)\np(z)∝/summationdisplay\nu,m,tn(u, m, t )p(z|u, m, t ).(15)3φ1341φ2φ43φ\n3φ1341φ2φ43φ\nφφ1341φφφ43φ\nφφ1341φφφ43φ\n3φφφφφφ2φφφφ\n3φφφφφφ2φφφφReRtRfor training for evaluation rating matrix\n#users=4\n#pieces=3\n: masked scoreuser1\n2\n34\nFigure 3. Example of dividing rating matrix Rinto training\nmatrix Rtand evaluation matrix Re.\nThe E and M steps are iterated alternately until the log-\nlikelihood Lconverges to a local maximum. For practical\nuse, it is better to adopt an extended version of the EM algo-rithm (e.g., the deterministic annealing EM algorithm [16])to cope with the data sparseness. In this paper, N\nZis set to\n10. After the training, musical pieces are ranked for target\nuseruaccording to p(m|u)∝/summationtext\ntp(u, m, t ).\n5. Evaluation\nTo compare our hybrid method with the conventional meth-\nods described in Section 3, we performed experiments.\n5.1. Experimental Conditions\nTo perform reliable experiments, it is ideal to use large-scale\nrating data in which the number of ratings given by each user\nis sufﬁcient to a certain extent. However, the construction\nof that data via subjective experiments is extremely time-consuming. One possible way is to collect rating scoresfrom web sites [15]. Amazon provides application program-\nming interfaces (APIs) [17] that allow us to download al-\nmost all information in web sites.\nThe musical pieces we used are Japanese songs of single-\nCDs that were ranked in the weekly top-20 sales rankings\nfrom Apr. 2000 to Dec. 2005. The corresponding scores\nwith user IDs were collected from Amazon. If a user hasrated multiple pieces, we can identify the scores given by the\nsame user. However, there were many unreliable users and\npieces that had few/no scores. To solve this, we extractedusers and pieces so that the number of scores given by a userand that of scores given to a piece were always more than 4.\nAs a result, N\nUwas316 andNMwas358. The density of\nactual scores in rating matrix Rwas2.19%, which is still\nsparse. This means Ris practical data.\nBy using the above mentioned rating data, we compared\nour hybrid method based on the aspect model (called AM)\nwith the two conventional methods: collaborative ﬁltering(called CF) and content-based recommendation (called CB).\nThe value for N\nT(the number of polyphonic timbres) in CB\nandAMwas10, while NZ(the number of latent variables)\ninAMwas set to 10.\n5.2. Evaluation Metrics\nThe experiments were conducted by using 10-fold cross val-\nidation; rating matrix Rwas randomly divided into training\nmatrix Rtand evaluation matrix Reby masking 10% of ac-\ntual scores in R, as shown in Figure 3. The three methodsR rating matrix13 24 5\nφ4φ\n221\n30 4φφφ φ0φ3 4φ\nφ4φ\n221\n30 4φφφ φ0φ3 4φ\n2 (4) 4 (φ)4 (4) 2 (φ)4 (φ) 5 (3)\n2 (4) 4 (φ)4 (4) 2 (φ)4 (φ) 5 (3)6\npiece\nnumberpiece number top-2 rankings\n12\nmasked \nactual scoreuser1\n2\n3\n: masked scoreempty\nscore\n02\n02\n12\n2 = = = NNN 12\n3=N\n02\n02\n12\n2 = = = FFF 33.02\n3=F22\n4=N\n67.02\n4=F32=N\nFigure 4. Example of evaluating results.\nranked musical pieces for each user by using Rtwithout Re.\nThe rankings were evaluated from the viewpoints of recom-mendation accuracy and artist variety.\nThe recommendation accuracy was evaluated by exam-\nining the total top- xrankings of all users ( x=1,3,10).\nHowever, we can only use the actual scores in R\nefor the\nevaluation because Reincludes many empty scores. Here,\nwe focus on x×NUpieces in top- xrankings of NUusers.\nLetNx\nrbe the number of scores that were masked in Rt\nbut were actually r(0≤r≤4)i nRe. Figure 4 shows an\nexample in the case of x=2.T h e t e r m Nxis deﬁned as\nNx=/summationtext\nrNx\nr, which is much less than x×NU. A ratio Fx\nr\nis then obtained by Fx\nr=Nx\nr/Nx. Therefore, the higher\nvalue of Fx\n4indicates the better performance. Note that the\nchance rate of Fx\nris not20% but is the same as the ratio of\npieces that were given r-scores in R. The chance rates of\nFx\n4,···,Fx\n0are 57.9%, 19.1%, 8.6%, 4.9%, and 9.5%.\nThe artist variety was evaluated by calculating the recom-\nmendation variety for each user. Given a target user u,vx\nA\nis deﬁned as the number of artists in top- xrankings. Here,\nvx\nMis deﬁned as the number of pieces by new artists whose\npieces have not been rated by user u.L e tVx\nAandVx\nMbe the\naverage of vx\nAandvx\nMover all users ( Vx\nA,Vx\nM≤x). The\nhigher values of Vx\nAandVx\nMindicate the richer variety.\n5.3. Experimental Results\nTable 1 presents the recommendation accuracy. The values\nFx\n4(x=1,10)byAM are much higher than those by CF\nandCB.F3\n4byAMis almost equal to that by CB. Although\nF10\n4byCBwas greatly degraded from F3\n4byCB,F10\n4by\nAM is almost equal to F3\n4byAM.T h ev a l u e s Fx\n0andFx\n1\n(x=1,3,10)b y AMare lower than those by CFandCB.\nThese results indicate that AM outperforms CFandCBin\nterms of the capability of recommending the favorites.\nNote that Nx\n4andNxbyAMare much lower than those\nbyCF, as indicated in Table 2. To explain this, we set up\na hypothesis that the same artists tend to be recommended\nbyCFbecause most users tend to rate pieces by the same\nartists. If this is right, Nx(the number of pieces that have\nbeen actually rated) will surely become large in CF.\nThe proof of our hypothesis is demonstrated by examin-\ning the artist variety shown in Table 3. The values for Vx\nA\nandVx\nMbyCFare much lower than those by AM. AlthoughTable 1. Evaluation of recommendation accuracy.\nFx\n4Fx\n3Fx\n2Fx\n1Fx\n0\nx CF CB AM CF CB AM CF CB AM CF CB AM CF CB AM\n1 77.6% 85.0% 92.0% 13.8% 5.00% 4.00% 3.45% 5.00% 4.00% 0.86% 5.00% 0.00% 4.31% 0.00% 0.00%\n3 77.5% 82.5% 80.3% 15.4% 12.5% 11.5% 3.08% 2.50% 6.56% 0.04% 2.50% 1.64% 3.52% 0.00% 0.00%\n10 70.8% 69.4% 79.5% 18.1% 17.9% 10.6% 6.51% 4.48% 6.21% 0.10% 5.22% 1.24% 3.61% 2.99% 2.48%\nTable 2. The number of evaluated pieces.\nNx\n4Nx\nx CF CB AM CF CB AM\n1 90 17 23 116 20 25\n3 176 33 49 227 40 61\n10 294 93 128 415 134 161Table 3. Evaluation of artist variety.\nVx\nAVx\nM\nx CF CB AM CF CB AM\n1 1.00 1.00 1.00 0.627 0.933 0.938\n3 2.49 2.93 2.80 2.09 2.76 2.78\n10 7.58 9.30 8.68 8.01 9.17 9.33\nTable 4. Veriﬁcation of capability for recommending no-rated musical pieces.\nFx\n4Fx\n3Fx\n2Fx\n1Fx\n0Nx\nx CB AM CB AM CB AM CB AM CB AM CB AM\n1 89.5% 100% 5.26% 0.00% 0.00% 0.00% 5.26% 0.00% 0.00% 0.00% 19 1\n3 76.9% 80.0% 12.8% 0.00% 5.12% 0.00% 5.12% 0.00% 0.00% 20.0% 39 5\n10 64.1% 72.7% 19.8% 9.10% 5.34% 0.00% 6.87% 0.00% 3.81% 18.2% 131 11\nVx\nAbyAMis lower than that by CB,Vx\nMbyAM is higher\nthat by CB. These results indicate that AMcan produce the\nmost suitable recommendation with a rich variety.\n5.4. Capability for Solving New-item Problem\nWe veriﬁed the capability of AMfor recommending pieces\nthat have not been rated. To do this, 10-fold cross validationwas performed by masking actual scores given to 10% ofall the pieces. Table 4 lists the results. CFcannot recom-\nmend unrated pieces. Because AMconsiders not only music\ncontent but also user ratings, the number of recommendedunrated pieces that can be evaluated is small. However, itseems that AM as well as CBcan reasonably recommend\nthe most favorite pieces even if they have no ratings.\n6. Conclusion\nThis paper has presented a music recommendation method\nthat simultaneously considers user ratings and content sim-ilarity. Our hybrid method is based on a three-way aspectmodel, which can directly represent substantial (unobserv-\nable) user preferences as a set of latent variables introduced\nin a Bayesian network. Probabilistic relations over users,ratings, and contents are statistically estimated. Experimen-tal results showed that our method outperforms conventional\ncollaborative or content-based methods in recommendation\naccuracy. We can conclude that the high recommendationaccuracy was achieved by the reliable modeling of user pref-erences and the integration of rating and content data.\nIn the future, we will try to use automatically-described\nvarious features such as tempi, rhythms and genres for rep-resenting the music content. It is also necessary to deal withentry of unregistered users and musical pieces by incremen-\ntally training the aspect model. In addition, we plan to apply\nour method to community assistance by associating a userwith other users who have similar preferences.Acknowledgments\nThis research has been partially supported by a Grant-in-Aid for Scientiﬁc\nResearch from JSPS Research Fellowship.\nReferences\n[1] A. Uitdenbogerd and R. van Schyndel, “A review of factors affecting\nmusic recommender success,” ISMIR , 2002.\n[2] U. Shardanand and P. Maes, “Social information ﬁltering: Algo-\nrithms for automating ”Word of Mouth”,” ACM CHI’95 Conference\non Human Factors in Computing Systems , 1995, pp. 210–217.\n[3] J. Breese, D. Heckerman, and C. Kadie, “Empirical analysis of pre-\ndictive algorithms for collaborative ﬁltering,” UAI, 1998, pp. 43–52.\n[4] W. Cohen and W. Fan, “Web-collaborative ﬁltering: Recommending\nmusic by crawling the Web,” Computer Networks , vol. 33, no. 1–6,\npp.685–698, 2000.\n[5] K. Hoashi, K. Matsumoto, and N. Inoue, “Personalization of user\nproﬁles for content-based music retrieval based on relevance feed-back,” ACM Multimedia , 2003, pp.110–119.\n[6] B. Logan, “Music recommendation from song sets,” ISMIR , 2004,\npp. 425–428.\n[7] O. Celma, M. Ramirez, and P. Herrera, “Foaﬁng the music: A music\nrecommendation system based on RSS feeds and user preferences,”\ninISMIR , 2005, pp.464–457.\n[8] R. Typke, F. Wiering, and R. Veltkamp, “A survey of music infor-\nmation retrieval systems,” ISMIR , 2005, pp. 153–160.\n[9] P. Melville, R. Mooney, and R. Nagarajan, “Content-boosted collab-\norative ﬁltering,” SIGIR , 2001.\n[10] C. Hayes, “Smart Radio: Building Community-Based Internet Mu-\nsic Radio.” Doctoral Thesis , Trinity College Dublin, 2003.\n[11] A. Popescul, L. Ungar, D. Pennock, and S. Lawrence, “Probabilistic\nmodels for uniﬁed collaborative and content-based recommendation\nin sparse-data environments,” UAI, 2001, pp. 437–444.\n[12] J.-J. Aucouturier, F. Pachet, and M. Sandler, “The way it sounds”:\nTimbre models for analysis and retrieval of music signals,” IEEE\nTrans. Multimedia , vol. 7, no. 6, pp. 1028–1035, 2005.\n[13] J.-J. Aucouturier and F. Pachet, “Musical genre: A survey,” New\nMusic Research , vol. 32, no. 1, pp. 83–93, 2003.\n[14] A. Dempster, N. Laird, and D. Rubin, “Maximum likelihood from\nincomplete data via the EM algorithm,” Royal Statistical Society ,B ,\nvol. 39, pp. 1–38, 1977.\n[15] M. Zadel and I. Fujinaga, “Web services for music information re-\ntrieval,” ISMIR , 2004, pp.478–483.\n[16] N. Ueda and R. Nakano, “Deterministic annealing EM algorithm,”\nNeural Networks, V ol.11, No. 2, pp.271-282, 1998.\n[17] Amazon Web Services: www.amazon.com/gp/aws/landing.html."
    },
    {
        "title": "ISMIR 2006, 7th International Conference on Music Information Retrieval, Victoria, Canada, 8-12 October 2006, Proceedings",
        "author": [],
        "year": "2006",
        "doi": "10.5281/zenodo.1285647",
        "url": "https://doi.org/10.5281/zenodo.1285647",
        "ee": "https://zenodo.org/records/1285647/files/annotated_jingju_arias_1.0.zip",
        "abstract": "The Annotated Jingju Arias Dataset is a collection of 34 jingju arias manually segmented in various levels using the software Praat v5.3.53. The selected arias contain samples of the two main shengqiang in jingju, name xipi and erhuang, and the five main role types in terms of singing, namely, dan, jing, laodan, laosheng and xiaosheng.\n\nThe dataset includes a Praat TextGrid file for each aria with the following tiers (all the annotations are in Chinese):\n\n\n\taria: name of the work (one segment for the whole aria)\n\tMBID: MusicBrainz ID of the audioi recording(one segment for the whole aria)\n\tartist: name of the singing performer (one segment for the whole aria)\n\tschool: related performing school (one segment for the whole aria)\n\trole-type: role type of the singing character(one segment for the whole aria)\n\tshengqiang:boundaries and label of theshengqiangperformed in the aria (including accompaniment)\n\tbanshi: boundaries and label of the banshi performed in the aria (including accompaniment)\n\tlyrics-lines: boundaries and annotation of each line of lyrics\n\tlyrics-syllables: boundaries and annotation of each syllable\n\tluogu: boundaries and label of each of the performed percussion patterns in the aria\n\n\nThe ariasInfo.txt file contains a summary of the contents per aira of the whole dataset.\n\nA subset of this dataset comprising 20 arias has been used for the study of the relationship between linguistic tones and melody in the following papers:\n\n\nShuoZhang, Rafael Caro Repetto, and Xavier Serra (2014) Study of the Similarity between Linguistic Tones and Melodic Pitch Contours in Beijing Opera Singing. In Proceedings of the 15th International Society for Music Information Retrieval Conference (ISMIR 2014), Taipei, Taiwan, October 2731, pp. 343348.\n\n\n\n______ (2015) Predicting Pairwise Pitch Contour Relations Based on Linguistic Tone Information in Beijing Opera Singing. In Proceedings of the 16th International Society for Music Information Retrieval Conference (ISMIR 2015), Mlaga, Spain, October 2630, pp. 107113.\n\n\nHere is the list of the arias from the dataset used in these papers.\n\nThe whole dataset has been used for the automatic analysis of the structure of jingju arias and their automatic segmentation in the following master&#39;s thesis:\n\n\nYile Yang(2016) Structure Analysis of Beijing Opera Arias. Masters thesis, Universitat Pompeu Fabra, Barcelona.\n\n\nUsing this dataset\n\nIf you use this dataset in a publication, please cite the above publications.\n\nWe are interested in knowing if you find our datasets useful! If you use our dataset please email us at mtg-info@upf.edu and tell us about your research.\n\nContact\n\nThe audio recordings used for these annotations are available for research purposes. Please contact Rafael Caro Repetto\n\nrafael.caro@upf.edu\n\n\n\nhttp://compmusic.upf.edu/node/349",
        "zenodo_id": 1285647,
        "dblp_key": "conf/ismir/2006"
    }
]