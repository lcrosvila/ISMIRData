[
    {
        "title": "Bass Playing Style Detection Based on High-level Features and Pattern Similarity.",
        "author": [
            "Jakob Abeßer",
            "Paul Bräuer",
            "Hanna M. Lukashevich",
            "Gerald Schuller"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1418213",
        "url": "https://doi.org/10.5281/zenodo.1418213",
        "ee": "https://zenodo.org/records/1418213/files/AbesserBLS10.pdf",
        "abstract": "In this paper, we compare two approaches for automatic classification of bass playing styles, one based on high- level features and another one based on similarity mea- sures between bass patterns. For both approaches, we com- pare two different strategies: classification of patterns as a whole and classification of all measures of a pattern with a subsequent accumulation of the classification results. Fur- thermore, we investigate the influence of potential tran- scription errors on the classification accuracy, which tend to occur when real audio data is analyzed. We achieve best classification accuracy values of 60.8% for the feature-based classification and 68.5% for the classifica- tion based on pattern similarity based on a taxonomy con- sisting of 8 different bass playing styles.",
        "zenodo_id": 1418213,
        "dblp_key": "conf/ismir/AbesserBLS10",
        "keywords": [
            "automatic classification",
            "bass playing styles",
            "high-level features",
            "similarity measures",
            "bass patterns",
            "classification accuracy",
            "transcription errors",
            "taxonomy",
            "bass playing styles",
            "best classification accuracy"
        ],
        "content": "BASS PLAYING STYLE DETECTIONBASED ONHIGH-LEVEL\nFE\nATURESAND PATTERNSIMILARITY\nJakobAbeßer\nFraunhofer IDMT\nIlmenau,Germany\n(abr@idmt.fraunhofer.de)Paul Br¨auer\nPiranhaMusik& IT\nBerlin, GermanyHanna Lukashevich, GeraldSchuller\nFraunhoferIDMT\nIlmenau,Germany\nABSTRACT\nIn this paper, we compare two approaches for automatic\nclassiﬁcation of bass playing styles, one based on high-\nlevel features and another one based on similarity mea-\nsuresbetweenbasspatterns. Forbothapproaches,wecom-\npare two differentstrategies: classiﬁcation of patternsasa\nwholeandclassiﬁcationofallmeasuresofapatternwitha\nsubsequentaccumulationof the classiﬁcation results. Fur-\nthermore, we investigate the inﬂuence of potential tran-\nscription errors on the classiﬁcation accuracy, which tend\ntooccurwhenrealaudiodataisanalyzed. Weachievebest\nclassiﬁcation accuracy values of 60.8% for the\nfeature-based classiﬁcation and 68.5% for the classiﬁca-\ntion based on patternsimilarity based on a taxonomycon-\nsisting of8differentbassplayingstyles.\n1. MOTIVATION\nMelodicandharmonicstructureswereoftenstudiedin the\nﬁeld of Music Information Retrieval. In genre discrimi-\nnation tasks, however, mainly timbre-related features are\nsomewhat satisfying to the present day. The authors as-\nsume, that bass patterns and playing styles are missing\ncomplementaries. Bass provides central acoustic features\nof music as a social phenomenon, namely its territorial\nrangeandsimultaneousbodilygrasp. Thesequalitiescome\nin different forms, which are what deﬁnes musical genres\nto a large degree. Western popular music with its world-\nwideinﬂuenceonotherstylesisbaseduponcompositional\nprinciples of its classical roots, harmonically structured\naroundthedeepestnote. Africanstylesalsooftenusetonal\nbass patterns as ground structure, while Asian and Latin\nAmericanstylestraditionallypreferpercussivebasssounds.\nIn contrast to the melody (which can easily be interpreted\nin “cover versions” of different styles), the bass pattern\nmost often carries the main harmonic information as well\nasacentralpartoftherhythmicandstructuralinformation.\nAmoredetailedstylisticcharacterizationofthebassin-\nstrument within music recordings will inevitably improve\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom useis granted without fee provided that copies are\nnotmadeordistributed forproﬁtorcommercialadvantageandthatcopies\nbear this notice and the full citation on theﬁrstpage.\nc/circlecopyrt2010 International Society for MusicInformation Retrieval.classiﬁcationresultsingenreandartistclassiﬁcationtasks.\nWithin the ﬁeld of Computational Ethnomusicology (CE)\n[19], the automatic detection of the playing styles of the\nparticipating instruments such as the bass constitutes a\nmeaningfulapproachtounravelthefusionofdifferentmu-\nsical inﬂuences of a song. This holds true for many con-\ntemporarymusicgenresandespeciallyforthoseofaglobal\nmusicbackground.\nTheremainderofthispaperisorganizedasfollows. Af-\nter outliningthe goalsandchallengesin Sec.2 andSec. 3,\nwe provide a brief overview over related work in Sec. 4.\nIn Sec. 5, we introduce novel high-level features for the\nanalysis of transcribed bass lines. Furthermore, we pro-\nposedifferentclassiﬁcationstrategies,whichweapplyand\ncomparelaterinthispaper. We introducetheuseddataset\nand describe the performed experiments in Sec. 6. After\ntheresultsarediscussed,weconcludethispaperinSec.7.\n2. GOALS\nThe goal of this publication is to compare different ap-\nproaches for automatic playing style classiﬁcation. For\nthis purpose, we aim at comparing different classiﬁcation\napproaches based on common statistical pattern recogni-\ntion algorithms as well as on the similarity between bass\npatterns. In both scenarios, we want to investigate the ap-\nplicabilityofaaggregationclassiﬁcationbasedonthesub-\npatternsofanunknownpattern.\n3. CHALLENGES\nThe extraction of score parameters such as note pitch and\nonset from real audio recordings requires reliable auto-\nmatictranscriptionmethods,whichnowadaysarestillerror-\nprone when it comes to analyzing multi-timbral and poly-\nphonic audio mixtures [4,13]. This drawback impedes a\nreliable extraction of high-level features that are designed\nto capture important rhythmic and tonal properties for a\ndescription of an instrumental track. This is one problem\naddressed in our experiments. Another general challenge\nis the translation of musical high-level terms such as syn-\ncopations,scale,orpatternperiodicityintoparametersthat\nare automatically retrievable by algorithms. Information\nregarding micro-timing, which is by the nature of things\nimpossibletoencompassin ascore[9],is leftout.\n93\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)4. PREVIOUS APPROACHES\nWi\nthin the last years, the use of score-based high-level\nfeatures became more popularfor tasks such as automatic\ngenreclassiﬁcation. Toderiveascore-basedrepresentation\nfrom real audio recordings, various automatic transcrip-\ntion algorithms have been proposed so far. The authors\nof [18], [13], and [4] presented algorithms to transcribe\nbass lines. Musical high-level features allow to capture\ndifferentpropertiesfrommusicaldomainssuchasmelody,\nharmony,andrhythm[1,3,10,11]. Bass-relatedaudiofea-\nturesweusedforgenreclassiﬁcationin[18],[1],and[17].\nAn excellent overviewover existingapproachesfor the\nanalysis of expressive music performance and artist-\nspeciﬁc playingstyles is providedin [23]and [24]. In [7],\ndifferent melodic and rhythmic high-levelfeatures are ex-\ntracted before the performed melody is modeled with an\nevolutionary regression tree model. The authors of [15]\nalso used features derived from the onset, inter-onset-\ninterval and loudness values of note progression to quan-\ntifytheperformancestyleofpianoplayersintermsoftheir\ntiming, articulation and dynamics. To compare different\nperformancesintermsofrhythmicanddynamicsimilarity,\nthe authorsof[14]proposeda numericalmethodbasedon\nthecorrelationat differenttimescales.\n5. NOVELAPPROACH\n5.1 Featureextraction\nIn this paper, we use 23 multi-dimensionalhigh-levelfea-\ntures that capture various musical properties for the tonal\nand rhythmic description of bass lines. The feature vec-\ntor consists of 136 dimensions in total. The basic note\nparameters , which we investigate in this paper, are the\nabsolute pitch ΘP, the loudness ΘV, the onset Θ[s]\nOand\nΘ[M]\nO, and the duration Θ[s]\nDandΘ[M]\nDof each note. The\nindices[s]and[M]indicatethatboththeonsetandthedu-\nration of a note can be measured in seconds as well as in\nlengths of measures. All these parameters are extracted\nfrom symbolic MIDI ﬁles by using the MIDI-Toolboxfor\nMATLAB[5].\nAfterwards, further advanced note parameters are de-\nrived before features are extracted. From the pitch dif-\nferences ∆Θ Pbetween adjacent notes in semitones, we\nobtain vectors containing the interval directions ∆Θ(D)\nP\n(being either ascending, constant, or descending), and the\npitch differences in terms of functional interval types\n∆Θ(F)\nP. To derive the functional type of an interval, we\nmapits size toa maximumabsolutevalueof12semitones\nor one octave by using the modulo 12 operation in case it\nis largerthanoneoctaveupwardsordownwards(12semi-\ntones). Theneachintervalisassignedtoafunctioninterval\ntype (prime, second, third etc.) according to well known\nmusicprinciples. Inadditiontothehigh-levelfeaturespre-\nsented in [1], we use variousadditional features related to\ntonality and rhythm in this paper, which are explained in\nthefollowingsubsections.Featuresrelatedtotonality\nWe derive features to measure if a certain scaleis applied\nin a basspattern. Therefore,we take differentbinaryscale\ntemplatesfornaturalminor(whichincludesthemajorscale),\nharmonic minor, melodic minor, pentatonic minor (subset\nof natural minor which also includes the pentatonic major\nscale), bluesminor,wholetone, whole tone halftone, ara-\nbian, minor gypsy and hungariangypsy [21] into account.\nEach scale template consists of 12 values representing all\nsemitones of an octave. The value 1 is set for all semi-\ntones that are part of the scale, the value 0 for those that\nare not. All noteswithin agivenpattern,whicharerelated\ntoacertainscale,areaccumulatedbyaddingtheirnormal-\nized note loudness values ΘV/ΘV,maxwithΘV,maxbe-\ning the maximum note loudness in a pattern. The same is\ndone for all notes, which are not contained in the scale.\nThe ratio of both sums is calculated over all investigated\nscales and over all 12 possible cyclic shifts of the scale\ntemplate. This cyclic shift is performedto cope with each\npossiblerootnoteposition. Themaximumratiovalueover\nall shiftsisdeterminedforeachscaletemplateandusedas\na feature value, whichmeasuresthe presenceof each con-\nsidered scale. We obtain the relative frequencies piof all\npossible values in the vector that contains the interval di-\nrections ( ∆Θ(D)\nP) as well as the vector that contains the\nfunctional interval types ( ∆Θ(F)\nP) and use them as fea-\ntures to characterize the variety of different pitch transi-\ntionsbetweenadjacentnotes.\nFeaturesrelatedtorhythm\nSyncopation embodiesan importantstylistic meansin dif-\nferentmusicgenres. Itrepresentstheaccentuationonweak\nbeats of a measure instead of an accentuation on a neigh-\nbored strong beat that usually would be emphasized. To\ndetect syncopated note sequences within a bass-line, we\ninvestigate different temporal grids in terms of equidis-\ntant partitioning of single measures. For instance, for an\neight-notegrid,wemapallnotesinsideameasuretowards\none of eightsegmentsaccordingto their onset positionin-\nside the measure. In a4\n4time signature, these segments\ncorrespond to all 4 quarter notes (on-beats) and their off-\nbeats in between. If at least one note is mapped to a seg-\nment, it is associated with the value 1, otherwise with 0.\nFor eachgrid,we countthe presenceof the followingseg-\nmentsequences- (1001),(0110),(0001),or(0111). These\nsequences correspond to sequences of alternating on-beat\nandoff-beataccentuationsthatarelabeledassyncopations.\nThe ratios between the number of syncopation sequences\nandthenumberofsegmentsareappliedasfeaturesforthe\nrhythmicalgrids4,8,16,and32.\nWecalculatetheratio Θ(M)\nD(k)/∆Θ(M)\nO(k)betweenthe\nduration value of the k-th note in measure lengths and the\ninter-onset-intervalbetween the k-th note and its succeed-\ning note. Then we derive the mean and the variance of\nthis value over all notes as features. A high or low mean\nvalue indicates whether notes are played legatoorstac-\ncato. The variance over all ratios captures the variation\nbetween these two types of rhythmic articulation within a\n94\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)given bass pattern. To measure if notes are mostly played\nono\nn-beatsoroff-beats, we investigate the distribution of\nnotes towards the segments in the rhythmical grids as ex-\nplained above for the syncopation feature. For example,\nthe segments 1, 3, 5, and 7 are associated to on-beat posi-\ntionsfor an eight-notegrid and a4\n4time signature. Again,\nthis ratio is calculated over all notes and mean and vari-\nance are taken as feature values. As additional rhythmic\nproperties, we derive the frequencies of occurrence of all\ncommonlyusednotelengthsfromhalfnotesto64thnotes,\neach in its normal, dotted, and triplet version. In addition,\nthe relativefrequenciesfromall note-note,note-breakand\nbreak-note sequences over the complete pattern are taken\nasfeatures.\n5.2 Classiﬁcationbasedon statisticalpattern\nrecognition\nWeinvestigatetheapplicabilityofthewell-establishedSup-\nportVectorMachines(SVM)usingtheRadialBasisFunc-\ntion (RBF) as kernel combined with a preceding feature\nselection using the Inertia Ratio Maximization using Fea-\ntureSpace Projection(IRMFSP)asa baselineexperiment.\nThefeatureselectionisappliedtochoosethemostdiscrim-\ninative features and thus to reduce the dimensionality of\nthe feature space prior to the classiﬁcation. Therefore,we\ncalculate the high-levelfeaturesintroducedin 5.1foreach\nbass pattern, which results in an 136 dimensional feature\nspace. Details on both the SVM and the IRMFSP can be\nfoundforinstancein[1].\n5.3 Classiﬁcationbasedon patternsimilarity\nInthispaper,weapply2differentkindsofpatternsimilar-\nity measures, pairwise similarity measures andsimilarity\nmeasures based on the Levenshtein distance . To compute\nsimilarity values between patterns, the values of the on-\nset vector Θ[M]\nOandtheabsolutepitchvector ΘParesim-\nply converted into character strings. In the latter case, we\ninitially subtract the minimum value of ΘPfor each pat-\nternseparatelytoremainindependentfrompitchtransposi-\ntions. Thisapproachcanofcoursebeaffectedbypotential\noutliers,whichdonotbelongtothepattern.\n5.3.1 SimilaritymeasuresbasedontheLevenshtein\ndistance\nThe Levenshtein distance DLoffersa metric for the com-\nputation of the similarity of strings [6]. It measures the\nminimumnumberofeditsintermsofinsertions,deletions,\nandsubstitutions,whicharenecessary,toconvertonestring\ninto the other. We use the Wagner-Fischer algorithm [20]\nto compute DLand derive a similarity measure SLbe-\ntweentwo stringsoflength l1andl2from\nSL= 1−DL/DL,max . (1)\nThe lengths l1andl2correspond to the number of notes\nin both patterns. DL,maxequals the maximum value of\nl1andl2. In the experiments, we use the rhythmic simi-\nlaritymeasure SL,Randthetonalsimilaritymeasure SL,Tderived from the Levenshtein distance between the onset\nΘ[M]\nOand the pitch ΘPas explained in the previous sec-\ntion. Furthermore,we investigate\nSL,RT,Max =/braceleftBigg\nSL,R, SL,R≥SL,T\nSL,T, SL,T> SL,R(2)\nand\nSL,RT,Mean =1\n2(SL,R+SL,T) (3)\nby using the maximum and the arithmetic mean between\nofSL,RandSL,Tasaggregatedsimilaritymeasures.\n5.3.2 Pairwise similarity measures\nIngeneral,we derivea pairwisesimilaritymeasure\nSP=1\n2/parenleftbiggNn,\nm\nNn+Nm,\nn\nNm/parenrightbigg\n(4)\nNn,mdenotesthe numberof notes in pattern n, for which\nat least one note in pattern mexists that have the same\nabsolute pitch value (for the similarity measure SP,T) or\nonset value (for the similarity measure SP,R).Nm,nis\ndeﬁned vice versa. By applying the constraint that both\nonset and absolute pitch need to be equal in Eq. 4, we\nobtainthemeasure SP,RT. Furthermore,we derivetheag-\ngregated similarity measures SP,RT,Max andSP,RT,Mean\nanalogoustoEq. 2andEq. 3.\n6. EVALUATION\n6.1 Data-set\nWe assembled a novel dataset from instructional bass lit-\nerature [12,21], which consists of bass patterns from the\n8genresSwing(SWI),Funk(FUN),Blues(BLU),Reggae\n(REG),Salsa & Mambo (SAL),Rock(ROC),Soul & Mo-\ntown(SOU)and Africa(AFR),arathergeneraltermwhich\nheresigniﬁesSub-SaharanPopularMusicStyles[16]. For\neach genre, 40 bass-lines of 4 measure length have been\nstored as symbolic audio data as MIDI ﬁles. Initial listen-\ning tests revealed that in this data set, which was assem-\nbledandcategorizedbyprofessionalbassplayers,acertain\namount of stylistic overlap and misclassiﬁcation between\ngenresasforinstanceBluesandSwingorSoul&Motown\nand Funk occurs. The overlap is partly inherent to the ap-\nproach of the data sets, which treat all examplesof a style\n(e.g. Rock) as homogenousalthough the sets include typ-\nical patterns of several decades. In some features, early\nRock patterns might resemble early Blues patterns more\nthan they resemble late patterns of their own style [22].\nThus, the data set will be extended further and revised by\neducatedmusicologistsforfutureexperiments.\n6.2 Experiments&Results\n6.2.1 Experiment1-Feature-basedclassiﬁcation\nAs described in Sec. 5.2, we performed a baseline experi-\nmentthatconsistsofIRMFSPforchosingthebest N= 80\nfeatures and the SVM as classiﬁer. The parameter Nhas\n95\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)AFRBLUFUNMOTREGROCSALSWISWISALROCREGMOTFUNBLUAFR 66.25.928.810.806.40\n046.1022.4011.83.915.7\n7.44.272.81.410.63.600\n22.96.951.64.621.810.30\n2104.210.649.48.36.50\n2.60010.7070.416.20\n2501.25.66.71447.50\n017.60000082.4Bass Playing Style (correct)\nBass Playing Style (classified)66.25.928.810.806.40\n046.1022.4011.83.915.7\n7.44.272.81.410.63.600\n22.96.951.64.621.810.30\n2104.210.649.48.36.50\n2.60010.7070.416.20\n2501.25.66.71447.50\n017.60000082.466.25.928.810.806.40\n046.1022.4011.83.915.7\n7.44.272.81.410.63.600\n22.96.951.64.621.810.30\n2104.210.649.48.36.50\n2.60010.7070.416.20\n2501.25.66.71447.50\n017.60000082.466.25.928.810.806.40\n046.1022.4011.83.915.7\n7.44.272.81.410.63.600\n22.96.951.64.621.810.30\n2104.210.649.48.36.50\n2.60010.7070.416.20\n2501.25.66.71447.50\n017.60000082.4\nFigure 1. Exp. 1 - Confusionmatrixfor the feature-based\np\nattern-wise classiﬁcation (all values given in %). Mean\nclassiﬁcation accuracy is 60.8% with a standard deviation\nof2.4%.\nbeen determined to perform best in previous tests on the\ndata-set. A 20-fold cross validation was applied to de-\ntermine the mean and standard deviation of the classiﬁ-\ncation accuracy. For a feature extractionand classiﬁcation\nbased on complete patterns, we achieved 60.8% of accu-\nracy with a standard deviation of 2.4%. The correspond-\ning confusion matrix is shown in Fig. 1. It can be seen,\nthat best classiﬁcation results were achieved for the styles\nFunk,Rock,andSwing. StrongconfusionsbetweenBlues\nand MotownrespectivelySwing, MotownandRock, Reg-\ngae and Africa as well as between Salsa and Africa can\nbe identiﬁed. These confusionssupport the musicological\nassessment of the data-set given in Sec. 6.1. In addition,\ntheycoincidewithhistoricalrelationsbetweenthestylesin\nAfrica, the Caribbean, and Latin America, as well as rela-\ntionswithinNorthAmericaasitiscommonmusicological\nknowledge[8].\nAs a second classiﬁcation strategy, we performed the\nfeatureextractionandclassiﬁcationbasedonsub-patterns.\nTherefore, we divided each pattern within the test set into\nN= 4sub-patterns of one measure length. It was en-\nsured, that no sub-patterns of patterns in the test set were\nused as training data. After all sub-patterns were classi-\nﬁed, the estimated playingstyle for the correspondingtest\nset pattern was derived from a majority decision over all\nsub-pattern classiﬁcations. In case of multiple winning\nclasses, a random decision was applied between the win-\nning classes. For the accumulated measure-wise classiﬁ-\ncation, we achieved only 56.4% of accuracy. Thus, this\napproach did not improve the classiﬁcation accuracy. We\nassume that the majorityof the appliedhigh-levelfeatures\nthatarebasedondifferentstatisticaldescriptors(seeSec.5.1\nfor details), can not providea appropriatecharacterization\nof the sub-patterns, which themselves only consist of 6 to\n9 notesinaverage.\n6.2.2 Experiment2-PatternSimilarity\nThis experiment is based on a leave-one-out cross-\nvalidation scheme and thus consists of N= 320evalu-\nation steps according to the 320 patterns in the data-set.\nWithineachevaluationstep, thecurrentpattern Pkisused\nas test data while all remainingpatterns Plwithl/negationslash=kare\nused as training data. We derive the class estimate ˆckofAFRBLUFUNMOTREGROCSALSWISWISALROCREGMOTFUNBLUAFR57.42.16.4176.408.52.1\n4.2504.218.82.16.34.210.4\n4.46.762.211.12.26.74.42.2\n00095.1002.42.4\n4.70711.665.172.32.3\n04.7014069.82.39.3\n6.84.54.56.84.5068.24.5\n012.507.500080Bass Playing Style (correct)\nBass Playing Style (classified)\nFigure2. Exp. 2-Confusionmatrixforthebestsimilarity-\nb\nased conﬁguration(measure-wiseclassiﬁcation usingthe\nSP,RT,Max similarity measure - all values given in %).\nMean classiﬁcation accuracyis 68.5% with a standardde-\nviationof3.1%.\nPkfromtheclasslabel ˆcofthebest-ﬁttingpattern /hatwidePas\nˆck=cˆl⇔ˆl= arg max\nlSk,l (5)\nwithSk,mrepresentingthesimilaritymeasurebetween Pk\nandPmin the given case. As in Sec. 6.2.1, if multiple\npatterns have the same (highest) similarity, we perform\na random decision among these candidates. This experi-\nment is performed for all similarity measures introduced\nin Sec.6.2.2.\nExp.2a: Pattern-wiseclassiﬁcation. Thebasicapproach\nforapattern-basedclassiﬁcationistouseeachpatternof4\nmeasureslengthasoneitemtobe classiﬁed.\nExp.2b: Accumulatedmeasure-wiseclassiﬁcation . Bass\npatterns are often structured in a way, that the measure or\na part of the measure, which precedes the pattern repeti-\ntion,isoftenalteredrhythmicallyortonallyandthusoften\nvariesgreatlyfromthepattern. Theseﬁguresseparatingor\nintroducingpatternrepetitionare commonlyreferredto as\npickupsorupbeats,meaningthat theydonotvaryorover-\nlapthefollowingpatternrepetitionwhichstartsontheﬁrst\nbeat of the new measure. A pattern-wise classiﬁcation as\ndescribed above thus might overemphasize the difference\nbetween the last measure because the patterns are com-\npared over their complete length. Hence, we investigate\nanotherdecisionaggregationstrategyinthisexperiment.\nAs described in Sec. 6.2.1, we divide each bass pattern\ninto sub-patternsofone measurelengtheach. Withineach\nfoldk, we classify each sub-pattern SPk,lof the current\ntest pattern Pkseparately. At the same time, we ensure\nthat only sub-patterns of the other patterns Piwithi/negationslash=k\nareusedastrainingset forthecurrentfold. Toaccumulate\nthe classiﬁcation resultsin eachfold, we addall similarity\nvalues Sk,lbetween each sub-pattern SPk,ltowards their\nassigned winning pattern(s) Pk,l,win. The summation is\ndone for each of the 6 genres separately. The genre that\nachievethehighestsumisconsideredasthewinninggenre.\nAs depicted in Fig. 3, the proposed accumulated\nmeasure-wise classiﬁcation strategy led to higher classiﬁ-\ncation accuracy values (blue bars) in comparison to a\npattern-wise classiﬁcation (red bars). This approach can\nbe generalized and adoptedto patternsof arbitrary length.\n96\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)020406080100Mean accuracy\nSP,RSP,TSP,RT\nSP,RT,MeanSP,RT,MaxSL,RSL,T\nSL,RT,MaxSL,RT,MeanAccumulated measure−wise classification\nPattern−wise classification\nFigure 3. Mean classiﬁcation accuracy results for experi-\nm\nent2\n0 10 20 30 40 50020406080100\nPercentage ε of transcription errorsAccuracy\n  \nSP,R\nSP,T\nSP,RT,Max\nFigure 4. Exp. 3 - Mean classiﬁcation accuracy vs. per-\nc\nentage εof pattern variation (dotted line - pattern-wise\nsimilarity, solid line - accumulated measure-wise similar-\nity).\nThesimilaritymeasure SP,RT,Max clearlyoutperformsthe\nother similarity measures by over 10 percent points of ac-\ncuracy. The corresponding confusion matrix is shown in\nFig.2. Wethereforeassumethatitisbeneﬁcialtousesim-\nilarityinformationbothbasedonpitchandonsetsimilarity\nof bass patterns. For the pattern-wise classiﬁcation, it can\nbe seen that similarity measures based on tonal similar-\nitygenerallyachieveloweraccuracyresultsincomparison\nto measures based on the rhythmic similarity. This might\nbeexplainedbythefrequentlyoccurringtonalvariationof\npatternsaccordingtothegivenharmoniccontextsuchasa\ncertainchordofachangedkeyin differentpartsofa song.\nThemostremarkableresultinconfusionmatrixisthevery\nhighaccuracyof95.1%fortheMotowngenre.\n6.2.3 Experiment3-Inﬂuenceof patternvariations\nFor the extraction of bass-patterns from audio recordings,\ntwo potential sources of error exist. In most music gen-\nres, the dominant bass patterns are object of small vari-\nations throughout a music piece. An automatic system\nmightrecognizethebasicpatternoravariationofthebasic\npattern. Furthermore, automatic music transcription sys-\ntems are prone to errors in terms of incorrect pitch, onset,\nanddurationvaluesofthenotes. Bothphenomenadirectly\nhaveanegativeeffectonthecomputedhigh-levelfeatures.\nWethereforeinvestigatetheachievableclassiﬁcationaccu-\nracy dependent on the percentage of notes with erroneous\nnoteparameters.\nWe simulate the mentioned scenarios by manipulating\na random selection of εpercents of all notes from each\nunknown pattern and vary εfrom 0% to 50%. The ma-nipulation of a single note consists of either a modiﬁca-\ntion of the onset Θ[M]\nOby a randomly chosen difference\n−0.25≤∆Θ(M)\nO≤0.25(which corresponds to a maxi-\nmum shift distance of one beat for a4\n4time signature), a\nmodiﬁcationoftheabsolutepitch ΘPbyarandomlycho-\nsen difference −2≤∆ΘP≤2(which corresponds to a\nmaximumdistanceof2semitones),orasimpledeletionof\nthe current note from the pattern. Octave pitch errors that\noftenappearinautomatictranscriptionalgorithmswerenot\nconsidered because of the mapping of each interval to a\nmaximum size of one octave as described in Sec. 5.1. In-\nsertions in terms of additional notes, which are not part of\nthepatternwillbetakenintoaccountinfutureexperiments.\nAs depicted in Fig. 4, the accuracy curve of the three\ndifferent pair-wise similarity measures SP,R,SP,Tand\nSP,RT,Max falls until about 40% for a transcription er-\nror rate of 50% Interestingly, the pattern-wise classiﬁca-\ntion based on SP,Rseems to be more robust to transcrip-\ntion errors above 15% in comparison to the accumulated\nmeasure-wise classiﬁcation even thoughit has a lower ac-\ncuracyratefortheassumptionofa perfecttranscription.\n6.2.4 Comparisonto therelatedwork\nThecomparisonoftheachievedresultstotherelatedwork\nis not directly feasible. On one side, it is caused by the\nfact, that different data sets have been utilized. Tsunoo\net al. [18] reported an accuracy of 44.8% for the GZTAN\ndataset1whileusingonlybass-linefeatures. Ontheother\nside, the performance of only bass-line features was not\nevery time stated. The work of Tsuchihashi et al. [17]\nshowed an improvement of classiﬁcation accuracy from\n53.6%to 62.7%while applyingbass-linefeaturescompli-\nmentary to other timbre and rhythmical features, but the\nresultsof genreclassiﬁcation with onlybassfeatureswere\nnotreported.\n7. CONCLUSIONS & OUTLOOK\nIn this paper, different approaches for the automatic de-\ntection ofplayingstylesfromscoreparameterswere com-\npared. These parameters can be extracted from symbolic\naudiodata(e.g. MIDI)orfromrealaudiodatabymeansof\nautomatic transcription. For the feature-based appraoch,\na best result of 60.8% of accuracy was achieved using a\ncombination of feature selection (IRMFSP) and classiﬁer\n(SVM) and a pattern-wise classiﬁcation. Regarding the\nclassiﬁcation based on pattern similarity, we achieved\n68.5% of accuracyusing the combinedsimilarity measure\nSP,RT,Max andameasure-wiseaggregationstrategybased\non the classiﬁcation of sub-patterns. The randombaseline\nis 12.5%. This approach outperformed the common ap-\nproachtoclassify thecompletepatternasonce.\nFor analyzing real-world audio recordings, further mu-\nsical aspects such as micro-timing, tempo range, applied\nplucking & expression styles [2], as well as the interac-\n1G.Tzanetakis and P. Cook. Musical genre classiﬁcation of audio\nsignals. IEEE Transaction on Speech and Audio Processing, 10(5):293-\n302, 2002.\n97\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)tion with other participating instruments need to be incor-\npo\nratedinto a all-embracingstyle descriptionof a speciﬁc\ninstrumentinamusicrecording. Theresultsofexperiment\n4 emphasize the need for a well-performing transcription\nsystem for a high-level classiﬁcation task such as playing\nstyle detection.\n8. ACKNOWLEDGEMENTS\nThis work has been partly supported by the German re-\nsearch project GlobalMusic2One2funded by the Federal\nMinistry of Education and Research (BMBF-FKZ:\n01/S08039B). Additionally, the Thuringian Ministry of\nEconomy,EmploymentandTechnologysupportedthisre-\nsearch by granting funds of the European Fund for Re-\ngional Development to the project Songs2See3, enabling\ntransnational cooperation between Thuringian companies\nandtheirpartnersfromotherEuropeanregions.\n9. REFERENCES\n[1] J. Abeßer, H. Lukashevich, C. Dittmar, and\nG. Schuller. Genre classiﬁcation using bass-related\nhigh-level features and playing styles. In Proc. of the\nInt. Society of Music Information Retrieval (ISMIR\nConference),Kobe,Japan ,2009.\n[2] J. Abeßer, H. Lukashevich, and G. Schuller. Feature-\nbased extraction of plucking and expression styles of\nthe electric bass guitar. In Proc. of the IEEE Int.\nConf. on Acoustics, Speech, and Signal Processing\n(ICASSP) ,2010.\n[3] P. J. Ponce de L´ eon and J. M. I˜ nesta. Pattern recogni-\ntionapproachformusic style identiﬁcationusingshal-\nlow statistical descriptors. IEEE Transactions on Sys-\ntem, Man and Cybernetics - Part C : Applicationsand\nReviews,37(2):248–257,March2007.\n[4] C. Dittmar, K. Dressler, and K. Rosenbauer. A tool-\nbox for automatic transcription of polyphonic music.\nInProc.oftheAudioMostly ,2007.\n[5] Tuomas Eerola and Petri Toiviainen. MIDI Toolbox:\nMATLAB Tools for Music Research . University of\nJyv¨ askyl¨ a,Jyv¨ askyl¨ a,Finland,2004.\n[6] D. Gusﬁeld. Algorithms on strings, trees, and se-\nquences: computer science and computational biol-\nogy. Cambridge University Press, Cambridge, UK,\n1997.\n[7] A.Hazan,M.Grachten,andR.Ramirez.Evolvingper-\nformance models by performance similarity: Beyond\nnote-to-note transformations. In Proc. of the Int. Sym-\nposium,2006.\n[8] Ellen Koskoff, editor. The Garland Encyclopedia of\nWorldMusic-TheUnitedStatesandCanada .Garland\nPublishing,NewYork,2001.\n2seehttp://www.globalmusic2one.net\n3seehttp://www.idmt.de/eng/research topics/songs2see.html[9\n] Gerhard Kubik. Zum Verstehen afrikanischer Musik .\nLitVerlag,Wien,2004.\n[10] C. McKay and I. Fujinaga. Automatic genre classiﬁ-\ncation using large high-level musical feature sets. In\nProc. of the Int. Symposium of Music Information Re-\ntrieval(ISMIR) ,2004.\n[11] C. McKay and I. Fujinaga. jSymbolic: A feature ex-\ntractorfor MIDI ﬁles. In Int. ComputerMusic Confer-\nence(ICMC) ,pages302–305,2006.\n[12] H.-J.Reznicek. I’mWalking-JazzBass . AMA, 2001.\n[13] M. P. Ryyn¨ anen and A. P. Klapuri. Automatic tran-\nscription of melody, bass line, and chords in poly-\nphonic music. Computer Music Journal , 32:72–86,\n2008.\n[14] C. S.Sapp.Hybridnumeric/ranksimilaritymetricsfor\nmusicalperformanceanalysis.In Proc.oftheInt.Sym-\nposiumonMusicInformationRetrieval(ISMIR) ,pages\n501–506,2008.\n[15] E.StamatatosandG.Widmer.Automaticidentiﬁcation\nofmusicperformerswithlearningensembles. Artiﬁcial\nIntelligence ,165:37–56,2005.\n[16] Ruth M. Stone, editor. The Garland Encyclopedia of\nWorld Music - Africa , volume 1. Garland Publishing,\nNewYork,1998.\n[17] Y. Tsuchihashi, T. Kitahara, and H. Katayose. Using\nbass-line features for content-based MIR. In Proc. of\nthe Int. Conference on Music Information Retrieval\n(ISMIR),Philadelphia,USA ,pages620–625,2008.\n[18] E. Tsunoo, N. Ono, and S. Sagayama. Musical bass-\nlineclusteringanditsapplicationtoaudiogenreclassi-\nﬁcation. In Proc. of the Int. Society of Music Informa-\ntionRetrieval(ISMIRConference),Kobe,Japan ,2009.\n[19] George Tzanetakis, Ajay Kapur, W. Andrew Schloss,\nand Matthew Wright. Computational ethnomusicol-\nogy.Journal of Interdisciplinary Music Studies ,\n1(2):1–24,2007.\n[20] Robert A. Wagner and Michael J. Fischer. The string-\nto-string correction problem. Journal of the ACM\n(JACM),21(1):168–173,1974.\n[21] PaulWestwood. BassBible . AMA,1997.\n[22] Peter Wicke. Handbuch der popul ¨aren Musik:\nGeschichte, Stile, Praxis, Industrie . Schott, Mainz,\n2007.\n[23] G. Widmer, S. Dixon, W. Goebl, E. Pampalk, and\nA. Tobudic. In search of the Horowitz factor. AI Mag-\nazine,24:111–130,2003.\n[24] G. Widmer and W. Goebl. Computational models of\nexpressive music performance: The state of the art.\nJournalofNewMusicResearch ,33(3):203–216,2004.\n98\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "Combining Chroma Features For Cover Version Identification.",
        "author": [
            "Teppo E. Ahonen"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1414766",
        "url": "https://doi.org/10.5281/zenodo.1414766",
        "ee": "https://zenodo.org/records/1414766/files/Ahonen10.pdf",
        "abstract": "We present an approach for cover version identification which is based on combining different discretized features derived from the chromagram vectors extracted from the audio data. For measuring similarity between features, we use a parameter-free quasi-universal similarity metric which utilizes data compression. Evaluation proves that combined feature distances increase the accuracy in cover version identification.",
        "zenodo_id": 1414766,
        "dblp_key": "conf/ismir/Ahonen10",
        "keywords": [
            "chromagram vectors",
            "cover version identification",
            "discretized features",
            "quasi-universal similarity metric",
            "audio data",
            "parameter-free",
            "data compression",
            "evaluation",
            "accuracy",
            "combined feature distances"
        ],
        "content": "COMBINING CHROMA FEATURES FOR COVER VERSION\nIDENTIFICA\nTION\nTeppoE. Ahonen\nDepartment of Computer Science,University of Helsinki\nteahonen@cs.helsinki.fi\nABSTRACT\nWe present an approach for cover version identiﬁcation\nwhich is based on combining different discretized features\nderived from the chromagram vectors extracted from the\naudio data. For measuring similarity between features,\nwe use a parameter-free quasi-universal similarity metric\nwhich utilizes data compression. Evaluation proves that\ncombined feature distances increase the accuracy in cover\nversion identiﬁcation.\n1. INTRODUCTION\nMeasuring similarity in music is an essential challenge in\nmusicinformationretrieval(MIR).However,thedeﬁnition\nofsimilarityisnottrivial. Clearly,piecesofmusicfromthe\nsame genre are similar in various features such as orches-\ntration,buttheessentialsimilarityofthecompositionscan\nvary largely within the genre.\nCover version identiﬁcation provides a valid, objective\nway to estimate how well similarity in music can be rec-\nognized and measured. Cover versions often differ in var-\nious musical features, but can still be distinguished to be\ndifferentperformancesofonecompositionbyahumanlis-\ntener. Thus, successful cover version identiﬁcation yields\nimportant information on how similarity in music can be\nmeasured and how features affecting the similarity can be\nrepresented.\nWe approach the problem of cover version identiﬁca-\ntion by taking into account several features derived from\nthe chromagram. These features are represented using dif-\nferent kinds of discrete alphabets and the similarity be-\ntweenfeaturesiscalculatedusingasimilaritymetriccalled\nnormalized compression distance (NCD) [9]. Evaluation\nshows that when using NCD for cover version identiﬁca-\ntion, better identiﬁcation accuracy can be obtained by tak-\ningseveralfeaturesintoaccountinsteadofjustfocusingon\na single feature.\nCover version identiﬁcation is an objective way to esti-\nmatetheperformanceofaretrievalsystembasedonmusi-\ncalsimilarity. Coverversions,especiallyinpopularmusic,\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnotmadeordistributedforproﬁtorcommercialadvantageandthatcopies\nbear this noticeand thefull citation on theﬁrst page.\nc/circlecopyrt2010International Society for Music Information Retrieval.are often intentionally different from the original record-\nings of the composition. Changes are common in such\nfeatures as the musical keys, structures, tempos and ar-\nrangements. Also, the lyrics can be altered, translated to\nanother language, or completely discarded. It is more dif-\nﬁculttoestimatethefeatureswhichdonotchange,butusu-\nally these are the melodic and harmonic features.\nWhen successful, cover version identiﬁcation provides\na reliable content-based way to measure the essential sim-\nilarity in different pieces of music. This provides various\npotentialtargetsofapplicationsforsuchsystemsandalgo-\nrithms, ranging from end users to music researchers.\nIn recent years, cover version identiﬁcation has gained\na signiﬁcant amount of interest from the MIR community.\nAlthougharelativelyshorttimehaspassedsincetheprob-\nlemofcoverversionidentiﬁcationwasaddressed,theprob-\nlemhasbeenstudiedextensivelyandwithvariousdifferent\nkinds of approaches.\nThe most important feature in cover version identiﬁca-\ntion is the chromagram. Chromagram, also known as the\npitch class proﬁle, is a sequence of 12-dimension vectors\nwhich describe the relative energy of each semitone pitch\nclass. As such, chromagram captures important tonal in-\nformation and represents the harmonic and melodic con-\ntent of the audio ﬁle.\nVarious different methods for measuring similarity be-\ntweenchromagramsorfeaturesderivedfromchromagrams\nexist. These include dynamic time warping and other edit\ndistancevariants,dotproductandcrosscorrelation. Foran\nextensive and comparative review on different cover ver-\nsion identiﬁcation approaches, we refer to [20].\nThe MIREX (Music Information Retrieval Evaluation\neXchange) is a community-driven effort providing evalua-\ntionfordifferentMIRapplications. Coverversionidentiﬁ-\ncationhasbeenaMIREXtasksince2006,andthroughthe\nyears,severaldifferentapproacheshaveparticipatedinthe\nevaluation and signiﬁcant improvement in the identiﬁca-\ntion performance can be perceived. In 2009, the best per-\nformingcoverversionidentiﬁcationapplicationperformed\nwith a mean of average precision value of 0.751, suggest-\ning that there still are several unsolved problems in cover\nversion identiﬁcation which need to be addressed until the\nproblem can be declared solved.\nWe propose an approach that uses a similarity metric\ncallednormalizedcompressiondistance(NCD)[9]formea-\nsuring the similarity between features extracted from the\n1http://www.music-ir.org/mirex/2009/index.php/Audio CoverSongIdentiﬁcation Results\n165\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)audioﬁles. Forfeatures,weextractseveraldifferentrepre-\nsentations\nfromthechromagramvectors. Asdatacompres-\nsion works with discrete symbols, we use several differ-\nent techniques for quantizing the continuous chroma val-\nues. Ourstartingpointisthatdifferentrepresentationshave\nmore distinguishingpower when combined than they have\nwhen used alone. Also, we assume that when using NCD\nthe chromagram cannot be quantized into a representation\nwhichbothcontainsalltherequiredinformationandisnot\ntoonoisy. Thus,differentfeaturesmustberepresentedand\nmeasured on their own.\nThe rest of this paper is organized as follows. In Sec-\ntion2,wegiveabrieftutorialontheconceptsandtheories\nbehind the normalized compression distance. In Section 3\nwe describe the chroma features we use for identiﬁcation.\nTheapproachisevaluatedinSection4. Finally,wepresent\nconclusions and discussion inSection 5.\n2. NORMALIZED COMPRESSION DISTANCE\nNormalizedcompressiondistance(NCD)isadistancemet-\nric that has its roots in information theory. The idea is to\nmeasure the information in an object using Kolmogorov\ncomplexity, the length in bits of the shortest binary pro-\ngram that produces the object as an output. Based on the\nKolmogorov complexity, a universal information distance\ncan be calculated. This distance, called normalized infor-\nmation distance [9], is denoted\nNID(x,y) =max {K(x|y),K(y|x)}\nmax {K(x),K(y)}(1)\nwhere K(x\n)is the Kolmogorov complexity of the string\nxandK(x|y)is the conditional Kolmogorov complexity,\nmeaning the length of K(x)given the information of y.\nHowever, Kolmogorov complexity is non-computable,\nand thus the normalized information distance cannot be\ncalculated. However, Kolmogorov complexity can be ap-\nproximated using any standard lossless data compression\nalgorithm. The better the compression of a string is, the\ncloser the approximation is to the Kolmogorov complex-\nity.\nThenormalizedcompressiondistanceapproximatesthe\nKolmogorovcomplexitywiththeaidofadatacompression\nalgorithm. For strings xandy, the NCD is denoted\nNCD (x,y) =C(xy)−min{C(x),C(y)}\nmax {C(x),C(y)},(2)\nwhere C(x\n)is the length of the string xwhen compressed\nusing a standard lossless data compression algorithm C\nandxyis the concatenation of the two strings.\nNCDisproventoberobustagainstnoiseinthedata[8],\nand studies have proven that observing several common\npitfalls of the compression algorithms will help to evade\nproblems when measuring the distances [7]. Especially,\nPPM-based (Prediction by Partial Matching) compression\nalgorithms have been proven to be resistant against noise\n[8]andperformwellinNCDcalculationdespitethelengths\nof the ﬁles [7].Normalizedcompressiondistancehasbeenusedforsev-\neral tasks in MIR. In the symbolic domain, there has been\nresearch at least in melody classiﬁcation [16], genre clas-\nsiﬁcation [9], composer classiﬁcation [9] and piano music\nclassiﬁcation[10]. Intheaudiodomain,NCDhasbeenap-\npliedfortaskssuchasstructure-basedclustering[3],genre\nclassiﬁcation [6,17], cover version identiﬁcation [1] and\nquery by example [12].\n3. CHROMA FEATURES\nThe chromagram seems to be the only valid feature to be\nused for cover version identiﬁcation. For example, the\nMFCC vectors capture the timbral information of the au-\ndio ﬁle, but this information has very little help in iden-\ntifying cover versions. The chromagram is robust against\nthe changes in instrumentation and dynamics, and it cap-\ntures both melodic and harmonic information from the au-\ndio ﬁle.\nTheeasiestwaytomeasuresimilaritybetweenchroma-\ngrams using NCD would seem to be converting the chro-\nmagram into a sequence of characters and calculating the\ndistance between these. However, we noticed that this ap-\nproach has several drawbacks. If the alphabet used in se-\nquences is small, the information contained in the chro-\nmagram will be too reduced and different sequences will\nturn out too similar, making distinguishing the sequences\nchallenging. A large alphabet that contains most of the\ninformation of the chromagram, on the other hand, will\nmakesequencesnoisyandleadintoinsigniﬁcantcompres-\nsion and thus into impractical identiﬁcation. Our solution\nis to extract various feature sets of the chromagram and\nmeasure the similaritiesbetween each set.\nFor obtaining chromagram from the audio ﬁle, we use\nMIRToolbox [15], version 1.3. The window length for the\nFourier transform needed in obtaining the chromagram is\n0.1858secondsandthehopfactoris0.875. Weuseafour-\noctaverangeoftransformationwithaminimumfrequency\nof 55 Hz.\nWe do not have any tempo estimation and beat aver-\naging over the chromagram frames. This is based on the\nassumption that unsuccessful tempo estimation might lead\ntoevennoisierrepresentationsandthustoworseidentiﬁca-\ntion results. A similar observation was made in [2], where\nframe-based identiﬁcation yielded better results than the\ntactus-based version. Also, in [1] it was suggested that the\nshorter chroma sequences produced by the beat averaging\nmay have a negative impact on the NCD values, because\nthe error between K(x)andC(x)minimizes as the ﬁle\nlength increases [9].\nFor compression, we use the PPMZ compression al-\ngorithm. The PPMZ is a statistical, more efﬁcient com-\npressionalgorithmthanthemorecommonlyusedgzipand\nbzip2. Thus,itprovidesabetterapproximationoftheKol-\nmogorov complexity. This may not, however, lead auto-\nmatically into better NCD values, as the improvements in\ncompressionmaybedifferentforthedifferentitemsinthe\nformulaandthuscausetheNCDvaluetomoveawayfrom\nthe NID value [9].\n166\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)3.1 Chroma Sequence Labeling\nIn\norder to measure similarity successfully with a com-\npression algorithm, the continuous chroma vectors need\nto be quantized. Out of the several existing quantization\nmethods, the hidden Markov model (HMM) has the ad-\nvantage of taking into account the temporal statistics. The\nHMMapproachhasbeenstudiedextensivelyinconverting\nchroma vectors into a discrete representation, and it is a\ncommonmethodwhenestimatingachordsequencerepre-\nsentation from the harmonic content of the audio. The ap-\nproach can be described as a process of using the chroma\nvectorsasobservationsforaHMMwhoseeachstaterepre-\nsentsatriadchord,trainingthemodelwiththeexpectation-\nmaximization (EM) algorithm, and ﬁnally obtaining the\nstate transition path using the Viterbi algorithm.\nOut of the several different methods, we use the one\nsuggested by Bello and Pickens [4]. This means initializ-\ning the state transition parameters according to a double-\nnested circle of ﬁfths and selecting the mean vectors and\nthecovariancematricesonthebasisofmusicalknowledge.\nWhen training the model with the EM algorithm, we train\nonly the state distribution and transition parameters and\nleave the observation parameters untrained.\nThe 24-chord estimation provides a robust but slightly\nnoisy representation of the harmonic content of the audio\nﬁle. When observing the representations we noticed that\ntheestimatedchordswereoccasionallyoscillatingbetween\nmajor and minor chords of the same root note. This sug-\ngests that the third of the chord can harm the sequence la-\nbeling. SimilarobservationcanbederivedfromtheMIREX\nchord detection task where average overlap scores usu-\nally become better when the major and minor chords are\nmerged (see for example the results of the MIREX Chord\nDetection Task 20092). This led us to an experiment with\na 12-state HMM, where the triad of the chord is discarded\nfromthechordtemplates. Inthe12-stateHMM,theinitial\nparameters are set in a similar manner as with the 24-state\nHMM, but with respect to the simpler model and reduced\nchords. Assuch,thestatesequenceprovidedbytheViterbi\nalgorithm can be seen as a “power chord” representation.\nSuch representation is clearly too reduced and inaccurate\ntodistinguishtheversionsontheirown,butitseemstoim-\nprove the identiﬁcation performance when used in parallel\nwith the 24-state HMM representation. In Figures 1 and 2\nwedisplaystatesequencesderivedfromasingleaudioﬁle\nusing 24- and 12-state HMMs, respectively.\n3.2 Chromagram Flux\nIn addition to the chromagram vectors themselves, we ex-\nperimented on whether the distance between subsequent\nchromagram vectors might have any effect. A somewhat\nsimilarapproachwaspresentedin[14],wherea12-dimen-\nsion dynamic chroma vector feature called delta chroma\nwas utilized. The delta chroma describes the degree of\nchroma changes on all possible intervals.\nHere, we do not consider the delta chroma, but instead\n2http://www.music-ir.org/mirex/2009/index.php/Audio ChordDetection Resultswe\ncalculatethedistancesbetweensuccessivechromavec-\ntors. A similar approach was utilized in [21], where corre-\nlation between adjacent chroma vectors was used as a fea-\nture in identiﬁcation. We discovered empirically that the\nManhattandistance(thecity-blockdistance)hadmoredis-\ntinguishing power for our work than Eucledian or cosine\ndistances.\nThe Manhattan distances between chroma vectors of a\nmusicalpiececanbeseenasatimeseries. Todiscretizethe\ntime series, we use SAX (Symbolic Aggregate approXi-\nmation)[18]. Inshort,SAXdiscretizesthecontinuousval-\nues by ﬁrst reducing their dimensionality using piecewise\naggregate approximation and then discretizing the values\naccordingtoaGaussiancurve. WechoseSAXafterexper-\nimenting with several quantization methods. Also, SAX\nhasbeenusedsuccessfullyforquantizationwhencalculat-\ning similaritybetween time series using NCD [13].\nSelection of the SAX parameters is not a trivial task.\nAs we want to represent the whole chromagram ﬂux as a\nstringof characters, the sliding window is set to the length\nof the chromagram. The alphabet size and SAX accuracy\nparameters are more difﬁcult to choose. We set the alpha-\nbet size to four and the number of frames per character to\nten. These were chosen empirically, and thus are open to\ndiscussion.\n3.3 Strongest Tone Sequence\nThe chromagram represents not only harmonic, but also\nmelodic information contained in the audio ﬁle. We tested\nseveral methods to have more melodic information from\nthe chromagram to be presented in a format suitable for\nNCD, but as with the chromagram quantization, different\nrepresentations proved either to be too noisy or too reduc-\ning.\nHowever, a straightforward way to represent some of\nthe mid-level melodic information proved to increase the\nidentiﬁcation accuracy. We took the index of the strongest\npitch class of a chroma vector (for a normalized chroma\nvector, the pitch class with the value of one), and repre-\nsented the piece of music as a sequence of the strongest\npitch class components. For a less densely orchestrated\npieceofmusic,thisrepresentationprovidessomeinforma-\ntion of the predominant melody of the piece. Even with\nmore dense arrangements, it provides a representation that\ndisplays information different from the sequence labeling.\n3.4 Transposition\nBecause cover versions are occasionally performed in a\ndifferent key, the distance between chroma features can\nturn out large if key invariance is not addressed, even if\nthe chroma features would otherwise be fairly similar. To\nobtain key invariance, a possible solution is to calculate\ndistances between all 12 transpositions of the candidate\nversion, but this is time-consuming. Another solution is to\ntranspose the chromagrams into a common key using key\nestimation, but as with the tempo estimation, key estima-\ntion can fall short and lead to even worse identiﬁcation re-\nsults. We do not estimate the keys from the chromagrams,\n167\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)0 200 400 600 800 1000 1200CC#DD#EFF#GG#AA#BCmC#mDmD#mEmFmF#mGmG#mAmA#mBm The Beatles: With A Little Help From My Friends\nFrameChord\nFigure 1. 24-state HMM Viterbi path for W ith A Little\nHelp From My Friends performed by The Beatles.\nbut instead use Optimal Transposition Index (OTI) [19]\nto transpose the chroma sequences into a common key.\nIn OTI, the transposition index is selected by ﬁrst taking\nthe global chroma vectors (by summing and normalizing\nthe chroma vectors) of the two pieces of music. Then,\nthe transposition index is selected by rotating the candi-\ndate global chroma vector 12 times and calculating the\ndot product between each pair of the target and candidate\nglobal chroma vectors. The rotation with the highest dot\nproductisselectedasthetranspositionindexandthewhole\nchromagramofthecandidateisrotatedaccordingtothein-\ndex. Fastandstraightforward,OTIhasalsobeenprovento\nprovide better identiﬁcation accuracy than using the key\nestimation [19]. We apply OTI before any feature extrac-\ntion.\n3.5 Total Distance\nAfter the distances between all the features of the pieces\nofmusicarecalculated, thetotaldistanceforapairofper-\nformancesisobtainedbysimplytakingthemeanofallthe\nfeaturedistances. Thedistancescouldbeweightedaccord-\ning to the importance of the features. To reduce the pos-\nsible bias in the mean values caused by outliers, we also\nmeasured the total distance as the median of all measured\nfeature distances.\n4. EVALUATION\n4.1 Test Data\nToevaluatetheperformanceofourapproach,wecollected\na data set of original performances and their cover ver-\nsions. For each original piece of music we included ﬁve\ncover versions. The data set included 25 such six-song\nsets and to complete the collection, a total number of 600\nuniquepiecesofmusicwereincluded,thusmakingthecol-\nlection a total of 750 pieces of music with 150 possible\nqueries.\nThe material was obtained from personal music collec-\ntions and contains mostly western popular music, but with0 200 400 600 800 1000 1200CC#DD#EFF#GG#AA#BThe Beatles: With A Little Help From My Friends\nFrameChord\nFigure 2. 12-state HMM Viterbi path for W ith A Little\nHelp From My Friends performed by The Beatles.\ncover versions ranging from classical music renditions to\nworld music and electronic versions. Apart from studio\ncoverversionsbydifferentartists,thedatasetalsoincludes\nlive versions and a few remixes of the original versions.\nThe complete detailed content of the data set can be re-\nquested from the authors.\n4.2 Results\nWeusedeachofthe150versionsinthedatasetasaquery.\nFrom the output distance matrix, we calculated the total\nnumber of identiﬁed covers in the top ﬁve (TOP-5), the\nmean of average precisions (MAP) and the mean rank of\nthe ﬁrst identiﬁed cover (RANK). The results, using the\nmean as the total distance, are depicted inTable 1.\nTo present the effect of each different feature in the\nidentiﬁcation, we ran the algorithm for the whole test data\nset using only selected features of the feature set. The re-\nsults for different feature sets, using the mean as the total\ndistance, are depicted in Table 2.\nThedifferencebetweenusingthemeanandmedianval-\nues as the total distance is depicted in Figure 3. Generally,\nusingthemedianasthetotaldistanceprovidedsmallerdis-\ntances. This suggests that outliers do exist in the feature\ndistances and overall identiﬁcation could be improved by\ntaking them into account. However, using the mean as the\ntotal distance provided slightly better identiﬁcation accu-\nracy with a TOP-5 rating of 263 against the TOP-5 rating\nof 243 of the median distance.\n4.3 Comparison to the LabROSA Cover Song\nDetection System\nToseehowwellourapproachperformsincomparisonwith\nanother cover version identiﬁcation approach, we ran our\ntestdatawiththeLabROSACoverSongIdentiﬁcationsoft-\nware [11]. To our knowledge, this is the only cover ver-\nsionidentiﬁcation application that isfreelydistributedand\navailable online3.\n3http://labrosa.ee.columbia.edu/projects/coversongs/\n168\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Measure Value Range\nTOP-5 263[0–750]\nMAP 0.410 [0–1]\nRANK 4.795[1–745]\nTable 1. Results of the 150 query evaluation.\nFeatures used TOP-5 MAP\n24-state HMM 2160.356\n24- and 12-state HMM 2420.378\nHMMs and Chroma ﬂux 2490.399\nAll features 2630.410\nTable 2. The effect of combining different features.\nThe\ncomparison between the results of our approach\nand the LabROSA application is depicted in Table 3. The\nresultsshowthattheperformanceofourapplicationiscom-\nparable with the performance of the LabROSA system.\nHowever,weareawarethattheLabROSAapplicationwas\nintroduced several years ago and is possibly not compara-\nblewithsomeofthestate-of-the-artapproaches. Forcom-\nparing the performance of our approach with more state-\nof-the-artapproaches,werefertothefutureMIREXcover\nsong identiﬁcation task where our application will be sub-\nmitted.\n5. CONCLUSIONS\nWe have presented an approach for cover version identi-\nﬁcation that combines different features derived from the\nchromagrams extracted from the audio ﬁles. To discretize\ncontinuous values, several techniques such as HMM and\nSAX have been used. The similarity between discretized\nfeatures is calculated using a distance metric called nor-\nmalized compression distance, which uses data compres-\nsion to approximate the Kolmogorov complexities of the\nobjects and as such is a quasi-universal, parameter-free\nsimilaritymetric.\nBased on the results, it is evident that the chroma fea-\nture combination together with the NCD metric can be\nusedforcoverversionidentiﬁcation. Asourresultsproved,\ncombining different features and composing the ﬁnal dis-\ntance based on the distances between these features pro-\nvides more accurate identiﬁcation with the NCD. The al-\ngorithm was tested with competent results against a large\ndata set consisting of various different kinds of versions\nfrom original performances.\nThe biggest obstacle for using normalized compression\ndistance for cover version identiﬁcation is the process of\nconverting continuous features to discrete representations.\nExtracting features from audio is likely to yield noisy rep-\nresentations, and although NCD has been proved to be re-\nsistant against noise [8], it still affects the identiﬁcation.\nOur approach has more emphasis on the harmonic fea-\ntures,andobservingtheresultssupportsthis: piecesofmu-\nsicwithdistinctive,recognizableharmoniccontentareeas-1 2 3 4 5 6 7 8 9 100.680.70.720.740.760.780.80.820.840.860.88\nCover Set IDDistance\n  \nMean distance\nMedian distance\nFigure 3. Mean distances for the ﬁrst identiﬁed covers\nin\nten 6-version cover sets using mean and median total\ndistances.\nily identiﬁed even when arrangements and structures vary.\nAlso, as stated, we comprise the total distance simply as\na mean of all distances, but this could be improved by\nweighting the different distances according to their rele-\nvance. Using the median as the total distance also gave a\nﬁnding of the bias caused by the outliers.\nAnother issue demanding attention is that the phases of\nthe measuring process each have a wide selection of pa-\nrameters. Parameter selection is present in every phase of\nthe identiﬁcation process: from selecting the parameters\nof the Fourier transform when obtaining the chromagram\nto the choice of the compression algorithm used for cal-\nculating the NCD values. It is unclear if the parameters\nwehaveselectedareoptimalfortheidentiﬁcationtaskand\nalso the possibility of overﬁtting is evident. Future work\naddressing the parameter selection is under consideration.\n5.1 Remark on Different Versions\nAs the cover version dataset also included live renditions\nand remixed versions of the original recordings, we took a\ncloser look at the cases of these versions.\nLive versions, either by the performers of the original\nversions or by a different performer, were in most cases\nidentiﬁedverywell. Weseetworeasonsforthis. First,live\nversionsareoftenquitesimilartooriginalversions,having\nonlyslightmodiﬁcationssuchaskey,tempoorsmallstruc-\ntural differences (lengthier introductions or solo sections).\nSecond, the live versions are less densely produced and\narranged, whereas the studio versions are usually far more\norchestrated. Thismakesthechromafeaturesderivedfrom\nlive versions less noisy, which in turn beneﬁts the similar-\nitymeasuring. Allinall,liveversiondetectioncanbeseen\nas a somewhat easier case of cover version identiﬁcation.\nThus, developing and testing cover version identiﬁcation\nalgorithms using predominantly live renditions may lead\nto slightlybiased results.\nRemixed versions, on the other hand, were often far\nmore difﬁcult to identify. In many cases, remixed versions\n169\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)System TOP-5 MAP\nOur approach 2630.410\nLabROSA 2560.405\nTable 3. The results between our approach and LabROSA\nsystem.\nshare\nonly limited elements similar to the original perfor-\nmance, usually combining audio elements of the original\nperformances with completely different, and often elec-\ntronic,instrumentation. Whereasliveversionsusuallyhave\nvery little changes in structures and a stripped-down in-\nstrumentation, the situation is often completely vice versa\nwith remix versions: the original structure is often com-\npletely discarded and the instrumentation is usually even\nmore dense than the original performance. We feel free\nto say that remix version identiﬁcation is a far more dif-\nﬁcult case of cover version identiﬁcation. Thus, it would\nbeinterestingtoseehowwellcoverversionidentiﬁersper-\nform when the task is speciﬁcally remix version identiﬁ-\ncation. To our knowledge, version identiﬁcation special-\nized in remix identiﬁcation has been done only on a small\nscale [5].\n6. ACKNOWLEDGEMENTS\nThisworkhasbeensupportedbytheAcademyofFinland,\ngrant#129909. TheauthorwouldliketothankKjellLem-\nstr¨om and Rainer Typke for their insightful comments on\nearlierversionsofthispaper. Theauthorisalsogratefulto\nP¨aivi Suomalainen for her valuable help in collecting and\norganizing the test data.\n7. REFERENCES\n[1] T. E. Ahonen. Measuring harmonic similarity us-\ning PPM-based compression distance. In WEMIS’09,\nCorfu, Greece, 2009.\n[2] J. P. Bello. Audio based cover song retrieval using ap-\nproximate chord sequences: Testing shifts, gaps, beats\nand swaps. In ISMIR’07, Vienna, Austria, 2007.\n[3] J.P.Bello.Groupingrecordedmusicbystructuralsim-\nilarity. In ISMIR’09, Kobe, Japan, 2009.\n[4] J. P. Bello and J. Pickens. A robust mid-level repre-\nsentation for harmonic content in music signals. In IS-\nMIR’05, London, UK, 2005.\n[5] M. Casey and M. Slaney. Fast recognition of remixed\nmusic audio. In ICASSP-07, Hawaii, USA, 2007.\n[6] Z. Cataltepe, Y. Yaslan, and A. Sonmez. Music\ngenre classiﬁcation using MIDI and audio features.\nEURASIP Journal on Applied Signal Processing,\n2007(1), 2007.\n[7] M. Cebri ´an, M. Alfonseca, and A. Ortega. Common\npitfalls using the normalized compression distance:what to watch out for in a compressor. Communica-\ntionsinInformationandSystems,5(4):367–384,2005.\n[8] M. Cebri ´an, M. Alfonseca, and A. Ortega. The\nnormalized compression distance is resistant against\nnoise.IEEE Transactions on Information Theory,\n53(5):1895–1900, 2007.\n[9] R. Cilibrasi and P. M. B. Vit ´anyi. Clustering by com-\npression. IEEE Transactions on Information Theory,\n51(4):1523–1545, 2005.\n[10] R. Cilibrasi, P. M. B. Vit ´anyi, and R. de Wolf. Algo-\nrithmicclustringofmusicbasedonstringcompression.\nComputer Music Journal, 28(4):49–67, 2004.\n[11] D. P. W. Ellis and G. E. Poliner. Identifying ’cover\nsongs’ with chroma features and dynamic program-\nming beat tracking. In ICASSP-07, Hawaii, USA,\n2007.\n[12] M.Hel ´enandT.Virtanen.Asimilaritymeasureforau-\ndio query by example based on perceptual coding and\ncompression. In DAFx-07, Bordeaux, France, 2007.\n[13] E. Keogh, S. Lonardi, and C. A. Ratanamahatana. To-\nwardsparameter-freedatamining.In KDD’04,Seattle,\nWashington, USA, 2004.\n[14] S. Kim and S. Narayanan. Dynamic chroma feature\nvectors with applications to cover song identiﬁcation.\nInMMSP’08, Cairns, Australia, 2008.\n[15] O. Lartillot and P. Toiviainen. A MATLAB toolbox\nformusicalfeatureextractionfromaudio.In DAFx-07,\nBordeaux, France, 2007.\n[16] M. Li and R. Sleep. Melody classiﬁcation using a\nsimilarity metric based on Kolmogorov complexity. In\nSMC’04, Paris, France, 2004.\n[17] M. Li and R. Sleep. Genre classiﬁcation via an LZ78-\nbased string kernel. In ISMIR’05, London, UK, 2005.\n[18] J. Lin, E. Keogh, S. Lonardi, and B. Chiu. A sym-\nbolic representation of time series, with implications\nfor streaming algorithms. In DMKD’03, San Diego,\nCalifornia, USA, 2003.\n[19] J. Serr `a, E. G´omez, and P. Herrara. Transposing\nchroma representations to a common key. In IEEE CS\nConferenceonTheUseofSymbolstoRepresentMusic\nand Multimedia Objects, 2008.\n[20] J. Serr `a, E. G´omez, and P. Herrara. Audio Cover\nSong Identiﬁcation and Similarity: Background, Ap-\nproaches, Evaluation, and Beyond. Springer-Verlab\nBerlin / Heidelberg, 2010.\n[21] Y. Yu, M. Crucianu, V. Oria, and L. Chen. Local sum-\nmarization and multi-level LSH for retrieving multi-\nvariant audio tracks. In MM’09, Beijing, China, 2009.\n170\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "Discovering Metadata Inconsistencies.",
        "author": [
            "Bruno Angeles",
            "Cory McKay",
            "Ichiro Fujinaga"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1415550",
        "url": "https://doi.org/10.5281/zenodo.1415550",
        "ee": "https://zenodo.org/records/1415550/files/AngelesMF10.pdf",
        "abstract": "This paper describes the use of fingerprinting-based querying in identifying metadata inconsistencies in music libraries, as well as the updates to the jMusicMeta- Manager software in order to perform the analysis. Test results are presented for both the Codaich database and a generic library of unprocessed metadata. Statistics were computed in order to evaluate the differences between a manually-maintained library and an unprocessed collection when comparing metadata with values on a MusicBrainz server queried by fingerprinting.",
        "zenodo_id": 1415550,
        "dblp_key": "conf/ismir/AngelesMF10",
        "keywords": [
            "fingerprinting-based querying",
            "identifying metadata inconsistencies",
            "music libraries",
            "jMusicMeta-Manager software",
            "analysis",
            "Codaich database",
            "generic library",
            "unprocessed metadata",
            "MusicBrainz server",
            "fingerprinting"
        ],
        "content": "DISCOVERING METADATA INCONSISTENCIES \nBruno Angeles  Cory McKay  Ichiro Fujinaga  \nCIRMMT  \nSchulich School of Music  \nMcGill University  \nbruno.angeles@ mail.\nmcgill.ca  CIRMMT  \nSchulich School of Music  \nMcGill University  \ncory.mckay@mail.mcgill.ca  CIRMMT  \nSchulich School  of Music  \nMcGill University  \nich@music.mcgill.ca  \nABSTRACT \nThis paper describes the use of fingerprinting-based \nquerying in identifying metadata inconsistencies in music \nlibraries, as well as the updates to the jMusicMeta-\nManager software in order to perform the analysis. Test \nresults are presented for both the Codaich database and a \ngeneric library of unprocessed metadata. Statistics were \ncomputed in order to evaluate the differences between a \nmanually-maintained library and an unprocessed \ncollection when comparing metadata with values on a \nMusicBrainz server queried by fingerprinting. \n1. INTRODUCTION \n1.1 Purpose \nMetadata is useful in organizing information, but in large \ncollections of data it can be tedious to keep that \ninformation consistent. Whereas decision making in \nprevious environments such as traditional libraries was \nlimited to a small number of highly trained and \nmeticulous people, the democratization of music brought \nabout by the digital age poses new challenges in terms of \nmetadata maintenance, as music can now be obtained \nfrom diverse and potentially noisy sources .  \nThe contributors to many popular metadata \nrepositories tend to be much less meticulous , and may \nhave limited expertise. The research presented here \nproposes a combination of metadata management \nsoftware, acoustic fingerprinting, and the querying of a \nmetadata database in order to discover possible errors and \ninconsistencies in a local music library. \nMetadata entries are compared between a library of \nmanually-maintained files and a metadata repository, as \nwell as between a collection of unprocessed metadata and \nthe same repository, in order to highlight the possible \ndifferences between the two. \n1.2 Metadata and its Value \nMetadata is information about data. In music, it is \ninformation related to recordings and their electronic \nversion (such as the performer, recording studio, or \nlyrics), although it can also be event information about \nartists or other attributes not immediately linked to a \nrecorded piece of music. Corthaut et al. present 21 \nsemantica lly related clusters of metadata [1], covering a \nwide range of info rmation that illustrates the variety of \nmetadata that can be found in music . Lai and Fujinaga [6] suggest more than 170 metadata elements organized  into \nfive types, in research pertaining t o metadata for \nphonograph record ings. Casey et al . [2] distinguish \nfactual from cultural metadata. The most widely used \nimplementation of musical metadata is the ID3 format \nassociated with  MP3 files [5].  \n The main problem with metadata is its inconsistency . \nThe fact that it is stored in databases containing \nthousands, if not millions, of entries often means that the \ndata is supplied by several people who may have different \napproaches. Spelling mistakes may go unnoticed for a \nlong time, and information such as an artist’s name might \nbe spelled in different but equally valid ways. \nAdditionally, several metadata labels —most notably \ngenre —are highly subjective.  \nWhen maintaining large databases of music, valid and \nconsistent metadata facilitates the retrieval an d \nclassification of recordings, be it for Music Information \nRetrieval (MIR) purposes or simply for playback. \n1.3 Metadata Repositories and Maintenance Software \nMetadata repositories are databases that include \ninformation about recordings. When they are supported \nby an Application Programming Interface (API), they \nprovide users with a convenient way of accessing the \nstored metada ta. Existing music metada ta repositories \ninclude MusicBrainz, Discogs, Last.fm, and Allmusic, to \nname just a few [3]. \nSeveral software solutions exist that provide ways to \naccess, use, and adjust musical metadata. These include \nMusicBrainz Picard, MediaMonkey, jMusicMeta-\nManager, and Mp3tag. The first three applications \nsupport fingerprinting in some form, but Mp3tag does \nnot. GNAT [10] allows querying by metadata or \nfingerprinting to explore aggregated semantic Web data. \njMusicMetaManager is the only application that performs \nextensive automated internal textual metadata error \ndetection, and it also produces numerous useful summary \nstatistics not made available by the alternatives. \n1.4 Acoustic Fingerprinting \nAcoustic fingerprinting is a procedure in which audio \nrecordings are automatically analyzed and \ndeterministically associated with a key that consumes \nconsiderably less space than the original recording. The \npurpose of using the key in our context is to retrieve \nmetadata for a given recording using only the audio \n195\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \ninformation, which is more reliable than using the often \nhighly noisy metadata packaged with recordings. \nAmong other attributes, fingerprinting algorithms are \ndistinguished by their execution speed; their robustness to \nnoise and to various types of filtering; and their \ntransparency to encoding formats and associated \ncompression schemes [1].  \nThe fingerprinting service used in this paper is that of \nMusicIP (now known as AmpliFIND). It is based on \nPortable Unique Identifier (PUID) codes [9]. These are \ncomputed using the GenPUID piece of software. The \nPUID format was chosen for its association with the \nMusicBrainz API. \n1.5 Method \nThis research uses jMusicMetaManager [7] (a Java \napplication for maintaining metadata), Codaich [7] (a \ndatabase of music with manually-maintained metadata), a \nreference library of music labeled with unprocessed \nmetadata, and a local MusicBrainz server at McGill \nUniv ersity’s Music Technology Area.  Reports were \ngenerated in jMusicMetaManager, an application for \nmusic metadata maintenance which was improved as part \nof this project by the addition of fingerprinting-based \nquerying. This was done in order to find the percentage of \nmetadata that was identical between the manually-\nmaintained metadata and that found on the MusicBrainz \nserver of metadata. In addition to comparing the artist , \nalbum , and title fields, a statistic was computed indicating \nhow often all three of these specific fields match ed \nbetween the local library and the metadata server, a \nstatistic that we refer to as ―identical metadata .‖ Raimond \net al. [10] present a similar method, with the ultimate \nobjective of accessing information on the Semantic Web. \nAn unprocessed test collection, consisting of music \nfiles obtained from file sharing services, was used in \norder to provide a comparison between unmaintained and \nmanually-maintained metadata. This unprocessed library \nis referred to as the ―re ference library‖ i n this paper. \n2. JMUSIC METAMANAGER  \njMusicMetaManager [7] is a piece of software designed \nto automatically detect metadata inconsistencies and \nerrors in musical collections, as well as generate \ndescriptive profiling statistics about such collections. The \nsoftware is part of the jMIR [8] music information \nretrieval software suite, which also includes audio, MIDI, \nand cultural feature extractors; metalearning machine \nlearning software; and research datasets. jMusic-\nMetaManager is, like all of the jMIR software, free, open-\nsource, and designed to be easy to use.  \nOne of the important problems that jMusic-\nMetaManager deals with is the inconsistencies and \nredundancies caused by multiple spellings that are often found for entries that should be identical. For example, \nuncorrected occurrences of both ―Lynyrd Skynyrd‖ and \n―Leonard Sk inard ‖ or of the multiple valid spellings of \ncomposers such as ―Stravinsky‖ would be problematic for \nan artist identification system that would incorrectly \nperceive them as different artists.  \nAt its simplest level, jMusicMetaManager calculates \nthe Levenshtein (edit) distance between each pair of \nentries for a given field. A threshold is then used to \ndetermine whether two entries are likely to, in fact, \ncorrespond to the same true value. This threshold is \ndynamically weighted by the length of the strings. This is \ndone separately once each for the artist, composer, title, \nalbum,  and genre  fields. In the case of titles, recording \nlength is also considered, as two recordings might \ncorrectly have the same title but be performed entirely \ndifferently \nThis approach, while helpful, is too simplistic to detect \nthe full range of problems that one finds in practice. \nAdditional pre-processing was therefore implemented and \nadditional post-modification distances were calculated. \nThis was done in order to reduce the edit distance of \nstrings that probably refer to the same thing, thus making \nit easier to detect the corresponding inconsistency. For \nexample: \n Occurences of ―The ‖ were removed (e.g., ―The \nPolice‖ should match ―Police‖).  \n Occurrences of ― and ‖ were replaced with ― & ‖  \n Personal titles were converted to abbreviations \n(e.g., ―Doctor John‖ to ―Dr. John‖).  \n Instances of ―in’‖ were replaced with ―ing‖ (e.g., \n―Breakin’ Down‖ to ―Breaking Down‖).  \n Punctuation and brackets were removed (e.g., \n―R.E.M.‖ to ―REM‖).  \n Track numbers from the beginnings of titles and \nextra spaces were removed. \nIn all, jMusicMetaManager can perform 23 pre-\nprocessing operations. Furthermore, an additional type of \nprocessing can be performed where word orders are \nrearranged (e.g., ―Ella Fitzgerald‖ should match \n―Fitzgerald, Ella,‖ and ―Django Reinhardt & Stéphane \nGrappelli‖ should match ―Stéphane Grappelli & Django \nReinhardt‖). Word subsets can also be considered (e.g., \n―Duke Ellington‖ might match ―Duke Ellington & His \nOrches tra‖).  \njMusicMetaManager also automatically generates a \nvariety of HTML-formatted statistical reports about music \ncollections, including multiple data summary views and \nanalyses of co-occurrences between artists, composers, \nalbums, and genres. This allows one to easily acquire and \npublish HTML collection profiles. A total of 39 different \nHTML reports can be automatically generated to help \nprofile and publish musical datasets. \n196\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \nUsers often need a graphical interface for viewing and \nediting a database’s metadata. It was therefore decided to \nlink jMusicMetaManager to the Apple iTunes software, \nwhich is not only free, well-designed, and commonly \nused, but also includes an easily parsed XML-based file \nformat. iTunes, in addition, has the important advantage \nthat it saves metadata modifications directly to the ID3 \ntags of MP3s as well as to its own files, which means that \nthe recordings can easily be disassociated from iTunes if \nneeded. iTunes can also access Gracenote’s metadata  \nautomatically, which can then be cleaned with \njMusicMetaManager. \njMusicMetaManager can extract metadata from iTunes \nXML files as well as directly from MP3 ID3 tags. Since \nMusic Information Retrieval systems do not typically read \nthese formats, jMusicMetaManager can also be used to \ngenerate ground-truth data formatted in ACE XML or \nWeka ARFF formats. \n3. CODAICH \nCodaich is a curated audio research dataset that is also \npart of jMIR [8]. It is constantly growing, and is now \nsignificantly larger than its original size of 20,849 \nrecordings. The version used for the experiments \ndescribed in this paper contains 32,328 recordings. \nThere is music by nearly 3,000 artists in Codaich, with \n57 different musical genres represented. The dataset can \nbe divided into four broad genres of Jazz, Popular, \nClassical, and (the somewhat problematic) World, \nhenceforth referred to as ―genre groups .‖ These \nrecordings are labeled with 19 metadata fields. \nThe metadata of the original version of Codaich was \nmeticulously cleaned, both by hand and with jMusic-\nMetaManager. Care has been taken to maintain this very \nhigh level of metadata quality as the dataset has grown. \nThe metada ta for the original version of Codaich is \navailable at the jMIR web site \n(http://jmir.sourceforge.net), and the metadata of the most \nrecent version can be obtained in iTunes XML form by \ncontacting the authors. \n4. THE REFERENCE LIBRARY \nIn order to provide context, it was decided that a \nbenchmark was needed against which the metadata \nconsistency between Codaich and MusicBrainz could be \ncompared. This was the motivation behind assembling the \nreference library, a combination of files downloaded from \ntorrent-based networks and files that were obtained before \nthe emergence of such systems. In the former case, files \nwere downloaded as entire albums, while the rest of the \nreference library consists of recordings that were \ndownloaded individually. The reference library consists \nof 1363 recordings by 446 artists, with 70 musical genres \nrepresented. Since the reference library contained many music files \nwith no ID3 metadata, but did hold some information in \nthe files ’ names, metadata was created in such cases based \non file names .  \n5. METHOD \n5.1 Overview of the Experiments \nExperiments were conducted to determine whether or not \nmanually-maintaine d Codaich musical metadata showed a \ndifferent level of consistency with MusicBrainz’s \ninformation than the unprocessed reference library, for a \nfixed number of metadata fields.  \nThe first step of the experiments consisted of obtaining \nPUID codes for each recording in each of the two \nlibraries. The PUID information was stored in an XML \nfile for later parsing by jMusicMetaManager. \nIn jMusicMetaManager, all the recordings in Codaich \nand the reference library were matched with entries in the \nXML file of PUID codes. PUID-based querying was \nperformed on the MusicBrainz server, and a report of \nmatching fields was generated for the chosen metadata \nfields. \nSimilar research done by Raimond et al. [10] presents \nGNAT, a Python application that supports PUID -based \nfinger printing for track identification on a personal m usic \nlibrary. The authors suggest accessing information \npertinent to the user through the Semantic Web by \nquerying, while we analyze the rate of consistency \nbetween the two datasets.  \n5.2 Changes to jMusicMetaManager \nRunning jMusicMetaManager on a large library of music \nfiles revealed that the application was not able to read the \nID3 tags of files using releases 1, 2, 3, and 4 of the ID3v2 \nprotocol. This was due to the choice of metadata API, \njava_mp3 , used in jMusicMetamanager. Replacing \njava_mp3  with the jaudiotagger  API allowed us to read \nthose formats. \nFingerprinting-based querying was added to \njMusicMetaManager to enhance its capabilities. \nMusicBrainz’s official (although no longer in active \ndevelopment) Java API was used (it is known as \nlibmusicbrainz-java ), since it allows querying by PUID , \nand a new corresponding report was added to \njMusicMetaManager. \nTo expedite the querying process, threaded querying \nwas implemented. This was applied to a copy of the \nMusicBrainz database hosted on a server at McGill \nUniversity , something that was important in overcoming \nthe one-query-per-second limitation of the public \nMusicBrainz server.  \n197\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \n \nGenre \ngroup  Number of \nidentified \nrecordings  Identical artist  Identical a lbum Identical title  Identical artist, \nalbum, and title  \nClassical 1,476  3% 2% 6% 0% \nJazz 3,179  70% 25% 64%  12% \nPopular 16,206  84%  52%  61% 32%  \nWorld  1,640  58% 29% 46% 11% \nTable 1.  Querying results for Codaich. Percentages represent the number of entries that were identical to those in Musi c-\nBrainz.  The top results per statistic are identified in bold. \nGenre \ngroup  Number of \nidentified \nrecordings  Identical artist  Identical a lbum Identical title  Identical artist, \nalbum, and title  \nClassical 285 17% 0% 5% 0% \nJazz 181 43% 14% 39% 4% \nPopular 481 79%  19%  51%  10%  \nWorld  115 57% 12% 41% 3% \nTable 2.  Querying results for the reference library. Percentages represent the number of entries that were identical to \nthose in MusicBrainz.  The top results per statistic are identified in bold. \nGenre group  Identical ar tist Identical a lbum Identical title  Identical artist, \nalbum, and title  \nClassical -14% 2% 1% 0% \nJazz 27%  11% 25%  8% \nPopular 5% 33%  11% 22%  \nWorld  2% 17% 6% 9% \n \n5.3 Reporting and Statistics \nNot all files listed in the XML file of PUID codes were \nsuccessfully identified by the MusicBrainz server (and, of \ncourse, MusicBrainz identification does not guarantee \ncorrectness). Several files list unanalyzable or pending  as \ntheir status, while other extracted PUID codes did n \nreturn any result at all. Only identified recordings are \nused in this paper’s statistics.  The ratios of files that were \nnot processed in each collection are specified in the \nfollowing sections. \nA case-insensitive string comparison was used in order \nto determine whether or not the artist , album,  and title \nfields were identical on the MusicBrainz server and in the \nfiles’ metadata.  \n6. RESULTS \nReports were generated for both Codaich and the \nreference library. The former database is maintaine d \nmanually and is assumed to contain very few metadata \nerrors and inconsistencies, while the latter contains many \nmetadata problems due to the wide range of contributors \nand their varied interest and methods in maintaining \nmetadata. 6.1 Codaich Results \nOf the 32,328 songs in Codaich, 22,501 (70%) were \nidentified on the MusicBrainz server using PUID values, \n44 files were assigned a status of unanalyzable  by \nGenPUID, and 84 were assigned the label pending.  Of the \nremaining files, 9,645 (30%) had a PUID value but \nresulted in no hit on the MusicBrainz server. \nTable 1 shows the metadata consistency between \nCodaich and MusicBrainz .  \n6.2 Reference Library Results \nOf the 1,363 songs in the reference library, 1,062 (78%) \nwere identified on the MusicBrainz server using PUID \nvalues. 5 files were assigned a status of unan alyzable  by \nGenPUID, and 18 were assigned the label pending . Of the \nremaining files, 274 (20%) had a PUID value but resulted \nin no hit on the MusicBrainz server.  \nTable 2 shows the metadata consistency between the \nreference library and MusicBrainz. \n6.3 Comparison of Codaich and the Reference L i-\nbrary \nTable 3 illustrates the difference between the entries of \nTable 2 and Table 1. Positive values indicate a higher rate \nof matching metadata between MusicBrainz and Codaich \n198\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \nthan between MusicBrainz and the reference library, \nwhile negative values mean the opposite. Although the \nfirst two tables are based on different libraries, the values \nof their difference provide us with a rough estimate of the \nquality difference between metadata in unprocessed music \nfiles collected from the internet and a curated library.  \n7. DISCUSSION \n7.1 Global Observations \nA comparison of Table 1 and Table 2 shows that the \nstrongest agreement with MusicBrainz for the artist  and \nalbum  fields, as well as for t he ―identical metadata‖  \nstatistic, is obtained for Popular music. This supports the \nargument that the main drivers of community-based \nmetadata services are musical genres with which the most \npeople are familiar, particularly among the techno-\nlogically-savvy younger generations who may be more \nlikely to contribute to metadata libraries.  \nWith respect to titles, however, there is a greater level \nof MusicBrainz consistency in the manually-maintained \nlibrary for Jazz recordings than for Popular recordings \n(albeit only 3% more, and the MusicBrainz title \nconsistency is greatest for Popular music in the reference \nlibrary ). This may be due to better knowledge of Jazz on \nthe part of Codaich’s curator relative  to the general \npublic. \nA potential cause of the relatively low percentages of \nTable 2 is the fact that part of the reference library \nconsists of files that used ID3v1 tags instead of ID3v2 \nones. ID3v1 tags limit the size of the title, artist , and \nalbum  fields to 30 characters [5], which could cause \nmismatches in the case of longer entries of the \nMusicBrainz server compared to the limited ones of the \nID3v1 tags. \n7.2 Differences Between a Curated Database and an \nUnprocessed Collection \nThe largest changes between the two collections, as can \nbe seen from Table 3, were obtained (in decreasing order) \nfor the album  field of Popular music (33%), the artist  \nfield of Jazz (27%), the title field of Jazz (25%), and all \nthree chosen fields for Popular music (22%). In 14 out of \n16 statistics, the MusicBrainz metadata matches the \ncurated database’s information more often than it does the \nunprocessed collection. We now focus on the two fields \nthat were more MusicBrainz-consistent for the reference \nlibrary than for Codaich. For Classical music, the \nconsistency of the artist  field for the reference library \n(17%) is much higher than that for Codaich (3%).  \nWe were surprised to notice that the artist  field results \nappeared to be ―worsened‖ by the manual mainte nance of \nmetadata in this way. This can perhaps be explained by \nnoting that Codaic h fills the artist  field with the name of \nthe performer , while reserving composer names for the composer field. Most metadata on the MusicBrainz server \nand in the reference library, in contrast, lists the \ncomposer’s name in the artist  field, ignoring the \ncomposer  field, possibly due to inherent limitations of the \nchoice of underlying database schema.  \nThe second statistic that is not higher in Table 1 than \nin Table 2 is ―identical metadata‖ (artist, album , and title) \nfor Classical music. In both cases, this statistic has a value \nof 0%, meaning that none of the Classical files considered \nhad values matching the MusicBrainz entries for all three \nof these fields. \nIndeed, the MusicBrainz consistency for Classical \nMusic was very low, even for individual fields. \nAssociating metadata with Classical music is challenging, \nas one must make decisions such as choosing how to \nconvert names from the Cyrillic alphabet to Latin \ncharacters1, choosing who the artist is, choosing whether \nto include long subtitles in album names , choosing \nwhether to include the key and opus number in the title , \netc. It is important to note that different ways of writing \nmetadata may be perfectly valid in these situations, but \nmultiple valid values can cause retrieval problems. \n7.3 Classical/World Music vs. Jazz/Popular Music \nTables 1 and 2 allow us to distinguish two sets of genre \ngroups, based on the frequency of matching metadata \nbetween the local files and the MusicBrainz server: Jazz \nand Popular music in one group, World and Classical \nmusic in the other .  \nIndeed, the highest matches are most often obtained for \nJazz and Popular music, while the lowest results are in \ngeneral obtained for Classical and World music. Popular \nand Classical music have different characteristics and use \ndifferent fields [4], which leads to complications when \napplying a uniform metadata structure to different genres. \nClassical music has by far the lowest MusicBrainz \nagreement in both music collections. World music has \nresults between those of Classical music and Popular/Jazz \nmusic. The fact that Popular (and to a certain extent Jazz) \nmusic uses clearly-defined artist , album,  and title tags \nfacilitates the matching of such information on a web \nserver. \n7.4 Matching Results and Correct Results \nHuman maintenance of metadata has the expected effect \nof making metadata consistent across a library. Let us \nconsider the case of Classical music. The low rate of \nconsistency of matching artist names between Codaich \nand MusicBrainz might lead us to think that manual \nmaintenance had a negative effect on the metadata, but in \nreality the Codaich metadata is arguably better than the \n                                                           \n1 Although this happens in other genres, it could be \nargued that conversion between languages is statistically \nmore likely in Classical and World music.  \n199\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \nMusicBrainz metadata. The ID3v2 protocols support a \ncomposer field, and should be used for that purpose, as \ndone in Codaich. \nIt was interesting to observe in Codaich’s file-by-file \nreport of metadata comparisons that certain files that were \nassumed to belong to the same album were listed as \nbelonging to different ones in the web service’s fetched \nmetadata. Consider the case of recordings that appear in \ndifferent contexts such as movie soundtracks, \ncompilations, and regular studio albums. Certain users \nmay want to keep these recordings identified as being \nassociated with the original studio release, while others \nwill prefer to associate them with the other releases on \nwhich they appear, both points of view being valid. Such \nan issue would be avoided by allowing multi-value \nmetadata fields.  \n8. CONCLUSIONS \nOur results indicate that manually-maintained music files \ntend to have a greater level of metadata consistency with \nMusicBrain z than do unprocessed files. This does not, \nhowever, mean that the web service’s metadata contains \nthe correct values. We also noted that the matching rates \nof metadata vary across the analyzed genre groups. \nDifferences in metadata between a manually-maintained \ndatabase and a metadata database such as MusicBrainz \nmay be due to a variety of reasons.  \nCombining jMusicMetaManager with fingerprinting \nquerying can facilitate the cleanup of local collections of \nmusic. The matching of metadata between local files and \na metadata server is particularly useful in the case of \nPopular music and Jazz, recent genres for which metadata \nfields are more easily attributed than for Classical and \nWorld music. With this new querying feature, jMusic-\nMetaManager is useful in more situations and unique in \nits reporting capability. We have also seen that, although \nat first sight the manual maintenance of metadata revealed \nsome lowering of matches with the MusicBrainz server, it \nwas justified by the proper use of attributes by the human \ncurator. This can be seen as an illustration of the \nunreliability of collaboratively-maintained databases such \nas MusicBrainz for musical genres that do not benefit \nfrom the same public exposure as Popular music and Jazz. \nIn light of this potential unreliability, the use of \nmetadata management software such as jMusicMeta-\nManager is recommended in order to detect potential \nerrors and inconsistencies in local libraries of music, as it \ncan detect problems without reference to external sources \nof potentially unreliable information. \nHaving discussed the possibility that multiple values of \na metadata field may all be valid in certain cases, we \nstress the need for multidimensional musical metadata. Through our analysis of statistical results, we confirm \nthe pertinence of the manual maintenance of metadata, \nand explain the reasons behind minor unexpected results. \njMusicMetaManager already computes metrics that \ncan be used to detect metadata inconsistencies. As future \nwork, integrating such features in the comparison of local \nand remote metadata would be helpful in that a threshold \nof comparison could allow the user to identify metadata \nthat is similar enough. We expect an improvement in \nClassical music retrieval with such a change.  \nFinally, a similar experiment could be performed by \nmanually correcting a given library of music while \nkeeping the unprocessed version as reference. \n9. REFERENCES \n[1] Cano, P., E. Battle, T. Kalker, and J. Haitsma. 2002. \nA Review of Algorithms for Audio Fingerprinting . \nProceedings of the IEEE Workshop on Multimedia \nSignal Processing , 169 –73. \n[2] Casey, M., R. Veltkamp, M. Goto, M. Leman, C. \nRhodes, and M. Slaney. 2008. Content-Based Music \nInformation Retrieval. Proceedings of the IEEE . \n668–96, 2008. \n[3] Corthaut, N., S. Govaerts, K. Verbert, and E. Duval. \n2008. Connecting the dots: Music metadata \ngeneration, schemas and applications,‖ Proceedings \nof the International Conference on Music \nInformation Retrieval . 249 –54. \n[4] Datta, D. 2002. Managing metadata, Proceedings of \nthe International Conference on Music Information \nRetrieval . 249 –51. \n[5] ID3.org. 2003. Home: ID3.org. http://www.id3.org.  \nAccessed 21 March 2010. \n[6] Lai, C., and I. Fujinaga. 2006. Data dictionary: \nMetadata for phonograph records. Proceedings of \nthe International Conference on Music Information \nRetrieval. 1–6. \n[7] McKay, C., D. McEnnis, and I. Fujinaga. 2006. A \nlarge publicly accessible prototype audio database \nfor music research,‖ Proceedings of the \nInternational Conference on Music Information \nRetrieval . 160–3. \n[8] McKay, C., and I. Fujinaga. 2009. jMIR: Tools for \nautomatic music classification,‖ Proceedings of the \nInternational Computer Music Conference . 65–8. \n[9] MusicBrainz. 2009. How PUIDs work : MusicBrainz \nWiki. http://wiki.musicbrainz.org/ HowPUIDsWork. \nAccessed 21 March 2010. \n[10] Raimond, Y., C. Sutton, and M. Sandler. 2008. \nAutomatic interlinking of music datasets on the \nsemantic web. Linked Data on the Web Workshop \n(LDOW2008).  \n \n200\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "Is There a Relation Between the Syntax and the Fitness of an Audio Feature?.",
        "author": [
            "Gabriele Barbieri",
            "François Pachet",
            "Mirko Degli Esposti",
            "Pierre Roy"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1416014",
        "url": "https://doi.org/10.5281/zenodo.1416014",
        "ee": "https://zenodo.org/records/1416014/files/BarbieriPER10.pdf",
        "abstract": "Feature generation has been proposed recently to generate feature sets automatically, as opposed to human-designed feature sets. This technique has shown promising results in many areas of supervised classification, in particular in the audio domain. However, feature generation is usually performed blindly, with genetic algorithms. As a result search performance is poor, thereby limiting its practical use. We propose a method to increase the search perfor- mance of feature generation systems. We focus on ana- lytical features, i.e. features determined by their syntax. Our method consists in first extracting statistical proper- ties of the feature space called spin patterns, by analogy with statistical physics. We show that spin patterns carry information about the topology of the feature space. We exploit these spin patterns to guide a simulated annealing algorithm specifically designed for feature generation. We evaluate our approach on three audio classification prob- lems, and show that it increases performance by an order of magnitude. More generally this work is a first step in using tools from statistical physics for the supervised clas- sification of complex audio signals.",
        "zenodo_id": 1416014,
        "dblp_key": "conf/ismir/BarbieriPER10",
        "keywords": [
            "feature generation",
            "automatic feature sets",
            "genetic algorithms",
            "supervised classification",
            "audio domain",
            "search performance",
            "topology of the feature space",
            "spin patterns",
            "simulated annealing algorithm",
            "analytical features"
        ],
        "content": "IS THERE A RELATION BETWEEN THE SYNTAX AND THE FITNESS OF\nAN AUDIO FEATURE?\nGabriele Barbieri\nDip. di Matematica\nUniversit `a di Bologna\ngbarbieri@dm.unibo.itFranc ¸ois Pachet\nSony CSL\nParis\npachet@csl.sony.frMirko Degli Esposti\nDip. di Matematica\nUniversit `a di Bologna\ndesposti@dm.unibo.itPierre Roy\nSony CSL\nParis\nroy@csl.sony.fr\nABSTRACT\nFeature generation has been proposed recently to generate\nfeature sets automatically, as opposed to human-designed\nfeature sets. This technique has shown promising results\nin many areas of supervised classiﬁcation, in particular in\nthe audio domain. However, feature generation is usually\nperformed blindly, with genetic algorithms. As a result\nsearch performance is poor, thereby limiting its practical\nuse. We propose a method to increase the search perfor-\nmance of feature generation systems. We focus on ana-\nlytical features , i.e. features determined by their syntax.\nOur method consists in ﬁrst extracting statistical proper-\nties of the feature space called spin patterns , by analogy\nwith statistical physics. We show that spin patterns carry\ninformation about the topology of the feature space. We\nexploit these spin patterns to guide a simulated annealing\nalgorithm speciﬁcally designed for feature generation. We\nevaluate our approach on three audio classiﬁcation prob-\nlems, and show that it increases performance by an order\nof magnitude. More generally this work is a ﬁrst step in\nusing tools from statistical physics for the supervised clas-\nsiﬁcation of complex audio signals.\n1. INTRODUCTION\nThe identiﬁcation of feature sets is a fundamental step in\nsolving supervised classiﬁcation problems [3]. For prob-\nlems involving complex signals (e.g. music, images, etc.)\nthe traditional approach is to use “off-the-shelf” features\n(see, e.g. [9]). However, general-purpose features are not\nalways adapted to solve difﬁcult classiﬁcation tasks. An-\nother solution is to design manually ad hoc features, spe-\nciﬁc to the problem at hand. Such a task can be con-\nducted only by experts knowledgeable both in the domain\n(e.g. music) and in signal processing, a difﬁcult, costly and\ntime-consuming task. Moreover, there is no guarantee that\nhumans will ﬁnd the best possible features.\nFeature generation has recently been introduced to address\nthis problem, by generating automatically problem-dependent\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc⃝2010 International Society for Music Information Retrieval.features, designed to be efﬁcient for any particular super-\nvised classiﬁcation problem [18]. Feature generation con-\nsists in building features by searching in a huge feature\nspace, usually through genetic programming techniques [7].\nThe ﬁtness of a feature is deﬁned as the performance of\na classiﬁer using this feature on the problem at hand [8].\nThese previous works have consistently demonstrated that\nfeature generation outperforms traditional approaches based\non feature selection (see e.g. [5, 14]). However, most, if\nnot all, feature generation systems proposed in the litera-\nture were shown to necessitate the exploration of a large\nnumber of features before ﬁnding relevant ones. For in-\nstance in [17], in the context of classiﬁcation of percus-\nsive sounds, the feature generation system evaluated about\n77500 features to eventually ﬁnd features which outper-\nformed standard ones.\nAlthough search performance is usually not an issue for\noff-line applications, poor search performance forbids the\nuse of these promising techniques in other contexts. As\nan example, the multiplication of portable entertainment\ndevices creates a need for application requiring fast clas-\nsiﬁcation of a priori unknown user data (audio, pictures,\ngestures, etc.). In these contexts, feature generation cannot\nbe used primarily because of performance issues. We pro-\npose a search method that reduces substantially the number\nof features to actually evaluate.\nThe main source of inefﬁciency of feature generation comes\nfrom the blind search strategy inherent to genetic program-\nming. Most of the computation time is lost in evaluating\nirrelevant features, as noted by [5]. This problem worsens\nin the signal domain, where some features can be costly to\nevaluate (for instance features involving the computation\nof complex transforms). In this context, search can take\nseveral days.\nSearch performance can be increased by guiding the search\nusing domain speciﬁc heuristics. To ﬁnd heuristics we\nhave to understand the mathematical structure of the fea-\nture space, to estimate a priori what regions are likely to\ncontain relevant features. However, this is impossible to do\nin general, because we do not have information about the\nsemantics of the features [11]. For this reason we restrict\nour study to the speciﬁc case of analytical features [14].\nAnalytical features (AF) are functional compositions of el-\nementary signal processing operators. An AF is deﬁned by\nits syntactical form, i.e. a tree of basic operators. A central\nquestion of our study, reﬂected in the title of this paper, is\n321\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)therefore ho\nw to exploit this syntax to extract information\nabout a features ﬁtness before actually computing it. As\nwe will see, this relation is complex.\nIn this paper, we ﬁrst show that predicting directly the ﬁt-\nness from the syntax is difﬁcult. We propose to model\nfeatures from a more ﬁne-grained perspective: borrowing\ntechniques from spin glass theory [10] we introduce the no-\ntion of spin patterns to model partial statistical information\nabout basic operators. We show that spin patterns contain\nprobabilistic information about the ﬁtness of features that\nuse a given operator. We also show how these patterns can\nbe used to predict feature ﬁtness. We then propose a fea-\nture generation algorithm guided by these predictions. Our\nalgorithm can be viewed as a variant of the simulated an-\nnealing [4]. The comparison between simulated annealing\nand genetic programming is a well studied topic [2], how-\never it is hard to establish what is the more efﬁcient opti-\nmization method in the general case [19]. In our context\nwe use simulated annealing because it is easier to guide by\nthe information obtained from the spin patterns.\nTwo versions of the algorithm are proposed. The basic ver-\nsion searches for individual features, and the extended ver-\nsion searches for feature sets. Even if it is well known that\nindividual features are not able to solve difﬁcult classiﬁ-\ncation problems [3], we present the basic version because\nit well describes the theoretical aspects of the algorithm.\nWe evaluate our algorithms on three audio classiﬁcation\nproblems. We show that our algorithms ﬁnd features and\nfeature sets which are as good or better than features found\nby a standard feature generation algorithm, but with a sig-\nniﬁcantly improved search performance (an order of mag-\nnitude).\nThis paper is structured as follows: In Section 2, we in-\ntroduce analytical features and syntactic neighborhood. In\nSection 3, we study the prediction of feature ﬁtness from\ntheir syntax. In Section 4, we introduce the notion of spin\npattern for operators. We illustrate these patterns on a sim-\nple audio classiﬁcation problem. In Section 5, we intro-\nduce our search algorithms. In Section 6, we describe\nthe performance of our algorithm on 3 audio classiﬁcation\nproblems, and compare it to a standard feature generation\nalgorithm.\n2. ANALYTICAL FEATURES\nAnalytical Features are expressed as a functional term, tak-\ning as only argument the input signal (represented here as\nx). This functional term is composed of basic operators.\nGiven a library of basic operators L, an analytical feature\nfis an application f=O1◦: : :◦ONsuch that Oi∈ L.\nS(f) ={O1; : : : ; O N}is the syntactical form off. The\nlength l(f)is the number of operators making up the fea-\nture.Fis 0the set of all possible analytical features built\nonL.\nFor instance, the following feature (A)computes the MFCC\n(Mel Frequency Cepstrum Coefﬁcients ) of successive frames\nof length 100 (samples) with no (0 ) overlap, and then com-Problem I(dF it;\ndSyn)\nPAN 0.032\nIN\nS 0.015\nMG 0.015\nTable\n1. Estimation of the mutual information\nI(dF it; dSyn)between the distances dF itanddSyneval-\nuated on three audio classiﬁcation problem.\nputes the variance of this value vector:\nA=V ariance (MFCC (Split (x;100;0))):\nTheneighborhood offis the set Vf={g∈ F|d Syn(f; g)≤1};\nwhere dSyn(f; g)is the Levenshtein distance.\n2.1 Feature Fitness\nGiven a classiﬁcation problem, the ﬁtness \u0015D(f)of an\nAF measures the capacity of the feature to distinguish ele-\nments of different classes.\nThere are several ways to assess the ﬁtness of a feature.\nWe follow here a wrapper approach [6], by which features\nare evaluated using a classiﬁer built on-the-ﬂy during the\nfeature search process [14]. The ﬁtness of the feature is\ndeﬁned as the performance of a classiﬁer built with this\nunique feature.\nWe use Support Vector Machines. To evaluate the perfor-\nmance of the classiﬁer (or more precisely its average F-\nmeasure) we use 10-fold cross validation on the training\ndatabase.\n3. PREDICTING FEATURE FITNESS\nWe deﬁne the distance dF it(f; g)based on the ﬁtness of f\nandg:\ndF it(f; g) =|\u0015D(f)−\u0015D(g)|:\nWe study here experimentally the relationship between dSyn\nanddF iton concrete problems. In this study we consider\nthree audio classiﬁcation problems. The problem PAN\nconsists in discriminating between six percussive sounds\n[15],INS consists in discriminating between sounds played\nby eight different instruments [12] and MG consists in dis-\ncriminating between six musical genres [13].\nWe compute a population Pof1000 features randomly\ngenerated from F. For each problem and for each couple\n(f; g)inP, we compute distances dF it(f; g),dSyn(f; g).\nNote that dF itdepends on D i, whereas dSynis problem-\nindependent.\nWe then estimate the mutual information I(dF it; dSyn)[1]\nbetween the distances. Table 1 shows that in each case the\nmutual information is smaller than 0.1: if a relation exists\nbetween syntax and ﬁtness, it is somehow hidden and dif-\nﬁcult to model.\n4. SPIN PATTERNS\nIn this section we introduce a representation of analytical\nfeatures taking into account the contribution of each sam-\n322\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)ple. Let Dbe\na labeled data set, composed of Naudio\nsamples divided in kclasses, C1; : : : ; C k:\nD={(x1; l1); : : : ; (xN; lN)};\nwhere xiis the i-th audio sample and li∈ C={Cj}j=1;:::;k\nis its label.\nGiven a feature f∈ F, we deﬁne its spin conﬁguration as:\nf→\u001bf={\n+1iffclassiﬁes xicorrectly\n−1otherwise\nIfC:R→ C ={Cj}j=1;:::;kis a classiﬁer trained on D\nwithfas a single feature, we have:\n\u001bf\ni={\n+1ifC(f(xi)) =li\n−1otherwise\nGiven f∈ F,\u0015(f)is the ﬁtness of fonD,0≤\u0015(f)≤1.\nBy analogy with the spin glass model [10], we can interpret\nﬁtness as the Hamiltonian H(\u001bf)of a spin conﬁguration\n\u001bfinduced by the feature f:\nH(\u001bf) =−\u0015(f):\nGiven a basic operator o∈ L, where Lis the operator\nlibrary used to construct F, we deﬁne\nFo={f∈ F|o∈ S(f); l(f)≥10}:\nSoFois the set of all features that contain owith length\nsmaller than 10. Considering only these features we have\n0<|Fo|<∞.\nThespin pattern of a basic operator ois a vector m(o) =\n{mi(o)}i=1;:::;Nsuch that:\nmi(o) =⟨\n\u001bf\ni⟩\nFo=1\n|Fo|∑\nf2Fo\u001bf\ni\nIn practice mi(\no)is an indicator of the probability that a\nfeature containing ocorrectly classiﬁes sample xi. Indeed,\nit is easy to show that, given f∈ F o,\nPr(\u001bf\ni= +1) =1 +mi(o)\n2\nPr(\n\u001bf\ni=−1) =1−mi(o)\n2\nTherefore we\nare interested in operators whose pattern has\nas many values as possible which are close to 1. Con-\nversely, zero spin patterns conﬁguration ( mi(o) = 0 ∀j)\ndo not provide any information about the set of features\nconsidered. In order to measure the amount of information\ngiven by a spin pattern, we can compute its total magneti-\nzation, deﬁned as follows:\nM(o) =N∑\ni=1mi(o)\nN\nFigure 1\nshows a graphical representation of the spin pat-\nterns of two operators. The samples are arranged in a pic-\nture composed by Nsquares. |mi(o)|is mapped to thedarkness of the corresponding square. In this representa-\ntion, high magnetization features correspond to dark im-\nages. In the ﬁgure we can observe that the spin pattern\nof a speciﬁc operator like LpFilter has more magnetiza-\ntion than the spin pattern of a more general operator like\nAbs (theAbs pattern is clearly lighter). The main intu-\nition in guiding search toward an optimal path in the fea-\nture space is that there is a rich, non uniform distribution of\nspin patterns for all basic operators. To support this claim\nwe compute the spin patterns of each operator on the three\nclassiﬁcation problems presented above. In general, op-\nerators yield a spin pattern whose magnetization is signif-\nicantly higher than 0 (the magnetization of the zero spin\npattern, taken as a reference): as we see in ﬁgure 2, for\neach problem, the distributions of the magnetizations for\nallo∈ Lare concentrated near 0:5. This study shows that\ninteresting patterns do exist. Of course we do not know\nthe spin patterns a priori for a given classiﬁcation prob-\nlem. However, we have seen experimentally that we can\nestimate these patterns using a relatively small number of\ncomputations (≈ 1000).\n5. THE ALGORITHM\nOur algorithm takes as input a labeled database Dand a\nlibrary of basic operators L. The algorithm searches the\nfeature space Fdeﬁned by L, guided by spin patterns, as\ndeﬁned in section 4.\n5.1 Individual feature search (IFS)\nWe describe here the IFS algorithm, that searches for indi-\nvidual features. Section 5.2 describes an extension to fea-\nture set search. IFS receives as input a labeled database D\nand a library of basic operators L. The output is a feature\nwith a high ﬁtness on domain D.\nThe algorithm is a variant of the simulated annealing al-\ngorithm. It is based on a Metropolis procedure [4] that\nguarantees the convergence to a global optimum.\nStarting from a random feature f0, the algorithm iteratively\nselects neighbors of the current feature, using the spin pat-\nterns. At each iteration, the algorithm selects a new feature\nin the syntactical neighborhood of the current feature. This\nchoice is done according to the estimation of the spin pat-\nterns. The algorithm terminates after a speciﬁed number\nof iterations, or as a result of an interactive user request,\nand returns the best feature (i.e. the feature with highest\nﬁtness) found during the search as output.\nIn the following subsections we detail the components of\nthe algorithm.\n5.1.1 The Spin Pattern Estimator\nThespin pattern estimator (SPE) is executed only once at\nthe beginning of the algorithm.\nThe SPE computes a population Tof1000 random fea-\ntures. These features are used to estimate the spin patterns\nof each operator o∈ L.\n323\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)(a) LpFilter\n (b) Abs\nFigur\ne 1. Graphical representation of the spin patterns of the basic operators Lpfilter (left) and Abs(right) evaluated on\nthe problem PAN (classiﬁcation of percussive sounds). The representation of a domain speciﬁc operator like LpFilter is\nclearly darker. In the spin pattern of LpFilter note the two dark stripes on the center and on the right. Magnetization are\n0:46 and0:28 (resp.)\n00.050.10.150.20.250.30.35\n0\n0.050.1\n0.150.2\n0.250.3\n0.350.4\n0.450.5\n0.550.6\n0.650.7\n0.750.8\n0.850.9\n0.951\nAbsolute MagnetizationFrequency\n(a)IN\nS\n00.050.10.150.20.250.30.35\n0\n0.050.1\n0.150.2\n0.250.3\n0.350.4\n0.450.5\n0.550.6\n0.650.7\n0.750.8\n0.850.9\n0.951\nAbsolute MagnetizationFrequency (b)MG\n00.050.10.150.20.250.30.35\n0\n0.050.1\n0.150.2\n0.250.3\n0.350.4\n0.450.5\n0.550.6\n0.650.7\n0.750.8\n0.850.9\n0.951\nAbsolute MagnetizationFrequency (c)PAN\nFigur\ne 2. Distributions of the absolute magnetizations of the spin patterns of all the operators o∈ L\n5.1.2 The Neighbor Selector\nThe task of the neighbor selector is to decide how to move\nin the feature space. The selector receives as input the cur-\nrent feature and the spin pattern estimation of each operator\no∈ L. The output of the selector is either a new feature,\nor the feature passed in input.\nGiven the input feature fand the estimations of the spin\npatterns, the selector assigns a score to each basic operator\no∈ L. This score is designed to favor operators that tend\nto correctly classify the samples that are wrongly classiﬁed\nbyf. The score \u0016(o)of operator ois deﬁned as:\n\u0016(o) =N∑\ni=1b(\u001bf\ni) ¯mi(o)\nN(1)\nwhere b(x) = −1\n2x+1\n2is a\nfunction that converts a ±1\nspin in a {0;1}boolean value (b (+1) = 0 ,b(−1) = 1).\n\u0016(o) induces a score function \u0016(g)for a feature g∈ F by\naveraging on all operators of f:\n\u0016(g) =∑\no2S(g)\u0016(o)\nl(g)(2)\nA crucial\npoint of our method is that in general the score\nfunction \u0016(g)is easier to compute than the ﬁtness \u0015(g).\nThe selector then computes the neighborhood Vfoff(see\nsection 3). Vfis then sorted according to the score func-\ntion deﬁned in (2). The feature ˜f∈Vfwith highest score\nis chosen by the neighbor selector. The spin conﬁguration\n\u001b~fand ﬁtness \u0015(˜f)of˜fare then computed.\nTo avoid local maximum effects, ˜fis accepted as the next\nfeature in a stochastic way, using the Metropolis proce-\ndure.More precisely, if ∆\u0015=\u0015(˜f)−\u0015(f)≥0fis accepted.\nThe case ∆\u0015 <0is accepted with a probability Pr(∆\u0015) =\ne∆\u0015\ntk, where\ntn=(t1\nt0)n\nt0\nandk=|T|.\nIf the\nfeature ˜fis not accepted, it is removed from Vf, oth-\nerwise it will be reselected by the neighbor selector in the\nnext iteration. Using the Metropolis procedure, we can as-\nsume that, for a good choice of parameters T0andT1, the\nalgorithm converges to the global maximum [4]. Follow-\ning [4], we heuristically set T1=T0= 0:95 andT0= 10 .\n5.2 The Feature Set Version\nAs described earlier, a single feature is usually not enough\nto solve a classiﬁcation problem and a feature set is re-\nquired. In principle, building feature sets instead of in-\ndividual features complexiﬁes drastically the procedure,\nsince all combinations of features must be considered at\neach step. For this reason we propose the follow simple ex-\ntension of IFS that searches feature sets, in order to main-\ntain an affordable computational cost.\nThe architecture of the algorithm is essentially the same\nof the basic version. The spin pattern estimator works ex-\nactly in the same way. The only module that changes is the\nneighbor selector. In this version it takes as input a feature\nset of dimension d(dis a ﬁxed parameter) and outputs a\nfeature set instead of a single feature.\nBecause the spin conﬁguration of a feature fis deﬁned by\nthe classiﬁer built with the values of f, it is possible to de-\nﬁne in the same way the spin conﬁguration \u001bFand the ﬁt-\nness\u0015(F)of a feature set F, using the classiﬁer produced\n324\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)by the\nvalues of F. Therefore we can deﬁne the score of\nan operator \u0016(o)and the score of a feature \u0016(g)as in the\nprevious subsection.\nIn this version of the algorithm, a feature fis randomly\nchosen ∈F. A feature ˜fin the neighborhood Vfis se-\nlected like in the basic version of the algorithm.\nThe new feature set ˜Fis then built from Fby substitut-\ningfby˜f. The new feature set is accepted with the same\nMetropolis procedure described above. Our algorithm is\nthen able to search for feature sets of any dimension.\nIn the next section we compare the search performance\nof our two algorithms against the search performance of\na standard genetic algorithm.\n6. RESULTS\n6.1 Individual Feature Search\nTo assess the search performance of IFS we compare it\nagainst the search performance of a standard genetic algo-\nrithm on three audio classiﬁcation problems. These prob-\nlems are the same presented in the section 3. The genetic\nalgorithm we use is described in [14].\nGiven a classiﬁcation problem D, we use our algorithm\nand the genetic algorithm to search the feature fwith high-\nest ﬁtness \u0015D(f)and we compare the sets of features ex-\nplored by the two algorithms. We are interested in three\nquantities: the ﬁtness of the best feature found, how many\nfeatures are computed before ﬁnding the best feature and\nthe distribution of the ﬁtness of the explored features.\nTo get statistically signiﬁcant results, we execute both al-\ngorithms three times for each classiﬁcation problem. The\nresults are shown in ﬁgure 3 and ﬁgure 4. In ﬁgure 3\nwe can observe that IFS ﬁnds features at least as good\nas the features found by the genetic algorithm but con-\nverges faster to the solution. In average, it computes less\nthan 3500 features to ﬁnd the best features. This ﬁgure in-\ncludes the number of features needed to estimate the spin\npatterns (approximately 1000, as described in section 4).\nConversely the genetic algorithm requires more than 48000\nfeatures to ﬁnd the optimal.\nAs we can see in ﬁgure 4 the distribution of the ﬁtness,\nwhen using IFS, is concentrated near high values of the\nﬁtness. Conversely the genetic algorithm explores blindly\nthe feature space, resulting in a more uniform distribution\nof the ﬁtness.\n6.2 Feature Set Search\nThe feature set version of our algorithm builds feature sets\nof dimension N. Again we test this algorithm against the\ngenetic algorithm used above to ﬁnd feature sets of dimen-\nsion3for the three reference problems.\nThe genetic algorithm, after a population has been created\nand each feature has been individually evaluated, selects\na subset of features to be retained for the next population.\nThe output of the genetic algorithm is a feature set FGAof\ndimension 3: FGAis the set {f1; f2; f3|f1; f2; f3∈Flast}\nwith maximum ﬁtness. Flastis the last population.\nHere we compare the two feature sets obtained by the twoDatabase GP FS IFS\nPAN 0.76 0.78 0.79\nIN\nS 0.56 0.56 0.59\nMG 0.77 0.79 0.77\nTable\n2. Comparison between the ﬁtness of the best fea-\nture sets obtained by IFS against the best feature sets ob-\ntained by the genetic algorithm. In two cases IFS outper-\nforms the genetic one. GP means genetic programming,\nFS means feature selection.\nDatabase GP and\nFS IFS\nPAN 77531 4043\nIN\nS 72837 4152\nMG 43265 7221\nTable\n3. Number of features needed to ﬁnd the best feature\nsets.\nalgorithms against an other feature set obtained by apply-\ning a feature selection algorithm (in our case, InfoGain\n[16]) to the whole set of features explored by the genetic\nalgorithm. In table 2 we observe that our algorithm per-\nforms as well as the genetic one and the feature selection\none on the three problems. However, the features sets ne-\ncessitate a smaller exploration: table 3 shows the number\nof features explored in order to ﬁnd the best feature set. It\ncan be seen again that our algorithm improves the search\nperformance by an order of magnitude.\n7. CONCLUSION\nWe have explored the relation between the syntax and the\nﬁtness in a supervised classiﬁcation context. Such a re-\nlation seems complex to grasp at a macroscopic level. We\nhave proposed a sample-based approach to model the topol-\nogy of feature spaces, and exhibited a computable crite-\nrion, spin patterns, to guide a feature search algorithm.\nThis algorithm is based on simulated annealing, with a\nMetropolis procedure, and exploits spin patterns, resulting\nin a better performance than genetic programming, as mea-\nsured by the total number of features actually evaluated.\nAs such, this approach is a promising one to reduce the tra-\nditionally high computational cost of feature generation,\nand increase the applications of this technique. The an-\nswer to our title question is therefore: “Yes, there is a\ncomplex relationship”. Furthermore, we showed how this\nrelationship can be exploited to improve the performance\nof a feature generation system. More generally, this work\nrepresents a ﬁrst step in applying tools from statistical me-\nchanics of complex systems to supervised classiﬁcation of\ncomplex audio signals.\n8. REFERENCES\n[1] T.M. Cover, J.A. Thomas, Elements of information the-\nory.Wiley, New York, 1991.\n[2] L. Davis, Genetic algorithms and Simulated Anneal-\n325\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)(a)IN\nS\n (b)MG\n (c)PAN\nFigur\ne 3. Performance of IFS compared to the performance of a standard genetic algorithm (GP). We report here only the\nbest execution for each algorithm.\n00.050.10.150.20.250.30.350.40.450.5\n0\n0.050.10.150.20.250.30.350.40.450.50.550.60.650.70.750.80.850.90.951\nFitnessFrequencyIFS3\nIFS2\nIFS1\nGP1\nGP2\nGP3\n(a)IN\nS\n00.10.20.30.40.50.6\n0\n0.050.10.150.20.250.30.350.40.450.50.550.60.650.70.750.80.850.90.951\nFitnessFrequencyGP3\nGP2\nGP1\nIFS1\nIFS2\nIFS3 (b)MG\n00.10.20.30.40.50.6\n0\n0.050.10.150.20.250.30.350.40.450.50.550.60.650.70.750.80.850.90.951\nFitnessFrequencyGP1\nGP2\nGP3\nIFS3\nIFS4\nIFS5 (c)PAN\nFigur\ne 4. Distributions of the ﬁtness of the features computed by the two algorithms. In dotted line, the distributions of the\nfeatures computed by the genetic algorithm, in black the features computed by IFS. For IFS, the distributions of the ﬁtness\nare concentrated near high values, whereas the features explored by the genetic algorithm have more spread distributions.\ning.Morgan Kaufman Publishers, Inc.,Los Altos, CA.\n1987.\n[3] I. Guyon and A. Elisseeff, “An introduction to variable\nand feature selection”, Journal of Machine Learning\nResearch, 3:11571182, 2003.\n[4] S. Kirkpatrick, C. D. Gelatt, M. P. Vecchi, “Optimiza-\ntion by Simulated Annealing”, Science, New Series,\nVol. 220, No. 4598, pp. 671-680, May 13, 1983.\n[5] Y. Kobayashi, “Automatic Generation of Musical In-\nstrument Detector by Using Evolutionary Learning\nMethod”, in Proc. of the 10th International Conference\non Music Information Retrieval (ISMIR ’09) , Kobe,\nJapan, October 2009.\n[6] R. Kohavi, G.H. John, “The Wrapper Approach”, in\nFeature Selection for Knowledge Discovery and Data\nMining, H. Liu & H. Motoda (eds.), Kluwer Academic\nPublishers, pp 33-50, 1998.\n[7] J. R. Koza, Genetic Programming: On the Program-\nming of Computers by Means of Natural Selection.\nCambridge, MA: The MIT Press. 1992.\n[8] S. Markovitch and D. Rosenstein, “Feature Genera-\ntion Using General Constructor Functions”, Machine\nLearning, Vol. 49, pp. 59-98, 2002.\n[9] M.F. McKinney, J. and Breebart, “Features for au-\ndio and music classiﬁcation”, in Proc. of the Interna-\ntional Conference on Music Information Retrieval (IS-\nMIR 2003), pp. 151-158, 2003\n[10] M. Mezard, G. Parisi, M.A. Virasoro, Spin Glasses\nTheory and Beyond , World Scientiﬁc, Singapore, 1987.[11] I. Mierswa, and K. Morik, “Automatic feature extrac-\ntion for classifying audio data”, Machine Learning\nJournal , Vol. 58, pp. 127-149, 2005.\n[12] University of Iowa Musical Instrument Samples:\nhttp://theremin.music.uiowa.edu/MIS.html\n[13] http://ismir2004.ismir.net/genre contest/index.htm\n[14]\nF. Pachet, P. Roy, “Analytical Features: A Knowledge-\nBased Approach to Audio Feature Generation”,\nEurasip Journal on Audio, Speech, and Music Process-\ning, 2009(1), February 2009.\n[15] http://www.csl.sony.fr/pandeiro/\n[16] J. R. Quinlan, C4.5: Programs for Machine Learning,\nMorgan Kaufmann, San Francisco, California, USA,\n1993.\n[17] P. Roy, F. Pachet, S. Krakowski, “Analytical Features\nfor the classiﬁcation of percussive sound: the case of\nPandeiro”, in Proc. of the 8th International Conference\non Music Information Retrieval (ISMIR ’07) , pp. 229-\n232, Vienna, Austria, September 2007.\n[18] E. K. Tang ; P. N. Suganthan; X. Yao, “Nonlinear fea-\nture extraction using evolutionary algorithm” in Lec-\nture notes in computer science, Vol. 3316, pp. 1014-\n1019, 2004.\n[19] A. Vasan, K.S. Raju, “Comparative analysis of Sim-\nulated Annealing, Simulated Quenching and Genetic\nAlgorithms for optimal reservoir operation,” in Applied\nSoft Computing, Vol. 9, No. 1, pp. 274-281, January\n2009.\n326\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "SongWords: Exploring Music Collections Through Lyrics.",
        "author": [
            "Dominikus Baur",
            "Bartholomäus Steinmayr",
            "Andreas Butz"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1416682",
        "url": "https://doi.org/10.5281/zenodo.1416682",
        "ee": "https://zenodo.org/records/1416682/files/BaurSB10.pdf",
        "abstract": "The lyrics of a song are an interesting, yet underused type of symbolic music data. We present SongWords, an ap- plication for tabletop computers that allows browsing and exploring a music collection based on its lyrics. Song- Words can present the collection in a self-organizing map or sorted along different dimensions. Songs can be ordered by lyrics, user-generated tags or alphabetically by name, which allows exploring simple correlations, e.g., between genres (such as gospel) and words (such as lord). In this paper, we discuss the design rationale and implementation of SongWords as well as a user study with personal music collections. We found that lyrics indeed enable a different access to music collections and identified some challenges for future lyrics-based interfaces.",
        "zenodo_id": 1416682,
        "dblp_key": "conf/ismir/BaurSB10",
        "keywords": [
            "lyrics",
            "symbolic music data",
            "SongWords",
            "tabletop computers",
            "self-organizing map",
            "user-generated tags",
            "genres",
            "words",
            "correlations",
            "music collections"
        ],
        "content": "SONGWORDS: EXPLORING MUSIC COLLECTIONS THROUGH LYRICS\nDominikus Baur, Bartholom ¨aus Steinmayr, Andreas Butz\nMedia Informatics Group\nUniversity of Munich (LMU), Munich, Germany\nfdominikus.baur,andreas.butzg@ifi.lmu.de, steinmayr@cip.ifi.lmu.de\nABSTRACT\nThe lyrics of a song are an interesting, yet underused type\nof symbolic music data. We present SongWords, an ap-\nplication for tabletop computers that allows browsing and\nexploring a music collection based on its lyrics. Song-\nWords can present the collection in a self-organizing map\nor sorted along different dimensions. Songs can be ordered\nby lyrics, user-generated tags or alphabetically by name,\nwhich allows exploring simple correlations, e.g., between\ngenres (such as gospel ) and words (such as lord). In this\npaper, we discuss the design rationale and implementation\nof SongWords as well as a user study with personal music\ncollections. We found that lyrics indeed enable a different\naccess to music collections and identiﬁed some challenges\nfor future lyrics-based interfaces.\n1. INTRODUCTION\nLyrics are an important aspect of contemporary popular\nmusic. They are often the most representative part of a\nsong. They verbally encode the songs general message,\nthereby strongly contributing to its mood. For most people,\nsinging along is one of the easiest ways to actively partici-\npate in the music experience. Lyrics are also regularly used\nfor identifying a song, since the ﬁrst or most distinct line\nof the chorus often also is the song’s title. This importance\nof lyrics makes purely instrumental pieces rather rare in\ncontemporary popular music.\nDespite this central role of lyrics, computer interfaces\nmostly still ignore them. Media player software for per-\nsonal computers mostly only shows lyrics after installing\nadditional plug-ins, and although the ID3 metadata stan-\ndard for digital music contains a ﬁeld for lyrics, it is rarely\nused. More complex operations, such as browsing and\nsearching based on lyrics, are even further away and scarce-\nly touched in research (e.g., [6]). We therefore think that\nlooking at music from the perspective of lyrics can allow\nusers a fresh view on their collection, reveal unknown con-\nnections between otherwise different songs and allow them\nto discover new patterns between the lyrics and other as-\npects of the music.\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2010 International Society for Music Information Retrieval.\nFigure 1. Browsing a music collection through its lyrics\non a tabletop\nIn this paper, we give an overview of SongWords (see\nﬁgure 1 and video1), an application for tabletop computers\nwhich supports navigating music collections and investi-\ngating correlations based on the lyrics of songs. We present\nrelated research on browsing and tabletop interfaces, de-\nscribe and explain our interface and interaction design de-\ncisions, talk about the implementation of SongWords and\npresent the results of a user study.\n2. RELATED WORK\nContent-based MIR often uses not only the instrumental\nbut also the vocal parts of a song. However, since ex-\ntracting the words of a song directly from the audio sig-\nnal has proven to be difﬁcult, a common approach is to\ngather lyrics from the internet based on available metadata\n(e.g., [14]). These lyrics then enable tasks that go beyond\npure retrieval, such as semantic or morphologic analysis\n(topic detection [13], rhymes in hip hop lyrics [9], genre\nclassiﬁcation from rhyme and style features [16]). Other\nwork is approaching the problem of mapping textual lyrics\nto an audio signal ( [12], [7]). Combining an ontology\nwith lyrics enables even more sophisticated tasks: Bau-\nmann et al. used natural language processing and mapped\ntext to a vector space model to calculate a lyrical simi-\nlarity value for pairs of songs [1]. Fujihara et al. pre-\nsented an approach for creating bi-directional hyperlinks\nbetween words in songs that could be applied not only to\ntextual lyrics but also to the actual audio data [6]. They\n1http://www.youtube.com/watch?v=FuNPhN6zyRw\n531\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Figure 2. Songs are organized on a map based on lyrics or tags (left), or sorted alphabetically by their artist’s name (right)\nalso describe an application called LyricSynchronizer [8]\nthat allows browsing collections by navigating through the\naligned song lyrics. There is, however, no work on visual-\nizing a complete music collection based on lyrics.\nIn order to make complex music collections accessible,\na multitude of browsing interfaces are available. Beyond\nthe sorted lists commonly used in media player software,\nabstraction and ﬁltering capabilities are useful, e.g., by ap-\nplying techniques from information visualization [23] or\nby providing views based on different facets [2]. Since mu-\nsic content provides a very high-dimensional data set, com-\nplexity also has to be reduced for visualization. Pampalk’s\nIslands of Music [17] is the best known example for this\napproach. It has also been extended to incorporate multiple\nviews on different acoustic aspects [18]. Self-organizing\nmaps have also widely been used for visualizing text doc-\numents (e.g., [5]). In a similar vein, several projects al-\nlow browsing a music collection on tabletop displays using\nself-organizing maps of different low- and high-level au-\ndio features (SongExplorer [11], MarGrid [10], DJen [3],\nMusicTable [22]). Lyrics, however, haven’t been used for\nbrowsing so far.\n3. INTERFACE DESIGN\nWhen designing SongWords we started from two user tasks:\nFirst, users should be able to easily browse and search\nthrough their personal collections based on lyrics. Song-\nWords should give them a new perspective on their own\nsongs and let them browse through the collection from word\nto word (similar to [7]). Second, we wanted to allow users\nto corroborate or disprove hypotheses about connections\nbetween lyrics and genres. It should be easy to discover\ncorrelations between different genres and words, such as\n”Hip hop lyrics often use cuss words” or”Pop songs often\nrevolve around ’love’ and ’baby’”.\nSince such patterns are hard to discover by scrolling\nthrough a text-based list, we decided to map the high-di-\nmensional information space to a two-dimensional canvas\nusing Self-Organizing Maps [15]. Furthermore, as the re-\nsulting map at a reasonable level of detail largely exceeded\nthe screen size, we also implemented a Zoomable User In-\nterface to navigate the large virtual canvas on a physicaldisplay. With a potentially very large number of items, we\nﬁnally chose to use an interactive tabletop display for its\nadvantages regarding screen space [24] and its potential for\nmulti-user interaction. In addition, zooming and panning\nwas found to work better using direct touch and bi-manual\ninteraction than using mouse input [4].\n3.1 Visualization and Interaction\nSongWords analyzes a given music collection and displays\nit on a two-dimensional canvas. The visualization consists\nof two self-organizing maps for lyrics and for tags, as well\nas an alphabetical list by artist’s names for direct access to\nsongs (see ﬁgure 2). In addition, there is a view for the\nresults of text searches (see below). The user can switch\nbetween these different views by pressing one of a number\nof buttons at the border of the screen.\nAll songs of the collection are represented on the virtual\ncanvas by their cover art. To optimize the use of screen\nspace, each item is displayed as large as possible with-\nout overlapping with other songs. The underlying self-\norganizing map guarantees spatial proximity between sim-\nilar items regarding the currently chosen aspect (lyrics or\ntags). The map contains black areas in the background that\nconnect clusters of items and displays the most relevant\nwords or tags next to the song items to give overview and\nallow orientation. A common interaction that is possible\nin each context is pan and zoom (see ﬁgure 3). Panning\nis triggered by putting the ﬁnger to the canvas outside of a\nsong icon and dragging, with the canvas sticking to the ﬁn-\nger. Zooming and rotation are controlled by two or more\nﬁngers and the system calculates the geometric transfor-\nmation of the canvas from their movements.\nIn addition to this geometric zoom for the virtual can-\nvas, SongWords also implements a semantic zoom for song\nicons (see ﬁgure 4): At the least detailed zoom level, songs\nare represented as colored squares to reduce screen clutter\nwith thousands of items. The item’s colors represent the\nhome collection of the song when several collections are\navailable. When zooming in, the solid colors are replaced\nby the artwork of the corresponding record. By zooming\nfurther in (or tapping once on the song icon) the artist, ti-\ntle and lyrics of the song become available. Here, the user\n532\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Figure 3. Uni- and bi-manual gestures for panning, rotation and zoom\ncan scroll through the text if the screen space does not suf-\nﬁce, mark sections of it and search for these sections in\nthe collection. Despite SongWords’ focus on text, we de-\ncided against using an on-screen keyboard for text search.\nNot only would it have taken up screen space (or required\nan explicit mode switch) and suffered from typical prob-\nlems of virtual keyboards such as missing tactile feedback,\nit would also have allowed erroneous search terms. Search\nresults in turn are displayed on a spiral based on their rele-\nvance (see below). If multiple song collections are visible\n(e.g., from different users), each song icon has a colored\nborder that represents its home collection.\nFigure 4. Different semantic zoom levels for song icons\nTouching a song with two ﬁngers displays the artist, ti-\ntle and tags for this song. Touching a section of the map\nwith one ﬁnger displays the relevant tags or words from\nthe lyrics. Songs can be played by holding a ﬁnger on their\nicon for a short time. In order to allow the discovery of\nnew music based on lyrics, songs with similar lyrics are re-\ntrieved from the internet and displayed alongside the songs\nfrom the collection. They are rendered slightly transparent\nin order to distinguish them from local songs. If the user\nwants to listen to them, a thirty-second sample is down-\nloaded and played.\nOne challenge in designing for tabletop displays is the\nso-called orientation problem. While PC screens have a\nﬁxed ’up’ direction, users can interact with a tabletop com-\nputer from any side. The straightforward two-ﬁnger rota-\ntion of SongWords prevents problems of readability (for\na single user) and lets the user quickly change position.\nWhen the canvas’ orientation changes, the view buttons at\nthe bottom of the screen move along to always remain at\nthe bottom and thus well reachable.\nFigure 5. Search results are arranged on a spiral based on\ntheir relevance\n3.2 User tasks\nSongWords enables several distinct user tasks from sim-\nple browsing to gaining novel insight and testing hypothe-\nses. By their working principle, the self-organizing maps\nvisualize the similarity between different songs regarding\neither lyrics or tags. While user-generated keywords can\nbe expected to be relatively consistent for one artist, the\nlyrics map can bear greater surprises: When songs by one\nartist are spread widely across the map, this means that this\nartist produces very diverse lyrics (or employs different\nsongwriters). Similarly, the (in)consistency between songs\nfrom different collections can also be seen from their col-\nored borders: If each color is dominant in a different corner\nof the map, the overlap between the lyrics of the collections\nis not very high. Discovery of new music based on lyrics is\nsupported in SongWords, as the lyrics and preview clips of\nrelated but unknown songs are automatically downloaded\nand added to ﬁll the map.\nThe user can navigate from song to song using the text\nsearch. By selecting a portion of the lyrics and double-\ntapping, the system switches to the search view, in which\n533\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)all songs containing this sequence of words are arranged\nby relevance. To use the two-dimensional space most efﬁ-\nciently, the linear result list is furled into a spiral (see ﬁgure\n5). Thereby, the user can quickly ﬁnd all songs that contain\na favorite text section.\nTo keep track of one or more songs across different\nviews, they can be selected. Songs are selected by press-\ning the select-button and drawing one or more arbitrary\npolygons around them. This causes all contained songs to\nbe highlighted with an overlay color, and when switching\nfrom view to view their movement can thus be followed\neffortlessly.\nSongWords’ different views also allow the user to ver-\nify hypotheses. To conﬁrm the hypothesis Hip hop lyrics\noften use cuss words a possible workﬂow is to switch to\nthe lyrics view, select songs that gather around cuss words,\nthen switch to the tag-view (where the genre of a song is\nusually the most prominent tag) and see how many songs\nappear near to the hip hop area. Similarly, other hypothe-\nses regarding the lyrics of one artist (selected from the al-\nphabetical view) or songs containing a certain word can be\nexamined.\n4. IMPLEMENTATION\nThe SongWords prototype has been implemented in C++\nfor Windows XP. We use the OpenGL framework for ren-\ndering and the BASS library for audio playback. Song-\nWords was deployed on a custom-built FTIR tabletop dis-\nplay (99 x 74 cm, 1024 x 768 resolution) and multi-touch\ninput is handled by the Touchlib library. We rely on the in-\nternet for all information besides the actual artist, title and\nalbum information of a song: After extracting this informa-\ntion from the MP3’s ID3 tag, we use the search capabilities\nof various internet lyrics databases (e.g., lyrics.wikia.com)\nand parse the resulting HTML pages (similar to, for exam-\nple, [14]) to retrieve the lyrics. Album covers and user-\ngenerated tags are accessed through the API of Last.FM.\nIn order to ﬁnd the most representative words for a song,\nwe ﬁlter the text for stop words and afterwards perform\na term frequency inverse document frequency (TF/IDF)\nanalysis [21] to ﬁnd discriminative terms. The resulting\nword lists are stemmed with Porter’s stemming algorithm\n[20], to merge related words. These stemmed words are\nalso shown on the map, and in some cases look misspelled.\nThe list of all discriminative stemmed terms forms the fea-\nture vector which is used for computing the self-organizing\nmap [15].\nFor creating the self-organizing map of tags, each song\nitem again receives a feature vector consisting of the tags\nand their popularity. Our self-organizing maps are based\non the classical machine learning approach by Kohonen\n[15] with one tweak: Since we wanted to make sure that\nitems do not appear at completely different positions in the\nlyrics and tag views of SongWords, we don’t initialize the\nlearning phase of the tag map with random values, but with\nthe results from the lyrics map. Therefore, the chances that\nidentical items appear at similar positions on the two maps\nare much higher without disturbing the dimensionality re-duction capabilities. We also use a relatively low number\nof 400 iterations for training in order to generate the visu-\nalization sufﬁciently fast.\nTo allow discovery and ﬁll the map with relevant re-\nlated music, for every ﬁve songs in the collection a ran-\ndom artist is picked, and a related artist is acquired from\nLast.FM’s API beforehand (related artists are calculated\nbased on collaborative ﬁltering of their massive user base).\nFor this artist, a search on Amazon.com is performed and\nfor each resulting song an audio sample is downloaded.\nSongWords then tries to ﬁnd lyrics on the aforementioned\nonline databases and once it succeeds, the complete song\nis added to the training set of the map.\n5. USER STUDY\nAfter implementing SongWords, we evaluated it in order\nto verify whether it properly supported the tasks for which\nit was designed. As evaluating a complete visualization\nsystem is difﬁcult and an active ﬁeld of research [19], we\ndecided to rely on qualitative user feedback and a small\nnumber of participants.\n5.1 Study Design\nThe main objectives of the study were to check usability\nof the application and identify possible design ﬂaws that\ncould prevent the user from fulﬁlling the two main tasks.\nIn addition, we wanted to test the application under real-\nistic conditions and therefore asked participants to select a\nsample of roughly thousand songs from their personal col-\nlections. For this set of songs we gathered covers and lyrics\nbefore the study and presented the participants with them.\nAs a third aspect of the evaluation we wanted to verify the\nchoice of using a tabletop display compared to a desktop\nPC. Therefore, we ported SongWords to a PC and mapped\nthe input to the mouse: A left mouse-click was used for\npanning and selection, a right click for displaying contex-\ntual information and the scroll wheel for zooming.\n5.2 Study Tasks\nThe participants were asked to fulﬁll tasks of increasing\ncomplexity to ﬁnd potential shortcomings of the user in-\nterface. Basic tasks were ”Play three different songs” or\n”Choose a song and ﬁnd out the last line of the lyrics”\nwhich could be easily completed using basic interaction\ntechniques. The more complex compound tasks required\nparticipants to combine multiple aspects of the application:\n”Find your favorite song, pick a word from the text that you\nregard as special and ﬁnd other songs that contain it” and\n”Find words that are typical for a certain genre” were two\nof them. For each task, we let our participants play around\nwith SongWords without previous instructions in order to\ngather spontaneous feedback and see how self-explanatory\nthe interaction techniques were. If users weren’t able to\nﬁnish the task on their own the experimenter explained\nhow to do it after a few minutes. Initially, all participants\nworked on and explored the desktop version of SongWords.\nAfter they had fulﬁlled all tasks, they moved on to the\n534\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Tabletop version and completed the same tasks again to\nﬁnd the differences between the two setups. We were only\ninterested in the differences they perceived between the\ntwo conditions interaction-wise and not quantitative data\nlike the required time, so using the same tasks and a pre-\ndeﬁned order of desktop and tabletop led to no problems.\nFinally, we also wanted to examine the inﬂuence of po-\ntential multi-user interaction on SongWords: Therefore,\nwe matched our participants to pairs after each of them\nhad worked on their own, displayed their two collections at\nthe same time with color coding and presented them with\nmulti-user tasks. Exemplary tasks were ”Find genres that\nappear in both collections” and ”Find the song from the\nother’s collection that you like best”. We thereby wanted\nto identify potential problems in coordinating the access to\nthe interface and in collaboration between the pairs. In the\nend, the participants were asked to ﬁll out a questionnaire\nthat collected demographic information and their opinions\nfor the desktop and tabletop version of SongWords.\n5.3 Participants\nWe recruited six participants from the undergraduate and\ngraduate students of the University of [Removed for anony-\nmous submission] (age: 24-30 years, one female). We sup-\nplied them with a small Java application beforehand that al-\nlowed them to conveniently choose a thousand songs from\ntheir collections, retrieved required meta-data and sent the\nresults to us via e-mail. Only one of the participants was\nrecruited on short notice and was not able to provide us\nwith his song set, so we used a different participant’s col-\nlection and adapted the tasks accordingly.\n5.4 Results\nUsing both the tabletop version and the desktop version of\nSongWords showed that the concepts work similarly well\non both platforms. Also, the participants mostly were able\nto transfer their knowledge from one to the other. One no-\ntable exception was ”hovering” by using two ﬁngers. None\nof the users ﬁgured this out by themselves. We also ob-\nserved an artifact from desktop computing: Participants\nkept trying to double click for starting song playback. The\nchange from a map view to the results view after searching\noften went by unnoticed as the song’s text ﬁlled the screen\nand occluded the switch. Additionally, none of the partic-\nipants actually discovered new music, as the slight trans-\nparency of the suggested items obviously wasn’t enough\nto make them stand out. Most of these ﬂaws can easily be\nﬁxed. Besides them, we didn’t discover any major usabil-\nity problems.\nFinally, we also observed the participants while inter-\nacting in pairs with SongWords. Observations were that\nthe color-coding of items from collections worked, even\nthough clear textual or tag distinctions between the col-\nlections were not visible. Also as expected from previous\nresearch on tabletop displays, participants quickly began\ntaking turns when interacting with the system in order not\nto get tangled up.6. DISCUSSION AND LIMITATIONS\nOne principal limitation of this approach is the fact that it\ndoesn’t apply to purely instrumental music. As discussed\nin the introduction, this is not a very strong limitation for\ncontemporary popular music, but entirely excludes much\nof the classical music genre, for example. One of the major\nchallenges in working with actual music collections is their\nsize. The hardware-accelerated graphics of SongWords\ncurrently produce interactive frame rates for collections of\nseveral thousands of songs, but for a practical deployment\nthere are other restrictions: Gathering the lyrics for a song\ntakes several seconds and has to be performed sequentially\n(in order not to ﬂood the web servers with requests). The\ntime for training the self-organizing map grows linearly\nwith the number of songs and has to happen twice (once\nfor the lyrics, once for the tags) when the program ﬁrst\nreads a new collection. Fortunately, the map can be incre-\nmentally updated when new songs are added.\nThe text analysis capabilities of SongWords are cur-\nrently limited to the most discriminative terms from each\nsong. These most important words can be seen in the maps\nat ﬁrst glance and spatial organization is correctly based\non these statistical relationships. As the analysis uses the\nTF/IDF approach [21], it works especially well when the\ncollection is quite dissimilar regarding the words to pro-\nduce clear distinctions. Subtler differences will go unno-\nticed, and would require more sophisticated methods from\nNatural Language Processing.\n7. CONCLUSION AND FUTURE WORK\nWe have presented SongWords, a user interface for brows-\ning music collections based on their lyrics. The visualiza-\ntion using self-organizing maps, combined in a zoomable\nuser interfaces with interactions for searching, marking and\nreordering, allows a new perspective on music collections.\nIn particular, we observed that users were able to explore\ncorrelations between fragments of the lyrics and genre or\nother user-generated tags. These correlations would be im-\npossible to discover with current list-based interfaces or vi-\nsualizations purely based on audio data analysis.\nIn an evaluation we identiﬁed a number of minor de-\nsign ﬂaws of our current prototype, which we will ﬁx in\na future version. We will also explore more sophisticated\nnatural language processing and visualization methods, for\nexample involving synonyms and hierarchical clusters of\nsimilarity in order to create an even more meaningful sim-\nilarity measure on lyrics.\n8. REFERENCES\n[1] S. Baumann and A. Kl ¨uter. Super Convenience for\nNon-Musicians: Querying MP3 and the Semantic\nWeb. In Proceedings of the International Conference\non Music Information Retrieval, pages 297–298. IS-\nMIR, 2002.\n[2] R. Dachselt and M. Frisch. Mambo: a facet-based\nzoomable music browser. In Proceedings of the 6th in-\n535\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)ternational conference on Mobile and ubiquitous mul-\ntimedia, pages 110–117. ACM, 2007.\n[3] D. Diakopoulos, O. Vallis, J. Hochenbaum, J. Mur-\nphy, and A. Kapur. 21st Century electronica: MIR\ntechniques for classiﬁcation and performance. In Proc.\n10th International Society for Music Information Re-\ntrieval Conference (ISMIR 2009), pages 465–469. IS-\nMIR, 2009.\n[4] Clifton Forlines, Daniel Wigdor, Chia Shen, and Ravin\nBalakrishnan. Direct-touch vs. mouse input for table-\ntop displays. In Proceedings of the SIGCHI confer-\nence on Human factors in computing systems - CHI\n’07, pages 647–656, New York, New York, USA, 2007.\nACM Press.\n[5] B. Fortuna, M. Grobelnik, and D. Mladeni ´c. Visual-\nization of text document corpus. In Informatica, vol-\nume 29, pages 497–502, 2005.\n[6] H. Fujihara, M. Goto, and J. Ogata. Hyperlinking\nLyrics: A Method for Creating Hyperlinks Between\nPhrases in Song Lyrics. In Proc. 9th International So-\nciety for Music Information Retrieval Conference (IS-\nMIR ’08), pages 281–286. ISMIR, 2008.\n[7] H Fujihara, M Goto, J Ogata, K Komatani, T Ogata,\nand H G Okuno. Automatic synchronization between\nlyrics and music CD recordings based on Viterbi align-\nment of segregated vocal signals. In Proceedings of the\nEighth IEEE International Symposium on Multimedia\n(ISM’06), pages 257–264, 2006.\n[8] Masataka Goto. Active Music Listening Interfaces\nBased on Signal Processing. In IEEE International\nConference on Acoustics, Speech and Signal Process-\ning - ICASSP ’07, volume 2007. IEEE, April 2007.\n[9] H. Hirjee and D.G. Brown. Automatic Detection of In-\nternal and Imperfect Rhymes in Rap Lyrics. In Proc.\n10th International Society for Music Information Re-\ntrieval Conference (ISMIR 2009), pages 711–716. IS-\nMIR, 2009.\n[10] S Hitchner, J Murdoch, and G Tzanetakis. Music\nBrowsing Using a Tabletop Display. In Proc. 8th Inter-\nnational Conference on Music Information Retrieval\n(ISMIR’07), pages 175–176. ISMIR, 2007.\n[11] Carles F. Juli `a and Sergi Jord `a. Songexplorer: a table-\ntop application for exploring large collections of songs.\nInProc. 10th International Society for Music Informa-\ntion Retrieval Conference (ISMIR ’09), pages 675–680.\nISMIR, 2009.\n[12] M.Y . Kan, Y . Wang, D. Iskandar, T.L. Nwe, and\nA. Shenoy. LyricaAlly: Automatic synchronization of\ntextual lyrics to acoustic music signals. IEEE Trans-\nactions on Audio Speech and Language Processing,\n16(2):338–349, 2008.[13] F. Kleedorfer, P. Knees, and T. Pohle. Oh oh oh whoah!\ntowards automatic topic detection in song lyrics. In\nProc. 9th International Society for Music Information\nRetrieval Conference (ISMIR ’08), pages 287–292. IS-\nMIR, 2008.\n[14] P. Knees, M. Schedl, and G. Widmer. Multiple lyrics\nalignment: Automatic retrieval of song lyrics. In Pro-\nceedings of 6th international conference on music in-\nformation retrieval (ISMIR 05), pages 564–569. IS-\nMIR, 2005.\n[15] T. Kohonen. The self-organizing map. Proceedings of\nthe IEEE, 78(9):1464–1480, 1990.\n[16] R. Mayer, R. Neumayer, and A. Rauber. Rhyme and\nstyle features for musical genre classiﬁcation by song\nlyrics. In Proc. 9th International Society for Music\nInformation Retrieval Conference (ISMIR ’08), pages\n337–342. ISMIR, 2008.\n[17] E. Pampalk. Islands of music: Analysis, organization,\nand visualization of music archives. Journal of the\nAustrian Soc. for Artiﬁcial Intelligence, 22(4):20–23,\n2003.\n[18] E. Pampalk, S. Dixon, and G. Widmer. Exploring mu-\nsic collections by browsing different views. Computer\nMusic Journal, 28(2):49–62, Juni 2004.\n[19] C. Plaisant. The challenge of information visualization\nevaluation. In Proceedings of the working conference\non Advanced visual interfaces, pages 109–116. ACM,\n2004.\n[20] M.F. Porter. An algorithm for sufﬁx stripping. Pro-\ngram, 14(3):130–137, 1980.\n[21] G. Salton and C. Buckley. Term-weighting approaches\nin automatic text retrieval. Information processing &\nmanagement, 24(5):513–523, 1988.\n[22] Ian Stavness, Jennifer Gluck, Leah Vilhan, and Sid-\nney Fels. The MUSICtable: A Map-Based Ubiq-\nuitous System for Social Interaction with a Digital\nMusic Collection. Lecture Notes in Computer Sci-\nence, 3711/2005(Entertainment Computing - ICEC\n2005):291–302, Juni 2005.\n[23] M. Torrens, P. Hertzog, and J.L. Arcos. Visualizing and\nexploring personal music libraries. In Proc. 5th Inter-\nnational Conference on Music Information Retrieval\n(ISMIR ’04), pages 421–424. ISMIR, 2004.\n[24] Stephen V oida, Matthew Tobiasz, Julie Stromer, Pe-\ntra Isenberg, and Sheelagh Carpendale. Getting Prac-\ntical with Interactive Tabletop Displays: Designing\nfor Dense Data, Fat Fingers, Diverse Interactions, and\nFace-to-Face Collaboration. In Proc. ACM Interna-\ntional Conference on Interactive Tabletops and Sur-\nfaces (ITS), pages 109–116. ACM, 2009.\n536\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "Scalable Genre and Tag Prediction with Spectral Covariance.",
        "author": [
            "James Bergstra",
            "Michael I. Mandel",
            "Douglas Eck"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1416942",
        "url": "https://doi.org/10.5281/zenodo.1416942",
        "ee": "https://zenodo.org/records/1416942/files/BergstraME10.pdf",
        "abstract": "Cepstral analysis is effective in separating source from fil- ter in vocal and monophonic [pitched] recordings, but is it a good general-purpose framework for working with mu- sic audio? We evaluate covariance in spectral features as an alternative to means and variances in cepstral features (par- ticularly MFCCs) as summaries of frame-level features. We find that spectral covariance is more effective than mean, variance, and covariance statistics of MFCCs for genre and social tag prediction. Support for our model comes from strong and state-of-the-art performance on the GTZAN genre dataset, MajorMiner, and MagnaTagatune. Our clas- sification strategy based on linear classifiers is easy to im- plement, exhibits very little sensitivity to hyper-parameters, trains quickly (even for web-scale datasets), is fast to ap- ply, and offers competitive performance in genre and tag prediction.",
        "zenodo_id": 1416942,
        "dblp_key": "conf/ismir/BergstraME10",
        "keywords": [
            "Cepstral analysis",
            "source separation",
            "filter separation",
            "vocal recordings",
            "monophonic recordings",
            "music audio",
            "covariance",
            "spectral features",
            "genre prediction",
            "tag prediction"
        ],
        "content": "SCALABLE GENRE AND TAG PREDICTION\nWITH SPECTRAL COVARIANCE\nJames Bergstra\nUniversity of Montreal\nbergstrj@iro.umontreal.caMichael Mandel\nUniversity of Montreal\nmandelm@iro.umontreal.caDouglas Eck\nUniversity of Montreal\neckdoug@iro.umontreal.ca\nABSTRACT\nCepstral analysis is effective in separating source from ﬁl-\nter in vocal and monophonic [pitched] recordings, but is it\na good general-purpose framework for working with mu-\nsic audio? We evaluate covariance in spectral features as an\nalternative to means and variances in cepstral features (par-\nticularly MFCCs) as summaries of frame-level features.\nWe ﬁnd that spectral covariance is more effective than mean,\nvariance, and covariance statistics of MFCCs for genre and\nsocial tag prediction. Support for our model comes from\nstrong and state-of-the-art performance on the GTZAN\ngenre dataset, MajorMiner, and MagnaTagatune. Our clas-\nsiﬁcation strategy based on linear classiﬁers is easy to im-\nplement, exhibits very little sensitivity to hyper-parameters,\ntrains quickly (even for web-scale datasets), is fast to ap-\nply, and offers competitive performance in genre and tag\nprediction.\n1. INTRODUCTION\nMany features for music classiﬁcation have a longer his-\ntory in speech recognition. One of the ﬁrst steps taken\nby most speech recognizers is to transform audio contain-\ning speech into a sequence of phonemes. A phoneme is\nthe smallest segmental unit of sound, for example the /b/\nin “boy”. For a speech recognizer to work with multiple\nspeakers, it needs to generalize over a range of voice types\n(adult verus child, male verus female). To achieve this\ngeneralization it can be useful to separate the audio sig-\nnal into two parts: the source excitation at the vocal cords\nand the transfer function (ﬁltering) of the vocal tract. Cep-\nstral analysis is commonly used to achieve this separation.\nThe cepstrum Cis deﬁned as\nC=jGlog(jFxj2)j2(1)\nwhereFis the discrete Fourier transform and Gis the\ninverse discrete Fourier transform or the discrete cosine\ntransform. One important property of the cepstrum is that\nconvolution of two signals can be expressed as the addition\nof their cepstra.\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2010 International Society for Music Information Retrieval.In general cepstral analysis for speech recognition or\nmusic analysis is done on a power spectrum jFxj2that\nhas been downsampled (compressed) non-linearly to bet-\nter model human perception of equidistant pitches (the Mel\nscale). The resulting cepstral-domain values are called Mel-\nFrequency Cepstral Coefﬁcients (MFCCs). See Section 2\nfor details.\nIn the domain of music, cepstral analysis can be used\nto model musical sounds as a convolution of a pitched\nsource (perhaps a vibrating string) and an instrument body\nthat acts as a ﬁlter. This deconvolution of musical source\n(pitch) from musical ﬁlter (instrument) can be seen for sev-\neral instruments in Figure 1. If our goal is to generalize\nacross different pitches for a single instrument timbre, it\nis clear that the cepstral domain has advantages over the\nspectral domain.\nThe main use of source / ﬁlter deconvolution in speech\nis allow for the elimination of the source. This is achieved\nby only retaining the ﬁrst few cepstral coefﬁcients (usu-\nally 12 MFCCs for speech recognition). However, music\nis not speech. The assumption that recorded music consists\nof a ﬁlter with it’s own sound quality (instrument timbre)\nacting on an irrelevant source is certainly false. For many\ninstruments, it is difﬁcult to distinguish between pitch and\ntimbre, and certainly the assumption breaks down in poly-\nphonic recordings.\nThis paper presents segment covariance features as an\nalternative to means and variances in MFCCs for condens-\ning spectral features for linear classiﬁcation. The covari-\nance, together with mean and variance in spectral features\nprovides a better basis for genre and tag prediction than\nthose same statistics of MFCCs. These features are quick\nand easy to compute (pseudo-code given below) work well\nwith linear classiﬁcation. Together they offer a viable ap-\nproach to web-scale music classiﬁcation, and a competitive\nnull model for research on new datasets.\n2. FEATURES\nWe follow [1] in distinguishing two levels, or stages, of\nfeature extraction. Firstly, at the frame level (typically\n20-50 milliseconds) we extract features such as Mel Fre-\nquency scale Cepstral Coefﬁcients (MFCCs). Secondly,\nat the segment level (typically 3-10 seconds) we summa-\nrize frame-level features via statistics such as the mean,\nvariance and covariance. For our frame-level features we\npartitioned the audio into contiguous frames of either 512,\n507\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)0.0 0.4 0.8\ninst. note0.0 0.4 0.8Spectrum CepstrumAccuracy AccuracyViolin Marimba OboeFigure 1. Comparison of spectral and cepstral analysis for eight notes from twelve instruments. Right: The six panels\nshow spectral and cepstral anslysis for three of the twelve instruments. The horizontal axis is frequency for the three\nspectral panels, and time for the three cepstrum panels. The vertical axis for each of the eight subplots is coefﬁcient\nmagnitude. Spectral analysis (above) highlights the overtone series. Cepstral analysis separates pitch and timbre for pitched\ninstruments, separation less clear for marimba. Left: Nearest neighbor classiﬁers using spectral and cepstral features to\npredict note and instrument labels–cepstrum predicts instruments, spectrum predicts notes.\n1024, or 2048 samples, depending on the samplerate –\nwhichever corresponded to a duration between 25 and 50\nms.\nWe compare two kinds of frame-level features in this\nwork: MFCCs and Log-scaled Mel-Frequency Spectro-\ngrams (denoted LM features). MFCCs have been used ex-\ntensively for music classiﬁcation [1, 3, 6–8, 12, 14]. As our\nnull model, we implemented MFCCs as in Dan Ellis’ Ras-\ntamat toolbox’s implementation of HTK’s MFCC.1Our\nMFCC frame-level feature was 16 coefﬁcients computed\nfrom 36 critical bands spanning the frequency range 0-\n4kHz using the HTK Mel-frequency mapping\nMel(freq) = 2595 log10(1 + freq=700); (2)\nusing a type-II DCT, and with Hamming windowing. Here\nthe variable freq denotes the frequency of an FFT band,\nand it is in units of Hz.\nThe deﬁnition of our LM features differs subtly from the\nMFCCs in ways that make the feature computation faster\nand the implementation simpler. The audio is scaled to\nthe range (-1.0,1.0). No windowing is applied to the raw\naudio frames prior to the Fourier transform. A small con-\nstant value (10\u00004) is added in the Fourier domain to rep-\nresent a small amount of white noise and prevent division\nby zero. The Fourier magnitudes (not the power of each\nband) were projected according to a Mel-scale ﬁlterbank\n(theMin Table 2). The loudness in each critical band was\napproximated by the logarithm of its energy. Our experi-\nments used from 16 to 128 critical bands (LM coefﬁcients)\nto cover the frequencies from 0Hz to16kHz .\nLM = log10(jFAjM + 10\u00004) + 4 (3)\nMFCC =G(LM) (4)\nEquations 3 and 4 describe the computation of the MFCC\nand LM features in terms of an audio matrix Athat has a\n1HTK: http://htk.eng.cam.ac.uk/sr <-samplerate\nnf <-number of Fourier bins\nnm <-number of mel-scale coefficients\nnyq = sr/2\nnyq_mel = 2595 *log10(1 + nyq/700.)\n# Build a Mel-Frequency Scale\n# Filterbank matrix M\nM = zero_matrix(rows=nf, cols=nf)\nfor i in 0..(nf-1):\nf = i *nyq / nf\nf_mel = 2595 *log10(1 + f/700.)\nm_idx = f_mel/nyq_mel *nm\nj = floor(m_idx)\nM(j+1,i) = m_idx-j\nM(j+0,i) = 1.0-m_idx+j\nTable 1. Pseudo-code for building a Mel-Frequency\nScaled ﬁlterbank in the frequency domain. This code as-\nsumes 0-based array indexing.\nrow for each frame in a segment, and a column for each\nsample in a frame. The FFT is taken over columns. The\nconstant 10\u00004quantiﬁes our uncertainty in jFAj and pre-\nvents taking a logarithm of zero. The subtraction of 4from\nthe logarithm ensures that LM features are non-negative,\nbut take value zero when the audio is silent. The MFCCs\nare the discrete cosine transform Gof LM.\n3. PERFORMANCE\nWe used three datasets to explore the value of these fea-\ntures in different descriptor prediction settings: tag fre-\nquency, tag presence, and genre. Segments were summa-\nrized by the mean, variance, and/or covariances of frame-\nlevel features. The summaries were classiﬁed by either a\n508\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)logistic regression model, or where noted, by a Support-\nVector Machine (SVM) with an RBF kernel.\nIn linear models, we trained binary and multi-class lo-\ngistic regression to maximize the expected log-likelihood\n\u0000X\nx2XX\nl2LPtrue(ljx) log(P predicted (ljx)) (5)\nwhereXis the set of examples, and Lthe set of labels.\nThe linear (technically afﬁne) prediction was normalized\nacross the classes using a Gibbs distribution.\nPpredicted (ljx) =eZlf(x)+b l\nP\nk2LeZkf(x)+b k(6)\nwheref(x)is the features we extract, Zlis thelthrow\nof the linear model, and blis a scalar bias for the lthpre-\ndictor. We ﬁt this model by gradient descent with a small\nlearning rate and regularize it by early stopping on held-\nout data. Our SVM experiments were performed using\nLIBSVM with the RBF kernel to implement all-pairs mul-\nticlass prediction. Held-out data was used to choose the\nbest kernel and cost parameters (typically called \randC\nin SVM literature). Features [by dimension] were normal-\nized to have zero mean and unit variance prior to training\na classiﬁer.\n3.1 Genre Prediction\nGenre classiﬁcation performance was estimated using the\nGTZAN dataset of 1000 monophonic audio clips [12], each\nof which is 30 seconds long. The dataset features 100\nclips each of the 10 genres: blues, classical, country, disco,\nhiphop, pop, jazz, metal, reggae, and rock. Although this\ncollection is relatively small, it has been used in several\nstudies of genre classiﬁcation [1, 5, 10, 13]. Classiﬁcation\nwas performed by partitioning the audio into 3-second seg-\nments and voting over the segments, as in [1]. Following\nstandard practice, performance was measured by standard\n10-fold cross-validation. For each fold, the training set was\ndivided into a hold-out set (of 100 randomly chosen songs)\nand ﬁtting set (of the remaining 800). The results of our\nmodels and some results from the literature are shown in\nTable 2\nOur best results with both the linear and SVM classi-\nﬁers were with the mean and covariance (or correlation) in\nLM features rather than MFCC features. Our linear model\nusing covariance in LM features was approximately 77%\naccurate, while the more expensive SVM was 81% accu-\nrate. The small size of the GTZAN dataset leaves con-\nsiderable variance in these performance estimates (scores\nare accurate to within about 4 percentage points with 95%\nprobability). To our knowledge, the only systems to sur-\npass our baseline linear classiﬁer are the AdaBoost-based\nmodel of [1] and the model of [10] based on L1-regularized\ninference and non-negative tensor factorization. Both of\nthese superior models are signiﬁcantly more complicated\nand CPU intensive than our baseline. In larger datasets, the\ncapacity of the linear model to use more data in a tractable\namount of time should make its performance improve in\ncomparison to the SVM.Algorithm Acc.(%)\nSparse rep. + tensor factor. [10] 92\nAdaBoost + many features [1] 83\n*RBF-SVM with LM (m32,r32) 81\n*RBF-SVM with LM (m32,c32) 79\n*Log. Reg. with LM (m32,r32) 77\n*RBF-SVM with MFCC (m32,c32) 76\n*Log. Reg. with LM (m32,c32) 76\n*RBF-SVM with MFCC (m32,r32) 74\n*Log. Reg. with MFCC (m32,r32) 72\n*Log. Reg. with MFCC (m32,c32) 70\nRBF+MFCC [11] 72\nLDA+MFCC(m5,v5) + other [5] 71\nGMM+MFCC(m5,v5) + other [13] 61\nTable 2. Classiﬁcation accuracies on the GTZAN dataset\nof our algorithms (*) and others in the literature. ‘m32’\nis the means of 32 frame-level features, ‘c32’ is the upper\ntriangle in their covariance matrix, ‘r32’ is the upper trian-\ngle in their correlation matrix. These scores are accurate to\nwithin about 4 percentage points with 95% probability.\n3.2 Tag Frequency\nWe estimated our models’ performance at tag frequency\nprediction using the MajorMiner dataset. The MajorMiner\ndataset consists of short audio clips labeled by players of\nthe MajorMiner web game [7]. In this game, players listen\nto 10-second clips of songs and describe them with free-\nform tags. They score points for using tags that agree with\nother players’ tags, but score more points for using orig-\ninal tags that subsequent players agree with. There are\n1000 unique tags that have been veriﬁed by two players\nand there are a total of 13,000 such veriﬁed usages on 2600\nclips. The average clip has been seen by 7 users and de-\nscribed with 30 tags, 5 of which have been veriﬁed. The\ntags describe genres, instruments, the singer (if present),\nand general musical or sonic qualities.\nThe MajorMiner dataset includes the number of times\neach tag was applied to each clip, which gives a graded\nrelevance for each (tag, clip) pair. After removing clips\nthat were tagged ‘silent’,‘end’,‘nothing’ or had a length\nless than eight seconds, 2578 remained. Clips are typically\nnine seconds in length but we used the ﬁrst 8 seconds. As\nsecond-level features, we summarized each clip as a sin-\ngle 8-second segment. We followed [7] in using the top 25\ntags (drum, guitar, male, synth, rock, electronic, pop, vo-\ncal, bass, female, dance, techno, piano, jazz, hip hop, rap,\nslow, beat, voice, 80s, electronica, instrumental, fast, sax-\nophone, keyboard) which accounted for about half of the\ntag usages in the dataset. To ensure that each clip had a\nvalid distribution over these tags, we added to every clip\nan extra virtual tag with a usage of 0.1. For most clips this\nusage accounted for about 1.5% of the total tag usage, but\nfor a small number of clips with none of the top 25 tags it\naccounted for 100%. The clips in the dataset were sorted\nby their order of occurrence in the ”three columns.txt” ﬁle.\nThe dataset was partitioned into train, validation, and test\n509\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)sets by taking the 6thof every 10 clips for validation, and\nby taking the 7thand8thfor testing.\nWe interpreted the tag usages in the MajorMiner dataset\nas being samples from clip-conditional multinomial distri-\nbutions. We would like a function to infer that whole dis-\ntribution by conditioning on the audio of the clip. This\nlearning setup differs from multiple independent label pre-\ndiction, for that case see the Tagatune results below.\nOur results are summarized in Figure 2. Our best results\nwere obtained by using LM features and keeping many co-\nefﬁcients: 128 means, 128 variances, covariance in a 32-\ndimensional downsampling of all 128 features. The best\nmodel (m128,v128,c32) had an average (across tag) AUC-\nROC of 0.78, which is competitive with the best models\nfrom the MIREX 2009 MajorMiner evaluation. The av-\nerage AUC-ROC values across the 25 tags used here for\nthe MIREX 2009 submissions range from 0.72 (submis-\nsion “HBC”) to 0.79 (submission “LWW2”).2\nThere are two interesting things to note about the ef-\nfect of covariance features in the MajorMiner experiments.\nFirstly, the covariance in MFCC features was strictly harm-\nful in this prediction task, when used alongside the mean\nand variance. It has been argued that MFCC features are al-\nready relatively decorrelated compared with spectral mag-\nnitudes [9]. Perhaps when we normalize the MFCC co-\nvariance features we add features with a very low signal\nto noise ratio. Secondly, the LM features without covari-\nance are poorer than a like number of MFCC features.\nThis is similar to the simple experiment in the introduction\nthat demonstrated the superior ability of cepstral features\nto generalize across pitch—that simple experiment corre-\nsponds to using the mean frame-level feature. Spectral fea-\ntures do not generalize as well across pitch without covari-\nance information, but with covariance information they are\nbetter at it.\nAll the models were trained in just a few minutes on\na desktop PC. Extracting features and training the model\nwere fast enough that the decoding of the MP3 ﬁles was a\nsigniﬁcant part of the overall experiment time.\n3.3 Tag Presence\nWe also tested spectral covariance features in a larger and\nsparser descriptor-based retrieval setting using the Magna\nTagatune (Tagatune) dataset. Tagatune contains 25863 30-\nsecond audio clips [4]. Each clip is labeled with one or\nmore binary attributes from a collection of 188 potential\ndescriptors such as ‘guitar’, ‘classical’, ‘no voices’, ‘world’,\n‘mellow’, ‘blues’, ‘harpsicord’, ‘sitar’. These descriptors\nwere collected from an online game in pairs of players use\nthese words/phrases to determine whether they are listen-\ning to the same song or not. The descriptors generally\nrefer to instrumentation, genre, and mood (see Table 4).\nAttributes in the dataset are binary and do not reﬂect the\ndegree to which any attribute applies to a song. Further-\nmore, the nature of the data-collection game is such that\nthe non-occurrence of an attribute is weak evidence that\n2MIREX 2009 Results:\nhttp://www.music-ir.org/mirex/2009/index.php\n0.40.450.50.550.60.650.70.750.80.850.90.95MFCC\nLM\nArea under ROCNumber of Tagatune tags\n0 10 20 30 40Figure 3. Histogram of the success of a linear classiﬁer at\npredicting Tagatune attributes from LM (light) and MFCC\n(dark) means and covariance. AUC-ROC reﬂects both pre-\ncision and recall; 0.5 is expected from a random predictor,\nand 1.0 from a perfect one. The MFCC features get 3 more\ntags 80% right, but the LM features get 16 more tags 90%\nright. Overall, LM features give better performance.\nthe attribute does not apply. There are many descriptors\nwhich might apply to a song, but no player thought to use\nthem. Still, the Tagatune dataset provides a great deal of\nhuman-veriﬁed annotations and audio–despite these minor\nproblems it is a great resource.\nFor the Tagatune dataset an independent logistic regres-\nsion model was used to predict each potential attribute.\nOnly logistic regression was used because the size of the\ndataset was prohibitive of the SVM’s quadratic training al-\ngorithm. Song classiﬁcation was performed by partition-\ning each clip into 3-second segments and voting over the\nsegments, as in [1]. The dataset was partitioned into train,\nvalidation, and test sets by taking the 6thof every 10 clips\nfor validation, and by taking the 7thand8thfor testing.\nA plateau in validation-set accuracy (averaged across all\ntasks) was reached after about 20 minutes of CPU (wall)\ntime, after about 10 passes through the training data.\nThe results of a linear classiﬁer applied to the LM and\nMFCC mean and covariance feature are shown in Figure 3\nand Table 4. Again, the LM features with covariance out-\nperformed statistics of MFCC features. For the LM-based\nmodel, 140 of 188 descriptors were over :8ROC and 68 of\nthose are over :9. The MIREX 2009 Tagatune evaluation\nused a test protocol almost identical to ours, and found that\nthe participants’ average ROC scores ranged from 0:67 to\n0:83. Our simple model appears actually to score slightly\nbetter than the MIREX 2009 participants did.\n510\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Feature Predicted�weights�sor ted�by�true�rank\nLM�m128,v128,c32\nMFCC�m128,v128,c32\nMFCC�m32,v32,c32MFCC�m16,v16,c16LM�m16,v16,c162.65\n2.74\n2.762.68\n2.71True�weights�sorted�b y�predicted�rank\n2.63\nLM�m32,var32,c32Cross-entropy\nMFCC�m32,v32 2.71\nLM�m32,v32 2.72Ground�TruthFigure 2. LM features outperform MFCC features for predicting tag distributions in the MajorMiner dataset. ‘m32’\n(similarly ‘m16’,‘m128’) denotes the mean in 32 coefﬁcients, ‘v32’ the variance, and ‘c32’ the covariance (the upper\ntriangle of the 32x32 matrix). When covariance information is used, LM features outperform MFCCs. Images in the\nmiddle column illustrate the recall of each model for each of the 25 most popular tags: the greater the density toward the\nleft, the higher the recall of the model. Images in the right column illustrate the corresponding precision. The best model\nhas AUC-ROC of 0.78.\n4. DISCUSSION\nOur covariance and correlation features help to summarize\n(in a time-independent way) the different kinds of timbres\nobserved during a segment. The covariance (or correla-\ntion) in LM frame-level features summarizes the way that\nenergy/loudness in different frequency bands co-occur in\na signal. Recall that the timbre of a sustained instrument\nnote can be crudely approximated by the shape of the spec-\ntral envelope. The difﬁculty in recognizing instruments\nin a segment after it has been summarized by the mean\nor variance is that when different instruments are present\nin one segment, the mean envelope can be indistinguish-\nable from the mean that would come from other instru-\nment combinations. The covariance is an improvement in\nthis respect. If different instruments in a segment do not\nplay simultaneously, then the covariance will encode ac-\ntivity corresponding to the (pairs of) peaks in the envelope\nof each instrument. To the extent that these instruments\nhave timbres with different pairs of peaks, the instruments\nwill not interfere with one another in the segment-level fea-\nture. Still, when instruments are played simultaneously (as\noften happens!) there is more interference.\n4.1 MAP vs. ROC\nTable 4 lists the mean average precision (MAP) and clas-\nsiﬁcation error rate (ERR) for some of the most popular\nand least popular attributes. The classiﬁcation accuracies\nfor most descriptors is quite close to the baseline of ran-\ndom guessing. That is because most descriptors are so rare\nthat when a trained model is forced to take a hard classi-\nﬁcation decision, it is very difﬁcult for the evidence from\nthe song feature to outweigh the overwhelming prior that\na rare descriptor does not apply. A similar ﬁnding is de-\nscribed in [2]. Since the attributes are rare, even the best\nmodels rank many negative-labeled examples also near the\nbeginning of the clip collection, so precision and classiﬁ-Attr Count\nfemale vocals 386\nmale vocals 465\nfemale vocal 644\nno vocal 995\nmale vocal 1002\nno vocals 1158\nvocals 1184\nvocal 1729\nTable 3. The number of applications of several Tagatune\nattributes over all 25863 clips. One of the difﬁculties with\nTagatune is the frequency of false negative attributes in the\ndataset. For example, although vocal was applied 1729\ntimes, the ’no vocal’ attribute was applied only 995 times;\n90% of the clips are labeled as neither ’vocal’ nor ’no vo-\ncal’.\ncation error are low. But the high rate of false-negative la-\nbels (see Table 3) biases the MAP and ERR measures more\nthan the ROC. In the case of MagnaTagatune, where there\nare many false-negative labels (true instances, labeled as\nnon-instances) MAP and ERR criteria are potentially very\nbiased estimators of model performance. The ROC mea-\nsure a more appropriate criterion in this context.\n5. CONCLUSION\nThe covariance of log-magnitude Mel frequency scale spec-\ntralcoefﬁcients (LM features) offer a superior alternative\nto statistics of MFCCs when summarizing frame-level au-\ndio features for genre and tag prediction. We have demon-\nstrated the advantage of our LM features on three standard\ngenre and tag prediction datasets: GTZAN, MajorMiner,\nand Tagatune. Furthermore, these features make state of\nthe art performance available with just a linear classiﬁer\n(such as L1- or L2-regularized logistic regression, or lin-\n511\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Attr Freq ERR MAP ROC\nguitar 0.187 0.159 0.62 0.85\nclassical 0.169 0.157 0.63 0.91\nslow 0.135 0.135 0.32 0.77\ntechno 0.110 0.090 0.58 0.92\nstrings 0.110 0.110 0.44 0.87\ndrums 0.102 0.102 0.34 0.84\nelectronic 0.096 0.097 0.33 0.84\nrock 0.093 0.057 0.71 0.96\nfast 0.093 0.093 0.31 0.79\npiano 0.077 0.074 0.54 0.84\nrepetitive 0.001 0.001 0.15 0.77\nscary 0.001 0.001 0.02 0.97\nwoodwind 0.001 0.001 0.01 0.82\nviola 0.001 0.001 0.08 0.95\nquick 0.001 0.001 0.00 0.56\nsoprano 0.001 0.001 0.17 0.97\nhorns 0.001 0.001 0.00 0.68\nsoft rock 0.001 0.001 0.00 0.70\nmonks 0.001 0.001 0.31 0.99\nclasical 0.001 0.001 0.00 0.83\nhappy 0.001 0.001 0.00 0.61\nTable 4. Test set performance of best model on the 10\nmost popular (above) and least popular (below) descrip-\ntors in Tagatune. Freq is application frequency, ERR is\nclassiﬁcation error, MAP is mean average precision, and\nROC is the area under the ROC.\near SVM). Our results on the GTZAN dataset suggest that\nRBF SVMs may offer slightly better performance when\ntime allows for training.\nThe LM features are straightforward to implement, com-\nputationally cheap, and the use of a linear classiﬁer makes\nour model viable on any size of genre or tag-prediction\ndataset. We believe that the model presented here has great\npotential for working with industrial-scale audio datasets.\nFinally, the results presented here demonstrate that the\ndiscrete cosine transform (or inverse Fourier transform) re-\nsponsible for the cepstrum’s deconvolution property actu-\nally hinders performance in some circumstances. One ex-\nplanation of this behavior is that our model learns a better\ndeconvolution-like transform of the spectral data than is\nprovided by the cepstrum. We admit that this is only one\npossible explanation of these results and that further anal-\nysis is necessary in order to make conclusions. We believe\nthat this is one fruitful direction for future research.\n6. REFERENCES\n[1] J. Bergstra, N. Casagrande, D. Erhan, D. Eck, and\nB. Kegl. Aggregate features and AdaBoost for music\nclassiﬁcation. Machine Learning, 65:473–484, 2006.\n[2] T. Bertin-Mahieux, D. Eck, F. Maillet, and P. Lamere.\nAutotagger: A model for predicting social tags from\nacoustic features on large music databases. Journal of\nNew Music Research, 37(2):115–135, 2008.[3] Thibault Langlois and Gonalo Marques. A music clas-\nsiﬁcation method based on timbral features. In ISMIR,\nKobe, Japan, October 2009.\n[4] Edith L. M. Law and Luis von Ahn. Input-agreement:\nA new mechanism for data collection using human\ncomputation games. In CHI, pages 1197–1206, 2009.\n[5] Tao Li and George Tzanetakis. Factors in automatic\nmusical genre classiﬁcation. In IEEE Workshop on Ap-\nplications of Signal Processing to Audio and Acoustics,\nNew Paltz, New York, 2003.\n[6] Beth Logan. Mel frequency cepstral coefﬁcients for\nmusic modeling. In ISMIR, Plymouth, Mass., October\n2000.\n[7] M. Mandel and D. Ellis. A web-based game for\ncollecting music metadata. J. New Music Research,\n37(2):151–165, 2008.\n[8] Michael I. Mandel and Daniel P.W. Ellis. Song-level\nfeatures and support vector machines for music clas-\nsiﬁcation. In ISMIR, pages 594–599, London, UK,\nSeptember 2005.\n[9] P. Mermelstein. Distance measures for speech recogni-\ntion, psychological and instrumental. Pattern Recogni-\ntion and Artiﬁcial Intelligence, pages 374–388, 1976.\n[10] Yannis Panagakis, Constantine Kotropoulos, and Gon-\nzalo R. Arce. Music genre classiﬁcation using locality\npreserving non-negative tensor factorization and sparse\nrepresentations. In ISMIR, Kobe, Japan, October 2009.\n[11] Douglas Turnbull and Charles Elkan. Fast recognition\nof musical genres using RBF networks. IEEE Transac-\ntions on Knowledge and Data Engineering, 17(4):1–5,\nApril 2005.\n[12] G. Tzanetakis, G. Essl, and P. Cook. Automatic mu-\nsical genre classiﬁcation of audio signals. In ISMIR,\nBloomington, Indiana, October 2001.\n[13] George Tzanetakis and Perry Cook. Musical genre\nclassiﬁcation of audio signals. IEEE Transactions on\nSpeech and Audio Processing, 10(5):293–302, July\n2002.\n[14] Kris West and Stephen Cox. Finding an optimal seg-\nmentation for audio genre classiﬁcation. In ISMIR,\nLondon, UK, October 2005.\n512\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "Clustering Beat-Chroma Patterns in a Large Music Database.",
        "author": [
            "Thierry Bertin-Mahieux",
            "Ron J. Weiss",
            "Daniel P. W. Ellis"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1416720",
        "url": "https://doi.org/10.5281/zenodo.1416720",
        "ee": "https://zenodo.org/records/1416720/files/Bertin-MahieuxWE10.pdf",
        "abstract": "A musical style or genre implies a set of common con- ventions and patterns combined and deployed in different ways to make individual musical pieces; for instance, most would agree that contemporary pop music is assembled from a relatively small palette of harmonic and melodic patterns. The purpose of this paper is to use a database of tens of thousands of songs in combination with a com- pact representation of melodic-harmonic content (the beat- synchronous chromagram) and data-mining tools (cluster- ing) to attempt to explicitly catalog this palette – at least within the limitations of the beat-chroma representation. We use online k-means clustering to summarize 3.7 mil- lion 4-beat bars in a codebook of a few hundred prototypes. By measuring how accurately such a quantized codebook can reconstruct the original data, we can quantify the de- gree of diversity (distortion as a function of codebook size) and temporal structure (i.e. the advantage gained by joint quantizing multiple frames) in this music. The most popu- lar codewords themselves reveal the common chords used in the music. Finally, the quantized representation of mu- sic can be used for music retrieval tasks such as artist and genre classification, and identifying songs that are similar in terms of their melodic-harmonic content.",
        "zenodo_id": 1416720,
        "dblp_key": "conf/ismir/Bertin-MahieuxWE10",
        "keywords": [
            "common conventions",
            "patterns",
            "individual musical pieces",
            "compact representation",
            "melodic-harmonic content",
            "data-mining tools",
            "cluster",
            "quantized codebook",
            "reconstruct the original data",
            "music retrieval tasks"
        ],
        "content": "CLUSTERING BEAT-CHROMA PATTERNS\nIN A LARGE MUSIC DATABASE\nThierry Bertin-Mahieux\nColumbia University\ntb2332@columbia.eduRon J. Weiss\nNew York University\nronw@nyu.eduDaniel P. W. Ellis\nColumbia University\ndpwe@ee.columbia.edu\nABSTRACT\nA musical style or genre implies a set of common con-\nventions and patterns combined and deployed in different\nways to make individual musical pieces; for instance, most\nwould agree that contemporary pop music is assembled\nfrom a relatively small palette of harmonic and melodic\npatterns. The purpose of this paper is to use a database\nof tens of thousands of songs in combination with a com-\npact representation of melodic-harmonic content (the beat-\nsynchronous chromagram) and data-mining tools (cluster-\ning) to attempt to explicitly catalog this palette – at least\nwithin the limitations of the beat-chroma representation.\nWe use online k-means clustering to summarize 3.7 mil-\nlion 4-beat bars in a codebook of a few hundred prototypes.\nBy measuring how accurately such a quantized codebook\ncan reconstruct the original data, we can quantify the de-\ngree of diversity (distortion as a function of codebook size)\nand temporal structure (i.e. the advantage gained by joint\nquantizing multiple frames) in this music. The most popu-\nlar codewords themselves reveal the common chords used\nin the music. Finally, the quantized representation of mu-\nsic can be used for music retrieval tasks such as artist and\ngenre classiﬁcation, and identifying songs that are similar\nin terms of their melodic-harmonic content.\n1. INTRODUCTION\nThe availability of very large collections of music audio\npresent many interesting research opportunities. Given mil-\nlions of examples from a single, broad class (e.g. con-\ntemporary commercial pop music), can we infer anything\nabout the underlying structure and common features of this\nclass? This paper describes our work in this direction.\nWhat are the common features of pop music? There\nare conventions of subject matter, instrumentation, form,\nrhythm, harmony, and melody, among others. Our interest\nhere is in the tonal content of the music – i.e. the harmony\nand melody. As a computationally-convenient proxy for a\nricher description of the tonal content of audio, we use the\npopular chroma representation, which collapses an acous-\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2010 International Society for Music Information Retrieval.tic spectrum into a 12-dimensional description, with one\nbin for each semitone of the western musical octave. In\naddition, we simplify the time axis of our representation to\ntake advantage of the strong beat present in most pop mu-\nsic, and record just one chroma vector per beat. This beat-\nsynchronous chromagram representation represents a typ-\nical music track in a few thousand values, yet when resyn-\nthesized back into audio via modulation of octave-invariant\n“Shepard tones”, the melody and chord sequences of the\noriginal music usually remain recognizable [7]. To the ex-\ntent, then, that beat-chroma representations preserve tonal\ncontent, they are an interesting domain in which to search\nfor patterns – rich enough to generate musically-relevant\nresults, but simpliﬁed enough to abstract away aspects of\nthe original audio such as instrumentation and other stylis-\ntic details.\nSpeciﬁcally, this paper identiﬁes common patterns in\nbeat-synchronous chromagrams by learning codebooks from\na large set of examples. The individual codewords consist\nof short beat-chroma patches of between 1 and 8 beats, op-\ntionally aligned to bar boundaries. The additional temporal\nalignment eliminates redundancy that would be created by\nlearning multiple codewords to represent the same motive\nat multiple beat offsets. The codewords are able to rep-\nresent the entire dataset of millions of patterns with min-\nimum error given a small codebook of a few hundred en-\ntries. Our goal is to identify meaningful information about\nthe musical structure represented in the entire database by\nexamining individual entries in this codebook. Since the\ncommon patterns represent a higher-level description of\nthe musical content than the raw chroma, we also expect\nthem to be useful in other applications, such as music clas-\nsiﬁcation and retrieving tonally-related items.\nPrior work using small patches of chroma features in-\ncludes the “shingles” of [3], which were used to identify\n“remixes”, i.e., music based on some of the same underly-\ning instrument tracks, and also for matching performances\nof Mazurkas [2]. That work, however, was not concerned\nwith extracting the deeper common patterns underlying dif-\nferent pieces (and did not use either beat- or bar-synchronous\nfeatures). Earlier work in beat-synchronous analysis in-\ncludes [1], which looked for repeated patterns within single\nsongs to identify the chorus, and [7], which cross-correlated\nbeat-chroma matrices to match cover versions of pop mu-\nsic tracks. None of these works examined or interpreted\nthe content of the chroma matrices in any detail. In con-\ntrast, here we hope to develop a codebook whose entries\n111\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)0 1 2 3 4 5 6 70246810\nbeatschroma binsFigure 1: A typical codeword from a codebook of size 200\n(code 7 in Figure 4), corresponding to a tonic-subdominant chord\nprogression. The patch is composed of 2bars and the pattern\nlength was set to 8beats.\nare of interest in their own right.\n2. APPROACH\n2.1 Features\nThe feature analysis used throughout this work is based on\nEcho Nest analyze API [4]. For any song uploaded to their\nplatform this analysis returns a chroma vector (length 12)\nfor every music event (called “segment”), and a segmen-\ntation of the song into beats and bars. Beats may span or\nsubdivide segments; bars span multiple beats. Averaging\nthe per-segment chroma over beat times results in a beat-\nsynchronous chroma feature representation similar to that\nused in [7]. Echo Nest chroma vectors are normalized to\nhave the largest value in each column equal to 1.\nNote that none of this information (segments, beats, bars)\ncan be assumed perfectly accurate. In practice, we have\nfound them reasonable, and given the size of the data set,\nany rare imperfections or noise can be diluted to irrele-\nvance by the good examples. We also believe that patch\nsizes based on a number of beats or bars are more meaning-\nful than an arbitrary time length. This is discussed further\nin Section 5.1.\n2.2 Beat-Chroma Patches\nWe use the bar segmentation obtained from the Echo Nest\nanalysis to break a song into a collection of beat-chroma\n“patches”, typically one or two bars in length. Because\nthe bar length is not guaranteed to be 4 beats, depending\non the meter of a particular song, we resample each patch\nto a ﬁxed length of 4 beats per bar (except where noted).\nHowever, the majority (82%) of our training data consisted\nof bars that were 4 beats long, so this resampling usually\nhad no effect. Most of the remaining bars (10%) were 3\nbeats in length. The resulting patches consist of 12\u00024or\n12\u00028matrices.\nFinally, we normalize the patches with respect to trans-\nposition by rotating the pattern matrix so that the ﬁrst row\ncontains the most energy. This can be seen in the example\ncodeword of Figure 1. Each patch within a song is normal-ized independently, so reconstruction of the original song\nrequires knowledge of the rotation index for each patch.\nThe representation resulting from this process is invari-\nant to both the key and meter of the original song. This en-\nables the study of broad harmonic patterns found through-\nout the data, without regard for the speciﬁc musical con-\ntext. In the context of clustering this avoids problems such\nas obtaining separate clusters for every major triad for both\nduple and triple meters.\n2.3 Clustering\nWe use an online version of the vector quantization algo-\nrithm [8] to cluster the beat-chroma patches described in\nthe previous section. For each sample from the data, the\nalgorithm ﬁnds the closest cluster in the codebook and up-\ndates the cluster centroid (codeword) to be closer to the\nsample according to a learning rate `. The clusters are up-\ndated as each data point is seen, as opposed to once per it-\neration in the standard k-means algorithm. The details are\nexplained in Algorithm 1. As in standard k-means clus-\ntering, the codebook is initialized by choosing Krandom\npoints from our dataset. Note that this algorithm, although\nnot optimal, scales linearly with the number of patches\nseen and can be interrupted at any time to obtain an up-\ndated codebook.\nAlgorithm 1 Pseudocode for the online vector quantization\nalgorithm. Note that we can replace the number of iterations\nby a threshold on the distortion over some test set.\n`learning rate\nfPngset of patches\nfCkgcodebook of Kcodes\nRequire: 0< `\u00141\nfornIters do\nforp2fPngdo\nc minc2C kdist(p; c)\nc c+ (p\u0000c)\u0003`\nend for\nend for\nreturnfCkg\n3. EXPERIMENTS\nIn this section we present different clustering experiments\nand introduce our principal training and test data. Some\ndetailed settings of our algorithm are also provided. As for\nany clustering algorithm, we measure the inﬂuence of the\nnumber of codewords and the training set size.\n3.1 Data\nOur training data consists of 43;300tracks that were up-\nloaded to morecowbell.dj,1an online service based on the\nEcho Nest analyze API which remixes uploaded music by\nadding cowbell and other sound effects synchronized in\n1http://www.morecowbell.dj/\n112\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)0 1K 5K 10K 50K 100K 250K 500K\ndata size0.0350.0400.0450.0500.0550.0600.0650.070average errorEncoding error per training data size for certain conditions\n1 bar 4 beats\n2 bars 8 beatsFigure 2: Distortion for a codebook of size 100encoding one\nbar at a time with by 4columns. Therefore, each codeword has\n12\u00024 = 48 elements. Distortion is measured on the test set.\nTraining data sizes range from 0(just initialization) to 500; 000.\nPatterns were selected at random from the dataset of approxi-\nmately 3:7million patterns.\ntime with the music. The 43:3K songs contain 3:7mil-\nlion non-silent bars which we clustered using the approach\ndescribed in the previous section.\nFor testing, we made use of low quality (32kbps, 8 kHz\nbandwidth mono MP3) versions of the songs from the us-\npop2002 data set [5]. This data set contains pop songs from\na range of artists and styles. uspop2002 serves as test set\nto measure how well a codebook learned on the Cowbell\ndata set can represent new songs. We obtained Echo Nest\nfeatures for the 8651 songs contained in the dataset.\n3.2 Settings\nWe take one or two bars and resample the patches to 4 or\n8 columns respectively. We learn a codebook of size K\nover the Cowbell dataset using the online VQ algorithm\n(Algorithm 1). We use a learning rate of `= 0:01 for200\niterations over the whole dataset. We then use the resulting\ncodebook to encode the test set. Each pattern is encoded\nwith a single code. We can measure the average distance\nbetween a pattern and its encoding. We can also measure\nthe use of the different codes, i.e., the proportion of pat-\nterns that quantize to each code.\nWe use the average squared Euclidean distance as the\ndistortion measure between chroma patches. Given a pat-\nternp1composed of elements p1(i; j), and a similar pat-\nternp2, the distance between them is:\ndist(p 1; p2) =X\ni;j(p1(i; j)\u0000p2(i; j))2\nsize(p 1)(1)\nWe assume p1andp2have the same size. This is enforced\nby the resampling procedure described in Section 2.\n3.3 Codebook properties\nThis section presents some basic results of the clustering.\nWhile unsurprising, these results may be useful for com-\nparison when reproducing this work.\n\u000fEncoding performance improves with increasing train-\ning data (Figure 2). Distortion improvements plateau\nby around 1000 samples per codeword (100; 000sam-\nples for the 100-entry codebook of the ﬁgure).Codebook size Distortion\n1 0:066081\n10 0:045579\n50 0:038302\n100 0:035904\n500 0:030841\nTable 1: Distortion as a function of codebook size for a ﬁxed\ntraining set of 50;000 samples. Codebook consists of 1bar ( 4\nbeat) patterns.\n\u000fEncoding performance improves with increasing code-\nbook size (Table 1). Computation costs scales with\ncodebook size, which limited the largest codebooks\nused in this work, but larger codebooks (and more\nefﬁcient algorithms to enable them) are clearly a promis-\ning future direction.\n\u000fLarger patterns are more difﬁcult to encode, thus re-\nquiring larger codebooks. See Figure 3. The in-\ncrease is steepest below 4beats (1 bar), although\nthere is no dramatic change at this threshold.\n4. VISUALIZATION\n4.1 Codebook\nWe trained a codebook containing 200patterns sized 12\u0002\n8, covering 2bars at a time. The results shown are on the\nartist20 test set described in Section 5.2.\nThe 25 most frequently used codewords in the test set\nare shown in Figure 4. The frequency of use of these code-\nwords is shown in Figure 5. The codewords primarily con-\nsist of sustained notes and simple chords. Since they are\ndesigned to be key-invariant, speciﬁc notes do not appear.\nInstead the ﬁrst 7 codewords correspond to a single note\nsustained across two bars (codeword 0), perfect ﬁfth (code-\nwords 1 and 2) and fourth intervals (codewords 3 and 6,\nnoting that the fourth occurs when the per-pattern transpo-\nsition detects the ﬁfth rather than the root as the strongest\nchroma bin, and vice-versa), and a major triads transposed\nto the root and ﬁfth (codewords 5 and 4, respectively).\nMany of the remaining codewords correspond to common\n0 2 4 6 8 10 12\nnumber of beats0.0100.0150.0200.0250.0300.0350.0400.0450.0500.055average errorEncoding error per number of beats (bar information ignored)\nFigure 3: Encoding patterns of different sizes with a ﬁxed size\ncodebook of 100 patterns. The size of the pattern is deﬁned by\nthe number of beats. Downbeat (bar alignment) information was\nnot used for this experiment.\n113\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Code 0 (1.28%)\n Code 1 (1.18%)\n Code 2 (1.07%)\n Code 3 (1.01%)\n Code 4 (0.97%)\nCode 5 (0.96%)\n Code 6 (0.93%)\n Code 7 (0.89%)\n Code 8 (0.87%)\n Code 9 (0.87%)\nCode 10 (0.85%)\n Code 11 (0.85%)\n Code 12 (0.84%)\n Code 13 (0.82%)\n Code 14 (0.81%)\nCode 15 (0.80%)\n Code 16 (0.79%)\n Code 17 (0.78%)\n Code 18 (0.78%)\n Code 19 (0.77%)\nCode 20 (0.77%)\n Code 21 (0.77%)\n Code 22 (0.76%)\n Code 23 (0.75%)\n Code 24 (0.74%)Figure 4: The 25 codes that are most commonly used for the\nartist20 test set. Codes are from the 200-entry codebook trained\non2bar, 12\u00028patches. The proportion of patches accounted\nfor by each pattern is shown in parentheses.\n0\n 50\n 100\n 150 2000.000\n0.002\n0.004\n0.006\n0.008\n0.010\n0.0120.014proportion of usefrequency on artist20\nfrequency on cowbell\nFigure 5: Usage proportions for all 200 codewords on the\nartist20 test set (which comprises 71;832patterns). Also shown\nare the usage proportions for the training set (“cowbell”), which\nare similar. Note that even though all codewords are initialized\nfrom samples, some are used only once in the training set, or not\nat all for test set. This explains why the curves drop to 0.\ntransitions from one chord to another, e.g. a V-I transition\nin codes 7 and 9 (e.g., Gmaj !Cmaj, or G5!C5 as a\nguitar power chord) and the reverse I-V transition in code\n21 (e.g., Cmaj!Gmaj).\nIn an effort to visualize the span of the entire codebook,\nwe used Locally linear embedding (LLE) [9]2to arrange\nthe codewords on a 2D plane while keeping similar pat-\nterns as neighbors. Figure 6 shows the resulting distribu-\ntion along with a sampling of patterns; notice sustained\nchords on the top left, chord changes on the bottom left,\nand more complex sustained chords and “wideband” noisy\npatterns grouping to the right of the ﬁgure.\nNoting that many of the codewords reﬂect sustained\npatterns with little temporal variation, Figure 7 plots the\naverage variance along time of all 200 patterns. Some 26%\nof the codewords have very low variance, corresponding to\nstationary patterns similar to the top row of Figure 4.\nWe made some preliminary experiments with codebooks\nbased on longer patches. Figure 8 presents a codewords\nfrom an 8 bar (32 beat) codebook. We show a random\nselection since all the most-common codewords were less\ninteresting, sustained patterns.\n2implementation: http://www.astro.washington.edu/\nusers/vanderplas/coding/LLE/\n0.15\n 0.10\n 0.05\n 0.00 0.05 0.10 0.150.15\n0.10\n0.05\n0.000.050.100.15\nFigure 6: LLE visualization of the codebook. Shown patterns\nare randomly selected from each neighborhood.\n0 50 100 150 200\ncodes0.000.020.040.060.080.10average varianceaverage variance of the codes in the codebook\nFigure 7: Average variance of codewords along the time dimen-\nsion. The vertical axis cuts at the 53rd pattern, roughly the num-\nber of codewords consisting entirely of sustained chords. Repre-\nsentative patterns are shown in each range.\n4.2 Within-cluster behavior\nIn addition to inspecting the codewords, it is important to\nunderstand the nature of the cluster of patterns represented\nby each codeword, i.e., how well the centroid describes\nthem, and the kind of detail that has been left out of the\ncodebook. Figure 9 shows a random selection of the 639\npatterns from the artist20 test set that were quantized to\ncodeword 7 from Figure 4, the V-I cadence. Figure 10 il-\nlustrates the difference between the actual patterns and the\nquantized codeword for the ﬁrst three patterns; although\nthere are substantial differences, they are largely unstruc-\nCode 0 (0.68%)\n Code 1 (0.68%)\nCode 2 (1.01%)\n Code 3 (5.41%)\nCode 4 (1.01%)\n Code 5 (3.72%)\nFigure 8: Sample of longer codewords spanning 8bars. Code-\nwords were randomly selected from a 100-entry codebook. Per-\ncentages of use are shown in parentheses. Most patterns consist\nof sustained notes or chords, but code 0 shows one-bar alterna-\ntions between two chords, and code 4 contains two cycles of a\n1!1!1!2 progression.\n114\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Figure 9: Cluster around centroid presented in Figure 1. Taken\nfrom the artist20 dataset, the cluster size is actually 639. Shown\nsamples were randomly selected. This gives an intuition of the\nvariance in a given cluster.\nFigure 10: First three patterns of ﬁgure 9 (2nd line) presented\nwith the centroid from Figure 1 (1st line) and the absolute differ-\nence between both (3rd line).\ntured, indicating that the codebook has captured the impor-\ntant underlying trend.\n4.3 Example song encoding\nFigure 11 gives an example of encoding a song using the\ncodebook, showing both the full, original data, and the re-\nconstruction using only the quantized codewords (at the\ncorrect transpositions). The quantized representation re-\ntains the essential harmonic structure of the original fea-\ntures, but has smoothed away much of the detail below the\nlevel of the 2 bar codewords.\n5. APPLICATIONS\nWe present two applications of the beat-chroma codebooks\nto illustrate how the “natural” structure identiﬁed via un-\nsupervised clustering can provide useful features for sub-\nsequent supervised tasks. We will discuss how the code-\nwords can be used in bar alignment, and artist recognition.\n5.1 Bar Alignment\nSince the clustering described in Section 2 is based on the\nsegmentation of the signal in to bars, the codewords should\n0 50 100 150 2000246810\n0 50 100 150 2000246810Figure 11: Good Day Sunshine by The Beatles. Original song\nand encoding with a 200entry codebook of 2bar patterns.\nOffset % of times chosen\n0 62:6\n1 16:5\n2 9:4\n3 11:5\nTable 2: Bar alignment experiment: offsets relative to ground-\ntruth 4-beat bar boundaries that gave minimum distortion encod-\nings from the bar-aligned codebook.\ncontain information related to bar alignment, such as the\npresence of a strong beat on the ﬁrst beat. In this section\nwe investigate using the codebook to identify the bar seg-\nmentation of new songs. We train a codebook of size 100\non bars resampled to 4beats. Then, we take the longest\nsequence of bars of 4beats for each song in the test set\n(to avoid the alignment skew that results from spanning\nirregularly-sized bars). We then encode each of these se-\nquences using an offset of 0,1,2or3beats, and record\nfor each song the offset giving the lowest distortion. The\nresults in Table 2 show that the “true” offset of 0beats\nis chosen in 62% of cases (where a random guess would\nyield 25%). Thus, the codebook is useful for identifying\nbar (downbeat) alignments. A more ﬂexible implementa-\ntion of this idea would use dynamic programming to align\nbar-length patterns to the entire piece, including the pos-\nsibility of 3- or 5-beat bars (as are sometimes needed to\naccommodate beat tracking errors) with an appropriate as-\nsociated penalty.\n5.2 Artist Recognition\nWe apply our codebook to a simple artist recognition task.\nWe use the artist20 data set, composed of 1402 songs from\n20artists, mostly rock and pop of different subgenres. Pre-\nviously published results using GMMs on MFCC features\nachieve an accuracy of 59%, whereas using only chroma\nas a representation yields an accuracy of 33% [6].\nAlthough beat-chroma quantization naturally discards\ninformation that could be useful in artist classiﬁcation, it\nis interesting to investigate whether some artist use certain\npatterns more often than others.\nThe dataset is encoded as histograms of the codewords\nused for each song, with frequency values normalized by\nthe number of patterns in the song. We test each song in a\n115\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)ae\nbe\ncr\ncu\nda\nde\nfl\nga\ngr\nle\nma\nme\npr\nqu\nra\nro\nst\nsu\nto\nu2\nRECOG\naerosmith\nbeatles\ncreedence_clearwater_revival\ncure\ndave_matthews_band\ndepeche_mode\nfleetwood_mac\ngarth_brooks\ngreen_day\nled_zeppelin\nmadonna\nmetallica\nprince\nqueen\nradiohead\nroxette\nsteely_dan\nsuzanne_vega\ntori_amos\nu2TRUEconfusion matrix (real/predicted)\n0\n4\n8\n12\n16\n20\n24\n28\n32\n3640Figure 12: Confusion matrix for the artist recognition task.\n0\n 1\n 2\n 3\n4\n 5\n 6\n 7\n0\n2\n4\n6\n8\n10\n0.32\n0.40\n0.48\n0.56\n0.64\n0.72\n0.80\n(a)Metallica\n0\n 1\n 2\n 3\n4\n 5\n 6\n 7\n0\n2\n4\n6\n8\n10\n0.16\n0.24\n0.32\n0.40\n0.48\n0.56\n0.64\n0.72 (b) Tori Amos /\nSuzanne Vega\nFigure 13: Typical patterns for different artists.\nleave-one-out setting, and represent each of the 20artists\nby the average of their (remaining) song-histograms. The\ntest song is matched to an artist based on minimum Eu-\nclidean distance to these per-artist averages. This gives an\naccuracy of 23:4%, where the random baseline is around\n5%. The confusion matrix can be seen in Figure 12, show-\ning that certain artists are recognized at an accuracy far\nabove the average.\nIt is interesting to inspect the most “discriminative” pat-\nterns for individual artists. To ﬁnd these patterns, we com-\npare a pattern’s use by one artist and divide by its use\nacross all artists. Figure 13 shows the dominant patterns\nfor Metallica, and for Tori Amos and Suzanne Vega (who\nshared a ‘favorite’ pattern). These three artists were eas-\nily identiﬁed. Artists like Metallica are characterized by\n“wideband” patterns, with energy spread across multiple\nadjacent chroma bins, indicative of noise-like transients in\nthe audio.\n6. CONCLUSION AND FUTURE WORK\nWe have presented a practical method to perform large-\nscale clustering of tonal patterns, and assessed the basic\nproperties of the method through experiments on a large\ncollection of music. We have explored several ways to in-\nspect and interpret the data and suggested the merit of the\nrepresentation through further experiments. We have dis-\ncussed the possibility to move to even larger scales and we\nprovide our source code for other interested researchers3.\nFuture work may include more sophisticated clustering\nthat moves beyond simple Euclidean-distance-based quan-\n3SeePapers section at www.columbia.edu/ ˜tb2332/tization, perhaps by separately modeling the spread within\neach cluster (i.e., a Gaussian mixture or other generative\nmodel). Summarizing patches with Gaussians, and then\ncomparing the distance between those Gaussians, could re-\nduce the inﬂuence of the noise in the distance measure.\nMoving on to larger scales, we would like to pursue a\nscheme of incrementally splitting and merging codewords\nin response to a continuous, online stream of features, to\ncreate an increasingly-detailed, dynamic model. We could\nalso cluster codebooks themselves, in a fashion similar to\nhierarchical Gaussian mixtures [10].\n7. ACKNOWLEDGEMENTS\nThanks to Graham Grindlay for numerous discussions and\nhelpful comments. T. Bertin-Mahieux is supported by a\nNSERC scholarship. This material is based upon work\nsupported by IMLS grant LG-06-08-0073-08 and by NSF\ngrant IIS-0713334. Any opinions, ﬁndings and conclu-\nsions or recommendations expressed in this material are\nthose of the authors and do not necessarily reﬂect the views\nof the sponsors.\n8. REFERENCES\n[1] M. A. Bartsch and G. H. Wakeﬁeld. To catch a chorus: us-\ning chroma-based representations for audio thumbnailing. In\nProc. IEEE Workshop on Applications of Signal Processing\nto Audio and Acoustics, Mohonk, New York, October 2001.\n[2] M. Casey, C. Rhodes, and M. Slaney. Analysis of minimum\ndistances in high-dimensional musical spaces. IEEE Transac-\ntions on Audio, Speech & Language Processing, 2008.\n[3] M. Casey and M. Slaney. Fast recognition of remixed mu-\nsic audio. In Proceedings of the International Conference on\nAcoustics, Speech and Signal Processing (ICASSP), 2007.\n[4] The Echo Nest Analyze, API, http://developer.\nechonest.com.\n[5] D. Ellis, A. Berenzweig and B. Whitman. The ”uspop2002”\npop music data set. http://labrosa.ee.columbia.\nedu/projects/musicsim/uspop2002.html.\n[6] D. Ellis. Classifying music audio with timbral and chroma\nfeatures. In Proceedings of the 8th International Conference\non Music Information Retrieval (ISMIR), 2007.\n[7] D. Ellis and G. Poliner. Identifying cover songs with chroma\nfeatures and dynamic programming beat tracking. In Pro-\nceedings of the International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP), 2007.\n[8] A. Gersho and R. Gray. Vector quantization and signal com-\npression. Kluwer Academic Publishers, 1991.\n[9] S.T. Roweis and L.K. Saul. Nonlinear dimensionality reduc-\ntion by locally linear embedding. Science, 290(5500):2323–\n2326, 2000.\n[10] N. Vasconcelos. Image indexing with mixture hierarchies. In\nProceedings of the IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), pages I–3–I–10 vol.1, 2001.\n116\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "Decomposition Into Autonomous and Comparable Blocks: A Structural Description of Music Pieces.",
        "author": [
            "Frédéric Bimbot",
            "Olivier Le Blouch",
            "Gabriel Sargent",
            "Emmanuel Vincent 0001"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1414734",
        "url": "https://doi.org/10.5281/zenodo.1414734",
        "ee": "https://zenodo.org/records/1414734/files/BimbotBSV10.pdf",
        "abstract": "The structure of a music piece is a concept which is often referred to in various areas of music sciences and technolo- gies, but for which there is no commonly agreed definition. This raises a methodological issue in MIR, when designing and evaluating automatic structure inference algorithms. It also strongly limits the possibility to produce consistent large-scale annotation datasets in a cooperative manner. This article proposes an approach called decomposition into autonomous and comparable blocks, based on principles inspired from structuralism and generativism. It specifies a methodology for producing music structure annotation by human listeners based on simple criteria and resorting solely to the listening experience of the annotator. We show on a development set that the proposed approach can provide a reasonable level of concordance across anno- tators and we introduce a set of annotations on the RWC da- tabase, intended to be released to the MIR community.",
        "zenodo_id": 1414734,
        "dblp_key": "conf/ismir/BimbotBSV10",
        "keywords": [
            "Music piece structure",
            "commonly agreed definition",
            "MIR",
            "automatic structure inference",
            "large-scale annotation datasets",
            "cooperative manner",
            "decomposition into autonomous and comparable blocks",
            "structuralism",
            "generativism",
            "human listeners"
        ],
        "content": "DECOMPOSITION INTO AUTONOMOUS AND COMPARABLE BLOCKS : \nA STRUCTURAL DESCRIPTION OF MUSIC PIECES \n \nFrédéric BIMBOT 1 \nfrederic.bimbot@irisa.fr \n Olivier LE BLOUCH 2 \nolivier.le_blouch@inria.fr Gabriel SARGENT 3 \ngabriel.sargent@irisa.fr \n Emmanuel VINCENT 2 \nemmanuel.vincent@inria.fr \nMETISS - Speech and Audio Research Group \n(1) IRISA, CNRS - UMR 6074 - (2) INRIA, Rennes Breta gne Atlantique – (3) Université de Rennes 1 \nCampus Universitaire de Beaulieu, 35042 RENNES cedex, France \n \nABSTRACT \nThe structure of a music piece is a concept which is often \nreferred to in various areas of music sciences and technolo-gies, but for which there is no commonly agreed definition. This raises a methodological issue in MIR, when designing and evaluating automatic struct ure inference algorithms. It \nalso strongly limits the possi bility to produce consistent \nlarge-scale annotation datasets in a cooperative manner.  \nThis article proposes an approach called decomposition into \nautonomous and comparable blocks , based on principles \ninspired from structuralism and generativism. It specifies a \nmethodology for producing music structure annotation by \nhuman listeners based on simple criteria and resorting solely to the listening experience of the annotator. \nWe show on a development set that the proposed approach \ncan provide a reasonable level of concordance across anno-tators and we introduce a set of annotations on the RWC da-tabase, intended to be released to the MIR community. \n1. INTRODUCTION \n1.1 Motivations \nThe automatic inference of musi cal structure is a research \narea of growing interest [1-6], which is illustrated for in-stance by the creation in 2009 of a task called structural segmentation  in the MIREX community [7], or the existence \nof a specific research topic called music structuring and \nsummarization  in the QUAERO project (started 2008) [8]. \nInference of musical structure has multiple applications, such as fast browsing of musi cal contents, automatic music \nsummarization, chorus detection, unsupervised mash-ups, music thumb-nailing, etc… but also, more fundamentally, it offers great potential for improving the acoustic and musi-cological modeling of a piece of music with the help of structural information such as  the relative position of events \nwithin structural elements or  the exploitation of recurring \nsimilarities across them. \nMusical structure deals with th e description of the formal \norganization of music pieces. Ho wever, several conceptions of musical structure coexist and there is no widely accepted \ndefinition. This raises a methodological issue when the \nquestion arises of evaluating and comparing automatic algo-rithms on a common “ground-truth” (see, in particular [9]) \nThis article presents an attempt to provide an operational \ndefinition and to specify an annotation procedure for pro-ducing a structural de scription of music pieces that can be \nobtained quasi-univocally and in a reproducible way by sev-eral human annotators. \nThe concepts and the methodology proposed in this article \nare intended to be applied to what we will call conventional \nmusic , which covers a large proportion of current western \npopular music and also a large subset of classical music. However, we keep in mind that some other types of music (in particular, contemporaneous music) are much less suited to the proposed approach. \n1.2 Objectives \nThe concepts and methodology presented in this work aim \nat specifying a process for annotating musical structure, with the following requirements : \n• based on the listening experience  of the annotator (and \nnot on his/her musicological expertise ) \n• unrelated  to any particular algorithm or application \n• independent  of any musical role assigned to structural \nelements \n• reproducible  across annotators  \n• applicable  to a wide variety of music genres  \nAt the current stage of our work, we have focused on the issue of locating structural el ements (i.e. segmentation) and \nwe postpone to a later step the question of how to label these elements. \nWe first present, in section 2, the fundamentals of our ap-\nproach, then we describe, in section 3, the proposed annota-tion process. We provide in section 4, an evaluation of the consistency of the methodology on a development set and we introduce to the MIR community a set of annotations on the RWC [10] Pop data set, based on the proposed ap-proach. \n \n \nPermission to make digital or hard copies  of all or part of this work for per-\nsonal or classroom use is granted w ithout fee provided that copies are no t \nmade or distributed for profit or comme rcial advantage and that copies bea r \nthis notice and the full citation on the first page.  \n© 2010 International Society for Music Information Retrieval  \n189\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \n 1.3 Preliminary definitions \nIn the rest of this paper, we consider that a piece of music is \ncharacterized by 3 reference properties , which may be con-\nstant or vary over time : \n• tonality/modality (reference key and scale) \n• tempo (speed / pace of the piece) \n• timbre (instrumentation / audio texture) \nWe also consider that a piece of music shows 4 levels of \ntemporal organization : \n• rhythm (relative duration and accentuation of notes) \n• harmony (chord progression) \n• melody (pitch intervals between successive notes) \n• lyrics (linguistic content) \nThese levels of description form 7 musical layers which we \nconsider independently. \n2. FUNDAMENTALS \n2.1 Framework \nThe proposed approach relies on concepts inspired from structuralism , a school of thoughts in itiated by Ferdinand \nde Saussure in the field of Linguistics [11] and later ex-tended to other domains, in particular to some areas of mu-sic semiotics. Our approach  also borrows ideas from  gener-\native theory as explored  by Lerdahl and Jackendoff [12]. \nIn this context, we consider a music piece as the layout of a number of constitutive elements governed by a specific as-sembling process, called syntagmatic process . The constitu-\ntive elements are related to one another through paradig-\nmatic relationships  which manifest themselves as a set of \nequivalence classes (i.e. two elements belong to the same subset if and only if the relation holds between them). The entire scheme forms a system  in the structuralist sense, \nnamely an “entity of internal  dependencies”, according to \nHjelmslev’s definition [13]. \nThe piece of music thus appear s as a particular observation \nproduced by this system and the problem of musical struc-\nture inference consists in determining the constitutive ele-ments of the piece (i.e. segmentation  task or, more general-\nly speaking, decomposition ) and to assign equivalence \nclasses to each of them ( labeling  or tagging task). \nAs a consequence, specifying a type of musical structure \nrequires the definition of : \n1. the nature and properties of the constitutive elements \n2. the assembling process used to combine them \n3. the equivalence relation(s) that  are referred to, so as to \nrelate them to one another. \n2.2 Worki ng assumptions \nIn\n the present work, the constitutive elements are assumed \nto be common to the 4 levels of temporal organization. They are limited in time and are assembled mainly by concatena-tion. They are called blocks . A block is defined as an autonomous  segment (see 3.2). It is \nspecified by a starting instant , a duration and a size  (the \nconcept of size is also detailed in 3.2). A block can be de-composed into a stem  (which is itself autonomous) and one \nor several affixes. \nSeveral equivalence relations can be considered and com-\nbined to qualify comparability between blocks, in particular: isometry (same size), interchangeability (possibility to swap), similarity in one or several layers, etc… \nThus, we approach music structure description as the \nprocess of decomposing the music piece into autonomous and comparable blocks. As a consequence, we elude the various hierarchical levels of music structure and focus on the segmental macro-organization of music pieces. \nBlocks share similarities with musical phrases but are not \nstrictly identifiable to them. The concept of blocks also has some connections with the notion of periode  introduced by \nRiemann (see [14]) and that of grouping structure  as devel-\noped in [12]. \n3. SPECIFICATIONS \nIn this section, we introduce a number of criteria which are used to specify as univocally as possible the structural de-\ncomposition of a music piece. We attempt to formulate these criteria without resorting to absolute acoustic properties of the musical segments nor to th eir musical role in the piece \n(chorus, verse, etc…), so as to remain as independent as possible from musical genre. \n3.1 Musical consistency w.r.t. simple transformations \nWe base the decomposition process on the assumption that \nan annotator is able to decide on the musical consistency  of \na passage resulting from a local transformation of the music piece. More specifically, the listener is supposed to be able to judge if simple operations such as the suppression, inser-tion, substitution or repetition of a given musical segment within the music piece creates  (or not) a morphological sin-\ngularity or a syntagmatic disruption with respect to the orig-inal piece. \nThis approach assumes that the structural organization of \nthe music piece is governed by  underlying processes which \nthe listener is able to infer (even though he/she may not be able to formulate them) and which he/she can refer to, to decide on the musical consistency of the transformed piece. In particular, the listener will be  strongly inclined to consid-\ner that musical consistency is preserved by a transformation when a similar passage is found somewhere else in the piece, or could be  found without creating any feeling of he-\nterogeneity with the rest of the piece. \nClearly, this definition is partly subjective but we believe \nthat it provides non-expert human listeners with an opera-tional criterion that requires from them some familiarity with the genre of the music piece but does not demand a sharp expertise in musicology. Note that, in the same way, a human listener is generally able to tell whether a sentence in his/her native language is grammatical or not, even if he is not a linguist. \n190\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \n 3.2 Properties of blocks \nA block is defined as a musical segment which is autonom-\nous, i.e. which fulfils one of the two following properties : \neither it is independent  (i.e. it is perceived as self-sufficient \nwhen played on its own) or it is iterable (it can be looped \nand the result is musically consistent). \nMoreover, blocks within a musical piece have the property \nof being suppressible , i.e. they can be removed from the \npiece without altering its musical consistency. This test is used to identify the most likely block boundaries. However, suppressibility is a necessary but not a sufficient condition to qualify a block. \nIt is also worth noting that blocks are not necessarily homo-\ngeneous : reference properties may evolve within a block \n(change of tonality, tempo modifications, appear-ance/disappearance of instruments or voice). \nThe size of a musical block is expressed as the number of \ntimes a listener would snap his fingers to accompany the \nmusic, at a rate which is as close as possible to 1 bps (beat \nper second). Conventionally, block boundaries are synchro-\nnized with the first beat of a bar. Occasionally, unusual situ-ations may arise, such as blocks having a fractional size or for which the listener is unable to decide what the size is.  \nBlocks can contain affixes, i.e. portions which can be sup-\npressed, yielding a reduced block which remains musically \nconsistent with the rest of the piece. The various types of affixes are : prefixes, suffixes  and infixes  (the latter can be \nnon-connex). A block can therefore be described as a stem  \ncombined with zero, one or several affixes. In general, af-fixes are short and not autonomous. \n3.3 Equivalence relations and comparability \nSeveral paradigmatic relations hips between blocks can be \nconsidered : \n• isometry  : blocks of the same size (absolute isometry) or \nblocks reducible to stems of the same size (stem isome-\ntry). \n• interchangeability : blocks that can be swapped within a music piece without altering its musical consistency. \n• similarity  : blocks identical across some of their musical \nlayers (over the whole blocks or their stems only). \n• isomorphy : blocks that can be obtained from each other \nby a transformation of their reference properties. \nAs mentioned earlier, these equivalence relations are re-sorted to in order to determine on what basis blocks are judged comparable with one another. \n3.4 Structural pulsation and regularity \nTo specify further the decomposition into blocks, it is hy-\npothesized that the structure of (most) music pieces is rather \ncyclic and therefore follows some form of regularity charac-terized by a small set of distinct block sizes. We thus suppose that the music piece is built upon structur-\nal pulsation periods which take, in order of preference : \n• One value (type I)  \n• Two values (type II)  \n• A li\nmited set of values observed in a quasi-regular se-\nquence, called structural pattern  (type III)  \n• A limited set of values, showing no structural pattern (type IV)  \n• A large variety of distinct values (type V)  \nWe also designate as type 0  (or undeterminable) a piece for \nwhich it turns out to be impossible (for the listener) to locate clear boundaries of autonomous segments. Blocks whose size complies with one value of the structural pulsation pe-riods are called regular  blocks. \nThe regularity property induces decompositions which tend to yield comparable blocks within a given piece. \nFigure 1 depicts a block-wise structural decomposition in \nthe case of a type I  structure (top) and illustrates several \nconfigurations of non-regular blocks (bottom) and their cor-responding notations : \n• Truncated block : block resulting from the suppression of a fragment inside a regular block \n• Extended block : block obtained by the insertion of an affix into a regular block  \n• Bridging block : irregular block, usually of a smaller size, and which is often isolated at the beginning of the piece, at the end or in-b etween regular parts. \n• Tiling : partially overlapping bl ocks (on all levels of or-\nganization simultaneously), as is the case for instance in canon singing or fugue-like compositions. \n3.5 PIC minimization and target duration \nThe various properties introduced in the previous para-graphs still do not necessarily elicit a unique structural de-\ncomposition. Several possibilities may remain, generally based on structural pulsation periods which are multiples or sub-multiples of each other. \nThese situations can be deci ded between by specifying a \ntarget duration for regular blocks, the value of which we de-\nrive from the minimization of the predominant informative context . \nFigure 2 illustrates a structural decomposition as a paradig-matic representation showing the correspondence between homologous parts across distinct blocks. \nIf this decomposition is exploited to predict the musical \nproperties of the music piece on a short time interval, the most relevant portions of the piece for this purpose are, on the one hand, the neighboring portions belonging to the same block (horizontally) and the homologous portions across the other blocks (vertically). \nThis is what we call the Predominant Informative Context \n(or PIC). It is distinct for each small portion of the music piece and it is solely determined by the structure. It consti-\n191\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \n \n \nFigure 1 :  Decomposition into structural blocks (syntagmatic point of view) and the corresponding notation of block sizes. \n \n \nFigure 2 :  Paradigmatic representation of structural blocks and visualisation of the PIC. \nAny small portion (in black) presents privileged internal depende ncies with the gray parts in the piece, namely other portions \nwithin the same structural block (horizontally) and homologous portions across other blocks (vertically)  \n \n \n \n \n \n \n \n \n \n \nFigure 3 :  Example of block-wise decompositions obtained on 6 s ongs from the RWC Pop set (d isplayed with Wavesurfer). \n192\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \n tutes the predominant source of information within the enti-\nty of internal dependencies mentioned in section 2.1. \nIf the total length of the music piece is equal to N and if the \ntypical block length is equal to n, then, the number of blocks \nin the piece is in the order of N/n and the total coverage of \nthe PIC is given by : \n                  C =  n + (N/n) – 2             (1)  \nwhich is minimal when N n = . \nTaking the second as a unit, and on the basis of music piec-\nes with a typical length of 4 minutes (i.e. N = 240 seconds), \nthe value of n which minimizes C is approximately equal \n15.5 seconds. \nIn the present work, we retain the value of 15 s as the  target \nduration for blocks, which leads to the following additional \ncriterion : at least one of the structural pulsation periods must have a duration as close as possible to 15 s, on a loga-rithmic scale. This criterion tends to provide structural de-compositions which result from a balanced contribution of the paradigmatic and syntagmatic axes. \nMore generally speaking a relative weight λ can be applied \nto the two terms in equation (1), leading to a PIC func-\ntion which writes : \n                           C( λ) = n + λ (N/n) – (λ+1)            (2) \nand whose minimization (in n) induces decompositions \nbased on an adjustable balan ce between the syntagmatic and \nthe paradigmatic axes. The constraint resulting from a target duration criterion \nenables the disambiguation of situations such as several identical medium-size segments in sequence and it provides blocks which are more adapted to comparisons  across mu-sic pieces. \n3.6 Subsidiary criteria \nIn some residual situations, several competing decomposi-\ntions may locally be compatible with a given structural pul-sation period while fulfilling satisfactorily all other criteria. For instance, sequences of an odd number of suppressible segments which have a size equal to half of the structural pulsation period. \nIn these circumstances, the following subsidiary criteria  are \nconsidered : \n- group preferably in a same block short neighboring \nsegments of this passage which are most similar \n- favor decompositions that yield the largest possible number of blocks which are interchangeable with other blocks outside this passage. \n3.7 Procedure (summary) \nThe box hereafter summarizes the annotation procedure re-sulting from the criteria presented above. \nOnce the process finalized, the annotator fills up a short re-\nport summing up the type of structure, the degree of diffi-culty, the level of confidence and any relevant information pertaining to the annot ation of that piece. 1) Identify a plausible value n (or set of values { n\ni}) for \nthe structural pulsation period(s), from the parts of the \npiece which are structured  with strong regularity. \nChoose in priority values as close as possible to the \ntarget duration. \n2) Locate suppressible blocks of size n, whether they are \nregular or can be derived straightforwardly from regu-\nlar blocks. Also search for tiling at this stage. \n3) Continue the decomposition by resorting to less regu-\nlar (suppressible) blocks considering in priority block \nsizes that are sub-multiple of n. \n4) Assess the regularity of the decomposition, and find \nout to which type (0, I, II, III, IV or V) the decomposi-\ntion tends to belong. The local structure around the be-\nginning and the end of th e piece should be given a \nlower importance and so should it be for affixes. \n5) Consider other options for the value(s) of the structural \npulsation period(s) and find out whether they would \nlead to a simpler decomposition. \n6) Refine the decomposition by resolving remaining am-\nbiguities with the help of the subsidiary criteria.  \nFigure 3 provides example of block-wise decompositions obtained on 6 songs from the Pop set of the RWC database. \n4. EVALUATION AND DISSEMINATION \n4.1 Evaluation protocol \nIn order to validate the annotation procedure proposed in this paper, we have measured the concordance between sev-eral annotators on a same annotation task. \nFour annotators are provided with a development set of 20 \nsongs in their audio version, the list of which was deter-mined by IRCAM, in the context of task 6.5 of the QUAERO Project (Table 2). \nThe concordance between annotators is evaluated by taking \nthem pair-wise and computing for each piece the F-measure between their annotations (with a tolerance of ± 0.75 s be-tween segment boundaries) and averaging the F-measure over all 20 pieces. \nAmong the four annotators, none is a musicologist nor a \nprofessional musician. However, it is important to mention that they are the 4 co-authors of this paper, which may in-duce some methodological bias, which needs to be taken into account in interpreting the experimental results. \nAnnotator N°1 N°2 N°3 N°4 \nN°1 - 88.9 95.7 92.9 \nN°2 88.9 - 88.7 88.7 \nN°3 95.7 88.7 - 92.8 \nN°4 92.9 88.7 92.8 - \nTable 1 :  Pre-final concordance between annotators evaluated as \nthe F-measure (%) between annotati ons averaged over 20 pieces of \nmusic. The mean value is 91.3 %.  \nScores presented in Table 1 correspond to what we call pre-final concordance between annotators, i.e. results of a round of annotation carried out after the annotation procedure was \n193\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \n specified in its main lines, but before the subsidiary criteria  \nof section 3.6 were introduced.  \nFigure 4 details the distribution of concordance scores \nacross pieces. The median score is 95.8 % and 2 pieces are responsible for almost 4% absolute error rate. \nThe subsidiary criteria were added in a last stage to resolve \nmost of the residual ambigu ities and a consensual annota-\ntion was produced for 19 of the 20 pieces, while the 20\nth \npiece (#11 in Table 2) was consid ered as type 0 (i.e. impos-\nsibility to define re liable block boundaries). \n01 Pink Floyd Brain Damage \n02 Queen Lazing On A S unday Afternoon\n03 DJ Cam Mad Blunted Jazz \n04 Outkast Return Of The G \n05 ACDC You Shook Me All Night Long\n06 Eric Clapton Old Love \n07 Stan Getz & J. Gilberto O Pato\n08 Enya Caribbean Blue \n09 Mickael Jackson Off The Wall \n10 Bass America Collection Planet\n11 Plastikman Fuk \n12 Shack Natalies Party \n13 Sean Kingston Take You There \n14 Lil Mama Shawty Get Loose \n15 Abba Waterloo \n16 Eiffel 65 Blue (Da Ba Dee) \n17 Meat Loaf I’d Do Anything For You \n18 Kaoma Lambada \n19 Vangelis Conquest Of Paradise \n20 Nirvana Smells Like Teen Spirit \nTable 2 :  List of music pieces used in the development set  \n4.2 Dissemination \nIn a later phase, we annotated the “Pop” subset (100 songs) of the RWC database [10], with the goal to make the result available to the MIR community via MIREX, and on : \nhttp://musicdata.gforge.inria.fr \nOut of the 100 songs, 77 appear to be of type I, 20 of type II and 3 of type III (none of the other types). The average number of blocks per song is 15.54 (minimum 9, maximum 22). A vast majority (67 %) of blocks are regular, 15 % are derivations of regular blocks and 18 % are irregular. The average duration of regular blocks is 17.11 seconds. Table 3 details the distribution of sizes across blocks. \n5. CONCLUSIONS \nIn this paper, we have specified and described thoroughly a consistent procedure for the description and the manual an-notation of music structure, intended to be usable by non-expert human listeners, and which does not resort to abso-lute acoustic properties, nor to the analysis of the musical role of the constitutive elemen ts. We hope that the proposed \nmethodology will be experiment ed and refined by other \ngroups and its usability widely established.  We are currently working on the definition of a procedure based on a multi-dimensional analysis, for assigning labels to the structural blocks, accounting for internal similarities and contrasts within the music piece. \nSize Raw (A) Stems (B) Regular (C) \n4 1.6 % 1.6 % 0.0 % \n8 10.5 % 9.7 % 3.6 % \n12 2.6 % 2.4 % 1.1 % \n14 1.1 % 0.1 % 0.0 % \n16 59.2 % 72.3 % 87.3 % \n18 5.6 % 0.1 % 0.0 % \n20 4.9 % 1.4 % 1.4 % \n24 1.8 % 1.3 % 1.5 % \n32 2.2 % 2.7 % 3.6 % \nOther 10.5 % 8.4 % 1.5 % \nTable 3 : Distribution of block sizes (in snaps) for raw blocks (A), \nstems (B) and regular blocks only (C). RWC – Pop database.  \n6. REFERENCES \n[1] Peeters, G. “Deriving Musical Structures from Signal Analysis \nfor Music Audio Summary Genera tion: Sequence and State Ap-\nproach”, in Lecture Notes in Computer  Science, Springer-Verlag, \n2004. \n[2] Abdallah, S. Noland, K., Sandl er, M., Casey, M., and Rhodes, \nC. “Theory and evaluation of a Bayesian music structure extrac-\ntor”, in Proc. ISMIR , London, UK, 2005. \n[3] Goto, M. “A Chorus Secti on Detection Method for Musical \nAudio Signals and Its Application to a Music Listening Station”, \nIEEE Trans. on Audio, Speech, and Language Processing , 2006. \n[4] Paulus, J. and Klapuri, A. “Music structure analysis by finding repeated parts”, in Proc. AMCMM , Santa Barbara, California, \nUSA, 2006. \n[5] Peeters, G. “Sequence Representation of Music Structure Using \nHigher-Order Similarity Matr ix and Maximum-Likelihood Ap-\nproach”, in Proc. ISMIR , Vienna, Austria, 2007. \n[6] Levy, M., Sandler, M. “Structural Segmentation of musical au-dio by constrained clustering”, IEEE Transactions  on Audio, \nSpeech and Language Processing , 2008. \n[7] MIREX 2009 : http://www.music-ir.org/mirex/2009\n \n[8] QUAERO Project :  http://www.quaero.org  \n[9] Geoffroy Peeters and Emmanuel Deruty : Is Music Structure Annotation Multi-Dimensional ? A Proposal For Robust Local Music Annotation. LSAS, Graz (Austria) 2009. \n[10] RWC : http://staff.aist.go.jp/m.goto/RWC-MDB\n \n[11] F. de Saussure : Cours de Linguistique Générale. 1916. \n[12] Fred Lerdahl & Ray Jackendoff : A Generative Theory of Tonal \nMusic, MIT Press, 1983, reprinted 1996. \n[13] Louis Hjelmslev : Essays  in Linguistics (1959). [14] Ian Bent with William Drabkin : Analysis. The New Grove \nHandbooks in Music. Macmillan P ublishers Ltd, London, 1987, \nreprinted 1998. pp. 90-93. \nFigure 4 : Inter-annotator concordance sorted in  descending order for the 20 musi c pieces in the development set. \n  \n194\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "Music Genre Classification via Compressive Sampling.",
        "author": [
            "Kaichun K. Chang",
            "Jyh-Shing Roger Jang",
            "Costas S. Iliopoulos"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1418289",
        "url": "https://doi.org/10.5281/zenodo.1418289",
        "ee": "https://zenodo.org/records/1418289/files/ChangJI10.pdf",
        "abstract": "Compressive sampling (CS) is a new research topic in signal processing that has piqued the interest of a wide range of researchers in different fields recently. In this pa- per, we present a CS-based classifier for music genre clas- sification, with two sets of features, including short-time and long-time features of audio music. The proposed clas- sifier generates a compact signature to achieve a significant reduction in the dimensionality of the audio music signals. The experimental results demonstrate that the computation time of the CS-based classifier is only about 20% of SVM on GTZAN dataset, with an accuracy of 92.7%. Several experiments were conducted in this study to illustrate the feasibility and robustness of the proposed methods as com- pared to other approaches.",
        "zenodo_id": 1418289,
        "dblp_key": "conf/ismir/ChangJI10",
        "keywords": [
            "Compressive sampling",
            "signal processing",
            "music genre classification",
            "audio music signals",
            "short-time features",
            "long-time features",
            "compact signature",
            "dimensionality reduction",
            "SVM",
            "GTZAN dataset"
        ],
        "content": "MUSIC GENRE CLASSIFICATION VIA COMPRESSIVE SAMPLING\nKaichun K. Chang\nDepartment of Computer Science\nKing’s College London\nLondon, United Kingdom\nken.chang@kcl.ac.ukJyh-Shing Roger Jang\nDepartment of Computer Science\nNational Tsing Hua University\nHsinchu, Taiwan\njang@cs.nthu.edu.twCostas S. Iliopoulos\nDepartment of Computer Science\nKing’s College London\nLondon, United Kingdom\ncsi@dcs.kcl.ac.uk\nABSTRACT\nCompressive sampling (CS) is a new research topic in\nsignal processing that has piqued the interest of a wide\nrange of researchers in different ﬁelds recently. In this pa-\nper, we present a CS-based classiﬁer for music genre clas-\nsiﬁcation, with two sets of features, including short-time\nand long-time features of audio music. The proposed clas-\nsiﬁer generates a compact signature to achieve a signiﬁcant\nreduction in the dimensionality of the audio music signals.\nThe experimental results demonstrate that the computation\ntime of the CS-based classiﬁer is only about 20% of SVM\non GTZAN dataset, with an accuracy of 92.7%. Several\nexperiments were conducted in this study to illustrate the\nfeasibility and robustness of the proposed methods as com-\npared to other approaches.\n1. INTRODUCTION\n1.1 Acoustic Features for Audio Music Analysis\nIn the literature of music information retrieval (MIR), var-\nious content-based features have been proposed [1] for ap-\nplications such as classiﬁcation, annotation, and retrieval\n[15]. These features can be categorized into two types, that\nis, short-time and long-time features. The short-time fea-\ntures are mainly based on spectrum-derived quantity within\na short segment (such as a frame). Typical examples in-\nclude spectral centroids, Mel-frequency cepstral coefﬁcients\n(MFCC) [1], and octave based spectral contrast (OSC) [2].\nIn contrast, the long-time features mainly characterize the\nvariation of spectral shape or beat information over a long\nsegment, such as Daubechies wavelet coefﬁcients histogram\n(DWCH) [3], octave-based modulation spectral contrast\n(OMSC), low-energy, beat histogram [1], and so on. Ac-\ncording to G. Tzanetakis et al. [1], the short and long seg-\nments are often referred to as “analysis window” and “tex-\nture window”, respectively.\nTheoretically, both short-time and long-time features\nshould be used together to realize efﬁcient and effective\nMIR system since they provide different information for\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2010 International Society for Music Information Retrieval.the task under consideration. However, in practice, too\nmany features usually degrade the performance since there\nmight be some noises instead of useful cues in the feature\nset. Moreover, too many features could also entail exces-\nsive computation to downgrade the system’s efﬁciency. As\na result, we need an effective method for feature selection,\nextraction, or distillation. CS turns out to be an effective\ntool for such a purpose.\n1.2 Compressive Sampling\nCS is ﬁrstly proposed by Cand \u0012es, Romberg, Tao and Donoho,\nwho have showed that a compressible signal can be pre-\ncisely reconstructed from only a small set of random linear\nmeasurements whose number is below the one demanded\nby the Shannon theorem Nyquist rate. It implies the po-\ntential of a dramatic reduction in sampling rates, power\nconsumption, and computation complexity in digital data\nacquisitions. CS has proved to be very effective in imag-\ning [6] [7], channel estimation [8], face recognition [9],\nphonetic classiﬁcation [18], sensor array [19] and motion\nestimation [20].\nIn this paper, we propose a CS-based classiﬁer with\nlong-time and short-time features for music genre classi-\nﬁcation. The remainder of this paper is organized as fol-\nlows. In section 2, the multiple feature sets used in the pro-\nposed method is brieﬂy discussed. In the section 3, we de-\nscribe multiple feature sets for audio music, and introduce\nthe corresponding CS-based classiﬁer. In section 4, exper-\nimental settings and results are detailed to demonstrate the\nproposed method’s feasibility. Finally, conclusions and fu-\nture work are addressed in the last section.\n2. MULTIPLE FEATURE SETS\nIn the proposed method, multiple feature sets including\nlong-time and short-time features are adopted for genre\nclassiﬁcation. These acoustic features include timbral tex-\nture features, octave-based spectral contrast (OSC), octave-\nbased modulation spectral contrast (OMSC), modulation\nspectral ﬂatness measure (MSFM), and modulation spec-\ntral crest measure (MSCM).\nTimbral texture features are frequently used in various\nmusic information retrieval system [11]. Some timbral tex-\nture features, described in Table 1, were proposed for au-\ndio classiﬁcation [1]. Among them, MFCC, spectral cen-\ntroid, spectral rolloff, spectral ﬂux, and zero crossings are\n387\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Table 1. Timbral texture features\nFeature Description\nMFCC Representation of the spectral char-\nacteristics based on Mel-frequency\nscaling [12]\nSpectral centroid The centroid of amplitude spectrum\nSpectral rolloff The frequency bin below which\n85% of the spectral distribution is\nconcentrated.\nSpectral ﬂux The squared difference of succes-\nsive amplitude spectrum.\nZero crossings The number of time domain zero\ncrossings of the music signal.\nLow-energy The percentage of analysis win-\ndows that have energy less than the\naverage energy across the texture\nwindow.\nshort-time features, thus their statistics are computed over\na texture window. The low-energy feature is a long-time\nfeature.\nBesides these features, OSC and OMSC features are\nalso considered. OSC considers the spectral peak, spec-\ntral valley, and spectral contrast in each subband [2]. The\nspectrum is ﬁrst divided into octave-based subband (as ex-\nplained next). Then spectral peaks and spectral valleys\nare estimated by averaging across the small neighborhood\naround maximum and minimum values of the amplitude\nspectrum respectively. OMSC [1] is extracted using long-\ntime modulation spectrum analysis [13].\nIn this paper, the amplitude spectrum of a music signal\nis divided into octave-based subbands of 0-100Hz, 100Hz-\n200Hz, 200Hz-400Hz, 400Hz-800Hz, 800Hz-1600Hz, 1600Hz-\n3200Hz, 3200Hz-8000Hz, 8000Hz-22050Hz. Within each\nsubband, the amplitude spectrum is summed. Then for\neach subband, the modulation spectrum is obtained by ap-\nplying the discrete Fourier transform (DFT) on the sequence\nof the sum of amplitude spectrum.\nOMSC is obtained from spectral peaks and spectral con-\ntrasts of the modulation spectrum. MSFM and MSCM\nare obtained from a texture window [4] using the long-\ntime modulation spectrum [13] that can describe the time-\nvarying behavior of the subband energy. These features are\nalso considered as parts of our multiple feature sets.\n3. COMPRESSIVE SAMPLING BASED\nCLASSIFIER\nAs inspired by CS and the sparse signal representation the-\nory, here we shall propose a CS-based classiﬁer for genre\nclassiﬁcation. First of all, we shall cover the basics of the\nCS theory [5].\nIn Figure 1, consider a signal x(lengthN) that isK-\nsparse in sparse basis matrix \t, and consider also an M\u0002\nNmeasurement basis matrix \b,M << N (Mis far less\nthanN), where the rows of \bare incoherent with the columns\nof\t. In term of matrix notation, we have x= \t\u0012, in\nFigure 1. The measurement of Compressive Sampling\nwhich\u0012can be approximated using only K << N non-\nzero entries. The CS theory states that such a signal xcan\nbe reconstructed by taking only M=O(KlogN)linear,\nnon-adaptive measurement as follows:\ny= \b\u0001x= \b\u0001\t\u0001\u0012=A\u0001\u0012; (1)\nwhereyrepresents an M\u00021sampled vector, A= \b\t\nis anM\u0002Nmatrix. The reconstruction is equivalent to\nﬁnding the signal’s sparse coefﬁcient vectors \u0012, which can\nbe cast into a `0optimization problem.\nmink\u0012k0s:t:y = \b\u0001x=A\u0001\u0012 (2)\nUnfortunately (2) is in general NP-hard, and an opti-\nmization`1is used to replace the above `0optimization\n[10].\nmink\u0012k1s:t:y = \b\u0001x=A\u0001\u0012 (3)\nLet the dimension of the extracted feature be denoted\nasm, and the extracted feature vector of the j-th music\nin thei-th class as \u0017i;j2Rm. Moreover, let us assume\nthere are sufﬁcient training samples for the i-th classAi=\n[\u0017i;1;:::\u0017i;n i]2Rm\u0002n i. Then any new (test) sample y2\nRm(i.e, the extracted feature of the test music) from the\nsame class will approximately lie in the linear span of the\ntraining samples associated with object i:\ny=niX\ni=1\u000bi;n i\u0017i;n i; (4)\nfor some scalars \u000bi;j(j= 1;::;n i). Since the member-\nshipi(or the label) of the test sample is initially unknown,\nwe deﬁne a new matrix Afor the entire training set as the\nconcatenation of the n training samples of all kclasses:\nA= [A 1;:::A k]Then the linear representation of ycan be\nrewritten in terms of all training samples as:\ny=Ax02R; (5)\nwherex0= [0;::; 0;\u000bi;1;:::;\u000b i;n;:::;0;::;0]T2Rnis a\ncoefﬁcient vector whose entries are zero except those as-\nsociated with the i-th class. As the entries of the vector\nx0encode the identity of the test sample y, it is tempting\nto obtain it by solving the equation (4). This is called a\nsparse representation based classiﬁer (SRC) [9].\n388\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)In SRC, for a new test sample yfrom one of the classes\nin the training set, we ﬁrst compute its sparse representa-\ntion^xvia (2). Ideally, the nonzero entries in the estimate ^x\nwill all be associated with the columns of Afrom a single\nobject class i, and we can easily assign the test sample y\nto that class. To better harness such linear structure, we\ninstead classify ybased on how well the coefﬁcients asso-\nciated with all training samples of each object reproduce\ny. For each class i, let\u000ei:Rn!Rnbe the characteris-\ntic function which selects the coefﬁcients associated with\nthei-th class[12]. For x2Rn,\u000ei(x)2Rnis a new vec-\ntor whose only nonzero entries are the entries in xthat are\nassociated with class i. Using only the coefﬁcients associ-\nated with the i-th class, one can approximate the given test\nsampleyas\n^yi=A\u000ei(^x) (6)\nWe then classiﬁed ybased on these approximations by\nassigning it to the object class that minimizes the residual\nbetweenyand^yi:\nminri(y) =kyi\u0000A\u000e(^x)k2(7)\nThe proposed CS-based classiﬁer is based on the prin-\nciple of SRC, with an additional random measurement on\nthe extracted features to reduce the dimension of the input.\nAccording to the CS theory, this reduction can capture the\nstructure of the features and automatically remove possi-\nble redundancy. The realization of the algorithm is sum-\nmarized in Table 2.\nIt should be noted that SRC is a sparse representation\nbased classiﬁer, without the dimension reduction over the\ninput signals. Here the random measurement of compres-\nsive sampling is used to perform a dimension reduction and\nfeature extraction. So the classiﬁcation complexity of CS\nbased method is remarkably lower than that of SRC. More-\nover, the multiple features will also improve the classiﬁ-\ncation accuracy. The sparse representation is one part of\ncompressive sampling. Taking the training samples ma-\ntrix as the transform matrix will be helpful to the clas-\nsiﬁcation. We will ﬁnd that the procedure of CS based\nclassiﬁer are very different from the classical methods, be-\ncause steps 3 to 6 are all based on compressive sampling.\nCurrently many non-linear dimensionality reduction meth-\nods have been proposed, such as Local Coordinates Align-\nment(LCA) and Non-Negative Matrix Factorization(NMF).\nCompressive sampling theory provides a random measure-\nment of signals, and proves to be able to keep the informa-\ntion of the signals under the condition of enough number of\nmeasurement and incoherence between the measurement\nmatrix and the transform matrix.\nConsequently, it is a natural compressive process of sig-\nnals, which can also be regarded as the process of dimen-\nsion reduction. CS is different from LCA and NMF due\nto that fact that it is a near method, which lend itself to\nefﬁcient implementation.Table 2. CS-based Classiﬁcation\nAlgorithm: CS-based Classiﬁcation\nStep 1:\nPerform a feature extraction on the music samples\nforkclasses.\nStep 2:\nPerform a feature extraction (described in section\n2) on the training songs to obtain a matrix of training\nsamples A= [A1;:::A k]and calculate the feature yof\nthe test sample.\nStep 3:\nPerform a random measurement (the measurement\nmatrix is a Gaussian random matrix) on the features of the\ntraining samples and the test sample feature to obtain\nA0=H\u0001Aandy0=H\u0001yrespectively.\nStep 4:\nNormalize the columns of A0to have unit `2norm\nand solve the `1-minimization problem:\nminkxk1s:t:y0=A0\u0001x\nStep 5:\nCompute the residuals\nminri(y0) =\r\ry0\u0000A0\u000ei(^x)\r\r\n2\nStep 6:\nOutput:identity (y) = arg min ri(y0)\nTable 3. Classiﬁcation accuracies achieved by various\nmethods on GTZAN dataset.\nMethod Dataset Accuracy Feature\ndimensions\nMF + CSC (Ours) GTZAN 92.7 64\nTPNTF + SRC GTZAN 93.7 135\nNTF + SRC GTZAN 92.0 135\nMPCA + SRC GTZAN 89.7 216\nGTDA + SRC GTZAN 92.1 216\n4. EXPERIMENTAL RESULTS\nThe experiments are divided into three parts. Section 4.1\ndetails our experiment with music genre classiﬁcation. Sec-\ntion 4.2 explores multiple features and dimension reduc-\ntion. Section 4.3 investigates the feature extractor in an\nnoisy environment.\n4.1 Music Genre Classiﬁcation\nOur experiments of music genre classiﬁcation are performed\non GTZAN dataset, which are widely used in the liter-\nature [16]. GTZAN consists of the following ten genre\nclasses: Classical, Country, Disco, Hip-Hop, Jazz, Rock,\nBlues, Reggae, Pop, and Metal. Each genre class contains\n100 audio recordings of 30 seconds, with sampling rate of\n44.1kHz and resolution of 16 bits.\nTo evaluate the proposed method for genre classiﬁca-\ntion, we set up all the experimental parameters to be as\nclose as possible to those used in [18]. In particular, the\nrecognition rate is obtained from 10-fold cross validation.\nTable 3 is a comparison table which lists several other ex-\n389\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Figure 2. Genre classiﬁcation result.(CSC is ours)\nisting methods together with their recognition rates, such\nas Topology Preserving Non-Negative Matrix Factoriza-\ntion (TPNMF), Non-Negative Tensor Factorization (NTF),\nMultilinear Principal Component Analysis (MPCA), and\nGeneral Tensor Discriminant Analysis (GTDA) [17]. As\ncan be seen from the table, the proposed method (MF +\nCSC) outperforms all the state-of-the-art SRC-based ap-\nproaches except one. Moreover, the feature dimension of\nthe proposed approach is considerably lower than those of\nthe SRC-based approaches, demonstrating the effective of\nCS in extracting features with discriminating power.\nThis experiment addresses the problem of genre classi-\nﬁcation using Compressive Sampling. A CS recovery is\napplied on short-term and long-term features that are rele-\nvant for genre classiﬁcation. The measurement vectors are\ntrained on labeled sets, then the classiﬁcation is performed\nby computing the approximation of unknown samples with\neach class-speciﬁc features.\nFigure 2 plots the recognition rates of the four meth-\nods with respect to no. of training samples per class. (The\nno. of training samples were randomly selected from each\nclass, while the test samples stayed the same.) The ﬁg-\nure demonstrates that multiple features indeed improve the\nclassiﬁcation accuracy. Moreover, CSC and SRC do have\nconsistent higher accuracy than SVM classiﬁer. More im-\nportantly, these two methods do not require the long train-\ning process of SVM. In Figure 3 , the computation time\nof MF+SRC and MF+CSC is only 30% and 20%, respec-\ntively, of SVM, due to dimension reduction in compressive\nsampling.\nTable 4 shows the confusion matrix of the CS-based\nclassiﬁer [1]. The columns stands for the actual genre and\nthe rows for the predicted genre. It can be seen that the\nrecognition rate of each class is almost evenly distributed.\n4.2 Multiple Features Dimension\nIn this experiment, we combine feature sets (long-time fea-\ntures and short-time features, and short-time features only)\nand different classiﬁers (SVM [14], SRC and the proposed\nclassiﬁer) to investigated their joint effects. The descrip-Table 4. Confusion matrix of the proposed method\ncl co di hi ja ro bl re po me\ncl 96 0 0 3 1 0 0 0 0 0\nco 0 92 4 0 2 0 0 1 0 1\ndi 0 4 93 0 0 1 0 1 0 1\nhi 3 0 0 94 0 1 1 1 0 0\nja 1 2 0 0 93 0 3 1 0 0\nro 0 0 1 1 0 89 2 3 3 1\nbl 0 0 0 1 3 2 90 1 3 0\nre 0 1 1 1 1 3 1 92 0 0\npo 0 0 0 0 0 3 3 0 94 0\nme 0 1 1 0 0 1 0 0 0 97\nFigure 3. Genre classiﬁcation time analysis.(CSC is ours)\ntions of these methods and their parameter settings are shown\nin Table 5.\nAll the samples are digitized 44.1 kHz, 16-bit, and mono\nsignal in preprocessing. The 30-seconds of audio clips af-\nter initial 12 seconds are used. The length of the analysis\nwindow was set to 93ms, and 50% overlap was used for\nfeature extraction. The length of texture window was set\nto 3 second, thus a texture window contains 63 analysis\nwindows. The 13-dimensional MFCCs are computed in an\nanalysis window, mean and variance of each dimension are\ncomputed in a texture window.\nTable 6 shows the multiple features set and dimension.\nAs mentioned in section 2, eight octave subbands were\nused to compute the OSC, OMSC, MSFM, and MSCM.\nThey are computed based on octave subband. Thus, the\ndimensions of the features are dependent on the number\nof octave subband (eight subbands were used in this ex-\nperiment). The dimensions of the OSC, the OMSC, the\nMSFM, and the MSCM are respectively 32, 32, 8 and 8.\n4.3 Under Noise Environment\nIn Figure 2, sparse representation based classiﬁer and CS-\nbased classiﬁer have similar performance in music genre\nclassiﬁcation. The robustness of the system is tested under\n390\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Table 5. Methods used in the experiment\nMethod Description Parameters\nSTF+SVM Short-time feature\nonly and followed\nby a SVM classi-\nﬁerSVM is used and \u000b\ntakes between 0 and\n1. The optimal value\nis chosen experien-\ntially.\nMF+SVM Multiple feature\nand followed by a\nSVM classiﬁerAs above\nMF+CSC Multiple feature\nand followed by\na compressive\nsampling based\nclassiﬁerThe sampling rate\ntakes 67% and the\noptimization algo-\nrithm is basis pursuit\nalgorithm.\nMF+SRC Multiple feature\nand followed by\na sparse repre-\nsentation based\nclassiﬁerThe optimization al-\ngorithm is basis pur-\nsuit algorithm.\nTable 6. Multiple features set and dimension\nFeature Set Dimension\nOMSC Long-time feature 32\nLow-energy Long-time feature 1\nOSC Short-time feature 32\nMFCCs Short-time feature 26\nMSFM Short-time feature 8\nSpectral centroid Short-time feature 2\nSpectral rolloff Short-time feature 2\nSpectral ﬂux Short-time feature 2\nZero crossings Short-time feature 2\nthe following conditions.\n\u000fAdditive white uniform noise (AWUN)\n\u000fAdditive white Gaussian noise (AWGN)\n\u000fLinear speed change (LSC)\n\u000fBand-pass ﬁlter (BPF)\nThe robustness of these two methods was compared, as\nshown in Table 7. We can ﬁnd the average BER of the CSC\nsystem is lower than SRC. CSC has better performance un-\nder the conditions of linear speed change, band-pass ﬁlter,\nand additive white uniform noise.\nFigure 4 shows the classiﬁcation results of different meth-\nods when the Gaussian noise with different variance are\nadded to the music. From the ﬁgure, we can see that the\nproposed method is quite immune to noise.\n5. CONCLUSIONS\nIn this study, we have proposed a CS-based classiﬁer and\nveriﬁed its performance by a common dataset for music\ngenre classiﬁcation. Moreover, we have also explored theTable 7. The comparison result about robustness\nCSC SRC\nRate(%) BER Rate (%) BER\nAWUN 73.8 0.262 73.5 0.265\nAWGN 76.6 0.234 78.8 0.212\nLSC 81.7 0.183 64.8 0.352\nBPF 71.2 0.288 65.8 0.342\nFigure 4. Genre classiﬁcation result under noise.\npossibility of using multiple feature sets for improving the\nperformance of genre classiﬁcation. The experiments demon-\nstrates that the proposed CS-based classiﬁcation together\nwith the use of multiple feature sets outperform quite a\nfew state-of-the-art approaches for music genre classiﬁca-\ntion. The success of the proposed CS-based classiﬁer is\nattributed to CS’s superb capability in feature extraction\nfor generating parsimonious representation of the original\nsignals.\nFor immediate future work, we will focus on the possi-\nbility of porting the proposed CS-based classiﬁer for other\nMIR tasks, such as onset detection, beat tracking, and tempo\nestimation.\n6. REFERENCES\n[1] G. Tzanetakis and P. Cook: “Musical genre classiﬁca-\ntion of audio signals,”IEEE Trans. Speech Audio Pro-\ncess., vol. 10, no. 5, pp. 293-302, 2002.\n[2] D. N. Jiang, L. Lu, H. J. Zhang, J. II. Tao, and L. II.\nCai: “Music type classiﬁcation by spectral contrast fea-\nture,”Proc. ICME 02, vol. I, pp. 113-116, 2002.\n[3] T. Li , M. Ogihara, and Q. Li: “A comparative study on\ncontent based music genre classiﬁcation,”Proc. ACM\nCon! on Research and Development in Information Re-\ntrieval, pp. 282-289, 2003.\n[4] D. Jang and C. Yoo: “Music information retrieval using\nnovel features and a weighted voting method,”IEEE In-\nternational Symposium on Industrial Electronics (ISlE\n2009) Seoul Olympic Parktel, Seoul, Korea July 5-8,\n2009.\n391\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)[5] D. Donoho: “Compressed sensing,”IEEE Transactions\non Information. Theory, 52(4), pp. 1289-1306, Apr.\n2006.\n[6] J. Romberg: “Imaging via compressive sam-\npling,”IEEE Signal Processing Magazine, 25(2),\npp. 14 - 20, March 2008.\n[7] Z. Chen, J. Wen, Y . Han, J. Villasenor, S. Yang: “A\ncompressive sensing image compression algorithm us-\ning quantized DCT and noiselet information,”ICASSP,\n2010.\n[8] W. Bajwa, J. Haupt, A. Sayeed and R. Nowak: “Joint\nsource-channel communication for distributed estima-\ntion in sensor networks,”IEEE Transactions on Signal\nProcessing, 53(10), pp. 3629-3653, October 2007.\n[9] J. Wright, A. Yang, A. Ganesh, S. Shastry, and Y .\nMa: “Robust face recognition via sparse representa-\ntion,”IEEE Transactions on Pattern Analysis and Ma-\nchine Intelligence, 31(2), pp. 210-227, February 2009.\n[10] E. Candes, M. Wakin, and S. Boyd: “Enhancing spar-\nsity by reweighed `1Minimization,”Journal of Fourier\nAnalysis and Applications, 14(5), pp. 877-905, Octo-\nber 2008.\n[11] L. Lu, D. Liu, and H-J. Zhang: “Automatic mood\ndetection and tracking of music audio signals,”IEEE\nTrans. on Audio, Speech, and Language Processing,\nvol. 14, no. 1, Jan. 2006.\n[12] X. Huang, A. Acero and H.-W. Hon: “Spoken Lan-\nguage Processing,”Prentice Hall PTR, 2001.\n[13] S. Sukittanon, L. E. Atlas, and 1. W. Pitton:\n“Modulation-scale analysis for content identiﬁca-\ntion,”IEEE Trans. on Signal Processing, vol. 52, no.\n10, pp. 3023-3035, Oct., 2004.\n[14] C.-C. Chang and C.-J. Lin: “LIBSVM - A Li-\nbrary for Support Vector Machines,”[Online] Avail-\nable: www.csie.ntu.edu.tw/.\n[15] I. Karydis , A. Nanopoulos , A. Papadopoulos , Y .\nManolopoulos: “Audio Indexing for Efﬁcient Music\nInformation Retrieval,”Proceedings of the 11th Inter-\nnational Multimedia Modeling Conference, p.22-29,\nJanuary 12-14, 2005.\n[16] D. Jang; M. Jin; C. Yoo: “Music genre classiﬁcation\nusing novel features and a weighted voting method,”In\nProceeding of the 2008 IEEE International Conference\non Multimedia and Expo, pp. 1377–1380, April 2008.\n[17] Y . Panagakis, C. Kotropoulos: “Music genre classiﬁca-\ntion via topology preserving non-negative tensor fac-\ntorization and sparse representations,”ICASSP, 2010.\n[18] T. Sainath, A. Carmi, D. Kanevsky, B. Ramabhadran:\n“Bayesian compressive sensing for phonetic classiﬁca-\ntion,”ICASSP, 2010.[19] Y . Yoon, M. Amin: “Through-the-wall radar imaging\nusing compressive sensing along temporal frequency\ndomain,”ICASSP, 2010.\n[20] N. Jacobs, S. Schuh, R. Pless: “Compressive sens-\ning and differential image motion estimation,”ICASSP,\n2010.\n392\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "ThumbnailDJ: Visual Thumbnails of Music Content.",
        "author": [
            "Ya-Xi Chen",
            "René Klüber"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1418271",
        "url": "https://doi.org/10.5281/zenodo.1418271",
        "ee": "https://zenodo.org/records/1418271/files/ChenK10.pdf",
        "abstract": "Musical perception is non-visual and people cannot de- scribe what a song sounds like without listening to it. To facilitate music browsing and searching, we explore the automatic generation of visual thumbnails for music. Tar- geting an expert user groups, DJs, we developed a con- cept named ThumbnailDJ: Based on a metaphor of music notation, a visual thumbnail can be automatically gener- ated for an audio file, including information of tempo, volume, genre, aggressiveness and bass. We discussed ThumbnailDJ and other 3 selected concepts with DJs, and our concept was preferred most. Based on the results of this interview, we refined ThumbnailDJ and conducted an evaluation with DJs. The results confirmed that ThumbnailDJ can facilitate expert users browsing and searching within their music collection.",
        "zenodo_id": 1418271,
        "dblp_key": "conf/ismir/ChenK10",
        "keywords": [
            "Automatic generation",
            "Visual thumbnails",
            "Music browsing",
            "Expert user groups",
            "DJs",
            "Metaphor of music notation",
            "Audio file",
            "Information of tempo",
            "Volume",
            "Genre"
        ],
        "content": "THUMBNAILDJ: VISUAL THUMBNAILS OF MUSIC \nCONTENT \nYa-Xi Chen René Klüber \nMedia Informatics, University of Munich \nAmalienstr. 17, 80333 Munich, Germany \nyaxi.chen@ ifi.lmu.de  Media Informatics, University of Munich \nAmalienstr. 17, 80333 Munich, Germany \nklueber@cip.ifi.lmu.de \n \nABSTRACT \nMusical perception is non-visual and people cannot de-\nscribe \nwhat a song sounds like without listening to it. To \nfacilitate music browsing and searching, we explore the automatic generation of visual thumbnails for music. Tar-geting an expert user groups , DJs, we developed a con-\ncept named ThumbnailDJ: Based on a metaphor of music notation, a visual thumbnail can be automatically gener-ated for an audio file, including information of tempo, volume, genre, aggressiveness and bass. We discussed ThumbnailDJ and other 3 selected concepts with DJs, \nand our concept was preferred most. Based on the results \nof this interview, we re fined ThumbnailDJ and conducted \nan evaluation with DJs. The results confirmed that ThumbnailDJ can facilitate expert users browsing and searching within their music collection. \n1. INTRODUCTION \nPeople can easily gain an overview of a photo by glimps-ing\n at its thumbnail.  With assistance of multiple thumb-\nnails, people can browse many photos in parallel and lo-cate the desired ones quickly. On the contrary, music car-ries no visual information and people cannot describe what a song sounds like without listening to it. Cover art is commonly used as a visual assistance for music. How-ever, it only visually encodes the relevant artist and al-bum, and has no reflection on the intrinsic music content. Will a visualization of musical content help, and who will \nbenefit from it? To answer these questions, we explore \nthe automatic generation of visual thumbnails for music content and develop a concept named ThumbnailDJ. We conducted several rounds of survey and interview, and the results confirmed that our concept can help expert us-ers browsing and searching within their music collec-\ntions. 2. RELATED WORK \nA map-based representation is widely used to display mu\nsic collections. Islands of Music [6] and MUSICtable \n[9] cluster songs based on their acoustic similarity. In Artist Map [13] the user can chose any two of the four criteria of mood, genre, year and tempo to display music on a map. In iCandy [3], songs are displayed in a grid layout and their order is determined by the selected crite-ria such as genre, most pl ayed artist or album. Besides \nmap-based representation, there are other visualizations of music collections. Torrens et al. [10] presented three \nvisualization concepts: a disc , a rectangle and a tree. In \nMusicRainbow [7] artists are displayed in rings of a rain-bow. \nThe aforementioned visualizations focus on the repre-\nsentat\nion of an entire music collection, in which single \nsongs are either displayed with the cover art or as the name of artist, album or song, and none of them reflect the intrinsic music content. Some work addressed this issue by producing visualization for the file content. Se-manticons [8] produce semantically meaningful icons for different file types. Music Icon [5] is a similar concept using a blossom metaphor: Each music file is represented as a blossom icon with two rings of petals (see Figure 1d \nfor a simplified version). The music feature is reflected \nby the color, shape and number of petals.  \nBesides these metaphoric visualizations, some re-\nsearchers focus on the sequen tial repr\nesentation. Beat \nHistogram [11] is a temporal representation of beat strength in audio signals (see Figure 1a). It helps to gain an overall impression of how beat strength changes over time. TimbreGrams [12] represents an audio file as se-quential stripes. Bright co lors correspond to speech or \nsinging, and purple and blue ones associate to classical music (see Figure 1c). In Arc Diagrams [14], instances of the identical notes are connect ed by arcs, thus depicting \nthe repetitive structure in a music file (see Figure 1b).  \n \nPermission to make digital or hard copi es of all or  par t of this work fo r \npersonal or classroom use is grante d without fee provided that copies \nare not made or distributed for profit or commercial advantage and that \ncopies bear this notice and the full citation on the first page. \n© 2010 International Society for Music Information Retrieval  3. ONLINE SURVEY \nIn order to assess the understandability and suitability of \nth\ne concept of visualizing music content, we conducted \nan online survey with four selected concepts (see Figure \n565\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \n1): Beat Histogram, Arc Di agram, TimbreGrams and \nsimplified Music Icon.  \n \n  \n(a)                                    (b)   \n \n  \n              (c)                                         (d) \nFigure 1 . Four selected concepts. a) Beat Histogram [11]. b) \nArc Diagram [14]. c) TimbreGrams [12]. d) A simplified Mu-\nsic Icon has one ring of petals. The number and shape of petals represent tempo and aggr essiveness respectively.\n \n3.1 Survey Desi gn \nThe participants first filled out a questionnaire about their \npersonal\n information and gene ral experience with music. \nThen the four visualization concepts were briefly intro-\nduced. The participants answered 15 questions about these concepts: We chose 8 popular songs and asked the participants about their familia rity with these songs. They \ncould follow the corresponding links to listen to these songs online. For each concept, they were required to map one visualization to a correct song out of 4 candi-dates, then map one song to a correct visualization out of \n4 candidates. After this, they were asked about informa-\ntion that can be derived from each concept. Then we asked them about their preference between and comments about these concepts. All scores were rated on a 5-point Linkert-scale where 5 represented the highest score. \n3.2 Participants \nIn total we received 38 com\nplete questionnaires, 9 female \nand 29 m a\nle. Their age ranged from 18 to 55 with an av-\nerage age of 26.6 years. 31 out of 38 participants were \nstudents and employees from Europe. \n3.3 Results \nRegarding the reflection on music content, the partici-\npants t\nhought attributes such as melody, mood, rhythm, \ninstrument and genre were more important than the gen-eral information of lyrics, length and release year. Con-\ncerning the usefulness of different information in helping gaining an overall impression of a song, the 30-second preview clip received the highest score (M=3.84, SD=0.17). Similar artists/tracks (M=3.16, SD=0.19) and top tags (M=2.78, SD=0.19) were scored lower. Unfortu-nately, our concept of visual thumbnails of music content \nwas rated lowest (M=2.22, SD=0.16). Although the participants were generally familiar with \nth\ne tested songs (M=3.50, SD=0.74), the correctness of \ntheir answers was quite low: To map a visualization to a \ncorrect song out of 4 candidates, 14 participants (36.8%) chose the correct song for Music Icon, 12 (31.6%) for TimbreGrams, 9 (23.7%) for BeatHistogram and 8 (21.1%) for Arc Diagrams. To map a song to a correct visualization out of 4 candidates, the performance was slightly better but still low: 17 (44.7%) for Music Icon, \n16 (42.1%) for BeatHistogram, 10 (26.3%) for Arc Dia-\ngrams and 8 (21.1%) for TimbreGrams. \nConcerning the information that can be derived from \neach concept, the results \nillustrated that BeatHistogram \nhelped to learn tempo and volume, and Music Icon tempo and volume.  TimbreGrams and Arc Diagrams facilitated gaining song structure and information of harmony. In general, the scores for easiness of deriving each informa-tion were rather low (all below 3). The usefulness of each concept was also rated quite low: BeatHistogram (M=2.17, SD=1.40), Arc Diagrams (M=2.09, SD=1.17), Music Icon (M=1.97, SD=1.32) and TimbreGrams (M=1.40, SD=0.55). \n3.4 Discussion \nAll the participants commented that these visualizations \nwere overal l\n too complicated for them. Although they \nagreed that attributes such as melody and rhythm are im-\nportant features of a song, they did not think such infor-mation can help them to gain the overall impression of a song. Instead, they would prefer direct and non-trivial assistance, such as 30-second preview clips. \n4. CONCEPT DEVELOPMENT \nThe results of the survey revealed an overall low appre-ciation for the concepts of vi\nsualizing musical content. \nThe normal music listeners seem not be unsuitable as the consumers of those thumbnails, as they have neither re-quirements nor efficient knowledge in understanding technical details of a song. Therefore, we shift our focus to more professional users, Disk Jockeys (DJs). We de-velop a concept named ThumbnailDJ. We discussed ThumbnailDJ and 3 other concepts with DJs. Thumb-nailDJ was preferred most, and we derived implications for the refinement of this concept. \n4.1 Four Tested Concepts \nWe first conducted a preliminary discussion with 7 DJs \nwit\nh the four concepts presented in Figure 1. They were \noverall appreciated the idea of  visualizing musical con-\ntent. Arc Diagram and Timbre Grams were commented as \nhelpful to gain an overall impression of a song, but lack-ing of precise values, such as those shown in Beat Histo-gram. Beyond the single criterion displayed in Beat His-togram, more attributes were  requested, such as aggres-\n566\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \nsiveness and volume. Music Icon was generally preferred \nmost among these four tested concepts. \nThe results of the preliminary test illustrated that ex-\npert \nusers require multiple attributes with precise values, \nand simplicity is vital for mental perception [1]. There-fore, we selected four simple and compact visualizations and tested them with DJs. Havre et al. [4] introduced a \nriver metaphor to represent topical changes within a document collection. We employ this concept to describe temporal changes of attributes (see Figure 2a). Border Community\n1 offers hand-drawn graphs to show the com-\nposition of songs. The amplit ude depicts intensity and \ngray color represents bass strength. We name this visu-alization TensionDiagram and map its background color to genre (see Figure 2b). The third concept is Music Icon, which has already been tested  in the previous online sur-\nvey (see Figure 1d). \n \n  \n  \n(a)                            (b)                           (c)  \nFigure 2 . Tested concepts with DJs. a) ThemeRiver [4]. b) \nTensionDiagram1. c) Initial concept of ThumbnailDJ. \nBesides these three selected visualizations, we develop \nour own concept ThumbnailDJ (see Figure 2c). Our de-sign is built on the metaphor of music notation, the most common symbolic representation of music, from which abundant information can be read out: Pitch is shown as the vertical position of notes on a five-line staff. Duration is illustrated as the note value and additional symbols such as dot and tie. A dot extends the value of a note and a tie connects two notes with same pitch. Tempo and dy-namics are shown above or below the staff. Tempo is normally represented as Beats per Minute (BPM), and \ndynamics as the overall volume of the whole piece. By \nreading the staff from left to right, the overall temporal impression can be gained.  \nIn our initial concept, we made some modification of \nth\ne original metaphor. The lengthy notation contradicts to \nthe fundamental characteristic of compactness, and thus we decided to employ only three notes, each representing 1/3 of the song. The vertical location of a note in the staff depicts the aggressiveness. Two notes are connected by a tie if they share similar aggr essiveness. On the contrary, a \ndot on the top of a note stands for change in aggressive-ness. Tempo and volume are shown in the top and bottom left corners respectively. Crescendo (<) and decrescendo (>) describe the increase/decrease of volume. The genre \nis shown in the top right corner and associated with the \nbackground color of the entire graphics. Figure 2c shows \n                                                          \n \n1http://www.bordercommunity.coma medium-tempo calm rock song which gets louder over \nti\nme. Aggressiveness keeps constant in the first two parts \nand changes in the third part. \n4.2 Discussion with DJs \nIn order to test the suitability of the four selected con-\ncepts\n, we conducted a second round of discussion with \nthe same 7 DJs. The open questions covered mainly their routine tasks and general impression of the tested con-cepts, which were introduced in a blind fashion. The par-ticipants have DJ experience about 10 years in average. They are all male, and their age range from 24 to 37 with an average age of 28.4 years.  Two of them play mainly \nHip-Hop music, three play Electronic, and the other two play diverse genres. Two DJs play analogue music and the others play digital. All DJs can read music notation. \nAnalogue DJs organize their collections on shelves, ei-\nth\ner sorting them by alphabet or genre, or no ordering at \nall. Digital DJs store their collections on hard disk and sort them by folders and ID 3tags. They rely heavily on \nthe search functionality in DJ software to look for music.  \nConcerning the desired music attributes, DJs playing \nmu\nltiple genres required a general impression of a song. \nDJs playing only one or few genres requested detailed temporal information. The generally important attributes were genre, tempo, aggressiveness and volume.  \nThe idea of visualizing music content was overall ap-\npreciated. TensionDiagram\n was well accepted, as it is \nsimilar to the signal histogram shown in most of the DJ software. Consistent with the preliminary test, Music Icon and ThemeRiver were co mmented as lacking of pre-\ncise values. ThumbnailDJ was preferred most, as it uses the general metaphor of music notation and reflects both overview and precise information for the most desired attributes of genre, tempo, aggressiveness and volume. The participants’ comments indicated further improve-ment of ThumbnailDJ. Representing the entire piece of \nsong as three separated parts was commented as simple \nand easy to learn. But a continuous flow description was desired, which helps to gain temporal changes and facili-tate observation of the represen tative parts, such as peaks \nand gaps. Bass value was desired, as it helps to achieve smooth transition between songs. The symbols of cres-cendo, decrescendo, dot and tie were commented as less important and poor readable in a size-limited thumbnail.  \n5. CONCEPT REFINEMENT \nBased on the implications derived in the discussion with DJs, we refi ned Thum\nbnailDJ (see Figure 3a): We ex-\ncluded the symbols of crescendo, decrescendo, dot and tie. We associated pitch (namel y the vertical location of a \nnote in the staff) with bass value. For example, a note on the bottom line implies a higher bass value. The bass value is also represented by the position of note head: A note with head on the bottom represents higher bass \n567\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \nvalue, and head on the top lower bass value. Aggressive-\nness is depicted as note value, for example, half note for calm and sixteenth for aggressive (see Figure 3b). A light gray flow is drawn under the staff, indicating changes of aggressiveness. Genre is displayed in the top right corner and also represented as the background color of the entire graphics. Tempo is represented as BPM in the top left corner, and volume as decibel (dB) in the bottom left cor-\nner. Currently we define 6 main categories of genres (see Figure 3c), and more genres can be easily included. Fig-\nure 3a shows an example thumbnail for a rock song, with \naverage tempo of 108 BPM and volume of 94 dB. It is quite aggressive and very heavy on bass in both begin-ning and end parts. The bac kground flow illustrates that \naggressiveness descends constantly in the middle part, rises again and becomes quite fluctuate in the last part. \n     \n  \n(a)                                (b) \n   \n(c) \nFigure 3 . The refined ThumbnailDJ. a): Rock song “Biffy \nClyro-As dust dances”. b) 8 kinds  of notes. The values of ag-\ngressiveness and bass are both ma pped to a discrete value be-\ntween 1 and 4. The note with head on the bottom represents \nbass value of 3 or 4, and on the top 1 or 2. Half to sixteenth \nnotes indicate aggressive values  from 1 to 4 respectively. c) \nSome example thumbnails for different genres. \n5.1 Implementation \nThumbnailDJ is implemented in Java. All songs and their \nrele\nvant data are saved in a SQLite database. We use Tri-\ntonus1 to read in the ID3tags of a song. JLayer2 is used to \ndecode the audio file. After comparing the performance of different audio features, we decided to use zero cross-ings and Fast Fourier Transformation (FFT) to calculate values of aggressiveness and bass. We first use mp3splt\n3 \nto cut the audio file into 15 snippets with equal length. \n                                                           \n                                                          1http://www.tritonus.org/  \n2http://www.javazoom.net/jav alayer/javalayer.html\n3http://mp3splt.sourceforge.net/mp3splt_pageWe then apply jAudio4 to extract the corresponding low-\nlevel features from each snippet. We compute the value \nof zero crossings over each sni ppet. The average value of \neach successive 5 snippets re presents aggressiveness for \neach 1/3 part of the song. Bass is determined by the FFT frequencies. For each snippet, the frequencies are sorted from low to high. The bass value is represented as the sum of lower 1/4 frequencies divided by the sum of all frequencies. The average valu e of each successive 5 snip-\npets represents bass value for each 1/3 part of the song. Both aggressiveness and bass are normalized to a discrete value between 1 and 4, in order to map them to one of the 8 note shapes (see Figure 3b). Volume is determined by the average value of Root Mean Square (RMS) and tempo by the average value of  beats in the beat histo-\ngram. \n6. EVALUATION \nWe conducted a user study with DJs to evaluate the per-form ance of Thum\nbnailDJ. We were specifically inter-\nested in how it helps gaining an overall impression of a song, and facilitating browsing and searching in a music \ncollection. \n6.1 Settings and Procedure \nAs DJ equipments were requi red, t h\n e evaluation was con-\nducted in the work places of the participants. Each par-\nticipant was asked to offer a collection of 100 songs. For \neach collection, the thumbnails  were generated before \nhand and shown in Windows Explorer in another laptop. On average the user study lasted about 90 minutes per participant. It was recorded on video using the Think-Aloud protocol was applied. A ll scores were rated on a 5-\npoint Linkert-scale where 5 represented the highest score. \nSince the participants alread y jo\n ined the former inter-\nview and they preferred ThumbnailDJ most, in this evaluation we focused on ThumbnailDJ and did not com-pare it with other concepts. After a brief introduction of refined ThumbnailDJ, the participants were asked to de-scribe their impression of two unfamiliar songs by view-\ning the corresponding thumbnails. Then they were shown thumbnails of two familiar songs and asked to rate how well these thumbnails describe these songs. After that, they executed a routine task with their own methods (two participants with analogue music and the other three with \ndigital) and through browsing the corresponding thumb-\nnails respectively: Finding some appropriate songs as in-tro, bridge and outro respectively for an X (the genre the participant often plays) party. The order of their own methods and ThumbnailDJ was counterbalanced between the participants to minimize learning effects. After com-\n \n4http://jmir.sourceforge.net/jAudio.html\n568\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \npleting all tasks, they fille d out a questionnaire concern-\ning their overall impression of ThumbnailDJ. \n6.2 Participants \nW\ne recruited 5 DJs, who took part in the earlier discus-\nsions. Their age ranged from  24 to 31, with an \naverage \nage of 27.4 years. They are all experience DJs with aver-\nage experience about 10 years. Two participants play \nmainly Electronic music, two play Hip-Hop, and the other one plays multiple genres. Two participants play analogue music and the other three digital. All partici-pants can read musical notation. \n6.3 Results \nThe meaning of attributes shown in the thumbnails was \nclear to all p\narticipants. Th ey claimed that these thumb-\nnails reflected efficiently the features of the tested songs. However, the combination of multiple attributes was still insufficient to help gaining the sense of an unfamiliar song. The participants were reluctant to “guess” the feel-ing of an unfamiliar song without listening to it. Thumb-nailDJ was thus considered rather useful as a quick visual reminder of a familiar song. Some additional attributes were desired, such as vocal and instrument.  \nSong selection was overall very subjective, which was \nalso influenced by the contex t, such as the audience feed-\nback, t\nhe performance duration, the order of DJs in the \nsame show and music played by the previous DJ. Songs played as intro were characterized as moody and relax-ing, and thus slow and calm songs were selected. Outro songs were similar to the intros, and those with similar tempo were composed in a block. Bridge song should fit the tempo and bass intensity of the connecting songs. \nUsing their own methods and ThumbnailDJ respec-\ntively, there were on averag e 5 songs selected in each \nsession. Am\nong these selections each DJ picked 2 to 3 \nsame songs and other songs were quite similar. When asked about their selection criteria, the participants pointed out the decisive factors such as song attributes and other contextual consider ations. However, they could \nnot formulate formally why they chose a specific song, as “music is kind of sense that can not be precisely de-scribed” (DJ 6). Therefore, with the general open tasks of song selection, we could not collect details about how \nThumbnailDJ assisted search and browsing in certain as-\npects. \nConsidering the completion time, digital DJs were \noverall faster. Their tag-based search was com\nparable \nwith browsing thumbnails. Analogue DJs were slightly slower by flipping through the analogue collections. They claimed the time difference would become more noticeable with a larger collection.  \nThe concept of visualizing music content was gener-\nall\ny appealing (M=4.20, SD=0.84). The impression of \nThumbnailDJ was quite positive in the aspects of ease of use (M=4.0, SD=1.40), learnability (M=4.80, SD=1.84) and understandablity (M=4.20, SD=1.27). Enjoyment was rated lower (M=3.40, SD=0.63). Most participants believed that the performance of ThumbnailDJ was \npromising, but needed more graphical and acoustical ap-\npealing effects. All participants expressed high willing-ness to have ThumbnailDJ as plug-in in their DJ soft-ware, (M=4.80, SD=0.45). \n6.4 Discussion \nThe final feedback was quite encouraging, and we re-\nceived \nvaluable implications for further improvement. \nBesides the included attributes of tempo, volume, genre, aggressiveness and bass, some additional information is desired, such as vocal and instrument, which are impor-tant for a smooth transition between songs. Currently, attributes are directly extr acted from low-level features, \nand more elaborate mapping algorithms should be inte-grated to achieve a better association between low-level features and high-level perception. Music taste is subjec-tive and different users may ha ve different requirements. \nBesides, it is not practical to display too much informa-tion in a compact thumbnail. Therefore, we suggest em-ploying a personalization mechanism, and thus the user can produce personalized thumbnails, for example, by defined the desired attributes shown in the thumbnail.  \nConcerning the evaluation method, the collected data \nwas ma\ninly qualitative. Detailed information about the \nperformance of ThumbnailDJ could not be collected with the open tasks. To derive deeper understanding of how such visualization facilitates browsing and decision mak-ing, more controlled tasks should be considered. With a possible integration in existing DJ software, a field study in a real DJ working environment will help to gain more insights on the practical usage of such a tool. \n7. CONCLUSION AND FUTURE WORK \nWe explore how to facilitate browsing and searching \nwithin\n music collections with  the assistance of visual \nthumbnails of music content. Based on the metaphor of music notation, we develope d ThumbnailDJ to generate \nthumbnails for music content, which include information of tempo, volume, genre, aggressiveness and bass. We discussed ThumbnailDJ and other 3 selected concepts with DJs, and our concept was preferred most. We then refined ThumbnailDJ and conducted an evaluation with DJs. Our concept received overall positive feedback, es-\npecially towards its helpfulness for quickly recalling of a familiar song. Moreover, DJs showed high willingness to have such a plug-in in their DJ software.  \nWe discussed with a senior developer of Traktor\n1 the  \npotential integration of ThumbnailDJ into their DJ soft-\nware. The feedback was quite encouragi ng. The di\nfferent \n                                                           \n1http://www.native-\ninstruments.com/en/products/dj/traktor-pro  \n569\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \nrepresentations of the same information, such as genre, \nbass and aggressiveness, were  especially appreciated. \nConsidering a possible integration into a commercial product, some modification of  ThumbnailDJ is necessary, \nfor example, displaying more meaningful information for tempo and volume beyond the currently discrete values. \nBesides DJs, online audio searchers might be another \npotent\nial user group. For example, in iStockPhoto1, to \ndetermine which audio file to buy, the user has to listen to many retrieval results, which is quite time-consuming. We expect that browsing and decision-making can be fa-cilitated by assisting the audio files with visual thumb-nails of their content. Th en the user can exclude un-\nmatching files quickly, or scan all thumbnails in parallel while looking for candidates satisfying certain graphical features. We discussed ThumbnailDJ with 10 online au-dio searchers. They were offered a set of 20 thumbnails, \ncovering diverse genre, tem po, volume and aggressive-\nness. They first categorized these thumbnails and sorted \nthem in each category. They th en selected an audio file \nfor a coffee advertisement. All participants could manage \nthese tasks and commented the assistance of Thumb-nailDJ as helpful. \nWe agree that “Music is mo re of an art than science” \n[2]. \nWe also believe that visualizations can help people \ngaining more musical insights. We hope our exploration can shed some light on facilitating browsing and search-ing within music collections by bridging the perceptions of vision and acoustics. \n8. ACKNOWLEDGEMENT \nThis research was funded by the Chinese Scholarship Counci\nl and the German state of Bavaria. We would like \nto thank the participants of our user study, and Anreas Butz, Friedemann Becker and Dominikus Baur for their valuable feedback. \n9. REFERENCES \n[1] M. D. Byrne: “Using Icons to Find Documents: \nSim\nplicity is Critical,” Proceedings of the SIGCHI \nConference on Human Factors in Computing Systems, pp. 446–453, 1993. \n[2] S. J. C unningham, D. Bainbridge and A. Falconer, “ \n‘M \nore of an art than a science’: Supporting the \nCreation of Playlists and Mixes, “ Proceedings of the International Symposium on Music Information Retrieval , 2006. \n[3] J. Grah am and J. J. Hull: “iCandy: A Tangible User \nInterface for iTunes,” \nProceedings of the SIGCHI \nConference on Human Factors in Computing Systems, Ext. Abstracts,  pp. 2343-2348, 2008. [4] S. Hav re, B. Hetzler and L. Nowell: “ThemeRiver: \nVi \nsualizing Theme Changes over Time,” \nProceedings  of the IEEE symposium on Information \nVisualization , pp. 115-123, 2000. \n[5] P. Kolhoff, J. Preuß and J. Loviscach: “Music Icons: \nProcedural  Gl\nyphs for Audio Files,” Proceedings of \nthe Brazilian Symposium on Computer Graphics and Image Processing , pp. 289-296, 2006. \n[6] E. Pam palk: “Islands of Music: Analysis, \nOrgani \nzation, and Visualization of Music \nArchives,” Journal of the Austrian Society for Artificial Intelligence , 2003. \n[7] E. Pam palk and M. Goto: “MusicRainbow: A New \nUser Interface to Discover Artists Using Audio-based Si \nmilarity and Web-based Labeling,” \nProceedings of the Inte rnational Symposium on \nMusic Information Retrieval , 2006. \n[8] V. Setlur, C. Albrecht-Buehler, A. A. Gooch, S. Ro \nssoff and B. Gooch: “Semanticons: Visual \nMetaphores as File Icons,” Computer Graphics \nForum , Vol. 24, pp. 647-656, 2005. \n[9] I. Stavness, J. Gluck, L. Vilhan and S. Fels: “The \nM\nUSICtable: A Map-based Ubiquitous System for \nSocial Interaction with a Digital Music,” Proceedings of the Inte rnational Conference on \nEntertainment Computing , pp. 291-302, 2005. \n[10] M. Torrens, P. Hertzog and J. L. Arcos: \n“Vi \nsualizing and Exploring Personal Music \nLibraries,” Proceedings of the International \nSymposium on Music Information Retrieval , 2004. \n[11] G. Tzanet aki s, G. Essl and P. Cook: “Human \nPerception and Computer Extraction of Musical Beat Strength,” Proceedings of the International Conference on Digital Audio Effects , pp. 257-261, \n2002. \n[12] G. Tzanet aki\n s and P. Cook: “3D Graphics Tools for \nSound Collections,” Proceedings of the \nInternational Conference on Digital Audio Effects , \n2000. \n[13] R. van Gulik and F. Vignoli: “Visual Playlist \nGenerat\nion on the Artist Map,” Proceedings of the \nInternational Symposium on Music Information Retrieval , 2005. \n[14] M. W attenberg: “Arc Diagrams: Visualizing \nst \nructure in strings,” Proceedings of the IEEE \nsymposium on Information Visualization, pp. 110-116, 2002. \n                                                                                             \n \n1http://www.istockphoto.com\n570\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "Multiple Viewpoints Modeling of Tabla Sequences.",
        "author": [
            "Parag Chordia",
            "Avinash Sastry",
            "Trishul Mallikarjuna",
            "Aaron Albin"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1416540",
        "url": "https://doi.org/10.5281/zenodo.1416540",
        "ee": "https://zenodo.org/records/1416540/files/ChordiaSMA10.pdf",
        "abstract": "We describe a system that attempts to predict the con- tinuation of a symbolically encoded tabla composition at each time step using a variable-length n-gram model. Us- ing cross-entropy as a measure of model fit, the best model attained an entropy rate of 0.780 in a cross-validation ex- periment, showing that symbolic tabla compositions can be effectively encoded using such a model. The choice of smoothing algorithm, which determines how information from different-order models is combined, is found to be an important factor in the models performance. We extend the basic n-gram model by adding viewpoints, other streams of information that can be used to improve predictive per- formance. First, we show that adding a short-term model, built on the current composition and not the entire corpus, leads to substantial improvements. Additional experiments were conducted with derived types, representations derived from the basic data type (stroke names), and cross-types, which model dependencies between parameters, such as duration and stroke name. For this database, such exten- sions improved performance only marginally, although this may have been due to the low entropy rate attained by the basic model.",
        "zenodo_id": 1416540,
        "dblp_key": "conf/ismir/ChordiaSMA10",
        "keywords": [
            "symbolically encoded",
            "variable-length n-gram model",
            "cross-entropy",
            "entropy rate",
            "cross-validation experiment",
            "smoothing algorithm",
            "information from different-order models",
            "short-term model",
            "derived types",
            "cross-types"
        ],
        "content": "MULTIPLE VIEWPOINTS MODELING OF TABLA SEQUENCES\nParag Chordia\nGeorgia Tech\nCenter for\nMusic Technology\nAtlanta, GA, USA\nppc\n@gatech.eduAvinash Sastry\nGeorgia Tech\nCenter for\nMusic Technology\nAtlanta, GA, USA\nasastry3\n@gatech.eduTrishul Malikarjuna\nGeorgia Tech\nCenter for\nMusic Technology\nAtlanta, GA, USA\ntmallikarjuna3\n@mail.gatech.eduAaron Albin\nGeorgia Tech\nCenter for\nMusic Technology\nAtlanta, GA, USA\naalbin3\n@mail.gatech.edu\nABSTRACT\nWe describe a system that attempts to predict the con-\ntinuation of a symbolically encoded tabla composition at\neach time step using a variable-length n-gram model. Us-\ning cross-entropy as a measure of model ﬁt, the best model\nattained an entropy rate of 0.780 in a cross-validation ex-\nperiment, showing that symbolic tabla compositions can\nbe effectively encoded using such a model. The choice of\nsmoothing algorithm, which determines how information\nfrom different-order models is combined, is found to be an\nimportant factor in the models performance. We extend the\nbasic n-gram model by adding viewpoints, other streams\nof information that can be used to improve predictive per-\nformance. First, we show that adding a short-term model,\nbuilt on the current composition and not the entire corpus,\nleads to substantial improvements. Additional experiments\nwere conducted with derived types, representations derived\nfrom the basic data type (stroke names), and cross-types,\nwhich model dependencies between parameters, such as\nduration and stroke name. For this database, such exten-\nsions improved performance only marginally, although this\nmay have been due to the low entropy rate attained by the\nbasic model.\n1. INTRODUCTION AND MOTIVATION\nWhen listening to music, humans involuntarily anticipate\nhow it will continue [8]. Such expectations help to process\ninformation efﬁciently, as well as allowing complex, noisy\nstimuli to be accurately interpreted. For musicians, this an-\nticipation is essential for synchronization and harmoniza-\ntion. In this paper, we explore a computational model of\nthis predictive process based on an ensemble of n-gram\nmodels. Speciﬁcally, we examine whether such a model\ncan successfully represent the structure of symbolically en-\ncoded tabla compositions. Our motivation for building a\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2010 International Society for Music Information Retrieval.predictive tabla model is to enable more intuitive modes of\ninteraction between musicians and computers.\nIn addition to this practical goal, we hope to work to-\nwards developing a computational model of musical antic-\nipation. Previous work [11] on Western melodies showed\nthat human judgments of melodic continuation were highly\ncorrelated with a variable-length n-gram model. Although\nwe will not address human subject data here, we hope to\nprovide converging evidence from a markedly different mu-\nsical tradition (tabla), that syntactic structure can be efﬁ-\nciently represented using an n-gram modeling approach.\n2. BACKGROUND AND RELATED WORK\nMarkov and n-gram models have been extensively used to\nmodel temporal structure in music [1]. They have been ex-\ntensively used in algorithmic composition, timbral analy-\nsis [2] [7], structure analysis [12], and music cogniton [14].\nMarkov models are based on a succession of states. In\nmusical contexts, states represent discretely valued attributes,\nsuch as pitch, duration, instrument, section, etc. The Markov\nassumption assumes that, given the current state, the next\nstate is independent of previous states. This can easily\nbe generalized so that the next state depends on a ﬁxed\nnumber of past states; a ﬁrst-order Markov chain depends\nonly on the current state, a second-order on the current\nand immediately preceding state, and so on. If sequences\nare directly observable, then most inference problems can\nbe solved by counting transitions. An alternative formula-\ntion is the n-gram model in which all possible symbols of\nlength nare constructed from the training sequences, and\ntheir frequency tabulated. It is easy to see that the tran-\nsition probabilities for an nth-order Markov chain can be\ncomputed by forming all n+ 1-grams.\nA signiﬁcant problem that arises with ﬁxed-order mod-\nels is that, as the order nincreases, the number of total\nn-grams increases as vn, where vis the number of sym-\nbols. In music applications, such as melody prediction,\nwhere the past ten events could easily inﬂuence the next\nevent, and where there might be a dozen or more sym-\nbols, we are left attempting to assess the relative frequency\nof greater than 1210n-grams. Even for large databases,\nmostn-grams will be unseen, leading to the so-called zero\nfrequency problem [10]. This sparsity problem leads to a\n381\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)fundamental tradeoff between using the predictive power\nof longer context and the increasing unreliability of higher\norder n-gram counts. Variable-length n-gram models at-\ntempt to overcome this problem in two ways: 1) by build-\ning many ﬁxed-order models and integrating information\nacross orders (smoothing), 2) and by reserving a certain\namount of probability mass for unseen n-grams (escape\nprobabilities). We describe these techniques in Section 4.2.\nVariable length n-gram modeling is an ensemble method\nin which the predictions of many ﬁxed-order models are\nintegrated. Ensemble methods such as boosting have been\nshown to be effective for classiﬁcation tasks [16]. Multiple\nviewpoint systems, introduced by Conklin [5], and devel-\noped by others such as Witten [5] and Pearce [15] can be\nthought of further generalizing the idea of integrating an\nensemble of predictive models. The extension is based on\nthe fact that music can be simultaneously represented in\nmany ways. For example, a melody can be thought of in\nterms of chromatic pitches, intervals, scale degrees, or con-\ntour. A rhythmic pattern can be thought of in terms of onset\ntimes, durations or position-in-bar. If, for example, we are\ntrying to predict the next note in a melody, having multiple\nrepresentations is useful in capturing structure that is ob-\nvious given one representation, but less so in another. For\nexample, a scale-degree representation of a melody might\nmake it obvious that the chromatic pitch, say B, is actu-\nally the leading tone, making it very likely that the next\nnote is C. However, if the training database contains many\nmelodies in many different keys, this might not be obvious\nfrom the chromatic pitch representation. We describe the\nmultiple viewpoints framework in Section 4.3.\nLittle work to date has been done on statistical mod-\neling of tabla. Gillet [7] and Chordia [4] both used an\nHMM framework for tabla transcription, while Bel and\nKippen [9] created a model of tabla improvisation based\non a context-free grammar, one of the earliest computa-\ntional tabla models.\nTabla is the most widely used percussion instrument in\nIndian music, both as an accompanying and solo instru-\nment. Its two component drums are played with the ﬁngers\nand hands and produce a wide variety of timbres, each of\nwhich has been named. A sophisticated repertoire of com-\npositions and theme-based improvisations has developed\nover hundreds of years. Although tabla is primarily learned\nas part of an oral tradition, it is also notated using a system\nthat indicates strokes and their durations. Unfortunately,\nthe correspondence between strokes and names is not one-\nto-one. Depending on the context and the stylistic school,\nthe same stroke will be given different names. And, in\nsome cases, different strokes will be given the same name.\nThis is unproblematic in the context of an oral tradition\nbut requires that care be taken when interpreting symbolic\nnotations.\n3. TABLA DATABASE\nThe database used for training the model is a set of tradi-\ntional tabla compositions compiled by tabla maestro Alok\nDutta [6]. The compositions were encoded in a Humdrum-based syntax called **bol that encoded the stroke name\nand duration [4]. The database which is available online\nconsists of 35 compositions in a variety of forms. Alto-\ngether there are 27,189 strokes in the dataset, composed of\n42 unique symbols.\n4.N-GRAM MODELING\nN-gram modeling is a commonly used technique to proba-\nbilistically model sequences of elements such as phonemes\nin speech, letters in a word or musical notes in a phrase.\n[13]N-grams can be efﬁciently stored in a tree-shaped\ndata structure, commonly referred to as a trie or preﬁx tree.\nFigure 1 is the trie for the sequence ABAB+C. In such\na trie, branches represent the succession of certain sym-\nbols after others, and a node at a certain level of the trie\nholds a symbol from the sequence, along with information\nabout the symbol such as the number of times it was seen\nin the sequence following the symbols above it, and the\ncorresponding probability of occurrence. In Figure 1, the\nsubscript below a symbol represents the symbols probabil-\nity given the context, deﬁned by the path through the trie\nto that node, while the superscript above it represents the\ncount value. Thus, in the topmost level, the probabilities\nrepresent the priors for the symbols. During construction\nof the trie, symbols are fed sequentially into the system\none-by-one. For the above example, after the sequence\nABAB, the trie looks like Trie1 in ﬁgure Figure 1. When\na new symbol ’C’ follows, corresponding nodes are cre-\nated at all levels of the trie: 5-gram node using ’ABABC’,\n4-gram node using ’BABC’, trigram node using ’ABC’,\nbigram node using ’BC’ and a 1-gram/prior entry for ’C’\nat the topmost level. The corresponding probabilities are\nalso updated resulting in Trie 2 in Figure 1.\nAfter the trie has been built in this manner, it can be\nused to predict the next symbol given a test sequence. This\nis done by following the nodes of the trie downwards from\nits top, in order of the symbols in the test sequence until the\nlast symbol in the sequence (and the corresponding node in\nthe trie) is reached. At that point, the probabilities associ-\nated with the children nodes represent the predicitve dis-\ntribution over the symbol set, given the observed context.\nTo allow for new symbols that may appear in the test se-\nquence and to subsequently allow for a better matching of\ntest sequences with missing or extra symbols compared to\ntraining sequences, we incorporate the concept of escape\nprobabilities into our trie structure, as described in [17].\nThe above example trie would then look like Trie3 in ﬁg-\nure Figure 1. We describe the use of escape probabilities in\nsection 4.2. For long training sequences, the depth of the\ntrie can become large and is often restricted to a maximum\norder to limit memory usage and to speed prediction given\na test sequence.\nThe modeling and evaluation framework was implemented\nin C++ as an external object in Max/MSP along with sup-\nporting patches.\n382\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)A2\n0.5\n1B2\n1\nA1\n1\nB1\n1B2\n0.5Trie 1 (’ABAB’) Trie 2 (’ABABC’) Trie 3 (’ABABC’ with Escapes)\nA1\n1\nB1\n1A2\n0.4\nB2\n1\nA1\n0.5\nB1\n1B2\n0.4\nA1\n0.5\nB1\n1C1\n0.5\nC1\n1C1\n0.5\nC1\n1C1\n0.2A2\n0.33\nB2\n0.66\nA1\n0.33\nB1\n0.5B2\n0.33\nA1\n0.33\nB1\n0.5C1\n0.33\nC1\n0.5C1\n0.33\nC1\n0.5C1\n0.17Esc(1)\n0.17\nEsc(1)\n0.33\nEsc(1)\n0.5\nEsc(1)\n0.5\nEsc(1)\n0.5Esc(1)\n0.5Esc(1)\n0.33\nEsc(1)\n0.33\n111 25\n11\n114\n2\n23\n2\n22336\n2\n1Level1\n2\n3\n4\n5(Top)Figure 1. Illustration of tries built for the sequence ’ABAB’ followed by the symbol ’C’. Superscripts represent count\nvalues, and subscripts represent probability values. Rounded boxes represent siblings, while italicized number at the left\nof a rounded box represents the total count among the siblings, which is used to calculate the ’probability’ values. Trie 3\nincludes escape probabilities.\n4.1 Escape Probabilities\nAs noted above, the zero frequency problem occurs be-\ncause, in high-order models, most n-grams will never have\nbeen observed [10]. Using a simple counting scheme, the\nmodel would assume zero probability for these unseen events,\nthereby returning inﬁnite entropy should they occur in the\ntest sequence. The solution is to reserve a small amount of\nprobability mass for events that haven’t occurred yet. This\nis done by reserving an escape proabability for each level\nof the trie. Whenever an event returns zero probability,\nit returns the escape probability instead. There are many\nways to assign the escape probability. Based on the results\nof Bell and Witten [17], we have implemented the Poisson\ndistribution method. The escape probability for each level\nis assigned by e(n) =T1(n)\nN(n), where T1is the number of\ntokens that have occurred exactly once and Nis the total\nnumber of tokens seen by the model so far.\n4.2n-gram Smoothing\nSmoothing addresses the tradeoff between the speciﬁcity\nof higher-order models (if a match can be found) and the\nreliability of the n-gram counts for lower-order models.\nSince higher order models are much sparser, many n-grams\nwill be assigned zero probability, and counts for n-grams\nthat have been observed will tend to vary greatly based on\nthe particular training database. This variance can be re-\nduced by incorporating information from lower order mod-\nels. There are two basic types of smoothing algorithms:\nbackoff models and interpolation models. Given a test\nsequence, a backoff model will search for the entire se-\nquence, and if no match is found in the trie, the processcontinues recursively after dropping the ﬁrst element of the\nsequence. The process stops once a positive match is found\nand the count for that n-gram count is greater then some\nthreshold. Interpolated smoothing, by contrast, always in-\ncoporates lower order information even if the n-gram count\nin question is non-zero.\nFor this study, two smoothing methods were primarily\nused, Kneser-Ney (KN) and an averaging method we term\n1=N. These were also compared to a simple backoff pro-\ncedure. KN was adopted because earlier work showed it to\nbe a superior smoothing method in the context of natural\nlanguage processing [3]. The basic idea of KN is to ensure\nthat lower order distributions are only used when there are\nfew, or no, counts in the higher order models. When incor-\nporating lower information, the probability is related not to\nthe true count of the n-grams but rather is proportional to\nthe number of different n-grams that it follows. An exam-\nple in music might be as follows: given a bigram consist-\ning of two rhythmic durations, where the second duration\nis transitional and not typically used on its own, we would\nnot assign a high unigram probability since it is only used\nin association with the ﬁrst duration. Implementation de-\ntails can be found in [3].\nGiven a model, with Mas the maximum order, the\nweights for each model are given by w(n) =1\nm(maxOrder \u0000n).\nIn other words, the higher orders receive greater weight\nthan the lower orders. It is worth noting what happens in\nthe case where a higher-order model has not seen a par-\nticular n-gram. In that case, even though the weight for\nthat model will be relatively higher than for a lower order\nmodel, the probability of the n-gram, which will be deter-\nmined by the escape probability, will be very small, and\n383\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)typically much smaller than the weight.\n4.3 Multiple Viewpoints\nOur focus in the work so far has been to implement a multi-\nple viewpoints system for music analysis, apply these prin-\nciples to traditional North Indian tabla compositions, and\nidentify the set of parameters that work best on this kind of\nmusic. Though we will touch upon the basics of the mul-\ntiple viewpoint system, a more detailed explanation can be\nfound here [5].\nConventional context-dependent predictive models track\nonly the most basic aspects of music, like pitch, rhythm\nand onset times. Moreover, these variables are tracked to-\ngether, so that ﬁnding an exact match for every possible\ncontext is practically impossible. A multiple viewpoints\nsystem, however, tracks each variable independently, main-\ntaining many predictive models simultaneously. The ﬁnal\nprediction is obtained by combining all these predictions\ninto a meaningful set of basic parameters (such as pitch\nand duration). Such a system not only incorporates infor-\nmation from different variables but can also model com-\nplex relationships between two or more of these variables\nand make use of that information to strengthen its predic-\ntion. Furthermore, a multiple viewpoint system can make\nmuch better predictions in rare event cases, because of its\nability to ﬁnd context matches in at least one of its many\nmodels.\nA viewpoint is nothing more than a set of events of a\nparticular type. For example, a set of all pitch classes (C,\nC#, D, D# and so on until B) would consititute a viewpoint\nfor a melody. Similarly, a viewpoint for rhythm would con-\nsist of the set of all onset times within a measure. These\ntwo viewpoints, pitch and rhythm, can be directly extracted\nfrom the music, are independent of each other and are called\nbasic types. Cross types are formed when two or more ba-\nsic types are combined and tracked simultaneously (T1 x\nT2). A cross type formed using Notes and Onset Times\nwould consist of all elements in the Notes viewpoint in\ncombination with all elements in the Onset Times view-\npoint. Each element of this viewpoint is represented as a\ntuplefNote, OnsetTimeg, instead of a single value. The\nnumber of all possible elements in a cross type is equal to\nthe product of the number of elements in each basic type.\nAderived type depends on information extracted from a\nbasic type. A simple example of this is melodic intervals,\nwhich are extracted from pitches. Derived types can use\ninformation from more than one viewpoint, and this can\nlead to the formation of cross types derived from derived\ntypes. Selection of appropriate representations is domain\ndependent and often uses prior knowledge of the music.\nHere we use two basic types – strokes and durations.\nWe also look at the following cross types: 1) Strokes x Du-\nrations and 2) Strokes x PositionInBar (PIB), where PIB\nrefers to the onset position of a stroke as a fraction of\nthe bar. Finally we introduce three derived types into the\nmodel. These were constructed by mapping the stroke\nnames to a reduced set. Reduced Set 1 was made by elim-\ninating different names for the same stroke, reducing thenumber of symbols from 41 to 32. Reduced Set 2 ex-\ntended this idea by mapping acoustically similar strokes\nto the same name, which further reduced the number of\nsymbols to 10. The open/closed mapping was made by\nclassifying each stroke as resonant or non-resonant.\n4.4 Merging Model Predictions\nAn important point here is the actual process of merging\nthe predictions of each of the models. Though there are\nmany different ways to do this, we use a weighted av-\nerage as described in [15]. Each viewpoint model is as-\nsigned a weight depending on its cross-entropy at each\ntime step. The weight for each model is given by wm=\nH(pm)=H max(pm), where H(pm)is the entropy of the prob-\nability distribution and Hmax(pm)is the maximum entropy\nfor a prediction in the distribution. Higher entropy values\nresult in lower weights. In this way, models that are uncer-\ntain (i.e., have higher entropy) make a lesser contribution\nto the ﬁnal distribution. The distributions are then com-\nbined by taking their weighted average.\n4.5 Long Term and Short Term Models\nA common limitation of such predictive models built on\nlarge databases is that the model is usually unaware of\nany patterns speciﬁc to a particular song. The model be-\ncomes too general to be effective, and very often patterns\nand predictions which seem obvious to humans are missed\nbecause they are infrequent in the global training database.\nTo solve this problem, we used two models: a long-term\nmodel (LTM) built on the entire training database, and a\nshort-term model that starts out empty and is built up as a\nparticular composition is processed. In this work, the LTM\nis not updated as the test composition is processed.\nWhen a composition is read, both models return a dis-\ntribution over the symbol set at each time step. The predic-\ntions are merged into a ﬁnal prediction using a weighted\naverage as described above. Whenever the STM is uncer-\ntain, such as the beginning of a composition or new sec-\ntion, the system gives more weight to the LTM. In other\nsections, such as the end of a song, where the STM is more\ncertain, the weighting scheme assigns more weight to the\nSTM. A comparison of the cross-entropy measure for each\nmodel is presented in Table 1.\n5. EVALUATION\nCross-validation was performed using a leave-one-out de-\nsign. For each of the 35 compositions, training of the\nLTM was performed on the remaining 34. Reported re-\nsults were averaged over all 35 trials. A common domain-\nindependent approach for evaluating the quality of the mod-\nels’ predictions is cross-entropy [11]. If the true distribu-\ntion is unknown, the cross entropy can be approximated by\n\u00001\nnPn\ni=1log2(pi), which is the mean of the entropy val-\nues for a given set of predictions. To illustrate, at a given\nstept, we note the true symbol. We then look at the predic-\ntive distribution for symbols at step t\u00001and calculate the\nentropy for the true symbol at step t. After running through\n384\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Durations Strokes Stroke-\nDuration\nOrder Priors Comb. STM LTM Priors Comb. STM LTM Priors Comb. STM LTM\n1 1.891 1.049 0.916 1.890 3.614 3.184 2.958 3.613 4.965 3.883 3.397 4.962\n5 1.891 0.563 0.510 0.897 3.614 1.117 0.994 1.780 4.965 1.294 1.162 2.354\n10 1.891 0.469 0.429 0.814 3.614 0.868 0.814 1.609 4.965 1.060 0.995 2.220\n20 1.891 0.383 0.356 0.736 3.614 0.805 0.780 1.584 4.965 1.007 0.966 2.201\nTable 1. Summary of cross-entropy results for LTM, STM, and combined models for order 1-20\n0 5 10 15 20 250123456\nOrderCross −Entropy\n  \nPriorsCombinedSTMLTM\nFigure 2. Cross-entropy for stroke prediction using LTM,\nSTM, and combined models for orders 1-20\nall the symbols in the test set, these entropies are averaged,\ngiving a cross-entropy result for that particular test set.\n6. RESULTS\nFigure 2 shows cross-entropy as a function of model or-\nder using 1=N smoothing. The cross-entropy for the order\n20 stroke LTM is 1.584, which is a surprisingly good re-\nsult given the number of symbols (42). Compared with\nusing a predictive distribution based on the prior proba-\nbility of each stroke, cross-entropy was reduced by 2.030\n(from 3.614 to 1.584). The STM entropy rate for strokes\nwas a remarkable 0.780. Combining the LTM and STM\nbased on entropy, as described in section 4.3, did not im-\nprove performance over the STM alone. The STM also\noutperformed the LTM when predicting durations (0.365\nvs. 0.736) and when jointly predicting the stroke and dura-\ntion (0.966 vs. 2.201). In both cases, the combined model\noffered no overall performance increase. Not surprisingly,\nin some cases the LTM outperformed the STM at the be-\nginning of the composition, before the STM model had\nseen much data.\nAs expected, cross-entropy decreases monotonically as\na function of model order. The curve decays roughly expo-\nnentially, with performance improving dramatically from\norder 1 to order 5, signiﬁcantly between 5 and 10, and lit-\ntle between 10 and 20. This suggests that, for these com-\npositions, memorizing more than the past 10 strokes does\n0 5 10 15 20 250123456\nOrderCross −Entropy\n  \n1/NKneser NeyFigure 3. Comparison of Kneser-Ney and 1/N smoothing\nModel Durations Strokes\nBasic 0.814 1.609\nBasic + SD 0.756 1.557\nBasic + SD entropy 0.744 1.546\nSD + PIB entropy 0.744 1.522\nTable 2. Cross-types LTM where SD is stroke X duration,\nand PIB is position-in-bar. Merging of models was done\nusing a weighted average with entropy-based weights, ex-\ncept for Basic + SD, which took the simple mean\nlittle to improve predictions. This is true for the prediction\nof durations, strokes, and joint prediction of the stroke and\nduration.\nFigure 3 shows the effect of different smoothing tech-\nniques on performance for the LTM. Both 1=N and Kneser-\nNey smoothing signiﬁcantly outperform a simple backoff\nmethod. 1=N is the clear winner for strokes, durations, and\njoint prediction. The difference in the quality of prediction\ndecreases as the model size increases but is large through-\nout. Unusually, Kneser-Ney smoothing decreases slightly\nin performance as the model order is increased from 1 to\n5.\nTable 2 shows that cross-types had a small impact on\nperformance with the addition of the stroke X duration type\nhaving the most impact.\nIn Table 3, we show the results for several LTM using\nderived stroke types, essentially more abstract sound cat-\negories based on the stroke name. For the LTM, Reduced\n385\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Derived Types Strokes\nStrokes only 1.60858\nStrokes + Reduced Set 1 1.83609\nStrokes + Reduced Set 2 1.81989\nStrokes + Open/Close Set 1.61803\nTable 3. Cross-entropy for stroke prediction using derived-\ntypes with LTM\nSet 1 and 2 decreased performance by approximately 0.2,\nwhereas open/closed marginally improved performance.\n7. DISCUSSION\nThese results suggest that tabla compositions can be ef-\nfectively encoded using a variable length n-gram model.\nGiven a set of 42 stroke symbols, the best model’s cross-\nentropy was 0.780, essentially meaning that it was on av-\nerage uncertain between 2 strokes, a dramatic reduction\nfrom the 42 strokes in the vocabulary, as well as from the\nprior distribution which corresponded to approximately 12\nstrokes. Interestingly, the results suggest that tabla compo-\nsitions exhibit strong local patterns that can be effectively\ncaptured using a STM, providing signiﬁcantly better per-\nformance when compared with the LTM alone. Because\nmany tabla compositions consist of a theme and variations,\nthis result is not surprising. These data also suggest that it\nis almost always better to only use the STM, except for the\nvery initial portion of the composition. Cross types seem\nto lead to small improvements, whereas derived types lead\nto small decreases. More experiments are needed in order\nto determine whether these changes are signiﬁcant.\nAnother important result is that smoothing can have a\nlarge impact on predictive performance and seems to be\nhighly domain dependent, with 1=N outperforming KN,\na technique that had been shown to be amongst the best in\nanother area. It is likely that the correct balancing of model\norder will depend on the database size and of course the\ndistribution of n-grams. It would be interesting if further\nwork could elucidate a clear theoretical basis for choos-\ning a given smoothing method. In the absence of this, it is\nlikely that performance could be improved by using a val-\nidation set and by adjusting how quickly weights fall off\nfor interpolated smoothing techniques as the model order\ndecreases.\n8. FUTURE WORK\nAs always, we plan to continue to encode more tabla com-\npositions to see if these results generalize. Additionally,\nwe hope to test other merging methods such as geomet-\nric combinaton, a technique shown to be superior to addi-\ntive combination in the context of melodies [11], as well\nas implementing cross and derived types for the STM. We\nalso hope to use our trained models to generate novel tabla\ncompositions and to use human evaluators to judge their\nquality. Lastly, we hope to use these results in an interac-\ntive tabla system that can anticipate and respond to a tabla\nimprovisation.9. REFERENCES\n[1] C Ames. The Markov Process as a Compositional\nModel: A Survey and Tutorial. 1989.\n[2] Jean-Julien Aucouturier, Franois Pachet, and M. San-\ndler. The way it sounds: timbre models for analysis and\nretrieval of music signals. 2005.\n[3] Stanley Chen and Joshua Goodman. An empirical\nstudy of smoothing techniques for language modeling.\nInPROCEEDINGS OF THE 34TH ANNUAL MEET-\nING OF THE ACL, pages 310–318, 1996.\n[4] Parag Chordia. Automatic Transcription of Solo Tabla\nMusic. PhD thesis, Stanford University, December\n2005.\n[5] Darrell Conklin and Ian H. Witten. Multiple viewpoint\nsystems for music prediction. 1995.\n[6] Alok E Dutta. Tabla: Lessons and Practice.\n[7] Olivier Gillet and Gael Richard. Supervised and unsu-\npervised sequence modeling for drum transcription. In\nProceedings of International Conference on Music In-\nformation Retrieval, 2007.\n[8] David Huron. Sweet Anticipation: Music and the Psy-\nchology of Expectation. MIT Press, 2006.\n[9] Bel B Kippen J. Bol Processor Grammars In Under-\nstanding Music with AI. AAAI Press, 1992.\n[10] W J Teahan John G Cleary. Experiments on the zero\nfrequency problem. 1995.\n[11] Marcus Pearce Johnston. The construction and evalua-\ntion of statistical models of melodic structure in music\nperception and cognition. PhD thesis, City University,\nLondon, 2005.\n[12] Kyogu Lee. Automatic chord recognition from audio\nusing an hmm with supervised learning. In In Proc. IS-\nMIR, 2006.\n[13] C. Manning and H. Schutze. Foundations of Statistical\nNatural Language Processing. MIT Press, 2002.\n[14] Pearce, Herrojo Ruiz, Kapasi, Wiggins, and Bhat-\ntacharya. Unsupervised statistical learning underpins\ncomputational, behavioural and neural manifestations\nof musical expectation. 2010.\n[15] Marcus Pearce, Darrell Conklin, and Geraint Wiggins.\nMethods for combining statistical models of music.\n2004.\n[16] Dietterich T.G. Ensemble methods in machine learn-\ning. 2000.\n[17] Ian H. Witten and Timothy C. Bell. The zero-frequency\nproblem: Estimating the probabilities of novel events\nin adaptive text compression. 1991.\n386\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "Quantifying the Benefits of Using an Interactive Decision Support Tool for Creating Musical Accompaniment in a Particular Style.",
        "author": [
            "Ching-Hua Chuan",
            "Elaine Chew"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1417337",
        "url": "https://doi.org/10.5281/zenodo.1417337",
        "ee": "https://zenodo.org/records/1417337/files/ChuanC10.pdf",
        "abstract": "We present a human-centered experiment designed to measure the degree of support for creating musical ac- companiment provided by an interactive composition de- cision-support system. We create an interactive system with visual and audio cues to assist users in the choosing of chords to craft an accompaniment in a desired style. We propose general measures for objectively evaluating the effectiveness and usability of such systems. We use melodies of existing songs by Radiohead as tests. Quanti- tative measures of musical distance – percentage correct and closely related chords, and average neo-Riemannian distance – compare the user-created accompaniment with the original, with and without decision support. Numbers of backward edits, unique chords explored, and repeated chord choices during composition help quantify composi- tion behavior. We present experimental data from musi- cians and non-musicians. We observe that decision sup- port reduces the time spent in composition, the number of revisions of earlier choices, redundant behavior such as repeated chord choices, and the gap between musicians’ and non-musicians’ work, without significantly limiting the range of users’ choices.",
        "zenodo_id": 1417337,
        "dblp_key": "conf/ismir/ChuanC10",
        "keywords": [
            "interactive composition decision-support system",
            "musical accompaniment",
            "objective evaluation measures",
            "melodies of Radiohead",
            "quantitative measures",
            "musicians and non-musicians",
            "reduces composition time",
            "reduces revisions",
            "reduces redundant behavior",
            "gap between musicians and non-musicians work"
        ],
        "content": "Quantifying the Benefits of Using an Interactive Decision Support Tool \nfor Creating Musical Accompaniment in a Particular Style \nChing -Hua Chuan  Elaine Chew  \nUniversity of North Florida  \nSchool of Computing  \nJacksonville, FL  \nchchuan@mail.barry.edu  University of Southern California  \nViterbi School of Engineering  \nThornton School of Music  \nLos Angeles, CA  \nechew@usc.edu  \nABSTRACT \nWe present a human-centered experiment designed to \nmeasure the degree of support for creating musical ac-\ncompaniment provided by an interactive composition de-cision-support system. We create an interactive system \nwith visual and audio cues to assist users in the choosing \nof chords to craft an accompaniment in a desired style. \nWe propose general measures for objectively evaluating the effectiveness and usability of such systems. We use melodies of existing songs by Radiohead as tests. Quanti-\ntative measures of musical distance – percentage correct \nand closely related chords, and average neo-Riemannian distance – compare the user-created accompaniment with \nthe original, with and without decision support. Numbers \nof backward edits, unique chords explored, and repeated chord choices during composition help quantify composi-\ntion behavior. We present experimental data from musi-\ncians and non-musicians. We observe that decision sup-port reduces the time spent in composition, the number of revisions of earlier choices, redundant behavior such as \nrepeated chord choices, and the gap between musicians’ \nand non-musicians’ work, without significantly limiting \nthe range of users’ choices.  \n1. INTRODUCTION \nBuilding computer-assisted composition software tools \nhas long been a common interest among researchers. Be-\nsides tools designed for professional use with advanced functions such as audio signal editing, many aim to help amateurs and music lovers write music and to enhance users’ musical creativity. These tools often provide alter-nate ways for representing music, using interfaces such as \na drawing pad to avoid score notation. While various sys-\ntems have been proposed in recent years, it remains un-\nclear how much these systems help the composition proc-ess. Evaluations of systems that support art creation often take the form of subjective opinion with varying degrees of success, and cannot be used to systematically study and improve the methodology. Music compositions, like other art forms, are often evaluated using the Consensual Assessment Technique [10], which measures quality ac-cording to experts’ global and subjective assessment of \nthe outcome. For studies that aim to design tools to sup-port music composition, this technique may not provide \nsufficient detail for evaluating the systems. For example, \nit would be difficult to determine to what extent users’ creations are improved by a composition assistance sys-tem using the Consensual Assessment Technique. \nIn this paper we focus on experiments that aim to \nquantifying the added benefit of style-imitating accompa-\nniment composition decision-support systems. Such sys-\ntems aim to help users create accompaniment to a melody in the style of a particular artist of band. When the goal is to imitate the style of a known song, the original accom-\npaniment serves as ground truth for any user-created ac-\ncompaniment to the song’s melody.  If the user is able to produce more chord patterns that are similar to the origi-\nnal accompaniment with automated assistance, it can then \nbe concluded that the system achieves its design goal. \nFor the experiments described in this paper, we built a \ncomposition decision-support software tool by adapting \nan automated system for generating style-specific accom-paniment. We designed an intuitive interactive user inter-face that assumes no formal music knowledge, using vis-\nual cues and sound to make music composition feasible \nfor experts and novices alike. The system provides sug-gestions to the user at two levels: the system generates a \nsequence of stylistically appropriate chords as initial sug-\ngestions, a range of triads is then listed to guide users in each chord selection.  \nTo measure the benefits of the system, we conducted a \nhuman-centered experiment with three composition tasks. In each task, participants were given a melody and were requested to complete the composition by choosing \nchords to accompany the melody in the style of a particu-\nlar artist. Before the experiment, the participants listen to several songs by the artist they should emulate. In the \nfirst task, participants were given a composing interface \nwithout decision support; in the latter two tasks, they worked with a composing interface with decision-support that put forward sequences of computer-suggested \nchords. The experiment was designed to explore the composition process by examining whether participants could mimic an artist’s composition decisions after hav-\ning heard samples of their songs, and by determining how \nmuch their performance improved when they worked with a composition decision-support system. \nWe separated the participants into two groups – musi-\ncians and non-musicians – according to their musical backgrounds, and evaluated the effectiveness of the com-position decision-support tool by examining whether non-\nmusicians, with the help of the system, can create accom-\n471\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \npaniments that are as close to the original style as those \nby musicians. We propose quantitative metrics for evalu-ating the system’s usability by statistically examining the changes in the users’ composition behaviors with and \nwithout decision support.  \n2. RELATED WORK \nIn addition to the systems that are designed to model mu-\nsic composition [1, 4, 13, 15], efforts have been made to \nbuild tools to support music composition [2, 7, 8]. Unlike \nmusic production software that only provide tools for no-\ntating scores and recording sound, software such as \nHyperscore [8] and Sonic Sketchpad [7] offer alternative modalities (e.g., drawing) through which to compose mu-\nsic. With this kind of software, users do not need to be \nfamiliar with, or knowledgeable about, music notation or \ninstrument-specific characteristics. Evaluations of these \nsystems focus on users’ level of satisfaction with the \nsoftware interface and its functionalities. \nIn particular, the automatic style-specific accompani-\nment (ASSA) system described in [5] aims to assist ama-\nteurs in songwriting by helping them compose music in \nthe style they desire. The system incorporates both music \ntheoretic knowledge and statistical learning, using small \nnumbers of examples of users’ favorite artists to model the accompaniment choices of that professional. It then \napplies the learned composition style to new, user-created melodies. The ASSA system has been evaluated based on \nsubjective opinion through a Turing test, and on style-\nrelated quantitative metrics [6]. \n3. SYSTEM DESIGN \n3.1 Interactive User Interfaces \nIn this study, the ASSA system [5] forms the basis of our composition decision-support tool. The interactive user interface allows users to create accompaniment to any \nmelody through simple mouse-click actions on graphic \nicons. Users can explore different chords in each bar and \nlisten to parts of the melody in any order with the accom-\npaniment they create. Users can also adjust the tempo of \nthe song to better fit their listening pace during composi-\ntion. The system takes care of composition details such as voice arrangement and chord alignment, leaving users \nfree to concentrate on using their aural perception to se-\nlect the appropriate chords. \nTo test the improvement resulting from computer as-\nsistance in music composition, we created two interface designs. The first interface, shown in Figure 1, provides users with all possible chord choices at all times, i.e. without decision support. The chord collection, all 24 tri-\nads (major and minor), appears in the top panel of the in-\nterface. Triads are arranged according to the circle-of-\nfifths, with the major chord cycle on the left and the mi-\nnor triad cycle  relative  to  the  major  on the  right.  The \n \nFigure 1 . Composing interface without ASSA support. \n \nFigure 2 . Composing interface with ASSA support and \ngraph of neo-Riemannian distance. \ntonic triad is centered at the top. Each triad is assigned a \ndifferent color to help users identify chords and the rela-\ntionships between them. We used the color assignment described in [12], in which related colors are assigned to chords considered close one to another.  \nDuring composition, users can choose chords by click-\ning on one of the colored circles in the chord collection. Once a chord is selected, a corresponding triad with \nproper pitch arrangements is inserted in the accompani-\nment MIDI track, and a circle of the same color is placed in the composing panel, the black rectangle in the center of Figure 1. The composing panel allows users to visually \nexamine the chord sequence they create. Major triads ap-\npear as larger colored disks, minor triads as smaller ones.  \nFigure 2 depicts the second interface. The most visible \ndifference between this interface and the previous one is \nthe graph at the bottom of the interface. Each point on the grid represents the neo-Riemannian distance [3] between \nadjacent chords. In neo-Riemannian chord space, chords are connected by neo-Riemannian operations (NROs), \nand the number of operations between chords reflects their musical distance. The user can select the time slice \nto be examined, and edit the neo-Riemannian distance \nthey wish to consider at that time slice. Other chords within that number of NROs from the previous chord are \ndisplayed in the top panel. In this manner, the number of \nchord choices is reduced and constrained by the NRO \n472\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \ndistance, which helps users categorize chords into sub-\ngroups that reflect their musical distance in the transition. \nIn addition to the use of neo-Riemannian distance, the \nuser starts with an ASSA-produced chord sequence and \nits corresponding sequence of neo-Riemannian distances. \nThe providing of this initial suggestion simplifies the \nprocess of creating accompaniments from scratch to one \nof editing the chords until the user is satisfied with the \naccompaniment. \n4. EXPERIMENT DESIGN \nOur study aims to determine the effectiveness of a com-\nposition decision-support system through quantitative comparisons of users’ compositions, and of their decision pathways, with and without decision support.  This sec-tion presents our experiments to further these goals. \n4.1 Experiment Setup \nWe designed a three-part experiment using the two com-\nposition interfaces described in the previous section. In \nthe first part of the experiment, participants were given a melody extracted from a song with which they are famil-iar, and were asked to compose a sequence of chords \nwithout decision support, using the interface shown in \nFigure 1. In parts two and three, participants were pre-sented with the interface shown in Figure 2, and asked to \ncompose accompaniments for two unfamiliar melodies in \na familiar style with decision support. \nWe used songs by Radiohead, a British rock band, for \nthe experiment. We chose Radiohead not only because of \nthe band’s popularity and reputation, but also because \ntheir unique style has been analyzed by music theorists in several book-length studies [9, 11, 14]. We selected 13 \nsongs from Radiohead’s albums Pablo Honey  and The \nBends  as samples to familiarize participants with the \nband’s style and as the training set for the ASSA system. \nAs test melodies for the three-part experiment, we extract \nthe melodies of three “hit” songs from the two albums, as indicated by their popularity ratings on the iTunes online music store. The participants’ training samples included \nthe test song for the first part, but not for the second and \nthird parts. Each participant was asked to complete all three parts in the experiment. The idea behind these \nchoices was to investigate whether participants, espe-\ncially non-musicians, could create accompaniments for new melodies (the second and third tasks) in a style simi-lar to the original with decision support. \n4.2 Experiment Procedure \nThe participants for this study consisted of 26 volunteers: 8 musicians and 18 music lovers. The majority of the \nnon-musician participants were college students from \nvarious disciplines. Participants who were categorized as \nmusicians either majored in Music at a university or \nplayed an instrument in a rock band. Participants were \nrequested to spend at least three days listening to the sample Radiohead songs provided, and to schedule an appointment for the experiment after they are familiar with the band’s style.  \nThe experiment was conducted in a computer lab, \nwhere each participant was assigned a computer and a headset to complete the task individually. Before begin-ning, participants first watched a 10-minute instructional \nvideo on how to use the two interfaces to create accom-\npaniments. Before they started composing with the soft-ware, participants were reminded to choose chords that \nthey think Radiohead would choose, based on their expe-rience listening to music by the band. Participants were \nthen left alone to compose without time constraints. \n4.3 Evaluation Metric \nThis section describes the evaluation criteria designed to fulfill the aforementioned goals. \n4.3.1 Effectiveness \nThe effectiveness of system assistance can be measured \nin many ways.  If musicians as well as non-musicians take less time to complete their compositions with the de-cision support, we can say that the system helps users by \nspeeding up the composition process. \nDecision support effectiveness can also be measured in \nthe improvement of musicians’ and non-musicians’ com-\npositions produced with (vs. without) decision support, \nand of non-musicians’ compositions with regard to their \nsimilarity to musicians’ compositions. For instance, if \nnon-musicians create accompaniments more similar to \nthe ones by musicians with decision support than without, \nit can be claimed that the decision support helps non-musicians make more knowledgeable decisions to better \nreach the targeted music style.  \nTo measure the quality of the user-created accompa-\nniments, we compared them to that of the original pro-duction, as documented in the commercial sheet music. \nWe counted the number of chords in the user-created ac-companiment that are identical to the original, and di-vided it by the total number of chords to report the same \nchord percentage. For the chords in the user-created ac-\ncompaniment that are different from the original, we fur-ther determined the percentage of chords that are closely \nrelated to the original. The metric, chords-in-grid,  reports \nthe percentage of chords that are related to the original by \na Dominant (D), Subdominant (S), Parallel (P), Relative \n(R), or by one of the compound relations: DP, SP, DR, \nand SR. We also measured the average NR distance  be-\ntween the user-created and the original accompaniment to assess how dissimilar the user-chosen chords are from the \noriginal in terms of NROs. \n4.3.2 Usability \nAnother goal of the study is to examine how the decision \nsupport system affects users’ composition processes. We \nanalyzed the changes in the way chords are selected (without decision support) or edited (with decision sup-\n473\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \nport) to investigate whether the decision support guides \nusers in their compositions or limits their choices. Spe-cifically, we focused on three types of behavior patterns: backward editing , exploration , and hesitation . \nBackward editing  occurs when users change the chord \nin a bar that is before the current one after they select a \nchord in the current position. For example, the following \naction sequence contains two backward edits at times T\n2 \nand T 4, respectively: \nBar:   5 4 5 3 \nChord selected: C Am G Dm \nTime:   T1 T2 T3 T4 \nBackward editing  indicates that the user has become un-\ncertain about a previous choice they made because of a \nlater decision.  \nWe define exploration as the average number of \nunique chords that users have tried assigning in each bar, including the ones they choose and then changed later. For example, in the scenario above, two unique chords \nwere explored in bar 5 at time T\n1 and T 3 respectively. In \ncontrast, hesitation  is defined as the total number of re-\npeated chord choices during the entire composition proc-ess. For instance, the following sequence contains a re-\npeated choice at time T\n4 in bar 6: \nBar:   5 6 6 6 \nChord selected: C Am C Am \nTime:   T1 T2 T3 T4 \nFor each of the two groups, musicians and non-\nmusicians, we calculated the means and standard devia-tions of the backward editing , exploration , and hesitation  \nvalues with and without decision support. Suppose µ\n1, m, b \nrepresents the true mean of musicians’ backward editing without decision support and µ\n2, m, b  the true mean with \ndecision support. We conducted a hypothesis test on \nwhether decision support has a positive impact on the be-\nhavior pattern (i.e. reduces backward editing) by setting the null hypothesis, H\n0, and the alternative hypothesis, \nH1, as: \nH0: µ1, m, b ≤ µ2, m, b \n               H 1: µ1, m, b > µ2, m, b                         (1) \nWe calculated the observed level of significance, the p-value, using the Student’s t-distribution with the calcu-\nlated means and standard deviation.  We chose the Stu-dent’s t-distribution over the normal distribution due to the small sample size.  We then compare the p-value with \nthe standard 5% significance level.  Similar hypothesis \ntests were conducted as well for the exploration and hesi-tation mean values. \n5. EXPERIMENT RESULTS AND ANALYSES \nIn this section, we report the results of the experiments, \nand describe the analyses of these results.  Twenty-six \nparticipants volunteered for the experiment, each spend-ing approximately about one hour in completing the tasks.  These users, and their composition results, are separated into two groups, namely musicians and non-\nmusicians, according to their musical backgrounds. \n5.1 Effectiveness: Inter-Group Comparisons \nThis section presents analyses of composition decision \nsupport effectiveness within the musician and non-musician groups. \n5.1.1 Task Completion Time \nTable 1 lists the average time musicians and non-\nmusicians spent on each composition task. For song 1, which was included in the participants’ training samples, \nmusicians and non-musicians spent an almost equal \namount of time choosing the 15 chords for the accompa-\nniment without decision support. For songs 2 and 3, which were new to the participants, non-musicians spent \na slightly shorter amount of time generating an accompa-niment with an initial computer-produced suggestion fol-lowed by edits guided by the neo-Riemannian interface. \nIf we calculate the time spent on each chord in the ac-companiment, we can observe that both musicians and non-musicians spent significantly less time on the second \nand third task than on the first.  \nTime (min)  Song 1  \n(15 chords)  Song 2  \n(36 chords)  Song 3  \n(30 chords)  \nMusicians  17.54  20.59  12.76  \nNon-musicians  17.51  14.13  7.77 \nTable 1.  Time spent on each accompaniment task. \n5.1.2 Created Accompaniment versus the Original \nFigure 4 shows the mean same chord , chords in grid , and \naverage NR distance  percentages for the user-created ac-\ncompaniments. The results of accompaniments produced \nwith decision support are the average of that for song 2 \nand song 3.  \n \nFigure 4 . Evaluation of user-created accompaniments.  \nFigure 4(a) shows that the average percentage of chords \nin the user-created accompaniment that are identical to those in the original accompaniment is much higher in \ncompositions by musicians than in those by non-\nmusicians. This performance gap between musicians and \nnon-musicians is greatly reduced with the assistance of \nthe ASSA system. Similar patterns can be observed in \nFigure 4(b) with the chords-in-grid  metric and in Figure \n4(c) with the average NR distance  metric. \n474\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \nIf we focus on the musicians’ results, we will find that, \neven with decision support, musicians performed worse \non songs 2 and 3, when they are unfamiliar with the test song, than on song 1, when they had heard the song in its \nentirety. This finding implies that it is more difficult for \nmusicians to come up with exactly the same chords as the original if they have never heard the song before. In con-\ntrast, non-musicians chose more suitable chords for songs \n2 and 3, in collaboration with the system, than they did \nfor song 1. With decision support, as in songs 2 and 3, the \nperformance of non-musicians approached that of musi-\ncians. \nNote that in the bars labeled without  in Figures 4(a) \nand 4(b), the standard deviation for the percentage of \nsame chords  for the musicians’ accompaniments is much \ngreater than that for chords-in-grid  for the same group. \nThis difference is due to the fact that the musician group consists of classical music majors as well as rock band \nmembers. It appears that imitating Radiohead’s style is a \nchallenge for some classical musicians who were less fa-\nmiliar with the band than rock musicians. However, be-\ncause of their training in music theory, their choices are \nclosely related to the original as the high chords-in-grid  \nvalues suggest. \n5.2 Usability: Intra-Group Comparisons \nThis section presents analyses of user composition pat-\nterns, with and without decision support, comparing the results of musicians against that of non-musicians. \n5.2.1 Backward Editing \nFigure 5 presents the average number backward edits  \nwith and without decision support for (a) musicians and \n(b) non-musicians. Note that for both musicians and non-musicians, decision support reduces the amount of back-\nward editing . The results are confirmed statistically by \nthe p-values; both the p-values for musicians (p-value = \n0.0066) and non-musicians (p-value = 0.015) are less \nthan 0.05, indicating that the reduction in backward edits is statistically significant. \n \nFigure 5 . Average backward edit counts with and without the \ndecision support. \n5.2.2 Exploration \nFigure 6 shows the average number of unique chords ex-\nplored in each bar for (a) musicians and (b) non-musicians, with and without the decision support. \n \nFigure 6 . Average number of unique choices explored in each \nbar with and with the decision support.  \nWhile both musicians and non-musicians explore fewer \nnumbers of unique chords on average with system sup-\nport, the p-values are higher than 0.05.  Thus, the ob-\nserved reduction in unique chords explored is not statisti-\ncally significant, and we conclude that the system does not significantly limit participants’ exploration. \n5.2.3 Hesitation \nThe average number of repeated choices in each bar with \nand without decision support are shown in Figure 7 for (a) musicians and (b) non-musicians. It can be observed \nthat the number of repeats is reduced when the system \nprovides suggestions during composition, as reflected in the figures as well as the p-values. The result implies that \nthe system helps musicians and non-musicians reach their \ngoals with less confusion. \n \nFigure 7 . Average number of repeated choices in each bar with \nand without decision support.  \n6. CONCLUSION AND FUTURE WORK \nIn this paper we described an interactive decision support \nsystem that aims to assist amateurs in the creating of ac-\ncompaniments in a desired style. The system uses visual \ncues to offer chord suggestions and sounds to evaluate chord choices, making composition feasible for people without formal musical training. To investigate the bene-\nfits associated with the decision support, we designed \nthree composition tasks. In the first task, participants were provided with the interface without decision sup-\nport, and asked to create an accompaniment for a melody \nof a familiar song. In the latter two tasks, participants were given the interface with decision support, and in-\nstructed to produce accompaniments for unfamiliar melo-\ndies in a familiar style.  \n475\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \nWith the data obtained in the experiment, we first ana-\nlyzed how well the user-created accompaniments con-\ncurred with the desired style, separating the results by \nuser group, using the quantitative measures: same chord \nand closely-related chord percentages, and average neo-\nRiemannian distance. We observed that when decision \nsupport was present, the performance gap between musi-\ncians and non-musicians was greatly reduced.  \nTo understand the usability of the system, we proposed \nmethods to measure the changes in participants’ composi-tion behavior. Statistical analyses revealed that when de-\ncision support was available, the number of backward ed-its and repeat choices were reduced, and the range of unique chords explored was not significantly limited by \nthe system.  \nAdditional analyses that can be performed on the data \ninclude examining whether common or distinct decision patterns exist between the two user groups, and separat-\ning the support given by the initial ASSA-produced se-quence from the assistance provided by the interactive interface and differentiating between them. \nIn the future, we plan to extend the experiment by con-\nsidering other musical factors such as timbre and instru-ment arrangement to investigate the impact of different \nstimuli on musical creativity. We will refine the interface \nand conduct studies with larger groups of users, and de-sign questionnaires to obtain their feedback on the sys-\ntem. Last but not least, we will further study the degree to \nwhich automation affects music creativity so as to design better composition decision support tools. \n7. ACKNOWLEDGEMENTS \nWe thank the volunteers who participated in the experi-\nment, including students and alumni of the University of Southern California and of Barry University. This work was supported in part by NSF Grant No. 0347988.  Any opinions, findings, and conclusions or recommendations \nexpressed in this material are those of the authors, and do \nnot necessarily reflect those of the NSF. \n8. REFERENCES \n[1] M. Allan and C. K. I. Williams: “Harmonising \nChorales by probabilistic inference.” Proceedings of \nthe Neural Information Processing Systems Conference , Vancouver, pp. 25–32, 2004. \n[2] J. Bamberger: “Developing Musical Intuitions: A Project-Based Introduction to Making and Understanding Music.”  Oxford University Press, \nUSA. \n[3] G. Capuzzo: “Neo-Riemannian theory and the \nanalysis of pop-rock music.” Music Theory \nSpectrum , Vol. 26, No. 2, pp. 177–199, 2004. \n[4] H. Chan and D. Ventura: “Automatic Composition of Themed Mood Pieces.” Proceedings of the 5\nth International Joint Workshop on Computational Creativity , Madrid, pp. 109–115, 2008. \n[5] C. H. Chuan and E. Chew: “A Hybrid System for Automatic Generation of Style Specific \nAccompaniment.” Proceedings of the 4th \nInternational Joint Workshop on Computational \nCreativity , London, pp. 57–64, 2007. \n[6] C. H. Chuan and E. Chew: “Evaluating and Visualizing Effectiveness of Style Emulation in Musical Accompaniment.” Proceedings of the 9th \nInternational Conference on Music Information Retrieval , Philadelphia, pp. 57–62, 2008. \n[7] T. Coughlan and P. Johnson: “Interaction in Creative Tasks: Ideation, Representation and \nEvaluation in Composition.” Proceedings of ACM \nConference on Human Factors in Computing \nSystems , Montréal, Canada, pp. 531-540, 2006. \n[8] M. M. Farbood, H. Kaufman, and K. Jennings: \n“Composing with Hyperscore: an Intuitive Interface for Visualizing Musical Structure.” Proceedings of \nInternational Computer Music Conference , 2007. \n[9] D. Griffiths: Radiohead’s OK Computer (Thirty Three and a Third series). Continuum, New York, 2004. \n[10] M. Hickey: “More or Less Creative? A Comparison of the Composition Processes and Products of \n‘Highly-Creative’ and ‘Less-Creative’ Children \nComposers.” www.faculty-\nweb.at.northwestern.edu/music/hickey/Exeter/Exeter.pdf. \n[11] N. Hubbs: “The Imagination of Pop-Rock \nCriticism.” Expression in Pop-Rock Music: Critical \nand Analytical Essays  (2nd edition). Routledge, \nNew York, pp. 215–237, 2008. \n[12] A. Mardirossian and E. Chew: “Visualizing Music: \nTonal Progressions and Distributions.” Proceedings \nof the 8th International Conference on Music Information Retrieval , Austrian Computer Society, \nVienna, pp. 189–194, 2007. \n[13] M. T. Pearce and G. A. Wiggins: “Evaluating Cognitive Models of Musical Composition.” \nProceedings of the 4\nth International Joint Workshop \non Computational Creativity , London, pp. 73–80, \n2007. \n[14] J. Tate:  Music and Art of Radiohead . Ashgate, \nAldershot UK, 2005. \n[15] N. Vempala and S. A. Dasgupta: “Computational \nModel of the Music of Stevie Ray Vaughan.” \nProceedings of the 6th Creativity and Cognition \nConference , Washington D.C., pp. 203–212, 2007. \n476\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "Computational Analysis of Musical Influence: A Musicological Case Study Using MIR Tools.",
        "author": [
            "Nick Collins"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1416756",
        "url": "https://doi.org/10.5281/zenodo.1416756",
        "ee": "https://zenodo.org/records/1416756/files/Collins10.pdf",
        "abstract": "Are there new insights through computational methods to the thorny problem of plotting the flow of musical influ- ence? This project, motivated by a musicological study of early synth pop, applies MIR tools as an aid to the inves- tigator. Web scraping and web services provide one an- gle, sourcing data from allmusic.com, and utilising python APIs for last.fm, EchoNest, and MusicBrainz. Charts of influence are constructed in GraphViz combining artist sim- ilarity and dates. Content based music similarity is the sec- ond approach, based around a core collection of synth pop albums. The prospect for new musical analyses are dis- cussed with respect to these techniques.",
        "zenodo_id": 1416756,
        "dblp_key": "conf/ismir/Collins10",
        "keywords": [
            "computational methods",
            "musicological study",
            "MIR tools",
            "web scraping",
            "web services",
            "Python APIs",
            "GraphViz",
            "artist similarity",
            "dates",
            "content based music similarity"
        ],
        "content": "COMPUTATIONAL ANALYSIS OF MUSICAL INFLUENCE: A\nMUSICOLOGICAL CASE STUDY USING MIR TOOLS\nNick Collins\nDepartment of Informatics, University of Sussex, Falmer, Brighton, BN1 9QJ, UK\nN.Collins@sussex.ac.uk\nABSTRACT\nAre there new insights through computational methods to\nthe thorny problem of plotting the ﬂow of musical inﬂu-\nence? This project, motivated by a musicological study of\nearly synth pop, applies MIR tools as an aid to the inves-\ntigator. Web scraping and web services provide one an-\ngle, sourcing data from allmusic.com, and utilising python\nAPIs for last.fm, EchoNest, and MusicBrainz. Charts of\ninﬂuence are constructed in GraphViz combining artist sim-\nilarity and dates. Content based music similarity is the sec-\nond approach, based around a core collection of synth pop\nalbums. The prospect for new musical analyses are dis-\ncussed with respect to these techniques.\n1. INTRODUCTION\nMusicians have always been aware of issues of musical in-\nﬂuence, from the lists of inﬂuences set out in adverts for\nnew band members, the intensive relationship of teachers\nand pupils in many traditions, to composers consciously\nadmitting their predecessors through interviews, personal\njournals, and in some cases unconscious or deliberate quo-\ntations. Whilst it is convenient to focus on grand examples\nin a ‘genius’ model of musical history, all eras of music\nhave had a host of active musicians, though no era more\nthan today’s hyper-warren of content creators. Chopin’s\nletters, for example, are littered with references to other\nactive pianist-composers of the day, most of whom are no\nlonger household names, yet Chopin writes ‘I shall not be\nan imitation of Kalkbrenner: hehas not the power to extin-\nguish my perhaps too audacious but noble wish and inten-\ntion to create for myself a new world’ [9, p. 103]. The\nliterature on human creativity is of note here in explor-\ning the processes of human invention within the engine of\nculture [5, 15]. Musicologists have re-cast traditional con-\ncerns over inﬂuence to questions of ‘inter-textuality’, and\nthe degree to which any musical work can be seen as dis-\ntinct from social and musical currents [16]. Inﬂuence is\nintimately connected to the continuous negotiation of mu-\nsical style as it transforms over time; the gradual formation\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2010 International Society for Music Information Retrieval.of genres is implicit in much discussion of the philosophy\nof stylistic categories in music [1, 10], and related to sim-\nilar questions in biology concerning speciation events and\nmemetics [4, 6].\nAutomated methods for the analysis of musical similar-\nity provide a new angle on relationships between works,\nwhether comparing individual pieces or within larger cor-\npora. For example, the data-driven analyses explored by\nDavid Cope across MIDI ﬁles [3] are primarily used for\nsynthesis, but can also help to explore the links between\ncomposers. Symbolic analysis tools in MIR parallel such\nmovements in algorithmic composition: McKay and Fu-\njinaga [11] discuss the application of their jSymbolic fea-\nture extractor and Autonomous Classiﬁcation Engine ma-\nchine learning tool to such projects as comparison between\na Chopin nocturne and Mendelssohn piano trio, or distin-\nguishing de Machaut and Palestrina. Charles Smith has\ncarried out perhaps the largest musicological study of in-\nﬂuence amongst classical composers by a series of mea-\nsures applied over library resources, and presents it in a\nwebsite describing the ‘Classical Music Universe’.1\nThere are many MIR studies which have analyzed the\ncurrent state of public opinion on artist similarity, for pur-\nposes of tracking popularity and making recommendations.\nZadel and Fujinaga [19] combine cultural meta-data from\nAmazon with a metric of similarity based on Google search\ncounts to generate a network of related artists through web\nservices. Fields et al. [8] scraped MySpace pages, trac-\ning the recursive (to sixth degree of separation) network\nof friends and evaluating musical similarity through audio\ncontent analysis of their sound examples. They mention\ninﬂuence as one potential link between artists, but do not\nunpack it from collaboration or general similarity. Park et\nal. [13] also study the network structure of artists, by scrap-\ning online music databases such as allmusic.com, but con-\ncentrate on collaboration or ‘expert’ annotated similarity\nrather than any explicit tie to dates. Again tackling MyS-\npace, Beuscart and Couronn ´e [2] namecheck inﬂuence in\ntheir title, but mean it as a general measure of recommen-\ndation amongst cliques of artists rather than as a formative\ninﬂuence on creative output.\nThus, although the topics of similarity and genre remain\ncentral tenets of much music information retrieval work,\nthe role of dates as markers of the ﬂow of inﬂuence is not\nso widely discussed. This paper makes dates a central part\nof a musicological investigation. The applicability of MIR\n1http://people.wku.edu/charles.smith/music/index2.htm\n177\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)tools to studies of inﬂuence both via online meta-data pars-\ning and content based analysis applications is explored. In\nthe latter content analysis, a tight knit set of synth pop al-\nbums from the years 1977-1981 are put under the micro-\nscope, providing a real challenge for discrimination and a\nmicrocosm of gradual stylistic change.\nSection 2 introduces the context of synth pop as well as\nthe central node, Depeche Mode (DM). Section 3 presents\nweb scraping and web service exploration of the network\nof artists around DM, with a technique to automatically\nextract dates for artists using the MusicBrainz web ser-\nvice. Network diagrams are constructed through python\nprograms and GraphViz. Section 4 tackles the inﬂuence\nquestion using a set of synth pop albums from the era up\nto four years before the ﬁrst DM release, looking for au-\ntomatic recognition of possible leads on inﬂuence through\nmusical similarity (marsyas is the tool of choice here). Re-\nsults and future extensions are discussed.\n2. SYNTH POP AND DEPECHE MODE\nThe cost of analog synthesizers decreased in the 1970s,\nuntil an all synthesizer band was a viable proposition for\nmusicians starting out in the post punk era [14]. Although\nthere are always earlier precedents, and synths had been\nlong known in popular music through such phenomena as\nmass selling Moog albums, prog rock keyboardists, and\nkrautrock, a real concentration of synth led bands emerged\nin the later 1970s into a position of mainstream chart suc-\ncess. The Second Invasion of the US by British bands\non the back of MTV featured a plethora of synthesized\nsounds, and the 1980s saw even greater availability of elec-\ntronic equipment as digital technology stole the show. Al-\nthough some ‘New Romatic’ bands such as Duran Duran\nhad only a single keyboardist, the more central examples\nof synth pop tend to feature all synthesizer backing, in-\ncluding sequencers and drum machine in place of acoustic\ndrummers, after the Kraftwerk model; but like all supposed\ncategories, inbetween cases exist.\nDepeche Mode were by no means the ﬁrst synth pop\nband, nor the ﬁrst with popular market appeal; both Kraftwerk\nas an all electronic band, and Gary Numan as an individ-\nual who featured synthesizers, had had greater commercial\nsuccess than their ﬁrst album was to achieve. Yet in longer\nterm commercial and artistic success, impact and inﬂu-\nence, DM are still of great importance, and a fascinating\nsubject of study in terms of tracking inﬂuence. They have\ntouched multiple putative genres, from early teen synth\npop, through darker industrial sampling, to electronic tinged\nstadium rock,2and inspired divergent artists (as one ex-\nample, see covers compilations such as For the Masses\n(1998) or the Swedish synth pop tribute I Sometimes Wish\nI Was Famous (1991)). Early DM is also of note in that\nVince Clarke was the chief songwriter, rather than Martin\nL. Gore, and such complications bring home the challenges\nof tracking a band’s inspirations and inﬂuence through ex-\n2Arguably, after DM’s best selling Violator (1990), even converging\nwith U2 for 1991’s Achtung Baby , as U2 chased the contemporary sound\nof electronic dance music’s commercial break throughtended careers.3In another example of the complications,\nas bands progress through multiple albums, they work with\nmany people, often bringing in younger producers who\nemerged in historically later scenes (in DM’s case, such\nas Flood, or Mark Bell). The network of musicians who\ninﬂuenced, and who were inﬂuenced by, Depeche Mode\nare examined in a web data analysis, and work historically\nclosely prior to the album Speak & Spell is the focus for\nthe audio content analysis.\nStatements by band members past and present provide\ninsight into the formative inﬂuences of the band. For ex-\nample, in Miller’s biography [12], the band admit early\ninﬂuences including Gary Numan (p.21), OMD and the\ntrack ‘Almost’ (p.23), The Human League and particularly\n‘Being Boiled’ (p.483), Kraftwerk (p.25) and John Foxx\n(p.26). DM gigged early on with the post-Foxx incarnation\nof Ultravox, and their label owner and ﬁrst producer was\nthe British DIY synth pioneer Daniel Miller; Mute Records\nartists would remain a central touchpoint as the band devel-\noped. Such references are further discussed below.\n3. WEB SCRAPING AND WEB SERVICES FOR\nTHE ANALYSIS OF INFLUENCE\nAlthough a musicologist might construct their own model\nof musical inﬂuence from analyzing primary and secondary\nsources such as original releases, reviews and interviews,\nthe wealth of online commentary and databases provides a\nfurther strand of evidence for systematic musicology to ex-\nploit. Although there can be issues with the veriﬁability of\ninformation, the much remarked problems of reliability of\nmeta-data in MIR [7], it can still be healthy to admit web\ncontent as part of the arsenal of the musicologist. This pa-\nper examines the use of web scraping and web services to\ncollect alternative viewpoints on the inﬂuences upon and\ninﬂuence of a particular central band. Although the tech-\nniques may be applied to any starting point, Depeche Mode\nare chosen in particular for this study.\nThe following APIs and websites were investigated:\n\u000fallmusic.com artist information explicitly contains\nentries for ‘Inﬂuenced By’ and ‘Followers’\n\u000fThe EchoNest API has convenience methods to ob-\ntain biographic data, lists of similar artists, and a\nmeasure of ‘familiarity’ for a given artist.\n\u000fThe MusicBrainz metadatabase has an API which\nallows interrogation of artist releases and dates.\n\u000fThe last.fm API can return a list of similar artists\namongst further functionality\nPrograms were written in python to utilise the APIs, and\nfor web scraping.\nThree tactics could generate graphs of related artists\nwith direction of edges determined by date, using recursive\nconstruction. In the ﬁrst case a similarity measure from a\n3A similar radical change of personnel is seen for example in The\nHuman League’s development in 1980.\n178\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Figure 1. Excerpt of a graph of inﬂuences based around Depeche Mode, ﬁltered by familiarity ratings of at least 0.6 per\nartist according to EchoNest. Note the errors and omissions in dating information, for example the start date for Cliff\nRichard, who appears via a supposed second order inﬂuence from Roxy Music. The overall graph, even relaxing the\nfamiliarity ﬁltering, is much richer for precedents than for successors, perhaps reﬂecting the balance of music history and\njournalism with regard to the present day.\nstarting artist was used, and dates imposed as a way of de-\ntermining directivity (most simply, earliest date of activity\nof a given artist, ignoring career overlaps). In the second,\nthe allmusic.com site was scraped for the pre-marked In-\nﬂuenced By and Followers lists, which gave the direction\nof arrows without needing dates (however, dates were still\nannotated with artist names in this case as a helpful guide;\nsimilarity ratings between artists could also rate strength of\nconnection). The third method is for a musicologist to pro-\nvide a list of artists for which they are interested in inter-\nconnections; they can then try various similarity measures\nto weight connections, and dates can be automatically de-\ntermined where they are not already known.4\nBecause the computational search should be as auto-\nmated as possible if larger networks are to be generated,\nmusicbrainz.org provided the ability to hunt for start and\nend dates of artists (the musicologist can always further\ncorroborate dates later if any promising links are revealed).\nThis was actually one of the hardest coding problems to\nsolve, because for the start date MusicBrainz returns the\ndate of birth of an individual artist, but the date of forma-\ntion for a group. Code was written for individuals and for\ncases where there was no returned valid start or end date,\nto hunt through associated album and single release dates,\ntaking minima and maxima. The API often failed to re-\nspond when called too often, but the program kept trying\nwith increasing gaps between calls until connection was\nre-established or it failed ten times in a row. All dates were\nstored in a local database to avoid the slow dependency on\nMusicBrainz, only checking artists if they were new to the\ndatabase or no date had yet been established in previous\nattempts (the musicologist can also in principle overwrite\ndates if they discover more reliable data).5\n4As a proviso to this process, different data sources and similarity\nmeasures reﬂect different construction principles from ‘expert’ annota-\ntion (allmusic) to community consensus (MusicBrainz), and the analyst\nshould keep this in mind.\n5As pointed out by a reviewer, an alternative model here may be toThe ﬁnal large networks of artists could be directly plot-\nted, but facilities were also added to cluster by the year an\nartist began their recording career,6and to exclude artists\nfalling below a certain threshold of familiarity with respect\nto the EchoNest measure. Figure 1 shows an excerpt of a\ngraph generated via the allmusic database method, recurs-\ning only up to second order connections, and annotating\ndates of artists via MusicBrainz.\nIt was deﬁnitely of beneﬁt to spend time with the tech-\nnology and with online opinion as a method of immer-\nsion into the subject. Results however must be interpreted\nwith caution; in particular, the allmusic.com annotations\nfor synth pop artists did not expose some expected links\n(for example, Depeche Mode as an inﬂuence on Alphav-\nille, the Pet Shop Boys, or Goldfrapp, to name but three,\nthough Camouﬂage and Nine Inch Nails did appear as suc-\ncessors; The Human League were not listed as an inﬂu-\nence despite DM’s own documented confession). They did\nhowever point to a few other possible leads worth pursu-\ning. Some second order links, such as Elvis Presley and\nChuck Berry were admitted by Martin L.Gore as his earli-\nest listening in a recent interview7, though the centrality\nof such ﬁgures, particularly with respect to the Beatles hub,\nis a little too obvious and a likely side effect of dominant\nnodes in artist networks [13].\nIt was found in practice that similarity of artists as a\nsingular term often proved insufﬁcient, in that it did not\nadequately respect musical characteristics over social. For\nexample, Pandora, which in any case admits no API, lists\nsimilar artists to DM as The Cure, New Order, Duran Du-\nran, The Smiths and Tears for Fears. There is one justiﬁ-\nexploit DBpedia and LinkedData for the Semantic Web.\n6All artists are in development off the commercial radar for a long\ntime, and formative inﬂuences not necessarily via mass released record-\nings; but the underlying assumption to keep this project manageable is\nthat a commercial release reveals the potential to inﬂuence a large num-\nber of followers.\n7http://www.bbc.co.uk/programmes/b00jn4ﬂ\n179\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)cation as UK bands all active circa 1983 or so, but in terms\nof tracing the history of synth pop, there are a few overlaps\nand some mishits (The Smiths, for example). Data was\nalso not always reliable; EchoNest listed Duran Duran Du-\nran, the breakcore artist, instead of Duran Duran; arguably\nthere is more electronic sound in the former, though it is\nprobably an erroneous appearance in this context.\n4. CONTENT BASED ANALYSIS\nThe other angle of approach in this project was to examine\nthe actual audio recordings for similarity between artists.\nArmed with associated date information for tracks, net-\nworks of prior art can be constructed. Though there is not\nalways causal proof that the authors of Speak and Spell\nwould have heard and actively internalised particular tracks\nby prior artists (though see comments above in section 2\nconcerning admitted inﬂuences such as The Human League),\nit is possible to speculate, guided by such an investigation,\nand hope in general that the search provides a pillar in ac-\ncumulating evidence for a particular linkage.\nTable 2 contains a list of 37 albums or compilations,\ncorresponding to 364 tracks, selected as the target data and\nspace for musicological ground truth. The choice of al-\nbums reﬂects our own analysis of possible formative in-\nﬂuences, with a bias to British acts, and covers the years\n1977-1981, during which electronic instrumentation was\nbreaking through to mass use in popular music (there are\nmany earlier precedents, but the scope of inquiry was ar-\nranged around the post punk years transitioning to the early\n80s). Depeche Mode were gigging in 1980, and released\ntheir ﬁrst singles in 1981 leading up to the Speak & Spell\nalbum that October. There are many other artists and re-\nleases of potential relevance, both outside and within the\nrestricted dates, but the core set is large enough to pro-\nvide ample scope for musicological investigation and pose\na signiﬁcant challenge for MIR technology.8For digital\nconvenience, despite originals being chieﬂy released on LP\n(CDs arrived in 1982), all data was sourced from purchased\nCD recordings, since these provide a guaranteed profes-\nsional transition from master tapes. A few were re-masters\n(as noted in the table), with possible changes in overall\ncompression and loudness, but since this did not substan-\ntially impact on human listening, in the ideal computer\nanalysis should be able to cope (the timbral features used\nhere did not include amplitude measurements as compara-\ntors). Any bonus tracks not readily available in the orig-\ninal era of release were excluded, which typically meant\nremoving any tracks not on an original LP. Release dates\nwere cross-referenced from online sources such as allmu-\nsic and discogs.com as well as liner notes and textbooks.\nOur primary interest was to analyze relevant recordings\nthat might show a strong similarity to tracks on Speak &\nSpell, and thus see if computer analysis could spot any\nlinks of inﬂuence. A secondary interest was the analysis of\nearly synth pop’s properties in general. There were various\n8Possibilities for extensions just with artists active in this period in-\nclude Telex, Joy Division/New Order, Throbbing Gristle, Jean-Michel\nJarre and Wendy Carlos to name a fraction.opinions and discoveries here from conventional listening,\nbut the computer offered an alternative perspective.\nMarsyas [17] and weka [18] provided the tools of choice\nfor audio feature extraction, similarity measurement, and\nmachine learning. The 44.1kHz 16 bit audio recordings\nwere each passed through marsyas’ bextract algorithm to\nobtain single vector averaged MFCC and spectral features\nover one minute 30 second sections taken from the middle\nof each track (window size and hop size 1024 samples).\nThese obtain a long exposure timbral summary vector (64\ndimensions) for each track. Similarity values between all\nindividual tracks could then be created. A python script\nwas written to order similar songs from a given starting\nsong across the database. The nearest and furthest neigh-\nbours from each track on Speak &Spell were listed; Table 1\ngives example results for the nearest and furthest ten tracks\nto the second DM single ‘New Life’.\nScore Artist Album Track\n1.047 DM Dreaming Of Me Speak & Spell\n1.116 Gary Numan Replicas We Have A Technical\n1.133 Ultravox Systems Of Romance Just For A Moment\n1.150 Gary Numan The Pleasure Princi-\npleRandom\n1.152 Gary Numan Telekon I’m An Agent\n1.153 OMD Orchestral Manoeu-\nvres In The DarkRed Frame White Light\n1.155 Ultravox Vienna Vienna\n1.211 Ultravox Vienna Mr. X\n1.221 Gary Numan Replicas Replicas\n1.234 YMO Solid State Surviver Day Tripper\n: : : : : : : : : : : :\n2.275 YMO Yellow Magic Or-\nchestraComputer Game Theme\nFrom The Invader\n2.320 Devo The Essentials Girl U Want\n2.324 Cabaret\nV oltaireThe Original Sound\nOf ShefﬁeldDo The Mussolini\n(Headkick)\n2.339 OMD Architecture &\nMoralityArchitecture And\nMorality\n2.373 Human\nLeagueReproduction Blind Youth\n2.490 Cabaret\nV oltaireThe Original Sound\nOf ShefﬁeldBaader Meinhof\n2.593 John Foxx Metamatic Plaza\n2.620 Human\nLeagureReproduction Medley Austerity Girl\nOne\n2.623 YMO Yellow Magic Or-\nchestraComputer Game Theme\nFrom The Circus\n3.151 Human\nLeagueDare The Sound Of The\nCrowd\nTable 1. Maximally similar and dissimilar tracks to ‘New\nLife’ by Depeche Mode within the database\nSome results were not so surprising; both other DM\nsingles from the ﬁrst album are close by (Just Can’t Get\nEnough comes in at 18th closest). Further away, the Baader\nMeinhof track is dark and unsettling and not rhythmic. The\nlow bit arcade timbre of the YMO computer game themes\nare unique amongst materials here. John Foxx’s Plaza fea-\ntures a prominent ﬂanging effect. On the other hand, in\nmusical terms the many distant up tempo Human League\ntracks, or the close appearance of Vienna are somewhat\nsuspicious. The Sound of the Crowd persistently came\nfar from all tracks on Speak and Spell, perhaps due to the\nloudly mixed vocal and particular synth percussion sounds.\n180\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)The album annotated feature data also underwent ma-\nchine learning algorithm investigation, by training classi-\nﬁers to differentiate artists’ releases. The musicological in-\nterest is to ﬁnd points of failure of discrimination as insight\ninto potential timbral/musical overlaps and thus through\ninformation on dates, promising leads on the ﬂow of in-\nﬂuence, Confusion matrices help to indicate this. Under\n10-fold validation, the best results were 31.8% correctly\nclassiﬁed instances, for a Support Vector Machine (SVM)\nclassiﬁer; related to some other classes, Speak & Spell\nfared badly, with 2/12 tracks accurately labelled (precision\n0.133, recall 0.167) and confusions for example with Re-\nproduction by the Human League and Penthouse and Pave-\nments by Heaven 17. Setting aside concerns over the per-\nceptual relevance of the timbral features, it is challenging\nto ask for all 37 albums to be well differentiated on the\nbasis of this data set (averaging 10 songs per label). As\na more reasonable test, the data was labelled by the ten\ngroupings shown by the horizontal lines in Table 2 (keep-\ning Speak & Spell as a class of its own), obtaining 77% ac-\ncurate classiﬁcations with an SVM. The confusion matrix\nfor the DM album then showed 11 out of 12 songs accu-\nrately classiﬁed, and one mislabelled as by Gary Numan.\nClassiﬁcation by year was also explored, despite con-\ncerns over the hard histogram boundaries; classiﬁcation\naccuracy of 55% was obtained, conﬁrming somewhat the\nclosely linked artists in this set (classiﬁcation by half year\nperiods dropped to 26%). Out of interest, I also tested how\nwell recent artist La Roux’s eponymous 2009 album was\ndifferentiated from the original synth pop sources to which\nit might be argued to pay substantial homage; in actual fact,\nwhen offered as a sixth category in the year based analysis\n9/12 tracks were correctly identiﬁed; closely similar tracks\nwere mainly drawn from the same album. Whether this\nis best traced to female vocals, to recent production and\nmastering trends, is a subject for further investigation.\nAny machine identiﬁed link must be followed up by hu-\nman analysis before imbuing signiﬁcance. It is clear in\nthe audio content analysis that gross timbral features are\nthe basis of comparison, not details of materials at the hu-\nman perceptual timeframe. These timbral links might indi-\ncate similar equipment or studio facilities more than links\nof inspiration. Nonetheless, it is valuable to make a start\nhere in applying such MIR tools, on the understanding that\nthrough intensive research efforts in computational audi-\ntory models, systems can only improve. It was clear in\nthis project that the machine tools were utilising different\ncriteria. For example, a similarity was observed in motifs\nbetween a section two minutes into the Fad Gadget track\n‘Ricky’s Hand’ and DM’s ‘Photographic’ but this was not\nclosely borne out in machine results (similarity was right\nin the middle of all tracks, at 165th most similar); this is\nprobably due to the smearing effects of the average con-\ncealing that particular section.\nNone of the leads presented by content analysis are them-\nselves a smoking gun of inﬂuence when date is taken into\naccount. It is preferable to seek further corroboration of\nany potential inﬂuence, and the status of conscious tributeor unconscious plagiarism will never be amenable to audio\nanalysis methods alone. For the artists concerned, a single\nlisten to a live gig or work in progress in the studio, ob-\ntaining a promotional copy ahead of public release, might\nall have provided undocumented links; this study was re-\nstricted to release dates, not recording dates. Certain off al-\nbum tracks were excluded, for example songs only played\nat gigs (‘Television’ in the case of DM), or particular B\nsides (‘Ice Machine’, Shout’),9all of which might turn\nout to be potential connections.\n5. CONCLUSIONS\nThis paper investigated the question of inﬂuences from two\ntechnological approaches, the ﬁrst online information seek-\ning, and the second content analysis over a database of rel-\nevant audio. Such a study can point to new leads that mu-\nsicologists may not have immediately heard or imagined.\nAlthough there is some danger of getting back what is al-\nready known, in the main the great advantage has been the\nstimulus of exploring the materials from a new perspec-\ntive. There may be more links between tracks in a close\nknit era of releases than the musicologist can comfortably\ntrack, and computer assistance certainly helps direct in-\nquiry, prompting possibilities of connection, even as the\nhuman ear currently remains the best ﬁnal judge.\nMuch future work remains, from further web data sourc-\ning tools, to more developed schemes for content analysis.\nFor the former, similarity through co-occurence is a prof-\nitable tool, and any similarity network can be mediated\nthrough the automatic artist date database. For the latter,\nmore developed similarity methods may compare subsec-\ntions and simultaneous voices within songs, perhaps with\nbeat or chord synchronous features. A musicologist may\nwish to focus on particular attributes, choosing weights for\nrhythmic, timbral, harmonic, melodic features and more.\nFor different pairs of songs, links might be posited based\non different parameters, and a more developed analysis\nsystem would ﬂag up signiﬁcant similarities with respect\nto a number of different weighting schemes. The methods\ninvestigated herein do not track the career of artists stage\nby stage, nor cope with any complex inter-linked develop-\nments. A solution may combine content based methods\nand accurate dating of releases.\n6. REFERENCES\n[1] J. J. Aucouturier and F. Pachet. Representing musical genre: A state\nof the art. Journal of New Music Research, 32(1):83–93, 2003.\n[2] J.-S. Beuscart and T. Couronn ´e. The distribution of online reputa-\ntion: Audience and inﬂuence of musicians on myspace. In Third In-\nternational AAAI Conference on Weblogs and Social Media, pages\n187–190. Association for the Advancement of Artiﬁcial Intelligence,\n2009.\n[3] David Cope. Virtual Music : Computer Synthesis of Musical Style.\nMIT Press, Cambridge, MA, 2001.\n[4] R. Dawkins. The Selﬁsh Gene. Oxford University Press, New York,\nNY , 1975.\n9The opening of Ice Machine bears a resemblance to elements of The\nWord Before Last on the Human League’s Reproduction album.\n181\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Artist Album Original Release Date Notes\nKraftwerk Trans-Europe Express May 1977\nKraftwerk The Man Machine May 1978\nKraftwerk Computer World May 1981 2009 remaster\nDavid Bowie Low January 1977 Remaster 1999, Brian Eno production of synth instru-\nmentals in particular of note\nDavid Bowie Heroes October 1977 Remaster 1999, Second of Berlin triology; few synths\nDavid Bowie Lodger May 1979 Remaster 1999, very few synths, last of Eno/Bowie\ncollaboration\nDevo The Essentials: Devo 1978-1981 Singles from 2002 collection\nGiorgio Moroder From Here to Eternity July 1977 Lays down a template for electronic dance music\nDonna Summer selected tracks 1977-1979 Giorgio Moroder/Pete Bellotte production with\nprominent synth instrumentation, from The Dance\nCollection (1987), Once Upon a Time (1977) and Bad\nGirls (1979), including the single I Feel Love (1977)\nSparks No. 1 in Heaven September 1979 Giorgio Moroder produced\nHuman League Reproduction October 1979 remaster 2003\nHuman League Travelogue May 1980 remaster 2003\nHuman League Dare! 20 October 1981 Martin Rushent produced, after split in original Hu-\nman League line-up\nHeaven 17 Penthouse and Pavement September 1981 Some members of original Human League\nCabaret V oltaire The Original Sound of\nShefﬁeld ‘78/‘82. Best Of;1978-1981 2002 remaster\nUltravox Ultravox! 25 February 1977 more guitar band at the earlier stages\nUltravox Ha!Ha!Ha! 14 October 1977 synthesizer creeping in\nUltravox Systems of Romance 8 September 1978 Conny Plank producer, last with John Foxx\nUltravox Vienna 11 July 1980 Conny Plank producer\nUltravox Rage in Eden 7 September 1981 Conny Plank producer\nJohn Foxx Metamatic 17 January 1980\nJohn Foxx The Garden 25 September 1981\nYellow Magic Orchestra Yellow Magic Orchestra 25 November 1978 even includes 8 bit computer game music\nYellow Magic Orchestra Solid State Survivor 25 September 1979 more pioneering Japanese synth pop\nVisage Visage November 1980 Containing ’Fade to Grey’\nBuggles The Age of Plastic Jan 1980 Containing ’Video Killed the Radio Star’\nOrchestral Manoeuvres in\nthe Dark (OMD)eponymous debut 22 February 1980 2003 remaster\nOMD Organisation 24 October 1980 2003 remaster\nOMD Architecture and Morality 8 November 1981 2003 remaster\nGary Numan and Tubeway\nArmyReplicas April 1979 Numan grabs the pop centre ground\nGary Numan The Pleasure Principle September 1979\nGary Numan Telekon 5 September 1980\nGary Numan Dance September 1981 Chart impact wanes\nSilicon Teens Music for Parties 1 September 1980 Daniel Miller’s prototype synth pop group\nFad Gadget Fireside Favourites 1 September 1980\nFad Gadget The Best Of Fad Gadget 1980-1981 tracks up to 1981\nDepeche Mode Speak & Spell October 5 1981(ﬁrst single\nFeb 20, 1981)2006 remaster\nTable 2. Table of recordings under comparison\n[5] Ir `ene Deli `ege and Geraint A. Wiggins, editors. Musical Creativ-\nity: Multidisciplinary Research in Theory and Practice. Psychology\nPress, London, 2006.\n[6] D. C. Dennett. Darwin’s Dangerous Idea: Evolution and the Mean-\nings of Life. Simon and Schuster, 1996.\n[7] J. Stephen Downie. Music information retrieval. Annual Review of\nInformation Science and Technology, 37:295–340, 2003.\n[8] Ben Fields, Michael Casey, Kurt Jacobson, and Mark Sandler. Do you\nsound like your friends? Exploring artist similarity via artist social\nnetwork relationships and audio signal processing. In Proceedings of\nthe International Computer Music Conference (ICMC), 2008.\n[9] Arthur Hedley, editor. Selected Correspondence of Fryderyk Chopin.\nWilliam Heinemann Ltd, London, 1962.\n[10] Scott Johnson. The counterpoint of species. In John Zorn, editor, Ar-\ncana: Musicians on Music, pages 18–58. Granary Books, Inc., New\nYork, NY , 2000.\n[11] C. McKay and I. Fujinaga. Style-independent computer-assisted ex-\nploratory analysis of large music collections. Journal of Interdisci-\nplinary Music Studies, 1(1):63–85, 2007.[12] Jonathan Miller. Stripped: Depeche Mode. Omnibus Press, London,\n2003.\n[13] Juyong Park, Oscar Celma, Markus Koppenberger, Pedro Cano,\nand Javier M. Buld ´u. The social network of contemporary popular\nmusicians. International Journal of Bifurcation and Chaos (IJBC),\n17:2281 – 2288, 2007.\n[14] Simon Reynolds. Rip It Up and Start Again. Faber and Faber Limited,\nLondon, 2005.\n[15] Robert J. Sternberg, editor. Handbook of Creativity. Cambridge Uni-\nversity Press, Cambridge, 1999.\n[16] Michael Talbot, editor. The Musical Work: Reality or Invention? Uni-\nversity of Liverpool Press, Liverpool, 2000.\n[17] George Tzanetakis and Perry Cook. Marsyas: a framework for audio\nanalysis. Organised Sound, 4:169–175, 2000.\n[18] Ian H. Witten and Eibe Frank. Data Mining: Practical Machine\nLearning Tools and Techniques (2nd Ed). Morgan Kaufmann Pub-\nlishers, San Francisco, 2005.\n[19] Mark Zadel and Ichiro Fujinaga. Web services for music information\nretrieval. In Proceedings of the International Symposium on Music\nInformation Retrieval, Barcelona, Spain, 2004.\n182\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "A Comparative Evaluation of Algorithms for Discovering Translational Patterns in Baroque Keyboard Works.",
        "author": [
            "Tom Collins",
            "Jeremy Thurlow",
            "Robin C. Laney",
            "Alistair Willis",
            "Paul H. Garthwaite"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1416070",
        "url": "https://doi.org/10.5281/zenodo.1416070",
        "ee": "https://zenodo.org/records/1416070/files/CollinsTLWG10.pdf",
        "abstract": "We consider the problem of intra-opus pattern discovery, that is, the task of discovering patterns of a specified type within a piece of music. A music analyst undertook this task for works by Domenico Scarlattti and Johann Sebas- tian Bach, forming a benchmark of ‘target’ patterns. The performance of two existing algorithms and one of our own creation, called SIACT, is evaluated by comparison with this benchmark. SIACT out-performs the existing algo- rithms with regard to recall and, more often than not, pre- cision. It is demonstrated that in all but the most care- fully selected excerpts of music, the two existing algo- rithms can be affected by what is termed the ‘problem of isolated membership’. Central to the relative success of SIACT is our intention that it should address this particu- lar problem. The paper contrasts string-based and geomet- ric approaches to pattern discovery, with an introduction to the latter. Suggestions for future work are given.",
        "zenodo_id": 1416070,
        "dblp_key": "conf/ismir/CollinsTLWG10",
        "keywords": [
            "Pattern Discovery",
            "Translational Patterns",
            "Baroque Keyboard Works",
            "Domenico Scarlatti",
            "Johann Sebastian Bach",
            "SIACT Algorithm",
            "Music Analysis",
            "Geometric Approaches",
            "Algorithm Evaluation",
            "Music Information Retrieval"
        ],
        "content": "A COMPARATIVE EV ALUATION OF ALGORITHMS FOR DISCOVERING\nTRANSLATIONAL PATTERNS IN BAROQUE KEYBOARD WORKS\nTom Collins\nThe Open University, UK\nt.e.collins@open.ac.ukJeremy Thurlow\nUniversity of Cambridge\njrt26@cam.ac.ukRobin Laney, Alistair Willis,\nand Paul H. Garthwaite\nThe Open University, UK\nABSTRACT\nWe consider the problem of intra-opus pattern discovery,\nthat is, the task of discovering patterns of a speciﬁed type\nwithin a piece of music. A music analyst undertook this\ntask for works by Domenico Scarlattti and Johann Sebas-\ntian Bach, forming a benchmark of ‘target’ patterns. The\nperformance of two existing algorithms and one of our own\ncreation, called SIACT, is evaluated by comparison with\nthis benchmark. SIACT out-performs the existing algo-\nrithms with regard to recall and, more often than not, pre-\ncision. It is demonstrated that in all but the most care-\nfully selected excerpts of music, the two existing algo-\nrithms can be affected by what is termed the ‘problem of\nisolated membership’. Central to the relative success of\nSIACT is our intention that it should address this particu-\nlar problem. The paper contrasts string-based and geomet-\nric approaches to pattern discovery, with an introduction to\nthe latter. Suggestions for future work are given.\n1. INTRODUCTION\nThis paper discusses and evaluates algorithms that are in-\ntended for the following task: given a piece of music in\na semi-symbolic representation, discover so-called transla-\ntional patterns [14] that occur within the piece. Transla-\ntional patterns (in the geometric sense) are discussed fur-\nther in Sec. 1.1. Although they are not the only type of pat-\ntern that could matter in music analysis, many music ana-\nlysts would acknowledge that such a discovery task forms\npart of the preparation when writing an analytical essay\n[6]. Even if the ﬁnal essay pays little or no heed to the dis-\ncovery of translational patterns, neglecting this preparatory\ntask entirely could result in failing to mention something\nthat is musically very noticeable, or worse, very important.\nHence we are motivated by the prospect of automating the\ndiscovery task, as it could have interesting implications for\nmusic analysts (and music listeners in general), enabling\nthem to engage with pieces in a novel manner. We also\nconsider this task to be an open problem within music in-\nformation retrieval (MIR), so attempting to improve upon\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2010 International Society for Music Information Retrieval.current solutions is another motivating factor. The algo-\nrithms are applied to Baroque keyboard pieces by Scarlatti\n(Sonatas L1 and 10) and—to ensure common ground with\nexisting work [7, 13]—Bach (Preludes BWV 849 and 854).\nTwo existing algorithms and one of our own creation are\nevaluated by comparing their output with a music analyst’s\n(the second author’s) independent ﬁndings for these same\nkeyboard pieces (Sec. 4).\n1.1 Review of existing work\nIn MIR there do not seem to be clear distinctions between\nthe terms pattern ‘discovery’ [5,8,14,16], ‘extraction’ [10,\n11, 17], ‘identiﬁcation’ [7, 9] , and ‘mining’ [3], at least in\nthe sense that most of the papers just cited address very\nsimilar discovery tasks to that stated at the beginning of\nSec. 1. Conklin & Bergeron [5] give the label ‘intra-opus’\ndiscovery to concentrating on patterns that occur within\npieces. An alternative is ‘inter-opus’ discovery, where pat-\nterns are discovered across many pieces of music [5, 9].\nThis makes it possible to gauge the typicality of a partic-\nular pattern relative to the corpus style. Terms that are\nclearly distinguished in MIR are pattern ‘discovery’ and\n‘matching’ [4]. Pattern matching is the central process in\n‘content-based retrieval’ [18], where the user provides a\nquery and then the algorithm searches a music database\nfor more or less exact instances of the query. The out-\nput is ranked by some measure of proximity to the origi-\nnal query. This matching task is quite different from the\nintra-opus discovery task, where there is neither a query\nnor a database as such, just a single piece of music, and\nno obvious way of ranking an algorithm’s output. While\nwe have stressed their differences, some authors attempt\nto address both discovery and matching tasks in the same\npaper [12, 13], suggesting that representations/algorithms\nthat work well for one task might be adapted and applied\nfruitfully to the other.\nSome attempts at pattern discovery have been made with\naudio representations of music [15]. However, we, like\nthe majority of work cited in this section, begin with a\nsemi-symbolic representation, such as a MIDI ﬁle. Work\non semi-symbolic representations can be categorised into\nstring-based [2,3,5,8–11,16,17] and geometric approaches\n[7, 12–14], and which approach is most appropriate de-\npends on the musical situation. For instance the string-\nbased method is more appropriate for the excerpt in Fig. 1.\nWe propose that the most salient pattern in this short ex-\n3\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)!!  Lento  q = 50 tempo rubato\n\"!!! #\nOntime:172\n0$71\n167$$\n647169\n2$$$$72 71\n$$\n67\n3$$$\n647169\n4$$$72\n71\n5$$ $$\n6974\n6$$69\n!$%\n72\n7$$\n7167$$\n6471\n869$$$$$$\n9$$%\n335Figure 1. Bars 1-3 of the Introduction from The Rite of\nSpring by Igor Stravinsky, annotated with MIDI note num-\nbers and ontimes in crotchets starting from zero. For clar-\nity, phrasing is omitted and ornaments are not annotated.\ncerpt consists of the notes C5, B4, G4, E4, B4, A4, ignor-\ning ornaments for simplicity. The simplest way to discover\nthe three occurrences of this pattern is to represent the ex-\ncerpt as a string of MIDI note numbers and then to use an\nalgorithm for pattern discovery in strings. The string 72,\n71, 67, 64, 71, 69, ought to be discovered, and the user re-\nlates this back to the notes C5, B4, G4, E4, B4, A4. The\ngeometric method is not appropriate here because each oc-\ncurrence of the pattern has a different rhythmic proﬁle.\nOn the other hand, the geometric method is better suited\nto ﬁnding the most salient pattern in Fig. 2a, consisting of\nall the notes in bar 13 except the tied-over G4. This pattern\noccurs again in bar 14, transposed up a fourth, and then\nonce more at the original pitch in bar 15. Each note is an-\nnotated with its relative height on the stave (or morphetic\npitch number [14]), taking C4 to be 60. Underneath the\nstave, ontimes are measured in quaver beats starting from\nzero. The ﬁrst note in this excerpt, G3, can be represented\nby the datapoint d1= (0; 57), since it has ontime 0 and\nmorphetic pitch number 57. A scatterplot of morphetic\npitch number against ontime for this excerpt is shown in\nFig. 2b. Meredith et al. [14] call the set of all datapoints\nrepresenting an excerpt a dataset, denoted by D. Restrict-\ning attention to bars 13-15, we begin with the dataset\nD=fd1;d2; : : : ;d26g: (1)\nApattern is deﬁned as a non-empty subset of a dataset. In\nour example, we will choose to look at the patterns\nP=fd1; : : : ;d8g;andQ=fd9;d11; : : : ;d17g:(2)\nThe vector that translates d1tod9is\nd9\u0000d1= (3;60)\u0000(0;57) = (3 ;3) =v: (3)\nWe have given this vector a label v= (3; 3). It is this\nsame vector vthat translates d2tod11,d3tod12; : : : ;d8\ntod17. Recalling the deﬁnitions of PandQfrom (2), it\nis more succinct to say that ‘the translation of Pbyvis\nequal to Q’. This translation is indicated in Fig. 2c.\nLooking at Fig. 2c it is evident that as well as Qbe-\ning a translation of P, pattern Ris also a translation of P.\nMeredith et al. [14] call fP; Q; Rgthetranslational equiv-\nalence class ofPinD, notated\nTEC (P; D ) =fP; Q; Rg: (4)\nThe TEC gives all the occurrences of a pattern in a\ndataset.\n!\"#$\n!%#$\n!&#$0246810606570\nOntimeMorphetic pitch number'$($)$*$+$,$-$.$/$''$'($')$'*$'+$'.$',$'0$'-$(0$('$(($()$(*$(,$(+$'/$(.$(-$(/$)'$)($)+$\n)0$)*$))$\n0246810606570\nOntimeMorphetic pitch number\n!\"#\"$\"!\"!\"  [Allegro]#13$717069686264%%%%$74 737271%%%%$71 70696869%%%%74 74%%%3&Ontime:610%%!!571582593%%%%%!!6046162561576%%%%%!!6475859896064#106962116760%%%%%%%%%Figure 2. (a) Bars 13-16 of the Sonata in C major L3\nby Scarlatti, annotated with morphetic pitch numbers and\nontimes; (b) each note from the excerpt is converted to a\npoint consisting of an ontime and a morphetic pitch num-\nber. Morphetic pitch number is plotted against ontime, and\npoints are labelled in lexicographical order d1tod35; (c)\nthe same plot as above, with three ringed patterns, P; Q; R .\nArrows indicate that both QandRare translations of P.\nSoPis an example of a translational pattern, as trans-\nlations of P, namely QandR, exist in the dataset D. The\nformal deﬁnition of a translational pattern is as follows.\nDeﬁnition. For a pattern Pin a dataset D, the pattern P\nis atranslational pattern if there exists at least one sub-\nsetQofDsuch that PandQcontain the same number\nof elements, and there exists onenon-zero vector vthat\ntranslates each datapoint in Pto a datapoint in Q.\nIn the example in Fig. 2, two dimensions were considered\n(ontime and morphetic pitch number). The deﬁnitions and\npattern discovery algorithms given by Meredith et al. [13]\nextend to kdimensions; speciﬁcally MIDI note number\nand duration are included as further dimensions.\nThe string-based method is not so well suited to Fig. 2a.\nThe ﬁrst step would be voice separation, generating per-\nceptually valid melodies from the texture. Sometimes the\nscoring of the music makes separation simple [9], but even\nwhen voicing contains ambiguities, there are algorithms\nthat can manage [1, 3]. Supposing fragments of the pat-\ntern in Fig. 2a were discovered among separated melodies,\n4\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)!\"#\"$\"%\"Figure 3. A Venn diagram (not to scale) for the number of\npatterns (up to translational equivalence) in a dataset. The\ntotalEis shown relative to the number typically returned\nbySIATEC (F),COSIATEC (G), and SIACT (H).\nthese fragments still would have to be correctly reunited.\nIn this instance, even the most sophisticated string-based\nmethod [5] does not compare with the efﬁciency of the ge-\nometric method. The key difference between geometric\nand string-based approaches is the binding of ontimes to\nother musical information in the former, and the decou-\npling of this information in the latter. Both are valid meth-\nods for discovering patterns in music.\nThe reporting of existing intra-opus algorithms will of-\nten mention running time [3, 8, 12–14], occasionally recall\nis given [11, 17], and sometimes precision [10]. With the\ninter -opus discovery task an algorithm’s output is seldom\ncompared with a human benchmark [5,9]. The justiﬁcation\nis that ‘investigations of entire collections require consider-\nable amounts of time and effort on the part of researchers’\n[9, p. 171]. Still, is it not worth knowing how an algorithm\nperforms on a subset of the collection?\n2. ALGORITHMS FOR PATTERN DISCOVERY\nIn equation 2, pattern Pwas introduced without explain-\ninghow it is discovered. It could be discovered by cal-\nculating all the TECs in the dataset D, and then certainly\nTEC (P; D )will be among the output. However this ap-\nproach is tremendously expensive and indiscriminate. It is\nexpensive in terms of computational complexity, as there\nare2npatterns to partition into equivalence classes, where\nn=jDjis the size of the dataset. Moreover, it is indis-\ncriminate as no attempt is made to restrict the output in\nterms of ‘musical importance’: while Pis arguably of im-\nportance, not all subsets of Dare worth considering, yet\nthey will also be among the output. The set Ein Fig. 3\nrepresents the output of this expensive and indiscriminate\napproach.\nTherefore Meredith et al. [14] restrict the focus to a\nsmaller set F, by considering how a pattern like Pismax-\nimal. Recalling (1) and (2), the pattern Pis maximal in\nthe sense that it contains all datapoints that are translatable\nin the dataset Dby the vector v= (3; 3). It is called a\nmaximal translatable pattern [14], written\nP=MTP (v; D) =fd2D:d+v2Dg: (5)Meredith et al.’s [14] structural inference algorithm (SIA)\ncalculates all MTPs in a dataset, which requires O(kn2)\ncalculations. While the TEC of each MTP must still be de-\ntermined to give the set Fin Fig. 3, this approach is enor-\nmously less expensive than partitioning 2npatterns and\ninvolves a decision about musical importance: ‘In music,\nMTPs often correspond to the patterns involved in percep-\ntually signiﬁcant repetitions’ [14, p. 331]. SIA works by\ntraversing the upper triangle of the similarity matrix\nA=0\nBBB@d1\u0000d1d2\u0000d1\u0001\u0001\u0001dn\u0000d1\nd1\u0000d2d2\u0000d2\u0001\u0001\u0001dn\u0000d2\n............\nd1\u0000dnd2\u0000dn\u0001\u0001\u0001dn\u0000dn1\nCCCA:(6)\nIf the vector w=dj\u0000diis not equal to a previously cal-\nculated vector then a new MTP is created, MTP (w; D),\nwithdias its ﬁrst member. Otherwise w=ufor some\npreviously calculated vector u, in which case diis included\ninMTP (u; D ). So it is possible to determine the set Ffor\na dataset Dby ﬁrst running SIA on the dataset and then\ncalculating the TEC of each MTP. The structural inference\nalgorithm for translational equivalence classes (SIATEC)\nperforms this task [14], and requires O(kn3)calculations.\nTo our knowledge there are two further algorithms that\napply the geometric method to pattern discovery: the cov-\nering structural inference algorithm for translational equiv-\nalence classes (COSIATEC) [13] and a variant proposed by\nForth & Wiggins [7]. COSIATEC rates patterns accord-\ning to a heuristic for musical importance and produces a\nsmaller output than SIATEC, the set labelled Gin Fig. 3.\nThe name COSIATEC derives from the idea of creating a\ncover for the input dataset:\n1. Run SIATEC onD0=D, rate the discovered pat-\nterns using the heuristic for musical importance, and\nreturn the pattern P0that receives the highest rating.\n2. Deﬁne a new dataset D1by removing from D0each\ndatapoint that belongs to an occurrence of P0.\n3. Repeat step 1 for D1to give P1, repeat step 2 to de-\nﬁneD2fromD1, and so on until the dataset DN+1\nis empty.\n4. The output is\nG=fTEC (P0; D0); : : : ; TEC (PN; DN)g: (7)\nForth & Wiggins’ variant on COSIATEC [7] uses a non-\nparametric version of the heuristic for musical importance\nand requires only one run of SIATEC. While only one run\nreduces the computational complexity of their version, it\ndoes mean that the output is always a subset of F, whereas\nrunning SIATEC on successively smaller datasets (steps 2\nand 3 above) makes it possible to discover patterns beyond\nF(the portion GnF in Fig. 3).\n3. THE PROBLEM OF ISOLATED MEMBERSHIP\nIn Sec. 2 we noted that pattern Pfrom (2) could be dis-\ncovered by running SIA on the dataset Dfrom (1). This\n5\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)is because Pis the MTP (cf. equation 5) for the vector\nv= (3; 3)andSIA returns all such patterns in a dataset.\nHowever, Dis a conveniently chosen example consisting\nonly of bars 13-15 of Fig. 2a. How might an MTP be af-\nfected if the dataset is enlarged to include bar 16? Letting\nD+=fd1; : : : ;d35g;v= (3;3); (8)\nit can be veriﬁed that\nP+=MTP (v; D+) =fd1; : : : ;d8;d18;d19;d22g:\n(9)\nUnfortunately P+, the new version of P, contains three\nmore datapoints, d18,d19,d22, that are isolated tempo-\nrally from the rest of the pattern. This is an instance of\nwhat we call the ‘problem of isolated membership’. It\nrefers to a situation where a musically important pattern\nis contained within an MTP, along with other temporally\nisolated members that may or may not be musically im-\nportant. Intuitively, the larger the dataset, the more likely\nit is that the problem will occur. Isolated membership af-\nfects all existing algorithms in the SIA family, and could\nprevent them from discovering some translational patterns\nthat a music analyst considers noticeable or important (see\nSec. 4 for further evidence in support of this claim).\nOur proposed solution to the problem of isolated mem-\nbership is to take the SIA output and ‘trawl’ inside each\nMTP from beginning to end, returning subsets that have a\ncompactness greater than some threshold aand that con-\ntain at least bpoints. The compactness of a pattern is the\nratio of the number of points in a pattern to the number\nof points in the region of the dataset in which the pattern\noccurs [13]. Different interpretations of ‘region’ lead to\ndifferent versions of compactness. The version employed\nhere is of least computational complexity O(kn), and uses\nthe lexicographical ordering shown in Fig. 2b. The com-\npactness of a pattern P=fp1; : : : ;plgin a dataset Dis\ndeﬁned by\nc(P; D ) =l=jfd i2D:p1\u0016di\u0016plgj: (10)\nFor instance, the compactness of pattern Qin Fig. 2c is\n8=9, as there are 8 points in the pattern and 9 in the dataset\nregionfd9;d10; : : : ;d17gin which the pattern occurs.\nOne of Meredith et al.’s [14] suggestions for improv-\ning/extending the SIA family is to ‘develop an algorithm\nthat searches the MTP TECs generated by SIATEC and\nselects all and only those TECs that contain convex-hull\ncompact patterns’ [p. 341]. The way in which our pro-\nposed solution is crucially different to this suggestion is to\ntrawl inside MTPs. It will not sufﬁce to calculate the com-\npactness of an entire MTP, since we know it is likely to\ncontain isolated members. Other potential solutions to the\nproblem of isolated membership are to:\n\u000fSegment the dataset before discovering patterns. The\nissue is how to segment appropriately—usually the\ndiscovery of patterns guides segmentation [2], not\nthe other way round.\n\u000fApply SIA with a ‘sliding window’ of size r. Ap-\nproximately, this is equivalent to traversing only theelements on the ﬁrst rsuperdiagonals of Ain (6).\nThe issue is that the sliding window could prevent\nthe discovery of very noticeable or important pat-\nterns, if their generating vectors lie beyond the ﬁrst\nrsuperdiagonals.\n\u000fConsider the set of all patterns that can be expressed\nas an intersection of MTPs, which may not be as\nsusceptible to the problem of isolated membership.\nThe issue with this larger class is that it is more com-\nputationally complex to calculate, and does not aim\nspeciﬁcally at tackling isolated membership.\nThe algorithmic form of our solution is called a com-\npactness trawler . It may be helpful to apply it to the exam-\nple of P+in (9), using a compactness threshold of a= 2=3\nand points threshold of b= 3. The compactness of succes-\nsive subsetsfd1g;fd1;d2g; : : : ;fd1; : : : ;d8gofP+re-\nmains above the threshold of 2=3but then falls below, to\n9=18, forfd1; : : : ;d8;d18g. So we return tofd1; : : : ;d8g,\nand it is output as it contains 8\u00153 =bpoints. The pro-\ncess restarts with subsets fd18g,fd18;d19g, and then the\ncompactness falls below 2=3to3=5forfd18;d19;d22g.\nSo we return tofd18;d19g, but it is discarded as it contains\nfewer than 3 points. The process restarts with subset fd22g\nbut this also gets discarded for having too few points. The\nwhole of P+has now been trawled. The formal deﬁnition\nfollows and has computational complexity O(kn).\n1. Let P=fp1; : : : ;plgbe a pattern in a dataset D\nandi= 1.\n2. Let jbe the smallest integer such that i\u0014j < l and\nc(Pj+1; D)< a, where Pj+1=fpi; : : : ;pj+1g. If\nno such integer exists then put P0=P, otherwise\nletP0=fpi; : : : ;pjg.\n3. Return P0if it contains at least bpoints, otherwise\ndiscard it.\n4. Ifjexists in step 2, re-deﬁne Pin step 1 to equal\nfpj+1; : : : ;plg, set i=j+ 1, and repeat steps 2\nand 3. Otherwise re-deﬁne Pas empty.\n5. After a certain number of iterations Pwill be empty\nand the output can be labelled P0\n1; : : : ; P0\nN, that is N\nsubsets of the original P, where 0\u0014N\u0014l.\nWe give the name ‘structural inference algorithm and\ncompactness trawler’ (SIACT) to the process of calculat-\ning all MTPs in a dataset (SIA), followed by the applica-\ntion of the compactness trawler to each. The compactness-\ntrawling stage in SIACT requires O(kmn )calculations,\nwhere mis the number of MTPs returned by SIA. If de-\nsired, it is then possible to take the output of SIACT and\ncalculate the TECs. These TECs are represented by the set\nHin Fig. 3. To our knowledge, this newest member of\ntheSIA family is the only algorithm intended to solve the\nproblem of isolated membership.\n6\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)4. COMPARATIVE EV ALUATION\nA music analyst (the second author) analysed the Sonata in\nC major L1 and the Sonata in C minor L10 by Scarlatti, the\nPrelude in C] minor BWV 849 and the Prelude in E major\nBWV 854 by Bach. The brief was similar to the intra-opus\ndiscovery task described in Sec. 1: given a piece of music\nin staff notation, discover translational patterns that occur\nwithin the piece. Thus, a benchmark of translational pat-\nterns was formed for each piece, the criteria for benchmark\nmembership being left largely to the analyst’s discretion.\nOne criterion that was stipulated was to think in terms of\nan analytical essay: if a pattern would be mentioned in\nprose or as part of a diagram, then it should be included in\nthe benchmark. The analyst is referred to as ‘independent’\nbecause of the relative freedom of the brief and because\nthey were not aware of the details of the SIA family, or\nour new algorithm. The analyst was also asked to report\nwhere aspects of musical interest had little or nothing to\ndo with translational patterns, as these occasions will have\nimplications for future work.\nThree algorithms—SIA [14],COSIATEC [13] and our\nown, SIACT—were run on datasets that represented L1,\nL10, BWV 849, and BWV 854. For COSIATEC the non-\nparametric version of the rating heuristic was used [7] and\nforSIACT we used a compactness threshold of a= 2=3\nand a points threshold of b= 3 . The choice of a=\n2=3means that at the beginning of an input pattern, the\ncompactness trawler will tolerate one non-pattern point be-\ntween the ﬁrst and second pattern points, which seems like\na sensible threshold. The choice of b= 3 means that a\npattern must contain at least three points to avoid being\ndiscarded. This is an arbitrary choice and may seem a lit-\ntle low to some. Each point in a dataset consisted of an\nontime, MIDI note number (MNN), morphetic pitch num-\nber (MPN), and duration (voicing was omitted for simplic-\nity on this occasion). Nine combinations of these four di-\nmensions were used to produce ‘projections’ of datasets\n[14], on which the algorithms were run. These projections\nalways included ontime, bound to: MNN and duration;\nMNN; MPN andduration; MPN; duration; MNN mod 12\nandduration; MNN mod 12; MPN mod 7 andduration;\nMPN mod 7. For the ﬁrst time to our knowledge, the use\nof pitch modulo 7 and 12 enabled the concept of octave\nequivalence to be incorporated into the geometric method.\nIf a pattern is in the benchmark, it is referred to as a\ntarget; otherwise it is a non-target. An algorithm is judged\nto have discovered a target if a member of the algorithm’s\noutput is equal to the target pattern or a translation of that\npattern. In the case of COSIATEC the output consists of\nTECs, not patterns. So we will say it has discovered a tar-\nget if that target is a member of one of the output TECs. Ta-\nble 1 shows the recall and precision of the three algorithms\nfor each of the four pieces. Often COSIATEC did not dis-\ncover any target patterns, so for these pieces it has zero\nrecall and precision. This is in contrast to the parametric\nversion’s quite encouraging results for Bach’s two-part in-\nventions [12,13]. When it did discover some target patterns\ninL10,COSIATEC achieved a better precision than thePiece! L1 L10 BWV 849 BWV 854\nAlgorithm# Recall\nSIA .29 .22 .28 .22\nCOSIATEC .00 .17 .00 .00\nSIACT .50 .65 .56 .61\nPrecision\nSIA 1:5e\u000051:1e\u000051:3e\u000051:8e\u00005\nCOSIATEC .00 .02 .00 .00\nSIACT 2:6e\u000031:5e\u000037:8e\u000042:0e\u00003\nTable 1. Results for three algorithms on the intra-opus pat-\ntern discovery task, applied to four pieces of music. Recall\nis the number of targets discovered, divided by the sum of\ntargets discovered and targets not discovered. Precision is\nthe number of targets discovered, divided by the sum of\ntargets discovered and non-targets discovered.\nother algorithms, as it tends to return far fewer patterns per\npiece (168 on average compared with 8,284 for SIACT and\n385,299 for SIA). Hence the two remaining contenders are\nSIA andSIACT. SIACT, deﬁned in Sec. 3, out-performs\nSIA in terms of both recall and precision. Having exam-\nined cases in which SIA andCOSIATEC fail to discover\ntargets, we attribute the relative success of SIACT to its\nbeing intended to solve the problem of isolated member-\nship. Across the four pieces, the running times of SIA and\nSIACT are comparable (the latter is always slightly greater\nsince the ﬁrst stage of SIACT isSIA).\n5. DISCUSSION\nThis paper has discussed and evaluated algorithms for the\nintra-opus discovery of translational patterns. One of our\nmotivations was the prospect of improving upon current\nsolutions to this open MIR problem. A comparative eval-\nuation was conducted, including two existing algorithms\nand one of our own, SIACT. For the pieces of music con-\nsidered, it was found that SIACT out-performs the existing\nalgorithms considerably with regard to recall and, more of-\nten than not, it is more precise. Therefore, our aim of im-\nproving upon the best current solution has been achieved.\nCentral to this achievement was the formalisation of the\n‘problem of isolated membership’. It was shown that for\na small and conveniently chosen excerpt of music, a maxi-\nmal translatable pattern corresponded exactly to a percep-\ntually salient pattern. However, when the excerpt was en-\nlarged by just one bar, the MTP gained some temporally\nisolated members, and the salient pattern was lost inside\nthe MTP. Our proposed solution, to trawl inside an MTP,\nreturning compact subsets, led to the deﬁnition of SIACT.\nThe weight placed on the improved results reported here\nis limited somewhat by the extent of the evaluation, which\nincludes only four pieces, all from the Baroque period,\nand all analysed by one expert. Extending and altering\nthese conditions and assessing their effect on the perfor-\nmance of the three algorithms is a clear candidate for fu-\nture work. There are also more sophisticated versions of\n7\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)compactness and the compactness trawler algorithm that\ncould be explored, and alternative values for the compact-\nness and points thresholds, aandb. The discovery of ‘ex-\nact repetition’ has provided a sensible starting point for\nthis research, but extending deﬁnitions such as (5) to al-\nlow for ‘inexact repetition’ is an important and challeng-\ning next step. Cases of failure, where SIACT does not dis-\ncover targets, will be investigated. Perhaps some of these\ncases share characteristics that can be addressed in a fu-\nture version of the algorithm. Although we have seen SIA\npresented before as the sorting of matrix elements [14],\nthe connection that Ain (6) makes with similarity matri-\nces [15, 16] may lead to new insights or efﬁciency gains.\nWe will be trying to elicit more knowledge about the\nattributes of a pattern that matter to human analysts, so\nas to rank output patterns and to compare these attributes\nwith the assumptions underlying SIACT. It could be that\ncurrent pattern discovery methods overlook particular as-\npects of musical interest. If so a string-based or geometric\nmethod might be easily adapted, or very different meth-\nods may have to be developed. Could one focused algo-\nrithm encompass the many and diverse categories of mu-\nsical pattern? It seems improbable, and the discussion of\nFigs. 1 and 2 in Sec. 1.1 could be interpreted as a coun-\nterexample. Hence, given the improved voice separation\nalgorithms, and string-based and geometric methods that\nnow exist, another worthy topic for future work would be\nthe uniﬁcation of a select number of algorithms within a\nsingle user interface. This would bring us closer to achiev-\ning our opening, more ambitious aim, of enabling music\nanalysts, listeners, and students to engage with pieces of\ntheir choice in a novel and rewarding manner. To this end,\nthe work reported here clearly merits further development.\n6. ACKNOWLEDGEMENTS\nThis paper beneﬁted from helpful discussions with David\nMeredith and Jamie Forth. We would also like to thank the\nfour anonymous reviewers for their comments.\n7. REFERENCES\n[1] E. Cambouropoulos: “V oice and stream: perceptual\nand computational modeling of voice separation,” Mu-\nsic Perception, V ol. 26, No. 1, pp. 75–94, 2008.\n[2] E. Cambouropoulos: “Musical parallelism and\nmelodic segmentation: a computational approach,”\nMusic Perception, V ol. 23, No. 3, pp. 249–267, 2006.\n[3] S-C. Chiu, M-K. Shan, J-L. Huang, and H-F. Li: “Min-\ning polyphonic repeating patterns from music data us-\ning bit-string based approaches,” Proceedings of the\nIEEE International Conference on Multimedia and\nExpo, pp. 1170–1173, 2009.\n[4] R. Clifford, M. Christodoulakis, T. Crawford,\nD. Meredith, and G. Wiggins: “A fast, randomised,\nmaximal subset matching algorithm for document-\nlevel music retrieval,” Proceedings of the International\nSymposium on Music Information Retrieval, 2006.[5] D. Conklin and M. Bergeron: “Feature set patterns\nin music,” Computer Music Journal, V ol. 32, No. 1,\npp. 60–70, 2008.\n[6] N. Cook: A guide to musical analysis, J.M. Dent and\nSons, London, 1987.\n[7] J. Forth and G. Wiggins: “An approach for identifying\nsalient repetition in multidimensional representations\nof polyphonic music,” London algorithmics 2008: the-\nory and practice, College Publications, London, 2009.\n[8] J-L. Hsu, C-C. Liu, and A. Chen: “Discovering non-\ntrivial repeating patterns in music data,” IEEE Transac-\ntions on Multimedia, V ol. 3, No. 3, pp. 311–325, 2001.\n[9] I. Knopke and F. J ¨urgensen: “A system for identify-\ning common melodic phrases in the masses of Palest-\nrina,” Journal of New Music Research, V ol. 38, No. 2,\npp. 171–181, 2009.\n[10] O. Lartillot: “Efﬁcient extraction of closed motivic pat-\nterns in multi-dimensional symbolic representations of\nmusic,” Proceedings of the International Symposium\non Music Information Retrieval, pp. 191–198, 2005.\n[11] C. Meek and W. Birmingham: “Automatic thematic\nextractor,” Journal of Intelligent Information Systems,\nV ol. 21, No. 1, pp. 9–33, 2003.\n[12] D. Meredith: “Point-set algorithms for pattern discov-\nery and pattern matching in music,” Proceedings of the\nDagstuhl Seminar on Content-Based Retrieval, 2006.\n[13] D. Meredith, K. Lemstr ¨om, and G. Wiggins: “Algo-\nrithms for discovering repeated patterns in multidimen-\nsional representations of polyphonic music,” Proceed-\nings of the Cambridge Music Processing Colloquium,\n2003.\n[14] D. Meredith, K. Lemstr ¨om, and G. Wiggins: “Algo-\nrithms for discovering repeated patterns in multidimen-\nsional representations of polyphonic music,” Journal\nof New Music Research, V ol. 31, No. 4, pp. 321–345,\n2002.\n[15] G. Peeters: “Sequence representation of music\nstructure using higher-order similarity matrix and\nmaximum-likelihood approach,” Proceedings of the\nInternational Symposium on Music Information Re-\ntrieval, 2007.\n[16] X. Ren, L. Smith, and R. Medina: “Discovery of\nretrograde and inverted themes for indexing musical\nscores,” Proceedings of the ACM/IEEE Joint Confer-\nence on Digital Libraries, pp. 252–253, 2004.\n[17] P-Y . Rolland: “FlExPat: ﬂexible extraction of sequen-\ntial patterns,” Proceedings of the IEEE International\nConference on Data Mining, pp. 481–488, 2001.\n[18] E. Ukkonen, K. Lemstr ¨om, and V . M ¨akinen: “Geomet-\nric algorithms for transposition invariant content-based\nmusic retrieval,” Proceedings of the International Sym-\nposium on Music Information Retrieval, 2003.\n8\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "Discovery of Contrapuntal Patterns.",
        "author": [
            "Darrell Conklin",
            "Mathieu Bergeron"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1417413",
        "url": "https://doi.org/10.5281/zenodo.1417413",
        "ee": "https://zenodo.org/records/1417413/files/ConklinB10.pdf",
        "abstract": "This paper develops and applies a new method for the dis- covery of polyphonic patterns. The method supports the notes that overlap in time without being simultaneous. Such relations are central to understanding species counterpoint. The method consists of an application of the vertical view- point technique, which relies on a vertical slicing of the musical score. It is applied to two-voice contrapuntal tex- tures extracted from the Bach chorale harmonizations. Re- sults show that the new method is powerful enough to rep- resent and discover distinctive modules of species coun- terpoint, including remarkably the suspension principle of fourth species counterpoint. In addition, by focusing on two voices in particular and setting them against all other possible voice pairs, the method can elicit patterns that il- lustrate well the unique treatment of the voices under in- vestigation, e.g. the inner and outer voices. The results are promising and indicate that the method is suitable for computational musicology research.",
        "zenodo_id": 1417413,
        "dblp_key": "conf/ismir/ConklinB10",
        "keywords": [
            "polyphonic patterns",
            "notes overlapping in time",
            "species counterpoint",
            "vertical slicing",
            "Bach chorale harmonizations",
            "distinctive modules",
            "suspension principle",
            "voice pairs",
            "unique treatment",
            "computational musicology"
        ],
        "content": "DISCOVERY OF CONTRAPUNTAL PATTERNS\nDarr\nell Conklin\nDepartment of Computer Science and AI\nUniversidad del Pa ´ısVasco\nSan Sebasti ´an,Spain\nIKERBASQUE,Basque Foundation for Science\nBilbao, Spain\ndarrell conklin@ehu.esMathieu\nBergeron\nCIRMMT\nMcGill University\nMontreal, Canada\nbergeron.mcgill@gmail.com\nABSTRACT\nThis paper develops and applies a new method for the dis-\ncovery of polyphonic patterns. The method supports the\nrepresentationofabstractrelationsthatareformedbetween\nnotesthatoverlapintimewithoutbeingsimultaneous. Such\nrelationsarecentraltounderstandingspeciescounterpoint.\nThemethodconsistsofanapplicationoftheverticalview-\npoint technique, which relies on a vertical slicing of the\nmusical score. It is applied to two-voice contrapuntal tex-\nturesextractedfromtheBachchoraleharmonizations. Re-\nsultsshowthatthenewmethodispowerfulenoughtorep-\nresent and discover distinctive modules of species coun-\nterpoint, including remarkably the suspension principle of\nfourth species counterpoint. In addition, by focusing on\ntwo voices in particular and setting them against all other\npossible voice pairs, the method can elicit patterns that il-\nlustrate well the unique treatment of the voices under in-\nvestigation, e.g. the inner and outer voices. The results\nare promising and indicate that the method is suitable for\ncomputational musicology research.\n1. INTRODUCTION\nPolyphonic music presents challenges for music informa-\ntion retrieval, and the representation and discovery of pat-\nterns that recur in a polyphonic corpus remains an open\nand interesting problem. The discovery of monophonic\npatterns in music could be considered a mature area, with\nseveralpowerfulmethodsproposedandeffectivelyapplied\nto this task.\nIn polyphonic music, monophonic patterns can natu-\nrallybediscoveredusingsuchmethodsprovidedthatvoice\nseparation is ﬁrst performed [12]. However, the discov-\nery of polyphonic patterns – those containing two or more\noverlapping voices – is largely an open problem and there\nremain few approaches in the literature. The difﬁculty of\nthis problem arises primarily from the presence of tempo-\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnotmadeordistributedforproﬁtorcommercialadvantageandthatcopies\nbear this noticeand thefull citation on theﬁrst page.\nc/circlecopyrt2010International Society for Music Information Retrieval.ral relations between notes that occur in different voices,\nwithout being simultaneous. These relations are central to\nthe understanding of counterpoint [9] and occur in all but\nthe simplest note against note (ﬁrst species) counterpoint.\nThe discovery of polyphonic patterns is a new area with\nfew existing approaches.\nAdifﬁcultywithpolyphonyisthatmostpolyphonicmu-\nsic cannot be easily separated into a regular number of\nvoices [2]. In piano music, for example, voices can appear\nanddisappearthroughoutapiece,anditwouldbemistaken\ntoparsethemusicintoapersistentnumber of voices. This\npaper uses the voiced Bach chorale harmonizations, and\nlatersomediscussionisprovidedregardingtheapplication\nof the method tounvoiced polyphony.\nThe simplest approach to ﬁnd patterns in a polyphonic\ncorpus is based on the conversion into chord symbol se-\nquences, followed by the application of monophonic pat-\ntern discovery algorithms [1, 5]. However, accurate au-\ntomatic chord labelling and segmentation is an unsolved\nproblem,andthepreparationoflargecorporaisdifﬁcultto\nachieve by hand.\nVerticalapproaches[4]areappliedtovoicedpolyphonic\ntexturesﬁrstconvertedtoahomophonicform. Thisisdone\nby fully expanding a polyphonic score by adding artiﬁcial\nnotes whenever a new onset time is encountered. This ex-\npanded score is then sliced, features computed for each\nsliceinthesequence,andtheresultingsequenceminedus-\ning a monophonic pattern discovery approach. A similar\nmethod using bit-string approaches has been described by\nChiuetal. [3]. Theproblemwiththeseverticalapproaches\nisinthelackofﬂexibletemporalrelationsbetweencompo-\nnents of a slice, which must all have the same onset time.\nGeometric approaches [10] can be applied to voiced or\nunvoicedpolyphonyandcanrevealpatternsnotcontiguous\non the music surface. The geometric approach however\nhas two drawbacks for discovering contrapuntal patterns.\nSince geometric patterns must conserve exact inter-onset\nintervalsinthetimedimension,theycannotrepresentgen-\neral local temporal relations among pattern components.\nFurthermore the expensive time complexity of the method\nleads to intractability for large pieces or corpora.\nPolyphonicpatternsareideallyrepresentedasrelational\nnetworks. Nodes represent events and edges represent re-\nlations between events. For example, consider the score\n201\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)fragment and small relational network presented in Fig-\nure\n1(i,ii). This pattern matches four notes (a,b,c,d ) in\nthe following relations: a m7 dissonance (a,c ) formed by\nthe bottom voice (c) approached by a downwards leap, re-\nsolving to a M3 consonance (b,d) by a stepwise motion\ndown in the top voice (b), and a leap up in the bottom\nvoice (d). In addition, an asterisk ∗indicates that (c) is\npreceded by a note that the pattern does not represent ex-\nplicitly,butratherimplicitlyviathenotionofmelodicmo-\ntion. Note that while (a,b ) and (c,d) follow each other in\nthe same voice, critically important in this pattern are the\ndifferenttemporalrelationsbetween(a,c )(cstartingwhile\nais sounding) and (b,d) (they start together).\ni) \n/timesig.C44/clefs.F\n/scripts.dfermata/noteheads.s2/noteheads.s2/noteheads.s2\n/noteheads.s2/scripts.ufermata\n/noteheads.s2/accidentals.2/noteheads.s2/noteheads.s2\n∗a\ncb\nd\nii)\n∗a b\nd c-\n-- ++b-sw,diss st,cons\niii) slice slice A slice B slice\nA2B2C3D3E3F3G3A3\ncdb ♯\n∗a\niv)\n∗ab\ncd\nv) \n\nqual:di\nss\ntime:b-sw\nbc:--\n\n\n\nqual:cons\ntime:st\ntc:-\nbc:++\n\n\nFigure 1. Two-voice counterpoint pattern: i) instance in\nﬁnal bar of BWV 257; ii) relational pattern; iii) piano-\nroll notation of instance showing the slicing process; iv)\na schematic representation of the pattern; v) an equivalent\nfeature set pattern.\nFigure 1(iv) shows a schematic representation that we\nhave developed as a visually appealing notation for a full\nrelationalnetworkoveralimitednumberofrelations. Hor-\nizontal lines indicate voicing. The placement of vertical\nlines indicates temporal relations, and their form indicates\nconsonance (dashed) or dissonance (dotted) between the\ntemporally related notes. The slope of horizontal lines\nis proportional to the melodic contour it indicates (i.e. asteeper slope indicates a melodic leap). When the contour\nis not deﬁned by the pattern, the horizontal line is dashed.\nAgain, an asterisk ∗indicates a note that must implicitly\nbe present for the pattern to match, but is not explicitly\nmatched by a pattern component.\nThis paper is concerned with discovering patterns like\nthe one presented in Figure 1 in a large corpus of poly-\nphonic music. We consider a precise notion of polyphonic\npattern, one that captures musical intervals between notes\nthat overlap in time, including between notes that are not\nsimultaneous. Our approach is computationally very efﬁ-\ncient as it translates the relational discovery problem to a\nformwhere sequential pattern discovery can be applied.\nThe paper is structured as follows. First, the new VVP\n(Vertical ViewPoint) method for polyphonic pattern dis-\ncovery will be described. Results will be highlighted and\ntranslated to schematic relational networks for clarity. Fi-\nnally,thestrengthsandweaknessesof VVParediscussed.\n2. METHOD\nTheverticalviewpointapproachasoriginallypresented[4]\napplies to homophony and cannot express temporal rela-\ntions that might hold between components of a vertical\nslice, as would be required for the pattern developed in\nFigure1. However,theapproachcanbeextendedresulting\nin what we call the VVPformalism. This is done by a)\naddingtheabilitytohandlelocaltemporalrelationswithin\nslices; and b) describing properties of slices using ﬂexible\nsets of features.\nTheﬁrststepwastoemployevent continuations,which\nspecially identify those events with onsets prior to the on-\nset of the slice. This method was inspired by the approach\nofDubnovetal.[7]whodevelopedareal-timemethodfor\npredictionofcontinuationsforpolyphonicimprovisations.\nFigure1(iii)illustratessliceswherecontinuationsareused\nto retain local temporal relations. For example, the slice\nlabelled Acontains two notes: B2 in the bottom voice (c)\nand a continued event A3 in the top voice (a).\nThe second step was to employ feature sets [6] to de-\nscribe successive slices in a piece. In this paper, four fea-\ntures are used to describe slices:\ntimedescribing the temporal relations between the two\nvoices, taking the values st(both voices start to-\ngether),b-sw(bottomvoicestartswhilethetopvoice\nissounding)and t-sw(topvoicestartswhilethebot-\ntom voice is sounding). These temporal relations\nare derived by inspecting the continuation features\nof events within slices;\nqualoftheharmonicinterval(non-compounddiatonicin-\ntervalbetweenlowerandhigherpitchesintheslice),\ntaking the values cons(intervals P1, m3, M3, P5,\nm6, M6) and diss(all other intervals);\nbc,tcdescribing the melodic contour of either the top\nvoice(tc)orbottomvoice (bc),bothtakingthepos-\nsible values leap up: ++; leap down: --; step up:+;\nstep down: -; unison:=.\n202\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Figure1(v)illustratesa VV Ppattern,usingtheabove fea-\ntures. The pattern is a sequence of two features sets which\nexpresses exactly the relational network in Figure 1(ii). It\nmatches the score fragment in Figure 1(i) and in general\nall pairs of contiguous slices that contain the features pre-\nsented.\nTo apply this representation in pattern discovery, a cor-\npus is sliced at every unique onset time, every slice is sat-\nuratedwiththeabovefeatures,thenapatterndiscoveryal-\ngorithm ﬁnds all patterns that are distinctive (with a score\nabove a threshold) and maximally general [5]. The algo-\nrithm for ﬁnding these patterns is a depth-ﬁrst search of a\nsubsumption space, that iteratively reﬁnes patterns at each\nsearchnodetomakethemmorespeciﬁc. Thisisdescribed\nin more detail in [5].\nRegarding the scoring of patterns, it is well-known that\ndiscovered patterns should not be ranked simply by their\ncount, because a general pattern will naturally occur more\nfrequently than a more speciﬁc one, regardless of the cor-\npus. Patterns are scored by an odds ratio, dividing their\ncorpus probability by their background probability:\n∆(P)def=p(P|⊕)\np(P|⊖), (1)\nwhere Pis\napatternandtheprobabilities p(P|⊕)andp(P|⊖)\nare computed as follows. The probability of pattern Pin\nthe corpus is p(P|⊕) = c⊕(P)/n⊕, where c⊕(P)is the\ntotal slice count of pattern Pin the corpus and n⊕is the\nnumberofslicesinthecorpus. Toevaluatethebackground\nprobability p(P|⊖),twodistinctmethodsmaybeused: the\nanticorpus method, and the the null model method.\nAnanticorpus isasetofpiecesthatissetupspeciﬁcally\nto contrast with the analysis corpus. When an explicit an-\nticorpus is used, the background probability is simply the\nanticorpus probability of the pattern:\np(P|⊖)def=c⊖(P)/n⊖, (2)\nwhere c⊖(P)isthetotalslicecountofthepattern Pinthe\nanticorpus, and n⊖is the number of slices in the anticor-\npus.\nIn cases where an explicit anticorpus is not used, the\nnull model method may be employed instead to rank pat-\nterns. Thenullmodelprobabilityofapattern Pcomprised\nof feature sets c1,... ,c nis deﬁned as:\np(P|⊖)def=n/productdisplay\ni=1/productdisplay\nf∈cic⊕(f)/n⊕, (3)\nwhere c⊕(f)is the total count (number of slices) of the\nfeature finthecorpus. Itestimateshowprobableapattern\nis by multiplying the probabilities of its constituent fea-\ntures as they are encountered in the corpus. For example,\ntheb-swandt-swtemporal relations might occur less fre-\nquentlythanthe sttemporalrelation. Thenullmodelprob-\nability of a pattern containing either b-swort-swwould\nthen be lower, allowing for such a pattern to be interesting\nevenifithasalowertotalcountthanasimilarpatternwith\nasttemporal relation.3. RESULTS\nThissectionpresentstheresultsof VVPonpairsofvoices\nextracted from 185 J.S. Bach chorale harmonizations in\nHumdrum format. Importantly, this format has diatonic\nspellingforallpitches,facilitatingthecomputationofcon-\nsonance and dissonance relations. Voices were extracted\nand recombined using the Humdrum toolset, and the ap-\npropriateﬁlteringappliedtoinsurewell-formedslices(e.g.\nremoving null lines to avoid slices where no event starts).\nAll6possibleorderedvoicepairswereextractedfromeach\npiece: soprano-alto(SA),soprano-tenor(ST),soprano-bass\n(SB),alto-tenor(AT),alto-bass(AB),andtenor-bass(TB).\nThisprovidesatotalof 185×6 = 1110 possibletwo-voice\npairs.\nTwo experiments were performed: one where an an-\nticorpus was used to reveal the most interesting patterns\nand one where a null model was used. In both experi-\nments, discovered pattern sets were ﬁltered to retain only\nthosepatternsthatcontainedatleastatemporal(time )and\nharmonic interval quality feature (qual ) in all components\nof a pattern. The method was calibrated to discover pat-\nterns with a score of at least 3 (Equation 1): with 3 times\nhigher probability in the corpus as compared to the anti-\ncorpus or null model background probabilities (this cali-\nbration has worked well for other pattern discovery exper-\nimentsinmusic[5]). Inaddition,onlypatternswithatotal\ncount of at least 100 were considered. This is a simple\nway to isolate patterns that are deemed to be reasonably\nfrequent (the suspension pattern reported in Section 3.2,\nfor example, has a total count of 450).\n3.1 Anticorpus\nIn this experiment, all 6 voice pairs (SA, ST, SB, AT, AB,\nTB) were in turn used as the corpus, with the remaining 5\nas the anticorpus. Thus each corpus comprises 185 two-\nvoice polyphonic extracts, and each anticorpus 185×5 =\n925extracts. Patterns are ranked by using the anticorpus\nmethod(Equation2)tocomputebackgroundprobabilities.\nResults for all experiments are concatenated, sorted from\nhigh to low ∆(P)(Equation 1), and the most distinctive 3\npatterns each from the AT and SB corpora were retained\n(Table 1). The inner voices (AT) and outer voices (SB)\nwere selected for illustration as they were previously dis-\ncussed in the literature [8], but the method returns results\nforeveryvoicepair. Beloweachpattern,Table1showsits\ntotal count and score as a number pair. The three patterns\nin each group are sorted by decreasing score ∆(P): note\nhowapatterncanhavehigherscoredespitehavingalower\ntotal count.\nThe ﬁrst pattern of Table 1 is roughly 5 times more\nlikely to occur between SB (outer voices) than any other\nvoice pair. As mentioned by Huron [8], outer voices are\nmore perceptually salient. They are hence expected to ex-\nhibit a good contrapuntal quality. Not surprinsingly, the\npatterns discovered by VVPas characteristic of SB are all\nconsistent with counterpoint. For example, the ﬁrst and\nthirdpatternsintroduceadissonanceovera b-swtemporal\n203\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Pattern Schema Examples\n/braceleftbigg\nqual:di\nss\ntime:b-sw/bracerightbigg\n\nqual:cons\ntime:st\nbc:+\n\n\n\nqual:cons\ntime:st\ntc:-\n\n\n(157, 4.88)a b\ndec\nf \n/timesig.C44/accidentals.2 /clefs.F/timesig.C44/accidentals.2/clefs.G\nf d/noteheads.s2a/noteheads.s2\n/dots.dot/noteheads.s2/noteheads.s2c\n/noteheads.s2/noteheads.s2\neb\n/noteheads.s2/noteheads.s2\n/flags.d3\nSB \n/timesig.C44/clefs.F/timesig.C44/clefs.G\nfc\n/noteheads.s2/noteheads.s2a\n/noteheads.s2/noteheads.s2\n/noteheads.s2/noteheads.s2/noteheads.s2\neb\n/noteheads.s2/noteheads.s2\nd/noteheads.s2\nSB\n/braceleftbigg\nqual:cons\nt\nime:st/bracerightbigg\n\nqual:cons\ntime:st\ntc:-\nbc:--\n\n\n(427, 4.78)ab\ncd \n/timesig.C44/clefs.F/timesig.C44/clefs.G\ndb\n/noteheads.s2/noteheads.s2\nca\n/noteheads.s2/noteheads.s2\n/noteheads.s2/noteheads.s2\n/scripts.ufermata\n/noteheads.s2/scripts.ufermata\n/noteheads.s2\nSB \n/timesig.C44/accidentals.M2/clefs.F/timesig.C44/accidentals.M2/clefs.G\ndb\n/noteheads.s2/noteheads.s2\n/noteheads.s2/noteheads.s2\n/noteheads.s2/noteheads.s2/noteheads.s2\nca\n/noteheads.s2/noteheads.s2/noteheads.s2\nSB\n\n\nqual:di\nss\ntime:b-sw\nbc:+\n\n\n\nqual:cons\ntime:st\ntc:-\n\n\n(271, 4.22)∗ab\ncd \n/timesig.C44/accidentals.2/accidentals.2/accidentals.2/clefs.F/timesig.C44/accidentals.2/accidentals.2/accidentals.2\n/clefs.G\n/noteheads.s2/noteheads.s2\n*a\n/noteheads.s2/noteheads.s2b\n/noteheads.s2/noteheads.s2/noteheads.s2\nd/noteheads.s2/noteheads.s2\nc/noteheads.s2\nSB \n/timesig.C44/accidentals.2 /clefs.F/timesig.C44/accidentals.2/clefs.G\nc/noteheads.s2\n*a\n/noteheads.s2 /noteheads.s2/noteheads.s1\ndb\n/noteheads.s2/noteheads.s1\nSB\n\n\nqual:di\nss\ntime:st\ntc:=\nbc:=\n\n\n(277, 7.08)∗\n∗a\nb \n/clefs.G/accidentals.2/timesig.C44\n/clefs.F/accidentals.2/timesig.C44/noteheads.s2/noteheads.s2/accidentals.2/noteheads.s2a\n/noteheads.s2/noteheads.s2\n/accidentals.2\n*/noteheads.s2\n/noteheads.s2\nb/noteheads.s2\n/noteheads.s2*\n/noteheads.s2\n/noteheads.s2/noteheads.s2/noteheads.s2\n/noteheads.s2\nAT \n/clefs.G/accidentals.M2/accidentals.M2/timesig.C44\n/clefs.F/accidentals.M2/accidentals.M2/timesig.C44/noteheads.s2/noteheads.s2\n/scripts.ufermata*\n/noteheads.s2/noteheads.s2\n/noteheads.s2/noteheads.s2/noteheads.s2\n/noteheads.s2\nba\n/noteheads.s2/scripts.ufermata\n/noteheads.s2\n*/noteheads.s2/noteheads.s2\nAT\n\n\nqual:di\nss\ntime:st\nbc:=\n\n/braceleftbigg\nqual:cons\ntime:t-sw/bracerightbigg\n(203, 6.89)∗a b\nc \n/timesig.C44/accidentals.2/accidentals.2/clefs.F/timesig.C44/accidentals.2/accidentals.2/clefs.G\nca\n/noteheads.s2/noteheads.s2\n*/noteheads.s2/noteheads.s2/noteheads.s2\n/noteheads.s2/noteheads.s2/noteheads.s2b\n/noteheads.s2\nAT \n/clefs.G/accidentals.2/accidentals.2/accidentals.2/timesig.C44\n/clefs.F/accidentals.2/accidentals.2/accidentals.2/accidentals.2\n/noteheads.s2/noteheads.s2\n*/noteheads.s2/noteheads.s2\n/noteheads.s2/noteheads.s2/noteheads.s2b\n/noteheads.s2\nc/timesig.C44a\n/noteheads.s2/noteheads.s2\nAT\n/braceleftbigg\nqual:di\nss\ntime:st/bracerightbigg\n\nqual:cons\ntime:st\ntc:=\n\n\n(533, 5.48)a b\nc d \n/clefs.G\n/accidentals.2/timesig.C44/accidentals.M2/noteheads.s2/noteheads.s2/accidentals.0a\n/noteheads.s2/noteheads.s2/noteheads.s2\n/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2\ndb\n/noteheads.s2/noteheads.s2\n/clefs.F/timesig.C44/accidentals.2\nc AT \n/timesig.C44/accidentals.2 /clefs.F/timesig.C44/accidentals.2/clefs.G\nca\n/noteheads.s2/noteheads.s2\n/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2\ndb\n/noteheads.s2/noteheads.s2\nAT\nTable 1. Patterns discovered by computing background probabilities using an anticorpus. Top: distinctive of SB; bottom:\ndistincti\nve of AT.\nrelationonaweakbeatandresolveitstraightawaythrough\nstepwise melodic motion.\nBy opposition, one can expect the inner voices (AT),\nwhich are less perceptually salient, to be used more freely\nto create harmonies, with less considerations for the pres-\nence of dissonance. The last three patterns of Table 1 all\ncharacterize the inner voices and are consistent with this\nidea. All patterns introduce a dissonance over a sttem-\nporalrelation. Pattern5clearlydemonstratesthis: adisso-\nnancesofaperfectfourth,usuallyforbiddenbetweenouter\nvoices, here freely occurs in both examples.\nIn addition to the results presented in Table 1, many of\nthe most interesting patterns discovered by VVPrefer to\neither SB or AT pairs (data not shown). This suggests that\nthe inner and outer voices are the two voice pairs that are\ntheeasiesttocharacterize,possiblybecausetheydifferthemost from other voice pairs in terms of their contrapuntal\nquality.\n3.2 Null model\nIn this experiment, we consider all voice pairs together (a\ncorpus of 1110 two-voice pairs). Patterns are ranked by\nusing a null model (Equation 3) to compute background\nprobabilities. The seven most distinctive patterns are pre-\nsented in Table 2. Again, the total count and score of each\npattern is shown as a number pair.\nTheﬁrstpatternexhibitsconsecutiveleaps. Onitsown,\na leap is less likely than a step. Similarly, a sequence of\nleaps will be deemed unlikely by the null model. How-\never, the ﬁrst pattern contains such a sequence and does\noccur signiﬁcantly in the chorales: 11 times more than ex-\n204\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)pected. This can be interpreted as a composition rule: a\nsequence\nof leaps is acceptable, given that it occurs over\ntwo consonances, over two b-swtemporal relations, and\nchanges contour. Note how the second instance of pat-\ntern 1 presents a rhythmic augmentation: the abstract pat-\ntern makes no constraints on rhythmic features.\nThesecond,fourthandﬁfthpatternevokesecondspecies\ncounterpoint,whilethesixthevokesﬁrstspecies. Pattern3\nevokes third species counterpoint, with a four note against\none texture, alternating with dissonance and consonance.\nIn one instance the bass line is rising: in the other it falls.\nIn pattern 2, similarly, the direction of motion of the tenor\nline is inverted between the two instances shown. Again,\nthis illustrates the abstraction power of the feature set pat-\ntern representation.\nPattern 7 is characteristic of the fourth species, with\nvoices overlapping via successive b-swandt-swtempo-\nral relations. Remarkably, this is a pattern precisely de-\nscribing the suspension pattern of fourth species counter-\npoint,includingthemelodiccontourofastepdownforthe\nresolution of the dissonance [9]. The two instances pre-\nsented are examples of the 4-3 and 7-6 suspension. Note\nhowthehighlevelofabstractionofthispattern(refraining\nfromspecifyingexactintervals,andlowervoicemovement\nfor the suspension resolution) is necessary to represent the\nconcept of a suspension.\n4. DISCUSSION\nThispaperpresentedthe VVPmethodforthediscoveryof\nrelational patterns in two-voice counterpoint. The method\nis based on a monophonic pattern discovery algorithm [5],\nand extends earlier results[4]through theuseof continua-\ntions of events across slices. The VVPapproach is fast as\nit transforms a relational data mining problem into a sim-\npler sequential one, an example of representation change\noftenemployedinmachinelearning. Nevertheless,therep-\nresentation is ﬂexible due to the abstraction of slices by\nfeature sets.\nIntheexperimentspresentedhere,therepresentational-\nlowstheconciseexpressionofmanycontrapuntalpatterns.\nThis demonstrates that the approach is powerful enough\ntodiscoverpolyphonicpatternsoftheoreticalsigniﬁcance.\nThe patterns reported here are not particularly surprising,\nbut their discovery is nonetheless promising: further ex-\nploration with the method could help discover signiﬁcant\npatterns that areyet unknown tomusictheory. Thisexplo-\nration is however outside the scope of the current paper.\nSimilar patterns to the those presented here have been\nstudied in the context of supervised learning [11], where\nthepatternswereidentiﬁedbeforehandandthetaskwasto\nlearn rules explaining the structure of the patterns. How-\never, only simple note against note counterpoint was stud-\nied.\nThe application of the method in this paper led to pat-\nternsthatwereveryabstract,withhighlydivergentinstances\non the musical surface. It was shown, however, that such\nabstraction is necessary to capture the concept of a fourth\nspecies suspension pattern in its full generality. Further-more, using only three simple background features (tem-\nporalrelations,harmonicintervalquality,andmelodiccon-\ntour), the method was able to discover the suspension in a\ncorpus of Bach chorale harmonizations.\nIf less abstraction is desired, more background features\ncanbeadded,forexamplethosereferringtomelodicinter-\nval and durations of notes. For this study, a calibration of\nthe method using the most basic relations of counterpoint\nwasdesired. Thishasbeensuccessfulandfutureworkwill\nfocus on further pattern representation aspects.\nThough applied to two-voice textures here, the method\neasily extends to more voices. Further temporal relations\nwould need to be added to accommodate the additional\ntemporal relations formed; for example for pattern discov-\neryin3voices(e.g.,SAT),threeoverlaprelations(SA,ST,\nAT) and three corresponding simultaneity relations would\nbe needed: in general a number of such features quadratic\nin the number of notes in a slice. In addition, three dif-\nferent harmonic interval relations would be necessary, or\nalternatively the notion of harmonic interval quality could\nsomehow be extended to triads.\nAlimitationofthisapproach,aspresentedhere,isthatit\nappliestovoicedpolyphony,orcorporawherevoiceshave\nalready been extracted from an unvoiced polyphonic tex-\nture. There are several possible ways to apply the method\ntorawunvoicedtextures. Temporalrelationsbetweenslice\ncomponentsmaysimplybediscarded,andany“static”view-\npoint (such as chord type, mode) of a slice may be used.\nAlternatively, the presence of a temporal relation between\nslice components can be considered but with voice lead-\ning relations (e.g., melodic contour) omitted. Finally, if\nboth temporal and voice-leading relations are desired, one\nmight replace the notions of top voice and bottom voice\nwith the highest note and lowest note in the slice, reminis-\ncent of the simple skyline approach to voice segregation.\nWith feature sets, one could even include melodic rela-\ntionshipsbetweensecondhighestpitches(ifpresent),third\nhighest pitches (ifpresent), lowest (possible bass line) and\nso on. The presence or absence of such features would\nalso indicate texture density, hence potentially expressing\npatterns occurring over changes of textures.\n5. ACKNOWLEDGEMENTS\nDarrell Conklin is supported by IKERBASQUE, Basque\nFoundation for Science, Bilbao, Spain. Mathieu Bergeron\nis supported by Le Fonds qu ´eb´ecois de la recherche sur la\nnature et les technologies, Qu ´ebec, Canada.\n6. REFERENCES\n[1] A. Anglade and S. Dixon. Characterisation of har-\nmony with inductive logic programming. In Interna-\ntional Conference on Music Information Retrieval (IS-\nMIR), pages 63–68, Philadelphia, USA, 2008.\n[2] E.Cambouropoulos. Voiceandstream: Perceptualand\ncomputational modeling of voice separation. Music\nPerception, 26(1):75–94, 2008.\n205\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Pattern Schema Examples\n\n\nqual:cons\nt\nime:b-sw\nbc:--\n\n\n\nqual:cons\ntime:b-sw\nbc:++\n\n\n(152, 11.69)∗a\ncd /flags.u3\n/noteheads.s2/noteheads.s2\nd/noteheads.s2\nc/noteheads.s2 /timesig.C44/accidentals.M2/clefs.F /noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2\n*/dots.dota\n/noteheads.s2/noteheads.s2/noteheads.s2\nTB \n43/accidentals.M2/accidentals.M2/clefs.G\nc/noteheads.s2\n*a\n/noteheads.s2/noteheads.s1 /noteheads.s2/noteheads.s2\nd/noteheads.s2\nSA\n/braceleftbigg\nqual:di\nss\ntime:t-sw/bracerightbigg/braceleftbigg\nqual:diss\ntime:st/bracerightbigg/braceleftbigg\nqual:cons\ntime:t-sw/bracerightbigg\n(158, 11.68)ab\ndec \n/noteheads.s2/noteheads.s2c/scripts.ufermata\n/noteheads.s2 /timesig.C44/accidentals.M2/clefs.F/accidentals.0\nd/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2\neb\n/scripts.dfermata/noteheads.s2/noteheads.s2a\n/noteheads.s2\nTB \n/clefs.G /noteheads.s2/noteheads.s2c\n/noteheads.s2 /timesig.C44\nd/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2\neb\n/noteheads.s2/noteheads.s2a\n/noteheads.s2\nSA\n/braceleftbigg\nqual:di\nss\ntime:b-sw/bracerightbigg/braceleftbigg\nqual:cons\ntime:b-sw/bracerightbigg/braceleftbigg\nqual:diss\ntime:b-sw/bracerightbigg\n(154, 10.11)a\ndef \n43/accidentals.M2/accidentals.M2/clefs.F/noteheads.s2\nd/noteheads.s2a\n/noteheads.s2/noteheads.s1\n/noteheads.s2/noteheads.s2\nf/noteheads.s2\ne/noteheads.s2\nTB \n/clefs.G /noteheads.s2/noteheads.s2/noteheads.s2 /timesig.C44 /accidentals.M2/accidentals.M2/accidentals.M2/noteheads.s2\nd/noteheads.s2/accidentals.0a\n/noteheads.s2/noteheads.s1\n/scripts.dfermata/noteheads.s2/scripts.ufermata\n/noteheads.s2\nf/noteheads.s2\ne SA\n\n\nqual:di\nss\ntime:st\nbc:=\n\n/braceleftbigg\nqual:cons\ntime:b-sw/bracerightbigg\n(1128, 9.69)∗a\nc d \n/clefs.F/noteheads.s2/noteheads.s2/noteheads.s2\n/timesig.C44/accidentals.2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2\nd/noteheads.s2\nca\n/noteheads.s2/noteheads.s2\n*TB \n/timesig.C44/accidentals.2/clefs.G /noteheads.s2/noteheads.s2\n*/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2 /noteheads.s2\nd/noteheads.s2\nca\n/noteheads.s2/noteheads.s2\nSA\n\n\nqual:di\nss\ntime:st\ntc:=\n\n/braceleftbigg\nqual:cons\ntime:t-sw/bracerightbigg\n(804, 9.00)∗a b\nc \n/clefs.F /noteheads.s2/noteheads.s2/noteheads.s2/timesig.C44/accidentals.2/accidentals.2*\n/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2b\n/noteheads.s2\nca\n/noteheads.s2/noteheads.s2\nTB \n/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/timesig.C44/accidentals.2/clefs.Ga\n/noteheads.s2/noteheads.s2/accidentals.0/noteheads.s2*\n/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1b\n/noteheads.s2\ncSA\n\n\nqual:cons\nt\nime:st\ntc:--\n\n\n\nqual:diss\ntime:st\ntc:++\n\n\n(116, 8.89)∗ab\nc d \n/accidentals.2/noteheads.s2/noteheads.s2\n/scripts.dfermata/noteheads.s2/timesig.C44/accidentals.M2/clefs.F\nca\n/noteheads.s2/noteheads.s2*\n/noteheads.s2/noteheads.s2/scripts.ufermata/noteheads.s2\n/noteheads.s2\ndb\n/noteheads.s2/noteheads.s2\nTB \n/accidentals.M2/clefs.G /noteheads.s2/accidentals.2 /timesig.C44\nca\n/noteheads.s2 /noteheads.s2*\n/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2\ndb\n/noteheads.s2/noteheads.s2\nSA\n/braceleftbigg\nqual:di\nss\ntime:b-sw/bracerightbigg\n\nqual:cons\ntime:t-sw\ntc:-\n\n\n(450, 7.73)ab\nc \n/timesig.C44/accidentals.2 /clefs.F/noteheads.s2/noteheads.s2\nc/noteheads.s2a/noteheads.s2/flags.u3\n/noteheads.s2/noteheads.s2\n/scripts.dfermata/noteheads.s2/scripts.ufermata/noteheads.s2/flags.u3b\n/noteheads.s2\nTB \n/timesig.C44/accidentals.2/accidentals.2/clefs.G /accidentals.2/flags.d3/noteheads.s2b\n/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2\nc/noteheads.s2/flags.d3a\n/noteheads.s2/noteheads.s2\nSA\nTable 2. Patterns discovered by computing background probabilities using a null model.\n[3]\nS-C. Chiu, M-K. Shan, J-L. Huang, and H-F. Li. Min-\ning polyphonic repeating patterns from music data us-\ning bit-string based approaches. In ICME 2009, pages\n1170–1173, 2009.\n[4] D. Conklin. Representation and discovery of vertical\npatterns in music. In Music and Artiﬁcial Intelligence:\nLecture Notes in Artiﬁcial Intelligence, number 2445,\npages 32–42. Springer-Verlag, 2002.\n[5] D. Conklin. Discovery of distinctive patterns in music.\nIntelligent Data Analysis, 14(5), 2010. To appear.\n[6] D. Conklin and M. Bergeron. Representation and dis-\ncovery of feature set patterns in music. Computer Mu-\nsicJournal, 32(1):60–70, 2008.\n[7] S. Dubnov, G. Assayag, O. Lartillot, and G. Bejer-\nano.Usingmachine-learningmethodsformusicalstyle\nmodeling. IEEE Computer, 36(10):73–80, 2003.[8] D. Huron. Voice denumerability in polyphonic music\nof homogeneous timbres. Music Perception, 6(4):361–\n382, 1989.\n[9] K. Kennan. Counterpoint. Prentice-Hall, 1959.\n[10] D. Meredith, K. Lemstr ¨om, and G. A. Wiggins. Algo-\nrithmsfordiscoveringrepeatedpatternsinmultidimen-\nsional representations of polyphonic music. Journal of\nNew Music Research, 31(4):321–345, 2002.\n[11] U.Pompe,I.Kononenko,andT.Mak ˇse.Anapplication\nofILPinamusicaldatabase: Learningtocomposethe\ntwo-voice counterpoint. In Proceedings of the MLnet\nFamiliarizationWorkshoponDataMiningwithInduc-\ntive Logic Programing, pages 1–11, Heraklion, Crete,\n1996.\n[12] P.E.UtgoffandP.B.Kirlin.Detectingmotivesandre-\ncurring patterns in polyphonic music. In ICMC, pages\n487–494, New Orleans, Lousianna, USA., 2006.\n206\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "Automatic Music Tagging With Time Series Models.",
        "author": [
            "Emanuele Coviello",
            "Luke Barrington",
            "Antoni B. Chan",
            "Gert R. G. Lanckriet"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1415274",
        "url": "https://doi.org/10.5281/zenodo.1415274",
        "ee": "https://zenodo.org/records/1415274/files/CovielloBCL10.pdf",
        "abstract": "State-of-the-art systems for automatic music tagging model music based on bag-of-feature representations which give little or no account of temporal dynamics, a key characteristic of the audio signal. We describe a novel approach to automatic music annotation and retrieval that captures temporal (e.g., rhythmical) aspects as well as tim- bral content. The proposed approach leverages a recently proposed song model that is based on a generative time series model of the musical content — the dynamic tex- ture mixture (DTM) model — that treats fragments of au- dio as the output of a linear dynamical system. To model characteristic temporal dynamics and timbral content at the tag level, a novel, efficient hierarchical EM algorithm for DTM (HEM-DTM) is used to summarize the common in- formation shared by DTMs modeling individual songs as- sociated with a tag. Experiments show learning the seman- tics of music benefits from modeling temporal dynamics.",
        "zenodo_id": 1415274,
        "dblp_key": "conf/ismir/CovielloBCL10",
        "keywords": [
            "automatic music tagging",
            "bag-of-feature representations",
            "temporal dynamics",
            "audio signal",
            "novel approach",
            "timbral content",
            "generative time series model",
            "dynamic texture mixture",
            "efficient hierarchical EM algorithm",
            "hem-dtm"
        ],
        "content": "AUTOMATIC MUSIC TAGGING WITH TIME SERIES MODELS\nEmanuele\nCoviello\nUniversity of California,\nSan Diego\nDept. of Electrical and\nComputer Engineering\necoviell@ucsd.eduLukeBarrington\nUniversity of California,\nSan Diego\nDept. of Electrical and\nComputer Engineering\nlukeinusa@gmail.comAntoni B. Chan\nCity University\nof Hong Kong\nDept. of Computer\nScience\nabchan@cityu.edu.hkGert. R. G. Lanckriet\nUniversity of California,\nSan Diego\nDept. of Electrical and\nComputer Engineering\ngert@ece.ucsd.edu\nABSTRACT\nState-of-the-art systems for automatic music tagging\nmodel music based on bag-of-feature representations\nwhich give little or no account of temporal dynamics, a\nkey characteristic of the audio signal. We describe a novel\napproach to automatic music annotation and retrieval that\ncapturestemporal(e.g.,rhythmical)aspectsaswellastim-\nbral content. The proposed approach leverages a recently\nproposed song model that is based on a generative time\nseries model of the musical content — the dynamic tex-\nture mixture (DTM) model — that treats fragments of au-\ndio as the output of a linear dynamical system. To model\ncharacteristictemporaldynamicsandtimbralcontentatthe\ntag level, a novel, efﬁcient hierarchical EM algorithm for\nDTM (HEM-DTM) is used to summarize the common in-\nformation shared by DTMs modeling individual songs as-\nsociatedwithatag. Experimentsshowlearningtheseman-\ntics of music beneﬁts from modeling temporal dynamics.\n1. INTRODUCTION\nThis paper concerns automatic tagging of music with de-\nscriptive keywords (e.g., genres, emotions, instruments,\nusages, etc.), based on the content of the song. Music\nannotations can be used for a variety of purposes, such\nas searching for songs exhibiting speciﬁc qualities (e.g.,\n“jazz songs with female vocals and saxophone”), or re-\ntrievalofsemanticallysimilarsongs(e.g.,generatingplay-\nlistsbased on songs with similarannotations).\nState-of-the-art music “auto-taggers” model a song as\na “bag of audio features” [7,9–11,14]. The bag of fea-\ntures representation extracts audio features from the song\nat a regular time interval, but then treats these features in-\ndependently, ignoring the temporal order or dynamics be-\ntweenthem. Hence,thisrepresentationfailstoaccountfor\nthe longer term musical dynamics (e.g. tempo and beat)\nor temporal structures (e.g. riffs and arpeggios), which are\nclearly important characteristics of a musical signal.\nWeaddressthislimitationbyadoptingthedynamictex-\nture (DT) model [6], a generative, time-series model of\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnotmadeordistributedforproﬁtorcommercialadvantageandthatcopies\nbear this noticeand thefull citation on theﬁrst page.\nc/circlecopyrt2010International Society for Music Information Retrieval.musical content that captures longer-term time dependen-\ncies. TheDTmodelissimilartotheHiddenMarkovmodel\n(HMM) which has proven robust in music identiﬁcation\n[12]. The difference is that HMMs require to quantize the\naudio signal into a ﬁxed number of discrete “phonemes”,\nwhile the DT has a continuous state space that is a more\nﬂexible model for music.\nMusical time series often show signiﬁcant structural\nchanges within a single song and have dynamics that are\nonly locally homogeneous. Hence, [1] proposes to model\nthe audio fragments from a single song as a dynamic tex-\nture mixture (DTM) model [3], for the task of automatic\nmusicsegmentation. These results demonstrated that the\nDTMprovidesanaccuratesegmentationofmusicintoho-\nmogeneous, perceptually similar segments (corresponding\nto what a human listener would label as ‘chorus’, ‘verse’,\n‘bridge’, etc.) by capturing temporal as well as textural\naspects of the musical signal.\nIn this paper, we adopt the DTM model to propose a\nnovel approach to the task of automatic music annotation\nthat accounts for both the timbral content and the tempo-\nral dynamics that are predictive of a semantic tag. We\nﬁrst model all songs in a music database as DTMs, cap-\nturing longer-term time dependencies and instantaneous\nspectral content at the song-level. Second, the character-\nistic temporal and timbral aspects of musical content that\narecommonlyassociatedwithasemantictagareidentiﬁed\nbylearninga tag-levelDTMthatsummarizesthecommon\nfeatures of a (potentially large) set of song-level DTMs\nfor the tag. Given all song-level DTMs associated with a\nparticular tag, the common information is summarized by\nclustering similar song-level DTs using a novel, efﬁcient\nhierarchical EM (HEM-DTM) algorithm. This gives rise\nto a tag-level DTM with few mixture components (as op-\nposedtotag-levelGaussianmixturemodelsin[14],which\ndo not capture temporal dynamics). Experimental results\nshow that the proposed time-series model improves anno-\ntationandretrieval,inparticularfortagswithtemporaldy-\nnamics that unfold inthe time span of a few seconds.\nThe remainder of this paper is organized as follows. In\nSection 2, we present the annotation and retrieval system\nusing time-series data, while in in Section 3, we present\nanefﬁcienthierarchicalEMalgorithmfordynamictexture\nmixtures. Finally, in Sections 4 and 5, we present experi-\nments using DTM for music annotation and retrieval.\n81\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)2. ANNOTATION AND RETRIEVAL\nIn\nthis section we formulate the tasks of annotation and\nretrieval of audio data as a semantic multi-class labeling\n(SML) problem [2] in the context of time-series models.\n2.1 Notation\nA songsis represented as a collection of Toverlapping\ntime series Y={y1\n1:τ, ..., yT\n1:τ}, where each yt\n1:τrepre-\nsentsτsequential audio feature vectors extracted by pass-\ning a short-time window over the audio signal (also called\nanaudiofragment). Thenumberoffragments, T,depends\non the length of the song. The semantic content of a song\nwith respect to a vocabulary Vof size|V|is represented\nin an annotation vector c= [c1, ..., c |V|], whereck>0\nonlyifthereisapositiveassociationbetweenthesongand\nthe word wk, otherwise ck= 0. Each semantic weight,\nck, represents the degree of association between the song\nand word wk. The data set Dis a collection of |D|song-\nannotation pairs (Yd,cd).\n2.2 Music Annotation\nWe treat annotation as a semantic multi-class problem [2,\n14]inwhicheachclassisaword w,fromavocabulary Vof\nuniquetags(e.g.,“bassguitar”,“hiphop”,“boring”). Each\nwordwkismodeledwithaprobabilitydistributionoverthe\nspaceofaudiofragments, p(yt\n1:τ|wk). Theannotationtask\nistoﬁndthesubset W={w1, ..., w A} ⊆ VofAwords\nthat best describe a novel song Y.\nGiven the audio fragments of a novel song Y, the most\nrelevant words are the ones with highest posterior proba-\nbility, computed using Bayes’ rule:\np(wk|Y) =p(Y|w k)p(wk)\np(Y), (1)\nwherep\n(wk)is the prior of the kthword, and p(Y) =/summationtext|V|\nk=1p(Y|w k)p(wk)is the song prior. To promote an-\nnotation using a diverse set of words, we assume an uni-\nform prior, p(wk) = 1/|V|. We follow [14] in estimat-\ning the likelihood term in (1) with the geometric aver-\nage of the individual sequence likelihoods, p(Y|w k) =/producttextT\nt=1(p(yt\n1:τ|wk))1\nT. Notethat, unlike bag-of-features\nmodels that discard any dependency between audio fea-\ntures vectors (each describing milliseconds of audio), we\nonlyassumeindependence betweendifferent sequences of\naudio feature vectors (describing seconds of audio). Cor-\nrelationswithinasinglesequenceareaccountedforbythe\nmodel presented in Section 3.\nThe probability that the song Ycan be described by\nwordwkis\npk=p(wk|Y) =/producttextT\nt=1(p(yt\n1:τ|wk))1\nT\n/summationtext|V|\nl=1/producttextT\nt=1(p(yt\n1:τ|wl))1\nT.(2)\nFinally\n, the song can be represented as a semantic multi-\nnomial,p= [p1, ..., p |V|], where each pk=p(wk|Y)\nrepresents the relevance of the kthword for the song, and/summationtext|V|\ni=1pi= 1. We annotate a song with the most likely\ntags according to p, i.e., we select the tags with the words\nwith the largest probability.2.3 Music Retrieval\nGiven a query word, songs in the database can be re-\ntrieved based on their relevance to the semantic query\nword1. In particular, the song’s relevance to the query\nwordwkis equivalent to the posterior probability of the\nword,p(wk|Y)in (2). Hence, retrieval involves rank-\nordering the songs in the database based on the k-th entry\n(pk) of the semantic multinomials p.\n2.4 Learning DTM tag models\nIn this paper, we model the tag-level distributions,\np(yt\n1:τ|wk), as dynamic texture mixture models. The tag-\nlevel distributions are estimated from the set of training\nsongs associated with the particular tag. One approach is\nto extract all the audio fragments from the relevant train-\ning songs, and then run the EM algorithm [3] directly on\nthis data to learn the tag-level DTM. This approach, how-\never, requires storing large amounts of audio fragments in\nmemory (RAM) for running the EM algorithm. For even\nmodest-sized databases, the memory requirements can ex-\nceed the RAM capacity of most computers.\nToallowefﬁcienttraininginbothcomputationtimeand\nmemory requirements, we propose to break the learning\nprocedure into two steps. First, a DTM is estimated for\neachsongusingthestandardEMalgorithm[3]. Next,each\ntag-level model is estimated using the hierarchical EM al-\ngorithm on all the song-level DTMs associated with the\nparticulartag. Becausethedatabaseisﬁrstprocessedatthe\nsong-level, the computation can be easily done in parallel\n(overthesongs)andthememoryrequirementisgreatlyre-\nduced to that of processing a single song. The memory\nrequirements for computing the tag-level models is also\nreduced, since each song is succinctly modeled by the pa-\nrameters of a DTM.\nSuch a reduction in computational complexity also\nensures that the tag-level models can be learned from\ncheaper, weakly-labeled data (i.e., missing labels, labels\nwithout segmentation data, etc.) by pooling over large\namountsofaudiodatatoamplifytheappropriateattributes.\nInsummary,adoptingDTM,ortime-seriesmodelsingen-\neral,asatag-modelforSMLannotationrequiresanappro-\npriate HEM algorithm for efﬁciently learning the tag-level\nmodelsfromthesong-levelmodels. Inthenextsection,we\nreviewtheDTMandpresenttheHEMalgorithmforDTM.\n3. HIERARCHICAL EM FOR DTMS\nIn this section, we ﬁrst review the dynamic texture (DT)\nand dynamic texture mixture (DTM) models for modeling\nmusical time-series. We then present the hierarchical EM\nalgorithm for efﬁciently learning a tag-level DTM from a\nset of song-level DTMs.\n3.1 The Dynamic Texture Model\nAdynamictexture[6](DT)isagenerativemodelthattakes\ninto account both the acoustics and the dynamics of audio\nsequences [1]. The model consists of two random vari-\nables,yt, which encodes the acoustic component (audio\n1Notethatalthoughthisworkfocusesonsingle-wordqueries,ourrep-\nresentation easily extends to multiple-word queries [13].\n82\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)featurevector)attime t,andxt, whichencodesthedynam-\nics (evolution) of the acoustic component over time. The\ntwo variables are modeled as a linear dynamical system,\nxt=Axt−1+vt, (3)\nyt=Cxt+wt+ ¯y, (4)\nwherext∈Rnandyt∈Rmare real vectors (typically\nn≪m). Using such a model, we assume that the dy-\nnamics of the audio can be summarized by a more parsi-\nmonious (n < m) hidden state process xt, which evolves\nas a ﬁrst order Gauss-Markov process, and each observa-\ntion variable yt, which encodes the acoustical component\n(audio feature vector at time t) is dependent only on the\ncurrent hidden state xt.\nThe matrix A∈Rn×nis astate transition matrix ,\nwhich encodes the dynamics or evolution of the hidden\nstate variable (e.g., the evolution of the audio track), and\nthe matrix C∈Rm×nis anobservation matrix, which\nencodes the basis functions for representing the audio se-\nquence. The vector ¯y∈Rnis the mean of the dy-\nnamic texture (i.e. the mean audio feature vector). vtis\nadriving noise process, and is zero-mean Gaussian dis-\ntributed , e.g., vt∼ N(0,Q), where Q∈Rn×nis a\ncovariance matrix. wtis theobservation noise and is\nalso zero-mean Gaussian, e.g., wt∼ N(0,R), where\nR∈Rm×mis a covariance matrix. Finally, the initial\ncondition is speciﬁed as x1∼ N(µ,S), whereµ∈Rnis\nthe mean of the initial state, and S∈Rn×nis the covari-\nance. The dynamic texture is speciﬁed by the parameters\nΘ ={A,Q,C,R,µ,S, ¯y}.\nIntuitively, the columns of Ccan be interpreted as the\nprincipalcomponents(orbasisfunctions)oftheaudiofea-\ntures vectors over time. Hence, each audio feature vector\nytcan be represented as a linear combination of principal\ncomponents,withcorrespondingweightsgivenbythecur-\nrenthiddenstate xt. Inthisway,theDTcanbeinterpreted\nas a time-varying PCA representation of an audio feature\nvector time-series.\n3.2 The Dynamic Texture Mixture Model\nA song is a combination of heterogeneous sequences\nwith signiﬁcant structural variations, and hence is not\nwell represented as a single DT model. To address\nthis lack of global homogeneity, [1] proposed to repre-\nsent audio fragments, extracted from a song, as sam-\nples from a dynamic texture mixture (DTM) [3], effec-\ntively modeling local structure of the song. The DTM\nmodel [3] introduces an assignment random variable z∼\nmultinomial(π 1,···,πK), which selects one of the K\ndynamic texture components as the source of the audio\nfragment. Each mixture component is parameterized by\nΘz={Az,Cz,Qz,Rz,µz,Sz,¯yz}, and the DTM model\nis parameterized by Θ ={πz,Θz}K\nz=1.\nGiven a set of audio samples, the maximum-likelihood\nparameters of the DTM can be estimated with recourse to\nthe expectation-maximization (EM) algorithm [3], which\nis an iterative optimization method that alternates between\nestimating the hidden variables with the current parame-\nters, and computing new parameters given the estimatedhidden variables (the “complete data”). The EM algo-\nrithmforDTMalternatesbetweenestimatingsecond-order\nstatistics of the hidden-states, conditioned on each audio\nsequence, with the Kalman smoothing ﬁlter (E-step), and\ncomputing new parameters given these statistics(M-step).\nPreviousworkin[1]hassuccessfullyusedtheDTMfor\nthe task of segmenting the structure of a song into acous-\ntically similar sections (e.g., intro, verse, chorus, bridge,\nsolo, outro). In this work, we demonstrate that the DTM\ncan also be used as a tag-level annotation model for mu-\nsic annotation and retrieval. We next present a hierarchi-\ncal EM algorithm for efﬁciently estimating these tag-level\nDTMs from large sets of song-level DTMs, previously es-\ntimated for the set of training songs associated witha tag.\n3.3 Hierarchical EM for learning DTM hierarchies\nGivenaDTMmodelofeachtrainingsongaslearnedinthe\nprevious section, the goal now is to learn a tag-level DTM\nmodel that summarizes the common features of the corre-\nsponding song-level DTMs. First, all song-level DTMs\nwithaparticulartagarepooledtogetherintoasingle,large\nDTM. Next, the common information is summarized by\nclusteringsimilarDTcomponentstogether,forminganew\ntag-levelDTM with fewer mixture components.\nThe DT components are clustered using the hierarchi-\ncal expectation-maximization (HEM) algorithm [15]. At\na high level, this is done by generating virtual samples\nfrom each of the song-level component models, merging\nall the samples, and then running the standard EM algo-\nrithmonthemergedsamplestoformthereducedtag-level\nmixture. Mathematically, however, using the virtual sam-\nples is equivalent to marginalizing over the distribution of\nsong-levelmodels. Hence,thetagmodelcanbelearneddi-\nrectlyandefﬁcientlyfromtheparametersofthesong-level\nmodels, without generating any virtual samples.\nThe HEM algorithm was originally proposed in [15]\nto reduce a Gaussian mixture model (GMM) with many\ncomponents to a representative GMM with fewer com-\nponents and has been successful in learning GMMs from\nlargedatasetsfortheannotationandretrievalofimages[2]\nand music [14]. We next present an HEM algorithm for\nmixtures with components that are dynamic textures [4].\n3.3.1 HEM Formulation\nFormally, let Θ(s)={π(s)\ni,Θ(s)\ni}K(s)\ni=1denote the com-\nbinedsong-levelDTMwith K(s)components,where Θ(s)\ni\nare the parameters for the ithDT component. The like-\nlihood of observing an audio sequence y1:τwith length τ\nfromthe combined song-level DTM Θ(s)is given by\np(y1:τ|Θ(s)) =K(s)/summationdisplay\ni=1π(s)\nip(y1:τ|z(s)=i,Θ(s)),(5)\nwherez∼multinomial(π(s)\n1,···π(s)\nK(s))is the hid-\nden variable that indexes the mixture components.\np(y1:τ|z(s)=i,Θ(s))is the likelihood of the audio y1:τ\nunder the ithDT mixture component, and π(s)\niis the prior\nweight for the ithcomponent. The goal is to ﬁnd a tag-\nlevel annotation DTM, Θ(a)={π(a)\nj,Θ(a)\nj}K(a)\nj=1, which\n83\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)represents(5)usingfewernumberofmixturecomponents,\nK(a), (i.e.,K(a\n)< K(s)). The likelihood of observing an\naudio sequence y1:τfromthe tag-level DTM Θ(a)is\np(y1:τ|Θ(a)) =K(a)/summationdisplay\nj=1π(a)\njp(y1:τ|z(a)=j,Θ(a)),(6)\nwherez(a)∼multinomial(π(a)\n1,···,π(a)\nK(a))is the hid-\nden variable for indexing components in Θ(a). Note that\nwe will always use iandjto index the components of\nthe song-level model, Θ(s), and the tag-level model, Θ(a),\nrespectively. To reduce clutter, we will also use the short-\nhandΘ(s)\niandΘ(a)\njto denote the ithcomponent of Θ(s)\nandthejthcomponentof Θ(a),respectively. Forexample,\nwe denote p(y1:τ|z(s)=i,Θ(s)) =p(y1:τ|Θ(s)\ni).\n3.3.2 Parameter estimation\nToobtainthetag-levelmodel,HEM[15]considersasetof\nNvirtual observations drawn from the song-level model\nΘ(s), such that Ni=Nπ(s)\nisamples are drawn from the\nithcomponent. Wedenotethesetof Nivirtualaudiosam-\nples for the ithcomponent as Yi={y(i,m)\n1:τ}Ni\nm=1, where\ny(i,m)\n1:τ∼Θ(s)\niis a single audio sample and τis the length\nof the virtual audio (a parameter we can choose). The en-\ntire set of Nsamples is denoted as Y={Yi}K(s)\ni=1. To\nobtain a consistent hierarchical clustering, we also assume\nthat all the samples in a set Yiare eventually assigned to\nthesametag-levelcomponent Θ(a)\nj. Theparametersofthe\ntag-level model can then be estimated by maximizing the\nlikelihood of the virtual audio samples,\nΘ(a)∗= argmax\nΘ(a)logp(Y|Θ(a)), (7)\nwhere\nlogp(Y|Θ(a)) = logK(s)/productdisplay\ni=1p(Yi|Θ(a)) (8)\n= logK(s)/productdisplay\ni=1K(a)/summationdisplay\nj=1π(a)\nj/integraldisplay\np(Yi,Xi|Θ(a)\nj)dXi(9)\nandXi={x(i,m)\n1:τ}are the hidden-state variables corre-\nsponding to Yi. Computing the log-likelihood in (9) re-\nquires marginalizing over the hidden assignment variables\nz(a)\niand hidden state variables Xi. Hence, (7) can also be\nsolvedwithrecoursetotheEMalgorithm[5]. Inparticular,\neach iteration consists of\nE-Step:Q(Θ(a),ˆΘ(a)) =EX,Z|Y,ˆΘ(a)[logp(X,Y,Z |Θ(a))]\nM-Step:Θ(a)∗= argmax\nΘ(a)Q(Θ(a),ˆΘ(a))\nwhereˆΘ(a)is the current estimate of the tag-level model,\np(X,Y,Z |Θ(a))is the “complete-data” likelihood, and\nEX,Z|Y,ˆΘ(a)is the conditional expectation with respect to\nthe current model parameters.\nAsiscommonwiththeEMformulation,weintroducea\nhiddenassignmentvariable zi,j,whichisanindicatorvari-\nableforwhentheaudiosampleset Yiisassignedtothe jthcomponent of Θ(a), e.g., when z(a)\ni=j. The complete-\ndata log-likelihood is then\nlogp(X,Y,Z |Θ(a)) (10)\n=K(s)/summationdisplay\ni=1K(a)/summationdisplay\nj=1zi,jlogπ(a)\nj+zi,jlogp(Yi,Xi|Θ(a)\nj).\nTheQfunction is then obtained by taking the conditional\nexpectation of (10), and using the law of large numbers to\nremove the dependency on the virtual samples. The result\nis aQfunction that depends only on the parameters of the\nsong-level DTs Θ(s)\ni.\nThe HEM algorithm for DTM is summarized in Algo-\nrithm 1. In the E-step, the expectations in Eq. (11) are\ncomputed for each song-level DT Θ(s)\niand current tag-\nlevel DT ˆΘ(a)\nj. These expectations can be computed using\n“suboptimalﬁlteranalysis”or“sensitivityanalysis”[8]on\nthe Kalman smoothing ﬁlter (see [4]). Next, the probabil-\nityofassigningthesong-levelDT Θ(s)\nitothetag-levelDT\nˆΘ(a)\njis computed according to (12), and the expectations\nare aggregated over all the song-level DTs in (14). In the\nM-step, the parameters for each tag-level component ˆΘ(a)\nj\nare recomputed according to the update equations in (15).\nMore details are available in [4].\n4. MUSIC DATA\nIn this section we describe the music collection and the\naudio features used in our experiments.\nThe CAL500 [14] dataset consists of 502 Western pop-\nularsongsfromthelast50yearsfrom502differentartists.\nEach song has been annotated by at least 3 humans, using\na semantic vocabulary of 174 words that includes genres,\ninstruments,vocalcharacteristics,emotions,acousticchar-\nacteristics,andsongusages. CAL500provideshardbinary\nannotations,whichare1whenatagappliestothesongand\n0whenthetagdoesnotapply. Weﬁndempiricallythatac-\ncuratelyﬁttingtheHEM-DTMmodelrequiresasigniﬁcant\nnumberoftrainingexamplessowerestrictourattentionto\nthe 78 tags with at least 50 examples.\nA popular feature for content-based music analysis,\nMel-frequency cepstral coefﬁcients (MFCCs) concisely\nsummarizetheshort-timecontentofanacousticwaveform\nby using the discrete cosine transform (DCT) to decorre-\nlate the bins of a Mel-frequency spectral histogram2. In\nSection 3.1 we noted how the DT model can be viewed as\na time varying PCA representation of the audio features.\nThisideasuggeststhatwecanrepresentthespectrumover\ntime as the output of the DT model yt. In this case,\nthe columns of the observation matrix C(PCA matrix)\nare analogous to the DCT basis functions, and the hidden\nstatesxtare the coefﬁcients (analogous to the MFCCs).\nThe advantage with this formulation is that a different C\nmatrix, i.e., basis functions, can be learned to best rep-\nresent the particular song or semantic concept of interest.\n2Thisdecorrelationisusuallyconvenientinthatitreducesthenumber\nof parameters to be estimated.\n84\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Algorithm 1 HEM algorithm for DTM\n1:Input: combinedsong-levelDTM {Θ(s)\ni,π(s)\ni}K(s)\ni=1,number\nof virtual samples N.\n2:Initialize tag-level DTM, {ˆΘ(a)\nj,π(a)\nj}K(a)\nj=1.\n3:repeat\n4:{E-step}\n5:Compute expectations using sensitivity analysis for each\nΘ(s)\niandˆΘ(a)\nj(see [4]):\nˆx(i)\nt|j=Ey|Θ(s)\ni/bracketleftbigg\nEx|y,ˆΘ(a)\nj[xt]/bracketrightbigg\n,\nˆP(i)\nt,t|j=Ey|Θ(s)\ni/bracketleftbigg\nEx|y,ˆΘ(a)\nj[xtxT\nt]/bracketrightbigg\n,\nˆP(i)\nt,t−1|j=Ey|Θ(s)\ni/bracketleftbigg\nEx|y,ˆΘ(a)\nj[xtxT\nt−1]/bracketrightbigg\n,\nˆW(i)\nt|j=Ey|Θ(s)\ni/bracketleftbigg\n(yt−¯yj)Ex|y,ˆΘ(a)\nj[xt]T/bracketrightbigg\n,\nˆU(i)\nt|j=Ey|Θ(s)\ni/bracketleftbig\n(yt−¯yj)(yt−¯yj)T/bracketrightbig\n,\nˆu(i)\nt=Ey|Θ(s)\ni[yt],\nℓi|j=EΘ(s)\ni[logp(y1:τ|ˆΘ(a)\nj)].(11)\n6:Compute assignment probability and weighting:\nˆzi,j=π(a)\njexp/parenleftbig\nNiℓi|j/parenrightbig\n/summationtextK(a)\nj′=1π(a)\nj′exp/parenleftbig\nNiℓi|\nj′/parenrightbig(12)\nˆwi,j=ˆzi,jNi=ˆzi,jπ(s)\niN (13)\n7:Computed aggregate expectations for each ˆΘ(a)\nj:\nˆNj=/summationtext\niˆzi,j, η j=/summationtext\niˆwi,jˆP(i)\n1,1|j,\nˆMj=/summationtext\niˆwi,j, γ j=/summationtext\niˆwi,j/summationtextτ\nt=1ˆu(i)\nt,\nξj=/summationtext\niˆwi,jˆx(i)\n1|j, βj=/summationtext\niˆwi,j/summationtextτ\nt=1ˆx(i)\nt|j,\nΦj=/summationtext\niˆwi,j/summationtextτ\nt=1ˆP(i)\nt,t|j,\nΨj=/summationtext\niˆwi,j/summationtextτ\nt=2ˆP(i)\nt,t−1|j,\nϕj=/summationtext\niˆwi,j/summationtextτ\nt=2ˆP(i)\nt,t|j,\nφj=/summationtext\niˆwi,j/summationtextτ\nt=2ˆP(i)\nt−1,t−1|j,\nΛj=/summationtext\niˆwi,j/summationtextτ\nt=1ˆU(i)\nt|j,\nΓj=/summationtext\niˆwi,j/summationtextτ\nt=1ˆW(i)\nt|j.(14)\n8:{M-step}\n9:Recompute parameters for each component ˆΘ(a)\nj:\nC∗\nj= ΓjΦ−1\nj, R∗\nj=1\nτˆMj(Λj−C∗\njΓj),\nA∗\nj= Ψjφ−1\nj, Q∗\nj=1\n(τ−1)ˆMj(ϕj−A∗\njΨT\nj),\nµ∗\nj=1\nˆMjξj, S∗\nj=1\nˆMjηj−µ∗\nj(µ∗\nj)T,\nπ∗\nj=/summationtextK(s)\ni=1ˆzi,\nj\nK(s),¯y∗\nj=1\nτˆMj(γj−C∗\njβj).(15)\n10:untilcon\nvergence\n11:Output: tag-level DTM {Θ(a)\nj,π(a)\nj}K(a)\nj=1.\nFurthermore, since we explicitly model the temporal evo-\nlution\nof the spectrum, we do not need to include the in-\nstantaneous deltas of the MFCCs.\nOur experiments use 34 Mel-frequency bins, computed\nfrom half-overlapping, 46ms audio segments. Each audio\nfragment is described by a time series yt\n1:τofτ= 450\nsequential audio feature vectors, which corresponds to\n10 seconds. Song-level DTM models are learned from\na dense sampling of audio fragments of 10 seconds, ex-\ntracted every 1 second.Model P R F-score AROC MAP P10\nHEM-GMM 0.49 0.23 0.26 0.66 0.45 0.47\nCB\nA 0.41 0.24 0.29 0.69 0.47 0.49\nHEM-DTM 0.47 0.25 0.30 0.69 0.48 0.53\nTable 1. Annotation and retrieval results for HEM-DTM\nand\nHEM-GMM.\n5. EVALUATION\nSong-level DTMs were learned with K= 16compo-\nnents and state-space dimension n= 7, using EM-DTM.\nTag-levelDTMswerelearnedbypoolingtogetherallsong\nmodels associated with a given tag and reducing the result\nto a DTM with K(r)= 2components with HEM-DTM.\nTo reduce the effect of low likelihoods in high dimen-\nsions, we normalize the single-segment likelihood terms,\ne.g.,p(yt\n1:τ|wk), by the length of the sequence τ.\nTo investigate the advantage of the DTM’s temporal\nrepresentation, we compare the auto-tagging performance\nof our model (HEM-DTM) to the hierarchically trained\nGaussian mixture models (HEM-GMM) from [14], a gen-\nerative model that ignores temporal dynamics. A compar-\nison to the CBA model of [9] is provided as well. We fol-\nlow the procedure of [14] for training HEM-GMMs, and\nour CBA implementation follows [9], with the modiﬁca-\ntionthatthecodebookisconstructedusingonlysongsfrom\nthe training set. All reported metrics are the results of 5-\nfold cross validation where each song appeared in the test\nset exactly once.\n5.1 Annotation and Retrieval\nAnnotation performance is measured following the proce-\ndurein[14]. Testsetsongsareannotatedwiththe10most\nlikely tags in their semantic multinomial (Eq. 2). Anno-\ntation accuracy is reported by computing precision, recall\nand F-score for each tag, and then averaging over all tags.\nFor a detailed deﬁnition of the metrics, see [14].\nTo evaluate retrieval performance, we rank-order test\nsongs for each single-tag query in our vocabulary, as de-\nscribed in Section 2. We report mean average precision\n(MAP), area under the receiver operating characteristic\ncurve (AROC) and top-10 precision (P10), averaged over\nallthequerytags. TheROCcurveisaplotoftruepositive\nrateversusfalsepositiverateaswemovedowntheranked\nlist. Random guessing would result in an AROC of 0.5.\nThe top-10 precision is the fraction true positives in the\ntop-10oftheranking. MAPaverages theprecisionateach\npoint in the ranking where a song is correctly retrieved.\n5.2 Results\nAnnotation and retrieval results are presented in Table 1,\ndemonstratingsuperiorperformanceforHEM-DTM,com-\npared to HEM-GMM,for all metrics except for precision.\nThis indicates that HEM-DTM is slightly more aggressive\nwhen annotating songs, but still its annotations are more\naccurate, as evidenced by the higher F-score. HEM-DTM\nperforms better than CBA in all metrics. For retrieval, al-\nthough AROC scores are comparable for CBA and HEM-\nDTM, HEM-DTM clearly improves the top of the ranked\nlistmore,asevidencedbythehigherprecision-at-10score.\n85\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Tag HEM-DTM HEM-GMM\nF-s\ncore MAP F-score MAP\nHEM-DTM better than HEM-GMM\nmale lead vocals 0.44 0.87 0.08 0.81\nfemale\nlead vocals 0.58 0.69 0.42 0.44\nfast rhythm 0.40 0.48 0.20 0.42\nclassic rock 0.41 0.37 0.18 0.36\nacoustic guitar 0.44 0.43 0.31 0.44\nelectric guitar 0.32 0.35 0.14 0.34\nHEM-GMM better than HEM-DTM\nmellow 0.34 0.41 0.37 0.49\nslo\nw rhythm 0.45 0.60 0.44 0.62\nweak 0.22 0.26 0.26 0.25\nlightbeat 0.36 0.58 0.53 0.61\nsad 0.13 0.23 0.28 0.30\nnegativefeelings 0.27 0.33 0.35 0.36\nTable 2. Annotation and retrieval results for some tags\nwith\nHEM-DTM and HEM-GMM.\nHEM-DTMclassic rock, driving, energy, fast,male\nlead vocals ,electric guitar, electric, indif-\nferent, powerful, rough\nHEM-GMMboring,major, acoustic, driving, not like-\nable, female lead vocals, recording quality,\ncold,synthesized, pop , guitar\nTable 3. Automatic 10-word annotations for ‘Every little\nthing\nshe does is magic’ by Police.\nHEM-DTM performs better on average by capturing\ntemporal dynamics (e.g., tempo, rhythm, etc.) over sec-\nonds of audio content. Modeling temporal dynamics can\nbeexpectedtoprovebeneﬁcialforsometags,whileadding\nno beneﬁt for others. Indeed, some tags might either be\nmodeledadequatelybyinstantaneouscharacteristicsalone\n(e.g., timbre), or require a global song model. Table 2\nlists annotation (F-score) and retrieval (MAP) results for a\nsubset of our vocabulary. As expected, HEM-DTM shows\nstrong improvements for tags associated with a clear tem-\nporal structure. For the genre “classic rock”, which has\ncharacteristic tempo and rhythm, HEM-DTM achieves an\nF-score of approximately 0.4, doubling the performance\nof HEM-GMM. Similarly, HEM-DTM proves particularly\nsuitable for tags with signiﬁcant temporal structure, e.g.,\n“male lead vocals” and “fast rhythm”, or the instruments\nsuch as electric or acoustic guitar. Conversely, our HEM-\nDTM shows no improvement over HEM-GMM when pre-\ndictingtagsforwhichtemporalstructureislesssigniﬁcant,\nsuch as “mellow” and “negative feelings”.\nFinally, Tables 3 and 4 show example annotations and\nretrieval rankings for both HEM-DTM and HEM-GMM.\nGround truthresults are marked in bold.\n6. CONCLUSIONS\nWe have presented the dynamic texture mixture model; a\nprincipled approach for capturing the temporal, as well as\ntimbral qualities of music. We derived a hierarchical al-Rank HEM-DTM\n1 James Taylor `Fire and rain'\n2 Arlo Guthrie `Alices restaurant massacree'\n3 Zombies ‘Beechwood park’\n4Crosby, Stills, Nash and Young `Teachyourchildren'\n5 Donovan `Catch thewind'\n6 American music club `Jesus hands'\n7 Aaron Neville ‘Tell itlike it is’\n8 10cc ‘For you and i’\n9 Byrds ‘Wasn’t bornto follow’\n10 Beautifulsouth ‘Onelast lovesong’\nRank HEM-GMM\n1 Stranglers ‘Golden brown’\n2Cr\nosby, Stills, Nash and Young `Teachyourchildren'\n3 Pet shop boys ‘Being boring’\n4 Counting crows ‘Speedway’\n5 Beth quist ‘Survival’\n6 Beautifulsouth ‘Onelast lovesong’\n7 Neutral milk hotel ‘Whereyou’llﬁnd me now’\n8 Police ‘Everylittle thing she does is magic’\n9 Ericclapton ‘Wonderfultonight’\n10 Belle and Sebastian ‘LikeDylan in the movies’\nTable 4. Top retrieved songs for ‘acoustic guitar’.\ngorithm\nfor efﬁciently learning DTM models from large\ntraining sets, enabling its usage as a tag model for seman-\ntic annotation and retrieval. Experimental results demon-\nstrate that the new model improves accuracy over current\nbag-of-feature approaches.\nAcknowledgments E.C., L.B. and G.R.G.L. wish to\nacknowledge support from NSF grants DMS-MSPA\n0625409 and CCF-0830535.\n7. REFERENCES\n[1] L. Barrington, A.B. Chan, and G. Lanckriet. Modeling music as a\ndynamictexture. IEEETASLP, 18(3):602–612,2010.\n[2] G. Carneiro, A. B. Chan, P. J. Moreno, and N. Vasconcelos. Super-\nvisedlearningofsemanticclassesforimageannotationandretrieval.\nIEEETPAMI, 29(3):394–410,March 2007.\n[3] A. B. Chan and N. Vasconcelos. Modeling, clustering, and seg-\nmenting video with mixtures of dynamic textures. IEEE TPAMI,\n30(5):909–926,May 2008.\n[4] A.B. Chan, E. Coviello, and G. Lanckriet. Clustering dynamic tex-\ntures with thehierarchical em algorithm. In CVPR,2010.\n[5] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood\nfrom incompletedata via theEM algorithm. JRSS B.\n[6] G. Doretto, A. Chiuso, Y. N. Wu, and S. Soatto. Dynamic textures.\nIntl. J.Computer Vision, 51(2):91–109,2003.\n[7] D.Eck,P.Lamere,T.Bertin-Mahieux,andS.Green.Automaticgen-\nerationofsocialtagsformusicrecommendation.In AdvancesinNeu-\nralInformation Processing Systems , 2007.\n[8] A. Gelb. Applied Optimal Estimation. MIT Press, 1974.\n[9] M. Hoffman, D. Blei, and P. Cook. Easy as CBA: A simple proba-\nbilistic model for tagging music. In ISMIR,2009.\n[10] M.I. Mandel and D.P.W. Ellis. Multiple-instance learning for music\ninformation retrieval. In InternationalConference on MIR, 2008.\n[11] S.R. Ness, A. Theocharis, G. Tzanetakis, and L.G. Martins. Improv-\ning automatic music tag annotation using stacked generalization of\nprobabilistic svm outputs. In Proceedings ofACM MULT., 2009.\n[12] J.ReedandC.H.Lee.Astudyonmusicgenreclassiﬁcationbasedon\nuniversalacoustic models. ISMIR,2006.\n[13] D. Turnbull, L. Barrington, D. Torres, and G. Lanckriet. Towards\nmusical query-by-semantic description using the CAL500 data set.\nSIGIR, page439446,2007.\n[14] D. Turnbull, L. Barrington, D. Torres, and G. Lanckriet. Semantic\nannotation and retrieval of music and sound effects. IEEE TASLP,\n16(2):467–476,February 2008.\n[15] N. Vasconcelos and A. Lippman. Learning mixture hierarchies. In\nNIPS,1998.\n86\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "A Segmentation-based Tempo Induction Method.",
        "author": [
            "Maxime Le Coz",
            "Hélène Lachambre",
            "Lionel Koenig",
            "Régine André-Obrecht"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1416766",
        "url": "https://doi.org/10.5281/zenodo.1416766",
        "ee": "https://zenodo.org/records/1416766/files/CozLKA10.pdf",
        "abstract": "The automatized beat detection and localization have been the subject of multiple research in the field of music infor- mation retrieval. Most of the methods are based on onset detection. We propose an alternative approach: Our method is based on the “Forward-Backward seg- mentation”: the segments may be interpreted as attacks, decays, sustains and releases of notes. We process the seg- ment boundaries as a weighted Dirac signal. Three meth- ods devived from its spectral analysis are proposed to find a periodicity which corresponds to the tempo. The experiments are carried out on a corpus of 100 songs of the RWC database. The performances of our system on this base demonstrate a potential in the use of a “ Forward- Backward Segmentation” for temporal information retrieval in musical signals.",
        "zenodo_id": 1416766,
        "dblp_key": "conf/ismir/CozLKA10",
        "keywords": [
            "automatized beat detection",
            "music information retrieval",
            "onset detection",
            "Forward-Backward segmentation",
            "weighted Dirac signal",
            "temporal information retrieval",
            "RWC database",
            "periodicity",
            "tempo",
            "system performance"
        ],
        "content": "A SEGMENTATION-BASED TEMPO INDUCTION METHOD\nMaxime Le Coz, Helene Lachambre, Lionel Koenig and Regine Andre-Obrecht\nIRIT, Universite Paul Sabatier,\n118 Route de Narbonne, F-31062 TOULOUSE CEDEX 9\nflecoz,lachambre,koenig,obrechtg@irit.fr\nABSTRACT\nThe automatized beat detection and localization have been\nthe subject of multiple research in the ﬁeld of music infor-\nmation retrieval. Most of the methods are based on onset\ndetection. We propose an alternative approach:\nOur method is based on the “Forward-Backward seg-\nmentation”: the segments may be interpreted as attacks,\ndecays, sustains and releases of notes. We process the seg-\nment boundaries as a weighted Dirac signal. Three meth-\nods devived from its spectral analysis are proposed to ﬁnd\na periodicity which corresponds to the tempo.\nThe experiments are carried out on a corpus of 100 songs\nof the RWC database. The performances of our system on\nthis base demonstrate a potential in the use of a “ Forward-\nBackward Segmentation” for temporal information retrieval\nin musical signals.\n1. INTRODUCTION\nThe automatized beat detection and localization have been\nthe subject of multiple research in the ﬁeld of music in-\nformation retrieval. The study of beat is indeed important\nas the structure of a music piece lies in the beat. West-\nern music uses however different levels in the hierarchy of\nscale measuring time. We have to distinguish the tatum\nwhich is “the regular time division that mostly coincides\nwith all note onsets” [3] from the tactus which is deﬁned\nas the rate at which most people would clap their hands\nwhen listening to the music [8]. Here, we look for the tac-\ntus, which will be named tempo and measured in beat per\nminute (BPM).\nSeveral methods have been suggested in order to extract\nthe tempo information from an audio signal. Most of them\nuse an onset detection method as onset localization carries\nthe temporal structure that leads to the estimation of the\ntempo. Theses methods use different observation features\nin order to propose a list of onset positions. They are very\ndependent on that detection. Dixon’s ﬁrst algoritm [4] uses\nan energy based detector in order to track the onset posi-\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2010 International Society for Music Information Retrieval.tions. Then a clustering is performed on the inter-onset-\ninterval values. Some best clusters are chosen as possible\nhypothesis. A hypothesis is ﬁnally validated with a beat\ntracking.\nIn Alonso’s algorithm [1], onset positions are deducted\nby using a time-frequency representation and a differen-\ntiator FIR ﬁlter to detect sudden changes in the dynamics,\ntimbre or harmonic structure. The tempo is then deduced\nusing either the autocorrelation or spectral product.\nKlapuri [9] proposes a more complex way of extract-\ning the onset positions. The loudness differentials in fre-\nquency subbands are computed and combined in order to\ncreate four accent bands. This aims at detecting harmonic\nor melodic changes as well as percussive changes. Using\ncomb ﬁlter resonators to extract features, and probalistic\nmodels, the values of tatum, tactus andmeasure meter are\ncomputed.\nUhle [12] suggests a method based on the segmenta-\ntion of the signal into long-term segments corresponding\nto its musical structure (for example, the verses and cho-\nrus of a song). The amplitude envelope of logarithmically\nspaced frequency subbands is computed; its slope signal\naims to represent accentuation on the signal. The analysis\nof an autocorrelation function on 2.5 second segments in-\nside each long-term segment gives the tatum estimator. A\nlarger-scale analysis over 7.5 second segments is then per-\nformed in order to give values corresponding to the mea-\nsure. The local maxima positions of the autocorrelation\nfunction are ﬁnally compared with a bank of pre-deﬁned\npatterns in order to deﬁne the best value of the tempo on\nthe long term segment.\nDixon [5] has proposed an alternative method to onset\ncalculation. The signal is splitted into 8 frequency bands\nand autocorrelation is performed on each smoothed and\ndownsampled subband. The three highest peaks of each\nband are selected and combined in order to determine the\nﬁnal tempo estimation.\nAnother algorithm is that of Scheirer [10]. This algo-\nrithm performs a comb ﬁlterbank that seeks for periodi-\ncally spaced clock pulse that best matches the envelope of\n6 frequency subbands.\nTzanetakis [11] suggests a method based on a wavelet\ntransform analysis. This analysis is performed over 3 sec-\nond signal segments with 50% of overlap. On each seg-\nment, amplitude envelope of 5 octave-spaced frequency\nbands is extracted. Autocorrelation is then computed. Three\n27\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)kind of autocorrelation analysis are computed in order to\nestimate the value of the tempo. The ﬁrst one is the me-\ndian of highest peak of the sum of the envelopes over every\nwindow. The second one returns the median value of the\nhighest peak on each subband and each segment. The last\none computes several best peaks from the autocorrelation\non the sum of every envelope and then chooses the most\nfrequent value.\nOur method is based on the analysis of an automatic\nsegmentation of the signal into quasi-stationary segments :\nthe segments may be interpreted as attacks, decays, sus-\ntains and releases of notes. So we propose to process the\nsegment boundaries in order to ﬁnd a periodicity which\nwould correspond to the tempo.\nIn section 2, we describe the segmentation used as a\nfront-end, the analysis of this segmentation in the frequency\ndomain and the different methods we use to extract the\nvalue of the tempo in BPM. In the last part, we present\nthe results of our experiments on the RWC [6, 7] corpus.\n2. METHOD\nOur method relies on the detection of quasi-stationnary\nsegments in the audio signal waveform. A frequency anal-\nysis of the boundaries is then performed in order to ﬁnd the\nmost present periodicities and thereby estimate the tempo\nconsequently.\nThe algorithm is based on three steps :\n\u000fSegmentation\n\u000fBoundary frequencial analysis\n\u000fTempo extraction\n2.1 Segmentation\nWe segment the signal using the “Forward Backward Di-\nvergence” [2]. The signal is assumed to be a sequence of\nquasi-stationnary units, each one characterized by the fol-\nlowing gaussian autoregressive model :\n(\nyn=Paiyn\u0000i+en\nvar(en) =\u001b2\nn(1)\nwhereynis the signal and enan uncorrelated zero mean\nGaussian sequence.\nAs the variance \u001bnis constant over an unit and equals \u001b,\nthe model of each area is parametered by the following\nvector :\n(AT;\u001b) = (a1;:::;ap;\u001b) (2)\nThe strategy is to detect changes in the parameters, using a\ndistance based on the mutual conditional entropy. A sub-\njective analysis of the segmentation shows a sub note seg-\nmentation and the location of attacks, sustains and releases.\nFor a solo musical sound, the segments of the signal\ncorrespond to the different steps of a note. On Figure 1,\nwe present a solo note of trombone. The note is segmented\ninto four parts, which correspond to the attack, the sustain\nand the release. Note that the attack and decay phases ofsome notes are often grouped together into a single seg-\nment. In such cases, the attack period is too short for the\nsegmentation algorithm as it imposes a minimal length to\ninitiate the autoregressive model.\nFigure 1. Segmentation of a trombone note. a) Waveform,\nb) Spectrogram, c) Time. 1) Attack, 2) Sustain, 3 & 4)\nRelease. The vertical lines are the boundaries of the seg-\nments. The ﬁrst boundary correspond to the onset.\nAs they represent a rupture point of the signal, we as-\nsume that onset localizations, containing the tempo infor-\nmation, are included in the list of boundaries time. We\ntherefore focus on positions of the boundaries.\n2.2 Boundary Frequencial analysis\nThe main objective is to ﬁnd a periodicity in the localiza-\ntion of the boundaries that would be the effect of the song’s\nrythmical pattern. In order to ﬁnd the periodicity, a signal\nbw(t)is created. This signal is a weighted Dirac signal,\nwhere each Dirac is positioned at the time of a boundary\ntk.\nThe Diracs are weighted in order to give more inﬂuence\nto the boundaries located at times that are most likely to be\nonsets. Asuming that at onset times, an increase of energy\nis observed, each Dirac is weighted by the difference be-\ntween the energy of the spectrum computed on 20 ms after\nand beforetk( resp.e+\nkande\u0000\nk).\nw(tk) =e+\nk\u0000e\u0000\nk(3)\nWe obtainbw(t)(see an example on Figure 2) :\nbw(t) =NX\nk=1\u000e(t\u0000tk)w(tk) (4)\nwhereNis the count of boundaries, tkis the time of the\nkthboundary.\nWe compute Bw, the Fourier transform of bwto extract\nfrequency information of this signal.\n28\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Figure 2. Representation of a bw(t)\nThe expression of the Fourier transform Bw(f)is :\nBw(f) =Z\nRNX\nk=1\u000e(t\u0000tk)e\u00002i\u0019ftw(tk)dt\n=NX\nk=1e\u00002i\u0019ft kw(tk)(5)\nThis formula offers the advantage of being fast to cal-\nculate.\n2.3 Tempo extraction\n2.3.1 Spectrum analysis\nWe analyse the spectrum Bwon the range of frequencies\n30 - 400 BPM (an example is given on Figure 3). We ﬁnd\nthe positions of the highest peaks as a base for each deci-\nsion.\nWe then extract the positions and energies of the main\npeaks in terms of energy. As it is computed over a long\ntime, the peaks of the spectrum are high and narrow, which\nmakes the localization easier.\nFigure 3. Spectrum j(Bw(f)j2of a whole song.\nThis localization is obtained by detecting the local max-\nima. This algorithm considers a point pand its two direct\nneighbors.pis a local maxima if\njBw(p\u00001)j2<jBw(p)j2\njBw(p+ 1)j2<jBw(p)j2(6)We then choose several of the highest peaks with the\nonly constraint that the distance between two peaks has to\nbe greater than 3 BPM. Only a few peaks are really higher\nthan others in the spectrum, so we choose to select only\nthe four greatest peaks in terms of energy, the position se-\nlected for further peaks would be considered as noise. Let\nP=fp1p2p3p4gbe the list of selected peak positions\nunder the constraint : jBw(pi)j2>jBw(pi+1)j2. We ob-\nserve that every selected peak carries information that can\nbe exploited in order to ﬁnd out the value of the correct\ntempo. We ﬁnally apply a decision algorithm on Pto ﬁnd\nthe tempo.\nTwo strategies are concidered. The ﬁrst one looks for the\ncorrelation between the length of the segments and each\nvaluepin the temporal domain. The second one tries to\nﬁnd the best comb matching the spectrum.\n2.3.2 “Inter-Boundaries-Intervals decision”\nThe ﬁrst approach is in the temporal domain, and uses the\nboundaries of the segmentation. Theses boundaries are ﬁl-\ntered on their weights in order to keep only the boundaries\nwhere a high increase of energy is experienced: we only\nkeep the boundaries with a signiﬁcant weight. This ﬁlter-\ning is computed in order to keep instants which are most\nlikely onset instants. The set Iof intervals between each\ncouple of following boundaries is then computed.\nFor eachpi, we perform the pseudo periods correspond-\ning to 1/4, 1/3, 1/2, 1, 2, and 3 times pi. These pseudo\nperiods have been chosen as they correspond to the period\nof half, quarter, eighth and sixteenth note in duple meter or\ntriple meter.\nThe scoreNum (pi)is the number of intervals in Iwhose\ndurations correspond to one of these pseudo periods.\nThe estimated tempo bpbis given by :\nbpb= argmax\npi;i=1;:::;4(Num (pi)) (7)\n2.3.3 “Comb decision”\nThe second method uses the spectrum and is in frequency\ndomain. This method is based on the ﬁrst peak p1, as we\nassume that it is always signiﬁcant for the tempo detection.\nWe then consider 7 tempi, which are1\n4p1,1\n3p1,1\n2p1,2p1,\n3p1and4p1, as well as p1itself, noted tpi,i= 1;:::;7\n. We only keep, among this list of tempi, those which are\nin the range 30 - 240 BPM, assuming that a value outside\nof these bounds would hardly be considered as the main\ntempo.\nFor each tempo value tpi, we compute the product of the\nspectrum and a Dirac comb with the 10 harmonic teeth cor-\nresponding to the tempo value.\nThe mean amplitude value of the so ﬁltred spectrum\ngives a score Ampl (tpi).\nThe estimated tempo bpcis given by\nbpc= argmax\ntpi;i=1;:::;7(Ampl (tpi)) (8)\n29\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)2.3.4 Combination of the strategies\nIn order to take advantage of both methods, we propose a\ncombined decision algorithm. Using pc1andpc2the two\nbest tempi returned by the “Comb decision” algorithm, we\napply the “Inter-Boundaries-Intervals” strategy to compare\nthe two values Num (pc1)andNum (pc2).\nThe tempo with the best Num is chosen as a ﬁnal deci-\nsion.\n3. EXPERIENCE\n3.1 Corpus\nWe choose to test our method on the part of the RWC\ndatabase [6, 7] that is BPM-annotated. This corpus has\nbeen created in order to provide a benchmark for experi-\nmentation on music information retrieval and is now well\nknown and widely used in this research ﬁeld. It therefore\nseems interesting to use it in order to facilitate comparisons\nbetween our algorithm’s results and others. This corpus is\na compilation of 100 tracks of Japanese Pop songs. Each\nsong lasts from 2 minutes 50 seconds to 6 minutes 07 sec-\nonds.\nAs the method needs no learning, our experiment protocol\nconsists in applying our algorithm on each full track.\n3.2 Experiments\nThe methods are based on the Forward Backward diver-\ngence segmentation: in order to implement this algorithm,\nwe choose to use the parameters deﬁned in [2] for voiced\nspeech signal. No speciﬁc adaptation is performed for mu-\nsic.\nAs previously mentioned, we observe that the highest\npeak of the spectrums has a strong link with the tempi.\nOver the 100 tracks computed, the highest peak position\nis linked with the tempo 98 times: it is located twice on\na position corresponding to the half of the ground-truth\ntempo, 3 times on the correct position, 60 times on the\ndouble tempo and 32 times on a position corresponding to\n4 times the tempo.\nTo assess quantitatively each version of our method, we\nintroduce a conﬁdent interval : the tempo value is con-\nsidered as “Correct” if its difference with the ground-truth\nvalue at strictly less than 4 BPM. The ratios and multiples\nare considered good when their distance to 2, 3, 4, 1/3 or\n1/2 is strictly less than 0.03.\nTwo metrics are computed in order to evaluate the accu-\nracy of each method. The ﬁrst one is the ratio of correctly\nestimated tempi over the whole corpus.\nAccuracy 1=#of correctlyestimatedtempi\nL(9)\nwhere L is the number of evaluated tracks.\nThe second one is more ﬂexible and assumes that the\ntempi corresponding to half, third double and three time\nthe annotated tempo are correct. This metric is computed\ntaking take into account that tempo value is subjective and\ncan vary from one listener to another.Accuracy 2=#of correctormultipletempi\nL(10)\n3.2.1 “Inter-Boundaries-Intervals decision”\nThe ﬁlltring of the boundaries involves a threshold: the se-\nlectionned boundaries have a weight greater than 10% of\nthe maximum weight among the boundaries. The detailled\nresults of the Inter-Boundaries-Intervals decision are visi-\nble in Table 1. The global result are 56 % of Accuracy 1\nand 95% ofAccuracy 2.\nRatios with the correct tempo\n1/2 1 2 4No linkAcc 1Acc 2\n7 56 28 1 5 56 95\nTable 1. “Inter-Boundaries decision Decision” : Number\nof music tracks in function of the ratios between the esti-\nmated tempo and the ground truth value. Accuracy 1and\nAccuracy 2are deducted.\n3.2.2 Comb decision\nIn order to optimize the results of this method and to be\nsure to get the peak value on each hypothesis multiple, the\nreturned value is the maximum of 7 equally spaced tempi\nin a neighborhood of \u00061BPM around each pmultiple\nvalue. Applying this method to our corpus and returning\nthe best two hypothesis, we observe that the ground-truth\ntempo is present for 98 of the tracks. The global result of\nthis method, choosing only the best comb as result, is 64%\nforAccuracy 1and 96% for Accuracy 2. The detailled re-\nsults are visible in Table 2.\nRatios with the correct tempo\n1/2 1 2 3No linkAcc 1Acc 2\n3 64 29 0 4 64 96\nTable 2. “Comb Decision” : Number of music tracks in\nfunction of the ratios between the estimated tempo and the\nground truth value. Accuracy 1andAccuracy 2are de-\nducted.\n3.2.3 Combination of the strategies\nAs shown in Table 3, the combination of the two previous\nmethods largely improves the results. The results in terms\nofAccuracy 1is 78% and 93% in terms of Accuracy 2.\nRatios with the correct tempo\n1/2 1 23 No linkAcc 1Acc 2\n13 78 20 7 78 93\nTable 3. Percentage of the returned values ratio of the\nground truth for the Fusion of the two algorithms\n30\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)The differences between their results is essentially due\nto the detection of the “double tempo”. This type of error\ndissapears. The number of serious errors is stable.\n3.3 Discussion\nThe 2004 MIREX evaluation was the last MIREX session\nwhich the task of tempo estimation was evaluated. These\nresults were obtained on a corpus of 3199 tempo-annotated\nﬁles ranging from 2 to 30 seconds, akd divided into three\nkinds : loops, ballroom music and songs exerpts.\nThe algorithms evaluated during this campaign are de-\ntailed and compared in [8]. The Klapuri’s algorithm [9] ob-\ntained the best score on this evaluation with an Accuracy 1\nof 67.29% and an Accuracy 2of 85.01% among the total\nset of evaluated signals and reaching 91.18% of Accuracy 2\non the song’s subset.\nAn exhaustive search for the best combination of ﬁve\nalgorithms, using a voting mechanism, has also be com-\nputed. The best combination achieved 68% in terms of\nAccuracy 1, whereas the best Accuracy 2reached 86%.\nThe MIREX corpus and the RWC part we use are dif-\nferent (in particular in terms of length). Nevertheless, our\nresults are comparable and experiments will be realized on\nshort extracts of the songs in order to deﬁne the robustness\nof our method.\n4. CONCLUSIONS\nIn this paper, we presented a tempo estimator based on an\nautomatic segmentation of the signal into quasi-stationnary\nzones. The use of this segmentation for the tempo induc-\ntion seems to be rather signiﬁcant: the spectrum of the\nDirac signal derivate from the segmentation shows a pre-\ndominant value directly linked with the tempo on 98% of\nour tests. The three methods which exploit this property\nhave good performence. These methods are still rather\nsimple, so we will investigate some potential improvements:\n\u000fSome experiments will be realized in order to eval-\nuate the sensitiveness of our method to the use of\nshort extract. Good results would allow the use of\nthis method on slipping windows of few dozens of\nsecond. Such treatment could be realized in order to\ndetect changes in the tempo.\n\u000fThe use of the phase of Bw(p)seems promissing for\nthe developpement of a precise onset localizator.\n5. REFERENCES\n[1] M. Alonso, B. David, and G. Richard. Tempo and beat\nestimation of music signals. In Proc. Int. Conf. Music\nInformation Retrieval, pages 158–163, 2004.\n[2] R. Andr ´e-Obrecht. A new statistical approach for the\nautomatic segmentation of continuous speech signals.\nIEEE Trans. on Acoustics, Speech and Signal Process-\ning, 36(1):29–40, 1988.[3] J. Bilmes. Timing is of the essence: Perceptual\nand computational techniques for representing, learn-\ning, and reproducing expressive timing in percus-\nsive rhythm. Master’s thesis, MIT, Cambridge, Mass.,\nUSA, 1993.\n[4] S. Dixon. Automatic extraction of tempo and beat from\nexpressive performances. Journal of New Music Re-\nsearch, 30(1):39–58, 2001.\n[5] S. Dixon, E. Pampalk, and G. Widmer. Classiﬁcation of\ndance music by periodicity patterns. In Proc. Int. Conf.\nMusic Information Retrieval, pages 159–165, 2003.\n[6] M. Goto. Development of the RWC music database.\nInProceedings of the 18 th International Congress on\nAcoustics (ICA 2004), pages 553–556, 2004.\n[7] M. Goto, H. Hashiguchi, T. Nishimura, and R. Oka.\nRWC music database: Popular, classical, and jazz mu-\nsic databases. In Proc. 3rd International Conference on\nMusic Information Retrieval (ISMIR 2002), pages 287–\n288, 2002.\n[8] F. Gouyon, A. Klapuri, S. Dixon, M. Alonso, G. Tzane-\ntakis, C. Uhle, and P. Cano. An experimental com-\nparison of audio tempo induction algorithms. IEEE\nTrans. on Audio, Speech, and Language Processing,\n14(5):1832 – 1844, september 2006.\n[9] A. Klapuri, A. Eronen, and J. Astola. Analysis of the\nmeter of accoustic musical signals. IEEE Trans. on\nAudio, Speech, and Language Processing, 14(1):342–\n355, 2006.\n[10] E. Scheirer. Tempo and beat analysis of acoustic music\nsignals. Journal of the Acoustical Society of America,\n104:588–601, 1998.\n[11] G. Tzanetakis and P. Cook. Musical genre classiﬁca-\ntion of audio signals. IEEE Trans. on Speech and Audio\nProcessing, 10(5):293–302, 2002.\n[12] C. Uhle, J. Rohden, M. Cremer, and J. Herre. Low\ncomplexity musical meter estimation from polyphonic\nmusic. In Proc. AES 25th International Conference,\npages 63–68, June 2004.\n31\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "Recognising Classical Works in Historical Recordings.",
        "author": [
            "Tim Crawford",
            "Matthias Mauch",
            "Christophe Rhodes"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1417429",
        "url": "https://doi.org/10.5281/zenodo.1417429",
        "ee": "https://zenodo.org/records/1417429/files/CrawfordMR10.pdf",
        "abstract": "In collections of recordings of classical music, it is normal to find multiple performances, usually by different artists, of the same pieces of music. While there may be differences in many dimensions of musical similarity, such as timbre, pitch or structural detail, the underlying musical content is essentially and recognizably the same. The degree of divergence is generally less than that found between ‘cover songs’ in the domain of popular music, and much less than in typical performances of jazz standards. MIR methods, based around variants of the chroma representation, can be useful in tasks such as work identification especially where disco/bibliographical metadata is absent or incomplete as well as for access, curation and management of collections. We describe some initial experiments in work-recognition on a test-collection comprising c. 2000 digital transfers of historical recordings, and show that the use of NNLS chroma, a new, musically-informed chroma feature, dramatically improves recognition.",
        "zenodo_id": 1417429,
        "dblp_key": "conf/ismir/CrawfordMR10",
        "keywords": [
            "collections",
            "classical music",
            "multiple performances",
            "different artists",
            "same pieces",
            "musical content",
            "underlying musical content",
            "cover songs",
            "jazz standards",
            "MIR methods"
        ],
        "content": "Recognizing Classical Works in  Historical Recordings  \nTim Crawford  Matthias Mauch  Christophe Rhodes  \nGoldsmiths, University of \nLondon, Centre for Cognition, \nComputation and Culture  \nt.crawford@ gold.ac.uk  Queen Mary, University of \nLondon, Centre  for Digital \nMusic \nmatthias.mauch@elec.qmul.\nac.uk Goldsmiths, University of \nLondon, Department of \nComputing  \nc.rhodes @gold.ac.uk  \nABSTRACT  \nIn collections of recordings of classical music, it is \nnormal to find multiple performances, usually by \ndifferent art ists, of the same pieces of music. While there \nmay be differences in many dimensions of musical \nsimilarity, such as timbre, pitch or structural detail, the \nunderlying musical content is essentially and \nrecognizably the same. The degree of divergence is \ngenerally less than that found between ‘cover songs’ in \nthe domain of popular music, and much less than in  \ntypical performances of jazz standards. MIR methods, \nbased around variants of the chroma  representation, can \nbe useful in tasks such as work identi fication especially \nwhere disco/bibliographical metadata is absent or \nincomplete as well as for access, curation and \nmanagement of collections. We describe some initial \nexperiments in work -recognition on a test -collection \ncomprising c.  2000 digital transfers of historical \nrecordings, and show that the use of NNLS chroma , a \nnew, musically -informed chroma  feature, dramatically \nimproves recognition.  \n1. INTRODUCTION  \nAs was pointed out by Richard Smiraglia in a paper at \nISMIR 2001, “musical works (as opposed to musical \ndocumen ts, such as scores or recordings of musical \nworks) form a key entity for music information retrieval. \n… However, in the [general] information retrieval \ndomain, the work, as opposed to the document, has only \nrecently received focused attention.” [1 ] This l argely \nremains true today; despite a steady advance in content -\nbased MIR techniques, we have hardly begun to realize \nthe potential power of using the m to extract higher -level \nmusical knowledge corresponding to what is embedded in \nbibliographical metadata, hitherto the exclusive domain \nof music -librarianship. In this paper we use the term \n‘work’ simply to refer to the musical composition as \nrepresented by the notes in a musical  score (though we \nacknowledge that the concept is much more complex than  \nthis naïve definition assumes) . The importance of the \nwork  concept  in classical music becomes immediately apparent when one is confronted with the kind of \nconfused or inaccurate metadata that often results from \nthe use of online CD -recognition sys tems which rely on \nthe ID3 tagging scheme [2] used for identifying mp3 \ntracks, which is rarely applied correctly  to classical \nmusic . Further problems arise when a  track becomes \nisolated from its original media (e.g. by digital copying or \n‘ripping’ from a CD) . The situation is even more \nproblematic when works are segmented differently in \ndifferent recorded manifestations: there is, for example, \nno standard way to divide up the continuous music of an \nopera into CD tracks; although there exist musicological \nconventions about the navigation through numbered acts \nand scenes, even these can break down when, for \nexample, it is not clear from the score w hether an \nintroductory recitative forms part of an aria or forms an \nindependent number.  \nIn the controlled environment of the digital music \nlibrary these issues can be  addressed by adopting  \ncataloguing standards such as FRBR [ 3], which deals \ncomprehensivel y with the musical work concept and its \nvarious manifestations  in physical and recorded form . \nThe correct identification of classical works (for example, \non uncatalogued archive tapes) , or fragments from them \n(as frequently encountered on movie or advertis ement \nsound -tracks)  remains a time -consuming task demanding \nconsiderable  expertise. The solution to some of these \nproblems may lie in a system built around content -based \nwork -recognition, operating over the internet on well -\ndocumented ‘authority’ coll ections of recorded works \nwhose metadata can be trusted.  \nFor much of the mainstream classical repertory, \nhowever, the work concept is fairly straightforward . The \ncollection  investigated here  can be claimed to be fairly \nrepresentative of the  taste of  classical -music record \nbuyer s in the years before the Second World War. This  \npaper deals with the particular case of historical \nrecordings of classical music, much of which is still in \nthe mainstream repertory, but in which the integrity of the \nwork may be compromised by the restrictions of the \nrecording process itself.  \nMusic reco rded before about 1960 almost exclusively \nexists in the form of 78 -rpm gramophone recordings. \nMany of these, by famous artists from Caruso to Glenn \nMiller, are available in modern commercial transfers,  \nPermission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fee provided that copies \nare not ma de or distributed for profit or commercial advantage and that \ncopies bear this notice and the full citation on the first page.   \n© 2010  International Society for Music Information Retrieval  \n495\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \noften painstakingly enhanced using a variety of digita l \ntechnologies.2 But there remains a vast heritage of \nrecorded music performed by less well -known artists that \nis unlikely ever to prompt the investment necessary to \nmake commercial release viable. Digitization initiatives \nin a number of countries are incr easingly making these \nrecordings available to scholars investigating the history \nof recording and of musical performance as well as to the \ngeneral public .3 Music information retrieval ( MIR ) \ntechniques offer rich possibilities for the curation and \nmanageme nt of, as well as the access to, such collections. \nProfessional metadata standards used by music librarians \nfor cataloguing mainstream classical music, whether in \nthe form of scores or recordings, universally make use of \nthe work concept, and it is natural  to seek ways to aid \nthem using MIR techniques such as those described in \nthis paper.  \nThe task we employ as a use -case in this paper, \nClassical Work Recognition, is described in Section 2.  \nEarly recordings present special problems and are not \nsuitable for  many MIR methods, which are usually \ndeveloped with modern commercial recordings of \npopular music in mind. We discuss some of these special \nfeatures of early recordings and our approach to them in \nsection 2.1.  \nIn section 3 we discuss the chroma  features we use on \nthe historical recordings, introducing a new feature, the \nNNLS chroma  (Non-Negative Least Squares chroma ), \nwhich proves to offer great advantages for this task. Here \nwe also discuss certain aspects of the search method we \nadopt in us ing the OMRAS2 specialized audio search \nsystem, audioDB.  \nSection 4 gives further details of our test collection \nand some of its special features. We describe the retrieval \nexperiment we carried out on the test , which \ndemonstrate s clearly the advantage of a musically -\ninformed approach, as is the case with NNLS chroma ; this \nis followed in Section 5 by a discussion of the results and \nmentions further work we shall be doing in the near \nfuture.  \n2. CLASSICAL WORK RECOGNITION  \nWe situate the research described here as a step towards \nautomatic metadata enrichment. The long-term aim, \nsimply put, is to develop a system which can help to \nidentify classical works in a collection of digital audio \nwhose descriptive metadata is either incomplete or \ninaccurate; the more modest task reported here is the \nidentification of classical works that appear more than \nonce in a collection of digitized historical recordings. \nSuch duplicates may range from identical repetitions of \nthe same digital file, through multiple digitizations (with \n                                                             \n2 For a detailed overview of the special features of early recordings that \nneed to be borne in mind, see [ 4]. \n3 Useful lists of  URLs for online collections of historical recordings are \n[5, 6]; to these [7, 8, 9] should be added.  or wi thout different parameter settings) of the same 78 -\nrpm disc, different performances by the same or different \nartists, to re -scorings, arrangements, extracts and \nmedleys, examples of all of which occur in our test \ncollection.  \nIn order to evaluate our method ’s performance on this \ntask, we have to establish ‘ground -truth’ in the form of a \nlist of duplicates and ‘covers’ within the test collection. \nIn principle, we should be able to process the \naccompanying machine -readable metadata for this, but, \nfor various r easons, this was not possible, so this has \ninevitably been a largely manual process  (see 4.2, below) . \nSince almost all commercial historical recordings carry \nclearly -printed labels, in general it should not be hard \nnaively to identify the works performed o n a 78 -rpm disc \nor set of discs. However, once the music has been \ndigitized and separated from this graphical information \n(as was the situation for us), the problem becomes \npotentially more complex. In general, for example, we \ncannot identify tracks with w orks in a one -to-one \ncorrespondence, as will be discussed below.  \nFurthermore, classical works often – perhaps usually – \ncomprise more than one movement. In the experiment \nreport ed here we actually treat work -movements  as if \neach was a separate ‘work ’; we make no attempt to \ncategorise different movements as belonging to the same \nwork, an exercise that would presuppose a degree of \nmusical unity which cannot be said to apply universal ly. \nIn a different use -case, matching music between different \nmovements of a work may be of great interest to \nmusicologists, as may close matches between different \nworks, or even works by different composers. Similarly, \nwe ignore multiple matches of musical sequences within \na single track, although this is of central importance for \nmusical structure analysis.  \nIf classical work -recognition could be robustly \nachieved with historical recordings despite their technical \ndrawbacks (discussed below) this would offer a useful \ntool for m etadata enrichment when used online in \nconjunction with a standard reference collection of \nrecordings  with high-quality metadata . \n2.1 Early Recordings  \nSome of the special features of early recordings which \ncan cause problems in audio analysis, and  thus in audio \nMIR, are: limited frequency range, surface noise, \ndistortion, variability of pitch (both global and local) and \nthe problem of side -breaks. We briefly mention some of \nthese in this section, though space precludes a full \ndiscussion here.  \nThe f requency range attainable in 78 -rpm recordings \nranged from 168–2000  Hz in early acoustic recordings to \n100–5000 Hz in electrical recordings from 1925. \nHowever, this is complicated by the various degrees of \nequalization that were applied to compensate for t he fact \nthat mechanical recording systems respond much more \n496\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \nstrongly to low -frequency sounds, leading to various \nkinds of distortion when global gain levels are adjusted to \ncapture the higher frequencies  [9]. In this research, we \ntake on trust the work o f the professional transfer \nengineers who carried out the digitizations.4  \nThe most immediately obvious difference between a \n78-rpm recording and a modern digital one is the amount \nof broadband background noise known as ‘surface noise ’; \nthis has various  causes, usefully summarized in [10]. \nThere is often other noise present, usually due to \nmechanical aspects of the recording process. Not all of \nthis can be completely eliminated by digital techniques, \nespecially when it has a more -or-less def inite ‘pitch’. \nProblems due to broadband noise can mostly be avoided \nby using chroma  features such as NNLS chroma , \ndesigned to ignore the non -harmonic components from \npercussion instruments. Distortion is a common feature in \nearly recordings, lik e noise due to a variety of causes, and \nit is a problem that cannot easily be sidestepped. We have \nobserved that highly -modulated loud passages in certain \nrecordings tend to be distorted and often behave \nanomalously in content -based matching. This will nee d to \nbe the subject of future research.  \nAs Daniel Leech -Wilkinson demonstrates5, the pitch \nof early recordings is by no means reliable; in general we \ncan neither be sure of the global pitch -standards used by \nthe performers (e.g. A=440Hz) nor of the actual \nfrequencies sounding in the studio during recording. We \nmention some strategies for overcoming this problem in \nSection 3, below. The problem of side -breaks is \naddressed in  Section 4.  \n3. FEATURE SELECTION & SEARCH  \nThe classical work -recognition problem is clo se, though \nnot identical, to the well -known MIR ‘cover song’ \nproblem. In fact, in some respects it is somewhat simpler, \nsince cover songs vary from their original model in ways \nthat are generally unpredictable and can occur in several \ndirections simultaneo usly. In general, we can be fairly \nsure that sequences of pitch -based data will be more -or-\nless invariant between recorded instances of the same \nwork. This is more likely to be true where the scoring and \ninstrumentation are the same, and both performers ar e \nworking from the same (or a similar) score; where more \nradical re -arrangement or rearranging of the music has \ntaken place there will be less similarity. For this reason, \nwe match  sequences of  chroma  feature s, rather than \nwhole -track features; unless the latter embody some \nnotion of sequence (as might be the case in an n -gram \nmodel) the number of false positives is likely to be high, \nsince many work -movements in the same key and using \nthe same general harmonic language are likely t o share \nsimilar overall pitch -class content. Furthermore, chroma s \n                                                             \n4 http://www.charm.kcl.ac.uk/history/p20_4_4_1.html  \n5 [4], chapter 3.1, heading ‘Misrepresentations in early recordings ’ are robust to variation in timbre, which allows us to \nmatch radically -differing instrumentations (subject to \nlimits of noise caused by percussive sounds or distortion).  \nIn this pap er we compare the performance of two \nchroma  features in our classical work -recognition task. \nThese are: (a) 36 -bin chroma features extracted using \nfftExtract [ 11] (FE) ; (b) the new NNLS chroma features \ndescribed in the following section  (NNLS) .  \n3.1 NNLS Chroma  \nIn this section we give a brief description of the new 12 -\ndimensional NNLS chroma  feature which has been \ndeveloped for the purpose of chord transcription  [12].  \nBut first we explain our reasons for comparing this with \nthe performance of 36 -d chroma s in the same task, as this \nmay not be immediately obvious.  \nOne commonly -observed feature of early recordin gs is \nthat they were often recorded on machines operating at \ndifferent speed than the standard 78 revolutions per \nminute that was normal. An additional complication here \nis that we cannot always be sure what pitch -standard was \nbeing used by the performers;  a variety of pitch standards \nhave co -existed across the world of music in the past , \nsome flatter, some sharper than today’s accepted standard \nof A=440Hz. While there is little we can do to reconcile \nthese conflicting sources of error, we can all ow some \ntolerance in matching covers recorded at different global \npitch standards by using three (or more) bins per equal -\ntemperament semitone bin in the chroma  feature, rotating \nthe query by plus or minus a single bin at query time, and \nchoosing  the best match from these three queries. We \npresent results using non -rotated  queries  and also rotated \nby ± one semitone below.  \nAlthough  our new NNLS chroma  features have only \n12-dimensions, corresponding to the 12 chromatic pitch \nclasses of conventional music theory, they are derived \nfrom a spectrogram with three bins per semitone, with the \nintention of achieving a similar invariance to s mall pitch \ndeviation; the most important practical difference is that a \nsingle exhaustive search of a collection of 12 -dimensional \nfeatures will inevitably be more efficient than three \nsearches of one of 36 -dimensional features.  \nNNLS chroma  features are ob tained using a prior \nNNLS -based approximate note transcription [ 12, 13]. We \nfirst calculate a log -frequency spectrogram (similar to a \nconstant -Q transform), with a resolution of a three bins \nper semitone. We derive the tuning of the piece in a \nquarte rtone neighbourhood of 440 Hz and adjust the log -\nfrequency spectrogram by linear interpolation such that \nthe centre bin (of the three) of every note corresponds to \nthe fundamental frequency of that note in equal \ntemperament, as is frequently done in chord - and key -\nestimation  [e.g. 14], we adjust the chromagram to \ncompensate for differences in the tuning pitch. First, the \ntuning is estimated from the relative magnitude of the \nthree bin classes. Using this estimate, the log -frequency \n497\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \nspectrogram is updated b y linear interpolation to ensure \nthat the centre bin of every note corresponds to the \nfundamental frequency of that note in equal temperament.  \nThe spectrogram is then updated again to attenuate \nbroadband noise and timbre. This is done using a kind of \nrunni ng standardization combining the removal of the \nbackground spectrum and a form of spectral whitening.  \nWe assume a linear generative model in which every \nframe Y of the log -frequency spectrogram can be \nexpressed approximately as the linear c ombination Y ≈ Ex \nof note profiles in the columns of a dictionary matrix E, \nweighted by the activation vector x. Finding the note \nactivation pattern x that approximates Y best in the least -\nsquares sense subject to x ≧ 0 is called the non -negative \nleast squares problem (NNLS). We choose a semitone -\nspaced note dictionary with exponentially declining \npartials, and use the NNLS algorithm proposed by \nLawson and Hanson [ 15] to solve the problem and obtain \na unique activation vector. This vector is then mapped to \nthe twelve p itch classes C,...,B by summing the values of \nthe corresponding pitches.  \nIn the work reported here, our feature -vectors are all \naveraged into one -second frames; future work will \ninvestigate the effect on retrieval of using finer \ngranularity. Similar ly, we do not consider here the effect \nof low -level DSP parameters such as FFT window -\nlength, using default values in most cases.  \n3.2 Search  \nSearching was carried out using the audioDB software \ndeveloped at Goldsmiths College in the OMRAS2 project  \n[16]. Inde pendent audioDB databases for each feature -set \nwere searched for best  matches  by Euclidean distance \nbetween queries and items in the database specified as \nfeature -vector sequences of a given length.  \n4. TEST COLLECTION & EX PERIMENT  \nThe collect ion of audio files we used is a subset of one \nprovided by the King’s Sound Archive (KSA) which \nrepresented the set of their digitisations completed by \nFebruary 2009. The current KSA is considerably bigger , \nnumbering over 4,500  sides  with highly -reliable \nmetadata , and free download access to most of the \ncollection is available via a metadata -search able web \ninterface .6 \n4.1 King’s Sound Archive (KSA)  \nThe King’s Sound Archive is based on the BBC’s \ndonation of their holdings of duplicate 78 -rpm records in \n2001; KSA now hold s over 150,000 discs including \nclassical and popular music as well as spoken -word and \nsound-effect recordings from c. 19 00 to c. 1960.7 \nOur test collection comprises digitizations (undertaken \nin the CHARM project  [17]) of 2 ,017 78 -rpm sides, \n                                                             \n6 www.charm.kcl.ac.uk/sound/sound_search.html  \n7 www.kcl.ac.uk/schools/humanities/depts/music/res/ksahistory.html  mostly classical but including some jazz, spoken -word \nand sound -effect recordings. This  number was arrived at \nby chance, being the number of sound files that we could \nprocess conveniently and reconcile with the metadata \nprovided by the KSA. We received the sound -files before \nthe detailed discographical data now on the CHARM \nweb-site8 was rea dy; we thus had to rely on the technical \nproduction metadata, which was not primarily concerned \nwith work identification, although it did include \ncatalogue numbers from the disc -labels as well as the 78 -\nrpm matrix numbers. This necessitated a lot of manual  \nmetadata editing.  \n4.2 Relevance judg ements  \nIn the metadata editing process we have identif ied a \n‘cover list’ of around 88 works for which duplicates or \nmultiple performances exist in the test collection. This \nlist forms the basis for relevance ju dgments in our \nexperiments. We are aware that there are often other \n‘relevant’ tracks for a given query, but since our \nexperiments are comparative in nature we do not regard \nthis as a problem; in fact their effect is likely to be \ndetrimental to our precisi on results.  \nThe existence of multi -side recordings of work -\nmovements in the collection complicates the issue of \nestablishing reliable relevance judgments. (The various \npossibilities for the disposition of work -movements and \nside-breaks is shown diagramati cally in Figure 1.) While \nit is often the case that the same musical material is \nrepeated or alluded to throughout a single movement of a \nclassical work, we cannot be sure that such repetitions are \ndistributed evenly so that each 78 -rpm side over which a \nmovement is spread contains a roughly -equal proportion \nof similar musical material. Furthermore, side -breaks do \nnot always occur at the same point in the music.  \nThere are two basic approaches that can be taken to \nsolve this problem, both of which present so me difficulty: \npost-processing of results or pre -manipulation of the data. \nGiven a list of tracks in the database (each of which \ncorresponds to a 78 -rpm side), we can post -process the \nsearch results so as to regard as mutually -relevant all \nmatches between a query and tracks that come from \nanywhere in the same movement. Alternatively, we can \nin a preprocessing stage digitally concatenate tracks that \nwe suspect are from the same movement. Clearly the \nlatter procedure does not fairly represent the case where \nwe cannot rely on our metadata, and the exact \ncorrespondence between sides and movements is unclear. \nIn our experiment we adjusted the lists to consider as \nrelevant only sections from similar sections of a work -\nmovement (so ‘side 1’ of a given recording of a work -\nmovement, say, is not considered relevant to ‘side 2’ of \nanother recording of the same movement); while we \nacknowledge the limitations of this approach it does not \naffect our comparative evaluation.  \n                                                             \n8 http://www.charm.kcl.ac.uk /discography/disco.html  \n498\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \n \nFigure 1. Disposi tion of work -movements on the sides \nof 78 -rpm gramophone records . \nOne problem that is not solved either way is that many \nof our database tracks contain material from more than \none movement (as in Figure 1, cases 4 and 5); \nfurthermore, because all performan ces are not necessarily \nat the same tempo, or do not observe the same repeats, or \nare recorded on 78 -rpm discs of different size (and \nconsequent time -duration) the same pattern of side -breaks \nis generally not duplicated. This is one good reason why \nwe use sequence -based matching, rather than using \nwhole -track features in which material extracted from the \nwhole of a track – even if some of it comes from a \ndifferent movement in a different key – are consolidated \ninto a single feature -vector.  \n4.3 Experiment  \n4.3.1 Method  \nWe extracted one -second features as described in Section \n3; we built an audioDB database with each set of features \ncorresponding to the 2,017 tracks; with each pre -\ndiscovered ‘cover’ track in turn as query9, we searched \nthe database for best  query/track matches of various -\nlength sequences of feature vectors. We found that a \n                                                             \n9 The query track will, of course, be in the database at search -time; since \nthe identity -match is always returned at the top of the ranked result -list \nwe adjust the result -lists accordingly for our evaluation.  sequence of 25 vectors consistently gave the best retrieval \nperformance for this task across all tested features . We \nrepeated the search with the queries rotated by up to a \nsemitone flat and sharp ( ± 1 bin for NNLS; ± 1 and 2 bins \nfor FE ) taking the best result for each search.  \n4.3.2 Results  \nThe 11 -point recall/precision graph in Fig. 2 and the \naverage precision values in Table 1 show a dramatic \nimprovement (20%) in performance in this particular task \nbrought about by the use of the NNLS chroma  feature as \nopposed to the ‘standard’ chroma  we used. While query -\nrotation for both features significantly improved retrieval \nperformance, NNLS still did far better than  FE. Bearing \nin mind the generally poor acoustic quality of the \nrecordings in the collection, this is particularly \nencouraging and suggests that the new fe ature will be \ngenerally useful for classical work -recognition tasks on \ncollections of higher recording quality, though as yet this \nremains to be tested.  \nFigure 2. 11-point interpolated Recall/Precision graph \nfor the classica l work -recognition task . \n NNLS \nchroma  NNLS \nrotated  FE \nchroma  FE \nrotated  \nAverage precision \nover all rel docs  0.80 0.83 0.57 0. \n54 \nTable 1. Average precision (non -interpolated) for non -\nrotated and rotated queries  over all  322 relevant tracks.  \n4.3.3 Discussion  \nThe improvement in the retrieval performance of the \nsystem with the NNLS chroma  feature (and no other \nchanges) is striking; particularly  since the feature was not \ndesigned with search or  similarity judgments in mind. \n The m odel underlying it (described in  section 3.1) does \nattempt to capture similar note content (in a way  that \ngeneric chroma features attempt to capture similar \nacoustic pitch  content) but there is potential to perform \neven better than our  current results by t uning the NNLS \nfeatures to better reflect  perceptual musical similarity.  \n499\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \nThe fact that a query -sequence length of 25 \nseconds/vectors gave the best retrieval results with all \nfeatures may be useful in distinguishing complete \n‘covers’ of the kind we are deal ing with here from works \nby classical composers which contain references to, or \nquotations of, other music. In general these are most \nlikely to be short. However, this interesting topic needs to \nbe the subject of further investigation.  \n5. CONCLUSIONS AND FUTU RE WORK  \nWe have demonstrated that using a chroma  feature based \non prior NNLS approximate transcription gives a 20% \nimprovement in retrieval performance over conventional \nchroma  for the work -recognition task that is the main \nfocus of this paper.  S ince this copes with historical \nrecordings of varying quality  and a number of the special \nfeatures of the collection (such as the a rbitrary \ndistribution of movements across 78 -rpm sides), we are \nencouraged to hope that it will prove a particularly \neffecti ve feature for general music al work  recognition in \nother MIR contexts.  \nAmongst other work, then, we plan to characterize the \ndetails of the NNLS chroma  feature in  order to be able to \nalign it better to human judgments of note -\ncontent  musical similarity, as  well as designing other \naudio features  which reflect other aspects of musical \nsound such as timbre or rhythm.  \n6. ACKNOWLEDGMENTS  \nThis work was supported by EPSRC grant \nEP/E02274X/1 and the NEMA (Networked Environment \nfor Music Analysis) project funded by the  Andrew S. \nMellon Foundation . We are most grateful to Daniel \nLeech -Wilkinson, Andrew Hallifax and Martin Haskell \nfor providing data from the King’s Sound Archive. \nThanks also to Ben Fields for help in various ways.  \n7. REFERENCES  \n[1] R. P. Smiraglia: “Musical work s as Information \nRetrieval Entities: Epistemological Perspectives,” \nProceedings of the 2nd Annual Symposium on Music \nInformation Retrieval (ISMIR 2001, Bloomington, \nIndiana, U.S.A.), 85 -91 \n[2] Anon, “ID3”, Wikipedia , website: \nhttp://en.wikipedia.org/wiki/ID3  \n[3] Anon, “ Functional Requirements for Bibliographic \nRecords ”, Wikipedia , website: \nhttp://en.wikipedia.org/wiki/Functional\n_Requirements_for_Bibliographic_Records  \n[4] D. Leech -Wilkinson: The Changing Sound of Music: \nApproaches to Studying Recorded Musical \nPerformanc e, online publication, London, 2009:  \nhttp://www.charm.kcl.ac.uk/studies/chap\nters/intro.html  [5] Thomas Edison National Historical Park, Links to \nOnline Recordings, web -site: \nhttp://www.nps.gov/edis/photosmultimedi\na/links-to-online-recordings.htm  \n[6] Anon , ‘Histor ic Sound Recordings Around the Web’ , \nweb-site: \nhttp://www.phonozoic.net/listening.html  \n[7] DISMARC web -site:   \nhttp://www.dismarc.eu/  \n[8] British Library, ‘ Archival Sound Recordings’, web -\nsite:  \nhttp://sounds.bl.uk/  \n[9] Anon, “Gramophone record”, Wikipedia , website: \nhttp://en.wikipedia.org/wiki/ Gramophone\n_record \n[10] R. Wilmut, “Reproduction of 78rpm records”, \nwebsite: \nhttp://home.clara.net/rfwilmut/repro78/\nrepro.html#s/n  \n[11] fftExtract website: \nhttp://omras2.doc.gold.ac.uk/software/f\nftextract/  \n[12] M. Mauch and S. Dixon: “Approxima te Note \nTranscription for the Improved Identification of Rare \nChords,” Proceedings of the 11th International \nSociety for Music Information Retrieval Conference \n(ISMIR 2010, Utrecht, The Netherlands)  \n[13] M. Mauch: “Automatic Chord Transcription from \nAudio Using  Computational Models of Musical \nContext”, unpublished PhD dissertation (Queen \nMary University of London, 2010)  \n[14] C. Harte and M.  Sandler: “Automatic Chord \nIdentification using a Quantised Chromagram,” \nProceedings of 118th Convention of the Audio \nEngineering  Society, 2005  \n[15] C. L. Lawson and R. J. Hanson: Solving Least \nSquares Problems , Prentice -Hall, Englewood Cliffs, \nNJ, 1974  \n[16] audioD B website: \nhttp://omras2.doc.gold.ac.uk/software/a\nudiodb/ \n[17] AHRC Cen tre for the History  and Analysis of \nRecorded Music (CHARM ); website:  \nhttp://www.charm.kcl.ac.uk / \n \n500\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "Music21: A Toolkit for Computer-Aided Musicology and Symbolic Music Data.",
        "author": [
            "Michael Scott Cuthbert",
            "Christopher Ariza"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1416114",
        "url": "https://doi.org/10.5281/zenodo.1416114",
        "ee": "https://zenodo.org/records/1416114/files/CuthbertA10.pdf",
        "abstract": "Music21 is an object-oriented toolkit for analyzing, searching, and transforming music in symbolic (score- based) forms. The modular approach of the project allows musicians and researchers to write simple scripts rapidly and reuse them in other projects. The toolkit aims to pro- vide powerful software tools integrated with sophisticated musical knowledge to both musicians with little pro- gramming experience (especially musicologists) and to programmers with only modest music theory skills. This paper introduces the music21 system, demon- strating how to use it and the types of problems it is well- suited toward advancing. We include numerous examples of its power and flexibility, including demonstrations of graphing data and generating annotated musical scores.",
        "zenodo_id": 1416114,
        "dblp_key": "conf/ismir/CuthbertA10",
        "keywords": [
            "Music21",
            "object-oriented toolkit",
            "analyzing",
            "searching",
            "transforming music",
            "symbolic forms",
            "modular approach",
            "musicians",
            "researchers",
            "simple scripts"
        ],
        "content": "music21: A Toolkit for Computer-Aided Musicology and  \nSymbolic Music Data \nMichael Scott Cuthbert Christopher Ariza \nMusic and Theater Arts \nMassachusetts Institute of Technology \n{cuthbert, ariza}@mit.edu\nABSTRACT\nMusic21  is an object-oriented toolkit for analyzing, \nsearching, and transforming music in symbolic (score-\nbased) forms. The modular approach of the project allows \nmusicians and rese archers to write simple scripts rapidly \nand reuse them in other projects. The toolkit aims to pro-\nvide powerful software tools integrated with sophisticated \nmusical knowledge to both musicians with little pro-\ngramming experience (especially musicologists) and to \nprogrammers with only modest music theory skills. \nThis paper introduces the music21 system, demon-\nstrating how to use it and the types of problems it is well-\nsuited toward advancing. We include numerous examples \nof its power and flexibility, including demonstrations of \ngraphing data and generating annotated musical scores. \n1. INTRODUCTION: WHY MUSIC21?\nComputers have transformed so many aspects of musi-\ncology—from writing and editing papers, to studying \nmanuscripts with digital files, to creating databases of \ncomposers’ letters, to typesetting editions—that it is in-\ncredible that most analytical tasks that music historians perform remain largely untouched by technology. The \nstudy of the rich troves of musical data in scores, \nsketches, intabulations, lead-sheets, and other sources of symbolic music data is still done almost exclusively by \nhand. Even when databases and spreadsheets are em-\nployed, they are usually created for a single purpose. Such specialized approaches cannot easily be reused. \nComputer scientists often assume that, compared to \nworking with scanned images of scores or sound files, manipulating symbolic data should be a cinch. Most of \nthe information from a score can easily be converted to \ntext-based or numeric formats that general-purpose statis-\ntical or information-retrieval tools can manipulate. In \npractice the complexities of music notation and theory \nresult in these tools rarely being sufficient. \nFor instance, a researcher might want to compare \nhow closely the music of two composers adheres to a par-ticular scale (say, the major scale). What begins as a \nstraightforward statistical problem requiring little musical knowledge—simply encode which notes are in the scale \nof the piece’s key and which are not—can quickly grow \nbeyond the capabilities of general statistics packages. \nSuppose that after some initial work, our researcher de-\ncides that notes on stronger beats should be weighed \nmore heavily than those on weaker beats. Now she must \neither add the information about beats by hand to each \nnote or write a new algorithm that labels the beats. Beat labeling is another task that initially seems easy but rapid-\nly becomes extremely troublesome for several reasons.  \nAre grace-notes accented or unaccented? Only a musical-\nly-trained ear that also knows the norms of an era can tell. Incompletely-filled measures, such as pickup measures \nand mid-bar repeats, present problems for algorithms. As \nthe researcher’s corpus expands, the time spent on meta-\nresearch expands with it. What began as a straightforward \nproject becomes a set of tedious separate labors: trans-\nforming data from multiple formats into one, moving \ntransposing instruments into sounding pitch, editorial ac-\ncidentals in early music, or finding ways of visualizing \ntroublesome moments for debugging. \nResearchers in other fields can call upon general-\npurpose toolkits to deal with time-consuming yet largely \nsolved problems. For instance, a scientist working with a \nlarge body of text has easy access to open-source libraries \nfor removing punctuation, converting among text-\nencoding formats, correcting spelling, identifying parts of \nspeech, sentence diagramming, automatic translation, and \nof course rendering text in a variety of media. Libraries and programs to help with the musical equivalents of \neach of these tasks do exist, but few exchange data with \neach other in standardized formats. Even fewer are de-signed in modern, high-level programming languages. As \na result of these difficulties, computational solutions to \nmusicological problems are rarely employed even when \nthey would save time, expand the scope of projects, or \nquickly find important exceptions to overly broad pro-\nnouncements. \nThe \nmusic21  project (http://web.mit.edu/music21) \nexpands the audience for computational musicology by \ncreating a new toolkit built from the ground up with intui-\ntive simplicity and object-oriented design throughout. \n(The “21” in the title comes from the designation for \nMIT’s classes in Music, Course 21M.) The advantages of \nobject-oriented design have led to its wide adoption in \nmany realms of software engineering. These design prin-ciples have been employed in music synthesis and gener-ation systems over the past 25 years [2, 9, 10] but have \nnot been thoroughly integrated into packages for the \nanalysis of music as symbolic data. Humdrum, the most \nwidely adopted software package [6], its contemporary \nports [7, 11], and publications using these packages show \nthe great promise of computational approaches to music \ntheory and musicology. Yet Humdrum can be difficult to \nuse: both programmers and non-programmers alike may \nfind its reliance on a chain of shell-scripts, rather than ob-\nject-oriented libraries, limiting and not intuitive.  \nNicholas Cook has called upon programmers to \ncreate for musicologists “a modular approach involving \n637\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)an unlimited number of individual software tools” [3]. A \nframework built with intuitive, reusable, and expandable objects satisfies Cook’s call without sacrificing power for \nmore complex tasks. \nAs a new, open-source, cross-platform toolkit written \nin Python, \nmusic21  provides such a modular approach, \nmelding object-oriented music representation and analy-\nsis with a concise and simple programming interface. \nSimplicity of use, ease of expansion, and access to exist-\ning data are critical to the design of music21 . The toolkit \nimports Humdrum/Kern, MusicXML [4], and user-\ndefined formats (with MIDI and MuseData forthcoming). \nBecause it is written in Python, music21  can tap into \nmany other libraries, integrating internet resources (such \nas geomapping with Google Maps), visualization soft-\nware, and sophisticated database searches with musical \nanalysis. \nThis brief paper gives an overview of the music21\ntoolkit. Through examples of musicological applications \nthat the system enables, the main distinguishing features \nare illustrated: simplicity of use and expansion. \n2. SCRIPTING AND OBJECTS \nMusic21  is built in Python, a well-established program-\nming language packaged with Macintosh and Unix com-\nputers and freely downloadable for Windows users.  The \ntoolkit adds a set of related libraries, providing sophisti-\ncated musical knowledge to Python. As shown in Figure \n1, after adding the system with “ from music21    \nimport * ”, straightforward tasks such as displaying or \nplaying a short melody, getting a twelve-tone matrix, or \nconverting from Humdrum’s Kern format to MusicXML \ncan each be accomplished with a single line of code. \nDisplay a simple melody in musical notation:\n  tinyNotation.TinyNotationStream( \n        \"c4 d8 f g16 a g f#\", \"3/4\").show() \nPrint the twelve-tone matrix for a tone row (in this case the \nopening of Schoenberg’s Fourth String Quartet): \nprint(serial.rowToMatrix( \n            [2,1,9,10,5,3,4,0,8,7,6,11]) )  \nor since most of the 2nd-Viennese school rows are already available as objects, you could instead type:    \nprint(serial.RowSchoenbergOp37().matrix() ) \nConvert a file from Humdrum’s **kern data format to Mu-sicXML for editing in Finale or Sibelius: \n  parse('/users/documents/composition.krn'). \n      write('xml') \nFigure 1. Three simple examples of one-line mu-\nsic21  scripts. \nThough single-line tasks are simpler to accomplish in \nmusic21  than in existing packages, the full power of the \nnew toolkit comes from bringing together and extending high-level objects. The framework includes many objects, \nincluding Pitches, Chords, Durations, TimeSignatures, Intervals, Instruments, and standard Ornaments. Through \nmethod calls, objects can perform their own analyses and \ntransformations. For instance, Chords can find their own roots, create closed-position versions of themselves, \ncompute their Forte prime forms, and so on. Researchers \ncan extend objects for their own needs, such as altering the pitch of open Violin strings to study scordatura,  spe-\ncializing (subclassing) the Note class into MensuralNote \nfor studying Renaissance Music, or grouping Measures \ninto Hypermeters. The object-oriented design of \nmu-\nsic21  simplifies writing these extensions. \n3. STREAMS: POWERFUL, NESTABLE, \nCONTAINERS OF TIMED ELEMENTS \nAt the core of music21  is a novel grouping of musical \ninformation into Streams: nestable containers that allow \nresearchers to quickly find simultaneous events, follow a \nvoice, or represent instruments playing in different tempi \nand meters. Elements within Streams are accessed with \nmethods such as getElementById() , an approach simi-\nlarly to the Document Object Model (DOM) of retrieving \nelements from within XML and HTML documents. Like \nnearly every music21  object, Streams can immediately \nbe visually displayed in Lilypond or with programs that \nimport MusicXML (such as Finale and Sibelius).  \nThrough the Stream model, a program can find notes or chords satisfying criteria that change from section to sec-\ntion of a piece, such as all notes that are the seventh-\ndegree of the current key (as identified either manually or \nwith an included key-detection algorithm) and then re-\ntrieve information such as the last-defined clef, dynamic, \nor metrical accent level at that point. \n Many tools to visualize, process, and annotate \nStreams come with the \nmusic21  framework. These tools \ninclude graphing modules, analytical tools, and conveni-\nence routines for metrical analysis [8], phrase extraction, \nand identification of non-harmonic tones. Figure 2 de-\nmonstrates the use of metrical analysis, derived from \nnested hierarchical subdivisions of the time signature [1], \nto annotate two Bach chorales in different meters. \nfrom music21.analysis import metrical \n# load a Bach Chorale from the music21 corpus of supplied pieces \nbwv30_6 = corpus.parseWork('bach/bwv30.6.xml') \n# get just the bass part using DOM-like method calls\nbass = bwv30_6.getElementById('Bass') \n# get measures 1 through 10\nexcerpt = bass.getMeasureRange(1,10) \n# apply a Lerdahl/Jackendoff-style metrical analysis to the piece.\nmetrical.labelBeatDepth(excerpt) \n# display measure 0 (pickup) to measure 6 in the default viewer  \n# (here Finale Reader 2009)\nexcerpt.show() \n638\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)#\nb\na\ne\nm\ne\nF\nla\nS\ntw\nw\nm\nta\np\na\nlo\na\nw# perform the sam e\nbwv11_6 = cor\nalto = bwv11 _\nexcerpt = al t\nmetrical.lab e\nexcerpt.show (\nFigure 2.  Ana\nabeler, are in c\nStreams (inclu d\nwo Bach chor a\nwith dots corre s\n4\nIn additio n\nmodern progr a\nakes advanta g\nproaches to so f\nand document a\nongevity of th e\nas the ability o\nwork of contri b\ne process on a dif fe\nrpus.parseWo r\n_6.getElemen t\nto.getMeasur e\nelBeatDepth( e\n()\nalytical tools, s\ncluded with mu\nding Scores a n\nales, each in a\nsponding to th e\n4. FURTHE R\nn to providing \namming langu\nge of some o f\nftware distribu t\nation. These a\ne system acro s\nof the system t\nbutors. \nferent chorale in 3 /\nrk('bach/bwv 1\ntById('Alto' )\neRange(13,20 )\nexcerpt) \nsuch as this m\nusic21  and w\nnd Parts). He r\na different met e\neir metric stre n\nR FEATURES \nsophisticated \nage, the musi\nfthe best con t\ntion, licensing ,\napproaches as\nss multiple pla\nto grow and i n\n/4 time\n11.6.xml') \n)\n)\nmetrical accen t\nork with mos t\nre, excerpts o f\ner, are labele d\nngths. \nresources in a\nic21 packag e\ntemporary ap -\n, development ,\nsure both th e\ntforms as wel l\nncorporate th e\nt\nt\nf\nd\na\ne\n-\n,\ne\nl\ne4.1 An I\nResearc\nMusic\ntion of fr\nincludin g\nnumero u\nRenaiss a\ncorpus p\nURL bo\nries, av a\nwhen fi r\nsearcher and Mu\ns\nonly grodepth o\nf\n4.2 Per m\nMus\ntogether \ntoolkit i s\nsoftwar e\nis relea s\n(LGPL),\nsoftwar e\nsynchro n\ndexed, a\nclasses, \ntest r\nweb.mit .\ncumenta t\nrequests ,\nBetter t h\nspecific \nexample\nutility.  \n5.1 Fin d\nThe s c\npart of a\nop. 133, \nnant sev e\nthe cho r\nand the across b\na\nop133 = \n       \nviolin2 \n#an emp t\ndisplay \nfor thi\n# get a l\n     # and r e\n  notes \n  skipU\n  skipR\n  pitchIntegrated an\nhers\nc21 comes wit\nfreely-distribut a\ng a complete \nus Beethoven \nance polypho n\npackage even \nokmarks to m\nailable online,\nrst requested a\nfor future us e\nsicXML files.\nw the tools fo r\nfthe corpus of w\nmissive Licen s\nic21  is a too l\nin a wide ra n\ns only achieve d\ne components i\nsed under th e\n allowing its u\ne. So that impl e\nnized, the too l\nand searchable\nautomatically \nroutines. T h\n.edu/music21)\ntion and rele a\n, and bug repo r\n5.\nhan an explan a\nexamples illu s\ns are chosen \nding Chords w\ncript in Figure\na MusicXML s\nto find meas u\nenth chords in \nrd in closed p\nForte prime \narlines would a\ncorpus.pars\n      'beet= op133.get\nty container for la\n= stream.St\nsMeasure in\nlist of consecutive \nests (and putting n\n= thisMeasu\nnisons = Tru\nests = True,\nes = stream.d Virtual Co r\nth a corpus pa c\nable music for \ncollection of \nString Quarte\nny. The virtua l\nfurther. Simil a\nmusic resource s\ncan be auto m\nand then mad e\ne. The corpus\nFuture syste m\nr analysis, but \nworks. \nse and Full D o\nlkit: a collecti o\nnge of contex t\nd if users can \nin their own w\ne Lesser Gen\nse within both \nementation an d\nlkit also feat u\ndocumentatio n\ncreated from \nhemusic2\nhosts up-to- d\nase links. Co d\nrts are housed \nEXAMPLE S\nation of high- l\nstrate the tool k\nfor both their \nwithin Melodi c\n3 searches th e\nscore of Beet h\nures that melo d\nconsecutive n\nposition, the s u\nform. (Runn i\nadd just a few l\neWork( hoven/opus13ElementById(\nter display \nream()  \nviolin2.meas\nnotes, skipping u\nn\nnothing in their pl a\nre.findConse\ne, skipOcta v\nnoNone = Tr\nStream(notesrpus of Music \nckage, a large c\nanalysis and t\nthe Bach C h\nts, and exam p\nl corpus exte n\nar to a collec t\ns, additional r e\nmatically dow n\ne available to t\n includes both\nm expansions w\nalso the brea d\nocumentation\non of tools th a\nts. The promi s\nexpand and in t\nwork. Thus, mu\neral Public L\nfree and com m\nd documentati o\nures high-qual i\nn of all modu l\nthe source co d\n21 site \ndate informati o\nde browsing, f\nat Google Co d\nS\nlevel features, \nkit’s promise.\nnovel and p r\nc Fragments\ne entire secon d\nhoven’s Groß e\ndically express \nnotes. It then d i\nurrounding m e\ning the same \nlines of code).\n3.xml')  \n'2nd Violin'\nures: \nnisons, octaves, \naces)\ncutiveNotes(\nves = True,\nrue)\n).pitches for \ncollec-\ntesting, \nhorales, \nples of \nnds the \ntion of \neperto-\nnloaded \nthere-\nh Kern \nwill not \ndth and \nat work \nse of a \ntegrate \nsic21\nLicense \nmercial \non stay \nity, in-\nles and \nde and \n(http:// \non, do-\nfeature \nde.\na few \nThese \nractical \nd violin \ne Fuge ,\ndomi-\nisplays \neasure, \nquery \n)\n639\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)d\nF\nm\n5\nto\nc\nn\nu\n(\nc\nty\nlo\ns\nT\nin\ntw\ng\nti\nf\nf\nf\n#\nm\nn\ng\ngfor i in ra\n   #makes ev e\n   testChor d\n   testChor d\n   if testCh\n     # A domi\n      \n            #  We lab e\n            #  and the f\n     testCh o\n            \n     primeF o\n         th i\n     firstN o\n     firstN o\n            # Then w e\n            #  by the m\n     chordM e\n     chordM e\n        tes t\n     displa y\n     displa y\ndisplay.show (\nFigure 3.  The r\nmelodically. \n5.2 Distributi o\nFigure 4 d e\no help visuali z\ncern. These gr a\nnotes, and ho w\nused. From tw o\na minuet, in r e\ncan be seen th a\nype of bell-c u\now notes, and \nstral space. C h\nThe difference\nnquiry is wor t\nween duration \ngraphing meth o\nion tool for t h\nformats.  \nfrom music21 .\nfrom music21 .\n#display 3D gra p\nmozartStream    xml.moza\nr\nnotes = moza r\ng = graph.Pl o\n   notes, c o\ng.process() \nange(len(pit c\nery set of 4 notes i n\nd = chord.Ch o\nd.duration.t y\nhord.isDomin a\nnant-seventh cho r\nel the chord with t h\nfirst note of the m\nord.lyric = \"\n   thisMeas u\norm = chord. C\nisMeasure.pi t\note = thisMe a\note.lyric = p\ne append the cho r\nmeasure containin g\neasure = str e\neasure.appen d\ntChord.close d\ny.append(cho r\ny.append(thi s\n()\nresults of a se a\nons of Notes b\nemonstrates t h\nze trends that a\naphs plot thre e\nw frequently th e\no small excerp t\ned) and by Ch\nat pitches in t h\nurve distributio\nmany notes t o\nhopin’s usage j\ns in pitch us a\nth pursuing fu r\nand pitch app\nods help resea r\nheir data, easi l\n.musicxml imp\n.humdrum impo\nphs of count, pitch ,\n= music21.p a\nrtTrioK581Ex c\nrtStream.fla t\not3DBarsPitc h\nolors=['r']) \nches) - 3): \nnto a whole-note c\nord(pitches[ i\nype = \"whole \"\nantSeventh() :\nrd was found in th i\nhe measure numb e\nmeasure with the F o\n\"m. \" + str( \nure.measureN u\nChord( \ntches).prime F\nasure.notes[ 0\nprimeForm    \nrd in closed positi o\ng the chord. \neam.Measure( )\nd(\ndPosition()) \nrdMeasure) \nsMeasure) \narch for chord s\nby Pitch and D\nhe ability of mu\nare otherwise d\ne features: pit c\nese pitches an d\nts of pieces in \nopin (a mazur k\nhe Mozart ex a\nn, with few h i\noward the mid d\njumps throug h\nage suggest th\nrther, but no c\nears. Music21\nrchers find the \nly switching a\nport testFil e\nort testFile s\n, and duration\narse( \ncerpt) \nt.stripTies( )\nhSpaceQuarte r\nchord\ni:i+4])     \n\"\n:\nis measure. \ner\norte Prime form\number) \nFormString \n0]\n     \non followed \n)\ns expressed \nDuration \nusic21  graph s\ndifficult to dis -\nch, duration o f\nd durations ar e\n3/4 by Mozar t\nka, in blue), i t\nample follow a\nigh notes, fe w\ndle of the regi-\nhout the piano.\nat this line o f\nconnection be -\n1’s easy-to-us e\nbest visualiza -\namong divers e\nes as xml \ns as kern \n)\nrLength( \n      \ns\n-\nf\ne\nt\nt\na\nw\n-\nf\n-\ne\n-\ne# perform \nchopinS\nnotes = g = gra\np\n    notg.proce\nFigure 4\nMozart a\nThe\ndistincti v\ntween p i\ntreme e\nd’intens i\nfirst wo r\ntween p i\n(isolate d\nthe same process o\ntream = musi\nchopinStrea\nph.Plot3DBar\nes, colors=[ss() \n4. Difference s\nand Chopin. \nMozart and \nve individual \nitch and durat i\nexample is M\nités” from Qua\nrk of total se r\nitch and dura t\nd for clarity), i s\non a different wor k\nc21.parse(kem.flat.stripsPitchSpace\nQ\n'b']) \ns in pitch dis t\nChopin exam p\nusage, show \nion. Many oth\nMessiaen’s “ M\natre études de \nrialism. A pe r\ntion, as found \ns plotted in Fi\nk\nrn.mazurka6)Ties() uarterLengt\nh\ntribution betw e\nples, while s h\nlittle correlati o\ner pieces do. A\nMode de vale\nrythme , perh a\nrfect correlati o\nin the middl e\ngure 5. An as p\nh(\neen\nhowing \non be-\nAn ex-\nurs et \naps the \non be-\ne voice \npect of \n640\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)the composition that is difficult to observe in the score \nbut easy to see in this graph is the cubic shape (- x3) made \nthrough the choice of pitches and rhythms. This shape is \nnot at all explained by the serial method of the piece.  Al-\nso easily seen is that, although Messiaen uses lower notes less often, there is not a perfect correlation between pitch \nand frequency of use (e.g., 21 B-flats vs. 22 A-flats). \nmessiaen = converter.parse( \n               'd:/valeurs_part2.xml') notes = messiaen.flat.stripTies() g = graph.PlotScatterWeightedPitch\\     SpaceQuarterLength(notes, xLog = False,     title='Messiaen, Mode de Valeurs, \n    middle voice') g.process() \nFigure 5.  A graph of pitch to duration relationships in \nMessiaen, “Mode de valeurs,” showing the correlation \nbetween the two note attributes. \n5.3 Pitch Class Density Over Time \nIn Figure 6, pitch class usage, over the duration of \nthe composition in the cello part of a MusicXML score of \nBeethoven’s Große Fuge , is graphed. Even though the \ntemporal resolution of this graph is coarse, it is clear that \nthe part gets less chromatic towards the end of the work.  \n(We have manually highlighted the tonic and dominant in \nthis example.) \nbeethovenScore = corpus.parseWork('opus133.xml') \ncelloPart = \\ \n      beethovenScore.getElementById('Cello') \n# given a “flat” view of the stream, with nested information  \n# removed and all information at the same hierarchical level, \n# combine tied notes into single notes with summed durations \nnotes = celloPart.flat.stripTies() \ng = graph.PlotScatterPitchClassOffset(notes, \n    title='Beethoven Opus 133, Cello') g.process() Figure 6. A graph of pitch class usage over time in \nBeethoven’s Große Fuge .\n5.4 Testing Nicolaus de Capua’s Regulae ofMusica \nFicta\nThis example shows a sophisticated, musicological \napplication of music21 . Among his other writings, the \nearly-fifteenth century music theorist Nicolaus of Capua \ngave a set of regulae , or rules, for creating musica ficta\n[5]. Musica ficta , simply put, was a way of avoiding tri-\ntones and other undesirable intervals and create more conclusive cadences through the use of unwritten acci-\ndentals that performers would know to sing. Unlike the rules of most other theorists of his time, Nicolaus’s four \nrules rely solely on the melodic intervals of one voice. \nHerlinger’s study of Nicolaus’s rules suggested that they could be extremely successful at eliminating harmonic \nproblems while at the same time being easy enough for \nany musician to master. However, as is conventional in musicology, this study was performed by hand on a few \nexcerpts of music by a single composer, Antonio Zachara \nda Teramo. Using \nmusic21  we have been able to auto-\nmatically apply Nicolaus’s rules to a much wider set of \nencoded music, the complete incipits and cadences of all \nTrecento ballate (about 10,000 measures worth of music) \nand then automatically evaluate the quality of harmonic \nchanges implied by these rules. Figure 7 shows an ex-\ncerpt of the code for a single rule, that a descending ma-\njor second (“M-2”) immediately followed by an ascend-\ning major second (“M2”) should be transformed into two \nhalf-steps by raising the middle note: \n#n1, n2, and n3 are three consecutive notes \n#i1 is the interval between n1 and n2\n#i2 is the interval between n2 and n3\ni1 = generateInterval(n1,n2) \ni2 = generateInterval(n2,n3) \n#we test if the two intervals are the ones fitting the rule\nif i1.directedName == \"M-2\"and \\ \n   i2.directedName == \"M2\":\n  # since the intervals match , we add an editorial accidental\n  n2.editorial.ficta = \\  \n         Accidental( \"sharp\")\n641\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)#we also color the affected notes so that if we display the music \n  # the notes stick out.  Different colors indicate different rules \n  n1.editorial.color = \"blue\"\n  n2.editorial.color = \"forestGreen\"\n  n3.editorial.color = \" blue\"\nFigure 7. Applying ficta accidentals with music21.\nThe results of applying one or all the rules to an in-\ndividual cadence or piece can be seen immediately. Fig-\nure 8 shows the rules applied to one piece where they create two “closest-approaches” to perfect consonances \n(major sixth to octave and minor third to unison). These \nare the outcomes one expects from a good set of regulae\nfor musica ficta.\n    # get a particular worksheet of an Excel spreadsheet\nballataObj = cadencebook.BallataSheet() \n    # create an object for row 267\npieceObj = ballataObj.makeWork(267)  \n    # run the four rules (as described above)\napplyCapua(pieceObj) \n    # display the first cadence of the piece (second snippet) by \n    # running it through Lilypond and generating a PNG file\npieceObj.snippets[1].lily.showPNG() \nFigure 8.  Music21 code for automatically adding musica \nficta to Francesco (Landini), De[h], pon’ quest’amor, \nfirst cadence. \nIn other pieces, Nicolaus’s rules have an injurious effect, \nas Figure 9 shows. With the toolkit, we were able to run \nthe rules on the entire database of Trecento ballatas and \ndetermine that Nicolaus’s rules cannot be used indiscri-\nminately. Far too many cases appeared where the pro-\nposed ficta hurt the harmony. One of the main advantages \nof the music21  framework is making such observations \non large collections of musical data possible. \nFigure 9.  Francesco, D’amor mi biasmo, incipit after au-\ntomatically applying ficta  accidentals. \n6. FUTURE WORK \nThe first alpha releases of music21  introduce fun-\ndamental objects and containers and, as shown above, \noffer powerful tools for symbolic music processing. The next stage of development will add native sup-\nport for additional input and output formats, including \nMIDI. Further, libraries of additional processing, analy-\nsis, visualization routines, as well as new and expanded \nobject models (such as non-Western scales), will be add-ed to the system. We are presently focusing on creating \nsimple solutions for common-practice music theory tasks \nvia short \nmusic21  scripts, and within a year hope to be \nable to solve almost every common music theory problem \nencountered by first-year conservatory students. \n7. ACKNOWLEDGEMENTS \nThe authors thank the Seaver Institute for their ge-\nnerous funding of music21 . Additional thanks are also \nextended to three anonymous reviewers for their helpful \ncomments. \n8. REFERENCES \n[1] Ariza, C. and M. Cuthbert. 2010. “Modeling Beats, \nAccents, Beams, and Time Signatures Hierarchically with music21 Meter Objects.” In Proceedings of the \nInternational Computer Music Conference. San Francisco: International Computer Music Association. \n[2] Buxton, W. and W. Reeves, R. Baecker, L. Mezei. \n1978. “The Use of Hierarchy and Instance in a Data Structure for Computer Music.” Computer Music \nJournal  2 (4): 10-20. \n[3] Cook, N. 2004. “Computational and Comparative \nMusicology.” In Empirical Musicology: Aims, \nMethods, Prospects . N. Cook and E. Clarke, eds. New \nYork: Oxford University Press. 103-126. \n[4] Good, M. 2001. “An Internet-Friendly Format for \nSheet Music.” In Proceedings of XML 2001.\n[5] Herlinger, J. 2004. “Nicolaus de Capua, Antonio Za-\ncara da Teramo, and musica ficta.” InAntonio Zacara \nda Teramo e il suo tempo. F. Zimei, ed. Lucca: LIM. \n67–89.\n[6] Huron, D. 1997. “Humdrum and Kern: Selective \nFeature Encoding.” In Beyond MIDI: the Handbook \nof Musical Codes . E. Selfridge-Field, ed. Cambrdige: \nMIT Press. 375-401. \n[7] Knopke, I. 2008. “The PerlHumdrum and \nPerlLilypond Toolkits for Symbolic Music Information Retrieval.” ISMIR 2008 147-152. \n[8] Lerdahl, F. and R. Jackendoff. 1983. A Generative \nTheory of Tonal Music . Cambridge: MIT Press. \n[9] Pope, S. T. 1987. “A Smalltalk-80-based Music \nToolkit.” In Proceedings of the International \nComputer Music Conference . San Francisco: \nInternational Computer Music Association. 166-173. \n[10] Pope, S. T. 1989. “Machine Tongues XI: Object-\nOriented Software Design.” Computer Music Journal\n13 (2): 9-22. \n[11] Sapp, C. S. 2008. “Museinfo: Musical Information \nProgramming in C++.” Internet: \nhttp://museinfo.sapp.org. \n642\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "Real-time Polyphonic Music Transcription with Non-negative Matrix Factorization and Beta-divergence.",
        "author": [
            "Arnaud Dessein",
            "Arshia Cont",
            "Guillaume Lemaitre"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1414824",
        "url": "https://doi.org/10.5281/zenodo.1414824",
        "ee": "https://zenodo.org/records/1414824/files/DesseinCL10.pdf",
        "abstract": "In this paper, we investigate the problem of real-time poly- phonic music transcription by employing non-negative ma- trix factorization techniques and the β-divergence as a cost function. We consider real-world setups where the mu- sic signal arrives incrementally to the system and is tran- scribed as it unfolds in time. The proposed transcription system is addressed with a modified non-negative matrix factorization scheme, called non-negative decomposition, where the incoming signal is projected onto a fixed basis of templates learned off-line prior to the decomposition. We discuss the use of non-negative matrix factorization with the β-divergence to achieve the real-time decomposition. The proposed system is evaluated on the specific task of piano music transcription and the results show that it can outperform several state-of-the-art off-line approaches.",
        "zenodo_id": 1414824,
        "dblp_key": "conf/ismir/DesseinCL10",
        "keywords": [
            "real-time",
            "polyphonic",
            "music",
            "transcription",
            "non-negative",
            "matrix",
            "factorization",
            "β-divergence",
            "cost",
            "function"
        ],
        "content": "REAL-TIME POLYPHONIC MUSIC TRANSCRIPTION WITH\nNON-NEGATIVE MATRIX FACTORIZATION AND BETA-DIVERGENCE\nArnaud Dessein, Arshia Cont, Guillaume Lemaitre\nIRCAM – CNRS UMR 9912, Paris, France\n{dessein,cont,lemaitre}@ircam.fr\nABSTRACT\nIn this paper, we investigate the problem of real-time poly-\nphonic music transcription by employing non-negative ma-\ntrix factorization techniques and the \f-divergence as a cost\nfunction. We consider real-world setups where the mu-\nsic signal arrives incrementally to the system and is tran-\nscribed as it unfolds in time. The proposed transcription\nsystem is addressed with a modiﬁed non-negative matrix\nfactorization scheme, called non-negative decomposition,\nwhere the incoming signal is projected onto a ﬁxed basis of\ntemplates learned off-line prior to the decomposition. We\ndiscuss the use of non-negative matrix factorization with\nthe\f-divergence to achieve the real-time decomposition.\nThe proposed system is evaluated on the speciﬁc task of\npiano music transcription and the results show that it can\noutperform several state-of-the-art off-line approaches.\n1. INTRODUCTION\nThe task of music transcription consists in converting a\nraw music signal into a symbolic representation such as a\nscore. Considering polyphonic signals, this task is closely\nrelated to the problem of multiple-pitch estimation which\nhas been largely investigated for music as well as speech,\nand for which a wide variety of methods have been pro-\nposed [8]. Non-negative matrix factorization has already\nbeen used in this context, with off-line approaches [1, 3,\n20, 22–24] as well as on-line approaches [4, 6, 7, 17, 21].\nGenerally speaking, non-negative matrix factorization\n(NMF) is a technique for data analysis where the observed\ndata are supposed to be non-negative [16]. The main phi-\nlosophy of NMF is to build up these observations in a con-\nstructive additive manner, what is particularly interesting\nwhen negative values cannot be interpreted (e.g. pixel in-\ntensity, word occurrence, magnitude spectrum).\nIn this paper, we employ NMF techniques to develop a\nreal-time system for polyphonic music transcription. This\nsystem is thought as a front-end for musical interactions in\nlive performances. Among applications, we are interested\nin computer-assisted improvisation for instruments such as\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2010 International Society for Music Information Retrieval.the piano. We do not discuss such applications in the paper\nbut rather concentrate on the system for polyphonic music\ntranscription and invite the curious reader to visit the com-\npanion website1for complementary information and ad-\nditional resources. The proposed system is addressed with\nan NMF scheme called non-negative decomposition where\nthe signal is projected in real-time onto a basis of note tem-\nplates learned off-line prior to the decomposition.\nIn this context, the price to pay for the simplicity of\nthe standard NMF is the overuse of templates to construct\nthe incoming signal, resulting in note insertions and sub-\nstitutions such as octave and harmonic errors. In [6, 7],\nthe issue has been tackled with the standard Euclidean cost\nby introduction of a sparsity constraint similar to [14]. We\nhere investigate the use of more complex costs by using the\n\f-divergence. This is in contrast to previous systems for\nreal-time audio decomposition which have either consid-\nered the Euclidean distance or the Kullback-Leibler diver-\ngence. NMF with the \f-divergence has recently proved its\nrelevancy for off-line applications in speech analysis [18],\nmusic analysis [11] and music transcription [3, 23]. We\nadapt these approaches to a real-time setup and propose\na tailored multiplicative update to compute the decompo-\nsition. We also give intuition in understanding how the\n\f-divergence helps to improve transcription. The provided\nevaluation show that the proposed system can outperform\nseveral off-line algorithms at the state-of-the-art.\nThe paper is organized as follows. In Section 2, we\nintroduce the related background on NMF techniques. In\nSection 3, we focus on NMF with the \f-divergence, pro-\nvide a multiplicative update tailored to real-time decompo-\nsition, and discuss the relevancy of the \f-divergence for the\ndecomposition of polyphonic music signals. In Section 4,\nwe depict the general architecture of the real-time system\nproposed for polyphonic music transcription, and detail the\ntwo modules respectively used for off-line learning of note\ntemplates and for on-line decomposition of music signals.\nIn Section 5, we perform evaluations of the system for the\nspeciﬁc task of piano music transcription.\nIn the sequel, uppercase bold letters denote matrices,\nlowercase bold letters denote column vectors, lowercase\nplain letters denote scalars. R+andR++denote respec-\ntively the sets of non-negative and of positive scalars. The\nelement-wise multiplication and division between two ma-\ntricesAandBare denoted respectively by A\nBandA\nB.\nThe element-wise power pofAis denoted by A:p.\n1http://imtr.ircam.fr/imtr/Realtime_Transcription\n489\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)2. RELATED BACKGROUND\nThis section introduces the NMF model, the standard NMF\nproblem, and the popular multiplicative updates algorithm\nused to solve it. We then present the relevant literature in\nsound recognition with NMF.\n2.1 NMF model\nThe NMF model is a low-rank approximation for unsu-\npervised multivariate data analysis. Given an n\u0002mnon-\nnegative matrix Vand a positive integer r <min(n;m),\nNMF tries to factorize Vinto ann\u0002rnon-negative matrix\nWand anr\u0002mnon-negative matrix Hsuch that:\nV\u0019WH (1)\nIn this model, the multivariate data are stacked into V,\nwhose columns represent the different observations, and\nwhose rows represent the different variables. Each column\nvjofVcan be expressed as vj\u0019Whj=P\nihijwi,\nwhere wiandhjare respectively the i-th column of W\nand thej-th column of H. The columns of Wthen form\nabasis and each column of His the decomposition of the\ncorresponding column of Vinto this basis.\n2.2 Standard problem and multiplicative updates\nThe standard NMF model of Equation 1 provides an ap-\nproximate factorization WH ofV. The aim is then to\nﬁnd the factorization which optimizes a given goodness-\nof-ﬁt measure called cost function. In the standard for-\nmulation, the Euclidean distance is used, and the NMF\nproblem amounts to minimizing the following cost func-\ntion subject to non-negativity of both WandH:\n1\n2kV\u0000WHk2\nF=1\n2X\njkvj\u0000Whjk2\n2(2)\nFor this particular cost function, factors WandHcan\nbe computed with the popular multiplicative updates intro-\nduced in [16]. These updates are derived from a gradient\ndescent scheme with judiciously chosen steps, as follows:\nH H\nWTV\nWTWHW W\nVHT\nWHHT(3)\nThe updates are applied in turn until convergence, and en-\nsure both non-negativity and decreasing of the cost, but not\nnecessarily local optimality of factors WandH.\nA ﬂourishing literature exists about extensions to the\nstandard NMF problem and their algorithms [5]. These ex-\ntensions can be thought of in terms of modiﬁed cost func-\ntions (e.g. using divergences or adding penalty terms), of\nmodiﬁed constraints (e.g. imposing sparsity), and of mod-\niﬁed models (e.g. using tensors). For example, the cost\nfunction deﬁned in Equation 2 is often replaced with the\nKullback-Leibler divergence for which speciﬁc multiplica-\ntive updates have been derived [16].2.3 Applications in sound recognition\nNMF algorithms have been applied to various problems in\nvision, sound analysis, biomedical data analysis and text\nclassiﬁcation among others [5]. In the context of sound\nanalysis, the matrix Vis in general a time-frequency rep-\nresentation of the sound to analyze. The rows and columns\nrepresent respectively different frequency bins and succes-\nsive time-frames. The factorization vj\u0019P\nihijwican\nthen be interpreted as follows: each basis vector wicon-\ntains a spectral template, and the decomposition coefﬁ-\ncientshijrepresent the activations of the i-th template wi\nat thej-th time-frame.\nNMF has already been used in the context of polyphonic\nmusic transcription (e.g. see [1, 22]). Several problem-\ndependent extensions have been developed to this end such\nas a source-ﬁlter model [24], an harmonic constraint [20],\nan harmonic model with temporal smoothness [3], or an\nharmonic model with spectral smoothness [23]. These ap-\nproaches rely in general on the off-line nature of NMF, but\nsome authors have used NMF in an on-line setup.\nA real-time system to identify the presence and deter-\nmine the pitch of one or more voices is proposed in [21].\nThis system is also adapted for sight-reading evaluation of\nsolo instrument in [4]. Concerning automatic transcrip-\ntion, a similar system is used in [17] for transcription of\npolyphonic music, and in [19] for drum transcription. A\nreal-time system for polyphonic music transcription with\nsparsity considerations is proposed in [6]. The approach\nis further developed in [7] for real-time coupled multiple-\npitch and multiple-instrument recognition. Yet, all these\napproaches are based on NMF with the Euclidean distance\nor the Kullback-Leibler divergence. We discuss the use of\nthe more general \f-divergence as a cost function and its\nrelevancy for decomposition of music signals in Section 3.\n3. NON-NEGATIVE DECOMPOSITION WITH\nTHE BETA-DIVERGENCE\nIn this section, we deﬁne the \f-divergence, give some of its\nproperties, and review its use as a cost function for NMF.\nWe ﬁnally formulate the non-negative decomposition prob-\nlem with the \f-divergence and give multiplicative updates\ntailored to real-time for solving it.\n3.1 Deﬁnition and properties of the beta-divergence\nThe\f-divergences form a parametric family of distortion\nfunctions [9]. For any \f2Rand any points x;y2R++,\nthe\f-divergence from xtoyis deﬁned as follows:\nd\f(xjy) =1\n\f(\f\u00001)\u0000\nx\f+ (\f\u00001)y\f\u0000\fxy\f\u00001\u0001\n(4)\nAs special cases when \f= 0and\f= 1, taking the limits\nin the above deﬁnition leads respectively to the well-known\nItakura-Saito and Kullback-Leibler divergences:\nd\f=0(xjy) =dIS(xjy) =x\ny\u0000logx\ny\u00001 (5)\nd\f=1(xjy) =dKL(xjy) =xlogx\ny+y\u0000x (6)\n490\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)For\f= 2, the\f-divergence specializes to the widely used\nhalf squared Euclidean distance:\nd\f=2(xjy) =dE(xjy) =1\n2(x\u0000y)2(7)\nConcerning their properties, all \f-divergences are non-\nnegative and vanish iff x=y. However, they are not nec-\nessary distances in the strict terms since they are not sym-\nmetric and do not satisfy the triangle inequality in general.\nA property of the \f-divergences relevant to the present\nwork is that for any scaling factor \u00152R++we have:\nd\f(\u0015xj\u0015y ) =\u0015\fd\f(xjy) (8)\nWe discuss further the interest of this scaling property for\ndecomposition of polyphonic music signals in Section 3.3.\n3.2 NMF and the beta-divergence\nThe\f-divergence was ﬁrst used with NMF to interpolate\nbetween the Euclidean distance and the Kullback-Leibler\ndivergence [15]. Starting with the scalar divergence in\nEquation 4, a matrix divergence can be constructed as a\nseparable divergence, i.e.by summing the element-wise\ndivergences. The NMF problem with the \f-divergence\nthen amounts to minimizing the following cost function\nsubject to non-negativity of both WandH:\nD\f(VjWH) =X\ni;jd\f(vijj[WH]ij) (9)\nFor\f= 2, this cost function specializes to the cost deﬁned\nin Equation 2 for standard NMF.\nAs for standard NMF, several algorithms including mul-\ntiplicative updates have been derived for NMF with the\n\f-divergence and its extensions [5, 15]. The \f-divergence\nhas also proved its relevancy as a cost function for audio\noff-line applications in speech analysis [18], music analy-\nsis [11] and music transcription [3, 23].\n3.3 Problem formulation and multiplicative update\nWe now formulate the problem of non-negative decompo-\nsition with the \f-divergence. We assume that Wis a ﬁxed\ndictionary of note templates onto which we seek to decom-\npose the incoming signal vasv\u0019Wh. The problem is\ntherefore equivalent to minimizing the following cost func-\ntion subject to non-negativity of h:\nD\f(vjWh) =X\nid\f(vij[Wh]i) (10)\nTo solve this problem, we update hiteratively by using\na vector version of the corresponding multiplicative update\nproposed in the literature [5, 15]. As Wis ﬁxed, we never\napply its respective update. The algorithm thus amounts to\nrepeating the following update until convergence:\nh h\nWT\u0000\n(Wh):\f\u00002\nv\u0001\nWT(Wh):\f\u00001(11)\nThis scheme ensures non-negativity of h, but not neces-\nsarily local optimality. Unfortunately, no proof has beenfound yet to show that the cost function is non-increasing\nunder this update for a general parameter \f, even if it has\nbeen observed in practice [11]. However, even if such theo-\nretical issues need to be investigated further, the simplicity\nof this scheme makes it suitable for real-time applications\nand gives good results in practice.\nConcerning implementation, we can take advantage of\nWbeing ﬁxed to employ a multiplicative update tailored\nto real-time decomposition. Indeed, after some matrix ma-\nnipulations, we can rewrite the updates as follows:\nh h\n\u0000\nW\n(veT)\u0001T(Wh):\f\u00002\nWT(Wh):\f\u00001(12)\nwhere eis a vector full of ones. This helps to reduce\nthe computational cost of the update scheme as the matrix\u0000\nW\n(veT)\u0001Tneeds only to be computed once.\nThe scaling property in Equation 8 may give an insight\nin understanding the relevancy of the \f-divergence in our\ncontext. For \f= 0, the Itakura-Saito divergence is the\nonly\f-divergence to be scale-invariant as it was remarked\nin [11]. This means that the corresponding NMF problem\ngives the same relative weight to all coefﬁcients, and thus\npenalizes equally a bad ﬁt of factorization for small and\nlarge coefﬁcients. Considering music signals, this amounts\nto giving the same importance to high-energy and to low-\nenergy frequency components. When \f > 0, more em-\nphasis is put on the frequency components of higher en-\nergy, and the emphasis augments with \f. When\f < 0,\nthe effect is the converse. In our context of music decom-\nposition, we try to reconstruct an incoming music signal\nby addition of note templates. In order to avoid common\noctave and harmonic errors, a good reconstruction would\nhave to ﬁnd a compromise between focusing on the funda-\nmental frequency, the ﬁrst partials and higher partials. The\nparameter\fcan thus help to control this trade-off.\n4. GENERAL ARCHITECTURE OF THE SYSTEM\nIn this section, we present the real-time system proposed\nfor polyphonic music transcription. The general architec-\nture is shown schematically in Figure 1. The right side of\nthe ﬁgure represents the music signal arriving in real-time,\nand its decomposition onto notes whose descriptions are\nprovided a priori to the system as templates. These tem-\nplates are learned off-line, as shown on the left side of the\nﬁgure, and constitute the dictionary used during real-time\ndecomposition. We describe the two modules hereafter.\n4.1 Note template learning\nThe learning module aims at building a dictionary Wof\nnote templates onto which the polyphonic music signal is\nprojected during the real-time decomposition phase.\nIn the present work, we use a simple rank-one NMF\nwith the standard cost function as a learning scheme. We\nsuppose that the user has access to isolated note samples of\nthe instruments to transcribe, from which the system learns\ncharacteristic templates. The whole note sample kis ﬁrst\n491\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)IsolatednotesamplesMusicalscene\nNon-negativematrixfactorizationV(k)≈w(k)h(k)Non-negativedecompositionvj≈WhjNotetemplatesNoteactivationsV(k)\nhjvjWw(k)Short-timesoundrepresentationShort-timesoundrepresentationNotetemplatelearning(oﬀ-line)Musicsignaldecomposition(on-line)Figure 1. Schematic view of the general architecture.\nprocessed in a short-time sound representation supposed to\nbe non-negative and approximatively additive (e.g. a short-\ntime magnitude spectrum). The representations are stacked\nin a matrix V(k)where each column v(k)\njis the sound rep-\nresentation of the j-th time-frame. We then solve standard\nNMF with V(k)and a rank of factorization r= 1, us-\ning the multiplicative updates in Equation 3. This learning\nscheme simply gives a template w(k)for each note sample\n(the information in the row vector h(k)is discarded).\n4.2 Music signal decomposition\nHaving learned the templates, we stack them in columns\nto form the dictionary W. The problem of real-time tran-\nscription then amounts to projecting the incoming music\nsignal vjontoW, where vjshare the same representa-\ntional front-end as the note templates. The problem is thus\nequivalent to a non-negative decomposition vj\u0019Whj\nwhere Wis kept ﬁxed and only hjis learned. The learned\nvectors hjwould then provide successive activations of the\ndifferent notes in the music signal. Following the discus-\nsion in Section 3, we learn the vectors hjby employing\nthe\f-divergence as a cost function and the multiplicative\nupdate tailored to real-time decomposition in Equation 11.\nAs such, the system reports only a frame-level activity\nof the notes. Some post-processing is thus needed to ex-\ntract more information about the eventual presence of the\nnotes, and provide a symbolic representation of the music\nsignal for transcription. This post-processing potentially\nincludes activation thresholding, onset detection, temporal\nmodeling, etc. It is however not thoroughly discussed in\nthis paper where we use a simple threshold-based detec-\ntion followed by a minimum duration pruning.\n5. EV ALUATION AND RESULTS\nIn this section, we evaluate the system on polyphonic tran-\nscription of piano music. We provide a subjective evalu-\nation with musical excerpts synthesized from MIDI refer-\nences. We also perform an objective evaluation with a real\npiano music database and standard evaluation metrics.5.1 Subjective evaluation\nAs sample examples, we transcribed two musical excerpts\nsynthesized from MIDI references with real piano samples\nfrom the Real World Computing (RWC) database [12].\nFor the non-negative decomposition, \fwas set to 0:5\nsince this value was shown optimal for music transcrip-\ntion in [23] and provided good results in our tests. The\nthreshold for detection was set to 2and no minimum du-\nration pruning was applied. For the dictionary, one note\ntemplate was learned and max-normalized for each of the\n88notes of the piano using corresponding samples taken\nfrom RWC. We used a simple short-time magnitude spec-\ntrum representation, with a frame size of 50ms leading\nto630samples at a sampling rate of 12600 Hz, and com-\nputed with a zero-padded Fourier transform of 1024 bins.\nThe frames were windowed with a Hamming function, and\nthe hopsize was set to 25ms for template learning and re-\nﬁned to 10ms for decomposition. The decomposition was\ncomputed in real-time simulation under MATLAB on a\n2:40 GHz laptop with 4:00 Go of RAM, and was about\nthree times faster than real-time.\nThe results of the decomposition are shown in Figure 2.\nFigures 2(a) and 2(b) depict the piano-roll representations\nof the two piano excerpts. The ground-truth references\nare represented with rectangles and the transcriptions with\nblack dots. Overall, this shows that the system is able\nto match reliably the note templates to the music signals.\nDuring note attacks, more templates are used due to tran-\nsients but some post-processing such as minimum duration\npruning would help to remove these errors. We also remark\na tendency to shorten sustained notes which may be due to\na different spectral content during note releases.\n5.2 Objective evaluation\nFor a more rigorous evaluation, we considered the stan-\ndards of the Music Information Retrieval Evaluation eX-\nchange (MIREX) [2] and focused on two subtasks: (1) a\nframe-level estimation of the present events in terms of\nmusical pitch, and (2) a note-level tracking of the present\nnotes in terms of musical pitch, onset and offset times.\nFor the evaluation dataset, we chose the MIDI-Aligned\nPiano Sounds (MAPS) database [10]. MAPS contains real\nrecordings of piano pieces with ground-truth references.\nWe selected 25pieces and truncated each of them to 30s.\nConcerning parameters, \fwas set to 0:5. The thresh-\nolds for detection were set empirically to 1and2for the\nframe and note levels respectively. The minimum dura-\ntion for pruning was set to 50ms. The templates were\nlearned from MAPS with the same representation front-\nend as above. This algorithm is referenced by BND.\nIn addition, we tested the system with the standard Eu-\nclidean decomposition algorithm referenced by END, and\nwith the sparse algorithm of [14] with projection onto the\ncone of sparsity s= 0:9. For these two algorithms, the\ndetection thresholds were set to 2and4for the frame and\nnote levels respectively. To compare results, we also per-\nformed the evaluation for two off-line systems at the state-\nof-the-art: one based on NMF but with an harmonic model\n492\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)0 5 10 15 20 25 30G1 C2 F2 A2#D3#G3#C4#F4#B4 E5 A5 D6 \nTime (s)Pitch(a) 1st movement, Pavane de la Belle au bois dormant.\n0 5 10 15 20 25 30F2 A2#D3#G3#C4#F4#B4 E5 A5 \nTime (s)Pitch\n(b) 4th movement, Les entretiens de la Belle et de la B ˆete.\nFigure 2. Transcription of two piano excerpts from Ma\nm`ere l’Oye, Cinq pi `eces enfantines pour piano `a quatre\nmains (1908-1910), Maurice Ravel (1875-1937).\nand spectral smoothness [23], and another one based on\na sinusoidal analysis with a candidate selection exploiting\nspectral features [25].\nWe report the evaluation results per algorithm in Ta-\nbles 1 and 2 at the frame and note levels respectively. Stan-\ndard evaluation metrics from the MIREX are used as de-\nscribed in [2]: precision P, recallR,F-measureF, ac-\ncuracyA, total errorEtot, substitution error Esubs, missed\nerrorEmiss, false alarm errorEfa, mean overlap ratio M.\nAt the note level, the subscripts 1and2represent respec-\ntively the onset-based and the onset/offset-based results.\nOverall, the results show that the proposed real-time\nsystem performs comparably to the state-of-the-art off-line\nalgorithms of [23,25]. Using the \f-divergence, the system\nBND even outperforms the other algorithms. The sparse\nalgorithm of [14] reduces insertions and substitutions, but\naugments the number of missed notes so that it actually\ndoes not perform better than the standard scheme END.\nThe standard Euclidean cost also shows its limits for tran-\nscription where more complex costs with the \f-divergence\ngive better results. We ﬁnally remark that the mean over-\nlap ratio scores corroborate the observation that sustained\nnotes tend to be shortened.Alg. P R F A E totEsubsEmissEfa\nBND 63:9 67 :365:5 48:7 58:9 11:9 20:8 26:2\nEND 55:3 58 :656:9 39:8 71:4 17 :3 24:1 29:9\n[14] 58:5 55 :2 56: 8 39:7 67:1 16:8 28:0 22:3\n[23] 61:0 66 :7 63: 7 46:8 65:6 10 :4 22:9 32:3\n[25] 60:0 70 :865:0 48:1 60:0 16:3 12:8 30:8\nTable 1. Frame-level transcription results per algorithm.\nAlg. P1R1F1A1M1P2R2F2A2\nBND 75:5 67 :171:1 55:1 56:7 30:0 26 :628:2 16:4\nEND 57:9 58 :258:1 40:9 53:9 21:4 21 :621:5 12:0\n[14] 57:2 56 :3 56:8 39: 654:1 21:0 20 :7 20: 8 11:6\n[23] 58:1 73 :765:0 48:1 57:7 20:7 26 :323:2 13:1\n[25] 33:0 58 :8 42:3 26: 8 55:1 11:6 20 :7 14: 9 8:0\nTable 2. Note-level transcription results per algorithm.\n6. CONCLUSION\nThis paper addressed the problem of real-time polyphonic\nmusic transcription by employing NMF techniques. We\ndiscussed the use of the \f-divergence as a cost function\nfor non-negative decomposition tailored to real-time tran-\nscription. The obtained results show that the proposed\nsystem can outperform state-of-the-art off-line approaches,\nand are encouraging for further development.\nA problem in our approach is that templates are inher-\nently considered as stationary. One way to tackle this is\nto consider representations that capture variability over a\nshort time-span as in [7]. We could also combine NMF\nwith a state representation and use templates for each state.\nThe template learning method can be further improved\nby using extended NMF problems and algorithms to learn\none or more templates for each note. Such issues have not\nbeen developed but interesting perspectives include learn-\ning sparse or harmonic templates. Using the \f-divergence\nduring template learning in our experience did not improve\nthe results. Further considerations are needed on this line.\nIn a live performance setup such as ours, the templates\ncan be directly learned from the corresponding instrument.\nYet in other setups, the issue of generalization must be\ncarefully considered and will be discussed in future work.\nWe think of considering adaptive templates by adapting an\napproach proposed in [13] to real-time decomposition.\nWe would like also to improve the robustness against\nnoise, by keeping information from the activations during\ntemplate learning, or by using noise templates as in [7].\nIn addition, we want to develop more elaborate sparsity\ncontrols than in [6, 7, 14]. In our approach, sparsity is\ncontrolled implicitly during decomposition. Yet in some\napplications, specially for complex problems such as audi-\ntory scene analysis, controlling explicitly sparsity becomes\ncrucial. A forthcoming paper will address this issue.\nLast but not least, the proposed system is currently un-\nder development for the Max/MSP real-time computer mu-\nsic environment and will be soon available for free down-\nload on the companion website.\n493\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)7. ACKNOWLEDGMENTS\nThis work was partially funded by a doctoral fellowship\nfrom the UPMC (EDITE). The authors would like to thank\nC. Yeh and R. Badeau for their valuable help, V . Emiya for\nkindly providing the MAPS database, as well as P. Hoyer\nand E. Vincent for sharing their source code.\n8. REFERENCES\n[1] S. A. Abdallah and M. D. Plumbley. Polyphonic music\ntranscription by non-negative sparse coding of power\nspectra. In Proc. of ISMIR 2004, pages 318–325,\nBarcelona, Spain, October 2004.\n[2] M. Bay, A. F. Ehmann, and J. S. Downie. Evaluation of\nmultiple-F 0estimation and tracking systems. In Proc.\nof ISMIR 2009, Kobe, Japan, October 2009.\n[3] N. Bertin, R. Badeau, and E. Vincent. Enforcing har-\nmonicity and smoothness in Bayesian non-negative\nmatrix factorization applied to polyphonic music tran-\nscription. IEEE Transactions on Audio, Speech and\nLanguage Processing, 18(3):538–549, March 2010.\n[4] C.-C. Cheng, D. J. Hu, and L. K. Saul. Nonnega-\ntive matrix factorization for real time musical analysis\nand sight-reading evaluation. In Proc. of ICASSP 2008,\npages 2017–2020, Las Vegas, NV , USA, March/April\n2008.\n[5] A. Cichocki, R. Zdunek, A. H. Phan, and S.-i. Amari.\nNonnegative Matrix and Tensor Factorizations: Appli-\ncations to Exploratory Multi-way Data Analysis and\nBlind Source Separation. Wiley-Blackwell, 2009.\n[6] A. Cont. Realtime multiple pitch observation using\nsparse non-negative constraints. In Proc. of ISMIR\n2006, Victoria, Canada, October 2006.\n[7] A. Cont, S. Dubnov, and D. Wessel. Realtime multiple-\npitch and multiple-instrument recognition for music\nsignals using sparse non-negative constraints. In Proc.\nof DAFx 2007, Bordeaux, France, September 2007.\n[8] A. de Cheveign ´e.Computational Auditory Scene\nAnalysis: Principles, Algorithms and Applications,\nchapter Multiple F0Estimation, pages 45–72. Wi-\nley-IEEE Press, 2006.\n[9] S. Eguchi and Y . Kano. Robustifying maximum likeli-\nhood estimation. Technical report, Institute of Statisti-\ncal Mathematics, Tokyo, Japan, 2001.\n[10] V . Emiya, R. Badeau, and B. David. Multipitch esti-\nmation of piano sounds using a new probabilistic spec-\ntral smoothness principle. IEEE Transactions on Au-\ndio, Speech and Language Processing, To appear.\n[11] C. F ´evotte, N. Bertin, and J.-L. Durrieu. Nonnegative\nmatrix factorization with the Itakura-Saito divergence:\nwith application to music analysis. Neural Computa-\ntion, 21(3):793–830, March 2009.[12] M. Goto, H. Hashiguchi, T. Nishimura, and R. Oka.\nRWC music database: popular, classical, and jazz mu-\nsic databases. In Proc. of ISMIR 2002, pages 287–288,\nOctober 2002.\n[13] M. Heiler and C. Schn ¨orr. Learning sparse representa-\ntions by non-negative matrix factorization and sequen-\ntial cone programming. J. of Machine Learning Re-\nsearch, 7:1385–1407, July 2006.\n[14] P. O. Hoyer. Non-negative matrix factorization with\nsparseness constraints. J. of Machine Learning Re-\nsearch, 5:1457–1469, November 2004.\n[15] R. Kompass. A generalized divergence measure for\nnonnegative matrix factorization. Neural Computation,\n19(3):780–791, 2007.\n[16] D. D. Lee and H. S. Seung. Learning the parts of\nobjects by non-negative matrix factorization. Nature,\n401(6755):788–791, October 1999.\n[17] B. Niedermayer. Non-negative matrix division for the\nautomatic transcription of polyphonic music. In Proc.\nof ISMIR 2008, pages 544–549, Philadelphia, PA,\nUSA, September 2008.\n[18] P. D. O’Grady and B. A. Pearlmutter. Discovering\nspeech phones using convolutive non-negative matrix\nfactorisation with a sparseness constraint. Neurocom-\nputing, 72:88–101, 2008.\n[19] J. Paulus and T. Virtanen. Drum transcription with non-\nnegative spectrogram factorisation. In Proc. of EU-\nSIPCO 2005, Antalya, Turkey, September 2005.\n[20] S. A. Raczy ´nski, N. Ono, and S. Sagayama. Harmonic\nnonnegative matrix approximation for multipitch anal-\nysis of musical sounds. In Proc. of ASJ Autumn Meet-\ning, pages 827–830, September 2007.\n[21] F. Sha and L. K. Saul. Real-time pitch determination\nof one or more voices by nonnegative matrix factoriza-\ntion. In Proc. of NIPS 2004, volume 17, pages 1233–\n1240, Cambridge, MA, USA, 2005.\n[22] P. Smaragdis and J. C. Brown. Non-negative matrix\nfactorization for polyphonic music transcription. In\nProc. of WASPAA 2003, pages 177–180, New Paltz,\nNY , USA, October 2003.\n[23] E. Vincent, N. Bertin, and R. Badeau. Adaptive har-\nmonic spectral decomposition for multiple pitch esti-\nmation. IEEE Transactions on Audio, Speech and Lan-\nguage Processing, 18(3), March 2010.\n[24] T. Virtanen and A. Klapuri. Analysis of polyphonic au-\ndio using source-ﬁlter model and non-negative matrix\nfactorization. In Proc. of NIPS Workshop AMAC 2006,\n2006.\n[25] C. Yeh. Multiple fundamental frequency estimation of\npolyphonic recordings. PhD thesis, Universit ´e Pierre et\nMarie Curie, Paris, France, June 2008.\n494\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "Modified Ais-based Classifier for Music Genre Classification.",
        "author": [
            "Noor Azilah Draman",
            "Campbell Wilson",
            "Sea Ling"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1417437",
        "url": "https://doi.org/10.5281/zenodo.1417437",
        "ee": "https://zenodo.org/records/1417437/files/DramanWL10.pdf",
        "abstract": "Automating human capabilities for classifying different genre of songs is a difficult task. This has led to various studies that focused on finding solutions to solve this problem. Analyzing music contents (often referred as con- tent-based analysis) is one of many ways to identify and group similar songs together. Various music contents, for example beat, pitch, timbral and many others were used and analyzed to represent the music. To be able to mani- pulate these content representations for recognition: fea- ture extraction and classification are two major focuses of investigation in this area. Though various classification techniques proposed so far, we are introducing yet anoth- er one. The objective of this paper is to introduce a possi- ble new technique in the Artificial Immune System (AIS) domain called a modified immune classifier (MIC) for music genre classification. MIC is the newest version of Negative Selection Algorithm (NSA) where it stresses the self and non-self cells recognition and a complementary process for generating detectors. The discussion will de- tail out the MIC procedures applied and the modified part in solving the classification problem. At the end, the re- sults of proposed framework will be presented, discussed and directions for future work are given.",
        "zenodo_id": 1417437,
        "dblp_key": "conf/ismir/DramanWL10",
        "keywords": [
            "Automating",
            "genre",
            "classification",
            "difficulty",
            "various",
            "studies",
            "content-based",
            "analysis",
            "music",
            "contents"
        ],
        "content": "MODIFIED AIS-BASED CLASSIFIER FOR MUSIC GENRE \nCLA\nSSIFICATION  \n   Noor Azilah Draman, Campbell Wilson and Sea Ling \nC\naulfield School of IT,  \nFaculty of Information Technology, Monash University,  \nCaulfield East, Melbourne, Victoria, 3145  \nnadra1@student.monash.edu.au  \nca\nmpbell.wilson@infotech.monash.edu.au \nchris.ling@infotech.monash.edu.au \n \nABSTRACT \nAu\ntomating human capabilities for classifying different \ngenre of songs is a difficult task. This has led to various \nstudies that focused on finding solutions to solve this \nproblem. Analyzing music contents (often referred as con-\ntent-based analysis) is one of many ways to identify and \ngroup similar songs together. Various music contents, for \nexample beat, pitch, timbral and many others were used \nand analyzed to represent the music. To be able to mani-\npulate these content representations for recognition: fea-\nture extraction and classification are two major focuses of \ninvestigation in this area. Though various classification \ntechniques proposed so far, we are introducing yet anoth-\ner one. The objective of this paper is to introduce a possi-\nble new technique in the Artificial Immune System (AIS) \ndomain called a modified immune classifier (MIC) for \nmusic genre classification. MIC is the newest version of \nNegative Selection Algorithm (NSA) where it stresses the \nself and non-self cells recognition and a complementary \nprocess for generating detectors. The discussion will de-\ntail out the MIC procedures applied and the modified part \nin solving the classification problem. At the end, the re-\nsults of proposed framework will be presented, discussed \nand directions for future work are given. \n1. INTRODUCTION \nMusic genre is defined as classes or groups of songs that \ncategorizes a collection of songs that have similar charac-\nteristics. It is a label created by music experts so that \nthese songs are easily described and recognized [1]. \nThere have been various studies on music genre classifi-\ncation over the years where generally the focuses would \nbe on the type of features extracted, feature extraction \ntechniques, feature selection mechanisms, and feature \nclassification algorithms. This is because music genre \nclassification is a unique topic, and an investigation that \ntries to imitate human capability to identify music. It is a \nprocess to automate the human skills in recognizing and \ngrouping different type of music into categories by using \ntheir hearing senses and logical judgment.  \nOur research is also about automating the human iden-\ntification process where we are investigating an algorithm \nfrom Artificial Immune System (AIS), called the modified \nimmune classifier (MIC). MIC is a modification of nega-\ntive selection algorithm, introduced in writer identifica-tion study [2]. Negative selection algorithm is one of a \nfew algorithms developed in AIS domain where it stresses \nthe antigen recognition process. Two processes involved: \nmonitoring, a process of recognizing self/non-self cells by \nperforming the affinity binding, and censoring, the proc-\ness where antibodies (also known as detectors) are ran-\ndomly generated to match with the antigens. The recog-\nnized antigens are called self cells whereas the non-\nrecognized antigens are known as non-self cells. In the \nhuman immune system, recognized antigen is referring to \ncells that prevent human body from disease and non-\nrecognized antigens are referring to cells that bring dis-\neases to human body. MIC  eliminates the process to gen-\nerate detectors randomly, which is the main aspect of the \nNSA, by introducing a complementary process. This \ncomplementary process will define self cells based on \nhow many classes of data they need to identify and then \ngenerate the detectors accordingly.  \nHowever, to be able to apply the modified immune \nclassifier in this research, which is to be able to identify \nand recognize different groups of music genre, we need to \nchange some part of the classifier in order to achieve high \naccuracy of results. We will discuss the changes that we \nhave made later.  \nWe present this paper with the intention of discussing \nmusic genre classification that applies modified immune \nclassifier in the classification process. We are discussing \nin detail the feature extraction and feature selection \nprocesses except to explain the features used in the expe-\nrimental work and the techniques used to select relevant \nand significant features. We elaborate the AIS approach \nin the context of music genre classification, their conse-\nquences in music recognition performances whether the \napproach will have a major impact to the classification \nperformances.  \nWe organize the remainder of this paper as follows: \nSection 2 discusses previous research in music genre rec-\nognition. Section 3 discusses the MIC and the changes \npart, the censoring and monitoring stages, and how these \nstages relate to the feature extraction, selection, and clas-\nsification. Section 4 then will be discussing the experi-\nmental setup and the classification results. We outline \nsome concluding remarks in the last section. \n2. BACKGROUND OF STUDY \nIn the music genre identification and classification stu-\ndies, initiated research was to solve problems that occur \n369\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \nd\nuring recognition such as, deciding which song belongs \nto which genre. [3], for example, did an early work of \nclassifying songs into different categories of genre using \nhuman auditory skills. Since then, many studies to find \nsolutions to increase the automation performances oc-\ncurred. Various recorded attempts to solve this problem \nare in [2] – [9]. Not only the problem of automating the \nprocess of classification but the question of how to fill the \ngap of accuracy behind human skilled classification [3] \nalso need to be answered and solved. \n[1] contributed by introducing new music features \nfrom pitch, timbre and rhythm contents. Their experi-\nments on genre classification have shown that their at-\ntempts can be investigated further as the classification ac-\ncuracy results were around 61 percent only. The focus of \ntheir research was to introduce a new range of music fea-\ntures for music genre classification. As the extracted fea-\ntures are too numerous, many irrelevant and insignificant \nfeatures were used in their experiments that contributed to \nthe low level of performances.  \n[10] introduced a new technique to extract music fea-\ntures called Daubechies Wavelet Coefficient Histograms \n(DWCHs) with a purpose to overcome the classification \naccuracy problems in the previous study. The authors \nused the Daubechies wavelet filter, Db8, to decompose \nmusic signals into layers where at the end of each layer \nthey constructed histograms of coefficient wavelet. Dur-\ning experiments they combined their new feature with [1] \nfeatures and improved the results but not by much.  \nThere is also another attempt that used pitch, rhythm \nand timbre contents to classify music into different genres \n[11]. In this study, the author used the neural network \nbased classifier which was not tested in the previous two \nstudies. Again similar problem that related to the classifi-\ncation performance occurred. The experiments have \nshown that the accuracy was quite high when the classifi-\ncation processes were to recognize one or two genres \nonly. But, as the classes of genres increased, the perform-\nances began to decrease.  \n[12] proposed a solution to the problem mentioned \nabove. The authors proposed a new feature extraction \nmethod called InMAF.  This new method was quite differ-\nent from previous approaches where previously, they re-\nlied mostly on the spectrum characteristics of music con-\ntent. InMAF  on the other hand integrated the acoustic fea-\ntures and the human musical perception into music feature \nvectors to increase the classification performances. The \nclassification results were so impressive that the achieved \naccuracies were as high as ninety percent. However, these \noutcomes were the results of a combination of this new \nmethod with pitch, rhythm and pitch contents. There is no \nclassification result from any individual features recorded \nin the study.  \n[8] attempted to classify the music genre using MIDI \n(Musical Instrument Digital Interface) and audio features, \nsuch as pitch, rhythm and timbre features by using the \ndata from [13], which contained two different sets of fea-\ntures, the first was MIDI features and the other group was \nthe audio features. However the attempt was not that suc-\ncessful as the result did not show any major improvement \nin the classification performances.  A new recent study proposed a new approach to \nclassify music genre by emphasizing the features from \ncepstral contents, such as MFCCs, OSC and MPEG 7 \nrepresentations [14]. They introduced a novel set of fea-\ntures that were derived from modulation spectral analysis \nof the spectral representations, and these features were the \nMel-Frequency Cepstral Coefficients  (MFCC), Octave-\nbased Spectral Contrast  (OSC), Normalized Audio Spec-\ntral Envelope  (NASE) and Modulation Spectral Analysis  \nof MFCC, OSC and NASE. Their experiments were con-\nducted on individual features and combinations of fea-\ntures.  \nThe results were very good, where the combination \nof features tested were able to achieve the accuracy \naround twenty percent higher than any studies that we \nhave discussed so far. That was an impressive achieve-\nment since low classification accuracy is the major prob-\nlem faced by the domain.  \n3.  AIS-BASED CLASSIFIER \nIn this part, we discuss Artificial Immune System (AIS) \napproach specifically on the modified negative selection \nalgorithm (MIC) to classify the music genre. According to \n[15], the human immunology system inspired this domain \nto observe the immune functions, models, and principles \nof immunology. Some references on AIS-based classifica-\ntion task can be found in [16 -17]. \nAIS are adaptive systems, emulating human body im-\nmunology system to solve problems. It is concerned with \nabstracting the whole concept of immune system to com-\nputational systems in solving problems from mathematics, \nengineering, and information technology point of view. \nAIS is developed based upon a set of general purposes \nalgorithms that are modelled to generate artificial compo-\nnents of the human immune system. [15] defined AIS as \nan adaptive system which is enthused by biological im-\nmunology and observed functions, principles and models \nto problem solving.  \n[18] introduced negative selection algorithm as in-\nspired by negative selection of T-cells in thymus. The al-\ngorithm focused on recognizing self or non-self cells \nwhere it eliminated the T-cells which thymus does not \nrecognized. Detail explanations of how negative selection \nalgorithm works is in [19]. As has been investigated be-\nfore, it would be impossible to apply NSA without mod-\nification as each problem and solutions are different. \nHowever, we will not discuss the NSA further as it is not \nin the research scope.  \nIn the next section, we will discuss the MIC, the cen-\nsoring and monitoring stages including features conver-\nsion, complementary and identification processes that we \nhave applied to suit with the problem in hand. Then we \ncontinue the discussion with detailed explanation of the \nchanges that we have made in the identification accuracy \ncalculation. \n3.1 Modified Immune Classifier (MIC)  \nThe inspiration to investigate MIC in this research comes \nfrom a writer identification study [2] where the proposed \n370\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \nc\nlassifier to identify different writers has provided excel-\nlent results as the identification test achieved the accuracy \nas high as 99 percent. The recognition is evaluated by \nemphasizing the affinity binding or similarities between \nthose cells.  \nIn this new version of NSA, the author introduced a \ncomplementary process, which is a process of generating \ndetectors to detect antigens. Originally, in NSA, the de-\ntectors are randomly generated and cost some time. They \nalso do not contain enough information to recognize the \nwhole range of antigens.  \nThis would become a problem because, in order to rec-\nognize the antigens, the generated detectors shall not be \ncreated randomly as the process will not guarantee there \nwill be enough detectors. By having the complementary \nprocess, the detectors will be generated accordingly to \ncompensate the antigens as has been done in the writer \nidentification research where the complementary process \ngenerated detectors according to the number of writers \nthat should be recognized. Imitating the immune system’s \nfunction, MIC works:  \n(1) self-cells (feature vectors) are transformed into anti-\nbodies (detectors) to detect similar cells (antigen),  \n(2) during detection (identification) antibodies will do \nthe affinity binding with the antigens (finding simi-\nlarities),  \n(3) both cells will bind (matched) if there are similari-\nties occurred – antibodies detected antigens as simi-\nlar to it cells – a pattern is recognized \nAs has been mentioned earlier, censoring and monitor-\ning modules are two important process of MIC. We will \ndiscuss them next. \n3.2 Censoring and monitoring modules \nCensoring module is responsible to produce detectors, \nwhich is the key aspect of identification. This module \nnormally starts after feature extraction and feature selec-\ntion processes. It involves data feature conversion where \nthe features will be represented by binary bit strings (for \nexample, a feature vector, -3.4523123 is converted into a \nbinary string, 101011001 using –XOR operation). After \nthe conversion, the binary bit strings then will go through \nthe complementary process and become the detectors.  \nWe applied the supervised learning experiments in this \nresearch and we used training data to generate the detec-\ntors. Once generated, we used them in the classification \nprocess by comparing the detectors and generated anti-\ngens (we converted testing data into antigens). The com-\nparison occurred in the monitoring module (the training \nmodel/detectors created earlier to predict the testing da-\nta/antigens) and it was to find matched data between de-\ntectors and antigens. If matched, we then calculate the af-\nfinity binding. \n The comparison produced binary bit ‘1’ or ‘0’ where \nbit ‘1’ means the data is bind. However, in this scenario, \nwe will use the word ‘match’ instead of ‘bind’ to define \nthe similarities. Figure 3.1 illustrates both modules where \ntwo important things occurred in censoring module, which are the conversion data from feature vectors into \nbinary bit strings using –XOR and detectors generated \nprocesses. In monitoring module, two important things \nalso occurred, which are antigens generated from testing \ndata and identification processes. During binary matching \nprocess, we used Hamming distance technique to calcu-\nlate matched binary bits.  \n \nF\nigure 1. Censoring and monitoring modules \n3.3 Accuracy calculation \nIn the writer identification problem, the calculation \nemphasized the recognition of each feature where these \nfeatures will be calculated individually based on a thre-\nshold value. The accuracy would be based on how many \nfeatures were correctly classified. To apply MIC to our \nproblem, we concentrated on the threshold value in the \naccuracy calculation where the value will be our bench-\nmark to decide whether the songs are classified accurately \nor not.  \nDuring the process, we calculated the result first by \ncombining all features and produced the data accuracy \npercentage. Then we compared the accumulated value \nwith the threshold value percentage. If the percentage of \nthe combined features is higher than the threshold value, \nthe data then is labeled as accurately classified. The fol-\nlowing Table 3.1 and 3.2 will show the difference be-\ntween the writer identification calculation and ours.  \nThe difference between the original MIC proposed in \n[2] with ours is that we combined all the feature vectors \nas one whole data and calculates the matched bits before \nwe compare them with the threshold value, whereas in the \nauthor identification study, the matched bit is calculated \n371\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \ns\neparately for each feature and the accuracy is computed \nbased on the total amount of features that exceeded the \nthreshold value. \n \nCategory  Calculation formulas \nFeature match-\ni\nng stage Num_of_bit_match ≥ threshold \nImage accuracy \ns\ntage  (Num_of_feature_match / \nnum_of_feature) ×100 \nData accuracy \ns\ntage (Num_of_genre_match / \nnum_of_testing_data) ×100 \nTable 1. The writer identification calculation \n \nCategory Calculation formulas \nData genre ac-\nc\nuracy %  ∑ bits_matched / ∑ features_bits × 100 \nThreshold (r ) % ( ∑ r * num_of_features / ∑ \nbits_per_feature * num_of_features) × \n1\n00 \nDataset accura-\nc\ny  (Num_of_genre_match / \nnum_of_testing_data) ×100  \nTable 2. T he music genre accuracy calculation \n4. EXPERIMENTS  \nIn this section, we explain our conducted experiments to \nevaluate the proposed algorithm .  \n4.1 Datasets \nWe used Latin music datasets which contains 3160 music \npieces in MP3 format classified in 10 different musical \ngenres [20][21]. The songs were grouped into 10 genres: \nTango, Bolero, Batchata, Salsa, Merengue, Axé, Forró, \nSertaneja, Gaúcha and Pagode. The extracted music fea-\ntures were from timbre contents (containing MFCC, spec-\ntral centroid, roll-off, flux, time domain zero crossings), \npitch-histograms related features and beat calculated fea-\ntures. The features were extracted using MARSYAS [22] \nwhere the combined total of the features produced 30 vec-\ntors for each song.  \nWe have prepared training and testing datasets where \nsimilar data is used in the experiments except that the data \nfor WEKA experiments was in the attribute related file \nformat (ARFF) and in the data file (DAT) format for MIC \ndemonstrations.  \n4.2 Feature selection technique \nWe have used WEKA tool to select relevant and signifi-\ncant features. We used filter approach in this study be-\ncause it is more practical and suitable for our problem as \nthe approach is independent and work separately from the \nclassifier. The filter approach also works faster than \nwrapper and embedded approaches.  \nWe have selected significant features using two search \napproaches, which are the best first search algorithm and \nthe greedy hill search algorithm. The techniques that we \nused to do the best first search selection and the greedy \nhill selection are the FilterSubsetEval, the CFSSubsetEval \nand the ConsistencySubsetEval. The produced selected features from these techniques contained 13, 17, and 18 \nfeature vectors. \nWe tested the MIC algorithm in the classification \nprocesses by defining the threshold value as 12. The rea-\nson is that we want to compare the proposed MIC with \nother classifiers without evaluating various threshold val-\nues to select the best one. The chosen threshold value is \nconsidered practical and enough to determine the relia-\nbility of the proposed technique. \nTable 3 describes the feature vectors in detail where \nthey have been numbered (1 to 30) for easy identification. \n \nFeatures Description \n1 - 6 Beat-related features (peak histograms, amplitude \na\nnd period) \n7 - 25 Timbral features (mean and standard deviation of \ns\npectral centroid, rolloff, flux, zero crossings, \nMFCC, low energy) \n26 - 30 Pitch related features (folded and unfolded histo-\ng\nrams, period, amplitude pitch interval of unfolded \nhistograms) \nTable 3. F eatures description \n4.2 Classification \nFor comparison purposes, we used classifiers from Wai-\nkato Environment for Knowledge Analysis (WEKA) [23] \nand the MIC algorithm that we have built using C++ lan-\nguage. We have chosen few classifiers from different cat-\negory in WEKA.  \nWe have setup the experiment cases according to the \nselected features from selection process. We also have \nsetup experiments to test individual group of features and \ncombinations between the groups. The reason is that we \nwant to test the robustness of our program and the relia-\nbility of AIS-based classifier performance in our classifi-\ncation problems. Table 4, 5, and 6 will explain these cas-\nes in detail. \n \nCases Description \nC1 Features 1, 2, 6, 9,10, 13, 17, 18, 22, 25, 26, 28 \nC2 Features 1, 4, 6, 7, 9, 10, 12, 13, 14, 15, 16, 17, 18, \n2\n1,  23, 26, 28 \nC3 Features 1, 4, 6, 9, 10,11, 12, 13, 14, 15, 16, 17, 18, \n2\n1, 22, 23, 25, 26 \nC4 Contains all 30features \nTable 4. L ist of selected features  \nCases Description \nF1 Features 1 – 6 (beat related features only) \nF2 Features 7-  25 (timbral related features only) \nF3 Features 26 – 30 (pitch related features only)  \nTable 5. Individual group of features  \nCases Description \nFBP Combination of beat and pitch related features \nFBT Combination of beat and timbral related features \nFTP Combination of timbral and pitch related features \nTable 6. Combination of group features  \n372\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \n4\n.3 Results \nTable 7, Table 8, and Table 9 list all classification results \nthat we have obtained from the prepared cases classifica-\ntion experiments. \n \nTechnique  \nCases Case 1 Case 2 Case 3 Case 4 \nBayesNet 50.00% 53.00% 53.00% 58.33% \nSMO 46.00% 48.67% 57.00% 56.33% \nIB1 49.00% 51.67% 56.00% 57.00% \nBagging 42.33% 44.00% 47.67% 48.33% \nJ48 38.00% 38.33% 42.00% 42.00% \nMIC 99.33% 95.00% 92.67% 73.00% \nTable 7. Selected features cases \n \nTechnique  \nCases F1 F2 F3 \nBayesNet 29.67% 29.67% 49.33% \nSMO 27.33% 31.33% 50.00% \nIB1 27.33% 1 00 % 50.33% \nBagging 30.33% 66.33% 53.00% \nJ48 28.67% 72.00% 45.00% \nMIC 100 % 100 % 93.33% \nTable 8. Individual group of features cases \n \nTechnique  \nCases FBP FBT FTP \nBayesNet 39.3333% 53.0000% 54.3333% \nSMO 33.3333% 57.6667% 56.3333% \nIB1 38.3333% 55.3333% 56.0000% \nBagging 35.0000% 49.3333% 52.6667% \nJ48 37.3333% 40.3333% 48.3333% \nMIC 99.00% 79.33% 91.33% \nTable 9. Combination of group features cases \nI\nn Table 7, for feature selection cases, all cases except \nfor the data without feature selection, MIC has obtained \nthe accuracies over 90% compared to other classifiers. \nThe performances of other classifiers did not show any \nsignificant improvement compared to MIC. \nTable 8, which is referring to the individual group of \nfeatures experiments. Overall performances for each fea-\nture when tested with various classifiers have shown that \nbeat related features produced the lowest accuracy results. \nTimbral related features came in second however, when \ntested with MIC classifier pitch and timbral features pro-\nduced similar percentages. Bagging classifier also pro-\nduced similar result when tested the timbral related fea-\ntures to classify the songs.  In Table 9, WEKA classifiers produced almost similar \nresults when we experimented with both beat+timbral re-\nlated and timbral+pitch related features. The lowest accu-\nracy recorded with beat+pitch related features when these \nfeatures were used for classification. However, the oppo-\nsite case occurred when the data were classified using \nMIC classifier because the lowest accuracy recorded \nwhen beat+timbral related features were tested.  \n5. CONCLUSION \nThe availability of techniques and methods for classifica-\ntion in music analysis field proved that researchers in this \narea are very concerned with the performance. As the col-\nlections of digital songs keep increasing online, their stu-\ndies have contributed a major breakthrough to the internet \nusers and others. \nIn this paper, we have experimented and explained the \nproposed MIC in different category of cases. In each ex-\nperiment, MIC has outperformed almost every classifier \nexcept for Bagging technique where in one of the cases, \nthe result is exactly similar to what MIC has produced. \nThe obtained results have clearly shown that MIC is a \nnew prospective approach for music genre classification. \nIt has been proven the proposed classifier in music recog-\nnition research has surpassed other classifiers and the im-\nprovement of classification accuracy is phenomenal. The \nresults also showed that among the features, timbral has \nprovided us good classification result in the most cases \nexcept for the combined features cases. \nWe strongly believe that our discussion throughout this \npaper has given opportunities to other researchers in this \narea of studies to fill the gaps, to explore further and to \nprovide solutions to the known and un-known problem \nthat has yet to be discovered. Future work will include an \ninvestigation on how to manage efficiently the threshold \nvalue and probably later on, exhaustive search approach \nshould be applied to evaluate the highest threshold value \nthat can provide high classification accuracies. \n6. REFERENCES \n[1] G. Tzanetakis and P. Cook, “Musical genre \nclassification of audio signals”, IEEE Transactions \non Speech and Audio Processing, vol.10, no.5, pp. \n293-302, 2002. \n[2] Draman, A. K. “Authorship invarianceness for \nwriter identification using invariant discretiation and \nmodified immune classifier”, PhD thesis. University \nof Technology Malaysia, 2009. \n[3]  Lippens, S., Martens, J. P. & Mulder, T. D. “A \ncomparison of human and automatic musical genre \nclassification”  Acoustics, Speech, and Signal \nProcessing, 2004. \n[4]    Brecheisen, S., Kriegel, H. P., Kunath, P. & \nPryakhin, A. “Hierarchical genre classification for \nlarge music collections”. 2006 IEEE International \n373\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \nC\nonference on Multimedia and Expo - ICME 2006, \nVols 1-5, Proceedings, 1385-1388, 2006. \n [5]   Ahrendt, P., Larsen, J. & Goutte, C. “Co-occurrence \nmodels in music genre classification”. 2005 IEEE \nWorkshop on Machine Learning for Signal \nProcessing (MLSP) , 247-252, 2005.  \n[6]   Bagci, U. & Erzin, E. “Boosting classifiers for music \ngenre classification”. Computer and Information \nSciences - Iscis 2005 , Proceedings, 3733, 575-584, \n2005. \n[7]   Bagci, U. & Erzin, E. “Inter genre similarity \nmodeling for automatic music genre classification”. \n2006 IEEE 14th Signal Processing and \nCommunications Applications , Vols 1 and 2, 639-\n642, 2006 \n[8]   Cataltepe, Z., Yaslan, Y. & Sonmez, A. “Music \ngenre classification using MIDI and audio features”, \nEurasip Journal on Advances in Signal Processing , \n2007. \n[9]   Cheng, H. T., Yang, Y. H., Lin, Y. C., Liao, I. B. & \nChen, H. H. “Automatic Chord Recognition for \nMusic Classification and Retrieval”.  Ieee \nInternational Conference on Multimedia and Expo,  \nVols 1-4, 1505-1508, 2008. \n[10]  T. Li, M. Ogihara and Q. Li, “A comparative study \non content-based music genre classification”, \nProceedings of the 26th annual international ACM \nSIGIR , Toronto, Canada, pp. 282-289, 2003. \n[11]  R. Neumayer and A Rauber, “Integration of text and \naudio features for genre classification in music \ninformation retrieval”, In Proceeding of 29th \nEuropean Conference on Information Retrieval, \nRome, Italy,  pp. 724-727, 2007. \n[12] Shen, J. L., Shepherd, J. & Ngu, A. H. H. “On \nefficient music genre classification” . Database \nSystems for Advanced Applications, Proceedings,  \n3453, 253-264. \n[13]  Mckay, C. & Fujinaga, I. “Musical genre \nclassification: Is it worth pursuing and how can it be \nimproved”. ISMIR 2006 . Victoria Canada. \n [14]  Lee, C. H., Shih, J. L., Yu, K. M. & Lin, H. S. \n“Automatic Music Genre Classification Based on \nModulation Spectral Analysis of Spectral and \nCepstral Features”. Ieee Transactions on \nMultimedia,  11, 670-682, 2009. \n[15] L.N. de Casto and J. Timmis, “ Artificial immune \nsystem: A new computational intelligence \napproach”, Great Britain, Springer,  pp. 76-79, \n2002.  [16] S. Doraisamy, S. Golzari, N. M. Norowi, M. N. \nSulaiman and N. I. Udzir, “A study on feature \nselection and classification techniques for automatic \ngenre classification of traditional Malay music”, in \nProceedings of Ninth International Conference on \nMusic Information Retrieval (ISMIR’08) , \nPhiladelphia, Pennsylvania USA, pp. 331-336, 2008. \n[17] D.N. Sotiropoulus, A.S. Lampropoulus and G.A. \nTsihrintzis, “Artificial immune system-based music \ngenre classification”, in New Directions in \nIntelligent Interactive Multimedia , pp. 191-200, \n2008. \n[18]  R.-B. Xiao, L. Wang and Y. Liu, “A framework of \nAIS based pattern classification and matching for \nengineering creative design”, in Proceedings of the \nFirst International Conference on Machine \nLearning and Cybernetics , Beijing, China, pp. \n1554-1558, 2002. \n[19]  S. Forrest, A.S. Perelson, L. Allen and R. Cherukuri, \n“Self-nonself discrimination in a computer”, in \nProceedings of 1994 IEEE Computer Society \nSymposium on Research in Security and Privacy , \nOakland, CA, USA, pp. 202-212, 1994. \n [20] C.N. Silla, A.L. Koerich and C.A.A. Kaestner,” The \nLatin Music Database“ Proceedings of Ninth \nInternational Conference on Music Information \nRetrieval (ISMIR’08) , Philadelphia, Pennsylvania \nUSA, 2008, pp. 331-336.  \n[21] C.N Silla, A.L Koerich ans C.A.A Kaesner. “A \nfeature selection approach for music genre \nclassification”. International Journal of Semantic \nComputing,  Vol 3, No 2, 183 -208, 2009. \n[22] G. Tzanetakis, P.Cook, “MARSYAS: A Framework \nfor Audio Analysis”. Organized Sound Journal.  Vol \n4 (3), 2000. \n[23] Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard \nPfahringer, Peter Reutemann, Ian H. Witten, “The \nWEKA Data Mining Software: An Update”; \nSIGKDD Explorations , Volume 11, Issue 1, 2009. \n374\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "Tunepal - Disseminating a Music Information Retrieval System to the Traditional Irish Music Community.",
        "author": [
            "Bryan Duggan",
            "Brendan O&apos;Shea"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1416312",
        "url": "https://doi.org/10.5281/zenodo.1416312",
        "ee": "https://zenodo.org/records/1416312/files/DugganO10.pdf",
        "abstract": "In this paper we present two new query-by-playing (QBP) music information retrieval (MIR) systems aimed at musicians playing traditional Irish dance music. Firstly, a browser hosted system - tunepal.org is pre- sented. Secondly, we present Tunepal for iPhone/iPod touch devices - a QBP system that can be used in situ in traditional music sessions. Both of these systems use a backend corpus of 13,290 tunes drawn from community sources and “standard” references. These systems have evolved from academic research to become popular tools used by musicians around the world. 16,064 queries have been logged since the systems were launched on 31 July, 2009 and 11 February, 2010 respectively to 18 May 2010. As we log data on every query made, including geocoding queries made on the iPhone, we propose that these tools may be used to follow trends in the playing of traditional music. We also present an analysis of the data we have collected on the usage of these systems.",
        "zenodo_id": 1416312,
        "dblp_key": "conf/ismir/DugganO10",
        "keywords": [
            "query-by-playing",
            "music information retrieval",
            "musicians",
            "traditional Irish dance music",
            "browser hosted system",
            "Tunepal for iPhone/iPod touch devices",
            "QBP system",
            "community sources",
            "standard references",
            "geocoding queries"
        ],
        "content": "TUNEPAL - DISSEMINATING A MUSIC INFORMATION RETRIEVAL \nS\nYSTEM TO THE TRADITIONAL IRISH MUSIC COMMUNITY \n \nB\nryan Duggan, Brendan O’ Shea \n \nSchool of Computing \nDublin Institute of Technology \nKevin St \nDublin 8 \nIreland \n{bryan.duggan, brendan.oshea}@dit.ie  \n \nABSTRACT \nIn this paper we present two new query-by-playing \n(QBP) music information retrieval (MIR) systems aimed \nat musicians playing traditional Irish dance music. \nFirstly, a browser hosted system - tunepal.org is pre-\nsented. Secondly, we present Tunepal for iPhone/iPod \ntouch devices - a QBP system that can be used in situ  in \ntraditional music sessions. Both of these systems use a \nbackend corpus of 13,290 tunes drawn from community \nsources and “standard” references. These systems have \nevolved from academic research to become popular tools \nused by musicians around the world. 16,064 queries have \nbeen logged since the systems were launched on 31 July, \n2009 and 11 February, 2010 respectively to 18 May \n2010. As we log data on every query made, including \ngeocoding queries made on the iPhone, we propose that \nthese tools may be used to follow trends in the playing of \ntraditional music. We also present an analysis of the data \nwe have collected on the usage of these systems. \n1. INTRODUCTION \nThere exist approximately seven thousand unique tradi-\ntional Irish dance tunes [1]. Musicians playing traditional \nmusic have a personal repertoire of up to a thousand \ntunes. Many of these tunes are known by multiple names, \nwhile many more are known simply as “gan anim” (with-\nout name). In the past, commercial recordings of tradi-\ntional music were accompanied by extensive sleeve notes \nproviding biographic information on the tunes in the re-\ncording. In the modern age two trends have emerged. \nFirstly, the use of digital audio formats and digital \ndownloading of music has meant that personal music col-\nlections do not contain this biographic data and many \nmusicians are unfamiliar with the history and background \nto the tunes they are playing. This fact is compounded by \nthe fact that although traditional tunes often have colour-\nful and memorable titles (Table 1), there is nothing to link \nthe title of a tune with its melody [2].  \nThe second trend is the development of extensive; \ncrowd sourced biographic references and discographies \nfor tunes on websites such as thesession.org [3]. Linking \nthe melodies of traditional Irish dance tunes to biographic \ndata about the tune, including names, is the goal of an \nongoing project at the DIT School of Computing. \n \n  \nName \nThe Bucks Of Oranmore \nCome West Along The Road \nRepeal Of The Union \nThe Chicken That Made The Soup \nMore Power To Your Elbow \nIf It's Sick You Are Tea You Wants \nThe Night We Made The Match \nLast Night’s Fun \nMy Former Wife \nThe First Night In America \n \nTable 1: Tune names taken from [4] \n \nIn our previous work [5-7], we described a proof of \nconcept music information retrieval (MIR) system \nadapted to the characteristics of traditional Irish dance \nmusic that addressed this very problem. In this paper, we \npresent follow up work in developing this research into \nrobust and reliable tools that are now being used by thou-\nsands of musicians around the world. Specifically we pre-\nsent tunepal.org – a browser hosted query-by-playing \n(QBP) system and Tunepal for the iPhone/iPod touch – a \nQBP system that can be used in situ  in traditional music \nsessions. As these systems log details of every query be-\ning made (including geotagging queries made on the \niPhone), they represent a unique opportunity to analyse \nthe zeitgeist  of traditional music. In other words, to iden-\ntify trends, popular tunes and tune types being played \naround the world. \n Section 2 of this paper presents a brief overview of \nour previous work in this area. Section 3 presents the ar-\nchitecture of tunepal.org. Section 4 presents Tunepal for \niPhone. Section 5 presents a summary of usage data col-\nlected from the two systems. Section 6 presents a sum-\nmary and conclusions.  \n2. RELATED WORK \nOur previous work describes Tunepal for Windows Mo-\nbile devices such as smartphones and PDA’s [8,9]. This \nis a symbolic MIR system that allows musicians to search \nfor tunes by name, retrieve the ABC notation [10] for the \ntune and playback the tune. Figure 1 presents screenshots \nof Tunepal running on a Windows Mobile smartphone. \nOur aim with this system was to facilitate musicians to \n583\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \nstart tunes that they could recall the name of, but not the \nmelody. \n \n \n \nFigure 1: Screenshot of Tunepal for Windows Mobile \n \nMATT2 (Machine Annotation of Traditional Tunes) \nis a standalone QBP MIR system for traditional Irish \ndance tunes initially developed for the tin-whistle and \nwooden flute [7] and subsequently enhanced to support \nqueries on a range of traditional instruments including the \nuilleann pipes, concertina and fiddle [5,11]. MATT2 is \nbased on two subsystems – a transcription subsystem and \na matching subsystem. The transcription subsystem uses \nan onset detection function based on comb filters (ODCF) \ndeveloped especially for the transcription of traditional \nmusic [12]. A harmonicity, pitch detection algorithm \nbased on Klapuri’s [13] multi-pitch estimator is used to \nextract frequencies from the FFT (Fast Fourier Trans-\nform) of a note frame. MATT2 incorporates Ornamenta-\ntion Filtering (OF) to remove expressiveness from the \ntranscription. The corpus used in MATT2 is Norbeck’s \nreel and jig collection [14], which is pre-processed to ex-\npand parts, separate variations, remove ornamentation \nand normalise for register. This collection contains 1582 \nreels and jigs, with variations. Matching is achieved using \nthe substring edit distance algorithm [15], with a cost \nfunction modified to take account of breath marks in the \ntranscription. An evaluation of this system is presented in \n[11]. \nAn enhancement to MATT2 is the TANSEY (Turn \nANnotation from SEts using SimilaritY profiles) algo-\nrithm, named after the traditional flute player Seamus \nTansey [6].  TANSEY is a segmentation algorithm to an-\nnotate tunes played in set (sequences of multiple tunes \nrepeated multiple times and played segue). TANSEY \nmakes use of melodic  similarity  profiles  and can retrieve the start and end of each repetition of a tune, count the \nrepetitions and retrieve the name and associated bio-\ngraphic data associated with each tune in a recording of a \nset of tunes. \n3. TUNEPAL.ORG \nOur first task in disseminating the work described in sec-\ntion 2 was to expand the corpus used in the experiments \ndescribed in [5,11] to include a comprehensive collection \nof traditional Irish music from definitive sources avail-\nable in ABC notation. The tunepal.org database contains \n13,290 tunes drawn from community sources, such as the \nwebsite thesession.org [3] and “standard” references in-\ncluding O’Neills Dance Music of Ireland [16] and Bren-\ndan Breathneach’s Ceol Rince Na hÉireann series in five \nvolumes [17]. Our corpus also includes collections of \nWelsh, Scottish and Breton music in addition to several \ndifferent transcriptions of the same tune from the canon \nof Irish traditional music. Table 2 presents an analysis of \nsources of the tunes in the tunepal.org corpus. \n \nSource Count \nthesession.org 9,310 \nHenrik Norbeck 1,474 \nO’Neills Dance Music of Ireland 994 \nCeol Rince na hÉireann 1 73 \nCeol Rince na hÉireann 2 192 \nCeol Rince na hÉireann 3 37 \nCeol Rince na hÉireann 4 220 \nJonny O’Leary 196 \nNigel Gatherer 794 \nTotal: 13,290 \n \nTable 2: Sources of Tunepal tunes \n \nIn order to make the system easily accessible to tradi-\ntional musicians without the necessity of installing soft-\nware, a browser hosted version of MATT2 – tunepal.org \nwas developed. For this version, the transcription algo-\nrithms were deployed in a Java applet, while the tune \ncorpus and matching subsystems were hosted on a server.  \nFigure 2 presents a screenshot of tunepal.org.  \nTo find a tune, a musician records a query played on \nan instrument such as the concert flute, tin-whistle, uil-\nleann pipes, accordion or concertina. An energy based \nFigure 2: A screenshot of tunepal.org \n584\n11th International Society for Music Information Retrieval Conference (ISMIR 2010) \n \nsilence detection algorithm removes silence at the start of \nrecorded queries, which would affect the \nthe quaver length (a core element of our \nuser can then click the transcribe button and the system \nwill extract the melody spelled in ABC notation from the \nrecording [10]. tunepal.org differs from similar web \nbased QBP systems such as Musipedia  \ntional instrument queries are explicitly supported. A\nthough Musipedia contains traditional Irish dance tunes \nas part of its corpus, it does not generate positive results \nwhen queries are played on the tin- whistle\nflute (as tested by the author).  \nUsers are also offered the ability to change the tra\nscription fundamental. This changes the frequencies used \nby the pitch spelling algorithm, so that \nwork with differently pitched instruments, such as Eb \nflutes and uilleann pipes pitched in B and \nThe query is then s ubmitted to the matchi\nJ2EE web application; hosted on tunepal.org\ning engine uses the substring edit distance algorithm \nagainst the corpus of search keys  - strings of musical \nnotes extracted from the tunes and normalised as d\nscribed in [5,11]. These are stored in a MySQL data\nFor each submitted query, tunepal.org  \nclosest matches in order of descending distance. MATT2 \ngives the correct tune as the closest match for 9\nqueries in experiments using real- world field recordings \nof traditional musicians from sessions, classes, concerts \nand commercial recordings including solo and ensemble \nplaying on traditional instruments recorded in a variety of \nreal-world settings such as noisy public  \ntunepal.org therefore we log the closest match\na query in the database. tunepal.org incorporates a fee\nback system, so users can however proof listen to the r\nsults and give feedback as to which (if any) of the r\nturned tunes was the correct one. We also store a conf\ndence score for the match calculated as per (1), where \nthe query length and ed is the minimum\ndistance between the query and the closest match\n \n/g1855/g34041/g3398 /g3436/g1857/g1856\n/g1869/g3440 \n \nEach tune in the database can be played, \nABC notation or stave notation. Stave notation display \nuses ABCJS, an open source, browser hosted rendering \nengine for ABC notation [19].  \ntunepal.org was launched on 31 July, 2009\nWindows, Mac and Linux systems. We \ntunepal.org on popular traditional music discussion f\nrums such as thesession.org and the chiff and fipple f\nrum. tunepal.org has been quite successful\nnow well known amongst traditional musicians having \nbeen profiled in a national newspaper [20]\nwriting (18 May 2010), 7,885 queries have been logged\nA more detailed analysis of the usage of \npresented in section 5. \nsilence at the start of \nrecorded queries, which would affect the evaluation of \nthe quaver length (a core element of our system). The \nuser can then click the transcribe button and the system \nin ABC notation from the \ntunepal.org differs from similar web \n [18] in that tradi-\ntional instrument queries are explicitly supported. A l-\ncontains traditional Irish dance tunes \nas part of its corpus, it does not generate positive results \nwhistle  or wooden \nUsers are also offered the ability to change the tra n-\nchanges the frequencies used \nby the pitch spelling algorithm, so that tunepal.org can \nwork with differently pitched instruments, such as Eb \nB and C. \nubmitted to the matchi ng engine, a \ntunepal.org . The match-\ning engine uses the substring edit distance algorithm \nstrings of musical \nnotes extracted from the tunes and normalised as d e-\nstored in a MySQL data base. \n presents the ten \nclosest matches in order of descending distance. MATT2 \ngives the correct tune as the closest match for 9 3% of \nworld field recordings \nof traditional musicians from sessions, classes, concerts \nsolo and ensemble \nplaying on traditional instruments recorded in a variety of \n sessions [11].  In \nthe closest match ing tune for \nincorporates a fee d-\nproof listen to the r e-\ngive feedback as to which (if any) of the r e-\nturned tunes was the correct one. We also store a conf i-\nper (1), where q is \nminimum  substring edit \ndistance between the query and the closest match  [6].  \n(1) \nEach tune in the database can be played, displayed in \nABC notation or stave notation. Stave notation display \nan open source, browser hosted rendering \n31 July, 2009 . It runs on \nWindows, Mac and Linux systems. We promoted \non popular traditional music discussion f o-\nrums such as thesession.org and the chiff and fipple f o-\nsuccessful  and the site is \nnow well known amongst traditional musicians having \n[20]. At the time of \nhave been logged . \nanalysis of the usage of tunepal.org is 4. TUNEPAL FOR IPHONE\nTraditional Irish music is most commonly played by \ngroups of musicians in a community setting known as a \nsession [21]. Sessions usually take place in shared public \nspaces. It was felt important therefore that for this work \nto become ubiquitous, it had to be made available on a \nmobile handheld device. We therefore p\ntionality of tunepal.org to the iPhone platform.\npresents screenshots of Tunepal running on an iPhone.\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n       \n  \nFigure 3 : Screenshots of Tunepal\niPhone \nCertain compromises were \nversion of Tunepal in order to make transcription speed \nacceptable. Firstly queries are limited to twelve seconds \nof audio (similar to Shazam [22]\nrate is reduced to 22.05 KHz and finally, onset detection is \nachieved using a combination of a\nFourier Transform) with a Han\nspeller instead of using ODCF\nthe signal using a frame size of 2\n50% overlap. This gives a frequency resolution of \n10.76Hz, discriminant enough to detect pitches of trad\ntional instruments without interpolation. Our \nbased, pitch detection algorithm \nintervals in the frequency spectrum \nuse in Tu nepal for iPhone. Identified frequencies \nassigned pitch classes using the pitch spelling algorithm. \nA note onset is annotated when the pitch class changes in \nthe time domain. The quaver length is determined using \nthe fuzzy histogram clustering algorithm described in \n[5,7,11] . Ornamentation notes are removed from the tra\nscription and long notes (crochets, dotted crochets) are \n \nTUNEPAL FOR IPHONE  \nmusic is most commonly played by \ngroups of musicians in a community setting known as a \n. Sessions usually take place in shared public \nspaces. It was felt important therefore that for this work \nto become ubiquitous, it had to be made available on a \nWe therefore p orted the func-\nto the iPhone platform.  Figure 3 \nscreenshots of Tunepal running on an iPhone.  \n       \n \n: Screenshots of Tunepal  running on an \nCertain compromises were necessary in the iPhone \nversion of Tunepal in order to make transcription speed \nacceptable. Firstly queries are limited to twelve seconds \n[22]). Secondly, the sample \nKHz and finally, onset detection is \nachieved using a combination of a n STFT (Short-time \nwith a Han ning window and a pitch \ninstead of using ODCF . An STFT is carried out on \nthe signal using a frame size of 2 ,048 samples, with a \nThis gives a frequency resolution of \nenough to detect pitches of trad i-\ntional instruments without interpolation. Our harmonicity \nbased, pitch detection algorithm [5] that analyses peak \nintervals in the frequency spectrum was ported to C++ for \nnepal for iPhone. Identified frequencies are then \npitch classes using the pitch spelling algorithm. \nA note onset is annotated when the pitch class changes in \nme domain. The quaver length is determined using \nthe fuzzy histogram clustering algorithm described in \n. Ornamentation notes are removed from the tra n-\nscription and long notes (crochets, dotted crochets) are \n585\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \nsplit into multiple quaver notes. The transcription string \n(a sequence of pitch classes) is then submitted to \ntunepal.org for matching.  \nTunepal for iPhone uses the same back end database \nand infrastructure as tunepal.org and so has access to a \ncorpus of 13,290 tunes. The iPhone version of Tunepal, \nreturns the top ten closest matching tunes for a query with \nconfidence scores. Similar to tunepal.org, we log each \nquery, with the closest matching tune and confidence \nscore. When a tune is matched both tunepal.org and \nTunepal for iPhone offer the option to link back to the \noriginal source of the ABC notation on the internet. In the \ncase of tunes indexed from the website thesession.org, \nthis often includes extensive discussions on the origin of \nthe tune, the source of transcription and recordings on \nwhich the tune appears (Figure 4). \n \n \n \nFigure 4: Biographic reference for the tune \"Kiss the \nMaid Behind the Barrel\" from the website thesession.org \ndisplayed on an iPhone \n \nRetrieved tunes are stored in a “My Tunes” tab on the \nuser’s device, in order of most recently tagged to facili-\ntate future retrieval for learning purposes. Playback is \nachieved using ABC2MIDI [23] and the FMOD audio \nengine [24]. The iPhone version of Tunepal has one ma-\njor advantage over tunepal.org and that is the ability for \naccurate geocoding (Figure 5).  \n \n \nFigure 5: Geotagged tunes displayed within Tunepal \non the iPhone \nTherefore with the users permission, we geotag each \nquery on the iPhone and store the longitude and latitude with each query in the tunepal.org database. This makes it \npossible for a user to track their queries on a map. \nTunepal for iPhone was released on 11 February, 2010 \nand at the time of writing (18 May 2010), 5,866 QBP \nqueries have been made, while 2,313 title searches were \nmade (title searches were added as a feature on 13 Febru-\nary 2010).  As the iPhone does not support programs \nwritten in Java, it was necessary to port the transcription \nsubsystem of MATT2 and tunepal.org to a combination \nof C++ and Objective C. Tunepal for iPhone was listed in \nthe top twenty cultural apps available on the iPhone by \nthe Sunday Times (an Irish national newspaper) [25]. \n5. RESULTS \nTo date (18 May 2010) tunepal.org and Tunepal for \niPhone have logged 16,064 queries since being released \n(Table 3).  \n \nClient QBP \ntunepal.org QBP 7,885 \niPhone QBP 5,866 \niPhone Title 2,313 \nTotal: 16,064 \n \nTable 3: Queries logged from tunepal.org and Tunepal for \niPhone  \n \nTable 4 gives the top ten tune types queried by users \nof tunepal.org and Tunepal for iPhone. The tunepal.org \ncount was generated by counting the user verified tunes \nfor each query. The iPhone count was generated by se-\nlecting the closest matching tune for each query.  \n \n tunepal.org  \n(Verified) iPhone  \n(QBP) \n# Type Count Type Count \n1 Reel 521 Reel 1,594 \n2 Jig 240 Jig 913 \n3 Hornpipe  68 Hornpipe  211 \n4 Polka 57 Polka 116 \n5 Slip Jig  28 Waltz 111 \n6 Slide  23 Slip Jig  89 \n7 Waltz 20 Slide 56 \n8 Double Jig  13 Barndance  46 \n9 Barndance  9 Double Jig  38 \n10 Strathspey  7 Strathspey  18 \n \nTable 4: Top ten tune types queried by users of \ntunepal.org and Tunepal for iPhone \n \nIn order to minimise the effect of false positives on \nthe iPhone counts, tunes with a confidence of < 65% are \nexcluded. The cut-off of 65% was derived by stochastic \nsampling and proof listening. While this undoubtedly re-\nmoves many true positives, it does eliminate most of the \n586\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \nfalse positives. The scores in Table 4 correspond broadly \nwith the profile of tunes in most traditional musicians’ \nrepertoire, where reels and jigs assume prominence [26]. \nWhile it would be interesting to analyse the frequency \nthat particular tunes appear in search results, more data is \nneeded to make this analysis significant as the profile of \ntune appearances is in fact mostly flat, with the majority \nof tunes appearing only once or twice and even the top \ntunes appearing less than twenty times.      \nTable 5 gives a breakdown of QBP queries submitted \nby day of the week, though as these are in the local time \nof the server (the server is hosted in Ireland), there will be \n“bleed” from day to day due to the different time zones of \nusers. Nevertheless, it is significant that weekends are \nmore popular than weekdays for uses of tunepal.org, \nplaying music being a leisure activity for many musi-\ncians. Tunepal for iPhone however demonstrates consis-\ntent usage across the week, which could be attributed to \nits portability. \n \n tunepal.org iPhone Total \nMon 999 793 1,792 \nTue 1,039 862 1,901 \nWed 985 728 1,713 \nThurs 860 957 1,817 \nFri 743 887 1,630 \nSat 1,773 744 2,517 \nSun 1,486 895 2,381 \nTotal: 7,885 5,866 1,3751 \n \nTable 5: Analysis of queries by day of the week \n \nFigure 6 better illustrates the trend towards high vo-\nlumes of usage over the weekend, with significant usage \non Monday and Tuesday, dropping off on Wednesday \nand Thursday to peak at the weekends. \n \n \n \nFigure 6: Plot of daily usage \n \nWe geotag queries generated by Tunepal for iPhone. \nAn extract of this plot is given in Figure 7.  \n  \n \nFigure 7: An extract from the worldwide geotagged \nquery map  \nThis is an optional feature that users must agree to; \nhowever 74% of queries made on an iPhone are geo-\ntagged. The realtime worldwide map of geotagged QBP \nqueries can be viewed on a google map at the website \nhttp://tunepal.org. \nTable 6 was generated by reverse geocoding the lon-\ngitude and latitude from tagged queries to generate a pro-\nfile of usage by country.  \n \nCountry Count \nIreland 1,276 \nUnited States 1,092 \nUnited Kingdom 393 \nGermany 179 \nCanada 122 \nSweden 91 \nSpain 89 \nFrance 73 \nNetherlands 44 \nAustralia 20 \n \n \nTable 6: Top ten countries for Tunepal for iPhone \nQBP queries \n \nAlthough the amount of data collected is insufficient \nto draw any firm conclusions, it is nonetheless interesting \nto observe that the United States and the United Kingdom \nare significant sources for queries, these being major cen-\nters for the Irish Diaspora. This is a correlation we hope \nto explore in more detail in future work. \n6. CONCLUSIONS AND FUTURE WORK \nIn this paper we presented two new QBP MIR systems \nfor traditional music that developed from academic re-\nsearch. These tools have become popular, being used by \nmusicians around the world to connect playing with tune \nnames and biographic data. To achieve this, we use a \ncorpus of 13,290 compositions collected by both the tra-\nditional music community and noted collectors such as \nO’Neill and Breathneach. Further we presented an analy-05\n0010001500200025003000\n1 2 3 4 5 6 7tunepal.org\niPhone\nTotal\n587\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \nsis of the data we have collected on the usage of these \nsystems since being launched.  \nIt is our aim to further disseminate these query-by-\nplaying systems to the traditional music community by \nmaking them available on a greater variety of platforms \nsuch as the iPad, Android, Symbian, Maemo and Win-\ndows Phone 7 platforms. Usage of Tunepal is growing as \nare our usage logs. Once sufficient data is collected we \nhope to be able to mine these to gather new insights into \nmusical trends and correlations that we hope to present in \nfuture work.  \n7. ACKNOWLEDGEMENTS \nWe are grateful for the support the School of Computing \nat the Dublin Institute of Technology who fund this work.  \n \n8. REFERENCES \n[1] S. Driscoll, “A Trio of Internet Stars: ABC's,” \nFiddler Magazine ,  vol. 11, Summer. 2004. \n[2] C. Carson, Last Night's Fun: A Book about Irish \nTraditional Music , North Point Press, 1997. \n[3] “The Session.” \n[4] F. O'Neill, The Music of Ireland , 1903. \n[5] B. Duggan, “Machine Annotation of Traditional \nIrish Dance Music,” Dublin Institute of Technolo-\ngy, 2009. \n[6] B. Duggan, M. Gainza, B. O'Shea, and P. Cun-\nningham, “Machine Annotation of Sets of Tradi-\ntional Irish Dance Tunes,” Ninth International \nConference on Music Information Retrieval \n(ISMIR), Drexel University, Philadelphia, USA , \nSep. 2008. \n[7] B. Duggan, B. O'Shea, and P. Cunningham, “A \nSystem for Automatically Annotating Traditional \nIrish Music Field Recordings,” Sixth International \nWorkshop on Content-Based Multimedia Indexing, \nQueen Mary University of London, UK , Jun. 2008. \n[8] B. Duggan, “Enabling Access to Irish Traditional \nMusic Archives on a PDA,” Eight Annual Irish \nEducational Technology Users Conference, DIT \nBolton St, Ireland , May. 2007. \n[9] B. Duggan, “Learning Traditional Irish Music us-\ning a PDA,” IADIS Mobile Learning Conference, \nTrinity College Dublin , Jul. 2006. \n[10] C. Walshaw, “The ABC home page” Available: \nhttp://www.walshaw.plus.com/abc/. \n[11] B. Duggan, M. Gainza, B. O'Shea, and P. Cun-\nningham, “Compensating for Expressiveness in \nQueries to a Content Based Music Information Re-\ntrieval System,” 2009 International Computer Mu-\nsic Conference , Aug. 2009. \n[12] M. Gainza, E. Coyle, and B. Lawler, “Onset Detec-\ntion Using Comb Filters,” IEEE Workshop on Ap-\nplications of Signal Processing to Audio and \nAcoustics ,  New Paltz, NY: 2005. \n[13] A. Klapuri, “Multiple fundamental frequency esti-\nmation based on harmonicity and spectral smooth-ness,” Speech and Audio Processing, IEEE Trans-\nactions on ,  vol. 11, 2003, pp. 804-816. \n[14] H. Norbeck, “ABC Tunes” Available: \nhttp://www.norbeck.nu/abc/index.html, \n[15] G. Navarro and M. Raffinot, Flexible Pattern \nMatching in Strings: Practical On-Line Search Al-\ngorithms for Texts and Biological Sequences , \nCambridge University Press, 2002. \n[16] F. O'Neill, The Dance Music of Ireland – 1001 \nGems,  Chicago, USA: 1907. \n[17] B. Breathnach, “Ceol Rince na hÉireann Cuid V \n[Dance Music of Ireland] Vol V,” 1999. \n[18] L. Prechelt and R. Typke, “An interface for melody \ninput,” ACM Transactions on Computer-Human \nInteraction (TOCHI) ,  vol. 8, 2001, pp. 133-149. \n[19] G. Dyke and P. Rosen, “abcjs - Project Hosting on \nGoogle Code” Available: \nhttp://code.google.com/p/abcjs/. \n[20] S. Long, “Film body makes case against bord \nsnips,” The Irish Times , Aug. 2009. \n[21] H. O'Shea, “Getting to the Heart of the Music: \nIdealizing Musical Community and Irish Tradition-\nal Music Sessions,” Journal of the Society for Mu-\nsicology in Ireland ,  vol. 2, 2006, p. 1. \n[22] Shazam, “Shazam - The amazing music discovery \nengine. Join our Community” Available: \nhttp://www.shazam.com/music/portal. \n[23] S. Shlien, “The ABC Music project - abcMIDI” \nAvailable: http://abc.sourceforge.net/abcMIDI/. \n[24] “fmod - interactive audio middleware” Available: \nhttp://www.fmod.org/. \n[25] “The Arts World on iPhone,” The Sunday Times , \nMar. 2010. \n[26] F. Vallely, The Companion to Irish Traditional \nMusic, New York University Press, 1999. \n \n588\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "Universal Onset Detection with Bidirectional Long Short-Term Memory Neural Networks.",
        "author": [
            "Florian Eyben",
            "Sebastian Böck",
            "Björn W. Schuller",
            "Alex Graves"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1417131",
        "url": "https://doi.org/10.5281/zenodo.1417131",
        "ee": "https://zenodo.org/records/1417131/files/EybenBSG10.pdf",
        "abstract": "Many different onset detection methods have been pro- posed in recent years. However those that perform well tend to be highly specialised for certain types of music, while those that are more widely applicable give only mod- erate performance. In this paper we present a new onset detector with superior performance and temporal precision for all kinds of music, including complex music mixes. It is based on auditory spectral features and relative spectral differences processed by a bidirectional Long Short-Term Memory recurrent neural network, which acts as reduction function. The network is trained with a large database of onset data covering various genres and onset types. Due to the data driven nature, our approach does not require the onset detection method and its parameters to be tuned to a particular type of music. We compare results on the Bello onset data set and can conclude that our approach is on par with related results on the same set and outperforms them in most cases in terms of F1-measure. For complex music with mixed onset types, an absolute improvement of 3.6% is reported.",
        "zenodo_id": 1417131,
        "dblp_key": "conf/ismir/EybenBSG10",
        "keywords": [
            "onset detection methods",
            "specialized for certain types of music",
            "moderate performance",
            "bidirectional Long Short-Term Memory recurrent neural network",
            "auditory spectral features",
            "relative spectral differences",
            "data-driven approach",
            "F1-measure",
            "complex music mixes",
            "absolute improvement"
        ],
        "content": "UNIVERSAL ONSET DETECTION WITH BIDIRECTIONAL LONG\nSHORT-TERM MEMORY NEURAL NETWORKS\nFlorian Eyben, Sebastian B ¨ock, Bj ¨orn Schuller\nInstitute for Human-Machine Communication\nTechnische Universit ¨at M ¨unchen\neyben@tum.de, sb@minimoog.org, schuller@tum.deAlex Graves\nInstitute for Computer Science VI\nTechnische Universit ¨at M ¨unchen\ngraves@in.tum.de\nABSTRACT\nMany different onset detection methods have been pro-\nposed in recent years. However those that perform welltend to be highly specialised for certain types of music,while those that are more widely applicable give only mod-erate performance. In this paper we present a new onsetdetector with superior performance and temporal precisionfor all kinds of music, including complex music mixes. Itis based on auditory spectral features and relative spectraldifferences processed by a bidirectional Long Short-TermMemory recurrent neural network, which acts as reductionfunction. The network is trained with a large database ofonset data covering various genres and onset types. Due tothe data driven nature, our approach does not require theonset detection method and its parameters to be tuned to aparticular type of music. We compare results on the Belloonset data set and can conclude that our approach is on parwith related results on the same set and outperforms themin most cases in terms of F\n1-measure. For complex music\nwith mixed onset types, an absolute improvement of 3.6%is reported.\n1. INTRODUCTION\nFinding onset locations is a key part of segmenting andtranscribing music, and therefore forms the basis for manyhigh level automatic retrieval tasks. An onset marks thebeginning of an acoustic event. In contrast to music infor-mation retrieval studies which focus on beat and tempo de-tection via the analysis of periodicities (e. g. [7, 9]), an on-set detector faces the challenge of detecting single events,which need not follow a periodic pattern. Recent onset de-tection methods (e. g. [5, 16, 17]) have matured to a levelwhere reasonable robustness is obtained for polyphonicmusic. However, the methods are specialised or tuned tospeciﬁc kinds of onsets (e. g. pitched or percussive) andlack the ability to perform well for music with mixed onsettypes. Thus, multiple methods need to be combined or amethod has to be selected depending on the type of onsets\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies arenot made or distributed for proﬁt or commercial advantage and that copiesbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2010 International Society for Music Information Retrieval.to be analysed.\nIn this paper we propose a novel, robust approach to\nonset detection, which can be applied to any type of music.\nOur approach is based on auditory spectral features andLong Short-Term Memory (LSTM) [13] recurrent neuralnetworks. The approach is purely data driven, and as wewill see, yields a very high temporal precision as well asdetection accuracy.\nThe rest of this paper is structured as follows. A brief\noverview of the state of the art in onset detection is given inSection 2, and Section 3 provides an introduction to LSTMneural networks. Section 5 describes the Bello onset dataset [2] as well as introducing a new data set. Experimentalresults for both data sets are provided in Section 6, alongwith a comparison to related systems.\n2. EXISTING METHODS\nMost onset detection algorithms are based on the three stepmodel shown in Figure 1. Some methods include a prepro-cessing step. The aim of preprocessing is to emphasiserelevant parts of the signal. Next, a reduction function isapplied, to obtain the detection function. This is the corecomponent of an onset detector. Some of the most com-mon reduction functions found in the literature are sum-marised later in this section.\nReduction Peak detection Signal Onsets Preprocessing\nFigure 1. Traditional onset detection workﬂow\nThe last stage is to extract the onsets from the detec-\ntion function. This step can be subdivided into post pro-cessing (e. g. smoothing and normalising of the detectionfunction), thresholding, and peak picking. If ﬁxed thresh-olds are used, the methods tend to pick either too many on-sets in louder parts, or miss onsets in quieter parts. Hence,adaptive thresholds are often used. Finally the local max-ima above the threshold, which correspond to the detectedonsets, are identiﬁed by a peak picking algorithm.\nEarly reduction functions, such as [14], operated in the\ntime domain. This approach normalises the loudness of thesignal before splitting it into multiple bands via bandpass\n589\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)ﬁlters. Onsets are then detected in each band as peaks in\nthe ﬁrst order difference of the logarithm of the amplitudeenvelope. These band-wise onsets are then combined toyield the ﬁnal set of detected onsets. More recent systemsemploy spectral domain reduction functions. We describethe most common ones in the following paragraphs.\n2.1 Spectral domain reduction functions\nSince onsets are often masked in the time domain by higher\nenergy signals, many reduction functions operate on a spec-tral representation of the audio signal. The methods listedbelow are all based on a short-time Fourier transform (STFT)of the signal.\n2.1.1 High Frequency Content\nPercussive sounds have a high energy in the upper fre-\nquency bands. This is exploited by weighting each STFTbin with a factor proportional to its frequency. Summingall weighted bins yields a measure called the high frequencycontent (HFC), which is used as a detection function. Al-though this method works well for percussive onsets, itshows weaknesses for other onset types [2].\n2.1.2 Spectral difference\nFor computation of the spectral difference function (SD),\nthe difference of two consecutive short-time spectra is com-puted bin by bin. All positive differences are then summedup across all bins. Some approaches use the L\n2-norm [2]\nfor calculating the difference, whereas others use the L1-\nnorm [5], in which case the function is referred to as spec-tral ﬂux (SF). Onset detection methods based on these meth-ods are among the best overall performers so far.\n2.1.3 Phase deviation\nThe methods mentioned so far rely on the spectral magni-\ntudes. In [2] a method utilising phase information is de-scribed. The change of the phase in a STFT frequencybin is a rough estimate of its instantaneous frequency. Achange of this frequency is an indicator of a possible onset.To reduce the chance of a missed onset due to phase wraparound, the mean phase change over all frequency bins isused. Dixon proposes an improvement to the phase devi-ation (PD) detection function called normalised weightedphase deviation (NWPD) [5], where each frequency bin’scontribution to the phase deviation function is weighted byits magnitude. The result is normalised by the sum of themagnitudes.\n2.1.4 Complex Domain\nAnother way to incorporate both magnitude and the phase\ninformation is proposed in [6]. First, the expected ampli-tude and phase is calculated for the current frame basedon the two previous frames, assuming constant amplitudeand phase change rate. The sum of the magnitude of thecomplex differences between the actual values for each fre-quency bin and the estimated values is then computed andused as a detection function. A variant of this method iscalled the rectiﬁed complex domain (RCD) [5]. Observingthat increases of the signal amplitude are generally morerelevant than decreases for onset detection, RCD modiﬁesthe original algorithm by only summing over positive am-plitude changes.\n2.2 Probabilistic reduction functions\nAn alternative approach is to base the description of sig-\nnals on probabilistic models. The negative log-likelihoodmethod [1] deﬁnes two different statistical models and ob-serves whether the signal follows the ﬁrst or the secondmodel. A sudden change from the ﬁrst model to the secondcan be an indication of an onset. This method shows goodresults for music with soft onsets, e. g. non-percussive sounds[2].\n2.3 Pitch-based onset detection techniques\nCollins describes an onset detection function based on a\npitch detector front-end [4]. Zhou presented a combinationof pitch and energy based detection functions [17]. In prin-ciple pitch-based onset detection is based on identiﬁcationof discontinuities and perturbations in the pitch contour,which are assumed to be indicators of onsets.\n2.4 Data-driven reduction functions\nTo build general detection functions, which are capable of\ndetecting onsets in a wider range of audio signals, clas-siﬁer based methods emerged. In [15] an onset detectionalgorithm based on a feed forward neural network, namelya convolutional neural network, is described. This systemperformed best in the MIREX 2005 audio onset detectionevaluation.\n3. NEURAL NETWORKS\nMotivated by the high performance of the onset detectionmethod of Lacoste and Eck, we investigate a novel artiﬁcialneural network (ANN) based approach. Instead of a simplefeed forward neural network we use a bidirectional recur-rent neural network with Long Short-Term Memory [13]hidden units. Such networks were proven to work wellon other audio detection tasks, such as speech recogni-tion [10].\nThis section gives a short introduction to ANN with a\nfocus on bidirectional Long Short-Term Memory (BLSTM)networks, which are used for the proposed onset detector.\n3.1 Feed forward neural networks\nThe most commonly used form of feed forward neural net-\nworks (FNN) is the multilayer perceptron (MLP). It con-sists of a minimum of three layers, one input layer, oneor more hidden layers, and an output layer. All connec-tions feed forward from one layer to the next without anybackward connections. MLPs classify all input frames in-dependently. If the context a frame is presented in is rel-evant, this context must be explicitly fed to the network,e. g. by using a ﬁxed width sliding window, as in [15].\n590\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)3.2 Recurrent neural networks\nAnother technique for introducing past context to neural\nnetworks is to add backward (cyclic) connections to FNNs.The resulting network is called a recurrent neural network(RNN). RNNs can theoretically map from the entire his-tory of previous inputs to each output. The recurrent con-nections form a kind of memory, which allows input valuesto persist in the hidden layer(s) and inﬂuence the networkoutput in the future. If future context is also necessary re-quired, a delay between the input values and the outputtargets can be introduced.\n3.3 Bidirectional recurrent neural networks\nA more elegant incorporation future context is provided by\nbidirectional recurrent networks (BRNNs). Two separatehidden layers are used instead of one, both connected to thesame input and output layers. The ﬁrst processes the inputsequence forwards and the second backwards. The net-work therefore has always access to the complete past andthe future context in a symmetrical way, without bloatingthe input layer size or displacing the input values from thecorresponding output targets. The disadvantage of BRNNsis that they must have the complete input sequence at handbefore it can be processed.\n3.4 Long Short-Term Memory\nAlthough BRNNs have access to both past and future in-\nformation, the range of context is limited to a few framesdue to the vanishing gradient problem [11]. The inﬂuenceof an input value decays or blows up exponentially overtime, as it cycles through the network with its recurrentconnections and gets dominated by new input values.\nForget\nGate\nOutput\nGateInput\nInput\nGate•\n••\n1.0\nOutputMemory\nCell\nFigure 2. An LSTM block with one memory cell\nTo overcome this deﬁciency, a method called Long Short-\nTerm Memory (LSTM) was introduced in [13]. In an LSTMhidden layer, the nonlinear units are replaced by LSTMmemory blocks (Figure 2). Each block contains one ormore self connected linear memory cells and three multi-plicative gates. The internal state of the cell is maintainedwith a recurrent connection of constant weight 1.0. This\nconnection enables the cell to store information over longperiods of time. The content of the memory cell is con-trolled by the multiplicative input, output, and forget gates,which – in computer memory terminology – correspondto write, read, and reset operations. More details on thetraining algorithm employed, and the bidirectional LSTMarchitecture in general can be found in [10].\n4. PROPOSED APPROACH\nThis section describes our novel approach for onset de-tection in music signals, which is based on bidirectionalLong Short-Term Memory (BLSTM) recurrent neural net-works. In contrast to previous approaches it is able tomodel the context an onset occurs in. The properties ofan onset and the amount of relevant context are therebylearned from the data set used for training. The audio datais transformed to the frequency domain via two parallelSTFTs with different window sizes. The obtained mag-nitude spectra and their ﬁrst order differences are used asinputs to the BLSTM network, which produces an onsetactivation function at its output. Figure 3 shows this basicsignal ﬂow. The individual blocks are described in moredetail in the following sections.\nSTFT & \nDifference\nSTFT & \nDifferenceBLSTM \nNetworkPeak\ndetectionSignal Onsets\nFigure 3. Basic signal ﬂow of the new neural networkbased onset detector\n4.1 Feature extraction\nAs input, the raw PCM audio signal with a sampling rate of\nf\ns=4 4.1kHz is used. To reduce the computational com-\nplexity, stereo signals are converted to a monaural signalby averaging both channels. The discrete input audio sig-nalx(t)is segmented into overlapping frames of Wsam-\nples length (W = 1024 andW= 2048, see Section 4.2),\nwhich are sampled at a rate of one per 10 ms (onset an-notations are available on a frame level). A Hammingwindow is applied to these frames. Applying the STFTyields the complex spectrogram X(n, k), with nbeing the\nframe index, and kthe frequency bin index. The com-\nplex spectrogram is converted to the power spectrogramS(n, k)=|X(n, k)|\n2.\nThe dimensionality of the spectra is reduced by apply-\ning psychoacoustic knowledge: a conversion to the Mel-frequency scale is performed with openSMILE [8]. A ﬁl-terbank with 40triangular ﬁlters, which are equidistant on\nthe Mel scale, is used to transform the spectrogram S(n, k)\nto the Mel spectrogram M(n, m). To match human per-\nception of loudness, a logarithmic representation is cho-sen:\nM\nlog(n, m)= log(M(n, m)+1 .0) (1)\n591\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)The positive ﬁrst order difference D+(n, m) is calcu-\nlated by applying a half-wave rectiﬁer function H(x)=\nx+|x|\n2to the difference of two consecutive Mel spectra:\nD+(n, m)= H(M log(n, m) −Mlog(n−1,m)) (2)\n4.2 Neural Network stage\nAs a neural network, an RNN with BLSTM units is used.\nAs inputs to the neural network, two log Mel-spectrogramsM\n23\nlog(n, m) andM46\nlog(n, m) (computed with window sizes\nof 23.2 ms and 46.4 ms (W = 1024 andW= 2048 sam-\nples), respectively) and their corresponding positive ﬁrstorder differences D\n+\n23s(n, m) andD+\n46s(n, m) are applied,\nresulting in 160 input units. The network has three hiddenlayers for each direction (6 layers in total) with 20 LSTMunits each. The output layer has two units, whose outputsare normalised to both lie between 0 and 1, and to sumto 1, using the softmax function. The normalised outputsrepresent the probabilities for the classes ‘onset’ and ‘noonset’. This allows the use of the cross entropy error crite-rion to train the network [10]. Alternative networks with asingle output, where a value of 1 represents an onset frameand a value of 0 a non-onset frame, which are trained us-ing the mean squared output error as criterion, were not assuccessful.\n4.2.1 Network training\nFor network training, supervised learning with early stop-\nping is used. Each audio sequence is presented frame byframe (in correct temporal order) to the network. Stan-dard gradient descent with backpropagation of the outputerrors is used to iteratively update the network weights.To prevent over-ﬁtting, the performance (cross entropy er-ror, cf. [10]) on a separate validation set is evaluated af-ter each training iteration (epoch). If no improvement ofthis performance over 20 epochs is observed, the trainingis stopped and the network with the best performance onthe validation set is used as the ﬁnal network. The gradi-ent descent algorithm requires the network weights to beinitialised with non zero values. We initialise the weightswith a random Gaussian distribution with mean 0 and stan-dard deviation 0.1. The training data, as well as validationand test sets are described in Section 5.\n4.3 Peak detection stage\nA network obtained after training as described in the previ-\nous section is able to classify each frame into two classes:‘onset’ and ‘no onset’. The standard method of choosingthe output node with the highest activation to determinethe frame class has not proven effective. Hence, only theoutput activation of the ‘onset’ class is used. Thresholdingand peak detection is applied to it, which is described inthe following sections:\n4.3.1 Thresholding\nOne problem with existing magnitude based reduction func-\ntions (cf. Section 2) is that the amplitude of the detectionFigure 4. Top: log Mel-spectrogram with ground truth on-\nsets (vertical dashed lines). Bottom: network output with\ndetected onsets (marked by dots), ground truth onsets (dot-ted vertical lines), and threshold θ(horizontal dashed line).\n4 s excerpt from ‘Basement Jaxx - Rendez-Vu’.\nfunction depends on the amplitude of the signal or the mag-\nnitude of its short time spectrum. Thus, to successfullydeal with high dynamic ranges, adaptive thresholds mustbe used when thresholding the detection function prior topeak picking. Similar to phase based reduction functions,the output activation function of the BLSTM network isnot affected by input amplitude variations, since its valuerepresents a probability of observing an onset rather thanrepresenting onset strength. In order to obtain optimal clas-siﬁcation for each song, a ﬁxed threshold θis computed\nper song proportional to the median of the activation func-tion (frames n=1...N ), constrained to the range from\nθ\nmin=0.1toθmax=0.3:\nθ∗=λ·median{a o(1),...,a o(N)} (3)\nθ= min (max(0.1,θ∗),0.3) (4)\nwithao(n)being the output activation function of the\nBLSTM neural network for the onset class, and the scalingfactor λchosen to maximise the F\n1-measure on the valida-\ntion set. The ﬁnal onset function oo(n)contains only the\nactivation values greater than this threshold:\noo(n)=/braceleftBigg\nao(n) forao(n)>θ\n0 otherwise(5)\n4.3.2 Peak picking\nThe onsets are represented by the local maxima of the on-\nset detection function oo(n). Thus, using a standard peak\nsearch, the ﬁnal onset function o(n) is given by:\no(n)=/braceleftBigg\n1foroo(n−1)≤oo(n)≥oo(n+1 )\n0otherwise(6)\n592\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)5. DATA SETS\nWe evaluate our onset detector using the data set intro-\nduced by Bello in [2], which consists of 23 sound excerptswith lengths ranging from a few seconds to one minute(cf. Table 1). the data set is divided into four categories:pitched percussive (PP), pitched non-percussive (PNP),non-pitched percussive (NPP ), and complex music mixes\n(MIX ). The set includes audio synthesised from MIDI\nﬁles as well as original recordings.\nIn order to effectively train the BLSTM network, the\nonset annotations had to be corrected in a few places: miss-ing onsets were added and onsets in polyphonic pieceswere properly aligned to match the annotation precision ofthe MIDI based samples. For rule-based onset detectionapproaches, minor inaccuracies of a few frames are notcrucial since these are levelled out by the detection win-dow during evaluation. For the BLSTM network, however,it is necessary to have temporally precise data for train-ing. Nonetheless, the original, unmodiﬁed transcriptionsare used for evaluation, to ensure a fair comparison.\nTo increase the size of the training data set, 87 10 s ex-\ncerpts of ballroom dance style music (BRD\noin the ongo-\ning) from the ISMIR 2004 tempo induction contest1[9]\nwere included (cf. Table 1). A part of the annotation workwas done by Lacoste and Eck for their neural network ap-proach\n2. The remaining parts were manually labelled by\nan expert musician3. As with the Bello data set, all anno-\ntations have been revised for network training.\nSet # ﬁles # onsets min/max/mean length [s]\nBRD o 87 5474 10.0 / 10.0 / 10.0\nPNP 1 93 13.1 / 13.1 / 13.1\nPP 9 489 2.5 / 60.0 / 10.5\nNPP 6 212 1.4 / 8.3 / 4.3\nMIX 7 271 2.8 / 15.1 / 8.0\nTable 1. Statistics of the onset data sets.\nFor network training, the full set (BRD oand Bello set)\nis initially randomly split on the ﬁle level into eight dis-junctive folds. Next, in an 8-fold cross validation, resultsfor the full set are obtained. Thereby for each fold six sub-sets are used for training, one for validation, and one fortesting. Since the initial weights of the neural nets are ran-domly distributed, the 8-fold cross validation is repeated10 times (using the exact same folds) and the means of theoutput activation functions are used for the ﬁnal evaluation.\n6. RESULTS\nIn [2] and [5], an onset is reported as correct if it is detectedwithin a 100 ms window (±50 ms) around the annotatedground truth onset position. In [3] a smaller window of±25ms was used for percussive sounds. We therefore de-\ncided to report two results for each set, one using a 100 ms\n1http://mtg.upf.edu/ismir2004/contest/tempoContest/node5.html\n2http://w3.ift.ulaval.ca/˜allac88/dataset.tar.gz\n3Data available at: http://mir.minimoog.org/window ω100for comparison with results in [2] and [5],\nand the second using a 50ms window ω50. All results were\nobtained with a ﬁxed threshold scaling factor of λ=5 0 .\nTable 2 shows the results of our BLSTM network ap-\nproach for each set of onsets in comparison to six other on-set detection methods as reported in [2] and [5].The PNP\ndata set consists of 93 onsets from only one audio ﬁle ofstring sounds. As a consequence, the results are not as rep-resentative as the others, and can vary a lot, depending onthe used parameters, as shown by [5]. The number of on-sets of the PPset has changed from originally 489 (used\nin [2, 5]) to 482 now, due to modiﬁcations by its author.The new results are therefore slightly worse (up to max.1.4%) than the original results but can still compete.\nBRD o& Bello-set Precision Recall F1-measure\nBLSTM (ω 100) 0.945 0.925 0.935\nBLSTM (ω 50) 0.920 0.901 0.911\nBLSTM (comb, ω 100) 0.938 0.916 0.927\nBLSTM (comb, ω 50) 0.911 0.890 0.900\nTable 3. 8-fold cross validation results for BLSTM on thefull data set with 100 ms and 50 ms detection windows (ω ).\ncomb: all onsets within 30 ms combined.\nTable 3 shows the results obtained by cross validation\nfor the full data set. The ﬁrst two rows reﬂect the resultsobtained with the same settings as for the individual Bellosets. It has been shown that two onsets are perceived as oneif they are not more than 30 ms apart [12]. Hence we alsoreport results, where all onsets less than 30 ms apart havebeen combined to a single one. There are 6 605 onsets inthe original annotations and 5 861 after combining.\n6.1 Discussion\nThe results show that our algorithm can compete with, and\nin most cases outperform, a range of existing methods forall types of onsets. However, we must temper this conclu-sion by adding that we were not able to compare to the lat-est MIREX participants (e.g. [16]), since the MIREX testdata is not publicly available and the authors did not pub-lish results on the Bello data set. Perhaps the most excitingaspect of our approach is that it does not require adaptationto speciﬁc onset types to achieve good results. This is animportant step towards a universal onset detector.\nIf a detection window of only 50 ms is chosen our ap-\nproach even outperforms the reference algorithms in somecases. This shows the excellent temporal precision of theBLSTM onset detector. In our opinion the results givenfor a detection window of 50 ms with all onsets less than30 ms apart combined to a single one should be used in thefuture, as they better reﬂect the temporal precision of thealgorithm and the perception of the human ear.\n7. CONCLUSION\nWe have presented a novel onset detector based on BLSTM-RNN, which – on the Bello onset data set – achieves results\n593\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)PNP PP NPP MIX\nPRF PRF PRF PRF\nHFC [2] 0.844 0.817 0.830 0.947 0.941 0.944 1.000 0.967 0.983 0.888 0.845 0.866\nSD [2] 0.910 0.871 0.890 0.983 0.949 0.966 0.935 0.816 0.871 0.886 0.804 0.843\nNLL [2] 0.968 0.968 0.968 0.968 0.924 0.945 0.980 0.929 0.954 0.889 0.860 0.874\nSF [5] 0.938 0.968 0.952 0.981 0.988 0.984 0.959 0.975 0.967 0.882 0.882 0.882\nNWPD [5] 0.909 0.968 0.938 0.961 0.981 0.971 0.950 0.966 0.958 0.916 0.845 0.879\nRCD [5] 0.948 0.978 0.963 0.983 0.979 0.981 0.944 0.983 0.963 0.945 0.819 0.877\nBLSTM (ω 100) 0.968 0.968 0.968 0.987 0.987 0.987 0.991 0.995 0.993 0.941 0.897 0.918\nBLSTM (ω 50) 0.918 0.957 0.937 0.955 0.981 0.968 0.982 0.995 0.989 0.844 0.865 0.855\nTable 2. Results for the Bello data sets PNP, PP,NPP , andMIX . Precision (P), Recall (R), and F1-measure (F) (as\nused in [5]). BLSTM with 100 ms and 50 ms detection windows (ω ) in comparison to other approaches: high frequency\ncontent (HFC), spectral difference (SD), negative log-likelihood (NLL), spectral ﬂux (SF), normalised weighted phase\ndeviation (NWPD), and rectiﬁed complex domain (RCD).\non par with or better than existing results on the same data\n(wrt.F1-measure), regardless of onset type. We have also\nintroduced a new thoroughly annotated data set of onsetsin ballroom dance music.\nThe average improvement on the whole Bello data set,\nis 1.7% F\n1-measure absolute. The improvement was best\n(3.6% F1-measure, absolute) for complex music mixes, re-\nﬂecting the adaptivity of our method to different musicalgenres. Competitive results are obtained even if the detec-tion window is halved in size (50 ms instead of 100 ms).\nIn future work we will investigate whether the approach\nis suitable for identifying the onset type (e. g. instrumenttype, vocal, etc.) via detectors trained on respective data.\n8. ACKNOWLEDGMENT\nWe would like to thank Juan Bello, Alexandre Lacoste, andDouglas Eck for sharing their annotated onset data sets.\n9. REFERENCES\n[1] M. Basseville and I. V . Nikiforov. Detection of Abrupt\nChanges: Theory and Application. Prentice-Hall,1993.\n[2] J. Bello, L. Daudet, S. Abdallah, C. Duxbury,\nM. Davies, and M. Sandler. A tutorial on onset de-tection in music signals. IEEE Transactions on Speech\nand Audio Processing, 13(5):1035–1047, Sept. 2005.\n[3] N. Collins. A comparison of sound onset detection al-\ngorithms with emphasis on psychoacoustically moti-vated detection functions. In Proc. of the AES Conven-\ntion 118, pages 28–31, 2005.\n[4] N. Collins. Using a pitch detector for onset detection.\nInProc. of ISMIR, pages 100–106, 2005.\n[5] S. Dixon. Onset detection revisited. In Proc. of DAFx-\n06, Montreal, Canada, pages 133–137, Sept. 2006.\n[6] C. Duxbury, J. P. Bello, M. Davies, M. Sandler, and\nM. S. Complex domain onset detection for musical sig-nals. In Proc. DAFx-03 Workshop, 2003.[7] F. Eyben, B. Schuller, and G. Rigoll. Wearable as-\nsistance for the ballroom-dance hobbyist - holisticrhythm analysis and dance-style classiﬁcation. In Proc.\nof ICME 2007, pages 92–95. IEEE, July 2007.\n[8] F. Eyben, M. W ¨ollmer, and B. Schuller. openEAR - In-\ntroducing the Munich Open-Source Emotion and Af-fect Recognition Toolkit. In Proc. of ACII 2009, pages\n576–581. IEEE, September 2009.\n[9] F. Gouyon, A. Klapuri, S. Dixon, M. Alonso, G. Tzane-\ntakis, C. Uhle, and P. Cano. An experimental compari-son of audio tempo induction algorithms. IEEE Trans-\nactions on Audio, Speech, and Language Processing ,\n14(5):1832–1844, Sept. 2006.\n[10] A. Graves. Supervised Sequence Labelling with Re-\ncurrent Neural Networks. PhD thesis, Technische Uni-versit ¨at M ¨unchen. Munich, Germany. 2008.\n[11] S. Hochreiter, Y . Bengio, P. Frasconi and J. Schmid-\nhuber. Gradient ﬂow in recurrent nets: the difﬁculty oflearning long-term dependencies. A Field Guide to Dy-\nnamical Recurrent Neural Networks IEEE Press, 2001.\n[12] S. Handel. Listening: an introduction to the percep-\ntion of auditory events. MIT Press, 1989.\n[13] S. Hochreiter and J. Schmidhuber. Long short-term\nmemory. Neural Computing, 9(8):1735–1780, 1997.\n[14] A. Klapuri. Sound onset detection by applying psy-\nchoacoustic knowledge. In Proc. of ICASSP’99, vol. 6,\npages 3089–3092, 1999.\n[15] A. Lacoste and D. Eck. Onset detection with artiﬁcial\nneural networks. MIREX, 2005.\n[16] A. R ¨obel. Onset Detection By Means Of Transient\nPeak Classiﬁcation In Harmonic Bands. MIREX, 2009\n[17] R. Zhou and J. Reiss. Music onset detection combin-\ning energy-based and pitch-based approaches. MIREX,2007.\n594\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "Timbral Qualities of Semantic Structures of Music.",
        "author": [
            "Rafael Ferrer",
            "Tuomas Eerola"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1416484",
        "url": "https://doi.org/10.5281/zenodo.1416484",
        "ee": "https://zenodo.org/records/1416484/files/FerrerE10.pdf",
        "abstract": "The rapid expansion of social media in music has pro- vided the field with impressive datasets that offer insights into the semantic structures underlying everyday uses and classification of music. We hypothesize that the organiza- tion of these structures are rather directly linked with the ”qualia” of the music as sound. To explore the ways in which these structures are connected with the qualities of sounds, a semantic space was extracted from a large collec- tion of musical tags with latent semantic and cluster anal- ysis. The perceptual and musical properties of 19 clus- ters were investigated by a similarity rating task that used spliced musical excerpts representing each cluster. The re- sulting perceptual space denoting the clusters correlated high with selected acoustical features extracted from the stimuli. The first dimension related to the high-frequency energy content, the second to the regularity of the spec- trum, and the third to the fluctuations within the spectrum. These findings imply that meaningful organization of mu- sic may be derived from low-level descriptions of the ex- cerpts. Novel links with the functions of music embedded into the tagging information included within the social me- dia are proposed.",
        "zenodo_id": 1416484,
        "dblp_key": "conf/ismir/FerrerE10",
        "keywords": [
            "social media",
            "music",
            "datasets",
            "semantic structures",
            "qualia",
            "latent semantic analysis",
            "cluster analysis",
            "perceptual space",
            "acoustical features",
            "tagging information"
        ],
        "content": "TIMBRAL QUALITIES OF SEMANTIC STRUCTURES OF MUSIC\nRafael Ferrer and Tuomas Eerola\nFinnish Centre of Excellence in Interdisciplinary Music Research\nrafael.ferrer-flores@jyu.fi; tuomas.eerola@jyu.fi\nABSTRACT\nThe rapid expansion of social media in music has pro-\nvided the ﬁeld with impressive datasets that offer insights\ninto the semantic structures underlying everyday uses andclassiﬁcation of music. We hypothesize that the organiza-tion of these structures are rather directly linked with the”qualia” of the music as sound. To explore the ways inwhich these structures are connected with the qualities ofsounds, a semantic space was extracted from a large collec-tion of musical tags with latent semantic and cluster anal-ysis. The perceptual and musical properties of 19 clus-ters were investigated by a similarity rating task that usedspliced musical excerpts representing each cluster. The re-sulting perceptual space denoting the clusters correlatedhigh with selected acoustical features extracted from thestimuli. The ﬁrst dimension related to the high-frequencyenergy content, the second to the regularity of the spec-trum, and the third to the ﬂuctuations within the spectrum.These ﬁndings imply that meaningful organization of mu-sic may be derived from low-level descriptions of the ex-cerpts. Novel links with the functions of music embeddedinto the tagging information included within the social me-dia are proposed.\n1. INTRODUCTION\nAttempts to craft a bridge between acoustic features andthe subjective sensation they provoke [3] have usually startedwith concepts describing instrument sounds, using adjec-tives or bipolar scales (e.g., bright-dark, static-dynamic)and matching these with acoustic descriptors (such as shapeof the envelope and energy distribution) [11, 20].\nIn this study, we present a purely bottom-up approach\nto the conceptual mapping between sound qualities andemerging meanings. We utilized social media to obtain awide sample of music and extract an underlying semanticstructure of this sample. Next, we evaluated the validityof the obtained mapping by investigating the acoustic fea-tures underlying the semantic structures. This was doneby an analyzing of the examples representing the semanticspace, and by having participants to rate the similarity of\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies arenot made or distributed for proﬁt or commercial advantage and that copiesbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2010 International Society for Music Information Retrieval.random spliced sound examples representing the semantic\nspace.\nSocial tagging is an activity, where descriptive verbal\ncharacterizations are given to items of interest, such assongs, images, or links as a part of the normal use of thepopular online services. Tags can be considered as se-mantic representations of abstract concepts created essen-tially for mnemonic purposes and used typically to orga-nize items [14]. Tagging music is not a novel idea, as anylabeling scheme such as musical genres may be consideredas tags themselves, but in recent years in the context of so-cial networks, tagging has acquired a new relevance andmeaning [1].\nDespite all the possibilities offered by large databases\ncontaining tags, a central problem remains on how to de-rive an ontology from them [19]. Starting with the assump-tion of an underlying structure existing in an apparentlyunstructured set, we consider a sample of tags to extract asemantic structure, explained next.\n2. ANALYSIS OF TAGS\n2.1 Material\nA collection of 6372 songs [7] representing 15 musical\ngenres (Alternative, Folk, Finnish Iskelm ¨a, Pop, World,\nBlues, Gospel, Jazz, Rock, Classical, Heavy, Soul, Elec-tronic, Hip-Hop, Soundtrack) served as the initial databaseof music. Musical genres were used in establishing thesample in order to maximize musical variety in the collec-tion and to be compatible with a host of music preferencestudies (e.g., [6, 22]) that have provided lists of 13 to 15broad musical genres relevant for most Western adult lis-teners. The tags related to the songs in this collection wereretrieved from an online music service ( last.fm\n1) with a\ndedicated API (Application programming interface) namedPylast\n2.\n2.2 Description of the corpus\nThe retrieved corpus consists of 5,825 lists of tags (mean\nlength of 62.27 tags), each list (document in this context)\nis associated with a piece of music. The number of times\neach tag had been used in the system until the time ofthe retrieval was also obtained, representing a measure of“popularity”.\n1http://www.last.fm\n2http://code.google.com/p/pylast/\n571\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)In total, the corpus contains 362,732 tags, from which\n77,537 are distinct. Each tag is formed by one or more\nwords (M=2.48, SD=1.86), a small proportion of the dis-tinct tags in the corpus contain long expressions (e.g. 6%of the distinct tags are formed by 5 words or more). Inthis study a tag is considered as a unit representing an ele-ment of the vocabulary, disregarding the number of words\nthat compose it. Treating tags as collocations (i.e. frequent\njuxtaposition of words) shifts the focus from data process-ing to concept processing [2], also allowing the tags tofunction as conceptual expressions [23] instead of wordsor phrases.\n2.3 Lexical layers of the vocabulary\nPreprocessing is necessary in any text mining application\nbecause retrieved data does not follow any particular set ofrules, and there are not standard steps to follow [13].\nThree ﬁltering rules where applied to the corpus in the\nquantitative domain. First, hapax legomena (i.e. tags used\nonly once in the corpus), are removed under the rationaleof discarding unrelated data. To capture the most prevalentand relevant tags, a second ﬁlter uses the associated popu-larity measure of each tag to eliminate the tags below themean popularity index of the vocabulary. The third stepeliminates tags with three or more words to prune shortsentence-like descriptions from the corpus. The subset re-sulting from such reductions represents 46.6% of the cor-pus (N=169,052, V ocabulary=2,029 tags).\nAt this point, data has been de-noised but for the ex-\ntraction of a meaningful semantic ontology from the tags,a semantic analysis and qualitative ﬁltering is necessary.To categorize the tags at a functional level [24] (e.g. musi-cological and lexicological), an analysis was performed byusing the Brown Corpus [9] as parts-of-speech (POS) tag-ger, Wordnet database [8] for word sense disambiguation,and Urban Dictionary online\n3and Last.fm database for\ngeneral reference. Tags are looked-up in these sources andthe selection of a category is decided by reviewing eachcase. The criteria applied in this process favors categoriesclosely related to music, such as genre, artist, instrument,form and company, then adjectives, and ﬁnally other types.For instance, “Acid” is a noun but it is also a term exten-sively used to describe certain musical genres, so it wasclassiﬁed according to its musical function. Proposed cat-egories, percentage of the vocabulary, deﬁnition and exam-ples are shown in Table 1. The resulting layers were usedto make a ﬁner discrimination of the tags to uncover the se-mantic structure. Since one of the main motivations of thisproject was to obtain prototypical timbral descriptions, wefocused on tags related to adjectives, nouns, instruments,temporal and verbs.\n2.4 Semantic structure\nTag structure (or folksonomy) is obtained by using latent\nsemantic analysis (LSA) as a framework [5], a method\nthat has been used before in the domain of musical tags\n3http://www.urbandictionary.com[17, 18]. In this study, detection of semantic structure has\nthree stages: 1) construction of a Term-Document Matrix,\n2) calculation of similarity coefﬁcients, and 3) cluster anal-ysis. First, a Term-Document Matrix X={x\nij}is con-\nstructed. Where each song i, corresponds to a “Document”\nand each unique tag (or item of the vocabulary) j,t oa\n“Term”. The result is a binary matrix X(0,1)containing\ninformation about the presence or absence of a particulartag to describe a given song. Second, a similarity matrixn×nDwith elements d\nijwheredii=0 for alli, is cre-\nated by computing similarity indexes between tag vectorsx\ni∗jofXwith:\ndij=ad/radicalbig\n(a+b)(a +c)(d+b)(d +c)(1)\nwhere ais the number of (1,1) matches, bfor (1,0), cfor\n(0,1) and dfor (0,0).\nThere are several methods to compute similarity coef-\nﬁcients between binary vectors (c.f., [10]). This coefﬁ-cient was selected because of its symmetric quality, which\nconsiders the double absence (0,0) as important as (1,1),that presumably has positive impact on ecologic applica-tions [10]. A hierarchical clustering algorithm was used totransform the similarity matrix into a sequence of nestedpartitions. The method used in the hierarchical clusteringwas Ward’s minimum variance, to ﬁnd compact, spheri-cal clusters [21] and because it has demonstrated its proﬁ-ciency in comparison to other methods [12].\nAfter obtaining a hierarchical structure, the clusters are\nderived from the resulting dendrogram by “pruning” thebranches with an algorithm that uses a partitioning aroundmedioids (PAM) clustering method in combination withthe height of the branches [15]. Figure 1 shows a two di-mensional projection (obtained with multidimensional scal-ing) of the similarity matrix used in the hierarchical clus-tering. Each dot represents a tag, and the numbers showthe centers of their corresponding clusters. Each numberis enclosed in a circle that shows the relative size of thecluster in terms of the number of tags contained in it. Amore detailed reference on the content of the clusters canbe consulted in Table 2.\n2.5 Ranking of musical examples in the clusters\nIn order to explore any acoustic or musical aspects of the\nclusters, we need to link the clusters with the speciﬁc songsrepresented by the tags. For this, a m×nTerm Docu-\nment Matrix (TDM) X={x\nij}is constructed, where lists\nof tags attributed to a particular song are represented asm, and preselected tags as n. A list of tags is a ﬁnite set\n{1,...,k}, where 1≤k≤96. Each element of the matrix\ncontains a value of the normalized rank of a tag if found ona list, and it is deﬁned by:\nx\nij=/parenleftBigrk\nk/parenrightBig−1\n(2)\nWhererkis the cardinal rank of the tag jif found in i, and\nkis the total length of the list. To obtain a cluster proﬁle,\n572\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Categories % Deﬁnition Examples\nGenre 36.72% Musical genre or style Rock, Alternative, Pop\nAdjective 12.17% General category of adjectives Beautiful, Mellow, Awesome\nNoun 9.41% General category of nouns Love, Melancholy, Memories\nArtist 8.67% Artists or group names Coldplay, Radiohead, Queen\nLocale 8.03% Geographic situation or locality British, American, Finnish\nPersonal 6.80% Words used to manage personal collections Seen Live, Favourites, My Radio\nInstrument 4.83% Sound source Female vocalists, Piano, Guitar\nUnknown 3.79% Unclassiﬁable gibberish aitch, prda, <3\nTemporal 2.41% Temporal circumstance 80’s, 2000, Late Romantic\nForm 2.22% Musical form or compositional technique Ballad, Cover, Fusion\nCompany 1.72% Record label, radio station, etc. Motown, Guitar Hero, Disney\nV erb 1.63% General category of verbs Chillout, Relax, Wake up\nContent 1.03% Emphasis in the message or literary content Political, Great lyrics, Love song\nExpression 0.54% Exclamations Wow, Y eah, lol\nTable 1. Main categories of tags, their prevalence, deﬁnition and examples.\nFigure 1. 19 clusters obtained with hierarchical clustering\nand hybrid pruning.\nmean rank of the tag across the TDM is calculated with:\n¯rj=/summationtextm\ni=1xij\nm(3)\nThus the cluster proﬁle or mean ranks vector is deﬁned\nas:\npl=¯rj∈Cl (4)\nCldenotes a given cluster lfor1≤l≤19(optimal num-\nber of clusters for this dataset), and pis a vector {5,...,k},\nwhere 5≤k≤334.\nLast step aims to obtain ranked lists of songs ordered\nin terms of its closeness to each cluster proﬁle. This is\ncarried out by calculating the euclidean distance betweeneach song rank vector x\ni,j∈Cland the cluster proﬁle pl:\ndi=/radicalBigg/summationdisplay\nj∈Cl(xij−pl)2 (5)\nThe examples of the results can be seen in Table 2, where\ntop artists of each cluster are displayed below central tagsof the cluster.3. EXPERIMENT\nIn order to explore whether the obtained clusters are per-ceptually meaningful and to further understand what kindsof acoustic and musical attributes they consist of, empiricaldata unrelated to the existing structures about the clustersis needed. A similarity rating experiment was designed toassess the timbral qualities of songs pertaining to each ofthe clusters. We chose to emphasize the low-level, non-structural qualities of music since we wanted to minimizethe confounding factors caused by recognition of songs,artists and the subsequent associations with these as wellas the lyrical contents of the music. To this end, the stim-uli for the experiment consisted of semi-randomly spliced,brief excerpts, explained in detail below.\n3.1 Experiment details\n3.1.1 StimuliInitially, 5-second audio samples were taken from a ran-\ndom middle part (25% after the beginning and 25% beforethe end) of the 25 top ranked songs (see ranking proce-dure in section 2.5) from each cluster. For each sample,the temporal position of notes onsets were estimated basedonspectral ﬂux using MIRToolbox [16]. The highest onset\nwas selected as a reference point from which slices of ran-dom length (150ms ≤t≤250ms ) were taken from 10ms\nbefore the peak onset of each sample, then equalized inloudness, and ﬁnally mixed together using a fade in-out of50ms with an overlap window of 100ms This resulted in\n19 excerpts (each representing a cluster) of variable length,that were ﬁnally trimmed to 1750ms , with a fade in-out of\n100ms To prepare these 19 excerpts for a similarity rating,\nthe 171 paired combinations were mixed with a silence of600ms. between them.\n3.1.2 Participants\n12 females and 9 males (age M=26.8, SD=4.15) partici-\npated to the experiment. 9 of them possessed least oneyear of musical training. 12 reported listening to musicattentively between one and 10 hours per week.\n573\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Cluster ID T ags proximate to cluster centroids Top artists in the cluster\n1 Energetic, Female vocal, Powerful, Hot, Sex Amy Adams, Fred Astaire, Kelly Clarkson\n2 Dreamy, Chill out, Haunting, Sleep, Moody Nick Drake, Radiohead, Massive Attack\n3 Sardonic, Sarcastic, Cynical, Humorous, Funny Alabama 3, Y ann Tiersen, Tom Waits\n4 Awesome, Amazing, Male vocalist, Loved, Great Guns N’ Roses, U2, Metallica\n5 Composer , Cello, Piano, Cello rock, Violin Camille Saint-Sa ¨ens, Tarja Turunen, Franz Schubert\n6 Female vocalist, Female vocalists, Female, 00s, Sexy Fergie, Lily Allen, Amy Winehouse\n7 Mellow, Beautiful, Chillout, Chill, Sad Katie Melua, Phil Collins, Coldplay\n8 Hard, Angry, Loud, Aggressive, Rock out System of a Down, Black Sabbath, Metallica\n9 60s, 70s, Guitar virtuoso, Sixties, Guitar solo Simon & Garfunkel, Janis Joplin, The Four Tops\n10 Feelgood, Summer , Feel good, Cheerful, Gute laune Mika, Goo Goo Dolls, Shekinah Glory Ministry\n11 Autumnal, Wistful, Intimate, Sophisticated, Reﬂective Soulsavers, Feist, Leonard Cohen\n12 High school, 90’s, 1990s, 1995, 1996 Fool’s Garden, The Cardigans, No Doubt\n13 50s, Saxophone, Trumpet, Tenor sax, Sax Miles Davis, Thelonious Monk, Charles Mingus\n14 1980s, 80’s, Eighties, 80er , V oci maschili Ray Parker Jr., Alphaville, Michael Jackson\n15 Afﬁrming, Lyricism, Life song, V ocalization Lisa Stansﬁeld, KT Tunstall, Katie Melua\n16 Choral, A capella, Acapella, Choir , A cappella Mediæval Bæbes, Alison Krauss, Blackmore’s Night\n17 V oce femminile, Femmina, V oci femminili, Femmine Avril Lavigne, The Cranberries, Diana Krall\n18 Tangy, Coy, Sleek, Attitude, Flirty Kylie Minogue, Ace of Base, Solange\n19 Rousing, Exuberant, Conﬁdent, Playful, Passionate James Brown, Does It Offend Y ou, Y eah?, Tchaikovsky\nTable 2. Most representative tags and typical artists of each of the 19 clusters.\n3.1.3 Procedure\nParticipants were presented with pairs of sound excerpts in\nrandom order using a computer interface and high-qualityheadphones. Their task was to rate the similarity of soundson a 9-level Likert scale, whose extremes were labeled asdissimilar and similar. Before the actual experimental tri-\nals, they were given instructions and practice trials to fa-miliarize themselves with the task.\n3.1.4 Audio features\nTo explore the acoustic and musical features underlying\nthe perceptual similarities of the clusters, 41 audio features(listed on Table 3) were extracted from each spliced stim-uli using MIR toolbox [16]. The choice of features wasrestricted to those which would be applicable to splicedexamples and would not require high-level feature analysissuch as structural repetition or tonality. The extraction wascarried out using frame-based approach with 50ms analy-\nsis frame using 50% overlap.\n3.2 Results\nHighly consistent pattern of similarities between the 21\nparticipants were obtained (Cronbach α=0.94). For this\nreason, a mean similarity matrix of the individual ratingswas subjected to metric multidimensional scaling (MDS)analysis based on stress minimization by means of ma-jorization (SMACOF) [4]. This yielded adequate low -dimensional projections of the data, from which we fo-cus on 2 - dimensional (stress=0.065) and 3 - dimensional(stress=0.027) solutions.\nThe organization of the clusters (represented with sliced\nsamples) illustrates a clear organization in terms of the se-mantic qualities of the clusters (see Figure 2), showing theAwesome and Hard examples on the left uppermost corner,\nand the semantically distant, Autumnal and Dreamy in the\nlower right-hand corner.\nTo investigate the perceived organization of the seman-\ntic clusters in terms of the acoustic qualities, the 3 dimen-sions were correlated with the extracted audio features.Category No. Feature\nDynamics 1-2 RMS energy\n3-4 Attack time (M, SD)\nRhythm 5-6 Fluctuation peak pos. (M, SD)\n7 Fluctuation centroid (M, SD)\nPitch 8-9 Pitch (M, SD)\n10-11 Chromagram (unwr.) centr. (M, SD)\nHarmony 12 Entropy (oct. collap. spectr.) (M)\n13 Roughness (M)\n14 Inharmonicity (M, SD)\nTimbre 15-16 Brightness (cut-off 110 Hz) (M, SD)\n17-18 Spectral centroid (M, SD)\n19-20 Zerocross (M, SD)\n20-21 Spread (M)\n22 Spectral entropy (M)\n23 Spectral ﬂux (M)\n24 Flatness (M)\n25 Kurtosis (M)\n26-27 Regularity (M, SD)\n28-29 1st MFCC (M, SD)\n......\n30-41 7th MFCC (M, SD)\nTable 3. List of extracted audio features (M= mean, SD=\nstandard deviation)\nHighly signiﬁcant correlations, top ﬁve shown in Table 4,\nwere observed for dimensions 1 and 2. We may interpretthese correlations in terms of the qualities of the soundspectrum: The ﬁrst dimension is related to the distributionof energy along the frequency (spectral centroid, ﬂatness,brightness, MFCC1, etc.), where the items in the MDS so-lution are arranged from the high-frequency energy contentin the left to the prevalence of low-frequency energy con-tent in the right. The second dimension may be interpretedas the periodic organization of the spectrum, i.e., whetherthe spectrum is harmonic (roughness,skewness, spread andﬂuctuation centroid). The clusters represented by the itemsin the lower part of the MDS solution possess clearer orga-nization of the spectrum in comparison with the the itemshigh on the MDS solution. The third dimension seem to berelated the temporal ﬂuctuation of the spectrum (MFCC6[SD], Fluctuation position [M], MFCC22 [M]).\n574\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Dimension 1 Dimension 2 Dimension 3\nAcoustic feature r Acoustic feature r Acoustic feature r\nMFCC 1 (M) 0.94 *** Fluctuation centroid (M) -0.72 *** MFCC 6 (SD) 0.51 *\nFlatness (M) -0.86 *** Roughness (M) 0.68 ** Fluctuation position (M) -0.50 *\nCentroid (M) -0.83 *** Skewness (M) 0.67 ** MFCC 2 (M) -0.46 *\nBrightness (M) -0.81 *** Spread (M) -0.65 ** Fluctuation peak (M) 0.45\nSpectral entropy (M) -0.80 *** Kurtosis (M) 0.57 * Irregularity (SD) 0.44\n∗∗∗ =p<. 001, ∗∗=p<. 01,∗=p<. 05\nTable 4. Correlations between the dimensions of the multidimensional scaling solution and acoustic descriptors.\n/.notdef.g00010.5 0.0 0.5/.notdef.g00010.5 0.0 0.5\nDim 1Dim 2Energetic, Powerful1\nDreamy, Chill out2Sardonic, Funny3 Awesome, Male vocalist4Composer, Cello5\nFemale vocalist, Sexy6Mellow, Sad7Hard, Aggresive8\n60's, Guitar virtuoso9Feelgood, Summer10\nAutumnal, Wistful11High school, 90's1250's, Saxophone1380's, V oci maschili14\nAffirming, Lyricism15Choral, A capella16\nV oce femminile, Femmina17Tangy, Coy18\nRousing, Exhuberant19\nFigure 2. Dimensions 1 and 2 of the MDS with be-\nhavioural responses and associated tags\n3.3 Discussion\nIn sum, when brief and spliced excerpts taken from the\nclusters representing semantic structures of the music de-scriptions are presented to listeners, they are able to formcoherent distances between them. An acoustic analysis ofthe excerpts was used to label the dimensions embeddedin the cluster similarities. This analysis showed clear cor-relations between the dimensional and timbral qualities ofmusic. However, it should be emphasized that the high rel-evance of many timbral features is only natural since thetimbral characteristics of the excerpts were preserved andstructural aspects were masked by the semi-random splic-ing.\nWe are careful in not taking these early results to mean\nliterally that the semantic structure of the initial samplewould be explainable by means of the same timbral fea-tures. This is of course another question which is easilyempirically approached using feature extraction of the typ-ical examples representing each cluster and either classifythe clusters based on features, or predict the coordinates ofthe clusters within a low dimensional space by means ofregression using a larger set of acoustic features (includ-ing those that are relevant for full excerpts such as tonalityand structure). However, we are positively surprised at thelevel of coherence from the part of the listener ratings andtheir explanations in terms of the acoustic features despitethe limitations we imposed on the setting (i.e. discardingtags connected with musical genres), splicing and having alarge number of clusters to test. Our intention is to followthis analysis with more rigorous selection of acoustic fea-tures (PCA and other data reduction techniques) and usemultiple regression to assess whether linear combinationsof the features would be necessary for explaining the per-ceptual dimensions.\n4. CONCLUSIONS\nThe present work provided a bottom-up approach to se-mantic qualities of music descriptions, which capitalizedsocial media, natural language processing, similarity rat-ings and acoustic analysis. Semantic structures of musicdescriptions have been extracted from the social media pre-viously [18] but the main difference here was the carefulﬁltering of such data. We used natural language process-ing to focus on categories of tags that are meaningful butdo not afford immediate categorization of music in a waythat, for example, musical genre does.\nAlthough considerable effort was spent on ﬁnding the\noptimal way of teasing out reliable and robust structuresof the tag occurrences using cluster analysis, several othertechniques and parameters within clustering could also havebeen employed. We realize that other techniques wouldprobably have led to different structures but it is an openempirical question whether the connections between thesimilarities of the tested items and their acoustic featureswould have been entirely different. A natural continua-tion of the current study would be to predict the typicalexamples of the clusters with the acoustic features by us-ing either classiﬁcation algorithms or mapping of the clus-ter locations within a low dimensional space using corre-lation and multiple regression. However, the issue at stakehere was the connection of timbral qualities with semanticstructures.\nThe implications of the present ﬁndings are related to\nseveral open issues. The ﬁrst one is the question whetherstructural aspects of music are required in explaining thesemantic structures or whether the low-level, timbral char-acteristics are sufﬁcient, as was indicated by the presentﬁndings. Secondly, what new semantic layers (as indicatedby categories of tags) can be meaningfully connected withthe acoustic properties of the music? Finally, if the timbral\n575\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)characteristics are indeed strongly connected with such se-\nmantic layers as adjectives, nouns and verbs, do these arise\nby means of learning and associations, or are the underly-ing regularities connected with emotional, functional andgestural cues of the sounds?\n5. REFERENCES\n[1] J.J. Aucouturier and E. Pampalk. Introduction-from\ngenres to tags: A little epistemology of music informa-tion retrieval research. Journal of New Music Research,\n37(2):87–92, 2008.\n[2] J. Brank, M. Grobelnik, and D. Mladenic. Automatic\nevaluation of ontologies. In Anne Kao and StephenR.Poteet, editors, Natural Language Processing and\nText Mining. Springer, USA, 2007.\n[3] O. Celma and X. Serra. Foaﬁng the music: Bridging\nthe semantic gap in music recommendation. Web Se-\nmantics: Science, Services and Agents on the WorldWide Web, 6(4):250–256, 2008.\n[4] J. de Leeuw and P . Mair. Multidimensional scaling us-\ning majorization: SMACOF in R. Journal of Statistical\nSoftware, 31(3):1–30, 2009.\n[5] S. Deerwester, S.T. Dumais, G.W. Furnas, T.K. Lan-\ndauer, and R. Harshman. Indexing by latent semanticanalysis. Journal of the American society for informa-\ntion science, 41(6):391–407, 1990.\n[6] M.J. Delsing, T.F. ter Bogt, R.C. Engels, and W.H.\nMeeus. Adolescents music preferences and personal-ity characteristics. European Journal of Personality,\n22(2):109–130, 2008.\n[7] T. Eerola and R. Ferrer. Setting the standards: Norma-\ntive data on audio-based musical features for musicalgenres. In Proceedings of the 7th Triennial Conference\nof European Society for the Cognitive Sciences of Mu-sic, ESCOM, 2009.\n[8] Christiane Fellbaum, editor. WordNet: An electronic\nlexical database. Language, speech, and communica-\ntion. MIT Press, Cambridge, Mass, 1998.\n[9] W.N. Francis and H. Kucera. Brown corpus. A Stan-\ndard Corpus of Present-Day Edited American English,for use with Digital Computers. Department of Lin-guistics, Brown University, Providence, Rhode Island,USA, 1979.\n[10] J.C. Gower and P . Legendre. Metric and euclidean\nproperties of dissimilarity coefﬁcients. Journal of clas-\nsiﬁcation, 3(1):5–48, 1986.\n[11] J.M. Grey. Multidimensional perceptual scaling of mu-\nsical timbres. Journal of the Acoustical Society of\nAmerica, 61(5):1270–1277, 1977.\n[12] A.K. Jain and R.C. Dubes. Algorithms for clustering\ndata. Prentice Hall, Englewood Cliffs, NJ, 1988.[13] Anne Kao and Stephen R. Poteet, editors. Natural Lan-\nguage Processing and Text Mining. Springer V erlag,2006.\n[14] P . Lamere. Social tagging and music information re-\ntrieval. Journal of New Music Research, 37(2):101–\n114, 2008.\n[15] P . Langfelder, B. Zhang, and S. Horvath. dynamicTree-\nCut: Methods for detection of clusters in hierarchi-cal clustering dendrograms., 2009. R package version1.20.\n[16] O. Lartillot, P . Toiviainen, and T. Eerola. A matlab tool-\nbox for music information retrieval. Data Aalysis, Ma-\nchine Learning and Applications, pages 261–8, 2008.\n[17] C. Laurier, M. Sordo, J. Serra, and P . Herrera. Music\nmood representation from social tags. In Proceedings\nof the 10th International Society for Music InformationConference, Kobe, Japan, 2009.\n[18] M. Levy and M. Sandler. Learning latent semantic\nmodels for music from social tags. Journal of New Mu-\nsicResear\nch, 37(2):137–150, 2008.\n[19] H. Lin, J. Davis, and Y . Zhou. An integrated approach\nto extracting ontological structures from folksonomies.InProceedings of the 6th European Semantic Web\nConference on The Semantic Web: Research and Ap-plications, page 668. Springer, 2009.\n[20] S. McAdams, S. Winsberg, S. Donnadieu, G. De Soete,\nand J. Krimphoff. Perceptual scaling of synthesizedmusical timbres: Common dimensions, speciﬁcitiesand latent subject classes. Psychological Research,\n58(3):177–192, 1995.\n[21] R Development Core Team. R: A Language and En-\nvironment for Statistical Computing . R Foundation for\nStatistical Computing, Vienna, Austria, 2009. ISBN 3-900051-07-0.\n[22] P .J. Rentfrow and S.D. Gosling. Message in a ballad:\nthe role of music preferences in interpersonal percep-tion. Psychol Sci, 17(3):236–242, 2006.\n[23] J.M. Siskind. Learning word-to-meaning mappings.\nModels of language acquisition: inductive and deduc-tive approaches, pages 121–153, 2000.\n[24] B. Zhang, Q. Xiang, H. Lu, J. Shen, and\nY . Wang. Comprehensive query-dependent fusion us-ing regression-on-folksonomies: a case study of multi-modal music search. In Proceedings of the seventeen\nACM international conference on Multimedia, pages213–222. ACM, 2009.\n576\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "Combining Features Reduces Hubness in Audio Similarity.",
        "author": [
            "Arthur Flexer",
            "Dominik Schnitzer",
            "Martin Gasser",
            "Tim Pohle"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1416360",
        "url": "https://doi.org/10.5281/zenodo.1416360",
        "ee": "https://zenodo.org/records/1416360/files/FlexerSGP10.pdf",
        "abstract": "In audio based music similarity, a well known effect is the existence of hubs, i.e. songs which appear similar to many other songs without showing any meaningful per- ceptual similarity. We verify that this effect also exists in very large databases (> 250000 songs) and that it even gets worse with growing size of databases. By combining different aspects of audio similarity we are able to reduce the hub problem while at the same time maintaining a high overall quality of audio similarity.",
        "zenodo_id": 1416360,
        "dblp_key": "conf/ismir/FlexerSGP10",
        "keywords": [
            "audio based music similarity",
            "hubs",
            "very large databases",
            "perceptual similarity",
            "effect exists",
            "worsens with size",
            "different aspects of audio similarity",
            "reduce the hub problem",
            "maintain high overall quality",
            "audio similarity"
        ],
        "content": "COMBINING FEATURES REDUCES HUBNESS IN AUDIO SIMILARITY\nArthur Flexer,1Dominik Schnitzer,1;2Martin Gasser,1Tim Pohle2\n1Austrian Research Institute for Artiﬁcial Intelligence (OFAI), Vienna, Austria\n2Department of Computational Perception\nJohannes Kepler University Linz, Austria\narthur.flexer@ofai.at, dominik.schnitzer@ofai.at\nmartin.gasser@ofai.at, tim.pohle@jku.at\nABSTRACT\nIn audio based music similarity, a well known effect is\nthe existence of hubs, i.e. songs which appear similar to\nmany other songs without showing any meaningful per-\nceptual similarity. We verify that this effect also exists in\nvery large databases (> 250000 songs) and that it even\ngets worse with growing size of databases. By combining\ndifferent aspects of audio similarity we are able to reduce\nthe hub problem while at the same time maintaining a high\noverall quality of audio similarity.\n1. INTRODUCTION\nOne of the central goals in music information retrieval\nis the computation of audio similarity. Proper modeling\nof audio similarity enables a whole range of applications:\ngenre classiﬁcation, play list generation, music recommen-\ndation, etc. The de facto standard approach to computa-\ntion of audio similarity is timbre similarity based on para-\nmeterization of audio using Mel Frequency Cepstrum Co-\nefﬁcients (MFCCs) plus Gaussian mixtures as statistical\nmodeling (see Section 3.1). However, it is also an es-\ntablished fact that this approach suffers from the so-called\nhub problem [3]: songs which are, according to the audio\nsimilarity function, similar to very many other songs with-\nout showing any meaningful perceptual similarity to them.\nThe hub problem of course interferes with all applications\nof audio similarity: hub songs keep appearing unwontedly\noften in recommendation lists and play lists, they degrade\ngenre classiﬁcation performance, etc.\nAlthough the phenomenon of hubs is not yet fully un-\nderstood, a number of results already exist. Aucouturier\nand Pachet [1] established that hubs are distributed along\na scale-free distribution, i.e. non-hub songs are extremely\ncommon and large hubs are extremely rare. This is true\nfor MFCCs modelled with different kinds of Gaussian\nmixtures as well as Hidden Markov Models, irrespective\nwhether parametric Kullback-Leibler divergence or non-\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2010 International Society for Music Information Retrieval.parametric histograms plus Euclidean distances are used\nfor computation of similarity. But is also true that hubness\nis not the property of a song per se since non-parametric\nand parametric approaches produce very different hubs. It\nhas also been noted that audio recorded from urban sound-\nscapes, different from polyphonic music, does not produce\nhubs [2] since its spectral content seems to be more ho-\nmogeneous and therefore probably easier to model. Di-\nrect interference with the Gaussian models during or after\nlearning has also been tried (e.g. homogenization of model\nvariances) although with mixed results. Whereas some au-\nthors report an increase in hubness [1], others observed the\nopposite [5]. Using a Hierarchical Dirichlet Process in-\nstead of Gaussians for modeling MFCCs seems to avoid\nthe hub problem altogether [6].\nOur contribution to the understanding of the hub prob-\nlem is threefold: (i) since all results on the hub problem so\nfar were achieved on rather small data sets (from \u0018100\nto\u001815000 songs), we ﬁrst establish that the problem also\nexists in very large data sets (> 250000 songs); (ii) we\nshow that a non-timbre based parameterization is not prone\nto hubness; (iii) ﬁnally we show how combining timbre\nbased audio similarity with other aspects of audio similar-\nity is able to reduce the hub problem while maintaining a\nhigh overall quality of audio similarity.\n2. DATA\n2.1 Web shop data\nFor our experiments we used a data set D(ALL) ofSW=\n254398 song excerpts (30 seconds) from a popular web\nshop selling music. The freely available preview song ex-\ncerpts were obtained with an automated web-crawl. All\nmeta information (artist name, album title, song title, gen-\nres) is parsed automatically from the hmtl-code. The ex-\ncerpts are from U= 18386 albums from A= 1700 artists.\nFrom the 280 existing different hierarchical genres, only\ntheGW= 22 general ones on top of the hierarchy are\nbeing kept for further analysis (e.g. “Pop/General” is kept\nbut not “Pop/V ocal Pop”). The names of the genres plus\npercentages of songs belonging to each of the genres are\ngiven in Table 1. Please note that every song is allowed\nto belong to more than one genre, hence the percentages\nin Table 1 add up to more than 100%. The genre informa-\ntion is identical for all songs on an album. The numbers of\n171\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)genre labels per albums range from 1 to 8. Our database\nwas set up so that every artist contributes between 6 to 29\nalbums.\nTo study the inﬂuence of the size of the database on re-\nsults, we created random non-overlapping splits of the en-\ntire data set: D(1=2) - two data sets with mean number of\nsong excerpts = 127199,D(1=20) - twenty data sets with\nmean number of songs excerpts = 12719:9, D(1=100) -\none hundred data sets with mean number of songs excerpts\n= 2543:98. An artist with all their albums is always a\nmember of a single data set.\nPop Classical Broadway\n49.79 12.89 7.45\nSoundtracks Christian/Gospel New Age\n1.00 10.20 2.48\nMiscellaneous Opera/V ocal Alternative Rock\n6.11 3.24 27.13\nRock Rap/Hip-Hop R&B\n51.78 0.98 4.26\nHard Rock/Metal Classic Rock Country\n15.85 15.95 4.07\nJazz Children’s Music International\n6.98 7.78 9.69\nLatin Music Folk Dance & DJ\n0.54 11.18 5.24\nBlues\n11.24\nTable 1. Percentages of songs belonging to the 22 genres\nwith multiple membership allowed for the web shop data.\n2.2 Music portal data\nWe also used a smaller data base comprised of the mu-\nsic of an Austrian music portal. The FM4 Soundpark is\nan internet platform1of the Austrian public radio station\nFM4. This internet platform allows artists to present their\nmusic free of any cost in the WWW. All interested par-\nties can download this music free of any charge. This mu-\nsic collection contains about 10000 songs and is organized\nin a rather coarse genre taxonomy. The artists themselves\nchoose which of the GM= 6genre labels “Hip Hop, Reg-\ngae, Funk, Electronic, Pop and Rock” best describe their\nmusic. The artists are allowed to choose one or two of the\ngenre labels. We use a data base of SM= 7665 songs for\nour experiments. Number of songs and percentages across\ngenres are given in Table 2. Please note that every song is\nallowed to belong to more than one genre, hence the per-\ncentages in Table 2 add up to more than 100%.\n1http://fm4.orf.at/soundparkHiHo Regg Funk Elec Pop Rock\n15.34 4.64 21.87 46.25 34.39 44.03\nTable 2. Percentages of songs belonging to genres with\nmultiple membership allowed for the music portal data.\nGenres are Hip Hop, Reggae, Funk, Electronic, Pop and\nRock.\n3. METHODS\nWe compare two approaches based on different parame-\nterizations of the data. Whereas Mel Frequency Cepstrum\nCoefﬁcients (MFCCs) are a quite direct representation of\nthe spectral information of a signal and therefore of the\nspeciﬁc “sound” or “timbre” of a song, Fluctuation Pat-\nterns (FPs) are a more abstract kind of feature describing\nthe amplitude modulation of the loudness per frequency\nband.\n3.1 Mel Frequency Cepstrum Coefﬁcients and Single\nGaussians (G1)\nWe use the following approach to compute music similar-\nity based on spectral similarity. For a given music collec-\ntion of songs, it consists of the following steps:\n1. for each song, compute MFCCs for short overlap-\nping frames\n2. train a single Gaussian (G1) to model each of the\nsongs\n3. compute a distance matrix MG1between all songs\nusing the symmetrized Kullback-Leibler divergence\nbetween respective G1 models\nFor the web shop data the 30 seconds song excerpts in\nmp3-format are recomputed to 22050Hz mono audio sig-\nnals. For the music portal data, the two minutes from the\ncenter of each song are recomputed to 22050Hz mono au-\ndio signals. We divide the raw audio data into overlapping\nframes of short duration and use Mel Frequency Cepstrum\nCoefﬁcients (MFCC) to represent the spectrum of each\nframe. MFCCs are a perceptually meaningful and spec-\ntrally smoothed representation of audio signals. MFCCs\nare now a standard technique for computation of spec-\ntral similarity in music analysis (see e.g. [7]). The frame\nsize for computation of MFCCs for our experiments was\n46:4ms (1024 samples), the hop size 23:2ms (512 sam-\nples). We used the ﬁrst d= 25 MFCCs for all experiments\nwith the web shop data and the ﬁrst d= 20 MFCCs for all\nexperiments with the music portal data.\nA single Gaussian (G1) with full covariance represents\nthe MFCCs of each song [8]. For two single Gaussians,\np(x) =N(x;\u0016p;\u0006p)andq(x) =N(x;\u0016q;\u0006q), the\nclosed form of the Kullback-Leibler divergence is deﬁned\nas [14]:\n172\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)KLN(pkq) =1\n2\u0012\nlog\u0012det (\u0006 p)\ndet (\u0006 q)\u0013\n+Tr\u0000\n\u0006\u00001\np\u0006q\u0001\n+ (\u0016 p\u0000\u0016q)0\u0006\u00001\np(\u0016q\u0000\u0016p)\u0000d\u0013 (1)\nwhereTr(M)denotes the trace of the matrix M,\nTr(M) = \u0006 i=1::nmi;i. The divergence is symmetrized\nby computing:\nKLsym=KLN(pkq) +KLN(qkp)\n2(2)\n3.2 Fluctuation Patterns and Euclidean Distance (FP)\nFluctuation Patterns (FP) [9] [12] describe the amplitude\nmodulation of the loudness per frequency band and are\nbased on ideas developed in [4]. For a given music\ncollection of songs, computation of music similarity based\non FPs consists of the following steps:\n1. for each song, compute a Fluctuation Pattern (FP)\n2. compute a distance matrix MFPbetween all songs\nusing the Euclidean distance of the FP patterns\nClosely following the implementation outlined in [10],\nan FP is computed by: (i) cutting an MFCC spectrogram\ninto three second segments, (ii) using an FFT to com-\npute amplitude modulation frequencies of loudness (range\n0\u000010Hz ) for each segment and frequency band, (iii)\nweighting the modulation frequencies based on a model of\nperceived ﬂuctuation strength, (iv) applying ﬁlters to em-\nphasize certain patterns and smooth the result. The result-\ning FP is a 12 (frequency bands according to 12 critical\nbands of the Bark scale [15]) times 30 (modulation fre-\nquencies, ranging from 0 to 10Hz ) matrix for each song.\nThe distance between two FPs iandjis computed as the\nsquared Euclidean distance:\nD(FPi;FPj) =12X\nk=130X\nl=1(FPi\nk;l\u0000FPj\nk;l)2(3)\nFor the web shop data an FP pattern is computed from\nthe full 30 second song excerpts. For the music portal data\nan FP pattern is computed from the central minute of each\nsong.\n4. RESULTS\n4.1 Hubs in very large data bases\nAs a measure of the hubness of a given song we use the\nso-calledn-occurrence [1], i.e. the number of times the\nsongs occurs in the ﬁrst nnearest neighbors of all the\nother songs in the data base. Please note that the mean\nn-occurrence across all songs in a data base is equal to\nn. Anyn-occurrence signiﬁcantly bigger than ntherefore\nindicates existence of a hub. For every song in the datadata set n maxhub maxhub% hub3%\nD(ALL) 500 29588 11.63 7.75\nD(1/2) 250 12094 9.52 7.56\nD(1/20) 25 590 4.68 6.13\nD(1/100) 5 62 2.49 4.62\nTable 3. Hub analysis results for web shop data using\nmethod G1. See Section 4.1 for details.\ndata set n maxhub maxhub% hub3%\nD(ALL) 500 3386 1.33 1.18\nD(1/2) 250 1639 1.29 1.18\nD(1/20) 25 137 1.08 1.12\nD(1/100) 5 25 1.02 1.22\nTable 4. Hub analysis results for web shop data using\nmethod FP. See Section 4.1 for details.\nbasesD(ALL),D(1=2),D(1=20) andD(1=100) (see\nSection 2.1) we computed the ﬁrst nnearest neighbors for\nboth methods G1 and FP. For method G1, the ﬁrst nnearest\nneighbors are the nsongs with minimum Kullback Leibler\ndivergence (Equation 2) to the query song. For method\nFP, the ﬁrst nnearest neighbors are the songs with mini-\nmum Euclidean distance of the FP pattern (Equation 3) to\nthe query song. To compare results for data bases of dif-\nferent sizes SW, we keep the relation n=SWconstant at\n0:001965: e.g. for D(ALL)SW= 254398 andn= 500,\nforD(1=100)SW= 2543:98 and therefore n= 5.\nThe results given in Tables 3 and 4 show mean values\nover 100 (D (1=100)), 20 (D (1=20)), 2 (D (1=2)) data sets\nor the respective single result for the full data set D(ALL).\nWe give the number of nearest neighbors n, the absolute\nnumber of the maximum n-occurrence maxhub (i.e. the\nbiggest hub), the percentage of songs in whose nearest\nneighbor lists this biggest hub can be found maxhub% =\nmaxhub=S Wand the percentage of hubs hub3% (i.e. the\npercentage of songs of which the n-occurrence is more\nthan three times n).\nWhen looking at the results for method G1 (Table 3) it is\nclear that hubs do exist even for very large data bases. As a\nmatter of fact, the hub problem increases signiﬁcantly with\nthe size of the data base. Whereas for the small data sets\nD(1=100) on average the biggest hub is in the neighbor\nlists of 2:49% of all songs, the biggest hub for D(ALL)\nis a neighbor to 11:63% of all songs. The number of hubs\nincreases from an average 4:62% of all songs in D(1=100)\nto7:75% inD(ALL). To sum up, there are more and big-\nger hubs in larger data bases when using method G1 for\ncomputation of audio similarity.\nThe results for method FP in Table 4 show a quite dif-\nferent picture. The size of the biggest hub is much smaller\nand the number of hubs is also much reduced. There is\nalso very little inﬂuence of the size of the data bases on the\nresults. We like to conclude that method FP is not as prone\nto hubness as method G1.\n173\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)wG1wFP maxhub maxhub% hub3%hub10%hub15%hub20% acc\n1.0 0.0 879 11.47 8.05 0.94 0.40 0.22 48.47\n0.9 0.1 598 7.80 8.15 0.86 0.35 0.09 49.84\n0.8 0.2 445 5.81 8.23 0.80 0.23 0.08 49.47\n0.7 0.3 342 4.46 8.11 0.72 0.16 0.05 48.44\n0.6 0.4 352 4.59 8.06 0.57 0.09 0.01 47.80\n0.5 0.5 344 4.49 8.04 0.51 0.07 0.01 46.58\n0.4 0.6 334 4.36 7.91 0.31 0.04 0.01 45.73\n0.3 0.7 315 4.11 7.80 0.21 0.01 0.01 44.93\n0.2 0.8 247 3.22 7.21 0.17 0.01 0.0 43.94\n0.1 0.9 215 2.81 6.72 0.04 0.0 0.0 42.82\n0.0 1.0 145 1.89 5.38 0.0 0.0 0.0 38.45\nTable 5. Hub analysis result for music portal data using combinations of G1andFP. Results for using G1 or FP alone as\nwell as for a moderate combination are in bold face. See Section 4.2 for details.\n4.2 Reducing hubs by combining G1 and FP\nRecent advances in computing audio similarity rely on\ncombining timbre-based approaches (MFCCs plus Gaus-\nsian models) with a range of other features derived from\naudio. In particular, combinations of timbre and, among\nother features, ﬂuctuation patterns or variants thereof have\nproven sucessfull [11, 13]. Such a combination approach\nwas able to rank ﬁrst at the 2009 MIREX “Audio Mu-\nsic Similarity and Retrieval”-contest2. Since our method\nbased on ﬂuctuation patterns is less prone to hubness than\nthe timbre based approach, we tried to combine distances\nobtained with methods G1 and FP. It is our hypothesis that\nsuch a combination could reduce hubness and at the same\ntime preserve the good quality of timbre based methods in\nterms of audio similarity.\nFollowing previous approaches towards combination of\nfeatures [10, 11] we ﬁrst normalize the distance matrices\nMG1andMFPby subtracting the respective overall means\nand dividing by the standard deviations:\n\u0016MG1=MG1\u0000\u0016G1\nsG1\u0016MFP=MFP\u0000\u0016FP\nsFP(4)\nWe combine the normalized distance matrices linearly\nusing weights wG1andwFP:\n\u0016MC=wG1\u0016MG1+wFP\u0016MFP (5)\nTo evaluate the quality of audio similarity achieved by\ncombining methods G1 and FP we computed the genre\nclassiﬁcation performance. We used nearest neighbor clas-\nsiﬁcation as a classiﬁer. For every song in the data base\nwe computed the ﬁrst nearest neighbor using the distance\nmatrix \u0016MC. The ﬁrst nearest neighbor to a query song is\nthe song with minimum distance according to \u0016MC. To es-\ntimate genre classiﬁcation accuracy, the genre label of a\nquery songsquery and its ﬁrst nearest neighbor snnwere\ncompared. The accuracy is deﬁned as:\nacc(s query;snn) =jgquery\\gnnj\njgquery[gnnj\u0002100 (6)\n2http://www.music-ir.org/mirex/2009/withgquery (gnn) being a set of all genre labels for the\nquery song (nearest neighbor song) and j:jcounting the\nnumber of members in a set. Therefore accuracy is deﬁned\nas the number of shared genre labels divided by the set size\nof the union of sets gquery andgnntimes 100. The latter is\ndone to acount for nearest neighbor songs with two genre\nlabels as compared to only one genre label. The range of\nvalues for accuracy is between 0 and 100. All genre classi-\nﬁcation results are averaged over ten fold cross validations.\nWe ran a series of experiments using the music por-\ntal data base (see Section 2.2) and a number of different\nweight combinations wG1andwFP. To measure the hub-\nness of a given song we use n-occurrence with nequal 15.\nThe results given in Table 5 show: the weights wG1and\nwFP, the absolute number of the maximum n-occurrence\nmaxhub (i.e. the biggest hub), the percentage of songs in\nwhose nearest neighbor lists this biggest hub can be found\nmaxhub%, the percentage of hubs hub3j10j15j20% (i.e.\nthe percentage of songs of which the n-occurrence is more\nthan3j10j15j20 timesn) and the genre classiﬁcation accu-\nracyacc.\nIt is evident that with the weight wFPfor method FP\ngrowing, the hubs become smaller and less in number but\nthe genre classiﬁcation accuracy also degrades. Whereas\nusing method G1 alone (i.e. wG1= 1:0 andwFP= 0:0)\nyields a maximum hub of size 879 that is in the nearest\nneighbor lists of 11:47% of all songs, a moderate combi-\nnation using weights wG1= 0:6 andwFP= 0:4 dimin-\nishes the biggest hub to a size of 352. This reduced hub is\nnow a member of only 4:59% of the nearest neighbor lists.\nAlso the number of especially large hubs decreases: e.g.\nthe percentage of songs of which the n-occurrence is more\nthan 20timesn(hub20%) drops from 0:22% to0:01%\n(in absolute numbers from 17 to 1); the number of more\nmoderate sized hubs (hub10%) is still about halfed (from\n0:94% to0:57%, or from 72 to 44 in absolute numbers).\nSuch a moderate combination does not impair the overall\nquality of audio similarity as measured with genre clas-\nsiﬁcation accuracy: it is at 47:80% which is at the level\nof using method G1 alone yielding 48:47%. The baseline\naccuracy achieved by always guessing the most probable\n174\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)01002003004005006007008009000100200300400\nn−occurences for 1.0 G1 and 0.0 FPn−occurences for 0.6 G1 and 0.4 FPFigure 1.n-occurrences of using method G1 alone (x-\naxis) vs.n-occurrences using a moderate combination of\nG1 and FP (y-axis, wG1= 0:6 andwFP= 0:4) for music\nportal data. The diagonal line indicates songs for which\nthen-occurence does not change.\ngenre “Electronic” (see Table 2) is 29:11%. Always guess-\ning the two most probable genres “Electronic” and “Rock”\nyields 36:46%.\nIn Figure 1 we have plotted the n-occurrences of using\nmethod G1 alone (i.e. wG1= 1:0 andwFP= 0:0) ver-\nsus then-occurrences of the moderate combination using\nweightswG1= 0:6 andwFP= 0:4. This is done for all\nsongs in the music portal data base. The n-occurrence of\nevery song beneath the diagonal line is reduced by using\nthe combination. All large hubs with an n-occurrence big-\nger than 300 are clearly reduced. The same is true for the\nmajority of hubs with n-occurrences between 200 and 300.\n5. CONCLUSION\nWe were able to show that the so-called hub problem in au-\ndio based music similarity indeed does exist in very large\ndata bases and therefore is not an artefact of using lim-\nited amounts of data. As a matter of fact, the relative\namount and size of hubs is even growing with the size of\nthe data base. On the same very large web shop data base\nwe were able to show that a non-timbre based parameteri-\nzation of audio similarity (ﬂuctuation patterns) is by far not\nas prone to hubness as the standard approach of using Mel\nFrequency Cepstrum Coefﬁcients (MFCCs) plus Gaussian\nmodeling. Extending recent successful work on combin-\ning different features to compute overall audio similarity,\nwe were able to show that this not only maintains a high\nquality of audio similarity but also decisively reduces the\nhub problem.\nThe combination result has so far only been shown on\nthe smaller music portal data base, but there is no reason\nwhy this should not hold for the larger web shop data. Only\nlimitations in computer run time led us to ﬁrst evaluate the\ncombination approach on the smaller data set. We are not\nclaiming that our speciﬁc combination of features is the\nbest general route towards audio similarity. But we are\nconvinced that going beyond pure timbre-based similarityis able to achieve two goals simultaneously: high quality\naudio similarity and avoiding the hub problem.\n6. ACKNOWLEDGEMENTS\nThis research is supported by the Austrian Science Fund\n(FWF, grants L511-N15 and P21247) and the Vienna\nScience and Technology Fund (WWTF, project “Audio-\nminer”).\n7. REFERENCES\n[1] Aucouturier J.-J., Pachet F.: A scale-free distribution of\nfalse positives for a large class of audio similarity mea-\nsures, Pattern Recognition, V ol. 41(1), pp. 272-284,\n2007.\n[2] Aucouturier J.-J., Defreville B., Pachet F.: The bag-of-\nframes approach to audio pattern recognition: A sufﬁ-\ncient model for urban soundscapes but not for poly-\nphonic music, Journal of the Acoustical Society of\nAmerica, 122 (2), 881-891, 2007.\n[3] Aucouturier, J.-J., Pachet F.: Improving Timbre Simi-\nlarity: How high is the sky?, Journal of Negative Re-\nsults in Speech and Audio Sciences, 1(1), 2004.\n[4] Fruehwirt M., Rauber A.: Self-Organizing Maps for\nContent-Based Music Clustering, Proceedings of the\nTwelth Italian Workshop on Neural Nets, IIAS, 2001.\n[5] Godfrey M.T., Chordia P.: Hubs and Homogeneity:\nImproving Content-Based Music Modeling, Proceed-\nings of the 9th International Conference on Music In-\nformation Retrieval (ISMIR’08), Philiadelphia, USA,\n2008.\n[6] Hoffman M., Blei D., Cook P.: Content-Based Musical\nSimilarity Computation Using the Hierarchical Dirich-\nlet Process, Proceedings of the 9th International Con-\nference on Music Information Retrieval (ISMIR’08),\nPhiliadelphia, USA, 2008.\n[7] Logan B.: Mel Frequency Cepstral Coefﬁcients\nfor Music Modeling, Proceedings of the Interna-\ntional Symposium on Music Information Retrieval (IS-\nMIR’00), Plymouth, Massachusetts, USA,2000.\n[8] Mandel M.I., Ellis D.P.W.: Song-Level Features and\nSupport Vector Machines for Music Classiﬁcation,\nProceedings of the 6th International Conference on\nMusic Information Retrieval (ISMIR’05), London, UK,\n2005.\n[9] Pampalk E.: Islands of Music: Analysis, Organiza-\ntion, and Visualization of Music Archives, MSc Thesis,\nTechnical University of Vienna, 2001.\n[10] Pampalk E.: Computational Models of Music Sim-\nilarity and their Application to Music Information\nRetrieval, Vienna University of Technology, Austria,\nDoctoral Thesis, 2006.\n175\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)[11] Pampalk E., Flexer A., Widmer G.: Improvements of\nAudio-Based Music Similarity and Genre Classiﬁca-\ntion, Proceedings of the 6th International Conference\non Music Information Retrieval (ISMIR’05), London,\nUK, September 11-15., 2005.\n[12] Pampalk E., Rauber A., Merkl D.: Content-based or-\nganization and visualization of music archives, Pro-\nceedings of the 10th ACM International Conference on\nMultimedia, Juan les Pins, France, pp. 570-579, 2002.\n[13] Pohle T., Schnitzer D., Schedl M., Knees P., Widmer\nG.: On rhythm and general music similarity, Proceed-\nings of the 10th International Conference on Music In-\nformation Retrieval (ISMIR’09), Kobe, Japan, 2009.\n[14] Penny W.D.: Kullback-Leibler Divergences of Normal,\nGamma, Dirichlet and Wishart Densities, Wellcome\nDepartment of Cognitive Neurology, 2001.\n[15] Zwicker E., Fastl H.: Psychoaccoustics, Facts and\nModels, Springer Series of Information Sciences, V ol-\nume 22, 2nd edition, 1999.\n176\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "Handling Repeats and Jumps in Score-performance Synchronization.",
        "author": [
            "Christian Fremerey",
            "Meinard Müller",
            "Michael Clausen"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1415942",
        "url": "https://doi.org/10.5281/zenodo.1415942",
        "ee": "https://zenodo.org/records/1415942/files/FremereyMC10.pdf",
        "abstract": "Given a score representation and a recorded performance of the same piece of music, the task of score-performance synchronization is to temporally align musical sections such as bars specified by the score to temporal sections in the performance. Most of the previous approaches as- sume that the score and the performance to be synchro- nized globally agree with regard to the overall musical structure. In practice, however, this assumption is often violated. For example, a performer may deviate from the score by ignoring a repeat or introducing an additional re- peat that is not written in the score. In this paper, we introduce a synchronization approach that can cope with such structural differences. As main technical contribu- tion, we describe a novel variant of dynamic time warping (DTW), referred to as JumpDTW, which allows for han- dling jumps and repeats in the alignment. Our approach is evaluated for the practically relevant case of synchronizing score data obtained from scanned sheet music via optical music recognition to corresponding audio recordings. Our experiments based on Beethoven piano sonatas show that JumpDTW can robustly identify and handle most of the oc- curring jumps and repeats leading to an overall alignment accuracy of over 99% on the bar-level.",
        "zenodo_id": 1415942,
        "dblp_key": "conf/ismir/FremereyMC10",
        "keywords": [
            "score-performance synchronization",
            "temporally align musical sections",
            "global agreement assumption",
            "structural differences",
            "novel variant of dynamic time warping",
            "JumpDTW",
            "handling jumps and repeats",
            "alignment accuracy",
            "over 99% accuracy",
            "Beethoven piano sonatas"
        ],
        "content": "HANDLING REPEATS AND JUMPS IN SCORE-PERFORMANCE\nSYNCHR\nONIZATION\nChristian Fremerey\nBonn University\nComputer Science\nBonn, Germany\nfremerey@iai.uni-bonn.deMeinard M ¨uller\nSaarland University and\nMPI Informatik\nSaarbr¨ucken, Germany\nmeinard@mpi-inf.mpg.deMichael Clausen\nBonn University\nComputer Science\nBonn, Germany\nclausen@iai.uni-bonn.de\nABSTRACT\nGiven a score representation and a recorded performance\nof the same piece of music, the task of score-performance\nsynchronization is to temporally align musical sections\nsuch as bars speciﬁed by the score to temporal sections\nin the performance. Most of the previous approaches as-\nsume that the score and the performance to be synchro-\nnized globally agree with regard to the overall musical\nstructure. In practice, however, this assumption is often\nviolated. For example, a performer may deviate from the\nscore by ignoring a repeat or introducing an additional re-\npeat that is not written in the score. In this paper, we\nintroduce a synchronization approach that can cope with\nsuch structural differences. As main technical contribu-\ntion, we describe a novel variant of dynamic time warping\n(DTW), referred to as JumpDTW, which allows for han-\ndling jumps and repeats in the alignment. Our approach is\nevaluatedforthepracticallyrelevantcaseofsynchronizing\nscore data obtained from scanned sheet music via optical\nmusic recognition to corresponding audio recordings. Our\nexperiments based on Beethoven piano sonatas show that\nJumpDTWcanrobustlyidentifyandhandlemostoftheoc-\ncurring jumps and repeats leading to an overall alignment\naccuracy of over 99%on the bar-level.\n1. INTRODUCTION\nGiven a score and a performance of the same piece of mu-\nsic, a common task of music information retrieval consists\nof synchronizing note events or musical sections given by\nthe score representation with time positions or temporal\nsections of the performance. A useful example applica-\ntionofsuchasynchronizationistoallowuserstonavigate\nin a recorded performance of a piece of music by select-\ning locations of interest from the visual sheet music rep-\nresentation of the synchronized score and simultaneously\nplayback the performance while highlighting the current\nplayback position in the sheet music [1].\nScores and performances can be given in many differ-\nent forms and formats. For example, scores can be given\nas scans of printed sheet music, vector graphics generated\nby a computer typesetting software, optical music recog-\nnitionresults,symbolicscoreformatssuchasMusicXML,\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnotmadeordistributedforproﬁtorcommercialadvantageandthatcopies\nbear this noticeand thefull citation on theﬁrst page.\nc/circlecopyrt2010International Society for Music Information Retrieval.Humdrum, or Lilypond, or as MIDI ﬁles. Performances\nare usually given as audio recordings or in form of MIDI\nﬁles generated by electronic instruments. When aligning\nscore and performance representations, challenging prob-\nlems arise when the two representations reveal differences\nintheirglobaloverallstructures. Forexample,aperformer\nmay ignore a repeat that is written in the score or may\nintroduce an extra repeat that is not written in the score\n(e.g.anadditionalverse). Furthermore,aperformancemay\ninclude parts that are not written in score at all (e.g., a\ncadenza or solo part) or may skip certain parts of an un-\nderlying score. Structural differences between scores and\nperformances have been encountered in previous work on\non-line score following such as [2–4]. In this scenario, the\nscores and performances that are synchronized are usually\nmonophonic. The most popular approach for this scenario\nis to use hidden Markov models (HMM) in combination\nwith a training process to determine model parameter that\nsuit the given type of data. For the case of off-line syn-\nchronization of polyphonic scores and performances, dy-\nnamic time warping (DTW) in combination with chroma\nfeatures has become a popular approach [5,6] because it\ncan deliver similar accuracy than HMMs but without the\nneed for creating and training models. Furthermore, ef-\nﬁcient multi-scale implementations can easily be realized\nfor this approach [7]. An overview on on-line and off-line\nscore-performancesynchronizationapproachesisfoundin\n[8]. In previous work on off-line score-performance syn-\nchronization, a basic assumption usually is that there are\nno structural differences between the two versions to be\naligned. In [6], the authors point out that classical DTW\ncan bypass additional segments such as repeated verses, at\nleast to some extent. Raphael [9] remarks in his work that\nstructural differences such as repeats are a common prob-\nlem in score-performance synchronization. Content-based\ncomparison of scores and performances also plays an im-\nportant role in retrieval scenarios [10–12]. As pointed\nout in [12], retrieval methods may also be used to de-\ntermine the structural differences between a score and a\nperformance. Further related work has focused on perfor-\nmances only, either in the scenario of general partial mu-\nsic synchronization [13] or structural analysis of perfor-\nmances [14,15].\nIn this paper, we describe a novel approach that allows\nfor synchronizing score and performance data in the pres-\nenceofstructuraldifferences. Themainmotivationforour\nworkoriginatesfromaproblemofhighpracticalrelevance\narising in the data acquisition and processing pipeline of a\ndigital music library [1]. Here, the score data is typically\nobtained by ﬁrst scanning the given printed sheet music\n243\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)(a) (b) (c)\n(d) (e) (f) (g) (h)\nFigur\ne1.Examplesforseveraltypesofblockboundaryindicators: (a)beginningandendofmovement/song,(b)doublebarlineswith\nand without repeat signs, (c) brackets for alternative endings, (d) segno marker, (e) textual jump directive, (f) coda, (g) ﬁne, (h) title\nheading of new musical section\nmaterial and then by converting the digitized images into\nasymbolicscorerepresentationusingopticalmusicrecog-\nnition (OMR). In this process, repeat and jump directives\nthat are written in the printed sheet music (as shown in\nFig. 1) are often not recognized reliably by the OMR soft-\nware. Besides the reasons given above, such missing di-\nrectives are a major source for structural differences be-\ntween the resulting score representation and a given audio\nrecording. As the main technical contribution of this pa-\nper,weintroduceanovelvariantofdynamictimewarping\n(DTW), which we refer to as JumpDTW. The main idea\nof our approach is to estimate the repeats and jumps that\nmake the score match the performance and to calculate\nthe actual score-performance alignment within a joint op-\ntimizationprocedurebasedonacontent-basedcomparison\nof the score and audio data. The task tackled in this paper\nis related to the task of computing a possibly large partial\nalignment of two data streams [13,16]. However, in con-\ntrast to these approaches, our goal is to somehow unfold\nthe score representation to best explain the performance.\nFurthermore, we assume that the jumps and repeats only\noccur onmusicallymeaningful positionsbyexploitingad-\nditional structural information given by the score. To this\nend, the score is searched for structural elements such as\ndouble bar lines to divide the score into blocks, see Fig. 1.\nThen repeats and jumps are allowed only at block bound-\naries but never inside blocks.\nThe remainder of this paper is organized as follows.\nIn Sect. 2, we formalize the task of handling repeats and\njumps in score-performance synchronization. In Sect. 3,\nwe describe our novel JumpDTW algorithm in detail and\nindicate several extensions. Finally, in Sect. 4, we present\nexperiments performed on a test dataset consisting of pi-\nano sonatas by Beethoven and conclude in Sect. 5 with a\ndiscussion of future work.\n2. PROBLEM MODELING\nWe now assume that we are given one sheet music repre-\nsentationandoneperformanceinformofanaudiorecord-\ning of the same piece of music. After processing the sheet\nmusicviaOMR,oneobtainsasymbolicrepresentationre-\nferred to as scorerepresentation. The score is naturally\ndivided into sections that are delimited by either bar lines\nor the left or right boundary of a grand staff. Even though\nthesesectionsmaydifferfromthemusicalbarsastheyareusually counted in Western sheet music notation, in this\npaper, we simply refer to each such section as bar.\nLetBdenote the set of bars appearing in the score and\nletK=|B|bethenumberofbars. Orderingthesetofbars\nby their visual occurrence in the sheet music (canonically\nordered by the page number, line number, and left to right\nwithin a line), one obtains a sequence σ= (σ1,...,σ K),\nσk∈ B,k∈[1 :K], which we refer to as score bar se-\nquence. Note that the score bar sequence does not account\nfor jump and repeat directives, see Fig. 2. Depending on\nthe context, we use the term barto denote either an ele-\nment ofB, the region in the sheet music image that rep-\nresents the bar, the musical content of the bar, or one of\npossibly many occurrences of the bar in the performance.\nAsdiscussedbefore,sheetmusicmaycontainjumpand\nrepeat directives such as repeat signs, alternative endings,\ndacapos or segnos, see Fig. 1. Because of these direc-\ntives, the given performance often deviates from the score\nbar sequence σ. The musician may even choose to ig-\nnore or add some of the displayed repeats or may intro-\nduce shortcuts. This leads to a possibly different sequence\nπ= (π1,...,π J),πj∈ B,j∈[1 :J], which we call\nperformance bar sequence, see Fig. 2. Note that in the\nscenario discussed in this paper, the performance bar se-\nquenceπis unknown. One application of the approach\nintroduced in the remainder of this paper is to determine\nthis sequence π.\nTo relate the score bar sequence σand the performance\nbar sequence π, intuitively, the score bar sequence, which\nrepresents the source material, has to be suitably unfolded\nto best explain the performance. Here, the unfolding typi-\ncallyappearsatthejumpandrepeatdirectivesindicatedby\nthesheetmusic. Makinguseofthisfact,theproblemofun-\nfoldingsequencesofbarscanbereducedtotheeasiertask\nof unfolding much shorter sequences of so-called blocks\nwhich are obtained by concatenating suitable subsquences\nof bars during which no repeats or jumps are expected to\noccur. Tothisend,thescoreissearchedfor blockboundary\nindicators thatindicatebarsinthescorethatmightserveas\nsource or target for jumps and repeats. Examples of these\nindicators are depicted in Fig. 1.\nLetk0= 0< k1< ... < k I−1< kI=Kbe\nboundaryindicescorrespondingtothejumpandrepeatdi-\nrectives. Then, we deﬁne the block\nβi= (σki−1+1,...,σ ki) (1)\n244\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)1.\n2.σ1\nπ1\nβ1σ2\nπ2π6σ3\nπ3π7\nβ2σ4\nπ4σ5\nπ5\nβ3\nσ6\nπ8σ7\nπ9\nβ4σ8\nπ10σ9\nπ11σ10\nπ12σ11\nπ13\nβ5\nFigur\ne 2.Illustration of the score bar sequence σ, the perfor-\nmance bar sequence πand the score block sequence β.\nof length |βi|=ki−ki−1fori∈[1 :I]. The resulting\nscoreblocksequence β:= (β1,...,β I)isapartitionof σ,\nsee Fig. 2. Now, the task of ﬁnding the performance bar\nsequence πis reduced to ﬁnding a sequence of block in-\ndicesb= (b1,...,b G),bg∈[1 :I],g∈[1 :G], such that\n(βb1,...,β bG)is as close as possible to the performance\nbar sequence π. The task of ﬁnding such a sequence bis\ndiscussed in the next section. For an example, we refer to\nFig. 3. Note that, depending on the context, we will later\nusetheterm blocknotonlytodenoteelementsofthescore\nblocksequence β,butalsotorefertoelementsoftheblock\nindex sequence b.\n3. PARTIAL SYNCHRONIZATION WITH JUMPS\nMost procedures for score-performance synchronization\nﬁrstconvertthetwodatastreamstobealignedintosuitable\nfeature representations. Then, based on a local cost mea-\nsurethatallowsforcomparingfeatures,aglobalalignment\npath between the feature sequences is computed using dy-\nnamic time warping (DTW). This procedure only works\nwell if the score and the performance are in global corre-\nspondence and do not differ in their overall structure.\nToaccountforstructuraldifferencesasoccurringinour\nscenario, we extend theclassical DTWapproach toenable\njumpsinthealignmentpath. Ourideaofallowingjumpsis\ninspiredbythewayapieceofmusicisoftenmodeledusing\na Hidden Markov Model (HMM). Here, the note events of\na score are modeled by states which are left-to-right con-\nnectedtoenforcethatthemusiccanonlymoveforwardbut\nnot backward. To account for possible repeats and jumps\nat certain block boundaries, one then simply adds further\nconnectionsthatconnectstatesrepresentingpossiblejump\nsources to states representing possible jump targets. After\na short review of classical DTW (Sect. 3.1), we show how\nthejumpdirectivescanbeincorporated(Sect.3.2)andthen\nindicate further DTW variants (Sect. 3.3).\n3.1 Classical DTW\nIntroducing some notation, we now summarize the clas-\nsical DTW approach using a slight reformulation. Let\nx= (x1,...,x N)andy= (y1,...,y M)be the feature\nsequencesobtainedfromthescoreandperformancerepre-\nsentation, respectively. Furthermore, let cdenote the local\ncostmeasureusedtocomparetwofeatures. Thenthe local\ncost matrix Cof dimension N×Mis deﬁned by\nC(n,m) := c(xn,ym) (2)β1β2β3β4β5Score\nPerformance\nFigure 3. Visualization of a score-audio synchronization result\nwith score block sequence b= (1,2,3,2,4,5)for the score and\nperformance bar sequences shown in Fig. 2. The red line indi-\ncates an alignment path with jumps.\nfor(n,m)∈Z,whereZ:= [1 :N]×[1 :M]isreferredto\nasthesetof cells. A(global) alignmentpath betweenxand\nyis a sequence p= (p1,...,p L)withpℓ= (nℓ,mℓ)∈Z\nforℓ∈[1 :L]satisfying the boundary condition p1=\n(1,1)andpL= (N,M )andthestepcondition pℓ−pℓ−1∈\nΣforℓ∈[2 :L]Here,Σ :={(1,0),(0,1),(1,1)}denotes\nthe set of possible steps. The cost of the path pis deﬁned\nby/summationtextL\nℓ=1C(pℓ). Anoptimal alignment path is deﬁned to\nbeanalignmentpathhavingminimalcostoverallpossible\nalignment paths.\nAn optimal alignment path can be computed using\ndynamic time warping (DTW). First, for a given cell\n(n,m)∈Z, one deﬁnes the set Zn,mof possible prede-\ncessorsby\nZn,m:={(n,m)−z|z∈Σ}∩Z. (3)\nThen, one computes an accumulated cost matrix Dof di-\nmensionN×M. First, one sets D(1,1) :=C(1,1)and\nthen recursively deﬁnes\nD(n,m) := C(n,m)+min/braceleftbig\nD(z)|z∈Zn,m/bracerightbig\n(4)\nfor(n,m)∈Z\\{(1,1)}. The value D(N,M)represents\nthe cost of an optimal alignment path. Such an optimal\npath can be constructed based on a simple back tracking\nalgorithm using D. For details, we refer to [17].\n3.2 JumpDTW\nToaccountforstructuraldifferencesbetweenthescoreand\ntheperformance caused byrepeats and jumps,we nowex-\ntend the concept of an alignment path and the classical\nDTW approach. Recall that we assume that the jumps oc-\ncur from ends to beginnings of the blocks βi,i∈[1 :I].\nWithregardtothefeaturerepresentation x= (x1,...,x N)\nof the score, we assume that the beginning of βicor-\nresponds to index si∈[1 :N]and the end to index\nti∈[1 :N], wheresi< ti. Furthermore, we assume that\nthe beginning of block βi+1immediately follows the end\nof blockβi, i.e.,si+1=ti+1. LetS:={si|i∈[1 :I]}\nandT:={ti|i∈[1 :I]}.\nNext, analignment path with jumps with respect to the\nsetsSandTis deﬁned to be a sequence p= (p1,...,p L)\nwithpℓ= (nℓ,mℓ)∈Zforℓ∈[1 :L]satisfying the\nboundaryconditionasbefore. However,thistimewemod-\nifythestepconditionbyrequiringthateither pℓ−pℓ−1∈Σ\n(as before) or\nmℓ−1=mℓ−1∧nℓ−1∈T∧nℓ∈S.(5)\n245\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)In other words, besides the regular steps, we also permit\njumps\nin the ﬁrst coordinate (corresponding to the score)\nfromtheendofanyblock(givenby T)tothebeginningof\nany other block (given by S), see also Fig. 3.\nWenowintroduceamodiﬁedDTWversion,referredto\nasJumpDTW, that allows for computing an optimal align-\nment path with jumps. Recall that, in classical DTW, the\nsetZn,mof possible predecessor cells encodes all cells\nfrom which one can reach the cell (n,m)by applying a\nsingle step from Σ, see (3). The main idea of our modiﬁ-\ncation is to add further predecessor cells that model possi-\nble jumps between the block boundaries. To this end, we\nextend all sets Zn,mforn∈Sby setting\n˜Zn,m:=Zn,m∪/parenleftbig\n{(t,m−1)|t∈T} ∩Z/parenrightbig\n.(6)\nFurthermore, we set ˜Zn,m:=Zn,mfor all other n∈\n[1 :N]\\S. Intuitively, the additional predecessor cells\nin˜Zn,m\\Zn,mpermitjumpsfromtheendofanyblockto\nthe beginning of any other block. As in the classical case,\none then computes an accumulated cost matrix simply by\nreplacingthesets Zn,mbythesets ˜Zn,mobtainingamatrix\n˜D. More precisely, we set ˜D(1,1) =C(1,1)and\n˜D(n,m) := C(n,m)+min/braceleftbig˜D(z)|z∈˜Zn,m/bracerightbig\n(7)\nfor(n,m)∈Z\\{(1,1)}. Note that for a given (n,m)/\\e}atio\\slash=\n(1,1), the set ˜Zn,monly contains cells of the form (n−\n1,m)or(k,m−1)for some k∈[1 :N]. In other\nwords,˜Zn,monly contains cells that lie below or to the\nleft of the current cell (n,m)when the axes are chosen as\ninFig.3. Therefore, ˜Dcanstillbecomputedrecursivelyin\na column-wise fashion. The matrix entry ˜D(N,M)yields\nthe cost of an optimal alignment path with jumps. As for\nthe classical case, such an optimal path can then be con-\nstructed based on a simple back tracking algorithm using\n˜D.\nFrom an optimal warping path with jumps one can\nderive the underlying sequence of block indices b=\n(b1,...,b G),bg∈[1 :I],g∈[1 :G], in a canonical\nway. Starting with the ﬁrst block, one either enters the\nsubsequent block via a step from Σor enters a different\nblock via a jump. For example, in the case of a jump from\npℓ−1= (tj,m−1)topℓ= (si,m)for some ℓ∈[2 :L],\none obtains bg−1=jandbg=ifor some g∈[2 :G],\nsee also Fig. 3 for an illustration. Having determined the\nsequence of block indices b, one can easily derive the per-\nformance sequence πby expanding blocks to bars.\n3.3 Further DTW Variants\nBecause of the boundary condition, an alignment path\nstarts atp1= (1,1)and ends at pL= (N,M ). There-\nfore, the score block sequence bis also restricted to start\nwith the ﬁrst block b1= 1and to end with the last block\nbG=K. In practice, however, a performance may end\nwith a different block. For example, this happens in the\npresence of a “dacapo”, where the piece ends at a block\nmarked with the keyword “ﬁne.” To account for this pos-\nsibility, one can easily modify the JumpDTW algorithm.\nInstead of looking at the entry ˜D(N,M), one simply has\nto determine the index\nn∗:= argmin/braceleftbig˜D(n,M)|n∈T/bracerightbig\n.(8)Then,thealignmentpathwithjumpsiscomputedviaback-\ntracking starting with the cell (n∗,M)instead of (N,M).\nSimilarly, one can relax the condition that one has to start\nwith the ﬁrst block, see [17] for details. Note that further\nconstraints on the jumps can easily be handled by suitably\nmodifyingthesets ˜Zn,mofpredecessorcells. Forexample,\nto restrict the jump possibilities for a given block βi, one\nsimply restricts the set Tto a suitable subset T′⊂Tand\nthen uses ˜Zsi,m:=Zsi,m∪/parenleftbig\n{(t,m−1)|t∈T′} ∩Z/parenrightbig\n.\n4. EXPERIMENTS\nTo evaluate the usefulness of JumpDTW in a practically\nrelevantapplication,experimentsareconductedontheﬁrst\n15piano sonatas by Beethoven including a total of 54\nindividual movements. The score data is obtained from\nOMR results of a printed sheet music edition, and the per-\nformances are given as audio CD recordings. Since the\nscoredatadoesnotincludeanytempoinformation,amean\ntempo is estimated for each movement using the number\nofbarsandthedurationofthecorrespondingperformance.\nForeachmovement,thescorebarsequence σisknownand\nthescoreblocksequence βisobtainedusingblockbound-\nary indicators extracted from the score. Note that this may\ninclude block boundary indicators where actually no jump\nor repeat occur in the performance. The performance bar\nsequence πis given as ground truth and is used to derive a\ngroundtruthblockindexsequence bwithrespectto β. For\nourtestdataset,thetotalnumberofscoreblocksappearing\nin the sequences βof the54movements is 242. The total\nnumber of score bars is 8832. Note that, because of re-\npeats and jumps, a score block may occur more than once\nin the performance. Therefore, the total number of blocks\nappearing in the sequences bis305which corresponds to\natotalof11836barsbeingplayedintheperformance. The\ntotal duration of the performance amounts to 312minutes.\nJumpDTW is performed on the data using βas de-\nscribed in Section 3.2. From the resulting warping\npath with jumps, an output block index sequence b′=\n(b′\n1,...b′\nG′)is obtained. In the optimal case, this block in-\ndex sequence b′would be equal to the ground truth block\nindex sequence b. Table 1 shows the results of comparing\nb′tobusing several different evaluation measures. Each\nrow shows the results for different sets of b′obtained us-\ning a different JumpDTW variant. Each entry in the table\nsummarizestheresultsforall 54movements. Theﬁrstrow,\ntaggednojumps,representstheresultswhenusingclassi-\ncalDTWasdescribedinSect.3.1,whichservesasbottom\nlineinourevaluation. Thesecondrow,tagged s1plain,\nrepresents\nthe basic JumpDTW algorithm as described in\nSection 3.2 including the relaxed boundary condition for\ndacapo/ﬁne cases as described in 3.3.\nThe numbers plotted in the ﬁrst six columns are based\non a direct comparison of the sequences b′andband\nmeasure how many blocks (abbreviated as blk) or per-\nformance bars (bar) match between the two sequences\n(mch), have been erroneously inserted into b′(ins), or\nhave been erroneously omitted in b′(omt) with respect\nto the ground truth b. To this end, we calculate an opti-\nmum alignment between the two block index sequences\nusing a variant of the edit distance that only allows in-\nsertions and deletions (but not replacements). To ﬁnd an\nalignment between the two block index sequences that is\noptimal with respect to the amount of inserted and omit-\n246\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)mch blk % (#) ins blk % (#) omtblk % (#) mch bar % (#) ins bar % (#) omt bar % (#) prf % (#)\nnojumps 70.2 (214) 0.3 (1) 29.8 (91) 74.6 (8831) 0.0 (1) 25.4 (3005) 69.8 (8258)\ns1plain 93.4 (285) 9.2 (28) 6.6 (20) 99.2 (11740) 0.9 (105) 0.8 (96) 98.5 (11661)\ns2addspecialstates 93.4 (285) 5.6 (17) 6.6 (20) 99.3 (11759) 0.7 (82) 0.7 (77) 98.8 (11692)\ns3penalize0.5100 94.4 (288) 9.5 (29) 5.6 (17) 99.4 (11767) 0.4 (51) 0.6 (69) 99.1 (11725)\nTable 1. Evaluation resultsfor classical DTW and different variants of JumpDTW.\nted\nbars (instead of blocks), each block index entry in the\nsequences is weighted by the length of the corresponding\nscore block. Each entry in Table 1 is given as a percent-\nage with respect to the total number of blocks/bars in the\nperformancefollowedbytheabsolutenumberinparenthe-\nses. For example, the entry 70.2(214) in the ﬁrst row and\ncolumnmeansthat 214blocksof b′haveamatchingcoun-\nterpartin b,whichis 214/305 = 70.2% ofthetotalnumber\nof blocks in b. Similarly, the entry 74.6(8831) for match-\ning bars means that the 214matching blocks have a total\nlengthof 8831bars,whichis 8831/11836 = 74.6% ofthe\ntotal length of bin bars.\nA further evaluation measure (prf), which is plotted\nin the last column of Table 1, expresses the alignment ac-\ncuracy on the bar-level. This measure is motivated by\nthe application of visually presenting sheet music that is\nlinkedonabar-wiseleveltoagivenrecordedaudioperfor-\nmance. For this application, we want to measure for how\nmanyoftheperformancebarsthealignmentcomputedvia\nJumpDTW is suitably accurate. To this end, the ground\ntruth block index sequence bis used to create a feature\nsequence xfrom the score data that matches the repeats\nandjumpsoftheperformance. Then,thisfeaturesequence\nis synchronized to a feature sequence yobtained from the\nperformance using classical DTW. From the output warp-\ningpath,wederiveabar-wisescore-performancesynchro-\nnization that maps each performance bar πj∈πto a tem-\nporal region with center time djin the performance, see\nFig. 4. Furthermore, this synchronization delivers a map-\npingφ: [0,D]→[1 :K], withDbeing the duration of\nthe performance, that for each time positions d∈[0,D]in\nthe performance returns an index k∈[1 :K]indicating\nthat barσkis played at time d, see also Fig. 4. Since,\nfrom manual inspection, the synchronization results ob-\ntained when using the ground truth block index sequence\nbare known to be suitably accurate on a bar-wise level,\nthey are used as a reference for ﬁnding deviations in the\nsynchronization resultsobtained using b′. For each perfor-\nmance bar πj, we take the bar center time djand input it\ninto the mapping φ′obtained from the synchronization re-\nsultsusing b′. Theperformancebariscountedascorrectly\nmatched if φ′(dj) =φ(dj), which means that in the syn-\nchronization obtained using b′, the time position djpoints\ntothesamebar σkasinthereferencesynchronization. Un-\nlike the mere number of matched bars listed in the column\nmch bar, this measure takes into account the extra con-\nfusionthatiscausedinthesynchronizationbyerroneously\ninserted or omitted bars.\nFrom the results using classical DTW (nojumps) one\ncan see that about 70–75%of the blocks and bars of the\nperformance are covered by the plain score bar sequence.\nThe remaining 25–30%are repeats that are omitted in this\nsequence. The synchronization-based measure indicates\na similar result: 69.8%of the center time positions of\nthe bars in the performance were aligned to the correct\nbar in the score. These results are improved signiﬁcantly,\nFigure 4. Illustration of a bar-wise score-performance synchro-\nnization.\nEach performance bar πjis synchronized to a temporal\nregion of a performance with bar center time dj. Furthermore, a\nmapping φcan be derived that for a given time position din the\nperformance outputs the index kof the corresponding score bar\nσk.\nwhen using JumpDTW (s1 plain). Here, 93 .4%of the\nblocksand 99.2%ofthebarsarematched correctly. Inthe\nsynchronization-based measure, 98.5%of the performed\nbars match the reference synchronization. Even though 28\nblocks have been erroneously inserted and 20blocks have\nbeen omitted, this amounts to only 105inserted bars and\n96omitted bars, revealing that the mean length of inserted\nand omitted blocks is only about 4.2bars.\nManual inspection of the results for the individual\nmovements reveals that in many cases an extra block is\ninserted at the beginning or the end of the sequence to\ncover for silence at the beginning or end of the perfor-\nmance. In one case, this even leads to the last block of\nthe sequence being confused with an incorrect one. To\nencounter this issue, we extend the JumpDTW algorithm\nby adding special states to the score representation that\nmodel silence at the beginning or end of the performance.\nThe results for this modiﬁcation are listed in the line la-\nbeleds2addspecial states and show slightly im-\npro\nvednumbers. Anin-depthanalysisoftheresultsshows\nthat this modiﬁcation solved all of the previously men-\ntioned problems caused by initial or trailing silence in the\nperformance. Furthermore, it turned out that 130of the\n82+77 = 159 insertedandomittedbarsoccurinjust 3of\nthe54movements. Theperformanceoftheﬁrstmovement\nof “Sonata 8, Op. 13, Pathetique” contains extreme tempo\nchanges with slow sections of roughly 20BPM (beats per\nminute) alternating with fast sections of about 300BPM.\nThis results in a large difference between the estimated\nmean tempo of the score and the tempo of the slow sec-\ntions in the performance. The JumpDTW algorithm reacts\nby erroneously inserting more or less random blocks to\ncover the unexpectedly slow sections of the performance.\nA different kind of problem occurs in “Sonata 12, Op. 26,\nAndante con variazioni”. Here, the second block is a vari-\nationoftheﬁrstblockthathasvirtuallythesameharmonic\nprogression. TheJumpDTWerroneouslytreatsthissecond\nblockintheperformanceasarepeatoftheﬁrstblockinthe\nscore. Thisbehaviorisnotverysurprisingconsideringthat\n247\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)the content-based comparison of score and performance is\nsome\nwhat noisy and for the chroma-based features used,\nsections with the same harmonic progression are almost\nindistinguishable. In “Sonata 13, Op. 27 No. 1, Andante–\nAllegro” it is again a signiﬁcant change in the tempo that\ncauses a problem. Here, a repeat of a block (length = 9\nbars)ofthefasterAllegrosectionisomittedbyJumpDTW,\nwhich is provoked bythe estimatedtempo ofthe scorebe-\ning signiﬁcantly slower than the tempo of the correspond-\ning section of the performance. For all of the remaining\nmovements, only blocks of length 2or lower are inserted\nor omitted.\nTo encounter the problems discussed above, we further\nextend the JumpDTW approach by introducing a penalty\ncostforperformingjumpsinthewarpingpaththatisadded\nto the accumulated cost. The cost value is set to 0.5·N\n100,\nwithNbeingthelengthofthescorefeaturesequence. The\nparticular formula is motivated by the idea of choosing a\ncost value that is close to the cost of matching 1/100-th\nof the score to a section of the performance that is not\nconsidered similar. Since in our implementation, we use\nnormalized chroma features with a cosine measure for the\nlocal cost, a local cost value of 0.5is already considered\nnot similar. The results for this modiﬁcation are listed\nin the row s3penalize 0.5100. A closer analysis\nsho\nws that adding the penalty solves the confusion for the\n“Andante con Variazioni” and lowers the amount of in-\nsertedbarsfortheslowsectionsofthe“Pathetique”,which\nleads to a better overall result. However, the penalty also\ncauses degradation for many of the other movements be-\ncause short blocks for alternative endings are no longer\nskipped. Tuning the penalty cost to higher or lower values\ndid not improve the situation. An increased penalty led to\nan increased amount of erroneously skipped short blocks\nwhile a decreased penalty no longer solved the confusion\nfor the two movements discussed above.\n5. CONCLUSIONS\nIn this paper, we have formally modeled the task of score-\nperformance synchronization in the presence of structural\ndifferences induced by jumps and repeats. To handle such\ndifferences, we introduced a novel DTW variant referred\nto as JumpDTW. The results of the experiments presented\nin Section 4 show that the JumpDTW approach can suc-\ncessfully align about 99%of the bars played in the perfor-\nmance on the given test dataset with less than 1%of bars\nbeingomittedandlessthan 1%ofextrabarsbeinginserted.\nThispositiveresultsuggeststhattheapproachmaybeuse-\nful for the large-scale automatic alignment of OMR data\nand audio recordings in a digital music library scenario.\nIntroducing penalty cost for performing jumps did ﬁx\nsomeproblemsoccuringonthetestdatasetbutalsocaused\nadditional errors. Further improvements of our approach\nare needed in situations where one has large differences\n(more than a factor of two) in the estimated tempo of the\nscoreandthetempooftheactualperformance. Also,when\nusing chroma features, blocks that reveal a similar har-\nmonicprogressionarepronetoconfusion. Here,combina-\ntionswithotherfeaturetypesmayhelptoresolvethisprob-\nlem. Note that, besides the segmentation of the score data\ninto blocks, the JumpDTW approach completely relies on\ncontent-based comparison of notes and acoustic data. If\nfurther structural information from the score can be incor-porated, as for example tempo directives or jumps and re-\npeats as suggested by the notation, many of the remaining\nissues and inaccuracies might be solved. Besides this, an-\nother direction of future work may be to incorporate the\ncaseofcadenzas,wheretheperformancecontainssections\nthat are not writtenin the score.\nAcknowledgement. This work was supported by the Ger-\nman Research Foundation (DFG, CL 64/6-1) and by the\nCluster of Excellence on Multimodal Computing and In-\nteraction at Saarland University. We would like to thank\nthe anonymous reviewers for their very helpful comments\nand suggestions.\n6. REFERENCES\n[1] D.Damm,C.Fremerey,F.Kurth,M.M ¨uller,andM.Clausen. Multi-\nmodal presentation and browsing of music. In Proc. ICMI, pp. 205–\n208,Chania,Crete, Greece, 2008.\n[2] M.E. Tekin, C. Anagnostopoulou, and Y. Tomita. Towards an intel-\nligent score following system: Handling of mistakes and jumps en-\ncounteredduringpianopracticing.In ComputerMusicModelingand\nRetrieval, pp.211–219,2005.\n[3] B. Pardo and W. Birmingham. Modeling form for on-line following\nof musical performances. In Proc. National Conference on Artiﬁcial\nIntelligence, Pittsburgh, Pennsylvania, 2005.\n[4] A.Arzt,G.Widmer,andS.Dixon. Automaticpageturningformusi-\ncians via real-time machine listening. In Proc. ECAI, Patras, Greece,\n2008.\n[5] R. Dannenberg and N. Hu. Polyphonic audio matching for score fol-\nlowing and intelligent audio editors. In Proc. ICMI, pp. 27–34, San\nFrancisco, USA, 2003.\n[6] R. Turetsky and D. Ellis. Ground-truth transcriptions of real music\nfrom force-aligned MIDI syntheses. In Proc. ISMIR, pages 135–141,\nBaltimore, Maryland, USA, 2003.\n[7] S. Salvador and P. Chan. Toward accurate dynamic time warping in\nlinear time and space. In Intelligent Data Analysis , 11(5):561–580,\n2007.\n[8] R.DannenbergandC.Raphael.Musicscorealignmentandcomputer\naccompaniment. Comm. ACM, Special Issue: Music Information Re-\ntrieval, 49(8):38–43,2006.\n[9] C. Raphael. A hybrid graphical model for aligning polyphonic audio\nwith musical scores. In Proc. ISMIR,Barcelona, Spain, 2004.\n[10] J.Pickens,J.P.Bello,G.Monti,T.Crawford,M.Dovey,M.Sandler,\nand D. Byrd. Polyphonic score retrieval using polyphonic audio. In\nProc. ISMIR,Paris, France, 2002.\n[11] I. Suyoto, A. Uitdenbogerd, and F. Scholer. Searching musical audio\nusing symbolic queries. IEEE TASLP, 16(2):372–381,2008.\n[12] C. Fremerey, M. Clausen, M. M ¨uller, and S. Ewert. Sheet music-\naudioidentiﬁcation.In Proc.ISMIR,pp.645–650,Kobe,Japan,2009.\n[13] M. M ¨uller and D. Appelt. Path-constrained partial music synchro-\nnization. In Proc. ICASSP, pp. 65–68, Las Vegas, Nevada, USA,\n2008.\n[14] M. Goto. A chorus section detection method for musical audio sig-\nnals and its application to a music listening station. IEEE TASLP,\n14(5):1783–1794,2006.\n[15] M. Levy and M. Sandler. Structural segmentation of musical audio\nby constrained clustering. IEEETASLP, 16(2):318–326,2008.\n[16] J. Serr `a, E. G´omez, P. Herrera, and X. Serra. Chroma binary simi-\nlarity and local alignment applied to cover song identiﬁcation. IEEE\nTASLP, 16:1138–1151,2008.\n[17] M. M ¨uller.Information Retrieval for Music and Motion. Springer,\n2007.\n248\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "Automated Music Slideshow Generation Using Web Images Based on Lyrics.",
        "author": [
            "Shintaro Funasawa",
            "Hiromi Ishizaki",
            "Keiichiro Hoashi",
            "Yasuhiro Takishima",
            "Jiro Katto"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1415996",
        "url": "https://doi.org/10.5281/zenodo.1415996",
        "ee": "https://zenodo.org/records/1415996/files/FunasawaIHTK10.pdf",
        "abstract": "In this paper, we propose a system which automatically generates slideshows for music, by utilizing images re- trieved from photo sharing web sites, based on query words extracted from song lyrics. The proposed system consists of two major steps: (1) query extraction from song lyrics, (2) image selection from web image search results. Moreover, in order to improve the display dura- tion of each image in the slideshow, we adjust image tran- sition timing by analyzing the duration of each lyric line in the input song. We have conducted subjective evalua- tion experiments, which prove that the proposal can gen- erate impressive music slideshows for any input song.",
        "zenodo_id": 1415996,
        "dblp_key": "conf/ismir/FunasawaIHTK10",
        "keywords": [
            "Automatic music slideshow generation",
            "Query extraction from lyrics",
            "Image selection from web",
            "Image transition timing adjustment",
            "Subjective evaluation experiments",
            "Song lyrics analysis",
            "Photo sharing web sites",
            "Display duration improvement",
            "System architecture overview",
            "Lyric line duration analysis"
        ],
        "content": "AUTOMATED MUSIC SLIDESHOW GENERATION USING \nWEB IMAGES BASED ON LYRICS  \nShintaro Funasawa †   Hirom i Ishizaki ‡   Keiichiro Hoashi‡ \nYasuhiro Takishima ‡   Jiro Katto † \n†Waseda University  \nshint@katto.comm.waseda.ac.jp, \nkatto@waseda.jp  ‡KDDI R&D Laboratories Inc.  \n{ishizaki,hoashi,takisima}@ kddilabs.jp   \nABSTRACT  \nIn this paper, we propose a system which  automatically \ngenerates slide shows for music, by utiliz ing images r e-\ntrieved from photo sharing web sites , based on  query \nwords  extracted from song lyrics.  The proposed system \nconsists of two major steps: (1) query extraction  from \nsong lyrics, (2) image se lection from web image search \nresults. Moreover, in order  to improve  the dis play dura-\ntion of each image in  the slideshow, we adjust  image tran-\nsition  timing by analyzing the duration of each lyric line \nin the input song. We have conducted s ubject ive evalu a-\ntion experiments, which prove that  the propos al can gen-\nerate impressive music slideshows  for any input song.   \n1. INTRODUCTION \nMusic video, i.e., a series of visual content displayed with \nmusic, is a popular and effective way to increase the en-tertainability o f the music listening experience. The syn-\nergetic  effect generated by combining visual and a udio \nsignals is known as the sympathy phenomenon in the field \nof psycholog y [1]. W hile it is easy to enjoy music videos \ncreated by others  (usually by experts) , it is  extremely di f-\nficult for common users to create music video by them-\nselves . Namely, the cost to collect video and/or image \nmaterial that is suitable for the selected music  is expe n-\nsive. Furthermore, the  editing  process to fuse the material \nwith music  also r equires much intensive effort .  \nAn important factor which reflects the image of a song \nis its lyrics.  Many songs have lyrics which impressively \nrepresent  its visual scenery, which are difficult to be ex-\ntracted from  their acoustic features . Numerous resear ch \neffort s focus ing on song lyric  analysis have been pr e-\nsented recently . For e xample , extraction of song genre, \ntopic and mood, have been  investi gated in recently pr e-\nsented work [2- 5]. \nThis paper proposes a syste m which generates a music \nslide show automat ically, by using images retrieved from \nthe web based on query words that are derived from song \nlyrics . By utilizing images from the w eb, which provides \nan abundant and d iverse resource of images , our proposal is able to generate slide shows of wide variety, without \napplying any burden to the user. In order to generate such \na system, we focus on two major issues. One is the aut o-\nmatic extraction of words from the lyrics that are appr o-\npriate for web ima ge search . The other is to select an op-\ntimal image to be dis played with each lyric line,  from the \nset of candidate images obtained by web i mage search. \nIn this paper, we firstly propose a query extraction m e-\nthod from song lyrics based on the frequency of social \ntags attached to retrieved images . This method is effe ctive \nto generate appropriate queries  to avoid the retrieval of \nimages that are unsui table for slideshows . Secondly , we \npropose a method which  select s image s from the search \nresult s, based on entire  impression of the  song lyrics. This \nmethod  is expected  to increase the unity  among the i m-\nages within the slideshow . Moreover , we a pply a method \nto adjust image transition time  within the slid eshow , by \nanalysis of the duration time per lyric line. S ubjective  us-\ner evaluations will show that the propos al is capable  of \ngenerating high- quality musi c slideshows aut omatically.  \n2. RELATED WORK  \nMainly, two types of methods h\n ave been proposed for au-\ntomatic generation of visual content from  music. One is to  \ngenerate visual contents using personal videos and/or photos [ 6-8], and the other is to utilize  web images \n[9][10]. An advantage for using personal videos/photos  is \nthat the  resulting slideshow will be more familiar  to the \nuser.  However,  in order to generate high- quality slid e-\nshows, a suff icient amount of personal material  must be \nprepared, which is a heavy burden for casual u sers. \nThe web image- based approach  has two major issue s: \nquery selection and image selection . Appropriate sele c-\ntion of query words is expected to be effective for the r e-\ntrieval of images for slideshows.  However, existing works  \n[9][10] have utilized na ive methods for query word sele c-\ntion, such as stop word rejection,  and selection of spec ific \nparts of speech (e.g., nouns). U sing values to measure the \nsignificance of words, e.g.  TF*IDF , can be utilized to s e-\nlect query words  which are significant with in the lyrics . \nHowever, it is unclear whether or not  such measures are \nappropriate to select query words for web image search  to \ngenerate slideshows .  \nFor the image selection problem , an idea has been pro-\nposed in [10] to select images containing human faces  and  \nPermission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fee pr ovided that copies \nare not made or distri buted for profit or commercial advantage and that \ncopies bear this notice and the full citation on the first page.   \n© 2010  International Society for Music Information Retrieval  \n63\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)out\ndoor scenery . However, no evidence has been pr o-\nvided  that such images are optimal for music slideshows. \nA naive approach is to use  the top-ranked  images in the \nsearch result s for the image selection . In this case though, \nhighly ranked images are expected to be selected repet i-\ntively for the same query, hence, the same image may  be \nused for different songs  with similar lyrics . Therefore, t his \napproach is expected to generate slid eshows with a lack \nof diver sity, which may cause boredom for system users.  \n3. SYSTEM CONFIGURATION \nThe configura tion of the  proposed system  is illustrated in \nFigure 1. The system selects one i mage for each lyric line \nof the input song. The selected image is displayed on the \nslideshow application  (Figure 2 ) during music play . Im-\nages for the slide show are collected from Flickr , a highly \npopular photograph sharing site [ 11], by using the Flickr \nAPI. As illustrated in Fig ure 1, we assume that a database \nwhich contains songs with their corresponding lyrics and timing in formation is prepared  beforehand, as in the case \nof karaoke systems.  \n \nFigure 1.  System configuration  \nThe process flow of the system consists of the follo wing \nthree steps.  1.  Candidate  Image  Retrieval  \nThis step extracts a candi date set of images per lyric \nline, by selecting appropriate query words from each line of the lyrics of the input song.  \n2.  Image Selection  \nThis step selects an image from the previously ex-tracted candidate image set  for each line, to compose  \nthe slide show.   \n3.  Synchronized Playback  \nSelected images for each line are displayed with the \nsong, according to the prepared timing inform ation. \n \nFigure 2.  A screenshot of the p roposed system  \nThe following section explains the slideshow gener a-\ntion method, namely the ca ndidate image retrieval and \nimage selection steps, in d etail.  \n4. SLIDESHOW GENERATION  METHOD  \n4.1 Candidate  Image  Retrieval  \nIn this step, the system generates a query (set of words) \nfor each lyric line  of the input song. T he image search r e-\nsult from Flickr , obtai ned by the  generated query is ut i-\nlized as the candidate image  set for the lyric line . The \nquery is generated by analyzing the frequency of query words that are applied to the  image s in the search result , \nas social tags . This method is based on the hypothes is that, \nquery words which are frequently used as so cial tags in \nFlickr  have a significant meaning in the web image dat a-\nbase, thus are expected to be effective to retrieve i mages \nwhich are expressive of the song lyrics. This method e x-\ntracts the optimum com bination of query words for each \nlyric line , based on  the following three ideas : \n Words used in a lyric line should be  prioritized , since \nsuch words accurately  represent the con tent of the line . \n The query should be  composed with as many words as \npossible , since such queries are more specific than sin-\ngle word queries, thus should result in more accurate image retrieval.  \n Multiple  words within a query tend to  co-occur  as im-\nage social tags .  \n4.1.1 Process Flow of Social Tag-B ased Query Selection  \nLet N\nline(li) represe nt the set of nouns used at the i- th line \nof the lyrics, N para(li) represent the nouns used in the p a-\nragraph which contains the i- th line, and N all(m) repre sent \nthe word set which describes the general  impression of \nsong m (hereafter referred to as “genera l impression \nwords ”, details explained  in Sec tion 4 .1.2). Furthermore, \nwhen W ex presses the set of words used as the query for ARTIST  TITLE \nLYRICS  \n64\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)the\n Flickr API, let DF( W) (Document Frequency) \nrepresent the number of images in the search results, and \nUF(W) (User Frequency) r epresent the number of unique \nusers (counted by the user ID inform ation of the Flickr \nimages) in the search results.  \nThe proposed method  extracts  candidate query words \nfor the i- th line in lyrics of music piece m, from  Nline(li) \nand Npara(li). Words which  have DF or UF value less than \na pre- defined  threshold are omitted. The thr esholds for \nDF and UF are empirically set  as 40 and 10, respe ctively.  \nNext, let P(Nline(li)) express the power set of Nline(li): \nP(Nline(li))={Wline,1, Wline,2, … Wline,x}, where W line,x ex-\npresses the x-th set of words in P (Nline(li)). From \nP(Nline(li)), Wmax is selected under the condition that \nDF(Wmax) is not zero and that | Wmax| is the highest in \nP(Nline(li)), where | W| expresses the number of words in \nW. If more than one Wmax can be selected, the set which \nhas the high est UF( Wmax) is selected. In this way, Wmax is \nregarded  as the set of quer ies for the i -th line,  Qline(li).  \nThen,  in order to maximize the number of query words \n(which is assumed to reduce the number of candidate i m-\nages, and improve search accuracy ), we expand the query \nby using words in N para(li). Namely, expanded set s of \nwords, which are composed of the power set of N para(li), \nplus the previously derived Qline(li) are generated as \nP’(Npara(li))={Wpara,1+Qline(li), Wpara,2+Qline(li), … Wpa-\nra,y+Qline(li)} = {W’ para,1, W’ para,2, … W’ para,y}. Then, in \nthe same way as explained above, W’ max is selected from \nP’(Npara(li)).  \nFinally, by sending the all elements of W’ max under the \ncondition of ‘ AND ’ combination to Flickr, the system r e-\ntrieves the candidate images  for each line. If W’ max has no \nelements, Nall(m) is used as the query.  \n4.1.2 Estimation of General Song Impression  \nAs mentioned above, N all(m) is the general impression \nword set , i.e., a set of words  which expresses  the collec-\ntive impression of song m. This word set can be used for \nlyric lines from which no effective query words could be \nextracted. Furthermore, the general impression word set is \nalso effective to generate slideshows with a sense of un ity, \nas will be described  in the next section .  \n The  general impression of a song is estimated by text-\nbased classif ication  based on its entire lyrics . Namely, \nsong classifiers are preliminarily constructed by  SVM  \n[12] for each of the categories showed in Table 1 . The \ncategories are divided into three  concepts:  Season, \nWeather, and Time. Each concept consists of several cat-\negories.  The concepts/categories in Table 1 are selected \nbecause they all represent important aspects of song ly rics, \nand are expressed by discriminative word s. For the cla s-\nsifier, we used the software SVMlight [13] with a linear \nkernel for learning. Here, lyrics have  been vect orized by \nTF*IDF, and the training data for the classifiers learning have been obtained by a manually collected d atabase of \nJapanese pop  songs with human- applied labels . If the \nclassifier determines that a song m is positive for its r e-\nspective category, the name of the cat egory is added to \nNall(m). Note that multiple words may be included in \nNall(m).  \n \nConcepts  Category labels  \nSeason  Spring, Summer, Autumn, Winter  \nWeather Sunny, Cloudy, Rain, Snow, Rai nbow \nTime  Morning, Daytime, Evening, Night  \nTable 1.  Concepts and category labels for describing  \ngeneral  impression of music.  \n4.2 Image Selection  \nThe next step is to select  an image  to compose the slid e-\nshow from the candidate image  set for each lyric line. We \npropose an image selection method based on an impre s-\nsion score , which represents strength of association b e-\ntween the image and the general  impression words of the \ninput song. Consider ation of the impression score is e x-\npected to select images that are more fit ting to  the overall \ntheme of the input song, thus increases  the sense of  unity \namong the images which compose the slideshow.  \n4.2.1 Relevan t Tag Extraction Based on Co-occurrence \nProbability \nRelevan t tags for calculating the impression score are e x-\ntracted based on co -occurrence probability of social tags \non Flickr. In this paper, the co-occurrence probability is \ncalculated  based on UF  instead of DF, since there are \nmany  tags with unusually high DF on Flickr , due to  users \nwho upload many  images with the exact same tag  set, \nwhile UF is more robust to the effect of such user  beha-\nvior.  \nThe relevance score between a general  impression \nword nall ∈ Nall(m), and a given tag t, is calculated by the \nco-occurrence probabilit y of t and nall, and also the i m-\npression words which belong to the same concept  as nall. \nFor example, when the relevance score between “ sum-\nmer” and tag t is calculated, th e same score for all other \ngeneral i mpression words in the “ Season ” concept, i.e., \n“spring” , “autumn” , and “ winter ”, are also calculated. In \nthis way, it is possible to extract tags which have specif i-\ncally high relevance to n all, and decrease the score of gen-\nerally popular tags, i.e., words which co-occur fr equently \nwith many other  words.  \nThe co-occurrence score  between general  impression \nword nall and tag t , CoScore (t,nall), is defined  as: \n( )( )\n( )allall\nallnUFn t UFn t CoScore∩= ,       (1) \n65\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)The\nn, the  relevance score R between nall and t is defined \nas: \n( ) ( )( )\nwgtCn t P\nn t CoScore n t Ralln n C n\nall all ×−− =∑\n≠ ∈\n1|\n, ,, (2) \nwhere C is the set of general  impression words which be-\nlong to the same concept of  nall. For example, w hen nall = \n“spring” , C = {“spring” , “summer ”, “autumn” , “winter ”}, \nsince “spring” belongs to the “ Season ” concept . In the \ndefinition of the relevance score in  Eq.(2), the first term \nincreases the score of  tags which have high co -occurrence \nprobability with  nall. Subtraction of the second term de-\ncreases  the score of  tags with high co-occurrence proba-\nbility  of impression words which belong to the same co n-\ncept as  nall. Note that wgt  is a coefficient to adjust  the im-\npact of the second term . This coefficient  is set to  3, e m-\npirically . \nBased on Eq.(2), t he relevance score between each \ngenera l impres sion word, and all tags which co -occur \nwith the ge neral impression word, are calculated . Tags \nwhose  relevance scores are over  0.024, and UF value  ex-\nceeds  5, are regarded as relev ant tags of each impression \nword . \n4.2.2 Definition of Impression S core \nFor i mage selection, we calculate t he impression score  for \nall images in the candidate image set, based on the tags \napplied to the image, and the  above relevance score. The \nobject of this method is to select image s with tags which \nhave  high relevance to the general  impression words of \nthe input song. As a result of this process, the impression \nscore of image s with “noisy ” tags, i.e., tags with low r e-\nlevance to the gene ral impression  of the input song, will \nbe degraded.  \nThe impression score of image i  is determined by  \n( )( )\n( )\n( ) ( )∑∑\n∈∩ ∈\n∩ −=\nm N n all related i in T T tall\nall allall related i\nn T T Tn t R\ni score,\n    (3) \nwhere Ti is the set of tags applied to  image i, Trelated(nall) \nis the relevance tag set of general  impression word n all, \nand R(t, nall) is the r elevance score between n all and tag t .  \nThis impression score is computed for  each candidate \nimage , and the  image with the highest score is s elected to \nbe displayed with its respective lyric line, during the sli-deshow . \n4.3 Image Transition Timing Adjustment  \nIn the  proposed system, the images obtai ned per lyric line \nare displayed in synchronization with each line during the  \nsong playback . Adequate usage of the line information  \nleads to natural image transition  during the slideshow,  since lines represent a semantic unit in the lyrics. Howe v-\ner, displa y duration of each image may be too short/long \nwhen using the line information naively . For example, in \na rap song with many lines, the image di splay time  maybe \ntoo short, so that users may not be able to comprehend the \nimages in the slideshow. On the othe r hand, in a slow ba l-\nlad song, image s may be displayed for a long time, which \nmay cause boredom.   \nIn order to improve the overall quality of the slid eshow, \nwe propose an image transition  timing  adjustment m ethod,  \nwhich adjusts the  display time  of images  according to  the \nduration of each lyric line . In this pro cess, we first e sti-\nmate the ty pical d uration time of im ages in a song. Then, \nthe line of lyrics is “combined ” or “divided ”, based on the  \ndiffe rence of the dur ation of the line and the typ ical dur a-\ntion time  of the input song. In the “ combi ning” process, \nlines with short dur ation time are combined with the ir ad-\njacent  lines , and a single  image is displayed for the co m-\nbined set of lines . In the “ dividing” process , lines with \nlong dur ation time are “ divided ” into plural sub -lines, and \nan image is to be  displayed  along with each sub-line.  \nThe process flow for image transition  timing  adjus t-\nment consists of the fo llowing steps.  \n1. Typical duration time  of song m is calcu lated from the \nlyrics data. Namely,  the mode value of the line dur a-\ntion time is used as the  typical image dur ation time  I\nm. \n2. Lines  which have  less than 4 [sec] duration  time are \n“combined ” with the next line . If there is no next line, \nit is “ combined ” with the previous line. However , line s \nare “combine d” only if they belong to the same par a-\ngraph. An imag e is retrieved for the new ly combined  \nline. \n3. A line whic h  has more than 12 [sec] duration  time is \n“divided ” equally. The number of division s is con-\ntrolled  so that approximate  duration time of the new  \n“divided ” line is equivalent to Im. When the line is “d i-\nvided” into n lines, n images are displayed from the \ncandidate image  set, which is retrieved based on the \nlyrics of the  original  line.  \n4. Interlude section s (which generally have no lyrics) are \ndivided by  the same process as step 3. The general  im-\npression wor ds are used as query for image r etrieval.  \n5. EXPERIMENTS \n5.1 Outline  \nIn order to evaluate the quality of the proposed method, we have conducted  a subje ctive evaluation  experiment . \nThis experiment compares the proposed method with ot h-\ner conventional methods, by as king 42 subjects to rate the \nslideshows generated by all methods. The subjects are \nasked to view the music slideshows of the same song, \nwhich are generated by the proposed and comparative \nmethods (details of the methods are explained in Section \n66\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)5.2). \nThen,  each subject is asked to apply  a five -ranked \nrating for each slideshow , based on the  following  evalua-\ntion measures:  \na) Accordance between lyrics and images  [content] \nb) Appropriateness of image display time [dur ation]  \nc) Unity of all images in slideshow  [unity]  \nd) Overall quality  [quality]  \nIn this experiment , we use 10 Japanese pop songs and \n28 ~ 29 subjects have provided evaluation results for each \nsong.  In order to evaluate the method described in Section \n4.3, we have selected songs so that h alf of these songs in-\nclude “combined ” lyric lines  (hereafter referred to as the \n“combined set ”) in the process of adjustment of image \ntransition  timing e xplained in Section 4.3 , and the other  \nhalf include “divided ” lines  (hereafter referred to as the \n“divided set ”). \n5.2 Evaluated Met hods  \nThe next three methods were evaluated and compared. \nA) MusicStory [9] \nThe first comparative method generates slideshows \nbased on the method proposed for MusicStory [9]. Nam e-\nly, all nouns are extracted from the entire lyrics of the i n-\nput song, and are sent to the Flickr API under the  ‘OR’ \ncombination. The images in  the search result are di s-\nplayed according to the transition  timing determined by \nthe BPM (beats per minute) of the input song \nB) TF*IDF based method \nThe second comparative method extracts query words \nfrom the lyrics  based on TF*IDF. The process flow to ob-\ntain the image for the i- th line in the lyrics is described as \nfollows. First, the nouns extracted from the i- th line in the \nlyrics, are sent to Flickr as query under the condition of \n‘AND ’ combinat ion. If the result has no image s, the noun \nwith the smallest TF*IDF is removed, and the rest of the nouns are sent to Flickr again.  This process is  repeated \nuntil a set of images are obtained. If the sy stem is unable \nto retrieve images by any of the nouns in the line, the im-\nages from  the previous line are re- used. Finally, the high-\nest-ranked image in the search result (according to the \nFlickr “interestingness ” ranking)  is selected. Images are \nobtained for each line and switched in synchroniz ation \nwith line appearance within input song. In this paper, the \nDF element of TF*IDF is calculated based on our dat a-\nbase, which contains 3062 Japanese pop songs.  \nC) Propos ed method \nThe third method is our proposal . Queries are generat-\ned from the lyrics by the  social tag -based method, i mages \nare selected from the image search results based on the impression score, and the image transition  timing is ad-\njusted by the method described in Se ction 4.3. 5.3 Experimental Results  \nFigure 3 shows the average rating of all subjects , for each  \nevaluation measure and method . The result s in this Fig ure \nshow  that the propos ed method has received the highest  \nratings , compared  to the other methods for all evaluation \nmeasures . Most significantly, the proposed method has \nreceived the best rating for t he overall quality, a diffe r-\nence which is statistically significant to the others based on t-test (p<0.001).  These results prove that the proposed \nmethod is capable of generating high -quality slid eshows.  \n1 2 3 4 5 \ncontent duration unity qualityAverage\nEvaluation MeasureMusicStory TF*IDF PROPOSAL\n \nFigure 3.  Average rating s of each evaluation meas ure. \n \nLyrics \nline “If this separation  means  departure,  I will \ngive my all smile s to you.” \nQuery  TF*IDF based  “departure ” (line word)  \nProposal  “smile ” (line word)  \nLyrics \nline “Will the memory of our encounter  and the \ntown we ’d walked in be kept in our heart? ” \nQuery  TF*IDF based  “heart ” (line word)  \nProposal  “town ” (line word)  \nLyrics \nline “I wish I could stay with you for even a \nmoment. ” \nQuery  TF*IDF based  “moment ” (line word)  \nProposal  “car” and “night scene ” \n(paragraph words)  \nLyrics \nline “But the shining  days will never  return  to \nme today  or tomorrow. ” \nQuery  TF*IDF based  “today ” (line word)  \nProposal  “evening ” \n(general impression word)  \nTable  2. Examples of  image search queries generated by \nTF*IDF based and proposed methods. \nIn order to analyze t he query selection process of the \nproposed method, we compare the queries generated by \nthe proposal to those of the TF*IDF based method . Ex-\namples are written  in Table 2. This table shows examples  \nof lyrics line s (English translation s by the authors  from \nthe original Japanese lyrics ) and the queries generated \nfrom the lines by the two methods. In the first  two exam-\nples in this table, it is clear that the proposal has  succes s-\n67\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)fu\nlly selected  words which represent visual concepts . \nContrarily, the TF*IDF method h as selected words which \nare important, but also are difficult to be represented in a \nvisual manner . This is due to the characteristic of the pr o-\nposed method, which considers  the UF values of the \nwords in Flickr . Furthermore , when there are no “ visual ” \nword s in the lyrics, the proposal can appropriately gener-\nate queries , either from the lyric paragraph , or general \nimpression  word s, as shown in the last two examples . \nThese examples indicate that the proposed method is e f-\nfective to ge nerate good quer ies from a ny song lyric . \nMoreover, even when the quer ies generated by the \nboth method s are the same , the proposal is capable of se-\nlecting more suitable images for the song. For example, \nwhen both methods retrieve images by the query “town ” \nfor a winter song, the pro posal appropriately selects an \nimage of a town with falling snow, while the TF*IDF \nbased method selects a general image of a town. Exam-\nples like this indicate that the proposed image selection \nmethod based on impression score can generate suitable \nslidesho ws which represent the overall theme of the song.  \nAdditionally, in the “ duration ” measure, t he proposal  \nhas achieved ratings  superior to the TF*IDF based me-\nthod for 9 songs , indicating  that the proposed adjus tment \nmethod has succeeded in improving slidesh ow qua lity. \nThe difference of  the average ratings between the propo s-\nal and the TF*IDF based method for “ combined sets ” is \n0.21, while  the difference for “divided sets ” is 0.61. This \nresult implies  that the proposed method is more e ffective \nto improve slide shows for songs  with lyrics that are slo w-\nly sung, as  in slow ballads.  \n6. CONCLUSIONS AND FUTURE WORK  \nIn this paper, we have proposed a system to generate sl i-\ndeshows for any given song, by using words in their ly rics \nto retrieve web images. We have proposed a query gene r-\nation method for image search and an image selection  me-\nthod to compose slideshows from the image search results. \nMoreover, we proposed a method to adjust image trans i-\ntion timing based on the lines of lyrics. Results of subje c-\ntive evaluation s have shown that our sy stem can generate \nhighly satisfactory music slid eshows.  \nIn the future, we plan to expand our system to utilize  \nnot only the lyrics , but also the acoustic features of the \ninput song. For example, display ing slideshows with var i-\nous effect s, such as zooming and panning, in accordance \nwith the excitement of the song; as well as the use of beat \ninformation for image transition all are expected  to im-\nprove the impression of the generated slideshow s.  \n7. REFERENCES  \n[1] S. Iwamiya: “ The interaction betw een auditory and \nvisual processing when listening to music via audio -visual media ,” The Journal of the Acoustical Society \nof Japan, Vol.48, No.3, pp.146- 153, 1992. [in \nJapanese]  \n[2] R. Mayer, R. Neumayer, and A. Rauber: “Rhyme \nand Style Features for Musical Genre Classification by Song Lyrics, ” Proceedings of ISMIR 2008, pp. \n337-342, 2008.  \n[3] F. Kleedorfer, P. Knees, and T. Pohle: “Oh Oh Oh \nWhoah! Towards Automatic Topic Detection in Song Lyrics, ” Proc eedings of ISMIR 2008, pp. 287-\n292, 2008.  \n[4] Y. Hu, X. Chen, and D. Yang: “ Lyric -based Song \nEmotion Detection with Affective Lexicon and Fuzzy Clustering Method, ” Proceedings of ISMIR \n2009, pp. 123- 128, 2009.  \n[5] X. Hu, J. S. Downie, and A. F. Ehmann: “L\n yric Text \nMining in Music Mood Classification, ” Proceedings \nof ISMIR 2009, pp. 411- 416, 2009.  \n[6] X. -S. Hua, L. Lu, and H. - J. Zhang: “P- Karaoke: \nPersonalized Karaoke System, ” Proceedings of the \n12th Annual ACM International Conference on Multimedia , pp.172- 173, 2004.  \n[7] T. Terada, M. Tsukamoto, and S. Nishino: “A \nSystem for  Presentin g Background Scenes of \nKaraoke Using an Active Database System,” \nProceedings of the ISCA 18th International Conference on Computers and Their Applications , \npp. 160- 165, 2003.  \n[8] S. Xu , T. Jin, and F. C. M. Lau:  “Automatic \nGeneration of Music Slide Show using Personal \nPhotos ,” Proceedings of 10th IEEE International \nSymposium on Multimedia, pp. 214- 219, 2008. \n[9] D. A. Shamma, B. Pardo, and K. J. Hammond: “MusicStory: a Personalized Music Video Creator, ” \nProceedings of the 13th Annual ACM International Conference on  Multimedia , pp.563- 566, 2005.  \n[10] R. Cai , L. Zhang, F. Jing, W. Lai, and W. -Y . Ma,: \n“Automated Music Video Generation Using Web \nImage Resource,”  Proceedings of IEEE \nInternational Conference on Acoustics, Speech, and Signal Processing, 2007, Vol.2, pp. 737- 740, 2007. \n[11] Flickr: http://www.flickr.com/ \n[12] C. Cortes and V. Vapnik: “Support Vector \nNetworks, ” Machine Learning, Vol. 20, pp.273- 297, \n1995.  \n[13] SVM -Light Support Vector Machine: \nhttp://svmlight.joachims.org/ \n68\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "Evaluation of a Score-informed Source Separation System.",
        "author": [
            "Joachim Ganseman",
            "Paul Scheunders",
            "Gautham J. Mysore",
            "Jonathan S. Abel"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1416062",
        "url": "https://doi.org/10.5281/zenodo.1416062",
        "ee": "https://zenodo.org/records/1416062/files/GansemanSMA10.pdf",
        "abstract": "In this work, we investigate a method for score-informed source separation using Probabilistic Latent Component Analysis (PLCA). We present extensive test results that give an indication of the performance of the method, its strengths and weaknesses. For this purpose, we created a test database that has been made available to the public, in order to encourage comparisons with alternative methods.",
        "zenodo_id": 1416062,
        "dblp_key": "conf/ismir/GansemanSMA10",
        "keywords": [
            "source separation",
            "Probabilistic Latent Component Analysis",
            "PLCA",
            "score-informed",
            "test results",
            "performance",
            "strengths and weaknesses",
            "test database",
            "publicly available",
            "alternative methods"
        ],
        "content": "EVALUATION OF A SCORE-INFORMED SOURCE SEPARATION SYSTEM\nJoachim Ganseman, Paul Scheunders\nIBBT - Visielab\nDepartment of Physics, University of Antwerp\n2000 Antwerp, BelgiumGautham J. Mysore, Jonathan S. Abel\nCCRMA\nDepartment of Music, Stanford University\nStanford, California 94305, USA\nABSTRACT\nIn this work, we investigate a method for score-informed\nsource separation using Probabilistic Latent Component\nAnalysis (PLCA). We present extensive test results that\ngive an indication of the performance of the method, its\nstrengths and weaknesses. For this purpose, we created a\ntest database that has been made available to the public, in\norder to encourage comparisons with alternative methods.\n1. INTRODUCTION\nSource separation is a difﬁcult problem that has been a\ntopic of research for several decades. It is desirable to\nmake use of any available information about the problem\nto constrain it in a meaningful way. Musical scores pro-\nvide a great deal of information about a piece of music.\nWe therefore use this information to guide a source sepa-\nration algorithm based on PLCA.\nPLCA [?] is a technique that is used to decompose mag-\nnitude spectrograms into a sum of outer products of spec-\ntral and temporal components. It is a statistical interpre-\ntation of Non-Negative Matrix Factorization (NMF) [ ?].\nThe statistical framework allows for a structured approach\nto incorporate prior distributions.\nExtraction of a single source out of a sound mixture by\nmodeling a user guidance as a prior distribution was pre-\nsented in [?]. In our previous work [?], we based ourselves\non that approach and extended it to a complete source sep-\naration system informed by musical scores, ﬁnally demon-\nstrating it by separating sources in a single real-world record-\ning.\nWe perform source separation by decomposing the spec-\ntrogram of a given sound mixture using PLCA, and then\nperforming reconstructions of groups of components that\ncorrespond to a single source. Before using PLCA on the\nsound mixture, we ﬁrst decompose synthesized versions of\nthose parts of musical scores that correspond to the sources\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc⃝2010 International Society for Music Information Retrieval.that we wish to separate (also using PLCA). The temporal\nand spectral components obtained by these decompositions\nof synthesized sounds are then used as prior distributions\nwhile decomposing the real sound mixture.\nIn this work we make a detailed evaluation of such source\nseparation system and its overall performance. To the best\nof our knowledge, a comprehensive and extensive dataset\nto use as ground truth for such problem does not exist,\nmainly because we also require the corresponding scores\nas additional information to the source separation system.\nWe therefore construct a test set of our own, mimicking\nrealistic conditions as well as possible even though it is\nsynthetic. This also allows us to make detailed evaluations\nof how the results are affected by common performance\npractices, like changes in tempo or synchronization. To get\nobjective quality measurements of this method, we use the\nmetrics deﬁned in the BSS EV AL\nframework [?], which\nare widely adopted in related literature.\n2. SCORE-INFORMED SOURCE SEPARATION\nWITH PLCA\nWe’re not the ﬁrst to propose source separation based on\nscore information. A method based on sinusoidal model-\ning has been proposed by Li [?], and Woodruff [?] used\nscores as information source for the separation of stereo\nrecordings. Our PLCA-based system for score-informed\nsource separation is set up as shown in ﬁg. ??:\n\u000fThe complete score gets synthesized;\n\u000fDynamic Time Warping (DTW) matches the spec-\ntrogram of the sound mixture to that of the score;\n\u000fThe resulting path is used to match single parts or\nsections from the score to the mix;\n\u000fComponents for each of the parts to extract are learned\nusing PLCA on separately synthesized parts;\n\u000fThese components are used as prior distributions in\nthe subsequent PLCA decomposition of the mix;\n\u000fWith the learned components ’ﬁtted’ to the mix, we\ncan now resynthesize only those components from\nthe mix that we want.\n219\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Figure\n1. Architecture of the score-informed PLCA-based\nsource separation system\nThe PLCA method that we adopt does not presuppose\nany structure, instead it learns the best representation for\na spectrogram through an EM (expectation-maximization)\nalgorithm. Both temporal and spectral components can as-\nsume any shape. The dictionary of spectral and tempo-\nral components resulting from decomposition of the syn-\nthesized score parts is only used to initialize the subse-\nquent PLCA decomposition of the sound mixture. The\nEM-iterations decomposing this mixture optimize those spec-\ntral and temporal components further in order to make them\nexplain the sources in the sound mixture. A drawback of\nPLCA is that it operates on magnitude spectrograms and\ndoes not take into account the phase, which easily leads to\nsome audible distortion in the resynthesized sounds.\nWe implemented our system largely in Matlab, with the\nDTW routine provided from [ ?]. We always work on mono\naudio. The method does not work in real-time - on a mod-\nern dual-core 3.0GHz computer with 4GB RAM memory,\nprocessing a 1 minute sound ﬁle (44100Hz samplerate)\ntakes about 3 to 4 minutes of calculation time with high\nquality settings. The DTW subroutine has a memory com-\nplexity that is quadratic in spectrogram size, due to the cal-\nculation of a complete similarity matrix between the spec-\ntrograms of the sound mixture and the synthesized score.\nAlternatives to DTW exist and could be used, there is e.g.\nprior work on aligning MIDI with audio without comput-\ning a complete rendering of the MIDI ﬁle (also available\nthrough [?]).It needs to be noted that the spectral and temporal com-\nponents of the synthesized score parts are initialized with\nrandom data. Starting from these random probability dis-\ntributions, the EM-algorithm then iteratively estimates bet-\nter candidates that ﬁt the data. The resulting estimates of\nthe components from the score data will be slightly dif-\nferent on each run. This will in turn affect the subsequent\nPLCA analysis of the real data and its path towards conver-\ngence. We will quantify this in more detail later, but it is\nimportant to keep in mind that all measurements presented\nin this paper are subject to a certain error margin that is a\ndirect result of this random initialization.\n3. TEST SETUP\nIn order to do large scale comprehensive testing of this\nmethod, we need a database of real sources and their scores\nwhich we can mix together and then try to separate. To\nthe best of our knowledge, a carefully crafted database\nfor research purposes containing separate sources and their\nscores for a wide range of instruments and/or styles does\nnot yet exist [?]. For source separation, evaluation databases\nwith multitrack recordings are available (e.g. [?]), but usu-\nally they don’t come with scores, MIDI ﬁles, or any other\nsymbolic information.\nWe decided to create our own database, generating short\nrandom MIDI tunes, and then running them through differ-\nent synthesizers. In testing, one of the synthesized sounds\nthen can take on on the role of ’real performance’, while\nthe other is used as ’synthesized score’. To better simulate\nreal performance, we generated several versions of each\nﬁle with tempos regularly changing, up to half or double\nthe speed of the original. This also allows the database to\nbe used to test alignment algorithms. The resulting dataset\nis available online1.\nWe generated a set of 10 second sound ﬁles using PortSMF\n[?]. The ﬁles were synthesized once using Timidity++\n[?] with the FluidR3 GM soundfont on Linux, and once\nwith the standard built-in MIDI player capabilities in Win-\ndows XP (DirectSound, saved to ﬁle using WinAmp [ ?]).\nEach ﬁle contains on average 20 note onsets, spread ran-\ndomly over 10 seconds. We reduced our test set to 20 com-\nmonly used instruments, both acoustic and electric. This\nwas done in part because of a lot of the sounds standard-\nized in General MIDI are rarely found in scores (helicopter\nsounds, gunshots), and to keep the size of the resulting data\nmanageable. With 380 possible duos with different instru-\nments out of 20 instruments, it allowed us to run repeated\nexperiments on all of these combinations.\nThis original test set of 20 sounds was expanded by in-\ntroducing timing variations in the MIDI ﬁles. Several sets\nof related ﬁles were generated, in which the tempo in each\n1https://ccrma.stanford.edu/˜jga/ismir2010/ismir2010.html\n220\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Figure\n2. Overall source extraction scores per instrument, mixed with any other instrument. On the x-axis, the MIDI\nProgram Change number. In this and all other ﬁgures, standard Matlab style boxplots are used.\nﬁle was changed 5 times - this to be able to test the effects\nof the method used to align symbolic data and recordings,\nwhich is part of the system. 2 distinctly different tempo\ncurves were deﬁned, and for each of these 2 curves, 5 new\nrenditions were made for every original source. The ﬁrst\nof these 5 would have the tempo changed by up to 10%,\neither slower and faster, while the last would allow devia-\ntions from the original tempo up to 50% . Thus we have\na dataset of 20 original sources, and for each original ﬁle\nalso 10 ﬁles with all different variations in tempo.\nWe acknowledge that there are a couple of drawbacks\nwith this dataset. The ﬁrst one that the ﬁles are randomly\ngenerated, while in most popular and classical music, har-\nmonic structure makes separation more difﬁcult due to over-\nlapping harmonic components. The second that even using\ntwo different soundbanks to synthesize can result in the\ntwo synthesized versions of a single ﬁle to be more similar\nto eachother than they might be similar to a real record-\ning. We found however that the timbres of the two sound-\nbanks used differ quite signiﬁcantly. As for the random\ngeneration: not using real data frees us from dealing with\ncopyright issues, and generating it randomly allowed us to\nquickly obtain a large and comprehensive body of test ﬁles,\nnot presupposing any structure or style.\nIn the following sections, we use the ﬁles generated on\nWindows as sources for the ’performance’ sound mixture,\nand the ﬁles rendered on Linux as ’scores’ from which\nwe obtain priors. The BSS EV AL\ntoolbox [?] calculates3 metrics on the separated sources given the original data.\nThe Signal-to-Interference Ratio (SIR) measures the inclu-\nsion of unwanted other sources in an extracted source; the\nSignal-to-Artefacts Ratio (SAR) measures artefacts like mu-\nsical noise; the Signal-to-Distortion Ratio (SDR) measures\nboth the interference and artefacts.\n4. MEASUREMENTS ON IDEAL DATA\n4.1 Error margin on the results\nAs mentioned previously, due to randomness in the ini-\ntialization, separation results might differ with every run,\nand so might the SDR, SIR and SAR scores. To properly\nquantify what we are dealing with, we ran the system with\nstandard parameters that give decent results (sampling rate\nof 44100Hz, 2048-point FFT with 75% overlap, 50 com-\nponents per source, 50 iterations) 10 times on each of the\npossible 380 instrument duos in the test set. This was done\nmin std max std mean std median std\nSDR 0.062 1.55 0.48 0.41\nSIR 0.056 14.21 1.89 1.20\nSAR 0.061 1.55 0.47 0.41\nTable\n1. Reliability of the results: statistics on the standard\ndeviation of SDR, SIR and SAR scores of 10 runs of the\nalgorithm with the same parameters on 380 data pairs.\n221\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Figure\n3. SDR, SIR and SAR vs. number of components\nused per source\non ’ideal’ data, where the score would exactly line up with\nthe sound mixture and no DTW was needed, so we com-\npute the effect of the random initialization only. SDR, SIR\nand SAR values tend to be pretty consistent in between\nruns on the same pair, except in a few rare combinations\nwhere there is a lot of variance in the results. Even in these\ncases, the mean values of the scores are still within normal\nrange.\nThe numbers in table ??show that the mean standard\ndeviation of calculated SDR and SAR scores stays below\n0.5 dB, while there can be highly variable results in some\nSIR scores. Incidences of high variance in SIR score seem\nunrelated to each other, and almost every instrument had\nsome combination with another instrument where SIR scores\nwould be very variable in between runs of the algorithm.\nFor evaluation purposes, SDR and SAR scores seem to be\nbetter suited to pay attention to.\nSome instruments seem to be easier to extract from mixes\nwith any other instrument, than others. Fig. ??gives an\nidea of the ’overall easiness’ with which an instrument can\nbe extracted from a mix.\n4.2 Components and iterations\nThe algorithm’s running time will increase linearly with\nthe amount of components. Generally, increasing the num-\nber of components that are available per source increases\nthe ability to model the priors accurately, and thus also the\noverall separation results. We ran a small test of the effect\nof the number of components across all couples of sources.\nThe effectiveness of the number of components is almost\nidentical for every instrument, so we can generally plot the\nnumber of components versus the outcome of the metrics,\nwhich is shown in ﬁg ??.\nWith on average 20 notes in each source, there is a huge\nFigure\n4. SDR, SIR and SAR vs. number of iterations in\nEM algorithm\nclimb in improvements up to 20 components after which\nthe scores level off. There is some small improvement\nafter this, but not drastic. We haven’t run complete tests\nwith signiﬁcantly larger amounts of components, but from\na couple of single tries we ﬁnd that overﬁtting becomes\nan issue when the amount of components is chosen too\nlarge. Superﬂuous components of a single source risk to\nstart modeling parts of other sources, which degrades sep-\naration performance again.\nThe number of iterations of the EM algorithm does not\nsuffer from this - since the likelihood of subsequent EM it-\nerations is monotonically increasing, more is always better.\nThe only constraint here is how much time we’re willing to\nspend on those iterations. We can see that the convergence\ntowards a good solution is obtained rather fast: indepen-\ndent of instrument, above 25 iterations there is hardly any\nimprovement of the scores (ﬁg. ??).\n4.3 Other parameters\nThe PLCA routine decomposes a magnitude spectrogram,\nand thus the properties of that spectrogram also play a role\nin the end result. Conducting a few small tests, we were\nable to conclude that the larger the FFT size, the better the\nresults generally are. In subsequent tests, we used 2048-\npoint FFTs. The overlap should be kept above 67.5% ;\n75% is a safe value. Binary masking (assigning each spec-\ntrogram time-frequency bin to a single source instead of\ndividing it among different sources) signiﬁcantly improves\nSIR scores, at the cost of a slight decrease in SDR and SAR\nscores.\nIt is possible to cut the spectrogram into ’timeslices’\nof variable length. Certainly when there are possibilities\nfor parallelization, or when due to system limitations the\nspectrogram size needs to be kept to a minimum, it might\nbe interesting to run the analysis on each slice separately.\n222\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Figure\n5. SDR, SIR and SAR vs. tempo deviation from\nreference, all sources with the same deviation\nThis makes that the spectral components, and their tem-\nporal counterparts, can change from slice to slice. Due to\nthe random initialization of the components, they are likely\nnot related to the components in other slices at all, and each\nslice will have its components deﬁned such that they rep-\nresent the data in that slice optimally. During resynthesis,\nsmall artefacts can be introduced on the slice borders due\nto these changes in basis vectors that occur. Our tests indi-\ncated that, though a decline in scores remains small and\nonly noticeable when slices were smaller than a second\nlong, it is a good idea to have the length of a slice as large\nas possible. How this relates to the number of needed com-\nponents or iterations remains to be studied in the future.\n5. THE EFFECT OF DYNAMIC TIME WARPING\n5.1 Quantifying the role of DTW\nWhereas in the previous section we discussed metrics on\nideally aligned data, this is not likely to occur in real life.\nPerformers use their artistic freedom to make the notes on\npaper into a compelling concert. One of the main means of\ndoing so are local changes in tempo. To cope with this, a\nDTW routine is attached at the beginning of the system. It\nserves to line up the score with the real performance.\nIn ﬁgure ??the performance of the algorithm on ide-\nally aligned sound mixtures (0% deviation from the score\ntempo) is compared to performance on mixtures with tempo\ndeviations, where alignment is needed. The sources that\nwere used were divided into 5 segments that each had a\ndifferent tempo assigned to them, in such a way that the\ntempo was in every ﬁle partly below the reference tempo,\nand partly above. The amount of change has a pretty high\ninﬂuence on the effectiveness of the subsequent source sep-\naration.Logically, the timing of an entire score applies to the\nindividual instrument parts too. We provide the output\nfrom the DTW routine to a phase vocoder that dilates or\ncompresses each of the synthesized parts in time, so that\nthey match up with the the performance mixture. This\nis a quick and practical solution to make sure that in the\nfollowing PLCA analysis steps, the temporal and spectral\ncomponents of the performance mixture and their associ-\nated priors obtained from the synthesized score parts, have\nthe same dimensions.\nBoth errors in alignment and the subsequent stretch-\ning of synthesized score parts introduce errors in the pri-\nors, which affect successful analysis. From the data in ﬁg.\n??, we conclude that heavy time warping and subsequent\nstretching of the spectrum puts the quality of the results at\nsevere risk. The DTW routine and phase vocoder that we\nused [?] were chosen because they were readily available\nto plug into our code. It is however a bottleneck in our\nsystem. In future work, alternative methods to align scores\nwith recordings are worth looking into [?]. If using DTW,\nin practical applications the possibility to manually correct\nor at least smoothen the time alignment should be avail-\nable.\nIn tests where source ﬁles with different tempo curves\nwere used in a single sound mixture (in order to simulate\nperformers that are out of sync with each other), very simi-\nlar results were observed. In such a case the time alignment\nis likely to contains errors for at least one of the sources,\nsince notes that should be played together according to the\nscore, are not necessarily played together in the mixture.\nWe can conclude that the application of DTW and sub-\nsequent time dilating and compressing of the synthesized\ndata with a phase vocoder can cause a considerable stir in\nthe computed priors, to such an extent that in the subse-\nquent decomposition of the mix, it becomes very difﬁcult\nto get decent separation results.\n5.2 Adaptations and alternatives\nThe DTW plus phase vocoder routine is the weak link in\nthe complete process, and we ventured on to do a couple\nof experiments adapting that part of our system. Inspired\nby recent work by Dannenberg et al [?] we substituted the\nspectrograms used in the DTW routine by chromagrams,\nusing code obtained from the same source [?]. The re-\nsults are practically equal to those in ﬁg. ??. Just like in\nthe case of DTW with spectrograms, some (manual) post-\nprocessing on the results of the DTW routine is likely to\nimprove the test results.\nWe also undertook a small experiment skipping the use\nof a phase vocoder to stretch the spectrograms of the scores,\ninstead only resampling the temporal vectors using piece-\nwise cubic Hermite polynomials to maintain nonnegativ-\nity. It turns out that the mean SDR and SAR scores plum-\nmet, and the standard deviation increases drastically, re-\nsulting in a small but not negligible number of test results\n223\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)that are\nactually better than what could be attained previ-\nously. Also, the SIR values stay remarkably high, and even\nat large tempo deviations. However, overall the system be-\ncame highly unreliable and unﬁt for general use.\nGiven that the parameters of the PLCA routine can be\nchosen optimally and that their effects are relatively well-\nknown, most of the future effort in improving this score-\ninformed separation system should clearly go into better\nand more accurate alignment and matching of the scores\nto the real performance data. Also more varied data and\nuse cases need to be considered - here, we only worked\non mixes of 2 instruments, and did not include common\nperformance errors like wrongly struck notes. Several ap-\nproaches to solve this problem, or parts of it, exist or are\nbeing worked on [?], and can contribute to a solution.\nFor alignment of scores with recordings, we have some\nfuture work set out, replacing the DTW and phase vocoder\nwith methods more ﬁt for our particular setup. In hind-\nsight, with symbolic data and performance recordings avail-\nable, we would very likely be better off applying a method\nthat directly aligns symbolic information with a single spec-\ntrogram, to then modify the timing of the symbolic data,\nonly then to synthesize it and compute priors from. For any\nfuture developments, we now have an extensive dataset to\nquickly evaluate the system.\n6. CONCLUSIONS\nIn this paper we quantiﬁed the performance of a recently\ndeveloped score-informed source separation framework based\non PLCA. Several parameter options were explored and we\npaid special attention to the effect of DTW. The use of met-\nrics that are prevalent in literature allows for future com-\nparison with competing methods. We synthesized our own\ntest dataset covering a wide range of instruments, using dif-\nferent synthesizers to mimick the difference between real-\nworld data and scores, and mimicking some performance\ncharacteristics by introducing tempo changes. This dataset\nhas been made freely available to the general public, and\nwe exempliﬁed its usability for extensive testing of align-\nment and source separation algorithms.\n7. REFERENCES\n[1] P. Smaragdis, B. Raj and M.V . Shashanka: “Super-\nvised and Semi-Supervised Separation of Sounds from\nSingle-Channel Mixtures,” Proc. of the 7th Interna-\ntional Conference on Independent Component Analysis\nand Signal Separation , London, UK, September 2007.\n[2] D. Lee and H.S. Seung: “Algorithms for Non-negative\nMatrix Factorization,” Proc. of the 2000 Conference on\nAdvances in Neural Information Processing Systems ,\nMIT Press. pp. 556562.[3] P. Smaragdis and G. Mysore: “Separation by ’hum-\nming’: User-guided sound extraction from mono-\nphonic mixtures,” Proc. of IEEE Workshop on Applica-\ntions Signal Processing to Audio and Acoustics , New\nPaltz, NY , October 2009.\n[4] J. Ganseman, G. Mysore, P. Scheunders and J. Abel:\n“Source separation by score synthesis,” Proc. of the\nInternational Computer Music Conference , New York,\nNY , June 2010.\n[5] C. F ´evotte, R. Gribonval and E. Vincent: “BSS EV AL,\nA\ntoolbox for performance measurement in\n(blind) source separation,” Available at http://bass-\ndb.gforge.inria.fr/bss eval/ ,\naccessed May 27, 2010.\n[6] Y . Li, J. Woodruff and D. L. Wang: “Monaural mu-\nsical sound separation using pitch and common am-\nplitude modulation,” IEEE Trans. Audio, Speech and\nLanguage Processing , vol. 17, no. 7, pp. 1361-1371,\n2009.\n[7] J. Woodruff, B. Pardo and R. B. Dannenberg: “Remix-\ning Stereo Music with Score-informed Source Sepa-\nration,” Proc. of the 7th International Conference on\nMusic Information Retrieval , Victoria, Canada, Octo-\nber 2006.\n[8] D. Ellis: “Matlab audio processing examples,” Avail-\nable at http://labrosa.ee.columbia.edu/matlab/ ac-\ncessed May 27, 2010.\n[9] A. Grecu: “Challenges in Evaluating Musical In-\nstrument Sound Separation Algorithms,” Proc. 9th\nInternational Student Workshop on Data Analysis\n(WDA2009) , Certovica, Slovakia, July 2009, pp. 3-9.\n[10] E. Vincent, R. Gribonval, C. F ´evotte et al,.\n“BASS-dB: the Blind Audio Source Sep-\naration evaluation database.” Available at\nhttp://www.irisa.fr/metiss/BASS-dB/ , accessed\nMay 27, 2010.\n[11] R. B. Dannenberg and contributors: “PortSMF,\npart of PortMedia” Available at http://portmedia\n.sourceforge.net/ , accessed May 27, 2010.\n[12] Masanao Izumo and contributors: “Timidity++,” Avail-\nable at http://timidity.sourceforge.net , accessed May\n27, 2010.\n[13] NullSoft, Inc: “Winamp,” Available at\nhttp://www.winamp.com , accessed May 27, 2010.\n[14] R. B. Dannenberg and C. Raphael: “Music Score\nAlignment and Computer Accompaniment,” Commu-\nnications of the ACM , vol. 49, no. 8 (August 2006), pp.\n38-43.\n[15] R. B. Dannenberg and G. S. Williams: “Audio-to-\nScore Alignment In the Audacity Audio Editor,” Late\nBreaking Demo session, 9th International Conference\non Music Information Retrieval , Philadelphia, USA,\nSeptember 2008.\n224\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "Singing / Rap Classification of Isolated Vocal Tracks.",
        "author": [
            "Daniel Gärtner"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1417089",
        "url": "https://doi.org/10.5281/zenodo.1417089",
        "ee": "https://zenodo.org/records/1417089/files/Gartner10.pdf",
        "abstract": "In this paper, a system for the classification of the vo- cal characteristics in HipHop / R&B music is presented. Isolated vocal track segments, taken from acapella ver- sions of commercial recordings, are classified into classes singing and rap. A feature-set motivated by work from song / speech classification, speech emotion recognition, and from differences that humans perceive and utilize, is presented. An SVM is used as classifier, accuracies of about 90% are achieved. In addition, the features are an- alyzed according to their contribution, using the IRMFSP feature selection algorithm. In another experiment, it is shown that the features are robust against utterance-speci- fic characteristics.",
        "zenodo_id": 1417089,
        "dblp_key": "conf/ismir/Gartner10",
        "keywords": [
            "system",
            "classification",
            "vo- cal characteristics",
            "HipHop",
            "R&B music",
            "acapella",
            "commercial recordings",
            "classes",
            "singing",
            "rap"
        ],
        "content": "SINGING / RAP CLASSIFICATION OF ISOLATED VOCAL TRACKS\nDa\nniel G ¨artner\nFraunhofer Institute for Digital Media Technology IDMT\ndaniel.gaertner@idmt.fraunhofer.de\nABSTRACT\nIn this paper, a system for the classiﬁcation of the vo-\ncal characteristics in HipHop / R&B music is presented.\nIsolated vocal track segments, taken from acapella ver-\nsions of commercial recordings, are classiﬁed into classes\nsinging and rap. A feature-set motivated by work from\nsong / speech classiﬁcation, speech emotion recognition,\nand from differences that humans perceive and utilize, is\npresented. An SVM is used as classiﬁer, accuracies of\nabout 90% are achieved. In addition, the features are an-\nalyzed according to their contribution, using the IRMFSP\nfeature selection algorithm. In another experiment, it is\nshown that the features are robust against utterance-speci-\nﬁc characteristics.\n1. INTRODUCTION\nAccording to the IFPI Digital Music Report 2010 [11] , the\ncatalogue of digital music from the licensed music services\ncontained more then 11 million tracks in 2009. For some\nyears, researchers have been working on tools that sim-\nplify the handling of this large amount of data. Automatic\ncontent-based analysis is now part of a multitude of differ-\nent applications. The algorithms help people to visualize\ntheir music collections and generate playlists. Music lovers\ncan discover new music with the help of music recommen-\ndation engines. DJs use software for automatic tempo and\nbeat detection.\nThis work is about automatically labeling short snippets\nof isolated vocal tracks according to their vocal character-\nistics. The segments are classiﬁed into two classes, rap and\nsinging. These two classes are the dominant vocal styles\nin HipHop and contemporary R&B music. A successful\nlabeling could further be useful in urban sub-genre clas-\nsiﬁcation, serve as a basis for vocal characteristics song\nsegmentation, and help analyzing the song structure. Also,\nintelligent audio players could be designed, that automati-\ncally skip all sung or all rapped parts in R&B and HipHop\nmusic songs, depending on the preferences of their users.\nRap is a form of rhythmically speaking, typically to ac-\ncompaniment music. As pointed out in [7], singing con-\ntains a larger percentage of voiced sounds than speaking.\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2010 International Society for Music Information Retrieval.For Western music, the singing voice also covers a wider\nrange of fundamental frequencies. In addition, singing\ntends to have a much wider dynamic range in terms of am-\nplitude. According to [8], singing voice tends to be piece-\nwise constant with abrupt changes of pitch in between.\nIn natural speech, the pitch frequencies slowly drift down\nwith smooth pitch change in an utterance. This peculiarity\ncan also often be observed in rap passages. While rapping,\nartists are quite free in their choice of the pitches, while the\nfundamental frequencies in singing are usually related to\nthe harmonic or melodic structure of the accompaniment.\nIn a survey conducted in [5], subjects had to label vocal\nutterances with a value from 1 (speaking) to 5 (singing),\nand explain their decision. For one utterance, 5 subjects\nused the word ”rap” in their explanation. The mean score\nof this utterance was 3.74. Rap seems to be perceived\nsomewhere in between singing and speaking, in that spe-\ncial case even a bit more singing than speaking. Differ-\nent subjects mentioned melody, rhythm, or rhyming com-\nbined with musical scales as features to discriminate sing-\ning from speaking. However, rhythm descriptions might be\nless important for rap / singing classiﬁcation, since rap and\nsinging are both rhythmically while speech is not. Further,\nrepetitions, the clarity and sharpness of pitch, or the pres-\nence of vibrato have been identiﬁed to be present in singing\nrather than speaking. Another feature for the discrimina-\ntion of speech and song as denoted in [9] is stress. It is\nstated that in English language speech, stress affects the\nmeaning of the utterance. This is another one of the points\nwhere speech and rap differ. In rap, where the voice is used\nas instrument, accentuation often is part of the rhythm.\nIn previous work [4] the classiﬁcation into singing and\nrap has been investigated on full songs (vocals + accompa-\nniment), using common low-level features and a Gaussian\nmixture model based classiﬁer. One of the outcomes of\nthis work has been, that, although the classiﬁer produced\nreasonable results, the classiﬁcation was highly inﬂuenced\nby the accompaniment music. We therefore suggest to\nbuild the system composed of two major components: vo-\ncal track isolation and the classiﬁcation of isolated tracks,\nusing a feature set designed towards this task. This paper\nfocuses on the second objective.\nTo the knowledge of the authors, automatic content-\nbased discrimination of isolated singing and rap tracks has\nnot yet been investigated elsewhere. However, research has\nbeen carried out on the task of singing and speaking clas-\nsiﬁcation. Investigations on the rap voice in a musicology\ncontext have been carried out though, e.g., [6].\n519\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)In [5], a set of features is presented to discriminate be-\ntw\neen singing and speaking including statistics over F0\nand∆F0, vibrato detection, repetition detection, and the\nproportions of voiced frames, unvoiced frames, and silent\nframes, and repetition features.\nAnother system is presented in [19]. Based on features\nlike the rate of voiced sounds, the standard deviation of\nthe duration of voiced sounds, and the mean silence dura-\ntions, an SVM is trained for singing / speaking classiﬁca-\ntion. This classiﬁer is used to forward sung queries to a\nquery by humming system, and spoken queries to a speech\nrecognition system.\n[12] present another study on speaking / singing dis-\ncrimination. The ﬁrst part addresses human performance.\nThey ﬁnd that already 1 second of audio signal is enough to\nclassify with an accuracy of 99.7% accuracy. Still 70% are\nreported on signals of 200 ms length. Further, it is inves-\ntigated, that the performance drops, when either spectral\nor prosodic information is distorted in the audio signal. In\nthe second part, the task is performed using Mel frequency\ncepstral coefﬁcients (MFCCs), ∆MFCCs, and ∆F0 as fea-\ntures and a maximum likelihood classiﬁer based on Gaus-\nsian mixture models (GMM).\nAnother ﬁeld working with energy-based and pitch-fea-\ntures on vocal signals is speech emotion recognition (e.g.,\n[17, 18]).\nThe remainder of this paper is organized as follows. In\nSection 2, the features and classiﬁer are described. Sec-\ntion 3 deals with the experiments that have been conducted.\nThere, also the used data and the general experimental set-\nup is introduced. The results and their meaning are dis-\ncussed in Section 4. Finally, the conclusions and an out-\nlook are given in Section 5.\n2. APPROACH\nIn this section, the used features and the classiﬁer that has\nbeen utilized, are explained.\n2.1 Features\nThe features contain the information about the audio sig-\nnal, that is accessed by the classiﬁer. Therefore, it is im-\nportant, that the features are well designed with respect to\nthe task.\nSome of the features are calculated from the pitch of the\nvocal segment. YIN [3] has been used as F0-estimator. In\naddition to an F0-estimation in octaves over time, YIN’s\noutput also includes the instantaneous power (IP) and the\nratio of aperiodic power to the total power (ATR).\nAll F0-estimations are transformed in the relative pitch\nrepresentation (RPR), which is a mapping into an interval\nof one octave width around the dominant frequency. First,\na histogram with 100 bins is calculated over the estimated\nF0 values. The center frequency of the bin with the highest\nvalue in the histogram is used as dominant frequency. Too\nlarge or small frequencies are halved or doubled respec-\ntively, until they ﬁt into the chosen interval. By doing so,\noctave-errors are removed. Of course, also absolute pitchinformation is removed, but absolute pitch is mainly artist\ndepended, and a contribution to rap / singing classiﬁca-\ntion is not expected. The resolution of the YIN features is\n1378 samples per second. Figure 1 and Figure 2 show the\nrelative pitch\nt[frames]0.5\n0\n0-0.5\n1000 2000 3000 4000 5000\nFigure 1 . RPR progression of a singing snippet.relative pitch\nt[frames]0.5\n0\n0-0.5\n1000 2000 3000 4000 5000\nFigure 2 . RPR progression of a rap snippet.\nRPR progression for an exemplary singing and rap snip-\npet respectively. One difference between the two examples\nis, that for singing, regions of almost constant pitch (RPR\nvalues of approximately 0.2, 0.0, -0.2, and -0.45 in Figure\n1) can be observed, while for rap the RPR values are per-\nmanently changing. Based on RPR, IP, and ATR, a set of\nfeatures is extracted.\nFirst of all, the number of non-silent frames is deter-\nmined, based on thresholding of IP. The ratio of non-silent\nframes to the number of overall frames will be denoted\nERatio .\nNext, from the non-silent frames, the number of voiced\nframes is determined, using a threshold on ATR. The ratio\nof voiced frames to the number of non-silent frames will\nbe denoted VRatio . As already stated, rap is supposed to\ncontain less voiced frames than song.\nIn another in-house study it has been discovered, that\nsong segments have a lower syllable-density than rap seg-\nments. IP can be used as onset detection function. Based\non adaptive thresholding, the number of onsets is estimat-\ned, which is then divided by the length of the segment.\nThis feature is denoted ORatio .\nAs another step, from the voiced frames the segments\nare determined, during which |∆RPR|is below a thresh-\nold. Segments of a length smaller then 10 frames are dis-\ncarded. The ratio of frames that contribute to such a seg-\nment and the number of voiced frames is denoted CRatio .\nAll the following calculations are performed on the RPR\nframes, that belong to a segment.\nThe mean of ∆RPR and the mean of ∆∆RPR also serve\nas features, denoted PitchDiff andPitchDDiff . Further,\nthe mean of |RPR|,MeanC, and the variance of RPR,\nVarC are calculated.\n520\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)The ratio of the number of frames with negative ∆RP R\nand the number of frames with positive ∆RPR is denoted\nSLRatio . In sung segments, either constant or with vi-\nbrato, both components are balanced. However, in rap seg-\nments a decreasing pitch can often be observed, and as a\nconsequence, the SLRatio would be larger than 1.\nA histogram over RPR with a resolution of 3 bins for\neach note is calculated, for a coarse approximation of the\nshape of the pitch distribution. Rap segments tend to have\nan unimodal RPR-distribution (Figure 3). Sung segments\noften have multimodal RPR-distributions, depending on\nthe number of different notes that are sung in an utterance,\nas depicted in the example of Figure 4. Further, the RPR-\ndistribution of a sung segment tends to have much sharper\npeaks than the distribution of a rap segment. The distance\nof the two bins with the largest values, divided by the width\nof the histogram will be denoted NoteDist . Dividing the\nsecond largest value in the histogram by the largest one,\nleads to the NRatio .\n0.50\n0 -0 .5400\n200occurrences\nRPR\nFigure 3 . RPR-histogram for a rap snippet.\n0.50\n0 -0 .51000\n500occurrences\nRPR\nFigure 4 . RPR-histogram for a singing snippet.\nIn addition, MFCCs are extracted from the audio sig-\nnal. MFCCs are a popular feature in speech recognition\nand describe the spectral envelope. In [10], their applica-\nbility to modeling music has been shown, and as a conse-\nquence they have been successfully used in different mu-\nsic information retrieval tasks since then. For each snippet,\nthe mean of all contributing frames is calculated. MFCCs\nare not part of the feature-set, they are used in a system\nfor comparison reasons to describe the robustness of the\nfeature-set in terms of utterance-sensitivity.\n2.2 Classiﬁer\nSupport vector machines (SVM, [2]) have been used as\nclassiﬁer. An SVM consists of a set of support vectors,\nthat span a hyperplane in the feature space. This hyper-\nplane separates two classes. The class of a test-observa-\ntion depends on on which side of the hyperplane the test-observation is located in the feature-space. This can be cal-\nculated incorporating the dot-products of the feature vector\nand the support vectors. In the training stage, the support\nvectors are determined based on training observations.\nIn order to use non-linear hyperplanes, the feature space\nis transformed in a higher-dimensional space by the use of\na kernel function. Computational costs for the transforma-\ntion and the calculation of the dot-products can be reduced\nby selecting the kernel in a way that the dot-product can\nalso be expressed in the original feature space. A radial\nbasis function (RBF) kernel has been used, that is parame-\nterized by γ.\nAnother parameter of the SVM is C, the weight of the\nerror term during training. LibSVM [1] has been used as\nSVM implementation.\n3. EXPERIMENTS\nFollowing the approach section, the system setup, includ-\ning the data, and the performed experiments are explained.\n3.1 Data\nA dataset of 62 songs from 60 different artists has been\nused in this study. Acapella versions of commercial Hip-\nHop and contemporary R&B songs, performed in English\nlanguage have been used. In these genres, songs are of-\nten released including an acapella and instrumental ver-\nsion. Other artists or DJs then can make remixes. For all\nsongs, the segments containing only monophonic singing\nor monophonic rap have been determined.\nEach segment is cut into 3s snippets, that overlap by\n0.5s. The inﬂuence of the segment length is not evaluated\nin this paper. Although [12] reports that already snippets\nof 1 second length contain enough information for humans\nto accurately classify speech and singing, a larger snippet\nsize has been chosen, since it is then more likely to ob-\nserve notes with different pitches in singing snippets. The\nﬁnal dataset consists of 815 rap-snippets and 584 singing-\nsnippets.\n3.2 System setup and evaluation\nTraining and evaluation is performed using 5-fold cross-\nvalidation. All snippets are randomly distributed amongst\nthe 5 folds using an utterance ﬁlter, which means, that all\nsnippets from one song (belonging to one utterance) are\ndistributed in the same fold. Each of the folds serves as\ntest-data once and is part of the training-data in the other\ncases. The training data is used to determine the parame-\nters of the SVM, i.e., the support vectors, C, andγ. It is\ncrucial in SVM training / classiﬁcation that all the features\nhave approximately the same range. Therefore the data has\nto be normalized. Variance-normalization is used, in order\nto make the data zero mean unit variance. The mean µand\nthe standard deviation σhave to be estimated.\nA reasonable choice of Candγis important for good\nclassiﬁcation results. Both parameters are estimated us-\ning 3-fold cross-validation on the training data. This stage\n521\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)will later be referred to as development stage. The distri-\nbu\ntion into the folds is done randomly again. However,\nat this point it is possible to decide whether an utterance\nﬁlter should be applied or not. A three-stage grid-search\nhas been employed. Since this process itself also consists\nof training and evaluation, µandσhave to be determined\nevery time the training-data changes due to recomposition\nfrom different folds.\nHaving determined Candγ,µandσare estimated\nbased on the whole training-data, the training-data is nor-\nmalized, and the SVM is trained with the previously de-\ntermined Candγ. Finally, the performance is measured\nusing the test-data, which is the test-observations from the\none out of ﬁve folds, that has not been used for training and\ndevelopment.\nThe performance of a trained system both in evaluation\nAtand development Adis measured in accuracy. The ac-\ncuracy of a classiﬁer on given data is calculated by dividing\nthe number of correctly classiﬁed test-observations by the\nnumber of all test-observations. Accuracy can be sensitive\nto imbalanced test-data. So if for example the test-data\ncontains 80% observations from one class and only 20%\nobservations from the other class, a classiﬁer, that would\nalways choose the same class would lead to a performance\nof either 80% or 20%, depending on which class he always\nchoses. Therefore the test data is made balanced during the\nevaluation by randomly picking 584 observations from the\n815 rap snippets.\nThe whole process, incorporating the random distribu-\ntion into ﬁve folds, the development and training of the\nclassiﬁer, and its evaluation, is performed multiple times\n(denoted #runs), since this process contains random ele-\nments and is therefore indeterministic. The mean and vari-\nance of the accuracies in the test series are given as ﬁnal\nmeasure, denoted µA,tandσ2\nA,t. Further, in Table 2 also\nµA,dandσ2\nA,d, are given, which are the accuracies during\ndevelopment for the chosen Candγ. Matlab is used as\nexperimental framework.\n3.3 Feature selection\nA feature selection algorithm (FSA) has been used to give\nan estimation of the contribution of each of the features.\nInertia ratio maximization using feature space projection\n[16] is a ﬁlter FSA, where the criteria of choosing features\nis distinct from the actual classiﬁer. For each feature di-\nmension an r-value is determined, which is the ratio of\nthe between-class inertia to the total-class inertia. The fea-\nture with the largest ris chosen, then an orthogonalization\nprocess is applied to the feature space, in order to avoid\nthe choice of redundant dimensions during following it-\nerations. These steps are repeated until a stop criterium\napplies. The order of the features after feature selection\nreﬂects their importance according to the feature selection\ncriterion, that should be correlated to the classiﬁcation per-\nformance to a certain extend.3.4 Utterance ﬁlter\nOne of the goals in machine learning is to build systems\nthat are able to generalize. Also, performances of classi-\nﬁers should be compared based on unseen test data. In\norder to achieve this, it is necessary to strictly discrimi-\nnate training-data and testing-data during development and\nevaluation of the system. The distribution of the data in\ntraining and test-set can be even more restricted. It is com-\nmon practice to put all the segments of a song in the same\ndataset, to for example avoid that the system is trained with\na segment from the song and tested with a similar segment\nfrom the same song. In [15], it is suggested to put all pieces\nof an artist in the same dataset in a genre classiﬁcation task.\nWith experiments it is shown, that the performance of a\nsystem decreases signiﬁcantly, if this so called artist ﬁlter\nis used. A possible reason for this is, that the system might\nfocus on perceptually not so relevant information such as\nproduction effects [14].\nAs described in 3.2, an utterance ﬁlter is always ap-\nplied in the 5-fold cross-validation setup, since it is pos-\nsible, that the suggested feature set also reﬂects utterance-\nspeciﬁc characteristics. In the 3-fold cross-validation de-\nvelopment stage however, the utterance-ﬁlter can be ei-\nther applied or omitted. Comparing performances based on\nsystems with and without utterance-ﬁlter helps in describ-\ning the robustness towards utterance-speciﬁc characteris-\ntics. If a system generalizes well, µA,tandµA,dshould be\napproximately equal.\nThe mean over the MFCC-frames of a snippet is a fea-\nture, that is supposed to be utterance-speciﬁc. In 4.3, the\nuse of an utterance-ﬁlter is analyzed for the proposed fea-\nture-set and the mean-MFCC feature.\n4. RESULTS AND DISCUSSION\nThe results of the performed experiments are listed and dis-\ncussed in this section.\n4.1 Feature contribution\nIn Table 1, the outcome of the FSA is denoted. Overall,\nfeature selection has been performed 69425 times. In all\nruns, the VRatio feature has been selected ﬁrst, as can be\nseen in column 2, belonging to rank 1. Further important\nfeatures are CRatio, SLRatio and ORatio, that have been\nchosen 54989, 8175, and 6240 times as second feature re-\nspectively. The most unimportant features according to the\nIRMFSP are PitchDDiff and VarC (often chosen on rank\n10 or 11 according to the values in column 11 and column\n12).\nThe mean r-value of the ﬁrst selected feature is 0.52,\nfollowed by 0.47 for the second selected feature. rde-\ncreases drastically from the second to the third selected\nfeature. In [16], it is suggested to stop the iterative fea-\nture selection process as soon as rof the current iteration\nis below 1/100 of rin the ﬁrst iteration. Following this\ncriterion, the 6 top features would have been selected.\n522\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Rank 1 2 3 4 5 6 7 8 9 10 11\n¯r 0.5191 0.4716 0.0459 0.0151 0.0094 0.0068 0.0046 0.0034 0.0018 0.0004 0.0001\nCRatio 0 54989 0 2 91 54 891 4511 5932 1678 1277\nERatio 0 0 0 359 2218 6062 13300 24101 17819 3644 1922\nMeanC 0 0 0 1367 21200 9070 8650 13060 12116 3257 705\nNoteDist 0 21 6432 50987 8455 2720 686 120 4 0 0\nNRatio 0 0 0 3505 12729 8077 12597 9916 21221 1305 75\nORatio 0 6240 1880 10822 11029 9791 20787 7385 1376 112 3\nPitchDDiff 0 0 0 0 5 54 794 3593 7596 39962 17421\nPitchDiff 0 0 12249 2342 12096 27695 9244 4585 1195 17 2\nSLRatio 0 8175 48864 41 1601 5902 2464 1999 357 22 0\nVarC 0 0 0 0 1 0 12 155 1809 19428 48020\nVRatio 69425 0 0 0 0 0 0 0 0 0 0\nTable 1 . Ranks of different features in the feature selection process.\n4\n.2 System Performance\nNumber of features determined with IRMFSPµA,t\n828486889092\n0 2 4 6 8 10 12\nFigure 5 . Performance subject to the number of features.\nThe ﬁnal performance of the system is plotted against\nthe number of features after IRMFSP in Figure 5. The top\nperformance, 90.62% is achieved using 9 features. Using\nthe feature-set consisting of all 11 features leads to a mean\naccuracy of 90.53%. The largest gain in performance is\nreported from 2 features (85.14%) to 3 features (88.78%).\n4.3 Inﬂuence of the utterance ﬁlter\nTable 2 contains the results of the investigation of utter-\nance-sensitivity. For the suggested feature-set (full) the\nperformance decrease is 1.09% (91.63% down to 90.54%\nfrom development to testing) with utterance ﬁlter. Without\nutterance-ﬁlter 3.07% (from 93.88% down to 90.81%) are\nobserved. These small decreases originate in the fact that\nµA,dis result of the optimization of Candγ, whileµA,tis\nnot. Further, during development, imbalanced test-data is\nused for the evaluation, which can also lead to differences\nbetween both values. On the full feature-set, µA,tis almost\nsimilar for both systems, with and without utterance ﬁlter.Feat. u.ﬁlter µA,tσ2\nA,tµA,dσ2\nA,d#runs\nfull yes 90.54 0.53 91.63 0.16 1588\nfull no 90.81 0.49 93.88 0.03 1583\nMFCC yes 67.71 6.71 72.84 1.17 1084\nMFCC no 65.08 3.85 96.36 0.04 934\nTable 2 . Inﬂuence of the utterance-ﬁlter.\nA\npplying an utterance-ﬁlter to the MFCC-feature re-\nsults to an decrease from 5.13% (from 72.84% down to\n67.71%), which again can be explained with the optimiza-\ntion procedure. If the system is trained with the MFCC-\nfeature without using an utterance-ﬁlter, the development-\nperformance is 96.36%, which is the highest one achieved\nin the experiments. But on new utterances, the perfor-\nmance drastically decreases to 65.08%. In our data, artists\nthat rap do not sing and vice versa. Without the utterance-\nﬁlter, different parts of the same utterance are in the test-set\nand the training-set during the system-development, and a\ntask like that can also be performed by an artist-detection\nor utterance-detection system. MFCCs are well known for\ntheir capabilities to capture speaker characteristics, and are\ntherefore often used in speaker recognition systems. So in\nthe development stage, the system is trained to classify into\nrap and singing by actually identifying utterances. A µA,d-\nvalue of 96.36% shows, that, MFCCs are an appropriate\nfeature for this task. On the contrary, µA,tis determined\nclassifying snippets from unknown utterances. An utter-\nance detection system cannot do that well, which leads to\na low accuracy of 65.08%. For the MFCC-system with\nutterance ﬁlter, as already reported the difference is much\nsmaller. For the full feature-set, no large difference be-\ntweenµA,tandµA,tcould be observed. This set therefore\nis not sensitive to utterance-speciﬁc characteristics.\nComparing µA,tfor the MFCC-systems with and with-\nout utterance-ﬁlter, one can see that the system trained with\nutterance-ﬁlter performs 2.63% better. A possible reason\nis that MFCCs seem to be able to also classify based the\nvocal characteristics to a certain extend, but when trained\n523\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)without utterance-ﬁlter, the classiﬁer seems to ”learn the\nta\nsk that is easier to perform”, which might be utterance-\nidentiﬁcation instead of vocal characteristics classiﬁcation.\nWhen trained with utterance-ﬁlter, there is no utterance-\nidentiﬁcation development data provided. But since the\ndifference is so small, there might be other reasons.\n5. CONCLUSIONS AND OUTLOOK\nA system for the classiﬁcation of isolated vocal tracks into\nthe classes singing and rap has been presented. A feature\nset, motivated by differences perceived by human is devel-\noped. Accuracies of over 91% are achieved on 3 second\nsnippets of isolated vocal tracks from commercial urban\nmusic recordings. Further, it has been shown in experi-\nments with an utterance-ﬁlter, that the suggested feature-\nset is not sensitive to utterance-speciﬁc characteristics.\nAs a next step, the application on full tracks, where\nno isolated vocal tracks are available, will be investigated.\nSince the described system is not designed to also work\non mixtures of vocal tracks and accompaniment, the vocal\ntrack has to be separated from the song. Methods for the\nseparation of the vocal track as described in, e.g., [13, 20,\n21] are currently investigated. The system that has been\ndescribed in this paper can also serve as benchmark for the\nsource separation algorithms. Further, a study incorporat-\ning listening test is intended, in order to evaluate human\nperformance on this task.\n6. REFERENCES\n[1] Chih-Chung Chang and Chih-Jen Lin. LIBSVM: a li-\nbrary for support vector machines , 2001. Software\navailable at http://www.csie.ntu.edu.tw/ cjlin/libsvm.\n[2] Corinna Cortes and Vladimir Vapnik. Support-vector\nnetworks. Machine Learning , 20:273:297, 1995.\n[3] Alain de Cheveign´ e and Hideki Kawahara. YIN, a\nfundamental frequency estimator for speech and mu-\nsic.Journal of the Acoustic Society of America , 111\n(4):1917:1930, 2002.\n[4] Daniel G¨ artner and Christian Dittmar. V ocal character-\nistics classiﬁcation of audio segments: An investiga-\ntion of the inﬂuence of accompaniment music on low-\nlevel features. In Proceedings of the ICMLA , 2009.\n[5] David Gerhard. Computationally measurable differ-\nences between speech and song . PhD thesis, Simon\nFraser University, Canada, 2003.\n[6] Ferdinand H¨ orner and Oliver Kautny. Die Stimme im\nHipHop . transcript Verlag, 2009.\n[7] Youngmoo E. Kim. Singing Voice Analysis/Synthesis .\nPhD thesis, Massachusetts Institute of Technology,\n2003.\n[8] Yipeng Li and DeLiang Wang. Separation of singing\nvoice from music accompaniment for monaural record-\nings. IEEE Transactions on Audio, Speech, and Lan-\nguage Processing , 15(4):1475–1487, May 2007.[9] George List. The boundaries of speech and song. Eth-\nnomusicology , 7(1):1:16, January 1963.\n[10] Beth Logan. Mel frequency cepstral coefﬁcients for\nmusic modeling. In Proceedings of ISMIR , 2000.\n[11] International Federation of the Phonographic Indus-\ntry. IFPI Digital Music Report 2010. Available at\nhttp://www.ifpi.org/content/library/DMR2010.pdf.\n[12] Yasunori Ohishi, Masataka Goto, Katunobu Itou, and\nKazuya Takeda. On human capability and acoustic\ncues for discriminating singing and speaking voices. In\nProceedings of ICMPC , 2006.\n[13] Alexey Ozerov, Pierrick Philippe, Fr´ ed´ eric Bimbot,\nand R´ emi Gribonval. Adaptation of bayesian models\nfor single-channel source separation and its applica-\ntion to voice/music separation in popular songs. IEEE\nTransactions on Audio, Speech, and Language Pro-\ncessing , 15(5):1564–1578, July 2007.\n[14] Elias Pampalk. Computational Models of Music Simi-\nlarity and their Application to Music Information Re-\ntrieval . PhD thesis, Vienna University of Technology,\nAustria, March 2006.\n[15] Elias Pampalk, Arthur Flexer, and Gerald Widmer. Im-\nprovements of audio-based music similarity and genre\nclassiﬁcation. In Proceedings of ISMIR , London, UK,\n2005.\n[16] Geoffroy Peeters and Xavier Rodet. Hierarchical gaus-\nsian tree with inertia ratio maximization for the classi-\nﬁcation of large musical instruments databases. In Pro-\nceedings of DAFx, 2003.\n[17] Thomas S. Polzin. Verbal and non-verbal cues in\nthe communication of emotions. In Proceedings of\nICASSP , 2000.\n[18] Bj¨ orn Schuller, Gerhard Rigoll, and Manfred Lang.\nHidden markov model-based speech emotion recogni-\ntion. In Proceedings of ICASSP , 2003.\n[19] Bj¨ orn Schuller, Gerhard Rigoll, and Manfred Lang.\nDiscrimination of speech and monophonic singing in\ncontinuous audio streams applying multi-layer support\nvector machines. In Proceedings of ICME , volume 3,\npages 1655–1658, 2004.\n[20] Shankar Vembu and Stephan Baumann. Separation of\nvocals from polyphonic audio recordings. In Proceed-\nings of ISMIR , 2005.\n[21] Tuomas Virtanen, Annamaria Mesaros, and Matti\nRyyn¨ anen. Combining pitch-based inference and non-\nnegative spectrogram factorization in separating vocals\nfrom polyphonic music. In Proceedings of the ISCA\nTutorial and Research Workshop on Statistical And\nPerceptual Audition , 2008.\n524\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "Tempo Induction Using Filterbank Analysis and Tonal Features.",
        "author": [
            "Aggelos Gkiokas",
            "Vassilis Katsouros",
            "George Carayannis"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1415210",
        "url": "https://doi.org/10.5281/zenodo.1415210",
        "ee": "https://zenodo.org/records/1415210/files/GkiokasKC10.pdf",
        "abstract": "This paper presents an algorithm that extracts the tempo of a musical excerpt. The proposed system assumes a constant tempo and deals directly with the audio signal. A sliding window is applied to the signal and two feature classes are extracted. The first class is the log-energy of each band of a mel-scale triangular filterbank, a common feature vector used in various MIR applications. For the second class, a novel feature for the tempo induction task is presented; the strengths of the twelve western musical tones at all octaves are calculated for each audio frame, in a similar fashion with Pitch Class Profile. The time- evolving feature vectors are convolved with a bank of resonators, each resonator corresponding to a target tempo. Then the results of each feature class are com- bined to give the final output. The algorithm was evaluated on the popular ISMIR 2004 Tempo Induction Evaluation Exchange Dataset. Re- sults demonstrate that the superposition of the different types of features enhance the performance of the algo- rithm, which is in the current state-of-the-art algorithms of the tempo induction task.",
        "zenodo_id": 1415210,
        "dblp_key": "conf/ismir/GkiokasKC10",
        "keywords": [
            "algorithm",
            "tempo",
            "musical excerpt",
            "constant tempo",
            "audio signal",
            "sliding window",
            "log-energy",
            "mel-scale triangular filterbank",
            "novel feature",
            "resonators"
        ],
        "content": "TEMPO INDUCTION USING FILTERBANK ANALYSIS AND \nT\nONAL FEATURES \nAggelos Gkiokas1,2, Vassilis Katsouros1 and George Carayannis2 \n1Institute for Language and Speech Processing \n2National Technical University of Athens \n{\nagkiokas, vsk, gcara}@ilsp.gr \nABSTRACT \nThis paper presents an algorithm that extracts the tempo \nof a musical excerpt. The proposed system assumes a \nconstant tempo and deals directly with the audio signal. A \nsliding window is applied to the signal and two feature \nclasses are extracted. The first class is the log-energy of \neach band of a mel-scale triangular filterbank, a common \nfeature vector used in various MIR applications. For the \nsecond class, a novel feature for the tempo induction task \nis presented; the strengths of the twelve western musical \ntones at all octaves are calculated for each audio frame, in \na similar fashion with Pitch Class Profile. The time-\nevolving feature vectors are convolved with a bank of \nresonators, each resonator corresponding to a target \ntempo. Then the results of each feature class are com-\nbined to give the final output. \nThe algorithm was evaluated on the popular ISMIR \n2004 Tempo Induction Evaluation Exchange Dataset. Re-\nsults demonstrate that the superposition of the different \ntypes of features enhance the performance of the algo-\nrithm, which is in the current state-of-the-art algorithms of \nthe tempo induction task.  \n1. INTRODUCTION \nTempo Induction has gained a great interest within the \nMusic Information Retrieval community the past few \nyears. Although in most systems, the tempo induction is \nmade simultaneously with the beat tracking process as a \nunified task, the need for an individual handling of these \ntasks is apparent. An example can be found in Gouyon \nand Dixon in [1], where a genre classifier for 8 different \nmusic genres, based solely on the tempo of each excerpt \nhas given remarkable results. \nBeyond the scope of music classification, tempo induc-\ntion and beat tracking are essential in many diverse appli-\ncations, such as music similarity and recommendation, \nautomatic transcription, audio editing, music to MIDI \nsynchronization, and automatic accompaniment. They al-\nmost always serve as an inter-step in algorithms handling more complicated problems such as meter extraction [2] \nand rhythm description. \nThe algorithms that extract tempo can be divided into \ntwo main categories. The first consists of algorithms that \nuse onset lists as input (either extracted directly from \nMIDI or audio). Indicative work can be found in [3], [4]. \nMost of these algorithms extract periodicities from the \ninter.onset intervals  (IOIs) or by applying the autocorre-\nlation function (ACF) to the onsets list in order to extract \nthe tempo. In the latter belong the algorithms that search \nfor periodicities directly from the audio (e.g. the ACF ap-\nplied to frame features). Respective work can be found in \n[5], [6]. Although the former have the advantage of gen-\neralization (handling both MIDI and audio), evidence that \nthe latter achieves better results is reported in [7]. An ex-\ntensive review on the rhythm description algorithms can \nbe found in [8]. \nA first step to systemize the tempo extraction task was \nthe evaluation exchange organized during the 5th Interna-\ntional Conference on Music Information Retrieval [7]. \nSeven participants submitted twelve different algorithms, \ntested on a collection of 3199 tempo-annotated music ex-\ncerpts. The data was hidden from the participants. After \nthe contest was conducted, the data were made available \nonline (except of the Loops data that are available under a \nfee). Detailed description can be found in [7]. In a similar \nfashion, MIREX 20051 and MIREX 20062  Audio Tempo \nExtraction evaluation exchanges were conducted, with the \ndifference that the evaluation procedure was more fo-\ncused on the perceived than actual tempo. Unfortunately, \nthe data is still not available except of a small portion that \nwas used as training data. \nAlthough a benchmark collection was created, few \ntempo induction algorithms have been tested on this data-\nset. A remarkable exception is Seyerlehner, Widmer and \nSchnitzer’s work [9]. They proposed two versions of an \nalgorithm that extracts rhythmic patterns using the auto-\ncorrelation function (ACF) as described in [10] and the \nFluctuation Pattern as described in [11], respectively, in \norder to determine the tempo. Their approach is based on \nthe assumption that pieces with similar rhythmic patterns \nare more likely to have similar tempo as well. The rhyth-\nmic patterns of excerpts are compared with those of a \ntempo-annotated music database. Their results showed \nthat the proposed algorithm outperformed all the algo-\nrithms presented in the ISMIR 2004 evaluation exchange \n                                                           \n1 http://www.music-ir.org/mirex/2005/index.php/Audio_Tempo_Extraction  \n2 http://www.music-ir.org/mirex/2006/index.php/Audio_Tempo_Extraction   \nPermission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fee pro vided that copies \nare not made or distributed for profit or commercial advantage and that \ncopies bear this notice and the full citation on the first page.  \n© 2010 International Society for Music Information Retrieval  \n555\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \non the ballroom data, and had similar results with the top \nperforming algorithm [2] on the songs excerpts  data. Note \nthat the results presented are based on accuracy1 , and not \non accuracy2 , where correct tempi are considered the \nfractions of the ground-truth tempo (half, double, three \ntimes, 1/3) which partly can be considered perceptually \nmore relevant. Another example is the work of Alonso et \nal. in [12]. They proposed a system that estimates the \ntempo by decomposing the music signal sub-bands into \nharmonic and noise components.  Then musical events are \nextracted with an “accentuation” weighting and peri-\nodicities are estimated. They tested the proposed algo-\nrithm on a corpus consisting of the songs excerpts  collec-\ntion and excerpts from the author’s private collection. The \nevaluation measures were accuracy1  and accuracy2 , but \nwith a 5% tolerance. Thus, their algorithm cannot be \ncompared directly with the aforementioned. \nIn this paper we present a system that extracts tempo \nwithout onset detection, in a similar fashion that Scheirer \ndoes in [5]. The difference is that additionally to the fil-\nter-bank analysis, we incorporate a novel feature for the \ntempo induction task, similar to Pitch Class Profile, intro-\nduced by Fujishima [13]. The proposed algorithm as-\nsumes no significant tempo variation within the music ex-\ncerpt.   \nThe rest of the paper is organized as follows. In Sec-\ntion 2 we describe the architecture of our system. Section \n3 focuses on details concerning the algorithm and indi-\nvidual processes of the implemented system. Comparable \nresults on the ballroom and songs excerpts  data of the \nISMIR 2004 tempo induction evaluation exchange are \nprovided in Section 4. Conclusions, drawbacks and future \nwork conclude this paper in Section 5.  \n2. ALGORITHM OVERVIEW \nThe overall architecture and the individual components \nof the proposed algorithm are shown in Figure 1. Initially \na moving Gaussian window is applied on to the input sig-\nnal. For each frame a filterbank of equally spaced trian-\ngular filters in the mel-scale is applied, and the log-\nenergy of each bank is calculated, in order to produce a \nvector m for each frame. Simultaneously, a similar proc-\ness takes place, using a larger window. Each frame is \nconvolved with twelve filters, each one corresponding to \none of the twelve musical tones, forming the vector t. \nThen a larger window of 8secs length is applied to \neach time-evolving feature, with a 1sec shift. Afterwards, \nthe features are differentiated and convolved with a bank \nof resonators, with frequencies corresponding to the target \ntempi, and the ∞iof each convolution is calculated. \nThen the norms are summed across features, independ-\nently for each feature type, forming two vectors mSCand \ntSCof length T, where T denotes the plurality of the tar-\nget tempi. Each vector indicates the strength of each tar-\nget tempo to the specific frame. Finally mSCand tSC are summed across the segments of the whole excerpt, to get \nthe final tempo strengths for each feature class. The two \nvectors are combined to get the final output. \n \nFigure 1. System Overview \n3. ALGORITHM DETAILS \n3.1 Extracting Filterbank Features \nA moving Gaussian window of 20ms length with 5ms \nshift is applied to the input signal. Each segment is ana-\nlyzed by a mel-scale triangular filterbank consisting of 12 \nbands, and the log-energy for each band is computed. \nThis process forms a 12-dimensional feature vector m, for \neach frame. \n3.2 Extracting Tonal Features \nIn a similar fashion with filterbank analysis, a Gaussian \nwindow is applied to the signal. In order to have better \nfrequency resolution, the window is chosen much larger \nthan the case before. A window with 80ms length and \n5ms shift was chosen after experiments.  \nEach segment is convolved with 12 reference signals \nof same length with the sliding window. Each signal \nrepresents one the 12 western musical tones. The refer-\nence signals are the sum of cosines of equal amplitudes, at \nfrequencies equal to the 0F of each musical tone, at all \noctaves within a range from 27.5Hz to 10 kHz. No har-\nmonic partials are considered. Formally, the reference \nsignals are given by the following formula: \n ( )( ) (2 ),    1..12\ni ktone k i\nfR n cos f n k π\n∈Ω= = ∑ (1) \nwhere kΩ denotes the set of fundamental frequencies of \ntone k in the range of 27.5Hz to 10 kHz and n the time \nindex. Afterwards the 2iof the twelve convolutions are \ncalculated to form the tonal feature vector t for each \nframe. Formally Differentiator  \nFilterbank Tonals \nResonators  \nSummation \nFilterbank \ntempo Tonal \nTempo \n556\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \n ( )2( ) ( ( ) )( ) ,   1...12k tone k t l s l R n k= ∗ = (2) \nw\nhere( , )s n l is the signal frame l and ( )tone kR the refer-\ne\nnce signal defined in Equation 1. \n3.3 Convolving with the Bank of Resonators \nThe feature extraction process is followed by the convolu-\ntion of the feature vectors with a bank of resonators. \nFirstly, we segment each time-evolving feature using a \nrectangular window of 8 secs with 1 sec overlap.  \nIn order to compute the rhythmic periodicities of the \nsignal, we convolve each feature segment with a bank of \nresonators, each resonator representing a specific tempo. \nWe consider resonators with impulse response support \nthat equals to the segment length for every integer value \nfrom 40 up to 280 bpm. The resonators can be any system \nwith periodic impulse response, making our algorithm \nflexible to adapt to different signals. Motivated by the \nmathematical model of entrainment, as it was presented \nby Large and Kolen in [14], we adopted the basic oscilla-\ntory unit as the impulse response of the resonator. The \nequation of the oscillatory unit is given by \n ( ) 1 tanh( (cos(2 ) 1))t r l l γ πψ= + ⋅ − (3) \nwhere tψdenotes the frequency of tempo t. Parameter γ is \ncalled the output gain.  \nAfter this process the ∞iis computed for the convolu-\ntion of every feature-resonator pair, resulting a vector in-\ndicating the strength of all tempi for each specific feature. \nFormally, we can write \n ( ) ( )( ) ,    \nif t i S t r f l t T∞= ∗ ∈ (4) \nwhere( )\nifS tis the strength of feature if at tempo t and \nT  is the target tempi set. ( { , , 1..12}i i if i∈ =m t  ) \n3.4 Combining the Feature Vectors \nTo compute the tempo for a specific segment, we sum-\nmate the tempo strengths ( )\nifS tacross the features, indi.\nvidually for each feature class, thus taking two vectors \nmSCand tSC, for filterbank and tonal features respec-\ntively. Formally, we can write \n 12 1 2\n1 1( ) ( ),  ( ) ( ),   \ni i m m t t\ni it t t t t T\n= == = ∈ ∑ ∑SC S SC S  (5) \nw\nhere T is the target tempi set. \nFinally, to combine the results of the two tempo detec-\ntors, we summate ( ), ( )m tt tSC SC  across the segments of \nth\ne music excerpt. Then by point-wise multiplication we \ncompute the final vector SC, indicating the tempo st-\nrengths within the excerpt. The tempo with the maximum \nstrength is considered as the correct tempo.   4. EXPERIMENTAL RESULTS \nIn this section we present the evaluation of the proposed \nalgorithm. The data we used for the experiments consists \nof 1163 excerpts from the ballroom and songs excerpts \ndatasets of the ISMIR 2004 Tempo Induction evaluation \nexchange. Details on the statistics, collection and annota-\ntion of the corpus can be found in [9]. \nFirstly, we evaluated our algorithm for each feature \nclass individually. Afterwards we combined the outputs of \nthe individual features as described in the previous sec-\ntion. The results on both ballroom and songs excerpts \ndatasets are presented in Table 1. \n \n Ballroom Songs \nFeature Type Acc1 Acc2 Acc1 Acc2 \nFilterbank  56.34 93.33 23.01 88.39 \nTonals 50.32 81.08 46.45 73.33 \nCombination 61.08 93.98 42.15 90.11 \nTable 1. Results (%) of the algorithm for the Ballroom \nand Songs Excerpts datasets, using feature classes indi-\nvidually and in combination . \nIt is clear that the algorithm yields better results using \nthe filterbank features in the Ballroom dataset for the ac.\ncuracy1 measure. On the other hand, the algorithm per-\nformed poorly in the songs data based on accuracy1  (only \n23%).  The above can be explained by the fact that the \nBallroom data consists of more “percussive” excerpts, \nthus the filterbank energies represent sufficiently the data.  \nAdditionally, the experimental results demonstrate that by \nusing solely the filterbank features, the proposed system \n“tends” to capture tempi double of the groundtruth tempo. \nFor most of the excerpts classified correctly using accu.\nracy2 and misclassified using accuracy1 , the detected \ntempo was double of the correct tempo. \nWhen we used solely tonal features as input to the sys-\ntem, the accuracy1  on the songs data increased signifi-\ncantly (from 23% to 46.5%) for the Songs data. This can \nbe explained by the more “melodic” nature of the ex-\ncerpts consisting Songs data, which prove tonal features \nto be more suitable for that case. On the other hand accu.\nracy2 measure degraded from 88.39% to 73.3%. A possi-\nble explanation is the large window used in the preproc-\nessing stage, which “cuts off” frequencies double or triple \nof the actual tempo. \nWhen combining the results from the two versions, we \nobserve that in both Ballroom and Songs excerpts data, \nthe superposition increases the algorithm performance, \nespecially in the Songs Data. Considering the filterbank \nfeatures as base features, tonal features provide additional \ninformation about the rhythm periodicities of the signal.  \nComparative results of the presented algorithm, namely \nGK, with the best five performing algorithms in [7], \nnamely Miguel Alonso  (AL), Simon Dixon  (DI), Anssi \n557\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \nKlapuri (KL), Christian Uhle (UH) and Eric Scheirer  \n(SC), plus Klaus Seyerlehner  (SE1,SE2)[9] are presented \nin Table 2. \n \n Ballroom Songs \nMethod Acc1 Acc2 Acc1 Acc2 \nGK 61.08 93.98 42.15 90.11 \nAL 34.1 69.48 37.42 68.6 \nDI 43.12 86.96 16.99 76.99 \nKL 63.18 90.97 58.49 91.18 \nUH 56.45 81.09 41.94 71.83 \nSC 51.86 75.07 37.85 69.46 \nSE1 78.51 - 40.86 - \nSE2 73.78 - 60.43 - \nTable 2. Comparative results (%) on Ballroom and Songs \ndatasets. \n5. CONCLUSION AND FURTHER WORK \nIn this paper we presented a system that extracts the \ntempo of a music signal. The proposed algorithm was \nevaluated on the benchmark corpus of the ISMIR 2004 \nwith encouraging results. Without taking into considera-\ntion any high-level musical information, our system per-\nformed within the current state-of-the-art algorithms of \nthe tempo induction task.  \nThe tonal features introduced in this work prove to \ncapture additional aspects of rhythmic periodicity in a \nmusical signal. It is evident that underlying rhythmic pe-\nriodicities of a musical signal can be found beyond the \nfilterbank energies, in a more “pitched context”. Without \nany multi-pitch estimation or chord detection process, we \nobserve that the simpler and more abstract tonal features \npresented in this paper similar to Pitch Class Profile, con-\ntain rhythmic information that can enhance the perform-\nance of a tempo induction system that does not take into \naccount any tonal information. \nHowever, during the experiments we observed that the \nperformance of the presented algorithm is sensitive to the \nwindow length and shift during the extraction process of \ntonal features, an effect that will be investigated in the \nfuture. Moreover we intend to extend tonal features in a \nmore sophisticated way, such as chords, and incorporate \nharmonic partials information. Finally, the superposition \nof the output for the features classes is a subject for future \nresearch. \n6. REFERENCES \n[1] Gouyon F. and Dixon S., “Dance Music \nClassification: A Tempo-Based Approach”, \nProceedings of the 5th International Conference on \nMusic Information Retrieval,  Barcelona, Spain 2004 [2] Klapuri A., Eronen A. and Astola J., “Analysis of \nthe Meter of Music Acoustic Signals”, IEEE Trans. \nAudio, Speech, and Language Processing, 14(1), \nJanuary 2006 . \n[3] Alonso M., David B., Richerd G., “Tempo and Beat \nEstimation of Musical Signals”, Proceedings of the \n5th International Conference on Music Information \nRetrieval,  Barcelona, Spain 2004. \n[4] Davies M., Plumbley M., “Context-Dependent Beat \nTracking of Musical Audio”, IEEE Transactions on \nAudio, Speech, and Language Processing, Vol. 15, \nNo. 3, March 2007. \n[5] Scheirer E., “Tempo and Beat Analysis of Acoustic \nMusical Signals.”, The Journal of the Acoustical \nSociety of America, Vol. 103, No. 1, January 1998. \n[6] Dannenberg R., “Toward Automated Holistic Beat \nTracking, Music Analysis, and Understanding”, \nProceedings of the 6th International Conference on \nMusic Information Retrieval, ,  London, UK, 2005. \n[7] Gouyon F., Klapuri A., Dixon S., Alonso M., \nTzanetakis G., Uhle C., and Cano P., “An \nExperimental Comparison  of Audio Tempo \nInduction Algorithms”,  IEEE Transactions on \nAudio, Speech, and Language Processing, Vol. 14, \nNo. 5, September 2006 . \n[8] Gouyon F. and Dixon S., “A Review of Automatic \nRhythm Description Systems”, Computer Music \nJournal, 29:1, pp 34.54, Spring 2005. \n[9] Seyerlehner K., Widmer G., and Schnitzer D., \n“From Rhythm Patterns to Perceived Tempo”, \nProceedings of the 8th International Conference on \nMusic Information Retrieval, , Vienna, Austria, \n2007. \n[10] Ellis D.P.W. “Beat Tracking with Dynamic \nProgramming”, Journal of New Music Research,  \nvol. 36 no. 1, March 2007, pp 51-60. \n[11] Pampalk E., Rauber A., Merkl D., “Content-Based \nOrganization and Visualization of Music Archives”, \nProceedings of the 10th ACM International \nConference on Multimedia , Juan les Pins, France, \n2002. \n[12] Alonso M., Richard G., David B., “Accurate Tempo \nEstimation Based on Harmonic + Noise \nDecomposition”, EURASIP Journal on Applied \nSignal Processing Volume 2007, Issue 1, January \n2007. \n[13] Fujishima T. “realtime Chord Recognition of \nMusical Sound: a System Using Common Lisp \nMusic”. \n[14] Large E. and Kolen J., “Resonance and the \nPerception of Musical Meter”, Connection Science \n6(1), pp 177.208,  1994. \n558\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "A Probabilistic Subspace Model for Multi-instrument Polyphonic Transcription.",
        "author": [
            "Graham Grindlay",
            "Daniel P. W. Ellis"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1416842",
        "url": "https://doi.org/10.5281/zenodo.1416842",
        "ee": "https://zenodo.org/records/1416842/files/GrindlayE10.pdf",
        "abstract": "In this paper we present a general probabilistic model suit- able for transcribing single-channel audio recordings con- taining multiple polyphonic sources. Our system requires no prior knowledge of the instruments in the mixture, al- though it can benefit from this information if available. In contrast to many existing polyphonic transcription sys- tems, our approach explicitly models the individual instru- ments and is thereby able to assign detected notes to their respective sources. We use a set of training instruments to learn a model space which is then used during transcrip- tion to constrain the properties of models fit to the target mixture. In addition, we encourage model sparsity using a simple approach related to tempering. We evaluate our method on both recorded and synthe- sized two-instrument mixtures, obtaining average frame- level F-measures of up to 0.60 for synthesized audio and",
        "zenodo_id": 1416842,
        "dblp_key": "conf/ismir/GrindlayE10",
        "keywords": [
            "probabilistic model",
            "transcribing audio",
            "multiple polyphonic sources",
            "no prior knowledge",
            "instrument information",
            "explicit modeling",
            "assign detected notes",
            "model space",
            "constrain properties",
            "target mixture"
        ],
        "content": "A PROBABILISTIC SUBSPACE MODEL FOR MULTI-INSTRUMENT\nPOLYPHONIC TRANSCRIPTION\nGraham Grindlay\nLabROSA, Dept. of Electrical Engineering\nColumbia University\ngrindlay@ee.columbia.eduDaniel P.W. Ellis\nLabROSA, Dept. of Electrical Engineering\nColumbia University\ndpwe@ee.columbia.edu\nABSTRACT\nIn this paper we present a general probabilistic model suit-\nable for transcribing single-channel audio recordings con-\ntaining multiple polyphonic sources. Our system requires\nno prior knowledge of the instruments in the mixture, al-\nthough it can beneﬁt from this information if available.\nIn contrast to many existing polyphonic transcription sys-\ntems, our approach explicitly models the individual instru-\nments and is thereby able to assign detected notes to their\nrespective sources. We use a set of training instruments to\nlearn a model space which is then used during transcrip-\ntion to constrain the properties of models ﬁt to the target\nmixture. In addition, we encourage model sparsity using a\nsimple approach related to tempering.\nWe evaluate our method on both recorded and synthe-\nsized two-instrument mixtures, obtaining average frame-\nlevel F-measures of up to 0.60 for synthesized audio and\n0.53 for recorded audio. If knowledge of the instrument\ntypes in the mixture is available, we can increase these\nmeasures to 0.68 and 0.58, respectively, by initializing the\nmodel with parameters from similar instruments.\n1. INTRODUCTION\nTranscribing a piece of music from audio to symbolic form\nremains one of the most challenging problems in music in-\nformation retrieval. Different variants of the problem can\nbe deﬁned according to the number of instruments present\nin the mixture and the degree of polyphony. Much research\nhas been conducted on the case where the recording con-\ntains only a single (monophonic) instrument and reliable\napproaches to pitch estimation in this case have been de-\nveloped [3]. However, when polyphony is introduced the\nproblem becomes far more difﬁcult as note harmonics of-\nten overlap and interfere with one another. Although there\nare a number of note properties that are relevant to poly-\nphonic transcription, to date most research has focused on\npitch, note onset time, and note offset time, while the prob-\nlem of assigning notes to their source instruments has re-\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2010 International Society for Music Information Retrieval.ceived substantially less attention. Determining the source\nof a note is not only important in its own right, but it is\nlikely to improve overall transcription accuracy by helping\nto reduce cross-source interference. In order to distinguish\nbetween different instruments, we might wish to employ\ninstrument-speciﬁc models. However, in general, we do\nnot have access to the exact source models and so must es-\ntimate them directly from the mixture. This unsupervised\nlearning problem is particularly difﬁcult when only a sin-\ngle observation channel is available.\nNon-negative Matrix Factorization (NMF) [8] has been\nshown to be a useful approach to single-channel music\ntranscription [10]. The algorithm is typically applied to the\nmagnitude spectrum of the target mixture, V, for which it\nyields a factorization V\u0019WH whereWcorresponds to\na set of spectral basis vectors and Hcorresponds to the set\nof activation vectors over time. There are, however, several\nissues that arise when using NMF for unsupervised tran-\nscription. First, it is unclear how to determine the number\nof basis vectors required. If we use too few, a single ba-\nsis vector may be forced to represent multiple notes, while\nif we use too many some basis vectors may have unclear\ninterpretations. Even if we manage to choose the correct\nnumber of bases, we still face the problem of determining\nthe mapping between bases and pitches as the basis order\nis typically arbitrary. Second, although this framework is\ncapable of separating notes from distinct instruments as in-\ndividual columns of W(and corresponding rows of H),\nthere is no simple solution to the task of organizing these\nindividual columns into coherent blocks corresponding to\nparticular instruments.\nSupervised transcription can be performed when Wis\nknown a priori. In this case, we know the ordering of the\nbasis vectors and therefore how to partition Hby source.\nHowever, we do not usually have access to this informa-\ntion and must therefore use some additional knowledge.\nOne approach, which has been explored in several recent\npapers, is to impose constraints on the solution of Wor its\nequivalent. Virtanen and Klapuri use a source-ﬁlter model\nto constrain the basis vectors to be formed from source\nspectra and ﬁlter activations [13]. Vincent et. al impose\nharmonicity constraints on the basis vectors by modeling\nthem as combinations of narrow-band spectra [12]. In prior\nwork, we proposed the Subspace NMF algorithm which\nlearns a model parameter subspace from training examples\nand then constrains Wto lie in this subspace [5].\n21\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \nPET ModelTest Mixture\nEigeninstrument Model\nTraining Instruments\npitchfrequency\npitchfrequencyProbabilistic\nEigeninstruments\nNMF\n(optional init.)\nPost\nProcessing\n...\nfrequency\ntime\nFigure 1. Illustration of the Probabilistic Eigeninstrument\nTranscription (PET) system. First, a set of training instru-\nments are used to derive the eigeninstruments. These are\nthen used by the PET model to learn the probability dis-\ntributionP(p;tjs), which is post-processed into source-\nspeciﬁc binary transcriptions, T1;T2;:::;TS.\nRecently, it has been shown [4, 9] that NMF is very\nclosely related to Probabilistic Latent Semantic Analysis\n(PLSA) [6]. In this paper, we extend the Subspace NMF\nalgorithm to a probabilistic setting in which we explicitly\nmodel the source probabilities, allow for multi-component\nnote models, and use sparsity constraints to improve sep-\naration and transcription accuracy. The new approach re-\nquires no prior knowledge about the target mixture other\nthan the number of instruments present. If, however, in-\nformation about the instrument types is available, it can be\nused to seed the model and improve transcription accuracy.\nAlthough we do not discuss the details here due to a\nlack of space, we note that our system effectively performs\ninstrument-level source-separation as a part of the tran-\nscription process: once the model parameters have been\nsolved for, individual sources can be reconstructed in a\nstraightforward manner.\n2. METHOD\nOur system is based on the assumption that a suitably-\nnormalized magnitude spectrogram, V, can be modeled\nas a joint distribution over time and frequency, P(f;t).\nThis quantity can be factored into a frame probability P(t),\nwhich can be computed directly from the observed data,\nand a conditional distribution over frequency bins P(fjt);\nspectrogram frames are treated as repeated draws from an\nunderlying random process characterized by P(fjt). We\ncan model this distribution with a mixture of latent factors\nas follows:\nP(f;t) =P(t)P(fjt) =P(t)X\nzP(fjz)P(zjt) (1)\nNote that when there is only a single latent variable\nzthis is the same as the PLSA model and is effectively\nidentical to NMF. The latent variable framework, however,\nmakes it much more straightforward to introduce additional\nparameters and constraints.Suppose now that we wish to model a mixture of Sin-\nstrument sources, where each source has Ppossible pitches,\nand each pitch is represented by a set of Zcomponents.\nWe can extend the model described by (1) to accommo-\ndate these parameters as follows:\n~P(fjt) =X\ns;p;zP(fjp;z;s)P (zjs;p;t)P (sjp;t)P (pjt)\n(2)\nwhere we have used the notation ~P(fjt)to denote the fact\nthat our model reconstruction approximates the true dis-\ntribution,P(fjt). Notice that we have chosen to factor\nthe distribution such that the source probability depends\non pitch and time. Intuitively, this may seem odd as we\nmight expect the generative process to ﬁrst draw a source\nand then a pitch conditioned on that source. The reason\nfor this factorization has to do with the type of sparsity\nconstraints that we wish to impose on the model. This is\ndiscussed more fully in Section 2.2.2.\n2.1 Instrument Models\nP(fjp;z;s) represents the instrument models that we are\ntrying to ﬁt to the data. However, as discussed in Section 1,\nwe usually don’t have access to the exact models that pro-\nduced the mixture and a blind parameter search is highly\nunder-constrained. The solution proposed in [5], which we\nextend here, is to model the instruments as mixtures of ba-\nsis models or “eigeninstruments”. This approach is similar\nin spirit to the eigenvoice technique used in speech recog-\nnition [7].\nSuppose that we have a set of instruments models Mfor\nuse in training. Each of these models Mi2M hasFPZ\nparameters, which we concatenate into a super-vector, mi.\nThese super-vectors are then stacked together into a matrix,\n\u0002, and NMF with some rank Kis used to ﬁnd \u0002\u0019\nC.1\nThe set of coefﬁcient vectors, C, is typically discarded at\nthis point, although it can be used to initialize the full tran-\nscription system as well (see Section 3.4). The Kbasis\nvectors in \nrepresent the eigeninstruments. Each of these\nvectors is reshaped to the F-by-P -by-Z model size to form\nthe eigeninstrument distribution, ^P(fjp;z;k ). Mixtures of\nthis distribution can now be used to model new instruments\nas follows:\nP(fjp;z;s) =X\nk^P(fjp;z;k )P(kjs) (3)\nwhereP(kjs)represents an instrument-speciﬁc distribu-\ntion over eigeninstruments. This model reduces the size of\nthe parameter space for each source instrument in the mix-\nture fromFPZ , which is typically tens of thousands, to\nKwhich is typically between 10and100. Of course the\nquality of this parametrization depends on how well the\neigeninstrument basis spans the true instrument parameter\nspace, but assuming a sufﬁcient variety of training instru-\nments are used, we can expect good coverage.\n1Some care has to be taken to ensure that the bases in \nare properly\nnormalized so that each section of Fentries sums to 1, but so long as\nthis requirement is met, any decomposition that yields non-negative basis\nvectors can be used.\n22\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)2.2 Transcription Model\nWe are now ready to present the full transcription model\nproposed in this paper, which we refer to as Probabilistic\nEigeninstrument Transcription (PET) and is illustrated in\nFigure 1. Combining the probabilistic model in (2) and the\neigeninstrument model in (3), we arrive at the following:\n~P(fjt) =X\ns;p;z;k^P(fjp; z; k )P(kjs)P(zjs; p; t)P (sjp; t)P (pjt)\n(4)\nOnce we have solved for the model parameters, we cal-\nculate the joint distribution over pitch and time conditional\non source:\nP(p;tjs) =P(sjp;t)P (pjt)P (t)P\np;tP(sjp;t)P (pjt)P (t)(5)\nThis distribution represents the transcription of source\ns, but still needs to be post-processed to a binary pianoroll\nrepresentation so that it can be compared with ground truth\ndata. This is done using a simple threshold \r(see Sec-\ntion 3.3). We refer to the ﬁnal pianoroll transcription of\nsourcesasTs.\n2.2.1 Update Equations\nWe solve for the parameters in (4) using the Expectation-\nMaximization algorithm. This involves iterating between\ntwo update steps until convergence. In the ﬁrst (expecta-\ntion) step, we calculate the posterior distribution over the\nhidden variables s,p,z, andk, for each time-frequency\npoint given the current estimates of the model parameters:\nP(s; p; z; k jf; t) =^P(fjp; z; k )P(kjs)P(zjs; p; t)P (sjp; t)P (pjt)\n~P(fjt)\n(6)\nIn the second (maximization) step, we use this poste-\nrior to maximize the expected log-likelihood of the model\ngiven the data:\nL=X\nf;tVf;tlog\u0010\nP(t)~P(fjt)\u0011\n(7)\nwhereVf;tare values from our original spectrogram. This\nresults in the following update equations:\nP(kjs) =P\nf;t;zP(s;p;z;kjf;t)Vf;tP\nf;k;t;zP(s;p;z;kjf;t)Vf;t(8)\nP(zjs;p;t) =P\nf;kP(s;p;z;kjf;t)Vf;tP\nf;k;zP(s;p;z;kjf;t)Vf;t(9)\nP(sjp;t) =P\nf;k;zP(s;p;z;kjf;t)Vf;tP\nf;k;s;zP(s;p;z;kjf;t)Vf;t(10)\nP(pjt) =P\nf;k;s;zP(s;p;z;kjf;t)Vf;tP\nf;k;p;s;zP(s;p;z;kjf;t)Vf;t(11)2.2.2 Sparsity\nThe update equations given in Section 2.2.1 represent a\nmaximum-likelihood solution to the model. However, in\npractice it can be advantageous to introduce additional con-\nstraints. The idea of parameter sparsity has proved to be\nuseful for a number of audio-related tasks [1, 11]. For\nmulti-instrument transcription, there are several ways in\nwhich it might make sense to constrain the model solu-\ntion in this way. First, it is reasonable to expect that if\npitchpis active at time t, then only a small fraction of the\ninstrument sources are responsible for it. This belief can\nbe encoded in the form of a sparsity prior on the distribu-\ntionP(sjp;t). Similarly, we generally expect that only a\nfew pitches are active in each time frame, which implies a\nsparsity constraint on P(pjt).\nOne way of encouraging sparsity in probabilistic mod-\nels is through the use of the entropic prior [2]. This tech-\nnique uses an exponentiated negative-entropy term as a\nprior on parameter distributions. Although it can yield\ngood results, the solution to the maximization step is com-\nplicated, as it involves solving a system of transcendental\nequations. As an alternative, we have found that simply\nmodifying the maximization steps in (10) and (11) as fol-\nlows gives good results:\nP(sjp;t) =hP\nf;k;zP(s;p;z;kjf;t)Vf;ti\u000b\nP\nshP\nf;k;zP(s;p;z;kjf;t)Vf;ti\u000b(12)\nP(pjt) =hP\nf;k;s;zP(s;p;z;kjf;t)Vf;ti\f\nP\nphP\nf;k;s;zP(s;p;z;kjf;t)Vf;ti\f(13)\nWhen\u000band\fare less than 1, this is closely related to\ntheTempered EM algorithm used in PLSA [6]. However, it\nis clear that when \u000band\fare greater than 1, theP(sjp;t)\nandP(pjt) distributions are “sharpened”, thus decreasing\ntheir entropies and encouraging sparsity.\n3. EVALUATION\n3.1 Data\nTwo data sets were used in our experiments, one contain-\ning both synthesized and recorded audio and the other con-\ntaining just synthesized audio. There are 15tracks, 3256\nnotes, and 18843 frames in total. The speciﬁc properties of\nthe data sets are summarized in Table 1. All tracks had two\ninstrument sources, although the actual instruments varied.\nFor the synthetic tracks, the MIDI versions were synthe-\nsized at an 8kHz sampling rate using timidity and the SGM\nV2:01 soundfont. A 1024-point STFT with 96ms window\nand24ms hop was then taken and the magnitude spectro-\ngram retained.\nThe ﬁrst data set is based on a subset of the woodwind\ndata supplied for the MIREX Multiple Fundamental Fre-\nquency Estimation and Tracking task.2The ﬁrst 21sec-\n2http://www.music-ir.org/mirex/2009/index.\nphp/Multiple_Fundamental_Frequency_Estimation_&_\nTracking\n23\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Type # Tracks # Notes # Frames\nWoodwind S/R 6 1266 5424\nBach S 3 724 7995\nTable 1. Summary of the two data sets used. S and R\ndenote synthesized and recorded, respectively.\nonds from the bassoon, clarinet, oboe, and ﬂute tracks were\nmanually transcribed. These instrument tracks were then\ncombined in all 6possible pairings. It is important to note\nthat this data is taken from the MIREX development set\nand that the primary test data is not publicly available. In\naddition, most authors of other transcription systems do\nnot report results on the development data, making com-\nparisons difﬁcult.\nThe second data set is comprised of three pieces by J.S.\nBach arranged as duets. The pieces are: Herz und Mund\nund Tat und Leben (BWV 147) for acoustic bass and pic-\ncolo, Ich steh mit einem Fuß im Grabe (BWV 156) for tuba\nand piano, and roughly the ﬁrst half of Wachet auf, ruft uns\ndie Stimme (BWV 140) for cello and ﬂute. We chose in-\nstruments that were, for the most part, different from those\nused in the woodwind data set while also trying to keep the\ninstrumentation as appropriate as possible.\n3.2 Instrument Models\nWe used a set of 33instruments of varying types to de-\nrive our instrument model. This included a roughly equal\nproportion of keyboard, plucked string, bowed, and wind\ninstruments. The instrument models were generated with\ntimidity, but in order to keep the tests with synthesized au-\ndio as fair as possible, a different soundfont (Papelmedia\nFinal SF2 XXL) was used.3Each instrument model con-\nsisted of 58pitches (C2-A6#), which were built as follows:\nnotes of duration 1s were synthesized at an 8kHz sampling\nrate, using velocities 40,80, and 100. A 1024-point STFT\nwas taken of each, and the magnitude spectra were aver-\naged across velocities to make the model more robust to\ndifferences in loudness. The models were then normal-\nized so that the frequency components (spectrogram rows)\nsummed to 1for each pitch. Next, NMF with rank Z(the\ndesired number of components per pitch) was run on this\nresult withHinitialized to a heavy main diagonal struc-\nture. This encouraged the ordering of the bases to be “left-\nto-right”.\nOne potential issue with this approach has to do with\nthe differences in the natural playing ranges of the instru-\nments. For example, a violin generally cannot play below\nG3, although our model includes notes below this. There-\nfore, we masked out (i.e. set to 0) the parameters of the\nnotes outside the playing range of each instrument used in\ntraining. Then, as described in Section 2.1, the instrument\nmodels were stacked into super vector form and NMF with\na rank ofK= 30 (chosen empirically) was run to ﬁnd the\ninstrument bases, \n. These bases were then unstacked to\nform the eigeninstruments, ^P(fjp;z;k ).\n3http://www.papelmedia.de/english/index.htm\n  Clarinet (ground truth)\npitch\npitchBassoon (ground truth)\ntimeBassoon (PET)\npitch\npitchClarinet (PET)\nFigure 2. Example PET (\f = 2) output distribution\nP(p;tjs) and ground truth data for the bassoon-clarinet\nmixture from the recorded woodwind data set.\nIn preliminary experiments, we did not ﬁnd a signiﬁcant\nadvantage to values of Z > 1and so the full set of exper-\niments presented below was carried out with only a single\ncomponent per pitch.\n3.3 Metrics\nWe evaluate our method using precision (P ), recall (R ),\nand F-measure (F ) on both the frame and note levels. Note\nthat each reported metric is an average over sources. In ad-\ndition, because the order of the sources in P(p;tjs) is arbi-\ntrary, we compute sets of metrics for all possible permuta-\ntions (two in our experiments since there are two sources)\nand report the set with the best frame-level F-measure.\n24\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)When computing the note-level metrics, we consider a\nnote onset to be correct if it falls within +/- 50ms of the\nground truth onset. At present, we don’t consider offsets\nfor the note-level evaluation, although this information is\nreﬂected in the frame-level metrics.\nThe threshold \rused to convert P(p;tjs) to a binary\npianoroll was determined empirically for each algorithm\nvariant and each data set. This was done by computing\nthe threshold that maximized the average frame-level F-\nmeasure across tracks in the data set.\n3.4 Experiments\nWe evaluated several variations of our algorithm so as to\nexplore the effects of sparsity and to assess the perfor-\nmance of the eigeninstrument model. For each of the three\ndata sets, we computed the frame and note metrics using\nthe three variants of the PET model: PET without spar-\nsity, PET with sparsity on the instruments given the pitches\nP(sjp;t) (\u000b= 2), and PET with sparsity on the pitches at\na given time P(pjt) (\f= 2). In these cases, all parame-\nters were initialized randomly and the algorithm was run\nfor100iterations.\nAlthough we are primarily interested in blind transcrip-\ntion (i.e. no prior knowledge of the instruments present\nin the mixture), it is interesting to examine cases where\nmore information is available as these can provide upper-\nbounds on performance. First, consider the case where we\nknow the instrument types present in the mixture. For the\nsynthetic data, we have access not only to the instrument\ntypes, but also to the oracle models for these instruments.\nIn this case we hold P(fjp;s;z )ﬁxed and solve the ba-\nsic model given in (2). The same can be done with the\nrecorded data, except that we don’t have oracle models for\nthese recordings. Instead, we can just use the appropriate\ninstrument models from the training set Mas approxima-\ntions. This case, which we refer to as “ﬁxed” in the experi-\nmental results, represents a semi-supervised version of the\nPET system.\nWe might also consider using the instrument models M\nthat we used in eigeninstrument training in order to initial-\nize the PET model in the hope that the system will be able\nto further optimize their settings. We can do this by taking\nthe appropriate eigeninstrument coefﬁcient vectors csand\nusing them to initialize P(kjs). Intuitively, we are trying\nto start the PET model in the correct “neighborhood” of\neigeninstrument space. These results are denoted “init”.\nFinally, as a baseline comparison, we consider generic\nNMF-based transcription (with generalized KL divergence\nas a cost function) where the instrument models (submatri-\nces ofW) have been initialized with a generic model de-\nﬁned as the average of the instrument models in the train-\ning set.\n3.5 Results\nThe results of our approach are summarized in Tables 2–4.\nAs a general observation, we can see that the sparsity fac-\ntors have helped improve model performance in almost all\ncases, although different data sets beneﬁt in different ways.Frame Note\nPRFPRF\nPET 0.56 0.64 0.56 0.42 0.73 0.51\nPET\u000b=2 0.60 0.61 0.60 0.46 0.73 0.56\nPET\f=2 0.57 0.64 0.56 0.51 0.79 0.58\nPETinit 0.71 0.68 0.68 0.64 0.86 0.71\nPEToracle 0.84 0.84 0.84 0.82 0.93 0.87\nNMF 0.34 0.48 0.39 0.19 0.59 0.29\nTable 2. Results for the synthetic woodwind data set. All\nvalues are averages across sources and tracks.\nFrame Note\nPRFPRF\nPET 0.52 0.52 0.50 0.41 0.73 0.50\nPET\u000b=2 0.49 0.57 0.51 0.41 0.78 0.51\nPET\f=2 0.58 0.53 0.53 0.46 0.72 0.55\nPETinit 0.60 0.60 0.58 0.48 0.82 0.58\nPETfixed 0.57 0.58 0.55 0.45 0.77 0.54\nNMF 0.35 0.55 0.42 0.27 0.77 0.38\nTable 3. Results for the recorded woodwind data set. All\nvalues are averages across sources and tracks.\nFor the synthetic woodwind data set, sparsity on sources,\nP(sjp;t), increased the average F-measure on the frame-\nlevel, but at the note-level, sparsity on pitches, P(pjt), had\na larger impact. For the recorded woodwind data, sparsity\nonP(pjt) beneﬁted both frame and note-level F-measures\nthe most. With the Bach data, we see that encouraging\nsparsity inP(pjt) was much more important than it was for\nP(sjp;t) on both the frame and note-level. In fact, impos-\ning sparsity on P(sjp;t) seems to have actually hurt frame-\nlevel performance relative to the non-sparse PET system.\nThis may be explained by the fact that the instrument parts\nin the Bach pieces tend to be simultaneously active much\nof the time.\nAs we would expect, the baseline NMF system per-\nforms the worst in all test cases – not surprising given\nthe limited information and lack of constraints. Also un-\nsurprising is the fact that the oracle models are the top-\nperformers on the synthetic data sets. However, notice\nthat the randomly-initialized PET systems perform about\nFrame Note\nPRFPRF\nPET 0.50 0.65 0.54 0.21 0.60 0.30\nPET\u000b=2 0.50 0.57 0.51 0.22 0.51 0.30\nPET\f=2 0.55 0.66 0.59 0.24 0.65 0.34\nPETinit 0.53 0.58 0.53 0.23 0.50 0.30\nPEToracle 0.91 0.85 0.87 0.53 0.83 0.64\nNMF 0.36 0.50 0.42 0.09 0.46 0.14\nTable 4. Results for the synthetic Bach data set. All values\nare averages across sources and tracks.\n25\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)as well as the ﬁxed model on recorded data. This im-\nplies that the algorithm was able to discover appropriate\nmodel parameters even in the blind case where it had no\ninformation about the instrument types in the mixture. It\nis also noteworthy that the best performing system for the\nrecorded data set is the initialized PET variant. This sug-\ngests that, given good initializations, the algorithm was\nable to further adapt the instrument model parameters to\nimprove the ﬁt to the target mixture.\nWhile the results on both woodwind data sets are rel-\natively consistent across frame and note levels, the Bach\ndata set exhibits a signiﬁcant discrepancy between the two\nmetrics, with substantially lower note-level scores. This\nis true even for the oracle model which achieves an aver-\nage note-level F-measure of 0:64. There are two possible\nexplanations for this. First, recall that our determination\nof both the optimal threshold \ras well as the order of the\nsources inP(p;tjs) was based on the average frame-level\nF-measure. We opted to use frame-level metrics for this\ntask as they are a stricter measure of transcription quality.\nHowever, given that the performance is relatively consis-\ntent for the woodwind data, it seems more likely that the\ndiscrepancy is due to instrumentation. In particular, the al-\ngorithms seem to have had difﬁculty with the soft onsets of\nthe cello part in Wachet auf, ruft uns die Stimme.\n4. CONCLUSIONS\nWe have presented a probabilistic model for the challeng-\ning problem of multi-instrument polyphonic transcription.\nOur method makes use of training instruments in order to\nlearn a model parameter subspace that constrains the solu-\ntions of new models. Sparsity terms are also introduced to\nhelp further constrain the solution. We have shown that\nthis approach can perform reasonably well in the blind\ntranscription setting where no knowledge other than the\nnumber of instruments is assumed. In addition, knowl-\nedge of the types of instruments in the mixture (informa-\ntion which is relatively easy to obtain) was shown to im-\nprove performance signiﬁcantly over the basic model. Al-\nthough the experiments presented in this paper only con-\nsider two-instrument mixtures, the PET model is general\nand preliminary tests suggest that it can handle more com-\nplex mixtures as well.\nThere are several areas in which the current system could\nbe improved. First, the thresholding technique that we\nhave used is extremely simple and results could probably\nbe improved signiﬁcantly through the use of pitch depen-\ndent thresholding or more sophisticated classiﬁcation. Sec-\nond, and perhaps most importantly, although early experi-\nments did not show a beneﬁt to using multiple components\nfor each pitch, it seems likely that the pitch models could\nbe enriched substantially. Many instruments have complex\ntime-varying structures within each note that would seem\nto be important for recognition. We are currently explor-\ning ways to incorporate this type of information into our\nsystem.5. ACKNOWLEDGMENTS\nThis work was supported by the NSF grant IIS-0713334.\nAny opinions, ﬁndings and conclusions or recommenda-\ntions expressed in this material are those of the authors and\ndo not necessarily reﬂect the views of the sponsors.\n6. REFERENCES\n[1] S.A. Abdallah and M.D. Plumbley. Polyphonic music\ntranscription by non-negative sparse coding of power\nspectra. In ISMIR, 2004.\n[2] M. Brand. Structure learning in conditional probability\nmodels via an entropic prior and parameter extinction.\nNeural Computation, 11(5):1155–1182, 1999.\n[3] A. de Cheveign ´e and H. Kawahara. YIN, a fundamen-\ntal frequency estimator for speech and music. The Jour-\nnal of the Acoustical Society of America, 111(1917),\n2002.\n[4] E. Gaussier and C. Goutte. Relation between PLSA and\nNMF and implications. In SIGIR, 2005.\n[5] G. Grindlay and D.P.W. Ellis. Multi-voice polyphonic\nmusic transcription using eigeninstruments. In WAS-\nPAA, 2009.\n[6] T. Hofmann. Probabilistic latent semantic analysis. In\nUncertainty in AI, 1999.\n[7] R. Kuhn, J. Junqua, P. Nguyen, and N. Niedziel-\nski. Rapid speaker identiﬁcation in eigenvoice space.\nIEEE Transactions on Speech and Audio Processing,\n8(6):695–707, November 2000.\n[8] D.D. Lee and H.S. Seung. Algorithms for non-negative\nmatrix factorization. In NIPS, 2001.\n[9] M. Shashanka, B. Raj, and P. Smaragdis. Probabilistic\nlatent variable models as non-negative factorizations.\nComputational Intelligence and Neuroscience, 2008,\n2008.\n[10] P. Smaragdis and J.C. Brown. Non-negative matrix fac-\ntorization for polyphonic music transcription. In WAS-\nPAA, 2003.\n[11] P. Smaragdis, M. Shashanka, and B. Raj. A sparse non-\nparametric approach for single channel separation of\nknown sounds. In NIPS, 2009.\n[12] E. Vincent, N. Bertin, and R. Badeau. Harmonic and\ninharmonic non-negative matrix factorization for poly-\nphonic pitch transcription. In ICASSP, 2008.\n[13] T. Virtanen and A. Klapuri. Analysis of polyphonic au-\ndio using source-ﬁlter model and non-negative matrix\nfactorization. In NIPS, 2006.\n26\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "What Makes Beat Tracking Difficult? A Case Study on Chopin Mazurkas.",
        "author": [
            "Peter Grosche",
            "Meinard Müller",
            "Craig Stuart Sapp"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1415852",
        "url": "https://doi.org/10.5281/zenodo.1415852",
        "ee": "https://zenodo.org/records/1415852/files/GroscheMS10.pdf",
        "abstract": "The automated extraction of tempo and beat information from music recordings is a challenging task. Especially in the case of expressive performances, current beat track- ing approaches still have significant problems to accurately capture local tempo deviations and beat positions. In this paper, we introduce a novel evaluation framework for de- tecting critical passages in a piece of music that are prone to tracking errors. Our idea is to look for consistencies in the beat tracking results over multiple performances of the same underlying piece. As another contribution, we further classify the critical passages by specifying musi- cal properties of certain beats that frequently evoke track- ing errors. Finally, considering three conceptually different beat tracking procedures, we conduct a case study on the basis of a challenging test set that consists of a variety of piano performances of Chopin Mazurkas. Our experimen- tal results not only make the limitations of state-of-the-art beat trackers explicit but also deepens the understanding of the underlying music material.",
        "zenodo_id": 1415852,
        "dblp_key": "conf/ismir/GroscheMS10",
        "keywords": [
            "tempo",
            "beat",
            "information",
            "automated extraction",
            "music recordings",
            "expressive performances",
            "critical passages",
            "consistencies",
            "musical properties",
            "beat tracking errors"
        ],
        "content": "WHAT MAKES BEAT TRACKING DIFFICULT?\nA CASE\nSTUDY ON CHOPIN MAZURKAS\nPeter Grosche\nSaarland University\nand MPI Informatik\npgrosche@mpi-inf.mpg.deMeinard M ¨uller\nSaarland University\nand MPI Informatik\nmeinard@mpi-inf.mpg.deCraig Stuart Sapp\nStandford University\nCCRMA / CCARH\ncraig@ccrma.stanford.edu\nABSTRACT\nThe automated extraction of tempo and beat information\nfrom music recordings is a challenging task. Especially\nin the case of expressive performances, current beat track-\ning approaches still have signiﬁcant problems to accurately\ncapture local tempo deviations and beat positions. In this\npaper, we introduce a novel evaluation framework for de-\ntecting critical passages in a piece of music that are prone\nto tracking errors. Our idea is to look for consistencies\nin the beat tracking results over multiple performances of\nthe same underlying piece. As another contribution, we\nfurther classify the critical passages by specifying musi-\ncal properties of certain beats that frequently evoke track-\ning errors. Finally, considering three conceptually different\nbeat tracking procedures, we conduct a case study on the\nbasis of a challenging test set that consists of a variety of\npiano performances of Chopin Mazurkas. Our experimen-\ntal results not only make the limitations of state-of-the-art\nbeat trackers explicit but also deepens the understanding of\nthe underlying music material.\n1. INTRODUCTION\nWhen listening to a piece of music, most humans are able\nto tap to the musical beat without difﬁculty. In recent years,\nvarious different algorithmic solutions for automatically\nextracting beat position from audio recordings have been\nproposed. However, transferring this cognitive process into\nan automated system that reliably works for the large va-\nriety of musical styles is still not possible. Modern pop\nand rock music with a strong beat and steady tempo can\nbe handled by many methods well, but extracting the beat\nlocations from highly expressive performances of, e.g., ro-\nmantic piano music, is a challenging task.\nTo better understand the shortcomings of recent beat\ntracking methods, signiﬁcant efforts have been made to\ncompare and investigate the performance of different\nstrategies on common datasets [6, 10, 13]. However, most\napproaches were limited to comparing the different meth-\nods by specifying evaluation measures that refer to an en-\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2010 International Society for Music Information Retrieval.tire recording or even an entire collection of recordings.\nSuch globally oriented evaluations do not provide any in-\nformation on the critical passages within a piece where\nthe tracking errors occur. Thus, no conclusions can be\ndrawn from these experiments about possible musical rea-\nsons that lie behind the beat tracking errors. A ﬁrst analysis\nofmusical properties inﬂuencing the beat tracking quality\nwas conducted by Dixon [6], who proposed quantitative\nmeasures for the rhythmic complexity and for variations in\ntempo and timings. However, no larger evaluations were\ncarried out to show a correlation between these theoretical\nmeasures and the actual beat tracking quality.\nIn this paper, we continue this strand of research by\nanalyzing the tracking results obtained by different beat\ntracking procedures. As one main idea of this paper, we\nintroduce a novel evaluation framework that exploits the\nexistence of different performances available for a given\npiece of music. For example, in our case study we revert\nto a collection of recordings for the Chopin Mazurkas con-\ntaining in average over 50 performances for each piece.\nBased on a local, beat-wise histogram, we simultaneously\ndetermine consistencies of beat tracking errors over many\nperformances. The underlying assumption is, that tracking\nerrors consistently occurring in many performances of a\npiece are likely caused by musical properties of the piece,\nrather than physical properties of a speciﬁc performance.\nAs a further contribution, we classify the beats of the crit-\nical passages by introducing various types of beats such\nas non-event beats, ornamented beats, weak bass beats, or\nconstant harmony beats. Each such beat class stands for a\nmusical performance-independent property that frequently\nevokes beat tracking errors. In our experiments, we evalu-\nated three conceptually different beat tracking procedures\non a corpus consisting of 300 audio recordings correspond-\ning to ﬁve different Mazurkas. For each recording, the\ntracking results were compared with manually annotated\nground-truth beat positions. Our local evaluation frame-\nwork and detailed analysis explicitly indicates various lim-\nitations of current state-of-the-art beat trackers, thus laying\nthe basis for future improvements and research directions.\nThis paper is organized as follows: In Sect. 2, we for-\nmalize and discuss the beat tracking problem. In Sect. 3,\nwe describe the underlying music material and specify var-\nious beat classes. After summarizing the three beat track-\ning strategies (Sect. 4) and introducing the evaluation mea-\nsure (Sect. 5) used in our case study, we report on the\nexperimental results in Sect. 6. Finally, we conclude in\n649\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)ID Composer Piece #(Meas.) #(Beats) #(Perf.)\nM17-4 Chopin Op. 17, No. 4 132 396 62\nM24-2 Chopin\nOp. 24, No. 2 120 360 64\nM30-2 Chopin Op. 30, No. 2 65 193 34\nM63-3 Chopin Op. 63, No. 3 77 229 88\nM68-3 Chopin Op. 68, No. 3 61 181 50\nTable 1: The ﬁve Chopin Mazurkas and their identiﬁers used in\nour\nstudy. The last three columns indicate the number of mea-\nsures, beats, and performances available for the respective piece.\nSect. 7 with a discussion of future research directions. Fur-\nther related work is discussed in the respective sections.\n2. PROBLEM SPECIFICATION\nFor a given piece of music, let Ndenote the number of mu-\nsical beats. Enumerating all beats, we identify the set of\nmusical beats with the set B= [1 : N] :={1,2,... ,N }.\nGiven a performance of the piece in the form of an audio\nrecording, the musical beats correspond to speciﬁc physi-\ncal time positions within the audio ﬁle. Let π:B → Rbe\nthe mapping that assigns each musical beat b∈ B to the\ntime position π(b)of its occurrence in the performance. In\nthe following, a time position π(b)is referred to as phys-\nical beat or simply as beat of the performance. Then, the\ntask of beat tracking is to recover the set {π(b)|b∈ B} of\nall beats from a given audio recording.\nNote that this speciﬁcation of the beat tracking problem\nis somewhat simplistic, as we only consider physical beats\nthat are deﬁned by onset events. More generally, a beat is\na perceptual phenomenon and perceptual beat times do not\nnecessarily coincide with physical beat times [7]. Further-\nmore, the perception of beats varies between listeners.\nFor determining physical beat times, we now discuss\nsome of the problems, one has to deal with in practice.\nTypically, a beat goes along with a note onset revealed by\nan increase of the signal’s energy or a change in the spec-\ntral content. However, in particular for non-percussive mu-\nsic, one often has soft note onsets, which lead to blurred\nnote transitions rather than sharp note onset positions. In\nsuch cases, there are no precise timings of note events\nwithin the audio recording, and the assignment of exact\nphysical beat positions becomes problematic. This issue is\naggravated in the presence of tempo changes and expres-\nsive tempo nuances (e.g., ritardando and accelerando).\nBesides such physical reasons, there may also be a num-\nber of musical reasons for beat tracking becoming a chal-\nlenging task. For example, there may be beats with no\nnote event going along with them. Here, a human may still\nperceive a steady beat, but the automatic speciﬁcation of\nphysical beat positions is quite problematic, in particular\nin passages of varying tempo where interpolation is not\nstraightforward. Furthermore, auxiliary note onsets can\ncause difﬁculty or ambiguity in deﬁning a speciﬁc physical\nbeat time. In music such as the Chopin Mazurkas, the main\nmelody is often embellished by ornamented notes such as\ntrills, grace notes, or arpeggios. Also, for the sake of ex-\npressiveness, the notes of a chord need not be played at the\nsame time, but slightly displaced in time. This renders a\nprecise deﬁnition of a physical beat position impossible.\n(a)\n(b)\n(c)\n(d)\n(e)\nFigure 1: Scores of example passages for the different beat\nclasses\nintroduced in Sect. 3. (a)Non-event beats (B 1) inM24-2,\n(b)Ornamented beats (B 3) inM30-2, (c)Constant harmony beats\n(B5) inM24-2, (d)Constant harmony beats (B 5) inM68-3, and\n(e)Weak bass beats (B 4) inM63-3.\n3. DATA AND ANNOTATIONS\nThe Mazurka Project [1] has collected over 2700 recorded\nperformances for 49Mazurkas by Fr ´ed´eric Chopin, rang-\ning from the early stages of music recording (Gr ¨unfeld\n1902) until today [15]. In our case study, we use 298\nrecordings corresponding to ﬁve of the 49Mazurkas, see\nTable 1. For each of theses recordings the beat positions\nwere annotated manually [15]. These annotations are used\nas ground truth in our experiments. Furthermore, Hum-\ndrum and MIDI ﬁles of the underlying musical scores for\neach performance are provided, representing the pieces in\nan uninterpreted symbolic format.\nIn addition to the physical beat annotations of the per-\nformances, we created musical annotations by grouping\nthe musical beats Bin ﬁve different beat classes B1toB5.\nEach of these classes represents a musical property that\ntypically constitutes a problem for determining the beat po-\nsitions. The colors refer to Fig. 4 and Fig. 5.\n•Non-event beats B1(black): Beats that do not co-\nincide with any note events, see Fig. 1(a).\n•Boundary beats B2(blue): Beats of the ﬁrst mea-\nsure and last measure of the piece.\n•Ornamented beats B3(red): Beats that coincide\nwith ornaments such as trills, grace notes, or arpeg-\ngios, see Fig. 1(b).\n•Weak bass beats B4(cyan): Beats where only the\nleft hand is played, see Fig. 1(e).\n•Constant harmony beats B5(green): Beats that\ncorrespond to consecutive repetitions of the same\nchord, see Fig. 1(c-d).\nFurthermore, let B∗:=∪5\nk=1Bkdenote the union of the\nﬁve beat classes. Table 2 details for each Mazurka the\nnumber of beats assigned to the respective beat classes.\n650\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)ID |B| |B 1| |B2| |B3| |B4| |B5| |B ∗|\nM17-4 396 9 8 51 88 0 154\nM24-2 360\n10 8 22 4 12 55\nM30-2 193 2 8 13 65 0 82\nM63-3 229 1 7 9 36 0 47\nM68-3 181 17 7 0 14 12 37\nTable 2: The number of musical beats in each of the different\nbeat\nclasses deﬁned in Sect. 3. Each beat may be a member of\nmore than one class.\nNote that the beat classes need not be disjoint, i.e., each\nbeat may be assigned to more than one class. In Sect. 6,\nwe discuss the beat classes and their implications on the\nbeat tracking results in more detail.\n4. BEAT TRACKING STRATEGIES\nBeat tracking algorithms working on audio recordings typ-\nically proceed in three steps: In the ﬁrst step, note onset\ncandidates are extracted from the signal. More precisely,\nanovelty curve is computed that captures changes of the\nsignal’s energy, pitch or spectral content [3, 5, 8, 12]. The\npeaks of this curve indicate likely note onset candidates.\nFig. 2(c) shows a novelty curve for an excerpt of M17-\n4(identiﬁer explained in Table 1). Using a peak picking\nstrategy [3] note onsets can be extracted from this curve. In\nthe second step, the local tempo of the piece is estimated.\nTherefore, the onset candidates are analyzed with respect\nto locally periodic or reoccurring patterns [5, 12, 14]. The\nunderlying assumption is that the tempo of the piece does\nnot change within the analysis window. The choice of the\nwindow size constitutes a trade-off between the robustness\nof the tempo estimates and the capability to capture tempo\nchanges. In the third step, the sequence of beat positions\nis determined that best explains the locally periodic struc-\nture of the piece, in terms of frequency (tempo) and phase\n(timing) [5, 12], see Fig. 2(d).\nIn our experiments we use three different beat track-\ners. First, we directly use the onset candidates extracted\nfrom a novelty curve capturing spectral differences [11]\nas indicated by Fig. 2(c). In this method, referred to as\nONSET in the following sections, each detected note on-\nset is considered as a beat position. Second, as a repre-\nsentative of the beat tracking algorithms that transform the\nnovelty curve into the frequency (tempo) or periodicity do-\nmain [5, 12, 14], we employ the predominant local period-\nicity estimation [11], referred to as PLPin the following.\nWe use a window size of three seconds and initialize the\ntempo estimation with the mean of the annotated tempo.\nMore precisely, we deﬁne the global tempo range for each\nperformance covering one octave around the mean tempo,\ne.g., for a mean tempo of 120 BPM, tempo estimates in\nthe range [90 : 180] are valid. This prevents tempo dou-\nbling or halving errors and robustly allows for investigating\nbeat tracking errors, rather than tempo estimation errors.\nThe third beat tracking method (SYNC) we use in our ex-\nperiments employs the MIDI ﬁle available for each piece.\nThis MIDI ﬁle can be regarded as additional knowledge,\nincluding the pitch, onset time and duration of each note.\nUsing suitable synchronization techniques [9] on the ba-\nsis of coarse harmonic and very precise onset information,\n35 36 37 38 39 40 41 42 4300.511.535 36 37 38 39 40 41 42 43\n35 36 37 38 39 40 41 42 4300.51(a)\n(b)\n(c)\n(d)\nT\nime (sec)\nFigure 2: Representations for an excerpt of M17-4. (a)Score\nrepresentation of beats 60to74.(b)Annotated ground truth beats\nfor the performance pid50534-05 by Horowitz (1985), see [1]. (c)\nNovelty curve (note onset candidates indicated by circles). (d)\nPLP curve (beat candidates indicated by circles).\nwe identify for each musical event of the piece (given by\nthe MIDI ﬁle) the corresponding physical position within\na performance. This coordination of MIDI events to the\naudio is then used to determine the beat positions in a per-\nformance and simpliﬁes the beat tracking task to an align-\nment problem, where the number of beats and the sequence\nof note events is given as prior knowledge.\n5. EV ALUATION MEASURES\nMany evaluation measures have been proposed to quantify\nthe performance of beat tracking systems [4] by comparing\nthe beat positions determined by a beat tracking algorithm\nand annotated ground truth beats. These measures can be\ndivided into two groups. Firstly, measures that analyze\neach beat position separately and secondly, measures that\ntake the tempo and metrical levels into account [5, 12, 13].\nWhile the latter gives a better estimate of how well a se-\nquence of retrieved beats correlates with the manual anno-\ntation, it does not give any insight into the beat tracking\nperformance at a speciﬁc beat of the piece.\nIn this paper, we evaluate the beat tracking quality on\nthe beat-level of a piece and combine the results of all per-\nformances available for this piece. This allows for detect-\ning beats that are prone to errors in many performances.\nFor a given performance, let Π := {π(b)|b∈ B} be\nthe set of manually determined physical beats, which are\nused as ground truth. Furthermore, let Φ⊂Rbe the\nset of beat candidates obtained from a beat tracking pro-\ncedure. Given a tolerance parameter τ >0, we deﬁne\ntheτ-neighborhood Iτ(p)⊂Rof a beat p∈Πto be\nthe interval of length 2τcentered at p, see Fig. 3. We say\nthat a beat phas been identiﬁed if there is a beat candidate\nq∈Φin the τ-neighborhood of p,i.e.,q∈Φ∩Iτ(p). Let\nΠid⊂Πbe the set of all identiﬁed beats. Furthermore, we\nsay that a beat candidate q∈Φiscorrect ifqlies in the\nτ-neighborhood Iτ(p)of some beat p∈Πand there is no\nother beat candidate lying in Iτ(p)that is closer to pthan\nq. LetΦco⊂Φbe the set of all correct beat candidates.\nWe then deﬁne the precision P = P τ, the recall R = R τ,\nand F-measure F = F τas [4]\nP =|Φco|\n|Φ|,R=|Πi\nd|\n|Π|,F=2·P·R\nP + R. (1)\n651\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Iτ(p)\nπ(b−1) p=π( b) π(b+ 1) Time\nH(p)\nFigur\ne 3: Illustration of the τ-neighborhood Iτ(p)and the half-\nbeat neighborhood H(p)of a beat p=π(b),b∈ B.\nTable 3 shows the results of various beat tracking proce-\ndures on the Mazurka data. As it turns out, the F-measure\nis a relatively soft evaluation measure that only moderately\npunishes additional, non-correct beat candidates. As a con-\nsequence, the simple onset-based beat tracker seems to out-\nperform most other beat trackers. As for the Mazurka data,\nmany note onsets coincide with beats, the onset detection\nleads to a high recall, while having only a moderate deduc-\ntion in the precision.\nWe now introduce a novel evaluation measure that pun-\nishes non-correct beat candidates, which are often musi-\ncally meaningless, more heavily. To this end, we deﬁne\nahalf-beat neighborhood H(p)of a beat p=π(b)∈Π\nto be the interval ranging fromπ(b−1) −π(b)\n2(orπ(b)for\nb= 1) toπ(b+1) −π(b)\n2(orπ(b)forb=N), see Fig. 3.\nThen, we say that a beat b∈ Bhas been strongly identiﬁed\nif there is a beat candidate q∈Φwithq∈Φ∩Iτ(p)and\nifH(p)∩Φ ={q}forp=π(b). In other words, qis the\nonly beat candidate in the half-beat neighborhood of p. Let\nΠstid⊂Πbe the set of all strongly identiﬁed beats, then\nwe deﬁne the beat accuracy A=Aτto be\nA=|Πstid|\n|Π|. (2)\n6.\nEXPERIMENTS\nWe now discuss the experimental results obtained using\nour evaluation framework and explain the relations be-\ntween the beat tracking results and the beat classes intro-\nduced in Sect. 3.\nWe start with discussing Table 3. Here, the results of\nthe different beat tracking approaches for all performances\nof the ﬁve Mazurkas are summarized, together with some\nresults from the MIREX 2009 beat tracking task [2]. All\nbeat trackers used in our evaluation yield better results\nfor the Mazurkas than all trackers used in the MIREX\nevaluation. As noted before, the F-measure only moder-\nately punishes additional beats. In consequence, ONSET\n(F = 0 .754) seems to outperform all other methods, except\nSYNC (F = 0. 890). In contrast, the introduced beat ac-\ncuracy Apunishes false positives more heavily, leading to\nA= 0.535forONSET, which is signiﬁcantly lower than for\nPLP(A= 0.729) and SYNC (A= 0.890). For SYNC, the\nevaluation metrics P,R,F, and Aare equivalent because\nthe number of detected beats is always correct. Further-\nmore, SYNC is able to considerably outperform the other\nstrategies. This is not surprising, as it is equipped with\nadditional knowledge in the form of the MIDI ﬁle.\nThere are some obvious differences in the beat tracking\nresults of the individual Mazurkas caused by the musical\nreasons explained in [6]. First of all, all methods deliverSYNC ONSET PLP\nID P/R/F/A P R F A P R F A\nM17-4 0.837 0.552 0.958 0.697 0.479 0.615 0.743 0.672 0.639\nM24-2 0.931 0.758 0.956 0.845 0.703 0.798 0.940 0.862 0.854\nM30-2 0.900 0.692 0.975 0.809 0.623 0.726 0.900 0.803 0.788\nM63-3 0.890 0.560 0.975 0.706 0.414 0.597 0.744 0.661 0.631\nM68-3 0.875 0.671 0.885 0.758 0.507 0.634 0.755 0.689 0.674\nMean: 0.890 0.634 0.952 0.754 0.535 0.665 0.806 0.728 0.729\nMIREX Our Methods\nMethod DRP3 GP2 OGM2 TL SYNC ONSET PLP\nF 0.678 0.547 0.321 0.449 0.890 0.754 0.728\nTable 3: Comparison of the beat tracking performance of the\nthree\nstrategies used in this paper and the MIREX 2009 results\n(see [2] for an explanation) based on the evaluation metrics Pre-\ncision P, Recall R, F-measure Fand the beat accuracy A.\nthe best result for M24-2. This piece is rather simple, with\nmany quarter notes in the dominant melody line. M17-\n4is the most challenging for all three trackers because\nof a frequent use of ornaments and trills and many beat\npositions that are not reﬂected in the dominating melody\nline. For the ONSET tracker, M63-3 constitutes a challenge\n(A= 0.414), although this piece can be handled well by\ntheSYNC tracker. Here, a large number of notes that do not\nfall on beat positions provoke many false positives. This\nalso leads to a low accuracy of PLP(A= 0.631).\nGoing beyond this evaluation on a piece-level, Fig. 4\nand Fig. 5 illustrate the beat-level beat tracking results of\nour evaluation framework for the SYNC and PLPstrategy,\nrespectively. Here, for each beat b∈ B of a piece, the bar\nencodes for how many of the performances of this piece\nthe beat was not strongly identiﬁed (see Sect. 5). High bars\nindicate beats that are incorrectly identiﬁed in many perfor-\nmances, low bars indicate beats that are identiﬁed in most\nperformances without problems. As a consequence, this\nrepresentation allows for investigating the musical proper-\nties leading to beat errors. More precisely, beats that are\nconsistently wrong over a large number of performances\nof the same piece are likely to be caused by musical prop-\nerties of the piece, rather than physical properties of a spe-\nciﬁc performance. For example, for both tracking strate-\ngies (SYNC and PLP) and all ﬁve pieces, the ﬁrst and last\nbeats are incorrectly identiﬁed in almost all performances,\nas shown by the blue bars (B 2). This is caused by boundary\nproblems and adaption times of the algorithms.\nFurthermore, there is a number of signiﬁcant high bars\nwithin all pieces. The SYNC strategy for M68-3 (see Fig. 4)\nexhibits a number of isolated black bars. These non-event\nbeats do not fall on any note-event (B 1). As stated in\nSect. 2, especially when dealing with expressive music,\nsimple interpolation techniques do not work to infer these\nbeat positions automatically. The same beat positions are\nproblematic in the PLP strategy, see Fig. 5. For M30-2\n(Fig. 4) most of the high bars within the piece are assigned\ntoB3(red). These beats, which coincide with ornaments\nsuch as trills, grace notes, or arpeggios are physically not\nwell deﬁned and hard to determine. For the Mazurkas,\nchords are often played on-beat by the left hand. However,\nfor notes of lower pitch, onset detection is problematic, es-\npecially when played softly. As a consequence, beats that\nonly coincide with a bass note or chord, but without any\nnote being played in the main melody, are a frequent source\n652\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)1 10 19 28 37 46 55 64 73 82 91 100 109 118 127 136 145 154 163 172 181 190 199 208 217 226 235 244 253 262 271 280 289 298 307 316 325 334 343 352 361 370 379 388 3960204060\n1 10 19 28 37 46 55 64 73 82 91 100 109 118 127 136 145 154 163 172 181 190 199 208 217 226 235 244 253 262 271 280 289 298 307 316 325 334 343 352 3600204060\n1 10 19 28 37 46 55 64 73 82 91 100 109 118 127 136 145 154 163 172 181 1930102030\n1 10 19 28 37 46 55 64 73 82 91 100 109 118 127 136 145 154 163 172 181 190 199 208 217 229020406080\n1 10 19 28 37 46 55 64 73 82 91 100 109 118 127 136 145 154 163 172 18101020304050M17-4\nM24-2\nM30-2\nM63-3\nM68-3\nb\nFigur\ne 4: The beat error histogram for the synchronization based beat tracking (SYNC) shows for how many performances of each of\nthe ﬁve Mazurkas a beat bis not identiﬁed. The different colors of the bars encode the beat class Ba beat is assigned to, see Sect. 3.\n1 10 19 28 37 46 55 64 73 82 91 100 109 118 127 136 145 154 163 172 181 190 199 208 217 226 235 244 253 262 271 280 289 298 307 316 325 334 343 352 3600204060\n1 10 19 28 37 46 55 64 73 82 91 100 109 118 127 136 145 154 163 172 18101020304050M24-2\nM68-3\nb\nFigur\ne 5: The beat error histogram for the PLP tracker shows for how many performances of M24-2 andM68-3 a beat bis not identiﬁed.\nThe different colors of the bars encode the beat class Ba beat is assigned to, see Sect. 3.\nfor errors. This is reﬂected by the cyan bars (B 3) frequently\noccurring in M17-4 (Fig. 4). Finally, B5(green) contains\nbeats falling on consecutive repetitions of the same chord.\nThis constitutes a challenge for the onset detection, espe-\ncially when played softly. Both M24-2 andM68-3 exhibit\na region of green bars that are incorrectly tracked by the\nSYNC (Fig. 4) and PLP(Fig. 5) trackers.\nAs mentioned in Sect. 4, PLP can not handle tempo\nchanges well. As a consequence, many of the beat errors\nforPLPthat are not assigned to any beat class (e.g., M24-2\nin Fig. 5, b= [260 : 264] ) are caused by sudden tempo\nchanges appearing in many of the performances. How-\never, these are considered a performance-dependent prop-\nerty, rather than a piece-dependent musical property and\nare not classiﬁed in a beat class.\nTable 4 summarizes the effect of each beat class on\nthe piece-level results. Here, the mean beat accuracy is\nreported for each of the ﬁve Mazurkas, when excluding\nthe beats of a certain class. For example, M30-2 contains\nmany beats of B3. Excluding these ornamented beats from\nthe evaluation, the overall beat accuracy increases from\nA= 0.900toA= 0.931forSYNC (Table 4 (left)) andfrom0.788to0.814forPLP(Table 4 (right)). The chal-\nlenge of M68-3 however, are non-event beats (B 1). Leav-\ning out these beats, the accuracy increases from 0.875to\n0.910forSYNC and from 0.674to0.705forPLP.\nAside from musical properties of a piece causing beat\nerrors, physical properties of certain performances make\nbeat tracking difﬁcult. In the following, we exemplarily\ncompare the beat tracking results of the performances of\nM63-3. Fig. 6 shows the beat accuracy Afor all 88per-\nformances available for this piece. In case of the SYNC\ntracker, the beat accuracy for most of the performances is\nin the range of 0.8−0.9, with only few exceptions that de-\nviate signiﬁcantly (Fig. 6(a)). In particular, Michalowski’s\n1933 performance with index 39(pid9083-16, see [1])\nshows a low accuracy of only A= 0.589due to a poor\ncondition of the original recording which contains a low\nsignal-to-noise ratio and many clicks. The low accuracy\n(A= 0.716) of performance 1(Csalog 1996, pid1263b-\n12) is caused by a high amount of reverberation, which\nmakes a precise determination of the beat positions hard.\nThe poor result of performance 81(Zak 1951, pid918713-\n20) is caused by a detuning of the piano. Compensating\n653\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)ID B B \\B1B\\B2B\\B3B\\B4B\\B5B\\B ∗\nM17-4 0.837 0.852 0.842 0.843 0.854 0.837 0.898\nM24-2 0.931\n0.940 0.936 0.941 0.933 0.939 0.968\nM30-2 0.900 0.900 0.903 0.931 0.905 0.900 0.959\nM63-3 0.890 0.890 0.898 0.895 0.895 0.890 0.911\nM68-3 0.875 0.910 0.889 0.875 0.875 0.887 0.948\nMean: 0.890 0.898 0.894 0.897 0.894 0.892 0.925ID B B \\B1B\\B2B\\B3B\\B4B\\B5B\\B ∗\nM17-4 0.639 0.650 0.641 0.671 0.593 0.639 0.649\nM24-2 0.854\n0.857 0.862 0.857 0.856 0.854 0.873\nM30-2 0.788 0.788 0.794 0.814 0.772 0.788 0.822\nM63-3 0.631 0.631 0.638 0.639 0.647 0.631 0.668\nM68-3 0.674 0.705 0.689 0.674 0.678 0.674 0.733\nMean: 0.729 0.735 0.734 0.739 0.723 0.729 0.751\nTable 4: Beat accuracy Aresults comparing the different beat classes for SYNC (left) andPLP (right): For all beats B, excluding\nnon-event beats B1, boundary beats B2, ornamented beats B3, weak bass beats B4, constant harmony beats B5, and the union B∗.\n1 10 20 30 40 50 60 70 80 8800.20.40.60.811 10 20 30 40 50 60 70 80 8800.20.40.60.811 10 20 30 40 50 60 70 80 8800.20.40.60.81(a)\n(b)\n(c)\nperformance\nFigur\ne 6: Beat accuracy Afor the beat tracker SYNC (a),ONSET\n(b), and PLP (c)of all88performances of M63-3.\nfor this tuning effect, the synchronization results and thus,\nthe beat accuracy improves from A= 0.767toA= 0.906.\nAs it turns out, ONSET tends to be even more sensitive to\nbad recording conditions. Again, performance 39shows\nan extremely low accuracy (A = 0.087), however, there\nare more recordings with a very low accuracy (70, 71,79,\n80,57, and 58). Further inspection shows that all of these\nrecordings contain noise, especially clicks and crackling,\nwhich proves devastating for onset detectors and leads to\na high number of false positives. Although onset detec-\ntion is problematic for low quality recordings, the PLPap-\nproach shows a different behavior. Here, the periodicity\nenhancement of the novelty curve [11] provides a cleaning\neffect and is able to eliminate many spurious peaks caused\nby recording artifacts and leads to a higher beat accuracy.\nHowever, other performances suffer from a low accuracy\n(performances 29,30, and 77). As it turns out, these ex-\namples exhibit extreme local tempo changes that can not\nbe captured well by the PLPapproach, which relies on a\nconstant tempo within the analysis window. On the other\nhand, some performances show a noticeably higher accu-\nracy (2, 5,11,31,74, and 87). All oft these recordings are\nplayed in a rather constant tempo.\n7. FUTURE DIRECTIONS\nOur experiments indicate that our approach of considering\nmultiple performances simultaneously for a given piece of\nmusic for the beat tracking task yields a better understand-\ning not only of the algorithms’ behavior but also of the un-\nderlying music material. The understanding and consider-\nation of the physical and musical properties that make beat\ntracking difﬁcult is of essential importance for improving\nthe performance of beat tracking approaches. Exploiting\nthe knowledge of the musical properties leading to beat er-rors one can design suited audio features. For example, in\nthe case of the Mazurkas, a separation of bass and melody\nline can enhance the quality of the novelty curve and alle-\nviate the negative effect of the ornamented beats or weak\nbass beats.\nAcknowledgment. The ﬁrst two authors are supported by\nthe Cluster of Excellence on Multimodal Computing and\nInteraction at Saarland University. The raw evaluation was\ngenerated by the third author at AHRC Centre for the His-\ntory and Analysis of Recorded Music (CHARM), Royal\nHollway, University of London.\n8. REFERENCES\n[1] The Mazurka Project. http://www.mazurka.org.uk, 2010.\n[2] MIREX 2009. Audio beat tracking results. http://www.\nmusic-ir.org/mirex/2009/index.php/Audio_Beat_\nTracking_Results, 2009.\n[3] J. P. Bello, L. Daudet, S. Abdallah, C. Duxbury, M. E. P. Davies, and\nM. B. Sandler. A tutorial on onset detection in music signals. IEEE\nTrans. on Speech and Audio Processing, 13(5):1035–1047, 2005.\n[4] M. E. P. Davies, N. Degara, and M. D. Plumbley. Evaluation methods\nfor musical audio beat tracking algorithms. Technical Report C4DM-\nTR-09-06, Queen Mary University, Centre for Digital Music, 2009.\n[5] M. E. P. Davies and M. D. Plumbley. Context-dependent beat track-\ning of musical audio. IEEE Trans. on Audio, Speech and Language\nProcessing, 15(3):1009–1020, 2007.\n[6] S. Dixon. An empirical comparison of tempo trackers. In Proc. of\nBrazilian Symposium on Computer Music, pages 832–840, 2001.\n[7] S. Dixon and W. Goebl. Pinpointing the beat: Tapping to expressive\nperformances. In Proc. of International Conference on Music Percep-\ntion and Cognition, pages 617–620, Sydney, Australia, 2002.\n[8] A. Earis. An algorithm to extract expressive timing and dynamics\nfrom piano recordings. Musicae Scientiae, 11(2), 2007.\n[9] S. Ewert, M. M ¨uller, and P. Grosche. High resolution audio syn-\nchronization using chroma onset features. In Proc. of IEEE ICASSP,\nTaipei, Taiwan, 2009.\n[10] F. Gouyon, A. Klapuri, S. Dixon, M. Alonso, G. Tzanetakis, C. Uhle,\nand P. Cano. An experimental comparison of audio tempo induction\nalgorithms. IEEE Trans. on Speech and Audio Processing, 14, 2006.\n[11] P. Grosche and M. M ¨uller. A mid-level representation for capturing\ndominant tempo and pulse information in music recordings. In Proc.\nof ISMIR, pages 189–194, Kobe, Japan, 2009.\n[12] A. P. Klapuri, A. J. Eronen, and J. Astola. Analysis of the meter of\nacoustic musical signals. IEEE Trans. on Audio, Speech and Lan-\nguage Processing, 14(1):342–355, 2006.\n[13] M. F. McKinney, D. Moelants, M. E. P. Davies, and A. Klapuri. Eval-\nuation of audio beat tracking and music tempo extraction algorithms.\nJournal of New Music Research, 36(1):1–16, 2007.\n[14] G. Peeters. Template-based estimation of time-varying tempo.\nEURASIP Journal on Advances in Signal Processing, 2007.\n[15] C. S. Sapp. Hybrid numeric/rank similarity metrics. In Proc. of IS-\nMIR, pages 501–506, Philadelphia, USA, 2008.\n654\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "Learning Features from Music Audio with Deep Belief Networks.",
        "author": [
            "Philippe Hamel",
            "Douglas Eck"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1414970",
        "url": "https://doi.org/10.5281/zenodo.1414970",
        "ee": "https://zenodo.org/records/1414970/files/HamelE10.pdf",
        "abstract": "Feature extraction is a crucial part of many MIR tasks. In this work, we present a system that can automatically ex- tract relevant features from audio for a given task. The fea- ture extraction system consists of a Deep Belief Network (DBN) on Discrete Fourier Transforms (DFTs) of the au- dio. We then use the activations of the trained network as inputs for a non-linear Support Vector Machine (SVM) classifier. In particular, we learned the features to solve the task of genre recognition. The learned features per- form significantly better than MFCCs. Moreover, we ob- tain a classification accuracy of 84.3% on the Tzanetakis dataset, which compares favorably against state-of-the-art genre classifiers using frame-based features. We also ap- plied these same features to the task of auto-tagging. The autotaggers trained with our features performed better than those that were trained with timbral and temporal features.",
        "zenodo_id": 1414970,
        "dblp_key": "conf/ismir/HamelE10",
        "keywords": [
            "Deep Belief Network",
            "Discrete Fourier Transforms",
            "genre recognition",
            "MFCCs",
            "non-linear Support Vector Machine",
            "Tzanetakis dataset",
            "timbral and temporal features",
            "audio",
            "feature extraction",
            "task"
        ],
        "content": "LEARNING FEATURES FROM MUSIC AUDIO WITH DEEP BELIEF\nNETWORKS\nPhilippe Hamel and Douglas Eck\nDIRO, Universit ´e de Montr ´eal\nCIRMMT\nfhamelphi,eckdougg@iro.umontreal.ca\nABSTRACT\nFeature extraction is a crucial part of many MIR tasks. In\nthis work, we present a system that can automatically ex-\ntract relevant features from audio for a given task. The fea-\nture extraction system consists of a Deep Belief Network\n(DBN) on Discrete Fourier Transforms (DFTs) of the au-\ndio. We then use the activations of the trained network\nas inputs for a non-linear Support Vector Machine (SVM)\nclassiﬁer. In particular, we learned the features to solve\nthe task of genre recognition. The learned features per-\nform signiﬁcantly better than MFCCs. Moreover, we ob-\ntain a classiﬁcation accuracy of 84.3% on the Tzanetakis\ndataset, which compares favorably against state-of-the-art\ngenre classiﬁers using frame-based features. We also ap-\nplied these same features to the task of auto-tagging. The\nautotaggers trained with our features performed better than\nthose that were trained with timbral and temporal features.\n1. INTRODUCTION\nMany music information retrieval (MIR) tasks depend on\nthe extraction of low-level acoustic features. These fea-\ntures are usually constructed using task-dependent signal\nprocessing techniques. There exist many potentially-useful\nfeatures for working with music: spectral, timbral, tempo-\nral, harmonic, etc (see [21] and [3] for good reviews), and\nit is not always obvious which features will be relevant\nfor a given MIR task. It would be useful to have a sys-\ntem that can automatically extract relevant features from\nthe audio, without having to depend on ad-hoc domain-\ndependent signal processing strategies.\nAmong the most widely used frame-level features for\naudio-related MIR tasks Mel-Frequency Cepstral Coefﬁ-\ncients (MFCCs). MFCCs take advantage of source/ﬁlter\ndeconvolution from the cepstral transform and perceptually-\nrealistic compression of spectra from the Mel pitch scale.\nBecause the ﬁrst few MFCC values capture pitch-invariant\ntimbral characteristics of the audio, they are commonly\nused in tasks where it is useful to generalize across pitch,\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2010 International Society for Music Information Retrieval.such as multi-speaker speech recognition and musical tim-\nbre recognition.\nPractically all audio-based music genre classiﬁcation\nmodels use different types of acoustic features to drive su-\npervised machine learning [4, 13, 14, 23]. These include\nsparse audio encodings in the time domain [17] and in the\nfrequency (spectral) domain [8]. Other approaches use a\nHidden Markov Model (HMM) to build a semantic rep-\nresentation of music [7, 22]. The best reported accuracy\non the Tzanetakis dataset [23] for genre classiﬁcation was\nachieved by a system that used auditory cortical represen-\ntations of music recordings and sparse representation-based\nclassiﬁers [20]. The challenges and motivations of genre\nclassiﬁcation are discussed in [18]. In these approaches it\nis difﬁcult to know whether the acoustic features or the ma-\nchine learning techniques are responsible for success. To\naddress this we apply our model to the Tzanetakis dataset.\nA closely related task to genre classiﬁcation is that of\n“autotagging” (automatic tag-based annotation of music\naudio). As for genre classiﬁcation, timbral and temporal\nfeatures are often used to solve this task [5]. To test the\nrobustness of our learned features, we applied them to the\ntask of autotagging on the Majorminer dataset [16].\nSome work in automatic feature extraction for genre\nclassiﬁcation have been done. In [19], automatic feature\nselection was done with genetic algorithms, and used for\none-on-one genre classiﬁcation. In our approach, we use a\nDeep Belief Network (DBN) [10] to learn a feature repre-\nsentation. DBNs have already been applied in some MIR\ntasks. In [9], a DBN is compared to other classiﬁers for the\ninstrument recognition task. In [12], convolutional DBNs\nare used to learn features for speech recognition and for\ngenre and artist classiﬁcation.\nCan we learn features for a given task directly from mu-\nsical audio that would better represent the audio than engi-\nneered signal-processing features? In this work, we inves-\ntigate this question.\nWe propose a method to automatically extract a rele-\nvant set of features from musical audio. We will show that\nthese learned features compare favorably against MFCCs\nand other features extracted by signal-processing.\nThe paper is divided as follows. In Section 2, we de-\nscribe the datasets that were used in our experiments. We\nthen explain brieﬂy the DBN model in Section 3. In Section 4\nwe describe the feature learning process. Then, in Section 5\nwe give the results of our features used in genre classiﬁca-\n339\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)tion and autotagging tasks. Finally, we conclude and pro-\npose future work in Section 6.\n2. DATASETS\nWe used two different datasets in our experiments. The\nﬁrst one is the Tzanetakis’ dataset for genre recognition.\nWe trained our feature extractor over this dataset. To test\nthe robustness of our learned features, we then applied\nthese same features to the task of autotagging on the Ma-\njorminer dataset.\n2.1 Tzanetakis\nThis dataset consists of 1000 30-second audio clips as-\nsigned to one of 10 musical genres. The dataset is bal-\nanced to have 100 clips for each genre. The dataset was\nintroduced in [24], and have since been used as a reference\nfor the genre recognition task.\n2.2 Majorminer\nThis dataset for autotagging was introduced in [16]. The\ntags were collected by using a web-based “game with a\npurpose”. Over 300 tags have been assigned to more than\n2500 10 second audio clips. For our experiment, we used\nonly the 25 most popular tags and compared our results to\nthose obtained in [16].\n3. DEEP BELIEF NETWORKS\nIn the last few years, a large amount of research has been\nconducted around deep learning [1]. The goal of deep\nlearning is to learn more abstract representations of the in-\nput data in a layer-wise fashion using unsupervised learn-\ning. These learned representations can be used as input for\nsupervised learning in tasks such as classiﬁcation and re-\ngression. Standard neural networks were intended to learn\nsuch deep representations. However, deep neural networks\n(i.e. networks having many hidden layers) are difﬁcult or\nimpossible to train using gradient descent [2]. The DBN\ncircumvents this problem by performing a greedy layer-\nwise unsupervised pre-training phase. It has been shown\n[2, 10] that this unsupervised pre-training builds a repre-\nsentation from which it is possible to do successful super-\nvised learning by “ﬁne-tuning” the resulting weights us-\ning gradient descent learning. In other words, the unsuper-\nvised stage sets the weights of the network to be closer to\na good solution than random initialization, thus avoiding\nlocal minima when using supervised gradient descent.\nThe Deep Belief Network (DBN) is a neural network\nconstructed from many layers of Restricted Boltzmann Ma-\nchines (RBMs) [2,10]. A schematic representation is shown\nin Figure 1. A RBM is structured as two layers of neurons:\na visible layer and a hidden layer. Each neuron is fully\nconnected to the neurons of the other layer, but there is no\nconnection between neurons of the same layer. The role\nof a RBM is to model the distribution of its input. We\ncan stack many RBMs on top of each other by linking the\nhidden layer of one RBM to the visible layer of the next\nInput LayerOutput Layer\nHidden Layer 3\nHidden Layer 2 \nHidden Layer 1}}\n}RBM 1RBM 2RBM 3Figure 1. Schematic representation of a DBN. The num-\nber of layer and the number of units on each layer in the\nschema are only examples. We do not require to have the\nsame number of units on each hidden layer.\nRBM. In our experiments, we used an algorithm inspired\nby Gibbs sampling called Contrastive Divergence (CD) to\noptimize our RBMs. Our focus here is on analyzing the\nperformance of the DBN, not in explaining the technical\ndetails of DBNs. The main idea for our purposes is that\nthat DBNs offer an unsupervised way to learn multi-layer\nprobabilistic representations of data that are progressively\n“deeper” (nonlinear) with each successive layer. For tech-\nnical and mathematical details see [2, 10]. We used the\nTheano1python library to build and train our DBNs.\n4. LEARNING THE FEATURES\nOur goal is is to learn a representation of audio that will\nhelp us to solve the subsequent tasks of genre classiﬁcation\nand autotagging.\n4.1 Training the DBN\nTo learn our representation, we split the Tzanetakis’ dataset\nin the following way: 50% for training, 20% for valida-\ntion and 30% for testing. We divided the audio into short\nframes of 46.44ms (1024 samples at 22050 Hz sampling\nrate). For each of these frames, we calculated the discrete\nFourier transform (DFT). We kept only the absolute values\nof the DFTs, and considering the symmetry in the DFT, we\nended up with inputs of dimension 513.\nThe DBNs were ﬁrst pre-trained with the training set in\na unsupervised manner. We then proceeded to the super-\nvised ﬁne-tuning using the same training set, and using the\nvalidation set to do early-stopping. The supervised step\nused gradient descent to learn a weighted mixture of ac-\ntivations in the deepest layer to predict one of 10 genre.\nBoth soft max and cross-entropy costs were minimized\nwith comparable results.\nWe tried approximately 200 different hyper-parameters\ncombinations and chose the model with the best validation\nerror on the frame level. The chosen DBN model is de-\nscribed in Table 1.\n1http://deeplearning.net/software/theano/\n340\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Number of hidden layers 3\nUnits per layer 50\nUnsupervised learning rate 0.001\nSupervised learning rate 0.1\nNumber of unsupervised epochs 5\nNumber of supervised epochs 474\nTotal training time (hours) 104\nClassiﬁcation accuracy 0.737\nTable 1. Hyper-parameters and training statistics of the\nchosen DBN\nThe classiﬁer trained from the last layer of the DBN\nyields a prediction of the genre for each frame. We aver-\nage over all predictions for a song and choose the highest\nscore as the wining prediction. This gave us a prediction\naccuracy of 73.7%.\nOnce trained, we can use the activations of the DBN\nhidden units as a learned representation of the input audio.\nWe analyzed the performance of each layer of the network\nindependently, and also all the layers together. To illustrate\nwhat is learned by the DBN, in Figure 2 we have plotted\na 2-dimensional projection of some of the representations\nused. The projection was done by using the t-SNE algo-\nrithm described in [25]. Notice how the clustering of the\nactivations of the hidden layers is more deﬁnite than for the\ninput or the MFCCs. As we will see in Section 5, this will\nimprove the accuracy of the classiﬁers.\n5. CLASSIFICATION USING OUR LEARNED\nFEATURES\nIn this section, we use our learned features as inputs for\ngenre classiﬁcation and autotagging. In the ﬁrst task we\nexplore different ways of using our features to get the best\nclassiﬁcation accuracy. In the second task, we use the\nmethod that gave us the best result in the genre recogni-\ntion in order to do autotagging.\nFor both experiments, we use a non-linear Support Vec-\ntor Machine (SVM) with a radial basis function kernel [6]\nas the classiﬁer. It would also be possible to train our DBN\ndirectly to do classiﬁcation. However our goal is to com-\npare the DBN learned representation with other represen-\ntations. By using a single classiﬁer we are able to carry out\ndirect comparisons.\n5.1 Genre classiﬁcation\n5.1.1 Frame-level features\nIn our ﬁrst experiment, we used our frame-level features\nas direct input to the SVM. Since the SVM doesn’t scale\nwell with large datasets, we subsampled the training set by\nrandomly picking 10;000frames. We compared these ac-\ncuracies to the accuracy of the SVM trained with MFCCs\nover these same frames of audio. As in Section 4.1, we\nused the frame predictions of a whole song and voted for\nthe best genre in order to compute the test accuracy. The\nresults for this experiments are shown in Table 2. We see\nInputs (DFTs)\nDBN Activations\nMFCCs\nblues\nclassical\ncountry\ndisco\nhiphopjazz\nmetal\npop\nreggae\nrockFigure 2. 2-Dimensional projections of different represen-\ntations of the audio with respect to their genre.\n341\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Accuracy\nMFCCs 0.630\nLayer 1 0.735\nLayer 2 0.770\nLayer 3 0.735\nAll Layers 0.770\nTable 2. Classiﬁcation accuracy for frame-level features\nthat, at the frame level, our learned features performed sig-\nniﬁcantly better than the MFCCs alone. We also see that\nthe second layer seems to have the best representation out\nof the three layers. By using all the layers as the input, we\ndon’t see any improvement compared to the second layer\nalone. Since we used the same dataset here that we used\nfor learning the features, we took care to reuse that same\ntraining, validation and testing splits as in Section 4, so as\nnot to contaminate our testing set. Because our learned\nDBN representation was learned on a single test/train split,\nwe were unable to do cross-validation on this dataset with\nthe SVM classiﬁer, since this would have given us a biased\nresult.\n5.1.2 Aggregated features\nBergstra et al [4] investigated the impact of feature ag-\ngregation on classiﬁcation performance for genre recog-\nnition. It is demonstrated that aggregating frame-level fea-\ntures over a period of time increases classiﬁcation accu-\nracy. The optimal aggregation time depend depends on the\nnature of the features and the classiﬁer, with many popu-\nlar features having optimal aggregation times of between\n3 and 5 seconds. With this in mind, we aggregated our\nfeatures over 5 seconds periods. Thus, for each 5 seconds\nsegment of audio (with 2.5 seconds overlap), we computed\nthe mean and the variance of the feature vectors over time.\nThis method not only raised our classiﬁcation accuracy, but\nalso reduced the number of training examples, thus accel-\nerating the training of the SVMs. With the aggregation, our\nclassiﬁcation accuracy by jumped to 84.3%, which is bet-\nter than the 83% accuracy reported in [4]. However, since\nthis result was reported on a 5-fold cross-validation on the\ndataset, we cannot directly compare our results. More im-\nportantly we observe that our results are in general com-\npetitive with the state-of-the-art signal-processing feature\nextraction for the genre classiﬁcation task. Also, given a\nﬁxed classiﬁer (the nonlinear SVM) our learned represen-\ntation outperforms MFCCs. As in Section 5.1.1, we see\nthat the second layer gives the best representation of all the\nlayers, but we gain a bit of accuracy by using all of the\nlayers.\n5.2 autotagging\nTo test the robustness of our learned features, we tested\ntheir performance on an autotagging task. Following the\nresults in Section 5.1, we used the activations of all the\nlayers of the DBN aggregated on 5 second windows as in-\nputs for the SVMs. We will refer to this set of feature asAccuracy\nMFCCs 0.790\nLayer 1 0.800\nLayer 2 0.837\nLayer 3 0.830\nAll Layers 0.843\nTable 3. Classiﬁcation accuracy for features aggregated\nover 5 seconds\n0.4 0.5 0.6 0.7 0.8 0.9keyboardvoicebassvocaldrumsynthinstrumentalslow80sjazzfastelectronicafemalepopsaxophonemalepianoelectronicdanceguitarbeattechnohip hoprockrapDBN\nMIM\nFigure 3. Accuracy of the DBN and the MIM feature sets\nfor the 25 most popular tags. As each tag training set was\nbalanced for positive and negative examples, the vertical\nline at 0.5 indicates chance accuracy.\nthe DBN feature set. We compare it to a set of timbral and\ntemporal features presented in [15]. We will refer to this\nset of feature as the MIM feature set. We used the same\nmethod as in [16] to train the SVMs over the dataset. The\nresults for the 25 most popular tags in the dataset are shown\nin Figure 3 and summarized in Table 4.\nMean Accuracy Standard Error\nDBN 0.73 0.02\nMIM 0.70 0.02\nTable 4. Mean and standard error of the autotagging re-\nsults.\nThe results show that our features give a better classi-\nﬁcation performance for almost all the tags. In particular,\nour features performed signiﬁcantly better better for tags\nsuch as ’rock’, ’guitar’, ’pop’ and ’80s’. Except for ’gui-\ntar’, these particular tags represent genres, which is what\nour features were optimized to classify.\n5.3 Discussion\nFrom the results presented in Section 5.1 and Section 5.2,\nwe see that it is indeed possible to learn features from audio\nrelevant to a particular task. In the case of genre classiﬁ-\ncation, our DBN features performed as well if not better\nthan most signal-processing feature extraction approaches.\nThe features were optimized to discriminate between the\n342\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)10 genres shown in Figure 2, but we showed that these fea-\ntures were also relevant to describe many other tags, such\nas ’guitar’, that were not related to genre. We believe this\nis evidence that a DBN can in fact learn to extract impor-\ntant and robust characteristics from audio. Another posi-\ntive point is that, once the DBN is trained, the feature ex-\ntraction from audio is very fast and can be done easily in\nreal-time, which could be useful for many applications.\nHowever, there are several areas for improvement. The\nmain one is the long computation time necessary to train\nthe DBN. The model that we used required a few days to\ntrain. This is mainly due to the size of the dataset. Since\nwe used uncompressed audio frames overlapping over half\na frame, the combination of the training and validation set\nrequired around 2 gigabytes of memory. There are many\nways to reduce the size of the training set and to speed up\nthe training. We could compress the DFTs with Princi-\npal Component Analysis (PCA). We could also aggregate\nthe DFTs over small windows before sending them to the\nDBN. Randomly choosing a subset of the frames in the\ndataset could also help. Another solution would be to aug-\nment the mini-batch size to optimize the time of training\nprocess. However, it is not clear how each of these so-\nlutions will affect the quality of the representation. This\nrequires further investigation.\nReducing the training time of a single model would also\nhelp to solve the second issue, which is the hyper-parameter\nsearch. As mentioned in Section 4.1, there are many hyper-\nparameters to optimize. It is not clear how the optimal\nhyper-parameters vary depending on the input and the task.\nCurrent research on deep learning is investigating the mat-\nter, and some techniques to automatically adjust the hyper-\nparameters are being developed.\nAnother ﬂaw of our model is that the features are ex-\ntracted at the frame level only, so that our model cannot\nmodel long-term time dependencies. To better represent\nmusical audio, we would need features that are able to\ncapture the long-term time structure. Convolutional DBNs\nmight provide a suitable model for time hierarchical repre-\nsentations [11].\n6. CONCLUSION AND FUTURE WORK\nIn this paper, we have investigated the ability for DBNs to\nlearn higher level features from audio spectra. We showed\nthat these learned features can outperform MFCCs and carefully-\ntailored feature sets for autotagging. These results moti-\nvate further research with deep learning applied to MIR\ntasks.\nIn future work, we will continue investigating ways to\nreduce the training time of our models. Furthermore, we\nwill learn features over a wider range of datasets and MIR\ntasks. We are interested, for example, in using the unsuper-\nvised DBN training approach to observe a large amount\nof unlabeled audio data. Finally, we will continue to in-\nvestigate how we can take advantage of structure found at\nmultiple timescales in music. To this end, a hierarchical\nconvolutional DBN may be appropriate.7. ACKNOWLEDGEMENTS\nSpecial thanks to Guillaume Desjardins and Michael Man-\ndel for their contribution to the code used in the exper-\niments. Thanks also to Simon Lemieux, Pierre-Antoine\nManzagol and James Bergstra for helpful discussions, and\nto the Theano development team for building such useful\ntools. This research is supported by grants from the Que-\nbec Fund for Research in Nature and Technology (FQRNT)\nas and the Natural Sciences and Engineering Research Coun-\ncil of Canada (NSERC).\n8. REFERENCES\n[1] Y . Bengio. Learning Deep Architectures for AI. Foun-\ndations and Trends in Machine Learning, 2:1–127,\n2009.\n[2] Y . Bengio, P. Lamblin, D. Popovici, and H. Larochelle.\nGreedy layer-wise training of deep networks. In Ad-\nvances in Neural Information Processing Systems 19,\npages 153–160. MIT Press, 2007.\n[3] J. Bergstra. Algorithms for classifying recorded mu-\nsic by genre. Master’s thesis, Universit ´e de Montr ´eal,\n2006.\n[4] J. Bergstra, N. Casagrande, D. Erhan, D. Eck, and\nB. K ´egl. Aggregate features and AdaBoost for mu-\nsic classiﬁcation. Machine Learning, 65(2-3):473–484,\n2006.\n[5] T. Bertin-Mahieux, D. Eck, and M. Mandel. Automatic\ntagging of audio: The state-of-the-art. In Machine Au-\ndition: Principles, Algorithms and Systems. IGI Pub-\nlishing, 2010.\n[6] C.-C. Chang and C.-J. Lin. LIBSVM: a library for sup-\nport vector machines, 2001.\n[7] K. Chen, S. Gao, Y . Zhu, and Q. Sun. Music genres\nclassiﬁcation using text categorization method. IEEE,\n2006.\n[8] R. Grosse, R. Raina, H. Kwong, and A. Y . Ng. Shift-\ninvariant sparse coding for audio classiﬁcation. In Pro-\nceedings of the Conference on Uncertainty in AI, 2007.\n[9] P. Hamel, S. Wood, and D. Eck. Automatic identiﬁ-\ncation of instrument classes in polyphonic and poly-\ninstrument audio. In in Proceedings of the 10th Inter-\nnational Conference on Music Information Retrieval\n(ISMIR’09) , Kobe, Japan, 2009.\n[10] G. E. Hinton, S. Osindero, and Y . Teh. A fast learn-\ning algorithm for deep belief nets. Neural Computa-\ntion, 18(7):1527–1554, 2006.\n[11] H. Lee, R. Grosse, R. Ranganath, and A. Y . Ng. Convo-\nlutional deep belief networks for scalable unsupervised\nlearning of hierarchical representations. In ICML ’09:\nProceedings of the 26th Annual International Con-\nference on Machine Learning, pages 609–616. ACM,\n2009.\n343\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)[12] H. Lee, Y . Largman, P. Pham, and A. Y . Ng. Unsu-\npervised feature learning for audio classiﬁcation using\nconvolutional deep belief networks. Advances in Neu-\nral Information Processing Systems (NIPS) 22., 2009.\n[13] T. Li and G. Tzanetakis. Factors in automatic musi-\ncal genre classiﬁcation. In IEEE Workshop on Appli-\ncations of Signal Processing to Audio and Acoustics,\n2003.\n[14] M. I. Mandel and D. P. W. Ellis. Song-level features\nand support vector machines for music classiﬁcation.\nInProceedings of the 6th International Conference on\nMusic Information Retrieval (ISMIR), pages 594–599,\n2005.\n[15] M. I. Mandel and D. P. W. Ellis. Multiple-instance\nlearning for music information retrieval. In Proceed-\nings of the 9th International Conference on Music In-\nformation Retrieval (ISMIR), pages 577–582, 2008.\n[16] M. I. Mandel and D. P. W. Ellis. A web-based game\nfor collecting music metadata. Journal of New Music\nResearch, 37(2):151–165, 2008.\n[17] P.-A. Manzagol, T. Bertin-Mahieux, and D. Eck. On\nthe use of sparse time relative auditory codes for music.\nIn9th International Conference on Music Information\nRetrieval (ISMIR 2008), 2008.\n[18] C. McKay and I. Fujinaga. Musical genre classiﬁca-\ntion: is it worth pursuing and how can it be. In Pro-\nceedings of the 7th International Conference on Music\nInformation Retrieval (ISMIR 2006), 2006.\n[19] I. Mierswa and K. Morik. Automatic feature extraction\nfor classifying audio data. Machine Learning Journal,\n58:127–149, 2005.\n[20] Y . Panagakis, C. Kotropoulos, and G.R. Arce. Mu-\nsic genre classiﬁcation using locality preserving non-\nnegative tensor factorization and sparse representa-\ntions. In Proceedings of the 10th International Confer-\nence on Music Information Retrieval (ISMIR), pages\n249–254, 2009.\n[21] G. Peeters. A large set of audio features for sound de-\nscription (similarity and classiﬁcation) in the cuidado\nproject. Technical report, IRCAM, 2004.\n[22] J. Reed and C.-H. Lee. A study on attribute-based tax-\nonomy for music information retrieval. In Proc. of Int.\nSymposium on Music Information Retrieval, 2007.\n[23] G. Tzanetakis and P. Cook. Musical genre classiﬁca-\ntion of audio signals. IEEE Trans. on Speech and Audio\nProcessing, 10(5):293–302, 2002.\n[24] G. Tzanetakis, G. Essl, and P. Cook. Automatic musi-\ncal genre classiﬁcation of audio signals. In Proceedings\nof the 2nd International Conference on Music Infor-\nmation Retrieval (ISMIR 2001), Bloomington, Indiana,\n2001.[25] L.J.P. van der Maaten and G.E. Hinton. Visualizing\nhigh-dimensional data using t-sne. Journal of Machine\nLearning Research, 9:2579–2605, 2008.\n344\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "Informed Source Separation of Orchestra and Soloist.",
        "author": [
            "Yushen Han",
            "Christopher Raphael"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1416750",
        "url": "https://doi.org/10.5281/zenodo.1416750",
        "ee": "https://zenodo.org/records/1416750/files/HanR10.pdf",
        "abstract": "A novel technique of unmasking to repair the degradation in sources separated by spectrogram masking is proposed. Our approach is based on explicit knowledge of the musi- cal audio at note level from a score-audio alignment, which we termed Informed Source Separation (ISS). Such knowl- edge allows the spectrogram energy to be decomposed into note-based models. We assume that a spectrogram mask for the solo is obtained and focus on the problem of repair- ing audio resulting from applying the mask. We evaluate the spectrogram as well as the harmonic structure of the music. We either search for unmasked (orchestra) partials of the orchestra to be transposed onto a masked (solo) re- gion or reshape a solo partial with phase and amplitude imputed from unmasked regions. We describe a Kalman smoothing technique to decouple the phase and amplitude of a musical partial that enables the modification to the spectrogram. Audio examples from a piano concerto are available for evaluation.",
        "zenodo_id": 1416750,
        "dblp_key": "conf/ismir/HanR10",
        "keywords": [
            "unmasking",
            "repair",
            "degradation",
            "sources",
            "separated",
            "spectrogram",
            "masking",
            "explicit knowledge",
            "musical audio",
            "score-audio alignment"
        ],
        "content": "INFORMED SOURCE SEPARATION OF ORCHESTRA AND SOLOIST\nYushen Han\nSchool of Informatics and Computing\nIndiana University Bloomington\nyushan@indiana.eduChristopher Raphael\nSchool of Informatics and Computing\nIndiana University Bloomington\ncraphael@indiana.edu\nABSTRACT\nA novel technique of unmasking to repair the degradation\nin sources separated by spectrogram masking is proposed.\nOur approach is based on explicit knowledge of the musi-\ncal audio at note level from a score-audio alignment, which\nwe termed Informed Source Separation (ISS). Such knowl-\nedge allows the spectrogram energy to be decomposed into\nnote-based models. We assume that a spectrogram mask\nfor the solo is obtained and focus on the problem of repair-\ning audio resulting from applying the mask. We evaluate\nthe spectrogram as well as the harmonic structure of the\nmusic. We either search for unmasked (orchestra) partials\nof the orchestra to be transposed onto a masked (solo) re-\ngion or reshape a solo partial with phase and amplitude\nimputed from unmasked regions. We describe a Kalman\nsmoothing technique to decouple the phase and amplitude\nof a musical partial that enables the modiﬁcation to the\nspectrogram. Audio examples from a piano concerto are\navailable for evaluation.\n1. INTRODUCTION\nWe address the “desoloing” problem, in which we attempt\nto isolate the accompanying instruments in a monaural record-\ning of music for soloist and orchestral accompaniment. The\nmotivation is to produce the audio of the accompaniment\npart for concertos in the “classical” domain as well as the\nkaraoke in popular music, whereas the ultimate goal is to\nhave the orchestra adapt timing to the live player, a prob-\nlem we do not discuss there. Nevertheless, the accompa-\nnying audio is needed and we offer solutions through our\ndemixing or isolation of the original sources (instruments).\nMost past effort in this “source separation” problem treats\nBlind Source Separation (BSS) problems and assumes lit-\ntle knowledge of the audio content rather than the indepen-\ndence of the sources [1] or relies on general cues of musical\nsources rather than the content of the sources [2]. In con-\ntrast, we assume explicit knowledge in the form of a score\nmatch, which establishes a correspondence between the\naudio data and a symbolic score representation giving the\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2010 International Society for Music Information Retrieval.onset times of all musical events. See Figure 1 for an ex-\nample. Such correspondence, known as “score following”\nor “alignment”, initially introduced and developed by Ver-\ncoe and Dannenberg [12] is the foundation of our approach\nwhich we termed Informed Source Separation (ISS). Other\nexamples in the category of ISS include Dubnov [6].\nFigure 1. Piano note onsets (vertical lines) of an excerpt\nfrom 2nd mvmt. of Ravel’s piano concerto in G\nIn our approach, we begin by masking the short time\nFourier transform (STFT) in an attempt to “erase” the soloist’s\ncontribution. We also based our exploration toward partial-\nwise phase/amplitude relationship on previous work [7]\nin which spectrogram magnitude is decomposed into each\npartial by ﬁtting note-based models. [11] is another ef-\nfort of spectrogram decomposition in speech. However,\nour emphasis here is not on estimating the mask or ﬁtting\nnote models, but on employing a novel set of procedures\n(see sect. 5) that estimates and transforms note partials,\nin which the damage caused by our masking procedure is\nrepaired.\nHere our assumption is that there is information redun-\ndancy in terms of phase and amplitude between the “ob-\nservable” partials (i.e. not signiﬁcantly overlapped by the\nsolo or an accompaniment instrument of a different family)\nand damaged partials. Our hope is to “copy and paste” mu-\nsical partials from the observable area to the damaged area\nwith some necessary transformations that exploit those re-\ndundancy to maintain the consistency between the observ-\nable and the damaged. These procedures can be automated\nby analyzing the texture of the music from the score and\ntesting the soundness of remaining partials on the desoloed\nspectrogram. We call this process unmasking in which the\nmasked-out solo regions will be recovered.\n315\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)The structure of this paper is as follows: we brieﬂy for-\nmulate the masking problem in sect. 2, followed by note-\nbased parameterization in sect. 3 and phase estimation in\nsect. 4. Such estimating enables our repair-by-unmasking\ntechnique in sect. 5 which is applied in the context of a\npiano concerto in sect. 6.\n2. SPECTROGRAM MASKING OF THE SOLO\nGiven our original audio signal, x(s), we deﬁne the short\ntime Fourier transform (STFT) by\nX(t;k) =N\u00001X\nn=0x(tH +n)w(n)e\u00002\u0019jkn=N\nwhereHis the hop size, Nis the window length and w\nis the window function. We will deﬁne our masking op-\neration in this STFT domain. To do so, we estimate two\n“complementary” masks, 1s(t;k), and 1a(t;k), taking val-\nues inf0;1gwith1s(t;k)+1a(t;k) = 1 . These masks are\nused to isolate the parts of Xwe attribute to the soloist and\naccompaniment through\nXs(t;k) = 1s(t;k)X(t;k) (1)\nXa(t;k) = 1a(t;k)X(t;k) (2)\nIn other words we label each time-frequency “cell” (t;k)\nas either solo or accompaniment. Since our focus here is\non the unmasking problem, we will bias our labeling of\neach time-frequency cell toward the solo category, since\nwe want to make sure the original soloist is completely re-\nmoved. Using our score match, it would be relatively easy\nto simply draw a rectangle around each solo partial while\ncalling the interior of these rectangles our solo mask. Our\napproach is somewhat more sophisticated, employing spe-\ncial treatment of the wide spectral dispersion associated\nwith note onsets by Ono et al. [13], as well as careful mod-\neling of the steady state partials. However, we will not\ndiscuss this mask estimation problem here.\nWhileXa(t;k)(andXs(t;k)) is, in general, not the\nSTFT of anytime signal, applying the inverse STFT opera-\ntion gives perceptually sufﬁcient results with appropriately\ndeﬁned STFT. In particular, if we use a Hann window with\nH=N=4, one can show that applying the STFT inverse\ntoXaresults in the audio signal whose STFT is closest to\nXain the sense of Euclidean distance [5].\nThe result of this process elimanates more than the soloist,\nof course, since the accompanying instruments also con-\ntributed to the STFT in the region we have masked out. A\npossible remedy in sect. 4 is the main focus of our paper.\n3. NOTE-BASED MUSIC PARAMETERIZATION\nIn this section we brieﬂy review our parameterization of\nthe music given the score, which is adapted from our tech-\nnique to decompose the spectrogram magnitude into note\nmodels in [7]. This parameterization is also used to facili-\ntate our phase estimation in sect. 4.\nFrom the score, suppose we have a collection of notes\nNin the piece of interest, for a note n2N , we know itsinstrumentation in2IwhereIis the set of instruments in\nthis piece and can be further partitioned into disjoint sub-\nsetsIsandIafor solo and accompaniment instruments\nseparately.\nMoreover, we know the time span of note n:Tn=\nfton\nn;:::;toff\nngfrom the score following. Also, as the\nnote pitchpnindicates its set of valid harmonics under a\ncertain Nyquist frequency: Hn=f1;:::;H ng, we con-\nﬁne the frequency bin span of each partial h2 Hnto\nKn;h=fklow\nn;h;:::;khigh\nn;hg.Kn;himplements a band-pass\nﬁlter to specify a frequency bin span where the contribu-\ntion from the partial of interest (very likely to be mixed\nwith other partials of close frequencies) is signiﬁcant in\nterms of spectrogram magnitude while the spectral energy\noutside ofKn;his ignored.\nSuch 2-dimensional, rectangular time-bin support Bn;h=\nf(t;k )jt2Tn;k2Kn;hgspeciﬁes a band-passed ﬁlter\nbank overTnto extract time domain partial ph(s)from\nX(t;k)We denoteBn=Bn;1[:::[Bn;Hnto be the\nsupport for all harmonic components of note n.\nWe then assume a Normal mixture model for the spec-\ntrogram magnitude of an orchestra note n: each harmonic\nof the note is one Gaussian component in the mixture with\nnormalized weight \u0017n;h, coupled frequency bin expecta-\ntion\u0016n;h(t) =h\u0016n;1(t), and unknown variance \u001b2\nn;h. To\naccommodate the (possibly dramatic) change in amplitude\nover time of a note, we also introduce a normalized non-\nnegative proﬁle, \u0011n;h(t), to outline the frame-wise ampli-\ntude ofhth partial of nth note.\nStrictly, the centroid of each partial may not be precisely\ncoupled by\u0016n;h(t) =h\u0016n;1(t). But it is approximately\ntrue for all the instruments except for piano in our study.\nTo summarize:\n\u000fa weight\u0017n;h>0for8(n;h) withP\nh2H n\u0017n;h= 1\n\u000fa time support Tn=fton\nn;:::;toff\nng, which is shared\namong all partials of note n\n\u000fan amplitude envelope \u0011n;h(t)>0for8(n;h) withP\nh2H n\u0011n;h(t) = 1\n\u000fa frequency bin support Kn;h=fklow\nn;h;:::;khigh\nn;hg\n\u000fa frequency bin centroid \u0016n;h(t)which reﬂected the\nfrequency of partial hatt. Among different partials,\nthey are coupled by \u0016n;1(t) =\u0016n;h(t)\nh\n\u000fa frequency bin variance \u001bn;hthat describes mag-\nnitude distribution of partial hover frequency bins\nwith expectation \u0016n;h(t)under Normal assumption.\nFinally we can deﬁne a“template” function qn;h(t;k)\n=8\n<\n:0;8(t;k ) :t =2Tnor k =2Kn;h\n\u0017n;h\u0011n;h(t)f(k;\u0016n;h;\u001b2\nn;h);otherwise(3)\nwheref(k;\u0016n;h;\u001b2\nn;h)is the normal density function. This\nparameterization is subjected to normalization to ensureP\nh2H nP\n(t;k)2Bn;hqn;h(t;k) = 1 for noten.\n316\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Figure 2. Wrapped and Unwrapped Phase\nOur assumption is that the magnitude contribution from\neach note partial indexed by (n;h) to the spectrogram is\nraised from a collection of independent Poisson random\nvariablesfZn(t;k)gfor(t;k)2Bn[3]. The expectation\nofZn(t;k)is\u000enP\nhqn;h(t;k)where\u000endescribes the de-\ngree to which Zn(t;k)contributes to X(t;k). Intuitively,\n\u000enis our estimate of the total spectrogram magnitude con-\ntribution from note nand can be interpreted as the overall\n“amplitude” of the note n. The estimation of \u000enis, no\ndoubt, a signiﬁcant factor of source separation quality and\nour solution by an EM algorithm is documented in [7]. For\nthe rest of the paper, we assume a somewhat reliable \u000en\nis known so we can focus on the unknown phase of each\npartial of note n.\n4. PARTIAL-WISE PHASE ESTIMATION AND\nTRANSFORMATIONS\nAs usually only a subset of partials of a note is damaged\nby removing the solo partial, we hope to exploited the har-\nmonicity assumption in wind and string instruments sup-\nported by Fletcher [9] and Brown [10] to impute the phase\nof those missing partials in the orchestra. To do so, we\nﬁrst introduce a generic method to decouple the phase and\nslow-changing amplitude of a band-limited signal in 4.1\nwhich enables our two major tools to “unmask” the dam-\naged spectrogram: harmonic transposition in 4.2 and phase-\nlocked modulation in 4.3.\n4.1 Phase Estimation by Kalman Smoothing\nIn this section we represent our note partial, ph(s), in terms\nof a time-varying amplitude and phase:\nph(s)\u0019\u000bh(s) cos(\u0012h(s))\nwhere the time-varying amplitude, \u000bh(s), is non-negative\nand varies slowly compared with ph(s), and the “unwrapped”\nphase (see Figure 2) function, \u0012h(s), is monotonically non-\ndecreasing. A more precise review of the slow-changing\n\u000bh(s)in a sinusoidal model is given by Rodet [14].\nIn order to estimate \u000bh(s)and\u0012h(s)we follow the model\nof Taylan Cemgil [8] and view the harmonic, ph(s), as the\nouput of a Kalman ﬁlter model [16] [17]. To this end we\ndeﬁne a sequence of two-dimensional state vectors fx(s) =\n(x1(s);x 2(s))tgwherex1(0)andx2(0)are independent0-mean random variables with variance \r2, and the re-\nmaining variables follow evolution equation x(s+ 1) =\nAx(s) +w(s)wherefw(s)g is an independent sequence\nof 0-mean 2-dimensional vectors with independent com-\nponents of ﬁxed variance (the variance can be tuned em-\npirically).Ais the rotation matrix, deﬁned in terms of the\nexpected phase advance per sample, \u001a, which is directly\ncomputable from the nominal frequency of the partial:\nA=\u0012cos\u001asin\u001a\n\u0000sin\u001acos\u001a\u0013\nThus,x(s) is a sequence of vectors that circle around the\norigin and an approximately known frequency with vari-\nable distance from the origin. We then model our observed\npartial asph(s) =x1(s) +v(s)wherefv(s)g is another\nsequence of independent 0-mean variables with a certain\nvariance (this variance is tuned empirically too).\nIt is well known that the Kalman ﬁlter allows straight-\nforward computation of the conditional distribution, p(x(s)jfp h(s0)g),\nand that this distribution is Normal for each value of s.\nThus we estimate x(s) by^x(s) =E(x(s)jfph(s0)g). The\nrepresentation of the partial in terms of amplitude and non-\ndecreasing phase follows from the polar coordinate repre-\nsentation of ^x(s):\n\u000bh(s) =q\n^x2\n1(s) + ^x2\n2(s)\n\u0012h(s) = 2\u0019k (s) + tan\u00001(^x2(s)\n^x1(s))\nwhere eachk(s)is chosen to be the non-negative minimal\ninteger value that ensures that \u0012h(s)is non-decreasing.\nNote that for phase sequence \u0012h(s);s2f1;:::;Sg, not\nonly the ﬁnal phase estimate ^\u0012h(S)but also all previous\nphase estimates are of interest. To get the “best” phase es-\ntimation, we need to update the state estimates backward\nto incorporate the observation that were not “available”\nat samplesin the forward pass. This motivates Kalman\nsmoothing (see chapter 5 of [17]) which calculates the smoothed\nphase estimate ^\u0012h(s)recursively backward from the last\nsample atS.\n4.2 Harmonic Transposition\nWith amplitude \u000bh(s)and phase\u0012h(s)decoupled from\nhth harmonic of a note, we are ready to “project” one har-\nmonic into a different harmonic while maintaining the har-\nmonicity between the source and the destination. Suppos-\ning we estimated the unwrapped phase of the ith harmonic\nas\u0012i(s), the “projected” phase sequence at jth harmonic is\ngiven by ~\u0012j(s) =j\u0012i(s)\niand the resulting jth harmonic by\n~pj(s) = ~\u000bj(s) cos(j\u0012i(s)\ni) (4)\nwhere ~\u000bj(s)is either known or imputed amplitude at jth\nharmonic. In this work, we usually have an estimate of\n~\u000bj(s)by scaling\u000enfrom sect. 3.\nOur harmonic transposition exploit such “harmonicity”\nbetween partials, which is a well-studied phenomenon. Early\n317\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Figure 3. Unwrapped Phase Difference\nwork mainly by Fletcher showed that frequencies of the\npartials in“the middle portion of the tone” of string instru-\nment are integral multiples of the fundamental frequency\nby using sonograph and also derived that partials of string\nand wind instrument are “rigorously locked into harmonic\nrelationship” [9]. By using single frame approximation on\na variety of digital samples, Brown concluded that “con-\ntinuously driven instruments such as the bowed strings,\nwinds, and voice have phase-locked frequency components\nwith frequencies in the ratio of integers to within the cur-\nrently achievable measurement accuracy of about 0.2%”\n[10] from experiments with and without vibrato.\nTo demonstrate such harmonicity in our framework, we\nfocus on the “projection” of the unwrapped phase \u0012i(s)\nfrom partial ito partialjby\n\u0012i;j(s) =j\u0012h1(s)\ni(5)\nBy “projecting” the phase of different partials to a com-\nmon harmonic, we can examine such phase relation on a\nvariety of orchestra instruments. We can visualize pair-\nwise phase difference \u0012i;1(s)\u0000\u0012j;1(s)at the fundamental\nfor anyi6=j. Figure 3 shows the pairwise phase differ-\nence for the ﬁrst 4 notes from a performance of the ﬁrst\nmovement of Stravinsky’s Three Pieces for Clarinet Solo.\nThe salient message from this plot is: the pairwise phase\ndifference is in a very small range (mostly (\u0000\u0019\n2;\u0019\n2)) and\nnever drifts away over the entire note; the error (including\nmeasurement error and true difference) is not accumula-\ntive. This supports our approximation of phase coherence.\nPiano and other impulsively driven instruments such as\nstrings played pizzicato are counter-examples whose par-\ntials deviate from integer ratios due to the stiffness of the\nstring [10].4.3 Phase-locked Modulation\nIn addition to the partial-wise relationship, we want to ex-\nploit timewise similarity in terms of phase and amplitude\nwithin one note.\nSuppose we have a partition T1=fs1;:::;sk\u00001g,\nT2=fsk;:::;s 2gon the sample indices T=fs1;:::;s 2g\nof the sustaining part of a reasonably long orchestra note,\nwe can only observe the unwrapped phase sequence at \u0012h(T1)\nbut\u0012h(T2)is missing. We can impute \u0012h(T2)sequentially\nby\n\u0012h(sk+n) =\u0012h(sk+n\u00001)+\u0012h(s1+1+n)\u0000\u0012h(s1+n)\n(6)\nfor any 0\u0014n\u0014s2\u0000sk. We omit the formula to obtain\n\u0012h(T1)if we observe \u0012h(T2).\nThis operation preserves the phase advance per sample\ninT1and applies such \u0001\u0012h(T1)cyclically to T2. This is\nsimilar to the phase vocoder except for that we are doing\nit on the sample level rather than frame level. For a long\nenough time span T1, we are capturing the pattern of fre-\nquency ﬂuctuation in \u0012h(T1). To synthesize a segment of\na partial, we also need the amplitude envelope over T2.\nA simple solution is to reuse the average amplitude \u000bh\noverT1(with some minor modulation) to “sustain” a note\nthrough the end of T2. If the orchestra note is holding for\nquite long, which is common in some orchestration, we are\neffectively synthesizing the sustaining part of the partial.\n5. SPECTROGRAM UNMASKING\nIn an attempt to ﬁx the damage caused by desolo, we ex-\namine the spectrogram with a focus on areas where the ac-\ncompaniment notes (harmonics) are damaged.\nIn the type of music that we (and many solo musicians)\nare mainly interested in, for instance, a piano concerto, it\nis common that a string section may double the solo in-\nstrument at the unison, ﬁfth, or octave in either direction.\nIn these cases, masking out the solo part usually results in\nmany damaged partials in the orchestra since consonant in-\ntervals mean more partials are likely to share the same fre-\nquencies. With this in mind, we use some heuristics to cre-\nate an algorithm to automatically perform the two partial-\nwise transformations developed in 4.2 and 4.3. Since the\ntexture of the music can be highly complex, we reconstruct\na somewhat “generic” scenario for illustration of this algo-\nrithm in Figure 4. The 1-bar score in the ﬁgure is a re-\nduction from a piano concerto where the piano part is fre-\nquently doubled by the lower string sections.\nSupposing we have obtained solo mask 1s(t;k), a dam-\naged region Bd\nn;h\u0012Bn;h, a template gn;h(t;k)and an\namplitude estimate \u000enfrom section 2 and 3 for a damaged\npartialhof noten, we summarize our heuristic algorithm:\nFirst, we need to evaluate the damage. IfP\n(t;k)2Bd\nn;hgn;h(t;k)\u001cP\n(t;k)2Bn;hgn;h(t;k),\nwe leave it as intact; otherwise we need to repair it. Spe-\ncially, if undamaged part Bn;hrBd\nn;his a narrow band-\nlimited “strip” (e.g. a single frequency bin), we need to\n“expand” the solo mask to remove those initially deemed\n318\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Figure 4. Evaluating Desolo Damage and Possible Fix Using Both Score and Spectrogram\n“undamaged” f-t cells as well because such residue tends\nto create artifact “musical noise” whose suppression de-\nserves treatment, mostly from speech enhancement. After\nsuch extra “masking”, we use Bu\nn;h\u0012Bn;hto denote the\nremaining undamaged region.\nSecond, since Bn1;h1\\Bn2;h26=;;n 16=n2for pos-\nsibly many different note partials contributing energy to\nthe same region, we choose one damaged orchestra partial\n(n;h) to repair: argmax\n(n;h)P\n(t;k)2Bn;h\u000engn;h(t;k)assum-\ning Max-Approximation that only one signal dominates in\neach time-frequency cell [3].\nThird, in the score we look for consonant intervals such\nas octaves, perfect 5th and perfect 4th in the hope to ﬁnd an\nobservable partial whose frequency is in a relatively sim-\nple ratio to the damaged one waiting to be “transposed” to.\nWe call this partial, if exists, a candidate. Usually more\nthan one candidate exist. Large modulus value, simple fre-\nquency ratio and identical instrumentation are factors that\nwe favor in choosing the best candidate without creating\nartifacts. Thus, harmonic transposition can be performed\nvertically on the spectrogram (e.g. from 3rd to 5th har-\nmonic of viola note B3 in Figure 4) if the duration of the\ncandidate partial covers that of the damaged area.\nFourth, when there is no candidate partial for the par-tial indexed by (n;h), if there exists a partial (m;i)whose\ntime support of its undamaged portion Tu\nm;iis adjacent to\nthe damaged duration Td\nn;hand whose frequency bin sup-\nportKm;isatisﬁesKd\nn;h\u0012Km;iwe can perform phase-\nlocked modulation with differenced phase sequence esti-\nmated from Bu\nm;itoBd\nn;h. The 2 cello partials in Figure 4\nare repaired this way.\nOccasionally, we are unable to perform either transfor-\nmation and label the damaged partial as such.\n6. EXPERIMENT RESULTS\nWe experiment with an excerpt of 45 seconds from the 2nd\nmovement of Ravel’s piano concerto in G major.\nTable 1 lists a breakdown of the number of partials1\nand the number of harmonic transpositions and phase-locked\nmodulation that our algorithm performed. The last column,\n“unable to ﬁx” gives the number of occurrences that no\nundamaged orchestra partial is available to estimate phase\nfrom. We relax on that the 4 sections of string instruments\ncan be used to repair each other by harmonic transposition\nbut do not allow any harmonic transposition between two\ndifferent instruments in the woodwind family. This is be-\n1the number of partials only include partials that have signiﬁcant spec-\ntral energy and are below Nyquist frequency at SR=8000Hz.\n319\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)note partial tran.\nfromtran.\ntomodu-\nlationunable\nto re-\npair\noboe 20 85 1 1 0 1\nclarinet 6 18 3 3 0 0\nﬂute 6 18 0 0 0 0\nviolin1 5 42 14 9 0 0\nviolin2 11 107 34 24 24 2\nviola 16 160 33 41 64 5\ncello 12 120 43 50 22 6\nTable 1. Instrument breakdown of partials being repaired\ncause the oboe is sharper than the other two in this excerpt.\nAt the end the most of damaged partials are ﬁxed in some\nway. We also notice that the woodwinds are less damaged\nbecause the notes are very high pitched and too loud to\nyield to the solo piano at their time-frequency region, while\nthe lower string instruments are frequently damaged.\nThe original, desoloed-but-unrepaired and repaired au-\ndio are available at our demo website http://xavier.\ninformatics.indiana.edu/ ˜yushan/ISMIR2010\nto evaluate the solo mask and improvement from unmask-\ning. Plots in color giving a breakdown of the partials on\nthe spectrogram are also available.\n7. CONCLUSION, EVALUATION AND FUTURE\nWORK\nInstead of merely extracting one source (instrument) of\nsound from the mixture, we distinguish our proposed ISS\nmethod from other known source separation methods by\nour explicit repair stage that addresses the audio degrada-\ntion caused by the separation procedure. This stage signif-\nicantly enhances the perceptual audio quality and boosts\nperformance measurement such as distortion due to inter-\nferences proposed by Vincent. That the reconstructed note\nsounds plausible for some orchestra instruments suggests\nthat the partial-wise phase/amplitude relationship is a po-\ntentially fruitful topic to investigate.\nAt this stage, we admit that the comparison of our method\nof “unmasking” with other missing data inference tech-\nniques such as [15] is not available and hence is our future\nwork. An ideal evaluation of any method of solo/orchestra\nseparation requires a “ground truth” of the two sources\nrecorded separately and an artiﬁcial mix of the two. How-\never, such “ground truth” is almost away absent in the real\ncase and the evaluation is mainly subjective. Our explo-\nration begins with a music sample library to artiﬁcially\nconstruct ground truth according to the score while main-\ntaining the texture of the music of interests.\n8. REFERENCES\n[1] Bell, A. J., and Sejnowski, T. J.: “An Information-\nMaximization Approach to Blind Separation and Blind\nDeconvolutionm Neural Computation, vol. 7, no. 6, pp.\n1129 - 1159, 1995.[2] E. Vincent: “Musical Source Separation Using Time-\nFrequency Source Priors,” IEEE Trans. on Speech and\nAudio Processing , V ol. 14, Iss. 1, Jan. 2006 pp. 91 -\n98.\n[3] D. Ellis: Chap. 4 of Computational Auditory Scene\nAnalysis: Principles, Algorithms and Applications,\nWiley/IEEE Press, pp.115-146, 2006.\n[4] A. S. Bregman: Auditory scene analysis. MIT Press:\nCambridge, MA, 1990.\n[5] Francis R. Bach and Michael I. Jordan: “Blind one-\nmicrophone speech separation: A spectral learning ap-\nproach.”, NIPS, pages 6572, 2005.\n[6] S. Dubnov: “Optimal ﬁltering of an instrument sound\nin a mixed recording using harmonic model and score\nalignment,” ICMC 2004, Miami.\n[7] Y . Han, C. Raphael: “Desoloing Monaural Audio Us-\ning Mixture Models,” ISMIR, Vienna, 2007\n[8] A. T. Cemgil, S. J. Godsill: “Probabilistic Phase\nV ocoder and its application to Interpolation of Miss-\ning Values in Audio Signals.” Antalya/Turkey, 2005.\nEURASIP.\n[9] H. Fletcher: “Mode locking in nonlinearly excited in-\nharmonic musical oscillators,” V ol. 64, pp. 1566-1569\nJ. Acoust. Soc. Am., 1978.\n[10] Judith C. Brown: “Frequency ratios of spectral compo-\nnents of musical sounds,” J. Acoust. Soc. Am. vol. 99,\nno. 2, pp. 1210-1218, February 1996.\n[11] B. Raj and P. Smaragdis: “Latent Variable Decomposi-\ntion of Spectrogram for Single Channel Speaker Sepa-\nration,” IEEE Workshop on Applications of Signal Pro-\ncessing to Audio and Acoustics , pp. 17-20, Oct. 2005.\n[12] R. Dannenberg and C.Raphael: “Music Score Align-\nment and Computer Accompaniment,” Communica-\ntions of the ACM, 49(8) (August 2006), pp. 38-43.\n[13] N. Ono, K. Miyamoto, J. Le Roux, H. Kameoka, and\nS. Sagayama: “Separation of a Monaural Audio Sig-\nnal into Harmonic/Percussive Components by Comple-\nmentary Diffusion on Spectrogram,” EUSIPCO., 2008\n[14] Xavier Rodet: “Musical Sound Signal Analy-\nsis/Synthesis: Sinusoidal+Residual and Elementary\nWaveform Models,” IEEE Time-Frequency and Time-\nScale Workshop 97, Coventry, Grande Bretagne, 1997\n[15] J Bouvrie and T Ezzat.: “An incremental algorithm for\nsignal reconstruction from short-time fourier transform\nmagnitude.” 9th Intl. Conf. on Spoken Language Pro-\ncessing, 2006.\n[16] R. E. Kalman: “A New Approach to Linear Filtering\nand Prediction Problems,” Transaction of the ASME -\nJournal of Basic Engineering, 35-45. March 1960.\n[17] R.L. Eubank: A Kalman Filter Primer, Chapman &\nHall/CRC, 2005.\n320\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "An Interchange Format for Optical Music Recognition Applications.",
        "author": [
            "Andrew Hankinson",
            "Laurent Pugin",
            "Ichiro Fujinaga"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1417633",
        "url": "https://doi.org/10.5281/zenodo.1417633",
        "ee": "https://zenodo.org/records/1417633/files/HankinsonPF10.pdf",
        "abstract": "Page appearance and layout for music notation is a critical component of the overall musical information contained in a document. To capture and transfer this information, we outline an interchange format for OMR applications, the OMR Interchange Package (OIP) format, which is designed to allow layout information and page images to be preserved and transferred along with semantic musical content. We identify a number of uses for this format that can enhance digital representations of music, and introduce a novel idea for distributed optical music recognition system based on this format.",
        "zenodo_id": 1417633,
        "dblp_key": "conf/ismir/HankinsonPF10",
        "keywords": [
            "Page appearance and layout",
            "OMR applications",
            "OMR Interchange Package (OIP)",
            "semantic musical content",
            "digital representations of music",
            "distributed optical music recognition system",
            "novel idea",
            "layout information",
            "page images",
            "transfer along with"
        ],
        "content": "AN INTERCHANGE FORMAT FOR OPTICAL MUSIC \nRECOGNITION APPLICAT IONS \nAndrew Hankinson1 Laurent Pugin2 Ichiro Fujinaga1 \n1Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT)  \nSchulich School of Music, McGill University  \n2RISM Switzerland  & Geneva University  \nandrew.hankinson@mail.mcgill.ca, lxpugin@gmail.com, \nich@music.mcgill.ca  \n \nABSTRACT  \nPage appearance and layout for  m u s i c  n o t a t i o n  i s  a  \ncritical component of the overall musical information \ncontained in a document . To capture and transfe r this \ninformation, we outline an interchange format for OMR \napplications, the OMR Interchange Package (OIP) format, which is designed to allow layout information \nand page images to be preserved and transferred along \nwith semantic musical content. We ident ify a number of \nuses for this format that can enhance digital \nrepresentations of music, and introduce a novel idea for \ndistributed optical music recognition system based on this \nformat. \n1. INTRODUCTION \nPage appearance and layout for music notation is a \ncritical component of the overall musical information contained in a document. For example, musically \nsemantic information, such as note duration, is often \nvisually augmented by adjusting horizontal spacing to \nreflect a spatial representation of note duration [1]. Some \nscholars infer geographical origin or time period based on note shapes, or even, in the case of handwritten manuscripts, by the particular “hand”  o f  a  s c r i b e  [2, 3]. \nThe layout may also reveal some subtle intent of the \ncomposer,  especially in sketches and autograph \nmanu scripts [4]. \nTo date, however, there has been little effort to \nattempt to preserve this informati on when a page is \nscanned and processed by optical music recognition \n(OMR) software. This presents several opportunities for \nimprovement. By maintaining a direct relationship between recognized musical symbols and the original image it was extracted from, we contend that musicians \nand music scholars will be better able to understand and \ninterpret digital facsimiles of musical documents while simultaneously providing the ability to index, search , and \nretrieve these documents.  \nFor OMR researchers, this also presents an \nopportunity to build large global ground- truth datasets. \nBy maintaining the relationship between the graphical representation and the semantic interpretation of a musical symbol, we can build sets of training data which exemplar -based adaptive supervised -learning software \ncan use to train and test its recognition models. Furthermore, by allowing for these datasets to be shared \nbetween different adaptive OMR platforms , we can take \nadvantage of work done by others who have created \ndifferent datase ts to further improve recognition  software . \nThis is discussed further in Section 4.  \nIn this paper, we present the OMR Interchange \nPackage (OIP) format, a common interchange format for OMR applications that bundles notation, images , a n d  \nmetadata together in  a  s i n g l e  f i l e .  Work on this format \nwas inspired by functionality present in large, established \ndigitization projects, most notably Google Books and the Internet Archive. These projects use file formats designed \nto preserve layout information in textual materials. We \ndiscuss two such formats, hOCR and DjVu, and examine them for ideas of how we might construct a similar music notation- specific format.  \nRather than build a completely separate set of \nspecifications, the OIP format combines established \nstandards  into an application profile —that is, we provide \nspecifications on how these standards should be \ncombined. These standards concern music, image , a n d  \nmetadata encoding formats, contained within an established standard for packaging and serializing  these \nfiles into a single file, for easy transport across multiple \nsystems . By taking an application profile approach, \ninstead of establishing a new, monolithic standard, we hope to take advantage of existing software to manipulate component files, e.g. , r e a d i n g  a n d writing images, and \ndelegate the maintenance and improvement of the \ncomponent standards to their respective communities.  \nOne of the  goal s for developing the OIP format is to \nprovide a mechanism for interchange between different \nelements in an OMR digitization workflow, from capture through recognition and int o any number of potential \nuses. Specific design considerations were made to ensure \nthat non -common practice notation systems are \n \nPermission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted witho ut fee provided that copies \nare not made or distributed for profit or commercial advantage and that \ncopies bear this notice and the full citation on the first page.  \n© 2010  International Society for Music Information Retrieval  \n51\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \naccommodated, to allow for encoding earlier musical \nprint and manuscrip t sources. \n2. BACKGROUND \n2.1 Optical Character Recognition  \nThe Google Books project [5] a n d  t h e  Internet Archive  \n[6] a r e  i n d u s t r i a l -scale initiatives to convert physical \ntextual items, e.g., books, magazines , and newspapers, to \nsearchable digital representations . A s  items in these \ncollections are digitized, the ir page images are processed \nby OCR software, extracting the textual content , a n d  \nfacilitating full- text searching of their collections. \nWithin the OCR workflow,  the precise location on the \npage where a word  occurs is saved through the use of a \nbounding box that defines a region around the word . \nWhen the words on the page are converted to searchable \ntext, the bounding box coordinates are stored, along with \nthe word itself. In some cases, similar coordinates c an be \nstored to outline higher -level page elements such as lines, \ncolumns , or paragraphs. Figure 1 shows an example from \nthe Internet Archive of a page image returned from a full -\ntext search with the phrase “Them will I gild with my \ntreasure” highlighted  in reference to its position on the \noriginal  page scan.  \n \n \nFigure 1: D o c u m e n t  w i t h  s e a r c h  t e r m s  h i g h-\nlighted in situ . (Source: Internet Archive ) \nIn contrast, we can find little evidence to suggest \nsimilar techniques are in widespread use for databases of \nmusic documents . Instead, collections either choose to \nsimply display the page image with no transcription of \nthe source available (e.g., [7, 8]), or transcribe the content \ninto a searchable and manipulable digital format without \nreference to the original page layout  (e.g., [9 ]). For music \ndocuments, where the layout of the symbols can play a critical role in determining the intended interpretation of \nthe music, we posit that a hybrid approach is needed, \nsimilar to that demonstrated by  G o o g l e  B o o k s  o r  t h e  \nInternet Archive.  \nCritical to the development of these systems is a \ncommon standard that allows various systems in an OMR \nworkflow to capture and preserve images, layout , a n d  \nmusic semantics. To help inform our development of \nsuch a standard, we identified formats used in the textual \ndomain for e n c o d i n g  l a y o u t  i n f o r m a t i o n :  The hOCR format, developed as an output format  for the Google -\nsponsored OCRopus document analysis software, and \nDjVu, a third -party document imaging solution adopted \nby the Internet Archive for displaying its digitized texts.  \n2.1.1 hOCR \nhOCR [10] is a format that uses standard HTML tags, but \nembeds OCR -specific information that can be read and \nmanipulated by other OCR software. According to the authors of the hOCR specification, it can be used to encode “layout information, character confidences, bounding boxes,  and style information” [ 11]. For generic \nHTML rendering software, like a web browser, the OCR -\nspecific information is ignored and the page is rendered \nwithout interference.  \nFor the developers of hOCR , HTML was preferred \nover the definition of a new XML format  since the \nHTML specification already contains many tags for \ndefining document elements, such as headings, tables, \nparagraphs, and page divisions. Furthermore , the files can \nbe viewed, manipulated , and processed with a wide range \nof existing tools, such as browsers, editors, converters , \nand indexers.  \nTo encode information about a page layout, hOCR \nuses the “class” and “title” attributes of HTML tags. For example, a bounding box outlining a paragraph may be \ndefined as:  \n \n \n<div class=”ocr_par” id=”par_7” \ntitle=”bbox 313 324 733 652”>  \n   ...paragraph text...  \n</div> \n \nFigure 2: h O C R  f o r m a t  d e f i n i n g  a  p a r a g r a p h  \nbounding box \nThe bounding box is given as two sets of pixel co -\nordinates corresponding to the upper -left and lower -right \ncorners of the box, relative to the upper- left corner of the \nimage. \nPage images corresponding to the text output are \nlinked from the hOCR document with either a local path \nname or an HTTP URL. The identity and integrity of the image file can be verified by embedding the MD5 checksum of the image file in the hOCR file.  \n2.1.2 DjVu  \nDjVu is primarily designed as a highly efficient method of compressing and transferring images and documents. Included in its specification, however, is the ability to include a “hidden” text layer within a binary DjVu file.  \nThe DjVu format specification [12] d e f i n e s  s e v e n  \ndifferent types of document “zones,” each featuring a bounding box defined by an offset co -ordinate from a \npreviously defined zone and a given width and height. These zones can define boundaries for pages, columns, \n52\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \nregions, paragraphs, lines, words, o r  c h a r a c t e r s .  T e x t  i s  \nencoded as UTF -8. \n2.2 Music Applications \nhOCR and DjVu are not the only formats that can provide \npositional information about text. The popular PDF standard allows for this functionality as well. They serve , \nhowever,  a s  e x a m p l e s  o f  e x i s t i n g  f o r m a t s  i n  t h e  t e x t u a l  \ndomain from which we can begin to discuss similar approaches in the musical domain.  \nTo begin building our standard, we define five  basic \ncriteria that the OIP format should conform to:  \n2.2.1 Must be self -contained  \nFiles conforming to the OIP format should be self -\ncontained in a single file. The choices here are between defining a unique binary format, as the DjVu format does, or allowing multiple files to be packaged as a single \nfile. \n2.2.2 Must encapsulate multi -page documents  \nBoth hOCR and DjVu encode multiple pages in a single file. hOCR provides only the textual content of those \npages and links to externally stored images, while DjVu stores both image and content for multiple pages within a single file.  \n2.2.3 Must encapsulate notation, images , and metadata  \nFor each page in the document our format must include a page image, the notation content, and, if available, any other metadata about that page.  Here, the music domain \nrequires a different approach than the text domain, owing largely to the complexity of encoding music notation over encoding text. In Section 3, we discuss the specific \nstandards chosen for this criteria.  \n2.2.4 Must use existing standards  \nDrawing largely on the arguments made by the hOCR \ndevelopers  to justify their use of HTML over creating a \nnew format [10], we specify that, wherever possible, \nexisting standards must be used in preference to creating one. This is especially true for encod ing notation, where \nnew formats are introduced every few years, often designed to meet very specific needs, and fall out of use within a few years of being introduced. By using existing standards, we hope to ensure a broader support \ncommunity beyond our specific application.  \n2.2.5 Must allow extended information  \nBeyond the required notation, images , a n d  m e t a d a t a  \nstorage, we see the OIP format as a general -purpose \ncontainer for storing any extra information about the page \ncontent. However, this extra information s hould be \nopaque to clients that do not support it, and should not interfere with their ability to read and write OIP files. For example, a specific application could save extended colour-space information about an image in the OIP, available to application s that can use it, but ignored by \nclients that cannot use it.  \n3. FORMAT SPECIFICATION \nAs discussed in the previous sections, we have chosen to \ncombine existing standards into an application profile. In \nthis section we will discuss our specific choices of standards and how they should be combined to create an OIP- compliant file. In the interests of space we will \nspecifically avoid any in -depth explanation  about the \ncomponent standards themselves, since they are freely available for consultation.  \n3.1 Packaging  \nAn OI P file is, at its most basic representation , a \ncollection of files and folders serialized as a single file. Rather than simply allowing  an ad hoc m e t h o d  o f  \nbundling these files and folders together, we chose to use a very minimal standard for organizing the content of these files.  \nThere are several ways to approach this problem. One \ntype of solution is exemplified by formats such as the Metadata Encoding and Transmission Standard (METS). \nData typically represented in b inary formats  (e.g., \nimages)  can be sto red, for example, within an XML file  \nby Base64 encoding . A single METS file containing \nmany high- quality i m a g e s  c ould potentially be many \ngigabytes in size , however. \nA second approach is the file bundle approach. This is \nused by many formats, including Mic rosoft’s XML- based \nOffice formats (e.g. , D O C X )  a n d  t h e  J a v a  J A R  f o r m a t .  \nThese files are simple file and folder hierarchies \ncontaining component files, such as images or text files. \nThey appear as a single file archive by using a well-\nknown file archiving system (e.g. , Z I P  o r  T A R ) .  Once \nthese bundles have been uncompressed , read and write \noperations on the smaller component files can be done \ndirectly via the native file system and not on the s i n g l e  \nmonolithic XML file. \nThe BagIt format is a lightweight file bundling \nspecification. It was created and is maintained by the \nLibrary of Congress and the California Digital Library. It is currently in the process of  becoming an IETF standard \n[13]. The name refers to a colloquial rendering of the \nEnclose and Deposit method [14], also known as the “bag \nit and tag it” method. \nThis format defines a simple hierarchy of files and \nfolders, known as a “bag.” These can be represented plainly on any computer system as standard files and folders, or they can be converted  into a single file using \nZIP or TAR packaging.  \nMinimally, one directory and two files must be \npresent in every bag in order to be considered compliant to the standard. A data directory contains any arrangement of files or folders are stored. This is the bag’s “payload .” One of the required files is a bagi t.txt \n53\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \nfile that simply stores the version of the BagIt \nspecification to which that bag conforms and the \ncharacter encoding used for the me tadata files. The \nsecond required file is a manifest file listing checksums for each file within the data directory, helping to ensure the integrity and identity of each of the files in the bag. Other optional files are outline d in the BagIt \nspecification  [13]. \nFigure 3: A generalized OIP structure. \nFor the OIP format, we further specify a file hierarchy \nwithin the data directory of a bag. A folder is created in the data directory for each page in a multi -page \ndocument , allowing the f o r m a t  t o  accommodate \ndocuments of any size . In each page folder, we store files \nrelating to this page. A generalized example of the OIP \nstructure is given in  Figure  3. \nThis does not create incompatibilities with the \noriginal BagIt specification, as there  i s  n o  s t r u c t u r e  t o  \nwhich the data directory must conform. Software for processing BagIt files will guarantee the integrity and \nidentity of each file in the bag without needing to \nunderstand the OIP format.  \n3.2 Notation  \nThere are many file formats for encoding  music notation, \nbut for this specific application we require a format that \ncan encode positional coordinates for every musical \nelement on the page. This eliminates many traditional formats used for notation  interchange , such as MIDI. The \nNotation Intercha nge File Format (NIFF) fits this \nrequirement, but is no longer actively maintained and is considered an obsolete standard  [15]. T h e  S h a r p E y e  \noutput format (MRO) [ 16] also encodes this information \nand is used by [17] to provide positioning information for \nmusical elements. This format , however, i s  s p e c i f i c a l l y  \ndesigned for use with common W estern notation  (CWN) , \nlimiting its usefulness for older or alternative notation systems. MusicXML [ 1 8 ]  a n d  N e u m e s X M L  [19] focus respectively on CWN and neumed notation, l imiting their \napplicability for a broad range of notation systems.  \nFor OIPs, we recommend the use of the Music \nEncoding Initiative (MEI)  format as a notation encoding \nscheme. MEI inherits many features of the Text E ncoding \nInitiative (TEI) , a format specifically designed for \nscholars representing  original text sources in digital form.  \nMEI can also adequately represent CWN as well  as other \nnotation systems  [20]. \nMEI allows for bounding boxes, or “zones,” to be \ndefined for a given image and identi fied with a unique \nID. These id’s can then be attached to semantically \ndefined musical elements in MEI. A bri ef example is \nshown in  Figure 4.  \nFigure 4: A MEI -formatted exa mple showing \nbounding box definitions.  \nIn MEI, t he \n<graphic> element defines a link to a \npage image, while subsequent <zone> elements outline \nregions of this image, identified with a unique xml:id \nattribute. These zones are then used later within the music  \nnotation markup, as illustrated in Figure 4 by the \n<measure> tag. It uses the facs attribute to link a \ndefined bounding box to a measure definition. This attribute is available to all music notation elements. \n3.3 Images \nFor image formats, we follow the guidel ines given in  [21] \nfor musical master archival images. These guidelines \nrecommend lossless file encoding formats such as TIFF \nor PNG  for archival formats. While there is no techni cal \nreason for not using other formats such as lossy JPEG, \nwe suggest lossless formats to maintain the highest possible image quality.  \nOne issue w e  h a v e  n o t  y e t  a d d r e s s e d  i s  h o w  t o  \nreconcile the differences between an original image and an image file that has been cropped, de -skewed , a n d  \nprepared for processing by an OMR package. Since any geometric manipulation will affect the co -ordinates of the \nmusical elements on the page, it would be difficult to automatically reconcile the position of musical elements  \nin an original image, when the notation was extracted <bag-directory>  \n     |- bagit.txt  \n     |- manifest -md5.txt \n     |- [other optional bagit files]  \n     |- data \n         | - [page 1]  \n         |     | - [image files]  \n         |     | - [notation  files] \n         |     | - [metadata files]  \n         | - [page 2]  \n         |     | - [image files]  \n         |     | - [notation files]  \n         |     | - [metadata files]  \n         | - [etc.] \n <facsimile source=\"s2\">  \n   <surface>  \n     <graphic xml:id=\"s2p1\"    \nxlink:href=\"m000001719_0001.tif\"/>  \n     <zone xml:id=\"s2p1z1\" lrx=\"0\" \nlry=\"0\" ulx=\"0\" uly=\"0\"/>  \n     <zone xml:id=\"s2p1z2\" lrx=\"1\" \nlry=\"1\" ulx=\"10\" uly=\"10\"/>  \n   </surface>  \n</facsimile>  \n<!-- ... --> \n<measure n=\"1\" xml:id=\"d1e656\" \nfacs=\"s2p1z1\"/>  \n \n54\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \nusing a processed image. This becomes especially \nimportant when considering the OIP format as an \ninterchange format between multiple OMR systems, each of which may use different image processing techniques, or even require that certain elements of an image be removed prior to recognition, such as  staff lines.  \nTo reconcile this, we specify that, at a minimum, an \nOIP should contain the original page image, and a page \nimage that the OMR system used during the recognition stage prior to removing any musically relevant elements. The additional inclusion of  any intermediary images used \nby OMR software is permitted, but not required . For an \nOIP that has been processed by multiple OMR packages, \neach package shou ld save its source image and \nrecognized notation in MEI.  \n3.4 Metadata  \nMEI has the facilities  t o  c a p t u r e  b i b l i o g r a p h i c ,  a n a l y t i c , \nand editorial metadata. There is also the possibility that other metadata can be captured and stored within the file \nhierarchy. Whi le we do not require any further metadata \nbeyond what can be captured in MEI, we do not prevent the inclusion of other files with metadata formats \ndescribing, for example, detailed image processing  \ntechniques, historical and archival information, or librar y-\nspecific local information.  \n4. APPLICATIONS \nWe have formulated the OIP format as an interchange format between multiple elements of an OMR workflow, from dig itization through recognition, and finally into a \ndelivery format specifically designed to capture and transfer page layout along with the semantic music content. In this section, we identify three specific \napplications  w h e r e  O I P  f i l e s  c a n  b e  implemented  as a \nstandardized format for constructing tools useful for \nmusic scholars and OMR research.  \n4.1 Diplomati c Facsimiles  \nWhile there is some disagreement on the actual definition of the term, we define diplomatic facsimile as “a \nvisualization (on -screen or in print) from the digital \ntranscription of a source artifact, such that it has the same \nsemantic content as the source, and its glyphs and layout are similar to the original source” [22]. \nFor notation styles outside of the CWN tradition, a \ndiplomatic facsimile provides the ability to transcribe a \nmusical source  w i t h  i t s  o r i g i n a l  l a y o u t  a n d  s y m b o l s ,  \nwithout interpreting it by using modern music notation \nsymbols. Barton, Caldwell , a n d  J e a v o n s  p r o v i d e  a n  \nexcellent overview of the importance of this distinction  \n[23]. Diplomatic facsimiles also allow libraries and \narchives to withhold distribution of original images due to copyright restrictions, while simultaneously allowing scholars access to a faithful electronic reproduction of the original musical content, including precise positioning for \neach musical element in the document. \n4.2 Online Music Document Databases  \nAn online database of music documents, similar to Go o-\ngle Books or the Internet Archive’s display of textual \ndocuments, could be constructed with OIP as a source \nformat for these documents. In an OMR workflow , OIP \nfiles would serve as an interchange format between the \nOMR software and a database system designed to org a-\nnize, index , and display these documents.  \nAs mentioned in the introduction, music scholars \noften use visual cues in the layout of a page of musi c to \ndetermine how a piece of music might be performed, or \nwhere it came from. Viewing these documents in their original form, while still making them available for content -specific searching and indexing, would provide a \nvaluable research tool for many mu sic scholars. \nFurthermore, an online music document database \ncould highlight relevant musical phrases matching a user’s query, displayed as an invisible layout on the original image. Advanced computer processing could potentially provide links between simi lar passages within, \nor across, musical pieces, allowing users to navigate a document by musical phrase.  \n4.3 Distributed Optical Music Recognition  \nThe extent, variety , a n d  v a r i a b i l i t y  o f  m u s i c a l  s y m b o l s  \npose a unique problem to optical music recognition softwa re. These symbols encompass indications of pitch, \nduration, dynamics, tempo , o r  p e r f o r m e r  i n t e r p r e t a t i o n  \n(e.g., turns and trills). Different printing practices or fonts \nalso introduce variations in these shapes.  \nAdaptive OMR (AOMR) software attempts to \naccount for this variability by using machine -learning \nmethods for understanding and interpreting new shapes, or variations on known shapes. These systems are often \ntrained using human annotators or correctors, who \nprovide a system with the correct musical interpretation \nof a graphical shape  [24]. \nThis training process is often the most tedious and \nexpensive part of the OMR process. Developing training \nsets of sufficient quantity and variety  is an expensive and \nlabour -intensive pro cess. Similarly, a poorly trained \nrecognition system will require more human intervention, leading to lower overall throughput for any digitization and recognition initiative. For large digitization projects, this can have a significant impact on the overall cost of \ndigitizing these materials  [25]. \nWith a common interchange format, however, these \ndata sets could be built cumulatively. As new pieces of \nmusic are recognized and corrected, this work can be \nsaved and used to train other AOMR clients with no further intervention by a human annotator.  \nPerhaps more importantly, this concept can be used to \nbuild a distributed global network of AOMR clients. Sharing training data with o ther networked OMR clients \n55\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \nwould allow them to build their recognition models using \ndata previously provided by other members of the \nnetwork. For example, an archive that has provided a data set of examples from a 16th -Century Italian music printer \ncan make this data set available immediately to other members of the network, allowing these clients to immediately re- train their recognition systems to take \nadvantage of this new data and increase their accuracy on this particular repertoire.  \n5. CURRENT AND FUTURE WORK  \nTo date, we have finished the initial release of an open source Python library for reading and writing BagIt files, available at [ 26]. T h i s  i s  p a r t  o f  a  l a r g e r  p r o j e c t  t o  \ndevelop a distributed optical music recognition system, a networked collection of adaptive OMR clients.  \n6. REFERENCES \n[1] Blostein, D., and L. Haken. 1991. Justification of \nprinted music . Communications of the ACM 34 (3): \n88–99. \n[2] Warmington, F. 1999. A survey of scribal hands in \nthe manuscripts . In The treasury of Petrus Alamire: \nMusic and art in F lemish court manuscripts 1500–\n1535, ed.  H. K e l l m a n , 41–52. Amsterdam: Ludion \nGhent.  \n[3] Luth, N. 2002. Automatic identification of music n o-\ntations. In Proceedings of the International Co nfer-\nence on Web Delivering of Music (WEDELMUSIC) , \n203– 10. \n[4] Hall, P. 1997. A view of Berg’s Lulu through the \nautograph sources . Berkeley: University of Califo r-\nnia Press.  \n[5] Google Books. http://books.google.com [A ccessed \n23 March 2010].  \n[6] Internet Archive . http://www. archive .org [Accessed \n23 March 2010].  \n[7] Schubert Autog raphs. http://www.schubert -\nonline.at/activpage/index_en.htm  [ A c c e s s e d  2 3  \nMarch 2010].  \n[8] Digital Image Archive of Medieval Music. \nhttp://www.diamm.ac.uk [Accessed 23 March 2010].  \n[9] The Computerized Mensural Music Editing Project. http://www.cmme.org [Accessed 23 March 2010].  \n[10] Breuel, T. M., and U. Kaiserslautern. 2007. The \nhOCR  m i c r o f o r m a t  f o r  OCR  w o r k ﬂow and results.  \nIn Proceedings of  the International Conference on \nDocument  Analysis and Recogn ition (ICDAR), 1063–\n7.  \n[11] hOCR- tools. http://code.google.com/p/hocr -tools \n[Accessed 23 March 2009]. \n[12] Celartem/Lizartech DjV u format reference. \nhttp://djvu.org/docs/ DjVu3Spec.djvu [Accessed 23 \nMarch 2010].  [13] The BagIt File Packaging Format. https://  \nsvn.tools.ietf.org/html/draft -kunze -bagit -04  \n[Accessed 23 March 2010]. \n[14] Tabata, K., T. Okada, M. Nagamori, T. Sakaguchi, \nand S. Sugimoto. 2005. A collaboration model be-\ntween a r c h i v a l  s y s t e m s  t o  e n h a n c e  t h e  r e l i a b i l i t y  o f  \npreservation by an enclose -and-deposit method. In \nProceedings of the International Web Archiving \nWorkshop.   \n[15] Castan, G. 2009. NIFF. http://www.music -\nnotation.info/en/formats/NIFF.html . [Accessed 29 \nMay 2010].  \n[16] Jones, G. OMR engine output file format. \nhttp://www.visiv.co.uk/tech -mro.htm [ A c c e s s e d  2 9  \nMay 2010].  \n[17] Kurth, F., D. Damm, C. Fremerey, M. Müller, and M. Clausen. 2008. A framework for managing mu l-\ntimodal digitized music collections. In Research and \nadvanced  t e c h n o l o g y  f o r  d i g i t a l  l i b r a r i e s , 334 –45. \nBerlin: Springer.  \n[18] Good, M. 2009. Using MusicXML 2.0 for music ed i-\ntorial applications. In Digitale Edition zwischen Ex-\nperiment und Standardisierung , ed. P. Stadler and J. \nVeit, 157 –74. Tübingen: Max Niemeyer.  \n[19] Barto n, L. 2002. The NEUMES  project: Digital tra n-\nscription of medieval chant manuscripts. In Proceed-\nings of the Web Delivering of Music \n(WEDELMUSIC), 211 –8. \n[20] Roland, P. 2009. MEI as an editorial music standard. In Digitale Edition zwischen Experiment und Stan-\ndardisierung , ed. P. Stadler and J. Veit, 175– 94. \nTübingen: Max Niemeyer.   \n[21] Riley, J., and I. Fujinaga. 2003. Recommended best practices for digital image capture of musical scores. \nOCLC Systems and Services  19 (2): 62 –9. \n[22] Glossary of terms used by the NEUMES p roject. \nhttp://www.scribeserver.com/NEUMES/help/glossary.htm [Accessed 23 March 2010].  \n[23] Barton, L. , J. Caldwell, and P. Jeavons. 2005. E -\nlibrary of medieval chant manuscript transcriptions . \nIn Proceedings of the Annual Joint Confere nce on \nDigital Libraries (JCDL), 320– 39. \n[24] Fujinaga, I. 1997. Adaptive optical music recogn i-\ntion. PhD diss., McGill University.  \n[25] Pugin, L., J. Burgoyne, and I. Fujinaga. 2007. Redu c-\ning costs for digitising early music with dynamic a d-\naptation. In Proceedings of the European Conference  \non Digital Libraries  (ECDL) , 471– 4. \n[26] Hankinson, A. 2010. PyBagIt  1 . 0  d ocumentation . \nhttp://www.musiclibs.net/pybagit  [Accessed 29 May \n2010].  \n56\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "String Quartet Classification with Monophonic Models.",
        "author": [
            "Ruben Hillewaere",
            "Bernard Manderick",
            "Darrell Conklin"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1417619",
        "url": "https://doi.org/10.5281/zenodo.1417619",
        "ee": "https://zenodo.org/records/1417619/files/HillewaereMC10.pdf",
        "abstract": "Polyphonic music classification remains a very challeng- ing area in the field of music information retrieval. In this study, we explore the performance of monophonic mod- els on single parts that are extracted from the polyphony. The presented method is specifically designed for the case of voiced polyphony, but can be extended to any type of music with multiple parts. On a dataset of 207 Haydn and Mozart string quartet movements, global feature models with standard machine learning classifiers are compared with a monophonic n-gram model for the task of composer recognition. Global features emerging from feature selec- tion are presented, and future guidelines for the research of polyphonic music are outlined.",
        "zenodo_id": 1417619,
        "dblp_key": "conf/ismir/HillewaereMC10",
        "keywords": [
            "polyphonic music classification",
            "music information retrieval",
            "monophonic models",
            "single parts",
            "voiced polyphony",
            "Haydn and Mozart string quartet movements",
            "composer recognition",
            "global feature models",
            "standard machine learning classifiers",
            "monophonic n-gram model"
        ],
        "content": "STRING QUARTET CLASSIFICATIONWITHMONOPHONICMODELS\nRu\nben Hillewaere andBernard Manderick\nComputationalModelingLab\nDepartmentofComputing\nVrijeUniversiteitBrussel\nBrussels,Belgium\n{rhillewa,bmanderi }@vub.ac.beDarrell Conklin\nDepartmentofComputerScienceand AI\nUniversidaddelPa´ ıs Vasco\nSan Sebasti´ an,Spain\nIKERBASQUE , BasqueFoundationorScience\nBilbao,Spain\ndarrell conklin@ehu.es\nAB\nSTRACT\nPolyphonic music classiﬁcation remains a very challeng-\ning area in the ﬁeld of music informationretrieval. In this\nstudy, we explore the performance of monophonic mod-\nels on single parts that are extracted from the polyphony.\nThe presentedmethodisspeciﬁcally designedfor thecase\nof voiced polyphony, but can be extended to any type of\nmusic with multiple parts. On a dataset of 207 Haydnand\nMozart string quartet movements, global feature models\nwith standard machine learning classiﬁers are compared\nwithamonophonic n-grammodelforthetaskofcomposer\nrecognition. Global features emerging from feature selec-\ntionarepresented,andfutureguidelinesfortheresearchof\npolyphonicmusicareoutlined.\n1. INTRODUCTION\nIn the ﬁeld of music information retrieval, much research\nhasbeendoneinsymbolicmusicgenreclassiﬁcation,where\namodelhastoassignanunseenscoretoacertainclass,for\nexamplestyle,period,composerorregionoforigin. There\nare two main categories of models that have been widely\ninvestigated: global feature models andn-gram models .\nGlobalfeaturemodelsexpresseverypieceasafeaturevec-\ntor and use standard machine learning classiﬁers, whereas\nn-grammodelsrelyonsequentialeventfeatures.\nInarecentpaper[10]theresultsofa thoroughcompar-\nison of these types of models are reported for the task of\nclassifying folk songs based on their region of origin on a\nlarge monophonic data set. That study demonstrates that\nthen-gram models are always outperforming the global\nfeature modelsforthis classiﬁcation task. It is an interest-\ning question whetherthis result still holdsin a polyphonic\nsetting.\nIn the literature, it appears that most research has been\ninvestigatingclassiﬁcation or characterizationof melodies\n(monophonic) [5,14,16], but only few efforts have been\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom useis granted without fee provided that copies are\nnotmadeordistributed forproﬁtorcommercialadvantageandthatcopies\nbear this notice and the full citation on theﬁrstpage.\nc/circlecopyrt2010 International Society for MusicInformation Retrieval.made to developpolyphonicmodels. In [15], orchestrated\npolyphonicMIDI ﬁles are classiﬁed using global features,\nincludingsomefeaturesaboutmusical textureandchords.\nAsetofpolyphonicfeaturesbasedoncounterpointproper-\ntieswasdevelopedby[19],andappliedtothetaskofcom-\nposerclassiﬁcation. Theyﬁndthat thedistinctionbetween\nHaydnandMozartstringquartets,whichisveryinteresting\nfroma musicologicalpointofview,isa hardclassiﬁcation\ntask.\nWhen considering polyphonic music, it is essential to\nqualifytheformofinput. Twoformatscanbe considered:\nvoiced:aﬁxedandpersistentnumberofparts;and,\nunvoiced: free polyphonythat is not available in, or can-\nnotbeeasilydividedintoparts.\nAtypicalexampleofvoicedpolyphonyisastringquar-\ntet, consisting of 4 well-deﬁned voices (Violin 1, Violin\n2, Viola and Cello). Unvoiced polyphonyis common, for\nexample,inpianomusic.\nAnotherwaytoviewthisdichotomyofpolyphonicmu-\nsic is in terms of a MIDI ﬁle type: voiced (type 1), or un-\nvoiced (type 0), realizing of course the grey area where\ntrackswithin a type1 ﬁle maycontaininternalpolyphony,\nand where type 0 ﬁles identify voices by use of channel\nnumbers.\nThispaperinvestigateshowmonophonicglobalfeature\nandn-grammodelsperformontheclassiﬁcationofHaydn\nand Mozart string quartets in their original voiced format.\nThe voiced structure is exploited since these monophonic\nmodelsare appliedto separatevoices. The initialdatabase\nused in [19]containing107 string quartet movementswas\nextended to a total of 207 movementsin order to measure\nstatistically morerelevantdifferences.\nTwo tentative hypotheses from previous work [11] are\nbeingveriﬁedinthispaper:\n1.n-gram models also perform better than global fea-\nturemodelsonmonophonicpartsextractedfromthe\npolyphonictexture.\n2. the ﬁrst violin is the most distinctive voice of the\nstringquartets.\n537\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Adagio\nWolfgang Amadeus Mozart\n43\n43\n43\n43tr43tr\n43\n43\n43\nFigure1. ThevoiceseparationofthestringquartetsintoViolin1,Violin2,Viola andCello.\nF\nor the global feature models, special care has been\ntakentoapplyfeatureselectionwithintheinnerloopofthe\ncross validation scheme, in order to avoid overoptimistic\nevaluationestimates [8,17]. A similar procedurehas been\nset up to tune the parameters of the learning algorithms\nduringthetrainingphase. Featuresthatemergerecurrently\nin thefeatureselectionprocessarehighlighted.\nTheremainderofthispaperisstructuredasfollows. We\nstart by introducing the dataset and the music representa-\ntions, global feature and n-gram models, and the classi-\nﬁcation methodology in the next section. Then, we give\nthe results of the described models on the Haydn/Mozart\ndataset. We end with a discussion and some directionsfor\nfuturework.\n2. METHODS\nIn this section we describe the dataset used for our exper-\niments and we will give a short overview of both global\nfeature and n-gram models. Furthermore, we introduce\nour classiﬁcation methodology outlining the cross valida-\ntion setup combinedwith supervised feature selection and\nSVM parametertuning.\n2.1 Datasetand music representation\nTheHaydn/Mozartdatasetiscomposedof112stringquar-\ntet movements from Haydn and 95 string quartet move-\nments from Mozart, includingmost of the pieces from the\ndataset used in [19], but extending it as much as possible\nto nearly double its size. We chose to focus on the pe-\nriod1770-1790in whichbothcomposerswereactive, dis-\ncarding early or late Haydn quartets which might be easy\nto classify. In order to maximize the number of quartet\nmovements from Mozart, we included 8 movementsfrom\ntwo ﬂute quartets (K.285, K.298) and one oboe quartet\n(K.370),whicharewrittenforﬂute/oboe,violin,violaand\ncello, and thereby very similar to the string quartets. The\noriginalscoresin**kernformatwerefoundonthewebsite\nof the Center for Computer Assisted Research in the Hu-\nmanities at Stanford University[1]. We transformedthese\ntocleanMIDIﬁles,ensuringthatthefourvoicesappearon\nseparate tracks and that all barlines are correctly synchro-\nnized in all voices by correcting several errors in note du-rations. We retainedonlythehighestnoteof doublestops,\nthereby reducing each voice to a purely monophonic se-\nquence of notes. To enable the use of monophonicclassi-\nﬁcation techniques, we created four monophonicdata sets\ncalledViolin1,Violin2,ViolaandCellobyisolatingeach\nvoice of every string quartet movement, as illustrated in\nFigure1.\n2.2 Globalfeaturemodels\nIn this section we introduce global features and the cor-\nresponding global feature models. A global feature sum-\nmarizes information about a whole piece into a single at-\ntribute, which can be a real, nominalor booleanvalue, for\nexample“averagenoteduration”,“meter”or“major/minor”.\nWith a set of global features, pieces can be simply re-\nexpressedas featurevectors, and a wide range of standard\nmachine learningalgorithmscan then be applied to evalu-\nate thefeatureset.\nVoiced polyphony presents the advantage of having a\nﬁxed number of monophonic parts, which enables us to\nisolate these parts and apply monophonic models. In this\npaper three global feature sets are used to represent the\nmonophonic parts. These features describe melody char-\nacteristics,mainlyderivedfrompitchorduration,whereby\nwe mean that at least one pitch or duration value is in-\nspectedforthefeaturecomputation.\nTheglobalfeaturesets chosenarethefollowing:\n•TheAlicanteset of 28 global features, designed by\nP.J. Ponce de L´ eon and J.M. I˜ nesta in [16]. This set\nwas applied to classiﬁcation of MIDI tunes in jazz,\nclassical, and pop genres. From the full set, we im-\nplemented the top 12 features that they selected for\ntheirexperiments.\n•TheJesserset, containing 39 statistics proposed by\nB. Jesser[13]. Mostofthesearederivedfrompitch,\nsince they are basic relative interval counts, such as\n“amajsecond”,measuringthefractionofmelodicin-\ntervals that are ascending major seconds. Similar\nfeatures are constructed for all ascending and de-\nscendingintervalsin therangeoftheoctave.\n538\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)•TheM cKay set of 101 global features [15], which\nwere used in the winning 2005 MIREX symbolic\ngenre classiﬁcation experiment and computed with\nMcKay’s software package jSymbolic [2]. These\nfeatures were developed for the classiﬁcation of or-\nchestratedpolyphonicMIDIﬁles,thereforemanyfea-\ntures, for example those based on dynamics, instru-\nmentation, or glissando, were superﬂuous for this\nanalysis of monophonic single voices and we were\nabletoreducetheset downto61features.\nThese global feature sets do not show many overlapping\nfeatures,onlysomeverybasiconesoccurinmaximumtwo\nof the sets, such asthe “pitchrange”. Thereforeit is inter-\nesting to join the three feature sets to form the Joinedset,\nwhichmeanseverypieceisrepresentedasadatapointina\n112-dimensional feature space. We are interested in ﬁnd-\ning out which features are relevant for this speciﬁc task\nof composerclassiﬁcation, thereforewe will applyfeature\nselectiononthisJoinedset.\n2.3n-grammodels\nIn this section we introduce n-gram models and how they\ncan be used for classiﬁcation of music pieces using event\nfeatures. n-grammodelscapturethe statistical regularities\nof a class by modeling the probability of an event given\nits preceding context and computing the probability of a\npiece as a product of event probabilities. This technique\nis particularly well-known for language modeling, a word\nin language being roughly analogous to an event in mu-\nsic. Thecontext ei−1= [e1, e 2, . . . , e i−1]ofanevent eiis\nusually limited to a short sufﬁx [ei−n+1, . . . , e i−1], mean-\ningtheprobabilityofthecurrenteventonlydependsonthe\nn−1previous events. The n-gram counts of the training\ndata are used to estimate the conditional event probabili-\ntiesp(ei|ei−1), and the probability of a new piece eℓis\no\nbtainedbycomputingthejointprobabilityoftheindivid-\nualeventsinthe piece:\np(eℓ) =ℓ/productdisplay\ni=\n1p(ei|ei−1) (1 )\nTo use an n-gram model for music classiﬁcation, for each\nclassaseparatemodelisbuilt,andanewpieceisthensim-\nplyassignedtotheclasswiththehighestpieceprobability.\nFormonophonicmusic, n-grammodelsandmorepow-\nerful extensions are naturally applicable [6,10], but poly-\nphonic music needs ﬁrst to be converted into a sequential\nform. Oneway todothisis tosimplyextracta voice(e.g.,\nViolin1)fromthe polyphonictexture.\nTo reduce the sparseness of the n-gram counts, we do\nnot model the pitch or durationdirectly,but we ﬁrst trans-\nform the music events by means of event features. An\nevent featureassigns a feature-valueto everyevent, in our\ncase to every note in the music piece. The chosen event\nfeature determinesthe level of abstraction of the data rep-\nresentation. The event feature we will use is the melodic\ninterval. Models are constructed for a class by extracting43tr\npitch 747479797879\nmelodicinterval ⊥0+50-1+1\nmelodiccontour ⊥rurdu\ndurationratio ⊥3/21/323/21/3\naveragepitch 77.1667\nrel. freq. m2 0.4\nHuroncontour ascending\nFigure 2. First measures of the ﬁrst violon of the Ada-\ng\nio K.080 of W.A. Mozart, illustrating the contrast be-\ntweenglobalfeatures(lowerthree)andeventfeatures(up-\nperfour).\nthe same voice (e.g., Violin 1) for every piece in a corpus,\nandviewingthatpieceasa sequenceofmelodicintervals.\nFigure 2 illustrates the difference between global fea-\nturesandeventfeaturesonanexcerptofthe ﬁrst violonof\nthe Adagio K.080 of W.A. Mozart. A global feature de-\nscribesa constantpropertyofthe wholepiece, whereasan\neventfeatureis associated to oneparticularnote. A global\nfeature summarizes the data much more, but one uses a\nwhole collection of global features to build a global fea-\nture model,whereas n-grammodelsare constructedusing\nonesingleeventfeature.\n2.4 Classiﬁcationmethodology\nIn this paper,two fundamentallydifferenttypesof models\nare appliedto the task ofcomposerclassiﬁcation. In order\nto presentanycomparativeresults, we have to ﬁnd a com-\nmonwayofevaluatingtheperformanceofthesemodels. It\nis common practice to set up a cross validation scheme to\nobtain classiﬁcation accuracies that generalize reasonably\nwell.\nOurdatasetisverysmallfromageneralmachinelearn-\ning point of view, only 207 samples, it is thereforeprefer-\nable to do leave-one-out cross validation , where one uses\nas much training data as possible to train the model, dis-\ncarding only one instance for testing purposes. For both\nglobal feature and n-gram models, a leave-one-out cross\nvalidationschemewasimplemented.\nSinceglobalfeaturesrepresenteveryinstanceasamul-\ntidimensional feature vector, any standard machine learn-\ning classiﬁer can be applied to get a performance accu-\nracy. SimpleclassiﬁerssuchasNaiveBayesand k-nearest\nneighbourscan give us a goodindication, but in this work\nweoptforthemoresophisticatedSupportVectorMachine,\nshortly SVM, which has been proven to be a state-of-the-\nart classiﬁer [7]. An SVM makes use of a so-called ker-\nnel function to determine non-linear decision boundaries\nbetween classes, and a well-known kernel function, the\n539\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)RBF-kernel, is used in this paper [4]. In this setting, an\nSV\nM hastwo parametersthat need to be trained. The ﬁrst\nisrelatedtothesoftnessofthedecisionmargin,expressing\nthetradeoffbetweengeneralityandclassiﬁcationaccuracy,\ncommonlydenotedas C. Thesecondisaparameter σspe-\nciﬁc to the RBF kernel function. In practice, these param-\neters can simply be tuned by doing a “grid-search” over a\nlargesearch-spaceofpairs (C, σ)asdescribedin [12].\nAnothercommonmachinelearningtechniqueis feature\nselection,whichisoftenusedtoreducethedimensionality\nof the data or to discover which features are highly corre-\nlated with the target class. In principle, feature selection\nis decreasingthe size of the hypothesisspace,which leads\nto a faster and more effective search for the learning al-\ngorithms and tends to avoid overﬁtting. Therefore, it has\nled to improvedclassiﬁcation accuraciesin some cases, or\nto a compact feature set that describes the data in a more\ninterpretable,summarizedway.\nHowever, there is a subtlety in both feature selection\nand SVM parameter optimization, a pitfall to avoid when\noneusessupervised learningmethodsincombinationwith\nacrossvalidationscheme[8,17]. Inthesimplecasewhere\na separate training and test set are given, one has to apply\nsupervised preprocessing methods followed by the learn-\ning algorithm on the training set only, before testing the\nresulting model on the test set. Expanding this to a cross\nvalidationscheme, this meansone must take care to apply\nthese methods within the inner cross validation loop. As\npointed out by [17], it is a common mistake to use both\ntrainingandtestsetforsupervisedfeatureselection,which\nleads to overoptimistic and exaggerated performance re-\nsults.\nInthispaper,SVM*denotesthemodelinwhichparam-\neter tuningwith a gridsearch hasbeendonewithinthe in-\nner loop of the cross validation scheme. Feature selection\nisalso implementedtakingthisspecial consideration.\n3. RESULTS\nIn thissectionwe describethe experimentalresultsforthe\nclassiﬁcationoftheHaydnandMozartstringquartetmove-\nments. As a baseline, we keep in mindthat the classes are\nquite equally distributed (112 Haydn, 95 Mozart), which\nmeans that 54.1% classiﬁcation accuracy can be achieved\nbyalwayspredictingHaydn.\nToevaluatetheglobalfeatureapproach,theSVM*clas-\nsiﬁer method is applied to the Joined set. As described\nabove, this includes an SVM parameter tuning by doing a\ngridsearchwithineachloopoftheleave-one-outcrossval-\nidation. Furthermore,asupervisedfeatureselectionmethod\ncalled Correlation-based Feature Selection (CFS) is also\napplied. CFS is a ﬁlter method aiming to ﬁnd a subset of\nfeatures that are highly correlated with the class but have\nfew intercorrelationamong them [9]. The implementation\nof SVM* and the CFS make use of the Weka machine\nlearningtoolbox[3,20].\nFor the n-gram model, we use a simple trigram model\nof the melodic intervals. For each Haydn and Mozart a\nseparate model is built on the training set and a test pieceVoices SVM* SVM*+feat.sel. 3-grams\nViolin1 74.4 73.4 63.8\nVi\nolin2 66.2 66.2 61.4\nVi\nola 62.8 57.0 61.4\nCe\nllo 65.7 59.4 75.4\nTable 1. The l.o.o. classiﬁcation accuracies of the Joined\ng\nlobal feature set and the trigram model on the separate\nvoicesextractedfromthevoicedstringquartetmovements.\nisassignedtotheclassofwhichthemodelgeneratesitwith\nthe highest probability according to Equation 1. A global\nclassiﬁcation accuracy is also computed with leave-one-\noutcrossvalidation. Theresultsforboththeglobalfeature\nmodels and the trigram models on the separate voices are\nreportedinTable1.\nItappearsimmediatelythattheresultsofpreviouswork\ndone on a smaller database of 107 pieces do not hold up\n[11]. Previously, we noticed a consistent tendency for n-\ngram models to performbetter than global feature models\nregardless of the voice. Now we observe that the n-gram\nmodels perform well on the Cello dataset with an accu-\nracyof75.4%,but poorlyon theothervoices,whereasthe\nglobal feature models achieve an almost equally high ac-\ncuracy of 74.4% on the Violin 1. Our second hypothesis,\nabout the ﬁrst violin being the most predictive one for a\ncomposer, is also weakened because of this surprising re-\nsultwith n-grammodelsontheCello. However,theglobal\nfeatureresultonViolin1isstillanindicationofitspredic-\ntive value. Additionalcomputationof global feature mod-\nels on the separate Alicante, Jesser and McKay sets con-\nﬁrm thisindication,andshowthat wecanorderthe voices\naccordingtotherepredictivenesswithglobalfeaturemod-\nelsasfollows: Violin1,Cello,Violin2 andViola.\nThesecondcolumnofTable 1 is showingthe resultsof\ntheSVM*withCFSfeatureselection. Theseclassiﬁcation\naccuraciesareslightlylowerthanwithoutapplyingfeature\nselection,whichconﬁrmsthatsupervisedfeatureselection\ndoesnotnecessarilyleadtoanimprovementwhenitisap-\npliedintheinnerloopofthecrossvalidationscheme. Nev-\nertheless, it is interesting for musicological reasons to see\nwhich features emerge in the selected feature subsets for\neach voice. Below we give three short examples of fea-\nturesthat areselectedinoneormorevoices.\n•“dmajsec”,i.e. the fractionofmelodicintervalsthat\nare descending major seconds, is selected for both\nViolin 1 and Violin 2. Looking at the relative fre-\nquencies of this feature, it appears that Mozart uses\nmore descending major seconds than Haydn for the\ntwoviolins.\n•“shortestlength”emergesinboththeViolin2andthe\nViola. Thisistheshortestdurationsuchthatalldura-\ntions are a multiple of this shortest duration (except\nfor triplets). Again by looking at the relative distri-\nbutions,onenoticesthatMozarttendstousesmaller\nshortestlengthsin thesevoices.\n540\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)•“ImportanceMiddleRegister” is one of the features\ns\nelected forthe Cello. Thisdenotessimply the frac-\ntion of noteswith MIDI pitchesbetween 55 and 72,\nwhich is roughly the upper range of the instrument.\nItseemsthatHaydnusesthecellomoreofteninthis\nrangethanMozartin thesestringquartets.\n4. CONCLUSIONS ANDFUTUREWORK\nThis paper has applied monophonic classiﬁcation models\nto the task of composer recognition in voiced polyphonic\nmusic,speciﬁcallyHaydnandMozartstringquartetswrit-\nten in the period 1770-1790. An earlier dataset of string\nquartet movements is extended to a total of 207 pieces to\nobtain more statistical signiﬁcance. The voiced structure\nis exploitedby extractingthe separate voicesto enablethe\nuse of monophonic models. Several conclusions emerge:\nthat a simple trigram model of melodic interval performs\nverywell onthe cello,achievingthebest classiﬁcation ac-\ncuracyof75.4%,butisoutperformedbytheglobalfeature\nmodelson the other voices. Therefore,we are also unable\nto conclude that the ﬁrst violin is indeed the most predic-\ntivevoiceforacomposer,eventhoughtheresultsonViolin\n1 areconsistentlybest withtheglobalfeatureapproaches.\nAt ﬁrst sight, these observations are rather disappoint-\ning, but they conﬁrmthe necessity of havinga sufﬁciently\nlarge dataset before making any claims. Learning algo-\nrithms in symbolic music have to cope with this kind of\nchallenge,what showsthere is still roomforimprovement\nona machinelearninglevel.\nCurrently,weareinvestigatingwhatcausesthisremark-\nableresultwiththetrigrammodelonthecelloandthelow\naccuracy on the ﬁrst violin, by looking carefully which\npieces are correctly classiﬁed by one method and not by\nanother, or correctly by both. Perhaps there is a core part\nof this dataset that is ‘easy’ to classify, or else we might\nconsider using an ensemble model where one combines\nthe global feature models and the n-gram models in order\nto improve the overall accuracies. One could also wonder\nhow the so-called Haydn Quartets, six quartets written by\nMozart but famously inspired by and dedicated to Haydn,\ninﬂuencethese results. So far we haveonlyfoundanindi-\ncation that these particular movements are slightly harder\nto recognize,thistopicwill bepartoffurtherresearch.\nFurtherfutureworkwilladdresstheissueofpolyphonic\nmusic in different ways. Figure 3 illustrates the global\nstructure of these future directions. As we detailed ear-\nlier inthispaper,polyphonicmusic canbevoiced,like the\nstring quartets used for this study, or unvoiced, for exam-\nple piano sonatas. Each of these types of polyphony can\nbe modelled by monophonic or polyphonic models. The\nmodels from this work were monophonic models, which\naresituatedintheouterleftbranchofthetree. Polyphonic\nmodelsforvoicedpolyphonycanforexamplebebasedon\npolyphonic global features taking into account voice in-\nformation or harmonic global features, such as those used\nin[15,19]. Toapplymonophonicmodelstounvoicedpoly-\nphonic music, one has to apply some voice extraction al-\ngorithm ﬁrst, for example the skylinemethod [18], which\nPOL\nYPHONIC MUSIC\nVOICED UNVOICED\nMONOPHONIC\nMODELSPOLYPHONIC\nMODELSMONOPHONIC\nMODELSPOLYPHONIC\nMODELS\nFigure 3. Tree structure outlining the possible ways to\na\npproachtheclassiﬁcation ofpolyphonicmusic.\nsimplyslicespolyphonyateachdistinctonsetandtakesthe\nhighest pitch of every slice. The outer right branch of the\ntree is dealing with unvoiced polyphonic music by means\nof polyphonicmodels. One can easily imagine globalfea-\ntures representing this kind of data, for example by com-\nputing relative frequencies of vertical intervals, i.e. in-\ntervals between simultaneous notes. However, building a\ntruly polyphonic n-gram model remains a challenge, as\none has to deal with segmentation and representation is-\nsuestocopewithsparsity.\n5. ACKNOWLEDGEMENTS\nDarrellConklinissupportedby IKERBASQUE ,BasqueFoun-\ndation for Science, Bilbao, Spain. Ruben Hillewaere is\nsupportedbytheprojectMessiaenWeerspiegeldincollab-\noration with the Royal Conservatory of Brussels. Special\nthanksgo to Stijn MeganckandJonatan Taminaufor their\nusefulcommentsandsupportduringthisresearch.\n6. REFERENCES\n[1]http://www.ccarh.org .\n[2]http://jmir.sourceforge.net/\njSymbolic.html .\n[3]http://www.cs.waikato.ac.nz/ml/\nweka/.\n[4] C.M.Bishop. Neuralnetworksforpatternrecognition .\nOxfordUniversityPress, USA, 1995.\n[5] D. Conklin. Melodic analysis with segment classes.\nMachineLearning ,65(2):349–360,2006.\n[6] D. Conklin and I. H. Witten. Multiple viewpoint sys-\ntems for music prediction. Journal of New Music Re-\nsearch,24(1):51–73,1995.\n[7] N. Cristianini and J. Shawe-Taylor. An introduction to\nsupportvectormachinesandotherkernel-basedlearn-\ningmethods .CambridgeUniversityPress, 2000.\n541\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)[8] R. Fiebrink and I. Fujinaga. Feature selection pitfalls\nan\nd music classiﬁcation. In Proceedings of the 7th\nInternational Conference on Music Information Re-\ntrieval,pages340–341,Victoria,Canada,2006.\n[9] M.A.Hall.Correlation-basedfeatureselectionfordis-\ncrete and numericclass machinelearning. In Proceed-\nings of the 17th International Conference on Machine\nLearning,pages359–366,Stanford,USA, 2000.\n[10] R. Hillewaere, B. Manderick, and D. Conklin. Global\nfeature versus event models for folk song classiﬁca-\ntion. InISMIR 2009: 10th International Society for\nMusic Information Retrieval Conference , pages 729–\n733,Kobe,Japan,2009.\n[11] R.Hillewaere,B.Manderick,andD.Conklin.Melodic\nmodels for polyphonicmusic classiﬁcation. In Second\nInternationalWorkshoponMachineLearningandMu-\nsic, Bled,Slovenia,2009.\n[12] C.W.Hsu,C.C.Chang,C.J.Lin,etal.Apracticalguide\ntosupportvectorclassiﬁcation.Technicalreport,2003.\n[13] B. Jesser. Interaktive Melodieanalyse. Peter Lang,\nBern,1991.\n[14] M. Li and R. Sleep. Melody classiﬁcation using a\nsimilarity metric based on Kolmogorovcomplexity.In\nSoundandMusic Computing ,Paris,France,2004.\n[15] C. McKay and I. Fujinaga. Automatic genre classiﬁ-\ncation using large high-level musical feature sets. In\nProceedings of the International Conference on Mu-\nsic Information Retrieval , pages 525–530, Barcelona,\nSpain,2004.\n[16] P. J. Ponce de L´ eon and Jos´ e M. I˜ nesta. Statistical de-\nscription models for melody analysis and characteri-\nzation. In Proceedingsof the 2004InternationalCom-\nputerMusicConference ,pages149–156,Miami,USA,\n2004.\n[17] P. Smialowski, D. Frishman, and S. Kramer. Pit-\nfalls of supervised feature selection. Bioinformatics ,\n26(3):440,2010.\n[18] A.L. Uitdenbogerdand J. Zobel. Matching techniques\nfor large music databases. In Proc. ACM Multimedia\nConference ,pages57–66,Orlando,Florida,1999.\n[19] P.vanKranenburgandE.Backer. Musicalstylerecog-\nnition - a quantitative approach. In Proceedings of the\nConference on Interdisciplinary Musicology (CIM) ,\npages106–107,Graz,Austria,2004.\n[20] I.H. Witten and E. Frank. Data Mining: Practical ma-\nchinelearningtoolsandtechniques.2ndedition .Mor-\nganKaufmann,SanFrancisco,2005.\n542\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "Solving Misheard Lyric Search Queries Using a Probabilistic Model of Speech Sounds.",
        "author": [
            "Hussein Hirjee",
            "Daniel G. Brown 0001"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1414942",
        "url": "https://doi.org/10.5281/zenodo.1414942",
        "ee": "https://zenodo.org/records/1414942/files/HirjeeB10.pdf",
        "abstract": "Music listeners often mishear the lyrics to unfamiliar songs heard from public sources, such as the radio. Since standard text search engines will find few relevant results when they are entered as a query, these misheard lyrics require phonetic pattern matching techniques to identify the song. We introduce a probabilistic model of mishear- ing trained on examples of actual misheard lyrics, and develop a phoneme similarity scoring matrix based on this model. We compare this scoring method to simpler pattern matching algorithms on the task of finding the correct lyric from a collection given a misheard query. The probabilistic method significantly outperforms all other",
        "zenodo_id": 1414942,
        "dblp_key": "conf/ismir/HirjeeB10",
        "keywords": [
            "phonetic pattern matching",
            "phoneme similarity scoring matrix",
            "probabilistic model",
            "misheard lyrics",
            "standard text search engines",
            "correct lyric",
            "collection",
            "query",
            "phonetic pattern matching",
            "pattern matching algorithms"
        ],
        "content": "SOLVING MISHEARD LYRIC SEARCH QUERIES USING A\nPROBABILISTIC MODEL OF SPEECH SOUNDS\nHussein Hirjee Daniel G. Brown\nUniversity of Waterloo\nCheriton School of Computer Science\nfhahirjee, browndgg@uwaterloo.ca\nABSTRACT\nMusic listeners often mishear the lyrics to unfamiliar\nsongs heard from public sources, such as the radio. Since\nstandard text search engines will ﬁnd few relevant results\nwhen they are entered as a query, these misheard lyrics\nrequire phonetic pattern matching techniques to identify\nthe song. We introduce a probabilistic model of mishear-\ning trained on examples of actual misheard lyrics, and\ndevelop a phoneme similarity scoring matrix based on\nthis model. We compare this scoring method to simpler\npattern matching algorithms on the task of ﬁnding the\ncorrect lyric from a collection given a misheard query. The\nprobabilistic method signiﬁcantly outperforms all other\nmethods, ﬁnding 5-8% more correct lyrics within the ﬁrst\nﬁve hits than the previous best method.\n1. INTRODUCTION\nThough most Music Information Research (MIR) work\non music query and song identiﬁcation is driven by audio\nsimilarity methods, users often use lyrics to determine the\nartist and title of a particular song, such as one they have\nheard on the radio. A common problem occurs when the\nlistener either mishears or misremembers the lyrics of the\nsong, resulting in a query that sounds similar to, but is not\nthe same as, the actual words in the song she wants to ﬁnd.\nFurthermore, entering such a misheard lyric query into\na search engine often results in many practically identical\nhits caused by various lyric sites having the exact same ver-\nsions of songs. For example, a Google search for “Don’t\nwalk on guns, burn your friends” (a mishearing of the line\n“Load up on guns and bring your friends” from Nirvana’s\n“Smells Like Teen Spirit”) gets numerous hits to “Shotgun\nBlues” by Guns N’ Roses (Figure 1). A more useful search\nresult would give a ranked list of possible matches to the\ninput query, based on some measure of similarity between\nthe query and text in the songs returned. This goal suggests\na similarity scoring measure for speech sounds: which po-\ntential target lyrics provide the best matches to a misheard\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc⃝2010 International Society for Music Information Retrieval.\nFigure\n1. Search for misheard lyrics from “Smells Like\nTeen Spirit” returning results for Guns N’ Roses.\nlyric query?\nThe misheard lyric phenomenon has been recognized\nfor quite some time. Sylvia Wright coined the autological\nterm “Mondegreen” in a 1954 essay. This name refers to\nthe lyric “They hae slain the Earl O’ Moray / And laid him\non the green,” misheard to include the murder of one Lady\nMondegreen as well [1]. However, the problem has only\nrecently been tackled in the MIR community.\nRing and Uitenbogerd [2] compared different pattern-\nmatching techniques to ﬁnd the correct target lyric in a\ncollection given a misheard lyric query. They found that\na method based on aligning syllable onsets performed the\nbest at this task, but the increase in performance over sim-\npler methods was not statistically signiﬁcant. Xu et al. [3]\ndeveloped an acoustic distance metric based on phoneme\nconfusion errors made by a computer speech recognizer.\nUsing this scoring scheme provided a slight improvement\nover phoneme edit distance; both phonetic methods signif-\nicantly outperformed a standard text search engine.\nIn this paper, we describe a probabilistic model of\nmishearing based on phonetic confusion data derived\nfrom pairs of actual misheard and correct lyrics found\non misheard lyrics websites. For any pair of phonemes\naandb, this model produces a log-odds score giving the\nlikelihood of abeing (mis)heard as b. We replicate Ring\n147\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)and Uitenbogerd’\ns experiments using this model, as well\nas phonetic edit distance as described in Xu et al.’s work,\non misheard lyric queries from the misheard lyrics site\nKissThisGuy.com. Our statistical method signiﬁcantly\noutperforms all other techniques, and ﬁnds up to 8% more\ncorrect lyrics than phonetic edit distance.\n2. RELATED WORK\nRing and Uitenbogerd [2] compared three different\npattern-matching techniques for ﬁnding the correct lyrics\nor matches judged to be relevant given a misheard lyric\nquery. The ﬁrst is a simple Levenshtein edit distance per-\nformed on the unmodiﬁed text of the lyrics. The second,\nEditex, groups classes of similar-sounding letters together\nand does not penalize substitutions of characters within\nthe same class as much as ones not in the same class.\nThe third algorithm is a modiﬁed version of Syllable\nAlignment Pattern Searching they call SAPS-L [4]. In this\nmethod, the text is ﬁrst transcribed phonetically using a\nset of simple text-to-phoneme rules based on the surround-\ning characters of any letter. It is then parsed into sylla-\nbles, with priority given to consonants starting syllables\n(onsets). Pattern matching is performed by local align-\nment where matching syllable onset characters receive a\nscore of +6, mismatching onsets score -2, and other char-\nacters score +1 for matches and -1 for mismatches. On-\nsets paired with non-onset characters score -4, encouraging\nthe algorithm to produce alignments in which syllables are\nmatched before individual phonemes. SAPS is especially\npromising since it is consistent with psychological models\nof word recognition in which segmentation attempts are\nmade at the onsets of strong syllables [5].\nThey found that the phonetic based methods, Editex and\nSAPS-L, did not outperform the simple edit distance for\nﬁnding all lyrics judged by assessors to sound similar to\na given query misheard lyric but SAPS-L most accurately\ndetermined its single correct match. However, due to the\nsize of the test set of misheard lyric queries, they did not\nestablish statistical signiﬁcance for these results.\nIn a similar work, Xu et al. [3] ﬁrst performed an\nanalysis of over 1000 lyric queries from Japanese question\nand answer websites and determined that 19% of these\nqueries contained misheard lyrics. They then developed an\nacoustic distance based on phoneme confusion to model\nthe similarity of misheard lyrics to their correct versions.\nThis metric was built by training a speech recognition\nengine on phonetically balanced Japanese telephone con-\nversations and counting the number of phonemes confused\nfor others by the speech recognizer. They then evaluated\ndifferent search methods to determine the correct lyric in\na corpus of Japanese and English songs given the query\nmisheard lyrics. Phonetic pattern matching methods sig-\nniﬁcantly outperformed Lucene, a standard text search\nengine. However, their acoustic distance metric only\nfound 2-4% more correct lyrics than a simpler phoneme\nedit distance, perhaps due to its basis on machine speech\nrecognition. They also implemented an indexed version of\nthe search which reduced the running time by over 85%with less than 5% loss of accuracy.\n3. METHOD\n3.1 A Scoring Approach\nSimilar to our method for identifying rhymes in rap\nlyrics [6], we use a model inspired by protein homology\ndetection techniques, in which proteins are identiﬁed as\nsequences of amino acids. In the BLOSUM (BLOcks of\namino acid SUbstitution Matrix) local alignment scoring\nscheme, pairs of amino acids are assigned log-odds scores\nbased on the likelihood of their being matched in align-\nments of homologous proteins – those evolving from a\nshared ancestor [7]. In a BLOSUM matrix M, the score\nfor any two amino acids iandj;is calculated as\nM[i; j ] = log2(Pr[i; jjH]=Pr[i; jjR]); (1)\nwhere Pr[i; jjH]is the likelihood of ibeing matched to jin\nan alignment of two homologous proteins, while Pr[i; jjR]\nis the likelihood of them being matched by chance. These\nlikelihoods are based on the co-occurrence frequencies of\namino acids in alignments of proteins known to be homol-\nogous. A positive score indicates a pair is more likely\nto co-occur in proteins with common ancestry; a nega-\ntive score indicates the pair is more likely to co-occur ran-\ndomly. Pairs of proteins with high-scoring aligned regions\nare labeled homologous.\nIn the song lyric domain, we treat lines and phrases as\nsequences of phonemes and develop a model of mishear-\ning to determine the probability of one phoneme sequence\nbeing misheard as another. This requires a pairwise scor-\ning matrix which produces log-odds scores for the likeli-\nhood of pairs of phonemes being confused. The score for\na pair of phonemes iandjis calculated as in Equation\n(1), where Pr[i; jjH]is the likelihood of ibeing heard as\nj, and Pr[i; jjR]is the likelihood of iandjbeing matched\nby chance.\nAs for the proteins that give rise to the BLOSUM\nmatrix, these likelihoods are calculated using frequencies\nof phoneme confusion in actual misheard lyrics. Given a\nphoneme confusion frequency table F, where F i;jis the\nnumber of times iis heard as j(where jmay equal i), the\nmishearing likelihood is calculated as\nPr[i; jjH] = Fi;j=∑\nm∑\nnFm;n: (2)\nThis corresponds to the proportion of phoneme pairs in\nwhich iis heard as j. The match by chance likelihood\nis calculated as\nPr[i; jjR] = Fi\u0002Fj=(∑\nmFm\u0002∑\nnFn); (3)\nwhere F iis the total number of times phoneme iappears\nin the lyrics. This is simply the product of the background\nfrequencies of each phoneme in the pair.\nWe note that our work is in some ways similar to that of\nRistad and Yianilos [8], for learning string edit distance.\n148\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)3.2 T\nraining Data for the Model\nTo produce the phoneme confusion frequency table F, we\nrequire a training set of misheard lyrics aligned to their\ncorrect versions. Our corpus contains query and target\npairs from two user-submitted misheard lyrics websites,\nKissThisGuy.com and AmIRight.com. In both cases, the\nﬁrst phrase in the pair is the song lyric heard by the sub-\nmitter and the second phrase is the true lyric in the song.\nThe KissThisGuy.com pairs were provided by Hu-\nmorBox Entertainment, the parent company of KissThis-\nGuy.com, and consist of 9,527 pairs randomly selected\nfrom the database and comprising 10% of the total num-\nber of misheard lyrics on the website. The pairs from\nAmIRight.com were selected from the pages for the top\n10 artists (by number of misheard lyrics submitted) on the\nsite and total 11,261 pairs, roughly corresponding to 10%\nof the misheard lyrics on the site. The artists included are\nThe Beatles, Michael Jackson, Elton John, Nirvana, Red\nHot Chili Peppers, Queen, Metallica, Madonna, traditional\nsongs, and Green Day.\n3.3 Producing Transcriptions\nWe ﬁrst use the Carnegie Mellon University pronouncing\ndictionary to obtain phonetic transcriptions of the lyrics.\nThe CMU pronouncing dictionary has phonetic transcrip-\ntions for over 100,000 words and is tailored speciﬁcally\nfor North American English, the language used by the\nmajority of artists in our data [9]. We use the Naval\nResearch Laboratory’s text-to-phoneme rules to transcribe\nany words not found in the dictionary [10].\nThe transcriptions contain 39 phonemes, consisting of\n24 consonants, including affricates such as /tS/ and /dZ/,\nand 15 vowels, including diphthongs like /OI/ and /aI/ [11].\nAdditionally, metrical stress is included for the vowels to\nindicate whether they are part of syllables with primary\n(1), secondary (2), or no (0) stress. To avoid overﬁtting\ndue to the relatively small number of secondary stressed\nsyllables in the dictionary, we combine primary and sec-\nondary stresses into strong stress to contrast with weak or\nunstressed syllables. This results in a set of 54 phonemes:\n24 consonants and 30 stress-marked vowels.\nTo better model actual prosody in singing, we reduce\nthe stress in common single-syllable words with less met-\nrical importance such as “a,” “and,” and “the.” To allow\nfor variation in the likelihood of different phonemes be-\ning missed (deleted) or misheard without having been sung\n(inserted), we introduce an additional symbol for gaps in\nalignment and treat it like any other phoneme. This would\nlet a “softer” approximant such as /r/ get a lesser penalty\nwhen missed than a “harder” affricate such as /tS/.\n3.4 Iterated Training\nWe perform an iterated alignment method with the lyric\npairs to determine the confusion frequencies. In the ﬁrst\nphase, phonemes are lined up sequentially starting from\nthe left end of each phrase in the pair. This may seem to be\ntoo rough an alignment method, but it results in the highestfrequencies for identical phoneme pairs since most of the\nmisheard lyrics contain some correct lyrics within them.\nFor example, “a girl with chestnut hair” being misheard\nas “a girl with just no hair” from Leonard Cohen’s “Dress\nRehearsal Rag” would be aligned as\n@ g \"Çl w I T dZ \"2 s t n oU h \"eI r\n@ g \"Çl w I T tS \"E s t n @ t h \"eI r ,\nwith all phonemes matching exactly until the /tS/ heard\nas /dZ/, then the /\"E/ heard as /\"2 /, etc. From these simple\nalignments, we construct an initial phoneme confusion fre-\nquency table F’.\nSince gaps do not appear explicitly in any lyrics, we\napproximate their occurrence by adding gap symbols to the\nshorter phrase in each pair to ensure the phrases are of the\nsame length. In the example above, we would count one\ngap, and have it occurring as an /r/ being missed in the F’\ntable. This approximation results in an essentially random\ninitial distribution of gap likelihood across phonemes.\nNow, given the initial frequency table, we calculate an\ninitial scoring matrix M’ using Equations (1) to (3) above.\nWe then use the scores found in M’ to align the pairs in\nthe second phase of training. In this stage, we use dy-\nnamic programming to produce the optimum global align-\nment between each misheard lyric and its corresponding\ncorrect version, which may include gaps in each sequence.\nWe then trace back through the alignment and update the\nphoneme co-occurrences in a new confusion frequency ta-\nble F. For the example cited above, the new alignment\nwould look like\n@ g \"Çl w I T dZ \"2 s t n oU h \"eI r\n@ g \"Çl w I T tS \"E s t n @ t h \"eI r .\nThe gap occurs earlier and results in a missed /t/ in the F\ntable. After all the pairs have been processed, we calculate\na ﬁnal scoring matrix M from frequency table F, as above.\n3.5 Structure of the Phonetic Confusion Matrix\nOne interesting property of the phonetic confusion matrix\nis that, from ﬁrst principles, we discover perceptual sim-\nilarities between sounds: if two phonemes aandbhave\npositive scores in our confusion matrix, then they sound\nsimilar to the real people who have entered these queries\ninto our database.\nTable 1 shows all of the pairs of distinct consonant\nphonemes aandbsuch that M[a; b] is positive. These\nconsist mainly of changes in voicing (e.g., / g/ versus /k/)\nor moving from a fricative to a plosive ( e.g., /f/ versus /p/);\nthe only distinct consonant pairs scoring above +1.0 are\npairs of sibilants (such as /tS / versus / dZ/ or /Z/ versus / S/).\nAll of these similarities are discovered without knowledge\nof what sounds similar; they are discovered by the training\nprocess itself.\nWhen examining stressed vowel scores in detail, it be-\ncomes evident that vowel height is the least salient articu-\nlatory feature for listeners to determine from sung words,\nas most of the confused vowels differ mainly in height.\nThese pairs include / A/ and /2/, /2/ and / U/, /æ/ and /E /, and\n/E/ and /I/. Other common confusions include vowels dif-\nfering mainly in length and diphthongs confused with their\n149\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Query Phoneme Tar\nget Phoneme\n/b/ /f/,/p/,/v/\n/tS/ /dZ/,/k/,/S/,/t/,/Z/\n/f/ /b/,/p/,/T/\n/g/ /dZ/,/k/\n/dZ/ /tS/,/S/,/y/,/Z/\n/k/ /g/\n/N/ /n/\n/p/ /b/,/f/,/T/,/v/\n/s/ /z/\n/S/ /tS/,/dZ/,/s/,/Z /\n/T/ /f/\n/z/ /s/,/Z/\n/Z/ /dZ/,/S/\nTable\n1. Non-identical consonants with positive scores.\nconstituent phonemes, such as / I/ with /i/, /A/ with /aU/, and\n/O/ with /OU/.\nWhen examining differences in gap scores, we ﬁnd that\nthe phonemes most likely to be missed (deleted) or heard\nwithout being sung (inserted) are /r /, /d/, /N/, and /z/. Al-\nthough the model is trained without any domain knowl-\nedge, a semantic explanation is likely for this ﬁnding since\n/d/ and /z/ are often added to words to form past tenses\nor plurals which could be easily confused. / N/ is often\nchanged to /n/ in verb present progressive tenses in popu-\nlar music; for example, “runnin’ ” could be sung for “run-\nning.” The phonemes least likely to be missed are / Z/, /S/,\n/OI/, and /I/, probably (with the surprising exception of /I/)\ndue to their relative “length” of sound. Similarly, / S/, /U/,\n/I/, and /Ç/ were least likely to be heard without being sung.\n3.6 Searching Method\nTo perform phonetic lyric search with this model, we use\nmatrix M to score semi-local alignments [12] between the\nquery phrase (sequence of phonemes) and all candidate\nsongs in the database. The highest scoring alignment in-\ndicates the actual song lyric most likely to be heard as the\nquery, according to our model.\nIn addition to this phonemic model, we develop a\nsyllable-based model which produces a log-likelihood\nscore for any syllable being (mis)heard as another. For any\npair of syllables aandb, we calculate this score as\nS[a; b] = align(a o; bo) +M[a v; bv] +align (ae; be);(4)\nwhere avis the vowel in syllable aand M[a v; bv]is de-\nﬁned in Equation ??above. align(a o; bo)is the score for\nthe optimal global alignment between the onset consonants\nofaandb, and aeis the ending consonants (or coda) for\nsyllable a.\nSearching and training are performed in the same way\nas with the phonemic method, except that syllables are\naligned instead of phonemes. Essentially, this ensures that\nvowels only match with other vowels and consonants only\nmatch with other consonants.4. EXPERIMENT\nTo compare the performance of the probabilistic model of\nmishearing with other pattern matching techniques, we re-\nproduced the experiment of Ring and Uitenbogerd [2] ﬁnd-\ning the best matches for a query set of misheard lyrics in a\ncollection of full song lyrics containing the correct version\nof each query.\n4.1 Target and Query Sets\nWe used Ring and Uitenbogerd’s collection, comprising\na subset of songs from the lyrics site lyrics.astraweb.com\ncontaining music from a variety of genres by artists such\nas Alicia Keys, Big & Rich, The Dave Matthews Band,\nQueensr ¨yche, and XTC. After removing duplicates, it con-\ntained 2,345 songs with a total of over 486,000 words. This\nformed our set of targets.\nWe augmented their original query set of 50 misheard\nlyrics from AmIRight.com with 96 additional misheard\nlyrics from the KissThisGuy.com data. These additional\nqueries have corresponding correct lyric phrases that\nmatch exactly with a phrase from a single song in the\ncollection. They do not necessarily match the same song\nthe query lyric was misheard from, but only had one\nunique match in the collection. For example, “you have\ngolden eyes” was heard for “you’re as cold as ice” from\nForeigner’s “Cold As Ice,” a song which does not appear\nin the collection. However, the same line occurs in 10cc’s\n“Green Eyed Monster,” which is in the collection. We\nincluded at most one query for each song in the collec-\ntion. In practice, misheard lyric queries may have correct\ncounterparts which appear in multiple songs, potentially\nmaking our results less generalizable for large corpora.\n4.2 Methods Used in Experiments\nWe implemented three different pattern-matching algo-\nrithms in addition to the probabilistic mishearing models\ndescribed above: SAPS-L and simple edit distance as the\nbest methods from Ring and Uitenbogerd’s paper, and\nphonemic edit distance to estimate a comparison with Xu\net al.’s Acoustic Distance. (The actual scoring matrix used\nin that work was unavailable.) We removed all test queries\nfrom the training set for the probabilistic models.\n4.3 Evaluation Metrics\nFor each method, we found the top 10 best matches for\neach misheard lyric in our query set and use these results\nto calculate the Mean Reciprocal Rank (MRR 10) as well as\nthe hit rate by rank for the different methods. The MRR 10\nis the average of the reciprocal ranks across all queries,\nwhere reciprocal rank is one divided by the rank of the\ncorrect lyric if it is in the top ten, and zero otherwise. Thus,\nif the second returned entry is the correct lyric, we score\n0.5 for that query and so on. The hit rate by rank is the\ncumulative percentage of correct lyrics found at each rank\nin the results.\n150\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Pattern\nMatching Method Mean Reciprocal\nRank\nProbabilistic Phoneme Model 0.774\nPhoneme Edit\nDistance 0.709\nProbabilistic Syllable\nModel 0.702\nSAPS-L 0.655\nSimple Edit\nDistance 0.632\nTable\n2. Mean reciprocal rank after ten results for different\nsearch techniques.\n5. RESULTS\nThe probabilistic model of phoneme mishearing signiﬁ-\ncantly outperformed all other methods in the search task,\nachieving an MRR of 0.774 and ranking the correct an-\nswer for 108 of the 146 queries (74.0%) ﬁrst. The next\nbest methods were phonemic edit distance and probabilis-\ntic syllable alignment, receiving MRRs of 0.709 and 0.702,\nrespectively. Performing a paired t-test on the recipro-\ncal rankings of the probabilistic phoneme model and the\nphonemic edit distance returned a p-value less than 0.001,\nstrongly indicating that the results were drawn from dif-\nferent distributions. There was no statistically signiﬁcant\ndifference between the probabilistic syllable model and the\nphonemic edit distance results. Both these methods were\nfound to signiﬁcantly outperform SAPS-L, with p-values\nless than 0.05 on the t-tests. SAPS-L produced an MRR of\n0.655, which was marginally better than the simple edit\ndistance’s MRR of 0.632. However, the difference be-\ntween these two was again not found to be statistically sig-\nniﬁcant. The Mean Reciprocal Rank results are shown in\nTable 2.\nThe hit rate by rank (Figure 2) further illustrates the ef-\nfectiveness of the probabilistic phoneme model as it ranks\nbetween 5% and 8% more correct lyrics within the top ﬁve\nmatches than phonemic edit distance and the probabilistic\nsyllable model. These next two methods appear to perform\nequally well and considerably better than SAPS-L and edit\ndistance. SAPS-L seems to improve in performance over\nsimple edit distance moving down the ranks, indicating\nthat it may be better at ﬁnding less similar matches.\n5.1 Analysis of Errors\nWe also observe that the performance of the probabilistic\nphoneme model plateaus at a hit rate of 83%. This corre-\nsponds to 121 of the 146 misheard lyric queries, and we\nprovide a brief analysis of some of the 25 queries missed.\n5.1.1 Differences Among Methods\nThe phoneme edit distance method did not return any cor-\nrect lyrics not found by the probabilistic phoneme model.\nThe one query for which SAPS-L returned a hit in the top\n10 and the statistical model did not was “spoon aspirator”\nfor “smooth operator,” from Sade’s song of the same name.\nIn SAPS-L, this was transcribed as “SPoon AsPiRaTor,”\ngetting a score of 24 when matched with “Smooth OPeRa-\nTor.” The relatively high number of matching syllable on-\nFigure\n2. Cumulative percentage of correct lyrics found\nby rank for different search methods. The probabilistic\nphoneme model ﬁnds 5-8% more correct targets in the ﬁrst\nﬁve matches than the next best method. The probabilistic\nsyllable model and phoneme edit distance perform nearly\nidentically, and signiﬁcantly better than SAPS-L and sim-\nple edit distance.\nsets (S, P, R, and T) in the short query gave SAPS-L the\nadvantage since it heavily emphasizes onsets. On the other\nhand, the probabilistic method produced higher scores for\nresults such as “spoon in spoon stir(ring)” and “I’m res-\npirating” due to the high number of exactly matching and\nsimilar phonemes.\nThe probabilistic syllable model also returned a hit for\none query for which the phoneme model did not. The mis-\nheard lyric in this case was “picture Mona runnin’ ” heard\nfor “get your motor runnin’ ”, presumably from Steppen-\nwolf’s “Born to be Wild.” This was likely due to the pars-\ning of the phonetic transcription so that paired syllables\nhad high scores at both the onset and ending consonants\n(“Mon” and “mot”, “run” and “run”). The top ranking\nmatch using the phoneme model was “picture on your but-\nton.” When the phrases are transcribed without word or\nsyllable boundaries, the only large differences are an in-\nserted /m/ from “Mona” and a missed /b/ from “button.”\n5.1.2 Common Types of Errors\nThough syllable parsing and alignment may have helped\nfor the two misheard lyrics described above, the majority\nof the queries not returning results tended to be quite dis-\nsimilar from their target correct lyrics. Some examples of\nthese include a young child hearing “ooh, Tzadee, I’m in a\ncheerio” for “we are spirits in the material” from The Po-\nlice’s “Spirits in the Material World;” “Girl, I wanna yo-\ndel” for “You’re The One That I Want” from Grease; “Ap-\nple, dapple, and do” for Prince’s “I Would Die 4 U;” and\n“Swingin’ the bat” for the Bee Gees’ “Stayin’ Alive.” In\nother interesting cases the listener superﬂuously heard the\nsinger’s name within the song lyrics: “Freddie time!” for\n“and turns the tides” in Queen’s My Fairy King, and “Oh,\nLionel (Oh Line’)” for Lionel Richie’s “All Night Long (all\nnight”). Without knowledge of the song artist, it would be\nhard to consider these similar to their originals.\nThe other common problem preventing the algorithms\n151\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Pattern\nMatching Method Correlation\nProbabilistic Phoneme Model 0.45\nPhoneme Edit\nDistance 0.54\nProbabilistic Syllable\nModel 0.55\nSAPS-L 0.53\nSimple Edit\nDistance 0.51\nTable\n3. Correlation between misheard query length and\nreciprocal rank of correct answer returned. The positive\ncorrelations indicate that longer queries are more likely to\nhave the correct lyric ranked higher, though this effect is\nleast pronounced for the probabilistic phoneme model.\nfrom ﬁnding the correct matches for many misheard lyrics\nstems from the short length of such queries. Some of\nthese include “chew the bug” for “jitterbug,” “can of tuna”\nfor “can’t hurt you now,” “rhubarb” for “move out”, and\n“wow thing” for “wild thing.” While these tend to be fairly\nsimilar to their correct counterparts, their short length\nmakes it much easier to ﬁnd exact partial matches which\nscore highly enough to balance the dissimilar remaining\nportions. Though the models are trained on mishearing,\nmost misheard lyrics tend to have parts heard correctly, so\nmatching identical phonemes will usually give the highest\nscores. For all methods, longer queries were more likely\nto have their correct lyrics found in the top 10, resulting in\na positive correlation between the length of the query and\nthe reciprocal rank of the correct result. Table 3 details\nthese correlations for the different algorithms. Note that\nthis correlation is smallest for the probabilistic phoneme\nmodel: it is the least fragile in this way.\n5.2 Running Time\nThe current implementation of the search algorithm is an\nexhaustive dynamic programming search over the entire\ncollection of lyrics, resulting in O( mn) computing com-\nplexity per query, where mis the length of the query and\nnis the size of the collection. This would likely not be\nfeasible in a commercial application due to the long search\ntime required (about 3 seconds per query on a 1.6 GHz\nlaptop). Xu et al. [3] did demonstrate the effectiveness\nof using n-gram indexing to reduce the running time by\npre-computing the distances from 90% of all syllable 3-\ngrams in their collection and pruning off the most dissimi-\nlar lyrics. However, this is simpler with Japanese pronun-\nciation than English due to the relatively limited number\nof possible syllables. Determining the effectiveness of En-\nglish phoneme n-gram indexing while balancing speed, ac-\ncuracy, and memory use remains an open problem.\n6. CONCLUSION\nWe introduce a probabilistic model of mishearing based\non phoneme confusion frequencies calculated from align-\nments of actual misheard lyrics with their correct coun-\nterparts. Using this model’s likelihood scores to perform\nphoneme alignment pattern matching, we were better ableto ﬁnd the correct lyric from a collection given a misheard\nlyric query. Tested on 146 misheard lyric queries with\ncorrect target lyrics in a collection of 2,345 songs, the\nprobabilistic phoneme model produces a Mean Reciprocal\nRank of 0.774 and ﬁnds up to 8% more correct lyrics than\nthe previous best method, phoneme edit distance, which\nachieves an MRR of 0.709.\n7. ACKNOWLEDGEMENTS\nWe thank Eric Barberio, from HumorBox Entertainment,\nfor supplying us with the KissThisGuy.com queries we\nhave used in this study. Our research is supported by\nthe Natural Sciences and Engineering Research Council\nof Canada and by an Early Researcher Award from the\nProvince of Ontario to Daniel Brown.\n8. REFERENCES\n[1] S. Wright: “The Death of Lady Mondegreen,” Harper’s\nMagazine, Vol. 209 No. 1254 pp. 48-51, 1954.\n[2] N. Ring and A. Uitenbogerd: “Finding ’Lucy in Disguise’:\nThe Misheard Lyric Matching Problem,” Proceedings of\nAIRS 2009, pp. 157–167, 2009.\n[3] X. Xu, M. Naito, T. Kato, and H. Kawai: “Robust and Fast\nLyric Search Based on Phonetic Confusion Matrix,” Proceed-\nings ISMIR 2009 , 2009.\n[4] R. Gong and T. Chan: “Syllable Alignment: A Novel Model\nfor Phonetic String Search,” IEICE Transactions on Informa-\ntion and Systems , Vol. 89 No. 1 pp. 332–339, 2006.\n[5] D. Norris, J.M. McQueen, and A. Cutler: “Competition and\nSegmentation in Spoken Word Recognition,” Third Interna-\ntional Conference on Spoken Language Processing, 1994.\n[6] H. Hirjee and D.G. Brown: “Automatic Detection of Internal\nand Imperfect Rhymes in Rap Lyrics,” Proceedings ISMIR\n2009 , 2009.\n[7] S. Henikoff and J.G. Henikoff: “Amino Acid Substitution\nMatrices from Protein Blocks” Proceedings of the NAS,\nVol. 89 No. 22 pp. 10915–10919, 1992.\n[8] E.S. Ristad and P.N. Yianilos: “Learning string-edit dis-\ntance,” IEEE Transactions on Pattern Analysis and Machine\nIntelligence , Vol. 20 No. 5 pp.522-532, 1998.\n[9] K. Lenzo: The CMU Pronouncing Dictionary , 2007 http:\n//www.speech.cs.cmu.edu/cgi-bin/cmudict .\n[10] H.S. Elovitz, R.W. Johnson, A. McHugh, J.E. Shore: “Au-\ntomatic translation of English text to phonetics by means of\nletter-to-sound rules,” Interim Report Naval Research Lab .\nWashington, DC., 1976\n[11] International Phonetic Association: Handbook of the Inter-\nnational Phonetic Association: A Guide to the Use of the In-\nternational Phonetic Alphabet , Cambridge University Press,\n1999.\n[12] R. Durbin, S. Eddy, A. Krogh, G. Mitchison: Biological Se-\nquence Analysis: Probabilistic Models of Proteins and Nu-\ncleic Acids , Cambridge University Press, 1999.\n152\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "Fast vs Slow: Learning Tempo Octaves from User Data.",
        "author": [
            "Jason Hockman",
            "Ichiro Fujinaga"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1416110",
        "url": "https://doi.org/10.5281/zenodo.1416110",
        "ee": "https://zenodo.org/records/1416110/files/HockmanF10.pdf",
        "abstract": "The widespread use of beat- and tempo-tracking methods in music information retrieval tasks has been marginalized due to undesirable sporadic results from these algorithms. While sensorimotor and listening studies have demon- strated the subjectivity and variability inherent to human performance of this task, MIR applications such as rec- ommendation require more reliable output than available from present tempo estimation models. In this paper, we present a initial investigation of tempo assessment based on the simple classification of whether the music is fast or slow. Through three experiments, we provide performance results of our method across two datasets, and demonstrate its usefulness in the pursuit of a reliable global tempo estimation.",
        "zenodo_id": 1416110,
        "dblp_key": "conf/ismir/HockmanF10",
        "keywords": [
            "beat- and tempo-tracking methods",
            "subjectivity and variability",
            "human performance",
            "sensorimotor and listening studies",
            "MIR applications",
            "recommendation",
            "reliable output",
            "tempo estimation",
            "classification",
            "fast or slow"
        ],
        "content": "FAST vs SLOW: LEARNING TEMPO OCTA VES FROM USER DATA\nJason A. Hockman and Ichiro Fujinaga\nCentre for Interdisciplinary Research in Music Media and Technology (CIRMMT)\nDistributed Digital Music Archives and Libraries (DDMAL)\nMusic Technology Area, Schulich School of Music\nMcGill University, Canada\njason.hockman@mail.mcgill.ca, ich@music.mcgill.ca\nABSTRACT\nThe widespread use of beat- and tempo-tracking methods\nin music information retrieval tasks has been marginalized\ndue to undesirable sporadic results from these algorithms.\nWhile sensorimotor and listening studies have demon-\nstrated the subjectivity and variability inherent to human\nperformance of this task, MIR applications such as rec-\nommendation require more reliable output than available\nfrom present tempo estimation models. In this paper, we\npresent a initial investigation of tempo assessment based\non the simple classiﬁcation of whether the music is fastor\nslow. Through three experiments, we provide performance\nresults of our method across two datasets, and demonstrate\nits usefulness in the pursuit of a reliable global tempo\nestimation.\n1. INTRODUCTION\nWithin the last ten years, beat tracking and tempo induction\nmethods have been signiﬁcantly improved. Several state-\nof-the-art methods [1–3] are now capable of identifying\nand providing reliable beat calculations through difﬁcult\npassages marked by features such as expressive timing\nor competing rhythms. However, the usefulness of such\nmethods for information retrieval tasks has been limited\ndue to the unpredictable behavior of these algorithms.\nWhile many studies demonstrate musical beat localization\nfor humans to be variable and highly subjective [4–8],\nMIR applications such as recommendation and harmonic\ndescription require more reliable tempo estimates. The\nmost frequent error in this context is the so-called octave\nerror , or the halving or doubling of the perceived tempo\ncaused by attributing the driving beat level to a metrical\nlevel other than the most predominant pulse.\nIdentiﬁcation of the most appropriate tempo octave has\nbeen shown to be a difﬁcult problem, as demonstrated\nin the discrepancy between beat tracking evaluations in\nwhich a single tempo octave and multiple tempo octaves\nare accepted [2, 3, 9, 16]. As metronomic values are not\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc￿2010 International Society for Music Information Retrieval.absolute, they are not well-suited for deﬁning the perceived\nrelative speed of a piece of music. Unfortunately, if a user\nof a recommendation system were to request slow music\nlabeled 60 BPM, and received music more commonly\nassociated with 120 BPM, they would not be satisﬁed!\nThis paper presents a novel approach to this problem,\nby identifying fast orslow music without the use of\na beat tracker, and demonstrates the usefulness of this\ncategorization in selecting the appropriate tempo octave of\na given piece of music.\n1.1 Background\nThe selection of tempo octave is most commonly achieved\nas an embedded step within the framework of the beat-\nor tempo-tracking task. The general procedure used in\nmost audio tempo-tracking algorithms is comprised of\nthree steps. First, the audio signal undergoes a process\nof reduction, which simpliﬁes the signal by accentuating\nprominent signal information such as transients. Second,\nperiodicity analysis is performed on the simpliﬁed signal,\nto extract possible beat periods (i.e., the duration between\nbeat events). Third, the algorithm identiﬁes which period is\nmost likely, and assigns this value as the tactus , or the most\ninﬂuential beat, which typically controls the local timing of\na musical piece.\nThe majority of recent efforts in beat tracking have\ncentered on this third step, mostly through attempts to\nincorporate musical knowledge. Musical knowledge is,\nin this sense, information of any complexity that is pro-\nvided to the model that allows it to focus on a particular\nsubset of candidates within the wide variety of possible\nsolutions. This knowledge may take on several forms,\nfrom a simple limiting of values to desired candidates,\nto conditional dependencies between metrical levels and\nprior decisions. The need for such knowledge comes from\nthe ambiguity faced in analyzing the output of periodicity\nfunctions of real signals, which may include intra-measure\ntiming variations (e.g., the swing factor in jazz music),\nsyncopation, and/or global tempo shifts. Inspection of the\noutput of periodicity functions during most musical signals\nwill demonstrate several peaks including both octave-\nrelated (e.g., half- or double-time periods) resonances as\nwell as other peaks due to rhythmic complexity and noise;\nthese peaks often overshadow the otherwise steady period.\n231\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Therefore, a selection of the tactus based on output energy\nof a periodicity function alone at each frame will result in\na highly unsteady tempo output for many music sources.\nTo address the tempo octave problem, Goto and Mu-\nraoka [10] limit the possible period values to those periods\nwhose tempi are within only one octave.\nAs an alternative to placing strict boundaries on tempo\nvalues, both Ellis [1] and Klapuri et al. [2] weigh the\noutput of their periodicity functions with log-Gaussian\ndistributions originally proposed by Parncutt [6]. The mo-\ntivation behind this approach is to model tactus preferences\nexhibited during listening tests [5, 6], and it is intended to\nprovide emphasis to tempi positioned around the mean of\nthe distribution.\nDavies and Plumbley [3] use a variable-state method\nthat alternates weighting functions based on the observed\nvariation of the autocorrelation output. The purpose of this\nmethod is to model the uncertainty of the listening process\nupon initial contact with the stimulus, and then to constrain\nthe possible values based on prior observations.\nKlapuri et al. [2] use a hidden Markov model to extract\nthe temporal evolution of a hidden metrical sequence\nexhibited in the output of a comb ﬁlterbank. The joint-\nstate estimates of the present tactus, tatum, and meter\nperiods are achieved through a ﬁrst-order Markov process,\nin which the present ﬁlterbank output and transition proba-\nbilities between periods are used to generate a probabilistic\ndetermination of the present state. Selection of bar-length\nperiodicities and tatum help to reduce incorrect tactus\nattribution. The strength of this model lies in its ability to\nreinforce a metrical framework within sections displaying\nless prevalent metrical observations.\nIn a method conceptually similar to our own, Xiao et al.\n[11] use a Gaussian mixture model to capture the timbral\ncharacteristics of a given tempo through the association of\nMel-frequency cepstral coefﬁcients (MFCCs) to discrete\nBPM values. While this method was demonstrated to\nreduce the occurrence of octave errors for the beat tracker\npresented in [1], its reliance on a discrete BPM values as\nclass labels requires a large amount of ground truth that is\ndifﬁcult to produce due to human subjectivity during data\ncollection.\n1.2 Motivation\nWith the exception of [11], the above methods rely on\nsome form of limiting or weighting curve applied to the\noutput of the periodicity function (e.g., autocorrelation and\ncomb ﬁlterbank) to reduce the effects of alternate tempo\noctaves, but these curves are based on BPM responses\nwhich are highly variable due to the subjectivity of the task.\nWhat can actually be inferred about a piece of music\nfrom a BPM value? Given that humans choose different\nlevels at which to tap when synchronizing with music,\nis it plausible that a BPM measure would provide us\nwith information about the speed of a piece? Certainly\nwithin a single tempo octave the BPM scale can be very\ninformative, but the plurality of acceptable BPM values\nacross tempo octaves makes an inter-octave comparison ofmusical rates less reliable.\nIn addition, other than [11], all above methods rely\nexclusively on periodicity functions and relatively few\nfeatures for determination of BPM and thus tempo octave.\nOur method relies instead on the assumption that the\ndifference between fast and slow music manifests itself\nacross multiple features.\n1.3 Organization of this paper\nSection 2 brieﬂy outlines our technique for the determina-\ntion of a piece of music as fast or slow. Section 3 presents\nboth experimentation and results for our method, as well as\nthe application of our method to tempo-tracking. Section\n4 presents discussion, and Section 5 provides conclusions\nand future work.\n2. METHOD\nTo address the problem of tempo octave estimation, we\npresent a classiﬁcation-based approach that does not rely\non discrete BPM values. Alternatively, the proposed\nmethod performs a binary classiﬁcation using broad cat-\negories of human response to the pace of music: fastand\nslow. There are several beneﬁts to the proposed classiﬁ-\ncation scheme. Unlike solving for a discrete BPM value,\nmusic classiﬁcation as fast or slow is a binary classiﬁcation\nproblem that offers higher accuracy than present multi-\nclass solutions (e.g., discrete BPM values). Evaluation\nmethodology and interpretation is greatly simpliﬁed with-\nout acceptance of multiple metrical levels. In addition,\nground truth—in this case class labels created through\nlistener response to music—is more readily available for\nthis particular problem.\nThe proposed technique has two immediate applica-\ntions: ﬁrst, as a feature within another retrieval task, and\nsecond, as a component within a tempo-tracker that guides\nthe algorithm to the more appropriate of two tempo ranges.\nWhile the taxonomy of fast or slow is not precisely analo-\ngous to a speciﬁc BPM range, we propose that the tempo\nrange can roughly be divided in half to accommodate two\ntempo octaves. With a training set approximately covering\nseveral musical styles in both fast and slow categories, a\nmapping may be achieved between these two taxonomies.\nOur assumption is that labelling a song as slow is indicative\nof the existence of prevalent acoustic characteristics that\nhave led to a selection of the lower tempo octave, while a\nclassiﬁcation of fast is indicative of features that prompted\na rate of synchronization within the faster tempo octave.\n2.1 Data collection\nTo generate our datasets, we created a data harvester1built\non the Last.fm and YouTube APIs. Our initial intention\nwas to extract features and train our classiﬁers based on\naudio for songs that were relevant to the fast and slow tags\non Last.fm. Because audio content is for the most part not\navailable on Last.fm, we opted instead to generate a list of\nartist and track names associated with either fast or slow\n1available at: http://www.music.mcgill.ca/ ∼hockman/other/mashup\n232\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)tags, and use each artist-track combination in this list as\nsearch terms for videos on YouTube.\nAn initial list of artist and track names was created by\nmining Last.fm for the most popular tracks related to the\nquery tags. Additional tracks were then appended to this\nlist through a search for similar tracks that also displayed\nthese tags. If the video matching the query was available,\nan audio track was automatically extracted from the video.\nEach ﬁle was then manually veriﬁed to be a version of the\nartist-track combination. The speciﬁc size and makeup of\nthe dataset varied with the experiment being performed (as\nexplained in Section 3).\n2.2 Feature extraction\nThe success of our classiﬁcation relies chieﬂy on our\nfeature set, which has been generated using jAudio [12], a\nJava-based feature extraction tool from the jMIR software\npackage [13].2\nEach of the tempo estimation methods discussed in\nSection 1.1 generates an onset detection function (also\nknown as a driving signal ) by analyzing either a single\nfeature or relatively few features, and tracks these over the\ncourse of overlapping windows; the aim being to highlight\nsigniﬁcant local signal characteristics, such as fast attack\ntransients, while attenuating steady-state components.\nAlternatively, our approach uses a signiﬁcantly larger\nfeature set, and characterizes features across entire tracks.\nWe suspect that the perception of acoustic cues differs for\nsongs heard as fast and slow, and that these cues are related\nto pitch, loudness, and timbre. We therefore extract a\nlarge number of features in hopes of exploiting regularities\nwithin these three musical attributes. Each audio track\nis ﬁrst converted into a normalized 8 kHz single-channel\n.wav ﬁle. For each audio ﬁle, we assess over 80 overall fea-\ntures, including spectral centroid, rolloff, ﬂux, variability,\npeak-based spectral smoothness, zero crossings, MFCCs,\nLPC, and Method of Moments, along with the aggregates\n[14] of several of these features, e.g., derivative, running\nmean, and standard deviation.\n2.3 Classiﬁcation\nClassiﬁcation is performed using jMIR’s Autonomous\nClassiﬁcation Engine (ACE) software [15]. Provided\nfeature vectors as created in Section 2.2 and a\nclassiﬁcations ﬁle containing a list of labels directly from\nuser data corresponding to each audio track as in Section\n2.1, ACE performs classiﬁcation with a variety of machine\nlearning classiﬁcation algorithms. Our experiments\nfocused on the following six classiﬁers available in ACE:\n•Unweighted k-Nearest Neighbor, with k=1 (k-NN)\n•Support Vector Machines (SVM)\n•Naive Bayes\n•C4.5 Decision Trees (C4.5)\n•AdaBoost seeded with C4.5 (AdaBoost)\n•Bagging seeded with C4.5 (Bagging)\n2available at: http://jmir.sourceforge.net3. EXPERIMENTS\nThe goal for our experiments was to measure how well\nthe above machine learning algorithms can identify fast\nand slow songs. To evaluate our method, we compared\nthe output of several classiﬁers tested on two separate\ndatasets. In all, we conducted three experiments: the ﬁrst\ntwo deal speciﬁcally with identifying the best classiﬁcation\nalgorithm for determining fast or slow tempo, and the third\ncompares our method against an existing tempo-tracking\nalgorithm modiﬁed to output fast or slow values.\n3.1 Experiment 1: Fast vs. slow\nFor the ﬁrst of these experiments, we tested the feasibility\nof our approach using a dataset comprised of audio that\nusers of Last.fm have tagged as fast or slow. The dataset\nwas constructed as explained in Section 2.1, using search\nterms restricted to fast andslow. The total size of this\ndataset was 397 full-length audio tracks, comprised of 109\nfast songs and 288 slow songs. Features were extracted\nas described in Section 2.2. Success rates are based\non averages of ﬁve runs of three-fold cross-validation\nperformed on the dataset with each classiﬁer. Overall\naverages are displayed in Table 1.\nClassiﬁer Avg. Success\nk-NN ( k=1) 97.48\nSVM 99.37\nNaive Bayes 98.24\nC4.5 99.18\nAdaBoost w/ C4.5 99.44\nBagging w/ C4.5 99.12\nTable 1 . 3-fold cross-validation results for Experiment 1.\nValues are presented in percentages for k-NN, SVM, Naive\nBayes, C4.5, AdaBoost, and Bagging classiﬁers.\nThe best performing classiﬁer was AdaBoost, closely\nfollowed by SVMs, C4.5, and Bagging. From the high\nsuccess rates of these learners, we may infer the effective-\nness of training exclusively with global features, as well as\nthe lack of need for a periodicity function.\nWe can identify two weaknesses in our approach for\nthis experiment, both related to genre. First, we did not\nattempt to control the inﬂuence of genre across tempo\nclasses; it is plausible that relatively few genres comprise\na large portion of the dataset, ultimately simplifying the\nclassiﬁcation task to one of basic genre classiﬁcation (e.g.,\nambient vs. punk). Without genre labels we cannot\nreliably isolate the effect of genre from the determination\nof fast or slow music within our dataset.\nSecond, the fast and slow tags may have been made\nwith respect to genre, and we cannot assume the motivation\nbehind the use of these tags. While one listener might use\nthese tags to describe the pace of a piece in relation to other\nmusic of many genres, others might use the same tags to\ndescribe its pace in relation to a speciﬁc genre. This could\npotentially be an issue if the two tag meanings were not\n233\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)consistent. For example, a slower Drum and Bass track\ncould conceivably be tagged as slow within the genre, orfast in comparison with other genres.\n3.2 Experiment 2: Intra-genre fast vs. slow\nFollowing the results of our previous experiment in Sec-\ntion 3.1, we designed an experiment to ensure that theclassiﬁers were not simply classifying genres. For thisexperiment, a new dataset was created. An ideal datasetwould have comprised of fast and slow versions of eachsong, eliminating any differences cause by genre that werenot related to tempo. As we neither have such music, nortags to describe it as fast or slow, we instead used our dataharvester to ﬁnd fast and slow music within each genre.For search tags we ﬁrst looked for tempo-genre pairs in theform of fast xand slow x, where xis a genre taken from\na list of over 1500 genres.\n3For a tempo-genre tag pair\nto be considered as search terms, each tag was required toreturn a tracklist result with no less than ﬁve audio tracksfor each genre. Once the list of tracks was established, theywere downloaded as in the ﬁrst experiment.\nFor this particular search, we found the distribution\nof tracklist results between fast and slow genres highlyunbalanced. Many of the returned tempo-genre pairs (fastxand slow x) had a large number of ﬁles in one category\nand close to the minimum in the other. We thereforeselected the ﬁve most evenly distributed genres (Country,Jazz, Rap, R&B, and Rock). Our desired dataset wascomprised of at least thirty tracks in each tempo-genreclass. As the number of tracks retrieved in each categorydid not meet our expectations, we decided to increasethe size of the dataset by mining YouTube directly usingthe tempo-genre terms as queries for playlists. Our ﬁnaldataset for this experiment was comprised of 831 veriﬁedfull-length audio tracks, as shown in Table 2, and thecomplete list of the songs is available online.\n4\nCountry Jazz Rap RnB Rock Totals\nFast 33 112 63 76 111 395\nSlow 66 103 78 120 69 436\nTotals 99 215 141 196 180 831\nTable 2. Dataset 2 breakdown by genre and tempo class.\nWe then tested our classiﬁcation method within each\nof the ﬁve genres using three-fold cross-validation, as inthe previous experiment. Results in Table 3 demonstratethe capability of each of the ﬁve classiﬁers in this task.Even the worst performer, the naive Bayesian classiﬁer,scored above 93%. The top performers for each of thegenres were either C4.5 or AdaBoost seeded with C4.5.The best classiﬁer across all genres was again AdaBoostseeded with C4.5, and the most difﬁcult genre tested acrosseach classiﬁer was Rap.\nNext, as in Section 3.1 we evaluated each classiﬁer’s\nability to determine fast or slow across the entire dataset,\n3http://en.wikipedia.org/wiki/List ofmusic genres\n4http://www.music.mcgill.ca/∼hockman/projects/fastSlow/dataset.zipGenre k-NN SVM Naive C4.5 Ada Bag\nCntry 94.83 97.26 92.51 98.48 97.95 97.46\nJazz 95.81 98.49 92.78 98.01 99.30 99.07\nRap 90.28 96.98 93.10 98.24 99.29 99.11\nR&B 89.04 95.16 93.98 98.47 98.21 98.08\nRock 92.92 95.71 93.32 99.17 99.28 97.93\nAvg. 92.58 96.72 93.14 98.47 98.80 98.33\nTable 3. 3-fold cross-validation results for intra-genre testsin Experiment 2. Values are presented in percentages fork-NN, SVM, Naive Bayes (Naive), C4.5, AdaBoost (Ada),\nand Bagging (Bag) for each genre: Country (Cntry), Jazz,Rap, R&B, and Rock.\nwithout genre separation. Results for this test are presented\nin Table 4. The top performing classiﬁer was AdaBoost,and success rates were only minimally affected by theabsence of genre speciﬁcation. We can therefore concludethat the classiﬁers were able to learn fast and slow char-acteristics of music without prior knowledge of musicalgenre.\nClassiﬁer Avg. Success\nk-NN (k =1) 95.97\nSVM 96.42\nNaive Bayes 90.94\nC4.5 95.10\nAdaBoost w/ C4.5 96.81\nBagging w/ C4.5 96.45\nTable 4. 3-fold cross-validation results (in percentages)for six classiﬁers tested across entire dataset (i.e., withoutgenre separation) in Experiment 2.\n3.3 Experiment 3: Applications in tempo-tracking\nA third experiment was undertaken to compare the pre-\nsented method to another method capable of fast and slowdetermination. This comparison was achieved using theresults of the top performing classiﬁer from Section 3.2and the binarized output of a beat tracker [16] modiﬁed toprovide a single tempo for each track in the second dataset.For each song n, the beat tracker calculates the derivative\n∆of beats θ\nnand outputs a single BPM value Γnas:\nΓn= 60/median (∆θn). (1)\nAn obstacle in the comparison between the two ap-\nproaches is the selection of a boundary λbetween fast and\nslow BPM values output by the tempo tracker. A plausibleapproach to scoring the output would be to identify a meantempo for the dataset. However, as we lack ground truthBPM values for this dataset, we were unable to generatean average tempo at which to divide the tempo range.We therefore instead tested a set of integer tempo values{50, ..., 150} forλ, deﬁning the optimal divisor as the\ntempo that provided the best results for the tempo tracker.\n234\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Table 5 shows the results of this experiment, with the\nbest performing divisor between fast and slow, λ= 93\nBPM.\nMethod Success Rate\nClassiﬁcation (AdaBoost) 96.81\nTempo tracking, λ= 93 BPM 61.85\nTable 5. Results for Experiment 3 (in percentages).\nResults for the classiﬁer (AdaBoost) were generatedusing 3-fold cross-validation. Tempo tracker output wasbinarized using λ= 93 BPM as a tempo range divisor.\nThe discrepancy between results of the two approaches\nled us to attempt to improve the tempo tracker outputusing a genre-speciﬁc average tempo for each song inthe dataset, as we felt that using ﬁxed BPM value λ\nwas unfairly scoring the tempo tracker. For these values,we used average genre tempi calculated from the BPMList\n5, a hand-annotated database of 20,000 BPM labels\nfor popular Western music listed by genre. Unfortunately,decomposition by genre did not improve results.\nThe success rates for the tempo tracker in this experi-\nment should notbe taken to be indicative of the algorithm’s\noverall performance, as the intention of the tracker is notto deﬁne musical pace as either fast or slow, but ratherto replicate the perceptual phenomenon of synchronizationwith a heard piece of music.\n4. DISCUSSION\nThrough the three experiments performed in Section 3,classiﬁcation of songs as either fast or slow has been shownto be a robust method of determining the overall pace ofmusic. We have achieved above 96% accuracy for twoseparate datasets and demonstrated its effectiveness in thistask over another existing methodology. The high successrate of the presented method suggests its reliability as anindependent feature within several MIR tasks. In additionto using classiﬁcation labels as features themselves, themethod could also be used to improve lower-level metricalanalysis such as tempo-tracking algorithms by selectivelycorrecting misclassiﬁed tempo-tracking octave errors bysimply using the classiﬁcation results.\nOur method differs considerably from existing\napproaches to the problem of tempo octave selection.First, we are currently using only two classes of possibleoutput, as opposed to discretized BPM values. To achievethese class labels, we use machine learning algorithmstrained on global features, calculated by aggregatingwindowed features for each training instance. In addition,we are using a large number of such features to describeeach audio track in our dataset. A key difference that setsour method apart from all existing methods is that noperiodicity calculation is attempted; we instead rely onlyon global features and statistics.\n5http://www.bpmlist.com/The two datasets used in the course of this study\nwere created through the use of Last.fm and YouTubeAPIs, and were speciﬁcally created based on listenerresponses to audio. The composition of generated datasetsis essential to the training of our classiﬁers, as the contentswill deﬁne the ability of our classiﬁers to differentiatebetween the two classes. In review of our ﬁrst experiment,we were concerned that our classiﬁcation results wereartiﬁcially high because our ﬁrst dataset was constructedby downloading tracks associated with fast and slow tags,and that tracks associated with these tags were possiblyleading to a division based on musical genre. We thereforeconstructed a second dataset for the following experiment,which contained examples of fast and slow music withineach genre, reducing the effect of musical genre separa-tion. Results of this experiment demonstrated that theclassiﬁcation approach could not only separate fast andslow music within each genre, but within the entire datasetas well.\nA weakness of this approach lies in the ambiguity of\nresponses to particular pieces of music. For example,songs in certain genres, such as Hip Hop, intentionallyjuxtapose a fast lyrical layer with slower percussion andbass loops (e.g., Bone Thugs’n’Harmony, Twista). Inthese scenarios, a number of listeners tagged some of thesesongs as fast, possibly referring to the unusually fast rateof lyrics, while other listeners tagged tracks in the samestyle as slow, possibly focusing on those characteristicsthat deﬁne the genre standards—namely the percussionand bass lines.\nA second issue is the variable number of annotations per\ntraining ﬁle. On Last.fm, more popular songs are likely tohave more instances of listeners using fast or slow tags,and thus improving tag reliability. In the present study, wehave combined user data from Last.fm with playlist resultsfrom YouTube without regard to the number of listenersagreeing with each tag. While this did not cause difﬁcultyfor our experiments, perhaps an optimal method might beto directly label more music with Last.fm tags or even toperform structured listening tests.\n5. CONCLUSIONS\nWe believe estimation of tempo octaves within music tobe a perceptual phenomenon that can be learned throughuse of the presented classiﬁcation model. In this paperwe have outlined the training of such a model using alarge number of global features related to the overall pitch,timbre, and loudness of an audio track. Through the useof the proposed fast or slow classiﬁcation, we believethat it is possible to improve the usefulness of tempo-tracking models within applications requiring a reliablesingle tempo value.\nIn our future work, we would like to perform further\nevaluation of our method with several datasets of variedcontent. Speciﬁcally, we would like to test our methodusing an artiﬁcial dataset containing fast and slow versionsof songs with the exact same spectral content. Such adataset could be created through the use of any commercial\n235\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)sequencer using MIDI ﬁles to control synthesizer and\nsampler output. Evaluation on signiﬁcantly larger datasetswould also be of interest. A difﬁculty here might lie in thecollection of ground truth for training. Towards this end,listening tests may be useful as an alternative source.\nWe also plan to investigate the applicability of the\nproposed method in the task of beat tracking. An obstaclein this area is that the proposed method deﬁnes entire\nsongs. As we cannot assume that segments of the audiocontain acoustic features that motivated the class labels(i.e., fast or slow) of the entire ﬁle, each segment wouldneed to be classiﬁed independently, which would requiremanually labeled segments for training. Informal tests,however, suggest only a slight decrease in performancewith audio segments of shorter durations, e.g., 10 seconds.\nFinally, we intend to explore alternative strategies for\nincorporating our approach into tempo- and beat-trackingmethods towards improved performance of these algo-rithms.\n6. ACKNOWLEDGEMENT\nThe authors would like to thank the Centre for Inter-disciplinary Research in Music Media and Technology(CIRMMT) for their generous support, M. Davies forproviding source code for his algorithm, and C. McKay, A.Hankinson, and J. Thompson for their technical assistance.\n7. REFERENCES\n[1] D. Ellis: “Beat tracking with dynamic programming,”\nhttp://www.music-ir.org/mirex/2006/mirex/abstracts/2006/TE\nBTellis.pdf (accessed March 1, 2009),\n2006.\n[2] A. Klapuri, A. Eronen, and J. Astola: “Analysis of the\nmeter of acoustic musical signals,” IEEE Transactions\non Speech and Audio Processing, Vol. 14, No. 1,pp. 342–355, 2006.\n[3] M. Davies and M. Plumbley: “Context-dependent\nbeat tracking of musical audio,” IEEE Transactions\non Audio, Speech, and Language Processing, Vol. 15,No. 3, pp. 1009–1020, 2007.\n[4] S. Dixon and W. Goebl: “Pinpointing the beat: Tapping\nto expressive performances,” Proceedings of the 7th\nInternational Conference on Music Perception andCognition, pp. 617–620, 2002.\n[5] P. Fraisse: “Rhythm and tempo,” in The Psychology\nof Music, ed. D. Deutsch, Academic Press, Orlando,Florida, pp. 649–680, 1982.[6] R. Parncutt: “A perceptual model of pulse salience and\nmetrical accent in musical rhythms,” Music Perception,\nVol. 11, No. 4, pp. 409–464, 1994.\n[7] J. Snyder and C. Krumhansl: “Tapping to ragtime:\nCues to pulse ﬁnding,” Music Perception, Vol. 18,\nNo. 4, pp. 455–489, 2001.\n[8] P. Toiviainen and J. Snyder: “Tapping to Bach:\nResonance-based modeling of pulse,” Music Percep-\ntion, Vol. 21, No. 1, pp. 43–80, 2003.\n[9] M. McKinney, D. Moelants, M. Davies, and A.\nKlapuri: “Evaluation of Audio Beat Tracking andMusic Tempo Algorithms,” Journal of New Music\nResearch, Vol. 36, No. 1, pp. 1–16, 2007.\n[10] M. Goto and Y. Muraoka: “A real-time beat tracking\nsystem for audio signals,” Proceedings of the 1995\nInternational Computer Music Conference, pp. 171–174, 1995.\n[11] L. Xiao, A. Tian, W. Li, and J. Zhou: “Using a\nstatistical model to capture the association betweentimbre and perceived tempo,” Proceedings of the\n9th International Conference on Music InformationRetrieval, pp. 659–669, 2006.\n[12] D. McEnnis, C. McKay, I. Fujinaga, and P. Depalle:\n“jAudio: A feature extraction library,” Proceedings of\nthe 6th International Conference on Music InformationRetrieval, pp. 600–603, 2005.\n[13] C. McKay and I. Fujinaga: “jMIR: Tools for\nautomatic music classiﬁcation,” Proceedings of the 6th\nInternational Society for Music Information RetrievalConference, pp. 65–68, 2009.\n[14] D. McEnnis, C. McKay, and I. Fujinaga: “jAudio:\nAdditions and improvements,” Proceedings of the\n7th International Conference on Music InformationRetrieval, pp. 385–386, 2009.\n[15] C. McKay, R. Fiebrink, D. McEnnis, B. Li, and I.\nFujinaga: “ACE: A framework for optimizing musicclassiﬁcation,” Proceedings of the 6th International\nConference on Music Information Retrieval, pp. 42–49, 2005.\n[16] M. Davies, N. Degara, and M. Plumbley: “Evaluation\nmethods for musical audio beat tracking algorithms,”Technical Report C4DM-TR-09-06, Queen MaryUniversity of London, Centre for Digital Music, 2009.\n236\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "Parataxis: Morphological Similarity in Traditional Music.",
        "author": [
            "Andre Holzapfel",
            "Yannis Stylianou"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1416896",
        "url": "https://doi.org/10.5281/zenodo.1416896",
        "ee": "https://zenodo.org/records/1416896/files/HolzapfelS10.pdf",
        "abstract": "In this paper an automatic system for the detection of sim- ilar phrases in music of the Eastern Mediterranean is pro- posed. This music follows a specific structure, which is re- ferred to as parataxis. The proposed system can be applied to audio signals of complex mixtures that contain the lead melody together with instrumental accompaniment. It is shown that including a lead melody estimation into a state- of-the-art system for cover song detection leads to promis- ing results on a dataset of transcribed traditional dances from the island of Crete in Greece. Furthermore, a general framework that includes also rhythmic aspects is proposed. The proposed method represents a simple framework for the support of ethnomusicological studies on related forms of traditional music.",
        "zenodo_id": 1416896,
        "dblp_key": "conf/ismir/HolzapfelS10",
        "keywords": [
            "Automatic system",
            "Similar phrases detection",
            "Eastern Mediterranean music",
            "Parataxis structure",
            "Audio signals",
            "Cover song detection",
            "Lead melody estimation",
            "State-of-the-art system",
            "Rhythmic aspects",
            "Ethnomusicological studies"
        ],
        "content": "PARATAXIS: MORPHOLOGICAL SIMILARITY IN TRADITIONAL\nMUSIC\nAndre Holzapfel\nInstitute of Computer Science, FORTH and\nUniversity of Crete\nhannover@csd.uoc.grYannis Stylianou\nInstitute of Computer Science, FORTH and\nUniversity of Crete\nyannis@csd.uoc.gr\nABSTRACT\nIn this paper an automatic system for the detection of sim-\nilar phrases in music of the Eastern Mediterranean is pro-\nposed. This music follows a speciﬁc structure, which is re-\nferred to as parataxis. The proposed system can be applied\nto audio signals of complex mixtures that contain the lead\nmelody together with instrumental accompaniment. It is\nshown that including a lead melody estimation into a state-\nof-the-art system for cover song detection leads to promis-\ning results on a dataset of transcribed traditional dances\nfrom the island of Crete in Greece. Furthermore, a general\nframework that includes also rhythmic aspects is proposed.\nThe proposed method represents a simple framework for\nthe support of ethnomusicological studies on related forms\nof traditional music.\n1. INTRODUCTION\nIn the ﬁeld of ethnomusicology, computer based methods\nare adequate for simplifying musicological studies. Use-\nful methods can be the recognition of intervals played by\nan instrument, or determining the meter structure of a sig-\nnal. Using such methods, a search engine can be developed\nthat can detect similarities between different pieces. Such a\ntool is valuable for research in ethnomusicology, because it\nenables to get a faster access to pieces that are interesting\nfor a comparison. In this paper, a general framework for\nthe morphological analysis of the Eastern Mediterranean\ntraditional music is proposed and the parts related to melodic\ncharacteristics are presented and evaluated.\nIn general, morphology of music is deﬁned as the me-\nthodical description of the structure of the form of mu-\nsical works [12]. The elements of this organization are\nthemes, phrases and motives, which themselves are made\nup of sound characteristics like tonal height, duration, in-\ntensity and timbre. The analysis aims at the discovery of\nthe sentence structure (Periodenbau) and the transforma-\ntive structure of these elements. This discovery is the core\nof morphological analysis.\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2010 International Society for Music Information Retrieval.Recently, the research presented in Sarris et al. [11] shed\nlight on the difﬁculty of understanding traditional music in\nthe eastern Mediterranean area: to a great extent, it is fol-\nlowing a different kind of morphology, the logic of parataxis.\nThe term parataxis stems from the ﬁeld of linguistics, where\nit denotes a way of forming phrases using short sentences,\nwithout the use of coordinating or subordinating conjunc-\ntions [10]. In music following this logic, the tunes are\nbuilt from small melodic phrases which do not follow a\nspeciﬁc morphologic structure. This means, that there is\nno composed elaboration of a theme like for example in a\nfuga, neither there is a clear periodic structure, according\nto which a musical theme is repeated, like the repeating el-\nement of a chorus in popular western music. As mentioned\nin Theodosopoulou [13], it is a major effort to transcribe\nand analyze a big number of pieces. In this paper, the goal\nis to derive at least some conclusions about the content and\nsimilarity between pieces in an automatic way. Thus, a\nconcept is presented that is aimed to discover recurring el-\nements in a musical signal. These recurring elements are\nthe melodic phrases that are the characteristic themes of the\nmusic following the logic of parataxis. The recognition of\nthese phrases and their assignment to a speciﬁc dance ap-\npears to be a complex task even for a human being. In\ninterviews the author conducted with local musicians, re-\npeatedly the recognition of a dance was connected with\nthe recognition of a speciﬁc melodic phrase. This process\nwas also described in Tsouchlarakis [15]. Also, in listen-\ning test conducted e.g. in [6], it was observed that danc-\ning teachers had memorized almost all melodies they have\nbeen presented with. With this knowledge they were able\nto conduct assignments to a class of dance much faster and\nwith higher accuracy than their students. It is apparent that\nthe similarity estimation between the used motifs is the key\nto a concept for a search engine for this music.\nRecently, similarity in folk song and traditional melodies\nhas drawn increasing attention of the Music Information\nRetrieval research community. Most of the related publi-\ncations investigate symbolic transcriptions of melodies [7,\n14,16]. For audio signals, Moelants et al. [9] and Bozkurt [1]\nderive pitch histograms from monophonic recordings, the\nformer using African music and the latter in the context\nof Turkish music. Both methods are aimed towards the\nrecognition of underlying tonal concepts (i.e. scales or\nmakams, respectively), and stress the importance of a ﬁner\nfrequency solution than the one provided by the chroma\n453\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)features. Cabrera et al. [2] investigate the estimation of\nmelodic similarity on a set of mainly monophonic vocal\nFlamenco recordings.\nIn this paper, the goal is to estimate similarities between\nOSSRHYTHM\nBEAT\nMELODYMELHISTAUDIO\nFigure 1. Block diagram of the proposed morphological\nanalysis system\nlead melodies in polyphonic mixtures. The focus lies upon\nthe melodic aspects of the morphological analysis system\ndepicted in Figure 1. This is achieved by the beat syn-\nchronous computation of melody histograms (MELHIST),\nas detailed in Section 4. The rhythmic aspects, such as\nthe computation of onset strength signals (OSS), rhythmic\nsimilarity and beat tracking on this kind of music has been\nthe subject of investigation in other publications [4, 6]. An\nintegration of rhythmic and melodic similarity as depicted\nin Figure 1 for such a task is meaningful: It has been re-\npeatedly conﬁrmed by local musicians, that not only the\nmelody is of importance for recognizing a speciﬁc dance,\nbut also the way the instrument player puts emphasis on\nparticular notes of the melody. However, the type of avail-\nable data made it necessary to concentrate on the melodic\naspect, as will be detailed in the following Section.\n2. DATASET\nA small dataset of polyphonic samples has been collected\nthat enables for a preliminary evaluation of a system for\nthe detection of morphological similarity. For this, sam-\nples from the Crinno data of the Institute of Mediterranean\nStudies1have been used. In the Crinno collection for\nsome samples of the dance Sousta the lead melodies have\nbeen transcribed by musicologists and then analyzed for\ntheir morphology. All encountered phrases have been in-\ndexed, and using the list of these indexes it is feasible to\nlocate the morphologically identical phrases in different\npieces. The way to index the phrases follows the method\ndescribed in Theodosopoulou [13]: the phrases have a length\nof either one or two bars as shown in Figures 2 and 3.\nBased on Theodosopoulou [13], during the analysis the\nﬁrst encountered two bar phrase will be titled 1a1b. If, for\nexample, the next encountered two bar phrase contains the\nsecond part of the ﬁrst phrase in its second measure, while\nits ﬁrst measure is an unknown phrase, then it will be titled\n2a1b, denoting the partial relation with the ﬁrst pattern. In\nFigures 2 and 3 the titles of the depicted melodic phrases\nare denoted above the score. It is obvious that an exact\n1http://gaia.ims.forth.gr/portal/partial or complete matching can be localized by using this\nway of indexing the phrases. However, no conclusions can\nbe drawn about the similarity of phrases with different ti-\ntles. As the amount of transcribed data is rather small\nFigure 2. Example of a one measure melodic phrase\nFigure 3. Example of a two measure melodic phrase\n(20 pieces), there are not many phrases that appear several\ntimes in various pieces. It has been achieved to compile\na data set of 40sound samples, each containing a com-\nplex musical mixture signal with the instruments Cretan\nlaouto andlyraand sometimes singing voice. Each sample\ncontains several repetitions of melodic phrase of two mea-\nsures length. Each of the 40pieces has a “partner” within\nthe dataset that contains a similar or equal musical phrase\nplayed by the lyra, according to the analysis of musicolo-\ngists. Thus, in this dataset exist 20pairs of samples that\ncontain similar phrases. Please note that according to the\nmusicological analysis these phrases are exactly the same.\nHowever, the audio ﬁles differ because they are performed\nby different artists and vary due to their different playing\nstyle.\n3. MELODIC PATTERN SIMILARITY\nFor the computation of similarity, a baseline system as\npresented in Ellis and Poliner [3] will be used. This sys-\ntem uses beat synchronous chroma features to describe the\nmelodic content. It was proposed for the detection of cov-\nersongs in western pop music, and it will serve as a starting\npoint for the studies of detecting morphological similarity\nin traditional music. The ﬁrst computational step in this\napproach is a beat tracking that uses a spectral ﬂux like\nOSS as an input, and derives the beat time instances us-\ning dynamic programming. Then, for each beat time a 12-\ndimensional chroma feature is computed. These chroma\nfeatures record the intensity associated with each of the\n12 semi-tones of the equal-tempered tonal system. In or-\nder to determine, how well two songs match, the cross-\ncorrelations between two feature matrices are computed\nfor each possible transposition. In the following, this sys-\ntem will be referred to as BASE-SYS.\nAs the sound ﬁles are complex mixtures, melodic similar-\nity is degraded by the other instruments contained in the\nmixture, which play to some extent a similar accompa-\nniment in all examples that is characteristic for this type\nof dance. Thus, a lead melody extraction using a method\nas the one proposed in Klapuri [8] could be included as\n454\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)a pre-processing. Furthermore, instead of using chroma\nfeatures, in the context of traditional music melodic his-\ntograms of a ﬁner resolution have been found useful for\nthe classiﬁcation of melodic content [1]. In order to de-\ntermine if such approaches can be adapted to the beat syn-\nchronous melody description framework, the lead melody\nwill be estimated using the algorithm presented in Kla-\npuri [8], which was provided by the author of the paper.\nThe parameters given as input to the algorithm are the de-\nsired number of fundamental frequency tracks to be esti-\nmated from the signal (set to 1), and the fundamental fre-\nquency range of the desired F0 tracks. This range was set\nto60Hz::: 480Hz , after an analysis of the available scores\nof the recordings. The next step is the computation of\nbeat synchronous melody histograms. Motivated by the\nwork presented in Bozkurt [1], the frequency resolution of\nthese histograms is set higher than necessary for music us-\ning scales of the equal-tempered system. This is because\nin Greek traditional music many modal scales are encoun-\ntered which make use of tonal steps different from the half\ntone of the equal-tempered system. For example, some of\nthese scales have their roots in the scales investigated in\nBozkurt [1]. Scales like Hidzaz andKurdi are examples for\nthis case, and because these scales are also used in Cretan\nmusic the ﬁner resolution of the histograms is theoretically\njustiﬁed. Thus, for a song a matrix is obtained with one\ncolumn for a beat instance which contains the melody his-\ntogram for this beat. Again, for matching two samples the\nmethod proposed in Ellis and Poliner [3] has been used in\nthe same way as for the chroma features. The system that\nuses this kind of melody histograms will be referred to as\nHIST-SYS.\nIn Ellis and Poliner [3], the features are computed beat syn-\nchronous. This means that a beat tracking is necessary as\na pre-processing step. For this purpose, OSS derived from\namplitude are used to perform the beat tracking [3]. How-\never, results described in Holzapfel and Stylianou [5] indi-\ncate that for the investigated type of music a beat tracking\nbased on phase characteristics gives more accurate results.\nThus, it should be evaluated as well if the accuracy of the\nbeat tracking has some impact on the results of the match-\ning experiments.\n4. EVALUATION METHODS\nTwo different evaluation methods are suggested. In the\nﬁrst one, only the 40short samples containing the melodic\nphrases are used to compute their mutual similarity regard-\ning melodic content. The quality of the obtained similarity\nmeasure can be evaluated using the Mean Reciprocal Rank\n(MRR)\nMRR =1\njQjQX\ni=11\nrank i(1)\nwhere jQjis the number of queries. For our data set this\nmeans that each sample is used as a query once, i.e. jQj=\n40. If e.g. the correct partner is found on place 3of the\nmost similar samples, the reciprocal rank is1\n3. This means\nthat the closer the MRR is to the value 1, the better thesimilarity measurement.\nIn the second evaluation method, a sample from the dataset\nis used as a query and similarities are computed for the\nwhole duration of the piece that contains its partner motif\nat some time instance. If this similarity measure shows a\npeak at the position of the true partner, the goal of locating\nit in a continuous piece is achieved.\n5. EXPERIMENTS\n5.1 Setup 1: Matching pairs\nIn the ﬁrst experiment, the BASE-SYS system was applied\nto the data set of 40 song excerpts. Each song was used as\na query and the mean reciprocal rank as deﬁned in (1) was\ncomputed, which resulted in a value of MRR BASE \u0000SY S =\n0:38, as shown in Table 1.\nIn the following we will show that the performance in\nTable 1. Mean reciprocal rates (MRR)\nBASE-SYS 0.38\nHIST-SYS 0.58\nterms of the mean reciprocal rank of the BASE-SYS sys-\ntem can be improved by involving an estimation of the\nmain melody from the polyphonic samples and the usage\nof high resolution histograms in the HIST-SYS system. In\nBozkurt [1], a resolution of one Holdrian comma (Hc) was\nreferred to as the smallest interval considered in Turkish\nmusic theory, and the authors used a resolution of1\n3Hc for\ntheir histograms. One Holdrian comma is equal to 22.6415\ncents, and the octave interval can be divided into 53Hc or\n1200 cents. Various resolutions have been tried, but no\nclear result regarding the optimum value could be obtained\non the limited sized dataset. For that reason, the resolution\nwas set to 2Hc, or about 2:25 times higher than the reso-\nlution of equal-tempered scales (about 4:5Hc). As it can\nbe seen from the second row in Table 1, the obtained mean\nreciprocal rank (0:58) is improved compared to the BASE-\nSYS system. This improvement is present almost indepen-\ndently of the histogram resolution, which indicates that the\nsensitivity to microtonal changes is not of importance at\nleast for the present dataset. We acknowledge, however,\nthat bigger and more diverse datasets have to be obtained\nto achieve more insight into the parameter settings.\n5.2 Setup 2: Matching queries in whole songs\nAs described in Section 4, the second evaluation method is\nusing one of the short samples contained in the dataset as\na query. For this experiment 10phrases of two measures\nlength have been selected as depicted in the ﬁrst column\nof Table 2. For example, the query ﬁle 13b42b:234 is\nthe phrase 13b42b taken from the recording number 234\nin the collection. The target ﬁle is the whole piece which\ncontains the partner of the query at some time instance (i.e.\n455\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Table 2. Results of matching patterns from MS1 in whole song ﬁles\nQUERY FILE max(R neg)Rsource max(R pos)MATCH\n(1) 13b42b:234 0.5796 0.9200 0.6403 EXACT\n(2) 4a31b:217 0.3602 0.9301 0.6741 EXACT\n(3) 3a3b:027 0.5059 0.9297 0.6238 CORRECT\n(4) 35a35b:196 0.5482 0.9416 0.6866 CORRECT\n(5) 3a21b:051 0.4511 0.8549 0.7040 EXACT\n(6) 89a46b:143 0.4881 0.6571 0.5451 EXACT\n(7) 31a31b:035 0.4830 0.8989 0.6351 WRONG\n(8) 6a72a:167 0.5535 0.8778 0.6578 EXACT\n(9) 7a6b:008 0.5073 0.8242 0.5870 EXACT\n(10) 62a62b:249 0.4484 0.8333 0.5869 EXACT\na different interpretation of the same phrase). It has been\ntried to locate the appearance of the phrase in the target ﬁle\nusing the HIST-SYS method, which lead to the best pattern\nmatching results as shown in Table 1. The highest corre-\nlation measures in these ﬁles are depicted in the column\ntitled max(R pos)in Table 2. In the column titled MATCH\nthe success of this matching is judged. If the position con-\nnected to this highest correlation measure is exactly the po-\nsition where the partner ﬁle has been extracted from, then\nthe label EXACT is assigned. If the position of the cor-\nrelation maximum is related to another appearance of the\nsame pattern in the ﬁle, it is labeled as CORRECT. Finally,\nwhen a different pattern from the query pattern is located\nat the position of the correlation peak, the label WRONG is\nassigned. This evaluation has been performed entirely by\nhand, by locating the time instance of the correlation maxi-\nmum of the melody histogram in the related musical score.\nIt can be seen that only in one case the matching gave a\nwrong result, while all the other 9matches were related to\nan appearance of the same melodic phrase in the target ﬁle.\nLet us stress again that all the target ﬁles are different from\nthe ﬁle that the query was taken from: The target ﬁles used\nin the column titled max(R pos)are different recordings\nthan the recordings which contain the query at some time\ninstance. They have been recorded by different players,\nbut they contain at one or more time instances a melodic\nphrase that has been judged to be identical with the query\nby an analysis conducted by musicologists.\nThe correlation between the F0 histogram of a query sam-\nple and the histogram of the whole recording it has been\nextracted from has been computed as well. This means that\nat some time instance exactly the same pattern is encoun-\ntered, without the variation introduced by a different inter-\npretation. This enables to determine how good the match-\ning works in the perfect case, when the pattern we are look-\ning for, is indeed contained in the ﬁle exactly as found in\nthe query. The resulting correlations are depicted in the\ncolumn entitled Rsource in Table 2. It can be seen that the\ncorrelations shown in Rsource are always larger than the\ncorrelation depicted in max(R pos), but never equal to 1.\nThis is likely to be caused by slightly differing beat track-\ning and F0 estimation results on the small query samples\nand on the whole ﬁle.Furthermore, each query has been applied also to a ﬁle,\nwhere according to the annotation the phrase is not con-\ntained neither as a whole nor half of it. The correlation\nmaxima are depicted in the column titled max(R neg), and\nthese values are always smaller than the correlation values\ncomputed in the other columns. This supports the assump-\ntion that the proposed method is able to separate similar\nphrases from those that do not share a large similarity with\nthe query phrase.\nIn Figures 4 and 5, all Rposvectors of the 10 queries shown\nin Table 2 are plotted. These vectors have been obtained by\ncomputing the two dimensional correlations between the\nquery and the target histogram matrices, and the choosing\nthe row (i.e. the tonal transposition between the ﬁles) in\nthe correlation matrix, that contains the maximum value.\nIn all plots, maxima have been chosen and it has be evalu-\nated if at the related measures in the score indeed the query\nphrase is found. For these cases, maxima are shown with\ndashed boxes, while maxima which are not related to the\nquery pattern have been marked with dotted boxes.\nA ﬁrst and important result of this analysis is that in none\nof the cases an occurrence of the query pattern in the in-\nvestigated audio ﬁle has been missed, which means that in\nevery case the occurrence of the pattern was related to a\nmaximum in Rpos. Also the overall number of true posi-\ntives (dashed boxes) is 21while the number of false pos-\nitives (dotted boxes) is only 7. However, these false pos-\nitives do not imply that there is no similarity between the\nquery and the target at the time instance of the false posi-\ntive. The false positive only indicates that at this position\nthe phrase played by the lead instrument does not have\nexactly the same label. Taking a closer look at the false\npositives reveals that for example all wrong detections for\nquery (3) are phrases which contain the pattern 3awhich\nis also contained in the query sample (3a3b). A closer\nlook has been taken at the only case, where the maximum\ninRposis connected to a false positive (query (7)). The\nquery phrase and the phrases found in the dotted boxes in\nFigure 5.(7) are depicted in Figure 6. It is apparent that at\nleast the ﬁrst parts of the two phrases share a big amount\nof similarity. Thus, at least in this case, the false positive is\nrelated to a similar melodic phrase.\nAnother observation from Figures 4 and 5 is that max-\n456\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)(1)\n(2)\n(3)\n(4)\n(5)\nFigure 4. Complete Rposobtained for queries 1-5 in Table\n2, positive matches in dashed boxes, negative matches in\ndotted boxes\nima related to true positives seem to be characterized by\na strong oscillation. This oscillation has been observed to\nhave the frequency of exactly two measures. This means\nthat the correlation shows a strong peak whenever the be-\nginnings of the query phrase and the related phrase in the\ninvestigated ﬁle are aligned. This effect should be further\ninvestigated when a larger dataset is available, and it is pos-\nsible that a detection of such oscillations, besides high cor-\nrelation envelopes, further improves the result of the pat-\ntern retrieval.\nFinally, the impact of the beat tracker has been evaluated.\nIn order to determine how large the change in the match-\ning procedures would be if the beat tracking and hence the\nsynchronization is optimized, all samples in the dataset and\nall complete samples used for the computation of Rposin\nTable 2 have been beat annotated by the author. However,\nrerunning all experiments in the experimental setups 1 and\n2 using these ground truth beat annotations did not quali-\ntatively change the results. Since the original beat tracker\nused in Ellis and Poliner [3] lead mainly to local misalign-\n(6)\n(7)\n(8)\n(9)\n(10)\nFigure 5. Complete Rposobtained for queries 6-10 in Ta-\nble 2, positive matches in dashed boxes, negative matches\nin dotted boxes\nments with the beat annotation, and it has to be concluded\nthat these misalignments have no impact on the systems\nused in this work, at least when applied to the limited size\nof data that is currently available.\n(a)\n(b)\nFigure 6. Two phrases found to be similar in query (7)\n457\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)6. CONCLUSION\nIn this paper, methods have been evaluated that help to de-\ntect melodic similarity in polyphonic recordings following\nthe logic of parataxis. It could be shown that a method\nbased on histograms of the F0 estimation of the leading\nmelody enables for an improvement compared to a base-\nline system that uses chroma features. Furthermore, it could\nbe illustrated that the proposed method is capable of spot-\nting appearances of small melodic patterns in a whole au-\ndio ﬁle, even when both ﬁles are polyphonic mixtures and\nthe query pattern has been derived from a different record-\ning. Such a method can be a valuable tool for research in\nthe ﬁeld of musicology, where similar phrases in a large\ncollection could be located without the necessity of tran-\nscription, thus leading to a large saving of time.\nFurthermore, the integration of melodic and rhythmic as-\npects is straight-forward, and it is likely to improve results\nfor datasets in which different types of rhythms are con-\ntained. As features for melody and for rhythm can both be\ncomputed in a beat synchronous way, the correlation val-\nues obtained for a query from these two aspects could be\nsimply added, or by using some weighting that favors ei-\nther melody or rhythm derived correlations. However, the\nrhythmic similarity measure is quite questionable on the\navailable dataset which is rhythmically very homogeneous,\nand for that reason it had to be postponed. As a future goal,\nthe integration of rhythmic similarity as depicted in Figure\n1 has to be evaluated on a more diverse dataset. However,\nfor the compilation of such a dataset the support of experts\nin musicology is necessary.\n7. REFERENCES\n[1] Baris Bozkurt. An automatic pitch analysis method for\nturkish maqam music. Journal of New Music Research,\n37(1):1–13, 2008.\n[2] Juan J. Cabrera, Jose Miguel D ´ıaz-B ´anez, Francisco J.\nEscobar-Borrego, Emilia G ´omez, Francisco G ´omez,\nand Joaqu ´ın Mora. Comparative melodic analysis of a\ncappella ﬂamenco cantes. In Proceedings of the fourth\nConference on Interdisciplinary Musicology (CIM08),\n2008.\n[3] Dan Ellis and G. Poliner. Identifying cover songs with\nchroma features and dynamic programming beat track-\ning. In Proceedings of the IEEE International Con-\nference on Acoustics, Speech, and Signal Processing.\nICASSP, pages IV–1429–1432, 2007.\n[4] Daniel P. W. Ellis. Beat tracking by dynamic program-\nming. Journal of New Music Research, 36(1):51–60,\n2007.\n[5] Andre Holzapfel and Yannis Stylianou. Beat tracking\nusing group delay based onset detection. In Proc. of IS-\nMIR - International Conference on Music Information\nRetrieval, pages 653–658, 2008.\n[6] Andre Holzapfel and Yannis Stylianou. Scale trans-\nform in rhythmic similarity of music. Accepted for pub-lication in IEEE Transactions on Speech and Audio\nProcessing, 2010.\n[7] Zolt ´an Juh ´asz. Motive identiﬁcation in 22 folksong\ncorpora using dynamic time warping and self organiz-\ning maps. In Proc. of ISMIR - International Conference\non Music Information Retrieval, pages 171–176, 2009.\n[8] Anssi Klapuri. Multiple fundamental frequency esti-\nmation by summing harmonic amplitudes. In Proc. of\nISMIR - International Conference on Music Informa-\ntion Retrieval, pages 216–221, 2006.\n[9] Dirk Moelants, Olmo Cornelis, and Marc Leman. Ex-\nploring african tone scales. In Proc. of ISMIR - Inter-\nnational Conference on Music Information Retrieval,\npages 489–494, 2009.\n[10] Edward P. Morris. On Principles and Methods in Latin\nSyntax. New York, C. Scribner’s sons, 1901.\n[11] Haris Sarris, Tassos Kolydas, and Panagiotis\nTzevelekos. A framework of structure analysis\nfor instrumental folk music. In Proc. of CIM08,\n4th Conference on Interdisciplinary Musicology,\nThessaloniki, Greece, 2008.\n[12] Dimitris Themelis. Morphology and analysis of music,\n(in Greek language). University Studio Press, Thessa-\nloniki, 1994.\n[13] Irini B. Theodosopoulou. Methodology of morpho-\nlogical analysis and analytic data of small rhythmic\npatterns of cretan folk music, (in Greek Language).\nAthens: Kultura, 2004.\n[14] Petri Toiviainen and Tuomas Eerola. Method for com-\nparative analysis of folk music based on musical fea-\nture extraction and neural networks. In In III Interna-\ntional Conference on Cognitive Musicology, pages 41–\n45, 2001.\n[15] Ioannis Tsouchlarakis. The dances of Crete - Legend,\nHistory, Tradition (in Greek language). Center of Cre-\ntan Culture Studies, Athens, 2000.\n[16] Peter van Kranenburg, Anja V olk, Frans Wiering, and\nRemco C. Veltkamp. Musical models for folk-song\nmelody alignment. In Proc. of ISMIR - International\nConference on Music Information Retrieval, pages\n507–512, 2009.\n458\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "Pitch Class Set Categories as Analysis Tools for Degrees of Tonality.",
        "author": [
            "Aline K. Honingh",
            "Rens Bod"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1417533",
        "url": "https://doi.org/10.5281/zenodo.1417533",
        "ee": "https://zenodo.org/records/1417533/files/HoninghB10.pdf",
        "abstract": "This is an explorative paper in which we present a new method for music analysis based on pitch class set cate- gories. It has been shown before that pitch class sets can be divided into six different categories. Each category inher- its a typical character which can “tell” something about the music in which it appears. In this paper we explore the pos- sibilities of using pitch class set categories for 1) classifica- tion in major/minor mode, 2) classification in tonal/atonal music, 3) determination of a degree of tonality, and 4) de- termination of a composer’s period.",
        "zenodo_id": 1417533,
        "dblp_key": "conf/ismir/HoninghB10",
        "keywords": [
            "explorative",
            "method",
            "music",
            "analysis",
            "pitch",
            "class",
            "categories",
            "classification",
            "tonal",
            "atonal"
        ],
        "content": "PITCH CLASS SET CATEGORIES AS ANALYSIS TOOLS FOR DEGREES\nOF\nTONALITY\nAline Honingh RensBod\nInstitute for Logic, Language and Computation\nUniversity of Amsterdam\n/uni007BA.K.Honingh,Rens.Bod/uni007D@uva.nl\nABSTRACT\nThis is an explorative paper in which we present a new\nmethod for music analysis based on pitch class set cate-\ngories. Ithasbeenshownbeforethatpitchclasssetscanbe\ndivided into six different categories. Each category inher-\nitsatypicalcharacterwhichcan“tell”somethingaboutthe\nmusicinwhichitappears. Inthispaperweexplorethepos-\nsibilitiesofusingpitchclasssetcategoriesfor1)classiﬁca-\ntion in major/minor mode, 2) classiﬁcation in tonal/atonal\nmusic, 3) determination of a degree of tonality, and 4) de-\ntermination of a composer’s period.\n1. INTRODUCTION\nIn Western classical music a distinction can be made be-\ntween tonal and atonal music. Tonal music is based on\na diatonic scale which inherits hierarchical pitch relation-\nships. The pitch relationships are based on a key center or\ntonic. In contrast, atonal music is music that lacks a tonal\ncenter or key, and each note is valued in the same way.\nFrom about 1908 onwards atonality has been used in\ncompositions. ComposerssuchasScriabin,Debussy,Bart ´ok,\nHindemith, Prokoﬁev, and Stravinsky have written music\nthat has been described, in full or in part, as atonal.\nIn the same way as there exists music that can be de-\nscribed as partly atonal, one can wonder if, in tonal mu-\nsic, a gradation of tonality can be found. One could argue\nfor example that, within the category of tonal music, mu-\nsic written by Bach is, on average, more tonal than music\nwritten by Debussy. In this paper we will show that it is\npossibletomakedistinctionsintonalityinacomputational\nway.\nThe method that we will use to investigate these gra-\ndations of tonality is based on the notion of pitch class\nsets (hereafter pc-sets). Pc-sets have been used before as\na tool to analyze atonal music [11]. Relations between pc-\nsets, such as transposition and inversion, have been for-\nmalized and even similarity measures have been proposed\n[15,16,20,21,25]. With our method, we propose a new\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnotmadeordistributedforproﬁtorcommercialadvantageandthatcopies\nbear this noticeand thefull citation on theﬁrst page.\nc/uni20DD2010International Society for Music Information Retrieval.approach to analyze music by using pc-sets, that may be\nvaluable by providing statistical information about pieces\nof music. Furthermore, the approach has possible applica-\ntions in several research areas, among others in automat-\nically separating tonal from atonal music, automatically\ndistinguishing music in a major key from music in a mi-\nnor key, ﬁnding degrees of tonality, music classiﬁcation,\nand possibly more.\nModeling tonality has been done in different ways [3,\n26],however,tothebestofourknowledge,noattempthas\nbeenmadetomeasuredegreesoftonality. Styleclassiﬁca-\ntion of music has been investigated using several different\nmethods [4,18,22], ranging from statistical [2,5,6,27] to\nmachine learning [8,13] approaches. It will be worth in-\nvestigating the possibilities of formalizing the degrees of\ntonality as a tool for classiﬁcation of musical style or pe-\nriod. Furthermore, there are, to the best of our knowledge,\nno methods based on pc theory for classiﬁcation of mu-\nsic in major/minor mode and classiﬁcation in tonal/atonal\nmusic.\nTherestofthispaperisorganizedasfollows. Section2\nexplains the notion of pc-set categories and motivates the\ntype of research questions that can possibly be addressed\nwith this tool. In section 3 we will show that the (average)\ncategory distribution for tonal music differs from the (av-\nerage) category distribution for atonal music. In section 4,\nwe will show that the category distribution for music in a\nmajor key differs from the category distribution for music\nin a minor key. Section 5 explores the question of whether\na degree of tonality can be found when investigating cate-\ngorydistributionsofmusicfromdifferentmusicalperiods.\nFinally, section 6 gives concluding remarks.\n2. CATEGORIES OF PITCH CLASS SETS\nA pc-set [10] can represent both a melody and a chord\nsince no distinction is made between notes at different on-\nsettimes. Despitethesesimpliﬁcations,pc-setshaveproven\nto be a useful tool in music analysis [24]. If one exhaus-\ntively lists all pc-sets (351 in total), all possible melodies\nandchordscanﬁtinthislist. Ithasbeenshownthatallpc-\nsets can be grouped into six different categories [14,19].\nThiscanbedonebyapplyingaclusteranalysis[19]tosev-\neral similaritymeasures [15,16,20,21,25] for pc-sets.\nEach pc-set category corresponds to a cycle of one of\nthe six interval classes. This can be understood in the fol-\n459\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)lowing way. A cycle of the interval 1 will read: 0,1,2,3,4,\netc.\nA cycle of the interval 2 will read: 0,2,4,6, etc. A\ncycle of the interval 3 will read: 0,3,6,9, etc., and so on.\nSince we only take into account the ﬁrst six of all twelve\npitchclasses(thelattersixarejusttheinverses),sixdiffer-\nent cycles appear (see Table 1). Every category turns out\nto have its own character resulting from the intervals that\nappearmostfrequently,andsetsofnotesthatbelongtothe\nsame category are ‘similar’in this respect.\nAprototypecanbeidentiﬁedforeachcategory. Ifacer-\ntainpc-setisgroupedintoacertaincategory,thispc-setcan\nbe said to be similar to the prototype of that category. The\nset/uni007B/uni0030/uni002C/uni0031/uni002C/uni0032/uni002C/uni0033/uni002C/uni0034/uni007DistheprototypeoftheIntervalCategory1\n(IC1)inthepentachordclassiﬁcation,theset /uni007B/uni0030/uni002C/uni0032/uni002C/uni0034/uni002C/uni0036/uni002C/uni0038/uni007D\nthe prototype of IC2, and so on. The cycles of IC’s that\nhave periodicities that are less than the cardinality of their\nclass (for example, pc 4 has a periodicity of 3: /uni007B0,4,8/uni007D)\nare extended in the way described by Hanson [12]: the\ncycle is shifted to pc 1 and continued from there. For ex-\nample, the IC-6 cycle proceeds /uni007B/uni0030/uni002C/uni0036/uni002C/uni0031/uni002C/uni0037/uni002C/uni0032/uni002C/uni0038/uni002E/uni002E/uni002E/uni007Dand the\nIC-4 cycle proceeds /uni007B/uni0030/uni002C/uni0034/uni002C/uni0038/uni002C/uni0031/uni002C/uni0035/uni002C/uni0039/uni002C/uni0032/uni002C/uni002E/uni002E/uni002E/uni007D. Thus for every\ncardinality,aseparateprototypecharacterizesthecategory.\nFor example, category IC4 has prototype /uni007B/uni0030/uni002C/uni0034/uni007Dfor sets of\ncardinality 2, prototype /uni007B/uni0030/uni002C/uni0034/uni002C/uni0038/uni007Dfor set of cardinality 3,\nand so on. Table 1 gives an overview of the prototypes of\npc-setcategories. Prototypescanbeenlistedforduochords\nto decachords. Pc-sets with less than 2 notes or more than\n10 notes can not be classiﬁed. This is because one pc-set\nof cardinality 1 exists, /uni007B/uni0030/uni007D, and it belongs equally to ev-\nery category. The same is true for cardinality 11: only one\nprimeformpc-setexists: /uni007B/uni0030/uni002C/uni0031/uni002C/uni0032/uni002C/uni0033/uni002C/uni0034/uni002C/uni0035/uni002C/uni0036/uni002C/uni0037/uni002C/uni0038/uni002C/uni0039/uni002C/uni0031/uni0030/uni007Dand\nbelongs to every category equally. The pc-set of cardinal-\nity 12 contains all possible pitch classes.\n2.1 Music analysis using pc-set categories\nEach category can be seen as having a particular charac-\nterresultingfromtheintervalsthatappearmostfrequently.\nInterval category /uni0031, or category /uni0031for short, consists of all\nsemitones and is the category of the chromatic scale. Cat-\negory/uni0032is the category of the whole-tones or whole-tone\nscale. Category /uni0033is the category of the diminished triads\nordiminishedscale. Category /uni0034isthecategoryoftheaug-\nmented triads or augmented scale. Category /uni0035is the cate-\ngoryofthediatonicscale. Category /uni0036isthecategoryofthe\ntritones or D-type all-combinatorial hexachord (see [12]).\nBecause of the typical character of each of the cate-\ngories, thesecategories can ‘tell’something about themu-\nsic in which they appear. If a piece of music is dominated\nbyaparticularcategory,themusicislikelytobroadcastthe\ncharacter of that category.\nEricksson [9] already argued that music can be divided\nintocategoriessimilartotheonesdescribedaboveandsays\nthat“itisoftenpossibletoshowthatoneregion[category]\ndominatesanentiresectionofapiece”. Ourapproachgoes\nfurther in that we fully formalize and automate these cate-\ngories. When a piece of music is segmented, the category\nof each segment can be calculated and the distribution of\ncategories for that piece can be presented. The categorydistribution of a piece of music can present information\nabout this piece that is possibly new and can lead to new\ninsights on speciﬁc music. Furthermore, this information\nmayleadtomethodsforautomaticdifferentiationofmusic\nin a major key from music in a minor key, automatic clas-\nsiﬁcation of tonal/atonal music, and style classiﬁcation [2,\n5,6,27]. Sinceapc-setcategoryisbydeﬁnitionacategory\nthatconsistsofsimilarpc-sets[14,19],thesecategoriesare\nalso expected to form a useful tool in the research area of\nmusic similarityproblems [1,7,17,21,23,28,29,31].\n2.2 Derivation of category distributions\nThe method has been implemented in Java, using parts of\ntheMusitechFramework[30],andoperatesonMIDIdata.\nThe MIDI ﬁles are segmented at the bar level, as a ﬁrst\nstep to investigate the raw regularities that occur on this\nlevel1. The internal time signature of the music is rec-\nognizedbymethodsoftheMusitechFramework, meaning\nthat, if there is a time signature change, the segmentation\nper bar will correctly continue.\nThe pitches from each segment form a pc-set. From\neachpc-set,theintervalclassvectorcanbecalculatedafter\nwhich the pc-set category can be calculated. This is done\nas follows. Using Rogers’ cos/u1D703 [21] as similarity measure\nwecalculatethesimilaritytoallprototypesoftherequired\ncardinality. The prototype to which the set is most simi-\nlar, represents the category to which the set belongs [14].\nHowever, if the pc-set that is constructed from a bar con-\ntains less than 2 or more than 10 different pitch classes,\nthe set belongs equally to every category, as we explained\nbefore. To overcome this problem, the segmentation is\nchanged as follows. If a set (bar) contains more than 10\ndifferentpitchclasses,thebarisdividedintobeatsandthe\nbeats are treated as new segments. If a set contains less\nthan 2 pitch classes, this set is added to the set that is con-\nstructed from the next bar, forming a new segment. In this\nway, the number of occurrences of the categories can be\nobtained, taking into account all pitches in the MIDI ﬁle.\nThe number of occurrences of all categories can be pre-\nsented as percentages, making comparison to other music\npossible.\n3. TONAL VERSUS ATONAL\nEverycategoryrepresentsaparticularcharacter;thusitcan\nbeexpectedthatdifferenttypesofmusicwillshowadiffer-\nent occurrence rate for each category. Since category 5 is\nthecategoryofthediatonicscale,weexpecttheoccurrence\nrate of category 5 to be high for tonal music. Choosing a\ndata set2of tonal music, the overall category distribution\ncan be calculated as we explained in the previous section.\n1Preliminaryexperimentsshowedthattheresultsfollowingfromseg-\nmentation per beat, bar or two bars vary onlyminimally.\n2Music in MIDI format has been downloaded from\nthe following websites: http://www.kunstderfuge.\ncom/, http://www.classicalarchives.com/,\nhttp://www.classicalmidiconnection.com/,\nhttp://www.musiscope.com/, and http://www.\nclassicalmusicmidipage.com/.\n460\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)(Interval) Category prototypes (pc sets) ‘character’ of category\nIC1 /uni007B/uni0030/uni002C/uni0031/uni007D,/uni007B/uni0030/uni002C/uni0031/uni002C/uni0032/uni007D,/uni007B/uni0030/uni002C/uni0031/uni002C/uni0032/uni002C/uni0033/uni007D, etc. semitones\nIC2 /uni007B/uni0030/uni002C/uni0032/uni007D,/uni007B/uni0030/uni002C/uni0032/uni002C/uni0034/uni007D,/uni007B/uni0030/uni002C/uni0032/uni002C/uni0034/uni002C/uni0036/uni007D, etc. whole-tones\nIC3 /uni007B/uni0030/uni002C/uni0033/uni007D,/uni007B/uni0030/uni002C/uni0033/uni002C/uni0036/uni007D,/uni007B/uni0030/uni002C/uni0033/uni002C/uni0036/uni002C/uni0039/uni007D, etc. diminished triads\nIC4 /uni007B/uni0030/uni002C/uni0034/uni007D,/uni007B/uni0030/uni002C/uni0034/uni002C/uni0038/uni007D,/uni007B/uni0030/uni002C/uni0031/uni002C/uni0034/uni002C/uni0038/uni007D, etc. augmented triads\nIC5 /uni007B/uni0030/uni002C/uni0035/uni007D,/uni007B/uni0030/uni002C/uni0032/uni002C/uni0037/uni007D,/uni007B/uni0030/uni002C/uni0032/uni002C/uni0035/uni002C/uni0037/uni007D, etc. diatonic scale\nIC6 /uni007B/uni0030/uni002C/uni0036/uni007D,/uni007B/uni0030/uni002C/uni0031/uni002C/uni0036/uni007D,/uni007B/uni0030/uni002C/uni0031/uni002C/uni0036/uni002C/uni0037/uni007D, etc. tritones\nTable 1. Prototypes expressed in pc-sets for the six categories. Prime forms have been used to indicate the prototypes (therefore IC5\nmay\nappear differently than one may expect).\ncomposer piece\nBach Brandenburg concerto no. 3\nMozart Piano concerto no. 5 part 1\nBeethoven Piano sonata Pathetique\nBrahms Clarinet quintet part 1\nMahler Symphony no. 4 part 1\nDebussy Nocturnes: Nuages\nTable 2. The tonal music that was used to calculate Table 3.\ncategory number of\noccurrencespercentage of\noccurrencestandard de-\nviation\n1 54 3.22 % 2.09 %\n2 83 4.96 % 5.96 %\n3 321 19.16 % 8.48 %\n4 247 14.75 % 8.33 %\n5 890 53.13 % 19.21 %\n6 80 4.78 % 2.50 %\nTable3. DistributionofcategoriesintonalmusiclistedinTable\n2.\nT\nable 2 lists the tonal music that has been used for this\nexperiment and Table 3 gives the percentages of occur-\nrences of the categories that are found in this corpus. We\nsee from Table 3 that the music is dominated by category\n5. We indeed expected a high occurrence rate of category\n5, as this is the category that represents the diatonic scale.\nHowever, since the standard deviation is relatively high,\nthe individual percentages vary quite a bit.\nFor atonal music, we expect a different behavior. We\nhave run the program on strict atonal music composed by\nSchoenberg, Webern, Stravinsky and Boulez. The com-\nplete list of music is shown in Table 4. On average, the\ndistribution as shown in Table 5 was found, using this cor-\npus of atonal music. We can see that the music is not\ndominated anymore by category /uni0035but a much more equal\ndistributionispresentinatonalmusic. Fromthedifference\nin these category distributions, it seems that especially the\noccurrence of category 5 could contribute to classiﬁcation\nmethods to separate atonal from tonal music. Cross vali-\ndation needs to be performed in order to verify this claim.\nHowever,sincewecouldﬁndonlyfewMIDIdataofatonal\nmusic(allMIDIdatawefoundonatonalmusicisgivenin\nTable 4), it is difﬁcult to perform a cross validation with\nenough data.composer piece\nSchoenberg Pierrot Lunaire part 1, 5, 8, 10, 12, 14, 17,\n21\nSchoenberg Piece for piano opus 33\nSchoenberg Six little piano pieces opus 19 part 2, 3, 4,\n5,\n6\nWebern Symphony opus 21 part 1\nWebern StringQuartet opus 28\nBoulez Notations part 1\nBoulez Piano sonata no 3, part 2: “Texte”\nBoulez Piano sonata no 3, part 3: “Parenthese”\nStravinsky in memoriam Dylan Thomas Dirge canons\n(prelude)\nTable 4. The atonal music that was used to calculate Table 5.\ncategory number of\noccurrencespercentage of\noccurrencestandard de-\nviation\n1 313 28.25 % 10.56 %\n2 117 10.56 % 6.14 %\n3 166 14.98 % 7.68 %\n4 179 16.16 % 7.97 %\n5 138 12.45 % 7.15 %\n6 195 17.60 % 6.20 %\nTable 5. Distrib ution of categories from music of Schoenberg,\nWebern, Stravinsky and Boulez.\n4. MAJOR VERSUS MINOR\nThe tonality turns out not to be the only factor to inﬂu-\nence the percentage of occurrence of category 5 in music.\nIf we focus on tonal music, an obvious difference can be\nmeasured in the occurrence of category 5 between music\nin major and in minor mode. To show this behavior, we\nhave chosen Bach’s Well-tempered Clavier book I as test\ncorpus and divided the corpus in two parts: 1) the pieces\nin a major key, and 2) the pieces in a minor key. From\nTable 6 we can see the differences in category distribution\nbetweenthetwopartsandtheoverallcorpus. Thepiecesin\nmajor mode have an average percentage of occurrence of\ncategory 5of 79.81 %, whilefor thepieces inminor mode\nthispercentageisconsiderablylower,namely53.58%(see\nTable6). Asonecansee,thestandarddeviationsinTable6\nforthemusicseparatedinmajorandminoraresmallerthan\nforallpiecestogether,whichmeansthatthemeasurements\nare distributed closer around their mean. We can now un-\nderstand that the standard deviation in Table 3 was rela-\ntivelylargesincethedatacontainedbothdatainmajorand\nin minor mode. Reconsidering the results of the previous\nsection,theaveragepercentageofoccurrenceofcategory5\n461\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)category occurrences for\npieces\ninmajorstandard de-\nviationoccurrences for\npieces\ninminorstandard de-\nviationoccurrences for\nallpiecesstandard de-\nviation\n1 (31) 2.30 % 1.03 % (85) 4.48 % 2.22 % (116) 3.57 % 2.12 %\n2 (25) 1.86 % 1.69 % (98) 5.16 % 2.29 % (123) 3.79 % 2.63 %\n3 (149) 11.06 % 3.23 % (453) 23.87 % 3.72 % (602) 18.55 % 7.23 %\n4 (47) 3.49 % 3.22 % (199) 10.48 % 3.73 % (246) 7.58 % 4.93 %\n5 (1075) 79.81 % 6.52 % (1017) 53.58 % 3.63 % (2092) 64.47 % 13.87 %\n6 (20) 1.48 % 1.24 % (46) 2.42 % 1.75 % (66) 2.03 % 1.63 %\nTable 6. Distribution of categories in percentages (the number in given between brackets) from the pieces in major mode, and minor\nmode,\nand all pieces together, from Bach’sWell-tempered Clavier book I.\ncomposer percentage of oc-\ncurrence\nof cate-\ngory 5 for major\nmodestandard de-\nviationpercentage of oc-\ncurrence\nof cate-\ngory 5 for minor\nmodestandard de-\nviation\nPalestrina 71.94 %/uni26054.55 %\nBach 85.71 % 3.99 % 57.72 % 11.02 %\nMozart 58.17 % 7.75 % 37.94 % 6.04 %\nBeethoven 47.98 % 6.62 % 36.09 % 7.66 %\nBrahms 40.79 % 1.42 % 40.11 % 4.55 %\nMahler 53.83 % 18.33 % 35.95 % 9.24 %\nDebussy 68.01 % 5.24 % 40.57 % 9.23 %\nStravinsky 27.65 %/uni26053.83 %\nTable7. Thepercentageofoccurrenceofcategory5forseveralcomposers,separatingmusicinamajorandminorkey./uni2605F orPalestrina\nand Stravinsky, the separation between music in major and minor mode has not been made (see text for details).\nin the musicin minor mode of Table 6 is stillconsiderably\nhigher than the average percentage of occurrence of cate-\ngory 5 in atonal music, where one cannot speak of major\norminormode. Moreover,thetonaldatafromtheprevious\nsection contained nearly as much music in major mode as\nmusic in minor mode. We have to remark, however, that\ningeneral, manypiecesofmusiccontainsegmentsinboth\nmajorandinminormode,althoughanoverallpieceissaid\nto be in either major or minor. In our method, we have\nclassiﬁed the pieces of music only in a global way (based\non the mode of the overall piece), motivated by the con-\nsensusthatapieceofmusicinaspeciﬁcmodewillusually\ncontain a majority of segments that are in that mode.\nItisunderstandablethatmusicinminormodeexhibitsa\nlower percentage of category 5 than music in major mode,\nforthereasonthatcategory5isthecategoryofthediatonic\n(major)scale. Althoughthenaturalminorscaleisdiatonic\nas well, in music in a minor key, other variants like the\nmelodicandharmonicminorscalearefrequentlyusedtoo.\nFor music in minor mode, apart from a high percentage\nof category 5, categories 3 and to a lesser extent category\n4 represent relatively high percentages as well. In con-\ntrast, the atonal music has also relatively high percentages\nofcategories1,2and6. Theraisedpercentageofcategory\n3 for tonal music in minor mode may be explained from\nthe presence of the minor third, and the raised percentage\nof category 4 fromthe presence of the minor sixth.\nThe example in this section shows that for a particular\ntype of music, measuring the percentage of occurrence of\ncategory 5, would enable to make a distinction between\nmusic in major and in minor mode.5. DEGREE OF TONALITY?\nIn the previous sections, we have seen that of all cate-\ngories, especially category 5 can give some information\nabout both the tonality and the mode. It may be worth to\nfocus on this category for speciﬁc composers and to study\nthe difference between them. In Table 7 the percentage of\noccurrence of category 5 is shown for several composers.\nThe composers are ordered chronologically. For two com-\nposers, Palestrina and Stravinsky, no separation is made\nbetweenmajorandminormode. AlotofworkbyStravin-\nsky is difﬁcult to be labeled as completely major or minor,\nand some of his later works can even be labeled as atonal.\nForPalestrina,noseparationbetweenmajorandminorhas\nbeen made, since in Renaissance music, besides the nor-\nmal major and minor scales, eight church modes are used\nas well. For each composer and for each mode (major or\nminor), on average 5 pieces of music have been selected,\nsuch as to form a representative sample that contains the\ndifferentmusicalforms(symphonies,chambermusic,con-\ncertos, etc.) present in the repertoire of the composer.\nBasedontheresultfromsection3statingthattonalmu-\nsic contains a higher percentage of category 5, we might\nexpectadecreasingpercentageofcategory5whenthecom-\nposersareorderedchronologically. Forexample,onemight\nlabelBachasmoretonalthanforexampleMahlersincethe\nlatter composer would be closer in time to the contempo-\nrary period in which the atonal music ﬂourished. This hy-\npothesis turned out not to be true however. It is indeed the\ncasethatBachembodiesahigherpercentageofcategory5\nthan Mahler, but if we focus on the composers for whom\na distinction was made between major and minor music,\nweseeadecreasingpercentageofcategory5fromBachto\n462\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)0 1 2 3 4 5 6 72030405060708090\nComposers in chronological orderpercentage of occurrence of category 5\n  \nmajor\nminor\nBach Mozart Brahms Mahler Debussy Beethoven\nFigure1. The percentageofoccurrenceofcategory5formostcomposersfromTable7,inchronologicalorder. Theerrorbarsrepresent\nthe standard deviation from Table 7.\nperiod percentage of oc-\ncurrence\nof cate-\ngory 5standard de-\nviationpercentage of oc-\ncurrence\nof cate-\ngory 5standard de-\nviation\nBaroque Bach H¨andel\n85.71 % 3.99 % 77.40 % 4.64 %\nRomantic Beethoven Schubert\n47.98 % 6.62 % 45.94 % 7.36 %\nImpressionist Debussy Ravel\n68.01 % 5.24 % 40.96 % 14.14 %\nTable 8. The percentage of occurrence of category 5 for different composers from the same musical period.\nBrahms\n(focusing on major mode), although fromBrahms\nto Debussy the percentage of category 5 increases again,\nsee Figure 1.\nBased on the result from section 4 we expect higher\npercentages of category 5 for major music than for minor\nmusic for each composer. Indeed, this turns out to be the\ncase(seeTable7),althoughthedifferenceisverysmallfor\nBrahms.\nOne could now wonder whether the results of Table 7\nshowageneralbehaviorthatistypicalforcomposersfrom\ndifferent musical periods from Renaissance to modern, or\nthat the results are just speciﬁc for these composers. We\nstudy the differences between composers who lived in the\nsame period, since this might explain the results of Ta-\nble 7 a bit further. We have zoomed in on music in ma-\njor mode in three different musical periods, namely the\nBaroque,RomanticandImpressionistperiods(seeTable8)\nandlookedatthedifferencebetweentwocomposerswithin\nthesameperiod. OnecanseethatfortheBaroqueandRo-\nmanticperiod,thepercentagesofcategory5areverymuch\nalike for the two composers chosen. However, for the Im-\npressionist period there is a substantial difference between\nthe percentages.\nTheﬁndingthateachcomposerrepresentsatypicalper-\ncentage of occurrence of category 5 can possibly be used\nin applications for style recognition.6. CONCLUDING REMARKS\nIn this paper, a new analysis method for music has been\nproposed, and we explored a number of possible applica-\ntions. We showed that the six pc-set categories [14,19]\ncan reveal speciﬁc information about music. When mu-\nsic is segmented, and when for each bar is calculated to\nwhich category its pc contents belongs, the percentages of\nthe different categories can reveal information about the\ntonality of the piece (tonal or atonal) and the mode of the\npiece (major or minor). In particular, category 5, which\nrepresents the major diatonic scale, is indicative of this in-\nformation.\nAlthough on the basis of the percentage of occurrence\nofcategory5,aseparationbetweentonalandatonalmusic\nmaybemade,itdoesnotallowustoorderspeciﬁcmusicin\ntime. Moreresearchneedstobedonetobeabletoexplore\nwhether the percentage of occurrence of category 5 can be\nindicative of a certain styleor musical period.\nWefullyrecognizethatcrossvalidationexperimentsneed\nto be carried out in order to verify the suggested possibil-\nityofusingpc-setcategoriesforthepurposeoftonal/atonal\nclassiﬁcationandmajor/minorclassiﬁcation,butthisisham-\npered so far by a lack of MIDI data especially for atonal\nmusic. Furthermore, in future research we hope to be able\ntoperformanactualclassiﬁcationtask. Sincethetonal/atonal\nclassiﬁcation and the major/minor classiﬁcation both de-\n463\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)pendonthepercentageofoccurrenceofpc-setcategory5,\nthis\nwill not be a trivial task.\nTo conclude, the distribution of pc-set categories can\nreveal information about music on different levels, and we\nsuggestthattheycanserveasanewtoolinmusicanalysis.\n7. ACKNOWLEDGEMENTS\nThe authors wish to thank Darrell Conklin and Tillman\nWeydeandthreeanonymousreviewersforconstructivecom-\nmentsandfeedback. Thisresearchwassupportedbygrant\n277-70-006 of the Netherlands Foundation for Scientiﬁc\nResearch (NWO).\n8. REFERENCES\n[1] Chantal Buteau. Automatic motivic analysis including\nmelodic similarity for different contour cardinalities: appli-\ncation to Schumann’s of foreign lands and people. In Pro-\nceedings of the International Computer Music Conference,\nBarcelona, 2005.\n[2] Wei Chai and Barry Vercoe. Folk music classiﬁcation using\nhiddenmarkovmodels.In ProceedingsofInternationalCon-\nference on Artiﬁcial Intelligence, 2001.\n[3] ElaineChew. Towardsamathematicalmodeloftonality.PhD\nthesis, Operations Research Center, Massachusetts Institute\nof Technology, Cambridge, 2000.\n[4] Elaine Chew, Anja Volk, and Chia-Ying Lee. Dance music\nclassiﬁcation using inner metric analysis: a computational\napproach and case study using 101 latin american dances\nand national anthems. In Proceedings of the 9th INFORMS\nComputing Society Conference, volume 29, pages 355–370.\nSpringer OR/CSInterfaces Series, 2005.\n[5] Rudi Cilibrasi, Paul Vitanyi, and Ronald de Wolf. Algorith-\nmic clustering of music based on string compression. Com-\nputer Music Journal, 28(4):49–67, 2004.\n[6] Roger Dannenberg, Belinda Thom, and David Watson. A\nmachine-learning approach to musical style recognition. In\nProceedingsofthe1997InternationalComputerMusicCon-\nference, pages 344–347, San Francisco, 1997.\n[7] Ir`ene Deli`ege. Introduction : Similarity perception, catego-\nrization, cue abstraction. Music Perception, 18(3):233–243,\n2001.\n[8] Shlomo Dubnov, Gerard Assayag, Olivier Lartillot, and Gill\nBejerano. Using machine-learning methods for musical style\nmodeling. Computer, 36(10):73–80, 2003.\n[9] Tore Ericksson. The ic max point structure, mm vectors and\nregions.Journal of Music Theory, 30(1):95–111, 1986.\n[10] AllenForte. TheStructureofAtonalMusic.NewHaven: Yale\nUniversity Press, 1973.\n[11] Allen Forte. Pitch-class set analysis today. Music Analysis ,\n4(1/2):29–58, 1985.\n[12] HowardHanson. HarmonicMaterialsofModernMusic.New\nYork: Appleton-Century-Crofts, 1960.\n[13] Ruben Hillewaere, Bernard Manderick, and Darrell Conklin.\nGlobal feature versus event models for folk song classiﬁca-\ntion. InISMIR 2009:10th International Society for Music In-\nformation Retrieval Conference, Kobe, Japan, 2009.[14] Aline K. Honingh, Tillman Weyde, and Darrell Conklin. Se-\nquential association rules in atonal music. In Proceedings of\nMathematics and Computation in Music (MCM2009) , New\nHaven, USA, June 19-22, 2009.\n[15] Eric J. Isaacson. Similarity of interval-class content between\npitch-class sets: the IcVSIM relation. Journal of Music The-\nory, 34:1–28, 1990.\n[16] Robert Morris. A similarity index for pitch-class sets. Per-\nspectives of New Music, 18:445–460, 1980.\n[17] Robert Morris. Equivalence and similarity in pitch and their\ninteraction with pcset theory. Journal of Music Theory,\n39(2):207–243, 1995.\n[18] Carlos Perez-Sancho, David Rizo, and Jose Inesta. Genre\nclassiﬁcation using chords and stochastic language models.\nConnection Science, 21(2-3):145–159, 2009.\n[19] Ian Quinn. Listening to similarity relations. Perspectives of\nNew Music, 39:108–158, 2001.\n[20] John Rahn. Relating sets. Perspectives of New Music,\n18:483–498, 1980.\n[21] David W. Rogers. A geometric approach to pcset similarity.\nPerspectives of NewMusic, 37(1):77–90, 1999.\n[22] Adi Ruppin and Hezy Yeshurun. Midi music genre classiﬁ-\ncation by invariant features. In Proceedings of the 7th Inter-\nnational Conference on Music Information Retrieval, pages\n397–399, Canada, 2006.\n[23] Art Samplaski. Mapping the geometries of pitch-class set\nsimilaritymeasuresviamultidimensionalscaling. MusicThe-\nory Online, 11(2), 2005.\n[24] Michiel Schuijer. Atonal music: Pitch-Class Set Theory and\nIts Contexts. University of Rochester Press,2008.\n[25] DamonScottandEricJ.Isaacson.Theintervalangle: Asimi-\nlaritymeasureforpitch-classsets. PerspectivesofNewMusic,\n36(2):107–142, 1998.\n[26] David Temperley. The tonal properties of pitch-class sets:\nTonal implication, tonal ambiguity and tonalness. Tonal The-\noryfortheDigitalAge,ComputinginMusicology,15:24–38,\n2007.\n[27] George Tzanetakis and Perry Cook. Music genre classiﬁca-\ntion of audio signals. IEEE Transactions on Speech and Au-\ndio Processing,10(5):293–302, 2002.\n[28] AnjaVolk,PetervanKranenburg,JoergGarbers,FransWier-\ning,RemcoC.Veltkamp,andLouisP.Grijp.Amanualanno-\ntationmethodformelodicsimilarityandthestudyofmelody\nfeature sets. In Proceedings of the Ninth International Con-\nference on Music Information Retrieval (ISMIR), pages 101–\n106, Philadelphia, USA, 2008.\n[29] Tillman Weyde. Integrating segmentation and similarity in\nmelodic analysis. In Proceedings of the International Con-\nferenceonMusicPerceptionandCognition2002,pages240–\n243, Sydney, 2002.\n[30] Tillman Weyde. Modelling cognitive and analytic musical\nstructures in the MUSITECH framework. In UCM 2005 5th\nConference ”Understanding and Creating Music”, Caserta,\nNovember 2005, pages 27–30, 2005.\n[31] Geraint Wiggins. Models of musical similarity. Musicae Sci-\nentiae, Discussion Forum 4a, pages 315–338, 2007.\n464\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "Combined Audio and Video Analysis for Guitar Chord Identification.",
        "author": [
            "Alex Hrybyk",
            "Youngmoo E. Kim"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1417465",
        "url": "https://doi.org/10.5281/zenodo.1417465",
        "ee": "https://zenodo.org/records/1417465/files/HrybykK10.pdf",
        "abstract": "This paper presents a multi-modal approach to automat- ically identifying guitar chords using audio and video of the performer. Chord identification is typically performed by analyzing the audio, using a chroma based feature to extract pitch class information, then identifying the chord with the appropriate label. Even if this method proves per- fectly accurate, stringed instruments add extra ambiguity as a single chord or melody may be played in different positions on the fretboard. Preserving this information is important, because it signifies the original fingering, and implied “easiest” way to perform the selection. This chord identification system combines analysis of audio to deter- mine the general chord scale (i.e. A major, G minor), and video of the guitarist to determine chord voicing (i.e. open, barred, inversion), to accurately identify the guitar chord.",
        "zenodo_id": 1417465,
        "dblp_key": "conf/ismir/HrybykK10",
        "keywords": [
            "multi-modal approach",
            "automatically identifying",
            "guitar chords",
            "audio and video",
            "pitch class information",
            "chord identification",
            "chroma based feature",
            "fretboard",
            "original fingering",
            "implied easiest way"
        ],
        "content": "COMBINED AUDIO AND VIDEO ANALYSIS FOR GUITAR CHORD\nIDENTIFICATION\nAlex Hrybyk and Youngmoo Kim\nElectrical & Computer Engineering, Drexel University\nfahrybyk, ykimg@drexel.edu\nABSTRACT\nThis paper presents a multi-modal approach to automat-\nically identifying guitar chords using audio and video of\nthe performer. Chord identiﬁcation is typically performed\nby analyzing the audio, using a chroma based feature to\nextract pitch class information, then identifying the chord\nwith the appropriate label. Even if this method proves per-\nfectly accurate, stringed instruments add extra ambiguity\nas a single chord or melody may be played in different\npositions on the fretboard. Preserving this information is\nimportant, because it signiﬁes the original ﬁngering, and\nimplied “easiest” way to perform the selection. This chord\nidentiﬁcation system combines analysis of audio to deter-\nmine the general chord scale (i.e. A major, G minor), and\nvideo of the guitarist to determine chord voicing (i.e. open,\nbarred, inversion), to accurately identify the guitar chord.\n1. INTRODUCTION\nThe ability of an instrument to produce multiple notes si-\nmultaneously, or chords, is a crucial element of that instru-\nment’s musical versatility. When trying to automatically\nidentify chords, stringed instruments, such as the guitar,\nadd extra difﬁculty to the problem, because the same note,\nchord, or melody can be played at different positions on the\nfretboard. Figure 1 depicts a musical passage in staff no-\ntation, followed by three representations in tablature form\n(the horizontal lines represent the strings of the guitar, and\nnumber is the fret of that string). All of these tablature\nnotations are valid transcriptions, in that they produce the\ncorrect fundamental frequencies as the staff notation when\nperformed. However, only one of these positions may cor-\nrespond to the original, perhaps easiest ﬁngering\nGuitar lessons are more accessible now than ever with\nthe rise of streaming Internet video and live interactive\nlessons. The research presented in this paper has direct ap-\nplications to these multimedia sources. A system which\ncan automatically transcribe chord diagrams from audio\nand video lessons between student and teacher would be\nan invaluable tool to aid in the learning process.\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2010 International Society for Music Information Retrieval.\n&TAB44GuitarGuitarœœœœ˙!!&Gtr.Gtr.!!!&Gtr.Gtr.!!!&Gtr.Gtr.!!!testA-Hry&TAB44GuitarGuitarœœœœ˙!!&Gtr.Gtr.!!!&Gtr.Gtr.!!!&Gtr.Gtr.!!!testA-Hry&TAB44GuitarGuitarœœœœ˙!!&Gtr.Gtr.!!!&Gtr.Gtr.!!!&Gtr.Gtr.!!!testA-Hry&TAB44GuitarGuitarœœœœ˙!!&Gtr.Gtr.!!!&Gtr.Gtr.!!!&Gtr.Gtr.!!!testA-Hry335753023085785\nThursday, December 10, 2009Figure 1. Three voicings of a C major scale in staff and\ntablature notation, shown in various positions along the\nguitar fretboard.\nAutomatic chord identiﬁcation algorithms have tradi-\ntionally used the chroma feature introduced by Fujishima\n[1]. The chroma based approach, though intuitive and eas-\nily implemented, presents many problems due to the ex-\nistence of overtones in the signal. This paper avoids this\nproblem by using a polyphonic pitch estimation method\nnamed Specmurt Analysis which ﬁlters out the overtones\nin the log-frequency spectrum to yield only a chord’s fun-\ndamental frequencies [2].\nVisual approaches to guitar chord and melody transcrip-\ntion have been attempted. Most of these methods, while\naccurate, are obtrusive to the guitarist; cameras must be\nmounted to the guitar [3], or the guitarist must wear col-\nored ﬁngertips to be tracked [4]. The method presented\nhere uses brightly colored dots placed at various points\nalong the guitar’s fretboard to be tracked by the camera.\nThese dots, which are unobtrusive to the guitarist, are used\nas reference points to isolate the fretboard within the im-\nage, so that principal components analysis may be used to\nidentify the guitarist’s particular voicing of that chord.\nThe multi-modal guitar chord identiﬁcation algorithm\npresented in this paper is as follows: ﬁrst, using Specmurt\nAnalysis, fundamental frequency information will be re-\n159\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)trieved and the general chord scale identiﬁed (i.e. G ma-\njor, A# minor, etc.). Next, using video analysis, the gui-\ntarist’s particular chord voicing (i.e. open, barred, inver-\nsion, etc.) will be identiﬁed using principal components\nanalysis (PCA) of the guitarist’s fretting hand.\n2. RELATED WORK\nThe chromagram or pitch class proﬁle (PCP) feature has\ntypically been used as the starting point for most chord\nrecognition systems. Fujishima ﬁrst demonstrated that de-\ncomposing the discrete Fourier transform (DFT) of a signal\ninto 12 pitch classes and then using template matching of\nvarious known chords produces an accurate representation\nof a song’s chord structure [1].\nThe main problem with chroma is apparent when us-\ning template matching for various chords. For example,\na C Major triad would have an ideal chroma vector of\n[1;0;0;0;1;0;0;1;0;0;0]. The existence of overtones in\nthe signal cause the ideal 0’s and 1’s to ﬂuctuate and create\nfalse chord identiﬁcations.\nModiﬁed versions of the chromagram, such as the En-\nhanced Pitch Class Proﬁle by Lee have been introduced to\nease the effects of overtones in the signal [5]. This method\ncomputes the chroma vector from the harmonic product\nspectrum rather than the DFT, suppressing higher harmon-\nics making the chroma vector more like the ideal binary\ntemplate. However, this method fails to identify the voic-\ning of the chord, such as a ﬁrst or second inversion.\nBurns et al. developed a visual system for left-hand ﬁn-\nger position tracking with respect to a string/fret grid [3].\nTheir method relies on the circular shape of ﬁngertips, us-\ning a circular Hough transform on an image of the left-\nhand to detect ﬁngertip locations with respect to the under-\nlying fretboard. However, this method requires mounting\na camera on the headstock of the guitar, which poses many\nproblems: it can be obtrusive to the guitar player’s natu-\nral method of playing, and also only captures information\nabout the ﬁrst ﬁve frets of the guitar.\nKerdvibulvech et al. proposed to track the ﬁngering po-\nsitions of a guitarist relative to the guitar’s position in 3D\nspace [4] . This is done by using two cameras to form a\n3D model of the fretboard. Finger position was tracked\nusing color detection of bright caps placed on each of the\nguitarist’s ﬁngertips. Again, this can hinder the physical\ncapabilities and creative expression of the guitarist, which\nshould not happen in the transcription process.\n3. AUDIO ANALYSIS\nWhen playing a single note, instruments produce natural\nharmonics (overtones) in addition to the note’s fundamen-\ntal frequency. Therefore, when playing multiple notes, the\nfrequency spectrum of the audio appears cluttered, mak-\ning detection of the fundamental frequencies (the actual\nnotes) hard to locate. Saito et al. have proposed a tech-\nnique called Specmurt analysis, which will be used to ex-\ntract the notes of a guitar chord from the audio signal [2].\nLog-frequency multipitch spectrum ˆ f c(ˆ f )\nˆ f 1ˆ f 2ˆ f 3Common harmonic structure ˆ f h(ˆ f )\nFundamental frequency pattern \nˆ f ˆ f 1ˆ f 2g(ˆ f )\nˆ f 3G(ˆ s )H(ˆ s )C(ˆ s )Log-frequency Specmurt Domain F!1Figure 2. Log-spaced frequency domain c(^f)as a con-\nvolution of common harmonic structure h(^f)with funda-\nmental frequency distribution g(^f).\n3.1 Specmurt Analysis\nMultiple fundamental frequency estimation using Spec-\nmurt analysis is performed by inverse ﬁltering the log-scale\nfrequency domain with a common harmonic structure of\nthat instrument [2]. The resulting log-frequency spectrum\ncontains only impulses located at the log-fundamental fre-\nquencies.\nHarmonics of a fundamental frequency f0normally oc-\ncur at integer multiples of the fundamental, nf0. Further-\nmore, if the fundamental frequency changes by some \u0001f,\nthe change in frequency of its respective higher harmonics\nwill also ben\u0001f . By resampling the frequency domain to\nhave a log-scaled axis, this allows the harmonics of a given\nfundamental to be consistently spaced by logn+ logf0,\nindependent of fundamental frequency.\n^f= logf (1)\n3.1.1 Common Harmonic Structure\nUsing the log-scale frequency axis, we can assume that\nthe harmonic frequencies are located at ^f+ log 2; ^f+\nlog 3;:::;^f+ logn. When a chord is played on an in-\nstrument, each note will presumably contain these same\nharmonic frequencies, beginning at different ^f’s. There-\nfore, we can assume that the log-scaled multipitch spec-\ntrum,c(^f), is a combination of these harmonic structures,\nshifted and weighted differently per note. Speciﬁcally, the\nresulting log-scale frequency spectrum, c(^f), is equal to\nthe convolution of a common harmonic structure, h(^f),\nwith a fundamental frequency distribution, g(^f).\nc(^f) =h(^f)\u0003g(^f) (2)\n160\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)The harmonic structure can be written in terms of its log-\nfrequency axis spacing, ^f0n, and its harmonic weights,\nWn, wheren= 1;2:::N harmonics.\nh(^f;W ) =NX\nn=1Wn\u000e(^f\u0000^f0n) (3)\nThe harmonic weights will initially be a guess, which will\nbe reﬁned later using an iterative process to minimize the\noverall error of Specmurt analysis.\n3.1.2 Specmurt Domain\nIn order to determine the desired fundamental frequency\ndistribution, g(^f), one can solve (2) by deconvolving the\nlog-spectrum with the common harmonic structure. An\neasier way of obtaining g(^f)would utilize the duality of\nthe time/frequency-convolution/multiplication relationship\n(shown in Figure 2). Therefore, taking the inverse Fourier\ntransform would yield the relationship\nF\u00001fc(^f)g=F\u00001fh(^f)\u0003g(^f)g (4)\nC(^s) =H(^s)G(^s) (5)\nwhere ^sis a temporary Specmurt domain variable. Simple\nalgebra followed by a Fourier tranform of G(^s)will yield\nthe resulting fundamental frequency spectrum.\nG(^s) =C(^s)\nH(^s)(6)\nFfG(^s)g=g(^f) (7)\nThe squared error after performing Specmurt analysis\ncan be deﬁned as\nE(Wn) =Z+1\n\u00001n\nc(^f)\u0000h(^f;W n)\u0003g(^f)o2\nd^f(8)\nMinimizing the error of Specmurt is done by reﬁning\nthe harmonic weights, Wn, of the harmonic structure. This\nis done by setting the error’s Npartial derivatives@E\n@Wn=\n0; n = 1:::N , and solving the system of equations for Wn.\nThe original Specmurt formulation assumed that the\nﬁrst weight, W1= 1, of the normalized common harmonic\nstructure. After experimentation with various guitar sig-\nnals, the higher harmonics were sometimes of larger mag-\nnitude than the fundamental frequency. By allowing the\nﬁrst harmonic’s magnitude to vary, the algorithm was able\nto better identify fundamental frequencies.\n4. VIDEO ANALYSIS\nIn order to visually identify the performing guitarist’s\nchord voicing, the guitar fretboard must ﬁrst be located\nand isolated within the image. However, the guitar can be\nheld in many different orientations relative to the camera,\nmaking it difﬁcult to ﬁnd the location or coordinates of the\nfretboard in the image plane.\nThe frets of a guitar are logarithmically spaced to pro-\nduce the 12 tones of the western scale. The coordinates\n! y! \" y ! x\n! \" x ! x0! xn! x4Figure 3. Ideal fretboard (top) with logarithmic xspacing\nofnfrets, and arbitrary neck width in ydirection, and seen\nimage (bottom) with warped spacing.\nin the (x;y)plane are plotted in Figure 3, where the xi\ncoordinates are related by\nxi=iX\nk=0x0\u00022k\n12 (9)\n4.1 Homography\nHomography is the process of applying a projective linear\ntransformation to a scene (a 2D image or 3D space), to de-\nscribe how perceived positions of observed objects change\nwhen the point of view of the observer (a camera) changes.\nHomography will be used to determine the correct perspec-\ntive transformation, i.e. rectify or warp the original image\nto ﬁt the ideal fretboard spacing in Figure 3. This will make\nit easy to isolate the fretboard in the image for analysis.\nThe general homography matrix equation\nwp0=Hp (10)\nstates that points in the image, p0can be expressed as a\nwarping of ideal points pwith a homography matrix H,\nincluding a scale factor w. The homography matrix is\na transformation matrix between the two images, based\non which a one-to-one relationship between the features\npoints p0andp[6]. Speciﬁcally, the points will have two\ndimensions, xandy, and will be expressed in terms of a\n3x3 homography matrix with elements hij.\nw2\n4x0\ni\ny0\ni\n13\n5\u00192\n4h00h10h20\nh10h11h12\nh20h21h223\n52\n4xi\nyi\n13\n5 (11)\nxiare determined from (9) and yiare determined as\nan arbitrary guitar neck width (from the ideal, rectangu-\nlar fretboard). The corresponding reference points (x0\ni;y0\ni)\nin the image now need to be established, to compute the\nhomography matrix, H.\n4.2 Reference Point Tracking\nIn order to perform the homography rectiﬁcation concepts\nin 4.1, the correct reference points in the image must be\n161\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Projected Fretboard/Tracking points\nxyxixnx0x5\nxxnx0Thursday, December 10, 2009Figure 4. (top) Original image showing tracking points (in\nred), projected frets (in green) using the homography ma-\ntrix. (bottom) Ideal fretboard, and subsection of original\nimage after applying homography matrix to each coordi-\nnate.\ndetermined. Attempts were made at using an iterative non-\nlinear error minimization method, which proved initially\nunsuccessful (see later section 6). Instead, distinct bright\ncolored stickers were placed at various fret locations on the\nneck of the guitar. The coordinates of these points (x0\ni;y0\ni)\nwere tracked in each frame of video using a simple color\nmasking followed by a k-means clustering algorithm. The\nsmall stickers were placed on the neck of the guitar on ei-\nther side of the metal frets, so as not to interfere with the\nguitarist’s playing and the timbre of the instrument.\nA set of (xi;yi)and(x0\ni;y0\ni)now exist, corresponding\nto the frets of the guitar. The homography matrix is deter-\nmined by minimizing the mean square error of (11) using\nthese points. Applying the inverse transformation, H\u00001,\nto the ideal grid in Figure 3 yields frets that overlay per-\nfectly with the frets in the image (Figure 4). Applying H\nto the original image and taking the subsection of coordi-\nnates yields the rectiﬁed fretboard (Figure 4), whose fret\nspacings are known from (9). The rectiﬁed fretboard is\nnow isolated and in a usable form for PCA.\n4.3 Determination of Chord Style\nThe next goal is to determine which chord voicing, given\nthe subset of voicings that exist for a particular chord. PCA\nis used to decompose the rectiﬁed fretboard in its “eigen-\nchord” components, and determine the correct chord voic-\ning.\nLet the training set of fretboard images be F1;F2:::FM\nwhich are vectors of length LW for an image with di-\nmensionsLbyW. An example training set of fret-\nFigure 5. Example fretboard images used for training.\nboard images is shown in Figure 5. The average image\nisA=1\nMPM\ni=1Fi, and each image with subtracted mean\nis\u0016Fi=Fi\u0000A. PCA seeks to ﬁnd the eigenvectors and\neigenvalues of the covariance matrix\nC=1\nMMX\ni=1\u0016Fi\u0016FT\ni (12)\n=SST(13)\nwhere S= [\u0016F1;\u0016F2:::\u0016FM]is a set of training of images in\nmatrix form. However, Cis of dimension LW; the im-\nages used in this experiment are of size 80x640 pixels, and\ncomputing 51200 eigenvectors and eigenvalues is compu-\ntationally intractable. Turk et. al presented a method for\nsolving for the LW eigenvectors by ﬁrst solving for the\neigenvectors of an MxM matrix STS[7]. TheMeigen-\nvectorsvlare used to form the eigenvectors ulofC.\nul=MX\ni=1vl\u0016Fil= 1:::M (14)\nA new image ~Fcan be reduced to its eigen-chord com-\nponents,ck, using theM0eigenvectors which correspond\nto the larger eigenvalues of STS.\nck=uk(~F\u0000A)k= 1:::M0(15)\n5. EXPERIMENTAL RESULTS\nThree guitarists were asked to perform a sequence of\nchords from chord diagrams. The chords were a selec-\ntion drawn from eight scales (major and minor), each in\nthree voicing-dependent positions: open (traditional open\nstringed), barred, and a 1st inversion, totaling 24 chords\nall together. The system was evaluated using various\ncombinations of features derived from audio only, video\nonly, and combinations thereof. All experiments were per-\nformed using leave-one-out training of audio and video\nwhen using PCA.\n162\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Specmurt Piano !roll of C#m7 Jazz ChordNote Name\nFrame number  \n5 10 15 20 25 30 35C2\nC#2\nD2\nD#2\nE2\nF2\nF#2\nG2\nG#2\nA2\nA#2\nB2\nC3\nC#3\nD3\nD#3\nE3\nF3\nF#3\nG3\nG#3\nA3\nA#3\nB3\nC4\nC#4\nD4\nD#4\nE4\nF4\nF#4\nG4\nG#4\nA4\nA#4\nB4\nC5\nC#5\nD5\nD#5\nE5\nF5\nF#5\nG5\nG#5\nA5\nA#5\nB5\nHighest EnergyFigure 6. Specmurt piano-roll output of a C#m7[5 jazz\nchord.\n5.1 Audio Only\nThe output of Specmurt analysis is a piano-roll vector of\nsize 48, each element corresponding to the energy of a\nchromatic note from C2 to B5 (4 octaves, 12 notes per oc-\ntave). An example of a piano-roll vector over multiple time\nframes is shown in Figure 6.\nTwo methods were used to calculate the correctness of\nthe chord scale and voicing using this vector. It is known\nwhat notes make up each major and minor scale. There-\nfore, the chord scale was evaluated by summing the energy\nover all octaves of the notes belonging to that chord - sim-\nilar to chroma analysis. The chord scale with the highest\nenergy was assumed to be correct, yielding an accuracy of\n98.6%.\nIt is not deterministic, however, as to which chord voic-\ning created a particular set of notes, or chord. For example,\nboth the G major open chord and G major barred chord\ncontain six notes total, all of the same fundamental fre-\nquencies, but the notes are rearranged on different strings,\nand hence use different ﬁngerings. Therefore, a training set\nusing the piano-roll energy vector was developed for each\nchord scale. Using PCA to identify chord voicing from the\npiano-roll vector shows some accuracy (62%) but is under-\nAudio only Video only Combined System\nScale 98.6 34.8 98.6\nV oicing 62.0 94.4 94.4\nBoth 61.1 32.8 93.1\nTable 1. Accuracy results for various combinations of\nmodes of information. Combined accuracy results using\nSpecmurt for scale identiﬁcation, and video for voicing\nidentiﬁcation showing highest accuracy.\n−2\n0\n2x 108\n−3−2−101234\nx 108−6−4−20246x 108 \nEigenchord 1Eigen−chord Space Separated by Voicing\nEigenchord 2 Eigenchord 3Barred\nInverted\nOpenFigure 7. Three voicings from A minor, G major, and C\nmajor, after being projected into the chord-space. Various\ncolors and symbols show the how the voicing of chords\nremain grouped after dimensionality reduction.\nstandably low, as the difference in note energies may be\nvery ﬁne and inseparable for different voicings with simi-\nlar notes.\n5.2 Video Only\nA training set of 240 images was used to build the eigen-\nchord space for each test. Frames of video were then pro-\njected into the chord-space using three eigen-chords of the\ntraining set using (15), and its closest centroid was as-\nsumed to be the correct chord.\nChord scale identiﬁcation using only video performed\nextremely poorly (34%). This is expected, as the chord\nscale centroids in the projected chord-space after PCA are\nsomewhat meaningless. For a particular chord scale, many\ndifferent voicings exist at various points on the fretboard,\nwhich is what we hope to separate by using PCA.\nFor chord voicing however, very high accuracy was\nachieved (94.4%). Figure 7 shows how various voicings\nof chords, irrespective of scale, tend to group together due\nto the similar hand shapes used by the guitarist.\n5.3 Combined System\nThe system which performs the best in terms of correctly\nidentifying the overall chord (scale and voicing) utilizes\nthe strong points of scale and voicing identiﬁcation within\nthe audio and video results. Since Specmurt analysis\nyielded extremely high accuracy for determining scale, it\nwas used as a preprocessing step to voicing identiﬁcation\nvia video.\n6. FUTURE WORK\nThe video and audio components of this guitar chord iden-\ntiﬁcation system have the potential to be expanded upon.\n163\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Figure 8. Guitar image (left) and edge-thresholded image\n(right).\n6.1 Automatic Fretboard Registration\nPlacing colored tracking points along the neck of the guitar\npresents additional constraints on how the guitar fretboard\ncan be rectiﬁed: all the tracking points must be visible in\nthe frame of video, and nothing else in the frame may have\nsimilar color. Ideally, we would like to locate the fretboard\nwithout these points. By looking at the edge-detected im-\nage of a guitar, this produces a fairly accurate represen-\ntation of where the frets are - the color of the metal frets\ncontrasts heavily with that of the wooden neck, providing\nedges at frets (Figure 8).\nUsing the homography concept in 4.1, the points de-\nnoted as edges, p0, should be warped using H\u00001to align\nwith the ideal fret-grid points p. This is equivalent to min-\nimizing an error function deﬁned as\nE(H) =jjp\u0000H\u00001p0jj2(16)\nH=argmin\nH(E(H)) (17)\nAfter experimentation, the error function E(H) is no-\nticeably non-convex, and contains local minima in H. The\ntwo fret-grids “align” in alternate orientations which are\nincorrect, but still minimize the error function. This area\nof research is being continued with the motive of constrain-\ning (16) and (17), such that the error function will always\nbe convex, and converge to a global minimum when the\ntwo images are correctly aligned.\n6.2 Larger Training Sets\nVery high accuracy of video voicing identiﬁcation (94.4%)\nwas achieved using image data from only three guitarists.\nA more robust classiﬁer of chord voicings could be cre-\nated by collecting more data, to account for players who\nuse non-traditional ﬁnger orientations for chords. With\nmore data, the accuracy of determining chord scale from\nvideo may increase (34.8%), as scales may then form more\nmeaningful distributions in the eigen-chord space.\n6.3 Additional Chord Types\nThis system is very extendable to detect different chord\nscales besides major and minor. Detection of diminished,\naugmented, 7th, and other jazz chords are easily imple-\nmented with the chroma-style analysis of Specmurt’s out-\nput, and reﬁned search using the eigen-chord decomposi-\ntion of the fretboard image.6.4 Fusing Audio/Video Data\nCurrently the system uses Specmurt analysis to determine\na chord’s scale as a pre-processing step to eigen-chord de-\ncomposition of the fretboard to determine voicing. This\nmeans that any error introduced by Specmurt propagates\nthroughout the rest of the system. Therefore it is desired to\njointly estimate the scale and voicing together using audio\nand video features simultaneously.\n7. CONCLUSION\nThis paper has presented an alternate approach to au-\ntomatic guitar chord identiﬁcation using both audio and\nvideo of the performer. The accuracy of chord identi-\nﬁcation increases from 61.1% to 93.1% when using au-\ndio for scale identiﬁcation, and video for voicing. The\n“eigen-chord” decomposition of fretboard images proved\nextremely successful in distinguishing between a given\nchords voicings (normal, barred, inverted) if the chord\nscale is known (94.4%).\n8. REFERENCES\n[1] T. Fujishima, “Realtime chord recognition of musical\nsound: A system using common lisp music.” in Pro-\nceedings of the International Computer Music Confer-\nence, 1999.\n[2] S. Saito, H. Kameoka, K. Takahashi, T. Nishimoto,\nand S. Sagayama, “Specmurt analysis of polyphonic\nmusic signals,” Audio, Speech, and Language Process-\ning, IEEE Transactions on, vol. 16, no. 3, pp. 639–650,\nFebruary 2008.\n[3] A.-M. Burns and M. M. Wanderley, “Visual methods\nfor the retrieval of guitarist ﬁngering,” in NIME ’06:\nProceedings of the 2006 conference on New interfaces\nfor musical expression. Paris, France, France: IR-\nCAM — Centre Pompidou, 2006, pp. 196–199.\n[4] C. Kerdvibulvech and H. Saito, “Vision-based guitarist\nﬁngering tracking using a bayesian classiﬁer and parti-\ncle ﬁlters,” in PSIVT07, 2007, pp. 625–638.\n[5] K. Lee, “Automatic chord recognition from audio us-\ning enhanced pitch class proﬁle,” in Proceedings of the\nInternational Computer Music Conference, 2006.\n[6] X. Wang and B. Yang, “Automatic image registration\nbased on natural characteristic points and global ho-\nmography,” in Computer Science and Software Engi-\nneering, 2008 International Conference on, vol. 5, dec.\n2008, pp. 1365 –1370.\n[7] M. Turk and A. Pentland, “Face recognition using\neigenfaces,” in Computer Vision and Pattern Recogni-\ntion, 1991. Proceedings CVPR ’91., IEEE Computer\nSociety Conference on, June 1991, pp. 586 –591.\n164\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "Singing Pitch Extraction by Voice Vibrato / Tremolo Estimation and Instrument Partial Deletion.",
        "author": [
            "Chao-Ling Hsu",
            "Jyh-Shing Roger Jang"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1417357",
        "url": "https://doi.org/10.5281/zenodo.1417357",
        "ee": "https://zenodo.org/records/1417357/files/HsuJ10.pdf",
        "abstract": "This paper proposes a novel and effective approach to extract the pitches of the singing voice from monaural polyphonic songs. The sinusoidal partials of the musical audio signals are first extracted. The Fourier transform is then applied to extract the vibrato/tremolo information of each partial. Some criteria based on this vibrato/tremolo information are employed to discriminate the vocal par- tials from the music accompaniment partials. Besides, a singing pitch trend estimation algorithm which is able to find the global singing progressing tunnel is also pro- posed. The singing pitches can then be extracted more robustly via these two processes. Quantitative evaluation shows that the proposed algorithms significantly improve the raw pitch accuracy of our previous approach and are comparable with other state of the art approaches submit- ted to MIREX.",
        "zenodo_id": 1417357,
        "dblp_key": "conf/ismir/HsuJ10",
        "keywords": [
            "novel",
            "effective",
            "pitch",
            "singing",
            "voice",
            "monaural",
            "polyphonic",
            "songs",
            "sinusoidal",
            "partial"
        ],
        "content": "SINGING PITCH EXTRACTION BY VOICE \nVIBRATO/TREMOLO ESTIMATION AND INSTRUMENT \nPARTIAL DELETION \nChao-Ling Hsu  Jyh-Shing Roger Jang \nMultimedia Information Retrieval Laboratory \nComputer Science Department, National Tsing Hua University \nHsinchu, Taiwan \n{leon, jang}@mirlab.org  \nABSTRACT \nThis paper proposes a novel and effective approach to \nextract the pitches of the singing voice from monaural polyphonic songs. The sinusoidal partials of the musical audio signals are first extract ed. The Fourier transform is \nthen applied to extract the vibrato/tremolo information of each partial. Some criteria based on this vibrato/tremolo information are employed to discriminate the vocal par-\ntials from the music accompanim ent partials. Besides, a \nsinging pitch trend estimation algorithm which is able to find the global singing progressing tunnel is also pro-\nposed. The singing pitches can then be extracted more robustly via these two proce sses. Quantitative evaluation \nshows that the proposed algorithms significantly improve the raw pitch accuracy of our previous approach and are comparable with other state of the art approaches submit-\nted to MIREX. \n1. INTRODUCTION \nThe pitch curve of the lead vo cal is one of the most im-\nportant elements of a song as it represents the melody. \nHence it is broadly used in many applications such as singing voice separation, music retrieval, and auto-tagging of the songs. \nLots of work which focuses on extracting the main \nmelody of songs has been proposed in the literature. Po-liner et al. [1] comparativ ely evaluated different ap-\nproaches and found that most of the approaches roughly follow the general framework as follows: Firstly, the pitches of different sound sources are estimated at a giv-en time and some of them are then selected as the candi-\ndates. The melody identifier then chooses one, if any, of these pitch candidates as a constituent of the melody for each time frame. Finally the output melody line is formed after smoothing the raw pitch line. Since the goal of most of these approaches is to ex tract the melody line carried \nby not only the singing voice but also the music instru-ments, they do not consider the different characteristics \nbetween the human singing voice and instruments: for-mants, vibrato and tremolo. More related work can be found in our previous work [3]. \nIn the present study, we apply the method suggested \nby Regnier and Peeters [2], which was originally used to detect the presence of singi ng voice. This method utilizes \nthe vibrato (periodic variation of pitch) and tremolo (pe-riodic variation of intensity) characteristics to discrimi-nate the vocal partials from the music accompaniment \npartials. We apply this technique to the singing pitch ex-traction so that the singing pitches can be tracked with less interference of instrument partials. \nThe rest of this paper is organized as follows. Section \n2 describes the proposed system in detail. The experi-mental results are presented in section 3, and section 4 concludes this work with possible future directions. \n2. SYSTEM DESCRIPTION \nFig. 1 shows the overview of the proposed system. The \nsinusoid partials are first ex tracted from the musical au-\ndio signal. The vibrato and tremolo information is then estimated for each partial. Af ter that, the vocal and in-\nstrument partials can be discriminated according to a \ngiven threshold, and the instrument partials can be there-fore deleted. With the help of instrument partials dele-tion, the trend of the singing pitches can be estimated more accurately. This trend is referred to as global pro-\ngressing path and indicates a series of time-frequency \nregions (T-F regions) where the singing pitches are like-ly to be present. Since the T-F regions consider relatively \nlarger periods of time and larger ranges of frequencies, they are able to provide robust estimations of the energy distribution of the extracted sinusoidal partials.  \nOn the other hand, the normalized sub-harmonic \nsummation (NSHS) map [3] which is able to enhance the \nharmonic components of the spectrogram is computed, and the instrument partials which are discriminated with lower thresholds are deleted from NSHS map. After that, the global trend is applied to the instrument-deleted NSHS map. \n \nPermission to make digital or hard copi es of all or part of this work fo r\npersonal or classroom use is grante d without fee provided that copies \nare not made or distributed for profit or commercial advantage and tha t\ncopies bear this notice and the full citation on the first page. © 2010 International Society for Music Information Retrieval  The energy at each semitone of interest (ESI) [3] is \nthen computed from the trend-confined NSHS map. Fi-\nnally, the continuous raw pitches of the singing voice are \n525\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)estimated by tracking the ESI values using the dynamic \nprogramming (DP) based pitch extraction. \nAn example is shown in the evaluation section (3.2). \nThe following subsections explain these blocks in detail. \n2.1 Sinusoidal Extraction \nThis block extracts the sinusoidal partials from the \nmusical audio signal by employing the multi-resolution \nFFT (MR-FFT) proposed by Dressler [4]. It is capable of covering the fast signal changes and maintaining an ade-\nquate discrimination of concurrent sounds at the same \ntime. Both of these properties are extremely well justified for the proposed approach. \nThe extracted partials with  short duration are excluded \nin this stage because they ar e more likely to be produced \nby some percussive instruments or unstable sounds. \n2.2 Vibrato and Tremolo Estimation \nAfter extracting the sinusoidal partials, the vibrato and \ntremolo information of each pa rtial are estimated by this \nblock by applying the method suggested by Regnier and Peeters [2]. \nVibrato refers to the periodic variation of pitch (or \nfrequency modulation, FM) and tremolo refers to the pe-riodic variation of intensity (or amplitude modulation, AM). Due to the mechanical aspects of the voice produc-tion system, human voice contains both types of the modulations at the same time, but only a few musical in-struments can produce them simultaneously [5]. In gen-\neral, wind and brass instruments produce AM dominant \nsounds, while string instruments produce the FM domi-nant sounds. \nTwo features are computed to describe vibrato and \ntremolo: frequencies (the rate of vibrato or tremolo) and amplitudes (the extent of vibrato or tremolo). For human singing voice, the average rate is around 6Hz [6]. Hence we determine the relative extent values around 6Hz by using the Fourier transform for both vibrato and tremolo. \nPolyphonic  songs\nSinusoidal  extraction\nVibrato and tremolo\nestimation\nInstrument/vocal  partials\ndiscriminationInstrument  \ndeleted NSHS mapPartials\nVibrato and tremolo\nof each partialLow \nthreshold \nresult\nESINSHS  map computation\nRaw pitch vectorsDP‐based\npitch extraction\n0 200 400 600 800 1000 120050607080\nTime frameSemitoneGround truth\nEstimated raw pitchESI extraction  \nfrom NSHSSinging pitch trend \nestimation  NSHS map\nInstrument  partials deletion\nfrom  NSHS map\nTrend confinement\nHigh threshold  \nresultEstimated  \ntrendTrend confined \nNSHS map\nFigure 1 . System overview \n More specifically, to comput e a relative extent value \nof vibrato for a partial existing from time to  , \nthe Fourier transform of its frequency values is \ngiven by: ) (t pk it\nfkpjt\n)t(\nLtf it\nt tpf p p e t f f Fj\nikk kπμ2) ) ( ( ) (−\n=∑− = , \nwhere \nkpfμ  is the average frequency of  and ) (t pk\nitjt L−= . The relative extent value in Hz is given by: \nkk\nk\npfp\nrelpLf Ff fμ) () (= Δ . \nLastly, the relative extent value around 6Hz is computed \nas follow: \n) ( max\n] 8 , 4 [f f fk k relp\nfpΔ=Δ\n∈. \nThe relative extent value for tremolo can be computed in \nthe same way except that amplitude  is used instead \nof . kpa\nkpf\n2.3 Instrument/Vocal Partials Discrimination \nThe instrument and vocal partials are discriminated ac-\ncording to the given thresholds of the relative extent of vibrato and tremolo. The instrument partials can then be deleted if both the relative extents are lower than speci-fied values. By selecting the thresholds, we can adjust the trade-off between instrument partials deletion rate and vocal partials deletion error rate. The higher thresholds are, the more instrument partials are deleted, but the more deletion errors of the vocal partials are. Usually a lower threshold is applied for instrument partials deletion from NSHS map, while a higher threshold is applied for the singing pitch trend estimation. The reasons will be ex-plained in the following subsections. \n2.4 Singing Pitch Trend Estimation \nOne of the major error types of singing pitch extraction is the doubling and halving errors where the harmonics or sub-harmonics of the fundamental frequency are erro-neously recognized as the singing pitches. Here we refer the harmonic partials to those partials whose frequencies are multiples of the F0 partials. And we use “vocal par-tials” to indicate the union of the disjoint sets of “vocal F0 partials” and “vocal harmonic partials”. Although the error can be handled by considering the time and fre-quency smoothness of the pitch contours, most of the ap-proaches only consider the local smoothness during a short period of time. However, there are many ‘gaps’ be-tween successive vocal partials such as the non-vocal pe-\n526\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Because the singing pitch tr end should be smooth, the \nproblem is defined as the finding of an optimal path riod between two segments of lyrics where instrument \npartials may be predominant in these gaps. These instru-ment partials often act like ‘bridges’ which may mislead the pitch tracking algorithm to connect two vocal partials erroneously. [ ]1 0 , , , ,− ⋅⋅⋅⋅⋅⋅n iF F F  that maximizes the score function: \n()∑∑−\n=−\n=−− × − =1\n01\n11 , ,n\nTn\nTT T F T F F s F score\nTθ θ , \nTo deal with this problem, we propose a method to es-\ntimate the trend of the singing pitches. Firstly, higher thresholds are applied to delete more instrument partials. This might also delete some vocal partials, but it will not affect the pitch trend estimation as long as we still have enough vocal partials. Secondly, the harmonic partials are deleted based on the assumption that the lowest-frequency partial within a frame is the vocal F0 partial. Moreover, these deleted harmonic partials are accumu-lated into their vocal F0 partials. This process is repeated until we have only several low-frequency partials representing potential vocal F0 partials. As a result, most of the harmonic partials are deleted and the energy of the vocal F0 partials is strengthened. The energy of the re-maining partials is then max-picked for each frame and summed up within a time-freque ncy region (T-F region). \nMore precisely, given a spectrogram  computed \nfrom the previous MR-FFT, the strength  of the T-F \nregion is defined as: \n] , [f t x\nF Ts,where  is the strength of the T-F region at the time \nindex  and frequency index . The first term in the \nscore function is the sum of strength of the T-F region \nalong the path, while the second term controls the \nsmoothness of the path with the use of a penalty coeffi-cient TF Ts,\nTTF\nθ. If θ is larger, the computed path is smoother. \nThe dynamic programming technique is employed to \nfind the maximum of the score function, where the opti-\nmum-valued function  is defined as the maximum \nscore starting from time index 1 to ) , (l T D\nT, with l FT=: \n∑−\n=− ∈+ + =1\n0] 1 , 0 [, ] , [ maxtime\nfreqM\ntfreq timeM fF T FL f TL t x s ,             \n               and  1 ,... 1 , 0− =n T 1 ,... 1 , 0− =m F\nwhere \nt is the index of the time frame. \nf is the index of the frequency bin. \nn is the number of T-F regions in the time axis \nm is the number of T-F regions in the frequency axis \nT,  F are the indices of the T-F region in time and frequency \naxes respectively. \ntimeL,  freqL are the time and frequency advance of the T-F region (hop-size) respectively. \ntimeM ,  freqM are the number of the time frames and the number of the frequency bins of a T-F region respectively. \n \nThe size of the T-F region should be large enough so \nthat the global trend of the singing pitches can be ac-\nquired. On the other hand, th e T-F region should also be \nsmall enough so that the harmonics of the singing pitches can be separated in differe nt frequency bands and the \npitch changes can be capture d in different time periods. \nNote that although  is fixed for all T-F regions, the \nfrequency ranges are different for the T-F regions in dif-ferent frequency bands. This is because the frequency bins in the result of sinusoidal extraction via MR-FFT are spaced by 0.25 semitone. In other words, the lower fre-\nquency T-F region has smalle r frequency range since the \nfrequency differences betw een low fundamental frequen-\ncy partials and their harmonics are relatively smaller than that of high fundamental  frequency partials. freqM[]{ }, l k k D s l\nm kl T − × − + =\n− ∈θ) max ) ,\n1 , 0,\n] 1t−, 1 ( T D(\n, 1 [ where −=n ] 1 0 [ t , and ,−= m l . The initial condi-\ntion is Dls l, 0 ), 0 (= , and the optimum score is equal \nto\n[](\n1n D\nm) , 1\n, 0l\nlmax−\n−∈. At last, this optimal path is applied to \nthe instrument-deleted NSHS  map described in section \n2.6. \n2.5 NSHS Computation \nInstead of simply extracting the singing pitches by track-\ning the remaining vocal partials, the NSHS proposed by our previous work [3] is us ed since the non-peak values \nof the spectrum are also usef ul for the later DP-based \npitch extraction algorithm. The NSHS is able to enhance \nthe partials of harmonic sound sources, especially the singing voice. It is modifi ed from the sub-harmonic \nsummation [7] by adding a normalizing term. The reason of the modification is based on the observation that most of the energy in a song locates at the low frequency bins, and the energy of the harmonic structures of the singing voice decays slower than that of instruments [8]. It is \ntherefore that, when more harmonic components are con-sidered, energy of the vocal sounds is further streng-thened. \n2.6 Instrument partials deletion and trend confine-\nment  \nIn these two blocks, the instru ment partials detected with \nthe lower thresholds in the previous block are first re-moved from the NSHS map by setting their magnitude to zero (within the range of neighboring local minima). For extracting singing pitches, the thresholds are set to be lower in order to delete the instrument partials without deleting too many vocal partia ls. After that, the instru-\nment deleted NSHS map can be further confined to the estimated pitch trend (section 2.4). In other words, only \nthe energy along the trend will be retained.                                              \n527\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)2.7  ESI Extraction from NSHS \nThe ESI computed from the trend-confined NSHS \nmap in the time frame t can be obtained as follows [3]: \n()(f A n vtp pp pp ppt\nn n\nnn n\nn2 21 1max ) (\n−+ < ≤−−+ −= ),                                         \n0 10 20 30 40 50 60 70 80 90 1000102030405060708090100\n   α=5.8\n   α=4.3   α=7.8\n   α=4.0\n   α=2.8   α=5.9\n   β=8.6\n   β=7.1   β=10.6\n   β=6.9\n   β=5.0   β=9.2\nInstrument Partials Miss Error Rate (%)Instrument Partials False Alarm Rate (%)DET Curve of Instrument Partials Detection\n  \nClass2 = vocal F0 partials with different α\nClass2 = all vocal partials with different α\nClass2 = vocal F0 partials with different β\nClass2 = all vocal partials with different β\nFigure 2 . The DET curves of instrument partials false \nalarm rate versus instrument partials miss error rate by \nusing different  values of α and β as the thresholds \nalone, respectively. (Here we assume class 1 is instru-ment partials, and class 2 is either vocal F0 partials or all vocal partials.)  where  is the NSHS map calculated in the previous \nstage, ,  is the total number of semi-\ntones that are taken into account, and  is the frequency \nof the -th semitone in the selected pitch range. ()∗tA\n1 , 0=n\nn1 ,..,−N N\nnp\nNote that we also need to  record the maximal frequen-\ncy within each frequency range of ESI in order to recon-struct the most likely pitch contours. \n2.8  DP-based Pitch Extraction \nThe DP-based pitch tracking algorithm is previously pro-\nposed in [3]. It is very si milar to the algorithm described \nin section 2.4. The most likely pitch contour can be final-\nly acquired by tracking the ESI computed in the previous block. Note that we do not  perform vocal/non-vocal de-\ntection since it is not the focu s of this study. In addition, \nthe vocal/non-vocal detection can be implemented by various methods such as [2][3]. \n3. EVALUATION 3.1 Evaluation for Instrument Partials Detection \nThe frame size and hop size used in the sinusoidal extrac-tion by MR-FFT are 64 ms and 8 ms respectively. The frequency bins in MR-FFT are spaced by 0.25 semitone from 80Hz to 1280Hz, resulting a total of 192 bins. The partials whose durations are less than 56 ms are removed since they are more likely to be generated by percussive instruments or unstable sounds. With regard to the rela-tive vibrato and tremolo extent estimation, the parameters are set to be the same as those suggested by [2]. Two datasets were used to evaluate the proposed ap-proach. The first one, MIR-1K, is a publicly available dataset proposed in our previ ous work [9]. It contains \n1000 song clips recorded at 16 kHz sample rate with 16-bit resolution. The duration of each clip ranges from 4 to 13 seconds, and the total length of the dataset is 133 mi-nutes. These clips were extracted from 110 karaoke songs which contain a mixed track and a music accompaniment track. These songs were selected (from 5000 Chinese pop songs) and sung, consisting of  8 females and 11 males. \nMost of the singers are amateurs with no professional training. The music accompaniment and the singing voice were recorded at the left and right channels respectively. The ground truth of the pitch values of the singing voices were first estimated from the pure singing voice and then manually corrected.  All songs are mixed at 0 dB SNR, indicating that the energy of  the music accompaniment is \nequal to the singing voice. Note that the SNRs for com-mercial pop songs are usually larger than zero, indicating that our experiments were set to deal with more adversary \nscenarios than the general cases. The second dataset, ADC2004, is one of the testi ng dataset for audio melody \nextraction task in MIREX. It  contains 20 song clips and \nthe average length of the clips is around 20 seconds. Only the 12 vocal songs of ADC2004 are used for testing in this study. Although the size of ADC2004 is much small-er than that of MIR-1K, it is convenient for comparing the performance of different algorithms which were sub-\nmitted to MIREX. Figure 2 shows the DET (detection error tradeoff) \ncurves of instrument partials false alarm rate versus in-\nstrument partials miss error ra te by using different rela-\ntive vibrato extent ( α) and relative tremolo extent ( β) as \nthe thresholds alone, respectively. A higher instrument partials false alarm rate indicates more vocal partials are erroneously recognized as instrument partials. On the other hand, a higher instrument  partials miss error rate \nindicates more instrument par tials are recognized as vocal \npartials. Here we assume class 1 is instrument partials, and class 2 is either vocal F0 partials or all vocal partials. The solid line and dotted line show the results of using \nvocal F0 partials as class 2 with different α and β respec-\ntively. The dashed line and da sh-dot line show the results \nof using all vocal partials as class 2 with different α\n and β \nrespectively. We want to show the results of using vocal F0 partials as class 2 because the goal of this study is to \nextract the singing pitches carri ed by these vocal F0 par-\ntials. In contrast, the harm onic partials of the singing \nvoice are comparably not as important. All of these par-tials are extracted from the MIR-1K dataset. Since the MIR-1K has separated tracks of singing voice and ac-companiment, the sources of the partials can be distin-\nguished. \nFrom Figure 2, it is obvious that α has better discri-\nminative capability to detect instrument partials than β. \n528\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)This is because the pop music in MIR-1K has less wind \nand brass instruments than st ring instruments. We have \nfound in our preliminary experiment1 that β  has better \nvocal/instrument discriminativ e power for wind and brass \ninstruments. \nThe instrument partials deletion block applied α = \n0.1125 and β = 3. The vocal F0 remaining rate is around \n94.3% (or equivalently, 5.7% instrument partials false \nalarm rate) and instrument pa rtial deletion rate is around \n60.4% (or equivalently, 39.6%  instrument partials miss \nerror rate). On the other ha nd, singing pitch trend estima-\ntion applied α = 0.3 and β  = 5.5 as the thresholds. The \nvocal F0 partials remaining ra te is 72.9% and instrument \npartials deletion rate is 82.8%. \n3.2 Evaluation for Singing Pitch Trend Estimation \nThe parameters for this experiment were set as follows. The sizes along time and frequency axes for each T-F re-gion were 3 seconds and 13.5 semitones, respectively. \nTheir hop sizes were 1.5 s econds and 4 semitones, \nrespectively. The penalty coefficient \nθ for the dynamic \nprogramming step was set to 1 empirically. \nTable 1 shows the results of the singing pitch trend es-\ntimation. More than 82% of vo cal F0 partials remain in \nthe pitch trend tunnel and the singing pitches remaining \nrate is 86%. On the other hand, only 19.19% of instru-\nment and vocal harmonic partials are retained within the pitch trend tunnel. In add ition, 66.18% of the non-vocal \nF0 partials left in the pitch trend tunnel are deleted by the NSHS computation stage, a nd 8.07% of the remaining \nvocal F0 partials are deleted erroneously at the same time. Finally, 75.82% of vocal F0 partials remain while only 6.49% of non-vocal F0 partials  are kept in both deletion \nprocedures. \nFigure 3 shows the stage-wise results in singing pitch \nextraction. Figure 3(a) shows all the partials after sinu-soidal extraction. Figure 3(b) and 3(c) applies different thresholds on 3(a) to delete instrument partials for differ-ent purposes. Because 3(b) applies lower thresholds than those of 3(c), more instrument partials are removed in 3(c). The harmonic partials in Figure 3(c) are then further \ndeleted in 3(d). Figure 3(f) is  obtained by subtracting the \n                                                          \n \n/1 The experiment was also performed on the University of Iowa \nMusical Instrument Samples which is available \nat http://theremin.music.uiowa.edu  detected instrument partials in Figure 3(b) from the \nNSHS map in 3(e). Figure 3(g) illustrates the T-F regions computed from Figure 3(d), w ith color depth indicating \nthe strength each T-F region. Finally, Figure 3(h) is the \nNSHS map (Figure 3(f)) confined by the pitch trend tun-nel. As can be seen in this  example, the identified pitch \ntrend tunnel is capable of covering the vocal F0 partials (represented by solid lines) wh ile most of the instrument \npartials are deleted. \nTime (secs)Frequency (Hz)(a) Sinusoidal extraction using MR-FFT\n0 1 2 3 4 5 6 7 8 980107143190 2543394536048061076\nTime (secs)Frequency (Hz)(e) The NSHS map\n0 1 2 3 4 5 6 7 8 9801071431902543394536048061076\nTime (secs)Frequency (Hz)(b) Instrunet partial deletion with α = 0.1125 and β =3\n0 1 2 3 4 5 6 7 8 9801071431902543394536048061076\nTime (secs)Frequency (Hz)(f) Instrument partial-deleted NSHS map with α=0.1125 and β=3\n0 1 2 3 4 5 6 7 8 9801071431902543394536048061076\nTime (secs)Frequency (Hz)(c) Instrunet partial deletion with α  = 0.3 and β =5.5 \n0 1 2 3 4 5 6 7 8 9801071431902543394536048061076\nT-F region time indexT-F region frequency index(g) The estimated singing pitches trend-diagram\n2 4 6 8 10 121234567\nTime (secs)Frequency (Hz)(d) Harmonic partials deletion\n0 1 2 3 4 5 6 7 8 9801071431902543394536048061076\nTime (secs)Freqency (Hz)(h) Trend confined NSHS map\n0 1 2 3 4 5 6 7 8 9801071431902543394536048061076\nFigure 3 . Stage-wise results of singing pitch extraction \nfor the clip ‘Ani_4_05.wav’ in MIR-1K. (a) Results af-\nter sinusoidal extraction using MR-FFT. (b) The re-maining partials after instrument partial deletion thre-sholds of α  = 0.1125 and β = 3. (c) The remaining par-\ntials after instrument partia l deletion after threshold of α \n= 0.3 and β  = 5.5. (d) The result after harmonic partials \ndeletion. (e) The NSHS map. (f) Instrument partial-deleted NSHS map with threshold of α = 0.1125 and β \n= 3. (g) The estimated singing pitches trend-diagram. (h) Trend confined NSHS map, where the solid line \nrepresents the ground truth of the singing pitches.  Vocal F0 Non-vocal F0 \nPartials remaining in the \npitch trend tunnel 82.47 % 19.19 % \nPartials remaining in the \npitch trend tunnel but de-\nleted by instrument par-\ntial deletion 8.07 % 66.18 % \nFinal partials remaining 75.82% 6.49% \nVocal pitches remaining \nin the pitch trend tunnel 86.30% \nTable 1.  Performance of singing pitch trend estimation \n \n3.3 Evaluation for Singing Pitch Extraction \nFigure 4 shows the results of singing pitch extraction. \nThe raw pitch accuracy is computed over the frames which were labeled as voiced in the ground truth. An es-timated singing pitch is consider ed as correct if the devia-\ntion from the ground truth is small than 1/4 tone (or 1/2 \n529\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \nSince only the features suggested in [2] were used in \nthis study, other characteristics of voice vibrato and tre-\nmolo could be use as new features for improving the per-formance. Moreover, it is worth noting that the proposed instrument partial deletion and singing trend estimation \ntechniques are general for pitch extraction, in the sense that they can be applied to any other spectrum-based me-thods to delete the unlikely pitch candidates. Our imme-diate future work is to e xplore the use of the proposed \ntechniques on top of existing methods to confirm their \nfeasibility in further improving the performance. semitone). The black bars show the performance of the \nprevious NSHS-DP method [3] (ranked 5-th out of 12 in \nMIREX2009). The dark gray bars show the result of combining the proposed instrument partial deletion and dynamic programming without using the NSHS. The light gray bars are the same as the dark gray bar except that the NSHS map is applied. The light gray bars per-form better than the ones without using the NSHS map, which confirms the argument th at the non-peak values of \nthe spectrum are also useful. Lastly the white bars show \nthe performance of the proposed approach where instru-ment partial deletion, singing pitch trend estimation, and \nNSHS are applied. MIR−1K ADC20045060708090100\nDatasetsRaw Pitch Accuracy (%)Result of Singing Pitch Extraction\nNSHS−DP\nInstrument partial deletion + DP\nInstrument partial deletion + NSHS−DP\nInstrument partial deletion + Trend estimation +NSHS−DP\nFigure 4 . The results of singing pitch extraction. \n \nhjc2 toos hjc1 rr jjy mw dr2 cl1 cl2 kdproposed dr1 pc405060708090\nMethodsRaw Pitch Accuracy (%)Performance Comparison for Different Methods Using ADC2004 \nFigure 5 . Performance comparison. \n 5. ACKNOWLEDGEMENT \nThis work was conducted under the ”Digital  Life  Sens-ing  and  Recognition  A pplication Technologies  \nProject” of  the  Institute  for  Information Industry which \nis subsidized by the Ministry of Economy Affairs of the Republic of China. \n6. REFERENCES \n[1] G. E. Poliner, D. P. W. Ellis, A. F. Ehmann, E. \nGomez, S. Streich, and B. Ong, \"Melody transcription from music audio: approaches and \nevaluation,\" \nIEEE TASLP , vol. 15, pp. 1247-1256, \n2007. \n[2] L. Regnier and G. Peeter s, “Singing voice detection \nin music tracks using direct voice vibrato detection,” \nIEEE  ICASSP , pp. 1685-1688, 2009. \n[3] C .  L .  H s u ,  L .  Y .  C h e n ,  J .  S .  J a n g ,  a n d  H .  J .  L i ,  “Singing pitch extraction from monaural polyphonic songs by contex tual audio modeling and \nsinging harmonic enhancement”, \nISMIR , pp. 201-\n206, 2009. It is clear that the proposed instrument partial deletion \nand singing pitch trend estimation facilitate extracting \nsinging pitches since its pe rformance improves signifi-\ncantly over the rest of the compared methods in both da-tasets. The raw pitch accuracy of proposed approach achieves 72.57% and 86.67% for MIR-1K and ADC2004, respectively, with the same setting of the parameters de-scribed in previous subsections. Comparing to the MIREX 2009 results shown in Figure 5, the performance of the proposed approach is co mparable to the state of the \nart approaches. [4]\n K. Dressler, “Sinusoidal extraction using an \nefficient implementation of a multi-resolution FFT,” \nDAFx , pp. 247–252, 2006 \n[5] V. Verfaille, C. Guastavino, and P. Depalle, “Perceptual evaluation of vibrato models,” \nProceedings of Conference on Interdisciplinary \nMusicology , 2005. \n[6] E. Prame, “Measurements of the vibrato rate of ten \nsingers,” JASA,  vol. 96, pp. 1979, 1994. \n[7] D. J. Hermes, “Measurement of Pitch by Subharmonic Summation,” \nJASA , vol.83, pp. 257-\n264, 1988. 4. CONCLUSIONS AND FUTURE WORK \nIn this paper, we propose a novel approach for singing \npitch extraction by deleting inst rument partials. It is sur-\nprising that the vocal and inst rument partials can be dis-\ncriminated by only two simple features, and the perfor-mance is also encouraging. Besides, a singing pitch trend \nestimation algorithm is proposed  to enhance the pitch ex-\ntraction accuracy. [8]\n Y. Li and D. L. Wang, “Detecting pitch of singing \nvoice in polyphonic audio,” IEEE ICASSP , pp. 17–\n20, 2005. \n[9] C. L. Hsu and J. S. Jang, “On the improvement of \nsinging voice separation for monaural recordings \nusing the MIR-1K dataset,” IEEE TASLP , volume \n18, pp. 310-319, 2010. \n530\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "When Lyrics Outperform Audio for Music Mood Classification: A Feature Analysis.",
        "author": [
            "Xiao Hu 0001",
            "J. Stephen Downie"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1415540",
        "url": "https://doi.org/10.5281/zenodo.1415540",
        "ee": "https://zenodo.org/records/1415540/files/HuD10.pdf",
        "abstract": "This paper builds upon and extends previous work on multi-modal mood classification (i.e., combining audio and lyrics) by analyzing in-depth those feature types that have shown to provide statistically significant improve- ments in the classification of individual mood categories. The dataset used in this study comprises 5,296 songs (with lyrics and audio for each) divided into 18 mood categories derived from user-generated tags taken from last.fm. These 18 categories show remarkable consistency with the popular Russell’s mood model. In seven catego- ries, lyric features significantly outperformed audio spec- tral features. In one category only, audio outperformed all lyric features types. A fine grained analysis of the signifi- cant lyric feature types indicates a strong and obvious semantic association between extracted terms and the cat- egories. No such obvious semantic linkages were evident in the case where audio spectral features proved superior.",
        "zenodo_id": 1415540,
        "dblp_key": "conf/ismir/HuD10",
        "keywords": [
            "multi-modal",
            "mood classification",
            "audio",
            "lyrics",
            "feature types",
            "statistically significant",
            "improvements",
            "dataset",
            "last.fm",
            "Russell’s mood model"
        ],
        "content": "WHEN LYRICS OUTPERFORM AUDIO FOR MUSIC MOOD \nC\nLASSIFICATION: A FEATURE ANALYSIS \nXiao Hu J. S tephen Downie \nGraduate School of Library and Information Science \nUniversity of Illinois at Urbana-Champaign \nxiaohu@illinois.edu  jdownie@illinois.edu  \nABSTRACT \nThis paper builds upon and extends previous work on \nmulti-modal mood classification (i.e., combining audio \nand lyrics) by analyzing in-depth those feature types that \nhave shown to provide statistically significant improve-\nments in the classification of individual mood categories. \nThe dataset used in this study comprises 5,296 songs \n(with lyrics and audio for each) divided into 18 mood \ncategories derived from user-generated tags taken from \nlast.fm. These 18 categories show remarkable consistency \nwith the popular Russell’s mood model. In seven catego-\nries, lyric features significantly outperformed audio spec-\ntral features. In one category only, audio outperformed all \nlyric features types. A fine grained analysis of the signifi-\ncant lyric feature types indicates a strong and obvious \nsemantic association between extracted terms and the cat-\negories. No such obvious semantic linkages were evident \nin the case where audio spectral features proved superior.  \n1. INTRODUCTION \nUser studies in Music Information Retrieval (MIR) have \nfound that music mood is a desirable access point to mu-\nsic repositories and collections (e.g., [1]). In recent years, \nautomatic methods have been explored to classify music \nby mood. Most studies exploit the audio content of songs, \nbut some studies have been using song lyrics in music \nmood classification as well [2-4].   \nMusic mood classification studies using both audio and \nlyrics consistently find that combining lyric and audio \nfeatures improves classification performance (See Section \n2.3). However, there are contradictory findings on wheth-\ner audio or lyrics are more useful in predicting music \nmood, or which source is better for individual mood \nclasses. In this paper, we continue our previous work on \nmulti-modal mood classification [4] and go one step fur-\nther to investigate these research questions: 1) Which \nsource is more useful in music classification: audio or lyr-\nics? 2) For which moods is audio more useful and for \nwhich moods are lyrics more useful? and, 3) How do lyr-\nic features associate with different mood categories? An-\nswers to these questions can help shed light on a pro-\nfoundly important music perception question: How does \nthe interaction of sound and text establish a music mood?  \nThis paper is organized as follows: Section 2 reviews related work on music mood classification. Section 3 in-\ntroduces our experimental dataset and the mood catego-\nries used in this study. Section 4 describes the lyric and \naudio features examined. Section 5 discusses our findings \nin light of our research questions. Section 6 presents our \nconclusions and suggests future work.   \n2. RELATED WORK \n2.1 Music Mood Classification Using Audio Features \nMost existing work on automatic music mood classifica-\ntion is exclusively based on audio features among which \nspectral and rhythmic features are the most popular (e.g., \n[5-7]). Since 2007, the Audio Mood Classification \n(AMC) task has been run each year at the Music Informa-\ntion Retrieval Evaluation eXchange (MIREX) [8], the \ncommunity-based framework for the formal evaluation of \nMIR techniques. Among the various audio-based ap-\nproaches tested at MIREX, spectral features and Support \nVector Machine (SVM) classifiers were widely used and \nfound quite effective [9]. \n2.2 Music Mood Classification Using Lyric Features \nStudies on music mood classification solely based on lyr-\nics have appeared in recent years (e.g., [10,11]). Most \nused bag-of-words (BOW) features in various unigram, \nbigram, trigram representations. Combinations of uni-\ngram, bigram and trigram tokens performed better than \nindividual n-grams, indicating higher-order BOW fea-\ntures captured more of the semantics useful for mood \nclassification. Features used in [11] were novel in that \nthey were extracted based on a psycholinguistic resource, \nan affective lexicon translated from the Affective Norm \nof English Words (ANEW) [12].  \n2.3 Multi-modal Music Mood Classification Using \nBoth Audio and Lyric Features \nYang and Lee [13] is often regarded as one of the earliest \nstudies on combining lyrics and audio in music mood \nclassification. They used both lyric BOW features and the \n182 psychological features proposed in the General In-\nquirer [14] to disambiguate categories that audio-based \nclassifiers found confusing. Besides showing improved \nclassification accuracy, they also presented the most sa-\nlient psychological features for each of the considered \nmood categories. Laurier et al. [2] also combined audio \nand lyric BOW features and showed that the combined \nfeatures improved classification accuracies in all four of \ntheir categories. Yang et al. [3] evaluated both unigram \nand bigram BOW lyric features as well as three methods \nfor fusing lyric and audio sources and concluded that le- \nPermission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fee provided that copies \nare not made or distributed for profit or commercial advantage and that \ncopies bear this notice and the full citation on the first page.  \n© 2010 International Society for Music Information Retrieval  \n619\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \nv\neraging lyrics could improve classification accuracy \nover audio-only classifiers.  \nOur previous work [4] evaluated a wide range of lyric \nfeatures from n-grams to features based on psycholinguis-\ntic resources such as WordNet-Affect [15], General In-\nquirer and ANEW, as well as their combinations. After \nidentifying the best lyric feature types, audio-based, lyric-\nbased as well as multi-modal classification systems were \ncompared. The results showed the multi-modal system \nperformed the best while the lyric-based system outper-\nformed the audio-based system. However, our reported \nperformances were accuracies averaged across all of our \n18 mood categories. In this study, we go deeper to inves-\ntigate the performance differences of the aforementioned \nfeature types on individual mood categories. More pre-\ncisely, this paper examines, in some depth, those feature \ntypes that provide statistically significant performance \nimprovements in identifying individual mood categories. \n2.4 Feature Analysis in Text Sentiment Classification \nExcept for [13], most existing studies on music mood \nclassification did not analyze or compare which specific \nfeature values were the most useful. However, feature \nanalysis has been widely used in text sentiment classifica-\ntion. For example, a study on blogs, [16] identified dis-\ncriminative words in blog postings between two catego-\nries, “happy” and “sad” using Naïve Bayesian classifiers \nand word frequency thresholds. [17] uncovered important \nfeatures in classifying customer reviews with regard to \nratings, object types, and object genres, using frequent \npattern mining and naïve Bayesian ranking. Yu [18] \npresents a systematic study of sentiment features in Dick-\nenson’s poems and American novels. Besides identifying \nthe most salient sentiment features, it also concluded that \ndifferent classification models tend to identify different \nimportant features. These previous works inspired the \nfeature ranking methods examined in this study. \n3. DATASET AND MOOD CATEGORIES \n3.1 Experimental Dataset \nAs mentioned before, this study is a continuation of a \nprevious study [4], and thus the same dataset is used. \nThere are 18 mood categories represented in our dataset, \nand each of the categories comprises 1 to 25 mood-\nrelated social tags downloaded from last.fm. A mood cat-\negory consists of tags that are synonyms identified by \nWordNet-Affect and verified by two human experts who \nare both native English speakers and respected MIR re-\nsearchers. The song pool was limited to those audio \ntracks at the intersection of being available to the authors, \nhaving English lyrics available on the Internet, and hav-\ning social tags available on last.fm. For each of these \nsongs, if it was tagged with any of the tags associated \nwith a mood category, it was counted as a positive exam-\nple of that category. In this way, one single song could \nbelong to multiple mood categories. This is in fact more \nrealistic than a single-label setting since a music piece \nmay carry multiple moods such as “happy and calm” or \n“aggressive and depressed”.      A binary classification approach was adopted for each \nof the mood categories. Negative examples of a category \nwere songs that were not tagged with any of the tags as-\nsociated with this category but were heavily tagged with \nmany other tags. Table 1 presents the mood categories \nand the number of positive songs in each category. We \nbalanced equally the positive and negative set sizes for \neach category. This dataset contains 5,296 unique songs \nin total. This number is much smaller than the total num-\nber of examples in all categories (which is 12,980) be-\ncause categories often share samples. \nCategory No. of \nsongs Category No. of \nsongs Category No. of \nsongs \ncalm 1,680  angry 254 anxious 80  \nsad 1,178 mournful 183  confident 61  \nglad 749 dreamy 146 hopeful 45  \nromantic 619  cheerful 142 earnest 40  \ngleeful 543  brooding 116  cynical 38  \ngloomy 471  aggressive 115  exciting 30  \n Table 1. Mood categories and number of positive examples \n3.2 M ood Categories \nMusic mood categories have been a much debated topic \nin both MIR and music psychology. Most previous stu-\ndies summarized in Section 2 used two to six mood cate-\ngories which were derived from psychological models. \nAmong the many emotion models in psychology, Rus-\nsell’s model [19] seems the most popular in MIR research \n(e.g., [2, 5]).  \nRussell’s model is a dimensional model where emotions \nare positioned in a continuous multidimensional space. \nThere are two dimensions in Russell’s model: valence \n(negative-positive) and arousal (inactive-active). As \nshown in Figure 1, this model places 28 emotion-\ndenoting adjectives on a circle in a bipolar space subsum-\ning these two dimensions. \n \nFigure 1.  Russell’s model with two dimensions \nFrom Figure 1, we can see that Russell’s space de-\nm\nonstrates relative distances or similarities between \nmoods. For instance, “sad” and “happy”, “calm” and “an-\ngry” are at opposite places while “happy” and “glad” are \nclose to each other. \nThe relative distance between the 18 mood categories \nin our dataset can also be calculated by co-occurrence of \n620\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \ns\nongs in the positive examples. That is, if two categories \nshare many positive songs, they should be similar. Figure \n2 illustrates the relative distances of the 18 categories \nplotted in a 2-dimensional space using Multidimensional \nScaling where each category is represented by a bubble in \na size proportional to the number of positive songs in this \ncategory. \n \nFigure 2.  Distances between the 18 mood categories in \nthe experimental dataset \nThe patterns shown in Figure 2 are similar to those \nf\nound in Figure 1: 1) Categories placed together are intui-\ntively similar; 2) Categories at opposite positions \nrepresent contrasting moods; 3) The horizontal and ver-\ntical dimensions correspond to valence and arousal re-\nspectively. Taken together, these similarities indicate that \nour 18 mood categories fit well with Russell’s mood \nmodel which is the most commonly used model in MIR \nmood classification research.  \n4. LYRIC AND AUDIO FEATURES \nIn [4], we systematically evaluated a range of lyric fea-\nture types on the task of music mood classification, in-\ncluding: 1) basic text features that are commonly used in \ntext categorization tasks; 2) linguistic features based on \npsycholinguistic resources; and, 3) text stylistic features. \nIn this study, we analyze the most salient features in each \nof these feature types. This section briefly introduces \nthese feature types. For more detail, please consult [4].  \n4.1 Features based on N-grams of Content Words \n“Content words” (CW) refer to all words appearing in \nlyrics except function words (also called “stop words”). \nWords were not stemmed as our earlier work showed \nstemming did not yield better results. The CW feature set \nused was a combination of unigrams, bigrams and tri-\ngrams of content words since this combination performed \nbetter than each of the n-gram types individually [4]. For \neach n-gram, features that occurred less than five times in \nthe training dataset were discarded. Also, for bigrams and \ntrigrams, function words were not eliminated because \ncontent words are usually connected via function words \nas in “I love you” where “I” and “you” are function \nwords. There were totally 84,155 CW n-gram features.  4.2 Features based on General Inquirer \nGeneral Inquirer (GI) is a psycholinguistic lexicon con-\ntaining 8,315 unique English words and 182 psychologi-\ncal categories [14]. Each of the 8,315 words in the lex-\nicon is manually labeled with one or more of the 182 psy-\nchological categories to which the word belongs. For ex-\nample, the word “happiness” is associated with the cate-\ngories “Emotion”, “Pleasure”, “Positive”, “Psychological \nwell being”, etc. GI’s 182 psychological features were a \nfeature type evaluated in [4], and denoted as “GI”. \nEach of the 8,315 words in General Inquirer conveys \ncertain psychological meanings and thus were evaluated \nin [4]. In this feature set (denoted as “GI-lex”), feature \nvectors were built using only these 8,315 words. \n4.3 Features based on ANEW and WordNet \nAffective Norms for English Words (ANEW) is another \nspecialized English lexicon [12]. It contains 1,034 unique \nEnglish words with scores in three dimensions: valence (a \nscale from unpleasant to pleasant), arousal (a scale from \ncalm to excited), and dominance  (a scale from submissive \nto dominated). As these 1,034 words are too few to cover \nall the songs in our dataset, we expanded the ANEW \nword list using WordNet [20] such that synonyms of the \n1,034 words were included. This gave us 6,732 words in \nthe expanded ANEW. We then further expanded this set \nof affect-related words by including the 1,586 words in \nWordNet-Affect [15], an extension of WordNet contain-\ning emotion related words. Therefore, this set of 7,756 \naffect-related words formed a feature type denoted as \n“Affe-lex”.  \n4.4 Text Stylistic Features \nThe text stylistic features evaluated in [4] included such \ntext statistics as number of unique words, number of \nunique lines, ratio of repeated lines, number of words per \nminute, as well as special punctuation marks (e.g., “!”) \nand interjection words (e.g., “hey”). There were 25 text \nstylistic features in total. \n4.5 Audio Features \nIn [4] we used the audio features selected by the \nMARSYAS submission [21] to MIREX because it was \nthe leading audio-based classification system evaluated \nunder both the 2007 and 2008 Audio Mood Classification \n(AMC) task. MARSYAS used 63 spectral features: \nmeans and variances of Spectral Centroid, Rolloff, Flux, \nMel-Frequency Cepstral Coefficients (MFCC), etc. Al-\nthough there are audio features beyond spectral ones, \nspectral features were found the most useful and most \ncommonly adopted for music mood classification [9]. We \nleave it as our future work to analyze a broader range of \naudio features.   \n5. RESULTS AND DISCUSSIONS \n5.1 Feature Performances \nTable 2 shows the accuracies of each aforementioned fea-\nture set on individual mood categories. Each of the accu-\n621\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \nr\nacy values was averaged across a 10-fold cross valida-\ntion. For each lyric feature set, the categories where its \naccuracies are significantly higher than that of the audio \nfeature set are marked as bold (at p < 0.05). Similarly, for \nthe audio feature set, bold accuracies are those signifi-\ncantly higher than all lyric features (at p < 0.05). \nCategory CW GI GI-lex Affe-lex  Stylistic Audio \ncalm 0.5905 0.5851 0.5804 0.5708 0.5039 0.6574 \nsad 0.6655 0.6218 0.6010 0.5836 0.5153 0.6749 \nglad 0.5627 0.5547 0.5600 0.5508 0.5380 0.5882 \nromantic 0.6866 0.6228 0.6721 0.6333 0.5153 0.6188 \ngleeful 0.5864 0.5763 0.5405 0.5443 0.5670 0.6253 \ngloomy 0.6157 0.5710 0.6124 0.5859 0.5468 0.6178 \nangry 0.7047 0.6362 0.6497 0.6849 0.4924 0.5905 \nmournful 0.6670 0.6344 0.5871 0.6615 0.5001 0.6278 \ndreamy 0.6143 0.5686 0.6264 0.6269 0.5645 0.6681 \ncheerful 0.6226 0.5633 0.5707 0.5171 0.5105 0.5133 \nbrooding 0.5261 0.5295 0.5739 0.5383 0.5045 0.6019 \naggressive  0.7966 0.7178 0.7549 0.6746 0.5345 0.6417 \nanxious 0.6125 0.5375 0.5750 0.5875 0.4875 0.4875 \nconfident 0.3917 0.4429 0.4774 0.5548 0.5083 0.5417 \nhopeful 0.5700 0.4975 0.6025 0.6350 0.5375 0.4000 \nearnest 0.6125 0.6500 0.5500 0.6000 0.6375 0.5750 \ncynical 0.7000 0.6792 0.6375 0.6667 0.5250 0.6292 \nexciting 0.5833 0.5500 0.5833 0.4667 0.5333 0.3667 \nAVERAGE  0.6172 0.5855 0.5975 0.5935 0.5290 0.5792 \nTable 2.Accuracies of feature types for individual categories \nFrom the averaged accuracies in Table 2, we can see \nt\nhat whether lyrics are more useful than audio, or vice \nversa depends on which feature sets are used. For exam-\nple, if using CW n-grams as features, lyrics are more use-\nful than audio spectral features in terms of overall classi-\nfication performance averaged across all categories. \nHowever, the answer is reversed if text stylistics is used \nas lyric features (i.e., audio works better).  \nThe accuracies marked in bold in Table 2 demonstrate \nthat lyrics and audio have their respective advantages in \ndifferent mood categories. Audio spectral features signif-\nicantly outperformed all lyric feature types in only one \nmood category: “calm”. However, lyric features achieved \nsignificantly better performance than audio in seven di-\nvergent categories: “romantic”, “angry”, “cheerful”, “ag-\ngressive”, “anxious”, “hopeful” and “exciting”.  \nIn the following subsections, we will rank (by order of \ninfluence), and then examine, the most salient features of \nthose lyric feature types that outperformed audio features \nin the seven aforementioned mood categories. Support \nVector Machines (SVM) were adopted as the classifica-\ntion model in [4] where a variety of kernels were tested \nand a linear kernel was finally chosen. In a linear SVM, \neach feature was assigned a weight indicating its influ-\nence in the classification model, and thus the features in \nthis study were ranked by the assigned weights in the \nsame SVM models trained in experiments in [4].  \n5.2 Top Features in Content Word N-Grams \nThere are six categories where CW n-gram features sig-\nnificantly outperformed audio features. Table 3 lists the \ntop-ranked content word features in these categories. \nNote how “love” seems an eternal topic of music regard-less of the mood category! Highly ranked content words \nseem to have intuitively meaningful connections to the \ncategories, such as “with you” in “romantic” songs, \n“happy” in “cheerful” songs, and “dreams” in “hopeful” \nsongs. The categories, “angry”, “aggressive” and “an-\nxious” share quite a few top-ranked terms highlighting \ntheir emotional similarities. It is interesting to note that \nthese last three categories sit in the same top-left quadrant \nin Figure 2.  \nromantic cheerful hopeful angry aggressive  anxious \nwith you i love you ll baby fuck hey \non me night strong i am  dead to you \nwith your ve got i get shit i am change \ncrazy happy loving scream girl left \ncome on for you dreams to you man fuck \ni said new i ll run kill i know \nburn care if you shut baby dead \nhate for me to be i can love and if \nkiss living god control hurt wait  \nlet me rest lonely don t know  but you waiting \nhold and now friend dead fear need \nto die all around dream love don t i don t \nwhy you heaven in the eye hell pain i m \ni ll met coming fighting lost listen \ntonight she says want hurt you i ve never again and  \ni want you ve got wonder kill  hate but you \nlove more than waiting if you want  have you my heart \ngive me the sun i love oh baby love you hurt \ncry you like you best you re my yeah yeah night \nTable 3. Top-ranked content word features for moods \nwhere content words significantly outperformed audio \n5.3 T op-Ranked Features Based on General Inquirer \n“Aggressive” is the only category where the GI set of 182 \npsychological features outperformed audio features with \na statistically significant difference. Table 4 lists the top \nGI features for this category. \nGI Feature E xample Words \nWords connoting the physical aspects of well \nbeing, including its absence blood, dead, drunk, pain \nWords referring to the perceptual process of \nrecognizing or identifying something by means \nof the senses dazzle, fantasy, hear, \nlook, make, tell, view   \nAction words hit, kick, drag, upset \nWords indicating time noon, night, midnight \nWords referring to all human collectivities people, gang, party \nWords related to a loss in a state of well being, \nincluding being upset  burn, die, hurt, mad \nTable 4. Top GI features for \"aggressive\" mood category \nIt is somewhat surprising that the psychological fea-\nt\nure indicating “hostile attitude or aggressiveness” (e.g., \n“devil”, “hate”, “kill”) was ranked at 134 among the 182 \nfeatures. Although such individual words ranked high as \ncontent word features, the GI features were aggregations \nof certain kinds of words. The mapping between words \nand psychological categories provided by GI can be very \nhelpful in looking beyond word forms and into word \nmeanings.  \nBy looking at rankings on specific words in General \nInquirer, we can have a clearer understanding about \nwhich GI words were important. Table 5 presents top GI \nword features in the four categories where “GI-lex” fea-\ntures significantly outperformed audio features.  \n622\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \nromantic  aggressive  hopeful exciting \nparadise baby i’m come \nexistence fuck  been now \nhit let would see \nhate am what up \nsympathy hurt do will \njealous girl in tear \nkill be lonely bounce \nyoung another saw to \ndestiny need like him \nfound kill strong better \nanywhere can there shake \nsoul but run everything \nswear just will us \ndivine because found gonna \nacross man when her \nclue one come free \nrascal dead lose me \ntale alone think more \ncrazy why mine keep \nTable 5. Top-ranked GI-lex features for categories \nwhere GI-lex significantly outperformed audio  \n5.4 T op Features Based on ANEW and WordNet \nAccording to Table 2, “Affe-lex” features worked signifi-\ncantly better than audio features on categories “angry” \nand “hopeful”. Table 6 presents top-ranked features.  \nCategory  Top Features (in order of influence)  \nangry one, baby, surprise, care, death, alive, guilt, happiness, hurt, \nstraight, thrill, cute, suicide, babe, frightened, motherfucker, \ndown, mi sery, mad, wicked, fighting, crazy  \nhopeful wonderful, sun, words loving, read, smile, better, heart, lone-\nly, friend, free, hear, come, found, strong, letter, grow, safe, \ngod, girl, memory, happy, think, dream  \nTable 6. Top Affe-lex features for categories where \nAffe-lex significantly outperformed audio \nAgain, these top-ranked features seem to have strong se-\nm\nantic connections to the categories, and they share \ncommon words with the top-ranked features listed in \nTables 3 and 5. Although both Affe-lex and GI-lex are \ndomain-oriented lexicons built from psycholinguistic re-\nsources, they contain different words, and thus each of \nthem identified some novel features that are not shared by \nthe other.   \n5.5 Top Text Stylistic Features \nText stylistic features performed the worst among all fea-\nture types considered in this study. In fact, the average \naccuracy of text stylistic features was significantly worse \nthan each of the other feature types ( p < 0.05). However, \ntext stylistic features did outperform audio features in two \ncategories: “hopeful” and “exciting”. Table 7 shows the \ntop-ranked stylistic features in these two categories. \nNote how the top-ranked features in Table 7 are all \ntext statistics without interjection words or punctuation \nmarks. These kinds of text statistics capture very different \ncharacteristics of the lyrics from other word-based fea-\ntures, and thus combining these statistics and other fea-\ntures may yield better classification performance. Also \nnoteworthy is that these two categories both have rela-\ntively low positive valence (but opposite arousal) as \nshown in Figure 2. hopeful exciting \nStd of number of words per \nline Average number of unique words \nper line \nAverage number of unique \nwords per line Average repeating word ratio per \nline \nAverage word length Std of number of words per line \nRatio of repeating lines Ratio of repeating words \nAverage number of words per \nline Ratio of repeating lines \nRatio of repeating words Average number of words per line \nNumber of unique lines Number of blank lines \nTable 7. Top-ranked text stylistic features for categories \nwhere text stylistics significantly outperformed audio \n5.6 T op Lyric Features in “Calm” \n“Calm”, which sits in the bottom-left quadrant and has \nthe lowest arousal of any category (Figure 2), is the only \nmood category where audio features were significantly \nbetter than all lyric feature types. It is useful to compare \nthe top lyric features in this category to those in catego-\nries where lyric features outperformed audio features. \nTop-ranked words and stylistics from various lyric fea-\nture types in “calm” are shown in Table 8.  \nCW GI-lex Affe-lex S tylistic \nyou all look float list Standard derivation (std) of  \nrepeating word ratio per line all look eager moral \nall look at irish saviour Repeating word ratio \nyou all i appreciate satan Average  repeating word ratio \nper line burning kindness collar \nthat is selfish pup Repeating line ratio \nyou d convince splash Interjection: “Hey” \ncontrol foolish clams Average number of unique \nwords per line boy island blooming \nthat s curious nimble Number of lines per minute \nall i thursday disgusting Blank line ratio \nbelieve in pie introduce Interjection: “ooh” \nbe free melt amazing Average number of words per \nline speak couple arrangement  \nblind team mercifully Interjection: “ah” \nbeautiful doorway soaked Punctuation: “!” \nthe sea lowly abide Interjection: “yo” \nTable 8. Top lyric features in \"calm\" category \nAs Table 8 indicates, top-ranked lyric words from the \nC\nW, GI-lex and Affe-lex feature types do not present \nmuch in the way of obvious semantic connections with \nthe category “calm” (e.g., “satan”!). However, some \nmight argue that word repetition can have a calming ef-\nfect, and if this is the case, then the text stylistics features \ndo appear to be picking up on the notion of repetition as a \nmechanism for instilling calmness or serenity. \n6. CONCLUSIONS AND FUTURE WORK \nThis paper builds upon and extends our previous work on \nmulti-modal mood classification by examining in-depth \nthose feature types that have shown statistically signifi-\ncant improvements in correctly classifying individual \nmood categories. While derived from user-generated tags \nfound on last.fm, the 18 mood categories used in this \nstudy fit well with Russell’s mood model which is com-\nmonly used in MIR mood classification research. From \nour 18 mood categories we uncovered seven divergent \ncategories where certain lyric feature types significantly \noutperformed audio and only one category where audio \n623\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \no\nutperformed all lyric-based features. For those seven \ncategories where lyrics performed better than audio, the \ntop-ranked words clearly show strong and obvious se-\nmantic connections to the categories. In two cases, simple \ntext stylistics provided significant advantages over audio. \nIn the one case where audio outperformed lyrics, no ob-\nvious semantic connections between terms and the cate-\ngory could be discerned. \nWe note as worthy of future study the observation that \nno lyric-based feature provided significant improvements \nin the bottom-left (negative valence, negative arousal) \nquadrant (Figure 2) while audio features were able to do \nso (i.e., “calm”). This work is limited to audio spectral \nfeatures and thus we also plan on extending this work by \nconsidering other types of audio features such as rhyth-\nmic and harmonic features. \n7. ACKNOWLEDGEMENT \nWe thank The Andrew Mellon Foundation for their fi-\nnancial support.  \n8. REFERENCES \n[1] J. S. Downie and S. J. Cunningham: “Toward a \nTheory of Music Information Retrieval Queries: \nSystem Design Implications.” In Proceedings of the \n1st International Conference on Music Information \nRetrieval (ISMIR’02).  \n[2] C. Laurier, J. Grivolla and P. Herrera: “Multimodal \nMusic Mood Classification Using Audio and \nLyrics,” In Proceedings of the International \nConference on Machine Learning and Applications, \n2008. \n[3] Y.-H. Yang, Y.-C. Lin, H.-T. Cheng, I.-B. Liao, Y.-\nC. Ho, and H. H. Chen: “Toward multi-modal music \nemotion classification,” In Proceedings of Pacific \nRim Conference on Multimedia (PCM’08) . \n[4] X. Hu and J. S. Downie: “Improving mood \nclassification in music digital libraries by combining \nlyrics and audio,” In Proceedings of Joint \nConference on Digital Libraries, (JCDL2010). \n[5] L. Lu, D. Liu, and H. Zhang: “Automatic Mood \nDetection and Tracking of Music Audio Signals,” \nIEEE Transactions on Audio, Speech, and Language \nProcessing , 14(1): 5-18, 2006. \n[6] T. Pohle, E. Pampalk, and G. Widmer: “Evaluation \nof Frequently Used Audio Features for \nClassification of Music into Perceptual Categories,” \nIn Proceedings of the 4th International Workshop on \nContent-Based Multimedia Indexing , 2005. \n[7] K. Trohidis, G. Tsoumakas, G. Kalliris, and I. \nVlahavas: “Multi-Label Classification of Music into \nEmotions,” In Proceedings of the 9th International \nConference on Music Information Retrieval \n(ISMIR’08) . \n[8] J. S. Downie: “The Music Information Retrieval \nEvaluation Exchange (2005-2007): A Window into Music Information Retrieval Research,” Acoustical \nScience and Technology  29 (4): 247-255, 2008. \nAvailable at: http://dx.doi.org/10.1250/ast.29.247 \n[9] X. Hu, J. S. Downie, C. Laurier, M. Bay, and A. \nEhmann: “The 2007 MIREX Audio Music Classifi-\ncation Task: Lessons Learned,” Proceedings of the \nInternational Conference on Music Information Re-\ntrieval (ISMIR’08) . \n[10] H. He, J. Jin, Y. Xiong, B. Chen, W. Sun, and L. \nZhao: “Language Feature Mining for Music Emo-\ntion Classification via Supervised Learning From \nLyrics,”  In Proceedings of Advances in the 3rd In-\nternational Symposium on Computation and Intelli-\ngence (ISICA’08).  \n[11] Y. Hu, X. Chen, and D. Yang: “Lyric-Based Song \nEmotion Detection with Affective Lexicon and \nFuzzy Clustering Method,” In Proceedings of the \n10th International Conference on Music Information \nRetrieval (ISMIR’09) . \n[12] M. M. Bradley and P. J. Lang: “Affective Norms for \nEnglish Words (ANEW): Stimuli, Instruction Ma-\nnual and Affective Ratings,” Technical report C-1. \nUniversity of Florida, 1999. \n[13] D. Yang, and W. Lee: “Disambiguating Music Emo-\ntion Using Software Agents,” In Proceedings of the \n5th International Conference on Music Information \nRetrieval (ISMIR'04) .  \n[14] P. J. Stone: General Inquirer: a Computer Approach \nto Content Analysis.  Cambridge: M.I.T. Press, 1966. \n[15] C. Strapparava and A. Valitutti: “WordNet-Affect: \nan Affective Extension of WordNet,” In Proceed-\nings of the International Conference on Language \nResources and Evaluation,  pp. 1083-1086, 2004. \n[16] R. Mihalcea and H. Liu: “A Corpus-based Approach \nto Finding Happiness,” In AAAI Symposium on \nComputational Approaches to Analysing Weblogs \n(AAAI-CAAW’06).  \n[17] X. Hu and J. S. Downie: “Stylistics in Customer Re-\nviews of Cultural Objects,” In Proceedings of the \n2nd SIGIR Stylistics for Text Retrieval Workshop, \npp.37-42. 2006. \n[18] B. Yu: “An Evaluation of Text Classification \nMethods for Literary Study,” Literary and Linguistic \nComputing , 23(3): 327-343, 2008. \n[19] J. A. Russell: “A Circumplex Model of Affect,” \nJournal of Personality and Social Psychology , 39: \n1161-1178, 1980. \n[20] C. Fellbaum: WordNet: An Electronic Lexical Data-\nbase, MIT Press, 1998. \n[21] G. Tzanetakis: “Marsyas Submissions to MIREX \n2007”, available at http://www.music-\nir.org/mirex/2007/abs/AI_CC_GC_MC_AS_tzaneta\nkis.pdf \n624\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "Automatic Characterization of Digital Music for Rhythmic Auditory Stimulation.",
        "author": [
            "Eric Humphrey"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1418135",
        "url": "https://doi.org/10.5281/zenodo.1418135",
        "ee": "https://zenodo.org/records/1418135/files/Humphrey10.pdf",
        "abstract": "A computational rhythm analysis system is proposed to characterize the suitability of musical recordings for rhyth- mic auditory stimulation, a neurologic music therapy tech- nique that uses rhythm to entrain periodic physical motion. Current applications of RAS are limited by the general in- ability to take advantage of the enormous amount of dig- ital music that exists today. The system aims to identify motor-rhythmic music for the entrainment of neuromuscu- lar activity for rehabilitation and exercise, motivating the concept of musical “use-genres.” This work builds upon prior research in meter and tempo analysis to establish a representation of rhythm chroma and alternatively describe beat spectra.",
        "zenodo_id": 1418135,
        "dblp_key": "conf/ismir/Humphrey10",
        "keywords": [
            "computational rhythm analysis system",
            "characterize suitability",
            "musical recordings",
            "rhythmic auditory stimulation",
            "neurologic music therapy",
            "motor-rhythmic music",
            "entrainment of neuromuscular activity",
            "rehabilitation and exercise",
            "motivating concept",
            "musical use-genres"
        ],
        "content": "AUTOMATIC CHARACTERIZATION OF DIGITAL MUSIC FOR\nRHYTHMIC AUDITORY STIMULATION\nEric Humphrey\nMusic Engineering Technology Group\nUniversity of Miami\nCoral Gables, FL 33124\nhumphrey.eric@gmail.com\nABSTRACT\nA computational rhythm analysis system is proposed to\ncharacterize the suitability of musical recordings for rhyth-\nmic auditory stimulation, a neurologic music therapy tech-\nnique that uses rhythm to entrain periodic physical motion.\nCurrent applications of RAS are limited by the general in-\nability to take advantage of the enormous amount of dig-\nital music that exists today. The system aims to identify\nmotor-rhythmic music for the entrainment of neuromuscu-\nlar activity for rehabilitation and exercise, motivating the\nconcept of musical “use-genres.” This work builds upon\nprior research in meter and tempo analysis to establish a\nrepresentation of rhythm chroma and alternatively describe\nbeat spectra.\n1. INTRODUCTION\nDigital multimedia is now an integral, and somewhat in-\nescapable, aspect of modern life. Personal handheld de-\nvices are designed to streamline the acquisition, manage-\nment and playback of large volumes of content as cutting-\nedge computing devices approach ubiquity. This trend, in\ntandem with the commercial success of devices like the\niPod and iPhone, has encouraged an environment where\nboth content providers and end-consumers have access to\nenormous digital music collections. As a result, individ-\nuals are consuming and purveying more music than ever\nbefore and this realization introduces the classic logisti-\ncal issue of content navigation; when a library becomes\nsufﬁciently large, more complex paradigms must be devel-\noped to facilitate the searching, indexing, and retrieval of\nits items.\nConventional music library systems employ metadata\nto organize the content maintained within them, but are\ntypically limited to circumstantial information regarding\neach music track – such as the artist’s name or the year\nit was produced – in addition to the somewhat amorphous\nattribute of genre. Understandably, stronger information\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2010 International Society for Music Information Retrieval.concerning the speciﬁc nature of a track allows for more\ninsightful and context-driven organizations or queries of a\nlibrary.\nThe need for content-speciﬁc metadata introduces the\nchallenge that someone, or something, must extract the rel-\nevant information necessary. One approach, like the one\ntaken by the Music Genome Project, is to manually anno-\ntate a predetermined set of attributes by a diligent group\nof human listeners, a scheme with clear beneﬁts and draw-\nbacks. While this method is substantiated by the obser-\nvation that no computational system has yet matched it-\nsreliability, it simply takes a human listener far too much\ntime to parse music. As an example, it would require about\n68 years to listen to every track currently available in the\niTunes Store,1which now contains some 12 million tracks.\nNeedless to say, the development of computational al-\ngorithms to extract meaningful information from digital\nmusic provides the ability to process content as fast as an\nimplementing machine can manage. Many efforts over the\nlast twenty years proceed to these ends in varying levels\nof scope and success. As mentioned however, no single\nsolution has been able to rival the performance and ver-\nsatility of even moderately skilled human listeners. It has\nbeen proposed previously that, in this period of continued\nresearch toward improved machine-listening technologies,\nalgorithms are likely to perform best when developed for a\nspeciﬁc application.\nIt is in this spirit that a computational system is pro-\nposed to characterize the suitability of musical recordings\nfor rhythmic auditory stimulation, a neurologic music ther-\napy technique that uses rhythm to entrain periodic physi-\ncal motion. The remainder of the paper is structured as\nfollows: Section II addresses the background of motor-\nrhythmic music as a use-genre and the physiological moti-\nvations; Section III brieﬂy reviews relevant computational\nmodels of human rhythm perception and details the pro-\nposed system; Section IV explores the evaluation and visu-\nalization of the algorithm results; and Section V discusses\nthe system behavior, observations, and directions of future\nwork.\n1With an average track duration of 3 minutes.\n69\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)2. BACKGROUND\nMusic and motion share a long and intertwined relation-\nship throughout human history. Dance comprised an in-\ntegral role in many ancient civilizations for spiritual and\nsocial purposes and work song served to synchronize the\nphysical labor of crews, as was common on sea-faring ves-\nsels. In modern times, physical exercise is often tightly\ncoupled with music, ranging from joggers with personal\nmedia players to ﬁtness classes.\nMany individuals empirically ﬁnd that music facilitates\nexercise, and recent advances in music therapy and neuro-\nscience give this notion credence. Through an increased\nunderstanding of the underlying mechanisms involved in\na human’s physiological response to music, current knowl-\nedge supports the position that rhythm serves as a powerful\nexternal timing mechanism capable of entraining gait pa-\nrameters and neuromuscular activity [1]. Building upon\nthis principle, rhythmic auditory stimulation (RAS) is “a\nneurological technique using the physiological effects of\nauditory rhythm on the motor system to improve the con-\ntrol of movement in rehabilitation and therapy” [2].\nThe impact of rhythmic auditory stimuli on movement\ncan be summarized as three primary components. Sensory\nmotor control provides priming and timing cues to indi-\nvidual in guiding a motor response. Motor programs are\nthought to be developed in the brain to control complex\nmotor movement, where rhythmic stimuli encourage the\ncreation of more efﬁcient and ﬂuid programs for cyclical\nmovement. Also, RAS supports goal-directed movement\nwhere motion is cued by anticipation, a key musical ele-\nment, rather than by explicit events like heel strikes.\nAppropriate music to achieve RAS, best described as\nmotor-rhythmic, must exhibit certain criteria: a strong beat-\npercept, regular meter, little to no tempo deviation, and\nmaintain a tempo that encourages the desired entrainment\nfrequency, referred to in the literature as an individual’s\nresonant frequency or limit cycle. The ability to succinctly\ndescribe a class of musical content for a speciﬁc applica-\ntion motivates its distinction as a use-genre.\nA fundamental problem faced in RAS-based research\nand applications is the inability to harness the abundance\nof available digital music as external entrainment stimuli,\nas no solution exists to characterize music for this purpose.\nIt is for this reason that nearly all uses of RAS are conﬁned\nto closely-monitored clinical settings that heavily rely on\nhuman supervision to provide, and sometimes compose,\nappropriate motor-rhythmic music. An automated system\nwould not only facilitate the practice of RAS as a clinical\nrehabilitation technique, but also allow the integration of\nRAS methodologies on a signiﬁcantly broader scale, such\nas exercise classes or personal ﬁtness technologies.\nSome previous systems attempt to link the rhythm, and\nmore speciﬁcally the tempo, of music and physical motion\nin the form of running [3]. Each effort, however, incor-\nporates the assumption that all content is accurately and\nsufﬁciently described by a single tempo value. Quickly\nconsidering the great diversity of musical content avail-\nable, it is intuitive to conclude that this is inadequate. Withthese goals in mind, we seek to develop a system capable\nof quantifying the motor-rhythmic attributes of digital mu-\nsic content for use in applications of RAS.\n3. PROPOSED SYSTEM\nComputational rhythm analysis algorithms for digital mu-\nsic recordings have been extensively researched over the\nlast twenty years. Early systems were developed to per-\nform tempo extraction of individual tracks and excerpts to\nascertain a single tempo value, and beat tracking to an-\nnotate the location of musical pulses in a recording, both\nachieving notable success. More recent efforts aim to im-\nprove upon these results by employing alternate mecha-\nnisms to fulﬁll various system tasks or seek to determine\nfurther information, such as meter [4] and beat spectrum\n[5]. A more thorough review of recent leading systems is\nprovided in [6].\nBeing that human rhythm analysis remains the best per-\nforming system, explicit modeling of the human auditory\nsystem would appear to be a viable approach toward the\ndevelopment of a machine-listening algorithm for rhyth-\nmic analysis. By reducing the task of rhythm perception\nto the functional components of the overall biological pro-\ncess, each stage can be approximated computationally. At\nthe most rudimentary level, human rhythm perception is\nachieved in a two-stage process of event perception and\nperiodicity estimation.\nThe idea of determining meaningful events in music\nperception is admittedly a loaded topic. However, a seman-\ntic debate can be mostly avoided in considering that there\nare arguably three orthogonal dimensions in basic music\nperception: rhythmic, tonal and timbral. In the context of\ncharacterizing the suitability of music for RAS, the focus\nof meaningful events can – and should – be constrained\nprimarily to rhythmic, or energy-based, events. Neglecting\nthe other two dimensions serves to emphasize the impor-\ntance of rhythmic content.\nPeriodicity estimation can be computationally achieved\nin a variety of different manners depending on performance\nconcerns, such as causality and complexity. One common\nschool of thought regarding human beat induction claims\nthat the phenomena of felt-beat it is achieved through the\nresonating, or entrainment, of oscillator banks in the brain\nas an interval-period based process [2]. This is a particu-\nlarly attractive option given the correlation between the os-\ncillations of the human body as a dynamic mechanical sys-\ntem during movement and those of a mathematical model.\nCoincidentally, these are essentially the main system\ncomponents presented by Scheirer in [7] and Klapuri et\nal in [4]. Building upon the work outlined therein, the pro-\nposed system proceeds in the following manner: an input\nsignal is ﬁrst decomposed into twenty-two subband com-\nponents via a maximally-decimated ﬁlterbank closely ap-\nproximating the critical bands of the cochlea and rhyth-\nmic events are derived for each. These onset events are\nreduced to a single stream of pulses and periodicity esti-\nmation is performed using a bank of modiﬁed comb-ﬁlter\noscillators. The resulting beat spectra is transformed into\n70\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)61\na human listener would easily discern between the two tones and perceive a\nrhythm, a computer model using only a ﬁlterbank for segregation will not.\nAlternatively, tonal or timbral onsets are not necessarily indicative of\nmotor-rhythmic music, but rather transient energy that can be traced to the\ntransduction of acoustic events in the cochlea.\nWith this in mind, a higher-resolution ﬁlterbank is presented to\ndecompose the acoustic waveform into a more accurate representation of motor\nrhythmic music perception. Approximating the critical bandwidths of the\ncochlea, a multi-level dyadic ﬁlterbank is designed to produce twenty-two\nmaximally decimated channels, as shown in Figure 14.\n!!\"!#$!%!%!#$!#$!%!%!!\"!#$!%!%!!\"!#$!%!%!!\"!#$!%!%\n%&!'(%)*&!'+)*&!'(+),*&!'-)*&!'(.&!'-&!'(-)*&!'%)*&!'(-&!'+),*&!'(%&!'/&!'(0&!'*&!'(/&!'.&!'(*&!'+%&!'(+/&!'+1&!'(+%&!'0&!'(+1&!',*1!'(0,*!'+)%*&!'(+)*&!'+&!'(+)%*&!'0,*!'(+&!'%*1!'(-,*!'/%*!'(,*1!'*11!'(/%*!'-,*!'(*11!'1!'(+%*!'+%*!'(%*1!'!!\"!#$!%!%!!\"!%!!\"!%!#$!%!!\"!#$!%!%!#$!%!!\"!#$!%!%!!\"!%!!\"!%!#$!%!!\"!#$!%!%!#$!%!!\"!#$!%!%!!\"!%!!\"!%!!\"!#$!%!%!#$!%!!\"!%!!\"!%!!\"!#$!%!%!#$!%!#$!%!#$!%!!\"!%!!\"!%23456\nFigure 14. A Perceptually–Motivated Dyadic Filterbank - Diagram of the\nmulti-rate decomposition of an input audio waveform using two complementary\nhalf-band ﬁlters.\nThere are several noteworthy advantages in this signal-decomposition\napproach. While the desire for a high-resolution ﬁlterbank that models humanFigure 1. A perceptually-motivated dyadic ﬁlterbank for\nthe decomposition of an input audio waveform.\nBand Range (Hz) Band Range (Hz)\n1 0 – 125 12 1750 – 2000\n2 125 – 250 13 2000 – 2500\n3 250 – 375 14 2500 – 3000\n4 375 – 500 15 3000 – 3500\n5 500 – 625 16 3500 – 4000\n6 625 – 750 17 4000 – 5000\n7 750 – 875 18 5000 – 6000\n8 875 – 1000 19 6000 – 8000\n9 1000 – 1250 20 8000 – 10000\n10 1250 – 1500 21 10000 – 12000\n11 1500 – 1750 22 12000 – 16000\nTable 1. Frequency ranges for the resulting subband com-\nponents.\nrhythm chroma over time, from which global features are\ncalculated to compactly describe the entirety of a music\ntrack.\n3.1 Cochlear Modeling\nAt this point in time, it is commonly held that the hu-\nman auditory system is reasonably understood so far as the\npoint where electrical signals are encoded and transmitted\nto the brain via the auditory nerve. Most stages prior to\nneural processing though, such as diffraction of the pinnae\nor dynamic compression from the bones of the inner ear,\nare not overly integral to the perception of rhythm. How-\never, the cochlea does perform a coarse frequency decom-\nposition as transduction occurs across the critical bands of\nthe organ. Scheirer observed that the perception of rhythm\nis maintained when amplitude modulating white noise with\nthe envelopes of as few as four subbands of an audio wave-\nform [7]. Therefore, it is proposed that monitoring the ﬂuc-\ntuation of energy in each critical band serves as a reason-\nable approximation of preconscious observation of mean-\ningful rhythmic events.\nMotivated in part by the system developed by Tzane-\ntakis et al [8], a multi-resolution time-domain ﬁlterbank\nis used to decompose an input waveform into twenty-two\nsubbands. Whereas wavelet processing implements com-\nplimentary half-band ﬁlters and a true pyramidal structure,\nthe ﬁlterbank divides frequency content similarly to the\ncochlea, the ranges of which are listed in Table 1 and dia-\ngramed in Figure 1.\nIt is important to note that, given the cascaded nature ofthe structure, non-linear phase distortion introduced by IIR\nﬁlters is unacceptable and errors will propagate differently\nin each band. This is particularly troublesome in the con-\ntext of a system developed to analyze the temporal relation-\nship between events. Therefore, half-band FIR ﬁlters of\nDaubechies’ coefﬁcients are chosen, and appropriate all-\npass ﬁlters are designed to ﬂatten the group delay at each\nsuccessive level to ensure alignment of the resulting sub-\nband components. The accumulative delay and complexity\nof the ﬁlterbank decomposition is mainly dependent on the\nlength of the Daubechies’ ﬁlter shape selected (N = 40 in\nour experiments), though the impact of using different ﬁl-\nter lengths on performance has yet to be explored.\n3.2 Rhythm Event Detection\nFollowing decomposition, each subband signal is processed\nidentically to identify rhythm event candidates. Consistent\nwith [7] and [4], subband envelopes are calculated by half-\nwave rectifying and low-pass ﬁltering each subband wave-\nform with a half-Hanning window, deﬁned by Equations 1\nand 2.\nXHWRk[n] = max(Xk[n];0) (1)\nEk[n] =Nk\u00001X\ni=0XHWRk[n]\u0003Wk[i\u0000n] (2)\nSubband envelopes are then uniformly down-sampled\nto 250 Hz, inﬂuenced by the temporal resolution of the hu-\nman auditory system, and compression is applied to the re-\nsulting signals according to Equation 3. Event candidates\nare calculated by ﬁltering the subband envelopes with the\nCanny operator deﬁned in Equation 4, commonly used in\ndigital image processing for edge detection and ﬁrst ap-\nplied to audio processing in [9]. The frequency response\nof the Canny operator is more desirable than that of a ﬁrst-\norder differentiator, being band-limited in nature and serv-\ning to attenuate high-frequency content.\nECk[n] =log10(1 +\u0016\u0003Ek[n])\nlog10(1 +\u0016)(3)\nC[n] =\u0000n\n\u001b2exp(\u0000n2\n2\u001b2);wheren= [\u0000L;L] (4)\nAt this stage, event candidates effectively represent the\nactivation potential of their respective critical bands in the\ncochlea. Though there are multiple hair cell transduction\ntheories concerning the signiﬁcance of place and rate on\npitch perception, the fact remains that temporal masking\nis caused by the necessary restoration time inherent to the\nchemical reaction associated with neural encoding. Known\nas the precedence effect, sounds occurring within a 50 mil-\nlisecond window–about 10 milliseconds before and 40 mil-\nliseconds behind–are perceived as a single event. This phe-\nnomena is modeled by a sliding window to eliminate im-\nperceptible or unlikely event candidates.\nRhythm event detection concludes with the summation\nof subband events to a single train of pulses and a zero-\norder hold to reduce the effective frequency of the pulses.\n71\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)0 5 10 15−35−30−25−20−15−10−50\nFrequency (Hz)Magnitude (dB)Figure 2. Magnitude response of a typical comb-ﬁlter\n(dashed line) and cascaded with a Canny ﬁlter (solid line).\nA single-sample pulse is the half-wave rectiﬁed counter-\npart to a single period of the highest frequency that can be\nrepresented by the current sampling rate. Rhythmic fre-\nquency content, such as the tactus or felt-beat, typically\nexists on the range of .25–4 Hz (or 30–240 BPM), with\ntatum and metrical levels falling just above and below that\nrange, respectively. Therefore, a zero-order hold of 50 ms\nis applied to band-limit the signal, constraining frequency\ncontent to 20Hz while maintaining the temporal accuracy\nnecessary.\n3.3 Periodicity Estimation\nIn continuing with modeling preconscious rhythm audi-\ntion, periodicity estimation is performed using a set of tuned\ncomb-ﬁlters spanning the frequency range of interest. This\nmethod was pioneered as a computational model of rhythm\ninduction by Scheirer in [7], and has since been incorpo-\nrated in a variety of derivative works due to reliability and\nmodest computational complexity. Importantly, modiﬁca-\ntions are introduced here to improve performance and tai-\nlor the model to better suit the target application.\nUnlike previous systems that aim to set a constant reso-\nnance half-life across each oscillator, we propose that per-\nceived resonance of a pulse train is dependent not on time\nbut the number of pulses observed. It seems intuitive that\na 40 BPM click track at 40BPM should take longer to per-\nceive at the same strength as one at 180 BPM. Though\na more perceptually-motivated method may better capture\nthis nuance, the value of \u000bis set at 0:825 to require a pe-\nriod of regularity before resonating, while maintaining the\ncapacity to track modulated tempi.\nBeat spectra is computed over time for each delay lag T,\nas deﬁned by the comb-ﬁlter difference equation in Equa-\ntion 5, varied linearly from 50–500 samples, inversely span-\nning the range of 30–300 BPM. Each comb-ﬁlter is also\ncascaded with a band-pass ﬁlter – the Canny operator –\nto augment the frequency response of the periodicity es-\ntimation stage. As shown in Figure 2, this attenuates the\nsteady-state behavior of the comb-ﬁlter effectively lower-\ning the noise ﬂoor, while additionally suppressing reso-\nnance of frequency content in the range of pitch perception\nover 20Hz. The Canny ﬁlter is also corrected by a scalar\nmultiplier to achieve a passband gain of 0dB.\nyk[n] = (1 \u0000\u000b)\u0003x[n] +\u000b\u0003yk[n\u0000Tk] (5)\nTimeResonance Period\n  0.02  0.04  0.06  0.08  0.130\n21060\n24090\n270120\n300150\n330180 0Figure 3. Example of a tempogram and chroma for\nbonus5.wav, from the MIREX practice data set.\nInstantaneous tempo is calculated by low-pass ﬁltering\nthe energies of each oscillator over time. Scheirer previ-\nously described this process of determining the energy in\nthe delay line over the length of the resonance period, and\nis analogous to computing an unweighted-average. A Han-\nning window of length Wk, set corresponding to the de-\nlay lag of its respective comb-ﬁlter channel and given in\nEquation 6, serves as an estimation of “resonance mem-\nory.” This time-frequency representation is referred to as\natempogram and estimates perceived tempo strength over\ntime, an example of which is shown in Figure 3.\nRk[n] =1\nWkTk\u00001X\ni=0wk[i]\u0003(yk[n\u0000i])2(6)\n3.4 Chroma Transformation\nAs observed by Kurth et al [5], the duality of pitch and\nrhythm allows the representation of beat spectra in terms of\nchroma. In the same way that all pitches can be described\nas having a height and class, various metrical levels exhibit\na similar relationship. Octave errors, a typical issue faced\nin tempo extraction, are mitigated by eliminating the sub-\njective aspect of rhythm and reducing the task to a purely\nobjective one. Fundamental tempo class is especially im-\nportant to RAS-applications, and is the ultimate focus of\nthe system.\nRhythm chroma is computed by ﬁrst transforming beat\nspectra to a function of frequency, rather than period, scaled\nby the base-2 logarithm and referenced to 30 BPM. Three\ntempo octaves (30–60, 60–120, and 120–240 BPM) are\ncollapsed by summing beat spectra with identical chroma,\nas detailed in Equation 7. Understanding this representa-\ntion is facilitated by plotting amplitude as a function of\nlog2tempo class in the polar coordinate system, shown in\nFigure 3, such that the harmonic structure of a given input\nbecomes readily apparent.\nFor clarity, rhythm chroma consists of a radial ampli-\ntude and an angular frequency, referred to as a class and\nmeasured in units of degrees or radians. The transforma-\ntion from tempo, in BPM, to class, in normalized radians,\nis deﬁned by Equation 8. This is a many-to-one mapping,\nand is not singularly invertible. Visualizing rhythm chroma\nin this alternative manner allows for deeper insight into the\nnature of musical content and the extraction of novel fea-\ntures, and will be discussed in greater detail shortly.\n72\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  0.05  0.1  0.15  0.230\n21060\n24090\n270120\n300150\n330180 0\n  0.02  0.04  0.06  0.08  0.130\n21060\n24090\n270120\n300150\n330180 0Figure 4. Chroma diagrams for a 148 BPM click track,\nbefore and after tempo automation. Note the difference in\nscale and amplitude of the fundamental.\n\tn[!] =1\nLL\u00001X\ni=0Rn[!+ 2\u0019 \u0003k] (7)\n!class = log2BPM\nBPM reference(8)\n3.5 Feature Vector Representation\nA single rhythm chroma is obtained for a track by sum-\nming over time and normalizing by the length. Several key\nfeatures of interest are emphasized by producing a global\nchroma, though this set presented is not intended to be\nexhaustive by any means. Beat strength is effectively de-\nscribed by the amplitude of the largest lobe, and fundamen-\ntal tempo class is given by the angle of this peak. Other\nlobes are actually subharmonics of the fundamental, and\nprovide further information about the rhythmic composi-\ntion. It is important to note that the radius and angle of all\nharmonics, the fundamental as well as the partials, are sig-\nniﬁcant, as they describe what is best referred to as rhyth-\nmic timbre. Amplitude ratios between the fundamental and\nthe various partials serve as a metric of beat salience– the\nclarity of the prevailing rhythmic percept– as well as a con-\nﬁdence interval regarding system reliability.\nAn added beneﬁt of averaging the rhythm chroma is\nfound in the fact that frequency modulations of the funda-\nmental chroma manifest as a widening of the primary lobe.\nDue to the behavior of comb-ﬁlter resonance, tempo devi-\nations will inherently attenuate the amplitude of the funda-\nmental. From these observations, optimal music for RAS\nwill exhibit a large, narrow and clearly-deﬁned fundamen-\ntal with smaller, though still clearly-deﬁned, partials.\n4. EVALUATION\nSince there are, to our knowledge, no previous attempts to\nmathematically quantify the motor-rhythmic attributes of\nmusical content, system behavior is explored for a small\nset of content deﬁned as ground-truths. Initially, we ex-\namine the responses for a constant-tempo click track and a\nfrequency-modulated version of itself. For familiarity, se-\nlect content from the MIREX tempo tracking practice data\nis then processed by the proposed system.\nTimeBPMFigure 5. Image of the tempo automation used to mod-\nulate the tempo of the click track, and the corresponding\nchromagram after analysis.\nThe prominent role of metronomes and click tracks in\npast RAS research is indicative of the fact that they are\nthe most basic form of motor-rhythmic stimuli. A thirty-\nsecond audio click track was created using a sampled clave\nin Propellorhead’s Reason software and the tempo was set\nat 148 BPM. The software also offers the capability of\ntempo automation and allowed for the creation of a sec-\nond, frequency-modulated click track to model an expres-\nsive performance. As shown in Figure 4, the constant-\ntempo click track produces a chroma with clearly deﬁned\nfundamental and several smaller subharmonics, while the\nchroma lobes of the frequency-modulated click track are\nsmeared and roughly half the amplitude. While salient,\ngiven the ratio of the signiﬁcant peaks, the widening of the\nlobes is a direct result of the tempo variance in over time.\nImportantly, a chromagram is shown above the tempo au-\ntomation curve used to modulate the tempo of the click\ntrack in Figure 5. Though the chromagram incurs some\ndelay in tracking the modulation of the click track, the sys-\ntem is able to follow the tempo throughout.\nThough informative and worthwhile examples to con-\nsider, click tracks are not the primary focus of this system\nand it is necessary to also examine the chroma of real mu-\nsic data. For ease of access and familiarity within the re-\nsearch community, musical content is selected from prac-\ntice data available on the MIREX website [10]. The set\nof excerpts contains a variety of different styles, but there\nare two tracks in particular – train8.wav andtrain12.wav\n– that serve as prime examples of what is and what is not\nmotor-rhythmic music.\nFigure 6 shows the chroma for the two separate tracks.\nIt is evident from the diagram that train8.wav, an elec-\ntronic piece by Aphex Twin, is signiﬁcantly more motor-\nrhythmic than train12.wav, an orchestral performance of a\ncomposition by J. S. Bach, with a beat strength nearly 40\ntimes greater in amplitude. Despite the lack of harmonic\ndeﬁnition in the chroma of the orchestral track, this system\nis capable of identifying the correct fundamental class for\nboth excerpts according to metadata provided.\n73\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  0.05  0.1  0.15  0.2  0.2530\n21060\n24090\n270120\n300150\n330180 0\n  0.002  0.004  0.00630\n21060\n24090\n270120\n300150\n330180 0Figure 6. Instances of good (left) and poor (right) motor-\nrhythmic music.\n  0.02  0.04  0.0630\n21060\n24090\n270120\n300150\n330180 0train5.wav\n  0.05  0.1  0.15  0.230\n21060\n24090\n270120\n300150\n330180 0bonus3.wav\nFigure 7. Chroma representations for non-binary meter\ntracks performed in 6=8(left) and 7=8(right).\n5. DISCUSSION\nContent analysis algorithms for the computation of feature-\nspeciﬁc metadata will no doubt play a vital role in the fu-\nture as digital music libraries continue to increase in vol-\nume seemingly without bound. The system presented here\ndetails one such application of a relatively straightforward\nuse-genre that extends previous machine listening efforts.\nThe task of characterizing music for RAS beneﬁts greatly\nfrom the circumstances of the context in which it is used,\nwherein the most relevant attributes of motor-rhythmic mu-\nsic are objectively quantiﬁable.\nFurthermore, representing the global rhythm in terms of\nchroma allows for a compact description of the temporal\nstructure of music. Succinctly stated, the degree of tempo\nvariation inherent in a track inﬂuences both the width and\nheight of the chroma partials. Any music track can be rea-\nsonably approximated as a set of rhythmic partials with\ncorresponding amplitudes, angles, and widths.\n5.1 Future Work\nOne of the more interesting observations to result from\nthis work is the realization that the harmonic structure of\nrhythm chroma may provide information about the meter\nand other time segmentations. Figure 7 shows the global\nchroma of two tracks of note from the MIREX practice\ndata set: train5.wav andbonus3.wav. These tracks are of\nparticular interest as they are not binary meter; the former\nis6=8 and the latter is 7=8. The chroma of train5.wav\nis really only comprised of a fundamental and a closely-\ncompeting subharmonic at a difference angle of about 150\u000e.Alternatively, bonus3.wav is comprised of a variety of sub-\nharmonics, but the partial located 70\u000efrom the fundamen-\ntal is not even remotely present in any other chroma repre-\nsentations observed. More work is necessary to determine\nthe true depth of the information contained within these\ndata.\n6. REFERENCES\n[1] M. Thaut, G. McIntosh, S. Prassas and R. Rice, “Effect\nof Rhythmic Auditory Cuing on Temporal Stride Pa-\nrameters and EMG Patterns in Normal Gait.” Journal\nof Neurologic Rehabilitation, V ol. 4, No. 6, pp. 185–\n190, 1992.\n[2] M. Thaut, Rhythm, Music, and the Brain: Scien-\ntiﬁc Foundations and Clinical Applications. Routledge,\n2008.\n[3] N. Masahiro, H. Takaesu, H. Demachi, M. Oono and H.\nSaito, “Development of an Automatic Music Selection\nSystem Based on Runner’s Step Frequency.” Proc of\nthe 9th Int Conf on MIR, pp. 193–198, 2008.\n[4] A. Klapuri, A. Eronen and J. Astola, “Analysis of\nthe Meter of Acoustic Musical Signals.” IEEE-TSAP,\n2006.\n[5] F. Kurth, T. Gehrmann and M. Muller, “The Cyclic\nBeat Spectrum: Tempo-related Audio Features for\nTime-scale Invariant Audio Identiﬁcation.” Proc of the\n7th Int Conf on MIR, pp. 35–40, 2006.\n[6] M. McKinney, D. Moleants, M. Davies and A. Klapuri,\n“Evaluation of Audio Beat Tracking and Music Tempo\nExtraction Algorithms.” New Music Research, 2007.\n[7] E. Scheirer. “Tempo and Beat Analysis of Acoustic\nMusical Signals.” Journal Acoustical Society of Amer-\nica, 1998.\n[8] G. Tzanetakis and P. Cook. “Musical Genre Classiﬁ-\ncation of Audio Signals.” IEEE-TSAP, V ol. 10. No. 5.\npp. 293–302, 2002.\n[9] L. Lu, D. Liu and H. J. Zhang. “Automatic Mood De-\ntection and Tracking of Music Audio Signals”, IEEE-\nTSAP, V ol. 14, No. 1, pp. 5–18, 2006.\n[10] MIREX Website, [Online]. http://www.music-\nir.org/mirex/2006/index.php/Audio Tempo Extraction.\n74\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "Upbeat and Quirky, With a Bit of a Build: Interpretive Repertoires in Creative Music Search.",
        "author": [
            "Charlie Inskip",
            "Andy MacFarlane",
            "Pauline Rafferty"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1417617",
        "url": "https://doi.org/10.5281/zenodo.1417617",
        "ee": "https://zenodo.org/records/1417617/files/InskipMR10.pdf",
        "abstract": "Pre-existing commercial music is widely used to accom- pany moving images in films, TV commercials and com- puter games. This process is known as music synchronisa- tion. Professionals are employed by rights holders and film makers to perform creative music searches on large catalogues to find appropriate pieces of music for syn- chronisation. This paper discusses a Discourse Analysis of thirty interview texts related to the process. Coded ex- amples are presented and discussed. Four interpretive re- pertoires are identified: the Musical Repertoire, the Soundtrack Repertoire, the Business Repertoire and the Cultural Repertoire. These ways of talking about music are adopted by all of the community regardless of their interest as Music Owner or Music User. Music is shown to have multi-variate and sometimes conflicting meanings within this community which are dynamic and negotiated. This is related to a theoretical feedback model of communication and meaning making which proposes that Owners and Users employ their own and shared ways of talking and thinking about music and its context to determine musical meaning. The value to the music information retrieval community is to inform system design from a user information needs perspective.",
        "zenodo_id": 1417617,
        "dblp_key": "conf/ismir/InskipMR10",
        "keywords": [
            "Music synchronization",
            "Professional music searches",
            "Interview texts",
            "Four interpretive repertoires",
            "Musical repertoire",
            "Soundtrack repertoire",
            "Business repertoire",
            "Cultural repertoire",
            "Dynamic negotiation",
            "Theoretical feedback model"
        ],
        "content": "UPBEAT AND QUIRKY, WITH A BIT OF A BUILD: INTERPRETIVE \nREPERTOIRES IN CREATIVE MUSIC SEARCH \nCharlie Inskip  Andy MacFarlane  Pauline Rafferty  \nDept of Info Science,  \nCity Unive rsity London  \nc.inskip@city.ac.uk  Dept of Info Science,  \nCity University Lo ndon \nandym@soi.city.ac.uk  Dept of Info Studies,  \nUniversity of Abe rystwyth  \npmr@aber.ac.uk  \n \nABSTRACT \nPre-existing commercial music is widely used to acco m-\npany moving images in films, TV commercials and co m-\nputer games. This process is known as music synchronis a-\ntion. Professionals are employed by rights holders and  \nfilm makers to perform creative music searches on large \ncatalogues to find appropriate pieces of music for sy n-\nchronisation. This paper discusses a Discourse Analysis \nof thirty interview texts related to the process. Coded e x-\namples are presented and discussed. Four interpretive r e-\npertoires are identified: the Musical Repertoire, the \nSoundtrack Repertoire, the Business Repertoire and the \nCultural Repertoire. These ways of talking about music \nare adopted by all of the community regardless of their \ninterest as Music Owner or Music User. \nMusic is shown to have multi-variate and sometimes \nconflicting meanings within this community which are \ndynamic and negotiated. This is related to a theoretical \nfeedback model of communication and meaning making \nwhich proposes that Owners and Users employ their own \nand shared ways of talking and thinking about music and \nits context to determine musical meaning. The value to \nthe music information retrieval community is to inform \nsystem design from a user information needs perspective. \n1. INTRODUCTION \nThe record and music publishing industries and artists and \nwriters benefit financially from secondary exploitation of \ntheir copyrights when they are used in films, TV shows, \nadvertising and computer games. This process is known \nas music synchronisation, or „sync‟.  The professional m u-\nsic Users employ specialists to search large catalogues for \npre-existing commercial music in conjunction with the \nOwners‟ in -house specialists. Often these creative music \nsearches are based on an ever-changing written query, or \n„brief‟, which is sometimes accompanied by a moving \nvisual clip or still images. [1] \nThe major Owners have attempted to disintermediate \nthis process somewhat by developing and maintaining \nweb-based applications which search their catalogues . \nThese mainly use controlled vocabularies to explore dat a-\nbases of textual metadata linked to the relevant audio \nfiles. As would be expected, the metadata fields used in \nthese applications include bibliographic information such as Artist, Title, Year and Chart position. Additionally \nthey recognize the need for the Users to search for u n-\nknown items, and include more descriptive domain-based \nfields such as Mood, Genre, Tempo and Subject. Catal o-\nguing is done by hand [2]. \nThis paper presents a Discourse Analysis of thirty face-\nto-face interviews with professionals involved in sync in \nthe UK. The se semi-structured interviews have taken \nplace over a period of two years as part of a wider inve s-\ntigation into the communication processes and inform a-\ntion needs of this group of under-researched creative m u-\nsic searchers. The aim of the paper is to present an anal y-\nsis of these texts which identifies the various interpretive \nrepertoires used by this community of specialist users. A \nrange of ways of talking about music is discussed, derived \nfrom a Discourse Analytic approach. The repertoires are \nadopted throughout the community and no repertoire is \nexclusive to one type of stakeholder. The varying di s-\ncourses represent different ways of constructing reality \nand reveal important factors which may contribute to the \ndesign of music information retrieval systems for the pu r-\npose of music synchronisation. \nPublications discussing qualitative research of user i n-\nformation needs traditionally bemoan the fact that there is \nlittle work in this area. However awareness of user needs \nand behaviour keeps users on the ISMIR radar, even \nthough they are not usually the focus of reported research. \nGenerally focus is on tagging and certain aspects of eva l-\nuation, such as ground truth and playlist evaluation. How-\never in [3] the word „user‟ does not appear in any top ten \nlists for ISMIR paper titles over the ten years of the co n-\nference, nor, indeed, in the top 20 bi-grams from titles \nand abstracts. Nevertheless, a pplying „ music information \nneed‟  or „user behaviour‟  as a query to the ISMIR Cloud \nBrowser [4] does generate a range of relevant work focu s-\ning on user information needs such as [5,6,7]. This paper \nis situated within the user information needs paradigm \nand reflects the call at ISMIR 2009 [8] for the community \nto meet a number of challenges, the first identified being \n“ISMIR needs to more actively encourage the particip a-\ntion of potential users of music-IR systems. ” [8] \nThe next section introduces and describes the meth o-\ndology. This is followed by a summary of the findings \nand some examples of the coding and analytic process. In \nthe final section the implications of the use of these repe r-\ntoires are discussed, applying them to a theoretical model, Permission to make digital or hard copies o f all or part of this work for \npersonal or classroom use is granted without fee provided that copies \nare not made or distributed for profit or commercial advantage and that \ncopies bear this notice and the full citation on the first page.   \n© 2010  Internatio nal Society for Music Information Retrieval  \n655\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \nand suggestions are made as to how this work may be r e-\nlevant to the music information retrieval community. \n2. METHODOLOGY \nIn Discourse Analysis (DA), language is seen to construct \nreality, rather than simply reflect and describe it [9]. \nThere are numerous methodologies under the DA umbre l-\nla, which vary widely in the amount of detail in which \nthey look at the texts being considered [ 10]. Texts may be \nany written or spoken form of interaction, including inte r-\nviews and other documents which are related to the su b-\nject in question. The linguistic approach identifies pauses \nand hesitations and detailed lexicographic units, while the \nsocial psychology approach, used here, seeks to identify \nattitudes, beliefs and attributions [9].  Interpretive re per-\ntoires are described as “a lexicon or register of terms and \nmetaphors drawn upon to characterize and evaluate a c-\ntions and events”  [9:138]. Although there is no „recipe‟ \n[11] for identifying interpretive repertoires [12] there is a \ndeveloping DA literature in the library and information \nstudies and human computer interaction domains [13-18].  \nSince October 2007, 23 professionals directly involved \nin searching for music to accompany moving images have \ntaken part in semi-structured interviews . Seven people \nwere observed while making relevance judgments, three \nof whom had previously been interviewed [ 19]. The sam-\nple was derived using snowball sampling [ 20], where \neach participant in the research recommended a small \nnumber of people to approach for the next interview. This \nmethod allows access to previously hidden communities \nand distances the sample from the researcher‟s preco n-\nceived ideas of who may be relevant. All participants \nwere provided with an explanatory statement detailing \nand contextualizing the research project and gave i n-\nformed consent. Interviews and observations lasted up to \none hour, were recorded digitally and transcribed using \nMS Word. The transcriptions were then imported into \nNVivo software [21] and coded manually by the corre s-\nponding author, ensuring consistency. \nThe objective of the analysis was to identify interpr e-\ntive repertoires within the interview and observation texts, \nhighlighting the ways in which this community of varied-\ninterest stakeholders talk about music. Interpretive repe r-\ntoires are drawn from and used by a wide community of \ninterest. One viewpoint of DA is that no one participant \nwill be consistent in their talk, and the researcher is likely \nto find consistencies and variability not only between \ntexts, which may be expected, but also within them. These \nconsistencies and contradictions are drawn from a variety \nof repertoires which represent different ways of thinking \nabout something [ 11,12], in this case, music. All of the \nparticipants are talking about searching for music in large \ncollections and using music with moving images. Howe v-\ner some of them are rights holders and their intermedi a-ries (Owners) while others are music supervisors and film \nmakers (Users). Each group draws from the other‟s repe r-\ntoires in their music talk. Analyzing these repertoires in \ndetail should identify more than one way of talking about \nmusic, informing work on meaning making in creative \nmusic search. \nFor the purposes of analysis there were two iterations \nof coding. On the first pass examples of „talk about m u-\nsic‟ were identified. These were marked up using the co d-\ning facility in NVivo. Th is enables the researcher to tag \nhighlighted text elements with bespoke codes and then \nextract, sort and analyse data tagged under specific codes \nin order to spot patterns , word and tag frequencies etc. . \nAll the sections of text coded as „talk about music‟ were \nthen examined to determine how music was being d e-\nscribed. Previous work had identified two broad groups of \nfacets used in sync search engines [2] and user sync que-\nries [22]: Bibliographic (content-based) and Descriptive \n(contextual). These facets were used as a starting point for \nthe coding . There seemed to be more of a focus on Bibl i-\nographic data (eg Artist, Title) in the Owners‟ search e n-\ngines while the Users‟ queries were more base d on D e-\nscriptive language (eg Mood, Novelty).  \n3. IDENTIFIED REPERTOIR ES \nThe language within each „talk about music‟ section was \ncarefully considered. This close reading of the transcri p-\ntions brought to light ways of talking about music that did \nnot fit into either Bibliographic or Descriptive talk. It was \nfound that a total of four types of language were consi s-\ntently employed. These were identified by contradictions \nwithin or between texts or signalled by regularly-arising \nmetaphors or phrases. Contradictions can be resolved by \nacknowledging a participant is switching repertoire and \nacknowledging the existence of more than one point of \nview. It is widely agreed in DA that this is a strong ind i-\ncation of interpretive repertoires. The words and phrases \nwere divided into categories based on their themes, and \ncoded within the interview texts (Table 1). Each theme, or \nrepertoire, positions music differently in a users ‟ world \nview. These are presented below as four interpretive r e-\npertoires, which have been named the Musical Repertoire, \nthe Business Repertoire, the Soundtrack Repertoire and \nthe Culture Repertoire. \n3.1 The Musical Repertoire \nIn this repertoire, music is an asset which is created and \nhas identifiable characteristics . The repertoire is ident i-\nfied by the appearance of bibliographic musical keywords \n(Table 1) , such as „artist‟, „title‟, „instrume ntal‟, „lyrics‟ . \nThese familiar facets are commonly used to identify a \npiece of music . However, they relate more to how the \nOwners identify the music in their catalogues than how \nthe musical elements are matched to a visual. Referring to \nan analysis of the Own ers‟ bespoke search engines [ 2] \nthese facets identify a recording or a composition and \n656\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \nhelp to isolate it within a large catalogue of recordings or \ncompositions. The record companies and music publis h-\ners responsible for curating commercial music catalogues \nand exploiting recordings and compositions use these \n„traditional‟ musical library facets when organi zing their \nmaterials.  \n3.2 The Business Repertoire \nIn the Business Repertoire, music is a large collection of \nrecordings which are marketable, contractual and neg o-\ntiable and ha ve monetary value to the Owner . There are a \nnumber of facets relating to music talk that are not imm e-\ndiately obviously musical, but they are important in e x-\nploitation terms nonetheless. These criteria are more co n-\ncerned with business issues relating to signing, exploiting, \nand licensing music and include such keywords as “li-\ncense” and “clearance” ‟. They also employ the words \nused to sell the music to cons umers, such as “ brand ne w”, \nand “ cool”. The size of a catalogue is very important in \nthis repertoire.  \nThere are frequent co-locations of physical metaphors \nwhen the Business Repertoire is used: “work with it ”, “at \nthe coalface ”, “splattering ”, “wall- to-wall” , “throw m u-\nsic up against it ”, “dig it ou t”, “churn up a ton of \nsongs ”, “trawl through a catalogue ”. These physical m e-\ntaphors indicate the way of thinking that music is a phys i-\ncal capital resource for the Owners and Users alike, and \nusing it as such adds value to their commercial activities. \n3.3 The Soundtrack Repertoire \nHere, music is a mood enhancing ingredient inextricably \nlinked to User‟s message being conveyed by moving i m-\nage to viewer / listener . This repertoire differs signifi cant-\nly from the Musical Repertoire. In the Soundtrack Repe r-\ntoire, music is „ upbeat and quirky, with a bit of a build ‟ \nas opposed to „ uptempo and leftfield, with a crescendo ‟. \nIt is „recessive and background‟  rather than „ acoustic \nwith sparse instrumentation ‟. This repertoire reflects the \nway in which the music functions when it is synchronized \nwith the music, and the goal of the film maker in this \nprocess. It predominates in user queries [22] but also a p-\npears in interviews across the stakeholder spectrum. \n3.4 The Cultural Repertoire \nFinally, music is represented as being a subjective a p-\npealing distraction which is personal and emotive . The \npiece of film has a final audience, which also includes the \nparticipants in this process in their recreational lives co n-\nsuming the media they are involved in creating. As rec r-\neational consumers themselves they often bring less „pr o-\nfessional‟ music talk to these discussions, indicating they \nare enthusiastic fans of the cultures of music and film: \nThese purely subjective evaluations of media content a p-\npear throughout the texts and are an important way of \ncommunicating the meaning and value of a piece of m u-\nsic, film, or the combination of the two. It is marked by a \nfrequent trope: „when it works, it works ‟, „you just know ‟, \nor „it‟s gut instinct ‟. This phrase arises throughout the i n-\nterviews in response to the ques tion „ what makes a great \nsync? ‟  These repertoires are summarised in Table 1 (below) \nalongside examples of nouns, phrases and adjectives \nwhich help to identify the repertoire in the data: \nRepertoire  Keywords  \nMusical Repertoire :  \nMusic is an asset which is \ncreated, and has identif iable \ncharacteristics.  Artist, song title, wri ter, \nyear, album title, chart \nposition, genr e, key-\nword, tempo, ly rics, \nmood, subject, v ocal \nmix / instrume ntal \nBusiness Repertoire : \nMusic is a large colle ction \nof recordings which are \nmarketable, contra ctual and \nnegotiable and ha ve mone-\ntary value to the Owner . Brand new, cool, big \ncatalogue, compr ehen-\nsive, demographic, one \nstop, originating terr i-\ntory, physical  \nSoundtrack Repertoire : \nMusic is a mood enhan cing \ningredient inextricably \nlinked to User‟s me ssage \nbeing conveyed by moving \nimage to viewer / listener.  Effervescent, uplif ting, \nrecessive, the me, build, \nquirky, une xpected, f a-\nmiliar, theme, bac k-\nground, match the m u-\nsic to the picture  \nCultural Repertoire : Mu-\nsic is a subjective a ppealing \ndistraction which is pe rson-\nal and emotive  Like it, opinion, bri l-\nliant, great, hate it , it \njust works, gut feeling,  \ninstinct  \nTable 1  Talk about music - interpretive repertoires \n4. REPERTOIRE ANALYSIS \n4.1 Extract 1 \nAn example of coded text can be seen in Appendix 8.1. It \ncan be seen from this extract that the participant is using a \nrange of approaches in her music talk. She is a synchron i-\nsation manager in a music publishing company (Owner) \nand her role is to secure syncs for the music in the cat a-\nlogue she represents. Her answer to the question: \n“How do you then match those to the briefs that you \nare sent and how do you promote them to to your p o-\ntential cli ents?” \nincorporates all four repertoires, which in the extract are \ntagged as <MR> (Musical Repertoire), <BR> (Business \nRepertoire, <SR> (Soundtrack Repertoire) and <CR> \n(Cultural Repertoire). (The colour coding used in NVivo \nhas been translated in this paper into XML-type codes for \nease of explanation and reproduction). In the BR firstly \nshe identifies her business resource, the physical “ dedi-\ncated music server ”, which contains a database of her co l-\nlection, which is “ quick ” and efficient ( “the most opt i-\nmum way” ) and refers to the physical acts of making cds \nand putting mp3s on an ftp site.  \nShe switches to SR, using the film makers ‟ special la n-\nguage of “briefs ”, “visuals ”, “matching the music to pi c-\nture” and “marry it up ”. Although it is not specifically her \nrole to match the music to the moving image it is fr e-\nquently described by participants as their preferred way of \ndetermining relevance. Incorporating this SR act in her \ndiscourse indicates an und erstanding of “ the other side” , \n657\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \ntheir way of thinking and working. Indeed she has work \nexperience in the film world and is therefore in a position \nto adopt repertoires representing different interests.  \nThe CR is clearly identifiable through the use of the \nsubjective opinion-oriented comments of “I think… ” \n(“…are going to work / fit  / appropriate ”). This repertoire \npresents the idea that the „fit‟ between music and film is \nvery subjective, and allows the User to make the final d e-\ncision. Forcing a piece of music on a User ( “this is the \none for you ”) arises throughout the interviews as a bad \napproach, whereas a subtle negotiation approach or “let-\nting the user decide / discover”  is preferred. The CR a l-\nlows this deference without devaluing the knowledge and \nexpertise of speaker and puts them in a safe position if the \nfinal choice is not successful or popular, distancing them \nfrom unpopular decisions. \nThe participant‟s use of MR in this section discusses \nthe key elements of the musical content of specific \n“songs”, including lyrics (“ words ”), genres (“ rock”, \n“pop”) and instrumentation (“ acoustic instrumentals ”). \nUnsurprisingly these facets appear throughout the texts \nand are used widely by the participants. Technical mus i-\ncal terms, however, such as melody, harmony, key, or \nrhythm are rarely mentioned. The MR is more focused on \nhigher level bibliographic metadata than technical mus i-\ncal content. This widespread use of layman‟s musical la n-\nguage enables easy communication between all parties \nand stakeholders regardless of their musical expertise. It \nconsists of easily identified facets which are used to o r-\nganize rights holders ‟ collections rather than more tec h-\nnical film or musical terms used in the SR, or the marke t-\ning-based language of the BR. \n4.2 Extract 2 \nHere (Appendix 8.2) a different participant (019SYN) \ndiscusses ” What makes a great sync ”. He draws from the \nCR and BR in his answer, switching quickly from one to \nthe other. Although he appears to believe that a “ great \nsync” is one that “ works perfectly with that film ” he fully \nacknowledges that there are other factors which come into \nplay from the BR, including “ cost”, “politics ”, “the PR \nand the story ”. Again, combining these repertoires just i-\nfies and explains self-contradiction and acknowledges the \nwide variety of factors that impact on the choice of music \nin this process. Although he initially aligns himself with \nthe CR, presenting the BR as an unpleasant but necessary \nfact of life, he reinforces his professional standing by a c-\nknowledging the importance of market-based factors to \nsuccessful synchronisation.  \n5. DISCUSSION \n5.1 Meaning-making \nThese repertoires combine dynamically to determine mu s-\nical meaning within this community. Music for synchron i-\nsation is not purely an abstract art form. It has commercial \nvalue, and can be bought and sold, negotiated and \ncleared; it has physicality, weight and volume; it is an \nidentifiable unique item in a large collection or an amorphous mass of a collection itself; it is defined by the \nfactors around its creation, the artist, the date, or it is d e-\nfined by its effect on the mood or even purchasing activity \nof the listener / viewer; it is personal and subjective or it \nis a perfect match.  \nAlthough there is often some emphasis on one or \nanother of the repertoires, each of the participants a c-\nknowledges this range of meanings in their music talk. \nThese repertoires can be used to identify their Codes \n(ways of looking at music) and Competences (ways of \nlooking at the world) [23]. Indeed, Owner Codes mainly \ndraw from MR, User Codes from SR while Owner Co m-\npetences relate more closely to BR and User Comp e-\ntences to CR (see Fig 1, below). \n \n     \n  \nCodes    Competences    \n  \n  \nOwner     \n  \nUser    Encoding    Decoding    \nDecoding    Encoding    Business    \nRepertoire    Cultural    \nRepertoire    \nSoundtrack    \nRepertoire    Musical    \nRepertoire    \n \nFigure 1  Repertoires as Codes and Competences \n(adapted from [23 ]) \nThe model in Figure 1 is adapted from [24], suggesting \nthat the meaning making process in music synchronisation \nis a dynamic feedback loop between the Owner and the \nUser. The Owners and Users draw from their own and \nshared Codes and Competences in determining and co m-\nmunicating musical meaning . The results of the DA r e-\nported here reinforce the Codes and Competences aspect \nof the model. The intention is to investigate the Encoding \n/ Decoding process in future analyses. \n5.2 Music Information Retrieval  \nThe value of this work to the wider discipline of Music \nInformation Retrieval is twofold. Firstly, the rich and d e-\ntailed insights into the Repertoires employed within this \ncommunity of users offered by the analysis indicate a \nwide variety of ways of thinking about music. In terms of \ntool and, ultimately, system design, recognizing that m u-\nsic is a multi-variate concept with conflicting features (it \nis abstract and concrete, it is objective and subjective and \nit can be used as part of a multi-media construct while \nstanding alone) is key to successfully meeting user info r-\nmation needs. For example, if these ideas were incorp o-\nrated in the design of a system to find music for sync then \nthe music would not only be described using bibliograp h-\nic metadata (MR) but would incorporate facets from all of \nthe repertoires. It would allow a user to search databases \nfor a selection of thirty second sections of tracks which \nare popular with a specific target audience (BR) , which \nhave  not been used in advertising (SR), have a build (SR) , \nno vocal (or a vocal with a specific lyric which is relevant \nto the commercial‟s message)  (MR), specific instruments \n658\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \nand feels (MR), price ranges and ease of approval (BR), \nand is of a style which is preferred by the stakeholders \n(CR). Much of the BR information can be found in the \nroyalties and business affairs services in Owners systems \nand attempts are being made by some corporations to i n-\ncorporate this into their search applications. Automated \ncontent- based tools such as „crescendo d etectors‟  or „ti m-\nbre identifier s‟ would be of use  for SR and MR , while a u-\ntotagging and playlist-building reflect CR. A holistic a p-\nproach can only benefit industry and the research co m-\nmunity.  \nSecondly, the dynamic element of this process reminds \nus that meaning is not static but relates both to content \nand to ever-changing context. This constant flux means \nthat any research is purely a snapshot of ways of thinking \nand talking about music. As the digital information soci e-\nty develops and music becomes all-pervasive, users and \nsystems become more sophisticated. As the music indu s-\ntry‟s relationship with music is forced by this develo p-\nment to change then the Codes and Competences made \napparent by this analysis are equally likely to develop and \nchange. \n6. CONCLUSION \nThere are appearances through the texts of four repe r-\ntoires. Music appears to have many forms, which are all \nconsidered by all of the participants. Although at first \nglance it may appear that one group of people (the Ow n-\ners) thinks one way while another (the Users) think an oth-\ner, this is not the case. Indeed their views are often sim i-\nlar. The ways of thinking about music in this community \nare more complex. There is certainly some value in an a-\nlyzing the texts for their surface content - indeed this is an \nuseful way to determine key themes and for the researcher \nto get an initial understanding of the dynamics of a multi-\nstakeholder information communications process [1] . \nHowever, although it is time-consuming, applying DA to \nthese texts has revealed patterns that were not already \nclear, given this analysis deeper insight into meaning \nmaking within this community and allowed some testing \nof the theoretical model [ 24]. \n7. ACKNOWLEDGEMENTS \nMany thanks to all the anonymous participants in this r e-\nsearch for being so free with their valuable time and in-\nsight . Charlie Inskip gratefully acknowledges financial \nsupport from AHRC for this PhD research. The anony m-\nous reviewers comments were also invaluable. \n8. APPENDICES \n8.1 Extract 1 \nIn this interview extract (001SYN) it can be seen how the \nparticipant, who works for a rights holder, uses a range of \nrepertoires to make a decision on the relevant piece of \nmusic. Each repertoire example is marked in <>: Question: How do you then match those to the briefs that \nyou are sent and how do you promote them to to your p o-\ntential clients? \nAnswer: <BR>I have all our music on a dedicated music \nserver</BR><SR> so I will get a brief in and quite often \nI‟ll actually get the vi sual in as well so if I have the visual \nup on s creen</SR><BR> I‟ll bring up my music dat a-\nbase </BR><SR>[the visual?]. The visual of the ad, for \ninstance, they‟ll send me the vi sual of the ad, so I‟ve got \nthe 60 second or the 30 second ad in front of me which \nreally helps, because it‟s very different re ading a brief \nand actually seeing how they shoot it. So I‟ll see it \n</SR><BR>then I‟ll bring up my music d atabase and \nthe</BR><MR> songs</MR> that <CR>I think \nwork</CR> <BR>I‟ll pick up</BR> and <MR>I‟ll play \nthe sections of the song</MR> that <CR>I think are \ngoing to fit</CR>. <SR>I‟ll match the music to the pi c-\nture. I‟ll marry it up and see if it works or \nnot.</SR><BR> That‟s the most optimum way of doing \nit </BR><SR>if you get the actual visual in. if I get the \nscript then I‟ll look at the script, </SR><MR> I‟ll see if \nsometimes they‟ll have a keyword search sometimes they \nwant words say sunshine in it, so I‟ll look at <BR>all ou r \nsongs</BR> you know which songs have the word su n-\nshine in </MR>and then <SR>match see </SR><BR> if \npitch those </BR><CR>see if those work</CR> . \n<MR> Or there‟ll be a genre, what kind of style, you \nknow they‟ll say „no rock, no pop, we just want purely \nacoustic instrumentals‟ anything like that, so I‟ll go \nthrough the all the instrumentals that I have in that genre \nand listen to those</MR><BR> and pitch \n</BR><CR> what I think‟s appropriate. </CR> \n<BR>Nowadays I have to say, I used to make up cds and \nsend them out but because of the fast turnaround I email \nmp3s, or I put them onto an ftp site and I say „here [indi s-\ntinct]  here‟s [indistinct]  package you know download \nthese,</BR><CR> these are the songs that I think are \ngoing to work for you</CR>.<BR> And that‟s how I get \nthem out there. Be cause it‟s much quicker now to do that, \nmuch.</BR> \n8.2 Extract 2 \nThis example, features a freelance creative music searcher \nemployed by ad agencies: \nQuestion: ok. Last one. What makes a great sync? \nAnswer: Good question, what makes a great sync? \n<BR>I think the most important thing for me is not to \ncompromise.<BR><CR> It has to be the best piece of \nmusic for that film.</CR><BR>.And away from all the \nother factors around it, ie cost, politics, all those things \nthat come into it,<CR> it has to have that feeling</CR> \nthat no matter where this piece of music has come from, \nno matter how much it costs, no matter who owns it, and \nwho‟s ge tting the money,</BR> <CR>it is the right piece \nfor this film. That‟s the essence, I \nfeel.</CR><BR>Beyond that, I think, other things on top \nof the sync, beyond the sync, can make it a great thing, I \nmean the PR and the story. If it‟s a band that have been \nlaunched off the back of an amazing spot I think that can \n659\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \nalso be really exciting, but that‟s just an added extra. \n</BR><C R>I think it‟s just how that piece of music \nworks perfectly with that film. ..  yes.</C R> \n9. REFERENCES \n[1] C. Inskip, A. MacFarlane & P. Rafferty: “Music, \nMovies and Meaning”, Proceedings of 9th International \nSociety for Music Information Retrieval Conference , \nVienna, Austria, 2008. \n[2] C. Inskip, A. MacFarlane & P. Rafferty: “Organi s-\ning Music for Movies”, Proceedings of International S o-\nciety for Knowledge Organization (UK) Content Arch i-\ntecture conference , London, UK, 22-23 Jun 2009 \n[3] J. Lee, M. Cameron Jones, J.S.Downie: “An anal y-\nsis of ISMIR Proceedings: Patterns of Authorship, Topic \nand Citation”, Proceedings of 10th International Society \nfor Music Information Retrieval Conference , Kobe, J a-\npan, 2009. \n[4] M.Grachten, M.Schedl, T.Pohle, G.Widmer: “The \nISMIR Cloud: A Decade of ISMIR conferences at your \nfingertips”, Proceedings of 10th International Society for \nMusic Information Retrieval Conference , Kobe, Japan, \n2009.  \n[5] D.Bainbridge, S.J.Cunningham, J.S.Downie: “How \nPeople Describe Their Music Information Needs: A \nGrounded Theory Analysis Of Music Queries”, Proceed-\nings of 4th International Society for Music Information \nRetrieval Conference,  Baltimore, 2003 \n[6] S.J.Cunningham & D.Nichols: “Exploring social \nmusic behaviour: an investigation into music search at \nparties”, Proceedings of 10th International Society for \nMusic Information Retrieval Conference , Kobe, Japan, \n2009.  \n[7] J.Lee, J.S.Downie, M.Cameron Jones: “Pr eliminary \nAnalyses of Information Features Provided by Users for \nIdentifying Music”, Proceedings of 8th International S o-\nciety for Music Information Retrieval Conference , Vie n-\nna, Austria, 2007. \n[8] J.S.Downie, D.Byrd, T.Crawford: “Ten Years of \nISMIR: Reflections on Challenges and Opportu nities”, \nProceedings of 10th International Society for Music I n-\nformation Retrieval Conference , Kobe, Japan, 2009. \n[9] J. Potter & M. Wetherell: Discourse and Social \nPsychology,  Sage Publications, London, 1987 \n[10] B.Paltridge: Discourse Analysis , Continuum, \nLondon, 2006 \n[11] C.Antaki, M.Billig, D.Edwards & J.Potter: “Di s-\ncourse analysis means doing analysis”, Discourse Anal y-\nsis Online,  2002 [available at <http://extra.shu.ac.uk/daol/articles/v1/n1/a1/antaki20020\n02-paper.html> last accessed March 2, 2010] \n[12] P.J.McKenzie: “Interpretive Repertoires” chapter \n36 in eds K. Fisher, S. Erdelez & L.McKenzie: Theories \nof Information Behaviour ”, ASIST, Information Today, \nMedford, 2005 \n[13] S. Talja: Music, Culture, and the Library , Scar e-\ncrow Press, Maryland, 2001 \n[14] J. Carlisle: “Digital Music and Generation Y: di s-\ncourse analysis of the online music information behaviour \ntalk of five you ng Australians”, Information Research , \nVol 12, No 4, 2007 \n[15] B. Frohmann: “Discourse Analysis as a R esearch \nMethod in Library and Information Science”, Library and \nInformation Science Research,  Vol 16, pt 2, pp 119-138, \n1994  \n[16] J.M.Budd & D.Raber: “Dis course Analysis: M e-\nthod and Application in the study of information ”, Infor-\nmation Processing and Management , Vol 32, No 2, pp \n217-226, 1996 \n[17] D.Stowell, A.Robertson, N.Bryan-Kinns & \nM.D.Plumbley: “Evaluation of live human -computer m u-\nsic-making: quantitative and qualitative approaches”, In-\nternational Journal of Human Computer Studies , Vol 67, \nNo 11, pp 960-975, 2009 \n[18] C. Inskip, A. MacFarlane & P. Rafferty: “Creative \nProfessional Users‟ Musical Relevance Cr iteria ”, Journal \nof Information Science  In Press, 2010 \n[19] M.Patton: Qualitative evaluation and research \nmethods.  Sage Publications, Newbury Park, California, \n1990  \n[20] NVivo software: http://www.qsrinternational.com/  \nQSR International, 2010 \n[21] C. Inskip, A. MacFarlane & P. Rafferty (2009) \n“Toward s the Disintermediation of Creative Music \nSearch”, Proceedings of ECDL Workshop on Exploring \nMusical Information Spac es, Corfu, Greece, 1-2 Oct 2009 \n[22] P.Tagg: Introductory notes to the Semiotics of \nMusic, version 3,  1999 [internet] (Accessed [07 Dec \n2006]), http://www.tagg.org/xpdfs/semiotug.pdf \n[23] C. Inskip, A. MacFarlane & P. Rafferty (2008) \n“Meaning, communication, music: towards a r evised \ncommunication model”, Journal of Documentation  Vol \n64, No 5, pp 687- 706. \n \n \n \n660\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "Understanding Features and Distance Functions for Music Sequence Alignment.",
        "author": [
            "Özgür Izmirli",
            "Roger B. Dannenberg"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1418353",
        "url": "https://doi.org/10.5281/zenodo.1418353",
        "ee": "https://zenodo.org/records/1418353/files/IzmirliD10.pdf",
        "abstract": "We investigate the problem of matching symbolic representations directly to audio based representations for applications that use data from both domains. One such application is score alignment, which aligns a sequence of frames based on features such as chroma vectors and distance functions such as Euclidean distance. Good representations are critical, yet current systems use ad hoc constructions such as the chromagram that have been shown to work quite well. We investigate ways to learn chromagram-like representations that optimize the classification of “matching” vs. “non-matching” frame pairs of audio and MIDI. New representations learned automatically from examples not only perform better than the chromagram representation but they also reveal interesting projection structures that differ distinctly from the traditional chromagram.",
        "zenodo_id": 1418353,
        "dblp_key": "conf/ismir/IzmirliD10",
        "keywords": [
            "symbolic representations",
            "audio based representations",
            "score alignment",
            "chroma vectors",
            "Euclidean distance",
            "learning representations",
            "classification task",
            "MIDI data",
            "projection structures",
            "traditional chromagram"
        ],
        "content": "UNDERSTANDING FEATURES AND DISTANCE FUNCTIONS \n FOR MUSIC SEQUENCE ALIGNMENT\nÖzgür İzmirli  Roger B. Dannenberg \nCenter for Arts and Technology \nComputer Science Department \nConnecticut College \noizm@conncoll.edu \n \n School of Computer Science \nCarnegie Mellon University \nrbd@cs.cmu.edu  \nABSTRACT \nWe investigate the problem of matching symbolic \nrepresentations directly to audio based representations for applications that use data from both domains. One such application is score alignment, which aligns a sequence of frames based on features such as chroma vectors and distance functions such as Euclidean distance. Good representations are critical, yet current systems use ad hoc constructions such as the chromagram that have been shown to work quite well. We investigate ways to learn chromagram-like representa tions that optimize the \nclassification of “matching” vs. “non-matching” frame pairs of audio and MIDI. New representations learned automatically from examples not only perform better than the chromagram representati on but they also reveal \ninteresting projection structures that differ distinctly from the traditional chromagram. \n1. INTRODUCTION \nScore alignment [4] , score following [3] , chord and key \nrecognition [6, 7] chorus spotting [1, 8], audio-to-audio \nalignment [9, 13] and music structure analysis [2, 11] are \nall tasks where it is useful to compare two segments of music. A common representation for this is the chromagram [1], a sequence of chroma vectors, where \neach vector typically has 12 elements and each element represents the energy corresponding to one pitch class in the spectrum but not necessarily one pitch class in the score. Most algorithms use a distance function in conjunction with the chromagram representation to measure the similarity between frames. While it may be obvious, especially in hindsight, why the chromagram works well in many applications, it should be noted that the chromagram is a contrived representation, and there is no reason to believe it should be optimal. Very little research has been conducted on alternative ways to compare audio to audio let alone audio to symbolic representations. The existing approaches are generally domain specific. For example, in [12] the chromagram is made less timbre dependent by discarding the lower mel-frequency cepstral coefficients  and then projecting the \nremaining coefficients onto the twelve chroma bins. Another example can be found in [15] in which a binary chroma similarity measure is used for alignment in the context of cover song detection. In this work, we explore various ways to derive good features and functions from real data. We specifically look at the problem of score alignment directly from a MIDI representation to audio without going through a synthesized version of the MIDI data. In this paper we give a formulation based on the score alignment task; however, results should be applicable to all other problems that require frame-based comparison. The goal of this work is to gain insight into why the chromagram works in practice and to learn what \nmodifications might make it work even better. Our results suggest that there is room for at least some improvement. \n2. THE SCORE ALIGNMENT TASK \nOur work is aimed at optimizing score alignment: finding a mapping from a symbolic score or standard MIDI file to an audio recording. The basic algorithm transforms \nboth the MIDI file and the audio file into chromagrams A \nand B, which are sequences of chroma vectors. We will \ndenote the chroma vector corresponding to the i\nth time \nframe (column) of A as Ai. Then, construct a distance \nmatrix Di,j = f(Ai, Bj), where f is a distance function. The \nidea is that f  is small when Ai is “similar” to Bj and large \notherwise. Often, f is based on the cosine distance, \ncorrelation distance, or Euclidean distance from Ai to B j. \nThe next step uses dynamic programming to find the lowest-cost path from D\n0,0 to D m-1,n-1 , where m and n are \nthe number of frames in A  and B respectively. \nPath smoothing or constraints may be useful to obtain \neven more accurate alignment. Experience has shown \nthat the chromagram representation for audio, and a chromagram-like representation for MIDI data [9] results \nin a very robust score alignment algorithm. However, the chromagram is an arbitrary choice. There are many other \npossible features, including the spectrum and mel cepstrum, and even the chromagram has parameters including the range of spectral bins considered. How can we search for better representations and distance functions? \n \nPermission to make digital or hard copi es of all or part of this work fo r\npersonal or classroom use is grante d without fee provided that copies\nare not made or distributed for profit or commercial advantage and tha t\ncopies bear this notice and the full citation on the first page.  © 2010 International Society for Music Information Retrieval  \n411\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)3. THE LOG-FREQUENCY SPECTRUM OR \n“SEMIGRAM” REPRESENTATION \nAlthough we are interested in learning better representa-\ntions and distance functions, it would be difficult to learn a relationship between audio and symbolic representations starting from raw signal frames and raw MIDI data. To simplify the representation, we use a magnitude spectrum with bins  logarithmically spaced by \nsemitones (12 bins per octave). The input audio is downsampled to a sampling rate of 11025 Hz. A frame duration of 93 ms with 50% overlap is used. This representation is able to resolve semitone differences in frequency across the spectrum with far less data than the standard magnitude spectrum where each bin has a constant bandwidth. By analogy to the spectrogram, we \ncall this representation the semigram  S: a matrix where \neach column is a semi vector  and each semi vector \nelement represents the magn itude associated with the \nfrequency range of one semitone. We note that the \ntraditional 12-element chromagram can be understood as an octave-folded version of the semigram.  \nFor MIDI data we construct a similar representation, a \nmatrix R (also called a semigram), where each column \nrepresents a time window and each row represents a pitch (key number). If only one note is sounding in the time window at a given pitch, the matrix element is the note’s MIDI velocity. If the note is not on during the entire time window, the velocity is weighted by the fraction of time the note is on. If there is more than one note on at the given pitch, the maximum of the weighted velocities is used. \n4. TRAINING DATA FOR LEARNING \nPROJECTIONS \nOne way to search for good distance functions is simply to attempt alignment with various parameter settings, but this kind of evaluation is difficult. How do we score alignments? And if the chromagram is already robust, then it might take a huge number of examples to find enough failure cases for another method to show improvement. \nAnother possibility is to change the task. In our study, \nwe use a classification task that labels frame pairs as “matching” or “non-matching.” We assume that opti-mizing performance on this task will also be very good for the alignment task. We derive labeled training data (for supervised machine learning) from aligned scores, using 7 orchestra and wind ensemble recordings from one collection and 2 sets of 20 pieces from the RWC classical collection, as listed in Table 1. CLA1 consists of mostly symphonic pieces whereas CLA2  is a random selection \nof pieces with different combinations of instruments. \nThe alignment for the orchestra and wind ensemble \nrecordings was done using chromagrams, but post proc-essed with some spline fitting and smoothing techniques that generally improved the perceptual alignment. The alignment for the RWC pieces are taken from alignment data provided by Ewert, Müller, and Grosche [5]. The alignments of the corresponding scores were verified to be acceptable by listening to the MIDI synthesized versions simultaneously with the original audio. \nFrom the aligned data, it is simple to extract all \nmatching frames. To increase the number of matching frames and reduce overfitting to specific keys, we trans-pose the matching frames up to +6 and –5 semitone steps, thus covering all 12 chromatic degrees. To obtain non-matching frames, we select a random frame from audio for each frame from the MIDI data. These randomly selected pairs will run the gamut from very similar to very different, but for training purposes, we consider them all to be examples of “non-matching.” For the training, the number of “non-matching” frames is equal to the number of “matching” frames including transpositions. All audio listed in the table was used for training, resulting in about 10\n6 matching and the same \nnumber of non-matching frame pairs after transposition. \nTable 1. The training data. \nRecordin g ID Duration (secs. )\nTarantella from Incidental Suite , C. T. Smith TAR 127\nNocturne from Incidental Suite , C. T. Smith NOC 351\nThe Music of Disne yland, arr. b y J. Brubake r DIS 499\nMedieval Le gend, M. Stor y LEG 248\nThe Travelin’ Hat Ra g,D. Bobrowitz HAT 162\nThe Thunderer , J. Sousa THU 148\nRondo from Incidental Suite ,C. T. Smith RON 168\nRWC Classical Music Collection (20 pieces ) CLA1 1182\nRWC Classical Music Collection (20 pieces ) CLA2 1192\n5. LEARNING A FEATURE VECTOR \nAs a preliminary study, to find a good distance function for alignment, we trained a multi-layer perceptron neural network to classify semi vector pairs as “matching” or “non-matching.” The inputs were midi and spectral vectors and the output was trained to be 0 or 1 based on whether the vectors were matching or not. 20 hidden nodes were used. We trained this on a particular set of three pieces: HAT, LEG  and RON. These yielded 92.3% accuracy on the training data . Testing individually we \nobtained HAT: 89.5%, LEG: 93.0%, RON: 90.5%, TAR: 83.8%, NOC: 85.3%, DIS: 87.7 % and THU: 86.9%. We also trained the neural netw ork separately on the 20-piece \nRWC sets and tested on the remaining 36 pieces in that set. We obtained 94.0% a nd 86.4% accuracy for CLA1, \nand 90.2% and 89.0% accuracy  for CLA2 on the training \nand test data respectively. These results showed us that a model of this nature could generalize a matched-unmatched classification quite well with the given input representations. To reiterate our aim in this work, we are interested in understanding why chroma vectors work so well, whether they work better than a trained neural net, and whether variations can work even better. After all, the chroma vector is basically one particular projection from the semi vector. We can  use machine learning to \nexplore the space of projections  and visualize the results \nto gain better understanding of the nature of the projections that work better. \nLet us first write the chroma vector computation as a \nprojection. For MIDI data, we have the p × m semigram \n412\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)R. We define an r × p matrix L (r = 12) that projects each \nsemi vector (column) of R to a chroma vector. Similarly, \nwe define an r × q matrix M to project the audio q x m \nsemigram S to chroma vectors: \n A  = LR (2) \n B  = MS (3) \nMatrices A and B consist of pairs of feature vectors \nresulting from the respective projections in L and M. We \nfirst use this framework with fixed projections and then  generalize the approach by training these projections to better understand their nature and compare them with the commonly used ones. Figure 1  illustrates the standard \nform of M (and similarly for L), which collapses octaves in the semigram to form a 12-element chroma vector. \nNote that the frequency ranges corresponding to the audio semigram (the horizontal axis) are labeled with midi numbers. \n \nFigure 1. The conventional projection from \nsemigram (log-frequency discrete magnitude spectrogram) to chromagram. \nNext, a standard distance is taken between two corre-\nsponding feature vectors to obtain a measure of \nsimilarity. Hence, the required distance for the score \nalignment algorithm in terms of two input semi vectors R\ni \nand Sj is given by f(Ai, Bj)=C(LR i, MS j), where f \nrepresents the desired distance and C is the centered \ncosine distance (found by first removing the means of the \nvectors and then calculating the cosine distance). To \nobtain a binary output (“matche d” or “non-matched”), the \ndistance is compared to a fixe d threshold. This result is \nused for evaluation, but for training, we use the continuous real value as the output and try to train the system to output a zero (0) or one (1) value. \nNow, suppose we generalize the chromagram to allow \nany projection. Although this is not a neural network, the \nback-propagation algorithm can be used to learn weights \nfor the matrices L and M. The basic idea is to evaluate \nthe partial derivative of the output with respect to each \nelement of each matrix. Then , for each training example, \nthe partial derivative for each co efficient is scaled by the \noutput error, multiplied by a small rate parameter, and \nsubtracted from the coeffici ent, thus adjusting each \ncoefficient in a direction that would move the output closer to the correct value. This update is applied to all \nelements in the M and L matrices for all training frame \npairs, and this process is iterated many times until the \noutput converges. Given a large enough dimension r, this \ngradient descent algorithm will normally converge to a \nlocal optimum. We can write C(LR\ni, MS j) as D(x k, R, S) where x k is \nsome element of M or L , letting the remainder of M and \nL be constants for the moment. We can then evaluate \nD(x k + eps, R, S) - D(x k, R, S) to estimate the partial \nderivative of D with respect to x k. The learning algorithm \nis as follows: \n \nwhile convergence criterion not met \n    for all pairs R and S \n        for each parameter indexed by k \n            delta k = D(x k + eps, R, S) - D(x k, R, S) \n            error k = D(x k, R, S) - GT \n            new x k = x k - alpha * error k * delta k \nIn this algorithm, eps is a small number used to \ncalculate the derivative, GT is the ground truth and it has a value of 0 when S matches R and 1 otherwise. The constant alpha is the learning rate. \nThe training can be performed in different ways. \nNormally, both matrices co-learn but it is also possible to fix the weights of one matrix and learn the other. The initial values for both matrices can be assigned to chroma mappings or assigned random va lues. In addition to this, \ndifferent learning rates for the matrices can be set. \nOne advantage of using a linear projection \n(multiplication by L and M) to obtain paired feature \nvectors is that the matrix can be visualized to give some insight as to what features are being used by the system. For example, if the chromagram representation were \noptimal, we would expect L  and M to maintain their \nprojections shown in Figure 1  during training. In \ncontrast, Figure 2  shows the actual result of learning \nmatrix M starting from a chroma mapping. In this \nparticular case, the learned weights are systematically \ndifferent. \n \nFigure 2. The trained matrix M with initial \nchroma pattern and fixed L with chroma projection (as in Figure 1). \n \nFigure 3. The trained M matrix with random initial values and initial chroma pattern for L. \nIn comparison to the preceding two figures, Figure 3  \nshows a trained M where initial weights were random. \nThe L matrix had initial values for the chroma projection \nand was allowed to co-learn with M. The matrix in \nFigure 3  is similar to the chro magram in that each row \n413\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)corresponds to the detection of a different pitch class or \nchroma. In at least some of  the rows, there is a clear \npattern of high weights separated by octaves. \nThe matrix in Figure 2 differs from the chroma \nmapping in several ways. First, the matrix is not symmetric, but this would be expected from the asymmetry of the training data and the nature of the training algorithm. Second, the rows are not just selecting octaves and pitch classes. It has been noted that the chromagram does not really compute the strength of 12 pitch classes because harmonics of the fundamental will generally include energy in bins that are mapped to other pitch classes. Here, we see th at learned rows are selecting \nnot only octave-related frequencies, but some fifths, thirds and other relationships. In fact, the rows are quite similar, at least for the octaves, fifths and major thirds, to pitch histograms for diatonic scales and the Krumhansl template [10] for key finding. This relationship has been \nstudied in [16]. In this study, the empirical profiles have been found to present statistically significant correlations with tonal profiles obtained from human judgments. They demonstrate this by extracting tonal profiles based on covariance analysis of chro ma features computed from \nwestern tonal musical recordi ngs. Similarly, in our case, \nwe find that the rows contain effects of both pitch distributions and overtone strength distributions. It seems likely that all of these factors play a role in determining the optimal patterns. A third property we can observe in the learned matrix is that low and high frequencies seem to have less significance. There is more variation between rows in the middle frequencies. In this work, the note range for the midi semigram was chosen to be from E\n1 to \nD# 7 and the range for the audio semigram was chosen to \nbe E 3 to D# 7. The audio semigram has a shorter note span \nthan the midi semigram b ecause of the time-frequency \ntrade-off for the given time window length, which is kept short in the interest of higher time resolution.  \nWe have learned the matrices many times using \ndifferent training data and different initial conditions. \nIdeally, the matrices would converge to a configuration where the 12 rows represent 12 unique transpositions of some underlying pattern. To test this, we can rotate each row left and right until the correlation with a commonly used pitch distribution, such as the Krumhansl template, is maximized. For this, the pitch-class Krumhansl template is unwrapped to span multiple octaves and is weighted by a Hann window. The choice of the type of pitch distribution is not cr itical because the purpose of \nthe window is only to shift the elements in a row to line up with the other rows.  Figure 4  shows the aligned \nmatrix M, the averages over rows of M  and the weighted \nKrumhansl template used in the alignment. We observe that there is usually a unique shift (modulo 12) for each row of a pattern that is somewhat similar between rows of both matrices. However, there are also some irregularities possibly due to registral pitch effects and co-learning dynamics of the matrices.  \nIt is reasonable to assume that there is some \nunderlying “ideal” pattern that is learned in 12 different transpositions. Next, we test this assumption by forcing all 12 rows to contain the same basic pattern, shifted by 12 different offsets. First, we  average the aligned matrix \nover all rows to find the estimated “ideal” pattern as shown in the middle plot of Figure 4 . We then form a \nnew matrix by copying the “ideal\" pattern into every row and then un-rotating the rows  according to the rotations \nperformed to obtain the aligned matrix. The effect is to force the matrix to a more symmetric configuration and perhaps eliminate any overfitting due to the many degrees of freedom offered by an unconstrained matrix. \nFigure 5  shows the resulting un-rotated matrix. We can \nthen re-evaluate the test data with the new matrices and \ncompare the performance to the trained versions. This shows us how well the single pattern captures the essential information. The evaluations have been carried \nout with this process applied to L and M separately.  \n \n \n \nFigure 4. Matrix M aligned (upper plot). Aver-\nage over rows of the aligned matrix (middle plot) and the weighted multi-octave Krumhansl template (lower plot). The sub-peaks in the middle plot represent major 3\nrds and perfect \n5ths or perhaps 5th and 3rd harmonics. \n \n \nFigure 5. Un-rotated M after averaging over rows of the aligned matrix. \nLearning can alternatively be started from random \nweights in both matrices. In this case, comparable classification accuracy is achieved, however, neither \n414\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)matrix exhibits an easily visualizable structure similar to \nthose seen in the preceding figures.    \n6. EVALUATION \nSeveral different evaluations  have been carried out on \nthe data set. The first evaluation used a group consisting of 4 pieces (\nTAR, NOC , DIS, RON ) for training and the \nremaining three pieces ( LEG, HAT , THU ) for testing. For \ntraining, eleven unique transpositions were added to the original aligned MIDI semigram - audio semigram pairs. The aligned pairs were followed by the same length of random pairs. For testing, four transpositions of the test pieces and a fresh set of random pairs were added to the aligned test set to assess its generalization capability. The classification accuracy of this test is given in the top row \nof Table 2 and is abbreviated TNDR. The table is divided into two groups of 3 columns with the first group showing the accuracy of the model run on the training data itself and the second gr oup showing the accuracy for \nthe test data. Within each gr oup the column labeled ‘LE’ \nshows the results for the learned matrices, ‘LU’ for the aligned, averaged and un-rotated matrices and ‘CH’ for the matrices in standard chroma form (as shown in Figure \n1 for M). A similar evaluation was performed by inter-\nchanging the test and training sets in the evaluation men-\ntioned above. The results are given in the second row of Table 2 with the abbreviation LHT. We also tried using Krumhansl templates (rotated to 12 transpositions) as the rows of the projection matrix, but this did not work as well as the standard chromagram. Although we omit those results here, as a summary, they performed about 3% less than the chroma mapping. \nAnother type of evaluation was carried out by training \non each piece in Table 1 and then testing the alignment function using the remaining si x pieces (hold out testing). \nThe remaining seven rows of Table 2  show the results of \nthis evaluation. \nTable 2. Accuracy for group and hold out tests. \nLE: learned, LU: learned with averaging and un-rotating, CH: chroma.  \n Training Data (% ) Test Data (%) \nRec. LE LU CH LE LU CH \nTNDR  89.6 88.7 85.4 90.5 89.6 87.4 \nLHT 91.3 90.5 87.3 88.9 88.5 85.3 \nTAR 90.0 86.8 83.4 88.2 87.8 86.2 \nNOC 87.6 87.0 83.5 88.9 89.1 86.5 \nDIS 90.8 90.6 87.4 89.0 88.8 85.3 \nLEG 91.9 88.0 88.0 88.1 84.3 85.6 \nHAT 91.4 90.0 83.9 88.9 88.6 86.1 \nTHU 91.6 91.2 88.3 88.5 88.5 85.7 \nRON 91.5 89.4 85.1 88.8 88.9 86.2 \nCLA1 93.1 92.7 90.6 90.0 89.6 88.3 \nCLA2 92.0 91.4 89.2 91.1 90.8 89.1 \n \nOverall, for all the tests explained above, learned \n(LE) and learned averaged (LU) tests performed better \nthan chroma (CH) with one exception in piece LEG \nwhere LU was lower than CH. This shows that averaging in this particular case did not work well and degraded the performance. In general, however, results suggest that an asymmetrical multi-octave chroma mapping is better than the commonly used octave independent symmetrical mapping as suggested by [7] and [14] and others.  \nCLA1 and CLA2 refer to the training data given in \nTable 1. Each of these were tested with the remaining 36 pieces (about 2000 seconds) from the RWC collection. \nThe accuracy numbers partially reflect the effects of \nsome foreseeable factors in performing this evaluation: training alignments are not perfectly aligned at the frame level, the time variation of the spectral content in the audio is not reflected in the MIDI representation (timbre effects), audio contains percussion but the MIDI does not.  \n7. RESULTS \nThe most interesting result is that learned \nrepresentations outperform chroma vectors on the task of discriminating aligned vs . unaligned audio frames. \nPerhaps this should not be too surprising since machine learning from large sets of training data often outperforms hand-tuned algorithms or features. Not only is there nothing “magic” about the chromagram, we see comparable performance from a neural network trained to answer the question “Does this MIDI frame align to that spectral frame?” \nWe also explored a particular model that maps the \nspectrum (and MIDI data) into 12-element vectors and computes similarity between these vectors using a standard distance function. Even though this certainly loses information, it allows us to study the representation, which can be viewed as a pr ojection of the spectrum to a \nnew space defined by a set of basis vectors. These vectors are particularly interesting. With chromagrams, the basis vectors are simply chroma (pitch classes), but \nwith our learned projections, the basis vectors also show a remarkable similarity to pitch histograms obtained from music in a fixed key. Thus, even assuming the general form of the chromagram as a projection from the spectrum to a lower-dimensiona l space, we see room for \nimprovement. This is evident from the fact that a learning \nsystem initialized with the chromagram projection will systematically adjust and improve to a new projection.    \n8. FUTURE WORK \nWe have limited our study to a 12-element vector repre-sentation for comparison to the chromagram. It would be interesting to study larger (and smaller) vectors. In par-ticular, we wonder whether w ith additional dimensions, \nthe learning algorithms would build different patterns for major, minor, and dominant tonalities, whether some patterns would reflect timbre or overtone characteristics, or whether other structures would be formed. The projection matrix formulation of the problem allows these potential structures to be observed. It should be noted that \nthe nature of the M and L matrices is slightly different in \nthat M incorporates the spect ral structure of notes \nwhereas L  deals with notes alone. \nThe similarity of our learned patterns to the pitch his-\ntogram or Krumhansl template deserves further analysis. \n415\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \nIs this a coincidence? Are the learned patterns a \nreflection of the pitch distri butions as well as average \novertone strength distributions in our training data, or have pitch distributions in tonal music evolved to optimize the listener’s ability to recognize music structures? Perhaps both forces are at work. \n9. CONCLUSIONS \nWe have described methods for learning features that are useful for score alignment and other comparative and similarity based tasks such as identification of repeating sections, subsequence searching and template based chord recognition. The learned features out-perform the chromagram representation at  least in the task of \ndiscriminating aligned from non-aligned frames of music. Unlike the chromagram representation, which is a simple projection based on pitch classes, the learned representation uses a projection that appears to be based on pitch distributions as well as the harmonic series common to most pitched musical instruments. In addition, the middle frequencies and pitches receive the most weight in the patterns, indicating that high and low frequencies are less useful for alignment. Another advantage of such an appr oach in MIR is that an \nalignment function can be directly learned from and used with almost native representations in both spectral and symbolic domains, thus bridging the gap between audio and symbolic music collections. We believe this work represents a significant advance by suggesting better features for music audio analysis, particularly for alignment and discovering music structure. \n10. ACKNOWLEDGEMENTS \nThis material is based on work partly supported by the National Science Foundation under Grant Nos. 0534370 and 0855958. We would like to thank Meinard Müller for providing alignment data for the RWC dataset. \n11. REFERENCES \n[1] M. Bartsch and Wakefield, G. H. “Audio Thumb-nailing of Popular Music Using Chroma-based Representations,” IEEE Transactions on \nMultimedia , vol. 7, pp. 96-104, Feb. 2005. \n[2] Dannenberg, R. and Hu, N. “Pattern Discovery Techniques for Music Audio,” Int. Symposium on \nMusic Information Retrieval (ISMIR), Paris: IRCAM, pp. 63-70, 2002. \n[3] Dannenberg, R. and Raphael, C. “Music Score Alignment and Computer Accompaniment,” Commun. ACM,  49(8) (August 2006), pp. 38-43. \n[4] Dixon, S. and Widmer, G. “Match: A Music Alignment Tool Chest,” Int. Symposium on Music \nInformation Retrieval (ISMIR) , London: Queen \nMary, Univ. of London and Goldsmiths College, Univ. of London, 2005. [5] Ewert, S., Müller, M., and Grosche, P. “High Resolution Audio Synchronization Using Chroma Onset Features,” Proc. of IEEE International \nConference on Acoustics, Speech, and Signal Processing (ICASSP), Taipei, Taiwan, pp. 1869-\n1872, 2009. \n[6] Fujishima, T. “Realtime Chord Recognition of Musical Sound: A System Using Common Lisp Music,” Proc. of the 1999 Int. Computer Music \nConference (ICMC), pp. 464-467, 1999. \n[7] Gómez, E., “Tonal Description of Music and Audio Signals,” Ph.D. dissertation, Barcelona: MTG, Universitat Pompeu Fabra, 2006. \n[8] Goto, M. “A Chorus-Section Detection Method for Musical Audio Signals and Its Application to a Music Listening Station,” IEEE Trans. On \nAudio, Speech, and Language Processing, vol. \n14, no. 5, pp. 1783-1794, Sep. 2006. \n[9] Hu, N., Dannenberg, R., and Tzanetakis, G. “Polyphonic Audio Matching and Alignment for Music Retrieval,” Proc. IEEE Workshop on \nApplications of Signal Processing to Audio and Acoustics (WASPAA), New Paltz, USA, pp. 185-\n188, 2003. \n[10] Krumhansl, C. Cognitive Foundations of Musical \nPitch. New York: Oxford Univ. Press, 1990. \n[11] Lu, L., Wang, M., and Zhang, H.-J. “Repeating Pattern Discovery and St ructure Analysis from \nAcoustic Music Data.” Proc. of the 6\nth ACM \nSIGMM International Workshop on Multimedia Information Retrieval. New York: Assoc. for \nComputing Machinery, pp. 275-282, 2004. \n[12] Müller, M., Ewert, S., and Kreuzer, S. “Making Chroma Features more Robust to Timbre Changes,” Proc. of IEEE International Con-\nference on Acoustics, Speech, and Signal Processing (ICASSP), Taipei, Taiwan, pp. 1869-1872, 2009. \n[13] Müller, M., Kurth, F., and Clausen, M. “Audio Matching via Chroma-Based Statistical Features,” Int. Symposium on Music Information Retrieval (ISMIR),  pp. 144-149, Oct. 2006. \n[14] Pauws, S. “Musical Key Extraction from Audio,” Int. Symposium on Music Information Retrieval (ISMIR) , Barcelona, Spain, 2004. \n[15] Serrà, J., Gómez, E., Herrera, P. and Serra, X. “Chroma Binary Similarity and Local Alignment Applied to Cover Song Identification,” IEEE \nTransactions on Audi o, Speech and Language \nProcessing, 16-6, pp. 1138-1152, August 2008. \n[16] Serrà, J., Gómez, E., Herrera, P. and Serra, X. “Statistical Analysis of Chroma Features in West-ern Music Predicts Human Judgments of Tonality,” Journal of New Music Research , 37-4, \npp. 299-309, December 2008. \n \n \n416\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "Querying Improvised Music: Do You Sound Like Yourself?.",
        "author": [
            "Michael O. Jewell",
            "Christophe Rhodes",
            "Mark d&apos;Inverno"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1417227",
        "url": "https://doi.org/10.5281/zenodo.1417227",
        "ee": "https://zenodo.org/records/1417227/files/JewellRd10.pdf",
        "abstract": "Improvisers are often keen to assess how their performance practice stands up to an ideal: whether that ideal is of tech- nical accuracy or instant composition of material meeting complex harmonic constraints at speed. This paper reports on the development of an interface for querying and navi- gating a collection of recorded material for the purpose of presenting information on musical similarity, and the ap- plication of this interface to the investigation of a set of recordings by jazz performers. We investigate the retrieval performance of our tool, and in analysing the ‘hits’ and particularly the ‘misses’, provide information suggesting a change in one of the authors’ improvisation style.",
        "zenodo_id": 1417227,
        "dblp_key": "conf/ismir/JewellRd10",
        "keywords": [
            "Improvisers",
            "assessment",
            "ideal",
            "technical accuracy",
            "instant composition",
            "complex harmonic constraints",
            "speed",
            "interface",
            "querying",
            "navigating"
        ],
        "content": "QUERYING IMPROVISED MUSIC: DO YOU SOUND LIKE YOURSELF?\nMichael O. Jewell, Christophe Rhodes and Mark d’Inverno\nDepartment of Computing\nGoldsmiths, University of London\nNew Cross, London, SE14 6NW\nUnited Kingdom\nfm.jewell,c.rhodes,dinvernog@gold.ac.uk\nABSTRACT\nImprovisers are often keen to assess how their performance\npractice stands up to an ideal: whether that ideal is of tech-\nnical accuracy or instant composition of material meeting\ncomplex harmonic constraints at speed. This paper reports\non the development of an interface for querying and navi-\ngating a collection of recorded material for the purpose of\npresenting information on musical similarity, and the ap-\nplication of this interface to the investigation of a set of\nrecordings by jazz performers. We investigate the retrieval\nperformance of our tool, and in analysing the ‘hits’ and\nparticularly the ‘misses’, provide information suggesting a\nchange in one of the authors’ improvisation style.\n1. INTRODUCTION\nQuery-by-Example systems for musical search offer the\npromise of rich interaction for their users with collections\nof music. The purpose of a search can be goal-driven or\nexploratory, while the musical content being searched can\nbe highly focused (as in a curated collection in a sound\narchive), heterogenous and largely known to the user (a\npersonal collection on a user’s personal music player) or\nheterogenous and largely unknown (an online music ven-\ndor’s catalogue). The ﬁrst Query-by-Example systems [8,\n16] stored their collections in MIDI format; they admitted\naudio queries (hence the ‘Query-by-Humming’ term in the\nMusic Information Retrieval community), and one of the\ntechnical hurdles in those systems was a sufﬁciently ac-\ncurate transcription of the hummed input – and a search\nrelevance ﬁlter that could account for error from imperfect\nhuman humming as well as from imperfect transcription\nalgorithms. This mode of interacting with a collection of\nMIDI-encoded music is available over the web at Musi-\npedia1.\nHowever, for usable systems, Query-by-Example needs\nto be augmented by some means of navigating the collec-\n1http://www.musipedia.org/\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2010 International Society for Music Information Retrieval.tion; typically, that navigation mode is specialized to the\nparticular use case enviseaged (and details of the collection\nbeing investigated); there exist numerous interfaces and vi-\nsualisations of collections (such as [9,12,17,18]) and their\nuse for music discovery has been discussed in tutorial ses-\nsions2.\nAchieving intuitive navigation through collections re-\nquires some kind of notion of similarity (of which there\nare many kinds [2]); for systems using primarily content-\nbased information, this means that the audio features or\ndescriptors must encode not only identity but one or more\nsimilarity relationships at some level of speciﬁcity. Viewed\nfrom this perspective, systems based on audio features for\nclassifying and clustering musical tracks [10, 18] or seg-\nments [1] are Query-by-Example systems, just as are more\nmodern implementations of the original idea (e.g. [7]).\nIn our work, we are interested in both small-scale and\nlarge-scale collections, and in particular at allowing the\nuser to search for and retrieve fragments of tracks (rather\nthan track-to-track or fragment-to-track matches); in prin-\nciple if given a 5-second audio snippet as a query, we con-\nsider all similarly-sized segments in the database – up to\nsome reasonable granularity – as potential matches. This\nmeans that collections of even a small number of tracks\nhave a large number of effective database entries to be\nconsidered. Achieving fast search through large databases\nof musical content has been considered in a few applica-\ntions [14, 15], including the ability to search for speciﬁc\ncontent within a track in a manner which can still be im-\nplemented efﬁciently [3] and can be generalized [6].\nIn this paper, we describe a practical use-case for ex-\nploratorily searching for fragments of audio by similarity\nwithin a small collection. In section 2, we describe in more\ndetail the use case in question; in section 3, we describe\nhow the technology we have developed can meet this need.\nOur preliminary experiments are reported in section 4, and\nwe draw conclusions and suggest further work in section\n5.\n2. CASE STUDY\nIt is often the case that when amateur and semi-professional\nmusicians hear themselves play they cringe at just how\n2e.g.http://musicviz.googlepages.com/home\n483\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)far away they are from being like the professional heroes\nthat have inﬂuenced them. There is a sense of ‘I wish I\ncould sound a bit less like me and more like someone re-\nallygood’. We propose to build a tool that provides a gen-\neral framework for the analysis of performance, allowing\nperformers to both self-analyse and also discover how they\nrelate to their inﬂuences.\nPerformers are often concerned with knowing if their\nplaying has improved in over a time period; whether they\ncan learn about their approach and technique from how\nprofessional musicians play particular phrases; or whether\nthey play differently depending on the instrument, event,\nensemble, etc.\nAs such, we are interested in building a tool that en-\nables performing musicians to analyse certain performance\ncharacteristics. We propose an iterative development cy-\ncle where we increase the scope incrementally in terms of\nwhat performance characteristics may be considered, the\nrange of media, the range of extractors, the type of searches\n(point, track, catalogue) and the options which we make\navailable to a user in the interface. The planned function-\nality includes, but is not limited to, investigating the fol-\nlowing queries:\n1. How do performance characteristics of a musician\ndevelop over time?\n2. How does the performance context (e.g. home record-\ning, studio recording) affect performance character-\nistics?\n3. How does the ensemble (e.g. solo, duo, trio, big band)\naffect performance characteristics?\n4. How does the type of instrument (e.g. in the case\nof piano, grand, upright, electric) affect performance\ncharacteristics?\n5. How do certain performance characteristics compare\nwith great musicians?\n6. How do performance characteristics develop through\na single piece performance?\nOne of the authors is a reasonable jazz pianist (he has\nreceived good reviews in the UK Guardian andObserver\nnewspapers), so we chose to focus on jazz piano perfor-\nmance, with our ultimate goal as being able to ask the\nquestion: ‘How much of a performer’s improvisation is\ngenuinely improvised, and how much is made from stock\npatterns?’\nMany jazz musicians can come up with phrases or ‘licks’\nthat work over chord changes but it is only the greats who\ncan actually approach improvisation as ‘instant composi-\ntion’: where what they play is not only appropriate to the\nsequence but an original passage of notes. The co-author\nwould ideally like to ﬁnd out where the stock patterns arise\nin their playing in order to remove them to free up space\nfor more creative improvisation.3. TECHNOLOGY\n3.1 Similarity Measurement\nThe necessary functionality for our application is the in-\nsertion and storage of numerical audio feature information\nextracted from tracks, and their subsequent searching for\nsimilarity. These two aspects are illustrated in ﬁgure 1: in\nthe left panel, we schematically show a track which has\nhadd-dimensional audio features extracted for a number\nof regions of audio. Subsequently, a user wishes to search\nusing a query of region length sl, so successive feature vec-\ntors are concatenated (illustrated by the arrows in the left\npanel) to arrive at shingled [4] feature vectors (right panel).\nThese shingled feature vectors are then compared against\nthe query by summing squared Euclidean distances, and a\nretrieved list is assembled.\n3.2 Interface\niAudioDB is an application developed for Mac OSX in\nObjective C which provides an intuitive user interface for\nthe creation and exploration of feature databases. As such,\nit binds directly to the audioDB libraries for creation and\nquerying, and employs Sonic Annotator to extract features\nfrom ﬁles provided by the user.\nUsage of iAudioDB follows a straight-forward process,\nwith the interface providing intuitive abstractions to pa-\nrameters where possible. The ﬁrst step is to create the\ndatabase itself, which is achieved via the interface in Fig-\nure 2. The user is prompted for the feature they wish to\nextract, which corresponds directly to the V AMP plugin3\nwhich is used with Sonic Annotator, and then a selection\nof parameters which are database-speciﬁc. The ﬁrst two,\n‘Max Tracks’ and ‘Max Length’ correspond to the number\nof audio ﬁles the user expects to import into the database\nand the maximum length in seconds of those tracks. The\nhop size and window size, equivalent to the step and block\nsize detailed above, are used in conjunction with these val-\nues to determine the initial size (in bytes) of the database.\nFurthermore, the chosen parameters are stored alongside\nthe database to remove the need to enter the settings at the\nimport stage.\nOnce created, the user imports any audio ﬁles, both\nground truth and queries. Aside from a standard ﬁle di-\nalogue, there is no interface for this, as all parameters re-\nquired are obtained at creation time. Multiple ﬁles may be\nselected, and progress is indicated as ﬁles are imported.\nAt this stage, Sonic Annotator extracts feature informa-\ntion as n3-serialized RDF, which is then imported into the\ndatabase. Future increments of the software will see it act-\ning as a V AMP host, allowing the use of extractors via a na-\ntive library. The ﬁlenames of the audio ﬁles are preserved\nalongside the unique keys of the tracks in the audioDB in-\nstance, thus easing the playback process.\nThe query process again has an intuitive user interface,\nshown in Figure 3. The user selects the audio ﬁle they wish\nto use as the query, and from this the length is determined.\nThis length is displayed in the Query Length ﬁelds in units\n3http://vamp-plugins.org/\n484\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)dd\u0002sln\u0000sl+ 1\nFigure 1. Illustration of the construction of concatenated or shingled feature vectors for our search. Note that while in\nprinciple this construction can be done for features over audio regions with temporally-varying extent and step (adjusted to\nthe local tempo), in this paper the step size and block size were kept constant.\nFigure 2. Creating a new database in iAudioDB. The fea-\nture extractor is chosen on the left-hand side, while param-\neters related to the database and the extractor are on the\nright.\nof Vectors and Seconds, and both of these ﬁelds may be\ncustomized by the user to vary the length of the query. The\nﬁelds are dynamically updated, so a change to the seconds\nvalue reﬂects instantly in the vectors value. If desired, the\nlength may also be reset to the full duration of the query\nﬁle. Finally, the user may opt to locate multiple matches\nof a query within the corpus, or to only determine the best\nmatch per track.\nOnce queried, results are displayed in the main appli-\ncation window (see Figure 4). By default, these are sorted\nby ascending distance values, but this may be customized\nby clicking the column headers. The other columns are,\nfrom left to right, a visual indicator of the closeness of\nthe match (though this varies depending on extractor, so\nshould not be used for comparison), the unique key within\nthe audioDB instance, and the position in seconds at which\nthe query occurs in the track. Results may be played in iso-\nlation from the match position, or synchronized with the\noriginal query.\nFigure 3. Querying a database in iAudioDB. The query\nlength is generated dynamically from the query audio ﬁle,\nand may then be customized by the user.\nFigure 4. Results generated from an iAudioDB query.\n485\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)4. FEATURE SPACE INVESTIGATION\nThe ﬁrst step in this investigation was to turn our attention\nto one track and to focus on a single element of the tune.\nThis would at least give us some ground truths whereby we\ncould start to map out a method for getting to our ultimate\ngoal. The track we chose was Looking Up, written by the\nlate great jazz pianist Michel Petrucciani. Speciﬁcally, we\nchose the following performance scenarios that would in\ntime enable us to look at all the issues of our case study:\n1. The co-author at home using the internal microphone\nof a laptop recorded three versions of Looking Up\nsolo on a Kawai grand piano in an informal setting.\nThese were stored as stereo 44100Hz WA V ﬁles.\n2. The co-author again, in the same session, but record-\ning three versions of two other tracks – Ambleside\nDays by John Taylor and My Romance by Rogers\nand Hart. (The signiﬁcance of recording these will\nbecome clear later.) As above, these were stored as\n44100Hz WA V ﬁles.\n3. The co-author again, but recorded in a studio con-\ntext, in trio ensemble and on a Technics electric pi-\nano ten years previously.\n4. The composer of Looking Up and an inﬂuence of the\nco-author, Michel Petrucciani recorded in a concert\non a solo grand piano.\n5. Michel Petrucciani again but in a band context on a\ngrand piano in a live setting.\n6. Another well-regarded pianist and inﬂuence of the\nco-author, Christian Jacob in a trio ensemble, a record-\ning studio with a grand piano.\nTo begin our iterative development cycle for this appli-\ncation, we focus on one speciﬁc phrase in the tune Looking\nUp (the very ﬁrst phrase, an 8-note Mixolydian scale in E).\nThis run appears several times in the piece, though the fre-\nquency and positions vary on a per-recording basis. The\nco-author recorded this phrase ﬁve times in the same set-\nting as 1 and 2 above to build a library of different queries.\nThese query tracks were played at an even tempo, with no\nmissing or mufﬂed notes.\nFrom this set of tracks three feature databases were built,\nall with a step size of 2048 samples (0.046s) and a block\nsize of 16384 (0.372s):\n1. An MFCC feature database with 20 cepstral coefﬁ-\ncients.\n2. A constant-Q feature database with 12 bins per oc-\ntave, a minimum frequency of 65.4064Hz, and a max-\nimum frequency of 1046.5Hz.\n3. A chromagram database with the same bins per oc-\ntave and frequency range as the constant-Q database,\nand a sum of squares accumulation method.\nTrack\nPosition (s)\nMissing Notes\nMufﬂed\nNotes\nRhythm\nAlterations\nChord Additions\nSustain Pedal\nLU115 x\n37\n59 x\n86 x\n162 x x\nLU215 x x\n37\n59 x\n107 x x\nLU316 x x x\n38\n59\n80 x\nTable 1. Locations and comments of fragments corre-\nsponding to our queries in the three single-take recordings\nthrough a laptop microphone.\nThe 3 Looking Up tracks were examined to locate the\npositions of the queried tune and thence act as a ground\ntruth. The resultant locations and notes on these instances\nare shown in Table 1.\nEach feature database was then queried with each of\nthe 5 recorded queries, with a maximum length of 20 vec-\ntors (1.3s). The recordings of My Romance andAmble-\nside Days were used as a boundary, with results examined\nup to the ﬁrst match of a track in this set, and duplicated\nresults were discarded. From this set, it was possible to\ndetermine those which matched the segments in Table 1\nand those which did not. Note that with queries of this\nlength, and with the audio features extracted every 2048\naudio samples, there are over 50,000 candidate matching\npoints in our 9-track database; the fact that we are search-\ning for fragments of track rather than whole tracks enlarges\nthe problem.\nThe mean precision and recall values from these queries\ncan be seen in Table 2, and it is immediately apparent\nthat chromagram features produce the most useful results.\nWhile the precision is not as high as that of the constant-Q\ndatabase the recall is signiﬁcantly improved, and thus of\nmost beneﬁt to this case study, where the user is looking\nfor a variety of similar matches rather than a small number\nof exact matches.\nWithin the results, some notable differences between\nfeature performance were present. Riff instances with muf-\nﬂed notes (15s, 59s, and 86s in Looking Up 1) were located\nin 73% of queries using the chromagram database, 47%\nusing constant-Q, and 20% using MFCCs. Instances with\nrhythm alterations (107s in Looking Up 2 and 16s in Look-\ning Up 3) were found in 100% of queries using the chroma-\ngram database, 50% using constant-Q (matching the Look-\n486\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Feature Precision Recall F-Score\nMFCC 0.89 0.29 0.44\nConstant-Q 1.00 0.57 0.73\nChromagram 0.97 0.83 0.89\nTable 2 . Average precision, recall, and balanced F-score\nfor our queries against recordings in the same recording\nenvironment.\ning Up 2 instance throughout), and none using MFCCs.\nFinally, the chromagram and constant-Q databases were\nmore resilient to missing notes, matching 75% of the cases\nin the former and 40% in the latter, with MFCC matching\n10%. Interestingly, the riff at 162s in Looking Up 1 was\nentirely unmatched, possibly due to the number of notes\nmissing from the melody.\nAs a second case, 4 performances of Looking Up by\nprofessional jazz pianists were added to the databases: a\ntrio studio recording (MDI), a solo piano studio recording\n(CJ), a live band recording (MP(B)), and a live solo pi-\nano recording of the same (MP(S)). The ground truth for\nthis collection is shown in Table 3, and the precision/recall\nmeans for the MFCC and chromagram databases in Table\n4.\nAs before, chromagrams provided the most useful re-\nsults, with a comparatively high mean precision and re-\ncall. The CJ recording obtained a mean recall of 1.00\nand a mean precision of 0.72, while the MDI recording\nresulted in a mean recall of 0.43 and a mean precision of\n1.00. MP(B) and MP(S) both obtained low recall (0.27\nand 0.32 respectively) and good precision (1.00 and 0.78\nrespectively). Both MP(B) and MP(S) were recorded in\na live setting, which may suggest the distance from the\nquery, but notably the queries which didn’t match often oc-\ncurred in locations where the sustain pedal was employed.\nThe CJ recording, while in a studio, was classically precise\nin terms of note velocity, timing, and consistency, with no\nsustain pedal employed during the riff instances. The MDI\nrecording only missed matches across all queries when the\nsustain pedal was used. Further investigation will examine\nthis characteristic more closely.\n5. CONCLUSIONS\nOur study, while still at a preliminary stage, is promising:\nwe can achieve good precision and recall for fragments of\naudio, both for queries recorded under the same conditions\nas the test database and for queries recorded on consumer\nhardware against a database of professional studio record-\nings.\nTreated as a pure retrieval task, recall performance is\nperhaps not as good as might be desired; our observation\nis that our audio features are not sufﬁciently robust to the\nkinds of difference that arise in practice between the query\nand the matches desired by our userbase. Enhancements\nin this area would be to incorporate more aspects of de-\nsired invariance [11] into our feature, such as for exam-\nple: constant-Q translations or chroma rotations to model\ntransposition invariance; and beat-based analysis windows\nTrack\nPosition (s)\nMissing/Altered Notes\nMufﬂed\nNotes\nRhythm/T\nempo Alterations\nChord Additions\nSustain Pedal\nMDI9 x\n37 x\n64 x\n258\n285 x\n310\nCJ15\n40\n66\n250\n276\n300\nMP(B)17 x\n43 x\n70 x\n342 x x\n368 x x\n394 x x\nMP(S)32 x x\n65 x x\n92 x x\n202 x x\n227 x\nTable 3. Locations and comments of fragments corre-\nsponding to our queries in the three professional-quality\nrecordings.\nFeature Precision Recall F-Score\nMFCC 0.77 0.04 0.08\nChromagram 0.80 0.51 0.62\nTable 4 . Average precision, recall, and balanced F-score\nfor our queries against the professional, studio recordings.\n487\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)to incorporate tempo invariance. Because we desire to al-\nlow our users to search large databases of audio as well\nas small ones, we wish to avoid providing invariants using\nmethods scaling worse than linearly with the database size\n(such as dynamic time warping [14, Chap. 4] for tempo\ninvariance).\nHowever, these invariants are not desired for all appli-\ncations of our searching technology; in particular, when\nexploring a corpus for changes in stylistic aspects of per-\nformance, it is important for sufﬁciently different rendi-\ntions notto match a query. The success of our initial ex-\nperiment in this respect is the observation that one appar-\nently robust characteristic of the ground truth matches in\nthe professionally-recorded corpus that are not found by\nour current features is that they are executed in the record-\nings with the sustain pedal on (which has previously been\nidentiﬁed as a problem in other MIR tasks [5,10]); design-\ning a feature to cope with this would be very desirable,\nbut the distinction between the performance practice with\nsustain and without was new information to our co-author\npianist.\nWe expect to go through several more design-and-test\niterations for our implementation of a user interface; known\ncurrently-missing features include: a quasi-live interface\nfor rapid, experimental search; and a means for navigation\nbetween regions [1,13]. However, we believe that what we\nhave already developed is good enough for a sophisticated\nuser to be able to explore his own performance practice, or\nfor a composer to use as a thesaurus. The software will be\navailable to download from the OMRAS website4shortly\nafter publication, and we welcome feedback from users.\n6. REFERENCES\n[1] Dominikus Baur, Tim Langer, and Andreas Butz.\nShades of Music: Letting Users Discover Sub-song\nSimilarities. In Proc. ISMIR, pages 111–116, 2009.\n[2] Donald Byrd. A Similarity Scale for Content-\nBased Music IR. Available at http://www.\ninformatics.indiana.edu/donbyrd/\nMusicSimilarityScale.html, 2008.\n[3] M. Casey, C. Rhodes, and M. Slaney. Analysis of Min-\nimum Distances in High-Dimensional Musical Spaces.\nIEEE Transactions on Audio, Speech and Language\nProcessing, 16(5):1015–1028, 2008.\n[4] M. Casey and M. Slaney. The Importance of Sequences\nin Music Similarity. In Proc. ICASSP, volume V , pages\n5–8, 2006.\n[5] Arshia Cont. Realtime Multiple Pitch Observation us-\ning Sparse Non-negative Constraints. In Proc. ISMIR,\n2006.\n[6] Mark d’Inverno, Christophe Rhodes, Michael Casey,\nand Michael Jewell. Content-based Search for Time-\nbased Media. in preparation.\n4http://www.omras2.org[7] Alexander Duda, Andreas N ¨urnberger, and Sebastian\nStober. Towards Query by Singing/Humming on Audio\nDatabases. In Proc. ISMIR, pages 331–334, 2007.\n[8] Asif Ghias, Jonathan Logan, David Chamberlin, and\nBrian C. Smith. Query by humming: musical informa-\ntion retrieval in an audio database. In Proc. ACM Con-\nference on Multimedia, pages 231–236, 1995.\n[9] Masataka Goto and Takayuki Goto. Musicream: New\nMusic Playback Interface for Streaming, Sticking,\nSorting, and Recalling Musical Pieces. In Proc. ISMIR,\npages 404–411, 2005.\n[10] Maarten Grachten and Gerhard Widmer. Who is who in\nthe end? Recognizing pianists by their ﬁnal ritardandi.\nInProc. ISMIR, pages 51–56, 2009.\n[11] Kjell Lemstr ¨om and Geraint A. Wiggins. Formaliz-\ning invariances for content-based music retrieval. In\nProc. ISMIR, pages 591–596, 2009.\n[12] M. Magas, M. Casey, and C. Rhodes. mHashup: fast\nvisual music discovery via locality sensitive hashing.\nInSIGGRAPH ’08: ACM SIGGRAPH 2008 new tech\ndemos, pages 1–1, Los Angeles, 2008. ACM.\n[13] Michela Magas and John Wood. A More User-Centric\nApproach to the Retrieval of Music Data. submitted to\nJNMR, 2010.\n[14] Meinard M ¨uller. Information Retrieval for Music and\nMotion. Springer-Verlag, Berlin Heidelberg, 2007.\n[15] Dominik Schnitzer, Arthur Flexer, and Gerhard Wid-\nmer. A Filter-and-Reﬁne Indexing Method for Fast\nSimilarity Search in Millions of Music Tracks. In\nProc. ISMIR, pages 537–542, 2009.\n[16] Yuen-Hsien Tseng. Content-based retrieval for mu-\nsic collections. In Proc. ACM SIGIR, pages 176–182,\n1999.\n[17] George Tzanetakis, Andreye Ermonlinskyi, and Perry\nCook. Beyond the Query-By-Example Paradigm: New\nQuery Interfaces for Music Information Retrieval. In\nProc. ICMC, pages 177–183, 2002.\n[18] Hugues Vinet, Perfecto Herrera, and Franc ¸ois Pachet.\nThe CUIDADO Project. In Proc. ISMIR, pages 197–\n203, 2002.\n488\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "Melody Extraction from Polyphonic Audio Based on Particle Filter.",
        "author": [
            "Seokhwan Jo",
            "Chang D. Yoo"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1414978",
        "url": "https://doi.org/10.5281/zenodo.1414978",
        "ee": "https://zenodo.org/records/1414978/files/JoY10.pdf",
        "abstract": "This paper considers a particle filter based algorithm to ex- tract melody from a polyphonic audio in the short-time Fourier transforms (STFT) domain. The extraction is fo- cused on overcoming the difficulties due to harmonic / per- cussive sound interferences, possibility of octave mismatch, and dynamic variation in melody. The main idea of the al- gorithm is to consider probabilistic relations between melody and polyphonic audio. Melody is assumed to follow a Markov process, and the framed segments of polyphonic audio are assumed to be conditionally independent given the parameters that represent the melody. The melody pa- rameters are estimated using sequential importance sam- pling (SIS) which is a conventional particle filter method. In this paper, the likelihood and state transition are defined to overcome the aforementioned difficulties. The SIS algo- rithm relies on sequential importance density, and this den- sity is designed using multiple pitches which are estimated by a simple multi-pitch extraction algorithm. Experimen- tal results show that the considered algorithm outperforms other famous melody extraction algorithms in terms of the raw pitch accuracy (RPA) and the raw chroma accuracy (RCA).",
        "zenodo_id": 1414978,
        "dblp_key": "conf/ismir/JoY10",
        "keywords": [
            "particle filter",
            "melody extraction",
            "polyphonic audio",
            "STFT domain",
            "harmonic/percussive sound",
            "octave mismatch",
            "dynamic variation",
            "Markov process",
            "sequential importance sampling (SIS)",
            "pitch extraction algorithm"
        ],
        "content": "MELODY EXTRACTION FROM POLYPHONIC AUDIO BASED ON\nPARTICLE FILTER\nSeokhwan Jo Chang D. Yoo\nDepartment of Electrical Engineering, Korea Advanced Institute of Science Technology,\n373-1 Guseong-dong, Yuseong-gu, Daejeon, 305-701, Korea\nantiland00@kaist.ac.kr cdyoo@ee.kaist.ac.kr\nABSTRACT\nThis paper considers a particle ﬁlter based algorithm to ex-\ntract melody from a polyphonic audio in the short-time\nFourier transforms (STFT) domain. The extraction is fo-\ncused on overcoming the difﬁculties due to harmonic / per-\ncussive sound interferences, possibility of octave mismatch,\nand dynamic variation in melody. The main idea of the al-\ngorithm is to consider probabilistic relations between melody\nand polyphonic audio. Melody is assumed to follow a\nMarkov process, and the framed segments of polyphonic\naudio are assumed to be conditionally independent given\nthe parameters that represent the melody. The melody pa-\nrameters are estimated using sequential importance sam-\npling (SIS) which is a conventional particle ﬁlter method.\nIn this paper, the likelihood and state transition are deﬁned\nto overcome the aforementioned difﬁculties. The SIS algo-\nrithm relies on sequential importance density, and this den-\nsity is designed using multiple pitches which are estimated\nby a simple multi-pitch extraction algorithm. Experimen-\ntal results show that the considered algorithm outperforms\nother famous melody extraction algorithms in terms of the\nraw pitch accuracy (RPA) and the raw chroma accuracy\n(RCA).\n1. INTRODUCTION\nMany people believe that people recognize music as a se-\nquence of monophonic notes called melody, and for this\nreason, melody extraction is playing an important role in\nmusic content processing which has recently become an\nimportant research area. Although the debate over the def-\ninition of melody is on going [1–3], many experts concur\nthat melody should be the dominant pitch sequence of a\npolyphonic audio. In this paper, melody is deﬁned to be\nthe singing voice pitch sequence in the vocal part and the\npitch sequence of the solo instrument in non-vocal part or\nnon-vocal music. When a music contains singing voice,\nmost people recognize music by the vocal melody line in\nthe vocal part. However, in non-vocal part such as inter-\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc⃝2010 International Society for Music Information Retrieval.mezzo and non-vocal music such as jazz and orchestra,\nmost people recognize music by the melody line of the solo\ninstrument.\nMany melody extraction algorithms have been proposed\nover the last one decade [1–6], albeit with limited success.\nMelody extraction from the polyphonic audio is still difﬁ-\ncult for the following reasons:\n1. Harmonic interference: Harmonics of other instru-\nment signal interfere in the estimation of the melody\npitch harmonics.\n2. Percussive sound interference: Percussive sound in-\nterfere to estimate the melody pitch because the en-\nergy of it forms a vertical ridge with strong and wide-\nband spectral envelopes.\n3. Octave mismatch: The estimated pitch can be one\noctave higher or lower than the ground-truth.\n4. Dynamic variation in melody: Accurate pitch esti-\nmation in the beginning, end and sudden transient\nregions of a melody is difﬁcult.\nIn this paper, melody pitch frequency and harmonic am-\nplitudes that represent the melody are estimated in the short-\ntime Fourier transforms (STFT) domain. The main idea\nof the algorithm is to consider a probabilistic relations be-\ntween melody and polyphonic audio. Melody pitch fre-\nquency and harmonic amplitudes are assumed to follow\nMarkov processes, and the framed segments of polyphonic\naudio are assumed to be conditionally independent given\nmelody pitch frequency and harmonic amplitudes. Thus,\nmelody pitch frequency and harmonic amplitudes can be\nestimated from the polyphonic audio based on the Bayesian\nsequential model once the likelihood and state transition\nare deﬁned. The likelihood is deﬁned to be robust to har-\nmonic and percussive sound interferences. The state tran-\nsition of melody pitch frequency is adjusted by control pa-\nrameters that discourages octave mismatch and dynamic\nvariation in the melody. The sequential importance sam-\npling (SIS) algorithm, a conventional particle ﬁlter algo-\nrithm, is used to estimate the melody parameters. The\nSIS algorithm relies on a so-called sequential importance\ndensity, and this density is designed using multiple pitches\nwhich are estimated by a simple multi-pitch extraction al-\ngorithm.\n357\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)This paper\nis organized as follows. Section 2 presents\nthe melody extraction from polyphonic audio based on par-\nticle ﬁlter. Section 3 provides experimental results. Fi-\nnally, Section 4 concludes this paper.\n2. MELODY EXTRACTION FROM POLYPHONIC\nAUDIO BASED ON PARTICLE FILTER\n2.1 Melody extraction from polyphonic audio\nThe melody pitch harmonics xt[n]in the tth frame is de-\nﬁned as follows:\nxt[n] = w[n]H∑\nm=1Am;tcos(mω0;tn+ϕm;t),(1)\nwhere Am;t,ω0;t,ϕm;t,Handw[n]are the amplitude of\nthemth harmonic in the tth frame, the melody pitch fre-\nquency in the tth frame, the phase of the mth harmonic in\nthetth frame, number of melody pitch harmonics, and the\nanalysis window function, respectively. The polyphonic\naudio can be expressed as\nzt[n] = xt[n] + yt[n], (2)\nwhere zt[n]andyt[n]are the polyphonic audio signal and\nsignal of other instruments in the tth frame, respectively.\nIn the frequency domain, the following relationship holds:\nzt=xt+yt, (3)\nwhere zt,xt, andytare the N-point discrete Fourier trans-\nforms (DFT) of zt[n],xt[n], and yt[n], respectively.\nThe parameters of the melody pitch harmonics – the\nmelody pitch frequency and the harmonic amplitudes –\nmust be estimated for the melody extraction. This paper\nassumes that the phase of the melody pitch harmonics is\nthe same as the phase of the polyphonic audio, i.e., the\nphase of the melody pitch is not estimated since human ear\nis assumed to be unsensitive to phase variations. Thus, the\ntth frame parameter set is deﬁned as\n\u0002t= (ω 0;t,At), (4)\nwhere At= [A 1;t, A2;t, ..., A H;t]. The objective of melody\nextraction is to estimate \u0002tfrom given zt. It is usually\nobserved that successive parameters – ω0;tandAt– are\nhighly correlated. In this paper, it is assumed that \u0002tis\nconsidered a Markov process and ytat each frame is con-\nditionally independent given \u0002t. Here, \u0002tis considered\nlatent while ytis observed. From this perspective, the\nBayesian sequential model for melody extraction can be\nconstructed as shown in Figure 1. In Figure 1, p(ztj\u0002t),\np(\u0002 tj\u0002t\u00001), and ρtare likelihood, state transition, and\ncontrol parameter to decide the state transition of the melody\npitch frequency, respectively. From this Bayesian sequen-\ntial model, the posterior probability p(\u00020:tjz1:t)1is es-\ntimated, and it is used to estimate \u0002tfor melody extrac-\ntion. To estimate p(\u00020:tjz1:t), likelihood and state evolu-\ntion equations with state transition needs to be deﬁned.\n1The notation a0:tmeans\nthat a0:t= [a0; a1; :::; a t]T\nFigure\n1. Bayesian sequential model for melody extrac-\ntion.zt,\u0002t, and ρtare polyphonic audio, melody param-\neter (ω 0;tandAt), and control parameter, respectively.\nTo obtain the likelihood, it is assumed that the DFT co-\nefﬁcients of ytfollow a zero mean complex multivariate\nGaussian distribution, which is given by\nyt\u0018 N (0,\u0006t),\u0006t= diag( σ2\nt;1, σ2\nt;2, ..., σ2\nt;N),(5)\nwhere \u0006tandσt;kare the covariance matrix in tth frame\nand the variance of the kth bin in the tth frame, respec-\ntively. Eqn. (5) yields the likelihood as follows:\np(ztj\u0002t) = N(zt;xt,\u0006t)\n/exp{\n\u0000(zt\u0000xt)H\u0006\u00001\nt(zt\u0000xt)}\n,(6)\nwhere (\u0001)His the Hermitian operator. To deﬁne p(ztj\u0002t),\nσt;kmust be estimated. In this paper, σt;kis estimated\nusing the decision-directed method [7] as follows:\nbσt;k=αbσt\u00001;k+ (1\u0000α)jYt;kj2, (7)\nwhere αandYt;kare a smoothing factor and the kth bin\nDFT coefﬁcient of yt, respectively. However, Eqn. (7) can\nnot be used directly since Yt;kis unknown. It is assumed\nthatYt;kis highly correlated with Yt\u00001;k . Therefore, the\nestimation is modiﬁed as follows:\nbσt;k=αbσt\u00002;k + (1\u0000α)jbYt\u00001;kj2. (8)\nAccurate estimation of \u0006twill lead to robustness to har-\nmonic and percussive sound interferences. Figure 2 shows\nan example of ztand an estimate of \u0006t, and it is easily\nshown that the likelihood in Eqn. (6) is maximized at the\ntrue\u0002t.\nThe state evolution equations, which describe relation-\nships of the parameters at frame t, are set as follows:\nAm;t=Am;t\u00001+vA;t\u00001 , (9)\nω0;t=ω0;t\u00001 +v!0;t\u00001, (10)\nwhere vA;t\u00001andv!0;t\u00001are the random perturbations\ncorresponding to harmonic amplitudes and melody pitch\nfrequency of the (t\u00001)th frame, respectively. This type of\nstate evolution equations is called random walk: the cur-\nrent state is a random perturbation of the previous state.\nIt is important to deﬁne p(vA;t\u00001 )andp(v!0;t\u00001)accu-\nrately, and in this paper, p(vA;t\u00001 )is assumed to be a trun-\ncated Gaussian as shown in Figure 3 since Am;t>0, and\n358\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)10020030040050060070080090010−210−1100101102\nfrequency binMagnitude\n  \nPolyphonic audio\nSqrt of variancesFigure\n2. Example of polyphonic audio (z t) and the esti-\nmated variances (\u0006 t) of other instrument signal.\np(v!0;t\u00001)is assumed to be a Gaussian whose variance\ncontrolled by ρt. Melody line is characterized by pro-\nlonged periods of smoothness, with infrequent sharp changes\nin note transition or during vibrato regions.\nFurthermore, there are two general rules concerning the\nmelody line: 1) the vibrato exhibits an extent of 60\u0018200\ncents2for singing voice and only 20\u001830 cents for other\n[8], and 2) the transitions are typically limited to one oc-\ntave [1]. Therefore, assumption that v!0;t\u00001follows a Gaus-\nsian distribution with ﬁxed variance is not appropriate. In\nthis paper, the state transition from the from the (t\u00001)th\nstate to the tth state of the melody pitch frequency is con-\ntrolled by ρtwhich indicates the degree of the melody line\nbeing whether in transition or not. Here, transition includes\nvibrato. And, ρtis deﬁned as\nρt=bω0;t\u00001\u0000bω0;t\u00002, (11)\nandp(v!0;t\u00001)is given by\np(v!0;t\u00001) =\n\nN(0,20 cent) ρt<50 cent\nN(0,50 cent) 50 cent \u0014ρt<100 cent\nN(0,100 cent) 100 cent \u0014ρt.\n(12)\nWhen ρtis small, the current melody pitch frequency rep-\nresents a certain note frequency and has a value similar to\nthe previous melody pitch frequency. When ρtis large, the\ncurrent melody pitch frequency is with high probability in\na note transition or vibrato regions and has a value dis-\nsimilar to the previous melody pitch frequency. The state\ntransition of melody pitch frequency deﬁned by Eqn. (12)\ncan lead to robustness to octave mismatch and dynamic\nvariation in melody.\n2Thecentis a\nunit of logarithmic frequency range, and it is deﬁned as\nfcent = 6900 + 1200 log2fHz\n440:\n−6 −4 −2 0 2 4 600.02 0.04 0.06 0.08 0.10.12 0.14 0.16 0.18 \n(a)p(vA;t\u0000 1)\n−2 0 2 4 6 800.02 0.04 0.06 0.08 0.10.12 0.14 0.16 0.18 \n (b)p(Am;tjAm;t\u00001 )\nFigure\n3. State transition in harmonic amplitudes.\n2.2 Melody extraction based on particle ﬁlter\nIn this paper, p(\u00020:tjz1:t)is approximated using Monte\nCarlo integration and \u0002tis estimated using the particle\nﬁlter. The SIS algorithm which is a common particle ﬁl-\nter method [9, 10] is adopted to estimate the parameters\nof the melody. If the likelihood and the state transition\nfollow a Gaussian distribution, the problem can be solved\nby Kalman ﬁlter. However, the state transition is not as-\nsumed to be a Gaussian. The SIS algorithm is used to ob-\ntainp(\u00020:tjz1:t)based on the Bayesian sequential model\nshown as Figure 1.\nThe posterior density p(\u00020:tjz1:t)can be approximated\nas follows:\np(\u00020:tjz1:t)\u0019Np∑\ni=1w(i)\ntδ(\u00020:t\u0000\u0002(i)\n0:t), (13)\nwhere \u0002(i)\n0:t,wt(i), and Npare the ith particle of \u00020:t, as-\nsociated weight, and the number of particles, respectively.\nThe weights are normalized such that∑Np\ni=1w(i)\nt= 1. The\nweights are chosen using the method of importance sam-\npling. If the particle \u0002(i)\n0:twere drawn from an importance\ndensity q(\u0002(i)\n0:tjz1:t), the weights in Eqn. (13) are deﬁned\nas follows:\nw(i)\nt/p(\u0002(i)\n0:tjz1:t)\nq(\u0002(i)\n0:tjz1:t). (14)\nIf the\nimportance density is chosen to factorize as follows\nq(\u00020:tjz1:t) =q(\u0002tj\u00020:t\u00001,z1:t)q(\u00020:t\u00001jz1:t\u00001),\n(15)\nthen one can obtain particles \u0002(i)\n0:t\u0018q(\u0002(i)\n0:tjz1:t)by aug-\nmenting each of the existing particles \u0002(i)\n0:t\u00001\u0018q(\u0002(i)\n0:t\u00001j\nz1:t\u00001)with the new state \u0002(i)\nt\u0018q(\u0002tj\u00020:t\u00001,z1:t). The\nweight update equation can be derived as follows using\nEqn. (14) and Eqn. (15)\nw(i)\nt/w(i)\nt\u00001p(ztj\u0002(i)\nt)p(\u0002(i)\ntj\u0002(i)\nt\u00001)\nq(\u0002(i)\ntj\u0002(i)\nt\u00001,zt). (16)\nA common\nproblem with the particle ﬁlter is the de-\ngeneracy phenomenon, where after a few iterations, most\nparticles have negligible weight [9,10]. A suitable measure\nof degeneracy is the effective particle size, Neff, which is\ngiven by\ndNeff=1\n∑Np\ni=1(w(i)\nt)2. (17)\n359\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)4000 4500 5000 5500 6000 6500 7000 7500 8000 8500 01234567x 10 −3 \npitch candidate [cent]  \nimportance density \nN−best\nprevious particle Figure\n4. Design of q(ω(i)\n0;tjω(i)\n0;t\u00001,zt).\nIn this paper, to avoid the degeneracy problem, resampling\nalgorithm is used when Neff\u0014Np\n2.\nFinally,\nestimation of parameters is achieved by poste-\nrior mean after obtaining p(\u0002 0:tjz1:t).\nbω0;0:t=Np∑\ni=1w(i)\ntω(i)\n0;0:t, (18)\nbA0:t=Np∑\ni=1w(i)\ntA(i)\n0:t. (19)\n2.2.1 Design of sequential importance density\nThe performance of the SIS algorithm depends on the choice\nofq(\u0002(i)\ntj\u0002(i)\nt\u00001,zt). Setting q(\u0002(i)\ntj\u0002(i)\nt\u00001,zt) =p(\u0002(i)\ntj\u0002(i)\nt\u00001)\nleads to not only unnecessary large number of particles\nbut also difﬁculties in estimating p(\u0002 0:tjz1:t). In this pa-\nper, a multiple pitch estimation algorithm is used to deﬁne\nq(\u0002(i)\ntj\u0002(i)\nt\u00001,zt)since the melody pitch frequency is as-\nsumed to be one of the pitch estimate given by the multiple\npitch estimates. A main idea in deﬁning q(\u0002(i)\ntj\u0002(i)\nt\u00001,zt)\nis to generate particles of the melody parameters similar to\nthe estimated multiple pitch parameters. To obtain multiple\npitch parameters, the multiple pitch estimation algorithm\nproposed in [11] is used.\nBefore drawing particles from the importance density,\nq(\u0002(i)\ntj\u0002(i)\nt\u00001,zt)is factorized as follows:\nq(ω(i)\n0;t,A(i)\ntjω(i)\n0;t\u00001,A(i)\nt\u00001,zt)\n=q(A(i)\ntjω(i)\n0;t,A(i)\nt\u00001,zt)q(ω(i)\n0;tjω(i)\n0;t\u00001,zt).(20)\nHere, ω0;tandAtare considered conditionally indepen-\ndent given ω(i)\n0;t\u00001,A(i)\nt\u00001, andzt. First, melody pitch parti-\ncles are drawn as given by\nω(i)\n0;t\u0018q(ω(i)\n0;tjω(i)\n0;t\u00001,zt), (21)\nwhere q(ω(i)\n0;tjω(i)\n0;t\u00001,zt)is shown as Figure 4. In deﬁn-\ningq(ω(i)\n0;tjω(i)\n0;t\u00001,zt), the current melody pitch particles\nare drawn near the N-best pitch candidates obtained fromthe multiple-pitch estimation and the melody pitch parti-\ncles drawn in the previous frame. After drawing melody\npitch particles, melody pitch harmonic amplitudes parti-\ncles are drawn as given by\nA(i)\nt\u0018q(A(i)\ntjω(i)\n0;t,A(i)\nt\u00001,zt)\n=N\nA(i)\nt\u00001+Az!(i)\n0;t\nt\n2,jA(i)\nt\u00001\u0000Az!(i)\n0;t\ntj\n2\n(22)\nwhere Az!(i)\n0;t\nt is the\nharmonic amplitudes corresponding\npitch candidate near ω(i)\n0;twith constraint A(i)\nt>0. In\ndeﬁning q(A(i)\ntjω(i)\n0;t,A(i)\nt\u00001,zt), the current harmonic am-\nplitude particles which are similar to the previous harmonic\namplitude particles and harmonic amplitudes of the N-\nbest pitch candidates are generated. If A(i)\nt\u00001andAz!(i)\n0;t\nt\nare similar, thenjA(i)\nt\u00001\u0000Az!(i)\n0;t\nt j\n2\u00190, therefore, A(\ni)\nt\u0019\nA(i)\nt\u00001+Az!(i)\n0;t\nt\n2. IfA(\ni)\nt\u00001andAz!(i)\n0;t\nt are not similar, then\njA(i)\nt\u00001\u0000Az!(i)\n0;t\nt j\n2>> 0, therefore, A(\ni)\ntis generated some-\nwhat randomly.\nThe outline of the considered algorithm is given below.\nOutline of\nthe considered algorithm\nMelody extraction\nbased on the SIS\nFori= 1, ..., N p\n1. Generate the particles\n\u000fMelody pitch particles\nω(i)\n0;t\u0018q(ω(i)\n0;tjω(i)\n0;t\u00001,zt)\n\u000fHarmonic amplitudes particles\nA(i)\nt\u0018q(A(i)\ntjω(i)\n0;t,A(i)\nt\u00001,zt)\n2.Update the weights : Eqn. (16)\nNormalize the weights (∑Np\ni=1w(i)\nt= 1).\nResampling: Resampling algorithm is used\nwhen Neff\u0014Np\n2.\nEstimation : Melody\npitch frequency in tth\nframe is estimated by Eqn. (18). Harmonic am-\nplitudes of melody pitch harmonics in tth frame\nare estimated by Eqn. (19).\n3. EV\nALUATION\nThe considered algorithm was evaluated and compared to\nother melody extraction algorithms using the ISMIR 2004\nAudio Description Contest (ADC04) database. The database\ncontains 20 polyphonic musical audio pieces. All test data\nare single channel PCM data with 44.1 kHz sample rate\nand 16-bit quantization. Table 1 shows the data composi-\ntion of the ADC04 set. Search range of melody pitch fre-\nquency was between 80Hz and 1280Hz in frequency do-\n360\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Melody Instrument Sytle\nSynthesized v oice (4) POP\nSaxophone (4) Jazz\nMIDI instruments\n(4) Folk(2),\nPop(2)\nHuman v\noice (2 male, 2 female) Classical opera\nMale V\noice (4) POP\nTable\n1. Summary of ADC04 data set. The number in\nparentheses is the number of corresponding pieces.\nRPA RCA\nGoto [2] 65.8% (2005) 71.8% (2005)\nPai\nva el al. [3] 62.7% (2005) 66.7% (2005)\nMarlot [4] 60.1% (2005) 67.1% (2005)\nRyynanen el\nal. [5] 68.6% (2005) 74.1% (2005)\nEllis el\nal. [6] 73.2% (2006) 76.4% (2006)\nConsidered algorithm 77.3% 83.8%\nTable\n2. Result comparison. The number in parentheses\nis the year when their algorithms were submitted to the\nMIREX.\nmain (3950 cent and 8750 cent in cent domain). The Han-\nning window was used with 48ms frame length and 10ms\nframe hop size. α= 0.98in Eqn. (8) was used. Np= 500\nin Eqn. (13) was used.\nThe estimated melody is correct when the absolute value\nof the difference between the ground-truth frequency and\nestimated frequency is less than 50 cent (1\n4tone). The\nperformance\nof the considered algorithm was evaluated in\nterms of raw pitch accuracy (RPA) and raw chroma ac-\ncuracy (RCA). The RPA is deﬁned as the proportion of\nframes in which the estimated melody pitch is within \u00061\n4\ntone of\nthe reference pitch. And the RCA is deﬁned in\nthe same manner as the raw pitch accuracy; however, both\nthe estimated and reference frequencies are mapped into a\nsingle octave in order to forgive octave transpositions.\nThe considered algorithm was compared to the other\nfamous melody extraction algorithms such as algorithms\nproposed by Goto [2], Paiva et al. [3], Marlot [4], Ryyna-\nnen el al. [5], and Ellis et al. [6]. Their performances are\nbased on results of the Music Information Retrieval Evalu-\nation eXchange (MIREX) [12].\nTable 2 shows the evaluation results for all algorithms\nconsidered. The considered algorithm outperformed the\nothers in terms of the RPA and the RCA. The difference be-\ntween the RPA and RCA is proportional octave mismatch\nerror. Although the algorithm in this paper is considered to\nbe robust against octave mismatch, the difference between\nthe RPA and the RCA is 6.5 %. The multiple pitch es-\ntimation algorithm proposed in [11] was quite simple and\nvulnerable to octave error, i.e., inaccuracy in sequential im-\nportance density led to inaccurate melody pitch candidates.4. CONCLUSION\nThe melody extraction algorithm from the polyphonic au-\ndio based on particle ﬁlter is considered in this paper. Most\npeople recognize music as not all of note sequences but a\nspecial monophonic note sequence called melody. How-\never, melody extraction from polyphonic audio is difﬁcult\ndue to the following impediments: harmonic interference,\npercussive sound interference, octave mismatch, and dy-\nnamic variation in melody. The main idea of the algorithm\nis to consider probabilistic relations between melody and\npolyphonic audio. Melody is assumed to follow a Markov\nprocess, and the framed segments of polyphonic audio are\nassumed to be conditionally independent given the param-\neters that represent the melody. The parameters are esti-\nmated using the SIS algorithm. This paper shows that like-\nlihood and state transition that are required in the SIS algo-\nrithm are deﬁned to be robust against the aforementioned\nimpediments. The performance of the SIS algorithm de-\npends on a sequential importance density, and this density\nis designed by multiple pitch. Experimental results show\nthat the considered algorithm outperformed the other fa-\nmous melody extraction algorithms.\n5. ACKNOWLEDGEMENTS\nThis work was supported by the Ministry of Culture, Sports\nand Tourism (MCST) and Korea Culture Content Agency\n(KOCCA) in the Culture Technology (CT) Research and\nDevelopment Program 2009.\n6. REFERENCES\n[1] G. E. Poliner, D. P. W. Ellis, and A. F. Ehmann:\n“Melody transcription from music audio: approach and\nevaluation,” IEEE Transactions on Audio, Speech, and\nLanguage Processing , Vol. 15, No. 4, pp. 1247–1256,\n2007.\n[2] M. Goto: “A real-time music-scene-description sys-\ntem: predominant-f0 estimation for detecting melody\nand bass lines in real-world audio signals,” Speech\nCommunication , Vol. 43, No. 4, pp. 311–329, 2004.\n[3] R. P. Paiva, T. Mendes, and A. Cardoso: “Melody de-\ntection in polyphonic musical signals: exploiting per-\nceptual rules, note salience, and melodic smoothness,”\nComputer Music Journal , Vol. 30, No. 4, pp. 80–98,\n2006.\n[4] M. Marolt: “On ﬁnding melodic lines in audio record-\nings,” Proceeding of 7th International Conference on\nDigital Audio Effects DAFx 04, pp. 217–221, 2004.\n[5] M. P. Ryynanen and A. P. Klapuri: “Note event model-\ning for audio melody extraction,” MIREX 2005 Audio\nMelody Extraction Contest, 2005.\n[6] D. P. W. Ellis and G. E. Poliner: “Classiﬁcation-based\nmelody transcription,” Machine Learning, Vol. 65,\npp. 439–456, 2006.\n361\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)[7] Yari\nv Ephraim: “Speech enhancement using a mini-\nmum mean-square error short-time spectral amplitude\nestimator,” IEEE Transactions on Acoustics, Speech,\nand Signal Processing , Vol. 32, No. 6, pp. 1109–1121,\n1984.\n[8] R. Timmers and P. W. M Desain: “Vibrato: the ques-\ntions and answers from musicians and science,” Pro-\nceedings of International Conference on Music Percep-\ntion and Cognition , 2000.\n[9] A. Doucet, N. de Freitas, and N. J. Gordon: Sequen-\ntial Monte Carlo methods in practice , Springer-Verlag,\nNew York, 2001.\n[10] M. S. Arulampalam, S. Maskell, N. Gordon, and\nT. Clapp,: “A tutorial on particle ﬁlters for on-\nline nonlinear/non-gaussian bayesian tracking,” IEEE\nTransactions on Signal Processing , Vol. 50, No. 2,\npp. 174–188, 2002.\n[11] S. Joo, S. Jo, and C. D. Yoo: “Melody extraction from\npolyphonic audio signal MIREX 2009,” MIREX 2009\nAudio Melody Extraction Contest , 2009.\n[12] J. S. Downie, K. West, A. Ehmann, and Vincent E:\n“The 2005 music information retrieval evaluation ex-\nchange (mirex 2005): preliminary overview,” Proceed-\nings of the Sixth International Conference on Music In-\nformation Retrieval , pp. 320–323, 2005.\n362\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "An Improved Hierarchical Approach for Music-to-symbolic Score Alignment.",
        "author": [
            "Cyril Joder",
            "Slim Essid",
            "Gaël Richard"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1417883",
        "url": "https://doi.org/10.5281/zenodo.1417883",
        "ee": "https://zenodo.org/records/1417883/files/JoderER10.pdf",
        "abstract": "We present an efficient approach for an off-line alignment of a symbolic score to a recording of the same piece, us- ing a statistical model. A hidden state model is built from the score, which allows for the use of two different kinds of features, namely chroma vectors and an onset detec- tion function (spectral flux) with specific production mod- els, in a simple manner. We propose a hierarchical prun- ing method for an approximate decoding of this statistical model. This strategy reduces the search space in an adap- tive way, yielding a better overall efficiency than the tested state-of-the art method. Experiments run on a large database of 94 pop songs show that the resulting system obtains higher recognition rates than the dynamic programming algorithm (DTW), with a significantly lower complexity, even though the rhyth- mic information is not used for the alignment.",
        "zenodo_id": 1417883,
        "dblp_key": "conf/ismir/JoderER10",
        "keywords": [
            "off-line alignment",
            "symbolic score",
            "recording",
            "statistical model",
            "hidden state model",
            "chroma vectors",
            "onset detection",
            "simple manner",
            "hierarchical pruning",
            "approximate decoding"
        ],
        "content": "AN IMPROVED HIERARCHICAL APPROACH FOR\nMUSIC-TO-SYMBOLIC SCORE ALIGNMENT\nCyril Joder, Slim Essid, Gaël Richard\nInstitut TELECOM, TELECOM ParisTech, CNRS LTCI\n{cyril.joder, slim.essid, gael.richard}@telecom-paristech.fr\nABSTRACT\nWe present an efﬁcient approach for an off-line alignment\nof a symbolic score to a recording of the same piece, us-\ning a statistical model. A hidden state model is built from\nthe score, which allows for the use of two different kinds\nof features, namely chroma vectors and an onset detec-\ntion function (spectral ﬂux) with speciﬁc production mod-\nels, in a simple manner. We propose a hierarchical prun-\ning method for an approximate decoding of this statistical\nmodel. This strategy reduces the search space in an adap-\ntive way, yielding a better overall efﬁciency than the tested\nstate-of-the art method.\nExperiments run on a large database of 94 pop songs\nshow that the resulting system obtains higher recognition\nrates than the dynamic programming algorithm (DTW),\nwith a signiﬁcantly lower complexity, even though the rhyth-\nmic information is not used for the alignment.\n1. INTRODUCTION\nWe address the problem of synchronizing a polyphonic\nmusical score with an audio performance of this score, in\nthe “off-line” version of this task. This allows one to con-\nsider the whole recording before estimating the positions\nof the score notes. We are interested in an alignment at the\n“symbolic level”, which means that the result is the time\nindexes of the score notes or chords.\nApplications of such a system can be a score retrieval\nfrom a musical query, or the ability to use both the audio\nand symbolic (score) content for music indexing. Some\nmusical content analysis tasks, such as motif detection or\nchord transcription, may indeed be easier on symbolic data\nthan on raw audio ﬁles.\nWhile most on-line score following systems use statis-\ntical models which can be rather complex [4, 8, 14], many\noff-line algorithms simply rely on the DTW algorithms or\nreﬁnements of it [6, 9]. These latter algorithms are often\nTHIS WORK WAS PARTLY SUPPORTED BY THE EUROPEAN\nCOMMISSION UNDER THE OSEO PROJECT QUAERO.\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2010 International Society for Music Information Retrieval.faster than the former and can also be applied to audio-to-\naudio synchronization.\nHowever, their complexity (in time and space) is quadratic\nin the number of audio frames. This complexity problem\nhas been addressed in [10], where a “short-time” DTW is\nproposed, which reduces the memory space requirement,\nat the cost of a greater time complexity. In [11], Müller et\nalintroduce a “multi-scale” DTW (MsDTW) which allows\nfor an efﬁcient pruning strategy in a coarse-to-ﬁne fashion.\nTo the authors’ knowledge, hierarchical approaches have\nnot been used for music synchronization, appart from [11].\nIn [3], Cont exploits a Hierarchical Hidden Markov Model.\nHowever, although its advantage in terms of interpretation,\nthis structure is equivalent to a “ﬂat” HMM [12].\nWith a dynamic programming framework, the use of\ndifferent kinds of descriptors can be difﬁcult. Hence such\nsystems use a single feature representation, generally chroma\nvectors. A notable exception can be found in [6], where a\nstrategy is proposed to combine local distances resulting\nfrom chroma vectors and onset features in a DTW scheme.\nThe use of a statistical model makes the fusion of different\npieces of information more natural. This structure is often\nused in real-time systems [2, 5], which model each feature\ndistribution with a Gaussian mixture.\nThe hidden state model presented here exploits a differ-\nent model for two different sets of features: a “histogram\nmodel” (see Sec. 2.1.1) for chroma vectors and a logistic\nmodel (see 2.1.2) for an onset indicator feature. This sys-\ntem obtains a very good alignment precision with a signif-\nicantly lower complexity than the DTW algorithm.\nWe also introduce a hierarchical approach for search\nspace reduction, which performs a pruning of the unlikely\nstates in a hierarchical way. We take advantage of struc-\ntural information given by the score (namely beat and bars),\nwhich allows for a meaningful hierarchical segmentation\nof the music. This method provides an alternative to the\ncommonly used beam search strategy, which consists in\nmaintaining only a ﬁxed (small) number of paths at each\ndecoding step. Our approach proves advantageous com-\npared to both beam search and MsDTW, in terms of global\nsearch space size and runtime, without affecting the align-\nment performance in practice.\nIn the next section we present our baseline models for\naudio-to-score alignment. Then, a hierarchical pruning method\nfor an approximate decoding of these models is proposed\nin Section 3. We expose the results of our experiments in\nSection 4 before suggesting some conclusions in Section 5.\n39\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)2: 1: 4: 3: 5: 6: 7:\nFigure\n1. Score Representations. Top: The original graph-\nical score. Middle: The score as a sequence of chord. Bot-\ntom: The ﬁnite state machine representing the score.\n2. BASELINE MODELS FOR AUDIO-TO-SCORE\nALIGNMENT\nSimilarly to [15], we segment the musical score into chords,\nwhich are sets of notes that sound at the same time. Every\ntime a note appears or disappears, a new chord is created.\nWe can then ﬁt a hidden state model to the audio signal, the\nstates of which are deﬁned by the chords of the score. The\nscore is seen as an automaton, as represented in Figure 1.\nIn this work, we chose not to take into account the rhyth-\nmic information given by the score, as we consider that we\nhave no prior knowledge of the tempo. We use the Maxi-\nmum Likelihood (ML) path in the automaton as the align-\nment path. Let y=y1;:::;yNbe the feature sequence ex-\ntracted from the signal. Let Snbe the random variables de-\nscribing the current state at time n. The ML path ^S, which\ncan be efﬁciently computed by the Viterbi algorithm, is:\n^S= argmax\nS2SP\u0000\ny\f\fS\u0001\n= argmax\nS2SNY\nn=1P(ynjSn); (1)\nwhereSis the set of acceptable paths. We consider as ac-\nceptable the paths which go through all the states in the\nright order. This model is thus a left-right Hidden Markov\nModel whose only transitions are self-transitions and tran-\nsitions from one state to the following one. All these tran-\nsitions have the same probabilities.\n2.1 Observation Models\nSimilarly to [13], two kinds of information are used in this\nwork: the pitch content and the onset information. Thus,\nwe use two types of features in order to take them into\naccount. Chroma vectors are used in order to model the\npitch content of the signal, and the spectral ﬂux is supposed\nto detect the note onsets.\n2.1.1 Chroma Vectors\nAs observed in [9], chroma vectors provide a compact, yet\nefﬁcient representation of the pitched content of a musical\nsignal for music-to-score matching. A chroma vector is a\ntwelve-dimension vector, each of whose component repre-\nsent the “power” in all the frequency bands of a chromatic\nclass (from A to G#). The chroma vectors we use are com-\nputed according to [16], with a 50 Hz time resolution.\nFor each state s, a probability distribution f~g(i)gi=1:::12\nover the 12 chroma components is built, as the superpo-\nsition of one-note distributions which correspond to thenotes that are present in the state. A one-note distribu-\ntion is a simple Kronecker function f\u000e(i;j)gi=1:::12 where\njis the pitch class of the considered note. Then, a constant\ncomponentqis added in order to model noise, and we ob-\ntain a distribution gdeﬁned byg(i) = (1\u0000q)~g(i) +q\n12.\nA value of 0:7has been found satisfactory for the noise\nparameterq. For example, the distribution values corre-\nsponding to the chord\b\nC3;E3;G3;C4\t\nare (represented\nin a vector)1\u0000q\n4(0;0;0;2;0;0;0;1;0;0;1;0)+q\n121, where\n1is a vector of ones.\nIn order to calculate the likelihood of each state, we use\nthe model exposed in [15]. The values of the chroma vec-\ntorvextracted from the audio is considered as a histogram\nof random samples drawn from the distribution g(corre-\nsponding to chord c). The probability of having vas a\nresult of such a sampling is:\np\u0000\nvjc\u0001\n=Z(v)12Y\ni=1g(i)\u000bv(i): (2)\nHere,\u000bis a scaling parameter. Since the value of this pa-\nrameter has no effect on the decoding result, it is ﬁxed to\n1.Z(v)is a positive number which only depends on the\nobservation v, hence it is the same for every path and its\nvalue is not considered.\n2.1.2 Spectral Flux Feature\nIn order to render the “burst of energy” which appears at a\nnote onset, we exploit the spectral ﬂux feature, which has\nbeen proven efﬁcient in a beat tracking task [1]. We use\nthis feature for a “probabilistic” onset detector.\nThe spectral ﬂux values are ﬁrst normalized so that their\nmaximum is 1. A local threshold is then computed by ap-\nplying a 67% rank ﬁlter of length 200 ms to the output.\nWe then obtain an “onset feature” by subtracting this local\nthreshold to the normalized spectral ﬂux. Finally, a simple\nlogistic model is used in order to calculate the likelihood\nof an onset. We denote by Athe random variable repre-\nsenting the attack (onset) indicator (fA = 1gmeans that\nthere is an attack). For a value fof the onset feature, we\nhave:\np\u0000\nA= 1jf\u0001\n=ebf\n1 +ebf(3)\nwherebis a positive parameter, which controls the “conﬁ-\ndence” of the onset detector: when the value increases, the\ndecision is closer to a deterministic detector (with proba-\nbilities 0 or 1).\n2.2Chord andOnset Models\nTwo structures of HMMs are evaluated in this work. In the\nChord structure, a chord is represented by a single state,\nand only the pitch information (described by the chroma\nvectors) is taken into account. The spectral ﬂux is not con-\nsidered.\nTheOnset model is a reﬁnement of the previous struc-\nture which takes the onset information into account. In this\nmodel, a lower “level of hierarchy” is added in order to\nmodel two possible phases of a chord: attack andsustain.\nEarch chord corresponding to an onset is split into two suc-\ncessive phase states: attack (A = 1) and sustain (A = 0),\n40\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)ModelOnsetModelChord2: 4: 3: 5: 1:\n2 3 5 14\nA1S1A2S2A3S3S4A5S5\nFigure\n2. State Structure of the chord andonset models for\nthe previous score (A andSstand for respectively attack\nandsustain).\nwhich share the same chroma vector model. The two types\nof features are supposed to be independent. The chroma\nfeature is assumed to depend only on the chord state while\nthe onset feature only depends on the phase state. Hence,\nif we assume an uninformative prior about the phase state,\ni.e.p(A=a) =1\n2, we have:\np\u0000\nv;fjc;a\u0001\n/p\u0000\nvjc\u0001\np\u0000\nA=ajf\u0001\n(4)\nwhere these other probabilities are expressed in (2) and (3).\nEach state of the model is then the combination of a chord\nand a phase: we write Sn= (Cn;An). Equation (1) is\nthen:\n^S= argmax\nS2SNY\nn=1p(vnjCn)p(Anjfn): (5)\nSix different values are tested for the parameter bof eq. (3):\n0, 0.1, 1, 10, 50 and 100.\nThe structures of the two models are compared in Fig. 2.\nFor the Onset model, the lower level states are represented\ninside the “chord super-states”. In the example, the fourth\nsuper-state contains only a sustain state, because the tran-\nsition from the previous chord to the fourth one does not\ncorrespond to an onset, but to the extinction of one note.\n3. A NOVEL HIERARCHICAL PRUNING\nAPPROACH\nIn order to speed up the decoding phase, we use a hierar-\nchical pruning approach, inspired by the multi-scale Dy-\nnamic Time Warping (MsDTW) algorithm [11]. The idea\nis to ﬁrst obtain a coarse alignment and then use the result\nto prune the search space at a more precise level.\nFor these coarse alignments, we take advantage of higher\nmusical structures than what we call chords, namely beat\nandbars. These structures, given by the score, allow for\na meaningful hierarchical segmentation of the music. At\neach of these levels, a HMM can be built, whose states\ncorrespond respectively to the beats and to the bars of the\nscore. As the considered temporal units are larger and the\nprecision needed at these levels is lower, the observations\nused for the alignment are calculated over longer windows,\nwith a smaller time resolution. Figure 3 illustrates the con-\nstruction of the automata and the calculation of the obser-\nvations, at the three levels of hierarchy.\nThe algorithm proceeds as follows: on the highest level\nautomaton, we calculate for every state sand every frame\nn, the maximum likelihood that can be obtained by going\nScore: automatonChordsBeatsBars\nAudio:\nintegration windows1\n1 2 3 4\n2: 4: 3: 5: 6: 7: 1:\nFigure\n3. Finite state machines (modeling the score) and\nintegration windows (over which are calculated the obser-\nvations) at the three considered levels of hierarchy.\nthrough state sat timen. This value is written\n\u0016P(s;n) = max\nS2S;Sn=s\b\nP(yjS)\t\n(6)\nwhereSis the set of acceptable paths and yis the ob-\nservation sequence. This calculation can be done by a\n“forward-backward version” of the Viterbi algorithm. It\nis very similar to the forward-backward algorithm, and can\nbe deduced from it by replacing the sum operation by a\nmax. This algorithm allows for the calculation of the opti-\nmal path ^S=^S1;:::; ^SNat the same time.\nThe values \u0016P(s;n)are then used to prune the low-score\npaths. We do not use the posterior probabilities P(Snjy)\ninstead of \u0016P(s;n)for the pruning process since we are in-\nterested in the path’s scores and not in the states’. Since a\nstate probability is the sum of the probabilities of the path\ngoing through this state, there is a risk that some states\ncontaining many average-score paths may be favored com-\npared to a state containing an isolated high-score path.\nThis “pruning score” \u0016P(s;n)constitutes an important\ndifference with the beam search strategy. Indeed, beam\nsearch operates directly at the low level and it uses the par-\ntial Viterbi score\n~Pn(s;n) = max\nS2S;Sn=s\b\nP(y1;:::;ynjS1;:::;Sn)\t\n(7)\nin order to prune the low-score path. Hence it only con-\nsiders the observation up to the current frame, whereas our\napproach takes into account the whole signal.\nThe structure of the automaton is left-right, thus the re-\nlation deﬁned on the set of states by: s\u0014s0iff there is\na path from stos0, is a total order. It is then possible to\ndeﬁne the “furthest admissible states” S\u0000\nnandS+\nnfor each\ntimenby:\nS\u0000\nn= min\b\ns\f\f~P(s;n)\u0015P(yj^S)\n\u0011\t\n(8)\nS+\nn= max\b\ns\f\f~P(s;n)\u0015P(yj^S)\n\u0011\t\n; (9)\nwhere\u0011is a parameter which controls the minimum likeli-\nhood of the paths that are kept in the pruning process. We\ndeﬁne the tolerance radii \u000e\u0000and\u000e+as the maximum num-\nber of states that separate respectively S\u0000\nnfrom ^Snand^Sn\nfromS+\nn, forn2f1;:::;Ng.\nThese tolerance radii specify a set of states around the\nalignment path, which allows for a reduction of the search\n41\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)6 3 4 5 7 1 212345\n1 21 6 11 16 26 31159131720\n35Bar Level\nBeat Level\nStates (bars) States (beats)Audio Frames\n(resolution 2 Hz)\nAudio Frames (resolution 10 Hz)\u000e+\n\u000e\u0000\nFigure\n4. Principle of the hierarchical pruning method.\nThe grey scale of a cell correspond to the maximum likek-\nihood of the paths going through this cell. At the beat level,\nonly the domain delimited by the black lines is explored.\nspace at the lower level. Hence, the alignment at the lower\nlevel is calculated by exploring only this domain. Figure 4\nillustrates this pruning process. The same procedure is re-\npeated at each level.\nThe observations used in the higher levels are integrated\n(moving average) versions of the chroma vectors, with a\nlower time resolution. This use of averaged observations is\nmusically justiﬁed since the harmony (and thus the chroma\ninformation) is in general homogeneous over a whole beat\n(or bar) duration. The spectral ﬂux is not considered in\nthese levels. The integration windows are chosen in order\nto take into account the fastest reasonable tempi. For the\nbeat level, this duration is 200 ms, corresponding to a very\nfast tempo of 300 beats per minute. For the bar level, the\nintegration window is 1 s, that is a four-four time with a\ntempo of 240 bpm. A 50% overlap is used, yielding time\nresolutions of respectively 10 Hz and 2 Hz. The histogram\nmodel exposed in Sec. 2 is used and the distribution g\ncorresponding to a state (beat or bar) is the superposition\nof the distributions associated to the chords that it contains,\nweighted by their theoretical durations (in beat).\nThe main difference between the MsDTW pruning ap-\nproach and ours is that the tolerance widths \u000eare not given\nas a parameter, but they are computed from the data in an\nadaptive way, controlled by the parameter \u0011. It is often\nmore advantageous to set the tolerance in terms of like-\nlihood (parameter \u0011) than in terms of deviation from the\nalignment path (parameter \u000e). Indeed, it is possible that\na wrong path obtains a slightly higher score than the right\none at a coarse level (for example a path following a differ-\nent repetition of a musical phrase). If this wrong path is far\n(in terms of states) from the right alignment, the latter onewill be discarded by the ﬁxed-radii pruning process. On\nthe other hand, it is reasonable to suppose that the “real”\nalignment path always obtains a high likelihood, and thus\nis not pruned out by our method.\n4. EXPERIMENTS\n4.1 Database and Evaluation Measure\nThe database used in this work comprises 94 songs of the\nRWC-pop database [7]. These songs are polyphonic multi-\ninstrumental pieces of length 2 to 6 minutes, most of which\ncontain percussion. The alignment ground-truth is given\nby the synchronized MIDI ﬁles provided with the record-\nings. The same MIDI ﬁles are exploited as target scores.\nHowever, as we intend to be able to handle any type of\nscore, in particular scores with missing or unreliable tempo\nindications, we artiﬁcially introduce (rather extreme) tempo\nmodiﬁcations in these MIDI ﬁles: every 4 bars, a random\ntempo change (between 40 and 240 bpm) is added.\nThe chosen evaluation measure is the recognition rate,\ndeﬁned as the fraction of onsets which are correctly de-\ntected less than \u0012= 300 ms away from the real onset time.\nThis threshold is based on the MIREX’06 contest1.\n4.2 Reference System: DTW\nWe compare our alignment models to a reference DTW\n(Dynamic Time Warping) system. The DTW algorithm\nsearches for the alignment between two sequences which\nminimizes the cumulative costs along the alignment path.\nThis method is used to synchronize the sequence of ob-\nservations (chroma vectors and spectral ﬂux) extracted from\nthe audio with a sequence built from the score. This “pseudo-\nsynthesis” is performed by associating to each chord a chroma\nvector template (having the same values as the probabil-\nity distributions of Sec. 2.1.1) and a duration given by the\nscore. The obtained sequence is then linearly stretched so\nthat its length is the same as the recording. For the onset\ndetection feature, the reference sequence is a sequence of\nzeros and ones, the ones correponding to the onset loca-\ntions in the “pseudo-synthesis”.\nFor this system, the spectral ﬂux sequence is locally\nnormalized so that its maximum is 1 on a 2-s sliding win-\ndow. The local distance between the observation (v;\u0016f)\n(respectively chroma vector and locally normalized spec-\ntral ﬂux) and the template counterpart (g;a) is given by:\nD\u0010\n(v;f);(g;a)\u0011\n=v\u0001g\nkvkkgk+wjf\u0000aj; (10)\nwhere\u0001denotes the inner product and wis a non-negative\nparameter which controls the weight given to the onset de-\ntection feature. Between three different values which have\nbeen testedf1\n2;1;2g, the value w= 1 has been found\nthe most efﬁcient on our database. A DTW system which\nconsiders only the chroma observation (corresponding to\nw= 0) is also evaluated.\n1Music Information Retrieval Evaluation eXchange 2006, score\nfollowing task: http://www.music-ir.org/mirex/2006/\nindex.php/Score_Following_Proposal\n42\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)System Recognition Rate Search Space\nDTW (only chroma) 78.77%100%DTW (chroma+onset) 86.07%\nChord 64.49% 16.2%\nOnset (b= 0) 69.70%\n26.3%Onset (b= 0:1) 70.49%\nOnset (b= 1) 73.14%\nOnset (b= 10 ) 82.90%\nOnset (b= 50 ) 87.16%\nOnset (b= 100 ) 84.71%\nTable 1 . Recognition rate and mean search space (fraction\nof the DTW algorithm search space) as a function of the\nalignment system.\n4.3 Performances of the Baseline Systems\nThe recognition rates and average search space of several\nsettings are summed up in Table 1. The search space is\nthe number of explored cells (state/frame pairs, or audio\nframe/pseudo-synthesis frame pairs, depending on the sys-\ntem) over the total number of cells required for the DTW\nalgorithm (the square of the number of audio frames).\nFirst, it can be seen that the DTW which considers only\nthe chroma observations performs better than the chord\nmodel. This is easily explained by the fact that the former\nsystem implicitly models the note durations in the pseudo-\nsynthesis stage, whereas the statistical models do not take\nthem into account. This increases the precision, but also\nthe search space (from 16.2% to 100%).\nHowever, the use of the onset information allows the\nonset model to overcome this shortcoming and to obtain a\nslighty better precision than the DTW systems, with a still\nlower complexity. Indeed, a recognition rate of 87.16% is\nobtained with a value of b= 50, against 86.07% for the\nDTW system wich takes into account the onset observa-\ntion, whereas the mean search space is 26.3%.\nThe increase of accuracy induced by the onset observa-\ntion is smaller in the DTW system than in the statistical\nmodels. This is probably due to the difﬁculty of modeling\nthe spectral ﬂux process. Indeed, this onset detection func-\ntion is not very well modeled by our binary templates, and\nthe logistic model of (3) seems to be more relevant to this\nprocess than the local distance of (10).\nThe increase of search space in the onset model is bene-\nﬁcial to the alignment precision. Indeed, even with a value\nofb= 0 (which means that the onset information is not\nused) the recognition rate increases from 64.49% (chord\nmodel) to 69.70%. The explanation lies in the fact that\nmost chords are then represented by two states. Thus the\nminimum duration of each chord is two frames instead of\none, which prevents the system from rapidly skipping sev-\neral states and leads to a smoother alignment path.\n4.4 Pruning Evaluation\nThis hierarchical pruning method is run on the RWC pop-\nular music database. The lowest-level model uses the on-\nsetstructure with parameter b= 50. Several values of the\npruning parameter \u0011have been tested and the experimental\nresults are summed up in Table 2. The mean search spaceSystemSearch Space Run time Errors\nBeats Onsets (in s) (nb)\nOnsetb= 50 – 26.26% 482 0\nBSNh= 700 – 5.74% 733 0\nMsDTW\u000e=150 2.24% 14.02% 1180 0\n\u000e= 60 0.81% 7.93% 362 0\n\u0011= 1000 0.42% 4.53% 300 0\n\u0011= 200 0.35% 4.07% 276 0\n\u0011= 100 0.33% 3.82% 265 0\n\u0011= 50 0.30% 3.59% 256 0\n\u0011= 20 0.26% 3.22% 240 0\n\u0011= 10 0.23% 2.97% 229 2\n\u0011= 5 0.19% 2.59% 215 2\nTable 2. Performance of our implementation of the align-\nment algorithm using different settings of the hierarchical\npruning method.\nsizes are displayed for each pruning setting at the beat and\nchord level, as the fraction of explored cells over the total\nnumber of cells used by the DTW algorithm. At the bar\nlevel, it is 0.16% for MsDTW and 0.04% for all the other\nsystems. For each setting, the total run-time is also pre-\nsented, as well as the number of “pruning errors” on the\nwhole database (94 songs). A pruning error occurs when a\npart of the ground truth alignment path is discarded by the\npruning process. The implementation of the algorithms is\nin MATLAB, and was run on a Intel Core2, 2.66 GHz with\n3.6 Go RAM under Linux.\nThe performance of three additional reference systems\nis displayed. This ﬁrst one is the baseline onset model with\nno pruning. The second reference system uses beam search\n(BS). This algorithm performs the decoding of the statisti-\ncal model similarly to the Viterbi algorithm, but maintains\nonly the best Nhpaths, according to the partial Viterbi\nscore of (7). The minimum value of Nhfor which the de-\ncoded path is the same as without pruning is Nh= 700.\nThe third reference system is a MsDTW (multi-scale\nDTW) system [11]. This system performs a DTW align-\nment, but it uses a coarse-to-ﬁne pruning process in order\nto keep only a ﬁxed neighborhood around the alignment\npath, at each level. The same three levels as in 3 are used.\nThe deviation parameter value \u000e= 150 is the minimum\nvalue yielding no pruning error on our database.\nFinally, the last one uses constant tolerance radii \u000e\u0000=\n\u000e+= 60. This value \u000eis the lowest one for which no\npruning errors are made.\nIn terms of alignment precision, all the systems which\ndo not make pruning errors obtain the same scores as the\nreference system (87.16%). Thus, the reduction of the\nsearch space does not affect the alignment precision.\nThe results show the beneﬁts of this pruning method,\nsince the search space and run time of all the tested systems\nwhich use it are lower than the reference system (without\npruning). As expected, the explored space decreases with\nthe value of\u0011. No pruning error occurs until a value of \u0011=\n20, whose corresponding run-time is half of the reference\nsystem (240 s against 484 s).\nThe beneﬁt of this method compared to a ﬁxed radius\n43\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Figure 5. Number of explored states per audio frame at\nthe onset level for each song of the database, without and\nwith pruning (onset model withb= 50 ).\n\u000ecan also be seen: the tested system with \u000e= 60 (the\nminimum value for no pruning error) runs in 362 s, and\nrequires more space than our “adaptive” pruning strategy.\nThe hierarchical strategy allows our approach to be more\neffective than beam search (3.53% search space against\n5.74%). Hence, considering the whole signal (although\nat a coarse level) seems to reduce the risk of following\na promising path which will eventually come to a “dead-\nend”. This problem could be adressed by estimating a\ntempo process in a beam search approach, such as in [15]\nor [4]. However the complexity of these models would be\nmuch higher.\nIn Fig. 5 are displayed the numbers of explored states\nper audio frame in each song of our database, for three\ndifferent pruning strategies: the reference system (without\npruning), the system using a ﬁxed radius \u000e= 60 and the\nsystem using our adaptive approach with \u0011= 20 . Both\npruning strategies achieve a signiﬁcant reduction of the\nsearch space on all the songs. More interestingly, we can\nsee that the search space width obtained with our pruning\nstrategy can greatly vary from songs to songs, whereas it\nis more or less constant with a ﬁxed \u000e(only affected by\nthe number of onset states in a beat). This variability is\nuncorrelated to the original number of states in the score,\nindicating that our approach manages to adapt the pruning\nprocess to the data. Thus, whereas in some cases, the width\nobtained with our method is greater than with a constant \u000e,\nit is most of the time signiﬁcantly smaller.\n5. CONCLUSION\nIn this paper, we show that a novel hierarchical pruning\napproach for the approximate decoding of a hidden state\nmodel leads to a good precision in our alignment task,\nwith a low complexity. In our experiments, we ﬁnd that\nthe recognition rate is even higher than a DTW system\nwhen a description of note onsets is used additionally to\nthe chroma vectors, while keeping a lower complexity than\nthis algorithm in the decoding phase.\nThe proposed hierarchical pruning method further re-\nduces the complexity without affecting the accuracy of the\nsystem. The main advantage of this strategy compared tothe one used in [11] is that the tolerance radii can adapt to\nthe data, yielding a better overall efﬁciency.\nIn the continuation of this work, we will address the use\nof a more elaborate model at the lowest level, which is now\nfeasible thanks to the pruning strategy. We will also try to\nfurther reduce the number of states in the model, by taking\nadvantage of the repetitions in the musical structure.\n6. REFERENCES\n[1] M. Alonso, G. Richard, and B. David. Extracting note\nonsets from musical recordings. In Proc. of ICME, 2005.\n[2] P. Cano, A. Loscos, and J. Bonada. Score-performance\nmatching using hmms. In Proc. of the ICMC, 1999.\n[3] A. Cont. Realtime audio to score alignment for poly-\nphonic music instruments using sparse non-negative\nconstraints and hierarchical hmms. In Proc. of ICASSP,\n2006.\n[4] A. Cont. A coupled Duration-Focused architecture for\nReal-Time Music-to-Score alignment. IEEE Trans. on\nPAMI, 32(6):974–987, June 2010.\n[5] A. Cont, D. Schwarz, and N. Schnell. Training ircam’s\nscore follower. In Proc. of ICASSP, 2005.\n[6] S. Ewert, M. Müller, and P. Grosche. High resolution\naudio synchronization using chroma onset features. In\nProc. of ICASSP, 2009.\n[7] M. Goto. Rwc music database: Popular, classical, and\njazz music databases, 2002.\n[8] L. Grubb and R. Dannenberg. A stochastic method of\ntracking a vocal performer. In Proc. of ICMC, 1997.\n[9] N. Hu, R. Dannenberg, and G. Tzanetakis. Polyphonic\naudio matching and alignment for music retrieval. In\nProc. of WASPAA, 2003.\n[10] H. Kaprykowsky and X. Rodet. Globally optimal short-\ntime dynamic time warping: Application to score to au-\ndio alignment. In Proc. of ICASSP, 2006.\n[11] M. Müller, H. Mates, and F. Kurth. An efﬁcient mul-\ntiscale approach to audio synchronization. In Proc. of\nISMIR, 2006.\n[12] K. Murphy. Dynamic Bayesian Networks: Representa-\ntion, Inference and Learning. UC Berkeley, july 2002.\n[13] N. Orio and D. Schwarz. Alignment of monophonic and\npolyphonic music to a score. In Proc. of ICMC, 2001.\n[14] C. Raphael. Automatic segmentation of acoustic musi-\ncal signals using hidden markov models. IEEE Trans.\non PAMI, 21:360–370, 1999.\n[15] C. Raphael. Aligning music audio with symbolic scores\nusing a hybrid graphical model. Machine Learning\nJournal, 65:389–409, 2006.\n[16] Y . Zhu and M. Kankanhalli. Precise pitch proﬁle feature\nextraction from musical audio for key detection. IEEE\nTrans. on Multimedia, 8(3):575–584, 2006.\n44\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "Music Structure Discovery in Popular Music using Non-negative Matrix Factorization.",
        "author": [
            "Florian Kaiser",
            "Thomas Sikora"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1418085",
        "url": "https://doi.org/10.5281/zenodo.1418085",
        "ee": "https://zenodo.org/records/1418085/files/KaiserS10.pdf",
        "abstract": "We introduce a method for the automatic extraction of musical structures in popular music. The proposed algo- rithm uses non-negative matrix factorization to segment re- gions of acoustically similar frames in a self-similarity ma- trix of the audio data. We show that over the dimensions of the NMF decomposition, structural parts can easily be modeled. Based on that observation, we introduce a clus- tering algorithm that can explain the structure of the whole music piece. The preliminary evaluation we report in the the paper shows very encouraging results.",
        "zenodo_id": 1418085,
        "dblp_key": "conf/ismir/KaiserS10",
        "keywords": [
            "automatic extraction",
            "musical structures",
            "popular music",
            "non-negative matrix factorization",
            "acoustically similar frames",
            "self-similarity matrix",
            "audio data",
            "structural parts",
            "NMF decomposition",
            "clustering algorithm"
        ],
        "content": "MUSIC STRUCTURE DISCOVERY IN POPULAR MUSIC USING\nNON-NEGATIVE MATRIX FACTORIZATION\nFlorian Kaiser and Thomas Sikora\nCommunication Systems Group\nTechnische Universit ¨at Berlin\nfkaiser, sikorag@nue.tu-berlin.de\nABSTRACT\nWe introduce a method for the automatic extraction of\nmusical structures in popular music. The proposed algo-\nrithm uses non-negative matrix factorization to segment re-\ngions of acoustically similar frames in a self-similarity ma-\ntrix of the audio data. We show that over the dimensions\nof the NMF decomposition, structural parts can easily be\nmodeled. Based on that observation, we introduce a clus-\ntering algorithm that can explain the structure of the whole\nmusic piece. The preliminary evaluation we report in the\nthe paper shows very encouraging results.\n1. INTRODUCTION\nMusic structure discovery (MSD) aims at characterizing\nthe temporal structure of songs. In the case of popular mu-\nsic, this means classifying segments of a music piece into\nparts such as intro, verse, bridge, chorus or outro. Knowing\nthis musical structure, one can introduce new paradigms in\ndealing with music collections and develop new applica-\ntions such as audio thumbnailing and summarization for\nfast acoustic browsing, active listening (audio based re-\ntrieval and organization engines), song remixing or restruc-\nturing, learning semantics, etc.\nIn the past years, MSD has therefore gained an increas-\ning interest in the music information retrieval community.\nThis also led to the constitution of common evaluation data\nsets and evaluation campaigns (MIREX 09) that strongly\nstimulate the research in this ﬁeld.\n1.1 Previous work\nStructure in music can be deﬁned as the organization of\ndifferent musical forms or parts through time. How we de-\nﬁne musical forms and what builds our perception of these\nforms is however an open question, and MSD algorithms\nthat have been proposed yet mainly differ in the way they\nanswer those questions. However, Bruderer gives in [2] a\ngeneral understanding of perception of structural bound-\naries in popular music, and shows that perception of struc-\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2010 International Society for Music Information Retrieval.ture is mainly inﬂuenced by a combination of changes in\ntimbre, tonality and rhythm over the music pieces. There-\nfore, MSD algorithms generally aim at ﬁnding similarities\nand repetitions in timbre, tonality and rhythm based de-\nscriptions of the audio signal.\nIn [4], Foote and Cooper addressed the task of music\nsummarization and proposed to visualize and highlight these\nrepetitions in the audio signal through a self-similarity ma-\ntrix. The audio signal is therefore parametrized through\nthe extraction of audio features and the similarity between\neach frame is then measured. Thus using different audio\nfeatures and similarity measures, most MSD algorithms\nare a processing of such a self-similarity representation.\nIn [13], the author distinguishes two categories of struc-\nture in the self-similarity matrix: the state representation\nand the sequence representation. The state representation\ndeﬁnes the structure as a succession of states (parts). Each\nstate is a succession of frames that show similar acoustic\nproperties and therefore forms blocks in the self-similarity\nmatrix. This representation is closely related to the notion\nof structural parts in popular music (intro - verse - chorus\n- outro), in which the acoustical information does not vary\nmuch. Algorithms based on state representation usually\nstart with a segmentation by audio novelty score method\n[5]. The segments are then merged together with mean of\nhierarchical clustering, spectral clustering, or HMM.\nOn the other hand, the sequence representation consid-\ners series of times (frames), that are repeated over the mu-\nsic piece. The sequence representation is more related to\nmusical concepts such as melody, progression in chords\nand harmony. Algorithms based on sequence representa-\ntion look for repetitions on the off-diagonals of the self-\nsimilarity matrix. Matrix ﬁltering of higher-order matrix\ntransformations [14] can also be applied to the self-similarity\nmatrix in order to emphasize off-diagonals. One of the\nmain drawbacks of the sequence representation is that the\nstructure of the music piece can not be fully explained un-\nless all sequences are repeated at least once.\n1.2 Approach\nNon-negative matrix factorization (NMF) is a low-rank ap-\nproximation technique that was ﬁrst introduced in [9]. It\nis known for extracting parts-based representation of data,\nthat strongly relates to some form of inherent structure\nin the data. Therefore, it has been successfully used in\n429\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Figure 1. Overview of the proposed music structure dis-\ncovery system\na wide range of multimedia information retrieval applica-\ntions such as text summarization [9] or sound classiﬁca-\ntion [1]. Moreover, Foote et al. showed in [3] that de-\ncomposing the self-similarity matrix of a video stream via\nNMF could help separating visually similar segments . We\npropose to extend the approach of Foote to music data.\nDeﬁning structural parts as acoustically similar regions\nlike in the state-representation, we apply NMF to the self-\nsimilarity matrix. We show that such structural parts can\neasily be discriminated over the dimensions of the obtained\ndecomposition. With a clustering approach, we are thus\nable to merge together similar audio segments in the NMF\ndecomposed matrices, and explain the structure of the whole\nmusic piece.\nIn the next section, we provide a detailed description\nof our system. Evaluation metrics, data set and results are\npresented in section 3. Section 4. concludes the paper.\n2. PROPOSED METHOD\nAn overview of our system is shown in Figure 1. In this\nsection each individual block of the system is described.\n2.1 Feature Extraction\nWe ﬁrst extract a set of audio features that are likely to\nmodel variations between different musical parts. As men-\ntioned in the introduction, perception of structural bound-\naries in music is mostly inﬂuenced by variations in timbre,\ntonality and rhythm [2]. However, few rhythmical changes\noccur between parts in our evaluation data set (see section\n3.) and we thus only focus on the description of timbre\nand tonality. Nevertheless, the reader might refer to [11]\nfor interesting work also using rhythmical clues for struc-\nture discovery.\nTimbre properties of the audio signal are described by\nextraction of the following features: the ﬁrst 13 MFCC\nFigure 2. Self-similarity matrix computed on the timbre-\nrelated features using the exponential variant of the cosine\ndistance. Audio ﬁle : ”Creep” by Radiohead\ncoefﬁcients, spectral centroid, spectral slope and spectral\nspread.\nTonality can be associated to the concepts of melody\nand harmony. Songs in a popular music context are how-\never very diverse and a melody extractor would hardly be\nrobust over a whole set of popular songs. We thus only fo-\ncus on the description of harmonic properties through the\nextraction of the chroma features. Chroma features are 12\ndimensional, each element corresponding to a pitch-class\nproﬁle of a 12 scaled octave.\nThe frame analysis is performed with mean of a window\nsize of 400 ms and a hop size of 200 ms. Each feature is\nnormalized to mean zero and variance one.\nTimbre-related features and chroma features are stored\nin two different feature matrices and processed separately.\n2.2 Self-Similarity Matrix\nAfter parameterization of the audio, we measure the simi-\nlarity between each signal frame in a self-similarity matrix\nS. Each element sijis deﬁned as the distance between the\nfeature vectors viandvj, extracted over frames iandj.\nThe cosine angle is used as a similarity measure :\nd(vi;vj) =<vi;vj>\njjvijjjjvjjj(1)\nAs proposed in [3], an exponential variant of this dis-\ntance is used to limit its range to [0,1] :\nde(v i;vj) =exp(d(v i;vj)\u00001) (2)\nAs an example, we extracted the timbre-related features\nover the song ”Creep” by Radiohead. The resulting self-\nsimilarity matrix is shown in Figure 2. One clearly sees\nthat structural information is conveyed by the self-similarity\nmatrix. Regions of acoustically similar frames form blocks\nin the matrix and one can also distinguish repetitions of\nthese blocks. This illustrates the state representation of\n430\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)structure, as explained in the introduction. In this spe-\nciﬁc example, there are few sequence repetitions to see\non the off-diagonals. In fact, the clearness of such se-\nquences in the self-similarity matrix pretty much depends\non the nature of the song and the features that describe it\n(chroma features tend to highlight sequences). In our ex-\nample, blocks are formed because of the strong presence\nof saturated guitar, which does not yield much timbre evo-\nlution within the structural parts.\n2.3 Segmentation\nOnce the audio has been embedded in the self-similarity\nmatrix S, a segmentation step is needed to estimate po-\ntential borders of the structural parts. Therefore the self-\nsimilarity matrix is segmented using the audio novelty score\nintroduced in [5]. The main idea is to detect boundaries\nby correlating a Gaussian checkerboard along with the di-\nagonal of the self-similarity matrix S. The checkerboard\nbasically models the ideal shape of a boundary in S. The\ncorrelation values yield a novelty score in which local max-\nima indicate boundaries. We apply an adaptive threshold\nas described in [6] to detect these maxima and generate the\nsegmentation.\n2.4 Non-negative Matrix Factorization\nMatrix factorization techniques such as principal compo-\nnents analysis (PCA), independent component analysis (ICA)\nor vector quantization (VQ) are common tools for the anal-\nysis of multivariable data and are mainly used for dimen-\nsionality reduction purposes. In [7], Lee and Seung intro-\nduced non-negative matrix factorization (NMF), and pro-\nposed to build the decomposition additively by applying\na non-negativity constraint on the matrix factors. Unlike\nPCA and other factorization techniques, cancelation of the\ndecomposed data is thus not allowed, leading to a parts-\nbased representation of the data. An intuitive justiﬁcation\nis that not allowing negative coefﬁcients in the decompo-\nsition will prevent the loss of the physical meaning of the\ndata.\nGiven an n\u0002mnon-negative matrix V, NMF aims at\nestimating the non-negative factors W(n\u0002r) andH(r\u0002\nm), that best approximate the original matrix :\nV\u0019WH (3)\nWcontains the basis vectors and Hthe encoding coef-\nﬁcients for the best approximation of V. The rank of the\ndecomposition ris usually chosen so that (n+m)r < nm,\nthus providing a compressed version of the original data.\nIn our approach, we compute NMF on the self-similarity\nmatrix of the audio in order to separate basic structural\nparts. The algorithm we use for the estimation of the ma-\ntrix factors WandHis detailed in [8]. In the next sec-\ntion, we describe how the factorization via NMF relates to\nstructure and show how we can use that result for music\nstructure discovery.\nFigure 3. Matrices A1andA2obtained by NMF decom-\nposition of the timbre self-similarity matrix of the song\n”Creep” (see Figure 2).\n2.5 NMF based feature space\nAfter decomposition via NMF, each element sijofScan\nbe written as:\nsij\u0019rX\nk=1Ak(i; j) (4)\nwith\nAk=W(:; k)H(k; :) (5)\nTo illustrate how NMF can decompose data into basic struc-\ntural parts, we compute NMF on the self-similarity matrix\ncalculated over the song ”Creep” by Radiohead. The rank\nof decomposition is set to 2 and the decomposed matrices\nA1andA2are shown in Figure 3.\nAccording to the timbre description in Figure 2, we can\nsay that the music piece is composed of two main struc-\ntural parts. Figure 3 shows that these two parts are strongly\nseparated over the two dimensions of the NMF decompo-\nsition.\nThis suggests that each dimension of the NMF decom-\nposition somehow relates the contribution of a structural\npart in the original data. In other words, that means that\nthere is a speciﬁc energy distribution over the dimensions\nof the decomposition for each structural part.\nTherefore it seems relevant to study for each segment\nhow the energy is distributed over the matrices Ak. In or-\nder to consider temporal dependencies, we choose to con-\nsider segments as successions of frames in matrices Ak,\nand not as blocks. That means that each frame from the\nmusic piece is represented by its corresponding values over\nthe diagonals of matrices Ak. We thus deﬁne the feature\nvector dk, representing the contribution of the kthdecom-\nposition over all frames:\ndk=diag(Ak) (6)\nEach frame can then be represented in the (n\u0002r)feature\nspace D:\nD= [d1d2: : :dr] (7)\nTo illustrate this approach, we show an example with\nthe song ”Help” by The Beatles. The self-similarity matrix\nScomputed on the timbre features of the song and the an-\nnotated structure are plotted in Figure 4. We compute the\n431\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Figure 4. Self-similarity matrix computed on the timbre-\nrelated features for the song ”Help” by The Beatles. The\nblack boxes indicate the annotated segments, with A being\nthe intro, B the verse, C the chorus and D the outro.\nFigure 5. Representation of the structural parts of the song\n”help” in the feature space D\nNMF decomposition of S. For visualization purposes, the\nrank of decomposition is set to 3. In Figure 5, each of the\nannotated segments is represented in the feature space D.\nIt is clear that structural parts chorus, verse and outro tend\nto be well represented over feature vectors d1,d2andd3\nrespectively. In this case, we can say that each dimension\nof the NMF decomposition relates the contribution of a\nstructural part. It is also interesting to note that segments\nof the same structural part seem to follow similar trajecto-\nries, suggesting that temporal dependencies should also be\nconsidered.\nIn classiﬁcation problems, a feature space should pro-\nvide good separability between classes. This means that\nthe set of observations for a single class should have a\nsmall variance, whereas the set of all observations (for all\nthe classes) should have a large variance. In that sense and\naccording to Figure 5, representing segments in the feature\nspace Dshould provide a good basis for structural classi-\nﬁcation.2.6 Clustering\nEach found segment is now represented in the NMF based\nfeature space D. In order to merge together segments be-\nlonging to the same structural part, we propose to use a\nclassical clustering approach. Therefore, the similarity be-\ntween segments in Dis measured with:\n\u000fThe Bayesian information criterion (BIC)\n\u000fThe Mahalanobis distance\nThe clustering is performed using the two measures sep-\narately. A comparison of the performance obtained with\nboth measures is done in section 3. The clustering is done\nwith a classical hierarchical approach.\n3. EVALUATION\n3.1 Data set\nThe evaluation data set consists of 174 songs from The\nBeatles, that were ﬁrst manually annotated at Universistat\nPompeu Fabra (UPF)1. Some corrections to the annotation\nwere made at Tampere University of Technology (TUT)2.\nWe call the data set TUT Beatles.\nThe structure in each music piece is annotated as a state\nrepresentation and not as sequences (see section 1.). Each\nframe is thus affected to a label.\n3.2 Metrics for the clustering evaluation\nEvaluating the performance of a music structure detection\nalgorithm is not simple. In fact musical structures are mostly\nhierarchical [10], meaning that the structure can be ex-\nplained at different levels. For example, a structure A-B-\nA, could be also be described as abc-def-abc. We choose\nto evaluate our system using the pairwise precision, re-\ncall and F-measure. Therefore, we deﬁne Fathe set of\nidentically labelled frames in the reference annotation, and\nFethe set of identically labelled frames in the estimated\nstructure. Pairwise precision, recall and F-measure, re-\nspectively noted P,RandFare then deﬁned as :\nP=jFeTFaj\njFej(8)\nR=jFeTFaj\njFaj(9)\nF=2PR\nP+R(10)\nThese measures are not perfect for evaluating MSD al-\ngorithms because they do not reﬂect hierarchical aspects\nin the description of structure. Nevertheless, they give an\nidea of the global performance of the system.\n1http://www.iua.upf.edu/%7Eperfe/annotaions/sections/license.html\n2http://www.cs.tut.ﬁ/sgn/arg/paulus/structure.html\n432\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)F-measure Precision Recall\nTimbre 58.6% 58.1% 61.9%\nChroma 50% 46.5% 52.2%\nboth 53.6% 49% 55%\nTable 1. Segmentation evaluation with the TUT Beatles\ndatabase\n3.3 Segmentation Evaluation\nWe evaluate the segmentation step with classical F-measure,\nprecision and recall. Table 1 reports the performance of the\nsegmentation computed on the timbre-related self-similarity\nmatrix, the chroma-related self-similarity matrix and the\nsum of the two matrices.\nThe low precision rate in the segmentation suggests that\nthe algorithm tends to over-segment the audio. In fact,\nstructure is hierarchical and the annotation labels high level\nparts of the structure. The clustering might cope with that\nby reassembling segments from the same structural part.\n3.4 Rank of decomposition\nWe ran a small experiment in order to choose a suitable\nrank for the NMF. Over a subset of ten songs from the\ndatabase, we compute the similarity matrices. Varying the\nrank of NMF rfrom 3 to 12, we measure the separability\nbetween structural parts along each dimension diofD. To\ndo so, we compute the inertia ratio of the variance of di\nwithin segments belonging to the same structural part and\nthe variance of diover the whole music piece [12]:\ns(i) =PK\nk=1N\nNk(mk\u0000mi)(mk\u0000mi)0\n1\nNPN\nn=1(di(n)\u0000mi)(di(n)\u0000mi)0(11)\nWith Kbeing the number of structural parts, Nkthe\nnumber of frames in structural part k and Nthe total num-\nber of frames. miis the mean of diover the all piece and\nmkthe mean value of diover the kthstructural part. For\na given rank of decomposition r, the separability is then\nmeasured as the mean of s:\nsep(r) =1\nrrX\ni=1s(i) (12)\nWe ﬁnd a maximum of separability with a rank of 9 for\nNMF (see Figure 6). It is larger than the median number\nof annotated parts. In fact, as structure can be explained at\ndifferent hierarchical levels, we don’t expect the NMF de-\ncomposition to match the parts described in the annotation\none-by-one.\n3.5 Experimental set up for the clustering\nSelf-similarity matrices are computed over the timbre and\nchroma features separately. As shown in Table 1, seg-\nmentation using the timbre features provides better per-\nformances. Therefore, in the evaluation of the clustering\nstep, we only use the segments positions extracted over\nthe timbre-related self-similarity matrix. We propose four\nFigure 6. Separability of structural parts given different\nranks of decomposition\nstrategies to evaluate our clustering approach. For the three\nﬁrst strategies, the NMF based feature space is obtained by\ndecomposition of the timbre-related self-similarity matrix\n(labeled as ”Timbre” ), the chroma-related self-similarity\nmatrix (labeled as ”Chroma”) and the sum of the two matri-\nces (labeled as ”Fusion 1”). We also study a second fusion\nstrategy where similarity between segments is computed\nseparately in the timbre and chroma related feature spaces\nand then summed for the clustering algorithm (labeled as\n”Fusion 2”).\nWe also compare the clustering obtained using the auto-\nmatic segmentation described in section 2. (labeled ”auto”)\nand using the annotated segments (labeled ”manual”). Fi-\nnally, each conﬁguration is run using the BIC (Table 2) and\nthe Mahalanobis distance (Table 3) as similarity measure\nfor the clustering algorithm.\nThe number of clusters is set to 4, which is the median\nnumber of annotated parts within a song in our evaluation\ndata set.\n3.6 Clustering Evaluation and Discussion\nAs a reference we use the system described in [11], that\nwas also evaluated on the TUT Beatles database. The sys-\ntem is based on a description of the audio signal through\nMFCC, chroma and rhythmogram features. Each of these\nfeatures is then used to estimate the probability for two\nsegments to belong to the same structural part and a ﬁt-\nness measure of the description is introduced. A greedy\napproach is used to generate the candidate descriptions.\nEvaluation of the whole system is reported in Tables\n2 and 3, using BIC and Mahalanobis distance respectively.\nCompared to the reference system, our system shows slightly\nbetter F-measure rates. The interesting result is that we\nshow signiﬁcantly better recall rates. This suggests that our\nalgorithm splits the parts in the annotation as sequences of\nsub-parts. This also explains why we don’t match the pre-\ncision rates in [11]. There again, the annotation relates a\nhigh stage of the structure hierarchy, and over-segmentation\ncauses a lack of precision. Modeling sequences of basic\nparts in our algorithm might cope with that. This also ex-\nplains the huge gain of performance when using the anno-\ntated segments for the evaluation.\n433\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Method Segmentation F P R\n[11] 59.9% 72.9% 54.6 %\nTimbre auto 60.2% 64.7% 60%\nmanual 76.1% 83.6% 72.6%\nChroma auto 60.5% 66% 59.6%\nmanual 80% 87% 76.6%\nFusion 1 auto 60.6% 65% 60%\nmanual 78.7% 85% 76.4%\nFusion 2 auto 60.2% 64.7% 60%\nmanual 80% 86.5% 77%\nTable 2. Evaluation on TUT Beatles, BIC\nMethod Segmentation F P R\n[11] 59.9% 72.9% 54.6 %\nTimbre auto 61% 62.4% 63.3%\nmanual 78.4% 82.1% 78.3%\nChroma auto 60.8% 61.5% 64:6%\nmanual 76.6% 81.2% 75.7%\nFusion 1 auto 62:1% 63.6% 64.5%\nmanual 77.8% 82.3% 77%\nFusion 2 auto 61% 62.4% 63.3%\nmanual 78% 81.7% 78.2%\nTable 3. Evaluation on TUT Beatles, Mahalanobis\nObviously, fusing both timbral and chroma description\nas in the ”Fusion 1” strategy makes sense and improves\nthe overall performance of the system. Finally, using the\nMahalanobis distance yields better performances than the\nBIC.\n4. CONCLUSIONS\nWe introduced a music structure discovery method that\nuses the ability of NMF to generate parts-based represen-\ntation of data. The evaluation conducted on the TUT Beat-\nlesdata set shows that we are able to obtain slightly better\nperformances than the reference system introduced in [11].\nThe improvements we obtain in the recall rates however\nsuggest that there is still room for improvements. More-\nover, the method used for the clustering of segments in the\nNMF based feature space only considers statistical similar-\nity between the segments over time. We will consider mod-\neling time dependencies between frames and thus model\ntrajectories in the feature space instead of clouds of points.\nThe NMF processing itself could also be enhanced by us-\ning sparse constraints on the matrix factors. Further eval-\nuation on more diverse audio material will be done. The\nﬁrst results we obtained are however very encouraging.\n5. ACKNOWLEDGMENT\nThis work was supported by the European Commission un-\nder contract FP7-21644 PetaMedia.6. REFERENCES\n[1] Emmanouil Benetos, Margarita Kotti, and Constan-\ntine Kotropoulos. Musical instrument classiﬁcation us-\ning non-negative matrix factorization algorithms. In IS-\nCAS. IEEE, 2006.\n[2] Michael J. Bruderer, Martin F. McKinney, and Armin\nKohlrausch. Structural boundary perception in popular\nmusic. In ISMIR, pages 198–201, 2006.\n[3] Matthew L. Cooper and Jonathan Foote. Summariz-\ning video using non-negative similarity matrix factor-\nization. In IEEE Workshop on Multimedia Signal Pro-\ncessing, pages 25–28. IEEE Signal Processing Society,\n2002.\n[4] Jonathan Foote. Visualizing music and audio using\nself-similarity. In ACM Multimedia (1), pages 77–80,\n1999.\n[5] Jonathan Foote. Automatic audio segmentation using a\nmeasure of audio novelty. In IEEE International Con-\nference on Multimedia and Expo (I), page 452, 2000.\n[6] A.L. Jacobson. Auto-threshold peak detection in phys-\niological signals. In Engineering in Medicine and Bi-\nology Society, 2001. Proceedings of the 23rd Annual\nInternational Conference of the IEEE, volume 3, pages\n2194–2195 vol.3, 2001.\n[7] Daniel D. Lee and H. Sebastian Seung. Learning the\nparts of objects by non-negative matrix factorization.\nNature, 401(6755):788–791, oct 1999.\n[8] Daniel D. Lee and H. Sebastian Seung. Algorithms for\nnon-negative matrix factorization, July 21 2000.\n[9] Ju-Hong Lee, Sun Park, Chan-Min Ahn, and Daeho\nKim. Automatic generic document summarization\nbased on non-negative matrix factorization. Inf. Pro-\ncess. Manage, 45(1):20–34, 2009.\n[10] Namunu C. Maddage. Automatic structure detection\nfor popular music. IEEE MultiMedia, 13:65–77, 2006.\n[11] Jouni Paulus and Anssi Klapuri. Music structure anal-\nysis using a probabilistic ﬁtness measure and a greedy\nsearch algorithm. IEEE Transactions on Audio, Speech\n& Language Processing, 17(6):1159–1170, 2009.\n[12] Geoffroy Peeters. Automatically selecting signal de-\nscriptors for sound classiﬁcation. In ICMC, 2002.\n[13] Geoffroy Peeters. Deriving musical structures from\nsignal analysis for music audio summary generation:\n”sequence” and ”state” approach. In Uffe Kock Wiil,\neditor, CMMR, volume 2771 of Lecture Notes in Com-\nputer Science, pages 143–166. Springer, 2003.\n[14] Geoffroy Peeters. Sequence representation of mu-\nsic structure using higher-order similarity matrix and\nmaximum-likelihood approach. In ISMIR, 2007.\n434\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "Looking Through the &quot;Glass Ceiling&quot;: A Conceptual Framework for the Problems of Spectral Similarity.",
        "author": [
            "Ioannis Karydis",
            "Milos Radovanovic",
            "Alexandros Nanopoulos",
            "Mirjana Ivanovic"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1417283",
        "url": "https://doi.org/10.5281/zenodo.1417283",
        "ee": "https://zenodo.org/records/1417283/files/KarydisRNI10.pdf",
        "abstract": "Spectral similarity measures have been shown to exhibit good performance in several Music Information Retrieval (MIR) applications. They are also known, however, to pos- sess several undesirable properties, namely allowing the existence of hub songs (songs which frequently appear in nearest neighbor lists of other songs), “orphans” (songs which practically never appear), and difficulties in distin- guishing the farthest from the nearest neighbor due to the concentration effect caused by high dimensionality of data space. In this paper we develop a conceptual framework that allows connecting all three undesired properties. We show that hubs and “orphans” are expected to appear in high-dimensional data spaces, and relate the cause of their appearance with the concentration property of distance / similarity measures. We verify our conclusions on real mu- sic data, examining groups of frames generated by Gaus- sian Mixture Models (GMMs), considering two similar- ity measures: Earth Mover’s Distance (EMD) in combi- nation with Kullback-Leibler (KL) divergence, and Monte Carlo (MC) sampling. The proposed framework can be useful to MIR researchers to address problems of spectral similarity, understand their fundamental origins, and thus be able to develop more robust methods for their remedy.",
        "zenodo_id": 1417283,
        "dblp_key": "conf/ismir/KarydisRNI10",
        "keywords": [
            "Spectral similarity measures",
            "Music Information Retrieval",
            "Hub songs",
            "Orphans",
            "Distance/concentration effect",
            "Gaussian Mixture Models",
            "Earth Movers Distance",
            "Kullback-Leibler divergence",
            "Monte Carlo sampling",
            "Real music data"
        ],
        "content": "LOOKING THROUGH THE “GLASS CEILING”: A CONCEPTUAL\nFR\nAMEWORK FOR THE PROBLEMS OF SPECTRAL SIMILARITY\nIoannis Karydis\nDept. of Informatics\nIonian University\nGreece\nkarydis@ionio.grMilo ˇs Radovanovi ´c\nFaculty of Science\nUniversity of Novi Sad\nSerbia\nradacha@dmi.rsAlexandros Nanopoulos\nInst. of Computer Science\nUniversity of Hildesheim\nGermany\nnanopoulos@ismll.deMirjana Ivanovi ´c\nFaculty of Science\nUniversity of Novi Sad\nSerbia\nmira@dmi.rs\nABSTRACT\nSpectral similarity measures have been shown to exhibit\ngood performance in several Music Information Retrieval\n(MIR) applications. They are also known, however, to pos-\nsess several undesirable properties, namely allowing the\nexistence of hub songs (songs which frequently appear in\nnearest neighbor lists of other songs), “orphans” (songs\nwhich practically never appear), and difﬁculties in distin-\nguishing the farthest from the nearest neighbor due to the\nconcentration effect caused by high dimensionality of data\nspace. In this paper we develop a conceptual framework\nthat allows connecting all three undesired properties. We\nshow that hubs and “orphans” are expected to appear in\nhigh-dimensional data spaces, and relate the cause of their\nappearance with the concentration property of distance /\nsimilarity measures. We verify our conclusions on real mu-\nsic data, examining groups of frames generated by Gaus-\nsian Mixture Models (GMMs), considering two similar-\nity measures: Earth Mover’s Distance (EMD) in combi-\nnation with Kullback-Leibler (KL) divergence, and Monte\nCarlo (MC) sampling. The proposed framework can be\nuseful to MIR researchers to address problems of spectral\nsimilarity, understand their fundamental origins, and thus\nbe able to develop more robust methods for their remedy.\n1. INTRODUCTION\nThe notion of audio-based music similarity is generally\nconsidered to be complex, subjective, and context depen-\ndent [13]. However, spectral similarity measures [2,10] are\nreceiving a growing interest and have been shown to ex-\nhibit good performance in several Music Information Re-\ntrieval (MIR) applications [14]. These measures describe\naspects related to timbre and model the “global sound” of\na musical signal based on features called Mel Frequency\nCepstrum Coefﬁcients (MFCCs).\nDespite the advantages of spectral similarity measures,\nrelated research has also reported a number of undesired\nproperties, summarized as follows:\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2010 International Society for Music Information Retrieval.•The existence of hub songs (also called “always sim-\nilar”) [2], which are close neighbors of many other\npieces to which they hold no perceptual similarity,\nthus increasing the rate of false positives.\n•The existence of “orphans” (also called “never simi-\nlar”) [12], which are songs that rarely become close\nneighbors of any other piece (despite possibly hav-\ning perceptual similarity to a large number of songs),\nincreasing therefore the rate of true negatives.\n•Songs are represented in a feature space whose di-\nmensionality is determined by the number of fea-\ntures (MFCCs). As the dimensionality grows, it is\nbecoming hard to identify meaningful nearest neigh-\nbors, since all songs tend to be at nearly the same\ndistance from each other. This property was identi-\nﬁed in other research areas [1, 5, 8], but was also ex-\namined in the contexts of MIR [6] and audio speech\ndata (based on MFCCs) [18].\nThese undesired properties constitute some of the main\ncauses for the empirically demonstrated upper limit for the\nperformance of spectral similarity measures, referred to as\nthe “glass ceiling” [2]. Recent research has focused mostly\non the amelioration of hubness (the attribute of being a hub\nsong), by proposing techniques for normalizing the dis-\ntances between songs in a way that reduces the inﬂuence of\nhubs [11,14,15], whereas other works [9,17,19] developed\nmeasures that try to avoid hubness.\nOur motivation is to develop a conceptual framework\nthat allows for relating all three aforementioned undesired\nproperties, and explains the mechanisms that create them.\nAucouturier and Pachet [3] have focused on the analysis of\nhubness and concluded that the creation of homogenized\nmodels (i.e., models that ignore the least likely mixture\ncomponents) are responsible for creating hubs. Despite\nthis detailed conclusion, our emphasis is to disclose a more\nfundamental reason that causes all three undesired proper-\nties, which is the high dimensionality of the feature space\nthat originates from the need to use multiple MFCCs in\norder to capture the “global sound”.\nA conjecture about the role of high dimensionality has\nbeen stated by Berenzweig in his thesis [4]. This conjec-\nture was drawn from two synthetic data sets that follow\nmultivariate Gaussian distributions. In particular, a main\nconclusion of this thesis [4, page 99] was: “First, hubness\n267\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)seems to be a natural consequence of the curse of dimen-\nsi\nonality, at least for the points distributed according to a\nGaussian in a space up to 32 dimensions. In high dimen-\nsions these points tend to be spread around the shell of the\nspace with very few points near the center; this implies that\nany points that do happen to remain near the center will\nbe extreme hubs.” However, this work neither generalized\nthe conclusion to real audio music data, nor even to other\nsettings besides simple synthetic data. More importantly,\nit did not provide a clear explanation of the mechanism\nthat creates hubness, leaving this question unresolved [4,\nquestion 1 on page 99]. A more thorough examination of\nhubness has been performed by Radovanovi´ c et al. [16],\nwherein using real vector-space data the authors relate hub-\nness with the intrinsic dimensionality of data, and show\nthat in (intrinsically) high-dimensional data sets hubs tend\nto appear in the proximity of cluster centers. However, [16]\nfocused primarily on general vector spaces and lpnorms,\nwith the results not directly applicable for MIR purposes.\nIn this paper, we propose a conceptual framework to\nprovide a clear explanation of the mechanism that creates\nhubness and show that hubs are expected to appear in high-\ndimensional spaces (i.e., they are not points that just hap-\npento remain near the center). Moreover, the framework\nhelps to understand the connection between all three un-\ndesired properties: hubs, “orphans” and the problem of\nﬁnding meaningful neighbors. Also, our conclusions are\nveriﬁed with real audio music data. The proposed concep-\ntual framework can be useful to MIR researchers to address\nthe problems of spectral similarity in relation to each other,\nunderstand their fundamental reasons, and thus be able to\ndevelop more robust methods for their remedy.\nIn the rest of this article, Section 2 reviews related work.\nSection 3 presents the proposed framework, whereas Sec-\ntion 4 provides empirical evidence for verifying the con-\nclusions of the proposed framework in the MIR domain.\nFinally, Section 5 concludes the paper.\n2. RELATED WORK\nResearch by Aucouturier and Pachet [3] focuses on the na-\nture and causes of hub songs. They propose methods to de-\ntect hubs and infer that hubs are distributed along a scale-\nfree distribution. Moreover, in their work they deduce that\nhubs neither exist due to the spectral features, nor are they\na property of a feature representation or a given modeling\nstrategy but rather tend to occur with any type of model that\nuses agglomeration of multiple frames of a sound texture.\nFurthermore, they establish that hubness is not a charac-\nteristic of certain songs, as different algorithms distribute\nhubs differently in a database. In addition, they also es-\ntablish that the class of algorithms studied is irrelevant to\nhubs which appear only for data with a given amount of\nheterogeneity. Finally, they conclude that hubness can be\nlocalized to certain parts of the distribution of a song.\nBerenzweig [4] offers insight as to the understanding of\nhubs and arrives to the conclusion that their existence is a\nnatural result of the curse of dimensionality. Additionally,\nin his work, the possibility of hubs being derived from sim-ilarity to a universal background is proven invalid through\nexperimentation, that is by showing that the discriminating\npower of speciﬁc frames is not ameliorated by weighting\nbased on their shared information.\nAs mentioned in Section 1, unlike Aucouturier and Pa-\nchet [3] our motivation is to provide high dimensionality as\na more fundamental reason for hubness, and for the other\ntwo undesired properties (see Section 1) as well. Differen-\ntiating also from the work of Berenzweig [4], we develop\na thorough conceptual framework that links all three prop-\nerties and clearly explains the mechanisms through which\nthey originate.\nOther related work includes techniques to ameliorate or\ntry to avoid hubness [9, 11, 14, 15, 17, 19]. We hope that\nour proposed framework will assist in this direction, by\nallowing MIR researchers to further analyze the causes of\nall examined undesired properties (not just hubness), and\ndevelop solutions that take into account all of them.\n3. PROPOSED CONCEPTUAL FRAMEWORK\nWe commence the description of the proposed conceptual\nframework by demonstrating the property of concentra-\ntion [8] that is exhibited by spectral similarity measures\ndue to the high dimensionality of their feature space. Next,\nwe examine how the generation of hubs and “orphans” can\nbe explained as a consequence of high dimensionality, in\nrelation with the concentration phenomenon. The conclu-\nsions (in this and the following section) are veriﬁed with\nreal data. Since our description involves some empirical\nmeasurements, we start by detailing the employed settings.\n3.1 Settings for Empirical Measurements\nWe focus on two characteristic spectral similarity measures\nthat have been widely used in related research. The ﬁrst\nis proposed by Logan and Salomon [10], and uses Earth\nMover’s Distance (EMD) in combination with Kullback-\nLeibler (KL) divergence to compute the distance between\ngroups of frames generated by a GMM approach. The sec-\nond is proposed by Aucouturier and Pachet [2], and uses\nMonte Carlo (MC) sampling to measure the similarities of\nGMMs. Henceforth, the ﬁrst measure is denoted as EMD-\nKL, whereas the second as GMM-MC. We based our im-\nplementation of both measures on the MA toolbox [12].\nThe main parameter examined for both measures is the\nnumber of MFCCs, denoted as d, which corresponds to the\ndimensionality of the feature space, since each frame of the\naudio signal is mapped to a point in a d-dimensional space.\nThe default value for the number of clusters used in\nGaussian mixture modeling performed by GMM-MC and\nEMD-KL is equal to one, since measures like G1C [12]\nhave demonstrated the efﬁcacy of this option. In our ex-\nperiments we also examined other values in order to ver-\nify that this factor does not have any impact on our con-\nclusions. For brevity we therefore present results only for\nthe default number of clusters. Regarding other parameters\n(sampling rate, frame size, etc.), empirical evidence in re-\nlated work [3], and our experiments, indicates that they are\nnot related with the examined issues. For this reason, we\n268\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)keep the remaining parameters at the default values from\nth\ne MA toolbox, which correspond to commonly used val-\nues in most related works. The sampling frequency of the\ninput wav ﬁle is 11025, the FFT window size is 512 sam-\nples and the FFT window hop size is 256 samples. Finally,\nwe used the MIREX’04 audio collection for the reasons\nthat it is widely used by the MIR community, has been in-\nvolved in all related work (e.g., [3]), and is publicly avail-\nable, allowing reproducibility of the presented results.\n3.2 The Property of Concentration\nThe concentration property of a distance (similarity) mea-\nsure refers to the tendency of all points in a high-dimen-\nsional feature space to be almost equally distant from (sim-\nilar to) each other. Concentration has been studied in vec-\ntor spaces for Euclidean distance and other lpnorms (in-\ncluding fractional distances) [1, 8], and was also analyzed\nin the MIR context [6], but not explicitly for the spectral\nsimilarity measures we are focusing on in this work.\nTo ease comprehension, we ﬁrst consider iid Gaussian\nrandom data with d-dimensional points, the components\nof which are independently drawn from N(0,1). Figure 1\nillustrates the concentration of Euclidean distance that in-\ncurs with high dimensionality. In particular, the ﬁgure de-\npicts from top to bottom: the maximal observed value,\nmean value plus one standard deviation, the mean value,\nmean value minus one standard deviation, and minimal\nobserved value of distances of all data points to the ori-\ngin. It can be seen that the mean value steadily increases\nwith increasing dimensionality, while the standard devia-\ntion remains constant, and that the observed minimal and\nmaximal values become constrained to a narrow interval as\ndimensionality increases. This means that distances of all\npoints to the origin (i.e., the norms) become very similar to\neach other as dimensionality increases, with the same be-\nhavior also extending to all pairwise distances within a data\nset [8], thus making it harder to distinguish between the\nfarthest and the nearest neighbor in high dimensions [1].\nIt is important to note that this property of distances was\nproven to hold for anyiid random data distribution [8].\n0 20 40 60 80 10002468101214\ndEuclidean distance\nFigure 1 . Concentration of Euclidean distance for iid\nG\naussian random data ( n= 2000 points).\nConcentration is usually expressed as the ratio between\nsome measure of spread, like the standard deviation, and\nsome measure of magnitude, like the mean value, of dis-\ntances of all points in a data set to some arbitrary reference\npoint [1, 8]. If this ratio converges to 0 as dimensionalitygoes to inﬁnity, it is said that distances concentrate. Re-\ngarding the aforementioned synthetic data set, if the origin\nis selected as the reference point, Fig. 2 illustrates the ra-\ntio between the standard deviation ( σdist) and mean value\n(µdist) of distances of all points to the origin, showing that\nit tends to 0 as dimensionality increases. Moreover, theo-\nretical results by Franc ¸ois et al. [8] indicate that the same\nasymptotic behavior holds with any other point selected as\nthe reference, and also extends to all pairwise distances in a\ndata set. In the case of Euclidean distance, the mean value\nµdistbehaves asymptotically as√\nd, whereas the standard\nd\neviation σdistremains asymptotically constant.\n0 20 40 60 80 10000.20.40.60.8\ndσdist / µdistEuclidean distance\nFigure 2 . Ratio between the standard deviation and mean\nv\nalue of the distribution of distances to the origin, for iid\nGaussian random data.\nThe property of concentration can be used to explain the\ngeneration of hubs and “orphans” as follows. Existing the-\noretical and empirical results [1, 5] specify that concentra-\ntion can be viewed as causing points in a high-dimensional\ndata set to approximately lie on the surface of a hyper-\nsphere centered at an arbitrary point. In addition, further\nresults [7, 8], as illustrated in Fig. 1, indicate that the dis-\ntribution of distances to any reference point has a ﬁnite\nvariance for any given d. If the data distribution center is\ntaken to be the reference point (as, coincidentally, is the\ncase in the previously used random data example), it can\nbe said that it is expected for points closer to the data cen-\nter to exist in high dimensions, since for any ﬁnite dthe\nstandard deviation of the distribution of distances to the\ndata set center is ﬁnite (in this case, constant). However, in\nhigher dimensions, the points closer to the center have the\ntendency to become closer, on average, to all other points,\nthus having increased probability of becoming hubs by be-\ning near neighbors of many remaining points. On the other\nhand, it is also expected to have a non-negligible number of\npoints farther from the data set center. Consequently, these\npoints, which are the “orphans”, become farther from all\nother points and more difﬁcult to be near neighbors of any\nother point. The aforementioned connection between con-\ncentration and hubs/“orphans” has been initially proposed\nby Radovanovi´ c et al. [16] using experimentation on iid\nuniform random data, in contrast to Gaussian random data\nwhich pertains to real musical data utilized in this paper.\nStill, it is important to note that based on the results in [8],\nthe reasoning followed in [16] can be applied to anyiid\nrandom data distribution.\n269\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)In order to clarify the mechanism through which the\n“c\nentrality” of a point close to the data center, i.e. its prox-\nimity to all other points, becomes ampliﬁed in high di-\nmensions, let us return to the iid Gaussian random data\nexample and observe as reference points (instead of the\norigin) two points with the following properties: point x0,\nwhich is at the expected distance from the data center for\na given dimensionality d, and point x2, which is two stan-\ndard deviations closer to the center than x0.1Next, we\ncompute the distributions of distances of x0andx2to all\nother points, and denote the means of these distributions\nµx0andµx2, respectively. Figure 3 plots the difference\nbetween µx0andµx2. It can be seen that this difference\nincreases with increasing dimensionality, meaning that x2\nbecomes closer, on average, to all other points, solely by\nvirtue of increasing dimensionality. According to the re-\nsults by Franc ¸ois et al. [8], veriﬁed by our empirical mea-\nsurements, both µx0andµx2asymptotically behave as√\nd.\nHo\nwever, convergence does not occur at the same speed ,\ngiving rise to the differences shown in Fig. 3 which ulti-\nmately result in the emergence of hubs. Similar arguments\nhold when, for example, point x2is taken to be two stan-\ndard deviations farther from the center than x0, explaining\nthe formation of “orphans.”\n0 20 40 60 80 1000.20.30.40.50.60.70.80.91\ndDifference between means\nFigure 3 . Difference between means of distance distribu-\nt\nions to points at analogous positions wrt the data center,\nfor iid Gaussian random data.\n3.3 Concentration in Real Audio Data\nThe preceding discussion suggests that concentration can\nhelp explain the generation of hubs and “orphans” for au-\ndio music data and spectral similarity measures. However,\nthe aforementioned conclusions were drawn with respect\nto distances between single points, whereas spectral sim-\nilarities are computed between Gaussian Mixture Models\n(GMMs). Moreover, in this case of spectral similarity, we\ncan only consider pairwise distances and not a point of\nreference like the center. Therefore, to examine the con-\ncentration of spectral similarity, we perform the following\nmeasurement. We vary the dimensionality of the feature\nspace (number of MFCCs). For each examined dimension-\nality, we deﬁne for each song in the examined collection its\nneighbor-range by computing the difference between the\n1Roughly speaking, for every dpointx0has the same “probability” of\noccurrence with regards to the distance from the data distribution center,\nand the same can be said for x2.distances to its farthest and nearest neighbor. To charac-\nterize the distribution of neighbor-range for each dimen-\nsionality, as explained before, we follow the approach of\nrelated work [8] and compute the ratio between the stan-\ndard deviation σrange and the mean µrange of the neighbor-\nranges of all songs.\nFigure 4 illustrates this ratio as a function of dimension-\nality, for the two examined spectral similarity measures.\nThe fact that the examined ratio reduces with increasing\ndimensionality indicates that the neighbor-range is narrow-\ning as dimensionality increases, making it more difﬁcult to\nseparate the closest from the farthest neighbor. Asymp-\ntotically, as dimensionality tends to inﬁnity, the examined\nratio is expected to become equal to zero, denoting that the\nclosest and the farthest neighbor of any song will tend to\ncoincide. This means that asymptotically all points tend\nto become equidistant. However, for the high but ﬁnite di-\nmensionality values used in MIR applications, the standard\ndeviation in the examined ratio will be small but nonzero,\ncausing an analogous ampliﬁcation of “centrality” to that\ndescribed above.\n0 50 100 1500.20.250.30.350.40.450.5\ndσrange / µrangeGMM−MC\n  \nFigure 4 . Ratio of standard deviation and mean value of\nn\neighbor-range as a function of dimensionality.\n4. DIMENSIONALITY: ROLE & IMPACT ON MIR\nThe proposed framework allows for explaining the emer-\ngence of hubs and “orphans” principally as a consequence\nof high dimensionality of the feature space, in relation with\nthe concentration it incurs. In this section we will verify\nwith the examined real audio music data that the high di-\nmensionality of the feature space creates the hubs and “or-\nphans” according to the mechanism described in the previ-\nous section. Additionally, we examine the resulting impact\nof high dimensionality on MIR speciﬁc applications.\n4.1 Verifying the Role of Dimensionality\nLetNk(x)denote the number of k-occurrences of each\nsongxin a collection, i.e., the number of times xoccurs\namong the knearest neighbors of other songs. Following\nthe approach in [16], we express the asymmetry of Nk(i.e.,\nthe skewness) using the standardized third moment:\nSNk= E(Nk−µNk)3/σ3\nNk,\nwhere µNkandσNkare the mean and standard deviation\nofNk.2For the examined real audio data, and for the two\n2AnSNkvalue of 0 signiﬁes that the distribution of Nkis symmetri-\ncal, positive (negative) values indicate skewness to the right (left).\n270\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)similarity measures GMM-MC and EMD-KL, Figures 5\nan\nd 6, respectively, depict skewness as a function of di-\nmensionality (three different values of kare examined in\neach case). As dimensionality increases, the increase of\nskewness, for all values of k, indicates that the distribution\nofNkbecomes considerably skewed to the right, resulting\nin the emergence of hubs , i.e., points which appear in many\nmorek-nearest neighbor lists than other points.\n5 10 15 20 25 30−0.200.20.40.60.811.21.41.61.82\ndSN\nkGMM−MC\n  \nk\n = 1\nk\n = 10\nk\n = 20\nFigure 5 . Skewness of Nka s a function of dimensionality\nfor the GMM-MC measure.\n5 10 15 20 25 30−0.200.20.40.60.811.21.41.61.82\ndSN\nkEMD−KL\n  \nk\n = 1\nk\n = 10\nk\n = 20\nFigure 6 . Skewness of Nka s a function of dimensionality\nfor the EMD-KL measure.\nRegarding “orphans,” Figure 7 depicts, for both exam-\nined measures, the number of songs with Nkequal to zero\nas a function of dimensionality ( kwas set to 10). As ex-\npected, with increasing dimensionality, the number of such\nsongs increases, demonstrating the relation of “orphans”\nwith dimensionality.\nAll the above results show that dimensionality is the\nfundamental reason for the emergence of hubs and “or-\nphans.” In the next section we examine the relevance of\nthis result to the objectives of MIR.\n4.2 Impact on MIR\nIn order to study how our main conclusion, concerning the\nrole of high dimensionality, affects MIR applications, we\nrely on external labels, such as the genre, to characterize\nthe similarity between songs. This assumption is being\nwidely applied in MIR (e.g., in MIREX contests), since\nresults in related work [13] indicate that this assumption\nis reasonable.5 10 15 20 25 30010203040506070\ndNumber of songs with N10 = 0\n  \nGMM−MC\nEMD−KL\nFigure 7 . Number of songs with Nke qual to 0 ( k= 10 ).\nIn this context, we measure the impact of hubs by exam-\nining the number of times they mismatch in label with the\nsongs to which they are close neighbors. In this sense, we\ncan measure how much of the total error (in terms of label\nmismatches) can be attributed to hubs. Figure 8 depicts,\nas a function of dimensionality, the fraction of total error\ndue to the 10% of the strongest hubs, i.e., songs with the\nlargest Nkvalues ( kwas set to 1). For low dimensionality\nvalues, when according to previous measurements hubs are\nnot strong, their responsibility for the total error is much\nsmaller compared to the case of larger dimensionality. It is\nworth to note that, for the largest examined dimensionality\nvalue, the strongest hubs are responsible for about 90% of\nthe total error.\n5 10 15 20 25 300.40.50.60.70.80.91\ndFraction of total error\n  \nGMM−MC\nEMD−KL\nFigure 8 . F raction of total error due to 10% of the\nstrongest hubs ( k= 1).\n5. CONCLUSIONS\nIn this paper we propose a conceptual framework that re-\nlates the known shortcomings of spectral similarity mea-\nsures for music data: the existence of hubs, “orphans” and\nthe distance concentration phenomenon, with the high di-\nmensionality of underlying data space. The framework\npresents a unifying view of the three examined problems\nof music similarity measures, offering an explanation of\ntheir fundamental origins, which will hopefully help MIR\nresearchers develop robust methods for their remedy.\nThe issue of high dimensionality is signiﬁcant for spec-\ntral similarity measures because small dimensionality usu-\nally leads to poor discriminating capability, while high di-\nmensionality produces the described negative effects per-\ntaining to hubs, “orphans,” and distance concentration.\n271\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)In future work we will take into consideration more the-\nor\netical aspects of the hub/“orphan” properties and similar-\nity concentration, providing a sounder theoretical backing\nfor the described relationships. Furthermore, it would be\ninteresting to examine why some approaches [9, 17, 19]\ntend to produce less hubs, by relating to the intrinsic di-\nmensionality of the space they produce (i.e., to consider\nthe proposed framework in explaining these approaches\nand understanding their reported properties). We also plan\nto conduct an extended experimental evaluation involving\nmore similarity measures and data collections, giving more\nprecise quantiﬁcation of relationships between high dimen-\nsionality and the aforementioned properties of (spectral)\nsimilarity measures. Finally, we will develop novel miti-\ngation methods for the problems induced by the existence\nof excessive hubs and “orphans”. In particular, we will ex-\namine machine learning methods that apply correction to\nspectral similarity measures in order to take into account\nthat retrieval error may not be distributed uniformly (as\nexempliﬁed in Section 4.2), thus focusing on hubs as the\nmain source of error, and in order to enable “orphans” to\nparticipate more prominently as nearest neighbors.\nAcknowledgments. Alexandros Nanopoulos gratefully ac-\nknowledges the partial co-funding of his work through the\nEC FP7 project MyMedia (www.mymediaproject.org/) un-\nder the grant agreement no. 215006.\n6. REFERENCES\n[1] C. C. Aggarwal, A. Hinneburg, and D. A. Keim. On\nthe surprising behavior of distance metrics in high di-\nmensional spaces. In Proc. 8th Int. Conf. on Database\nTheory (ICDT) , pages 420–434, 2001.\n[2] J.-J. Aucouturier and F. Pachet. Improving timbre simi-\nlarity: How high’s the sky? Journal of Negative Results\nin Speech and Audio Sciences , 1(1), 2004.\n[3] J.-J. Aucouturier and F. Pachet. A scale-free distribu-\ntion of false positives for a large class of audio sim-\nilarity measures. Pattern Recognition , 41(1):272–284,\n2008.\n[4] A. Berenzweig. Anchors and Hubs in Audio-based Mu-\nsic Similarity . PhD thesis, Columbia University, New\nYork, USA, 2007.\n[5] K. S. Beyer, J. Goldstein, R. Ramakrishnan, and\nU. Shaft. When is “nearest neighbor” meaningful? In\nProc. 7th Int. Conf. on Database Theory (ICDT) , pages\n217–235, 1999.\n[6] M. Casey, C. Rhodes, and M. Slaney. Analysis of min-\nimum distances in high-dimensional musical spaces.\nIEEE Transactions on Audio, Speech and Language\nProcessing , 16(5):1015–1028, 2008.\n[7] P. Demartines. Analyse de Donn ´ees par R ´eseaux de\nNeurones Auto-Organis ´es. PhD thesis, Institut Nat’l\nPolytechnique de Grenoble, Grenoble, France, 1994.[8] D. Franc ¸ois, V . Wertz, and M. Verleysen. The con-\ncentration of fractional distances. IEEE Transactions\non Knowledge and Data Engineering , 19(7):873–886,\n2007.\n[9] M. Hoffman, D. M. Blei, and P. R. Cook. Content-\nbased musical similarity computation using the hier-\narchical dirichlet process. In Proc. 9th Int. Conf. on\nMusic Information Retrieval (ISMIR) , pages 349–354,\n2008.\n[10] B. Logan and A. Salomon. A music similarity function\nbased on signal analysis. In Proc. IEEE Int. Conf. on\nMultimedia and Expo (ICME) , pages 952–955, 2001.\n[11] E. Pampalk. Speeding up music similarity. In 1st\nAnnual Music Information Retrieval Evaluation eX-\nchange (MIREX) , 2005.\n[12] E. Pampalk. Computational Models of Music Similar-\nity and their Application in Music Information Re-\ntrieval . PhD thesis, Vienna University of Technology,\nVienna, Austria, 2006.\n[13] E. Pampalk, A. Flexer, and G. Widmer. Improvements\nof audio-based music similarity and genre classiﬁca-\ntion. In Proc. 6th Int. Conf. on Music Information Re-\ntrieval (ISMIR) , pages 628–633, 2005.\n[14] T. Pohle. Post processing music similarity computa-\ntions. In 2nd Annual Music Information Retrieval Eval-\nuation eXchange (MIREX) , 2006.\n[15] T. Pohle, M. Schedl, P. Knees, and G. Widmer. Au-\ntomatically adapting the structure of audio similarity\nspaces. In Proc. 1st Workshop on Learning the Seman-\ntics of Audio Signals (LSAS) , pages 76–86, 2006.\n[16] M. Radovanovi´ c, A. Nanopoulos, and M. Ivanovi´ c.\nNearest neighbors in high-dimensional data: The emer-\ngence and inﬂuence of hubs. In Proc. 26th Int. Conf. on\nMachine Learning (IMCL) , pages 865–872, 2009.\n[17] K. Seyerlehner, G. Widmer, and P. Knees. Frame level\naudio similarity - a codebook approach. In Proc. 11th\nInt. Conf. on Digital Audio Effects (DAFx) , pages 349–\n356, 2008.\n[18] Z. Wang, W. Dong, W. Josephson, Q. Lv, M. Charikar,\nand K. Li. Sizing sketches: A rank-based analysis\nfor similarity search. In Proc. ACM SIGMETRICS Int.\nConf. on Measurement and Modeling of Computer Sys-\ntems, pages 157–168, 2007.\n[19] K. West, S. Cox, and P. Lamere. Incorporating\nmachine-learning into music similarity estimation. In\nProc. 1st ACM Workshop on Audio and Music Com-\nputing Multimedia , pages 89–96, 2006.\n272\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "Locating Tune Changes and Providing a Semantic Labelling of Sets of Irish Traditional Tunes.",
        "author": [
            "Cillian Kelly",
            "Mikel Gainza",
            "David Dorran",
            "Eugene Coyle"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1418303",
        "url": "https://doi.org/10.5281/zenodo.1418303",
        "ee": "https://zenodo.org/records/1418303/files/KellyGDC10.pdf",
        "abstract": "An approach is presented which provides the tune change loca- tions within a set of Irish Traditional tunes. Also provided are semantic labels for each part of each tune within the set. A set in Irish Traditional music is a number of individual tunes played segue. Each of the tunes in the set are made up of structural seg- ments called parts. Musical variation is a prominent characteris- tic of this genre. However, a certain set of notes known as ‘set accented tones’ are considered impervious to musical variation. Chroma information is extracted at ‘set accented tone’ locations within the music. The resulting chroma vectors are grouped to represent the parts of the music. The parts are then compared with one another to form a part similarity matrix. Unit kernels which represent the possible structures of an Irish Traditional tune are matched with the part similarity matrix to determine the tune change locations and semantic part labels.",
        "zenodo_id": 1418303,
        "dblp_key": "conf/ismir/KellyGDC10",
        "keywords": [
            "tune change locations",
            "semantic labels",
            "Irish Traditional tunes",
            "semantic labels",
            "structural segments",
            "set accented tones",
            "chroma information",
            "unit kernels",
            "part similarity matrix",
            "tune change locations"
        ],
        "content": "LOCATING TUNE CHANGES AND PROVIDING A SEMANTIC LABELLING OF\nSETS OF IRISH TRADITIONAL TUNES\nCillian Kelly, Mikel Gainza, David Dorran and Eugene Coyle\nAudio Research Group\nDIT Kevin St.\nDublin 8\nIreland\ncillian.kelly@dit.ie\nABSTRACT\nAn approach is presented which provides the tune change loca-\ntions within a set of Irish Traditional tunes. Also provided are\nsemantic labels for each part of each tune within the set. A set\nin Irish Traditional music is a number of individual tunes played\nsegue. Each of the tunes in the set are made up of structural seg-\nments called parts. Musical variation is a prominent characteris-\ntic of this genre. However, a certain set of notes known as ‘set\naccented tones’ are considered impervious to musical variation.\nChroma information is extracted at ‘set accented tone’ locations\nwithin the music. The resulting chroma vectors are grouped to\nrepresent the parts of the music. The parts are then compared\nwith one another to form a part similarity matrix. Unit kernels\nwhich represent the possible structures of an Irish Traditional\ntune are matched with the part similarity matrix to determine the\ntune change locations and semantic part labels.\n1. INTRODUCTION\nThe approach presented here is speciﬁc to Irish Traditional Mu-\nsic. This music type consists of structural segments called ‘tunes’\nwhich are concatenated to form ‘sets’. The tunes are themselves\nmade up of shorter structural segments called ‘parts’. The struc-\nture of Irish Traditional Music is illustrated in Figure 1. Within\nthis music type performers are encouraged to introduce musical\nvariation. Parts which are notated as equivalent are aurally dif-\nferent due to this musical variation. The approach presented in\nthis paper has two aims. The ﬁrst is to determine the locations\nwhere tune changes occur within ‘sets’ of Irish Traditional tunes.\nThe second aim is to assign a semantic label to each of the parts\nof each of the tunes within the music.\nThe information provided by a structural segmentation can be\nused for audio browsing. Instead of browsing through the mu-\nsic manually, using the structural segmentation information the\nuser can browse directly to the part of interest within the music.\nLooping is a further application of the information provided by\nstructural segmentation. Once a user has browsed to the required\npart within the music, the part can be looped to facilitate repeated\nplayback of a certain segment. The structural segmentation in-\nformation can provide exact loop points so that the start and end\nPermission to make digital or hard copies of all or part of this work for personal\nor classroom use is granted without fee provided that copies are not made or\ndistributed for proﬁt or commercial advantage and that copies bear this notice\nand the full citation on the ﬁrst page.\nc\r2010 International Society for Music Information Retrieval.\nFigure 1. A representation of the structure present within a piece\nof Irish Traditional music. There are two distinct hierarchical\nlevels of segmentation. The piece of music consists of segments\ncalled tunes, and each tune consists of further segments called\nparts.\nof the selected loop will align rhythmically. This promotes aural\nlearning which is common practice for Irish Traditional musi-\ncians. Structural segmentation information can also be used to\ncreate an audio thumbnail. For popular music, an audio thumb-\nnail is the most repeated segment, often considered to be the\nchorus. For Irish Traditional Music however there is no chorus\nand no segment which repeats more often than others. Using the\nstructural segmentation information a reduced form of the music\ncan be created by discarding repeated sections.\nDetailed in this paper is an approach which aims to extract\ntune change locations within sets of Irish Traditional music. The\napproach also provides a semantic labelling for each part of the\ntunes within each set. A set of notes known as ‘set accented\ntones’ [17] are utilised which are considered impervious to any\nmusical variation. These notes are extracted and are used to rep-\nresent the music. Using the harmonic information of these notes,\nsets are segmented into separate tunes, semantic labels are pro-\nvided for each part of each resulting tune using a kernel matching\ntechnique.\nThe remainder of this paper is divided as follows: Section 2\ndetails previous relevant approaches toward the structural seg-\nmentation of music. In Section 3 an overview of the structure of\nIrish Traditional Music is provided. The proposed approach to-\nward locating tune changes and providing a semantic labelling of\nsets of Irish Traditional tunes is detailed in Section 4. Section 5\nprovides the results of testing the approach on a database of Irish\nTraditional tunes. The presented approach is compared directly\nwith a previous approach which also attempts to locate the tune\n129\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)change locations within sets of Irish Traditional tunes. Finally,\nin Section 6 conclusions based on the results are provided.\n2. LITERATURE REVIEW\nApproaches which provide a structural segmentation of music\naim to search for similarities within the audio signal. The signal\nis divided into audio frames and certain audio features are ex-\ntracted for each of the resulting audio frames. Low level audio\nfeatures such as zero crossings or spectral centroid are extracted\nand are combined to produce an aggregated description of the\naudio signal such as in [8, 11, 16, 21]. Mel Frequency Cepstral\nCo-efﬁcients (MFCCs) are a further example of a low level audio\nfeature which is commonly used for approaches which attempt\nto extract structure from music. In [13] MFCCs are extracted for\neach audio frame and the resulting frames are clustered in order\nto locate the repeated phrase or chorus within a ‘rock’ or ‘pop’\nsong. Certain heuristics are then used to choose a key phrase\nwhich corresponds to the chorus. These low level audio features\nare indicators of the timbre and loudness of the music. For Irish\nTraditional music, the timbre and loudness often remain constant\nthroughout an entire musical piece therefore using these features\nis not suitable for this musical genre.\nThe audio features used for structural segmentation approaches\nmay also be of a higher level of abstraction, such as pitch [2,14]\nor chroma [1, 7] (See Section 4.3). Sections of the audio which\nshare similar audio feature values are grouped together. The re-\nsulting groups of frames indicate the overall structure of the mu-\nsic. Extracting pitch or chroma values for every audio frame of\nan Irish Traditional music piece would include any musical vari-\nation present within the piece. This increases the difﬁculty of\ndetermining which structural segments are similar.\nIn [5], an approach is presented which segments audio us-\ning a measure of audio novelty. A Short Time Fourier Trans-\nform (STFT ) is applied to the signal and each resulting frame\nis compared with every other frame to create an audio similar-\nity matrix. The method of comparison used in [5] is the co-\nsine distance measure. Points of signiﬁcant musical change are\ndetermined by using kernel correlation. In [5], a checkerboard\nunit kernel is correlated along the diagonal of the resulting self-\nsimilarity matrix. Locations which result in a high correlation\nvalue are considered to be points of signiﬁcant musical change\nwhich themselves are considered possible structural segmenta-\ntion boundaries.\nStructural segmentation of Irish Traditional music has attracted\nthe attention of researchers. There have been a number of ap-\nproaches which have attempted to structurally segment this mu-\nsic type [3, 4, 9, 10].\nIn [9] an approach is presented which aims to segment Irish\nTraditional tunes into their constituent parts and to provide a se-\nmantic labelling for the resulting parts. The ‘set accented tones’\nwhich are considered impervious to variation are located within\nthe music using a beat tracker. Pitch values are determined at\nthese speciﬁc locations using a pitch detector. This results in a\nselective pitch contour. Melodic patterns are searched for amongst\nthis pitch contour to determine the overall structure of the music.\nThis approach was tested on a database of monophonic pieces\nof Irish Traditional music. The approach presented in [9] is ex-\ntended further in [10] where chroma is calculated at ‘set accented\ntone’ locations rather than single pitch values. Following this,\nthe chroma vectors are grouped according to heuristics speciﬁc\nto Irish Traditional Music. The resulting groups of chroma vec-tors correspond to the structural segments of the music and are\ncompared using three different distance measures to determine\nwhich of the segments are similar. Extracting chroma rather than\nsingle pitch values at ‘set accented tone’ locations allows the ap-\nproach in [10] to be applied to polyphonic music rather than only\nmonophonic music as in [9].\nIn [4] an approach is presented which aims to provide the lo-\ncations of tune changes (see Section 3) within a set of Irish Tra-\nditional tunes. This approach relies on a pre-existing database\nof transcribed Irish Traditional tunes. The music is transcribed\nusing a pitch detection algorithm and is converted into a format\nconsistent with the ABC music notation language [20]. Sections\nof the audio are compared with tunes contained within a database\nof ABC notated tunes using the edit distance algorithm [12].\nOnce the identity of the tune contained within the section has\nbeen determined the version of the tune from the database is\ncompared with every possible section of the transcribed music\nagain using the edit distance. This allows the algorithm to de-\ntermine where the end of the current tune is located within the\nmusic. This process repeats for subsequent tunes within the set\nuntil all tunes have been processed.\nThe approaches presented in both [9] and [10] were tested on\na database of pieces of Irish Traditional music containing one\ntune only. There is no attempt made to structurally segment sets\nof Irish Traditional tunes. The approach presented in [4] specif-\nically addresses the problem of segmenting a set of Irish Tradi-\ntional tunes by providing the locations of tune changes within the\nmusic. However in [4] the requirement of a pre-existing database\nof Irish Traditional tunes is a notable limitation. If a tune within\na set is not contained within the pre-existing database of tunes,\nthe segmentation of that set will not be successful. The approach\npresented in Section 4 attempts to provide a semantic labelling\nof a piece of Irish Traditional music as in [9]. However, un-\nlike [9] the approach presented in Section 4 aims to provide this\nsemantic labelling for sets of tunes rather than single tunes. Sec-\ntion 4 also details a method to locate each tune change within\na set of Irish Traditional tunes as in [4]. A method using unit\nkernels is detailed which overcomes the requirement in [4] of a\npre-existing database of transcribed tunes.\n3. IRISH TRADITIONAL MUSIC\nIrish Traditional Music is comprised of short musical pieces called\ntunes. Each tune is made up of two or more ’parts’ which are\nnotated using upper case letters as can be seen in Figure 1. Al-\nthough each tune is quite short (a typical two part tune consists\nof sixteen bars), the parts are repeated to extend the tune and the\ntune itself can also be repeated in its entirety.\nIn both studio recordings and live performances of this mu-\nsic type often two or more tunes are concatenated into ‘sets’ to\nextend the music even further as shown in Figure 1. For exam-\nple, a piece of music consisting of a two part tune followed by a\nthree part tune may be played with a musical structure of AAB-\nBAABB/AABBCCAABBCC.\nWhile two renditions of the same ’A’ part may be notated\nidentically, they are rarely performed identically. This is due to\nthe large presence of musical variation inherent with this music\ntype. Embellishments introduced by a musician will render two\nidentical parts as being aurally different.\nDespite the considerable presence of musical variation within\nthis genre, for each tune there are a certain set of notes which\nare left unchanged by the musician. These notes are called the\n130\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Figure 2. A block diagram of the proposed approach.\n‘set accented tones’ of the tune [9, 17]. In order to avoid the in-\nﬂuence which musical variation has on determining which parts\nare equivalent, the ‘set accented tones’ are used to characterise\nthe music for this approach.\n4. PROPOSED APPROACH\n4.1 Overview\nAn approach toward locating tune changes and providing a se-\nmantic labelling of sets of Irish Traditional tunes is detailed in\nthis section. This approach is illustrated in the block diagram\nin Figure 2. The locations of the ‘set accented tones’ are deter-\nmined using a beat tracker. Following this, chroma vectors are\ncalculated at the resulting ‘set accented tone’ locations and are\ncompared to create a part similarity matrix. Kernel matching is\nperformed on the resulting matrix using unit kernels which rep-\nresent the various musical structures present within this genre.\nThe kernel matching technique presented here provides a solu-\ntion to both determining the location of tune changes within a set\nand also to assigning a semantic label to each resulting part.\n4.2 Beat Tracking and Set Accented Tone Identiﬁcation\nTo extract chroma at ‘set accented tone’ locations within the mu-\nsic, the locations of the ‘set accented tones’ must be deﬁned.\nWithin Irish Traditional Music these notes are considered to be\nthe ﬁrst note of each beat. Therefore a beat tracker is employed\nto determine the location of each beat within the music. The beat\ntracker used for this approach is detailed in [6]. The beat tracker\nprovides the location of each beat of the music along with an\nonset detection function which provides the locations of each\nnote within the music. To encapsulate each ‘set accented tone’\na window is created extending from the start of each beat to the\nnext detected onset as illustrated in Figure 3. This maximises\nthe available harmonic information when determining chroma\nFigure 3. An onset detection function of one bar of Irish Tra-\nditional music. Each ‘set accented tone’ is located between the\nstart of the beat and the next detected onset. For each ‘set ac-\ncented tone’ a window is created between these two points of\nthe onset detection function. Chroma is calculated for each re-\nsulting window.\nvalues at each ‘set accented tone’ location. Following the cre-\nation of each ‘set accented tone’ window, chroma information is\nextracted at each of these locations.\n4.3 Chroma Calculation\nTo create the part similarity matrix detailed in Section 4.6 chroma\nmust be calculated at each ‘set accented tone’ location. In [10]\nresults showed that extracting chroma at ‘set accented tone’ lo-\ncations provide better structural segmentation results than ex-\ntracting a single pitch value. As such, for this approach chroma\nwill also be used to represent each ‘set accented tone’. Chroma\nis a spectral representation of music in which frequencies are\nmapped onto a set of 12 chroma values which correspond to the\n12 notes of the equal tempered scale [18].\nTo calculate the chroma a Harmonic Pitch Class Proﬁle (HPCP)\napproach is employed [19]. For each ‘set accented tone’ win-\ndow (the section denoted as ‘SAT’ in Figure 3), a Short Time\nFourier Transform is applied with a frame length of 2048 sam-\nples. The local maxima contained within each of the resulting\nSTFT frames are identiﬁed using a peak picking algorithm. Fol-\nlowing this, the magnitudes of each frequency at each result-\ning peak location are added to the appropriate chroma bin ac-\ncording to the note of the musical scale to which the frequency\nmost closely corresponds. Only frequencies between 130Hz and\n3140Hz are considered for this approach as 130Hz is the fre-\nquency of the lowest note on a banjo which is the lowest note\nlikely to be present within an Irish Traditional tune and 3140Hz\nis the third harmonic of the highest note of a standard tin whistle,\nthe highest note likely to be present within an Irish Traditional\ntune. This gives an appropriately rich description of the fre-\nquency content of a given audio frame. This results in a chroma\nvector of twelve elements each containing the amount of each\nnote which was present in the given ‘set accented tone’ window.\n4.4 Part Length Calculation\nFollowing chroma calculation at each ‘set accented tone’ loca-\ntion, it is necessary to determine how many ‘set accented tones’\nper part there are in the piece of music. This is required in order\nto determine the correct groups of chroma vectors to use when\n131\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)creating the part similarity matrix in Section 4.5. According to\nthe Irish Traditional Music heuristics detailed in [9] there can\nonly be 12, 16 or 24 ‘set accented tones’ per part (SATpp) in an\nIrish Traditional tune. Consequently, each of these three con-\nditions are tested and a conﬁdence score is calculated for each\npossible SATpp value. Following this, the chroma vectors repre-\nsenting the ‘set accented tones’ are divided into groups accord-\ning to the SATpp value currently being tested. For example, if\nthe current SATpp value is equal to 12, the chroma vectors are\ndivided into groups of 12. The resulting groups of chroma vec-\ntors now represent potential parts of a tune. Following this, each\npotential part is compared with every other potential part and a\nconﬁdence score is calculated based on these part comparisons.\nTheSATpp value which results in the highest conﬁdence score is\nthe value used when creating the part similarity matrix in Section\n4.5.\nIndividual chroma vectors are compared using the Euclidean\nDistance formula given in Equation (1).\nD(v1; v2) =p\n(12X\ni=1(v1(i)\u0000v2(i))2) (1)\nwhere v1andv2are the two chroma vectors being compared.\nEntire parts are compared with one another using Equation\n(2). The resulting value Sis low if the two parts being compared\nare similar, therefore Sis a measure of the dis-similarity between\ntwo parts.\nS=PN\u00001\nn=0D(v1(n); v 2(n))\nN(2)\nwhere Nis equal to the SATpp value currently being tested.\nFinally, a conﬁdence value Cfor the SATpp value being tested\nis calculated according to Equation (3).\nC= 1=PM\u00001\nm=0SSATpp\nM(3)\nwhere Mis equal to the total number of part comparisons. The\nSATpp value which results in the greatest conﬁdence value Cis\nthe value used to create the part similarity matrix in Section 4.5.\n4.5 Part Similarity Matrix\nAs detailed in Section 4.4, once the number of ‘set accented\ntones’ per part has been determined, the part similarity matrix\nis created according to this SATpp value. The values of S(cal-\nculated in Section 4.4 using Equation (2)) associated with this\nparticular SATpp value are used to create the part similarity ma-\ntrix. These values of Sindicate the similarity of each part of\nlength SATpp with every other part of length SATpp within the\nmusic.\nPositioning these values into a matrix results in a part similar-\nity matrix of size PbyPwhere Pis equal to the total number of\nparts within the music. An example of a part similarity matrix is\nshown in Figure 5. The part similarity matrix is used along with\nunit kernels to determine the structure of the music as described\nin Section 4.6.\n4.6 Kernel Matching\nThe following section details how unit kernels are matched with\nthe part similarity matrix created in Section 4.5. Firstly, the ker-\nFigure 4. A unit kernel representing the structure of two tunes.\nThe ﬁrst tune represented has a structure of AABBCCAABBCC\nthe second tune represented has a structure of AABBAABB. Black\nrepresents similar parts and white represents dis-similar parts.\nnels that are used for matching are described, along with justiﬁ-\ncations for using these particular kernels. The process of how the\nunit kernels are matched with the part similarity matrix is then\ndetailed.\nKernel matching relies on the availability of pre-existing unit\nkernels which each represent a speciﬁc musical structure. A unit\nkernel is a matrix consisting of ones and zeros which represent\nthe pattern of a musical structure. A total of 24 unit kernels\nwhich represent a single tune are used here. Kernels representing\nthe structure of more than a single tune are created by combining\nthe kernels which represent a single tune. An example of a two-\ntune kernel is shown in Figure 4. There are 24 kernels used to\nrepresent the possible structures of one tune. As such combining\neach one-tune kernel with every other one-tune kernel results\nin 576 possible combinations for a two-tune kernel. The unit\nkernels represent the musical structures which are most common\nwithin Irish Traditional Music.\nThe kernels that are used limit the number of possible parts\nper tune to four. According to [15], tunes containing two, three\nand four parts make up 97% of the volume of tunes within this\ngenre. Both one-tune kernels andtwo-tune kernels are utilised\nto give a total number of kernels of 600. The unit kernels are\ncorrelated with sections of the part similarity matrix as illustrated\nin Figure 5. The kernel which yields the highest matching value\nis the kernel which represents the most likely musical structure\npresent within the given section of the part similarity matrix. The\nfollowing steps describe the kernel matching technique:\n1. At location (i,i)of the part similarity matrix, each K x K\nunit kernel is matched with a K x K section of the part\nsimilarity matrix using inner matrix multiplication. For\nthe ﬁrst iteration only, i = 1.\n2. The kernel which results in the highest match value is\ndeemed to represent the structure of that section of the part\nsimilarity matrix.\n3. The value iis updated to be (i + K ) where Kis equal to\nthe length of the kernel in step 2.\n4. Steps 1-3 are repeated until the entire part similarity ma-\ntrix has been processed.\nThe outcome of this kernel matching as outlined in Figure 2\nis the location of each tune change within the set of Irish Tradi-\ntional tunes along with a semantic label for each resulting part.\n132\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Figure 5. The section of a part similarity matrix with which a\nunit kernel is correlated. Each cell of the part similarity matrix\ncorresponds to a part of an Irish Traditional tune. This section\nof the part similarity matrix is correlated with each unit kernel.\nThe unit kernel which results in the highest matching value cor-\nresponds to the structure of this section of the part similarity\nmatrix.\nThe semantic part labels are represented by the concatenation of\nthe descriptions of the unit kernels which were matched to each\ntune. The locations of each tune change are determined by the\nbeat locations of the start of each successfully matched unit ker-\nnel. Results for the approach which has been presented here are\ndetailed in Section 5.\n5. RESULTS\nThe evaluation of this approach was carried out on a hand an-\nnotated database of 30 sets of Irish Traditional music. These 30\nmusical pieces consist of 75 separate Irish traditional tunes with\n34 tune changes and a total of 589 parts. The results of detect-\ning the location of tune changes and determining labels for each\nstructural segment were calculated separately. A tune change lo-\ncation was considered correct if the automatically detected tune\nchange locations were within 1 second of the hand annotated\ntune change locations. Table 1 details the results of labelling the\nparts of the music and also details the results of locating tune\nchanges within the music. In Table 1, Nis equal to the number\nof annotations, GPis equal to Good Positives, FPis equal to\nFalse Positives and FNis equal to False Negatives. Results were\ncalculated using three different measures, precision, recall and\naccuracy. These three measures are deﬁned by Equations (4),\n(5) and (6).\nPrecision =GP\nGP +FP(4)\nRecall =GP\nGP +FN(5)\nAccuracy =N\u0000FP\u0000FN\nN(6)\nThe results show that the approach can label the parts con-\ntained in a piece of Irish Traditional Music with an accuracy of\n86% along with a precision value of 90% and a recall value ofN GP FP FN Prec Rec Acc\nPart\nLabels589 479 51 31 90% 94% 86%\nTune\nChange\nLocations34 24 7 6 77% 80% 62%\nTable 1. Results of the part labelling and locating tune changes\nproduced by the presented approach.\nTolerance Prec Rec\n1 second 69.7% 64.79%\n2 seconds 87% 81%\nTable 2. Results of the approach presented in [3] and [4] toward\nlocating tune changes within a set of Irish Traditional tunes.\n94%. Additionally, this approach can correctly identify the loca-\ntion of tune changes within a set of Irish Traditional tunes with\nan accuracy of 62% along with a precision value of 77% and a\nrecall value of 80%.\nThe accuracy value is higher for labelling parts than for de-\ntecting tune changes. This is because even when a tune change\nis not accurately detected, the algorithm may still correctly iden-\ntify subsequent parts contained within the music. This is due\nto many kernels having common part locations. The high recall\nvalues should be noted, this indicates that the algorithm detects\nmost tune changes present in the music.\nAn approach is presented in [4] which also attempts to cal-\nculate the tune change locations within sets of Irish Traditional\nmusic. In [4] a tune change location is considered to be correct\nif the automatically generated tune change locations are within 2\nseconds of the equivalent hand annotated tune change locations.\nThe approach presented in [4] is detailed further in [3] where re-\nsults are also provided for a tolerance of 1 second. The results\nof detecting tune change locations as detailed in [3] and [4] can\nbe seen in Table 2 for a tolerance window of both 1 second and\n2 seconds. The results detailed in Table 1 were obtained from\ntesting on the same database used in [3].\nWhen detecting tune changes within a set the approach de-\ntailed in [3] claims a precision value of 69.7% and a recall value\nof 64.79% for a tolerance of 1 second. The approach presented\nin Section 4 has improved these values by 7.3% and 15.21% re-\nspectively and also does not require the database of Irish Tradi-\ntional tunes which is utilised in [3]. In [3], to correctly detect the\ntune changes within a set of Irish Traditional tunes, each tune in\nthe set must also be in a pre-existing database of tunes.\n6. CONCLUSIONS\nThis paper presented an approach toward locating tune changes\nand providing a semantic labelling of sets of Irish Traditional\ntunes. This music type consists of sets of tunes, the tunes them-\nselves are made up of parts. This approach utilised certain notes\nwithin the music which remain constant despite the presence of\nmusical variation. Chroma was extracted at these speciﬁc note\nlocations and was compared to create a part similarity matrix.\nUnit kernels representing common structures present within Irish\nTraditional Music were then matched with sections of the part\nsimilarity matrix. The unit kernel which resulted in the high-\n133\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)est match value corresponds to the structure of the music at the\ngiven location within the part similarity matrix.\nUsing chroma at the ‘set accented tone’ locations within the\nmusic signiﬁcantly reduces the amount of data required to pro-\nduce a structural segmentation. This reduced representation of\nthe music also ﬁlters out musical variation which can affect the\nprocess of determining which parts are equivalent. The approach\npresented here was tested on a database of 30 sets of Irish Tra-\nditional tunes. The results of the approach presented here were\ncompared with a similar approach toward detecting tune change\nlocations within a set of Irish Traditional tunes by testing on the\nsame database of Irish Traditional music. When a tolerance of\n1 second between automatically detected tune changes and hand\nannotated tune changes is used, the approach presented here per-\nforms signiﬁcantly better than a previous approach toward the\nsame goal. For this approach, increasing the tolerance window\nwill not result in an increase in performance as tune change lo-\ncation times are calculated using part locations and are not cal-\nculated on a scale which is sensitive to increments of less than\nthe length of a part.\nThe approach presented in this paper relies on correctly calcu-\nlating the amount of ‘set accented tones’ per part (SATpp). If this\nvalue is calculated incorrectly, the resulting part similarity ma-\ntrix will not accurately reﬂect the parts which are present within\nthe set of Irish Traditional tunes. Consequently, it would not\nbe possible to identify the correct tune change locations or de-\ntermine the correct semantic part labelling. This approach also\nrelies on the accuracy of the beat tracker in order to correctly\nidentify the ‘set accented tone’ locations.\nFuture work will aim to combine the approach presented here\nwith the approach presented in [4] to identify tune change loca-\ntions within a set. Firstly, the method detailed in [4] would be\nused to determine the tune change locations. If there are tunes\npresent within a set that are not present within the pre-existing\ndatabase used in [4] the tune change locations cannot be calcu-\nlated using this method. In this case, the approach presented in\nSection 4 would be used as an alternative, as there is no pre-\nexisting knowledge required of the particular tunes within the\nset for this approach.\n7. REFERENCES\n[1] Mark A. Bartsch and Gregory H. Wakeﬁeld. To catch a cho-\nrus: using chroma-based representations for audio thumb-\nnailing. In IEEE Workshop on Applications of Signal Pro-\ncessing to Audio and Acoustics, New Paltz, New York, 2001.\n[2] Roger B. Dannenberg and Ning Hu. Discovering Musical\nStructure in Audio Recordings. Lecture Notes in Computer\nScience. Springer Berlin, 2002.\n[3] Bryan Duggan. Machine Annotation of Traditional Irish\nDance Music. PhD thesis, Dublin Institute of Technology,\n2009.\n[4] Bryan Duggan, Brendan O’Shea, Mikel Gainza, and Padraig\nCunningham. Machine annotation of sets of traditional irish\ndance tunes. In International Conference on Music Informa-\ntion Retrieval, Philadelphia, PA, USA, 2008.\n[5] Jonathan Foote. Automatic audio segmentation using a mea-\nsure of audio novelty. In IEEE Intl Conf. on Multimedia and\nExpo, New York, 2000.[6] Mikel Gainza. On the use of a dynamic hybrid tempo detec-\ntion model for beat tracking. In IEEE International Confer-\nence on Multimedia and Expo, Singapore, 2010.\n[7] Masataka Goto. A chorus-section detecting method for musi-\ncal audio signals. In IEEE Conference on Acoustics, Speech,\nand Signal Processing, Hong Kong, 2003.\n[8] Min-Hong Jian, Chia Han Lin, and Arbee L.P. Chen. Per-\nceptual analysis for music segmentation. In Storage and Re-\ntrieval Methods and Applications for Multimedia, San Jose,\nCalifornia, USA, 2003.\n[9] Cillian Kelly, Mikel Gainza, David Dorran, and Eugene\nCoyle. Structural segmentation of music using set accented\ntones. In 124th Audio Engineering Society Convention, Am-\nsterdam, The Netherlands, 2008.\n[10] Cillian Kelly, Mikel Gainza, David Dorran, and Eugene\nCoyle. Structural segmentation of irish traditional music us-\ning chroma at set accented tone locations. In 127th Audio En-\ngineering Society Convention, New York, New York, U.S.A.,\n2009.\n[11] S. Lefevre, B. Maillard, and N. Vincent. A two level clas-\nsiﬁer process for audio segmentation. In 16th International\nConference on Pattern Recognition, Washington DC, USA,\n2002.\n[12] Vladimir Iosifovich Levenshtein. Binary codes capable of\ncorrecting deletions, insertions and reversals. Soviet Physics\nDoklady, 1966.\n[13] Beth Logan and Stephen Chu. Music summarization using\nkey phrases. In International Conference on Audio Speech\nand Signal Processing, Istanbul, Turkey, 2000.\n[14] Benoit Meudic. Musical pattern extraction: from reptition\nto musical structure. In Computer Music Modelling and Re-\ntrieval, Montpellier, France, 2003.\n[15] Donncha Se ´an´O’Maid ´ın.A Programmer’s Environment for\nMusic Analysis. PhD thesis, University College Cork, 1995.\n[16] Bee Suan Ong and Perfecto Herrera. Semantic segmentation\nof music audio contents. In International Computer Music\nConference, Barcelona, Spain, 2005.\n[17] M ´ıch´ael´O’S´uilleabh ´ain. Innovation and Tradition in the\nMusic of Tommie Potts. PhD thesis, Queen’s University,\n1987.\n[18] Steffen Pauws. Musical key extraction from audio. In In-\nternational Conference on Music Information Retrievals,\nBarcelona, Spain, 2004.\n[19] Joan Serra, Emilia Gomez, Perfecto Herrera, and Xavier\nSerra. Chroma binary similarity and local alignment applied\nto cover song identiﬁcation. IEEE Transactions on Audio,\nSpeech, and Language Processing, 2008.\n[20] Chris Walshaw. Abc notation - an introduction,\nhttp://abcnotation.com/. Accessed March 2010.\n[21] Yibin Zhang and Jie Zhou. Audio segmentation based on\nmulti-scale audio classiﬁcation. In IEEE International Con-\nference on Acoustics, Speech, and Signal Processing, Mon-\ntreal, Quebec, Canada, 2004.\n134\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "State of the Art Report: Music Emotion Recognition: A State of the Art Review.",
        "author": [
            "Youngmoo E. Kim",
            "Erik M. Schmidt",
            "Raymond Migneco",
            "Brandon G. Morton",
            "Patrick Richardson",
            "Jeffrey J. Scott",
            "Jacquelin A. Speck",
            "Douglas Turnbull"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.6424213",
        "url": "https://doi.org/10.5281/zenodo.6424213",
        "ee": "https://zenodo.org/records/6424213/files/D2.4_State_of_the_Art_Analysis.pdf",
        "abstract": "This deliverable presents models for the impedance of biological synapses and electrode kinetics. Section 3 describes the concept of neural stimulation and possible approaches to the problem. Section 4 focuses on electrical stimulation, the state-of-the-art and specifications for the system to be developed. Section 5 discusses the state-of-the-art of materials for electrodes. Section 6 concludes the deliverable.",
        "zenodo_id": 6424213,
        "dblp_key": "conf/ismir/KimSMMRSST10",
        "keywords": [
            "impedance",
            "biological synapses",
            "electrode kinetics",
            "neural stimulation",
            "possible approaches",
            "electrical stimulation",
            "state-of-the-art",
            "system development",
            "materials for electrodes",
            "concludes the deliverable"
        ],
        "content": "1 \n \n \nThis project has received funding from the European Union’s Horizon 2020 research and inn ovation programme under grant \nagreement No 829060.  \n \n \nSTATE OF THE ART ANALYSIS  \nDeliverable 2.4 – M12 \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nDOCUMENT HISTORY  \nVersion  Date  Status of document / Reason  of change (responsible \nperson)  \n1 29 February 2020  First draft (UAVR ) \n2   \n   \n \n \n \n  \nNeuroStim Spinal  Identifier  829060 _D02.4_State of the art  \nSubmission date  29 February 2020  \nActual date of submission  29 February 2020  \nResponsible partner  1 – UNIVERSITY OF AVEIRO  \nInternal reviewer  \nWritten by  \n \n \nContributed by   1- UNIVERSITY OF AVEIRO  \nLuis Nero Alves  (UAVR)  \nPatrícia Martins (UAVR=  \nBruno Figueiredo (‘Graphenest’)  \nGoran Bijelic (Tecnalia)  \nConfidentiality  Public  2 \n \n \nThis project has received funding from the European Union’s Horizon 2020 research and inn ovation programme under grant \nagreement No 829060.  \n \nContents  \n \n \n \n1. INTRODUCTION  ................................ ................................ ................................ ................................ ... 4 \n2. BACKGROUND CONCE PTS ................................ ................................ ................................ ....................  6 \n2.1. The Memristor  ................................ ................................ ................................ .........................  6 \n2.1.1.  Dynamic Route Maps  ................................ ................................ ................................ ...... 8 \n2.2. The Hodgki nson -Huxley Axon Model  ................................ ................................ ......................  8 \n2.2.2.  Sodium Channel Dynamics  ................................ ................................ ............................  11 \n2.3. Electrode Kinetics  ................................ ................................ ................................ ..................  12 \n2.3.1.  Cyclic voltammetry  ................................ ................................ ................................ ........  13 \n2.3.2.  Electrochemical Impedance Spectroscopy  ................................ ................................ .... 14 \n3. NEURAL STIMULATION CONCEPTUAL SCHEM ES ................................ ................................ ....................  17 \n3.1. Electrical Stimulation  ................................ ................................ ................................ .............  17 \n3.2. Optical Stimulation  ................................ ................................ ................................ ................  19 \n3.3. Chemical Stimulation ................................ ................................ ................................ .............  20 \n4. ELECTRICAL STIMULATION SPECIFICATION ’S ................................ ................................ ........................  22 \n4.1. State -of-the-art Assessment  ................................ ................................ ................................ . 22 \n4.2. In-Vitro Stimulation System Specification  ................................ ................................ .............  24 \n4.3. In-Vivo Stimulation System Specification  ................................ ................................ ..............  25 \n5. ELECTRODE ’S MATERIALS AND GEOMETRIES  ................................ ................................ .......................  26 \n5.1. METAL Electrodes  ................................ ................................ ................................ ..................  28 \n5.2. Platinum Electrodes  ................................ ................................ ................................ ..............  28 \n5.3. Gold Electrodes  ................................ ................................ ................................ .....................  29 \n5.4. ORGANIC Electrodes  ................................ ................................ ................................ ..............  29 \n5.4.1.  Conductive polymers -based electrodes  ................................ ................................ ........  29 \n5.4.2.  Polypyrrole  ................................ ................................ ................................ ....................  30 \n5.4.3.  PEDOT  ................................ ................................ ................................ ............................  30 \n5.4.4.  PANI  ................................ ................................ ................................ ...............................  30 \n5.5. Carbon -based electrodes  ................................ ................................ ................................ ...... 31 \n5.5.1.  CNT ................................ ................................ ................................ ................................  31 \n5.5.2.  Graphene  ................................ ................................ ................................ .......................  32 \n5.6. Hybrid electrodes  ................................ ................................ ................................ ..................  34 \n5.6.1.  PEDOT –CNT ................................ ................................ ................................ ...................  34 \n5.6.2.  Polypyrrole – graphene  ................................ ................................ ................................ . 34 \n5.6.3.  Polyaniline -graphene  ................................ ................................ ................................ ..... 35 3 \n \n \nThis project has received funding from the European Union’s Horizon 2020 research and inn ovation programme under grant \nagreement No 829060.  \n5.6.4.  Polycaprolactone fumarate -Polypyrrole  ................................ ................................ .......  35 \n5.6.5.  Polypyrrole -polydapamine  ................................ ................................ ............................  35 \n6. FINAL REMARKS  ................................ ................................ ................................ ................................  44 \n7. REFERENCES  ................................ ................................ ................................ ................................ ..... 45 \n \n \nList of figures  \n \nFigure 1 - Schematic representation of the methodologies steps projected to achieve SCI repair  .......  4 \nFigure 2 - Example of the fingerprints of an ideal current -controlled memristor, showing the pinched \nhysteresis loops when driven by a sinusoidal current and the limiting frequency behavior. .................  6 \nFigure 3 - Memristor definitions (current controlled version s, voltage controlled)  ...............................  7 \nFigure 4 - Dynamic route map of a hypothetical memristor.  ................................ ................................ .. 7 \nFigure 5 - Details of the Hodgkinson -Huxley exp eriment: a) schematic showing the neuron and axon; \nb) the HH model, comprising dissipative couplings and HH cells; c) details of the experimental set -up \nused to measure the current through the axon.  ................................ ................................ .....................  9 \nFigure 6 - HH cell equivalent circuit, depicting the two memristors associated to the sodium and \npotassium ionic channels.  ................................ ................................ ................................ .......................  9 \nFigure 7 - Fingerprints of the potassium channel memristor: pi nched hysteresis loops and admittance \nmapping for different frequencies, under sinusoidal excitation.  ................................ .........................  10 \nFigure 8 - Illustration of the LTD (blue curves) and LTP (red curves) process with the Po tassium \nchannel memristor: In the LTD process, the admittance decreases making the current passing \nthrough weaker (depressed). In the LTP process, the admittance of increases gradually, making the \nthrough stronger (potentiated).  ................................ ................................ ................................ ............  11 \nFigure 9 - Fingerprints of the potassium channel memristor: pinched hysteresis loops and admittance \nmapping for different frequencies, under sinusoidal excitation.  ................................ .........................  12 \nFigure 10 - Simulation results of applied voltage and simulated current during a CV analysis  ............  13 \nFigure 11 Bode and Nyquist plots of frequency sweep for Randles circuit  ................................ ..........  15 \nFigure 12 Randles circuit reconstruction under the presence of random perturbations.  ....................  15 \nFigure 13 Absolute error trends for different number of samples.  ................................ ......................  16 \nFigure 14 - Conceptual circuits for voltage (a) and current stimulation (b).  ................................ .........  18 \nFigure 15 - Different types of PNI accor ding to their sensitivity and invasiveness. (Kim and Romero -\nOrtega, 2012) ................................ ................................ ................................ ................................ .........  27 \n \nList of tables  \n \nTable 1 - Summary of electrode characteristics used for direct coupling ele ctrical stimulation.  .........  19 \nTable 2 - Summary of multifunctional probes for optical stimulation.  ................................ .................  20 \nTable 3 - Summary of multifunctional  chemical stimulation probes.  ................................ ...................  21 \nTable 4 - Different stimulation techniques based on Blint et al., 2013. ESC, embryonic stem cell; MSC, \nmesenchymal stem cell; ROS, reactive oxygen species; ECM, extracellular matrix; TGF transforming \ngrowth factor; ALP, alkaline phosphatase; VEGF, vascular endothelial growth factor.  ........................  24 \nTable 5 - In-vitro and in -vivo system specification summary.  ................................ ...............................  24 \nTable 6 - Summary of systems used for electrical stimulation  ................................ .............................  36 \n \n \n  4 \n \n \nThis project has received funding from the European Union’s Horizon 2020 research and inn ovation programme under grant \nagreement No 829060.  \n1. INTRODUCTION  \nThe design of implantable medical devices is usually a complex and multidisciplinary t ask. In what \nconcern electronic systems, considerations such as size and form factor, power consumption, energy \nharvesting means, stable and continuous operation, biocompatibility and security are of paramount \nimportance. Neural stimulation devices are not  an exception.  \n \nTherapies of spinal cord injury patients target the important objective to restore neural paths that \nhave been comprom ised. Project NeuroStimSpinal proposes a radical approach to this problem \ncomprising an innovative stimulus responsive cell -laden biomaterial able to repair the injured nervous \ntissue. The proposed innovative biomaterial characteristic is a scaffold for im plantation in the \ntraumatic injury point composed of graphene -based materials in combination with human adipose \nderived decellularized tissue, coupled with a wireless electrical stimulation device to promote the grow \nand reconnection of the ruptured nerves . \nThe electrical stimulation unit will have to interact with a system comprising the scaffold filled with the \nbiomaterial where the nervous tissues will grow. These interactions is accomplished with the addition \nof electrodes. In conjunction, the electrode s, the scaffold and the nervous tissues, present a complex \nand non -linear electrical impedance. Application of voltage (or current) pulses raise quite naturally \nconstraints on the maximum compliance currents (or voltages) that the unit must handle.  \nThe sha pe and pattern of the stimulation signals is also of concern. It has been established in many \nstudies, that electrical stimulation favors the growth of the nervous tissues. However, there seems to \nbe little consensus on the shape and pattern of the applied  signals. The qualitative understanding of \nhow neural cells react to electrical signals may disclose important clues. In this sense, what seems to \nbe of higher importance is not the shape of the signals but the charge these signals transfer into the \nsystem . \nAnother challenge is linked to the material of the electrodes. Here it is not only important to ensure \nthe biocompatibility, as the electrodes directly interact with cells (the stimulation device will require \ndedicated encapsulation to ensure biocompatib ility). Different materials have been investigated for \nthis purpose. Furthermore, different materials promote different electrical behavior (impedance).   \nFigure 1 - Schematic representation of the methodologies steps projected to achieve SCI repair  \n5 \n \n \nThis project has received funding from the European Union’s Horizon 2020 research and inn ovation programme under grant \nagreement No 829060.  \nThis deliverable starts by presenting models for the impedance of biological synapses and electrode \nkinetics. The qualitative understanding of these models reveals some clues which are of mainstream \nusage in stimulation devices. Section 3 describes the concept of neural stimulation and possible \napproaches to the problem. Section 4 focuses on electrical stim ulation, the state -of-the-art and \nspecifications for the system to be developed. Section 5 discusses the state -of-the-art of materials for \nelectrodes. Section 6 concludes the deliverable.  \n  6 \n \n \nThis project has received funding from the European Union’s Horizon 2020 research and inn ovation programme under grant \nagreement No 829060.  \n \n2. BACKGROUND CONCEPTS  \nThis section presents and discusses models for the impedance of neural cells and e lectrode kinetics. \nThese models provide a qualitative means to investigate the behavior of the impedance of the real \nsystem. The section begins by introducing the memristor, a device theoretically predicted in 1971 by \nLeon Chua (Chua, 1971) , which plays a key role in the understanding of how biological brains work.  \n2.1. The Memristor  \nThe memristor was proposed in 1971 by Leon Chua as an electrical element linking the relation \nbetween charge and flux li nkage (Chua, 1971) . Electrical circuit theory had, by that time, a set of \nelements that was thought to be complete. These are the resistor, the inductor and the capacitor. Each \nof these elements link two electrical variables in their definition: voltage and current in a resistor, \nvoltage and charge in a capacitor, current and flux in an inductor. Other two relations come from \nMaxwell equations, and link current and charge, and voltage and flux. To be m athematically consistent, \nthese set should include the missing relation between charge and flux. What Chua realized is that an \nelement building upon a flux -charge relation gives rise to a resistor. Moreover, if this relation  is non -\nlinear, it gives rise to  a resistor with memory. The memristor found in those initial year great opposition \nand disbelief, in part due to the fact, that there was no element showing the properties highlighted by \nChua. These properties  stated for the first time in 1971 paper (Chua, 1971)  are still valid today and \ndefine what became known as the fingerprints of the memristor  (see figure 2) . Very shortly, the \nfingerprints of the memristor are: i) the voltage -current relation, ex hibits hysteresis loops pinched at \nthe origin when the device is stimulated by a periodical and symmetric signal; ii) the area of the \nhysteresis decrease with frequency; and iii) for high frequencies the pinched hysteresis loops \ndegenerate in a single valu ed relation  (Adhikari et al., 2013) . In another paper (Chua and Kang, 1976) , \nthe authors  extended the definition of the memristor to a class of devices which became known as  \nFigure 2 - Example of the fingerprints  of an ideal current -controlled memristor, showing the pinched hysteresis loops when \ndriven by a sinusoidal current and the limiting frequency behavior.  \n \n7 \n \n \nThis project has received funding from the European Union’s Horizon 2020 research and inn ovation programme under grant \nagreement No 829060.  \nmemristive devices. One of the examples offere d as memristive device was the model of the axon of \na giant squid, proposed by Hodgkinson and Huxley (Hodgkin and Huxley, 1952) . Since them, not much \ninterest was raised by the memristor, a situation that re grettably sustained till 2008, when a paper by \nHewlett -Packard researchers established a link between resistive switching devices and the memristo r \n(Strukov et al., 2008) . Resistive switching devices are of high relevance for the electronics industry, in \nparticular for high density non -volatile memory chips and neuromorphic computing platforms. Thi s \nwas the beginning of a new era for memristors, with a high number of research groups investing both \nin theory and device fabrication.  \nThe definition of the memristor suffered several modifications and generalizations (Chua, 2015, 2014) . \nFigure 3 depicts an overview of different definitions, starting with the ideal definition for 1971, and \nprogressing to higher complexity of the extended memristor. In the most general form, the extended  \nmemristor has a set of defining equations, given by,  \n 𝑣=𝑀(𝑥,𝑖)𝑖 (2.1)  \n 𝑑𝑥\n𝑑𝑡=𝑓(𝑥,𝑖) (2.2)  \nEquation 2.1 is the input -output mapping function, which is general may depend on the input, in the \npresent case, current, i, and state, x. For simplicity sake, x is assumed a single variable , but in general  \nFigure 3 - Memristor definitions (current controlled versions, voltage controlled)  \n \nFigure 4 - Dynamic route map of a hypothetical memristor.  \nIdeal Memristor\nv=M(q)i\ndq/dt=iIdeal Generic Memristor\nv=M(x)i\ndx/dt=f(x)iGeneric Memristor\nv=M(x)i\ndx/dt=f(x,i)Extended Memristor\nv=M(x,i)I\nM(x,0)   \ndx/dt=f(x,i)\ndx/dt\nxI1I2I3\n-I3-I2-I1x1x2'x2' Move to the right\nMove to the leftDt\nDt\nI=08 \n \n \nThis project has received funding from the European Union’s Horizon 2020 research and inn ovation programme under grant \nagreement No 829060.  \nin can be a multi -valued vector of state variables. The only restriction applied is that M(x,0)  must have \na finite value. The second equation (2.2) is the state equation, specifying how the state evolves in time. \nThe state equation plays a  key role in the analysis of the behavior of the memristor. On important \naspect is the fact that this is a differential equation, which implies that the state depends on the \nprevious history of the stimulus and state.  \n2.1.1.  Dynamic Route Maps  \nDynamic rou te maps (DRM) are graphical tools that allow to have a good insight on the behavior of \ndifferential equations. In the case of a memristor, DRMs are of paramount importance as they reveal \nimportant information on how the state evolves. The DRM plot is a fam ily of curves for the state \nequation, the vertical scale shows how the time derivative of the state changes, while the horizontal \naxis shows the state values. The multiple curves on the plot, are the representation of the graphs of \nf(x,I)  when I assumes fi xed values.  \nFigure 4  provides an example of a DRM plot of a hypothetical current controlled memristor. As the plot \nshows, there are two distinct regions in the plot: the upper part, where dx/dt  is positive and the lower \npart, where dx/dt  is negative. When dx/dt  is positive, means that the state will increase, that is, it \nmoves to the right of the plot. When dx/dt  is negative, the state decreases, moving to the left of the \nplot. Also, depending on the applied current, the state will move along a specific cur ve, say, when I=I3, \nthe state moves on the top red curve. The amplitude of dx/dt, tells us how fast the sate moves. So, \nwhen I=I3, the state evolves faster from x=x 1 to x=x 2’’ than when I=I1, for the same amount of time Dt. \nIn this sense, dx/dt  is a measure of the velocity of the state update. This a very important and \nremarkable characteristic of memristors, shared with biological synapses. In that we can trade stimulus \namplitude for stimulus duration to reach the same e ffect (Chua, 2018) . \nAnother interesting feature revealed be the DRM lies on the curve for I=0, known as power -off-plot \n(POP). It says how th e state evolves in the absence of stimulus. When the POP coincides with the \nhorizontal axis, the state freezes, since dx/dt=0 , there is no evolution. Memristors with such \ncharacteristics  are universal memories , that is, are capable of non -volatile memory r etention \nproperties and are passive (meaning, memory retention does not depend on an external power \nsource). This is also shared by biological synapses, which behave as passive devices and have \ninformation storing characteristics (Chua, 2018) . \nAnother important aspect that can be easily understood using the DRM, is the concepts of long -term  \npotentiation (LTP) and long -term depression (LTD). Th ese processes explain how biological synapses \nlearn from experience. The current reasoning is as follows, synapses that are frequently stimulated \n(used) see their strength improving, or potentiated (LTP). While, synapses that are seldom stimulated, \nlose th eir significance, or get depressed (LTD). It is remarkable that memristors, devices theoretically \npredicted in the frame of circuit theory, exhibit a similar behavior. In fact, several experiments have \nrepeatability demonstrated how memristors, see (Chua, 2018, 2015, 2014)  for examples. To \nunderstand the dynamics of LTP and LTD in memristors, assume that M(x,i)  is some monotonic \nincreasing funct ion of x. Applying a sequence of positive stimuli (positive amplitudes) to the \nmemristors lead to the increase of the state variable (x moves to the right), this means that M(x,i)  will \nincrease, that is, it is potentiated. On the contrary, applying a seque nce of negative stimuli (negative \namplitudes), leads the state to decrease, leading M(x,i)  to decrease, or being depressed.  \n2.2. The Hodgkinson -Huxley Axon Model  \nThis section justifies the need for the introduction of the memristor and its interesting feat ures. The \nreason to bring this discussion is simply explained with the Hodgkinson -Huxley model for the Axon. In 9 \n \n \nThis project has received funding from the European Union’s Horizon 2020 research and inn ovation programme under grant \nagreement No 829060.  \n1952, Hodgkinson and Huxley introduce the electrical model of the axon of the giant squid Loligo Pialii  \n(Hodgkin and Huxley, 1952) . This model has stood the test of time and prevailed as a reference model \nin the fields of neurophysiology and brain research. The giant squid is endowed with very large axons, \nreaching 1mm diameter for large specimens, which make the experiments and tissue extraction \nsimpler. The HH circuit model , has it became  known, describes the current conduction as a process \ncomposed of two ionic currents carried by potassium and sodium ions. In its original form, the model \nincluded  time varying resistances associated to these processes. These seems unconventional from \nthe circuit theoretic point of view. A correct description states that these resistances should dependent \non the applied voltage and not directly on time. The original  paper by Hogkinson and Huxley, raised \nconcerns due to the prediction of effects that were not plausible for biological nerves. The model \npredicted the presence of signal rectification, something which was taught to be possible in \nsemiconductors and valves . A second effect was that the current exhibited inductive behaviors, though \nnone of these could be directly traced in brains. These concerns were only clarified recently in (Chua \net al., 2012a) .  \nThe explanation for this inconsistency raised due to the memristive nature of the model. In the more  \nrecent formulation introduced by Chua, the potassium and sodium channels are described by a state \ndependent Ohm’s law similar to what is found in extended memri stors. The potassium channel is a 1st \norder memristor, with one state variable, while the sodiu m channel is a 2nd order memristor with two  \nFigure 5 - Details of the Hodgkinson -Huxley experiment: a) schematic showing the neuron and axon; b) the HH model, \ncomprising dissipative couplings and HH cells; c) details of the experimental set -up used to measure the curr ent through the \naxon. \n \n \n \nFigure 6 - HH cell equivalent circuit, depicting the two memristors associated to the sodium and potassium ionic channels.  \n \nneuron\naxon\nDissipative (diffusion ) couplings\nHH cellsVaxonI\nElectrode inside \nthe axon\nElectrode \noutside the axonBath container\nAxona)\nb)c)\nCMGNa GK GL\nEL EK ENaINa IK IL ICM I\nV10 \n \n \nThis project has received funding from the European Union’s Horizon 2020 research and inn ovation programme under grant \nagreement No 829060.  \nstate variables (Chua et al., 2012a) . Figure 5 details  the Hodgkinson -Huxley model. The model has \ndistributed nature, with several HH cells connected by dissipative co uplings. All elements in the HH \ncells are thus expressed as densities. Each HH cell, is described according to the circuit of figure 6. The \nmodel comprises 7 elements: CM models capacitive effects, memristor GNa and voltage source ENa \ndescribe the sodium c hannel, memristor G K and voltage source E K describe the potassium channel, \nwhile GL and EL account for dissipative effects. The HH model is a non -linear with high complexity, \nwhich prevents its adoption for large networks of neurons. For that matter, model s with reduced \ncomplexity, such as the LIF model (Leak -Integrate -and-Fire) are commonly used. The HH model allows \ndeeper investigation on the effects of single synapses, such as the emergence of edge of caos  (Chua et \nal., 2012b, 2012a) . \n2.2.1.  Potassium Channel Dynamics  \nThe dynamics of the potassium channel current conduction are adequately represented by a 1st order \nextended memristor, described be the following equations  (Sah et al., 2014) , \n \nWhere 𝐺𝐾(𝑛)=𝑔𝐾̅̅̅̅𝑛4 is the voltage -current mapping function. Simulation of these equat ions reveals \nthe memristive nature of the model. Figure 7 depicts  an illustration of the fingerprints of the potassium \nchannel memristor, when driven by a sinusoidal voltage of different frequencies. Figure 7 also, shows \nthe double valued admittance mappin g G K(n). Figure 8 provides an illustration of the LTP and LTD \nprocess in the potassium channel. For this study, the stimulus consisted of a sequence of positive  \nFigure 7 - Fingerprints of the potassium channel memristor: pinched hysteresis loops and admittance mapping for different \nfrequencies, under sinusoi dal excitation.  \n \n 𝑖𝐾=𝐺𝐾(𝑛)𝑣𝐾 (2.3)  \n  \n𝑑𝑛\n𝑑𝑡=(0.01(𝑣𝐾+𝐸𝐾+10)\n𝑒𝑣𝐾+𝐸𝐾+10\n10 −1)(1−𝑛)−(0.125 𝑒𝑣𝐾+𝐸𝐾\n80 )𝑛 (2.4)  \n11 \n \n \nThis project has received funding from the European Union’s Horizon 2020 research and inn ovation programme under grant \nagreement No 829060.  \n(negative) pulses for the LTD (LTP) process. The current through the potassium channel is highl y \nasymmetrical (in both amplitude and shape), due to the dynamics of the channel. In the LTD case, the \ncurrent at each pulse decreases in amplitude, leading to a decreasing admittance, resulting in the \ndepression of the channel. For the LTP case, the curre nt at each pulse increases in amplitude, leading \nto an increase in the admittance. This favors the conductivity of channel, that is, potentiates the \nchannel.  \n \n2.2.2.  Sodium Channel Dynamics  \nThe dynamics of the potassium channel current conduction are adeq uately represented by a 2nd order \nextended memristor, described be the following equations (Sah et al., 2014) , \n 𝑖𝑁𝑎=𝐺𝑁𝑎(𝑚,ℎ)𝑣𝑁𝑎 (2.5)  \n 𝑑𝑚\n𝑑𝑡=(0.1(𝑣𝑁𝑎−𝐸𝑁𝑎+25)\n𝑒𝑣𝑁𝑎+𝐸𝑁𝑎+25\n10 −1)(1−𝑚)−(4𝑒𝑣𝑁𝑎−𝐸𝑁𝑎\n18 )𝑚 (2.6)  \n 𝑑ℎ\n𝑑𝑡=(0.07𝑒𝑣𝑁𝑎−𝐸𝑁𝑎\n20 )(1−ℎ)−(1\n𝑒𝑣𝑁𝑎−𝐸𝑁𝑎+30\n10 +1)ℎ (2.7)  \nWhere 𝐺𝑁𝑎(𝑚,ℎ)=𝑔𝑁𝑎̅̅̅̅̅𝑚3ℎ is the voltage -current mapping function. Simulation of these equations \nreveals the memristive nature of the model. Figure 9 depicts an illustration of the fingerprints of the \npotassium channel memrist or, when driven by a sinusoidal voltage of different frequencies. Figure 9 \nalso, shows the double valued admittance mapping G Na(m,h).  \n  \nFigure 8 - Illustration of the LTD (blue curves) and LTP (red curves) process with the Potassi um channel memristor: In the \nLTD process, the admittance decreases making the current passing through weaker (depressed). In the LTP process, the \nadmittance of increas es gradually, making the through stronger (potentiated).  \n \n12 \n \n \nThis project has received funding from the European Union’s Horizon 2020 research and inn ovation programme under grant \nagreement No 829060.  \n2.3. Electrode Kinetics  \nElectro chemistry measures are widely used to understand the interaction electrode -cell and \nconsequently the solution characteristics. Due to this, the electrode characteristics and the essential \nphysical laws must be considered in a first place since the interact ions electrode -electrolyte must be \nwell known in order to understand the system’s characteristics when some sort of variation on the \nstandard conditions occur (Olivieri et al., 2006) . \nThe electrodes play an important role in electrochemistry analysis. Furthermore, their interaction with \nthe cell can determine many of the visi ble phenomenon and also helping improving results in a simpler \nand better way according to the configuration, 2, 3 or 4 probes . \nThere are two different kinds of current in electrochemical cells, faradaic and capacitive currents, also \nknown as non -faradaic (Scholz, 2015) : \n The faradaic currents are responsi ble for the mass transfer and can be related with diffusion, \nmigration or convection transport mechanisms that should be considered according to the cell \ncharacteristics.  \n The non -faradaic currents are the result of the polarized molecules accumulation near  the \nelectrodes’ surface, creating a capacitive layer also known as electrochemical double layer \n(EDL).  \nThe Butler -Volmer (equation 2.8) equation and Fick’s laws (equations 2.9 and 2.10)  are especially \nimportant when the goal is to understand this electrochemical phenomenon (Bard and Faulkner, \n2001) . The Butler -Volmer equation describes the electrode kinetics in terms of the applied potential \nand the chemical species concentrations. The chemical species are dependent on the electrode \nmaterial composition and the electrolyte. Fick’s Laws for diffusion allow to determine how the \nconcentrations evolve in in time and space. Here for simplicity sake, we assume a simple one-\ndimensional  mode l. It is important to understand that Fick’s laws apply only for the case where \nmigration and convection transport mechanisms are absent. When these transport mechanisms are  \nFigure 9 - Fingerprints of the potassium channel memristor: pinched hysteresis loops and admittance mapping for different \nfrequencies, under sinusoidal excitation.  \n \n13 \n \n \nThis project has received funding from the European Union’s Horizon 2020 research and inn ovation programme under grant \nagreement No 829060.  \nimportant (migration due to the strength of the applied potential, or convection d ue to stirred \nsolutions), equations 2.9 and 2.10  must be replace  by the more general  \n 𝐼=𝐹𝐴𝑘0 (𝑐0(0,𝑡)𝑒−𝛼𝐹\n𝑅𝑇(𝐸−𝐸0′)−𝑐𝑟(0,𝑡)𝑒(1−𝛼)𝐹\n𝑅𝑇(𝐸−𝐸0′)) (2.8) \n 𝑗𝑖(𝑥,𝑡)=−𝐷𝑖𝜕𝑐𝑖(𝑥,𝑡)\n𝜕𝑥 (2.9) \n 𝜕𝑐𝑖(𝑥,𝑡)\n𝜕𝑥=−𝐷𝑖𝜕2𝑐𝑖(𝑥,𝑡)\n𝜕𝑥2 (2.10 ) \nNote that 𝐼 is the current flowing through the cell, 𝑗𝑖 is the electrode current density associated to the \nchemical species i, F is the Faraday constant, T is the absolute temperature, R is the universal gas \nconstant, E is the electrode potential , A is the area  of the electrode, 𝑐𝑖 is the concentration of the 𝑖 \nspecie and 𝛼 is the charge transfer coefficient. 𝑥 and 𝑡 are the position and time, respectively.  \n \n 𝐷𝑡1\n2\n0𝐶𝑐𝑖\n𝑐𝑖∗= ± 𝐼\n𝐹𝐴√𝐷𝑖𝑐𝑖∗ (2.11) \n 𝐷𝑡𝛼\n0𝐶𝑓(𝑡)=1\nΓ(n−α)∫𝑓(𝑛)(𝜏)\n(𝑡−𝜏)𝛼+1−𝑛𝑑𝜏𝑡\n𝛼 (2.12) \n   \nEquation 2.11  shows the form of the general solution of the Fick's equations for infinite planar \nelectrodes immersed in non -stirred solutions , where α is the fractional order and  Г is a convolution \nintegral: Γ(𝑧)=∫𝑥𝑧−1𝑒𝑥𝑑𝑥∞\n0 para 𝔑(𝑧)>0. Solutions for cylindrical electrodes are similar in form, \nwith differences due to the geometry of the electrodes. This solution comes in the form of a fractional \ndifferential equ ation, where the differential operator, is defined in Caputo sense according equation \n2.12 . It is interesting to see that this form shares in common the differential nature of the state \nequations in a memristor  (Martins, 2019) . \n \n2.3.1.  Cyclic voltammetry  \n  \nCyclic voltammetry is an electrochemical method used to experimentally characterized ce lls, it can be \ndirectly simulated using Butler -Volmer equation  (2.8)  and the solution of the Fick’s laws (2.9 and 2.10) \nfor the specific conditions of the interaction. The electrons are ejected on one of the electrode based   \nFigure 10 - Simulation results of applied voltage and simulated current during a CV analysis  \n14 \n \n \nThis project has received funding from the European Union’s Horizon 2020 research and inn ovation programme under grant \nagreement No 829060.  \non a triangle waveform signal (f igure 10) and the current that flows through the cell is measured \ncausing a duck shape hysteresis loop as shown in figure 10. There are many variables that can provide \ndifferent “duck shape” characteristics: the redox pair, the other compounds in the solut ion, the sweep \ncharacteristics, environmental variables and the set -up physical characteristics (electrodes’ material, \ndistance between them, etc.)  (Martins, 2019) . \nFrom the CV cycle, it is possible to infer several features of the electrode -electrolyte reaction , such us  \nthe cathodic and anodic currents. Those currents are reached when all the substance at the surface is \nreduced or oxidized, respectively , and they are represented as the maximum and minimum val ue of \nthe current during the CV voltage sweep . These currents are especially  important because when the \ncomposition of the solution varies, also these peaks can change  and provide information about these \ncompounds  and the solution stat e (Martins, 2019) . \n2.3.2.  Electrochemical Impedance Spectroscopy  \nElectrochemical Impedance Spectroscopy, EIS, is a method usually applied for the sensing of \nelectrochemical sensors and small signal characterization of electrode r eaction. This kind of sensing \nsystems is one of the most complex ones as  it entails a small signal excitation and frequency sweeping \nprotocols. Being a small signal method, the extracted characterization of the cell is also a small signal \nequivalent circui t, composed by linear elements. This small signal equivalent circuit is commonly \nreferred in the literature as Randles circuit (Bard and Faulkner, 2001) . Figure 11 depicts the typical \nimpedance chart and the associated Randles circuit when excited with a sequence of sinus oidal signals \nwith varying frequencies. The Randles circuit comprises four elements: i) the limiting resistance Rs, \nwhich corresponds to the resistance of the sensor at infinite frequency; ii) the transfer resistance Rct, \nusually representing the sensitivity of the transducer to some particular chemical species; iii) the Cdl, \nrepresenting the non -Faradaic  response of the transducer, usually modelled as a capacitive double \nlayer accounting for the interface between the transducer and the aqueous environment; iv) and the \nWarburg element Zw, which accounts for the low frequency response of the transducer, typ ically \nexhibiting a 45º degrees inclination (𝑍𝜔=𝐴𝜔√𝑗𝜔⁄ ). \nExperimental EIS uses dedicated laboratory equipment such as  VersaSTAT 3 from AMETEK, Inc. These \nequipment includes a potentiostat (the cell interfacing circuit), a DDS system (direct digital synthesizer, \nresponsible for the gene ration of the excitation signals) and a vector analyzer (embedding a FFT engine \n(fast Fourier transform) responsible for the signal analyzes in the frequency domain). The potentiostat \nis highly  sensitive instrumentation amplifying chain, design to ensure s table operation and negligible \nperturbation on the reference electrode. In the frequency domain, the impedance values are \nrepresented by default by real and imaginary parts. The Randles circuit extraction step involves non -\nlinear optimization methods, such  as non -linear least squares based on Levenberg -Marquard or trust -\nregion -reflective algorithms. This procedure outputs the set of circuit parameters P={Rs, Rct, Cdl, Zw} \nthat best match the set of impedance values taken by the EIS system. Mathematically th e problem \nspecification can be cast as:  \n 𝑃=min\n𝑃∑(𝑦𝑘−𝑍𝑇(𝜔𝑘,𝑃))2\n𝑘 (2.13) 15 \n \n \nThis project has received funding from the European Union’s Horizon 2020 research and inn ovation programme under grant \nagreement No 829060.  \nWhere P is the set of Randles circuit parameters, yk is the measure value of the impedance of t he \ntransducer at the test frequency ωk. The Randles circuit impedance is,  \n \n 𝑍𝑇(𝜔,𝑃)=𝑅𝑆+𝑍𝑐𝑑(𝜔)(𝑅𝑐𝑡+𝑍𝑤(𝜔))\n𝑅𝑐𝑡+ 𝑍𝑤(𝜔)+ 𝑍𝑐𝑑(𝜔) (2.14) \nFigure 12 depicts a reconstruction example of the Randles circuit impedance (equation 2.14) for a set \nof 10 simulated values of the impedance at 10 logarithmic spaced frequencies between 1Hz and 10kHz. \nThe figure sh ows that the reconstruction using the trust -region -reflective algorithm is possible even in \nthe presence of random variations , due to noise or parameter changes. The test was made assuming \n10% and 1% standard deviation tests.   \nFigure 11 Bode and Nyquist plots of frequency sweep for Randles circuit  \n \nFigure 12 Randles circuit reconstruction und er the presence of random perturbations.  \n16 \n \n \nThis project has received funding from the European Union’s Horizon 2020 research and inn ovation programme under grant \nagreement No 829060.  \nModel reconstruction also depend on the number of measured samples. In order to have a clear idea \non what the real performance can be, we made some simulations. These simulations assume the \npres ence of random perturbations, following as before the 1% and 10% standard deviation as above \nand a Gaussian distribution. We run for each value of the number of samples, 1000 simulations in order \nto gain  some statistical significance. The results depicted on figure 13, show that the average absolute \nerror follows a decreasing trend when we increase the number of samples. Furthermore, even with a \nsmall number of samples, it is possible to infer with high accuracy the parameters of the Randles circuit, \nif the  random perturbations are of small amplitude. The error bar plots show also the standard \ndeviation (vertical bars) for each trial, which also decrease with the number of samples.  \n \n \n   \nFigure 13 Absolute error trends for different number of samples.  \n17 \n \n \nThis project has received funding from the European Union’s Horizon 2020 research and inn ovation programme under grant \nagreement No 829060.  \n3. NEURAL STIMULATION CONCEPTUAL SCHEMES  \nThe stimulation of nervous tissues can take different approaches. These approaches are linked to the \ntissue reactions to the external stimulation. Nervous tissues are composed by biological substrates \nwhich exhibit specific responses to stimuli of different natures. As these responses are r elated to ionic \ninteractions at the cell level, external interaction is possible either through the application of electrical \nsignals, release of specific chemicals and even light pulses of specific wavelengths (Kim et al., 2019) . \nFigure 14 depicts the conceptual diagrams associated to each of these possibilities. The next sections \npresent and overview of these possibilities and high light their properties.  \n \n3.1. Electrical Stimulation  \nIn electrical stimulation, the essence of the technique is to motivate the flow of ions in the cells. These \ncan be accomplished using three possible approaches: d irect coupling, capacitive coupling and \ninductive coupling. In direct coupling the ionic motion is motivated by the application of a \nvoltage/current/charge signal to a pair of electrodes that directly interact with the cells (Kim et al., \n2019) . These is the most used method for high density neural implants, as it is the most amenable \nmethod to circuit integration (X. Li et al., 2017) . Direct interaction with the cells raises some  challenges \nand concerns, associated with  biocompatibility of the electrode’s materials or unwanted cell \nresponses. In capacitive coupling, there is no direct contact with the cells. The ionic motion is achieved \nthrough the influence of an applied electric al field. Th is method implies the usage of two plate type \nelectrodes in parallel configuration (similar to a capacitor). The cells are exposed to the electrical field \nbetween the plates. Another alternative is inductive coupling, where the electric field i s replaced by a \nmagnetic field in a coil like system. The cells are embedded within the coil axis. Both of these methods  \nFig. 14 – Nervous tissues stimulation approaches: a) electric al stimulation; b) chemical stimulation; c) optical \nstimulation.  \nCurrent /Voltage /\nCharge DriverStimulation \npulseStimulation \nElectrode\nReference \nElectrodeFlow of \nions\nStimulation \npulseHigh -Voltage\nDriverMicrofluidics\nChemicals\nLED\nDriverMicro -LEDStimulation \npulse\nLighta)\nb)\nc)18 \n \n \nThis project has received funding from the European Union’s Horizon 2020 research and inn ovation programme under grant \nagreement No 829060.  \nprevent the direct contact with the cells, but demand highly complex infrastructures, not compatible \nwith circuit integration (X. Li et al., 2017) . \nElectrical stimulation with direct coupli ng can adopt different forms concerning type of stimulus \napplied: voltage, current or charge. Current and charge are commonly preferred solutions for \nintegrated circuit design as they are based on simple bidirectional current mirrors. High voltage \ncomplian ce is a problem for these cases, as with current technologies the voltage headroom is low \n(current state -of-the-art CMOS processes operate with bias below 1V). However, as seem in the \nsection 2, the dynamics of biological synapses exhibit an amplitude -time  inverse relation which can be \nsuitably exploited for the design of electrical stimulation circuits. The choice of current or charge \nstimulation is also a natural one, given that is often desirable to have a precise control over the current \nor charge trans ferred to the cells. This is explained by the limits that can be applied to charge and \ncharge density during the stimulation (Cong, 2016) . There is an experimental based Shannon criterion \nrelating the charge and the charge density during stimulation, this criterion establishes limits for safe \noperation. Oper ating the stimulation above these limits implies damage to the cells  (Cong, \n2016) .Voltage mode stimulation does not allow a direct control over the charge transferred to the \ncells. Thus,  it implies the usage of additional current limitation circuitry, which further complicates the \ndesign  \nFigure 15 depicts the conceptual circuits for voltage a current stimulation. Starting with the voltage \nstimulation circuit , it is based on a conventional potentiostat used  to test electrochemical cell s (Bard \nand Faulkner, 2001) . It is basically a feedback amplifying circuit design to for ce the input voltage Vi on \nthe reference electrode (RE). In order to operate properly, the RE amplifier should exhibit a very high \ninput impedance in order to induce minimal perturbation of the cell. Furthermore, the amplifier \nfeeding the working electrode  (WE) should have high voltage drive capabilities in order to handle large \nvoltage variations implied by the cell. For safe stimulation, the system should consider a second \nfeedback loop, controlled by a current sensing element. The current sensing control  allows to prevent \ndamage of the cells, by acting appropriately on the stimulus input. Current and charge stimulation \noperate on the same principle: direct current and current control. Possible implementations use \nbidirectional current sources such as the one depicted in figure 15. Current direction is controlled  \nFigure 14 - Conceptual circuits for voltage (a) and current stimulation (b).  \n \n+\n-\n+\n-WE\nRECEVi\nCSFlow of \nionsA1\nA2\nCurrent \nsensingFlow of \nionsVDD\nIA\nICS1\nS2\na)b)19 \n \n \nThis project has received funding from the European Union’s Horizon 2020 research and inn ovation programme under grant \nagreement No 829060.  \nthrough switches S 1 and S 2, while current control is directed controlled through the sources (or current \nmirrors) I A and I C. The output voltage handling is constrained by the bias voltage V DD. Char ge control \ncan be achieved with the same circuit by implementing a precise timing control circuit for the opening \nand closing of switches S 1 and S 2. that charge is just the time integral of current), this is easily achieved \nwith fixed amplitude current pul ses. Given that charge is the time integral of the current, for a fixed \namplitude current pulse, the charge transferred is simply the product of the pulse amplitude by the \npulse duration. A suitable modification of this circuit, with just one current contr olled source are \npossible using a H bridge to interface the electrodes (X. Li et al., 2017) . \n \n \nTable 1 - Summary of electrode characteristics used for direct coupling electrical stimulation.  \nAnother important aspect related to electrical stimulation, and especially with the electrodes is the \ncharge storage capacity (CSC), that is, the total charge available at the electrode per unite area. The \ngoal of any stimu lation system is to maximize the CSC, this encompasses the selection of electrode \nmaterials. The choice of a particular materials has implications on the electrochemical impedance, and \nthus, with overall performance of the stimulation unit (Kim et al., 2019) . Different materials have \ndifferent values of CSC and produce different electrochemical impedance levels. Electro -deposited \niridium oxide is one of the most used materials for electrodes, with a CSC sufficiently hig h (25 mC/cm2). \nOther materials based on carbon nanotubes and graphene exhibit higher CSC but with lower charge \ninjection limits . Table 1 summarizes  the characteristics of some materials used for electrodes.  \n3.2. Optical Stimulation  \n Electrode \nMaterials  \n CSC \n(mC/cm2) \n Charge \nInjection limit \n(mC/cm2) Impedance  \n@ 1 kHz  \n(k) Electrod\ne size  \n(m2) Impedance\n/area  \n(/m2) Reference  \nAu 3.18  0.1 52 10000  5.2 (Bozkurt and \nLal, 2011)  \nPt 6.26  0.026  - - - (Cisnal et al., \n2018)  \nTiN 2.35  0.87  - 4000  - (Weiland et al., \n2002)  \nIrO x 25 - 7.1 2400  3 (Han an d \nMcCreery, \n2008)  \nPtIr 1.2 0.15  26.6  17000  1.6 (Vitale et al., \n2015)  \nIrO x/Pt 46.28  - - - - (Chung et al., \n2018)  \nITO 0.058  - - 78.5  - (Yang et al., \n2018)  \nCNT fiber  372 6.52  14.1  1.45  9.7 (Vitale et al., \n2015)  \nGraphene  50 3.2 0.519  90000  0.006  (Lu et al., 2016)  \nPEDOT  75.6  - 23.3  177 132 (Wilks et al., \n2009)  \nGlassy \nCarbon  61.4  3 5.8 70650  0.08  (Nimbalkar et \nal., 2018)  20 \n \n \nThis project has received funding from the European Union’s Horizon 2020 research and inn ovation programme under grant \nagreement No 829060.  \nOptical stimulation exp loits the optical response of nervous tissues, and in particular, neurons (Kim et \nal., 2019) . This is the focus of optogenetics, with the aim to address high spatial resolution stimulation \nof neural cells suing light. Neurons are naturally insensitive to light, but once they genetically modified \nwith opsins, they can become sensitive. Excitation and inhibition are two approaches follows for neural \nstimulation. This genetic modification can be achieve using light sensiti ve proteins. For instance, using \nchannelrhodopsin -2 (ChR2) turns neurons sensitive to blue light (~473 nm), while halorhodopsin \n(NpHR) triggers sensitivity to yellow light (~590 nm). ChR2 is used for excitation while NpHR is used for \ninhibition (Kim et al., 2019) . The neural probes for optical stimulation used LEDs (light emitting diodes) \nor laser diodes (LD) with wavelengths matched to the proteins sensitivity peaks. Table 2 summarizes  \nexamples of optical probes develo ped for optical stimulation of neurons.  \nProtein \n(color)  Light Source  Wavelength \n(nm)  Power  Device \nsize (m2) Reference  \n- Laser  473/593  1.35 mW – 473nm  \n3.6 mW - 593nm  - (Rubehn et al., \n2011)  \nChR2 (blue)  Laser  473 51 mW/mm2 70 (Wu et al., \n2013)  \nChR2 (blue)  LED 675/530/450  17.7/23.5 mW/mm2 2500  (Kim et al., \n2013)  \nChR2 (blue)  Laser  475 500 mW/mm2 300 (Son et al., \n2015)  \nChR2 (blue)  LED 460 353 mW/mm2 150 (Wu et al., \n2015)  \nArchT (green)  Laser  550 1.52 mW  1.963  (Kim et al., \n2015)  \n- LD 650 96.9 mW/mm2 195 (Schwaerzle et \nal., 2017)  \nTable 2 - Summary of multifunctional probes for optical stimulation.  \nOptical stimulation is out of scope of project NeuroStimSpinal. From the point of view of stimu lation \ntechnologies and circuit design considerations, it can be a promising candidate for future research. \nUsing optical means for stimulation offers several interesting possibilities:  \n Reduced power consumption, as the power required for stimulation is no t dependent on the \nimpedance of the tissues.  \n Indirect access method, as the electrodes may not need to be in contact with the nerves. In \nthis sense, the scaffold could embed biocompatible light sensitive materials, that would \nproduce an optical response in  reaction to an external stimulus. The external stimulus could \nbe applied externally, exploiting sub -cutaneous optical transmission to the scaffold.  \n Reduced the need to external devices.  \nThese possibilities are at this stage based on research on optical tr ansdermal communications \n(Ghassemlooy et al., 2017) . The goal for these systems is to establish secure communications links \nwith implantable devices, such as pacemakers. Research results so far, disclosed the possibility to \ncommunicate wi relessly with transcutaneous devices. Studies have also addressed the characteristics \nof the medium, human dermis, quantifying the attenuation to optical wavelengths . \n3.3. Chemical Stimulation  \n 21 \n \n \nThis project has received funding from the European Union’s Horizon 2020 research and inn ovation programme under grant \nagreement No 829060.  \nChemical stimulation explores the chemical interaction between  neural cells. These interactions are \nlinked to the action potentials (electrical signals), but also to the exchange of neurotransmitters. This \nraises the possibility to use specific chemicals to trigger neural responses (Kim et al., 2019) . There are \ntwo main drug delivery mechanisms in use for chemical stimulation: pressure driven (or displacement \ndriven) microfluidic delivery; and electrochemical controlled release. Microfluidics systems, such as \nmixers and valves, ha ve been used for drug delivery with success. Electrochemical release involves the \nusage of electrodes specific design to release the chemical drugs from their surface. Table 3 presents  \na summary of probes developed for chemical stimulation of neural cells.  \nDelivery \nmethod  Features  Materials for \nchannel  Chemicals  Drug \nmixing  In vivo \ntest Reference  \nMicrofluidic  - LPCVD/thermal \ndielectrics  Kainic acid, \nGABA  - Yes (Jingkuang \nChen et \nal., 1997)  \nMicrofluidic  - Parylene  Evans blue, \nalbumin  - Yes (Neeves et \nal., 2006)  \nMicroflui dic Micropump, \nreservoir  Silicon  - - No (Park et \nal., 2008)  \nMicroflu idic Pressure \nreservoir  Silicon nitride  Hoechst 33342, \npropidium \niodide, HEPES \nbuffer, glucose  - Yes (Retterer \net al., \n2008)  \nMicrofluidic  Microfuidic \nchip  silicon  Pilocarpine, \ntetrodoxin, \nsaline, DAPI  Yes Yes (Shin et al., \n2015)  \nMicrofluidic  Fluidic \nchannel  Borosilicate  Saline, \nmuscimol, aCSF  - Yes (Dagdeviren \net al., 2018)  \nElectrochemical  PEDOT  No channel  Dexametha sone  - Yes (Boehler et \nal., 2017)  \nElectrochemical  Polypyrrole  No channel  Dexamethasone \nphosphate, \nmeropenem  - No (Shah et al., \n2018)  \nTable 3 - Summary of multifunctional chemical stimulation probes.  \n  22 \n \n \nThis project has received funding from the European Union’s Horizon 2020 research and inn ovation programme under grant \nagreement No 829060.  \n4. ELECTRICAL STIMULATION SPECIFICATION ’S  \nThis section presents a state of the art on electrical stimulation of nervous tissues with highlight on  \nthree techniques: direct coupling, capacitive coupling and inductive coupling. For each technique, the \nstudy focused the characteristics of the applied stimuli and the results achieved in terms of \nmorphology of the cells under stimulation. This initial st udy provides information for the development \nof the stimulation unit. Relevant information such as: shape of the stimulus, amplitude and frequency, \nrecurrence and duration of the trials, are essential for design considerations. Equipped with this \ninformati on, the other section focuses on the development of an in -vitro stimulation -acquisition unit, \nand an in -vivo stimulation unit. For the in -vitro unit, the design approach considers the usage of \ncommercial off the shelf components, and less demanding restric tion in terms of power consumption, \nvoltage -current capabilities and size. It also includes the option for electrical signal acquisition, which \nthough not part of the project, may lead to important contribution for the state of the art on the field \nof the project. The in -vivo unit  poses some design challenges linked to power consumption, size and \ncurrent -voltage capabilities. The design approach for this unit involves integrated circuit design \ncapabilities in order to handle size constraints.  \n \n4.1. State -of-the-art Assessment  \n \nThere are 3 main methods of delivering  the stimulus  to neural cells  (Balint et al., 2013) : \nDirect coupling : the electrode is in direct contact with the cell culture or implanted into the animal. \nThis a pproach has some disadvantages: i) Toxicity due t o incompatibility of electrodes material ; ii) pH \nchanges; iii) Molecular reduction ; and iv) Generation of Faradaic byproducts . \nCapacitive coupling : an electric field is created between the two layers of the plate capacitor. The \ndistance between the plates varies depending on the culture's size in the middle. Some of the \nadvantages of this type of coupling are mainly: avoiding most of the inconvenient environmental \nchanges  introduced by direct coupling and generate a homogeneous electric field, which affects  all the \ncells in the same way.  \nInductive coupling : The cell culture is placed in the center of one or more coils that generate a magnetic \nfield. Pulsed magnetic field stimulation (PEMF) is a specific type of an inductive coupling, where there's \na pulsed s timulus instead of a static or sinusoidal.  \n \nTable 1  compares these different stimulation approaches . Direct stimulation is mainly used to \ninfluence the morphology and orientation of cells. Capacitive stimulation seems to be more effective \nin increasing pr oliferation, while cells treated with inductive stimuli display greater embryonic stem \ncell (ECM) deposition.  There is  also the possibility of mixing the modalities like it was done by Brighton \n(Brighton et al., 2001)  with MC3T3 -E1 clonal osteoblastic cells applying and comparing 3 modalities: \nconductive (0.002V/mm sinewave at 60 Hz for 0.5 –24 h), inductive (22.5 – 2.5G pulses in 15 Hz bursts \nfor 0.5 –24 h) and combined (340 – 140mG static and 370 – 47 mG, 76.6 Hz alternating field for 0.5 –24 \nh). The results showed increment in DNA content for all of the modalities but some effects on Ca2+ \nvaried between them.  Table 1 p resents also relevant information concerning the type and \ncharacteristics of the stimulation signals. Depending on the type of cells there are limits for electrical \nspecifications and there are thresholds that can result in cell death (e.g., 20 µA in the c ase of \nnonunions ). A wide range of frequencies (7.5 -60k Hz) have been applied in some experiments but most 23 \n \n \nThis project has received funding from the European Union’s Horizon 2020 research and inn ovation programme under grant \nagreement No 829060.  \nof them were under 100 Hz, which makes sense according to in vivo natural processes. It  is especially \nimportant to understand the type of cells under  stimulation to provide an appropriated stimulus  \n(Gulrez et al., 2011) . \n \nArticle  Cell types  Stimulation  Results  \nDirect coupling : \n(Schmidt  et al., \n1997a)  Rat PC -12 cells   0.1V with 10 mA for 2 h \nusing a Polypyrrole - (PPy) \nfilm as anode  Stimulation on the PPy film produced \nsignificantly larger neurite length than \nin unstimulated and tissue culture \nplastic control groups.  \n(Serena et al., \n2009)  Human ESC line \nH13 1V/mm at 1Hz  Spontaneous contractions, expression, \nand sarcomnetric organization  \n(Shi et al., 2008)  Human \ncutaneous \nfibroblasts  0.05V/mm DC for 24 –48 h \nusing a PPy/PLLA \nconductor  Cells adhered, spread and proliferated \non the conductors  \n(Sun et al., 2006)  Rat bone \nmarr ow MSCs  0.2, 0.4, and 0.7V/mm DC \nfor 0 –60 min on Coll. Type \nI gel Cytokine production enhanced 10 -fold \nby stimulation; Changes in morphology \nobserved with both MSCs and \nfibroblasts, but only the latter aligned \nin response to the stimulus.  \n(Radisic et al., \n2004)  Rat fibrob lasts \nHT1080; \nNeonatal rat \nventricular \nmyocy tes 0.5 V/mm, 2ms \nrectangular pulses at 1Hz  Stimulation resulted in alignment and \ncoupling, increased amplitude \ncontractions, and enhanced \nultrastructural organization.  \nCapacitive  coupling:  \n(Au et al., 2007)  Neonatal rat; \ncardiomyocytes \nand NI H3T3 \nfibroblasts  0.23 and 0.43V/mm, 1ms \nrectangular pulses at 1Hz  Stimulation enhanced elongation of \nboth cell types and fibroblast alignment \non abraded surfaces. Topographical \ncues were stronger.  \n(Hart ig et al., \n2000)  Bovine primary \nosteoblasts  6 V/mm, 62.5 ms \nsawtooth pulses at 16 Hz \nfor 5 -18 days  Significant increase in proliferation in \nnonconfluent and enhanced ECM \nrelated protein secretion in confluent \ncultures.  \n(Kim et al., 2006)  Rat calvarial \nosteoblasts  1.5 mA/cm2 biphasic el. \ncurrent at 3000 Hz for 6 h \nor 24 h/day  Higher proliferation in continuously \nstimulated samples. No change in ALP, \nosteopontin, coll. Type I,  BMP -2, -4, \nIGF-2, and TGF -b1 levels.  \n(Kim et al., 2008)  Human bone \nmarrow MSCs  1.5 mA/cm2 biphasic el \ncurrent at 100Hz for 24 \nh/day for 7 days  Proliferation increased by 57%. No \nincrease in osteoblast differentiation. \nStimulation induced VE GF expression.  \n(Kim et al., 2009)  Human MSCs  1.5 or 15 mA/cm2 \nbiphasic current  250/25 \nms at 100 Hz for 7 days  Increased proliferation. ALP activity \nand calcium deposition enhanced after \nstimulation. Expression of VEGF and \nBMP -2. \n(Zhuang et al., \n1997)  MC3T3 -E1 \nclonal \nosteoblastic \ncells  0.002V/mm, 300 mA/cm2 \nsine signal at 60 kHz for \n30 min -24 h  Stimulation enhanced proliferation and \nincreased the levels of TGF -b1. \nInductive coupling:  \n(Bodamyali et al., \n1998)  Neonatal rat \ncalvarial \nosteoblasts  1G, 0.225 ms pulses in 4.5 \nms bursts for 6 h  Exposure significantly increased the  \nnumber and size of deposited bone -like 24 \n \n \nThis project has received funding from the European Union’s Horizon 2020 research and inn ovation programme under grant \nagreement No 829060.  \nnodules. Enhanced BMP -2 and BMP -4 \nexpression.  \n(Lohmann et al., \n2003)  MLO -Y4 \nosteocyte -like \ncells ROS \n17/2.8 cell line  16G sawtooth pulses in \n4.5 ms bursts at 15 Hz for \n8h/day  Increased ALP activity, TGF -b1, and \nprostaglandin E2 expression, while \nosteocalcin or cell numbers went \nunchanged.  \n(Mcleod et al., \n1993)  Osteosarcoma \ncell line ROS \n17/2.8  0.0018 T at 30Hz for 12 or \n72 h  Exposure limited the normal increase in \ncell numbers, while enhanced ALP \nactivity. Effects were cell density \ndependent.  \n(Molen et al., \n2000)  Osteosarc oma \ncell line ROS \n17/2.8  0.0018 T at 30Hz for 120 h  Stimulation inhibited cell growth. ALP \nactivity was dependent on gap \njunctional coupling. Results suggest \ndifferential effects.  \nTable 4 - Different s timulation techniques based on  Blint et al., 2013 . ESC, embryonic stem cell; MSC, mesenchymal stem cell; \nROS, reactive oxygen species; ECM, extracellular matrix; TGF transforming growth factor; ALP, alkaline phosphatase; VEGF, \nvascular endothelial growth factor.  \n \n4.2. In-Vitro Stimulat ion System S pecification  \nFollowing the state -of-the-art review for electrical stimulation and considering the discussion on the  \nprevious sections, this section draws the specification of the in -vitro stimulation circuit. The circuit \narchitecture that will be adopted for the in -vitro stimulation unit will rely on the following \ncharacteristics:  \n Electrical stimulation with direct coupling.  \n Support current/charge stimulation and control.  \n Support multimode pulsed waveforms (periodical, bursts, and single shot).  \n Support high compliance voltages.  \n Support stimulation of multiple cell cultures.  \n Support electrical signal acquisition (cell’s voltage).  \n Based on commercial off the shelf c omponents.  \nTable 5 summarizes  the characteristics of the system , providing a quantitative description of the \nmain characteristics, concerning operation limits, resolutions and features.  \nCharacteristics  In-Vitro  In-Vivo  \nMode (current/voltage)  Current  Current  \nAmplitude range  1 -200 A 0.1 – 20 mA  \nDC value control  yes yes \nAmplitude resolution  0.5 A 50 A \nMaximum Compliance  25 V  5 V (tolerant)  \nStimulus shape  Square pulses  Square pulses  \nFrequency  1 Hz – 100 kHz 1 Hz – 100 kHz  \nDuty -cycle control  Yes Yes \nTiming mode  Periodic/burst/pulsed  Periodic/burst/pulsed  \nTime resolution  20 s 20 s \nNº Channels  32 4 \nAcquisition support  Voltage  No support  \nDesign approach  COTS components  ASIC  \nTable 5 - In-vitro and in -vivo system specification summary.  25 \n \n \nThis project has received funding from the European Union’s Horizon 2020 research and inn ovation programme under grant \nagreement No 829060.  \n4.3. In-Vivo Stimulation System Specification  \nFollowing the state of t he art review for electrical stimulation and considering the discussion on the \nprevious sections, this section draws the specification of the in -vivo stimulation circuit. The circuit \narchitecture that  will be adopted for the in -vivo stimulation unit will r ely on the following \ncharacteristics:  \n Electrical stimulation with direct coupling.  \n Support current/charge stimulation and control.  \n Support multimode pulsed waveforms (periodical, bursts, and single shot).  \n Based on an application specific integrated circuit  (ASIC).  \nTable 5 summarizes the characteristics of the system, providing a quantitative description of the \nmain characteristics, concerning operation limits, resolutions and features.  \n  26 \n \n \nThis project has received funding from the European Union’s Horizon 2020 research and inn ovation programme under grant \nagreement No 829060.  \n5. ELECTRODE ’S MATERIALS AND GEOMETRIES  \nSeveral lines of research are cu rrently directed towards the development of advanced electrodes that \ncan be reliably safe and sensitive. The amount and diversity of such electrodes seem almost unlimited \nsince different applications demand different electrode types with respect to their s ize, invasiveness, \nselectivity, materials and performance. Some important considerations are:  \n For every electrode there is an intrinsic charge injection capacity (CIC), restricting the voltage \nthat can be safely generated at the electrode’s surface. Beyond  this electrochemical limit, also \nknown as the water -window, irreversible electrolysis of water can result in tissue damage, \nelectrode dissolution, pH variations and production of unwanted species. (Aryan et al., 2015; \nHarris et al., 2018; Wellman et al., 2018)  \n To gener ate an effective electrical stimulation, it is crucial to select new electrode \ngeometries/sizes, materials and/or coatings in order to increase the charge transfer surface \narea such that the charge injection safety limits are preserved.  \n Biocompatible, corr osion -resistive materials for electrical recording and stimulation include \ngold, titanium, tungsten, platinum, iridium oxide, stainless steel, alloys of the mentioned \nmetals, highly doped semiconductors such as silicon, and conductive polymers. Lately, sev eral \nelectrodes have been improved by modification of the surface chemistry with carbon -based \ncoatings, as well as with biological ones. (Kim and Romero -Ortega, 2012) .  \n The current development towards smaller electrodes to improve spatial resolution present \nanother set of prob lems, as they must withstand a higher voltage to transport the same charge \nper area than a larger electrode in order to induce the same response. (Harris et al., 2018)  \nSmaller electrodes enhance the applied charge density, as they inevitably suffer from high \nelectrical impedance and consequent high thermal noise, leading to potential tissue damage \nand of the electrodes themselves, and thus deteriorating the signal -to-noise ratio. (Aryan et \nal., 2015; Boehler et al., 2015)  \nThe most well -established type of e lectrodes consists of an assembled array of metallic (gold, platinum, \nand titanium) microelectrodes (10 -200 µm in diameter), fabricated by complementary metal -oxide -\nsemiconductor (CMOS) technology, passivated (to work in a liquid environment) and connected  to an \nexternal measurement unit – the so -called MicroElectrode Array (MEA). MEAs have been studied for \ndecades and are typically fabricated on rigid substrates (e.g., quartz, silicon, sapphire, glass, etc.) and \ndifficult to combine with soft tissue, due t o their high mechanical mismatch. A specific type of MEAs \nthat have been used as stimulation electrodes, as well as for the recording of an electroneurogram in \nthe area of functional electrical stimulation are the multielectrode cuffs. Cuff electrodes have  an \nestablished story for long -term recording of neural activity – up to 63 weeks after implantation in \nhuman volunteers. (Fisher et al., 2009)  Cuff electrodes typically enclose s the nerve with flexible and \nself-sizing silicone or polyimide that avoid stretching or compression damage of the enclosed nerve. \nOn the other hand, when two individually addressable microelectrodes array strips are integrated onto \nthe same plane with an interdigitate approach, they are designated as planar InterDigitated Electrodes \n(IDE). (Ahadian et al., 2012; Lim et al., 2013; Liu et al., 2017)  This type of electrodes generates \nstimulation under low voltage and in a direct, effective, highly reproducible and controlled manner. \nIDEs are widely utilized in technological applications, par ticularly in the field of biological and chemical \nsensors due to their inexpensiveness, ease of fabrication process and high sensitivit y. MEA are part of 27 \n \n \nThis project has received funding from the European Union’s Horizon 2020 research and inn ovation programme under grant \nagreement No 829060.  \na group of peripheral nerve interfacing (PNI) electrodes. In fact, PNI can be both extraneural (e.g. C uff \nelectrode s) or intraneural (e.g. MEA). Intraneural electrodes are placed directly in the peripheral \nnerves and their proximity provides increased neural selectivity and sensitivity. According to the \nspecific mode of intraneural electrode placement with in the nerve tissue, the scientific community \nrecognizes four major electrode designs (Kim and Romero -Ortega, 2012) : \n Longitudinally Implanted Intrafascicular Electrodes (LIFE) – limitations include drift in the \npopulation of cells being recorded and a decrease in signal -to-noise ratio over time;  \n Transverse Intrafascicular Multi -chanel Electrodes (TIME) – which penetrates the peripheral \nnerve and is designed to selectively activate different fascicles within the nerve;  \n Coiled Wire Intrafascicular Electrodes (CWIE) – made from ny lon-coated stainless -steel  wire \nand is wound into helical flexible coils to allow for expansion and contraction within the \nsurrounding nerve tissue; and  \n Utah Slanted Electrode Array (USEA) – a version of MEA but made of needle electrodes of \nvarying lengths  (0.5 to 1.5 mm with 0.1 mm difference in length between rows of neighboring \nelectrodes) designed to access most fascicles within the nerve.  \nA specific subset of intraneural PNIs are regenerative electrodes, which are based on an alternative \ndesign to penetrating electrodes and are used for the spontaneously regrowth of peripheral nerves \nafter injury. In general, the s ensitivity of regenerative electrodes is superior to that of extraneural \nelectrodes. However, their implantation requires invasive surgery, which might be justified in most \namputee cases (Kim and Romero -Ortega, 2012) . There are three types of regenerative electrodes:  \n Sieve ele ctrodes – can be either ring or needle shaped placed between two ends of the cut \nnerve. Can originate nerve compression injury.;  \n Regenerative Multielectrode Interface (REMI) – placed between the transected ends of an \nend-to-end repaired nerve. Does not res trict the growth of the nerve, and thus the nerve \ncompression injury typically observed for Sieve electrodes does not occur.  \n Microchannel Roll Electrode (MCRE) – rolled into a 3D channel bundle and designed to fit the \ntransected peripheral nerve at both en ds of the roll.   \nFigure 15 - Different types of PNI according to their sensitivity and invasiveness. (Kim  and Romero -Ortega, 2012)  \n28 \n \n \nThis project has received funding from the European Union’s Horizon 2020 research and inn ovation programme under grant \nagreement No 829060.  \nAll the above mentioned PNI suffer from limitations, particularly in terms of safe reliability, as well as \nsensitivity. (Kim and Romero -Ortega, 2012)  distributed the different types of PNI according to their \nsensitivity and invasiveness (see Figure 15). For th e authors, sensitivity  is related to the electrode’s \nability to record action potentials, while invasiveness relates to the damage to the nerve while placing \nthe electrode.  \n \nThe purpose of this essay is to describe the use of different kinds of electrodes,  taking into \nconsideration a set of important features that may interfere with the cell stimulation when using \nelectrical fields, namely: 1) its composition, such as the material and surface treatment (metal, \ninorganic and organic coatings, and surface bio logical molecules); and 2) configuration, such as area, \nformat (MEA, interdigitated or spiral cuff) or shape (3D scaffold hybrids). A summary of the systems \npresented on the next lines of this documents (i.e., materials along with their electrical stimulat ion \nparadigm) used for electrical stimulation is then presented on Table 6. \n5.1. METAL E lectrodes  \nTypical platinum, gold and platinum -iridium are used for fabricating biomedical electrodes. (Dmitry \nKireev et al., 2016)  Platinum h as historically been considered the preferred metal used for electrodes \nin neuroprostheses. (Aregueta -Robles et al., 2014)  However, there are electrical, mechanical and \nbiological aspects of these electrodes that remain as limiting factors and prevent their application o n \nhigh -density microelectrode arrays for neural interfacing. In the literature, the limitations associated \nwith this type of electrodes’ performance have been addressed through several and varied \napproaches, which include passivation and surface texturing to reduce impedance and enhance tissue \nintegration.  \nFor instance, organic and inorganic coatings (including platinum -black, titanium nitride, and iridium \noxide) have been used to increase the charge injection capacity of metallic electrodes. (Aregueta -\nRobles et al., 2014 ) For instance, a commercial MEA based on titanium nitride (TiN) typically exhibits a \nvalue of 0.11 mC/cm2 (impedance in the range 30 -400 kΩ) (Gomes et al., 2019)  and platinum \nelectrodes allows 300 –350 mC/cm2 , while electrodes coated with iridium oxide (IrOx) have a capacity \nof 2–3 mC/cm2, being safer as it prevents tissue electrolytic damage. (Kim and Romero -Ortega, 2012)  \nHowever, electrode coatings like IrOx have additionally been reported to be brittle or prone to \ndelaminate, f urther restricting their use. (Aregueta -Robles et al., 2014; Thaning et al., 2010) . \n5.2. Platinum Electrodes  \nAlthough there is no electrical stimulation of cells reported, (Boehler et al., 2015)  described the use \nnanostructu red platinum grass coating, enabling superior impedance reduction for neural \nmicroelectrodes (with sizes between 5 and 500 µm). The coating was applied via a simple \nelectrochemical process on silicon wafer. The cell studies were performed using human neuro blastoma \ncell line SH -SY5Y and this sterile culture medium was exposed to the coating surfaces at three different \nconcentrations, 1.25 cm2/mL, 2 cm2/mL and 3.2 cm2/mL, for 24h at 37ºC. The stability of the coating \nwas also assessed during two weeks by appl ying a rectangular current pulse with 200 nC/phase at a \nfrequency of 200 Hz. Curiously, the values reported for the impedance (at 100 Hz) were found to be \nlower than IrOx and PEDOT:PSS (a conductive polymer), while the CIC was where the lowest \nimprovement was attained.  \n(Harris et al., 2018)  prepared a set of electrodes by mechanically polishing platinum w ith different sizes \n(2 mm, 0.6 mm, or 25 µm of diameter) and the charge density proved to be dependent on the charge \ninjection capacity and electrode area, ranging from 0.15 to 5.57 mC/cm2.  29 \n \n \nThis project has received funding from the European Union’s Horizon 2020 research and inn ovation programme under grant \nagreement No 829060.  \n(Ahadian et al., 2012)  applied a pulsed voltage stimulation on muscle tissue using a platinum electrode \nand observed a higher level of C2C 12 muscle tube alignment (>80%) and muscle transcription factors, \namong others benefits. The stimulation was performed through an IDE using electrical pulse of 6 V, 1 \nHz and duration of 10 ms for 1 day. The results demonstrated superior performance and mat uration \ncompared with a conventional setup (platinum wires in close proximity).  \n(Pfau et al., 2019)  studied the influence of standard clinical electri cal pulsing on flexible polyimide -\nbased thin -film platinum macro (338 and 444 µm) and micro (85 µm) electrodes for neuroprostheses, \neither sputter -deposited or evaporated. These electrodes were stressed with 120 million current \ncontrolled biphasic, cathodi c-first, charge balanced pulses with a charge density of 60 µm/cm2/ phase \nand a frequency of 500 Hz in 0.01 M PBS in vitro. Only sputtered electrodes with 88 µm showed the \ncathodic potential excursion starting about the 160 mV within the safe limit of wate r electrolysis.  \nOn an interesting work, (Rozman et al., 2014)  investigated the electrochemical performance of \nplatinum electrodes within a multi -electrode spiral cuff by testing it to selectively stimulate isolated \nporcine left cervical vag us nerve segment. The original cuff was simplified into a half -cuff and \ncontained a single row of nine electrodes (0.5 x 2 mm) at 2 mm from its inner surface. Charge injection \ncapacity of the electrode was around 75 µC/cm2 and the results showed that both the most negative \nand positive potentials across the electrode -electrolyte interface reached -0.54 and 0.59 V, \nrespectively, not exceeding the safe potential limits for water electrolysis.  \n5.3. Gold  Electrodes  \nAlthough without describing a procedure for el ectrical stimulation of cells, (Vafaiee et al., 2019)  \nreported a simple and cost -effective method for the fabrication of gold multielectrode arrays with high \nscratch strength for electrophysiologic al recordings. The authors described an enhancement on the \nadhesion of noble metals as gold and silver to glass substrates, as they typically are damaged while \nexposed to water -based medium.  \n(McCullen et al., 2010)  used a gold electrode with interdigitated shape to deliver stimulation under \nalternating (AC) electr ic field of 1 V/cm, 4h for 14 days, and the results showed an increase in cellular \nosteogenic differentiation potential of human adipose -derived stem cells (hASCs).  \n(Körbitzer et al., 2019)  prepared 0.31 mm2 electrodes with pure gold (along with graphene on gold \nand graphene/SiO2, which is presented latter on this document) to electrical stimulate cryo -conserved \nembryonic cortical rat neurons and found a charge injection cap acity of 0.03 C/cm2. Moreover, at \nfrequencies between 1 Hz and 1 MHz, the lowest impedance value found was 7 kΩ.  \n5.4. ORGANIC E lectrodes  \nIn particular, the use of carbon allotropes (like carbon nanotubes, graphene, etc.), and conductive \npolymers (like Pol ypyrrole, PEDOT, Polyaniline, etc.) have shown that tailored approaches can be used \nto create microelectrode arrays (MEA) which not only improve the electrode material properties, but \nalso provide biomolecules to aid in the establishment of a chronically s table neural interface. Organic \ncoatings hold a significant benefit over the inorganic coatings as they can be easily modified to include \nfunctional molecules to influence the biological behavior . (Aregueta -Robles et al., 2014) . \n5.4.1.  Conductive polymers -based electrod es \nElectrode coatings, particularly polymeric films which utilize conductive polymers, have been shown \nto bear a softer electrode interface and can be used to diminish or mediate the mechanical difference \nbetween a metal electrode and the tissue with which  it interfaces. Polypyrrole (PPy) along with \npoly(3,4 -ethylenedioxythiophene) (PEDOT) and polyaniline (PANI) are among the conjugated polymers 30 \n \n \nThis project has received funding from the European Union’s Horizon 2020 research and inn ovation programme under grant \nagreement No 829060.  \nwith higher conductivity that have been used as high -performance biomaterials for cell electrical \nstimulation.  \n5.4.2.  Polypyrrole  \nTo deliver effective electrical stimulation for reversing the neurite outgrowth induced by dysregulation \nof risk genes neuregulin -1 (NRG1) and disrupted in schizophrenia 1 (DISC1) loss -of-function in pre -\nfrontal cortical (PFC) neural cell s in mice, (Zhang et al., 2017)  used the conductive polymer polypyrrole \nPPy. The cells were stimulated at ± 0.25 mA/cm2 using a biphasic waveform of 100 µm pul ses with 20 \nµm interphase open circuit potential and a 3.78 ms short circuit (250 Hz). (Liu et al., 2018)  also \nprepared PPy planar electrodes to regulate cellular osteogenic differentiation trough an electrical \nstimulation of MC3T3 -E1 cells seeded at a density of 2x104 cel ls per cm2. In this work, the charge \ninjection quantity (Qinj) proved to be an important parameter, since when the cells were electrically \nstimulated for 1 hour per day, 0.08 –0.15 µQ had an obvious role in enhancing cellular osteogenic \ndifferentiation, whi le Qinj lows than 0.03 or higher than 0.30 µQ produced stimulations with no or \nnegative effects. In this case, the frequency of stimulation (applied voltages of 5 mV) also played an \nimportant role, as 1 or 25 Hz showed to enhance differentiation, whereas t he stimulation with 50 Hz \ngave an inhibiting effect.  Other relevant studies using PPy were presented by (Cui et al., 2001; Kim et \nal., 2004; Stewart et al., 2015)  \n5.4.3.  PEDOT  \nReNcellVM Neural Stem Cells (NSC) were differentiated by electri cal stimulation using a cross -linked \nPEDOT substrate for the first time by (Pires et al., 2015) . NSC were seeded (140  000 cells/cm2) in \nlaminin coated surfaces, cultured for 4 days and differ entiated over 8 additional days under 100 Hz \npulsed DC electrical stimulation, 1V with 10 ms pulses. In this work, the population of neurons (Tuj1) \nwas 1.6 times higher with longer neurites (73 vs. 108 µm) for cells cultured under electrical stimulus \nwith cultured NSC.  \n(Carli et al., 2019)  used fluorinated polymer Nafion as counter ion instead of PSS (polystyrene -\nsulfonate) to fabricate a electrodeposited PEDOT:Nafion composite for neural recording and \nstimulation. The PEDOT:Nafion coa ted electrodes provide a large signal -to-noise ratio in a murine \nanimal model (when compared to PEDOT:PSS) as result of a minimized polarization during electrical \nstimulation, thereby resulting in an improved charge injection limit equal to 4.4 mC/cm2. Thi s value is \nalmost 80% larger than the 2.5 mC/cm2 observed for PEDOT:PSS. The electrodes used in this study had \na 140 µm diameter, so a charge density of 0.5 mC/cm2 would correspond to a current amplitude of \n154 µA for a stimulating pulse of 500 µs.  \n(Schander et al., 2016)  reported an in vitro evaluation of the long -term stability of PEDOT -coated \ncircular gold microelectrodes (wit h approximately 560 µm of diameter, A = 0.25 mm2). The electrodes \nwere coated using a galvanostatic electropolymerization process. The continuous stimulation \nconsisted of a bipolar current amplitude of 5 mA, a pulse duration of 100 µs per phase, an interph ase \ngap (pulse pause) of 50 µs and a frequency of 1 kHz. This corresponded to 86.4 million bipolar current \npulses per day at a current density of 2A/cm2.  Other relevant studies using PPy were presented by \n(Richardson -Burn s et al., 2007; Wilks et al., 2009) . \n5.4.4.  PANI  \nSelf-doped sulfonated polyaniline (SPANI) -based interdigitated electrodes (IDEs) for controlled \nelectrical stimulation of human osteosarcoma (HOS) cells were studied by (Min et al., 2013) . The \nelectrode exhibited good cell attachment and proliferation under in vitro mild electrical stimulation. \nThe authors claimed that the condu ctivity of the copolymer increased with the degree of the 31 \n \n \nThis project has received funding from the European Union’s Horizon 2020 research and inn ovation programme under grant \nagreement No 829060.  \nsulfonation (DS) and described an increased number of HOS cells when DS increased from 0% to 50%. \nThe experimental results also unveiled that a further increase above this DS led to a marginal decre ase \nin both the conductivity of the SPAN and the HOS cell count, due to possible increased solubility of \nSPAN in aqueous medium. The electrical stimulation was performed varying both voltage and \nfrequency. When the applied voltage or the frequency was incr eased by keeping the other constant at \n800 mV or 1 kHz, an increased HOS cell count was found. (Min et al., 2013)  also decla red the existence \nof a threshold for the cell growth at an applied voltage (1200 mV) or frequency (200 kHz). Cell death \nwas observed by the authors above these threshold limits.  Other relevant studies using PANI were \npresented by (Thrivikraman et al., 2015, 2014; Wang et al., 2015)  \n5.5. Carbon -based electrodes  \nBetter integration of tissues and neural electrodes can be achieved by “humani zing” the material, i.e., \nmaking it structurally and chemically more similar to tissues surrounding neurons. Carbon -based \nelectrodes have remarkable mechanical and electrical properties that exhibit a notable interaction \nwith neural tissue. While there is number of methods that can be used to yield carbon coatings film \nprocessing in substrates for neural growth, not all of them are consensually accepted for stimulation \npurposes, including simple solvent evaporation, polyelectrolyte layering, chemical vapor  deposition \n(CVD) and electrochemical deposition.  \n5.5.1.  CNT \nSeveral studies (Aregueta -Robles et al., 2014)  on electrodes coated with carbon nanotubes (CNTs) \nreport a substantial increase in signal -to-noise ratio for recording electrodes predominantly due to the \nlow imp edance CNTs imparting to the electrodes. CNTs are tubes made of carbon with diameters \ntypically measured in nanometers and exhibit remarkable electrical conductivity, as well as exceptional \ntensile strength and thermal conductivity because of their nanostr ucture and strength of the bonds \nbetween carbon atoms. The safe charge injection limit of CNT -containing coatings has been reported \nto be between 1.6 and 2.5 mC/cm2. This is more than 10 times the charge injection reported for \nplatinum. However, (Burblies et al., 2016)  increased the thickness of a coating containing CNT and a \nCSC has been reported to be as high as 70 mC/cm2.  \nAlthough potential cytoxicity of CNT remains a controversial issue, (Mooney et al., 2012)  previously \ndemonstrated no adverse effects on mesenchymal stem cell (MSC) behavior  at low concentrations of \nboth single and multiwalled nanotubes. (Mooney  et al., 2012)  exposed MSC to a medium containing \nCNT and seeded (with a density of 20  000 cells/cm2) MSC on a CNT -based polylactic acid scaffolds, \nwhile electrically stimulating them in an electrophysiological bioreactor. The electrical stimulation was \ncarried out using a current of 0.15V/cm for 2 ms at a frequency of 1Hz for more than 10 days. The \nauthors observed that the majority of the cells exposed to the medium containing CNT or seeded on \nthe CNT -based scaffolds reorient perpendicularly to the elect rical current at an angle between 0º and \n10º and adopted an elongated morphology.  \n (Kam et al., 2009)  fabricated CNT thin films via a layer -by-layer method with alternating layers \nof single walled CNT (SWCNT) and the protein laminin to stimulate NSC cells. The conductivity of a 10 -\nbilayer SWCNT/laminin thin fil m was estimated to be 430 and 2140 S/m before and after heat \ntreatment, respectively. The NSC cell response appeared to be, independent of the type of stimulus \napplied, though the employed signals consisted of a series (~10 -15) of 1 ms pulses spaced in 1 -10 s \nintervals. The pulses were applied to allow both the electrodes and the cells to discharge. Cellular \nstimulations were carried out over areas of the films of about 4 cm2, and output currents from the film \nrange from the order of 1 -10 µA/cm2.  32 \n \n \nThis project has received funding from the European Union’s Horizon 2020 research and inn ovation programme under grant \nagreement No 829060.  \n(Lin et al., 2009)  were the first rese archers to grow CNTs electrode arrays (CNT -MEA) on a silicon \nsubstrate, transfer it onto a flexible and biocompatible polymeric film (Parylene -C), and then \nsuccessfully record the APs of crayfish nerve cords. The reported dimensions of the flexible CNTs pl anar \nelectrode were: 1) an opening area of the electrode site of 1962.5 µm2; and 2) a length of the \nconducting wire of 8100 µm. The measurements presented by the authors demonstrated the superior \nperformance of the flexible CNTs electrode with low impedanc e (11.07 kΩ @ 1 kHz) and high peak -to-\npeak amplitude action potential (about 410 V). In addition, the signal -to-noise ratio (SNR) of the \nflexible CNTs electrode is approximately 257, whereas the SNR of the reference (a pair of Teflon -coated \nsilver wires) i s only 79. The crayfish nerve cord was stimulated by a square pulse (amplitude of 7 V, and \npulse duration of 0.1 ms) every 5 ms during the test, and then the action potential was recorded by \nthe presented CNTs electrode and the reference electrode.  \n(Huang et al., 2012)  synthetized CNT by CVD and prepared it into a ro pelike structure with a diameter \nof 1 mm and length of 1.5 cm. This CNT rope substrate was then used to allow the electrical stimulation \nof differentiated neural stem cells (NSCs) in a culture medium and the in situ observation of the \nresponse of these ste m cells after stimulation. The authors declared that the use of an electrical \nstimulation (5mV, 0.5 mA, 25ms intermittent) promoted a neuronal maturity, while also increasing the \nspeed of neurite outgrowth. The intermittent stimulation instead of constant stimulation was \nperformed to protect cells from constant electrical stress.  \n Another flexible MEA, completely made from CNTs that have been embedded in a polymeric support, \nhas been presented by (David -Pur et al., 2014) . This device showed the same characteristics as planar \nCNTs -MEAs, combined with the advantages of being flexible and consisting on a continuous rough  \nsurface. Actually, this paper has a great summary on CNT -MEAS on rigid and flexible substrates. (Shein \net al., 2009)  developed CNT -MEAs with improved electrochemical properties by synthesizing CNT \n“islands” on SiO2, and confirmed the capability of the device for recording the spontaneous activity of \ncultured rat cortical n eurons. Focal electrical stimulations were applied to individual electrodes by \ndelivering voltage biphasic (positive -then -negative) pulses, 500 mV in amplitude, with each phase \nlasting 400 μs. A stimulation session was composed of 60 pulses separated by 20 s intervals.  \nAlthough without describing a procedure for electrical stimulation of cells, (Gabriel et al., 2013)  \nreported the preparation of CNT MEAs, by depositing CNTs by CVD on platinum to grow ganglion cells \nin rabbit retinas onto uncoated P 35 culture dishes.  \n5.5.2.  Graphene  \nGraphene has many promising features such as biocompatibility, intrinsic flexibility and excellent \nelectrical properties, allowing applications in the high -frequency regime. In some cases, the \ntransparency of graphene als o provides possibilities for the development of new tools for \noptogenetics. (D. Kireev et al., 2016; Kuzum et al., 2014; Park et al., 2014)  Researchers have only \nrecently begun to investigate graphene and its derivat ives as potential scaffolds for neuronal growth. \nGraphene has attracted a generalized interest due to its excellent conductive properties, but also \nbecause its nanostructure and its chemical stability render it a good candidate for favoring cell \nadhesion. (Monaco and Giugliano, 2014)  In vitro studies, carried out on human cell lines (i.e., HepG2, \nBEAS -2B, PC12, hMSCs), have demonstrated that the cyto - and genotoxicity of graphene depends on \nthe dose, shape, and size of the nanomaterial itself, as well as on the presence of metal contaminants, \noxides and/or residues of the g raphene oxidation (graphene oxide) preparation method in samples.  \nFor instance, the surface charges on graphene substrates has been investigated by (Tu et al., 2014) , \nwho c ultured primary rat hippocampal neurons on carboxylated GO (GO -COOH; negative surface 33 \n \n \nThis project has received funding from the European Union’s Horizon 2020 research and inn ovation programme under grant \nagreement No 829060.  \ncharge; control condition), the surface of which had been chemically modified by functionalization \nwith three functional groups: methoxy ( -OCH3; nearly neutral surface ch arge), amino (NH2; positively \ncharged surface) and poly -m-aminobenzene sulfonic acid ( -NH2/ -SO3H, PABS; zwitterionic). Although \nno relevant differences in morphology were observed, neurons cultured on a positively charged \nsurface showed a greater number of  neurites per neuron, a longer length of neurites and a greater \nnumber of branches per neurite.  \nThe use of graphene as an in vitro or an in vivo stimulator device was the primary focus of a study \nconducted by (Heo et al., 2011)  who developed a graphene/PET film to test the effect s of a non -contact \nfield stimulation on cell -to-cell coupling. This film was found to be biocompatible and improved SHSY5Y \nhuman neuroblastoma cell proliferation and viability compared to those observed in the control \ncultures. A transient non -contact elec tric field was produced by charge -balanced biphasic stimuli \nthrough the graphene/PET film electrodes and applied to cultured neural cells. We found that weak \nelectric field stimulation (pulse duration of 10 s) as low as 4.5 mV/mm for 32 min was particularl y \neffective in shaping cell -to-cell interaction.  \nThe differentiation into neurons of human neural stem cells (hNSCs) cultured on graphene has been \nstudied by (Park et al., 2014) . The results shared by the authors showed not only an improved \ndifferentiation but also an enhanced cell adhesion and neurites formation compared to control \nconditions. A series of voltage pulses (usually, 1 ≈ 10 of 500 mV monophasic/cathodic voltage puls es \nwith 1 ≈ 100 ms duration in a second) were applied to the differentiated cells via the graphene \nelectrodes.  \n(Li et al., 2013)  designed a three -dimensional (3D) graphene foam scaffold for neural stem cells (NSC). \nThis scaffold allowed the formation of a three -dimensional neural network, resulting in an excellent \nsubstra te for cell adhesion and proliferation by up -regulating Ki -67 protein expression. To test whether \nsuch a scaffold could be used as a neural stimulation electrode, its electrochemical properties were \ninvestigated by cyclic voltammetry. The results showed th at electrical stimulation via a capacitive \ncharge injection was enhanced (due to the larger specific surface area of the 3D scaffold) compared to \nthe use of conventional graphene film electrode.  \n(Dmitry Kireev et al., 2016)  prep ared a graphene microelectrode array (GMEA) with 20 µm diameter \non a flexible polyimide substrate. Electrical Impedance Spectroscopy using graphene as working \nelectrode and Ag/AgCl pellet as reference electrodes in PBS solution was studied. PBS was used as  \nelectrolyte to mimic physiological condition in vivo. A 10 mV alternating current (AC) potential was \napplied and a frequency range between 1 Hz and 1MHz was scanned in order to perform extracellular \nrecordings by measuring electrical activities from acute  heart tissue and cardiac muscle cells \n(cardiomyocyte -like cells, HL -1). The impedance of GMEA measured at 1kHz was around 1±0.5 x 105 \nΩ. The results showed good signal -to-noise ratios for both heart tissue and HL -1 cells. (Gomes et al., \n2019)  also fabricated a GMEA (composed of graphene microlectrodes with TiN tracks and contact \npads) and reported a mean value of charge injection capacity of 0.7 mC/cm2, which they claimed to \nbe a satisfactory value and highe r than the amplitudes obtained for MEAs reported in the literature. \nTo determine if the GMEA worked properly, the authors used Electrical Impedance Spectroscopy and \napplied a sinusoidal excitation between 10 and 50 mV of unit frequency (<1 to 105 Hz).  The  produced \nGMEA exhibited an average impedance at 1 kHz of 5.2 kΩ. (Gomes et al., 2019)  compared their results \nwith previous studies for graphene electrodes. For instance, they cited that the 0.31 mm2 electro des \nprepared by (Körbitzer et al., 2019)  with pure gold, grap hene on gold and graphene/SiO2, found a CIC \nof 0.03, 0.2 and 0.8 mC/cm2, respectively. Moreover, at frequencies between 1 Hz and 1 MHz, the 34 \n \n \nThis project has received funding from the European Union’s Horizon 2020 research and inn ovation programme under grant \nagreement No 829060.  \nlowest impedance values found were 7 kΩ, 4.4 kΩ, and 10 kΩ, for gold, graphene on gold, \ngraphene/SiO2, respectively.   \n(Du et al., 2015)  reported the impedance of MEAs with graphene and with gold electrodes (20 µm \ndiameter) tested at frequencies between 0.1 and 100  kHz, showing higher impedance values than the \nones observed by (Gomes et al., 2019)  and (Körbitzer et al., 2019) : 170 kΩ for the graphene electrode \nand 186 kΩ for the gold electrode.  \nIn a different work, (Koerbitzer et al., 2016)  prepared 3 different MEAs with an electrode size of about \n700 µm2. Gold, graphene on a thin gold layer, and plain graphene were fabricated on borosilicate glass \nsubstrates. Impedance studies at 1 kHz revealed a value of 2.3 MΩ and 0.88 MΩ for plain graphene \nand graphene on gold, respectively. Neural cell culture was performed using cryo -conserved \nembryonic cortical rat neurons . Neuronal recordings showed enough signal -to-noise ratio for both \nelectrodes and a stable impedance. Stimulation measurements yielded a CIC of 0.15 mC/cm2 using \nbiphasic pulses of 1 ms and 1 µA for transparent graphene electrodes. (Koerbitze r et al., 2016) , cited \nother authors, namely (Park et al., 2014) , (Kuzum et al., 2014) , and (Dmitry Kireev et al., 2016) , who \nalso studied different size graphene electrodes (from 310000 to 314 µm2), on different substrates \n(polimide, Parylene -C, quartz glass and borosilicate glass) and then with different impedances.  \n5.6. Hybrid electrodes  \nThe addition of car bon-based allotropes to conductive polymers results in electrodes with higher \ncharge storage capacity (CSC) and lower impedance. In stimulation electrodes, a low impedance and \ndecreased activation threshold have been achieved by several groups who have pre pared hybrid \nelectrodes.  \n5.6.1.  PEDOT –CNT \nTo overcome the problems related to spatial resolution and signal -to-noise ratio, (Gerwig et al., 2012)  \ndeveloped a PEDOT –CNT composite coating that combined the ionic and electronic conductivity of \npoly(3,4 -ethylenedioxythiophene) (PEDOT), a conductive polymer, with the hi gh mechanical stability \nof carbon nanotubes (CNT). This combination, employed as a coating l ayer on conventional MEAs, \nresulted in reduced impedance (<16 kΩ @ 1 kHz), and thus in improved performances not only when \ncompared with TiN (~50 kΩ) or Au (~330 kΩ) electrodes, but also compared to pure PEDOT electrodes \n(20 kΩ). The interpretation of this  phenomenon can most likely be found in the conductivity of the \nmeshwork of CNT, which enhances the electrical conductivity of the whole composite. Furthermore, \nPEDOT –CNT electrodes demonstrated excellent biocompatibility, allowing for the adhesion of prim ary \nchicken cardiomyocytes and the development of a two -dimensional syncytium, accompanied by good \nquality recording after six and ten days in vitro.  \n5.6.2.  Polypyrrole – graphene  \n(Yan et al., 2016)  reported the use of aligned nanofibers from polypyrrole -graphene (PPy -G) as \nelectrodes for r egeneration of optic nerve via electrical stimulation. The authors used graphene into \nthe conductive polymers scaffold structure to make it more conductive and robust. The electrode size \nwas 1 cm2 and the scan rate was 50 mV/s. The stimulation led to 137% improvement in cell length \nwith a significantly enhanced anti -aging effect for retinal ganglion cells. A PPy -G composite electrode \nwas built by (Zhu et al., 2019)  as an electrochemically controlled release system. T he results exhibited \nfavourable conductivity with a large capacitance and a stronger charge injection ability, which can be \nutilized as a drug carrier to verify the electro -responsive controlled release of fluorescein sodium. The \nEIS were conducted at the open circuit potential in a frequency range from 1 Hz to 100 kHz. To assess \nthe electric responsiveness behaviour, a two -electrode electrochemical cell was designed by using Pt 35 \n \n \nThis project has received funding from the European Union’s Horizon 2020 research and inn ovation programme under grant \nagreement No 829060.  \nelectrode as the counter electrode. The best conditions were found when a const ant stimulation of -\n1.5 V was applied.  \n5.6.3.  Polyaniline -graphene  \n(Zheng et al., 2019)  describe d a highly electroactive and biocompatible nanoelectrode made from a \nnovel polyaniline functionalized graphene (PANI -G) composite prepared by a facile and efficient \npolymerization -enhanced ball -milling method. The nanoelectrode was used for the regeneratio n of \nPC12 cells (seeded with a density of 0.25x103/mm2 and pre -cultured for 24h) via electrical stimulation \nand the results showed a 60% increase in axon length, with no adverse impact on the cell density and \nenhanced wound regeneration ability. The stimul ation was carried out with a forward 500 mV/cm and \na reverse -500 mV/cm for 1 or 3 hours each during 3, 5 and 7 days.  \n5.6.4.  Polycaprolactone fumarate -Polypyrrole  \n(Moroder et al., 2011)  studied the mechanical and electrical properties of pol ycaprolactone fumarate -\npolypyrrole (PCLF -PPy) scaffolds as potential conductive nerve conduits. P12 cells (PC12 is a cell line \nderived from a pheochromocytoma of the rat adrenal medulla) were electrically stimulated with 10 µA \ncurrent for 1 h/day, either u sing constant direct current or direct current at 20 Hz. The surfac e \nresistivity of the scaffolds was 2 kΩ and the scaffolds were electrically stable during the application of \nelectrical stimulation (ES). The presence of ES showed significant increases in the percentage of neurite \nbearing cells (67%), number of neurites p er cell (52%) and neurite length (41%).  \n5.6.5.  Polypyrrole -polydapamine  \nElectrically conductive polypyrrole -polydopamine (PPy -PDA) coatings were studied by (Kim et al., \n2018)  as high -performance biomaterials for cell stimulation in vitro and electrical signal recording in \nvivo. These coatings were prepared by electrochemical copolymerizaion with a panoply of thicknesses \n(and, consequently, an equal set of properties). They were applied o n a gold electrode as working \nelectrode on an electrochemical working station, that also possesses a platinum wire as a counter \nelectrode, and a saturated calomel reference electrode (SCE). As part of the characterization of \nelectrochemical properties, imp edance spectra were collected in a range of 0.0001 -10 kHz, applying \nan AC sinusoidal signal at 5 mV vs. SCE, in PBS solution. For electrical stimulation, 24 h after the seeding, \nPC12 cells were cultured for 24h and then stimulated with 100 mV for 2 h, foll owed by an additional \n24h culture period. bThe electrical properties of electrodes coated with PPy -PDA were superior to \nelectrodes coated with PPy alone. The growth and differentiation of C2C12 myoblasts and PC12 \nneuronal cells on PPy -PDA was enhanced comp ared to PPy. The PPy -PDA coated electrodes where also \nused on in vivo electromyography demonstrating more sensitive signals from mice tibia muscles \n(tibialis anterior) than bare gold or PPy -coated electrodes.  \nIn relation of hybrid electrodes, there are oth er interesting works published using polypyrrole hybrids \n(Cui et al., 2003, 2001; Schmidt et al., 1997b; Shi et al ., 2004) , polyaniline hybrids (Hsiao et al., 2013; \nY. Li et al., 2017; Mohammadi Amirabad et al., 2017; Petrov et al., 2016; Prabhakaran et al., 2011) , as \nwell as PEDOT -PEG (Ostrakhovitch et al., 2012) ,  \n                                                                                                                                                                                          Page 36 \n \n                                                        \nThis project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant agreem ent No 829060.  \n \nTable 6 - Summary of systems used for electrical stimulation  \nSystem  Electrical \nconductivity  Biocompatibility \nStudy  Electrical Stimulation Paradigm  Additional Notes  Ref. \nMetals  \nPlatinum  N.A.  human neuroblastoma \ncell line SH -SY5Y  N.A.  Neural microelectrodes (with sizes \nbetween 5 and 500 µm)  (Boehler et al., \n2015) \nPlatinum  N.A.  C2C12 muscle  pulsed of 6 V, 1 Hz and duration of 10 \nms for 1 day  IDE (Ahadian et al., \n2012)  \nPlatinum  N.A.  0.01 M PBS solution \nwas used  120 million current controlled biphasic, \ncathodic -first, charge balance d pulses \nwith a charge density of 60 µm/cm2/ \nphase and a frequency of 500 Hz  polyimide -based thin -film platinum \nmacro (338 and 444 µm) and micro \n(85 µm) electrodes for \nneuroprostheses, either sputter \ndeposited or evaporated  (Pfau et al., 2019)  \nPlatinum  N.A.  isolated porcine left \ncervical vagus nerve \nsegment  Charge injection capacity of the \nelectrode was around 75 µC/cm2 and \nthe results showed that both  the most \nnegative and positive potentials across \nthe electrode -electrolyte interface \nreached -0.54 and 0.59 V, respectively, \nnot exceeding the safe potential limits \nfor water electrolysis.  Half-cuff electrode and contain ing \nsingle row of nine electrodes ( 0.5 x 2 \nmm) at 2 mm from its inner surface  (Rozman et al., \n2014)  \nGold  N.A.  adipose -derived stem \ncells (hASCs)  AC electric field of 1 V/cm, 4h for 14 \ndays IDE (McCullen et al., \n2010)  \nGold  N.A.  cryo-conserved \nembryonic cortical rat \nneurons  CIC of 0.03. Between frequencies \nbetween 1 Hz and 1 MHz, the lowest \nimpedance value found  was 7 kΩ.  0.31 mm2 electrodes  (Koerbitzer et al., \n2016)  \nConductive Polymers                                                                                                                                                                                           Page 37 \n \n                                                        \nThis project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant agreem ent No 829060.  \n \nPPy N.A.  Dysregulation of risk \ngenes neuregulin -1 \n(NRG1) and disrupted \nin schizophrenia 1 \n(DISC1) loss -of-\nfunction in pre -frontal \ncortical neural cells \n(PFC) in mice  ± 0.25 mA/cm2 using a biphasic \nwaveform of 100 µm pulses with 20 µm \ninterphase open circuit potential and a \n3.78 ms short circuit (250 Hz)  N.A.  (Zhang et al., 2017)  \nPPy N.A.  MC3T3 -E1 cells seeded \nat a density of 2x104 \ncells per cm2 5 mV, 1 or 25 Hz  Planar electrodes  (Liu et al., 2018)  \nPPy(DBS)  N.A.  hNSCs (5x104 cell/cm2) ± 0.25 mA/cm2 using a biphasic \nwaveform of 100 µm pulses with 20 µm \ninterphase open circuit potential and a \n3.78 ms short circuit (250 Hz)  DBS as the anionic dopant. Grown \ngalvanostatiscally on gold -coated \nmylar  (Stewart et al ., \n2015)  \nPPy:PSS  N.A PBS solution was used  AC sinusoidal signal of 5 mV amplitude \nand the DC potential was set to 0 V. The \nvalue of impedance was determined \nover a range of 10 -100 000 Hz. The \nsmaller impedance of 120 kΩ was found \nfor 1 KHz.  Electrochemic al grown on hydrogel \nscaffolds deposited on the surface of \nmicrofabricated neural prosthetic \ndevices (gold electrodes)  (Kim et al., 2004)  \nPPy:PPS  N.A PBS solution was used  AC sinusoid with 5 mV of amplitude as \nthe inpu t signal with the DC potential \nset to 0 V. The value of impedance was \ndetermined at five discrete frequencies \nover the range of 10 to 105 Hz. The \nlowest impedance (<10 kΩ) was \nobserved at a thickness of ~13 µm.  Electrochemical grown on hydrogel \nscaffolds deposited on the surface of \nmicrofabricated neural prosthetic \ndevices (gold and iriudium sites with \n1250 and 3900 µm2 (Cui et al., 2001)  \nPEDOT  N.A.  ReNcellVM  NSC \n(140 000 cells /cm2)  100 Hz pulsed DC electrical stimulation, \n1 V with 10 ms pulses  N.A.  (Pires et al., 2015)                                                                                                                                                                                           Page 38 \n \n                                                        \nThis project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant agreem ent No 829060.  \n \nPEDOT  N.A.  SH-SY5Y \nneuroblastoma -derived \ncells and mouse primary \ndissociated cortical \ncultures (MCC)  AC current over the frequencies of 100 \n– 1000 Hz. PEDOT coatings showed a \ndecrease of the phase ange of the \nimpedance at frequencies > 100 Hz.  PEDOT, PEDOT+live neurons and \nneuron -templated PEDOT coatings on \nAu/Pd electrodes  (Richardson -Burns \net al., 2007)  \nPEDOT  N.A.  PBS solution was used.  5 mV sinewaves at 36 frequencies \nlogarithmically spaced from 1 Hz to 1 \nMHz  PEDOT as neural interface material \nfor microstimulation of small are a \n(177 µm2) iridium electrodes  (Wilks et al., 2009)  \nPEDOT:PSS  N.A.  PBS solution was used.  5 mA, a pulse duration of 100 µs per \nphase, an interphase gap (pulse pause) \nof 50 µs and a frequency of 1 kHz. 86.4 \nmillion bipolar current pulses per day at \na current density of 2A/cm2 PEDOT -coated circular \nmicroele ctrodes (with approximately \n560 µm of diameter, A = 0.25 mm2)  (Schander et al., \n2016)  \nPEDOT:PSS  N.A.  Neural Stem Cells \n(NSC) seeded at 2x107 \ncells/ml (on culture \ndishes)  Impedance of 5 -10 kΩ at 1kHz. \nDetection of spontaneous activity. 600 s \nrecording with a 1 ms bin width.  Modified MEA. Square array of 8x8 \nITO electrodes, each with 20 µm \nsquare and distance from the \nremaining wi th 70 µm.  (Furukawa et al., \n2013)  \nPEDOT:Nafion  N.A.  Rat central nervous \nsystem  charge density of 0.5 mC/cm2 would \ncorrespond to a current amplitude of 154 \nµA for a stimulating pulse of 500 µs  140 µm diameter  (Carli et al., 2019)  \nPANI  N.A.  Human osteosarcoma \n(HOS) cells  800 mV and 1 kHz  N.A.  (Min et al., 2013)  \nPANI  N.A. Cell viability, gene \nexpression analysis, \ncytotoxicity study and \ncalcium detection for \nbone marrow -derived \nhMSCs  Rectangular electrical pulses ( 7 ms, 3.6 \nmV/cm, 10 Hz) for 4 h followed by 4 h \nbreak for 28 days  Polymer films  (Thrivikraman et al., \n2015)  \nPANI  N.A.  Cell proliferation of \nPC12 cells. Dynamic \nprotein adsorption Rectangular electrical pu lses (100 µA, \n0.8 ms) repeating every 1 s for 1, 2 and \n4 h. Nanostructured PANI on ITO glass  (Wang et al., 2015)                                                                                                                                                                                           Page 39 \n \n                                                        \nThis project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant agreem ent No 829060.  \n \nanalysis using the SDS -\nPAGE technique  \nPANI  10-9 to 10 S/cm  hMSCs  1 mV – 2V DC field  Square -shaped films  (Thrivikraman et al., \n2014)  \nCarbon  \nCNT  N.A.  Mesenchymal stem cell \n(MSC) with a density of \n20 000 cells/cm2 0.15V/cm for 2 ms duration at a \nfrequency of 1Hz for more th an 10 days  N.A.  (Mooney et al., \n2012)  \nCNT  N.A.  Differentiated neural \nstem ce lls (NSCs)  5mV, 0.5 mA, 25ms intermittent  CNT synthetized by CVD and \nprepared it into a ropelike structure \nwith a diameter of 1 mm and length of \n1.5 cm  (Huang et al., 2012)  \nCNT  430 and 2140 \nS/m before and \nafter heat \ntreatment \nrespectively  Neural stem cells \n(NSCs)  Series (~10 -15) of 1 ms pulses spaced in \n1-10 s interv als. and output currents \nfrom the film range from the order of 1 -\n10 µA/cm2 CNT thin films fabricated by a layer -\nby-layer method with alternating \nlayers of single walled CNT \n(SWCNT) and the protein laminin. \nAreas of the films of about 4 cm2 (Kam et al., 2009)  \nCNT  N.A.  rat cortical neurons (700 \ncells/mm2) biphas ic (positive -then-negative) \npulses, 500 mV in amplitude, with \neach phase lasting 400 μs. A \nstimulation session was composed of \n60 pulses separated by 20 s intervals.  CNT MEAs. CNT grown by CVD on \nSiO 2 (Shein et al., 2009)  \nCNT  N.A.  crayfish nerve cords  square pulse (amplitude of 7 V, and \npulse duration of 0.1 ms) every 5 ms  CNT grown by CVD  on Parylene -C \nfilm (Lin et al., 2009)  \nCNT  N.A.  Chick retinas  charge -balanced bi -phasic (cathodic \nfirst) current stimulation (pulse width: 1 \nms and pulse amplitude: 1 –10 μA). Each \nstimulation session included \nstimulations at the entire intensity range CNTs w ere grown by CVD on \npolyimide and parylene C  (David -Pur et al., \n2014)                                                                                                                                                                                           Page 40 \n \n                                                        \nThis project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant agreem ent No 829060.  \n \n(increased by 1 μA every 10 s) and was \nrepeated five times.  \nCNT  N.A.  Ganglion cells in rabbit \nretinas (seeded at  a \ndensity of 2.5 x105 \ncells/ml onto uncoated \nP35 culture dishes)  N.A.  CNT MEAs. CNTs deposited by \nCVD on platinum  (Gabriel et al., 2013)  \nGraphene  N.A.  human neural stem cells \n(hNSCs) for brain repair \nand \nneural regeneration  1 ≈ 10 of 500 mV monophasic/cathodic \nvoltage pulses with 1 ≈ 100 ms duration \nin a second  Graphene grown by CVD and then \ntransferred to glass slides and coated \nwith laminin  (Park et al., 2014)  \nGraphene  N.A.  Neural stem cell  (seeded \nwith a desnsity of 5x104 \ncells/mL)  monophasic cathodic pulses were \napplied, and the stimulation threshold \nwas 20 –30 mA  3D graphene foam  synthesized by \nCVD method with Ni foam as templ at (Li et al., 2013)  \nGraphene  N.A.  Extracellular recordings \nby mea suring electrical \nactivities from acute \nheart tissue and cardiac \nmuscle cells \n(cardiomyocyte -like \ncells, HL -1) The impedance of GMEA measured at \n1kHz was around 1±0.5 x 105 Ω Graphene microelectrode array \n(GMEA) with 20 µm diameter on a \nflexible polyimide substrate  (Dmitry Kireev et \nal., 2016)  \nGraphene  N.A.  dorsal root  \nganglion neuron of \nWistar rats  Sinusoidal excitation between 10 and 50 \nmV of unit frequency (<1 to 105 Hz).  \nThe produced GMEA exhibited an \naverage impedance at 1 kHz of 5.2 kΩ.  0.31 mm2 electrodes . \nGMEA with TiN tracks and contact \npads.  (Gomes et al., 2019)  \nGraphene  N.A.  cryo-conserved \nembryonic cortical rat \nneurons  CIC of 0.2 mC/cm2. Between 1 Hz and \n1 MHz, the lo west impedance value \nfound was 4.4 kΩ.  0.31 mm2 electrodes.  \nGMEA with graphene on gold  (Koerbitzer et al., \n2016)  \nGraphene  N.A.  cryo-conserved \nembryonic cortical rat \nneurons  CIC of 0.8 mC/cm2. Between 1 Hz and \n1 MHz, the lowest impedance val ue \nfound was 10 kΩ.  0.31 mm2 electrodes.  \nGMEA with graphene/SiO 2 (Koerbitzer et al., \n2016)                                                                                                                                                                                           Page 41 \n \n                                                        \nThis project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant agreem ent No 829060.  \n \nGraphene  N.A.  rat cortical neuron  Impedance tested at frequencies \nbetween 0.1 and 100 kHz was 170 kΩ  20 µm diameter electrodes.  \nGMEA with graphene on gold  (Du et al., 2015)  \nGraphene  N.A.  Neural cell culture was \nperformed using cryo -\nconserved embryonic \ncortical rat neurons  Impedance studies at 1 kHz revealed a \nvalue of 0.88 MΩ . Graphene on a thin gold layer \nfabricated on borosilicate glass \nsubstrate. Electrode size 700 µm2. (Körbitzer et al., \n2019)  \nGraphene  N.A.  Neural cell culture was \nperforme d using cryo -\nconserved embryonic \ncortical rat neurons  CIC of 0.15 mC/cm2 using biphasic \npulses of 1 ms and 1 µA for transparent \ngraphene electrodes . Impedance studies \nat 1 kHz revealed a value of 2.3 MΩ. Graphene on a thin gold layer \nfabricated on borosili cate glass \nsubstrate. Electrode size 700 µm2. (Körbitzer et al ., \n2019)  \nGraphene  N.A.  SHSY5Y human \nneuroblastoma cells \n(3x105 cells in a 30 mm \nculture dish)  A transient non -contact electric field \nwas produced by charge -balanced \nbiphasic stimuli. Weak electric field \nstimulation (pulse duration of 10 s) as \nlow as 4.5 mV/mm for 32 min was \nparticularly effective in shaping cell -to-\ncell interaction  Graphene grown by CVD and \ntransferred to a PET film  (Heo et al., 2011)  \nHybrids  \nPPy-G N.A.  Regeneration of optic \nnerve via electrical \nstimulation of retinal \nganglion cells  scan rate was 50 mV/s  1 cm2 aligned nanofibers  (Yan et al., 201 6) \nPPy-G N.A.  Electro -responsive \ncontrolled release of \nfluorescein sodium  open circuit potential in a frequency \nrange from 1 Hz to 100 kHz . -1.5V  N.A.  (Zhu et al., 2019)  \nPPy-PDA  N.A.  Growth and \ndifferentiation of  C2C12 \nmyoblasts and PC12 \nneuronal cells  Impedance spectra were collected in a \nrange of 0.0001 -10 kHz, applying an \nAC sinusoidal signal at 5 mV.  Coatings applied on a gold electrode  (Kim  et al., 2018)                                                                                                                                                                                           Page 42 \n \n                                                        \nThis project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant agreem ent No 829060.  \n \nPPy-PLGA  N.A.  PC12 cells and primary \nchicken sciatic nerve \nexplants  Steady Potential of 100 mV for 2h.  Thin films grown on PLGA  (Schmidt et al., \n1997b)  \nPPy-PDLLA  Surface \nresistivity 1000 \nΩ/square (3% \nPPy)  Fibroblasts  100 mV DC applied voltage for 1000 h  Composite membrane (3.0 x 2.5 x \n0.03) cm3 in size (Shi et al., 2004)  \nPPy-\nbiomolecules \nblend  N.A.  Rat glacial and human \nneuroblastoma cells  AC sinusoid wit h 5 mV of amplitude as \nthe input signal with the DC potential \nset to 0 V. The value of impedance was \ndetermined at five discrete frequencies \nover the range of 10 to 105 Hz. PPy-biomolecules grown \ngalvanostatiscally on gold sites (1250 \nand 3900 µm2). The bi omolecules \nused were silk -like polymer having \nfibronectin fragments and \nnonapeptide (CDPGYIGSR)  (Cui et al., 2003, \n2001)  \nPPy-\nneurotrophins \n(NT-3 and \nBDNF)  N.A.  Cochlear neural explant   \nBiphasic ± 1 mA current puls es with \n100 µs pulse width, 20 µs open -circuit \ninterphase gap and 3.78 ms short -circuit \nphase between pulses at 250 Hz  The neurotrophin -doped PPy were  \ngalvanostatiscally deposited onto a \n1cm2 Au-Mylar electrodes  (Thompson et al., \n2010)  \nPEDOT -CNT  N.A.  primary chicken \nembryonic \ncardiomyocytes  iphasic anodic first voltage pulses with a \nfrequency of 1 kHz and amplitudes of \n200, 500, and 700 mV  MEA with 59 electrodes (30 µm \ndiameter)  (Gerwig et al., 2012)  \nPANI -G N.A.  Regeneration of PC12 \ncells (seeded with a \ndensity of 0.25x103/mm2 \nand pre -cultured for \n24h) 500 mV/cm and a reverse -500 mV/cm \nfor 1 or 3 hours each during 3, 5 and 7 \ndays nanoelectrode  (Zheng et al., 2019)  \nPANI -PLLA  3.0x10-9 S/cm  Adhesion and \nproliferation of rat nerve \nstem cells ( C17.2)  100 mV/mm for 60 minutes  Electrospun nanofibers  (Prabhakaran et al., \n2011)  \nPANI -PLG A N.A.  Cell viability and \nimmunofluorescence Electrical pulses of 1.25 Hz and 5 V/cm  Electrospun nanofibers  (Hsiao et al., 2013)                                                                                                                                                                                           Page 43 \n \n                                                        \nThis project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant agreem ent No 829060.  \n \nstudy using Rat neonatal \ncardiomyocytes  \nPANI -HEC  10 S/cm  Cell viability and actin \nfluorescent imaging of \nL929 mouse fibroblasts  2.5 V/cm and 2 mA for 24 h  nanocomposite  (Petrov et al. , 2016)  \nPANI -coated \nPCL  6.7x10-3 S/cm  Cell viability of \nHUVECs  200, 300, and 400 mV/cm, 30 min/day, \nfor 4 days  Fibber scaffold  (Y. Li et al., 2017)  \nPANI -PES N.A.  Cell viability, RT -PCR \nstudy, karyotyping study \nand \nimmunofluorescence \nstaining of CVD -iPSCs  Electrical pulses (1 Hz for 2 ms and 50 \nmV/cm) for 1 h/day for 15 days  nanof ibers  (Mohammadi \nAmirabad et al., \n2017)  \nPCLF -PPy surface \nresistivity of the \nscaffol ds was 2 \nkΩ P12 Cells  10 µA current for 1 h/day, either using \nconstant direct current or direct current \nat 20 Hz  Scaffolds as potential conductive \nnerve conduits  (Moroder et al., \n2011)  \nPEDOT -PEG  >10-4 S/cm2 NSCs and P19 \npluripotent embryona l \ncarcinoma cells (P19 \nEC) N.A.  Conductive films  (Ostrakhovitch et \nal., 2012)  \n \n \n                                                                                                                                                                                           \nPage 44 \n \n                                                        \nThis project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant \nagreement No 829060.  \n \n6. FINAL REMARKS  \nThis deliverable report presented a survey on nervou s tissues stimulation techniques. The major \nfocus was on electrical stimulation with direct coupling, since this will be the targeted approach \nfor the systems under development in project NeuroStimSpinal.  The survey presented relevant \nbackground informatio n regarding the impedance characteristics of nervous tissues and \nelectrochemical cells. Both present great relevance for the work under development, as they \nallow to take informed decisions on circuit design. Considering the stimulation system, the \nreporte d focused on the approaches referenced in the literature and derived a set of \ncharacteristics for in -vitro and in -vivo system development.  \n \n                                                                                                                                                                                           \nPage 45 \n \n                                                        \nThis project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant \nagreement No 829060.  \n \n7. REFERENCES  \nAdhikari, S., Sah, M., K im, H., Chua, L.O., 2013. Three Fingerprints of Memristor. IEEE Trans. \nCircuits Syst. Regul. Pap.  \nAhadian, S., Ramón -Azcón, J., Ostrovidov, S., Camci -Unal, G., Hosseini, V., Kaji, H., Ino, K., \nShiku, H., Khademhosseini, A., Matsue, T., 2012. Interdigitated  array of Pt electrodes \nfor electrical stimulation and engineering of aligned muscle tissue. Lab. Chip 12, 3491. \nhttps://doi.org/10.1039/c2lc40479f  \nAregueta -Robles, U.A., Woolley, A.J., Poole -Warren, L.A., Lovell, N.H., Green, R.A., 2014. \nOrganic electrode  coatings for next -generation neural interfaces. Front. \nNeuroengineering 7, 15. https://doi.org/10.3389/fneng.2014.00015  \nAryan, N.P., Kaim, H., Rothermel, A., 2015. Electrode Materials: State -of-the-Art and \nExperiments. pp. 45 –64. https://doi.org/10.1007/9 78-3-319-10052 -4_6 \nAu, H.T.H., Cheng, I., Chowdhury, M.F., Radisic, M., 2007. Interactive effects of surface \ntopography and pulsatile electrical field stimulation on orientation and elongation of \nfibroblasts and cardiomyocytes. Biomaterials 28, 4277 –4293. \nhttps://doi.org/10.1016/j.biomaterials.2007.06.001  \nBalint, R., Cassidy, N.J., Cartmell, S.H., 2013. Electrical Stimulation: A Novel Tool for Tissue \nEngineering. Tissue Eng. Part B Rev. 19, 48 –57. \nhttps://doi.org/10.1089/ten.teb.2012.0183  \nBard, A.J., Faulkn er, L.R., 2001. Electrochemical methods: fundamentals and applications, 2nd \ned. ed. Wiley, New York.  \nBodamyali, T., Bhatt, B., Hughes, F.J., Winrow, V.R., Kanczler, J.M., Simon, B., Abbott, J., Blake, \nD.R., Stevens, C.R., 1998. Pulsed Electromagnetic Field s Simultaneously Induce \nOsteogenesis and Upregulate Transcription of Bone Morphogenetic Proteins 2 and 4 in \nRat Osteoblastsin Vitro. Biochem. Biophys. Res. Commun. 250, 458 –461. \nhttps://doi.org/10.1006/bbrc.1998.9243  \nBoehler, C., Kleber, C., Martini, N., X ie, Y., Dryg, I., Stieglitz, T., Hofmann, U.G., Asplund, M., \n2017. Actively controlled release of Dexamethasone from neural microelectrodes in a \nchronic in vivo study. Biomaterials 129, 176 –187. \nhttps://doi.org/10.1016/j.biomaterials.2017.03.019  \nBoehler, C ., Stieglitz, T., Asplund, M., 2015. Nanostructured platinum grass enables superior \nimpedance reduction for neural microelectrodes. Biomaterials 67, 346 –353. \nhttps://doi.org/10.1016/J.BIOMATERIALS.2015.07.036  \nBozkurt, A., Lal, A., 2011. Low -cost flexible p rinted circuit technology based microelectrode \narray for extracellular stimulation of the invertebrate locomotory system. Sens. \nActuators Phys. 169, 89 –97. https://doi.org/10.1016/j.sna.2011.05.015  \nBrighton, C.T., Wang, W., Seldes, R., Zhang, G., Pollack, S.R., 2001. Signal Transduction in \nElectrically Stimulated Bone Cells: J. Bone Jt. Surg. -Am. Vol. 83, 1514 –1523. \nhttps://doi.org/10.2106/00004623 -200110000 -00009  \nBurblies, N., Schulze, J., Schwarz, H. -C., Kranz, K., Motz, D., Vogt, C., Lenarz, T., Warnecke , A., \nBehrens, P., 2016. Coatings of Different Carbon Nanotubes on Platinum Electrodes for \nNeuronal Devices: Preparation, Cytocompatibility and Interaction with Spiral Ganglion \nCells. PLOS ONE 11, e0158571. https://doi.org/10.1371/journal.pone.0158571  \nCarli, S., Bianchi, M., Zucchini, E., Di Lauro, M., Prato, M., Murgia, M., Fadiga, L., Biscarini, F., \n2019. Electrodeposited PEDOT:Nafion Composite for Neural Recording and \nStimulation. Adv. Healthc. Mater. 8, 1900765. \nhttps://doi.org/10.1002/adhm.201900765                                                                                                                                                                                           \nPage 46 \n \n                                                        \nThis project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant \nagreement No 829060.  \n \nChua, L., 2018. Five non -volatile memristor enigmas solved. Appl. Phys. A 124, 563. \nhttps://doi.org/10.1007/s00339 -018-1971 -0 \nChua, L., 2015. Everything You Wish to Know About Memristors But Are Afraid to Ask. \nRadioengineering 24, 319 –368. https://doi.org/10 .13164/re.2015.0319  \nChua, L., 2014. If it’s pinched it’s a memristor. Semicond. Sci. Technol. 29, 104001. \nhttps://doi.org/10.1088/0268 -1242/29/10/104001  \nChua, L., 1971. Memristor -The missing circuit element. IEEE Trans. Circuit Theory 18, 507 –519. \nhttps:// doi.org/10.1109/TCT.1971.1083337  \nChua, L., Sbitnev, V., Kim, H., 2012a. HODGKIN –HUXLEY AXON IS MADE OF MEMRISTORS. Int. J. \nBifurc. Chaos 22, 1230011. https://doi.org/10.1142/S021812741230011X  \nChua, L., Sbitnev, V., Kim, H., 2012b. NEURONS ARE POISED NEAR T HE EDGE OF CHAOS. Int. J. \nBifurc. Chaos 22, 1250098. https://doi.org/10.1142/S0218127412500988  \nChua, L.O., Kang, S.M., 1976. Memristive Devices and Systems. Proc. IEEE 64, 209 –223. \nhttps://doi.org/10.1109/PROC.1976.10092  \nChung, T. -W., Huang, C. -N., Chen, P .-C., Noda, T., Tokuda, T., Ohta, J., 2018. Fabrication of \nIridium Oxide/Platinum Composite Film on Titanium Substrate for High -Performance \nNeurostimulation Electrodes. Coatings 8, 420. \nhttps://doi.org/10.3390/coatings8120420  \nCisnal, A., Fraile, J. -C., Pér ez-Turiel, J., Muñoz -Martinez, V., Müller, C., R. Ihmig, F., 2018. A \nMeasurement Setup and Automated Calculation Method to Determine the Charge \nInjection Capacity of Implantable Microelectrodes. Sensors 18, 4152. \nhttps://doi.org/10.3390/s18124152  \nCong, P.,  2016. Neural Interfaces for Implantable Medical Devices: Circuit Design \nConsiderations for Sensing, Stimulation, and Safety. IEEE Solid -State Circuits Mag. 8, \n48–56. https://doi.org/10.1109/MSSC.2016.2573918  \nCui, X., Hetke, J.F., Wiler, J.A., Anderson, D. J., Martin, D.C., 2001. Electrochemical deposition \nand characterization of conducting polymer polypyrrole/PSS on multichannel neural \nprobes. Sens. Actuators Phys. 93, 8 –18. https://doi.org/10.1016/S0924 -\n4247(01)00637 -9 \nCui, X., Wiler, J., Dzaman, M., Altsc huler, R.A., Martin, D.C., 2003. In vivo studies of \npolypyrrole/peptide coated neural probes. Biomaterials 24, 777 –87. \nhttps://doi.org/10.1016/s0142 -9612(02)00415 -5 \nDagdeviren, C., Ramadi, K.B., Joe, P., Spencer, K., Schwerdt, H.N., Shimazu, H., Delcasso, S., \nAmemori, K., Nunez -Lopez, C., Graybiel, A.M., Cima, M.J., Langer, R., 2018. \nMiniaturized neural system for chronic, local intracerebral drug delivery. Sci. Transl. \nMed. 10, eaan2742. https://doi.org/10.1126/scitranslmed.aan2742  \nDavid -Pur, M., Bareket -Keren, L., Beit -Yaakov, G., Raz -Prag, D., Hanein, Y., 2014. All -carbon -\nnanotube flexible multi -electrode array for neuronal recording and stimulation. \nBiomed. Microdevices 16, 43 –53. https://doi.org/10.1007/s10544 -013-9804 -6 \nDu, X., Wu, L., Cheng, J., Huang , S., Cai, Q., Jin, Q., Zhao, J., 2015. Graphene microelectrode \narrays for neural activity detection. J. Biol. Phys. 41, 339 –47. \nhttps://doi.org/10.1007/s10867 -015-9382 -3 \nFisher, L.E., Tyler, D.J., Anderson, J.S., Triolo, R.J., 2009. Chronic stability and selectivity of four -\ncontact spiral nerve -cuff electrodes in stimulating the human femoral nerve. J. Neural \nEng. 6, 046010. https://doi.org/10.1088/1741 -2560/6/4/046010  \nFurukawa, Y., Shimada, A., Kato, K., Iwata, H., Torimitsu, K., 2013. Monitoring neural s tem cell \ndifferentiation using PEDOT –PSS based MEA. Biochim. Biophys. Acta BBA - Gen. Subj. \n1830, 4329 –4333. https://doi.org/10.1016/j.bbagen.2013.01.022                                                                                                                                                                                           \nPage 47 \n \n                                                        \nThis project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant \nagreement No 829060.  \n \nGabriel, G., Illa, X., Guimera, A., Rebollo, B., Hernandez -Ferrer, J., Martin -Fernandez, I., Teresa, \nM., Godignon, P., V., M., Vill, R., 2013. Carbon Nanotubes as Suitable Interface for \nImproving Neural Recordings, in: Physical and Chemical Properties of Carbon \nNanotubes. InTech. https://doi.org/10.5772/52174  \nGerwig, R., Fuchsberger, K., Schroeppel, B., Li nk, G.S., Heusel, G., Kraushaar, U., Schuhmann, \nW., Stett, A., Stelzle, M., 2012. PEDOT –CNT Composite Microelectrodes for Recording \nand Electrostimulation Applications: Fabrication, Morphology, and Electrical \nProperties. Front. Neuroengineering 5, 8. https ://doi.org/10.3389/fneng.2012.00008  \nGhassemlooy, Z., Alves, L.N., Zvánovec, S., Khalighi, M. -A. (Eds.), 2017. Visible Light \nCommunications: Theory and Applications, 1st ed. CRC Press. \nhttps://doi.org/10.1201/9781315367330  \nGomes, V.P., Pascon, A.M., Panepuc ci, R.R., Swart, J.W., 2019. Maskless production of neural -\nrecording graphene microelectrode arrays. J. Vac. Sci. Technol. B 37, 022202. \nhttps://doi.org/10.1116/1.5048216  \nGulrez, S.K.H., Al -Assaf, S., Phillips, G.O., 2011. Hydrogels: Methods of Preparation , \nCharacterisation and Applications, in: Carpi, A. (Ed.), Progress in Molecular and \nEnvironmental Bioengineering - From Analysis and Modeling to Technology \nApplications. InTech. https://doi.org/10.5772/24553  \nHan, M., McCreery, D.B., 2008. A new chronic neu ral probe with electroplated iridium oxide \nmicroelectrodes, in: 2008 30th Annual International Conference of the IEEE \nEngineering in Medicine and Biology Society. Presented at the 2008 30th Annual \nInternational Conference of the IEEE Engineering in Medicin e and Biology Society, IEEE, \nVancouver, BC, pp. 4220 –4221. https://doi.org/10.1109/IEMBS.2008.4650140  \nHarris, A.R., Newbold, C., Carter, P., Cowan, R., Wallace, G.G., 2018. Measuring the effective \narea and charge density of platinum electrodes for bionic d evices. J. Neural Eng. 15, \n046015. https://doi.org/10.1088/1741 -2552/aaba8b  \nHartig, M., Joos, U., Wiesmann, H. -P., 2000. Capacitively coupled electric fields accelerate \nproliferation of osteoblast -like primary cells and increase bone extracellular matrix \nformation in vitro. Eur. Biophys. J. 29, 499 –506. \nhttps://doi.org/10.1007/s002490000100  \nHeo, C., Yoo, J., Lee, S., Jo, A., Jung, S., Yoo, H., Lee, Y.H., Suh, M., 2011. The control of neural \ncell-to-cell interactions through non -contact electrical field stim ulation using graphene \nelectrodes. Biomaterials 32, 19 –27. \nhttps://doi.org/10.1016/J.BIOMATERIALS.2010.08.095  \nHodgkin, A.L., Huxley, A.F., 1952. A quantitative description of membrane current and its \napplication to conduction and excitation in nerve. J. Ph ysiol. 117, 500 –544.  \nHsiao, C. -W., Bai, M. -Y., Chang, Y., Chung, M. -F., Lee, T. -Y., Wu, C. -T., Maiti, B., Liao, Z. -X., Li, R. -\nK., Sung, H. -W., 2013. Electrical coupling of isolated cardiomyocyte clusters grown on \naligned conductive nanofibrous meshes for t heir synchronized beating. Biomaterials \n34, 1063 –1072. https://doi.org/10.1016/j.biomaterials.2012.10.065  \nHuang, Y. -J., Wu, H. -C., Tai, N. -H., Wang, T. -W., 2012. Carbon Nanotube Rope with Electrical \nStimulation Promotes the Differentiation and Maturity of Neural Stem Cells. Small 8, \n2869 –2877. https://doi.org/10.1002/smll.201200715  \nJingkuang Chen, Wise, K.D., Hetke, J.F., Bledsoe, S.C., 1997. A multichannel neural probe for \nselective chemical delivery at the cellular level. IEEE Trans. Biomed. Eng. 44, 760 –769. \nhttps://doi.org/10.1109/10.605435  \nKam, N.W.S., Jan, E., Kotov, N.A., 2009. Electrical Stimulation of Neural Stem Cells Mediated by \nHumanized Carbon Nanotube Composite Made with Extracellular Matrix Protein. Nano \nLett. 9, 273 –278. https://doi.org/10.10 21/nl802859a                                                                                                                                                                                           \nPage 48 \n \n                                                        \nThis project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant \nagreement No 829060.  \n \nKim, D. -H., Abidian, M., Martin, D.C., 2004. Conducting polymers grown in hydrogel scaffolds \ncoated on neural prosthetic devices. J. Biomed. Mater. Res. 71A, 577 –585. \nhttps://doi.org/10.1002/jbm.a.30124  \nKim, E.G.R., Tu, H., Luo, H., Liu, B., B ao, S., Zhang, J., Xu, Y., 2015. 3D silicon neural probe with \nintegrated optical fibers for optogenetic modulation. Lab. Chip 15, 2939 –2949. \nhttps://doi.org/10.1039/C4LC01472C  \nKim, I.S., Song, J.K., Song, Y.M., Cho, T.H., Lee, T.H., Lim, S.S., Kim, S.J., H wang, S.J., 2009. Novel \nEffect of Biphasic Electric Current on In Vitro  Osteogenesis and Cytokine Production in \nHuman Mesenchymal Stromal Cells. Tissue Eng. Part A 15, 2411 –2422. \nhttps://doi.org/10.1089/ten.tea.2008.0554  \nKim, I.S., Song, J.K., Song, Y.M., Lee, T.H., Cho, T.H., Lim, S.S., Pan, H., Kim, S.J., Hwang, S.J., \n2008. Novel action of biphasic electric current in vitro osteogenesis of human bone \nmarrow mesenchymal stromal cells coupled with VEGF production. Bone 43.  \nKim, I.S., Song, J.K., Zhang, Y.L. , Lee, T.H., Cho, T.H., Song, Y.M., Kim, D.K., Kim, S.J., Hwang, S.J., \n2006. Biphasic electric current stimulates proliferation and induces VEGF production in \nosteoblasts. Biochim. Biophys. Acta BBA - Mol. Cell Res. 1763, 907 –916. \nhttps://doi.org/10.1016/j .bbamcr.2006.06.007  \nKim, M.K., Jeon, H., Lee, H.J., Je, M., 2019. Plugging Electronics Into Minds: Recent Trends and \nAdvances in Neural Interface Microsystems. IEEE Solid -State Circuits Mag. 11, 29 –42. \nhttps://doi.org/10.1109/MSSC.2019.2939337  \nKim, S., Jan g, L.K., Jang, M., Lee, S., Hardy, J.G., Lee, J.Y., 2018. Electrically Conductive \nPolydopamine –Polypyrrole as High Performance Biomaterials for Cell Stimulation in \nVitro and Electrical Signal Recording in Vivo. ACS Appl. Mater. Interfaces 10, 33032 –\n33042. https://doi.org/10.1021/acsami.8b11546  \nKim, T. -i., McCall, J.G., Jung, Y.H., Huang, X., Siuda, E.R., Li, Y., Song, J., Song, Y.M., Pao, H.A., \nKim, R. -H., Lu, C., Lee, S.D., Song, I. -S., Shin, G., Al -Hasani, R., Kim, S., Tan, M.P., Huang, \nY., Omenetto, F.G ., Rogers, J.A., Bruchas, M.R., 2013. Injectable, Cellular -Scale \nOptoelectronics with Applications for Wireless Optogenetics. Science 340, 211 –216. \nhttps://doi.org/10.1126/science.1232437  \nKim, Y., Romero -Ortega, M.I., 2012. Material considerations for peri pheral nerve interfacing. \nMRS Bull. 37, 573 –580. https://doi.org/10.1557/mrs.2012.99  \nKireev, D., Sarik, D., Wu, T., Xie, X., Wolfrum, B., Offenhäusser, A., 2016. High throughput \ntransfer technique: Save your graphene. Carbon 107, 319 –324. \nhttps://doi.org/1 0.1016/J.CARBON.2016.05.058  \nKireev, Dmitry, Seyock, S., Ernst, M., Maybeck, V., Wolfrum, B., Offenhäusser, A., 2016. \nVersatile Flexible Graphene Multielectrode Arrays. Biosensors 7. \nhttps://doi.org/10.3390/bios7010001  \nKoerbitzer, B., Krauss, P., Nick, C., Yadav, S., Schneider, J.J., Thielemann, C., 2016. Graphene \nelectrodes for stimulation of neuronal cells. 2D Mater. 3, 024004. \nhttps://doi.org/10.1088/2053 -1583/3/2/024004  \nKörbitzer, B., Krauß, P., Belle, S., Schneider, J.J., Thielemann, C., 2019. Electroch emical \nCharacterization of Graphene Microelectrodes for Biological Applications. \nChemNanoMat 5, 427 –435. https://doi.org/10.1002/cnma.201800652  \nKuzum, D., Takano, H., Shim, E., Reed, J.C., Juul, H., Richardson, A.G., de Vries, J., Bink, H., \nDichter, M.A., Lucas, T.H., Coulter, D.A., Cubukcu, E., Litt, B., 2014. Transparent and \nflexible low noise graphene electrodes for simultaneous electrophysiology and \nneuroimaging. Nat. Commun. 5, 5259. https://doi.org/10.1038/ncomms6259                                                                                                                                                                                           \nPage 49 \n \n                                                        \nThis project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant \nagreement No 829060.  \n \nLi, N., Zhang, Q., Gao, S., Song, Q., Huang, R., Wang, L., Liu, L., Dai, J., Tang, M., Cheng, G., \n2013. Three -dimensional graphene foam as a biocompatible and conductive scaffold \nfor neural stem cells. Sci. Rep. 3, 1604. https://doi.org/10.1038/srep01604  \nLi, X., Zhong, S., Morizio, J., 201 7. 16 -Channel biphasic current -mode programmable charge \nbalanced neural stimulation. Biomed. Eng. OnLine 16, 104. \nhttps://doi.org/10.1186/s12938 -017-0385 -0 \nLi, Y., Li, X., Zhao, R., Wang, Chuying, Qiu, F., Sun, B., Ji, H., Qiu, J., Wang, Ce, 2017. Enhanced  \nadhesion and proliferation of human umbilical vein endothelial cells on conductive \nPANI -PCL fiber scaffold by electrical stimulation. Mater. Sci. Eng. C 72, 106 –112. \nhttps://doi.org/10.1016/J.MSEC.2016.11.052  \nLim, J. -H., McCullen, S.D., Piedrahita, J.A., Loboa, E.G., Olby, N.J., 2013. Alternating Current \nElectric Fields of Varying Frequencies: Effects on Proliferation and Differentiation of \nPorcine Neural Progenitor Cells. Cell. Reprogramming 15, 405 –412. \nhttps://doi.org/10.1089/cell.2013.0001  \nLin, C. -M., Lee, Y. -T., Yeh, S. -R., Fang, W., 2009. Flexible carbon nanotubes electrode for neural \nrecording. Biosens. Bioelectron. 24, 2791 –2797. \nhttps://doi.org/10.1016/J.BIOS.2009.02.005  \nLiu, Z., Dong, L., Cheng, K., Luo, Z., Weng, W., 2018. Charge injection based electrical \nstimulation on polypyrrole planar electrodes to regulate cellular osteogenic \ndifferentiation. RSC Adv. 8, 18470 –18479. https://doi.org/10.1039/C8RA02601G  \nLiu, Z., Dong, L., Wang, L., Wang, X., Cheng, K., Luo, Z., Weng, W., 2017. Mediation of cel lular \nosteogenic differentiation through daily stimulation time based on polypyrrole planar \nelectrodes. Sci. Rep. 7, 17926. https://doi.org/10.1038/s41598 -017-17120 -8 \nLohmann, C.H., Schwartz, Z., Liu, Y., Li, Z., Simon, B.J., Sylvia, V.L., Dean, D.D., Bone wald, L.F., \nDonahue, H.J., Boyan, B.D., 2003. Pulsed electromagnetic fields affect phenotype and \nconnexin 43 protein expression in MLO -Y4 osteocyte -like cells and ROS 17/2.8 \nosteoblast -like cells. J. Orthop. Res. 21, 326 –334. https://doi.org/10.1016/S0736 -\n0266(02)00137 -7 \nLu, Y., Lyu, H., Richardson, A.G., Lucas, T.H., Kuzum, D., 2016. Flexible Neural Electrode Array \nBased -on Porous Graphene for Cortical Microstimulation and Sensing. Sci. Rep. 6, \n33526. https://doi.org/10.1038/srep33526  \nMartins, P.A., 2019. Instrumentação para biossensor eletroquímicoutilizando voltametria \ncíclica e espetroscopia deimpedância eletroquímica. Universidade de Aveiro, Aveiro.  \nMcCullen, S.D., McQuilling, J.P., Grossfeld, R.M., Lubischer, J.L., Clarke, L.I., Loboa, E.G., 2010. \nAppl ication of low -frequency alternating current electric fields via interdigitated \nelectrodes: effects on cellular viability, cytoplasmic calcium, and osteogenic \ndifferentiation of human adipose -derived stem cells. Tissue Eng. Part C Methods 16, \n1377 –86. http s://doi.org/10.1089/ten.TEC.2009.0751  \nMcleod, K.J., Donahue, H.J., Levin, P.E., Fontaine, M. -A., Rubin, C.T., 1993. Electric fields \nmodulate bone cell function in a density -dependent manner. J. Bone Miner. Res. 8, \n977–984. https://doi.org/10.1002/jbmr.5650 080811  \nMin, Y., Yang, Y., Poojari, Y., Liu, Y., Wu, J. -C., Hansford, D.J., Epstein, A.J., 2013. Sulfonated \nPolyaniline -Based Organic Electrodes for Controlled Electrical Stimulation of Human \nOsteosarcoma Cells. Biomacromolecules 14, 1727 –1731. \nhttps://doi. org/10.1021/bm301221t  \nMohammadi Amirabad, L., Massumi, M., Shamsara, M., Shabani, I., Amari, A., Mossahebi \nMohammadi, M., Hosseinzadeh, S., Vakilian, S., Steinbach, S.K., Khorramizadeh, M.R., \nSoleimani, M., Barzin, J., 2017. Enhanced Cardiac Differentiatio n of Human \nCardiovascular Disease Patient -Specific Induced Pluripotent Stem Cells by Applying                                                                                                                                                                                          \nPage 50 \n \n                                                        \nThis project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant \nagreement No 829060.  \n \nUnidirectional Electrical Pulses Using Aligned Electroactive Nanofibrous Scaffolds. ACS \nAppl. Mater. Interfaces 9, 6849 –6864. https://doi.org/10.1021/acsami.6b152 71 \nMolen, M.A., Donahue, H.J., Rubin, C.T., McLeod, K.J., 2000. Osteoblastic networks with \ndeficient coupling: differential effects of magnetic and electric field exposure. Bone 27, \n227–231. https://doi.org/10.1016/S8756 -3282(00)00315 -X \nMonaco, A.M., Giugl iano, M., 2014. Carbon -based smart nanomaterials in biomedicine and \nneuroengineering. Beilstein J. Nanotechnol. 5196 5, 1849 –1863. \nhttps://doi.org/10.3762/BJNANO.5.196  \nMooney, E., Mackle, J.N., Blond, D.J. -P., O’Cearbhaill, E., Shaw, G., Blau, W.J., Barry,  F.P., \nBarron, V., Murphy, J.M., 2012. The electrical stimulation of carbon nanotubes to \nprovide a cardiomimetic cue to MSCs. Biomaterials 33, 6132 –6139. \nhttps://doi.org/10.1016/j.biomaterials.2012.05.032  \nMoroder, P., Runge, M.B., Wang, H., Ruesink, T., Lu , L., Spinner, R.J., Windebank, A.J., \nYaszemski, M.J., 2011. Material properties and electrical stimulation regimens of \npolycaprolactone fumarate -polypyrrole scaffolds as potential conductive nerve \nconduits. Acta Biomater. 7, 944 –53. https://doi.org/10.101 6/j.actbio.2010.10.013  \nNeeves, K.B., Lo, C.T., Foley, C.P., Saltzman, W.M., Olbricht, W.L., 2006. Fabrication and \ncharacterization of microfluidic probes for convection enhanced drug delivery. J. \nControlled Release 111, 252 –262. https://doi.org/10.1016/j.j conrel.2005.11.018  \nNimbalkar, S., Castagnola, E., Balasubramani, A., Scarpellini, A., Samejima, S., Khorasani, A., \nBoissenin, A., Thongpang, S., Moritz, C., Kassegne, S., 2018. Ultra -Capacitive Carbon \nNeural Probe Allows Simultaneous Long -Term Electrical S timulations and High -\nResolution Neurotransmitter Detection. Sci. Rep. 8, 6958. \nhttps://doi.org/10.1038/s41598 -018-25198 -x \nOlivieri, A.C., Faber, N.K.M., Ferré, J., Boqué, R., Kalivas, J.H., Mark, H., 2006. Uncertainty \nestimation and figures of merit for mu ltivariate calibration (IUPAC Technical, Report \n1998). Pure Appl Chem 78, 633 –661. https://doi.org/10.1351/pac200678030633  \nOstrakhovitch, E.A., Byers, J.C., O’Neil, K.D., Semenikhin, O.A., 2012. Directed differentiation of \nembryonic P19 cells and neural st em cells into neural lineage on conducting PEDOT –\nPEG and ITO glass substrates. Arch. Biochem. Biophys. 528, 21 –31. \nhttps://doi.org/10.1016/j.abb.2012.08.006  \nPark, D. -W., Schendel, A.A., Mikael, S., Brodnick, S.K., Richner, T.J., Ness, J.P., Hayat, M.R., At ry, \nF., Frye, S.T., Pashaie, R., Thongpang, S., Ma, Z., Williams, J.C., 2014. Graphene -based \ncarbon -layered electrode array technology for neural imaging and optogenetic \napplications. Nat. Commun. 5, 5258. https://doi.org/10.1038/ncomms6258  \nPark, S., Jang,  Y., Kim, H.C., Chun, K., 2008. Fabrication of drug delivery system with \npiezoelectric micropump for neural probe. Proc ITC -CSCC Int Tech Conf Circuits Syst. \nComput. Commun. 1149 –1152.  \nPetrov, P., Mokreva, P., Kostov, I., Uzunova, V., Tzoneva, R., 2016. No vel electrically conducting \n2-hydroxyethylcellulose/polyaniline nanocomposite cryogels: Synthesis and \napplication in tissue engineering. Carbohydr. Polym. 140, 349 –355. \nhttps://doi.org/10.1016/J.CARBPOL.2015.12.069  \nPfau, J., Ganatra, D., Weltin, A., Urban,  G., Kieninger, J., Stieglitz, T., 2019. Electrochemical \nStability of Thin -Film Platinum as Suitable Material for Neural Stimulation Electrodes, \nin: 2019 41st Annual International Conference of the IEEE Engineering in Medicine and \nBiology Society (EMBC). I EEE, pp. 3762 –3765. \nhttps://doi.org/10.1109/EMBC.2019.8856621  \nPires, F., Ferreira, Q., Rodrigues, C.A.V., Morgado, J., Ferreira, F.C., 2015. Neural stem cell \ndifferentiation by electrical stimulation using a cross -linked PEDOT substrate:                                                                                                                                                                                          \nPage 51 \n \n                                                        \nThis project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant \nagreement No 829060.  \n \nExpanding the use of biocompatible conjugated conductive polymers for neural tissue \nengineering. Biochim. Biophys. Acta BBA - Gen. Subj. 1850, 1158 –1168. \nhttps://doi.org/10.1016/j.bbagen.2015.01.020  \nPrabhakaran, M.P., Ghasemi -Mobarakeh, L., Jin, G., Ramakrishna, S., 2011. E lectrospun \nconducting polymer nanofibers and electrical stimulation of nerve stem cells. J. Biosci. \nBioeng. 112, 501 –507. https://doi.org/10.1016/j.jbiosc.2011.07.010  \nRadisic, M., Park, H., Shing, H., Consi, T., Schoen, F.J., Langer, R., Freed, L.E., Vunja k-Novakovic, \nG., 2004. Functional assembly of engineered myocardium by electrical stimulation of \ncardiac myocytes cultured on scaffolds. Proc. Natl. Acad. Sci. 101, 18129 –18134. \nhttps://doi.org/10.1073/pnas.0407817101  \nRetterer, S.T., Smith, K.L., Bjornsson , C.S., Turner, J.N., Isaacson, M.S., Shain, W., 2008. \nConstant pressure fluid infusion into rat neocortex from implantable microfluidic \ndevices. J. Neural Eng. 5, 385 –391. https://doi.org/10.1088/1741 -2560/5/4/003  \nRichardson -Burns, S.M., Hendricks, J.L., Foster, B., Povlich, L.K., Kim, D. -H., Martin, D.C., 2007. \nPolymerization of the conducting polymer poly(3,4 -ethylenedioxythiophene) (PEDOT) \naround living neural cells. Biomaterials 28, 1539 –1552. \nhttps://doi.org/10.1016/j.biomaterials.2006.11.026  \nRozman, J., Pečlin, P., Mehle, A., Šala, M., 2014. Electrochemical performance of platinum \nelectrodes within the multi -electrode spiral nerve cuff. Australas. Phys. Eng. Sci. Med. \n37, 525 –533. https://doi.org/10.1007/s13246 -014-0282 -9 \nRubehn, B., Wolff, S.B.E., To vote, P., Schuettler, M., Luthi, A., Stieglitz, T., 2011. Polymer -based \nshaft microelectrodes with optical and fluidic capabilities as a tool for optogenetics, in: \n2011 Annual International Conference of the IEEE Engineering in Medicine and Biology \nSociety . Presented at the 2011 33rd Annual International Conference of the IEEE \nEngineering in Medicine and Biology Society, IEEE, Boston, MA, pp. 2969 –2972. \nhttps://doi.org/10.1109/IEMBS.2011.6090815  \nSah, M.P., Hyongsuk Kim, Chua, L.O., 2014. Brains Are Made of Memristors. IEEE Circuits Syst. \nMag. 14, 12 –36. https://doi.org/10.1109/MCAS.2013.2296414  \nSchander, A., Tesmann, T., Strokov, S., Stemmann, H., Kreiter, A.K., Lang, W., 2016. In -vitro \nevaluation of the long -term stability of PEDOT:PSS coated microelectrode s for chronic \nrecording and electrical stimulation of neurons, in: 2016 38th Annual International \nConference of the IEEE Engineering in Medicine and Biology Society (EMBC). IEEE, pp. \n6174 –6177. https://doi.org/10.1109/EMBC.2016.7592138  \nSchmidt, C.E., Shast ri, V.R., Vacanti, J.P., Langer, R., 1997a. Stimulation of neurite outgrowth \nusing an electrically conducting polymer. Proc. Natl. Acad. Sci. 94, 8948 –8953. \nhttps://doi.org/10.1073/pnas.94.17.8948  \nSchmidt, C.E., Shastri, V.R., Vacanti, J.P., Langer, R., 19 97b. Stimulation of neurite outgrowth \nusing an electrically conducting polymer. Proc. Natl. Acad. Sci. 94, 8948 –8953. \nhttps://doi.org/10.1073/pnas.94.17.8948  \nScholz, F., 2015. Voltammetric techniques of analysis: the essentials. ChemTexts 1, 1 –24. \nhttps:// doi.org/10.1007/s40828 -015-0016 -y \nSchwaerzle, M., Paul, O., Ruther, P., 2017. Compact silicon -based optrode with integrated laser \ndiode chips, SU -8 waveguides and platinum electrodes for optogenetic applications. J. \nMicromechanics Microengineering 27, 0650 04. https://doi.org/10.1088/1361 -\n6439/aa6ad4  \nSerena, E., Figallo, E., Tandon, N., Cannizzaro, C., Gerecht, S., Elvassore, N., Vunjak -Novakovic, \nG., 2009. Electrical stimulation of human embryonic stem cells: Cardiac differentiation \nand the generation of re active oxygen species. Exp. Cell Res. 315, 3611 –3619. \nhttps://doi.org/10.1016/j.yexcr.2009.08.015                                                                                                                                                                                           \nPage 52 \n \n                                                        \nThis project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant \nagreement No 829060.  \n \nShah, S., Firlak, M., Berrow, S., Halcovitch, N., Baldock, S., Yousafzai, B., Hathout, R., Hardy, J., \n2018. Electrochemically Enhanced Drug Delivery Using Pol ypyrrole Films. Materials 11, \n1123. https://doi.org/10.3390/ma11071123  \nShein, M., Greenbaum, A., Gabay, T., Sorkin, R., David -Pur, M., Ben -Jacob, E., Hanein, Y., 2009. \nEngineered neuronal circuits shaped and interfaced with carbon nanotube \nmicroelectrode a rrays. Biomed. Microdevices 11, 495 –501. \nhttps://doi.org/10.1007/s10544 -008-9255 -7 \nShi, G., Rouabhia, M., Wang, Z., Dao, L.H., Zhang, Z., 2004. A novel electrically conductive and \nbiodegradable composite made of polypyrrole nanoparticles and polylactide. \nBiomaterials 25, 2477 –2488. https://doi.org/10.1016/J.BIOMATERIALS.2003.09.032  \nShi, G., Zhang, Z., Rouabhia, M., 2008. The regulation of cell functions electrically using \nbiodegradable polypyrrole –polylactide conductors. Biomaterials 29, 3792 –3798. \nhttps:// doi.org/10.1016/j.biomaterials.2008.06.010  \nShin, H., Lee, H.J., Chae, U., Kim, H., Kim, J., Choi, N., Woo, J., Cho, Y., Lee, C.J., Yoon, E. -S., Cho, \nI.-J., 2015. Neural probes with multi -drug delivery capability. Lab. Chip 15, 3730 –3737. \nhttps://doi.org/10 .1039/C5LC00582E  \nSon, Y., Jenny Lee, H., Kim, J., Shin, H., Choi, N., Justin Lee, C., Yoon, E. -S., Yoon, E., Wise, K.D., \nGeun Kim, T., Cho, I. -J., 2015. In vivo optical modulation of neural signals using \nmonolithically integrated two -dimensional neural pro be arrays. Sci. Rep. 5, 15466. \nhttps://doi.org/10.1038/srep15466  \nStewart, E., Kobayashi, N.R., Higgins, M.J., Quigley, A.F., Jamali, S., Moulton, S.E., Kapsa, R.M.I., \nWallace, G.G., Crook, J.M., 2015. Electrical stimulation using conductive polymer \npolypyr role promotes differentiation of human neural stem cells: a biocompatible \nplatform for translational neural tissue engineering. Tissue Eng. Part C Methods 21, \n385–93. https://doi.org/10.1089/ten.TEC.2014.0338  \nStrukov, D.B., Snider, G.S., Stewart, D.R., Wil liams, R.S., 2008. The missing memristor found. \nNature 453, 80 –83. https://doi.org/10.1038/nature06932  \nSun, S., Titushkin, I., Cho, M., 2006. Regulation of mesenchymal stem cell adhesion and \norientation in 3D collagen scaffold by electrical stimulus. Bioel ectrochemistry 69, 133 –\n141. https://doi.org/10.1016/j.bioelechem.2005.11.007  \nThaning, E.M., Asplund, M.L.M., Nyberg, T.A., Inganäs, O.W., von Holst, H., 2010. Stability of \npoly(3,4 -ethylene dioxythiophene) materials intended for implants. J. Biomed. Mater.  \nRes. B Appl. Biomater. 93B, 407 –415. https://doi.org/10.1002/jbm.b.31597  \nThompson, B.C., Richardson, R.T., Moulton, S.E., Evans, A.J., O’Leary, S., Clark, G.M., Wallace, \nG.G., 2010. Conducting polymers, dual neurotrophins and pulsed electrical stimulation  \n— Dramatic effects on neurite outgrowth. J. Controlled Release 141, 161 –167. \nhttps://doi.org/10.1016/j.jconrel.2009.09.016  \nThrivikraman, G., Lee, P.S., Hess, R., Haenchen, V., Basu, B., Scharnweber, D., 2015. Interplay of \nSubstrate Conductivity, Cellular Microenvironment, and Pulsatile Electrical Stimulation \ntoward Osteogenesis of Human Mesenchymal Stem Cells in Vitro. ACS Appl. Mater. \nInterfaces 7, 23015 –23028. https://doi.org/10.1021/acsami.5b06390  \nThrivikraman, G., Madras, G., Basu, B., 2014. Intermitte nt electrical stimuli for guidance of \nhuman mesenchymal stem cell lineage commitment towards neural -like cells on \nelectroconductive substrates. Biomaterials 35, 6219 –6235. \nhttps://doi.org/10.1016/j.biomaterials.2014.04.018  \nTu, Q., Pang, L., Chen, Y., Zhang , Y., Zhang, R., Lu, B., Wang, J., 2014. Effects of surface charges \nof graphene oxide on neuronal outgrowth and branching. The Analyst 139, 105 –115. \nhttps://doi.org/10.1039/C3AN01796F                                                                                                                                                                                           \nPage 53 \n \n                                                        \nThis project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant \nagreement No 829060.  \n \nVafaiee, M., Vossoughi, M., Mohammadpour, R., Sasanpour, P., 2019. Gold -Plated Electrode \nwith High Scratch Strength for Electrophysiological Recordings. Sci. Rep. 9, 2985. \nhttps://doi.org/10.1038/s41598 -019-39138 -w \nVitale, F., Summerson, S.R., Aazhang, B., Kemere, C., Pasquali, M., 2015. Neural Stimulation \nand Recording with B idirectional, Soft Carbon Nanotube Fiber Microelectrodes. ACS \nNano 9, 4465 –4474. https://doi.org/10.1021/acsnano.5b01060  \nWang, L., Huang, Q., Wang, J. -Y., 2015. Nanostructured Polyaniline Coating on ITO Glass \nPromotes the Neurite Outgrowth of PC 12 Cells b y Electrical Stimulation. Langmuir 31, \n12315 –12322. https://doi.org/10.1021/acs.langmuir.5b00992  \nWeiland, J.D., Anderson, D.J., Humayun, M.S., 2002. In vitro electrical properties for iridium \noxide versus titanium nitride stimulating electrodes. IEEE Trans. Biomed. Eng. 49, \n1574 –1579. https://doi.org/10.1109/TBME.2002.805487  \nWellman, S.M., Eles, J.R.,  Ludwig, K.A., Seymour, J.P., Michelson, N.J., McFadden, W.E., \nVazquez, A.L., Kozai, T.D.Y., 2018. A Materials Roadmap to Functional Neural Interface \nDesign. Adv. Funct. Mater. 28, 1701269. https://doi.org/10.1002/adfm.201701269  \nWilks, S.J., Richardson -Burns, S.M., Hendricks, J.L., Martin, D.C., Otto, K.J., 2009. Poly(3,4 -\nethylenedioxythiophene) as a Micro -Neural Interface Material for Electrostimulation. \nFront. Neuroengineering 2, 7. https://doi.org/10.3389/neuro.16.007.2009  \nWu, F., Stark, E., Im, M., Cho,  I.-J., Yoon, E. -S., Buzsáki, G., Wise, K.D., Yoon, E., 2013. An \nimplantable neural probe with monolithically integrated dielectric waveguide and \nrecording electrodes for optogenetics applications. J. Neural Eng. 10, 056012. \nhttps://doi.org/10.1088/1741 -2560/10/5/056012  \nWu, F., Stark, E., Ku, P. -C., Wise, K.D., Buzsáki, G., Yoon, E., 2015. Monolithically Integrated \nμLEDs on Silicon Neural Probes for High -Resolution Optogenetic Studies in Behaving \nAnimals. Neuron 88, 1136 –1148. https://doi.org/10.1016/j.neur on.2015.10.032  \nYan, L., Zhao, B., Liu, X., Li, X., Zeng, C., Shi, H., Xu, X., Lin, T., Dai, L., Liu, Y., 2016. Aligned \nNanofibers from Polypyrrole/Graphene as Electrodes for Regeneration of Optic Nerve \nvia Electrical Stimulation. ACS Appl. Mater. Interface s 8, 6834 –6840. \nhttps://doi.org/10.1021/acsami.5b12843  \nYang, W., Broski, A., Wu, J., Fan, Q.H., Li, W., 2018. Characteristics of Transparent, PEDOT:PSS -\nCoated Indium -Tin-Oxide (ITO) Microelectrodes. IEEE Trans. Nanotechnol. 17, 701 –\n704. https://doi.org/10. 1109/TNANO.2017.2785627  \nZhang, Q., Esrafilzadeh, D., Crook, J.M., Kapsa, R., Stewart, E.M., Tomaskovic -Crook, E., \nWallace, G.G., Huang, X. -F., 2017. Electrical Stimulation Using Conductive Polymer \nPolypyrrole Counters Reduced Neurite Outgrowth of Primary P refrontal Cortical \nNeurons from NRG1 -KO and DISC1 -LI Mice. Sci. Rep. 7, 42525. \nhttps://doi.org/10.1038/srep42525  \nZheng, Z., Huang, L., Yan, L., Yuan, F., Wang, L., Wang, K., Lawson, T., Lin, M., Liu, Y., 2019. \nPolyaniline Functionalized Graphene Nanoelectr odes for the Regeneration of PC12 \nCells via Electrical Stimulation. Int. J. Mol. Sci. 20, 2013. \nhttps://doi.org/10.3390/ijms20082013  \nZhu, M., Hao, Y., Ma, X., Feng, L., Zhai, Y., Ding, Y., Cheng, G., 2019. Construction of a \ngraphene/polypyrrole composite e lectrode as an electrochemically controlled release \nsystem. RSC Adv. 9, 12667 –12674. https://doi.org/10.1039/C9RA00800D  \nZhuang, H., Wang, W., Seldes, R.M., Tahernia, A.D., Fan, H., Brighton, C.T., 1997. Electrical \nStimulation Induces the Level of TGF -b1 mR NA in Osteoblastic Cells by a Mechanism \nInvolving Calcium/Calmodulin Pathway. Biochem. Biophys. Res. Commun. 237, 5."
    },
    {
        "title": "Supervised and Unsupervised Web Document Filtering Techniques to Improve Text-Based Music Retrieval.",
        "author": [
            "Peter Knees",
            "Markus Schedl",
            "Tim Pohle",
            "Klaus Seyerlehner",
            "Gerhard Widmer"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1417173",
        "url": "https://doi.org/10.5281/zenodo.1417173",
        "ee": "https://zenodo.org/records/1417173/files/KneesSPSW10.pdf",
        "abstract": "We aim at improving a text-based music search engine by applying different techniques to exclude misleading infor- mation from the indexing process. The idea of the original approach is to index music pieces by “contextual” informa- tion, more precisely, by all texts to be found on Web pages retrieved via a common Web search engine. This represen- tation allows for issuing arbitrary textual queries to retrieve relevant music pieces. The goal of this work is to improve precision of the retrieved set of music pieces by filtering out Web pages that lead to irrelevant tracks. To this end we present two unsupervised and two supervised filtering ap- proaches. Evaluation is carried out on two collections pre- viously used in the literature. The obtained results suggest that the proposed filtering techniques can improve results significantly but are only effective when applied to large and diverse music collections with millions of Web pages associated.",
        "zenodo_id": 1417173,
        "dblp_key": "conf/ismir/KneesSPSW10",
        "keywords": [
            "text-based music search engine",
            "misleading information",
            "Web pages",
            "Web search engine",
            "contextual information",
            "Web retrieval",
            "arbitrary textual queries",
            "relevant music pieces",
            "filtered out",
            "supervised filtering approaches"
        ],
        "content": "SUPERVISED AND UNSUPERVISED WEB DOCUMENT FILTERING\nTECHNIQ\nUES TO IMPROVE TEXT-BASED MUSIC RETRIEV AL\nPeter Knees, Markus Schedl, Tim Pohle, Klaus Seyerlehner, and Gerhard Widmer\nDepartment of Computational Perception\nJohannes Kepler University Linz, Austria\npeter.knees@jku.at\nABSTRACT\nWe aim at improving a text-based music search engine by\napplying different techniques to exclude misleading infor-\nmation from the indexing process. The idea of the original\napproach is to index music pieces by “contextual” informa-\ntion, more precisely, by all texts to be found on Web pages\nretrieved via a common Web search engine. This represen-\ntation allows for issuing arbitrary textual queries to retrieve\nrelevant music pieces. The goal of this work is to improve\nprecision of the retrieved set of music pieces by ﬁltering\nout Web pages that lead to irrelevant tracks. To this end we\npresent two unsupervised and two supervised ﬁltering ap-\nproaches. Evaluation is carried out on two collections pre-\nviously used in the literature. The obtained results suggest\nthat the proposed ﬁltering techniques can improve results\nsigniﬁcantly but are only effective when applied to large\nand diverse music collections with millions of Web pages\nassociated.\n1. MOTIV ATION AND CONTEXT\nSearching for music by issuing “semantic” and descriptive\nqueries has become a hot research topic recently [2, 4, 8,\n11–13, 15]. While typical query-by-example systems re-\nquire the user to have a speciﬁc piece of music at hand (or\nat least in mind) when searching for other music, query-\nby-description systems allow for typing in a short charac-\nterisation or a related term to ﬁnd desired music. More-\nover, it is generally desirable to build systems that are ca-\npable of linking music to meaningful textual descriptions\n(i.e., bridging what is often misleadingly called “seman-\ntic gap” [17]). For instance, this capability can be used to\nrecommend music based on other textually represented in-\nformation, e.g., by analysing the user’s currently viewed\nWeb page [7].\nFor the dedicated task of building a music search en-\ngine, several approaches have been presented. In [4], Bau-\nmann et al. describe a system incorporating various kinds\nof meta-data, lyrics, and acoustic properties. To analyse\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2010 International Society for Music Information Retrieval.queries, natural language processing methods and knowl-\nedge from a semantic ontology are applied to map the query\ntokens to the corresponding concepts. In [8], Celma et al.\npropose usage of a Web crawler focused on audio blogs to\nobtain textual descriptions for music. Blog entries are ex-\ntracted and the associated music pieces are indexed based\non this information. From a text-based retrieval result,\nalso acoustically similar songs can be discovered. Yang\net al. [18] index a music collection using lyrics and apply a\ncombination of text and audio descriptors to cluster results.\nIn [15,16], Turnbull et al. present a method for semantic\nretrieval. Based on the CAL500 data set – a collection of\n500 songs manually labelled with descriptions represent-\ning music-relevant properties – models of these properties\nare learned from audio features. The system can then be\nused to retrieve relevant songs based on queries consist-\ning of the words used for annotation. In [2], this approach\nis extended by incorporating multiple sources of features\n(i.e., acoustic features related to timbre and harmony, so-\ncial tags, and Web documents). These largely complemen-\ntary sources are combined to improve prediction accuracy.\nIn [13], we propose an unsupervised strategy for music\nretrieval that is capable of dealing with a large and arbitrary\nvocabulary. Contrary to learning a pre-deﬁned set of labels\n(cf. [2,15]), music pieces are represented in a vector space\nconstructed from related Web documents. An improved\nversion of this approach is presented in [11]. Instead of\naggregating Web pages to construct term vectors, the re-\ntrieved Web documents are stored in an index. A given\nquery is processed by passing the query to this index and\napplying a technique called rank-based relevance scoring\nto the resulting document ranking. This scoring is based on\nthe associations between music tracks and Web documents\n(as we further extend this approach in this paper, a more\ndetailed description can be found in Section 2). In [12],\nwe propose unsupervised methods to improve search re-\nsults by integrating audio similarity. Results show that the\ncombinations can raise performance slightly but mainly in-\ntroduce noise.\nWith this work, we aim at enhancing the approach from\n[11] by constructing ﬁlters that remove misleading infor-\nmation from the Web document index and raise precision\nof the retrieved music piece rankings (cf. [3]). Two of\nthese ﬁlters are built in an unsupervised manner, whereas\nthe other two make use of external annotations for learning\nto distinguish between informative and noisy content.\n543\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)2. WEB-BASED MUSIC INDEXING\nIn\nthe approach from [11, 12], music is indexed by using\n“contextual” meta-information about the pieces under con-\nsideration. This context-data is assumed to be found on\nrelated Web pages retrieved via Google by issuing three\nqueries for each music piece m. Constructed from the\nmeta-information categories artist name ,album name , and\ntrack title , these queries are “artist” music, “artist” “al-\nbum” music review -lyrics, and “artist” “title” music re-\nview -lyrics. For each of these queries, the top-100 Web\npages are retrieved and joined into Dm, the set of pages\nassociated with m. All retrieved documents are also stored\nin an index I. Relevance of a music piece mwrt. a given\nquery qis assessed by querying Iwithqand applying\nrank-based relevance scoring (RRS) to thenmost relevant\nWeb documents in I(Equation 1).\nRRS n(m,q) =/summationdisplay\np∈Dm∩Dq,n1 +|Dq,n| −rnk(p,D q,n)(1)\nIn this equation, ndenotes the maximum number of top-\nranked documents when querying I,Dq,nthe ordered set\n(i.e., the ranking) of the nmost relevant Web documents\ninIwith respect to query q, andrnk(p,D q,n)the rank of\ndocument pinDq,n. For retrieval, the ﬁnal ranking of mu-\nsic tracks is obtained by sorting the music pieces according\nto their RRS value.\nIn the published evaluations [11, 12], precision hardly\never exceeds 30% using this scoring approach, i.e., rank-\nings usually contain three times more irrelevant music piec-\nes than relevant. Based on this, also subsequent steps such\nas combination with audio similarity may suffer from er-\nroneous input. Clearly, the reason for the high number of\nirrelevant pieces has to be searched for in the underlying\nWeb pages. For indexing, all pages returned by Google\nare considered relevant, irrespective of whether they actu-\nally contain information about or descriptions of the cor-\nresponding music piece or artist. Furthermore, the page\nindexer does not distinguish between text that occurs in\nthe “main part” of the Web page and text that is used for\nnavigation or links to stories about other, completely un-\nrelated artists. Thus, to improve precision of the retrieved\nset of music pieces, in the next section, we propose four\ndifferent ﬁltering approaches to remove noisy information\nand documents.\n3. DOCUMENT FILTERING TECHNIQUES\nThis section describes the proposed ﬁltering methods to\nexclude noisy information from the indexing process. We\nexplore two types: Unsupervised and supervised ﬁlters.\n3.1 Unsupervised Filtering\nThe characteristic of these ﬁlters is that they aim at iden-\ntifying misleading texts without information from external\nsources. Hence, they can be applied to the index directly\nafter building it. The ﬁrst ﬁlter does not remove full doc-\numents from the index, but tries to identify those portionswithin the indexed text that do not contain speciﬁc infor-\nmation. The second approach identiﬁes and removes com-\nplete documents.\n3.1.1 Alignment-Based Noise Removal\nAs mentioned earlier, most indexed Web pages do not only\ncontain relevant and interesting information (if any at all).\nAlmost every page contains a site-speciﬁc header, naviga-\ntion bar, links to related pages, and copyright disclaimers,\nfrequently automatically generated by a content manage-\nment system, cf. [9, 19]. Especially on music pages, these\nsegments often feature lists of other music artists, genres,\nor tag clouds to facilitate browsing. This surrounding in-\nformation is usually not relevant to the associated music\npiece and should thus be ignored.\nRemoval of this kind of text is the aim of this ﬁlter.\nSince large parts of the surrounding text remain the same\nfor most pages within a Web domain, we can identify re-\ndundant segments by comparing several texts from the same\ndomain. Coherent parts are most likely to be non-speciﬁc\nfor a given music piece and can therefore be removed.\nTo this end, we adopt the multiple lyrics alignment tech-\nnique originally used to extract lyrics from multiple Web\nsources by matching coherent parts and preserving over-\nlapping segments [14]. In the current ﬁltering scenario, the\noverlapping segments are going to be removed.\nTo apply the ﬁlter, we collect all documents belonging\nto the same domain. Since for many blogs, the domain\nalone does not indicate similarly structured pages – differ-\nent blogs are typically accessible via separate subdomains\n(e.g., for blogspot.com) – we keep the subdomain if the\nhost section of the URL contains the word “blog”. For do-\nmains that occur only up to ﬁve times in the page index,\nno ﬁltering is performed. For all other domains up to eight\ndocuments are chosen randomly and used for alignment.\nFrom the alignment, we choose all aligned tokens occur-\nring in at least 60% of the aligned texts and ﬁnally select\nall text sequences consisting of at least 2 tokens. The re-\nsulting sequences are then removed in all Web pages orig-\ninating from the domain.\n3.1.2 Too-Many-Artists Filtering\nWith this ﬁlter, the goal is to detect pages that do not deal\nwith only one type of music, i.e., pages that provide an\nambiguous content and are therefore a potential source of\nerror. Some of these pages can be identiﬁed easily, since\nthey contain references to many artists. Hence, we query\nthe page index with all artist names from the music collec-\ntion and count the occurrences of each page in the result\nsets. Constructing the ﬁlter simply consists in selecting a\nthreshold for the maximum number of allowed artists per\npage. By systematically experimenting with this thresh-\nold, we yielded most promising results when removing all\npages containing more than 15 distinct artists. Throughout\nthe rest of this paper, too-many-artists ﬁltering refers to the\nremoval of pages containing more than 15 artists.\n544\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)3.2 Supervised Filtering\nAs\nalready mentioned in [12], automatic optimisation of\nthe (unsupervised) Web-based indexing approach is difﬁ-\ncult, since for arbitrary queries, there is no learning tar-\nget known in advance (in contrast, for instance, to the ap-\nproaches presented in [2, 15], where the set of possible\nqueries is limited). However, in terms of identifying sources\nof noise, automatic optimisation approaches are somewhat\nmore promising, provided that a set of potential queries\nwith corresponding relevance judgements is available. The\nidea is that by observing performance on a given set of\nqueries, it should be possible to learn to identify and ex-\nclude misleading Web pages and therefore yield better re-\nsults also on other, previously unseen queries. This is based\non the assumption that documents responsible for introduc-\ning noise to a music piece ranking contain erroneous (at\nleast ambiguous) information and are likely to introduce\nnoise to other queries too.\n3.2.1 Query-Based Page Blacklisting\nFollowing the general idea outlined in Section 3.2, we con-\nstruct a simple ﬁlter that blacklists (i.e., excludes) Web\npages contributing more negatively than positively to query\nresults. Hence, based on RRS we calculate a simple score\nto rate a page p:\nSn(p) =/summationdisplay\nq∈Q/parenleftBigg/summationdisplay\nm∈M p∩TqRRS n(m,q)−/summationdisplay\nm∈M p∩TqRR\nSn(m,q)/parenrightBigg\n(2)\nwhere Qdenotes the set of all available queries/annota-\ntions,Mpthe set of all music pieces associated with page\np,Tqthe set of all pieces annotated with q(i.e., relevant\nto query q), and Tqits complement (i.e., all music pieces\nnot\nrelevant to q). Informally speaking, over all queries,\nwe subtract the sum of RRS scores contributed to nega-\ntive examples from the sum of RRS scores contributed to\npositive examples. We then remove all Web documents p\nwithSn(p)<0, i.e., all documents that contributed more\nnegatively than positively over the course of all queries.\n3.2.2 Query-Trained Page Classiﬁcation\nWhile the query-based page blacklisting ﬁlter represents\n(if any) just the “laziest” form of machine learning (i.e.,\nmerely recognising instances without any kind of general-\nisation), this ﬁlter aims at learning to automatically clas-\nsify Web pages as either “positive” (keep) or “negative”\n(remove). Hence, it should be better suited to deal with\nnew queries that provoke previously unseen (and thus un-\nrated) Web pages. To get positive and negative examples as\ntraining instances for the classiﬁer, only pages that have ei-\nther contributed exclusively positively or exclusively neg-\natively are considered. Positive examples are deﬁned as\n{p|p∈Dq,n,∀q∈Q:Mp∩Tq=∅}and negative\nas{p|p∈Dq,n,∀q∈Q:Mp∩Tq=∅}(cf. Eq. 2).\nAs a further requirement, only pages that appear in at least\ntwo query result sets are considered. As feature represen-\ntation for Web pages, we incorporate characteristic valuessuch as the length of the page’s (unparsed) HTML content,\nthe length of the parsed content, the number of different\nterms occurring on the page, the number of associated mu-\nsic pieces (i.e., |Mp|), the number of contained artist names\n(cf. 3.1.2), as well as ratios between these numbers. Fur-\nthermore, we utilise title and URL of the pages as very\nshort textual representations that are converted into a term\nvector space (using the functions provided by WEKA [10])\nand added as numerical features.\nFor classiﬁcation, we decided to use the Random For-\nest Classiﬁer [5] from the WEKA package (with 10 trees).\nSince there are usually signiﬁcantly more negative than\npositive examples, we also apply a cost-sensitive meta-\nclassiﬁer to raise importance of positive instances (mis-\nclassiﬁcation of positive instances is penalised by the ratio\nof negative to positive examples).\n4. EV ALUATION\nFor evaluation of the different ﬁltering approaches, we use\nboth test collections from [12]. The ﬁrst collection, called\nc35k, is a large real-world collection and contains 35,000\nmostly popular pieces. For evaluation purposes, a bench-\nmarking set consisting of 200 queries and relevance judge-\nments has been created from Last.fm tags1. The second\ncollection is the CAL500 set, a collection of 500 songs\nmanually labelled with words representing various music-\nrelevant properties [15]. For comparison, we adopted the\n139 category subset used in [12]. To test effectiveness of\nthe retrieval approaches, annotations are used as queries to\nthe system. They also serve as relevance indicator, i.e., a\ntrack is considered to be relevant for query qif it has been\ntagged with tag q. For evaluation of the supervised ﬁltering\napproaches, a 10-fold cross validation is performed on the\ntest collections, i.e., in each fold, 90% of the queries are\nused to train the ﬁlters which are then applied and evalu-\nated on the remaining 10%.\nTo measure the quality of the obtained rankings, stan-\ndard evaluation measures for retrieval systems are calcu-\nlated, cf. [1]. Additionally to the “global” measures preci-\nsion andrecall, ranking measures like precision@10 doc-\numents, r-precision (i.e., precision at the rthreturned doc-\nument, where ris the number of tracks relevant to the\nquery), and (mean) average precision (MAP, i.e., the arith-\nmetic mean of precision values at all encountered relevant\ndocuments) are used for evaluation. To further compare\ndifferent retrieval strategies, we calculate precision at 11\nstandard recall levels. For each query, precision P(rj)at\nthe 11 standard recall levels rj,j∈ {0.0,0.1,0.2,...,1.0}\nis interpolated according to P(rj) =max rj≤r≤rj+1P(r).\nThis allows averaging over all queries and results in char-\nacteristic curves for each retrieval algorithm, enabling com-\nparison of distinct settings. To obtain a single value for\ncomparison of these curves, we calculate the area under the\ncurve (prec@11std.recall - AUC ). For presentation of the\nc35k collection, we decided to use tables instead of graphs\nto show more detailed results (including signiﬁcance tests).\n1http://www.last.fm\n545\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Recall Precision\nUNF ANR 2MA A+2 QPB QPC UNF ANR 2MA A+2 QPB QPC\nn= 10 2.18 2.01 2.07 1.95 2.01 2.69 30.15 31.89 35.72 30.71 34.32 36.98\nn= 20 3.74 3.95 3.93 3.66 3.89 4.93 29.02 31.30 32.86 30.91 33.91 37.11\nn= 50 7.17 7.48 7.87 7.34 8.15 9.15 27.61 29.50 31.13 28.56 32.50 34.79\nn= 100 12.72 12.09 12.58 11.92 12.80 14.26 25.99 27.64 29.05 27.25 31.75 32.42\nn= 200 18.67 18.22 18.13 17.95 19.08 20.73 23.77 25.77 26.21 25.26 28.60 28.74\nn= 500 29.31 29.60 29.84 28.16 29.96 32.00 20.12 21.69 21.71 21.00 24.76 23.19\nn= 1,000 40.38 40.31 40.11 36.41 40.43 41.52 16.88 17.86 18.19 18.00 20.75 18.92\nn= 10,000 80.50 79.56 76.80 57.55 73.50 73.32 7.29 7.42 7.87 11.90 9.63 8.62\nPrec@10 r-Precision\nUNF ANR 2MA A+2 QPB QPC UNF ANR 2MA A+2 QPB QPC\nn= 10 31.19 34.94 37.76 32.74 36.45 37.32 2.16 1.99 2.05 1.79 2.01 2.68\nn= 20 32.40 34.96 37.05 31.93 36.75 37.89 3.63 3.68 3.67 3.39 3.69 4.57\nn= 50 38.45 36.20 41.70 36.52 40.25 43.90 6.52 6.85 7.08 6.29 7.30 8.38\nn= 100 44.10 39.05 46.65 40.52 43.40 48.40 10.24 10.41 10.78 10.27 11.52 12.48\nn= 200 47.75 42.15 48.90 43.82 46.95 49.60 14.22 14.54 14.68 14.27 16.03 16.83\nn= 500 50.30 45.95 51.20 47.32 49.75 52.45 19.84 21.01 20.35 19.85 22.54 23.52\nn= 1,000 52.55 48.75 52.85 48.47 53.15 54.20 24.22 25.43 25.00 23.66 27.48 27.85\nn= 10,000 57.45 57.20 56.70 50.12 62.35 58.00 35.20 35.77 35.03 28.28 35.69 34.70\nAvg. Prec (MAP) Prec@11Std.Recall - AUC\nUNF ANR 2MA A+2 QPB QPC UNF ANR 2MA A+2 QPB QPC\nn= 10 1.19 1.32 1.38 1.12 1.39 1.83 3.05 3.15 3.34 2.95 3.23 3.61\nn= 20 1.84 2.14 2.24 1.86 2.26 2.95 3.64 3.98 4.07 3.74 4.06 4.78\nn= 50 3.24 3.79 4.06 3.58 4.49 5.22 4.99 5.63 5.86 5.44 6.39 6.67\nn= 100 5.54 5.93 6.29 5.67 7.00 7.64 7.11 7.56 7.91 7.34 8.51 9.14\nn= 200 8.23 8.61 8.78 8.26 10.24 10.65 9.62 10.12 10.19 9.75 11.72 12.20\nn= 500 12.39 13.30 13.19 12.38 15.06 15.93 13.76 14.69 14.57 13.89 16.45 17.43\nn= 1,000 16.10 17.37 17.01 15.45 19.41 19.84 17.22 18.84 18.36 16.82 20.82 21.03\nn= 10,000 29.98 30.60 29.80 21.86 30.92 29.22 31.25 31.84 31.07 23.07 32.05 30.39\nTable 1. Comparison of unﬁlter ed RRS (UNF) vs. Alignment-Based Noise Removal (ANR), Too-Many-Artists Filtering\n(2MA), ANR+2MA (A+2), Query-Based Page Blacklisting (QPB), and Query-Trained Page Classiﬁcation (QPC n=200 )\nfor the c35k collection and different values of n, i.e., the maximum number of retrieved Websites incorporated in RRS.\nQPB and QPC are performed upon ANR. Values (given in %) are obtained by averaging over 200 evaluation queries (for\nsupervised approaches via 10-fold Cross Validation). Entries in bold face indicate that there is no signiﬁcant difference\nbetween this entry and the best performing, i.e. bold entries indicate the “best group” (Friedman test, α= 0.01). Note that\ndue to the rank-based nature of the non-parametric Friedman test, results may belong to the best group even with lower\naverage values than signiﬁcantly worse results.\nTable 1 shows evaluation results on the c35k collection\nfor different values of n(number of top ranked Web docu-\nments when querying the page index). For the alignment-\nbased noise removal (ANR), we observe slight improve-\nments for the averaged results especially for precision, r-\nprecision, average precision and the area under the stan-\ndardized precision-recall curve. However, in the Friedman\ntest these results are not signiﬁcant. For recall and preci-\nsion@10 we can see a signiﬁcant drop in performance.\nThe too-many-artists ﬁlter (2MA) outperforms the un-\nﬁltered RRS signiﬁcantly in terms of precision and aver-\nage precision for smaller values of n. A decrease is most\nclearly visible for recall. In addition, we evaluated also the\ncombination of both unsupervised ﬁlters (A+2). In mostcases, this combination worsens results signiﬁcantly which\nis rather surprising, considering that these ﬁlters target dif-\nferent levels of noise removal. However, it seems that too\nmuch information is excluded when using both.\nExcept for recall, both supervised approaches are con-\nstantly in the best performing group, superiority is clearly\nvisible for precision. For the query-trained page classiﬁca-\ntion ﬁlter (QPC), it has to be mentioned that for values of\nn >500the number of training instances gets very high,\nslowing down the evaluation progress. For this reason, we\ndecided to use the QPC ﬁlter with the n= 200 setting also\nfor experiments with n/ne}ationslash= 200 . This explains also the slight\ndrop for n≥1,000. Still, results are more than acceptable\nfor QPC.\n546\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Recall Precision\nUNF ANR 2MA A+2 QPB QPC UNF ANR 2MA A+2 QPB QPC\nn= 10 5.96 6.10 5.85 6.00 6.10 6.69 25.77 27.17 25.72 26.99 27.22 28.35\nn= 20 10.19 9.68 9.46 9.49 9.83 10.69 24.87 25.39 24.69 25.44 25.07 25.73\nn= 50 17.99 18.09 17.20 17.61 18.23 17.89 22.84 23.14 22.44 22.94 23.22 22.68\nn= 100 26.80 26.34 25.48 25.90 27.11 24.34 21.02 21.05 20.80 21.10 21.69 20.16\nn= 200 38.63 38.62 37.24 37.21 37.95 31.75 19.15 19.30 19.11 19.25 19.67 18.54\nn= 500 56.31 55.65 54.26 53.31 51.21 38.09 16.86 16.95 16.92 17.05 17.74 17.24\nn= 1,000 66.91 66.48 63.78 62.69 53.10 40.10 15.54 15.81 15.74 16.01 17.36 16.74\nn= 10,000 73.27 72.93 69.06 68.02 37.98 40.76 14.56 14.84 14.82 15.17 20.75 16.56\nTable 2. Comparison of unﬁltered RRS vs. the ﬁlter approaches for the CAL500 set averaged over 139 queries (cf. Table 1).\nNote\nthat in contrast to Table 1, in this table, bold and italic appearing entries indicate a signiﬁcant difference to the group\nof best approaches, i.e., worse results are marked. For all experiments with QPC, the setting QPC n=50 is used.\n10 20 50 100 200 500 1,000 10,00005101520253035\nn  P@10 UNF\nP@10 ANR\nP@10 2MA\nP@10 A+2\nP@10 QPB\nP@10 QPC\nMAP UNF\nMAP ANR\nMAP 2MA\nMAP A+2\nMAP QPB\nMAP QPC\nFigure 1. Pr ecision@10 (upper curves) and Avg. Prec\n(MAP) (lower curves) for the CAL500 set and different\nvalues of n(cf. Table 2).\nFor the CAL500 set, results are very disappointing (Ta-\nble 2). No proposed ﬁlter can signiﬁcantly improve results\n(except for precision of the supervised ﬁlters with high val-\nues of n, which go along with a dramatic loss in recall due\nto a very high number of excluded pages). The reasons are\nnot directly comprehensible. One possibility could be that\nin the case of the c35k set with associated Last.fm tags, the\napproaches beneﬁt from the inherent redundancies in the\ntags/queries (e.g., metal vs.black metal vs.death metal ).\nIn the case of the CAL500 set, queries exhibit no redun-\ndancy, as the set is constructed to describe different di-\nmensions of music. However, this would only affect the\nsupervised ﬁlters.\nAnother explanation could be that the CAL500 page in-\ndex contains considerably less pages than the c35k index\n(approx. 80,000 vs. approx. 2 million pages). First, and\nalso in the light that the CAL500 set has been carefully de-\nsigned, it seems possible that the index does not contain\nso much noise. Hence, the proposed noise removal strate-gies don’t work here. Second, since the index is rather\nsmall, removal of a relatively high number of pages has a\nhigher impact on the overall performance. This becomes\nespecially apparent when examining the results of the su-\npervised approaches for high n. Apart from the results,\nit should be noted that the CAL500 set is without doubt\nvery valuable for research (high quality annotations, freely\navailable, etc.) but at the same time, it is a highly artiﬁcial\ncorpus which can not be considered a “real-world” collec-\ntion. Hence, some “real-world” problems maybe can not\nbe tested with such a small set.\n5. CONCLUSIONS AND FUTURE WORK\nWe have demonstrated the usefulness of two unsupervised\nand two supervised ﬁltering approaches for Web-based in-\ndexing of music collections. Evaluation showed inconsis-\ntent results for two collections with very different charac-\nteristics and suggests that the proposed ﬁltering techniques\ncan improve results signiﬁcantly when applied to large and\ndiverse music collections with millions of Web pages as-\nsociated.\nRegarding the proposed ﬁltering techniques, more or\nless all of them proved to be useful and could improve not\nonly the overall precision but also the ranking of music\npieces. By introducing supervised optimisation into this\noriginally unsupervised technique, there is still more po-\ntential to tweak performance. For instance, we are con-\nvinced that a more carefully selected feature set can easily\nimprove results of page classiﬁcation further. Using anno-\ntated sets for learning, also proper combination with audio\nsimilarity, e.g., to raise recall, could be possible.\nInstead of ﬁnding redundant portions in Web pages from\nthe same domain by aligning and matching their content,\ntechniques like vision page segmentation [6] could help in\nidentifying the relevant parts of a Web page. By extract-\ning smaller segments from Web pages, the principle of the\nRRS weighting could be transferred to “blocks” and scor-\ning could be designed more speciﬁcally.\nAnother aspect not directly related to ﬁltering pages be-\ncame apparent during experiments with the Too-many-ar-\ntists ﬁlter. When querying the page index with the names\n547\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)of the contained artists, artists with common speech names\ncan\nbe easily identiﬁed. As each artist only has a limited\nnumber of associated pages in the index, true occurrences\nare somehow normalised. For artists that occur much more\noften than expected (outlier), it can be assumed that they\nhave common speech names. This ﬁnding could be inter-\nesting for related tasks in future work.\n6. ACKNOWLEDGMENTS\nThis research is supported by the Austrian “Fonds zur F ¨or-\nderung der Wissenschaftlichen Forschung” (FWF) under\nproject number L511-N15.\n7. REFERENCES\n[1] Ricardo Baeza-Yates and Berthier Ribeiro-Neto. Mod-\nern Information Retrieval. Addison-Wesley, Reading,\nMassachusetts, 1999.\n[2] Luke Barrington, Douglas Turnbull, Mehrdad Yazdani,\nand Gert Lanckriet. Combining audio content and so-\ncial context for semantic music discovery. In Proceed-\nings of the 32nd ACM SIGIR, Boston, MA, USA, 2009.\n[3] Stephan Baumann and Oliver Hummel. Using Cultural\nMetadata for Artist Recommendation. In Proceedings\nof the 3rd International Conference on Web Delivering\nof Music (WEDELMUSIC 2003), September 2003.\n[4] Stephan Baumann, Andreas Kl ¨uter, and Marie Nor-\nlien. Using natural language input and audio analysis\nfor a human-oriented MIR system. In Proceedings of\nthe 2nd International Conference on Web Delivering\nof Music (WEDELMUSIC 2002), Darmstadt, Germany,\n2002.\n[5] Leo Breiman. Random forests. Machine Learning,\n45(1):5–32, 2001.\n[6] Deng Cai, Shipeng Yu, Ji-Rong Wen, and Wei-Ying\nMa. Extracting content structure for web pages based\non visual representation. In Fifth Asia Paciﬁc Web Con-\nference (APWeb 2003), 2003.\n[7] Rui Cai, Chao Zhang, Chong Wang, Lei Zhang, and\nWei-Ying Ma. Musicsense: contextual music recom-\nmendation using emotional allocation modeling. In\nProceedings of the 15th ACM Multimedia, 2007.\n[8] Oscar Celma, Pedro Cano, and Perfecto Herrera.\nSearch Sounds: An audio crawler focused on weblogs.\nInProceedings of the 7th International Conference\non Music Information Retrieval (ISMIR’06), Victoria,\nB.C., Canada, 2006.\n[9] Sandip Debnath, Prasenjit Mitra, and C. Lee Giles.\nAutomatic extraction of informative blocks from web-\npages. In Proceedings of the 2005 ACM Symposium on\nApplied Computing (SAC’05), 2005.[10] Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard\nPfahringer, Peter Reutemann, and Ian H. Witten. The\nweka data mining software: An update. SIGKDD Ex-\nplorations, 11(1), 2009.\n[11] Peter Knees, Tim Pohle, Markus Schedl, Dominik\nSchnitzer, and Klaus Seyerlehner. A Document-\ncentered Approach to a Natural Language Music\nSearch Engine. In Proceedings of the 30th European\nConference on Information Retrieval (ECIR’08), Glas-\ngow, Scotland, UK, March 30-April 3 2008.\n[12] Peter Knees, Tim Pohle, Markus Schedl, Dominik\nSchnitzer, Klaus Seyerlehner, and Gerhard Widmer.\nAugmenting Text-Based Music Retrieval with Audio\nSimilarity. In Proceedings of the 10th International So-\nciety for Music Information Retrieval Conference (IS-\nMIR 2009), Kobe, Japan, October 2009.\n[13] Peter Knees, Tim Pohle, Markus Schedl, and Gerhard\nWidmer. A Music Search Engine Built upon Audio-\nbased and Web-based Similarity Measures. In Pro-\nceedings of the 30th ACM SIGIR, Amsterdam, the\nNetherlands, July 23-27 2007.\n[14] Peter Knees, Markus Schedl, and Gerhard Widmer.\nMultiple Lyrics Alignment: Automatic Retrieval of\nSong Lyrics. In Proceedings of 6th International Con-\nference on Music Information Retrieval (ISMIR’05),\nLondon, UK, September 2005.\n[15] Douglas Turnbull, Luke Barrington, David Torres, and\nGert Lanckriet. Towards Musical Query-by-Semantic-\nDescription using the CAL500 Data Set. In Proceed-\nings of the 30th ACM SIGIR, Amsterdam, the Nether-\nlands, July 23-27 2007.\n[16] Douglas Turnbull, Luke Barrington, David Torres, and\nGert Lanckriet. Semantic annotation and retrieval of\nmusic and sound effects. IEEE Transactions on Au-\ndio, Speech and Language Processing, 16(2):467–476,\nFebruary 2008.\n[17] Geraint Wiggins. Semantic Gap?? Schemantic\nSchmap!! Methodological Considerations in the Scien-\ntiﬁc Study of Music. In Proceedings of the 11th IEEE\nInternational Symposium on Multimedia (ISM’09):\nWorkshop on Advances in Music Information Research\n(AdMIRe), 2009.\n[18] Yi-Hsuan Yang, Yu-Ching Lin, and Homer Chen. Clus-\ntering for music search results. In Proceedings of the\nIEEE International Conference on Multimedia and\nExpo (ICME), 2009.\n[19] Lan Yi, Bing Liu, and Xiaoli Li. Eliminating noisy in-\nformation in web pages for data mining. In Proceed-\nings of the 9th ACM SIGKDD Conference, 2003.\n548\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "Collaborative Filtering Based on P2P Networks.",
        "author": [
            "Noam Koenigstein",
            "Gert R. G. Lanckriet",
            "Brian McFee",
            "Yuval Shavitt"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1415620",
        "url": "https://doi.org/10.5281/zenodo.1415620",
        "ee": "https://zenodo.org/records/1415620/files/KoenigsteinLMS10.pdf",
        "abstract": "Peer-to-Peer (P2P) networks are used by millions of peo- ple for sharing music files. As these networks become ever more popular, they also serve as an excellent source for Music Information Retrieval (MIR) tasks. This paper re- views the latest MIR studies based on P2P data-sets, and presents a new file sharing data collection system over the Gnutella. We discuss several advantages of P2P based data-sets over some of the more “traditional” data sources, and evaluate the information quality of our data-set in com- parison to other data sources (Last.fm, social tags, biog- raphy data, and MFCCs). The evaluation is based on an artists similarity task using Partial Order Embedding (POE). We show that a P2P based Collaborative Filtering data- set performs at least as well as “traditional” data-sets, yet maintains some inherent advantages such as scale, avail- ability and additional information features such as ID3 tags and geographical location.",
        "zenodo_id": 1415620,
        "dblp_key": "conf/ismir/KoenigsteinLMS10",
        "keywords": [
            "Peer-to-Peer (P2P) networks",
            "Music Information Retrieval (MIR)",
            "Gnutella",
            "file sharing data collection system",
            "P2P based data-sets",
            "information quality evaluation",
            "artists similarity task",
            "Partial Order Embedding (POE)",
            "Collaborative Filtering data-set",
            "traditional data-sets"
        ],
        "content": "COLLABORATIVE FILTERING BASED ON P2P NETWORKS\nNoam Koenigstein1, Gert Lanckriet2, Brian McFee3, and Yuval Shavitt1\n1School of Electrical Engineering, Tel Aviv University\n2Electrical and Computer Engineering, University of California, San Diego\n3Computer Science and Engineering, University of California, San Diego\nABSTRACT\nPeer-to-Peer (P2P) networks are used by millions of peo-\nple for sharing music ﬁles. As these networks become ever\nmore popular, they also serve as an excellent source for\nMusic Information Retrieval (MIR) tasks. This paper re-\nviews the latest MIR studies based on P2P data-sets, and\npresents a new ﬁle sharing data collection system over the\nGnutella. We discuss several advantages of P2P based\ndata-sets over some of the more “traditional” data sources,\nand evaluate the information quality of our data-set in com-\nparison to other data sources (Last.fm, social tags, biog-\nraphy data, and MFCCs). The evaluation is based on an\nartists similarity task using Partial Order Embedding (POE).\nWe show that a P2P based Collaborative Filtering data-\nset performs at least as well as “traditional” data-sets, yet\nmaintains some inherent advantages such as scale, avail-\nability and additional information features such as ID3 tags\nand geographical location.\n1. INTRODUCTION\nThe usage of P2P based information for music information\nretrieval (MIR) tasks is gaining momentum. The process\nof collecting Collaborative Filtering (CF) data from P2P\nnetworks is typically more complex than from more “tra-\nditional” sources such as Last.fm or social networks, but\nthere are several advantages that signiﬁcantly undermine\nthis small impairment.\nBarrington et al. [2] compared different approaches for\nmusic recommendation with a user study of 185 subjects.\nThey concluded that approaches based on collaborative ﬁl-\ntering which essentially capture the “wisdom of the crowds”,\noutperform content-based approaches so long as the data-\nset used is sufﬁciently comprehensive. However, when the\ndata-set is insufﬁcient, or the artists are less popular (those\nin the long tail), we are compelled to use content-based ap-\nproaches. The scale of a CF data-set is therefore of great\nimportance. Using a crawler of the Gnutella ﬁle-sharing\nnetwork, we were able to record 281,865,501 user-to-song\nrelations of over 1.3 million users in a single 24 hours\ncrawl. Such scales far exceed the “traditional” CF data-\nsets such as the well-established Last.fm data-set provided\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc°2010 International Society for Music Information Retrieval.by [6] (17,559,530 records from 359,347 users).\nAnother advantage of P2P data-sets over traditional data-\nsets is the availability of information, mitigating the need\nfor agreements with website operators and various restric-\ntions they pose on the amount of data collected or its usage.\nDue to their decentralized nature and open protocols, P2P\nnetworks are a source for independent large scale data col-\nlection. Anyone who overcomes the initial technological\nbarrier can crawl the network and collect valuable infor-\nmation.\nData-sets based on shared folders typically include ID3\ntags that reveal information such as the title, artist, album\nand track number. Although sometimes these records are\nabsent or conﬂicting, it is often still possible to restore the\ncorrect values. In this paper for example, we used ma-\njority voting to decide on the correct artist names for dif-\nferent ﬁles. P2P data-sets typically include also the IP\naddresses of the users. The IP address can be used as a\nunique user identiﬁer for short time spans, but more im-\nportantly, it also allows for geographical classiﬁcation of\nusers. IP-based geographical classiﬁcation is highly accu-\nrate and can reveal not only the user’s country and state,\nbut also the user’s city and sometimes even smaller areas\nlike the boroughs of New-York City. Such geographical\nresolution was used by [14] for identifying emerging local\nmusical artists with high potential for a breakthrough.\nDespite all their advantages, P2P networks are quite\ncomplex, making the collection of a comprehensive data-\nset far from being trivial, and in some cases practically un-\nfeasible. First, P2P networks have high user churn, caus-\ning users to constantly connect and disconnect from the\nnetwork, being unavailable for changing periods. Second,\nusers in P2P networks often do not expose their shared data\nin order to maintain high privacy and security measures,\ntherefore disabling the ability to collect information about\ntheir shared folders. Finally, users often delete content af-\nter using it, leaving no trace of its usage.\nA different complexity involves the usage of meta-data,\nwhich was shown to be particulary useful for ﬁnding simi-\nlarity between performing artists [17]. The content on ﬁle\nsharing networks is mostly ripped by individual users for\nconsumption by other users. User based interactions are a\ndesirable property in IR data-sets, however when it comes\nto meta-data, its the main source for ambiguities and noise.\nBe it a movie, a song, or any other ﬁle type, typically there\nwould be several similar duplications available on the net-\nwork. The ﬁles may be digitally identical, thus having the\nsame hash signature, yet bearing different ﬁle names, and\nmeta-data tags. Duplication in meta-data tags typically\n153\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)caused by\nspelling mistakes, missing data, and different\nvariations on the correct values. In the Gnutella network\nfor example, only 7-10% of the queries are successful in\nreturning useful content [30]. A common hash signature\ncan facilitate similar ﬁles grouping, nonetheless it does not\nsolve the problem of copies that are not digitally identical.\nThe problem of meta-data ambiguities in P2P data-set is\naddressed in [16].\n2. BACKGROUND\nMIR studies based on P2P networks belong to one of two\ncategories:\n²Studies Based on Queries: Queries in a ﬁle-sharing\nnetwork represent the current tastes and interests of\nusers. A query is issued upon a request by a user\nsearching for a speciﬁc ﬁle, or content relevant to the\nsearch string. Query data-sets are time dependent,\nand because of dynamic IP assignments, it can be\ndifﬁcult to track a single user over time. Therefore,\nquery-based studies tend to focus on temporal trends\nsuch as predicting artists success or artists ranking.\nQueries are generally ineffective for predicting artist\nsimilarity and general recommendation systems be-\ncause the information gathered per user is limited to\na short time period, and thus only a few ﬁles per user\nare usually available.\n²Studies Based on Shared Folders: The content of a\nuser’s shared folder accumulates over time. It can be\nviewed as an integration of a user’s taste over an ex-\ntended period of time. Data-sets derived from shared\nfolders are therefore the preferred choice for similar-\nity tasks such as recommender systems.\n2.1 Previous Query-based Studies\nIn [14], geographically identiﬁed P2P queries were used in\norder to detect emerging musical talents. The detection al-\ngorithm is based on the observation that emerging artists,\nespecially rappers, have a discernible stronghold of fans in\ntheir hometown area, where they are able to perform and\nmarket their music. In a ﬁle-sharing network, this is re-\nﬂected as a spike in the spatial distribution of queries. The\nalgorithm mimics human scouts by looking for performers\nwhich exhibit a sharp increase in popularity within a small\ngeographic region.\nThe algorithm in [14] is effective for predicting the suc-\ncess of emerging artists, but it cannot be applied on well-\nestablished artists. Bhattacharjee et al. [4,5] have used P2P\nactivity to predict an album’s life cycle and trends on the\nBillboard’s top 200 albums chart. Both papers used the\nWinMx ﬁle-sharing network. In [4], they showed that P2P\nsharing activity levels provide leading indicators for the di-\nrection of movement of albums on the Billboard charts. In\n[5], a linear regression model was used to show that shar-\ning activity may be used to predict an album’s life cycle.\nMore recently, [17] used the C4.5 [22] and BFTree [8, 26]\nalgorithms on queries collected from the Gnutella network\nin order to predict a song’s top rank on the Billboard sin-\ngles chart.A different approach for using P2P queries was taken\nby [13]. Grace et al. [10] noticed that although music sales\nare losing their role as means for music dissemination, they\nare still used by the music industry for ranking artist suc-\ncess, e.g., in the Billboard Magazine chart. They therefore\nsuggested using social networks as an alternative ranking\nsystem; a suggestion which is problematic due to the ease\nof manipulating the list and the difﬁculty of implementa-\ntion. Koenigstein et al. [13] used Gnutella queries in or-\nder to build an alternative to the Billboard song ranking\nchart. They compared trends in sales and air-play counts,\nto piracy popularity trends, and showed that piracy popu-\nlarity of singles by well-established artists, is highly corre-\nlated with the Billboard charts.\n2.2 Previous Shared Folders Studies\nFirst attempts to use P2P shared folders for artist similar-\nity were presented in [3,7]. The centralized and somewhat\nundersized OpenNap network was used in order to gener-\nate a similarity measurement that was based on artists co-\noccurrences in shared folders. The authors compared the\nP2P information to other similarity measurements such as\nsocial tags in [7], and also Gaussian mixtures over MFCCs\nand playlists co-occurrences in [3]. The evaluation was\ndone against survey data, and similarities were measured\nby a pre-determined similarity function.\nWe took the same approach of evaluating data against a\nhuman based survey. The evaluation in this paper is based\non the Partial Order Embedding (POE) algorithm of [18],\nwhich learns an optimized artist similarity space from la-\nbeled (partially ordered) examples. The key difference be-\ntween the evaluation in [3] and the present work is that\nwe report accuracy achievable by an optimized similar-\nity function, whereas [3] relies on a ﬁxed similarity func-\ntion. The results in Section 4 emphasize the importance\nof training the embedding before evaluating with a human\nbased survey. The scale of the data-set used here (13.8\nmillion user-to-song relations after processing), is much\nhigher than in [3,7] (400K user-to-song relations after pro-\ncessing), although our experimental results are restricted\nto a subset for evaluation purposes.\nThe ﬁrst working recommender system based on P2P\ninformation was demonstrated in [25]. Shared folders data\nfrom the Gnutella network was used in order to generate\na user-to-artists matrix. The artists were clustered using\nk-means algorithm, and recommendations were done from\nthe centroid or from the nearest neighbor.\n3. DATA COLLECTION METHODOLOGY\nThe practice of collecting information from ﬁle-sharing\nnetworks is relatively common in the ﬁeld of computer\ncommunication. P2P measurement techniques fall into ﬁve\nbasic categories:\n1.Passive Monitoring: Monitoring P2P activity by\nanalyzing data from a gateway router.\n2.Participate: Developing a client software that can\ncapture and log interesting information [13, 14, 17].\n154\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Figur e\n1. Crawling and Browsing in a Two-Tier Gnutella Segment\n3.Crawl: Developing a crawler which recursively “walks”\nthe network by asking each peer for a list of its neigh-\nbors [15, 16, 25].\n4.Sample: Sampling a set of peers and gathering static\npeer properties [4, 5].\n5.Central: Study information gathered from a central\nentity in the network [3, 7].\nThe data collection system described here belongs to the\nthird category. We crawled the Gnutella ﬁle-sharing net-\nwork as described below.\n3.1 The Gnutella Network\nGnutella started its operations on March 2000, as the ﬁrst\ndecentralized ﬁle-sharing network. It is arguably the most\nacademically studied ﬁle-sharing network [1, 9, 11, 23, 24,\n27, 28].In late 2007, it was the most popular ﬁle-sharing\nnetwork on the Internet with an estimated market share of\nmore than 40% [21], serving millions of users.\nModern Gnutella, as well as other popular P2P ﬁle-\nsharing applications, adopted a two-tier topology. In this\narchitecture, a small fraction of nodes, called ultrapeers ,\nform an ad-hoc top-level overlay whereas the remaining\nnodes, called leaves, each connect to the overlay through\na small number of ultrapeers. Ultrapeers belong to regular\nusers with higher computing and network resources. These\nnodes route search requests and respond to other users who\nconnected to them. Ultrapeers typically have a high degree\n(i.e., maintain 30 neighbors) in order to keep a short path\nlengths between participating peers [28]. We crawl both\nleaves and ultrapeers in a similar manner.\n3.2 Crawling the Network\nP2P crawlers operate in a similar way to web crawlers. The\ncrawler treats the network as a graph. The starting points of\nthe crawling operation are taken from an ofﬂine initializa-\ntion list of known hosts. This initialization list must con-\ntain some redundancies, because unlike web crawling, theGnutella nodes might be ofﬂine and therefore unrespon-\nsive. To maximize the performance of the highly paral-\nlelized architecture of the crawler, we used a very large\ninitialization list of 104,767 IP addresses. This allows us\nto make use of all the crawling clients right at the begin-\nning of the crawling operation1.\nFigure 1 depicts the crawling and browsing operations\nin a two-tier Gnutella segment. The crawling process is a\nbreadth-ﬁrst exploration, where newly discovered leaves\nand ultrapeers are enqueued in a list of un-crawled ad-\ndresses (The IPs Queue). The parallel crawling threads\nconstantly ask the Crawling Manager for new IP addresses\nfrom the queue, and send back newly received results. The\nresults are stored in text log ﬁles, and new IPs are enqueued\nin the IPs Queue.\nGnutella’s “Ping-Pong” protocol is used by the crawl-\ning threads to discover new Gnutella nodes in the network.\nA node receiving a “Ping” message is expected to respond\nwith one or more “Pong” messages. A “Pong” message in-\ncludes the address of a connected Gnutella node and infor-\nmation regarding the amount of data it is making available\nto the network. An incoming Ping message with TTL = 2\nand Hops = 0 is a “Crawler Ping” used to scan the network.\nIt should be replied to with Pongs containing information\nabout the node receiving the Ping and all other nodes it is\nconnected to. More details about the the Gnutella protocol\ncan be found in [29].\nThe crawling of large scale dynamic networks, such as\nﬁle-sharing networks never reaches a full stop. As clients\nconstantly connect and disconnect from the network, the\ncrawler will always discover new IP addresses. We thus\nuse two stopping conditions: A time constraint (typically\n1 hour), or reaching a low rate of newly discovered nodes,\nwhich indicates the completion of a crawl. In the begin-\nning of a crawl, the rate of newly discovered nodes in-\ncreases dramatically and typically reaches over 300,000\nnew clients per minute. As the crawling process proceeds,\n1Such a\nlarge list of IP addresses can be easily generated from the\nresults of a previous crawling operation.\n155\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)disco v\nery rate slows down until it reaches a few hundreds\nper minute. At this point, the networks is almost fully cov-\nered, and the newly discovered nodes are mostly the ones\nthat have joined the network only after the crawling opera-\ntion started.\n3.3 Browsing Shared Folders\nThe browsing operation begins shortly after the crawling\noperation started. Once the ﬁrst crawling log ﬁle is created,\ntheBrowsing Manager can start assigning IP addresses\n(taken from the crawling logs) to the browsing threads.\nThe browsing threads send “Query” messages to the Gnutella\nnodes, and wait for a “QueryHit” message in return. Query\nmessages with TTL=1, hops=0 and Search Criteria=“ ”\n(four spaces)\nare used to index all ﬁles a node is sharing.\nA node should reply to such queries with all of its shared\nﬁles. The sharing information is stored by the Browsing\nManager in the browsing logs. These ﬁles are used to gen-\nerate the CF data.\n4. EV ALUATION\nTo evaluate the information content of the present P2P data,\nwe test its performance on an artist similarity prediction\ntask. [18,19] developed the Partial Order Embedding (POE)\nalgorithm for integrating multiple data sources to form an\noptimized artist similarity space, and applied it to acoustic\nmodels (Gaussian mixtures over MFCCs and chroma vec-\ntors), semantic models (semantic multinomial auto-tags,\nand social tags from Last.fm2), and text models (biogra-\nphy data)3. By applying the same algorithm to collabora-\ntive ﬁltering data, we can evaluate the amount of high-level\nartist similarity information captured by P2P collaborative\nﬁltering data, and quantitatively compare it to alternative\ndata sources.\nIn general, collaborative ﬁltering has been repeatedly\ndemonstrated to be an effective source of information for\nrecommendation tasks (see, e.g., [2, 12]). One may then\nwonder how one source of collaborative ﬁltering data com-\npares to another. Because [18] did not include collabora-\ntive ﬁltering in their experiments, there is no existing base-\nline to compare against for the artist similarity task. We\ntherefore repeat the experiment with the Last.fm collabo-\nrative ﬁltering data provided by [6], allowing us to quan-\ntitatively compare P2P data to a more conventional source\nof collaborative ﬁltering information.\nWe sampled 100K (7.69%) users out of over 1.3 million\nGnutella users recorded on a single 24 hours crawl. We ﬁl-\ntered the ﬁles that correspond to musical ﬁles according to\nﬁle sufﬁx (.mp3 ﬁles). In the entire (1.3 million users) data\nset, we identiﬁed 531,870 different songs. In our 100K\nusers sample, we identiﬁed 511K songs, a value that is not\nmuch lower than the total number. Ambiguities in artist\nnames due to typos and misspellings were corrected by\nmajority voting. After this step, we had 13,839,361 user-\nto-song relations, which was the base for a collaborative\nmatrix (ARTISTSxUSERS). The artist names are the same\nas in the aset400 data set of Ellis and Whitman [7]. These\n2http://last.fm/\n3Thedata\nfrom [18] can be found at http://mkl.ucsd.edu/.\n373,555,801 user-to-file\n1.3 million users\n281,865,501 user-to-file\n1.2 million users\n531,870 songs\n13,839,361 user-to-song\n100K users\n511K songs\n80,119 users\n389 artists\n(aset400)Artists-by-users\nMatrix100K Users\nSampleFiltered Musical \nContent24-hour CrawlFigur e\n2. A quantitative summary of the data-set scale\nafter each processing stage\nartists were found in the shared folders of 80,119 users.\nThe above numbers are summarized in Figure 2. Our sim-\nilarity matrix will be available on the authors website by\npublication time.\n4.1 The embedding problem\nFormally, the goal in this experiment is to learn an embed-\nding function g:X !Rn, which maps a set of artists\nXinto Euclidean space. The embedding is trained to re-\nproduce relative comparison measurements (i; j; k ), where\n(i; j)are more similar to each-other (i.e., closer) than (i; k).\nEach artist is represented as a vector in some feature\nspace, and the embedding function is parameterized as a\nlinear projection from that feature space to the embedding\nspace. This can be expressed in terms of inner products:\ng(i) =NKi;\nwhere Nis a linear projection matrix to be learned, and Ki\nis a vector containing the inner product of i’s feature vec-\ntor with each other point in the training set. As described\nin [18], this readily generalizes to non-linear kernel func-\ntions and heterogeneous data sources, but we do not make\nuse of these extensions in the present experiment.\nTo summarize, given a set of training artists, relative\nsimilarity measurements between the artists, and a feature\nrepresentation of each artist (equivalently, a kernel matrix\nover the training artists), the algorithm ﬁnds a linear pro-\njection matrix Nwhich attempts to satisfy the similarity\nmeasurements under Euclidean distance calculations:\n(i; j; k ), kN (Ki¡Kj)k<kN(Ki¡Kk)k:\nThe matrix Nis found by solving a convex optimization\nproblem, which involves three competing terms:\nmax\nWX\ni;jkKi¡Kjk2\nW¡¯¢X\nijk»ijk¡°¢tr(WK )\nkKi¡Kjk2\nW:= (K i¡Kj)TW(Ki¡Kj);\nwhere Wis a positive semi-deﬁnite matrix which can be\nfactored to recover the projection matrix: W=NTN.\n156\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Theﬁrst\nterm maximizes the variance of the data in the\nembedding space, which prevents points from being col-\nlapsed onto each-other.\nThe second term tries to minimize the number of or-\ndering mistakes made by the embedding function. This is\naccomplished by using a slack variable »ijk¸0for each\ntriplet constraint (as in support vector machines), allowing\nfor margin violations:\nkKi¡Kjk2\nW· kK i¡Kkk2\nW+ 1¡»ijk:\nFinally, the third term limits the complexity of the learned\nspace by penalizing a convex approximation to the rank of\nthe embedding space. For more details about the optimiza-\ntion procedure, see [18].\nAt test time, similarity queries are presented in a similar\nform: (q; i; j ), where qis previously unseen, and iandj\ncome from the training set. The query artist is mapped into\nthe embedding space by ﬁrst computing inner products to\nthe training set, resulting in a vector Kq, and then project-\ning by N:g(q) =NKq. Once in the embedding space,\ndistances are calculated to iandj, and the similarity pre-\ndiction is counted as correct if the distance to iis smaller\nthan the distance to j.\n4.2 From P2P to artist similarity\nIn order to apply the POE algorithm to collaborative ﬁl-\ntering data, we need to deﬁne a kernel function between\nartists in terms of the collaborative ﬁltering matrix. One\nstraightforward choice of kernel function is to simply count\nthe number of users shared between two artists iandj.\nHowever, this may suffer from popularity bias if ihas many\nusers and jhas relatively few. To counteract this, we nor-\nmalize each artist by the number of users to which it is\nmatched. This gives rise to the kernel function:\nk(i; j) =#users for iandj\n(#users fori)¢(#\nusers for j):\nEquivalently, we can interpret this kernel function as the\ncosine-similarity between bag-of-users representations of\nartists iandj, i.e., an artist is represented by a binary vec-\ntor where coordinate zis 1 if user zis present and 0 oth-\nerwise. This is similar to the bag-of-words representation\ncommonly used in text applications, and like in text, the\ndimensionality of the feature representation is much larger\nthan the number of data points (i.e., there are many more\nusers than artists). Consequently, it is more economical to\nuse the kernel matrix representation than to work directly\non the feature vectors.\n4.3 Results\nWe reproduced the main experiment of [18], using P2P\ncollaborative ﬁltering data, as well as listener data from\nLast.fm [6]. We ﬁrst pruned both data sets down to the\n412 artists of aset400 [7]. Of these artists, 23 were missing\nfrom P2P, and 5 were missing from Last.fm. Nonetheless,\nwe retain similarity measurements for these artists to main-\ntain comparability with the previously published results.\nAs in [18], the artists (and corresponding similarity mea-\nsurements) are split by 10-fold cross-validation, and theData source Native Learned Restricted\nP2P 0.5610.728 0.741\nLast.fm 0.570 0.760 0.763\nMFCC 0.535 0.620\nBiography 0.514 0.705\nTags 0.705 0.776\nTable 1 . Test accuracy for artist similarity. Native corre-\nsponds to similarity measurements taken from the raw ker-\nnel matrix, and learned corresponds to similarities learned\nby POE. The restricted column reports accuracy achieved\nby testing only on artists observed in the data (389 artists\nfor P2P and 407 for Last.fm). See Section 4.3 for details.\ntraining and test procedure is repeated for each fold. We\nthen calculate the accuracy of the learned embeddings, av-\neraged across all folds. Results are presented in Table 1.\nThe accuracy of similarity predictions may be skewed\ndue to testing on artists for which the data source may have\nno information (i.e., no users shared songs by that artist).\nTo quantify this effect, we also computed accuracy on sim-\nilarity measurements restricted to include only those artists\nobserved in collaborative ﬁltering data. These results are\ngiven in the restricted column of Table 1.\nOverall, Table 1 indicates that both P2P and Last.fm\ncollaborative ﬁltering data captures a great deal of high-\nlevel artist similarity information. Both sources perform\ncomparably to highly detailed social tags (Tags), and both\noutperform similarity models derived from artist biogra-\nphies (Biography) or acoustic content (MFCCs) as reported\nin [18].\nIn this experiment, the Last.fm data achieves slightly\nhigher accuracy than the P2P data. However the difference\nis quite small, and might be eliminated by using a larger\nsample of P2P users (we only used 7.69%). Also note that\nthe results dramatically improve once the embedding is\ntrained. This emphasizes the importance of learning an op-\ntimal similarity space, rather than using a pre-determined\nsimilarity function as in [3].\n5. SUMMARY\nWe reviewed the latest P2P based MIR studies, and pre-\nsented a new Gnutella-based data collection system. We\nevaluated the information content of our P2P data-set on\nan artist similarity prediction task based on the Partial Or-\nder Embedding (POE) presented in [18], and compared it\nto the “traditional” data sources, such as Last.fm collabora-\ntive ﬁltering, tags, and acoustic models. We showed that a\nP2P based Collaborative Filtering data-set performs com-\nparably to “traditional” data-sets, yet maintains some in-\nherent advantages such as scale, availability and additional\ninformation features such as ID3 tags and geographical lo-\ncation.\nAccording to the International Federation of the Phono-\ngraphic Industry (IFPI) 95% of all music is downloaded in\nﬁle sharing networks [20]. We expect that as the practice\nof ﬁle-sharing becomes even more widespread, the usage\nof P2P based data-sets will become increasingly relevant.\n157\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)6.REFERENCES\n[1]\nEytan Adar and Bernardo A. Huberman. Free riding on\ngnutella. First Monday , 5, 2000.\n[2] Luke Barrington, Reid Oda, and Gert Lanckriet.\nSmarter than genius? human evaluation of music rec-\nommender systems. In International Symposium on\nMusic Information Retrieval , 2009.\n[3] Adam Berenzweig, Beth Logan, Daniel P. W. Ellis, and\nBrian Whitman. A large-scale evaluation of acoustic\nand subjective music similarity measures. In Computer\nMusic Journal , 2003.\n[4] Sudip Bhattacharjee, Ram Gopal, Kaveepan\nLertwachara, and James R. Marsden. Whatever\nhappened to payola? an empirical analysis of online\nmusic sharing. Decis. Support Syst. , 42(1):104–120,\n2006.\n[5] Sudip Bhattacharjee, Ram D. Gopal, Kaveepan\nLertwachara, and James R. Marsden. Using P2P shar-\ning activity to improve business decision making:\nproof of concept for estimating product life-cycle.\nElec. Commerce Research and Applications , 4(1):14–\n20, 2005.\n[6] O. Celma. Music Recommendation and Discovery in\nthe Long Tail. PhD thesis, Universitat Pompeu Fabra,\nBarcelona, Spain, 2008.\n[7] Daniel P. W. Ellis and Brian Whitman. The quest\nfor ground truth in musical artist similarity. In Inter-\nnational Symposium on Music Information Retrieval ,\npages 170–177, 2002.\n[8] Jerome Friedman, Trevor Hastie, and Robert Tibshi-\nrani. Additive logistic regression : A statistical view of\nboosting. Annals of statistics, 28(2):337–407, 2000.\n[9] Adam Shaked Gish, Yuval Shavitt, and Tomer Tankel.\nGeographical statistics and characteristics of p2p query\nstrings. In International Workshop on Peer-to-Peer\nSystems, February 2007.\n[10] J. Grace, D. Gruhl, K. Haas, M. Nagarajan, C. Robson,\nand N. Sahoo. Artist ranking through analysis of online\ncommunity comments. International World Wide Web\nConference , 2008.\n[11] Mihajlo A. Jovanovic. Modeling large-scale peer-to-\npeer networks and a case study of gnutella. Master’s\nthesis, University of Cincinatti, Cincinatti, OH, USA,\n2001.\n[12] J. Kim, B. Tomasik, and D. Turnbull. Using artist\nsimilarity to propagate semantic information. In Proc.\nInternational Symposium on Music Information Re-\ntrieval , 2009.\n[13] Noam Koenigstein and Yuval Shavitt. Song ranking\nbased on piracy in peer-to-peer networks. In Inter-\nnational Symposium on Music Information Retrieval ,\nKobe, Japan, October 2009.\n[14] Noam Koenigstein, Yuval Shavitt, and Tomer Tankel.\nSpotting out emerging artists using geo-aware analysis\nof p2p query strings. In The 14th ACM SIGKDD In-\nternational Conference on Knowledge Discovery and\nData Mining, pages 937–945, Las Vegas, NV , USA,\n2008.\n[15] Noam Koenigstein, Yuval Shavitt, Tomer Tankel, Ela\nWeinsberg, and Udi Weinsberg. A framework for ex-tracting musical similarities from peer-to-peer net-\nworks. In IEEE International Conference on Multime-\ndia and Expo (ICME 2010), Singapore, July 2010.\n[16] Noam Koenigstein, Yuval Shavitt, Ela Weinsberg, and\nUdi Weinsberg. On the applicability of peer-to-peer\ndata in music information retrieval research. In Pro-\nceedings of the 11th International Society for Mu-\nsic Information Retrieval Conference (ISMIR 2010),\nUtrecht, the Netherlands, August 2010.\n[17] Noam Koenigstein, Yuval Shavitt, and Noa Zilberman.\nPredicting billboard success using data-mining in p2p\nnetworks. In ISM ’09: Proceedings of the 2009 11th\nIEEE International Symposium on Multimedia, De-\ncember 2009.\n[18] Brian McFee and Gert Lanckriet. Heterogeneous em-\nbedding for subjective artist similarity. In International\nSymposium on Music Information Retrieval , Kobe,\nJapan, October 2009.\n[19] Brian McFee and Gert R.G. Lanckriet. Partial order\nembedding with multple kernels. In Proceedings of\nthe 26th annual International Conference on Machine\nLearning (ICML), pages 721–728, 2009.\n[20] IFPI: International Federation of the Phonographic In-\ndustry. Digital music report 2009.\n[21] Ars Technica Report on P2P File Shar-\ning Client Market Share. http://\narstechnica.com/old/content/2008/04/\nstudy-bittorren-sees-big-growth-limewire\\\n-still-1-p2p-app.ars, 2008.\n[22] J. R. Quinlan. Learning with continuous classes. pages\n343–348, 1992.\n[23] Amir H. Rasti, Daniel Stutzbach, and Reza Rejaie. On\nthe long-term evolution of the two-tier gnutella overlay.\nInIEEE Global Internet Symposium, Barcelona, Spain,\nApril 2006.\n[24] Matei Ripeanu, Ian Foster, and Adriana Iamnitchi.\nMapping the gnutella network: Properties of large-\nscale peer-to-peer systems and implications for system\ndesign. IEEE Internet Computing Journal , 6, 2002.\n[25] Yuval Shavitt and Udi Weinsberg. Song clustering us-\ning peer-to-peer co-occurrences. In ISM ’09: Proceed-\nings of the 2009 11th IEEE International Symposium\non Multimedia, December 2009.\n[26] Haijian Shi. Best-ﬁrst decision tree learning. Master’s\nthesis, University of Waikato, Hamilton, NZ, 2007.\nCOMP594.\n[27] K. Sripanidkulchai. The popularity of gnutella queries\nand its implications on scalability, February 2001. Fea-\ntured on O’Reilly’s www.openp2p.com website.\n[28] Daniel Stutzbach and Reza Rejaie. Characterizing\nthe two-tier gnutella topology. SIGMETRICS Perform.\nEval. Rev. , 33(1):402–403, 2005.\n[29] The Gnutella Protocol Speciﬁcation v0.41.\nhttp://www9.limewire.com/developer/\ngnutella_protocol_0.4.pdf, 2010.\n[30] Matei A. Zaharia, Amit Chandel, Stefan Saroiu, and\nSrinivasan Keshav. Finding content in ﬁle-sharing net-\nworks when you can’t even spell. In IPTPS, 2007.\n158\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "On the Applicability of Peer-to-peer Data in Music Information Retrieval Research.",
        "author": [
            "Noam Koenigstein",
            "Yuval Shavitt",
            "Ela Weinsberg",
            "Udi Weinsberg"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1414784",
        "url": "https://doi.org/10.5281/zenodo.1414784",
        "ee": "https://zenodo.org/records/1414784/files/KoenigsteinSWW10.pdf",
        "abstract": "Peer-to-Peer (p2p) networks are being increasingly adopted as an invaluable resource for various music information re- trieval (MIR) tasks, including music similarity, recommen- dation and trend prediction. However, these networks are usually extremely large and noisy, which raises doubts re- garding the ability to actually extract sufficiently accurate information. This paper evaluates the applicability of using data orig- inating from p2p networks for MIR research, focusing on partial crawling, inherent noise and localization of songs and search queries. These aspects are quantified using songs collected from the Gnutella p2p network. We show that the power-law nature of the network makes it relatively easy to capture an accurate view of the main-streams using relatively little effort. However, some applications, like trend prediction, mandate collection of the data from the “long tail”, hence a much more exhaustive crawl is needed. Furthermore, we present techniques for overcoming noise originating from user generated content and for filtering non informative data, while minimizing information loss.",
        "zenodo_id": 1414784,
        "dblp_key": "conf/ismir/KoenigsteinSWW10",
        "keywords": [
            "Peer-to-Peer (p2p) networks",
            "music information retrieval (MIR)",
            "music similarity",
            "recommendation",
            "trend prediction",
            "power-law nature",
            "partial crawling",
            "inherent noise",
            "localization of songs",
            "search queries"
        ],
        "content": "ONTHEAPPLICABILITYOF PEER-TO-PEERDATAIN\nM\nUSICINFORMATIONRETRIEVALRESEARCH\nNoam Koenigstein1, YuvalShavitt1, ElaWeinsberg2, and UdiWeinsberg1\n1School ofElectricalEngineering,Tel-AvivUniversity\n2Dept. ofIndustrialEngineering,Tel-AvivUniversity\nABSTRACT\nPeer-to-Peer(p2p)networksarebeingincreasinglyadopted\nasaninvaluableresourceforvariousmusicinformationre-\ntrieval(MIR)tasks,includingmusicsimilarity,recommen-\ndation and trend prediction. However, these networks are\nusually extremelylarge and noisy, which raises doubtsre-\ngarding the ability to actually extract sufﬁciently accurate\ninformation.\nThispaperevaluatestheapplicabilityofusingdataorig-\ninating from p2p networks for MIR research, focusing on\npartial crawling, inherent noise and localization of songs\nandsearchqueries. Theseaspectsarequantiﬁedusingsongs\ncollected from the Gnutella p2p network. We show that\nthe power-law nature of the network makes it relatively\neasytocaptureanaccurateviewofthemain-streamsusing\nrelatively little effort. However, some applications, like\ntrend prediction, mandate collection of the data from the\n“longtail”,henceamuchmoreexhaustivecrawlisneeded.\nFurthermore, we present techniques for overcomingnoise\noriginating from user generated content and for ﬁltering\nnoninformativedata,whileminimizinginformationloss.\n1. INTRODUCTION\nPeer-to-Peer(p2p)networksarebeingincreasinglyadopted\nasaninvaluableresourceforvariousmusicinformationre-\ntrieval (MIR) tasks [11], including music and user simi-\nlarity [3,5,15],recommendation[16],ranking[9,14],and\neven trend prediction[10,12]. Variousinformationcan be\nextracted from a p2p network, including ﬁles shared by\nusers,searchqueries,andspatialandtemporalchangesthat\ntake placein thenetwork.\nThis type of informationis traditionally extracted from\nserver-basedservices, such asLast.FM andYahoo! Music\nservices. Web based services have the potential to pro-\nvide a complete view of their data, either by commercial\nagreements or by crawling using a centralized interface.\nHowever,while p2p networkshave practically unbounded\ngrowth potential, web-based services are often limited in\nsize. This limitation is problematic for collaborative ﬁl-\ntering techniques, that were shown to out-performcontent\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom useis granted without fee provided that copies are\nnotmadeordistributed forproﬁtorcommercialadvantageandthatcopies\nbear this notice and the full citation on theﬁrstpage.\nc/circlecopyrt2010 International Society for MusicInformation Retrieval.basedapproaches,giventhatthedatasetusedissufﬁciently\ncomprehensive[2].\nAnotheradvantageofp2pdatasetsovertraditionaldatasets\nis the availability of information, mitigating the need for\nagreementswithwebsiteoperatorsandvariousrestrictions\nthey pose on the data usage. Due to their decentralized\nnature and open protocols, p2p networks are a source for\nindependentlargescale datacollection.\nDespitealltheiradvantages,p2pnetworksarequitecom-\nplex,makingthe collectionof a comprehensivedataset far\nfrom being trivial, and in some cases practically unfeasi-\nble. First, p2p networks have high user churn, causing\nusers to constantly connect and disconnect from the net-\nwork, being unavailable for changing periods. Second,\nusers in p2p networks often do not expose their shared\ndata in order to maintain high privacy and security mea-\nsures, therefore disabling the ability to collect informa-\ntion about their shared folders. Finally, users often delete\nsharedﬁlestosavespacemakingitinvisibletoacrawlbe-\ningperformedafterthe deletion.\nIt is yet unknown to what extent data that is collected\nfrom large-scale p2p networks actually represents sufﬁ-\nciently accurate information in general, and particularly\nfrom a MIR point of view. The objective of this work is\ntobridgethisgapbyanalyzingtheefﬁciencyandextentof\ncrawling required for obtaining accurate information for\nvarious MIR tasks. We focus on sufﬁcient sampling in a\nsparsedomainwith alongtail ofcontentdistribution.\nIn order to understand how well the crawl captures the\nunderlyingnetwork, we performan empirical study of the\nutility of an exhaustive crawl relative to a partial crawl.\nWhen discussing shared ﬁles, a partial crawl means that\nnot all users are reached, resulting in not all songs being\ncollected. Additionally, in the context of search queries,\nonlyaportionofthequeriesarecollectedsinceitispracti-\ncally impossible to collect all queriesin a fully distributed\np2pnetwork.\nWe ﬁnd that some of the graphsmodelingp2p network\ndata exhibit a power-law [1] distribution. This distribu-\ntion indicates that collecting the majority of popular ﬁles\nand extracting accurate information for the main-streams,\nis relatively easy. By collecting the high degree nodes,\nwhich are easily reached, one may extract an abundance\nof information regarding the core of the network. On the\notherhand,reachingmoreexoticnichesorfollowingsmall\nchangesintrendyhitsmandatesamorethroughcrawlwith\nsigniﬁcantlyhighercollectioneffort,asthecollectionpro-\ncess must visit the long “tail” of the distribution. Fur-\nthermore, we observe the existence of geographiclocality\n273\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)of both ﬁles and queries, indicating that applications that\na\nregeographicaware(liketrendprediction[10]),mandate\nsamplingfromdifferentgeographiclocations.\n2. MEASUREMENT INFRASTRUCTURE\nThis section details the architecture of the measurement\nsystem developed to crawl the Gnutella [13] network and\ncollectqueriesina distributedmanner. Althoughtheexact\ndetails are adapted to comply to the Gnutella architecture\nand protocols, similar techniques can be applied to other\np2p networks. As such is Apollo [17], an efﬁcient frame-\nwork forcrawlingthe BitTorrent p2pnetwork,whichuses\na centralized server that collects trackers, enabling it to\nreachrelatedpeersandextractﬁles thatpeerhold.\n2.1 CrawlingandBrowsingSharedFiles\nOur crawler traverses the network as a graph, similar to\nthe method used by web crawlers. The crawler employs\na highlyparalleltechniqueby spawningnumerousthreads\nthat attempt connecting to a set of provided IP addresses.\nGnutellanodesimplementa“Ping-Pong”protocol[18]used\nfordiscoveringnodes,whereanodethatreceivesa“Ping”\nrequestreplieswithinformationaboutadditionalnodesthat\nit isconnectedto. TheresultingIP addressesarefedtothe\nworkerthreadsforfurthercrawling.\nCrawling dynamicp2p networks never reaches a com-\nplete stop, as clients constantly connect and disconnect\nfrom the network, and the crawler keeps discovering new\nIPaddress. Thismeansthatan“exhaustive”crawlisamat-\nter of deﬁnition, i.e., deciding when to stop the crawling\nprocess. We use two stop conditions that deﬁne how ex-\nhaustive the crawl will be: (a) a time constraint, and (b)\nreachingalow rateofnewlydiscoverednodes.\nAttheearlystagesofacrawlwithaninitialsetofroughly\n100ktargetnodeIPaddresses,therateofnewlydiscovered\nnodes increases dramatically and can typically reach over\n300,000 new clients per minute. As the crawling process\nproceeds, discoveryrate slows down until it reachesa few\nhundreds per minute. At this point, the network is almost\nfully covered, and the newly discovered nodes are mostly\nthe onesthat havejoinedthe networkonlyafter thecrawl-\ning operation started, whereas some of the crawled nodes\nalreadyleft thenetwork.\nThebrowsingoperationcloselyfollowsthecrawlingre-\nsultsandoperatesinparallel. Thebrowsingthreadscollect\nactive node IP addresses reported from the crawler, and\nuse a “Query” message [18] to retrieve information about\nthe ﬁles that a nodeshares. Notice that somenodesignore\nthese queriesduetoprivacyorbandwidthconsiderations.\nAlthoughwe do not downloadany of the ﬁles, the task\nof browsingmillionsof sharedfoldersisbandwidthinten-\nsive, andrequireshighbandwidthInternetaccess. Ourde-\nployed system uses a 1 Gbit/s network card connected to\ntwo 2.5Gbit/sSTM-16lines. Despite our fast connection,\nbrowsing takes about 24 hours, whereascrawling ends af-\nter roughly 1 hour. More details on our crawler can be\nfoundin[8].2.2 CollectionofQueries\nTheprocessofquerycollectionishighlydependantonthe\nsearch paradigmthat the p2pprotocolemploys. Fully dis-\ntributedsearches,likeinGnutella,propagatesearchstrings\nbetweenpeers. Whileitispossibletocapturealargequan-\ntity of queries by deploying several hundred “listening”\nnodes, it is not trivial to determine the queries origin (re-\nquired for geographical location). The basic problem in\nidentifyingtheoriginofcapturedqueriesisthatqueriesdo\nnotingeneralcarrytheiroriginIP address. Mostpeersare\n“hidden” behind a ﬁrewall, hence it is impossible to send\nthe results directlyto them. Instead, proxypeersthat have\nroutable IP address (in Gnutella – Ultrapeers) are used to\nconveytheinformationforﬁrewalledpeers.\nIn cases where geographic query analysis is required,\nthis usage of ultrapeerscauses a difﬁculty to match a peer\nto its geographic location, since the correlation between\nan ultrapeer geographic location and its attached peers is\nlow [7,10]. The authors suggest a method to determine\nqueries origin IP, based on the number of hops they tra-\nversed. Our geographical resolution is based on a similar\ntechnique. Moredetailscanbefoundin [7].\nAlternatively,somenetworks,e.g. BitTorrent,employa\ncentralizedsearchengine,whichisoperatedbywebservers.\nUserssearchforcontentusingawebinterface,ﬁnd“track-\ners” and use them to ﬁnd active peers that hold the re-\nquested ﬁles. This technique greatly simpliﬁes the data\ncollectioneffort. However,itmandatescooperationofweb\nsite operators, which are often reluctant to share informa-\ntionontheirusers.\n3. SONGDISTRIBUTION\nWe start by looking at the distribution of songs per users,\nconsideringall usersin the dataset,and onlyusersthat are\nlocated in the US. For this end, we consider only music\nﬁles sharedbyusers,namelyﬁlesendingwith .mp3,.wav,\n.midand.wma.\nFigure1(a) shows that all users and US-only users ex-\nhibit a power-law [1] distribution, with a very strong cut-\noff around the middle of the plot. This indicates that the\nvast majority of users share less than 300 songs, whereas\nonly several thousandsof users share more than 1k songs.\nNotice that only a few users share more than 10k music\nﬁles, whileover45kusersshareonlya singlesong.\nThesetwoextremespresentdifferentaspectsof“noise”.\nThefew“heavysharers”arenotinformative,whilethelat-\nter simply contribute to a very long tail that is hardly in-\nsightful. In collaborative ﬁltering for example, users that\nshareonlyonesong,contributenosimilarityrelations,while\nusers that share songs from thousandsof artists, are likely\nto “pollute” the dataset with false relations, since they ap-\npearto“like everything”.\nNext, we look a the popularity distribution of songs,\nby counting the number of different users that share each\nsong. Figure1(b) shows a clear power-law distribution\ncontaininga long tail, whichis attributedto popularsongs\nthat are shared by many users. The percentage of popu-\nlarsongssharedbymanyusersisslightlylowerintheUS,\nyet the two distributions mostly overlap. There are a few\nextremely popular songs shared by more than 10k users,\n274\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)10010110210310410510−510−410−310−210−1100101102\nNumber of shared songsPercent of users\n  \nAll users\nUSA users\n(a) Shared songs10010210410610−610−410−2100102\nNumber of sharing usersPercent of songs\n  \nAll users\nUSA users\n(b) Song popularity\nF\nigure1. Distributionsofsharedsongsandsongpopularity\nwhilethevastmajorityofthesongsaresharedbylessthan\n1k users. Consideringthat there are over1.2 million users\nin the dataset, songs that are shared by less than 1k users\narequiteborderlineforbeingconsidered“popular”.\nTheﬁgurealsoshowsthattherearemanysongsthatare\nshared by less than 100 users, which means that reaching\nthem, or recording their relations to other songs, requires\nan extensive crawl. These songs surely do not represent\nanysigniﬁcantmain-streamartistorgenre,butforthepur-\nposeofdetectinghypesorﬁndingsmallcommunitieswith\nveryspeciﬁcpreferences,reachingtheseusersandcollect-\ningthesesongsmightbeimportant.\nGiventhesedistributions,we wishto evaluatethenum-\nber ofnew songsthat are discoveredasmoreusersare be-\ning crawled. Two difﬁculties arise regardingthis analysis.\nTheﬁrst is thewayto identifythattwo ﬁlesare indeedthe\nsamesong,andforthisend,eithertheﬁlehashorthemeta-\ndatacanbeutilized. Usingtheﬁle hashisstraightforward,\nas everyﬁle in the p2p network has a ﬁle hash, taken over\nits content. However, there can be many slightly different\ncopies of the same ﬁle, each with a different hash, mostly\ndue to different disc ripping software or originating song.\nOn the other hand,metadata is often missing and contains\ndifferent spelling mistakes, hence it can also result in in-\ncorrectidentiﬁcationofsimilar songs.\nTherefore, we used both ﬁle hash and metadata tech-\nniquesforidentiﬁcationofuniquesongs. First,wejustuse\nthe ﬁle hash as the song id, and when hashes are exactly\nthe same, we consider them as the same song. When us-\ningmetadata,weconsideronlysongsthathaveboth“title”\n(nameofthesong)and“artist”tags,andusetheirconcate-\nnationasthe songid.\nThe second difﬁculty is that many songs appear only\nonce in the dataset. These are mostly attributed to faulty\nmusic ﬁle (not necessarily songs) that were uploaded by\nusers and are of no interest to other users, rendering these\nﬁles is useless for most MIR tasks. Therefore, we ﬁrst\ncountedthe numberof occurrencesof each song,once us-\ning ﬁle hash andthen usingmetadata, andremovedall the\nsongsthathaveonlya singleappearancein thedataset.\nFigure2showsthenumberofuniquesongspernumber\nof crawled users, showing all users and US-based users.\nThe order of users was randomly selected to reduce spa-tial bias. Both ﬁgures show a converging trend, indicat-\ning that the utility of crawling many users decreases. Fur-\nthermore,theconvergencewitnessedwhenusingmetadata\nseems faster than when using ﬁle hashes, indicating that\nﬁlehashesaremorenoisythanthemetadata. Alternatively,\nthis can be attributed to the observation that roughly 75%\nof the songs did not have both title and artist tags present,\nhence were removed from the analysis. This contributes\nto the reduction of “noise” resulting in a more stable and\nquicklyconvergingset ofsongs.\nTheconvergenceobservedwhencrawlingonlyUS-based\nusers(56%oftheusers)seemsslowerthanwhencrawling\nall users. Looking back at the distribution of songs per\nusers (Figure1(a)) shows that US users tend to have more\nsongs, i.e., higherpercentageof users have morethan 200\nshared songs. This explainsthe slower convergence,since\ntheprobabilitythatauserwillcontributepreviouslyunseen\nsongs is higher. The number of songs seen in US-based\nshared foldersis only half ofthe entire worldwide collec-\ntion. However the usage of metadata over hash for songs\nidentiﬁcationseemtobeaseffectiveasinthegeneralcase,\nsince thepercentageofnoisereductionremainsthesame.\n4. SONGCONNECTIVITY\nItem-basedrecommendationsystemsrequireanestimation\nofthedistancebetweensongs. Thistaskisoftenperformed\nusing expensive content-based similarity. However, song\nsimilarity can be efﬁciently extracted from p2p networks,\nby transforming the bipartite graph that connects users to\nsongsintoa1-modesong-similaritygraph,wheretheweight\nof a linkwijbetween two songs iandjis the number of\nusersthathavebothsongsintheirsharedfolders.\nIn this analysis we wish to obtain a stable similarity\ngraph,thereforewe domoreprocessingto identifyunique\nsongs. Similar to the previous analysis, all the songs that\nhave hash value that appearedonly once are removed. We\nthen group together all ﬁle hashes that relate to identical\nmetadata value (artist and title). At this stage we have\ngroupedtogetherdifferentdigitalversionsofthesamesong.\nAccounting for spelling mistakes is achieved by grouping\ntogether artist and title values that have a small edit dis-\ntance[19](countinginsert,deleteandsubstitute). Thedis-\n275\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)0 2 4 6 8 10 12\nx 10500.511.522.5x 107\nNumber of usersNumber of songs\n  \nHash\nMetadata\n(a) Allusers0 1 2 3 4 5\nx 10502468101214x 106\nNumber of usersNumber of songs\n  \nHash\nMetadata\n(b) US-based users0 0.5 1 1.5 2 2.5 3\nx 10600.511.522.533.544.5x 106\nNumber of usersNumber of queries\n  \nAll queries\nAppearance > 1\n(c) Unique queries\nF\nigure2. Numberofuniquesongs(usingﬁle hashandmetadata)anduniquequeriesvs. numberofuserscrawled\ntance threshold is determined by a function of the string’s\nlength. Representative metadata values are chosen using\nmajority voting. Finally, after this aggregation, all songs\nthat have less than 7 occurrencesare removed. This value\nis a tradeoff between ﬁltering and memory consumption,\ntakingonly3bitsofmemoryforeachsong.\nThisuniﬁcationofsongsreducedthenumberofunique\nsongs from over 21 million when using hashes and 5 mil-\nlion when using metadata to 530k songs, meaning only\n2.5%ofthesongsusinghashandroughly10%ofthesongs\nusingmetadata. Althoughthistechniquecanslightlyover-\nﬁlter,it successfullyovercomesthelowsignal-to-noisera-\ntio thatinherentlyexistsinthe p2pnetwork,primarilydue\nto usergeneratedcontent.\nWefurtherperformﬁlteringof“weak”song-to-songre-\nlations, to remove noise as the one witnessed in the pres-\nence of extremely “heavy sharers”. During the collection\nofsongsweonlyincludelinksthatappearinatleast16dif-\nferent users, a valueswhich was again selected as a trade-\noff between ﬁltering and memory consumption. Then, we\nkept for each ﬁle, only the top 40% links (ordered by de-\nscending similarity value) and not less than 10. Notice\nthatthisﬁlteralsoremovesmaliciousandspamsongsfrom\nthe graph, assuming that these are not downloaded by too\nmanyusers. Aftertheremovalofthese“weak”links,roughly\n20millionundirectedlinksremainin thegraph.\n4.1 DegreeDistribution\nIntuitively, since some popular songs are shared by many\nusers while many songs are shared by only a few users,\nit is more likely for a song to be co-shared with a popular\nsong,henceincreasingtheconnectivityofthepopularsong\nin the similarity graph. This type of connectivity results\nin a power-law degree distribution, which results in high\ndegreesofthefewpopularsongsandlowerdegreeofmany\nless-popular songs. An important feature of such power-\nlaw distributions is the ability to efﬁciently capture many\nof the underlying graph properties, by sampling a partial\nviewoftheoverallnetwork.\nOn the other hand, when the “tail” of the power-law\nis long, meaning many songs have very low connectivity,\nthecrawlingeffortandrequiredresourcesaresigniﬁcantly\nhigher. The value of the data that exists in the tail greatly\ndependson the application [4]. Most applicationsdo con-sider such “rare” ﬁles as noise; in that case, their added\nvalueismarginal.\n10010110210310410510−610−410−2100Pc(Degree)\nDegree\nFigure 3. Cumulativedegreedistributionof the song sim-\nilaritygraph\nSeveral previous studies on p2p networks [6,7] show\nthatgraphsthatmodelvariousp2pnetworksexhibitpower-\nlaw distributions. As can be seen in Figure4.1 shows the\ncumulativesongdegreedistributioninthesimilaritygraph,\nexhibiting a power-law with a strong cut-off. This power-\nlawdistributionsuggeststhattherearerelativelyafewsongs\nwithveryhighconnectivityandmanysongswithlowcon-\nnectivity.\n4.2 PartialSampling\nWe wish to verify that partial sampling does not signif-\nicantly alter the distribution of the similarity graph. We\nﬁrst normalizethe similarityvaluebetweenanytwo songs\nso it reﬂects their popularity. Hence, the new similarity\nis/hatwidewij=wij//radicalbig\nPi·Pj, where wijis the link weight be-\ntween songs iandj, andPi,Pjare their corresponding\noverallnumberofoccurrences(popularity).\nWe then create a new graph, denoted by TR N, which\ncontains, for each ﬁle, only the top Nneighbors, ordered\nby non increasing normalized similarity. This extends the\nbasic ﬁlters since it uses the normalized similarity values,\nthuscapturingtherelativepopularityofadjacentﬁles. This\nﬁlter is analogousto the effect of a partial sampling in the\n276\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)(a) Toprelations100101102103104100101102103104105106\nDegreeNumber of songs\n  \nTR1\nTR5\nTR10\nTR20\nTR50\n(b) Degree distribution\nF\nigure4. Effectofsamplingonsongsimilaritydistribution\np2p network, where many users are simply not reached\nduring the crawling phase. In this case, the crawl “skips”\nmany of the weak relations between songs, while keep-\ning only the strong ones that appear in many users. We\ntherefore, wish to evaluate the way the similarity graph is\naffectedbypartialsampling.\nThe number of times each song appears as the nearest\nneighborfordifferentvaluesof NispresentedinFigure4(a).\nThe ﬁgure shows that for N=1,5 the distributions are sig-\nniﬁcantly different, whereas for N≥10the distributions\nalmost overlap. Similar results can be seen when looking\nat the degree distribution depictedin Figure4(b). The ﬁg-\nureshowsthat whilefor N=1thedistributionis extremely\nsparse,reaching N≥10resultsinanalmostidenticaldis-\ntributionwith slightlyhighernodedegrees.\nThe above results indicate that obtaining partial infor-\nmation on the network is sufﬁcient for generating a com-\nprehensivesimilaritygraph,astheutilityofhavingamore\ncomplete view of the network quickly decreases. This is\nattributed to the fact that the songs that are most affected\nfrom this partial crawl are the high-degreesongs (best no-\nticed in Figure4(b)). Since many links are gone, songs\nthat did not have too many links to begin with, are hardly\naffected, while songs that had many links “lose” a lot of\nthem. However, when enough links remain (a sufﬁcient\nnumberof users that share these songsare crawled), these\nsongsretaintheirhighdegreerelativeto theothersongs.\n5. QUERY COLLECTION\nCollection of queries is often a much more complicated\ntask than crawling the shared folders. Hence, we seek\nto quantify the utility of collecting queries from an in-\ncreasing number of users, similar to the way we did for\nuniquesongs. Forthisend,wecollectedalmost4.5million\nqueriesfrom over 3 million users duringa week in Febru-\nary 2007. Notice that these queries are not related only to\nmusic, however analysis of keywords used for searching\ntheGnutellanetworkshowsthatalmost70%ofthequeries\naremusicrelated[10].\nFigure2(c) depicts the number of unique queries per\nnumber of crawled users, using all the queries, and us-ing only queries that appeared more than once. The ﬁg-\nure shows that when all the queries are considered, there\nis no convergence,meaning that each additional user con-\ntributes some new queries. However, when we consider\nonlyqueriesthat appearedmorethanonce,there is a clear\nconvergence,andtheoverallnumberofuniquequeriesgoes\ndowntolessthan2million. Wetherefore,learnthatthedi-\nversity in search terms is mostly attributed to very “rare”\nstringsthatoriginatefromsingleusers,whereasthemajor-\nityofthecommonqueriesarefrequentlyrepeatingamongst\nthedifferentusers,hencecanbe moreeasilyreached.\nQueries were shown to be highly correlated with geo-\ngraphic location [7], which is rather intuitive considering\nthe cultural and language differences between countries.\nIn order to quantify the implications of localized query\ncollection, we compared the top-1000 string queries per-\nformed by users in different countries, and deﬁne the cor-\nrelationasthetotalnumberofmatchingstrings.\n2 4 6 8 10 12 14 160100200300400500600\nWeeksCorrelation\n  \nAU\nUK\nCN\nDE\nFR\nIT\nJP\nRU\nFigure5. Correlationbetweentop-1000searchqueriesbe-\ntweentheUS anddifferentcountriesovertime\nFigure5 depicts the correlation factor between the US\nand other countries over a period of 17 weeks in early\n2007. Theﬁgureshowsthat,asexpected,theEnglishspeak-\ning countries (Australia and United Kingdom) have much\nhighercorrelationwiththeUSthanthenon-Englishspeak-\ning countries. Japan appears to have the lowest overall\n277\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)correlation, with less than 20 matching queries. Interest-\ni\nngly,the correlationis quite consistent overthe entirepe-\nriod, showing profound differences between the Anglo-\nsphere and the non-English speaking countries. Putting\naside the musical anthropology aspects of these results,\nthis analysis indicates that when performing targeted re-\nsearch, it is sufﬁcient to focus on a bounded geographi-\ncal region or country. However, conclusions drawn using\nqueries collected in a speciﬁc region should be carefully\nexaminedbeforeassumingthem onothergeographicallo-\ncations.\n6. DISCUSSION AND CONCLUSION\nIn the presence of an increasing demand for large scale\ndatasets in MIR research, this paper investigates the dif-\nferentconsiderationsin usinga p2pbaseddataset. Several\ndifﬁculties are considered– the inability to crawl all users\nandcollectinformationonallsongs,thecomplexitiesinin-\nterceptingall search queriesand the inherentnoise of user\ngeneratedcontent.\nContentdistributioninap2pnetworkstypicallyexhibits\napower-law,hencecollectingthemajorityofsongsisrather\neasy. Partial crawling is shown to have much less impact\non the availability of main-streamcontentthan on speciﬁc\n“niches”. On the other hand, when popularity is consid-\nered, partial sampling is more likely to effect the popular\nsongs. Although their relative popularity decreases, song-\nto-songrelationsremainintact.\nSpatial analysis reveals that p2p networks are highly\nlocalized, with profound differences in songs and queries\nbetweengeographicalregions. Thiscanhelpinducelocal-\nizedresearchregardingmusicaltrendsandpreferences,but\nmandatescarefulconsiderationbeforeinferringconclusion\ndrawnfromlocal samples.\nFile sharing networks were shown to have low signal-\nto-noiseratio,mandatingcarefuldataprocessingwhencom-\npared to “traditional” datasets (e.g., website). In order to\nimprove the ability to extract insightful information from\nthedata,wesuggestremovingsongsthatappearonlyonce\nin the dataset, and users that share too many songs, there-\nfore,removingtheextremesthatarenotinsightfulandmay\n“pollute”thedataset. Furthermore,wepresentmethodsfor\nsong identiﬁcation that help merge similar songs, further\nimproving the signal-to-noise ratio. This extensive ﬁlter-\ning can be applied to reduce redundant records and false\nrelations, but may result in loss of data, which can be of\ninteresttosomeMIRtasks,suchaspopularitypredictions.\nOverall, p2p networks provide an abundance of infor-\nmation that can be utilized in MIR research. Main-stream\ndatacanbeeasilycollectedfromp2pnetworks,whilehav-\ning all the beneﬁts over standard website data. However,\nwhen seeking to harness the power of the long tail, where\np2p networks have a signiﬁcant advantage, careful analy-\nsis is key for sufﬁcient noise reduction while maintaining\nrelevantinformation.\nAcknowledgment . Thisresearchwassupportedinpartby\na grant from the Israel Science Foundation(ISF) center of\nexcellenceprogram(grantnumber1685/07).7. REFERENCES\n[1] Albert-L´ aszl´ o Barab´ asi and R´ eka Albert. Emergence\nof scaling in random networks. SCIENCE , 286:509 –\n512,1999.\n[2] Luke Barrington, Reid Oda, and Gert Lanckriet.\nSmarter than genius? human evaluation of music rec-\nommendersystems. In ISMIR,2009.\n[3] AdamBerenzweig,BethLogan,DanielP.W.Ellis,and\nBrian Whitman. A large-scale evaluation of acoustic\nandsubjectivemusicsimilaritymeasures.In Computer\nMusicJournal ,2003.\n[4] Oscar Celma and Pedro Cano. From hits to niches?\norhowpopularartistscanbiasmusicrecommendation\nand discovery. In 2nd Workshop on Large-Scale Rec-\nommenderSystems ,2008.\n[5] Daniel P. W. Ellis and Brian Whitman. The quest for\nground truth in musical artist similarity. In ISMIR,\n2002.\n[6] F. Le Fessant, A. M. Kermarrec, and L. Massoulie.\nClustering in peer-to-peer ﬁle sharing workloads. In\nIPTPS,2004.\n[7] Adam Shaked Gish, Yuval Shavitt, and Tomer Tankel.\nGeographicalstatisticsandcharacteristicsofp2pquery\nstrings. In IPTPS,2007.\n[8] Noam Koenigstein, Gert Lanckriet, Brian McFee, and\nYuvalShavitt.Collaborativeﬁlteringbasedonp2pnet-\nworks.In ISMIR,Utrecht,theNetherlands,2010.\n[9] Noam Koenigstein and Yuval Shavitt. Song ranking\nbased on piracy in peer-to-peer networks. In ISMIR,\n2009.\n[10] Noam Koenigstein, Yuval Shavitt, and Tomer Tankel.\nSpottingoutemergingartistsusinggeo-awareanalysis\nofp2pquerystrings. In KDD, 2008.\n[11] Noam Koenigstein, Yuval Shavitt, Ela Weinsberg, Udi\nWeinsberg, and Tomer Tankel. A framework for ex-\ntracting musical similarities from peer-to-peer net-\nworks.In AdMIRe,Singapore,July2010.\n[12] NoamKoenigstein,YuvalShavitt,andNoaZilberman.\nPredicting billboard success using data-mining in p2p\nnetworks.In AdMIRe,2009.\n[13] Matei Ripeanu. Peer-to-peer architecture case study:\nGnutellanetwork,2001.\n[14] MarkusSchedl,TimPohle,NoamKoenigstein,andPe-\nter Knees. What’s Hot? Estimating Country-Speciﬁc\nArtist Popularity. In ISMIR, Utrecht, the Netherlands,\nAugust2010.\n[15] YuvalShavitt, Ela Weinsberg, and Udi Weinsberg. Es-\ntimating peer similarity using distance of shared ﬁles.\nInIPTPS,2010.\n[16] Yuval Shavitt and Udi Weinsberg. Song clustering us-\ningpeer-to-peerco-occurrences.In AdMIRe,2009.\n[17] Georgos Siganos, Josep Pujol, and Pablo Rodriguez.\nMonitoringthe Bittorrentmonitors: A bird’seye view.\nInPAM,2009.\n[18] The Gnutella Protocol Speciﬁcation v0.41.\nhttp://www9.limewire.com/developer/\ngnutella_protocol_0.4.pdf ,2010.\n[19] Robert A. Wagner and Michael J. Fischer. The string-\nto-string correction problem. J. ACM, 21(1):168–173,\n1974.\n278\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "A Multi-Perspective Evaluation Framework for Chord Recognition.",
        "author": [
            "Verena Konz",
            "Meinard Müller",
            "Sebastian Ewert"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1415686",
        "url": "https://doi.org/10.5281/zenodo.1415686",
        "ee": "https://zenodo.org/records/1415686/files/KonzME10.pdf",
        "abstract": "The automated extraction of chord labels from audio recordings constitutes a major task in music information retrieval. To evaluate computer-based chord labeling pro- cedures, one requires ground truth annotations for the un- derlying audio material. However, the manual generation of such annotations on the basis of audio recordings is te- dious and time-consuming. On the other hand, trained mu- sicians can easily derive chord labels from symbolic score data. In this paper, we bridge this gap by describing a pro- cedure that allows for transferring annotations and chord labels from the score domain to the audio domain and vice versa. Using music synchronization techniques, the gen- eral idea is to locally warp the annotations of all given data streams onto a common time axis, which then allows for a cross-domain evaluation of the various types of chord labels. As a further contribution of this paper, we extend this principle by introducing a multi-perspective evaluation framework for simultaneously comparing chord recogni- tion results over multiple performances of the same piece of music. The revealed inconsistencies in the results do not only indicate limitations of the employed chord labeling strategies but also deepen the understanding of the under- lying music material.",
        "zenodo_id": 1415686,
        "dblp_key": "conf/ismir/KonzME10",
        "keywords": [
            "automated extraction",
            "music information retrieval",
            "ground truth annotations",
            "manual generation",
            "time-consuming",
            "trained musicians",
            "bridge gap",
            "music synchronization",
            "cross-domain evaluation",
            "multi-perspective evaluation"
        ],
        "content": "AMULTI-PERSPECTIVEEVALUATIONFRAMEWORKFORCHORD\nRE\nCOGNITION\nVerenaKonz\nSaarlandUniversity\nandMPIInformatik\nvkonz@mpi-inf.mpg.deMeinardM¨uller\nSaarlandUniversity\nandMPIInformatik\nmeinard@mpi-inf.mpg.deSebastianEwert\nComputerScienceIII\nUniversityofBonn\newerts@iai.uni-bonn.de\nABSTRACT\nThe automated extraction of chord labels from audio\nrecordingsconstitutesamajortaskinmusicinformation\nretrieval.Toevaluatecomputer-basedchordlabelingpro-\ncedures,onerequiresgroundtruthannotationsfortheun-\nderlyingaudiomaterial.However,themanualgeneration\nofsuchannotationsonthebasisofaudiorecordingsiste-\ndiousandtime-consuming.Ontheotherhand,trainedmu-\nsicianscaneasilyderivechordlabelsfromsymbolicscore\ndata.Inthispaper,webridgethisgapbydescribingapro-\ncedurethatallowsfortransferringannotationsandchord\nlabelsfromthescoredomaintotheaudiodomainandvice\nversa. Usingmusicsynchronizationtechniques,thegen-\neralideaistolocallywarptheannotationsofallgivendata\nstreamsontoacommontimeaxis,whichthenallowsfor\nacross-domainevaluationofthevarioustypesofchord\nlabels. Asafurthercontributionofthispaper,weextend\nthisprinciplebyintroducingamulti-perspectiveevaluation\nframeworkforsimultaneouslycomparingchordrecogni-\ntionresultsovermultipleperformancesofthesamepiece\nofmusic.Therevealedinconsistenciesintheresultsdonot\nonlyindicatelimitationsoftheemployedchordlabeling\nstrategiesbutalsodeepentheunderstandingoftheunder-\nlyingmusicmaterial.\n1. INTRODUCTION\nInrecentyearsautomatedchordrecognition,whichdeals\nwith the computer-based harmonic analysis of audio\nrecordings,hasbeenofincreasinginterestintheﬁeldof\nmusicinformationretrieval(MIR),seee.g.[1,4,5,7,12,\n14]. Theprincipleofharmonyisacentralattributeof\nWesterntonalmusic,wherethesuccessionofchordsover\ntimeoftenformsthebasisofapieceofmusic.Suchhar-\nmonicchordprogressionsarenotonlyofmusicalimpor-\ntance,butalsoconstituteapowerfulmid-levelrepresenta-\ntionfortheunderlyingmusicalsignalandcanbeapplied\nforvarioustaskssuchasmusicsegmentation,coversong\nidentiﬁcation,oraudiomatching[10,13].\nThe evaluation of chord labeling procedures itself,\nPermissiontomakedigitalorhardcopiesofallorpartofthisworkfor\npersonalorclassroomuseisgrantedwithoutfeeprovidedthatcopiesare\nnotmadeordistributedforproﬁtorcommercialadvantageandthatcopies\nbearthisnoticeandthefullcitationontheﬁrstpage.\nc/circlecopyrt2010InternationalSocietyforMusicInformationRetrieval.whichistypicallydonebycomparingthecomputedchord\nlabelswithmanuallygeneratedgroundtruthannotations,\nisfarfrombeinganeasytask. Firstly,theassignmentof\nchordlabelstospeciﬁcmusicalsectionsisoftenambigu-\nousduetomusicalreasons.Secondly,dealingwithperfor-\nmancesgivenasaudiorecording,thegroundtruthannota-\ntionshavetobespeciﬁedintermsof physicalunitssuch\nasseconds. Thus,specifyingmusicalsegmentsbecomes\nacumbersometask,which,inaddition,hastobedonefor\neachperformanceseparately.Ontheotherhand,musicians\ntrainedinharmonicsarefamiliarwithassigningchordla-\nbelstomusicalsections.However,theanalysisistypically\ndoneonthebasisofmusicalscores,wherethesectionsare\ngivenintermsofmusicalunitssuchasbeatsormeasures.\nWhendealingwithperformedaudiorecordings,suchan-\nnotationsareonlyoflimiteduse.\nAsonemaincontributionofthispaper,weintroduce\nanautomatedprocedurefortransferringannotationsand\nchordlabelsfromthescoredomaintotheaudiodomain\nandviceversa,thusbridgingtheabovementionedgapbe-\ntweenMIRresearchersandmusicians.Giventhescoreof\napieceofmusic,weassumethatmusicalsectionsspec-\niﬁedintermsofbeatsormeasuresarelabeledusingthe\nconventionsintroducedbyHarte[4]. Incasethescore\nisgiveninsomecomputer-readableformatsuchasMu-\nsicXMLorLilyPond[6],recentsoftwareallowsforex-\nportingthescoreintoanuninterpretedMIDIﬁle,where\nthetempoissettoaknownconstantvalue.Thisallowsfor\ndirectlytransferringthescore-basedgroundtruthannota-\ntionstoaMIDI-basedgroundtruthannotation. Wethen\nusemusicsynchronizationtechniques[9]totemporally\naligntheMIDIﬁletoagivenaudiorecording. Finally,\ntheresultingalignmentinformationcanbeusedtotempo-\nrallywarptheaudioannotationsontoacommonmusically\nmeaningfultimeaxis,thusallowingadirectcomparisonto\nthegroundtruthannotations.\nAsasecondcontribution,weextendthisprincipleby\nsuggesting a novel multi-perspectiveevaluation frame-\nwork,wherewesimultaneouslycomparechordrecogni-\ntionresultsovermultipleperformancesofthesamepiece\nofmusic.Inthisway,consistenciesandinconsistenciesin\nthechordrecognitionresultsoverthevariousperformances\narerevealed.Thisnotonlyindicatesthecapabilityofthe\nemployedchordlabelingstrategybutalsoliesthebasisfor\namoredetailedanalysisoftheunderlyingmusicmaterial.\nAsaﬁnalcontribution,weindicatethepotentialofour\nframeworkbygivingsuchdetailedharmonicanalysesby\n9\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)meansofthreerepresentativeexamples.\nTh\neremainderofthispaperisorganizedasfollows.\nFirst,inSect.2wegiveanoverviewaboutmusicsynchro-\nnization.Then,inSect.3wepresentthemulti-perspective\nevaluationframework.InSect.4wedemonstrateourframe-\nworkgivinganin-depthanalysisoftypicalchordrecogni-\ntionerrors.Conclusionsandprospectsonfutureworkare\ngiveninSect.5.\n2. MUSICSYNCHRONIZATION\nForthemethodspresentedinthefollowingsectionsthe\nconceptofmusicsynchronizationisofparticularimpor-\ntance.Ingeneral,thegoalofmusicsynchronizationisto\ndetermineforagivenpositioninoneversionofapiece\nofmusic,thecorrespondingpositionwithinanotherver-\nsion.Mostsynchronizationalgorithmsrelyonsomevari-\nantofdynamictimewarping(DTW)andcanbesumma-\nrizedasfollows. First,twogivenversionsofapieceof\nmusicareconvertedintofeaturesequences,say X:=\n(X1, X2, . . ., X N)andY:= (Y1, Y2, . . . , Y M),respec-\ntively. Inthiscontext,chromafeatureshaveturnedout\ntoyieldrobustalignmentresultseveninthepresenceof\nsigniﬁcantartisticvariations. Inthefollowingweem-\nployCENS(ChromaEnergyNormalizedStatistics)fea-\ntures,avariantofchromafeaturesmakinguseofshort-\ntimestatisticsoverenergydistributionswithinthechroma\nbands,foradetaileddescriptionsee[9]. Additionally,\nweconsidernon-standardtuningssimilartoG´omez[3].\nThen,an N×Mcostmatrix Cisbuiltupbyevaluat-\ningalocalcostmeasure cforeachpairoffeatures,i.e.,\nC(n, m) =c(xn, ym)forn∈[1 :N] :={1,2, . . ., N }\nandm∈[1 :M]. Eachtuple p= (n, m)iscalleda\ncellofthematrix.A(global) alignmentpathisasequence\n(p1, . . ., p L)oflength Lwithpℓ∈[1 :N]×[1 :M]\nforℓ∈[1 :L]satisfying p1= (1,1),pL= (N, M)\nandpℓ+1−pℓ∈Σforℓ∈[1 : L−1]. Here,\nΣ = {(1,0),(0,1),(1,1)}denotesthesetofadmissible\nstepsizes. Thecostofapath (p1, . . . , p L)isdeﬁnedas/summationtextL\nℓ=1C(pℓ). Acost-minimizingalignmentpath,which\nconstitutestheﬁnalsynchronizationresult,canbecom-\nputedviadynamicprogrammingfrom C.Foradetailedac-\ncountonDTWandmusicsynchronizationwereferto[9].\nBasedonthisgeneralstrategy,weemployasynchro-\nnizationalgorithmbasedonhigh-resolutionaudiofeatures\nasdescribedin[2]. Thisapproach,whichcombinesthe\nhightemporalaccuracyofonsetfeatureswiththerobust-\nnessofchromafeatures, generallyyieldsrobustmusic\nalignmentsofhightemporalaccuracy.\n3. MULTI-PERSPECTIVEVISUALIZATION\nAscoreinacomputerreadableformatsuchasLilyPondor\nMusicXMLisavailableformanyclassicalpiecesofmusic\n[11].Foratrainedmusicianitismuchmoreintuitivetoan-\nnotatethechordsofapieceonthebasisoftheunderlying\nscorethanonthebasisofanaudiorecording. However,\nsuchanannotationisnotdirectlytransferabletoanaudio\nrecordingofthesamepiece,asbothuseverydifferentno-\ntionsoftime. Furthermore,thisalsoimpliesthatthisan-notationcannotbeuseddirectlytoevaluatetheresultsof\nanaudio-basedautomaticchordlabelingmethod. Inthis\nsection,wepresentamethodintegratingmusicsynchro-\nnizationtechniques,whichallowsforadirectcomparison\nofchordlabelsderivedfromdifferentversionsofapiece\nofmusic. Thisapproachhasseveraladvantages. Firstly,\nthemanualannotationbecomesmuchmoreintuitive.Sec-\nondly,thepositionofachordrecognitionerrorinanaudio\nrecordingcanbeeasilytracedbacktothecorresponding\npositioninthescore. Thisallowsforaveryefﬁcientin-\ndeptherroranalysisaswewillshowinSect.4.Thirdly,a\nsinglescore-basedannotationcanbetransferedtoanarbi-\ntrarynumberofaudiorecordingsfortheunderlyingpiece.\nInthefollowing,weassumethatanaudiorecordingand\nascoreincomputerreadableformataregivenforapiece\nofmusic. Additionally,chordlabelsmanuallyannotated\nbyatrainedmusicianonthebasisofascorearegivenas\nwellaslabelsautomaticallyderivedfromtheaudiorecord-\ningviasomecomputer-basedmethod. Inaﬁrststep,we\nexportthescoretoaMIDIrepresentation. Thiscanbe\ndoneautomaticallyusingexistingsoftware.Beatandmea-\nsurepositionsarepreservedduringtheexport,suchthat\nthescore-basedannotationsarestillvalidfortheMIDI\nﬁle. Inanextstep,wederiveCENSfeaturesfromthe\nMIDIaswellasfromtheaudioasmentionedinSect.2,\nsayX:= (X1, X2, . . . , X N)andY:= (Y1, Y2, . . ., Y M),\nrespectively. SinceeachCENSfeaturecorrespondstoa\ntimeframe,wecanalsocreatetwobinarychordvectorse-\nquences, A:= (A1, . . . , A N)andB:= (B1, . . . , B M),\nwhichencodethegivenchordlabelsinaframewisefash-\nion. Here, An, Bm∈ {0,1}dforn∈[1 :N]and\nm∈[1 :M].Theconstant dequatesthenumberofcon-\nsideredchords.Avalueofoneinavectorcomponenten-\ncodesthechordprevalentinthecorrespondingtimeframe.\nAsweconsiderinthefollowingonlythe24majorandmi-\nnorchords( d= 24),wehavetomapthegivenchordlabels\ninameaningfulwaytooneofthese.Tothisend,weem-\nploytheintervalcomparisonofthedyad,whichwasused\nforMIREX2009[8]andtakesintoaccountonlytheﬁrst\ntwointervalsofeachchord.Thus,augmentedanddimin-\nishedchordsaremappedtomajorandminorrespectively,\naswellasanyotherlabelhavingamajororminorthirdas\nitsﬁrstinterval.UsingtheﬁrstfourmeasuresofChopin’s\nMazurkaOp. 68No.3asanexample,weillustratethe\nsequences Aforthescoreand BfortheaudioinFig.1(b)\nand1(c),respectively.NotethatinFig.1(b)thetimeisex-\npressedintermsofmeasures,whileinFig.1(c)thetimeis\ngiveninseconds.Thisdifferentnotionoftimepreventsa\ncomparisonof AandBatthispoint.\nThenextstepconsistsofsynchronizingthetwoCENS\nfeaturessequences XandYasmentionedinSect.2.The\nresultingalignmentpath p= (p1, . . ., p L)encodestempo-\nralcorrespondencesbetweenelementsof XandY. Fol-\nlowingthesametimeframedivision,thealignmentpath\nalsoencodescorrespondencesbetweenthesequences A\nandB.Usingthislinkinginformation,welocallystretch\nandcontracttheaudiochordvectorsequence Baccording\ntothewarpinginformationsuppliedby p.Here,wehave\ntoconsidertwocases.Intheﬁrstcase, pcontainsasubse-\n10\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n1 2 3 4 5    CC#/Db    DD#/Eb E/Fb E#/FF#/Gb    GG#/Ab    AA#/Bb B/Cb    cc#/db    dd#/eb e/fb e#/ff#/gb    gg#/ab    aa#/bb b/cb\n−50−40−30−20−1001020304050  \n1 2 3 4 5    CC#/Db    DD#/Eb E/Fb E#/FF#/Gb    GG#/Ab    AA#/Bb B/Cb    cc#/db    dd#/eb e/fb e#/ff#/gb    gg#/ab    aa#/bb b/cb  \n1 2 3 4 5    CC#/Db    DD#/Eb E/Fb E#/FF#/Gb    GG#/Ab    AA#/Bb B/Cb    cc#/db    dd#/eb e/fb e#/ff#/gb    gg#/ab    aa#/bb b/cb  \n0 1 2 3 4 5 6    CC#/Db    DD#/Eb E/Fb E#/FF#/Gb    GG#/Ab    AA#/Bb B/Cb    cc#/db    dd#/eb e/fb e#/ff#/gb    gg#/ab    aa#/bb b/cb  \n1 2 3 4 5    CC#/Db    DD#/Eb E/Fb E#/FF#/Gb    GG#/Ab    AA#/Bb B/Cb    cc#/db    dd#/eb e/fb e#/ff#/gb    gg#/ab    aa#/bb b/cb\n(a)\n(b\n)\n(c)\n(d)\n(e)\n(f)Musicaltime(measures)\nPhysicaltime(seconds)\nMusicaltime(measures)\nMusicaltime(measures)\nMusicaltime(measures)\nFigure1.VariouschordannotationsvisualizedfortheChopin\nMazurkaOp.68No.3(Fmajor),mm.1-4. (a)Score.(b)Score-\nbasedgroundtruthchordlabels. (c)Computedaudiochordlabels\n(physicaltimeaxis). (d)Warpedaudiochordlabels(musicaltime\naxis).(e)Overlayedscoreandaudiochordlabels. (f)Multi-\nperspectiveoverlayofscoreandaudiochordlabels.quenceoftheform\n(n, m),(n+ 1, m), . . . ,(n+ℓ−1, m)\nfor some ℓ∈N, i.e., the ℓscore-related vectors\nAn, . . ., A n+ℓ−1arealignedtothesingleaudio-related\nvector Bm. Inthiscase,weduplicatethevector Bmby\ntaking ℓcopiesofit.Inthesecondcase, pcontainsasub-\nsequenceoftheform\n(n, m),(n, m+ 1), . . . ,(n, m+ℓ−1)\nforsome ℓ∈N,i.e.,thescore-relatedvector Anisaligned\ntoℓaudio-relatedvectors Bm, . . ., B m+ℓ−1.Inthiscase,\nwereplacethe ℓvectorsbythevector Bm+⌊ℓ/2⌋.There-\nsultingwarpedversionof Bisdenotedby ¯B. Notethat\nthelengthof ¯Bequalsthelength NofA,seeFig.1(d).\nForthevisualizationwesetallvectorsin ¯Bto0,whereno\ngroundtruthchordlabelisavailable,asforexampleinthe\nmiddleofmeasure(abbreviatedmm.)4,seeFig.1(d).\nOverall,wehavenowconvertedthephysicaltimeaxis\noftheaudiochordvectorsequence Btothemusically\nmeaningfulmeasureaxis, asusedfor A. Finally, we\ncanvisualizethedifferencesbetweenthescore-basedand\ntheaudio-basedchordlabelsbyoverlaying Aand¯B,see\nFig.1(e). Here,theverticalaxisrepresentsthe24ma-\njor/minorchords,startingwiththe12majorchordsand\ncontinuingwiththe12minorchords.Blueentriesnowin-\ndicateareas,wherethegroundtruthlabelsandtheaudio\nchordlabelscoincide.Onthecontrary,greenandreden-\ncodethedifferencesbetweenthechordlabels.Here,green\nentriescorrespondtothegroundtruthchordlabelsderived\nfromthescore,whereasredentriescorrespondtotheau-\ndiochordlabels.Forexample,atthebeginningofmm.2\nthescoreaswellastheaudiochordlabelsindicateaC\nmajorchord. Onthecontrary,attheendofmm.2there\nisaCmajorchordspeciﬁedinthescore,whilethechord\nlabelsderivedfromtheaudioincorrectlyspecifyanAmi-\nnorchord.Usingthemeasure-basedtimeinformation,we\ncanlookdirectlyatthecorrespondingpositioninthescore\nandanalyzetheunderlyingreasonforthiserror.Wewill\ndemonstratethisprincipleextensivelyinSect.4,wherewe\npresentanin-depthanalysisoftypicalerrorsproducedby\nautomaticchordlabelingmethods.\nNext,weextendthejustdevelopedconceptbyintroduc-\ningamulti-perspectivevisualization,seeFig.1(f). Here,\nwemakeuseofthefactthatforclassicalpiecesusually\nmanydifferentinterpretationsandrecordingsareavailable.\nVisualizingthechordrecognitionresultssimultaneously\nformultipleaudiorecordingsofthesamepiece,wecanan-\nalyzetheconsistencyoferrorsacrosstheserecordings.On\ntheonehand,ifanerrorisnotconsistent,thenthismight\nindicateachordambiguityatthecorrespondingposition.\nOntheotherhand,aconsistenterrormightpointtoanin-\ntrinsicweaknessoftheautomaticchordlabeler,oranerror\ninthemanualannotations.Thisway,errorsmightbeauto-\nmaticallyclassiﬁedbeforetheyaremanuallyinspected.\nInFig.1(f)themulti-perspectivevisualizationforthe\nﬁrstfourmeasuresoftheChopinMazurkaisrepresented.\nHere,wewarpedtheautomaticallygeneratedchordlabels\nfor51differentaudiorecordingsontothemusicaltimeaxis\n11\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)usingthestepsdescribedabove.Byoverlayingtheresult-\nin\ngchordvectorsequences ¯Bforallpieces,wegetavi-\nsualizationsimilartothepreviousoneinFig.1(e),sothat\nthevisualizationforoneaudiorecordingcanbeseenasa\nspecialcaseofthemulti-perspectivevisualization.Inthe\nmulti-perspectivevisualization,wedistinguishtwocases\nusingtwodifferentcolorscales: onecolorscaleranges\nfromdarkbluetobrightgreen,andtheothercolorscale\nrangesfromdarkredtoyellow.Theﬁrstcolorscalefrom\nbluetogreenservestwopurposes.Firstly,itencodesthe\nscore-basedgroundtruthchordlabels.Secondly,itshows\nthedegreeofconsistencybetweentheautomaticallygen-\neratedaudiolabelsandthescorelabels.Forexample,the\ndarkblueentriesatthebeginningofmm.2show,thata\nCmajorchordisspeciﬁedinthescorelabels,andmost\naudio-basedlabelscoincidewiththescorelabelhere.At\ntheendofmm.2thebrightgreenshowsthatthescorespec-\niﬁesaCmajor,butmostaudio-basedresultsdifferhere\nfromthescorelabel.Analogously,thesecondcolorscale\nfromdarkredtoyellowalsofulﬁllstwopurposes.Firstly,\nitencodestheaudio-basedchordlabelsthatdifferfromthe\nscorelabels. Secondly,itshowshowconsistentanerror\nactuallyis. Forexample,atthebeginningofmm.2there\narenoredoryellowentries,sincethescorelabelsandthe\naudiolabelscoincidehere.However,attheendofmm.2,\nmostaudio-basedchordlabelsdifferfromthescorelabels.\nHeremostchordlabelseitherspecifyanFmajororanA\nminorchord.\n4. EVALUATION\nNoneofthecurrentlyavailableautomaticchordlabeling\napproachesworksﬂawlessly.Errorscaneitherbecaused\nbytheinherentambiguityinchordlabeling,orbyaweak-\nnessspecialtotheemployedchordlabeler. Anin-depth\nanalysisallowingforadistinctionbetweentheseerror\nsourcesisaveryhardandtime-consumingtask. Inthis\nsection,weshowhowthisprocesscanbesupportedand\nacceleratedusingtheevaluationandvisualizationframe-\nworkpresentedinSect.3.Tothisend,wemanuallycre-\natedscore-basedchordannotationsforseveralpiecesof\nmusic.Furthermore,weimplementedaverysimplebase-\nlinechordlabelertostudyverycommonsourcesoferror\ninchordlabeling.\n4.1 Annotations\nForthefollowingevaluation,atrainedmusician(Verena\nKonz)manuallyannotatedthechordsforthreepiecesof\nWestern classicalmusic. Firstly, Mazurkain Fmajor\nOp.68No.3byChopin. Secondly,PreludeinCma-\njorBWV846byBach. Thirdly,theﬁrstmovementof\nBeethoven’sFifthSymphony,Op.67. Usingtheunder-\nlyingscore,theannotationswerecreatedonthebeat-level,\nandinthecaseoftheBachPreludeonthemeasure-level.\nTheformatandnamingconventionsusedfortheannota-\ntionwereproposedbyHarte[4].Theannotatorpaidmuch\nattentiontocaptureevenslightdifferencesbetweenadja-\ncentchords. Hence,thebasstoneaswellasmissingoraddedtonesinchordsaremarkedexplicitlyusingthecor-\nrespondingshorthands.\n4.2 Baseline-methodforchordrecognition\nAbaselinechordlabelercanbeimplementedusingonlya\nfewsimpleoperations.Givenanaudiorecording,weﬁrst\nextractCENSfeatures(seeSect.2)resultinginafeature\nsequence Y:= (Y1, Y2, . . . , Y M).Wederivetenfeatures\npersecond,witheachfeatureconsideringroughly1100ms\noftheoriginalaudiosignal.Non-standardtuningsarecon-\nsideredasdescribedinSect.2.Then,wedeﬁneatotalof\n24chordtemplates,12templatesforthemajorchordsand\n12fortheminorchords.Theconsideredtemplatesare12-\ndimensionalvectors,inwhichtherespectivethreetonesof\nthecorrespondingmajor(minor)chord(therootnote,the\nmajor(minor)thirdandtheﬁfth)aresetto1andtherestto\n0.Thus,weobtaine.g.forCmajorthetemplate\n(1,0,0,0,1,0,0,1,0,0,0,0)\nandforCminorthetemplate\n(1,0,0,1,0,0,0,1,0,0,0,0).\nLetTinthefollowingdenotethesetofall 24chordtem-\nplates. Inanextstep,wechooseadistancefunction d,\nwhichmeasuresthedistanceofthei-thfeaturevector Yito\natemplate t∈T.\nd: [0,1]12×[0,1]12/mapsto→[0,1]\nd(t, Yi) = 1 −/an}bracketle{tt, Yi/an}bracketri}ht\n/bardblt/bardbl · /bardblYi/bardbl,\nw\nhere/an}bracketle{t·,·/an}bracketri}htdenotestheinnerproductand /bardbl·/bardbltheEuclidean\nnorm. Byminimizingover t∈Twecanﬁndthebest\nmatchingchordtemplate t∗forthei-thfeaturevector.\nt∗=argmin\nt∈Td(t, yi)\nThechordlabelassociatedwith t∗constitutestheﬁnalre-\nsultforthei-thframe.\n4.3 Experiments\nWestartourevaluationbylookingagainatourrunning\nexample,ChopinMazurkaOp.68No.3. Ourproposed\nvisualizationmethodclearlyrevealsvariouschordrecog-\nnitionerrors,seeFig.1(e). Makinguseofthemusical\ntimeaxis,theseerrorscannoweasilybetracedbacktothe\ncorrespondingpositioninthescoreandanalyzedfurther.\nForexample,atthebeginningofthepiece,thescore-based\ngroundtruthannotationcorrespondstoFmajor,whereas\nthecomputedaudio-basedannotationcorrespondstoFmi-\nnor. Amix-upofmajorandminoroftenappearsinthe\nchordrecognitiontask. Thenextmisclassiﬁcationoccurs\nattheendofmm.1,wherethegroundtruthstillcorre-\nspondstoFmajor,butthecomputedannotationspeciﬁes\naCmajor,whichisactuallythesubsequentchordinthe\ngroundtruth.Thismaybeaboundaryproblemoranerror\ninthesynchronization.\n12\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Inthemiddleofmm.2,wenotethatthegroundtruth\nch\nordisBminor,whereasthecomputedchordisCmajor.\nHavingalookatthescore,onecanseethatthechordin\nquestionisactuallyaBdiminishedchord.Duetothere-\nductionofthemanualannotationtomajor/minorchords,\nthischordismappedtoaBminorchordintheground\ntruth.Causingamisclassiﬁcationhere,thisisoftenaprob-\nleminthemajor/minorevaluationbasedonthecomparison\nofthedyad.\nThenextmisclassiﬁcationsareduetothemusicalam-\nbiguityofchords.Attheendofmm.2weobserveinthe\nscoreaCmajorchord,wheretheﬁfthismissing.Compar-\ningonthedyadlevel,thischordismappedtoaCmajor\nchordinthegroundtruth. However,allthenotesofthe\nchord(C,E)arealsopartofanAminorchord,whichisac-\ntuallycomputedatthisposition.Asimilarproblemoccurs\natthebeginningandattheendofmm.3,wheretheground\ntruthannotationcorrespondstoDminor,whereasthecom-\nputedannotationcorrespondstoFmajor. Thesamephe-\nnomenonappearsalasttimeattheendofmm.4,whereF\nmajorisrecognizedinsteadofAminor.Thisphenomenon\niscausedbyambiguitiesinherenttothechordlabelingtask\nandconstitutesaverycommonproblem. Thechordsin\nclassicalmusicrarelyarepuremajororminorchords,be-\ncausetonesareoftenmissingoradded.Hence,therecog-\nnitionaswellasthemanualannotationprocessbecomea\nhardtask.\nNext,weillustratewhatkindofadditionalinformation\nourmulti-perspectivevisualizationcanprovidecompared\ntothejustdiscussedvisualizationthatonlymakesuseof\nasingleaudiorecording.Here,weconsideragaintheﬁrst\nfourmeasuresoftheChopinMazurka. Insteadofusing\nonlyoneaudiorecordingweoverlaythechordrecogni-\ntionresultsfor51differentaudiorecordingsinourmulti-\nperspectivevisualization,seeFig.1(f). Lookingforcon-\nsistenciesandinconsistencies,itispossibletoclassifyand\ninvestigatesingleerrorsevenfurther. Forexample,the\nmisclassiﬁedFminorchordinthebeginningofmm.1(see\nFig.1(e))seemstobeanexceptionforthespeciﬁcrecord-\ning. Thiscanbeclearlyseenfromthemulti-perspective\nvisualizationwhereonlyforafewofthe51audiorecord-\ningsFminoriscomputedinsteadofFmajor. Also,the\nmisclassiﬁcationattheendofmm.4(FmajorinsteadofA\nminor)isnotconsistentacrossallconsideredaudiorecord-\nings.Onthecontrary,someofthemisclassiﬁcationswhich\nweobservedinthecaseofoneaudiorecording(Fig.1(e)),\nareconsistentlymisclassiﬁedformostoftheotheraudio\nrecordings.Forexample,thediminishedchordinthemid-\ndleofmm.2,thechordambiguityproblemoccuringat\ntheendofmm.2(AminorinsteadofCmajor),thebe-\nginningofmm.3(FmajorinsteadofDminor)andthe\nendofmm.4(FmajorinsteadofAminor). Overall,the\nmulti-perspectivechordrecognitionallowsforaclassiﬁca-\ntionofrecognitionerrorsintothosespeciﬁctoarecording\nandthoseindependentofarecording.\nAsafurtherexamplewenowconsiderthefamousBach\nPreludeinCmajor,BWV846.Themulti-perspectivevi-\nsualizationfor5differentaudiorecordingsformm.19-24\n(seeFig.2)againreﬂectsthechordrecognitionproblems\n  \n19 20 21 22 23 24 25    CC#/Db    DD#/Eb E/Fb E#/FF#/Gb    GG#/Ab    AA#/Bb B/Cb    cc#/db    dd#/eb e/fb e#/ff#/gb    gg#/ab    aa#/bb b/cb\n−6−4−20246(b)(a\n)\nMusicaltime(measures)\nFigure2.BachBWV846,mm.19-24. (a)Score,(b)Multi-\nperspectiveoverlayofscoreandaudiochordlabels.\nrelatedtodiminishedchords.Atthebeginningoftheex-\ncerpt(mm.19-21)andattheend(mm.24)thechordrecog-\nnitionresultforallaudiorecordingsconsistentlyagrees\nmoreorlesswiththegroundtruth. However,onecan\nobservetwopassageswithgreenentriesinmm.22-23.\nLookingatthecorrespondingpositioninthescore,we\nﬁndtwodiminishedseventhchords,inmm.22an F#:dim7\nandinmm.23an Ab:dim7. Duetothereductiontoma-\njor/minorchordsthesetwochordsaremappedtoF#minor\nandAbminorinthegroundtruthannotation,respectively,\nseeFig.2.However,inmostaudiorecordingsanAminor\nchordisdetectedinsteadof F#:dim7,havingtwotones(A\nandC)incommon.Andinsteadofthe Ab:dim7chordan\nFminorchordisfound,forwhichevenallthreetonesare\npresent(F,AbandC)duetotheadditionalpassingnoteC\nintheAb:dim7.Whiletheseventhchordinmm.20isrec-\nognizedwellforallrecordings,weseethatinmm.21the\nFmajorseventhchordwasmistakenforanAminorchord,\nagainduetochordambiguityreasons.\nAsalastexamplewenowconsidertheﬁrstmove-\nmentofBeethoven’sFifthSymphonyin37differentaudio\nrecordings. Actually,thispieceofmusicismuchmore\ncomplicatedintermsofharmonicaspectsthanthepre-\nviouslyconsideredChopinandBachexamples. Inthe\nBeethovenexample,wecanoftenﬁndthemusicalprin-\nciplesofsuspension,passingnotesor“unisono”passages.\nHere,theautomaticchordrecognitionaswellastheman-\nualannotationarechallengingandambiguoustasks.One\nexamplefortheuseofnonharmonictonesinchordscanbe\nfoundinmm.470-474,visualizedinFig.3.Lookingatthe\nscore,weobserveinthelefthandaDmajorchordwitha\nmissingﬁfth(mm.470-473),butintherighthandaGis\naddedinoctavestothisDmajorchord.Beingthefourth\nofD,theGcanbeseenasanonharmonictoneinDmajor.\nThiscausesachordmisclassiﬁcationforabout15record-\nings,whereGmajororalternativelyGminoriscomputed.\n13\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)ff/uniF032/uniF034\n/uniF032/uniF034/uniF026/uniF062/uniF062/uniF062\n/uniF03F/uniF062/uniF062/uniF062/uniF0CF\n/uniF0CF/uniF06E/uniF06E /uniF0CF\n/uniF0CF/uniF0CF\n/uniF0CF/uniF023\n/uniF023/uniF0CF\n/uniF0CF/uniF0CF\n/uniF0CF/uniF023\n/uniF023/uniF0CF\n/uniF0CF/uniF0CF\n/uniF0CF/uniF023\n/uniF023/uniF0CF\n/uniF0CF/uniF0FA/uniF0FA/uniF0FA/uniF06E/uniF06E\n/uniF0CE/uniF0CF/uniF0CF/uniF0CF/uniF023 /uniF0CE/uniF0CF/uniF0CF/uniF0CF/uniF023 /uniF0CE/uniF0CF/uniF0CF/uniF0CF/uniF023 /uniF0CE/uniF0CF/uniF0CF/uniF0CF/uniF023\n/uniF0FA/uniF0FA/uniF0FA\n  \n470 471 472 473 474 475    CC#/Db    DD#/Eb E/Fb E#/FF#/Gb    GG#/Ab    AA#/Bb B/Cb    cc#/db    dd#/eb e/fb e#/ff#/gb    gg#/ab    aa#/bb b/cb\n−30−20−100102030(b)(a\n)\nMusicaltime(measures)\nFigure3.Beethoven’sFifth,mm.470-474. (a)Score,(b)Multi-\nperspectiveoverlayofscoreandaudiochordlabels.\n/uniF032/uniF034\n/uniF032/uniF034/uniF026/uniF062/uniF062/uniF062/uniF02E /uniF02E /uniF02E /uniF02E /uniF02E /uniF02E\n/uniF06E/uniF06E /uniF02E /uniF02E/uniF02E /uniF02E\n/uniF03F/uniF062/uniF062/uniF062/uniF0CF/uniF0CF/uniF0CF/uniF0CF/uniF06E/uniF0CF/uniF0CF/uniF0CF/uniF0CF/uniF0CE/uniF0CF/uniF0CF/uniF06E/uniF0CF/uniF06E/uniF0CF/uniF0CF/uniF0CE/uniF0CF/uniF0CF/uniF0CF/uniF0CF/uniF06E/uniF0CF/uniF06E/uniF0CF/uniF06E/uniF0CF/uniF0CF/uniF0CF/uniF0CF /uniF0CE/uniF0CF/uniF0CF/uniF0CF/uniF0CF/uniF0CF/uniF06E/uniF0CF/uniF0CF/uniF0CF/uniF0CF/uniF0CF\n/uniF0CF/uniF06E/uniF06E/uniF0CF\n/uniF0CF/uniF0CF\n/uniF0CF\n/uniF0CF/uniF0CF/uniF0CF/uniF0CF/uniF06E\n/uniF06E /uniF0CF/uniF0CF/uniF0CF/uniF0CF\n/uniF0CF/uniF0CF/uniF06E\n/uniF06E /uniF0CF/uniF0CF\n/uniF0CF/uniF0CF /uniF0CF\n/uniF0CF/uniF06E\n/uniF06E /uniF0CF/uniF0CF\n/uniF0CF/uniF0CF /uniF0CF/uniF0CF /uniF0CF/uniF0CF/uniF0CF/uniF0CF/uniF0CF/uniF0CF/uniF0CF/uniF0CF/uniF06E/uniF0CF/uniF06E/uniF0CF/uniF0CF\n  \n484 485 486 487 488 489 490 491    CC#/Db    DD#/Eb E/Fb E#/FF#/Gb    GG#/Ab    AA#/Bb B/Cb    cc#/db    dd#/eb e/fb e#/ff#/gb    gg#/ab    aa#/bb b/cb\n−30−20−100102030(b)(a\n)\nMusicaltime(measures)\nFigure4.Beethoven’sFifthmm.484-490. (a)Score,(b)Multi-\nperspectiveoverlayofscoreandaudiochordlabels.\nOnthecontrary,theGseventhchordinmm.474isrecog-\nnizedverywellforallrecordings.Notethattheﬁrstbeats\nofthemeasures470-474arenotmanuallyannotated,since\ntheoctavesdonotrepresentmeaningfulchords.\nAnotherexampleofamusicalpatternthatisfoundto\nbeextremelyproblematicinthechordrecognitiontask,is\ntheprincipleofsuspension.Weillustratetheproblemsre-\nlatedtothismusicalcharacteristicusinganotherexcerpt\n(mm.484-490)ofBeethoven’sFifth,seeFig.4. Ineach\nofthemeasures484-488,onecanﬁndasuspensiononthe\nﬁrsteighth,whichresolvesintoamajorchordonthesec-\nondeighth.Thismusicalcharacteristiccaneasilybespot-\ntedinthemulti-perspectivevisualization. Here,wesee\nthatatthebeginningofeachmeasurethenumberofau-\ndiorecordingsforwhichthecomputedannotationagrees\nwiththegroundtruthisverylowandgetshigherafter-\nwards. Inmm.490ﬁnallytheﬁrstcompletepuremajor\nchordisreached.Notethatthesecondbeatsofmm.485-\n487consistofpassingnotestothenextsuspension.Hence,\nameaningfulchordcannotbeassignedresultinginseveral\nbeatsmissingagroundtruthannotation.5. CONCLUSIONS\nInthispaper,wehaveintroducedamulti-perspectiveeval-\nuation frameworkthat allows for comparingchord la-\nbelannotationsacrossdifferentdomains(e.g.,symbolic,\nMIDI,audio)andacrossdifferentperformances. This\nbridgesthegapbetweenMIRresearchers,whooftenwork\nonaudiorecordings,andmusicologists,whoareusedto\nworkwithscoredata. Inthefuture,weplantoapply\nourframeworkforacross-modalevaluationofseveral\ncomputer-basedchordlabelingprocedures,someofwhich\nworkinginthesymbolicdomainandothersworkinginthe\naudiodomain.Furthermore,inacollaborationwithmusi-\ncologists,weareinvestigatinghowrecurrenttonalcenters\nofacertainkeycanbedeterminedautomaticallywithin\nlargemusicalworks. Here,again,ourmulti-perspective\nvisualizationbasedonamusicallymeaningfultimeaxis\nhasturnedouttobeavaluableanalysistool.\nAcknowledgement. Theﬁrsttwoauthorsaresupported\nbytheClusterofExcellenceonMultimodalComputing\nandInteractionatSaarlandUniversity. Thethirdauthor\nisfundedbytheGermanResearchFoundation(DFGCL\n64/6-1).\n6. REFERENCES\n[1] J.P.BelloandJ.Pickens.Arobustmid-levelrepresentationforhar-\nmoniccontentinmusicsignals.In Proc.ISMIR,London,UK ,2005.\n[2] S.Ewert,M.M¨uller, andP.Grosche.Highresolutionaudiosyn-\nchronizationusingchromaonsetfeatures.In Proc.ofIEEEICASSP ,\nTaipei,Taiwan,2009.\n[3] E.G´omez.Tonaldescriptionofpolyphonicaudioformusiccontent\nprocessing.INFORMSJournalonComputing ,18(3):294–304,2006.\n[4] C.Harte,M.Sandler,S.Abdallah,andE.G´omez.Symbolicrepre-\nsentationofmusicalchords:Aproposedsyntaxfortextannotations.\nInProc.ISMIR,London,UK ,2005.\n[5] K.LeeandM.Slaney.Auniﬁedsystemforchordtranscriptionand\nkeyextractionusinghiddenMarkovmodels.In Proc.ISMIR,Vienna,\nAustria,2007.\n[6] LilyPond. http://www.lilypond.org .\n[7] M.Mauch,D.M¨ullensiefen,S.Dixon,andG.Wiggins.Canstatis-\nticallanguagemodelsbeusedfortheanalysisofharmonicprogres-\nsions?InProceedingsofthe10thInternationalConferenceonMusic\nPerceptionandCognition,Sapporo,Japan ,2008.\n[8] MIREX 2009. Audio Chord Detection Subtask. http:\n//www.music-ir.org/mirex/2009/index.php/\nAudio_Chord_Detection .\n[9] M.M¨uller.InformationRetrievalforMusicandMotion .Springer,\n2007.\n[10] M.M¨uller,F.Kurth,andM.Clausen.Audiomatchingviachroma-\nbasedstatisticalfeatures.In Proc.ISMIR,London,GB ,2005.\n[11] MutopiaProject. http://www.mutopiaproject.org .\n[12] J.T.Reed,Y.Ueda,S.Siniscalchi,Y.Uchiyama,S.Sagayama,and\nC.-H.Lee.Minimumclassiﬁcationerrortrainingtoimproveisolated\nchordrecognition.In Proc.ISMIR,Kobe,Japan ,2009.\n[13] J.Serr`a,E.G´omez,P.Herrera,andX.Serra.Chromabinarysimi-\nlarityandlocalalignmentappliedtocoversongidentiﬁcation. IEEE\nTransactionsonAudio,Speech&LanguageProcessing ,16(6):1138–\n1151,2008.\n[14] A.ShehandD.P.W.Ellis.Chordsegmentationandrecognitionus-\ningEM-trainedhiddenMarkovmodels.In Proc.ISMIR,Baltimore,\nMaryland,USA,2003.\n14\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "Unsupervised Accuracy Improvement for Cover Song Detection Using Spectral Connectivity Network.",
        "author": [
            "Mathieu Lagrange",
            "Joan Serrà"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1416998",
        "url": "https://doi.org/10.5281/zenodo.1416998",
        "ee": "https://zenodo.org/records/1416998/files/LagrangeS10.pdf",
        "abstract": "This paper introduces a new method for improving the accuracy in medium scale music similarity problems. Re- cently, it has been shown that the raw accuracy of query by example systems can be enhanced by considering pri- ors about the distribution of its output or the structure of the music collection being considered. The proposed ap- proach focuses on reducing the dependency to those priors by considering an eigenvalue decomposition of the afore- mentioned system’s output. Experiments carried out in the framework of cover song detection show that the proposed approach has good performance for enhancing a high accu- racy system. Furthermore, it maintains the accuracy level for lower performing systems.",
        "zenodo_id": 1416998,
        "dblp_key": "conf/ismir/LagrangeS10",
        "keywords": [
            "medium scale",
            "music similarity",
            "query by example",
            "prior knowledge",
            "eigenvalue decomposition",
            "cover song detection",
            "enhancing accuracy",
            "system performance",
            "accuracy level",
            "lower performing systems"
        ],
        "content": "UNSUPERVISED ACCURACY IMPROVEMENT FOR COVER SONG\nDETECTION USING SPECTRAL CONNECTIVITY NETWORK\nMathieu Lagrange\nAnalysis-Synthesis team,\nIRCAM-CNRS UMR 9912,\n1 place Igor Stravinsky, 75004 Paris, France\nmathieu.lagrange@ircam.frJoan Serr `a\nMusic Technology Group,\nUniversitat Pompeu Fabra,\nRoc Boronat 138, 08018 Barcelona, Spain\njoan.serraj@upf.edu\nABSTRACT\nThis paper introduces a new method for improving the\naccuracy in medium scale music similarity problems. Re-\ncently, it has been shown that the raw accuracy of query\nby example systems can be enhanced by considering pri-\nors about the distribution of its output or the structure of\nthe music collection being considered. The proposed ap-\nproach focuses on reducing the dependency to those priors\nby considering an eigenvalue decomposition of the afore-\nmentioned system’s output. Experiments carried out in the\nframework of cover song detection show that the proposed\napproach has good performance for enhancing a high accu-\nracy system. Furthermore, it maintains the accuracy level\nfor lower performing systems.\n1. INTRODUCTION\nExpressing the similarity between music streams is of in-\nterest for many multimedia applications [3]. Though, in\nmany tasks in music information retrieval (MIR), one can\nobserve a glass ceiling in the performance achieved by cur-\nrent methods and algorithms [5]. Several research direc-\ntions can be considered for tackling this issue. In this pa-\nper, we focus on the cover song detection task, but most of\nthe argumentation may be transferred to more general sim-\nilarity tasks involving a query by example (QBE) system.\nOne option to boost the accuracy of current QBE sys-\ntems is to use an enhanced description of the musical stream\nusing the segregation principle [2]. Intuitively, a lot can\nbe gained if an audio signal is available for each instru-\nment. This way, one can easily focus on the stream of in-\nterest for each MIR task. In this line, Foucard et al. [8]\nshow that considering a dominant melody removal algo-\nrithm as a pre-processing step is a promising approach for\nobserving more robustly the harmonic progression and, in\nthis way, achieve a better accuracy in the cover song de-\ntection task. However, it may be a long way until such\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2010 International Society for Music Information Retrieval.pre-processing based on segregation will be beneﬁcial for\nmanaging medium to large scale musical collections.\nAn efﬁcient alternative is to consider post-processing\napproaches exploiting the regularities found in the results\nof a QBE system for a given music collection. Indeed,\nmusic collections are usually organized and structured at\nmultiple scales. In the case of cover detection, songs nat-\nurally cluster into so-called cover sets [17]. Therefore, if\nthose cover sets can be approximately estimated, one can\ngain signiﬁcant retrieval accuracy, as evidenced by Serr `a et\nal. [17] and Egorov & Linetsky [6]. A different and very\ninteresting post-processing alternative is the general classi-\nﬁcation scheme proposed by Ravuri & Ellis in [15], where\nthey employ the output of different cover song detection\nalgorithms and a z-score normalization scheme to classify\npairs of songs.\nUnsupervised post-processing methods that have been\nintroduced so far are rooted on (a) the knowledge of an ex-\nperimental similarity threshold deﬁning whether two songs\nare covers or not [17], or (b) the potential number of or\ncardinality of clusters of the dataset being considered [6].\nThus, these methods are either algorithm or data-dependant.\nThe scheme in [15] is a supervised system trained on dif-\nferent algorithms outputs for some ground truth data. There-\nfore, it might potentially fail into one or both of the afore-\nmentioned dependencies1.\nIn this paper, we focus on improving the output of a\nsingle QBE system in an unsupervised way. In contrast\nwith the aforementioned references, we propose to con-\nsider “more global” approaches in order to alleviate their\nneeds and in order to advance towards unsupervised param-\neter-free post-processing steps for QBE systems. To this\nextent we introduce spectral connectivity network (SCN).\nIn addition, we focus on the beneﬁts this technique might\nprovide if the raw accuracy of the QBE system is rather\nlow. This could be the case of a particularly difﬁcult dataset,\nof a more simple and efﬁcient system (or merely a subop-\ntimal one), or a combination of both cases.\nThe remaining of the paper is organized as follows: af-\nter a presentation of previous work in Sec. 2, we intro-\nduce our new accuracy improvement scheme in Sec. 3. In\nthis section, the algorithm is motivated and illustrated on\n1Furthermore, issues could arise with the employed z-score normal-\nization for some intricate data structures or algorithm outputs (e.g., bino-\nmially distributed classiﬁer inputs).\n595\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Enhanced\nSimilarity-Combination\nEq. 1- Clustering\n6- QBE\nFigure 1. Combination scheme used for clustering based\nsystems.\ngeneric artiﬁcial datasets. In Sec. 4 we use the evaluation\nmethodology considered in [17] to show the potential of\nthe proposed approach.\n2. PREVIOUS WORK\nThere exist different proposals for the unsupervised post-\nprocessing of the output of a single QBE cover song de-\ntection system [6, 17]. Most promising strategies so far\nconsist in ﬁrst estimating the cover sets and use this clus-\ntering information in order to increase the overall accuracy\nas shown in Fig. 1. This can be achieved by considering\na classical agglomerative hierarchical clustering algorithm\nsuch as the well-known group average linkage (UPGMA)\nmethod [10,19] or alternatively the Community Clustering\nmethod (CC) presented in [17], which looks for connected\ncomponents in a complex network built upon the results of\nthe considered QBE system. Once a clustering solution is\nobtained, the output distance for a couple of song entries\n(ei;ej)given by a QBE system can be modiﬁed to increase\nthe overall accuracy [17]:\nd0\ni;j=(d(ei;ej)\nmax(d)ifei;ej2Ek,\nd(ei;ej)\nmax(d)+\fotherwise.(1)\nWe denotedi;jas the raw dissimilarity output of the QBE\nsystem between two songs eiandej,Ekrepresents a given\ncluster, and\f >1.\nBoth UPGMA and CC depend on the setting of a thresh-\nold similarity value that overall discriminates between\ncover and non-cover song pairs. This parameter is usu-\nally algorithm-dependent. Therefore, for different music\ncollections analyzed through the same QBE system, one\nshould expect similar values for the similarity threshold.\nThat seems to be the case for the algorithm presented in\n[16] when analyzing different datasets2(Fig. 2).\nAt a ﬁrst glance one could screen Fig. 2 and set a dis-\nsimilarity threshold for roughly separating between covers\nand not covers. In the present case this threshold could be\naround 0.6 (or below, if we want to have less false pos-\nitives). The threshold then would provide the necessary\ninformation to the post-processing clustering stage. How-\never, this dissimilarity threshold might not directly corre-\nspond to what the clustering algorithm is using internally\n(e.g., intra-cluster cophenetic distances [10, 19]). In the\nend one might better perform a grid search for the involved\nparameter.\nIn a more general scenario, one might not always be\nsure about the data or algorithm dependencies of the prob-\n2We notice however that both datasets have some similarities, e.g., in\nterms of genres.\n0 0.5 1 1.5 2 2.5 3 3.5 400.050.10.150.20.25\nDissimilarity Measure  \nCovers (Serra)\nNot Covers (Serra)\nCovers (Ellis)\nNot Covers (Ellis)Figure 2. Normalized histograms for the dissimilarity\nmeasure [16] on the music collection of [17] (lines with\ncrosses) and on the “covers80” dataset [7] (lines with tri-\nangles).\nlem. So, to be on the safe side, some data exploration, al-\ngorithm analysis, and/or parameter optimization needs to\nbe done. To avoid those tedious steps is what motivates us\nto consider unsupervised parameter-free post-processing\nstrategies.\n3. SPECTRAL CONNECTIVITY NETWORK (SCN)\nWithout any a priori knowledge about the problem at hand,\none needs to root the method on a statistical analysis that is\nable to identify the underlying structure of the observation,\nbeing in our case the output of a QBE system over a large\nmusic collection.\nSpectral graph clustering has gained popularity in many\ninformation retrieval areas, specially in gene, web, image,\nand audio processing [1,11,18]. The interested reader may\nbe referred to [12] for a tutorial introduction.\nIfSis a square matrix encoding the similarities of all\nthe entrieseiof our music collection E, it can be shown\n[14] that the eigenvectors of the corresponding Laplacian\nare relevant clustering indicators for determining the kdis-\njoint set of clusters E1;:::;E k(see Fig. 3). We propose to\nconsider this property in order to increase the overall accu-\nracy of QBE systems using the processing scheme shown\nin Fig. 4. Each of the steps are further detailed in the re-\nmaining of this section.\n3.1 Similarity Computation\nAs most QBE systems output a dissimilarity value di;j\nmeasuring how “far” a given couple of entries (ei;ej)are,\none needs to convert this distance into a similarity value\nsi;j. This is performed using the traditional radial basis\nfunction\nsi;j=e\u0012\n\u0000d2(ei;ej)\n\u001b2\u0013\n; (2)\n596\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Figure 3. Eigenvalues and eigenvectors of the Laplacian\ngraph corresponding to a dataset made of 4 bi-dimensional\nsets of 50 components with low overlapping.\n-QBE Laplacian -Enhanced\nSimilarity-Connectivity\nNetwork- Eigenvectors\nFigure 4. Processing scheme used for the proposed system\nbased on SCN.\nwith\u001bdetermined using the local scaling procedure pro-\nposed in [20]. The similarities si;jlead to a matrix S,\nwhich is further normalized as [18]:\nSd=D\u00001=2SD\u00001=2; (3)\nwhereDis a diagonal matrix with the degrees d1;:::;dn\nalong the diagonal, dk=P\njsk;j.\n3.2 Eigenvalue Decomposition\nAs proposed in [14] and illustrated in Fig. 3, the eigenvec-\ntors corresponding to the khighest eigenvalues of Sdcan\nbe considered as cluster indicators. For that purpose, the\ncontribution of each eigenvector is ﬁrst normalized with\nrespect to each of the entries, (i.e., per rows).\nFor a clustering task, any traditional clustering algo-\nrithm may then be considered. The k-means algorithm is\nusually considered in the literature. Though, in the case\nof cover set detection, the number of clusters is high and\ntheir cardinality is low, which makes the algorithm rather\nslow and highly sensitive to the random initialization. In\npre-analysis, it was found more suitable to use the afore-\nmentioned UPGMA algorithm. However, in this scenario,\none still needs to perform the clustering decision based on\na prior, be it the number of clusters or the similarity thresh-\nold and consider Eq. 1 for accuracy improvement.3.3 Connectivity Network\nAn alternative approach is to consider the Connectivity\nNetwork (CN) as our enhanced dissimilarity d0(i;j)by us-\ning the projection matrix of the normalized eigenvectors:\nP=NqX\nk=1qkqT\nk; (4)\nwhereqkis the eigenvector corresponding to the khigh-\nest eigenvalue \u0015kandNqis the number of eigenvectors to\nconsider. This principle has been originally used for corre-\nspondence analysis of contingency tables [9] and reintro-\nduced later in the context of spectral clustering [4].\nThe usual procedure is to set Np=kin order to retain\nonly the relevant eigenvectors. If kcannot be considered as\na prior (which is the case for cover set detection), one has\nto consider a method that can robustly estimate k. Unfor-\ntunately, no standard estimation procedure gave satisfying\nresults both in terms of accuracy and complexity.\nHowever, notice that in Fig. 3 the eigenvalues are high\nfor the ﬁrstkeigenvalues and lower afterwards. Consider-\ning the eigenvalues as weights in the computation leads us\nto the so-called Green’s function\nG=NqX\nk=2qk\u0015kqT\nk; (5)\nwhereNgcan more safely be set to a high value. An alter-\nnate formulation was proposed in [4]:\nSPCA =DNqX\nk=2qk\u0015kqT\nkD: (6)\nIn the experiments reported in this paper, the Green’s\nfunction outperformed signiﬁcantly the two others in the\ncase of unknown k,i.e. whenNgis set to the total number\nof eigenvectors. Since we are interested in a parameter-free\nsystem, only the results obtained using this function are re-\nported. Fig. 5 illustrates the use of the Green’s function\nwhile considering a dataset made of four bi-dimensional\nGaussian clusters with signiﬁcant overlap. Fig. 5(b) is ob-\ntained by a bi-dimensional scaling of the Green’s function.\n4. RESULTS\nWe split our results into two parts. The ﬁrst part concerns\naccuracy improvements related to QBE systems expected\nto have already a good accuracy and the second part relates\nto what might happen to systems with worse raw accura-\ncies before the post-processing stages applied in this paper.\n4.1 High accuracy QBE systems\nIn this subsection we attempt to improve a QBE system\nwith quite high raw accuracy. We exactly use the same\nmethodology and input data as in [17].\n597\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)(a)\nï8 ï6 ï4 ï2 0 2 4 6 8ï8ï6ï4ï20246810\n(b)\nï1 ï0.8 ï0.6 ï0.4 ï0.2 0 0.2 0.4 0.6 0.8 1ï0.8ï0.6ï0.4ï0.200.20.40.60.81\nFigure 5. Four bi-dimensional Gaussian clusters with sig-\nniﬁcant overlap (a) and bi-dimensional scaling plot of the\ncorresponding Green’s function (b).\n4.1.1 Methodology\nIn order to replicate those experiments we use both the\nsame synthetic and real data. Synthetic data is generated\nby considering a Gaussian noise font N(0;0:25) with zero\nmean and 0.25 standard deviation. A dissimilarity measure\nbetween songs iandjis then deﬁned as:\ndi;j=8\n><\n>:0 ifi=j,\njN(0;0:25)j ifiandjare covers,\n1\u0000jN (0;0:25)j otherwise.(7)\nReal data is provided by the Qmax measure presented in\n[16] and sampled from the 2125 song collection of [17].\nWe also employ the same data setups as in [17] (Table 1,\nwhere var. means that cover sets have a variable cardinal-\nity). Different number of cover sets (n C) and cardinalities\nare considered, as well as the fact of adding a different\nnumber of noise songs (n N). For setups 1.1 to 2.4 we re-\npeat the experiments 20 times.\nTo evaluate QBE systems we employ the mean of aver-\nage precisions (MAP) over all queries. The MAP is rou-\ntinely employed in a wide variety of tasks in the IR [13]\nand MIR communities, including the MIREX cover song\nidentiﬁcation task [5]. The average precision (AP) for aSetup Parameters\nnC Card.nN Trials\n1.1 25 4 0 20\n1.2 25 var. 0 20\n1.3 25 4 100 20\n1.4 25 var. 100 20\n2.1 125 4 0 20\n2.2 125 var. 0 20\n2.3 125 4 400 20\n2.4 125 var. 400 20\n3 525 var. 0 1\nTable 1. Setup summary.\nqueryiis calculated from the retrieved answer A ias\nAPi=1\nCiNX\nr=1Pi(r)Ii(r); (8)\nwhereCiis the total number of covers for the i-th query,\nNis the total number of songs in the dataset, P iis the\nprecision of the sorted list A iat rankr,\nPi(r)=1\nrrX\nl=1Ii(l); (9)\nand I iis a relevance function such that I i(z)=1 if the song\nwith rankzin A iis a cover of the i-th song, I i(z) = 0\notherwise. A relative MAP increase is then computed just\ndividing the post-processed MAP by the raw one, subtract-\ning 1, and multiplying by 100. For further details about\nmethodology we resort to [17]. In the case of UPGMA\nand CC we report results with the optimal threshold found,\nindependently for each data source.\n4.1.2 Results\nAs it can be seen in Table 2, a signiﬁcant accuracy im-\nprovement can be gained over the synthetic dataset. UP-\nGMA performs best, followed by SCN which is handi-\ncapped by the cluster size variability (setups 2.2 and 2.4).\nOn the real dataset, UPGMA and CC perform equally\nwell (Table 3). SCN achieves lower performance, proba-\nbly due to the fact that real data has less intrinsic regularity\nUPGMA CC SCN\n1.1 10.17 5.49 6.17\n1.2 9.76 4.31 4.08\n1.3 10.01 3.88 10.20\n1.4 9.54 3.73 3.27\n2.1 20.95 5.33 20.00\n2.2 20.70 4.95 5.98\n2.3 21.54 4.62 25.20\n2.4 20.35 5.08 10.90\nTable 2. Accuracy improvement (expressed as relative\nMAP-improvement %) for the synthetic dataset processed\nusing the QBE proposed in [17] as input.\n598\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)UPGMA CC SCN\n1.1 5.49 4.91 3.55\n1.2 4.31 4.00 3.15\n1.3 3.88 3.97 3.26\n1.4 3.73 4.05 3.45\n2.1 5.33 6.44 2.82\n2.2 4.95 5.02 2.47\n2.3 4.62 6.08 2.43\n2.4 4.77 5.06 1.70\n3 5.08 5.57 1.14\nTable 3. Accuracy improvement (expressed as relative\nMAP-improvement %) for the real dataset processed using\nthe QBE proposed in [17] as input.\nthan the synthetic one. Actually, no post-processing im-\nproves more than 5-6%. This may be explained by the fact\nthat the MAP achieved by the considered system over this\nconcrete dataset is rather high. As a consequence, setting\na threshold distance can be done reliably (recall Fig. 2).\nTherefore, one can speculate that the best MAP that can be\nachieved given this conﬁguration is in that range.\nAs a conclusion, it seems that approaches focusing on\nlocality (UPGMA and CC) are more relevant than global\napproaches (SCN) for improving the performance of a QBE\nsystem with rather high raw accuracy provided that their\nclustering threshold can be set reliably.\n4.2 Lower Accuracy QBE systems\nIn light of the previous results, we are interested in seeing\nhow these clustering schemes perform on lower accuracy\nsystems. Motivations for that could be that we either do\nnot have a good, high performing QBE system for a given\ntask, but a more modest one, or either that we are using\na faster and more efﬁcient version of the original system.\nFurthermore, we could be dealing with a particularly dif-\nﬁcult dataset where our (otherwise reliable) QBE system\nperforms more poorly.\nIn these cases, the accuracy improvement provided by\nthe post-processing steps outlined in this paper could be\nmore signiﬁcant than with the original high accuracy sys-\ntem. It could even be the case that, with a (in principle)\nlower performing QBE system, we reached the same (or a\nhigher) ﬁnal MAP.\nFor lower accuracy systems it is theoretically relevant to\nconsider more global approaches, as setting a dissimilarity\nthreshold is more difﬁcult due to the noise level. However,\nthe overall structure of the dataset might not be completely\nlost, and therefore we can still take beneﬁt of this fact by\nusing a method like SCN. This can be asserted by com-\nparing the MAP increase achieved by the studied methods\nwhen considering as input a lower accuracy system [7] (Ta-\nble 4).\n4.2.1 Methodology\nWe propose to further verify the previous assertion by sim-\nulating a QBE system with a controllable accuracy. ForMAP UPGMA CC SCN\nSerr`a et al. [16] 0.73 4.01 1.27 1.14\nEllis & Cotton [7] 0.42 8.06 3.04 19.70\nTable 4. MAP and MAP increase (%) for two QBE systems\nover the “covers80” dataset [7]. UPGMA and CC thresh-\nolds were speciﬁcally optimized for this dataset (however\nno signiﬁcant difference was observed, c.f. Sec. 2).\n0 0.5 1 1.5 2 2.5 3 3.5 400.050.10.150.20.25\nDissimilarity Measure  \nCovers\nNot Covers\nCovers (noise)\nNot Covers (noise)\nFigure 6. Normalized histograms for real data with no\nnoise (lines with crosses) and with \u001b= 0:45 (lines with\ntriangles).\nthat purpose, noise is added to our real data di;j(the out-\nput of the high accuracy reference QBE system) such that\n~di;j=jdi;j+N(0;\u001bd mx)j; (10)\nwhere\u001bis the noise level and dmxis a normalization factor\nset to the maximal dissimilarity found (see Fig. 6 for the\ncorresponding histograms).\n4.2.2 Results\nAs it can be seen in Fig. 7, CC does not maintain its initial\nMAP increase when the noise level raises up. In contrast,\nUPGMA maintains or slightly increases its relative MAP.\nWe ﬁnally see that SCN really boosts the MAP increase\nas more noise is added. This conﬁrms our hypothesis and\nleads us to speculate that these methods are more robust\nfor low accuracy QBE systems.\n5. CONCLUSION\nWe proposed a global approach for improving the accu-\nracy of query-by-example (QBE) systems based on spec-\ntral connectivity network. Contrasting with other state-of-\nthe-art approaches, it does not rely on any parameter set-\nting such as a dissimilarity threshold or the expected num-\nber of or cardinality of clusters within the data.\n599\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45123456789\nNoise levelRelative MAP improvement %\n  \nUPGMA\nPM1\nSCNFigure 7. Relative accuracy increase as a function of the\nnoise level for setup 2.4 using Serra’s data/QBE combina-\ntion as input.\nThe experiments showed that the proposed approach\nexhibits comparable results for improving high accuracy\nQBE systems and becomes highly competitive for improv-\ning lower accuracy QBE systems. Future research will in-\nclude a more in depth study upon the selection of the rele-\nvant eigenvectors (a problem closely linked to the estima-\ntion of the number of clusters in a dataset).\n6. ACKNOWLEDGEMENTS\nM.L. has been partially funded by the Quaero project within\nthe task 6.4: “Music Search by Similarity”. J.S. has been\npartially funded by the Music 3.0 project TSI-070100-2008-\n318 of the Spanish Ministry of Industry, Tourism, and Trade.\n7. REFERENCES\n[1] F. Bach and M. I. Jordan. Learning spectral clustering,\nwith application to speech separation. Journal of Ma-\nchine Learning Research, 7:1963–2001, 2006.\n[2] A. S. Bregman. Auditory Scene Analysis: The Percep-\ntual Organization of Sound. The MIT Press, 1990.\n[3] M. Casey, R. C. Veltkamp, M. Goto, M. Leman,\nC. Rhodes, and M. Slaney. Content-based music in-\nformation retrieval: current directions and future chal-\nlenges. Proceedings of the IEEE, 96(4):668–696, April\n2008.\n[4] C. Ding, X. He, H. Zha, and H. Simon. Unsupervised\nlearning: Self-aggregation in scaled principal compo-\nnent space. In European Conference on Principles and\nPractice of Knowledge Discovery in Databases, 2002.\n[5] J. S. Downie. The music information retrieval eval-\nuation exchange (2005–2007): a window into musicinformation retrieval research. Acoustical Science and\nTechnology, 29(4):247–255, 2008.\n[6] A. Egorov and G. Linetsky. Cover song identiﬁcation\nwith IF-F0 pitch class proﬁles. MIREX extended ab-\nstract, September 2008.\n[7] D. P. W. Ellis and C. Cotton. The 2007 LabROSA\ncover song detection system. MIREX extended ab-\nstract, September 2007.\n[8] R. Foucard, J.-L. Durrieu, M. Lagrange, and\nG. Richard. Multimodal similarity between musical\nstreams for cover version detection. In International\nConference on Acoustics, Speech, and Signal Process-\ning (ICASSP’10), Dallas, Texas, USA, March 2010.\n[9] M. J. Greenacre. Theory and Applications of Corre-\nspondence Analysis. Academic Press, 1984.\n[10] A. K. Jain, M. N. Murty, and P. J. Flynn. Data clus-\ntering: a review. ACM Computing Surveys, 31(3):264–\n323, September 1999.\n[11] M. Lagrange, L. G. Martins, J. Murdoch, and\nG. Tzanetakis. Normalized cuts for predominant\nmelodic source separation. IEEE Transactions on Au-\ndio, Speech, and Language Processing, 16(2):278–\n290, Feb. 2008.\n[12] U. Luxburg. A tutorial on spectral clustering. Statistics\nand Computing, 17:395–416, 2007.\n[13] C. D. Manning, R. Prabhakar, and H. Schutze. An in-\ntroduction to Information Retrieval. Cambridge Uni-\nversity Press, 2008.\n[14] M. Meila and J. Shi. Learning segmentation by random\nwalks. In Advance on Neural Information Processing\nSystems, 2000.\n[15] S. Ravuri and D. P. W. Ellis. Cover song detection:\nfrom high scores to general classiﬁcation. IEEE Int.\nConf. on Acoustics, Speech, and Signal Processing\n(ICASSP), pages 55–68, March 2010.\n[16] J. Serr `a, X. Serra, and R. G. Andrzejak. Cross recur-\nrence quantiﬁcation for cover song identiﬁcation. New\nJournal of Physics, 11:093017, September 2009.\n[17] J. Serra, M. Zanin, C. Laurier, and M. Sordo. Unsuper-\nvised detection of cover song sets: Accuracy improve-\nment and original identiﬁcation. In International Soci-\nety for Music Information Retrieval Conference, 2009.\n[18] J. Shi and J. Malik. Normalized cuts and image seg-\nmentation. IEEE Transactions on Pattern Analysis and\nMachine Intelligence, 22:888–905, 1997.\n[19] R. Xu and D. C. Wunsch. Clustering. IEEE Press,\n2009.\n[20] L. Zelnik-Manor and P. Perona. Self-tuning spectral\nclustering. In Annual Conference on Neural Informa-\ntion Processing Systems, 2004.\n600\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "Users&apos; Relevance Criteria in Music Retrieval in Everyday Life: An Exploratory Study.",
        "author": [
            "Audrey Laplante"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1415578",
        "url": "https://doi.org/10.5281/zenodo.1415578",
        "ee": "https://zenodo.org/records/1415578/files/Laplante10.pdf",
        "abstract": "The paper presents the findings of a qualitative study on the way young adults make relevance inferences about music items when searching for music for recreational purposes. Data were collected through in-depth interviews and analyzed following the constant comparative method. Content analysis revealed that participants used four types of clues to make relevance inferences: bibliographic metadata (e.g., names of contributors, labels), relational metadata (e.g., genres, similar artists), associative meta- data (e.g., cover arts), and recommendations/reviews. Relevance judgments were also found to be influenced by the external context (i.e., the functions music plays in one’s life) and the internal context (i.e., individual tastes and beliefs, state of mind).",
        "zenodo_id": 1415578,
        "dblp_key": "conf/ismir/Laplante10",
        "keywords": [
            "qualitative study",
            "young adults",
            "relevance inferences",
            "music items",
            "searching for music",
            "recreational purposes",
            "in-depth interviews",
            "constant comparative method",
            "content analysis",
            "four types of clues"
        ],
        "content": "USER S’ RELEVANCE CRITERIA  IN MUSIC RETRIEVAL I N \nEVERYDAY LIFE: AN EXPLORATORY STUDY  \nAudrey Laplante  \nÉcole de bibliothéconomie et des sciences de l’information  \nUniversité de Montréal, Montréal, QC, Canada  \naudrey.laplante @umontreal.ca  \nABSTRACT  \nThe paper presents the findings of a qualitative study on \nthe way young adults make relevance inferences about \nmusic items when searching for music for recreational \npurposes . Data were collected through in- depth interviews \nand analyzed following the constant comparative method. \nContent analysis revealed that participants used four types of clues to make relevance inferen ces: bibliographic \nmetadata (e.g., names of contributors, labels), relational metadata (e.g., genres, similar artists), associative met a-\ndata (e.g., cover arts), and recommendations/reviews.  Relevance judgments were also found to be influenced by \nthe external context (i.e., the functions music plays in \none’s life) and the internal context (i.e., individual tastes \nand beliefs, state of mind).   \n1. INTRODUCTION \nIn recognition of  the need to provide researchers with an \ninfrastructure for the evaluation of MIR systems and alg o-\nrithms , Music Information Retrieval Evaluation eXchange \n(MIREX)  w a s  e s t a b l i s h e d  i n  2 0 0 5 .  C o n t e s t s  have been \nheld annually since then. Like Text Retrieval Conference \n(TREC)  e x p e r i m e n t s  from which it is inspired , MIREX \nuses precision -recall  m e a s u res to evaluate system  p er-\nform ance. These are used to measure “the probability of \nagreement between what the system retrieved or failed to \nretrieve as relevant (systems relevance) and what the user \nassessed as relevant (user relevance) where user relevance \nis the gold standard on the basis of which evaluations are made” [1]. Hence, to establish ‘ ground truth’  f o r  t h e  \nevaluation of MIR tasks that called for human judgment  \n(e.g., audio music similarity), user surrogates (as opposed \nto the real users who originated the search queries) were \nasked to judge a post eriori whether the results retrieved \nwere relevant [2].  \nAlthough this approach has the advantage of taking \ninto account th e human dimension of the process , it also  \npresents limitations . The validity of relevance judgment s \nmade in experimental setting is questionable since the cri-\nteria used by participants might not correspond to those \nused by people in real situations. Studies on user- defined \nrelevance conducted in naturalistic settings show that, \napart from content -based criteria, criteria pertaining to the \nuser and the user’s  situation play a significant role in the \nevaluation of relevance [3, 4] . Therefore, failing to take into consideration the situation within which relevance \njudgment s occur  raises concerns and stresses the impor t-\nance of studying h o w  r e l e v a n c e  judgment s are made in \nreal life. While research on relevance criteria used in te x-\ntual information retrieval can have some utility  f o r  t h e  \nMIR community , studies on video and image information \nretrieval  suggest that diff erences in information type can \nresult  in differences in the criteria used to make relevance \njudgments [5, 6], hence the need to conduct research on \nuser-based relevance in the context of MIR. Unfortu-\nnately, this area of research has hitherto remained essen-\ntially unexplored.  \nThe present study was designed to bridge  t hi s  ga p by \ninvestigating  how young adults make relevance inferences \nabout music items when searching for music for recre a-\ntional purposes.  More specifically, it aim s to address the \nfollowing research questions: (1) What clues  d o  y o u n g  \nadults use t o  m a k e  r e l e v a n c e  i n f e r e n c e s  a b o u t  m u s i c  \nitems ? (2) How do individual characteristics (e.g., know l-\nedge, experience) influence their  relevance judgments? \n(3) How does the context influence their relevance jud g-\nments? By providing a  r i c h  u n d e r s t a n d i n g  o f  relevance \njudgment s in context, this study  will be beneficial in many \nways. It will provide the MIR community with a better \nunderstanding of the behavior of current  a n d  p o t e n t i a l  \nMIR systems users , which  m a y  t r a n s l a t e  i n to improv e-\nment s in MIR system design and evaluation measures.  \n2. RELATED RESEARCH \nSince the 1990s, information scientists  h a v e  c o n d u c t e d  \nnumerous empirical studies on user -based relevance. This \nhas led to a redefinition of the concept of relevance and to \nan increased knowledge of the criteria used by people \nwhen making relevance judgment s. \n2.1 Concept of R elevance \nResearchers distinguish system -oriented  ( o r  o b j e c t i v e )  \nrelevance from user -oriented  (or subjective) relevance [7]. \nAccording to the former, a document is considered re l-\nevant if it is topically related to the search query , a mea s-\nure that  has the useful property of being objective . From \nthe user’s point of view, however, topicality  was found to \nbe the most important but not necessarily the only re l-\nevance criterion. Therefore, a user- oriented definition of \nrelevance was proposed where a document is considered \nrelevant if the user who originated the query judges that it \nmeets his/her information need. This conception of re l-\nevance implies that r e l e v a n c e  judgment s are interpreta-Permission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fee provided that copies \nare not made or distributed for profit or commercial advantage and that \ncopies bear this notice and the full citation on the first page.   \n© 2010  International Society for Music Information Retrieval  \n601\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \ntional: they can only be made by end- users and are closely \ntied to the context within which they occur.  \nUser -oriented  r e l e v a n c e  c a n  b e  studied from different \nstandpoints : some  researchers have focused on its  cogn i-\ntive aspects (cognitive relevance ) [8]; some on its  psych o-\nlogical aspects (psychological relevance) [9]; some on its \ndynamic nature ( dynamic relevance) [10]; and others on \nits relation with the  situation or task at hand ( situational \nrelevance  or utility ) [11]. Hence, in an attempt to enco m-\npass all these dimensions, Barry  a n d  Schamber [12] de-\nscribe relevance in the following terms:  \n“[…] relevance is (1) cognitive and subjective, depen d-\ning on users’ knowledge and perceptions; (2) situational, \nrelating to users’ information problems; (3) complex and multidimensional, influenced by many factors; (4) d y-\nnamic , constantly changing over time; and yet (5) sy s-\ntematic, observable and measurable at a single point in \ntime.”  \nIt is with  this multidimensional and situational perspective \nof relevance in mind that the present research project was \ndesigned.   \n2.2 Studies on Use r-Defined Relevance  \nIn 1998, Barr y and Schamber compare the find ings of two \nstudies on user -defined relevance and conclude that a high \ndegree of overlap exists in the relevance criteria used by \nthe participants in both studies. Subsequent studies have \nconfirmed it since: there seems to exist a core set of crit e-\nria people use to make relevance judgment s regardless of \nthe context [13]. However, depending on the nature of the \ninformation , the situation or the user, the weight people \nattribute to each criterion varies  a n d  a d d i t i o n a l  c r i teria \nmay be employed.  Of particular importance to MIR is the \nresearch on relevance criteria used when searching for \nnon-textual docum ents. Choi and Rasmussen  [6] f o u n d  \nthat in the context of image information retrieval, autho r-\nity was less important t h a n  i n  t e x t u a l  i n f o r m a t i o n  r e-\ntrieval, whereas sub jectivit y and affectiveness— the em o-\ntional reaction to an image —played a s i g n i f i c a n t  r o l e  i n  \nthe selection stage. Yang and Marchionini [5] found that \nusers of video retrieval systems used textual criteria to start their search but mostly employed  visual criteria (e.g., \nstyle, color, motion) in the final selection stage . In both \ncases,  a s  i n  t e x t u a l  i n f o r m a t i o n  r e t r i e v a l , t o p i c a l i t y  was \nthe most common and important criterion .  \nAlso of  interest for MIR is the research on relevance in \nnon-problem -solving contexts , which correspond more \nclosely to situations where people search for music for \nrecreational purpo ses. Xu [14], who studied how users \nmake relevance judgment s when searching for info rma-\ntion for its epistemic or entertainment value, found that \nnovelty displaces topicality as the most commonly used \nrelevance criterion . \n3. RESEARCH DESIGN \nThe purpose of the present study was to provide a rich de-\nscription o f  t h e  w a y  young adults make relevance judg-\nment s when seeking m u s i c  f o r  r e c r e a t i o n a l  p u r p o s e s .   \nConsidering the complex and subjec tive nature of the \nphenomenon, a qualitative approach was considered best \nsuited.  3.1 Data Collection  \nThe subjective and interpretational nature of relevance \ncalled for a method that would allow us to gain insights \ninto the internal behavior of participants (e.g., thoughts, feelings, intentions). In -depth interviewing  was deemed \nthe most appropriate method  to attain this objective . The \nliterature review on user -oriented relevance provided a \nuseful theoretical background for the development of an interview guide . This gui de enabl ed us t o ensure consi s-\ntency in the topics covered in the interviews  while facil i-\ntating comparison between participants. During the inte r-\nviews, participants were asked to talk about their pr e-\nferred music information sources, to discuss the strengths \nand weaknesses of these sources, and to explain how, why \nand in which contexts they use  t h e m .  P a r t i c i p a n t s  w e r e  \nalso asked to relate in detail  a recent music information -\nseeking experience.  \n3.2 Participants  \nSince music behavior is known to vary according to  age \nand culture, we decided to reduce the heterogeneity of the population by limiting our study to the French -speaking \nyoung adults (18 -29 years) of the Montreal metropolitan \ncommunity.  Participants (n=15) were selected following the maximum variation sa mpling strategy as described in \n[11]. Recruitment continued until the saturation point was reached, that is when the information obtained through interviews started to be redundant so that no new themes or patterns were emerging from the analysis.  \nAmong t he fifteen participants, ten were male. At the \ntime of the interview, five were full -time students, seven \nwere full -time workers, and three were unemployed. All \nhad a high school diploma, 13 had a college diploma (or \nthe equivalent), and ten had a university degree or were \ncurrently enrolled in a university program. None of them \nwere professional musicians but six play ed a t  l e a s t  o n e  \nmusical instrument. The group comprised a majority of avid music listeners, although the sample also included a \nfew light o r moderate music consumers.     \n3.3 Data Analysis  \nThe interviews were recorded and transcribed. Each inte r-\nview lasted between 38 and 62 minutes, for a total of 724 \nminutes of recording and over 120,000 words of transcri p-\ntions and not es. The software pa ckage NViv o by QSR I n-\nternational was used to facilitate the encoding and anal y-\nsis process.  \nThe data were analyzed inductively using the constant \ncomparative method (CCM) as defined in [12]. CCM co n-\nsists in a step -by-step method according to which the r e-\nsearchers (1)  prepare the data for analysis by subdividing \nthe transcripts into units of meaning (in this case into \nparagraphs); (2) read through the data to identify emerg-\nin\ng themes and patterns in order to create a provisional set \nof categories; (3) categorize each unit of meaning into a \ncategory, forming new categories as needed; (4) refine the categories by comparing all units comprised into each \ncategory in order to identify the common properties or \ncharacteristics, merging, subdividing, or restating categ o-\nries as needed; and (5) explore relationships and patterns \nacross categories. \n \n602\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \n4. RESULTS \nThe interviews elicited a wealth of information about pa r-\nticipants’ music information -seeking behavior, including \ntheir likes and dislikes in terms of music information \nsources and the way they interact with these sources. The \nanalysis revealed that the participants’ relevance jud g-\nments were the results of a combination  of different crite-\nria and factors : criteria pertaining to the music itself  (e.g., \nquality),  t h e  p h y s i c a l  d o c u m e nt (e.g., disk) , the external \ncontext (e.g., intended use), the internal context (e.g., di s-\nposition) , and their personal knowledge and experience. \nTo determine the likelihood that a music item meets these criteria, participants  used a variety of clues.  \nThe results presented here cover  the clues used to make \nrelevance inferences  a n d  t h e  c r i t e r i a  a s s o c i a t e d  w i t h  t h e \ninternal and external context s of the search . Quotes were \ntranslated from French to English, while maintaining as \nmuch as possible the level o f language used by the pa r-\nticipants.  \n4.1 Relevance Cl ues  \nFor all participants, l istening to the music retrieved  played  \na crucial role in determining its relevance. This, however, \nrequires time and effort  that, in some conditions , partic i-\npants did not consider worthwhile . Moreover, the content \nof the music was not the only criterion . Therefore, at least \nin the first stage of their search, participants reported e m-\nploying a variety of clues or extra- music al information  to \nmake inferences about the type of experi ence a music \nitem offers,  and as s es s  t he pr obabi l i t y t hat  i t  meet s  t hei r  \ndesired criteria.  A s  t h e s e  c l u e s  a r e  n o t  a l w a y s  s e l f -\nexplanatory, previous knowledge and experience was o f-\nten called upon to interpret them.   \n4.1.1 Bibliographic Metadata  \nBibliographic metadata refer to the information used to describe an item, which, for m u s i c  r e cordings, includes  \nperformers, composers, authors of lyrics, titles of songs/pieces and albums, label s, etc. This type of info r-\nmation appeared to be commonly used by participants in \nthe selection process. The names of the main contributors  \n(e.g., singers, band s) seemed to have the great er impact on \ntheir selection. A  g o o d  e x p e r i e n c e  w i t h  a n  a r t i s t  c o u l d  \neven transform some of them into  committed fans having  \nan almost unwavering  f a i t h  in any new project to which \nthis artist c o n t r i b u t e s .  T h i s  e x p l a i n s  w h y  a  f e w  p a r t i c i-\npants admitted buy ing CDs of their favorite artists without \neven listen ing to them beforehand . Conversely, disa p-\npointing experiences with an artist also increase  the likel i-\nhood of di scarding an item without listening  t o  i t .  This \nspeaks of the notion of authority, a relevance criterion also used in other contexts of information retrieval.  \nIn the same way,  for the most avid music listeners, \ntrusted  l a b e l s  represent ed a  g u a r a n t e e  o f  q u a l i t y .  When \nasked whether he would borrow an albu m by  an unknown \nartist from the library, one participant explains: “if it’s on \na label, maybe . […] if it’s a bluegrass band, for instance, \nbecause it ’s on Smithsoninan, it must b e good.” Another \naffirms that he “pretty much trust[s] w h a t  [Matador ] re-\nlease.”  Of cour se, metadata regarding contributors or l a-\nbels are only useful if those are familiar . The fact that la-\nbels were only used by heavy music listeners suggests that extensive music knowledge is required to interpret this type of information.  \n  Other bibliographic metadata proved to be of lesser i m-\nportance for relevance judgments. Composers and a uthors \nof lyrics were explicitly mentioned by none of the partic i-\npants, which migh t be due to the fact that many of the p ar-\nticipants’ favo rite bands and si ngers composed their own \nsongs. Album titles did not seem to affect selection  either \nand only one participant affirmed relying on  song titles  to \ndeter mine if  an al bum was  worth borrow ing from the li-\nbrary.   \n4.1.2 Relational Metadata  \nLee a n d  Downie [15] d e f i n e  r elational metadata as data \nregarding relationships between music  items (e.g.,  music \ngenre s and similari ty). Relationship between  artists (e.g., \ncollaborations, influences ) c o u l d  b e  a d d e d  t o  t h i s  c a t e-\ngory. For several participants, links between artists  allow \nthem to situate an unknown artist in the music sphere, \nthus helping them assess  the probability that  the music of \nthis artist corresponds to their taste . Hence, one partic i-\npant related  having discovered a group by reading an i n-\nterview in a mu sic magazine in which the group  member s \nwere citing “their inspirations ”, w h i c h  t u r n e d  o u t  t o  b e  \n“things [he] l ike[s] ”, w h i c h  c o n v i n c e d  h i m  t o  l i s t e n  t o  \ntheir music. This could also explain  why MySpace Musi c \nand allmusic , two sources that incorporate a plethora of \nrelational metadata, reached the top of the  list as the most \npopular music information sources on the W eb a m o n g  \nparticipants. In MySpace Music , the “Friend Space” that \nconnects artists and regular users was considered the source’s most useful feature. Browsing the list of friends \nof a group one loves was a common strategy to discover new (but similar) group s as exemplified by this quote \nfrom a participant: “It’s also a good way […] to find groups that make similar music and groups with which they do concerts… It’s really, really useful!” Likewis e, in \nallmusic , participants greatly appreciated the links to i n-\nfluencers , followers and similar artists in the articles, a l-\nthough the ‘simi lar artists’  l i n k s ,  w h i c h  a r e  c r e a t e d  b y  \nmusic experts , were not unanimously considered reliable. \nIndeed, two participants complained about links leading to artists that “were not really similar” or “not similar at all .” \nWhile l\ninks between artists are  primarily used to make \ninferences about individual objects, music genres , which \nare meant to bring  s i m i l a r  o r  s o m e w h a t  s i m i l a r  a l b u m s  \ntogether,  w e r e  m o s t l y  e m p l o y e d  a t  a n  e a r l i e r  s t a g e ,  f o r  \ninstance to discard several items at once in order to get a \nmanageable set of items to browse. Hence, one participant  \nreported going directly to the “Film Music” section at the \nlibrary , whereas an other mention ed regularly searching \nfor “rock” in online music store s. Even though partici-\npants widely used genres to search or browse music co l-\nlections, they also frequently complained about them. \nCommon criticism s i n c l u d e d  (1) some groups do not \nclearly fit into one genre (“It’s oversimplified. You  can’t \nalways categorize a group into something.”); (2 ) genres \nare too broad ( “[searching by genres on the Web]  gene r-\nates e n d l e s s  l i s t s ”, o r  “ [In music stores] t hey put every-\nthing that is rock, alternative, punk, metal toget her!); and \n(3) genres are too narrow (“[On allmusic] t hey have \nsomething like 600 styles of music, it’s not concise \n603\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \nenough. […] They make up terms I’ve never seen and \nthere are basically two groups that make this type of m u-\nsic, maybe even just one!”). As i llustrated by this last \nquote, genres that are too specific also often sound unf a-\nmiliar and are therefore useless  (“I could tell you there are \n40-50% [of the genr es on Limewire] I have no idea where \nit comes from or what it is”). The apparent contradiction  \nbetween participants’ complains regarding the level of \nspecificity of genres can be explained by the source used \nto find music: genres in brick -and-mortar music stores are \nusually very broad, whereas specialized music sources on the Web tend to use very s pecific genres.  But this does not \nexplain entirely their dissatisfaction. Their knowledge of \nmusic or of a par ticular genre also influenced their  p er-\nception of genres. While one participant admits knowing \n“seven genres, eight at most!” an other explains tha t “the re \nare five, six styles of punk .”  \n4.1.3 Associative M etadata  \nLee and Downie  [15] define a ssociative metadata as data \nabout the relatio nship of a music item with events or other \nforms of art, which includes  c o v e r  a r t s. While some a p-\npreciated cover arts for their intrinsic beauty, participants \nalso appeared to attach importance to them for what they tell about the music. When little or no costs are involved, \nassessing the potential value of an albu m on the basis of \nits cover art could even represent  a  v a l i d  a l t e r n a t i v e  t o  \nsampling the music: “At Cheap Thrills [a music store], they have a one -dollar rack. There are groups I don’t \nknow. The only thing I can look at, really, is the cover art. If it looks cool… And it often happens to fit. I find music \nI like in jackets I like!” Most of the time, however, pa r-\nticipants employed cover arts only to make a first sele c-\ntion among albums, the following step being to listen to \nthe music. Hence, when asked how she selects CDs in music stores, one participant descr ibes that she looks at \nCDs “one by one” and selects those who have a nice cover art because “it says a lot” and “it has to represent something.”  She will then wait to be at home to download \nthe album an d see if it is worth buying it. Another partic i-\npant, whose previous experiences had led him to conclude that cardboard jewel cases oft en contained music he liked, \nhad come to use  that criterion to make a fi rst selection, a \nmethod that seemed to pay  off (“Often, it happens to be \nalbums I like!”).  For two parti cipants, however, cover arts \nhad no influence on their selection. One partic ipant ex-\nplains that “[shopping for] music is not really visual” and \ncannot be done “simply by wandering around, looking at album covers.” Another  mentio ns that her previous exp e-\nriences have convinced her  that cover arts should not be \nconsidered as indicators of quality (“I’ve bought so many good albums in hideous jackets in my life!”).  \n4.1.4 Recommendations and Reviews  \nA majority of participants affirmed that they attached i m-\nportance to recom mendations, reviews and ratings . The \ntrustworthiness of the source was determinant in the value \nthey ascribe d to the information. Hence, recommendations \nfrom friends or colleagues  perceived has having discrim i-\nnating judgment and tastes that are similar to theirs had \nthe greatest influence on their relevance judgment s. A few \nparticipants also relied occasionally on record store staff. In these cases, as they did not know them personally, the decision to trust a person was based on (1) his/her general look: the person has to look like someone who “pretty much listens to what I listen”); and/or (2) where that pe r-\nson works: people working in small and specialized music stores (“a small, underground CD store”) tended to be perceived as especially trustworthy.  I n  c o n t r a s t ,  b e c a u s e  \nof past disagreements with critics’ reviews, a majority of participants affirmed not being influenced by them.   \nRecommendations and reviews provided information \nthat could be useful  at different stages of a search. Partic i-\npants sought recommendations to begin a search , to obtain \ninformation that has  b e e n  f i l t e r e d  s p e c i f i c a l l y  f o r  t h e m  \n(“It’s like a filter [or] a bit like a shortcut”). It saves them \ntime, while increasing their chances of finding something \ninteresting (“You’re more likely to come across som e-\nthing good right away”). This information can also be use-\nful to make relevance inferences en route . Hence, they \nwere more likely to pick up an album if they had heard of \nthe artist before.  Star ratings or popularity sorting, on th e \ncontrary, were used at a later stage, to identify al bums or \nsongs “that best represent the career of an artist .” Reviews \nor recommendations could even change their initial rel e-\nvance judgment. Indeed, one participant ad mitted  having \nchanged her mind about  s o m e  m u s i c  a r t i s t s  b e cause of  \nfriends who “had found argument s” that had allowed her \nto listen  to the music “from a new angle .”  \nTo conclude this section on the clues participants used \nto make relevance inferences about music  items , we shall \nmention the t ypes of information that had little or no in-\nfluence . Interviews or biographies were mentioned only \nby a two, who were mostly interested in the professional \nlife of the artists a n d  i n  t h e  relational metadata (infl u-\nences, collaborators, etc.) they find  i n  t h e m .  Related to \nthat, p articipants seemed to favo r information that could \nbe scanned quickly: ratings or editor’s picks, for instance, were far more popular than long, written reviews.  Also \nabsent from the picture were the lyrics (or the topic of the lyrics). Although the topic of a song or an album could \nsometimes be determinant in deciding what one would \nlisten to in a specific situation, the lyrics  appeared to be of \nlittle importance in determining the relevance of an item \nduring the search  process. This might be explained by the \nfact that although the  participants spoke French, a majo r-\nity mostly listened to  Anglophone music.  \n4.2 Context \nSaracevic [16] m a i n t a i n s  t h a t  r e l e v a n c e  “ c a n n o t  b e  c o n-\nsidered without a context ” and defines the context as the \nresult of a “dynamic interaction between a number of  ex-\nternal and internal aspects.” In line with this definition, \nour analysis revealed that the context in which a search \noccur s a f f e c t e d  t h e  r e l e v a n c e  j u d g m e n t s  o f  t h e  p a r t i c i-\npants in various ways.  \n4.2.1 Situation  \nWe usually define situational relevance as the rela tionship \nbetween an information object and the user’s information \nproblem or task. This definition, however, did not seem entirely appropriate for this study since, according to our  \nparticipants’ accounts, searching for music for recre a-\n604\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \ntional purposes is  rarely a task -oriented activity. But this \ndoes not mean that the situation had no influence on their \nrelevance judgments. In reality, the roles music plays in their lives affect their information -seeking behavior. Pa r-\nticipants used music for a va riety of fu nctions. While \nsome reported listening to the same type of music regar d-\nless of the context, most  affirmed selecting different gen-\nres of music  in different situations . Thus, when seeking \nmusic, they always bore in mind the potential functions the music enco untered could potentially fulfil l.   \nIt is easy to see how this can affect relevance jud g-\nments. Music, for instance, was the soundtrack of mental \nor artistic work for 11 participants. The reasons for listen-\ning to m usic while working were various: music can (1) \ncontrib ute to inducing concentration (“when I forget my \nheadphone s, I’m kind of not productive”);  ( 2 )  m a k e  the \nwork more pleasant, especially a tedious or repetitive task \n(“it helps me get through the  day”) ; or (3) provide inspira-\ntion for artistic activities (“[it] helps me get into a mood”). \nWhen used in this context, the primary selection criterion \nwas that music did not interfere with their thinking, which usually meant instrumental and/or repetitive music such \nas classical, techno, or electronic; m usic genres they \nwould not necessarily listen to otherwise. Hence , for one  \nparticipant who usually listened  to old French music, the \nmusic that played  in the background when she was wor k-\ning was  totally different: “If I’m at work and need a lot of \nconcentration, I will put on techno music with no lyrics.”   \nWhen music is used to maintain or establish interpe r-\nsonal relationships, the selection is once again affected. \nAs a matter of fact, some participants mentioned listening to different genres of music with different persons so that \nit would please everyone, maybe even music they \nwouldn’t have listened alone, as illustrated by this quote: “My mother hates Pink Floyd. […] So when I’m with my father, we listen to Pink Floyd, but when I’m with my \nmother, I’ll lis ten to something else. I can even listen to \nsome Luce Dufault.”   \nMusic is also used to manage one’s mood. Whereas \nsome participants used music to modulate or enhance their \nmood (“If I get up on the wrong side of the bed, I put on \nmusic that will make me h appy for the rest of the day”); \nothers sought  music that matched their current —usually \ndepressed — mood (“If you’re broken -hearted [… and you \nlisten to] Rose by Portishead, you clearly know that they \nfeel like shit, just like you .”).  \n4.2.2 Individual Tastes and  Beliefs \nNot surprisingly, one of the main criteria employed to \nmake relevance judgment s was affectiveness or the em o-\ntional response to music. In other words, music usually \nhas to meet one’s taste to be considered relevant. Indeed, \nalthough participants reported occasionally select ing mu-\nsic outside of their regular tastes to fulfill specific fun c-\ntions, they still want ed this music to be as good as it could  \nbe. T h i s  e x p l a i n s  w h y  participants believed listening to \nthe music was an essential step  in formulating relevance \njudgments  unless, as mentioned before, they considered \nthat the risk incurred was low (e.g., highly trusted artist, \nmusic is f r e e  o r  a l m o s t  f r e e ). For that purpose, partic i-pants frequently visited the MySpace profiles of artists o r \ndownloaded music illegally to be able to lis ten to entire \nsongs or albums (“it allows you to really see what the song is like, the melody, see if you like it or not”). Indeed, \nalthough most online music stores or other music informa-\ntion sources propose  30-second excerpts, this was consi d-\nered insufficient  to make inferences about the work of an \nartist.  \nAs seen in Section 4.2.1, music serves  different fun c-\ntions , one of which being to help people  define their iden-\ntity, an area of research that has  b e e n  wi d e l y studied by \nsociologists and psychologists. Through their m usic \ntastes, people ex press who they are —their attitudes, val-\nues, and opinions. Of course, people also use music pref-\nerences to make inferences about others. Thi s has r epe r-\ncussion s o n  t h e i r  i n f o r m a t i o n -seeking behavior: people \nwant the music they retrieve to correspond to their values and beliefs. Such behavior was common among partic i-\npants. One participant said t h a t  h e  l i k e d m u s i c  t h a t  “ h a s  \nmeaning” and “would feel guilty if [he] l iked the music of \nsomeone [he] hated .” Another admitted  attributing a lot of \nimportance to finding underground groups “because I tend \nto want to be unique.” In fact, many participants showed a \nstrong penchant for non -commercial music, which could \nbe explained by their desire to distinguish themselves from others by having unique music taste.  \n Related to that, the geographical provenance of music \nartists seemed to influence the perception of a few since \nfive participants showed a marked preference for local \nartists (one confessed that she was m o r e  “ o p e n e d ”  t o  Quebe c groups, while another explained that she really \nliked  following “what’s happening on the Quebec scene”).      \n4.2.3 State of Mind  \nOne’s state of mind also affects music perception. As a \nmatter of fact, nine participants admitted that discovering \nnew music artists or genres required mental effort and an \nopenness of mind they only had in certain contexts . Four \nsaid they could only appreciate un familiar music if they \nhad “time to waste” so that they could s ettle down and \nconcentrate on the music. Three affirmed they needed to be receptive to novelty (“I need to be in a different state of mind. […] You really have to say ‘Ok, I need to adapt’”). Moreover, since music is often used for mood manag e-\nment (as seen  in Section 4.2.1), o ne’s current mood also \ninfluenced music selection.   \n5. CONCLUSION \nThis study s h e d s l i g h t  o n  some of the particularities of  \nrelevance judgment in the context of MIR, more specif i-\ncally in situations where people seek music information \nfor recreational purposes . Although we found that a sig-\nnificant  set of criteria used in t extual informa tion retrieval \nwere  also applicable in this context (e.g., quality, autho r-\nity, familiarity, situation, user’s knowledge and exper i-\nence), some unique characteristics  also emerged.  Findings \nsuggest that criteria pertaining to the user, especially ind i-\nvidual tastes  and beliefs, have a greater impact on sele c-\ntion than in other contexts. This could be due to the su b-\njective nature of music perception and to the fact that m u-\n605\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \nsic tastes often act as a ‘social badge’ that con veys infor-\nmation about peop le. Moreover, w hile topicality was \nfound to be the most common and import ant relevance \ncriterion in textual, image and video information retrieval, \nour study reveal ed that it was not used by our participant s, \npossibly because they attached little importance to lyrics. Therefore, genre , which  has the property of allowing one \nto obtain rapidly  a  m a n a g e a b l e  s e t  o f  i t e m s ,  displaced \ntopicality as the most commonly used c riterion to start a \nsearch. Our analy sis also uncovered t h e importance of \nrecommendations and reviews from trustwor thy sources at \ndifferent stages of the music selection proc ess. On the \nother hand,  this study reinforces findings from  p r e v i o u s  \nstudies by confirming  the importan ce of affectiveness as a \ncriterion for making relevance judgments about non -\ntextual information [5]; and the importance of novelty in \nrecreational contexts [14] .  \nThis study provide s indications for the design of MIR \nsystems and interfaces that support users in their re l-\nevance judgments.  It reiterates the need to provide rich \nmetadata, including links between artists, not only to f a-\ncilitate the search but also to better assist users i n their s e-\nlection. However, the fact that some metadata were  use-\nless to people who did not have the required knowledge to interpret them suggest that systems should also assist the \nuser in this task, for instance by providing descriptions for \nmusic genres or music labels. The importance ascribed to \nrecom mendations from people one knows  and trust s indi-\ncates that systems should include social net working tools \nthat f a c i l i t a t e  t h e  s h a r i n g  o f  i n f o r m a t i o n  b e t w e e n  u s e r s .  The study also revealed that music preferen ces could \nchange depending on the context (e.g., one’s current \nmood, functions music plays in one’s life). A successful \nrecommender system sho uld therefore be able to handle \nthis complexity and allow people to have multiple ‘music \npersonalities,’  thus reco gnizing the dynamic nature of re l-\nevance.  \n6. ACKNOWLEDGEMENTS \nThis work has been supported by the  Social Sciences and \nHumani ties Research Council of Canada . I am grateful to  \nDr. John Leide and Dr. J. Stephen Downie for their con-\ntribution  to this project.    \n7. REFERENCES \n \n[1] T. Saracevic, \"Effects of inconsistent relevance \njudgments on information retrieval test results: A \nhistorical perspective,\" Library Trends, vol. 56, pp. \n763-783, Spring 2008.  \n[2] M. C. Jones , et al., \"Human similarity judgments: \nimplications for the design of formal evaluations,\" in \nProceedings of the 8th International Conference on Music Information Retrieval, Vienna, Austria, September 23- 27, S. Dixon , et al. , Eds., ed Vienna: \nÖsterreichische Compu ter Gesellschaft, 2007, pp. \n539-542. [3] L. Schamber, \"Relevance and information behavior,\" Annual Review of Information Science and Technology (ARIST), vol. 29, pp. 3- 48, 1994.  \n[4] C. L. Barry, \"User -defined relevance criteria: An \nexploratory study,\" Journal of the American Society \nfor Information Science, vol. 45, pp. 149- 159, 1994.  \n[5] M. Yang and G. Marchionini, \"Exploring users’ video relevance criteria: a pilot study \" in ASIST \n2004, Proceedings of the 67th ASIST Annual Meeting , \nL. Schamber and C. L. Barry, Eds., ed Medford, NJ: \nInformation Today, 2004, pp. 229 -238. \n[6] Y. Choi and E. Rasmussen, \"Users' relevance criteria in image retrieval in American history,\" Information \nprocessing and management, vol. 38, pp. 695- 726, \n2002.  \n[7] D. Swanson, \"Subject ive versus objective relevance \nin bibliographic retrieval systems,\" Library \nQuarterly, vol. 56, pp. 389- 398, 1986.  \n[8] P. Wang and D. Soergel, \"A cognitive model of document use during a research project. Study I. Document selection,\" Journal of the Americ an \nSociety for Information Science, vol. 49, pp. 115- 133, \n1998.  \n[9] S. P. Harter, \"Psychological relevance and information science,\" Journal of the American \nSociety for Information Science, vol. 43, pp. 602- 615, \n1992.  \n[10] L. Schamber , et al., \"A re -examination of relevance: \ntoward a dynamic, situational definition,\" Information Processing & Management, vol. 26, pp. \n755-776, 1990.  \n[11] P. Wilson, \"Situational Relevance,\" Information \nstorage and retrieval, vol. 9, pp. 457- 471, 1973.  \n[12] C. L. Barry and L. Schamber, \"Users' criteria for relevance evaluation: A cross -situational \ncomparison,\" Information Processing & \nManagement, vol. 34, pp. 219- 236, 1998.  \n[13] T. Saracevic, \"Relevance: A review of the literature and a framework for thinking on the notion in \ninformation science. Part III: behavior and effects of \nrelevance,\" Journal of the American Society for \nInformation Science and Technology, vol. 58, pp. \n2126- 2144, 2007.  \n[14] Y. Xu, \"Relevance judgment in epistemic and \nhedonic information searches,\" Journal o f the \nAmerican Society for Information Science and Technology, vol. 58, pp. 179- 189, 2007.  \n[15] J. H. Lee and J. S. Downie, \"Survey of music information needs, uses, and seeking behaviours: Preliminary findings,\" in 5th International \nConference on Music Information Retrieval , \nBarcelona, Spain, 2004, pp. 441 -446. \n[16] T. Saracevic, \"Relevance: A review of the literature and a framework for thinking on the notion in information sicence. Part II: nature and manifestations of relevance,\" Journal of the  \nAmerican S o c i e t y  f o r  I n f o r m a t i o n  S c i e n c e  a n d  Technology, vol. 58, pp. 1915- 1933, 2007. \n \n606\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "Crowdsourcing Music Similarity Judgments using Mechanical Turk.",
        "author": [
            "Jin Ha Lee 0001"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1416478",
        "url": "https://doi.org/10.5281/zenodo.1416478",
        "ee": "https://zenodo.org/records/1416478/files/Lee10.pdf",
        "abstract": "Collecting human judgments for music similarity evalua- tion has always been a difficult and time consuming task. This paper explores the viability of Amazon Mechanical Turk (MTurk) for collecting human judgments for audio music similarity evaluation tasks. We compared the simi- larity judgments collected from Evalutron6000 (E6K) and MTurk using the Music Information Retrieval Evaluation eXchange 2009 Audio Music Similarity and Retrieval task dataset. Our data show that the results are highly comparable, and MTurk may be a useful method for col- lecting subjective ground truth data. Furthermore, there are several benefits to using MTurk over the traditional E6K infrastructure. We conclude that using MTurk is a practical alternative of music similarity when it is used with some precautions.",
        "zenodo_id": 1416478,
        "dblp_key": "conf/ismir/Lee10",
        "keywords": [
            "Amazon Mechanical Turk",
            "human judgments",
            "Music Information Retrieval Evaluation eXchange",
            "Audio Music Similarity",
            "ground truth data",
            "precautions",
            "benefits",
            "time consuming task",
            "Viability",
            "comparison"
        ],
        "content": "Crowdsourcing Music Similarity Judgments using Mechanical Turk \n Jin Ha Lee   \n University of Washington  \njinhalee @uw.edu  \nABSTRACT \nCollecting human judgments for music similarity evalu a-\ntion has always been a difficult and time consuming task. \nThis paper explores the viability of Amazon Mechanical \nTurk (MTurk) for collecting human judgments for audio \nmusic similarity evaluation tasks. We compared the sim i-\nlarity judgments collected from Evalutron6000 (E6K) and \nMTurk using the Music Information Retrieval Evaluation \neXchange 2009 Audio Music Similarity and Retrieval \ntask dataset. Our data show that the results are highly \ncomparable, and MTurk may be a useful method for col-\nlecting subjective ground truth data. Furthermore, there \nare several benefits to using MTurk over the traditional \nE6K infrastructure. We conclude that using MTurk is a \npractical alternative of music similarity when it is used \nwith some precautions .  \n1. INTRODUCTION \nA constant source of frustration for designers and deve l-\nopers of music information retrieval systems is finding \nusers to generate ground truth for evaluation. This is pa r-\nticularly true in music similarity tasks where algorithms \nare attempting to model some aspect of human intuition \nor understanding and predict the similarity among a set of \nsongs. Getting humans to verify the results of these alg o-\nrithms is tedious as a modest collection of several hu n-\ndred tracks can require tens of thousands of pair-wise \ncomparisons which potentially need to be evaluated.  \nOur motivation for this study is to explore the usefu l-\nness of Amazon Mechanical Turk (MTurk) \n(http://mturk.com) for collecting the human judgments \nnecessary for evaluating music similarity tasks like the \nAudio Music Similarity (AMS) and Symbolic Melodic \nSimilarity (SMS) tasks in the Music Information Retrie v-\nal Evaluation eXchange (MIREX). In this paper, we \ncompare the similarity judgments obtained from MTurk \nand Evalutron6000 (E6K) on the same data set used in \nthe MIREX 2009 A MS task . We also compare how these \njudgments affect the ultimate evaluation outcomes as \npublished by the International Music Information R e-\ntrieval Systems Evaluation Laboratory (IMIRSEL) in the annual MIREX evaluation. Additionally, we were inte r-\nested in exploring how MTurk could be used to suppl e-\nment or replace E6K in future music similarity evalu a-\ntions, opening the possibility for continuous evaluation \nwithout incurring the overhead of a full MIREX/E6K-\nbased evaluation. \n2. BACKGROUND \nThe AMS and SMS tasks were carried out as part of \nMIREX using the E6K infrastructure . Both tasks rely on \nhuman judgments of music similarity as ground truth for \nevaluation of algorithm performance. Every year the \nIMIRSEL group at the University of Illinois seeks volu n-\nteers from the ISMIR community to complete a set of s i-\nmilarity judgments. In addition to the MIREX AMS and \nSMS tasks , a number of studies have looked at the human \njudgments of music similarity; to name a few, Aucoutur i-\ner & Pachet [2], Ellis et al. [5], Berenzweigh et al. [4] , \nTimmers [13], Schubert & Stevens [11], and Novello & \nMcKinney [10]. \nIn these studies, the human judgments were collected \nby a web survey or by recruiting a number of subjects i n-\ncluding musicians and non-experts. Two typical methods \nwere used. In some studies, the users were presented a set \nof three song excerpts (triads) and were asked to choose \nthe most similar and most dissimilar of the three possible \npairs. In other studies, the users were presented with pairs \nof song excerpts and were asked to rate the similarity b e-\ntween the pairs . Regardless of which method was used, \ncollecting human similarity judgments has always been a \nchallenging, expensive, and time-consuming process. \nSearching for a better model for obtaining human simila r-\nity judgments is especially important considering the fact \nthat the general trend in recent MIREX A MS submissions \nis to submit multiple variations of an algorithm; there \nwere a total of 15 submissions from 9 participants in \n2009 compared to 6 submissions from 5 participants in \n2006 . There is also a trend towards larger datasets, and \nevaluating more queries [9 ]. \n2.1 Evalutron6000 (E6K) \nIMIRSEL collect similarity judgments from human gra d-\ners using E6K which is in the form of a web-based su r-\nvey. The graders are supposed to be music experts since \nthey are volunteers from the ISMIR community who have \nbackgrounds in music or music-related research. Collec t-\ning human judgments is a long and arduous process every  \nPermission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fee provided that copies \nare not made or di stributed for profit or commercial advantage and that \ncopies bear this notice and the full citation on the first page.   \n© 2010  International Society for Music Information Retrieval  \n183\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \nyear since the organizers must rely on volunteer labor . \nEvery year it takes days to weeks to complete the evalu a-\ntion. Table 1 shows the number of days it took to collect \nthe human similarity judgments for the AMS and SMS \ntasks in past MIREX cycles .  \n AMS  SMS  \n2006  15 days  18 days  \n2007  8 days  4 days  \n2009  14 days  n/a \nTable 1.  Number of days needed to collect human \nsimilarity judgments in past MIREX cycles.  \n2.2 Amazon Mechanical Turk \nAmazon Mechanical Turk (MTurk) is a service which \nallows people to leverage human-computational power at \nscale to complete large numbers of tasks requiring h u-\nman-intervention, cheaply and efficiently. Requesters \nupload tasks to the service, where they are matched with \nwilling workers. Payment is mediated by Amazon, with a \nsmall per- task fee charged to the requester. \nTasks in MTurk are called HITs (Human Intelligence \nTasks). Requesters define their HITs using an HTML-\nbased template language and a data source which is used \nto populate the templates and generate the individual \nHITs. The requester also offers a payment amount and \ntime limit for each HIT, and can limit who is able to \ncomplete the HITs, such as requiring workers to have a \nminimum percentage of previously accepted HITs.  \nRequesters can create a qualification test and require \nworkers to have to pass it before being eligible to work \non the ir HITs. Upon completion of a HIT, the requester \ncan review the work and approve or reject payment on \nHITs individually. Furthermore, workers can be blocked \nby the requester which would reject all their un-approved \nHITs and prevent them from completing and submitting \nadditional HITs for that same task. \nWorkers in MTurk call themselves Turkers. There are \nover 200,000 Turkers, from all parts of the globe. Ipeir o-\ntis [6] conducted a survey of 1,000 Turkers in February \n2010. In total, Turkers represented 66 different countries, \nwith 46.8% from the United States, and 34% from India. \nAmong US workers, most (65.6%) are women; however, \namong Indian workers, most (70%) are men. 62.8% of \nrespondents had a Bachelor’s degree or higher. \nMTurk has an API through which HITs can be created \nand results approved and downloaded, making it readily \nintegratable into automated processes. For example, \nMTurk is successfully used to complete tasks such as fi l-\ntering user-generated web content, searching through s a-\ntellite photos for missing aircraft [3], and is gaining tra c-\ntion as a resource in research. Within the SIGIR comm u-\nnity, MTurk has been proposed for use in generating r e-\nlevance judgments in TREC-like evaluations [1]. Alonso \nand Mizzaro [1] compared MTurk to TREC experts and \nfound the results from MTurk to be comparable to TREC ’s expe rt-generated ground truth data. They even \nclaim Turkers found several errors in the TREC data. \nSnow, et al. [ 12] have explored using MTurk for gen e-\nrating ground truth data for several kinds of Natural La n-\nguage Processing tasks, including determining valence \nand affect in text, and assigning similarity scores. They \nfound it possible to obtain results on par with those of \ndomain experts. \nKittur et al [8] used MTurk to rate the quality of Wiki-\npedia articles. Among their experiments, they found a \nnaï ve approach of simply asking Turkers to rate an article \nled to inconsistent responses which did not correlate \nstrongly with ratings given by experts. However, when \nthey redesigned the HITs to include several verifiable \nquestions which could be used to filter out “bad” re s-\nponses, the results improved significantly. They argue \nthat the verification questions serve two purposes: first, \nthey allow the requester to assess the quality of the r e-\nsponse; and second, they signal to the Turkers that their \nresponses are being scrutinized. \n3. RESEARCH QUESTIONS & STUDY DESIGN \nIn this paper, we explore two main research questions:  \nI) How do music similarity judgments obtained from \nMechanical Turk compare to those collected from \nmusic experts in the Evalutron6000 ?; and  \nII) How do evaluation outcomes for tasks like MIREX’s \nAudio Music Similarity evaluation differ when based \non similarity judgments collected from Mechanical \nTurk as compared to Evalutron6000? \nTo study these questions, we replicated the E6K similar i-\nty assessment and subsequent evaluation of the AMS task \nin the 2009 MIREX evaluation using MTurk. We o b-\ntained the query-candidate (QC) results lists from the \nIMIRSEL lab, consisting of 100 queries, and the top 5 \ncandidates per query returned from the 15 participating \nalgorithms in MIREX 2009. There were a total of 6,732 \nunique QC pairs which needed to be judged. \nIn order to keep the amount of work in each HIT re a-\nsonable, we limited the number of similarity judgments \nper HIT to 15 QC pairs, and a ll QC pairs in a HIT shared \nthe same query. Among the 15 QC pairs in a HIT, two \ncandidates were included for checking the quality of the \nratings. One was an identity check and it asked the Turker \nto rate the similarity of the query to itself. The Turker \nshould indicate that this candidate is “Very Similar (VS)” \nto the query as they are identical. This was also done in \nE6K in 2009; however, we were unable to locate those \ndata published for comparison. \nThe other quality check was a consistency check; the \nsame candidate was included twice in a single HIT, once \ntowards the beginning, and again towards the end of the \nlist of candidates. The expectation here was that the \nTurker should provide the same response for both i n-\n184\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \nstances since they are the same candidate. Excluding \nthese two QC pairs for checking the quality of the results, \nthere were 13 unique QC pairs in an individual HIT. The \nquality checks were mixed among the other candidates \nand were not specially demarcated in any way. \nThe list of all candidates for each query was broken down \ninto multiple HITs containing 13 unique QC pairs. In the \nevent that the last HIT contained less than 13 candidates, \nthe list was padded to 13 with additional candidates s e-\nlected from that query’s other HITs. These padded jud g-\nments were not used in the evaluation. Each HIT was \ncompleted by single Turker, with the possibility that a \nsingle query could be evaluated by multiple Turkers. This \nwas different from MIREX 2009  AMS where a single \ngrader was responsible for judging all candidates for a \nsingle query, but is similar to MIREX 2006 AMS, wher e \ncandidate lists were divided among multiple graders.  \nA total of 583 HITs were created, and we offered \n$0.20 per completed HIT. Instructions similar to what are \ngiven in E6K were given to Turkers as shown in Figure 1. \nFigure 2 shows a partial screenshot of MTurk’s evalu a-\ntion page. We tried to reproduce the E6K interface as \nmuch as possible. The Turkers were asked to rate the s i-\nmilarity on the E6K BROAD scale (Not Similar, Some-\nwhat Similar, and Very Similar) and were not asked to \nprovide a FINE score (0- 10) in order to simplify the task. \nAdditionally, we used the Yahoo! Media Player in the \nMTurk version of the interface, rather than the E6K pla y-\ner because it was much simpler to use and has much be t-\nter cross-browser compatibility. \nIn addition to the HITs described above, we created 4 \nmore HITs in order to see how much variability there was \namong the Turkers’  responses. In these HITs, we took \none candidate from each query, put 15 QC pairs into each \nHIT and had 3 different Turkers complete each HIT. This \ngave us multiple ratings for the same QC pairs, and a l-\nlowed us to test the inter-rater agreement. We paid $0. 20 \nfor each of these 12 HITs. The total cost for all 595 HITs, \nincluding Amazon’s administrative fees, was $13 0.90. \nUltimately, we paid less than $0.02 per usable judgment, \nfor a rate of approximately 53 judgments per US dollar.  \n4. DATA & OBSERVATIONS \nIn total we collected 15,705 similarity judgments from \n1,047 submitted HITs, plus 180 additional judgments \ncreated to test agreement among the Turkers. Of the \n1,047 HITs submitted, we approved 583 (55.7%), and \nrejected 464 (44.3%). We rejected HITs which were \nmissing responses, those which were completed too \nquickly (less than 45 seconds), those in which Turkers \nfailed to assign a Very Similar score to the identity case \nof a query compared to itself, and those in which Turkers \nassigned two different scores to the same candidate r e-\npeated in the list. Accounting for the rejected HITs, the \nintegrity-check judgments, and for list padding, we ended \nup with 6,732 unique judgments. Even having to discard almost half the judgments, we were still able to obtain the \nneeded results in less than 12 hours, an order of magn i-\ntude faster than the average E6K cycle (see Table 1).  \n \nFigure 1.  Instructions given to Turkers are based \non the instructions given to E6K graders. \n \nFigure 2.  Partial screenshot of an MTurk HIT. \n \nResearch Question I: How do music similarity jud g-\nments obtained from Mechanical Turk compare to those \ncollected from music experts in the Evalutron6000? \nThe similarity scores derived from MTurk are different \nfrom those obtained from E6K, but they are not entirely \nincomparable. We compared the 6,732 similarity jud g-\nments obtained from MTurk to the 6,732 judgments o b-\ntained via E6K in AMS 2009 for the same set of QC \npairs. We measured the percent-agreement between the \nMTurk results and the E6K results, and found that 54.6% \nof the pair-wise ratings were the same. Agreement in-\ncreases to 72.4% when we consider similarity as a binary \ndecision (Very Similar & Somewhat Similar vs. Not Si m-\nilar). To our knowledge, E6K has not been used to do \nmultiple evaluations of the same data set in this way, so \nwe do not have a basis for comparison. However, the re l-\natively low agreement between MTurk and E6K does u n-\nderscore the subjective nature of similarity ratings and \nsimilarity-based tasks in general. \nThe two sets of similarity judgments (MTurk & E6K) \nhave a Pearson’s correlation of r=0.495 , which while not \nparticularly strong, is comparable to the correlation \nHow similar are these songs?  \nListen to the following pairs of song clips, the 'query' is the \nsame for all 15 pairs. Evaluate how 'musically similar' each \ncandidate is to the given query. You will be presented with \nsongs from a number of different music genres. Please assign \nthe scores according to what you find 'sounds' similar and do \nnot take into account whether you like the music or not. Pr o-\nvide your best estimation of the similarity for each pair. You \nshould listen to a reasonable portion of every candidate b e-\nfore making your judgment. Answers w hich are incomplete \nor missing responses will be rejected. Answers which do not \nappear to contain honest judgments will be rejected.  \n \nAssign a similarity rating using the following 3 -point scale:  \n Not Similar  \n Somewhat Similar  \n Very Similar  \n185\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \n(r=0.433) found by Snow [ 12] between NLP experts and \nTurkers in an affect annotation study. \nLooking at the data in aggregate, Figure 3 shows the \nsimilarity judgments derived from MTurk tended to skew \ntowards Not Similar (NS=3,605), where as E 6K graders \ntended to assign similarity scores more uniformly across \nthe categories . In AMS 2009, as a requirement of partic i-\npation each team had to provide an E6K volunteer to help \nwith the judging for each algorithm they submitted. These \nvolunteers might have had some stake in the outcome of \nthe evaluation which might explain the greater proportion \nof VS scores in 2009 compared to 2006 and MTurk \nwhere the ratings were generated by independent volu n-\nteers. However, Figure 3 also shows the distribution of \nscores from previous MIREX cycles, and while the u n-\nderlying queries and candidates differ across the years, \nthe distribution of scores from MTurk is not dissimilar to \nother distributions from previous E6K results. \n \n \nFigure 3.  Distributions of scores from the 3 years of \nMIREX AMS evaluation using E6K compared to the di s-\ntribution of scores derived from MTurk. \nIn order to further investigate the similarity of MTurk \njudgments to E6K judgments, we collected multiple sim i-\nlarity judgments over the same set of QC pairs from se v-\neral different Turkers. Specifically, we randomly selected \none candidate from each candidate list for each of the 60 \nqueries used in MIREX 2006 AMS, and created 4 lists of \n15 QC pairs to be evaluated by three different Turkers. \nThis setup is very similar to how E6K was configured in \n2006  (multiple graders rating portions of candidate lists) , \nand given we were working with a subset of the 2006 d a-\nta, we are able to compare the inter-Turker agreement to \nthat found by Jones, et al. [7]. While we did not test SMS \nin MTurk, we have provided the data for comparison. \nTable 2 shows the agreement using the 3-level and 2-\nlevel analysis used in [7]. The overall distribution of \nscores is fairly similar to the AMS 2006 results; the pr o-\nportions of QC pairs across the various levels of agre e-\nment are comparable between the two data sources . The \npercentage of cases of total agreement are the same, and \nthere is some shifting of cases between partial and total \ndisagreement with slightly more cases of total disagre e-\nment (VS,SS,NS) among Turkers. This may be due to the \nnature of the QC pairs sampled for this evaluation which is plausible given the small sample size, or it may be i n-\nherent given what [7] describes the vague definition of \n“music similarity”.  \n \n3-level  SMS 2006  AMS 2006  MTURK  \nVS,VS,VS  114 12.6% 61 3.7% 4 6.7% \nSS,SS,SS  38 4.3% 137 8.4% 1 1.7% \nNS,NS,NS  263 29.1% 293 18.0% 13 21.7% \nTriples  415 45.9%  491 30.1%  18 30.0%  \nVS,VS,*  24 2.7% 150 9.2% 3 5.0% \nSS,SS,*  158 17.5% 469 28.8% 18 30.0% \nNS,NS,*  288 31.8% 404 24.8% 11 18.3% \nDoubles  470 51.9%  1023  62.8%  32 53.3%  \nVS,SS,NS  20 2.2% 115 7.1% 10 16.7% \n2-level  SMS 2006  AMS 2006  MTURK  \nS,S,S  188 20.8% 494 30.3% 19 31.7% \nNS,NS,NS  263 29.1% 293 18.0% 13 21.7% \nTriples  451 49.8%  787 48.3%  32 53.3%  \nS,S,N  166 18.3% 438 26.9% 17 28.3% \nN,N,S  288 31.8% 404 24.8% 11 18.3% \nDoubles  454 50.2%  842 51.7%  28 46.7%  \nTable 2.  Comparison of disagreement among \nTurkers and E6K graders from MIREX 2006 \nAMS evaluation. \nWhen we examine the results using a binary similarity \nmeasure (SS+VS against NS), we see greater similarity \nbetween the E6K graders and the Turkers. The distrib u-\ntions across the levels of agreement are nearly identical \nbetween the two sets. Jones [7] also found greater co n-\nsensus when considering similarity on a binary scale, and \nsuggest that the binary metric might be sufficient for the \ntask of evaluation. \nResearch Question II: How do evaluation outcomes for \ntasks like MIREX’s Audio Music Similarity evaluation \ndiffer when based on similarity judgments collected from \nMechanical Turk as compared to Evalutron6000? \nGiven the similarity judgments derived from MTurk \nappear to be different from those generated via E6K, we \nwish ed to see if those differences have any substantial \nbearing on MIREX evaluations. It is possible that the i n-\ndividual ratings differ, but still produce similar outcomes \nin comparing the performance of individual algorithms in \nthe audio music similarity task. Conversely, the diffe r-\nences may in fact be substantive and produce significan t-\nly different end results. \nMIREX evaluates the performance of similarity alg o-\nrithms using Friedman test with repeated-measures. Fig-\nure 4 shows a graphical depiction of the results of the \nMIREX 2009 AMS Friedman evaluation, comparing the \naverage rankings among the different algorithms. There \nare clearly two distinct groupings to the data: ANO, \nBSWH1, BSWH2, CL2, GT, LR, PS1, PS2, SH1, SH2 ; \nand BF1, BF2, CL1, ME1, ME2. One way to interpret the \nfigure is that all algorithms in one group are significantly \ndifferent from all algorithms in the other group, but wit h-\nin the groups the algorithms are not all significantly di f-00.10.20.30.40.50.6\nNS SS VSPercentage of JudgmentsComparison of Score Distributions\nAMS06\nAMS07\nAMS09\nMTurk\n186\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \nferent from each other. So, while PS2 does appear to lie \nslightly outside the rest of the larger group, it is not si g-\nnificantly different from all members of that group ( it \noverlaps partially with PS1). \n \nFigure 4 . Friedman rank comparison for MIREX 2009 \nAMS based on judgments from E6K (from [9]).  \n \nFigure 5.  Friedman rank comparison for MIREX \n2009 AMS based on judgments from MTurk. \nFigure 5 shows the Friedman evaluation results pe r-\nformed using the similarity judgments derived from \nMTurk. As we can see, the two main groupings are still \nevident, with clearly significant differences in the pe r-\nformance of the algorithms between the groups. Howe v-\ner, the gap between the groups is narrower, and the data \nare more compact. Furthermore, the global ordering of \nthe algorithms has changed. Notwithstanding these di f-\nferences, the overall results are remarkably similar.   \nThe main significant differences between the grou p-\nings are still evident and significant differences are still \npreserved among most of the algorithm pairs. Table 3 \nsummarizes the differences between the E6K and MTurk \nresults . Out of the 105 possible pair-wise comparisons \namong the 15 algorithms submitted to MIREX 2009 \nAMS, only six (5.7%) algorithm pairings were dete r-\nmined to be significantly different based on E6K jud g-\nments and not found to be significantly different based on MTurk judgments. No algorithms which were not signif i-\ncantly different under E6K were found to be significantly \ndifferent under MTurk. This discrepancy is not substa n-\ntially different compared to the Friedman results co m-\nputed using the E6K BROAD scores and FINE scores in \nAMS 2009 . The Friedman test based on the FINE scores \nrates 3 (2.9%) algorithm-pairs differently than the \nBROAD score results [9 ].  \n \nAlgo rithm  \n1 Algo rithm   \n2 Significant \nin E6K ? Significant \nin MTurk ? \nBSWH2  SH1 TRUE  FALSE  \nPS1 SH1 TRUE  FALSE  \nPS1 SH2 TRUE  FALSE  \nPS2 CL2 TRUE  FALSE  \nPS2 GT TRUE  FALSE  \nPS2 LR TRUE  FALSE  \nTable 3.  Excerpt from Friedman table of diffe r-\nences in significance between E6K and MTurk. \n5. IMPLICATIONS FOR MIR EVALUATION \nOverall, we were quite impressed with the quality of the \ndata we were able to obtain from MTurk. We can identify \nmany benefits to using MTurk in MIR evaluation, some \nof which include: \n1. The tasks were inexpensive to submit, costing appro x-\nimately USD$0.02 per judgment, although other r e-\nsearchers have obtained quality results for far less \npayment (c.f., [1],[12]). \n2. Evaluation was fast, taking less than 12 hours to co m-\nplete. It took over 2 weeks to obtain the same number \nof judgments using E6K. \n3. MTurk has a scriptable API, meaning it can be used \nfor “on-demand”  evaluation. This is especially attra c-\ntive given the developments of the MIREX Do- It-\nYourself infrastructure. \n4. No judging fatigue. MTurk provides a nearly endless \nsupply of willing labor, and it is not feasible to expect \nthe ISMIR community to continuously provide input \nfor use in MIREX DIY evaluations. Several Turkers \nsent us messages saying they found the HITs more fun \nthan most other HITs on MTurk, one even expressed a \nwillingness to work for free. \n5. Using MTurk for similarity judgments avoids any \nconflict of interest inherent in asking participants or \ntheir labmates to evaluate the results. \n6. MTurk provides a mechanism for compensating v o-\nlunteers for their time and effort. \n7. MTurk is considered “exempt” by human subjects r e-\nview as it falls under the description of a web survey, \nor as paid workers. \n8. Using MTurk does not preclude restricting particip a-\ntion in the evaluation to only ISMIR members. It is \npossible to require Turkers obtain a qualification prior \nto working on any HITs. Qualification credentials \ncould include membership in the ISMIR Society. \n187\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \n9. MTurk is very stable. It is built on Amazon’s cloud \ninfrastructure and is very robust. \nHowever, there are several limitations which need to be \nkept in mind when using MTurk, including: \n1. HITs need to contain validation questions which allow \nyou to check responses for quality and consistency. \nEarly in our exploration of MTurk we created HITs \nwithout validation questions and the data we collected \nwas highly inconsistent. \n2. The instructions we provided also needed clarification \nover several tests. We found it helpful to spell out the \nprecise conditions why a HIT would be rejected in the \ninstructions. Likewise, you need to provide clear e x-\nplanations why HITs were rejected.  \n3. It does cost money to use MTurk. While each HIT is \ncheap, in aggregate the price adds up quickly. Am a-\nzon’s overhead is either $ 0.005 or 10% per HIT, whi-\nchever is greater. However, the total costs are small \ncompared to the value of E6K volunteers’ time.  \n4. Some university human subjects review boards might \nnot be very familiar with MTurk and might be unsure \nhow to handle it. This could slow applications. \n6. CONCLUSIONS \nOur data show that while the specific judgments may di f-\nfer, using MTurk produces comparable results to using \nE6K for collecting human similarity judgments . The di f-\nferences are not dissimilar to the findings of other studies \nof MTurk and do not significantly alter the MIREX eva l-\nuation outcomes, indicating that MTurk may be a used as \na reliable source of similarity judgments for audio-based \nmusic similarity comparisons. Overall, the differences \nbetween MTurk and E6K judgments resulted in a 5.7% \ndifference in the ultimate outcome of the Friedman test \ncomparing the 15 algorithms submitted to AMS 2009. \nWe are excited by the possibilities MTurk offers to the \ndevelopment of future evaluation infrastructure, like the \nMIREX DIY, but are most excited by what MTurk can do \nfor the community as a whole. There are many types of \ndata which could be collected from Turkers, and it r e-\nmains to be seen how well MTurk is suited for collecting \nthose data. In future work we would like to explore other \ndata types; for example, music mood labels, music ta g-\nging, onset annotation work, key-identification, humming \nor singing, tapping, transcribing, etc.  \n7. ACKNOWLEDGEMENTS \nWe would like to thank IMIRSEL for providing us with \naccess to the MIREX data and helping us to reproduce \nthe MIREX analyses. \n8. REFERENCES \n[1] O. Alonso and S. Mizzaro: “Can we get rid of TREC \nassessors? Using Mechanical Turk for relevance \nassessment,” Proceedings of the SIGIR 2009 Workshop on the Future of IR Evaluation , pp. 15-16, \n2009.  \n[2] J-J. Aucouturier and F. Pachet: “Music Similarity \nMeasures: What’s the Use?” Proceedings of the 3rd \nInternational Society of Music Information Retrieval \n(ISMIR) Conference,  pp. 157- 163, 2002.   \n[3] D. Axe; “Geeks Spot Fossett?” Wired, Retrieved \nfrom http://www.wired.com/dangerroom/2007/09/ \ngeeks-spot-foss/comment-page- 3/, 2007.  \n[4] A. Berenzweig, B . Logan, D . P. W. Ellis, and B. \nWhitman: “A Large-Scale Evaluation of Acoustic \nand Subjective Music Similar ity Measures,” \nProceedings of the 4th ISMIR , pp. 99 –105, 2003. \n[5] D. P. W. Ellis, B. Whitman, A. Berenzweig, and S. \nLawrence: “The Quest For Ground Truth in Musical \nArtist Similarity,” Proceedings of the 3rd ISMIR \nConference, pp. 170-177, 2002.  \n[6] P. G. Ipeirotis: “Demographics of Mechanical \nTurk,” CeDER Working Papers, Retrieved from \nhttp://hdl.handle.net/2451/29585, 2010. \n[7] M. C. Jones, J. S. Downie, and A. F. Ehmann: \n“Human Similarity Judgments: Implications for the \nDesign of Formal Evaluations,” Proceedings of the \n8th ISMIR,  pp. 539-542, 2007. \n[8] A. Kittur, E. H. Chi, and B. Suh: “Crowdsourcing \nUser Studies With Mechanical Turk,” CHI 2008 \nProceedings,  pp. 453-456, 2008. \n[9] MIREX 2009 Wiki, Retrieved from http://music-\nir.org/mirex/2009/, 2009. \n[10] A. Novello and M. F. McKinney: “Assessment of \nPerceptual Music Similarity,” Proceedings of the 8th \nISMIR,  pp. 111-112, 2007. \n[11] E. Schubert and C. Stevens: “The Effect of Implied \nHarmony, Contour and Musical Expertise on \nJudgments of Similarity of Familiar Melodies,”  \nJournal of New Music Research,  Vol. 35, No. 2, pp. \n161-174, 2006. \n[12] R. Snow, B. O’Connor, D. Jurafsky, and A. Y. Ng: \n“Cheap and Fast –  But is it Good? Evaluating Non-\nExpert Annotations for Natural Language Tasks,” \nProceedings of the 2008 Conference on Empirica l \nMethods in Natural Language Processing,  pp. 254-\n263, 2008. \n[13] R. Timmers: “Predicting the Similarity Between \nExpressive Performances of Music From \nMeasurements of Tempo and Dynamics,” Journal of \nthe Acoustical Society of America,  Vol. 117, No. 1, \npp. 391- 399, 2005. \n \n188\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "Towards More Robust Geometric Content-Based Music Retrieval.",
        "author": [
            "Kjell Lemström"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1415150",
        "url": "https://doi.org/10.5281/zenodo.1415150",
        "ee": "https://zenodo.org/records/1415150/files/Lemstrom10.pdf",
        "abstract": "This paper studies the problem of transposition and time-scale invariant (ttsi) polyphonic music retrieval in symbolically encoded music. In the setting, music is represented by sets of points in plane. We give two new algorithms. Applying a search window of size w and given a query point set, of size m, to be searched for in a database point set, of size n, our algorithm for exact ttsi occurrences runs in O(mwn log n) time; for partial occurrences we have an O(mnw2 log n) algo- rithm. The framework used is flexible allowing devel- opment towards even more robust geometric retrieval.",
        "zenodo_id": 1415150,
        "dblp_key": "conf/ismir/Lemstrom10",
        "keywords": [
            "transposition",
            "time-scale invariant",
            "polyphonic music retrieval",
            "symbolically encoded music",
            "sets of points in plane",
            "search window",
            "query point set",
            "database point set",
            "exact ttsi occurrences",
            "partial occurrences"
        ],
        "content": "TOWARDS MORE ROBUST GEOMETRIC\nCONTENT-BASED MUSIC RETRIEVAL\nKjell Lemstr om\nDepartment of Computer Science\nUniversity of Helsinki\nABSTRACT\nThis paper studies the problem of transposition and\ntime-scale invariant ( ttsi) polyphonic music retrieval\nin symbolically encoded music. In the setting, music\nis represented by sets of points in plane. We give two\nnew algorithms. Applying a search window of size w\nand given a query point set, of size m, to be searched\nfor in a database point set, of size n, our algorithm for\nexact ttsioccurrences runs in O(mwn logn) time; for\npartial occurrences we have an O(mnw2logn) algo-\nrithm. The framework used is \rexible allowing devel-\nopment towards even more robust geometric retrieval.\n1. INTRODUCTION\nQuery-by-humming type problems have been under\nstudy for over \ffteen years. First, the music under\ninvestigation was assumed to be monophonic [3], later\nthe term has been used with a wider meaning address-\ning problems where the task is to search for excerpts\nof music, resembling a given query pattern, in a large\ndatabase. Moreover, both the query pattern and the\ndatabase may be polyphonic, and the query pattern\nconstitutes only a subset of instruments appearing in\nthe database representing possibly a full orchestra-\ntion of a musical piece. Although current audio-based\nmethods can be applied to rudimentary cases where\nqueries are directed to clearly separable melodies, the\ngeneral setting requires methods based on symbolic\nrepresentation that are truly capable of dealing with\npolyphonic subset matching.\nTo this end, several authors have recently used geo-\nmetric-based modeling of music [1, 7{9]. Geometric\nrepresentations usually also take into account another\nfeature intrinsic to the problem: the matching process\nignores extra intervening notes in the database that do\nnot appear in the query pattern. Such extra notes are\nalways present because of the polyphony, various noise\nsources and musical decorations. There is, however, a\nPermission to make digital or hard copies of all or part of this\nwork for personal or classroom use is granted without fee pro-\nvided that copies are not made or distributed for pro\ft or com-\nmercial advantage and that copies bear this notice and the full\ncitation on the \frst page.\nc\r2010 International Society for Music Information Retrieval.notable downside to the current geometric methods:\nthey do not usually allow distortions in tempo (ex-\ncept for individual outliers that are not even discov-\nered) that are inevitable in the application. Even if\nthe query could be given exactly on tempo, the occur-\nrences in the database would be time-scaled versions\nof the query (requiring time-scale invariance ). If the\nquery is to be given in a live performance, local jit-\ntering will inevitably take place and a stronger invari-\nance, namely the time-warping invariance [4], would\nbe a desired property for the matching process.\nIn this paper, new time-scale invariant geometric\nalgorithms that deal with symbolically encoded, poly-\nphonic music will be introduced. We use the pitch-\nagainst-time representation of note-on information, as\nsuggested in [9] (see Fig 1). The musical works in a\ndatabase are concatenated in a single geometrically\nrepresented \fle, denoted by T;T=t0;t1;:::;tn\u00001,\nwheretj2R2for 0\u0014j\u0014n\u00001. In a typical re-\ntrieval case the query pattern P,P=p0;p1;:::;pm\u00001;\npi2R2for 0\u0014i\u0014m\u00001, to be searched for is often\nmonophonic and much shorter than the database T\nto be searched. Sometimes a search window wis ap-\nplied and typically w <m , that isw <m\u001cn. It is\nassumed that PandTare given in the lexicographic\norder. If this is not the case, the sets can be sorted in\nO(mlogm) andO(nlogn) times, respectively.\nThe problems under consideration are modi\fed ver-\nsions of two problems originally represented in [8].\nThe following list gives both the original problems and\nthe modi\fcations under consideration; for the partial\nmatches in P2 and S2, one may either use a threshold\n\u000bto limit the minimum size of an accepted match, or\nto search for maximally sized matches only.\n(P1) Find translations of Psuch that each point in P\nmatches with a point in T.\n(P2) Find translations of Pthat give a partial match\nof the points in Pwith the points in T.\n(S1) Find time-scaled translations of Psuch that each\npoint inPmatches with a point in T.\n(S2) Find time-scaled translations of Pthat give a\npartial match of the points in Pwith the points\ninT.\nFig. 2 gives 4 query patterns to be searched for in\nthe excerpt of Fig. 1, exemplifying these 4 problems.\n577\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)In [6], Romming and Selfridge-Field gave a geomet-\nric hashing-based algorithm for S2. Without window-\ning, it works in O(n3) space and O(n2m3) time. This\npaper studies another way to solve problems S1 and\nS2. The new algorithms to be introduced resemble\nUkkonen et al's PI and PII algorithms. The algo-\nrithm for S1 runs in time O(mn2logn) andO(mn2)\nspace; the algorithm for S2 in O(m2n2logn) time and\nO(m2n2) space. An advantage of our method over the\none by Romming and Selfridge-Field is that the perfor-\nmance can be spedup by applying an index-\flter pre-\nprocessing [5]. Our method also seems to be adaptable\nto time-warping invariant cases. Thus, the method\nis an important step towards more robust geometric\ncontent-based music retrieval.\n2. RELATED WORK\nLet\u000bdenote a similarity threshold for P2, and let\np0;p1;:::;pm\u00001andt0;t1;:::;tn\u00001be the pattern and\ndatabase points, respectively, lexicographically sorted\naccording to their co-ordinate values: pi< pi+1i\u000b\npi:x < pi+1:xor (pi:x=pi+1:xandpi:y < pi+1:y),\nandtj< tj+1i\u000btj:x < tj+1:xor (tj:x=tj+1:x\nandtj:y < tj+1:y). In our application the elapsing\ntime runs along the horisontal axes, represented by\nx, the perceived height, the pitch, is represented by\ny. A translation of Pby vectorfis denotedP+f:\nP+f=p0+f;:::;pm\u00001+f. Using this notation,\nproblem P1 is expressible as the search for a subset I\nofTand somefsuch thatP+f=I. Note that\ndecomposing translation finto horisontal and ver-\ntical components f:xandf:y, respectively, captures\ntwo musically distinct phenomena: f:xcorresponds\nto aligning the pattern time-wise, f:ytotransposing\nthe musical excerpt to a lower or higher key. Note\nalso that a musical time-scaling \u001b,\u001b2R+, has an\ne\u000bect only on the horisontal translation, the vertical\ntranslation stays intact.\nExample 2.1. Letp=h1;1i,f=h2;2iand\u001b= 3.\nThenp+\u001bf=h7;3i.\nA straight-forward algorithm solves P1 and P2 in\nO(mnlog(mn)) time. The algorithm \frst exhaustively\ncollects all the translations mapping a point in Pto\nanother point in T. The set of the collected translation\nvectors are then sorted in lexicographic order. In the\ncase of P1, a translation fthat has been used mtimes\ncorresponds to an occurrence; in the case of P2, any\ntranslation fthat has been used at least \u000btimes would\naccount for an occurrence. Thoughtful implementa-\ntions of the involved scanning (sorting) of the transla-\ntion vectors, will yield an O(mn) (O(mnlogm)) time\nalgorithm for P1 (P2) [8].\nIndeed, the above O(mnlogm) time algorithm is\nthe fastest online algorithm known for P2. Moreover,\nany signi\fcant improvement in the asymptotic run-\nning time, exceeding the removal of the logarithmic\nfactor, cannot be seen to exist, for P2 is known to bea3SUM -hard problem [2]. It is still possible that P2\nis also a Sorting X+Y -hard problem, in which case\nUkkonen et al's PII algorithm would already be an\noptimal solution. In [2], Cli\u000bord et al introduced an\nO(nlogn) time approximation algorithm for P2.\nTo make the queries more e\u000ecient, several index-\ning schemes have been suggested. The \frst indexing\nmethod using geometric music representation was sug-\ngested by Clausen et al. [1]. Their sublinear query\ntimes were achieved by using inverted \fles, adopted\nfrom textual information retrieval. The performance\nwas achieved with a lossy feature extraction process,\nwhich makes the approach non-applicable to problems\nP1 and P2. Typke [7] proposed the use of metric\nindexes that works under robust geometric similar-\nity measures. However, it is di\u000ecult to adopt his\nmethod to support translations and partial matching\nat the same time. Lemstr om et al's approach [5] com-\nbines sparse indexing and (practically) lossless \flter-\ning. Their index is used to speed up a \fltering phase\nthat charts all the promising areas in the database\nwhere real occurrences could reside. Once a query\nhas been received, the \fltering phase works in time\nO(gf(m) logn+n) where function gf(m) is speci\fc to\nthe applied \flter f. The last phase involves checking\nthe foundcf(cf\u0014n) candidate positions using Ukko-\nnen et al's PI or PII algorithm executable in worst-case\ntimeO(cfm) orO(cfmlogm), respectively.\nThe only non-brute-force solution known for S1 and\nS2 is by Romming and Selfridge-Field [6]. It is based\non geometric hashing and works in O(n3) space and\nO(n2m3) time. By applying a window on the database\nsuch thatwis the maximum number of events that\noccur in any window, the above complexities can be\nrestated as O(w2n) andO(wnm3), respectively.\n3. MATCHING ALGORITHMS\nOur matching algorithms for the time-scale invariant\nproblems S1 and S2 resemble somewhat Ukkonen et\nal's PI and PII algorithms in that they all use a prior-\nity queue as a focal structure. Ukkonen et al's PI and\nPII work on trans-set translations, or trans-set vectors ,\nf=t\u0000p, wherepandtare points in a given query\npattern, of length m, and in the underlying database,\nof lengthn, respectively. Let us assume (without loss\nof generality) that all the points, both in the pattern\nand in the database, are unique. The number of trans-\nset vectors is within the range [ n+m\u00001;nm]. In order\nto be able to build an index on the database in an of-\n\rine phase, Lemstr om et al's method [5] is based on\nintra-set vectors . For instance, translation vector fis\nanintra-pattern vector , if there are two points pand\np0(p;p02P) such that p+f=p0.Intra-database vec-\ntors are de\fned accordingly. Naturally, the number\nof intra-pattern and intra-database vectors are O(m2)\nandO(n2), respectively.\nThe set of positive intra-pattern vectors include trans-\nlationspi0\u0000piwhere in the case of S1: 0 \u0014i < m\n578\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)/noteheads.s2/flags.d3\n/noteheads.s2Lei\n/noteheads.s2/noteheads.s2/noteheads.s2\n/noteheads.s1/noteheads.s1/flags.u3\n/dots.dot/noteheads.s2\nsei/flags.d3/noteheads.s2\nne/flags.d3/noteheads.s2\n43\n/clefs.F43/noteheads.s2/noteheads.s2\n/noteheads.s2/noteheads.s2 /clefs.G43/clefs.G\n/noteheads.s2/noteheads.s2/flags.d3\nund/noteheads.s2\n/accidentals.sharp/flags.d3\n/noteheads.s1nim\n/rests.1drecht,/noteheads.s2\n/noteheads.s1/noteheads.s1/noteheads.s1/dots.dot\n/dots.dot/dots.dot/flags.u3\n/dots.dot/dots.dot/noteheads.s2/flags.d3\nihm/noteheads.s2\n/dots.dot/dots.dot/dots.dot /noteheads.s2/rests.1steht/noteheads.s2/flags.u3\ner/noteheads.s2\n/accidentals.sharpstill,/noteheads.s2/flags.u3\nmer/noteheads.s2/dots.dot\n/noteheads.s1/noteheads.s1/noteheads.s2/noteheads.s2/rests.1\nc1\ntimepitch\nCc\n2 3 1Figure 1 .On top, an excerpt from Franz Schubert's song\ncycle Winterreise. Below, the related geometric, point-set\nrepresentation. The points associated with the vocal part\nare represented distinctly (by squares). The depicted 6\nintra-database vectors will be discussed later.\nandi0=i+ 1, and in the case of S2: 0 \u0014i < i0\u0014\nm. The set of positive intra-database vectors include\ntranslations tk0\u0000tkwhere, independently of the case,\n0\u0014k<k0\u0014n. To reduce the search space, one may\napply a window that restates the bounds for i0(in the\ncase of S2) and k0in the obvious way: 0 \u0014i < i0\u0014\nminfi +w;mgand 0\u0014k<k0\u0014minfk +w;ng.\nFor the convenience of the algorithms, we pretend\nthat there is an extra elements pmin the pattern and\ntnin the database. The matching algorithms take\nas input intra-set vectors, stored in tables K[i];0\u0014\ni < m . TableK[i] stores intra-database translations\nthat may match1the positive intra-pattern vectors\npi0\u0000pi, i.e., translation vectors starting at point pi.\nSee Fig. 3 for an illustration on tables K[i].\nThe entries in our main data structures will be\nsorted in a lexicographic order. We will specify the\nunderlying order by an ordered set @.@is formed by\nmembers offa;b;sg, where a;bandscorrespond to\nthe columns named accordingly in tables K[i]. For in-\nstance, lexicographic order ha;si is \frstly based on the\nvalues on column a(the starting point of the associ-\nated intra-database vector), secondly on the values on\ncolumns(the associated scaling value). A main loop\nthat goes exhaustively through all the possibilities of\npositive intra-pattern and positive intra-database vec-\ntors to initialise the tables K[i] is needed. To this end,\nlet a positive intra-database vector g=tk0\u0000tkbe such\nthat there is a positive intra-pattern vector f=pi0\u0000pi\n1Please note the distinction between an occurrence and a\nmatch. An occurrence involves as many matching pairs of intra-\ndatabase and intra-pattern vectors as is required.h0;7i h0; 12i h 2;15i h0; 5i h 2;8i h 4;4i\nh2;3i h4;\u00001i h4; 2i h2;\u00004i h 2;\u00001i h4;\u00008i\nh0;3i h2;\u00004i h4; 3i h2;\u00007i h4; 0i h 6;\u000014i\nh2;7i h4;\u00007i h4; 0i h 2;\u000014i h2;\u00007i h2;\u00002i\nh0;7i h0; 12i h 0;24i h0; 5i h0; 17i h 1;24i\nh0;12i h 1;19i h 2;17i h1; 7i h2; 5i h 3;8i\nh1;\u00002i h2; 1i h3; 0i h1 ;3i h2; 2i h 4;\u000014i\nh1;\u00001i h3;\u000017i h3;\u00008i h2;\u000016i h2;\u00007i h4;\u000018i\nh0;9i h2;\u00001i h 2;11i h2;\u000010i h 2;2i h 4;\u000012i\nh0;12i h 2;\u00002i h 2;13i h2;\u000014i h 0;1i h0; 15i\nTable 1 .The intra-database vectors generated by the\nexample given in Fig. 1 when ignoring the \frst bar and\nsetting w= 3. The \frst and the last intra-database vectors\nunder consideration are depicted in Fig. 1 as arrows with\nsolid and dashed stems, respectively.\nfor whichg:y=f:y(ie. the pitch intervals of the two\nvectors match). Because gmay be part of an occur-\nrence, a new row, let it be the hth, inK[i] is allocated\nand the following updates are conducted:\nK[i]h:a k;K[i]h:b k0; (1)\nK[i]h:s tk0:x\u0000tk:x\npi0:x\u0000pi:x; (2)\nK[i]h:y nil;K[i]h:w 1; (3)\nK[i]h:c i0;K[i]h:z 0: (4)\nAbove, in (1), the associated starting and ending points\nof the matching intra-database vector are stored in\nK[i]h:aandK[i]h:b, respectively. The required time\nscaling for the intra-vectors to match is stored in K[i]h:s\n(2); here extra carefulness is needed to avoid zero di-\nvision: If both the numerator and the denominator\nequal zero, we set K[i]h:s= 1. If only one of them\nequals zero or both equal in\fnity, the whole row is\ndeleted from the table (they would represent impos-\nsible time scalings). Columns yandw, initialised in\n(3), are used for backtracking a found occurrence and\nstoring the length of a candidate occurrence, respec-\ntively. The last columns, initialised in (4), will be\nneeded when searching for partial occurrences (in Sec-\ntion 3.2): column cstores the ending point of the as-\nsociated intra-pattern vector, zis used for identifying\nan occurrence.\nDenoting by \u0006 pithe number of rows generated above\nfor tableK[i];0\u0014i<m, for the aforementioned ex-\ntra elements we set:\nK[i]\u0006pi:a K[i]\u0006pi:b 1;\nK[i]\u0006pi:s K[i]\u0006pi:w 0;K[i]\u0006pi:c i+ 1\nAs each iteration of the main loop takes constant\ntime, this exhaustive initialisation process runs in time\nO(nmw2). Finally, the columns in K[i] are sorted in\nlexicographic order ha;si. The matching algorithms\nhave an associated priority queue Qifor each table\nK[i];0< i\u0014m2. ForQi, a lexicographic order\nhb;siis used. As a reminder, the order is given in the\nsuperscript of a priority queue (e.g. Qhb;si\ni).\n2A single priority queue would su\u000ece, but the algorithm\nwould become more complicated.\n579\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)A\n/timesig.C44/noteheads.s2 /noteheads.s2 /accidentals.sharp/noteheads.s2/noteheads.s2 /noteheads.s2/noteheads.s2 /noteheads.s2/noteheads.s2B\n/noteheads.s2/noteheads.s2 /noteheads.s2 /accidentals.flat/accidentals.flat/clefs.G /noteheads.s2\n/accidentals.sharp/noteheads.s1/dots.dot/noteheads.s2/dots.dot/noteheads.s2/dots.dot/noteheads.s2/dots.dot/noteheads.s1/dots.dot /clefs.G/accidentals.flat/accidentals.flat\n46C\n/noteheads.s2/dots.dot/noteheads.s1/dots.dot/noteheads.s2/dots.dot/noteheads.s2/dots.dot/noteheads.s2/dots.dot /noteheads.s2D\n/dots.dot/noteheads.s1/dots.doth6;3i h6;\u00002i h6;\u00007i h6; 7i h12;\u00002i\nFigure 2 .On top, 4 example queries. For query Aan\noccurrence in the excerpt given in Fig. 1 would be found\nin all four cases P1, P2, S1 and S2; for Bin cases P2, S1\nand S2; for Cin S1 and S2; and for Din S2 only. At the\nbottom, the positive intra-pattern vectors, associated with\nthe query Cin case S1.\n3.1 S1: Time Scaled Exact Matching\nOnce the tables K[i] have been initialised and their\ncolumns have been sorted in lexicographic order ha;si,\nthe transposition-invariant time-scaled exact occur-\nrences can be found using the matching algorithm\ngiven in Fig. 4. The algorithm works by observing\npiecewise matches between positive intra-database and\nintra-pattern vectors\ntki0\u0000tki=\u001bi(pi+1\u0000pi) (5)\nthat are stored in the associated K[i]. Above\u001bi2\nR+is the time-scaling factor (recall Example 2.1).\nThe piecewise matches may form a chain T\u001c0:::\u001cm\u00001=\nt\u001c0;t\u001c1;:::;t\u001cm\u00001, where\u001c0;\u001c1;:::;\u001cm\u00001is an increas-\ning sequence of indices in T;t\u001ci+1\u0000t\u001ci=\u001b(pi+1\u0000pi)\nfor 0\u0014i < m\u00001 and\u001b2R+is a time-scaling fac-\ntor common to all the piecewise matches in the chain.\nAs such chains represent transposition-invariant, time-\nscaled exact occurrences, the task is to look for them.\nA chainT\u001c0:::\u001cm0m0< m\u00001, is called a pre\fx\noccurrence (of lengthm0);T\u001cm0\u00001;\u001cm0is the \fnal su\u000ex\nof the pre\fx occurrence T\u001c0:::\u001cm0. Lett\u001ci+1\u0000t\u001ci(that,\nby de\fnition, equals \u001b(pi+1\u0000pi)) be the \fnal su\u000ex of\na pre\fx occurrence T\u001c1:::\u001cm0. The pre\fx occurrence is\nextensible if there is a piecewise match tk0\ni+1\u0000tki+1=\n\u001b(pi+2\u0000pi+1) such that\nt\u001ci+1=tki+1 (6)\nand scaling factor \u001bis the one that was used in form-\ningT\u001c1:::\u001cm0. The binding in Equation 6 is called the\nbinding of extension, t\u001ci+1\u0000t\u001citheantecedent and\ntk0\ni+1\u0000tki+1thepostcedent of the binding.\nLemma 3.1. If a pre\fx occurrence is extensible, its\n\fnal su\u000ex is also extensible.\nProof. Immediate.\nTo be more e\u000ecient, at point i+ 1, the algorithm\nactually considers any piecewise match tk0\ni\u0000tki=\n\u001bi(pi+1\u0000pi) as an antecedent to the binding and\ntries to extend it. Because in this case the piecewise\n1 0\n LEGEND\ni\ni’h: running index on the associated table\na: id of the point in T associated with p \nw: cumulative weight; the length of the occurrence thus far\nz: running number (id) of an associated occurrenceb: id of the point in T associated with p\ny: backward link to be able to construct the matchs: scaling factor of the associated vector c: i’     = # of matches (−1) found for p − p0 12 13 1 1 / 3 1 nil 0\nΣp0sK[5] \nK[1]\nh \n K[0]  \na b c y w z\n14 17 1 2 / 3 1 nil 0\n23 24 1 1 / 3 1 nil 01\n2=\nΣp0Figure 3 .Illustration of tables K[i] when considering\nproblem (S1) and searching for occurrences for the query\nCof Fig. 2 within the two last bars of Fig. 1, w= 3. The\nintra-database vectors under consideration are the ones\ngiven in Table 1. Having initialized the tables Kin equa-\ntions (1-4), K[0] contains the depicted 3 rows.\nmatches in an occurrence chain have to be consecu-\ntive inP, the antecedents of the binding are all found\ninK[i] and their possible extensions, postcedents, in\nK[i+ 1]. To process all the bindings of extension at\npointi+ 1, therefore, involves going through all the\nentries both in K[i] and inK[i+1]. To make this pro-\ncess e\u000ecient, no entry of either of the tables should be\nobserved more than once for one iteration. In order\nfor this to be possible, both sides of the binding of ex-\ntension (associated with antecedents and postcedents)\nshould be enumerated in the same (increasing) order.\nHowever, the lefthand side of the binding involves end\npoints of the intra-database vectors in K[i] and the\nrighthand side the start points of the intra-database\nvectors inK[i+ 1]. Therefore, we use a priority queue\nQhb;si\niwhose entries are addresses to rows associated\nwith the antecedents of the binding at i. In this way,\nthe binding of extension at ican be done e\u000eciently\nby enumerating the entries in QiandK[i]. Note that\nthe set of piecewise matches extended this way also\nincludes all the \fnal su\u000exes, and therefore, according\nto Lemma 3.1, also all the pre\fx occurrences.\nThe binding of extension takes place in line (8) of\nthe algorithm. If a piecewise match is extensible, its\nlength is updated (line 9) and a backtracking link is\nstored (line 10). The latter becomes useful if any of\nthe extended piecewise matches extends into a proper\noccurrence, and the whole occurrence is to be revealed\n(instead of just reporting it).\nLet us now demonstrate the main idea of the algo-\nrithm by using a musical example.\nExample 3.2. The vocal line in Fig. 1 ends in a sus-\npension that is dissolved at the beginning of the third\n580\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)TimeScaledExactOccurrence (K[i])\n(0)K[0]\u0006p0:s 1\n(1)forj 0;:::; \u0006p0do\n(2)Qhb;si\n1 push (&K [0]j)\n(3)fori 1;:::;m\u00002do\n(4)q pop(Qhb;si\ni)\n(5) forj 0;:::; \u0006pi\u00001do\n(6) while [q:b;q:s]<[K[i]j:a;K [i]j:s]do\n(7) q pop(Qhb;si\ni)\n(8) if[q:b;q:s] = [K [i]j:a;K [i]j:s]then\n(9) K[i]j:w q:w+ 1\n(10) K[i]j:y q\n(11) Qhb;si\ni+1 push (&K [i]j)\n(12) q pop(Qhb;si\ni)\n(13)K[i]\u0006pi:s 1\n(14)Qhb;si\ni+1 push (&K [i]\u0006pi)\n(15)ifK[m\u00002]j:w=m\u00001for some 0\u0014j\u0014\u0006pm\u00002\n(16) then report an occurrence\nFigure 4 .Algorithm for \fnding transposition-invariant\ntime-scaled exact occurrences.\nbar. As the expectation for the dissolution is strong\nand the lower part of the accompaniment resides in\nthe same register as the vocal part, if suitably arranged,\nthe listener perceives the higher \"a\" of the lower part of\nthe accompaniment to belong to the vocal melody. The\nqueries in Fig. 2 look for occurrences with such a dis-\nsolution. To solve S1 with the query Cof Fig. 2, the\nalgorithm \frst \flls Table K[0]with rows corresponding\nto the intra-database vectors that match the interval of\nthe \frst intra-pattern vector h6;3i(bolded in the \fg-\nure). The matching vectors are depicted in Fig. 1 (ar-\nrows with a dotted stem) and given bolded in Table 1.\nNote that the vector h0;3iis not accepted since the\nassociated scaling would actually squeeze any melody\nto a chord. The three accepted piecewise matches are\nstored inK[0](see Fig. 3) and the algorithm contin-\nues by looking for piecewise matches in K[1]that could\nextend them.\nAnalysis. Let us denote by jQijandjK[j]jthe num-\nber of entries in QiandK[j], respectively. Clearly,\nin this case,jQij=jK[i\u00001]jfor 1\u0014i\u0014m. More-\nover, let \u0006 = maxm\ni=1(jQij;jK[i\u00001]j). The outer loop\n(line (3)) is iterated mtimes. Within the inner loop\n(line (5)), all the entries in Qiand inK[i] are pro-\ncessed exactly once, resulting in O(\u0006) entry processing\nsteps. The only operation taking more than a constant\ntime is the updating of the priority queue; it takes at\nmostO(log \u0006) time. Thus, the algorithm runs in time\nO(m\u0006 log \u0006). Moreover, the tables K[i] and priority\nqueuesQirequireO(m\u0006) space.\nIn this case \u0006 = O(wn), because each table K[i]\ncontains the piecewise matches for the positive intra-TimeScaledPartialOccurrence (K[i])\n(0)` 0;K[0]\u0006p0:s 1\n(1)fori 0;:::;m\u00002\n(2) forj 0;:::; \u0006p0\n(3) Qhb;si\nK[i]j:c push (&K [i]j)\n(4)fori 1;:::;m\u00002do\n(5)q pop(Qhb;si\ni)\n(6) forj 0;:::; \u0006pi\u00001do\n(6,5) if[q:b;q:s]>[K[i]\u0006pi:a;K [i]\u0006pi:s]break\n(7) while [q:b;q:s]<[K[i]j:a;K [i]j:s]do\n(8) q pop(Qhb;si\ni)\n(9) if[q:b;q:s] = [K [i]j:a;K [i]j:s]then\n(10) while min (Qhb;si\ni) = [q:b;q:s] do\n(11) r pop(Qhb;si\ni)\n(12) ifr:w>q:w thenq r\n(13) K[i]j:w q:w+ 1\n(14) K[i]j:y q\n(15) ifK[i]j:w=\u000bthen\n(16) ` `+ 1\n(17) K[i]j:z=`\n(18) \u0014[`] &K[i]j\n(19) ifK[i]j:w>\u000b then\n(20) K[i]j:z=q:z\n(21) \u0014[q:z ] &K[i]j\n(22) Qhb;si\nK[i]j:c push (&K [i]j)\n(23)K[i]\u0006pi:s 1\n(24)Qhb;si\ni+1 push (&K [i]\u0006pi)\n(25)ReportOccurrences (\u0014)\nFigure 5 .Algorithm for \fnding transposition-invariant\ntime-scaled partial occurrences. The optimizer at line (6,5)\ncan be omitted without breaking functionality; it can also\nbe used to optimize the previous algorithm (as line (5,5)).\npattern vector pi+1\u0000pi, and there are O(wn) possi-\nbilities to this end. Naturally w=nif no windowing\nhas been applied.\n3.2 S2: Time Scaled Partial Matching\nIn order to be able to \fnd transposition-invariant time-\nscaled partial occurrences, we need the two extra columns\ncandz, that were initialised in Equation 4, for tables\nK[i]. Recall that K[i]h:cstores the ending point i0for\nan intra-pattern vector pi0\u0000pithat is found to match\nan intra-database vector tk0\u0000tkwith some scaling fac-\ntor\u001bi. Columnzis used for storing a running number\nthat is used as an id, for a found partial occurrence.\nFurthermore, we use an extra table, denoted by \u0014, for\nstoring all the found occurrences.\nThe structure of the algorithm (see Fig. 5) is sim-\nilar to the previous algorithm. Again, at point i, the\nantecedents in Qiare to be extended by postcedents\nfound inK[i]. However, as we are looking for par-\ntial occurrences this time, we cannot rely on piecewise\n581\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)matches that are consecutive in Pbut any piecewise\nmatch associated with a positive intra-pattern vector\ntki0\u0000tki=\u001bi(pi0\u0000pi) (7)\nhas to be considered. Here 0 \u0014ki< ki0\u0014minfki+\nw;ng; 0\u0014i<i0\u0014minfi +w;mgand\u001bi2R+. Given\na threshold \u000b, a chainT\u001c0:::\u001c\f\u00001, such thatt\u001cj\u0000t\u001cj\u00001=\n\u001b(p\u0019j\u0000p\u0019j\u00001), for 0< j\u0014\f\u00001; \f\u0015\u000b, where\n\u001c0:::\u001c\f\u00001and\u00190:::\u0019\f\u00001are increasing sequences of\nindices inTandP, respectively, would constitute for a\ntransposition-invariant time-scaled partial occurrence\n(of length\f).\nThat piecewise matches can now be between any\ntwo points in the pattern makes the problem some-\nwhat more challenging. This has the e\u000bect that, at\npointi, pushing a reference to a priority queue (lines\n(2) and (21) of the algorithm) may involve any fu-\nture priority queue, from Qi+1toQm, not just the\nsuccessive one as in the previous case; the correct pri-\nority queue is the one that is stored in K[i]j:c(recall\nthat it stores the end point of the intra-pattern vec-\ntor associated with the piecewise match). Conversely,\nthe antecedents at point i(stored inQi) may include\nreferences to any past tables within the window size,\nexpanding the size of the priority queue Qi.\nThe two remaining di\u000berences to the algorithm above,\nare in lines (11) and (14-20). In line (11), the algo-\nrithm chooses to extend the piecewise match that is\nassociated with the longest pre\fx occurrence. This is\na necessary step, once again, because we are no more\ndealing with piecewise matches that are consecutive in\nP. In lines (14-20) the algorithm deals with a found\noccurrence. Lines (14-17) deal with a new occurrence:\ngenerate a new running number, `, for it (that is used\nas its id) and store a link to the found occurrence to\nthe table of occurrences \u0014. Lines (18-20) deal with\nextending a previously found occurrence.\nAnalysis. Let \u0006 =maxm\ni=1(jQij;jK[i\u00001]j). With\nan analogous reasoning to that of the previous anal-\nysis, we arrive at similar complexities: the algorithm\nruns inO(m\u0006 log \u0006) time and O(m\u0006) space. Let us\nnow analyse the order of \u0006 in this case. Still it holds\nthat for a positive intra-pattern vector, pi+1\u0000pi, there\nareO(wn) possible piecewise matches. However, the\ntableK[i] may contain entries associated with piece-\nwise matches with any positive intra-pattern vector\nending at point i+ 1. Thus, maxm\ni=1(jK[i\u00001]j) =\nO(minfm;wgwn). AsjQij=jK[i\u00001]jfor 0<i\u0014m\nand assuming w<m, we conclude that the algorithm\nhas anO(mnw2logn) running time and works in a\nspaceO(mnw2).\n4. CONCLUSIONS\nIn this paper we suggested novel content-based mu-\nsic retrieval algorithms for polyphonic, geometrically\nrepresented music. The algorithms are both transpo-\nsition and time-scale invariant. Given a query pat-\nternP=p0;:::;pm\u00001 to be searched for in a musicdatabaseT=t0;:::;tn\u00001and applying a search win-\ndow of size w, the algorithms run in O(m\u0006 log \u0006) time\nandO(m\u0006) space, where \u0006 = O(wn) when search-\ning for exact occurrences under such a setting, and\n\u0006 =O(nw2) when searching for partial occurrences.\nWhether this is an improvement in practice over the\nexisting algorithm by Romming and Selfridge-Field\n[6], working in space O(w2n) and time O(wnm3), is\nleft for future experiments on real data.\nHowever, the framework seems to be very \rexible:\nit is currently under modi\fcation to a more complex\ncase, where an uneven time deformation is known just\nto preserve the order of the notes; there are no known\nsolutions for this time-warping invariant problem [4].\nMoreover, it seems that with some modi\fcations to\nthe data structures and ideas presented in [5] it would\nbe possible to adopt the idea of using a three-phase\nsearching process (indexing, \fltering and checking) re-\nsulting in a smaller search space and a better running\ntime to those presented here.\nAcknowledgements This work was supported by\nthe Academy of Finland (grants #108547 and #218156).\n5. REFERENCES\n[1] M. Clausen, R. Engelbrecht, D. Meyer, and J. Schmitz.\nProms: A web-based tool for searching in polyphonic\nmusic. In Proc. ISMIR'00 , Plymouth, MA, October\n2000.\n[2] R. Cli\u000bord, M. Christodoulakis, T. Crawford,\nD. Meredith, and G. Wiggins. A fast, randomised,\nmaximal subset matching algorithm for document-level\nmusic retrieval. In Proc. ISMIR'06 , pp. 150{155, Vic-\ntoria, BC, October 2006.\n[3] A. Ghias, J. Logan, D. Chamberlin, and B. Smith.\nQuery by humming - musical information retrieval in\nan audio database. In Proc. ACM Multimedia , pages\n231{236, San Francisco, CA, 1995.\n[4] K. Lemstr om and G. Wiggins. Formalizing invariances\nfor content-based music retrieval. In Proc. ISMIR'09 ,\npp. 591{596, Kobe, October 2009.\n[5] K. Lemstr om, N. Mikkil a, and V. M akinen. Filter-\ning methods for content-based retrieval on indexed\nsymbolic music databases. Journal of Information Re-\ntrieval, 13(1):1{21, 2010.\n[6] C. Romming and E. Selfridge-Field. Algorithms for\npolyphonic music retrieval: The hausdor\u000b metric and\ngeometric hashing. In Proc. ISMIR'07 , pp. 457{462,\nVienna, September 2007.\n[7] R. Typke. Music Retrieval based on Melodic Similarity .\nPhD thesis, Utrecht University, Netherlands, 2007.\n[8] E. Ukkonen, K. Lemstr om, and V. M akinen. Geomet-\nric algorithms for transposition invariant content-based\nmusic retrieval. In Proc. ISMIR'03 , pp. 193{199, Bal-\ntimore, MA, October 2003.\n[9] G. Wiggins, K. Lemstr om, and D. Meredith.\nSIA(M)ESE : An algorithm for transposition invariant,\npolyphonic content-based music retrieval. In Proc. IS-\nMIR'02 , pp. 283{284, Paris, October 2002.\n582\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "Hierarchical Co-Clustering of Artists and Tags.",
        "author": [
            "Jingxuan Li",
            "Tao Li 0001",
            "Mitsunori Ogihara"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1416718",
        "url": "https://doi.org/10.5281/zenodo.1416718",
        "ee": "https://zenodo.org/records/1416718/files/LiLO10.pdf",
        "abstract": "The user-assigned tag is a growingly important research topic in MIR. Noticing that some tags are more specific versions of others, this paper studies the problem of orga- nizing tags into a hierarchical structure by taking into ac- count the fact that the corresponding artists are organized into a hierarchy based on genre and style. A novel clus- tering algorithm, Hierarchical Co-clustering Algorithm (HCC), is proposed as a solution. Unlike traditional hi- erarchical clustering algorithms that deal with homoge- neous data only, the proposed algorithm simultaneously organizes two distinct data types into hierarchies. HCC is additionally able to receive constraints that state certain ob- jects “must-be-together” or “should-be-together” and build clusters so as to satisfying the constraints. HCC may lead to better and deeper understandings of relationship between artists and tags assigned to them. An experiment finds that by trying to hierarchically cluster the two types of data better clusters are obtained for both. It is also shown that HCC is able to incorporate instance-level constraints on artists and/or tags to improve the clustering process.",
        "zenodo_id": 1416718,
        "dblp_key": "conf/ismir/LiLO10",
        "keywords": [
            "user-assigned tag",
            "MIR",
            "organizing tags",
            "hierarchical structure",
            "novel clustering algorithm",
            "Hierarchical Co-clustering Algorithm (HCC)",
            "traditional hierarchical clustering",
            "data types",
            "hierarchies",
            "constraints"
        ],
        "content": "HIERARCHICAL CO-CLUSTERING OF MUSIC ARTISTS AND TAGS\nJingxuan Li\nSchool of Computer Science\nFlorida International University\nMiami, FL USA\njli003@cs.fiu.eduTao Li\nSchool of Computer Science\nFlorida International University\nMiami, FL USA\ntaoli@cs.fiu.eduMitsunori Ogihara\nDepartment of Computer Science\nUniversity of Miami\nCoral Gables, FL USA\nogihara@cs.miami.edu\nABSTRACT\nThe user-assigned tag is a growingly important research\ntopic in MIR. Noticing that some tags are more speciﬁc\nversions of others, this paper studies the problem of orga-\nnizing tags into a hierarchical structure by taking into ac-\ncount the fact that the corresponding artists are organized\ninto a hierarchy based on genre and style. A novel clus-\ntering algorithm, Hierarchical Co-clustering Algorithm\n(HCC), is proposed as a solution. Unlike traditional hi-\nerarchical clustering algorithms that deal with homoge-\nneous data only, the proposed algorithm simultaneously\norganizes two distinct data types into hierarchies. HCC is\nadditionally able to receive constraints that state certain ob-\njects “must-be-together” or “should-be-together” and build\nclusters so as to satisfying the constraints.\nHCC may lead to better and deeper understandings of\nrelationship between artists and tags assigned to them. An\nexperiment ﬁnds that by trying to hierarchically cluster the\ntwo types of data better clusters are obtained for both. It is\nalso shown that HCC is able to incorporate instance-level\nconstraints on artists and/or tags to improve the clustering\nprocess.\n1. INTRODUCTION\nThe user-deﬁned tags are becoming an essential compo-\nnent in web databases and social network services. The\ntags assigned to events and data objects as a whole rep-\nresent how they are received by the community and pro-\nvide keys to other users accessing them. In music informa-\ntion retrieval some recent papers study how to incorporate\ntags effectively for fundamental data retrieval tasks such as\nclustering, recommendation, and classiﬁcation (see, e.g.,\n[4, 19, 21, 22]).\nAn important characteristic of the tags is that sometimes\ntags are extensions of others and thus more speciﬁc than\nthose they extend, e.g., “Soft Metal” extending “Metal”,\n“Dance Pop” extending “Pop”, and “Extremely Provoca-\ntive” extending “Provocative”. Since there is no limit in\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc⃝2010 International Society for Music Information Retrieval.the length of tags, a tag can be an extension of another\none, which is an extension of yet another one. This sug-\ngests that the music tags can be not only clustered as has\nbeen done before but hierarchically clustered.\nMany approaches have been developed to produce hi-\nerarchical organizations of words and of documents, and\nso organizing tags into a hierarchy is a problem that is\nalready-solved. However, we observe that the artists, to\nwhich tags are assigned, too can be organized into a hi-\nerarchical structure based on their prominent genres and\nstyles. In fact, these hierarchies are much related to each\nother, since style labels often appear as tags. This leads\nto the questions of whether an attempt to build simultane-\nously hierarchical organizations of tags and of artists will\nlead to better organizations of both and of whether such\norganizations can be effectively and efﬁciently built.\nIn data mining the problem of developing hierarchical\norganization of data is referred to as hierarchical cluster-\ningand the problem of clustering two data types is referred\nto as co-clustering. While co-clustering essentially aims\nat simultaneously clustering rows and columns of a ma-\ntrix, where the rows and the columns correspond to sep-\narate data types (e.g., terms and documents), hierarchical\nclustering aims at building a tree-like structure of the rows\nbased on the columns a tree-like structure of the columns\nbased on the rows. While both organizations have their\nown advantages, such as natural facilitation of data navi-\ngation and browsing in hierarchical clustering [6], few al-\ngorithms simultaneously build both [2].\nIn this paper, we develop a novel method, called\nHCC, for simultaneously clustering two data types, and\nwe use HCC for building hierarchical co-clusters of tags\nand styles. HCC is designed based on the approaches\nin [10, 14]. HCC is essentially agglomerative hierarchi-\ncal clustering: it starts with singleton clusters and then re-\npeatedly merging two nearest clusters into one until there\nremains only one cluster. However, it may merge groups\nfrom different data types at any point. In our case, this\nmeans that at each step of the merging process, HCC can\nmerge a subset of the artists with a subset of the tags based\non their internal heterogeneity. In practice, one sometimes\nobserves that a group of artists and a group of tags are\nexclusively correlated with each other (i.e., not correlated\nwith any other artists or tags). HCC aims at, in such a\nsituation, merging them into a single group at the earliest\npossible stage.\n249\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Our hope\nis that such artist-tag mixed clusters will be\nused for better retrieval when both artists and tags are spec-\niﬁed in a query.\nFigure 1 shows a sample output dendrogram of HCC\nwhile Figure 2 shows a sample output dendrogram of a tra-\nditional hierarchical clustering method. We show that such\nmixed-data-type hierarchical clusters can be generated by\nHCC and empirically better clusters are generated by con-\ncurrent use of two data types. Furthermore, we show that\nHCC can be extended to incorporate instance-level con-\nstraints that specify certain tags must be or must not be\ntogether or certain artists must be or must not be together\nfor better organization.\nThe rest of the paper is organized as follows: Section 2\ndiscusses the related work; Section 3 describes the details\nof HCC and the techniques for incorporating instance-level\nconstraints; Section 4 presents experimental results; and\nﬁnally Section 5 provides our conclusions.\n2. RELATED WORK\nHierarchical Clustering is generation of tree-like cluster\nstructures without user supervision. Hierarchical cluster-\ning algorithms organize input data either bottom-up (ag-\nglomerative) or top-down (divisive) [20]. Co-clustering\nrefers to clustering of more than one data type. Dhillon [7]\nproposes bipartite spectral graph partitioning approaches\nto co-cluster words and documents. Long et al. [15] pro-\nposed a general principled model, called Relation Sum-\nmary Network, for co-cluster heterogeneous data presented\nas a k-partite graph. While hierarchical clustering deals\nwith only one type of data and the organization that co-\nclustering produces consists of just one level, Hierarchi-\ncal Co-clustering aims at simultaneously construction of\ntwo or more hierarchies [12, 13].\nRecently much work has been done on the use of back-\nground information in the form of instance level must-\nlink and cannot-link constraints. This topic is referred to\nasConstrained Clustering . Here a must-link constraint\nenforces that two instances must be placed in the same\ncluster and a cannot-link constraint enforces that two in-\nstances must not be placed in the same cluster. Most of\nthese constraint-based algorithms are developed for parti-\ntional clustering (e.g, K-means clustering, spectral cluster-\ning, and non-negative matrix factorizations) [1], and little\nhas been done on utilizing constraints for hierarchical clus-\ntering.\n3. HIERARCHICAL CO-CLUSTERING (HCC)\n3.1 Problem Formulation\nSuppose we are given a set of m artists\nA=fa1; a2; : : : ; a mg, and a set of nunique tags that are\nassigned to the music of these artists T=ft1; t2; : : : ; t ng.\nSuppose we are also given an m\u0002nartist-tag relationship\nmatrix X= (x ij)2Rm×n, such that xijrepresents the\nrelationship between the i-th artist in Aand the j-th tag in\nT. Our goal is to simultaneously generate a hierarchical\nclustering of Aand of Tbased on matrix X.3.2 HCC\nLike agglomerative hierarchical clustering algorithms,\nHCC starts with singleton clusters and then successively\nmerges the two nearest clusters until only one cluster is\nleft. However, unlike traditional algorithms, it may unify\nclasses from two different data types. This means that the\ncluster left at the end consists of all the rows and columns\nand so if there are mrows and ncolumns exist, HCC exe-\ncutes m+n\u00001rounds. The output of HCC is thus a single\ntree where the leaves are the rows and the columns of the\ninput matrix, where nodes having both rows and columns\nas descendants may appear at any non-leaf level. Note that,\nin Figure 1, at the third layer the artist A3 - Led Zeppelin\nis joined with the tag B2 - Classic rock.\nThe algorithm of HCC is presented in Algorithm 1. The\nAlgorithm 1 HCC\nAlgorithm Description\nCreate an\nempty hierarchy H\nList Objects in A +Objects in B\nN size[A] + size[B ]\nAddList toHas the bottom layer\nfori= 0toN\u00001do\np; q= PickUpTwoNodes( List)\no= Merge(p; q )\nRemove p; qfrom List and add otoList\nAddList toHas the next layer\nend for\ncentral part\nin the design of Algorithm 1 is the method\nPickUpTwoNodes, which is for selecting two nodes (cor-\nresponding to two clusters) to merge. For the purpose of\ncreating groups consisting of two different data types, we\nuse cluster heterogeneity measurement, denoted by CH.\nGiven a group Cconsisting of rrows, P, and scolumns,\nQ, we deﬁne CH(C)as\nCH(C) =1\nrs∑\ni\n∈P;j∈Q(xij\u0000\u0016)2; (1)\nwhere \u0016is the average of entries over rows Pand columns\nQ; i.e., \u0016=1\nrs∑\ni\n∈P;j∈Qxij. For a merger, we choose the\ntwo nodes whose merging would result in the least increase\nin the total cluster heterogeneity [10].\n3.3 Incorporating Instance-level Constraints\nIn practice, one may observe pairs of artists that should\nbe clustered into the same cluster. Similarly, one may ob-\nserve pairs of tags that must be always in the same tag\ncluster. These observations are represented as the afore-\nmentioned “must-link” and “cannot-link” constraints. We\ndesign HCC so as to incorporate such constraints.\nThere are two issues in incorporating these constraints.\nOne is how to use them for grouping data points of the\nsame type; i.e., how to use artist constraints for grouping\nartists and tag constraints for grouping tags. The other is\nhow to transfer constraints on one data type to the other\ndata type. To address the ﬁrst issue, we use Dunn’s Index\n250\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)$UWLVWV\n$\u0014 1LQH ,QFK 1DLOV7DJV\n%\u0014 $OWHUQDWLYH $\u0014 1LQH \u0003,QFK \u00031DLOV\n$\u0015 5DGLRKHDG\n$\u0016 /HG\u0003=HSSHOLQ\n$\u0017 3LQN\u0003)OR\\G\n$\u0018 4XHHQA3\nA4\nA5\nA6%\u0014 $OWHUQDWLYH\n%\u0015 &ODVVLF\u0003URFN\n%\u0016 ,QGXVWULDO\n%\u0017 5RFNB2\nB1 $\u0019 7KH\u0003%HDWOHVA2B1\nB4\nA3\nA4\nA6B2\nA3A1\nB2B3\nA3A6\nB2\nA2A3B2\nB1\nA1 A2 A3 A4 A5 A6 B2 B1 B3 B4Figure\n1. Part of HCC dendrogram. Rectangles represent artists and ellipses represent tags assigned to these artists. The\nnodes containing both rectangles and ellipses are clusters containing both an artist and a tag.\nto determine the best layer for cutting the HCC-generated\ndendrogram and then apply the constrained K-Means to\nincorporate the constraints of the same data type. To ad-\ndress the second issue, we use an alternating exchange al-\ngorithm.\n3.3.1 Best Layer\nSince HCC produces a list of clustering results and each\nclustering corresponds to one layer of the dendrogram, we\nuse Dunn’s Validity Index [9] to measure and compare\nthese clusterings. This validity measure is based on the\nidea that good clustering produces well-separated compact\nclusters. Given a clustering layer consisting of rclusters\nc1; : : : ; c r, Dunn’s Index is given by:\nD=min 1≤i<j≤rd(ci; cj)\nmax 1≤k≤rd′\nk; (2)\nwhere d(ci; cj))is\nthe inter-cluster distance between the i-\nth and the j-th clusters and d′\nkis the intra-cluster distance\nof the k-th cluster. Generally, the larger Dunn’s Index, the\nbetter the clustering.\nAfter determining the best layer to cut the dendrogram,\nwe can easily make use of the constraints of the same data\ntype. In particular, we perform constrained K-Means on\nthe best layer with the parameter Kset to the number of\nclusters in that layer. For this purpose, we use the MPCK-\nMeans algorithm in [3].\n3.3.2 Alternating Exchange\nHere we show how to transfer the constraints between dif-\nferent data types. Speciﬁcally, at the best layer of the den-\ndrogram generated by HCC, if some artist (or tag) data\npoints of certain node are being re-assigned to another\nnode at the same layer after using the instance-level con-\nstraints, we can use an alternating exchange algorithm [11]to improve tag (or artist) clustering. The objective function\nof clustering can be written as [11]:\nZ=r∑\nk=1m∑\nl=1∑\ni∈Ak∑\nj∈Tl(xij\u0000wkl)2; (3)\nwith\nwkl=1\naktl∑\ni∈A k∑\nj∈Tlxij: (4)\nHereris the\nnumber of type Aclusters, mis the number of\ntypeTclusters, Akis the k-th cluster contains data points\nof type A,Tlis the l-th cluster contains data points of type\nT,akandtlrespectively denote data points of type Aand\nT. As before, xijis the value representing the relationship\nbetween the i-th type-A data point and the j-th type- Tdata\npoint.\nTo transfer constraints from tags to artists, we do the\nfollowing: Suppose we have just obtained a clustering of\nartists, CA, and a clustering of tags, CT, by cutting the\nHCC dendrogram using Dunn’s index, as described be-\nfore. We ﬁrst incorporate into these clusterings the tag\nconstraints using the techniques described in Section 3.3.1\nthereby obtain an improved tag clustering, C′\nT. Then we\nexecute the greedy algorithm shown in Algorithm 2 to\nmake changes on artist class assignments. The greedy al-\ngorithm is aimed at minimizing the quantity Zin (3) and\nin each round one artist is moved from the current cluster\nto another if that move decreases the value of Z. Transfer-\nring constraints backward (i.e., from artists to tags) could\nbe done by simplying switch the role of tags and artists. In\nour implemenation, we transfer only from tags to artists.\n251\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Algorithm 2 Alternating\nExchange Algorithm\nInput: clusterings CAandC′\nT, and normalized A-T\nmatrix X, where C′\nTis obtained by using the\nMPCK-Means on the output of HCC with respect to\ntag constraints.\nwhile There is an artist whose relocation from the\ncurrent cluster to another decreases the value of Zdo\npick an artist-destination pair that maximizes the\ndecrease and relocate the artist to the destination\nend while\nOutput the resulting artist clustering C′\nA\n4. EXPERIMENT\n4.1\nData Set\nWe use the data set in [22] consisting of 403 artists. For\neach artist, tags and styles are collected from Last.fm\n(http://www.last.fm). There are 8,529 unique tags and 358\nunique style labels. Note that an artist may receive the\nsame tag more than once. By counting the number of as-\nsignments by the same tag, each artist is represented by a\n8,529-dimensional integer vector. We scale these tag vec-\ntors so that the total of the 8,529 entries is equal to a ﬁxed\nconstant. We will use Xto denote the artist-tag frequency\nmatrix thus generated.\nAs to the style labels, each artist belongs to at least one\nstyle and each style contains at least one artist. We generate\nan artist-style incident matrix from the data, so that the\nentry at coordinate (i; j)is1if the i-th artist has the j-th\nstyle label and 0otherwise.\n4.2 Hierarchies Generated from HCC\nWe use HCC to generate a dendrogram of the artists and\nthe tags. Figure 1 is part of the dendrogram generated by\nHCC in our experiment. In the dendrogram, each leaf rep-\nresents one artist or one tag, each internal node contains\nsubsets of artists and tags, and the top layer is the cluster\ncontains all artists and tags. Because many people assign a\ntag “Industrial” to artist Nine Inch Nails , “Industrial” and\nNine Inch Nails are clustered together. The novelty here is\nthat artists and tags are jointly organized into a hierarchical\nstructure. Once such a hierarchical organization has been\ngenerated, an artist can be described by the tags that appear\nin its cluster. The more representative are the tags for cer-\ntain artists, the larger possibility for them to be clustered\ntogether.\nWe compare the HCC-generated dendrogram with\none generated by single linkage hierarchical cluster-\ning (SLHC). This is the standard hierarchical clustering\nmethod and thus serves as our baseline. Since SLHC can\ncluster only one type of data, we provide SLHC with the\nnormalized artist-tag matrix by viewing each row as the\nfeature vector of the corresponding artist and produce hier-\narchical clustering of artists. The artist dendrogram gener-\nated by SLHC is shown in Figure 2. To evaluate and com-\npare these two artist dendrograms, we utilize CoPhenetic\n$UWLVW\n$\u0014 $,5\n$\u0015 1LQH ,QFK 1DLOV $\u0015 1LQH \u0003,QFK \u00031DLOV\n$\u0016 5DGLRKHDG\n$\u0017 0HWDOOLFD\n$\u0018 /HG\u0003=HSSHOLQ\n$\u0019 1LUYDQD\n$\u001a 3LQN\u0003)OR\\G\n$\u001b 6RXQGJDUGHQ\n$\u001c 0DVVLYH\u0003$WWDFN\n$\u0014\u0013 7KH\u0003%HDWOHV\nA1 A2 A3 A4 A5 A6 A7 A8 A9 A10Figure\n2. Part of the dendrogram generated by SLHC.\nCorrelation Coefﬁcient (CPCC) [17] as evaluation mea-\nsure. Intuitively CPCC measures how faithfully a dendro-\ngram preserves the pairwise distances between the original\ndata points. CoPhenetic Correlation Coefﬁcient (CPCC) is\ngiven as:\n∑\ni<j(d(i; j)\u0000d)(t(i; j )\u0000t)√\n(∑\ni<j(d(i; j)\u0000d\n)2)(∑\ni<j(t(i; j )\u0000t)2)(5)\nHere d(i; j )andt(i; j)are respectively the ordinary Eu-\nclidean distance and the dendrogrammatic distance be-\ntween the i-th and the j-th data points, and dandtare\ntheir respective averages. The CPCC for HCC was 3.71\nwhile that for SLHC was 3.69, and so we can say that our\nHCC method generates faithful dendrogram with reason-\nable clustering performance on artist-tag dataset. Through\nthe coupled dendrogram, one can observe the relationship\nbetween artists and tags, also make use of the tags within\nthe same cluster as some artists to explain why these artists\nare clustered together.\n4.3 Clustering Performance Comparisons\nWe also evaluate the artist clustering performance of HCC,\nby comparing it with three co-clustering algorithms includ-\ning Information-Theoretic Co-clustering (ITCC) [8], Eu-\nclidean Co-clustering (ECC), and Minimum Residue Co-\nclustering (MRC) [5] on the artist-tag dataset.\nUsing style labels we obtain artist clusters and cluster\nlabels. We ﬁrst cluster the styles using KMeans clustering\nbased on the artist-style matrix (that is, clustering of the\ncolumns, where each column is the 403-dimensional 0=1\nvector that shows assignments of the style corresponding\nto the column to the 403 artists). We then treat each cluster\nas a label and assign to each artist one label in the following\nmanner:\n\u000fIf all the styles assigned to an artist abelongs to a\nsingle cluster, we use that cluster as the label of a.\nOtherwise, choose the cluster with the largest num-\nber of styles assigned to a. If there is a tie, choose\nthe one with the larger total number of styles, and if\nthat doesn’t break the tie, break it arbitrarily.\n252\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)00.10.20.30.40.50.6\nAccuracyEuclidean \u0003co Ͳclustering\nInformation \u0003theoretic \u0003co Ͳ\nclustering\nMinimum \u0003squared \u0003residue \u0003co Ͳ\nclustering\nHCC\nHCC(Constraints)Figure\n3. Accuracy of various clustering methods.\nHCC(constraints) represents HCC with 10 artist con-\nstraints.\nWe use these labels as our ground truth class labels in the\nclustering performance measurements presented below.\n4.3.1 Evaluation Measures\nWe use Accuracy, Normalized Mutual information (NMI),\nPurity, and Adjusted Rand Index (ARI) as performance\nmeasures. These measures have been widely used in clus-\ntering evaluation and we hope they would provide insights\non the performance of our HCC method. For all these mea-\nsures, the higher the value, the better the clustering.\nSuppose we are given clusters C1; : : : ; C kof size\nc1; : : : ; c k, respectively and we are comparing this cluster-\ning against the ground-truth clustering E1; : : : ; E kof size\ne1; : : : ; e k. Letnbe the total number of data points and for\nalliandj, let\u0016ijdenote the number of data points in both\nCiandEj.\nAccuracy measures the extent to which each cluster\ncontains the entities from corresponding class and is given\nby:\nAccuracy = max\n\u0019∑\ni;\u0019(i)\u0016i\u0019(i)\nn; (6)\nwhere \u0019ranges all\npermutations of 1; : : : ; k .Purity mea-\nsures the extent to which a cluster contains entities of a\nsingle class and is given by:\nPurity =1\nnk∑\ni=1\u0016i\u001a(i); (7)\nwhere \u001a(i)is the jthat\nmaximizes \u0016ij.Adjusted Rand\nIndex is the corrected-for-chance version of Rand Index,\nand measures the similarity between two clusterings [16].\nIt is given by:\nARI =a\u00002bc\nn(n−1)\nb+c\n2\u00002bc\nn(n−1): (8)\nHere a=∑\ni;j\u0016ij(\u0016ij−1)\n2,b=∑\nici(ci−1)\n2, and c=\n∑\njej(ej−\n1)\n2.NMI is the\nnormalized version of mutual\ninformation and measures how much information the two\nclusterings share [18] and is given by:\nNMI =∑\ni;j\u0016ijlog(n\u0016ij\nciej)\n√\n(∑\nicilogci\nn)(∑\njejlogej\nn): (9)\n00.10.20.30.40.50.60.70.80.9\nPurityEuclidean \u0003co Ͳclustering\nInformation \u0003theoretic \u0003co Ͳ\nclustering\nMinimum \u0003squared \u0003residue \u0003co Ͳ\nclustering\nHCC\nHCC(Constraints)Figure\n4. Purity of various different clustering meth-\nods. HCC(constraints) represents HCC with 10 artist con-\nstraints.\n00.0050.010.0150.020.0250.030.0350.04\n0 1 2 3 4 5 6 7 8 9 1011121314151617181920 NMI \u0003(Artists \u0003Constraints)\nNMI \u0003(Artists \u0003Constraints)\nFigure\n5. NMI on HCC with artists constraints range from\n0 - 20.\n4.3.2 Experimental Results\nAs we mentioned in Section 3.3.1, Dunn’s Index can be\nused to ﬁnd the best layer of the dendrogram generated by\nHCC. After computing Dunn’s Index on the clustering of\neach layer, it is found that there are 11 clusters in the best\nlayer. Since we have already obtained the best layer, the\nclustering of this layer is compared against Co-clustering\nalgorithms. This clustering is based on artist data points,\nwe applied co-Clustering algorithms for clustering artists.\nFigures 3 and Figure 4 show the experiment results on\nthe clustering methods using accuracy and purity as the\nperformance measures, respectively. The results in both\nﬁgures demonstrate that our HCC method outperforms the\nco-clustering methods. Similar behaviors can be observed\nwhen using ARI and NMI measures. Due to space limita-\ntion, we do not include the ﬁgures for ARI or NMI. Fig-\nures 3 and 4 also show that using the artist constraints im-\nproves the clustering performance.\nWe also evaluate NMI on HCC with increasing num-\nber of constraints. The result in Figure 5 shows that the\nartist clustering performance improves with the increasing\nnumber of artist constraints. In other words, the artist con-\nstraints improves the clustering performance of HCC. Fig-\nure 6 shows that artist clustering performance improves as\nthe number of tag constraints increases.\n5. CONCLUSION\nIn this paper, we propose a novel clustering method, HCC,\nto hierarchically cluster artists and tags simultaneously.\nWith the dendrogram generated by HCC one can have a\npicture of all artists and tags, so as to ﬁnd the relationship\nbetween the artists and tags within the same cluster. Fur-\nthermore, we perform experiments on artist-tag dataset, the\nresults show that HCC outperforms its competitors, provid-\n253\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)\u000300.0050.010.0150.020.0250.030.0350.04\n0 1 2 3 4 5 6 7 8 9 1011121314151617181920NMI \u0003(Tags \u0003Constraints)\nNMI \u0003(Tags \u0003Constraints)Figure\n6. NMI on HCC with tags constraints range from 0\n- 20.\ning reasonable dendrograms with clusterings in each layer.\nIn the future, we would try out HCC on larger datasets to\nfurther conﬁrm its ability in MIR area.\n6. ACKNOWLEDGMENT\nThe work is partially supported by NSF grants IIS-\n0546280, CCF-0939179, and CCF-0958490 and an NIH\nGrant 1-RC2-HG005668-01.\n7. REFERENCES\n[1] S. Basu, I. Davdison, K. Wagstaff (eds): “Constrained\nClustering: Advances in Algorithms, Theory, and Ap-\nplications,” Chapman & Hall/CRC Data Mining and\nKnowledge Discovery Series, 2008.\n[2] P. Berkhin: “A survey of clustering data mining tech-\nniques,” Grouping Multidimensional Data, pp. 25–71,\nSpringer, Heidelberg, 2006.\n[3] M. Bilenko, S. Basu, and R. J. Mooney: “Integrat-\ning constraints and metric learning in semi-supervised\nclustering,” in Proc. 21st International Conference on\nMachine Learning, pp. 81–88, 2004.\n[4] K. Bosteels, E. Pampalk, and E. E. Kerre: “Music re-\ntrieval based on social tags: A case study,” in Proc. 9th\nInternational Conference on Music Information Re-\ntrieval, 2008.\n[5] H. Cho, I. S. Dhillon, Y. Guan, and S. Sra: “Minimum\nsum-squared residue co-clustering of gene expression\ndata,” in Proc. 4th SIAM International Conference on\nData Mining, pp. 114–125, 2004.\n[6] D. R. Cutting, D. R. Karger, J. O. Pedersen, and J. W.\nTukey: “Scatter/gather: a cluster-based approach to\nbrowsing large document collections,” in Proc. 15th\nACM SIGIR Conference , pp. 318–329, 1992.\n[7] I. S. Dhillon: “Co-clustering documents and words us-\ning bipartite spectral graph partitioning,” in Proc. 7th\nACM SIGKDD International Conference on Knowl-\nedge Discovery and Data Mining, pp. 269–274, 2001.\n[8] I. S. Dhillon, S. Mallela, and D. S. Modha:\n“Information-theoretic co-clustering,” in Proc. 9th\nACM SIGKDD International Conference on Knowl-\nedge Discovery and Data Mining, pp. 89–98, 2003.[9] J. C. Dunn: “A fuzzy relative of the ISODATA process\nand its use in detecting compact well-separated clus-\nters,” J. Cybernetics, 3(3):32–57, 1974.\n[10] T. Eckes and P. Orlik: “An error variance approach\nto two-mode hierarchical clustering,” J. Classiﬁcation,\n10(1):51–74, 1993.\n[11] W. Gaul and M. Schader: “A new algorithm for two-\nmode clustering,” in Data analysis and information\nsystems, pp. 15–23, Springer, Heidelberg, 1996.\n[12] M. Hosseini and H. Abolhassani: “Hierarchical co-\nclustering for web queries and selected urls,”. in\nProc. Web Information Systems Engineering (WISE),\npp. 653-662, 2007.\n[13] D. Ienco, R.G. Pensa, and R. Meo: “Parameter-\nFree Hierarchical Co-clustering by n-Ary Splits,”. in\nProc. Machine Learning and Knowledge Discovery in\nDatabases (ECML/PKDD), pp. 580–595, 2009.\n[14] J. Li and T. Li: “HCC: A Hierarchical Co-Clustering\nAlgorithm,” in Proc. 33rd ACM SIGIR Conference ,\n2010.\n[15] B. Long, X. Wu, Z. M. Zhang, and P. S. Yu: “Unsu-\npervised learning on k-partite graphs,” in Proc. 12th\nACM SIGKDD Conference on Knowledge Discovery\nand Data Mining, pp. 317–326, 2006.\n[16] G. W. Milligan and M. C. Cooper: “A study of\nthe comparability of external criteria for hierarchical\ncluster analysis,” Multivariate Behavioral Research ,\n21(4):441–458, 1986.\n[17] R. R. Sokal and F. J. Rohlf: “The comparison of\ndendrograms by objective methods,” Taxon , 11:33–40,\n1962.\n[18] A. Strehl and J. Ghosh: “Cluster ensembles - a knowl-\nedge reuse framework for combining multiple par-\ntitions,” J. Machine Learning Research, 3:583–617,\n2003.\n[19] P. Symeonidis, M. M. Ruxanda, A. Nanopoulos, and\nY. Manolopoulos: “Ternary semantic analysis of so-\ncial tags for personalized music recommendation,” in\nProc. 9th International Conference on Music Informa-\ntion Retrieval , pp. 219–224, 2008.\n[20] P. Tan, M. Steinbach, and V. Kumar: “Introduction to\nData Mining,” Addison-Wesley, 2005.\n[21] D. Turnbull, L. Barrington, and G. Lanckriet: “Five\napproaches to collecting tags for music,” in Proc. 9th\nInternational Conference on Music Information Re-\ntrieval, pp. 225–230, 2008.\n[22] F. Wang, X. Wang, B. Shao, T. Li, and M. Ogihara:\n“Tag integrated multi-label music style classiﬁcation\nwith hypergraphs,” in Proc. 10th International Society\nfor Music Information Retrieval , pp. 363–368, 2009.\n254\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "A Cartesian Ensemble of Feature Subspace Classifiers for Music Categorization.",
        "author": [
            "Thomas Lidy",
            "Rudolf Mayer",
            "Andreas Rauber",
            "Pedro J. Ponce de León",
            "Antonio Pertusa",
            "José Manuel Iñesta Quereda"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1415598",
        "url": "https://doi.org/10.5281/zenodo.1415598",
        "ee": "https://zenodo.org/records/1415598/files/LidyMRLPQ10.pdf",
        "abstract": "We present a cartesian ensemble classification system that is based on the principle of late fusion and feature sub- spaces. These feature subspaces describe different aspects of the same data set. The framework is built on the Weka machine learning toolkit and able to combine arbitrary fea- ture sets and learning schemes. In our scenario, we use it for the ensemble classification of multiple feature sets from the audio and symbolic domains. We present an extensive set of experiments in the context of music genre classifi- cation, based on numerous Music IR benchmark datasets, and evaluate a set of combination/voting rules. The results show that the approach is superior to the best choice of a single algorithm on a single feature set. Moreover, it also releases the user from making this choice explicitly.",
        "zenodo_id": 1415598,
        "dblp_key": "conf/ismir/LidyMRLPQ10",
        "keywords": [
            "ensemble classification",
            "late fusion",
            "feature sub-spaces",
            "Weka machine learning toolkit",
            "arbitrary feature sets",
            "learning schemes",
            "music genre classification",
            "Music IR benchmark datasets",
            "combination/voting rules",
            "superiority"
        ],
        "content": "A CARTESIAN ENSEMBLE OF FEATURE SUBSPACE CLASSIFIERS\nFOR MUSIC CATEGORIZATION\nT. Lidy, R. Mayer, A. Rauber\nVienna University of Technology, Austria\nDepartment of Software Technology\nand Interactive SystemsP. J. Ponce de Le ´on, A. Pertusa, J. M. I ˜nesta\nUniversity of Alicante, Spain\nDepartamento de Lenguajes y\nSistemas Inform ´aticos\nABSTRACT\nWe present a cartesian ensemble classiﬁcation system that\nis based on the principle of late fusion and feature sub-\nspaces. These feature subspaces describe different aspects\nof the same data set. The framework is built on the Weka\nmachine learning toolkit and able to combine arbitrary fea-\nture sets and learning schemes. In our scenario, we use it\nfor the ensemble classiﬁcation of multiple feature sets from\nthe audio and symbolic domains. We present an extensive\nset of experiments in the context of music genre classiﬁ-\ncation, based on numerous Music IR benchmark datasets,\nand evaluate a set of combination/voting rules. The results\nshow that the approach is superior to the best choice of a\nsingle algorithm on a single feature set. Moreover, it also\nreleases the user from making this choice explicitly.\n1. INTRODUCTION AND RELATED WORK\nClassiﬁcation of music into different categories is an im-\nportant task for retrieval and organization of music libraries.\nPrevious studies reported about a glass ceiling reached us-\ning timbral audio features for music classiﬁcation [1]. Our\napproach is based on the assumption that a diversity of mu-\nsic descriptors and a diversity of machine learning algo-\nrithms are able to make further improvements. We created\nan ensemble learning system with these two dimensions\n(feature sets, learning schemes) as input and train models\nfor each combination of those two input dimensions. We\ncall our approach a cartesian ensemble system.\nOur original motivation has been to combine multiple\napproaches from the music information retrieval (MIR) do-\nmain in order to improve (the reliability of) genre classi-\nﬁcation results based on the assumption that the various\nmusic descriptors are complementary [12]. In our previ-\nous work we combined spectrum-based audio features that\ncover timbral and rhythmic aspects of the sound with sym-\nbolic descriptors, based on note and chord sequence statis-\ntics. A polyphonic transcription system has been presented\nas the “missing link” that transcribes audio data into a sym-\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2010 International Society for Music Information Retrieval.bolic notation. In this approach the combination of mul-\ntiple features from the audio and symbolic domains was\nperformed by a concatenation of feature vectors, jointly\nused as input to a classiﬁcation algorithm (early fusion) .\nIn a previous comparison of employing MIR algorithms on\nWestern vs. ethnic music [10] we included a time decom-\nposition approach, which was already a ﬁrst ensemble-like\napproach, applying one learning scheme on multiple input\nfeatures from different segments of a piece of music and\nusing four different combination (voting) rules to make the\nﬁnal prediction.\nThe Autonomous Classiﬁcation Engine ACE [13], by\ncontrast, is a general framework for model selection. In\nmachine learning, model selection is the task of selecting\none classiﬁcation model from a pool of models. ACE trains\na range of classiﬁers, with different parameters, and feature\nselection methods, and then selects the most ﬁtting ones for\nthe current task at hand. ACE is built on top of Weka [20]\nand thus provides the ensemble techniques implemented\nin the toolkit, most prominently boosting and bagging, but\nis not capable of handling feature subspaces, or weighted\nmethods as the ones described in Section 3.\nThe combination of different segments extracted from\nthe same song is studied in [2]. The approach is based on\ngrouping and aggregating non-overlapping blocks of con-\nsecutive frames into segments. The segments are then clas-\nsiﬁed individually and the results are aggregated for a song\nby majority voting. Three different ensemble methods and\ntheir applicability to music are investigated in [7]. The ﬁrst\nmethod is based on a one against all scheme, i.e. for each\nclass, a classiﬁer is trained on the class and its complement.\nA second method is based on building a classiﬁer for each\npairwise combination of classes. The third method investi-\ngates in training different classiﬁers on different subsets of\nthe feature space. In all methods, the ﬁnal class label is de-\ntermined by the probabilities of the individual classiﬁers.\nThe approach presented in this paper is a cartesian en-\nsemble classiﬁcation system, which trains a matrix of mod-\nels built from the combination of a range of individual fea-\nture sets and a number of classiﬁcation algorithms. Our\nsystem builds on the Weka machine learning toolkit [20] in\nan open and ﬂexible way. In contrast to ACE no preselec-\ntion of classiﬁcation algorithms has been made – any clas-\nsiﬁcation algorithm available can be used with arbitrary pa-\nrameters in the ensemble. Further, an arbitrary number of\nfeature ﬁles can be used. We provide a number of com-\n279\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Audio ﬁleTranscriptionMIDI ﬁleChord extraction Chord sequenceAudiodescriptorsSymbolicdescriptorsClassification schemes\nDecision combinationCategorylabel. . .. . .Figure 1. Framework of the cartesian ensemble system\nbination and voting rules, which are employed to obtain\nthe ﬁnal prediction of the classiﬁer ensemble. Our frame-\nwork is not limited to MIR applications. With regard to our\noriginal motivation and our research background, however,\nwe focus on the scenario of music classiﬁcation into genre\ncategories, in order to show the applicability of the system\nand the progress in our domain.\nThe overall scheme of our proposed ensemble classiﬁ-\ncation system is shown in Figure 1. It includes our sce-\nnario of a music classiﬁcation system that processes dif-\nferent descriptors from the audio and symbolic domains\n(c.f. Section 2). Audio feature extraction algorithms are\napplied directly to the audio signal data. There is an inter-\nmediate step for the symbolic descriptors: A polyphonic\ntranscription system converts the audio information into a\nsymbolic notation (i.e. MIDI ﬁles). A chord inference al-\ngorithm is applied to provide information about the poly-\nphonic structure of the note stream. Finally, a symbolic\nfeature extractor is applied on the resulting representation.\nThe feature extraction stage provides multiple viewpoints\non music objects, called feature subspaces. There are sev-\neral ways of combining them for building a music classi-\nﬁcation system. Early fusion concatenates all feature sub-\nspaces to produce so called superinstances, including all\nfeatures at hand. Then a suitable classiﬁcation scheme is\nused to learn categories from such data. This approach was\nused in our previous work [12]. Late fusion combines clas-\nsiﬁer outcomes rather than features. This is the approach\nemployed in our proposed framework.\nSection 3 describes the general architecture of our en-\nsemble framework. In Section 4, we evaluate our approach\non numerous well-known reference music datasets and show\nthe applicability of the approach. It includes also prelimi-\nnary research on the use of audio segmentation for generat-\ning extended feature subspaces. Finally, Section 5 provides\nconclusions and an outlook on future work.\n2. MUSIC DESCRIPTION\nWe use two sources of input to our ensemble music classiﬁ-\ncation approach: audio features extracted from audio ﬁles\nand symbolic music descriptors derived from MIDI ﬁles\nthat are generated from audio ﬁles through a transcriptionsystem. We employ features that proved well in our previ-\nous works [5, 10–12], also in order to be able to compare\nprogress and results of the new ensemble approach with\nprevious ﬁndings. We emphasize, however, that arbitrary\nfeature sets can be used with our classiﬁer ensemble ap-\nproach presented in Section 3.\n2.1 Audio Features\nAll the following descriptors are extracted from a spectral\nrepresentation of an audio signal, partitioned into segments\nof 6 sec. Features are extracted segment-wise, and then ag-\ngregated for a piece of music computing the median (RP,\nRH) or mean (SSD, MVD) from features of multiple seg-\nments. We describe the feature extraction algorithms very\nbrieﬂy, please refer to the references for further details.\nRhythm Pattern (RP) The feature extraction process for\na Rhythm Pattern is composed of two stages. First, the\nspeciﬁc loudness sensation on 24 critical frequency bands\nis computed through a Short Time FFT, grouping the re-\nsulting frequency bands to the Bark scale, and successive\ntransformation into the Decibel, Phon and Sone scales. This\nresults in a psycho-acoustically modiﬁed Sonogram repre-\nsentation that reﬂects human loudness sensation. In the\nsecond step, a discrete Fourier transform is applied to this\nSonogram, resulting in a spectrum of loudness amplitude\nmodulation per modulation frequency for each critical band.\nAfter additional weighting and smoothing steps, a Rhythm\nPattern exhibits magnitude of modulation for 60 modula-\ntion frequencies on the 24 critical bands [11].\nRhythm Histogram (RH) A Rhythm Histogram (RH)\naggregates the modulation amplitude values of the critical\nbands computed in a Rhythm Pattern and is a descriptor for\ngeneral rhythmic characteristics in a piece of audio [11].\nStatistical Spectrum Descriptor (SSD) The ﬁrst part of\nthe algorithm, the computation of speciﬁc loudness sen-\nsation, is equal to the Rhythm Pattern algorithm. Subse-\nquently at set of statistical values1are calculated for each\nindividual critical band. SSDs describe ﬂuctuations on the\ncritical bands and capture both timbral and rhythmic infor-\nmation very well [11].\nModulation Frequency Variance Descriptor (MVD) This\ndescriptor measures variations in the critical bands for a\nspeciﬁc modulation frequency of the Rhythm Pattern ma-\ntrix, representing the amplitudes of 60 modulation frequen-\ncies on 24 critical bands. The MVD vector is computed by\ntaking statistics1for each modulation frequency over the\n24 bands [10, 12].\nTemporal Features (TRH, TSSD) Feature sets are fre-\nquently computed on a per segment basis and do not incor-\nporate time series aspects. We introduced therefore TRH\nand TSSD features that include a temporal dimension de-\nscribing variations over time.\nFor TRH, statistical measures1are computed over the\nindividual Rhythm Histograms extracted from the individ-\nual 6-second segments in a piece of audio. Thus, change\nand variation of rhythmic aspects in time are captured.\n1mean, median, variance, skewness, kurtosis, min and max\n280\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)TSSD analogously capture timbral variations and changes\nover time in the spectrum on the critical frequency bands.\nHence, a change of rhythmics, instruments, voices, etc.\nover time is reﬂected by this feature set [10].\n2.2 Transcription from Audio to MIDI\nA multiple fundamental frequency (f 0) estimation method\nis used to convert the audio ﬁles to MIDI ﬁles. This is a\njoint estimation approach, which experimentally obtained\na high accuracy with a low computational cost. It extends\na previous work [16] by adding information about neigh-\nboring frames to get a smooth temporal estimation.It does\nnot separate instruments, therefore producing single track\nMIDI ﬁles without any timbral information.\n2.3 Symbolic Features\nA set of statistical descriptors is extracted directly from\ntranscribed notes. This set is based on the features de-\nscribed in [5], well suited for monophonic classical/jazz\nclassiﬁcation, and on features described in [17], used for\nmelody track selection in MIDI ﬁles. Overall statistics,\nsuch as the average number of notes per beat, the occupa-\ntion rate (non-silence periods with respect to song length)\nand polyphony rate (proportion of sounding note periods\nwith more than one note active simultaneously) are com-\nputed. Further, note pitches, pitch intervals, note durations,\nsilence durations, Inter Onset Intervals (IOI) and non-diatonic\nnotes are analyzed; each property is described by min and\nmax values, range, average, standard deviation, and a nor-\nmality distribution estimator. Other features include the\nnumber of distinct pitch intervals, pitch interval mode, and\nan estimation of the number of syncopations in the song.\nMost of these features are somewhat ’melody-oriented’\n(e.g., interval-based features). In order to capture rele-\nvant information about the polyphonic structure of the tran-\nscription, a chord sequence is extracted from it, using the\nalgorithm from Pardo and Birmingham [14], and subse-\nquently analyzed. The different kinds of chord extracted\nare: major triad, major 7th, dominant 7th, dominant sus-\npended 7th, dominant 7th (sharp 5th), dominant 7th (ﬂat\n5th), minor 7th, half diminished and fully diminished chords.\nThe relative frequencies of these chords in a chord sequence\nare computed as symbolic features. A total of 61 statisti-\ncal descriptors are therefore provided to the system as a\nsymbolic feature subspace.\n3. CARTESIAN ENSEMBLE SYSTEM\nOur approach is name a cartesian ensemble because the\nset of models used as base classiﬁers is the cartesian prod-\nuct ofDfeature subspaces by Cclassiﬁcation schemes. A\nmodel is build by training classiﬁcation scheme cion fea-\nture subspace dj. This produces a total of D\u0002Cbase mod-\nels as the ensemble. The aim of this approach is to obtain\na sufﬁciently diverse ensemble of models that will guaran-\ntee, up to a certain degree, an improvement of the ensemble\naccuracy over the best single model trained. Moreover, theensemble abstracts from the selection of a particular classi-\nﬁer and feature set to use for a particular problem. Select-\ning sufﬁciently different schemes (different classiﬁcation\nparadigms, methods,...) the ensemble provide results that\nare at least comparable to the best single scheme.\nModel diversity is a key design factor for building ef-\nfective classiﬁer ensembles [9]. This has been empirically\nshown to improve the accuracy of an ensemble over its\nbase models when they are numerous enough. For se-\nlecting the most diverse models within the ensemble the\nPareto-optimal selection strategy is applied in order to dis-\ncard models not diverse or not accurate enough.\nWhen a new music instance is presented to the trained\nensemble, predictions are made by selected models, which\nare then combined to produce a single category prediction\noutcome. A number of decision combination (or label fu-\nsion) rules, can be used for this ﬁnal prediction.\nThe cartesian ensemble system is built on the Weka\ntoolkit [20]. The ensemble is a Weka classiﬁer itself, so\nit can be plugged into any system using this toolkit.\n3.1 Pareto-optimal Classiﬁer Selection\nThis strategy for selecting the best set of models is based\non ﬁnding the Pareto-optimal set of models by rating them\nin pairs, according to two measures [9]. The ﬁrst one is the\ninter-rater agreement diversity measure \u0014, deﬁned on the\ncoincidence matrix Mof the two models. The entry mr;s\nis the proportion of the dataset, which model hilabels as\nLrand modelhjlabels asLs. The agreement between\nboth classiﬁers is given by\n\u0014ij=P\nkmkk\u0000ABC\n1\u0000ABC(1)\nwhere ABC is agreement-by-chance\nABC =X\nr(X\nsmr;s)(X\nsms;r) (2)\nThe second one is the pair average error, computed by\neij= 1\u0000\u000bi+\u000bj\n2(3)\nwhere\u000biand\u000bjare the estimated accuracy of the two\nmodels, computed as described in Section 3.3. The Pareto-\noptimal set contains all non-dominated pairs. A pair of\nclassiﬁers is non-dominated iff there is no other pair that is\nbetter than it on both criteria.\n3.2 Combination Rules\nThe combination rules implemented in the system are both\nweighted and unweighted majority voting rules. A sum-\nmary of weighted and unweighted combination rules is\npresented in Table 1, where P(Lkjxi)is the posterior prob-\nability of instance xto belong to category Lk, given by\nmodelhi.xiis whathiknows about x, i. e., feature val-\nues that correspond to the feature subspace hiwas trained\non. Unweighted combination rules are described in [8],\nand used through their implementation in Weka.\n281\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)ekekekakakak\n0 N(1−1/M)1 1 1\neWeBeWeB0Figure 2. Model weight computation: RSWV (left),\nBWWV (center), QBWWV (right), giving the model au-\nthorityakas a function of the estimated number of errors\nekmade by model hkon a validation set. Nis the number\nof instances in the set, Mis the number of class labels.\nAll weighted rules multiply model decisions by weights\nand select the label Lkthat gets the maximum score. Model\nweights are based on the estimated accuracy \u000biof the trained\nmodels. The authorityaiof each model hiis established\nas a function of \u000bi, normalized, and used as its weight !i.\nWeighted methods discussed in [6] have been used in this\nwork. SVW computes weights as described. Weight func-\ntions for rules RSWV , BWWV and QBWWV are shown\nin Figure 2. There, eBis the lowest estimated number of\nerrors made by any model in the ensemble on a given vali-\ndation dataset, and eWis the highest estimated number of\nerrors made by any of those classiﬁers. WMV is a theoret-\nically optimal weighted vote rule described in [9], where\nmodel weights are set proportionally to log(\u000bi=(1\u0000\u000bi)).\nTable 1. Summary of combination rules.\nRule mnemonic Description\nUnweighted rules\nMAJ Majority vote rule\nA VG Average of P(Lkjxi)\nMAX Maximum of P(Lkjxi)\nMED Median of P(Lkjxi)\nWeighted rules\nSWV Simple Weighted V ote\nRSWV Rescaled Simple Weighted V ote\nBWWV Best-Worst Weighted V ote\nQBWWV Quadratic Best-Worst Weighted V ote\nWMV Weighted Majority V ote\n3.3 Inner/Outer Cross Validation\nThe classiﬁcation results presented below are estimated by\ncross-validating the ensemble. The accuracy of individual\nensemble models (\u000b i), used to compute model weights for\ncombining their outputs, is also estimated through cross-\nvalidation. In order to avoid using test data for the ensem-\nble for single model accuracy estimation, an inner cross-\nvalidation, relying only on ensemble training data, is per-\nformed. The number of folds for the ensemble (outer) and\nthe single models (inner) cross-validation are parameters.\nOuter testOuter  trainInner trainInner test\nFigure 3. Inner and outer cross-validation scheme.4. EV ALUATION\nWe performed an extensive evaluation of our ensemble ap-\nproach on a range of well-known MIR benchmark datasets\nin order to show both the feasibility and generality of our\napproach. Classiﬁcation results are presented as accuracy\nvalues with standard deviations.\n4.1 Datasets\nA dataset overview is given in Table 2. Either full songs or\n30 second excerpts were available. 9GDB is originally a\nMIDI collection, but was synthesized to wav for our exper-\niments and re-transcribed to MIDI to obtain symbolic fea-\ntures. For all other collections audio ﬁles were transcribed\nto MIDI. The GTZAN collection was assembled and used\nin experiments by G. Tzanetakis [19]. The ISMIRgenre\nand ISMIRryhthm collections were compiled for the genre\nand rhythm classiﬁcation tasks, respectively, of the ISMIR\n2004 Audio Description contest [3] and used frequently\nthereafter by Music IR researchers. ISMIRgenre consists\nof 6 popular music genres and ISMIRryhthm comprises\n8 Latin and Ballroom dances. The Latin Music Database\ncomprises 10 Latin music genres [18]. The African col-\nlection is a sub-set of 1024 instances of the audio archive\nof the Royal Museum of Central-Africain Belgium, digi-\ntized in the course of the DEKKMMA project [4]. Various\nmeta-data categories are available for this set, including 27\ndifferent functions, 11 different instrument families, 11 dif-\nferent countries and 40 ethnic groups [10]. The number of\nﬁles varies according to number of meta-data available in\neach category.\nTable 2. Datasets used in experiments\ndataset ﬁles genres ﬁle length ref.\n9GDB 856 9full [15]\nGTZAN 1000 10 30 sec [19]\nISMIRgenre 1458 6full [3]\nISMIRrhythm 698 830 sec [3]\nLatinMusic 3225 10 full [18]\nAfrica 1024 var. full [4]\n4.2 Classiﬁcation Schemes and System Parameters\nFor our experiments, we set the system to perform 10-fold\nouter cross-validation and 3-fold inner cross-validation. As\nfor the classiﬁcation schemes, a selection of classiﬁers from\ntheWeka toolkit has been made, aiming at choosing schemes\nfrom different machine learning paradigms. We chose Na ¨ıve\nBayes, Nearest Neighbor (IB12) with Euclidean distance,\n3-NN with Manhattan distance (IBk), the RIPPER rule learner\n(JRip), the C4.5 (J48) decision tree learner, the REPTree,\na fast decision tree learner, Random Forest, a forest of ran-\ndom trees, and three Support Vector Machines, the ﬁrst\nwith a linear kernel, the second with a quadratic one and\nthe third with the Puk kernel, a Pearson VII function-based\nuniversal kernel with parameter values C= 4,!= 3:2,\n\u001b= 13 . Please consult [20] for further reference on these\nmethods.\n2Weka names for these classiﬁers in parenthesis.\n282\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)4.3 Ensemble Classiﬁcation Results\nTable 3. Best results on individual classiﬁcation of feature\nsets and classiﬁers on different datasets\nDataset Classiﬁer Featureset Accuracy\n9GDB SVM-Puk TSSD 78.15\nGTZAN SVM-lin SSD 72.60\nISMIRgenre SVM-quad TSSD 81.28\nISMIRrhythm SVM-lin RP 87.97\nLatinMusic SVM-Puk TSSD 89.46\nAfrica/country SMO-quad SSD 86.29\nAfrica/ethnic group SVM-lin TSSD 81.10\nAfrica/function 1-NN SSD 51.06\nAfrica/instrument SVM-Puk TSSD 69.90\nTo have a baseline for the cartesian ensemble, we trained\nall the classiﬁcation schemes described in Section 4.2 on\nall the feature sets described in Section 2, i.e. one model\nfor each cell in the cartesian set D\u0002C. Table 3 gives an\nextract of the accuracies achieved with these single models\n– due to space limitation, only the best combination of an\nalgorithm and a feature set are given. It can be observed\nthat there is no clear trend, neither for a classiﬁer, nor a\nfeature set. While SVMs clearly dominate the results, the\nchoice of the kernel is not obvious, and results can vary by\nseveral percent points. Also the feature sets do not show a\nclear trend – in approximately half of the cases, TSSDs are\nthe best set to use, while also SSD and RP features some-\ntimes yield clearly better results. These results nourish the\nhypothesis that ensemble classiﬁers may provide means to\nrelease the user from the difﬁcult choice of the proper fea-\nture set and classiﬁer combination.\nThe accuracy results for the classiﬁer ensembles are shown\nin Table 4, with the best single classiﬁer as our assumed\nbaseline to improve on. Note that achieving the baseline\nresult would require to know the best combination of fea-\nture set and classiﬁer in advance. On each of the datasets,\nwe can observe higher classiﬁcation accuracies with the\nensembles than with the baseline. The improvements are\nthree percent points on average. The highest gains are on\nthe GTZAN dataset, with ﬁve percent points, while the im-\nprovements on the ISMIRrhythm dataset are of 1.14 per-\ncent point. However, the baseline on this dataset is already\nvery high, at approx. 88%.\nOut of the nine classiﬁcation tasks, the QBWWV rule\nwas ﬁve times the best, followed by WMV which is three\ntimes the best performing rule. A VG and BWWV are both\nonce the highest ranked combination rule. In the tasks\nwhere QBWWV is not the rule with the highest accuracy,\nthe relative difference to the top rule is minimal – the largest\nmargin is 0.7 percent points, or 0.86% relative difference.\n4.4 Segmentation Ensemble Approach\nA logical next step for ensemble classiﬁcation is the use of\nindividual features from different segments of an audio ﬁle\nas an input to classiﬁcation. We conducted an experiment\nsegmenting each audio ﬁle into 3 equal-sized segments,\nand extracting individual features from each of those seg-\nments. Note that for audio collections with 30 second ex-cerpts we did not do this for TSSD and TRH features, as\nthere would be no temporal variation within a segment,\ngiven the feature algorithm’s segment-window-length of 6\nseconds (c.f. Sec. 2.1). In those cases we used TSSD and\nTRH features from the full song, as in the previous exper-\niments. Also the symbolic features were used from full\nsongs. Our hypothesis was that with more (detailed) infor-\nmation about the audio content, results would be improved\nin the ensemble setting. However, results of this segmen-\ntation approach were in general inferior compared to us-\ning features aggregated over entire songs, as seen from the\nbottom two lines of Table 4. As the performance decrease\nwas independent of the combination rule applied, we in-\ncluded only the results of the two best combination rules\n(QBWWV and WMV) for space reasons.\nEven though the results of this ﬁrst experiment did not\nimprove the ensemble approach, we will further pursue this\nstrategy and reﬁne it in multiple ways: First, we will extend\nthe segmentation also to symbolic features. Then we will\nconduct research on different classiﬁer model combination\nstrategies. Instead of a combination of all classiﬁer/feature\nset models into one ensemble, a two-tier approach is envis-\naged, where a decision is made by an ensemble of features\nfrom different segments ﬁrst and then the decisions of mul-\ntiple different feature sets and classiﬁers are combined on\na second level. Further future work will be the experimen-\ntation with different degrees of segmentation of an audio\nﬁle. Moreover, instead of using equally sized segments, a\nstructural audio segmentation algorithm for segmentation\ninto chorus, verse etc. could be used for semantic segmen-\ntation, aiming at an enhanced diversity of the features and\nthe knowledge of the content.\n5. CONCLUSIONS\nIn this paper, we presented a framework for automatic clas-\nsiﬁcation of music data. Our system builds ensembles of\nclassiﬁers in two ways – ﬁrst, several different algorithms\n(and parameter variations) are used, and secondly, a set of\ndifferent features, describing different aspects of the same\ndataset. We have demonstrated the power of this approach\non the classiﬁcation task for six different datasets and achieved\nimprovements on the classiﬁcation accuracies in each sin-\ngle task. When comparing the results of the ensemble to\nthe single feature sets, we could observe that there is no\nclear trend on which classiﬁcation algorithm, and which\nfeature set to use for a speciﬁc dataset. The advantage\nof the ensemble approach is that the user is released from\nthis task. The ensemble approach delivers superior results\nthrough adding a reasonable amount of feature sets and\nclassiﬁers. Even though we did not discover a combina-\ntion rule that always outperforms all the others, relying on\nthe QBWWV rule seems feasible.\nFuture work will include an even wider set of exper-\niments on more datasets, also involving other modalities\nsuch as song lyrics. Another area is the above mentioned\nensemble of different segments from the same song.\n283\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Table 4. Results of the ensemble classiﬁcation on different datasets (Standard deviations are given in parentheses). The\nlower section of the table shows the results of the segmentation approach.\nISMIR ISMIR Latin Africa Africa Africa Africa\nRule 9GDB GTZAN genre rhythm Music country ethnic group function instrument\nSingle best 78.15 (2.25) 72.60 (3.92) 81.28 (3.13) 87.97 (4.28) 89.46 (1.62) 86.29 (2.30) 81.10 (2.41) 51.06 (6.63) 69.90 (4.69)\nMAJ 79.56 (4.78) 72.60 (3.31) 77.78 (2.15) 88.25 (5.08) 89.33 (1.55) 85.31 (4.04) 71.86 (3.41) 37.37 (7.36) 59.63 (5.79)\nMAX 60.05 (6.67) 44.00 (6.60) 60.97 (6.71) 54.87 (8.95) 50.64 (2.06) 77.67 (9.16) 73.16 (6.40) 40.38 (7.10) 61.32 (5.88)\nMED 74.30 (4.32) 55.90 (3.84) 72.02 (2.74) 77.79 (4.27) 73.64 (2.37) 83.84 (3.77) 70.71 (3.62) 39.49 (5.22) 60.34 (4.67)\nA VG 81.66 (3.96) 68.40 (2.37) 79.70 (3.35) 86.82 (4.29) 86.85 (1.96) 87.66 (2.28) 78.21 (3.50) 53.73 (5.35) 70.60 (3.82)\nSWV 81.31 (3.32) 77.10 (3.98) 78.33 (2.48) 88.97 (5.39) 92.00 (1.34) 86.97 (2.98) 75.47 (3.62) 46.83 (4.44) 67.09 (3.99)\nRSWV 80.96 (3.26) 77.40 (4.22) 79.22 (2.38) 88.97 (4.94) 92.25 (1.16) 87.17 (2.77) 75.47 (3.62) 48.39 (5.63) 68.35 (4.22)\nBWWV 81.54 (3.17) 77.40 (4.22) 82.03 (1.83) 89.11 (4.62) 92.25 (1.16) 88.34 (2.22) 79.37 (3.95) 52.61 (5.76) 72.71 (3.47)\nQBWWV 80.96 (2.94) 77.50 (4.30) 84.02 (1.50) 88.97 (3.86) 92.71 (0.99) 89.03 (1.63) 82.68 (3.18) 54.84 (6.29) 72.86 (3.52)\nWMV 80.84 (2.90) 76.10 (4.20) 84.02 (2.02) 87.97 (3.92) 92.59 (1.29) 88.93 (1.76) 82.97 (3.30) 51.28 (6.93) 73.00 (4.25)\nQBWWV 81.31 (2.78) 76.80 (3.33) 76.95 (3.28) 88.25 (4.39) 91.66 (1.17) 88.44 (2.75) 78.35 (4.08) 50.95 (6.62) 71.03 (3.99)\nWMV 80.49 (2.40) 74.50 (4.53) 81.48 (3.01) 87.68 (3.74) 91.56 (1.29) 88.05 (2.12) 80.23 (3.35) 44.83 (4.54) 72.29 (4.45)\n6. REFERENCES\n[1] J.-J. Aucouturier and F. Pachet. Improving timbre sim-\nilarity: How high is the sky? Journal of Negative Re-\nsults in Speech and Audio Sciences, 1(1), 2004.\n[2] J. Bergstra, N. Casagrande, D. Erhan, D. Eck, and\nB. Kegl. Aggregate features and Adaboost for music\nclassiﬁcation. Machine Learning, 65:473–484, 2006.\n[3] P. Cano, E. G ´omez, F. Gouyon, P. Herrera, M. Koppen-\nberger, B. Ong, X. Serra, S. Streich, and N. Wack. IS-\nMIR 2004 audio description contest. Technical Report\nMTG-TR-2006-02, Pompeu Fabra University, 2006.\n[4] O. Cornelis, R. De Caluwe, G. De Tr ´e, A. Hallez,\nM. Leman, T. Matth ´e, D. Moelants, and J. Ganse-\nmans. Digitisation of the ethnomusicological sound\narchive of the royal museum for central africa (bel-\ngium). International Association of Sound and Audio-\nvisual Archives Journal, 26:35–43, 2005.\n[5] P.J. Ponce de Le ´on and J. M. I nesta. A pattern recog-\nnition approach for music style identiﬁcation using\nshallow statistical descriptors. IEEE Trans. on Systems\nMan and Cybernetics C, 37(2):248–257, 2007.\n[6] F. Moreno-Seco; J. M. I ˜nesta; P. Ponce de Le ´on;\nL. Mic ´o. Comparison of classiﬁer fusion methods\nfor classiﬁcation in pattern recognition tasks. Lecture\nNotes in Computer Science, 4109:705–713, 2006.\n[7] M. Grimaldi, P. Cunningham, and A. Kokaram. An\nevaluation of alternative feature selection strategies and\nensemble techniques for classifying music. In Proc.\nWorkshop on Multimedia Discovery and Mining, 2003.\n[8] J. Kittler, M. Hatef, R. P.W. Duin, and J. Matas. On\ncombining classiﬁers. IEEE Trans. on Pattern Analysis\nand Machine Intelligence, 20(3):226–239, 1998.\n[9] L. I. Kuncheva. Combining Pattern Classiﬁers: Meth-\nods and Algorithms. Wiley-Interscience, 2004.\n[10] T. Lidy, C. N. Silla Jr., O. Cornelis, F. Gouyon,\nA. Rauber, C. A. A. Kaestner, and A. L. Koerich. Onthe suitability of state-of-the-art music information re-\ntrieval methods for analyzing, categorizing, structur-\ning and accessing non-western and ethnic music col-\nlections. Signal Processing, 90(4):1032 – 1048, 2010.\n[11] T. Lidy and A. Rauber. Evaluation of feature extractors\nand psycho-acoustic transformations for music genre\nclassiﬁcation. In Proc. ISMIR, London, UK, 2005.\n[12] T. Lidy, A. Rauber, A. Pertusa, and J.M. I ˜nesta. Im-\nproving genre classiﬁcation by combination of audio\nand symbolic descriptors using a transcription system.\nInProc. ISMIR, Vienna, Austria, 2007.\n[13] C. McKay, R. Fiebrink, D. McEnnis, B. Li, and I. Fu-\njinaga. Ace: A framework for optimizing music classi-\nﬁcation. In Proc. ISMIR, London, UK, 2005.\n[14] B. Pardo and W. P. Birmingham. Algorithms for\nchordal analysis. Comput. Music J., 26:27–49, 2002.\n[15] C. Perez-Sancho, D. Rizo, and J. M. I ˜nesta. Genre clas-\nsiﬁcation using chords and stochastic language models.\nConnection Science, 21(2 & 3):145–159, May 2009.\n[16] A. Pertusa and J. M. I ˜nesta. Multiple fundamental\nfrequency estimation using Gaussian smoothness. In\nProc. IEEE Int. Conf. on Acoustics, Speech, and Sig-\nnal Processing (ICASSP), Las Vegas, USA, 2008.\n[17] D. Rizo, P.J. Ponce de Le ´on, C. P ´erez-Sancho, A. Per-\ntusa, and J.M. I ˜nesta. A pattern recognition approach\nfor melody track selection in midi ﬁles. In Proc. IS-\nMIR, Victoria, Canada, 2006.\n[18] C. N. Silla Jr., A. L. Koerich, and C. A. A. Kaestner.\nThe latin music database. In Proc. ISMIR, Philadel-\nphia, USA, 2008.\n[19] G. Tzanetakis. Manipulation, Analysis and Retrieval\nSystems for Audio Signals . PhD thesis, Computer Sci-\nence Department, Princeton University, 2002.\n[20] I.H. Witten and E. Frank. Data Mining: Practical ma-\nchine learning tools and techniques. Morgan Kauf-\nmann, San Francisco, 2nd edition, 2005.\n284\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "Boosting for Multi-Modal Music Emotion Classification.",
        "author": [
            "Qi Lu",
            "Xiaoou Chen",
            "Deshun Yang",
            "Jun Wang"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1417085",
        "url": "https://doi.org/10.5281/zenodo.1417085",
        "ee": "https://zenodo.org/records/1417085/files/LuCYW10.pdf",
        "abstract": "With the explosive growth of music recordings, automatic classification of music emotion becomes one of the hot spots on research and engineering. Typical music emotion classification (MEC) approaches apply machine learning",
        "zenodo_id": 1417085,
        "dblp_key": "conf/ismir/LuCYW10",
        "keywords": [
            "Music Emotion Classification",
            "MEC",
            "AdaBoost Algorithm",
            "Multimodal Integration",
            "Audio Features",
            "MIDI Features",
            "Lyrics Features",
            "Fusion by Subtask Merging",
            "Semantic Analysis",
            "Machine Learning for Music"
        ],
        "content": "BOOSTING FOR MULTI-MODAL MUSIC EMOTION \nCLASSIFICATION \nQi Lu, Xiaoou Chen, Deshun Yang, Jun Wang \nPeking University \nInstitute of Computer Science & Technology \n{luqi,chenxiaoou,yangdeshun,wangjun}@icst.pku.edu.cn \n \nABSTRACT \nWith the explosive growth of music recordings, automatic \nclassification of music emo tion becomes one of the hot \nspots on research and engineering. Typical music emotion classification (MEC) approaches apply machine learning methods to train a classifier based on audio features. In addition to audio features, the MIDI and lyrics features of music also contain useful semantic information for pre-dicting the emotion of music. In this paper we apply AdaBoost algorithm to integrate MIDI, audio and lyrics information and propose a two-layer classifying strategy called Fusion by Subtask Merging for 4-class music emo-tion classification. We ev aluate each modality respec-\ntively using SVM, and then combine any two of the three modalities, using AdaBoost algorithm (MIDI+audio, MIDI+lyrics, audio+lyrics). Moreover, integrating this in a multimodal system (MIDI+a udio+lyrics) allows an im-\nprovement in the overall performance. The experimental results show that MIDI, audio and lyrics information are complementary, and can be co mbined to improve a clas-\nsification system. \nKey Words: Music Emotion Classification, Mul-\nti-Modal, AdaBoost, Fusion by Subtask Merging \n1. INTRODUCTION AN D RELATED WORKS \nMusic Information Retrieval is a sub-area of information retrieval. Important research directions include for exam-ple similarity retrieval, musical genre classification, or music analysis and knowledge representation. As the mu-sic databases grow, classification and retrieval of music by emotion [2]-[7] has recen tly received increasing atten-\ntion. Traditionally music emotion classification (MEC) ap-\nplies algorithms of machine learning on audio features, such as Mel frequency cepst ral coefficient (MFCC), to \nrecognize the emotion embedded in the audio signal. Meanwhile we can also use some mid-level audio fea-tures such as chord [5] or rhythmic patterns [8] for this problem, but sometimes it can’t get a promising result because of the semantic gap. \nComplementary to audio features, lyrics are semanti-\ncally rich and expressive and have profound impact on human perception of music [17]. It is often easy for us to tell from the lyrics whether a song expresses love, sad-ness, happiness, or something else. Incorporating lyrics in the analysis of music emotio n is feasible because most \npopular songs sold in the market come with lyrics and because most lyrics are co mposed in accordance with \nmusic signal [18].  \nBesides music’s audio and lyrics features, the MIDI \nfeatures of music have been ever used in music instru-ment classification and retrieval. As a popular file format for storing music, MIDI carri es more abstract music in-\nformation than audio. In this paper we firstly apply the music’s MIDI file to the mu sic emotion classification. \nA multi-modal analysis approach using audio and lyr-\nics features has been proposed and evaluated in music genre classification by Mayer and Neumayer [1]. And promising results have been achieved by combining the audio and lyrics using various types of machine learning algorithms such as SVM and k-NN. Besides, several mul-ti-modal fusion methods using audio and lyrics for music emotion classification are proposed by Yang [2]. Howev-er, little has been reported in the literature that applies \n105\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)AdaBoost to multi-modal automatic music emotion clas-\nsification. In this paper, we propose a new multi-modal fusing approach that uses fe atures extracted from MIDI \nfiles, audio signal and lyrics for 4-class music emotion classification. We focus on how to combine the three modalities: MIDI, audio and lyrics using AdaBoost.   \nThe remainder of the paper is organized as follows. \nSection 2 describes the MIDI, audio and lyrics features we need respectively. Section 3 describes the details of the proposed multi-modal approach. Section 4 provides the result of a performance study, and Section 5 con-cludes the paper. \n2. FEATURES \nIn our experiment we use a free program jMIR1.0 with default parameter values to extract MIDI and audio fea-tures. jAudio and jSymbolic are two important compo-nents of jMIR for extracting audio and MIDI features. jAudio is a software package for extracting features from audio files. These extracted features can then be used in many areas of music information retrieval (MIR) research. jSymbolic is a software package for extracting high-level musical features from symbolic music representations, specifically MIDI files. \n2.1 MIDI Features \nThe MIDI music files are fi rstly transformed from the \ncorresponding waveform files by a computer tool WIDI \nRecognition System Professional 4.1 which could be found on the internet [19]. And then we use jSymbolic \nwith default parameter values to extract MIDI features \nfrom the MIDI files. The ex tracted MIDI features, which \nare listed in Table 1 , are adopted in our experiments. \n# Feature Dimensions \n1 Duration 1 \n2 Acoustic Guitar Fraction 1 \n3 Average Melodic Interval 1 \n……   \n101 Voice Separation 1 \n102 Woodwinds Fraction 1 \nTable 1.  MIDI features extracted by jSymbolic. From Table 1  we can see there are 102 features ex-\ntracted by jSmbolic from each MIDI music file. As each feature just has one dimension, a whole MIDI feature \nvector has 102 dimensions. \n2.2 Audio Features \nWe use jAudio to extract a number of low-level audio \nfeatures from the waveform files. The extracted features, \nwhich are listed in Table 2 , have been commonly used \nfor MEC in pervious works [3]-[5]. \n# Feature Dimensions\n1 Magnitude Spectrum Variable \n2 FFT Bin Frequency Labels Variable \n3 Spectral Centroid 1 \n……   \n25 Zero Crossings 1 \n26 Beat Sum 1 \nTable 2.  Audio features extracted by jAudio. \nFrom Table 2  we can see there are 26 features ex-\ntracted by jAudio from each audio file. Among these 26 \nfeatures, there are 5 features  such as Magnitude Spectrum \nand MFCC with variable dimensions, other ones with 1 \ndimension. In our experiment, an audio feature vector has \n79 dimensions. \n2.3 Lyrics Features \nLyrics are normally available on the web and downloada-\nble with a simple crawler. The acquired lyrics are pre-\nprocessed with traditional info rmation retrieval operations \nsuch as stopword removal, stemming, and tokenization. \nIn our experiment, two algorithms are adopted to generate \ntextual features. \nUni-gram  A standard textual feature representation \ncounts the occurrence of uni-gram terms (words) in each \ndocument, and constructs the bag-of-words model [10], which represents a document as a vector of terms weighted by a tf-idf function defined as: \n) ( #| |log ) , ( # ) , (\nij i j it DDd t d t tfidf=   (1) \nwhere  ) , ( #j id t denotes the frequency of termitoc-\n106\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)curs in documentjd , ) ( #it Dthe number of documents \nin which itoccurs, and | |Dthe size of the corpus. We \ncompute the tf-idf for each te rm and select the M most \nfrequent terms as our features (M is empirically set to \n2000 in this work by a validation set). \nBi-gram  N-gram is sequences of N consecutive words \n[10]. An N-gram of size 1 is a uni-gram (single word), \nsize 2 is a bi-gram (word pairs). N-gram models are widely used to model the dependency of words. Since negation terms often reverse the meaning of the words next to them, it seems reasonable to incorporate word pairs to the bag-of-words model to take the effect of ne-gation terms into account. To this end, we select the M most frequent uni-gram and bi-gram in the bag-of-words model and obtain a new feature representation. \n3. TAXONOMY \nWe adopt Thayer’s arousal-valence emotion plane [15] as our taxonomy and define four emotion classes happy, angry, sad, and relaxing, according to the four quadrants \nof the emotion plane, as shown in Figure 1 . As arousal \n(how exciting/calming) and valence (how positive/ nega-\ntive) are the two basic emotion dimensions found to be most important and universal [16], we can also view the four-class emotion classificati on problem as the classifi-\ncation of high/low arousal and positive/negative valence. This view will be used in mutli-modal music emotion classification. \n \nFigure 1. Thayer’s arousal-valence emotion plane. We define \nfour emotion classes according to the four quadrants of the \nemotion plane. We can also su bdivide the four-class emotion \nclassification to binary arousal classification and valence clas-\nsification.  4. PROPOSED APPROACH \nIn this paper, we use AdaBoost, an ensemble learning \nalgorithm, to train a classifier by integrating MIDI, audio and lyrics features. Boosting is a method to combine a collection of weak classifica tion functions (weak learner) \nto form a stronger classifier [21]. AdaBoost is an adaptive algorithm to boost a sequence of  classifiers, in that the \nweights are updated dynamica lly according to the errors \nin previous learning [22].  \nTieu and Viola [12] adapte d AdaBoost algorithm for \nnatural image retrieval. They made the weak learner work \nin a single feature each time. So after \nT rounds of \nboosting, Tfeatures are selected together with the T \nweak classifiers. We adapted AdaBoost algorithm of Tieu \nand Viola’s version for music emotion classification and retrieval. In each iteration, we made the weak learner work on each modality independently. So we can get three classifiers which are trained according to MIDI, audio and lyrics features respectively each time. And then we select the classifier of the minimum learning error as \nthe representative of this iteration. After\n T rounds of \nboosting, T weak classifiers are pr oduced in the end. \nThe classic AdaBoost algorithm is only used for binary \nclassification. In a 4-class scenario, we propose a \ntwo-layer classifying strate gy called Fusion by Subtask \nMerging.  \n•Fusion by Subtask Merging (FSM):  Use AdaBoost to \nclassify arousal and valence separately and then merge \nthe result. To enhance readability, we denote the classifi-cation model trained by AdaBoost for classifying arousal and valence as M\nA and M V, respectively. For example, a \nnegative arousal (predicted by M A) and negative valence \n(predicted by M V) would be merged to class 3. We make \nthe three modalities focus on different emotion classifica-tion subtasks because empirical  test reveals MIDI, audio \nand text clues are complementary and useful for different subtasks. In addition, training models for arousal and va-lence separately has been shown adequate. \n4.1 AdaBoost \nThe AdaBoost algorithm We adapted in our experiment \nas follows: \nInput: 1)\n ntraining examples \n) , ( , ), , (1 1 n ny x y xK with 1=iy or0;  \n107\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)2) the number of iterations T. \nInitialize weights lwi21\n, 1= orm21for 1=iy or0, \nwith n m l= + .  \nDo for T t, , 1K= && 5 . 0≤tε :  \n1. Train one hypothesisjhfor each modality jwithtw, \nand error i tn\ni i i j j w y x h, 1) ) ( (∗ − =∑=ε . \n2. Choose ) ( ) (⋅ = ⋅k t h h such thatj k k jεεp,≠ ∀ .Let \nk tεε= . \n3. Update: ie\nt i t i t w wβ, , 1=+ , where 1=ie or0for  \nexample ix classified correctly or incorrectly respec-\ntively, and \ntt\ntεεβ−=1. \n4. Normalize the weights so th at they are a distribution, \n∑=++\n+←n\nj j ti t\ni twww\n1 , 1, 1\n, 1. \nOutput the final hypothesis, \n⎪⎩⎪⎨⎧≥= ∑ ∑= =\notherwisex h ifx hT\nt tT\nt t t\nf\n021) ( 1) ( 1 1α α\n  (2) \nwhere \nttβα1log= .\n \n4.2 Support Vector Machine \nSupport vector machine (SVM ) learns an optimal sepa-\nrating hyperplane (OSH) given a set of positive and nega-\ntive examples. Kernel functions are used for SVM to learn a non-linear boundary if necessary. See Vapnik [14] for a detailed introduction of SVM. Li and Guo [13] tried to use the SVM for audio classification and retrieval. In this paper, SVM is selected as our weak learner. In our \nexperiment we use the SMO which is a fast implementa-tion of SVM algorithm provided by WEKA3.6.1 [20].  5. EXPERIMENTS \nThe music database is made up of 500 Chinese pop songs, whose emotions are labeled through a subjective test \nconducted by 8 participants. The corresponding lyrics are \ndownloaded from the Internet by a web crawler. Classifi-cation accuracy is evaluated by randomly selecting 400 songs as training data and 100 songs as testing data. We conducted 2 experiments. To assure the confidence, we performed the experiments based on a five-fold cross validation. We use the features  extracted by jSymbolic for \nMIDI feature representation, the features extracted by \njAudio for audio feature representation and the uni-gram and bi-gram based bag-of-words model for lyrics feature representation.  \n5.1 Single Feature Sets \nIn our first experiment, we apply SVM to mono-modal \nbased music emotion 4-class classification (MEC) using MIDI, audio and lyrics info rmation respectively. There-\nfore, we got three SVM cla ssifiers which are trained on \neach mono-modality. Our SVM implementation is the SMO algorithm provided by WEKA3.6.1 and the kernel function is Polynomial. To enhance readability, we de-note the classification model trained by MIDI, audio and \ntextual features as MO, AO and LO  respectively. \n• MIDI-Only (MO): Use MIDI features only and apply \nSVM to classify emotion. This serves as a baseline. MO \nis used to assess the importance of the MIDI modality. \n• Audio-Only (AO):  Use audio features only and apply \nSVM to classify emotion. This serves as a baseline be-\ncause many existing MEC work adopts it [1-2]. AO is used to assess the importance of the audio modality. \n• Lyrics-Only (LO):  Use lyrics features only and apply \nSVM to classify emotion. This serves as a baseline be-\ncause many existing MEC work adopts it [1-2]. LO is used to assess the importance of the text modality. \nThe Results of experiment 1 are shown in Table 3 : \nClassifier Name Features Accuracy(4-class)\nMO MIDI 0.586 \nAO audio 0.598 \nLO lyrics 0.491 \nTable 3. Results of mono-modal method using SVM for \n4-class emotion classification. \n108\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)5.2 Multi-Modal Feature Set Combinations \nIn our second experiment, we apply AdaBoost to mul-\nti-modal based music emotion classification. And we se-lect SVM as the weak learner in AdaBoost. We develop and evaluate the following method for fusing MIDI, audio and lyrics. To enhance readability, we denote the classi-fication model trained by MIDI and audio features set, MIDI and lyrics features set, audio and lyrics features set, \nMIDI, audio and lyrics features set as MA, ML, AL and \nMAL  respectively. \n• MIDI+Audio (MA):  Use MIDI and audio features and \napply AdaBoost to classify em otion. The weak learner is \nSVM. \n• MIDI+Lyrics (ML): Use MIDI and lyrics features and \napply AdaBoost to classify em otion. The weak learner is \nSVM. • Audio+Lyrics (AL):  Use audio and lyrics features and \napply AdaBoost to classify em otion. The weak learner is \nSVM. • MIDI+Audio+Lyrics (MAL): Use MIDI, audio and \nlyrics features and apply AdaBoost to classify emotion. \nThe weak learner is SVM. \nThe Results of experiment 2 are shown in Table 4 : \nClassifier \nName Features Accuracy(4-class)\nMA MIDI+audio 0.616 \nML MIDI+lyrics 0.712 \nAL audio+lyrics 0.72 \nMAL MIDI+audio+lyrics 0.724 \nTable 4. Results of multi-modal fusion method using \nAdaBoost for 4-class emotion classification. \n4.3 Comparison and Analysis of Experimental Results \nBecause of the different databa se, it is difficult to quanti-\ntatively compare the proposed approach with existing \nones. Alternatively, we treat MO, AO and LO as the three baselines, and compare the classification accuracy of mono-modal and multi-modal approaches.  \nIt can be observed from row 2 to 4 of Table 3  that \nMIDI features, audio features and textual features per-forms very poor on 4-class emotion classification, with MO’s accuracy 58.6%, AO’s accuracy 59.8%, LO’s ac-\ncuracy 49.1%. But from row 2 to 4 of Table 4 , we can \nsee MIDI features, audio features and lyrics features are fairly complementary, beca use the combination of any \ntwo of them outperforms the mono-modal approach, with \nMA’s accuracy 61.6%, ML’s accuracy 71.2%, AL’s ac-\ncuracy 72.0%. Table 4 also indicates that the 4-class \nemotion classification accurac y can be significantly im-\nproved by fusing all the three modalities. Among the fu-sion methods (rows 2-5 of Table 4 ), MAL achieves the \nbest classification accuracy (72.4%) and contributes a \n23.3% relative improvement over the lyrics-only (LO) baseline. This seems to imply the individual strength of the three modalities should be emphasized separately.  \n6. CONCLUSION \nIn this paper we have described a preliminary mul-ti-modal approach to music emotion classification that \nexploits features extracted from the MIDI, audio and the \nlyrics of a song. We apply AdaBoost algorithm to ensem-\nble the three modalities. A new approach of multi-modal fusion method called Fusion by Subtask Merging (FSM) is developed and evaluated. Experiments on a moderately large-scale database show that MIDI, audio and lyrics indeed carry semantic information complementary to each other. By the proposed fusion by subtask merging \nstrategy, we can improve the classification accuracy from \n49.1% to 72.4%. Using lyrics features also significantly \nimproves the accuracy of va lence classification from \n61.6% to 72.4%. Meanwhile, we find that MIDI and au-dio features contribute fairly  to the music emotion classi-\nfication. From the result, we can see that the accuracy of \nMO is 58.6%, while that of AO is 59.8%. Besides, the \naccuracy of ML is 71.2%, while that of AL is 72.0%. An \nexplanation for this phenomenon is that there exists some redundancy between MIDI and audio information. As well, an exploration of more natural language processing algorithms and more effectiv e features for modeling the \ncharacteristics of lyrics is underway. Besides, we’re try-ing to verifying more ensemble learning algorithms on multi-modal music emotion classification. \n7. REFERENCES \n[1] Rudolf Mayer et al: “Multi-modal Analysis of Music: A large-scale Evaluation,”  In Proceedings of the \nWorkshop on Exploring Musical Information Spac-es , 2009. \n109\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)[2] Yang, Y.-H. et al: “Toward multi-modal music emo-\ntion classification,” Proc. PCM , pp. 70-79, 2008. \n[3] Yang, Y.-H. et al: “A regression approach to music emotion recognition,”  IEEE Trans. Audio, Speech \nand Language Processing , Vol. 16, No. 2, pp. \n448-457, 2008. \n[4] Lu, L. et al: “Automatic mood detection and track-ing of music audio signals,” IEEE Trans. Audio, \nSpeech and Language Processing , Vol. 14, No. 1, \npp. 5-18, 2006. \n[5] Cheng, H.-T. et al: “Automatic chord recognition for music classification and retrieval,” Proc. ICME , pp. \n1505-1508, 2008. \n[6] Yang, D. et al: “Disambiguating music emotion us-ing software agents,” Proc. ISMIR , pp. 52-58, 2004. \n[7] Chuang, Z.-J. et al: “Emotion recognition using au-dio features and textual contents,” Proc. ICME , pp. \n53-56, 2004. \n[8] Chua, B.-Y. et al: “Perceptual rhythm determination of music signal for emotion-based classification,” Proc. MMM , pp. 4-11, 2006. \n[9] Yo-Ping Huang and Guan-Long Guo et al: “Using Back Propagation Model to Design a MIDI Music Classification System,” Int. Computer Symposium, Dec. 15-17, 2004, Taipei, Taiwan. \n[10] Sebastiani F.: “Machine learning in automated text categorization,” ACM CSUR , Vol. 34, No. 1, pp. \n1-47, 2002. \n[11] G. Guo, H. Zhang, and S. Z. Li: “Boosting for con-tent-based audio classification and retrieval: an evaluation,” Microsoft Research Tech. Rep . \nMSR-TR-2001-15. \n[12] K.. Tieu and P. Viola: “Boo sting image retrieval,” in \nProc. of Computer Vision and Pattern Recognition , \nv. 1, pp. 228-235, 2000. \n[13] S. Z. Li and G. Guo: “Content-based audio classifi-cation and retrieval using svm learning,”  (invited \ntalk), PCM , 2000. \n[14] V. N. Vapnik: “Statistical learning theory,” John Wiley& Sons, New York, 1998. \n[15] Thayer, R. E. et al: “The Biopsychology of Mood and Arousal”, Oxford University Press, New York, 1989. \n[16] Russel, A.: “A circumplex model of affect”, Journal \nof Personality & Social Science , Vol. 39, No. 6, pp. 1161-1178, 1980. \n[17] Omar Ali, S. et al: “Songs and emotions: are lyrics and melodies equal partners”, Psychology of Music, \nVo\nl. 34, No. 4, pp. 511-534, 2006. \n[18] Formas, J.: “The words of music”, Popular Music \nand Society , Vol. 26, No. 1, 2003. \n[19] http://www.widisoft.com/english/download.html  \n[20] http://www.cs.waikato.ac.nz/ml/weka/  \n[21] Yoav Freund and Robert E. Schapire: “A short in-troduction to boosting,” Journal of Japanese Society for Artificial Intelligence, Vol. 14, No. 3, pp. 771–780, 1999. \n[22] Y. Freund and R. E. Schapire: “A decision-theoretic generalization of online learning and an application to boosting,” Journal  of Computer and System  \nSciences, Vol. 55, No. 1, pp. 119-139, 1997. \n[23] C. Cortes and V. Vapnik: “Support-Vector Net-works,” Machine Learning , Vol. 20, No. 3, pp. \n273-297, 1995. \n110\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "Accurate Real-time Windowed Time Warping.",
        "author": [
            "Robert Macrae",
            "Simon Dixon"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1416156",
        "url": "https://doi.org/10.5281/zenodo.1416156",
        "ee": "https://zenodo.org/records/1416156/files/MacraeD10.pdf",
        "abstract": "Dynamic Time Warping (DTW) is used to find alignments between two related streams of information and can be used to link data, recognise patterns or find similarities. Typically, DTW requires the complete series of both in- put streams in advance and has quadratic time and space requirements. As such DTW is unsuitable for real-time applications and is inefficient for aligning long sequences. We present Windowed Time Warping (WTW), a variation on DTW that, by dividing the path into a series of DTW windows and making use of path cost estimation, achieves alignments with an accuracy and efficiency superior to other leading modifications and with the capability of synchro- nising in real-time. We demonstrate this method in a score following application. Evaluation of the WTW score fol- lowing system found 97.0% of audio note onsets were cor- rectly aligned within 2000 ms of the known time. Results also show reductions in execution times over state-of-the- art efficient DTW modifications.",
        "zenodo_id": 1416156,
        "dblp_key": "conf/ismir/MacraeD10",
        "keywords": [
            "Dynamic Time Warping",
            "alignments between two related streams",
            "real-time applications",
            "path cost estimation",
            "synchro- nising in real-time",
            "score following application",
            "audio note onsets",
            "correctly aligned",
            "reductions in execution times",
            "state-of-the-art efficient DTW modifications"
        ],
        "content": "ACCURATE REAL-TIME WINDOWED TIME WARPING\nRobert Macrae\nCentre for Digital Music\nQueen Mary University of London\nrobert.macrae@elec.qmul.ac.ukSimon Dixon\nCentre for Digital Music\nQueen Mary University of London\nsimon.dixon@elec.qmul.ac.uk\nABSTRACT\nDynamic Time Warping (DTW) is used to ﬁnd alignments\nbetween two related streams of information and can be\nused to link data, recognise patterns or ﬁnd similarities.\nTypically, DTW requires the complete series of both in-\nput streams in advance and has quadratic time and space\nrequirements. As such DTW is unsuitable for real-time\napplications and is inefﬁcient for aligning long sequences.\nWe present Windowed Time Warping (WTW), a variation\non DTW that, by dividing the path into a series of DTW\nwindows and making use of path cost estimation, achieves\nalignments with an accuracy and efﬁciency superior to other\nleading modiﬁcations and with the capability of synchro-\nnising in real-time. We demonstrate this method in a score\nfollowing application. Evaluation of the WTW score fol-\nlowing system found 97.0% of audio note onsets were cor-\nrectly aligned within 2000 ms of the known time. Results\nalso show reductions in execution times over state-of-the-\nart efﬁcient DTW modiﬁcations.\n1. INTRODUCTION\nDynamic Time Warping (DTW) is used to synchronise two\nrelated streams of information by ﬁnding the lowest cost\npath linking feature sequences of the two streams together.\nIt has been used for audio synchronisation [3], cover song\nidentiﬁcation [13], automatic transcription [14], speech pro-\ncessing [10], gesture recognition [7], face recognition [1],\nlip-reading [8], data-mining [5], medicine [15], analytical\nchemistry [2], and genetics [6], as well as other areas. In\nDTW, dynamic programming is used to ﬁnd the minimal\ncost path through an accumulated cost matrix of the ele-\nments of two sequences. As each element from one se-\nquence has to be compared with each element from the\nother, the calculation of the matrix scales inefﬁciently with\nlonger sequences. This, combined with the requirement of\nknowing the start and end points of the sequences, makes\nDTW unsuitable for real-time synchronisation. A real-time\nvariant would make DTW viable at larger scales and capa-\nble of driving applications such as score following, auto-\nmatic accompaniment and live gesture recognition.\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2010 International Society for Music Information Retrieval.Local constraints such as those by Sakoe and Chiba [10]\nimprove the efﬁciency of DTW to linear time and space\ncomplexity by limiting the potential area of the accumu-\nlated cost matrix to within a set distance of the diagonal.\nHowever, not all alignments necessarily ﬁt within these\nbounds. Salvador and Chan proposed, in FastDTW [11],\na multi-resolution DTW where increasingly higher resolu-\ntion DTW paths are bounded by a band around the previous\nlower resolution path, leading to large reductions in the ex-\necution time. On-Line Time Warping by Dixon [3] made\nreal-time synchronisation with DTW possible by calculat-\ning the accumulated cost in a forward manner and bound-\ning the path by a forward path estimation.\nWhile the efﬁciency of DTW has been addressed in\nFastDTW [11] and the real-time aspect has been made pos-\nsible with On-Line Time Warping [3], WTW contributes\nto synchronisation by offering steps to further improve the\nefﬁciency whilst working in a progressive (real-time ap-\nplicable) manner and preserving the accuracy of standard\nDTW. This method consists of breaking down the align-\nment into a series of separate bounded sub-paths and using\na cost estimation to limit the area of the accumulated cost\nmatrix calculated to small regions covering the alignment.\nIn Section 2 we explain conventional DTW before de-\nscribing how WTW works in Section 3. In Section 4 we\nevaluate the accuracy and efﬁciency of WTW in a score\nfollowing application. Finally, in Section 5, we draw con-\nclusions from this work and discuss future improvements.\n2. DYNAMIC TIME WARPING\nDTW requires two sets of features to be extracted from the\ntwo input pieces being aligned and a function for calcu-\nlating the similarity between any two frames of these fea-\nture sets. One such measurement of the similarity is the\ninner product. As the inner product returns a high value\nfor similar frames, we subtract the inner product from one\nso that the optimal path cost is the path with the minimal\ncost. Equation 1 shows how to calculate this similarity\nmeasurement between frames AmandBnfrom feature se-\nquences A= (a1; a2; :::; a M)andB= (b1; b2; :::; b N)\nrespectively:\ndA;B(m; n) = 1\u0000< am; bn>\nkamkkbnk(1)\nDynamic programming is used to ﬁnd the optimum path,\nP= (p1; p2; :::; p W), through the similarity matrix C(m; n)\n423\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Figure 1. Dynamic Time Warping aligning audio with a\nmusical score. The audio is divided into chroma frames\n(bottom) which are then compared against the score’s\nchroma frames (left). The similarity matrix (centre) shows\na path where the sequences have the lowest cost (highest\nsimilarity). Any point on this path indicates where in the\nscore the corresponding audio relates to.\nwithm2[1 :M]andn2[1 :N]where each pk=\n(mk; nk)indicates that frames amkandbnkare part of the\naligned path at position k. An example of this similar-\nity matrix, including the features used and the lowest cost\npath, can be seen in Figure 1. The ﬁnal path is guaranteed\nto have the minimal overall cost D(P) =PW\nk=1dA;B(mk; nk),\nwithin the limits of the features used, whilst satisfying the\nfollowing conditions:\nBounds: p1= (1;1)\npW= (M; N )\nMonotonicity: mk+1\u0015mkfor all k2[1; W\u00001]\nnk+1\u0015nkfor all k2[1; W\u00001]\nContinuity: mk+1\u0014mk+ 1for all k2[1; W\u00001]\nnk+1\u0014nk+ 1for all k2[1; W\u00001]\n3. WINDOWED TIME WARPING\nWTW consists of calculating small sub-alignments and com-\nbining these to form an overall path. Subsequent sub-paths\nare started from points along the previous sub-paths. Real-\ntime path positions can then be extrapolated from these\nsub-paths. The end points of these sub-alignments are ei-\nther undirected, by assuming they lie on the diagonal, or\ndirected, by using a forward path estimate. As such WTW\ncan be seen as a two-pass system similar to FastDTW and\nOTW. The sub-alignments make use of an optimisation\nthat avoids calculating points with costs that are over the\ncost estimate (provided by the initial direction path), re-\nferred to as the A-Star Cost Matrix. WTW also requires\nthe use of Features, Window Dimensions, and Local Con-straints that all affect how the alignments are made. The\noverall process is outlined in Algorithm 1. In order to\ndemonstrate WTW we implemented a score following ap-\nplication using this method to synchronise audio and mu-\nsical scores.\nInput: Feature Sequence A and Feature Sequence B\nOutput: Alignment Path\nPath = new Path.starting(1,1);\nwhile Path.length <min (A.length,B.length) do\nStart = Path.end;\nEnd = Start;\nwhile (End - Start).length <Window Sizedo\nEnd =\nargmin(Inner Product(End.next points));\nend\nCost Estimate = End.cost;\nA-Star Matrix =\nAStar FillRect(Start,End,Cost Estimate);\nPath.add(A Star Matrix.getPath(1,Hop Size));\nend\nreturn Path;\nAlgorithm 1: The Windowed Time Warping algorithm.\n3.1 Features\nThe feature vector describes how the sequence data is rep-\nresented and segmented. The sequence is divided up into\nfeature frames in order to differentiate the changes in the\nsequence over time. The frame size and spacing are re-\nferred to as the window size and hop size respectively. The\nimplementation of WTW for score following requires a\nmusically based feature vector. In this case, we use chroma\nfeatures, a 12dimensional vector corresponding to the unique\npitch classes in standard Western music. The intensities of\nthe chroma vectors can be seen as a representation of the\nharmonic and melodic content of the music. In our imple-\nmentation we use a window size of 200ms and a hop size\nof 50ms.\n3.2 Window Dimensions\nSimilar to how the sequence data is segmented, the win-\ndows of standard DTW in WTW have a window size and\nhop size to describe their size and spacing respectively. A\nlarger window size and/or smaller hop size will increase\nthe accuracy of the alignment, as more of the cost matrix\nis calculated, however will this will be less efﬁcient. Ex-\namples of different window and hop sizes can be seen in\nFigure 2 and a comparison of Window and Hop sizes is\nmade in Section 4.\n3.3 Local Constraints\nWe refer to two types of local constraints in Dynamic Pro-\ngramming. The ﬁrst, henceforth known as the cost con-\nstraint, indicates the possible predecessors of a point pk\non a path. The predecessor pk\u00001with lowest path cost\nD(pk\u00001)is chosen when calculating the accumulated cost\n424\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Figure 2. The regions of the similarity matrix computed\nfor various values of the window size (top row) and hop\nsize (bottom row).\nFigure 3. Some example local constraints as deﬁned by\nRabiner and Juang [9].\nmatrix. The second, referred to as the movement constraint,\nindicates the possible successors of a point pk. Standard\nDTW doesn’t make use of a movement constraint as all the\nframes in the cost matrix are calculated. Examples of local\nconstraints by Rabiner and Juang [9] are show in Figure 3.\nThese constraints deﬁne the characteristics of the dynamic\nprogramming. For example, Type I allows for horizon-\ntal and vertical movement which corresponds to a single\nframe of one sequence being linked to multiple frames of\nthe other. All the other Types allow high cost frames to be\nskipped and Type III and II show how the paths can skip\nthese frames directly or add in the single steps, respec-\ntively. The two path ﬁnding algorithms, described next,\nmake use of the Type I and a modiﬁed version of the Type\nVII (where the steps are taken directly as in Type III) local\nconstraints.\n3.4 Window Guidance\nThe sequential windows that make up the alignment of\nWTW can be either directed or undirected. Whilst it can\nhelp to direct the end point of the windows of DTW (partic-\nularly for alignments between disproportional sequences\nwhere the expected path angle will be far from 45\u000e), the\nsub-paths calculated within these windows can make up\nfor an error in the estimation. A low hop size should en-\nsure the point taken from the sub-path as the starting point\nfor the next window is likely to be on the correct path.For the windows to be directed, a forward estimation is\nrequired. The Forward Greedy Path (FGP) is an algorithm\nwhich makes steps through the similarity matrix based on\nwhichever subsequent step has the highest similarity (min-\nimal cost) using a movement constraint to decide which\nframes are considered. In this manner the path can work in\nan efﬁcient forward progressive manner, however, will be\nmore likely to be thrown off the correct path by any periods\nof dissimilarity within the alignment. The ﬁrst FGP path\nF= (f1; f2; :::; f W)where fk= (mk; nk)starts from po-\nsition f1= (m1; n1)and from then on each subsequent\nframe is determined by whichever of the available frames,\nas determined by the local constraint, has the lowest cost.\nTherefore the total cost D(m; n) to any point (m; n) on\nthe FGP path FisD(fk) =Pk\nl=1d(fl)and any point is\ndependent on the previous point: fk+1=argmin (d(i; j))\nwhere the range of possible values for iandjare deter-\nmined by fkand the local constraints.\nThe FGP path only needs to calculate similarities be-\ntween frames considered within the local constraints and\nso at this stage a vast majority of the similarity matrix does\nnot need to be calculated. When the FGP reaches fW, the\nwindow size, the ﬁnal point fW= (m W; nW)is selected\nas the end point for the accumulated cost-matrix.\nNote that some combinations of constraints that skip\npoints (i.e. where iorjare greater than 1) will require that\njumps in the FGP are ﬁlled in order to compute a complete\ncost estimate, like in the Type V local constraint, so that\nthe cost estimation of the FGP is complete. A comparison\nof guidance measures is made in Section 4.\n3.5 A-Star Cost Matrix\nThe windowed area selected is calculated as an accumu-\nlated cost matrix between the beginning and end points\nof the FGP i.e.C(m; n) ofm2[mf1:mfL]andn2\n[nf1:nfL]. This accumulated cost matrix can be calcu-\nlated in either a forward or reverse manner, linking the start\nto the end point or vice versa. This uses the standard Type\nIcost constraint to determine a frame’s accumulated cost\nas shown by Equation 2:\nD(m; n) = d(m; n) + min8\n<\n:D(m\u00001; n\u00001)\nD(m\u00001; n)\nD(m; n\u00001)9\n=\n;(2)\nThe sub-path S= (s 1; s2; :::; s V)is given by the accu-\nmulated cost constraints by following the cost progression\nfrom the beginning point in this window until the hop size\nis reached. When the sub-path reaches sV, the ﬁnal point\nfV= (mv; nv)is then taken as the starting point for the\nnext window and so on until the end of either sequence is\nreached. The sub-paths are concatenated to construct the\nglobal WTW path. This process can also be seen in Figure\n4.\nEither of the undirected and directed window end point\nestimations provide an estimate cost D(F)for each sub-\npath. This estimate can be used to disregard any points\nwithin the accumulated cost matrix that are above this cost\n425\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Figure 4. The complete Windowed Time Warping path.\nas it is known there is a potential sub-path that is cheaper.\nThe calculation of the similarity for most of these inefﬁ-\ncient points can be avoided by calculating the accumulated\ncost matrix in rows and columns from the end point fL\nto the start f1. When each possible preceding point for\nthe next step of the current row/column has a total cost\nabove the estimated cost i.e.min(D (m\u00001; n\u00001); D (m\u0000\n1; n); D (m; n\u00001))>=D(F)the rest of the row/column\nis then set as more than the cost estimate, thus avoiding cal-\nculating the accumulated cost for a portion of the matrix.\nThis procedure can be seen in Figure 5.\n4. EXPERIMENTAL EV ALUATION\nTo evaluate WTW we used the score following system with\nground truth MIDI, audio and path reference ﬁles and com-\npared the accuracy of the found alignments with the known\nalignments. MATCH, the implementation of On-Line Time\nWarping [4], was also used to align the test pieces for com-\nparison purposes. In both cases the MIDI was converted to\naudio using Timidity.\n4.1 Mazurka Test Data\nThe CHARM Mazurka Project by the Centre for the His-\ntory and Analysis of Recorded Music led by Nick Cook\nat Royal Holloway, University of London has published a\nlarge number of linked metadata ﬁles for Mazurka record-\nings in the form of reverse conducted data,1produced by\nCraig Sapp [12]. We then used template matching to com-\nbine this data with MIDI ﬁles, establishing links between\nMIDI notes and reverse conducted notes at the ms level.\nThis provided a set of ground truth ﬁles linking the MIDI\nscore to the audio recordings. These ground truths were\ncompared with an off-line DTW alignment and manually\nsupervised to correct any differences found. Overall, 217\n1http://mazurka.org.uk/info/revcond/\nFigure 5. The calculation of the accumulated cost ma-\ntrix. The numbering shows the order in which rows and\ncolumns are calculated and the progression of the path\nﬁnding algorithm is shown by arrows. Dark squares repre-\nsent a total cost greater than the estimated path cost whilst\nblack squares indicate points in the accumulated cost ma-\ntrix that do not need to be calculated.\nsets of audio recordings, MIDI scores and reference ﬁles\nwere produced.\n4.2 Evaluation Metrics\nFor each path produced by WTW, each estimated audio\nnote time was compared with the reference and the dif-\nference was recorded. For differing levels of accuracy re-\nquirements (100 ms,200ms,500ms and 2000 ms), the\npercentages of notes that were estimated correctly within\nthis requirement for each piece were recorded. These piece-\nwise accuracies are then averaged for an overall rating. The\n2000 ms accuracy requirement is used as the MIREX score\nfollowing accuracy requirement for notes hit.\n4.3 Window Dimensions\nThe effect of the window size and hop size in WTW is\nexamined in Table 1. The accuracy tests (shown in the top\nhalf) show a trend that suggests larger window sizes and\nsmaller hop sizes lead to greater accuracy, as is similar to\nfeature frame dimensions. However, larger window sizes\nand smaller hop sizes also lead to slower execution times\nas more points on the similarity matrix were calculated.\n4.4 Window Guidance\nA comparison of guidance methods for WTW is shown\nin Table 2. This comparison shows that for the test data\nused, there was not much difference between directed and\nundirected WTW and directed only offered an improve-\nment when a large local constraint was used.\n426\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Alignment Accuracy at 2000 ms\nWindow Size\nHop Size 100 200 300 400\n100 76.0% 83.1% 81.8% 81.9%\n200 63.7% 82.2% 82.0% 82.0%\n300 57.7% 77.7% 81.2% 82.5%\n400 57.4% 66.1% 81.1% 82.0%\nTable 1. Accuracy test results comparing different window\nand hop sizes for WTW. For this test there was a guidance\nFGP that used a Type VII +6 movement constraint (see\nTable 2) and the accumulated cost matrix used a Type I\ncost constraint and Type I movement constraint.\nAlignment Accuracy\nAcc. Req. 100 ms 200 ms 500 ms 2000 ms\nNone 63.8% 75.9% 82.0% 86.9%\nType I 56.2% 68.7% 74.2% 78.1%\nType IV 63.3% 74.8% 80.7% 86.0%\nType VII 64.0% 76.9% 82.6% 86.6%\nType IV +4 58.0% 70.3% 75.8% 79.5%\nType VII +6 59.4% 72.2% 78.0% 81.2%\nType II 63.9% 75.8% 81.8% 87.3%\nType V 64.9% 78.0% 83.9% 88.1%\nTable 2. Accuracy test results comparing different meth-\nods for guiding the windows in WTW. The name of the\nguidance method refers to the movement constraint used\nin the Forward Greedy Path. The ‘Type 4 +4’ and ‘Type\n7 +6’ constraints include additional horizontal and verti-\ncal frames to complete the block. For this test the window\nand hop size were set at 300ms and the accumulated cost\nmatrix used a Type I cost constraint and Type I movement\nconstraint.\n4.5 Accuracy Results\nThe results of the accuracy test can be seen in Table 3.\nFrom this test we can see WTW produces an accuracy\nrate comparable with that of OTW. What separates the two\nmethods is that the OTW method took on average 7:38\nseconds to align a Mazurka audio and score ﬁle where\nas WTW took 0:09 seconds, (approximately one 80th of\nthe time of OTW). The average length of the Mazurka\nrecordings is 141:3 seconds, therefore, in addition to hav-\ning the ability to calculate the alignment path sequentially,\nboth methods achieve greater than real-time performance\nby some margin.\n4.6 Efﬁciency Results\nThe efﬁciency tests consisted of aligning sequences of dif-\nferent lengths and recording the execution time. The re-\nsults of this test can be seen in Table 4. These results show\nthat WTW has linear time costs in relation to the length\nof the input sequence, unlike standard DTW. The optimi-\nsations suggested in this work are shown to decrease the\ntime cost in aligning larger sequences over FastDTW.Alignment Accuracy\nAcc. Req. 100 ms 200 ms 500 ms 2000 ms\nWTW 73.6% 88.8% 94.9% 97.0%\nOTW 70.9% 86.7% 94.8% 97.3%\nTable 3. Accuracy test results comparing WTW and OTW\nestimated audio note onset times against references for 217\nMazurka recordings at 4 levels of accuracy requirements.\nFor this test the window and hop size were set at 300ms and\nthe accumulated cost matrix used a Type I cost constraint\nand Type VII movement constraint.\nExecution time (seconds)\nSequence length 100 1000 10000 100000\nDTW 0.02 0.92 57.45 7969.59\nFastDTW (r100) 0.02 0.06 8.42 207.19\nWTW 0.002 0.06 0.90 9.52\nTable 4. Efﬁciency test results showing the execution time\n(in seconds) for 4 different lengths of input sequences (in\nframes). Results for FastDTW and DTW are from [11].\nThe r value for FastDTW relates to the radius factor.\n5. DISCUSSION AND CONCLUSION\nThis paper has introduced WTW, a linear cost variation on\nDTW for real-time synchronisations. WTW breaks down\nthe regular task of creating an accumulated cost matrix be-\ntween the complete series of input sequence vectors, into\nsmall, sequential, cost matrices. Additional optimisations\ninclude local constraints in the dynamic programming and\ncut-off limits for the accumulated cost matrices.\nEvaluation of WTW has shown it to be more efﬁcient\nthan state of the art DTW based off-line alignment tech-\nniques. WTW has also been shown to match the accu-\nracy of OTW whilst improving on the time taken to process\nﬁles. Whilst this difference has little effect when synchro-\nnising live sequences on standard computers, the greater\nefﬁciency of WTW could be useful in running real-time\nsynchronisation methods on less powerful processors, such\nas those in mobile phones, or when data-mining large data-\nsets for tasks such as cover song identiﬁcation.\nFuture work will involve evaluating WTW on a wider\nvariety of test data-sets, including non-audio related tasks\nand features. Possible improvements may be found in novel\nlocal constraints and/or the dynamic programming used to\nestimate the start and end points of the accumulated cost\nmatrices. Presently, WTW assumes the alignment is con-\ntinuous from the start to the end. A more ﬂexible approach\nwill be required to handle alignments made of partial se-\nquence matches. Also, the modiﬁcations of WTW could\npotentially be combined with other modiﬁcations of DTW,\nsuch as those in FastDTW in order to pool efﬁciencies.\nLastly, WTW, like DTW, is applicable to a number of tasks\nthat involve data-mining, recognition systems or similarity\nmeasures. It is hoped WTW makes DTW viable for appli-\ncations on large data sets in a wide range of ﬁelds.\n427\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)6. ACKNOWLEDGMENTS\nThis work is supported by the EPSRC project OMRAS2\n(EP/ E017614/1). Robert Macrae is supported by an EP-\nSRC DTA Grant.\n7. REFERENCES\n[1] Bir Bhanu and Xiaoli Zhou. Face recognition from face\nproﬁle using dynamic time warping. In ICPR ’04: Pro-\nceedings of the Pattern Recognition, 17th International\nConference on (ICPR’04) Volume 4, pages 499–502,\nWashington, DC, USA, 2004. IEEE Computer Society.\n[2] David Clifford, Glenn Stone, Ivan Montoliu, Serge\nRezzi, Franc ¸ois-Pierre Martin, Philippe Guy, Stephen\nBruce, and Sunil Kochhar. Alignment using variable\npenalty dynamic time warping. Analytical Chemistry,\n81(3):1000–1007, January 2009.\n[3] Simon Dixon. Live tracking of musical performances\nusing on-line time warping. In Proceedings of the\n8th International Conference on Digital Audio Effects,\npages 92–97, Madrid, Spain, 2005.\n[4] Simon Dixon and Gerhard Widmer. MATCH: A music\nalignment tool chest. In Proceedings of the 6th Inter-\nnational Conference on Music Information Retrieval,\npage 6, 2005.\n[5] Eamonn J. Keogh and Michael J. Pazzani. Scaling up\ndynamic time warping for datamining applications. In\nProceedings of the sixth ACM SIGKDD international\nconference on Knowledge discovery and data mining,\npages 285–289, New York, NY , USA, 2000. ACM.\n[6] Beno ´ıt Legrand, C. S. Chang, S. H. Ong, Soek-Ying\nNeo, and Nallasivam Palanisamy. Chromosome classi-\nﬁcation using dynamic time warping. Pattern Recogn.\nLett., 29(3):215–222, 2008.\n[7] Meinard M ¨uller. Information Retrieval for Music and\nMotion. Springer-Verlag New York, Inc., Secaucus,\nNJ, USA, 2007.\n[8] Hiroshi Murase and Rie Sakai. Moving object recogni-\ntion in eigenspace representation: gait analysis and lip\nreading. Pattern Recogn. Lett., 17(2):155–162, 1996.\n[9] Lawrence Rabiner and Biing-Hwang Juang. Funda-\nmentals of speech recognition. Prentice-Hall, Inc., Up-\nper Saddle River, NJ, USA, 1993.\n[10] Hiroaki Sakoe and Seibi Chiba. Dynamic program-\nming algorithm optimization for spoken word recog-\nnition. IEEE Transactions on Acoustics, Speech, and\nSignal Processing, 26(1), 1978.\n[11] Stan Salvador and Philip Chan. FastDTW: Toward ac-\ncurate dynamic time warping in linear time and space.\nInWorkshop on Mining Temporal and Sequential Data,\npage 11, 2004.[12] Craig Sapp. Comparative analysis of multiple musi-\ncal performances. In Proceedings of the International\nConference on Music Information Retrieval (ISMIR)\n2007, pages 497–500, 2007.\n[13] J Serr `a, E G ´omez, P Herrera, and X Serra. Chroma\nbinary similarity and local alignment applied to cover\nsong identiﬁcation. Audio, Speech, and Language\nProcessing, IEEE Transactions on, 16(6):1138–1151,\n2008.\n[14] Robert J. Turetsky and Daniel P.W. Ellis. Ground-truth\ntranscriptions of real music from force-aligned midi\nsyntheses. In Proceedings of the 4th International Con-\nference on Music Information Retrieval, 2003.\n[15] H. J. L. M. Vullings, M. H. G. Verhaegen, and H. B.\nVerbruggen. Automated ECG segmentation with dy-\nnamic time warping. In Proceedings of the 20th An-\nnual International Conference of the IEEE In Engi-\nneering in Medicine and Biology Society, 1998, vol-\nume 1, pages 163–166, 1998.\n428\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "Query-by-conducting: An Interface to Retrieve Classical-music Interpretations by Real-time Tempo Input.",
        "author": [
            "Akira Maezawa",
            "Masataka Goto",
            "Hiroshi G. Okuno"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1416614",
        "url": "https://doi.org/10.5281/zenodo.1416614",
        "ee": "https://zenodo.org/records/1416614/files/MaezawaGO10.pdf",
        "abstract": "This paper presents an interface for finding interpretations of a user-specified music, Query-by-Conducting. In clas- sical music, there are many interpretations to a particular piece, and finding “the” interpretation that matches the lis- tener’s taste allows a listener to further enjoy the piece. The critical issue in finding such an interpretation is the way or interface to allow the listener to listen through differ- ent interpretations. Our interface allows a user, by swing- ing a conducting hardware interface, to conduct the desired global tempo along the playback of a piece, at any time in the piece. The real-time conducting input by the user dy- namically switches the interpretation being played back to the one closest to how the user is currently conducting. At the end of the piece, our interface ranks each interpretation according to how close the tempo of each interpretation was to the user input. At the core of our interface is an automated tempo es- timation method based on audio-score alignment. We im- prove tempo estimation by requiring the audio-score align- ment of different interpretations to be consistent with each other. We evaluate the tempo estimation method using a solo, chamber, and orchestral repertoire. The proposed tempo estimation decreases the error by as much as 0.94 times the original error.",
        "zenodo_id": 1416614,
        "dblp_key": "conf/ismir/MaezawaGO10",
        "keywords": [
            "conducting interface",
            "interpreting music",
            "real-time tempo adjustment",
            "audio-score alignment",
            "tempo estimation",
            "tempo error reduction",
            "classical music",
            "user-defined interpretations",
            "tempo-based ranking",
            "swing conducting hardware"
        ],
        "content": "QUERY-BY-CONDUCTING: AN INTERFACE TO RETRIEVE\nCLASSICAL-MUSIC INTERPRETATIONS BY REAL-TIME TEMPO INPUT\nAkira Maezawa,†Masataka Goto‡and Hiroshi G. Okuno†\nyDept. of Intelligence Science and TechnologyzNational Institute of Advanced Industrial\nGraduate School of Informatics, Kyoto University Science and Technology (AIST)\nSakyo-ku, Kyoto 606-8501 Japan Tsukuba, Ibaraki 305-8568 Japan\n{amaezaw1, okuno}@kuis.kyoto-u.ac.jp m.goto@aist.go.jp\nABSTRACT\nThis paper presents an interface for ﬁnding interpretations\nof a user-speciﬁed music, Query-by-Conducting . In clas-\nsical music, there are many interpretations to a particular\npiece, and ﬁnding “the” interpretation that matches the lis-\ntener’s taste allows a listener to further enjoy the piece. The\ncritical issue in ﬁnding such an interpretation is the way\nor interface to allow the listener to listen through differ-\nent interpretations. Our interface allows a user, by swing-\ning a conducting hardware interface, to conduct the desired\nglobal tempo along the playback of a piece, at any time in\nthe piece. The real-time conducting input by the user dy-\nnamically switches the interpretation being played back to\nthe one closest to how the user is currently conducting. At\nthe end of the piece, our interface ranks each interpretation\naccording to how close the tempo of each interpretation\nwas to the user input.\nAt the core of our interface is an automated tempo es-\ntimation method based on audio-score alignment. We im-\nprove tempo estimation by requiring the audio-score align-\nment of different interpretations to be consistent with each\nother. We evaluate the tempo estimation method using a\nsolo, chamber, and orchestral repertoire. The proposed\ntempo estimation decreases the error by as much as 0.94\ntimes the original error.\n1. INTRODUCTION\nClassical music is unique in that many audio recordings\nexist for a given piece of music. For example, as of\nMarch 2010, a search on an on-line shopping site for\n“Mendelssohn Violin Concerto” returns 1200+ hits, or that\nof “Beethoven Spring Sonata” returns 300+ hits. Each\nof these recordings is an acoustic rendition of a particu-\nlar music score, embodied by an unique interpretation of\nthe performer. Finding an interpretation that matches the\nlistener’s taste is an important aspect of enjoying classical\nmusic. However, searching for such recording is tiresome\nbecause it requires the listener to listen through the same\npiece many times.\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc⃝2010 International Society for Music Information Retrieval.\nFigure\n1: System diagram of Query-by-Conducting .\nOur goal is to retrieve interpretations1on the basis of\nvarious aspects of music interpretation, similar to content-\nbased music information retrieval (CBMIR), which re-\ntrieves pieces on the basis of various aspects of music such\nas rhythm and timbre [2, 3]. What constitutes musical in-\nterpretation is a difﬁcult question, though it seems that mu-\nsicians express interpretation by manipulating the tempo,\nthe timbre, or bringing out interesting melodic lines. This\npaper focuses on the global tempo – the average tempo\nof a piece over a few beats. We believe tempo is an as-\npect of music interpretation that many listeners take note\nof. Studies in music cognition suggest that similarity of in-\nterpretations is strongly reﬂected in global tempo [4], and\nmany studies are motivated by the signiﬁcance of tempo\non interpretation [1, 5].\nWe present Query-by-Conducting , a new interface for\nﬁnding interpretations of a user-speciﬁed music by con-\nducting the global tempo. The interface reads, as the mu-\nsic score, a standard MIDI ﬁle of a piece of music, and\ndifferent interpretations of the music score as audio ﬁles.\nThe interface facilitates playback, visualization and query\nof interpretations by supporting the following features, as\nshown in Figure 1:\n1. Visualization of global tempi of the interpretations\n2. Hardware conducting interface (a Nintendo Wii re-\nmote) for intuitively entering, along the playback of\na piece, the user’s tempo query in real time\n3. Ranking and retrieval of interpretations on the basis\nof the similarity between each interpretation and the\ncurrent tempo query entered by the user.\nVisualization allows the user to view the range of in-\nterpretations available, and thus, the valid range of tempo\n1We\nshall use the term “interpretation” to mean a rendition of a par-\nticular symbolic representation of music, as per [1].\n477\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Figure\n2:System-level state diagram. The interface alternates\nbetween (1) and (2) until the piece ends.\nin which the user should conduct to obtain a meaningful\nquery. As a particular interpretation is played back, the\nuser may become dissatisﬁed with its tempo. Then, the\nuser would “conduct” the desired tempo using the hard-\nware conducting interface. Based on the user’s tempo in-\nput, the interface retrieves and switches the interpretation\nbeing played back to one that is closest to the input. The\nuser may stop conducting if the current interpretation is\nsatisfactory. At the end of playback, the system ranks each\ninterpretation on the basis of how similar the overall global\ntempo trajectory was to the user’s conducting.\nOur interface differs from existing conducting inter-\nfaces [6–9] in three respects. First, we use conducting de-\nvice to switch the interpretation being played back, instead\nof specifying the tempo of the entire piece. The user has\nthe freedom of either listening to a piece, or conducting\nthe tempo of an interpretation that the user wants to lis-\nten. Second, our method allows the user to control only\nthe global tempo instead of local tempo or dynamics. We\nbelieve that such restriction is an effective way to retrieve\na particular interpretation; global tempo is easy for a typ-\nical user to specify, but specifying local tempo requires\na precise control of the tempo. The notion of restricting\nthe user control to a few dimensions has been proposed\nin other studies aimed at easily manipulating expressive\nmusic [10]. Finally, our conducting interface is meant to\nretrieve a particular interpretation to play back, whereas\nmost conducting interfaces are aimed at real-time tempo-\nral manipulation of a particular audio signal. Unlike query\nby tapping [11], which uses rhythm pattern as the query,\nas our method uses the tempo as the query.\nThe interface relies on tempo estimation that is deter-\nmined through audio-score alignment. In existing stud-\nies [1, 12–14], audio-score alignment was created using\nonly the information obtained from the audio of interest\nand the score. There may be errors in the alignment, but\ngiven one alignment, there is no way of knowing where\nan error is. When aligning multiple interpretations, how-\never, it is also possible to create audio-score alignment by\naligning the score to some other audio, and then aligning\nthat audio to the audio of interest. Thus, given Ninter-\npretations to align, Nunique audio-score alignments to\noneinterpretation can be generated. We use these multi-\nple audio-score alignments generated to estimate the true\naudio-score alignment that is error-free.\nFigure\n3:The interface in playback-mode visualizes the tempo\nalong playback of an interpretation.\nA video demonstration of our interface is available at\nhttp://www.youtube.com/QueryByConducting\n2. INTERFACE DESIGN\nThe interface offers functions in a conventional music\nplayback interface such as playback and rewind. More-\nover, it features a visualizer of global tempi of various in-\nterpretations, a conducting hardware to enter the global\ntempo query in real time, and retrieval of interpretations\non the basis of the user query. As shown in the state di-\nagram in Figure 2, our system alternates between playing\nback the current interpretation (“playback-mode”), and ac-\ncepting user conducting and retrieving appropriate inter-\npretation to play back (“conduct-mode”). At the end, our\nsystem ranks each interpretation on the basis of the tempo\n(“ranking-mode”).\nFigure 3 shows the interface during playback\n(playback-mode ). Bottom of the screen displays the\ntitle, the performer and the playback time, similar to\nconventional music playback interface. Top of the screen\nvisualizes the global tempi, and presents each interpre-\ntation sorted in descending order of the current global\ntempo. Bottom right shows the state of the conducting\ninterface.\nAs the piece is played back, the user may become dis-\nsatisﬁed with the tempo of the piece (“I liked the introduc-\ntion, but the development section is too slow,” a user might\nthink). As shown in Figure 4, the interface allows a user\nto “conduct” the desired global tempo in conduct-mode ,\nin real time. In conduct-mode , the interface accepts beat\ninput from the conducting hardware interface, and also vi-\nsualizes the beat at the bottom left to facilitate proper con-\nducting. The entered tempo is used as a query to retrieve\nthe interpretation whose global tempo is closest to what\nthe user conducts, and to switch the current playback to\nit. This mode offers the user an active listening experience\nby constantly retrieving and cross-fading the playback to\ninterpretation that plays like how the user is conducting.\nAt the end of the piece, the interface enters the ranking-\nmode, and ranks each interpretation based on how similar\neach interpretation was to the user’s overall conducting.\nThe ranking is presented to the user.\n478\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Figure\n4:The interface in conduct-mode accepts user’s con-\nducting using a controller, and switches interpretations being\nplayed back. Beat visualizer facilitates user’s conducting.\n2.1 Interpretation Visualizer\nTop half of the interface (in Figure 3) is the interpretation\nvisualizer. It shows the tempi of different interpretations,\nalong with tempo trajectory of the interpretation that is be-\ning played back and the user query.\nFigure 5 shows the visualizer in further detail. It\npresents tempo information against time. To allow the user\nto view the detailed tempo near the current playback posi-\ntion as well as the tempo of the entire piece, we distort the\nnormalized x-coordinate, x(t). x(t) is distorted such that\nthe vicinity of current playback position tcis zoomed like\na lens. Let t, the current beat of playback, be deﬁned for\n[0; t l], where tlis the duration of the piece, and the range\nofx∈[0;1]. Then, the visualizer applies the following\nfunction:\nx(t) =t\n2tl+1\n2exp\u0000t−tc\nT\u0001\nexp\u0000t−tc\nT\u0001\n+ 1(1)\nThis\nshows approximately T-neighborhood of the current\nplayback position in more detail than the rest. Tis chosen\nto be four quarter notes.\nA vertical straight line indicates the current playback\nposition. Line segment to the left of the current playback\nposition is the past tempo trajectory of the user’s global\ntempo query. Line segment to the right of the current play-\nback position is the future tempo trajectory of the inter-\npretation that is being played back. This way, the user is\nable to view the query entered so far, and how the current\ninterpretation will unfold.\nTo show the range of possible interpretations, the range\nof global tempi is expressed as a colorful strip, overlayed\nto to the line segments described above. The strip is col-\nored using a gradation of hue angle, such that fast tempo is\nassociated with small hue (orange), and slow tempo with\nlarge hue (blue). At beat t, given the slowest tempo \u001cmin(t),\nfastest tempo \u001cmax(t), and some tempo in between, \u001c(t),\nwe set the hue to the following angle:\nhue(t) = 240◦−τ(t)−τmin(t)\nτmax(t)−τmin(t)230◦(2)\nRight half\nof the visualizer prints the performer of each\ninterpretation, sorted in descending order of the current\nglobal tempo. The interpretation that is being played back\nis highlighted. Next to each name, a box whose hue value\nis as described in Equation (2) is painted.\nFigure\n5:Interpretation visualizer shows the tempo trajectory\nof the current interpretation, how the user has conducted, and the\nrange of tempi.\n2.2 Hardware Conducting Interface\nThe hardware conducting interface detects beat from an\naccelerometer embedded in the hardware controller, and\nconverts it into tempo. The user interface shows beat visu-\nalizer to facilitate tempo entry.\n2.2.1 Beat detection\nWe accept the user’s conducting query using a game con-\ntroller that features a 3-axis accelerometer (a Nintendo Wii\ncontroller). Our system detects beat by checking for peaks\nin the axis vertical to the controller. Such peak is gener-\nated when the controller is ﬂicked up, as a conductor would\nﬂick the baton to indicate the beat.\nOnce a beat is detected, the accelerometer input is ig-\nnored for 200msec to prevent false triggering. Therefore,\nour system accepts tempo of up to 300 beats-per-minute\n(BPM), which is sufﬁcient for virtually all classical music.\n2.2.2 Converting beat input to tempo query\nTo specify a new tempo, the user must conduct a tempo dif-\nferent from the playback. We observed that people tends\nto conduct notin the desired tempo, but instead ahead or\nbehind of the beat of the playback to indicate faster or\nslower tempo relative to the current playback. We con-\njectured that such phenomenon occurs because people are\ndistracted by the downbeat of the playback, and sets the de-\nsired beat location relative to the last downbeat he/she has\nheard. Therefore, we convert the offset of the user’s con-\nducting with respect to the beat of the playback, to the de-\nsired tempo in BPM. Suppose the user conducts ∆tbehind\nthe beat. Then, supposing the current BPM of the playback\nisBPM 0, we convert ∆tto user-speciﬁed tempo, BPM ,\nas follows:\nBPM =BPM 01\n∆t\n60=BP\nM0+ 1(3)\nThe average of user-speciﬁed BPM over four beats is used\nas the query.\n2.2.3 Beat visualizer\nWe observed that, in a preliminary experiment using a few\ntest subjects, people did not always have a clear sense of\nrhythm, and had trouble ﬁnding where the beat is. This\nwas especially true for music whose instrumentation did\nnot include instrument with strong attack and decay, such\nas the piano or plucked strings.\nTo facilitate tempo input, we display a beat visualizer,\nas shown at the bottom left of Figure 3, and in detail in\n479\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Figure\n6:Visualizer for facilitating tempo input. The color bar\nrotates in synchrony with the beat so that the user could easily\ngrasp the downbeat.\nFigure 6. The visualizer has a colored stripe that rotates\naround the large circle, in synchrony with the beat of the\nplayback. At each downbeat, the stripe crosses the small\ncircle at the bottom. The arc-length of the rotating stripe\ncorresponds to the range of global tempi at the current beat,\nand the hue is calculated using Equation (2). Therefore, if\nthe user wants to switch to fast interpretation, for exam-\nple, from tempo shown as green in the tempo visualizer to\norange-colored tempo, the user could ﬂick the controller\nas the orange-colored segment of the arc crosses the small\ncircle at the bottom.\n2.3 Interpretation Retriever\nAfter the user has ﬁnished listening through a piece, the\ninterface ranks, in ranking-mode , each interpretation on\nthe basis of the similarity between the tempo trajectory of\nthe interpretation and the user query.\nWe use the tempo trajectories of the interpretations that\nwere played as the query. For example, if the user listened\nto interpretation xfor the ﬁrst minute and yfor the next two\nminutes, our query would consist of the tempo trajectory of\ninterpretation xfor the ﬁrst minute and yfor the next two.\nLet us deﬁne the dissimilarity score of the user query\nand each interpretation. Let \u001ci(t)be the global tempo tra-\njectory of the ith interpretation, and \u001cq(t)be the query\ntempo trajectory. Then, we deﬁne tempo dissimilarity for\ninterpretation i,rias follows:\nri=1\nTZT\n0\u0012τi(t)−τq(t)\nτq(t)\u00132\ndt (4)\nThe interpretations\nare sorted in the ascending order of\ntempo dissimilarity, as shown in Figure 7. A transparency\nvalue is associated to each interpretation being drawn, such\nthat interpretation with lowest dissimilarity is opaque, and\nthe highest transparent. Moreover, dissimilarity measure\nthat is inverted, shifted and scaled between 0 and 1 is\nshown next to each interpretation.\n3. TEMPO EXTRACTION METHOD\nGlobal tempo extraction is based on evaluating the audio-\nscore alignment. Since accurate alignment is essential for\naccurate tempo estimation, we propose a method to im-\nprove the audio-score alignment.\n3.1 Initial Audio-Score Alignment\nThe initial audio-score alignment is based on dynamic\ntime-warping (DTW) using chroma vector as the feature,\nsimilar to other works [13–15].\nLetc(t)\nkbe a 12 dimensional vector that contains the\nchroma vector computed for tth audio frame of the kth\nFigure\n7:Ranking-mode ranks each interpretation based on\nthe tempo similarity, presents similar interpretations as opaque\nand dissimilar ones transparent.\ninterpretation. Let c(t)\nSbe a 12 dimensional chroma vec-\ntor computed from the music score at tick t. We generate\nthe alignment from the score to the kth interpretation, de-\nnoted Mk s, or from interpretation itoj, denoted Mj i.\nTo generate Mk s, a similarity matrix Rk sis ﬁrst com-\nputed. Let Nsbe the number of ticks in the music score\nandNkbe the number of audio frames contained in in-\nterpretation k. Let Rk sbe a Ns-by-N kmatrix, whose\nelement i; jcontains:\nRk←s(i, j) = 1 −c(i)\ns·c(j)\nk\n∥c(i)\ns∥ ·\n∥c(j)\nk∥(5)\nNext, we ﬁnd the alignment path using DTW. Formally, we\ndeﬁne a cost matrix Ns-by-N kmatrix, as follows:\nCk←s(i, j) =Rk←s(i, j) + min8\n><\n>:Ck←s(i−1, j)\nCk←s(i, j−1)\nCk←s(i−1, j−1)(6)\nwhere for all t,Ck s(t;−1) = Ck s(−1; t) = 0. Next,\nwe determine the parametric representation of the audio-\nscore alignment, M(t)\nk s, by backtracking the cost matrix.\nFirst, we set M(0)\nk sto(Ns; Nk), and update in the follow-\ning manner while incrementing t, until M(t)\nk s= (0; 0):\nM(t+1)\nk←s := argmin\n(I;J)∈SCk←s(I, J) (7)\nS ={(i−1, j),(i, j−1),(i−1, j−1)}\nwhere (i; j) =M(t)\nk s. Audio-audio alignment from inter-\npretation itoj,Mj i, can be achieved in the same way,\nby computing the similarity matrix between chroma vector\nsequence of interpretation iandj.\n3.2 Improving Audio-Score Alignment\nWe improve audio-score alignment by requiring the align-\nments of different interpretations to be consistent with\neach other. Given one music score and Ninterpretations,\nthere are Npossible paths to generate the alignment from\nthe music score to interpretation i, as shown in Figure 8.\nNamely, in addition to the direct mapping from the score\nto interpretation i, it is also possible to generate mapping\nfrom the score to interpretation j(audio-score alignment),\nwhich is then mapped by using the map from interpretation\njtoi(audio-audio alignment). Ideally, all Npaths from\nthe score to an interpretation should be identical. In real-\nity, however, they are not because they are generated using\ndifferent similarity matrices.\nIn order to generate a map from the score to some inter-\npretation ivia interpretation j,Mi j◦Mj S, both Mj S\nandMi jmust be one-to-one, but the alignments gener-\nated in the previous section are not.\nTherefore, we trace, over the alignment determined in\nthe previous section, a new map that is one-to-one. We per-\nform the following procedure for each alignment between\nsome interpretation (or score) sandk:\n480\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Figure\n8: By combining two alignments, there are multi-\nple ways to align the score to an interpretation.\n1. Set t= 0, and the initial point of the reﬁned alignment\n˜M(t)\nk←sto(0,0).\n2. For ϵ < θ <\u0019\n2−ϵ, compute\nthe following cost function:\nc(θ) =Eq∼exp (− 3q=Q) [min\nnd(t)\nn(q, θ)] (8)\nwhere\nd(t)\nn(q, θ) =∥˜M(t)\nk←s+q(cos(θ ),sin(θ ))−M(n)\nk←s∥(9)\nQ is chosen to be 20 frames, and ϵto be π/20radian. θis\nevaluated every π/20radians.\n3. Update ˜Mk←sas follows, for some ∆r∈(0,1]:\n˜M(t+1)\nk←s := ˜M(t)\nk←s+ ∆r\u0010\ncos(ˆθ),sin(ˆθ)\u0011\n(10)\nˆθ= argmin\n\u0012c(θ)\nWe chose ∆r= 1frame.\n4. Exit if ˜M(t)\nk←s·(1,0)≥Nsor˜M(t)\nk←s·(0,1)≥Nk.\n5. Set t:=t+ 1, and go to 2.\nWe assume that observed alignments are corrupted by\nindependent and identically distributed noise that follows\nthe Laplace distribution with location parameter ˆMi S(t)\nand scale parameter b, for each beat t:\np(t) = exp\u0010\n−∥M i←S(t)−ˆMi←S(t)∥/b\u0011\n/2b (11)\nand likewise for Mi j◦Mj Sforj̸=i. We interpret\nˆMi Sas the underlying “correct” alignment that generates\nMi SandMi j◦Mj S. Since an estimator of ˆMi Sis\nthe sample median, we update Mi Sas follows:\nMi←S(t) := median ({Mi←j◦Mj←S(t)}) (12)\nAs will be shown in the experiment, iterating this step\nyields in improved alignment accuracy.\n3.3 Tempo Extraction\nThe tempo is estimated by determining the slope of the\naudio-score alignment. We compute the tempo at MIDI\nticktusing alignment information obtained between tick\nt−Ttot+TforT > 0. Only information at note onsets\nare used, as alignment results between two note onsets are\nnot reliable. We choose Tdynamically such that at least 20\naudio frames that correspond to note onsets are within this\nrange. Let (s(p); a(p)) contain a parametric representation\nofMi S that contain the audio frames chosen. scorre-\nsponds to the domain (tick of note onsets) and athe range\n(audio frame). Then, we compute the BPM at tick t,\u001c(t)\nby ﬁrst ﬁnding the slope m(t)of(s(p); a(p))using linear\nregression, and multiplying its inverse by a scalar factor:\nτ(t) =1\nm(t)audio frame-per\n-minute\nticks-per-beat(13)\n4.\nEXPERIMENTS\nWe evaluate the tempo estimation method, and retrieval of\ninterpretation on the basis of global tempo query. We ana-\nlyzed nine classical pieces of varying instrumentation. OfTable 1: Average MSE (mean-squared error) improvement\nin thousandths (10\u00003)after iterating Equation (12).\nPiece (No.\nInterp.) None Iter\n. 1 Iter. 2 Iter. 10\nsolo-1 (13) 8.9 8.6\n8.4 8.4\nsolo-2 (6) 17.3 15.0\n13.0 12.7\nsolo-3 (5) 266.7 73.1\n85.4 98.8\nduo-1 (5) 4.5 3.9\n3.7 3.8\nduo-2 (4) 34.8 22.1\n20.6 20.4\nduo-3 (4) 185.4 12.5\n10.2 10.2\norch-1 (5) 646.8 54.4\n47.2 44.9\norch-2 (5) 231.5 14.7\n13.3 13.2\norch-3 (5) 3941.6 1091.4\n1038.3 833.2\nnine pieces,\nthree are orchestral (denoted orch-1 toorch-\n3), three are written for small ensemble (denoted duo-1 to\nduo-3), and three are solo piano (denoted solo-1 tosolo-3 ).\nFor each work, multiple interpretations (between four and\nthirteen) were obtained and their ground truth tempo data\nwere entered using an in-house tempo entry utility.\n4.1 Evaluation of Audio-Score Alignment\nLet\u001cg(t)be the ground truth tempo trajectory. Given an\nestimated tempo trajectory ˆ\u001c(t), we evaluate the error us-\ning scaled mean squared error (MSE), deﬁned as follows:\nMSE =1\nTZT\n0\u0012τg(t)−ˆτ(t)\nτg(t)\u00132\ndt (14)\nMSE can\nbe considered as the dissimilarity measure be-\ntween the ground truth and the estimated tempo.\nTable 1 shows the average of MSE over all interpreta-\ntion for each of the nine pieces, as the number of iterations\nof the update step (Equation (12)) is changed.\nThe results suggest that, ﬁrst, our method is capable of\ndecreasing the error, more so if the initial error is high. For\nexample, duo-3 has its error decreased by 0.94 times the\noriginal error, after ten iterations. Second, in most cases,\niterating our method multiple times yields in decreased er-\nror. When the error increases with increased number of\niterations, we believe that our assumption that alignments\nare corrupted by independent noise fails. For example, in\npieces that involve unnotated candenza (e.g. solo-3), incor-\nrect alignment occurs consistently at the cadenza. Then,\ntaking the median of such corrupted data yields not in\nthe underlying “true” alignment, as our method posits, but\nsome meaningless data instead.\n4.2 Evaluation of Music Query\nWe evaluate the robustness of our system against errors in\nconducting. When a user conducts like some interpretation\ni, the system should retrieve ias the most similar interpre-\ntation. Other results may be returned for two reasons:\n1. The user could not conduct the piece accurately\nenough to return the desired query.\n2. Imprecision in tempo estimation method causes in-\ncorrect result to be returned.\nIn these cases, imay not be the most similar, but one of M\nmost similar interpretations.\nFirst, we synthesize an artiﬁcial query that models hu-\nman errors in conducting, by adding a smooth noise to the\nground truth tempo trajectory of each data. For each in-\nterpretation i, we use the following tempo trajectory as the\nquery with some noise variance s:\nτquery,i(t;s) = τg;i(t)·2√s\nLPL\u00001\nl=0n(t−l)|L= 10 (15)\nn\n(t;s)∼ N (0,1)\n481\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Top 1 Top 2 Top 3 Top 4 Top 500.20.40.60.81F-measureNoise variance\ns=0.0\ns=0.1\ns=0.15\ns=0.2\ns=0.5(a)solo-1\nTop 1 Top 2 Top 3 Top 4 Top 500.20.40.60.81F-measureNoise variance\ns=0.0\ns=0.1\ns=0.15\ns=0.2\ns=0.5\n(b)duo-1\nTop 1 Top 2 Top 3 Top 4 Top 500.20.40.60.81F-measureNoise variance\ns=0.0\ns=0.1\ns=0.15\ns=0.2\ns=0.5\n(c)orc\nh-1\nFigure 9: Evaluation results when retrieving up to top 5results\nthat are similar to artiﬁcially generated query which deviates from\nthe ground truth by variance s.\nNext, we retrieve Minterpretations that are most simi-\nlar to the artiﬁcial query for each interpretation, and eval-\nuate the performance of retrieval the F-measure. Let n\nbe the number of interpretations correctly retrieved by the\nquery. Let Nbe the total number of interpretations. Then,\nwe let recall R=n=N , and precision P=n=(N ×M).\nThe F-measure Fis2PR=(P +R). We show the results\nfrom solo-1, duo-1, and orch-1 in Figure 9 (a), (b), and (c).\nFigure 9 (a) and (b) show that the system tolerates small\nerror in conducting, of up to about s= 0:15, or 0.7 to 1.3\ntimes the original tempo (three-sigma). Figure 9 (c), how-\never, shows that the F-measure of orch-1 is considerably\nlower than the other two.\nRetrieving orchestral piece ( orch’s) is difﬁcult because\nthere is very small variation in the two closest playing, and\nexacerbated by the particularly unreliable tempo estima-\ntion. We compute the smallest dissimilarity measure be-\ntween ground truth tempo trajectories of any pair of inter-\npretations. The smallest dissimilarity of orch-1 is about\n2×10\u00003,solo-1 is20×10\u00003, and duo-1 is23×10\u00003. We\nsimilarly observed that for orchestral piece, the smallest\ndissimilarity is much smaller compared to that of cham-\nber (duo’s) or solo (solo’s). On the other hand, we observe\nthat the average MSE, as seen in Table 1, is substantially\ngreater for orchestral pieces than chamber or solo.\nThese results suggest that our system retrieves the de-\nsired interpretation with robustness against minor errors in\nconducting, as long as the average MSE is small enough to\ndifferentiate the most similar pair of interpretations. The\nsimilarity of interpretation is typically inﬂuenced by the\nscale of orchestration, and the average MSE is inﬂuenced\nby the complexity of the ensemble, and the degree to which\nthe interpretation deviates from the music score.\n5. CONCLUSION\nThis paper presented Query-by-Conducting , an interface\nfor ﬁnding interpretations of a given piece of music. It of-fers the listener an interactive experience of “conducting”\nthe global tempo to dynamically tailor the interpretation\nplayed back to the user’s choice. It moreover presents the\nlistener with a ranking of interpretation based on how the\nuser conducted through the piece, offering the listener with\na list of interpretations whose tempi that the user might\nlike, without the hassle of listening through various inter-\npretations. The accuracy of tempo estimation method im-\nproved as a result of considering the consistency of audio-\nscore alignment among different interpretations.\nAs future work, we would like to deal with aspects\nof music interpretation other than the global tempo, such\nas the local tempo deviation and emphasis of a particu-\nlar melodic line. Integrating these aspects would further\nenhance the system’s capability to retrieve the interpreta-\ntion of choice. Furthermore, we would like to realize more\nways to visualize and interact with various aspects of mu-\nsic interpretation, to allow a listener to further enjoy clas-\nsical music.\nAcknowledgment: This research was partially supported\nby Graint-in-Aid for Scientiﬁc Research (S) of the Min-\nistry of Education, Culture, Sports, Scince and Technology\n(MEXT), and the CrestMuse Project of the Japan Science\nand Technology Agency (JST).\n6. REFERENCES\n[1] M. M ¨uller et al. Towards automated extraction of tempo pa-\nrameters from expressive music recordings. In ISMIR ’09 ,\npages 69–74, 2009.\n[2] C.A.Michael et al. Content-based music information re-\ntrieval: Current directions and future challenges. volume 96,\npages 668–696, April 2008.\n[3] S. Dixon, F. Guyon and G. Widmer. Towards characterization\nof music via rhythmic patterns. In ISMIR ’04 , pages 509–516,\n2004.\n[4] R. Timmers. Predicting the similarity between expressive\nperformances of music from measurements of tempo and dy-\nnamics. JASA, 117(1):391–399, 2005.\n[5] H. Honing. From time to time: The representation of timing\nand tempo. Comp. Music J., 25(3):50–61, 2001.\n[6] S.Schertenleib et al. Conducting a virtual orchestra. IEEE\nMultiMedia, 11(3):40–49, 2004.\n[7] J. Segen, S. Kumar and J. Gluckman. Visual interface for\nconducting virtual orchestra. ICPR ’00 , page 1276, 2000.\n[8] H. Katayose and K. Okudaira. Using an expressive perfor-\nmance template in a music conducting interface. In NIME\n’04, pages 124–129, 2004.\n[9] F.Bevilacqua et al. Wireless sensor interface and gesture-\nfollower for music pedagogy. In NIME ’07 , pages 124–129,\n2007.\n[10] S. Dixon, W. Goebl and G. Widmer. The ”air worm”: An\ninterface for real-time manipulation of expressive music per-\nformance. In ICMC ’05, pages 614–617, 2005.\n[11] J.R.Jang, H.Lee and C.Yeh. Query by tapping: A new\nparadigm for content-based music retrieval from acoustic in-\nput. In PCM ’01 , pages 590–597. Springer-Verlag, 2001.\n[12] F. Kurth et al. SyncPlayer - an advanced system for multi-\nmodal music access. In ISMIR ’05 , pages 381–388, 2005.\n[13] S.Dixon and G.Widmer. MATCH: Music alignment tool\nchest. In ISMIR’05 , pages 11–15, 2005.\n[14] N.Hu, R. Dannenberg and G. Tzanetakis. Polyphonic audio\nmatching and alignment for music retrieval. In WASPAA ’03 ,\npages 185–188, 2003.\n[15] M. M ¨uller, F. Kurth and T. R ¨oder. Towards an efﬁcient al-\ngorithm for automatic score-to-audio synchronization. In IS-\nMIR’04 , pages 365–372, 2004.\n482\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "Similarity Measures for Chinese Pop Music Based on Low-level Audio Signal Attributes.",
        "author": [
            "Chun-Man Mak",
            "Tan Lee 0001",
            "Suman Senapati",
            "Yu Ting Yeung",
            "Wang-Kong Lam"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1415636",
        "url": "https://doi.org/10.5281/zenodo.1415636",
        "ee": "https://zenodo.org/records/1415636/files/MakLSYL10.pdf",
        "abstract": "In this article a method of computing similarity of two Chinese pop songs is presented. It is based on five attributes extracted from the audio signal. They include music instrument, singing voice style, singer gender, tempo, and degree of noisiness. We compare the com- puted similarity measures with similarity scores obtained with subjective listening by over 200 human subjects. The results show that rhythm and mood related attributes like tempo and degree of noisiness are most correlated to human perception of Chinese pop songs. Instrument and singing style are relatively less relevant. The results of subjective evaluation also indicate that the proposed method of similarity computation is fairly correlated with human perception.",
        "zenodo_id": 1415636,
        "dblp_key": "conf/ismir/MakLSYL10",
        "keywords": [
            "Chinese pop songs",
            "audio signal",
            "five attributes",
            "music instrument",
            "singing voice style",
            "singer gender",
            "tempo",
            "degree of noisiness",
            "human perception",
            "subjective listening"
        ],
        "content": "Similarity Measures for Chinese Pop Music Based on Low-Level \nAudio Signal Attributes  \nChun-Man Mak; Tan Lee; Suman Senapati; Yu-Ting Yeung; Wang-Kong Lam \nDepartment of Electronic Engineering \nThe Chinese University of Hong Kong \n{cmmak, tanlee, ssuman, ytyeung, wklam}@ee.cuhk.edu.hk \nABSTRACT \nIn this article a method of computing similarity of two \nChinese pop songs is presented. It is based on five \nattributes extracted from the audio signal. They include music instrument, singing voice style, singer gender, \ntempo, and degree of noisiness. We compare the com-\nputed similarity measures with similarity scores obtained \nwith subjective listening by over 200 human subjects. \nThe results show that rhythm and mood related attributes \nlike tempo and degree of noisiness are most correlated to \nhuman perception of Chinese pop songs. Instrument and singing style are relatively less relevant. The results of \nsubjective evaluation also indicate that the proposed \nmethod of similarity computation is fairly correlated with \nhuman perception.  \n1. INTRODUCTION \nWith the rapidly increasing popularity of digital music \nand related technologies, thousands of new songs are made available over the Internet everyday. The conven-\nience of low-cost digital storage also promotes the in-\ncrease in personal collection of music files. However, a \nuser may find it more difficult to look for a song that he \nor she likes to listen. This calls for music recommenda-\ntion systems to sort and find music efficiently.  \nA recommendation system aims to identify songs that \nmatch a user’s taste and recommend these songs to the \nuser. There are two types of music recommendation sys-\ntems: Content-based and metadata-based recommenda-\ntion. A content-based system mainly exploits audio fea-\ntures extracted from the music file itself to make recom-\nmendation [2], [6]. Metadata-based systems, like the Last.fm  music network [11], make use of textual meta-\ndata associated with music documents, such as the artist’s name, the song title, or the album name. These systems \nare often combined with collaborative filtering tech-\nniques to capture users’ preferences. Lyrics of a song can \nalso be used [1]. There are also hybrid systems that com-\nbine audio content and metadata for recommendation, \n[3],[4],[10]. \n  In content-based recommendation system, the recom-\nmended songs are those that sound similar to the existing \nsongs that the user likes to listen to. Recommendation \nbecomes a task of matching in this case. A kind of simi-\nlarity between a query song and a set of candidate songs \nneeds to be computed. The candidates with the highest similarity measure will be returned as the results of rec-\nommendation. The computation of an effective similarity \nmeasure is therefore a crucial step in many recommenda-\ntion systems.  \nSimilarity of two songs can be computed from the con-\ntents or textual metadata embedded in the songs. In this \narticle, we focus on studying content-based music simi-\nlarity computed from songs in a specific genre, namely, \nChinese pop songs. Most recommendation systems pre-\nviously proposed were developed in a cross-genre envi-\nronment [2],[10]. The song database contains a large \nnumber of songs of different genres, and genre classifica-\ntion is performed to put songs into the right group. Rec-\nommendation of songs within the same genre also has \nimportant applications. For example, a pop music pro-\nducer may wish to promote his/her new productions by matching the taste of potential listeners. It is necessary to \nunderstand the aspects that humans consider when they \ndecide if two songs are similar in intra-genre case. In ad-\ndition, to the best of authors’ knowledge, there does not \nexist any content-based recommendation system for Chi-\nnese pop songs. Our investigation results can provide \nvaluable information in building such system in the fu-\nture. \nTo facilitate the computation of a similarity measure, \nthe important attributes of a song must be represented \nnumerically. There are many attributes that can be used \nto represent a song, such as melody structure and chord. \nSuch features, however, are difficult to extract accurately.  Therefore, in the proposed method, we use a set of low-\nlevel audio descriptors, i.e., instrument identity, singing \nstyle, gender of the singer, tempo, and degree of noisi-\nness (DoN) to represent the songs. The usefulness of \nthese features is verified via subjective evaluation.  \nThe paper is organized as follows. Section 2 describes \nthe audio attributes that are used to represent Chinese pop \nsongs, and the method of computing similarity from these \nattributes. Section 3 explains the process of subjective \nevaluation. Section 4 gives the experimental results, and \nthe correlation between the proposed similarity measure \nand the findings of subjective evaluation. Conclusions \nare drawn in section 5.  \n \nPermission to make digital or hard copies of all or part of this work fo r \npersonal or classroom use is granted without fee provided that copies \nare not made or distributed for profit or commercial advantage and that \ncopies bear this notice and the full citation on the first page.  \n© 2010 International Society for Music Information Retrieval  \n513\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \n2. MUSIC ATTRIBUTES \nPrior to signal analysis, all audio files are down-sampled \nto 16 KHz. Most digital music are recorded at sampling \nrate above 16 KHz. By unifying to this common fre-\nquency, we try to minimize the discrepancies of signal quality among the many possible data sources. In the \ncase of stereo recording, the two tracks are averaged to \nobtain a monaural wave signal. In short, all signal analy-\nsis operations are performed based on monaural wave \nsignal at 16 kHz sampling rate.  \n2.1 Vocal / Non-vocal Segmentation \n \nA typical Chinese pop song is about 3 to 4 minutes in \nduration and is composed of four parts. It starts with an \ninstrument intro, followed by vocal verse, and instru-\nment interlude, and finally the chorus. The vocal part is sung in the Cantonese dialect or Mandarin. Assuming \nthis song structure, we divided each song into the vocal \npart (with human singing voice) and the non-vocal part \n(instruments only) in our analysis. The extraction of in-\nstrument attribute is done on the non-vocal part, while \nthe attributes of singing style and gender are extracted \nfrom the vocal part.  Tempo and DoN are estimated from the whole song. This is illustrated in Figure 1.  \nThe wave signal is divided into short-time frames of \n100ms long with frame shift of 50ms. Short-time Fourier \nTransform (STFT) of 2048 points is applied to each \nframe and 13 Mel-frequency cepstral coefficients \n(MFCC) are computed. A statistical classifier is built for \nvocal / non-vocal segmentation. About 500 songs are manually segmented into vocal and non-vocal parts. \nThese songs, along with other songs used in our experi-\nments, are all Chinese pop songs purchased from CD \nstores. These segments are used as the training data for \nthe vocal / non-vocal classifier. The minimum segment \nlength was set to be 1 second. A vocal class and a non-\nvocal class are modeled by two Gaussian mixture models (GMM), each with 64 mixtures.  \nThe segmentation is performed by dividing the audio \nsignals into non-overlapping segments of 1 second long, \ni.e., 20 frames, and classifying these segments as either \nvocal or non-vocal. For each segment, the log-likelihood \nwith respect to the vocal class is computed as \n/g11/g12 /g11/g12/g166\n/g32/g3220\n1, , log\nnvocal vocal vocal n vocal Prob L /g305 μ w /g300 (1) \nwhere n/g300is the MFCC feature vector of the nth frame in \nthe segment, andvocal vocal vocal /g305 μ w, ,  are respectively the \nmixture weights, means, and covariance matrices of the \nvocal GMM.  The log-likelihood with respect to the non-  \n \nFigure 1 .  Segments of songs \n \nvocal class, denoted by Lnon-vocal , is computed in the same \nway. The segment is classified to the class with the higher log-likelihood. This method of statistical classifi-\ncation is also used in the extraction of instrument, sing-\ning style, and gender attributes. The classification accu-\nracy of the vocal / non-vocal classifier is 90%. The test data used in our experiments are not included in the \ntraining set and this applies to all other classification ac-\ncuracy reported in this article.  Readers may also notice that the numbers of mixtures used vary for different clas-\nsifiers described in latter sections. These numbers are determined empirically to achieve the best trade-off be-\ntween computation time and accuracy. \n \n2.2\n Extraction of Music Attributes  \n \nFive attributes are used to describe a Chinese pop song:  \ninstrument, singing style, gender of singer, tempo, and \ndegree of noisiness.  Instrument, singing style, and gen-\nder are computed based on MFCC features and the statis-\ntical classification technique. \n2.2.1 Instrument \n \nFor simplicity, we assume that only a single instrument is \npresent or dominant at a particular time instant. Eight in-\nstruments commonly used in Chinese pop songs are mod-\neled: reedy, electronic-guitar, piano, strings, synthesizer, \nguitar, flute, and percussion. Each instrument is repre-\nsented by a GMM of 256 mixtures. The training data in-\ncludes the 500 manually annotated songs and part of the \nRWC database [7]. Following an approach similar to vo-cal/non-vocal segmentation, each non-vocal segment in a \nsong is assigned to an instrument class. Then the instru-\nment attribute is represented by a vector with eight ele-\nments, i.e., \n /g94 /g968 2 1,..., ,I I Iinst/g32F  (2) \nwhere Ii is the percentage of segments in the song that are \nassigned to the ith instrument class. In our experiments, \nthe classification accuracy was 72%. \n \n \n514\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \n2.2.2 Singing Style \n \nIt is not trivial to describe the singing style in a Chinese \npop song. In our study, singing style is defined with ref-\nerence to a few famous singers of Chinese pop songs. For \na given song, we try to measure the degree of similarity \nbetween the singing voice in the song and the voice of \neach reference singer. Among the 500 training songs, we \nchoose 6 male and 6 female singers with distinct voices \nand styles. Each singer has about 30 training songs. A \nseparate class is established for children’s voice. As a re-\nsult, we have a total of 13 singing style classes. Each class is represented by a GMM of 64 mixtures. The sing-\ning style attribute, F\nsing, is defined as  \n/g94 /g9613 2 1 sing ,..., ,S S S /g143F  (3) \nwhere Si is the percentage of segments in the song that \nare assigned to the ith class. In our experiments, the clas-\nsification accuracy of singing style is 82%. \n2.2.3 Gender \n \nAn explicit gender classifier is built. A male voice and a \nfemale voice model are trained to be 128–mixture GMM.  \nThe gender attribute, Fgend, is defined as: \n/g94 /g96female male gend G G, /g143 F  (4) \nwhere Gmale and G female are the percentage of male and \nfemale voice segments in the song, respectively. The \nsong-level gender classifier has an accuracy of 97%.  \n2.2.4 Tempo \n \nThe tempo detection method proposed in [9] is used to \ngenerate the tempo information we need. The tempo of a \ngiven song is estimated by the Fourier analysis of the \nbeat onset pattern. Complex domain spectral difference is \nused as the detection function for the beat onset. To find \nthe spectral difference, we first estimate the instantaneous \nspectral change /g536(m) at the mth short-time frame, which is \ndefined as \n/g11/g12 /g11 /g12 /g11 /g12/g166 /g16 /g32\nkm m k Y k Y m ˆ /g75 (5) \nwhere Ym(k) is the spectral value (DFT coefficient) at \nframe m and frequency bin k, and /g11/g12k Ymˆ  is the corre-\nsponding value predicted from the immediately preceding frame, i.e. \n/g11/g12 /g11/g12/g11/g12k /g301 j\nm mme k Y k Yˆ\n1ˆ\n/g16/g32 . (6) \nThe expected phase /g11/g12k /g301mˆ is predicted from phase in \nprevious two frames as follows: \n/g11/g12 /g11/g12 /g11/g12 /g11/g12 /g11/g12 k k k k /g301m m m m 2 1 1ˆ\n/g16 /g16 /g16 /g16 /g14 /g32/g77 /g77 /g77  (7) \nwhere  /g307m-1(k) and /g307m-2(k) are the observed phases for frame m-1 and m-2 respectively. Frame size of 100ms \nand frame shift of 10ms are used. The frame shift is much shorter than the 50ms shift as used in the extraction of \nother attributes because of the need to detect fast tempo. \nTo obtain a beat onset pattern with clear and well-defined \npeaks, /g536(m) is subtracted by the moving average thresh-\nold \n/g11/g12m/g75  with window size W = 10, i.e. \n/g11/g12 /g11/g12/g166/g14\n/g16 /g32/g322\n21Wm\nWm iiWm /g75 /g75. (8) \nHalf-wave rectification is then performed on the differ-ence to obtain \n/g11/g12m/g75ˆ, the beat onset value of frame m, i.e. \n/g11/g12 /g11/g12 /g11/g12 /g11/g12 m m HWR m/g75 /g75 /g75 /g16 /g32 ˆ  (9) \nwhere HWR (x) = (x + |x|)/2.  \nTo handle tempo variations over the entire duration of \na song, we divide a song into segments of 12s long, with time shift of 4s. Each of these segments contains 1200 \nframes. Fourier analysis is performed on beat onset pat-\ntern of each segment, and the frequency axis is mapped \ninto tempo values. The analysis result is represented by a \ntempogram, which is a two-dimensional time-tempo rep-\nresentation of the strength of tempo values in local seg-\nments. An example of tempogram is shown in Figure 2. \nThe tempo value is limited to the range of 30 beat per \nminute (bpm) to 300 bpm, which covers most of the pop \nmusic. The tempo value with the strongest impulse is \npicked as the local tempo value for each segment. The \nlocal tempo values of all segments in the song form a dis-\ntribution, from which the tempo attribute is derived as  \n/g94 /g963 2 1, ,T T Ttempo/g32 F  (10)  \nwhere T1, T2, and T3 corresponds to the 25%, 50%, and \n75% percentile tempo values.   \nSegment NumberTempo ValueTempogram\n  \n5 10 15 20 25 30 35 40 45 5060\n80\n100\n120\n140\n160\n180\n200\n220\n 050100150200250300350\n \nFigure 2 .  An illustration of the tempogram \n2.2.5 Degree of Noisiness \n \nThe mood of a song is an important factor to be consid-\nered when computing similarity of songs. The energy of a song is in some way related to the mood of the song. Ac-\ncording to Thayer’s emotion model [8], songs with low \nintensity are usually associated with calm, relax, or de-\n515\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \npressed emotions, while songs with high intensity, are \nassociated with exciting or angry emotion. This is illus-\ntrated in Figure 3. In addition, songs with thick texture, \ni.e., many instruments and voices playing simultaneously, \nare generally associated with more intense and excited \nfeeling. On the other hand, songs with thin texture, i.e., a few instruments or voices, are commonly found in calm \nand relax music. Audio signals in thick-texture music are \nlikely to have flatter spectra, compared to those in the \nthin-texture music. Thus we propose to use both the sig-nal intensity and spectral flatness features to compute a \n“degree of noisiness” (DoN) attribute, which is related to \nthe mood of the song.  \n \n \nFigure 3 .  Thayer’s model of mood \n[8] \n Three values are defined as the DoN label: 0. 0.5, and \n1, which correspond to “ low”, “medium ”, and “ high” re-\nspectively. A low DoN means that the song is perceived \nas quiet and calm, while a high DoN means loud and \nnoisy. We found that in many pop songs, the DoN level \nvaries noticeably between the first and the second half of \nthe song. Therefore, two DoN labels are used to describe \na song, one for the first half of the song and one for the \nsecond half.  \nPrior to the DoN analysis, the audio signal is normal-\nized by its maximum amplitude. Since the beginning and ending of the songs usually contains silence, the first \n10% and the last 10% of the signal in each song are dis-\ncarded. The remaining signal is then divided into two \nhalves as described above. For each frame of 100 ms \nlong, the signal intensity is computed by  \n/g11/g12 /g11/g12/g184\n/g185/g183/g168\n/g169/g167/g32 /g166/g16\n/g321\n02\n101log 10T\ntmt XTm P (11)  \nwhere P(m) is the power of mth frame, Xm is the signal of \nmth frame in time domain, and T is equal to 1600 for \n16KHz sampling and 100ms frame size. From the frame-\nlevel signal intensity values, we compute the mean and \nstandard deviation of the whole segment (half of the \nsong), which are denoted by Mpower and SD power, respec-\ntively. Similarly, we compute the mean and standard de-viation of the spectral flatness over each segment, which \nare denoted by M\nSF and SD SF, respectively. The spectral \nflatness,  SF(m), is defined by /g11/g12/g11/g12\n/g11/g12/g166\n/g32/g32/g150\n/g32K\nkmKmK\nk\nk FKk F\nm SF\n11\n1 (12)  \nwhere | Fm(k)| is the magnitude spectrum of Xm computed \nby 2048-point DFT, K  is equal to 1023. The DC term \n(k=0) is ignored in the computation.  \nEach DoN class is represented by a single-mixture \nGaussian model, which models the four-element feature \nvector { Mpower, SD power, MSF, and SD SF}.  The DoN attrib-\nute vector of a song is defined as: \n/g94 /g962 1,N NDoN/g32 F  (13)  \nwhere N1 and N2 corresponds to the label in the first and \nsecond half of the song, respectively.   \n2.3\n Computation of Similarity Score \nLet A and B denote two songs. The overall similarity be-\ntween A and B is the weighted sum of the similarities computed for the five audio attributes. The similarity \nvalue of each attribute has the range of 0 (most dissimi-\nlar) to 1 (identical). The superscript in the attribute vec-\ntors and elements denotes the song from which the at-\ntribute is computed. For instrument and singing style at-\ntributes, the similarities s\ninst(A,B) and ssing(A,B) are com-\nputed by cosine similarity, i.e.  \n/g11/g12B\ninstA\ninstB\ninstA\ninst\ninst B A s\nF FF F/g120/g32, (14)  \nand \n/g11/g12B AB A\nsing B A s\nsing singsing sing,\nF FF F/g120/g32. (15)  \n \nFor gender, the similarity, sgend(A,B), is defined as: \n/g11/g12/g11 /g12\n/g11/g12B\nmaleA\nmaleB\nmaleA\nmale\ngendG GG GB A s, max, min,/g32  (16)  \nTempo similarity, stempo(A,B) , is computed in a similar \nway as the gender, except that we take the average over \nthe three components in the tempo attribute vector, i.e.,  \n/g11/g12/g11 /g12\n/g11/g12/g166\n/g32/g323\n1 , max, min\n31,\niB\niA\niB\niA\ni\ntempoT TT TB A s  (17)  \nFor the discrete class labels in the DoN attribute vectors, \na normalized form of Euclidean distance is used for \nsDoN(A,B), which is defined as, \n/g11/g12\n21 ,B\nDoNA\nDoN\nDoN B A sF F/g16\n/g16 /g32  (18)  Energy \nLevel \nStress Level Anxious \nAngry Terrified \nSad \nDepressed Bored Excited Happy Pleasure \nCalm \nRelaxed Serene \n516\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \nThe overall similarity score is the weighted sum of the \nsimilarities by all attributes, \n/g11/g12 /g11/g12 /g166\n/g143/g32\nattribute music xx xB A s w B A s\n , ,  (19)  \nwhere wx denotes the weight for attribute x. The weights \nare determined empirically based on the observations from subjective evaluation. \n3.\n SUBJECTIVE EVALUATION \nSubjective evaluation is carried out to obtain a set of ref-erence similarity scores that can be considered as the \nground truth. These similarity scores will be used for two \npurposes: (a) to analyze which attributes are more impor-\ntant to human listeners when comparing two songs, and (b) to determine the optimal weights in the proposed ob-\njective similarity computation.  \nA total of 215 subjects participated in the listening \ntests. 43 Chinese pop songs, each with duration of about \n4 minutes, were selected as the test materials. These \nsongs were not included in the training set of 500 songs. \nThey are chosen in a way that within this limited number of test songs, the varieties of instruments, singers, tempo, \nand mood are maximized. For the test song Q, 10 candi-\ndate songs were manually selected from the remaining songs in the test set so that we have various degrees of \ndifferences in the attributes between the candidates and \nthe query songs. Five subjects were asked to listen to Q \nand the 10 candidate songs, denoted as C\ni, i={1,2,…,10}. \nThe subject was asked to rate the similarity between Q \nand Ci, on the scoring scale as shown in Figure 4.  \nA test session was divided into two parts, in the first \npart, a subject was asked to first listen to song Q, and 5 of \nthe candidate songs. The subject was allowed to repeti-\ntively listen to Q if he/she liked to. There was a short \nbreak after the first part. Then the second part starts by listening to Q  again, and then the remaining 5 candidates. \nThe orders of playing the candidate songs are different from one listener to another. \n4.\n EXPERIMENTAL RESULTS \nWe first investigated on the importance of each music attribute in the proposed similarity measure. This is done \nby assigning the weight of one attribute to be 1 and the \nothers to be zero. For the query song Q in the set of sub-\njective test songs, we compute the objective similarity score of Q and each of the 10 candidates using (19) and \nobtain a similarity vector S\nQ = { s(Q,C1), s(Q,C2), … , \ns(Q,C10)}. From the results of subjective evaluation, we \ncompute the average subjective similarity scores from all subjects that were tested with this set of songs. The re-sulted scores form the subjective similarity vector de-\nnoted as E\nQ={e(Q,C1), e(Q,C2), … , e( Q,C10)}. The \nSpearman’s rank correlation between SQ and EQ, denoted   \nFigure 4 .  Scoring scale \nby /g545Q, is computed for each query song. Subsequently, \nthe overall average of correlation is computed over the 43 \nsongs. The results are shown in Table 1. It is observed \nthat the importance of the attributes in human listening is \nhighly uneven. Instrument and gender are the least rele-vant attributes. The correlation values are close to zero, \nindicating that subjective scores are almost uncorrelated \nto the objective similarity. Singing style and tempo are \nmore important, with a small but positive correlation. \nDoN is the most important attribute with the highest cor-\nrelation between objective similarity and subjective \nscores.  \n \nAttribute Average Correlation \nInstrument 0.07 \nSinging style 0.20 \nGender 0.06 \nTempo 0.24 \nDegree of Noisiness 0.49 \n \nTable 1 . Correlation of each attribute to subjective scores \n \nNext we try to determine a set of optimal weights for \nsimilarity computation in (19) by maximizing the \ncorrelation to subjective listening. Exhaustive search is \nperformed. The attribute weights are varied from 0 to 10 \nwith an increment step size of 1. It was found that the op-\ntimal set of weights are 1, 2, 0, 3, and 4 for instrument, \nsinging style, gender, tempo, and DoN, respectively. The \naverage correlation achieved with this set of weights is \nequal to 0.544. Although it is not a very high correlation, \nthe result shows that these audio attributes are still useful \nin modeling subjective similarity judgment of Chinese \npop songs. The weights are in agreement with our obser-vations on the study of single attribute. \nFigure 5 shows the distribution of the correlation val-\nues for all test songs when the optimal weights are ap-\nplied. Among the 43 songs, 5 songs have negative values of correlation, and 27 have correlations higher than 0.6. \nThis indicates that the proposed objective similarity \nmeasure can fairly model the subjective similarity for \nmost of the songs. \nAs part of the subjective test, each human subject was \nasked to fill in a survey questionnaire. The subject had to very simila r\nsimila r\nslightly similar \ndifferent \nvery different #1 #2\n100 \n80 \n60 \n40 \n20 \n0 \n517\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \nrank from 1 (most important) to 5 (least important) the \nfollowing attributes when he/she considers the similarity \nof a pair of songs: rhythm, accompaniment instrument, singing style, lyrics, and languages. The average ranks of \nthese attributes are tabulated in Table 2. The results \nmatch with what we found from the study of correlation \nbetween objective and subjective scores. Tempo and \nDoN, which somehow affect the perception of the rhythm, \nare the most important factors. Instrument and singing \nstyle are more or less the same in importance. Lyrics and languages (Cantonese or Mandarin) are the least impor-\ntant factors. At the current stage, we only use attributes \nlike tempo and DoN to model the rhythm. More complex \nattributes such as melody and chord information should \ngive us a better model for the rhythm and obtain an ob-\njective similarity metric that better correlates with subjec-\ntive scores. One interesting note is that some subject sug-gests that attributes like atmosphere of the song, style of \nsongs in different generations, and harmony of the songs \nmay also be important. However these attributes are more \nabstract and difficult to extract from low-level audio fea-\ntures. Metadata such as publication year, genre, etc. may \nbe used in future system to better describe these attributes. \n \n \nFigure 5 .  Histogram of correlations of 43 songs \n \nAttributes Average rank \nRhythm 1.3 \nInstrument 2.7 \nSinging Style 2.9 \nLyrics 4.1 \nLanguage 4.4 \n \nTable 2 . Average rank of attributes from survey \n5. CONCLUSIONS \nIn this article we proposed five audio signal attributes that can be used to generate an attribute vector for a song \nin the Chinese pop song genre. The attribute vectors can then be used to compute similarity between songs, which \nis a fundamental process in content-based music recom-\nmendation system. We found that among these attributes, \ntempo and degree of noisiness play the most important \nrole in approximating the subjective scores, followed by \nsinging style and instrument. The results also indicate \nthat rhythmic and mood information is crucial in objec-tive similarity computation.  6.\n ACKNOWLEDGEMENTS \nThis work was supported by the Innovation and Technol-ogy Commission of the HKSAR Government. \n7.\n REFERENCES \n[1] Y. Xia, L. Wang, and K.-F. Wong: “Sentiment Vector Space Model for Lyric-Based Song \nSentiment Classification,” International Journal of \nComputer Processing Of Languages, Vol. 21, No. 4, pp. 309-330, Dec. 2008. \n[2]\n X. Zhu, Y.-Y. Shi, H.-G. Kim, K.-W. Eom: \"An integrated music recommendation system,\" IEEE \nTransactions on Consumer Electronics,Vol.52, No.3, \npp.917-925, Aug. 2006. \n[3]\n T. Yoon, S. Lee, K.H. Yoon, D. Kim, J.-H. Lee: \"A \npersonalized music recommendation system with a \ntime-weighted clustering,\" Proceedings of the 4th \nInternational IEEE Conference Intelligent Systems, \nVol.2, pp.10-48-10-52, 6-8 Sept. 2008. \n[4] K. Yoshii, M. Goto, K. Komatani, T. Ogata, H.G. \nOkuno: \"An Efficient Hybrid Music Recommender \nSystem Using an Incrementally Trainable \nProbabilistic Generative Model,\" IEEE Transactions \non Audio, Speech, and Language Processing, Vol.16, \nNo.2, pp.435-447, Feb. 2008. \n[5] W. Cohen and W. Fan: “Web-collaborative filtering: \nRecommending music by crawling the web,” \nComputer Networks, Vol. 33, No. 1–6, pp. 685–698, \n2000. \n[6] JJ Aucouturier, F Pachet, \"Music similarity \nmeasures: What's the use,\" Proceedings of the \nISMIR, 2002. \n[7] RWC database:  available at: http://staff.aist.go.jp/m.goto/RWC-MDB/  \n[8]\n R.E. Thayer: The Biopsychology of Mood and \nArousal , Oxford University Press, 1989. \n[9] P. Grosche and M. Muller: “A Mid-level \nRepresentation for Capturing Dominant Tempo and \nPulse Information in Music Recordings,” Proceedings of the International Society for Music \nInformation Retrieval Conference (ISMIR 2009), pp. \n189-194, 2009. \n[10]\n A. Berenzweig, B. Logan, D.P.W. Ellis, and B. Whitman: \"A Large-Scale Evaluation of Acoustic \nand Subjective Music-Similarity Measures,\" \nComputer Music Journal, Vol. 28, No. 2, pp. 63-76, 2004. \n[11]\n Last.fm music radio website:  \nhttp://www.last.fm/home \n518\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "Learning Tags that Vary Within a Song.",
        "author": [
            "Michael I. Mandel",
            "Douglas Eck",
            "Yoshua Bengio"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1416130",
        "url": "https://doi.org/10.5281/zenodo.1416130",
        "ee": "https://zenodo.org/records/1416130/files/MandelEB10.pdf",
        "abstract": "This paper examines the relationship between human gener- ated tags describing different parts of the same song. These tags were collected using Amazon’s Mechanical Turk ser- vice. We find that the agreement between different people’s tags decreases as the distance between the parts of a song that they heard increases. To model these tags and these relationships, we describe a conditional restricted Boltz- mann machine. Using this model to fill in tags that should probably be present given a context of other tags, we train automatic tag classifiers (autotaggers) that outperform those trained on the original data.",
        "zenodo_id": 1416130,
        "dblp_key": "conf/ismir/MandelEB10",
        "keywords": [
            "Human-generated tags",
            "song parts",
            "Amazon Mechanical Turk",
            "agreement between tags",
            "distance between parts",
            "conditional restricted Boltzmann machine",
            "tag relationships",
            "fill in tags",
            "automatic tag classifiers",
            "outperform original data"
        ],
        "content": "LEARNING TAGS THAT VARY WITHIN A SONG\nMichael I Mandel, Douglas Eck, Yoshua Bengio\nLISA Lab, Universit ´e de Montr ´eal\nfmandelm,eckdougg@iro.umontreal.ca, yoshua.bengio@umontreal.ca\nABSTRACT\nThis paper examines the relationship between human gener-\nated tags describing different parts of the same song. These\ntags were collected using Amazon’s Mechanical Turk ser-\nvice. We ﬁnd that the agreement between different people’s\ntags decreases as the distance between the parts of a song\nthat they heard increases. To model these tags and these\nrelationships, we describe a conditional restricted Boltz-\nmann machine. Using this model to ﬁll in tags that should\nprobably be present given a context of other tags, we train\nautomatic tag classiﬁers (autotaggers) that outperform those\ntrained on the original data.\n1. INTRODUCTION\nSocial tags are short free-form descriptions of music that\nusers apply to songs, albums, and artists. They have proven\nto be a popular way for users to organize and discover music\nin large collections [5]. There remain, however, millions\nof tracks that have never been tagged by a user that cannot\nbe included in these systems. Automatic tagging, based on\nan analysis of the audio of these tracks and user tagging\nbehavior, could enable them to be included in these systems\nimmediately. To this end, this paper explores the relation-\nship between audio and the tags that humans apply to it,\nespecially at different time scales and at different points\nwithin the same track.\nWe perform this examination in the context of a “Human\nIntelligence Task” (HIT) on the Mechanical Turk website1,\nwhere users are paid small amounts of money to perform\ntasks for which human intelligence is required. Mechan-\nical Turk has been used extensively in natural language\nprocessing [10] and vision [11, 13], but to our knowledge\nhas not been used in music information retrieval before.\nMechanical Turk is one means to the end of human compu-\ntation, the ﬁeld of cleverly harnessing human intelligence to\nsolve computational problems. This ﬁeld has been growing\nin popularity recently, especially in the context of games\nfor collecting descriptions of music [6, 7, 12]. While these\n1http://mturk.com\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2010 International Society for Music Information Retrieval.games have proven popular among researchers for collect-\ning these data, they require signiﬁcant investment of devel-\nopment time and effort in order to attract and retain players.\nBy using Mechanical Turk, a researcher can trade a little\nextra money for signiﬁcant savings in development time.\nThis paper makes three contributions. First, in Section 2\nwe discuss data collection and analysis from a new source,\nMechanical Turk, and Section 2.1 shows that clips from\ndifferent parts of the same song tend to be described dif-\nferently from one another. Second, Section 3.1 presents\na probabilistic model of tags and their relationships with\neach other to combat the sparsity of music tagging data.\nSection 3.3 shows that explicitly including information link-\ning tags from the same user, track, and clip improves the\nlikelihood of held out data under the model. Finally, we use\nthis model to “smooth” tag data, i.e. to infer tags that were\nnot provided, but perhaps should have been, given the tags\nthat were. Section 4 shows that these smoothed tags are\nmore “learnable” from the audio signal than the raw tags\nprovided directly by the users, especially when fewer users\nhave seen a given clip.\n2. DATA COLLECTION\nUsers of the Mechanical Turk website, known as “turk-\ners”, were asked to listen to a clip from a song and de-\nscribe its unique characteristics using between 5 and 15\nwords. The task was free response, but to provide some\nguidance, we requested tags in 5 categories: Styles/Genres,\nV ocals/Instruments, Overall sound/feel (global qualities like\nproduction and rhythm), Moods/Emotions, Other (sounds\nalike artists, era, locale, song section, audience, activities,\netc.). In order to avoid biasing the turkers’ responses, no\nexamples of tags in each category were provided. Turkers\nwere paid between $0.03 and $0.05 per clip, on which they\ngenerally spent about one minute.\nThe music used in the experiment was collected from\nmusic blogs that are indexed by the Hype Machine2. We\ndownloaded the front page of each of the approximately\n2000 blogs and recorded the URLs of any mp3 ﬁles linked\nfrom them, a total of approximately 17,000 mp3s. We\ndownloaded 1500 of these mp3s at random, of which ap-\nproximately 700 were available, error free, and at least 128\nkbps while still being below 10 megabytes (to avoid DJ sets,\npodcasts, etc). Of these, we selected 185 at random.\nFrom each of these 185 tracks, we extracted ﬁve 10-\nsecond clips evenly spaced throughout the track. We pre-\n2http://hypem.com/list\n399\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)User Track Clip Tags Num pairs\n+ + + 6.0370\u00060.0290 2,566\n+ +\u0000 2.3797\u00060.0511 690\n+\u0000 \u0000 1.2006\u00060.0026 227,006\n\u0000 + + 1.1137\u00060.0142 4,838\n\u0000 +\u0000 1.0022\u00060.0083 13,560\n\u0000 \u0000 \u0000 0.5240\u00060.0004 3,702,481\nTable 1 . Average number of tags ( \u00061 standard error)\nshared by HITs with various characteristics in common and\nnumber of such pairs of HITs. A +indicates that the clips\nshared that characteristic, a \u0000that they differed in it.\nsented these clips to turkers in a random order, and gener-\nally multiple clips from the same track were not available\nsimultaneously. Each clip was seen by 3 different turkers.\nMechanical Turk gives the “requester” the opportunity\nto accept or reject completed HITs either manually or au-\ntomatically. In order to avoid spammers, we designed a\nnumber of rules for automatically rejecting HITs based on\nanalyses of each and all of a user’s HITs. Individual HITs\nwere rejected if: (1) they had fewer than 5 tags, (2) a tag\nhad more than 25 characters, or (3) less than half of the tags\nwere found in a dictionary of Last.fm tags. All of a users’\nHITs were rejected if: (1) that user had a very small vocabu-\nlary compared to the number of HITs they performed (fewer\nthan 1 unique tag per HIT), (2) they used any tag too fre-\nquently (4 tags were used in more than half of their HITs),\n(3) they used more than 15% “stop words” like nice,music ,\ngenre , etc., or (4) at least half of their HITs were rejected\nfor other reasons. The list of stop words was assembled by\nhand from HITs that were deemed to be spam.\nWe pre-processed the data by transforming tags into a\ncanonical form. We normalized the spelling of decades and\nthe word “and”, removed words like “sounds like” from the\nbeginning of tags, removed words like “music”, “sound”,\nand “feel” from the ends of tags, and removed punctuation.\nWe also stemmed each word in the tag so that different forms\nof the same word would match each other, e.g. drums ,\ndrum, and drumming.\nWe posted a total of 925 clips, each of which was to be\nseen by 3 turkers for a total of 2775 HITs. We accepted\n2566 completed HITs and rejected 305 HITs. Some of\nthe rejected HITs were re-posted and others were never\ncompleted. The completed HITs included 15,500 (user, tag,\nclip) triples from 209 unique turkers who provided 2100\nunique tags. Of these tags, 113 were used by at least 10\nturkers, making up 13,000 of the (user, tag, clip) triples.\nWe paid approximately $100 for these data, although this\nnumber doesn’t include additional rounds of data collection\nand questionnaire tuning.\n2.1 Co-occurrence analysis\nThe ﬁrst analysis that can be applied to these data is a simple\ncounting of the number of tags shared by pairs of HITs. By\ncategorizing the relationships between two HITs in terms of\nthe users, tracks, and clips involved, an interesting picture\n60\n 40\n 20\n 0 20 40 60\nSeparation (% of track)0.400.450.500.550.600.65Co-occurring tags above baselineFigure 1 . Average number of tags above the baseline shared\nby HITs from the same track as a function of the separation\nbetween the clips measured as % of a track.\nemerges. Table 1 shows the ﬁrst analysis of the number of\nshared tags for all possible pairs of HITs grouped by the\nrelationships of these characteristics.\nThe bottom row of the table shows that HITs with noth-\ning in common still share 0.5240 tags on average because\nof the distribution of tags and music in this dataset. The\nsecond line from the bottom shows that HITs involving dif-\nferent users and different clips within the same track share\n1.002 tags on average. And the third to last row shows that\nHITs with different users, but the same clip share 1.11 tags\non average, signiﬁcantly more than HITs that only share\nthe same track. This same pattern also holds for HITs from\nthe same user, but with higher co-occurrences. The large\ndifference between HITs from the same user and HITs from\ndifferent users can probably be attributed to the lack of\nfeedback to the users in the task, allowing somewhat id-\niosyncratic vocabularies to perpetuate. Note that the top\nrow of the table shows the average number of tags per HIT.\nA related analysis can be performed measuring the de-\npendence of tag co-occurrence on the distance between\nclips in the same track. Figure 1 shows the average tag co-\noccurrence of two clips in the same track above the baseline\nlevel of co-occurrences for two clips from different tracks.\nIt reveals that the number of tags shared by clips decreases\nas the clips get farther apart. The error bars show that this\nresult is not quite statistically signiﬁcant, but it is still a no-\ntable trend. Results are similar for HITs from the same user\nand for cosine similarity instead of plain co-occurrence.\n3. DATA MODELING\nWhile stemming can make connections between certain\ntags in the dataset, it is only able to do this for tags which\nare syntactically related to one another. Another kind of\nmodel is required to capture relationships between tags like\nindie androck . We choose to capture these relationships\nusing a restricted Boltzmann machine (RBM), a generative\n400\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)probabilistic model. The RBM observes binary vectors\nrepresenting the tags that a single user gave to a single clip.\nOnce trained, the model can compare the relative likelihood\nof two such observations and can draw samples from the\nobservation distribution.\n3.1 Conditional restricted Boltzmann machine\nMore formally, an RBM [9] is a probabilistic model of the\nrelationship between binary visible units, denoted, viand\nbinary hidden units, denoted hj. Conditioned on the visible\nunits, the hidden units are independent of one another, and\nvice-versa. The joint probability density function is\np(v;h ) =1\nZexp\u0000\nvTWh+bTv+cTh\u0001\n(1)\nwhere the partition function Z\u0011P\nv;hp(v;h )is compu-\ntationally intractable (exponential either in the number of\nvisibles or of hiddens). The likelihood of the observation\nvis obtained by marginalizing over h:p(v) =P\nhp(v;h ),\nand can be computed easily up to Z. In this paper, we\ncondition the model on “auxiliary” hidden units, a,\np(v;hja) =1\nZexp\u0000\nvTWh+vTWaa+bTv+cTh\u0001\n(2)\nwhere the partition function is now conditioned on aas\nwell,Z=P\nv;hp(v;hja). Conditional RBMs have been\nused for collaborative ﬁltering [8], although in that case the\nconditioning variables inﬂuenced the hidden states, whereas\nin our model they directly inﬂuence the visible units. The\nmatricesWandWaand the bias vectors candbare learned\nusing the contrastive divergence algorithm [4]. In addition\nto the normal contrastive divergence updates, we place an\nL1penalty onWato promote sparseness of its entries.\nIn practice, the vector ais set a priori to represent the\nuser, the artist, the track, and/or the clip using a so-called\none hot representation. For example, each user has their\nown column of the Wamatrix, providing a different bias\nto the tag probabilities. We sometimes refer to the quantity\nWaaas the auxiliary biases for this reason. Each user in\neffect has a different baseline probability for the visible\nunits, meaning that they tend to use the tags in different\nproportions. Because the entries of the Wamatrix areL1-\npenalized, the user columns tend to represent discrepancies\nbetween a user’s tags and the global average, which is cap-\ntured in the bias vector b. Thus theWamatrix is like a term\nfrequency-inverse document frequency (TF-IDF) represen-\ntation (see e.g. [14]) of the variables that it is modeling, but\nlearned in a more probabilistically grounded way.\n3.2 Purely textual datasets\nWe apply this model to three different tag datasets with the\ngoal of discovering relationships between tags, and the tags\nthat are used unexpectedly frequently or infrequently on\nparticular items. The ﬁrst dataset is purely textual, from\nLast.fm [1]. It includes (artist, tag) pairs, along with the\nnumber of times that that pair appears. The second dataset,\nfrom MajorMiner [7], includes (clip, user, tag) triples andalso includes the audio associated with each clip. The third\ndataset, from the Mechanical Turk experiments described\nin Section 2, similarly includes (clip, user, tag) triples and\naudio. While it is smaller than the MajorMiner data, it\nincludes many more clips per track, and so can provide per-\nhaps more insight into clip-level and track-level modeling.\nThe dataset from [1] was collected from Last.fm in the\nspring of 2007. It includes the tags that users applied to\napproximately 21,000 unique artists and the number of\nusers who applied each tag to each artist. There are ap-\nproximately 100,000 unique tags, and 7.2 million (artist,\ntag) pairs, including duplicates. To reduce the size of the\nrequired model, we discarded tags that had been applied\nto fewer than 8000 artists (98 tags), and only kept the 200\nmost frequently tagged artists.\nIn order to transform this dataset into a form that can be\nused by the RBM model, we simulated taggings from indi-\nvidual users. We characterized each artist with independent\nBernoulli probabilities over each tag and drew multi-tag\nsamples from this distribution. The probability of each tag\nwas proportional to the number of times each tag was ap-\nplied to an artist, so the counts were ﬁrst normalized to sum\nto 1. These normalized counts were multiplied by 5 (and\ntruncated to prevent probabilities greater than 1) so that the\nexpected total number of tags was 5, a number that a typical\nuser might provide. To create the dataset, we repeatedly\ndrew an artist at random and simulated a user’s tagging of\nthat artist. The artists’ tag probabilities provided a baseline\nagainst which to measure the estimation of the relevant Wa\ncolumns, which only modeled artist auxiliary information.\nThe dataset from [7] was collected from the MajorMiner\nmusic labeling game over the course of the last three years.\nIt includes approximately 80,000 (clip, user, tag) triples\nwith 2600 unique clips, 650 unique users, and 1000 unique\ntags. Each observation was encoded as a binary vector\nindicating the tags that a single user applied to a single clip.\nTheavector in this case indicated both the clip, the track\nthat it came from, and the user. On average, each track was\nrepresented by fewer than two clips.\nFinally, this new Mechanical Turk dataset provides (clip,\nuser, tag) triples along with relationships between clips and\ntracks. While it contains the fewest triples, it contains the\nmost structure of the datasets because by design there are\nﬁve clips per track. To model it, the avector represents the\nuser, the track, and the clip, so there is a separate auxiliary\nterm learned for each of them.\n3.3 Textual experiments\nQualitative experiments on the Last.fm dataset showed that\nour model successfully learned the auxiliary inputs, i.e.\ntheWamatrix acted as a sort of TF-IDF model for tags.\nSpeciﬁcally, the Wmatrix modeled relationships between\npairs of tags, the bvector modeled overall popularity of\nindividual tags, and the columns of Wamodeled any tags\nthat were unusually prevalent or absent for an artist given\nits other tags. For example, Nirvana’s Wacolumn included\na large value for grunge , and the Red Hot Chili Peppers’\nincluded a large value for funk , both of which might not\n401\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)have been expected from their other tags like rock and\nalternative . Similarly, the Beatles have a negative bias for\nseen live because presumably fewer Last.fm listeners have\nseen the Beatles live than other artists tagged rock andpop.\nThese issues are addressed more quantitatively below.\nAll three of the datasets described in Section 3.2 can be\nused in a leave-one-out tagprediction task. In this task,\nthe relative probability of a novel observation is compared\nto that of the same observation with one bit ﬂipped (one\ntag added or deleted). If the model has captured important\nstructure in the data, then it will judge the true observation\nto be more likely than the bit-ﬂipped version of it. This ratio\nis directly connected to the so-called pseudo-likelihood of\nthe test set [2]. Because it is a ratio of probabilities, it does\nnot require the computation of the partition function, Z,\nwhich is very computationally intensive. Mathematically,\nthe pseudo-likelihood is deﬁned as\nPL(vja)\u0011Y\nip(vijvni;a) =Y\nip(vja)\np(vja) +p(~vija)\n(3)\nwhereviis theith visible unit, vniis all of the visible units\nexcept for the ith unit, and ~viis the observation vwith the\nith bit ﬂipped. Even though our observation vectors are\ngenerally very sparse ( \u00184% of the bits were 1s), the 1s are\nmore important than the 0s, so we compute the average log\npseudo-likelihood over the 1s and 0s separately and then\naverage those two numbers together. This provides a better\nindication of whether the model can properly account for\nthe tags that are present, than the tags that aren’t present.\nThis leave-one-out tag prediction can be done with any\nmodel that computes the likelihood of tags. Thus we can\ntrain models with different combinations of auxiliary vari-\nables, or different models entirely, as long as they can pre-\ndict the likelihood of novel data. A baseline comparison\nto all of our RBMs is a factored model that estimates the\nprobability of each tag independently from training data\nand then measures the likelihood of each tag independently\non test data. Because of the independence of the variables,\nin this case the pseudo-likelihood is identical to the true\nlikelihood.\nWe performed this experiment with the textual compo-\nnent of these three datasets, dividing the data 60-20-20 into\ntraining, validation, and test sets. The observations were\nshufﬂed, but then rearranged slightly to ensure that all of the\nauxiliary classes appeared at least once in the training set to\navoid “out-of-vocabulary” problems. We ran a grid search\nover the number of hidden units, the learning rate, and the\nregularization coefﬁcients using only the track-based aux-\niliary variables, those with the most even coverage. This\ngrid search involved training approximately 500 different\nmodels, each taking 10 minutes on average. We selected the\nsystem with the best hyperparameters based on the pseudo-\nlikelihood of the validation dataset. Once we had selected\nreasonable hyperparameters, we ran experiments using all\ncombinations of the auxiliary variables with the other hyper-\nparameters held constant. Five different random divisions\nof the data allowed the computation of standard errors.\nThe log pseudo-likelihoods of the test datasets underAuxiliary info\nDataset User Track Item log(PL)\u0006stderr\nMajorMiner + + + \u00000:9179\u00060:0088\nMajorMiner + +\u0000 \u0000 0:9189\u00060:0070\nMajorMiner +\u0000 \u0000 \u0000 0:9416\u00060:0074\nMajorMiner\u0000 \u0000 \u0000 \u00001:0431\u00060:0095\nMajorMiner baseline \u00001:4029\u00060:0024\nMech. Turk + +\u0000 \u0000 0:893\u00060:015\nMech. Turk +\u0000 \u0000 \u0000 0:904\u00060:013\nMech. Turk + + + \u00000:914\u00060:012\nMech. Turk\u0000 \u0000 \u0000 \u00001:039 \u00060:013\nMech. Turk baseline \u00001:300\u00060:007\nLast.fm\u0000 \u0000 +\u00000:5623\u00060:0042\nLast.fm\u0000 \u0000 \u0000 \u00000:7082\u00060:0029\nLast.fm baseline\u00001:1825\u00060:0018\nTable 2 . Average per-bit log pseudo-likelihood (less neg-\native is better) for restricted Boltzmann machines condi-\ntioned on different types of auxiliary information. A +\nindicates that the auxiliary information was present, a \u0000in-\ndicates that it was absent. The baseline system is a factored\nmodel evaluated in the same way.\nthese systems are shown in Table 2. The results are not\nstrictly comparable across datasets because they involved\nslightly different numbers of visible units. The results are\nshown on a per-bit basis, however, to facilitate compari-\nson. These results show ﬁrst that non-conditional restricted\nBoltzmann machines (rows with three \u0000s) are much more\neffective than the factored models at modeling test data.\nThis is because in addition to modeling the relative frequen-\ncies of tags, the RBM also models the relationships between\ntags through its hidden units. Conditioning the RBM on\nauxiliary information (rows with at least one +) further\nimproves the pseudo-likelihoods. Speciﬁcally, it seems that\nthe most useful auxiliary variable is the identity of the user,\nbut the identity of the track helps as well. Including clip\ninformation is slightly detrimental, although not statistically\nsigniﬁcantly so, possibly because it introduces a large num-\nber of extra parameters to estimate in the Wamatrix from\nfew observations.\n4. AUTOTAGGING EXPERIMENTS\nThe ﬁnal set of experiments involves not just the textual\ntags, but also the audio for both the MajorMiner dataset\nand this new data collected from Mechanical Turk. In this\nexperiment, we measure the usefulness of the RBM model\nfrom Section 3.1 for “smoothing” the tag data. Speciﬁcally,\nwe create two datasets: the ﬁrst, labeled “raw”, consists of\njust the original (clip, user, tag) triples in the dataset. The\nsecond, labeled “smoothed”, consists of labels imputed by\nthe RBM trained with all of the available auxiliary informa-\ntion. For each clip, we drew 1000 samples from the RBM\nconditioned on that sample’s auxiliary information, but with\nno user indicated. We factored out the user so the taggers\nwere trained from a general point of view, not that of any\n402\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Mechanical Turk\nTested\nTrained Raw Smoothed\nRaw 56.87 \u00060.52 56.56\u00060.36\nSmoothed 61.43 \u00060.51 63.40\u00060.35MajorMiner\nTested\nTrained Raw Smoothed\nRaw 65.97 \u00060.49 60.58\u00060.35\nSmoothed 66.67 \u00060.49 63.09\u00060.35\nTable 3 . Average classiﬁcation accuracy and standard errors of autotaggers trained and tested on different tag labelings for\nMechanical Turk and MajorMiner data. The tags were either raw or smoothed from RBM samples.\nparticular user. Because the model assumes the effects of\nuser, track, and clip are additive on the tag probabilities,\nthe effect of one can be factored out by not adding it. This\nis further ensured by the regularization of the Wamatrix,\nwhich forces many of the elements of the matrix to 0 and\nthe rest to be small.\nTo compare these datasets, we hold the acoustic fea-\ntures constant, but change the labels used to train and test\nclassiﬁers. We ﬁrst split the data into 5 cross-validation\nfolds. Then the positive and negative test examples for a\nparticular tag are the top- and bottom-ranked clips from\none cross-validation fold. The training examples are the\ntop- and bottom-ranked clips excluding that fold. Because\nthe cross-validation breakdowns are preserved across tag\nsets, it is possible to train on one tag set and test on another.\nFor the smoothed dataset, we select the top and bottom 100\nexamples for each tag. For the raw counts, we choose for\neach tag the smaller of the top 100 examples or all of the\nexamples veriﬁed by at least 2 people.\nThe autotaggers are inspired by those from [7], which\nuse timbral and rhythmic features and a support vector\nmachine (SVM) classiﬁer. For this experiment we use Lib-\nSVM’s\u0017-SVM as our SVM implementation, with probabil-\nity estimates and a linear kernel [3]. Performance with the\nGaussian kernel was similar. One binary SVM is trained\nper tag using a balanced number of positive and negative\nexamples selected in order of tag afﬁnity in the training set.\nPerformance is measured in terms of average accuracy on a\ntest dataset that is balanced in terms of positive and negative\nexamples to set a constant baseline of 50% for a randomly\nguessing classiﬁer. This metric is more appropriate than\noverall classiﬁcation accuracy for tasks like autotagging\nwhere it is important to recognize positive examples in the\npresence of a large number of negative examples. To avoid\nthe “album effect”, the cross-validation folds were assigned\nso that clips from the same track were in the same fold\nin the Mechanical Turk data and that clips from the same\nalbum were in the same fold in the MajorMiner data.\nThe results of these experiments are shown in Table 3\nand Figure 2. Each row of the tables represents a training\ntag labeling and each column represents a test tag labeling.\nThe tables show these accuracies averaged over the 95 tags\nused by the most people on each dataset. The ﬁrst column\nof each table shows the result of training on different tag\nlabelings and testing on the raw tags. For both the Ma-\njorMiner and Mechanical Turk datasets, smoothing with the\nRBM improves test performance on the raw, user-supplied\ntags, although for the MajorMiner dataset, this differenceis not statistically signiﬁcant. The second column of each\ntable indicates the performance of both models in predicting\nthe smoothed data. In this case as well, the smoothed data\ntrains more accurate models.\nThe diagonals of these tables show the “learnability” of\nthe tag labelings. For the Mechanical Turk dataset, the\nsmoothed tag set is more learnable than the raw tags. For\nthe MajorMiner dataset, however, the raw tags are more\nlearnable than the smoothed tags. These accuracies may\nnot be directly comparable, however, because the measure-\nments differ in both the models used and the test data. The\ndifference in accuracy might indicate that the smoothing is\nless necessary in the MajorMiner dataset due to its larger\nsize and larger number of repeated (clip, tag) pairs.\nFigure 2 shows the autotag classiﬁcation accuracy on\nthe raw tags when trained with the raw and smoothed tags.\nThe tags shown are the 50 used by the most people, and\nare sorted in the plots by the performance of the best sys-\ntem, that trained on the smoothed tags. For the Mechanical\nTurk data, shown in Figure 2(a), these smoothed tags train\nbetter classiﬁers almost across the board. Certain tags per-\nform slightly better when trained on the raw data, but not\nsigniﬁcantly so. Smoothing is particularly useful for train-\ningangry ,violin , and country , where autotaggers trained\nfrom the raw tags perform at chance levels.\nFor the MajorMiner data, shown in Figure 2(b), the\nsmoothed tags and the raw tags perform similarly to one an-\nother. The smoothed tags train better autotaggers for club,\nfolk,pop, and funk , while the raw tags train better auto-\ntaggers for silence ,strings ,country , and acoustic . The\noccurrence of the silence tag was due to the inclusion of a\nfew broken clips in the game, which makes it a very speciﬁc,\ncontext-dependent tag that the RBM might not be able to\ngeneralize. It is not clear why performance on country is\nso different between the two datasets. It could be because in\nthe Mechanical Turk dataset the top co-occurring tags with\ncountry areguitar 61% of the time and folk 27%, while in\nMajorMiner, they are guitar 44% of the time, female 27%,\nandmale 26%. Thus in Mechanical Turk smoothing gives\nbetter results for country because it occurs more frequently\nwithguitar and occurs with the more informative tag folk.\n5. CONCLUSION\nThis paper has discussed the relationships between tags and\nmusic at a sub-track scale. We found that Mechanical Turk\nwas a viable means of collecting ground truth tag data from\nhumans, although the lack of the immediate feedback of a\ngame might have contributed to lower inter-user agreement.\n403\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)0.4 0.5 0.6 0.7 0.8 0.9harmonicaclassicrockchristmasalternativeduetloveandhappyindiemalefunkjazzbanjotrancebasspianofemalesadupbeatinstrumentalsoftfastpopcountrymetal80sloudrelaxedfemalevocalsslowelectricguitarclassicmalevocalsbluesguitarviolinrocksynthesizerfolkclubpartyacousticguitaracousticangryrapelectronicadancediscotechnohiphop\nSmoothed\nRaw\n0.4 0.5 0.6 0.7 0.8 0.9 1.0soloorganinstrumentskeyboardindiedrumfastbritishhornbassacoustictrumpetcountryvoicevocalelectronicendpunkmaleelectronicastringsguitarsample80ssynthrepetitivenoisedistortedfemaledrummachinesaxophonefunkpoppianohouseslowsofttechnobeatambientdancejazzrocksilencefolkloudclubquietraphiphop\nSmoothed\nRaw(a) Mechanical Turk (b) MajorMiner\nFigure 2 . Accuracy of autotaggers for the top 50 tags in the Mechanical Turk and MajorMiner datasets. The autotaggers\nwere trained on raw and smoothed tags and tested on the raw, human generated tags. Error bars show 1 standard error.\nWe also found that different parts of the same song tend\nto be described differently, especially as they get farther\nfrom one another. By modeling these differences with a\nconditional restricted Boltzmann machine, we were able\nto recover false negative tags in the user-generated data\nand use these data to more effectively train autotaggers,\nespecially in smaller datasets. In the future we will ex-\nplore additional models of tag-tag similarity, joint tag-audio\nmodels, and models of tagging that take into account the\nrelationships between clips’ different distances from one\nanother.\nAcknowledgements The authors acknowledge the support\nof an NSERC Discovery grant and would like to thank\nRazvan Pascanu and Johanna Devaney for their assistance.\n6. REFERENCES\n[1] T. Bertin-Mahieux, D. Eck, F. Maillet, and P. Lamere. Autotagger: A\nmodel for predicting social tags from acoustic features on large music\ndatabases. J. New Music Res., 37(2):115–135, 2008.\n[2]J. Besag. Statistical analysis of non-lattice data. The Statistician,\n24(3):179–195, 1975.\n[3]C. Chang and C. Lin. LIBSVM: a library for support vector machines,\n2001. Software available at http://www.csie.ntu.edu.tw/\n˜cjlin/libsvm.\n[4]G. Hinton. Training products of experts by minimizing contrastive\ndivergence. Neural Computation, 14:1771–1800, 2002.[5]P. Lamere. Social tagging and music information retrieval. J. New\nMusic Res., 37(2):101–114, 2008.\n[6]E. Law, K. West, M. I. Mandel, M. Bay, and J. S. Downie. Evaluation\nof algorithms using games: the case of music annotation. In Proc.\nISMIR, pages 387–392, 2009.\n[7]M. I. Mandel and D. P. W. Ellis. A web-based game for collecting\nmusic metadata. J. New Music Res., 37(2):151–165, 2008.\n[8]R. Salakhutdinov, A. Mnih, and G. Hinton. Restricted Boltzmann\nmachines for collaborative ﬁltering. In Proc. ICML, pages 791–798,\n2007.\n[9]P. Smolensky. Information processing in dynamical systems: founda-\ntions of harmony theory. MIT Press, 1986.\n[10] R. Snow, B. O’Connor, D. Jurafsky, and A. Ng. Cheap and fast – but is\nit good? evaluating non-expert annotations for natural language tasks.\nInProc. Empirical Methods in NLP, pages 254–263, 2008.\n[11] A. Sorokin and D. Forsyth. Utility data annotation with amazon me-\nchanical turk. In CVPR Workshops, pages 1–8, 2008.\n[12] D. Turnbull, L. Barrington, and G. Lanckriet. Five approaches to\ncollecting tags for music. In Proc. ISMIR, pages 225–230, 2008.\n[13] J. Whitehill, P. Ruvolo, T. Wu, J. Bergsma, and J. Movellan. Whose\nvote should count more: Optimal integration of labels from labelers of\nunknown expertise. In NIPS 22, pages 2035–2043, 2009.\n[14] J. Zobel and A. Moffat. Exploring the similarity space. SIGIR Forum,\n32(1):18–34, 1998.\n404\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "It&apos;s Time for a Song - Transcribing Recordings of Bell-playing Clocks.",
        "author": [
            "Matija Marolt",
            "Marieke Lefeber"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1416704",
        "url": "https://doi.org/10.5281/zenodo.1416704",
        "ee": "https://zenodo.org/records/1416704/files/MaroltL10.pdf",
        "abstract": "The paper presents an algorithm for automatic transcrip- tion of recordings of bell-playing clocks. Bell-playing clocks are clocks containing a hidden bell-playing me- chanism that is periodically activated to play a melody. Clocks from the eighteenth century give us unique insight into the musical taste of their owners, so we are interested in studying their repertoire and performances - thus the need for automatic transcription. In the paper, we first present an analysis of acoustical properties of bells found in bell-playing clocks. We propose a model that describes positions of bell partials and an algorithm that discovers the number of bells and positions of their partials in a given recording. To transcribe a recording, we developed a probabilistic method that maximizes the joint probabili- ty of a note sequence given the recording and positions of bell partials. Finally, we evaluate our algorithms on a set of recordings of bell-playing clocks.",
        "zenodo_id": 1416704,
        "dblp_key": "conf/ismir/MaroltL10",
        "keywords": [
            "automatic transcription",
            "bell-playing clocks",
            "hidden bell-playing mechanism",
            "melody",
            "eighteenth century",
            "unique insight",
            "musical taste",
            "repertoire",
            "performances",
            "acoustical properties"
        ],
        "content": "IT’S TIME FOR A SONG –  TRANSCRIBING  \nRECORDINGS OF BELL-PLAYING CLOCKS \nMatija Marolt  Marieke Lefeber  \nUniversity of Ljubljana  \nFaculty of Computer and Information Science  \nmatija.marolt@fri.uni -lj.si  \nMeertens Instituut  and Museum Speelklok  \nmarieke.lefeber@meertens.knaw.nl  \nABSTRACT \nThe paper presents an algorithm for automatic transcri p-\ntion of recordings of bell-playing clocks . Bell-playing \nclocks are clocks containing a hidden bell-playing m e-\nchanism that is periodically activated to play a melody . \nClocks from the eighteenth century give us unique insight \ninto the musical taste of their owners, so we are interested \nin studying their repertoire and performances - thus the \nneed for automatic transcription. In the paper, w e first \npresent an analysis of acoustical properties of bells found \nin bell-playing clocks. We propose a model that describes \npositions of bell partials and an algorithm that discovers \nthe number of bells and positions of their partials in a \ngiven recording. To transcribe a recording, we develop ed \na probabilistic method that maximizes the joint probabil i-\nty of a note sequence given the recording and positions of \nbell partials. Finally, we evaluate our algorithms on a set \nof recordings of bell-playing clocks. \n1. INTRODUCTION \nBell-playing clocks are clocks containing a hidden bell-\nplaying mechanism, which is activated every hour, every \nhalf an hour or even every quarter of an hour to play a \nmelody (see Figure 1 ). To make this happen, the going \ntrain activates the musical train, which starts the rotation \nof the musical cylinder. The cylinder contains a pattern of \npins which 'play' a series of keys as the cylinder rotates. \nThrough threads these keys are connected to hammers, \nwhich strike the bells. Such bell-playing mechanisms are \nusually part of longcase- or bracket clocks.  \nBell-playing clocks probably originate from carillons \nwhich played their melodies already in the thirteenth ce n-\ntury in the towns of the Low Countries. From the end of \nthe fifteenth century these instruments were also made for \ndomestic use, but they were unique pieces, only afford a-\nble for the very rich. From the end of the seventeenth \ncentury, bell-playing clocks became more and more po p-\nular, although they still remained a status symbol, only \naffordable for the rich elite. Many eighteenth-century \nbell-playing clocks have been preserved. Clock restorer Melgert Spaander from Zutphen (Netherlands) restored \nand recorded over 150 of these clocks and made these r e-\ncordings available for our researches. The collection co n-\nsists of approximately 1500 melodies, which offer us, in a \nway, recordings from the eighteenth century. We are \nstudying the repertoire of these clocks and also the pe r-\nformances of melodies with the aim of gaining more i n-\nsight into the musical taste of the eighteenth-century elite. \nIn order to study the repertoire of clocks, we need to tra n-\nscribe all of the recorded melodies, so that they can be \nanalyzed. Transcribing these melodies by hand requires a \nlot of practice and is made even more difficult by the i n-\nharmonicity and long decay times of bell sounds.  \n \nFigure 1 . Melodies of a bell-playing clock. \nIn this paper, we present an algorithm for automatic \ntranscription of recordings of bell-playing clocks. Auto-\nmatic music transcription is a difficult problem to solve, \nalthough methods are improving constantly; Klapuri and \nDavy provide an extensive overview of the current state \nof the art [1] . Because we know little of the acoustical \nproperties of clock bells, we could use unsupervised \nlearning techniques for transcription. Such techniques \nhave been used previously by several authors: Abdallah \nand Plumbley [2] used sparse coding for transcription of \nsynthesized harpsichord music, while Virtanen used it to \ntranscribe drums [3]. A number of authors use variants of \nnon-negative matrix factorization to transcribe polypho n-\nic music [4-7]. Their methods, however, were devised for \nmusic composed of harmonic sounds and are thus diff i-\ncult to apply to our domain. Recently, Marolt [8] pro-\nposed to use non-negative matrix factorization with sele c-\ntive sparsity constraints to transcribe recordings of church \nbells. \n \nPermission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fee provided that copies \nare not made or distributed for profit or commercial advantage and that \ncopies bear this notice and the full citation on the first page.   \n© 2010  International Society for Music Information Retrieval  \n333\n11th International Society for Music Information Retrieval Conference (ISMIR 2010) \n \nWhile we initially experimented with unsupervised \nlearning techniques, we obtained better results with the \nmethod proposed in this paper. We propose a two step \napproach to transcription: first, we present an analysis of \nacoustical properties of bell sounds and derive an alg o-\nrithm that discovers the number of bells and positions of \ntheir partials in a given recording . A probabilistic method \nthat relies on analysis of the recorded signal, the found \nbell partials , and on some higher-level musical kno w-\nledge is used to perform the transcription. We evaluate \nour approach on a collection of recordings of bell-playing \nclocks .  \n2. IDENTIFYING BELLS IN A RECORDING \n2.1 Modeling Positions of Bell Partials \nThe shape or profile of a bell determines the relative fr e-\nquencies of its vibrations . Bells have distinct but inha r-\nmonic partials – a partial being a frequency of vibration \npresent in the sound of a bell. Little is known of the \nacoustical properties of bells used in clocks, so we first \nconducted a study to determine whether we can model the \npositions of bell partials. To estimate the properties of \nclock bells, we analyzed a set of 10 recordings of diffe r-\nent clocks, containing a total of 88 bell sounds and m a-\nnually annotated positions of their partials . Bells were \nfound to have six strong partials; their positions relative \nto the perceived pitch (in cents) are listed in Table 1 .  \n  \nMean and st. dev. of  \npartial - pitch freq . (cents)  Mean  magnitude  relative to  \nthe strongest partial (dB)  \n 0 ± 0 \n  1485  ± 81 \n  2433  ± 111 \n  3145  ± 124 \n  3719  ± 123 \n  4236  ± 91 -21.8 \n-11.4 \n-4.8 \n-8.6 \n-13.0 \n-17.9 \nTable 1.  Mean s and std. deviations of relative partial p o-\nsitions (in cents) for the analyzed bells. Mean magnitude \nof each partial relative to the strongest partial (in dB) is \nalso shown \nWe can observe that partials are centered at appro x-\nimately 2.4, 4, 6, 8 .5 and 11 .5 times the fundamental fr e-\nquency (in Hz), the 3th and 4th partial being the loudest. \nThe fundamental frequency corresponds to the perceived \npitch. When studying relationships between the se par-\ntials, we discovered a regularity, not unlike what Hibbert \n[9] discovered for church bells, namely that relationships \nbetween relative positions of partials (i.e. logarithmic fr e-\nquency ratios) are linear . Figure 2  shows scatter plots of \nrelative positions of partials 2, 4, 5 and 6 versus the rel a-\ntive position of the third partial - the linearity is obvious. \nThis enables us to fit a linear regression model (also \nshown in Figure 2 ) that can be used to predict the pos i-tions of all six bell partials, if we know the positions of \ntwo of them, with an average error below 20 cents.  \n \n \nFigure 2 . Linearity of relative partial positions. \nFormally, a model that defines positions of bell par-\ntials given positions of two partials is expressed as a sum \nof Gaussians: \n \n26\n1, 2\n1, 2 2\n1()( ) exp2k\nff\nff\nkxrMx  , (1) \nwhere f1 and f2 are positions of any two bell partials, \n1, 2k\nffr\n the k-th partial position as calculated by the regre s-\nsion fit and  the allowed deviation from the regression \nfit. \n \n \n \nFigure 3 . Time evolution of the first four partials of a \nclock bell. \nTime evolution of partials follows an exponential d e-\ncay curve , and is faster for higher partials, which on the \nother hand, are initially louder. Individual partials fr e-\nquently exhibit beating, also evident in Figure 3 . Beating \nis caused by the so-called doublets, which arise when \nbells are not symmetrical about a vertical axis through \n334\n11th International Society for Music Information Retrieval Conference (ISMIR 2010) \n \ntheir centers. This asymmetry causes most of the vibr a-\ntional modes in bells to split into two distinct modes with \nslightly different frequencies that beat against each other. \nBeating is problematic when we try to estimate the onsets \nand time evolution of partials, so we try to remove its e f-\nfect, as described in section 3.  \n2.2 Bells? What Bells? \nWhen analyzing a recording of a bell-playing clock, we \ninitially have no information on the number of bells i n-\nvolved, their tuning or positions of their partials. In this \nsection, we introduce an algorithm that uses the bell pa r-\ntial model presented in section 2.1 to estimate the number \nof bells in a recording and positions of their partials . The \nalgorithm is based on the observation that bells are rarely \nstruck at the same time, which is mainly due to the i n-\nharmonic nature of bell sounds and their imperfect tu n-\ning. Therefore, we can use the “common fate” auditory \ngrouping principle, especially onset synchrony to find \ngroups of partials that belong to individual bells.  \nWe first calculate a magnitude spectrogram F of a r e-\ncording. To reduce variance in partial magnitudes in di f-\nferent frequency regions, we multiply the spectrogram \nwith a perceptual weighting model, as introduced by Vi n-\ncent [10] . Weights are calculated on an average spectrum \nand applied globally to yield a flattened time-frequency \nrepresentation Fw. Such flattening “amplifies” partials \nwith small magnitudes, which makes it easier for the a l-\ngorithm to consider those partials in the process of fin d-\ning partial groups. This is especially important, because \nmagnitude of the fundamental frequency of a bell lies \nover 20 dB below its loudest partial (see Table 1). As the \nfundamental corresponds to pitch, we need to accurately \nestimate it, otherwise the pitch of a bell can only be a p-\nproximated. \nBells have sharp onsets and long decay times, so the \nnext step of the algorithm accentuates the fast positive \nchanges (sharp onsets) in the magnitude spectrogram. \nThe dynamics of changes within frequency bins of Fw is \nestimated by calculating first order delta coefficients D of \nthe bins with a sliding window of length Nd. Delta coeff i-\ncients provide estimates of the gross shape of short time \nsegments of the frequency bins. They emphasize fast and \nbig changes, such as onsets, and deemphasize slower and \nsmaller changes, such as beating. This is illustrated in \nFigure 4  that displays delta coefficients of a bell partial \ncalculated on a recording of a bell-playing clock. \nTo discover groups of partials with synchronous o n-\nsets, we calculate covariances of their delta coefficients:  \n \n\n11max ,01n\nij ik i jk j\nkc d dn\n    ,    (2) \nwhere dij represents an element of the delta spectrogram \nD and i the average of the i-th row of D. Because delta \ncoefficients emphasize onsets, the value cij represents a measure of onset synchrony of partials with frequencies \ncorresponding to bins i and j. Bells do not share many \npartials and are rarely struck at the same time, so a bell’s \npartial will have high synchrony with other partials of the \nsame bell, but not with partials of other bells.  \n \n \nFigure 4 . Amplitude envelope and delta coefficients of a \nbell partial from a bell-playing clock recording \nA global covariance matrix C could be calculated on \nthe entire delta spectrogram D, but we found that this puts \ntoo much emphasis on bells that occur frequently in a r e-\ncording and fails to find partials groups of other less fr e-\nquent bells. We therefore calculate local covariance m a-\ntrices on all segments that are obtained by sliding a wi n-\ndow of length n over the delta spectrogram D with a step \nsize of n/2. This results in a set of local covariance m a-\ntrices C(t). The overall measure of onset synchrony of a \npartial i is then calculated by weighting the contributions \nof local covariance matrices with the overall energy of \nthe partial i n each segment, as approximated by\n()t\niic :  \n \n( ) ( )\n() 1\n11m\ntt\nij ii ij m\ntt\nii\nts c c\nc\n\n  .                    (3) \n \nFigure 5 . Onset synchrony of a partial at 6 840 cents. \nFour other partials from the same bell (8320, 9240, 9920 \nand 10460 cents) are clearly visible. \nFigure 5  displays one row of the resulting matrix S, \nrepresenting onset synchrony of a partial in a bell clock \nrecording. A group of five partials belonging to the same \nbell sound clearly stands out.  \nTo discover groups of synchronous partials, we ana-\nlyze each row si of the matrix S, and search for param e-\nters of the bell model presented in section 2.1 that best \n335\n11th International Society for Music Information Retrieval Conference (ISMIR 2010) \n \ndescribe si. Specifically, for each row si we find model \nparameters f1 and f2 that maximize:  \n \n1, 2\n1, 2arg maxf f i\nffMs  ,                    (4) \nwhere \n  denotes the dot product operator. Due to the \nsparseness of S (see Figure 5 ), an exhaustive search for \noptimal parameters can be performed efficiently. If the \ndot product in eq. (4) exceeds a preset threshold T, the \nmodel Mf1,f2 is considered to represent one of the bells in \nthe analyzed recording . The actual positions of bell’s pa r-\ntials may deviate from the model, so we estimate them \nfrom si by simple component-wise multiplication:   \n \n1\n1, 2 ()K\nf f iMbs\n  , (5) \nwhere K is used to compress partial magnitudes.  \nThe final outcome of the algorithm is a set of vectors \nBb\ndescribing the sounds of bells in a recording. We \npresent an evaluation of the algorithm in section 4. \n3. TRANSCRIPTION \nTo transcribe a recording of a bell-playing clock, we need \nto find which notes (bells) were played and when they \nwere played - their onset times. Although this may seem \nto be an easy task given positions of partials of bell \nsounds, the task is complicated due to several factors. \nFirst, partials interact; they get amplified, cancelled or \nbeat against each other, which makes it difficult to follow \ntheir amplitude envelopes and find their onsets. Decay \ntimes of bell sounds are long and although bells are \nusually not played at the same time, the number of co n-\ncurrently sounding bells (polyphony) is always high. Par-\ntials decay at different rates, so the spectrum of bells \nchanges with time. Recordings contain fast passages with \ninter-onset times of less than 100ms, as well as embe l-\nlishments such as grace notes an d arpeggios that further \ncomplicate transcription. And last, these are not synthetic \nrecordings, nor are they very professionally made; they \ncontain many noisy artefacts, such as background noise, \nnoises coming from the clock mechanism or similar.  \nWe chose to take a probabilistic approach to transcri p-\ntion and search for the most probable sequence of notes \nin a recording . The transcription process starts by calc u-\nlating the onset times and onset probabilities of bells . We \nuse the complex domain onset detection function and \npeak picking algorithm [11], which performs well with \nbell sounds, because of their sharp percussive onsets. O n-\nset probabilities are calculated from the value of the onset \ndetection function at each onset.  \nGiven N onset times and the fact that bells are seldom \nstruck at the same time, transcription can be viewed as a \nproblem of finding a sequence of notes and rests \ns1,s2,s3...sN that best describes the analyzed signal; s1 \nstarts at the first found onset, s2 at the second and so on. s \nmay represent a note (all notes ni, i=1..M are described by their corresponding bell models from the set B); or may \nbe a rest ( r). Specifically, we wish to find a sequence of \nnotes and rests that maximizes the joint probability: \n \n1 2 1 3 2 1( ) ( | ) ( | ) ... ( | )NN P s P s s P s s P s s  . (6) \nTo estimate probability of a note P(si=nj), we take two \nfactors into consideration: the probability that note nj de-\nscribed by the corresponding bell model bj actually o c-\ncurred in the signal at onset i, and the probability of that \nonset. Note probability is calculated by multiplying the \nbell model with the time-frequency representation D (as \ndefined in section 2.1 ). Onset probability is proportional \nto the value of the onset detection function at the onset. \nWe can thus write the probability of a note nj occurring at \nonset i as: \n \n1( ) ( )\nii j i j i CP s n P o   bd . (7) \nwhere \n  denotes dot product, di represents the time-\nfrequency representation D at time i and P( oi) the prob a-\nbility of an onset at that time. Ci is a scaling factor used to \nnormalize the dot product to a [0-1] range. \nProbability of a rest is defined as: \n \n \n1( ) (1 ( )) 1 ( )M\ni i i k\nkP s r P o P s n\n      , (8) \nthus if no notes are likely to occur and the onset is also \nnot likely, a rest will be likely .  \nTo define conditional probabilities P(si | si-1), we intr o-\nduced two changes to the above expressions . First, if note  \nnk occurred at time i-1, we subtract the note from the \ntime-frequency representation D, thus eliminating its e f-\nfect at time i:  \n \n11( ) max( ( 1 , ) , 0)i i k i k is n N i     d d b d\n . (9) \nOperator \n  denotes component-wise multiplication \nand N the unscaled normal distribution, which models the \ntime evolution of delta coefficients. As we can observe in \nFigure 4 , delta coefficients are roughly bell-shaped at o n-\nsets, so we approximate them with a normal distribution. \nIf si-1 is a rest, nothing is subtracted, so di(si-1=r)=di. \nAs intervals between adjacent notes in a melody are \nusually small (a phenomenon also known as pitch pro x-\nimity), w e include an additional factor into P(si | si-1). \nPitch proximity is modeled by a proximity profile R(n), \nwhich as in [12], is represented by a normal distribution \ncentered around a given pitch n, indicating pitch probabi l-\nities of the following note. The obtained conditional \nprobability of consecutive notes can thus be written as: \n \n1\n11 ( | ) ( ) ( ) ( )\nii j i k i i k j i i k CP s n s n P o R n s n     bd . (10) \nIf si is a rest, we can calculate the conditional probability \nwith the expression given in eq. (8) , whereby we replace \nnote probabilities with conditional probabilities and mul-\ntiply the expression with a constant representing the pro x-\nimity profile. \n336\n11th International Society for Music Information Retrieval Conference (ISMIR 2010) \n \nThe most likely sequence of notes and rests can be e f-\nficiently estimated with dynamic programming; the r e-\nsulting set of onset times and notes represents the tra n-\nscription of a recording. \n4. EVALUATION \nIn order to evaluate our algorithm, we manually tra n-\nscribed and annotated positions of partials in a set o f 10 \nrecordings of different bell-playing clocks. Results and \ndiscussion are given in the following sections. \n4.1 The Bell-finding Algorithm  \nWe used the following parameters to test the bell-finding \nalgoritm: the magnitude spectrogram was calculated with \nthe Constant- Q transform [13] , using a maximum wi n-\ndow size of 100ms, a step size of 25 ms and 20 cent spa c-\ning between adjacent frequency bins. The deltas were \ncalculated with a sliding window of Nd=9 frames, the c o-\nvariance matrices on n=100 frames long segments. Fina l-\nly, the threshold T that determines whether a bell model \nshould be included in the final results was set to 1/20th of \nthe maximum value of all models and the compression \ncoefficient K to 10. \nFor comparison, we also developed an alternative a p-\nproach for estimating partials in bell sounds. We used \nnon-negative matrix factorization (NMF) to factorize the \ndelta magnitude spectrogram D into matrices  W and H, \nwhere the basis vectors in W would ideally correspond to \nbell spectra and H would explain how bell magnitudes \nchange over time. Several efficient implementations of \nNMF exist in the literature; in our experiments we used \nthe SNMF algorithm introduced by Kim and Park [14] . \nThe algorithm is based on the alternating non-negativity \nconstrained least squares and active set method and a l-\nlows to impose sparsity constraints on H or W.  \nWith NFM learning, the number of basis vectors is \nfixed and we need to set it in advance, prior to the actual \nlearning . Because in our case basis vectors correspond to \nspectra of individual bells, we need to know the number \nof bells in a recording prior to learning. This is not usua l-\nly the case, but to perform the comparison of both a p-\nproaches , we give the NMF algorithm a small “adva n-\ntage” by setting the number of basis vectors to the actual \nnumber of bells as was manually annotated for each r e-\ncording.  \n \n precision  recall  \nproposed algorithm  0.94 0.98 \nSNMF  0.87     0.87     \nTable 2 . Comparison of two bell finding algorithms \nTable 2  shows average precision and recall scores of \nthe two algorithms on all recordings. Although both pe r-\nform well, the proposed approach outperforms non-\nnegative matrix factorization . We contribute the diffe r-ence to two main reasons. To find partials of bell sounds, \nthe proposed algorithm uses a local approach; namely c o-\nvariance matrices are calculated on short segments of the \nentire recording and then combined based on magnitudes \nof the analyzed partials in these segments . On the other \nhand, NMF works globally by iteratively minimizing the \nfactorization error . The difference is important when \nsearching for bells that are not frequently played. NMF \nwill tend to ignore them and rather focus on minimizing \nthe overall error which may lie in varying decay times of \nbell partials or noise. The local nature of our approach \nwill not fail for such cases, as the bells will stand out in \nindividual local segments and will consequently also \nshow in the global matrix S. Another advantage of the \nproposed approach is its use of the knowledge provided \nby the model of bell partial positions, as presented in sec-\ntion 2.1. Namely , the search for bell partials is limited by \nthe model, so only regions of the signal that correspond \nto predicted partial frequencies are considered. Therefore, \nnoise, either background or made by the clock mecha n-\nism or other external factors, can largely be ignored. \nNMF uses no such high-level knowledge, so it is affected \nby noise on all levels, as it tries to accommodate it and \ninclude it into the basis vectors. \nAll of the false negative errors (missed bells) made by \nour bell finding algorithm were bells a semitone apart \nfrom another more dominant bell, with most of their pa r-\ntials overlapping . Such bells are mainly used as embe l-\nlishments and were ignored because their onsets were \nmasked by the more dominant bell. However, since these \nbells are not very frequently played , these errors do not \nhave a large influence on overall transcription accuracy , \nas we show in the following section.  \n4.2 The Transcription Algorithm \nTo evaluate how various choices made when designing \nour transcription algorithm influence its performance, we \ntested several variants of the algorithm : A – the described \nalgorithm, B – excluding the pitch proximity profile, thus \nmaking all note transitions equally probable, C – exclu d-\ning note subtraction, thus avoiding conditional note pro b-\nabilities and D – using annotated onsets instead of the \ncalculated ones and E – using annotated bell partials i n-\nstead of the calculated ones. Average precision and recall \nscores of transcriptions of all recordings are shown in Ta-\nble 3 .  \nAs we can observe, differences between these variants \nare not very big. This is due to the fact that the differen c-\nes mostly affect “problematic” parts of recordings that \ninclude fast passages and embellishments, while els e-\nwhere the combination of the delta magnitude spectr o-\ngram, accurately estimated positions of bell partials, and \ncorrectly found onsets makes them irrelevant.  \nFor the problematic parts, the pitch proximity profile \nthat favors smaller intervals ( B) and especially note su b-\ntraction ( C), which mostly prevents repetitions of pred o-\n337\n11th International Society for Music Information Retrieval Conference (ISMIR 2010) \n \nminant notes, do have a positive effect on performance . \nAs (D) shows, approx. half of the missed notes are caused \nby missed onsets and recall is raised by approx. 0.05 if \nperfect onset detection is used. On the other hand, not h-\ning is gained by using the manually annotated bell par-\ntials, so the bell finding algorithm seems to be working \nvery well and the errors it made seem to be almost irrel e-\nvant. \n \n transc ription  \n precision  recall  \nA: proposed algorithm  0.95     0.89     \nB: no proximity  0.94 0.87 \nC: no conditional prob.  0.91 0.88 \nD: perfect onsets  0.94 0.94 \nE: perfect bell models  0.95 0.89 \nTable 3 . Comparison of variants of the transcription alg o-\nrithm  \nMost of the errors, either missed notes (false neg a-\ntives) or extraneous notes (false positives) are made in \nfast passages, where note repetitions are missed, notes are \ntranscribed in an incorrect order or weak onsets ignored. \nOverall, the performance is good enough, so that tra n-\nscriptions will be used for further analysis and included in \na searchable database of melodies; in fact when analyzing \nthe errors, we discovered that several errors were in the \nground truth and not in the calculated transcriptions.  \n5. CONCLUSION \nThe proposed approach to transcription of bell-playing \nclock recordings is a good first step towards analysis of \nthese recordings. The bell- finding and transcription alg o-\nrithms perform well and will be used to transcribe the e n-\ntire collection of recordings of bell-playing clocks. We \nwill add the resulting transcriptions to a searchable dat a-\nbase of melodies, thus making them available to interes t-\ned researchers for further analysis . There is room for i m-\nprovement s of the algorithm; we plan to consider ways of \nallowing for correct treatment of simultaneous notes, as \nwell as to test the algorithm on other recordings contai n-\ning bell sounds. \n \nAcknowledgments. This work was supported in part by \nthe Slovenian Government-Founded R&D project Et h-\nnoCatalogue: creating semantic descriptions of Slovene \nfolk song and music. \n6. REFERENCES \n[1] A. Klapuri and M. Davy, Eds., Signal \nProcessing Methods for Music Transcription . \nNew York: Springer, 2006, p.^pp. Pages. \n[2] S. A. Abdallah and M. D. Plumbley, \n\"Unsupervised analysis of polyphonic music by sparse coding,\" Neural Networks, IEEE \nTransactions on, vol. 17, pp. 179-196, 2006. \n[3] T. Virtanen, \"Monaural Sound Source \nSeparation by Nonnegative Matrix Factorization \nWith Temporal Continuity and Sparseness \nCriteria,\" Audio, Speech, and Language \nProcessing, IEEE Transactions on, vol. 15, pp. \n1066 -1074, 2007. \n[4] P. Smaragdis and J. C. Brown, \"Non-negative \nmatrix factorization for polyphonic music \ntranscription,\" in Applications of Signal \nProcessing to Audio and Acoustics, 2003 IEEE \nWorkshop on. , 2003, pp. 177- 180. \n[5] E. Vincent , et al. , \"Harmonic and inharmonic \nNonnegative Matrix Factorization for \nPolyphonic Pitch transcription,\" in Acoustics, \nSpeech and Signal Processing, 2008. ICASSP \n2008. IEEE International Conference on , 2008, \npp. 109- 112. \n[6] S. A. Raczynski , et al. , \"Multipitch Analysis \nwith Harmonic Nonnegative Matrix \nApproximation,\" in ISMIR 2007, 8th \nInternational Conference on Music Information \nRetrieval , Vienna, Austria, 2007, pp. 381- 386. \n[7] B. Niedermayer, \"Non-negative Matrix Division \nfor the Automatic Transcription of Polyphonic \nMusic,\" in ISMIR 2008, 9th International \nConference on Music Information Retrieval , \nPhiladelphia, USA, 2008, pp. 544- 545. \n[8] M. Marolt, \"Non-negative matrix factorization \nwith selective sparsity constraints for \ntranscription of bell chiming recordings,\" in \nSound and Music Computing Conference , Porto, \nPortugal, 2009, pp. 137- 142. \n[9] W. A. Hibbert, \"The Quantification of Strike \nPitch and Pitch Shifts in Church Bells,\" Ph.D., \nFaculty of Mathematics, Computing and \nTechnology, The Open University, Milton \nKeynes, UK, 2008. \n[10] E. Vincent and M. D. Plumbley, \"Low Bit-Rate \nObject Coding of Musical Audio Using \nBayesian Harmonic Models,\" Audio, Speech, \nand Language Processing, IEEE Transactions \non, vol. 15, pp. 1273-1282, 2007. \n[11] J. P. Bello , et al. , \"On the use of phase and \nenergy for musical onset detection in the \ncomplex domain,\" Signal Processing Letters, \nIEEE, vol. 11, pp. 553-556, 2004. \n[12] D. Temperley, Music and Probability : The MIT \nPress, 2007. \n[13] J. C. Brown, \"CALCULATION OF A \nCONSTANT-Q SPECTRAL TRANSFORM,\" \nJournal of the Acoustical Society of America, \nvol. 89, pp. 425-434, Jan 1991. \n[14] H. Kim and H. Park, \"Nonnegative Matrix \nFactorization Based on Alternating \nNonnegativity Constrained Least Squares and \nActive Set Method,\" SIAM Journal on Matrix \nAnalysis and Applications, vol. 30, pp. 713-730, \n2008.  \n338\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "Recognition of Variations Using Automatic Schenkerian Reduction.",
        "author": [
            "Alan Marsden"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1418179",
        "url": "https://doi.org/10.5281/zenodo.1418179",
        "ee": "https://zenodo.org/records/1418179/files/Marsden10.pdf",
        "abstract": "Experiments on techniques to automatically recognise whether or not an extract of music is a variation of a giv- en theme are reported, using a test corpus derived from ten of Mozart‘s sets of variations for piano. Methods which examine the notes of the ‗surface‘ are compared with methods which make use of an automatically derived quasi-Schenkerian reduction of the theme and the extract in question. The maximum average F-measure achieved was 0.87. Unexpectedly, this was for a method of match- ing based on the surface alone, and in general the results for matches based on the surface were marginally better than those based on reduction, though the small number of possible test queries means that this result cannot be regarded as conclusive. Other inferences on which factors seem to be important in recognising variations are dis- cussed. Possibilities for improved recognition of match- ing using reduction are outlined.",
        "zenodo_id": 1418179,
        "dblp_key": "conf/ismir/Marsden10",
        "keywords": [
            "automatic recognition",
            "music variations",
            "surface analysis",
            "quasi-Schenkerian reduction",
            "F-measure",
            "Mozarts sets",
            "theme comparison",
            "test corpus",
            "experimental methods",
            "surface-based matching"
        ],
        "content": "G5 C5 \nC3 C4         \n        \n        \n        \nG5 C5 \nC3 C4    G5 C4     \n        \nC5 C3   G5 C4   A5 C4   G5 C4   \nC5 C3  C5 C4  G5 E4  G5 C4  A5 F4  A5 C4  G5 E4  G5 C4  \n \nFigure 1. The first four bars of the theme of Mozart‘s \nvariations K.265 (using a tune known in English as ‗Twi n-\nkle, twin kle little star‘), and the highest -scoring r eduction \nderived from these bars by the software.  RECOGNITION OF VARIATIONS USING AUTOMATIC \nSCHENKERIAN REDUCTION \nAlan Marsden  \nLancaster Institute for the Contemporary Arts, Lancaster University, UK  \nA.Marsden@lancaster.ac.uk  \nABSTRACT \nExperiments on techniques to automatically recognise \nwhether or not an extract of music is a variation of a gi v-\nen theme are reported, using a test corpus derived from \nten of Mozart‘s sets of variations for piano. Methods \nwhich examine the notes of the ‗surface‘ are compared \nwith methods which make use of an automatically derived \nquasi-Schenkerian reduction of the theme and the extract \nin question. The maximum average F-measure achieved \nwas 0.87. Unexpectedly, this was for a method of matc h-\ning based on the surface alone, and in general the results \nfor matches based on the surface were marginally better \nthan those based on reduction, though the small number \nof possible test queries means that this result cannot be \nregarded as conclusive. Other inferences on which factors \nseem to be important in recognising variations are di s-\ncussed. Possibilities for improved recognition of matc h-\ning using reduction are outlined . \n1. SCHENKERIAN REDUCTION \nEarlier work [6] has shown that Schenkerian analysis by \ncomputer is possible, though not easy. (Currently only \nshort segments of music can be analysed, and confidence \nin the analyses produced cannot be high.) The aim of the \nresearch reported here is a first attempt at testing whether \nthese automatic analyses produce information which is \nuseful for information retrieval. \nSchenkerian analysis is a technique, with a long ped i-\ngree in music theory, which aims to discover the stru c-\ntural ‗framework‘ which is believed to underlie the ‗su r-\nface‘ of a piece of music  (see [1], for example). Redu c-\ntion according to the theory of Lerdahl & Jackendoff, \nwhich has also been subject to computational impleme n-\ntation [2], is broadly similar. Figure 1  shows the first four \nbars of the theme of a set of variations for piano by M o-\nzart, and its reduction as derived by the software used \nhere.  (This is by far the simplest of the t hemes used here; \nto show other themes and their reductions would take more space than is available.)  Schenker‘s reductions were \nnotated in a different fashion, and also included inform a-\ntion not given here, but the basic information of which \npitches are reg arded as more ‗structural‘, and so included \nin the higher levels, is sim ilar. \nThe research reported here fits into that body of MIR \nresearch which aims to improve MIR procedures through \nthe application of ideas from music theory. \n2. VARIATIONS \nA common type of composition in classical music is \n‗theme and variations‘. In this kind of piece, a theme is \npresented, followed by a number of variations of that \ntheme. There is no single and established definition of \nwhat constitutes a variation of a theme, but in the Class i-\ncal period (the period of Haydn, Mozart and Beethoven) \nit is clear that a variation is not simply the presentation of \nthe same melody in different arrangements (as it was for \nsome later composers) but rather a composition which has \nthe same structural features as the theme. This is partic u-\nlarly clear in Mozart‘s variations : they are almost always \nthe same length as the theme, have the same number of \nphrases, and have matching cadences for those phrases (at \nleast in their harmony; often in other featur es also).  The \n \nPermi ssion to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fee provided that copies \nare not made or distributed for profit or commercial advantage and that \ncopies bear this notice and the full citation  on the first page.   \n© 2010  International Society for Music Information Retrieval  \n501\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \ninternal structure of those phrases can also show common \nfeatures: the harmony is often similar; there can be co m-\nmon notes, especially in important positions like begi n-\nnings and endings, and the variation sometimes clearly \ngives a decorated version of the melody and/or bass of the \noriginal. Figure 2  shows the first four bars of two varia-\ntions of the theme shown in Figure 1.  \nIf Schenkerian analysis validly reveals musical stru c-\nture, then the analysis of each variation should, to some \ndegree, match the analysis of the theme. To test this r e-\nquires analyses of variations and themes which are unb i-\nased in the sense that the analyses of each variation \nshould be made with no knowledge of the theme. To \nachieve unbiased analyses with human analysts would be \nvery difficult: expert analysts are required, and one would \nhave to recruit as many analysts as there are variations in \na set. Furthermore, it is well known that different analysts \nproduce different analyses, and it would be difficult to \nneutralise these personal differences. The computer sof t-\nware described below gives a means for generating unb i-\nased analyses, and so allows this kind of empirical test of \nthe validity of Schenkerian analysis. \n3. REDUCTION SOFTWARE \nThe method of reduction used here is described more \nfully in [6]. There is space here only to give a brief ou t-\nline. An analysis of a piece is a binary tree whose leaves \nare the ‗segments‘ of the su rface of the music (the notes \nof the score). A segment is a span of music, containing all \nthe notes sounding at that time. At least one note begins at \nthe start of the segment and at least one note finish es at its \nend. No notes begin or finish at other points within the \nspan. A note is defined by its pitch and by whether or not \nit is tied to a note in the preceding segment. A single note \nin the score can be split into a series of tied notes across \nseveral segments. \nSegments above the surface are related to a pair of \n‗child‘ segments through a set of ‗atomic elaborations‘. \nThese define how a note in a higher-level segment can be elaborated to become two shorter notes (or a note and a \nrest) in the child segments. The set of atomic elaborations \nis derived from Schenkerian theory and consists of such \nthings as repetitions, neighbour notes, anticipations, co n-\nsonant skips, etc. Atomic elaborations can imply that ce r-\ntain pitches are consonant, and the implications of the set \nof atomic elaborations relating a higher-level segment to \nits children must be consistent (i.e., the consonant pitches \nmust form an acceptable harmony). \nAn analysis is therefore a kind of parse tree employing \na grammar defined by the atomic elaborations. The sof t-\nware used here effectively employs a chart parser [4 ] as a \nstep towards generating such a tree, but the computational \ncomplexity of the algorithm is of order O(n4) time. With \ntypical computing resources, it is therefore possible to \nderive a parse chart from extracts of simple piano music \nup to only four to eight bars in length.  \nThe parse chart is a triangular matrix whose cells co n-\ntain the possible reductions at each stage of reduction. \nThe bottom (longest) row contains the segments of the \nsurface. The first row above contains segments which r e-\nsult from reduction of each of the pairs of consecutive \nsegments below. Rows further above contain segments \nwhich result from reductions of those with other se g-\nments, etc., until the top row, with just one cell, contains \nthe segments which derive from reduction of the entire \nextract. The top part of Figure 1 shows a reduction chart \nin which the best-scoring analysis has been selected (see \nbelow). Most of the cells of this chart are empty and those \nthat are not contain just one segment, each containing two \nto four notes. Before an analysis is selected from a chart, \nits cells are generally fuller, and each contains a number \nof segments corresponding to the different ways in which \na group of surface segments may be reduced. Each d e-\nrived segment has an associated score, intended to su g-\ngest how likely that segment is to be a part of a complete \n‗good‘ analys is of the entire extract.  \nAn analysis can be derived from the chart by selecting \na high-scoring segment in the top cell, and then recu r-\nsively selecting its highest-scoring children until a co m-\nplete tree to all the segments of the surface has been d e-\nrived. However, complications of context-sensitivity \nmean that selecting the locally highest-scoring children at \neach stage does not guarantee the highest-scoring com-\nplete analysis. The current procedure to ensure derivation \nof the highest-scoring analysis from the chart is of exp o-\nnential complexity, so in some cases a chart containing \ninformation on possible analyses can be derived, but it is \nnot practical, by current means, to derive a single best \nanalysis from this chart. \nThe research reported in [6] derived some scoring \nmechanisms by comparing the output of the analysis-\nderivation software with pre-existing analyses of the same \npieces. One can therefore have some confidence in the \nscores the software derives, but because of a lack of rea d-\n \n \nFigure 2. The beginning of t wo variations of the theme \nshown in Figure 1.  \n502\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \nily available test material, the research so far has been \nbased on a very small quantity of music (just five short \nthemes by Mozart). At this stage, therefore, results from \nresearch in this general area can only really be regarded \nas provisional. \nThat earlier research also showed that low-scoring po s-\nsible reductions can be omitted from the chart, vastly r e-\nducing the computation time required for its derivation, \nwithout jeopardising the derivation of a good analysis. \nThis project has used the same limits as outlined in [6]. In \nderiving the reduction chart, no more than 25 segments \nwere recorded in each cell, discarding lower-scoring po s-\nsibilities if necessary. In [6] scores were computed from \ncomparison of good analyses with random analyses co n-\ntaining an Ursatz (a structure Schenker regarded as ind i-\ncating a complete musical statement). In this project, the \nextracts of music do not constitute complete statements \n(most importantly they often do not end on the tonic), so \nnew scores were computed from the same raw data from \ncomparisons of good analyses with random analyses, r e-\ngardless of the presence of an Ursatz . The new scores \nwere similar to the old ones. \nSmall changes were also made to the set of possible \natomic reductions because certain configurations not \nfound in the five themes used in [6] were found in the m a-\nterial used here. An ‗échappée‘ ( a following incomplete \nneighbour note) elaboration was added, with tight ha r-\nmonic constraints. New harmonic constraints, looser in \nsome respects but tighter in others, were defined for some \nelaborations to allow situations where a dissonant note \ncan be elaborated by ‗repetition‘, ‗delay‘ or ‗shortening‘ \n(i.e., being preceded or followed by a rest).  \n4. RECOGNISING VARIATIO NS \nThe objective of the research reported here is to explore \nmechanisms for recognising whether or not a passage of \nmusic is a variation of a given theme, and in particular to \ntest whether or not a procedure using reduction yields be t-\nter recognition than one relying only on the ‗surface‘ of \nthe music. To be precise, if a procedure which uses redu c-\ntions of the theme and variations produces better results \nthan a similar procedure which does not use reductions, \nthen we can conclude with some confidence that the r e-\nduction software does produce useful information co n-\ncerning musical structure. \n4.1 Materials \nThe materials used in this project are encodings made by \nmyself of four bars from the theme and most of the vari a-\ntions of 10 sets of varia tions for piano by Mozart:  K. 179, \n180, 264, 265, 352, 354, 398,  455, 573 and 613 . These \nare all the  sets of variations in section 26 of the Neue \nMozart Ausgabe —the source used —with the exception of \ntwo sets written when Mozart was nine years old, and which cannot therefore safely be regarded as mature \ncompositions, one set in the metre 6/8, and one whic h has \na theme beginning and ending half -way through a bar.  In \nall but one case it  is the first four bars which are  used. In \nK. 613 the first four bars are taken from  the theme proper, \nwhich begins after an introduction . In each case the four \nbars form a co herent phrase. Variations in a minor key, or \nin a di fferent metre from the theme, were omitted. Some \nsmall changes to the music were required in order to f a-\ncilitate successful reduction by the software: all anacruses \n(pickups) were omitted as the reduction  software cannot \ncope with these;  all grace notes, and trills plus any termi-\nnating turn, were omitted;  in a very few cases notes from \nsome middle voices were omitted because the software \noperated with a limit of 4 notes in a segment; notes at the \nend of th e last bar which clearly led into the following bar \nrather than belonging to the first phrase were omitted.  \nThe encoding gave the pitch of each note (the pitch \nspelling of the score is used in the encoding, but pitches \nare converted to MIDI values in the software) and its d u-\nration. Voices are indicated, and were determined by \nhand when the encoding was made. This information is \nused only when matching surfaces as the reduction proc e-\ndure changes the composition of voices. \nTo neutralise differences of key, each theme and vari a-\ntion was transposed to the key of F major, a key selected \nbecause it allowed each entire set of theme and variations \nto be transposed in the same direction and still remain in \nrange for the software. It is not so simple to neutralise di f-\nferences of metre, so themes in a triple metre were only \ncompared with variations in a triple metre, and similarly \nfor themes in a duple metre. This made a corpus in two \nparts, for duple and triple metres, of 5+5  themes and \n41+36  variations.  This is not a s ufficiently large corpus \nfor definitive r esults, but further materials are not readily \navailable.  \n4.2 Procedure \nA reduction ‗chart‘ (i.e., a matrix of the possible redu c-\ntions) was derived from each of the extracts of themes \nand variations, using the software as described above. \n(This took about 24 hours of computing time.) The best-\nscoring analyses were derived for each of the themes. \n(This was not possible for the variations because of the \nexcessive demand of computing time in some cases.)  \nThere has been considerable research on techniques of \nmeasuring melodic similarity (see, for example [3]), but \nto ask if some extract of music is a variation of another, at \nleast in the case of ‗Classical‘ variations as described \nabove, is not the same as to ask if two extracts are similar. \nSome work in measuring melodic similarity has attempted \nto make use of concepts of structure from music theory \n[5, 7], with encouraging results. Unlike that work, the r e-\nsearch reported here is concerned with full textures rather \nthan just melodies, and unlike [7], which shares some of \n503\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \nthe underlying concepts of this work, the comparison \nmethod requires no manual intervention (though it does \nmake use of an encoding which gives the key and metre). \nInstead a large number of methods specialised to compa r-\ning extracts to determine if one is a variation of the other, \nboth at the surface and comparing a best analysis to a r e-\nduction chart, were implemented in software. Each \nmethod resulted in a single match value for each pair \ncompared. If a comparison method is successful, it will \nconsistently yield higher values for comparisons between \na theme and a variation of that theme than between a \ntheme and a variation of a different theme.  \n4.2.1 Comparison Methods \nSimilar principles were used in the design of the methods \nfor comparing both surfaces and reductions, as follows. \n1. Pitch-matching: pitches/pitch classes.  Some met h-\nods count exactly matching pitches; some methods a c-\ncept matching pitch classes (i.e., the matched pitch \ncan be transposed up or down any number of octaves). \n2. Voices to test: all/melody+bass/melody/bass.  There \nare four different kinds of match under this heading: \nthose which seek to match all notes of each segment \nfrom the theme, those which match only the melody \nand bass, those which match only the melody, and \nthose which match only the bass. For reduction \nmatches, the lowest note of a segment is taken to b e-\nlong to the bass and the highest to the melody.  \n3. Voice-matching: yes/no.  Some methods only accept \nmatches of pitches in the same voice; some accept \nmatches no matter in which voice the note occurs in \nthe variation. The concept of voice used here is only \n‗melody‘, ‗middle‘ and ‗bass‘. The middle contains all \nthe notes which are not in the melody and bass. \n4. Match tied notes: yes/no.   Some methods seek to \nmatch only notes which are not tied to a preceding \nnote, while others seek to match all notes. \n5. Weighting by duration: yes/no.  Some methods \nweight matches in proportion to the duration of the \nsegment in the theme to be matched. \n6. Weighting by metre/level: yes/no.  In surface-\nmatching methods, matches can or cannot be weighted \nby the metrical level of the beginning of the note, gi v-\ning notes at the beginning of the bar the greatest \nweight. (The metre of a piece is specified in the e n-\ncoding.) In reduction- matching methods, the corr e-\nsponding weight is determined by the level of the \nsegment in the analysis tree. Weight steadily decreases \nfrom the root to the leaves.  \n7. Limiting by parent match: yes/no (reduction only).  \nSome matching methods for reductions limit the level \nof match found for child segments to be no greater \nthan the level of match found for their parents, on the grounds that matches of children when the parent is \nnot matched are accidental. \n8. Values: present/proportion/bar; maximum/aver-\nage/score-weighted/score-weighted*2.  Different \nvalues can be recorded for any individual segment. In \nthe case of surface matches, some methods only look \nfor a matching pitch to be present within the time span \noccupied by the original pitch. In other cases, the pr o-\nportion of the original time span during which a \nmatching pitch is sounding in the variation is used. In \nyet others, it is sufficient merely for a matching pitch \nto be present somewhere with the same bar, since \nvariations clearly sometimes involve changes in \nrhythm. For reduction-matching measures, a segment \nof the theme can be matched with up to 25 possible \nsegments in the reduction chart for the variation. In \ndifferent methods, four different values are recorded: \nthe maximum match; the average match; the average \nmatch weighted by the score of the matching segment; \nand the average match weighted by the square of the \nscore of the matching segment. Score weights are \ncomputed in relation to the maximum weight in a r e-\nduction chart so as to always fall in the range 0 to 1 \nand decrease exponentially in relation to decreases in \nscore. \nThe combination of all these parameters results in 384 \ncomparison methods for surfaces and 1024 for reductions. \nIn each case, the match value for a segment is based on \nthe number of notes from the segment of the theme which \nare matched in the corresponding segments of the vari a-\ntion, divided by the number of notes to be matched, \nweighted as appropriate by proportion for surfaces or \nscore for reductions with parent-match limiting applied if \nappropriate. The overall result of a comparison between a \ntheme and a variation is the average of the results from \nmatching each segment of the theme (and its reduction, if \nappropriate) with the corresponding segments of the \nvariation (and its reduction), weighted by duration and/or \nmetre/level as appropriate. \n4.2.2 Testing Methods \nEvery theme was compared with every variation in the \nsame class of metre —those which were variations of this \ntheme and those which were variations of another \ntheme —using each of the comparison methods outlined \nabove. Each test can be thought of as retrieval from a d a-\ntabase using a theme as the query. A perfect response \nwould retrieve all the variations of that theme, and none \nof the variations of other themes. An appropriate measure \nof success is therefore the F-measure, the harmonic mean \nof ‗precision‘ (the proportion of correctly retrieved vari a-\ntions to the total retrieved) and ‗recall‘ (the proportion of \ncorrectly retrieved variations to the total number of vari a-\ntions for that theme).  \n504\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \nA simple query mechanism would retrieve all vari a-\ntions whose comparison with the theme yields a value \nabove a certain threshold. Possible values for this thres h-\nold lie between the lowest value for any comparison b e-\ntween a theme and one of its variations, and the highest \nvalue for any comparison between a theme and a variation \nof a different theme. For each comparison method, the \naverage F-measure, using each theme as a query, was \ncomputed, at each candidate value of the threshold. The \nbest possible F-measure (on this corpus) using each co m-\nparison method was thus be computed. \nAn alternative test is to ask, for each variation, of \nwhich of the five candidate themes is it a variation. The \nsimple answer would be the theme which yields the hig h-\nest comparison value. This test will be called the ‗reco g-\nnition measure, and for each comparison method the \nvalue recorded is the percentage of variations whose \ntheme is correctly recognised. \n5. RESULTS \nThe main hypothesis of this study, that reduction will lead \nto better recognition of variations, is not confirmed by the \nresults, as shown in Table 1. In fact twelve of the 384 \nmethods comparing surfaces produced a better average  \nF-measures than the best reduction-comparing method , \nand two produced better recognition measures. The dif-\nference is small, however. It is impossible to know wit h-\nout further research whether this is because the fundame n-\ntal idea that variations share common reductions is mi s-\ntaken, or whether it is because the reductions produced by \nthis reduction software are incorrect. Currently there is no \nsimple way of determining the correctness of an analysis. \nThe values of match between the analysis of a theme \nand the reductions of its variations are generally high, but \nthey can also be high for reductions of variations of other \nthemes. This is illustrated in Figure 3, which shows a \ngraph of the match values for K. 265, using the best r e-\nduction-matching method (matching pitch classes from \nthe melody and bass in the appropriate voice in the vari a-\ntion, but not matching tied notes; weighted by duration \nbut not level and not limited; taking the maximum match \namong alternative segments). The best threshold value for \nthis comparison method is 0.78, which causes one vari a-\ntion of this theme not to be recognised, and a number of \nfalse positives from variations of other themes. According \nto Schenkerian theory, pieces of tonal music become \nmore alike each other the higher up the structural tree one \nlooks, until all (proper) pieces share one of only three \npossible Ursätze . Perhaps the reduction-matching met h-\nods have been confounded by this underlying universal \nsimilarity. \nThe match values for surface matches are typically \nlower and more spread out, as illustrated in Figure 4, \nwhich shows the results for the same theme using the best surface-matching method (matching all pitch classes in \nthe appropriate voice in the variation, including tied \nnotes; weighted by duration but not metre; taking the pr o-\nportion a pitch class is present in a segment‘s span). T he \nbest threshold for this method is 0.36, causing all vari a-\ntions of this theme to be correctly recognised but also a \nfalse positive. \n5.1 Factors leading to better recognition \nAnalysis of the results indicates that many of the factors \nlisted above make little difference to the quality of a re c-\nognition method. One notable exception is that weighting \nby level in the case of reduction matches generally leads \nto worse results. This is consistent with the general co n-\nclusion above that reduction does not lead to better re c-\nognition of variations. Also consistent with this is a \nweaker result that weighting by duration does not improve \nrecognition in the case of reduction matches, probably \nbecause higher-level segments are likely to have longer \ndurations. In the case of surface matches, however, \nweighting by duration, but not by metre, leads to a slight \nimprovement. \nOn average, counting a surface match simply by the \npresence of the required pitch or pitch class within the \nspan of a segment gives slightly better results than mea s-\nuring the proportion of the span in which it is present, and \nboth give better results than counting matches anywhere \nwithin the bar. However, there are interdependencies \namong the various parameters. For example, when pitch \nclasses are matched within voices, measuring the propo r-\ntion gives consistently better results. \nIn the case of reduction-based methods, taking the \nmaximum match among alternative segments yields the \nbest results, on average. This is consistent with the idea \nthat variations should have reductions which match the \nreductions of the theme. The listener hears the theme first, \nand so ambiguities in the structure of variations can be \nresolved by reference to the structure of the theme. It is \ntherefore sufficient that there be some possible  reduction \nof the variation which matches the theme. \nIn both surface- and reduction-based methods, the \nworst results come from matching only the bass, followed \nby matching only the melody. The difference between \nmatching all notes and just the melody and bass is small. \nIn every case, if pitch classes are matched, the best results \ncome from matching them in the appropriate voices,  Surface methods  Reduction methods  \nAverage  \nF-meas.  Recog. \nmeasure Average  \nF-meas.  Recog. \nmeasure \nBest  0.867  94.8%  0.842  90.9%  \nAverage  0.776  74.8%  0.748  70.3% \nWorst  0.540  42.9%  0.671  35.1%  \nTable 1. Summary results.  \n505\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \nwhereas if pitches are matched, the best results come from \nignoring the voice in which they occur in the variation. \nThis might be because sometimes Mozart writes a new \npart above  the melody, and in such cases the melody often \noccurs at its original register. \n5.2 Possible Improvements \nA half-way house has been tested, which looked for \nmatches of segments at higher levels only if there was no \nmatch at a level below. However, this produced no better \nresults than those given above. Better results might come \nfrom matching melody and bass voices separately, poss i-\nbly at different levels, but this has not yet been tested. \nIn examination of some of the false negatives and false \npositives, similarities and dissimilarities are revealed in \nthe reductions which are not present at the surface, but as \nyet no consistent pattern has been discerned which would \nlead to a consistently better variation-recognition method. \nIt is possible that harmony should be taken into account. \n(Harmonic analysis is a bi-product of the reduction pr o-\ncedure.) Matching on harmony alone, however, would not \nproduce good results because many of the themes have \nsimilar harmonic structures; it would have to be combined \nwith other factors. \nOverall, variation has been found to be more compl i-\ncated than first thought. The quantitative results do not \nshow reduction to reveal the relationship between theme \nand variations, but examination of false results suggests \nthat further research might yet show this to be the case. 6. REFERENCES \n[1] Forte, A. & Gilbert, S.E. Introduction to \nSchenkerian Analysis , Norton, New York, 1982.  \n[2] M. Hamanaka, K. Hirata, and S. Tojo: \n―Implementing ‗A Generative Theory of Tonal \nMusic‘‖, Journal of New Music Research , Vol. 35 \nNo. 4, pp. 249 –277, 2007. \n[3] W. Hewlett and E. Selfridge-Field (eds.): Melodic \nSimilarity  (Vol. 11 of Computing in Musicology ), \nMIT Press, Cambridge MA, 1998. \n[4] D. Jurafsky and J.H. Martin:  Speech and Natural \nLanguage Processing  (2nd edition), Pearson, Upper \nSaddle River NJ, 2009. \n[5] P. Kranenburg, A. Volk, F. Wiering and R.C. \nVeltkamp: ―Musical Models for Folk -Song Melody \nAlignment‖, Proceedings of the International \nSymposium on Music Information Retrieval , pp. \n507–512, 2009.  \n[6] A. Marsden: ‗Schenkerian Analysis by Computer: A \nProof of Concept‘, Journal of New Music Research , \nforthcoming. \n[7] N. Orio and A Rodà: ―A Measure of Melodic \nSimilarity Based on a Graph Representation of the \nMusic Structure‖, Proceedings of the International \nSymposium on Music Information Retrieval , pp. \n543–548, 2009.  \n \nFigure 3 . Match values for the theme of K. 265 using a reduction-based comparison method. \n \nFigure 4 . Match values for the theme of K. 265 using a surface-based comparison method. \n506\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "YAAFE, an Easy to Use and Efficient Audio Feature Extraction Software.",
        "author": [
            "Benoît Mathieu",
            "Slim Essid",
            "Thomas Fillon",
            "Jacques Prado",
            "Gaël Richard"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1418321",
        "url": "https://doi.org/10.5281/zenodo.1418321",
        "ee": "https://zenodo.org/records/1418321/files/MathieuEFPR10.pdf",
        "abstract": "Music Information Retrieval systems are commonly built on a feature extraction stage. For applications involv- ing automatic classification (e.g. speech/music discrimi- nation, music genre or mood recognition, ...), traditional approaches will consider a large set of audio features to be extracted on a large dataset. In some cases, this will lead to computationally intensive systems and there is, therefore, a strong need for efficient feature extraction. In this paper, a new audio feature extraction software, YAAFE 1 , is presented and compared to widely used li- braries. The main advantage of YAAFE is a significantly lower complexity due to the appropriate exploitation of re- dundancy in the feature calculation. YAAFE remains easy to configure and each feature can be parameterized inde- pendently. Finally, the YAAFE framework and most of its core feature library are released in source code under the GNU Lesser General Public License.",
        "zenodo_id": 1418321,
        "dblp_key": "conf/ismir/MathieuEFPR10",
        "keywords": [
            "YAAFE",
            "audio feature extraction",
            "efficient feature extraction",
            "redundancy",
            "configurable",
            "parameterized",
            "GNU Lesser General Public License",
            "software release",
            "automatic classification",
            "feature calculation"
        ],
        "content": "YAAFE, ANEASY TO USE AND EFFICIENTAUDIO FEATURE\nEX\nTRACTION SOFTWARE\nBenoit Mathieu, Slim Essid, Thomas Fillon,Jacques Prado, Ga ¨elRichard\nInstitutTelecom, TelecomParisTech, CNRS/LTCI\nfirstname.lastname@telecom-paristech.fr\nABSTRACT\nMusic Information Retrieval systems are commonly\nbuiltonafeatureextractionstage. Forapplicationsinvolv-\ning automatic classiﬁcation (e.g. speech/music discrimi-\nnation, music genre or mood recognition, ...), traditional\napproacheswillconsideralargesetofaudiofeaturestobe\nextractedonalargedataset. Insomecases,thiswillleadto\ncomputationally intensive systems and there is, therefore,\na strongneedforefﬁcientfeatureextraction.\nIn this paper, a new audio feature extraction software,\nYAAFE1, is presented and compared to widely used li-\nbraries. The main advantage of YAAFE is a signiﬁcantly\nlowercomplexityduetotheappropriateexploitationofre-\ndundancyin the featurecalculation. YAAFE remainseasy\nto conﬁgure and each feature can be parameterized inde-\npendently. Finally, the YAAFE frameworkand most of its\ncore feature library are released in source code under the\nGNU LesserGeneralPublicLicense.\n1. INTRODUCTIONAND RELATED WORK\nMostMusicalInformationRetrieval(MIR)systemsinclude\nan initial low-level or mid-level audio feature extraction\nstage. For applications involving automatic classiﬁcation\n(e.g. speech/music discrimination, music genre or mood\nrecognition,...),traditional approachesconsider a large set\nof audio features to be extracted on a large dataset, possi-\nbly combined with early temporal integration2. The im-\nportance of the feature extraction stage therefore justiﬁes\nthe increasing effort of the community in this domain and\na number of initiatives related to audio features extraction\nhaveemergedinthelasttenyears,withvariousobjectives.\nFor example, Marsyas is a software framework for au-\ndio processing [1], written in C++. It is designed as a\ndataﬂow processing framework, with the advantage of ef-\nﬁciency and low memory usage. Various building blocks\n1http://yaafe.sourceforge.net\n2Temporal integration is the process of summarizing features values\nover asegment or atexture window by computing mean, standard devia-\ntion, and/or any relevant statistical function. The term earlyrefers to an\nintegration performed before the classiﬁcation step.\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom useis granted without fee provided that copies are\nnotmadeordistributed forproﬁtorcommercialadvantageandthatcopies\nbear this notice and the full citation on theﬁrstpage.\nc/circlecopyrt2010 International Society for MusicInformation Retrieval.areavailabletobuildreal-timeapplicationsforaudioanal-\nysis, synthesis, segmentation, and classiﬁcation. Marsyas\niswidelyandsuccessfullyusedforvarioustasks.\nNote however, that the audio feature extraction ( bex-\ntractprogram) is only a small component of the whole\nMarsyas’s framework. Extracted features are written in\nARFF format, and can be directly reused with the WEKA\n[6] machine learning toolkit. Some classic features are\navailable out-of-the-box. The user can select which fea-\nturestoextract,butparameterslikeframesizeandoverlaps\nareglobal. Theuseralsohaslowcontrolupontemporalin-\ntegration.\nVAMP Plugins3is the speciﬁcation of a C++ Appli-\ncation Programming Interface (API) for plugins allowing\nextractionof low level featuresonaudiosignals. The very\npermissive BSD-style license permits the user to develop\nhis own plugin or application that uses existing plugins.\nSeveral plugin libraries have been developed by various\nresearch labs. VAMP Plugins comes with the Sonic Visu-\nalizer [2]application,a tool forviewingcontentsof music\naudioﬁles togetherwith extractedfeatures.\nBatch feature extraction using VAMP Plugins can be\ndonewith thecommandlinetoolSonicannotator4. Users\ncan declare features to extract in RDF ﬁles5with precise\ncontrolovereachfeatureparameter. Outputcanbewritten\nto CSV6or RDF ﬁles. Early temporal integration is lim-\nited to predeﬁned segment summaries, and it is not possi-\nble to perform temporal integration over overlapping tex-\nturewindows. VAMPPluginsAPIallowsthedevelopment\nof independent libraries, but prevents the development of\nnew plugins that would depend on already existing plug-\nins.\nAnotherexample,theMIRtoolbox,isaMatlabtoolbox\ndedicatedtomusicalfeatureextraction[3]. Algorithmsare\ndecomposed into stages, that the user can parameterize.\nFunctions are provided with a simple and adaptive syn-\ntax. The MIR toolbox relies on the Matlab environment\nand thereforebeneﬁtsfromalreadyexistingtoolboxesand\nbuilt-invisualizationcapabilities,butsuffersfrommemory\nmanagementlimitations.\nOtherprojectsalso exist. jAudio[5] isa java-basedau-\ndio feature extractor library, whose results are written in\n3http://vamp-plugins.org/ ,Queen Mary, University of\nLondon.\n4http://www.omras2.org/SonicAnnotator\n5Resource Description Framework isa semantic web standard.\n6CommaSeparated Values\n441\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)XML format. Maaate is a C++ toolkit that has been de-\nve\nlopedto analyzeaudioin the compressedfrequencydo-\nmain. FEAPI[4]isapluginAPIsimilartoVAMP.MPEG7\nalso provides Matlab and C codes for feature extraction.\nLately, MIR web serviceshave surfaced. For instance, the\nEcho Nest7provides a web service API for audio feature\nextraction. Input ﬁles are submitted through the web, and\ntheuser receivesa XMLdescription.\nWhatever the objectives are, the computational\nefﬁciency of the feature extraction process remains of ut-\nmostinterest. Itisalsoclearthatmanyfeaturessharecom-\nmon intermediate representations, such as spectrum mag-\nnitude, signal envelope and constant-Q transform. As al-\nreadyobservedfortheVAMPpluginswiththeFastFourier\nTransform (FFT), performances can be drastically\nimprovedif those representationsare computedonly once\nand this especially when large feature sets are extracted.\nNote also that this philosophy can be extended to the dif-\nferenttransformations(such asderivatives)of a givenfea-\nture.\nYAAFE has therefore been created both to get the best\nof the previous tools and to address their main limitations\ninsituationswherealargefeaturesetneedstobeextracted\nfrom large audio collections with different parameteriza-\ntions. In particular, YAAFE has been designed with the\nfollowingrequirementsinmind:\n•Computational efﬁciency with an appropriate exploita-\ntionoffeaturecalculationredundancies.\n•Usage simplicity with a particular attention to the fea-\nturedeclarationsyntax.\n•Capabilitytoprocessverylongaudioﬁles.\n•Storageefﬁciencyandsimplicity.\nThe paper is organized as follows: the architecture of\nYAAFE is detailed in section 2. A detailed benchmark is\nthen proposedin section 3. Finally, we suggest some con-\nclusionsandfutureworkin section4.\n2. YAAFE\n2.1 Overview\nYAAFE is a command line program. Figure1 describes\nhow YAAFE handles feature extraction. The user has to\nprovide the audio ﬁles and a feature extraction plan. The\nfeatureextractionplanisatextﬁlewheretheuserdeclares\nthe features to extract, their parameters and transforma-\ntions(see section2.2).\nTo take advantage of feature computation redundancy,\nYAAFE proceeds in two main stages. In a ﬁrst stage, a\nparser analyzes the feature extraction plan in order to ﬁnd\ncommon computational steps (implemented in C++ com-\nponents),and a reduceddataﬂow graphis produced. Then\nina secondstage,featureextractionisappliedtothegiven\naudioﬁlesaccordingtothereduceddataﬂowgraphandre-\nsultsarestoredinHDF5 ﬁles(seesection2.6).\n7http://echonest.com/Figure1. YAAFE internalsoverview.\nP\nythonispreferredtoC++forimplementingthefeature\nlibrary and the parser, because the Python object model\nand reﬂection allow more concise and readable code to be\nwritten. The dataﬂow engine and the component library\nhavebeendevelopedintheC++languageforperformance.\nYAAFE can be extended. Anyonecan create their own\nextensionwhichconsistsofafeaturelibraryanda compo-\nnentlibrary. Providedextensionsareloadedat runtime.\n2.2 Featureextractionplan\n2.2.1 Features\nYAAFE feature extraction plan is a text ﬁle that describes\nthe features to extract. Each line deﬁnes one feature, with\nthefollowingsyntax:\nname: Feature param=value param=value\nAnexample:\nm: MFCC blockSize=1024 stepSize=512\nz: ZCR blockSize=1024 stepSize=512\nl: LPC LPCNbCoeffs=10\nss: SpectralSlope\nThe example above will produce4 output datasets (see\nsection 2.6) named m,z,landss, which will hold fea-\ntures MFCC8, ZCR9, LPC10, SpectralSlope with given\nparameters. Missing parameters are automatically set to a\npredeﬁneddefaultvalue.\n2.2.2 Transformationsandtemporalintegration\nOne can also use spatial or temporal feature transforms,\nsuch as Derivate11, StatisticalIntegrator12, or SlopeInte-\n8Mel-Frequency Cepstral Coefﬁcients\n9Zero Crossing Rate\n10Linear Prediction Coefﬁcients\n11Derivate computes ﬁrstand/or second derivatives.\n12StatisticalIntegrator computes mean and standard deviation over the\ngiven frames.\n442\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Figure2. Automaticredundancyremovalperformedwhen\np\narsingfeatureextractionplan. Fr(N)boxesaredecompo-\nsitions into analysis frames of size N. Step size is omitted\nbutassumedequal.\ngrator13to enrich his feature extraction plan. For exam-\nple, a plan to extract MFCCs along with derivatives and\nperformearlyintegrationover60frameswilllooklikethis:\nm: MFCC > StatisticalIntegrator NbFrames=60\nm1: MFCC > Derivate DOrder=1 ...\n> StatisticalIntegrator NbFrames=60\nm2: MFCC > Derivate DOrder=2 ...\n> StatisticalIntegrator NbFrames=60\nObviously, m, m1, m2 are all based on MFCCcompu-\ntation which should be computed only once. This is dis-\ncussedin thenextsection.\n2.3 Featureplanparser\nWithin YAAFE, each feature is deﬁned as a sequence of\ncomputational steps. For example, MFCC is the succes-\nsionofsteps: Frames,FFT,MelFilterBank,Cepstrum. The\nsame applies to feature transforms and temporal integra-\ntors.\nAs shown in Figure2, the feature plan parser decom-\nposes each declaredfeatureinto steps and groupstogether\nidentical steps which have the same input into a reduced\ndirectedgraphofcomputationalsteps.\nThe reduced graph can be dumped into a dotﬁle, so\nan advanced user can discern how the features are really\ncomputed.\n2.4 Dataﬂowengine\nEachcomputationalstepis implementedina C++ compo-\nnentwhichperformscomputationonadatablock. Speciﬁc\n13SlopeIntegrator computes the slopeover the given frames.\nFigure 3. Temporally aligned frame decomposition for\nd\nifferentframesizesA andB, withsamestep sizes.\ncomponentsmanageaudioﬁlereadingandoutputﬁlewrit-\ning.\nThedataﬂow engine loads components, links them ac-\ncordingto the givendataﬂow graph, and managescompu-\ntations and data blocks. Reading, computations and writ-\ningisdoneblockbyblock,sothatarbitrarilylongﬁlescan\nbeprocessedwith alow memoryoccupation.\n2.5 Featuretimestampsalignment\nIna featureextractionplan,eachfeaturemayhaveitsown\nanalysis frame size and step size. Some features require\nlongeranalysisframesizesthan others. As we intendedto\nuseYAAFEasinputforclassiﬁcationsystems,wehaveen-\nsured that extracted features are temporally aligned. This\nisespeciallyimportantwithoperationsliketheConstant-Q\nTransform(CQT)thatmayhaveverylargeanalysisframes.\nYAAFEaddressesthisissueasfollows. Weassumethat\nwhena featureiscomputedoverananalysisframe,there-\nsultingvaluecorrespondsto thetimeoftheanalysisframe\ncenter. Then, beginningwith a frame centered on the sig-\nnal start (left padded with zeros) ensures that all features\nwith the same step size will be temporally aligned (see\nFigure3).\nA feature may also have an intrinsic time-delay. For\nexample, when applying a derivative ﬁlter, we want the\noutput value to be aligned with the center of the deriva-\ntive ﬁlter. The design of YAAFE ensures that this is han-\ndled properly and that output features will be temporally\naligned.\nYAAFEonlydealswithequidistantlysampledfeatures.\nHowever, some features like onsets have a natural repre-\nsentation which is event-based. In the current version,\nevent-basedfeatures are represented as equidistantly sam-\npled features for which the ﬁrst dimension is a boolean\nvaluedenotingthepresenceofanevent.\n2.6 Outputformat\nYAAFEoutputsresultsinHDF5ﬁles14. Otheroutputfor-\nmats will be added in the future. The choice of the HDF5\nformathasinitiallybeenmotivatedbystoragesizeandI/O\nperformance. HDF5 allows for on-the-ﬂy compression.\nResults are stored as double precision ﬂoating point num-\nbershencewithnoprecisionloss.\nHDF5 is a binary format designed for efﬁcient storage\noflargeamountsofscientiﬁcdata. HDF5ﬁlescanberead\n14HierarchicalDataFormat, http://www.hdfgroup.org/HDF5/\n443\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)in the Matlab environment through built-in functions15,\na\nnd in the Python environment with the h5py package16.\nHDF5ﬁlesareplatformindependent,sotheycanbeeasily\nshared.\nA HDF5 ﬁle can hold several datasets organizedinto a\nhierarchicalstructure. Adatasetcanbeatablewithseveral\ncolumns(orﬁelds)ofdifferentdatatypes,orsimplya2-D\nmatrix of a speciﬁc data type. Attributes can be attached\ntodatasets, anattributehasanameanda valueofanydata\ntype.\nYAAFEcreatesoneHDF5ﬁleforeachinputaudioﬁle.\nForeachfeaturedeclaredinthefeatureextractionplan,one\ndataset is created, with some attributes attached such as\nthe feature deﬁnition, the frame size, the step size and the\nsamplerate.\n2.7 AvailabilityandLicense\nThe YAAFE framework and a core feature library are re-\nleased together under the GNU Lesser General Public Li-\ncense, so that it can freely be reused as a component of\na bigger system. The core feature library contains sev-\neral spectral features, Mel-Frequencies Cepstrum Coefﬁ-\ncients,Loudness,Autocorrelation,LinearPredictionCoef-\nﬁcients, Octave Band Signal Intensities, OBSI ratios, am-\nplitude modulation (tremolo and graininess description),\ncomplex domain onset detection [7], Zero Crossing Rate.\nDerivative and Cepstral transforms as well as statistical,\nslope and histogram early integrators are also provided.\nYAAFE is available for Linux platforms, source code can\nbedownloaded17.\nA separate feature library will be available in binary\nversion and for non commercial use only. It will provide\nConstant-Q Transform, Chromas [8], Chord detection [9],\nOnset detection [10], Beat histogram summary [11]. An\nimplementation of CQT with normalization and kernels\ntemporalsynchronicityimprovements[12]from reference\nimplementation18isproposed.\n3. BENCHMARK\nWe have run a small benchmarkto compare YAAFE with\nMarsyas’bextractandSonicAnnotator. Theobjectiveisto\ncompare the design of the three system, and not the algo-\nrithmsused to computefeature. We chosefew similar and\nwell-deﬁned features, available for the three systems for\nwhich we compared CPU time, memory occupation and\noutputsize whenextractingthosefeaturesonthesameau-\ndiocollection.\n3.1 Protocol\nWe choseto extractthe followingfeatures: MFCC (13co-\nefﬁcients), spectral centroid,spectral rolloff,spectral crest\nfactor, spectral ﬂatness, and zero crossing rate. Features\n15See thehdf5info,hdf5read andhdf5write functions. YAAFE also\nprovide useful scripts to directly load feature data into amatrix.\n16http://code.google.com/p/h5py/\n17http://yaafe.sourceforge.net\n18B.Blankertz, “The Constant Q Transform”, http://wwwmath.uni-\nmuenster.de/logik/Personen/blankertz/constQ/constQ.htmlS.A. Marsyas YAAFE\nCPU time 52m05s 24m21s 6m34s\nRAM used 14.0Mbs 10.6Mbs 15.5Mbs\nOutputformat CSV ARFF HDF5\nOutputsize 1.74Gbs 2.7Gbs 1.22Gbs\nFeaturedim. 16 16(32) 19\nTable 1. Feature extraction with Sonic Annotator with\nV\nAMP libxtract plugins, Marsyas’s bextract and YAAFE.\nAllfeaturesareextractedsimultaneously. Audiocollection\nis40hoursof32KHz monowav ﬁles.\nFeature S.A. Marsyas YAAFE\nMFCC 25m06s 19m28s 2m22s\nCentroid 12m04s 15m42s 3m55s\nRolloff 12m11s 15m51s 3m14s\nZCR 3m41s 10m20s 0m57s\nTotal 53m02s 61m21s 10m28s\nTable 2. CPU times for single feature extraction on the\ns\name collectionasTable1.\nlike chroma or beat detection have been avoided because\ntheassociatedalgorithmscanbeverydifferent. Inthecase\nof Sonic Annotator, all features are available in the Vamp\nlibxtract plugins19[13]. Early temporal integrationis not\ncomputed.\nWe ranthefeatureextractionoverabout40hoursof32\nKHzmonowavﬁles(8.7Gbs). Thecollectioniscomposed\nof 80 radio excerpts of about 30mn each. The measures\nhavebeendoneonaIntelCore2Duo3GHzmachine,with\n4GbsofRAM,undertheDebianLennyoperatingsystem.\nWecheckedthatallsystemsusedonecoreonly. TheRAM\nusedhasbeenmeasuredwiththe psmem.py script20.\nWe ﬁrst ran the benchmarkmeasuringthe extractionof\nall features simultaneously. Then we ran the benchmarka\nsecondtimemeasuringtheextractionofeachfeatureinde-\npendently.\n3.2 Results\nThe results are described in Table1 and Table2. It is im-\nportanttonotesomedifferencesbetweenthe3systemsthat\ninﬂuencetheresults. Firstly,wecouldnotpreventMarsyas\nfromperformingtemporalintegration,sowereducedinte-\ngration length to 1. Consequently, the output generated\nby Marsyas has 32 columns: 16 columns of feature data\n(mean)and16columnsofzeros(standarddeviation). This\nexplains why Marsyas has a larger output size. Secondly,\nYAAFEextractsspectralspread,skewnessandkurtosisto-\ngetherwiththespectralcentroid. Thisexplainswhyoutput\nfeature dimension is 19 for YAAFE and 16 for other sys-\ntems.\nDue to those differences the measures must be taken\nwith caution. We can say that all systems performedwell.\n19The VAMP libxtract plugins rely on the libxtract library:\nhttp://libxtract.sourceforge.net/\n20http://www.pixelbeat.org/scripts/ps mem.py\n444\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)YAAFE\nCPU time 11m15s\nRAM used 30.3Mbs\nOutputformat HDF5\nOutputsize 0.64Gbs\nFeaturedim. 288\nTable 3. Large feature set extractionwith YAAFE. Audio\nc\nollectionis 40hoursof32KHz monowavﬁles.\nThey all succeed at extracting features over audio ﬁles of\n30minuteslengthandwithlowmemoryoccupation.\nThesumofsingleextractiontimesinTable2compared\nto the extractiontime in Table1 showsthat SonicAnnota-\ntor does not exploit computation redundancy. The VAMP\nplugin API allows for computing feature in the frequency\ndomain, but this is not done by Vamp libxtract plugins.\nThat explains why Sonic Annotator requires more CPU\ntime thanothers.\nMarsyas performance clearly suffers from writing 16\ncolumn of zeros. For the evaluated task, the CPU times in\nTable1showthatYAAFEtendstobefasterthanMarsyas.\nAsSonicAnnotatorstoredthetimestampineachoutput\nﬁles(oneperfeature),andhalfofMarsyas’outputisaddi-\ntionalzeros,wecansaythatSonicAnnotatorandMarsyas\noutputs are roughly equivalent in space. This is not a sur-\nprise as both CSV and ARFF format are text formats. Us-\ning HDF5 format, YAAFE stores more feature data, with\nnoprecisionloss, usinglessspace.\n3.3 Extractingmanyfeatures\nYAAFE is designed for extracting a large number of fea-\nturessimultaneously. Tocheckhowitperformsinsuchsit-\nuation we ran YAAFE a second time under the same con-\nditionsbutwith alargerfeatureextractionplan.\nIn this run, we extracted MFCCs, various spectral fea-\ntures, loudness, loudness-basedsharpness and spread, and\nzero crossing rate. For each feature except zero crossing\nrate, we added ﬁrst and second derivatives. Then we per-\nformedearlytemporalintegrationbycomputingmeanand\nstandard deviation over sliding windows of 1 second with\na stepof0.5second. Thetotaloutputdimensionis288.\nThe results are presented in Table3. It should be em-\nphasized that temporal integration is done, so the output\nsize is much smaller than in the previous run. As a larger\nfeature set is extracted, the dataﬂow graph is larger and\nuses more RAM. The CPU time shows that YAAFE re-\nmainsveryefﬁcientinthissituation.\n4. CONCLUSIONS ANDFUTUREWORK\nIn this paper, a new audio feature extraction software,\nYAAFE, is introduced. YAAFE is especially efﬁcient in\nsituations where many features are simultaneously\nextracted over large audio collections. To achieve this,\nthefeaturecomputationredundanciesareappropriatelyex-\nploited in a two step extraction process. First, the featureextraction plan is analyzed, each feature is decomposed\ninto computational steps and a reduced dataﬂow graph is\nproduced. Then,adataﬂowengineprocessescomputations\nblockbyblockoverthegivenaudioﬁles.\nYAAFE remains easy to use. The feature extraction\nplan is a text ﬁle where the user can declare features to\nextract,transformationsand earlytemporalintegrationac-\ncordingto a verysimple syntax. YAAFE hasalreadybeen\nused in Quaero project internal evaluation campaigns for\nthe music/speech discriminationand musical genrerecog-\nnitiontasks.\nFuture plans include the extension of the toolbox with\nadditional high level features such as fundamental\nfrequency estimator, melody detection and tempo estima-\ntorandtheextensiontoalternativeoutputformats.\n5. ACKNOWLEDGMENT\nThis work was done as part of the Quaero Programme,\nfundedbyOSEO, FrenchState agencyforinnovation.\n6. REFERENCES\n[1] G. Tzanetakis, and P. Cook: “MARSYAS: A frame-\nwork for audio analysis,” Org. Sound , Vol. 4, No. 3,\npp.169-175,1999.\n[2] C. Cannam, C. Landone, M. Sandler, and J. Bello:\n“The Sonic Visualiser: A Visualisation Platform For\nSemantic Descriptiors From Musical Signals,” Pro-\nceedings of International Conference on Musical In-\nformationRetrieval ,Victoria,Canada,2006.\n[3] O. Lartillot, and P. Toiviainen: “A MATLAB TOOL-\nBOX FOR MUSICAL FEATURE EXTRACTION\nFROM AUDIO,” Proceedings of the International\nConferenceonDigitalAudioEffects(DAFx’07) ,2007.\n[4] A. Lerch, G. Eisenberg, and K. Tanghe: “FEAPI, A\nLOW LEVEL FEATURES EXTRACTION PLUGIN\nAPI,”Proceedings of the International Conference on\nDigitalAudioEffects ,(DAFx’05),2005.\n[5] D. McEnnis, C. McKay, I. Fujinaga, and P. Depalle:\n“jAudio: A feature extraction library,” Proceedings of\ntheInternationalConferenceonMusicInformationRe-\ntrieval,pp.600–603,2005.\n[6] Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard\nPfahringer, Peter Reutemann, Ian H. Witten: “The\nWEKA Data MiningSoftware: An Update”, SIGKDD\nExplorations ,Vol.11,Issue1,2009\n[7] C.Duxbury et al., “Complex domain onset detection\nfor musical signals”, Proceedings of the International\nConference on Digital Audio Effects , (DAFx’03),\n2003.\n[8] J.P. Bello and J. Pickens: “A Robust Mid-level Rep-\nresentation for Harmonic Content in Music Signals.”,\nProceedings of the 6th International Conference on\nMusicInformationRetrieval ,(ISMIR-05),2005.\n445\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)[9] L.Oudre,Y.Grenier,C.Fevotte: “TEMPLATE-BASED\nCH\nORD RECOGNITION : INFLUENCE OF THE\nCHORD TYPES”, Proceedings of the International\nConferenceonMusic InformationRetrieval ,2009\n[10] M.Alonso, G.Richard. B.David: “EXTRACTING\nNOTE ONSETS FROM MUSICAL RECORDINGS”,\nInternational Conference on Multimedia and Expo\n(IEEE-ICME’05),2005.\n[11] G.Tzanetakis,“MusicalGenreClassiﬁcationofAudio\nSignals”, IEEETransactionsonspeechandaudiopro-\ncessing,vol.10,No.5,2002.\n[12] J.Prado, “Transform´ ee ` a Q constant”, technical report\n2010D004 , http://service.tsi.telecom-paristech.fr/cgi-\nbin/valipub download.cgi?dId=185, Institut TELE-\nCO\nM,TELECOMParisTech,CNRS LTCI, 2010.\n[13] J. Bullock, “Libxtract: A lightweight library for audio\nfeature extraction,”in Proceedingsof the International\nComputerMusicConference,2007.\n446\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "Approximate Note Transcription for the Improved Identification of Difficult Chords.",
        "author": [
            "Matthias Mauch",
            "Simon Dixon"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1416598",
        "url": "https://doi.org/10.5281/zenodo.1416598",
        "ee": "https://zenodo.org/records/1416598/files/MauchD10.pdf",
        "abstract": "The automatic detection and transcription of musical chords from audio is an established music computing task. The choice of chord profiles and higher-level time-series modelling have received a lot of attention, resulting in",
        "zenodo_id": 1416598,
        "dblp_key": "conf/ismir/MauchD10",
        "keywords": [
            "automatic detection",
            "transcription",
            "musical chords",
            "audio",
            "music computing",
            "task",
            "chord profiles",
            "higher-level time-series modelling",
            "attention",
            "resulting in"
        ],
        "content": "APPROXIMATE NOTE TRANSCRIPTION FOR THE IMPROVED\nIDENTIFICATION OF DIFFICULT CHORDS\nMatthias Mauch and Simon Dixon\nQueen Mary University of London, Centre for Digital Music\nfmatthias.mauch, simon.dixong@elec.qmul.ac.uk\nABSTRACT\nThe automatic detection and transcription of musical\nchords from audio is an established music computing task.\nThe choice of chord proﬁles and higher-level time-series\nmodelling have received a lot of attention, resulting in\nmethods with an overall performance of more than 70% in\nthe MIREX Chord Detection task 2009. Research on the\nfront end of chord transcription algorithms has often con-\ncentrated on ﬁnding good chord templates to ﬁt the chroma\nfeatures. In this paper we reverse this approach and seek\nto ﬁnd chroma features that are more suitable for usage in\na musically-motivated model. We do so by performing a\nprior approximate transcription using an existing technique\nto solve non-negative least squares problems (NNLS). The\nresulting NNLS chroma features are tested by using them\nas an input to an existing state-of-the-art high-level model\nfor chord transcription. We achieve very good results of\n80% accuracy using the song collection and metric of the\n2009 MIREX Chord Detection tasks. This is a signiﬁcant\nincrease over the top result (74%) in MIREX 2009. The na-\nture of some chords makes their identiﬁcation particularly\nsusceptible to confusion between fundamental frequency\nand partials. We show that the recognition of these diffcult\nchords in particular is substantially improved by the prior\napproximate transcription using NNLS.\nKeywords: chromagram, chord extraction, chord de-\ntection, transcription, non-negative least squares (NNLS).\n1. INTRODUCTION\nChords are not only of theoretical interest for the under-\nstanding of Western music. Their practical relevance lies\nin the fact that they can be used for music classiﬁcation,\nindexing and retrieval [2] and also directly as playing in-\nstructions for jazz and pop musicians. Automatic chord\ntranscription from audio has been the subject of tens of\nresearch papers over the past few years. The methods usu-\nally rely on the low-level feature called chroma, which is a\nmapping of the spectrum to the twelve pitch classes C,...,B,\nin which the pitch height information is discarded. Never-\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2010 International Society for Music Information Retrieval.theless, this feature is often sufﬁcient to recognise chords\nbecause chord labels themselves remain the same whatever\noctave the constituent notes are played in. An exception is\nthe lowest note in a chord, the bass note, whose identity\nis indeed notated in chord labels. Some research papers\nhave taken advantage of the additional information con-\nveyed by the bass note by introducing special bass chro-\nmagrams [18, 12] or prior bass note detection [21].\nThere is much scope in developing musical models to\ninfer the most likely chord sequence from the chroma fea-\ntures. Many approaches use models of metric position\n[16], the musical key [8, 21], or combinations thereof [12],\nas well as musical structure [13], to increase the accuracy\nof the chord transcription. Although in this work we will\nalso use such a high-level model, our main concern will be\nthe low-level front end.\nMany previous approaches to chord transcription have\nfocussed on ﬁnding a set of chord proﬁles, each chord pro-\nﬁle being a certain chroma pattern that describes best the\nchroma vectors arising while the chord is played. It usu-\nally includes the imperfections introduced into the chro-\nmagram by the upper partials of played notes. The shape\nof each pattern is either theoretically motivated (e.g. [15])\nor learned, usually using (semi-) supervised learning (e.g.\n[8, 9]). A few approaches to key and chord recogni-\ntion also emphasise the fundamental frequency compo-\nnent before producing the chromagrams [5, 18] or use a\ngreedy transcription step to improve the correlation of the\nchroma with true fundamental frequencies [19]. Emphasis-\ning fundamental frequencies before mapping the spectrum\nto chroma is preferable because here all spectral informa-\ntion can be used to determine the fundamental frequencies\n–before discarding the octave information.\nHowever, in order to determine the note activation, the\nmentioned approaches use relatively simple one-step trans-\nforms, a basic form of approximate transcription. A dif-\nferent class of approaches to approximate transcription as-\nsumes a more realistic linear generative model in which the\nspectrum (or a log-frequency spectrum) Yis considered to\nbe approximately represented by a linear combination of\nnote proﬁles in a dictionary matrix E, weighted by the ac-\ntivation vector x, withx\u00150:\nY\u0019Ex (1)\nThis model conforms with our physical understanding of\nhow amplitudes of simultaneously played sounds add up1.\n1Like the one-step transforms, the model assumes the absence of si-\n135\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Approaches to ﬁnding the activation vector xin (1) dif-\nfer from the one-step transforms in that they involve it-\nerative re-weighting of the note activation values [1]. To\nour knowledge, such a procedure has not been used to\ngenerate chromagrams or otherwise conduct further auto-\nmatic harmony analysis. Unlike traditional transcription\napproaches, we are not directly interested in note events,\nand the sparsity constraints required in [1] need not be\ntaken into account. This allows us to use a standard proce-\ndure called non-negative least squares (NNLS), as will be\nexplained in Section 2.\nThe motivation for this is the observation that the par-\ntials of the notes played in chords compromise the correct\nrecognition of chords. The bass note in particular usually\nhas overtones at frequencies where other notes have their\nfundamental frequencies. Interestingly, for the most com-\nmon chord type in Western music, the major chord (in root\nposition), this does not pose a serious problem, because the\nfrequencies of the ﬁrst six partials of the bass note coincide\nwith the chord notes: for example, a C major chord (con-\nsisting of C, E and G) in root position has the bass note C,\nwhose ﬁst six partials coincide with frequencies at pitches\nC, C, G, C, E, G. Hence, using a simple spectral mapping\nworks well for major chords. But even just considering the\nﬁrst inversion of the C major chord (which means that now\nE is the the bass note), leads to a dramatically different sit-\nuation: the bass note’s ﬁrst six partials coincide with E, E,\nB, E, G], B – of which B and G] are deﬁnitely not part of\nthe C major triad. Of course, the problem does not only\napply to the bass note, but to all chord notes2.\nThis is a problem that can be eliminated by a perfect\nprior transcription because no partials would interfere with\nthe signal. Section 2 focusses mainly on describing our ap-\nproach to an approximate transcription using NNLS, and\nalso gives an outline of the high-level model we use. In\nSection 3 we demonstrate that the problem does indeed\nexist and show that the transcription capabilities of the\nNNLS algorithm can improve the recognition of the af-\nfected chords. We give a brief discussion of more general\nimplications and future work in Section 4, before present-\ning our conclusions in Section 5.\n2. METHOD\nThis section is concerned with the technical details of\nour method. Most importantly, we propose the use of\nNNLS-based approximate note transcription, prior to the\nchroma mapping, for improved chord recognition. We\ncall the resulting chroma feature NNLS chroma. To ob-\ntain these chroma representations, we ﬁrst calculate a\nlog-frequency spectrogram (Subsection 2.2), pre-process\nit (Subsection 2.3) and perform approximate transcrip-\ntion using the NNLS algorithm (Subsection 2.4). This\ntranscription is then wrapped to chromagrams and beat-\nsynchronised (Section 2.5). Firstly, however, let us brieﬂy\nconsider the high-level musical model which takes as input\nnusoid cancellation.\n2For example, a major third will create some energy at the major 7th\nthrough its third partial.\nmetric pos.keychordbassbass chromatreble chromaMi−1Ki−1Ci−1Bi−1Xbsi−1Xtri−1MiKiCiBiXbsiXtri\n1Figure 1: High-level dynamic Bayesian network, repre-\nsented as two slices corresponding to two generic consecu-\ntive beats. Random variables are shown as nodes, of which\nthose shaded grey are observed, and the arrows represent\ndirect dependencies (inter-slice arrows are dashed).\nthe chroma features, and which we use to test the effect of\ndifferent chromagrams on chord transcription accuracy.\n2.1 High-level Probabilistic Model\nWe use a modiﬁcation of a dynamic Bayesian network\n(DBN) for chord recognition proposed in [10], which in-\ntegrates in a single probabilistic model the hidden states\nof metric position, key, chord, and bass note, as well as\ntwo observed variables: chroma and bass chroma. It is an\nexpert model whose structure is motivated by musical con-\nsiderations; for example, it enables to model the tendency\nof the bass note to be present on the ﬁrst beat of a chord,\nand the tendency of the chord to change on a strong beat.\nThe chord node distinguishes 121 different states: 12 for\neach of 10 chord types (major, minor, major in ﬁrst inver-\nsion, major in second inversion, major 6th, dominant 7th,\nmajor 7th, minor 7th, diminished and augmented) and one\n“no chord” state. With respect to the original method, we\nhave made some slight changes in the no chord model and\nthe metric position model3. The DBN is implemented us-\ning Murphy’s BNT Toolbox [14], and we infer the jointly\nmost likely state sequence in the Viterbi sense.\n2.2 Log-frequency Spectrum\nWe use the discrete Fourier transform with a frame length\nof 4096 samples on audio downsampled to 11025 Hz. The\nDFT length is the shortest that can resolve a full tone in the\nbass region around MIDI note 444, while using a Ham-\n3Theno chord model has been modiﬁed by halving the means of the\nmultivariate Gaussian used to model its chroma, and the metric position\nmodel is now fully connected, i.e. the same low probability of 0.0167 is\nassigned to missing 1, 2 or three beats.\n4Smaller musical intervals in the bass region occur extremely rarely.\n136\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)ming window. We generate a spectrogram with a hop size\nof 2048 frames (\u00190.05s).\nWe map the magnitude spectrum onto bins whose cen-\ntres are linearly-spaced in log frequency, i.e. they corre-\nspond to pitch (e.g. [17]), with bins spaced a third of a\nsemitone apart. The mapping is effectuated using cosine\ninterpolation on both the linear and logarithmic scales:\nﬁrst, the DFT spectrum is upsampled to a highly over-\nsampled frequency representation, and then this intermedi-\nate representation is mapped to the desired log-frequency\nrepresentation. The two operations can be performed as\na single matrix multiplication. This calculation is done\nseparately on all frames of a spectrogram, yielding a log-\nfrequency spectrogram Y= (Yk;m).\nAssuming equal temperament, the global tuning of the\npiece is now estimated from the spectrogram. Rather than\nadjusting the dictionary matrix we then update the log-\nfrequency spectrogram via linear interpolation, such that\nthe centre bin of every semitone corresponds to the cor-\nrect frequency with respect to the estimated tuning [10].\nThe updated log-frequency spectrogram Yhas 2561=3-\nsemitone bins (about 7 octaves), and is hence much smaller\nthan the original spectrogram. The reduced size enables us\nto model it efﬁciently as a sum of idealised notes, as will\nbe explained in Subsection 2.4.\n2.3 Pre-processing the Log-frequency Spectrum\nWe use three different kinds of pre-processing on the log-\nfrequency spectrum:\no: original – no pre-processing,\nsub : subtraction of the background spectrum [3], and\nstd: standardisation: subtraction of the background spec-\ntrum and division by the running standard deviation.\nTo estimate the background spectrum we use the running\nmean\u0016k;m, which is the mean of a Hamming-windowed,\noctave-wide neighbourhood (from bin k\u000018tok+ 18).\nThe values at the edges of the spectrogram, where the full\nwindow is not available, are set to the value at the closest\nbin that is covered. Then, \u0016k;mis subtracted from Yk;m,\nand negative values are discarded (method sub). Addition-\nally dividing by the respective running standard deviation\n\u001bk;m, leads to a running standardisation (method std). This\nis similar to spectral whitening (e.g. [6]) and serves to dis-\ncard timbre information. The resulting log-frequency spec-\ntrum of both pre-processing methods can be calculated as\nY\u001a\nk;m=(Yk;m\u0000\u0016k;m\n\u001b\u001a\nk;mifYk;m\u0000\u0016k;m>0\n0 otherwise,(2)\nwhere\u001a= 0 or\u001a= 1 for the cases subandstd, respec-\ntively.\n2.4 Note Dictionary and Non-Negative Least Squares\nIn order to decompose a log-frequency spectral frame into\nthe notes it has been generated from, we need two basic in-gredients: a note dictionary E, describing the assumed pro-\nﬁle of (idealised) notes, and an inference procedure to de-\ntermine the note activation patterns that result in the closest\nmatch to the spectral frame.\nWe generate a dictionary of idealised note proﬁles in the\nlog-frequency domain using a model with geometrically\ndeclining overtone amplitudes [5],\nak=sk\u00001(3)\nwhere the parameter s2(0;1)inﬂuences the spectral\nshape: the smaller the value of s, the weaker the higher\npartials. Gomez [5] favours the parameter s= 0:6for her\nchroma generation, in [13] s= 0:9 was used. We will test\nboth possibilities, and add a third possibility, where sis\nlinearly spaced (LS) between s= 0:9 for the lowest note\nands= 0:6for the highest note. This is motivated by the\nfact that resonant frequencies of musical instruments are\nﬁxed, and hence partials of notes with higher fundamental\nfrequency are less likely to correspond to a resonance. In\neach of the three cases, we create tone patterns over seven\noctaves, with twelve tones per octave: a set of 84 tone pro-\nﬁles. The fundamental frequencies of these tones range\nfrom A0 (at 27.5 Hz) to G]6 (at approximately 3322 Hz).\nEvery note proﬁle is normalised such that the sum over all\nthe bins equals unity. Together they form a matrix E, in\nwhich every column corresponds to one tone.\nWe assume now that—like in Eqn. (1)—the individual\nframes of the log-frequency spectrogram Yare generated\napproximately as a linear combination Y\u0001;m\u0019Exof the\n84 tone proﬁles. The problem is to ﬁnd a tone activation\npatternxthat minimises the Euclidian distance\njjY\u0001;m\u0000Exjj (4)\nbetween the linear combination and the data, with the con-\nstraintx\u00150, i.e. all activations must be non-negative.\nThis is a well-known mathematical problem called the non-\nnegative least squares (NNLS) problem. Lawson and Han-\nson [7] have proposed an algorithm to ﬁnd a solution, and\nsince (in our case) the matrix Ehas full rank and more\nrows than columns, the solution is also unique. We use\nMATLAB’s implementation of this algorithm. Again, all\nframes are processed separately, and we ﬁnally obtain an\nNNLS transcription spectrum Sin which every column\ncorresponds to one audio frame, and every row to one\nsemitone. Alternatively, we can choose to omit the approx-\nimate transcription step and copy the centre bin of every\nsemitone in Yto the corresponding bin of S[17].\n2.5 Chroma, Bass Chroma and Beat-synchronisation\nThe DBN we use to estimate the chord sequence requires\ntwo different kinds of chromagram: one general-purpose\nchromagram that covers all pitches, and one bass-speciﬁc\nchromagram that is restricted to the lower frequencies. We\nemphasise the respective regions of the semitone spectrum\nby multiplying by the pitch-domain windows shown in\nFigure 2, and then map to the twelve pitch classes by sum-\nming the values of the respective pitches.\n137\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)log-freq. NNLS\nspectrum no NNLS s= 0:6s= 0:9 LS\no 38.6 43.9 43.1 47.5\nsub 74.5 74.8 71.5 73.8\nstd 79.0 80.0 76.5 78.6\n(a) MIREX metric – correct overlap in %log-freq. NNLS\nspectrum no NNLS s= 0:6s= 0:9 LS\no 31.0 35.1 33.9 37.4\nsub 58.1 58.2 56.1 57.3\nstd 61.3 62.7 62.0 63.3\n(b) metric using all chord types – correct overlap in %\nTable 1: Results of the twelve methods in terms of the percentage of correct overlap. Table (a) shows the MIREX metric,\nwhich distinguishes only 24 chords and a “no chord” state, Table (b) is shows a ﬁner metric that distinguishes 120 chords\nand a “no chord” state.\n20 30 40 50 60 70 80 90100 11001\nMIDI notefactor\nFigure 2: Proﬁles applied to the log-frequency spectrum\nbefore the mapping to the main chroma (solid) and bass\nchroma (dashed).\nBeat-synchronisation is the process of summarising\nframe-wise features that occur between two beats. We use\nthe beat-tracking algorithm developed by Davies [4], and\nobtain a single chroma vector for each beat by taking the\nmedian (in the time direction) over all the chroma frames\nbetween two consecutive beat times. This procedure is ap-\nplied to both chromagrams, for details refer to [10]. Fi-\nnally, each beat-synchronous chroma vector is normalised\nby dividing it by its maximum norm. The chromagrams\ncan now be used as observations in the DBN described in\nSection 2.1.\n3. EXPERIMENTS AND RESULTS\nOur test data collection consists of the 210 songs used\nin the 2009 MIREX Chord Detection task, together with\nthe corresponding ground truth annotations [11]. We run\n12 experiments varying two parameters: the preprocessing\ntype (o, suborstd, see Section 2.3), and the kind of NNLS\nsetup used (s = 0:6,s= 0:9, LS, or direct chroma map-\nping, see Section 2.4).\n3.1 Overall Accuracy\nThe overall accuracy of the 12 methods in terms of the\npercentage of correct overlap\nduration of correctly annotated chords\ntotal duration\u0002100%\nis displayed in Table 1: Table 1a shows results using the\nMIREX metric which distinguishes only two chord types\nand the “no chord” label, and 1b shows results using a ﬁnerevaluation metric that distinguishes all 121 chord states\nthat the DBN can model; see also [10, Chapter 4].\nWhen considering the MIREX metric in Table 1a it\nis immediately clear that one of the decisive factors has\nbeen the spectral standardisation: all four stdmethods\nclearly outperform the respective analogues with subpre-\nprocessing or no preprocessing. We performed a 95%\nFriedman multiple comparison analysis on the song-wise\nresults of the stdmethods: except for the difference be-\ntween no NNLS and LS all differences are signiﬁcant, and\nin particular the NNLS method using s= 0:6 signiﬁcantly\noutperforms all other methods, achieving 80% accuracy.\nWith ap-value of 10\u000010in the Friedman test, this is also a\nhighly signiﬁcant increase of nearly 6 percentage points\nover the 74% accuracy achieved by the highest scoring\nmethod [20] in the 2009 MIREX tasks.\nIn Table 1b the results are naturally lower, because a\nmuch ﬁner metric is used. Again, the stdvariants per-\nform best, but this time the NNLS chroma with the linearly\nspacedshas the edge, with 63% accuracy. (Note that this\nis still higher than three of the scores in the MIREX task\nevaluated with the MIREX metric.) According to a 95%\nFriedman multiple comparison test, the difference between\nthe methods std-LS and std-0.6 is not signiﬁcant. However,\nboth perform signiﬁcantly better than the method without\nNNLS for this evaluation metric which more strongly em-\nphasises the correct transcription of difﬁcult chords.\nThe reason for the very low performance of the ometh-\nods without preprocessing is the updated model of the “no\nchord” state in the DBN. As a result, many chords in nois-\nier songs are transcribed as “no chord”. However, this\nproblem does not arise in the subandstdmethods, where\nthe removal of the background spectrum suppresses the\nnoise. In these methods the new, more sensitive “no chord”\nmodel enables very good “no chord” detection, as we will\nsee in the following subsection.\n3.2 Performance of Individual Chords\nRecall that our main goal, as stated in the introduction, is to\nshow an improvement in those chords that have the prob-\nlem of bass-note induced partials whose frequencies do not\ncoincide with those of the chord notes. Since these chords\nare rare compared to the most frequent chord type, ma-\njor, differences in the mean accuracy are relatively small\n(compare the stdmethods with NNLS, s= 0:6, and with-\nout in Table 1a). For a good transcription, however, all\n138\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)0102030405060708090Naugdimmin7maj77maj6maj/5maj/3minmaj71.90\n71.5319.1128.3624.0641.7511.4210.4332.0225.3867.78\npercentage of correct overlapchord type(a)stdmethod without NNLS\nï15ï10ï5051015Naugdimmin7maj77maj6maj/5maj/3minmajï0.43\nï2.4711.832.872.46ï2.744.458.4912.129.484.28\ndifference in percentage pointschord type (b) improvement of stdwith NNLS chroma (s = 0:6) over baseline\nstdmethod.\nFigure 3: Percentage of correct overlap of individual chord types.\nchords are important, and not only those that are most fre-\nquently used. First of all we want to show that the prob-\nlem does indeed exist and is likely to be attributed to the\npresence of harmonics. As a baseline method we choose\nthe best-performing method without NNLS chroma (std ),\nwhose performance on individual chords is illustrated in\nFigure 3a. As expected, it performs best on major chords,\nachieving a recognition rate of 72%. This is rivalled only\nby the “no chord” label N(also 72%), and the minor chords\n(68%). All other chords perform considerably worse. This\ndifference in performance may of course have reasons\nother than the bass note harmonics, be it an implicit bias in\nthe model towards simpler chords, or differences in usage\nbetween chords. There is, however, compelling evidence\nfor attributing lower performance to the bass note partials,\nand it can be found in the chords that differ from the major\nchord in only one detail: the bass note. These are the major\nchord inversions (denoted maj/3, and maj/5): while the\nchord model remains the same otherwise, performance for\nthese chords is around 40 percentage points worse than for\nthe same chord type in root position.\nTo ﬁnd out whether the NNLS methods suffer less from\nthis phenomenon, we compare the baseline method dis-\ncussed above to an NNLS method (std, with the chord dic-\ntionary parameter s= 0:6). The results of the compari-\nson between the baseline method and this NNLS method\ncan be seen in Figure 3b. Recognition rates for almost all\nchords have improved by a large margin, and we would like\nto highlight the fact that the recognition of major chords in\nsecond inversion (maj/5) has increased by 12 percentage\npoints. Other substantial improvements can be found for\naugmented chords (also 12 percentage points), and major\nchords in ﬁrst inversion (9 percentage points). These are all\nchords in which even the third harmonic of the bass note\ndoes not coincide with the chord notes (the ﬁrst two always\ndo), which further assures us that our hypothesis was cor-\nrect. Note that, conversely, the recognition of major chords\nhas remained almost stable, and only two chords, major\n7th and the “no chord” label, show a slight performance\ndecrease (less than 3 percentage points).4. DISCUSSION\nWhile the better performance of the difﬁcult chords is eas-\nily explainable by approximate transcription, there is some\nscope in researching why the major 7th chord performed\nslightly worse in the method using NNLS chroma. Our\nhypothesis is that the recognition of the major 7th chord\nactually beneﬁts from the presence of partials: not only\ndoes the bass note emphasise the chord notes (as it does in\nthe plain major chord), but the seventh itself is also empha-\nsised by the third harmonic of the third; e.g. in a C major\n7th chord (C, E, G, B), the E’s third harmonic would em-\nphasise the B. In future work, detailed analyses of which\nmajor 7th chords’ transcriptions change due to approxi-\nmate transcription could reveal whether this hypothesis is\ntrue.\nOur ﬁndings provide evidence to support the intuition\nthat the information which is lost by mapping the spectrum\nto a chroma vector cannot be recovered completely: there-\nfore it seems vital to perform note transcription or calculate\na note activation pattern before mapping the spectrum to a\nchroma representation (as we did in this paper) or directly\nuse spectral features as the input to higher-level models,\nwhich ultimately may be the more principled solution.\nOf course, our approximate NNLS transcription is only\none way of approaching the problem. However, if an ap-\nproximate transcription is known, then chord models and\nhigher-level musical models can be built that do not mix\nthe physical properties of the signal (“spectrum given a\nnote”) and the musical properties (“note given a musical\ncontext”). Since the components of such models will rep-\nresent something that actually exists, we expect that train-\ning them will lead to a better ﬁt and eventually to better\nperformance.\n5. CONCLUSIONS\nWe have presented a new chroma extraction method using\na non-negative least squares (NNLS) algorithm for prior\napproximate note transcription. Twelve different chroma\nmethods were tested for chord transcription accuracy on a\n139\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)standard corpus of popular music, using an existing high-\nlevel probabilistic model. The NNLS chroma features\nachieved top results of 80% accuracy that signiﬁcantly ex-\nceed the state of the art by a large margin.\nWe have shown that the positive inﬂuence of the ap-\nproximate transcription is particularly strong on chords\nwhose harmonic structure causes ambiguities, and whose\nidentiﬁcation is therefore difﬁcult in approaches without\nprior approximate transcription. The identiﬁcation of these\ndifﬁcult chord types was substantially increased by up to\ntwelve percentage points in the methods using NNLS tran-\nscription.\n6. ACKNOWLEDGEMENTS\nThis work was funded by the UK Engineering and Physical\nSciences Research Council, grant EP/E017614/1.\n7. REFERENCES\n[1] S. A. Abdallah and M. D. Plumbley. Polyphonic music\ntranscription by non-negative sparse coding of power\nspectra. In Proceedings of the 5th International Con-\nference on Music Information Retrieval (ISMIR 2004),\n2004.\n[2] M. Casey, R. Veltkamp, M. Goto, M. Leman,\nC. Rhodes, and M. Slaney. Content-Based Music Infor-\nmation Retrieval: Current Directions and Future Chal-\nlenges. Proceedings of the IEEE, 96(4):668–696, 2008.\n[3] B. Catteau, J.-P. Martens, and M. Leman. A proba-\nbilistic framework for audio-based tonal key and chord\nrecognition. In R. Decker and H.-J. Lenz, editors,\nProceedings of the 30th Annual Conference of the\nGesellschaft f ¨ur Klassiﬁkation, pages 637–644, 2007.\n[4] M. E. P. Davies, M. D. Plumbley, and D. Eck. Towards\na musical beat emphasis function. In Proceedings of\nthe IEEE Workshop on Applications of Signal Process-\ning to Audio and Acoustics (WASPAA 2009), 2009.\n[5] E. Gomez. Tonal Description of Audio Music Signals.\nPhD thesis, Universitat Pompeu Fabra, Barcelona,\n2006.\n[6] A. P. Klapuri. Multiple fundamental frequency estima-\ntion by summing harmonic amplitudes. In Proceedings\nof the 7th International Conference on Music Informa-\ntion Retrieval (ISMIR 2006), pages 216–221, 2006.\n[7] C. L. Lawson and R. J. Hanson. Solving Least Squares\nProblems, chapter 23. Prentice-Hall, 1974.\n[8] K. Lee and M. Slaney. Acoustic Chord Transcription\nand Key Extraction From Audio Using Key-Dependent\nHMMs Trained on Synthesized Audio. IEEE Trans-\nactions on Audio, Speech, and Language Processing,\n16(2):291–301, February 2008.\n[9] N. C. Maddage. Automatic structure detection for pop-\nular music. IEEE Multimedia, 13(1):65–77, 2006.[10] M. Mauch. Automatic Chord Transcription from Audio\nUsing Computational Models of Musical Context. PhD\nthesis, Queen Mary University of London, 2010.\n[11] M. Mauch, C. Cannam, M. Davies, S. Dixon, C. Harte,\nS. Kolozali, D. Tidhar, and M. Sandler. OMRAS2\nmetadata project 2009. In Late-breaking session at the\n10th International Conference on Music Information\nRetrieval (ISMIR 2009), 2009.\n[12] M. Mauch and S. Dixon. Simultaneous estimation of\nchords and musical context from audio. to appear in\nIEEE Transactions on Audio, Speech, and Language\nProcessing, 2010.\n[13] M. Mauch, K. C. Noland, and S. Dixon. Using musical\nstructure to enhance automatic chord transcription. In\nProceedings of the 10th International Conference on\nMusic Information Retrieval (ISMIR 2009), pages 231–\n236, 2009.\n[14] K. P. Murphy. The Bayes Net Toolbox for Matlab.\nComputing Science and Statistics, 33(2):1024–1034,\n2001.\n[15] L. Oudre, Y . Grenier, and C. F ´evotte. Template-based\nchord recognition: Inﬂuence of the chord types. In Pro-\nceedings of the 10th International Society for Music In-\nformation Retrieval Conference (ISMIR 2009), pages\n153–158, 2009.\n[16] H. Papadopoulos and G. Peeters. Simultaneous estima-\ntion of chord progression and downbeats from an au-\ndio ﬁle. In Proceedings of the 2008 IEEE International\nConference on Acoustics, Speech and Signal Process-\ning (ICASSP 2008), pages 121–124, 2008.\n[17] G. Peeters. Chroma-based estimation of musical key\nfrom audio-signal analysis. In Proceedings of the 7th\nInternational Conference on Music Information Re-\ntrieval (ISMIR 2006), 2006.\n[18] M. Ryyn ¨anen and A. P. Klapuri. Automatic Transcrip-\ntion of Melody, Bass Line, and Chords in Polyphonic\nMusic. Computer Music Journal, 32(3):72–86, 2008.\n[19] M. Varewyck, J. Pauwels, and J.-P. Martens. A novel\nchroma representation of polyphonic music based on\nmultiple pitch tracking techniques. In Proceedings of\nthe 16th ACM International Conference on Multime-\ndia, pages 667–670, 2008.\n[20] A. Weller, D. Ellis, and T. Jebara. Structured pre-\ndiction models for chord transcription of music\naudio. In MIREX Submission Abstracts. 2009.\nhttp://www.cs.columbia.edu/ ˜jebara/papers/\nicmla09adrian.pdf.\n[21] T. Yoshioka, T. Kitahara, K. Komatani, T. Ogata, and\nH. G. Okuno. Automatic chord transcription with con-\ncurrent recognition of chord symbols and boundaries.\nInProceedings of the 5th International Conference on\nMusic Information Retrieval (ISMIR 2004), pages 100–\n105, 2004.\n140\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "Learning Similarity from Collaborative Filters.",
        "author": [
            "Brian McFee",
            "Luke Barrington",
            "Gert R. G. Lanckriet"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1416198",
        "url": "https://doi.org/10.5281/zenodo.1416198",
        "ee": "https://zenodo.org/records/1416198/files/McFeeBL10.pdf",
        "abstract": "Collaborative filtering methods (CF) exploit the wis- dom of crowds to capture deeply structured similarities in musical objects, such as songs, artists or albums. When CF is available, it frequently outperforms content-based",
        "zenodo_id": 1416198,
        "dblp_key": "conf/ismir/McFeeBL10",
        "keywords": [
            "Collaborative filtering",
            "wisdom of crowds",
            "deeply structured similarities",
            "musical objects",
            "songs",
            "artists",
            "albums",
            "content-based",
            "outperforms",
            "wisdom of crowds"
        ],
        "content": "LEARNING SIMILARITY FROM COLLABORATIVE FILTERS\nBrian McFee Luke Barrington\u0003Gert Lanckriet\u0003\nComputer Science and Engineering\u0003Electrical and Computer Engineering\nUniversity of California, San Diego\nbmcfee@cs.ucsd.edu lukeinusa@gmail.com gert@ece.ucsd.edu\nABSTRACT\nCollaborative ﬁltering methods (CF) exploit the wis-\ndom of crowds to capture deeply structured similarities in\nmusical objects, such as songs, artists or albums. When\nCF is available, it frequently outperforms content-based\nmethods in recommendation tasks. However, songs in the\nso-called “long tail” cannot reap the beneﬁts of collabora-\ntive ﬁltering, and practitioners must rely on content-based\nmethods. We propose a method for improving content-\nbased recommendation in the long tail by learning an op-\ntimized similarity function from a sample of collabora-\ntive ﬁltering data. Our experimental results demonstrate\nsubstantial improvements in accuracy by learning optimal\nsimilarity functions.\n1. INTRODUCTION\n“Collaborative ﬁltering” (CF) is a popular method for multi-\nmedia recommendation applications in which data (e.g.,\nsongs, artists, books or movies) are represented and com-\npared in terms of the people who use them. Systems based\non collaborative ﬁltering exploit the “wisdom of crowds”\nto deﬁne similarity between items, which can then be used\nfor recommendation. Indeed, collaborative ﬁltering sys-\ntems beneﬁt from several attractive properties: CF explic-\nitly represents individual users, and is therefore inherently\npersonalized; data collection can be done passively, rather\nthan requiring users to actively tag items; and CF data di-\nrectly captures usage habits: exactly the quantity that rec-\nommendation engines strive to affect.\nIt is therefore not surprising that CF methods have be-\ncome an active research topic in recent years, due in no\nsmall part to the recently concluded competition for the\nNetﬂix Prize [1]. Within the Music Information Retrieval\n(MIR) community, recent studies have shown that CF sys-\ntems consistently outperform content-based methods for\nplaylist generation [6] and tag prediction [15]. However,\ncollaborative ﬁltering suffers from the dreaded “cold start”\nproblem: CF methods fail on items which have not yet\nbeen used, and are therefore unsuitable for recommenda-\ntion in the “long tail”. While this problem persists for all\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2010 International Society for Music Information Retrieval.media (e.g., movies, books, etc.), it is especially deadly\nin music, due to the relatively large number of unknown\nsongs and artists in the world today. Netﬂix boasts 100,000\nDVD titles [1], while Apple’s iTunes store provides access\nto over 13 million songs [2].\nMotivated by the cold-start problem, MIR researchers\nhave worked steadily to improve content-based recommen-\ndation engines. Content-based systems operate solely on\nfeature representations of music, eliminating the need for\nhuman intervention. While this approach naturally extends\nto long-tail data, the deﬁnition of similarity in these sys-\ntems is frequently ad-hoc and not explicitly optimized for\nthe speciﬁc task. As a result, it remains unclear if, or to\nwhat extent, content-based systems can capture relevant\nsimilarity information expressed by collaborative ﬁltering.\nIn this paper, we pose the question: can we learn content-\nbased similarity from a collaborative ﬁlter? Empirically,\nCF data provides a highly reliable means for determining\nsimilarity between musical objects. Our main contribution\nin this paper is a method for optimizing content-based sim-\nilarity by learning from a collaborative ﬁlter.\nThe proposed method treats similarity learning as an\ninformation retrieval problem, where similarity is evalu-\nated according to the ranked list of results in response to a\nquery example, e.g., a list of artists ordered by similarity to\n“The Beatles”. Optimizing similarity for ranking requires\nmore sophisticated machinery than is used in other meth-\nods,e.g., genre classiﬁers. However, it does offer a few key\nadvantages, which we believe are crucial for realistic music\napplications. First, there are no assumptions of transitivity\nor symmetry in the proposed method. As a result, “The\nBeatles” may be considered a relevant result for “Oasis”,\nand not vice versa; this is not possible with other methods\nin the literature, e.g., the embedding technique described\nin [21]. Second, CF data can be collected passively from\nusers (e.g., via scrobbles [16]) and directly captures their\nlistening habits. Finally, optimizing similarity for ranking\ndirectly attacks the main quantity of interest, i.e., the or-\ndered list of retrieved items, rather than potentially irrele-\nvant or overly coarse abstractions (e.g., genre).\nOur proposed method is quite general, and can improve\nsimilarities derived from semantic descriptions provided\nby humans or an auto-tagging engine. As we will demon-\nstrate, even hand-crafted song annotations can be optimized\nto more accurately reﬂect and predict the similarity struc-\nture encoded by collaborative ﬁltering data.\n345\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)1.1 Related work\nA signiﬁcant amount of research has been devoted to the\ntopic of musical similarity in the past decade. Ellis, et\nal. [9] evaluated similarity metrics derived from various\ndata sources against human survey data. Similarly, Kim,\net al. [15] evaluate several sources of artist similarity for a\ntag prediction task, and observe that methods based on col-\nlaborative ﬁltering signiﬁcantly out-perform acoustic or se-\nmantic similarity. However, neither of these works attempt\nto optimize similarity for a speciﬁc task.\nSlaney, et al. [21] apply several learning algorithms to\nﬁnd similarity metrics over acoustic features which are op-\ntimized to cluster songs of the same artist, album, or that\nappear on the same blog. Our previous work [19] applies\nsimilar techniques to predict human survey data and op-\ntimally integrate multiple data sources. The method pro-\nposed here falls squarely within this line of work, but dif-\nfers in that the metric is trained from collaborative ﬁltering,\nand optimized for ranking performance, rather than classi-\nﬁcation or (comparatively scarce) human survey data.\nThere is a large body of work which treats collaborative\nﬁltering as a matrix completion problem (see, e.g., [24]).\nIn the matrix completion view, the goal is to perform user-\ncentric recommendation by ﬁlling in missing entries of the\nusers-by-content matrix, i.e., recommending content to a\nuser based on his or her speciﬁc preferences. Our applica-\ntion here is slightly different: rather than trying to complete\nthe matrix, we interpret the collaborative ﬁltering matrix\nas the ground truth, from which, similarity can be derived.\nOur goal is to train a content-based system to match simi-\nlarities derived from CF data. We also stress that our pro-\nposed method is nota hybrid method: once the metric has\nbeen trained, collaborative ﬁltering data is not necessary to\ncompute similarities for unseen, long-tail songs.\n2. LEARNING SIMILARITY\nOur goal is to learn an optimal similarity function for songs,\nand as such, we must choose a family of similarity func-\ntions over which to optimize. Many families of similarity\nfunctions have been proposed in the MIR literature, such as\ndistance between generative models of acoustic [3, 17] or\nsemantic [5] descriptors, and playlist-based similarity [18].\nHere, we opt for Euclidean distance between song rep-\nresentations. The primary reason for this choice is that Eu-\nclidean distance naturally lends itself to optimization by\nmetric learning (see, e.g., [19, 21]). In metric learning,\neach data point is described by a vector in Rd, and the goal\nis to learn a linear projection matrix Lsuch that distances\nafter projection (kLi \u0000Ljk) are small for “similar” pairs\n(i;j)and large for “dissimilar” pairs. Due to computa-\ntional issues, optimization is performed not on L, but on\na positive semi-deﬁnite1(PSD) matrix W=LTL\u00170.\nIn the metric deﬁned by W, distance between points (i;j)\n1A positive semi-deﬁnite matrix W, denoted W\u00170is square, sym-\nmetric, and has non-negative eigenvalues.\nq\nqW\nRanking by distance fro m q: Ranking by distance fro m q:\n1        2       3        4 1        2       3        4Figure 1. Metric Learning to Rank (MLR) learns a met-\nric (W ) so that a query song qis close to relevant results\n(+) and far from irrelevant results (-). Optimization is per-\nformed with respect to the rankings induced by distance\nfrom the query.\nafter projection is denoted by the quadratic form\nd(i;j ) =ki\u0000jk2\nW= (i\u0000j)TW(i\u0000j)\n= (i\u0000j)TLTL(i\u0000j) =kLi\u0000Ljk2:(1)\nFor the present application, we apply the Metric Learn-\ning to Rank (MLR) algorithm [20]. Here, we provide a\nbrief overview of the algorithm.\n2.1 Metric learning to rank\nMetric Learning to Rank (MLR) [20] is an extension of\nStructural SVM [13]. Structural SVM has been demon-\nstrated to be an effective method for solving ranking prob-\nlems in information retrieval systems [8], and the MLR al-\ngorithm extends the methodology to the query-by-example\nsetting by learning a metric space, rather than a discrim-\ninant vector. Speciﬁcally, MLR learns a positive semi-\ndeﬁnite matrix Wsuch that rankings induced by learned\ndistances are optimized according to a ranking loss mea-\nsure, e.g., ROC area (AUC) or precision-at-k . In this set-\nting, “relevant” results should lie close in space to the query\nq, and “irrelevant” results should be pushed far away.\nFor a query song q, a natural ordering of the database\nXis obtained by sorting x2X according to increasing\ndistance from qunder the metric deﬁned by W(see Fig-\nure 1). The metric Wis learned by solving a constrained\noptimization problem such that, for each training query q,\na higher score is assigned to the “true” ranking y\u0003\nqthan to\nany other ranking y2Y (the set of all rankings):\nhW; (q;y\u0003\nq)i\u0015hW; (q;y)i+ \u0001(y\u0003\nq;y)\u0000\u0018q:(2)\nHere, the “score” for a query-ranking pair (q;y)is com-\nputed by the Frobenius inner product:\nhW; (q;y)i= tr(W (q;y)): (3)\n (q;y)is a matrix-valued feature map which encodes the\nquery-ranking pair (q;y), and \u0001(y\u0003\nq;y)computes the loss\nincurred by predicting yinstead ofy\u0003\nqfor the query q(e.g.,\n346\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Algorithm 1 Metric Learning to Rank [20]\nInput: dataX=fq1;q2;:::;qng\u001aRd,\ntrue rankings y\u0003\n1;y\u0003\n2;:::y\u0003\nn,\nslack trade-off C\u00150\nOutput:d\u0002dmatrixW\u00170\nmin\nW\u00170;\u0018tr(W) +C\u00011\nnX\nq2X\u0018q\ns:t:8q2X;8y2Ynfy\u0003\nqg:\nhW; (q;y\u0003\nq)i\u0015hW; (q;y)i+ \u0001(y\u0003\nq;y)\u0000\u0018q\n\u0018q\u00150\nloss in AUC score), essentially playing the role of the “mar-\ngin” between rankings y\u0003\nqandy. Intuitively, the score for\na true ranking y\u0003\nqshould exceed the score for any other y\nby at least the loss \u0001(y\u0003\nq;y). (In the present context, the\n“true” ranking is any one which places all relevant results\nbefore all irrelevant results.) To allow violations of mar-\ngins during training, a slack variable \u0018q\u00150is introduced\nfor each query.\nMLR encodes query-ranking pairs (q;y)by the partial\norder feature [13]:\n (q;y) =X\ni2X+\nqX\nj2X\u0000\nqyij\u0012\u001e(q;i)\u0000\u001e(q;j )\njX+qj\u0001jX\u0000qj\u0013\n;(4)\nwhereX+\nq(resp.X\u0000\nq) is the set of relevant (resp. irrele-\nvant) songs for q, the ranking yis encoded by\nyij=(\n+1ibeforejiny\n\u00001iafterj;\nand\n\u001e(q;i ) =\u0000(q\u0000i)(q\u0000i)T(5)\ncaptures the afﬁnity between the query qand a single item\ni. Intuitively,  is constructed by adding the difference\n\u001e(q;i)\u0000\u001e(q;j )wheneveryplaces (relevant) ibefore (ir-\nrelevant)jand subtracted otherwise. This choice of  \ntherefore emphasizes directions in the feature space which\nare correlated with good rankings.\nFor a test query q0, the predicted ranking yis that which\nachieves the highest score by W,i.e.,argmaxyhW; (q0;y)i.\nThis can be found efﬁciently by sorting the corpus in de-\nscending order ofhW;\u001e(q0;x)i. Equation 5 deﬁnes \u001eso\nthat when taking the inner product with W,\nhW;\u001e(q0;x)i=\u0000tr\u0000\nW(q0\u0000x)(q0\u0000x)T\u0001\n(6)\n=\u0000(q0\u0000x)TW(q0\u0000x) =\u0000kq0\u0000xk2\nW;\nthe result is the (negative, squared) distance between q0\nandxunder the metric deﬁned by W. Thus, decreasing\nhW;\u001e(q0;x)icorresponds to increasing distance from q0.\nThe MLR optimization problem is listed as Algorithm 1.\nAs in support vector machines, the objective consists of\ntwo competing terms: tr(W)is a convex approximation\nto the rank of the learned metric, and 1=nP\u0018qmeasures\nthe empirical (hinge) loss on the training set, and the twoterms are balanced by a trade-off parameter C. Although\nthe full problem includes a super-exponential number of\nconstraints (one for each y2 Y , for eachq), [20] de-\nscribes an efﬁcient approximation algorithm based on cut-\nting planes [14] which works well in practice.\n3. DATA\nSince our goal is to learn a content-based similarity metric\nfor songs, it would seem logical to derive similarity from\nCF data relating users to songs. However, in practice, such\nmatrices tend to exhibit high sparsity, which would lead to\nunstable similarity computations. We instead opt to derive\nsimilarity at the artist level, and then transfer similarity to\nthe song level. Given a set of artists, and a collaborative\nﬁltering matrix over the artists, our experimental procedure\nis as follows:\n1. extract artist similarity from the CF data,\n2. transfer artist similarity to song similarity,\n3. construct a feature representation for each song,\n4. learn a metric Wover song representations to pre-\ndictsong similarities, and\n5. evaluate Wby testing retrieval of similar songs in\nresponse to (unseen) test songs.\nSteps 1 and 2 are described in Section 3.2, and step 3 is\ndescribed throughout Section 3.3. Next, we describe the\nsources of our audio and collaborative ﬁltering data.\n3.1 Swat10k\nOur experiments use the Swat10k dataset of 10,870 songs\nfrom 3,748 unique artists [22]. Each song has been weakly-\nlabeled from a vocabulary of 1,053 tags from Pandora’s\nMusic Genome Project2that include multiple genres and\nacoustically objective descriptors.\n3.2 Collaborative ﬁltering\nTo deﬁne similarity between songs, we use the collabora-\ntive ﬁltering (CF) data mined from Last.fm3by [7]. The\nraw collaborative ﬁltering matrix consists of approximately\n17.5 million user-song interactions over 359K users and\n186K artists with MusicBrainz4identiﬁers (MBIDs).\nWe ﬁrst ﬁltered the CF matrix down to include only the\nSwat10k artists by matching MBIDs, resulting in a reduced\nCF matrixF:\nFui=(\n1userulistened to artist i\n0otherwise;(7)\nof 356,026 users and 3,748 artists.\nFrom the CF matrix, we deﬁne the similarity between\nartistsiandjas the cosine-similarity between the column-\nvectorsFiandFj:\nSij=FT\niFj\nkFik\u0001kFjk: (8)\n2http://www.pandora.com/mgp.shtml\n3http://last.fm\n4http://www.musicbrainz.org/\n347\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Training Validation Test Discard\n# Artists 746 700 700 1602\n# Songs 1842 1819 1862 5347\n# Relevant 39.5 37.7 36.4\nTable 1. Statistics of the Swat10k data. “# Relevant” is the\naverage size of the relevant set for each song.\nIntuitively,Sijcounts the number of users shared between\nartistsiandj, and normalizes by popularity.\nTo ensure stable similarity measurements, we discarded\nall artists from the set which had fewer than 100 users. This\nleaves 2,146 artists, which we split roughly into thirds for\ntraining, validation, and test sets. For each artist, we then\ndeﬁne the set of “relevant” artists as the 10 closest training\nartists according to Equation 85.\nFinally, we convert artist-level relevance to song-level\nrelevance. For each song of an artist a, the relevant set\nis the union of the sets of songs from each of a’s relevant\nartists. Table 1 summarizes the statistics of the data used\nin our experiments.\n3.3 Features\nFor each song in our database, we construct three different\nfeature representations: acoustic, auto-tags, and tags pro-\nvided by human labelers.\n3.3.1 Vector quantized MFCCs\nOur representation of acoustic features is based upon vector-\nquantized Mel-Frequency Cepstral Coefﬁcients (MFCCs),\nand consists of a 3-step process: feature extraction, vec-\ntor quantization, and kernelization. The method described\nhere is similar to that of [12], and is inspired by similar\nmethods found in the computer vision literature [10].\nFirst, for each song, we extract the ﬁrst 13 MFCCs from\n25ms half-overlapping windows. Each MFCC vector is\naugmented by appending the ﬁrst and second instantaneous\ntime derivatives, resulting in a sequence of 39-dimensional\ndelta-MFCC (\u0001MFCC) vectors for each song.\nUsing the songs which were discarded due to insufﬁ-\ncient collaborative ﬁltering data, we trained a codebook\nfor use as a vector quantizer. We randomly selected 1000\nsongs from the discard set, and from each selected song,\nrandomly sampled 1000 \u0001MFCC vectors, for a total of\n1 million codebook-training vectors. Each training vector\nvwas z-scored, so that the ithcoordinatevibecomes\nvi7!vi\u0000\u0016i\n\u001bi; (9)\nwhere (\u0016i;\u001bi)are the sample mean and standard deviation\nof theithcoordinate in the codebook-training set. We ran\nk-means with k= 5000 on the z-scored training vectors,\nusing the implementation provided by [11]. The result is\na set of 5000 codewords, each of which was subsequently\n“un”-z-scored by\nvi7!\u001bivi+\u0016i: (10)\n5For training artists, we assume self-similarity, so there are technically\n11 relevant artists for each training artist.With the codebook in hand, the \u0001MFCC vectors for\neach song in the training, validation, and test splits were\nquantized by ﬁnding the closest (in Euclidean distance)\ncodeword. Each song was summarized by a histogram over\nthe5000 codewords, corresponding to the frequency with\nwhich a codeword was selected as a quantizer in that song.\nFinally, we constructed a \u001f2-kernel over songs, so that\nthe similarity between two codeword histograms uandv\nis calculated as6\nk(u;v) = exp\u0000\n\u0000\u001f2(u;v)\u0001\n(11)\n\u001f2(u;v) =5000X\ni=1(ui\u0000vi)2\nui+vi: (12)\n(This kernel can itself be viewed as a soft vector quan-\ntizer, this time operating at the song-level rather than the\nfeature-level.) Each song is represented by a vector in\nR1842, where theithdimension represents similarity to the\nithtraining song. We then compress these vectors by prin-\ncipal components analysis to 35 dimensions, which capture\n95% of the variance in the training set.\n3.3.2 Auto-tags\nWe can alternatively represent a song’s acoustic informa-\ntion by using descriptive semantics. By learning from ex-\nample songs that humans have labeled with tags, an “auto-\ntagger” (e.g., [12,23]) can automatically rate the relevance\nof these tags to new, unlabeled songs. The resulting “auto-\ntags” offer a concise description of the song, and semantic\nsimilarity between auto-tags has been shown to improve on\ncontent-based similarity derived from acoustics alone [5].\nWe use the auto-tagger described in [23] to label each song\nwith a real-valued vector of 149 auto-tags: the ithdimen-\nsion of this vector corresponds to the probability that the\nithtag applies to the song, given the observed \u0001MFCCs.\n3.3.3 Human tags\nOur third feature describes songs with “human tags” mined\nfrom the Music Genome Project by [22] that include de-\nscriptors of a song’s genre, style, instrumentation, vocals\nand lyrics. Each song is represented by a 1,053-dimensional\nbinary vector that is “weakly-labeled”, meaning that a “1”\nimplies that the tag is relevant but a “0” does not guarantee\nthat the tag does not apply to the song. We consider these\n“human tags” to be “acoustically objective” as they have\nbeen applied by musicological experts and refer only to\nthe acoustic content of the songs. They represent the ideal\noutput that a content-based auto-tagger might achieve.\n4. EXPERIMENTS\nThe MLR algorithm requires that a few parameters be set\nwhen training: not only the slack trade-off C, but also the\nchoice of ranking measure to optimize. The implemen-\ntation described in [20] supports several standard ranking\nmeasures: the area under the ROC curve (AUC), mean\n6For the summation in Equation 12, we adopt the convention\n0=0 = 0 .\n348\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Data source AUC MAP MRR\nMFCC 0.630 0.057 0.249\nOptimized MFCC 0.719 0.081 0.275\nAuto-tags 0.726 0.090 0.330\nOptimized auto-tags 0.776 0.116 0.327\nHuman tags 0.770 0.187 0.540\nOptimized human tags 0.939 0.420 0.636\nTable 2. Ranking performance of each data source\n(MFCC, auto-tags, and human tags), before and after\nlearning with MLR.\nreciprocal rank (MRR), mean average precision (MAP),\nprecision-at-k (P@k), and normalized discounted cumula-\ntive gain (NDCG); see [8] for a brief summary of these\nranking measures. For P@k and NDCG, an additional\nparameterkmust be set, which deﬁnes how many songs\nshould be retrieved when evaluating the ranking.\nFor each data source described in Section 3, we trained\nmetrics with all ﬁve variants of MLR. We swept over C2\nf10\u00002;10\u00001;:::; 1011g, and for the P@k and NDCG vari-\nants, we also swept over k2f2; 4;8;:::; 256g. Perfor-\nmance was evaluated on the validation set, and the best-\nperforming metric was then tested on the test set.\n4.1 Embedding results\nAfter learning W, we evaluate on the validation and test\nsets by computing for each query song q, the ranked list\nof training songs xordered by increasing kq\u0000xkW. The\nresulting rankings are scored, and scores are averaged over\nallqto produce a single score for the learned metric. For\ncomparison purposes, we also evaluate rankings derived\nfrom native metrics (i.e., without learning W). The native\nmetric for auto-tags is taken to be the Kullback-Leibler di-\nvergence between auto-tag distributions. For MFCC and\nhuman tags, we use standard Euclidean distance.\nTable 4 displays some example playlists generated by\nthe native and optimized MFCC spaces. At a high level, the\nlearned metrics successfully de-noise the feature space to\ngenerate more cohesive playlists. Table 2 lists ranking per-\nformance for each data source, before and after optimiza-\ntion. In all but one case (auto-tag MRR), performance im-\nproves across all evaluation criteria. For each data source\n(MFCC, auto-tags, and human tags), we observe dramatic\nimprovements in accuracy over the corresponding native\nsimilarity metric.\nQuantitatively, the purely acoustic model improves in\nAUC score from 0.630 to 0.719. The optimized similarity\nperforms comparably to native auto-tag similarity, but can\nbe constructed entirely from passive data (as opposed to\nthe actively collected data necessary for building auto-tag\nmodels). Similarly, optimizing auto-tags improves AUC\nfrom 0.726 to 0.776, which is comparable to the native per-\nformance of human tags. Finally, optimizing human tags\nimproves AUC substantially, from 0.770 to 0.939. This\nindicates that even when annotations are hand-crafted by\nexperts, recommendation may still be greatly improved by\nusing an appropriate model of the tag vocabulary.Top tags Bottom tags\n1. LATIN 1044. TWO -STEP STYLE\n2. A REGGAE FEEL 1045. UNUSUAL VOCAL SOUNDS\n3. REGGAE 1046. UPBEAT LYRICS\n4. CHRISTIAN 1047. CALL -AND -RESPONSE VOCALS\n5. NEW -AGE 1048. ELECTRIC PIANOS\n6. ROCK ON THE RANGE RADIO 1049. MODAL HARMONIES\n7. WAKARUSA RADIO 1050. TONAL HARMONIES\n8. SASQUATCH RADIO 1051. VOCAL COUNTERPOINT\n9. CMJ MUSIC MARATHON 1052. VOCAL SAMPLES\n10. REGGAE /CARIBBEAN 1053. WESTERN SWING\nTable 3. The top and bottom 10 tags learned by MLR,\nordered by weight. 85 tags receive 0 weight.\n4.2 Learning tag weights\nGiven the substantial improvement observed by optimiz-\ning human tags, one may wonder what conclusions can be\ndrawn from the learned metric. In particular, since Wcan\nbe interpreted as “translation matrix” or vocabulary model,\nit is natural to ask which tags deﬁne the similarity space,\nand which tags are redundant or non-informative.\nBecauseWcontains both positive and negative entries,\nit is not immediately clear how to interpret a full matrix W\nin terms of tags. However, placing further restrictions on\nthe form ofWcan ease interpretability (at the expense of\nmodel ﬂexibility). We repeated the “human tags” experi-\nment with a modiﬁcation of Algorithm 1 that restricts W\nto be diagonal and non-negative. In the restricted model,\ntheithelement of the diagonal Wiican be interpreted as a\nweight for the ithtag. The diagonal metric achieves AUC\nof 0.875 (compared to 0.776 native and 0.939 for a full W).\nTable 3 lists the top and bottom 10 tags, ordered by\nweightsWii. Several interesting observations can be made\nhere: all of the top tags refer either to genre (e.g., LATIN ,\nREGGAE ) or streaming radio identiﬁers (e.g., W AKARUSA ,\nCMJ). This corroborates previous studies which indicate\nthat grouping music by social cues, such as radio playlists\nor blogs, can assist recommendation [4]. By contrast, the\nbottom tags are primarily musicological terms (e.g., VO-\nCAL COUNTERPOINT ) which apparently convey little use-\nful information for recommendation.\nThis view of MLR as a supervised learning procedure\nfor vocabulary models suggests comparison to standard,\nunsupervised techniques, such as TF-IDF with cosine sim-\nilarity. It turns out that for this data set, using TF-IDF\nweighting results a decrease in AUC from 0.770 to 0.724!\nFrom this, we can conclude that it is suboptimal to rely on\nthe natural statistics of tags to deﬁne similarity.\n5. CONCLUSION\nWe have proposed a method for improving content-based\nsimilarity by learning from a sample of collaborative ﬁlter-\ning data. The proposed method learns an optimal transfor-\nmation of features to reproduce high-quality CF similarity,\nand can be used to improve the quality of recommendation\nin the long tail. If songs are described by semantic tags,\nour method reveals which tags play the most important\nrole in deﬁning an optimal similarity metric. By revealing\nthe most important tags for predicting CF similarity, our\nmethod may also be useful for guiding the development of\ndiscovery interfaces and automatic tagging algorithms.\n349\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Query song Native space playlist Optimized space playlistMFCCChick Corea\nElektric Band - Beneath the MaskKatalyst - Break Up IMichael Brecker - Two Blocks From The Edge\nStevie Wonder - Superstition ICharlie Parker - Wee\nJames Brown - Soul Power Coleman Hawkins - There Is No Greater Love\nTina Turner - What’s Love Got To Do With It IMiles Davis - Walkin’\nThe Whispers - And The Beat Goes On Clifford Brown - Love Is A Many Splendored ThingAuto-tagsWhite Zombie\n- Electric Head (Pt. 2.) (Remix)ISepultura - Apes Of God ISepultura - Apes Of God\nArctic Monkeys - A Certain Romance IMetallica - Nothing Else Matters (Live)\nSecret Machines - Lightning Blue Eyes Secret Machines - Lightning Blue Eyes\nGreen Day - Longview (Live) The Warlocks - Gypsy Nightmare\nPerry Farrell - Kinky Mastodon - Crystal SkullHuman tagsAaliyah -\nMiss YouIGinuwine - In Those Jeans IMonica - Don’t Take It Personal\nIMonica - Don’t Take It Personal IGinuwine - In Those Jeans\nIAshanti - Foolish IAshanti - Foolish\nFoo Fighters - DOA INe-Yo - Go On Girl\nSay Hi To Your Mom - Northwestern Girls Jodeci - Freak N You\nTable 4. Example playlists in native and optimized MFCC, auto-tag, and human tag spaces. Playlists are generated by\nﬁnding the ﬁve nearest neighbors of the query; relevant results are indicated by I.\n6. REFERENCES\n[1] Netﬂix press release, 2009.\nhttp://netﬂix.mediaroom.com/index.php?s=43&item=307.\n[2] Apple itunes, 2010. http://www.apple.com/itunes.\n[3] Jean-Julien Aucouturier and Franc ¸ois Pachet. Music\nsimilarity measures: What’s the use? In Inerna-\ntional Symposium on Music Information Retrieval (IS-\nMIR2002), pages 157–163, 2002.\n[4] Claudio Baccigalupo, Justin Donaldson, and Enric\nPlaza. Uncovering afﬁnity of artists to multiple gen-\nres from social behaviour data. In International Sym-\nposium on Music Information Retrieval (ISMIR2008),\nSeptember 2008.\n[5] Luke Barrington, Antoni Chan, Douglas Turnbull,\nand Gert Lanckriet. Audio information retrieval us-\ning semantic similarity. IEEE Int. Conf. on Acoustics,\nSpeech, and Signal Processing (ICASSP), 2007.\n[6] Luke Barrington, Reid Oda, and Gert Lanckriet.\nSmarter than genius? Human evaluation of music rec-\nommender systems. In Proceedings of the 10th Inter-\nnational Conference on Music Information Retrieval,\n2009.\n[7] O. Celma. Music Recommendation and Discovery in\nthe Long Tail. PhD thesis, Universitat Pompeu Fabra,\nBarcelona, Spain, 2008.\n[8] Soumen Chakrabarti, Rajiv Khanna, Uma Sawant,\nand Chiru Bhattacharyya. Structured learning for non-\nsmooth ranking losses. In KDD ’08: Proceeding of\nthe 14th ACM SIGKDD international conference on\nKnowledge discovery and data mining, pages 88–96,\nNew York, NY , USA, 2008. ACM.\n[9] D. Ellis, B. Whitman, A. Berenzweig, and\nS. Lawrence. The quest for ground truth in musi-\ncal artist similarity. In Proeedings of the International\nSymposium on Music Information Retrieval (ISMIR),\npages 170–177, October 2002.\n[10] L. Fei-Fei and P. Perona. A bayesian hierarchical\nmodel for learning natural scene categories. In IEEE\nComputer Society Conference on Computer Vision and\nPattern Recognition (CVPR 2005), volume 2, 2005.\n[11] Peter Gehler. MPIKmeans, 2007.\nhttp://mloss.org/software/view/48/.\n[12] M. Hoffman, D. Blei, and P. Cook. Easy as CBA: A\nsimple probabilistic model for tagging music. In Pro-\nceedings of the 10th International Conference on Mu-sic Information Retrieval, 2009.\n[13] Thorsten Joachims. A support vector method for mul-\ntivariate performance measures. In Proceedings of the\n22nd international conference on Machine learning,\npages 377–384, New York, NY , USA, 2005. ACM.\n[14] Thorsten Joachims, Thomas Finley, and Chun-\nNam John Yu. Cutting-plane training of structural\nsvms. Mach. Learn., 77(1):27–59, 2009.\n[15] Joon Hee Kim, Brian Tomasik, and Douglas Turnbull.\nUsing artist similarity to propagate semantic informa-\ntion. In Proceedings of the 10th International Confer-\nence on Music Information Retrieval, 2009.\n[16] Last.FM, January 2009. http://www.last.fm/.\n[17] B. Logan. Music recommendation from song sets. In\nInternational Symposium on Music Information Re-\ntrieval (ISMIR2004), 2004.\n[18] Franc ¸ois Maillet, Douglas Eck, Guillaume Desjardins,\nand Paul Lamere. Steerable playlist generation by\nlearning song similarity from radio station playlists. In\nProceedings of the 10th International Conference on\nMusic Information Retrieval, 2009.\n[19] Brian McFee and Gert Lanckriet. Heterogeneous em-\nbeddding for subjective artist similarity. In Proceed-\nings of the 10th International Conference on Music In-\nformation Retrieval, 2009.\n[20] Brian McFee and Gert Lanckriet. Metric learning to\nrank. In Proceedings of the 27th annual International\nConference on Machine Learning (ICML), 2010.\n[21] M. Slaney, K. Weinberger, and W. White. Learning\na metric for music similarity. In International Sym-\nposium on Music Information Retrieval (ISMIR2008),\npages 313–318, September 2008.\n[22] D. Tingle, Y . Kim, and D. Turnbull. Exploring auto-\nmatic music annotation with “acoustically-objective”\ntags. In IEEE International Conference on Multimedia\nInformation Retrieval (MIR), 2010.\n[23] D. Turnbull, L. Barrington, D. Torres, and G. Lanck-\nriet. Semantic annotation and retrieval of music and\nsound effects. IEEE TASLP, 16(2):467–476, Feb. 2008.\n[24] K. Yoshii, M. Goto, K. Komatani, T. Ogata, and H.G.\nOkuno. An efﬁcient hybrid music recommender sys-\ntem using an incrementally trainable probabilistic gen-\nerative model. IEEE Transactions on Audio, Speech,\nand Language Processing, 16(2):435–447, 2008.\n350\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "Evaluating the Genre Classification Performance of Lyrical Features Relative to Audio, Symbolic and Cultural Features.",
        "author": [
            "Cory McKay",
            "John Ashley Burgoyne",
            "Jason Hockman",
            "Jordan B. L. Smith",
            "Gabriel Vigliensoni",
            "Ichiro Fujinaga"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1415706",
        "url": "https://doi.org/10.5281/zenodo.1415706",
        "ee": "https://zenodo.org/records/1415706/files/McKayBHSVF10.pdf",
        "abstract": "This paper describes experimental research investigating the genre classification utility of combining features ex- tracted from lyrical, audio, symbolic and cultural sources of musical information. It was found that cultural features consisting of information extracted from both web searches and mined listener tags were particularly effec- tive, with the result that classification accuracies were achieved that compare favorably with the current state of the art of musical genre classification. It was also found that features extracted from lyrics were less effective than the other feature types. Finally, it was found that, with some exceptions, combining feature types does improve classification performance. The new lyricFetcher and jLyrics software are also presented as tools that can be used as a framework for developing more effective classi- fication methodologies based on lyrics in the future.",
        "zenodo_id": 1415706,
        "dblp_key": "conf/ismir/McKayBHSVF10",
        "keywords": [
            "genre classification",
            "lyrical features",
            "audio features",
            "symbolic features",
            "cultural features",
            "web searches",
            "listener tags",
            "classification accuracies",
            "current state of the art",
            "lyrics"
        ],
        "content": "EVALUATING THE GENRE CLASSIFICATION \nP\nERFORMANCE OF LYRICAL FEATURES RELATIVE TO \nAUDIO, SYMBOLIC AND CULTURAL FEATURES \nCory McKay, John Ashley Burgoyne, Jason Hockman, Jordan B. L. Smith,  \nGa\nbriel Vigliensoni and Ichiro Fujinaga \nCentre for Interdisciplinary Research in Music Media and Technology (CIRMMT) \nMcGill University, Montréal, Québec, Canada \n[cory.mckay,jason.hockman,jordan.smith2]@mail.mcgill.ca, \n[\nashley,gabriel,ich]@music.mcgill.ca  \nABSTRACT \nT\nhis paper describes experimental research investigating \nthe genre classification utility of combining features ex-\ntracted from lyrical, audio, symbolic and cultural sources \nof musical information. It was found that cultural features \nconsisting of information extracted from both web \nsearches and mined listener tags were particularly effec-\ntive, with the result that classification accuracies were \nachieved that compare favorably with the current state of \nthe art of musical genre classification. It was also found \nthat features extracted from lyrics were less effective than \nthe other feature types. Finally, it was found that, with \nsome exceptions, combining feature types does improve \nclassification performance. The new lyricFetcher and \njLyrics software are also presented as tools that can be \nused as a framework for developing more effective classi-\nfication methodologies based on lyrics in the future.  \n1. INTRODUCTION \nAutomatic music classification is an important area of \nmusic information retrieval (MIR) research. Areas such as \nclassification by genre, mood, artist and user tag have all \nreceived significant attention in the MIR literature. Clas-\nsification is typically performed by training machine \nlearning algorithms on features extracted from audio re-\ncordings, symbolic data or cultural information mined \nfrom the Internet. An interest in features extracted from \ntextual transcriptions of lyrics has also become increa-\nsingly evident recently.  \nMost research to date has involved experiments involv-\ning one or, at most, two of these four types of data. This \nleaves unanswered questions as to whether improvements \nin classification performance might be achieved by com-\nbining features extracted from various combinations of \nthese four musical data sources, especially with respect to the relatively new area of classification based on lyrics. \nThe first goal of the research presented here is to in-\nvestigate this issue through a series of genre classification \nexperiments on each possible subset combination of fea-\ntures extracted from lyrical, audio, symbolic and cultural \ndata. Genre classification in particular is chosen because \nit is a well-established area of inquiry in the MIR litera-\nture that can be particularly difficult to perform well, and \nas such provides a good general basis for evaluation. \nThe second goal of this paper is to present software for \nmining lyrics from the Internet and for extracting features \nfrom them. There is not yet an established research toolset \nfor performing these tasks, and the lyricFetcher and jLyr-\nics software described here are intended to fill this gap. \n2. PREVIOUS RESEARCH \n2.1 Mining Lyrics from the Web \nThere are many web sites providing access to lyric tran-\nscriptions, including industry-approved pay services (e.g., \nGracenote Lyrics), specialized lyric-scraping services \n(e.g., EvilLyrics, iWeb Scraping and Web Data Extrac-\ntion), and other sites that amalgamate user contributions. \nThe main difficulties encountered when automatically \nmining lyrics are associated with high variability in dis-\nplay formatting and content. Many sites also attempt to \nobscure lyrical content in the page source because of cop-\nyright concerns. There have been several attempts to ex-\ntract and align lyrics from multiple sources automatically \nusing dynamic programming [2,6], but these have encoun-\ntered difficulties due to varying search results. \nLyricsFly is one site that promises well-formatted lyr-\nics and simplified searches accessible via a published \nAPI. Lyrics are provided in a convenient XML format, \nand multiple versions of songs are accessible. LyricWiki \nonce provided a public API as well, but has since discon-\ntinued this service due to copyright concerns. Its content \nis still accessible via web browsing, however. \n2.2 Extracting Classification Features from Lyrics \nLogan et al. [10] and Mahedero et al. [11] provide \nimportant early contributions on analyzing lyrics using a  \nPermission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fee provided that copies \nare not made or di stributed for profit or commercial advantage and that \ncopies bear this notice and the full citation on the first page.  \n© 2010 International Society for Music Information Retrieval  \n213\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \nv\nariety of techniques drawn from natural language \nprocessing, including topic modelling, to quantify musical \nsimilarity. Maxwell [12] also uses a large and varied \nfeature set extracted from lyrics to rank similarity. \nMayer et al. [13] provide a particularly helpful exami-\nnation of the classificatory power of various lyrical fea-\ntures with respect to genre. Kleedorfer et al. [5] and Wei \net al. [18] present strategies for identifying topics, which \ncan be adapted for use as classification features. Hirjee \nand Brown [3] present a sophisticated tool for extracting \nrhymes from lyrics, with a focus on hip-hop styles.  \nSome research has been performed on combining lyri-\ncal features with audio features in the context of artist, \ngenre and mood classification [4,7,8,13]. Brochu and \nFreitas [1] have done research on combining lyrical fea-\ntures with features extracted from symbolic music.  \n2.3 jMIR \njMIR [14] is a suite of software tools and other resources \ndeveloped for use in automatic music classification re-\nsearch. It was used to perform all of the experiments de-\nscribed in this paper. jMIR includes the following com-\nponents: \n• jAudio: An audio feature extractor. \n• jSymbolic: A symbolic feature extractor. \n• jWebMiner 2.0:  A cultural feature extractor. \n• ACE 2.0: A metalearning-based classifier. \n• jMusicMetaManager: Software for managing and \ndetecting errors in musical datasets.  \n• jMIRUtilities: Performs infrastructural tasks.  \n• ACE XML: Standardized MIR file formats.  \n• Codaich, Bodhidharma MIDI and SAC:  Musical \nresearch datasets. \nThe jMIR software is all implemented in Java, which \nhas advantages with respect to platform-independence. \nAll jMIR components are open-source and are distributed \nfree of charge at jmir.sourceforge.net. \n2.4 Comparing the Performance of Feature Types \nThis paper expands upon the research described in [15], \nwhich experimentally investigated the classification utility \nof combining features extracted from audio, symbolic and \ncultural sources of musical information using an earlier \nversion of jMIR. It was found that combining feature \ntypes did indeed substantially improve classification per-\nformance, in terms of both overall classification accuracy \nand the seriousness of those misclassifications that did \noccur. \nTo the best of the authors’ knowledge, [15] is the only \nprevious study involving cultural, symbolic and audio da-\nta. There have, however, been many important studies in-\nvolving features extracted from pairs of musical data \ntypes, including [9] and [19]. Section 2.2 highlights addi-\ntional work involving lyrics. 3. THE SLAC DATASET \nThe new SLAC (Symbolic Lyrical Audio Cultural) data-\nset is an expansion of the SAC dataset [15] that now in-\ncludes lyrics. The purpose of this dataset is to facilitate \nexperiments comparing the relative performance of fea-\ntures extracted from different types of musical data. \nSAC consists of 250 MP3 recordings, 250 matching \nMIDI recordings and identifying metadata for each re-\ncording. This metadata is stored in an iTunes XML file \nthat can be parsed by software such as jWebMiner in or-\nder to extract cultural features from the web. \nSLAC adds lyrics to all of the non-instrumental musi-\ncal pieces in SAC. These lyrics were mined from the In-\nternet, as described in Section 4. \nSLAC is divided into 10 genres, with 25 pieces of mu-\nsic per genre. These 10 genres consist of 5 pairs of similar \ngenres, as shown in Figure 1. This arrangement makes it \npossible to perform 5-class genre classification experi-\nments as well as 10-class experiments simply by combin-\ning each pair of related genres into one class, thus provid-\ning an indication of how well systems perform on both \nsmall and moderately sized genre taxonomies. \n \nBlues: Modern Blues and Traditional Blues  \nClassical: Baroque and Romantic  \nJazz: Bop and Swing  \nRap: Hardcore Rap and Pop Rap  \nRock: Alternative Rock and Metal \nFigure 1: The ten genres found in the SLAC dataset and \nthe five super-genres that they can be paired into.  \nSLAC includes some instrumental music. This compli-\ncates classification based on lyrics, as lyrics provide no \nway to distinguish one instrumental piece from another. \nNonetheless, the inclusion of some instrumental music is \nnecessary to evaluate classification performance properly, \nas one must simulate the music that classification systems \nwill encounter in practice, including instrumental music. \n4. MINING LYRICS WITH LYRICFETCHER \nA new lyrics mining script called lyricFetcher was \nimplemented in Ruby to automatically harvest lyrics from \nLyricWiki and LyricsFly. These two repositories were \nchosen for their large sizes and because of the simplicity \nof querying their collections: LyricsFly provides a simple \nAPI and LyricWiki offers a standardized URL naming \nscheme that is relatively easy to mine. \nOnce provided with a list of artist names and song \ntitles to search for, lyricFetcher obtains lyrics in three \nsteps: first, a query is made to the lyrics source; second, \nthe lyrics themselves are extracted from the result; and \nthird, lyrical content is cleaned and standardized in post-\nprocessing, an important step given the variability in for-\n214\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \nm\natting of user-contributed lyrics. In particular, raw re-\ntrieved lyrics are often abridged by providing a label for \nthe first occurrence of a section (e.g., “chorus,” “hook,” \n“refrain,” etc.) and repeating only this label when the sec-\ntion reoccurs. lyricFetcher automatically searches for and \nexpands such sections. Common keywords added to the \nlyrical transcriptions, such as “verse,” are also removed. \nlyricFetcher was used to mine LyricWiki and LyricsFly \nfor the lyrics to the recordings in Codaich and SLAC. \nThese lyrics were used in the experiments described be-\nlow in Section 6. Lyrics were manually retrieved from \nother web sources for the 20 pieces out of the 160 non-\ninstrumental pieces in SLAC for which lyrics could not be \nharvested automatically from LyricWiki and LyricsFly. \n5. EXTRACTING FEATURES FROM SLAC \n5.1 Lyrical Features Extracted \nA large number of features were implemented and ex-\ntracted based on a survey of previous work and on origi-\nnal ideas: AutomatedReadabilityIndex, AverageSyllab-\nlesPerWord,  ContainsWords, FleshKincaidGradeLevel, \nFleshReadingEase, FunctionWordFrequencies, LetterBi-\ngramComponents, LetterFrequencies, LettersPerWordA-\nverage, LettersPerWordVariance, LinesPerSegmentAve-\nrage,1 LinesPerSegmentVariance, NumberOfLines, Num-\nb\nerOfSegments, NumberOfWords, PartOfSpeechFre-\nquencies,2 PunctuationFrequencies, RateOfMisspelling, \nS\nentenceCount, SentenceLengthAverage, TopicMember-\nshipProbabilities,3 VocabularyRichness, VocuabularyS-\ni\nze, WordProfileMatch, WordsPerLineAverage and \nWordsPerLineVariance . Descriptions of these features \nare provided at jmir.sourceforge.net/index_jLyrics.html. \n5.2 The jLyrics Feature Extractor \nA new Java-based feature extraction framework called \njLyrics was implemented as part of this research. Like the \nexisting jMIR feature extractors, it is designed to serve as \nan easy-to-use feature extraction application as well as an \nextensible framework for developing new features. It has \nthe usual jMIR advantages in this respect [14], including \na modular architecture, automatic resolution of feature \ndependencies and the option of saving feature values in \nseveral file formats. Many of the features described in \nSection 5.1 were implemented directly in jLyrics, al-\nthough some features based on third-party libraries re-\nmain to be ported to the Java framework.  \nIn addition to extracting features jLyrics can, given \nsets of lyrics belonging to a class, generate profiling re-\nports indicating ranked lists of the most commonly used \n                                                           \n1 A “segment” is a unit of text separated by line breaks. \n2 Extracted using the Stanford parts-of-speech tagger [18]. \n3 Trained on Codaich (with SLAC instances filtered out) \nu\nsing latent Dirichlet allocation [8]. words in each class. These profiles can be used to “train” \nWordProfileMatch  features to measure how well novel \nlyrics match each class’ profile. Lyrics mined with lyric-\nFetcher for the music in Codaich (with all pieces in SLAC \nfiltered out) were used to do just this, in preparation for \nthe experiments described in Section 6. \n5.3 Audio, Symbolic and Cultural Feature Extraction \njMIR, as described in Section 2.3 and [14], was used to \nextract audio, symbolic and cultural features from SLAC. \nOf particular interest, the new jWebMiner 2.0 [17] soft-\nware was used to extract cultural features based on both \nYahoo! co-occurrence page counts and Last.FM user tags, \nas opposed to the older jWebMiner 1.0 used in [15], \nwhich only extracted features based on web searches. A \nnewer version of ACE, ACE 2.0, was also used.  \n6. EXPERIMENTAL PROCEDURE \nThe first step of the experiment was to extract feature \nvalues from SLAC, as described in Section 5. This re-\nsulted in a set of 26 features (A) extracted from the audio \nversion of each piece, 101 features (S) extracted from the \nMIDI version of each piece, 26 features (L) extracted \nfrom the lyrics for each piece and 20 features (C) ex-\ntracted from the Internet based on the identifying meta- \ndata for each piece.4 These four types of features were \nt\nhen grouped into all 15 possible subset combinations us-\ning jMIRUtilites. These feature groups are identified us-\ning the codes indicated in Table 1. \n \nFeature Types Identifying Code \nSymbolic S \nLyrical L \nAudio A \nCultural C \nSymbolic + Lyrical SL \nSymbolic + Audio SA \nSymbolic + Cultural SC \nLyrical + Audio LA \nLyrical + Cultural LC \nAudio + Cultural AC \nSymbolic + Lyrical + Audio SLA \nSymbolic + Lyrical + Cultural SLC \nSymbolic + Audio + Cultural SAC \nLyrical + Audio + Cultural LAC \nSymbolic + Lyrical + Audio + Cultural SLAC \nTable 1: The identifying codes for the feature type \ngroups used in each of the experiments.  \n \n                                                           \n4 The jMIR feature extractors are each capable of extract-\ni\nng more features than this, but were set to omit unpro-\nmising features in order to save processing time. Also, \nmany of the features that were extracted are in fact feature \nvectors consisting of multiple values. \n215\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \n \nFeatures 5-Genre Accuracy (%) 10-Genre Accuracy (%) \nS 85 66 \nL 69 43 \nA 84 68 \nC 100 86 \nSL 89 70 \nSA 95 74 \nSC 99 89 \nLA 88 66 \nLC 100 81 \nAC 100 85 \nSLA 93 77 \nSLC 99 84 \nSAC 100 89 \nLAC 99 83 \nSLAC  99 85 \nTable 2: Classification accuracies for each of the expe-\nriments. Feature codes are identified in Table 1. All val-\nues are averages across cross-validation folds.   \n \nFigure 2: Results of the 5-genre experiments, as detailed \nin Table 2. Feature set codes are defined in Table 1. \n \nFigure 3: Results of the 10-genre experiments, as de-\ntailed in Table 2. Feature set codes are defined in Table 1.  \n \nFigure 4: Classification accuracies averaged for all \ngroups of, respectively, 1, 2, 3 and 4 feature types. \n \nFigure 5: Average accuracies for feature groups includ-\ning cultural features (C), compared to groups without C. \n \nFigure 6: Average accuracies for feature groups includ-\ning lyrical features (L), compared to groups without L. \n216\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \n \njMIR ACE 2.0 was then used to classify each of these \n15 feature sets by genre in 30 separate 10-fold cross-\nvalidation metalearning-based experiments,5 such that \ne\nach of the 15 feature sets was processed once in a 5-\ngenre experiment and once in a 10-genre experiment. The \nresults of these experiments are shown in Table 2. Figures \n2 and 3 also represent this information graphically for the \n5- and 10-genre taxonomies, respectively. The error bars \non all figures represent standard error (i.e., the standard \ndeviation of the cross-validation accuracies divided by the \nsquare root of the number of measurements). \n7. RESULTS AND DISCUSSION \n7.1 Overall Classification Performance \nOverall, excellent classification accuracies were obtained \nwith jMIR, with peak performances of 100% on the 5-\ngenre taxonomy and 89% on the 10-genre taxonomy. For \nthe purpose of comparison, the MIREX (www.music-\nir.org/mirex/2010/) contests provide the best benchmark-\ning references available. The highest MIREX symbolic \ngenre classification performance to date is 84%, attained \non a 9-genre ontology, and all six audio genre classifica-\ntion evaluations to date on genre ontologies larger than \nsix classes have failed to achieve success rates above \n80%. Although it is inappropriate to compare results ob-\ntained on different datasets directly, this does cast the re-\nsults obtained here with jMIR in a favourable light. \n7.2 Effect on Accuracy of Combining Feature Types  \nThe next thing to consider was, now that lyrical features \nwere included and the new jWebMiner 2.0 cultural fea-\ntures were used, whether combining different feature \ntypes still improved classification performance, as was the \ncase in [15]. Figure 4 demonstrates the results of averag-\ning together the classification accuracies of all feature \ngroups with the same number of feature types (i.e., S, L, \nA and C; SL, SA, SC, LA, LC and AC; etc.), with a sepa-\nrate curve for each of the two genre taxonomies. It can be \nseen that, on average, classification accuracy did indeed \nincrease with the number of feature types available. \n It thus appears, at least upon first consideration, that \ncombining features from different types of data does tend \nto improve performance. A closer examination of Table 2 \nshows that this was only true on average, however, as \n                                                           \n5 A validation partition was reserved for each of the 30 \ne\nxperiments in order to guard against overfitting. Any ex-\nperiment that resulted in an average cross-validation suc-\ncess rate that was higher than the validation performance \nwith statistical significance was redone. It should also be \nnoted that ACE includes dimensionality reduction func-\ntionality, so training was actually performed with auto-\nmatically chosen subsets of the available features in order \nto avoid the “curse of dimensionality.” there were some cases where combining feature groups \nactually decreased performance (e.g., LC performed less \nwell than C in the 10-genre experiments). Furthermore, an \nexamination of Figure 5, described below, suggests that \nthere was no advantage to combining cultural features in \ngeneral with any other feature types. \n7.3 Effectiveness of Cultural Features \nFigure 5 shows, for the 10-class taxonomy, the average \nperformance of all feature groups of the same size that \ncontain cultural features, compared with the average per-\nformance of all feature groups of the same size that do not \ncontain cultural features. The experimental results as a \nwhole demonstrate that, for both taxonomies, cultural fea-\ntures significantly outperformed all other feature types.6 \nT\nhis dominance of cultural features was not apparent \nin [15], which only used cultural features derived from \nweb searches. As described in [17], the new jWebMiner \n2.0 combines these features with additional tag-based fea-\ntures extracted from Last.FM. This is likely responsible \nfor the much higher performance of cultural features in \nthis study relative to the results from [15]. \n7.4 Effectiveness of Lyrical Features \nFigure 6 shows, for the 10-class taxonomy, the average \nperformance of all feature groups of the same size that \ncontain lyrical features, compared with the average per-\nformance of all feature groups of the same size that do not \ncontain lyrical features. The results indicate that lyrical \nfeatures were significantly less effective than the other \nfeature types.7 It is notable, however, that combining lyri-\nc\nal features with other feature types did, in some but not \nall cases, improve performance relative to the features \noperating individually. This is true for SL and SLA in \nboth the 5- and 10-genre experiments. Furthermore, it is \nimportant to emphasize that 90 of the SLAC recordings \nwere instrumental (although these recordings were strong-\nly correlated with the Jazz and Classical genres). \n8. CONCLUSIONS \nThis paper introduces the lyricFetcher and jLyrics tools \nfor, respectively, mining lyrics from the web and extract-\ning features from them. These tools are available for use \nin other research projects, and jLyrics in particular is de-\nsigned to provide an easily extensible framework for im-\nplementing, testing and extracting new features. \nWith respect to the experiments described in this pa-\nper, excellent overall classification accuracies were ob-\ntained relative to the current state of the art of genre clas-\n                                                           \n6 Based on a Wilcoxon signed-rank test with a signifi-\nc\nance level of 0.05.  \n7 Based on a Wilcoxon signed-rank test with a signific-\na\nnce level of 0.05. \n217\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \ns\nification. In particular, the jWebMiner 2.0 cultural fea-\ntures based on both web searches and listener tags ex-\ntracted from Last.FM were especially effective. It was al-\nso found that combining different feature types improved \nperformance on average if cultural features were unavail-\nable, but was not necessary if cultural features were avail-\nable. With respect to lyrical features, it was found that \ncombining them with other types of features did, in cer-\ntain cases, improve classification performance. Overall, \nhowever, lyrical features performed poorly relative to the \nother feature types. \nThe disappointing performance of the lyrical features \nwas probably due in part to noisiness in the mined lyrical \ntranscriptions, including inconsistent annotation practices, \noccasional errors and the inclusion of non-standardized \nmarkup in XML and other formats. The relatively low \nperformance of lyrics was likely also partly due to inhe-\nrent limitations with respect to classifying instrumental \nmusic, as well as to the general-purpose text mining \norientation of the lyrical features used. This highlights the \nneed for continued research on more specialized music-\noriented lyrical features, and on still better lyric mining \nand cleaning methodologies. Both of these could poten-\ntially lead to significantly improved performance by lyri-\ncal features. \n9. REFERENCES \n[1] Brochu, E., and N. de Freitas. 2003. “Name that \nsong!”: A probabilistic approach to querying music \nand text. In Advances in Neural Information \nProcessing Systems  15, 1505–12. Cambridge, MA: \nMIT Press. \n[2] Geleijnse, G., and J. Korst. 2006. Efficient lyrics \nextraction from the web. Proc. of the Int. Conference \non Music Information Retrieval. 371–2. \n[3] Hirjee, H., and D. G. Brown. 2009. Automatic \ndetection of internal and imperfect rhymes in rap \nlyrics. Proc. of the Int. Society for Music \nInformation Retrieval Conference . 711–6. \n[4] Hu, X, J. S. Downie, and A. F. Ehman. 2009. Lyric \ntext mining in music mood classification. Proc. of \nthe Int. Society for Music Information Retrieval \nConference . 411–6. \n[5] Kleedorfer, F., P. Knees, and T. Pohle. 2008. Oh oh \noh whoah! Towards automatic topic detection in \nsong lyrics. Proc. of the Int. Conference on Music \nInformation Retrieval . 287–92. \n[6] Knees, P., M. Schedl, and G. Widmer. 2005. \nMultiple lyrics alignment: Automatic retrieval of \nsong lyrics. Proc. of the Int. Conference on Music \nInformation Retrieval. 564 –9. [7] Laurier, C., J. Grivolla, and P. Herrera. 2008. \nMultimodal music mood classification using audio \nand lyrics. Proc. of the Int. Conference on Machine \nLearning and Applications.  688–93. \n[8] Li, T., and M. Ogihara. 2004. Semi-supervised \nlearning from different information sources. \nKnowledge and Information Systems 7 (3): 289–309. \n[9] Lidy, T., A. Rauber, A. Pertusa, and J. M. Iñesta. \n2007. Improving genre classification by combination \nof audio and symbolic descriptors using a \ntranscription system. Proc. of the Int. Conference on \nMusic Information Retrieval . 61–6. \n[10] Logan, B., A. Kositsky, and P. Moreno. 2004. \nSemantic analysis of song lyrics. Proc. of the IEEE \nInt. Conference on Multimedia and Expo . 827–30. \n[11] Mahedero, J. P. G., Á. Martínez, and P. Cano. 2005. \nNatural language processing of lyrics. Proc. of the \nACM Int. Conference on Multimedia.  475–8. \n[12] Maxwell, T. 2007 Exploring the Music Genre: Lyric \nClustering with Heterogeneous Features . M.Sc. \nthesis, University of Edinburgh. \n[13] Mayer, R., R. Neumayer, and A. Rauber. 2008. \nCombination of audio and lyrics features for genre \nclassification in digital audio collections. Proc. of \nthe ACM Int. Conference on Multimedia . 159–68. \n[14] McKay, C. 2010. Automatic music classification \nwith jMIR. Ph.D. Dissertation . McGill University, \nCanada. \n[15] McKay, C., and I. Fujinaga. 2008. Combining \nfeatures extracted from audio, symbolic and cultural \nSources. Proc. of the Int. Conference on Music \nInformation Retrieval . 597–602. \n[16] Neumayer, R., and A. Rauber. 2007. Integration of \ntext and audio features for genre classification in \nmusic information retrieval. Proc. of the European \nConference on IR Research. 724–7. \n[17] Vigliensoni, G., C. McKay, and I. Fujinaga. 2010. \nUsing jWebMiner 2.0 to improve music \nclassification performance by combining different \ntypes of features mined from the web. Accepted for \npublication at the Int. Society for Music Information \nRetrieval Conference . \n[18] Wei, B., C. Zhang, and M. Ogihara. 2007. Keyword \ngeneration for lyrics. Proc. of the Int. Conference on \nMusic Information Retrieval . 121–2. \n[19] Whitman, B., and P. Smaragdis. 2002. Combining \nmusical and cultural features for intelligent style \ndetection. Proc. of the Int. Symposium on Music \nInformation Retrieval . 47–52. \n218\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "Geoshuffle: Location-Aware, Content-based Music Browsing Using Self-organizing Tag Clouds.",
        "author": [
            "Scott Miller",
            "Paul Reimer",
            "Steven R. Ness",
            "George Tzanetakis"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1417703",
        "url": "https://doi.org/10.5281/zenodo.1417703",
        "ee": "https://zenodo.org/records/1417703/files/MillerRNT10.pdf",
        "abstract": "In the past few years the computational capabilities of mobile phones have been constantly increasing. Frequently these smartphones are also used as portable music players. In this paper we describe GeoShuffle – a prototype system for content-based music browsing and exploration that tar- gets such devices. One of the most interesting aspects of these portable devices is the inclusion of positioning ca- pabilities based on GPS. GeoShuffle adds location-based and time-based context to a user’s listening preferences. Playlists are dynamically generated based on the location of the user, path and historical preferences. Browsing large music collections having thousands of tracks is challenging. The most common method of inter- action is using long lists of textual metadata such as artist name or genre. Current smartphones are characterized by small screen real-estate which limits the amount of tex- tual information that can be displayed. We propose self- organizing tag clouds, a 2D tag cloud representation that is based on an underlying self-organizing map calculated using automatically extracted audio features. To evalute the system the Magnatagatune database is utilized. The evaluation indicates that location and time context can im- prove the quality of music recommendation and that self- organizing tag clouds provide faster browsing and are more engaging than text-based tag clouds.",
        "zenodo_id": 1417703,
        "dblp_key": "conf/ismir/MillerRNT10",
        "keywords": [
            "content-based music browsing",
            "portable music players",
            "GeoShuffle",
            "positioning capabilities",
            "location-based context",
            "time-based context",
            "dynamic playlist generation",
            "large music collections",
            "textual metadata",
            "self-organizing tag clouds"
        ],
        "content": "GEOSHUFFLE: LOCATION-AWARE, CONTENT-BASED MUSIC\nBROWSING USING SELF-ORGANIZING TAG CLOUDS\nScott Miller, Paul Reimer\nUniversity of Victoria\nElectrical and Computer Engineering\nsmiller@ece.uvic.caSteven Ness, George Tzanetakis\nUniversity of Victoria\nComputer Science\ngtzan@cs.uvic.ca\nABSTRACT\nIn the past few years the computational capabilities of\nmobile phones have been constantly increasing. Frequently\nthese smartphones are also used as portable music players.\nIn this paper we describe GeoShufﬂe – a prototype system\nfor content-based music browsing and exploration that tar-\ngets such devices. One of the most interesting aspects of\nthese portable devices is the inclusion of positioning ca-\npabilities based on GPS. GeoShufﬂe adds location-based\nand time-based context to a user’s listening preferences.\nPlaylists are dynamically generated based on the location\nof the user, path and historical preferences.\nBrowsing large music collections having thousands of\ntracks is challenging. The most common method of inter-\naction is using long lists of textual metadata such as artist\nname or genre. Current smartphones are characterized by\nsmall screen real-estate which limits the amount of tex-\ntual information that can be displayed. We propose self-\norganizing tag clouds, a 2D tag cloud representation that\nis based on an underlying self-organizing map calculated\nusing automatically extracted audio features. To evalute\nthe system the Magnatagatune database is utilized. The\nevaluation indicates that location and time context can im-\nprove the quality of music recommendation and that self-\norganizing tag clouds provide faster browsing and are more\nengaging than text-based tag clouds.\n1. INTRODUCTION\nPortable mobile phones with strong multimedia capabili-\nties and computational power are rapidly gaining popular-\nity. As these devices frequently also function as portable\ndigital music players it is important to investigate how mu-\nsic information retrieval systems can be adapted to the unique\nchallenges and opportunities they present. In this paper\nwe describe GeoShufﬂe a music browsing application de-\nsigned to address the challenge of limited screen real estate\nand to take advantage of the opportunity of location infor-\nmation that smart phones provide.\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2010 International Society for Music Information Retrieval.Automatic music recommendation is an active topic of\nresearch. Such systems can be based on collaborative ﬁl-\ntering, expert annotations, folksonomies, automatic con-\ntent analysis and any of their combinations. However, all\nthese approaches suffer from the limitation that their re-\nsults are the same irrespective of the listening context. The\npreferences of a listener change depending on where they\nare and what they are doing. For example the music a stu-\ndent would like recommended when studying might be dif-\nferent from the music desired when riding the bus.\nLocation-aware devices based on technologies such as\nGPS are common. We propose that the quality of auto-\nmatically generated playlists can be improved by taking\ninto account this newly available location data. This infor-\nmation can be used to determine a user’s listening habits\nwhile in transit to common destinations, as people often\nhave daily routines such as return trips to work, school, so-\ncial activities, and so on. It provides context to a user’s\nlistening preferences beyond general ratings. A user pro-\nviding a rating to a song does not provide context about\nthe conditions under which a user would enjoys listening\nto that song. For example, a high-energy song that a user\nrates highly may never be desired when the user wants to\nrelax.\nAnother unique characteristic of smart phones is their\nlimited screen real-estate. The size of personal digital au-\ndio collections is steadily increasing. Effective interaction\nwith these large audio collections poses signiﬁcant chal-\nlenges to traditional user interfaces. Music management\nsoftware typically allow users to select artist, genres or in-\ndividual tracks by browsing long sortable lists of text. This\nmode of interaction, although adequate for small music\ncollections, becomes increasingly problematic as collec-\ntions become larger especially when screen estate is lim-\nited. A variety of alternative ways of browsing music col-\nlections have been proposed mostly in academic contexts.\nThey typically rely on a combination of audio signal anal-\nysis to automatically extract features followed by visual-\nization techniques to map the feature space to a 2D or 3D\nrepresentation for browsing and navigation.\nTag clouds provide both an overview of the information\nspace as well as direct search support that is particularly\nsuited for mobile phones with small touch screens. In this\npaper, we present content-aware self-organizing tag clouds\na technique that attempts to support querying, browsing,\nand summarization using the familiar information model\n237\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)of a tag cloud while taking into account automatic content\nanalysis information as well as location based information.\n2. RELATED WORK\nAlthough there is existing work in location-based appli-\ncations and automatic/semi-automatic playlist generation\nthere seems to be a lack of published material on location-\naware playlist generation. With respect to intelligent playlist\ncreation, Flexer et al. have proposed using audio similarity\nbased on Mel Frequency Cepstrum Coefﬁcients (MFCC)\nand Gaussian models to create a similarity matrix and se-\nlect songs that blend from and into a user-selected start and\nend track in a playlist [1]. Pampalk et al. have proposed\nusing user behaviour based on track skipping to determine\nwhat artists, genres, rhythms, etc., the user prefers to pass-\nover [5]. With respect to location-aware playlist creation\nmost existing work simply associates particular pieces of\nmusic with speciﬁc locations [7].\nThe current generation of mobile phones feature decent\nsized displays that also include touch functionality. Inter-\nfaces for managing large audio collections based on long\nlists of scrollable text are not particularly convenient in\nsuch displays. An alternative that has mostly been explored\nin research literature is the use of content-based visualiza-\ntions of music collections [4].\nTagging systems allow users to add keywords, or tags,\nto resources without relying on a controlled vocabulary\nand have become ubiquitous in web-based systems. Tags\nare aggregated from many users forming “folksonomies”\nwhich, although not as accurate as well-designed ontolo-\ngies, have the advantage of reﬂecting how users perceive\nthe data and how their vocabulary and perception evolve\nover time. Tagging is simple and does not require a lot of\nthinking. Tags form an essential part of personalized inter-\nnet radio and music community websites such as Last.fm\n1. Tag clouds are the most common way of visualizing\ntags. They are two-dimensional stylized visual representa-\ntions of a list of words where the more prominent words\nare typically assigned a larger font. They are useful for\nquickly giving users the gist of a set of words. Tag clouds\nare in common usage on a number of different social net-\nworks such as Flickr2but trace their origins back at least\n90 years to Soviet Constructivist art [16].\nThere has been considerable research in recent years\ninto the design, use and effectiveness of tag clouds. A\nhistorical look at tag clouds is presented in Viegas and\nWattenburg [16], which looks at the development of tag\nclouds since their inception a decade ago, and speculates\nabout their development in the future. In the paper “Seeing\nthings in clouds” [2], an extensive evaluation of different\ntypes of visual features in tag clouds, including font size,\nfont weight, intensity, number of characters and area were\ninvestigated. Tag navigation in general has been examined\nin detail with particular focus on “Last.FM”, an online so-\ncial community for music [10]. A context aware browser\n1http://www.last.fm\n2http://www.flickr.comfor mobile devices that uses tag clouds is presented in Miz-\nzaro et al. [11].\nIslands of Music [12] is a a content-based visualiza-\ntion of music collections that uses Self-Organizing Maps\n(SOM) to generate a two-dimensional representation of a\ncollection of music. MusiCream [8] is an interface that\nallows users to interact with a music collection using a\ndynamic visualization interface. MusicRainbow [13] is\na similar system that uses web-based labelling and audio\nsimilarity to visualize music collections. Examples of vi-\nsualizations for music discovery in commercial and research\nsystems can be found in the Visualizing Music blog3.\n3. SYSTEM DESCRIPTION\nOur proposed system takes as input the user’s location, the\ncurrent playing and associated metadata as well as content-\nbased similarity information between all tracks in a user\ncollection. This information is stored in a database for or-\nganization and retrieval. The system processes these in-\nputs to generate location-based information such as com-\nmon paths and make automatic recommendations based\non them. Semantic information related to the generated\nplaylists such as track names, artists, genres, tags, playlists\nare rendered based on self-organizing tag clouds that are\ncomputed based on automatically extracted audio features.\n3.1 Location and Path Logging\nWe introduce the following terms to describe location in-\nformation: Paths consist of a start and end location and a\ncollection of Path Segments which consist of a start, end,\nbearing and segment speed. The Path Segments are deter-\nmined by a list of Location Points which are instantaneous\nsnapshots of what song is playing and where. This includes\na track’s metadata (artist, album, title, etc.), current coor-\ndinate and time, and whether a song started or skipped.\nAs a user’s location or music changes and location points\nare generated, the system interpolates the user’s current\nline-of-travel in real-time and generates a path segment\nconsisting of a line between start and end coordinates. These\npath segments are then associated to a path from the start\nlocation of the ﬁrst path segment to the end location of\nthe last path segment. These paths can then be proﬁled by\ncounting the songs that are played or skipped, the most lis-\ntened to genres or tempos, etc.; therefore, as the user builds\nup a path history, it can be used to generate a more accurate\nrepresentation of the user’s listening tastes.\nOne of the challenges of determining path segments is\nthat location estimates vary in accuracy and are sampled\nirregularly. In addition a user following the same path in\ndifferent days (for example taking the bus to school) will\nnot have exactly the same set of location points. Therefore\nwe have developed an algorithm for determining determin-\ning path segments from a running list of location points.\nThe basic idea is to ﬁrst determine the bearing between the\nﬁrst two location points in a path segment. Subsequently\n3http://visualizingmusic.com/\n238\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Figure 1. Visualization of paths and location points on a\nmap and schematic of path ﬁnding algorithm\nthe bearing between the start point of the segment and sub-\nsequent points is determined. If the new point has the same\nbearing as the original pair, the new point becomes the end\nto the segment. This continues until a coordinate yields a\nbearing of the current segment’s path. This basic algorithm\nworks when travelling in very straight lines, and with very\naccurate positioning hardware, but in real world usage will\ngenerate segments between almost every pair of points, as\nany deviation in bearing will result in a new segment being\ngenerated.\nIn order to account for the accuracy of the positioning\nsystem, an algorithm was devised to allow for variation\nin the absolute location based on the intrinsic accuracy of\nthe mobile device. Each absolute position is reported as a\nbox bounded by the accuracy of the device. Consequently,\nany points in the bounding box are considered the same\nabsolute coordinate. The same bounding box is used in\ncalculating the bearing for path segments.\nThese located segments are combined from a start lo-\ncation to an end location in order to generate a path. Fig-\nure 1 shows a schematic diagram of the algorithm and a\nmap with paths and location points overlayed. Currently,\na path is started when the ﬁrst change in a user’s location\nis sensed. A path is ended when a user stays at a location\nfor more than 15 minutes. Basic equations for ﬁnding dis-\ntances based on decimal degree coordinates for latitudes\nand longitudes, and for ﬁnding the bearing between two\ncoordinates are based on the WGS84 world representation\n(currently used by GPS systems).\n3.2 Audio Feature Extraction and Recommendations\nThe goal of audio feature extraction is to represent each\ntrack as a vector of features that characterize musical con-\ntent. First low-level features such as the Spectral Centroid,\nRolloff, Flux and the Mel-Frequency Cepstral Coefﬁcients\n(MFCC) are computed approximately every 20 millisec-\nonds. To capture the feature dynamics we compute a run-\nning mean and standard deviation over the past M frames\n(the so-called “texture window” typically around 1 sec-\nond). The result is a feature vector of 32 dimensions at\nthe same rate as the original 16D feature vector. The se-\nquence of feature vectors is collapsed into a single feature\nvector representing the entire audio clip by taking again\nthe mean and standard deviation across the 30 seconds (of\nthe sequence of dynamics features), resulting in the ﬁnal64D feature vector per audio clip. A more detailed descrip-\ntion of the features and their motivation can be found in\nTzanetakis and Cook [15]. For the calculation of the self-\norganizing map described in the next section all features\nare normalized so that the minimum of each feature across\nthe music collection is 0 and the maximum value is 1. This\nfeature set has shown state-of-the-art performance in audio\nretrieval and classiﬁcation tasks for example in the Mu-\nsic Information Retrieval Evaluation Exchange (MIREX)\n2008 and was computed using the free Marsyas audio pro-\ncessing framework4. Most audio feature sets proposed\nexhibit similar performance so we expect that any audio\nfeature front end can be used.\nBased on a distance matrix calculated between all pairs\nof tracks, 3 different recommendation algorithms are im-\nplemented. In the naive similarity case, a random seed-\nsong is selected, and playlists of the ten most similar songs\n(based on pre-calculated Euclidean distances) were cre-\nated. If the user skipped a song, a new seed is selected and\na new playlist is generated along with it. In the similary-\nwith-history case, a proﬁle is constructed based on songs\nthe user listened to at the same time and day of the week to\nrecommend similar songs. A seed song is selected based\non tracks that the user enjoyed at similar times (current\ntime +=\u0000an hour) in the past and their three nearest neigh-\nbours. If a user skipped a track, a new seed based on their\nhistory is selected and a new playlist is generated. Using\nlocation information, the system predicted a path that the\nuser is taking and selects a seed from a similar track that\nwas listened to on that path previously. Finally we pro-\nvide interactive control to the speciﬁcity of the generated\nplaylists using the accelerometers included in more mobile\ndevices. Shaking the device at varying levels results in se-\nlecting seeds scuh that recommendations are more similar\nif the shake is light and less similar if it is heavy.\n3.3 Self-Organizing Maps\nFor creating the visualization layout we utilized the self-\norganizing map (SOM) which is a type of neural network\nused to map a high dimensional feature space to a lower\ndimensional representation while preserving the topology\nof the high dimensional space. This facilitates both sim-\nilarity quantization and visualization simultaneously. The\nSOM was ﬁrst documented in 1982 by T. Kohonen, and\nsince then, it has been applied to a wide variety of diverse\nclustering tasks [14]. In our system the SOM is used to\nmap the audio features (64-dimensions) corresponding to\neach track to two discrete coordinates on a grid.\nThe traditional SOM consists of a 2D grid of neural\nnodes each containing a n-dimensional vector, x(t)of data.\nThe goal of learning in the SOM is to cause different neigh-\nbouring parts of the network to respond similarly to certain\ninput patterns. The network must be fed a large number of\nexample vectors that represent, as closely as possible, the\nkinds of vectors expected during mapping. The data asso-\nciated with each node is initialized to small random values\nbefore training. During training, a series of n-dimensional\n4http://marsyas.info\n239\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)vectors of sample data are added to the map. The “win-\nning” node of the map known as the best matching unit\n(BMU) is found by computing the distance between the\nadded training vector and each of the nodes in the SOM.\nThis distance is calculated according to some pre-deﬁned\ndistance metric which in our case is the standard Euclidean\ndistance on the normalized feature vectors.\nOnce the winning node has been deﬁned, it and its sur-\nrounding nodes reorganize their vector data to more closely\nresemble the added training sample. The training utilizes\ncompetitive learning. The weights of the BMU and neu-\nrons close to it in the SOM lattice are adjusted towards the\ninput vector. The magnitude of the change decreases with\ntime and with distance from the BMU. The time-varying\nlearning rate and neighborhood function allow the SOM to\ngradually converge and form clusters.\n3.4 Self-Organizing Tag Clouds\nThe technique of self-organizing tag clouds can be viewed\nas a fusion of concepts from text-based visualization inter-\nfaces and more abstract content-aware visualization inter-\nfaces. We use the term tag loosely to denote any metadata\nassociated with a track such as genre, artist or year of re-\nlease. Traditional systems based on long lists of sortable\ntext such as iTunes provide little support for browsing, dis-\ncovery and summarization. An alternative is visualization\ninterfaces that are based on automatic analysis of musical\ncontent. By mapping the music collection onto a 2D or 3D\nrepresentation they enable quick browsing and navigation\nespecially in the case of music that is not known to the user\nor that has not been tagged.\nTag-clouds provide a simple, familiar interface that partly\novercomes these limitations. For example they support\nboth direct searching as well as browsing and navigation.\nHowever they come with their own problems. In order for\na tag to assist search or browsing it is necessary for the user\nto have some notion of its meaning. For example a special-\nized term such as indie pop might be completely unfamiliar\nto a particular listener while at the same time essential to\nanother. This problem becomes even more acute using the\nmore generalized notion of tags that includes information\nsuch as artist or album. As one of the goals for an effec-\ntive interface of music collection browsing is the discovery\nof new music by artists not known to the listener, this is\nan important disadvantage. Simple tag clouds do not pro-\nvide the user with any information about the connections\nand similarity relations between tags. A ﬁnal problem with\nany system based solely on tag information is that there is\nno way to access music tracks that have not been tagged\n(the so-called “cold start” problem). By contrast content-\nbased visualizations allow any track to be accessed and do\nnot require familiarity with the music explored.\nWe describe a new method for organizing music tag\nclouds that makes a persistent map taking into account the\nmusical similarity between songs. Figure 2 shows an ex-\nample of a self-organized tag cloud. Each label (artist,\ngenre, tag) is associated with a set of tracks that have been\nannotated with it. As the tracks have been mapped to fea-\nWind\nFlute\nFolk\nChoir\nOpera\nMonksSilenceDiﬀerentDrums\nFunky\nElectroHeavy Metal\nClassicalPopOrgan\nMale Singer\nFemale VocalElectronic\nJazzyPunk\nLoud\nDark\nHarp\nMedieval\nMenFemale Voice\nSinging\nVoicesQuietHeavy\nBeatsPiano\nWind\nFlute\nFolk\nChoir\nOpera\nMonksSilenceDiﬀerentDrums\nFunky\nElectroHeavy Metal\nClassical\nPopOrgan\nMale Singer\nFemale VocalElectronic\nJazzyPunk\nLoud\nDark\nHarp\nMedieval\nMenFemale Voice\nSinging\nVoicesQuietHeavy\nBeatsPianoFigure 2. Self-Organizing tag cloud before and after mass-\nspring layout algorithm\nture vectors and subsequently to 2D grid coordinates by\nthe SOM, each tag is associated with a set of 2D grid coor-\ndinates. The SOM process ensures that neighboring points\n(tracks) will have similar high-dimensional audio features\nand therefore similar musical content. The tags are placed\non the centroids of their corresponding set of 2D grid co-\nordinates. Their placement reﬂects the underlying musical\ncontent but results in visual overlap between them.\nThis initial layout contains many overlapping words, so\nthe position of each tag is repositioned using a mass, spring\nand damper force-based algorithm for drawing [6]. In our\nimplementation each tag is anchored to its original position\nusing a spring and an electrostatic-like force is applied be-\ntween every pair of tags that is proportional to the inverse\nof their squared distance. Therefore tags that are close and\noverlapping will be pushed away while still trying to re-\nmain close to their original location. An additional wall\nforce term was added to keep all tags within the desig-\nnated window. The font size for each tag was determined\nby counting the number of instances of that tag.\nThere are some interesting characteristics of the result-\ning visualization that we would like to highlight. The ﬁrst\nis that tags that are not correlated with the acoustical con-\ntent will correspond to tracks spread across the underlying\nself-organizing map and therefore their placement will be\nin the center. For example in Figure 2 the tags Male Singer,\nSinging and Female V ocal are near the center as they have a\nlarge variety of tracks that have been annotated with them.\nIn contrast more specialized tags such as Heavy Metal or\nMonks are more localized. The second important charac-\nteristic is that faceted browsing is naturally supported. For\nexample an artist name, that the user might not be familiar\nwith, located near the left corner will correspond to the tag\nMonks. Finally a track for which there are no tag annota-\ntions will still be placed on the underlying self-organizing\nmap and that way receive an implicit visual automatic tag\nannotation addressing to some extent the cold-start prob-\nlem.\n3.5 Implementation\nThe feature extraction, music similarity calculation and self-\norganizing map training are performed using the Marsyas\naudio processing framework. Our current prototype appli-\ncation GeoShufﬂe has been implemented for Apple Inc.’s\niPhone or iPod Touch devices. The application dynami-\ncally generates music playlists that can be played in the\n240\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)default iPhone/iPod Touch music player based on location,\npath of travel, historical information and content similar-\nity. To provide feedback to the user on their preferences\nby path, as well as to test the accuracy of the application,\na Google Map generated map has been embedded into the\napplication (see Figure 1). This map supports annotations\nin the form of paths or absolute location points. The de-\nvice’s positioning system provides real-time updates on the\nuser’s absolute position. This allows the user to visually\ntrace their daily commutes and inspect their musical taste\nover each path.\n4. EVALUATION\nEvaluating a complex system and user interface such as the\none described in this paper is challenging due to its subjec-\ntive nature. We focus on two aspects of our work: 1) the\nuse of self-organizing tag clouds as a way to explore large\nmusic collections that combines text and content informa-\ntion without requiring large displays 2) the use of location\ninformation to improve music recommendation.\nFor evaluation purposes we used a subset of the Mag-\nnatagatune dataset consisting of 1141 tracks with each artist\nrepresented by at most 3 tracks. This was chosen as a large\nenough dataset to have considerable variability while at the\nsame time being manageable in the limited storage of the\niPod Touch used for development. There are 341 artists\nrepresented and also 14 top-level genre labels. In addi-\ntion to the regular meta-data information such as artist and\ngenre, also includes tags derived from the Tagatune Game\nwith a purpose [9]. The dataset has been made available to\nthe scientiﬁc community for use in research.\nFor evaluating the self-organizing tag clouds, 14 partic-\nipants were recruited from graduate Computer Science stu-\ndents. Three were female and 11 were male. All subjects\nhad normal or corrected-to-normal vision, enjoyed listen-\ning to music and were experienced computer users. None\nof the participants had previous knowledge of the Mag-\nnatune dataset. The user study consisted of a 5-point sys-\ntem usability survey (SUS) [3].\nThe survey consisted of six questions, each rated on\na ﬁve point scale, where “1” was labelled “Strongly dis-\nagree” and “5” was labelled “Strongly agree”. The 6 ques-\ntions were: 1) I thought the application was easy to use,\n2) I needed to learn a lot before I could accomplish tasks\nwith the application, 3) I think people would need technical\nsupport to learn how to use the application, 4) I think most\npeople would learn to use the application very quickly, 5)\nOverall, accomplishing tasks using the self-organizing tag\ncloud was easy 6) Overall, accomplishing tasks using the\nself-organizing tag cloud was fun\nResults from survey are detailed in Table 1. On average\nusers rated Question 4 highest, which indicated that they\nthought most other people would be able to learn the appli-\ncation quickly. This question also had the lowest variance.\nIn Table 1 we detail all the responses from the participants.\nWe can see that two participants chose the middle check\nbox, six chose the next one to the right, and six chose the\ncheckbox labelled “Strongly agree”.Table 1. System Usability Survey\nQuestion 1 2 3 4 5 Mean Std\n1 0 1 3 8 2 3.79 0.8\n2 5 7 1 1 0 1.86 0.86\n3 5 3 3 1 2 2.43 1.45\n4 0 0 2 6 6 4.29 0.73\n5 0 2 1 4 7 4.14 1.1\n6 0 2 0 6 6 4.14 1.03\nFigure 3. Screen shot of playlist visualization using the\nSelf-Organizing Tag Cloud\nIn a similar vein, participants also rated questions 5 and\n6 highly, although notably, two participants rated this ques-\ntion as one box to the right of “Strongly Disagree”. This\nshows that certain users found our interface easy to use and\nﬁt in well with their expectations of an interface to explore\nmusic collections, but for other users it did not. For Ques-\ntion 2, the average response was 1.85, which implies that\non average, users strongly disagree that they would have\nto learn a lot before accomplishing tasks with this applica-\ntion. It is important to include negative examples on such a\nuser study to ensure that participants are not just choosing\nanswers to questions randomly; this question performs this\ncontrol function.\nFor evaluating the location-aware music recommenda-\ntion component it was necessary to collect data over an\nextended period of usage. Usage data was collected from\nonly one subject. The subject used the system over a pe-\nriod of three weeks through their daily routine. GeoShufﬂe\nlogged their musical preference over the time period and\ngenerated sets of user paths (consisting of an origin, des-\ntination, and linear path segments). The device switched\nbetween four modes of recommendation without the user’s\nknowledge (random, similarity, similarity with history, sim-\nilarity with location-awareness) and logged which tracks\nwere skipped throughout operation. These results were\nthen used to determine the amount of user skips in each\nmode of recommendation without biasing the data.\nSelf-organizing tag clouds can also be used to visual-\nize text information associated with a playlist. Figure 3\nshows the self-organizing tag cloud text associated with\nthree playlists (from left to right: random, similarity and\npath). The ﬁgure clearly shows the increase in speciﬁcity\nand the content distribution of the recommended playlists.\n241\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Table 2. Number of skips and genres present in playlists\ncreated with different generators\nSkips / Track Played Genres in Playlist\nRandom 4.3 12\nSimilarity 1.7 7\n+ History 1.2 3\n+ Path 0.3 10\nTable 2 shows the analysis of skipping behavior be-\ntween different conﬁgurations of the system. We assume\nthat playlists that result in less skipping are better and show\nthe results as average number of skips per track played.\nThe baseline of 4.3 corresponds to randomly selecting songs\nfrom the collection in similar fashion to the iPod shufﬂe.\nThe similarity conﬁguration returns tracks that are similar\nto all the tracks played in the logging period. The history\nconﬁguration in addition to similarity takes into account\nthe time of the day. The last conﬁguration also takes into\naccount information about paths taken during the day and\nis the only one that requires portable devices with location\ninformation. As can be seen there is a signiﬁcant reduction\nin the number of skips when taking into account location\ninformation.\n5. CONCLUSIONS AND FUTURE DIRECTIONS\nIn this paper we describe our investigations in designing an\ninterface for content-aware music browsing, discovery and\nrecommendation that is designed based on the unique char-\nacteristics of modern smartphones. We propose using lo-\ncation information to improve the quality of music recom-\nmendations and introduce self-organizing tag clouds: a vi-\nsualization of metadata information such as genres, artists,\ntags and playlists that takes into account automatically ex-\ntracted musical content information. The speciﬁcity of the\nmusic recommendation algorithm can be interactively con-\ntrolled using the accelerometers. The resulting interface is\nparticularly suited for small screen real-estate and touch-\nscreens. Our evaluation indicates that self-organizing tag\nclouds are an effective and fun way of exploring music\ncollections and that location information can improve the\nquality of music recommendations.\nThere are many directions for future work. We plan to\nexplore visualizing tag-based similarities as edges between\ntags with proportional thickness. Another interesting di-\nrection is the addition of social networking and collabora-\ntion features such as sharing playlists for particular paths or\ncomparison of collections between different users. Several\nof the user study participants suggested using the same in-\nterface for personalized tag annotation. Finally we plan to\nconduct a wider ethnographic study where self-organizing\ntag clouds and location-based recommendation are used in\npersonal music collections.\n6. REFERENCES\n[1] G. Widmer A. Flexer D. Schnitzer, M. Gasser. Playlist\ngeneration using start and end songs. In Int. Conf.on Music Information Retrieval (ISMIR), Philadelphia,\nUSA, 2008.\n[2] S. Bateman, C. Gutwin, and M. Nacenta. Seeing things\nin the clouds: the effect of visual features on tag cloud\nselections. In Proc. ACM conf. on Hypertext and Hy-\npermedia, pages 193–202, 2008.\n[3] J. Brooke. Sus: a ”quick and dirty” usability scale. In\nUsability Evaluation in Industry. 1996.\n[4] M. Cooper, J. Foote, E. Pampalk, and G Tzanetakis. Vi-\nsualization in audio-based music information retrieval.\nComputer Music Journal, 30(2):42–62, 2006.\n[5] G. Widmer E. Pampalk, T. Pohle. Dynamic playlist\ngeneration based on skipping behavior. In The Interna-\ntional Society for Music Information Retrieval (ISMIR\n2005), London, UK, 2005.\n[6] J. Ellson, E.R. Gansner, E. Koutsoﬁos, S.C. North, and\nG. Woodhull. Graphviz - open source graph drawing\ntools. Graph Drawing, pages 483–484, 2001.\n[7] K. Eustice and et al. The smart party: A personalized\nlocation-aware multimedia experience. In Consumer\nCommunications and Networking Conference (CCNC\n2008), Las Vegas, USA, 2008.\n[8] M. Goto and T. Goto. Musicream: New music play-\nback interface for streaming, sticking, sorting, and re-\ncalling musical pieces. In Proc. Int. Conf. on Music In-\nformation Retrieval (ISMIR), 2005.\n[9] E. Law and L. von Ahn. Input-agreement: a new mech-\nanism for collecting data using human computation\ngames. In Proc. CHI 2009, pages 1197–1206, 2009.\n[10] C.S. Mesnage and M.J. Carman. Tag navigation. In\nProc. Int. Workshop on Social software engineering\nand applications, pages 29–32, 2009.\n[11] S. Mizzaro, E. Nazzi, and L. Vassena. Collaborative\nannotation for context-aware retrieval. In Proc. of the\nWSDM ’09 Workshop on Exploiting Semantic Annota-\ntions in Information Retrieval, pages 42–45, 2009.\n[12] E. Pampalk, S. Dixon, and G. Widmer. Exploring mu-\nsic collections by browsing different views. In Proc.\nInt. Conf. on Music Information Retrieval (ISMIR),\n2003.\n[13] E. Pampalk and M. Goto. Musicrainbow: A new user\ninterface to discover artists using audio-based similar-\nity and web-based labeling. In Proc. Int. Conf. on Mu-\nsic Information Retrieval (ISMIR), 2006.\n[14] Kohonen. T. Self-Organizing Maps. 1995.\n[15] G. Tzanetakis and P. Cook. Musical Genre Classiﬁca-\ntion of Audio Signals. IEEE Trans. on Speech and Au-\ndio Processing, 10(5), July 2002.\n[16] F.B. Vi ´egas and M. Wattenberg. Timelines : Tag clouds\nand the case for vernacular visualization. interactions,\n15(4):49–52, 2008.\n242\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "Improving Auto-tagging by Modeling Semantic Co-occurrences.",
        "author": [
            "Riccardo Miotto",
            "Luke Barrington",
            "Gert R. G. Lanckriet"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1415590",
        "url": "https://doi.org/10.5281/zenodo.1415590",
        "ee": "https://zenodo.org/records/1415590/files/MiottoBL10.pdf",
        "abstract": "Automatic taggers describe music in terms of a multino- mial distribution over relevant semantic concepts. This pa- per presents a framework for improving automatic tagging of music content by modeling contextual relationships be- tween these semantic concepts. The framework extends ex- isting auto-tagging methods by adding a Dirichlet mixture to model the contextual co-occurrences between semantic multinomials. Experimental results show that adding con- text improves automatic annotation and retrieval of music and demonstrate that the Dirichlet mixture is an appropri- ate model for capturing co-occurrences between semantics.",
        "zenodo_id": 1415590,
        "dblp_key": "conf/ismir/MiottoBL10",
        "keywords": [
            "Automatic taggers",
            "multinomial distribution",
            "semantic concepts",
            "framework",
            "improving automatic tagging",
            "modeling contextual relationships",
            "Dirichlet mixture",
            "contextual co-occurrences",
            "experimental results",
            "Dirichlet mixture"
        ],
        "content": "IMPROVING AUTO-TAGGING BY MODELING SEMANTIC\nC\nO-OCCURRENCES\nRiccardo Miotto\nUniversity of Padova\nmiottori@dei.unipd.itLukeBarrington\nUC SanDiego\nlukeinusa@gmail.comGert Lanckriet\nUC San Diego\ngert@ece.ucsd.edu\nABSTRACT\nAutomatic taggers describe music in terms of a multino-\nmial distribution over relevant semantic concepts. This pa-\nperpresentsaframeworkforimprovingautomatictagging\nof music content by modeling contextual relationships be-\ntweenthesesemanticconcepts. Theframeworkextendsex-\nisting auto-tagging methods by adding a Dirichlet mixture\nto model the contextual co-occurrences between semantic\nmultinomials. Experimental results show that adding con-\ntext improves automatic annotation and retrieval of music\nand demonstrate that the Dirichlet mixture is an appropri-\natemodelforcapturingco-occurrencesbetweensemantics.\n1. INTRODUCTION\nA central goal of music information retrieval (MIR) is to\ncreate systems that can efﬁciently and effectively retrieve\nsongs from massive music collections. A potential solu-\ntion to this challenge is to describe songs with a collec-\ntionofmanuallyannotatedmeaningfulwords(tags)andto\nperform retrieval based on these text descriptions. Com-\nmercial recommendation systems such Last.fm1and Pan-\ndora2extensivelyusethissemanticsimilarityapproachto\ncreate recommendation lists. Tags are useful because they\ncontextualize a song by describing human emotions, per-\nsonal style, geographic origins, spiritual foundations, his-\ntorical period, or particular uses of the song.\n1.1 Auto-Tagging\nThe continuous growth of music collections is making\nmanual human annotation of every song infeasible. In re-\nsponse, several scalable approaches have been proposed\nfor labeling music with semantics including social tag-\nging [7], web mining [6] or tag propagation from similar\nsongs [12], each with advantages and disadvantages [14].\nIn particular, MIR researchers have proposed content-\nbased “auto-taggers” – methods that analyze acoustic\nwaveforms and automatically assign meaningful words to\n1http://www.last.fm\n2http://www.pandora.com\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnotmadeordistributedforproﬁtorcommercialadvantageandthatcopies\nbear this noticeand thefull citation on theﬁrst page.\nc/circlecopyrt2010International Society for Music Information Retrieval.songs. Much of this work has been inspired by related\nmethods for automatic image annotation [13].\nOne of the the ﬁrst proposed approaches used Gaus-\nsianMixtureModels(GMM)computedovertheaudiofea-\ntures of the training examples to represent a vocabulary of\nwords[15]. Analternativemodel,theCodewordBernoulli\nAverage(CBA)[5]attemptedtopredicttheprobabilitythat\na tag applies to a song based on a vector quantized rep-\nresentation of the audio signal. Regardless of the model\nused,theoutputofanauto-taggerisavectoroftagprobab-\nilitieswhich may be interpreted as a semantic multinomial\n(SMN), a distribution that characterizes relevance of each\ntag to a song. Semantic multinomials capture patterns in a\nsong’s waveform that represent high-level properties such\nas genres, emotions or instrumentation.\n1.2 Tag co-occurrence\nAuto-tagging models aim to capture statistically regular\npatterns in the audio content and associate these patterns\nwith descriptive semantics. In general, these models treat\neach tag independently, ignoring the contextthat derives\nfrom associations between tags. Indeed, while some se-\nmanticassociationsinmusicareinspiredbydirectauditory\ncues (e.g., hearing a “violin”), others are inferred through\ncontextual relationships (e.g., inferring “cello” and “bas-\nsoon”, when listening to “orchestral classic music”). This\ngivesrisetostatisticallysigniﬁcant co-occurrence patterns\nofsemanticconceptsinthetrainingdata(e.g.,many“rock”\nsongs also tagged as “loud”), and thus in the SMNs. We\nsuggest that actively capturing correlations in SMNs can\nimprove the semantic description of a song.\nTwosituationscausetagstoco-occurinsemanticmulti-\nnomial distributions. The ﬁrst is when a tag acciden-\ntally co-occurs with another concept. Accidental co-\noccurrences could be due to many reasons, ranging from\npoor posterior probability estimates arising from auto-\ntagger errors, to the unavoidable ambiguous interpretation\nof music, such as confusing “trumpet” and “trombone”.\nThe second type of tag co-occurrence results from feature\nvectors that truly describe multiple musical concepts. For\nexample, a “cello” piece is very likely to have feature vec-\ntors that also ﬁt tags such as “classical music” or “vio-\nlin”. While only co-occurrences of the second type are\nindicative of truecontextual relationships, SMN distribu-\ntions derived from acoustic content exhibit both types of\nco-occurrences.\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)\n297Emotion\nGenre\nInstrument\nAcoustic\nUsage\nVocals\nFigure 1. Co-occurrence patterns for CAL500; redder\npoints imply high correlation between tags.\nTo understand the extent of tag co-occurrences, we ex-\namine the Computer Audition Lab 500 (CAL500) dataset,\nused later in our experiments (see Section 4 for more de-\ntails). Figure 1 depicts the pairwise correlation matrix be-\ntween CAL500 tags. Correlation values have been com-\nputed through an application of Jaccard’s Coefﬁcients [8],\nnij=P(wi∩wj)\nP(wi) + P(wj)−P(wi∩wj), (1)\nwhich provide a measure of the strength of the association\nbetween the general words wiandwj, normalized by the\ntotal number of times the two words appear. The nijcoef-\nﬁcientsrangebetween0and1,with nij>0ifthetagsare\nnot mutually exclusive (i.e., if they occur together in some\nsongs). In Figure 1, redder parts represent tag pairs that\nare highly correlated (i.e., where nijis large). As can be\nseen, correlation is present in many tags and it is particu-\nlarlyprevalentinthe“Emotion”and“Acoustic”categories\nwhereas tags categorized as “Genre” display few correla-\ntion patterns.\nThe co-occurrence patterns illustrated in Figure 1 are\nnot explicitly captured by auto-taggers that model acous-\ntics independently for each tag. Although SMNs capture\npatterns at the song level that are predictive of semantic\ntags, each dimension of the semantic space (i.e., each tag)\nis assumed to be independent from all others. Exploiting\ntheseregularco-occurrences-givingthesemanticscontext\n- could provide a better semantic description of music.\nThis suggests an extension of auto-tagging models by\naddingoneadditionallayerofsemanticrepresentationthat\nexplicitly captures tag co-occurrences. We began by mod-\neling the probability distribution of tags given audio fea-\ntures, placing each song in a semantic space. Now, by\nmodeling a probability distribution of the SMNs derived\nfrom each song - a distribution over distributions - we can\nobtain a richer semantic description. We refer to these rep-\nresentations as contextual models.1.3 Modeling Context\nInthispaper,wepresentanovelapproachtoautomatically\ntagging music with descriptive words by thinking of each\nsemantic concept as deﬁning a broader contextthat causes\nmultiple, related tags to co-occur in the description of a\nsong. For each tag, we learn a Dirichlet mixture (DM)\nto model the distribution of the SMNs derived from all\ntrainingsongsforthattag. ThisDM-based“contextualtag\nmodel”isinspiredbysimilarworkonmodelingtheseman-\ntics of images [11] where it was proposed as a framework\nfor combining object-centric and scene-centric methods to\nmodel contextual relationships between visual concepts.\nThe DM can robustly infer contextually meaningful co-\noccurrence patterns between tags in semantic multinomi-\nals, while removing accidental co-occurrences that might\nbe present in some of the individual song-level SMNs.\n2. RELATED WORK\nSome recent work in music information retrieval has ex-\nploited tag correlation and context. Yang et al. [16] formu-\nlate tag detection as an ordinal regression problem to ex-\nplicitly take advantage of the ordinal relationship between\nconcepts. Moreover, they proposed to leverage the co-\noccurrence patterns of tags for context fusion and employ\ntagselectiontoremoveirrelevantornoisytags. Unlikeour\napproach, the latter is a single-level model, incorporating\nthe tag correlation during the training of each individual\ndetector. Ness et al. [10] propose a hierarchy of two lin-\near SVMs where the ﬁrst classiﬁer highlighted the audio\npatterns and output a vector of tag afﬁnities (analogous to\na SMN), and the second layer modeled the contextual re-\nlationships between tags. Modeling context was also pro-\nposed in [3] where a second stage used a learning and cor-\nrelationreweighingschemetoboosttheresultoftagdetec-\ntion, and, earlier, in [1] where authors used a decision tree\nto reﬁne the result of individual detectors.\nOur approach using the DM to model context is appro-\npriatefortworeasons. First,theDMisa generative model\nthat is learned from only positive training examples i.e.,\nsongswhichhavebeenpositivelyassociatedwithaseman-\ntic tag. Unlike discriminative models (e.g., SVMs, boost-\ning, decision trees) which also require negative examples,\ngenerative models can accommodate weakly labeled train-\ning data where the absence of an association between a\nsong and a tag does not guarantee that no such association\nexists. Second, the Dirichlet is a distribution over parame-\ntersofthemultinomialdistribution,makingitaprobabilis-\nticallyappropriatemodelofsemanticmultinomialsderived\nfromauto-taggers.\n3. AUTO-TAGGING WITH\nDIRICHLET MIXTURES\nWe start by brieﬂy deﬁning the problem and by reviewing\nthe song-level auto-tagging systemdescribed in [15].\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)\n2983.1 Problem formulation\nT\nhe task of semantic annotation and retrieval can be seen\nas a supervised multiclass, multilabel classiﬁcation prob-\nlem,whereeachclassisaword wifromavocabulary V=\n{w1,...,w|V|}ofuniquetags,andeachsongislabeledwith\nmultiple words. A song is represented as a series of audio\ncontent features, X={x1,...,xT}, where xtrepresents\na vector of features, and Tis related to the length of the\naudio content; the goal is to ﬁnd the words wi∈ Vwhich\nbest describe a given song. Each song can then be rep-\nresented as an annotation vector π= (π1,...π|V|), where\nπi>0ifwihas a positive semantic association with the\nsong and πi= 0otherwise. The coefﬁcients πirepresent\nthe strength of semantic association between the song and\nwordwiand are termed semantic weights [15] orafﬁnity\nvalues[10].\n3.2 Deﬁning a semantic space\nVarious auto-tagging methods have been proposed for de-\nriving the semantic weights from acoustic features includ-\ning hierarchical Gaussian mixture models [15], support\nvector machines [2,10], codeword Bernoulli averaging [5]\nand boosting [3]. Any of these auto-taggers may be used\nto produce semantic multinomials — a set of semantic\nweights — that describe songs, a process that is illustrated\non the left of Figure 2. In this work, we use the hierarchi-\ncalGMMapproachandbrieﬂyreviewithereafterbutrefer\nthe reader to [15] for the details of this model.\nForeachword wiinthevocabulary,wetrainatag-level\nprobability distribution over the audio feature space, e.g.\nPX|W(x|wi)fori= 1, ..., |V|. The most relevant tags\nforasong Xarethewordswithhighestposteriorprobabil-\nity, computed using Bayes’ rule:\nπi=PW|X(wi|X) =PX|W(X|wi)PW(wi)\nPX(X),(2)\nwherePW(wi)is the prior of the ithword. We as-\nsume an uniform prior, e.g., PW(wi) = 1/|V| fori=\n1, ...,|V|. We compute the song prior as p(X) =/summationtext|V|\ni=1p(X|w i)p(wi). We follow [15] in estimating the\nlikelihood term in Equation 2, PX|W(X|wi), with the ge-\nometric average of the individual feature likelihoods of all\nthe songs positively associated with word wi:\nPX|W(X|wi) =T/productdisplay\nt=1/parenleftbig\nPX|W(xt|wi)/parenrightbig1\nT,(3)\nwhere the distribution PX|W(x|wi)is modeled as a mix-\nture of Gaussians. The PX|W(x|wi)distributions capture\nthe patterns of audio content that are predictive of each\nwordwi.\nGiven an unseen test song, represented by a set of au-\ndio feature vectors X, we compute the posterior probab-\nilities for the presence of concept wi∈ Vfrom Equa-\ntion 2. Collecting the posterior probabilities of each\nword results in an annotation vector describing the song,\nπ={π1,...,π|V|}, where πidenotes the posterior wordprobability PW|X(wi|X). With appropriate normalization\n(s.t./summationtext\niπi= 1), this vector can be conceived of as a se-\nmanticmultinomial (SMN)whichliesonaprobabilitysim-\nplex deﬁned as a semantic space. The semantic multino-\nmialisanalogoustoa documentvector ofwordcounts,of-\ntenusedinnaturallanguageprocessing[8],anditcaptures\nall the semantic information about the song.\n3.3 A model tolearn context\nTo capture the common patterns in the SMNs and model\nco-occurrencesbetweentags,welearn contextualtagmod-\nelsin the semantic space from the SMNs of the all songs\nin a training set that have been labeled with each tag. This\ncontextual modeling stageisillustratedontherightofFig-\nure 2. Just as we modeled acoustic feature vectors as sam-\nples from a mixture of Gaussians, we consider that seman-\ntic multinomials πare drawn from a mixture of Dirichlet\ndistributions over the semantic space [11]:\nPΠ|W(π|w;Ωw) =/summationdisplay\nkβw\nkDir(π|αw\nk), (4)\nThe contextual model for the word wis characterized by a\nvector of parameters Ωw={βw\nk,αw\nk}, whereβkis a prob-\nability mass function (/summationtext\nkβw\nk= 1), Dir (π;α)a Dirichlet\ndistribution of parameter α={α1,...,α|V|},\nDir(π|α) =Γ(/summationtext|V|\ni=1αi)\n/producttext|V|\ni=\n1Γ(αi)|V|/productdisplay\ni=1(πi)αi−1, (5)\nandΓ(.)the Gamma function.\nThe parameters Ωware learned from the SMNs πnof\nall the songs annotated with word w. Note that the con-\ntextual models PΠ|W(π|w)play, in the semantic space, a\nsimilar role to the models PX|W(X|w)in the acoustic fea-\nture space.\nThelearningprocessfortheDirichletmixturemodelre-\nlies on the maximum likelihood estimation, via the gener-\nalized expectation-maximization (GEM) algorithm. GEM\nis an extension of the standard EM algorithm, applicable\nwhen the M-step of the latter is intractable. The E-step\ncomputestheexpectedvaluesofthecomponentprobability\ndistribution βk, whereas the generalized M-step estimates\nthe parameters αk. Rather than solving for the parameters\nof maximum likelihood, each M-step simply produces an\nestimate of the likelihood which is higher than that avail-\nable in the previous iteration. This is known to be suf-\nﬁcient for EM convergence [4]. Parameter estimation is\nachieved through an application of the Newton-Raphson\nalgorithm [9].\nGiven an unseen test song described by the SMN\nπ={π1,...,π|V|}, the assignment of a word, wi, results\nfrom a Bayes decision rule based on the posterior word\nprobabilities in the context space:\nPW|Π(wi|π) =PΠ|W(π|wi)PW(wi)\nPΠ(π).(6)\nAgain we assume a uniform word prior probability\nPW(wi). Collecting all the posterior probabilities\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)\n299auto-tag\nm\nodels\n(GMM)...Rock\nsemantic\nm\nultinomial\n(SMN)...\ncontextual\nm\nultinomial\n(CMN)Dirichlet\nmixture\n(DM)Rock\nGu\nitar\nFunGuitar\nFun\nacoustic\nwaveform\nsemantic  model contextual  model\nFigure 2. Overview of the system: the Dirichlet Mixture models context by considering co-occurrences patterns between\nauto-tags lying in a semantic space.\nPW|Π(wi|π) =θiand normalizing (s.t./summationtext\niθi= 1), we\nbuild the vector θ= (θ1,...,θ|V|), denoted as the contex-\ntual multinomial (CMN) distribution of a song. Similar to\nthe semantic space deﬁned in Section 3.2, CMN vectors\nlies inacontextual space (see Figure 2).\n4. EXPERIMENTAL RESULTS\nIn this section, we demonstrate the impact of contextual\nmodels,andinparticulartheDM,onautomaticallytagging\nmusic with meaningful words.\n4.1 CAL500 Dataset\nThe Computer Audition Lab 500 (CAL500) [15] dataset\ncomprises 502 songs by 502 different artists. Each song\nhasbeenannotatedbyatleast3humansusingavocabulary\ncomposedof174tagsfrom6differentsemanticcategories,\nrepresenting both objective and subjective concepts.\nThesongsaredescribedbyMel-FrequencyCepstralCo-\nefﬁcient (MFCC) feature vectors; each MFCC vector sum-\nmarizes the spectral content of 23ms windows of a song.\nOur experiments use 39-dimensional MFCC-Delta feature\nvectors, composed by appending the ﬁrst and second in-\nstantaneous derivatives to the 13-component MFCCs.\nA ﬁrst analysis of the dataset demonstrates an imbal-\nance in the distribution of tags: while frequent tags can\nhave more than 300 positive examples, some others have\nless than 10 ones. This is not a big problem when train-\ning auto-taggers since each song is described by a large\nnumber of features vectors. However, the resulting set of\nSMNs describing songs is much smaller than the number\nof feature vectors and thus, we require more songs to ad-\nequately train the contextual models. For this reason, our\nevaluationconsidersonlythetagswithmorethan30exam-\nples, aiming to have at least 20−25examples in the train-\ning set with the remainder in the test set. This reduces the\nCAL500 vocabulary to 97 tags: 11 genres, 14 instruments,25 acoustic qualities, 6 vocal characteristics, 35 emotions\nand 6 usages.\nTo provide sufﬁcient data to train the DM, we extract\nmultiple SMNs from each song, each derived from clips\nlasting 3 seconds. We ﬁnd empirically that, unlike im-\nages which generally depict only a few semantic concepts\n(i.e., their SMNs have a few peaks that dominate all other\ntags), even a short music clip can be reasonably tagged\nwith many words and the resulting SMNs tend to be much\nmore uniform. For this reason, when learning DM models,\nwe threshold the SMNs, retaining at most the ten largest\nafﬁnity values and setting all other dimensions to zero.\n4.2 Annotation and Retrieval\nWe evaluate auto-tagging performance on both annotation\nand retrieval tasks. In the annotation task, we use Equa-\ntion 6 to label each test song with the ten most likely tags.\nPerformanceismeasuredusingmeanper-tagprecision,re-\ncall and F-score. Per-tag precision is the probability that a\ntag used by the model is correctly applied to a song. Per-\ntagrecallistheprobabilitythatthemodelannotatesallthe\ntags that should apply to a song. F-score is the harmonic\nmean of precision and recall, and is a single measure of\noverall annotation performance.\nIn theretrievaltask, we rank-order all songs according\nto their relevance to a query tag. The retrieval goal is to\nhave highly relevant songs at the top of the ranking list as\nthisisthemostcrucialrequirementinamusicretrievalsys-\ntem. We consider the mean average precision (MAP) and\ntheprecisionatk (k = 3, 5, 10). Forcompleteness,wealso\nreport the area under the receiver operating characteristic\ncurve (AROC) as a measure of the quality of the complete\nranking [8].\nEvaluationwasperformedusing5-foldcrossvalidation,\nwith 400 songs in the training set, and 100 in the test set.\nThe folds were built such that each song appeared in the\ntest set exactly once. The results reported in Table 1 dis-\nplaytheannotationandretrievalmetrics,averagedoverall\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)\n300Annotation Retrieval\nP\nrecision Recall F-Score P3 P5 P10 MAP AROC\nSemanticCBA 0.361 0.212 0.267 0.463 0.458 0.440 0.425 0.691\nGMM 0.405 0.202 0.269 0.456 0.455 0.441 0.433 0.698\nContextSVM 0.380 0.230 0.286 0.512 0.487 0.449 0.434 0.687\nDM0.441 0.232 0.303 0.519 0.501 0.470 0.443 0.697\nUpper Bound 0.716 0.471 0.568 1.000 0.993 0.942 1.000 1.000\nR\nandom 0.231 0.101 0.140 0.255 0.249 0.250 0.277 0.504\nTable 1. Performance of different auto-taggers: the Codeword Bernoulli Average (CBA) and Gaussian Mixture Models\n(GMM) consider semantics alone whereas the Support Vector Machine (SVM) and Dirichlet Mixture (DM) models learn\ncontextual relationships between the semantic multinomials produced by the GMM. All experiments were performed on\nthe same songs represented by the same set of features. “Random” is a baseline that annotates and ranks songs randomly.\n“Upper Bound” uses the optimal labeling for each evaluation metric and shows the upper limit on what any system could\nachieve.\ntags in the vocabulary.\n4.3 Contextual improvement\nThe proposed contextual modeling approach is compared\ntosomerecentstateoftheartauto-taggingapproaches: the\nGMMmodel[15]alone(i.e.,withoutcontext)andtheCBA\nmodel [5]. For the CBA model, each song is represented\nas a histogram over a codebook of 500 vector-quantized\nMFCCs. For each fold we trained the codebook models\nonly on the songs in the training set. All the code was\nprovided by the authors of [5].\nWe see in Table 1 that there is signiﬁcant beneﬁt from\nmodeling context on almost all annotation and retrieval\nmetrics. In particular, the precision-at-k metrics demon-\nstrate improvements at the top of the ranked retrieval list\nbut not throughout list (based on AROC). It can be argued\nthat precision-at-k metrics consider the part of the ranked\nlistwhichismostinterestingforusersofasemanticmusic\nretrieval engine.\n4.4 DM as a model of context\nThe center rows of Table 1 compare the DM approach for\nmodelingsemanticco-occurrencestoaSupportVectorMa-\nchine (SVM). As with the DM, we trained a contextual\nSVM for each tag using the semantic multinomials as the\ninputfeaturevector. UsingSVMasamodelofcontextwas\nﬁrstproposedin[10]althoughtheirapproachdiffersinthe\nfeatures used (median MFCC texture windows) and in the\nsemantic model (SVM), so our results do not present a di-\nrect comparison with [10]. Our goal is simply to compare\nthe DM and SVM as models of contextual relationships.\nThecontextSVMdoesnotbeneﬁtfrompre-processingthe\nSMNs(resultsnotshown),thusSVMsaretrainedonallthe\noriginalsemanticvalues. Table1showsthatDMgenerally\nimproves on all the metrics and never performs worse. In\nparticular, the DM signiﬁcantly improves on the SVM for\nthe annotation precision, F-score, P5, P10 and AROC met-\nrics (t-test, 10%signiﬁcance level); all the other metrics\ngenerally improve and never perform signiﬁcantly worse.\nTable 2 breaks up the evaluation over the different tagcategories. Ascanbeseen,allcategoriesbut“Genre”show\nclearbeneﬁtfromcontextualmodeling. Notethatimprove-\nmentsarerelatedtothetagco-occurrencesdepictedinFig-\nure 1. In fact, all the categories showing a high degree of\nco-occurrences (“Emotion”, “Instrument” and “Acoustic”)\nimproved with respect to the GMM. Though not exhibit-\ning as much co-occurrence, the “Usage” and “Vocals” cat-\negories, which perform poorly using semantics alone, ben-\neﬁt from the de-noising effect of learning contextual rela-\ntions. In these cases, the extra information from even only\nfew co-occurrences can lead to improvements in the qual-\nityofauto-tagging. Conversely,sincethe“Genre”category\ndoesnotexhibitmuchco-occurrence(i.e.,genresaremore\nexclusive), we do not gain beneﬁt from additional contex-\ntualmodeling. IthastobenotedthatSVMperformsbetter\nfor the “Genre” category, especially in the top of the rank-\ning list; we believe that in this case SVM beneﬁts from\nsome de-noising effects that DM is not able to capture.\n4.5 Predictive co-occurrences\nFinally, we include some examples of learned contextual\nmodels for 6 tags, representing each semantic category in\nCAL500. Table 3 shows the top three semantic multino-\nmialdimensionsthathavemostinﬂuenceonthecontextual\nmodels for each tag. These examples illustrate how the\nDM uses context to improve automatic tagging by learn-\ning to put most weight on semantic dimensions that are\npredictive ofthetagbeingmodeled e.g.,“calming, lowen-\nergy, mellow” music is good for “going to sleep”. This\ndemonstration of the dependence between tags indicates\ntheimportanceofincludingcontextwhenmodelingthere-\nlationship between semantics and music.\n5. CONCLUSIONS\nIn this paper we have presented the Dirichlet mixture\nmodel, a novel approach for improving automatic music\ntagging by effectively modeling contextual relationships\namong tags. Starting from the SMN of each song, the DM\nadds an additional layer to model tag co-occurrences, giv-\ning context to the semantic representations derived from\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)\n301Category # Tags Model P5 P10 MAP\nEmotion 35GMM 0.513 0.506 0.477\nSVM 0.539 0.514 0.481\nDM0.561 0.535 0.489\nGenre 11GMM 0.367 0.325 0.355\nSVM0.396 0.336 0.350\nDM 0.360 0.331 0.341\nInstrument 14GMM 0.460 0.431 0.441\nSVM 0.495 0.452 0.455\nDM0.506 0.458 0.463\nAcoustic 25GMM 0.508 0.501 0.472\nSVM 0.524 0.516 0.471\nDM0.564 0.546 0.496\nUsage 6GMM 0.253 0.233 0.258\nSVM 0.266 0.226 0.237\nDM0.308 0.273 0.281\nVocals 6GMM 0.253 0.240 0.261\nSVM 0.260 0.210 0.235\nDM0.287 0.267 0.278\nTable 2. Retrieval results considering the different word\ncategoriesforthesemanticGMM,andthecontextualSVM\nand DM models.\nContext Tag Semantic Inﬂuence\ncalming low energy tender slow tempo\nhard rock hard rock rock strong\nacoustic guitar slow tempo tender acoustic guitar\nacoustic texture low energy soft rock light beat\ngoing to sleep calming low energy mellow\nemotional tender sad softrock\nTable 3. Examples of the top three semantic inﬂuences on\ncontextual tag models.\nacoustic content. A tag’s afﬁnity with a song is computed\nastheposteriorprobabilityunderthetag’sDMmodel. The\nset of all posterior tag probabilities provides a contextual\ndescription of the song.\nExperiments reported that modeling context outper-\nforms approaches based on a semantic representation\nalone,especiallyconsideringthetopoftherankedretrieval\nlists. WedemonstratethattheDMisanappropriatechoice\nfor modeling semantic context by comparison to learning\ncontext with an SVM. More speciﬁcally, examining the\nperformance across semantic categories, we showed that\nthe DM improves performance for tags that exhibit a high\ndegree of correlation, as well as for noisy tags that are\npoorly represented by acoustic patterns.\n6. ACKNOWLEDGEMENTS\nTheauthorswouldliketothankN.Rasiwasiaforcodeand\nsuggestions, and N. Orio, and B. McFee for their support.7. REFERENCES\n[1] J.J. Aucouturier, F. Pachet, P. Roy, and A. Beuriv. Sig-\nnal + context = better classiﬁcation. In Proceedings of\nISMIR, 2007.\n[2] L. Barrington, M. Yazdani, D. Turnbull, and G. Lanck-\nriet. Combining feature kernels for semantic music re-\ntrieval. In Proceedings of ISMIR, 2008.\n[3] T. Bertin-Mahieux, D. Eck, F. Maillet, and P. Lamere.\nAutotagger: a model for predicting social tags from\nacoustic features on large music databases. Journal of\nNew Music Research, 37(2), June 2008.\n[4] A.P. Dempster, N.M. Laird, and D.B. Rubin. Maxi-\nmum likelihood from incomplete data via the EM al-\ngorithm. Journal of the Royal Statistical Society B,\n39(1):1–38, 1977.\n[5] M. Hoffman, D. Blei, and P. Cook. Easy as CBA: A\nsimple probabilistic model for tagging music. In Pro-\nceedings of ISMIR, 2009.\n[6] P.Knees,T.Pohle,M.Schedl,andG.Widmer.Amusic\nsearch engine built upon audio-based and web-based\nsimilarity measures. In Proceedings of ACM SIGIR,\n2007.\n[7] P. Lamere. Social tagging and music information re-\ntrieval.Journal of New Music Research, 37(2):101–\n114, 2008.\n[8] C.D. Manning, P. Raghavan, and H. Schtze. Introduc-\ntion to Information Retrieval. Cambridge University\nPress, 2008.\n[9] T. Minka. Estimating a Dirichlet distribu-\ntion. 2009. http://research.microsoft.com/en-\nus/um/people/minka/papers/dirichlet/.\n[10] S.R.Ness,A.Theocharis,G.Tzanetakis,andL.G.Mar-\ntins. Improving automatic music tag annotation using\nstackedgeneralizationofprobabilisticSVMoutputs.In\nProceedings of ACM MULTIMEDIA, 2009.\n[11] N. Rasiwasia and N. Vasconcelos. Holistic context\nmodeling using semantic co-occurences. In Proceed-\nings of IEEE CVPR, 2009.\n[12] M. Sordo, C. Laurier, and O. Celma. Annotating mu-\nsic collections: How content-based similarity helps to\npropagate labels. In Proceedings of ISMIR, 2007.\n[13] C.F. Tsai and C. Hung. Automatically annotating im-\nages with keywords: a review of image annotation sys-\ntems.Recent Patents on Computer Science, 1, 2008.\n[14] D. Turnbull, L. Barrington, and G. Lanckriet. Five ap-\nproaches to collecting tags for music. In Proceedings\nof ISMIR, 2008.\n[15] D.Turnbull,L.Barrington,D.Torres,andG.Lanckriet.\nSemantic annotation and retrieval of music and sound\neffects.IEEE TASLP, 16(2):467–476, February 2008.\n[16] Y.H. Yang, Y.C. Lin, A. Lee, and H. Chen:. Improving\nmusical concept detection by ordinal regression and\ncontext fusion. In Proceedings of ISMIR, 2009.\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)\n302"
    },
    {
        "title": "A Probabilistic Approach to Merge Context and Content Information for Music Retrieval.",
        "author": [
            "Riccardo Miotto",
            "Nicola Orio"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1415256",
        "url": "https://doi.org/10.5281/zenodo.1415256",
        "ee": "https://zenodo.org/records/1415256/files/MiottoO10.pdf",
        "abstract": "An interesting problem in music information retrieval is how to combine the information from different sources in order to improve retrieval effectiveness. This paper intro- duces an approach to represent a collection of tagged songs through an hidden Markov model with the purpose to de- velop a system that merges in the same framework both acoustic similarity and semantic descriptions. The former provides content-based information on song similarity, the latter provides context-aware information about individ- ual songs. Experimental results show how the proposed model leads to better performances than approaches that rank songs using both a single information source and a their linear combination.",
        "zenodo_id": 1415256,
        "dblp_key": "conf/ismir/MiottoO10",
        "keywords": [
            "music information retrieval",
            "combining information from different sources",
            "improving retrieval effectiveness",
            "hidden Markov model",
            "content-based information",
            "semantic descriptions",
            "acoustic similarity",
            "context-aware information",
            "experimental results",
            "better performances"
        ],
        "content": "APROBABILISTIC APPROACH TO MERGE CONTEXT AND CONTENT\nINFORMATION FOR MUSIC RETRIEV AL\nRiccardo Miotto\nUniversity of Padova\nmiottori@dei.unipd.itNicola Orio\nUniversity of Padova\norio@dei.unipd.it\nABSTRACT\nAn interesting problem in music information retrieval is\nhow to combine the information from different sources in\norder to improve retrieval effectiveness. This paper intro-\nduces an approach to represent a collection of tagged songs\nthrough an hidden Markov model with the purpose to de-\nvelop a system that merges in the same framework both\nacoustic similarity and semantic descriptions. The former\nprovides content-based information on song similarity, the\nlatter provides context-aware information about individ-\nual songs. Experimental results show how the proposed\nmodel leads to better performances than approaches that\nrank songs using both a single information source and a\ntheir linear combination.\n1. INTRODUCTION\nThe widespread diffusion of digital music occurred dur-\ning the last years has brought music information retrieval\n(MIR) to the general attention. A central goal of MIR is to\ncreate systems that can efﬁciently and effectively retrieve\nsongs from a collection of music content according to some\nsense of similarity with a given query. In information re-\ntrieval systems, the concept of similarity plays a key role\nand can dramatically impact performances. Yet, in music\napplications, the problem of selecting an optimal similar-\nity measure is even more difﬁcult because of the intrinsic\nsubjectivity of the task: users may not consistently agree\nupon whether, or at which degree, a pair of songs or artists\nare similar.\nIn the last years, in order to deal with the subjective\nnature of music similarity, it became very common to de-\nscribe songs as a collection of meaningful terms, or tags,\nas done in Last.fm1and Pandora2. In particular, tags are\noften, directly or indirectly, provided by end users and can\nrepresent a variety of different concepts including genre,\ninstrumentation, emotions, geographic origins, and so on.\n1http://www .last.fm\n2http://www\n.pandora.com\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc°2010 International Society for Music Information Retrieval.Many approaches have been developed to collect tags, rang-\ning from mining the Web and exploiting social behavior of\nusers, to automatic annotation of music through machine\nlearning algorithms. Tags are useful because they contex-\ntualize a song – for instance describing an historical pe-\nriod, a geographical area, or a particular use of the song\n– through an easy high-level representation. This infor-\nmation can then be used to retrieve music documents, to\nprovide recommendations or to generate playlists.\nExcluding the case of Pandora, where songs are anno-\ntated by human experts to guarantee high quality and con-\nsistency, in automatic systems or when the social behavior\nof users is kept into account, the semantic descriptions may\nbe very noisy. In automatic approaches, for example, the\nquality of the prediction strictly depends on the quality of\nthe training set, on the quality of the model, and on other\nissues such as parameter overﬁtting or term normalization.\nOn the other hand, standard content-based music similar-\nity, computed directly on music features, can be exploited\nto improve the quality of the retrieval, without requiring\nadditional training operations.\nThe goal of this paper is to provide a general model to\ndescribe a music collection and easily retrieve songs com-\nbining both content-based similarity and context-aware tag\ndescriptions. The model is based on an application of hid-\nden Markov models (HMMs) and of the Viterbi algorithm\nto retrieve music documents. The main applicative sce-\nnario is cross-domain music retrieval, where music and text\ninformation sources are merged.\n1.1 Related Work\nThere has been a considerable amount of research devoted\nto the topic of music retrieval, recommender systems and\nmusic similarity. Some of the most well-known commer-\ncial and academic systems have been described in [2]. The\nmodel proposed in this paper ﬁts the scenario of item-based\nretrieval systems, combining pure acoustic similarity and\nsemantic descriptions.\nMethodologies that merge different heterogeneous sour-\nces of information have been recently proposed in [1] for\nthe task of semantic discovery, in [9] for artist recommen-\ndation and in [16] for music classiﬁcation. All of these ap-\nproaches learn a metric space to join and compare the dif-\nferent sources of information in order to provide the user\nwith a single ranking list. Our approach is consistently\ndifferent, because it is built on a graph-based representa-\ntion of the collection that model both sources of informa-\n15\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Figur e\n1. Overview of the proposed model. Songs siare states of a HMM: observation probabilities provide semantic\ndescriptions, transitions probabilities are ruled by acoustic similarity between songs.\ntion and thus it does not rely on an additional processing\nto combine them. Content-based music similarity can be\ncomputed directly on music features as done in [4, 7] or\nthrough a semantic space which describes music content\nwith meaningful words [12, 18]. In our work, we exploit\nthe properties of an HMM to combine these two descrip-\ntions to improve retrieval performances.\nAs it is well known, HMMs have been extensively used\nin many applications, which in particular involve processes\nthrough time such as speech recognition [13]. In the music\ninformation retrieval research area, they have been used\nin different scenarios: query-by-example [15], automatic\nidentiﬁcation [10], alignment [11], segmentation [14], and\nchord recognition [5]. At the best of our knowledge, this is\nthe ﬁrst application of HMMs in the task of cross-domain\nretrieval where music and text information is modeled in a\nsingle framework.\n2. STATISTICAL MODELING OF A MUSIC\nCOLLECTION\nThe general goal of music search engines is to retrieve a\nlist of songs according to a particular principle. The prin-\nciple could be described either directly by a general seman-\ntic indication, such as the tag “classic rock”, or indirectly\nby a song, such as the set of tags assigned to “Yesterday”.\nIn both cases, the principle represents a user information\nneed, and it can be assumed that the goal of an user is to\nobserve consistently the application of this principle dur-\ning the time of his access to the music collection. In the\nparticular case of playlist generation, a system should be\nable to retrieve a list of music documents that are acous-\ntically similar to the music the user likes and, at the same\ntime, are relevant to one or more semantic labels that give\na context to his information need.\nThe methodology presented in this paper aims at pro-\nviding a formal and general model to retrieve music docu-ments combining acoustic similarity and semantic descrip-\ntions given by social tags. That is, the goal is to propose a\nmodel that encompasses both content-based similarity and\ncontext-aware descriptors. To this end, HMMs are particu-\nlarly suitable because they allow us to model two different\nsources of information. In fact, HMMs represent a dou-\nbly embedded stochastic process where, at each time step,\nthe model performs a transition to a new state according to\ntransition probabilities and emits a new symbol according\nto observation probabilities.\nThus HMMs can represent either content and context\ninformation, under the following assumptions:\n²if each state represents a song in the collection, acous-\ntic content-based similarity can be modeled by tran-\nsition probabilities\n²if the symbols emitted by the HMM are semantic\nlabels, the context that describes each state can be\nmodeled by observation probabilities.\nA suitably built HMM (see Section 2.1) may be ex-\nploited to address the examples provided at the beginning\nof this section. On the one hand, the model can generate a\npath across songs while observing, for a deﬁned number of\ntime steps, the semantic label “classic rock”. On the other\nhand, the model can start the path from the state associated\nto “Yesterday” and proceed to new states while observing\nthe semantic labels associated to the seed song. In both\ncases, the songs in the path are likely to have a similar\ncontent because of transition probabilities and are likely to\nbe in the same context because of emission probabilities.\nSince states of a HMM are not directly observable, the\npaths across the song collection need to be computed by\na decoding step, which highlights the most probable state\nsequence according to a sequence of observations. A rep-\nresentation of the proposed model is depicted in Figure 1.\n16\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)2.1Deﬁnition\nof the HMM\nAn HMM ¸that represents a collections of tagged songs\ncan be formally deﬁned by:\n1. The number of songs Nin the collection, each song\nrepresented by a state of the HMM. The set of states\nis denoted as S=fs1; s2; :::; s Ng.\n2. The number Mof distinct tags that can be used to\ndescribe a song. The set of symbols is denoted as\nV=fv1; v2; :::; v Mg.\n3. The state transition probability distribution A=aij,\nwhich deﬁnes the probability to move from state ito\nstatejin a single step. Transition probabilities aij\ndepends to the similarity between songs siandsj.\n4. The observation probability distribution of each state\nj,B=bj(k), which deﬁnes the probability that tag\nvkis associated to song j. Observation probabil-\nity values represent the strength of the relationships\nsong-tag, which is indicated as afﬁnity value.\n5. The initial state distribution ¼=f¼ig, that deﬁnes\nthe probability to start a path across the model be-\nginning at state si. Differently from the standard\ndeﬁnition of HMMs, the initial state distribution is\ncomputed dynamically at retrieval time, since it is\nstrictly connected to the type of information need,\nas described in Section 2.3.\nAlthough acoustic similarity is always a positive value,\nimplying aij>08i; j, with the aim of improving scala-\nbility, each state is directly connected to only the Pmost\nsimilar songs in the collection, while the transition proba-\nbilities with all the other states are set to 0. Heuristically,\nwe set Pto be the 10% of the global number of songs.\nAt present, no deeper investigation has been carried out\nto highlight an optimal value of P. In order to obtain a\nstochastic model, both transition and emission probabili-\nties are normalized, that isP\njaij= 1andP\nkbj(k) = 1.\nBecause of these two steps, transition probabilities are usu-\nally not symmetric, then aij6=aji\nAfter setting all the parameters, the HMM can be used\nto generate random sequences, where observed symbols\nare tags. Dually, well known algorithms can be used to\ndecode the most probable state sequence according to a\ngiven observation sequence.\n2.2 Computing the Relevance of Songs\nThe task at retrieval time is to highlight a sub-set of songs\nin the collection that are relevant to a particular query, ei-\nther expressed by semantic labels or by a seed song. In\nthe context of HMMs, the general problem can be stated\nas follows [13]: “given the model ¸, and the observation\nsequence ¹O=fo(1); : : : ; o (T)gwithoj2V, the goal is\nto choose a state sequence ¹S=fs(1); : : : ; s (T)gwhich\nis optimal in some sense”. Clearly, the observations se-\nquence represents the semantic description speciﬁed by the\nuser need.In literature, this problem is solved using the max-sum\nalgorithm, which in HMMs applications is known as the\nViterbi algorithm. The algorithm efﬁciently searches in\nthe space of paths, in order to ﬁnd the most probable one,\nwith a computational cost that grows only linearly with the\nlength of the chain. The algorithm is composed by a for-\nward computation to ﬁnd the maximization for the most\nprobable path, and by a backward computation to decode\nthe sequence of states. Although the general structure of\nthe algorithm has been maintained, some key modiﬁca-\ntions in the recursion part of the forward computation have\nbeen introduced. Following the notation and the algorithm\ndescription provided in [13] the normal initialization and\nthe modiﬁed recursion steps follow, for 1·j·N:\nInitialization: fort=1\n±t(\nj) =¼j¢obsj(t) (1)\nÃt(j) = 0 (2)\nRecursion: for2·t·T\n±t(j)=\nmax\n1·i·N[±t¡1(i)¢aij]¢obsj(t) (3)\nÃt(j) = arg max\n1·i·N[±t¡1(i)¢aij] (4)\nakj=akj\ndwith k=Ãt(j) (5)\nAsit\ncan be seen, we introduce obsj(t), deﬁned in the\nnext section, which is a general function that indicates how\nthe semantic description is considered during the retrieval\nprocess. This function plays the role of observations in\ntypical decoding applications.\nEquation 5 introduces a variation of the role of transi-\ntion probabilities. In fact, because of the structure of the\nmodel, it could happen that the optimal path enters a loop\nbetween the same subset of songs or, in the worst case,\njumps back and forth between two states. Clearly, this is a\nproblem because the retrieved list would present the same\nset of songs multiple times. Moreover, the loop could be\ninﬁnite, meaning that the algorithm cannot exit from it and\nthe retrieval list would be composed by only few songs. We\naddressed this problem by introducing a decreasing factor\nd, which is applied to the transitions probabilities when\nthey are selected in the forward step. So, when a transition\nis chosen, the probability aijis decreased by factor d(we\nsetd= 10), as shown in Equation 5, in order to make un-\nlikely that the state sequence would pass again through the\ncorresponding edge. It has to be noted that the attenuation\nis carried out locally, meaning that it affects the structure\nof the model only during the current retrieval operation.\nAnother issue that has to be addressed is a limitation\nin the structure of standard HMMs. Because of the ﬁrst-\norder Markov chain assumption, HMMs are generally poor\nat capturing long-range correlations between the observed\n17\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)variables,\nthat is between variables that are separated by\nmany steps [3]. Earlier experiments showed that this limi-\ntation involved a decrease in precision when decoding long\npaths. In order to solve this problem, we considered the re-\ntrieval composed by many sub-retrieval operations, each\none retrieving a sub-list of songs. Instead of performing a\nsingle backward decoding, the algorithm works for a sub-\nset of iterations, from which an optimal sub-path is built.\nOnly the ﬁrst nsongs of this sub-path are considered in the\nﬁnal ranking list; at the end of each iteration the algorithm\nrestarts from the last state of the nsuggested. Given the lo-\ncality of the approach, in this way we aim to keep constant\nthe quality along the retrieved list, avoiding a decrease in\nprecision.\n2.3 Querying the Model\nAs often assumed in the interaction with music search en-\ngines, in our scenario a user can submit a query in two dis-\ntinct ways: by providing a tag or by selecting a seed song\nin the collection. According to the kind of query, some of\nthe model parameters are set differently.\nIn the tag-based scenario, the goal is to rank the songs\naccording to their relevance with the provided tag and, at\nthe same time, to their acoustic similarity. In this case, the\nobservation sequence is composed simply by the chosen\ntag. We decided to set the initial state probability equal\nfor all the states, in order to let the algorithm decide the\nbeginning of the retrieved list. This scenario is very related\nto the standard HMMs case, then the function obsj(t)of\nEquations 1 and 3 is deﬁned as\nobsj(t) = bj(ot) (6)\nfor a generic state j, where observations otmay be the\nsame tag for all the time steps or it may change over time in\ncase of playlist generation through more complex patterns.\nIn the seed-song scenario, when the query is submit-\nted as a song q, the system is required to provide the user\nwith a list of songs potentially similar to the query. In this\ncase, the initial state distribution is forced to be 1for the\nstate representing the seed song and 0for all the others.\nThe observation sequence to be decoded is modeled as the\nvector of observations characterizing the seed song. The\nfunction obsj(t)of Equations 1 and 3 is proportional to the\ninverse of the Kullback-Leibler (KL) divergence between\nthe semantic description of the seed song and the chosen\nstate [6]. The choice of the KL divergence aims at gener-\nalizing the terms used for the tags, because it is related to\nthe similarity of concepts associated to the tags rather than\nto the pure distance between lists of tags. It is important to\nnote that the KL divergence is required also because each\nsong is described by a set of tags. Clearly, we consider\nthe inverse because the goal is to maximize the probability\nwhen the divergence is small. Therefore,\nobsj(t)'1\nKL\n(bj(¢); b q(¢))(7)\nfor the generic state jand the initial seed state q; clearly,\nobservations of qdo not change over time tbeing linkedto observations of the seed song. Since it is an observation\nprobability, the actual value of obsj(t)undergoes a nor-\nmalization process. It is worth noting that the use of KL\ndivergence can be extended also to the tag-based scenario\nwhen the user provides a set of tags (instead of a single\none) although this extension has not been tested yet.\n3. EXPERIMENTAL EV ALUATION\nA big challenge when designing a music retrieval system\nis how to evaluate a novel methodology. Although several\nefforts have been made within the MIREX campaigns, be-\ncause of well-known copyright issues, data of past cam-\npaigns are not always available to test new approaches.\nIdeally, the list of retrieved songs should be evaluated by\nhumans, in order to consider effectively the subjective na-\nture of the concept of music similarity. Being human evalu-\nation a time consuming task, we use an automatic approach\nconsidering that reliable annotations on songs can be ex-\nploited to measure the quality of a ranking list.\nWe tested our model through the Computer Audition\nLab (CAL500) dataset [18]: 502songs played by 502uni-\nque artists, each one annotated by a minimum of 3individ-\nuals using a vocabulary of 174tags. A song is considered\nto be annotated with a tag if 80% of the human annotators\nagreed that the tag would be relevant. CAL500 is a reason-\nable ground truth because annotations are highly reliable,\ncomplete and redundant – i.e., multiple persons explicitly\nevaluated the relevance of every tag for each song. So far,\nit has been mainly used to evaluate automatic music anno-\ntation systems, but we believe that it could be a reasonable\nground truth also to evaluate qualitatively a retrieval task.\nAlthough the size of the dataset does not allow to perform\nexperiments in terms of scalability, we argue that, at this\npoint, it is more signiﬁcant to test the effectiveness of the\napproach, to show if the model can provide improvements\nin the retrieval process.\nIn the experiments reported in this section, we require\nthat each tag is associated with at least 30songs and re-\nmove some tags that seemed to be redundant or overly sub-\njective. The semantic space is then composed by 62tags\ndescribing information about: genre, instrument, acoustic\nqualities, vocal characteristic, emotion, and usage.\nRetrieval is evaluated with metrics considering both per-\nformances at the top and along the whole ranking list. Since\na music retrieval system should maximize the quality of the\nretrieved items in the ﬁrst positions, we evaluate the preci-\nsion at the ﬁrst 3,5and10positions (P3, P5, P10). Beside,\nwe include the mean average precision (MAP) measure, in\norder to have also an evaluation along the whole ranking\nlist. All these metrics are extensively used in the literature\nto assess the effectiveness of a retrieval system [8].\n3.1 Acoustic Content-based Similarity\nA number of methodologies have been proposed in litera-\nture to compute direct acoustic content-based similarity. In\nthis set of experiments, we rely on the algorithm proposed\nin [7], which uses a single Gaussian with full covariance to\n18\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)model a\nsong. Although, some alternative approaches have\nbeen recently proposed [4], we use this one because of\nits efﬁciency and simplicity in the implementation. Songs\nare represented through vectors of Mel-Frequency Cepstral\nCoefﬁcients together with their ﬁrst and second derivatives\n(MFCC + delta) extracted from about one minute of mu-\nsic content, and the similarity between songs is computed\nusing a symmetrized version of the KL divergence.\nSection 2.1 describes how transition probabilities are\ncomputed from these similarity values, in particular by se-\nlecting for each state sithe ﬁrst Pmost similar songs and\nperforming the normalizationP\njaij= 1 withsj2P.\nIt is important to note that we aim at proposing a general\napproach, which is independent on the way acoustic sim-\nilarity is actually computed and which can be applied to\nother audio descriptors and other similarity measures. For\nthis reason the computation of acoustic similarity is pre-\nsented within the experimental evaluation section.\n3.2 Semantic space\nThere are several approaches to collect tags for music, each\nwith its own advantages and disadvantages [17]. Among\nall, we chose two different representations.\nA ﬁrst semantic description has been computed from\nthe music content. We used the supervised multiclass la-\nbeling (SML) model described in [18] to automatically an-\nnotate songs with tags based on an audio content analysis.\nFor a given song, the output of this algorithm is a vector\nof posterior probabilities named semantic multinomial that\nrepresents the strength of the relationship tag-song .\nA second representation has been created by gather-\ning the social tags from Last.FM, as reported on February\n2010. For each song of the dataset, we collected two lists\nof social tags using their public data sharing AudioScrob-\nbler3website. We gathered both the list of tags related to a\nsong, and the list of tags related to an artist. The relevance\nscore between a song and a tag is given by the sum of the\nscores in both lists, plus the tag score for any synonym\nor other wild matches of the tag in both lists [1]. Social\ntag scores are then mapped to the equivalent class in our\nsemantic description. If no gathered tag for a given song\nbelonged to the semantic space, the semantic description\nis represented by a uniform distribution, where all the tags\nshare the same score. This lead to a very sparse and noisy\ndescription, which is useful to test the effectiveness of our\napproach.\nWe addressed these descriptions with two different eval-\nuations, although they could be combined together in a sin-\ngle richer semantic description [1].\n3.3 Tag-based Retrieval\nIn this ﬁrst experiment, the model is queried using a tag; a\nsemantic concept is provided to the system, and the goal is\nto rank all the songs according to their relationships with\nthat term. Metrics are then averaged through all the terms\nin the vocabulary. Retrieval performances are measured\n3http://ws.audioscrobbler .com/2.0/Semantic Model\nP3 P5 P10 MAP\nSMLHMM 0.516 0.488\n0.452 0.361\nTag 0.419 0.431 0.405 0.332\nLast.fmHMM 0.347 0.331\n0.268 0.225\nTag 0.303 0.297 0.218 0.207\nTable\n1. Results of the tag-based retrieval experiments.\nby ﬁnding the positions, along the ranking list, of the doc-\numents annotated with the considered tag in the ground\ntruth. HMM-based retrieval is compared with the retrieval\nperformed by simply ranking the songs according to their\nafﬁnity value for that tag. Results are reported in Table 1,\nconsidering both types of semantic description.\nAs it can be seen, HMM-based retrieval clearly outper-\nforms the retrieval based on a single tag, with a major im-\nprovement in the quality at the top of the ranking list. On\nthe other hand, retrieval along the full list tends to decrease\nits effectiveness, as it can be inferred by the lower improve-\nment achieved by MAP. This is probably due to the prob-\nlem, discussed in Section 2.2, of HMMs generally poor\nat capturing long-range correlations between the observed\nvariables. Still we believe that the most important aspect\nto consider in a retrieval system is the quality on the top\nof the ranking list. Results based on Last.fm tags tend to\nhave lower performances in terms of absolute values. This\nlikely depends on the fact that the semantic descriptions\nare rather sparse and noisy and that sometimes songs were\nrepresented through a uniform distribution.\n3.4 Seed Song Retrieval\nIn this experiment, retrieval is carried out by submitting to\nthe system 50randomly selected seed songs and consider-\ning the sequence of states highlighted by the optimal path\nas a ranking list of retrieved documents. A ground truth,\nagainst which retrieval results are compared, has been cre-\nated for each query song by selecting the 30most similar\nsongs according to their human-based annotations. Seman-\ntic similarity has been computed using an application of the\nKL divergence to the set of tags for each pair of songs.\nWe compare different approaches: the HMM-based re-\ntrieval, a direct content-based retrieval where songs have\nbeen ranked according to their acoustic similarity with the\nseed (“Content”), a semantic similarity measured as KL\ndivergence between the semantic descriptions of the seed\nsong and each document in the collection (“Tags”), and a\nlinear combination between the two distances (“LinComb”).\nAs it can be seen from the results reported in Table 2,\neven in this case the proposed model leads to outperform-\ning results; the same consideration reported in Section 3.3\ncan be extend to the current evaluation. The only different\naspect is that, in this case, the Last.fm tags better quantize\nthe similarity relationships among songs; thus, the abso-\nlute values of the metrics is not very different between the\n19\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Semantic Model\nP3 P5 P10 MAP\nSMLTag\n0.266 0.270 0.246 0.211\nContent 0.237 0.234 0.236 0.187\nLinComb 0.280 0.278 0.244 0.204\nHMM 0.295 0.288 0.258 0.225\nLast.fmTag\n0.273 0.272 0.262 0.191\nContent 0.237 0.234 0.236 0.187\nLinComb 0.305 0.292 0.262 0.198\nHMM 0.304 0.299 0.284 0.219\nTable\n2. Results of the song-based retrieval experiments.\ntwo semantic representations.\n4. CONCLUSIONS\nWe introduce a novel methodology that represents a mu-\nsic collection through an hidden Markov model with the\npurpose to build a music retrieval system that combines\ncontent-based acoustic similarity and context-aware seman-\ntic descriptions. In the model, each state represents a song,\ntransitions probabilities depend on acoustic similarity and\nobservation probabilities represent semantic descriptions.\nAn application of the Viterbi algorithm allows us to cre-\nate paths across the model, which provides a ranking list\nof the songs. This approach represents an application of\ncross-domain retrieval combining audio content and text\nfor item-based retrieval. It is important to note that the ap-\nproach can be generalized also to other multimedia tasks\nwhere content can be combined with context, such as video\nor image retrieval.\nSome issues are still open and will be addressed in fu-\nture work. First of all, evaluation tested only the effective-\nness of the model; scalability needs to be evaluated with\na larger collection, in terms of number of songs and tags.\nMoreover, future research will be also devoted to the anal-\nysis of the effects introduced by different content descrip-\ntors and similarity measures. Finally, the extension to other\nmusic retrieval tasks, such as music recommendation and\nplaylist generation, will be explored.\n5. REFERENCES\n[1] L. Barrington, G. Lanckriet, D. Turnbull, and M. Yaz-\ndani: “Combining audio content and social context\nfor semantic music discovery,” Proc. of ACM SIGIR,\npp. 387–394, 2009.\n[2] L. Barrington, R. Oda, and G. Lanckriet: “Smarter than\nGenius? Human evaluation of music recommender sys-\ntems,” Proc. of ISMIR, pp. 357–362, 2009.\n[3] C.M. Bishop: Pattern Recognition and Machine\nLearning, Springer, 2006.[4] M. Hoffman, D. Blei, and P. Cook: “Content-based\nmusical similarity computation using the hierarchical\nDirichlet process,” Proc. of ISMIR , pp. 349–354, 2008.\n[5] M. Khadkevich, and M. Omologo: “Use of Hid-\nden Markov Models and Factored Language Models\nfor Automatic Chord Recognition,” Proc. of ISMIR ,\npp. 561–566, 2009.\n[6] S. Kullback, and R.A. Leibler: “On information and\nsufﬁciency,” Annals of Mathematical Statistics, V ol. 2,\nNo. 1, pp. 79-86, 1951.\n[7] M. Mandel, and D. Ellis: “Song-level features and sup-\nport vector machines for music classiﬁcation,” Proc. of\nISMIR, pp. 594–599, 2005.\n[8] C.D. Manning, P. Raghavan, and H. Schtze: Introduc-\ntion to Information Retrieval, Cambridge University\nPress, 2008.\n[9] B. McFee, and G. Lanckriet: “Heterogenous embed-\nding for subjective artist similarity,” Proc. of ISMIR,\npp. 513–518, 2009.\n[10] R. Miotto, and N. Orio: “A methodology for the seg-\nmentation and identiﬁcation of music works,” Proc. of\nISMIR, pp. 273–278, 2007.\n[11] N. Montecchio, and N. Orio: “A discrete ﬁlter bank\napproach to Audio to Score matching for polyphonic\nmusic,” Proc. of ISMIR, pp. 495–500, 2009.\n[12] S.R. Ness, A. Theocharis, G. Tzanetakis, and\nL.G. Martins: “Improving automatic music tag annota-\ntion using stacked generalization of probabilistic SVM\noutputs,” Proc. of ACM MULTIMEDIA , pp. 705–708,\n2009.\n[13] L.R. Rabiner: “A Tutorial on Hidden Markov Mod-\nels and Selected Applications in Speech Recognition,”\nProc. of the IEEE, V ol. 77, No. 2, pp. 257–286, 1989.\n[14] C. Raphael: “Automatic segmentation of acoustic mu-\nsical signals using hidden Markov models,” IEEE\nTransactions on Pattern Analysis and Machine Intel-\nligence, V ol. 21, No. 4, pp. 360–370, 1999.\n[15] J. Shifrin, B. Pardo, C. Meek, and W. Birming-\nham: “HMM-Based Musical Query Retrieval,” Proc.\nof ACM/IEEE JCDL, pp. 295–300, 2002.\n[16] M. Slaney, K. Weinberger, and W. White. Learning:\n“Learning a metric for music similarity,” Proc. of IS-\nMIR, pp. 313–138, 2008.\n[17] D. Turnbull, L. Barrington, and G. Lanckriet: “Five ap-\nproaches to collecting tags for music,” Proc. of ISMIR,\npp. 225–230, 2008.\n[18] D. Turnbull, L. Barrington, D. Torres, and G. Lanck-\nriet: “Semantic annotation and retrieval of music and\nsound effects,” IEEE Transactions on Audio, Speech\nand Language Processing, V ol. 16, No. 2, pp. 467–476,\n2008.\n20\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "Evidence for Pianist-specific Rubato Style in Chopin Nocturnes.",
        "author": [
            "Miguel Molina-Solana",
            "Maarten Grachten",
            "Gerhard Widmer"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1417793",
        "url": "https://doi.org/10.5281/zenodo.1417793",
        "ee": "https://zenodo.org/records/1417793/files/Molina-SolanaGW10.pdf",
        "abstract": "The performance of music usually involves a great deal of interpretation by the musician. In classical music, the final ritardando is a good example of the expressive aspect of music performance. Even though expressive timing data is expected to have a strong component that is determined by the piece itself, in this paper we investigate to what de- gree individual performance style has an effect on the tim- ing of final ritardandi. The particular approach taken here uses Friberg and Sundberg’s kinematic rubato model in or- der to characterize performed ritardandi. Using a machine- learning classifier, we carry out a pianist identification task to assess the suitability of the data for characterizing the in- dividual playing style of pianists. The results indicate that in spite of an extremely reduced data representation, when cancelling the piece-specific aspects, pianists can often be identified with accuracy above baseline. This fact suggests the existence of a performer-specific style of playing ritar- dandi.",
        "zenodo_id": 1417793,
        "dblp_key": "conf/ismir/Molina-SolanaGW10",
        "keywords": [
            "interpretation",
            "musical performance",
            "final ritardando",
            "expressive aspect",
            "kinematic rubato model",
            "machine learning classifier",
            "pianist identification task",
            "performer-specific style",
            "piece-specific aspects",
            "accuracy above baseline"
        ],
        "content": "EVIDENCE FOR PIANIST-SPECIFIC RUBATO STYLE IN CHOPIN\nNOCTURNES\nMiguel Molina-Solana\nDpt. Computer Science and AI\nUniversity of Granada, Spain\nmiguelmolina at ugr.esMaarten Grachten\nIPEM - Dept. of Musicology\nGhent University, BelgiumGerhard Widmer\nDpt. of Computational Perception\nJohannes Kepler Univ., Austria\nABSTRACT\nThe performance of music usually involves a great deal of\ninterpretation by the musician. In classical music, the ﬁnal\nritardando is a good example of the expressive aspect of\nmusic performance. Even though expressive timing data\nis expected to have a strong component that is determined\nby the piece itself, in this paper we investigate to what de-\ngree individual performance style has an effect on the tim-\ning of ﬁnal ritardandi. The particular approach taken here\nuses Friberg and Sundberg’s kinematic rubato model in or-\nder to characterize performed ritardandi. Using a machine-\nlearning classiﬁer, we carry out a pianist identiﬁcation task\nto assess the suitability of the data for characterizing the in-\ndividual playing style of pianists. The results indicate that\nin spite of an extremely reduced data representation, when\ncancelling the piece-speciﬁc aspects, pianists can often be\nidentiﬁed with accuracy above baseline. This fact suggests\nthe existence of a performer-speciﬁc style of playing ritar-\ndandi.\n1. INTRODUCTION\nPerformance of music involves a great deal of interpre-\ntation by the musician. This is particularly true of piano\nmusic from the Romantic period, where performances are\ncharacterized by large ﬂuctuations of tempo and dynam-\nics. In music performance research it is generally acknowl-\nedged that, although widely used, the mechanical perfor-\nmance (with a constant tempo throughout the piece) is not\nan adequate norm when studying expressive timing, since\nit is not the way a performance should naturally sound.\nAs an alternative, models of expressive timing could be\nused, as argued in [18]. However, only few models exist\nthat deal with expressive timing in general [2, 16]. Due\nto the complexity and heterogeneity of expressive timing,\nmost models only describe speciﬁc phenomena, such as the\ntiming of grace notes [15] or the ﬁnal ritardando.\nPrecisely, the ﬁnal ritardando —the slowing down to-\nward the end of a musical performance to conclude the\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc⃝2010 International Society for Music Information Retrieval.piece gracefully— is one of the clearest manifestations of\nexpressive timing in music. Several models have been pro-\nposed [3,14] in the related literature to account for its spe-\nciﬁc shape. Those models generally come in the form of a\nmathematical function that describes how the tempo of the\nperformance changes with score position.\nIn a previous empirical study by Grachten et al. [4] on\nthe performance of ﬁnal ritardandi, a kinematic model [3]\nwas ﬁtted to a set of performances. Even though some sys-\ntematic differences were found between pianists, in gen-\neral the model parameters tend to reﬂect primarily aspects\nof the piece rather than the individual style of the pianist\n(i.e. expressive timing data is expected to have a strong\ncomponent that is determined by piece-speciﬁc aspects).\nThis fact is relevant in a recurrent discussion in the ﬁeld\nof musicology, about which factor (the piece or the per-\nformer) mostly inﬂuences a performance [9]. Some experts\nargue that the performance should be preceded of a thor-\nough study of the piece; while others indicate that the per-\nsonal feeling of music is the ﬁrst and main point to be con-\nsidered. Works supporting both views can be found in [12].\nA study by Lindstr ¨omet al. [7] involving a questionnaire,\nshowed that music students consider both the structure of\nthe piece and the feelings of the performer as relevant in a\nperformance.\nThe current paper extends that previous work by Grachten\net al., by investigating whether or not canceling piece-speciﬁc\naspects leads to a better performer characterization. Musi-\ncologically speaking, the validation of this hypothesis im-\nplies that performers’ signatures do exist in music inter-\npretation regardless of the particular piece. We present a\nstudy of how ﬁnal ritardandi in piano works can be used\nfor identifying the pianist performing the piece. Our pro-\nposal consists in applying a model to timing data, normal-\nizing the ﬁtted model parameters per piece and searching\nfor performer-speciﬁc patterns.\nPerformer characterization and identiﬁcation [8, 13] is\na challenging task since not only the performances of the\nsame piece by several performers are compared, but also\nthe performance of different pieces by the same performer.\nOpposed to performer identiﬁcation (where performers are\nsupposed to have distinctive ways of performing) is piece\nidentiﬁcation —which requires the structure of the piece\nto imply a particular expressive behavior, regardless of the\nperformer.\nA further implication of this work would be that, when\n225\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)an estimation\ncan be made of the prototypical performance\nbased on the musical score, this estimation could be a use-\nful reference for judging the characteristics of performances.\nThis knowledge can also allow the artiﬁcial interpretation\nof musical works by a computer in expressive and realistic\nways [17].\nThis paper is organized as follows: Section 2 describes\nthe dataset used for this study, including the original timing\ndata and the model we ﬁt them to. Section 3 deals with the\ndata processing procedure. Results of the pianist classiﬁ-\ncation task are presented and discussed in Section 4, while\nSection 5 states conclusions and future work.\n2. DATA\nThe data used in this paper come from measurements of\ntiming data of musical performances taken from commer-\ncial CD recordings of Chopin’s Nocturnes. This collection\nhas been chosen since these pieces exemplify classical pi-\nano music from the Romantic period, a genre that is char-\nacterized by the prominent role of expressive interpretation\nin terms of tempo and dynamics. Furthermore, Chopin’s\nNocturnes is a well-known repertoire, performed by many\npianists, and thus facilitating large scale studies.\nAs explained before, models of expressive timing are\ngenerally focused in a certain phenomenon. In our study,\nwe will focus on the ﬁnal ritardando of the pieces. Hence,\nwe select those Nocturnes whose ﬁnal passages have a rel-\natively high note density and are more or less homoge-\nneous in terms of rhythm. With these constraints we avoid\nthe need to estimate a tempo curve from only few interon-\nset intervals, and reduce the impact of rhythmic particular-\nities on the tempo curve.\nIn particular, we used ritardandi from the following pieces:\nOp. 9 nr. 3, Op. 15 nr. 1, Op. 15 nr. 2, Op. 27 nr. 1, Op. 27\nnr. 2 and Op. 48 nr. 1. In two cases (Op. 9 nr. 3 and Op. 48\nnr. 1), the ﬁnal passage consists of two clearly separated\nparts, being both of them performed individually with a\nritardando. These ritardandi were treated separately —\nnamely rit1andrit2. So that, we have 8 different ritardandi\nfor our study.\nThe data were obtained in a semi-automated manner,\nusing a software tool [10] for automatic transcription of the\naudio recordings. From these transcriptions, the segments\ncorresponding to the ﬁnal ritardandi were then extracted\nand corrected manually by means of Sonic Visualiser , a\nsoftware tool for audio annotation and analysis [1].\nThe dataset in this paper is a subset of that used in\nprevious work [4], as we are only considering those pi-\nanists from whom all eight recordings are available. Ta-\nble 1 shows the names of these pianists and the year of\ntheir recordings. Hence, the dataset for the current study\ncontains a total amount of 136 ritardandi from 17 different\npianists.\n2.1 Friberg & Sundberg’s kinematic model\nAs mentioned in Section 1, we wish to establish to what\ndegree the speciﬁc form of the ﬁnal ritardando in a musicalArrau (1978) Falv\nai (1997) Pires (1996)\nAshkenazy\n(1985) Harasiewicz\n(1961) Pollini (2005)\nBarenboim (1981) Hewitt (2003) Rubinstein (1965)\nBiret (1991) Leonskaja (1992) Tsong (1978)\nd’Ascoli (2005) Mertanen (2001) Woodw\nard (2006)\nEngerer (1993) Ohlsson (1979)\nTable\n1. Performer and year of the recordings analyzed in\nthe experiments\nq = -4\nw = .3q = 1 q = 5\nw = .5 w = .7\nFigure\n1. Examples of tempo curves generated by the\nmodel using different values of parameters wandq. In\neach plot, the x and y axis represent score position and\ntempo respectively, both in arbitrary units.\nperformance is dependent on the identity of the performing\npianist. We address this question by ﬁtting a model to the\ndata, and investigating the relation between the piece/pianist\nidentity and the parameter values of the ﬁtted model. To\nsuch a task, we employ the kinematic model by Friberg &\nSundberg [3].\nThis model is based on the hypothesized analogy of mu-\nsical tempo and physical motion, and is derived from a\nstudy of the motion of runners when slowing down. From\na variety of decelerations by various runners, the deceler-\nations judged by a jury to be most aesthetically pleasing\nturned out to be those where the deceleration force is held\nroughly constant. This observation was implying that ve-\nlocity was proportional to square root function of time, and\nto a cubic root function of position. Equating physical po-\nsition to score position, Friberg and Sundberg used this ve-\nlocity function as a model for tempo in musical ritardandi.\nThus, the model describes the tempo v(x)of a ritardando\nas a function of score position x:\nv(x) = (1 + (wq\u00001)x)1=q(1)\nThe parameter qis added to account for variation in cur-\nvature, as the function is not necessarily a cubic root of\nposition. The parameter wrepresents the ﬁnal tempo, and\nwas added since the tempo in music cannot reach zero. The\nmodel can be ﬁtted to ritardandi performed by particular\npianists by means of its parameters.\nParameters wandqgenerate different plots of tempo\ncurves (see Figure 1). Values of q > 1lead to convex\ntempo curves, whereas values of q < 1lead to concave\n226\n11th International Society for Music Information Retrieval Conference (ISMIR 2010) 0 5 10 15 20 25 30 35 40\n 0.3  0.4  0.5  0.6  0.7  0.8  0.9Parameter q\nParameter wArrau\nAshkenazy\nBarenboim\nBiret\ndAscoli\nEngerer\nFalvai\nHarasiewicz\nHewitt\nLeonskaja\nMertanen\nOhlsson\nPires\nPollini\nRubinstein\nTsong\nWoodwardFigure\n2. Original data representation in the w-qplane\ncurves. The parameter wdetermines the vertical end posi-\ntion of the curve.\nEven though this kind of models are incomplete as they\nignore several musical characteristics [6], the kinematic\nmodel described above was reported to predict the evolu-\ntion of tempo during the ﬁnal ritardando quite accurately,\nwhen matched to empirical data [3]. An additional advan-\ntage of this model is its simplicity, both conceptually (it\ncontains few parameters) and computationally (it is easy\nto implement).\nThe model is designed to work with normalized score\nposition and tempo. More speciﬁcally, the ritardando is\nassumed to span the score positions in the range [0,1], and\nthe initial tempo is deﬁned to be 1. Although in most cases\nthere is a ritardando instruction written in the score, the ri-\ntardando may start slightly before or after this instruction.\nWhen normalizing, we must assure that normalized posi-\ntion 0 coincide with the actual start of the ritardando. A\nmanual inspection of the data showed that the starting po-\nsition of the ritardandi strongly tended to coincide among\npianists. For each piece, the predominant starting position\nwas determined and the normalization of score positions\nwas done accordingly.\nThe model is ﬁtted to the data by non-linear least-squares\nﬁtting through the Levenberg-Marquardt algorithm1, us-\ning the implementation from gnuplot . The model ﬁtting is\napplied to each performance individually, so for each com-\n1The ﬁtting\nmust be done by numerical approximation since the model\nis non-linear in the parameters wandqbination of pianist and piece, three values are obtained: w,\nqand the root mean square of the error after ﬁtting (serving\nthis value as a goodness-of-ﬁt measure).\nAt this point, we can represent each particular ritar-\ndando in the corpus as a combination of those two attributes:\nwandq. In Figure 22, the values obtained from ﬁtting are\ndisplayed as a scatter plot on the two-dimensional attribute\nspace qversus w. The whole dataset —136 instances—\nis shown in this plot. Each point location correspond to a\ncertain curve with parameters wandq. We refer the reader\nto Figure 1 to visualize the shape of different combination\nof parameters.\nAs can be seen from Figure 2, there are no clusters that\ncan be easily identiﬁed from this representation. Hence,\nthe performer identiﬁcation task using these original data\nis expected to have a low success rate.\n3. METHOD\nIn Section 1, we already mentioned that the expressive tim-\ning data is expected (as stated in [4]) to have a strong com-\nponent that is determined by piece-speciﬁc aspects such as\nrhythmical structure and harmony. In order to focus on\npianist-speciﬁc aspects of timing, it would be helpful to\nremove this piece-speciﬁc component.\nLetXbe the set of all instances (i.e. ritardando perfor-\nmances) in our dataset. Each instance x2Xis a duple\n(w; q). Given a ritardando i,Xiis the subset of Xthat\n2this ﬁgure\nis best viewed in color\n227\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)contains those\ninstances x2Xcorresponding to that par-\nticular ritardando.\nIn order to remove the piece-speciﬁc components, we\npropose to apply a linear transformation to the 2-attribute\nrepresentation of ritardandi. This transformation consists\nin calculating the performance norm for a given piece and\nsubtracting it from the actual examples of that piece. To\ndo so, we ﬁrst group the instances according to the piece\nthey belong. We then calculate the centroid of each group\n(e.g. mean value between all these instances) and move it\nto the origin, moving consequently all the instances within\nthat group.\nWe are aware that modelling the performance norm of\na given ritardando as the mean of the performances of that\nritardando is not the only option and probably not the best\none. In fact, which performance is the best and which one\nis the most representative is still an open problem with no\nclear results. Moreover, several performance norms can be\nequally valid for the same score. In spite of these difﬁcul-\nties, we chose to use the mean to represent the performance\nnorm, for its simplicity and for the lack of an obvious al-\nternative.\nTwo approaches were then devised in order to calculate\nthat performance norm. In the ﬁrst one, the mean perfor-\nmance curve is calculate as a unweighted mean of the at-\ntributes wandq(see Equation 2); whereas in the second\none,\ftserves to weight the mean (see Equation 3).\nIn the ﬁrst approach, the performance norm for a given\nritardando ican be calculated as:\nnorm i=∑\nxi∈X ixi\njXij(2)\nIn the\nsecond approach, it is calculated as a weighted\nmean, where \ftistands for the \ftvalue of instance xi:\nnorm i=∑\nxi∈X ixi\fti\n∑\fti(3)\nIn either\ncase, all instances xiare then transformed into\nx′\niby subtracting the corresponding performance norm:\nx′\ni=xi\u0000norm i (4)\nX′would be then the dataset that contains all x′. Af-\nter this transformation, all x′contain mainly information\nabout the performer of the ritardando, as we have removed\nthe common component of the performances per piece.\n4. EXPERIMENTATION\nIn order to verify whether pianists have a personal way\nof playing ritardandi, independent of the piece they play,\nwe have designed a classiﬁcation experiment with different\nconditions, in which performers are identiﬁed by their ri-\ntardandi. The ritardandi are represented by the ﬁtted model\nparameters. In one condition, the data instances are the\nsetX, i.e. the ﬁtted model parameters are used as such,\nwithout modiﬁcation. In the second and third conditions,\nFigure\n3. % success rate in the performer identiﬁcation\ntask using the whole dataset, with different k-NN classi-\nﬁers. Baseline value (5.88%) from random classiﬁcation is\nalso shown\nthe piece-speciﬁc component in every performance is sub-\ntracted (data set X′). The second condition uses the un-\nweighted average as the performance norm, the third con-\ndition uses the weighted average.\nNote that accurate performer identiﬁcation in this setup\nis unlikely. Firstly the current setting, in which the number\nof classes (17) is much higher than the number of instances\nper class (8), is rather austere as a classiﬁcation problem.\nSecondly, the representation of the performer’s rubato by\na model with two parameters is very constrained, and is\nunlikely to capture all (if any) of the performer’s individual\nrubato style. Nevertheless, by comparing results between\nthe different conditions, we hope to determine the presence\nof individual performer style independent of piece.\nAs previously explained, the training instances (ritar-\ndandi of a particular piece performed by a particular pi-\nanist) consist of two attributes (w andq) that describe the\nshape of the ritardando in terms of timing. Those attributes\ncome from matching the original timing data with the kine-\nmatic model previously cited.\nThe pianist classiﬁcation task is executed as follows.\nWe employ k-NN (Nearest Neighbor) classiﬁcation, with\nk2 f1; : : : ; 7g. The target concept is the pianist in all the\ncases, and two attributes ( wandq) are used. For validation,\nwe employ leave-one-out cross-validation over a dataset of\n136 instances (see Section 2). The experiments are carried\nout by using the Weka framework [5].\nFigure 3 shows the results for the previously described\nsetups, employing a range of k-NN classiﬁers with differ-\nent values of k2 f1; : : : ; 7g. We also carry out the clas-\nsiﬁcation task using the original data (without the transfor-\nmation) that were shown in Figure 2, in order to compare\nthe effect of the transformation.\nThe ﬁrst conclusion we can extract from the results is\nthat the success rate is practically always better when trans-\nforming the data than when not. In other words, by remov-\ning the (predominant) piece-speciﬁc component, it gets eas-\nier to recognize performers. This is particularly interesting\nas it provides evidence for the existence of a performer-\nspeciﬁc style of playing ritardandi, which was our initial\n228\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)hypothesis.\nNotehowever, that the success rate is not so good to\nallow this representation for being a suitable estimation of\nthe performer of a piece, even in the best case. A model\nwith only two parameters cannot comprise the complexity\nof a performer expressive ﬁngerprint. Although improving\nperformer identiﬁcation is an interesting problem, that is\nnot the point of this work.\nAs can be seen, employing a weighted mean of wand\nqfor calculating the performance norm of a piece —being\n\ftthe weight— leads to better results when kis small (i.e.\nk < 3). However, this approach, which is methodologi-\ncally the most valid, does not make a remarkable differ-\nence with respect to the original data for larger values of\nk.\nAn interesting and unexpected result is that the transfor-\nmation with the unweighted mean (see equation 2), gives\nbetter results for medium-large kvalues. The lower results\nfor smaller kcould be explained by the fact that instances\nwith a low ﬁt (which are actually noisy data), interfere with\nthe nearest-neighbor classiﬁcation process. The better re-\nsults for higher ksuggest that in the wider neighborhood\nof the instance to be classiﬁed, the instances of the correct\ntarget dominate —and thus that the noise due to low ﬁt is\nonly limited.\nNote also that this approach is more stable with respect\nto the size of kthan the original or the weighted ones. It\nalso outperforms the random classiﬁcation baseline —that\nis 5.88% with 17 classes— for all the different values of k.\nFurther experiments show that those are the trends for\nthose two different transformation of the data. Employing\nthe weighted mean leads to the highest accuracy using a 1-\nNN classiﬁer, but it quickly degrades as kis increased. On\nthe other hand, an unweighted mean leads to more stable\nresults, with the maximum reached with an intermediate\nnumber of neighbors.\nAlthough (as expected with many classes, few instances\nand a simplistic model) the classiﬁcation results are not sat-\nisfactory from the perspective of performer identiﬁcation,\nthe improvement that transforming the data (by removing\npiece-speciﬁc aspects) gives in classiﬁcation results, sug-\ngests that there is a performer-speciﬁc aspect of rubato tim-\ning. Even more, it can be located speciﬁcally in the curva-\nture and depth of the rubato (w andqparameters).\n5. CONCLUSIONS AND FUTURE WORK\nRitardandi in musical performances are good examples of\nthe expressive interpretation of the score by the pianist.\nHowever, in addition to personal style, ritardando perfor-\nmances tend to be substantially determined by the musical\ncontext they appeared in. Because of this fact, we propose\nin this paper a procedure for canceling these piece-speciﬁc\naspects and focus on the personal style of pianists.\nTo do so, we make use of collected timing variations\nduring ritardando in the performances of Chopin Nocturnes\nby famous pianists. We obtain a two-attributes (w ,q) rep-\nresentation of each ritardando, by ﬁtting Friberg and Sund-\nberg’s kinematic model to the data.A performer identiﬁcation task was carried out using\nk-Nearest Neighbor classiﬁcation on, comparing the (w ,q)\nrepresentation to another condition in which average wand\nqvalues per piece are subtracted from each (w ,q) pair.\nThe results indicate that in even in this reduced repre-\nsentation of ritardandi, pianists can often be identiﬁed by\nthe tempo curve of the ritardandi above baseline accuracy.\nMore importantly, removing the piece-speciﬁc component\nin the wandqvalues leads to better performer identiﬁca-\ntion.\nThis suggests that even very global features of ritar-\ndandi, such as its depth (w ) and curvature ( q), carry some\nperformer-speciﬁc information. We expect that a more de-\ntailed representation of the timing variation of ritardandi\nperformances will reveal more of the individual style of\npianists.\nA more detailed analysis of the results is necessary to\nanswer further questions. For instance, do all pianists have\na quantiﬁable individual style or only some? Also, there is\na need for alternative models of rubato (such as the model\nproposed by Repp [11]), to represent and study ritardandi\nin more detail.\nFinally, we intend to relate our empirical ﬁndings with\nthe musicological issue of the factors affecting music per-\nformances. Experiments supporting whether or not the\nstructure of the piece and the feelings of the performer are\npresent in renditions could be of interest to musicologists.\n6. ACKNOWLEDGMENTS\nThis research is supported by the Austrian Research Fund\nFWF under grants P19349 and Z159 (‘Wittgenstein Award’).\nM. Molina-Solana is supported by the Spanish Ministry of\nEducation (FPU grant AP2007-02119).\n7. REFERENCES\n[1] Chris Cannam, Christian Landone, Mark Sandler, and\nJuan Pablo Bello. The sonic visualiser: A visualisation\nplatform for semantic descriptors from musical signals.\nInProc. Seventh International Conference on Music\nInformation Retrieval (ISMIR 2006) , Victoria, Canada,\nOctober 8-12 2006.\n[2] Anders Friberg. Generative rules for music perfor-\nmance: A formal description of a rule system. Com-\nputer Music Journal , 15(2):56–71, 1991.\n[3] Anders Friberg and Johan Sundberg. Does musical per-\nformance allude to locomotion? A model of ﬁnal ri-\ntardandi derived from measurements of stopping run-\nners. Journal of the Acoustical Society of America,\n105(3):1469–1484, 1999.\n[4] Maarten Grachten and Gerhard Widmer. The kinematic\nrubato model as a means of studying ﬁnal ritards across\npieces and pianists. In Proc. Sixth Sound and Music\nComputing Conference (SMC 2009) , pages 173–178,\nPorto, Portugal, July 23-25 2009.\n229\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)[5] Mark Hall,\nEibe Frank, Geoffrey Holmes, Bernhard\nPfahringer, Peter Reutemann, and Ian H. Witten. The\nWEKA Data Mining Software: An update. SIGKDD\nExplorations , 11(1):10–18, 2009.\n[6] Henkjan Honing. When a good ﬁt is not good enough:\na case study on the ﬁnal ritard. In Proc. Eighth Inter-\nnational Conference on Music Perception & Cognition\n(ICMPC8), pages 510–513, Evanston, IL, USA, Au-\ngust 3-7 2004.\n[7] Erik Lindstr ¨om, Patrik N. Juslin, Roberto Bresin, and\nAaron Williamon. ”expressivity comes from within\nyour soul”: A questionnaire study of music students’\nperspectives on expressivity. Research Studies in Mu-\nsic Education, 20:23–47, 2003.\n[8] Miguel Molina-Solana, Josep Lluis Arcos, and Emilia\nGomez. Using expressive trends for identifying violin\nperformers. In Proc. Ninth Int. Conf. on Music Infor-\nmation Retrieval (ISMIR2008), pages 495–500, 2008.\n[9] Miguel Molina-Solana and Maarten Grachten. Na-\nture versus culture in ritardando performances. In\nProc. Sixth Conference on Interdisciplinary Musicol-\nogy (CIM10), Shefﬁeld, United Kingdom, July 23-24\n2010.\n[10] Bernhard Niedermayer. Non-negative matrix division\nfor the automatic transcription of polyphonic music.\nInProc. Ninth International Conference on Music In-\nformation Retrieval (ISMIR 2008) , Philadelphia, USA,\nSeptember 14-18 2008.\n[11] Bruno H. Repp. Diversity and commonality in music\nperformance - An analysis of timing microstructure\nin Schumann’s “Tr ¨aumerei”. Journal of the Acoustical\nSociety of America, 92(5):2546–2568, 1992.\n[12] John Rink, editor. The Practice of Performance: Stud-\nies in Musical Interpretation. Cambridge University\nPress, 1996.\n[13] Efstathios Stamatatos and Gerhard Widmer. Automatic\nidentiﬁcation of music performers with learning en-\nsembles. Artiﬁcial Intelligence, 165(1):37–56, 2005.\n[14] Johan Sundberg and Violet Verrillo. On the anatomy of\nthe retard: A study of timing in music. Journal of the\nAcoustical Society of America, 68(3):772–779, 1980.\n[15] Renee Timmers, Richard Ashley, Peter Desain, Henk-\njan Honing, and W. Luke Windsor. Timing of orna-\nments in the theme of Beethoven’s Paisiello Varia-\ntions: Empirical data and a model. Music Perception ,\n20(1):3–33, 2002.\n[16] Neil P. Todd. A computational model of rubato. Con-\ntemporary Music Review , 3(1):69–88, 1989.\n[17] Gerhard Widmer, Sebastian Flossmann, and Maarten\nGrachten. YQX plays Chopin. AI Magazine , 30(3):35–\n48, 2009.[18] W. Luke Windsor and E.F. Clarke. Expressive tim-\ning and dynamics in real and artiﬁcial musical perfor-\nmances: Using and algorithm as an analytical tool. Mu-\nsic Perception , 15(2):127–152, 1997.\n230\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "Characterization and Similarity in A Cappella Flamenco Cantes.",
        "author": [
            "Joaquín Mora",
            "Francisco Gómez 0001",
            "Emilia Gómez",
            "Francisco Escobar-Borrego",
            "José Miguel Díaz-Báñez"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1415242",
        "url": "https://doi.org/10.5281/zenodo.1415242",
        "ee": "https://zenodo.org/records/1415242/files/MoraGGED10.pdf",
        "abstract": "This paper intends to research on the link between musi- cal similarity and style and sub-style (variant) classifica- tion in the context of flamenco a cappella singing styles. Given the limitation of standard computational models for melodic characterization and similarity computation in this particular context, we have proposed a specific set of melodic features adapted to flamenco singing styles. In order to evaluate them, we have gathered a collection of music recordings from the most representative singers and have manually extracted those proposed features. Based on those features, we have defined a similarity measure between two performances and have validated their usefulness in differentiating several styles and vari- ants. The main conclusion of this work is the need to in- corporate specific musical features to the design of simi- larity measures for flamenco music so that flamenco- adapted MIR systems can be developed.",
        "zenodo_id": 1415242,
        "dblp_key": "conf/ismir/MoraGGED10",
        "keywords": [
            "musi- cal similarity",
            "style",
            "sub-style classification",
            "flamenco a cappella singing",
            "melodic features",
            "flamenco music",
            "standard computational models",
            "melodic characterization",
            "similarity measure",
            "flamenco- adapted MIR systems"
        ],
        "content": "Characterization and Melodic Similarity of A Cappella Flamenco Cantes  \nJoaquín Mora  Francisco Gómez  Emilia Gómez  \nDepartment of Evolutive  \nand Educational Psychology  \nUniversity of Seville   \nmora@us.es  Applied Mathematics Department,  \nSchool of Computer Science  \nPolytechnic University of Madrid  \nfmartin@eui.upm.es  Music Technology Group,  \n Universitat Pompeu Fabra  \nemilia.gomez@upf.edu   \n \nFrancisco Escobar -Borrego  \nDepartment of Audiovisual Communication,  \nPublicity and Literature  \nUniversity of Seville  \n fescobar@ us.es  \n José Miguel Díaz -Báñez  \nDepartment of Applied Mathematics II,  \nUniversity of Seville   \ndbanez@us.es  \nABSTRACT \nThis paper intends to research on the link between mus i-\ncal similarity and style and sub-style (variant) classific a-\ntion in the context of flamenco a cappella singing styles. \nGiven the limitation of standard computational models \nfor melodic characterization and similarity computation in this particular context, we have proposed a specific set \nof melodic features adapted to flamenco singing styles. In \norder to evaluate them, we have gathered a collection of music recordings from the most representative singers \nand have manually extracted those proposed features. \nBased on those features, we have defined a similarity measure between two performances and have validated their usefulness in differentiating several styles and var i-\nants. The main conclusion of this work is the need to i n-\ncorporate specific musical features to the design of sim i-\nlarity measures for flamenco music so that flamenco-\nadapted MIR systems can be developed.  \n1. INTRODUCTION \nThere is a wealth of literature on music research that f o-\ncuses on the understanding of music similarity from di f-\nferent viewpoints as well as the use of similarity di s-\ntances to cluster different pieces according to artist, genre \nor mood. Over the past few years, the Music Information \nRetrieval (MIR) community has been exploring several ways to measure the similarity between two musical pieces. They are often based on comparing musical e x-\ncerpts in audio format and computing the distance b e-\ntween a representative set of their musical features (e.g., instrumentation, rhythmic patterns or harmonic progre s-\nsions). This way, current systems can, for instance, locate different versions of the same song with a high accuracy rate [9] .  \nAlternative approaches are based on comparing co n-\ntextual information from the considered pieces or artists \n(e.g., influences, temporal coincidences, geographical l o-\ncation), which is usually extracted from the web. A co m-\nbination of these two approaches seems to be the most \nadequate solution [2], but there is still a glass ceiling on current systems. This seems to be owing to the fact that there are still fundamental musical features to be consi d-\nered or fully incorporated into existing models [ 10].\n On the other hand, research on music similarity has \nmainly focused on the analysis of music from the so-\ncalled \"Western tradition\", although there is an increasing interest in analyzing music from different traditions, and \nsome recent MIR studies are devoted to this issue [3]. \nIn this paper we study the relationship between music \nsimilarity and style definition in the context of a group of flamenco a cappella singing styles. Because of its oral \ntransmission, formal classification and musical analysis of flamenco present considerable difficulties. Classific a-\ntion of styles is usually carried out by flamenco experts and rests upon the analysis of the oral corpus of flamenco music. Flamenco experts have used several criteria in their classification, some of which, unfortunately, are not \nfully explicit and clear-cut. Moreover, there exists co n-\nsiderable controversy among flamenco experts and at pr e-\nsent there is a lack of a unified, unequivocal classification of a cappella cantes . \nOur previous research has focused on analyzing the \nrelationship between music similarity and style definition from different perspectives, such as computational mo d-\nels for melodic similarity [3] and human ratings of \nrhythmic patterns [6]. In Cabrera et al. [3], we applied \nstandard melodic similarity measures to a music colle c-\ntion of a cappella flamenco music. Extraction of melodic \ncontours, although arduous, produced faithful descri p-\ntions of cantes . However, when applied some standard \nalgorithms for comparing melodic contours  (i.e. correl a-\ntion between pitch and interval histograms and edit di s-\ntance), we obtained rather poor results. Although global \nstyle classification seemed to work to some extent, fl a-\nmenco experts found that for variant classification of \nsome styles those algorithms failed to distinguish subtle, \nspecific nuances of the cantes. We applied some cluste r-\ning techniques and it turned out that c antes that by mus i-\ncal reasons should be scattered were unexpectedly clu s-\ntered together. Thus, we concluded that further research \nhad to be carried out. It soon became apparent that two tasks, at least, had to be accomplished: first, to musically formalize the specific and subtle features of a cappella \ncantes ; second, to refine our computational models a c-\ncording to those features.  \nThis paper intends to research on the link between \nmusic al similarity and style classification and sub-style \n351\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \n(variant) definition in the context of flamenco a cappella \nsinging styles. The main contributions are (1) Identify relevant and specific features that characterize a given performance in the context of its style. Here, we consider that each style is characterized by a prototypical melodic contour. The features will then account for variations within this contour. (2) Define a similarity measure based on the identified features and provide an automatic method of clustering and classification. (3) Evaluate the results with a music collection of recordings from the most representative performers and contrast them with \nexisting theories for the definition of styles and variants. \nThis paper is organized as follows. In the next two \nsections we analyze the characteristics of flamenco sin g-\ning and a cappella cantes . Next we give an overview of \nthe cantes to be analyzed and we describe the music co l-\nlection used in our study. We then present the set of fe a-\ntures that describes the two chosen cante s, namely, de-\nblas and martinetes . The following section deals with the \nsimilarity distance between cantes . A conclusion section \nsummarizes the main findings and presents some propo s-\nals for future research.  \n2. FLAMENCO SINGING \nWe now describe the main features that characterize fl a-\nmenco singing and differentiate it from other styles: \n- Instability of pitch . In general, notes are not clearly \nattacked. Pitch glides or portamenti  are very common.  \n- Sudden changes in volume (loudness).  \n- Short pitch range or tessitura.  It is normally limited to \na major sixth interval and characterized by the insistence \non a note and those contiguous.  \n- Intelligibility of voices. Since lyrics are important in \nflamenco, there is a strong preference for intelligibility over range or timbre. Contralto and baritone voices are very common.  \n- Timbre . Timbre characteristics of flamenco singers d e-\npend on the period in which it was performed. As rel e-\nvant timbre aspects, we can mention breathiness in the voice and the absence of high frequency (singer) formant, \nwhich is characteristic of classical singing styles.  \n From a musical point of view, a cappella cantes  retain the \nfollowing properties. \n- Conjunct degrees. Melodic movement mostly occurs \nby conjunct degrees.  \n- Scales. Certain scales such as the dominant Phrygian \nmode (with a major tonic) and Ionian mode (E-F-G#-A-\nB-C-D) are predominant.  \n- Ornamentation . There is also a high degree of complex \nornamentation, melismas being one of the most signif i-\ncant devices of expressivity .  \n- Microtonality. The use of intervals smaller than the \nequal-tempered semitones of Western classical music is \nfrequent.   \n- Enharmonic scales.  Microtonal interval differences b e-\ntween enharmonic notes. \n Classification of a cappella cantes  is subject to many di f-\nficulties. Two cantes  belonging to the same style may \nsound very different to an unaccustomed ear. In general, underlying each cante  there is a melodic skeleton. \nDonnier [4] called it the “ cante’s melodic gene”. The \nsinger fills this melodic skeleton by using different kind of melismas, ornamentation and other expressive r e-\nsources. A flamenco aficionado recognizes two versions as to be the same cante  because certain notes appear in \ncertain order. What happens between two of those note s \ndoes not matter regarding style classification, but does \nmatter for assessing a performance or the piece itself. \nAficionado’s ears recognize the wheat from the chaff when listening and appreciate a particular performance in terms of the quality of the melodic filling, among other features.   In order to put the reader in the position of u n-\nderstanding this point, in Figures 1 and 2 we show a tra n-\nscription of two versions of the same cante  to Western \nmusical notation. In this respect, Western notation has been found limited for this kind of music (e.g. Donnier \n[4] proposed the use of plainchant neumes ).  \n \nFigure 1: Manual transcription of a cante (debla).  \nFigure 2: Transcription of another version of the previous \ncante . \n3. THE STYLES OF TONÁS \nAs defined earlier, a cappella cantes  are songs without \ninstrumentation, or in some cases with some percussion  \n(in the flamenco argon also cantes a palo seco) . An i m-\nportant group of such cantes  is composed of tonás , style \nthat generically include martinetes, deblas, saetas, tonás \nand carceleras . Since in flamenco music the word toná \nrefers to both the style and one of its variants, we will r e-\nfer to “ tonás style” as the whole style and toná as the sub-\nstyle or cante.   \n352\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \nThese cantes are played in free rhythm (sometimes \nthe pattern of seguiriya is used as rhythmic accompan i-\nment). Each singer chooses his or her own reference \npitch. Modality, depending on the particular style, may be either the major mode or the Andalusian mode (Phrygian mode with a tonic major chord), though frequently both modes alternate within the same song [5]. Lyrics of these \nsongs range widely. Lefranc  [8] carried out a classific a-\ntion of the tonás style mainly based on lyrics. Blas Vega \n[1] also studied the tonás  style from a historical stan d-\npoint. A toná typically possesses verses formed by words \neither of three or four syllables, or eight syllables, where the second and the forth verses may have assonant rhyme; often the final verse finishes with an imperfect tercet (a tercet is a group of three lines of a verse; an imperfect tercet is an off-rhyme tercet).  \nThe toná cante  is normally sung in a four-verse form \nof eight syllables each. Rhyme is assonant on even verses. The toná cante  appears in several modes, the \nPhrygian mode being the most prevailing one. Ionian or \nAeolian modes can also be found as well as alternation of two modes. Ornamentation is very complex and profuse; \nno strict tempo is followed at all.   \n4. MUSIC COLLECTION \nTo start with, we gathered a set of 365 pieces belonging \nto the tonás  style. Somehow, these cantes  have scarcely \nbeen recorded compared to other styles. Their ritual mood and lyricism might be a plausible reason for that shortage of recordings. In spite of that, we spared no effort to gather as many recordings as possible from all feasible sources (private collections, libraries, historical recor d-\nings, several institutions, etc.). We may safely state that \nour collection is quite representative of this type of can-\ntes. \nAfter the analysis phase of the corpus, we decided to \nfocus on two styles, deblas  and martinetes , because: (1) \nBoth styles are central to flamenco music; (2) Contrary to \nother cantes , we had information about singers, ge o-\ngraphical locations, singing schools, dates, etc., which allows us to have a complete, in-depth characterization of \nthem from a more general standpoint than just the mus i-\ncal one; (3) In general, recordings had acceptable quality to carry out this research and the future ones, which i n-\nclude, for instance, automatic feature extraction from audio files; (4) There was a high number of recordings, 72 songs in total, where 16 were deblas  and 56 mart i-\nnetes ; (5) Apart from the number, there was enough var i-\nability in the sample for a proper evaluation of our met h-\nods. \nThe specific feature set of deblas  and martinetes\n1, to \nbe described below, were obtained after a thorough study. First, we opened an analysis phase to identify which m u-\nsical features were relevant to the characterization of the chosen cantes. Preliminary analysis produced too many \nvariables or just variables with little explanatory power. Next, in search of the least complex yet meaningful d e-\n                                                \n1 The music collection studied in this paper can be found at \nhttp://mtg.upf.edu/research/projects/cofla  Please, contact the authors.  scription of cantes , we removed several variables. Most \nof the features identified were related to melody and form. During the analysis, in our mind it also underlay the idea of capturing some of the features used by fl a-\nmenco experts and aficionados to recognize the different styles. After deciding on the final feature set, we man u-\nally extracted it for the different performances. A wor k-\ning group of flamenco experts agreed on the proposed feature set and annotations. The next two sections d e-\nscribe the musical features in detail. \n5. MUSICAL FEATURES \n5.1. Deblas  \nThe debla  is a song from the style of tonás . In general, it \nis marked by its great melismatic ornamentation, more abrupt than the other songs from this style, which chara c-\nterizes its melody. Deblas are characterized by a partic u-\nlar melodic contour.  \nThe musical features that characterize the different \nvariants within the debla style  can be summarized as fo l-\nlows: 1. Beginning by the word ¡Ay! : ¡Ay! is an interjection \nexpressing pain, which is quite idiosyncratic to fl a-\nmenco music.  Values of the variable: Yes and no. \n2. Linking of ¡Ay!  to the text. That initial ¡Ay! may be \nlinked to the text or just be separated from it. Values of the variable: Yes and no. \n3. Initial note.  It refers to the first note of the tercet. \nNormally, it is the sixth degree of the scale (VI), but the fifth degree (V) can also appear. Values of the variable are 5 and 6. \n4. Direction of melody movement in the first hem i-\nstich (a hemistich is half of a line of a verse.) The d i-\nrection can be descending (fast appoggiatura in V, \nthen the progression VI-IV), symmetric (III- VI-IV) or \nascending. Values here are D, S and A. \n5. Repetition of the first hemistich.  That repetition \nmay be of the whole hemistich or just a part of it. \nValues of the variable: Yes or no. \n6. Caesura. The caesura is a pause that breaks up a \nverse into two hemistiches. Values of the variable: Yes and no.  \n7. Direction of melody movement in the second hem i-\nstich, defined as in the first hemistich. Values of the \nvariables are D, S and A. \n8. Highest degree in the second hemistich.  It is the \nhighest degree of the scale reached in the second hemistich. Usually, the seventh degree is reached, but fifth and sixth degrees may also appear. Values of the variable are 5, 6 and 7.  \n9. Frequency of the highest degree in the second hemistich. The commonest melodic line to reach the \nhighest degree of the scale consists of the concaten a-\ntion of two torculus . A torculus  is a neume signifying \nthree notes, the second higher than the others. The value of this variable indicates how many times this neume is repeated in the second hemistich.  \n353\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \n10. Duration.  Although the duration is measured in ms, \nour intention was to classify the cantes  into three \ncategories, fast, regular and slow. To do so, we first \ncomputed the average µ and the standard deviation σ \nof the durations of all the cantes in the same style. \nThen, fast cantes are those whose duration is less than \nµ-σ, regular cantes  have their duration in the interval \n[µ-σ, µ+σ], and slow cantes  have durations greater \nthan µ+σ. Values of this variable are F, R and S. \n5.2. Martinetes \nThe martinete  is also considered a variant of the toná. It \ndiffers from deblas in the theme of its lyrics and in its \nmelodic model, which always finishes in the major mode. \nDebla ’s mood is usually sad, played without guitar a c-\ncompaniment, like the other tonás . Martinetes , however, \nare usually accompanied by the sound of a hammer struck \nagainst an anvil.  \nThere are three clear variants of martinetes. The first \none, to be called martinete  1, has no introduction, \nwhereas the second one, to be called martinete  2, starts \nwith a couple of verses from a toná. The third one, to be \ncalled martinete  3, is a concatenation of a toná and some \nof the previous variants of martinetes ; the toná of mart i-\nnete 2 and 3 is called toná incipit. A trait of martinete  3 is \nthe singing of several poems, all having the same theme as in medieval romances. Because martinete 3  is a co m-\nbination of toná and martinetes 1 and 2, we removed it \nfrom our current study, as we just sought characterizing the most fundamental styles.  \nNext, musical features of martinete 1 are presented. \n1. Repetition of the first hemistich.  As for deblas, \nrepetition may be complete or partial. Values of the variable: Yes and no.  \n2. Clivis/flexa  (a neume of two notes, the second lower \nthan the first one)  at the end of the first hemistich.  \nNormally, fall IV- III or IV-IIIb are found.  The co m-\nmonest ending for a tercet is the fourth degree, whose sound is sustained until reaching the caesura. Some singers like to end on III or IIIb. Values of the var i-\nable are: Yes and no.  \n3. Highest degree in both hemistiche s. The customary \npractice is to reach the fourth degree; some singers reach the fifth degree. Values of the variable are 4 and 5. \n4. Frequency of the highest degree in the second hemistich. The melodic line is formed by a torculus, \na three-note neume, III- IV-III in this case. This var i-\nable stores the number of repetitions of this neume.  \n5. Final note of the second hemistich. The second \nhemistich of martinete 1  is ended by falling on the \nsecond degree. Sometimes, the second degree is fla t-\ntened, which produces Phrygian echoes in the c a-\ndence. Th is variable takes two values, 1 when the f i-\nnal note is the tonic and 2 when the final note is II.  \n6. Duration.  This variable is defined as in the case of \ndeblas  (that is, in terms of µ and σ). Values of this \nvariable are F, R and S. As for martinete 2 we have the following features. \n1. Highest degree in both hemistiche s. In this case the \ncustomary practice is to reach the sixth degree; in some cases singers just reach the fourth or fifth d e-\ngrees. Values of the variable are 4, 5 and 6.  \n2. Frequency of the highest degree in the second hemistich. In this case the neume is also a torculus . \nThis variable stores the number of repetitions of this neume.  \n3. Symmetry of the highest degree in the second hemistich. The second hemistich of a martinete 2  is \nrich in melismas. This feature describes the distrib u-\ntion of the melismas around the highest reached d e-\ngree, which is usually the sixth degree. Melismas can occur before and after reaching the highest degree (symmetric distribution), only before the highest d e-\ngree (left asymmetry) or only after the highest degree (right asymmetry).   \n4. Duration.  This variable is defined as in the previous \ncases. Values of this variable are F, R and S. \n6. SIMILARITY COMPUTATION \n6.1 Inter-Style Similarity  \nCarrying out the preceding analysis allowed us to extract a set of musical features to be used in the definition of melodic similarity between cantes . As a matter of fact, \njust using features very peculiar to a given style would distort the analysis, as their discriminating power would be very high. Our intention was to select a set of a few features capable of discriminating between different cantes. We removed those variables that only made sense within one style, gathering the following final set: \n1. Mode of type of scale : Ionian (major mode), dom i-\nnant Phrygian (Phrygian mode with a major third) or \nbimodal (alternation of both modes). This variable makes sense to classify the different styles.  \n2.  Direction of melody movement in the first hem i-\nstich .  \n3. Symmetry of the highest degree in the second hemistich.  \n4. Clivis (or flexa) at the end of the first hemistich.   \n5. Repetition of the first hemistich.  \n6.  Initial note.  \n7. Final note of the second hemistich.  \n8. Highest degree in both hemistiches .   \n9. Frequency of the highest degree in the second hemistich with respect to a cante. This variable has \nbeen normalized as follows. Let µ be the average of \nthe frequencies of the all cantes , and σ its standard \ndeviation. The frequency takes three values, 0 (se l-\ndom), if the frequency is less than µ -σ; 1 (normal) if it \nis within the interval [µ -σ, µ+σ]; and 2 (often), when \nit is greater than µ+σ.  \n10. Frequency of the highest degree in the second h\nemistich with respect to the whole corpus. In this \n354\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \ncase the mean and the standard deviation are taken \nwith respect to all pieces in the corpus.  \n11. Duration with respect to the piece.  This variable is \ndefined as above, taking three values fast, regular and slow.   \n12. Duration with respect to the whole corpus . Now the \nmean and the standard deviation are computed for the \nwhole corpus. \nNote that some of these variables do not appear as \nspecific features in some cante. We removed those var i-\nables that only accounted for one cante and tried to keep \nthe smallest set of explanatory variables. Moreover, we performed a principal component analysis (SPSS 15 were used for this). As a consequence, two variables were di s-\ncarded: Clivis (or flexa ) at the end of the first hemistich \nand repetition of the first hemistich. The number of final dimensions turned out to be 3 and the Cronbach’s alpha equalled 0.983, a very good value, indeed. \nThe distance we used to measure the similarity b e-\ntween two cantes was the simplest one could think of: the Euclidean distance between features vectors. Our inte n-\ntion was to test how powerful the musical features would be. The Euclidean is just a geometrical distance and does not reflect perceptual distance whatsoever. However, b e-\ncause of the robustness and power of the musical fe a-\ntures, results were good. \nWe used phylogenetic trees [7] to better visualize \nthe distance matrix. For the actual computing, we used the tool SplitsTree [7]. SplitsTree computes a tree (or \nmore generally a graph) with the property that the di s-\ntance in the drawing between any two nodes reflects as closely as possible the true distance between the corr e-\nsponding two pieces in the distance matrix. In general, clustering and other properties are easier to visualize with phylogenetic trees .  \n6.2 Intra-Style Similarity  \nOn a second step, we analyzed similarities between pe r-\nformances within each particular style. In this case, we \nconsidered all the variables presented below for each pa r-\nticular style ( martinete1 , martinete 2 or debla ). We co m-puted the corresponding distance matrices and generated their phylogenetic trees. As before, we used the Eucli d-\nean distance. Again, our intention was to evaluate how well the defined features characterize these cantes .  \n7. RESULTS \n7.1 Inter-Style Similarity  \nRegarding style classification, Figure 2 displays the \nphylogenetic tree obtained for the whole corpus under study. Label Dx stands for debla , label M1 stands for \nmartinete 1  and label M2 for martinete 2. \nAs it can be noticed, discrimination among cantes is \nvery good. There are three clearly separated clusters, each corresponding to a cante. \nIn addition, we have computed for each cante  its clo s-\nest neighbour and checked if both belong to the same style. That was the situation for 88.6% of the cases. There was a striking case in one particular debla. Debla D2- 14 \nwhose three nearest neighbours turned out to be of class martinete 2 . That debla  is one of the oldest known songs, \naccording to our musicological knowledge. Meaningfully \nenough, the closest martinetes are also old, which su g-\ngests a possible common origin or an evolution of mart i-\nnete 2 from debla (martinete 2 seems to be a mixture of \npreviou s cantes ). Those martinetes are M2-66, M2- 68 and \nM2-65; see Figure 3 at the end of this paper. \nWe also observed that some distance values were \nequal to zero for the same style; this will be discussed \nlater in the conclusions section. Finally, precision and  re-\ncall, two common performance measures, curves are pr o-\nvided in Figure 4 .  \n7.2 Intra-Style Similarity  \nWe did not expect a very fine clustering in the intra-style \ncase, as the musical features account for general prope r-\nties rather than for specific properties. In general, there \nwas one big cluster, often with several cantes  having the \nsame distances. Overall, this was in general coherent to the expert’s knowledge on the musical relation among performers.  \nFigure 3:  A phylogenetic graph  \nfor the whole corpus.  \n355\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \n \n \nFigure 4: Precision versus recall for style classification. \n8. CONCLUSIONS \nThere are many automatic systems that compute simila r-\nity between pieces of music by measuring the distance \nbetween some musical features. One common feature is the melodic contour. Therefore, the problem of measuring musical similarity is transformed into the problem of computing distances between melodic contours. For ce r-\ntain type of music that approach may be appropriate and even produce good results. It is not the case for flamenco a cappella flamenco cantes, a kind of music characterized by very special  features. In our previous study [3] we fo l-\nlowed that approach and understood its drawbacks. R e-\nducing melodic similarity to computing the distance b e-\ntween melodic contours leaves significant variables aside, which results in a limited similarity measure. \nIn the present study we took another approach. We \nidentified a set of musical features as basis for our mea s-\nure. Those musical features are related to melodic prope r-\nties, duration and form and reflect in general high-level characteristics of the cantes.  We also found some limit a-\ntions. As the reader may have noticed, many cantes  give \nthe same feature vector and, therefore, in the phylog e-\nnetic trees they all appear on the same node. Why is that so? A moment’s thought will reveal that this situation is not as surprising as it might seem at first glance. As we mentioned, the musical features account for high-level characteristics. Two cantes may be different at the fine \ndetail level and yet have the same musical features. That \nis the hardness of dealing with flamenco cantes . \n It is clear that in order to obtain good similarity \nmeasures musical knowledge of the particular type of music must be incorporated into the model. On the other hand, since we know this is not enough, fine detail info r-\nmation such as melodic contours should be incorporated, too. Thus, there should be a tradeoff between both a p-\nproaches. That is the research we are presently carrying out. \nThe identification of the musical features of a ca p-\npella cantes  is one of the main contributions of this paper. \nThey have not been described before and our experiments have proved they work quite well for inter-style classif i-\ncation. Our approach can be easily adapted to build a classification system based on machine learning. For the case of intra-style classification, more musical features should be identified. Our goal was to distinguish general musical features rather than specific features. This situ a-\ntion shows that special needs require special treatments and that inter-style and intra-style classification needs a \ndifferent set of musical features.    \nAt present all the work put forward here has been \ndone in a manual way. First, we wanted to check that the \nextraction of the musical features were correct and, fu r-\nthermore, gave a good measure of  similarity. We are cu r-\nrently working on building an automatic system based on \nour measure. This includes the automatic extraction of the musical features from music recordings through an aut o-\nmatic transcription procedure. \n9. REFERENCES\n \n[1] Blas-Vega, J Historia de las tonás , ed. Guadalhorce, \nMálaga, 1967. \n[2] Celma, O. Music Recommendation and Discovery in \nthe Long Tail. PhD thesis, Universitat Pompeu Fabra, 2008.   \n[3] Cabrera, J.J., Díaz-Báñez, J.M., Escobar, F.J., Gómez, \nE., Gómez, F., Mora, J. Comparative Melodic Anal y-\nsis of A Cappella Flamenco Cantes. In Conference on Interdisciplinary Musicology. Thessaloniki, Greece , \n2008. \n[4]  Donnier, P. Flamenco: elementos para la transcri p-\nción del cante y la guitarra, Proceedings of the IIIrd \nCongress of the Spanish Ethnomusicology Society , \n1997. \n[5] Fernández, L. Flamenco Music Theory. Acordes \nConcert. 2004. \n[6] Guastavino, C., Gómez, F., Toussaint, G., Marandola, \nF., Gómez, E. Measuring Similarity between Fl a-\nmenco Rhythmic Patterns.  Journal of New Music R e-\nsearch.  38(2), 129- 13, 2009.  \n[7] Huson, D. H. SplitsTree: Analyzing and visualizing \nevolutionary data, Bioinformatics , 14:68-73, 1998. \n[8] Lefranc, P. El Cante Jondo. Del territorio a los \nrepertorios: tonás, seguiriyas, soleares.  Publicaciones \nde la Univ. de Sevilla, Cultura Viva, Vol. 16, 2000. \n[9] Serrá, J., Gómez, E., Herrera, P., and Serra, X. \nChroma Binary Similarity and Local Alignment A p-\nplied to Cover Song Identification.  IEEE Transactions \non Audio, Speech and Language Processing , Vol . \n16(6), pp. 1138-1151, 2008. \n[10] Widmer, G., Dixon, S., Knees, P., Pampalk, E. \nPohle, T. From Sound to Sense via Feature Extraction \nand Machine Learning: Deriving High-Level Descri p-\ntors for Characterizing Music , in Sound to Sense, \nSense to Sound: A State of the Art in Sound and M u-\nsic Computing, Chapter 5, 2008. \n Acknowledgements : This research has been partially \nfunded by the Junta de Andalucía COFLA project (P09-\nTIC- 4840). 0 10 20 30 40 50 60 70 80 90 10030405060708090100Precision vs Recall\n356\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "Monophonic Instrument Sound Segregation by Clustering NMF Components Based on Basis Similarity and Gain Disjointness.",
        "author": [
            "Kazuma Murao",
            "Masahiro Nakano",
            "Yu Kitano",
            "Nobutaka Ono",
            "Shigeki Sagayama"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1417113",
        "url": "https://doi.org/10.5281/zenodo.1417113",
        "ee": "https://zenodo.org/records/1417113/files/MuraoNKOS10.pdf",
        "abstract": "This paper discusses a method for monophonic instrument sound separation based on nonnegative matrix factoriza- tion (NMF). In general, it is not easy to classify NMF com- ponents into each instrument. By contrast, monophonic in- strument sound gives us an important clue to classify them, because no more than one sound would be activated simul- taneously. Our approach is to classify NMF components into each instrument based on basis spectrum vector sim- ilarity and temporal activity disjointness. Our clustering employs a hierarchical clustering algorithm: group average method (GAM). The efficiency of our approach is evalu- ated by some experiments.",
        "zenodo_id": 1417113,
        "dblp_key": "conf/ismir/MuraoNKOS10",
        "keywords": [
            "monophonic",
            "instrument",
            "sound",
            "separation",
            "nonnegative",
            "matrix",
            "factorization",
            "NMF",
            "basis",
            "spectrum"
        ],
        "content": "MONOPHONIC INSTR UMENTSOUND SEGREGATIONBY\nCLUSTERINGNMF COMPONENTS BASEDON BASISSIMILARITY AND\nGAINDISJOINTNESS\nKazumaMurao MasahiroNakano YuKitano NobutakaOno Shigeki Sagayama\nGraduateSchool of Information Science and Technology\nThe Universityof Tokyo,Japan\nfk-murao, mnakano, kitano, onono, sagayama g@hil.t.u-tokyo.ac.jp\nABSTRACT\nThispaperdiscussesamethodformonophonicinstrument\nsound separation based on nonnegative matrix factoriza-\ntion(NMF).Ingeneral,itisnoteasytoclassifyNMFcom-\nponentsintoeachinstrument. Bycontrast,monophonicin-\nstrumentsoundgivesusanimportantcluetoclassifythem,\nbecausenomorethanonesoundwouldbeactivatedsimul-\ntaneously. Our approach is to classify NMF components\ninto each instrument based on basis spectrum vector sim-\nilarity and temporal activity disjointness. Our clustering\nemploysahierarchicalclusteringalgorithm: groupaverage\nmethod (GAM). The efﬁciency of our approach is evalu-\natedby some experiments.\n1. INTRODUCTION\nIn music signals, there are usually multiple sound sources\nsuchasahumansingingvoiceandinstrumentssound. The\ntask to separate mixed signals into individual sources is\ncalled sound source separation for music signals. It has\nseveral applications such as music equalizer, music infor-\nmation searching, automatic transcription, and structured\ncoding of music. This paper discusses a method to sepa-\nratemonauralmusicalaudiointoindividualmusicalinstru-\nments.\nSoundsourceseparationformusicsignalhasbeenwidely\ninvestigated recently. Some methods are based on super-\nvised learning of individual source models [1–3]. They\nneed solo excerpts beforehand. Other unsupervised ap-\nproaches have also been studied [4–6]. Because any prior\ninformationforinstrumentalsoundsourcescannotbeused,\nsome unsupervised methods make assumption about com-\nmon harmonic structure [4,5] or employ the excitation-\nﬁlter model of sound production [6]. We propose an ef-\nﬁcient unsupervised method focusing on monophonic in-\nstrumentsound.\nOur method have two stages. At the ﬁrst stage, we\nfactorize the observed spectrogram into some components\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnotmadeordistributedforproﬁtorcommercialadvantageandthatcopies\nbearthis notice and the full citation on the ﬁrst page.\nc°2010 International Society for Music Information Retrieval.based on nonnegative matrix factorization (NMF) [9,10].\nIn the case of music signals, each component usually rep-\nresents a musically meaningful element, so that different\nelements are expected to correspond to different compo-\nnents.\nHowever, considering music instrumental source sepa-\nration, methods based on NMF generally encounter difﬁ-\nculties in the components clustering step. And most of the\nalgorithmcountonmanualclustering[7]. Someclustering\nmethodsseparatepercussiveinstrumentsources[8,12],but\nare rarely used with harmonic instruments sources.\nThis paper proposes a method for clustering compo-\nnents that employs not only spectral information but also\ntemporal information. The outline of this paper is as fol-\nlows. Section 2 gives a overview of NMF algorithm and\ncomponent-clustering problem. The proposed clustering\nmethod is explained in Section 3, and experimental evalu-\nation of proposed method are presented in Section 4. Sec-\ntion 5 coversthe conclusions and future works.\n2. NONNEGATIVEMATRIXFACTORIZATION\nNonnegative matrix factorization and some unsupervised\nsound source separation algorithms are based on a signal\nmodelwherethespectrumvector xt(t= 1; :::; T )inframe\nismodeledasalinearcombinationof basisvectors bj(j=\n1; :::; J). This can be written as\nxt=JX\nj=1gj;tbj; (1)\nwhere Jisthenumberofbasisvectors,anditstime-varying\ngain(amplitude) gj;t,Tbeing the number of frames.\nThismodel can be written using a matrix notation as\nX=BG; (2)\nwhere X= [x 1; :::;xT],B= [b 1; :::;bJ], and [G]j;t=\ngj;t.\nHere,gj= [g j;1; :::; g j;t]Tisdeﬁnedas gainvector cor-\nresponding to the basis vector, then the term component\nrefers to one basis vector bjand one corresponding gain\nvector gj. Eachsourceismodeledasasumofthecompo-\nnents. The separation is done by ﬁrst factorizing the spec-\ntrogram of the input signal into components and second\ngrouping these to sound sources.\n375\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Input signal \nNMF \nComponents Basis (spectrum) vector \nGain (note) vector \nSpectrum \nsimilarity Note \ndisjointness Clustering \nSeparated \nsignals Reconstruction \nof instrument-\nwise signals Figure\n1. Flowdiagram of the method.\nTheNMFalgorithmsproposedbyLeeandSeung[9]do\nthe decomposition by minimizing the reconstruction error\nbetween the observation and the model while constraining\nthematrices to be entry-wise nonnegativeas follows:\nD(XkBG) =X\nf;td([X] f;tk[BG] f;t);(3)\nhered(ykz)is a function of two scalar variables. The var-\nious measures for reconstruction error are proposed. The\nEuclidean distance, the generalized Kullback-Leibler di-\nvergence[9],ortheItakura-Saitodivergence[11]aremostly\nused. WechooseherethegeneralizedKullback-Leiblerdi-\nvergence,whichhasproducedgoodresultsinearliersound\nsourceseparation studies [14].\nIn standard NMF, the only constraints is the elemen-\ntwise non-negativity of all matrices. Then, several con-\nstraints have been proposed in order to achieve expected\nsolutions. The most famous constrains are sparsity [13]\nand temporal continuity [11,14]. We use the sparsity and\ntemporalcontinuity proposed in [14].\nWe wish to use NMF to decompose the observed sig-\nnal into the components. However, it is not easy to know\nwhich source each component is assigned to. In the next\nsectionan automatic clustering method is proposed.\n3. CLUSTERING OF NMF COMPONENTS\n3.1 Outline\nAs a result of NMF, basis vectors bjand gain vectors gj\nare obtained, each of which could ideally represent spec-\ntrum and temporal activity of each note, respectively. Theproblemhereishowtoclassifyobtainedcomponents (bj;gj)\ninto each instrument. The contribution of this paper is\nto exploit both information of bjandgj)for mixture of\nmonophonic instrumental tracks without any prior about\neach instrument. Our approach consists of 1) measuring\nthe basis spectrum similarity C1(i; j)for any pairs of bj\nandbj, 2) measuring the temporal activity disjointness\n˜C2(i; j)for any pairs of giandgj, 3) calculating a close-\nness measure C(i; j)for any pairs of (bi;gi)and(bj;gj)\nbyproductof C1(i; j)and˜C2(i; j),and4)applyingakind\nof hierarchic clustering method.\n3.2 Similarity of Basis Spectra\nMonophonic source signal is represented by a sinusoidal\nmodel [15] as\ns(t) =RX\nr=1Ar(t) cos[µ r(t)] + e(t) (4)\nwhere e(t)isthenoiseterm, Ar(t)andµr(t) =Rt\n02¼rf 0(¿)d¿\nare the instantaneous amplitude and phase of the rth har-\nmonic,respectively, f0(¿)isthe fundamentalfrequencyat\ntime¿, and Ris number of the harmonic overtone. Har-\nmonic structure is an approximately invariant feature for a\nharmonic instrument when it is played in a narrow pitch\nrange. [16]\nInlogarithmicfrequency(log-frequency)scale,thehar-\nmonic frequencies are located log 2,log 3, ..., away from\nthe log-fundamental frequency, and the relative-location\nrelation remains constant no matter how fundamental fre-\nquencyﬂuctuatesandisanoverallparallelshiftdepending\non the ﬂuctuation degree. Thus among the harmonics be-\ntween the two spectrums of the same instruments are sim-\nilar; even in case spectrums fundamental frequencies are\ndifferent,shapes of the spectrums are same when shifted.\nThe basis vector bj, which NMF factorize into, rep-\nresents average spectrum in logarithmic frequency scale.\nTherefore the correlation-like criterion between two basis\nvectorsare deﬁned as\nC1(i; j) = max\nqP\npbp+q;ibp;j\njbijjbjj; (5)\nwhere bp;jispthv\nalue of the basis vector bj. Put another\nway, criterion C1(i; j)means maximum cross-correlation\nbetween normalized biandbj. In intuitive explanation,\ntwospectraarecompared,movingalongthefrequencyaxis,\nand are measured largest overlap. For the spectra by har-\nmonic instrument, two spectrums overlap most when two\nfundamental pitches nearly go over. As a side-effect, two\nspectrums by inharmonic instruments mark higher value\nthanvaluebetweenharmonicandinharmonicinstrumental\nspectrums.\nTable 1 shows an example of this correlation-like crite-\nrions that is calculated by real instrumental signals: RWC\nmusicdatabase[17]RWC-MDB-I-2001No.31-1andNo.33-\n1, down-sampled to 16 kHz single-channel ﬁles. Each\nspectrum is taken by Wavelet transform of single tone sig-\nnal. Two spectra of same instrument almost mark higher\n376\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Clarinet Flute\nA4H4 C5A4H4 C5\nClarinetA41.000.96\n0.810.580.670.81\nClarinetH4 1.000.740.630.66\n0.72\nClarinetC5 1.000.820.730.92\nFluteA4 1.000.95\n0.80\nFluteH4 1.000.80\nFluteC5 1.00\nTable\n1. Thesimiraritymeasureofbasisspectracalculated\nby indivisual instrumental signals. The higher values than\n0.8are shownin bold style.\nvalue than two spectrums of other instrument mark. How-\neverinsomecasestwospectrawhichbelongingtootherin-\nstrument mark high numerical number: for example, Clar-\ninet C5 and Flute C5. This result presents that criterion\nas basis spectrum similarity (5) indicates measure to some\nextent,butis not enough for the grouping.\n3.3 Disjointness of TemporalActivities\nNot only basis spectrum bj, but also temporal activity gj\nshouldalsoincludecuesforclusteringcomponentsintoin-\nstrumental tracks. As a simple case to exploit such infor-\nmation, we suppose that all instrumental tracks are mono-\nphonic, which means each instrumental track consists of a\nsinglenote sequence.\nFigure 2 shows an example of piano-roll representation\nof three monophonic instrumental tracks. Obviously, any\ndifferentnoteactivitiesaredisjointinthesametrack. Note\nthattherearealsomanypairsofdisjointnoteactivitiesover\ndifferent tracks. Hence, we can’t assert that two different\nnoteactivitiesbelongtothesametrackeveniftheyaredis-\njoint. However,iftwodifferentnoteactivitiesareNOTdis-\njoint,theyshould belong to differentinstrumental tracks.\nThedisjointnessoftwodifferenttemporalactivitiesrep-\nresentedbygainvectors giandgjcanbesimplycalculated\nby\nC2(i; j) = 1¡gi¢gj\njgijjgjj: (6)\nIfgiandgjaredisjoint, C2(\ni; j) = 1. While if they have\nco-occurrence, C2(i; j)should take a small value. There-\nfore, it can be exploited as a closeness measure. Figure\n3 shows an expected result, which was calculated by (6)\nwith using temporal activities in piano roll representation\nshownin Figure 2 as gj.\nThe magnitude of C2(i; j)itself is not signiﬁcant be-\ncause it depends on the frequency of the co-occurence. It\nisonlyimportantforclusteringwhetheritisalmostzeroor\nnot. Furthermore, because of imperfect decomposition by\nNMF, spectral leakage, reverbration, etc, C2(i; j)is actu-\naly not equal to zero even if ith component and jth com-\nponent belong to the same instrumental track. Therefore,\nwe 1) neglect tiny values of gt;jand set them to be zero,\n2) calculate C2(i; j)by (6), and 3) binarize it with a small\nFigure\n2. The piano roll representation of three mono-\nphonicinstrumentaltrack. Anydifferentnoteactivitiesare\ndisjoint in the same track.\nFigure\n3. Criterions between two gain vectors according\nto the equation (6), corresponding to ﬁgure 2. The two\nverticallineandtwohorizontallineshowtheborderlinesof\nthe instruments. Values on diagram position are ignorable\nfor the clustering.\nthreshold ²suchas\n˜C2(i; j) =½1 (C2(i; j)¸²)\n0 (C2(i; j)< ²):(7)\n3.4 Combining TwoDifferentCriterions\nPrevious criterions are both scales running from zero to\none. In both criterions, higher value means two compo-\nnents’ sameness. This paper examines the measure of two\ndifferentcomponents’ closeness as\nC(i; j) =C1(i; j)¢˜C2(i; j): (8)\n3.5 Clustering by GroupAverageMethod\nTo ﬁnd an optimal partitioning of the components into N\nclasses,thefollowingclusteringalgorithmcalled groupav-\neragemethod (GAM) is employed.\n1. At the beginning, all components are considered as\ndifferentclusters.\n2. Twocomponentsthathavethehighestcriterionvalue\nare connected into the same (new)cluster.\n3. Criterions between new cluster and other cluster are\nupdated under the update rule:\nd(K1; K2) =1\nn1n2X\ni2K1X\nj2K2d(i;j);(9)\n377\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)inputdatasampling rate 16kHz\nlength 10sec\nnumberof\ninstruments 3\nfrequencyframeshift 16ms\nanalysis frequency\nresolution 12:0cent\nfrequency\nrange 50–7961Hz\nNMF iteration 200\n[9] numberof\ncomponents 10–40\nClustering ² 0:05\nnumberof\nclusters 4\nTable\n2. Experimental conditions\nwhere d(A; B )isthecriterionbetweencluster Aand\nB,d(i; j) =C(i; j)isthecriterionbetweencompo-\nnents iandj,n1andn2are the number of compo-\nnentsthat K1andK2contain.\n4. Iteration: repeat step 2 and 3 until total number of\nclustersreaches L.\nCriterion-updateavoidschainingeffectwherewrongcom-\nponentsconnects into a chain reaction.\n3.6 Reconstruction of Instrument-wise Spectrograms\nSpectrogramscorrespondingtoacertaininstrument Kl(l=\n1; :::; L), ˆXl, can be reconstructed by the equation:\nˆXl=X\nj2KlˆX(j)=X\nj2Klbjgj: (10)\nSpectrogramof instrument lisreconstructed as\n[ˆYl]f;t=[ˆXl]f;t\n[ˆX]f;t[\nX]f;t: (11)\nwhere ˆX=PL\nl=1ˆXl.\n4. EXPERIMENTALEVALUATION\n4.1 SourceConditions\nToverifythepotentialperformanceoftheproposedmethod\nassoundsourceseparation,theproposedmethodwastested\nonarealperformancemusicdatafrom MIREX2007Eval-\nuation Tasks [18] : transcription of String Quartet No.5\n3rdMovementVar.5 composedbyL.V.Beethoven(seeta-\nble 2 for the list of the experimental data). We used the\ndatacomposedofthreewoodwindinstruments(ﬂute,oboe\nand bassoon). Mixed signal was the result of summing the\nsourcesignalsintimedomain,and9inputsignals(10sec-\nonds)wereclippedfromthemixedsignalevery5seconds.\nTime series of amplitude spectrum was analyzed using\nGabor wavelet transform with a frame shift of 16msfor\ninput digital signals of 16kHzsampling rate. The lower\nboundofthefrequencyrangeandthefrequencyresolution\nwere 50Hzand12cent, respectively.4.2 EvaluatedAlgorithms and Conditions\nThe followingalgorithms were tested.\n²Proposedmethod1: Componentsclusteringemployed\nboth basis vector similarity and gainvector disjoint-\nness.\nSince there is no reliable method for the estimation\nof the number of the components, proposed method\nwastestedbyfactorizingtheinputsignalinto10–40\ncomponentsandwedecidedittoearnthebestresult.\nIntheclusteringstep,thenumberoftheclusterswas\nchosen as 4 because in the real performance music\nother than pure instrumental sound (e.g. sounds of\nbreath) were contained.\n²Proposedmethod2: Componentsclusteringemployed\nonly basis vector similarity. Compared with Pro-\nposedmethod1,thecontributionofthetimeactivity\ndisjointness can be evaluated.\n²Correct clustering: Components clustering to be as-\nsigned each component to a source which leads to\nthe highest signal-to-noise (SNR) as\nSNR(m; j ) = 10 log10P\nf;t[Ym]2\nf;tP\nf;t([\nYm]f;t¡[ˆX(j)]f;t)2:\n(12)\nwhere YmandˆX(j)are themth reference and jth\nseparated component. A component jis assigned to\na sourcemwhich leads to the highest SNR.\n²NMF2D [4]: Factorization is doned by NMF2D in-\nstead of NMF. When analyzing real music signals,\nthe NMF2D wasconsidered to givegood results.\n4.3 EvaluationCriterion\nThe quality of the separated sources was measured by cal-\nculating the SNR improvement between the original spec-\ntrogram Yand corresponding separated magnitude spec-\ntrogram ˆYaccording to the equation\nSNR[dB] =1\nMMX\nm=110log10Ã P\nf\n;t[Ym]2\nf;tP\nf;t([\nYm]f;t¡[ˆYl]f;t)2\n¡P\nf;t[Ym]2\nf;tP\nf;t([\nYm]f;t¡[X]f;t)2!\n:\n(13)\nFor each original spectrogram, the SNR improvement that\nemploys baseline using mixed signal are measured. The\nSNR has been used in several source separation studies to\nmeasure the separation quality.\n4.4 Results\nThe SNR improvement for each data and algorithms are\nshown in table 3. Average values are means among all of\ndata.\nProposedmethod1marksanaverageimprovement2.75\ndB. For all data, proposed method 1 sets positive values.\n378\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)SNR[dB]\nproposed 1proposed 2NMF2D correctclustering\ndata(1):\n0–10 sec 5.62 -9.05 -0.16 5.94\ndata(2):\n5–15 sec 4.87 -0.71 -0.88 4.88\ndata(3):\n10–20 sec 4.41 -8.88 -0.30 4.45\ndata(4):\n15–25 sec 0.25 -6.23 -2.82 2.52\ndata(5):\n20–30 sec 2.08 -2.86 -1.29 3.34\ndata(6):\n25–35 sec 3.00 -7.82 -0.48 3.66\ndata(7):\n30–40 sec 0.72 -3.17 -1.12 1.48\ndata(8):\n35–45 sec 1.11 -5.71 -3.15 1.42\ndata(9):\n40–50 sec 2.70 -13.13 -1.65 3.93\nav\nerage 2.75 -6.40 -1.32 3.51\nTable\n3. SNR results of the evaluatedalgorithm in dB.\ntime [s]frequency [Hz]\n0 1 2 3 4 5 6 7 8 9 103520\n1760\n880\n440\n220\n50\n0 1 2 3 4 5 6 7 8 9 10-101\ntime [s]\nFigure\n4. An input signal with three instrumental tracks\n(ﬂute, oboe, and bassoon). Spectrogram (upper) and cor-\nrespondingwaveform(lower).\nThe average improvement value of correct clustering is\n3.54 dB. For two data (data (2) and data (3)) proposed\nmethod 1 and correct clustering mark almost same val-\nues. It shows that clustering step is maximally effective.\nIn some other data proposed method sets close values to\ncorrectclustering.\nComparingSNRvaluesbetweenproposedmethod1and\n2, it shows that in clustering step the contribution of the\ngainvectordisjointness is effective.\nTheSNRvaluesofNMF2Dmethodarelowerthanthat\nofproposedmethod1. Thereasonisconsideredtobethat,\nin these real music data, the NMF2D assumption that all\nnotes for an instrument is an identical pitch shifted time-\nfrequencysignature does not hold.\nFigures 4 , 5 and 6 show an example of experimental\nresults: ﬁgure 4 is an input signal in which three instru-\nmental signals (ﬂute, oboe and bassoon) are mixed, ﬁgure\n5isasourcesignalwithbassoonsounds,ﬁgure6isasepa-\nratedsignalwhichiscorrespondedtothebassoon’ssource\nsignal. Even in other two instrumental sounds the results\ntime [s]frequency [Hz]\n0 1 2 3 4 5 6 7 8 9 103520\n1760\n880\n440\n220\n50\n0 1 2 3 4 5 6 7 8 9 10-101\ntime [s]Figure\n5. A source signal (bassoon track) of the mixture\nshownin ﬁgure 4.\ntime [s]frequency [Hz]\n0 1 2 3 4 5 6 7 8 9 103520\n1760\n880\n440\n220\n50\n0 1 2 3 4 5 6 7 8 9 10-101\ntime [s]\nFigure\n6. Aseparatedsignal(bassoontrack)fromthemix-\nture shownin ﬁgure 4.\n379\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)equaledto\nit.\n5. CONCLUSION\nThispaperdiscussedamethodformonophonicinstrument\nsound separation. The method used nonnegative matrix\nfactorization to factorize the spectrogram of the input sig-\nnal into components. Then we introduced an criterion that\nmeasuredtwodistinguishcomponents: basisspectrumsim-\nilarity and temporal activity disjointness. The grouping\nwas done by clustering components under this measure.\nThe experiment results showed that in some data the pro-\nposedmethodmarkedvaluesequaltothecorrectclustering\nwhichemployedsource signals.\nFuture work includes the improvement of nonnegative\nmatrix factorization by including the proposed criterion,\nthataims at accuracyenhancement of the decomposition.\n6. ACKNOWLEDGEMENT\nThis research was supported by CrestMuse Project under\nJSTandGrant-in-AidforScientiﬁcResearch(KAKENHI)\n(A)20240017.\n7. REFERENCES\n[1] M. Hel ´ en and T. Virtanen: “Separation of Drums from\nPolyphonic Music using Non-negative Matrix Factor-\nization and Support Vector Machine,” in Proc. ISMIR,\npp.337–344, 2005.\n[2] F. R. Bach and M. I. Jordan: “Blind One-microphone\nSpeechSeparation: ASpectralLearningApploach,”in\nProc.NIPS ,pp. 65–72, 2005.\n[3] P.Leveau,E.Vincent,G.Richard,andL.Daudet: “In-\nstrumentspeciﬁcHarmonicAtomsforMid-levelMusic\nRepresentation,” IEEETrans.Audio,Speech,andLan-\nguageProcessing , Vol.16, No. 1, pp. 116–128, 2008.\n[4] M. N. Schmidt and M. Mørup: “Nonnegative matrix\nFactor 2-D Deconvolution for Blind Single Channel\nSourceSeparation,”in Proc.ICA ,pp. 700–707, 2006.\n[5] K.Miyamoto,H.Kameoka,T.Nishimoto,N.Onoand\nS. Sagayama: “Harmonic-Temporal-Timbral Cluster-\ning(HTTC)FortheAnalysisofMulti-instrumentPoly-\nphonicMusicSignals,”in Proc.ICASSP,pp.113–116,\n2008.\n[6] A.Klapuri,T.VirtanenandT.Heittola: “SoundSource\nSeparationinMonauralMusicSignalsusingExitation-\nﬁlter Model and EM Algorithm,” in Proc. ICASSP,\npp.5510–5513, 2010.\n[7] B. Wang and M. D. Plumbley: !HInvestigating single-\nchannel audio source separation methods based on\nnon-negative matrix factorization, !IinProc. ICA, pp.\n17–20,2006.\n[8] M. A. Casey and A. Westner: “Separation of Mixed\nAudio Sources by Independent Subspace Analysis,” in\nProc.ICMC ,pp. 154–161, 2000.[9] D. D. Lee and H. S. Seung: “Algorithms for Non-\nnegative Matrix Factorization,” Advances in Proc.\nNIPS,Vol.13, pp. 556–562, 2000.\n[10] P. Smaragdis and J. C. Brown: “Non-negative Matrix\nFactorization for Polyphonic Music Transcription,” in\nIEEEWorkshoponApplicationsofSignalProcess.Au-\ndio Acoust., NewPaltz,NY,pp. 177–180, 2003.\n[11] C. F ´ evotte, N. Bertin, and J.-L. Durrieu: “nonnegative\nmatrix factorization with the Itakura-Saito divergence.\nWith application to music analysis,” Neural Computa-\ntion, Vol.21, No. 3, pp. 793–830, 2009.\n[12] C.Uhle,C.DittmarandT.Sporer: “ExtractionofDrum\nTracksfromPolyphonicMusicusingIndependentSub-\nspace Analysis,”in Proc.ICA ,pp. 843–848, 2003.\n[13] P. O. Hoyer: “Non-negative Matrix Factorization\nwith Sparseness Constraints,” J. Mach. Learning Res. ,\nVol.5, pp. 1457–1469, 2004.\n[14] T. Virtanen,: “Monaural Sound Source Separation\nby Non-Negative Matrix Factorization with Tempo-\nral Continuity and Sparseness Criteria,” IEEE Trans-\nactions on Audio, Speech and Language Process. ,\nVol.15, No. 3, pp. 1066–1074, 2007.\n[15] X. Serra: “Musical Sound Modeling with Sinusoids\nPlus Noise,” in Musical Signal Processing , C. Roads,\nS. Popea, A. Picialli and G. D. Poli, Eds. London,\nU.K.:Swets & Zeitlinger,1997.\n[16] M.KimandS.Choi: “MonauralMusicSourceSepara-\ntion: Nonnegativity, Sparseness, and Shift-invaliance,”\ninProc.ICA, pp. 617–624, 2006.\n[17] M. Goto, H. Hashiguchi, T. Nishimura and R. Oka,\n“RWC music database: music genre database and mu-\nsical instrument sound database,” Proc. ISMIR, pp.\n229–230, 2003.\n[18] MIREX 2007 Evaluation Tasks: “Multiple Fun-\ndamental Frequency Estimation & Tracking,”\nhttp://www.music-ir.org/mirex/2007/index.php/Mul\ntipleFundamental FrequencyEstimation &Tracking,\n2007.\n380\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "A Multi-pass Algorithm for Accurate Audio-to-Score Alignment.",
        "author": [
            "Bernhard Niedermayer",
            "Gerhard Widmer"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1415910",
        "url": "https://doi.org/10.5281/zenodo.1415910",
        "ee": "https://zenodo.org/records/1415910/files/NiedermayerW10.pdf",
        "abstract": "Most current audio-to-score alignment algorithms work on the level of score time frames; i.e., they cannot differen- tiate between several notes occurring at the same discrete time within the score. This level of accuracy is sufficient for a variety of applications. However, for those that deal with, for example, musical expression analysis such micro- timings might also be of interest. Therefore, we propose a method that estimates the onset times of individual notes in a post-processing step. Based on the initial alignment and a feature obtained by matrix factorization, those notes for which the confidence in the alignment is high are chosen as anchor notes. The remaining notes in between are re- vised, taking into account the additional information about these anchors and the temporal relations given by the score. We show that this method clearly outperforms a reference method that uses the same features but does not differenti- ate between anchor and non-anchor notes.",
        "zenodo_id": 1415910,
        "dblp_key": "conf/ismir/NiedermayerW10",
        "keywords": [
            "audio-to-score alignment",
            "score time frames",
            "differentiate between notes",
            "micro-timings",
            "musical expression analysis",
            "post-processing step",
            "anchor notes",
            "confidence in alignment",
            "feature obtained by matrix factorization",
            "temporal relations"
        ],
        "content": "A MULTI-PASS ALGORITHM FOR ACCURATE AUDIO-TO-SCORE\nALIGNMENT\nBernhard Niedermayer1\n1Department for Computational Perception\nJohannes Kepler University Linz, Austria\nmusic@jku.atGerhard Widmer1;2\n2Austrian Research Institute for Artiﬁcial Intelligence\nVienna, Austria\nmusic@ofai.at\nABSTRACT\nMost current audio-to-score alignment algorithms work on\nthe level of score time frames; i.e., they cannot differen-\ntiate between several notes occurring at the same discrete\ntime within the score. This level of accuracy is sufﬁcient\nfor a variety of applications. However, for those that deal\nwith, for example, musical expression analysis such micro-\ntimings might also be of interest. Therefore, we propose a\nmethod that estimates the onset times of individual notes in\na post-processing step. Based on the initial alignment and\na feature obtained by matrix factorization, those notes for\nwhich the conﬁdence in the alignment is high are chosen\nas anchor notes. The remaining notes in between are re-\nvised, taking into account the additional information about\nthese anchors and the temporal relations given by the score.\nWe show that this method clearly outperforms a reference\nmethod that uses the same features but does not differenti-\nate between anchor and non-anchor notes.\n1. INTRODUCTION\nThere are several scenarios in which one wants to know\nthe exact parameters (such as onset time, loudness, and\nduration) of each individual note within a musical perfor-\nmance. Most of these scenarios can occur in musicology,\nwhere data from different performances is used to extract\ngeneral performance rules or to analyze individual artists’\nexpressive styles. Other applications of such data are ped-\nagogical systems or augmented audio editors and players.\nUnless the pieces under consideration are played on spe-\ncial computer-monitored instruments, audio recordings are\nthe only sources of data describing expression within ac-\ntual musical performances.\nOur aim here was to extract timing (note onset) param-\neters from a great variety of classical piano music perfor-\nmances automatically. The most general method for this\nwould be blind audio transcription, but current state of the\nart methods in this ﬁeld are not reliable enough to base per-\nformance analysis on their results. However, since in clas-\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2010 International Society for Music Information Retrieval.sical music the piece and score corresponding to an audio\nrecording can be assumed to be known, we can address the\nmuch simpler task of audio-to-score alignment.\nHere, most state of the art algorithms start by extracting\nfeatures (mainly chroma vectors) from each time frame of\nthe audio signal as well as from the score representation.\nTo obtain an optimal alignment between the two feature\nsequences, a distance measure between the feature vectors\nis then used as input either for Dynamic Time Warping\n(DTW) or for graphical models, such as Hidden Markov\nModels.\nHowever, an inherent shortcoming of these methods is\nthat – since only time frames are matched – they cannot\ndistinguish individual onsets of notes that occur simultane-\nously in the score. This impedes the analysis of expressive\nelements, such as arpeggios or the asynchronies between\na pianist’s hands or within a chord. To resolve this short-\ncoming, the method proposed here extracts an onset time\nestimation for each individual note. In order to do so, notes\nfor which the timing can be extracted with a relatively high\nconﬁdence level are marked as ”anchor notes”. In a second\npass, the system then tries to reﬁne the timings of the re-\nmaining notes by combining the expected position between\nthe anchors with spectral information.\nSection 2 gives an overview of related work. We ex-\nplain the extraction of anchor notes in Section 3, and de-\nscribe the reﬁnement method applied to the notes between\nsuch anchors in Section 4. Section 5 presents our experi-\nmental results and Section 6 provides the conclusion and\nan outlook on future work.\n2. RELATED WORK\nOnline versus ofﬂine differentiation aside, state-of-the-art\naudio-to-score alignment algorithms can be clustered into\ntwo main approaches. One is based on statistical, graphical\nmodels built from the score, such as those in [1,2,11]. The\nother one uses Dynamic Time Warping (DTW) in order to\nalign sequences of features calculated from both the audio\nand the score representation [5, 8].\nThe latter method normally uses chroma vectors as fea-\nture, resulting in relatively robust global alignments. How-\never, their temporal accuracy cannot compete with other\nfeatures which are used in onset detection. In [3], so-called\nDLNCO-features were introduced, which in essence com-\nbine chroma vectors and (pitch-wise) spectral ﬂux. More-\n417\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)over, a very high temporal feature resolution is used. This\nis not trivial, since DTW is of complexity O(n2), and com-\nputational costs constrain the number of frames that are\naligned. A multi-scale approach introduced in [8], how-\never,allows the temporal resolution to be increased itera-\ntively.\nAnother combination of chroma-based alignment with\nonset detection was presented in [6]. Here, an initial align-\nment is used in order to train an onset detector. Results\nfrom the onset detection are then used iteratively to re-\nﬁne the alignment and to train a better onset detector on\nthis more accurate data. This allows the use of supervised\nmachine learning techniques without the need for external\ntraining data.\nIn [7] and [9], results of a DTW-based alignment are\nreﬁned in a second pass. Both approaches place a search\nwindow around the tentative onset time of a note. This\nwindow is then scanned for a compatible note onset. While\nthe ﬁrst method relies on an STFT spectrogram, the latter\nuses a dictionary-based decomposition of the spectrum in\norder to differentiate between spectral energies induced by\nindividual notes.\nLike the method proposed here, these two approaches\ncan distinguish between the onsets of notes that occur at\nthe same discrete time within the score. This is different\nfrom most other systems, since it is inherent to both the\nDTW algorithms and to the graphical models used in [2,\n11] that they work on features representing discrete time\nsteps and that they cannot differentiate between two events\noccurring within the same time step.\n3. ANCHOR NOTE ESTIMATION\nAt ﬁrst the anchor notes are extracted using a two-pass pro-\ncedure as proposed in [9]. In the ﬁrst step, a state-of-the-art\naudio-to-score alignment based on chroma vectors and Dy-\nnamic Time Warping (DTW) is performed. Then, a dictio-\nnary of tone models is used in order to extract each note’s\nactivation function. Notes for which a signiﬁcant rise in\nactivation energy can be found near the corresponding es-\ntimated onset are selected to serve as anchors.\n3.1 Chroma Features and DTW\nIn [5], chroma vectors were found to perform best amongst\nseveral features in the context of audio matching and align-\nment. They consist of 12 elements per time frame corre-\nsponding to the pitch classes (C, C#, D,. . . ). The values\nare calculated from an audio recording by mapping the fre-\nquency bins of a short-time Fourier transform to pitch class\nindicesiusing\ni=\u0014\nround\u0012\n12 log2\u0012fk\n440\u0013\u0013\n+ 9\u0015\nmod 12 (1)\nwherefkrepresents the center frequency of the kthbin.\nThe term inside the brackets gives the number of the pitch\n(A4b= 0) that is nearest to fk, and applying the module\ngives the pitch class. The summand 9shifts the indicessuch thati= 0corresponds to the pitch class C. The actual\nvalues of the chroma vectors are then obtained by summing\nup the energies of all bins mapped to a certain pitch class.\nThere are two approaches to calculating chroma fea-\ntures from score representations [5]. One of them is to\nrender the score using a software synthesizer to reduce the\nproblem to the one described above. The other method\ncalculates the chroma vector directly from the score. Here,\nthe mapping becomes trivial, since the pitches are already\ngiven. However, one must make assumptions about pitch\nenergies and either use constant energies or a decay model.\nIn our experiments, we compared both methods – the ﬁrst\none using the free software synthesizer timidity1, and the\nsecond one using constant midi note energies. Preliminary\nexperiments showed that the resulting alignments did not\ndiffer signiﬁcantly between the two approaches. There-\nfore, we decided to use the second one, since it is much\ncheaper in terms of computational costs.\nGiven two sequences of feature vectors, a cost function\nmust be deﬁned which accounts for the error made when\naligning one speciﬁc frame within the ﬁrst sequence to an-\nother speciﬁc frame within the other sequence. Our ex-\nperiments showed that the Euclidean distance yields bet-\nter results than other possible measures, such as the cosine\ndistance.\nBased on the cost function, a similarity matrix SM can\nbe constructed. The rows of this matrix represent the time\nframes of the audio recording, whereas the columns repre-\nsent the time frames of the score. Hence, the value of each\ncellSMijcontains the cost of aligning the ithframe of\nthe audio signal to the jthframe of the score. Any contin-\nuous, monotonic path through this matrix that begins and\nends at the two end-points of the main diagonal represents\na valid alignment between the two sequences. The objec-\ntive is to minimize the global alignment cost, i.e., the sum\nof all local costs SMijalong the path through the similar-\nity matrix.\nUsing DTW, the optimal alignment is calculated in two\nsteps. The forward step starts at [0;0]and the correspond-\ning costSM 0;0. Then, all other optimal partial alignments\nending with the ithframe of the audio recording aligned\nto thejthframe of the score are obtained by recursively\nbuilding a second matrix Accu according to\nAccu(i;j ) = min8\n><\n>:Accu(i\u00001;j\u00001) +SMij\nAccu(i\u00001;j) +SMij\nAccu(i;j\u00001) +SMij(2)\nIn the forward step, another matrix stores which of these\nthree options has been used in order to advance to the next\ncell. As soon as the end point [N\u00001;M\u00001]has been\nreached, this information is utilized to reconstruct the path,\ni.e., the optimal alignment. A more detailed description of\nthe DTW algorithm is given in [10].\n1http://timidity.sourceforge.net\n418\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)3.2 NMF and Anchor Selection\nThe global alignment resulting from the DTW is robust.\nHowever, local inaccuracies are inherent to the algorithm.\nTherefore, an additional feature based on non-negative ma-\ntrix factorization (NMF) is used to reestimate the onset of\neach individual note.\nNMF is the decomposition of one matrix Vof sizem\u0002\nninto two output matrices WandHof sizesm\u0002rand\nr\u0002n, respectively, such that all elements of WandHare\nnon-negative and\nV\u0019WH (3)\nApplied to audio processing, such a decomposition of a\nspectrogram results in a dictionary Wofrweighted fre-\nquency groups and the corresponding activation energies\nHof these frequency groups over time.\nHere we use a modiﬁcation, as described in [12] and\n[9], in which Wis set to a pretrained set of tone models.\nThese models are computed from audio recordings of sin-\ngle tones played on a piano by, in essence, taking each bin’s\nweighted average energy over the time span where the tone\nis sustained. The weight of a frame is the inverse of the am-\nplitude envelope to compensate for different loudnesses.\nAssuming a ﬁxed W, onlyHis estimated. Since the\npitch described by an individual tone model is known, the\nithcolumn ofHis a feature representing the activation\nenergy of each pitch in time frame i.\nTo improve the onset time estimates, a search window\nof lengthlis centered around the onset time tdtwobtained\nby the DTW algorithm. Within this search window a fac-\ntorization is performed using a dictionary Wconsisting of\ntone models of all those pitches that are expected to be\nplayed within that time span and an additional white noise\ncomponent in which the energies are spread uniformly over\nall frequency bins. A new onset time candidate tnmf is\nthen obtained by choosing the time frame with the largest\nincrease in energy of the pitch under consideration. In con-\ntrast totdtw,tnmf can deviate from other notes with the\nsame score time.\nWhen thinking of repeated notes or of fast passages in\nwhich a certain pitch is played several times within the\nsearch window, it becomes obvious that this method is too\nsimple to yield meaningful results. However, estimating\nthe onsets of repeated notes is a relatively hard problem in\nitself. Spectral energy of a sustained note weakens the in-\ndicators for the onset of a new note if they have the same\npitch. Under these circumstances, algorithms are likely to\nget mislead by onsets of other notes with overlapping har-\nmonics. This fact makes such notes ineligible to be anchor\nnotes, as a high conﬁdence in the exact estimation of the\nonset time is essential. Thus, all notes which are played\ntwice or even more often within the time span of the search\nwindow, as determined from the score, are discarded from\nthe anchor candidates.\nLikewise, all notes are dropped from the list of anchor\ncandidates, for which the initial onset estimate tdtwand\nthe estimate given by the factorization-based feature tnmf\ndiffer by more than a certain time span which could haveplausibly been caused by an arpeggio or a simple asyn-\nchrony. This is justiﬁed because such a conﬂict decreases\nthe conﬁdence in the onset estimation. Moreover, there is\nno safe way to give either tdtwortnmf a preference over\nthe other. On the one hand, tdtwis supposed to be more ro-\nbust, since much more context information is incorporated.\nOn the other hand, tnmf is not bound by the constraints in-\nherent to the DTW algorithm, and therefore able to yield\nmore accurate results [9].\nIn summary, the two times tdtwandtnmf are calculated\nby the DTW algorithm and ﬁnding the maximum slope\nwithin the factorization-based pitch activation. A note is\nthen selected as an anchor if the following two criteria are\nmet:\n1.jtdtw\u0000tnmfj<threshold\n2. there are no other notes of the same pitch within\ntdtw\u0006l=2\nIn our experiments, we used an STFT with window and\nhop sizes of 4095 and 1024 frames, respectively, to com-\npute the chroma features from the audio signal. In or-\nder to extract chroma vectors from the score, window and\nhop sizes had been scaled such that the overall number\nof frames and the overlap ratio remained unchanged rel-\native to the audio representation. Since the DTW mis-\nplaces only a negligible fraction of all notes by more than\na second, we chose 2.0 seconds for the size lof the search\nwindow. Within this search window the hop size was de-\ncreased to 256 frames. The maximum difference jtdtw\u0000\ntnmfjallowed between the two onset estimates was set to\n20 frames, i.e., a little more than a tenth of a second.\nAn evaluation of the extraction of anchor notes is pre-\nsented in Section 5.\n4. NOTE REFINEMENT\nAfter extracting the anchor notes, the remaining notes must\nbe revised. For each of them (with the exception of notes\nplayed before the ﬁrst or after the last anchor notes) the\nspan of time during which it can be played is clearly con-\nstrained by the preceding and the successive anchor.\n4.1 Beta distribution\nIn addition to a new search window, bounded by the near-\nest anchors, rhythmic information in the score can be used\nto make even more detailed predictions on where to look\nfor an onset. Therefore, the numbers or fractions of beats\nbetween the anchor notes and the note under considera-\ntion are extracted and their relation is transferred onto the\ntimescale of the audio recording. To account for inex-\nactnesses of the anchor extraction and expressive tempo\nchanges, the ”expectation strength” of the onset occurring\nat timetis modeled by a beta distribution2. The beta dis-\ntribution is deﬁned continuously on the interval [0;1]and\n2The beta distribution was chosen for pragmatic reasons (the ﬂexibil-\nity of its shape and its restriction to a ﬁxed interval) rather than for precise\nprobability-theoretic reasons.\n419\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)zero outside this range. Depending on the values of its\nparameters\u000band\f, the density function can take several\nforms, for example, that of a uniform distribution, it can\nbe strictly increasing or decreasing, U-shaped, or – as in\nour case – it is unimodal (\u000b > 1and\f > 1). Its density\nfunction is deﬁned as\nf(x)\u000b;\f=1\nB(\u000b;\f )x\u000b\u00001(1\u0000x)\f\u00001(4)\nwhereBis the beta function\nB(\u000b;\f ) = 2\u0019=2Z\n0cos2\u000b\u00001\u0012sin2\f\u00001\u0012d\u0012 (5)\nMode ^xand variance \u001b2of the distribution are therefore\ngiven by\n^x=\u000b\u00001\n\u000b+\f\u00002(6)\n\u001b2=\u000b\f\n(\u000b+\f)2(\u000b+\f+ 1)(7)\nIn our application, we set the parameters \u000band\fby\nﬁxing a mode ^xand a variance \u001b2. The former is assumed\nto be at the onset time we expect according to score and\nanchor notes. Since the density function is only deﬁned\non[0;1], we use a linear projection to convert between the\ndomain of the beta distribution and the score time.\nThe variance is chosen such that it allows for expres-\nsive variations and inexactnesses of the anchor extraction,\nbut prevents notes from being placed at rhythmically un-\nreasonable timings. Experiments showed that the value\nmin(^x;1\u0000^x)=20 results in plausible expectation strengths.\nTwo such functions are depicted in Figure 1. The up-\nper plot shows the onset likelihood for the onset time of\nthe third note, assuming that the ﬁrst and ﬁfth note are an-\nchors. The time span between the anchor comprises three\nbeats. Since the note should be played after the ﬁrst out\nof these three beat-to-beat intervals, the function is clearly\nskewed. This is desirable because a musician’s freedom of\nexpressive timing is greater when the score calls for longer\ninter-onset intervals. The second function is the likelihood\nof the fourth note’s onset time given notes number one and\nsix as anchors. The function is now symmetric, since the\nonset time given by the score is exactly half the time span\n(two out of four beat-to-beat intervals).\nIn order to transfer these expectation strength functions\nfrom the score into the audio domain, another linear pro-\njection is applied.\n4.2 Onset estimation\nTo extract revised onset estimates for non-anchor notes, we\ncalculate the constant Q spectrogram over the time span in\nwhich the onset likelihood as described above is greater\nthan zero. The parameters of the constant Q spectrogram\nare chosen such that each energy bin corresponds to a spe-\nciﬁc pitch. The hop size is set to 256 frames, resulting in a\nvery high overlap ratio at the lower bins.\nFigure 1. Onset expectation strength for the 3rdand 4th\nnote.\nFor the purpose of onset detection, energy changes are\ncalculated and half-wave rectiﬁed. In order to incorporate\nthe score information, the result is then weighted by the\nexpectation strength. The ﬁnal onset is set to the time cor-\nresponding to the maximum of this detection function.\n5. EXPERIMENTAL RESULTS\n5.1 Evaluation Method\nSince this work was done in the context of musical perfor-\nmance and style analysis, we used classical (polyphonic)\npiano music to evaluate our system. The test data con-\nsisted of the ﬁrst movements of 11 Mozart sonatas played\nby a professional pianist. The overall performance time\namounted to more than one hour, comprising more than\n30.000 notes. The instrument used for the performance\nwas a computer-controlled B ¨osendorfer SE290 grand pi-\nano, which enables exact logging of all events such as keys\nbeing hit or released and changes in pedal pressure.\nThe evaluation was done using mechanical scores rep-\nresented in MIDI format and the real audio recording from\nthe performances as input data. The data recorded by the\nB¨osendorfer SE290 served as ground truth. The main eval-\nuation criterion was the absolute timing displacement be-\ntween aligned notes and the ground truth.\nOn the one hand, robustness and a high overall accu-\nracy are important issues. On the other hand, our work\nis directed towards providing methods for semi-automatic\naudio annotation. One objective of such a system must\nbe to minimize human input. In post-processing, the user\nmust correct the onset time as soon as there is a notice-\nable error. Therefore, we investigated not only the median\nand percentile errors, but also how many of the notes were\ndetected well enough for a human to accept it.\nIn [4], listening tests showed that the human hearing\nsystem does not detect timing variations of up to 10 ms in\nsequences of short notes, and even greater displacements\nin sequences of very long notes. Therefore, our evaluation\ncriteria were the proportions of notes aligned with a dis-\nplacement of less than 10 ms and 50 ms respectively. The\n50 ms tolerance was included because it is a common mar-\ngin in onset detection.\n420\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)50%<x[ms] 75%<x[ms] 95%<x[ms] max [ms]\npiece duration # notes # anchorsnon.a. anch. non.a. anch. non.a. anch. non.a. anch.\nK.279-1 4:55 2803 1136 15.2 5.5 29 11 138 37 879 494\nK.280-1 4:48 2491 1257 23.2 5.4 45 11 165 46 687 664\nK.281-1 4:29 2648 1235 23.7 6.1 48 12 176 48 993 442\nK.282-1 7:35 1907 705 23.8 6.9 60 13 439 72 4805 3008\nK.283-1 5:22 3304 1130 16.2 7.9 28 13 75 34 673 467\nK.284-1 5:17 3700 1223 15.2 6.1 31 14 120 71 1000 502\nK.330-1 6:14 3160 1176 16.3 5.6 30 10 179 35 960 835\nK.332-1 6:02 3470 1017 23.2 11.8 42 19 171 82 857 632\nK.333-1 6:44 3774 1471 17.8 7.5 31 13 132 38 941 404\nK.457-1 6:15 2993 1086 22.0 8.9 42 16 317 62 1773 1787\nK.475-1 4:58 1284 483 38.4 16.3 98 24 304 115 4471 2663\nTable 1. Comparison between accuracy (median, 75thpercentile, 95thpercentile, and maximum) of the anchor notes\n(anch.) and the non-anchor notes (n.a.)\npiece # non-anchors 50%<x[ms] 75%<x[ms] 95%<x[ms] max [ms]\nK.279-1 1667 9.1 28 127 879\nK.280-1 1234 9.2 24 147 706\nK.281-1 1413 11.2 31 187 1035\nK.282-1 1202 15.9 42 432 4822\nK.283-1 2174 12.0 21 92 464\nK.284-1 2477 9.0 26 125 1004\nK.330-1 1983 9.6 21 134 835\nK.332-1 2453 18.0 30 175 781\nK.333-1 2303 12.1 22 93 1000\nK.457-1 1907 16.5 37 246 1790\nK.475-1 812 24.1 49 398 4377\nTable 2. Accuracy of non-anchor notes after the reﬁnement step (median, 75thpercentile, 95thpercentile, and maximum)\n5.2 Evaluation Results\nTable 1 presents the results of the anchor detection step.\nAbout a third of the overall notes were chosen as anchors.\nAlthough this seems to be a very large fraction, it is justi-\nﬁed by the high accuracy of the selected notes. For half of\nthe pieces, the 95thpercentile still met the 50 ms criterion\nused for the evaluation of onset detection algorithms.\nHowever, for each piece a small number of outliers were\npicked as well. Some of them are due to our trade-off\nbetween a small search window at the NMF calculation\nand computational costs. Notes for which the initial align-\nment deviates from the real onset by more than a second\nare post-processed using a time frame that does not even\ncontain the correct onset.\nTable 2 shows that, in comparison to Table 1, a major-\nity of non-anchor notes were improved by the reﬁnement\nstep. Both the median deviation and the 75thpercentile im-\nproved for all the pieces. Only the accuracy of the outliers\ndecreased further in some cases. This might be due to poor\nanchor notes, which mislead onset detection.\nThe overall result as given by Table 3 shows the poten-\ntial of the proposed method. It clearly outperformed the\nreference algorithm from [9] in which the initial alignment\nand the factorization-based post-processing were done in\na similar way but without using score information to re-\nﬁne critical notes. Especially the proportion of note on-sets identiﬁed with a deviation of less than 10 ms – i.e.,\nthe threshold of human perception, according to [4] – was\nincreased signiﬁcantly from 40.0% to 49.8%. This is im-\nportant for the construction of data acquisition tools which\nare able to extract descriptions of musical expression from\naudio recordings semi-automatically.\n6. CONCLUSION AND FUTURE WORK\nWe have proposed a multi-pass method for the accurate\nalignment of musical scores to corresponding audio record-\nings. The main contribution is the introduction of an ex-\npectation strength function modeling the expected onset\ntime of a note between two anchors. Although results are\nencouraging, there are speciﬁc circumstances where the al-\ngorithm fails, i.e., temporal displacement of notes is large.\nOne class of such errors are poor alignments at a piece’s\nending. There, two disadvantageous factors coincide. On\nthe one hand, there is no additional subsequent note which\ncould serve as hint for the alignment or as anchor in the\npost-processing. On the other hand, a high degree of poly-\nphony in combination with long and soft notes is to be ex-\npected at endings. Such passages are inherently difﬁcult to\nhandle from a signal processing point of view.\nAn interesting example of such an error can be found\nin the sonata K.282, in which one note was even wrongly\npicked as an anchor although it was out of place by more\n421\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)50%<x[ms] 75%<x[ms] 95%<x[ms]x<10msx<50ms\npiece # notesref. new ref. new ref. new ref. new ref. new\nK.279-1 2803 12 7.2 27 18 101 103 43.2% 61.7% 88.4% 90.2%\nK.280-1 2491 14 7.1 34 16 127 93 42.5% 63.1% 85.0% 90.8%\nK.281-1 2648 15 8.5 36 19 112 114 38.5% 56.8% 83.4% 89.9%\nK.282-1 1907 15 11.8 44 27 380 378 39.2% 43.5% 76.8% 83.2%\nK.283-1 3304 12 10.2 26 18 65 70 44.2% 49.1% 92.2% 92.4%\nK.284-1 3700 13 8.0 29 21 98 110 41.7% 58.2% 87.2% 87.7%\nK.330-1 3160 11 7.6 24 15 124 103 46.7% 61.0% 89.7% 91.2%\nK.332-1 3470 18 16.0 37 27 147 148 32.5% 29.7% 82.7% 87.9%\nK.333-1 3774 13 9.9 20 18 80 68 42.2% 50.5% 90.1% 92.8%\nK.457-1 2993 15 13.4 35 26 257 183 35.9% 38.2% 83.2% 84.8%\nK.475-1 1284 24 20.1 75 37 393 376 23.6% 22.5% 66.8% 78.6%\nall 31534 14 10.1 32 21 137 121 40.0% 49.8% 85.6% 88.9%\nTable 3. Overall accuracy of the proposed anchor-based method (new) compared to the reference method as described\nin [9] (ref.)\nthan three seconds. The explanation is, that the last two\nchords of this piece differ by only one single note (b[-a[-\nd-f and e[-a[-d-f, respectively). The algorithm was not able\nto distinguish the two chords. As a consequence, the notes\nof the last chord were aligned to the onset of the preced-\ning chord as well. The resulting temporal displacement of\nabout three seconds is slightly shorter than the duration of\nthe ﬁrst of these chords.\nThis clearly leads further work towards the issues of im-\nproved mechanisms for anchor detection and the handling\nof inherently ”difﬁcult“ passages, such as the endings. An\napproach that could beneﬁt both ﬁelds is the introduction\nof a more sophisticated local conﬁdence or ﬁtness measure\nfor arbitrary sections of an alignment.\nAnother aspect which has not been considered yet is the\ndetection of deviations from the score, such as when the\npianists adds ornamentations or playing errors occur.\n7. ACKNOWLEDGEMENTS\nThis research is supported by the Austrian Federal Min-\nistry for Transport, Innovation and Technology, and the\nAustrian Science Fund (FWF) under project numbers TRP\n109-N23, P19349-N15, and Z159.\n8. REFERENCES\n[1] A. Cont: “Realtime Audio to Score Alignment for\nPolyphonic Music Instruments Using Sparse Non-\nnegative Constraints and Hierarchical HMMs”, Pro-\nceedings of the IEEE International Conference in\nAcoustics and Speech Signal Processing (ICASSP),\nToulouse, 2006.\n[2] A. Cont: “A coupled duration-focused architecture for\nrealtime music to score alignment”, IEEE Transactions\non Pattern Analysis and Machine Intelligence, V ol.\n99(1), 2009.\n[3] S. Ewert and M. M ¨uller: “Reﬁnement Strategies for\nMusic Synchronization”, Proceedings of the 5th Inter-national Symposium on Computer Music Modeling and\nRetrieval (CMMR 2008), Copenhagen, 2008.\n[4] A. Friberg and J. Sundberg: “Perception of just no-\nticeable time displacement of a tone presented in a\nmetrical sequence at different tempos”, Proceedings of\nthe Stockholm Music Acoustics Conference, pp. 39–43,\nStockholm, 1993.\n[5] N. Hu, R. B. Dannenberg, and G. Tzanetakis: “Poly-\nphonic Audio Matching and Alignment for Music Re-\ntrieval”, IEEE Workshop on Applications of Signal\nProcessing to Audio and Acoustics, New York, 2003.\n[6] N. Hu and R. B. Dannenberg: “Bootstrap Learning\nfor Accurate Onset Detection”, Machine Learning,\nV ol. 65, No. 2, pp. 457–471, 2006.\n[7] Y . Meron and K. Hirose: “Automatic alignment of a\nmusical score to performed music”, Acoustical Science\nand Technology, V ol. 22, No. 3, pp. 189–198, 2001.\n[8] M. M ¨uller, H. Mattes, and F. Kurth: “An Efﬁcient Mul-\ntiscale Approach to Audio Synchronization”, Proceed-\nings of the 7th International Conference on Music In-\nformation Retrieval (ISMIR), Victoria, 2006.\n[9] B. Niedermayer: “Improving Accuracy of\nPolyphonicMusic-to-Score Alignment”, Proceed-\nings of the 10th International Conference on Music\nInformation Retrieval (ISMIR), Kobe, 2009.\n[10] Rabiner, L. R. and Juang, B.-H. “Fundamentals of\nspeech recognition”. Prentice Hall, Englewood Cliffs,\nNJ, 1993.\n[11] C. Raphael: “Aligning Music Audio with Symbolic\nScores Using a Hybrid Graphical Model”, Machine\nLearning, V ol. 65 (2-3), pp. 389–409, 2006.\n[12] F. Sha and L. Saul: “Real-time pitch determination of\none or more voices by nonnegative matrix factoriza-\ntion”, Advances in Neural Information Processing Sys-\ntems 17, K. Saul, Y . Weiss, and L. Bottou (eds.), MIT\nPress, Cambridge, MA, 2005.\n422\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "IBT: A Real-time Tempo and Beat Tracking System.",
        "author": [
            "João Lobato Oliveira",
            "Fabien Gouyon",
            "Luis Gustavo Martins",
            "Luís Paulo Reis"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1416470",
        "url": "https://doi.org/10.5281/zenodo.1416470",
        "ee": "https://zenodo.org/records/1416470/files/OliveiraGMR10.pdf",
        "abstract": "This paper describes a tempo induction and beat track- ing system based on the efficient strategy (initially intro- duced in the BeatRoot system [Dixon S., “Automatic ex- traction of tempo and beat from expressive performances.” Journal of New Music Research, 30(1):39-58, 2001]) of competing agents processing musical input sequentially and considering parallel hypotheses regarding tempo and beats. In this paper, we propose to extend this strategy to the causal processing of continuous input data. The main rea- sons for this are threefold: providing more robustness to potentially noisy input data, permitting the parallel consid- eration of a number of low-level frame-based features as input, and opening the way to real-time uses of the system (as e.g. for a mobile robotic platform). The system is implemented in C++, permitting faster than real-time processing of audio data. It is integrated in the MARSYAS framework, and is therefore available under GPL for users and/or researchers. Detailed evaluation of the causal and non-causal ver- sions of the system on common benchmark datasets show performances reaching those of state-of-the-art beat track- ers. We propose a series of lines for future work based on careful analysis of the results.",
        "zenodo_id": 1416470,
        "dblp_key": "conf/ismir/OliveiraGMR10",
        "keywords": [
            "tempo induction",
            "beat tracking",
            "sequential processing",
            "parallel hypotheses",
            "causal processing",
            "continuous input",
            "robustness",
            "parallel consideration",
            "real-time use",
            "audio data"
        ],
        "content": "IBT: A REAL-TIME TEMPO AND BEAT TRACKING SYSTEM\nJo˜ao Lobato Oliveira1;2Fabien Gouyon1Luis Gustavo Martins3Luis Paulo Reis2\n1Institute for Systems and Computer Engineering of Porto (INESC Porto), Porto, Portugal\n2Artiﬁcial Intelligence and Computer Science Laboratory (LIACC), FEUP, Porto, Portugal\n3Research Center for Science and Technology in Art (CITAR), UCP, Porto, Portugal\nfjmso,fgouyong@inescporto.pt lgustavomartins@gmail.com lpreis@fe.up.pt\nABSTRACT\nThis paper describes a tempo induction and beat track-\ning system based on the efﬁcient strategy (initially intro-\nduced in the BeatRoot system [Dixon S., “Automatic ex-\ntraction of tempo and beat from expressive performances.”\nJournal of New Music Research, 30(1):39-58, 2001]) of\ncompeting agents processing musical input sequentially and\nconsidering parallel hypotheses regarding tempo and beats.\nIn this paper, we propose to extend this strategy to the\ncausal processing of continuous input data. The main rea-\nsons for this are threefold: providing more robustness to\npotentially noisy input data, permitting the parallel consid-\neration of a number of low-level frame-based features as\ninput, and opening the way to real-time uses of the system\n(as e.g. for a mobile robotic platform).\nThe system is implemented in C++, permitting faster\nthan real-time processing of audio data. It is integrated\nin the MARSYAS framework, and is therefore available\nunder GPL for users and/or researchers.\nDetailed evaluation of the causal and non-causal ver-\nsions of the system on common benchmark datasets show\nperformances reaching those of state-of-the-art beat track-\ners. We propose a series of lines for future work based on\ncareful analysis of the results.\n1. INTRODUCTION\nComputational tracking of musical beats from audio signal\nis a very important feature to automated music analysis. In\nthe context of Music Information Retrieval applications,\nsuch as e.g. automatic genre classiﬁcation, music similar-\nity computation, autotagging, or query-by-example, recent\nliterature indicates that audio descriptors of higher level of\nabstraction are needed [1]. It is a relatively safe bet to say\nthat reliable beat trackers will be helpful in this endeavour.\nRecent evaluations of existing beat tracking systems (see\ne.g. MIREX1) show that, although progresses have unde-\nniably been achieved in the last years, there is still room for\n1http://www.music-ir.org/mirex/2009/\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2010 International Society for Music Information Retrieval.improvement. Many open directions to beat tracking re-\nsearch are also detailed in a recent and very thorough eval-\nuation in [6]. Particularly, there is to date, to our knowl-\nedge, no real-time and open-source audio beat tracker avail-\nable.\nVery many papers in the literature address the problem\nof tempo induction and beat tracking of audio signals. Pro-\nviding a review of existing systems and algorithms is out\nof the scope of this paper. Interested readers are referred\nto [8] for a review of rhythm description systems.\nIt is however important to mention the main functional\naspects commonly found in beat tracking algorithms. A\ngeneric description includes the following computing\nblocks: (1) Audio feature extraction, (2) Induction (or “Pre-\ntracking” herein), and (3) Beat Tracking per se.\nIt is also interesting to notice that recent systems, e.g.\n[12], [7], [3], [14], tend to implement beat tracking as a\nrepeated induction process, in which tempo and beats are\ncomputed on consecutive windows of signal (usually a few\nseconds, where it is usually considered that the tempo is\nconstant), with overlap, and in which estimating tempo\nevolution and beat positions is done by connecting obser-\nvations between windows. We argue that a problem with\nthis approach is a potential computational overload, the in-\ntrinsic difﬁculty to adapt these tracking strategies to causal\nand real-time scenarios, as well as lack of continuity be-\ntween windows. Instead, we propose to follow the track-\ning strategy initially proposed in the system BeatRoot [4],\nwhere competing agents process musical input data sequen-\ntially and consider parallel hypotheses regarding tempo and\nbeats.\nWe propose to differ from BeatRoot’s strategy by imple-\nmenting a causal decision process over competing agents\n(instead of taking decisions after the whole data has been\nanalysed). Further, we extend the algorithm to the process-\ning of continuous input data. Our aim is to provide more\nrobustness to potentially noisy input data, and opening the\nway to (faster than) real-time uses of the system (as e.g.\nfor a mobile robotic platform). The system is implemented\nin C++ and the source code is available as GPL. Although\nthis paper does not provide experiments with respect to the\nusefulness of diverse low-level features as input to track-\ning beats [9] [2], it should be noted that a particularity of\nthe proposed architecture is precisely to be open to such\nexperiments. Another difference with BeatRoot lies in an\nattempt to not bias results towards faster metrical levels.\n291\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Audio Feature ExtractionMulti-Agent SystemAgent RefereeTempo Hypothesis InductionAgent InitializationPhase Hypothesis SelectionPre-TrackingBeat TrackingAudio Input\nBeat EstimationFigure 1. IBT block diagram.\nIn section 2, we describe one-by-one the functional\nblocks of IBT,2a tempo induction and beat tracking al-\ngorithm in the line of BeatRoot [4]. The algorithm follows\na modular workﬂow composed by: (1) an audio feature\nextraction module, “parsing” the audio data into a continu-\nous3feature sequence assumed to convey the predominant\ninformation relevant to rhythmic analysis; followed by (2)\na pre-tracking module, which outputs initial hypotheses re-\ngarding possible beat periods and phases; followed by (3)\na beat tracking module, which propagates hypotheses, pro-\nceeds to their online creation, killing and ranking, and out-\nputs beats on-the-ﬂy (see Figure 1). Section 3 reports on\na thorough evaluation of the system. Section 4 proposes\nsome practical hints for those intending to use the system,\nand/or make changes to its code. Section 5 discusses the\nsystem performances and proposes lines for future work.\n2. SYSTEM DESCRIPTION\n2.1 Audio Feature Extraction\nAccording to recent comparative studies evaluating alter-\nnative onset detection functions [5] and the accuracy of\nseveral low-level features applied to beat tracking purposes\n[9], we selected the spectral ﬂux as the audio feature over\nwhich all further processing will be done.\nOur implementation follows that proposed in [5]. Par-\nticular parameters are: Hamming window, window size of\n1024 samples (23.2ms at a sampling rate of Fs= 44100Hz),\nand 50% overlap.\nIn order to smooth the onset detection function and re-\nduce false detections, a low-pass Butterworth ﬁlter is ap-\nplied on the extracted spectral ﬂux values. As a way to\navoid phase distortion the spectral ﬂux values in the induc-\ntion window are ﬁltered in both the forward and reverse\ndirections, resulting in a precisely zero-phase distortion.\n2.2 Pre-tracking\nThe system is initialized on an induction window, set to a\nlength of 5s. The following sections (until Section 2.3) re-\nport on computations done on the induction window only.\n2Standing for INESC Porto Beat Tracker.\n3i.e. sampled, with typical sampling rate in the tenth of msecDuring the processing of that bit of data, the system does\nnot output beats. At the end of that pre-processing step, hy-\npotheses regarding periods, phases and scores (Pi;\u001ei;Si)\nof a number of beat agents are passed along to the beat\ntracking module.\nThe length of the induction window is a high-level pa-\nrameter that the user can deﬁne.\n2.2.1 Period Hypotheses Induction\nThe ﬁrst step in the pre-tracking stage is to compute a con-\ntinuous periodicity function, based on the spectral ﬂux au-\ntocorrelation, along time-lags \u001c:\nA(\u001c) =mX\nn=0SF(n)SF (n+\u001c); (1)\nwhereSF(n)is the (smoothed) spectral ﬂux for frame n,\nandmis the induction window size (in frames).\nThe periodicity function is then parsed by an adaptive\npeak-picking algorithm to retrieve Nglobal maxima, whose\ntime-lags constitute the initial set of period hypotheses Pi:\n(\nPi= arg max i(A(\u001c));i= 1;:::;N\nA(\u001c)>\u000e\u0003rms(A(\u001c ))\nM; (2)\nwhere\u000eis a ﬁxed threshold parameter, empirically set to\n0:75, and Mis the chosen tempo range, deﬁned to [50;250]\nBPM (i.e. periods of 240 ms to 1.2 s), at a 6ms granularity.\n2.2.2 Phase Hypotheses Selection\nFor each one of the period hypothesis Pi, a number of\nphase hypotheses \u001ej\ni(wherejis the index of the alternative\nhypotheses for the i-th period hypothesis) are considered\namong detected onsets (detection is done on the induction\nwindow only, and computed as proposed in [5]).\nFor each period hypothesis, we generate an isochronous\nsequence of beats (a “beat train template”) of constant pe-\nriod for each possible phase \u001ei, with the same length as the\ninduction window.\nUsing a simpliﬁed tracking procedure (see Section 2.3),\nconsidering a constant tempo and phase, we then select the\nbeat train template that best matches the detected onsets\nand retrieve its corresponding phase [10].\nAt this point, we have computed a set of period and\nphase hypotheses, (Pi;\u001ei). The next step is to compute a\nscore for each hypothesis and to rank them.\n2.2.3 Agents Setup\nA raw score Sraw\niis given to each (Pi;\u001ei)hypothesis, cor-\nresponding to the sum of time deviations between elements\nof the chosen beat train template and local maximum in the\nspectral ﬂux (see eq. (10)).\nScores are then updated via the consideration of possi-\nble metrical relationships between each pair of period hy-\npothesesnij. As proposed in [4], we deﬁne a score Srel\ni\nthat favors candidates whose periods are in integer rela-\ntionships:\nSrel\ni= 10\u0003Sraw\ni+NX\nj=0\nj 6=ir(nij)\u0003Sraw\nj (3)\n292\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)r(n) =8\n><\n>:6\u0000n;1\u0014n\u00144\n1;5\u0014n\u00148\n0;otherwise(4)\nFinally, we deﬁne the ﬁnal scores, Si, as follows:\nSi=Srel\ni\u0003max(Sraw) (5)\nThe estimated hypotheses (Pi;\u001ei;Si)can now be used\nto initialize a set of Nbeat agents, which will start their\nbeat tracking activity, as described in the following sec-\ntions.\n2.3 Beat Tracking\nFollowing the pre-tracking stage described in the previous\nsections, the process of on-line beat tracking will consist\non the supervision of the incoming spectral ﬂux values,\nconstantly handling any tempo/timing variations, while\nkeeping a good balance between reactiveness (speed of re-\nsponse to system changes) and inertia (stability of the sys-\ntem). As illustrated in Figure 1, this process is handled by\na multi-agent system mediated by a central referee.\n2.3.1 Agents Operation\nInitialized using the pre-tracking (Pi;\u001ei;Si)hypotheses,\nan initial set of Nbeat agents will start to propagate, in a\ncausal manner, predictions based on incoming data, by rep-\nresenting alternative hypotheses regarding beat positions\nand tempo. Each prediction is evaluated with respect to its\ndeviation (i.e. error) to the local maximum in the observed\ndata, within a two-level tolerance window. This is a stage\nwhere the system differs signiﬁcantly from BeatRoot: al-\nthough the tolerance windows are akin to [4], processing\ncontinuous data is necessarily different here than onsets;\nand the generation of new agents also differs as we include\nmore than one new hypothesis, accounting more speciﬁ-\ncally for tempo and/or timing deviations.\nThis two-level tolerance window consists in an inner\ntolerance region, Tin2[Tl\nin;Tr\nin]; Tl\nin=Tr\nin= 46:4\nms, for handling short period and phase deviations, and\nan asymetric outer tolerance region, Tout2[Tl\nout;Tl\nin[[\n]Tr\nin;Tr\nout], with a left margin Tl\nout= 0:2\u0003Piand a right\nmarginTr\nout= 0:4\u0003Pi, see Figure 2. This allows to con-\ntemplate eventual sudden changes in tempo expression (the\nasymmetry reﬂects the higher tendency for tempo reduc-\ntions than increases).\nConsequently, two alternative scenarios arise. A ﬁrst\nscenario corresponds to a local maximum found inside the\ninner tolerance window. In such case, the agent’s period\nand phase are compensated by a fraction of that error:\n(\nPi=Pi+ 0:25\u0003error\n\u001ei=\u001ei+Pi+ 0:25\u0003error;9m2Tin: (6)\nA second scenario considers bigger deviations, with lo-\ncal maxima in the outer tolerance window. On this con-\ndition the agent under analysis keeps its period and phase\nbut, in order to cope for potential sudden variations of tempo\nand/or timing, it generates three children fC1;C2;C3gtofollow three alternative hypotheses, considering alternative\npossible deviations of its own current hypothesis: timing\n(phase), tempo (period), or timing and tempo:\nC1:(\nP1\nC=Pi\n\u001e1\nC=\u001ei+Pi+error;9m2Tout; (7)\nC2:(\nP2\nC=Pi+error\n\u001e2\nC=\u001ei+Pi+error;9m2Tout; (8)\nC3:(\nP3\nC=Pi+ 0:5\u0003error\n\u001e3\nC=\u001ei+Pi+ 0:5\u0003error;9m2Tout:(9)\nTo keep the competitiveness, these new agents inherit a\nportion (80% in the current implementation) of their father\ncurrent score.\nUltimately, alternative possible situations may termi-\nnate an agent operation, at any analysis frame: replace-\nment, redundancy, obsolescence, or loss. An agent is killed\nif it is currently the worst agent in a pool of agents that has\nreached a maximum number (limited to 30agents), and if\nits score is lower than a newly created agent. In order to in-\ncrease the algorithm efﬁciency, an agent is killed if it is du-\nplicating the work of another agent whose score is bigger\n(their periods do not differ by more than 11:6ms and their\nphases no more than 23:2ms). An agent is also terminated\nif the difference between its score and the best agent’s is\nhigher than 80% of the best score. Finally, an agent may\nbe also killed if it seems to be “lost,” suggested by a high\nnumber (i.e. 8) of consecutive beats predictions outside its\ninner tolerance window.\n2.3.2 Agent Referee\nIn order to determine the best agent at each data frame,\na central Agent Referee keeps a running evaluation of all\nagents at all times. This is conducted by scoring the beat\npredictions of each agent with respect to its goodness-of-ﬁt\nto incoming data.\nThe following evaluation function, \u0001s, is applied around\neach beat prediction bp, which evaluates distance between\nbeat prediction and the local maximum minside either the\ninner or the outer window (see Figure 2):\n(\n\u0001s=\u0000\n1\u0000jerror j\nTr\nout\u0001\n:(Pi\nPm):SF (m);9m2Tin\n\u0001s=\u0000\u0000jerror j\nTr\nout\u0001\n:(Pi\nPm):SF (m);9m2Tout;(10)\nwherePmis the maximum admitted period, in frames. The\nPi\nPmfraction is used to normalize the score function by the\nperiod as a way to deﬂate faster tempi hypotheses, which\nwould otherwise tend to get higher scores due to a higher\nnumber of beat predictions. Note also the fact an agent\nscore can undergo positive as well as negative updates.\n2.3.3 Non-Causal Version\nWhereas causal processing retrieves the beats of the cur-\nrent best agent, at any time-frame, in the non-causal ver-\nsion only the lastbest agent is considered. For such, every\nagents keep an history of their beat predictions, attached\nto the one inherited form their relatives, and transmit it to\n293\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Figure 2. Score function around a beat prediction, bp, with\nPi= 120BPM . Example of local maxima mfound in the\nconsidered inner tolerance window Tin.\nfuture generations. Distinctively to the former, this process\ndistinguishes the family of agents whose cumulative score\nprevails for the whole piece.\nIn the non-causal version, after pre-tracking and intial\nagents setup, the analysis “jumps back in time,” and beat\ntracking is performed from the beginning of the signal.\n3. EVALUATION\nIn this section we report on performance evaluation of the\nproposed algorithm with respect to 2 tasks: tempo esti-\nmation and beat tracking. In order to ease comparison to\ncurrent state-of-the-art systems, we use current benchmark\ndatasets and evaluation measures.\n3.1 Datasets\nIBT was evaluated using two distinct datasets. For mea-\nsuring global tempo estimation performance, we use the\nISMIR 2004 Tempo Induction Contest data [11]. It con-\nsists on 3199 tempo-annotated instances, divided in three\ncategories: Ballroom, Loops, and Songs.\nFor the beat tracking evaluation, we use 1360 beat-label-\ned musical pieces (previous use of this dataset is reported\nin [9] and [6]).\n3.2 Evaluation Measures\nThe system estimation of tempo is evaluated via the two\nmetrics proposed in [11]: a1(estimations are considered\ncorrect only if they are equal to the annotated tempo) and\na2(correct estimations also include related metrical levels\nat2,3,1\n2, and1\n3of the ground-truth). Both metrics allow a\n4% tolerance window.\nBeat-tracking performances are measured via the\nP-score [13], with a 20% tolerance around median Inter-\nBeat-Interval (IBI) annotations (as in MIREX 2006 Audio\nBeat Tracking Contest and [6]).\nIn order to evaluate IBT’s robustness to noise distor-\ntions, we also applied a number of signal degradations:\ndownsampling, GSM encoding/decoding, ﬁtering, volume\nadjustment, addition of reverb and white noise (see [11] for\nmore details).3.3 Global Tempo Estimation\nTable 1 presents accuracies obtained for global tempo esti-\nmation, with regular and distorted data. The global tempo\nwas measured as the median IBI of ﬁnal beat predictions,\ni.e. after beat tracking the whole piece. We also report\naccuracies obtained before tracking, at the output of the\npre-tracking stage, where we select the period hypothesis\nwith highest rank.\nConditionBallroom Loops Songs Overall\na1 a2 a1 a2 a1 a2 a1 a2\nIBT(c) 48 83 41 73 30 73 40 76\nIBT(c) dist. 44 76 40 72 31 66 38 71\nIBT(nc) 49 90 37 76 36 82 41 83\nIBT(nc) dist. 48 82 37 74 34 74 40 77\npre-tracking 42 75 40 74 29 71 37 73\nTable 1. Global tempo estimation accuracies, in %; “(c)”\nand “(nc)” stand for the causal and non-causal versions\nof the system, repectively; “dist.” indicates distorted data\n(see 3.2).\nConditionMetrical relation to annotation (theor. max)\n1:1(100) 2:1(50) 1:2(50) 3:1(33) 1:3(33) all\nIBT(c) 74(558) 47(282) 46(201) 40(14) 27(9) 57\nIBT(nc) 81(613) 46(266 )45(238) 40(15) 24(10) 61\n1beat(c) 80(544) 49(354) 40(109) 39(13) 26(7)59\n1beat(nc) 88(618) 47(321) 42(153) 38(18) 26(6)64\n2beats(c) 79(1087 )53(20) 28(2) — (0) — (0) 72\n2beats(nc) 82(1080) 47(44) 37(13) — (0) — (0) 74\ndist.(c) 73(547 )46(247 )45(164) 39(13) 20(9) 55\ndist.(nc) 81(599) 45(255) 44(196 )37(17 )22(9) 59\ndind.(c) 72(511) 48(369) 45(128) 38(22) 23(5) 55\ndind.(nc) 81(582) 47(347 )44(168) 37(15) 29(4) 60\nBeatRoot 81(613) 48(535) 44(7)34(41) N/A 60\nBR 2beats 80(1245) 45(10) 32(4) 33(3) N/A 77\nTable 2. Beat tracking P-scores by metrical relation with\nthe ground-truth, under different conditions; “(c)” and\n“(nc)” stand for the causal and non-causal versions of\nthe system, repectively; “dist.” indicates distorted data\n(see 3.2); “dind.” stands for “dumb” induction (see text).\nThe ﬁrst line of the table indicates the metrical relation\nfound between the algorithm output and the ground-truth\nand the corresponding theoretical maximum P-Score. The\nformat of other lines is as follows: fP-Score (number of\nexcerpts tracked at each metrical level)g.\n3.4 Beat Tracking\nTable 2 provides results of beat tracking experiments un-\nder diverse conditions. The ﬁrst two lines show results of\nthe causal and non-causal system under regular conditions.\n294\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)The next four lines refer to beat tracking with biased ini-\ntialization, either giving the annotated ﬁrst beat, or giving\nthe ﬁrst two beats. This help evaluating the performance of\nbeat tracking per se, independently of the performance of\ntempo induction and phase estimation. It is also convenient\nin order to compare with BeatRoot performances [6].\nResults were grouped with respect to metrical relations\nbetween the system outputs and the ground-truth annota-\ntions. This provides useful information regarding the sys-\ntem tracking performance regardless of it having choosen\nthe “correct” metrical level. (Note that given the eval-\nuation metrics used (the P-score), the theoretical perfor-\nmance maximum is different for different metrical rela-\ntions between output and annotations.)\nAll results were generated with the same default param-\neters concerning reactiveness vs. stability of the system. In\nterms of computational time, IBT took around 11% of the\ndataset length to process it non-causally, and about 10% to\ndo it causally. (The tests were run on a Core2Duo 2.8 GHz\nWindows Vista (32-bit) machine.)\n4. PRACTICAL USE\nIBT was developed in C++ and is freely available, un-\nder GPL licensing, in MARSYAS (http://marsyas.\ninfo/). (At the date of writing, revision 3827.) The algo-\nrithm includes three main modes of operation, executable\nwith the following commands:\n$./ibt input.mp3 (causal mode (default));\n$./ibt -mic (live mode (microphone captured data));\n$./ibt -nc input.mp3 (non-causal mode);\n$./ibt -a input.mp3 (play audio w/ clicks on beats).\n4.1 Important parameters\nThe presented evaluation was run with default parameters,\nempirically chosen to conciliate reactivity and stability of\nthe system. Values of diverse parameters can be increased\nto obtain a more reactive system: the margins of tolerance\nwindows (LFT OUTTER MARGIN, RGT OUTTER MAR-\nGIN, INNER MARGIN ); portion of an agent current score\ntransmitted to its children, (CHILDREN SCORE FACTOR);\nand children correction factors (CHILDX FACTOR).\n5. DISCUSSION AND FUTURE WORK\n5.1 On tempo estimation\nTable 1 shows that the non-causal version of IBT performs\ncomparatively to the best algorithms tested in the ISMIR\n2004 contest [11]. The non-causal version shows slightly\nworse results, but still remains in the best third of the algo-\nrithms. Overall it is fair to say that the system ﬁnds either\nthe correct tempo or make (somehow acceptable) errors of\nmetrical level in 80% of the cases.\nComparable tempo estimation results are observed on\nthe second dataset (Table 2). Careful evaluation of the ﬁrst\nand second lines shows that the tempi of a total of 1064\nexcerpts and 1142 excerpts (i.e. 78% and 83%) are corrector correspond to metrical level errors, in the causal and\nnon-causal versions, respectively.\nThe last line of Table 1 also shows that tempo estima-\ntion is more reliable after tracking the whole excerpt than\nat the output of the induction stage. For instance, non-\ncausal beat tracking outperforms pre-tracking by around\n10 points. Also, tempo induction seems to work worse on\ntheLoops dataset. This is due to the fact that many of these\nexcerpts are very short (many are in fact shorter than our\ninduction window length —5s).\nIt is also interesting to notice that, after tracking the\nwhole piece, tempo estimation results obtained with “dumb”\ninduction are sensibly similar —albeit an apparent decrease\nof the number of correct metrical levels found— to those\nobtained with a more informed induction process (82% vs.\n83%, respectively, in the non-causal case, considering all\nacceptable metrical levels).\nTempo estimation is quite robust to distortions of the au-\ndio signal, although the accuracy loss is still about 5 points.\nThis is a clear advantage with respect to systems that pro-\ncess discrete lists of onsets instead of continuous features,\nsuch as BeatRoot (see [11] for a detailed comparison —\nnote that BeatRoot’s results are relative to a previous ver-\nsion of the software and that recent changes in the onset\ndetection function are likely to have improved them).\nThese ﬁndings seem to indicate that, although tempo\ninduction in IBT reaches good levels, comparable to the\nstate-of-the-art, further increase in accuracy will certainly\nbe obtained if future work is dedicated to improving the\ninduction process. Previous ﬁndings indicate that worth-\nwhile lines of work include research on the amount of data\nneeded for induction, reliability of the estimation, improved\nrobustness to noise, and the possibility to trigger induction\non different parts of the data, depending on a monitoring\nof the tracking process self-evaluation.\nWe can see on Table 1 that accuracy with a2 is much\nbetter than with a1 (36-37 points overall difference). Ta-\nble 2 also shows that, as BeatRoot, IBT tracks a signiﬁcant\nnumber of excerpts at the “wrong” metrical level. How-\never, at the difference with BeatRoot, these excerpts are\nmore uniformally distributed among lower and higher lev-\nels. This is the direct effect of the period normalization\nfactor found in the scoring function, eq. (8). These ﬁnd-\nings indicate that more work should be done on the issue of\nﬁnding the “correct” metrical level, which may be contem-\nplated by the scoring function itself. In that respect, results\nfrom [12] on a1 indicate that a promising direction lies in\nbeat tracking at several metrical levels simultaneously.\n5.2 On beat tracking\nTable 2 permits us to focus on the tracking performance\nof the system, independently of its performance in ﬁnding\nthe correct tempo. The ﬁrst two lines of the ﬁrst column\nshows us that when IBT ﬁnds the correct tempo, it tracks\nbeats correctly in 74% of the cases, the non-causal version\ndoes it slightly better: 81%. This is the same performance\nas BeatRoot. Tracking performances when IBT follows\nbeats on a different metrical level than the annotations are\n295\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)also similar to BeatRoot.\nCareful listening and visualization to tracking errors pro-\nduced by the causal vs. the non-causal system shows, as\nshould be expected, that the former is more prone to inter-\nchanges between phase and metrical levels, compromising\ncontinuity.\nWhen one correct beat is given as input to IBT, perfor-\nmance increases up to 88% in the non-causal case. This\nis a good result, although it also suggests that a number\nof errors are made at the stage of phase selection (2.2.2)\nduring pre-tracking. Here again, as argued in the previous\nsection, this suggests than future work should be dedicated\nto improving the induction phase. When two correct beats\nare given, global performance increases, although perfor-\nmance at the correct metrical level suffers a slight decrease,\ndue to the fact that this ﬁgure is computed on signiﬁcantly\nmore data (i.e. IBT ﬁnds more correct metrical levels).\nWith regards to robustness to signal distortions, it seems\nthat as with tempo estimation, the use of continuous fea-\ntures instead of discrete onsets results in higher robustness.\nHowever, IBT performance still decreases about 2 points\non average with respect to clean data, calling for future\nwork related to more robust feature extraction.\nWhen the tracking module is given “dumb” period hy-\npotheses, tracking results are only marginally lower than\nwhen the period is inferred with a more informed method.\nThis shows that the system has the desirable property to not\ndepend too heavily on correct estimation of the tempo and\nto recover from errors. Future work should be dedicated\nto evaluating the speed at which the system recovers from\nerrors, and experiments should be dedicated to ﬁne-tuning\nsystem parameters towards the best trade-off between re-\nactiveness to changes and error recovery, on one side, and\nstability on the other side.\n6. SUMMARY\nThis paper presents IBT, an agent-based tempo and beat\ntracking system that causally (and non-causally) processes\nincoming values of a continuous audio feature (e.g. on-\nset detection function). Benchmarks on causal and non-\ncausal versions reveal competitive results, under alterna-\ntive conditions. In particular, the proposed algorithm pro-\nduces equivalent beat tracking results to those of BeatRoot,\nand accurately estimates tempo at the level of state-of-the-\nart algorithms. A special care has been put on design-\ning a system usable for real-time processing, with good\nnoise robustness, and with no bias towards particular met-\nrical levels. IBT is open-source and freely available with\nMARSYAS. Promising paths for future work include: tempo\ninduction improvements; informed alternates of the induc-\ntion and tracking phases; beat tracking at several metrical\nlevels simultaneously; use of several input features.\n7. ACKNOWLEDGEMENTS\nThis work was funded by a PhD scholarship endorsed by\nFCT, with ref. SFRH/BD/43704/2008, and was supported\nby QREN’s project Palco 3.0, led by Palco Principal.8. REFERENCES\n[1] J.-J. Aucouturier. Sounds like teen spirit: Computa-\ntional insights into the grounding of everyday musical\nterms. In J. Minett and W. Wang, editors, Language,\nEvolution and the Brain, Frontiers in Linguistics Se-\nries. Taipei: Academia Sinica Press, 2009.\n[2] M. Davies and M. Plumbley. Comparing mid-level rep-\nresentations for audio based beat tracking. In Proceed-\nings of the DMRN Summer Conference, 2005.\n[3] M. Davies and M. Plumbley. Context-dependent beat\ntracking of musical audio. IEEE Transactions on Au-\ndio, Speech, and Language Processing, 15(3):1009–\n1020, 2007.\n[4] S. Dixon. Automatic extraction of tempo and beat from\nexpressive performances. Journal of New Music Re-\nsearch, 30(1):39–58, 2001.\n[5] S. Dixon. Onset detection revisited. In in Proceedings\nof the 9th International Conference on Digital Audio\nEffects, pages 133–13, Montreal, Canada, 2006.\n[6] S. Dixon. Evaluation of the audio beat tracking system\nbeatroot. Journal of New Music Research, 36(1):39–\n50, 2007.\n[7] D. P. W. Ellis. Beat tracking by dynamic programming.\nJournal of New Music Research, 36(1):51–60, 2007.\n[8] F. Gouyon and S. Dixon. A review of automatic\nrhythm description systems. Computer Music Journal,\n29(1):34–54, 2005.\n[9] F. Gouyon, S. Dixon, and G. Widmer. Evaluating low-\nlevel features for beat classiﬁcation and tracking. In\nIEEE International Conference on Acoustics, Speech,\nand Signal Processing, 2007.\n[10] F. Gouyon, P. Herrera, and P. Cano. Pulse-dependent\nanalyses of percussive music. In AES 22nd Interna-\ntional Conference on Virtual, Synthetic and Entertain-\nment Audio, 2002.\n[11] F. Gouyon, A. Klapuri, S. Dixon, M. Alonso, G. Tzane-\ntakis, C. Uhle, and P. Cano. An experimental compari-\nson of audio tempo induction algorithms. IEEE Trans-\nactions on Audio, Speech and Language Processing,\n14(5):1832–1844, 2006.\n[12] A. Klapuri, A. Eronen, and J. Astola. Analysis of\nthe meter of acoustic musical signals. IEEE Trans-\nactions on Audio, Speech, and Language Processing,\n14(1):342–355, 2006.\n[13] M. F. McKinney, D. Moelants, M. Davies, and A. Kla-\npuri. Evaluation of audio beat tracking and music\ntempo. Journal New Music Research, 36(1):1–6, 2007.\n[14] G. Peeters. Template-based estimation of time-varying\ntempo. EURASIP, Journal on Applied Signal Pro-\ncessing (Special Issue on Music Information Retrieval\nBased on Signal Processing), 36(1):51–60, 2007.\n296\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "Sparse Multi-label Linear Embedding Within Nonnegative Tensor Factorization Applied to Music Tagging.",
        "author": [
            "Yannis Panagakis",
            "Constantine Kotropoulos",
            "Gonzalo R. Arce"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1417036",
        "url": "https://doi.org/10.5281/zenodo.1417036",
        "ee": "https://zenodo.org/records/1417036/files/PanagakisKA10.pdf",
        "abstract": "A novel framework for music tagging is proposed. First, each music recording is represented by bio-inspired audi- tory temporal modulations. Then, a multilinear subspace learning algorithm based on sparse label coding is devel- oped to effectively harness the multi-label information for dimensionality reduction. The proposed algorithm is re- ferred to as Sparse Multi-label Linear Embedding Non- negative Tensor Factorization, whose convergence to a sta- tionary point is guaranteed. Finally, a recently proposed method is employed to propagate the multiple labels of training auditory temporal modulations to auditory tem- poral modulations extracted from a test music recording by means of the sparse ℓ1 reconstruction coefficients. The overall framework, that is described here, outperforms both humans and state-of-the-art computer audition systems in the music tagging task, when applied to the CAL500 dataset.",
        "zenodo_id": 1417036,
        "dblp_key": "conf/ismir/PanagakisKA10",
        "keywords": [
            "novel",
            "framework",
            "music",
            "tagging",
            "bio-inspired",
            "audi-tory",
            "temporal",
            "modulations",
            "multilinear",
            "subspace"
        ],
        "content": "SPARSE MULTI-LABEL LINEAR EMBEDDING WITHIN NONNEGATIVE\nTENSOR FACTORIZATION APPLIED TO MUSIC TAGGING\nYannis Panagakis∗Constantine Kotropoulos∗\n∗Dept. of Informatics\nAristotle University of Thessaloniki\nBox 451 Thessaloniki, GR-54124, Greece\n{panagakis,costas}@aiia.csd.auth.grGonzalo R. Arce†\n†Dept. of Electrical & Computer Engineering\nUniversity of Delaware\nNewark, DE 19716-3130, U.S.A.\narce@ece.udel.edu\nABSTRACT\nA novel framework for music tagging is proposed. First,\neach music recording is represented by bio-inspired audi-\ntory temporal modulations. Then, a multilinear subspace\nlearning algorithm based on sparse label coding is devel-\noped to effectively harness the multi-label information for\ndimensionality reduction. The proposed algorithm is re-\nferred to as Sparse Multi-label Linear Embedding Non-\nnegative Tensor Factorization , whose convergence to a sta-\ntionary point is guaranteed. Finally, a recently proposed\nmethod is employed to propagate the multiple labels of\ntraining auditory temporal modulations to auditory tem-\nporal modulations extracted from a test music recording\nby means of the sparse ℓ1reconstruction coefﬁcients. The\noverall framework, that is described here, outperforms both\nhumans and state-of-the-art computer audition systems in\nthe music tagging task, when applied to the CAL500 dataset.\n1. INTRODUCTION\nThe emergence of Web 2.0 and the success of music ori-\nented social network websites, such as last.fm, has revealed\nthe concept of music tagging. Tags are text-based labels\nthat encode semantic information related to music (i.e., in-\nstrumentation, genres, emotions, etc.). They result into a\nsemantic representation of music, which can be used as\ninput to collaborative ﬁltering systems assisting users to\nsearch for music content. However, a drawback of such\napproach is that a newly added music recording must be\ntagged manually ﬁrst, before it can be retrieved [18, 19],\nwhich is a time consuming and expensive process. There-\nfore, an emerging problem in Music Information Retrieval\n(MIR) aims to automate the process of music tagging. This\nproblem is referred to as automatic music tagging orauto-\nmatic multi-label music annotation.\nMIR has mainly focused on content-based classiﬁcation\nof music by genre [11–13] and emotion [14]. These clas-\nsiﬁcation systems effectively annotate music with class la-\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc⃝2010 International Society for Music Information Retrieval.bels, such as “rock”, “happy”, etc., by assuming a prede-\nﬁned taxonomy and an explicit mapping of a music record-\ning onto mutually exclusive classes. However, such as-\nsumptions are unrealistic and result into a number of prob-\nlems, since music perception is inherently subjective [19].\nThe latter problems can be overcome by the less restrictive\napproach of annotating the audio content with more than\none labels in order to reﬂect more aspects of music. Rel-\natively little work has been made on multi-label automatic\nmusic annotation compared to the work made on multi-\nlabel automatic image annotation (cf. [3, 20] and the ref-\nerences therein). However, various automatic music tag-\nging algorithms have been proposed [2, 6, 8, 17, 19]. For\ninstance, audio tag prediction is treated as a set of binary\nclassiﬁcation problems where standard classiﬁers, such as\nthe Support Vector Machines [17] or Ada-Boost [2] can be\napplied. Furthermore, methods that resort to probabilis-\ntic modeling have been proposed [6, 19]. These methods\nattempt to infer the correlations or joint probabilities be-\ntween the tags and the low-level acoustic features extracted\nfrom audio.\nIn this paper, the problem of automatic music tagging is\naddressed as a multi-label multi-class classiﬁcation prob-\nlem by employing a novel multilinear subspace learning\nalgorithm and sparse representations. Motivated by the\nrobustness of the auditory representations in music genre\nclassiﬁcation [11–13], each audio recording is represented\nin terms of its slow temporal modulations by a two di-\nmensional (2D) auditory representation as in [13]. Con-\nsequently, an ensemble of audio recordings is represented\nby a third-order tensor. The auditory temporal modulations\ndo not explicitly utilize the label set (i.e., the tags) of music\nrecordings. Due to the semantic gap, it is unclear how to\nexploit the semantic similarity between the label sets asso-\nciated to two music recordings for efﬁcient feature extrac-\ntion within multi-label music tagging. Motivated by the au-\ntomatic multi-label image annotation framework proposed\nin [20], the semantic similarities between two music record-\nings with overlapped labels are measured in a sparse rep-\nresentation based way rather than in one-to-one way as\nin [2, 6, 17, 19]. There is substantial evidence in the liter-\nature that the multilinear subspace learning algorithms are\nmore appropriate for reducing the dimensionality of tensor\nobjects [13, 16]. To this end, a novel multilinear subspace\nlearning algorithm is developed here to efﬁciently harness\n393\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)the multi-label\ninformation for feature extraction. In par-\nticular, the proposed method incorporates the Multi-label\nLinear Embedding (MLE) [20] into the Nonnegative Ten-\nsor Factorization (NTF) [11] by formulating an optimiza-\ntion problem, which is then solved by the Projected Gra-\ndient method [1, 9]. The proposed method is referred to\nasSparse Multi-label Linear Embedding Nonnegative Ten-\nsor Factorization (SMLENTF). The SMLENTF reduces\nthe high-dimensional feature space, where the high-order\ndata (i.e. the auditory temporal modulations) lie, into a\nlower-dimensional semantic space dominated by the label\ninformation. Features extracted by the SMLENTF form\nan overcomplete dictionary for the semantic space of mu-\nsic. If sufﬁcient training music recordings are available,\nit is possible to express any test representation of auditory\ntemporal modulations as a compact linear combination of\nthe dictionary atoms, which are semantically close. This\nrepresentation is designed to be sparse, because it involves\nonly a small fraction of the dictionary atoms and can be\ncomputed efﬁciently via ℓ1optimization. Finally, tags are\npropagated from the training atoms to a test music record-\ning with the coefﬁcients of sparse ℓ1representation.\nThe performance of the proposed automatic music tag-\nging framework is assessed by conducting experiments on\nthe CAL500 dataset [18,19]. For comparison purposes, the\nMLE [20] is also tested in this task. The reported experi-\nmental results demonstrate the superiority of the proposed\nSMLENTF over the MLE, the human performance as well\nas that of state-of-the-art computer audition systems in mu-\nsic tagging on the CAL500 dataset.\nThe paper is organized as follows. In Section 2, basic\nmultilinear algebra concepts and notations are deﬁned. In\nSection 3, the bio-inspired auditory representation derived\nby a computational auditory model is brieﬂy described.\nThe SMLENTF is introduced in Section 4. The multi-label\nannotation framework, that is based on the sparse repre-\nsentations, is detailed in Section 5. Experimental results\nare demonstrated in Section 6 and conclusions are drawn\nin Section 7.\n2. NOTATION AND MULTILINEAR ALGEBRA\nBASICS\nTensors are considered as the multidimensional equivalent\nof matrices (i.e., second-order tensors) and vectors (i.e.,\nﬁrst-order tensors) [7]. Throughout the paper, tensors are\ndenoted by boldface Euler script calligraphic letters (e.g.\nX,A), matrices are denoted by uppercase boldface letters\n(e.g.U), vectors are denoted by lowercase boldface letters\n(e.g. u), and scalars are denoted by lowercase letters (e.g.\nu). The ith row of Uis denoted as ui:while its jth column\nis denoted as u:j.\nLetZandRdenote the set of integer and real numbers,\nrespectively. A high-order real valued tensor Xof order N\nis deﬁned over the tensor space RI1×I2×:::×IN, where In∈\nZandn= 1, 2, . . . , N . Each element of Xis addressed\nbyNindices, i.e., xi1i2i3:::iN. Mode- nunfolding of tensor\nXyields the matrix X(n)∈RIn×(I 1:::In\u00001In+1:::IN). In\nthe following, the operations on tensors are expressed inmatricized form [7].\nAnN-order tensor Xhas rank-1, when it is decom-\nposed as the outer product of Nvectors u(1),u(2), . . . , u(N),\ni.e.X=u(1)◦u(2)◦. . .◦u(N). That is, each element of the\ntensor is the product of the corresponding vector elements,\nxi1i2:::iN=u(1)\ni1u(2)\ni2. . . u(N)\niNforin= 1, 2, . . . , I n. The\nrank of an arbitrary N-order tensor Xis the minimal num-\nber of rank-1 tensors that yield Xwhen linearly combined.\nNext, several products between matrices will be used, such\nas the Khatri-Rao product denoted by ⊙and the Hadamard\nproduct (i.e. element-wise product) denoted by ∗, whose\ndeﬁnitions can be found in [7] for example.\n3. AUDITORY REPRESENTATION OF\nTEMPORAL MODULATIONS\nA key step for representing music signals in a psycho-\nphysiologically consistent manner is to resort on how the\naudio is encoded in the human primary auditory cortex .\nThe primary auditory cortex is the ﬁrst stage of the cen-\ntral auditory system, where higher level mental processes\ntake place, such as perception and cognition [10]. To this\nend the auditory representation of temporal modulations\nis employed [13]. The auditory representation is a joint\nacoustic and modulation frequency representation that dis-\ncards much of the spectro-temporal details and focuses on\nthe underlying slow temporal modulations of the music\nsignal [15]. Such a representation has been proven very\nrobust in representing music signals for music genre clas-\nsiﬁcation [12,13].\nThe 2D representation of auditory temporal modula-\ntions can be obtained by modeling the path of auditory\nprocessing as detailed in [13]. The computational model\nof human auditory system consists of two basic process-\ning stages. The ﬁrst stage models the early auditory sys-\ntem. It converts the acoustic signal into an auditory repre-\nsentation, the so-called auditory spectrogram , i.e. a time-\nfrequency distribution along a tonotopic (logarithmic fre-\nquency) axis. At the second stage, the temporal modula-\ntion content of the auditory spectrogram is estimated by\nwavelets applied to each channel of the auditory spectro-\ngram. Psychophysiological evidence justiﬁes the discrete\nrater∈ {2, 4,8,16,32,64,128, 256} (Hz) in order to rep-\nresent the temporal modulation content of sound [13]. The\ncochlear model, employed in the ﬁrst stage, has 96ﬁlters\ncovering 4octaves along the tonotopic axis (i.e. 24ﬁl-\nters per octave). Accordingly, the auditory temporal mod-\nulations of a music recording are represented by a real-\nvalued nonnegative second-order tensor (i.e. a matrix) X∈\nRI1×I2\n+ , where I1=If= 96 andI2=Ir= 8. Hereafter,\nletx= vec(X) ∈RI1·I2\n+ =R768\n+denote the lexicographi-\ncally ordered vectorial representation of the auditory tem-\nporal modulations.\n4. SPARSE MULTI-LABEL LINEAR EMBEDDING\nNONNEGATIVE TENSOR FACTORIZATION\nMultilinear subspace learning algorithms are required in\norder to map the high-dimensional original tensor space\n394\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)onto a\nlower-dimensional semantic space deﬁned by the\nlabels. In conventional supervised multilinear subspace\nlearning algorithms, such as the General Tensor Discrim-\nininant Analysis [16], it is assumed that data points anno-\ntated by the same label should be close to each other in the\nfeature space, while data bearing different labels should\nbe far away. However, this assumption is not valid in a\nmulti-label task, as discussed in [20]. Accordingly, such\nsubspace learning algorithms will fail to derive a lower-\ndimensional semantic space based on multiple labels.\nLet{Xi|I\ni=1}be a set of Itraining nonnegative tensors\nXi∈RI1\n+×I2×:::×INof order N. We can represent such\na set by a (N+ 1)-order tensor Y∈RI1×I2×:::×IN×IN+1\n+\nwithIN+1=I. Furthermore, let us assume that the multi-\nlabels of the training tensor Yare represented by the matrix\nC∈RV×I\n+, where Vindicates the cardinality of the tag\nvocabulary. Obviously, cki= 1if the ith tensor is labeled\nwith the kth tag in the vocabulary and 0 otherwise. Since,\nevery tensor object (music recording here) can be labeled\nby multiple labels, there may exist more than one non-zero\nelements in a label vector (i.e. c:i).\nTo overcome the limitation of conventional multilinear\nsubspace learning algorithms, the MLE [20] is incorpo-\nrated into the NTF. To this end, two methods exploit the\nmulti-label information in order to drive semantically ori-\nented feature extraction from tensor objects. First, the ten-\nsor objects with the same label set, that is c:i=c:j, are\nconsidered to be fully semantically related and thus the\nsimilarity graph W1has elements w1\nij=w1\nji= 1 and\n0otherwise. However, in real-world datasets, data sam-\nples with exactly the same label set are rare. In such a\ncase, the semantic relationship between the data samples\ncan be inferred via the ℓ1semantic graph as proposed in\n[20]. Let us denote by W2theℓ1semantic graph. W2\ncontains the coefﬁcients that represent each label vector\nc:ias a compact linear combination of the remaining se-\nmantically related label vectors. Formally, let us deﬁne\n^Ci= [c :1|c:2|. . .|c:i−1|c:i+1|. . .|c:I]. IfV≪Ithe lin-\near combination coefﬁcients acan be obtained by seeking\nthe sparsest solution to the undetermined system of equa-\ntionsc:i=^Cia. That is, solving the following optimiza-\ntion problem:\narg min\na∥a∥ 0subject to ^Cia=c:i, (1)\nwhere ∥.∥0is the ℓ0quasi-norm returning the number of\nthe non-zero entries of a vector. Finding the solution to the\noptimization problem (1) is NP-hard due to the nature of\nthe underlying combinational optimization. In [5], it has\nbeen proved that if the solution is sparse enough, then the\nsolution of (1) is equivalent to the solution of the following\noptimization problem:\narg min\na∥a∥ 1subject to ^Cia=c:i, (2)\nwhere ∥.∥1denotes the ℓ1norm of a vector. (2) can be\nsolved in polynomial time by standard linear programming\nmethods [4].\nTheℓ1semantic graph W2is derived as follows. For\neach label vector, ^Ciis constructed and then it is normal-ized so as its column vectors have unit norm. Then, (2) is\nsolved by replacing ^Ciwith its normalized variant and the\nsparse representation vector ais obtained. Next, w2\nij=aj\nfor1≤j≤i−1;w2\nij=aj−1fori+ 1≤j≤I. Clearly,\nthe diagonal elements of W2are equal to zero.\nLetd1\nii=∑\ni̸=jw1\nijbe the diagonal elements of the\ndiagonal matrix D1. Given {Xi|I\ni=1}, one can model the\nsemantic relationships between the tensor objects by con-\nstructing the multi-label linear embedding matrix, which\nexploits W1andW2as in [20]: M=D1−W1+\f\n2(I−\nW2)T(I−W2), where β\n>0is a parameter, which ad-\njusts the contribution of the ℓ1graph in the multi-label lin-\near embedding [20]. Let {U(n)|N+1\nn=1}be the mode- nfactor\nmatrices derived by the NTF applied to Y[11]. We deﬁne\nZ(n),U(N+1)⊙. . .⊙U(n+1)⊙U(n−1)⊙. . .⊙U(1).\nOne can incorporate the semantic information of tensor ob-\njects into the NTF by minimizing the following objective\nfunction for the SMLENTF in matricized form:\nf(\nU(n)|N+1\nn=1)\n=1\n2∥Y(n)−U(n)[\nZ(n)]T∥2\nF\n+λtr{[\nU(N+1)]TM U(N+1)}\n,(3)\nwhere λ\n>0is a parameter, which controls the trade off\nbetween the goodness of ﬁt to the training data tensor Y\nand the multi-label linear embedding and ∥.∥Fdenotes the\nFrobenious norm. Consequently, we propose to minimize\n(3) subject to the nonnegative factor matrices U(n)∈RIn×k\n+ ,\nn= 1, 2, . . . , N + 1, where kis the desirable number of\nrank-1 tensors approximating Ywhen linearly combined.\nLet∇U(n)f=@f\n@U(n)be the\npartial derivative of the\nobjective function f(U(n)|N+1\nn=1)with respect to U(n). It\ncan be shown that for n= 1, 2, . . . , N we have\n∇U(n)f=U(n)[\nZ(n)]TZ(n)−Y(n)Z(n), (4)\nwhile for n=N+ 1we obtain\n∇U(N+1)f=U(N+1)[\nZ(N+1)]TZ(N+1)\n+λ MU(N+1)−Y(N+1)Z(N+1).(5)\nFollowing the strategy employed in the derivation of the\nProjected Gradient Nonnegative Matrix Factorization [9],\nwe obtain an iterative alternating algorithm for the SM-\nLENTF as follows. Given N+ 1randomly initialized non-\nnegative matrices U(n)|N+1\nn=1∈RIn×k\n+ , a stationary point\nof (3) can be found by the update rule:\nU(n)\n[t+1]= [U(n)\n[t]−n[t]∇U(n)\n[t]f]+, (6)\nwhere tdenotes the iteration index and [.]+is the pro-\njection operator, which is deﬁned element-wise as [.]+,\nmax(., 0). The projection operator ensures that U(n)\n[t+1]con-\ntains only nonnegative elements after each iteration. The\nlearning rate n[t]can be determined by the Armijo rule\nalong the projection arc [1] or more effectively by the Al-\ngorithm 4 in [9] in order to ensure the convergence of the\nalgorithm to a stationary point. The update rule (6) is ex-\necuted iteratively in an alternating fashion for n= 1, 2,\n395\n11th International Society for Music Information Retrieval Conference (ISMIR 2010). .\n. , N + 1until the global convergence criterion is met:\nN+1∑\nn=1∥∇P\nU(n)\n[t]f∥F≤ϵN+1∑\nn=1∥∇U(n)\n[t]f∥F, (7)\nwhere [∇P\nU(n)\n[t]f]ij= min(\n0,[∇U(n)\n[t]f]ij)\nif[U(n)\n[t]]ij= 0;\nand[∇P\nU(n)\n[t]f]ij= [∇U(n)\n[t]f]ijif[U(n)\n[t]]ij≥0. The pa-\nrameter ϵis a predeﬁned small positive number, typically\n10−5[9]. The convergence criterion (7) is employed in or-\nder to check the stationarity of the solution set {U(n)\n[t]|N+1\nn=1}\nsince it is equivalent to the Karush-Kuhn-Tucker optimal-\nity condition [1,9].\n5. MULTI-LABEL ANNOTATION VIA SPARSE\nREPRESENTATIONS\nIn this section, the task of automatic music tagging is ad-\ndressed by sparse representations of auditory temporal mod-\nulations projected onto a reduced dimension feature space,\nwhere the semantic relations between them are retained.\nFor each music recording a 2D auditory representation\nof temporal modulations is extracted as is brieﬂy described\nin Section 3 and detailed in [13]. Thus, each ensemble\nof recordings is represented by a third-order data tensor,\nwhich is created by stacking the second-order feature ten-\nsors associated to the recordings. Consequently, the data\ntensorY∈RI1×I2×I3\n+ , where I1=If= 96 ,I2=Ir= 8,\nandI3=Isamples is obtained. Let Ytrain ∈RI1×I2×I\n+ ,\nI < I samples , be the tensor where the training auditory\ntemporal modulations representations are stored. By ap-\nplying the SMLENTF onto the Ytrain three factor matri-\nces are derived, namely U(1),U(2),U(3), associated to\nthe frequency, rate, and samples modes of the training ten-\nsorYtrain , respectively. Next, the projection matrix P=\nU(2)⊙U(1)∈R768×k\n+ , with k≪min(768 , I), is ob-\ntained. The columns of Pspan a reduced dimension fea-\nture space, where the semantic relations between the vec-\ntorized auditory temporal modulations are retained. Con-\nsequently, by projecting all the training auditory temporal\nmodulations onto this reduced dimension space an over-\ncomplete dictionary D=PTYT\ntrain (3)∈Rk×I\n+is ob-\ntained. Alternatively, the dictionary can be obtained by\nD=P†YT\ntrain (3), where (.)†denotes the Moore-Penrose\npseudoinverse.\nGiven a vectorized representation of auditory temporal\nmodulations x∈R768\n+associated to a test music record-\ning, ﬁrst is projected onto the reduced dimension space and\na new feature vector is obtained i.e. \u0016x=PTx∈Rk\n+or\n\u0016x=P†x∈Rk. Now, \u0016xcan be represented as a compact\nlinear combination of the semantically related atoms of D.\nThat is, the test auditory representation of temporal modu-\nlations is considered semantically related to the few train-\ning auditory representations of temporal modulations with\nnon-zero approximation coefﬁcients. This implies that the\ncorresponding music recordings are semantically related,\nas well. Again, since Dis overcomplete, the sparse coef-\nﬁcient vector bcan be obtained by solving the followingoptimization problem:\narg min\nb∥b∥1subject to D b =\u0016x. (8)\nBy applying the SMLENTF, the semantic relations between\nthe label vectors are propagated to the feature space. In\nmusic tagging, the semantic relations are expected to prop-\nagate from the feature space to the label vector space. Let\nus denote by \u0016athe label vector of the test music recording.\nThen, \u0016ais obtained by\n\u0016a=C b. (9)\nThe labels with the largest values in \u0016ayield the ﬁnal tag\nvector of the test music recording.\n6. EXPERIMENTAL EVALUATION\nIn order to assess the performance of the proposed frame-\nwork in automatic music tagging, experiments were con-\nducted on the CAL500 dataset [18, 19]. The CAL500 is\na corpus of 500 tracks of Western popular music, each of\nwhich has been manually annotated by three human anno-\ntators at least, who employ a vocabulary of 174 tags. The\ntags used in CAL500 dataset annotation span six semantic\ncategories, namely instrumentation, vocal characteristics,\ngenres, emotions, acoustic quality of the song, and usage\nterms (e.g. “I would like to listen this song while driving,\nsleeping etc.”) [19]. All the recordings were converted to\nmonaural wave format at a sampling frequency of 16kHz\nand quantized with 16 bits. Moreover, the music signals\nhave been normalized, so that they have zero mean am-\nplitude with unit variance in order to remove any factors\nrelated to the recording conditions.\nFollowing the experimental set-up used in [2,6,19], 10-\nfold cross-validation was employed during the experimen-\ntal evaluation process. Thus each training set consists of\n450audio ﬁles. Accordingly, the training tensor Ytrain∈\nR96×8×450\n+ was constructed by stacking the auditory tem-\nporal modulations representations. The projection matrix\nPwas derived from the training tensor Ytrain by employ-\ning either the SMLENTF or the MLE [20]. The length of\nthe tag vector returned by our system was 10. That is, each\ntest music recording was annotated with 10 tags. Through-\nout the experiments, the value of λin SMLENTF was em-\npirically set to 0.5, while the value of βused in forming\nthe matrix Mwas set to 0.5for both the SMLENTF and\nthe MLE.\nThree metrics, the mean per-word precision and the mean\nper-word recall and the F 1score are employed in order to\nassess the annotation performance of the proposed auto-\nmatic music tagging system. Per-word recall is deﬁned as\nthe fraction of songs actually labeled with word wthat the\nsystem annotates with label w. Per-word precision is de-\nﬁned as the fraction of songs annotated by the system with\nlabel wthat are actually labeled with word w. As in [6],\nif no test music recordings are labeled with the word w,\nthen the per-word precision is undeﬁned, accordingly these\nwords are omitted during the evaluation procedure. The F1\n396\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)score is\nthe harmonic mean of precision and recall, that is\nF1= 2·precision ·recall\nprecision +recall.\nIn T\nable 1, quantitative results on automatic music tag-\nging are presented. In particular, CBA refers to the prob-\nabilistic model proposed in [6]. MixHier is Turnbull et\nal.system based on a Gaussian mixture model [19], while\nAutotag refers to Bertin-Mahieux et al. system proposed\nin [2]. Random refers to a baseline system that annotates\nsongs randomly based on tags’ empirical frequencies. Even\nthough the range of precision and recall is [0,1], the afore-\nmentioned metrics may be upper-bounded by a value less\nthan 1 if the number of tags appearing in the ground truth\nannotation is either greater or less than the number of tags\nthat are returned by the automatic music annotation sys-\ntem. Consequently, UpperBnd indicates the best possible\nperformance under each metric. Random and UpperBnd\nwere computed by Turnbull et al. [19], and give a sense\nof the actual range for each metric. Finally, Human indi-\ncates the performance of humans in assigning tags to the\nrecordings of the CAL500 dataset. All the reported per-\nformance metrics are means and standard errors (i.e. the\nsample standard deviation divided by the sample size) in-\nside parentheses computed from 10-fold cross-validation\non the CAL500 dataset.\nSystem Precision Recall\nCBA\n[6] 0.286 (0.005) 0.162 (0.004)\nMixHier [19] 0.265 (0.007) 0.158 (0.006)\nAutotag [2] 0.281 0.131\nUpperBnd [19] 0.712 (0.007) 0.375 (0.006)\nRandom [19] 0.144 (0.004) 0.064 (0.002)\nHuman [19] 0.296 (0.008) 0.145 (0.003)\nTable\n1. Mean annotation results on the CAL500 Dataset.\nIn Figure 1, the mean precision, the mean recall, and\ntheF1score is plotted as a function of the feature space\ndimensionality derived by the MLE and the SMLENTF.\nClearly, the SMLENTF outperforms the MLE for all the\ndimensions of the feature space. The best music annotation\nperformance with respect to the mean per-word precision\nand the mean per-word recall is summarized in Table 2.\nThe numbers inside parentheses are the standards errors\nestimated thanks to the 10-fold cross-validation.\nSystem Dimension (\nk)Precision Recall F1Score\nMLE [20] 150 0.346 (0.004) 0.154 (0.002) 0.2128\nSMLENTF 150 0.371 (0.003) 0.165 (0.002) 0.2291\nTable\n2. Best mean annotation results obtained by MLE\nand SMLENTF on the CAL500 Dataset.\nBy inspecting Table 1, Table 2, and Figure 1 SMLENTF\nclearly exhibits the best performance with respect to the\nper-word precision and per-word recall among the state-\nof-the-art computer audition systems that is compared to,\nno matter what the feature space dimensionality is. Fur-\nthermore, MLE outperforms the CBA, the MixHier, and\nthe Autotag system with respect to the per-word precision,\nwhile in terms of the per-word recall its performance is\ncomparable to that achieved by the MixHier. In additionboth the SMLENTF and the MLE perform better than hu-\nmans with respect to the per-word precision and the per-\nword recall in the task under study. These results make\nour framework the top performing system in music tag-\nging motivating further research. The success of the pro-\nposed system can be attributed to the fact that the seman-\ntic similarities between two music signals with overlapped\nlabels that are measured in a sparse representation-based\nway rather than in an one-to-one way as in [2,6,17,19] by\napplying the multi-label linear embedding and the sparse\nrepresentations both in the features extraction and the clas-\nsiﬁcation process.\n7. CONCLUSIONS\nIn this paper, an appealing automatic music tagging frame-\nwork has been proposed. This framework resorts to audi-\ntory temporal modulations for music representation, while\nmulti-label linear embedding as well as sparse represen-\ntations have been employed for multi-label music annota-\ntion. A multilinear subspace learning technique, the SM-\nLENTF, has been developed, which incorporates the se-\nmantic information of the auditory temporal modulations\nwith respect to the music tags into the NTF. The results re-\nported in the paper outperform humans’ performance as\nwell as any other result obtained by the state-of-the-art\ncomputer audition systems in music tagging applied to the\nCAL500 dataset.\nIn many real commercial applications, the number of\navailable tags is large. Usually most of the tags are asso-\nciated to a small number of audio recordings. Thus, it is\ndesirable the automatic music tagging systems to perform\nwell in such small sets. Future research will address the\nperformance of the proposed framework under such condi-\ntions.\n8. REFERENCES\n[1] D. P. Bertsekas: Nonlinear Programming, Athena Scientiﬁc,\nBelmont, MA, 1999.\n[2] T. Bertin-Mahieux, D. Eck, F. Maillet, and P. Lamere: “Au-\ntotagger: A Model for Predicting Social Tags from Acoustic\nFeatures on Large Music Databases,” J. New Music Research,\nVol. 37, No. 2, pp. 115-135, 2008.\n[3] G. Carneiro, A. B. Chan, P. J. Moreno, and N. Vasconcelos:\n“Supervised Learning of Semantic Classes for Image Anno-\ntation and Retrieval,” IEEE Trans. Pattern Analysis and Ma-\nchine Intelligence, Vol. 29, No. 3, pp. 394–410, 2007.\n[4] S. S. Chen, D. L. Donoho, and M. A. Saunders: “Atomic\nDecomposition by Basis Pursuit,” SIAM J. Sci. Comput. , Vol.\n20, No. 1, pp. 33–61, 1998.\n[5] D. L. Donoho, and X. Huo: “Uncertainty Principles and Ideal\nAtomic Decomposition,” IEEE Trans. Information Theory ,\nVol. 47, No. 7. pp. 2845–2862, 2001.\n[6] M. Hoffman, D. Blei, and P. Cook: “Easy as CBA: A Simple\nProbabilistic Model for Tagging Music,” Proceedings of the\n10th Int. Symp. Music Information Retrieval, Kobe, Japan,\n2009.\n[7] T. Kolda and B. W. Bader: “Tensor Decompositions and Ap-\nplications,” SIAM Review , Vol. 51, No. 3, pp. 455–500, 2009.\n397\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)50 75 100 125 15000.050.10.150.20.250.30.350.40.45\nFeature Space DimensionMean Precision\n  \nMLE\nSMLENTF\n50 75 100 125 15000.020.040.060.080.10.120.140.160.180.2\nFeature Space DimensionMean Recall\n  \nMLE\nSMLENTF(a) (b)\n50 75 100 125 1500.150.160.170.180.190.20.210.220.230.240.25\nFeature Space DimensionF1 Score\n  \nMLE\nSMLENTF\n(c)\nFigure\n1. Mean annotation results for the MLE and the SMLENTF with respect to (a) the mean precision, (b) the mean\nrecall, and (c) the F1score on the CAL500 dataset.\n[8] E. Law, K. West, M. Mandel, M. Bay, and J. S. Downie:\n“Evaluation of Algorithms Using Games: The Case of Music\nTagging,” Proceedings of 10th Int. Symp. Music Information\nRetrieval , Kobe, Japan, pp. 387–392, 2009.\n[9] C. J. Lin: “Projected Gradient Methods for Nonnegative Ma-\ntrix Factorization,” Neural Computation , Vol. 19, No. 10, pp.\n2756–2779, 2007.\n[10] R. Munkong and J. Biing-Hwang: “Auditory Perception and\nCognition,” IEEE Signal Processing Magazine, Vol. 25, No.\n3, pp. 98–117, 2008.\n[11] Y. Panagakis, C. Kotropoulos, and G. R. Arce: “Music Genre\nClassiﬁcation Using Locality Preserving Non-Negative Ten-\nsor Factorization and Sparse Representations,” Proceedings\nof 10th Int. Symp. Music Information Retrieval, Kobe, Japan,\npp. 249–254, 2009.\n[12] Y. Panagakis, C. Kotropoulos, and G. R. Arce: “Music Genre\nClassiﬁcation via Sparse Representation of Auditory Tempo-\nral Modulations,” Proceedings of EUSIPCO 2009 , Glasgow,\nScotland, 2009.\n[13] Y. Panagakis, C. Kotropoulos, and G. R. Arce: “Non-\nNegative Multilinear Principal Component Analysis of Audi-\ntory Temporal Modulations for Music Genre Classiﬁcation,”\nIEEE Trans. Audio Speech and Language Technology, Vol.\n18, No. 3, pp. 576–588, 2010.\n[14] S. Rho, B. Han, and E. Hwang: “SVR-based Music Mood\nClassiﬁcation and Context-based Music Recommendation,”\nProceedings of 17th ACM Int. Conf. Multimedia , pp. 713–\n716, Beijing, China, 2009.[15] S. Sukittanon, L. E. Atlas, and J. W Pitton: “Modulation-\nscale Analysis for Content Identiﬁcation,” IEEE Trans. Signal\nProcessing, Vol. 52, No. 10, pp. 3023–3035, 2004.\n[16] D. Tao, X. Li, X. Wu, and S. J. Maybank: “General Tensor\nDiscriminant Analysis and Gabor Features for Gait Recog-\nnition,” IEEE Trans. Pattern Analysis and Machine Intelli-\ngence, Vol. 29, No. 10, pp. 1700–1715, 2007.\n[17] K. Trohidis, G. Tsoumakas, G. Kalliris, and I. Vlahavas:\n“Multilabel Classiﬁcation of Music into Emotions,” Proceed-\nings of 9th Int. Symp. Music Information Retrieval , Philadel-\nphia, USA, pp. 325–330, 2008.\n[18] D. Turnbull, L. Barrington, D. Torres, and G. Lanckriet: “To-\nwards Musical Query-By-Semantic-Description Using the\nCAL500 Data Set,” Proceedings of 30th ACM Int. Conf. Re-\nsearch and Development in Information Retrieval, Amster-\ndam, The Netherlands, pp. 439-446, 2007.\n[19] D. Turnbull, L. Barrington, D. Torres, and G. Lanckriet: “Se-\nmantic Annotation and Retrieval of Music and Sound Ef-\nfects,” IEEE Trans. Audio Speech and Language Processing,\nVol. 16, No. 2, pp. 467–476, 2008.\n[20] C. Wang, S. Yan, L. Zhang, and H.-J. Zhang: “Multi-label\nSparse Coding for Automatic Image Annotation,” Proceed-\nings of IEEE Int. Conf. Computer Vision and Pattern Recog-\nnition, Florida, USA, pp. 1643-1650, 2009.\n398\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "Improving Markov Model Based Music Piece Structure Labelling with Acoustic Information.",
        "author": [
            "Jouni Paulus"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1416732",
        "url": "https://doi.org/10.5281/zenodo.1416732",
        "ee": "https://zenodo.org/records/1416732/files/Paulus10.pdf",
        "abstract": "This paper proposes using acoustic information in the la- belling of music piece structure descriptions. Here, mu- sic piece structure means the sectional form of the piece: temporal segmentation and grouping to parts such as cho- rus or verse. The structure analysis methods rarely pro- vide the parts with musically meaningful names. The pro- posed method labels the parts in a description. The base- line method models the sequential dependencies between musical parts with N-grams and uses them for the labelling. The acoustic model proposed in this paper is based on the assumption that the parts with the same label even in dif- ferent pieces share some acoustic properties compared to other parts in the same pieces. The proposed method uses mean and standard deviation of relative loudness in a part as the feature which is then modelled with a single multi- variate Gaussian distribution. The method is evaluated on three data sets of popular music pieces, and in all of them the inclusion of the acoustic model improves the labelling accuracy over the baseline method.",
        "zenodo_id": 1416732,
        "dblp_key": "conf/ismir/Paulus10",
        "keywords": [
            "acoustic information",
            "music piece structure",
            "temporal segmentation",
            "musically meaningful names",
            "N-grams",
            "sequential dependencies",
            "musical properties",
            "relative loudness",
            "multi- variate Gaussian distribution",
            "labelling accuracy"
        ],
        "content": "IMPROVING MARKOV MODEL-BASED MUSIC PIECE STRUCTURE\nLA\nBELLING WITH ACOUSTIC INFORMATION\nJouni Paulus\nFraunhofer Institute for Integrated Circuits IIS\nErlangen, Germany\njouni.paulus@iis.fraunhofer.de\nABSTRACT\nThis paper proposes using acoustic information in the la-\nbelling of music piece structure descriptions. Here, mu-\nsic piece structure means the sectional form of the piece:\ntemporal segmentation and grouping to parts such as cho-\nrus or verse. The structure analysis methods rarely pro-\nvide the parts with musically meaningful names. The pro-\nposed method labels the parts in a description. The base-\nline method models the sequential dependencies between\nmusical parts with N-grams and uses them for the labelling.\nThe acoustic model proposed in this paper is based on the\nassumption that the parts with the same label even in dif-\nferent pieces share some acoustic properties compared to\nother parts in the same pieces. The proposed method uses\nmean and standard deviation of relative loudness in a part\nas the feature which is then modelled with a single multi-\nvariate Gaussian distribution. The method is evaluated on\nthree data sets of popular music pieces, and in all of them\nthe inclusion of the acoustic model improves the labelling\naccuracy over the baseline method.\n1. INTRODUCTION\nThis paper proposes a method for providing musically mean-\ningful labelling to sectional parts in Western popular music\nusing two complementary statistical models. The ﬁrst one\nrelies on the sequential dependencies between the occur-\nrences of different parts, while the second models some\nacoustic properties of the them. A labelling method us-\ning the sequence model was proposed earlier by Paulus\nand Klapuri [9] and this paper proposes an extension that\nmethod by including also acoustic information.\nIn sectional form a music piece is constructed from shorter,\npossibly repeated parts . Especially many Western pop/rock\nThis work was performed when the author was at the Department of\nS\nignal Processing, Tampere University of Technology, Tampere, Finland.\nThis work was supported by the Academy of Finland, (application num-\nber 129657, Finnish Programme for Centres of Excellence in Research\n2006–2011).\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\n© 2010 International Society for Music Information Retrieval.pieces follow this form. The parts can be named accord-\ning to the musical role they have in the piece, for example,\n“intro” is in the beginning of the piece and provides an in-\ntroduction to the song and “verse” tells the main story of\nthe song. Music piece structure analysis aims to provide\na description of the sectional form of the piece based on\nthe acoustic signal. Usually the description consists of a\ntemporal segmentation of the piece to occurrences of parts,\nand of grouping of segments being occurrences of the same\npart. For a review of methods proposed for the task, see\nthe book chapter by Dannenberg and Goto [2] or the dis-\nsertation by Paulus [8]. With the exception of few meth-\nods [6,14], most structure analysis methods do not provide\nthe segment groups with musically meaningful label , in-\nstead they only provide a tagfor distinguishing the differ-\nent groups. However, if the analysis result is presented for\na user, providing also meaningful labels for the segments\nwould be valued, as noted by Boutard et al. [1].\nA method for musical part labelling given the descrip-\ntion with arbitrary tags was proposed by Paulus and Kla-\npuri [9]. It relies on the assumption that musical parts have\nsequential dependencies which are then modelled with N-\ngrams. The method searches for the labelling that max-\nimises the overall N-gram probability over the resulting\nlabel sequence. The obtained results indicate that such a\nmodel manages to capture useful information of the music\npiece structures. This paper proposes to extend that work\nby including acoustic information in the process. This is\nmotivated by the frequently encountered assumption that\nthe chorus is louder than the other parts. It should be noted\nthat this paper does not discuss the underlying problems in\ndeﬁning the structural description that have been discussed\nby Peeters and Deruty [11], but instead studies the perfor-\nmance of the proposed models in replicating the labelling\nin the manual annotations.\nThe rest of this paper is organised as follows: Sec. 2\ndescribes the labelling problem more formally, revisits the\nsequential modelling baseline method, and details the pro-\nposed acoustic modelling method. Sec. 3 describes the ex-\nperiments for evaluating the proposed method and presents\nthe obtained results. Finally Sec. 4 provides the conclu-\nsions of this paper.\n303\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)2. PROPOSED METHOD\nThi\ns section provides a more formal deﬁnition of the la-\nbelling problem, provides a short description of the base-\nline method relying only on sequence modelling, and de-\ntails the proposed acoustic modelling extension.\n2.1 Labelling Problem\nThe input to the method consists of a music piece descrip-\ntion and the acoustic signal. The description itself is a tem-\nporal segmentation of the piece and a grouping of the seg-\nments. Each of the groups is assigned with a unique tag r.\nWhen the Ktags in the description are organised into a se-\nquence based on the temporal locations of the segments, a\ntag sequence r1:K≡r1,r2,..., rKis obtained. The problem\nof label assignment is to ﬁnd an injective1mapping\nf:R→L (1)\nfrom the set Rof tags present in the description to the set\nLof musically meaningful labels. Application of the map-\nping is denoted with\nf(r)=l, (2)\nand it can be done also on sequences:\nf(r1:K)=l1:K. (3)\nSince any injective mapping is a valid mapping from tags\nto labels, the problem is to select the “best” mapping from\nall the possible choises. The earlier publication [9] pro-\nposed a statistical sequence model for the the labels lfor\nselecting the mapping producing the highest model proba-\nbility. This paper proposes to include acoustic information\nto the process of selecting the mapping function.\n2.2 Markov Model Baseline Method\nSome sectional forms are more common in music than the\nothers. An example of this was presented in [9] where it\nwas noted that almost 10% of the songs by The Beatles\nhave the form “intro”, “verse”, “verse”, “bridge”, “verse”,\n“bridge”, “verse”, “outro”. Though this cannot be directly\ngeneralised to all pieces, some sequences of parts occur\nmore frequently than others and this can be utilised in the\nlabelling.\nIn sequence modelling the prediction problem is to pro-\nvide probabilities for the possible continuations of a given\nsequence. p/parenleftbig\nsi|s1:(i−1)/parenrightbig\ndenotes the conditional probabil-\nity of sito follow the sequence s1:(i−1). Markov models\nmake the assumption that the process has a limited mem-\nory and the probabilities depend only on a limited length\nhistory. The length of the history is parametrised with N\nwhich provides a motivation for the alternative name of\nN-grams. An N-gram of length Nutilises N−1thorder\nMarkov assumption\np/parenleftbig\nsi|s1:(i−1)/parenrightbig\n=p/parenleftbig\nsi|s(i−N+1):(i−1)/parenrightbig\n. (4)\n1All tags in input sequence are mapped to a label, but each tag can be\nmapped only to one label and no two tags may be mapped to same label.Given a sequence s1:Kand the conditional N-gram proba-\nbilities the total probability of a sequence can be calculated\nwith\np(s1:K)=K\n∏\ni=1p/parenleftbig\nsi|s(i−N+1):(i−1)/parenrightbig\n. (5)\nFor more information on N-grams and language modelling,\nsee [5].\nThe baseline method proposed by Paulus and Klapuri [9]\ncalculates N-grams using the musical part labels as the al-\nphabet L, and then locates the mapping fOPTmaximising\nthe overall sequential probability of (5) while conforming\nto the injectivity constraint:\nfOPT=argmax\nf{pL(f|r1:K)},f:R→Linjective.(6)\nIn (6) pL(f|r1:K)denotes the Markov probability of the\nsequence resulting from applying the mapping f\npL(f|r1:K)=p(f(r1:K)). (7)\nThe combinatorial optimisation problem of (6) can be\nsolved, e.g., in a greedy manner by applying a variant of N-\nbest token passing algorithm proposed in [9], or by apply-\ning the Bubble token passing algorithm proposed in [10].\nBoth operate on the same basic principle of creating a di-\nrected acyclic graph from the parts and possible labellings,\nand searching a path through it. Each part in the sequence\nis associated with each possible label and these combina-\ntions form the nodes of the graph. Edges are created be-\ntween parts that are directly consecutive in the input se-\nquence. Paths through the graph represent label mappings,\nand the path with the highest probability is returned as the\nresult. Even though the search does not guarantee ﬁnd-\ning the optimal solution, in small experiments it found the\nsame solution as an exhaustive search with a fractional\ncomputational cost. Viterbi or similar more efﬁcient search\nalgorithm cannot be employed here as the mapping has to\nrespect the injectivity and the whole sequence history af-\nfects the probabilities instead of only the limited memory\nof N-grams.\n2.3 Sequence Modelling Issues\nThe number of conditional probabilities p/parenleftbig\nsi|s1:(i−1)/parenrightbig\nthat\nneed to be estimated for N-gram modelling increases rapidly\nas a function of the model order Nand the alphabet size V:\nthere are VNprobabilities that need to be estimated. Usu-\nally, the probabilities are estimated from a limited amount\nof training data, and not all probabilities can be estimated\nreliably. This problem can be partly alleviated by applying\nsmoothing to the probabilities (assigning some of the prob-\nability mass of the more frequently occurring combinations\nto the less frequent ones), or by discounting methods (esti-\nmating high-order models as combinations of lower-order\nmodels). Variable-order Markov models (VMMs) [13] at-\ntempt solving the model order problem based on the train-\ning data by setting the order independently to different sub-\nsequences. In other words, if increasing the model order\ndoes not bring more accurate information, it is not done.\n304\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)2.4 Acoustic Modelling Method\nThe\nbaseline method operates only on the sequential in-\nformation of the musical parts and has no information of\nthe actual content of them. However, if the acoustic signal\nis available, it can be utilised in the labelling. Naturally,\nthe parts of a song differ from each other in view of the\nacoustic properties. This is closely related to the deﬁnition\nof sectional form. However, the assumption made here is\nthat there exists acoustic properties that exhibit similar be-\nhaviour in large body of the pieces, e.g., it is often stated\nthat “chorus” is the most energetic, or the loudest, part in\na song. In addition to “chorus” being most energetic, very\nfew other parts can be said to have any typical acoustic\nproperty. Still, e.g., “break” or “breakdown” often has con-\nsiderably reduced instrumentation, thus it is expected that\nit exhibits a lower average loudness than the other parts.\nDespite this, the acoustic modelling is applied to all parts\neven though is might not produce meaningful information\nfor all labels.\nThe proposed acoustic modelling represents the acous-\ntic information by associating a single observation vector\nxito each of the musical parts, thus utilising a highly con-\ndensed representation. The input to the labelling now con-\nsists of the tag sequence r1:Kand acoustic observations\nx1:K, one vector xifor each part. The acoustic model con-\nsiders now the likelihoods pA(xi|l)of observing xiif the\nmusical part label is l. The overall likelihood of the map-\nping deﬁnition fin view of the acoustic observations x1:K\nis now calculated with\npA(f|r1:K,x1:K)=K\n∏\ni=1pA(xi|f(ri)). (8)\n2.5 Combined Method\nAssuming statistical independence, combining the two mod-\nels (7) and (8) in the same function produces a new likeli-\nhood function for the mapping f\np(f|r1:K,x1:K)=p(x1:K|f(r1:K))p(f(r1:K)) (9)\n=K\n∏\ni=1p(xi|f(ri))K\n∏\ni=1p/parenleftbig\nf(ri)|f(r1:(i−1))/parenrightbig\n, (10)\nwhere the ﬁrst term is from the acoustic observations and\nthe latter from the N-gram models. The labelling problem\ncan be expressed as the optimisation task\nfOPT=argmax\nf{p(f|r1:K,x1:K)},f:R→Linjective.\n(11)\nThe optimisation of (11) can be done with the same al-\ngorithm as the optimisation of the sequential model alone.\nThe only required modiﬁcation is to include the acous-\ntic observation likelihoods. It should be noted that even\nthough the problem resembles hidden Markov model de-\ncoding, the injectivity requirement violates the Markov as-\nsumption thus prohibiting the use of Viterbi decoding.\n2.6 Acoustic Features\nAs the assumption about the globally informative acous-\ntic property was related to the energy level or loudness,LOUDNESS LOUDNESS DEVIATION\nch\norus\nversebridge\nintro\npre-verseoutroc\nthemesolochorus_aa\nchorus_b\nMISC\n0.6 0.6 0.8 0.8 1.0 1.0 1.2 1.2 1.4 1.4 1.6 1.8\nFigure 1. Statistics of the features used in data set TUT-\nstructure07. The mean of all occurrences of the part is in-\ndicated with circle and the surrounding error bars illustrate\nthe standard deviation over the occurrences. Note that the\nmean loudness of “chorus” and it’s variations support the\noriginal assumption.\nthey were tested for the acoustic modelling. The energy\nis measured by calculating the root-mean-squared value of\nthe signal within the part. However, in preliminary exper-\niments it was noted that using perceived loudness instead\nproduced better results. This is presumably because the\nloudness calculation addresses also the non-linear proper-\nties of human auditory system in amplitude, frequency, and\ntemporal dimensions, the main difference being in the dy-\nnamic amplitude scale compression from representing the\ndata in logarithmic decibel scale.2The calculation is done\nusing the function ma_sone from the MA Toolbox by Pam-\npalk [7]. The loudness is calculated in 11.6 ms frames with\n50% overlap and the part loudness is approximated by the\nmean loudness of the frames within the part in question.\nIn addition to the mean loudness also standard deviation\nof the framewise loudness values over the part is used to\ndescribe the dynamics of the signal. The features are nor-\nmalised by dividing them by the mean over the piece mak-\ning the mean over the piece to be 1. An illustration of the\nfeature distributions is provided in Fig. 1.\nThe acoustic observation likelihoods pA(x|l)are mod-\nelled as a single multivariate Gaussian distribution\npA(x|l)=1/radicalbig\n(2π)D|Σ|exp/parenleftbigg\n−1\n2(x−µ)TΣ−1(x−µ)/parenrightbigg\n,\n(12)\nw\nhere Dis the feature vector dimensionality, Σandµare\nthe covariance matrix and mean vector of the estimated dis-\ntribution of the part label l.\n2The preliminary experiments included also acoustic features corre-\nsponding to the brightness (spectral centroid) and bandwidth of the sig-\nnal. The various combinations of different features were tested and based\non the results of the small-scale experiments, the set used was limit to\nloudness and it’s deviation.\n305\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)3. EV ALUATIONS\nThe\nproposed extension is evaluated with three data sets of\npopular music pieces. The ﬁrst set TUTstructure07 con-\nsists of 557 pieces from various genres, mainly from pop\nand rock, but including also pieces from metal, hip hop,\nschlager, jazz, blues, country, electronic, and rnb. The\npieces have been manually annotated at Tampere Univer-\nsity of Technology (TUT).3The second data set UPF Bea-\ntlesconsists of 174 pieces by The Beatles. The piece forms\nwere analysed by Alan W. Pollack [12], and the part time\nstamps were later added at Universitat Pompeu Fabra (UPF)\nand TUT.4The third data set RWC pop contains 100 pieces\nfrom the Real World Computing Popular Music collec-\ntion [3, 4] aiming to represent typical 1980’s and 1990’s\nchart music from Japan and USA.\n3.1 Evaluation Setup\nSince the ground truth annotations in the data sets originate\nfrom different sources, the used labels also differ. For this\nreason the evaluations are run separately for each data set.\nThe data sets contain relatively large number of unique part\nlabels (e.g., TUTstructure07 has 82 unique labels) some of\nwhich occur very rarely making the modelling more dif-\nﬁcult. To alleviate this problem only the most frequent\nlabels contributing to 90% of all part occurrences are re-\ntained, and the rest are replaced with an artiﬁcial label\n“MISC”. This reduces the number of labels considerably\n(e.g., to 13 in TUTstructure07 ). The evaluations are run\nin leave-one-out cross-validation scheme and the presented\nresults are calculated over all folds.\nThe performance is evaluated with per-label accuracy,\nwhich is the ratio of the sum of durations of correctly iden-\ntiﬁed label occurrences to the sum of durations of all oc-\ncurrence of the label, calculated over the entire data set.\nSimilarly, the total accuracy describes how much of the\nentire data set duration is labelled correctly, effectively ap-\nplying weighting to the more frequently occurring labels,\nsuch as “chorus”.\nIt should be noted that the segmentation to the input tag\nsequence r1:Kis obtained from the ground truth annota-\ntions instead of an automatic signal-based analysis method.\nThis is done to be enable evaluating the accuracy of the\nlabelling method independent of the segmentation perfor-\nmance.\nThe complementary aspects of the proposed method are\nevaluated: sequence modelling alone (effectively repro-\nducing the results from [9]), acoustic modelling alone, and\nthe two combined. The sequence modelling is attempted\nwith N-gram length of 1 to 5 (from only prior probabilities\nto utilising history of length 4), and with a variable-order\nMarkov model. The VMM method employed was decom-\nposed context tree weighting after the earlier results, and\n3A full list of pieces is available at http://www.cs.tut.fi/sgn/\narg/paulus/TUTstructure07_files.html .\n4The annotations are available at http://www.iua.upf.edu/\n%7Eperfe/annotations/sections/license.html , and including\nsome corrections at http://www.cs.tut.fi/sgn/arg/paulus/\nstructure.html#beatles_data .the implementation was from [13]. These results operate\nas the baseline on top of which the acoustic modelling is\nadded. The sequence modelling choises were done to fol-\nlow the experiments in the earlier paper, thus providing a\nclear baseline for comparing the effect of the added acous-\ntic model.\n3.2 Results\nThe evaluation results are presented in Tables 1–3, each\ntable containing the results for a different data set. The\ncolumn denoted with “N=0” provides the result for using\nonly the proposed acoustic model, while the other columns\ncontain the results of the combined modelling with differ-\nent N-gram lengths. The results of using only the sequence\nmodel are provided in parentheses.\nThe results indicate that including the acoustic informa-\ntion into the labelling model improves the result in some\ncases. In all data sets the best overall result is obtained by\nincluding the acoustic information, though the improve-\nment in UPF Beatles is so small that it may not be sta-\ntistically signiﬁcant.5The same relatively small obtained\nimprovement is observed in the results for individual labels\ninUPF Beatles . This may be because the pieces are from\na single band mainly from the 1960’s and thus may not ex-\nhibit all the stereotypical properties found in more modern\npop music, as noted also by Peeters [11]. The improvement\ninTUTstructure07 is slightly larger. It is assumed that the\nlower impact of the acoustic model is partly caused by the\nlarge variety of musical styles present in the data, thus the\nmodelling assumption may not hold in all cases. The im-\nprovement due to the inclusion of the acoustic model is\nmost prominent with the RWC pop data which represents\nmore typical chart music.\n4. CONCLUSIONS\nThis paper has presented a method for assigning musically\nmeaningful labels music piece structure descriptions. The\nbaseline method utilises the sequential dependencies be-\ntween musical parts. This paper proposes a simple acoustic\nmodel for the labelling and combines it with the sequential\nmodelling method. The proposed method is evaluated on\nthree data sets of real popular music. The obtained results\nsupport the original assumption that musical parts differ in\ntheir loudness, and the acoustic information alone can be\nused to some extent to label the parts. The acoustic in-\nformation alone has the labelling performance in par with\nusing only part occurrence priors. Combining the acoustic\nmodel with the baseline sequential model provides in most\ncases a improvement in the accuracy. However, the im-\nprovement cannot be obtained with all data, because typi-\ncal loudness relations between different parts seem to de-\npend on the musical genre. Finally, the same search al-\ngorithm as with the baseline method can be used for the\ncombined model with very small modiﬁcations.\n5Asthe entire data set forms one instance in the evaluation measure\ncalculation, no statistical measure could be calculated for proper compar-\nison.\n306\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)N=0 N=1 N=2 N=3 N=4 N=5 VMM\na 0.0 0.0 (0.0) 0.8 (0.0) 22.2 ( 34. 9) 23.8 (31.7) 27.8 (27.0) 24.6 (29.4)\nbridge 18.6 25.8 (17.9) 47.4 (38.6) 51.1 (45.4) 50. 2(47.4) 49.1 (43.9) 47.7 (41.4)\nc 3.3 13.5 (3.6) 41.6 (38.3) 44.6 (42.1) 47.4 (47.7) 56. 2(54.8) 49.6 (48.5)\nchorus 29.5 75.5 (67.9) 83.4 (76.3) 85. 0(80.6) 82.7 (76.6) 79.8 (75.3) 82.4 (77.9)\nchorus_a 11.9 0.0 (0.0) 0.0 (0.0) 8.2 (7.5) 15. 7(15.7) 15.7 (11.2) 0.0 (3.0)\nchorus_b 4.4 0.0 (0.0) 0.9 (0.9) 8.8 (5.3) 12. 4(12.4) 12.4 (7.1) 0.9 (2.7)\nintro 32.7 43.4 (22.7) 97.2 (97.6) 97.6 (98.2) 97.0 (97.8) 98. 2(97.8) 97.0 (96.8)\noutro 52.4 47.4 (9.9) 98.3 (98.3) 98. 6(98.6) 98.3 (97.6) 95.9 (90.9) 98.1 (98.3)\npre-verse 30.1 10.0 (3.7) 51.4 (40.5) 55. 6(45.6) 50.7 (43.3) 46.8 (41.7) 52.5 (42.6)\nsolo 23.8 2.2 (0.0) 6.6 (4.4) 6.6 (7.2) 13.8 (15.5) 23. 2(18.8) 21.0 (16.0)\ntheme 5.5 0.0 (0.0) 2.7 (0.0) 2.7 (2.7) 2.7 (4.4) 6. 6(3.3) 3.3 (0.5)\nverse 46.5 59.0 (38.4) 72.5 (62.6) 71.0 (64.6) 70.6 (64.5) 72.1 (64.7) 74. 5(65.4)\nMISC 7.8 21.4 (11.7) 35.5 (29.2) 44. 4(38.6) 42.8 (37.9) 41.7 (40.6) 40.3 (37.3)\ntotal 27.7 42.1 (29.6) 61.7 (55.6) 64. 3(60.2) 63.6 (59.9) 63.6 (59.3) 63.7 (59.2)\nTable 1. Per-label accuracy (%) on TUTstructure07 obtained using only acoustic modelling (N=0 column), only sequence\nmodelling (values in parentheses), and combining sequence and acoustic modelling (other values).\nN=0 N=1 N=2 N=3 N=4 N=5 VMM\nbridge 6.2 22.0 (24.3) 48.0 (45.8) 77. 4(76.8) 75.1 (75.1) 70.1 (74.0) 69.5 (69.5)\nintro 43.8 50.0 (41.4) 92.6 (92.0) 93.2 (92.6) 93. 8(93.8) 93.8 (93.8) 93.2 (93.2)\noutro 73.2 60.6 (0.0) 99. 3(99.3) 99.3 (99.3) 98.6 (97.9) 97.9 (93.7) 99.3 (99.3)\nrefrain 20.1 30.1 (28.1) 43.8 (45.4) 61.8 (62.2) 69.1 (69.9) 65.1 (67.5) 69.1 ( 70.3)\nverse 37.2 73.4 (70.6) 80.9 (81.5) 88. 5(87.9) 86.5 (85.3) 83.7 (84.5) 87.1 (87.5)\nverses 23.2 0.0 (0.0) 8.9 (8.9) 51.8 ( 53. 6) 37.5 (37.5) 44.6 (44.6) 42.9 (42.9)\nversea 39.2 0.0 (0.0) 2.0 (2.0) 7.8 (7.8) 23.5 (19.6) 25.5 (19.6) 11.8 (11.8)\nM\nISC 5.7 3.8 (4.5) 17.8 (17.2) 28.7 (29.3) 43. 9(37.6) 26.1 (25.5) 30.6 (29.9)\ntotal 31.1 43.7 (36.1) 61.8 (61.8) 73.8 (73.7) 75. 7(74.6) 71.9 (72.4) 73.6 (73.9)\nTable 2. Per-label accuracy (%) on UPF Beatles obtained using only acoustic modelling (N=0 column), only sequence\nmodelling (values in parentheses), and combining sequence and acoustic modelling (other values).\nN=0 N=1 N=2 N=3 N=4 N=5 VMM\nbridge a 20.1 20.1 (8.2) 72.3 (62.9) 73. 6(66.7) 64.8 (66.0) 59.7 (49.7) 71.7 (62.9)\nchorus a 51.2 70.9 (45.6) 85.3 (76.2) 85.6 (77.6) 80.6 (73.5) 73.5 (71.5) 86. 2(79.7)\nchorus b 28.0 37.5 (6.5) 79.2 (73.2) 79. 8(71.4) 72.6 (65.5) 72.0 (71.4 76.2 (72.0)\nending 80.6 84.7 (32.7) 100(100 ) 99.0 ( 100) 98.0 (94.9) 99.0 (88.8) 100(99.0)\nintro 50.0 45.1 (10.8) 100(100 ) 100(100) 100(100) 100(100) 100(100)\npre-chorus 7.6 7.6 (3.3) 64. 1(51.1) 60.9 (52.2) 63.0 (39.1) 48.9 (42.4) 63.0 (45.7)\nverse a 35.0 48.1 (20.3) 85. 7(76.8) 84.4 (78.9) 81.4 (78.1) 77.6 (73.4) 81.0 (76.4)\nverse b 19.4 30.3 (17.4) 85. 6(76.6) 84.1 (80.1) 79.6 (74.6) 73.6 (69.2) 82.1 (76.6)\nverse c 41.9 14.0 (0.0) 60. 5(30.2) 55.8 (39.5) 47.7 (30.2) 32.6 (30.2) 47.7 (33.7)\nMISC 29.8 52.4 (8.4) 84. 9(67.6) 80.4 (73.8) 77.8 (68.4) 69.8 (67.6) 83.1 (74.7)\ntotal 36.0 45.5 (19.1) 82. 8(72.8) 81.7 (75.3) 77.5 (70.9) 71.8 (68.0) 80.7 (74.1)\nTable 3. Per-label accuracy (%) on RWC pop obtained using only acoustic modelling (N=0 column), only sequence\nmodelling (values in parentheses), and combining sequence and acoustic modelling (other values).\n307\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)5. REFERENCES\n[1]\nGuillaume Boutard, Samuel Goldszmidt, and Geoffroy\nPeeters. Browsing inside a music track, the experimen-\ntation case study. In Proc. of 1st Workshop on Learning\nthe Semantics of Audio Signals , pages 87–94, Athens,\nGreece, December 2006.\n[2] Roger B. Dannenberg and Masataka Goto. Music\nstructure analysis from acoustic signals. In David\nHavelock, Sonoko Kuwano, and Michael V orländer,\neditors, Handbook of Signal Processing in Acoustics ,\nvolume 1, pages 305–331. Springer, New York, N.Y .,\nUSA, 2008.\n[3] Masataka Goto. AIST annotation for the RWC music\ndatabase. In Proc. of 7th International Conference on\nMusic Information Retrieval , pages 359–360, Victoria,\nB.C., Canada, October 2006.\n[4] Masataka Goto, Hiroki Hashiguchi, Takuichi\nNishimura, and Ryuichi Oka. RWC music database:\nPopular, classical, and jazz music databases. In Proc.\nof 3rd International Conference on Music Information\nRetrieval , pages 287–288, Paris, France, October\n2002.\n[5] Daniel Jurafsky and James H. Martin. Speech and lan-\nguage processing. Prentice-Hall, Upper Saddle River,\nN.J., USA, 2000.\n[6] Namunu C. Maddage. Automatic structure detection\nfor popular music. IEEE Multimedia, 13(1):65–77,\nJanuary 2006.\n[7] Elias Pampalk. A Matlab toolbox to compute mu-\nsic similarity from audio. In Proc. of 5th Interna-\ntional Conference on Music Information Retrieval ,\nBarcelona, Spain, October 2004.\n[8] Jouni Paulus. Signal Processing Methods for Drum\nTranscription and Music Structure Analysis . PhD the-\nsis, Tampere University of Technology, Tampere, Fin-\nland, December 2009.\n[9] Jouni Paulus and Anssi Klapuri. Labelling the struc-\ntural parts of a music piece with Markov models. In\nSølvi Ystad, Richard Kronland-Martinet, and Kristof-\nfer Jensen, editors, Computer Music Modeling and Re-\ntrieval: Genesis of Meaning in Sound and Music - 5th\nInternational Symposium, CMMR 2008 Copenhagen,\nDenmark, May 19-23, 2008, Revised Papers, volume\n5493 of Lecture Notes in Computer Science, pages\n166–176. Springer Berlin / Heidelberg, 2009.\n[10] Jouni Paulus and Anssi Klapuri. Music structure\nanalysis using a probabilistic ﬁtness measure and a\ngreedy search algorithm. IEEE Transactions on Audio,\nSpeech, and Language Processing, 17(6):1159–1170,\nAugust 2009.\n[11] Geoffroy Peeters and Emmanuel Deruty. Is music\nstructure annotation multi-dimensional? A proposalfor robust local music annotation. In Proc. of 3rd Work-\nshop on Learning the Semantics of Audio Signals ,\npages 75–90, Graz, Austria, December 2009.\n[12] Alan W. Pollack. ’Notes on...’ series.\nThe Ofﬁcial rec.music.beatles Home Page\n(http://www.recmusicbeatles.com), 1989–2001.\n[13] Dana Ron, Yoram Singer, and Naftali Tishby. The\npower of amnesia: Learning probabilistic automata\nwith variable memory length. Machine Learning,\n25(2–3):117–149, 1996.\n[14] Yu Shiu, Hong Jeong, and C.-C. Jay Kuo. Musical\nstructure analysis using similarity matrix and dynamic\nprogramming. In Proc. of SPIE Vol. 6015 - Multimedia\nSystems and Applications VIII , pages 398–409, 2005.\n308\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "State of the Art Report: Audio-Based Music Structure Analysis.",
        "author": [
            "Jouni Paulus",
            "Meinard Müller",
            "Anssi Klapuri"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.13926925",
        "url": "https://doi.org/10.5281/zenodo.13926925",
        "ee": "https://zenodo.org/records/13926925/files/POLISH SCIENCE JOURNAL 47 (october).pdf",
        "abstract": "Abstract: In this article, information is given about Yunus Rajabi's life path, his role in Uzbek music, and his work as a composer. Opinions were also expressed about the creative works created by Yunus Rajabi and their content and role in our national music today.\nKey words: hafiz, playing instruments, music school, creation, work, melody, song\n",
        "zenodo_id": 13926925,
        "dblp_key": "conf/ismir/PaulusMK10",
        "keywords": [
            "Yunus Rajabis life path",
            "Uzbek music",
            "composer",
            "creative works",
            "national music",
            "melody",
            "song",
            "music school",
            "playing instruments",
            "Opinions"
        ],
        "content": "POLISH  SCIENCE  JOURNAL  \n \n \n \n \nISSUE 2(47)  \nOCTOBER \n \n \n \n \nINTERNATIONAL  SCIENCE  JOURNAL  \n \n \n \n \n \n \n \n \n \n \n \nWARSAW,  POLAND  \nWydawnictwo  Naukowe  \"iScience\"  \n2024 \n  «POLISH  SCIENCE  JOURNAL»    \n \n \nISBN  978-83-949403 -4-8 \n \n \nPOLISH  SCIENCE  JOURNAL  (ISSUE  2(47),  2024) - Warsaw: Sp. z o. o.  \"iScience\",  2024. – \n143 p. \n \n \nEditorial  board:  \nBakhtiyor  Akhtamovich  Аmonov , Doctor  of Political  Sciences,  Professor  of the National  University  of \nUzbekistan  \nMukhayokhon  Botiraliyevna  Artikova , Doctor  of Science,  Namangan  state  university  \nBugajewski  K. A., doktor  nauk  medycznych,  profesor  nadzwyczajny  Czarnomorski  Państwowy  Uniwersytet  \nimienia  Piotra  Mohyły  \nTahirjon Z.  Sultanov , Doctor  of Technical  Sciences,  docent  \nShavkat  J. Imomov , Doctor  of Technical  Sciences,  professor  \nBaxitjan  Uzakbaevich  Aytjanov , Doctor  of Agricultural  Sciences,  Senior  Scientific  Researcher,  Karakalpak  \nInstitute of  Agriculture  and Agrotechnology  \nYesbosi'n  Polatovich  Sadi'kov , Doctor  of Philosophy  (Ph.D),  Nukus  branch  Tashkent  state  agrarian  \nuniversity  \nNazmiya  Muslihiddinovna  Mukhitdinova , Doctor  of Philology,  Samarkand  State  University,  Uzbekistan  \nSayipzhan Bakizhanovich Tilabaev , Candidate of Historical Sciences, Associate Professor. Tashkent State  \nPedagogical  University  named  after  Nizami  \nTemirbek Ametov , PhD  \nMarina Berdina , PhD  \nHurshida  Ustadjalilova , PhD \nDilnoza Kamalova , PhD (arch) Associate Professor, Samarkand State Institute of Architecture and Civil  \nEngineering  \nSarvinoz  Boboqulovna  Juraeva , Associate  Professor  of Philological  Science,  head  of chair  of culturology  \nof Khujand  State  University  named  after  academician  B. Gafurov  (Tajikistan)  \nOleh  Vodianyi , PhD \n \n \nLanguages of publication : українська, русский, english, polski, беларуская, казақша, o’zbek,  \nlimba  română, кыргыз тили,  Հայերեն  \n \nScience journal are recomanded for scientits and teachers in higher education esteblishments.  \nThey  can be used  in education,  including  the process  of post  - graduate  teaching,  preparation  \nfor obtain  bachelors' and masters' degrees.  \nThe review of all articles was accomplished by experts, materials are according to authors  \ncopyright.  The authors  are responsible  for content,  researches  results  and errors.  \n \n \nISBN  978-83-949403 -4-8 \n \n© Sp. z o. o. \"iScience\", 202 4 \n© Authors,  2024   «POLISH  SCIENCE  JOURNAL»    \n \n \nSCIENCECENTRUM.PL  ISSUE  2(47)  ISBN  978-83-949403 -4-8 \n \nTABLE  OF CONTENTS  \n \nSECTION:  ART STUDIES   \nAbdulloyev Ma ’mur   (Tashkent,  Uzbekistan)  \nYUNUS RAJABI IS A FIGURE OF UZBEK MUSIC ………………… ………… ………………..  \n8 \n  \nSECTION:  BIOLOGY  SCIENCE   \nBuronova Fotima, Juraboyev Madadbek (Samarqand, Uzbekistan)  \nLIPIDLARNING UMUMIY TASNIFI HAMDA LIPIDLAR ALMASHINUVINING  \nBUZILISHI  NATIJASIDA  KELIB  CHIQADIGAN  KASALLIKLAR  ..................................   \n \n10 \nSECTION:  ECONOMICS   \nJarlykassymova  A. S., Duisebaeva  N. R., Mussagulova A.  K. \n(Almaty,  Kazakhstan)  \nPHENOMENOLOGY  OF NATIONAL  ECONOMIC  SECURITY  \nOF THE STATE  IN THE PROCESS  OF GLOBALIZATION............................................   \n \n \n15 \nТагиева Гюльнара, Багирова Тарана, Османова Гюнель,  \nБайрамова  Земфира,  Исмаилова  Тарана,  Бабаева  Фидан  \n(Гянджа, Азербайджан)  \nАНАЛИЗ  ФИНАНСИРОВАНИЯ  И СОВЕРШЕНСТВОВАНИЯ  АГРАРНОЙ  ОТРАСЛИ   \n \n \n21 \nSECTION: INFORMATION AND COMMUNICATION  \nTECHNOLOGIES   \nRo`ziyeva  Dilafro`z  Muzaffarovna,  Sayidova  Malika  Hamidovna  \n(Buxoro,  O`zbekiston)  \nINFORMATIKA  FANI  HAMDA  DASTURLASHDA  MATEMATIKANING  AHAMIYATI   \n \n27 \nSECTION:  MEDICAL  SCIENCE   \nАбдурашидов Мирзоулугбек Мамашокирович,  \nХудойбердиева  Хамрохон  Тилаволдиевна,  \nКосимов  Дилмурот  Сирожитдинович  (Андижан,  Узбекистан)  \nСОСТОЯНИЕ  ВЕГЕТАТИВНОГО  ГОМЕОСТАЗА  \nУ ДЕТЕЙ  С ЮВЕНИЛЬНЫМ  ХРОНИЧЕСКИМ  АРТРИТОМ  ...................................   \n \n \n \n30 \nБугаевский  К. А. (Николаев,  Украина)  \nТРАНСПЛАНТОЛОГИЯ  В ОТРАЖЕНИИ  СРЕДСТВ  КОЛЛЕКЦИОНИРОВАНИЯ  ...  \n35 \nГофуров Жавлонбек Абдувахобович,  \nХудойбердиева  Хамрохон  Тилаволдиевна,  \nХашимова  Замира  Махмуджановна  (Андижан,  Узбекистан)  \nОЦЕНКА  АНТРОПОМЕТРИЧЕСКИХ  ПОКАЗАТЕЛЕЙ  \nФИЗИЧЕСКОГО  РАЗВИТИЯ СТУДЕНТОВ  ...............................................................   \n \n \n \n47   «POLISH  SCIENCE  JOURNAL»    \n \nSCIENCECENTRUM.PL  ISSUE  2(47)  ISBN  978-83-949 \n \nSECTION:  PEDAGOGY  403-4-8 \nHojiyeva  Nargiza  Mardonovna,  Ne’matova  Gulmira  Sayfullayevna  \n(Buxoro,  O`zbekiston)  \nLOY VA PLASTILINDAN  O`YINCHOQLAR  YASASHNING  TARBIYAVIY  AHAMIYATI   \n \n51 \nKamolova Azima, Qosimova Sarvara Bahodirovna (Buxoro, O`zbekiston)  \nLOY VA PLASTILINDAN O`YINCHOQLAR YASASH ORQALI BOLALAR  \nIJODKORLIGINI  RIVOJLANTIRISH  ............................................................................ ..  \n \n54 \nShovqiyeva  Shohida  Bobosher  qizi (Tashkent,  O`zbekiston)  \nMAKTABGACHA  YOSHDAGI  BOLALARNI  KITOBXONLIKKA  MEHR  \nUYG`OTISHNING  O`ZIGA  XOS XUSUSIYATLARI  .......................................................   \n 56 \nNizomova Indira Akramovna, Xakimova Dilnoza G`aniyevna  \n(Buxoro,  O`zbekiston)  \nMAKTABGACHA  YOSHDAGI  BOLALARDA  KITOBXONLIKKA  MEHR  \nUYG`OTISHNING  YO`LLARI  .......................................................................................   \n \n \n59 \nQo’chqarov  Sherzod,  Qo’chqarov  Bekzod,  Nurjonova  Gulnoza  \n(Urganch,  O’zbekiston)  \nSURUNKALI  SOMATIK  KASALLIKLARDA  O’SMIRLAR  XAYOT  SIFATINING  \nPSIXOLOGIK  OMILLARI  ............................................................................................   \n \n \n61 \nQurbonova Shafoat Xayriddinovna, Yuldashova Nasiba Nusratovna  \n(Buxoro,  O`zbekiston)  \nMAKTABGACHA  TARBIYA  YOSHIDAGI  BOLALAR  \nTARBIYASIDA  BOLALAR  ADABIYOTINING  O`RNI  ....................................................   \n \n \n65 \nRahimova  Shoira  Adilbekovna  (Urganch,  O’zbekiston)  \nBOSHLANG’ICH SINF O’QUVCHILARINING MATEMATIK QOBILIYATLARINI  \nOSHIRISHDA  MATEMATIK  ERTAKLARNING  O’RNI  .................................................   \n \n68 \nSadridinova  Umida  To'xtaboyevna,  Qilicheva  Feruza  Baxranovna  \n(Buxoro,  O`zbekiston)  \nMAKTABGACHA TARBIYA YOSHIDAGI BOLALARDA MILLIY QADRIYATLAR  \nVOSITASIDA  XULQ  MADANIYATINI  SHAKLLANTIRISH...........................................   \n \n \n72 \nSultonova Marg`uba Olimovna, Nazarova Saodat Ibrohimovna  \n(Buxoro,  O`zbekiston)  \nMAKTABGACHA  TA’LIM  MUASSASALARIDA  BOLALARNI  \nTABIAT  BILAN  TANISHTIRISH...................................................................................   \n \n \n75 \nБошманова  Шахноза  Абдуҳаким  қизи , Абдуллаева  Феруза  Абдулла  қизи  \n(Джизак,  Узбекистан)  \nОРГАНИЗАЦИЯ  РАБОТЫ  ПО ФОРМИРОВАНИЮ  \nЧИТАТЕЛЬСКОЙ  КУЛЬТУРЫ  У УЧАЩИХСЯ  ..........................................................   \n \n \n78 \nSECTION:  PHILOLOGY  AND  LINGUISTICS   \nOchilova  Noila,  Hojiyeva  Zumrad,  Shodiyeva  Lutfiya,  Ridvanova  Hulkar  \n(Samarkand,  Uzbekistan)  \nTHE METHODOLOGICAL VIEW  TO PRAGMATICS  AND  SEMANTICS  .....................   \n \n81   «POLISH  SCIENCE  JOURNAL»    \n \nSCIENCECENTRUM.PL  ISSUE  2(47)  ISBN  978-83-949 \n \nSaydaliyeva  Anora  Baxtiyor  qizi (Tashkent,  Uzbekistan)  \nTHE MYTHOLOGICAL  BASES  OF THE IMAGE  OF PARY  IN TURK  FOLKLORE  ........  403-4-8 \n \n \n84 \nTemirova  Maftuna  Allayor qizi  (Samarkand,  Uzbekistan)  \nIMPLEMENTING  AUTHENTIC  MATERIALS IN ENGLISH  TEACHING.......................   \n88 \nTuropova  Oydin  Uktam  kizi, Salieva  Zarrina  Ilkhomovna  \n(Samarkand,  Uzbekistan)  \nLEXICO -SEMANTIC CHARACTERISTICS OF ABREVIATIONS IN MODERN ENGLISH  \nNEWSPAPER  DISCOURSE  AND  TRANSLATION  STRATEGIES..................................   \n \n \n91 \nShoabdullayeva Zulfiya  (Tashkent ,Uzbekistan ) \n      A LOOK AT MUKIMI'S WORK IN OUR LIFE .............................. ...........................   \n   97 \n \nSECTION:  PHILOSOPHY   \nAtavullayev  MirkomilAhmadovich,  Ro’ziyeva  Go’zal  Ro’zimurotovna,  \nO’ktamova Dilfuza O’ktamovna, Karimberdiyeva Nodira Ilhom qizi  \n(Tashkent,  Uzbekistan)  \nIJTIMOIY  HAYOT  SOHALARINING  G‘OYAVIY -MAFKURAVIY  JIHATLARI.................   \n \n \n101 \nSECTION:  PHYSICAL  CULTURE   \nShukurov  Bunyod  Shuhrat  o‘g‘li  \n(Tashkent,  O’zbekiston)  \nTHE OWNER  OF THE HEART  WHO  ROBBED  THE PROPERTY  OF THE SO................   \n \n106 \nSherov  Zokir,  Qarriyev  Bekzod  (Urganch,  O’zbekiston)  \nG’OVLAR  OSHA  YUGURUVCHILADA  YILLIK  TAYYORGARLIK  \nMASHG’ULOTLARINI  REJALASHTIRISH  ................................................................... ..  \n \n109 \nXudayberganov  Otabek,  Davletov  Jaxonali  (Urganch,  O’zbekiston)  \n8 – 9 YOSHLI  GIMNASTIKACHILARNING  JISMONIY SIFATLARINI  \nRIVOJLANTIRISHNING  O’ZIGA  XOS JIHATLARI  .......................................................   \n \n112 \nJahongir Shukur  (Tashkent,  O‘zbekiston)  \nMUSTAQILLIK YILLARIDA MILLIY QO‘SHIQCHILIK SAN’ATIGA OID BAG‘ISHLANGAN  \nTADQIQOTLAR: SHE’R VA MILLIY IJOD .....................................................................   \n 116 \nSECTION:  PHYSICS  AND  MATHEMATICS   \nGuleva Dilrabo Shamsiyevna, Jumayeva Farida Mizrobovna  \n(Buxoro,  O`zbekiston)  \nMATEMATIKA  DARSLARIDA  O`QUVCHILARNI  MASALA  YECHISHGA  O`RGATISH   \n \n121 \nSaparov  Zaripbay  Begdullaevich,  Kalbaev  Sultanbek  Nazerbaevich,  \nIbragimov  Aybek  Uzaqbaevich  (Nukus,  Uzbekistan)  \nFUNKSIYALIK  TENGLAMALARNI  YECHISHDA  FUNKSIYANING  \nXOSSALARIDAN  FOYDALANISH...............................................................................   \n \n \n126   «POLISH  SCIENCE  JOURNAL»    \n \nSCIENCECENTRUM.PL  ISSUE  2(47)  ISBN  978-83-949 \n \nБекмуродов  Беҳзод  Бахтиёр  ўғли,  Қаюмов  Яшнар  Зокир  Ўғли  \n(Ташкент,  Узбекистан)  \nГАРМОНИК  ФУНКЦИЯНИНГ  ХОССАЛАРИ  ...........................................................  403-4-8 \n \n \n \n129 \nSECTION:  TECHNICAL  SCIENCE.  TRANSPORT   \nБоллиев  К., Хошимов А.  (Карши,  Узбекистан)  \nПРОЦЕССЫ  ПОЛУЧЕНИЯ  КАЧЕСТВЕННОЙ  СТРУКТУРЫ  СПЛАВА  \nДЛЯ  ИЗГОТОВЛЕНИЯ  ДЕТАЛЕЙ  ФОРМОВЫХ  КОМПЛЕКТОВ  \nСТЕКЛОФОРМУЮЩЕЙ  МАШИНЫ  ........................................................................   \n \n \n132 \nSECTION:  TOURISM  AND  RECREATION   \nТилабаев  С., Муминов  А., Ли  Е. (Ташкент,  Узбекистан)  \nИСТОРИЯ  РАЗВИТИЯ  ЭКОЛОГИЧЕСКОГО  ТУРИЗМА  В УЗБЕКИСТАНЕ  .............   \n138   «POLISH  SCIENCE  JOURNAL»   \n8  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \nSECTION:  ART  STUDIES  \n \nYUNUS RAJABI IS A FIGURE OF UZBEK MUSIC  \nAbdulloyev Ma'mur  \nTutor of the State Institute of Art and Culture of Uzbekistan  \n \nAbstract:  In this article, information is given about Yunus Rajabi's life path, his role in Uzbek \nmusic, and his work as a composer. Opinions were also expressed about the creative works created \nby Yunus Rajabi and their content and role in our national music today . \nKey words:  hafiz, playing instruments, music school, creation, work, melody, song  \n \nYunus Rajabi took a place among the musicians who achieved high skills in both hafiz and \nplaying instruments. He achieved this as a result of perfecting the art of playing Uzbek national musical \ninstruments dutor and flute from a very young age. Along with  musical instruments, Yunus Rajabi also \nmastered the art of singing.  \nIn 1918, Yunus Rajabi headed the music club established at the \"Namuna\" school in \nTashkent. He gathered dozens of young music lovers around him, taught them folk songs, light folk \ninstrument tunes, songs, and rubai, which he learned from his teachers, and learned carpentry at a \nvocational school in order to gain another profession. , and then studied at the teacher training course \nin this school for three months. Y.Rajabiy made a great contribution to the formation of Uzbek musical \ndrama, at first together with Sh.Shoumarov, he performed such plays as \"Farhod and Shirin\", \"Layli \nand Majnun\", \"Rustam\", \"Avaz\", \"Kholishkhan\". composed melodies based on folk songs. Then Chalik \nin collaboration with B. Nadejden \"Revenge\", N. Mironov with \"Kochkor Turdiyev\", G. M ushel with \n\"Mukanna\", O. Halimov with \"Nadira\", B. Zeidman with B. Zeidman \"Ogillant Uy\" , made musical \ndramas like \"Navoi Astrobodda\" with Saif Jalil. In 1958, Y. Rajabi participated in the writing of the \nopera \"Zaynab and Amon\" together with B. Zeidman, D. Zokirov and T. Sadikov. [1, p. 87]  \n In 1935, he began to record folk tunes and songs, and as a result, 29 folk music samples \nrecorded by Y. Rajabi from the collection \"Uzbek folk songs\" compiled by Y. Romanovskaya and Il. \nAkbarov were included. In 1955 -59, he published the 5 -volume \"Uzbek fo lk music\" collection. There \nare about a thousand Uzbek, Tajik, Uyghur melodies and songs of different genres, Bukhara \nShashmaqomi, Fergana - Tashkent status roads, big songs, 20th century composers: K. Jabborov, N. \nHasanov, S. Kalonov, The works of F. Sodi kov, T. Jalilov, M. Mirzayev and others are included. During \nhis work with the \"Maqom\" ensemble, Y.Rajabiy prepared a new 6 -volume edition of Shashmaqom \nand a perfect collection of gramophone records in the Tashkent gramophone studio in 1970.[2, p. 65]  \nT. Sodikov, D. Zokirov, D. Soatkulov, K. Mominov, O. Imomkhozhayev, B. Davidova, K. \nIsmoilova, O. Alimahsumov, T. Alimatov and many other artists of Y. Rajabi are his students. In 1971, \nhe was awarded the State Prize of Uzbekistan, and in 2000, he was awar ded the Order of Merit. The \n\"Maqom\" ensemble under the Uzbekistan Television and Radio Company, the Jizzakh Musical Drama \nand Comedy Theater, the Tashkent School of Music Pedagogy, one of the streets and a metro station \nin Tashkent are named after Yunus Ra jabi. In 1997, the Rajabi House Museum was opened in the \nhouse where Yunus Rajabi lived in 1957 -76. In 2001, the Yunus Rajabi cultural center was established. \n[3, p. 76]            \n   «POLISH  SCIENCE  JOURNAL»   \n9  \n Yunus Rajabi was born on January 5, 1897 in the Chakar neighborhood of Tashkent city, in \nthe family of a gardener. His father, Rajab, was an enthusiastic, cheerful, funny, artistic, generous \nperson who was not afraid of any hard work. Rajab's father had a large garden in Novza dakha (in front \nof the current Chilanzor metro station), and as soon as spring came, things would heat up there. \nRajab's father moved with his family from the city to the field and lived there until late autumn. Since \nRajab was a fami ly man, he taught his children the first lesson of hard work and hard work in this field \ngarden. The 10 children born before his mother, Aysha Bibi, died of various diseases when they were \nyoung. Yunus Rajabi was the youngest of the last 6 children who gre w up. After a heavy cocktail, Rajab \nhad a habit of going to various gatherings: teahouses, weddings and performances. He often took his \nyoungest beloved and clever son Yunusvoy with him. Ata Rajab used to observe such things from the \nsidelines. Rajab began  to dream that his son would become a well -known musician or khafiz. Very \nfond of music, his father was fond of his neighbors Mirza Kasim Hafiz and Mulla Toychi Hafiz. His son \nYunusvoy's special attention to music, his pleasure, and his behavior were notic eable. He was happy \nthat he could immediately sing a tune he heard, and that he had a good ability to memorize it.  \n Ayesha Bibi is his mother, a housewife, she is a very proud owner, she memorized the Qur'an \nand epics and the ghazals of many of our classic poets, she also writes poetry, she is kind and kind. \nshe was a learned, horse -riding, literate woman. The wives of a house gathered together and read \nvarious stories and epics in Arabic writing, and attracted everyone's attention. At the beginning, Yunus \nRajabi joined his brother Risqi Rajabi in his studio, following his tanbur playing, and he began to \npractice while s inging light folk songs. Later, Mirza Qasim, Mulla Toychi, who were neighbors of the \ngarden, learned from the khafiz folk songs and fragments of status that were sung in Tashkent and the \nFargona Valley. Soon he started playing music and singing in the clas sroom. Entered circles. He began \nto listen to the performances of the mature singers and musicians of that time and music with his \nsoulful and sensitive heart. Yunus Rajabi's love for art and music passed down from his father, ability \nto remember from his mother, and his talent played a big role in choosing his own path.[4, p. 112]  \nThe years 1904 -1913 were the first time to step into the world of music, to follow the \nteachers, to dare them. He studied at the old school, then at the madrasa. Now it was time for him to \nstudy, study, research, and in 1913 he lost his father.  \nIn 1913 -1914, he was not afraid of all kinds of drinks to help with household chores, passed \nthe school of life, entered the circle of great master musicians, learned classical music by listening \ncarefully to master performers, and passed various tests. ti sh enters the period when he played Uzbek \nfolk tunes and songs with a dutor and became known as a khafiz, started teaching in music clubs and \nschools, and chose musicology as his profession.[5, p. 65]       \nSince Yunus Rajabi spent his whole life creating and directing Uzbek folk musical \ninstruments, ensembles, and orchestras, he certainly knew the performance style of each instrument, \ntheir sound, what they are capable of, and what they are capable of. he kn ew very well where and \nhow he should be able to use the instrument.  \n \nREFERENCES USED  \n \n1. R. Yunusov. \"Fakhriddin Sadikov\". Tashkent -2005.  \n2. O. Matyokubov. \"Maqamot\" Tashkent -2004.  \n3. K. Avaz. \"Distant melodies\". Tashkent - 1997.  \n4. Y. Norbutaev's article \"The new sound of the national voice\" Tashkent - 2016.  \n5. B. Mirpayazov's article in Sanat Yoli magazine. Tashkent - 2017  \n6. Talaboev, A., Akbarov, T., & Haydarov, A. The European Journal of Arts, Issue 1/2020.    «POLISH  SCIENCE  JOURNAL»   \n10  \n SCIENCECENTRUM.PL  ISSUE  2(47)  ISBN  978-83-949403 -4-8 \n \nSECTION:  BIOLOGY  SCIENCE  \n \n \nBuronova Fotima, Juraboyev Madadbek  \nSamarqand  davlat tibbiyot  instituti  talabalari  \n(Samarqand,  Uzbekistan)  \n \nLIPIDLARNING UMUMIY TASNIFI HAMDA LIPIDLAR ALMASHINUVINING BUZILISHI  \nNATIJASIDA  KELIB  CHIQADIGAN KASALLIKLAR  \n \nAnnotatsiya.  Ushbu  maqola  lipidlar  almashinuvining  buzilishi  natijasida  kelib  \nchiqadigan kasalliklar hamda ularning tasniflari, shuningdek ularning oldini olish choralari  \nhaqida so’z yuritadi. Organik birikmalarning boshqa asosiy klasslari (nuklein kislotalar, oqsillar  \nva uglevodlar) organik eritgichga qaraganda suvda juda ko'p eriydi. Lipidlar - uglevodorodlar  \n(vodorod va kisloroddan iborat molekulalar), lekin ular umumiy molekulyar tuzilishga ega  \nemaslar.  \nKalit  so’zlar:  Lipid,  Sfingolipidlar,  Fosfoglitseridlar,  Giperlipoproteinemiya,  \nGipertriglitserinemiya  \n \nKirish  qism  \nLipidlar  - ularning  umumiy  nomlari  bilan  tanishishingiz  mumkin  bo'lgan  tabiiy  \nmuhitdagi organik birikmalar: yog'lar. Bu moddalar guruhining asosiy xarakteristikasi shundaki,  \nular suvda  eriydi.  \nEster  funktsional  guruhini  o'z ichiga  olgan  lipidlar  suvda  gidrolizlanadi.  Vakslar,  \nglikolipidlar, fosfolipidlar va neytral mumlar gidrolizlanadigan lipidlardir. Ushbu funktsional  \nguruhga  ega bo'lmagan  lipidlar  gidroizlanmaydigan  hisoblanadi.  Non-gidrolizlanadigan  lipidlar  \norasida  steroidlar  va yog'li  eruvchan  A, D, E va K vitaminlari  mavjud.  \nLipidlar  suvda  erimaydigan,  ammo  organik  erituvchilarda  eruvchan,  kimyoviy  tuzilishi  \njihatidan  xilma -xil bo‘lgan  birikmalardir.  \nLipidlar  organizmda  muhim  vazifalarni  bajaradi:  \n1. Biomembranalaming  asosiy  tarkibiy  qismini  tashkil  etadi.  \n2. Biologik  membranalar  o4tkazuvchanligini ta’minlaydi.  \n3. Nerv  impulslarini  o4tkazishda  ishtirok  etadi.  \n4. Hujayralararo  kontaktni  ta’minlashda  ishtirok  etadi.  \n5. Organizmda  energetik  zaxira  vazifasini  o‘taydi.  \n6. Organizmga  yog‘da  eruvchi  vitaminlaming  tushishi  va ulaming  \no‘zlashtirili$hini  ta’minlaydi.  \nEng ko‘p  uchraydigan  lipid  neytral  yog’lar  bo'lib,  ular asosan  glitserin  va yog’  \nkislotalaridan  tarkib  topgan.  Lipidlarning  asosiy  sinflari:  \n- Neytral  yog’lar  (yoki  neytrel  gliserollar);  \n- Mumlar;  \n- Sfingolipidlar:  sfingomieliplar,  serebrozidlar,  gangliozidlar.    «POLISH  SCIENCE  JOURNAL»   \n11  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \n- Steroidlar;  \n- Fosfoglitseridlar:  fosfatidiletanolamidlar,  fosfatidilserinlar,  fosfatidilinozitol - lar, \nplazmalogenlar,  kardiolipinlar.  \nTabiatda uchraydigan yog’ kislotalari shartli ravishda to'yingan, bitta qo‘shbog‘ tutuvchi  \nva ko‘p qo‘shbog‘ tutuvchi guruhlarga bo‘lish mumkin. Yuqorida ko’rsatilganidek, ko‘pchilik yog’  \nkislotalari tarkibida uglerod juft sonlami tashkil etadi. Hayvon va o'simlik lipidlari tarkibiga  \nkiruvchi to’yinmagan yog’ kislotalar molekulasida qo‘shbog‘ ko‘pincha 9 va 10 -uglerodlar orasida  \nbo‘ladi.  \nNeytral  yog’lar  (yoki atsilglitserollar)  glitserin  va yog‘ kislotalaridan  tarkib  topgan  efirdir.  \nFosfoglitseridlar fosfatid  kislota  hosilalari  bo'lib,  ular glitserin,  yog’  kislotalar,  fosfor  \nkislota  va odatda  bironta  azot tutuvchi  birikmalardan tarkib  topgan:  \nBir necha  xil fosfoglitseridlar  guruhi  mavjud:  \nFosfatidilxolin (letsitin);  \nFosfatidiletanolamin;  \nFosfatidilserin;  \nPlazmalogen;  \nFosfatidilinozitol;  \nKardiolipin.  \nSfingolipidlaming  uch guruhi  mavjud:  \nSfingomiyelinlar,  serebrozidlar  va gangliozidlar.  \nSfingolipidlar  tarkibiga  ikki atomli  to’yinmagan  aminospirt  sfingozin  kiradi.  \nSfingomiyelinlar  hujayra  membranalarida,  nerv  to‘qimasi,  buyrak,  jigar  to'qimalarida  \nko‘p miqdorda uchraydi. Serebrozidlar tarkibida fosfor kislota ham, xolin ham bo’lmaydi. Uning  \ntarkibida geksoza sfingozinning gidroksil guruhi bilan efir bog’i orqali birikkan, yog’ kislota  \nqoldig’i  esa sfingozinning  aminoguruhi  orqali  birikkan:  \nGangliozidlar tarkibida yuqori yog’ kislota, spirto -sfingozin, D -glyukoza, D -galaktoza va  \nN-atsetuglyukozamin yoki N -atsetilneyramin kislota mavjud. Gangliozidlar ko'proq miyaning  \nkulrang moddasida, nerv va glial hujayralarning plazmatik membranalarida ko‘p miqdorda  \nbo'ladi.  \nYog'larning  hazmlanishi  \nYog’lar  og’iz  boshligida  hazm  bo'lmaydi.  Luqma  me’daga  tushadi  va u yerda  kislo - tali \nmuhit yuqori bo‘lganligi uchun yog’lar parchalanmaydi. Lipaza uchun optimal muhit pH 5,5 -7,5. \nGo’daklar oshqozonida yog'lar parchalanadi, chunki ular iste’mol qilgan yog’lar sut yog‘lari  \nbo'lib,  ular emulsiyalangan  holatda  hamda  sut oshqozondagi  kislotani  bog‘lab  muhitni  kuchsiz  \nishqoriy  tomonga  siljitadi.  Shuning  uchun  go'dak  me’dasida  lipaza  fermenti  ishlab  chiqiladi  va \nu sut tarkibidagi yog‘lami parchalaydi. Katta yoshdagi odamlarda yog'lar o‘n ikki barmoqli  \nichakda parchalanadi. Me’da osti bezining shirasi o‘n ikki barmoqli ichakka quyiladi va uning  \ntarkibida  lipaza  fermenti  bordir.  Bu fermentning  ta’sir  etishi  uchun  quyidagi  sharoitlar  bo'lishi  \nzarur: ichak shirasidagi muhit kuchsiz ishqoriy bo’lmoqligi; yog'lar emulsiyalangan holatda  \nbo'lishi  lozim.  Pankreatik  shira  tarkibidagi  bikarbonatlar  oshqozondan  tushgan  xlorid  kislotani  \nneytrallaydi.  Jigar  o'z navbatida  kuchli  emulgator  safroni  o'n ikki barmoqli  ichakka  quyadi.  \nKasalliklar  \nYog'larning  to'planishi  (semizlik)    «POLISH  SCIENCE  JOURNAL»   \n12  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \nOrganizmda  yog'laming  me’yoridan  ko'p  to'planishi  kuzatilsa,  semizlik  deb nomlanadi  \nva bu quyidagi  holatlarda  kuzatiladi:  \n- alimentar faktor: iste’mol qilingan ovqatning kaloriyasiga nisbatan sarflanayotgan  \nenergiya  tengligi  buzilgan  holda.  Natijada  yog'lar  to'planib,  semizlik  vujudga  keladi;  \n- gipotalamus  shikastlanganda:  ma’lumki  yog'lar  almashinuvi  gormonal  boshqaruvga  \nega va gipotalamus shikastlanganda ishtaha markazi boshqarishi buzilib ochofatlik holatlari  \nkuzatiladi,  ko‘p  ovqat iste’mol  qilish  semirishga  sabab  bo'ladi;  \n- irsiy semizlik; ma’lumki ota -ona semizlikka moyil bo'lsa, genetik axborotga ko'ra  \nfarzandlari  ham  moyil  bo'ladi.  Bu irsiy axborot dominant xususiyatiga  ega;  \n- me’yoridan ko'piiglevodlar iste’mol qilish energiya manbai sifatida asosan uglevodlar  \nsarflanib,  yog'lar  zaxirasi  ortadi.  Shu bilan  birga  uglevodlar  iste’mol  qilish  doimo  insulin  ishlab  \nchiqarishga majbur qiladi va bu gormon yog'laming zaxirada to'planishiga sababchi bo'ladi.  \nQalqonsimon bezning gipofunksiyasi: qalqonsimon bez ishlab chiqaradigan gormonlar umumiy  \nmodda almashinuvini boshqaradi va bu gormon miqdori kamayishi modda almashinuvining  \nsusayishiga  olib kelib  zaxira  moddalaming  to'planishi  bilan  kuzatiladi.  \nYog'lar  sarflanishining  kuchayishi  yoki kaxeksiya  \nYog'lar sarflanishining kuchayishi va organizmning o'ta ozib ketishi kaxeksiya deb  \nnomlanadi.  Bu holat  quyidagi  omillarga  bog'liq:  \na) alimentar  omil:  iste’mol  qilgan  ovqat  kaloriyasi  jihatidan  past  bo'lib  sarflanayotgan  \nenergiya  yuqori  bo'lgan  holatlarda;  \nb) saraton  kasalligi  rivojlanayotganda;  o'sma  to'qimasida  modda  almashinuvi  \nkuchaygan bo'lib butun organizmdan energiya moddalarini tortib oladi, natijada ozib ketishi  \nkuzatiladi;  \nd) qandli  diabetda;  modda  almashinuvi  yuqorida  ko'rsatilgan  va yog'lar  asosan  \nglyukoza  sinteziga  sarflanadi;  \ne) gipertireozda: qalqonsimon bez gormonlari asosiy modda almashinuvini kuchaytirib  \nyuboradilar.  \nGiperlipoproteinemiya  \nQonda  tashiladigan  lipidlarning  umumiy  miqdori  ovqatlanish  maromiga,  uning  \ntarkibiga, inson yoshiga, uning yashash sharoitiga, shu jumladan, iqlimi va yilning fasllariga  \nma’lum darajada bog‘liq bo'ladi. Bu ko‘rsatkich hatto bir kecha -kunduz davomida ham sezilarli  \ndarajada o‘zgarib boradi. Gap shundaki, ovqat yeyilgandan keyin 4 -5 soat davomida qonda  \nlipoproteidlar konsentratsiyasi xilomikronlar hisobiga ortib boradi. Qondagi lipidlar umumiy  \nmiqdorini  aniqlash  uchun  qon nonushtadan  oldin  olinadi.  Bunday  qonda  xilomikronlar  \nbo‘lmaydi  va faqat  ZJPL  (lipidlar  umumiy  miqdorining  15 foiz).  ZPL (60%)  va ZYL (25%)  mavjud  \nbo‘lib,  ular birgalikda  qondagi  lipidlarning  umumiy  miqdorini  tashkil  etadi.  Qonning  bu \nko‘rsatkichi O’zbekiston sharoitida yashovchi odamlarda qonidagiga qaraganda anchagina past  \ndarajada bo‘lib, 630 -670 mg % (o‘rtacha 650±ll,3mg %) ga teng (O.A.Abrarov, A.R.Isroilov,  \n1985).  Qondagi  triglitserid  va xolesterinlar  \nlipoproteinlar  tarkibida  b o ‘ladi.  Triglitseridlarning  qondagi  konsentratsiyasi  105-120 \n(o‘rtacha 113±3,7) g %ni tashkil etadi, xolesterinning umumiy miqdori 165,0± 176,0 mg %  \natrofida  bo‘lib,  o‘rtacha 170±3,3  mg %ni tashkil  etadi.  Uning  13,0%  ZJPL  tarkibida,  67,0%  ZPL \nva 20,0% ZYL  tarkibida  bo‘ladi  (O.A.Abrarov,  A.R.Israilov,  1985;  O.A.Levkovich,  1994).    «POLISH  SCIENCE  JOURNAL»   \n13  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \nQonda lipoproteinlar konsentratsiyasining me’yordan ortib ketishi lipoproteinemiya  \ndeyiladi.  Bunda  bir yo‘la  xolesterin  va triglitseridlar  miqdori  ham  ko‘payadi.  Xolesterin  \nmiqdorining  ortishi  ZPL va ZYL lar konsentratsiyasi  bilan  bog‘liq  bo‘lsa,  triglitseridlar  \nkonsentratsiyasi  esa xilomikronlar  va ZJPLlar  miqdori  bilan  bog‘liq.  Shunga  ko‘ra  \nlipoproteinemiyalaming  uchta  shakli  tafovut etiladi:  \n- Giperxolesterinemiya  (ZPL  yoki ZYL konsentratsiyasi  ortgan).  \n- Gipertriglitserinemiya  (xilomikronlar  yoki EJPL  konsentratsiyasi  oshgan).  \n- Aralash  shakli.  \nKelib chiqish mexanizmiga ko‘ra giperlipoproteinemiyalarning birlamchi (tug‘ma) va  \nikkilamchi  (turmushda  orttirilgan)  xillari  farq qilinadi.  Tug‘ma  giperlipoproteinemiyaga  \nlipoproteinlipazaning tug‘ilishdan nuqsonli, ya’ni faolligi ancha pasaygan holda bo‘lishi bilan  \nbog‘liq bo‘lgan giperxilomikronemiya hamda ZPL katabolizmining pasayishi tufayli vujudga  \nkeladigan  giperxolesterinemiya  yoki P~ lipoproteinemiyalar  misol  bo’ladi.  Ikkilamchi  giperlipo - \nproteinemiyalar  qandli  diabet,  surunkali  gepatit  va alkogolizm  kabi kasalliklarda  kuzatiladi.  \n \nFOYDALANILGAN  ADABIYOTLAR:  \n1. A.Ya.  Nikolayev.  Biologik  kimiyo.  Toshkent  Ibn Sino  nomidagi  nashriyot.  1991.  \n2. А. Ленинжер.  Основи  биохимии.  Москва  «Мир»  1985.  \n3. А.Я. Николаев.  Биологическая  химия.  Москва.  2004.  \n4. T.T. Березов,  Б. Коровкин.  Биологическая  химия.  Москва  «Медицина»  1990.  \n5. Р. Марри  и др. Биохимия  человека. Т. 1. Москва «Мир»  1993.    «POLISH  SCIENCE  JOURNAL»   \n14  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \nSECTION:  ECONOMICS  \n \n \n \nUDC  06.58.31   \n \nJarlykassymova A. S.  \nmaster  of economics,  \nsenior  lecturer  of the department  of Economics  and Law.,  \nDuisebaeva N. R.  \nsenior  lecturer  of the department  of Economics  and Law.,  \nMussagulova  A. K. \ndoctor  of PhD.,  \nsenior lecturer of the department of Economics and Law.,  \nAblai  Khan  Kazakh  university  of International  \nrelations  and World  languages.  \n(Almaty,  Kazakhstan)  \n \nPHENOMENOLOGY  OF NATIONAL  ECONOMIC  SECURITY  OF THE STATE  \nIN THE PROCESS  OF GLOBALIZATION  \n \nAbstract.  At the end of the XX century  the term  \"national  economic  security\"  began  to \ntake an important meaning in the management of the state. National economic security is the  \nposition  of the country,  resistant  to both  internal  and external  threats,  can ensure  the stability  \nof the economy,  develop  providing the needs  of the people  living  in this country.  Sustainability  \nand stability  of the national  economy  shows  the strength  of the economic  system,  protection  \nfrom various unintended factors. For this purpose, countries develop ways of developing  \nnational economic security. Important for economic security are indicators that reflect the  \nstate of the country as a whole, such as: inflation, unemployment rate, Gini coefficient and  \nthe proportion of the population with incomes below the subsistence level, etc. The article  \ngives  a general  notion  of economic  security,  analysis  of its main  indicators,  as well as provides  \nways  of development of  Kazakhstan.  \nKeywords: economic  security,  sustainability,  security,  threat,  economic  indicators.  \n \nЖАҺАНДАНУ  ҮДЕРІСІНДЕГІ  ҰЛТТЫҚ  ЭКОНОМИКА  \nҚАУІПСІЗДІГІНІҢ  ФЕНОМЕНОЛОГИЯСЫ  \n \nТүйін. XX ғ. соңындa «ұлттық экономикaлық қaуіпсіздік» термині мемлекет  \nбaсқaруындa мaңызды орынғa ие болa бaстaды. Ұлттық экономикaлық қaуіпсіздік – бұл \nелдің сыртқы және ішкі қaуіп қaтер­лерге төтеп бере aлуы, экономикaның тұрaқтылығын  \nқaмтaмaсыз етуі мен елде тұрaтын хaлықтың қaжеттіліктерін қaмтaмaсыз ете отырып,  \nдaму.  Ұлттық  экономикaның  тұрaқтылығы  экономикaлық  жүйенің  орнықтылығын,  \nкүтпеген фaкторлaрдaн қорғaныштығын білдіреді. Ол үшін мемлекеттер ұлттық  \nэкономикaлық  қaуіпсіздіктің  дaму  жолдaрын  әзірлейді . Экономикaлық  қaуіпсіздікте    «POLISH  SCIENCE  JOURNAL»   \n15  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \nелдің тұтaстaй экономикaлық жaғдaйын сипaттaйтын көрсеткіштер aйтaрлықтaй мәнге  \nие, мысaлы:  ин фляция,  жұмыссыздық  деңгейі,  Джини  коэффициенті,  тaбысы  күнкөрістің  \nең төменгі деңгейінің шaмaсынaн төмен хaлықтың үлесі т.б. Мaқaлaдa экономикaлық  \nқaуіпсіздік ұғымынa aнықтaмa бе ріліп, негізгі көрсеткіштерге тaлдaу жaсaлғaн,  \nсондaй­aқ  Қaзaқстaн  үшін  дaму  жолдaры  ұсынылғ aн. \nКілт  сөздер:  экономикaлық  қaуіпсіздік,  тұрaқтылық,  қaуіпсіздік,  қaуіп,  \nэкономикaлық  көрсеткіштер.  \n \nФЕНОМЕНОЛОГИЯ Н AЦИОН AЛЬНОЙ ЭКОНОМИЧЕСКОЙ БЕЗОП AСНОСТИ  \nГОСУД AРСТВ A В ПРОЦЕССЕ  ГЛОБ AЛИЗ AЦИИ  \n \nАннотация. В конце XX в. термин «н aцион aльнaя экономическ aя безоп aсность»  \nнaчaлa приним aть вaжное  знaчение  в упрaвлении  госуд aрств a. Нaцион aльнaя \nэкономическ aя безоп aсность  – это положение  стрaны, устойчивой  кaк внутренним,  тaк и \nвнешним  угроз aм, может  обеспечить  стaбильность  экономики,  рaзвив aться  \nобеспечив aя нужды  людей,  прожив aющих  в этой  стрaне. Устойчивость  и стaбильность  \nнaцион aльной  экономики  покaзывaет прочность  экономической  системы,  \nзaщищенность  от рaзных  непредусмотренных  фaкторов.  Для этого  стр aны \nрaзрaбaтыв aют пути р aзвития н aцион aльной экономической безоп aсности. В aжное  \nзнaчение  для экономической  безоп aсности  имеют  покaзaтели,  отр aжaющие  состояние  \nстрaны в целом, т aкие к aк: инфляция, уровень безр aботицы, коэффициент Джини и доля  \nнaселения с доход aми ниже прожиточного минимум a и т.п. В ст aтье д aно общее понятие  \nэкономической  безоп aсности,  aнaлиз основных  ее покaзaтелей,  a тaкже предост aвлены  \nпути  рaзвития  Кaзaхстaнa. \nKлючевые  слов a: экономическ aя безоп aсность,  устойчивость,  безоп aсность,  \nугроз a, экономические  покaзaтели.  \n \nIntroduction:  The process  of globalization  encompasses  new  industries  and \ntechnologies.  In order  to achieve competitiveness  and develop  the level  of security,  a country  \nmust  have  a \"high\" status.  The role of man  in the economy  and the man  himself,  the risks  of \nthe economy  and the possibility of  its development  are changing.  \nThe basis  for the analysis  of economic  security  in the context  of globalization  lies in the \nprevention  of dependence  and risk in rapidly  developing  segments  of world  markets:  capital,  \nintellectual  property,  capital  markets,  services  [intellectual  property].  \nAt the end of the XX and the beginning of the XXI century, the issue of security is  \nrelevant  for the countries  of the global  economy.  There  are many  types  of security:  political,  \neconomic,  technical,  information,  environmental,  food,  etc. Economic  security  as a complex  \nsocio -economic  phenomenon  requires  comprehensive  research.  \nEconomic  security  is a set of external  and internal  conditions,  such  as the rapid growth  \nof the national economy, its satisfaction of the needs of society, the state, the individual,  \nensuring competitiveness in foreign markets. Therefore, economic security of states is  \nimportant. First, the economic situation of the state determines the political power of the state  \nin world  cooperation.  Second, the economic  potential  of the state  affirms  its ability  to resist    «POLISH  SCIENCE  JOURNAL»   \n16  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \nthe intervention of other states. Third, food security, technical, information and financial  \nsecurity depend on the economic situation of the country. At the same time, economic security  \nis prone to various risks. This deepening and complication of security is the result of integration  \nprocesses and globalization. In order to prevent economic security, it is possible to suggest  \nways  to prevent  it, as a result  of a thorough  understanding  of its concept,  what  are the causes  \nand what are the consequences. However, there is no common definition of economic security,  \nwhich is interpreted by different authors. Therefore, the main purpose of this article is to  \nprovide a general definition of the concept of economic security, to analyze the factors  \ninfluencing  it and  on its basis  to suggest ways  of development of  the state.  \nMaterial and method of research . The shortest definition of economic security is  \ngiven by V.I. Yarochkin: It was a condition of protection of the economy from external and  \ninternal  threats.  Well -known  expert  in the field  of economic  security  According  to \nV.K. Senchagova, the socially oriented development of the state as a whole, which guarantees  \nthe protection of national interests, is the essence of economic security as an institution of  \neconomic security, which can have sufficient defense potential even in the unfavorable  \ndevelopment  of external  and internal  processes.  \nAccording to I.P. Belozerov, the economic security of the state is the capacity of the  \nnational economy to ensure the sustainability and development of limited economic resources  in \norder  to fully  realize  the economic  interests  in the face  of global  competition.  That  is, as we see, \nthere  are different  definitions  of \"economic  security\",  and in general  we would  give the \nfollowing definition: economic security - the protection of the state from economic threats, the  \nprotection  of the state,  the sustainable  development  of the state.  Russian  academician  \nL. Abalkin  identifies  three  main  elements  of the structure  of economic security:  \nEconomic  independence:  Since  the international  division  of labor  makes  the national  \neconomy interdependent, it does not have an absolute character in the modern world  \neconomy.  Economic  security  means  the ability  of the state  to control  national  resources  and \nparticipate  equally in  all international  economic relations;  \nStability  and stability  of the national  economy;  \nAbility  to participate  in self-development  and processes.  From this  we can see that it is \neconomically  safe.The  structure  of the structure  is complex [2].  \nStability  and security  are important  characteristics  of the economy  as a whole  system.  \nThere  is no need  to oppose them,  each  of them  describes  the state  of the economy.  \nSecurity is the state of the object in the system in terms of the desire to survive and  \nthe relationship between  the internal  and external  threat,  as well as the action  of factors  that \nare unpredictable  or difficult to predict  \nAs the economic system becomes more stable, production and financial banking  \ncapital, etc. The more favorable the relationship to economic life, that is, the higher the  \nassessment  of its security [3].  \nMany scientists pay attention to the level of economic security, which, in their opinion,  \nwill not be able  to solve  the problems  facing  the international  level, both  inside  and outside  \nthe country, without ensuring a sufficient level of economic security. Threats to economic  \nsecurity can lead to imbalances in the activities of any economic entity. Ensuring this economic  \nsecurity  at the state  and global  level  should  become  a general  strategic  task of the state.  The   «POLISH  SCIENCE  JOURNAL»   \n17  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \nfollowing  are the main  levels  of economic  security  used  in the spatial  constraints  of different  \neconomic systems:mega -economic, global level. At this level, the economic interests of the  \nworld economic system as a whole are considered, connected with the observance of key  \nindicators  of economic  life. The concept  of international  economic  security  is being  formed,  \nand the role of international organizations in ensuring it is special. International organizations  \nstrive  to build  stability  at the global  level  through  measures  such  as food  security,  increasing  \nliteracy,  peace, and  the fight against  poverty.  \nThe integration of individual countries into integration groups of different nature in  \norder to fully realize the national economic interests in the world economic space. In this case,  \nthe economic  risks  come  from the  states  and blocs  that have  extensive  economic  resources  \nand carry  out their  economic  expansion  in relation  to the member  states  of the Union.  \nThe macroeconomic  level  is given  to the national  economy  and,  accordingly,  reflects  \nthe national  economic interests.  \nConsideration  of economic  security  at the economic  level  is taken  as a factor  \ninfluencing the overall security of the nation. The economic security of the region characterizes  \nthe structure of the economic security system, the stability and development of the economy. It  \ncan be concluded that overall economic security can be affected by a breakdown of economic  \nsecurity  by level.  \nThreats  to global  economic  security:  The global  financial  crisis;  World  energy  \ncrisis;Man -made and natural disasters, large -scale terrorist acts. Military tension, regional and  \nworld  wars.  \nThese threats destabilize the world economy and, in turn, affect many countries around  \nthe world.  In general,  the growing  number  of threats  to economic  security  is the result  of this \nglobalization effort. As mentioned above: the crisis, world wars, etc. In addition to external  \ninfluences,  domestic  threats  contribute  to the deterioration  of the economic  situation,  which  \nis characterized  by certain  indicators.  \nInternal  threats  to economic  security  include,  for example,  a significant  decline  in GDP,  a \ndecline in investment and innovation activity, and the inability of the agricultural sector to  \nsupply  the domestic  market.External  threats  are the weakening  of the role of the state  in the \nworld economic space and the country's transformation into a carrier of raw materials, energy  \nresources and high -tech science -intensive goods and food products. As a small economic  \ncountry in the world, we should focus on the risks that affect the state of the national economy,  \nnot at the global  level,  but at  the level  of the economy  [2]. \nThe system of indicators of economic security should highlight the following: living  \nstandards  and quality  of life,  inflation, unemployment,  public  debt, the  state  budget  deficit,  \neconomic growth, gold reserves. It can be seen that the assessments of these general economic  \nsecurity  cover  important  areas  of the economy  and cover  the basic  human  activities  [4]. \nThere is no single definition of the concept of national economic security in the  \nscientific  literature.  Some  researchers  believe  that the security  of the international  economic  \nsystem and the issues it deals with, such as the diversity of economic development, debt  \ngrowth,  the spread  of hunger  and cyclical  fluctuations,  and other  uncertainties  of the world  \neconomy.  Other  experts  began  to ensure  the creation  of favorable  conditions  for the  effective  \ndevelopment  of a competitive  national  economy.    «POLISH  SCIENCE  JOURNAL»   \n18  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \nputs in place. These include: free access to foreign sources of raw materials and energy,  \nthe guarantee  of stable  foreign  investment  and the free exchange  of goods  and services.  We \ncan divide the national economy into three factors that can withstand external shocks. The  \nfirst group:  natural  factors:  the territory  of the country,  its geographical  location,  the presence  of \nminerals, climatic conditions. The second factor is the level of development of production,  \ntechnology, income level of the population, gold and foreign exchange reserves and other  \nindicators of wealth, which may change in the relatively long term. La stly, the factors that need  \nto be regulated  in the short  term  are: foreign  trade  quotas,  level  of specialization,  \ndiversification, trade concentration, etc. can be included [5]. The stability of the national  \neconomy means the stability of the economic system, protection from unforeseen factors. To  \nthis end,  states  shall  develop  ways  to develop  national  economic  security.Ensuring  the needs  \nof one of the types  of economic  security  specified  in the law, energy  resources.  \nFinding out about energy security, which provides protection from related hazards.The  \neconomy  of the republic  is characterized  by the share  of high -energy  technology  and energy - \nintensive  production.  Environmentally  safe energy  is focused  on wind  energy.  In general,  the \nentire  territory  of Kazakhstan  is suitable  for the installation  of wind  generators [5].  Taking  into \naccount external factors, there are three main characteristics of the reduction of energy  \nsecurity  in Kazakhstan:Justification  of the economic  relationship  between  the cost of \ntransportation and the volume of sales in the consumer market of energy resources;Direct  \nsupply  of raw energy  to consumers;of  economic  and political  barriers  to transit  through  third  \ncountries.  \nConclusion: A proper analysis of the security of the national economy requires the  \nimplementation of the necessary measures based on its results.The existence of laws and  \nregulations  in the country  that ensure  national  security,improvement.  It is based  on finance,  \ntransport infrastructure, energy, etc. elimination of problems in the industry and achieving  \nstability. National economic security can be ensured only on the condition of effective law and  \nstate  regulation.  \nTo reduce the dependence of the national economy on external factors, to achieve  \nsocial  and economic  security  through  the development  of the country's  economy,  improving  \nthe welfare of the population, employment, reducing the level of the population with a  \nminimum  income.  \nIn the field of energy security: revision of normative documents on the amount and  \nprocedure for payment of environmental protection payments, the use of funds of the  \nEnvironmental Protection Fund.Increasing the cost of research and development, because  \nwithout the development of domestic science, it is impossible to develop new innovative  \nsectors  and create  areas  that require  science.  \nIn conclusion,  economic  security  is the ability  to ensure  its competitiveness  in foreign  \nand domestic  markets,  consisting  of external  and internal  factors  that affect  the growth  of the \nnational  economy, protected  by various  levels  of threats.    «POLISH  SCIENCE  JOURNAL»   \n19  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \nREFERENCES:  \n1. Vorozhihin V.V. Ob opredelenii jekonomicheskoj bezopasnosti v uslovijah globalizacii //  \nJekonomicheskaja  bezopasnost’ – 2014.  – № 3. – S. 3-7. \n2. Belozerov  I.P. Jekonmicheskaja  bezopasnost’  gosudarstva  v uslovijah  globalizacii.  Vestnik  \nOmskogo  universiteta.  Serija  «Ekonomika».  – 2006.  – № 4. – S. 21–26. \n3. Ajtbembetova  A.B. Ocenka  problem  jekonomicheskogo  razvitija  i jekonomicheskoj  \nbezopasnosti  RK. \n4. Mironov V.A. Nacional’naja jekonomicheskaja bezopasnost’ i problemy ee obespechenija v  \nRossijskoj  Federacii  v uslovijah  globalizacii  mirovoj  jekonomiki.  [Rezhim  dostupa]:  \nhttp://old.rea.ru/UserFiles/NeskorozenyVV/FSB/ Econ_bez.PDF  \n5. Zagashvili  V.S. Jekonomicheskaja  bezopasnost’  Rossii.  – M.: Jurist#,  1997.  – \n239[Rezhimdostupa]: http://ecsocman.hse.ru/data/940/812/1219/002_Zagashvili.pdf  \n6. Internet -resurs:  http://www.stat.  gov.kz/    «POLISH  SCIENCE  JOURNAL»   \n20  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \nТагиева Гюльнара  \nстарший  преподаватель,  \nБагирова Тарана  \nстарший  преподаватель,  \nОсманова Гюнель  \nстарший  преподаватель,  \nБайрамова  Земфира  \nасистент,  \nИсмаилова  Тарана  \nасистент,  \nБабаева  Фидан  \nасистент                               \nАзербайджанский  Государственный  Аграрный  Университет  \n(Гянджа, Азербайджан)  \nАНАЛИЗ  ФИНАНСИРОВАНИЯ  И СОВЕРШЕНСТВОВАНИЯ  АГРАРНОЙ  ОТРАСЛИ  \nАннотация. Изучение рыночной конъюнктуры должно давать гибкий ответ на  \nпроблемы, которые могут возникнуть в аграрном секторе. Оптимальное согласование  \nрыночного  и государственного  регулирования  в части  регулирования  цен на \nсельскохозяйственную  продукцию  при оптимизации  новой  системы  управления  является  \nодним из важных условий. Формирование новой системы управления в основном  \nнаправлено на создание благоприятных условий для увеличения доходов в сельском  \nхозяйстве  и создание  благоприятных  условий  для формирования  рыночной  \nинфраструктуры в сельском хозяйстве. В связи с этим повышение роли службы  \nмаркетинга  является  одной  из необходимых  мер,  как внутри  регионов,  так  и внутри  \nхозяйствующих  субъектов.  \nКлючевые  слова : государственное  регулирование,  система  управления,  \nфинансирование,  агропромышленный  комплекс,  успешная  реализация  \n \nANALYSIS  OF FINANCING  AND  IMPROVEMENT  OF THE AGRICULTURAL  SECTOR  \n \nAbstract. It is important to study the factors that shape the market situation in the  \nagricultural sector. Thus, it is important to implement the agrarian policy of the state  \neffectively, taking into account the existing pricing conditions. From the point of view of the  \neffective  formation  of the agrarian  policy  of the state,  state  intervention  in agriculture  should  \nbe carried out under the conditions of cost management and pricing. İn this article discusses  \nthe implementation  of these  measures  and studies the market  situation.  \nKeywords : state regulation, management system, financing, agricultural industry,  \nsuccessful  implementation    «POLISH  SCIENCE  JOURNAL»   \n21  \n SCIENCECENTRUM .PL ISSUE  2(47)  ISBN  978-83-949403 -4-8 \n \nВведение  \nПривлечение инвестиций в аграрный сектор является одним из основных условий  \nувеличения объема товаров на аграрном рынке и успешной реализации структурных  \nпреобразований.  Безусловно,  характеристики  сельскохозяйственного  производства  \nоказывают существенное влияние на привлечение инвестиций в сельское хозяйство.  \nСледует отметить, что особенности сельского хозяйства значительно снижают фактор  \nпривлекательности  для инвесторов.  Это приводит  к значительному  отсутствию  \nинвестиций  в сельское  хозяйство  по сравнению  с другими  отраслями  экономики.  \nФинансирование  аграрных  предприятий  является  важной  предпосылкой  \nобеспечения конкурентоспособного развития экономической системы страны. Назрела  \nсерьезная необходимость в реализации государственной инвестиционной политики в  \nцелях  ликвидации  инвестиционного  дефицита  в сельском  хозяйстве  и удовлетворения  \nпотребностей  товаропроизводителей в  капитальных  ресурсах.  \nПривлечение инвестиций в аграрный сектор является одним из основных условий  \nувеличения объема товаров на аграрном рынке и успешной реализации структурных  \nпреобразований.  Безусловно,  характеристики  сельскохозяйственного  производства  \nоказывают существенное влияние на привлечение инвестиций в сельское хозяйство.  \nСледует отметить, что особенности сельского хозяйства значительно снижают фактор  \nпривлекательности  для инвесторов.  Это приводит  к значительному  отсутствию  \nинвестиций  в сельское  хозяйство  по сравнению  с другими  отраслями экономики  [1, \nc.447].  \nИсследования показывают, что вмешательство государства в агропромышленное  \nпроизводство  всегда  было  противоречивым.  Взгляды  экономистов  в этой  области  \nпрактически  совпадают.  Группа  экономистов  считает,  что государство  не должно  \nрегулировать  сельскохозяйственное  производство  и не должно  вмешиваться  в сельское  \nхозяйство.  Другие  считают,  что сельское  хозяйство  нуждается  в постоянном  \nрегулировании и государственной поддержке. Экономисты, поддерживающие вторую  \nточку  зрения,  считают,  что агропромышленное  производство  не может  выжить  в \nконкурентной  среде  в рыночных  условиях.  \nЭто усугубляется неэластичностью спроса на сельскохозяйственную продукцию,  \nчто приводит к значительному снижению продаж продовольствия по сравнению с  \nдругими отраслями экономики и ограничивает место сельского хозяйства в структуре  \nэкономики  по сравнению  с другими  отраслями.  Кроме  того,  производство  \nагропромышленной  продукции,  большое  количество  хозяйствующих  субъектов,  \nзанимающихся  ею, их географические  и экономические  условия  существенно  влияют  на \nспрос на товарную продукцию, что, безусловно, приводит к це новым колебаниям на  \nаграрном  рынке  [2, c. 227].  Обратим  внимание  на динамику  внешнеторгового  оборота.    «POLISH  SCIENCE  JOURNAL»   \n22  \n SCIENCECENTRUM .PL ISSUE  2(47)  ISBN  978-83-949403 -4-8 \n \nТаблица  1 \nДинамика  внешнеторгового  оборота  \nГоды  В миллион  долларов(США)  в практических  ценах,  в процентах  к \nпредыдущему  году  \nТорговый  \nоборот  Импортировать  экспорт  остаток  \nсредств  Торговый  \nоборот  Импортировать  экспорт  \n2017  24103,3  8783,3  15320  6536,7  85,2  83,8  86,1  \n2018  30955  11465,9  19489,1  8023,2  128,4  100,2  93,1  \n2019  33302,7  13667,5  19635,2  5967,7  107,6  93,5  98,8  \n2020  24471,3  10730,7  13740,6  3009,9  73,5  64 78.9  \nИсточник:  Azstat.orq  \n \nСогласно  таблице,  в 2020  году  произошло  увеличение  общего  товарооборота  на \n1,5% по сравнению с 2017 годом. По сравнению с тем же периодом импорт увеличился на  \n22,2%, экспорт уменьшился на 1579,4 млн долларов, то есть наблюдался снижение на  \n10,3%.  Согласно таблице  баланс  сократился более  чем на  50%.  \nОпыт  развитых  стран  показывает,  что государство  вмешивается  в \nинвестиционные процессы с целью выхода из кризиса и  обеспечения устойчивого  \nразвития сельского хозяйства. Эти вмешательства в первую очередь направлены на  \nповышение инвестиционной привлекательности отрасли, при этом отдавая приоритет  \nреализации  эффективных  крупномасштабных  воспроизводственных  процессов  в \nсельском хозяйстве. Принимаемые государством протекционистские меры, включая  \nувеличение  субсидий,  возмещение  части  затрат  на получение  производительных  \nресурсов  в сельском  хозяйстве  и другие  подобные  стимулы,  направлены  на \nстимулирование  сельскохозяйственного  производства  [3, c. 54-59]. \nВ большинстве  развитых  стран  мира  государственные  инвестиции  в \nагропромышленный  комплекс  превышают  рыночную  стоимость  этой  продукции  \nпримерно в 1,5 -2 раза. Исследования показывают, что стимулирование инвестиций в  \nсельское  хозяйство  является  важной  частью  структурной  политики  на \nсельскохозяйственном рынке. В связи с этим реализация государственной поддержки  \nинвестиций направлена на повышение инвестиционной активности и привлекательности  \nсельского хозяйства.  \nПрямые государственные инвестиции в сельское хозяйство считаются косвенным  \nсредством  бюджетной  помощи  в этой  сфере.  Одним  из важнейших  принципов  \nгосударственного  инвестирования  является  высокая  экономическая  эффективность  \nреализуемых  проектов,  диверсификация  рисков  по сравнению  с капиталом,  приоритет  \nделовой  активности  при их финансировании,  осуществление  инвестиций  на конкурсной  \nоснове. Реализация инвестиционной политики в условиях рыночной экономики также  \nдолжна  быть согласована  с интересами аграрного сектора.  \nПри этом  особое  внимание  уделяется  обеспечению  потребностей  \nсельхозпроизводителей  в материально -технических  ресурсах,  важным  условием  \nявляется реконструкция и техническое перевооружение сельского хозяйства. Поддержка  \nсеменоводческой системы в сельском хозяйстве, развитие материально -технической  \nбазы  хранения  и переработки  продукции,  привлечение  новых  технологий,  повышение    «POLISH  SCIENCE  JOURNAL»   \n23  \n SCIENCECENTRUM .PL ISSUE  2(47)  ISBN  978-83-949403 -4-8 \n \nплодородия почв, содействие в развитии рыночной инфраструктуры и др. проблемы  \nтакже  должны выйти на первый  план.  \nИсследования  показывают,  что одним  из важнейших  аспектов  увеличения  \nинвестиций  в сельское  хозяйство  в современных  условиях  является  повышение  \nинвестиционной привлекательности. Инвестиционная привлекательность тесно связана  \nс реализацией мер, направленных на увеличение инвестиций в сельское хозяйство по  \nсравнению с другими отраслями экономики и страхование инвестиционных рисков в  \nэтой  отрасли.  В мировой  практике  важно  определить  рейтинг  инвестиционной  \nпривлекательности  [4, c. 572].  \nВ современных  условиях  либерализации  производственных  отношений  в \nформировании  и функционировании  аграрного  рынка  используются  более  \nэкономические  механизмы  регулирования.  В связи  с этим  реализация  необходимых  мер по \nформированию  производственной  инфраструктуры  является  важным  условием  \nпривлечения  иностранных  инвесторов  в сельское  хозяйство,  в том числе  в регионы.  \nМировой  опыт  показывает,  что различные  формы  бюджетной  помощи  \nсельхозпроизводителям не только увеличивают доходы сельхозпроизводителей, но и  \nсоздают  благоприятные  условия  для роста  инвестиционных  источников  за счет  \nспециальных  средств  [5, c. 36-39]. \nИспользование зарубежного опыта расширения инвестиционных процессов на  \nоснове взаимного долга также может стать важным инструментом удовлетворения  \nпотребности  сельхозпроизводителей  в капитальных  ресурсах.  Анализ  показывает,  что в \nряде  случаев  эффективность  системы  бюджетного  обеспечения  инвестиционных  \nпроцессов  не столь  высока  по сравнению  с другими  направлениями.  На самом  деле  эти \nпроцессы имеют свои причины, и одной из важных причин этого является низкая  \nрентабельность  сельскохозяйственного  производства.  \nИсследование  показывает,  что важную  роль  играют  модернизация  форм  и \nметодов государственной поддержки сельского хозяйства, а также выявление новых  \nформ  механизмов  реализации  продукции.  С точки  зрения  существующих  реалий  нашей  \nреспублики  одним  из важных  условий  является  реализация  мероприятий,  направленных  на \nрешение  проблем,  возникающих  в процессе  реализации  продукции.  \nАнализ показывает, что в последнее время существуют некоторые ограничения  \nдоступа сельхозпроизводителей на рынок, следствием чего являются более низкие  \nдоходы сельхозпроизводителей по сравнению с другими отраслями экономики. Поэтому  \nвозникает серьезная потребность в инвестициях в формирование аграрно -рыночной  \nинфраструктуры  в сельском хозяйстве  [6, c. 48]. \nНаправление  бюджетных  инвестиций  на формирование  инфраструктуры  \nаграрного рынка, направленное на решение проблем сбыта в сельском хозяйстве,  \nтребует осуществления необходимых изменений в структуре расходов государственного  \nбюджета. Формирование необходимых изменений в структуре расходов, направляемых  \nна сельское  хозяйство  из государственного  бюджета,  в первую  очередь,  может  оказать  \nсущественное  влияние  на ликвидацию  ценовой  диспропорции  между  сельским  \nхозяйством  и другими отраслями экономики.    «POLISH  SCIENCE  JOURNAL»   \n24  \n SCIENCECENTRUM .PL ISSUE  2(47)  ISBN  978-83-949403 -4-8 \n \nИсследования показывают, что создание кредитной кооперации в сельском  \nхозяйстве  может  сыграть  важную  роль  в удовлетворении  потребности  \nтоваропроизводителей в производственных ресурсах за счет корпоративных средств.  \nЕще одним важным инструментом в плане стимулирования инвестиций в сельское  \nхозяйство  и повышения  инвестиционной  привлекательности  отрасли  является  \nсовершенствование  системы  страхования.  \nНеэффективное  страхование  рисков  приводит  к проблемам  не только  с \nпривлечением  иностранных  инвесторов  в аграрный  сектор,  но и с выделением средств  в \nсельское  хозяйство  местными  финансовыми  институтами.  Поэтому  совершенствование  \nсистемы  страхования  можно  считать  одним  из важнейших  инструментов  успеха  \nинвестиционной  политики  государства.  Хотя  инвестиции  в сельское  хозяйство  \nрискованны,  они имеют  свои  преимущества.  Таким  образом,  инвестиции  обычно  \nделаются на длительный период времени. С точки зрения специфики сельского хозяйства  \nвложения  в эту сферу  не считаются  возможными  в краткосрочной  перспективе  окупить  \nсебя.  В настоящее  время  доля  инвестиций  государственного  бюджета  в сельское  \nхозяйство недостаточна. Это приводит к тому, что развитие рыночной инфраструктуры и  \nсоциальной  инфраструктуры  не отвечает  современным  требованиям.  \nТаким образом,  разработка целевых инвестиционных программ  в сельском  \nхозяйстве за счет средств государственного бюджета является важным условием, которое  \nтакже  является  одним  из основных  инструментов  реализации  структурно - \nтехнологической  политики.  \nОсновной задачей инвестиционной политики государства является воздействие  \nна ликвидацию  дефицита  инвестиционных  ресурсов  и привлечение  иностранного  \nкапитала.  Мировой  опыт  показывает,  что активизация  инвестиционных  процессов  \nнапрямую связана с безопасностью инвестиций. Важное значение в этой сфере имеет  \nзащита прав инвесторов и реализация совместных проектов с государством. Этот фактор, в  \nпервую очередь, отражает предоставление государственных гарантий деятельности  \nиностранных инвесторов. В большинстве стран мира государство гарантирует выделение  \nкредитных  ресурсов  в сельском хозяйстве.  \nРезультаты  и выводы  \nПроанализируя  ситуацию  можем  прийти  к выводу  что одним  из важных  условий  \nявляется  реализация  мероприятий,  направленных  на процесс  адаптации  к \nизменяющимся  условиям  на аграрном  рынке.  Также  для преодоления  проблем,  \nвызванных  влиянием  глобализации,  постоянно  растет  потребность  в реализации  \nструктурной  политики  государства  на аграрном  рынке.  Опыт  показывает,  что \nструктурная  политика  должна  основываться  прежде  всего  на экономических  средствах.  \nЭто напрямую  связано  с неэффективным  использованием  административного  ресурса.  \nПоэтому  важно,  чтобы  меры,  направленные  на осуществление  структурных  изменений,  \nосновывались  на экономических  механизмах.  Безусловно,  важно  постоянно  \nсовершенствовать  механизмы  структурного  регулирования,  направленные  на \nосуществление  структурных  изменений на  аграрном рынке.    «POLISH  SCIENCE  JOURNAL»   \n25  \n SCIENCECENTRUM .PL ISSUE  2(47)  ISBN  978-83-949403 -4-8 \n \nСПИСОК  ИСПОЛЬЗОВАННЫХ  ИСТОЧНИКОВ:  \n1. Аллахвердиев  Х.В.,  Гафаров  К.С.,  Ахмедов  А.М.,  Регулирование  государственной  \nэкономики,  Баку: Насир, 2002, с.  447 \n2. Алиев И.Х., Мамедова Ш.А., Основное финансирование развития аграрного сектора,  \nБаку 2007,  с. 227 \n3. Кузнецова  Г. Эффективность  господдержки  АПК  через  меры  «зеленой  корзины»  \n//АПК:экономика, управление,  2011.  №1,  с. 54-59 \n4. Киселев  С.В. Сельская  экономика:  учебник,  М.. ИНФРА -М, 2008,  с. 572 \n5. Михайлюк  О.Н.  Формы  государственной  поддержки  субъектов  хозяйствования  АПК  \nэкономика  селскохозяйственных перерабатывающих  предприятий,  2009,  №7 с. 36-39 \n6. Финансовая  экономика,  научно -экономический  журнал  30 октябрь  2021,  с. 48 \n7. Azstat.orq    «POLISH  SCIENCE  JOURNAL»   \n26  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \nSECTION: INFORMATION AND  \nCOMMUNICATION  TECHNOLOGIES  \n \n \nRo`ziyeva Dilafro`z Muzaffarovna  \nBuxoro  tumani 12 -maktab  matematika  fani o`qituvchisi,  \nBuxoro  tumani  barkamol  avlod  bolalar  maktabi  avtomodel  to`garagi  rahbari,  \nSayidova  Malika  Hamidovna  \nBuxoro  tumani  12-maktab  informatika  fani o`qituvchisi  \n(Buxoro, O`zbekiston)  \nINFORMATIKA  FANI  HAMDA  DASTURLASHDA  MATEMATIKANING  AHAMIYATI  \nRezyume . Maqolada  informatika  fani hamda  dasturlashda  matematikaning  \nahamiyati  haqida ma’lumot berilgan.  \nKalit  so`zlar : metematika,  informatika,  kompyuter,  dasturlash,  ta’lim  tizim.  \n \nInsoniyat  o`zining  rivojlanishi  tarixi  mobaynida  modda,  quvvat  va axborotlarni  \no`zlashtirib kelgan. Bu rivojlanishning butun bir davrlari shu bosqichning ilg`or texnologiyasi  \nnomi bilan atalgan. Masalan: «Tosh asr» - mehnat quroli yasash uchun toshga ishlov berish  \ntexnologiyasini egallash bosqichi, «kitob chop etish asri» - axborotni tarqatishning yangi usulini  \no`zlashtirish bosqichi, «elektr asri» - quvvatning yangi turlarini o`zlashtirish bosqichi shular  \njumlasidandir.  Bundan  20-30 yillar  ilgari  «atom  asri»  boshlandi  deyilgan  bo`lsa,  hozirgi  kunda  \n«axborot asri», «EHM asri» deb ataladi. «Informatika va axborot texnologiyalari» fanining kelib  \nchiqishi, uning uch tarkibiy qismi algoritm, dastur va hisoblash vositalarini paydo bo`lishi va  \nrivojlanishi bilan bog`liq. Informatika axborotlarni EHM yordamida tasvirlash, saqlab turish,  \nuzatish va ishlov berish usullarini o`rganadigan fandir.  Davrimizning o`ziga xos xususiyatlaridan  \nbiri bo`lgan  axborot  kommunikatsiya  texnologiyalarining  jadal  rivojlanishi  bilan  uning  \nimkoniyatlaridan foydalanib, ta’lim jaray oniga yangicha yondashish va uni tashkil etishga  \nalohida e’tibor qaratilmoqda. XXI asr — yuksak texnologiyalar asri bo`lib, zamonaviy yoshlarimiz  \nnafaqat  davr  ruhiga  monand,  balki  elektron  olamdagi  taraqqiyotga  muvofiq  qadam  \ntashlamoqdalar.  Shu boisdan  ham,  yosh  avlodga  ta’lim  berish  jarayoniga  boshqacha  \nyondashishni  talab  etmoqda.  \nTaraqqiyot  va rivojlanish  bor joyda  dasturlash  tillari  rivojlanishdan  to`xtamaydi.  \nBugungi kunda ma’lumotlarni ifodalash shakli, tuzilmasi va ishlash tamoyillari avvalgilaridan  \nbutunlay farq qilib, imkoniyatlari juda ham boy bo`lgan yangi dasturlash tillari shakllandi va  \navvalgilariga nisbatan bir qator afzalliklarga ega bo`lgan yangi versiyalar hisobiga rivojlanib  \nbormoqda.  Ma’lumki  hozirgi  kunda  dastur  va dasturlash  asoslariga  bo`lgan  ehtiyoj  yildan  yilga  \noshib  bormoqda.  Chunki  dasturlar  hozirgi  kunda  insonlar  ishini  yengillashtirish  hamda  \ndaromad manbaiga aylanib bormoqda. Dunyo bo`yicha hozirgi kunda dasturlar eng qimmat  \nmahsulot  sifatida e’tirof etiladi. Informatika va dasturlashni o`qitish sohasida N. Virtning  \nma’ruzasi  alohida  diqqatga  sazovor.  Unda  zamonaviy  informatika  fanini  o`qitishda  yuzaga    «POLISH  SCIENCE  JOURNAL»   \n27  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \nkelayotgan muammolar, ayniqsa, qaysi tilni va qanday o`qitish lozimligi birinchi o`ringa olib  \nchiqilgan. Muallif dasturlash asoslarini o`qitish uchun darslik loyihasini taklif qiladi. Uning  \nta’kidlashicha, dasturlash asoslarini to`la yorituvchi bunday darslik hozircha yaratilgani yo`q va  \nbunday darslik yaratishning iloji ham yo`q. Chunki, to darslik yaratilib, ta’lim tizimiga joriy  \nqilinguncha,  raqobatchilar  bilan  kurashda  orqada  qolmaslik  uchun  dasturiy  vositalarni  ishlab  \nchiquvchi kompaniyalar yangi dastu rlash texnologiyalarini yaratib ulguryaptilar. Shu o`rinda  \nmashxur shaxslardan biri Stiv Jobsning so`zlarini keltirib o`tmoqchiman “Mamalakatdagi har bir  \nbola dasturlashni o`rganishi kerak chunki bu ularni mantiqiy fikrlashga o`rgatadi”. Zamon  \ntexnika rivojlanishi natijasida dasturlash deyarli barcha sohalarga kirib bordi. Dasturlashni  \nrivojlantirish  kerak  chunki dasturlash  matematika  singari  anchayin  murakkab  jarayon.  \nO`zbekistonda esa matematika azaldan yaxshi rivojlangan. Respublikamizda “Bir millio  \nn dasturchi”  online  o`quv kursi  tashkil  etilgan.  \nFoydalanuvchi kompyuter bilan muloqat qilish uchun kompyuter “tili” ni bilishi ham  \ntalab  qilinadi.  Kompyuter  tushunadiga  “til” dasturlash  tili deb ataladi.  Biror  masalani  \nkompyuterda yechish uchun avvalo, uning algoritmi tuzilishi va bu algoritmni kompyuter  \ntushunadigan ko`rsatmalar va qonun -qoidalar asosida yozilisi kerak bo`ladi. Bu yozuv dastur  \nbajarishi mumkin bo`lgan ko`rsatmalarning izchil tartibidan iborat ekan. Kompyuter uchun  \ndastur tuzish  jarayoni  dasturlash  va dasturni  tuzadigan  kishi  dasturchi  deb ataladi . Hisoblash  \nmashinasiga  algoritm  dastur  shaklida  beriladi.  Bitta  masalani  yechishning  bir necha  algoritmi  \nmavjud bo`lishi mumkin. Ular orasida eng samaralisini, bajarilishi uchun eng kam amallar,  \nmashina vaqti, xotira va h.k.ni talab qiluvchi algoritmni tanlash lozim. Samarali algoritmlar  \nmavjud bo`lishi shartlari va ularni qurish (ishlab chiqish)ni o`rganish algoritmlar nazariyasi  \nasosini tashkil etadi. Hozirgi kunda matematika fanini yaxshi o`zlashtirgan va chet tillarini  \nyaxshi  bilgan  o`quvchilar  dasturni  qolgan  o`quvchilarga  nisbatan  tez va oson  tarzda  o`zlashtira  \noladilar. Bunga sabab dasturdagi algoritmlar ketma -ketligi matematik formula va hiso b-kitoblar  \nbilan bog`liq. Shu o`rinda bir savol tug`iladi, hisob -kitobni bilmagan o`quvchi qanday qilib  \nalgoritm tuza olishi mumkin. Dasturlash matematikaga bog`liq lekin farqi ham bor dasturlashda  \nrasm chizish, musiqa bastalash kabi o`zi xohlaganday ijod qilish mumkin. Dastur matni esa  \ningliz tilida ifodalanadi. Darhaqiqat, ta’lim sifatini, samaradorligini oshirish, o`quvchilarni  \nqiziqtiradigan  o`yin  shaklidagi  darslarni  maqsadli  tashkil  etishda  zamonaviy  pedagogik  \ntexnologiyalar hamda axborot -kommunikatsi ya vositalarining roli juda katta. Ularni qo`llash  \nimkoniyatlari ko`lami kundan -kunga kengayib, texnik vositalar o`quv jarayonining ajralmas  \nqismiga aylanib bormoqda. O`quvchilarga fanni kompyuter orqali o`rgatish uchun birinchi  \nnavbatda o`qituvchining o`zi kompyuter texnologiyalaridan professional darajada foydalana  \noladigan  bo`lishi  kerak.  Bu darajadagi  foydalanishga  nafaqat  kompyuterning  mavjud  \nimkoniyatlaridan to`liq foydalana olish, balki u asosida yangi o`quv dasturlarini tashkil eta olish  \nham  kiradi.  Bunday  darajadagi  pedagog  bo`lish  uchun  har bir o`qituvchi  zamonaviy  \ntexnologiyalar sohasi bo`yicha o`z ustida ko`proq ishlashi zarur. Fanlarni kompyuter yordamida  \no`rgatishning  an’anaviy  o`rgatish  usullariga  nisbatan  bir necha  ustunlik  jihatlari  bor. \nDasturchilikka  qiziquvchi  har bir inson  matematika  fanini  mukammal  bilishlari  lozim.    «POLISH  SCIENCE  JOURNAL»   \n28  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \nFOYDALANILGAN  ADABIYOTLAR:  \n1. Muslimov  N.A,  Usmonboyeva  M.H.,  Sayfurov  D.M.,  To`rayev  A.B. “Innovatsion  ta`lim  \ntexnologiyalari”  – T.: “Sano  standart”  nashriyoti,  2015.  \n2. Бердиева  С.М.,  Имомова  Ш.М.  Использование  инновационных  технологий  на уроках  \nинформатики//  Наука, техника  и образование. 2018.  № 10 (51).  С. 28-31. \n3. Бердиева  С.М.,  Имомова  Ш.М.  Построени e двухмерных  графиков  на уроках  \nинформатики  средствами  Excel //ТЕОРИЯ  И ПРАКТИКА  СОВРЕМЕННОЙ  НАУКИ.  2017.  \n№12(30).    «POLISH  SCIENCE  JOURNAL»   \n29  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \nSECTION:  MEDICAL  SCIENCE  \n \n \nАбдурашидов Мирзоулугбек Мамашокирович,  \nХудойбердиева Хамрохон Тилаволдиевна,  \nКосимов  Дилмурот  Сирожитдинович  \n(Андижан,  Узбекистан)  \n \nСОСТОЯНИЕ  ВЕГЕТАТИВНОГО  ГОМЕОСТАЗА  \nУ ДЕТЕЙ  С ЮВЕНИЛЬНЫМ  ХРОНИЧЕСКИМ  АРТРИТОМ  \n \nАннотация. Явилось оценить состояние вегетативного гомеостаза у больных  \nювенильным  ревматоидным  артритом.  \nКлючевые слова: кардиоинтервалография, клиноортостазе, ревматоидного  \nартрита,  ЭКГ, ортостаз,  ювенильным  ревматоидным  артритом  (ЮРА)  \n \nResults of vegetative nervous system (VNS) estimation in 129 patients with different  \ntypes  of juvenile  chronic  arthritis  were  showed.  We have  been  established  that  the regulation  \nof cardiac rhythm depend on vegetative tone condition. In the initial en - and vagotonia the  \nhumoral conture increases. In the initial sympathiconothia the parasympathetic part of VNS  \ngrow  weak  positive  advanus  in the vegetative  homeostasis  appearence  in boys  with  joints  form of  \ndisease  after specific  therapy.  \n \nЦель исследования. Изучение состояния вегетативного гомеостаза у больных  \nювенильным  хроническим артритом  \nМатериалы и методы. Представлены результаты оценки вегетативной нервной  \nсистемы  у 129 больных  с различными  клиническими  формами  ювенильного  \nхронического артрита. Больные, кроме общей программы обследования (изучение  \nжалоб, анамнестических данных и объективных данных - биохимических, клинико - \nинструментальных  и иммунологических),  обследовались  по унифицированной  и \nстандартизированной методике исследования вегетативной нервной системы: таблицы,  \nкардиоинтервалография  (КИГ),  клино -ортостатическая  проба (КОП).  \nРезультаты.  Установлено,  что регуляция  сердечного  ритма  у больных  зависит  от \nисходного  вегетативного  тонуса.  При исходной  эй- и ваготонин  происходят  \nнеблагоприятные  изменения  в ритме  сердца  - усиление  влияния  гуморального  контура  \nна фоне уменьшении влияния нервных механизмов в организации ритма сердца. В  \nотличие от больных с эй - и ваготонией при исходной симпатикотонии происходит  \nистощение парасимпатического отдела ВНС, значительное усиление нервного контура  \nрегуляции  ритма  сердца,  что свидетельствует  о наступлении  “аварийной”  фазы  в \nкомпенсаторных возможностях организма по обеспечению и организации ритма сердца.  \nПолученные результаты требуют дальнейшей разработки методов лечения ЮХА в плане  \nкоррекции  вегетативного  гомеостаза  с применением  вегетотропных  препаратов.    «POLISH  SCIENCE  JOURNAL»   \n30  \n SCIENCECENTRUM .PL ISSUE  2(47)  ISBN  978-83-949403 -4-8 \n \nКак известно, при заболеваниях суставов участие нейрогуморальных факторов  \nопределяется обшебиологическими закономерностями, связанными с их адаптационной  и \nрегуляторной функциями [1, 4]. В развитии иммунологического дисбаланса. кроме  \nнарушений во взаимоотношениях иммунокомпетентных клеток и эндокринной системы,  \nнемаловажную  роль  играют  и гормональные  факторы  (адренокортикотропный  гормон,  \nкортизол),  непосредственно  влияющие  на активность  симпато -адреналовой  и \nгипоталамо -гипофизарно -надпочечниковой  систем  [2, 5, 6]. Однако  до сих пор в \nлитературе не освещено состояние вегетативного гомеостаза у больных ювенильным  \nхроническим артритом (ЮХА), что послужило основанием для проведения настоящего  \nисследования.  \nМатериалы  и методы  \nВ ходе  исследования  обследованы  129 детей  с различными  клиническими  \nформами ЮХА. Больные, кроме общей программы обследования (изучение жалоб,  \nанамнестических  данных  и объективных  данных  - биохимических,  клинико - \nинструментальных  и иммунологических),  обследовались  по унифицированной  и \nстандартизированной методике исследования вегетативной нервной системы: таблицы,  \nкардиоинтервалография (КИГ), клино -ортостатическая проба (КОП) -рассматривались  \nвегетативный  тонус,  реактивность  и обеспеченность  11, 2]. При  исследовании  \nпоказателей  вегетативного  обеспечения  деятельности  (ВОД)  мы использовали  данные,  \nполученные Н.А. Белоконь с соавт. [1], с учетом региональных особенностей ВОД,  \nсвязанных  с возрастом,  полом  и данными  физического  обследования  больных  [7]. \nРезультаты  и обсуждение  \nУ больных  ЮХА  наряду  со специфическими  признаками  болезни  имелись  \nжалобы,  характеризующие  нарушения  вегетативного  гомеостаза:  утомляемость,  \nнарушение сна, потливость ладоней и подошв, сниженное настроение и апатия, головные  \nболи и ухудшение памяти и зрения и т. д. Эти жалобы клинически сочетались с такими  \nсимптомами, как гиперемия лица, ладоней и подошв, похолодание конечностей, высокая  \nчастота  красного  дермографизма,  усиленная  пульсация  шейных  сосудов,  \nприглушенность  и расщепление  тонов  сердца,  систолический  шум  на верхушке  и \nосновании  сердца.  Естественно,  отличить  эти жалобы  и клинические  симптомы  от \n«истинных» вегетативных нарушений или обусловленных вторичными нарушениями  \nвысших  отделов  ЦНС  достаточно сложно.  \nРезультаты  исследования  исходного  вегетативного  тонуса  в условиях  \nотносительного  покоя  показали  значительное  преобладание  случаев  исходной  \nсимпатикотонии у 73 детей (56,6%; Р<0,001), уменьшение удельного веса эйтонии у 34  \nдетей (26,4%; Р<0,01) и ваготонин у 22 больных (17,0%; Р<0,05) по сравнению со здоровой  \nпопуляцией данного региона [7] (эй -, ваго - и симпатикотонии соответственно в 44,9, 22,3  \nи 32,8%).  \nУчитывая,  что характеристика  исходного  вегетативного  тонуса  по индексу  \nнапряжения  (ИН1)  в условиях  относительного  покоя  не всегда  точно  отражает  истинное  \nсостояние  вегетативного  тонуса,  мы придавали  особое  значение  изменению  \nвегетативного тонуса  при ортопробе  (КИГ).    «POLISH  SCIENCE  JOURNAL»   \n31  \n SCIENCECENTRUM .PL ISSUE  2(47)  ISBN  978-83-949403 -4-8 \n \nУ больных  ЮХА  исходный  вегетативный  тонус  эйтонии  (ИН 1=30,0  - 90,0  усл.ед.)  \nи ваготонин  (ИН1<30,0  усл.ед.)  относительного  покоя  поддерживались  благодаря  \nчрезмерному напряжению в парасимпатическом отделе ВНС. Это свидетельствует о том,  \nчто у 1 /2 детей с ЮХА исходные эй - и ваготонин имели изначально дистонический  \nхарактер.  У больных  ЮХА  при ортопробе  уменьшаются  значения  ДХ и Мо (ЧСС  \nучащается), увеличиваются показатели АМо, ВПР, ИН2 и производные АМо, Мо и ДХ  \n(Р<0,05 - 0,001). При этом выраженные сдвиги выявляются у больных с исходной эй - и \nваготонией, чем при симпатикотонии, что согласуется с законом исходного значения у  \nобследованных здоровых детей: чем ниже исходный уровень работы, тем больше сдвиги.  \nЭто подтвер ждается  также  выраженным  увеличением  соотношения  ИН2/ИН1  при \nэйтонии  (6,327±0,59;  Р<0,01)  и ваготонии  (7,427+0,57;  Р<0,001),  чем при симпатикотонии  \n(2,477±0,08).  Комплексное  лечение  больных  ЮХА  приводило  к определенным  \nблагоприятным сдвигам в исходном вегетативном тонусе, в частности к уменьшению  \nчисла случаев симпатикотонии от 56,6 до 43,5% (Р<0,01) и увеличению удельного веса  \nэйтонии  - от 26,4  до 43,5%  (Р<0,001).  Однако  динамика  исходного  вегетативного  тонуса  \nво время  ортостаза  у больных  эй- (17,1%;  Р>0,05),  ваго - (9,3%;  Р>0,05)  и симпатикотонии  \n(73,6%; Р>0,05) не подтвердила «благоприятность этих двух сдвигов» по сравнению с  \nудельным  весом  исходного  вегетативного  тонуса  при ортостазе  до лечения  \n(соответственно 17,8; 3,88 и 78,3%). Следовательно, мероприятия, направленные на  \nспецифическую  терапию,  оказывают  временный и  нестойкий  эффект.  \nВ этом плане представляет интерес изучение вегетативной направленности детей  \nс ЮХА  на ортонагрузку.  \nДанные исследования показывают, что в целом больные ЮХА лишь в 34,1%  \nслучаев  (у здоровых  77,7%;  Р<0,001)  на ортостатическую  нагрузку  реагировали  \nнормальными значениями (ИН2/ИН1) и в 65,9% случаев (у здоровых - 22,3%; Р<0,01)  \nреакции  были  патологическими  (у 56,6%  больных  гиперсимпатикотоническая  и у 9,3% - \nасимпатикотоническая реакции), что значительно отличается от данных здоровых детей  \n(соответственно 12,1%; Р<0,001 и 10,2%; Р>0,05). Сравнительный анализ вегетативных  \nреакций  у больных  ЮХА  показал  снижение  нормальных  реакций  при эй- (Р<0,001),  ваго - \n(Р<0,01)  и симпатикотонии  (Р<0,001)  по сравнению со здоровыми  детьми данного  \nрегиона, у которых вегетативные реакции наблюдались соответственно в 81,7, 71,9 и  \n76,9%. Среди больных значительно увеличены гиперсим - патикотонические реакции: при  \nэйтонии  соответственно  50,0%,  ваготонии  - 50,0%  и симпатикотонии  - 61,6%,  что \nсущественно отличается от данных здоровых детей (соответственно 10,4%; 22,8% и 7,1%).  \nСлучаи  асимпатикотонических  реакций  у больных  при эйтонии  встречались  чаще  (17,6%  \nпротив 7,8% у здоровых; Р<0,05) и реже - при исходной симпатикотонии (8,22% против  \n16,7% у здоровых  детей; Р<0,05).  \nУ больных с исходной симпатикотонией наблюдаются дальнейшее усиление  \nгуморального  контура  (Р<0,001),  максимальное  снижение  показателя  ДХ (Р<0,01),  \nзначительное  увеличение  показателей  ВПР и ИН1  (Р<0,01)  по сравнению  со здоровыми  \nдетьми  (Мо=0,607±0,06  сек.,  ДХ=0,167±0,05сек.,  ВП  Р= 1 1,37±0,43  усл.ед.;  ИН1  = \n179,707±7,05  усл. ед). При этом  необходимо  отметить,  что уменьшение  показателя  X до \nнизких  значений  (0,0957±0,004  сек.)  на фоне  высоких  показателей  ВПР,  ИН1    «POLISH  SCIENCE  JOURNAL»   \n32  \n SCIENCECENTRUM .PL ISSUE  2(47)  ISBN  978-83-949403 -4-8 \n \nсвидетельствует  о состоянии  предельного  напряжения  и ухудшении  качества  регуляции  \nкровообращения  - наступлении  «аварийной»  фазы  в регуляции  ритма  сердца.  \nУвеличение отношений АМо/Мо (Р<0,05) и АМо/ ДХ(Р<0,01) по сравнению со здоровыми  \nдетьми  (50,37±2,14  и 208,17±7,1  соответственно)  свидетельствует  об истощении  \nкомпенсаторных возможностей  парасимпатического  отдела ВНС  и о значительном  \nусилении  центрального  (нервного)  контуров  и увеличении  степени  централизации  \nуправления  сердечным ритмом.  \nИтак,  регуляция  сердечного  ритма  у больных  ЮХА  зависит  от исходного  \nвегетативного тонуса. Так, при исходной эй - и ваготонии происходят неблагоприятные  \nизменения в ритме сердца, выражающиеся более высоким сердечным ритмом, низкими  \nприспособительными регуляторными механизмами - усилением влияния гуморального  \nконтура  на фоне  уменьшения  влияния  нервных механизмов в  организации  ритма  сердца.  \nВ отличие от  больных  с эй- и ваготонией при исходной симпатикотонии  происходит  \nистощение  парасимпатического  отдела  ВНС,  значительное  усиление  нервного  контура  \nрегуляции  ритма  сердца,  что свидетельствует  о наступлении  «аварийной»  фазы  в \nкомпенсаторных возможностях организма по обеспечению и организации ритма сердца.  \nТаким  образом,  больные  ЮХА  со стороны  вегетативной  реактивности  в 56,6%  \nслучаев характеризуются  гиперсимпатикотонической реакцией. При  этом в менее  \nвыгодных  условиях  находятся  дети  с исходной  симпатикотонией.  \nГиперсимпатикотонические реакции у этих детей осуществляются за счет значительного  \nувеличения ВПР (39,1+1,64 усл. ед.; Р<0,01), ИН2 (804,2±38,2 усл. ед.; Р<0,001) при условии  \nменее выраженных  компенсаторных парасимпатических реакций  - низкие резервы  АХ \n(-23,3% от исходного) по сравнению с детьми, имеющими исходные эй - (-41,4%; Р<0,01)  и \nваготонин  (-69,5%;  Р<0,001).  Эти данные  свидетельствуют  о чрезмерной  напряженности  в \nсимпатическом отделе ВНС и истощении функции парасимпатического отдела, что  \nсвидетельствует о развитии «стадии истощения» в гипоталамо -гипофизарной и симпато - \nадреналовой  системе,  что требует  целенаправленний  терапии  с учетом  вегетативных  \nреакций  детей,  больных  ЮХА.  \n \nСПИСОК  ИСПОЛЬЗОВАННЫХ  ИСТОЧНИКОВ:  \n1. Белоконь  Н.А.,  Кубергер  М.Б.  Болезни  сердца  и сосудов  у детей.  М., Медицина.  1987.  \n2. Вейн А.М., Соловьева А.Д., Колосова О.А. Вегетососудистая дистония. М., Медицина.  \n1981.  \n3. Насонова  В.А.,  Астапенко  М.Г.  Клиническая  ревматология.  М., Медицина.  1989.  \n4. Прашева В.И. Функциональная специализация пейсмеккер -систем сердца. Успехи  \nфизиол.науки.  1998;(29):3:79 -91. \n5. Ситдиков Ф.Б. Адренергические и холинергические факторы регуляции сердечно - \nсосудистой системы в онтогенетическом развитии. Бюллетень экспериментальной  \nбиологии  и медицины. 1998;(26):26:318 -320.  \n6. Кузнецов  С.В. Состояние  первичного  кардиального  ритма  у новорожденных.  \nБюллетень  экспериментальной  биологии  и •глицины.  1984;(22):4:422 -424.    «POLISH  SCIENCE  JOURNAL»   \n33  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \n7. Яхудаев  Э.М.  Исследование  вегетативного  гомеостаза,  сердечно -сосудистой  \nреактивности  и факторов  риска  атеросклероза  у вельских  школьников  с \nвегетососудистой дистонией гипер -и гипотонического типа. Автореф. канд. мед. наук.  \nТашкент.  1991:24.    «POLISH  SCIENCE  JOURNAL»   \n34  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \nБугаевский К. А.  \nкандидат  медицинских  наук,  доцент,  \nЧерноморский  государственный  университет  имени  Петра  Могилы  \n(Николаев, Украина)  \nТРАНСПЛАНТОЛОГИЯ  В ОТРАЖЕНИИ  СРЕДСТВ  КОЛЛЕКЦИОНИРОВАНИЯ  \nАннотация. В статье представлены материалы исследования, касающиеся  \nизучения отражения  данных о  трансплантологии и  известных  врачах -трансплантологах,  \nв таких  средствах  коллекционирования,  как филателия  (почтовые  марки,  конверты,  \nблоки, почтовые карточки), нумизматика (памятные монеты и медали) и фалеристика  \n(наградные и памятные знаки и значки). Текст статьи снабжён сопроводительным  \nиллюстративным  материалом, пояснительными описаниями  и комментариями.  \nКлючевые слова: трансплантология, врачи -трансплантологи, филателия,  \nпочтовые марки, почтовые конверты, тематические почтовые блоки, памятные и  \nнастольные  медали.  \n \nБугаєвський  К. А. \nкандидат  медичних  наук,  доцент  \nЧорноморський  державний  університет  імені  Петра  Могили  \n(Миколаїв,  Україна)  \n \nУ статті представлені матеріали дослідження, що стосуються вивчення  \nвідображення даних про трансплантологію та відомих лікарів -трансплантологів, у таких  \nзасобах колекціонування, як філателія (поштові марки, конверти, блоки, поштові картки),  \nнумізматика (пам'ятні монети та медалі) та фалеристика (нагородні та пам'ятні знаки та  \nзначки).  Текст  статті  має супровідний  ілюстративний  матеріал,  пояснювальні  описи  та \nкоментарі.  \nКлючові слова: трансплантологія, лікарі -трансплантологи, філателія, поштові  \nмарки,  поштові  конверти,  тематичні  поштові  блоки,  пам'ятні  та настільні  медалі.  \n \nBugaevsky K.A. \nCandidate  of Medical  Sciences,  Associate  Professor  \nPetro  Mohyla  Black  Sea State  University  \n(Mykolaiv, Ukraine)  \nTRANSPLANTOLOGY  IN THE REFLECTION  OF COLLECTIBLES  \nAnnotation.  The article  presents research materials related to the  study  of the \nreflection  in collectibles,  the necessary  information  about  the reflection  in modern  collectibles,  \nsuch  as philately  (postage  stamps,  envelopes,  blocks,  postcards),  numismatics  \n(commemorative  coins  and medals)  and faleristics  (award  and commemorative  signs  and   «POLISH  SCIENCE  JOURNAL»   \nSCIENCECENTRUM.PL  ISSUE  2(47)  ISBN  978-83-949403 -4-8 \n35  \n  \nbadges). The text is supplied with a rich illustrative material, explanatory descriptions and  \ncomments.  \nKey words: , transplantology, transplant doctors, philately, postage stamps, postal  \nenvelopes,  themed  postal blocks,  commemorative and  table medals.  \n \nВведение. Современная медицинская наука многосторонняя и разнообразная.  \nОдним  из самых  молодых  и перспективных  направлений современной  медицины,  \nявляется  трансплантология,  которая  активно  развивается  во многих  странах  мира.  \nНаиболее  широко  известным  и применяемым  методом  донорства,  является  донорство  и \nпереливание  (трансплантация  крови  как органа  человека)  крови  и её компонентов.  Но \nрассказ  об этом  виде  трансплантации  требует  отдельной  статьи,  а возможно  и не одной.  \nЦель  статьи:  представить  отражение  в современных  средствах  \nколлекционирования, таких  как филателия,  фалеристика  и нумизматика, сведения об  \nряде  известных  врачах  и учёных -трансплантологах  и современной  трансплантологии.  \nМатериалы и  методы.  При проведении  данного исследования,  нами  были  \nактивно  использован  литературно -критический  анализ  доступных  отечественных  и \nзарубежных  источников,  таких,  как справочники,  научные  статьи  и интернет.  \nОсновная  часть  статьи.  В данной  статье,  по результатам  проведённого  \nисследования, посвящённого отражению в средствах коллекционирования, данных о  \nряде отечественных и  зарубежных учёных и  врачах –трансплантологах и  состоянии  \nсовременной трансплантологии, речь пойдёт о пересадке различных человеческих  \nорганов.  \nСвой  рассказ,  хотелось  бы начать,  с представления  известного  научного  и \nпрактического  вклада  всемирно  известного,  советского  и российского  врача - \nтрансплантолога,  академика  Шумакова  Валерия  Ивановича  (1931 -2008),  директора  НИИ  \nтрансплантологии  и искусственных  органов  РФ, автора  первой  пересадки  сердца  в СССР  \n(1968), в отражении средств филателии и фалеристики. Так, на рис. 1, представлены  \nпочтовые марки  Российской Федерации  (2012),  и памятный  значок  периода  СССР,  \nпосвящённых В.И. Шумакову и первой в СССР пересадке сердца, а также памятная  \nмедаль  в его честь  [1]. \n \n  «POLISH  SCIENCE  JOURNAL»   \nSCIENCECENTRUM.PL  ISSUE  2(47)  ISBN  978-83-949403 -4-8 \n36  \n  \n   \nРис.  1. Филателистические  и фалеристические  материалы,  посвящённые  Шумакову  В.И.  \n \nНа постсоветском  пространстве,  помимо  РФ, наиболее  активно  вопросы  \nтрансплантологии  развиваются  в Республике  Беларусь.  Почтовая  марка  и почтовый  блок  \nэтого  государства,  посвящённые  достижениям  в области  трансплантологии  и \nгематологии,  представлены на рис.  2 [4].  \n \nРис.  2. Филателистические  материалы Беларусии,  посвящённые  трансплантологии  \n \nДалее,  на рис.  3, представлена  большая  подборка  разнообразных  \nфилателистических материалов, посвящённых отражению памяти о великом южно - \nафриканском хирурге -трансплантологе Кристиане Нетлинге Барнарде (1922 -2001) и  \nпроведённой им 3 декабря 1967 года, первой в мире, удачной операции по пересадке  \nсердца пациенту [1]. Интересным является факт, что после встречи в Москве с В.П.  \nДемиховым,  Кристиан  Барнард,  называл  его истинным  отцом трансплантологии»  [2]. \n \n  «POLISH  SCIENCE  JOURNAL»   \nSCIENCECENTRUM.PL  ISSUE  2(47)  ISBN  978-83-949403 -4-8 \n37  \n  \n \nРис.  3. Филателистические  материалы,  посвящённые  первой  \nв мире  пересадке сердца  и Кристиану  Барнарду  \n \nПродолжая развитие темы операций по пересадке сердца, и вопроса о донорстве  \nэтого  органа,  на рис.  4, представлена  небольшая  подборка  филателистических  \nматериалов  разных  стран  мир  и годов  выпуска,  посвящённых  этой  важной  проблеме  \n[14, 17].  \n  «POLISH  SCIENCE  JOURNAL»   \nSCIENCECENTRUM.PL  ISSUE  2(47)  ISBN  978-83-949403 -4-8 \n38  \n  \n \nРис.  4. Филателистическая  подборка,  посвящённая  пересадке  сердца  \n \nДалее,  на рис.  5, представлена  небольшая  подборка  филателистических  \nматериалов (почтовые марки и конверты первого дня) разных стран мира, посвящённые  \nтрансплантологии, донорству человеческих органов, пересадке разных органов тела  \nчеловека,  и о пациентах,  живущих  с этими  пересаженными  органами  [19, 15]. \n \n  «POLISH  SCIENCE  JOURNAL»   \nSCIENCECENTRUM.PL  ISSUE  2(47)  ISBN  978-83-949403 -4-8 \n39  \n  \n \n  «POLISH  SCIENCE  JOURNAL»   \nSCIENCECENTRUM.PL  ISSUE  2(47)  ISBN  978-83-949403 -4-8 \n40  \n  \n \n  «POLISH  SCIENCE  JOURNAL»   \nSCIENCECENTRUM.PL  ISSUE  2(47)  ISBN  978-83-949403 -4-8 \n41  \n  \n \nРис. 5. Подборка филателистических материалов, посвящённая донорству органов и их  \nпересадке  \n \nНа рис. 6, представлена небольшая подборка почтовых марок, посвящённая  \nтрансплантации  печени у  детей  и у взрослых  пациентов  [11].  \n \nРис.  6. Пересадка  печени  в отражении  филателии  \n \nПодборка почтовых марок разных стран, представленная на рис. 7, посвящена  \nпроблеме  трупного  донорства  и пересадки  глазного  яблока  и роговицы,  нуждающимся  \nпациентам [9,  19]. \n  «POLISH  SCIENCE  JOURNAL»   \nSCIENCECENTRUM.PL  ISSUE  2(47)  ISBN  978-83-949403 -4-8 \n42  \n  \n \nРис.  7. Донорство  глаз  и роговицы в  отражении  филателии  \n \nНа рис. 8, представлены 2 почтовые марки (Малайзии и Греции), посвящённые  \nредкому  виду  трансплантационных  операций  – пересадке  верхней  и нижней  \nконечности  [8, 11, 19].  \nОтдельной,  небольшой  подборкой,  на рис.  9, представлены  филателистические  \nматериалы, посвящённые пересадке/трансплантации пациентам костного мозга [11, 19].  \nСреди почтовых марок, касающихся трансплантации костного мозга, существует марка  \nУругвая  (2008),  в память  о кончине  доктора  Роберто  Де Беллиса,  уругвайского  \nгематолога,  который  выполнил  первую  аутологичную  трансплантация  костного  мозга  в \nЮжной  Америке,  в 1985  году,  в Британской  больнице  в Монтевидео  [16, 19]. \n \nРис.  9. Пересадка  костного  мозга  в отражении  средств  филателии  \n \nНа рис.  10, представлены  почтовые  марки  таких  государств,  как Транскей,  \nМикронезия, Гвинея -Биссау и Швеция, посвящённые Алексису Каррелю (1873 -1944),  \nкоторый  был известным  французским  хирургом.  Он внёс  значительный  вклад  во многие  \nдостижения  в области  сосудистой  хирургии,  кардио -торакальной  хирургии  и \nтрансплантации органов. А. Каррель продемонстрировал, что кровеносные сосуды могут  \nбыть соединены встык, и впервые применил методику триангуляционного сшивания при  \nсосудистых анастомозах. Разработанные им методы, используются и по сей день. Он  \nнастаивал на важности абсолютной асептики в сосудистой хирургии, когда о такой  \nпрактике  почти  никто  не слышал.  Он также  считается  одним  из отцов  трансплантации  \n  «POLISH  SCIENCE  JOURNAL»   \nSCIENCECENTRUM.PL  ISSUE  2(47)  ISBN  978-83-949403 -4-8 \n43  \n  \nтвердых органов. А. Каррель был удостоен Нобелевской премии (1912), в знак признания  \nего работы.  Вместе  с Чарльзом  Линдбергом  он разработал  экстракорпоральный  \nперфузионный  насос  для поддержания  жизни  органов  вне человеческого  тела  [5- 7, 10, \n12, 13].  \nРис.  10. Коллекционные  материалы,  посвящённые  А. Каррелю  \nНа рис. 11, представлена подборка нумизматических материалов (памятные  \nмонеты  и медали),  посвящённые  Кристиану  Барнарду  и первой  операции  по пересадке  \nчеловеческого сердца  [1, 3, 18].  \n  «POLISH  SCIENCE  JOURNAL»   \nSCIENCECENTRUM.PL  ISSUE  2(47)  ISBN  978-83-949403 -4-8 \n44  \n  \n \nРис.  11. Памятные  монеты  и медали,  посвящённые  К. Барнарду  \nи первой  в мире  пересадке сердца  \nНа рис. 12, представлена почтовая марка Доминики (1990), посвящённая Джозефу  \nМюррею, одного из пионеров мировой трансплантологии. Благодаря новаторской работе  \nдоктора Джозефа Мюррея (1919 -2012) в 1950 -х и 1960 -х годах трансплантация почек  \nтеперь  проводится  во всем  мире  ежедневно.  Д. Мюррей  был первым  хирургом,  успешно  \nвыполнившим  трансплантацию  почки.  За свою  деятельностьпо  трансплантации  органов  \nпри лечении заболеваний человека Д. Мюррей был удостоен Нобелевской премии по  \nфизиологии  и медицине  в 1990  году [15].  \nРис.  12. Почтовая  марка,  посвящённая  Д. Мюррею  \nНа этом закончена  статья,  посвящённая  истории  трансплантологии  и её героям,  в \nотражении  таких  средств  коллекционирования,  как филателия,  нумизматика  и \nфалеристика.  \nВыводы:  1. В представленных,  в статье  подборках  филателистических,  \nнумизматических и фалеристических материалов, достаточно полно, информативно и  \nярко, представлены такое важные медицинские направления, как трасплантология и  \nдонорство  органов  человека.  \n2. Материалы,  как информационные,  так и иллюстративные,  могут  быть  \nиспользованы,  как форма  дополнительной  информации,  при изучении  таких  \nмедицинских дисциплин, как «Хирургия», «Урология», «Медицинская этика» и «История  \nмедицины».  \n \nСПИСОК  ИСПОЛЬЗОВАННЫХ  ИСТОЧНИКОВ:  \n1. Бугаевский  К.А. Кардиохирургия  и кардиохирурги  в филателии,  фалеристике  и \nнумизматике  / К.А. Бугаевский, Н.А.  Бугаевская  // Вестник  СМУС74.  – 2017.  – №3 (18).  \n– С. 42-49. \n  «POLISH  SCIENCE  JOURNAL»   \nSCIENCECENTRUM.PL  ISSUE  2(47)  ISBN  978-83-949403 -4-8 \n45  \n  \n2. Каледа В.И. Кристиан Барнард (1922 -2001) и его путь к пересадке сердца // ПКиК.  \n2017. №3s. – С. 92 -100. URL: https:// cyberleninka.ru/article/n/kristian -barnard -1922 - \n2001 -i-ego-put-k-peresadke -serdtsa  (дата  обращения:  31.07.2021).  \n3. Монеты на каждый день - Страница 325 - Основной нумизматический форум - \nЦентральный  Форум  Нумизматов  СССР  URL: https :// coins .su (дата  обращения:  \n31.07.2021).  \n4. Филателия Беларусь 2020 год, почтовые марки Беларуси, филателия URL: https :// \nПочтовые  марки (дата  обращения: 31.07.2021).  \n5. Alexis  Carrel  (1873 –1944):  Pioneer  of vascular  surgery  and organ...  URL:  https://  Semantic  \nScholar  (дата  обращения: 31.07.2021).  \n6. associazione malati di reni materiali utili URL: https:// Associazione Malati di Reni (дата  \nобращения:  31.07.2021).  \n7. Details about Medal Surgeon Alexis Carrel Anatomy Skeleton URL: https:// eBay (дата  \nобращения:  31.07.2021).  \n8. Dettaglio francobollo - catalogo completo dei francobolli italiani URL: https:// Dettaglio  \nfrancobollo  - catalogo  completo  dei francobolli  italiani  (дата  обращения:  31.07.2021).  \n9. Eye Donation Poster 012 – Drishti Connect by Antardrishti URL: https:// Drishti Connect  \nby Antardrishti  (дата обращения: 31.07.2021).  \n10. Lai Aida Alexis Carrel (1873 -1944): visionary vascular surgeon and pioneer in organ  \ntransplantation  // J Med  Biogr.  – 2014.  Aug;22(3):172 -5. \n11. Le Don D'organes. - PHILATÉLIE POUR TOUS URL: https:// PHILATÉLIE POUR TOUS (дата  \nобращения:  31.07.2021).  \n12. MEDICINE/ 1912 NOBEL PRIZE French Biologist ALEXIS CARR URL: https:// eBay (дата  \nобращения:  31.07.2021).  \n13. POFIS - Catalog - Products - The Bone Marrow URL: https:// Transplant Unit of the (дата  \nобращения:  31.07.2021).  \n14. SELLOS  FICCIÓN:  Día Nacional  del Trasplante  URL:  https://  SELLOS  FICCIÓN  (дата  \nобращения:  31.07.2021).  \n15. Tan Y.S. Joseph  Murray  (1919 –2012):  First  transplant  surgeon  / Y.S. Tan,  J. Merchant  // \nSingapore  Med  J 2019; 60(4): 162 -163.  \n16. The Gift of Life - Blood and Organ Donation in philately - Page 6 - Postage Stamp Chat  \nBoard  & Stamp  Forum  URL:  https://  Stampboards  (дата  обращения:  31.07.2021).  \n17. Timbre:  2004  LE DON  D'ORGANES  | WikiTimbres  URL:  https://  WikiTimbres  (дата  \nобращения:  31.07.2021).  \n18. Rand  (First  Heart  Transplant)  - South  Africa  URL:  https://  Numista  (дата  обращения:  \n31.07.2021).  \n19. 3227  / 32c Organ  and Tissue  Donation  PSA 1998  Fleetwood  FDC URL:  https://  First  Day \nCovers  Online  (дата обращения: 31.07.2021).    «POLISH  SCIENCE  JOURNAL»   \n46  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \nГофуров  Жавлонбек  Абдувахобович,  Худойбердиева  Хамрохон  Тилаволдиевна,  \nХашимова  Замира  Махмуджановна  \nАндижанский  государственный  медицинский  институт  \n(Андижан,  Узбекистан)  \n \nОЦЕНКА АНТРОПОМЕТРИЧЕСКИХ ПОКАЗАТЕЛЕЙ  \nФИЗИЧЕСКОГО  РАЗВИТИЯ СТУДЕНТОВ  \n \nАннотация. Изучение показателей физического развития студентов 1 го курса  \nАГМИ,  поступивших  в институт  в 2021  году,  и сравнение  их с аналогичными  \nпоказателями,  полученными  в 2011  году  при изучении  состояния  здоровья  \nпервокурсников.  \nКлючевые  слова:  соматометрия,  физическое  развитие,  медицинское  \nобследование,  масса  тела,  окружность  грудной  клетки,  коэффициент  корреляции.  \n \nЦель  исследования . Изучение  показателей  физического  развития  студентов  1 го \nкурса АГМИ, поступивших в институт в 2021 году, и сравнение их с аналогичными  \nпоказателями,  полученными  в 2011  году  при изучении  состояния  здоровья  \nпервокурсников.  \nМатериалы  и методы.  Обследованы  238 студентов  узбекской  национальности,  \nпроживающих  в г.Андижан  не менее  5 лет. Исследования  проведены  с использованием  \nметодов  соматометрии.  \nРезультаты.  Выявлено,  что показатели  физического  развития  юношей  \nзначительно превышали показатели девушек по всем изученным признакам. Сравнение  \nпоказателей физического развития студентов в динамике позволило выявить их рост у  \nсовременных  юношей  и девушек  по сравнению  с первокурсниками  2011  года.  \nИзучение  физического  развития  населения  представляет  теоретический  интерес  \nи имеет практическое значение, так как научно обоснованные данные о росте и развития  \nорганизма  позволяют  своевременно  проводить  целенаправленную  профилактическую  \nработу среди населения. Отклонения в уровне физического развития, как правило,  \nскрывают под собой различного рода заболевания. В связи, с чем недооценка этих  \nотклонений сказывается на состоянии здоровья не только в детском и подростковом  \nвозрасте,  но и при профессиональном  обучении  и дальнейшей  трудовой  деятельности.  \nПоэтому  в АГМИ,  как и в других  ВУЗах  республики,  оценка  физического  развития  \nстудентов  должна  стать  неотъемлемой  частью  ежегодного  углублённого  медицинского  \nобследования. При этом следует принимать во внимание данные наиболее современных  \nстатистических исследований, отражающих физическое развитие студентов высших  \nучебных заведений. При оценке физического развития принято пользоваться условными  \nнормами  (стандартами),  разрабатываемыми  на основе  проведения  массовых  \nобследований  различных  возрастно -половых  контингентов  населения.  Подобного  рода  \nнормативные  оценочные  таблицы  для детей  и подростков  от 7 до 17 лет были  \nразработаны сотрудниками  НИИ санитарии,  гигиены профзаболеваний  МЗ РУз (2). \nНаличие  подобного  рода  стандартов  физического  развития  является  одной  из   «POLISH  SCIENCE  JOURNAL»   \n47  \n SCIENCECENTRUM .PL ISSUE  2(47)  ISBN  978-83-949403 -4-8 \n \nпредпосылок правильной организации работы лечебно -профилактических учреждений,  \nобслуживающих  контингенты студентов.  \nМатериалы  и методы  исследования.  \nДля определения  уровня  физического  развития  студентов,  обучающихся  в \nмедицинском ВУЗе, проведено измерение длины и массы тела, окружности грудной  \nклетки  (ОГК).  Обследование  проводилось  по унифицированной  методике  исследования  \nи оценки  физического  развития  детей  и подростков(1).  Среди  обследованных  были  106 \nюношей  и 132 девушки.  Индивидуальная  оценка  физического  развития  студентов  \nпроведена  по шкалам  регрессии,  представленным  в виде  скрининга  таблиц.  (2). \nСтатистическая  обработка  материалов  проведена  методом  вариационной  статистики  с \nпомощью персонального компьютера. Были определены средние значения (М ср), ошибка  \nсреднего  значения  (m), среднеквадратичное  отклонение  (д) и критерий  достоверности  \nСтьюдента  (t). В разработку  вошли  карты  студентов  узбекской  национальности  в \nвозрасте  до 17 лет.  \nРезультаты  и обсуждение.  \nАнализ  полученных  данных  выявил,  что показатели  физического  развития  \nюношей  значительно  выше,  чем у девушек,  по всем  изученным  признакам.  \nСравнительный  анализ  результатов  осмотра  2011  года  и 2021  года  \nсвидетельствует  о том,  что параметры  физического  развития  юношей  и девушек  имели  \nзначительные различия. Так, в 2011 г. рост юношей превышал рост девушек на 8,3 см,  \nтогда  как в 2021  г. на 8,1 см: масса  тела - на 2,6 и 4,8 кг, ОГК  –на 3,4 и 4,3 см соответсвенно  \nв 2011 и  2021 гг.  \n \nСредние  значения  параметров  физического  развития  студентов,  М±m \nПоказатели  Юноши  Девушки  \n2011  г 2021  г Разность  2011  г 2021  г Разность  \nМ m М m М m М m  \nДлина  тела,см  169,8  0,6 173,8  0,6 4,0 161,5  0,5 165,7  0,4 4,2 \nМасса  тела,  кг 58,6  0,6 59,4  0,7 0,8 56,0  0,5 54,6  0,6 -1,4 \nОГК,  см 83,9  0,3 85,2  0,5 1,3 80,5  0,3 80,9  0,4 0,4 \n \nСравнение показателей физического развития в динамике позволило выявить их  \nрост у студентов, поступивших в институт в 2021 г., по сравнению с первокурсниками  \n2011г.  (см.таблицу).  У юношей  за десять  лет произошло  достоверное  увеличение  роста  на \n4 см (Р<0,001) и ОГК – на 1,3 см (Р<0,05). У девушек достоверно изменились в сторону  \nувеличения  показатели  роста  (на 4,2 см-Р<0,001)  и несколько  уменьшились  показатели  \nмассы  тела  (на 1,4 кг  - Р>0,05).  \nКоэффициент  корреляции  между  длиной  и массой  тела  у современных  юношей  \nравен 0,47, в 2011 г. - 0,62, что свидетельствует о средней степени связи между этими  \nпоказателями физического развития. У девушек коэффициент корреляции между ростом  и \nмассой тела в 2021 г. равен 0,54, в 2011г. —0,67, что также указывает на среднюю степень  \nвзаимосвязи. Коэффициент корреляции между ростом и ОГК колебался у юношей от 0,56  \n(2021г.) до 0,72 (2011г.) и указывает на среднюю и сильную степень связи. У девушек  \nкоэффициент  корреляции  между  этими  параметрами  физического  развития  в 2011  г.   «POLISH  SCIENCE  JOURNAL»   \n48  \n SCIENCECENTRUM .PL ISSUE  2(47)  ISBN  978-83-949403 -4-8 \n \nравен 0,31, в 2021 г. – 0,29, что говорит о средней и слабой степени взаимосвязи этих  \nпризнаков.  \nПри проведении комплексной индивидуальной оценки физического развития  \nстудентов - первокурсников 2021 года было установлено, 66,1% из них имели средний  \nуровень физического развития, выше среднего имели 14,1%, высокий – 3,1%: в 2011 г.  \nтакой  уровень  развития  имели  77%  юношей. Уровень  физического  развития  ниже  \nсреднего и низкий в 2021г. отмечался у 16,7% юношей, в 2011 г. такие юноши составляли  \n15 %.  \nУ 78,2 % девушек, обследованных в 2021 году, физическое развитие оценивалось  \nкак среднее,  выше  среднего  и высокое:  в 2011  г. 74% девушек  относились  к этим  группам  \nфизического  развития.  Физическое  развитие  ниже  среднего  и низкое  отмечено  у 15,5%  \nсовременных  девушек и у  13% - в 2011 году.  \nТаким образом, анализ результатов проведённого исследования свидетельствует  о \nстойком  росте  основных  показателей  физического  развития  студентов - \nпервокурсников, поступивших в 2021 году, по сравнению с аналогичными данными  \nстудентов 2011 года. Выявленный сдвиг показателей физического развития 17 летних  \nподростков  в сторону  увеличения  указывает  на процесс  акселерации,  который  \nнаблюдается  повсеместно,  в том числе  и у жителей  города  Андижан.  \nНастоящее  исследование  станет  основанием  для проведения  дальнейших  \nдинамических  наблюдений  за физическим  развитием  студентов,  обучающихся  в АГМИ.  \nВыводы.  \n1. Сопоставление результатов исследования показателей физического развития  \nстудентов  1-го курса  АГМИ  в возрасте  17 лет с данными  обследования,  проведённого  10 \nлет назад,  показывает,  что за этот  период  произошли  существенные  сдвиги  по \nпоказателю роста как у юношей, так и у девушек. Сдвиги по показателю массы тела у  \nюношей  были  незначительными,  тогда  как у девушек  они носили  отрицательный  \nхарактер,  что косвенно  может  указывать  на некоторую  «астенизацию»  девушек  в \nнастоящее  время.  \n2. Выявленный сдвиг по показателям длины тела и ОГК у 17 –летних подростков в  \nсторону  роста  указывает  на процесс  акселерации,  наблюдающийся  повсеместно,  в том \nчисле  и у жителей г.Андижан.  \n \nСПИСОК  ИСПОЛЬЗОВАННЫХ  ИСТОЧНИКОВ:  \n1. Анисимова Е.Н. Антропометрические характеристики и биохимические показатели  \nкрови  юношей  различных  типов  телосложения:  Автореферат.  дисс..  канд.  мед.  наук.  \nКрасноярск,  2004. 25  с. \n2. Апанасенко Г.Л. Физическое развитие детей и подростков. Киев: Изд.Здоровья, 1985.  \n77 с.  \n3. Блинова  Е.Г. Научные  основы  социально -гигиенического  мониторинга  условий  \nобучения студентов в образовательных учреждениях высшего профессионального  \nобразования:  Автореф. дисс..  мед.  наук, Москва, 2010.  46 с.  \n4. Бунак  В.В. Антропометрия:  Практический  курс.М.,  1941.  368 с.   «POLISH  SCIENCE  JOURNAL»   \n49  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \n5. Галкина  Т.Н. Антропометрические  и соматотипологические  особенности  лиц \nюношеского  возраста  в Пензенском  регионе:  Автореф.  дисс..канд.  мед.  Наук.  \nволгоград, 2008. 22 с.    «POLISH  SCIENCE  JOURNAL»   \n50  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \nSECTION:  PEDAGOGY  \n \n \nHojiyeva Nargiza Mardonovna  \nShofirkon  tuman  37-son davlat  maktabgacha  ta’lim  tashkiloti  direktori,  \nNe’matova Gulmira Sayfullayevna  \nShofirkon  tuman  16-son davlat  maktabgacha  ta’lim  tashkiloti  direktori  \n(Buxoro, O`zbekiston)  \nLOY VA PLASTILINDAN O`YINCHOQLAR  YASASHNING  TARBIYAVIY  AHAMIYATI  \nRezyume . Maqolada  loy va plastilindan  o`yinchoqlar  yasashning  tarbiyaviy  ahamiyati  \nhaqida  ma’lumot berilgan.  \nKalit  so’zlar : o`yinchoq,  loy, plastilin,  tarbiya,  tasavvur,  fikrlash.  \n \nMaktabgacha ta’lim muassasalari tarbiyachilari bolalarga o`yinchoq yasash uchun  \nko`pincha  qoroz  va qalam  beradilar -u, loy va plastilinni  ahyonda  bir tavsiya  etadilar.  Ular yana  \nshu mashg`ulotni  yetarli  darajada  baholamaydilar,  vaholanki,  bolalarning  loy va plastilin  bilan  \nbemalol ishlashi ularning ko`nikmalarini mustahkamlaydi. Bundan tashqari ana shu faoliyat  \nturiga  moyil  bolalar  o`z qiziqishlarini  qondirishlari  va o`z fantaziyalarini  ro`yobga  chiqarishlari  \nmumkin. Bu mashg`ulotlarda tarbiyachi ish jarayoniga kamroq aralashadi, bolaga mavzuni,  \ntasvirlash  texnikasini,  shuningdek,  ifoda  vositalarini  tanlashning  ko`p  masalalarini  mustaqil  hal \nqilish  imkoni  beriladi.  Mustaqil  faoliyat  bolalarni  tasvirlash  usullaridan  dadilroq  foydalanishga  \no`rgatadi.  Bundan  tashqari,  bolalar  loy va plastilin  o`yinchoqlar  yasashda  xuddi  rasm  \nchizishdagi kabi materialdan bemalol foydalanishga odatlanadilar. Bolalarga loy va plastilin  \no`yinchoqlar yasashni o`rta guruhda tavsiya etish ma’qul. Chunki bu guruhdagi bolalarda loy  \n(plastilin)dan foydalanish bo`yicha elementar ko`nikmalar bo`ladi va faoliyatning bu tu riga \nqiziqish  vujudga  keladi.  \nBolalar bog`chasida o`yinchoq yasash uchun asosiy material loydir va bo`yash vaqtlarda  \nham loydan foydalanishni tavsiya etish ma’qul. Bunda eng muhimi bolalar loy bilan ishlashning  \noddiy qoidalariga rioya qilishi: uni stollar ustiga, polga to`kib -sochib, surtib tashlamasligidir.  \nKatta guruhlarda har qanday materialni: loyni ham, plastilinni ham berish mumkin. Bu esa  \nmaterial tanlashda mustaqillikni vujudga keltiradi (kattaroq predmetlarni loydan, kichikroq  \npredmetlarni  plastilindan  yasash  ma’qul).  \nPlastilin bilan ishlayotgan paytda bolalar hajmni yaratish vazifasinigina emas, balki  \no`yinchoqqa rang tanlash bilan bogliq masalalarni ham mustaqil holda hal qiladilar. Ijodiy  \ntarzda  o`yinchoq  yasash  turlicha  xarakter  kasb  etadi.  U bitta  predmetning,  haykallar  \nguruhining, idishlarning, dekorativ plastinkalar yoki marjonlarning tasviri bo`lishi mumkin.  \nBolalar o`yinchoqni individual tarzda va guruhlarga birlashib yasashlari mumkin, bunda gap  \nbolalarning xohishida va tarbiyachining umumiy rahbarligidadir. O`yinchoq yasash taklifi  \nboladan yoki tarbiyachidan chiqishi mumkin. Bu ishni yaxshi ko`rib qolgan kichkintoylar  \nmateriallarni  o`zlari  oladilar  va ish boshlab  yuboradilar.  Lekin  tarbiyachi  bu ishga  o`yinchoqlarni    «POLISH  SCIENCE  JOURNAL»   \n51  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \nyaxshi yasay olmaydigan, yasashni unchalik yaxshi ko`rmaydigan boshqa bolalarni ham jalb  \netishi  mumkin.  U bolalarni  qiziqtirish  uchun  o`zi ham  o`yinchoq  yasash  jarayoniga  qo`shiladi.  \nBa’zi bolalarga muayyan mavzuni taklif qiladi. Masalan, bolalar birgalikda shug`ullanayotgan  \nvaqtlarida biror ertakning epizodini yoki magazin o`yini uchun sabzavot va ho`l mevalar  \nyasaydilar. Bunday o`yinlar bolalarni birlashtiradi va o`yinchoq yasashni hali yaxshi uddalay  \nolmayotgan  bolalar  o`rtoqlari  qanday  ishlayotganini  kuzatib,  ulardan  o`yinchoq  yasash  \nusullarini  o`rganadilar,  o`z ishlarini  boshqalarnikiga  solishtirib  ko`radilar,  ular bilan  \nmaslahatlashadilar  va asta -sekin mazkur faoliyatga  qiziqa  boshlaydilar.  \nBolalar  uchun  yozda  katta  imkoniyatlar  tug`iladi,  ular maydonchada  o`yinchoq  yasash  \nbilan mustaqil holda shug`ullanadilar, har xil predmetlarni: hayvonlarni, uy hayvonlarini,  \nhasharotlarni, idishlar va turli mazmundagi o’yinchoqlarni tasvirlaydilar. Bolalarning loy va  \nplastilindan  o`yinchoq  yasashi  — asosan  tasavvur  va fikrlash  bo`yicha  yasashdan  iborat  bo`lib,  \no`z qimmatiga  ega.  Chunki  bolalar  bunda  mustaqil  holda  mavzu  topadilar,  ishni  \nrejalashtiradilar,  tasvirlashning  tanish  usullaridan  foydalanadilar,  o`zlari  yangi  usullarni  o`ylab  \nchiqaradilar.  \nBolalar tasavvuri bo`yicha o`yinchoq yasash doimo predmet, uning shakli, mutanosibligi  \nhaqidagi bilimlarga asoslanadi. Bunday mashg`ulotlarda bolalar o`z ijodiy imkoniyatlarini  \nochadilar  va predmetni  tasvirlash  ko`nikmalarini  mustahkamlaydilar.  Tez-tez o`yinchoq  \nyasashga  odatlangan  bolalar  mavzuni  darrov  o`ylab  topadilar  — ba’zan  ularda  ishga  \nkirishmaslaridan oldin (masalan, sayrda ko`rganlari ta’siri bilan) biror g`oya vujudga keladi.  \nAyrim hollarda bola uni shu zahoti tarbiyachiga aytadi va kattalarning ishi - bolaning istagiga  \ne’tibor  bilan  qarash  va kechqurun  unga  shu istagini  eslatib  qo`yishdir.  Shunday  qilib,  \ntarbiyachilar bolalarning o`yinchoq yasashga qiziqishini qo`llab -quvvatlab, ularda faoliyatning  \nana shu turi bilan  shug`ullanish  istagini  kuchaytiradilar.  Loy va plastilindan  o`yinchoq  yasashni  \no`rgatishning muvaffaqiyati ko`p jihatdan mashg`ulotda bolalarga rahbarlik qilish qanday  \namalga oshirilishiga bog`liqdir. Bunda asosiy vazifa bolalarni ishning umumiy borishini buzmay,  \ng`oyani mustaqil holda hal qilishga yo`naltirishdan iboratdir. Masalan, biror idish yasalayotgan  \nvaqtda boladan «Sen vazaning bandini unga qanday usul bilan o`rnatasan?», «Mana bu  \ntog`orachaning hoshiyasini sen qanday rangga bo`yamoqchisan?», «Qani, o`ylab ko`r -chi, yana  \nqanday  oson  usul bilan  tuzdonga  oyoq  yasash  mumkin?», — deb so`raladi.  Agar  bola  omonat  \nturadigan  idish  yasagan  bo`lsa,  uni mustahkam  qilish  to`g`risida  o`ylab  ko`ris h tavsiya  etiladi.  \nYoki boshqa bir misol: tarbiyachi bola yasagan jirafa oyoqlarining bir xil uzunlikda bo`lib  \nqolganini  ko`radi.  Shunda  bolaga  “Bir o`ylab  ko`r-chi, jirafaning  oyoqlari  qanaqa  edi. Ularning  \nuzunligi bir xilmidi?” - deydi. Agar bola savolga to`g`ri javob bersa, tarbiyachi ishni tekshirib  \nko`rishni va zarur tuzatishlar kiritishni aytadi. Mabodo, bola savolga noto`g`ri javob bersa,  \ntarbiyachi xato nimadan iboratligini tushuntirib, figuraning mutanosibligini eslatadi, bundan  \ntashqari,  u jirafaning  rasmdagi  tasvirini  ko`rsatishi  ham  mumkin.  \nLoy va plastilindan yasalgan o`yinchoqning tarbiyaviy ahamiyati rang -barang va ko`p  \nqirralidir. O`yinchoq o`yinni yaratishda ishtirok etadi, bolaning o`ziga xos sherigi sifatida  \nnamoyon  bo`ladi,  uning  shaxsiga  katta  ta’sir  ko`rsatadi.  U bolaga  shunchalik  yaqinki,  bola  o`zi \nyasagan  o`yinchoqlarini  sevadi,  ularga  bog`lanib  qoladi,  shu tufayli  o`zini  atrofdagi  hayotning  \nto`laqonli  a’zosi  deb hisoblaydi.  Loy va plastilindan  yasalgan  o`yinchoqlar  bilan  o`ynash    «POLISH  SCIENCE  JOURNAL»   \n52  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \nbolalarda  avaylashni,  e’tiborlilikni,  mehribonlik  va nafosatni  uyg`otadi,  ijobiy  axloqiy  \nkechinmalar  tajribasini  to`plashga  yordam  beradi.  \n \nFOYDALANILGAN  ADABIYOTLAR:  \n1. Sakulina  N.P.,  Kamarova  T.S. Bolalar  bog`chasida  tasviriy faoliyat. - T., ”O`qituvchi”,  1996.    «POLISH  SCIENCE  JOURNAL»   \n53  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \nKamolova Azima  \nShofirkon  tuman  8-son davlat maktabgacha  ta’lim tashkiloti  tarbiyachisi,  \nQosimova  Sarvara  Bahodirovna  \nShofirkon  tuman  8-son davlat maktabgacha  ta’lim  tashkiloti  tarbiyachisi  \n(Buxoro,  O`zbekiston)  \nLOY VA PLASTILINDAN  O`YINCHOQLAR  YASASH  ORQALI  \nBOLALAR  IJODKORLIGINI RIVOJLANTIRISH  \nRezyume . Maqolada loy va plastilindan o`yinchoqlar yasash orqali bolalar ijodkorligini  \nrivojlantirish  haqida ma’lumot berilgan.  \nKalit  so’zlar : loy, plastilin,  rasm  chizish,  tasavvur,  o`yinchoq.  \nBolalar  ijodkorligini  rivojlantirish  pedagogikaning  dolzarb  muammosidir.  Chunki  \nvoqeligimiz yoshlarda tevarak -atrofdagi narsalarga ijodiy munosabatni, faollik va mustaqil  \nfikrlashni  shakllantirishni  taqozo  etadi.  Biz bolalarimizda  aql-idrokni,  o`tkir  zehnni,  \ntashabbusni,  tadbirkorlik va fantaziyani  tarbiyalamog`imiz kerak.  \nHozirgi  tadqiqotlar  maktabgacha  yoshdagi  bolalar  rasm  chizish,  loy va plastilin  \no`yinchoqlar yasashda ijodiy faoliyat ko`rsata olishlarini isbotladi. Albatta, bolalar ijodkorligi  \no`ziga xos xususiyatlarga ega bo`lib, kattalar ijodkorligi mezonlariga javob bera olmaydi.  \nBog`cha bolasi ham san’atkor singari rasm chizish, loy va plastilin o`yinchoqlar yasashda o`z  \ntajribasiga asoslangan g`oyani ro`yobga chiqarishga kirishadi. Bu g`oya mavzuning vujudga  \nkelishini, uni ifodalash vositalarini qidirib topishni o`z ichiga oladi. Bog`cha bolalarining rasm  \nchizishini  yoki loy va plastilin  o`yinchoqlar  yasashini  kuzatish  ularning  g`oyalari  yo kattalarining  \ntopshiriqlari munosabati bilan, yo bo`lmasa qandaydir bir obrazning, materialning be vosita  \nta’siri  bilan  vujudga  kelishini  ko`rsatadi.  Demak,  kattalar  ijodi  singari  bola  ijodkorligining  ham  \nbirinchi  bosqichi  atrofdagi  voqelikni  idrok  ilish asosida  g`oyaning  - mavzuning  vujudga  \nkelishidan  iboratdir.Lekin  badiiy  vositalarni  oldindan  qidirib  ko`rish,  o`z ishlarini  rejalashtirish,  \nuni o`ylab olish  bola  ijodi  uchun  xarakterli  emas.  Bola  loy yoki plastilindan  biror narsa  yasash  \nuchun  mavzu  tanlaganidan  keyin  o`sha  zahoti  uni tasvirlashga  kirishadi.  U tasvirlash  \njarayonidagina o`z ishining mazmuni to`g`risida fikr yuritadi, uni qanday bajarish, qanday  \nvositalardan foydalanysh to`g`risida o`ylaydi, ya’ni san’atkor asosan oldindan o`ylab qo`yadigan  \nbarcha masalalarni bola ish jarayonida hal qiladi. San’atkor ijodining davri hamma ishni aniqla b \nolishga  bog`liq  bo`lib,  g`oya  amalga  oshgunicha  davom  etadi,  bog`cha  yoshidagi  bolada  esa bu \ndavr g`oyaning amalga oshishiga to`gri keladi. Bolalar ijodkorligining o`ziga xos ana shu  \nxususiyati  ularda  analiz  va sintez  qobiliyati  yetarli  emasligi,  ular o`z ishlarini  oldindan  \nrejalashtira olmasligi bilan izohlanadi. G`oyani amalga oshirish uchun tasavvurdagi barcha  \nnarsalar ichidan eng xarakterlilarini ajrata bilish, yaratiladigan obrazni xayolan tasavvur eta  \nolish, tasviriy va texnik vositalarni tanlay bilish kerak. Bu ishlarning hammasi mukammal  \ntafakkur jarayonlarini talab qiladi. Biroq, loy va p lastilin o`yinchoqlar yasash bo`yicha olib  \nborilgan tadqiqotlar va bolalar bog`chalari tajribasi ko`rsatadiki, bolalarni tasviriy san’at  \nfaoliyatiga o`rgatish bo`yicha doimiy ish olib borilsa, ularni o`z ishini oldindan o`ylashga  \nodatlantirish  mumkin.  Bolada  ijod kurtaklarini  rivojlantirish  vazifalari  uning  yoshiga  xos \nanatomik -psixologik  xususiyatlariga  bog`liq  bo`ladi.    «POLISH  SCIENCE  JOURNAL»   \n54  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \nMaktabgacha  yoshdagi  bolaning  predmetlar  sifati  va xususiyatlari  to`g`risidagi  \ntasavvurining  to`laligi  uning  analizatorlari  qay darajada  ishlashiga  bog`liq  bo`ladi.  Tadqiqotlar  \n6-7 yashar  bolalarda  ko`rish  va sezish  idroki  yetarli  darajada  rivojlanishini  ko`rsatadi.  \nKattalarning tegishlicha yo`lga solishi bilan kichkintoy predmetning barcha asosiy sifatlarini:  \nshakli, mutanosibligi, rangi, tarkibiy qismlarini, shuningdek, fazodagi o`rnini idrok eta oladi.  \nBolalar  idrokining  ko`lamiga  analizatorlarning  rivojlanish  darajasidan  tashqari  kuzatilayotgan  \nobyektga  qiziqish  ham  ta’sir ko`rsatadi.  \nLoy va plastilindan  o`yinchoqlar  yasashda  bolalar  ijodkorligini  rivojlantirishning  asosiy  \nvazifalari quyidagilardan iborat: faoliyatning mazkur turiga qiziqishni rivojlantirish; kichik  \nguruhdan  boshlab  bolalarni  predmetlarni  kuzatishni  o`rgatish  yo`li  bilan  voqelikni  idrok  etish  \nasosida,  mustaqil  izlash  asosida  predmetlar  shaklini  yaratishga  o`rgatib  borish;  ilgari  yasalgan  \nshakllardan foydalanishga va ularni mujassamlashga odatlantirish. Har bir yosh guruhidagi  \nbolalarga iloji boricha loy va plastilin o`yi nchoqlar yasash usullaridan va tasvirlash vositalaridan  \nerkin foydalanishni o`rgatib borish;  bolalarni  loy va plastilin o`yinchoqlar yasash  uchun  \nmavzularni mustaqil holda o`ylab topishga asta-sekin o`rgatib borish, buning uchun tevarak - \natrofni  kuzatish,  badiiy  adabiyotlarni  tinglash,  turli rasmlarni  tomosha  qilish,  san’at  asarlarini  \nko`rish orqali idrok etilgan taassurotlardan foydalanish; bolalarni tarbiyachining ko`rsatmalarini  \nfaol idrok  etish  o`yinchoqlarning  shakllarini,  ularni  yasash  usul — tasvirlash  vositalarini  o`ylab  \ntopishda  tashabbus  \nko`rsatishga  o`rgatish.  \nBolalarda ijodkorlikning, biror mavzuni erkin tanlash va hal etish ko`nikmasining  \nrivojlanishi  ularning  loy va plastilin  o`yinchoqlar  yasash  texnikasini  qanchalik  puxta  \negallaganiga bog`liqligi tufayli ishning mana shu tomoniga katta e’tibor berish kerak. Har bir  \nmashg`ulotda  ijodiy  xarakterdagi  topshiriqlar  bilan  birga  bolalarga  o`yinchoqlar  yasash  \ntexnikasini o`rgatish vazifasi ham qo`yiladi. Bola tasviriy va texnik usullarni qanchalik ko`p bilsa,  \nuning  o`yinchoqlar  yasashi  shunchalik mustaqil hamda  ijodiy  xarakter kasb  etadi.  \nMaktabgacha  yoshdagi  bolalar  loy va plastilin  o`yinchoqlar  yasashda  ularni  \nhar doim  predmetning  harakati  qiziqtiradi.  Kichik  guruhdagi  bola  hali obrazni  harakatli  \ntasvirlashni  bilolmasligi  sababli  harakatni  og`zaki  sharhlaydi  va ayni bir vaqtda  o`yinchoqni  bir \njoydan ikkinchi joyga surib o`ynaydi. Bog`chaning katta guruhida bolalar predmetlarning uncha  \nmurakkab bo`lmagan harakatini o`yinchoqlarda tasvirlay boshlaydilar (masalan, oyoqlarni  \nqadam qo`yilayotgandek, qo`lni yuqoriga ko`tarilgan holda, gavdani sal oldinga engashtirib  \nishlaydilar). Tayyorlov guruhida harakatlarni tahlil qilishda umumiy tajribaga ega bo`ladilar.  \nUlar qadam tashlash va boshqa harakatlarni tasvirlash uchun faqat qo`l va oyoqni ko`tarib  \no`rnatmay, ularni bukish ham kerakligini, shunda figura ifodaliroq bo`lishini tushunadilar. Katta  \nguruhda  bolalar  avval  harakat siz figuralarni  yasagan,  keyin  esa uning  qismlarini  kerakli  holatda  \no`rnatib harakatlantira boshlagan bo`lsalar, tayyorlov guruhida loyning asosiy hajmi «Filning  \nxartumi  yaxshi  chiqayapti»  tasvirlanadigan  predmetning  mo`ljallangan  holatiga  yetish - \nyetmasligini darhol  bila oladilar.  \n \nFOYDALANILGAN  ADABIYOTLAR:  \n1. Sakulina  N.P.,  Kamarova  T.S. Bolalar  bog`chasida  tasviriy  faoliyat.  - T., ”O`qituvchi”,  1996.    «POLISH  SCIENCE  JOURNAL»   \n55  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \nShovqiyeva Shohida,  \nO‘zbekiston  davlat  san’at  va madaniyat  instituti  \n3-bosqich talabasi  \n(Tashkent,  O`zbekiston)  \n \nMAKTABGACHA YOSHDAGI BOLALARNI KITOBXONLIKKA  \nMEHR  UYG`OTISHNING  O`ZIGA  XOS  XUSUSIYATLARI  \n \nRezyume . Maqolada  maktabgacha  yoshdagi  bolalarni  kitobxonlikka  mehr  \nuyg`otishning  o`ziga  xos xususiyatlari  haqida  ma’lumot  berilgan.  \nKalit  so`zlar : badiiy  adabiyot,  tarbiyachi,  xalq og`zaki  ijodi,  tarbiya,  odob  \n \nBadiiy  adabiyot  bolalarni  aqlan,  axloqan  va estetik  tarbiyalashning  qudratli,  ta’sirchan  \nquroli  sifatida  xizmat  qiladi,  u bola  nutqining  rivojlantirish  va boyitishga  ulkan  ta’sir  ko`rsatadi.  \nBolalarga  bilim  va tarbiya  beruvchi  asosiy  manbalardan  biri kitobdir.  Kitob  o`qish  uchun  bo`lib  \nta’lim - tarbiya muassasalarida muntazam o`qish san’ati bilan tanishadilar. Tarbiyachi bolalarga  \ntarbiya  va bilim  berishda  asosiy  manba  bo`lgan  kitobdan  keng  foydalanadi.  Kitob  kichkintoylar  \nfikricha hamma narsasi bor, hamma narsaga javob beruvchi vosita sifatida tan olinadi. Lekin  \nshuni alohida ta’kidlash lozimki, kitob qanchalik qiziqarli, hissiyotlarga boy bo`lmasin, agar  \nkitobxon qalbiga yetib bormasa, hayajon solmasa, bunday kitoblarning tarbiyaviy ta’siri kam  \nbo`ladi.  \nBolalarga  atalgan  kitoblarning  maqsadi  ularga  tabiat  ato etgan  his- tuyg`ularni  \no`stirishdan  iborat.  Bunday  kitoblarning  bilvosita  ta’siri  bolalarning  aqliga  emas  balki  ularning  \nhis-tuyg`ulariga qaratilmog`i kerak. His bilishdan oldin bo`ladi, haqiqatni sezmagan kishi uni  \ntushunmaydi,  bilmaydi  ham.  Kattalarga  oid bo`lgan  nasalar  bolalarga  ham  oiddir,  faqat  ularni  \nbolalar  tushunchasiga  muvofiq  suratda  bayon  qilmoq  kerak,  bu ishning  eng muhim  \ntomonlaridan  biridir, degan  edi V.G.  Belinskiy.  \nTarbiyaviy  tizimda  o`qishga  rahbarlik  qilishda  hilma -xillik  yo`nalishlarda  ish olib borilar  \nekan, tarbiyachi kitobxonni tarbiyalashda doimo yuqoridagi fikrlardan kelib chiqib tarbiyaviy  \nta’sirini oshirishga harakat qiladi. Tarbiyaviy tizimning muhim yo`nalishlaridan biri mehnat  \ntarbiyasidir. Bolalar uchun yozilgan barcha kitoblar axloq va odob mavzusi bilan bog`liq. Ushbu  \nyo`nalishdagi tarbiya umumiy tarbiya tizimining bir bo`lagi hisoblanadi va doimo boshqa  \ntarbiyaviy yo`nalishlar bilan birga olib boriladi. Kichik yoshdagi bolalar o`ta ta’sirchan va  \nhissiyotga beriladigan bo`ladilar. Bu yoshda ki toblarda yozilgan narsalar, kattalar tomonidan  \naytilgan  nasihatlar  tezda  qabul  qilinadi.  Ular  o`zlarinikidan  ko`ra  boshqalarning  axloqiy  \ntomonlariga, qiliqlariga, qilgan ishlariga tezda baho beradilar. Ana shularni e’tiborga olib  \ntarbiyachi suhbat, ovozli va ifodali o`qish, ertak aytib berish, so`zlab berish kabi tadbirlar,  \nmashg`ulotlar orqali o`qilgan yoki aytilgan, ertaklar qahramonlarning xilma -xil xislatlarini,  \nularning o`zaro munosabatlarini, vaziyat va sharoitlarni tushuntirib berar ekan. Shunday  \nxislatlar bolalarning o`zlarida ham borligini aytib o`tadi. Masalan, bolalarning juda ko`pchilgida  \nuchraydigan  maqtanchoqlikning  yomonligi  va uni nimalarga  olib kelishi  haqida  o`zbek  xalq   «POLISH  SCIENCE  JOURNAL»   \n56  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \nertaklaridan “maqtanchoq quyon”ni o`qib bersa, halollik, to`g`rillikning afzalligi haqidagi “ O`g`ri  \nva to`g`ri”  ertagini  aytib  beradi.  \nDo`stlik  va ayyorlik  haqida  suhbatlashib,  Gulxaniyning  “Toshbaqa  bilan  chayon,  “Bo`ri  \nbilan Tulki” ertagini tushuntirib tulki bo`rini aldab qop -qonga tushurib ayyorlik qilib qochib  \nketganini aytib beradi. Shunday odamlar bizning ichimizda ham bo`lib, birov boylikka, birov  \nqo`rqqanidan, birov mansabga o`tirish uchun do`st bo`ladi va keyinchalik ular baribir aldab  \nketadi.  Haqiqiy do`st  yaxshi  va yomon  kunlarda  ham do`st  bo`lib  qolaveradi, deb  xulosa  qiladi.  \nTarbiyachi  bolalarga  estetik  tarbiya  berishda  quyidagi  asoslarga  tayanadi:  \na) Shoir  yozuvchining  mahorati,  uslubi  imkoniyati;  \nb) Kitobning  mazmuni,  ma’nosi,  kuchi  g`oyasi  badiiyligi;  \nv) Tarbiyachining  tayyorgarligi,  uslubi bilimi  va haqozo;  \nMana shu uch holat birlashgandagina, hamkorlik bilan muvoffaqiyatga erishiladi.  \nTarbiyachi badiiy va  san’at asarlari bilan ishlaganda  estetik  tarbiyaga  ko`proq ahamiyat  \nberadi.Chunki  bu asarlarda  insonning  ruhiyati,  xarakteri,  faoliyati  his-hayajoni,  kechinmalari,  \ntabiati  dunyoda  bo`layotgan  voqea  va hodisalarning  hammasi  bir butunligiga  aks ettiriladi  va \nbu bolaga  ta’sir etadi.  \nShe’riy  obrazlarda badiiy  adabiyot  jamiyat  va tabiat  hayotini,  insoniy  his-tuyg`ular va  \no`zaro munosabatlar olamini ochib beradi hamda tushuntiradi. Bu namunalar o`z tas’ir kuchiga  \nko`ra turlicha bo`ladi: hikoyalarda bolalar so`zlarning lo`ndaligi va aniqligini bilib oladilar;  \nshe’rlarda o`zbekcha nutqning musiqiyligini, ohangdorligini ilg`aydilar; xalq ertaklari ular oldida  \ntilning aniqligi va ifodaliligini namoyon qiladi, ona tilidagi nutqning yumor, jonli va obrazli  \ntaqqoslashlar,  ifodalarga  qanchalik bo yligini  ko`rsatadi.  \nBolalar  badiiy  asarlar  qahramonlariga  qayg`urishni  o`rganganlaridan  so`ng  ular \nyaqinlari va atrofdagi odamlar kayfiyatini payqay boshlaydilar. Ularda insonparvarlik his - \ntuyg`ulari  – birovning  dardiga  shyerik  bo`lish,  yaxshilik  qilish,  adolatsizlikka  nisbatan  qarshilik  \nko`rsatish qobiliyati uyg`ona boshlaydi. Bu prinsipiallik, halollik, haqiqiy fuqarolik hislari  \ntarbiyalanadigan  poydevordir.  \nXalq  – bolalarning  betakror  o`qituvchisidir.  Xalq  asarlaridan  boshqa  hech  qaysi  \nasarlarda qiyin talaffuz qilinadigan tovushlarning bunday qoyilmaqom qilib joylashtirilishini,  \njarangiga  ko`ra  bir-biridan  zo`rg`a farq qiladigan  so`zlarni  hayron  \nqolarli darajada yonma -yon terilishini uchratish qiyin. Beozor hazillar, nozik yumor,  \nsanoq she’rlar – pedagogik ta’sir ko`rsatishning samarali vositasi, dangasalik, qo`rqoqlik, o`jarlik,  \ninjiqlik,  faqat  o`zini  o`ylash  kabi xislatlarga qarshi  yaxshigina  «malhamdir».  \nErtaklar  olamiga  sayohat  bolalar  tasavvurini,  ularning  xayvonot  olamini,  fantaziyasini  \nrivojlantiradi. Eng yaxshi adabiy namunalar asosida insonparvarlik ruhida tarbiyalangan bolalar  \no`z hikoyalarida va ertaklarida mazlumlar va zaiflarni himoya qilish, yomonlarni jazolash orqali  \no`zlarining  adolatparvarligini  namoyon  qiladilar.  \nMaktabgacha davrdagi har bir yosh bosqichi o`zining nutqiy rivojlantirish vazifalarini  \nqo`yadi.  Yosh  o`sib  borishi  bilan  adabiy asarlarni  qabul qilish  darajasi  \nham ortib borishi tufayli ular asta -sekin murakkablashtirilib boriladi. Bolalarda she’r  \ntinglay olish qobiliyati rivojlanadi. Shundan kelib chiqqan holda kattalar bolalarning yosh  \nimkoniyatlarini  bilishlari  zarur.    «POLISH  SCIENCE  JOURNAL»   \n57  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \nFOYDALANILGAN  ADABIYOTLAR:  \n1. Ergashev,  Yo`ldoshev  E. Kutubxona  va kitobxon.  Toshkent.:  O`zbekiston  nashriyoti.  2018y    «POLISH  SCIENCE  JOURNAL»   \n58  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \nNizomova  Indira  Akramovna  \nShofirkon tuman  8-son davlat  maktabgacha  ta’lim  tashkiloti  direktori,  \nXakimova Dilnoza G`aniyevna  \nShofirkon tuman maktabgacha ta’lim bo`limi Davlat va nodavlat maktabgacha ta’lim  \ntashkilotlari  faoliyatini  hamda  o`qitishning muqobil  shakllarini  \ntashkil  etish  sho`basi  mudiri  \n(Buxoro,  O`zbekiston)  \nMAKTABGACHA YOSHDAGI BOLALARDA KITOBXONLIKKA  \nMEHR  UYG`OTISHNING YO`LLARI  \nRezyume . Maqolada  maktabgacha  yoshdagi  bolalarda  kitobxonlikka  mehr  \nuyg`otishning  yo`llari  haqida ma’lumot berilgan.  \nKalit  so`zlar : kitob,  kitobxon,  ovozli  o`qish,  so`zlab  berish,  bola.  \nKo`pgina  maktabgacha  yoshdagi  bolalar  qiziqarli  kitoblar  berishini  so`raydilar.  \nTarbiyachi kitob tavsiya etar ekan, kitobxonlarga mustaqil ravsihda kitob tanlash malakasini  \nham  singdirib  beradi.  Shuning  uchun  bog`cha  javonidagi  kitoblarning  katalog  va kartotekalari  \nrasman  bo`lsa  maqsadga  muvofiq  bo`lar  edi. \nKitobxonlarning  yosh  xususiyatida  katta  farq bo`lgani  kabi ularning  tushunishi  va unga  \nmunosabatida  ham  katta  farq qiladi.  Bundan  tashqari  har bir kitobxonning  o`ziga  xos tomoni,  \nxarakteri,  qiziqish  mavjud  bo`lib,  tarbiyachi  suhbat  davomida  bularni  tezda  ilg`ab  olish  lozim.  \nShuning uchun tarbiyachi imkoniyatini hisobga olib ular bilan yakka holda ishlashii va ularga  \nyordam  berish  zarur.  \nBolaning  ko`z oldida  tarbiyachi  kitoblarni  yaxshi  biladigan,  uning  qiziq  joylarini  gapirib  \nbera oladigan maslahatchi bo`lib gavdalanish lozim. Tarbiyachining tarbiyaviy roli birinchi marta  \nkitobga  qiziqqan  bola  bilan  suhbatiga  bog`liq.  Tarbiyachi  bolalar  bilan  suhbatlashib  kitob  bilan  \nishlash qonun qoidalarini ham aytib o`tishi lozim. Chunki bola olgan kitobiga javob berish,  \nlozimligi  uqtiriladi.  \nMaktabgacha yoshdagi bolalar kitob tanlashda kutubxonachiga yoki tarbiyachiga  \nmuhtojdirlar.  Shuning  uchun  ham  kutubxonani  yoki tarbiyachi  kitob  tavsiyasining  eng yaxshi  \nusuli  – suhbatdan  foydalaniladi.  \nTavsiyaviy  suhbat  o`z oldiga  quyidagi  maqsadlarni  qo`yadi:  \n- bolaga  kitob  tanlashga  yordam  berish;  \n- kitobga nisbatan qiziqishni uyg`otish bolaning kitobga juda ham qiziqishi kitobni  \noxirigacha  yakunlashiga  yordam  beradi;  \n- o`qishga  qo`llanma  berishdan  iboratdir.  \nTavsiyaviy suhbatda so`ralgan kitob xarakteriga, ya’ni so`roq motiviga bog`liq holda har  \nxil tuziladi.  Suhbat  davomida  bolalarga  yoqqan  yoki yoqishi  mumkin  bo`lgan  bir necha  \nkitoblarni ko`rsatadi, ularni qiziqtira oladigan jihadlarni aytib beradi. Masalan mehr oqibat  \ntuyg`usini shakllantiruvchi “Yoriltosh ertagini bilasizmi?”. Bu kitob yetim qizlar Gulnora va  \nOymomo haqida. Gulnora juda chiroyli, chaqqon, mehribon, ishlab charchamaydigan qiz  \nbo`ladi. Uning onasi vafot etganidan keyin otasi begona ayolga uylanadi. Ayol juda badjahl  \nyomon  bo`ladi.    «POLISH  SCIENCE  JOURNAL»   \n59  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \nGulnora singlisini o`padi -da zor – zor yig`lab qochib ketadi. Uni quvlab kelayotganlar  \nyetay deganda  tog`dagi tosh yorilib Gulnora uning uchiga  kirib  ketadi.  Quvlaganlarning  \nhammasi kirib yoriltosh desa ham tosh yorilmaydi. Lekin hech kim yo`g`ida singlisi kelib yoriltosh  \ndesa u ochiladi, nimaga shunday bo`lganini kitobni o`qisalaring bilib olasizlar deb kitobxonni  \nqiziqtirib  kitoblar  tavsiya etiladi.  \nBolaga  ertak  kitoblar  tavsiya  qilinayotganda  unga  shu kitobning  boshidan  ozgina  o`qib  \nberib, kitob qahramonini harakterlab beradi. Ertaklarda ko`rsatilgan rasmlar ham muhim rol  \no`ynaydi.  Bunda  rasmlarning  hammasini  birdan  ko`rsatish  maqsadga  muvofiqmas.  Rasmlarni  \nbolaning hayajonini kuzatgan holda asta – sekin ko`rsatish kerak. 1 -2 ta asosiy rasmlarni  \nko`rgan  bola  kitob  bilan  birga  shu kitobdagi  epizodlarga,  qahramonlarga,  voqealarga,  \nqahramonlarning harakteriga  qiziqib  qoladi.  \nMaktabgacha yoshdagi bolalarga kitob tavsiya qilayotganda rasm ko`rsatish usuli  \nayniqsa muhimdir. Ular o`quv texnikasini yaxshi bilishmaydi, kitobdagi rasmlar ularni shu  \nkitobga qiziqtiruvchi asosiy kuch bo`lib hisoblanadi. Tavsiya suhbati kitobxonni rasmni va  \nmatini  chuqur  qabul  qilishga  tayyorlaydi.  \nOvozli  o`qish  va so`zlab  berish  bolani  kitobga  jalb qilishning tabiiy  usuli  bo`lib,  undan  \noilada  ham  keng  foydalaniladi.  Mashhur  pedagog  A.S. Makarenko  oilada  ovozli  o`qish  muhim  \nahamiyatga ega ekanligini ta’kidlab: Imkoni boricha o`qish har kuni va doimiy ravishda olib  \nborilishi  lozim deb  ta’kidlaydi.  \nOvozli o`qish va so`zlab berish maktabgacha yoshdagi bolalar bilan ishlashda muhim  \no`rin egallaydi. Ular hali o`qish texnikasini yaxshi egallamagan bo`ladilar. Shuning uchun bu  \nyoshdagi bolalar birinchi – ikkinchi yilda o`qishga qiynalishlari sababli o`qishdan ko`ra eshitishni  \nyaxshi ko`radilar. Bundan tashqari, bolalar o`qishgagina emas, diqqat bilan eshitishga ham,  \neshitganlarini  o`zlashtirishga  va aytib  berishga  ham  o`rganishlari  kerak.  \nOvozli o`qishga yoki so`zlab berishga xalq ertaklari juda qiziq manba bo`lib xizmat  \nqiladi. Har xil fantastik sehrli ertaklarni so`zlab berish maqsadga muvofiq. Aniq va yorug` obrazli  \nkichik, qiziqarli ma’nodor ertaklar bolalarni atroflarini o`rab turgan dunyo bilan tanishtiradi.  \nUlarning ko`pgina savollarga javob beradi. Xalq ertaklariga xos bo`lgan xususiyat yaxshilikni  \nyomonlik, rostni yolg`on ustidan g`alaba qilish xususiyati bolalarda haqgo`ylikni tarbiyalaydi.  \nNatijada  ertaklar  bolalarda  badiiy  adabiy otga  qiziqish  uyg`otadi.  Ammo  hamma  xalq ertaklari  \nham  bolalarga  to`g`ri  kelavermaydi.  Ertak  tanlayotganda  bolalarning  yosh  xususiyatiga,  \nertakning  g`oyasi  va mazmuniga  e’tibor berish  kerak.  \nMaktabgacha yoshdagi kitobxonlikka mehr uyg`otish yana bir usuli so`zlab berishdir.  \nSo`zlab  berish  asar tanlangandan  so`ng  birinchi  marta  o`qib,  hamma  tomoniga  e’tibor  berish  \nkerak.  Uning  mazunini  bilan  shunday  tanishtirish  kerakki  eshituvchilar  ham  tarbiyachi  asardan  \nqanday  ta’sirlangan bo`lsa,  shunday  ta’sirlanishlari  kerak.  Shundagina  kitobdagi  hamma  \nnarsalar eshituvchilarning esida qoladi, ular holatlarni hatto so`zma – so`z bilib olishadi. So`zlab  \nberishda  ma’noga  amal  qilish  lozim.  Chunki  so`zlab  berishda  juda  buzib  yuborish  kitobxonlarni  \nzeriktiradi.    «POLISH  SCIENCE  JOURNAL»   \n60  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \nQo’chqarov Sherzod  \nUrganch  davlat  universiteti,  \nQo’chqarov  Bekzod  \nO’zbekiston  Milliy  universiteti,  \nNurjonova Gulnoza  \nUrDU,  Psixologiya  1-kurs  Magistranti  \n(Urganch,  O’zbekiston)  \n \nSURUNKALI SOMATIK KASALLIKLARDA O’SMIRLAR XAYOT  \nSIFATINING  PSIXOLOGIK OMILLARI  \n \nAnnotatsiya. Ushbu maqolada bolalar va o'smirlar o'rtasida uchraydigan samatik  \nkasalliklarining ko’payishi, psixologik omillar va yoshlarni turmush tarzidagi o’ziga xosligi,  \nkasallik sharoitida o'smirning hayot sifati uning hayotining integral xarakteristikasi, shu  \njumladan  jismoniy,  psixologik  va ijtimoiy  faoliyati,  shuningdek  sub'ektiv  idrok  etishning  o'ziga  \nxos xususiyatlarini  o’rganishga  e’tibor  berilgan.  \nKalit so’zlar: somatik kasalliklar, reabilitatsiya, tibbiyot, texnologiyalar, o'smirning  \nhayot sifati, sub'ektiv, psixologik, ijtimoiy, jismoniy, shaxsning shakllanishi, debyuti, stress,  \nfenomeni,  o'smirlik davri,o'z -o'zini  ongni  shakllantirish.  \n \nАннотация.  В данной  статье  рассмотрен  рост  соматических  заболеваний  среди  \nдетей  и подростков,  психологические  факторы  и специфика  образа  жизни  молодежи,  \nкачество  жизни  подростка  в условиях  заболевания  является  интегральной  \nхарактеристикой его жизни, в том числе физической, психологической и социальной  \nдеятельности,  а также  изучение  особенностей  субъективного  восприятия.  \nКлючевые  слова:  соматические  заболевания,  реабилитация,  медицина,  \nтехнологии, качество жизни подростков, субъективное, психологическое, социальное,  \nфизическое, формирование личности, дебют, стресс, феномен, подростковый возраст,  \nсамо -о'формирование  сознания.  \n \nBolalar  va o'smirlar  o'rtasida  kasallanishni  ko'payish  masalalarining  dolzarbligi  hozirgi  \nvaqtda  ham  dolzarb  bo'lib qolmoqda . Shu bilan birga, balog'atga etmagan bemorlarda surunkali  \nsomatik  kasalliklar  diagnostikasining  chastotasi  oshishi  kuzatilmoqda.  Zamonaviy  tadqiqotlar  \nshuni  ta'kidlaydiki,  davolash  va reabilitatsiya  tadbirlarining  samaradorligini  baholashning  eng \nmuhim  ko'rsatkichlaridan  biri bu bemorlarning  hayot  sifati  darajasidir.  \nTibbiyot  texnologiyalarining  jadal  rivojlanishi  va tibbiy  yordam  sifatining  \nyaxshilanishiga qaramay, bemorlar davolanish natijasida o'z hayotlaridan har doim ham  \nmamnun  emaslar.  \nA. A. Novik va T. I. Ionova (2013) tomonidan hayot sifatining ta'rifiga tayanamiz. Shunga  \nko'ra,  kasallik  sharoitida  o'smirning  hayot  sifati  uning  hayotining  integral  xarakteristikasi,  shu \njumladan  jismoniy,  psixologik  va ijtimoiy  faoliyati,  shuningdek  sub'ektiv  idrok  etishning  o'ziga  \nxos xususiyatlarini  hisobga  olgan  holda  hisoblanadi.  Zamonaviy  tadqiqotlar  kasallikning  \nbemorlarning  hayotiga  katta hissa  qo'shganligini  ko'rsatadi.  Qanday  bo'lmasin,  kasallik  debyut    «POLISH  SCIENCE  JOURNAL»   \n61  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \nyoshidan qat'i nazar, hayot uchun maxsus shartlarni belgilaydi. Bolalik va o'smirlik davrida u  \ndoimo shaxsning shakllanishi va rivojlanishiga ta'sir qiladi. Surunkali somatik kasallikning  \ndebyuti  bolani  ham, uning  butun  oilasini  ham  yangi,  maxsus  sharoitlarga  qo'yadi.  \nSo'nggi  tadqiqotlar  stresslilik  fenomenini  birinchi  o'ringa  qo'ydi  tashxis  qo'yish.  \nBolaning  tajribasining  intensivligi  turli omillarga,  jumladan,  uning  yoshiga,  kasallikning  \nog'irligiga, ma'lum bir kasallik haqida mavjud bilimlarga va ota -onalarning reaktsiyalariga  \nbog'liq. Biroq, tadqiqotchilarning ta'kidlashicha, tashxis qo'yish sharoitida stress darajasi  \ntravmadan keyingi stress buzilishining klinik ko'rinishi bilan taqqoslanadigan reaktsiyalarga  \nerishishi mumkin va bolaning ruhiy moslashuvining turli b elgilarining paydo bo'lishi istisno  \nqilinmaydi.  \nO'smirlik  davrida  surunkali  kasallikning  boshlanishi  butunlay  boshqacha  \ntendentsiyalarni belgilaydi. Ular kelajak istiqbollarini shakllantirish jarayonlari, shuningdek,  \najralib chiqish va o'z -o'zini ongni shakllantirish jarayonlarini aktuallashtirish bilan o'zaro  \nbog'liqdir. Tashxis qo'yish holatida o'smir hissiy shokni boshdan kechiradi, bu shaxsiyat  \nrivojlanishining  ushbu  bosqichida  muhim  ahamiyatga  ega bo'lgan  tengdoshlari  bilan  muloqot  \nqilishda mumkin bo'lgan cheklovlar haqida tashvishlanishdan kelib chiqadi. Ushbu bosqich  \nuchun begonalashish, yolg'izlikning  eng xarakterli  tuyg'ulari.  \nBundan tashqari, bu erda alohida rol o'ynaydi, bu yoshda bir oz realizmga ega bo'lishni  \nboshlaydigan  kelajak  uchun  rejalarning  qulashi.  Shu munosabat  bilan,  o'smirlik  mavjud  \nkasallikning kuchayishini ko'proq tashvishli idrok etish bilan tavsiflanadi. Kelajak noaniq,  \nqo'rqinchli bo'ladi, ko'p qo'rquvlarni keltirib chiqaradi, bu ko'pincha o'smirning kasallik bilan  \nbog'liq  cheklovlarni  bilishi  bilan  bog'liq.  \nHayot sifatining qoniqarli darajasiga erishishning mumkin emasligi, shuningdek, o'z - \no'zini  hurmat  qilish  va o'ziga  ishonchsizlik  darajasining  pasayishi  bilan  bog'liq.  Biz ishonamizki,  \nbu holda o'z -o'zini hurmat qilish darajasi va profili o'rtasidagi munosabatlar hayot sifatiga ta'sir  \nqiladi  va hissiy  o'zini  o'zi boshqarish  jarayonlari  bilan  bog'liq  yagona  mexanizm  sifatida  amalga  \noshiriladi.  \nShu bilan  birga,  ijtimoiy -psixologik  moslashuvning  buzilishi  surunkali  somatik  \nkasallikning mavjudligining majburiy natijasi emas. Ba'zi hollarda mobilizatsiya va aqliy tartibga  \nsolishning  kompensatsion  jarayonlarini  kiritish  qayd  etilgan  boshqa  tendentsiya  aniqlanadi.  \nKasallikning  holatlari,  masalan,  o'smirni  kasallik  tufayli  yuzaga  keladigan  to'siqlarni  \nbartaraf  etish  uchun  zarur  bo'lgan  kognitiv  faollikka  undashi  mumkin.  Shu bilan  birga,  o'smir  \nuchun  mavjud  bo'lgan  hayot  sohalarida  o'zini  o'zi anglash uchun  motivatsiya  safarbar  qilinadi.  \nKasallik bilan kurashishning o'ziga xos xususiyatlari qiyin hayotiy vaziyatlarga duch  \nkelgan  o'spirin  bilan  kurashishning  odatiy  usullariga  bog'liq  bo'ladi.  Hayotda  yetarlicha  \ntajribaga ega bo‘lmagan o‘smir ota -onasidan yordam so‘rashi tabiiy. Biroq, ota -onalarning o'zlari  \nmoslashish qobiliyatining pasayishiga duch kelishlari mumkin, bu esa zarur yordamning  \netishmasligi  tufayli  o'smirning  ahvolini  og'irlashtiradi.  \nKasallik  sharoitida  bolalar  va o'smirlarning  hissiy,  shaxsiy  xususiyatlari  va hayot  sifatini  \ntavsiflovchi juda ko'p sonli zamonaviy tadqiqotlarga qaramay, surunkali kasalliklarga chalingan  \no'smirlarning  hayot  sifati  kontekstida  psixologik  omillarni  tavsiflovchi  tadqiqotlar  etishmaydi.  \nsomatik  kasalliklar.    «POLISH  SCIENCE  JOURNAL»   \n62  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \nStress (ing. stress — bosim, kuchlanish, tanglik) - odam va hayvonlarda kuchli taʼsirotlar  \nnatijasida sodir bo ʻladigan o ʻta hayajonlanish, asabiylik holati. Organizmda har xil taʼsirotlarga  \nnisbatan rivojlanadigan nomaxsus neyrogormonal reaksiya. \"Stress.\" terminini kanadalik  \npatolog G. Selye taʼriflab, tibbiyotga kiritgan (1936). Olim stress holatiga olib keluvchi omilni  \nstressorlar  deb,  ular taʼsirida  organizmda  roʻy beradigan  oʻzgarishlarni  moslashish  \n(adaptatsiya) sindromi deb atadi. Fizik (issiq, sovuq, shikastlanish va boshqalar) va psixik  \n(qoʻrquv, qattiq tov ush, o ʻta xursandchilik) stressorlar ajratiladi. Organizmda bu omillar  \ntaʼsirini yengishga qaratilgan moslashuvchi biokimyoviy va fiziologik o ʻzgarishlar rivojlanadi, bu  \nstressorning kuchi, taʼsir etish muddati, odam yoki hayvonning fiziologik sistemasi va ruhiy  \nholatiga bog ʻliq. Nerv sistemasi yuqori rivojlangan odam va hayvonlarda, his -tuygʻu koʻpincha  \nstressor  vazifasini  oʻtaydi  va u fizik stressor  taʼsiriga  zamin  tugʻdiradi.  Odamda  bir xil kuchdagi  \nstress ham xavfli, ham ijobiy bo ʻlishi mumkin. Shuning uchun maʼlum bir darajadagi Stresslarsiz  \nfaol hayot  kechirib  boʻlmaydi,  chunki  stresslar  boʻlmasligi bu  oʻlim bilan  barobar  degan  edi \nG. Selye. Demak, stresslar nafaqat xavfli, balki organizm uchun foydali ham bo ʻlishi mumkin  \n(eustress), bu holat organizm imkoniyatlarini ishga soladi, salbiy taʼsirotlarga chidamliligini  \noshiradi (masalan, infeksiyalar, qon yo ʻqotish va boshqalar), maʼlum bir somatik kasalliklar  \n(mas, yara kasalligi, allergiya, yurak kasalliklari va boshqalar) kechishini yengillashtiradi yoki  \nbemorning  ulardan  form  boʻlishiga  yordam  beradi.  Zararli  Stresslar  (distress)  organizm  \nrezistentligini pasaytiradi, ko ʻpgina kasalli klarning kechishini og ʻirlashtiradi. Stress taʼsirida  \nkasalliklar  paydo  boʻlishida  organizmning  dastlabki  holati  katta  ahamiyatga  ega.  Mas,  \ngipertoniya kasalligi bilan og ʻrigan bemorda Stress og ʻirroq, yaʼni gipertonik krizislar bilan  \nkechadi.  \nG. Selye,  Stress  natijasida  rivojlanadigan  kasalliklar  yo stressorning  kuchli  taʼsir  etishi,  \nyoki gormonal tizimning \"noto ʻgʻri\" reaksiya berishiga bog ʻliq deb hisoblagan. Chunki ayrim  \nhollarda  distress  uncha  kuchli  boʻlmagan  stressor  taʼsirida  yuzaga  keladi.  Stressning  \norganizmga  ijobiy  yoki salbiy  taʼsir  etishi  organizmning  ushbu  stressorga  nisbatan  reaksiyasiga  \nbogʻliq. Stress  xolatini  faol oʻzgartirishga  qaratilgan  choralar  organizm  chidamliligini  oshiradi  \nva natijada  kasallik  rivojlanmaydi  yoki,  aksincha,  faol kurashish  boʻlmasa,  moslashish  sindromi  \nsusayib,  ogʻir holatlarda  organizmni  nobud  boʻlishigacha  olib kelishi  ham  mumkin.  \nOrganizmdagi hamma o ʻzgarishlarni nazorat qilishda miyadagi katexolaminlar miqdori katta  \nahamiyatga  ega. Shunday  qilib,  nerv  sistemasi  organizmning  Stressga  reaksiya  berish  holatini  \nbelgilaydi  (qarang  Neyrogumoral  regulyasiya).  \nHozirgi \"Stress\" termini juda keng maʼnoda tushuniladi, mas, nerv sistemasi bo ʻlmagan  \nquyi hayvonlar, hatto o ʻsimliklarda ham suv miqdori yoki harorat keskin o ʻzgarsa, ularda  \nkechayotgan  fiziologik jarayonlar  buziladi.  \nBolalar va o'smirlar o'rtasida kasallanishning o'sishi bilan bog'liq muammolar hozirgi  \nvaqtda  ham  dolzarb  bo'lib  qolmoqda.  Buni  so'nggi  o'n yillikdagi  statistik  ma'lumotlar  \ntasdiqlaydi.  Voyaga  etmagan  bemorlar  surunkali  somatik  kasalliklardan  ko'proq  azob  \nchekishadi. Shu bilan birga, ko'plab kasalliklar sezilarli darajada \"yoshroq\" va avvalroq kechroq  \nnamoyon  bo'ladigan  yosh  davrlarida  aniqlanadi  ontogenez  bosqichlari  (Dedov  I.I. va \nboshqalar). Shu bilan birga, baholashning eng muhim ko'rsatkichi tibbiy muassasalar sharoitida  \ndavolash va reabilitatsiya tadbirlari sifatining samaradorligi bemorlarning hayot sifatining  \nko'rsatkichidir.  (Yuriyev  V.K.,  Sayfulin  M.X.)  Zamonaviy  sharoitda  shaxsiylashtirilgan  tibbiyot    «POLISH  SCIENCE  JOURNAL»   \n63  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \ndasturlarini  ishlab  chiqish  tobora  ko'proq  samarali  tibbiy  yordam  ko'rsatmoqda  hayot  \ndavomiyligi.  Terapevtik  va rekreatsion  tadbirlarga  qaratilgan  surunkali  kasalliklarga  chalingan  \nodamlarning  barqaror  rivojlanishini  qo'llab -quvvatlash;  voyaga  etmaganlarning  hayotini  \nnormallashtirishga  hissa  qo'shish.  Lekin,  davolanish  natijasida  jismoniy  holati  yaxshilanganiga  \nqaramay,  o'smir  har doim ham  hayotidan  mamnun  emas.  \n \nADABIYOTLAR:  \n1. Абросимов И. Н. Выбор копинг -стратегии как фактор психологической адаптации  \nпациента  к хроническому   соматическому   заболеванию   /   И.   Н.   Абросимов,  В. \nМ. Ялтонский  // Вестник  Омского  университета.  Серия  «Психология». - 2018.  \n2. Агеева -Подобед И. Б. Психологическая и социально -культурная адаптация детей к  \nусловиям  стационарного  лечения  / И. Б. Агеева -Подобед  // Педиатрический  вестник  \nЮжного  Урала. - 2014.  \n3. Александрова  О. В. Психологические  особенности  детей  и подростков  в восприятии  \nими  болезни  на разных  этапах  ее течения  / О. В. Александрова,  А. Е. Ткаченко,  \nИ. В. Кушнарева  // Педиатр.  - 2018.  \n4. Shamdiev  E.С.,Т ursunhodjaeva  L.A, «Psihoterapiya » - Т.: 2011.    «POLISH  SCIENCE  JOURNAL»   \n64  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \nQurbonova Shafoat Xayriddinovna  \nShofirkon  tuman  8-son davlat  maktabgacha  ta’lim tashkiloti  tarbiyachisi,  \nYuldashova Nasiba Nusratovna  \nShofirkon  tuman  8-son davlat maktabgacha  ta’lim  tashkiloti  tarbiyachisi  \n(Buxoro,  O`zbekiston)  \n \nMAKTABGACHA  TARBIYA  YOSHIDAGI  BOLALAR  \nTARBIYASIDA  BOLALAR  ADABIYOTINING  O`RNI  \n \nRezyume . Maqolada  maktabgacha  tarbiya  yoshidagi bolalar  tarbiyasida  bolalar  \nadabiyotining  o`rni  haqida  ma’lumot berilgan.  \nKalit  so’zlar : maktabgacha  tarbiya,  badiiy  adabiyot,  bolalar  adabiyoti,  ertak  \n \nMaktabgacha tarbiya yoshidaga bolalar bilan ishlashda badiiy so`z katta o`rin tutadi.  \nBolalar xalq ertaklari, she’r, hikoyalar eshitishni yaxshi ko`radilar. Bolalar adabiyoti, avvalo  \no`zining  qiziqarli  mazmuni,  badiiy  obrazlarining  go`zalliga,  tilning  ifodaliligi,  she’riy  so`zlarning  \nmusiqaviyligi bilan bolalarga quvonch baxsh etadi. Ayni vaqtda u bolalarga tarbiyaviy ta’sir  \nko`rsatadi.  Buyuk  rus demokratik  V.G.  Belinskiy  bolalar  kitobi  tarbiya  uchun  yoziladi,  «Tarbiya  \n– buyuk  ish, u insonning  taqdirini  hal qiladi»  - degan  edi. \nBadiiy  adabiyotning  qimmati  bolaning  har bir jihatdan  o`sishiga  ta’sir  ko`rsatishidadir.  \nBadiiy adabiyot bolaga jonajon o`lka tabiatining kishilarning mehnati va hayotini, ularning  \nqilayotgan ishlari va ko`rsatayotgan qahramonliklarini, bolalar hayotidan olingan voqyealar,  \nbolalarning o`yinlari, ermaklari va qilayotgan mehnatlarini himoya qilib beradi. Kishilarning ichki  \ndunyosini  yoritib,  ularning  xarakterlari,  his-tuyg`ulari,  xatti -harakatlarini,  shuningdek,  \ntasvirlangan  hodisalarga  yozuvchining  munosabat ini ko`rsatib,  badiiy  adabiyot  asarlari  bolani  \nhayajonlanishga, asar qahramonlariga achinishga yoki ularni qoralashga majbur qiladi. Eng  \nyaxshi badiiy adabiyot asarlari bolalarda biror narsaga yaxshi -yomon, adolatli -adolatsiz, to`g`ri - \nnoto`g`ri deb, o`zlariga xos bir yordam beradi. Bolalar kitobning qiziqarli mazmuni do`stlik,  \nhalollik, mehnatsevarlik, o`rtoqlik namunalarini ko`rsatadi. Bolalar badiiy adabiyot asarlari  \nestetik  jihatdan  \ntarbiyalashga  ham  katta  ta’sir  ko`rsatadi.  Badiiy  asarlarning  yorqin  obrazlari,  jonajon  \ntabiatning shoirona manzaralari, she’rlarning musiqaviyligi, tilning o`tkirligi, ifodaliligi bolalarga  \nyoqadi.  Bolalar  badiiy  so`z qudratini  his etadilar,  uncha  katta  bo`lmagan  ertaklar,  xalq ashula  \nhamda  she’rlarini  tez va osongina  eslab  qoladilar.  \nBadiiy so`zga muhabbatni ilk yoshdan boshlab tarbiyalab borish zarur, bola bog`chadan  \nmaktabga ana shu muhabbat bilan o`tadi va keyinchalik Vatan adabiyotini sevadigan bo`lib  \nqoladi. Xalq, og`zaki ijodi – ertaklar, topishmoqlar, maqollar, qo`shiqlarning ko`plari uzoq  \no`tmishda va hozirgidan tubdan farq qiladigan voqyeligidan tubdan farq qiladigan turmush  \nsharoitida  yaratilgan  xalq ijodi  asarlaridan  faqat  tarbiya  vazifalariga  javob  beradiganlari  tanlab  \nolinadi.  Aql-idroki  yoki jasurligi  bilan,  sabr -toqati  yoki mehnati  bilan  har qanday  qiyinchiliklarni  \nyengadigan  dovyurak,  vijdonli  qahramonlar  ishtirok  etadigan  ertaklardan  bolalar  juda  \nhayratlanadilar,  bunday  ertaklar  bolalarni  ijobiy  qahramonlarga  nihoyatda  hayrixoh  bo`lishga,    «POLISH  SCIENCE  JOURNAL»   \n65  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \nyovuzlikka,  nohaqlikka,  qizg`anchiqlikka,  makr -xiylaga  nisbatan murosasiz  bo`lishga  majbur  \netadi.  \nBolalarga  o`qib  beriladigan  asarlar  doirasiga  har xil janrdagi  asarlar:  hikoya  va \npovestlar, proza hamda she’r shaklidagi ertaklar, dostonlar, xazil she’rlari, topishmoqlar,  \nmasallar  kiradi.  Bolalar  bog`chasi  dasturida  ko`rsatilgan  asarlarning  ko`pchiligi  bolalar  \nadabiyotining oltin fondini tashkil etadi. Har yili bolalar uchun yangi kitoblar nashr etib turiladi.  \nTarbiyachi bosmadan chiqayotgan kitoblarni kuzatib borishi, jurnallardagi tanqidiy maqolalarni  \no`qishi va bolalar kitoblarining muhokama etilishida ishtirok etishi kerak. Tarbiyachi har bir yosh  \nguruh ida bolalarni juda ko`p bolalar badiiy adabiyoti asarlari bilan tanishtirib borishi lozim.  \nBolalar bog`chasining muhim vazifasi bolalarning asarlar mazmunini o`zlashtirib olishlariga,  \nularni  to`g`ri  tushunishlariga  erishishdir.  Bolalar  asarlarning  bir qismini  yod olishlari,  bir qismini  \nesa matnga  yaqinroq  qilib  aytib  bera olishlari  lozim.  \nTarbiyachi bolalarda adabiy asarni idrok qilish ko`nikmasini tarkib toptiradi. Bola asarni  \ntinglayotib, uning mazmunini o`zlashtiribgina qolmay, balki muallif tasvirlayotgan his-tuyg`ular  \nva kayfiyalarni his etishi ham lozim. Bolalar bog`chasida asarning mazmuni va formalarini analiz  \nqilishning  ba’zi  elementlari  ham  tarkib  toptiriladi.  Har bir bola  maktabga  o`tish  paytida  asarda  \nhikoya qilangan asosiy qahramonlarni aniqlay olishi, ularga nisbatan o`z munosabatini (kimning  \nnima uchun yoqqanini) ayta bilishi,  asarning formasini aniqlay bilishi (she’r, hikoya, ertak)  \nlozim.  \nBolalarda birgalashib eshitish malakalarini, uyushqoqlik bilan savollarga javob berish  \nva o`qib berilgan asar asosida savollar berish, illyustrasiyani diqqat bilan ko`zdan kechirish,  \nkitobga yaxshi munosabatda bo`lish ko`nikmalarini tarbiyalash zarur. Bog`cha bolalarda kitobga  \nberilish, bilishga qiziqish, tinglangan asar haqidaga taassurotlarni o`rtoqlashish istagi va  \nko`nikmasini  tarbiyalashi  lozim.  Bolalar  kitobi  bolaga  tushunarli  bo`lgan,  uning  aqli va qalbiga  \nyetib borgan, ya’ni bola yozuvchining hikoya qilayotgan narsasini tushunadi hamda his eta  \nolgan taqdirdagina  u o`zining tarbiyaviy rolini bajaradi. Tarbiyachilar bolalarning asarlar  \nhaqidagi bilimlarini mustahkamlab borish uchun oldingi yosh guruhlarida ularning qaysi asarlar  \nbilan  tanishganlarini  bilishlari  kerak.  Bolalarning  qiziqqan  narsalarini,  ularning  o`zaro  \nmunosabatlarini,  o`yinlarini  kuzatish  yaxshi  kitob  tanlashga  ancha  yordam  beradi.  \nTarbiyachi asar tanlagach, bolalarga uni qanday yetkazish lozimligini aniqlab olishi  \nkerak. Notanish so`zlari ko`p bo`lgani uchun bolalar mazmunini tushuna olmaydigan asarni  \no`qimagan  ma’qul.  O`tmishda  tarixiy  davr  bilan  bog`liq  bo`lgan  mumtoz  adabiyot  \nnamoyondalarining  asarlarida,  masalan,  A.S. Pushkin  ertaklarida,  xalq ertaklarida  uchraydigan  \njuda  ko`p  notanish  so`zlarni  tushuntirish  kerakmi?  degan  savol  tug`iladi.  Bu so`zlar  bolalarning  \nasar mazmunini  tushunib  olishlariga  xalaqit  bermaydimi,  shuning  uchun  ularni  tushuntirmasa  \nham  bo`ladi.  Badiiy  adabiyotdan  foydalanadigan  mashg`ulotlar  g`oyat  osoyishta  sharoit  \nyaratilishini  talab  etadi.  Muhimi,  xech  narsa  bolalarni  eshitishdan  chalg`itmasligi  lozim.  \nMashg`ulotlarni tez uyushtirish, uni bolalar adabiyotiga xos sog`lom, optimistik xarakterda,  \nxushchaqchaqlik bilan o`tkazish kerak. Tarbiyachi bolalarni xushmuomalalik va osiyishtalik  \nbilan o`tirishga taklif etib, bolalarning qanday o`tirganini tekshirib, mashg`ulotni boshlaydi.  \nMashg`ulot  boshlangan  vaqtdayoq  bolalarning  diqqatini  jalb etish  juda  muhim.  Bunga  \nogohlantirish,  po`pisa  qilish  («Eshitinglar,  keyin  sizlardan  so`rayman»)  bilan  emas,  balki  qiziqarli    «POLISH  SCIENCE  JOURNAL»   \n66  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \nmuqaddima so`zi, muqaddima suhbati bilan erishish mumkin. Chunki ularda kitob mazmuniga  \nqarab  yorqin  surat,  topishmoq,  bolalar  tajribasidan  va hokazolardan  foydalansa  bo`ladi.  \n \nFOYDALANILGAN  ADABIYOTLAR:  \n1. Qodirova  F., Qodirova  M. Bolalar  nutqini  o’stirish  nazariyasi  va metodikasi.  Toshkent  – \n2006  yil.   «POLISH  SCIENCE  JOURNAL»   \n67  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \nRahimova  Shoira  Adilbekovna  \nO’qituvchi  \nUrDU  \n(Urganch,  O’zbekiston)  \n \nBOSHLANG’ICH  SINF  O’QUVCHILARINING  MATEMATIK  QOBILIYATLARINI  \nOSHIRISHDA  MATEMATIK  ERTAKLARNING  O’RNI  \n \nAnnotatsiya.  Ushbu  maqolada  o’quvchilarning  hayotiy  mazmunli  matematik  \nertaklardan keng foydalanish va shu asosda mantiqiy biri -biriga bog’langan tushunchalar,  \nta‘riflar,  qoidalar  va xulosalarni  keltirib  chiqarish  o’quvchilar  matematik  qobiliyatlarini  \nrivojlantirish  haqida so’z yuritiladi.  \nKalit  so’zlar:  matematik  ertak,  ta’rif,  matematik  qobiliyat,matematik  tasavvur,  \ntafakkur.  \n \nAnnotation.  In this  article,  it is given the development  of pupils'  mathematical  skills  by \nmaking extensive use of vital mathematical tales and drawing logical, interconnected concepts,  \ndefinitions,  rules,  and conclusions.  \nKeywords:  mathematical  tale,  definition,  mathematical  ability,  mathematical  \nimagination,  thinking.  \n \nАннотация. В этой статье обсуждается развитие математических навыков  \nучащихся путем широкого использования жизненно важных математических сказок и  \nсоставления  логических,  взаимосвязанных  понятий,  определений,  правил  и выводов.  \nКлючевые  слова:  математическая  сказка,  определение,  математические  \nспособности,  математическое  воображение,  мышление.  \n \nBoshlang ’ich ta‘limda  matematika  o’qitishning  asosiy  maqsadlaridan  biri ham  \no’quvchilar  intellektual  tafakkurini  shakllantirish  asosida  o’quvchilar  qobiliyat  va qiziqishlarini  \nrivojlantirish  hisoblanadi . Demak , boshlang ’ich sinflarda  arifmetik  amallar  tushunchasini  \nmohiyatini  va hisoblash  usullarini  yetkazish  uslubiyatini  ishlab  chiqish  o’z ichiga  o’quvchilarda  \numuman  boshlang ’ich matematika  asosiy  tushunchalarni  shakllantirish  va ularni  amalda  qo’llay \nolish  ko’nikma  va malakalarini  rivojlantirishni  oladi . \nBoshlang ’ich ta‘limda  matematika  o’qitish  arifmetik  amallar  va tushunchalar  mohiyatini  \nochish  orqali , hayotiy  mazmunli  matematik  ertaklardan  keng  foydalanish  va shu asosda  \nmantiqiy  biri-biriga  bog’langan  tushunchalar , ta‘riflar , qoidalar  va xulosalarni  keltirib  chiqarish  \no’quvchilar  matematik  qobiliyatlarini  rivojlantirishga  xizmat  qiladi . Shu fikrlardan  kelib  chiqqan  \nholda  aytish  mumkinki  hozirgi  kunda  bolalarda  matematik  tasavvurlarni  o’stirishda  ertaklarda  \nifodalangan  matematik  tushunchalar  orqali  bolaning  matematika  darslarini  o’qitish  sifatini  \noshirishdagi  matematik  ertaklarning  ahamiyati  uning  tafakkur  dunyo  qarashi  va matematik  \ntasavvurini  o’stirish  matematika  ta’limining  keyingi  bosqichlari  uchun  bolalarni  tayyorlaydi  va \nkomtensiyasini  rivojlantiradi . Matemetik ertaklar orqali noodatiylikka yondashilgan holda  \nboshlang’ich  sinf o’quvchilarining  matematika  darslari  samaradorligi  oshirish  mumkin.    «POLISH  SCIENCE  JOURNAL»   \n68  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \n“Majoziy ertaklarda majoz poetik vosita sifatida hal qiluvchi hisoblanadi. Qatnashuvchi  \npersonajlar  faqat  hayvonlar  bo‘lib,  doimo  inson  tilida  so‘zlab,  o‘zaro  munosabatda  bo‘ladilar.  \nHayvonlarning  o‘zaro  to‘qnashuvi  orqali  insoniy  munosabatlar  ochiladi.  Shunday  ekan  ertaklar  \norqali  bolaning  insoniylik  fazilatlarini  ham  tarbiyalab  boramiz  va darslarda  qo’llanilgan  \nmatematik  ertaklarning ahamiyati  juda  katta ahamiyat  kasb  etadi.  Demak,  quyida  bir nechta  \nmatematik  ertaklarni  keltiramiz:  \n \n \n \nberish.  Qiziquvchan maymuncha  \n(4 sinf o’quvchilari  uchun)  \nMaqsad:  O`quvchilarga  harakat  tezligi,vaqti  va masofa  haqida  tushunarli  ma`lumot  \n \nBir bor ekan, bir  yo`q  ekan,  katta  o`rmonning  bir chekkasida  maymunlar  oilasi tinch  va \nahil yashashar ekan. Bu oilaning eng kichik va o`jar, qiziquvchan maymunchasi ham bo`lib, u  \nhech kimning gapiga quloq solmas ekan. Na buvisini, na bobosini, na ota -onasini, na opa -singil - \nu aka-ukasini  gapiga  kirar  ekan.  \nKunlardan  bir kuni o`jar  maymuncha  juda  zerikib  ketibdi.  Uyidagilar  unga  o`rmonga  \nyolgir chiqishi xavfli ekanligini qancha uqtirishmasin baribir foydasi bo`lmabdi.U o`rmonni sayr  \nqilishni  xohlab  qolibdi.  O`jarlik  qilib  yolg`iz  o`zi yashirincha  o`rmonga  sayohatga  chiqibdi  ketibdi.  \nMaymuncha avvaliga 5 km soat tezlik bilan yura boshlabdi. 20 km yurgandan keyin  \nuning oldidan kattakon ayiq chiqib qolibdi. Ayiq uni yeb qo`ymoqchi bo`lgan ekan, uning  \nqo`lidan maymuncha arang qochib qutulibdi. O`jar maymuncha yana 42 km yo`lni 7 soatda  \nbosib  o`tibdi  va daryo  bo’yiga  yetib  kelibdi.  Bechora  maymuncha  juda  chanqab  ketib  daryodan  \nsuv ichmoqchi  bo`libdi.  U daryo  ichida  pistirmada  yotgn  timsohni  sezmay  endi  suv ichmoqchi  \nbo’lganida timsoh unga hujum qilibdi. Maymuncha timsohning changalidan ham arang qochib  \nqutulibdi. Och qolgan, chanqagan maymuncha oilasi gapiga quloq silmagani uchun juda  \nafsuslanibdi.  U arang  juda  past  tezlikda  6 soatda  uyiga  yetib  boribdi.  Oila a`zolaridan  kechirim  \nso`rab, boshqa o`jarlik qilmaslikka, aytilgan gapga quloq solishga va`da beribdi. Shunda  \nuyidagilar  uni kechirishibdi  va aqli kirgani  uchun  juda  xursand  bo`lishibdi.  Uyidagilarni  \naytishicha  daryodan  uyigacha  bo`lgan  masofa  12 km ekan…  \nSavollar:  \n1. Maymuncha  ayiqni  uchratguncha  necha  soat  yo`l yurgan?  \n2. Maymuncha  daryogacha  necha  tezlik bilan  harakatlangan?  \n3. Maymuncha uyigacha  qancha  tezlik  bilan  harakatlangan?  \n4. U jami  qancha yo`l  yurgan?  \n5. U jami  necha  soat  yo`l yurgan?  \nJavoblar:  \n1. 20:5=4  soat \n2.42:7=6  km/ soat \n3.12:6=2  km/soat  \n4.20+42+12=74  km \nBu matematik ertagimizda xulosa sifatida aytadigan bo’lsak o’quvchilarning matematik  \nqobiliyatlarini oshirishda kommunikativ kompetensiyalari shakllanishida yordam beribgina  \nqolmay,  matematikaga  oid atamalarning  ma’nosini  tushunib  to‘g‘ri  o‘qiy  olish  kabi vazifalarni  \nshakllantirishga  xizmat qiladi.    «POLISH  SCIENCE  JOURNAL»   \n69  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \nQuyonvoylar  oilasi  \n(3 -sinf o'quvchilari  uchun)  \nMaqsad:O’quvchilarning  matematik  savodxonligini  va qobiliyatlarini  oshirish,  \n \n \nBor ekanda yo’q ekan,och ekanda to’q ekan,shunday tinchlik zamonda,bizga yaqin  \no’rmonda  quyonboyevlar  oilasi  yashagan  ekan.  Uzunquloq  tog’aning  oz emas  ko’p  emas  naq \nyettita  bolasi  bor ekan.To’rttasi  o’g’il  qolgani  qiz ekan.  Aka-uka, opa-singillar  nihoyatda  ahil va \njuda  mehnatkash ekan.Hosil yig’im  terimi avjiga  chiqqan paytda  quyonbolalar otalariga  \nergashib,o’rmondan  uncha  uzoq  bo’lmagan  ekinzorga  borishibdi,Borishibdiyu  pishib  yetilgan  \nsabzavotlarni  ko’rib  sevinib  ketishibdi.  Otalari  boshchiligida  har biri oltitadan  sabzi  \nterishibdi.So’ng sabzilarni qopga solib xursand bo’lib uyga qaytishibdi.Yo’lda uzunquloq va  \nuning bolalari ayyor tulkiga duch kelishibdi. Uzunquloq va uning  beshta bolasi o’t -o’lanlar ichiga  \nkirib yashirinishibdi.qolgani esa jon holatda qocha boshlabdi. Tulki Uzunquloq bobo va uning  \nbolalarini qayerga yashiringanini payqay olmay qolgan quyonlarning izidan quvib ketibdi.  \nQochib  ketgan  quyonlar  sohil  bo’yida  qayiqda  suzib  ketayotgan  tipratikan  va toshbaqani  ko’rib  \nulardan yordam so’rashibdi va shu qayiqda suzib ketishibdi. Tulki esa ularni tutolmabdi.  \nUzunquloq bobo va quyonbolalar ularni hamma yerdan qidirishibdi ammo topolmay qayg’uga  \nbotishibdi.Qochib  ketgan  quyonlar  tipratikan  va toshbaqaga  bo’lib  o’tgan  voqeani  aytib  berib  \nuylariga eltib qo’yishlarini iltimos qilishibdi. Shunda tipratikan va toshbaqa ularga savol  \nberishini,  savolga  to’g’ri  javob  berishsa  olib borishlarini  aytibdi.  Quyonlar  rozi bo’lishibdi.Savol  \nquyidagicha ekan: Toshbaqa menda 21 ta olma bor, tipratikanning olmalari esa mening  \nolmalarimning 3/1 qismiga teng.Sizlar bizga tipratikan bobongizda qancha olma borligini va  \nikkalamizda  nechta  olma  bo’lishini  ayting  debdi.  Quyonlar  birga  o’ylab  shunday  javob  \nberishibdi:Tipratikan  bobomda  sizdagi  olmaning  3 dan 1 qismicha  olma  bo’lsa  ularda  7 ta olma  \nbor bo’lgan.Ikkalangizda esa birgalikda 28 ta olma bo’ladi deyishibdi. Bu javobni eshitgan  \ntoshbaqa va tipratikan quyonvoylarning aqliga qoyil qolishibdi va ularni uylariga olib borib  \nqo’yishibdi. Uzunquloq quyonvoy va uning bolalari ularni ko’rib juda sevinishibdi.Quyonlar  \noilasi a’zolari toshbaqa va tipratikanga rahmatlar aytib ular bilan bir umrga do’st bo’lib  \nqolishibdi.  \nSavollar:  \n1. Uzunquloq  boboning  nechta  bolasi  bor edi?(7  ta) \n  «POLISH  SCIENCE  JOURNAL»   \n70  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \n2. Quyonlarning  nechtasi  o’g’il,nechtasi  qiz edi? (7-4=3)  \n3. Ekinzorga sabzi  terish  uchun  hammasi  bo’lib  nechta  quyon bordi?  (7+1=8)  \n4. Uzunquloq  bobo  boshchiligida  quyonlar  har biri nechtadan  sabzi  terishdi?  (6 tadan)  \n5. Quyon  bolalar  birgalikda  qancha  sabzi  terishgan?  (6*7=42)  \n6. Uzunquloq  boboning  nechta  quyon  bolasi  tulkidan  qo’rqib  qochib  ketdi?  (7-5=2)  \n7. Quyonlar  masalaning  javobini  qanday  yo’l bilan  topishdi?  \nYechish:  \n21:3=7  \n7* 1=7 \n21+7=28  \nJavob:28  ta \nMatematika darslarida ertak ichida masala  keltirish orqali darslarni tashkil  etish  \njarayonida  o`quvchilarga  ertaklarni  taqdim  qilish  ularning  fanga  bo`lgan  qiziqishlarini  yanada  \norttirishga va o`z qobilyatlarini yaxshiroq namoyon etishga yordam beradi. Ayniqsa bunday  \ndarslarni ko’proq o’tkazish orqali bolalarning turli vaziyatlarga kreativ yondashishlariga yordam  \nberadi,  matematik  bilimlar  orqali  adabiyotga  bo`lgan  qiziqishlarini  ham  yanada  kuchaytiradi.  \n \nFOYDALANILGAN  ADABIYOTLAR:  \n1. “Математические  сказки”,  Е.А. Алябьева.  «ТЦ Сфера»,  оформление,  иллюстрации,  \n2018  \n2. O’zbek  xalq ertaklari  \n3. through fairy -tales to math in the lessons.:  \nhttps:// www.researchgate.net/publication/26629435    «POLISH  SCIENCE  JOURNAL»   \n71  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \nSadridinova Umida To'xtaboyevna  \nShofirkon  tuman  5-son davlat  maktabgacha  ta’lim  tashkiloti  psixologi,  \nQilicheva Feruza Baxranovna  \nShofirkon  tuman  5-son davlat  maktabgacha  ta’lim tashkiloti  tarbiyachisi  \n(Buxoro,  O`zbekiston)  \n \nMAKTABGACHA TARBIYA YOSHIDAGI BOLALARDA MILLIY  \nQADRIYATLAR  VOSITASIDA  XULQ  MADANIYATINI  SHAKLLANTIRISH  \n \nRezyume . Maqolada  maktabgacha  tarbiya  yoshidagi bolalarda milliy  qadriyatlar  \nvositasida  xulq madaniyatini  shakllantirish  haqida  ma`lumot  berilgan.  \nKalit so’zlar : maktabgacha  tarbiya,  xulq madaniyati,  qadriyat,  axloq,  odob,  ma’naviy  \nxulq.  \nMilliy  qadriyatlar  maktabgacha  tarbiya  yoshidagi  bolaning  kundalik  faoliyatida  odat,  \nehtiyojga  aylangan  taqdirdagina  uning  ma’naviy  shakllanishiga  ta’sir  etadi.  Ayniqsa,  \nmaktabgacha tarbiya yoshidagi bola xulqiga ta’sir etuvchi insonparvarlik, ma’rifatparvarlik,  \nmehr -muhabbat,  poklik,  katta  yoshlilarga,  ota-onalarga  hurmat,  mehr -muhabbat,  \nxushmuomalalik,  do`stlik, saxiylik, shirinso`zlik, kabi xislatlarni  tarkib toptiruvchi  axloqiy  \nmazmundagi  qadriyatlar  bog`chada,  oilada,  jamoat  joylarida  samarali  bo`ladi.  \nQadriyatlar  ijtimoiy  hayotimizda  shaxsiy,  oilaviy  qadriyatlar  tarzida  namoyon  bo`ladi.  \nQadriyatlarimizni  tiklash  va yosh  avlod  tarbiyasiga  singdirish  o`zbek  xalqi  an’analari  orqali  olib \nboriladi. An’ana avloddan -avlodga o`tib boradigan va jamoatchilik fikri bilan qo`riqlanadigan  \nijtimoiy axloq va munosabatlarning umumlashgan me’yor va tamoyillaridir. Har bir an’ana  \nshakllanishida obyektiv zaruriyatlar – xalqning yashash sharoiti, madaniy ishlab chiqarishi va  \nma’naviy madaniyati muhim ahamiyatga ega. Markaziy Osiyo xalqlarida, jumladan, o`zbek  \nxalqida  qadimdan  tarkib  topgan an’analar  jumlasiga  mehnatga  muhabbat,  Vatanga  sadoqat,  \ninsonlarga hurmat, mehr -muhabbat, hashar, bir -biriga ko`maklashish va boshqalar ni kiritish  \nmumkin.  \nO`zbek xalqiga xos bo`lgan ota -onani hurmat qilish va ularga yaxshi munosabatda  \nbo`lish, jamoat joylarda o`zini tuta bilish, mehnatsevarlik, halollik, rostgo`ylik, andishalilik,  \nshirinso`zlik o`zbek xalqining ajoyib odob -axloq, ma’naviy xulq jumlasiga kiradi. Shuning uchun  \nmilliy qadriyatlarimiz sanaladigan ma’naviy madaniyat manbalarini o`rganish, ularda olg`a  \nsurilgan g`oyalarga amal qilish, kelgusi avlodlarga qadriyat sifatida o`tkazish jamiyatning,  \nqolaversa, har bir kishining asosiy burchi. Qadriyatlar dan maktabgacha tarbiya yoshidagi  \nbolalar  xulq  madaniyati  tarbiyasida  foydalanish  o`ziga  xos xususiyatga  ega.  Chunki  bu davrda  \nbolalar  xulqi  va irodasi  shakllana  boradi.  Masalan,  ular ijtimoiy  faydali  faoliyati  jarayonida  o`z \nguruhida tartib -qaidaga rioya etish, tengdoshlariga yordam berish, o`yin va mashg`ulotlarda  \nturli narsalar yasashda jiddiy munosabatda bo`lishi orqali xulq -odob qoidalariga rioya etish  \nko`nikma va malakalari shakllana boradi. Bunga, albatta, oila va bog`chadagi muhit va milliy  \nqadriyatlar katta turtki bo`ladi. Malakalar takomillasha borib, ong ko`nikmalariga aylanadi.  \nMasalan,  ertalabki  gimnastika  mashqlarin  uyida  ham  bajarishi,  o`z o`yinchoqlarini  va \nkiyimlarini, o`z xonasini tartibli saqlashi, kattalar va  o`z tengdoshlari bilan xushmuomalada  \nbo`lishi  kabilar  kundalik  ehtiyojlar  vositasida  ko`nikma hosil  qilib  boradi.  Bunda bolalar,  albatta,    «POLISH  SCIENCE  JOURNAL»   \n72  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \no`z xatti -harakatlarini ongli ravishda tushunib, to`g`ri xulosalar chiqarishi va ijobiy natidaga olib  \nkelishi  kerak.  \nBolalarda xulq madaniyati qoidalariga rioya etishning to`g`ri olib borilmasligi sababli  \nquyidagi zararli xatti -harakatlarni paydo qilishi mumkin: tirnoqni tishi bilan sindirib tashlash,  \nto`g`ri  yuvinmaslik,  ovqatlanish  qoidalariga  rioya  etmaslik,  o`z aytganini  qildirish  uchun  \njazavaga tushish holatlari va boshqalar. Bunday xatti -harakatlar oldini olish uchun milliy  \nqadriyatlardan faydalanishda tarbiya jarayonining yaxlit va uzviyligiga rioya qilish, ya’ni tarbiya  \nuzluksizligi,  uslub  va vositalar  samaradorli gi, tarbiyaviy  ta’sir  tizimliligi,  bolalarda  xulq - \nmadaniyat  qoidalarining  odatga  aylanishiga,  bolalarni  hurmat  qilish  va ularda  ishonch  \ntuyg`usini paydo qilishga erishish muhim ahamiyatga ega. Chunki ozgina bo`lsa ham xatoga yo`l  \nqo`yish,  bolalarda  hurmat  – dushmanlikka,  hamdardlik  va ayanch  – boqibeg`amlik  va \nko`rolmaslikka, adolat – zulmga, insonparvarlik – jaholatga aylanishi mumkin. Eng yomon  \ntomoni  shundaki,  bolada  o`ziga  va o`zgalarga  bo`lgan  ishonch  tuyg`usining  yo`qolishi.  \nMaktabgacha tarbiya yoshidagi bolalarda xulq madaniyatini tarbiyalash, eng avvalo,  \nijtimoiy  yo`nalish  kasb  etishi  muhim  hisoblanadi.  Mazkur  faoliyat  bolalar  o`zaro  \nmunosabatlarida,  o`rtoqlarining  fikrlarini  baholashda,  ularning  xatti -harakatlariga  munosabat  \nbildirishda  ko`rinadi.  Bunda  salbiy  xatti -harakatlarni  qoralash  va ijobiy  ishlarni  \nrag`batlantirishlari muhim ahamiyatga ega. Lekin bu holatlar hali beqaror bo`lishi mumkin.  \nShunga ko`ra maktabgacha tarbiya yoshidagi bolalarda xula -odob me’yorlariga amal qilishi,  \nbajarayotgan ishini oxiriga yetkazishi, hamkorlikda mehnat qilishga o`rganishi, tartib -qoidalarni  \nbuzgan bolalarning bu xatti -harakatlarini bartaraf etishga yo`naltirishda turli uslub va usullarni  \nqo`llashi  muhim ahamiyat kasb  etadi.  \nMaktabgacha  tarbiya  yoshidagi  bolalarning  his-tuyg`ulari  va o`ziga  xos xususiyatlarini  \nhisobga olgan holda, ularning turli o`yinlarni bajarishga kirishib keta olishi, o`zaro kelishib  \no`ynay  olishi,  bir-birining  qiziqishini  hisobga  olishini  nazarda  tutib,  ana shu faoliyati  jarayonida  \nxulq  madaniyati  tarkib  toptirib  boriladi.  Bu tajribalar,  taassurotlar  o`zaro  munosabatlar  \nmohiyatini o`zgartirib boradi. Chunki hamkorlikdagi o`yin bolalarda bir -biriga g`amxo`rlik,  \no`rtoqlik va  do`stlik munosabatlarini paydo  qiladi.  Katta bog`cha  yoshida  o`rta bog`cha  \nyoshdagilarga  qaraganda  munosabatlarida  onglilik  paydo  bo`ladi.  Ularda  faollik,  \ntashabbuskorlik,  mustaqillikka  intilish  xususiyatlari ortib boradi.  Mustaqillik esa,  ba’zida  \ntengdoshlari,  katta  yoshlilar  bilan  nizo paydo  bo`lishiga  olib kelishi  mumkin.  Bunday  vaziyatda  \nmurabbiy yoki boshqa katta yoshlilar beparvo bo`lsa, bolalarni o`z holiga tashlab qo`ysa, ularda  \nmada niy xulq  xislatlari  emas,  balki  injiqlik,  o`jarlik,  xudbinlik  paydo  bo`lishi  mumkin.  \nMaktabgacha  tarbiya  yoshidagi  bolalarda  tashkilotchilik  xislatlari,  rivojlana  boradi.  Ular  o`yin  \nva mashg`ulotlar  mavzuini  tanlab,  o`rtoqlarini  bu mashg`ulotlarga  jalb etadi.  Mazkur  \ntashkilotchilik  faoliyat  ularda  jasurlik,  faollik,  mustaqillik  kabilarni  tarbiyalay  boshlaydi.  Ammo  \nbolalar  o`rtasida  bu yoshda  dangasalar,  shuhratparastlar,  xudbinlar,  maqtanchoqlar  bilan  bir \nqatorda  yengiltaklar,  o`ziga  ishonmaydiganlar  ham  uchraydi.  Ana shundaylar  bilan  \ntarbiyachilar,  ota-onalar  va boshqa  katta  yoshdagilar  ehtiyotkorlik  bilan  munosabatda  \nbo`lishlari, ularga faol bolalar kabi bir xil munosabatda bo`lib, tarbiyaviy ishlar olib borishlari  \nkerak.  \nQadriyatlarimiz  bolalarning  jismoniy,  aqliy,  ma’naviy  kamol  topishida,  uning  kelajakda  \nturmush  tarzini,  hayotini,  jamiyatda  o`z o`rnini  topishda  asosiy  omil  bo`lishi,  ya’ni  to`laqonli    «POLISH  SCIENCE  JOURNAL»   \n73  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \nmadaniyatli fuqaro bo`lib yetishishining asosiy mezoni bo`la olishi bilan muhim tarbiya vositasi  \nsanaladi. Ammo uni tanlashda bolalar o`yinlarini to`g`ri yo`lga boshqara olish, samarali uslub va  \nusullarni qo`llash,  bolalar  yosh  xususiyatini  hisobga  olish  muhim  va zurur.  \n \nFOYDALANILGAN  ADABIYOTLAR:  \n1. Pedagogik  maxorat  asoslari.  Muhammadova  Z. T.: TNB kitob.  2009.    «POLISH  SCIENCE  JOURNAL»   \n74  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \nSultonova  Marg`uba  Olimovna  \nShofirkon  tuman  11-son davlat  maktabgacha  ta’lim  tashkiloti  tarbiyachisi,  \nNazarova  Saodat  Ibrohimovna  \nShofirkon  tuman  11-son davlat  maktabgacha  ta’lim  tashkiloti  tarbiyachisi  \n(Buxoro, O`zbekiston)  \nMAKTABGACHA  TA’LIM  MUASSASALARIDA  BOLALARNI  TABIAT  BILAN  TANISHTIRISH  \nRezyume . Maqolada  maktabgacha  ta’lim  muassasalarida  bolalarni  tabiat  bilan  \ntanishtirish  haqida ma’lumot berilgan.  \nKalit  so`zlar : bola,  tabiat,  ekologiya, tarbiya,  o`simlik,  hayvonlar.  \n \nBolalarni  tabiat  bilan  tanishtirish  jarayoni  har tomonlama  rivojlangan  shaxs  ma’naviy  \nqiyofasini  kamol  topish  jarayonining  muhim  va tarkibiy  qismidir.  U bolalarda  milliy  va \numuminsoniy  qadriyat  sifatida  tabiatga  ongli  munosabat,  tabiat  zaxiralarini  saqlash  va \nko`paytirishga  oid mas’uliyat  tuyg`usi,  ekologik  muammolarni  amaliy  hal qilishdagi  \nko`nikmalarning  rivojlantirilishini  o`zida  qamrab  oladi.  \nSayyoramizda jumladan, yurtimiz hududida vujudga kelgan ekologik vaziyat ta’lim - \ntarbiya  tizimi  oldiga  quyidagilarni  hal etishni  muhim  vazifa  etib qo`yadi,  ular:  \n1) Bolalarning tabiatga ongli munosabatlari, tabiat komponentlarini yaxlitlikda va  \no`zaro  aloqadaligini  his etish;  \n2) Tabiatning  bolalar  ekologik  madaniyatini  belgilab  beradigan  vosita  sifatida  namoyon  \nbo`lishi;  \n3) Atrof  muhitga  munosabatni  ma’naviy  madaniyatning  ajralmas  qismi  ekanligini  \nanglash;  \n4) Tabiat bolaning ekologik madaniyatini shakllantirishi uchun asosiy omil ekanligini  \ntushunish;  \n5) Atrof  muhitga  nisbatan  bolalarni  tabiat  bilan  tanishtirishning  shakllantirishda  \nularning  individual  xususiyatlarini  hisobga  olgan  holda  ta’lim -tarbiya  jarayonida  atrof  muhitni  \nmuhofaza qilishni ekologik qadriyat sifatida anglab yetishlariga ilmiy -amaliy jihatdan sharoit  \nyaratish;  \n6) Bolalarni tabiat bilan tanishtirishni shakllantirishning maqsad va vazifalarini aniq  \nbelgilab  olish,  uning  amaliyotga  yo`naltirilganligini  tushunish;  \n7) Ekologik  muammoga  umummilliy,  umuminsoniy,  ijtimoiy  muammo  sifatida  \nyondashish  kabilar.  \nBolalarga  o`rgatiladigan  ekologik  bilim  mazmuni  o`z ichiga  quyidagilarni  qamrab  olishi  \nlozim: Bolalar ongida olamning ilmiy manzarasini shakllantirish; tabiat va jamiyat o`rtasidagi  \nma’lum bo`lgan ekologik bilimlar ahamiyatini ilmiy -amaliy jihatdan yoritib berish; Bolalar  \ntomonidan  ekologik  qonuniyatlarning  o`zlashtirilishiga  erishish;  tabiatda  sodir  bo`ladigan  turli \nxildagi fojialarni anglash, ularning mohiyatini bilib olish, ekologiyaga oid ta’lim vazifalarini  \nifodalash, kuzatish va tajribalarni rejalashtirish  hamda amalga  oshirish, ekologiyaga oid  \nnazariy -amaliy  g`oyalarni  oydinlashtira  olish, ulardan  ta’lim -tarbiya  jarayonida  foydalana  olish  \nmalakalarini  hosil  qilish;  tabiatga  qadriyat  sifatida  qarash  tuyg`usini  uyg`otish  va madaniyatini    «POLISH  SCIENCE  JOURNAL»   \n75  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \nshakllantirish,  tabiatni  muhofaza  qilish  va uning  boyliklaridan  unumli  foydalanishni  \ntakomillashtirish; yurtimiz hududidagi biologik xilma – xillikni bilishga imkon beradigan nazariya  \nva g`oyalarni o`rganishni tashkil etish; ekologiya sohasidagi muayyan bilimlar hajmini, tasavvur  \nva tushunchalar  doirasini  ilmiy -amaliy  jihatdan  ishlab  chiqish;  atrof  muhitni  muhofaza  qilish,  \nuning zahiralaridan unumli foydalanishga o`rgatish; bolalarni atrof muhit bilan muloqot davrida  \nekologik amaliyotni kengaytirish, ularda atrof muhitga n isbatan faol ta’sir etuvchi ekologik  \nmunosabatni  tarkib  toptirish.  \nTabiat  vositasi  tarbiya  berish  umumiy  tarbiyaning  bir qismi  bo`lib,  u o`z oldiga  \nbolalarning  yosh  xususiyalarini,  bilish  jarayonlarini,  qobiliyatlarini  yaxshi  bilgan  holda  tarbiya  \nberishni vazifa qilib quyadi. Maktabgacha tarbiya ishida uning ko`p qirralari bor, bu pedagogika  \nfani qonuniyatlarini  o`rganish  bilan  shug`ullanadigan  juda  keng  ijtimoiy xodisa.  \nBolalar yoshiga xos jonli va jonsiz tabiat haqida oddiy tushunchalar berish bolalarga  \nberiladigan bilimlar oddiy bo`lishi bilan birga ilmiy aniq bo`lishi kerak. Masalan, o`simlikni  \nyorug`likka, suvga, issiqlikka bo`lgan ehtiyoji. Bolalarga beriladigan bilimlarning barchasi  \nko`rgazma  asosida  berilishi  shart.  \nBolalarni  tabiat  bilan  tanishtirishda  ilmiylik  prinsipiga  amal  qilinadi.  Beriladigan  tabiat  \nhaqidagi bilimlar ilmiy bo`lishi bilan birga bolalar tushunadigan bo`lishi shart. Masalan, erta  \nbaxorda  kunlar  isiydi.  Kunlar  isigani  sababli  daraxtlar  kurtak  chiqara  boshlaydi.  Osmon  ko`m - \nko`k tus oladi, biz endi bog`cha yer maydonchasiga ekin ekishimiz mumkin. Mana shunday  \noddiy  misollar  orqali  bolalar  aniq  va ilmiy  asosda  tushunchalarga  ega bo`ladi.  Tabiatshunoslik  \nbilimlarini oddiydan –murakkablikka tizimli ravishda yaqindan, ya`ni o`z o`lkasi tabiatidan  \nboshlab  uzoq,  ya`ni  boshqa  katta  tabiatni  tanish,  o`rganish  asosida  amalga  oshiriladi.  \nO`z o`lkasi  tabiati  bilan  tanishish  bevosita  bolani  tevarak - atrofdagi,  bog`chasidagi  yer \nmaydonchasidagi o`simliklar, gullat ko`chatlar, daryo va ko`l nomlari, tog`lar, yo`lda uchraydigan  \nmaysalar nomi, xayvonot olami, ya`ni bola kundalik hayoti davomida duch keladigan jonli va  \njonsiz tabiat bilan tanishishdan boshlash zarur. Shu tizimi asosida ish tashkil qilinsa, o`zga  \no`lkaning  tabiati  bilan  tanishishish  engil, oson  kechadi.  \nMaktabgacha  ta’lim  muassasalarida  tabiat bilan yaqindan tanishtirmasdan turib,  \nbolalarni jismonan rivojlantirish, nutq va tafakkurlarini boyitish vazifalarini amalga oshirish  \nmumkin emas. Maktabgacha tarbiya yoshidagi bolalarni tabiat bilan tanishtirar ekanmiz,  \ntabiatga  doir bilimlarning  sodda,  hayotiy,  tushunarli  ekanligini  ham  alohida  uqtirish  kerak.  \nBog`cha bolalari tabiat haqida ma’lumot olar ekan, eng avvalo o`zlari yashaydigan, o`zi  \ntarbiyalanadigan  bog`cha  uchastkasidagi,  uning  tevarak -atrofidagi  o`simlik  va hayvonlar  bilan,  \ntabiat elementlari bilan, u yerning suvi, tuprog`i, iqlimi bilan tanishadilar. Buning foydali tomoni  \nshundaki,  yosh  bolalar  o`zi yashaydigan,  o`zi tarbiyalanadigan  joydagi  o`simlik  va hayvonlarni  \nhar kuni ko`radi, ularning nomini bir kunda bir necha marta eshitadi. Ular bolalar yodida tez  \nqolib,  uzoq  saqlanadi.  \nMaktabgacha  tarbiya  yoshidagi  bolalarni  tabiat  bilan  tanishtirishda  ularning  turli yosh  \nxususiyatlari hysobga olinadi. Ayniqsa jonsiz tabiat, o`simlik va hayvonlar, tabiatdagi mavsumiy  \no`zgarishlar,  tabiatdagi  insonlar  mehnati  haqidagi  bilim  va tushunchalar  oddiylikdan  \nmurakkablikka  prinsipi  asosida  tushuntirilmog`i  zarur.    «POLISH  SCIENCE  JOURNAL»   \n76  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \nFOYDALANILGAN  ADABIYOTLAR:  \n1. Yusupova  P.A. Ekologik  tarbiya.  Tabiat  bilan  tanishtirishning  yo`l va shakllari.  // \nMaktabgacha  tarbiya.  – T.: 1992. – 63 b.    «POLISH  SCIENCE  JOURNAL»   \n77  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \nБошманова  Шахноза  Абдуҳаким  қизи  \nмагистрант  первого  курса факультета  “Педагогика  и психология”,  \nАбдуллаева  Феруза  Абдулла  қизи  \nмагистрант  первого  курса  факультета  “Педагогика  и психология”  \nДжизакский  государственный  педагогический  институт  \n(Джизак,  Узбекистан)  \n \nОРГАНИЗАЦИЯ РАБОТЫ ПО ФОРМИРОВАНИЮ  \nЧИТАТЕЛЬСКОЙ  КУЛЬТУРЫ  У УЧАЩИХСЯ  \n \nАннотация . В статье приведена информация об организация работы по  \nформированию  читательской  культуры  у учащихся.  \nКлючевые  слова : книга,  чтения,  читательский  интерес,  читательская  \nкомпетентность,  читательская  культура.  \n \nКнига играет огромную роль в духовном развитии человека. Особенно велико ее  \nзначение  в период  интенсивного  становления  личности  – в детские  годы.  Книга,  \nпрочитанная  в детстве,  нередко  оставляет  неизгладимый  след  в душе,  запоминается  на \nвсю жизнь. Ведь не зря чтение всегда рассматривалось как средство учения и обучения и  \nкак средство  воспитания в  человеке человека.  \nЧитательский  интерес  ‒ это направленный  интерес,  проявляемый  в активном  \nотношении  читателя  к человеческому  опыту,  заключенному  в книгах,  и к своей  \nспособности  самостоятельно  добывать  этот  опыт  из книг.  При этом  обязательно  \nпроявление  читателем  умственной  и эмоциональной  активности,  чтобы  \nцеленаправленно  ориентироваться  в книжном  окружении,  в книге,  как инструменте  для \nчтения, в тексте, как основном компоненте книги, хранящем и передающем читателю  \nэтот  опыт. (Горшкова  М.Л.).  \nКультура чтения – это процесс, включающий восприятие текста, его понимание и  \nинтерпретацию  произведения,  причем  уровень  восприятия  определяется  читательским  \nопытом ученика, его литературным развитием. Высокий уровень чтения обязательно  \nхарактеризуется творческим восприятием, способностью создавать новые образы, новую  \nреальность,  способностью вступать  в диалог с автором.  \nВ связи  с этим  развитие  культуры  чтения  художественного  текста  является  \nважной ступенью в достижении главной цели образования – саморазвития личности и  \nформирования  базовых  общечеловеческих  ценностей.  Читательская  культура  является  \nболее  широким,  разноплановым,  объемным  понятием,  чем культура  чтения.  \nЧитательская культура ‒ это процесс формирования образованного читателя,  \nумеющего  понимать  и эстетически  оценивать  литературные  произведения.  \nЧитательская компетентность – это совокупное личностное качество ученика и  \nуже взрослого человека, сформированное на базе интеллектуальных способностей и  \nличностных  свойств.  \nДеятельность по формированию читательской культуры у учащихся следует  \nрассматривать  как составную  часть  учебно -воспитательного  процесса  и как одну  из форм    «POLISH  SCIENCE  JOURNAL»   \n78  \n SCIENCECENTRUM .PL ISSUE  2(47)  ISBN  978-83-949403 -4-8 \n \nорганизации досуга школьников. Деятельность учителя должна способствовать развитию  \nчитательского интереса и, как следствие, углублению полученных знаний, раскрытию  \nиндивидуальных особенностей каждого ученика, развитию самостоятельности и их  \nтворческой активности. Педагог, формируя читательскую культуру учеников в ходе  \nучебно -воспитательного  процесса,  должен  решать  две основополагающие  задачи:  \nразвитие  потребности  в чтении;  развитие  читательской  компетентности.  \nВ основе работы с детьми по развитию интереса к чтению должны лежать  \nосновополагающие принципы деятельностного подхода и диалогического воспитания,  \nизложенные  в работах А.  Н. Леонтьева,  С. Л. Рубинштейна, В.  И. Слободчикова,  Н. Е. \nЩурковой:  принцип  учета  возрастных  и индивидуальных  периодов  развития  ученика; \nпринцип определения «зоны ближайшего развития» и организации в ней  совместной  \nдеятельности  детей  и взрослых;  принцип  высокой  мотивации  любых  видов  \nдеятельности,  направленных  на формирование  читательской  культуры;  принцип  \nобязательной  рефлексивности  деятельности  ученика;  принцип  сотрудничества  при \nорганизации  управления  различными  формами  деятельности;  диалогический  принцип.  \nРазвитие интереса к чтению школьников можно представить в виде работы по  \nследующим  направлениям:  Внеклассная  литературная  деятельность.  \nЭту деятельность нужно рассматривать как составную часть образовательного  \nпроцесса, которая способствует развитию читательского интереса и, как следствие,  \nуглублению полученных знаний, раскрытию индивидуальных особенностей каждого  \nученика,  развитию  самостоятельности  и творческой  активности  детей.  \nВнеклассная работа по литературному чтению имеет большое значение для  \nформирования  читательской  культуры  школьников.  При организации  этой  работы  \nучитель должен ставить главную цель: формировать интерес школьников к чтению и при  \nэтом  решать  следующие  задачи:  \n- совершенствование  качества  чтения  как основы  глубокого  и полноценного  \nвосприятия  художественного  текста;  \n- развитие  речи  учащихся,  умение  выражать  свои  мысли  и чувства  в устной  и \nписьменной  речи;  \n- освоение  учениками  нравственных норм  и ценностей  посредством  чтения  \nхудожественных  произведений;  \n- формирование  нравственных  качеств  личности  на  примере  литературных  \nгероев.  \nВнеклассная  работа  по литературному  чтению  всегда  планируется  учителем,  при \nэтом  учитель  должен  помнить  о разумном  сочетании  произведений  русской  и \nзарубежной  классики,  современной  и региональной  литературы;  разнообразии  \nтематического плана работы; сочетании произведений различных жанров; чередовании  \nразных  видов  внеклассных  работ  по литературному  чтению;  приёмах  активизации  \nчитательской  культуры  учащихся.    «POLISH  SCIENCE  JOURNAL»   \n79  \n SCIENCECENTRUM .PL ISSUE  2(47)  ISBN  978-83-949403 -4-8 \n \nСПИСОК  ИСПОЛЬЗОВАННЫХ  ИСТОЧНИКОВ:  \n1. Алексеевская  А.Т. статья:  Формирование  читательских  интересов  младших  \nшкольников. - М., 2008.  \n2. Бирюков  Б.В.,  Бутенко  И.А. «Человек  читающий»,  Москва,  2000.    «POLISH  SCIENCE  JOURNAL»   \n80  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \nSECTION:  PHILOLOGY  AND  LINGUISTICS  \n \n \nOchilova Noila  \nTeacher of SamSIFL,  \nHojiyeva Zumrad  \nTeacher of SamSIFL,  \nShodiyeva Lutfiya  \nStudent of SamSIFL,  \nRidvanova Hulkar  \nStudent of SamSIFL  \n(Samarkand,  Uzbekistan)  \n \nTHE METHODOLOGICAL  VIEW  TO PRAGMATICS  AND  SEMANTICS  \n \nAnnotation. This article informs about the methodological view to pragmatics and  \nsemantics,  its types and history.  \nKey words: objective, subjective, analytic, synthetic, Chomskyan approach, Saint - \nSimonian.  \n \nThe first way to articulate  the relationship  concerns  methodologies  of, or met  \ntheoretical stances toward, linguistic sciences. Clearly, they can be objective (deagentivized) or  \nsubjective  (agentive),  analytic  (compositional)or  synthetic  (holistic),  or various  combinations  \nthereof; but we may detect three ideal types in the recent history. The first of such types, best  \nrepresented by the Chomskyan approach and generally characterized by objective and analytic  \nattitudes, is called ‘componential, ’compartmental,’ or ‘modular’ [1, 39]. It sees the linguistic  \nsciences as analytic pursuits dealing with decomposable objects (i.e., modules) such as  \nphonetics, phonology, morphophonology, morphology, syntax, semantics, and pragmatics,  \nnaturalistically  studied  by compositional  methods  and described  independently  of the \nparticular  sociohistorical  contexts  in which  analysts  are involved  as social  agents.  Clearly,  this \nmetatheoretical stance, which underlies the organizational institutions and academic curricula  \nof linguistics,  is typical  of the modern  sciences  coming out  of Baconian  natural  philosophy  in \n17th -century England [2, 88 -131]. That is, it is the stance of the modern natural sciences  \n(Naturwis  Senschaften),  with  their  laws  and other  regularities,  which  try to explain  empirically  \noccurring  phenomena  by appealing  to hypothetically  idealized  quasiempirical  \nregularities(‘covering  laws’)  [3, 112]  analytically  abstracted  from  the total  contexts  of scientific  \nactivities, which are there by decomposed into distinct fields, in our case semantics and  \npragmatics.  \nThis epistemological  characterization  may  be supplemented  with  a pragmatic  \nunderstanding  of the social  and historical  processes  of epistemic  authorization  [4, 78]. That  is, \nthe componential stance toward language is pragmatically positioned in, and presupposing  \nindexes,  the historical  context  of the post -Baconian  specialized  sciences  and,  more  broadly,    «POLISH  SCIENCE  JOURNAL»   \n81  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \nDurkheimian  ‘organic  society’  in modernity,  or the age of social  specialization  and the division  \nof labor. Because the truthfulness, appropriateness, and effectiveness of any actions, including  \nscientific ones, depend on the contexts in which they take place, this implies that the  \ncomponential  stance  and its ‘scientific’  results  receive  their  authority  primarily  from  the post - \nBaconian  modernization  project.  \nThis project is characterized not only by social and epistemic specialization, but also  \nby the standardizing regularization, methodical rationalization, and experimental controlling of  \ncontextual contingencies, chance happenings, and unique events, which earlier were under  \nstood as fate, epiphanies, and other phenomena originating in the heavenly universe, and  \ntranscending the sublunar, empirically manipulable space of human agency [3], [40], [44].  \nHere,  we must  note  that standardizing  regularization,  Saint -Simonian  technocratic  \nspecialization, and Benthamite instrumental rationalization generally characterize the modern  \ndiscipline  of logic  (or, when  applied  to linguistics, of  formal  syntax  and semantics),  which  came  \nto dominate  American  philosophy around  1900  and whose  rise is precisely  due to these  social  \nforces  [2, 43 -70]. \nThus far, we have explored how the three method logical stances are related to how we  \ndefine the disciplinary boundaries and noted the correspondence between the methodological  \nscale, stretching from componentialismto perspectivalism to critical pragmaticism, and another  \nscale, extending from semanticism to complementarism, reductionistic pragmaticism, and total  \npragmaticism.  Like any ideology,  this configuration  of metalinguistic  ideologies  is also \nembedded  in the sociohistoricalcontext;  thus,  we may  observe  a historical  drift  of pragmatics  \nfrom componentialism and semanticism to perspectivalism and complementarism and, finally,  \nto total  and critical  pragmaticism  (Koyama,2001b).  \nThis led to the birth  of modern  semantics,as  it came  to be pursued  by Russell  (1872 – \n1970), Carnap (1891 –1970), Tarski (1902 –1983), and Quine(1908 –2000), as well as by the  \nlinguistic  formalists  of the Copenhagen  and the Neo-Bloomfieldian  Schools,  whose  impressive  \nsuccesses allowed the componential semantic tradition to penetrate into more empirical  \ndisciplines such as psychology and anthropology, where ‘ethnoscience’ came to flourish in the  \nmid-20th century (see Anthropology and Pragmatics). Moreover, the semantic traditions of  \nCarnap  and the Neo-Bloom  fieldian  structural  linguists,  especially  Harris  (1909 –1992),  \nconverged in Chomsky (1928 –), whose generativism began to dominate linguistics in the 1960s,  \n[7, 138] when the semantic tradition moved more deeply into th e empirical fields, thereby giving  \nrise to empirical  semantics,  as witnessed  by the rise of Generative  Semantics,  fuzzy  logic,  and \nRosch’s prototype semantics in the late1960s tomid -1970s, eventually congealing into today’s  \ncognitive linguistics (see Cognitive Linguistics).Such was the evolution of the branch of the  \npost -Fregean  semantic  tradition  that focused  on propositional  content.  In contrast,  the other  \nbranch, focusing on ‘force,’ was developed by the Ordinary Language Philosophers, starting  \nwith Austin, who translated Frege’s oeuvre into English, to be followed by Searle, Grice, and  \nothers.  This branch  is usually  called  ‘pragmatics,’  but it is actually  a kind  of empirical  \nsemanticism, dealing with pragmatic mattersonly insofar as they can be systematically related  \nto propositions  or referential  texts.  \nMeaning  is one of the most  controversial  terms  in the theory  of language.  At first sight  \nthe understanding  of this term seems  to present  no difficulty  at all—it is freely  used  in teaching,    «POLISH  SCIENCE  JOURNAL»   \n82  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \ninterpreting  and translation.  The scientific  definition  of meaning  however  just as the definition  \nof some other basic linguistic terms, such as word, sentence, etc., has been the issue of  \ninterminable discussions. Since there is no universally accepted definition of meaning1 we shall  \nconfine ourselves to a'Л brief survey of the problem as it is viewed in modem linguistics both in  \nour country and  elsewhere.  \nSome critics believe that the cognitive function of a text is more important than its  \npersuasive (or expressive) function; that whilst in translation cognitive (i. e. extra linguistic)  \naccuracy  can and must  always  be achieved,  the other  factor,  the connotative,  lexical,  stylistic  \nor pragmatic,  defies  accuracy,  and is therefore  secondary.  K. Baldinger  has referred  to it as a \n\"halo\" round the conceptual of content; A. Neubert states that only the \"pragmatic\" is  \nuntranslatable.  This widespread  idea  appears  mistaken  to me; a glance  at the entries  for, say, \n\"Munich\"or \"Hitler\" in, say, the Petit Larousse or the Quillet - Flammarion suggests that  \nlexicographers  tend  to leave  out the most important.  \nThe Limits  of word Meaning for  Accuracy’s  sake.  \nA word  can mean  anything at all  under  the following  conditions:  \na) that it has a  stipulative  licence  to do so; \nb) that it forms  part of a special  code;  \nc) that it is spoken  or written  in error,  is a misprint;  \nd) that the author  is writing  under  stress  (fear,  illness).  \nIn all the above cases, the translator still has to discover the word's meaning. However,  \nunder  normal  circumstances,  the meaning  of a word can  never  be wholly  dictated  or \nconditioned  by its linguistic  or situational  context.  The semantic  contours  of conceptual  terms  \nare often vaguer and wider, but system must not be translated as \"arrangement\", unless as part  \nof a recognized  collocation.  \nTheoretically,  at any rate,  all words  have  a minimum  semantic  content,  that is one or \ntwo primary semantic components which form part of each of their meanings, and which must  \ntherefore be \"transferred\" in any translation; these are the boundaries of translation, beyond  \nwhich  translation  becomes  paraphrase.  \n \nREFERENCES:  \n1. Chomsky  N. Language  and consciousness.  Language  50: 1974.  – P 111-33. \n2. Green,  Georgia  M. Pragmatics  and natural  language  understanding.  Pragmatics,  grammar,  \nand discourse.  Radical  pragmatics,  ed. by Peter  Cole,  New York  1989.  – P 167-81. \n3. Jerry  L Morgan.  Pragmatics,  grammar,  and discourse.  Radical  pragmatics,  ed. by Peter  Cole,  \nNew  York:  Academic.  1981. – P 167-81. \n4. Levinson,  Stephen  C. Pragmatic  reduction  of the binding  conditions  revisited.  Journal  of \nLinguistics  27. 1991. – P 107 -61. \n5. Levinson  S. Pragmatics.  Cambridge,  UK: Cambridge  University  Press.  1983.  - 435 p. \n6. Лайонз Дж. Лингвистическая семантика. Введение. - М.: Языки славянской культуры,  \n2003.  - 400 с.    «POLISH  SCIENCE  JOURNAL»   \n83  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \nSaydaliyeva  Anora  Baxtiyor  qizi \ngraduate students  \nTashkent  State  Oriental  Studies  \n(Tashkent,  Uzbekistan)  \n \nTHE MYTHOLOGICAL  BASES  OF THE IMAGE  OF PARY  IN TURK  FOLKLORE  \n \nAbstract. This article examines the mythological basis of the image of a fairy in Turkish  \nfolklore.  Oral works  provide  information  about  the mythical  expression  of the image  of a fairy,  \nits interpretation under different names, and its mythological basis. Also, in this scientific work,  \nthe image of the fairy is analyzed not only in Turkish folk legends but also in the mythology  \ncontext  of the Turkic  peoples.  We study  the subject  in the myths  of Turkish  folklore.  \nKeywords : Turkish  folklore,  fairy  tale,  myth,  mythology,  legend.  \n \nFolklore is a spiritual mirror of a nation, it reflects the customs, traditions, beliefs of the  \npeople,  the specific  character,  and views  of the nation.  The masterpieces  of folklore  are like a \nfountain. The more we study it, the more we realize that we have a national and spiritual  \nheritage.  \nSince  the independence  of Uzbekistan, the  focus  on spirituality has  risen  to the level  of \npublic policy. The principal goal of the Cabinet of Ministers \"On Education\" and \"National  \nTraining Program\" is to educate young people in the spirit of our national values, to instill in  \nthem  a sense  of love  and respect  for our cultural  heritage.  Therefore,  today  the study  of the \noral traditions  of the Uzbek  and fraternal  peoples  is of great  importance  for the development  \nof literature. Uzbek folklorist Jabbor Eshanqul says about folklore: “Folklore is the golden cradle  \nof the nation's spirituality. The endless evolution of the human race is reflected in the spiritual  \nproperty  of each  nation,  in the interpretation  of unique  symbols  in the world  of thought\"6. \nTurkish folklore is a cultural heritage created by the creative children of the Turkish  \npeople over the centuries. This heritage includes proverbs, parables, anecdotes, songs, fairy  \ntales, epics, and other genres. The historical roots of oral art go back to the mythical worldview  \nof the Turkic peoples of Central Asia. This mythical worldview is reflected in the writings of  \nOrhun -Enasay,  \"Oguznoma\",  \"Kitabi  Dada  Korkut\",  \"Devonu  lug'atit -turk\"  composed  by \nKashgari,  \"Hibat -ul haqoyiq\"  by A. Yugnaki,  Yusuf  It is associated  with  literary  monuments  such  \nas Khas  Hajib's  Qutadg'u  Bilig.  The notions  of the eternity  and infinity  of the world, darkness  \nand light, good and evil, the \"friend\" and \"enemy\" forces in nature, gods and spirits, giants,  \nfairies and demons, and other supernatural beings common in mythology. Indeed, mythology  \nreflects  the aspirations  of our ancient  ancestors  to understand  the universe  and their  place  in \nthe universe. After all, mythology comprises man's notions of nature and the universe.  \nMythology is an integral part of human spiritual history. The primitive form of thinking first  \nemerged in a syncretic way. A set of syncretic imaginations is a phenomenon that occurs within  \nthe general requirements of primitive lifestyles and worldviews. Consequently, “primitive man,  \nwho  lacks  the ability  to think  analytically  and abstractly,  does  not separate  himself  from  nature  \n \n6 Eshonqul  J. Folklor:  obraz  va talqin.  Qarshi, “Nasaf”  nashriyoti,  1999.  2-b.   «POLISH  SCIENCE  JOURNAL»   \n84  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \nand sees  himself  as one with  the forces  and phenomena  of nature.  That  is why  man  observes  a \nseries  of phenomena,  tries  to draw  them  to himself,  and thinks  that he is capable  of creating  \nthem,  and on the other  hand,  the forces  of nature  and things,  that is, what  we call inanimate,  \nare unique  to man.  he also sees  the ability  inherent  in the life he lives  in general.  It was natural  \nfor primitive  man  to think  and draw  conclusions  in this way because, in  order  to understand  \nthe existing  historical  conditions  and the essence  of various  phenomena  in nature,  he had to \nhave at least a certain level of imagination and worldview about natural phenomena. As a  \nresult, our ancient ancestors' mixture of natural and social phenomena, that is, generalizations  \nand imaginations,  led to the emergence  and spread  of many  mythological  views,  and then  to \nthe creation of fairy tales. The legends narrated on the ground of myth are distinguished by  \ntheir syncretic character. Ancient examples of them appear in the form of descriptions of  \nsupernatural phenomena, and they tell about traditions, ceremonies, elements of religion,  \ngods”7. \nThe image  of the fairy  in mythological  legends  belongs  to the first seed, the  tribal  form  \nof religion. They formed a separate group and played a key role in determining the origin,  \ndevelopment, and evolution of Islam. These myths, which form a separate category, have their  \nown system of images. It comprises gods, gods, patron cults, titans, warriors, myths, spirits,  \nand holy  forces.  Each  of them  has taken  on symbolic  meaning  and has become  a supporter  of \nclans,  ethnic groups, tribal  beliefs,  and convictions . \nExamples  of legends  involving  the image  of Paris  have  come  down  to us from  historical  \nsources,  memoirs,  the composition  of the \"Avesta\"  created  among  the ancient  Turkic  peoples.  \nAccording  to Avesta,  Payrika is  a symbol  of evil power.  Based  on the information  of the Iranian  \nAvesta  scholar  B. Sarkarati,  A. E. Bertels writes,  \"In one text of Avesta,  the notion  of a fairy  is \ninterpreted  in connection  with  ignorance  and insanity.  In all the texts,  this image  is described  \nonly as an evil being. According to Avesta, \"only certain events believe fairies do good.\" This  \nshows that the cult of the tradition of worshiping it has survived in some places even when  \nZoroastrianism was the dominant religion”8. Positive examples of this type promote such  \nleading ideas as peace, tranquility, glorification of goodness, and condemnation of evil.  \nNegative  copies  serve  as a symbol  of evil, calamity,  and disaster.  The plot is compact, based  on \none or two episodes,  and the complex  events  are unusual.  The episodes  play a crucial  role in \nthe extraordinary power of fairy -tale fairies. In mythological legends, their enlightening  \nsignificance and informational function distinguished images of this type. It based the attitude  \nof these  myths  to reality  on the fact that the events  of the story  are true,  and something  based  \non the criteria for its reflection on exaggeration, fiction. It convinced the primitive listener that  \nhis events were true. Actual events in the fictional system, such as earthquakes and hurricanes,  \ndo not hinder  the story  but help  and build  trust.  We based  the style  of interpretation  on fiction  \nand performed the function of a message. Interpretation of natural phenomena brought  \nimaginary  events  closer  to life and attracted  the listener's  attention  as a reliable  source9. One \nof the most  important  features  of this type  of myth  is that we based  it on a miracle.  Formed  \n \n7 Yildiz  Naciye.  Türk  destanlarinda  kötü  huylu  devler.  Millî  Folklor,  2010.  S. 87. \n8 Bertels  A. N.  Peri.  Kultura  i iskusstva narodov  Srednoy  Azii v derevnosti  i srednevekove.  M., 1979.  S. 126. \n9 Qayumov O. Pari  obrazining  genezisiga  doir.  O‘zbek  tili va  adabiyoti.  1998.  №4.    «POLISH  SCIENCE  JOURNAL»   \n85  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \nbased on mythological images of everyday meaning, these legends mainly told stories related  \nto the concepts  of taboo,  magic,  dualism,  shamanism,  contributed  to the emergence  and \ndevelopment of the religion of fire, and, importantly, the taboo of events. Led to the emergence  \nof a ban on mysterious means. We based it on primitive society, tribe, tribal customs, early  \nworldviews,  and religious  attitudes.  Legends  of this type  originated  and became  a tradition  in \nthe early days of the feudal -patriarchal system, after the collapse of the primitive order of life. \nTheir colorful themes, unique sources, various auxiliary images, and the transmission of ancient  \nbeliefs in fictional myths distinguished Pari figurative mythological legends. We base the origin  \nof this type of myth on the influence of dualism, the constant conflict between the gods of good  \nand evil. They described this in mythological legends as a confrontation between the two gods.  \nTurkish folk legends reflect the ideal scenes of human life through the life of fairies. In  \nTurkish folklore, they can find the image of a fairy under different names, such as mermaid,  \nmermaid,  mermaid,  tree mermaid,  swan,  and they  come  in different  colors,  often  in white  and \nblue  dresses  and looks,  mainly.  Hall girl, pigeon,  half female,  half fish,  snake,  hairless,  \neyebrowless zoomorphic  (animal form),  and Anthropomorphic (human form)  manifested  in \nimages  associated  with  the movement  of divine  forces,  are possessors  of supernatural  powers.  \nThey  even  have  the ability  to change  their  appearance  at the same  time. In  some  mythological  \nsources,  there  are two types  of fairy  images,  one of which  is a symbol  of patronage  for good,  \nand the other  is a loyal  woman,  a friend,  counselor,  who  is not inferior  to men  in intelligence,  \ncourage,  and prestige.  He was able  to go to war with  him and become  a hero.  In some  \nmythological  works,  fairies  are represented  in the form  of mermaids,  and the image  of a \nmermaid  is unique  in its beauty,  its human  form,  its magical  power,  and its connection  with  \ngiants.  We can see that he was the leader  of the mermaids  or one of the characters  with  strong  \nmagical  properties.  \nIn short, the myths encourage the struggle for goodness and victory as a result through  \nthe images  of fairies.  And great  ideas  are told on the scale  of fighting  evil. These  myths  have  an \nancient history and are based on an unconscious attitude to natural and social phenomena,  \nreflecting  social  thinking,  various  rituals,  concepts,  and some  elements  of patron  cults.  Most  \nimportantly, these types of myths are based on mythological concepts, beliefs in cults, and are  \ncharacterized by the ability to describe the origin of nature, the characteristics of primitive  \nsociety.  They  are simple,  trying to  figure  out the cause  of a strange  symptom.  \nIn Turkish folklore, fairies are described as extremely beautiful, intelligent, wise,  \ngraceful, and gentle. They have magical properties and can take on different appearances when  \nthe time comes, allowing the hero to win with his fair actions, correct and clear advice. Although  \nthey  are sometimes  portrayed  in a negative  light  and sometimes  in a positive  light,  the socio - \naesthetic nature of such myths serves to reflect the aspirations of the masses in a relatively  \nbroad  and interesting  way.  \n \nREFERENCES:  \n1. “Avesto”.  Asqar  Mahkam  tarjimasi.  “San’at”.  1991.  №5.16 -20-b. \n2. Bertels  A. N. Peri.  Kultura  i iskusstva  narodov  srednoy  azii v derevnosti  i srednevekove.  M., \n1979.    «POLISH  SCIENCE  JOURNAL»   \n86  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \n3. Eshonqul  J. Folklor:  obraz  va talqin.  Qarshi,  “Nasaf”  nashriyoti,  1999.  2-b. \n4. Qayumov  O. Pari obrazining  genezisiga  doir.  O‘zbek  tili va adabiyoti.  1998.  №4. \n5. Yildiz  Naciye.  Türk  destanlarinda  kötü  huylu  devler.  Millî  Folklor,  2010.  Sayı 87.   «POLISH  SCIENCE  JOURNAL»   \n87  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \nTemirova  Maftuna  Allayor  qizi \nStudent of  Samarkand  state  institute  of foreign  language  \n(Samarkand, Uzbekistan)  \nIMPLEMENTING  AUTHENTIC  MATERIALS  IN ENGLISH  TEACHING  \nAbstract.  the constructivist  learning  method  states  that the person  develops  \ninformation  actively  and in a unique  way while  engaging  with  his or her environment, rather  \nthan just waiting for the information to be given straight to him or her. Students should interact  \nwith  genuine  activities  and resources that  mirror  real-life situations  in constructivist  learning  \nsettings.  Because  genuine  learning  environments  incorporate  real-world  difficulties  and \ncomplexities,  they  give students  with real -world  experiences.  \nKey words:  constructive  learning  environment,  authentic  material,  foreign  language,  \nrealia,  documentaries.  \n \nRealistic  resources,  when  utilized  correctly  in authentic  learning  situations,  can have  a \nwide range of applications in foreign language education, despite the fact that they are not  \nexpressly  created  to teach  a foreign  language.  As a result,  in constructivist  learning  \nenvironments, foreign language teachers should function as guides for students to interact with  \nreal content. Furthermore, while educating potential foreign language instructors, learning  \nspaces where real materials are utilized should be designed to create an example for future  \nteachers  to use  authentic resources  in their  own  classes.  \nFurthermore, children collaborate and communicate with classmates, parents, and  \nmore  sophisticated  adults,  such  as instructors,  in both  school  and non-school  situations.  As a \nresult,  students  continue  to study both  inside  and outside of  school.  \nAs a result, prospective instructors will be able to see and feel the benefits of these  \nresources,  as well as the factors  to consider  while  employing  them.  The purpose  of this study  \nis to concentrate  on the significance  and application  of genuine  materials  in foreign  language  \nteacher training programs, as well as to provide some recommendations in this regard. As a  \nresult of the literature research, it was discovered that employing genuine materials in foreign  \nlanguage  teacher  training  programs plays  an essential  part in their  future  professional  life. \nThe requirement to equip persons with current language credentials today makes it  \nimperative  to improve  the quality  of education  provided  to pupils.  To satisfy  this need,  more  \ninnovative, diverse, and effective teaching -learning practices must be implemented both in and  \nout of the classroom.  Creating  an authentic  learning  environment  and using  real materials  in \nclass  is one strategy  to improve  knowledge.  According  to Newmann  and Wehlage  (1993),  the \nmore a lesson is connected to the real world in which students live, the more authentic it  \nbecomes.  As a result,  it is critical  that real learning  settings  be arranged  in pre-service  teacher  \ntraining  and that potential  teachers  be taught  in authentic learning  contexts.  Habitats  with  a \nlink to the  outside  world.  \nConstructivist  learning  environments  and authentic  learning  \nThere  are several  perspectives  on learning  and learning  settings, each  of which  has a \nlot to contribute  to learning.  Constructivism  is one of the learning -related  methods  that has   «POLISH  SCIENCE  JOURNAL»   \n88  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \nreceived a lot of attention recently. Learning is produced by the learner in an active, meaningful,  \ncooperative, and genuine manner, according to the constructivist approach. Learners in  \nconstructivist learning settings should take responsibility for their own learning and collaborate  \nwith others. The learner is accountable for meaningfully integrating material with his or her  \nprevious  knowledge  rather  than  storing  the provided  information  in his or her memory.  \nUsing  authentic  materials  in foreign  language  \nOne of the most difficult problems that language teachers have is motivating pupils to  \nstudy, stimulating their creativity, and capturing and maintaining their attention. One method  \nfor doing this is to augment the course text with actual information. TV shows aired in English,  \nfor example, can be utilized as legitimate resources in English language training (Boran, 1999).  \nStudents  will be able  to enjoy  studying  a foreign  language  in this manner.  Authentic  materials  \nare defined as spoken or written language data generated during actual dialogue and not  \nexpressly for language training. As a result, it may be deduced that they are designed for native  \nspeakers  of that language  rather  than  language  teachers  or language  instruction.  \nThere are several legitimate materials that may be used in foreign language instruction.  \nThese materials can be classed based on their properties. Authentic materials are divided into  \nfour  groups.  \nAuthentic Listening -Viewing Materials: Songs, documentaries, radio and TV ads, or  \nbroadcasts,  quiz shows,  cartoons,  movies,  soap  operas,  sit-coms,  professionally  audio -taped  \nshort  stories  and novels, web  pages.  \nAuthentic  Visual  Materials:  Photographs,  paintings,  postcards,  pictures,  stamps,  stick - \nfigure drawings,  wordless  street  signs,  wordless  picture  books,  posters.  \nAuthentic Printed Materials: Newspaper articles, song lyrics, restaurant menus, tourist  \ninformation  brochures, leaflets, receipts.  \nRealia  (Real  Objects)  Used  in EFL-ESL Classrooms:  Masks,  dolls,  puppets, etc.  \nAs seen above, though lots of authentic materials mentioned, it must be paid attention  \nto their choice and use. For example, for listening comprehension activities authentic materials  \ncan be used in foreign language classes (Karaduman, 1990). However, the materials used in  \none environment may not work to realize another aim. To develop the students’ four language  \nskills,  auditory,  sometimes  visual  or tactile  authentic materials  can be used  in class  using  active  \nteaching techniques taking the objectives of the lesson into consideration. When used properly  \nin the learning  environment,  authentic  materials  may  have  a lot of gains  both  for teachers  and \nstudents.  \nBenefits of  using authentic materials  in teaching  \nThere are several benefits to using real content in language learning situations. For  \nstarters, they focus both the learners' and the teacher's attention on the language being taught.  \nThey have a high interest value because they are relevant to the actual world and keep students  \nupdated  about what  is going  on in the world  in which  they live.  \nConclusion  \nIn foreign language teaching a variety of authentic materials can be used. The materials  \nwhich can be obtained from various resources can attract students’ attention and increase their  \nmotivation towards learning a foreign language. As these materials connect the students to the  \nsocial  world  and enable  them  to put their  theoretical  knowledge  into practice,  they open  a way   «POLISH  SCIENCE  JOURNAL»   \n89  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \nfor positive transfer of learning. So, authentic materials can increase the quality of foreign  \nlanguage  teaching.  However,  these  materials  should  be chosen  and used  taking  the \ninstructional objectives and student characteristics into account. For example, having the  \nbeginner level language learners listen to English radio broadcast may decrease their motivation  \nas they may have difficulty in understanding it. Similarly, asking the students who are not  \ninterested  in politics  to examine  an article  on politics  written  in English  may  arouse  negative  \nfeelings in them. At this point, foreign language teachers have important roles for not using any  \nauthentic  material  randomly.  \n \nREFERENCES:  \n1. Brooks,  M.G.  and J.G. Brooks,  1999.  The courage  to be constructivist.  Educational  \nLeadership, http://www.  ascd.org,  57 (3): 6 -11. \n2. Casas,  M., 2004.  Making  pedagogical  theory  come  alive.  The Teacher  Educator,  39: 3. \n3. Demirel,  O., 2006.  Instructional  planning  and evaluation  art of teaching.  Ankara:  Pegem  a \nPublications.  \n4. Kesal,  F. and M. Aksu,  2005.  Constructivist  Learning  Environment  in ELT Methodology II  \nCourses.  J. Ed.  Hacettepe  Uni. Fac. Edu., 28: 118 -126.  \n5. Taylor,  L.M.,  D.J. Casto  and R.T. Walls,  2004.  Tools,  time  and strategies  for integrating  \ntechnologies  across  the curriculum.  J. Constructivist  Psychol.,  17 (2): 121-136.    «POLISH  SCIENCE  JOURNAL»   \n90  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \nTuropova  Oydin  Uktam  kizi \nStudent of the department Translation Theory and Practice  \nScientific  Supervisor:  Salieva  Zarrina  Ilkhomovna  \nAssociate Professor  \nSamarkand  State  Institute  of Foreign  Languages  \n(Samarkand,  Uzbekistan)  \n \nLEXICO -SEMANTIC  CHARACTERISTICS  OF ABREVIATIONS  IN MODERN  ENGLISH  \nNEWSPAPER  DISCOURSE  AND  TRANSLATION  STRATEGIES  \n \nAbstract.  The article  observes  the lexical  and semantic  characteristics  of \nabbreviations  of the modern  English -language  newspaper  discourse  based  on material  of the \nEnglish -language  electronic  publications  from  of The New  York  Times,  The Guardian,  Forbes,  \nThe Independent, The Telegraph, Sunday -Times, discusses the functioning of abbreviations.  \nNewspaper  discourse  is presented  as the sphere  of functioning  of various  abbreviations,  such  \nas contractions, clipping, blending, abbreviations. The subject is abbreviated lexical units, their  \nfunctioning in the modern press and strategies for their transfer from English into Uzbek.  \nFactual material analysis allows carrying out a quantitative analysis of the vocabulary and  \ndetermine  the most  typical  type  of abbreviations  for media  texts,  their  functions.  The analysis  \nof the mate - rial proves quantitative superiority of abbreviations that prevail in modern  \nnewspaper discourse that confirms the influ ence of eventfulness on the change in the lexical  \ncomposition of the language and is a powerful means of replenishing it. The functioning of  \nfigurative  and evaluative  and culturally -marked  abbreviations  in newspaper  discourse  \nfacilitates  and complicates  the perception  of information  at the same  time.  However,  \nabbreviated lexical units contain a wide information potential, which allows fixing the main  \nmeaning  of the text in the receiver's  memory  and affects  the perception  of information  in the \ndirection the author needs. The vocabulary included in the text acquires informative and  \nemotionally  evaluative value as  well.  \nKey words:  abbreviations  (abbreviation),  semantics,  newspaper  discourse,  \ntransmission  of values,  contractions,  blending,  truncations,  abbreviations.  \n \nThe language  functions  as a sign system;  accordingly,  most  of the new  lexical  \nformations can be cumbersome and inconvenient to use. It is the reduction of lexical neoplasms  \nthat makes it possible to give them a shorter and more complex form, while preserving the  \nmeaning of these lexical units. The whole range of abbreviations is aimed at creating short  \nforms  of words  compared  to the original  forms.  \nThe relevance of the work is due to modern trends in the use of abbreviations, the study  \nof linguistic  norms  and linguistic  means,  as well as methods  and techniques  for their  \ntranslation. Abbreviated lexical units in many respects remain a mystery in linguistic terms,  \nsince their study is carried out in terms of the structure of the word and its meaning,  \nmorphemes.  \nThe object of the study is the abbreviation, or abbreviations in the modern English - \nlanguage  press  of recent years.    «POLISH  SCIENCE  JOURNAL»   \n91  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \nThe subject of the study is abbreviated lexical units, their functioning in the modern  \npress  and strategies  for their transfer  from English  into Uzbek.  \nThe purpose of the article is to analyze semantic and structural features, as well as  \nways  to translate  abbreviated  English  lexical  units  of the  modern  press  from  English  into Uzbek.  \nThe application of an integrative approach to the classification of abbreviations and  \nthe ways  of their  translation  from  English  into Uzbek  makes  it possible  to achieve  this goal.  \nThe material of the study is lexical units (80LU) obtained by continuous sampling from  \nthe texts  of English -language  publications  of recent  years:  The New  York  Times,  The Guardian,  \nForbes,  The Independent,  The Telegraph, Sunday -Times.  \nThe rapid and continuous increase in the amount of information (in particular, the  \nincrease  in the number  of printed  publications),  on the one hand,  and the growth  of complex,  \nnon-single -word names, on the other, aroused a natural desire to shorten the text by  \nintroducing abbreviations. Reduction is considered a historical phenomenon that has gone  \nthrough a number of stages in its development, namely: arbitrary reduction in antiquity and in \nthe era of antiquity; abbreviation in the Middle Ages and Modern times; creat ion and  \nfunctioning of abbreviated nominations in the modern language situation. Abbreviation, or  \nreduction,  is a way of word  formation,  the essence  of which  was the reduction  of a word, or  \none of its parts, as a result of which a new form acquires some linguistic value in itself. An  \nabbreviation is one way of shortening phrasal terms to their initial letters. Domestic and foreign  \nresearchers consider various reasons for the creation of abbreviated LU: extralingual and  \nintralingual factors of generating abbreviations. Among intralingual factors V.V. Borisov, for  \nexample, refers to the context, the commonality of the language skills of the speakers, the  \nlanguage  habit,  the frequency  of use in speech,  and the \"stereotyping\"  of the generating  unit.  \nHowev er, most often the creation of various abbreviations is explained by the “principle of least  \neffort”, or “the law of economy of speech means” [1, p. 28]. Reduction is a way of word  \nformation,  by reducing  the word,  or one of its parts,  as a result  of which  the new  form  acquires  \nsome linguistic value in itself. So, abbreviations perform specific tasks - they greatly expand the  \nderivational possibilities of vocabulary, and this is their value. Refusal to use abbreviations  \nwould  lead  to exorbitant  growth  of texts.  \nNote the synonymy of the terms \"reduction\" (shortening, reduction) and \"abbreviation\".  \nSo, V.I. Zabotkina  distinguishes  four  types  of abbreviations:  abbreviations,  acronyms,  \ntruncations,  mergers  (blending)  [2, p. 36]. A.P. Shapovalova  among  the lingual  factors  \nhighlights the desire to pronounce abbreviations as single words, the influence of colloquial  \nand jargon  languages,  as well as the use of terms  [3, p. 36]. New  Zealand  linguist  L. Bauer  [4] \nproposes to divide abbreviations into: 1) contractions (fusion) - two words combined together  \nand pronounced  as one new  word  (usually  functional  words  such  as auxiliary  verbs,  articles  \nand prepositions)  don‟t (do +  not),  wanna (want  + to); 2) truncation - removal  of part of the \nword:  a) initial  (apheresis)  - the front  part of the word  is omitted:  (tele)phone;  b) medial  \n(syncope)  - the middle  part of the word  is omitted:  spec(ification)s;  c) final  (apocope)  - the extra  \npart of the word  is discarded:  gas (oline);  e) complex  (mixed  = mixed  type)  - two or more  types  \n(one or more types) of cutting used together to form a new word: (re)frige(rator), sci(ence) - \nfi(ction);  3) blending  - the formation  of a new  word  that has a distinctive  meaning  from  two or \nmore  other  words:  smog  (smoke  + fog),  hangry  (hungry +  angry);  4) abbreviations - the creation    «POLISH  SCIENCE  JOURNAL»   \n92  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \nof new words  based  on another  word  or words  that are reduced  only to its or  their initial  letters:  \na) acronym - an abbreviated word is pronounced as a whole new word, and not letter by letter,  \nfor example: UNESCO (UNESCO) (pronounced [ju ːˈnɛskәʊ]); b) initialism (alphabetism) - the \nabbreviated  word  is pronounced  letter  by letter:  OCR  - optical  character  recognition  \n(pronounced  [әʊ siː ˈaːr]). \nSo, modern abbreviation is a common language phenomenon that deserves serious  \ntheoretical  and practical  attention.  In total,  the study  considered  80 lexical  units  - \nabbreviations,  which  we took  as 100%.  1. Contraction  (10 LU - 12%)  (usually  functional  words  \nsuch  as auxiliary  verbs,  articles  and prepositions):  you‟d, don‟t, who‟ve, it ‟s, didn‟t, that‟s, \nhe‟s, next year ‟s, aren ‟t, we ‟re. Auxiliary verbs and the negative particle „not ‟ (not) are  \nsubject to contractions, since they are unstressed in the sentence. An analysis of the factual  \nmaterial  showed  that in newspaper  texts  they  often  resort  to contraction,  but their  variability  \nis limited. For example, common abbreviations in spoken English are a typical news text  \npractice, thus bringing the reader closer and creating an informal type of communication:  \n“Prima facie, you ‟d think this was simple dumbing down, but Howe did claim a practical reason”  \n(On at first glance, you  might  think  that it was just stupid, but  Howe  justified  it from  a practical  \npoint of view) [5]. 2. Clipping (14 LE - 17%) - the removal of part of the word, is considered more  \napplicable in colloquial speech, the studied texts are noted: 1) with the initial form (apheresis)  \nthe front  part of the word  is omitted:  “When  Mr. Pompeo  phoned  his European  counterparts  \nafter  the strike,  they  expressed  concern”  (When  Mr. Pompeo  called  his European  counterparts  \nafter the strike, they expressed concern) [6]; 2) medial (syncope) - the middle part of the word  \nis omitted:  Dr. (doctor),  Mr. (mister),  Ms. (Mistress).  Such  abbreviations  are typical  for printed  \nnews publications, they are understandable to the reader, since they are generally accepted, in  \naddition,  the frequency  of such  abbreviations  in one publication is quite  high.  3) final  (apocope)  \n- Dec. (December), Fri, Feb, Mar, Oct, Tue, Mon (Monday): “Two days later, on Dec. 31, pro - \nIranian  protesters  backed  by many  members  of the same  militia responded  by breaking  into \nthe American  Embassy  compound  in Baghdad  and setting  fires”  in the building  of the American  \nembassy  in Baghdad  and arson)  [6]; 4) complex  (mixed  = mixed  type)  - Robert  C. O'Brien  \n(Robert  Charles  O'Brien),  movie  (moving -picture): For  example:  “Some of  the Marines  made  dry \njokes about  the movie”  (Some  of the Marines  made  fun of the movie) [6].  3. Blending  (7 LU - \n9%) - the formation  of a new  word  by mixing  two or more  words.  Blending  is one of the ways  \nto form  new  lexical  units  in the language,  which  in recent  years  has been  indispensable  for \ncreating  neologisms  in political  discourse:  Brexit  (Brexit),  crunk  (crazy  and drunk  - crazy  and \ndrunk),  Grexit  (Grexit),  Mexit  (Megzit),  midterms  (midterms),  Obamanomics  (an economic  \nprogram  proposed  by US President  Obama),  podcast  (eng.  podcasting,  from  iPod  and eng.  \nbroadcasting  (the process  of creating  and distributing  sound  or video  files (podcasts).  Any news  \nsituation can give rise to another author's neologism that attracts the reader's attention and is \ncreated  by analogy:  “Prince  Harry  laughs  off question  about  Mexit  in first appearance  since  \nroyal split”) [7]. The language of the news text becomes more informal, the news becomes multi - \ngenre,  so Internet  criticism  gives  rise to neologisms -abbreviations:  “But  whoever  wrote  the FBI \nlist must  have  been  crunk  (crazy and drunk)”  (But  the one who  wrote  the FBI list must  have  \nbeen  crazy (crazy and  drunk) [8].  \nAbbreviations  (acronyms  and initialisms):  49 LE - 62%:  acronyms:  BAFTA  (The  British    «POLISH  SCIENCE  JOURNAL»   \n93  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \nAcademy of Film and Television Arts); CEO (Chief Executive Officer); C.I.A. (Central Intelligence  \nAgency);  etc. (et cetera:  Latin:  And So Forth);  IRS (the Internal  Revenue  Service);  FBI (Federal  \nBureau of Investment); LOL (lots of love); LOL (laugh out loud); NATO (North Atlantic Treaty  \nOrganization); OPEC (Organization of the Petroleum Exporting Countries); RIP (Rest In Peace);  \nRNLI (The Royal National Lifeboat Institution); START (the Strategic Arms Reduction Treaty);  \nUN (United  Nations);  US (United  States);WTI  (West  Texas  Intermediate).  Initialisms  \n(alphabetisms) - AI (artificial intelli gence); a.m. (lat. ante meridiem); AKA (Also Known As); bpd  \n(barrels per day); CV (Curriculum Vitae); ie. (id est, Lat.); ECB (European Central Bank);  \nFMTKFYTFO (for me to know, for you to find out); Fed. (Federal Reserve Board); 5G (fifty  \ngeneration);  IBM (International  Business  Machines);  INF (Intermediate  Nuclear  Forces);  GDP  \n(Gross  Domestic  Product);  GDS  (Global  Distribution  System);  MQ-9 (Message  Queue  \nComputing);  MPs  (members  of parliament);  MRI (Magnetic  resonance  imaging);  NWLC  \n(National  Women's  Law Center);  NHS (National  Health  Service);  NFL (National  Football  \nLeague); NSAW (not saying a word); ONS (Office for National Statistics); p.m. (lat. post  \nmeridiem); PPI (producer price index); R&D (Research & Development); TLGO (the list goe s on);  \nTV (television); UK (United Kingdom); XM (extended module)  - (English satellite radio);  \nYKWRGMG  (You  know  what  really  grinds  my gears)?;  X-rays (electromagnetic  radiation).  \nIt should be noted that some of the acronyms presented in the printed publications we  \nstudied  have  already  firmly  entered  our lives  and do not present  difficulties  in understanding:  \nEU, FBI, NATO,  OPEC,  UN, US, Latin  acronyms,  etc. News  publications  inform  readers  not only  \nabout world events, but also some facts related directly to the participants in Internet  \ncommunication,  so the article  “Trending  FBI gives  new  meaning  to capital  offense”  (FBI gives  \nnew meaning to crimes against capital) introduces readers to Internet chat abbreviations,  \namong  the presented  list there  is even  an explanation  of the acronym  LOL with  the meanings  \n(lots of love) and (laugh out loud) [9]. Among the initialisms there are also abbreviations known  \nto most  readers  a.m.  (lat. ante  meridiem),  CV (Curriculum  Vitae),  ie. (id est, Lat.),  p.m.  (lat. post  \nmeridiem), TV (television), UK (United Kingdom), X -rays (electromagnetic radiation). It is  \nimpossible not to notice the presence of initialisms that are not typical for printed publications,  \nbut are known  to most  participants  in chats  and online  communities:  FMTKFYTFO  (for me to \nknow, for you to find out), YKWRGMG (You know what really grinds my gears). The English  \nlanguage  is characterized  by ambiguity,  i.e. the more  significant  the semantic  range  of a word,  \nthe greater its compatibility with other lexical units, respectively, various options for conveying  \nthe meaning  of lexical  units  are possible.  \nThe use of translation  transformations  makes  it possible  to transfer  without  deviation  \nall the information  contained  in the original  text,  taking  into account  the norms  of the target  \nlanguage. The main types of transformations in the translation of abbreviations and other  \nabbreviations include such translation techniques as: equivalent translation, transliteration,  \ntranscription, tracing, explication (descriptive translation), borrowing a foreign abbreviation  \nwhile maintaining the Latin spelling, creating a  new Uzbek abbreviation. It should be noted that  \n“any  translation  act always  takes  place  in a certain  discursive  context:  the translation  process  \ncannot  be imagined  as taking  place  in isolation  from contextual  extralinguistic  factors”  [10].  An \nequivalent  translation  of 15 LU (20%)  is a translation  option  when  the meaning  of an English  \nword  fully  corresponds  to the meaning  of one Uzbek  word:  bpd (barrels  per day - barrels  per   «POLISH  SCIENCE  JOURNAL»   \n94  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \nday),  CV (Curriculum  Vitae  - summary),  Dec.  (December),  Dr. (doctor),  ie. (id est.- that is), etc. \n(et cetera:  Latin:  And So Forth  - and others,  and the  like, and so  on), Fri (Friday  - Friday),  Feb \n(February - Feb- ral), Mar. (March), Oct. (October), phone (telephone), plane (airplane), Tue  \n(Tuesday  - Tuesday),  TV (television),  X-rays (electromagnetic  radiation  - X-ray).  Transliteration  \n(4LE –5%): IBM (International Business Machines - an American company, one of the world's  \nlargest manufacturers and suppliers of hardware and software), OPEC (Organization of the  \nPetroleum Exporting Countries - OPEC), NATO (North Atlantic Treaty Or gani - zation - NATO  \nNorth Atlantic Alliance), NFL (National Football League - NFL National Football League).  \nTranscription (transmission of the phonetic form of abbreviation in Uzbek letters) - (8 LE - 10%):  \nBrexit:  from  Britain  (Britain)  + Exit (exit)  - (Brexit),  Mr [‟mɪstәr]  - Mr. (mister),  Ms [mɪs] (miz),  \nGrexit (Greece)+ (Exit) (Grexit), Mexit (Megzit), podcast (English podcasting, from iPod and  \nEnglish  broadcasting  - podcast).  \nThe correct application of translation transformations and analysis of translation  \nmethods helps to achieve the most adequate translation. An analysis of the vocabulary and  \nways  of transferring  the meanings  of abbreviations  from  English  into Uzbek  showed  that the \nmost common way to convey the meaning of abbreviations is tracing, then the method of  \nequivalent translation is a common method that is used when translating abbreviations in  \nnewspaper texts. So, tracing can be considered worthy a way of comprehend ing an abbreviated  \nlexical unit, in which all the meanings embedded in it are preserved. Thus, we can conclude that  \nabbreviations  are a functional  component  of a newspaper  news  text,  help  to bring  the reader  \nup to date,  and also participate  in saving  language  resources,  at the same  time, they  require  \nadditional encyclopedic, background  knowledge.  \n \nREFERENCES:  \n1. Borisov  V.V. Abbreviation  and acronymy.  Military  and scientific  technical  abbreviations  \nin foreign languages. Mos - cow: Military. publishing house of the USSR Ministry of  \nDefense,  1972. 320 p.  \n2. ZabotkinaV.I.  New  vocabulary  of modern English.  Moscow,  1989.126  p. \n3. Shapovalova A.P. Experience of constructing a general theory of abbreviation: on the  \nmaterial  of French  abbreviated  lexical  units:  thesis  … Dr. of Philology.  Rostov,  2004.  421 \np. \n4. Bauer,  L. English  Word -Formation.  Cambridge  University  Press,  1983.  311 p. \n5. Benedictus  Leo.  Goodbye,  etc: why the UK government  will stop  using  Latin  \nabbreviations  online.  URL:  \nhttps:// www.theguardian.com/science/shortcuts/2016/jul/25/  (date  of application:  \n10.01.2020)  \n6. Baker  Peter,  Ronen  Bergman.  Seven  Days  in January:  How  Trump  Pushed  U.S. and Iran to \nthe Brink  of War.  URL:  https:// www.nytimes.com  (date  of application:  10.10.2020)  \n7. Desk Web. Prince Harry laughs  off question about Mexit in first appearance since  royal  \nsplit.  URL:  https:// www.thenews.com.pk/latest/599495 -prince -harry - laughs -off- \nquestion -about -mexit -in-first-appearance -since - royal -split  (date  of application:  \n28.09.2020)  \n8. Trending  FBI gives  new meaning  to capital  offence.  URL:    «POLISH  SCIENCE  JOURNAL»   \n95  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \nhttps:// www.timeslive.co.za/sunday -times/lifestyle/2014 - 06-22-trending -fbi-gives -new- \nmeaning -to-capital -offence/  (date  of application:  25.01.2020)  \n9. The Daily  Telegraph.  Trending  FBI gives  new  meaning  to capital  offence  // Sunday -Times,  \n22 June  2014  [Electronic  resource].  URL:  https:// www.timeslive.co.za/sunday - \ntimes/lifestyle/2014 -06-22-trending -fbi-gives -new -meaning -to- capital -offence/ (дата  \nобращения:  25.01.2020)  \n10. Berseneva N.S., Gvishiani N.B. Discursive features of word -formation strategies in the  \ntranslation  of terminological  units  // The Bulletin  of MSU.  Ser. 9. Philology.  2018.  Vol. 5. \nP. 65-74.   «POLISH  SCIENCE  JOURNAL»   \n96  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \nA LOOK AT MUKIMI'S WORK IN OUR LIFE  \n \nShoabdullayeva Zulfiya  \n(Tashkent, Uzbekistan)  \nKimyo international university in Tashkent   \nM.Sc. Special Pedagogy, Defectology (Logopedics) faculty, 1 st course  \n                                                               zulfiyashoabdullayevaa@gmail.com  ,   \nTel:+998996005851              \n                                                    Senior teacher of TSUE: Eshmatova Yulduz (Phd)  \n \nAnnotation:  This popular -scientific article includes an artistic analysis of the literary and \nartistic heritage of Muhammad Aminhaja Muqimi, information about the creator, a review of ghazals \nin “Devon”. Musings were made on the nature of the characters depicted in hi s work and the \nhistorical -social psyche of the period. The literary and artistic value of Muqimi's Works has been \ndescribed.  \nKey words:  Aminhaja Muqimi , ghazals, satire, folk language, “travelogue” “Ol Khabar”.  \nАннотация : Данная научно -популярная статья включает художественный анализ \nлитературно -художественного наследия Мухаммеда Аминхаджи Мукими, информацию о \nсоздателе, рецензию на газели в “Девоне”. Были высказаны размышления о характере \nперсонажей, изображенных в творчестве Мукими, и историко -социальной психике того \nпериода. Была описана литературная и художественная ценность произведений  мукими.  \nКлючевые слова: Аминхаджа Мукими, газели, сатира, народный язык, “травелог” “Ол \nХабар”.  \nIntroduction  \nIn classical texts belonging to the ancient Pen, which we have always encouraged to learn, \nannotated words are also found. We tried to create them with the help of dictionaries a tabdil, which \nis used today. The subject to be studied is significant and rel evant in that, to date, ghazals and poems \nbelonging to the Muqimi Pen have been extremely poorly analyzed and studied. These gazelles have \ntheir own echolocation, like an unopened guard. The original ideas and goals hidden in them, the \ndisclosure of its in ternal content, are the main goals of literary studies.  \nInterest in the life of muqimi, the study of his creativity has been going on for many years. In \nUzbek literary studies, Muqimi's oeuvre was composed by Fitrat, Vadud Mahmud, P.The Guardian, \nA.Kayumov, M.Kadyrova, A.Madaminov, H.It was studied by Boltaboye v, on the basis of which \nresearch works were published.  \nSatire is a kind of artistic reflection of reality, in which absurd, unjustified, erroneous phenomena, \nvices in society are exposed. An object is shown by changing, exaggerating, overstating the realistic \nappearance of events.    «POLISH  SCIENCE  JOURNAL»   \n97  \n SCIENCECENTRUM .PL ISSUE  2(47)  ISBN  978-83-949403 -4-8 \n \nSatirical styles in Muqimi's lyrics  \nMuqimi wrote mainly satires. In their satire, tsarist officials and indentured servants of some \nlocal rich were revealed. In the” confessions “,” wedding “,” Mosker boy's definition “,” Hajvi Victor \nboy “,” election “,” Dar mazammati zamona\" and the like, injustice, the pain of the common people, \ncapitalist and inhuman relations and their consequences are covered. Below is an analysis of an \nexcerpt from the poet's satire “The bodyguards”, which  is purposeful.  \nThe satire \"bodyguards\" is written from the peasant language. In it, the deeds of officials \nsuch as Sultan Ali and Hakimjan and the fact that they trampled on the labor of a simple quarter \npeasant are revealed by Muqimi in very strong words.  \nWas surprised curious hangams,  \nLet's write down the names,  \nJust listen in my condition,  \nHe wronged me, and I was wronged.  \nIn twelve months, coming is a body,  \nEnjoy others and torment me.  \nSultan Ali Khoja, Hakimjan both of them,  \nOne became wife and one became kuyav.  \nThe two became the couple alliance,  \nAs if a dream woman, (not discord),  \nThey worried about everyting,when they it a palov in the middle,  \nKhoja -chiragyoggi, Hakimjan -pilik…  \nMuqimi wrote a great deal of such satire. Although through Hajviya, the wrongs that were \ntaking place in relation to his people, expressed their opinion on the injustices. The poet wrote his \nworks in ordinary folk language. He created more and more works w ith very sharp verses towards the \nofficials of the capitalist system. Who wanted Happy Days to come for their people. It was for this \nreason that many evils were suffered by officials. Muqimi devoted his life to writing lyric sh'er \nalongside the hajvias. M any of his poems have become songs. Most of the ghazals in the Muqimi \nDevan are romantically themed and are sung to the praise of the divine love. It is also known that \nMuqimi brought a unique way into Uzbek literature. The technique is \"travelogue\", and i ts travelogue \nis highly distinctive. In particular, the characteristic thirst in it, the analogies, the comparison -it's the \nopposites – are all created in an incredibly beautiful artistic style. We will get acquainted with the \nradifli Ghazali “Ol Khabar”, which is presented in the muqimi Devan:Aminhaja Muhammad Muqimi \ndied in the Khanate of Kokand on 25 May 1903.    «POLISH  SCIENCE  JOURNAL»   \n98  \n SCIENCECENTRUM .PL ISSUE  2(47)  ISBN  978-83-949403 -4-8 \n \nAs a poet gives information about each region, he gives information about the nature of the \nplace, the inhabitants, the religious -mythological views of the tradition, social classes, the attitude of \nthe individual to the individual, the harshness of the li ving conditions. The cool, pleasant nature of \nsome places, while praising their soulless flesh, does not hide the displeasure of some places. For \nexample:  \nI come to this space to do it, I feel sorry,  \nA man who does not have a worthy tab, he is sad.  \nThere is no one to talk to until tomorrow evening too,  \nThe day is cold, a black lamp, a roaring house -it is low in wood.  \nThe above verses are also from Muqimi's “travelogue”, in which the poet expressed his \ndispleasure. Lamenting that he had come to this place, that there was no worth talking about, that \nthe cold of the day was not a light, that his sitting room was as dark as it was.  \nAs you analyze the ghazals “Ol message”and “Dagmen“, which are presented in the “Devani” \nof muqimi, the playful tone in them, the ghazal tasbehs, the oh -u groans of the love's path to the \nanniversary of hearing only the news of the “visol” the infatuation of this longing from their \nexuberance will not leave you indifferent. These two ghazals have such a magical power that you can \neasily memorize it in two or three readings. The use of rhyming words in Muqimiy ghazals is so \nbeautiful that it is as if it is i mpossible not to be surprised to see that the melodic words coming by \nbyte by Byte are complementary not only in form, but also in meaning. While the poet is overcome \nwith grief in one of his ghazals, one day he says that he comforts himself with the dream  of reaching \nthe anniversary. Muqimi's” Devon \" is as if it were a romantic diary. It is as if the daily images of the \nheart of a lover are finished in it. Each Ghazal is full of mixed expressions of grief and joy that do not \nhurt each other. It is impossi ble not to say super to his talent for creating such uniqueness. We will \nnot be mistaken, even if we say that another reason why Muqimiy ghazals are so elegant and charming \nto themselves is that it is about divine work. Because Allah Wasfi, the product of any creation in which \nhis interpretation is sung, certainly does not cease to be beautiful.  \nConclusion  \nThe life and creative heritage of Muqimi, son of Muhammad Aminhaja Mirzahuja, has not \nbeen studied impartially and fully in the context of communist ideology. There is a need for a \ncomparative -textual study of manuscripts of muqimi poetry and a Re -study of the works of the poet \nNazmi on the basis of today's achievements in textual studies and literary resource. Muqimi's large -\nscale  lyrical and comic works have been variously interpreted. It can also be noted that these \ninterpretations are often far from the essence of the text, based on certain narrow molds. Accordingly, \nthere is a great need to study the issue of textual interpreta tion on the example of the works of the \npoet Nazmi.  \n   «POLISH  SCIENCE  JOURNAL»   \n99  \n SCIENCECENTRUM .PL ISSUE  2(47)  ISBN  978-83-949403 -4-8 \n \nReferences:  \n1. Isakhan ibrat Foundation, which is held at the Institute of Oriental Studies named after FA \nAbu Rayhan Beruniy. R -11620 / I -no.  \n2. The central state archive of Uzbekistan is kept N.Ostroumov fond. Folder number 185.  \n3. Muqimi poems. // Gazetteer of Turkestan province. 1891, 1903, 1907.  \n4. Ostroumov N. Pesnya is a satire Victorboy. Collection \"Zapiski Vostochnogo otdeleniya \nimperatorskogo russkogo archaeologicheskogo obtshestva\". 1894, Vol IX.   \n5. Ўзбекистон ФА Абу Райҳон Беруний номидаги Шарқшунослик институтида \nсақланаётган Исҳоқхон Ибрат фонди. Р -11620/I -сон.  \n6. Остроумов Н. Песня -сатира Викторбой. “Записки Восточного отделения \nимператорского Русского Археологического общества” тўплам. 1894 йил, IХ том.    «POLISH  SCIENCE  JOURNAL»   \n100  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \nSECTION:  PHILOSOPHY  \n \n \nAtavullayev MirkomilAhmadovich  \nMirzo  Ulug’bek  nomidagi  O’zbekiston  Milliy  \nunversiteti  Ijtimoiy fanlar  fakulteti  \nFuqarolik  jamiyati  va huquq  ta’limi kafedrasi  \ndotsent  v.b, falsafa  fanlari  bo’yicha  falsafa  doktori  (PhD),  \nRo’ziyeva Go’zal Ro’zimurotovna  \nO’zbekiston Milliy Unversiteti magistranti,  \nO’ktamova  Dilfuza O’ktamovna  \nO’zbekiston Milliy Unversiteti magistranti,  \nKarimberdiyeva  Nodira  Ilhom  qizi \nO’zbekiston  Milliy  Unversiteti  magistranti  \n(Tashkent, Uzbekistan)  \nIJTIMOIY  HAYOT  SOHALARINING  G‘OYAVIY -MAFKURAVIY  JIHATLARI  \nAnnotatsiya.  Globallashuv  jarayonlarida  xalqaro  darajadagi  tartibga  soluvchi  \nSog’liqni Saqlash tashkilotlari va ularning birgalikdagi harakati, global sog’liqni saqlash  \nstrategiyalarini  amalga oshirishdagi  o’rni  ochib  berilgan.  \nKalit  so’zlar:  davlat,  siyosat,  sport,  sotsiologiya,  TMK,  farmasevtika,  olimpia,  \nmadaniyat,  ma’naviyat,  mafkura,  menejment,  sanoat,  gumanizm  \n \nGloballashuv sog‘liqni saqlash sohasini xalqaro darajada tartibga soluvchi xalqaro  \ninstitutlarning  (BMT,  JSST,  OIV /  OITS  va silga  qarshi  kurash  fondi,  bezgak  va ularning  \nbo‘linmalari) o‘sib borishiga yordam beradi. Bu xalqaro tashkilotlar birgalikda harakat qiladi,  \nbir-birlarining pozitsiyalarini hisobga oladi, shu bilan butun dunyo bo‘ylab sog‘liqni saqlash  \ntarmog‘ini  va tegishli  global  siyosatni  shakllantiradi,  xususan,  ushbu  muassasalar  harakatlarni  \nmuvofiqlashtirish, Global Sog‘liqni saqlash strategiyalarini amalga oshirish uchun mablag‘  \nyig‘ish  uchun  mo‘ljallangan.  Globallashuvning  sog‘liqni  saqlash  tizimlariga  ta’siri  aholining  umr \nko‘rish davomiyligi va sog‘liqni saqlash harajatlarining oshishi bilan bog‘liq bo‘lgan sog‘liqni  \nsaqlash  tizimlariningunifikatsiyalashuvida  namoyon  bo‘ladi.  Dunyoning  aksariyat  \nmamlakatlarida sog ‘liqni saqlashni xususiy va davlat tomonidan moliyalashtirishda tibbiy  \nsug‘urta  tamoyillari  ustunlik qiladi.  \nJahon sog‘liqni saqlash tashkiloti mutaxassislarining fikriga ko‘ra, majburiy tibbiy  \nsug‘urta (MTS) va ixtiyoriy tibbiy sug‘urtaning (ITS) kombinatsiyasi sog‘liqni saqlashni tashkil  \netish  va moliyalashtirishning  eng muvaffaqiyatli  yechimi  bo‘lgan  sog‘liqni  saqlashning  ijtimoiy  \nsug‘urta  tizimidan  foydalanayotgan  mamlakatlarning  soni tobora  ko‘payib  bormoqda.  \nGloballashuv intellektual mulk huquqlari sohasidagi qonunchilik va sud amaliyoti  \ntufayli  transmilliy  korporatsiyalar  (TMK)  tomonidan  global  farmatsevtika  bozorining  \nmonopoliyalashuvini  keltirib  chiqardi.  Sanoatning  o‘sishi  va farmatsevtika  gigantlari  tomonidan    «POLISH  SCIENCE  JOURNAL»   \n101  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \nkatta daromad olinishi innovatsiyalarga va yangi dori -darmonlarni ishlab chiqarishga ulkan  \nsarmoyalar  kiritish,  axborot  assimetriyasiga  asoslangan  agressiv  xalqaro  marketing,  tovarlarni  \nilgari  surishda  aloqalarni  tezlashtirish  uchun  Internet -resurslardan,  shu jumladan  \nshifokorlardan  foydalanish  tufayli  mumkin  bo‘ldi.  \nGloballashuv barqaror iqtisodiy o‘sishni va umuman ijtimoiy rivojlanishni saqlab qolish  \nuchun keng imkoniyatlar yaratmoqda. Davlatning asosiy vazifasi - globallashuv yaratayotgan  \nimkoniyatlar va aholi uchun yuzaga kelayotgan xatarlar o‘rtasidagi muvozanatga erishishdadir.  \nMilliy sog‘liqni saqlash tizimiga salbiy omillarning ta’sir darajasi davlatning samarali ichki  \nsiyosati  bilan  o‘zaro bog‘liq  bo‘lishi  kerak.  \nSport  ko‘pgina  fanlarning  o‘rganish  ob’ekti  hisoblanadi.  Bularga,  pedagogika,  \npsixologiya, tarix, jismoniy madaniyat nazariyasi hamda madaniyatshunoslik fanlari kiradi.  \nUshbu  fanlarning  har biri o‘zining  spetsifik  xususiyatlaridan  kelib  chiqib  e’tibor  qaratadi:  \n• Pedagogika – jismoniy  madaniyatning  metodlarini  o‘rganadi.  \n• Tarix  - jismoniy  madaniyat  va sportning  tarixiy  bosqichlarini  vujudga  kelishini  \no‘rganadi.  \n• Sotsiologiya  – jismoniy  madaniyat  va sportning  sotsial  funksiyalarini  tadqiq  qiladi.  \nSotsiologiya  fani shuning  bilan  birga  boshqa  fanlardan  farqli  o‘laroq  jismoniy  \nmadaniyat  va sportni  chuqur  va mukammal  o‘rganadi  va boshqa fanlar  elementlarini  o‘z ichiga  \noladi.  \nMadaniyat,  avvalo,  har qanday  shaklning  hamda  insonning  yaratilishini  chizib  beradi.  \nShuningdek, binoning qanday qad ko‘tarilishi, suratlarning chizilishi, matnlarning vujudga  \nkelishi ham o‘ziga xos ko‘rinishga ega hisoblanadi. Inson ham jismoniy madaniyat va sport bilan  \no‘zining tanini vujudga keltiradi. Bunday holat, albatta, sportning ma’lum bir turi orqali amalga,  \noshiriladi.  Keyin  shu holat yordamida  turli ko‘rinishdagi  harakatlar kelib  chiqadi.  \nSport  madaniyati  - umumiy  madaniyatning  tarkibiy  qismi  hisoblanib,  insonning  \njismoniy va ma’naviy shakillanishidagi kategoriyalar umumlashuvi, qonuniyatlari, bellashuvli  \nfaoliyat  doirasini  anglatadi,  ko‘pgina  madaniy  tadqiqotchilar  sportni  insonni  o‘ziga  tez \ntortadigan tomonlari borligini tushuntiradilar. Ammo, bunday hissiyot nimadaligini bilmaydilar.  \nChunki, sport baribir insonlarga qiziqish va xursandchilik beraveradi. Tadqiqotchi Xans Ullix  \nGumbert  fikricha,  sportdan  keladigan  quvonch,  hayajon  insonga  salbiy  ta’sir  qilmaydi,  ammo  \nsportdagi muvaffaqiyat insonlar sog‘ligiga ham muvaffaqiyatlariga ham sherik bo‘lolmaydi.  \nKo‘pgina faylasuflar bu holatni uning avtonomligini, alohidaligini san’at bilan qiyoslaydilar.  \nXans Ulrix Gumbert ham sportning san’atga yaqinligini ko‘rsatib o‘tgan. Shu o‘rinda aytish joizki,  \nbu narsa sport tomoshabinlari faoliyati bilan bog‘liq, ya’ni sportda sporsimenlar, trenerlar,  \nmenedjerlar qatnashadilar, ular sportni ham tashqi ham ichki tomonidan ko‘rib his qiladilar.  \nSportning  san’at  bilan  o‘xshashligi  shunisi  bilan  aniqki,  xuddi  arxitektor,  dizayner  va \nsuvratchilar o‘z asarlari bilan insonlarni mamnun qilgandek, sport ham o‘zining yo‘nalishlari  \nbilan  insonlarga  quvonch, havas  uyg‘otadi.  \nShuningdek, yana bir muhim mavzu doimo tadqiqotchilar diqqatini tortib kelgan, bu  \nsportdagi qadriyat masalasi. Darhaqiqat, sport faoliyatida sportchilar qadriyatning qaysi turi  \nbilan  ko‘proq  qiziqadilar,  adolat  bilanmi?  Yoki  qanday  bo‘lmasin  g‘alabani  qo‘lga  kiritish  yoki \nzo‘rlik  va go‘zallik ko‘rinishlari  bilanmi?    «POLISH  SCIENCE  JOURNAL»   \n102  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \nMasalan,  zo‘rlikni  sportning  ko‘pgina  turlarida  ko‘rish  mumkin.  Bu holat  sport  turidagi  \ndistansiya masofa bilan xarakterlanadi. Misol uchun golf o‘yinini oladigan bo‘lsak, bunda oraliq  \nmasofa ancha katta. Shuningdek, sportning futbol, regbi, xokkey turlarida esa bu holat o‘ta  \nyuqoriligi bilan xarakterlanadi. Ammo, sportning kurash turida esa barcha rekordlar zo‘rlik  \ndarajasi bilan baholanadi. Sportning yana mavzularidan biri bu adolatli o‘yin mavzusidir.  \nAdolatli  o‘yin  sportdagi  sotsial  norma  va qadriyat  sifatida  jamoatchilikka  tarqalgan  \no‘yinlardandir.  Biz bilamizki,  agar  sportda  natija  asosiy  maqsad  bo‘lib  qolganda,  nohaq  g‘alaba  \nqilish  holati  kuchayadi.  \nOlimpiya  o‘yinlarining  kelib  chiqishi  insoniyatning  uzoq  tarixiga  borib  taqaladi.  Stixiyali  \ntarzda bo‘lsa -da, ibtidoiy jamiyat odamlari qator hayvonlarni ovlashda zarur bo‘lgan xatti - \nharakatlarni  bajarishni  mashq  qilishgan,  bu faoliyat  jarayonini  an’ana  sifatida  keyingi  avlodga  \no‘tkazishgan. Qadimgi Misr, Mesopatamiya, Hindiston, Xitoy tarixiga e’tibor beradigan bo‘lsak,  \ndavlatchilik  an’analari,  uning  himoyasi  bilan  bog‘liq  bo‘lgan  harbiy  ish o‘ziga  xos chaqqonlikni,  \nkuchlilikni, epchillikni, botirlikni  talab etgan. Bunga o‘xshash bir  qator omillarni sportni  \ninsoniyat faoliyatining maxsus bir shakli sifatida namoyon bo‘lishini ta’minladi. Qadimgi Yunon  \ndengiz  bo‘yidagi  yarimorol  bo‘lgani  bois,  uni himoya  qilish  asosan  dengizda  amalga  oshirilgan.  \nShunday  ekan harbiy  madaniyat, harbiy  san’at, epchillik, botirlik, dushman havfi paydo  \nbo‘lganida  tezkorlik  bilan  portdagi  marralarni  egallash  ular uchun  muhim,  strategik  vazifa  edi. \nBa’zi manbalarda sport – portga yugurish ma’nolarida ham kelgan. Shu sababli qadimgi  \nYunonda sportning bir qator turlari keng rivojlangan bo‘lib ular amalda harbiy soha bilan bog‘liq  \nedi: yugurish, kurash, disk uloqtirish, nayza uloqtirish. Miloddan avalgi 776 - yildan boshlab  \nPeloponnesning  Olimp  shahrida  har to‘rt  yilda  Olimpiada  o‘yinlari  o‘tkazilib  kelgan.  Olimpiya  \no‘yinlarining  Olimp  xudolari  sharafiga  o‘tkazilishi  dastlabki  xristianlik  aqidalariga  to‘g‘ri  kelmay  \nqoldi.  Va olimpiya  o‘yinlari  milodiy  396 -yilga  kelib  o‘tkazilmay  qoldi.  \nXIX asr insoniyat  qo‘l mehnatiga  asoslangan  ishlab  chiqarishdan  sanoatlashgan  ishlab  \nchiqarishga o‘tish asri bo‘lib qoldi. Chetga kapital chiqarish, tovar chiqarish keskin oshdi. Jahon  \nmamlakatlari o‘rtasida jamiyat hayotining turli sohalarida aloqalar kuchaydi. Ayni damda bu  \ndavr  jahonning  yetakchi  mamlakatlari  o‘rtasida  dunyoni  tenglikka  asoslanmagan  holda  bo‘lib  \nolinishining yakuniga yetayotgan bir payt edi. Siyosiy -harbiy sohalardagi qarama -qarshiliklarga  \nto‘lib  toshgan  dunyo  turli sohalardagi  hamkorlik larning  yanada  kuchayishiga  to‘sqinlik  qilardi.  \nJahon mamlakatlari o‘rtasidagi tinchlikka, adolatga, hamkorlikka, sog‘lom muloqot madaniyatga  \nasoslangan  vaziyatni  yaratish  tinchlikparvar  kuchlar  tomonidan,  gumanistlar  tomonidan  \nadabiyot sohasida, tasviriy san’at sohasida, musiqa sohasida keng ilgari surila boshladi.  \nAlbatta, bunday ijobiy mazmundagi xatti -harakatdan sport ham chekkada qolmadi. 1894 -yil \n23- iyunda Parijda Baron P’er de Kuberten tashabbusi bilan Xalqaro Olimpiada qo‘mitasi tashkil  \netildi. Bu o‘yinlar jahonning etakchi antogonistik davlatlarini jang maydonid a emas, sport  \narenalarida kurashishga chaqirishi zarur; bu o‘yinlar turli millatlarni irqidan qat’iy nazar  \ndo‘stlikka, inoqlikka chorlashi zarur edi; bu o‘yinlar turli din vakillarini, diniy konfessiyalarni  \ne’tiqoddan  qat’iy  nazar  bag‘rikenglik  asosida  ahillikka  etaklashi  zarur  edi. Yunon  shoiri  \nDemetrius Vikelas Afina olimpiya o‘yinlari tiklanar ekan, an’ananing hurmati bois, vorisiylik  \nnuqtayi nazaridan birinchi olimpiada o‘yinlarini o‘tkazish joyi sifatida Afina shahrini taklif etadi.  \nDastlabki  olimpiya  o‘yinlarida  13 davlatdan  300 dan ortiq  sportchi  ishtirok  etdi.  O‘tgan  davr    «POLISH  SCIENCE  JOURNAL»   \n103  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \nmobaynida  globalashuv  tufayli  bir qator  omillar  sababli  sportning  ommamiylashuvini  ko‘rish  \nmumkin[1].  \nYuqorida aytib o‘tilgan qator fikrlarni umumlshatirgan holda sportning globallashuvida  \nasos  bo‘lib  xizmat  qiladigan  omillarni  tahlil  etadigan  bo‘lsak,  ular sirasiga  quyidagilarni  kiritish  \nmumkin:  \nBirinchidan,  sport  insonni  jismonan  yetuklikka,  epchillikka  undaydi.  Insonning  ijtimoiy  \nhayotdagi  barcha xatti -harakatlarining  negizida  uning  jismoniy holati  yotadi.  \nIkkinchidan,  sport  insonni  faqatgina  jismoniy  yetuklikka  emas,  balki  ma’naviy  yetuklik  \nsari ham yetaklaydi. Lev Tolstoyning mashhur so‘zi bilan aytganda: “Ma’nan sog‘lom bo‘lish  \nuchun jismonan bardam bo‘lmoq kerak”. Sport nafaqat jismoniy, balki ma’naviy kamolotga  \neytishishda ham muhim omildir. U irodani toblaydi, aniq maqsad sari intilish, qiyinchiliklarni  \nbardosh va chidam bilan yengishga o‘rgatadi. Inson qalbida g‘alabaga ishonch, g‘urur va iftixor  \ntuyg‘ularini  tarbiyalaydi.  \nUchinchidan, sportning globallashuvida bir qator institutsional tashkilotlar muhim  \nahamiyat kasb etadi - XOQ,  FIFA, OOQ, OFK, FIDE, UEFA va  shu kabilar. Ushbu xalqaro  \ntashkilotlar o‘z faoliyatini samarali yo‘lga qo‘yish maqsadida qator tadbirlar rejasini ishlab  \nchiqishgan va shu asosda o‘z faoliyatini yuritishmoqda: turli musobaqalar uyushtirish, sport  \ninshootlari  barpo  etish,  sohaga  tegishli  qonun -qoidalar  ishlab  chiqish,  soha  kadrlarni  \ntayyorlash  tizimini  yaratish, reklama va  shu kabilar.  \nTo‘rtinchidan,  iqtisodiy  manfaaatdorlik.  Yuqorida  sanab  o‘tilgan  xususiyatlaridan  \ntashqari  sport  bugungi  kunda  katta  daromad  manbayi ham  bo‘lib  xizmat  qiladi.  \nBeshinchidan,  shuningdek,  sportning  globallashuvida  ommaviy  axborot  vositalarining  \nchiqishlari  muhim ahamiyat kasb  etadi.  \nSportning globallashuvi sabab XXI asrda sportning eng dolzarb va muhim falsafiy - \nijtimoiy  muammolaridan  biri bu insonparvarlik  muammosidir.  Ushbu  muammo  asosan  \nsportning  bellashuvli  faoliyati  bilan  chambarchas  bog‘liqdir.  Ma’lumki,  ko‘pgina  ta’limotlarda  \ne’tirof etilishicha har qanday qarama -qarshi bellashuvlar (antiguman) insonparvarlikka zid  \nfaoliyatlar  hisoblanadi.  \n1706  -yilda  Bernarden  Sen-P’er o‘zining  “Tarbiya  to‘g‘risida  fikrlar”deb  nomlagan  asarida,  \n“har qanday raqobatli faoliyat yaxshilikdan xabar bermaydi” deb yozadi. Shuningdek, buyuk  \nrus yozuvchisi L.N.Tolstoy ham bunga salbiy munosabat bildirgan. U shunday degan edi: “Shuni  \nhech unutmangki, qarama -qarshilikdan yaxshi natija chiqmaydi, huddiki manmanlikdan oliy  \nhimmatlikka erishib bo‘lmaganidek”. Xuddi shunday fikrni jismoniy tarbiya ilmiy usulining  \nasoschilaridan bo‘lgan Rossiyalik bir qator olimlar P.F.Lesgaft P .De Kubertenlar ham bolalar  \nolimpiya  bellashuvlarini  xavfli  va noamaliy  faoliyat deb  baho beradilar.  \nSport turlari cheksiz. Ularni barchasini ham insonni jismoniy va ma’naviy etuklikka  \nundaydi  deb bo‘lmaydi.  “Qoidasiz jang”  singari  sport  turlari  (agar  ularni  sport  deyish  mumkin  \nbo‘lsa)  yoshlarda  qanday  axloqiy  fazilatlarni  shakllantirishi  mumkin.  \nSport globallashuvining salbiy tomonlaridan yana biri katta sport musobaqalarini  \ntashkil  etish, o‘tkazish  daromad  ketidan  quvish  bo‘lib  qolmasligi  lozim.  \nSportchilarning  chetga  chiqishi.  Sporti  yuksak  rivojlangan,  iqtisodiyoti  ustun  \nmamlakatlar yosh sportchilarni o‘ziga chaqirib olish holatlari kuchaymoqda. Bu yosh sportchilar  \no‘zga  mamlakatning  bayrog‘i  ostida  maydonga  chiqishmoqda.  Braziliyada  o‘tkazilgan  JCH ning    «POLISH  SCIENCE  JOURNAL»   \n104  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \ng‘olibi Germaniya terma jamoasining olti nafar a’zosi ajnabiy sportchilar edi. Sochi -2014  \nolimpiya o‘yinlarida Rossiya qo‘lga kiritgan medallarning 14% i chet eldan kelgan sportchilar  \nhissasiga to‘g‘ri kelgan. Rossiya hukumati mamlakat jamoalarida xorijlik sportchilar soni  \numumiy sportchilar sonining chorak qismidan oshmasligini belgilab beruvchi hujjat qabul  \nqildi  [2]. \nJahonning  yetakchi  tashkilotlari,  yetakchi  mamlakatlari  turli sohadagi  \nkelishmovchiliklarini hal etishda sportni vosita qilish, sportning xalqaro maydondagi mavqeidan  \nfoydalanishga intilishga urinishmoqda. Albatta, bu ijobiy holat. Doimiy raqobatda bo‘lgan  \ndavlatlar sport maydonlaridagi uchrashuvlar sabab do‘st davlatga aylansa, turli xil irq, din,  \nmazhab vakillari ahil, inoq bo‘lib ketishsa buning nimasi yomon? Lekin sport jazo vositasi  \nsifatida foydalanish teskari natija berishi ham mumkin. Antverpenda (B el’giya) o‘tkazilishi  \nbelgilangan  VII olimpiada  o‘yinlariga  Olimpiya  qo‘mitasi  qaroriga  binoan  I jahon  urushi  \nsababchisi hisoblangan Germaniya va unga ittifoqdosh bo‘lgan mamlakatlarning olimpiada  \no‘yinlarida  ishtirok  etishi  ta’qiqlab  qo‘yildi.  Shuningdek,  Sobiq  ittifoq  vakillarining  ham  \nolimpiada  uchrashuvlaridagi  ishtiroki  man  etildi.  1952  -yil Xel’sinkidagi  (Finlyandiya)  XV \nolimpiada  o‘yinlarida  ilk marotaba  SSSR  (300  ga yaqin  sportchilar  bilan)  ishtirok  etdi.  Yuqorida  \naytib o‘tilgan 1980 - yil Moskvada (SSSR), 1984 yil Los -Anjelosda (AQSH) o‘tkazilgan olimpiya  \no‘yinlarini ham bunga misol qilib aytish mumkin. Bolqon, Kavkazdagi, Sharqiy Yevropadagi ba’zi  \nkelishmovchilik tufayli Rossiya -Ukraina, Rossiya -Gruziya, Armaniston -Ozarbayjon, Ispaniya - \nGibraltar terma jamoalarining o‘zaro guruh bo sqichlarida uchrashuvi man etilgan. Lekin negadir  \noktyabr  oyining  (2014  -yil) o‘rtalariga  belgilangan  doimiy  raqobatda  bo‘lgan  Serbiya -Albaniya  \nuchrashuviga ruxsat berilishi tushunarsiz holat. O‘yinning 42 daqiqasiga kelib maydon uzra  \nBuyuk Albaniya banneri, alban millatchilarining bayrog‘i ilingan uchuvchisiz boshqariladigan  \nvertolyotning ucha boshlashi serb futbolchilarining jiddiy noroziligiga sabab bo‘ldi. Serblar  \nkapitani Stefan Mitrovich aytishiga qaraganda u janjalga sabab bo‘lishi mumkin bo‘lgan bu  \nbayroqni shunchaki olib tashlashga harakat qilgan. Natijada katta janjal kelib chiqdi[3]. O‘yinni  \ndavom  ettirish  imkoni  bo‘lmadi.  \nIjtimoiy hayot o‘zgarishlarini xolisona baholash, uni yot va zararli g‘oyalar ta’siridan xoli  \nbo‘lishi  muhim  ahamiyatga  ega. Ijtimoiy  hayot  g‘oyalar  va mafkuralar  xilma -xilligiga  asoslanishi  \nuning  biror -bir sohasi mafkuralashtirilmasligi  zarur.  Lekin  bugungi  global dunyoda  uni ayrim  \ngeosiyosiy  va mafkuraviy  maqsadlar  yo‘lida  foydalanishga  urinishlar  mavjudligini  ta’kidlash  \nlozim,  bunda  iqtisodiy  bosim  o‘tkazish,  noxolis  sanksiyalar  qo‘llash,  ayrim  yoshlarning  ong va \ntafakkurini manipulyatsiya  qilishga  urininshlar,  “vatansizlik” g‘oyasini  targ‘ib  qilish  yoshlarni  \nvatanga  bo‘lgan  dahldorlik  hissiga  sakbiy  ta’sir  ko‘rsatadi.  Ijtimoiy  hayot  sohalaridagi  \nmuammolarning  hal etib borilishi  esa ularni  g‘oyaviy -mafkuraviy  tahdid  ko‘rinishlaridan  himoya  \nqiladi.  \n \nFOYDALANILGAN  ADABIYOTLAR  RO’YXATI:  \n1. https://ru.wikipedia.org/wiki/Letnie_Olimpiyskie_igri  \n2. http://ria.ru/sport/20120417/628733993.html#ixzz3HMUxWaFI  \n3. http://www.championat.com/football/article -207575 -match -serbija -albanija -sorvan.html    «POLISH  SCIENCE  JOURNAL»   \n105  \n SCIENCECENTRUM.PL  ISSUE  2(47)  ISBN  978-83-949403 - \n4-8 \n \nSECTION:  PHYSICAL  CULTURE  \n \nBunyod Shukurov,  \nUzbek state art and culture  \nInstitute  of Folk  Art, Faculty  \n“Establishment of cultural and art institutions  \n3 course  student  of the direction  \"Development  and management \" \n \n \nTHE OWNER  OF THE HEART  WHO  ROBBED  THE PROPERTY  OF THE SOUL  \n \nAnnotatsiya:  Ushbu  maqolada  Alisher  Navoiy  ijodining  murakkablik  xususiyatlari,  \nasarlarida talqin qilingan odamiylik va insoniylik fazilatlari xususida batafsil ma’lumotlar  \nkeltirib o‘tilgan. Shuningdek, Navoiy ijodining umumbashariyat va xalqlar oldida qanchalik  \ndarajada ma’naviy ozuqa bera olishi, ijod olamida botiniy va zohiriy tuyg'ularni to‘g‘ri  \ntarbiyalash  kabi xususiyatlarni  o‘zida  jamlagan.  \nKalit  so‘z:  Navoiy  ijodi,  zullinisonayn,odamiylik,  ijtimoiy  hayot,  botiniy  \ntuyg‘ular,foniy,  turkiy til  \nAnnotation: This article provides detailed information on the complexity of  \nAlisher  Navoi's  work,  the humane  and humane  qualities  interpreted  in his works.  It is also \nthe universality  of Navoi's  work  and the extent  to which  it can provide  spiritual  \nnourishment  to the people,  and properly  nurture  the inner  and outer  senses  in the world  \nof creation.  \nKeywords:  Navoi's  work,  Zullinisonayn,  humanity,  social  life, inner  feelings,  fan, \nTurkish  language  \nАннотация:  В данной  статье  представлена  подробная  информация  о \nсложности творчества Алишера Навои, интерпретированных в его произведениях  \nгуманных и человечных качествах. Это еще и универсальность творчества Навои и  \nто, насколько  оно может  дать  духовную  пищу  людям,  правильно  взрастить  \nвнутренние  и внешние  чувства  в мире  творения.  \nКлючевые  слова:  творчество  Навои,  Зуллинисонайн,  человечество,  \nобщественная жизнь, внутренние  переживания, веер.    «POLISH  SCIENCE  JOURNAL»   \n106  \n  \n“No one has been  able  to recite  Turkish  poetry  better than  him \nand could  not paint  the lines  of poetry  better than him.  \" \n(Hakimshah  Qazvini)  \n \nAlisher  Navoi  is one of the great  figures  in the history  of Uzbek  culture  and literature.  He \nis a man who  has dedicated his  life, and his creativity,  to the truth  and to  the people.  \nHe has a special place in the history of our culture as a lyric and epic poet, literary scholar,  \ntranslator.  \nThe works of great people have always played an important role in human education.  \nZahiriddin  Muhammad  Babur  testified,  “Alisherbek  was a unique  man.  They  recited  poems  in Turkish,  \nbut no one told them much.  He wrote  six Masnavi  books, five in response  to Hamsa,  and another  in \nLison ut -tayr in Mantiq ut -tayr. He has arranged four ghazals: \"Gharayib us -sig'ar\", \"Navodir ush - \nshabab\", \"Badoyi ul -vasat\", \"Favoyid ul -kibar\". there is also a good rhyme. I have some other  \nmusannafoti,  which  is a lower  and slower  phenomenon  for these  people.  He compiled  his essays  in \nimitation  of Mawlana  Abdurahman  Jami.  Khasiyat  Kalam,  I have  finished  and collected  every  letter  for \neveryone  for every  work.  ”[] \nNavoi's  full name  is Mir Nizamiddin  Alisher  Navoi  The word  \"Mir\"  [] is an abbreviated  form  of \nthe word amir and is the name of an action in the royal palace. The word \"regulation\" [] means  \ndirection,  rule.  Navoi  was a representative  of the great  religion  of his time,  and when  Giyosiddin  \nMuhammad  named  his son Alisher,  he dreamed  that he would  resemble  one of the Companions  of \nthe Prophet, Ali, and the word \"lion\" was taken from Ali. The Prophet, may Allah bless him and grant  \nhim peace,  used  to say, \"Ali is the lion of Allah.\"  It is the nickname  of the Navoi  poet,  derived  from  the \nword  \"Navo\"  []. At the heart  of this word  are the meanings  of \"tune,\"  \"happiness,\"  \"enjoyment,\"  \n\"singing.\"  The poet  based  his nickname  on the word  \"bahra\".  \nAlthough Navoi used the pseudonym Navoi for his works in Turkish and Foni for his works in  \nPersian -Tajik, we find in some sources that he used the pseudonym Foni for his works in Turkish and  \nNavoi  for his works  in Persian -Tajik.  \nIndeed,  Navoi  made  a great  contribution  to the development  of the Turkish  language.  His \ncontemporary  and student,  the historian  Khandamir,  also said of Navoi:  edi ”[] \nNavoi, who fought for the service of fiction to life and the needs of society, enriches the  \nthemes,  ideological  motives  and images  of lyrical  poetry.  The human  heart  is a fragrant  flower,  Navoi    «POLISH  SCIENCE  JOURNAL»   \n107  \n is a flower  ifori,  that is, a unique  and multifaceted  creator  who  can find his way into the hearts  of men.  \nAlisher  Navoi  embodied  the heroes  of his works  in such  a way that the beauty  of the images  in them  \nnot only warms  people's  hearts,  but also gives  them  a special  charm.  \nLife is so unfaithful  that we emerge  from  nothingness  and go to nothingness.  In it both  the \ngood  man  and the bad man  are born  and leave  this world.  In this transient  world,  a person  who  spends  \nhis life meaningfully will remain forever in every heart. In particular, Navoi lives forever in the hearts.  \nNavoi's  works  are an example  of calling  each  of us to the right  path.  \n\"It simply  came  to our notice  then  \nIt is obligatory  upon  you to grieve  before  you die, \nIt is impossible  to pass  the world  unfinished  \nErur comes  out of the bathroom  dirty.  ”[3] \nSo, it is clear  that on the continent,  the poet  encourages  people  to grow  in every  way.  In each  \nof Navoi's  works  we can see how deeply  he approached  the issues  of morality.  At the same  time,  the \npoet  emphasizes  that the more  irrational  it is to go to the bathroom,  the more  absurd  it is to come  \ninto the world  immature.  \nNavoi knew how to play the words and composed melodies. Navoi, who still lives, composed  \nthe song  \"Qari  navo\".  Navoi  also drew  great  pictures.  The picture  of the chain  poem  that has come  \ndown  to us was drawn  by Navoi.  Navoi  was not only a versatile  artist,  but also a mature  statesman  and \npublic figure of his time. In particular, his closest friend, Hussein Boykaro, served during his reign, at  \nthe seal level,  and later  as a minister. In the person  of Navoi,  we can see the image  of a patriotic  man  \nwho  is ready  to give his life for his country.  It was better  for him to serve  the people,  not the career,  \nthe glory.  It should  be noted  that Navoi  has built  more  than  300 facilities  at his  own  expense  in Herat  \nand throughout  Khorasan.  It is obvious  that Navoi  was a man  with  open  arms  and ready  to do anything  \nfor his people.  \nWe glorify  the name  of Alisher  Navoi.  Although  he was a great  man,  he was a very  humble,  \nkind,  patriotic  and noble  man.  Navoi  is a noble,  humble  and loyal  man.  His heart  shone  like the sun, \nand he was a great  man  who  will be remembered  for his good  deeds  throughout  his life. The great  \npoet,  thinker,  statesman  died  on January  3, 1501.  This incident  has become  a tragedy  for Navoi's  \nrelatives,  not only  for his relatives,  but for the whole  nation.    «POLISH  SCIENCE  JOURNAL»   \n108  \n  \nEvery  house  mourned  his death,  \nFigs came  from  every  corner  of the country.  \nIron-hearted,  stone -hearted  people,  too \nThey  won the blood  of this terrible  tragedy.  [2] \n(Muhammad  Amin)  \nIn conclusion,  we can say  a lot about  Alisher  Navoi. We can know  Alisher  Navoi  not only  \nas a person,  but also as a perfect  person.  In Oybek's  novel  Navoi,  at a time  when  Hussein  Bayqara  \nwas in a hurry  to enter  the war and his officials  were  also supporting  Hussein  Bayqara,  Navoi  \nexpressed  a reasonable  opinion.  \"The  king,  the first to start  a war,  looks  cowardly.  You must  wait  \nfor the right  time,  so that there  is no bloodshed.\"  After  that,  Hussein  Bayqara  will not rush  into \nthe war and this war will end in a truce.  We can see that Navoi's  thoughtful  thinking  reflects  \npeace  and humanity.  Anyone  who  reads  his lyrics,  poems  and poems  will enjoy  it. Reading  Navoi's  \nworks  teaches  everyone  a sense  of homeland,  love  for the spirit  of ancestors,  a culture  of mutual  \nrespect  and communication  between  people.  The past  cannot  be undone.  So every  day is history.  \nWhat name do we leave of ourselves in history? Ask yourself a question? Dear contemporaries!  \nAnd the memory  of Navoi  lives and  will live in the depths  of our hearts  every  day,  every  \nminute.   \nREFERENCES  \n \n \n1. Available  at https://kh -davron.uz/.  \n2. From  the site https://forum.ziyouz.com/.  \n3. The 10th  Continent  of The Strange  Cow  \n4. From  the textbook  \"Literature\"  (Bakijon  Tokhliyev,  Bahodir  Karimov,  Komila  \nUsmanova. Textbook for students of 10th grade of secondary schools and secondary  \nspecial,  vocational  education.  State  Scientific  Publishing  House  \"National  Encyclopedia  of \nUzbekistan\",  Tashkent -2017) .    «POLISH  SCIENCE  JOURNAL»   \n109  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \nSherov Zokir  \nUrganch  davlat  universiteti,  \nQarriyev Bekzod  \nUrDU  Jismoniy  madaniyat  1 kurs  magistir  \n(Urganch,  O’zbekiston)  \n \nG’OVLAR  OSHA  YUGURUVCHILADA  YILLIK  TAYYORGARLIK  \nMASHG’ULOTLARINI  REJALASHTIRISH  \n \nAnnotatsiya.  ushbu  maqolada  g’ovlar  osha  yuguruvchilarning  yillik  tayyorgarlik  \ndavridagi mashg’ulotlarni rejalashtirishda sportchilarga berilgan yuklamalarni to’g’ri berish va  \nto’siqlardan yugurishni texnikasini mukammal urganishga e’tibor berilgan.Maqolada ilmiy  \nuslubiy  adabiyotlar va  ilmiy tadqiqotlar taxlil  qilingan.  \nKalit so’zlar : mukammal yugurish texnikasi, tezlik, maxsus chidamlilik, chaqqonlik,  \negiluvchanlik,  alaktat -anaerob,  anaerob -glikolitik,  aralash  anaerob -aerob,ilmiy,  uslubiy,  \nadabiyotlar,  yuklamalar.  \n \nАннотация. В данной статье акцентируется внимание на точности даваемых  \nспортсменам  нагрузок  и совершенном  изучении  техники  бега  через  препятствия  при \nпланировании годовой подготовки бегунов с барьерами. В статье анализируется  \nнаучная  литература  и исследования.  \nКлючевые слова: отличная техника бега, скорость, специальная выносливость,  \nловкость, гибкость, алактатно -анаэробная, анаэробно -гликолитическая, смешанная  \nанаэробно -аэробная,  научная,  методическая,  литература,  нагрузка.  \n \n400 m t/o tezlikda  yugurish  yugurish  turlaridan  eng qiyin  mashqlardan  biri bo‘lib, \nsportchi  tanasiga  nihoyatda  yuqori  talablar  qo‘yadi. Bu masofada  yuqori  sport  natijalariga  \nerishish  uchun  mukammal  yugurish  texnikasi , tezlik  sifatlarining  yuqori  darajada  rivojlanganligi , \ntezlik  va maxsus  chidamlilik , shuningdek , chaqqonlik  va egiluvchanlik  bo'lishi kerak . 400 m t/o \nmasofani  juda  yuqori  tezlik  bilan  engib  o'tish  mumkin  (to'siqsiz  400 m yugurish  tezligining  92- \n96%).  \nTo'siqlarni joylashtirish: birinchi to'siq 45 m gacha, to'siqlar orasidagi masofa 35 m,  \nto'siq balandligi 91,4 sm. Ularni to‘g‘ri chiziq va burilish bo‘yicha yengib o‘tish zarurati,  \nmasofaning oxirgi choragida kuchli charchoq fonida yugurish 400 m masofaga to‘siqlardan  \no‘tish  texnikasining  o‘ziga  xos xususiyatlarini  va to‘siqlardan  o‘tuvchilarning  texnik  mahoratini  \nbelgilovchi omillardir. 400 t/o  tezlikda  to‘siqlarni  yengib  o‘tish  texnikasi  o‘zining asosiy  \nxususiyatlariga ko‘ra qisqa to‘siqli masofalarda to‘siqlarni yengib o‘tish texnikasidan amalda  \nfarq qilmaydi.  \nTurli  tadqiqotlar  natijalari  (V.G.  Alabin,  M.I. Dolgiy,  A.V. Karasev,  V.V. Stepanov,  \nM.M. Maisutovich, C. Gevat) shuni ko'rsatadi 400 metrga to'siqlar bilan yugurishda sport  \nko'rsatkichlari ko'p jihatdan maxsus chidamlilikni rivojlantirish darajasiga bog'liq. Biroq, hozirgi  \nvaqtda  ilmiy  va uslubiy  adabiyotlarda  rivojlanishning  samarali  usullari  mavjud  emas  lekin    «POLISH  SCIENCE  JOURNAL»   \n110  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \nadabiy manbaalarga qaraganda jismoniy sifatlarni rivojlantirishga ko’proq qaratilganini ko’rish  \nmumkin.  \nMashg'ulotlarda  400 metrga  to'siqlardan  otish  maxsus  chidamlilikni  rivojlantirishning  \neng samarali vositasi sifatida qaraladi. Mashg’ulotlarni rejalashtirishda yuklamalar hisobiga  \nasoslangan  mashqlar  tasnifi  qo'llaniladi  mushak  faoliyatini  energiya  bilan  ta'minlash  \nmexanizmlari. Bunday tasnif berilgan yuklamalarni to'rtta quvvat zonasiga bo'linishni o'z ichiga  \noladi (shunga ko'ra yo'nalishlari): alaktat -anaerob, anaerob -glikolitik, aralash anaerob -aerob va  \naerob  (N.I. Volkov, A.L.  Novikov,  M.R.  Smirnov)  \nIlmiy  va uslubiy  adabiyotlar  tahlili  mualliflar  tomonidan  taklif  qilinganligini  ko'rsatadiki  \nto'siqlarni o'rgatishda etarlicha katta miqdordagi yuklamalardan foydalanish aerob yuklamalar  \nham tayyorgarlik, ham yillik o'quv tsiklining raqobat davrlari, asosiy yuklammalarning hajmidan  \nalaktat -anaerob yo'nalishi ortish tendentsiyasiga ega musobaqaga tayyorgarlik davri. Asosiy  \nyuklammalarning hajmlari anaerob -glikolitik va aralash anaerob -aerob yo'nalishi ular juda  \nxilma -xil bo'lib,  batafsil  ko'rib  chiqishni  talab  qilinishini  bir guruh  olimlar  tavsiya  qilganlar.  \nStandart  metodlardan  foydalanish  imkoniyatini  o'rganish  uchun  maxsus  chidamlilikni  \nrivojlantirishga qaratilgan guruh va ta'riflar turli yo'nalishdagi yuklarning optimal nisbati 400 m  \nto'siqlar bo'yicha yuguruvchilarni tayyorlashning yillik tsikli, o'tkaziladi. Tadqiqotlar shuni  \nko'rsatdiki, uch hafta davomida yuguruvchilar davom etganda 400 m to'siqlar anaerob glikolitik  \nyo'nalish katta miqdorda bajailgan yuklammalarni amalga oshirdi. Mikrotsiklning 1, 3 va 5 - \nkunlarida 7 kun davom etgan, to'siqlar uzunligi  bo'lgan segmentlarni yugurishdi 100 –300 m,  \nintensivligi 91 –95%. Bundan tashqari, bundaylarning tuzilishi va mazmuni eksperimentning 1, 2  \nva 3 haftalaridagi  mashg'ulotlar  standart  mashq uslubidan  foydalandi.  \nMikrotsiklning qolgan kunlarida kattalar bilan mashg'ulotlar o'tkaziladi va o'rtacha  \nyuklamalar  yuqori  tezlik,  quvvat,  tezlik -quvvat  va texnik  orientatsiya.  Ushbu  tadqiqot  \nmusobaqa davrida o'tkazildi yillik mashg'ulot tsikli, shuning uchun yanada tekiz yugurish va  \nto'siqlar anaerob -glikolitik energiya ta'minoti zonasida amalga oshirildi 400 m masofaga  \nto'siqlar  bilan  yugurish  bo'yicha  ixtisoslashgan  yuguruvchi -yoshlar  va o'smirlar  bilan  \nishlaydigan  murabbiylar  to'siqlardan  o'tishning  barcha  sifatlari  va usullarining  uyg'un  \nkombinatsiyasi tamoyiliga amal qilishlari kerak, shuning uchun sezgir ta'sir zonalari tugaguncha  \nkutish  kerak  (o'z ichiga  oladi,  yuguruvchilarning  biologik  yoshini  hisobga  olish)  vosita  \nmahoratining  individual  tomonlarini  tashkil  etuvchi  barcha  fazilatlarning  rivojlanish  \ndarajasining tabiiy o'sishi va shuning uchun ixtisoslikni boshlash juda kech. Shu munosabat  \nbilan  mashg'ulot  jarayonini  boshidanoq  uni individuallashtirish  orqali  olib borish  \nsamaraliroqdir.  \nAvvalo, yuguruvchilarni har bir etakchi ko'rsatkich bo'yicha va aniq yillik tsiklning butun  \ndavrida, joriy ish yo'naltirilgan yo'nalishning selektiv testlarini nazorat qilish bilan cheklanib  \nqolmasdan,  sinovdan  o'tkazish  kerak,  chunki  bu juda  muhim.  Kechiktirilgan  ta'sirning  \nxususiyatlari yoki turli ishlarning butun kompleks ko'rsatkichlarga bilvosita ta'siri haqida  \nma'lumot  olish.  Ushbu  ma'lumotlar  bilan  tasodifiy  emas,  balki  keyingi  ishlarni  qurish  mumkin  \nbo'ladi. Uni ritmik va texnik tarkibning testlari, shuningdek, kuchga chidamlilik testi bilan  \nto'ldirish tavsiya etilishi mumkin. Sportchining \"kuch -chidamlilik\", \"tezlik -tezlik chidamliligi\",  \n\"tezlikka  chidamlilik -to'siqlar  texnikasi\"  va boshqalar  kabi mashg'ulotlarning  turli \nkombinatsiyalariga  individual  reaktsiyalariga  alohida  e'tibor  berish  kerak.  Har bir sportchi    «POLISH  SCIENCE  JOURNAL»   \n111  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \nuchun  bu bog'lanishlar  engil  atletika  metodologiyasida  yoki sport  mashg'ulotlari  nazariyasida  \n(B. Yushko, 1987 va boshqalar) qabul qilingan bunday juftliklarning mosligi yoki qarama - \nqarshiligi haqidagi umumlashtirilgan g'oyalarga qaraganda ancha noaniqdir, shuning uchun har  \nbir yuguruvchi  o'rganishi  kerak  turli tuzilmalar  mashg’ulotlarga  moslashish.  \nMusobaqa  davrida  shuni  hisobga  olish  kerakki,  agar  kalendar  rejada  yugurish  \nnatijasida tayyorgarlikni amalga oshirish uchun zarur bo'lgan minimal startlarni o'z ichiga  \nolmasa (hatto o'rganilgan mahorat darajasidagi to'siqlari bo'lgan 400 m to'siqlar uchun) kamida  \n10), keyin maxsus musobaqalarda to'liq masofani bosib o'tish kerak - hisob -kitoblar, lekin hatto  \nbu startlar ketma -ket 4-5 haftadan ko'p bo'lmagan holda rejalashtirilgan bo'lishi kerak va keyin  \nhamma  uchun  umumiy  qoida  bo'lishi  kerak.  tortib  olishg a qaratilgan  mikro -tayyorgarlik  tsikli  \nbir yoki uchta (alohida  ortda  qoladigan)  tayyorgarlik  komponentlarini  beradi.  \n \nADABIYOTLAR:  \n1. Артюшенко  А. Техническая  подготовка  барьеристов.  // Легкая  атлетика.  - 1978. -N6. - \nС. 12-13. \n2. Аракелян Е.Е., Разумовский Е.А., Черенева Л.А. Барьерный бег. //Легкая атлетика:  \nУчебник для институтов физ.культ./ Под ред. Н.Г. Озолина, В.И. Воронкина, Ю.Н.  \nПримакова. - Изд.4 -е, доп.,  перераб. - М.:Физкультура  и спорт,  1989.  - С. 334-372.  \n3. Алабин  В., Майшутович  М., Масловский  Е. 400 м с/б. //Легкая атлетика.  - 1974.  - N 9. \n- С. 26-27. \n4. Степанова  М.И.  Подготовка  спортсменок  мирового  класса  в беге  на 400м  с \nбарьерами;  Автореф. дис..  канд. пед.  наук.  СПб, 1996,  - 24 с.    «POLISH  SCIENCE  JOURNAL»   \n112  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \nXudayberganov Otabek  \nUrganch  davlat  universiteti,  \nDavletov Jaxonali  \nUrDU  Jismoniy  madaniyat  1 kurs  magistir  \n(Urganch,  O’zbekiston)  \n \n8 – 9 YOSHLI  GIMNASTIKACHILARNING  JISMONIY  SIFATLARINI  \nRIVOJLANTIRISHNING  O’ZIGA  XOS  JIHATLARI  \n \nAnnotatsiya.  Ushbu  maqolada  so’nggi  yillarda  gimnastik  elementlarning  \nmurakkablashib  borishi  yosh  gimnastikachilar  jismoniy  sifatlarini  rivojlantirish  va \ntakomillashtirish masalasiga alohida e’tibor qaratmoqda. Ammo ko’pincha shu sifatlarning  \nrivojlanganlik  darajasi  gimnastik  elementlarni  samarali  o’zlashtirish  imkonini  bermaydi.  \nKalit so’zlar: jismoniy va funksional imkoniyatlar, yuklamalar, gimnastik mashqlar,  \nkombinatsiyalar, sportchilar, tayyorgarlik, ilmiy -tadqiqot, malakali sportchilar, nazorat qilish,  \nsport anjomlari, xalqaro musobaqalar, jismoniy sifatlar, rivojlantirish, gimnastik element  \ntexnikasi . \n \nАннотация.  Данная  статья  посвящена  развитию  и совершенствованию  \nфизических качеств юных гимнасток по мере усложнения гимнастических элементов в  \nпоследние  годы.  Но зачастую  уровень  развития  этих  качеств  не позволяет  эффективно  \nосваивать  элементы  гимнастики.  \nКлючевые  слова:  физические  и функциональные  возможности,  нагрузки,  \nгимнастические упражнения, комбинации, спортсмены, тренировка, исследование,  \nквалифицированные спортсмены, контроль, спортивный инвентарь, международные  \nсоревнования,  физические  качества,  развитие,  техника гимнастических  элементов.  \n \nYosh  sportchilarni  tayyorlash , shu jumladan  gimnastika  bo’yicha  iste’dodli  sportchi  \nbolalarni  yetishtirish  o’quv-trenirovka  jarayoni  yangicha  ilmiy  nazar  bilan  yondoshish  hamda  \nsamarali  pedagogik  texnologiyalar  yaratish  darkorligini  taqozo  etadi . Sport  amaliyoti  tajribasi  \nshuni  ko’rsatadiki , sportchilarni  tayyorlashda  qo’llaniladigan  an’anaviy  uslub  va vositalar  ko’p \nyillik  tayyorgarlik  jarayonining  muayyan  bosqichida  o’z samaradorligini  yo’qotishi  yoki \nsustlashtirishi  mumkin . Natijada, sport natijalari ham, jismoniy va funksional imkoniyatlar ham  \ntaraqqiyotdan  to’xtashi  ehtimoldan  holi emas.  Bunday  vaziyatni  kelib  chiqishi  tabiiy  holatdir.  \nChunki  organizmning  yuklamalarga  moslashish  imkoniyati  cheksiz  bo’lmaydi,  an’anaviy  \nyuklamalarga ko’nikish hosil bo’ladi, ixtisoslashtirilgan maxsus mashqlar sport mahoratini  \no’sishiga  qudrati  etmay qoladi.  \nMa’lumki, yildan yilga gimnastik mashqlar dasturi yangicha mazmun va mohiyatga ega  \nbo’lib,  ijro etilayotgan  harakat  kombinatsiyalari  borgan  sari murakkablashib  borayapti.  \nBinobarin, ushbu vaziyat gimnastik ko’pkurash turlari istiqbolini oldindan belgilab berish  \nimkonini yaratadi. Bularning barchasi olimpiada o’yinlarining ommalasha borishi, ularning  \ndasturlari va medallar sohiblari geografiyasining kengaya borishi hamda katta sportning  \nprofessional  va tijorat  qiyofasiga  kirib  borayotganligi  bilan  bog’liqdir.  Bu esa o’z navbatida    «POLISH  SCIENCE  JOURNAL»   \n113  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \nmashg’ulot va musobaqalar yuklamasi hajmi hamda shiddatini ortishi, sportchilar mahoratining  \no’sishi, tayyorgarlikning yangi samarali uslub va vositalarini ishlab chiqish va amalga tadbiq  \netish, katta sport yo’nalishidagi ilmiy -tadqiqot ishlarining jaddallashishi va shu jarayonni  \nta’minlashda qo’llanilidagan ilmiy -tadqiqot asboblarining yangi avlodini yaratilishi, malakali  \nsportchilarning tayyorgarlik jarayonini kompyuter tizimi asosida nazorat qilish, sport anjomlari,  \nuskunalari  va sport  kiyimlarini  takomillashib  borish iga asoslanadi.  Sport  industriyasining  keng  \nquloch  yozishi,  sport  infrastrukturasining  rivojlanib  borishi,  xalqaro  musobaqalar  taqvimining  \nkengayishi,  sportchi  va murabbiylarning  turli mamlakatlarda  faoliyat  ko’rsatishi,  sport  va ilmiy  \naxborot  hajmining  ortib  borishi,  ilmiy -sport  markazlari  va ko’pdan -ko’p  tayyorgarlik  \nbazalarining tashkil etilishi bilan belgilanadi. Shu bilan bir qatorda, ta’kidlash joizki, katta  \nsportga olib chiquvchi ko’p yillik tayyorgarlik tizimidan joy olgan dastlabki tanlov va sportga  \nyo’naltirish  jarayoni  zamonaviy  sportning  o’ta dolzarb  va fundamental  ahamiyatga  ega \nmasalalardan biridir.  \nGimnastik  mashqlarga  dastlabki  o’rgatish  jarayonida  bolalar  trenerlari,  aksariyat  \nhollarda,  ayrim  muammolarga  duch  kelishi  kuzatilib  turadi.  Jumladan,  hatto  eng oddiy  \ngimnastik  element  texnikasiga  o’rgatishda  muayyan  jismoniy  sifatlarning  rivojlanganlik  \ndarajasiga tayaniladi. Jismoniy sifatlarni rivojlantirish jarayoni turli malakali sportchilarni  \ntayyorlash tizimining ustivor va ajralmas qismiga mansubdir. Ko’pgina tadqiqotlar shuni  \nko’rsatadiki,  ushbu  sifatlarni  rivojlantirish  samaradorligi  harakat  malakalariga  o’rgatish  \nmuddati  va shu malakalarni  o’zlashtirish  sur’atini  belgilaydi.  So’nggi  yillarda  gimnastik  \nelementlarning  murakkablashib  borishi  yosh  gimnastikachilar  jismoniy  sifatlarini  rivojlantirish  \nva takomillashtirish  masalasiga  alohida  e’tibor  qaratmoqda.  Ammo  ko’pincha  shu sifatlarning  \nrivojlanganlik  darajasi  gimnastik  elementlarni  samarali  o’zlashtirish  imkonini  bermaydi.  \nShunday  vaziyatlarda  odatda  ikki yo’nalishda  tashkil  etiladigan  maxsus  mashg’ulotdan  \nfoydalanish  tavsiya  etiladi.  Ushbu  turdagi  mashg’ulotlarning  yo’nalishlaridan  biri - bu harakat  \nqobiliyatini  shakllantirish,  ikkinchisi  - harakat  ko’nikmalarini  tarbiyalash.  Qayd  etilgan  \nvazifalarni alohida mustaqil va birgalikda hal etish mumkinligidan qat’iy nazar muvofiq jismoniy  \nsifatlarni  ham  bir yo’la  rivojlantirish  zarur  bo’ladi.  Binobarin,  zamonaviy  gimnastik  mashqlarni  \nsamarali o’zlashtirish ustivor jihatdan ixtisoslashtirilgan jismoniy tayyorgarlikka bog’liqdir.  \nJismoniy  tayyorgarlik  umumiy  va maxsus  jismoniy  sifatlarni  rivojlantirish  jarayonini  o’z ichiga  \noladi. Mutaxassis olimlarning fikriga ko’ra, dastlabki tayyorgarlik bosqichida shug’ullanayotgan  \nyosh  gimnastikachilarda  shu yoshda  qulay  kechadigan  jismoniy  sifatlarni,  jumladan,  tezkorlik - \nkuch, egiluvchanlik, chaqqonlik va alohida koordinatsion sifatlarni rivojlantirmoq lozim. Ushbu  \njismoniy  sifatlarni  rifojlantirishda  qaysi  biriga  ustunlik  bilan  yondoshish  masalasida  \nmutaxassislar  fikri turlicha.  Bir guruh  mutaxassislar  tezkorlik,  kuch -tezkorlik  sifatlari  va \nkoordinatsion qobiliyatga urg’u berish lozimligini ta’kidlaydilar. Ular shu sifatlarga ustunlik  \nberilishini bolalarning yosh xususiyatiga mutanosib ekanligi bilan izohlaydilar. Boshqa bir guruh  \nolimlar jismoniy sifatlar o’rtasidagi uzviy bog„liqlikni e’tiborga olgan holda, mazkur yoshdagi  \nbolalarda  barcha  sifatlarni  barovar  rivojlantirish  muhimligiga  e’tibor  qaratadilar.  Shu bilan  bir \nqatorda  yosh  gimnastikachilar  jismoniy  tayyorgarligining  asosiy  jismoniy  sifatlarini  turli \nyo’nalishda, jumladan gimnastik elementlarga mos yo’nalishda o’rganish bo’yicha o’tkaziladigan  \ntadqiqotlar  deyarli  yo’q  ekanligi  kuzatildi.    «POLISH  SCIENCE  JOURNAL»   \n114  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \nKo’p yillik musobaqa amaliyotida sport natijalarining o’sishini ta’minlash dastlabki  \ntayyorgarlik  bosqichida  olib boriladigan  o’quv -trenirovka  mashg’ulotlari  samaradorligini  \noshirish zarurligiga e’tibor qaratadi. Keyingi bosqichlarda yosh gimnastikachilar tayyorgarlik  \njarayonini boshqarish yuksak sport mahorati talablari doirasida amalga oshirilishi darkor.  \nBuning  uchun  maksimal  sport  natijalarini  ko’rsatish  mumkin  bo’lgan  yosh  davrigacha  harakat  \nqobiliyatlarini takomillashtirishga qaratilgan uslub va vositalardan unumli foydalanish lozim  \nbo’ladi.  \nZamonaviy gimnastikada o’rgatilishi zarur bo’lgan harakat malakalariga xos jismoniy  \nsifatlarni  qisqa  muddat  ichida  rivojlantirish  imkoniyatini  yaratuvchi  uslub  va vositalar  \nqo’llaniladi. Bu esa o’z navbatida gimnastik snaryadlarda ijro etiladigan mashqlarni tezroq  \no’zlashtirish  va bu masalaga  ko’proq  vaqt  ajratishni  taqozo  etadi.  Gimnastikachilarning  texnik  \ntayyorgarligi  ularning  jismoniy  sifatlariga  asoslanadi.  Agar  gimnastikachining  jismoniy  \nimkoniyati  gimnastik  element  texnikasi  talablariga  javob  bermasa,  birorta  mashqni  maromida  \nbajarish mumkin bo’lmaydi. Boz ustiga, texnik mahoratni takomillashtirish nuqtai nazaridan  \nkuchga  oid tayyorlov  mashqlari  tarbiyalanayotgan  sifatni  yuksak darajada  shakllanishini  \nta’minlashi kerak. Bu borada gimnastikachilarga maxsus yo’naltirilgan mashg’ulotlar az qotadi.  \nAyniqsa, musobaqa davrida gimnastikachilar uchun o’rganilay otgan elementlarga xos sifatlarni  \nshakllantirish  muhim  ahamiyatga  egadir.  Binobarin,  shunday  ekan,  har bir muayyan  vaziyatda  \nharakat  malakasiga  o’rgatish  jarayoni  o’ziga  xos xususiyatlarga  ega bo’lishi  zarur.  \nXulosa o’rnida shuni aytish kerakki jismoniy sifatlarni rivojlantirish jarayoni shunday  \ntashkil etilishi lozimki, oldindan o’zlashtirilgan bir guruh mashqlar nafaqat boshqa turdosh  \nmashqlar  elementlariga  o’rgatish  masalasini  engillashtirishi  darkor,  balki  dastlabki  \no’zlashtirilgan mashqlar qayd etilgan turdosh mashqlarga o’rgatishda asosiy manba bo’lib  \nxizmat qilishi maqsadga muvofiqdir. Bunga etakchi texnik xususiyatlarga ega keng gimnastik  \nmashqlar  majmuasiga  asosan  ishlab  chiqilgan  muvofiq  jismoniy  va maxsus  harakat  \ntayyorgarligi hamohang bo’lishi mumkin. Tashkiliy va uslubiy jihatdan bunday ixtisoslashtirilgan  \ntayyorgarlik  nafaqat  tasnifiy  asosda  uyushtirilishi  kerak,  balki  maxsus  tanlangan  o’quv - \ntrenirovka  mashqlari  yordamida  olib borilishi  muhim  ahamiyat  kasb  etadi.  \n \nADABIYOTLAR:  \n1. Umarov  M.N.,  Eshtaev  A.K. Planirovanie  i raspredelenie  sredstv  trenirovki  gimnastov  na \nnachalnom  etape  podgotovki.  Ucheb.  posobie. – T.: UzGIFK, 2004.  -154 s.  \n2. Eshtaev  A.K. Gimnastika  darsi:  O’quv  qo’llanma.  - T.: 2004.  – 120 b. \n3. Umarov  M.N.  Gimnastika:  O’quv  qo’llanma.  – T.: Vneshin  vestrom,  2015.  – 400 b.   «POLISH  SCIENCE  JOURNAL»   \n115  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \n \nMUSTAQILLIK YILLARIDA MILLIY QO‘SHIQCHILIK SAN’ATIGA OID BAG‘ISHLANGAN   \nTADQIQOTLAR: SHE’R VA MILLIY IJOD  \nJahongir Shukur  \nO'zbekiston davlat konservatoriyasi,  \n\"Kompozitorlik va cholg'ulashtirish\" kafedrasi dotsenti  \n \nAnnotatsiya : Ushbu  maqolada  mustaqillik yillarida milliy qo‘shiqchilik san’atiga oid \nbag‘ishlangan  tadqiqotlar: she’r va milliy ijod  xususida  batafsil  ma’lumotlar  keltirib  \no‘tilgan . Shuningdek , mustaqillik  yillarida  xalqimizning  milliy  qadriyatlarga  qaratilgan  \ne’tibori  va ijtimoiy  hayoti  haqida  tahliliy  fikr-mulohazalar  keltirib  o‘tilgan . \nAnnotation: This article provides detailed information on family ceremonies  \nof the Uzbek  people.  It also analyzes  the attention  paid  to the national  values  and social  \nlife of our people  during  the years  of independence.  \nАннотация:  В данной  статье  представлена  подробная  информация  о \nсемейных  обрядах  узбекского  народа.  Также  анализируется  то внимание,  которое  \nуделялось национальным ценностям и общественной жизни нашего народа в  \nгоды  независимости.  \nKalit  so‘zlar: oilaviy  marosim , an’ana, udum , ijtimoiy  hayot , urf-odat , islohot  \nKeywords : family  ceremony , tradition , customs , social  life, customs , reform  \nКлючевые  слова:  семейный  обряд,  традиция,  обычаи,  общественная  \nжизнь,  обычаи,  реформа.  \n \nO`tgan  ХX asr so`nggi  choragidan  boshlab  dunyo  miqiyosida  etnolog  va sotsial  \nantropologlar  tomonidan  marosimlarni  хalq madaniyatining  fenomeni  tarzida  \no`rganishga  alohida  e`tibor  qaratildi  va aynan  mazkur  mavzu  doirasida  bajarilgan  ilmiy  \ntadqiqotlar  fandagi  dolzarb  mavzulardan  biri tarzida  e`tirof  etila  boshlandi . Ayniqsa  \nso`nggi  o`n yilliklarda  dunyo  miqiyosida  kechayotgan  globalizatsion  jarayonlar  etnik  \nхususiyatlarning  saqlab  qolish  omillarini  va milliy  marosimlarni  tadqiq  etish  \nmuammosini  dolzarb  muammolar  tarzida  kun tartibiga  qo`yilishiga  sabab  bo`lmoqda . \nO`zbek  xalqining  qadimiy  rasm -rusumlari , dunyoqarashi , ishonch -e`tiqodlari , turmush -\ntarzi  va hayot  haqidagi  falsafiy  qarashlari , asrlar  bo`yi jamlangan  hayotiy  tajribalari  va \nturmush  ko`nikmalarini  o`zida mujassamlashtirgan  eng jozibali  marosimlarimizdan  biri \n- nikoh  to`yidir . Xalqimizning  qadimiy  e`tiqodlarini , madaniyat    «POLISH  SCIENCE  JOURNAL»   \n116  \n tarixi , orzu  intilishlari , hayotga  munosabatini  o`rganishda  nikoh  to`yi bilan  bog’liq \nmarosimlar  muhim  ahamiyatga  egadir . \nDunyodagi  ko`plab  boshqa  xalqlar  singari  o`zbek  to`y marosimlari  sirasida  \nnikoh  to`ylari  uzining  turli xildagi  marosimlarga , irim-sirimlarga  boyligi , etnografik  \nmanbalarining  rang -barangligi  bilan  alohida  ajrab  turadi . \nO‘zbek  xalqi  qadim  o’tmishdan  bugungi  kungacha  o’zining  boy tarixi , ma’naviy  \nqadriyatlari , milliy  urf-odatlariga  ega bo’lib, zamonlar  osha  uni o’z qalbi  va \nma’naviyatiga  saqlab  kelib , avloddan -avlodga  meros  qilib  qoldirib  kelmoqda . Tarix  \ninsoniyat  hayotining  shakllanishi , taraqqiy  etib borishi  va kelajak  uchun  zaruriy  \nehtiyojlarni  yaratishga  asos  bo‘lib xizmat  qilishi  bilan  birgalikda , o‘tmish  \najdodlarimizning  bosib  o‘tgan  turmush  tarzini  halollik , haqqoniylik  hamda  tanqidiy  \nruhda  tahlil  qilishni  ham  o‘z zimmasiga  yuklaydi . Tarixni  o‘rganish , tarixiy  voqealarni  \nkelajak  avlodga  haqqoniy  ochib  berish  haqida  O‘zbekiston  Respublikasi  Birinchi  \nPrezidenti  Islom  Karimov  “Yuksak  ma’naviyat  – yengilmas  kuch ” asarida  quyidagi  \nfikrlarni  bildirgan : “El yurtimiz  o‘zining  ko‘p asrlik  tarixi  davomida  mash ’um xatarlarni  \nnecha  bor ko‘rgan , ularning  jabrini  tortgan . Shunday  asoratlar  tufayli  tilimiz , dinimiz  va \nma’naviyatimiz  bir paytlar  qanday  xavf ostida  qolganini  barchamiz  yaxshi  bilamiz . Ana \nshu fojiali  o‘tmish , bosib  o‘tgan  mashaqqatli  yo‘limiz  barchamizga  saboq  bo‘lishi, \nbugungi  voqelikni  teran  tahlil  qilib , mavjud  tahdidlarga  nisbatan  doimo  ogoh  bo‘lib \nyashashga  da’vat etishi  lozim . O‘z tarixini  bilmaydigan , kechagi  kunini  unutgan  \nmillatning  kelajagi  yo‘q. Bu haqiqat  kishilik  tarixida  ko‘p bora  o‘z isbotini  topgan ”. \nVatanimiz  milliy  mustaqillikni  qo‘lga kiritganidan  keyin  tarixiy  haqiqatni  mafkuraviy  \ntazyiq  va ta’sirlardan  xoli tarzda , xolisona  va haqqoniy , adolat  va tarixiylik  nuqtai  \nnazaridan  xalqimizga  yetkazish  imkoniyati  yuzaga  keldi . \nO‘zbekiston tarixan olib qaraganda juda ham qadimiy davlatlar qatoridan joy  \noladi.  Uning  bu ko‘hnaligi  nafaqat  moddiy  balki,  madaniy  qadriyatlarda  ham  o‘z \nifodasini  topgan.  Bu an’ana  hamda  udumlarimiz  etnograf  olimlar,  tarixchilar  va \nsayohatchilar o’z asar va kundaliklarida ko’plab ma’lumotlar keltirib o’tganlar. Bunga  \nmisol  qilib  Rui Gonsales  de Klavixoning  “Kundalik”  hamda  Xerman  Vamberining  \n“Sayohatnoma” asarlarini keltirib o‘tishimiz  mumkin.  \nShuningdek,  K.Shoniyozov  ilmiy  nazariy  asarlarida  o‘zbek  xalqining  etnik  tarixi,  \nmoddiy va ma’naviy hayoti xususan, toʽy marosimlari to‘g‘isida qimmatli ma’lumotlar  \nkeltirilgan,  Muallif  o‘zbek  nikoh  to‘yining  barcha  jarayonlariga:  sovchilik,  fotiha,  \nbevosita nikoh to‘yi urf-odatlariga va to‘ydan keyin o‘tkaziladigan udumlarga alohida  \ne’tibor  qaratadi.  \nEtnografiyaning  nazariy  muammolari  sirasiga  kiruvchi  ma’naviy  madaniyatning  \nmuhim qismi bo‘lgan oila -nikoh masalalariga bag‘ishlangan mutaxassislarning ilmiy  \nishlariga. A.Doniyorov, G.Zununova, K. Kubakov, M. Kosven, Hoji Muin, N.Azimova,  \nS.Davletova,  M.Ibragimova,  X.Ismoilov,  I.Jabborov,  K.Shoniyozov,  O.Bo‘riyev,  A.Ashirov  \nilmiy  ishlari  maqolaning  uslubiy  asoslarini  belgilab  olishga  yordam  berdi.Dala  etnografik  \nmateriallari  suhbat  va shaxsiy  kuzatish  asosida  to‘plandi.  Maqolada  tavsifiy  va qiyosiy    «POLISH  SCIENCE  JOURNAL»   \n117  \n tahlil,  tarixiylik,  tizimli  yondashuv,  sematik - semiotik  nazariya  usullaridan  foydalanildi.  \nMustaqillik  bizga  qadim  qadimdan  ota bobolarimizdan  meros  urf \nodatlarimizga  erkin  amal  qilish  imkonini  berdi.  Shu o’rinda  azaldan  to’y-tomosha  \noshuftasi  o’zbek  xalqining  to’y marosimlari  bilan  bog’liq  an-a’nalari  hamda  rasm - \nrusmlari faqatgina xursandchilik va shodiyonalik bo’libgina qolmasdan, balki, muayyan  \nmaqsadga qaratilgan, asrlar davomida rioya qilinib kelinayotgan diniy udumlar majmui  \nhamdir. To’y -bashariyat aql zakovati, tabiiy, ma’naviy -axloqiy va huquqiy ehtiyojlari,  \ntalablari asosida vujudga kelib, uzoq tarixiy taraqqiyot bosqichlarini bosib o’tib, hozirgi  \nkunimizgacha  yetib  kelgan  va istiqbolda  ham  ma’naviy  qadriyatlarimiz  tarkibida  \nmustahkam saqlanib keladigan noyob hayotiy marosimlar majmuidir. Darhaqiqat, to’y  \ndeb ataluvchi so’z anglatgan marosimlar hamma vaqt hamma joyda kishilarning talabi,  \nehtiyoji bilan  o’tkazilar  ekan,  ushbu  hayotiy  hodisaning  turli elatlar  va xalqlar  va \nmillatlardagi vujudga kelish tarixi, shakllari, tarixiy jihatdan kasb etgan xususiyatlarini  \no’rganish,  uning  tarkibida  saqlanib  qolgan  umuminsoniy  , milliy  va mahalliy  ijobiy  \nan’analarni  saqlab  qolish:  qoloq,  davr  talablariga  mos  kelmaydigan  jihatlarni  isloh  etish,  \nkamxarajatli, ixcham to’ylar qilishga xalqimizni odatlantirish bugungi kunda dolzarb  \nmasalalardan biriga aylanmoqda.[2 -65] \nTarixiy manbalarda saqlanib qolgan ma’lumotlarga qaraganda “to’y” -atamasi  \nturkiy so’z bo’lib, “toy” yoki “qurultoy” shaklida mo’g’ullar va chig’atoy xonlari davrida  \nham keng qo’llanilgan. XIV asrda Movarounnahr hududiga sayohat qilgan arab sayyohi  \nIbn Batuta esdaliklariga asosan “toy” -bu har yili o’tkaziladigan yig’in (qurultoy) bo’lib,  \nunda Chingizxon avlodlari, amirlar va turkey beklar, obro’li ayollar va lashkarboshi  \nishtirok etgan. Yakunlangan muddati 1864 yil 5 may bo’lgan “Tavorixi xorazmshoxiya ”  \ntarixiy asarida uchraydigan ma’lumotga  ko’ra, vazir Xasan -Murod qushbeginig o’g’li  \nMuhammad Yusufbek xonning qiziga uylangan bo’lib, ziyofat -toy (to’y) bir necha kun  \ndavom  etgan.  Tadqiq  etilgan  aksariyat  ma’lumotlar  asosida,  “to’y”  –qadim  zamonlardan  \nberi doimo  katta  jamoa,  yuqori  martabali  zadogonlar,  katta  hajmdagi  yig’in,  \ncheklanmagan  miqdordagi  ziyofat  va dabdaba  bilan  uzviy  bog’liq  bo’lgan  odatligi  \naniqlandi.  \nO’zbek to’y marosimlari davrlar o’tishi bilan qisman o’zgarib borgan va har bir  \netnik guruhda o’ziga xos xususiyatlari bilan o’zaro farq qiladi. Qadimdan meros bo’lib  \nkelayotgan  to’y marosimlari  islomgacha  va islomdan  keying  ayrim  magik  xarakterga  ega \nbo’lgan udumlarning hozirgi zamonaviy odatlar bilan qorishib ketganligini millatlar  \no’rtasidagi  aloqalarning  globallashuvi  natijasidir.  Jamiyatning  asosiy  bo’g’ini  \nhisoblanmish  oila munosabatlari  nikohdan  boshlanadi.  Bunda  ikki yoshning  yangi  \nhayotga qadam qo’yayotganligi el -ulus , qo’ni -qo’shnilarga ma’lum qilinib, ularning  \nturmush qurishga kirishayotganligi e’tirof etiladi. Ana sh u nikoh to’yi o’zbek xalqida  \nboshqa  to’ylardan  to’y ishtirokchilari  tomonidan  murakkab  urf-odatlar  va irim- \nsirimlarning bajarilishi, ko’plab qadimiy e’tiqod va topinishlarning masalan, fetishizm,  \ntotemizm, ajdodlar ruhiga sig’inish, olov va boshqa bir qator demonologik ko’rinishlar  \nizlarining  mavjudligi bilan ajralib  turadi.    «POLISH  SCIENCE  JOURNAL»   \n118  \n O’zbek xalqinig  uzoq tarixga ega yana bir  to’y marosimi  beshik to’yidir.  Ushbu  \nto’y nikoh to’yinig mahsuli bo’lib, oilada tug’ilgan ilk farzand sharafiga beriladi. Bunda  \nchaqaloqning ona tomondan qarindoshlari chaqaloqqa beshik va zarur bo’lgan barcha  \nbuyumlarni  karnay -surnay  sadolari  ostida olib  borishadi.  \nMillatimizning  qadimgi  to’ylaridan  yana  biri sunnat(xatna)  to’yidir,  undan  maqsad  \nislomning  sunnatini  bajarishdir.  To’y  o’g’il  bolalarning  tog’ yoshlarida,  ya’ni  1-3-5 \nyoshlarida  o’kaziladi.  To’y  oldidan  xatmu  qur’on  marosimi  o’tkazilib,  elga osh tortish  va \nto’y bolaga ezgu tilaklar tilash  bilan  yakunlanadi.  \nUmuman  olganda,  xalqimizning  tarix  sinovlaridan  o’tgan  bayramlari  \nmarosimlari,  urf-odatlari  xalqimizning  milliy  xususiyatlari,  qadriyatga  aylangan  \nxalqchillik ruhi bilan sug’orilgan jihatlarini, halollik, poklik, mehnatsevarlik xislatlarini  \nnamoyon  etadi.  Xalqning  o’ziga  xos milliy,  ruhiy  ko’rinishlarining  barchasi  xalq to’ylarida  \no’z ifodasini topadi.Aynan mazkur marosimlar va odatlar yashovchanligining asosiy  \nsababi ularning asosiy mazmun mohiyati hamda maqsadi kelajak avlod davomiyligini  \nta’minlashga  qaratilganligi  bilan  xarakterlanadi.  \nAmmo  so’ngi  kunlarda  xalqimiz  orasida  to’y marosimlaridagi  asosiy  e’tibor  \nyoshlarning  baxtli  bo’lishiga  emas,  quda  tomonning  to’y uchun  qilgan  xarajati  \nmiqdoriga  qaratilmoqdagiki,  bu jamiyatning  ma’naviy  muhitiga  salbiy  ta’sir  \no’tkazmoqda.  Kuzatishlar  shuni  ko’rsatmoqdaki,  aholining  aksariyat  qismi  orasida  to’y \nmarosimlari  uchun  sarf-xarajatlarning  oshib  borayotganligi,  isrofgarchilik,  ko’p  sonli  \nmehmonlarning  taklif  etilishi -bu yillar  davomida  kutilgan  orzu -havasning  yaxshi  niyatda  \namalga  oshirilayotganidan  ko’ra,  xalq orasida  “kim  o’zarga”  obro’  talashishga  aylanib  \nqolgan.  Natijada  so’ngi  kunlarda  milliy  to’y an’analarimizga  aloqasi  bo’lmagan,  ayrim  \nqo’shtirnoq  ichidagi  urf-odatlarga  mos bo’lish  uchun  daromadlariga  yarasha  emas,  balki  \nqarz  olib bo’lsada  , katta  sarf-xarajat  qilishmoqda.  Buning  oqibatida  to’ydan  keyin  kelib  \nchiqadigan  kelishmovchiliklar  yoshlarimizni  ajrashib  ketishlariga  ham  sabab  bo’lmoqda,  \nyoki oilani  bir necha  yillar  davomida  qiynalib  yashashga  majbur  qilmoqda.  Yuzaga  \nkelgan  vaziyatni e’tiborga  olib diniy  ulamolar, imom -xatiblar  aholi orasida  marosimlarda  \nisrofgarchilikka  yo’l qo’yib  bo’lmasligi,  islom  dinida  to’ylar  va ma’rakalarda  katta  sarf- \nxarajatlar  oqlanmasligi  haqida  tushuntirish  ishlarini  olib bormoqda.Yuqoridagi  mavjud  \nvaziyatni  inobatga  olib O’zbekiston  Respublikasining  parlamenti  2019  yil 14 sentabr  \nkuni  “To’y  hashamlar,  oilaviy  tantanalar,  ma’raka  va marosimlar,  marhumlarning  \nxotirasiga  bag’ishlangan  tadbirlar  o’tkazilishini  tartibga  solish  tizimini  takomillashtirish”  \nto’g’risidagi  qarorni  qabul  qildi.  Qarorda  har xil ko’ngilsizliklarning  oldini  olish  \nmaqsadida  to’y hashamlar  va boshqa  oilaviy  tadbirlarning  davomiylik  vaqti  hamda  \nchaqiriladigan  mehmonlar  soni aniq  belgilandi.  Shuningdek  qarorda  to’y \nmarosimlarining katta  xarajat talab  etadigan har  bir qismiga  to’xtalib,  asosiy  e’tibor  \nto’ylarning  ixchamligi  va kamxarajatli  qilib  o’tkazilishiga  qaratilgan.[5 -Oliy Majlis  qarori]  \nXalqimiz  azaldan  saxiy,  mehmondo’st  xalq.  Ammo  me’yordan  oshgan  har \nqanday  holat,  zararga  yo’liqtirishi  mumkinligini  hisobga  olib,  to’ylarning  ortiqcha  \ndabdabali  bo’layotganligi  va buning  oqibatida  yuzaga  kelayotgan  moddiy  qiyinchiliklar    «POLISH  SCIENCE  JOURNAL»   \n119  \n hamda  isrofgarchiliklarning  oldini  olsak  maqsadga  muvofiq bo’lardi.  \n \nFOYDALANILGAN  ADABIYOTLAR  \n1. Ashirov  A.A. O’zbek  xalqining  qadimiy  e’tiqod  va marosimlari.  –T., 2019.  \n2. Klyashtorniy  S.G. Qadimgi  turkey  runic  yodgorliklari  O’rta  Osiyo  tarixi  uchun  \nmanba  sifatida. -M., 2017  \n3. Mahmud  Qoshg’ariy Devonu  lug’atut  turk. -T., 2022 \n4. O’zbekiston  Respublikasining  parlamenti  2019  yil 14 sentabr   kuni  “To’y  \nhashamlar,  oilaviy  tantanalar,  ma’raka  va marosimlar,  marhumlarning  \nxotirasiga  bag’ishlangan  tadbirlar  o’tkazilishini  tartibga  solish  tizimini  \ntakomillashtirish”  to’g’risidagi  qarori.  \n5. Шаниязов  К. Узбеки -карлуки.  \n6. Дониёров  А.Х. Марказий  Осиё  халқлари  этнографияси,  этногенези  ва \nэтник тарихи фани  бўйича ўқув -услубий  қўлланма . \n7. Зунунова  Г.Ш.  Материальная  культура  узбеков  Ташкента:  трансформация  \nтрадиций. – Ташкент,  \n8. Кубаков  К. Тўй ва тўй маросимлати  ўтмишда  ва хозир.  авлоди,  2022. \n9. Hoji Muin. To’y va aza marosimi haqinda // Mehnatkash tovushi, 2019. 22 – \nmart.  \n10. Азимова  Н.Х. Система  традиционного  воспитания  детей  в узбекских  \nсельских  семьях  (на примере  Андижанской  области).  Автореф.  канд.  \nдисс. -.ист.  наук.  2017.   «POLISH  SCIENCE  JOURNAL»   \n120  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \nSECTION:  PHYSICS  AND  MATHEMATICS  \n \n \nGuleva  Dilrabo  Shamsiyevna  \nBuxoro  tumani  12-maktab  matematika  fani o`qituvchisi,  \nJumayeva  Farida  Mizrobovna  \nBuxoro  tumani 12 -maktab  matematika  fani o`qituvchisi  \n(Buxoro, O`zbekiston)  \nMATEMATIKA  DARSLARIDA  O`QUVCHILARNI  MASALA  YECHISHGA  O`RGATISH  \nRezyume . Maqolada  matematika  darslarida  o`quvchilarni  masala  yechishga  o`rgatish  \nhaqida  ma’lumot berilgan.  \nKalit so’zlar : o’qituvchi, o’quvchi, matematika, masala, son, arifmetik amal, yechim,  \nаrifmеtik  usul.  \n \nМаshqlаrdаn  fоydаlаnish  o`quvchilаrdа  dunyoqаrаshini  shаkllаntirishgа  хizmаt  qilib,  \nulаrgа  “sоn”,  “аrifmеtik  аmаl”,  kаbi аbstrаkt  tushunchаlаr  rеаl hаyotdаn,  аmаliy  fаоliyatdаn  \nоlingаnligigа ishоnchni mustаhkаmlаydi. Маshqlаrni yеchish jаrаyonidа o`quvchilаr tasavvurini  \nkеngаytiruvchi  fаktlаr  bilаn  tаnishаdilаr.  Bu bilаn ulаrning  fаrqlаsh  dоirаsi  kеngаyadi, hаmdа  \nmаshg`ulоt  bilаn  hаyot,  (аmаliyot)  o`rtаsidа  uzviy  аlоqа  o`rnаtilаdi.Маshqlаrni  yеchish  \no`quvchilаrning  аqliy  rivоjlаnishigа  kаttа  tа’sir  ko`rsаtib,  ulаrdа  tаhlil  etish,  tаqqоslаsh,  \numumlаshtirish vа аbstrаkt fаrqlаshgа ko`nikmаlаrni shаkllаntirаdi. Маshqlаrning tаrbiyaviy  \nаhаmiyati  hаm  bеqiyosdir.  \nТаyyorgаrlik dаvridаgi ishdаn mаqsаd – bоlаlаrgа rеаl hаyotdа yuz bеrаdigаn hоlаtlаrni  \nmаtеmаtik simvоllаr tiligа o`tkаzish imkоniyatini аnglаtishdаn ibоrаtdir. Bu hоlаtdа rаsmlаr  \nyordаmidа  mаsаlаlаr  tuzilishining zаrurаti  yo`q.  Yaхshisi  kichik  hikоya  shаklidа  bаyon  etilgаn  \nhоlаtni  bоlаlаr  mаtеmаtik  bеlgilаr  bilаn  dаftаrgа  yozib  оlish  imkоniyati  bo`lsin.  Hikоya  uchun  + \n= yoki – =. Sхеmаtik  shаkldаgi  yozuvlаr  ko`rsаtkich  (yo`llаnmа)  bo`lib  хizmаt  qilishi  mumkin.  \nSоddа vа murаkkаb mаsаlаlаr bоlаlаrning fikrlаsh qоbiliyatlаrini rеjаlаshtirishning  \nfоydаli vоsitаsi bo`lib, оdаtdа, o`z ichigа “yashirin infоrmаtsiyani” оlаdi. Bu infоrmаtsiyani  \nqidirish, mаsаlа yеchuvchidаn аnаliz vа sintеzgа mustаqil murоjааt qilish, fаktlаrni tаqqоslаsh,  \numumlаshtirish vа hоkаzоlаrni tаlаb qilаdi. Bilishning bu usullаrini o`rgаtish mаtеmаtikа  \no`qitishning muhim mаqsаdlаridаn biri hisоblаnаdi. Маsаlаlаr yеchish оrqаli o`quvchilаrdа  \nushbu  mаlаkаlаr  tаrkib  tоpmоg`i  lоzim.  1. Маsаlаni  tingl аshni  o`rgаnish  vа uni mustаqil  o`qiy  \nоlish. Маsаlа ustidа ishlаsh uning mаzmunini o`zlаshtirishdаn bоshlаnаdi. O`quvchilаr hаli  \no`qish  mаlаkаsigа  egа bo`lmаgаn  dаstlаbki  vаqtlаrdа  ulаrni  o`qituvchi  o`qib  bеrаdigаn  mаsаlа  \nmatnini  tinglаshgа,  shаrtning  muhim  elеmеntlаrini  tоvush  chiqаrib  аjrаtishgа  o`rgаtish  kеrаk.  \nShundаn kеyin mаsаlа shаrtini yaхshirоq o`zlаshtirish mаqsаdidа, hаr bir o`quvchi mаsаlа  \nmаtnini  tinglаbginа  qоlmаy,  bаlki  mаsаlаni  mustаqil  o`qib  chiqishi  zаrur;  Маsаlа  matni  \no`qituvchi yoki o`quvchilаr tоmоnidаn bir -ikki mаrtа o`qilаdi, аmmо bundа bоlаlаrni mаsаlа  \nmаtnini  bir mаrtа  o`qishdаyoq  uning  mаzmunini  tushunib  оlishgа аstа -sеkin  o`rgаtа  bоrish    «POLISH  SCIENCE  JOURNAL»   \n121  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \nkеrаk. 2. Маsаlаni dаstlаbki аnаliz qilish (mа’lumni nоmа’lumdаn аjаrаtа оlish mаlаkаsi).  \nМа’lumni  nоmа’lumdаn,  muhimni  nоmuhimdаn  аjrаtish,  mаsаlаdа  bеrilgаnlаr  bilаn  \nizlаnаyotgаnlаr оrаsidаgi bоg`lаnishni оchish - bu eng muhim mаlаkаlаrdаn biri. Bundаy  \nmаlаkаgа  egа bo`lmаy  turib,  mаsаlаlаrni  mustаqil  yеchishgа  o`rgаnib  bo`lmаydi.  3. Маsаlаni  \nqisqа yozish mаlаkаsi. Маsаlа  matni ustidа оg`zаki ishlаgаndаn kеyin uning mаzmunini  \nmаtеmаtik atamalаr tiligа o`tkаzish vа qisqа yozuv shаklidаgi mаtеmаtik strukturа sini bеlgilаsh  \nkеrаk  (rаsmlаr,  chizmаlаr,  sхеmаlаr, jаdvаllаr).  Shuni  nаzаrdа  tutish  kеrаkki, bаrchа  hоllаrdа  \nhаm qisqа yozuvni bаjаrish bilаn bir vаqtdа mаsаlа shаrtining tаhlii hаm аmаlgа оshirilаdi.  \nAslini аytgаndа, qisqа yozuvning vаzifаsi shundаn ibоrаt. Hаqiqаtаn hаm mаsаlа shаrtining  \nqisqа yozuvi o`quvchilаr хоtirаsigа tаyanch bo`lib, sоn mа’lumоtlаrni tushunish vа аjrаtish  \nimkоnini  bеrаdi,  shu bilаn  birgа  ulаrning  rаtsiоnаl  yozilishi  mаsаlаdа  nimа  bеrilgаn  vа nimаni  \nizlаsh  kеrаkligini  bаyoniy  tushuntirish  imkоnini  yarаtаdi.  4. Sоddа  mаsаlаlаrni  yеchishdа  аmаl  \ntаnlаshni  аsоslаb  bеrish  vа murаkkаb  mаsаlа  tаhlilini  аmаlgа  оshirish,  so`ngrа  yеchish  rеjаsini  \ntuzish mаlаkаsi. Оldin sоddа mаs аlаni yеchishdа аmаl tаnlаsh mаsаlаsini qаrаb chiqishgа  \nto`хtаlаmiz. Bu mаlаkа birinchi sinfdаn bоshlаb tаrkib tоpа bоshlаydi, ikkinchi vа uchinchi  \no`quv yillаridа yanаdа rivоj tоptirilаdi, ya’ni bа’zi tаnish mаsаlаlаrgа nisbаtаn аmаl tаnlаsh  \nishini bаjаrish аsоsi o`zgаrtirilаdi. 5. Yechimni bаjаrish, uni o`qituvchi tаlаbigа mоs qilib  \nrаsmiylаshtirish vа mаsаlа sаvоligа jаvоb bеrish mаlаkаsi. Sоddа mаsаlаlаrdаn bоshlаymiz.  \nSоddа mаsаlаni аrifmеtik usul bilаn hаm, аlgеbrаik usul bilаn hаm yеchish mumkin. Bu o`rindа  \nmаsаlаlаrni аrifmеtik usul bilаn yеchish hаqidаginа so`z bоrаdi, mаsаlаni аlgеbrаik usuldа  \nyеchish  kеyinrоq  аlоhidа  qаrаlаdi.  6. Маsаlа  yechimini  tеkshirа  оlish  mаlаkаsi.  Маsаlа  \nyechimining tеkshirish quyidаgi usullаrdа qo`llаnilаdi: а) oling аn jаvоb bilаn mаsаlа shаrti  \no`rtаsidа mоslik o`rnаtish; b) tеskаri mаsаlа tuzish vа yеchish; v) mаsаlаni bоshqа usullаr bilаn  \nyеchish;  g) jаvоbning  chеgаrаlаrini  аniqlаsh  (jаvоbni  chаmаlаsh);  d) grаfik  tеkshirish.  \n7. Маsаlаlаr  ustidа  ishlаshdа  mа’lum  sistеmаni  bеlgilаsh  vа uni jоriy  qilish  mаlаkаsi.  \nМаsаlаlаr ustidа ishlаsh rеjаsi 1. Маsаlаni o`qib chiqing, mаsаlаdа nimа hаqidа gаp  \nbоrаyotgаnini o`zing tаsаvvur qiling 2. Маsаlаdа nimа mа’lum vа nimаni tоpish kеrаkligini  \nаniqlаb  оling.  Agаr  mаsаlа  matnini  tushunib  оlish  qiyin  bo`lsа,  uni qisqа  yozing  (yoki  mаsаlаgа  \nоid chizmа tаyyorlаng). 3. Qisqа yozuv bo`yichа hаr bir sоn nimаni ko`rsаtishini tushuntiring vа \nmаsаlа sаvоlini tаkrоrlаng 4. O`ylаb ko`ring, mаsаlа sаvоligа birdаnigа jаvоb bеrish mumkinmi,  \nаgаr mumkin bo`lmаsа, nеgа? Оldin nimаni, kеyin nimаni bilish mumkin? Маsаlаni yеchish  \nrеjаsini tuzing. 5. Yechishni bаjаring vа jаvоbini yozing. 6. O`z yechimingizning to`g`riligini  \ntеkshirib  ko`ring.  7. O`zingizgа  qiziqаrli  sаvоllаr  bеring  vа ulаrgа  jаvоb  bеring.  \nO`quvchilаrni mustаqil mаsаlаlаr yеchishgа o`rgаtishning bir qаnchа bоsqichini аjrаtib  \nko`rsаtish  mumkin:  1-bоsqich.  Маsаlа  o`qituvchining  yo`nаltiruvchi  sаvоllаri  bo`yichа  yеchilаdi vа  \nbu yеchish dоskаdа vа dаftаrlаrdа bir vаqtdа bаjаrilаdi. 2 -bоsqich. Маsаlа shаrti o`qituvchi  \nrаhbаrligidа tаhlil qilinаdi vа yеchish rеjаsi tuzilаdi. Yechishning o`zi dоskаgа yozilmаydi,  \nоg`zаki  аytilmаydi  hаm,  o`quvchilаr  esа uni mustаqil  bаjаrаdilаr.  3-bоsqich.  O`qituvchi  \nrаhbаrligidа mаsаlа fаqаt аnаliz qilinаdi. Yechish rejasi vа yеchishning o`zini o`quvchilаr  \nmustаqil  bаjаrishаdi.  4-bоsqich.  Маsаlаni  o`qituvchining  hеch  bir yordаmisiz  mustаqil  yеchish.  \nO`quvchilаrdа mаsаlаlаr yеchish mаlаkаsini tаrkib tоptirishdа ijоdiy хаrаktеrdаgi mаshqlаrning  \nhаm  muhim  аhаmiyati  bоr. Bungа  quyidаgilаr kirаdi:  1. Маsаlаlаrni  hаr хil usullаr  bilаn  yеchish.    «POLISH  SCIENCE  JOURNAL»   \n122  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \n2. Мuаmmоli хаrаktеrdаgi mаsаlаlаrni yеchish. 3. Маsаlаlаr tuzish vа ulаrni аlmаshtirishgа dоir  \ntоpshiriqlаr.  \nMаtеmаtik mаsаlа ustidа ishlаsh jаrаyonidа shungа intilish kеrаkki, hаr bir mаsаlа  \nbоlаlаr  uchun  hаqiqiy  bilim  mаnbаi  bo`lib  qоlsin.  Buning  uchun  o`quvchining  diqqаtini  mаsаlа  \nshаrtidаn  tаfаkkurini  vа bilish  qоbiliyatlаrini  rivоjlаntirаdigаn  dаrаjаdа  ko`prоq  mа’lumоtlаrni  \nоlishgа  yo`nаltirish  kеrаk.  \n \nFOYDALANILGAN  ADABIYOTLAR:  \n1. Jumaеv  M.E,  Bolalarda  matematik  tushunchalarni  rivojlantirish  nazariyasi  va metodikasi.  \n(KHK  uchun) Toshkеnt. “Ilm Ziyo”  2005 yil.  \n2. Имомова Ш.М., Исмоилова М.Н. Вычисление наибольшего собственного значения  \nматрицы  и соответствующего  ей собственного  вектора  в среде  Mathcad // ACADEMY . \n№ 6(57),  2020.  C. 9.   «POLISH  SCIENCE  JOURNAL»   \n123  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \nSaparov  Zaripbay  Begdullaevich  \nkatta o‘qituvchi,  \nKalbaev  Sultanbek  Nazerbaevich  \nstajer o‘qituvchi,  \nIbragimov  Aybek  Uzaqbaevich  \nmagistrant  \nAjiniyoz  nomidagi  Nukus  davlat  pedagogik  institute  \n(Nukus,  Uzbekistan)  \n \nFUNKSIYALIK TENGLAMALARNI YECHISHDA FUNKSIYANING XOSSALARIDAN  \nFOYDALANISH  \n \nRezyume. Ushbu maqolada talabalarning kompleks analiz faniga qiziqishini oshirish  \nuchun misollar va topshiriqlar berilgan. Bu misol va topshiriqlar ularning ko‘nikma va bilimlarini  \nrivojlantirishda  katta  yordam  beradi.  \nKalit  so’zlar:  inektivlik,  soha,  o’suvchi,  bir qiymatli.  \n \nАннотация. В этой статье приведены примеры и задачи для студентов,  \nповышающие интерес к комплексному анализу. Эти примеры и задачи имеют большое  \nзначение  в развитие  их навыков  и знаний.  \nКлючевые  слова:  инектив,  область,  рост,  однозначный.  \n \nSummary: This article provides examples and tasks for students to increase interest  \nin complex analysis. These examples and tasks go a long way in developing their skills and  \nknowledge.  \nKey words:  Injectivity,  sphere,  growing,  monosemantic.  \n \nHozirgi kunda funksional tenglamalar maktab matematika kursida 11 – sinf darsliklarda,  \nolimpiyada masalalarida va yuqori o’quv o’rinlarida matematika bo’yicha test sinovlarida  \nbelgisizlari son emas, funkciyalar bo’lgan tenglamalar, tengsizliklar uchirashib tur. Bunday  \ntenglamalar funkciyalik tenglama deb ataladı. Bu masalalar forması bo’yicha ham, yechish  \nusullari bo’yicha ham farqli. Bunday tenglamalar bilan bir qatorda funksional tenglamalarni  \nyechishni talab etmaydigan bazi masalalarga umumiy jihattan va  funkciyalik tenglamalar  \nnazariyasining uncha murakkab bo’lmagan tushunchalaridan, usullaridan foydalanish juda  \ntabiiy  bo’ladigan  hollarda  yechim  olish,  bunday  har xil masalalar  orasidagi  bog’liqliklarni  \nko'rsatish,  o'quvchining  matematik  madaniyatini  ko'tarish  mumkinligin  beradi.  \n1-masala.  f (x) funksiya   x  R uchun  \n \n \ntenglikni  qanoatlandiradi.  x + f (x) = f ( f (x)) \nf ( f (x)) = 0 tenglamani  yeching.  (1) \n \nYechish.  Masalani  yechish  uchun  inektivlik  funksiya  tushunchasin  kiritamiz.    «POLISH  SCIENCE  JOURNAL»   \n124  \n \n− SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \nTarif.  Agar  f (x) funksiyaning  aniqlanish  sohasinan  olingan   x1 , x2 nuqtalar  uchun  \nf (x1) = f (x2 )   tenglikdan    x1 = x2     tenglik  kelib  chiqsa,  unda bunday  funksiya  inektivlik  \nfunksiya  deb ataladi.  \nO’z aniqlanish sohasinda qat’iy o’suvchi (kamayuvchi) funksiya inektivlik funksiya  \nbo’ladı. Funksiyalarning monotonliginan foydalanıladigan ko’pchilik masalalarda ko’pincha  \nfunksiyaning inektivligi kerak bo’ladı. Inektivlik funksiya termini bilan bir qatorda ineksiya va \no’z- aro bir qiymatli  moslik terminlari  qo’llaniladi.  \nf (x1 ) = f (x2 )  x1 = x2 nisbat  x1  x2  f (x1)  f (x2 ) ( x1, x2 nuqtalar  \nfunksiyaning aniqlanish sohasinan olingan)  nisbatga  teng kuchli.  Shunday  ekan ineksiyani  \nelementlarini  «yelimlaydigan»  akslantirish  sifatida  aniqlash  mumkin.  \nEndi  masalamizga qaytamız  va masalada  so’z aytilgan  funksiya  ineksiya  bo’ladiganligini  \nisbotlaymiz.   Haqiyqattan   ham,   agar  \nx = f ( f (x)) − f (x) ko’rinishda  yozib olib,  f (x1) = f (x2 ) bo’lsa,  unda  (15)  tenglamani  \n \nx1 = f ( f (x1)) − f (x1) = f ( f (x2 )) − f (x2 ) = x2 \ntenglikni  olamız.  \nEndi   f ( f (x)) = 0 tenglama  bilan  shug’ullanamız.  Dastlab  bu tenglama  necha  \nyechimga ega bo’lishi mumkin ekaninligini anıqlaymız. Agar x1 , x2 nuqtalar bu tenglamaning  \nyechimlari bo’lsa,  f ( f (x1 )) = f ( f (x2 )) va f (x) funksiyaning inektivligidan  f (x1 ) = f (x2 ) \nbo’lib,  inektivlik  xossasidan  yana  bir marta  foydalanib,  x1 = x2 tenglikni  olamız.  Shunday  etib,  \nf ( f (x)) = 0 tenglama  bir yechimdan  ortıq  yechimga  ega emas.  \nEndi  berilgan  funksiyaviy  tenglamada  x o’rniga  0 sonin  qo’yamiz  va f (0) = f ( f (0)) \ntenglikni  olamiz.  f (x) funksiyaning  inektivligidan  0 = f (0) bo’lib,  f ( f (0)) = f (0) = 0 tenglikga  \nega bo’lamiz,  ya’niy  0 soni f ( f (x)) = 0 tenglamaning  yechimi  bo’ladi.  \nJavob:  x = 0. \n1 – masalaning  tekstidan  (1) funksiyaviy  tenglamani  qanoatlandiradigan  \nfunksiyalarning bor ekani malum bo’lsa ham, bunday kamida bir funksiyani topish qiziq masala.  \nUmumiy  ko’rinishda bu tenglamani  yechish  qiyin,  degan  bilan,  yuqori  ko’rsatilgan  usullar  bilan,  \nf (x) = 1 +      x f (x) = 1 5 x \nmisol  uchun,  ko’phadlar  sinfida  bu tenglama  ikki 1 \nyechimlarga  ega bo’ladiganligin  isbotlash  mumkin.  2 va 2 2   «POLISH  SCIENCE  JOURNAL»   \n125  \n Haqiyqattan  ham,  agar  f (x) − n  1 darajali  ko’phad  bo’lsa,  unda  (1) tenglikning  chap    «POLISH  SCIENCE  JOURNAL»   \n126  \n tamonida n −darajali, o’ng tamonida  n2 darajali ko’phad turadi. Bu faqat  n = 1 bo’lsagina  \no’rinli.  Shunday  ekan  biz f (x) = kx + b chiziqli  funksiyalarni  qarash  bilan  chegaralanishimiz  \nmumkin.  Bu funksiyalar  uchun  (1) tenglama   x uchun  x + (kx + b) − k(kx + b) + b , ya’niy  \n \n(1 + k)x + b − k1x + (kb + b) ko’rinishda  yoziladi. Bu  nisbat    «POLISH  SCIENCE  JOURNAL»   \n127  \n \n1 −   5 \n2   \n( ) SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \nk + 1 = k 2 \n \nb = kb + b \nsistemaga  teng  kuchli.  Bu sistema  ikki yechimga ega:  \n(k; b) = \n( ; 0), (k; b) = ( ; 0). \n2 2 \n \nUlarga  (1) funksiyaviy  tenglamani  qanoatlandiradigan  \nf (x) = 1 +   5 x, f (x) = x \n1 2 2 2 \nfunksiyalar  mos keladi.   \n \n(2) \n \n(1) tenglamaning  yechimlari  ushbu  ikki funksiya  bilan  cheklanmaydi.  Masalan,  \n1 +   5 ,    x = a + b (a,b  Q), \n \nf (x) =   \n1 + 5 \n ,   qal g an ja g dayda  \n 2 \nfunksiya  ham  bu tenglamani  qanoatlantiradi.  \nQaralgan  masala  berilgan  funksiyaviy  tenglamani  yechish  va f (x) funksiyani  \n \nf ( f ( x)) = 0 \noshkormas  ko’rinishda  yozish  mumkin  bo’lmasa  ham,  asosiy  masalani  tenglamani  \nyechish)  to’liq  yechish  mumkinligi  bilan  qiziqarli.  \n \nADABIYOTLAR:  \n1. В.П. Супрун   «Математика   для   старшеклассников»   Москва   Книжный   дом  \n«ЛИБРОКОМ»  2008.  \n2. С.Н. Олехник,  М.К.  Потапов,  П.И.  Пасиченко  «Уравнение  и неравенства,  \nнестандартные  методы решение»  М. Наука  1987    «POLISH  SCIENCE  JOURNAL»   \n128  \n SCIENCECENTRUM .PL ISSUE  2(47)  ISBN  978-83-949403 -4-8 \n \nБекмуродов  Беҳзод  Бахтиёр  ўғли  \nЎзбекистон  Миллий  университети  2-курс  математика  \n(йўналишлар бўйича) магистранти,  \nҚаюмов  Яшнар  Зокир  Ўғли  \nЎзбекистон  Миллий  университети  2-курс  математика  \n(йўналишлар  бўйича)  магистранти  \n(Ташкент, Узбекистан)  \nГАРМОНИК  ФУНКЦИЯНИНГ  ХОССАЛАРИ  \nРезюме . Мақолада  Грин  формулалардан  ва гармоник  функциянинг  интеграл  \nтасвиридан  унинг  хоссалари  келтириб  чиқарилган.  \nКалит  сўзлар : Гармоник  фунциялар,  теорема,  Лаплас  тенгламаси,  Грин  \nформуласи,  узлуксиз функция.  \n \nГармоник фунциялар - Лаплас тенгламасини қаноатлантирадиган  бирор соҳада  \nбиринчи  ва иккинчи  тартибли  ҳосилалари  билан узлуксиз  бўлган  ҳақиқий  фунциялардир.  \nГрин  формулалардан  ва гармоник  функциянинг  интеграл  тасвиридан,  унинг  \nоддий  хоссаларини  келтириб чиқарамиз:  \nТеорема 1. Агар D соҳада гармоник 𝑢(𝑥) функция учун, 𝑢(𝑥) ∈ 𝐶1(𝐷 𝖴 𝜎) бўлса,  \nу ҳолда  \n∫ 𝑑 𝜎   = 0 (1) 𝜕𝑢(𝑦) \n \n𝜎    𝜕𝑣 𝑦 \nбўлади.  \n \n \nкўра  Исбот.  Гриннинг  биринчи  формуласида  фойдаланамиз,  чунки  теорема  шартига  \n \n𝑢 ∈ 𝐶2(𝐷) ∩ 𝐶1(𝐷 𝖴 𝜎), \nшунинг  учун  1 – Грин  формуласи  \n𝜕𝑢 𝑛 \n𝜕𝖯 𝜕𝑢 \n∫ 𝖯 𝜕𝑣 𝑑 𝜎 = ∫ ∑ 𝜕𝑦    𝜕𝑦  𝑑𝑦 + ∫ 𝖯𝛥𝑢𝑑𝑦  \n𝜎 𝐷 𝑘=1 𝑘 𝑘 𝐷 \nўринли.  Бу ерда  𝖯 ≡ 1 деб оламиз,  у ҳолда  \n𝜕𝑢 𝑑 𝜎 = 0 + 0 = 0. \n𝜎 𝜕𝑣 \nТеорема  2. Агар  𝑢(𝑥) функция  D соҳада  гармоник  бўлса,  у ҳолда  u(x) чексиз  \nдифференциалланувчидир,  яъни  \n𝑢(𝑥) ∈ 𝐶∞(𝐷) \nбўлади.  \nИсбот.  𝑢(𝑥) функция  D соҳада  гармоник  бўлсин.  У ҳолда  ўзининг  чегараси  билан  \nтўла D соҳада ётувчи 𝐷1 соҳани оламиз. 𝐷1 соҳани шундай танлаб оламизки унинг  \nчегараси 𝜕𝐷 1 = 𝜎1 - бўлаклари  силлиқ  сиртдан  иборат  бўлсин.  ∫   «POLISH  SCIENCE  JOURNAL»   \n129  \n 𝑢(𝑥) ∈ 𝐶2(𝐷1) бўлган и1учун  (7) гармон и𝜕𝑢к(ф𝑦)ункция уч𝜕𝜀ун(𝑥и,𝑦н)теграл тасвирдан \n𝑢(𝑥) = ∫ (𝜀 (𝑥, 𝑦) − 𝑢(𝑦)    𝑛 ) 𝑑𝜎 (2) \n \n \n(𝑛−2)𝜔𝑛   𝜎 𝑛 \nфойдаланамиз.  Бу ерда   \n \n𝜕𝑣 𝜕𝑣 1   «POLISH  SCIENCE  JOURNAL»   \n130  \n 1 \n1 SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n1 \n𝜀𝑛(𝑥, 𝑦) = |𝑥 − 𝑦|𝑛−2 \n(2) интеграл  остидаги  функция  х ва у ўзгарувчиларнинг  узлуксиз  функция  бўлиб  \n𝑥 = (𝑥1, 𝑥2, . . . , 𝑥𝑛) нуқтанинг  барча  𝑥𝑗 – координаталари  бўйича  барча  тартибли  \nҳосилаларга  эга. Параметрга  боғлик  интегралларни  дифференциаллаш  ҳақидаги  \nтеоремега  асосан 𝑢(𝑥) функция  х – бўйича  барча  тартибли ҳосилаларга  эга. \nТеорема  3. (Ўрта  қиймат  ҳақида ) \nАгар  𝑢(𝑥) функция  𝐾(𝑥, 𝑅) – шарда  гармоник  бўлиб,  𝐾(𝑥, 𝑅) – ёпиқ  шарда  \nузлуксиз  бўлса, у ҳолда  1 \n𝑢(𝑥) = ∫ 𝑢(𝑦)𝑑𝜎 (3) \n𝜔𝑛𝑅𝑛−1     |𝑦−𝑥|=𝜀 𝑦 \nформула  ўринли  бўлади.  \nИсбот.  Ушбу  \n𝐾1(𝑥1, 𝑅1): |𝑦 − 𝑥| ≤ 𝑅1 < 𝑅 \nшарни  караймиз.  Танлашимизга  кўра  𝐾1 ⊂ 𝐾 ва 𝑢(𝑦) ∈ 𝐶2(𝐾1) бўлгани  учун,  \nгармоник фун кция1га интеграл тасвир ёза𝜕м𝑢и(𝑦з). 𝐾1  – шар𝜕𝜀у(ч𝑥у,𝑦н) \n𝑢(𝑥) = ∫ (𝜀 (𝑥, 𝑦) − 𝑢(𝑦)    𝑛 ) 𝑑𝜎   = 𝜎 : |𝑦 − 𝑥| = 𝑅 \n \n  \n(𝑛−2)𝜔𝑛   |𝑦−𝑥|=𝑅1        𝑛 𝜕𝑣 \nсферада  \n1  \n \n𝜕𝑣 𝑦 1 1 \n \n𝑙𝑛 𝑅 , 𝑛 = 2 \n1 \n1 \n𝜀𝑛(𝑥, 𝑦)|𝑦∈𝜎1    =    𝑅 , 𝑛 = 3 = 𝑐𝑜𝑛𝑠𝑡  \n1 1 \n{ 𝑅𝑛−2 , 𝑛 > 2 \n \n𝜎1 сферага ташки 𝜈 – нормалнинг йуналиши 𝑅1 – радиус йуналиши билан бир ҳил  \nбўлади.  Шунинг учун  1 − , 𝑛 = 2 \n𝜕𝜀 (𝑥,𝑦) 𝜕𝜀 (𝑥,𝑦) − 1 , 𝑛𝑅1= 3 = 𝑐𝑜𝑛𝑠𝑡  \n𝑛 𝑛 \n \n \n| = | = 𝜕𝑅  \n \n𝑅2 . \n𝜕𝑣 𝑦∈𝜎1 1 𝑦∈𝜎1 1      𝑛−2 \n− , 𝑛 > 2 \n{ 𝑅𝑛−1 \n \nБуларни  эътиборга  олиб  юқоридаги  интегрални  қуйидагича  ҳисоблаймиз : \n= 1 (𝑛 − 2)𝜔 𝑅𝑛−2 \n∫   «POLISH  SCIENCE  JOURNAL»   \n131  \n  \n𝜕𝑢(𝑦) (𝑛 − \n2) 𝜕𝑣  𝑑𝜎 𝑦 + (𝑛 − 2)𝜔 𝑅𝑛−1 ∫ 𝑢(𝑦)𝑑𝜎 𝑦 = \n𝑛  1 \n1 |𝑦−𝑥|=𝑅1 𝑛    1 |𝑦−𝑥|=𝑅1   «POLISH  SCIENCE  JOURNAL»   \n132  \n 1 𝜔 𝑅 \n𝜔 𝑅 = 𝑛−1 ∫ 𝑢(𝑦)𝑑𝜎 𝑦 ; \n𝑛    1 |𝑦−𝑥|=𝑅1 \nДемак, 𝑢(𝑥) = 1 ∫ 𝑢(𝑦)𝑑𝜎  (3′) \n𝜔𝑛𝑅𝑛−1     |𝑦−𝑥|=𝑅1 𝑦 \n𝑢(𝑥) функция  𝐾(𝑥, 𝑅) – шарда  узлуксиз  бўлгани  учун  бу тенгликда  𝑅1 → 𝑅 да \nинтеграл  остида лимитга  ўтиш  мумкин.  \n1 \n𝑢(𝑥) = 𝑛−1 ∫ 𝑢(𝑦)𝑑𝜎 𝑦 \n𝑛 |𝑦−𝑥|=𝑅   «POLISH  SCIENCE  JOURNAL»   \n133  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \nформулани  ҳосил  қиламиз . \n𝜔 𝑅𝑛−1 = { 2𝜋𝑅, агар 𝑛 = 2 (3′) \n𝑛 \nформулани  4𝜋𝑅2, агар 𝑛 = 3 \n1 \n𝑅𝑛−1𝑢(𝑥) = ∫ 𝑢(𝑦)𝑑𝜎 \n 1 𝜔𝑛 𝑦 \n|𝑦−𝑥|=𝑅 \nкўринишда   ёзиб   оламиз.   Бу   тенгликни   𝑅1    бўйича  0 ≤ 𝑅1 ≤ 𝑅 оралиқда  \nинтеграллаб,  𝑛 𝑢(𝑥) = ∫ 𝑢(𝑦)𝑑 𝑦 (3″) \n𝜔𝑛𝑅𝑛 𝐾(𝑥,𝑅) \n \n𝑛 \nформулага  эга бўламиз.  Бу ерда  𝜔𝑛𝑅  − 𝐾(𝑥, 𝑅) шарнинг  ҳажми . (3) ва (3″) \n𝑛 \nформулалар  мос равишда  сфера  ва шар  бўйича  гармоник  функциялар  учун  ўрта  \nарифметик формулалар  номи билан  юритилади.  \n \nНатижа  1. Агар  𝑢(𝑥) функция  D соҳада  гармоник  𝐷 = 𝐷 𝖴 𝜎 да узлуксиз  бўлиб  ва \n𝑢(𝑥)|𝑥∈𝜎0 = 0 \nбўлса,  у ҳолда  \n \nбўлади.   \n𝑢(𝑥) ≡ 0, ∀𝑥 ∈ 𝐷 \n \n \nНатижа  2. Агар  𝑢1(𝑥), 𝑢2(𝑥) функциялар  D соҳада  гармоник,  𝐷 = 𝐷 𝖴 𝜎 да \nузлуксиз  бўлиб  \n \nбўлса, у ҳолда  \nбўлади.  (𝑢1(𝑥) − 𝑢2(𝑥))| 𝑥∈𝜎 ≤ 0 \n \n𝑢1(𝑥) − 𝑢2(𝑥) ≤ 0, ∀𝑥 ∈ 𝐷 \nНатижа  3. Агар  𝑢1(𝑥), 𝑢2(𝑥) функциялар  D да гармоник,  𝐷 да узлуксиз  бўлиб,  \n𝑢1(𝑥)|𝜎 ≤ 𝑢2(𝑥)|𝜎 \nбўлса, у ҳолда  \nбўлади.   \n𝑢2(𝑥)|𝜎 ≤ 𝑢2(𝑥), ∀𝑥 ∈ 𝐷 \n \nСПИСОК  ИСПОЛЬЗОВАННЫХ  ИСТОЧНИКОВ:  \n1. Салоҳитдинов  М.С.,  Математик  физика  тенгламалари,  Т., «Ўзбекистон»,  2002.  \n2. Исмоилова  М.Н.,  Имомова  Ш.М.  Интерполяция  функции//  ВЕСТНИК  НАУКИ  И \nОБРАЗОВАНИЯ  2020.  №3(81).  Часть  3. С5.    «POLISH  SCIENCE  JOURNAL»   \n134  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \nSECTION:  TECHNICAL  SCIENCE.  \nTRANSPORT  \n \n \nБоллиев  К., Хошимов  А. \nПреподавательи  Каршинского  государственного  университета  \n(Карши, Узбекистан)  \n \nПРОЦЕССЫ ПОЛУЧЕНИЯ КАЧЕСТВЕННОЙ СТРУКТУРЫ СПЛАВА  \nДЛЯ ИЗГОТОВЛЕНИЯ ДЕТАЛЕЙ ФОРМОВЫХ КОМПЛЕКТОВ  \nСТЕКЛОФОРМУЮЩЕЙ  МАШИНЫ  \n \nВ данной статье рассматривается вопрос изготовления форм для стеклянных  \nизделей, изготовление стеклянных изделей и технологический процесс обработки  \nстеклянных  изделей  на токарных  станках  \n \nВысокий рост потребительской активности населения сформировал массовый  \nспрос  на стеклянные  изделия  различного  назначения.  Основными  потребителями  \nстеклянных изделий являются химическая, пищевая, фармацевтическая и косметическая  \nпромышленности.  Немалый  интерес  описанные  выше  индустрии  проявляют  к \nтранспортировочной  таре  и упаковочным  изделиям,  изготовленным  из стекла.  \nПопулярность полых стеклянных изделий в повседневной жизни и промышленности  \nсвязана с относительно невысокой их стоимостью, повышенной прочностью, а для  \nпроизводителей  стеклянных  изделий  - со способностью  получения  практически  любой  \nконфигурации,  возможностью  повторного  применения,  то есть  рециклингом  сырья.  \nДля изготовления полых стеклянных изделий применяют металлические литые  \nформовые  комплекты.  Формовой  комплект  представляет  собой  совокупность  \nформообразующих элементов: поддон, стеклоформы (пресс -формы) и горловые кольца,  \nкак правило,  имеющие  особый  класс  точности  и высокие  показатели  надежности  \nэксплуатации.  Однако  при изготовлении  фасонных  стеклянных  изделий  крупными  \nпартиями  стеклотарные  заводы -производители  сталкиваются  с проблемой,  связанной  с \nпостоянным  выходом  из строя  ответственных  узлов  формовых  комплектов.  \nГлавная проблема изготовления литых заготовок стеклоформ заключается в  \nполучении материала, отвечающего задачам функционирования готовых деталей, а  \nименно в обеспечении высокой термостойкости контактных с расплавленным стеклом  \nповерхностей,  в повышенной  теплопроводности  формового  комплекта,  высокой  \nпрочности.  \nПроблема  повышения  эксплуатационного  ресурса  деталей  стеклоформ  связана  с \nодной из особенностей их работы - цикличностью процесса во времени с использованием  \nпринципа  двойного  выдувания  стеклянного  изделия  и, соответственно,  с применением  \nдвух  стеклоформующих  комплектов  различной  конфигурации  (черновой  и чистовой    «POLISH  SCIENCE  JOURNAL»   \n135  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \nметаллических  форм).  Помимо  этого,  значительное  влияние  на стойкость  \nстеклоформующей оснастки оказывает стремление производителей тарного стекла  \nуменьшить массу выпускаемых стеклянных изделий (сокращение толщины стенки),  \nповысить  скорость  выпуска  тары,  при одновременном  улучшении  показателей  качества  \nвыплавляемого  стекла.  \nВследствие этого материал деталей стеклоформ должен обладать рядом свойств,  \nпозволяющих эксплуатировать его в бесперебойном режиме в течение длительного  \nпериода: иметь высокую прочность для противостояния ударных нагрузок со стороны  \nкак расплавленного стекла, так и других узлов формовых комплектов, противостоять  \nтермической  усталости  в зонах  циклических  термических  ударов,  хорошо  \nобрабатываться  резанием  в процессе  механической  обработки  заготовок,  \nпрепятствовать элементной диффузии, иметь минимальные структурные превращения в  \nпроцессе  накопления  количества  теплосмен,  а также  обладать  высокой  \nтеплопроводностью  для отвода  излишек  тепловой  энергии  с внутренних  поверхностей.  \nОбеспечить  комплекс  таких  свойств  детали  требуется  уже на этапе  изготовления  литой  \nзаготовки стеклоформы.  \nСтекло является наиболее перспективным и востребованным материалом для  \nизготовления  изделий,  применяемых  в качестве  расфасовочной,  транспортировочной  и \nупаковочной  тары  в различных  отраслях  отечественной  промышленности.  Наибольшую  \nпопулярность стеклянные изделия получили в пищевой, химической, автомобильной,  \nфармацевтической  и косметической  индустриях.  \nРаспространенность данного материала во многих промышленных секторах  \nэкономики связана с тем, что стекло является аморфным изотропным материалом,  \nполучаемым  путем  переохлаждения  стеклорасплава  и, благодаря  этому,  обладающим  \nрядом  уникальных  свойств  по сравнению  со своими  аналогами  (пластмасса,  \nстеклопластик  и т.п.):  \n- высокой  герметичностью  и, как следствие,  газонепроницаемостью  и химической  \n- защищенностью  от агрессивных газовых  сред;  \n- низкой  химической  активностью  к содержимому  стеклянного  изделия;  \n- отсутствием  подверженности  временному  износу  (например,  коррозии);  \n- регулируемостью  светопроницаемости;  \n- высокой  регенерируемостью  материала  (до 40% для тарного  стекла  [1]); \n- экологической  безопасностью  утилизации.  \nОднако существует и ряд недостатков стекла при рассмотрении его с точки зрения  \nупаковочного материала:  \n- низкая  механическая  прочность  (при  ударных  нагрузках);  \n- большой удельный  вес на единицу  содержимого  стеклянного изделия.  \nВопреки  такому  сочетанию  разнородных  показателей  - набору  уникальных  \nсвойств  - тарное  стекло  остается  одним  из самых  востребованных  упаковочных  \nматериалов  и его производство  неуклонно  растет  из года  в год. Во многом  это \nобусловлено спецификой изготовления данного материала, где ключевым моментом  \nявляется возможность формирования стеклянного изделия сложной конфигурации,  \nширокого  диапазона  габаритных  размеров  с варьируемой  толщиной  стенки.  Такими    «POLISH  SCIENCE  JOURNAL»   \nSCIENCECENTRUM.PL  ISSUE  2(47)  ISBN  978-83-949403 -4-8 \n136  \n  \nпреимуществами формования стекло обеспечено, согласно теории агрегатных состояний  \nпроцесса  кристаллизации,  за счет  его экзотермической  кристаллизации.  Стекло  \nхарактеризуется  способностью  к перекристаллизации  - \"силе  самопроизвольной  \nкристаллизации\", согласно и к высокой скорости образования кристаллов и их росту.  \nВследствие  этого  значительное  понижение  текучести  расплавленного  стекла  при \nпонижении  температуры  (затвердевании)  способно  приводить  к фиксации  \nнеравновесной  высокотемпературной  структуры  жидкого  стекла.  Дальнейшее  \nохлаждение  такой  замороженной  структуры  приводит  к закреплению  структуры  \nжидкости,  приобретённой  в состоянии  высокой  вязкости,  что вызывает  аморфное  \nсостояние  материала.  При изготовлении  же стеклянной  тары  все эти процессы  \nпроисходят по причине того, что при высокой вязкости расплава - порядка 10 пуаз - \nструктура  стекла  не успевает  следовать  за изменением  температуры  и в результате  этих  \nпроцессов  либо  реализуется,  либо  не реализуется  способность  стекла  к формированию  \nполого изделия. Таким образом, основным свойством стеклообразующего расплава  \nявляется его вязкость, которая зависит в первую очередь от температуры разогрева  \nрасплава  материала.  \nИменно характер изменения вязкости стекла при снижении его температуры  \nслужит  основой  для определения  температурных  режимов  варки,  формования  и \nтермической  обработки.  Из рисунка  1.1 следует,  что в температурном  интервале  23.1500  \n°С вязкость стекол изменяется на 18 порядков (при этом в твердом состоянии вязкость  \nсоставляет примерно 1019 Па-с, в расплавленном состоянии - 10 Па -с), то есть имеет  \nзначительный диапазон варьирования. На этом основании наибольший интерес для  \nизучения представляют процессы формования расплава стекла и его стеклования, а  \nзависимость  вязкости  от температуры  в этих  условиях  можно  представить  в виде  \nуравнения  Евстропьева  К.С. [2]:  \nП = AeE0/T 2, (1.1)  \nгде А - константа, Eo - величина пропорциональная прочности связки элемент - \nкислород, Т - температура, °К.  \nИз уравнения 1.1 и рисунка 1.1, становится очевидным, что существует такой \"  \nрабочий интервал\" вязкости, в пределах которого происходит размягчение стекломассы  \nдо состояния способности принять какую -либо конфигурацию, и получение изделия,  \nкогда  его внешний  слой  затвердевает  настолько,  что препятствует  дальнейшей  \nдеформации. Границами интервала являются температуры: tg - температура, ниже  \nкоторой  стекло  приобретает  хрупкость  (ей соответствует  вязкость  10 Па-с); f - \nтемпература,  выше  которой  стекло  приобретает  свойства,  типичные  для жидкого  \nсостояния  зависимость  свойств  стекла  от температуры:  внутренней  энергии,  энтальпии,  \nэнтропии, молярного объема (1); вязкости, удельного электрического сопротивления (2);  \nтемпературных  коэффициентов  линейного  и объемного  расширений  (3); \nтеплопроводности  (4); tg-tf - аномальный  интервал  (550...700  °С для тарного  стекла)  [3].   «POLISH  SCIENCE  JOURNAL»   \nSCIENCECENTRUM.PL  ISSUE  2(47)  ISBN  978-83-949403 -4-8 \n137  \n  \n \nРисунок  1.1. Температурный  диапазон  формования  стекла  \n \nДругим  фактором  качественного  формообразования  стеклянного  изделия  \nвыступает  поверхностное  натяжение  стекла,  то есть  действие  межмолекулярных  сил на \nчастицы его поверхностного слоя. При этом влияние поверхностных сил на процесс  \nформования  возрастает  с уменьшением  вязкости  стекла  и размеров  формуемых  \nстеклянных изделий. Важность данного фактора обусловлена явлениями смачивания  \nрасплавленным стеклом стенок форм, адсорбции, а также действиями капиллярных сил  \nи фрикционной стойкостью данных стенок к проявлению кристаллов твердых фаз в массе  \nрасплава стекла. Ввиду того, что поверхностное натяжение при падении температуры  \nрасплавленного стекла на 100 °С изменяется в среднем на 1...3%, это не позволяет  \nизменять температурный диапазон изготовления с теклянных изделий и исключает  \nвозможность  стабилизации  размеров  форм  изменением  данного  фактора  \n(температуры). Именно высокое поверхностное натяжение промышленных стекол в  \nсочетании  со своеобразных  ходом  температурной  кривой  вязкости  обуславливает  \nприменение  различных  способов  формования  стеклянных  изделий  [4]. \nНаиболее важные свойства для формования стекла представлены на рисунке 1.1  \nТаким образом, из уравнения 1.1 и рисунка 1.1, следует, что существует \"жесткая\"  \nзависимость вязкости стекла от температуры и степени прочности окислов расплава  \nстекла,  а поверхностного  натяжения  - от химического  состава,  то есть  степени  \nвзаимодействия этих окислов. При этом в производственных условиях химический  \nсостав  стекла  определяется  как эстетическими  потребительскими  свойствами  будущего  \nстеклянного  изделия  (цвет,  светоотражаемость  и т.п.),  так и техническими  \nпреимуществами  применения  того  или иного  состава  (способность  химической  реакции  \nс содержимым стеклянной упаковки, поверхностная прочность и т.п.). По этой причине  \nтарные стекла изготавливают в основном из натриевокальциевосиликатного стекла  \nследующего химического состава (% масс.): 73 SiO 2, 11 CaO, 14 Na 2O и 2 Al2O3. В небольших  \nколичествах  добавляют  MgO , K2O, SO 3. Последний  применяется,  как правило,  в качестве  \nосветлителя [14]. Температура варки такого стекла - в печи она достигает 2500 °С - \nопределяется  его составляющими  компонентами  - оксидами,  фосфидами,  фосфатами  и \nпр. [15].  Помимо  этого,  химический  состав  стекла  оказывает  значительное  влияние  и на \nповерхностное натяжение, в результате чего ведется точный расчет вводимых в печь  \nкомпонентов. Так, окислы Al2O3 и MgO , вводимые в расплавленное стекло, увеличивают  \nего поверхностное  натяжение,  а K2O и B2O3, наоборот,  понижают  его. Также  обычно  при \n  «POLISH  SCIENCE  JOURNAL»   \nSCIENCECENTRUM.PL  ISSUE  2(47)  ISBN  978-83-949403 -4-8 \n138  \n  \nвыборе состава стекла на производстве стремятся к тому, чтобы температура верхнего  \nпредела кристаллизации была на 25.30 °С ниже температуры ее формования. Структура  \nтакого  стекла  готовые  стеклянные  изделия  представлены  на рисунке  1.1. \nВ настоящее время - помимо описанного выше так называемого \"классического\"  \nсостава стекла - применяют другие химические составы, используемые на стеклоавто - \nматах как зарубежного, так и отечественного производства по изготовлению стеклянных  \nизделий  (таблица  1.1).  \nВ основном, производители формокомплектов используют готовые отливки,  \nлишь  изготавливая  из них свои  изделия.  Сама  технология  литья  чугуна  - очень  сложный  \nи высокотехнологичный процесс, и этим занимаются специализированные компании.  \nЕсть,  правда,  производители  формокомплектов,  которые  имеют  собственные  литейные  \nцеха, но это не так часто встречается, ввиду того, что качество формокомплекта, в  \nосновном, определяется качеством чугуна, а изготовить такой чугун возможно, лишь  \nимея  мощную  материально -техническую  базу.  Это не всегда  выгодно  небольшим  \nкомпаниям  \nПосле изготовления отливки в литейном цеху, производится необходимое её  \nтестирование.  Измеряется  ряд параметров,  позволяющих  сделать  заключение  о \nсоответствии изготовленного образца принятым ГОСТам и нормативам. Далее заготовку  \nобрабатывают  на станках  повышенной  точности,  оснащенных  ЧПУ.  В компьютер  \nвносится  специальная  программа,  которая  руководит  перемещениями  режущих  \nинструментов и режимами обработки. Возможности такого оборудования достаточно  \nшироки,  главное  здесь  - верно  задать  программу  и произвести  настройку  инструмента.  \nПосле того как изделие изготовлено, его направляют на приемку в ОТК. Там дается  \nзаключение о соответствии изготовленного образца конструкторской документации, и  \nпроизводится замер высокоточными измерительными приборами всех размеров и  \nшероховатости  поверхностей с  учетом требуемых  параметров.  \nЧем выше  предполагаемый  тираж  выпуска  изделий,  тем более  износоустойчивым  \nдолжен  быть  формокомплект.  Если  выпуск  бутылки  планируется  на моносекционном  \nоборудовании,  то достичь  большого  тиража  не удастся  ввиду  малой  скорости  работы  \nмашины.  В основном  на современных  заводах  используются  многосекционные  \nстеклоформирующие машины (СФМ), позволяющие увеличить выпуск изделий. Зачастую  \nпри изготовлении  формокомплекта  производят  дополнительное  упрочнение  кромок  как \nчистовой  и черновой  формы,  так и поддона  прессголовки,  горлового  и центрирующего  \nкольца  плунжера.  Когда  необходимо   выпустить  максимальное  количество  \nстеклоизделий, чистовая и черновая формы упрочняются полностью по всей поверхности  \nИзделия  изготавливаемые  из стекла  широко  используется в  народном  хозяйстве  \nи являются  важными  в потреблении.  \n \nСПИСОК  ИСПОЛЬЗОВАННЫХ  ИСТОЧНИКОВ:  \n1. Казеннова, Е.Г. Общая технология стекла и стеклянных изделий. - М.: Высшая школа,  \n2007.  - 234 с.  \n2. Евстропьев,  К.С. Стеклообразное  состояние.  - М.-Л.: Изд.  АН СССР,  1960.  - 542 с.   «POLISH  SCIENCE  JOURNAL»   \n139  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \n3. Леушин, И.О. Проблема повышения эксплуатационного ресурса литых изделий из  \nчугуна в условиях высоких температур и основные пути её решения / И.О. Леушин,  \nД.Г. Чистяков // Литейные процессы: Межрегиональный сборник научных трудов. - \nМагни¬тогорск:  Изд-во Магнитогорского  гос. техн. ун -та им. Г.И. Носова, 2011.  \n4. Леушин,  И.О.  Некоторые  направления  модернизации  технологии  изготовления  \nотливок чугунных стеклоформ для массового производства стеклянной тары / И.О.  \nЛеушин, Д.Г. Чистяков // Труды одиннадцатого Съезда литейщиков России (16 - 20 \nсентября 2013  года, г.  Екатеринбург).  - Нижний  Тагил:  Изд-во УВЗ, 2013.    «POLISH  SCIENCE  JOURNAL»   \n140  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \nSECTION:  TOURISM  AND  RECREATION  \n \n \nТилабаев  С., Муминов  А., Ли Е. \nТГПУ  имени  Низами  \n(Ташкент,  Узбекистан)  \n \nИСТОРИЯ  РАЗВИТИЯ  ЭКОЛОГИЧЕСКОГО  ТУРИЗМА  В УЗБЕКИСТАНЕ  \n \nАннотация.  В данной  статье  рассматриваются  возможности  развития  \nэкотуризма в регионах и особенности их эффективного использования. Авторы  \nпредлагают свое видение проблемы и конкретные предложения по преодолению  \nпроблем и  задач  в этой  области.  \nКлючевые  слова:  экологический  туризм,  экология,  ресурсы  экотуризма,  \nэкологические  туры,  туроператоры.  \n \nВ Узбекистане стремительно развивается туристическая отрасль. В частности,  \nУказ Президента Республики Узбекистан Ш.М. Мирзиёева «О мерах по обеспечению  \nразвития туристической отрасли Республики Узбекистан» определяет ряд возможностей  \nи преимуществ,  что свидетельствует  о том,  что есть  веские  основания  для превращения  \nнашей  страны  в туристический  центр.  Как отметил  Президент,  «...разработку  и \nреализацию  национальной  и региональных  программ  комплексного  развития  \nвнутреннего, въездного и выездного туризма, направленных на интенсивное развитие  \nтуризма  в стране  и наиболее  полное  и эффективное  использование  имеющегося  \nогромного  туристского  потенциала,  ускоренное  развитие,  наряду  с традиционным  \nкультурно -историческим  туризмом,  других  потенциальных  видов  туризма  — \nпаломнического,  экологического,   познавательного,  этнографического,  \nгастрономического,   спортивного,  лечебно -оздоровительного,  сельского,  \nпромышленного,  делового  и иных  видов  туризма...».  [1]. \nНаша  страна  большим  потенциалом  в сфере  международного  туризма.  \nЧрезвычайно выгодное географическое положение Узбекистана, прекрасные природно - \nклиматические условия также играют важную роль в культурном развитии страны.  \nУзбекистан обладает уникальными историческими и архитектурными памятниками,  \nвкусными  фруктами,  разнообразием  блюд  национальной  кухни,  традициями  и \nобычаями,  а его жители  отличаются  гостеприимством,  доброжелательностью  и \nдружелюбием. Все это привлекает внимание иностранных туристов и побуждает их  \nотправиться  в туристические  поездки.  \nЭкономическая стабильность Узбекистана имеет важное значение в развитии  \nмеждународного  туризма.  Следует  отметить,  что особое  внимание  уделяется  развитию  \nтуристического  сектора,  который  является  неотъемлемой  частью  сферы  услуг,  в третьем  \nнаправлении  стратегии  действий  по пяти  приоритетным  направлениям  развития  \nРеспублики  Узбекистан  на 2017 -2021  годы:  дальнейшее  развитие  и либерализация    «POLISH  SCIENCE  JOURNAL»   \n141  \n SCIENCECENTRUM.PL  ISSUE 2(47)  ISBN  978-83-949403 -4-8 \n \nэкономики.  Кроме  того,  в результате  введения  безвизового  режима  для граждан  86 стран  \nи упрощения визового  режима  для граждан 57  стран Узбекистан в прошлом году  \nпосетили  6,7 миллиона  иностранных  туристов  по сравнению  с 4,7 миллиона  в 2016  году  \nна человека или в  3,3 раза.  [2]. \nВ целях  последовательного  продолжения  комплексных  реформ,  принятой  2 марта  \n2020  года в Государственной  программе по реализации Стратегии  действий  по пяти  \nприоритетным направлениям развития Республики Узбекистан на 2017 -2021 годы в год  \nнауки,  просвещения  и цифровой  экономики  было  подчеркнуто,  что превращение  \nтуризма  в стратегический  сектор  экономики  остается  для нас приоритетом.  Одной  из \nважнейших задач государства является увеличение количества туристов, посещающих  \nнашу  страну  до 7,5 млн  в этом  году,  а также  необходимость  ускорения  развития  \nвысокопотенциального паломничества и медицинского туризма. [3]. Известно, что ряд  \nвлиятельных  зарубежных  СМИ  считают,  что в 2020  году  Узбекистан  должен  быть  \nвключен в список рекомендуемых стран для путешествий, и что мы должны эффективно  \nиспользовать  возможности  в сфере  туризма  в нашей  стране.  Принимая  во внимание  \nвышесказанное,  в нашей  стране  «… координация  разработки  национальных  и \nрегиональных  программ  комплексного  развития  внутреннего,  въездного  и выездного  \nтуризма  и мониторинг  их реализации,  ускоренное  развитие  широкого  спектра  туризма,  \nв соответствии с международными стандартами…». [4] Ускоренное развитие экотуризма  \nв нашей  стране  положительно  скажется  на стремительном  развитии  экономики  страны.  \nРазвитие экотуризма в Узбекистане требует всесторонне скоординированного  \nкомплексного подхода. Необходимо привлечь к этой работе все заинтересованные  \nстороны. Только тогда удастся предотвратить ошибки, которые могут возникнуть в  \nпроцессе развития экотуризма и помогут избежать негативные последствия. Особое  \nвнимание стоит уделить предприятиям и организациям, основным видом деятельности  \nкоторых  является  туризм.  \nРазвитие  предприятий  и организаций,  имеющих  отношение  к развитию  туризма  \nдолжно поддерживаться государством и обществом. В частности, землепользование,  \nналогообложение, лицензирование хозяйственной деятельности и регистрация - это \nрешение пограничных проблем. Исходя из действующего законодательства, можно  \nпредложить  несколько  подходов  к развитию экотуризма:  \n- создание отдела по развитию экотуризма на базе Государственного комитета по  \nразвитию  туризма  и налаживание  региональных  мероприятий;  \n- создание центра развития и поддержки экотуризма (разработка законов о  \nтуризме  на особо  охраняемых  природных  территориях,  адресах,  территориях  \nэкотуризма  и т. д.); \n- создание инфраструктуры экологического туризма (свободные туристические  \nзоны,  строительство  туристических  объектов,  гарантия  иностранных  инвестиций,  \nпринятие  законов  о поддержке  частного  предпринимательства  в туризме  и др.).  \nУзбекистан имеет большой потенциал для развития экологического туризма.  \nЖивописные горы и реки, красивые пейзажи, традиционный образ жизни, культурно - \nисторические  и религиозные  центры  являются  наиболее  важными  факторами  для \nэкологического  туризма.  Развитие  экотуризма  в Узбекистане  должно  стать    «POLISH  SCIENCE  JOURNAL»   \n142  \n SCIENCECENTRUM .PL ISSUE  2(47)  ISBN  978-83-949403 -4-8 \n \nнеотъемлемой частью стратегии устойчивого развития туризма в стране. Стоит отметить,  \nчто в Узбекистане не развит туризм активного отдыха, и поэтому он не так привлекателен  \nдля экотуристов.  Поэтому  основной  упор  следует  делать  на такой  вид туризма,  который  \nпозволяет  наслаждаться  природными красотами.  \nУзбекистан очень богат ресурсами и широкими возможностями экотуризма.  \nЭкотуры,  проводимые  туроператорами  по маршрутам  «Ташкент -Чарвак -Ташкент»,  \n«Ташкент -Билдирсай -Чимган -Ташкент»,  «Ташкент -Замин -Ташкент»,  «Ташкент -Айдаркул - \nТашкент»,  прогулки  на верблюдах  по пустыне  не только  в Узбекистане,  но и завоевывает  \nбольшое  внимание  и признание  туристов  всего  мира.  Согласно  анализу,  туроператоры,  \nпредоставляющие услуги экотуризма, в основном работают в зоне отдыха Чимган - \nЧарвак.  За последние  три года  горы  Гиссар,  Боботаг,  Кохитангтог  (Сурхандарья),  Чакчар  \n(Кашкадарья), Нурата (Навои -Джизак), Зарафшан (Самарканд), Туркестан (Джизак), Алай  \n(Фергана), Курама, Чаткал (Наманган), Остальные горы пустыни Кызылкум, пустыня  \nКызылкум, высохшее дно Аральского моря и соленые озера, плато Устюрт, борсинская  \nкельмасская  соль,  бассейн  Мингбулак,  тугай  на берегах  Айдаркуля,  Денгизкуля,  \nАмударьи и Сырдарьи, карьер Мурунтау и уникальные памятники природы проекты  \nреализуются.  \nВ пустынях  и равнинах  Узбекистана  есть  прекрасная  возможность  и организовать  \nпрогулки  на верблюдах,  чтобы  увидеть  барханы,  дюны,  тугайи,  солончаки,  белоголовых  \nорлов, колодцы, а также пастбища, отражающие образ жизни местных пастухов. Горы  \nБольшой Чимган, Заамин, Чаткал, Коксу, Писком, Туркестан, Гиссар, Зарафшан, Алой,  \nКурама,  Нурата  имеют  природно -географические  условия  для организации  пеших  \nпоходов, треккинга и альпинизма. Также существуют большие возможности для развития  \nрафтингового туризма во многих серостонах и восточных реках и ручьях (Угам, Чаткал,  \nПиском, Коксув, Тополондарья, Шохимар дон, Сангзор, Зарафшан и др.), Текущих с гор.  \nПо мнению научно -исследовательских институтов и экспертов, сегодня направления  \nразвития  и перспективы  экотуризма  формируются  в связи  с законами  природы  и \nустойчивостью  геосистем.  \nЕще одним аспектом внимания государства к развитию экотуризма в стране  \nявляется то, что 8 января 2019 года «Экологическое движение» стало независимой  \nэкологической партией, работа по развитию экотуризма в стране поднялась на новый  \nуровень. Несмотря на свой молодой возраст, Экологическая партия в короткие сроки  \nдобивается больших результатов, проводя практическую работу в регионах. В частности,  \nв пустыне Кызылкум есть четырехсторонние лагеря, где можно провести время очень  \nинтересно и насыщенно. Пикники на набережной, прогулки  на лодке по озеру Айдарколь,  \nнаблюдение  за звездами  в телескоп,  пение  у костра,  путешествие  на джипе  по красным  \nпескам,  катание  на верблюдах  и пешие  прогулки,  а также  многие  другие  увлекательные  \nмероприятия  предоставляются  туристам.  \nЭкологический  вид туризма  может  быть  выделен  как отдельный  или как \nсмешанный вид. Мировой опыт показывает, что смешанные виды: культурный туризм и  \nэкотуризм вместе составляют 15 -23% от общего числа. Например, туристы могут посетить  \nУгам -Чаткальский национальный парк на 1 день после приезда в Ташкент. Затем три  \nизвестных  исторических  города  - Самарканд,  Бухара,  возможно  продолжение    «POLISH  SCIENCE  JOURNAL»   \n143  \n SCIENCECENTRUM .PL ISSUE  2(47)  ISBN  978-83-949403 -4-8 \n \nпутешествия  по Хиве.  По пути  из одного  города  в другой  они могут  заходить  в \nэнтодеревни для знакомства с культурой, бытом и народными промыслами нашего  \nнарода.  \nКонечно,  в этой  сфере  есть  свои  проблемы  и вызовы, а  именно:  \n- инфраструктура  не соответствует  требуемому  уровню;  \n- отсутствие  высококвалифицированных  специалистов  в сфере  экотуризма;  \n- отсутствие  развитых  туристических  маршрутов  к основным  ресурсам  \nэкотуризма  в стране;  \n- отсутствие инфраструктуры, способствующей продвижению занятости местного  \nнаселения  и его  вовлечение  в развитие  экотуризма  в экотуризме.  \n- отсутствие  отечественной  и международной  рекламы  экотуризма,  \nспособствующей  развитию экотуризма;  \n- низкая осведомленность общества о возможностях и перспективах экотуризма,  \nвидах  его услуг  как на внутреннем  туристическом  рынке,  так и на внешнем  \nтуристическом  рынке.  \nВ настоящее время экотуризм новое явление для нашей страны. Экотуризм  \nнуждается  в популяризации,  креативной  рекламе  и пропаганде.  Прежде  всего,  \nнеобходимо понять, что экотуризм выгоден как для экономики, так и для местного  \nнаселения, и экотуризм один из самых эффективных способов сохранения природы. К  \nработе в этой сфере необходимо привлекать население, проживающее в регионах  \nпригодных для экотуризма, то есть они должны стать специалистами. Для развития  \nэкотуризма  необходимо  формирование  экологической  культ уры и образования  \nузбекского  народа.  В то же время  необходимо  создавать  и улучшать  информационные  \nресурсы,  которые  могут  предоставить  полную  информацию  об экотуризме  и связанных  с \nним организациях. В то же время необходимо упростить процедуру открытия небольших  \nгостиниц за городом. И самое главное, необходимо контролировать, чтобы развитие  \nтуризма  не наносило  вред  природе,  не приводило  к исчезновению  входящих  в капитал  \nтрадиций и ключевых элементов. Развивая экотуризм в стране, мы сможем добиться  \nследующих  результатов:  \n- повышение  уровня  экологической  грамотности  населения;  \n- стремление местных жителей рассматривать охраняемые территории как места,  \nпредставляющие  эстетическую  и экономическую  ценность,  и поощрять  развитие  \nэкотуризма  к совместной работе;  \n- улучшение социально -экономических условий в регионах, а также создание  \nновых  рабочих  мест  для местного  населения;  \n- появление  притока  дополнительных  средств  финансовой  поддержки  в бюджет  \nрегионов  и страны.  \nРазвитие экотуризма в Узбекистане является важным фактором обеспечения  \nдолголетия людей, создание свободной и процветающей Родины для будущих поколений,  \nа также  экономического  развития  общества.  Развитая  инфраструктура  различных  видов  \nтуризма  способствует  экономической  и социальной  выгоде,  а именно  сохранение  \nисторических  памятников,  национальной  культуры  и природных    «POLISH  SCIENCE  JOURNAL»   \n144  \n SCIENCECENTRUM .PL ISSUE  2(47)  ISBN  978-83-949403 -4-8 \n \nдостопримечательностей  страны,  а также  продемонстрирует  мировому  сообществу  \nгостеприимство,  открытость  и доброжелательность  узбекского  народа.  \n \nСПИСОК  ИСПОЛЬЗОВАННЫХ  ИСТОЧНИКОВ:  \n1. Указ Президента Республики Узбекистан «О мерах по обеспечению ускоренного  \nразвития  туризма  в Республике  Узбекистан».  Народное  слово,  7 декабря  2016  г. \n№ 242 (6677).  \n2. Послание Президента Республики Узбекистан Шавката Мирзиёева Олий Мажлис.  \nНародная слово, 25  января 2020 г.  \n3. Состояние реализации Стратегии действий по пяти приоритетным направлениям  \nразвития Республики Узбекистан на 2017 -2021 годы в «Год науки, просвещения и  \nцифровой экономики» с целью последовательного проведения комплексных реформ  в \nинтересах  народа,  принятой  2 марта  2020  года  программного  обеспечения.  \nНародное  слово, 3 марта  2020 г.  \n4. Постановление Президента Республики Узбекистан № ПП -2666 «Об организации  \nГосударственного комитета по развитию туризма Республики Узбекистан», 2 декабря  \n2016  г. 145  \n  \n \n \n \n \n \n \nPOLISH  SCIENCE  JOURNAL  \n \nExecutive  Editor -in-Chief:  PhD Oleh  M. Vodianyi  \n \n \n \n \n \n \n \n \n \n \nISSUE  2(47)  SEPTEMBER  \n \n \n \n \n \n \n \n \n \n \n \n \nFounder:  “iScience”  Sp. z o. o., \nNIP 5272815428  \n \n \nSubscribe  to print  13/10/202 4. Format  60×90/16.  \nEdition  of 100 copies.  \nPrinted  by “iScience” Sp.  z o. o. \nWarsaw,  Poland  \n08-444, str. Grzybowska, 87  \ninfo@sciencecentrum.pl,  https://sciencecentrum.pl"
    },
    {
        "title": "Eigenvector-based Relational Motif Discovery.",
        "author": [
            "Alberto Pinto"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1415906",
        "url": "https://doi.org/10.5281/zenodo.1415906",
        "ee": "https://zenodo.org/records/1415906/files/Pinto10.pdf",
        "abstract": "The development of novel analytical tools to investigate the structure of music works is central in current music information retrieval research. In particular, music sum- marization aims at finding the most representative parts of a music piece (motifs) that can be exploited for an effi- cient music database indexing system. Here we present a novel approach for motif discovery in music pieces based on an eigenvector method. Scores are segmented into a network of bars and then ranked depending on their cen- trality. Bars with higher centrality are more likely to be relevant for music summarization. Results on the corpus of J.S.Bach’s 2-part Inventions demonstrate the effectiveness of the method and suggest that different musical metrics might be more suitable than others for different applica- tions.",
        "zenodo_id": 1415906,
        "dblp_key": "conf/ismir/Pinto10",
        "keywords": [
            "novel",
            "analytical",
            "tools",
            "music",
            "information",
            "retrieval",
            "research",
            "motif",
            "discovery",
            "music"
        ],
        "content": "EIGENVECTOR-BASED RELATIONAL MOTIF DISCOVERY\nAlberto Pinto\nUniversit `a degli Studi di Milano\nDipartimento di Informatica e Comunicazione\nVia Comelico 39/41, I-20135 Milano, Italy\npinto@dico.unimi.it\nABSTRACT\nThe development of novel analytical tools to investigate\nthe structure of music works is central in current music\ninformation retrieval research. In particular, music sum-\nmarization aims at ﬁnding the most representative parts of\na music piece (motifs) that can be exploited for an efﬁ-\ncient music database indexing system. Here we present a\nnovel approach for motif discovery in music pieces based\non an eigenvector method. Scores are segmented into a\nnetwork of bars and then ranked depending on their cen-\ntrality. Bars with higher centrality are more likely to be\nrelevant for music summarization. Results on the corpus of\nJ.S.Bach’s 2-part Inventions demonstrate the effectiveness\nof the method and suggest that different musical metrics\nmight be more suitable than others for different applica-\ntions.\n1. INTRODUCTION\nListening to music and perceiving its structure is a rela-\ntively easy task for humans, even for listeners without for-\nmal musical training. However, building computational\nmodels to simulate this process is a hard problem. On the\nother hand, the problem of automatically identifying rele-\nvant characteristic motifs and efﬁciently store and retrieve\nthe digital content has become an important issue as digital\ncollections are increasing in number and size more or less\neverywhere.\nNotwithstanding the conspicuousness of the literature,\ncurrent approaches seem to rely just on the repetition para-\ndigm [20] [8], assigning higher scores to recurring equiv-\nalent melodic and harmonic patterns [11]. Recently re-\nported approaches to melodic clustering based on string\ncompression [10], motivic topologies [18], graph distance\n[21] and paradigmatic analysis [19] have been used to se-\nlect relevant subsequences among highly repeated ones by\nheuristic criteria [15] [1]. However, this approach is not\ncompletely satisfying as the repetition paradigm can pro-\nvide just a ﬁrst approximation of the perceptual ranking\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2010 International Society for Music Information Retrieval.mechanism [3] and produces too many false positives shar-\ning the same repetition rates.\nMoreover, the repetition paradigm, in order to be ap-\nplied, needs by no means a precise deﬁnition of “varied\nrepetition”, a concept not easy to deﬁne. Of course, it has\nto include standard music transformation, but it is very dif-\nﬁcult to adopt a simple two-valued logic (this is a repeti-\ntion and this is not) in this context, where a more fuzzy\napproach seems to better address such a problem.\nSometime repetitions may even lead to evident mistakes,\nas it might happen that highly repeated patterns turn to be\ntotally irrelevant from a musicological point of view. In\nfact cases occur where the most repeated pattern in the\nwhole composition is an ornament, like a trill. This is to\nshow that the repetition paradigm is not sufﬁcient in itself\nto identify relevant themes but it needs some heuristics to\nselect among relevant and irrelevant patterns.\nHere we present an alternative ranking method based on\nconnections instead of repetitions. We show that a distance\ndistribution on a graph of note subsequences induced by\nmusic similarity measures generates a ranking real eigen-\nvector whose components reﬂect the actual relevance of\nmotives. False positives of the repetition paradigm turned\nout to be less connected nodes of the graph due to their\nhigher degree of dissimilarity with relevant motives.\nOur results show how higher indexes of connection, or\n“centrality”, are more likely to perform better than higher\nrepetition rates in motif discovery, with no additional as-\nsumptions on the particular nature of the sequence or the\nadopted similarity measure.\n2. RELATED WORKS\nMusic segmentation is usually realized through musicolog-\nical analysis by human experts and, at the moment, auto-\nmatic segmentation is a difﬁcult task without human inter-\nvention. The supposed music themes have often to undergo\na hand-made musicological evaluation, aimed at recogniz-\ning their expected relevance and completeness of results.\nAs a matter of fact, an automatic process could extract a\nmusical theme which is too long, or too short, or simply\nirrelevant. Thats why a human feedback is still required in\norder to obtain high-quality results.\nWe present here an overview of current approaches based\non different musical assumptions. We start this section\nwith a general overview of the literature. Then we intro-\n207\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)duce harmony related approaches, with a focus to reduc-\ntionistic ones. Finally we introduce topology-based mod-\nels, which share much more similarities than others with\nour approach. All those methods makes use of the repeti-\ntion paradigm.\n2.1 General approaches\nLartillot [15] [16] deﬁned a musical pattern discovery sys-\ntem motivated by human listening strategies. Pitch inter-\nvals are used together with duration ratios to recognize\nidentical or similar note pairs, which in turn are combined\nto construct similar patterns. Pattern selection is guided\nby paradigmatic aspects and overlaps of segments are al-\nlowed.\nCambouropoulos [6], on the other hand, proposed meth-\nods to divide given musical pieces into mostly non-over-\nlapping segments. A prominence value is calculated for\neach melody based on the number of exact occurrences of\nnon-overlapping melodies. Prominence values of melodies\nare used to determine the boundaries of the segments [7].\nHe also developed methods to recognize variations of ﬁll-\ning and thinning (through note insertion and deletion) into\nthe original melody. Cambouropoulos and Widmer [9] pro-\nposed methods to construct melodic clusters depending on\nthe melodic and rhythmic features of the given segments.\nBasically, similarities of these features up to a particular\nthreshold are used to determine the clusters. High com-\nputational costs of this method make applications to long\npieces difﬁcult.\n2.2 Tonal harmony-based approaches\nTonal harmony based approaches exploit particular har-\nmonic patterns (such as tonic-subdominant-dominant-tonic),\nmelodic movements (e.g. sensible-tonic), and some rhyth-\nmical punctuation features (pauses, long-duration notes,\n...) for a deﬁnition of a commonly accepted semantic in\nmany ages and cultures.\nThese approaches typically lead towards score reduc-\ntions (see Figure 1), made possible by taking advantage of\nadditional musicological information related to the piece\nand assigning different level of relevance to the notes of\na melody. For example one may choose to assign higher\nimportance to the stressed notes inside a bar [22]. In other\nwords, the goal of comparing two melodic sequences is\nachieved by reducing musical information into some “prim-\nitive types” and comparing the reduced fragments by means\nof suitable metrics.\nGGG222`(ˇ`(ˇ`(ˇ`ˇ`(ˇˇ4ˇˇ`(ˇˇˇˇ`ˇ`(ˇˇˇˇ`(ˇˇˇˇ`(ˇ`(ˇˇˇˇ\nFigure 1. J.S. Bach, BWV 1080: Score reductions.A very interesting reductionistic approach to music anal-\nysis has been attempted by Fred Lerdahl and Ray Jack-\nendoff. Lerdahl and Jakendoff [17] research was oriented\ntowards a formal description of the musical intuitions of\na listener who is experienced in a musical idiom. Their\npurpose was the development of a formal grammar which\ncould be used to analyze any tonal composition.\nThe study of these mechanisms allows the construction\nof a grammar able to describe the fundamental rules fol-\nlowed by human mind in the recognition of the underlying\nstructures of a musical piece.\n2.3 Topological approaches\nMazzola and Buteau [5] proposed a general theoretical frame-\nwork for the paradigmatic analysis of the melodic struc-\ntures. The main idea is that a paradigmatic approach can\nbe turned into a topological approach. They consider not\nonly consecutive tone sequences, but allow any subset of\nthe ambient melody to carry a melodic shape (such as rigid\nshape, diastematic shape, etc.). The mathematical con-\nstruction is very complex and, as for the motif selection\nprocess, it relies on the repetition paradigm.\nThe method proposed by Adiloglu, Noll and Obermayer\nin [1] does not take into account the harmonic structure of\na piece and is based just on similarities of melodies and on\nthe concept of similarity neighborhood. Melodies are con-\nsidered as pure pitch sequences, excluding rests and rhyth-\nmical information.\nA monophonic piece is considered to be a single melody\nM, i.e. they reduce the piece to its melodic surface. Sim-\nilarly, a polyphonic piece is considered to be the list M=\n(Mi)i=1;:::;N of its voices Mi. The next step is to model a\nnumber of different melodic transformations, such as trans-\npositions, inversions and retrogradations and to provide\nan effective similarity measure based on cross-correlation\nbetween melodic fragments that takes into account these\ntransformations. They utilize a mathematical distance mea-\nsure to recognize melodic similarity and the equivalence\nclasses that makes use of the concept of neighbourhood to\ndeﬁne a set of similar melodies.\nFollowing the repetition paradigm stated by Cambouro-\npoulos in [7] they deﬁne a prominence value to each melody\nbased on the number of occurrences, and on the length of\nthe melody. The only difference is that they allow also\nmelody overlapping. In the end, the signiﬁcance of a melody\nmof lengthnwithin a given piece Mis the normalized\ncardinality of the similarity neighbourhood set of the given\nmelody. If two melodies appear equal number of times, the\nlonger melody is more signiﬁcant than the shorter one.\nIn [1] the complete collection of the Two-part Inven-\ntions by J. S. Bach is used to evaluate the method, and this\nwill be also our choice in section 4.\n3. THE RELATIONAL MODEL\nAs stated in Section 2, current methods rely on the repeti-\ntion paradigm. Our point of view can be synthesized in the\nfollowing points:\n208\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)1. We consider a music piece as a network graph of\nsegments,\n2. we do take into account both melodic and rhythmical\nstructures of segments\n3. we do not consider harmony, as it is too much related\nto tonality.\nframe(i-1,1)frame(i,1)frame(i+1,1)frame(i-1,2)frame(i,1)frame(i+1,1)\nframe(i-1,n)frame(i,n)frame(i+1,n)Voice 1\nVoice 2\nVoice nTime ﬂow\nFigure 2. A representation of the (ﬁrst-order) network of\nframes.\nA single frame may represent, for instance, a bar or a\nspeciﬁc voice within a bar like in Fig. 2, but also more\ngeneral segments of the piece. We do not take into account\nhere the problem of windowing as the method is basically\nindependent from any speciﬁc segmentation of the piece.\nWhat we provide here is a different point of view which,\nlike the repetition paradigm, can be applied in principle to\nany speciﬁc segmentation.\n3.1 Score representation\nThe natural consequence is that a music piece can be looked\nat like a complete graph Kn. In graph theory, a complete\ngraph is a simple graph where an edge connects every pair\nof distinct vertices. The complete graph on nvertices has\nn(n\u00001)=2 edges and is a regular graph of degree n\u00001.\nIn this representation, score segments correspond to graph\nnodes and the similarity between couples of segments cor-\nrespond to edge weights. This approach can be better ex-\nplained if we think to a score like a network graph of “pages”,\nso we can establish a parallelism between score segments\nranking and the World Wide Web ranking process as orig-\ninally depicted in [4] by S. Brin and L. Page.\nAs stated before, the problem of windowing is partly\novercome in the network concept as it does not strongly\naffect the model. In fact by using undersized windows we\nnormally get just more detailed results. In our experiments(see Section 4) we decided to adopt a one-bar length win-\ndow, as we considered metric information relevant to mu-\nsic segmentation, avoiding any form of overlapping. In fact\nit turned out that if the metric information is taken into ac-\ncount, overlapping windows are not relevant in a relational\nmodel, as they can lead to inaccurate motif discovery, due\nto overrates given to highly self-similar segments.\n3.2 Metric weights\nAs stated above, graph nodes correspond to score segments.\nThe next issue is the deﬁnition of a suitable concept of dis-\ntance between segments. This should be apparently at the\nvery heart of the method, and in a sense it is. Every time\nthere is a similarity concept the question is: which kind of\nsimilarity? There are so many different concepts of music\nsimilarity (perceptual, structural, melodic, rhythmical, and\nso on) that is not possible to provide a unique deﬁnition.\nThe variety of segmentations reﬂects to a large extent\nthe variety of musical similarity concepts, and that is the\nreason why it is correct to have this parameter here. Nev-\nertheless, as stated in Section 4 the model is rather robust\nrespect to metric changes.\nIn general, we can just say that the set of segments can\nbe endowed with a notion of distance\nd:S\u0002S! R\nbetween pairs of segments and turns this set into a (possi-\nbly metric) space (S;d). A natural choice for point sets of\na metric space is the Hausdorff metric [13] but any other\ndistance discovered to be useful in music perception, like\nEMD/PTD [23], can be chosen as well.\nHere we assume dto be:\n1. real,\n2. non-negative,\n3. symmetric and\n4. such that d(s;s) = 0,8s2S\nAs a matter of fact, most musically relevant perceptual\ndistances do not satisfy all metric axioms [23]. Therefore\nno further property, like the identity of indiscernibles or the\ntriangle inequality, is assumed.\nGiven two segments s1ands2, for the experiments we\nadopted the two following simple metrics:\nd1(s1;s2) =sX\njsjj[s1]12\u0000[s2]12j (1)\nd2(s1;s2) =sX\njsj(s0\n1\u0000s0\n2)2 (2)\nwheres0is the derivative operator on the sequence s,jsjis\nthe length of sand[s]12is the sequence swhere each entry\nhas been chosen in the interval [0;11].\nd1is a ﬁrst-order metric that takes into account just oc-\ntave transpositions of melodies. In fact, pitch classes out\nof the range [0;11]are folded back into the same interval,\n209\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)so melodies which differ for one or more octaves belong\nto the same congruence class modulo 12 semitones. d2\nis a second-order metric that takes into account arbitrary\ntranspositions and inversions of a melody. No other as-\nsumptions on possible variations have been made, so that\nan equivalence class of melodies is composed just of trans-\npositions and inversions of the same melody like in [1].\nBoth distances can be applied to single voice sequences\nbut also to multiple voice sequences, given that a suitable\nrepresentation has been provided. For instance, in a two\nvoice piece, with voices v1andv2, one can consider the\ndifference vector v=v1\u0000v2as a good representation\nof a speciﬁc segment, and then apply d1ord2to this new\nobject. The advantage of using this differential represen-\ntation is that it is invariant respect to transpositions and\ninversions of the two voices so that, for instance, it makes\nalsod1invariant respect to transpositions and inversions,\nand not just to octave shifts.\nBy exploiting those distance concepts, it is possible to\nendow the edges of the complete graph with metric weights\nin order to compute the weights of nodes in terms of the\nmain eigenvector, as we are going to show in the following\nSections.\n3.3 Matrix representation and ranking eigenvector\nThe adopted algebraic representation of the ‘score graph’\nKis the adjacency matrix A(K ). This is a nonnegative\nmatrix as its entries are the distance values between the dif-\nferent segments in which the score has been divided into.\nPerron-Frobenius theory for nonnegative matrices grants\nthat, ifA2Mn\u0002n andA\u00150, then there is an eigenvec-\ntorx2Rnwithx\u00150and andPn\ni=1xi= 1, called the\nPerron vector ofA[14].\nThis result has a natural interpretation in the theory of ﬁ-\nnite Markov chains, where it is the matrix-theoretic equiv-\nalent of the convergence of a ﬁnite Markov chain, formu-\nlated in terms of the transition matrix of the chain [2].\nThe Perron vector can be viewed as a probability dis-\ntribution of presence of a ‘random listener’ on a particular\nsegment of a musical piece. This listener recalls with prob-\nabilityd(si;sj)segmentsjfrom segment si, following the\n‘links’ represented by the values of the similarity function.\n3.4 The algorithm\nLetd:S\u0002S! Rdenote a distance function on S, like\nthose deﬁned in Section 3.2, which assigns each pair of\nsegmentssiandsja distanced(si;sj). We can describe\nthe algorithm through the following steps:\n1. Form the distance matrix A= [ai;j]such thatai;j=\nd(si;sj);\n2. Form the afﬁnity matrix W= [wi;j]deﬁned by\nwi;j= exp \n\u0000a2\ni;j\n2\u001b2!\n(3)\nwhere\u001bis a parameter that can be chosen experi-\nmentally. A possible choice is the standard devia-\n5 10 15 20 2500.20.40.60.81Invention N.2\nBarsCentralityFigure 3. Normalized eigenvector proﬁle for bars in BWV\n773. Higher values correspond to higher centrality (see\nalso Table 2). The metric space is (S;d1).\ntion of the similarity values within the considered\nnetwork graph;\n3. Compute the leading eigenvector x= [xi]ofWand\nrank each segment siaccording to the component xi\nofx.\n4. EXPERIMENTAL RESULTS\nIn order to evaluate the relevance of the results of the pro-\nposed method we need a suitable data collection together\nwith a commonly acceptable ground truth for that collec-\ntion. Following [1], Johann Sebastian Bach’s Two-part In-\nventions has been our choice. For this collection, a com-\nplete ground truth is provided by musicological analysis\nand it can be found for example in [12] and [24].\nThe ﬁrst choice we had to make was the segment size.\nMany experiments has been conducted but, as stated be-\nfore, it turned out that reductions of the segment size (for\nexample from two bars to one bar) did not sensibly affect\nthe results. So experiments have been performed with a\none-bar long window. Experiments have been performed\nalso to verify the suitability of an overlapping technique\nbut we did not observed any improvement in the results.\nSecond, we implemented the functional metrics described\nin Section 3.3. By performing the experiments, we ob-\nserved a few variations in the ﬁrst two ranked levels, and\nthis means that top ranked bars tend to be more “stable” re-\nspect to metric changes. Thus we can say that the method\nis rather robust, as far as these metrics are concerned. In\nthe synthesis reported in Table 2 we considered just the top\nranked segments, i.e. corresponding to the two (different)\nhighest values of xcomponents.\nWhen compared to musicological analysis [1] [12] [24]\nit is evident that the centrality-based model outperforms\nthe repetition-based model, providing also more signiﬁca-\ntive information. Segments with higher rank in the rela-\ntional model represent always relevant bars of the score,\neven if they may be different by using different metrics.\nThis means that relevant bars contain a main motif or char-\nacterizing sequences. It is not the same for the model based\n210\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)!40 !30 !20 !10 0 10 20 30 40 50 60!25!20!15!10!505101520\n 1 2 3 4 5 6 7\n 8 910\n111213\n141516\n1718\n192021\n2223\n2425\n26\nStudent Version of MATLABFigure 4. 2D projection of the metric space (S;d1)for\nBWV 773. Bars with higher centrality values (darker la-\nbels) tend to occupy the central region of the graph.\nModel Precision (%)\nRepetition 43\nd1 77\nd2 95\nTable 1. Precision results for the three models applied to\nJ. S. Bach’s Inventions.\non repetitions: here the relevancy really depends just on the\nnumber of repetitions, so it can happen that a trill turns to\nbe more relevant than the rest of the piece just because its\nrepetition rate is higher than that of the other bars.\nBar ranking is in principle not affected by the repeti-\ntion rate of patterns and higher importance is equally given\nto higher and lower repetition rates. Of course, superpo-\nsitions of the two methods may happen too. On the other\nhand, cases exist for which no repetition occurs and, conse-\nquently, the repetition paradigm is not applicable in princi-\nple, unless deﬁning ad hoc neighborhood concepts for each\npiece. In these cases, motif centrality can provide signiﬁ-\ncant results.\nIn Figure 3 the components of the main eigenvector for\nBWV 773, representing the degree of centrality of each\nbar, have been plotted against bar numbers. This provides\nan immediate representation of the “importance” of each\nbar within the whole piece. Bars with higher values are\nmore likely to contain a main motif of the piece. In partic-\nular, for BWV 773, bars 1 and 2 actually contain the main\nmotif.\nFigure 4 shows a two-dimensional projection of the 26-\ndimensional metric space for BWV 773 obtained through a\ndimensionality reduction algorithm. From this picture it is\nevident how the top ranked results (1, 2) occupy the central\nregion of the graph and have darker labels, as the darkness\nis directly proportional to the correspondent component of\nthe main eigenvector, and thus to the centrality, in the sense\nof graph theory, of the correspondent segment.Table 1 presents a synthesis of the results shown in Ta-\nble 2 in terms of the precision of the three methods. As for\nthe computational complexity, suitable linear eigensolvers\nare available, and they can be easily applied, especially in\ncase of very long pieces.\n5. CONCLUSIONS\nWe presented a new approach for motif discovery in music\npieces based on an eigenvector method. Scores are seg-\nmented into a network of bars and then ranked depending\non their graph centrality. Bars with higher centrality are\nmore likely to be musically relevant and can be exploited\nfor music summarization. Experiments performed on the\ncollection of J.S.Bach’s 2-parts Inventions show the effec-\ntiveness of our method.\nBesides music information retrieval, we expect this ap-\nproach to ﬁnd applications in music theory, perception and\nvisualization. For instance, one could investigate how par-\nticular mathematical entities (e.g. spectra) relate to partic-\nular musical issues (e.g. genre, authorship).\nSecond, one could investigate how different metrics d\nrelate to different concepts of melodic and harmonic simi-\nlarity; in this context, the inverse problem of ﬁnding met-\nricsdinduced by a priori eigenvectors (coming from a\nhand-made musicological analysis) could provide interest-\ning insights into music similarity perception.\nThird, it is also possible to compare different music\npieces from a structural point of view by comparing their\nassociated eigenvectors.\nFinally, the method could be extended to the audio do-\nmain, for instance to organize large audio collections, where\nheuristic methods can be hardly applied and it is usually\ndifﬁcult or even impossible to separate different voices and/\nor musical instruments.\n6. REFERENCES\n[1] K. Adiloglu, T. Noll, and K. Obermayer. A paradig-\nmatic approach to extract the melodic structure of\na musical piece. Journal of New Music Research,\n35(3):221–236, 2006.\n[2] T. Bedford, M.S. Keane, and C. Series. Ergodic the-\nory, symbolic dynamics, and hyperbolic spaces. Ox-\nford University Press New York, 1991.\n[3] R. Bod. Memory-Based Models of Melodic Analysis:\nChallenging the Gestalt Principles. Journal of New Mu-\nsic Research, 31(1):27–36, 2002.\n[4] S. Brin and L. Page. The anatomy of a large-scale hy-\npertextual Web search engine. Computer Networks and\nISDN Systems, 30(1-7):107–117, 1998.\n[5] C. Buteau and G. Mazzola. From Contour Similarity to\nMotivic Topologies. Musicae Scientiae, 4(2):125–149,\n2000.\n[6] E. Cambouropoulos. Extracting’Signiﬁcant’Patterns\nfrom Musical Strings: Some Interesting Problems. Pre-\nsente aux London String Days, 2000, 2000.\n211\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)NoCatalog Repeated bars d1 d2\n1 BWV 772 ; 16, 18, 17, 3 16, 18\n2 BWV 773 2, 3, 24, 25 2, 1, 21, 15 3, 25, 2, 24\n3 BWV 774 2, 20, 42, 50, 55 54, 41, 19, 48 2, 20, 42, 50, 55\n4 BWV 775 1, 2, 5, 6, 19, 20, 21, 44, 45 6, 2, 29, 22 1, 5, 44, 2, 6, 45\n5 BWV 776 2, 3, 28, 29 2, 16, 17, 27 2, 28, 3, 29\n6 BWV 777 5, 25, 42, 63, 84, 105 56, 21, 6, 20 5, 25, 42, 63, 84, 105\n7 BWV 778 ; 10, 8, 5, 1 7, 4\n8 BWV 779 ; 1, 6, 20 6, 26\n9 BWV 780 1, 2, 3, 4, 29, 30, 31, 32 31, 14, 1, 2 1, 3, 29, 2\n10 BWV 781 20, 21, 22, 23, 27, 29, 31 1, 17, 27, 32 1, 27, 29, 31\n11 BWV 782 ; 17, 4, 14, 6 6, 17\n12 BWV 783 ; 5, 6, 17, 16 9, 5\n13 BWV 784 ; 18, 2, 1, 21 8, 1\n14 BWV 785 12, 14 8, 6, 7, 16 12, 14, 16\n15 BWV 786 ; 7, 10, 14, 9 12, 4\nTable 2. Experimental results for the repetition paradigm (using both metrics d1andd2) and the relational paradigm. Gray\nnumbers represents irrelevant bars.\n[7] E. Cambouropoulos. Musical pattern extraction for\nmelodic segmentation. Proceedings of the ESCOM\nConference 2003, 2003.\n[8] E. Cambouropoulos, M. Crochemore, C. Iliopou-\nlos, L. Mouchard, and Y . Pinzon. Algorithms for\ncomputing approximate repetitions in musical se-\nquences. International Journal of Computer Mathe-\nmatics, 79(11):1135–1148, 2002.\n[9] E. Cambouropoulos and G. Widmer. Automated mo-\ntivic analysis via melodic clustering. Journal of New\nMusic Research, 29(4):303–318, 2000.\n[10] R. Cilibrasi, P. Vit ´anyi, and R. de Wolf. Algorithmic\nClustering of Music Based on String Compression.\nComputer Music Journal, 28(4):49–67, 2004.\n[11] T. Crawford, C.S. Iliopoulos, and R. Raman. String\nMatching Techniques for Musical Similarity and\nMelodic Recognition. Computing in Musicology,\n11:73–100, 1998.\n[12] E. Derr. The Two-Part Inventions: Bach’s Composers’\nVademecum. Music Theory Spectrum, 3:26–48, 1981.\n[13] P. Di Lorenzo and G. Di Maio. The Hausdorff Met-\nric in the Melody Space: A New Approach to Melodic\nSimilarity. In Ninth International Conference on Music\nPerception and Cognition, 2006.\n[14] R.A. Horn and C.R. Johnson. Matrix Analysis. Cam-\nbridge University Press, 1985.\n[15] O. Lartillot. Discovering musical patterns through per-\nceptive heuristics. Proceedings of the 4th International\nConference on Music Information Retrieval (ISMIR\n2003), pages 89–96, 2003.[16] Olivier Lartillot. A musical pattern discovery system\nfounded on a modeling of listening strategies. Comput.\nMusic J., 28(3):53–67, 2004.\n[17] Fred Lerdahl and Ray Jackendoff. A Generative Theory\nof Tonal Music. MIT Press, Cambridge, Massachusetts,\n1996.\n[18] G. Mazzola and S. M ¨uller. The Topos of Music: Ge-\nometric Logic of Concepts, Theory, and Performance.\nBirkh ¨auser, 2002.\n[19] A. Nestke. Paradigmatic Motivic Analysis. Perspec-\ntives in Mathematical and Computational Music The-\nory, Osnabr ¨uck Series on Music and Computation ,\npages 343–365, 2004.\n[20] A. Pienimaki. Indexing Music Databases Using Auto-\nmatic Extraction of Frequent Phrases. Proceedings of\nthe International Conference on Music Information Re-\ntrieval, pages 25–30, 2002.\n[21] Alberto Pinto, Reinier van Leuken, Fatih Demirci,\nFrans Wiering, and Remco C. Veltkamp. Indexing mu-\nsic collections through graph spectra. In Proceedings of\nthe ISMIR 2007 Conference, Vienna, September 2007.\n[22] E. Selfridge-Field. Towards a Measure of Cognitive\nDistance in Melodic Similarity. Computing in Musicol-\nogy, 13:93–111, 2004.\n[23] Rainer Typke, Frans Wiering, and Remco C. Veltkamp.\nTransportation distances and human perception of\nmelodic similarity. Musicae Scientiae, Discussion Fo-\nrum 4A, 2007 (special issue on similarity perception in\nlistening to music), p. 153-182.\n[24] P.F. Williams. JS Bach. Cambridge University Press.\n212\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "Multiple Pitch Transcription using DBN-based Musicological Models.",
        "author": [
            "Stanislaw Andrzej Raczynski",
            "Emmanuel Vincent 0001",
            "Frédéric Bimbot",
            "Shigeki Sagayama"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1415198",
        "url": "https://doi.org/10.5281/zenodo.1415198",
        "ee": "https://zenodo.org/records/1415198/files/RaczynskiVBS10.pdf",
        "abstract": "We propose a novel approach to solve the problem of estimating pitches of notes present in an audio signal. We have  developed  a  probabilistically  rigorous model  that takes  into  account  temporal  dependencies  between musical notes and between the underlying chords, as well as the instantaneous dependencies between chords, notes and  the  observed  note  saliences.  We  investigated  its modeling  ability  by  measuring  the  cross-entropy  with symbolic (MIDI) data and then proceed to observe the model's performance in multiple pitch estimation of audio data.",
        "zenodo_id": 1415198,
        "dblp_key": "conf/ismir/RaczynskiVBS10",
        "keywords": [
            "novel approach",
            "estimating pitches",
            "audio signal",
            "probabilistically rigorous model",
            "temporal dependencies",
            "underlying chords",
            "instantaneous dependencies",
            "cross-entropy",
            "symbolic data",
            "pitch estimation"
        ],
        "content": "MULTIPLE PITCH TRANSCRIPTION USING DBN-BASED \nMUSICOLOGICAL MODELS\nStanisław A. Raczyński*\nraczynski@Emmanuel Vincent ⁺\nemmanuel.vincent@Frédéric Bimbot ⁺\nfrederic.bimbot@Shigeki Sagayama*\nsagayama@\n*  The University of Tokyo, 7–3–1 Hongo, Bunkyo-ku, Tokyo 133-8656, Japan,  email: *hil.t.u-tokyo.ac.jp\n ⁺INRIA Rennes, Britagne Atlantique, 35042 Rennes Cedex, France, email: *inria.fr\nABSTRACT\nWe  propose  a  novel  approach  to solve  the  problem  of \nestimating pitches of notes present in an audio signal. We \nhave  developed  a  probabilistically  rigorous model  that \ntakes  into  account  temporal  dependencies  between \nmusical notes and between the underlying chords, as well \nas the instantaneous dependencies between chords, notes \nand  the  observed  note  saliences.  We  investigated  its \nmodeling  ability  by  measuring  the  cross-entropy  with \nsymbolic  (MIDI) data  and then  proceed  to observe  the \nmodel's performance in multiple pitch estimation of audio \ndata.\n1. INTRODUCTION\nThe  problem  at  hand  is  musical  note  detection ,  i.e. \nestimating pitches, onset and offset times, and, if desired, \nvelocities  of  notes  present,  often  simultaneously,  in  a \nrecorded audio signal. Typically, this problem is solved \nby  a  two-step  process  [9].  First,  pitch  candidates  are \nestimated  within  short  time  frames  and  confidence  for \neach  is  quantified  by  a  salience  measure  (see,  for \nexample, [4,6,11]). Then the salience is tracked over time \nin order to identify the musical notes.\nThe salience  can be represented by a note salience  \nmatrix  S. Its rows contain estimated power envelopes of \nnotes for different pitches, which typically correspond to \nfrequencies  of  a  diatonic  scale,  e.g. twelve-tone  equal \ntemperament scale. The activity of the underlying musical \nnotes can be expressed by a note activity matrix  N, i.e. a \nbinary matrix of the same dimensions as  S, elements of \nwhich indicate note presence at corresponding times and \npitches.\nA  standard  practice  is  to  threshold  the  estimated \nsaliences to detect notes. This step, although common, is \nquite problematic: there is no simple way to determine the \nthreshold value  and even  an optimal  value  can  lead  to \nspurious  detections  and  split  notes.  Some  of  the  false \npositives and negatives can be removed by filtering, but it \ndoes not solve the problem completely and is not elegant.\nThresholding can in fact be interpreted as a maximum \nlikelihood (ML) estimator of the note activities:\nN=arg maxNPS∣N, (1)If we assume that the detected saliences St , k are mutually \nindependent and only depend on whether a corresponding \nnote was active at that moment, we get:\nNt , k=arg maxNt , kPSt , k∣Nt ,k, (2)\nwhere  k is the piano key number and t is the time frame \nnumber.  If the  probability  distributions  PSt , k∣Nt ,k=1\nand PSt , k∣Nt , k=0 have only one crossing point T, this \nprocedure  will  be  equivalent  to  thresholding  with  the \nthreshold value equal to T.\nRecently, some researchers have used more advanced \nmusicological models in order to overcome the limitations \nof  thresholding.  Ryynänen  and  Klapuri  [9]  proposed  a \nmelody transcription method that uses a Hidden Markov \nModel (HMM) together with a simple musical key model. \nTheir approach is limited in the sense that it models only a \nsingle voice  at a time,  and so it is not probabilistically \nrigorous.  It  also  lacks  modeling  of  instantaneous \ndependencies  between  estimated  pitches.  Raphael  and \nStoddard [8] proposed to use an HMM as a musicological \nmodel  for  harmonic  analysis,  i.e. estimating  the  chord \nprogression behind  a  sequence  of notes.  Similar  HMMs \nhave also been successfully used for harmonic analysis of \naudio  signals  (for  a  recent  paper  see  e.g. [10]).  These \napproaches, however, lack note modeling and the temporal \ndependencies  are  only  present  between  chords.  A  very \ninteresting model has been presented by Cemgil  et al.  in \n[1],  but  the  presented  results  are  still  preliminary  (the \nmodel, like ours, is computationally expensive).\nIn  this  paper  we  have  proposed  a  single, \nprobabilistically  rigorous  framework  based  on  the \nDynamic Bayesian Networks (DBNs). We model both the \ninstantaneous dependencies between notes (harmony) and \nthe temporal dependencies between notes and chords. The \nnotes  our  found  with  a  maximum  a  posteriori  (MAP) \nestimator:\nN=arg maxNPS∣NPN. (3)\nThe  prior  over  the  notes  PN models  the  temporal \ndependencies  between  the  hidden  variables  (similar  to \nthose of an Hidden Markov Model) and includes a hidden \nlayer of variables representing chords.\nIn our work we used a NMF-based front-end proposed \nin [7] to obtain note salience matrices  with 88 rows that \ncorrespond to the full range of a piano: from A0 (27.5 Hz) \nto C8 (4186 Hz).\nThe model with its theoretical grounds and its practical \naspects is described in section 2. Inference of the hidden \nnotes  is  discussed  in  section 3. Experiments  involving \nsymbolic and audio data are described in section 4. and the \nconclusion is given in section 5.Permission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fee provided that copies \nare not made or distributed for profit or commercial advantage and that \ncopies bear this notice and the full citation on the first page. \n© 2010 International Society for Music Information Retrieval \n363\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)2. THE MODEL\n2.1 Structure\nDBNs provide us with complete freedom as to what set of \nprobabilistic variables and the relations between them can \nbe, and so it is a perfect tool to solve the above formu-\nlated problem. We have chosen a network structure that, \ncompared to thresholding, includes dependencies between \nhidden  variables  in  neighboring  time  frames  and  an \nadditional layer of hidden chords (see Fig. 1).\nFigure 1: Structure of the Bayesian network used in  \nexperiments\nThe network consist of 3 layer of nodes: hidden chord \nlayer  Ct,  hidden  note  combination  layer  Nt and  an \nobserved note salience  layer St.. The prior distribution of \nnotes is therefore given by:\nPN =∑\nCPC0PN0∣C0 ⋅\n⋅∏\nt=2T\nPNt∣Nt−1,CtPCt∣Ct−1. (4)\n2.2 Chord level probabilities\nFig. 4 shows the  chord  transition  probabilities  that  has \nbeen trained on the available dataset. A simple smoothing \nwas used: each element was increased by 1 after counting \nthe occurrences and before normalizing. Nevertheless, the \ndata sparsity problem is visible (especially for the minor, \nrarer, chords). To deal with this problem, chord tying was \nused: each chord transition probability was assumed to be \na function of only the interval between roots of the chords \nRt and Rt-1 , and their types Tt and Tt-1:\nPCt∣Ct−1=PRt−Rt−1,Tt, Tt−1 . (5)The motivation behind this approximation is that the \nprobability depends on relative chord positions rather than \non  the  absolute  ones.  Because  the  tonal  center  is  not \nmodeled in our approach, it is reasonable to assume the \nsame probability should be given to the transition from C-\nmajor chord to F-major (I→IV transition in C-major key) \nand the from A -major to D -major (I→IV transition in ♭ ♭  \nA -major key).♭\nThe  same  motivation  led  us  to  use  a  uniform \ndistribution as the initial chord probability distribution:\nPC0=const  . (6)\n2.3 Note level probabilities\nAnother practical problem concerning the size of the note \ncombination space is the problem of training the model's \nparameters.  The  note  combination  probability \nPNt∣Nt−1, Ctis a  discrete  distribution  with   ∣L∣2∣C∣\nparameters to train, which, even for small values of  L is \ncomputationally infeasible. To decrease the complexity of \nthe problem, we again tie together some of the parameters: \nwe  replace  that  the  note  combination  probability  an \napproximation,  in  which  it  is  factorized  into  the  note \ntransition probability  PNt∣Nt−1 and the  note emission \nprobability PNt∣Ct:\nPNt∣Nt−1,Ct≈PNt∣Nt−1PNt∣Ct\n∑\nNtPNt∣Nt−1PNt∣Ct. (7)\nThe  note  probability  distribution,  as  well  as  the  note \nemission and transition distributions, was normalized over \nall unique note combinations in the reduced search space. \nIn case of calculating joint likelihood from symbolic data, \nthe sum is performed over all note combinations present in \nthe analyzed data.\n2.3.1 Note emission probabilities\nThere  is  commonly  used  multivariate  parametric \ndistribution  over  a  discrete  set,  so  to  model  the  note \nemissions we chose a multivariate Gaussian distribution in \nthe 12-tone chroma space.\nCrt ,l=∑\nk≡lmod 12 Nt ,k (8)\nPNt∣Ct=m=N12Crt;m,m\n∑\nNtN12Crt;m,m(9)\nThe  distribution  parameters  were  estimated  on  the \nground truth data (see Fig. 3)and parameters corresponding \nto the same chord type were tied together as for the chord \nFigure 2:  Covariance matrix for the C-major chord. Note  \nthe positive high covariance between the root and the  \nperfect fifth (harmonic interval) and weak covariance \nbetween root and minor second (inharmonic interval). \n Figure 3:  Mean chroma vectors for different chords. C C\nN N\nS SC\nN\nS···\n···\n···C\nN\nS······\n···\n364\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)level probabilities. To avoid singular covariance matrices \ndue to sparse training data, chroma vectors obtained from \nreference  data  were  concatenated  with  a  smoothing \ndiagonal  matrix  pI,  where  p is  a  control  parameter \n(p = 2 was used). The chroma variance is modeled with a \nfull-rank  matrix  because  the  pitch  classes  are  not \nindependent (see Fig.  2).\n2.3.2 Note transition probabilities\nThe note transition probability PNt∣Nt−1 is responsible \nfor modeling note lengths. There are five basic kinds of \nchanges in the note combination state that can occur in \nthe  data  (depicted  in  Fig. 7):  no  change,  insertion  of \nnotes,  deletion  of  notes,  voice  movement  (one  note \nchanges pitch) and harmony movement or other complex \nchanges  (many  notes  change  pitches  simultaneously). \nBecause  in  real  life  situations  note  offsets  are  seldom \naligned with other notes' onsets, the last two situations are \nvery rare. In out training data they made up for only 0.2% \nof note  transitions types,  while  transitions in which  all \nnotes stayed the same, if we don't count the insertions and \ndeletions,  made  up  for  remaining  99.8%  of  situations. \nMotivated  by  this,  in  order  to  simplify  the  model,  we \nassumed that only the first three kinds are allowed.\nThe  note  transition  probability  is  therefore  further \napproximated with the following factorization:\nPNt∣Nt−1≈PlenLt∣Lt−1PmovNt∣Nt−1\n∑\nNtP1Lt∣Lt−1P2Nt∣Nt−1 (10)\nPmovNt, Nt−1={1 for no pitch movement\n0 for pitch movement (11)\nwhere  Lt is  the  size  of  the  current  note  combination \n(number  of  active  notes).  P1Lt, Lt−1 is  presented  in \nFig. 5.\n2.3.3 Output probabilities\nThe observed note saliences are assumed to be mutually \nindependent:\nPSt∣Nt=∏\nk∈188\nPSt ,k∣Nt ,k. (12)\nBoth obtained by measuring the histograms of the detected \nsalience (see Fig. 6).\nFigure 4:  Chord transition probability matrices:  \nwithout state tying (top) and with state tying (bottom).  \nFour quarters represent: the major-to-major (M→M),  \nminor-to-major (m→M), major-to-minor (M→m) and \nminor-to-minor (m→m) transition.Figure 7:  Five basic note combination transition  \nsituations: (a) no change, (b) insertion, (c) deletion, (d)  \nvoice movement and (e) harmony movement or other  \ncomplex changes.\nFigure 5:  Distribution of note combination length  \ntransitions. The probability matrix is “smeared” more in  \nthe area of simultaneous insertions of multiple notes  \n(e.g. at beginnings of chords) are more probable than  \nsimultaneous deletions. The z-axis is logarithmic.Figure 6: The estimated output probability . The black  \nsolid line depicts the distribution of observed note  \nsalience if the note was active ( PSt ,k∣Nt ,k=1) and \nthe red dashed line the distribution in case the note was  \ninactive ( PSt ,k∣Nt , k=0). The lines cross at about  \n-70 dB.`\n365\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)3. DECODING\n3.1 Inference\nThe problem of multiple frequency estimation becomes a \nproblem  of  inferring  the  hidden  sequence  of  note \ncombination states (and, as a side effect, the hidden chord \nprogression). In other  words, we need  to find the most \nlikely hidden state sequence ( C, N) given the model and \nthe observed note saliences S:\nC,N=arg maxC,NPC,N∣S. (13\nThis  problem  is  in  fact  directly  related  to  the  Viterbi \ndecoding in Hidden Markov Models (HMMs).\nAs  in  with  Viterbi  decoding,  a  dynamic-\nprogramming-based algorithm can be used to solve this \ninference problem for DBNs.  We refer to this algorithm \nas  modified  Frontier  Algorithm.  The  original  Frontier \nAlgorithm was proposed by Murphy in [5] to calculate the \nprobability of a given observed sequence  (equivalent of \nthe HMM's Forward-Backward algorithm). Murphy noted \nthat  it  can  easily  be  modified  to  calculate  the  most \nprobable  sequence  of  hidden  states  in  any  finite-state \nDBN, i.e. solve our inference problem.\n3.2 Reduced solution space\nNt is a variable that holds a list of notes active at a certain \ntime (or, equivalently, a vector of binary note presence \nindicators). The number of all possible values (states) of \nNt  is enormous: 3.1× 1026 if we limit the musical range \nto that of a 88-key piano. Even if we limit the number of \nsimultaneously active notes to K=10 (if no sustain pedal is \nused this is the physical limit for a single piano player), it \nis still computationally infeasible: 5.2×1012 if K=10 and \n4.2×107 if K=5.\nTo  deal  with  this  problem,  we  reduce  the  solution \nprior  to  inferring  the  hidden  sequence:  for  each  time \nframe  only  the  most  probable  note  combinations  are \nconsidered.  To  identify  the  most  probable  note  combi-\nnations, first, for each  time  frame,  we select  K highest \nelements, or  note candidates . Then, a list of all 2K  pos-\nsible note combinations is created and each such combi-\nnation is evaluated with a fitness function . Finally, the  L \nfittest note combinations are selected and used for further \nanalysis. Additionally, a rest (empty note combination) is \nalways selected.\nThe  fitness function  was designed  to penalize  long \nnote  combinations  (note  combinations  containing  many active  notes)  while  rewarding  better  explanation  of  the \nobserved note saliences St:\nFNt=∑\nk∈{k:Nt , k=1}St , k\n∑\nk=188\nSt , k∣Nt∣−a\n (14)\nwhere  Nt is  the  note  combination  for  the  current  time \nframe  and  a is  a  control  parameter.  A  similar  fitness \nfunction was used by Klapuri in [3].\nLimiting the solution space poses a threat to the note \nestimation  process:  if  the  real  note  combination  is  not \nselected due to fluctuations in the note salience, the lan-\nguage  model  will  not  be  able  to  compensate  for  that. \nTherefore,  to avoid some  of the  deletions,  the  observed \nnote  saliences  are  pre-filtered  with  a  causative  moving \naverage (MA) filter:\nSt ,k=∑\n=0R\nSt−, k (15)\nAdditionally, this filtering removes short spurious peaks, \ne.g. the ones around onset times resulting from the wide-\nband onset noise. Unfortunately, it also smooths out the \nonsets.\nWe have analyzed  how much of the ground truth is \ncontained within the reduced solution space, depending on \nthe chosen K, L and a, and on the chosen MA filter order \n(length),  by  measuring  the  note  recall.  The  results  are \npresented in Fig. 8. Optimal values were determined to be \nR = 20 (400 ms) and a = 0.65 (similar to Klapuri's [3]).\n3.3 Fudging\nTo  gain  additional  control  over  the  behavior  of  the \nalgorithm, a set of fudge factors was introduced:\nPNt∣Nt−1,Ct≈PNt∣Nt−1PNt∣Ct\n∑\nNtPNt∣Nt−1PNt∣Ct (16)\nEach factor controls the influence of individual probability \ndistribution  on  the  algorithm.  The  first  factor  controls \nmainly the ratio between the self-transition probability of \nNt and the cross-transition probability, so smaller values \nare better for slower pieces and bigger values for higher \ntempo.\nThe  values of first two factors  were  then  optimized \nempirically  by  maximizing  the  joint  likelihood  of  the \nhidden note variables PN (see Fig. 9) and found to be \n=1.05  and =0.0015 . The fact that the first factor is \nclose  to  one  does  not surprise,  because  the  Gaussian  is \nvery sparse due to high dimensionality. A very small value \nFigure 8:  Note recall for different values of N and L.  \nData obtained for a = 0.65 and R = 20.\nFigure 9:  Optimization of the fudge factors α and β.\n366\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)of β is due to a very high sparseness of the note emission \ndistribution,  i.e. small number of note combinations are \nassigned significantly higher probability values than the \nothers (which is a result of the curse of dimensionality).\n4. EXPERIMENTS\n4.1 The dataset\nThe data used in the experiments comes from the widely \nused RWC database [2]. We have used 19 pieces from the \nclassical portion of the dataset (listed in Table 1).\nAs  a  joint  effort  of  the  University  of  Tokyo's \nSagayama  Laboratory  and  the  Toho  Gakuen  School  of \nMusic (under the supervision of prof. Hitomi Kaneko), the \nclassical pieces of the RWC database were annotated with \ndetailed harmony labels that include: keys and modula-\ntions, and chords with their roots, inversions, types and \nmodifications. This data uses abstract musical time (mea-sures and beats), so, additionally, manual labeling of the \nRWC's audio data was performed.\nUnfortunately,  the  RWC  database's  MIDI  and  audio \nfiles are not synchronized. What is more, it is not only a \nmatter of linear time transformation, but rather a complex \none. Further  synchronization with the MIDI was needed \nfor  the  purpose  of  training  model  parameters  (note \nemission probabilities). This was done automatically with \ndynamic time warping (DTW).\n4.2 Symbolic data\nA simple procedure to evaluate the proposed approach is \nto measure how well does our Bayesian network model the \nsymbolic  data.  This  can  be  assessed  by  calculating  the \nlikelihood of the data given the model PN.\nSix variants of the proposed model were evaluated:\n(a) Reference uniform model\nPN=∏\nt=1T\nPNt=AT (17)\n(b) Harmony model only\nPN=∑\nC∏\nt=1T\nPNt∣Ct (18)\n(c) Harmony + chord progression\nPN =∑\nCPC0PN0∣C0⋅\n⋅∏\nt=2T\nPNt∣CtPCt∣Ct−1(19)\n(d) Note duration model only\n PN=PN0∏\nt=2T\nPNt∣Nt−1 (20)\n(e) Duration + harmony\nPN=∑\nCPC0PN0∣C0∏\nt=2T\nPNt∣Nt−1,Ct (21)\n(f) Duration + harmony  + chord progression\nPN =∑\nCPC0PN0∣C0⋅\n⋅∏\nt=2T\nPNt∣Nt−1, CtPCt∣Ct−1(22)\nThe  variants  are  presented  graphically  in  Fig. 10.  The \nfrontier algorithm was used to evaluate the likelihood for \nmodel  variants  with  hidden  variables.  Each  model  was \nevaluated  by  calculating  the  cross-entropy,  i.e.  the \nnormalized log-likelihood of the data N given the model:\nEN=−1\nTlog2PN (23)\n4.3 Note detection\nTo evaluate the results of multiple frequency estimation, \nthe F-measure was calculated by comparing the detected \nnotes with the ground truth. A note was considered detec-\nted ( true positive ) if its onset was within 100 ms from a \ntrue note onset. By measuring the number of true positives, \nfalse positives  (spurious notes) and false negatives  (unde-\ntected  notes),  the  precision,  recall  and  F-measure  were \ncalculated.\nFig. 11 depicts preliminary note detection results ob-\ntained for 7 different models. The first two models were \nsimple thresholding with -40 dB (optimal threshold, deter-RWC Composer Instrument Length\n1 22 Brahms 2 pianos 2:25\n2 23A Ravel Piano 1:20\n3 23B 〃 Piano 2:45\n4 23C 〃 Piano 3:25\n5 23E 〃 Piano 4:09\n6 24A Bach Harpsichord 1:26\n7 24B 〃 Harpsichord 1:29\n8 24C 〃 Harpsichord 0:52\n9 25A 〃 Harpsichord 2:03\n10 25B 〃 Harpsichord 2:11\n11 25C 〃 Harpsichord 1:31\n12 29 Schumann Piano 2:25\n13 30 Chopin Piano 4:02\n14 31 〃 Piano 4:16\n15 32 〃 Piano 1:49\n16 35A Satie Piano 3:49\n17 35B 〃 Piano 3:01\n18 35C 〃 Piano 2:46\n19 40 Massanet Piano + violin 5:06\nTotal: 50:50\nTable 1: RWC pieces used in the experiments.\n(a)(b)\n(c) (d)\n(e)(f)\nFigure 10:  Six variants of the model used in the  \nevaluation.NC\nN\nC C\nN NC C\nN N\nN NC C\nN N\n367\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)mined  empirically)  and  -70 dB (crossing point  between \nthe  output  probability  distributions).  In the  third model \nthe note were detected based on the trained output proba-\nbility,  but  only  from  the  reduced  solution  space.  This \nmeans that no prior on the notes was present (no language \nmodel) and this model was equivalent to the model (a) \nfrom subsection 4.2. The last 4 models correspond to the \nones described in subsection  4.2, but with the note vari-\nables hidden and the note salience layer on the bottom. \nThe proposed model performed not worse than threshol-\nding  and  generally  yielded  better  recall,  but  worse \nprecision. The results for RWC-C24A were significantly \nimproved over the thresholding, which can be attributed \nto  the  fact  that  this  piece  is played  on  a  harpsichord, \nwhich has very strong overtones that were mistaken for \npitches. The proposed model was able to remove most of \nthese thanks to the prior distribution on the notes.\n5. CONCLUSION\nWe have proposed a uniform probabilistic framework that \nestimates note onsets and pitches from a salience matrix \nobtained by a pitch estimation front-end. The model was \nevaluated on symbolic (MIDI) data and, preliminarily, on \naudio signals. The results show significant improvement \nof the model over a reference model with uniformly and \nindependently distributed notes, with the biggest improve-\nment coming from using temporal dependencies.\nCompared  to  the  thresholding,  the  estimation  was \nmore  robust  and  yielded  higher  precision,  though  the \nrecall was sometimes lower.\nIn future we plan to focus on improving the accuracy \nand, therefore, the impact of the simultaneous pitch model \nPNt∣Ct. We would also like to explore the possibili-\nties of unsupervised training that would allow us to use a \nmuch larger training set, but also investigate the influence \nof  the  chosen  chord  dictionary  size  (for  example, \ncommonly used chord dictionaries are: 24, 48 [10], 168 \n[8] and 288,  i.e.  12 keys × 24 chords, but even larger \ndictionaries are possible).\n6. ACKNOWLEDGMENT\nThis  work  is supported  by  INRIA  under  the  Associate \nTeam Program VERSAMUS (http://versamus.inria.fr/).\n7. REFERENCES\n[1] A. Cemgil, H. Kappen, D. Barber, S. Netwerken and \nN.  Nimegen:  “A  generative  model  for  music \ntranscription,”  in IEEE trans. ALSP , vol. 14, nr. 2, \npp. 679–694, 2006.\n[2] M. Goto, H. Hashiguchi, T. Nishimura, and R. Oka, \n“RWC Music Database: Music Genre Database and \nMusical  Instrument  Sound  Database,”  in  Proc.  \nISMIR , pp. 229–230, 2003.\n[3] D. Kawakami, H. Kaneko, S. Sagayama: “Developing \nfunctional  harmony  labeled  data  and  its  statistical \nanalysis,”  in Proc. ASJ Spring Meeting , p. 2, 2010. \n(in japanese)\n[4] A.  Klapuri:  “Multiple  fundamental  frequency \nestimation  by  summing  harmonic  amplitudes,”  in \nProc. ISMIR , pp. 216–221, 2006.[5] K.  Murphy:  “Dynamic  bayesian  networks:  repre-\nsentation, inference and learning,” PhD thesis, 2002.\n[6] A.  Pertusa  and  J.M.  Iñesta:  “Multiple  fundamental \nfrequency estimation using Gaussian smoothness,”  in \nProc. ICASSP , pp. 105–108, 2008.\n[7] S. Raczyński, N. Ono, S. Sagayama: “Extending Non-\nnegative  Matrix  Factorization  –  a  discussion  in  the \ncontext  of multiple  frequency  estimation  of musical \nsignals,'' in Proc. EUSIPCO , pp.934–938, 2009.\n[8] C. Raphael and J. Stoddard: “Harmonic Analysis with \nProbabilistic Graphical Models,”  in Proc. ISMIR , pp. \n177–181, 2003.\n[9] M.  Ryynänen  and  A.  Klapuri:  “Modelling  of  Note \nEvents for Singing Transcription,” Proc. ITRW , 2004\n[10]Y.  Ueda,  Y.  Uchiyama,  T.  Nishimoto,  N.  Ono,  S. \nSagayama,  “HMM-based  Approach  for  Automatic \nChord Detection Using Refined Acoustic Features,” in \nProc. ICASSP , 2010.\n[11]E.  Vincent,  N.  Bertin,  and  R.  Badeau:  “Adaptive \nHarmonic Spectral Decomposition for Multiple Pitch \nEstimation,” in IEEE Trans. ASLP , vol. 18, nr. 3, pp. \n528–537, 2010\nFigure 11:  Note detection results for 7 different models  \nobtained for RWC-C24A (top, α = 2.3, L = 12, N = 70) \nand RWC-C22 (bottom, α = 1.3, L = 12, N = 75).\nFigure 12:  Average cross-entropy between symbolic  \ndata and different variants of the model.\n368\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "Symbol Classification Approach for OMR of Square Notation Manuscripts.",
        "author": [
            "Carolina Ramirez",
            "Jun Ohya"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1415122",
        "url": "https://doi.org/10.5281/zenodo.1415122",
        "ee": "https://zenodo.org/records/1415122/files/RamirezO10.pdf",
        "abstract": "Researchers in the field of OMR (Optical Music Recogni- tion) have acknowledged that the automatic transcription of medieval musical manuscripts is still an open problem [2, 3], mainly due to lack of standards in notation and the physical quality of the documents. Nonetheless, the amount of medieval musical manuscripts is so vast that the consensus seems to be that OMR can be a vital tool to help in the preserving and sharing of this information in digital format. In this paper we report our results on a preliminary approach to OMR of medieval plainchant manuscripts in square notation, at the symbol classification level, which produced good results in the recognition of eight basic symbols. Our preliminary approach consists of the pre- processing, segmentation, and classification stages.",
        "zenodo_id": 1415122,
        "dblp_key": "conf/ismir/RamirezO10",
        "keywords": [
            "OMR",
            "medieval musical manuscripts",
            "open problem",
            "notation",
            "physical quality",
            "preserving",
            "sharing",
            "digital format",
            "symbol classification",
            "basic symbols"
        ],
        "content": "Symbol Classification Approach for OMR of Square Notation \nMan uscripts  \nCarolina Ramirez  Jun Ohya  \nWaseda University  \nramirez@akane.waseda.jp  Waseda University  \nohya@waseda.jp \nABSTRACT  \nResearchers in the field of OMR (Optical Music Recogn i-\ntion) have acknowle dged that the automatic tra nscription \nof medieval musical manuscripts is still an open problem  \n[2, 3], mainly due to lack of standards in notation and the \nphysical quality of the documents. Nonetheless , the \namount of medieval m usical manuscripts is so vas t that \nthe co nsensus seems to be that  OMR can be a vital tool to \nhelp in  the preserving and sharing of this information in \ndigital form at. \nIn this paper we report our results on a preliminary \napproach to OMR of medieval plainchant man uscripts in \nsquare not ation, at the symbol classification level, which \nproduced good  results in the recognition of  eight basic \nsymbols.  Our preliminary approach consists of  the pr e-\nprocessing, segmentation , and class ification stages.  \n1. INTRODUCTION  \nSeveral groups  are currently wor king to build digital a r-\nchives and catalogues  using digital technologies [10, 11, \n12, 13, 14] , of the huge number of early mus ical man u-\nscripts accessible  from multiple sources.  The lines of r e-\nsearch of these groups in early music information re-\ntrieval  range from the design of web prot ocols for digital \nrepresentation of scanned early music sources to the \nautomatic transcri ption of those sources through ada ptive \ntechniques [2, 5, 9, 10].  Given the physical and s emantic \ncharacteristics of many of these documen ts (de gradation, \nnon-standard notation, etc.), great variability is intr o-\nduced to the data, and the subs equent analysis can be a \nquite difficult and time consuming task, usually requi ring \nadvanced expert knowledge. So, until very recently, those \nmentioned efforts were r estricted mostly to build text \ncatalogues and repos itories of scanned images.  \nIn the case of standard modern music notation, OMR \nhas achieved high levels of accuracy, and there are se v-\neral OMR systems commercially available [1, 15]. In the \ncase of early music manuscripts, attempts to achieve good \nOMR r esults become  more challenging as our sources go \nback in time. Still, researchers have e xtended their work \nto early music manuscripts, and in the past years we have \nobserved advances in renai ssance printed music and \nhandwritten music [4, 5, 17], but still little has been r e-ported about experimental results with western plainchant \nmedieval sources [2]. The work done by the NEUMES \nproject [10] , and most recently by Burgoyne et al. [3] , are \namong th e few experimental results with this particular \ntype of source . In [2] the problem of non -standard not a-\ntion is me ntioned as the most critical issue for early \nmanuscript OMR. For this reason, we start our research  \nby restricting the manuscripts in square no tation to be-\nlong to  the XIV century  and later , when square notation \nwas already an e stablished practice and basic symbols \nwere more standardized than in previous neumatic alph a-\nbets [16].  \nIn this paper we aim to successfully cla ssify the eight \nbasic charact ers of western square notation, see Figure 1, \nusing relatively simple and widely known image proces s-\ning and pattern recognition algorithms. If this proves su c-\ncessful, we believe that more complex models, context \ninformation , and adaptive techniques can be used in the \nfuture to minimize the e rrors at th e classification  stage, to \nextend the span of examples that can be an alyzed , i.e. less \nstandard documents,  and to include a whole semantic \nanalysis.  \n \n  \n  \n  \nclivis  climacus.  pes scandicus  \n \n \n  \n  \npunctum  porrectus torculus virga  \nFigure 1: Square notation basic symbols.  \nFinally, it is necessary to mention that a big concern \nin this research area is the evaluation methods to be used. \nSymbol classification can be evaluated  using  the us ual \ntechniques, but creating a ground -truth for a full man u-\nscript (where even the experts sometimes disagree) would \nrequire an effort that is beyond the scope of this paper.  \n2. OUTLINE  \nIn section 3 we describe the preprocessing stage, which \nincludes binarizat ion of the manuscript image, location of \nstaff lines and staves that define our ROI ( Region of I n-\nterest ), and stave  deskewing . In section 4 we describe our \nsegmentation and classification strategy. Lastly, in se c-\ntion 5 we present our conclusions and deline ate some f u-\nture work ideas.    \nPermission to make digital or hard copies of all or part of this work for \npersonal or classroo m use is granted without fee provided that copies \nare not made or distributed for profit or commercial advantage and that \ncopies bear this notice and the full citation on the first page.   \n© 2010  International Society for Music Information Retrieval  \n549\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \n3. PREPROCESSING  \n3.1 Binarization and ROI Extraction  \nAs we said above , one of the biggest difficulties in \nanalyzing early music manuscripts comes from the high \nvariability on the image data introduced by the deteri o-\nrated state of the documents [9] . Besides dealing with a \nnon-standard notation or non -standard scanning methods, \nthe physical condition of some documents (high degrad a-\ntion, discoloration, mis sing parts, etc.) call s for an ad e-\nquate amount of preprocessing. Some possibilities for the \npreprocessing stage include filtering, spatial transforms \n(Hough transform has been proposed to co rrect staff line \npositions [5]), and adaptive thresholding.  \nIn order to binarize and extract the ROI  we implement \nthe adaptive approach proposed by Ga tos et al. in [6]. The \nmain adva ntage of this method is that it is able to deal \nwith degradations due to shadows, non -uniform illumin a-\ntion, low contrast, smear , and strain. The disadvantage is \nthat it is a parametric method, and in order to obtain good \nresults some amount of parameter tuning is required [4]. \nThe steps i nclude an initial denoising using a 3x3 Wiener \nfilter, a rough foreground estimation using Sauvola’s L o-\ncal Ada ptive Threshold, a background estimation, and a \nfinal local thresholding using th e distance between the \nWiener filtered image and the background estimation. We \ndid not implement the up -sampling stage in [6], because \nprelim inary tests showed that it was not critical to detect \nour ROI.  \nThe original image I, the filtered image Iw, the bac k-\nground image Ib, and the final binary image If are shown  \nin Figure 2.  \n \n \nOriginal image I \n \nFiltered imaged Iw \n \nBackground image Ib \n \nFinal binary image If \nFigure 2: Binarization stages . \nWe use the binary image If to detect our  region of i n-\nterest, the area of  the image where the relevant symbols are located, which in a mus ical document is a stave, i.e. a \ngroup of staff lines. There can be many staves in one \ndocument and we want to extract each one of them sep a-\nrately. This also h elps to minimize the pre sence of text \nand drawings in the analyzed images, el ements that could \nmake our analysis more difficult.  \nAs an initial approach, we perform a rough localiz a-\ntion of the staff lines by first detecting the positions of all \nthe lines in  the document using polar Hough transform. \nAfter the lines are e xtracted, we use another feature to \ndecide if a group of lines is a stave. This feature is the \nspace between lines, which can be also estimated from \nthe Hough transform. Here we use the hypoth esis that \nspaces b etween staff lines on the same stave are relatively \nsmaller than the space between staves. We use a k -means \nclassifier to group the spaces and detect the staves. Figure \n3 shows an example of stave d etection. Only whole \nstaves will be extr acted, so staff lines that do not form a \ncomplete stave are not consi dered as part of the ROI.  \n \n \nFigure 3: Stave detection.  \nIn Figure 3 it can be noticed that the whole length of \nthe stave is not detected. To solve this problem we  use \nheuri stics based on the inter -staff line and inter -staves \nspaces and the dimensions of the image.  \n3.2 Staves Deskewing  \nMany OMR algorithms assume that staff lines are hor i-\nzontal, but this is not necessarily true in old man uscripts.  \n \nFigure 4: Aligned staves.  \nIn order to facilitate the analysis , and in case we want \nto apply  standard  OMR techniques , it is useful to hor i-\nzontally align the images as much as possible. This can \nbe done with the information already obtained from  the \nHough transform, by rotating against the Hough angle. \nThe result of applying this rotation can be seen in Figure  \nPerm ission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fee provided that copies \nare not made or distributed for profit or commercial advantage and that \ncopies bear this notice and the full citatio n on the first page.   \n© 2010  International Society for Music Information Retrieval  \n550\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \n4. Note  that this approach does not address the issue of \ndeformed staff lines . \n4. SEGMENTATION AND CLA SSIFICATION  \nAs explained  in the introduct ion, we aim to obtain good \nsymbol classification results while at the same time using \na relatively simple methodology. In general, the standard \napproach is to binarize the document and then segment \nand classify the symbols using binary representations. We \ncanno t use this approach because, even though the binar i-\nzation we used above allows us to find the region of i n-\nterest in the image, it is not accurate enough to conserve \nall the pixel information of the sy mbols across all the \ndocuments in our database. Hence, w e carry out the se g-\nmentation directly from the extracted staves in grayscale. \nDue to the difficulty in removing lines from heavily d e-\ngraded and deformed documents, we d ecided to skip the \nstaff lines removal stage , and thus avoid a pixel -wise a p-\nproach for s ymbol segmentation. Instead , we detect and \nsegment whole symbols using pattern matching via corr e-\nlation, and then we use a SVM ( Support Vector Machine ) \nto classify the symbols from gradient -based features.  \n4.1 Segmentation  \nWe use normalized correlation s on eac h stave image to \nmatch an artificially generated binary pa ttern of each \nsymbol to the regions where that symbol potentially a p-\npears . Some of the binary patterns can be seen in Fi gure \n1, but the classes  that present more variability in size and \ngeometrical distribution ( pes, torculus , porre ctus, clivis ) \nare also divided in subclasses. These patterns were a p-\nplied in 3 di fferent scales, based in the height of the stave, \nto each stave image. After this process, a set of d etected \ncandidates is obtained. These ca ndidates are the input for \nthe SVM. An example of this process is shown in Figures \n5, 6, and 7.  \n \nFigure 5: From top to bottom. Grayscale stave, \nnormalized correlation image, and peaks of the \ncorrelation image.  \n \nFigure 6: Pattern Detection, class virga . \n \nFigure 7: Segmented symbols, class virga  (left, \nfalse detection) . \nAfter testing our detection algorithm in real doc u-\nments [14], we observe that  all basic symbols were d e-\ntected wi th the b inary patterns, but also many “false” \ncandidates were e xtracted. These “false” candidates were \nmainly due to two causes: first, a basic pattern is actually \npart of another one, and second, a geome trical configur a-\ntion similar to the basic pattern is  formed by certain el e-\nments in the document. Examples of both cond itions are \nshown  in Figure 8.  \n \n  \nFigure 8: Left, false pes detection (part of \nscandicus ). Right, false torculus  detection (part of \nporrectus flexus ). \n4.2 Classification  \nFor Classification purposes, 1334 sample images of the 8 \nbasic symbols were manually segmented and labeled \nfrom 47 sheets of music available at the Digital Script o-\nrium [14]. These sources are square notation manuscripts \nfrom the XIV to the XVII centuries (to avoid trans itional \ntimes [16]), and from different geographical loc ations \n(Spain, Germany, Italy, etc.).  \nA size and position normalization using aspect ratio \nwas performed on the samples [7], and 4 directional \nSobel masks were applied to them (horizont al, vertical, \nleft-diagonal, and right -diagonal) to obtain the gradient -\nbased features used for classification. These Sobel images \nwere divided in 96 blocks, and the mean gradient for each \nblock was calc ulated. Finally, all the values were stacked \nin a fe ature vector [8].  \nWe trained a SVM with a quadratic kernel function, \nand we tested it using cross -validation. The training was \nmade using a one-against -all approach, thus obtaining a \nclassifier for each of the eight classes. A simple voting \nalgorithm is use d to decide the final class from the ou t-\nputs of the eight independent classifiers. Three exper i-\n551\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \nments were conducted, each with a different type of i n-\nput. In the first experiment, we used grayscale samples \nwithout any quality enhancement, in the second expe ri-\nment we used grayscale samples with contrast enhanc e-\nment, and in the third experiment we used binary sa m-\nples. R esults are shown in Table 1.  \n \nSample  Recall  \nBinary  0.8453  \nGrayscale  0.9208  \nContrast enhanced  0.9610  \nTable 1: Classi fication rates for SVM cross -\nvalidation experiments. Values range from 0 to 1.  \n \nTable 2 shows the test results from 3000 independent \nexamples, by class, for co ntrast-enhanced samples.  \n \nClass  Precision  Recall  \nClivis  0.9331  0.9914  \nClimacus  0.9429  0.9519  \nPes 0.9646  0.9542  \nPunctum  0.9674  0.9132  \nPorrectus  0.8476  0.8580  \nScandicus  0.8667  0.8228  \nTorculus  0.9261  0.9482  \nVirga  0.9744  0.9311  \nTable 2:  Classification results for co ntrast-\nenhanced samples . Values range from 0 to 1.  \nThe candidates extracted from Section 4.1 were tested \nin the most successful of the three SVMs, with good cla s-\nsification rates. In the case of  “false” candidates, the \nclassifier is currently not capable of discern them as a di f-\nferent class, i.e. a class of  “wrong” samples independent  \nof the 8 basic classes.  \n \n5. SUMMARY AND DISCUSSI ON \nWe believe that our results, while not being completely \nconclusive, show that using a gradient -based feature ge n-\nerates good classification results of square n otation at the \nsymbol level provided  the results from both detection and \nsegmentation stage s are good . When combining the d e-\ntection stage with the classification stage, the perfor m-\nance is degraded by the presence of “false” detections o b-\ntained with the normalized correlation pattern matching. \nHowever, ev en if these r esults are not ideal, we consider \nthat the errors in the classif ication  of the “false” cand i-\ndates can be reduced if we introduce  two valuable el e-\nments into the analysis. The first element is the use of the \nredundancy in the detection , i.e. when two or more ca n-\ndidates are extracted from similar or overlapping pos i-\ntions in the i mage; the second element is the use of the \ncontext in which the symbol is found . In the first case, the \nsole presence of redundancy will alert us to the occu r-rence of an a bnormal situation, and therefore allow us to \nact on it accordingly. In the second case, context inform a-\ntion can be used to minimize errors: think of a basic pa t-\ntern being part of a nother (for the worst case scenario, \nthink of a punctum !). In that case, obs erving the context \nis essential to obtain complete i nformation about the \nsymbol under analysis, and be able to determine its co r-\nrect class.  \nIn terms  of future work, our first concern is to i m-\nprove the segmentation via pattern matching, without r e-\nnouncing t o other segmentation techniques. It is quite i n-\ntuitive to imagine that some classes are more di fficult to \ndeal with. For instance, we observed that in many cases \nthe classes virga  and punctum  were detected as the other, \nwhich makes us think that the charac teristic stem of the \nvirga  has a weak influence in the normalized corr elation \npattern matching.  \nFinally, we believe that a robust analysis of these \nmanuscripts cannot be completely achieved without also \ntaking in account semantic context information. In ge n-\neral terms, p lainchant is a sequence of sounds and rhyt h-\nmic patterns evolving in time, and as such, models or \ntechniques that deal with time sequences look like an a t-\ntractive altern ative to complement the symbol -based \nanalysis and i mprove error management  strat egies. We \nknow that certain rules are observed in Gregorian Chant, \nso, if some probabilistic rules can be derived from its s e-\nmantics, even soft ones, we would like to undertake that \ndirection of r esearch.  \n \n6. ACKNOWLEGMENTS  \nWe would like to thank the Fr ee Library of Philadelphia, \nRare Book Department, for granting their permission to \nreproduce i mages from their repository [18].  \n7. REFERENCES  \n[1] Bainbridge, D. and Bell, T. The Challenge of \nOptical Music Recognition. Computers and the \nHumanities, No 35, pp95 -121. 2001 . \n[2] Barton, L.W. G., Caldwell, J. A. and Jeavons, P. G. \nELibrary of Medieval Chant Manuscript \nTranscriptions . Proceedings of the 5yth ACM/IEEE \nJoint Conference on Digital Libraries (Digital \nLibraries Cyberinfraestructure for Research and \nEducation). A ssociation for Computing Machinery. \n2005, pp320 -329. \n[3] Burgoyne, J.A., Y. Ouyang, T. Himmelman, J. \nDevaney, L. Pugin, and I. Fujinaga. Lyric extraction \nand recognition on digital images of early music \nsources . Proceedings of the 10th International \nSociety fo r Music Information Retrieval Conference \n(ISMIR 2009)  2009 . \n552\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \n[4] Burgoyne, J. A., L. Pugin, G. Eustace, and I. \nFujinaga. 2007. A comparative survey of image \nbinarisation algorithms for optical recognition on \ndegraded musical sources. Proceedings of \nInternationa l Conference on Music Information \nRetrieval . Vienna. 509 –12. \n[5] Fornes, A., Llados, J. & Sanchez, G. Primitive Se \nmentation in Old Handwritten Music Scores . Lecture \nNotes in Computer Science, vol. 3926, pp. 279 -290. \n2006.  \n[6] Gatos, B., Pratikakis, I.E., Peranton is, S.J. Adaptive \ndegraded document image binarization . Pattern \nRecogn ition, Vol.39, No. 3, pp. 317 -327. March \n2006.  \n[7] CL Liu, K Nakashima, H Sako, H Fujisawa. \nHandwritten Digit Recognition: Investigation of \nNormalization and Feature Extraction Techniques. \nPattern Recognition, vol. 37, pp. 265 -279.2004 .  \n[8]  CL Liu, K Nakashima, H Sako, H Fujisawa. \nHandwritten Digit Recognition: Benchmarking of \nState_of_the_Art Techniques . Pattern Recognition, \nvol. 36, pp. 2271 -2285.2003  \n[9] Pugin, L., Burgoyne, J.A. & Fujinaga, I. MAP \nAdaptation to Improve Optical Music Recognition of \nEarly Music Documents Using Hidden Markov \nModels . Pr oceedings of the 8th International \nConference on Music Information Retrieval (ISMIR \n2007), pp. 513 -16. V ienna, Austria.  \n[10] NEUMES Project  \nhttp://www.scr ibeserver.com/NEUMES/index.html  \n[11] CANTUS Database   \nhttp://publish.uwo.ca/~cantus/  \n[12] The CAO -ECE Project  \nhttp://www.zti.hu/earlymusic/cao -ece/cao -ece.html  \n[13] Cantus Planus Study group  \nhttp://www.cantusplanus.org/  \n[14] Digital Scriptorium  \nhttp://www.digital -scriptorium. org \n[15] OMR Systems  \nhttp://www.informatics.indiana.edu/donbyrd/OMRS\nystemsTable.html  \n[16] Nota Quadrata  \nhttp://notaquadrata.ca/index.html  \n[17] Aruspix  \nhttp://www.aruspix.net/project.html  \n[18] Lewis E M 73:13v.  Used by permission of the rare \nBook Department, Free Library of Ph iladelphia.  \n \n \n553\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "Concurrent Estimation of Chords and Keys from Audio.",
        "author": [
            "Thomas Rocher",
            "Matthias Robine",
            "Pierre Hanna",
            "Laurent Oudre"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1417485",
        "url": "https://doi.org/10.5281/zenodo.1417485",
        "ee": "https://zenodo.org/records/1417485/files/RocherRHO10.pdf",
        "abstract": "This paper proposes a new method for local key and chord estimation from audio signals. A harmonic content of the musical piece is first extracted by computing a set of chroma vectors. Correlation with fixed chord and key templates then selects a set of key/chord pairs for every frame. A weighted acyclic harmonic graph is then built with these pairs as vertices, and the use of a musical distance to weigh its edges. Finally, the output sequences of chords and keys are obtained by finding the best path in the graph. The proposed system allows a mutual and beneficial chord and key estimation. It is evaluated on a corpus com- posed of Beatles songs for both the local key estimation and chord recognition tasks. Results show that it performs better than state-of-the art chord analysis algorithms while providing a more complete harmonic analysis.",
        "zenodo_id": 1417485,
        "dblp_key": "conf/ismir/RocherRHO10",
        "keywords": [
            "local key",
            "chord estimation",
            "audio signals",
            "harmonic content",
            "chroma vectors",
            "fixed chord",
            "key templates",
            "weighted acyclic harmonic graph",
            "musical distance",
            "best path"
        ],
        "content": "CONCURRENT ESTIMATION OF CHORDS AND KEYS FROM AUDIO\nThomas Rocher, Matthias Robine, Pierre Hanna\nLaBRI, University of Bordeaux\n351 cours de la Libration\n33405 Talence Cedex, France\n{rocher,robine,hanna}@labri.frLaurent Oudre\nInstitut TELECOM, TELECOM ParisTech\n37-39 rue Dareau\n75014 Paris, France\noudre@telecom-paristech.fr\nABSTRACT\nThis paper proposes a new method for local key and chord\nestimation from audio signals. A harmonic content of the\nmusical piece is ﬁrst extracted by computing a set of chroma\nvectors. Correlation with ﬁxed chord and key templates\nthen selects a set of key/chord pairs for every frame. A\nweighted acyclic harmonic graph is then built with these\npairs as vertices, and the use of a musical distance to weigh\nits edges. Finally, the output sequences of chords and keys\nare obtained by ﬁnding the best path in the graph.\nThe proposed system allows a mutual and beneﬁcial\nchord and key estimation. It is evaluated on a corpus com-\nposed of Beatles songs for both the local key estimation\nand chord recognition tasks. Results show that it performs\nbetter than state-of-the art chord analysis algorithms while\nproviding a more complete harmonic analysis.\n1. INTRODUCTION\nHarmony, like rhythm, melody or timbre, is a central as-\npect of Western music. This paper focuses on chord se-\nquences and key changes, which are strong components of\nthe harmony. Audio chord transcription has been a very\nactive ﬁeld for the past recent years. In particular, the in-\ncreasing popularity of Music Information Retrieval (MIR)\nwith applications using mid-level tonal features, has estab-\nlished chord transcription as useful and challenging task.\nAmong the numerous chord recognition methods, we can\ndistinguish four main types of systems. The ﬁrst ones can\nbe referred as template-based methods [6, 9, 14], since a\ncentral information they need to perform the transcription\nis the deﬁnition of the chords they want to detect. Working\njust like pattern recognition methods, they choose for ev-\nery frame the chord whose template ﬁts the best the data.\nThe temporal structure of the song is often captured thanks\nto post-processing methods working either on the sequence\nof detected chords or on the calculated ﬁtness features.\nOther methods rely on musical information (such as rhythm\nor musical structure) in order to capture a harmonically\nrelevant chord transcription. These music-based methods\n[2, 12], implicitly or explicitly exploit information from\nmusic theory in the construction of their systems. In partic-\nular, the transitions between chords or the rhythmic struc-\nture are often modeled with parameters reﬂecting musical\nknowledge, by estimating the likelihood of a given chord\nbeing followed by a different chord, for example. Some\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2010 International Society for Music Information Retrieval.data-driven methods [10, 17], use completely or partially\nannotated data in order to build a system which ﬁts the\naudio data. In these methods, all the parameters are eval-\nuated with training. Finally, some systems merge music-\nand data-based approaches in order to build hybrid meth-\nods[15, 16], which combine the use of training data and\nmusic theory knowledge.\nAll these methods have the opportunity to compare to\neach other in the MIREX [4], which is an annual community-\nbased framework for the evaluation of MIR systems and\nalgorithms. In 2009, the results for the audio chord detec-\ntion were pretty close and the different methods seemed\nto compete at the same level of accuracy. The aim of this\nto work is to offer a chord estimation with a comparable\nlevel of accuracy, and estimating a sequence of local keys\nas well as chords.\nFewer works were achieved to estimate musical keys\nfrom audio, and the vast majority of them only consider\nthe main key (or global key) of a piece of music [8, 13].\nIn these works, because only the main key is handled, key\nchanges are ignored (songs having different local keys are\neither ignored or considered to be in the ﬁrst local key en-\ncountered). Chai [3] presented one of the few studies on\nkey change from audio. In this work, local key tracking\nwas performed by a HMM-based approach, and evaluated\non ten classical piano pieces.\nThe main contribution of this paper relies in the fact that\nboth chord and key can beneﬁt from each other’s estima-\ntion, as chords bring out information about local key and\nvice versa. We present a new system estimating simulta-\nneously both chord and key sequences from audio. The\nproposed method is both template-based andmusic-based\nand no training is required.\nWe begin to present our work by describing the system\nused for both key and chord estimation in Section 2. Sec-\ntion 3 presents the experiments performed to evaluate the\naccuracy of the proposed method. Conclusion and future\nwork follow in Section 4.\n2. SYSTEM DESCRIPTION\nIn this section, we provide the description of the proposed\nmethod, which is adapted for audio from the proposed sys-\ntem in [anonymous self-reference]. The overall process is\nillustrated in Figure 1. The system works in four major\nsteps: (1) chroma vectors are computed from audio sig-\nnal; (2) a set of harmonic candidates are selected for each\nframe (Figure 1(a)); (3) a weighted acyclic graph of har-\nmonic candidates is built (Figure 1(b)), (4) the dynamic\nprocess takes place (Figure 1(c)) and the ﬁnal sequence\nof chords/keys corresponding to the best path is outputted\n(Figure 1(d)).\n141\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)An additional step consists in post-ﬁltering the outputted\nsequence, to correct some analysis errors remaining.\n2.1 Chroma Computation\nThe input audio ﬁle of the analysis system proposed is rep-\nresented as sequences of chromas. This mid-level feature\ncaptures the tonal information since it represents the short-\ntime energy related to each pitch class independently of\noctave [5]. Indeed, information about octave is not neces-\nsary for chord and key analysis purposes.\n2.1.1 Tuning Issues\nThe chromas are computed on each frame. One of the main\nproblem when analyzing audio musical piece is the varia-\ntion in tuning. All the instruments are not always tuned to\nthe same value, and this value often varies in time. Two\noptions are possible. The tuning value may be analyzed,\nbut tuning analysis assumes a stationarity. We choose to\navoid this analysis by computing chroma on 36bins and by\nshifting chroma at each frame according to possible tuning\nvariations. This way, two chords played with different tun-\ning result in two different 36bin chromas, but results in\nalmost the same 12bin chroma [5].\n2.1.2 Multi-Scale Approach\nInstead of relying exclusively on one chromagram (sequence\nof chroma vectors over time), the proposed method in-\ncludes a set of chromagrams. Each one has its own param-\neters but all share a common multiple hop size, to combine\ninformation at the same times during the piece of music.\nThese chromagrams bring out different kinds of informa-\ntion, and may be subject to different treatments. Longer\nchromas may bring out information for key analysis, and\ndifferent set of sizes for shorter chromas may ﬁt different\ntempos and carry out different information useful for chord\nidentiﬁcation.\n2.1.3 Filter\nIn order to reduce the inﬂuence of the noise, transients or\nsharp edges, we ﬁlter the chromagram on several frames [2,\n6]. The ﬁltering method used here is the median ﬁltering,\nwhich has been widely used in image processing in order\nto correct random errors.\n2.2 Selection of Harmonic Candidates\nAn harmonic candidate is a pair (Ci, Ki), where Ci(resp.\nKi) represents a potential chord (resp. local key) for the\nith frame of audio signal. Ciis then considered as a chord\ncandidate (among possible others), and Kias a key candi-\ndate. This section presents the processes allowing to select\none or several chord/key pairs as harmonic candidate(s),\nand discard others.\n2.2.1 Chord\nThe chords studied here are major and minor triads (12\nmajor and 12 minors). Lots of works [6, 9, 14] have used\nchord templates to determine the likelihood of each of the\n24 chords according to a chroma vector. With 12 dimen-\nsional vectors, major/minor triadic chord proﬁle may be\ndeﬁned like the following:\nMajor-triad = (1, 0,0,0,1,0,0,1,0,0,0)\nMinor-triad = (1, 0,0,1,0,0,0,1,0,0,0)\nFn+ 1 Fn Fn-1\nCn-1,1 Cn+1,1 \nCn,1 \nCn-1,2 \nCn-1,2 Cn+1,2 \nCn+1,3 Cn,1 \nCn,2 (a)\nFn+ 1 Fn Fn-1\nCn-1,1 Cn+1,1 \nCn,1 \nCn-1,2 \nCn-1,2 Cn+1,2 \nCn+1,3 Cn,1 \nCn,2 \n(b)\nFn+ 1 Fn Fn-1\nCn-1,1 Cn+1,1 \nCn,1 \nCn-1,2 \nCn-1,2 Cn+1,2 \nCn+1,3 Cn,1 \nCn,2 \n(c)\nFn+ 1 Fn Fn-1\nCn-1,1 Cn+1,1 \nCn,1 \nCn-1,2 \nCn-1,2 Cn+1,2 \nCn+1,3 Cn,1 \nCn,2 \n(d)\nFigure 1. (a) Enumeration of harmonic candidates Ci,j\nfor consecutive audio frames Fi.Ci,jrepresents the jth\nharmonic candidate for frame Fi. Time appears from left\nto right. (b) Creation of the edges of a weighted acyclic\ngraph. An edge is built from each of the ﬁrst frame’s can-\ndidates to each of the second frame. (c) Dynamic process\nselects an unique path to each candidate of a given frame\n(here, frame n). (d) Selection of ﬁnal path. The ﬁnal\nchord/key sequences is then outputted from the sequence\nof chosen harmonic candidates.\n142\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)All the major (resp. minor) chord templates can be ob-\ntained by rotation of the major (resp. minor) proﬁle.\nFor each of the 24 chord templates, we compute a cor-\nrelation score by scalar product. The following details the\ncorrelation Cbetween a chord template Tand a 12 dimen-\nsional chroma vector V.\nCT,V=12/summationdisplay\ni=1(T[i].V [i])\nThe higher the correlation is, the more likely the chord\ncorresponding to the template is played in the considered\nframe. A direct way to get chord candidates is thus by\nselecting the chords whose templates get the higher corre-\nlation score with the chroma of a given audio frame. In the\nmulti-scale approach as exposed in Section 2.1.2, it is pos-\nsible to consider different highest correlated chords from\ndifferent windowed chromas as candidate for the same frame.\nThe different chord candidates may thus be carrying differ-\nent kind of information.\n2.2.2 Key\nKey selection is carried out with the same approach as for\nchords, but with larger time frame as keys have a larger\ntime persistence than chords. The key proﬁles used are\npresented in [18]:\nMajor = (5, 2,3.5,2,4.5,4,2,4.5,2,3.5,1.5,4)\nMinor = (5, 2,3.5,4.5,2,4,2,4.5,3.5,2,1.5,4)\nAs for chord candidate computation, the correlation of\neach of the 24 keys (12 minors + 12 majors) are computed\nusing a scalar product between shifted key template and\nchroma vectors.\n2.2.3 Harmonic candidates\nThe harmonic candidates ﬁnally enumerated are all the pos-\nsible combination of previously selected keys and chords.\nIfnchords and mkeys are selected for a given audio\nframe, nxmpairs are enumerated. For example, with CM\nandAmas selected chords and CMandGMas compat-\nible keys, the harmonic candidates enumerated would be\n(CM, CM), (C M, GM), (A m, CM) and (A m, GM). A dif-\nferent choice can be made, by considering a compatibility\nbetween chords and keys. But an incorrect chord selected\nmay discard the correct key (and vice versa), because the\ntwo are not compatible. For this reason, adding a compat-\nibility between chords and keys has led to a decrease of\naccuracy.\n2.3 Weighted Acyclic Harmonic Graph\nOnce the harmonic candidates are enumerated for two con-\nsecutive frames, an edge is built from each of the ﬁrst\nframe’s candidates to each of the second frame. This edge\nis weighted by a transition cost between the two chord can-\ndidates. This transition cost must take into account both\nthe different selected chords, and the different selected lo-\ncal keys.\nWe thus choose to use Lerdahl’s distance [11] as transi-\ntion cost. If (Cx, Kx)represents the chord Cxin the key\nKx, Lerdahl deﬁnes the transition cost from x= (Cx, Kx)\ntoy= (Cy, Ky)as follows:\nδ(x→y) =i+j+kwhere iis the distance between KxandKyin the circle\nof ﬁfths, jis the distance between CxandCyin the circle\nof ﬁfths and kis the number of non-common pitch classes\nin the basic space of ycompared to those in the basic space\nofx(see [11] for more details).\nThe distance thus provides an integer cost from 0 to\n13, and is adequate for a transition cost in the proposed\nmethod, since both compatible chords and keys are in-\nvolved in the cost computation. Nevertheless, this distance\noffers a small range of possible values. As we need to\ncompare different paths between harmonic candidates, this\nsmall range induces a lot of equality scenarios. The Ler-\ndahl’s distance is thus slightly modiﬁed and the cost be-\ntween two consecutive candidate is set to iα+jβ+k,\nwithi,jandkdeﬁned in Section 2.3. We choose α > 1\nto discourage immediate transitions between distant keys,\nand encourage progressive key changes, since modulations\noften involve two keys close to each other in the circle of\nﬁfths. For the same reason with chords, we also choose\nβ > 1. After experiment, αandβhave been set to 1.1 and\n1.01.\n2.4 Finding the Best Path\nOnce the graph between all the harmonic candidates is\nformed, the best path has to be found. This task is achieved\nby dynamic programming [1]. In the graph, from left to\nright, only one edge to each harmonic candidate is pre-\nserved. Several ways to select this edge can be considered.\nWe choose to preserve the edge minimizing the total sum\nof weights along the path leading to each candidate, as il-\nlustrated in Figure 1(c). The number of ﬁnal paths is the\nnumber of harmonic candidates for the last frame. The ﬁ-\nnal selected path is the path minimizing its total cost along\nits edges. This path is outputted by the program.\n2.5 Post-smoothing computation\nAmong the selected sequence of chord/key, some errors\nmay still be corrected by applying a post-smoothing treat-\nment. For example, if an instrument (or a singer) plays a\nﬂattened third (Eb) as a blue note, it may induce a mode\nerror on the selected chord (making Cmas a chord can-\ndidate and discarding CMfor the considered frame). The\noutputted chord sequence may thus present several con-\nsecutive frames analyzed as CMare followed by a single\nframe analyzed as Cm, and then by another several CM. A\nsimple post treatment on the outputted sequence of chords\nmay resolve this kind of errors.\n3. EXPERIMENTS\nThis section presents the database used for experiments,\nthe evaluation procedure, and the inﬂuence on the different\nparameters on the system accuracy. Once the best settings\ndetermined, we compare the system to a state-of-the-art\nmethod for chord estimation, and a direct template-based\nmethod for key estimation.\n3.1 Database\nAs both local key and chord ground truth were needed,\nwe choose to evaluate the proposed system on the Beat-\nles audio discography (174 songs) with a 44100 Hz sam-\npling rate. In this database, the average number of chord\nchanges by song is 69, with an average of 7.7 different\n143\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)chords by song. The average number of different local\nkeys by song is 1.69. Chords transcriptions were checked\nby Christopher Harte and the MIR community, and keys\nannotations were provided by the Centre for Digital Mu-\nsic (C4DM). Both sets of transcriptions are available at\nhttp://www.isophonics.net.\n3.2 Evaluation\nIn the transcriptions, chords have a root note and a type\nwhich belongs to a vast dictionary [7]. In this paper, we\nonly focus on the root note (C, C#, D, ..., B) and the mode\n(maj/min) of chords. All the ground truth chords of the\ndatabase have thus been mapped to min/maj triads. When\nthe chord has no third and cannot be mapped to a min/maj\ntriad, only the root note is considered and later compared\nto the corresponding estimated root note. Silences and no-\nchords (part of a song when no chord is deﬁned) are ig-\nnored, as the chord/no-chord detection issue has not yet\nbeen addressed in the proposed system. The other compo-\nnents of the evaluation are the same as for the evaluation\nled in the 2009 MIREX audio chord detection task1. The\naudio signal is divided in frames of approximately 100 ms\n(4096 audio samples). The estimated chord is compared\nfor each frame to the ground truth at the time correspond-\ning to the center of the frame. The ﬁnal score for a song is\nthe number of frames where estimated chord matches the\nground truth divided by the number of frames analyzed.\nFor the local key evaluation, the procedure is identical. For\neach frame, the estimated key is compared to the ground\ntruth key at the center of the frame.\n3.3 Chord Estimation\nFollowing the multi-scale approach presented in 2.1.2, we\nneed to use different size of chromas vectors for chord es-\ntimation. The parameters for the different chroma scales\nare the following:\n•”long” chromas: 32768 samples as window length\n(approximately 0.8 sec) and 8192 (approximately 0.2\nsec) as hop size,\n•”medium” chromas: 8192 samples as window length\n(approximately 0.2 sec) and 8192 (approximately 0.2\nsec) as hop size,\n•”short” chromas: 4096 samples as window length\n(approximately 0.1 sec) and 4096 (approximately 0.1\nsec) as hop size.\n3.3.1 Inﬂuence of Filtering\nA ﬁrst experiment has been carried out on long chromas\nto measure the inﬂuence of chroma ﬁltering on chord es-\ntimation. For each frame, we set as chord candidate the\nnhighest correlated chords with the maj/min chord tem-\nplates . Tests go from n= 1 ton= 4. For each value\nofn, we compute the ratio of correctness as the number of\nframes for which the correct chord is among the selected\ncandidates over the total number of frames. This ratio rep-\nresents the system theoretical maximum accuracy, and is\nreached if every correct chord candidate is present in the\nﬁnal chord sequence outputted. Obviously, the higher the\nnumber of considered chord candidates is, the higher the\nchance is for one of them to be the correct chord, and thus\nthe higher the ratio of correctness is. In the other hand,\n1http://www.music-ir.org/mirex/2009/index.php/\nAudio Chord Detection ResultsNo ﬁltering\nNb. of chord candidates 1 2 3 4\nRatio of correct. (%) 58.3 71.5 78.6 82.9\nSystem (%) 58.3 64.9 64.1 62.2\nFiltering\nNb. of chord candidates 1 2 3 4\nRatio of correct. (%) 68.4 79.1 85.5 88.9\nSystem (%) 68.4 70.0 64.3 59.3\nTable 1. Percentage of correct chord in harmonic candi-\ndates and system output accuracy depending on the num-\nber of candidates and chroma ﬁltering. Best results are\nachieved by limiting the number of candidates and ﬁlter-\ning chromas.\nmore chord candidates considered increase the likelihood\nof the system to pick a incorrect chord. Finding the bal-\nance between these two parameters is thus a real need. Ta-\nble 1 presents the ratio of correctness and the system score\nwith or without chroma ﬁltering and for different values\nofn(number of chord candidates). Best chroma ﬁltering\nsetting is achieved by taking into account a window of 9\nchromas, centered on the considered chroma.\nThese ﬁrst results shows that ﬁltering leads to an im-\nprovement of the ratio of correctness, from 6% with four\nselected candidates (82.9% to 88.9%) to more than 10%\nwith one (58.3% to 68.4%). Filtering thus seems to correct\nsome errors due to chroma vectors, by taking into account\ninformation from adjacent frames.\n3.3.2 Inﬂuence of the Number of Chord Candidates\nIn Table 1, we notice the drop of the system’s performance\nwhen the number of selected candidates per frame exceeds\ntwo. This can be explained by the close relationship ex-\nisting among the highest correlated chord candidate of a\ngiven chroma vector. Indeed, chord templates of two major\nand minor chords sharing the same root note often induce\na close correlation score for a given chroma. The same\ngoes for any couple of chords close to each other in terms\nof Lerdahl’s distance. In 80% of the frames, top 2 corre-\nlated chord candidate have a distance less or equal to 1 on\nthe circle of ﬁfths. Considering different candidates from\nthe same chroma thus does not seem proﬁtable to gain a\nmaximum system accuracy.\n3.3.3 Inﬂuence of the Multi-Scale Approach\nSince a drop of accuracy is noticed when too many candi-\ndates from the same chroma are selected as candidates, we\npropose a new approach by considering top correlated can-\ndidates from different sized chromas. Table 2 presents the\nratio of correctness as well as the system score depending\non the combination of chroma size, and ﬁltering. The gen-\neral idea is to add highest correlated chord candidates from\nshorter chromas to the highest chord candidate of a given\nlong chroma. Tri-candidate means the combination of the\ntwo best candidates from the two adjacent short chromas\ncentered in a long chroma with the best candidate from\nthe long chroma. Bi-candidate means the combination of\nthe best candidate from the medium chroma centered in a\nlong chroma with the best candidate from the long chroma.\nSince top correlated chords of two different sized chroma\n144\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Tri-candidate (1 long and 2 short chromas)\nFiltering none long short both\nRatio of correct. (%) 69.8 78.2 76.6 79.1\nDistinct chords (av.) 1.86 1.95 1.43 1.36\nSystem (%) 64.5 72.2 71.8 73.7\nBi-candidate (1 long and 1 medium chromas)\nFiltering none long medium both\nRatio of correct. (%) 65.1 75.1 73.5 76.7\nDistinct chords (av.) 1.38 1.46 1.29 1.23\nSystem (%) 62.8 71.6 70.7 72.8\nTable 2. Percentage of correct chord in harmonic candi-\ndates and system score depending on the selection of can-\ndidates from different sized chromagrams. Each time, the\naverage number of distinct chord candidates is mentioned.\nBest results are reached by ﬁltering both long and short\nchromagrams, and by combining their information. Tri-\ncandidate means the two best candidates from the two ad-\njacent short chromas centered in a long chroma with the\nbest candidate from the long chroma. Bi-candidate means\nthe best candidates from the medium chroma centered in a\nlong chroma with the best candidate from the long chroma.\nmay be identical, we also show the average number of dis-\ntinct chord candidates per frame. As for the previous ex-\nperiment, best ﬁltering is achieved by taking into account\na windows of 9 chromas and centered on the considered\nchroma, whatever its length.\nFiltering effective in each case. If ﬁltering is applied to\nonly one scale, the system accuracy and ratio of correct-\nness both beneﬁt a little more of the long chroma ﬁltering\nthan the short (resp. medium) for the tri-candidate (resp.\nbi-candidate). Nevertheless, the best overall ﬁltering efﬁ-\nciency is reached by combining the ﬁlter on both chromas\nsize. Compared to no ﬁlter, the double sized-ﬁlter leads\nto an increase of around 10% for the ratio of correctness\nand the system accuracy, both in the tri-candidate and the\nbi-candidate cases. The difference between the ratio of cor-\nrectness and the system score is less important by consider-\ning candidates from different chromas than by considering\nseveral candidates from the same chroma. With ﬁltering,\nthis difference is of 5.4% (79.1 - 73.7) for the tri-candidate\nconﬁguration and of 2.9% (76.7 - 72.8) for the bi-candidate\n(see Table 2), when it is more than 9% (79.1 - 70) with 2\ncandidates from the same long chromas (see Table 1). A\nﬁrst way to explain this improvement is by considering the\ndecrease of average number of distinct chord candidates,\nwhich is always lesser than 2 in both tri-candidate and bi-\ncandidate conﬁguration. This decrease means fewer chord\ncandidates to consider for the system, thus decreasing the\nlikelihood to select an incorrect chord.\nMaximum accuracy is reached with tri-candidate con-\nﬁguration, with a system accuracy of 73.7%.\n3.3.4 Post-Smoothing Treatment\nWe decide to apply a post-smoothing treatment to the out-\nput of the system with the best settings, which performs\na 73.7% accuracy (see Table 2). The post-smoothing ﬁlter\nlooks for output chord sequence in the form of ...AABAA...\n(resp. ...AAABCAAA...) and corrects it in ...AAAAA...\n(resp. ...AAAAAAAA...). By applying the post-smoothingMethod Root Root and Mode\nOGF2 (%) 78.9 72.3\nProposed System (%) 77.9 74.9\nTable 3. Comparison of the proposed method with the\nOGF2 method, which scored 1st (resp 2nd) in the 2009\nMIREX Audio Chord Detection ”root estimation” task\n(resp. root and mode task).\ntreatment with the system best previous settings, chord de-\ntection reaches a 74.9% accuracy.\n3.3.5 Comparison to a state-of-the-art Method\nWe compare the best conﬁguration of our system to one of\nthe best methods of the 2009 MIREX Audio Chord Esti-\nmation task, evaluated on the same database with the same\nevaluation procedure. The comparison is made with the\nOGF2 method, proposed by Oudre et al. Results are shown\nin Table 3. On the root estimation only, OGF2 is 1% more\naccurate than the proposed method (78.9% compared to\n77.9%). On the root and mode estimation, the proposed\nsystem performs better than the OGF2 method and im-\nproves by almost 3% the accuracy of the detected chords\n(74.9% compared to 72.3%). This comparison shows that\nthe proposed method is comparable, and maybe even more\naccurate than one of the best methods presented at the 2009\nMIREX when it comes to chord estimation, and compares\nthe local key sequence as well as the chord sequence.\n3.4 Local Key Estimation\nKey estimation is performed on the same database than for\nchord estimation. We compare the key sequence output\nof the proposed system to a direct template-based method\n(DTBM). The same settings are used for the two compared\nmethods, as we wish to evaluate the system’s contribution.\nThe window size is set to 30 sec approximately. For the\nproposed method, the number of key candidates per frame\nis set to 3. Results, shown in Table 4, detail the estimated\nkey error made by the two compared method, by presenting\nrelative and neighbor errors as well as correct key accuracy.\nRelative keys share the same key signature (for example,\nCMandAmare relative keys of each other). A neighbor\nkey differs from the original key by an accidental. Each\nkey has two neighbors (for instance, CMhasFMandGM\nas neighbors).\nThe system performs better than the DTBM method by\nestimating more correct keys (62.4% compared to 57.6%).\nMoreover, the number of errors due to non related key (dif-\nferent from neighbor or relative) is less important when the\nanalysis is performed by the system (17.3% compared to\n21.9%).\n3.5 Reciprocal Beneﬁt of Simultaneous Estimation\nWe present here an evaluation to measure the reciprocal in-\nﬂuence of the chord and key simultaneous estimation. We\ncompared the proposed system, which takes into account\nharmonic candidates (i.e. pairs of chord AND key candi-\ndates), to the same system with only chord OR key can-\ndidates. When only chord (resp.) are considered, the dis-\ntance used to weigh edges in the harmonic graph is edited\nto take only chord (resp. key) into account. Results are\nshown in Table 5.\n145\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Key estimation Correct Rel. Nei. Oth.\nSystem (%) 62.4 2.9 17.4 17.3\nDTBM (%) 57.6 1.6 18.9 21.9\nTable 4. DTBM: Direct Template-Based Method. Keys\nscores are shown in % and are split among possible errors:\ncorrect keys, relative keys (Rel.), neighbor keys (Nei.) and\nothers (Oth.). The system performs the highest accuracy in\nterms of correct key detected. The number of due to non\nrelated keys is also less important with the system analysis\nthan with the DTBM.\nHarmonic Chord Key Both\nCandidate C •• K C K\nSystem (%) 73.1 ••57.8 74.9 62.4\nTable 5. System accuracy considering (chord,key), only\nchord and only key as harmonic candidates. The system\nperforms better in chord and key estimation when taking\ninto account both information simultaneously.\nWe note that both key and chord estimation are bet-\nter when the harmonic candidate is the (chord,key) pair.\nChord estimation accuracy drops of almost 2% (74.9 com-\npared to 73.1) and key estimation accuracy drops of almost\n5% (62.4 compared to 57.8).\n4. CONCLUSION AND FUTURE WORK\nThis paper presents a new method for chord and local key\nestimation where the analysis of chord sequence and key\nchanges are performed simultaneously. A multi-scale ap-\nproach for chroma vectors is proposed, and we show an\nincrease in accuracy when the chords are selected from dif-\nferent sized chromas. While the key estimation performs\nbetter than a direct template-based method, the chord ac-\ncuracy shows better results than a state-of-the-art method.\nFuture work will involve analysis of different chord types,\nsilence and no-chord detection as well weighing the har-\nmonic graph of the proposed method in a probabilistic ap-\nproach. Applications for MIR using both local key and\nchord information are also studied. For example, harmonic\ninformation may be helpful for estimating the musical struc-\nture of pieces since changes of local key generally occur\nat the beginning of new patterns. Furthermore, we aim at\ninvestigating the possible improvements induced by a re-\ntrieval system based on all the harmonic information, com-\npared to existing systems that only consider chord progres-\nsions.\n5. REFERENCES\n[1] R. Bellman. Dynamic Programming. Princeton Univer-\nsity Press, 1957.\n[2] J.P. Bello and J. Pickens. A robust mid-level represen-\ntation for harmonic content in music signals. In Proc.\nof the International Symposium on Music Information\nRetrieval (ISMIR), pages 304–311, London, UK, 2005.\n[3] W. Chai and B. Vercoe. Detection of key change in\nclassical piano music. In Proc. of the International\nSymposium on Music Information Retrieval (ISMIR),\npages 468–473, London, UK, 2005.[4] J. Stephen Downie. The music information retrieval\nevaluation exchange (2005–2007): A window into mu-\nsic information retrieval research. Acoustical Science\nand Technology, 29(4):247–255, 2008.\n[5] E. G ´omez. Tonal Description of Music Audio Sig-\nnals. PhD thesis, University Pompeu Fabra, Barcelona,\nSpain, July 2006.\n[6] C. Harte and M. Sandler. Automatic chord identiﬁca-\ntion using a quantised chromagram. In Proc. of the Au-\ndio Engineering Society, Barcelona, Spain, 2005.\n[7] C. Harte, M. Sandler, and A. Samer. Symbolic repre-\nsentation of musical chords: A proposed syntax for text\nannotations. In Proc. 4th Int. Conf. on Music Informa-\ntion Retrieval (ISMIR), 2005, pages 66–71, 2005.\n[8] O. Izmirli. Audio key ﬁnding using low-dimensional\nspaces. In Proc. of the International Symposium on\nMusic Information Retrieval (ISMIR), pages 127–132,\nVictoria, Canada, 2006.\n[9] K. Lee. Automatic chord recognition from audio using\nenhanced pitch class proﬁle. In Proc. of the Interna-\ntional Symposium on Music Information Retrieval (IS-\nMIR), Victoria, Canada, 2006.\n[10] K. Lee and M. Slaney. Acoustic chord transcription\nand key extraction from audio using key-dependent\nHMMs trained on synthesized audio. IEEE Trans. on\nAudio, Speech and Language Processing, 16(2):291–\n301, 2008.\n[11] F. Lerdahl. Tonal Pitch Space. Oxford University Press,\n2001.\n[12] M. Mauch and S. Dixon. Simultaneous estimation of\nchords and musical context from audio. IEEE Trans.\non Audio, Speech and Language Processing, 2010.\n[13] K. Noland and M. Sandler. Key estimation using a hid-\nden markov model. In Proc. of the International Sym-\nposium on Music Information Retrieval (ISMIR), pages\n121–126, Victoria, Canada, 2006.\n[14] L. Oudre, Y . Grenier, and C. F ´evotte. Template-based\nchord recognition : inﬂuence of the chord types. In\nProc. of the International Symposium on Music In-\nformation Retrieval (ISMIR), pages 153–158, Kobe,\nJapan, 2009.\n[15] H. Papadopoulos and G. Peeters. Large-scale study\nof chord estimation algorithms based on chroma rep-\nresentation and HMM. In Proc. of the International\nWorkshop on Content-Based Multimedia Indexing,\npages 53–60, Bordeaux, France, 2007.\n[16] M.P. Ryyn ¨anen and A.P. Klapuri. Automatic transcrip-\ntion of melody, bass line, and chords in polyphonic mu-\nsic.Computer Music Journal, 32(3):72–86, 2008.\n[17] A. Sheh and D.P.W. Ellis. Chord segmentation and\nrecognition using EM-trained hidden Markov models.\nInProc. of the International Symposium on Music In-\nformation Retrieval (ISMIR), pages 185–191, Balti-\nmore, MD, 2003.\n[18] D. Temperley. The Cognition of Basic Musical Struc-\ntures. The MIT Press, 1999.\n146\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "Autoregressive MFCC Models for Genre Classification Improved by Harmonic-percussion Separation.",
        "author": [
            "Halfdan Rump",
            "Shigeki Miyabe",
            "Emiru Tsunoo",
            "Nobutaka Ono",
            "Shigeki Sagayama"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1418239",
        "url": "https://doi.org/10.5281/zenodo.1418239",
        "ee": "https://zenodo.org/records/1418239/files/RumpMTOS10.pdf",
        "abstract": "In this work we improve accuracy of MFCC-based genre classification by using the Harmonic-Percussion Signal Sep- aration (HPSS) algorithm on the music signal, and then calculate the MFCCs on the separated signals. The choice of the HPSS algorithm was mainly based on the observa- tion that the presence of harmonics causes the high MFCCs to be noisy. A multivariate autoregressive (MAR) model was trained on the improved MFCCs, and performance in the task of genre classification was evaluated. By combin- ing features calculated on the separated signals, relative er- ror rate reductions of 20% and 16.2% were obtained when an SVM classifier was trained on the MFCCs and MAR features respectively. Next, by analyzing the MAR features calculated on the separated signals, it was concluded that the original signal contained some information which the MAR model was capable of handling, and that the best per- formance was obtained when all three signals were used. Finally, by choosing the number of MFCCs from each sig- nal type to be used in the autoregressive modelling, it was verified that the best performance was reached when the high MFCCs calculated on the harmonic signal were dis- carded.",
        "zenodo_id": 1418239,
        "dblp_key": "conf/ismir/RumpMTOS10",
        "keywords": [
            "Harmonic-Percussion Signal Separation (HPSS)",
            "MFCC-based genre classification",
            "High MFCCs",
            "Noisy MFCCs",
            "Multivariate autoregressive (MAR) model",
            "SVM classifier",
            "Relative error rate reductions",
            "Information handling",
            "Best performance",
            "Number of MFCCs"
        ],
        "content": "AUTOREGRESSIVE MFCC MODELS FOR GENRE CLASSIFICATION\nIMPR\nOVED BY HARMONIC-PERCUSSION SEPARATION\nHalfdan Rump,Shigeki Miyabe, Emiru Tsunoo,Nobukata Ono, ShigekiSagama\nTheUniversity of Tokyo, Graduate School of Information Scienceand Technology\n{rump,miyabe,tsunoo,onono,sagayama}@hil.t.u-tokyo.ac.jp\nABSTRACT\nIn this work we improve accuracy of MFCC-based genre\nclassiﬁcationbyusingtheHarmonic-PercussionSignalSep-\naration (HPSS) algorithm on the music signal, and then\ncalculate the MFCCs on the separated signals. The choice\nof the HPSS algorithm was mainly based on the observa-\ntionthatthepresenceofharmonicscausesthehighMFCCs\nto be noisy. A multivariate autoregressive (MAR) model\nwas trained on the improved MFCCs, and performance in\nthe task of genre classiﬁcation was evaluated. By combin-\ningfeaturescalculatedontheseparatedsignals,relativeer-\nror rate reductions of 20% and 16.2% were obtained when\nan SVM classiﬁer was trained on the MFCCs and MAR\nfeaturesrespectively. Next,byanalyzingtheMARfeatures\ncalculated on the separated signals, it was concluded that\nthe original signal contained some information which the\nMARmodelwascapableofhandling,andthatthebestper-\nformance was obtained when all three signals were used.\nFinally,bychoosingthenumberofMFCCsfromeachsig-\nnal type to be used in the autoregressive modelling, it was\nveriﬁed that the best performance was reached when the\nhigh MFCCs calculated on the harmonic signal were dis-\ncarded.\n1. INTRODUCTION\nMusic information retrieval (MIR) is a diverse research\nﬁeld with many different areas of interest, such as chord\ndetection, melody extraction etc. One of the popular tasks\nis classifying music into genres, which not only serves to\nease organization of large music databases, but also drives\nthe general development of features for representing the\nvariousimportantaspectsofmusic. Thetaskofgenreclas-\nsiﬁcation draws upon many different kinds of information\nwhichmeansthatonecaneitherusefeaturescapableofex-\npressing the music as a whole, or use many different types\nof features, each describing speciﬁc aspects of the music,\nsuch as the beat, melody, timbre etc. A low level feature\nfrequently usedformodelling musicistheMel-Frequency\nCepstral Coefﬁcients (MFCC), originally proposed in [1],\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnotmadeordistributedforproﬁtorcommercialadvantageandthatcopies\nbear this noticeand thefull citation on theﬁrst page.\nc/circlecopyrt2010International Society for Music Information Retrieval.(see [2] for a comprehensive review). The MFCCs are of-\nten calculated on the unaltered spectrum, thus containing\ninformation of all aspects of the music. The MFCCs ef-\nfectively function as a lossy compression of a short part\nof the music signal into a small number of coefﬁcients. It\nmayhappenthatcertaincharacteristicsofthemusicsignal\nwhich could be useful for genre classiﬁcation are blurred\nby the compression. A possible way to resolve this issue\nistobreakdownthemusicsignalintoseveralsignals,each\ncontaining a speciﬁc kind of information about the signal,\nand then calculate the MFCCs on the new signals. An ex-\nample could be to separate the instruments and then cal-\nculate the MFCCs for the signals, each containing only a\nsingleinstrument. However,itispossiblethatsuchasepa-\nrationwillfail,thusgeneratingunpredictableresultswhich\nmight actually be worse than just using the original signal\nforclassiﬁcation. Inthisworkwehaveusedasimplealgo-\nrithm that separates the music signal into two signals, one\ncontainingharmonicsandtheothercontainingpercussion.\nThechoiceofthisalgorithmisbasedonsomeobservations\nabout the nature of the MFCCs, discussed in section 2.\nAfter the music signal has been separated, MFCCs can\nbe calculated on all three signals (original signal, harmon-\nics and percussion). A classiﬁer can be trained directly on\nthe MFCCs, or more elaborate models can be constructed\nand used for classiﬁcation. In this paper we investigate if\nhigher classiﬁcation performance can be achieved by sep-\narating the music signal as described above. We train a\nmultivariate autoregressive (MAR) model on the MFCCs\nfromthe three signal types, and use it in a classiﬁer.\nThe MAR model has proven to be efﬁcient for the task\nof genre classiﬁcation. First of all, the MAR model inte-\ngrates the short time feature frames temporally, and sec-\nondly it is capable of modelling the covariances between\nthe MFCCs. Since the ultimate goal of genre classiﬁca-\ntion algorithms is to reach an accuracy of 100%, it is most\nmeaningfultoanalysethemodelwiththehighestaccuracy.\nTherefore the article will focus mostly on the results ob-\ntained when using the MAR model for classiﬁcation. Fur-\nthermore,bycomparingperformanceoftheMARfeatures\ncalculated on the different signal types, it can be inferred\nwhich aspects of the music the MAR model analyses.\n87\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)2. THE MEL-FREQUENCY CEPSTRAL\nCOEFFICIENTS\nThe\nMel-Frequency Cepstral Coefﬁcient (MFCC) feature\nextractionisausefulwayofextractingtimbreinformation.\nThe music signal is divided into a number of short time\nframes. For each frame, Nmcoefﬁcients are calculated,\nthus yielding Nmtime series to be modelled by the MAR\nmodel, described in section 3.\nIn the following we explain the motivation for includ-\ning a separation step by considering how the MFCCs are\ncalculated. In the Mel ﬁlter-bank analysis, the bandwidth\nof each ﬁlter is linear for frequencies under around 1 kHz,\nand thereafter grows logarithmically. Therefore each of\nthe lower Mel coefﬁcients is the mean of a relatively nar-\nrow frequency band. If the spectrum is characterized by\nnarrow pitch spikes, the difference between two adjacent\nMelcoefﬁcientsislikelytobelarge. SincetheMFCCsare\nobtainedbyapplyingtheDCTtransform,thesedifferences\nwill be described by the high MFCCs. In other words, the\nhighMFCCsarecapableofcloselyﬁttingthepitchpresent\nin the frame on which they are calculated. Pitch is usually\nnot a very good indicator for music genre, and therefore\nthehighMFCCsshouldbediscarded. Ontheotherhand,if\nthespectrumhasasmoothenvelopethehighorderMFCCs\nwillnotmodelpitch,andthereforemaybeusableforgenre\nclassiﬁcation. Most music signals contain both harmonics\n(pitch spikes) and percussion (smooth spectral envelope).\nSince the presence of pitch is harmful to the information\ncontent of the high MFCCs, it seems feasible to separate\nharmonics frompercussion.\nFurthermore it is possible that the shape of the spec-\ntral envelope of harmonics and percussion when they have\nbeen separated is useful for genre classiﬁcation, and that\nthe information content of the lower MFCCs will be im-\nproved by separating the music signal.\n3. THE MULTIVARIATE AUTOREGRESSIVE\nMODEL\nThe MAR model is similar to the normal autoregressive\nmodel, in that it predicts the next sample of a time series\nas a linear combination of past samples. The MAR model\nextends the capabilities of the normal AR, as it capable\nof making predictions for multiple time series and utilizes\ncorrelations between time series for prediction. The pre-\ndiction of the n’thNmtime series is calculated as\nxn=P/summationdisplay\np=1Apxn−I(p)+un (1)\nwherexnisaNm×1vectorcontainingthepredictions,and\nnis the frame index. Pis the model order which speciﬁes\nthe number of time lags used for prediction. The MAR\nmodel is not constrained to using only time lags 1... P,\nbut an arbitrary set of time lags I={τ1... τ P}can be\nchosen.A1...APare the Nm×Nmweight matrices for\ntime lags τ1... τ P. Element [A]i,j\npis the weight that con-\ntrolshowmuchofsignal j,time-lagged τpsamples,isusedforpredictionofsignal i.unistheoffsetvectorandcanbe\nomittedifeachtimeseriesissubtractedbyit’smeanbefore\nestimating the coefﬁcient matrices. The model parameters\ncanbeestimatedbyusingtheleastmeansquaresapproach.\nThePweightmatrices A1...APandthetheoffsetvector\nunarestackedintoa PN2\nm+Nmdimensionalvector,and\nthis constitutes the feature vector used for classiﬁcation.\nA basic assumption of the MAR model is that the time\nseriesupon which it iscalculated has astationary distribu-\ntion. At ﬁrst glance this assumption does not seem to go\nwell with the nature of the percussive signal since it does\nnothaveasmoothtimeenvelope. However,overlongerpe-\nriodsroughlythesamepercussionsoundsandthusMFCCs\nwill appear again and again, which can be interpreted as\nstationarity. On the other hand, even though the harmonic\nsignal has a smooth time envelope for a given note, mean-\ningthattheMFCCswillhaveastationarydistributiondur-\ningthenote,thedistributionwillchangeasthenextnoteis\nstruck. Sincetheexactsamecombinationofharmonics,or\nin other words the same pitch spikes which are modelled\nby the high order MFCCs, is unlikely to occur more than\nmaybeafewtimes,thedistributioncannotbeassumedsta-\ntionary.\nHigh order models are characterized by a high variance\nwhich gives them the power to ﬁt closely to a time series,\nbutalsomakesthempronetoover-ﬁtting. Lowordermod-\nels are more dominated by bias which makes them more\nsuitable in cases where the signal envelope is the desired\ntarget. In [3], the MAR model was found to perform best\nwithP= 3whenthetaskwasgenreclassiﬁcation,butthe\noptimal value might differ according to the application for\nthe reasons listedabove.\n4. HARMONIC-PERCUSSION SIGNAL\nSEPARATION\nTheHarmonic-PercussionSignalSeparation(HPSS)algo-\nrithm proposed in [5], is a simple and fast method of di-\nvidingamusicalsignal, N,intotwosignals, HandP,each\ncontaining only the harmonic and percussive elements re-\nspectively. HPSS can be thought of as a two-cluster soft\nclustering, where each spectrogram grid-point is assigned\na graded membership to a cluster representing harmonics\nand a cluster representing percussion. The algorithm uses\nthefactthatpercussionhasashorttemporaldurationandis\nrichinnoise,whileharmonicelementshavealongtempo-\nraldurationwithmostofthesignalenergyconcentratedin\npitch spikes. Thus in the spectrogram, percussion appears\nasverticallinesofhighpower,whereasharmonicelements\nappear as horizontal lines.\nInbroadterms,theHPSSalgorithmworksbyassuming\nindependence between HandP, and using Bayes formula\nto calculate p(H,P|N)\nlogp(H,P|N) = log p(N|H,P)+log p(H)+log p(P)(2)\nThe prior distributions p(H)andp(P)are deﬁned as func-\ntions that measure the degree of smoothness in time and\nfrequency respectively.\n88\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)logp(H) =/summationdisplay\nω\n,τ−1\n2σ2\nH(Hγ\nω,τ−1−Hγ\nω,τ)2(3)\nlogp(P) =/summationdisplay\nω,τ−1\n2σ2\nP(Pγ\nω−1,τ−Pγ\nω,τ)2(4)\nWhere σH,σPandγhas been manually speciﬁed as in\n[5]. Thus the prior for Hwill be high when each row of\nthe spectrogram is characterized by slow ﬂuctuations, and\nsimilarly the prior for Pwill be high when this is the case\nfor columns of the spectrogram. The likelihood function\nhas been deﬁned by measuring the I-divergence between\nNandH+P:\nlogp(N|H,P) = (5)\n−/summationdisplay\nω,τ/parenleftbig\nNω,τlogNω,τ\nHω,τ+Pω ,τ−Nω,τ+Hω,τ+Pω,τ/parenrightbig\nand so the likelihood is maximized when Nω,τ=Hω,τ+\nPω,τfor all ωandτ. The log-likelihood function is max-\nimized by using the EM-algorithm. The update equations\nhave been omitted in this work, but can be found in [5].\nIt is important to realize that since the HPSS algorithm\nis not a source separation algorithm but rather a decompo-\nsitionoftheoriginalsignal,nocriteriaofsuccesshasbeen\ndeﬁned, and so the algorithm cannot fail unless it fails to\nconverge.\n5. DATASET\nWeusedtheTZGENREdatasetproposedin[8]. Thedataset\nhasNs= 1000songsdividedequallyinto10genres: blues,\nclassic, country, disco, hip-hop, jazz, metal, pop, reggae\nand rock. Each song is a 30s sound snippet, and only one\nMARmodeliscalculatedforthewholesong. Othermeth-\nodsforcalculatingmultipleMARmodelsonasinglesong\nandcombiningthemafterwardscanbefoundin[3]and[4].\n6. EXPERIMENTAL SETUP\nFirst the music signal was separated by using HPSS, and\nMARfeatureswerecalculatedforeachsignal. IftheMAR\nmodel is capable of using both harmonics and percussive\nelements at the same time, such a decomposition will not\nresult in higher performance. However, if for instance the\nMAR model analyses the harmonic elements, then remov-\ning percussion will enable the MAR features to perform\nbetter. In the following, MAR features calculated on the\nharmonics, percussion and normal signals will be referred\nto asmh,mp,mnrespectively, whereas MFCCs will be\nreferred to as ch,cpandcn. In addition to the three sin-\ngle signal feature types, four combinations features of the\nMAR features and four combinations of the MFCCs were\nconstructed: mhp,mhn,mpn,mhpn,chp,chn,cpnand\nchpn.\nThesample-rateofthesongswas22.05kHz. TheMFCCs\nwerecalculatedon20mswindowswithanoverlapof10ms.\n40 ﬁlter-banks were used in the MFCC calculation. SincethenumberofMFCCsusedtocalculatetheMARfeatures\nhas a great inﬂuence on performance, each combination\nof features was evaluated with 19 different values of Nm.\nFor each combination an Ns×Ddata matrix was created\nby stacking the Nsfeatures vectors, each of dimension D.\nFor features containing only MAR combinations, the di-\nmensionis D=c(PN2\nm+Nm),where c∈ {1,2,3}isthe\nnumber of stacked MAR models.\nThe classiﬁer used was a support vector machine with\na Gaussian kernel. Kernel parameters σandCwere not\ntuned, but each column of the data matrix was normalized\nwith respect to standard deviation. 500-fold cross valida-\ntion was used for each of the 19 values of Nm, resulting\ninaNs×19matrix,whereeachcolumncontainedtheav-\nerage accuracy for each song for a given Nm. The overall\nperformance for a given Nmwas obtained by taking the\nmean of that column.\n7. RESULTS\nIn this section the results of the experiments described in\nsection 6 are presented and discussed.\n7.1 Combining features from the separated signals\nFigure1showstheclassiﬁcationperformanceoftheseven\ncombinations when the classiﬁers were trained directly on\nthe MFCCs. The difference between the classiﬁer trained\non the MFCCs calculated on the original signal to the best\nperforming feature, chp, is 7.5%, corresponding to a rel-\native error rate reduction of 20.0%. This is a signiﬁcant\nimprovement, and conﬁrms that the MFCCs have prob-\nlemsexpressingbothharmonicandpercussiveinformation\nwhen present at the same time.\nchreaches its near peak performance for low Nm. This\nmeansthatfortheharmonicsignal,verylittleusableinfor-\nmation is contained in the high MFCCs. The MFCCs are\nfairly low-dimensional which means that the SVM classi-\nﬁer is still able to achieve optimal performance, and thus\nperformance only degrades slightly. Performance of cp\nkeeps increasing when including more MFCCs, meaning\nthat the higher MFCCs in the percussion signal contains\nusable information. Furthermore, the performance gained\nbyincludinghigherMFCCsismorethanfortheharmonics\nsignalbutlessthanforthepercussionsignal. Thisconﬁrms\nthat the presence of harmonics degrades the information\nquality of the higher MFCCs.\nNext, weusetheMARmodel forclassiﬁcationandtest\nperformance of mh,mpandmn, and of the combinations\nof them. The performance of the seven combination fea-\nturesisshownonFigure2. mnisthemostpowerfulofthe\nthree single model features peaking with a performance of\n74.1%. Pleasingly, all three single model features have a\nlower performance than the combination features. mhnp\nhad a peak performance of 77.6%, a gain of 3.6% com-\npared to the best single signal model.\nAs was also seen when using the MFCCs in the clas-\nsiﬁer,mhpperforms signiﬁcantly better than mn. This\nshowsthattheautoregressivemodellingoftheMFCCscal-\n89\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)051015202530350.20.30.40.50.60.70.8\nNmPerformance\n  \nCh\nCp\nCn\nChp\nChn\nCpn\nChpn\nFigure 1. Performance curves for the classiﬁer trained on\nMFCCs\nculated\non the original signal cannot compensate for the\nMFCCs’ inability to handle the mixture of harmonic and\npercussive information.\nAnimportantdifferencebetweenusingMFCCsorMAR\nfeatures in the classiﬁer is that mhpnoutperformed mhp,\nwhereaschpnandchphad the same level of performance.\nThus the MAR model is capable of modelling some prop-\nerties of the original signal N, which are present in neither\nHnorP. More speciﬁcally, the MAR model can in some\ncasespredictpercussionfromharmonicsorviceversa,due\ntotheautoregressivemodelling. Thisisareasonableclaim\nwhen keeping in mind that the HPSS algorithm is not a\nsourceseparationalgorithm,andthatsomeinstrumentswill\nproduce both harmonics and percussive sounds.\nAs an example, when a note is played on a piano the\nhammer hits the string causing it to vibrate, resulting in a\nsound with a high attack part and a slowly declining en-\nvelope. Since this will happen every time the piano is\nused, the MAR model can use the attack part to make a\nprediction about the rest of the sound. When using HPSS\nto separate the signal however, percussion is assumed to\nbeindependentfromharmonics,andtheattackpart,which\nis rich in noise and has a short temporal duration, is as-\nsigned to the percussion signal while the rest of the sound\nis assigned to the harmonic signal. When this happens the\nMAR model can no longer model the dependencies, so in-\ncluding MAR features calculated on the original signal in-\ncreases performance.\n7.2 Differences between the signal type MAR features\nInthissectionweanalysesomeofthedifferencesbetween\nthe MAR features calculated on each of the separated sig-\nnals.\nAnimportantsteptowardsunderstandingtheMARfea-\ntures and specify their application domain is to investigate\nto which degree features calculated on the different signal\ntypes classify the same songs or not. In the former case,\nclassiﬁcationaccuracywithdifferentsignaltypesislargely\ngenre dependent, and in the latter case there will be some123456789101112141618202224280.40.450.50.550.60.650.70.750.8\nNmMean accuracy\n  \nMh\nMp\nMn\nMhp\nMhn\nMpn\nMhpn\nFigure 2. Performance curves for the classiﬁer trained on\nMAR\nfeatures\n123456789101112141618202224280.10.20.30.40.50.60.70.8\nNmMean accuracy\n  \nMh, blues\nMp, blues\nMn, blues\nMh, rock\nMp, rock\nMn, rock\nFigure 3. Examples of genre speciﬁc performance, only\nMAR\nfeatures\neasy songs which can be classiﬁed by all signal models,\nand some hard songs that only the features with an overall\nhigh performance can classify.\nAnalysis is carried out by ﬁnding the point where all\nsignal models have approximately the same accuracy, and\ncalculating the correlation between the Ns×1song accu-\nracyvectors. Itwasobservedthatthereisalowcorrelation\nbetween which songs mhandmpclassify. This suggests\nthat the two signal models contains different information\nwhich allows for the classiﬁcation of different songs, and\nthus are efﬁcient with different kinds of music. For most\ngenresmnis slightly better than mp, withmhbeing the\nworst performing of the three. However, for some genres\nmhachieves the best performance when the high MFCCs\nwere discarded, as can be seen on Figure 3. Furthermore,\nthe fact that the correlation of the song classiﬁcation vec-\ntorsofmpandmnwashigh,meansthattheyclassifymore\nof the same songs than mhandmn, which is consistent\nwith the fact that mhnandmpclassify more of the same\nsongs than mpnandmh. These results suggest that MAR\n90\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Feature Performance Relative ERR\ncn 61.1% N/A\nchp,\nConstr. 68.9% 20.0%\nmn 74.1% N/A\nmhpn,\nConstr. 77.6% 13.5%\nmhpn,\nN.Constr. 78.3% 16.2%\nTable\n1. Overviewofthebestperformingfeatures. Constr.\nor N.Constr. refer to the constraint on Nm.\nfeatures calculated on the original music reﬂect the per-\ncussive elements to a higher degree than the harmonics el-\nements.\nThe fact that mpnis even higher than mhpseems like\na contradiction to the statement made earlier that mnis\nmore correlated with mpthan with mh. The explanation\nto this is most likely that the gains from combining un-\ncorrelated features, i.e. mhpandmhn, cannot match the\npenalty caused by the low performance of mh. Although\nmpandmnare somewhat correlated, there are still some\ndifferences in what songs they classify, and this seems to\nresults in a performance gain when combined.\n7.3 Selecting Nmfor each signal type\nFigure 2 in section 7.1 shows that the MAR features cal-\nculated on the different signal types perform best for dif-\nferent values of Nm. In this section we investigate if per-\nformancecanbeimprovedbyremovingtheconstraintthat\nthe number of MFCCs used to calculate the MAR model\nmust be the same for all signal types. Since it is possible\nthat simply combining the best performing models does\nnot achieve the highest performance, the ﬁve best models\nof each signal type were used to form a number of combi-\nnation features.\nFigure 4 shows the performance plotted versus the di-\nmensionality of the feature vector, using the same number\nofMFCCs,andwithdifferentnumberofMFCCs. Theﬁg-\nuremakesiteasytocomparefeatureefﬁciencies,asapoint\nthat is situated higher and on the left side of another point\nofthesametype,meansthatafeatureoflowerdimension-\nality had higher performance.\nFromFigure4itseemsthatthemethodofselecting Nm\nfor each single MAR model is not particularly capable of\nproducing low dimensional features, but the method do\nachieve the highest overall performance. However, since\nit is in general infeasible to try all combinations of Nm\nbefore selecting the best one, a general tendency must be\ndiscovered. In section 2 it was suggested that the high\nMFCCs calculated on the harmonics signal should be dis-\ncarded, whereas high MFCCs from the percussion signal\ncould be used. This was the case when the classiﬁer was\ntraineddirectlyontheMFCCs,andwhentheclassiﬁerwas\ntrainedontheMARfeatures. Itisnotsurprisingtherefore,\nthat the best performance of 78.3% was obtained by dis-\ncardingthehighMFCCsfortheharmonicsignalandusing\nhigh MFCCs fromthe percussion signal.01000 2000 3000 4000 5000 60000.720.730.740.750.760.770.780.790.8Performance\nDimensionality  \nComb., same NNMFCC\nComb., individual NNMFCC\nOriginal signal\nFigure4. Performanceanddimensionalityofcombination\nmodels\n8.\nPERFORMANCE DEMONSTRATION\nThis section contains a short demonstration of the perfor-\nmanceobtainedwhencombiningtheimprovedfeatureswith\ntwo other features types, each describing different aspects\nof music. The ﬁrst type is the Rhythm Map features, pro-\nposedin[6],whicharecalculatedonthepercussionsignal.\nAsongisrepresentedasatendimensional vector, eachel-\nement describing the membership to a rhythmic template\nextracted from the entire dataset. The second feature type,\nhenceforth referred to as TZ-features, represents a song as\nan68-dimensionalvectorcontainingasetoftimbrerelated\nfeatures proposed in [8]. The Rhythm Map is of special\ninterest since it is calculated on the percussive signal pro-\nvided by the HPSS algorithm, and thus provide no infor-\nmation about the harmonics. The TZ-features were cho-\nsen because they were tested in combination with Rhythm\nMap (see [7]), where it was shown that the two feature\ntypes compliment each other well. An accuracy of 75.0%\nwasobtainedonthedatasetbythecombinationofRhythm\nMap and TZ-features. When the MAR features calculated\nontheoriginalsignalwereincludedaswell,aperformance\nof 80.1% was achieved. Finally, by separating the signal\nwithHPSSandcalculatingMARfeaturesonthethreesig-\nnals as proposed, a performance of 82.46% was obtained,\ncorresponding to a relative error rate reduction of 12.0%.\n9. CONCLUSION\nIn this work we proposed that separating the music signal\ninto more signals, each containing certain characteristics\nof the original signal, could produce better features, lead-\ning to increased performance in the task of music genre\nclassiﬁcation. Based on the observation that the presence\nofharmonics causesthehighMFCCstobenoisy, weused\ntheHPSSalgorithmtoseparatethesignalintotwosignals,\nonecontainingharmonicsandtheothercontainingpercus-\nsion. The separation increased performance signiﬁcantly,\nboth when the classiﬁer was trained on the MFCCs and\nwhenitwastrainedontheMARfeatures. Thebestperfor-\n91\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)mance obtained with the MAR features was 78.3%, corre-\nsponding\ntoarelativeerrorratereductionof16.2%. Itwas\nseen that the MAR model uses both harmonic and percus-\nsive information to make predictions, but that the percus-\nsive informationseems tobethedominating. The fact that\nthebestperformancewasreachedwhentheMARfeatures\nfrom the separated signals were combined with the origi-\nnal signal showed us that the MAR-model could, to some\nextend, model dependencies between harmonic and per-\ncussive elements. The combination of MFCCs calculated\nontheharmonicssignalandMFCCscalculatedontheper-\ncussionsignalperformedbetterthanMFCCscalculatedon\nthe original signal, and this was interpreted as an inability\nof the MFCCs to model the presence of both harmonics\nand percussion in the same signal. An important conclu-\nsion of this is that separating the music signal as proposed\nsimply creates better low level features, which means that\nmodels trained on these features will also be improved.\n10. REFERENCES\n[1] S. B. Davis and P. Marmelstein, “Comparison of\nParametric Representations for Monosyllabic Word\nRecognitioninContinuouslySpokenSentences,” IEEE\nTrans. Acoustics, Speech, Signal Proces., Vol. 28,\nNo. 4, pp. 357–366, 1980\n[2] B. Logan, “Mel Frequency Cepstral Coefﬁcients for\nMusic Modeling,” Proc. ISMIR, 2000\n[3] A. Meng, P. Ahrendt, J. Larsen and L. K. Hansen,\n“Temporal Feature Integration for Music Genre Clas-\nsiﬁcation,” IEEE Trans. Audio, Speech, Lang. Proces.,\nVol. 15, No. 5, pp. 1654–1664, 2007\n[4] J. S. Shawe-Taylor and A. Meng, “An Investigation of\nFeature Models for Music Genre Classiﬁcation Using\nthe Support Vector Classiﬁer,” Proc. ISMIR, pp. 604–\n609, 2005\n[5] N.Ono,K.Miyamoto,H.Kameoka,andS.Sagayama,\n“A real-time equalizer of harmonic and percussive\ncomponets in music signals,” Proc. ISMIR, pp. 139–\n144, 2008\n[6] E. Tsunoo, N. Ono and S. Sagayama, “Rhythm map:\nExtraction of unit rhythmic patterns and analysis of\nrhythmic structure from music acoustic signals,” Proc.\nICASSP, pp. 185–188, 2009\n[7] E. Tsunoo, G. Tzanetakis, N. Ono and S. Sagayama,\n“Audio genre classiﬁcation using percussive pattern\nclustering combined with timbral features,” Proc.\nICME, pp. 382–385, 2009\n[8] G. Tzanetakis and P. Cook, “Musical genre classiﬁca-\ntionofaudiosignals,” IEEETrans.Speech,AudioPro-\ncessing, Vol. 10, No. 5, pp. 293–302, 2002\n92\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "The Standardized Variogram as a Novel Tool for Music Similarity Evaluation.",
        "author": [
            "Simone Sammartino",
            "Lorenzo J. Tardón",
            "Cristina de la Bandera",
            "Isabel Barbancho",
            "Ana M. Barbancho"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1417195",
        "url": "https://doi.org/10.5281/zenodo.1417195",
        "ee": "https://zenodo.org/records/1417195/files/SammartinoTBBB10.pdf",
        "abstract": "Most of methods for audio similarity evaluation are based on the Mel frequency cepstral coefficients, employed as main tool for the characterization of audio contents. Such approach needs some way of data compression aimed to optimize the information retrieval task and to reduce the computational costs derived from the usage of cluster ana- lysis tools and probabilistic models. A novel approach is presented in this paper, based on the standardized vario- gram. This tool, inherited from Geostatistics, is applied to MFCCs matrices to reduce their size and compute compact representations of the audio contents (song signatures), ai- med to evaluate audio similarity. The performance of the proposed approach is analyzed in comparison with other alternative methods and on the base of human responses.",
        "zenodo_id": 1417195,
        "dblp_key": "conf/ismir/SammartinoTBBB10",
        "keywords": [
            "audio similarity evaluation",
            "Mel frequency cepstral coefficients",
            "data compression",
            "information retrieval",
            "computational costs",
            "cluster analysis tools",
            "probabilistic models",
            "vario-gram",
            "Geostatistics",
            "MFCCs matrices"
        ],
        "content": "THE STANDARDIZED VARIOGRAM AS A NOVEL TOOL FOR AUDIO\nSIMILARITY\nMEASURE\nSimoneSammartino, LorenzoJ. Tard ´on,Cristina de la Bandera,Isabel Barbancho,Ana M. Barbancho\nDept. Ingenier ´ıa de Comunicaciones, E.T.S.Ingenier ´ıa de Telecomunicaci ´on,\nUniversidad de M ´alaga, Campus Universitario de Teatinos s/n, 29071, M ´alaga, Spain\nssammartino@ic.uma.es\nABSTRACT\nMost of methods for audio similarity evaluation are based\non the Mel frequency cepstral coefﬁcients, employed as\nmain tool for the characterization of audio contents. Such\napproach needs some way of data compression aimed to\noptimize the information retrieval task and to reduce the\ncomputational costsderived fromtheusage ofclusterana-\nlysis tools and probabilistic models. A novel approach is\npresented in this paper, based on the standardized vario-\ngram. This tool, inherited from Geostatistics, is applied to\nMFCCsmatricestoreducetheirsizeandcomputecompact\nrepresentations of the audio contents (song signatures), ai-\nmed to evaluate audio similarity. The performance of the\nproposed approach is analyzed in comparison with other\nalternative methods and on the base of human responses.\n1. INTRODUCTION\nThecomputationofthedegreeofsimilarityamongsongsis\none of the most demanded tasks in the ﬁeld of multimedia\nprocessing,anditsinterestisstillgrowingwiththeincrea-\nsing popularity of on line services and databases. Music\nInformation Retrieval (MIR) stands for the tools to access\naudio contents with the aim to reorder, search and classify\nthem [1]. InMIREX2006 [3],the term‘Audio Similarity’\nwasintroducedfortheﬁrsttimeinthetaskslistand,conse-\nquently, a human evaluation system (Evalutron6000) was\ncreated to make quantitative evaluations of the proposed\nalgorithms.\nThemaintaskofaudiosimilarityevaluationisbasedon\nthe deﬁnition of some form of representation of the songs\n(signatures) to compare them and measure the closeness\nof the signature songs. The base of most of the known\nalgorithms for audio similarity evaluation are the Mel fre-\nquency cepstral coefﬁcients (MFCCs) [10]. The spectral\ninformationsuppliedbytheMFCCsisproposedtobecom-\npressed in a wide variety of different approaches by diffe-\nrentauthors[13][4][11][1]. Inthiswork,thestandardized\nvariogram [8] is presented as a novel tool to conveniently\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnotmadeordistributedforproﬁtorcommercialadvantageandthatcopies\nbear this noticeand thefull citation on theﬁrst page.\nc/circlecopyrt2010International Society for Music Information Retrieval.compress the MFCCs vectors for sorting similarsongs.\nThe outline of the paper follows: in Section 2, a gene-\nral description of the MFCCs is presented. In Sections 3\nand3.2,thedetailsoftheapproachbasedonthevariogram\nand its application to signal processing are described. In\nSection 4, the use of MFCCs matrices is presented and in\nSection 5 the application of the variogram is discussed in\ndetail. Finally, in Section 6, the experimental results are\npresentedandinSection7,theconclusionsandfuturepro-\nposals are discussed.\n2. MEL COEFFICIENTS AND AUDIO\nSIMILARITY\nThe MFCCs are short-term spectral-based features, origi-\nnally developed for speech recognition and successfully\nadapted to music information retrieval [10]. The compu-\ntation of MFCCs follows some crucial steps [13]: 1) the\ncalculus of the short-term spectrum of the signal, 2) the\ntransformationofthespectrumintotheMelscale(through\na triangular ﬁlter bank), 3) the calculus of the logarithm\nof the Mel spectrum and 4) the compression of the resul-\nting matrix through the application of the DCT (Discrete\nCosine Transform). MFCCs are widely used to generate\ncompact spectral representations of the song: the signal\nis framed into short fragments (usually some tens of mil-\nliseconds) and their coefﬁcients are computed frame by\nframe [13]. In order to conveniently represent the global\nspectralbehaviorofthesonginacompactway,theMFCCs\nvectors have to be clustered. For this task, several ap-\nproaches have been proposed by different authors. Pam-\npalk [13] uses GMM and EM approach, by modelling the\nprobability distribution functions of the coefﬁcients vec-\ntors. Foote[4]proposesasupervisedtree-structuredquan-\ntizer as discriminant approach for the sequential labeling\nof the coefﬁcients. Aucouturier and Pachet [1] present a\ncombinationofGMM/EMandMonteCarloapproachesto\nevaluate the likelihood between the MFCCs of two diffe-\nrent songs. Finally, Logan and Salomon [11] propose the\npopular K-means method for MFCCs clustering. In this\narticle, an alternative approach is proposed based on the\ncomputation of the variogram of the MFCCs, allowing for\nacomputationallylow-costcompressionofthecoefﬁcients\nand a simple calculus of the distance among the spectral\nsignatures.\n559\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)3. THE STANDARDIZED VARIOGRAM\nThe\nterm ‘variogram’, inherited from Geostatistics, stands\nfor the function describing the evolution of the spatial de-\npendence of a random ﬁeld [16]. Empirically used by the\nmine engineer D.G. Krige in South Africa mines [9], and\nlater formalized by G. Matheron in its pioneer works [12],\nthe variogram or ‘semivariance function’ is widely em-\nployed in spatial statistics to perform uncertainty model-\ninginaspatialframework. Itisoftenusedascharacteristic\nweighting function for the spatial interpolation technique\nknown as Kriging[9].\n3.1 Some mathematical issues\nA formal deﬁnition of the variogram is now provided. Let\nzα,withα= 1,···,nrepresentsasetof nsampledobser-\nvationsofaspatialphenomenon. Thevariogramisdeﬁned\nas half the variance of the increment [zα−zα+h][16]:\nγ(α,h) =1\n2E{[zα−zα+h]2}−{E[zα−zα+h]}2(1)\nAssuming the intrinsic stationarity of order two [16],\nthe mean of the variable E[z]is invariant for any transla-\ntion,thatis E[zα] =E[zα+h],thesecondtermofequation\n(1) can be neglected and the variance of the increment is\nsaid to be depending only on the distance vector hand not\non the position α[8]:\nγ(h) =1\n2E{[zα−zα+h]2} (2)\nwherezαandzα\n+hare two different samples of the ran-\ndom variable zseparated by a distance h.\nGiven a set of spatially distributed samples, the vario-\ngram can be estimated empirically [16]:\nγ∗(h) =1\n2N(h)Nh/summationdisplay\nα=1[zα−zα+h]2(3)\nwhere the number of pairs N(h)depends on the value of\nh. For its mathematical relation with the variance, the va-\nriogram is also known as semivariance function (or semi-\nvariogram).\nThe variogram is strictly related with the auto-covaria-\nnce function of the increment. In particular the covariance\nof the increment Cov(zα,zα+h), in condition of transla-\ntion invariance of the mean, can be expressed as follows:\nCov(zα,zα+h) =Cov(h) =E[zα·zα+h]−E[zα]2(4)\nwhereE[zα] =E[zα+h]. Whenhis zero,Cov(0)ismax-\nimum and it corresponds to the variance of the variable:\nCov(0) =E{[zα]2}−{E[zα]}2=Var(zα)(5)\nNote that equation (2) can be written as:\nγ(h) =1\n2E{[zα]2−2·zα·zα+h+[zα+h]2}(6)\nand using equations (4) and (5), we can express the vario-\ngram in termof the covariance function:\nγ(h) =Cov(0)−Cov(h) (7)The last equation shows the relation between the vario-\ngram and the covariance function [14]. Under the condi-\ntion of translation invariance of the mean, at h= 0, the\ncovariance is just the variance of the variable, Cov(0) =\nVar(z), and the variogram is zero, γ(0) = 0. Conversely,\nwhenthepairofelements, zαandzα+h,aretoofarawayto\nshow any kind of relation, their covariance is zero and the\nvariogram is the variance of the variable, γ(h) =Var(z).\nIn general, the covariance function shows a behavior op-\nposed to the behavior of the variogram (see Fig. 1).\nThe empirical variogram is usually ﬁtted by a theore-\ntical model to obtain a continuous function, modeling the\ncovariance exhaustively in the whole domain. The models\nare chosen within a group of admissible models that must\nbe positive-deﬁnite [6]. Moreover, the theoretical models\ncanbecharacterizedbyfewshapeparameters[6]: the Sill,\nthe asymptotic variance value the function tends to when\nthe lag distance, h, increases, the Range, the lag value at\nwhich the theoretical variogram reaches the sill, and the\nNugget effect, the discontinuity of the function at the ori-\ngin.\nWhensemivariancevaluesarenormalizedbytheglobal\nvariance, the variogram is reported as standardized vario-\ngram[8]anditscorrespondencecovariancefunctionisthe\ncorrelation function. Both the empirical and itscorrespon-\ndent theoretical standardized variogram are shown in Fig.\n1. The correspondent correlation function is shown too.\n0 100 200 300 400 500 600 70000.20.40.60.811.2\nLag□(h)Standardized□semivariance□( γ)\nExperimental□variogram\nTheoretical□variogramRangeSill\nNugget\neffect\nCovariance□function\nFigure1. Anexampleofatypicalstandardizedvariogram.\nThe\nempirical variogram (dotted line) isﬁtted by the theo-\nreticalmodel(solidline). Thecorrelationfunction(dashed\nline) is shown too.\n3.2 The variogram in signal processing\nManyauthorshavedealtwiththeuseofthevariogramcou-\npled to classical signal processing techniques, as a tool\nfor periodicity analysis of signals and time series analy-\nsis. Khachatryan and Bisgaard [8] employ the variogram\nas tool for estimating the stationarity of industrial time se-\nries data. Haslett [5] proposes the use of the variogram\nas a functional approach for time estimation in case of\nfault of the stationarity conditions. Kacha et al. [7] ap-\nply the generalized variogram to the linear prediction in\ndisordered speech analysis.\nIn spite of the different origins of the spatial variogra-\n560\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)phic approach and the time series analysis in signal pro-\ncessing,\ntheformercanbesuccessfullyappliedasanalter-\nnative tool for spectral analysis. In the case of time-signal\nprocessing,theparameter hisunidimensionalanditrepre-\nsent the time lag among the samples.\nIf we take a signal and we add an uncorrelated noise\ncomponentwithknownmeanandvariance(seeboxinside\nFig. 2(a)), we can observe that it is well reﬂected both in\nthe waveform and in the frequency spectrum. In the va-\nriogram, the added signal leads to a very small change in\nthe general shape of the curve (Fig. 2(b) and 2(c)), while\na marked increase of the variance at the origin (nugget\neffect) is noticeable. Such value corresponds to a contri-\nbution of about the 31% of the total variance of the dirty\nsignal. However, in the frequency spectrum, this change\nis mainly reﬂected in central-high frequency bands, where\nonlyadiffusedincreaseinamplitudeisapparent(Fig. 2(a)).\n0 0,5 1 1,5 2 2,5 300.511.52x 104\nNatural Frequency ( ω)Amplitude\n  \nOriginal Signal Spectrum\nDirty Signal Spectrum\n0100200300400500600700800240260280300320340\nSamplesAmplitude\n  \nOriginal Signal\nDirty Signal\nπ\n(a) Spectra of original (darker thick line) and dirty signal (lighter thin\nline). Inner box: raw signal.\n0 100 200 300 40000.51\nLag (h)Std. semivariance ( γ)\n  \nExp. Variogram (Original Signal)\nFitted Model (Original Signal)\n(b) Standardized Variogram of\noriginal\nsignals.0 100 200 300 40000.51\nLag (h)Std. semivariance ( γ)\n  \nExp. Variogram (Dirty Signal)\nFitted Model (Dirty Signal)\n(c) Standardized Variogram of\ndirty\nsignals.\nFigure 2. Spectra and standardized variograms of a clean\nsignalandofthesamesignalcorruptedwithadditivenoise.\nExperimentalvariogramsarerepresentedwithadottedline\nand the theoretical ﬁtted model with a solid line.\nThevariogramcanalsobeconvenientlyusedastoolfor\nsound analysis applications. Dillon et al. [2] noted as the\nvariogram ﬂuctuations, once reached the sill, are strictly\nrelated with signal spectrum. He remarks that the vario-\ngram can be especially useful for fundamental frequency\ndetection, by taking into account the variance pseudope-\nriodic pattern known as hole effect [6].\n4. COMPRESSION OF MATRICES\nThe standardized variogram described in Section 3 is pro-\nposedhereasanovelmethodforreducingthedimensiona-\nlity of the MFCCs matrices. It is computed on each vectorof coefﬁcients throughout the frames of the song, to ob-\ntain a function describing the evolution of the covariance\nthrough the time. With the aim to compress the MFCCs\ninformation, the variogram is computed only on a reduced\nnumber of lags (values of distance hfor which the vario-\ngram is calculated). As shown in Fig. 1, the variogram\ntypically presents a logarithmic-like rising behavior at the\nlowest lags and an asymptotic trend to the global variance\n(equal to one in the case of standardized variogram) from\nlags approaching the range, forward. Taking into account\nthesetwofactors,atotalamountoftenlagsvaluesaresam-\npled logarithmically from 1 to half the length of Mel coef-\nﬁcients.\nFor each row of the MFCCs matrices, the semivariance\niscomputedforallthepairsofsampleslocatedatdistances\nequal to the lags selected, and the values are normalized\nby the variance of the MFCCs row data. The outcome is\nacompactfunctionkeepingenoughinformationtocharac-\nterize the signal.\nTheexperimentalstandardizedvariogramcanbechara-\ncterized on the basis of two parameters extracted from its\ncorrespondent theoretical function (although the latter is\nnot explicitly calculated in this application): the range and\nthe nugget effect. In this case, these parameters can be\ninterpreted on the base of their spectral meaning.\nThe range can be interpreted as the time scale at witch\ntheperiodicityofthesignalbeginstobeevident. Uptothe\nrange, the structured variability of the variable masks its\nperiodicity, while, when the pairwise covariance starts to\nbe weak enough (from range forward), that periodic beha-\nviorrisesanditbecomesevident. Clearly,duetothestrong\nreduction of lags, sometimes the range can be poorly de-\ntected by the reduced variogram.\nThe nugget effect is very important to understand the\nsmall-scale behavior of the Mel coefﬁcients. The discon-\ntinuity at lag h→0explains the variation of the signal at\nverysmall-scale. Intermsofspectralanalysis,itstandsfor\nthe high frequency contribution to the total variance in the\nMel spectra.\nAn example of the application of the variogram to the\nMFCC spectra is shown in Fig. 3. The ﬁrst song (Fig.\n3(a)) is a piece from the genre ‘Classic’ [3], its spectrum\nshows a rather clear periodicity, with some peculiar pat-\nterns repeating with a certain regularity. The second song,\nbelongingtothegenre‘Heavymetal’,showsamorefuzzy\nspectrum with higher frequency variations and a less evi-\ndentperiodicity. Suchdifferencesarewellreﬂectedintheir\ncorrespondent variograms.\nTheclearerregularityoftheclassicpieceisreﬂectedby\na certain degree of periodicity in the variogram (although\nnot exhaustively revealed by the reduced number of lags).\nMoreover, the very low nugget variance value reﬂects the\nhigh degree of regularity with reduced high frequency os-\ncillations (Fig. 3(b)).\nIn the case of the heavy metal piece, the very high fre-\nquency oscillations (Fig. 3(c)) are well reﬂected by a no-\ntablenuggetvariance(aboutthe50%ofthetotalvariance)\nand by a larger range indicating the lack of a structured\n561\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)0501001502002503000204060\nSong framesAmplitude\n(a) The second Mel coefﬁcient of a\nclas\nsical music song.0 50 100 15000.511.5\nLag(h)Standardized semivariance ( γ)\n  \nExperimental variogram\nGlobal variance\nFitted theoretical model\n(b) Standardized reduced vario-\ngram\nof the signal in ﬁg. 3(a) (dot-\nted line).\n05010015020025030001020304050\nSong framesAmplitude\n(c) The second Mel coefﬁcient of a\nhea\nvy metal song.0 50 100 15000.511.5\nLag(h)Standardized semivariance ( γ)\n  \nExperimental variogram\nGlobal variance\nFitted theoretical model\n(d) Standardized reduced vario-\ngram\nof the signal in ﬁg. 3(c) (dot-\nted line).\nFigure 3. A comparison between the variograms of the\nMFCCsspectraoftwoverydifferentsongs,aclassicalmu-\nsic song (top) and a heavy metal song (bottom). Fitted\ntheoretical model (thick lighter line) and global variance\n(dashed line) are shown too. The pieces are 35 seconds\nlength.\nvariability. Note that, although poorly reﬂected by the re-\nducedavailabilityoflags,somedegreeofnon-stationarity,\nexpressedasthelackofawelldeﬁniteasymptoticityofthe\nvariogram [16], is present in this case.\n5. STANDARDIZED VARIOGRAM FOR AUDIO\nSIMILARITY ASSESSMENT\nAs mentioned before, the variogram has been employed\nfor audio similarity assessment. For each piece, the Mel\ncoefﬁcients are calculated and the standardized variogram\nis computed for each coefﬁcient, obtaining a compact sig-\nnature of the track. Successively, the signatures are com-\npared, by computing a weighted difference of their ele-\nments.\nAfter computing the standardized variogram (10 lags)\nof the 19 Mel coefﬁcients for each song (the ﬁrst one has\nbeen neglected [13]), the resulting 10×19matrices (sig-\nnatures) are compared by averaging the weighted absolute\nvalue of their difference, according to the following equa-\ntion:\nD=1\nI·J10/summationdisplay\nj=119/summationdisplay\ni=1ωj|Va(i,j)−Vb(i,j)|(8)\nwhereVaandVbare the signature matrices for song aand\nb,respectivelyandtheindexes iandjarereferredtothe19\ncoefﬁcients and the 10 lags, respectively. Differences are\nlinearly weighted in order to give more importance to the\nsmall-scale lags of the variogram vectors. The vector Ω =\n[ω1,...,ω 10]contains the 10linearly decreasing weights\nωj,suchthat/summationtext10\nj=1ωj= 1. Each j-thweightiscomputed\nas follows:ωj=11−j\nD(9)\nwhereD=10/summationtext\nj=\n1j. Audio similarity is simply evaluated by\nsortingthesongswithrespecttoareferencepiece,accord-\ning to their reciprocal distance, computed using equation\n(8).\n6. EXPERIMENTAL RESULTS AND DISCUSSION\nAnobjectiveevaluationofthesortingcapabilityoftheme-\nthod is very hard to achieve because of the subjectiveness\nof the concept of ‘audio similarity’. Actually, one song\ncanbejudgedasmoresimilartoanotheronedependingon\na series of parameters (rhythm, spectral content, melody\netc.) that are subconsciously evaluated by the listeners.\nIn order to obtain a robust and objective estimate of the\nperformance of the method, a series of tests performed by\na group of users, have been carried on. A total of 5 lists\nof songs have been submitted to 10 users who sorted them\nwith respect to a set of reference songs, without any pre-\nvious knowledge about any tagging or taxonomy of the\ndataset. The test songs are sampled by the Audio Descrip-\ntionContext database oftheISMIR2004 [3]and belong to\nall the genres presented in the database.\nSuccessively,thelistscreatedmanuallyhavebeencom-\npared with the outcomes of 4 automatic methods, the va-\nriogram-basedmethodandotherthreemethodsthatcanbe\nfound in the literature:\n1. Fluctuation Patterns [13]\n2. MFCCs with GMM/EM clustering approach [13]\n3. MFCCs with K-means clustering approach\nTheﬂuctuationpatterns,describingtheamplitudemod-\nulationoftheloudnessofthefrequencybands,areusedby\nPampalk[13]tobrieﬂycharacterizethesongspectralcon-\ntent. The Gaussian Mixture Models coupled with Expec-\ntation/Maximization approach are employed by the same\nauthor to cluster the Mel coefﬁcients in 30 vectors (G30)\nof 19 elements. The third method is the same approach\nusedbyLoganandSalomon[11],basedonthecalculusof\ntheMFCCsclusteredbythepopularK-means,withtheEu-\nclidean distance instead of the Kullback-Leibler distance.\nAtotalamountofsometensoflistshavebeenobtained\nby the manual sorting by the users. A rapid look at these\nlists reveals a strong lack of homogeneity among them. It\nis related to the high subjectiveness of the sorting process\nand the variability of the human perception of the ‘audio\nsimilarity’. Thisleadstothelackofarepresentativelistfor\neach reference song. Instead of trying to extract a unique\nreferencelistamongtheusers,theauthorsturnedtoderive\na measure of the agreement among the users.\nA weighted matching score has been computed, taking\ninto account the reciprocal distance of the songs (in terms\nofpositionindexinthelist). Suchdistanceshavebeenline-\narlyweighted,suchthattheﬁrstsongsinthelistsreﬂected\nmoreimportancethanthelastones. Actually,itiseasierto\n562\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)deﬁne the order of few very similar songs, than to sort the\nvery\ndifferent ones.\nLetLαandLβrepresent two different lists of nsongs,\nforthesamereferencesong,thematchingscore Shasbeen\ncomputed using the following equation:\nS=n/summationdisplay\ni=1|i−j|·ωi (10)\nwhereiandjare the indexes for lists LαandLβ, respec-\ntively. In particular, jis the index of the j-th song in list\nLβ, such that Lα(i)≡Lβ(j). In practice, the i-th song\nin the list Lαis searched in Lβand their correspondent\nindexes are compared. The absolute difference is linearly\nweighted by the weights ωias referred in equation (9).\nFinally, the scores are transformed to be represented as\npercentage of the maximum score attainable.\nForeachreferencesong,thematchingscoreshavebeen\ncomputed among all the available lists, both among the\nusers lists and among the users lists and the ones returned\nbytheautomaticmethods. Thus,twodifferentsetsofscores\nhave been obtained: the inter-users scores and the users-\nautomatic scores. The measure of the performance of the\nautomatic method is drawn by the degree of similarity of\nthe two sets, that is, how close are the scores computed\namongtheuserslistsandthelistsreturnedbytheautomatic\nmethod. In order to have an estimation of such closeness,\nthe coherence among the two sets of scores is computed\nby a statistical test. The Kolmogorov-Smirnov test [15]\nhas been used to measure the correspondence between the\ntwodistributionsofthetwosetsofscores,beforeandafter\nthe inclusion of the automatic list.\nIn Table 1, the basic statistics for both the distributions\noftheinter-usersscoressetandtheusers-automaticscores\nsets are shown. The results of the statistical test (H ) is\nshown too.\nThedegreeofsimilarityamongthesongsisaverysub-\njective response and only a high number of cases can gua-\nrantee a reliable response. Nevertheless, the statistical re-\nsults are enough to have an idea of the performance of the\nautomatic methods. The response of the users can be seen\nassomeformofquantifyingthedifﬁcultylevelofthesort-\ning task. When the songs are easily sortable, the users\nshowahighdegreeofagreement(highmeanscores). Con-\nversely, when the similarity among the songs is not very\nclear, the discrepancy among the users increases and, to-\ngether with a decrease of the centrality measures (mean\nand median), an increase of the variance is appreciable.\nActually,thestandarddeviationisanindexofthedisagree-\nmentamongtheusersandcanberelatedwiththecomplex-\nity of the sorting procedure.\nIn the test results, the discordance among the users is\nwell reﬂected by high values of the standard deviation in\nmost of the cases. The mean standard deviation for the 5\ncases is about the 14% of the mean score.\nIngeneral,awidevarietyofperformancesareshownby\nthe different methods. The method based on theclustering\noftheMelcoefﬁcientsbytheGMM/EMapproachreaches\nthebestscorein3cases,forsongsB,CandD,whileitfailsRef.song Method MeanMedian Min Max Skewness St.Dev. H\nSong AUs\ners 72.3 74.9 33.4 90.9 -1.0 13.2 -\nMFCC-Var 71.5 75.2 42.8 82.5 -1.8 11.3 0\nFP 71.2 72.5 43.5 85.3 -1.3 11.6 0\nMFCC-EUC 70.8 71.8 41.8 84.8 -1.4 12.1 0\nMFCC-G30 55 54.3 41.3 69.4 0.1 91\nSong BUs\ners 81.8 83.9 52.7 99.2 -1 9.6 -\nMFCC-Var 75.4 76.8 54.4 87.8 -1.2 8.7 1\nFP 66.6 66.1 58 81.8 0.9 6.9 1\nMFCC-EUC 67.5 66.6 61.572.9 0.1 4.31\nMFCC-G30 76.7 82.3 41.599.2 -0.7 14.2 1\nSong CUs\ners 84.3 85.6 66.6 96.2 -0.2 6.6 -\nMFCC-Var 71.3 70.8 66.1 76.2 0.2 3.51\nFP 70 69.9 58 82.8 0.1 6.5 1\nMFCC-EUC 81.8 83.2 71.4 90.9 -0.4 6.1 0\nMFCC-G30 86.9 89.1 73.7 92.9 -1.1 6.2 0\nSong DUs\ners 77.2 77.7 57.5 96.2 0 8.2 -\nMFCC-Var 60.8 60.1 57.2 71.1 1.4 4.5 1\nFP 57.3 54.6 50.9 73.4 1.3 7 1\nMFCC-EUC 66.1 65.1 59.7 84.6 1.9 7 1\nMFCC-G30 77.8 83.0 63.0 86.6 -0.5 8.8 0\nSong EUs\ners 65.7 66.1 22.8 93.2 -0.4 15.4 -\nMFCC-Var 67.8 68.5 56.2 82.3 0.18.1 0\nFP 59.2 60.6 42.3 70.1 -0.5 9.8 0\nMFCC-EUC 34.1 34.2 11.6 62 0.1 15.4 1\nMFCC-G30 49.9 49.4 30.9 63.3 -0.4 9.7 1\nMeanUs\ners 76.3 77.6 - - - 10.6 -\nMFCC-Var 69.4 70.3 - - - 7.2-\nFP 64.9 64.7 - - - 8.4 -\nMFCC-EUC 64.0 64.2 - - - 9.0 -\nMFCC-G30 69.371.6- - - 9.6 -\nTable 1. Basic statistics of the distributions of the inter-\nusers\nscores set and the users-automatic scores set. Val-\nues are in percent. Results of statistical test are shown\ntoo:H= 0means that the two distributions are coher-\nent, while H= 1stands for a distributions mismatch. The\ncodes for the automatic methods stand for: MFCC/Var =\nMFCCs clustered by standardized variogram, FP = ﬂuc-\ntuation patterns, MFCC-EUC = MFCCs clustered by K-\nmeans,MFCC-G30=MFCCsclusteredbyGMM/EMme-\nthod. Best results in bold.\nthe test for song B. The method based on the clustering of\nthe Mel coefﬁcients by the variogram returns the highest\nscoresforsongsAandE.Italsoreturnsthesecondhighest\nscore for song B, although failing the test, but with the\nhighestp-value (not shown in the table).\nThebestresultsareattainedforthesongA,wherethree\nof the four methods pass the test, while, for song B, none\nof them return a sufﬁcient matching with the inter-users\ndistribution. This last issue is basically related with the\nhigh agreement shown by the users (about 82%) that is\nhardly attained by the automatic methods. Quite the same\nsituation occurs for song C and D, with high mean scores\namong the users lists (more than 84% and 77%, for songs\nC and D, respectively) approached by only two of the four\nmethods proposed. Finally, the song E reveals a very low\nmean inter-users score (about 66%), well reﬂected by all\nthemethods. Globally,allthemethodsshowagoodperfor-\n563\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)mance, with averaged mean values higher than 64%. The\nvariogram-based\napproach shows the highest mean value,\nwith69.4%,veryclosetotheresultbytheGMM/EMbased\nmethod.\n7. CONCLUSIONS AND FUTURE WORKS\nA new approach based on the use of the standardized va-\nriogramfortheclusteringoftheMelcoefﬁcientsforaudio\nsimilarity evaluation has been proposed. The variogram\nis calculated on a reduced vector of ten lag elements and\nit is standardized by the global variance, in order to ob-\ntaincomparablesignaturematricesfordifferentsongs. The\nmethod capability is evaluated on the base of a statistical\ncomparisonamongthedistributionsofthematchingscores\ncomputed among a set of users lists and the ones returned\nby the automatic method. Moreover, for a more complete\nassessment of the method performance, other three known\nmethods employed in literature for audio similarity eval-\nuation are computed, and their correspondent scores are\ncompared.\nPerformances vary from quite poor to very good for all\nthe methods, with mean matching scores varying from the\nlowest mean value of about 34% for the method based\non the Euclidean distance and the highest value of about\n87%forthemethodbasedontheclusteringbyGMM/EM.\nThe averaged mean values reveal a good global perfor-\nmance of the method based on the variogram and on the\nGMM/EMapproach,withquitepoorerresultsbytheother\ntwo ones. In practice, the variogram-based method pro-\nposed here works quite well and its performance can be\ncomparedwiththeoneofothermorepopularmethodsthat,\ninsomecases,showahigherdegreeofcomputationalcom-\nplexity.\nThe capability of the method can be improved, by opti-\nmizingsomecalculationparameters,asthesamplingofthe\ndistance lags values. Moreover, the theoretical variogram\ncan be evaluated and its shape parameters can be taken\ninto account to optimize the modeling of the spectral con-\ntentofthesongtoimprovetheaudiosimilarityassessment\ntask. The evaluation task can be improved by increasing\nthe number of users and broadening the test samples.\n8. ACKNOWLEDGMENTS\nThis work was supported by the Ministerio de Educaci ´on\ny Ciencia of the Spanish Government, under Project No.\nTSI2007-61181, by the Ministerio de Industria, Turismo y\nComercio of the Spanish Government, under Project No.\nTSI020501-2008-0117 and by the Junta de Andaluc ´ıa, un-\nder Project No. P07-TIC-02783.\n9. REFERENCES\n[1] Jean-Julien Aucouturier and Francois Pachet.\nMusic similarity measures: What’s the use ?\nhttp://citeseerx.ist.psu.edu/viewdoc/summary?\ndoi=10.1.1.9.5609, 2002.\n[2] C.G.Dillon,C.Lloyd,andL.Philip.Identifyingshort-\nrange and long-range structural components of a com-\npactedsoil: anintegratedgeostatisticalandspectralap-proach.Computers and Geosciences, 29:1277–1290,\nDecember 2003.\n[3] Stephen J. Downie. The music informa-\ntion retrieval evaluation exchange (mirex).\nhttp://www.dlib.org/dlib/december06/downie/12-\ndownie.html.\n[4] Jonathan T. Foote. Content-based retrieval of music\nand audio. In Multimedia Storage and Archiving Sys-\ntems II, Proc. of SPIE, pages 138–147, 1997.\n[5] John Haslett. On the sample variogram and the sam-\nple autocovariance for non-stationary time series. The\nStatistician, 46(4):475–485, 1997.\n[6] Edward H. Isaaks and Mohan R. Srivastava. An In-\ntroduction to Applied Geostatistics. Oxford University\nPress, USA, January 1990.\n[7] A. Kacha, F. Grenez, J. Schoentgen, and K. Benma-\nhammed.Dysphonicspeechanalysisusinggeneralized\nvariogram. In Acoustics, Speech, and Signal Process-\ning, 2005. Proceedings. (ICASSP '05). IEEE Interna-\ntionalConferenceon,volume1,pages917–920,2005.\n[8] DavitKhachatryanandSrenBisgaard.Someresultson\nthe variogram in time series analysis. Quality and Re-\nliability Engineering International, March 2009.\n[9] Daniel G. Krige. A statistical approach to some basic\nminevaluationproblemsonthewitwatersrand. Journal\nof the Chemical, Metallurgical and Mining Society of\nSouth Africa, 52(6):119–139, December 1951.\n[10] Beth Logan. Mel frequency cepstral coefﬁcients for\nmusic modeling. In In International Symposium on\nMusic Information Retrieval, 2000.\n[11] BethLoganandArielSalomon.Acontent-basedmusic\nsimilarity function. Technical report, Processing Lan-\nguages – Document Style Semantics and Speciﬁcation\nLanguage(DSSSL).Ref.No.ISO/IEC10179:1996(E),\n2001.\n[12] George Matheron. The theory of regionalized vari-\nables and its applications. Les cahiers du CMM de\nFontainebleau, 5, 1971.\n[13] E. Pampalk. Computational Models of Music Simila-\nrity and their Application to Music Information Re-\ntrieval. PhD thesis, Vienna University of Technology,\nVienna, March 2006.\n[14] S. Sammartino. Geostatistical models for environmen-\ntal datasets. PhD thesis, University of Napoli ”Fed-\nerico II”, Napoli (Italy),March 2006.\n[15] M. A. Stephens. Edf statistics for goodness of ﬁt and\nsomecomparisons. JournaloftheAmericanStatistical\nAssociation, 69(347):730–737, 1974.\n[16] Hans Wackernagel. Multivariate Geostatistics: An In-\ntroduction With Applications. Springer-Verlag Telos,\nJanuary 1999.\n564\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "On the Use of Microblogging Posts for Similarity Estimation and Artist Labeling.",
        "author": [
            "Markus Schedl"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1418215",
        "url": "https://doi.org/10.5281/zenodo.1418215",
        "ee": "https://zenodo.org/records/1418215/files/Schedl10.pdf",
        "abstract": "Microblogging services, such as Twitter, have risen enor- mously in popularity during the past years. Despite their popularity, such services have never been analyzed for MIR purposes, to the best of our knowledge. We hence present first investigations of the usability of music artist-related microblogging posts to perform artist labeling and simi- larity estimation tasks. To this end, we look into different text-based indexing models and term weighting measures. Two artist collections are used for evaluation, and the dif- ferent methods are evaluated against data from last.fm. We show that microblogging posts are a valuable source for musical meta-data.",
        "zenodo_id": 1418215,
        "dblp_key": "conf/ismir/Schedl10",
        "keywords": [
            "Microblogging services",
            "popularity",
            "MIR purposes",
            "artist labeling",
            "similarity estimation",
            "text-based indexing models",
            "term weighting measures",
            "artist collections",
            "last.fm",
            "valuable source"
        ],
        "content": "ON THE USE OF MICROBLOGGING POSTS FOR\nSIMILARITY\nESTIMATION AND ARTIST LABELING\nMarkusSchedl\nDepartment of Computational Perception\nJohannes Kepler University\nLinz, Austria\nmarkus.schedl@jku.at\nABSTRACT\nMicroblogging services, such as Twitter, have risen enor-\nmously in popularity during the past years. Despite their\npopularity,suchserviceshaveneverbeenanalyzedforMIR\npurposes, to the best of our knowledge. We hence present\nﬁrst investigations of the usability of music artist-related\nmicroblogging posts to perform artist labeling andsimi-\nlarity estimation tasks. To this end, we look into different\ntext-based indexingmodels and term weighting measures.\nTwo artist collections are used for evaluation, and the dif-\nferentmethodsareevaluatedagainstdatafrom last.fm. We\nshow that microblogging posts are a valuable source for\nmusical meta-data.\n1. INTRODUCTION\nWiththeemergence ofblogging services,socialnetworks,\nplatforms to share user-generated content and correspond-\ning tags, services for music recommendation and person-\nalized Web radio, such as last.fm[12], and in general all\nservicesandplatformscommonlysummarizedbytheterm\n“Web 2.0”, a new era of Web-based user interaction has\nstarted. The term “Web 2.0” was coined in 1999 by DiN-\nucci [5], but did not become popular until 2004, when\nO’Reilly launched the ﬁrst Web 2.0 conference [19].\nMicroblogging is one of the more recent phenomena in\nthecontextofthe“Web2.0”. Microbloggingservicesoffer\ntheir users a means of communicating to the world in real\ntime what is currently important for them. Such services\nhad their origin in 2005, but gained greater popularity not\nbefore the years 2007 and 2008 [28]. Today’s most pop-\nular microblogging service is Twitter[30], where millions\nof users post what they are currently doing or what is cur-\nrently important for them. [9]\nDespite the enormous rise in usage of microblogging\nservices, to the best of our knowledge, they have not been\nused for music information extraction and retrieval yet.\nHence, in this paper we present ﬁrst steps towards assess-\ning microblogging posts for the MIR tasks of music artist\nlabelingandsimilarity measurement. We will show that\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnotmadeordistributedforproﬁtorcommercialadvantageandthatcopies\nbear this noticeand thefull citation on theﬁrst page.\nc/circlecopyrt2010International Society for Music Information Retrieval.even though such data is noisy and rather sparse, results\ncomparabletoothertext-basedapproachescanbeachieved.\nThe remainder of the paper presents and discusses re-\nlated literature (Section 2), elaborates on the methods em-\nployedforsimilaritymeasurementandartistlabeling(Sec-\ntion 3), gives details on the conducted evaluation experi-\nments and discusses their results (Section 4), and ﬁnally\nsummarizes the work (Section 5).\n2. RELATED WORK\nAs this work is strongly related to text-based music infor-\nmation extraction and to Web content mining, we are go-\ningtoreviewrelatedworkonthesetopicsinthecontextof\nMIR. The past ﬁve years have seen the emergence of vari-\noustext-basedstrategiestoaddressMIRtasks,suchasau-\ntomated labeling, categorizing artists according to a given\ntaxonomy, or determining similarities between tracks or\nartists.\nEarlyworkontext-basedMIRfocusedonextractingin-\nformation from artist-related Web pages. Cohen and Fan\n[4]querysearchenginestogathermusic-relatedWebpages,\nparse their DOM trees, extract the plain text content, and\ndistill lists of artist names. Similarities based on co-occur-\nrences of artist names are then used for artist recommen-\ndation. Web pages as data source for MIR tasks are also\nusedin[7,32],wheretheauthorsrelyonasearchengine’s\nresults to artist-speciﬁc queries to determine artist-related\nWeb pages. From these pages, weighted term proﬁles,\nbasedonspeciﬁctermsets(e.g.,adjectives,unigrams,noun\nphrases),arecreatedandusedforclassiﬁcationandrecom-\nmendation. BaumannandHummel[3]extendthisworkby\nintroducingcertainﬁlterstoprunethesetofretrievedWeb\npages, aiming at suppressing noisy pages. Another exten-\nsion is presented in [10] for similarity measurement and\ngenre classiﬁcation. Knees et al. do not use speciﬁc term\nsets, but create a term list directly from the retrieved Web\npages and use the χ2-test for term selection, i.e., to ﬁlter\nout terms that are less important to describe certain gen-\nres. Other Web-based MIR approaches use page count es-\ntimatesreturnedbysearchengines. Forexample,in[8,26]\nco-occurrences of artist names and terms speciﬁc to the\nmusic domain, as returned by search engine’s page count\nestimates, are used to categorize artists.\nAnother category of Web-based approaches to derive\nartist similarity exploits user-generated playlists. For ex-\nample, in [2] Baccigalupo et al. analyze co-occurrences of\nartists in playlists shared by members of a Web commu-\n447\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Term Set Cardinality Description\nallterms 681,334 Alltermsthatoccurinthecorpusoftheretrieved Twitterposts.\nartist names 224Names of the artistsfor which data was retrieved.\ndictionary 1,398Manually created dictionary of musically relevant terms.\nlast.fm toptags overall 250Overall top-ranked tags returned by last.fm ’s\nTags.getTopTags function.\nlast.fm toptags collection 5,932Aggregatedtop-rankedtagsretrievedfrom last.fmfor allartists\nin the collection.\nlast.fm toptags topartists 12,499 Aggregated top-ranked tags retrieved from last.fmforlast.fm ’s\n2,000 top-played artists.\nTable1. Listofthetermsetsusedtoindexthe T witterposts. Thecardinalitiesoftermsets allterms, artist names,\nandlast.fm toptags collection are based on the collection C224a.\nnity. More than one million playlists made publicly avail-\nable viaMusicStrands [18] (no longer in operation) were\ngathered. The authors not only consider the co-occurrence\noftwoartistsinaplaylistasanindicationfortheirsimilar-\nity, but also take into account that two artists that consec-\nutively occur in a playlist are probably more similar than\ntwo artists that occur farther away from each other.\nA recent approach derives similarity information from\ntheGnutella [22]P2P ﬁle sharing network. Shavitt and\nWeinsberg [27] collected metadata of shared music ﬁles\nfrommorethan1.2million Gnutellausers.Theauthorsuse\nthis data for artist recommendation and song clustering,\ngivingspecialemphasistoadjustingforthepopularitybias.\nAnother data source related to the “Web 2.0” is social\ntags. [11] gives a good overview of their use in MIR. In\n[15] a semantic space is built, based on social tags ex-\ntracted from last.fmandMusicStrands. The authors use\nthis data for categorizing tracks into mood categories and\npresent a user interface to browse a music collection ac-\ncordingtomood. Asanalternativetoretrievingsocialtags\nfrommusicinformationsystems,tagsmayalsobegathered\nvia games designed to encourage their players to assign\nmeaningful descriptions to a music piece [14,17,29]. Due\nto their design, this method can effectively reduce noise.\n3. MININGTWITTER POSTS\nTo acquire user posts we queried Twitter’s Web API [31]\nin February and March 2010 with the names of the mu-\nsic artists under consideration. We downloaded up to 100\nposts per query and extracted the plain text content. Ear-\nlier work on text-based music information retrieval [10,\n26,32] suggests to enrich the artist names with additional\nkeywords, such as “music review” or “music genre style”,\nto guide the retrieval process towards sources that con-\ntain information on music. However, preliminary classi-\nﬁcation experiments with various additional music-related\nkeywordsrevealedthatthisstrategydoesnotworkwellfor\nTwitterposts. Restricting the search with any keyword in\naddition to the artist name in fact decreases the number of\navailable user posts so strongly that even for the popular\nartists in our test collection C224a(cf. Subsection 4.1) the\nresulting feature vectors become very sparse.\nAfterhavingdownloadedthe Twitterpostsforeachartist,\nwebuiltan invertedword-levelindex [34]basedonamod-\niﬁed version of the lucene[16] indexer. To investigate the\ninﬂuenceofthetermsetusedforindexing,webuiltvarious\nindexes using the term sets depicted in Table 1. The tablefurther gives the term sets’ cardinality. In cases where this\ncardinalitydependsonthesizeofthecorpus,thevaluesare\nbased on collection C224a(cf. Subsection 4.1). The list\ndenotedas dictionary consistsoftermsthatwemanu-\nally collected from various sources and somehow relate to\nmusic. This list resembles the one used in [21] and [24].\nIncluded termsrepresent, forexample, musical genres and\nstyles, locations, instruments, emotions, and epochs.\nTerm weighting is performed using variants of the term\nfrequency (tf) measure and the term frequency ·inverse\ndocument frequency (tf·idf) measure [33]. The term fre-\nquency tft,aisthetotalnumberofoccurrencesofterm tin\nallTwitterposts retrieved for artist a. The tf·idft,afunc-\ntion is deﬁned as follows, where nis the total number of\nartistsand dftisthenumberofartistswhoseretrievedposts\ncontain tat least once:\ntf·idft,a= ln (1 + tft,a)·ln/parenleftig\n1 +n\nd ft/parenrightig\n(1)\nThe\nbasic idea of the tf·idft,ameasure is to increase the\nweight of tiftoccurs frequently in the posts retrieved for\na, and decrease t’s weight if toccurs in a large number of\nposts retrieved for differentartists and is thus not very dis-\ncriminative for a.\nSince we are not interested in individual Twitterposts, but\nrather in a document describing a certain music artist, we\naggregateallpostsretrievedforanartistintoavirtualdoc-\nument, based on which the term weights are calculated.\n3.1 Similarity Estimation\nBased on the term weighting vectors, we derive similar-\nity between artists by applying the cosine similarity mea-\nsure[23]. The cosine measure normalizes the data in that\nit accounts for different document lengths. To this end,\nonly the angle between the weight vectors in the feature\nspace is considered. In our case, the virtual documents for\ntwo artists aandbmay be of very different length (de-\npending on the number and length of the corresponding\nposts), which is likely to distort the weighting.1There-\nfore, we apply the cosine similarity measure between the\ntf·idfvectors of each pair of artists (a,b)according to\nFormula2,where |T|isthecardinalityofthetermset,i.e.,\nthe dimensionality of the term weight vectors. θgives the\n1Thefact that usually much more data is available for popular artists\nthan for lesser known ones, and the resulting likely distortion of results,\nis commonly referred to as “popularity bias”.\n448\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)angle between a’s andb’s feature vectors in the Euclidean\nspace.\nsim(a,b) = cos θ=\n\n|T|/summationdisplay\nt=1tf·idft,a·tf·idft,b\n/radicaltp/radicalvertex/radicalvertex/radicalvertex/radicalvertex/radicalbt|T|/summationdisplay\nt=1t\nf·idf2\nt,a·/radicaltp/radicalvertex/radicalvertex/radicalvertex/radicalvertex/radicalbt|T|/summationdisplay\nt=1t\nf·idf2\nt,b\n(2)\n3.2 Labeling\nA good similarity estimation function is crucial for many\napplication areasofMIRtechniques, forexample, tobuild\nrecommender systems, to generate intelligent user inter-\nfaces via clustering, or for automated playlist generation.\nAnother related MIR task is automatically assigning la-\nbels/descriptors to an artist or a song. This allows to per-\nform categorization of artists or songs into certain classes,\nfor example, mood categories or a genre taxonomy.\nWe were interested in analyzing the potential of user-gen-\neratedTwitterpoststoperformautomatedcategorizationor\nlabeling of music artists, also known as “autotagging” [6].\nTo this end, we compiled a list of last.fm’s top tags for the\ntopartists(56,396uniqueterms)andsubsequentlyindexed\ntheTwitterposts,takingthislistasdictionaryforourmod-\niﬁedluceneindexer. Employing either the tfor the tf·idf\nmeasure, we used the top-ranked terms of each artist to\ngenerate labels.\n4. EVALUATION\n4.1 Test Collections\nTo compare the results of the proposed approaches to ex-\nisting methods, we ﬁrst ran evaluation experiments on the\ncollection presented in [10]. It comprises 224 well-known\nartists,uniformlydistributedacross14genres. Wewillde-\nnote this collection as C224ain the following.\nSince we further aim at evaluating the approaches on a\nreal-worldcollection,weretrievedthemostpopularartists\nas of the end of February 2010 from last.fm. To this end,\nweusedlast.fm’sWebAPI[13]togatherthemostpopular\nartists for each country of the world, which we then ag-\ngregated into a single list of 201,135 artist names. Since\nlast.fm’s data is prone to misspellings or other mistakes\ndue to their collaborative, user-generated knowledge base,\nwe cleaned the data set by matching each artist name with\nthedatabaseoftheexpert-basedmusicinformationsystem\nallmusic.com [1]. Starting this matching process from the\nmost popular artist found by last.fm, and including only\nartist names that also occur in allmusic.com, we eventu-\nally are given a list of 3,000 artists. We will denote this\ncollection, which is used for artist labeling, as 3000a.\n4.2 Similarity Estimation\nWhiletheauthorsarewellawareofthefactthat“genre”is\nanill-deﬁnedconceptandthatgenretaxonomiestendtobe\nhighly inconsistent [20], we unfortunately do not have ac-\ncess to reliable and comprehensive similarity data, against\nwhich we could perform comparison. We therefore optedfor a genre classiﬁcation task that serves as a proxy for\nsimilarity measurement. We employed a k-nearest neigh-\nborclassiﬁer (leave-one-out), and we investigated classi-\nﬁcation accuracy for different values of k, different term\nsets used for indexing, and different term weighting mea-\nsures(tfandtf·idf). Werantheclassiﬁcationexperiments\non collection C224a, since this artist set is already well-\nestablished in the literature, and results are therefore easy\nto compare.\n4.2.1 Results\nFigure 1 shows a detailed illustration of the k-NN classi-\nﬁcation results for different term sets and term weighting\nmeasures,usingcollection C224a. Ingeneral, tf·idfworks\nbetter for the task of similarity estimation than the single\ntfvalue. The best classiﬁcation results achievable using\ntf·idfare72.52%accuracywith alltermsanda9 -NN\nclassiﬁerand 72.38%accuracywithan 8-NN-classiﬁerand\nlast.fm toptags collection.\nInterestingly\n, thetf-based predictors (which, in general,\nperform worse than the tf·idf-based predictors), perform\ncomparable to the best tf·idf-based classiﬁers when us-\ningartist namesfor indexing. This setting resembles\nthe\nco-occurrenceapproachdescribedin[25],whereaccu-\nracies of 54% and 75% (depending on the query scheme)\nwere achieved for collection C224a. Using tf-weighting,\nour approach achieves a maximum of 65.34% accuracy\nwith a 5-NN classiﬁer. The authors of [10] report accu-\nracyvaluesofupto 77%usinga k-NNclassiﬁerandupto\n85%using aSupport Vector Machine (SVM).\nAs for the different term sets used for indexing, using all\ntermsinthecorpusof Twitterposts(termlist allterms)\nyields\nthebestclassiﬁcationresults,butiscomputationally\nmost complex. Using artist namesfor indexing does\nnot\nsigniﬁcantly reduce classiﬁcation accuracy, while re-\nmarkably decrease space and time complexity. The good\nperformance of the artist namesset can be explained\nby\nmanyTwitterpostscontaininglistsofcurrentlylistened\nor favored artists. Such data therefore reveals information\non personal playlists.\nTo investigate which genres tend to be confused with\nwhich others, Figures 2 and 3 show confusion matrices of\nthe two best performing approaches. Using allterms\n(Figure\n2), “Folk” artists are often confused with “Coun-\ntry” artists, “Alternative Rock/Indie” performers are fre-\nquently predicted to make “Metal” music, and “Rock ’n’\nRoll”isoftenpredictedforartistsperforming“RnB/Soul”.\nUsinglast.fm toptags collection (Figure 3),the\nmostfrequentconfusionsare“Electronic”artistspredicted\nas“Rap”artistsand“RnB/Soul”artistsmistakenfor“Rock\n’n’ Roll” artists.\nWhile some confusions are easy to explain, for example,\n“Country” and “Folk” music is pretty close and in some\ntaxonomies even considered one genre, others are likely\nonlytheresultofusers’preferencerelationsinsteadofsim-\nilarity relations. For example, the co-occurrence of two\nartists(onefromgenre“Electronic”,theotherfrom“Rap”)\nin a user’s post may not necessarily indicate that these\nartistsaresimilar,butthattheyaresimilarlylikedorplayed\ntogether by the user.\n449\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)0 10 20 30 40 50 60 7000.10.20.30.40.50.60.70.8k−NN classification experiments on C224a\nvalue of kaccuracy\n  1: TF,allterms\n2: TF,artist_names\n3: TF,dictionary\n4: TF,last.fm_toptags_overall\n5: TF,last.fm_toptags_collection\n6: TF,last.fm_toptags_topartists\n7: TFIDF,allterms\n8: TFIDF,artist_names\n9: TFIDF,dictionary\n10: TFIDF,last.fm_toptags_overall\n11: TFIDF,last.fm_toptags_collection\n12: TFIDF,last.fm_toptags_topartists\n3\n4\n5\n6\n17\n2810\n911 12\nFigure 1. Results of the genre-classiﬁcation-experiments for different kv alues of the k-NN classiﬁer, using C224a.\npredicted genrescorrect genresconfusions for C224a\n65.6\n2.1\n8.3\n3.1\n8.3\n6.3\n3.1\n3.193.8\n8.3\n6.393.83.1\n2.1\n68.8\n25\n6.3\n0.7\n5.2\n6.33.1\n64.6\n3.1\n6.3\n3.115.6\n24\n0.7\n6.3\n5.2\n2.1\n3.118.8\n6.3\n3.1\n84.4\n5.9\n6.36.3\n6.3\n9.4\n86.5\n2.83.1\n6.3\n6.3\n2.1\n73.612.5\n0.7\n71.9\n3.13.1\n6.3\n2.1\n5.9\n3.1\n93.8\n12.5\n6.39.4\n58.32.1\n3.1\n3.1\n0.7\n6.3\n61.5\n3.13.1\n2.1\n9.4\n2.1\n14.6\n0.7\n3.1\n6.3\n9.4\n21.9\n75\nARBluClaCouEleFolkHMJazzPopPunkRapRegRnBRnRAlternative Rock/Indie\nBlues\nClassical\nCountry\nElectronica\nFolk\nHeavy Metal/Hard Rock\nJazz\nPop\nPunk\nRap/Hip−Hop\nReggae\nRnB/Soul\nRock ’n’ Roll\nFigure 2. Confusion matrix for the 9-NN classiﬁer on the\nC224acollection\nusing the termlist allterms.\n4.3\nLabeling\nTo assess the performance of Twitterposts for the task of\nlabeling artists, we use an artist a’s top-ranked terms (ac-\ncording to each term weighting measure), to predict la-\nbels for a. To this end, we index the posts using term\nlistlast.fm toptags overall and a list of tags ex-\ntracted\nfromlast.fmforseveralthousandstop-playedartists.\nIn total, 56,396 unique terms were obtained.\nFor evaluation we compare the top-ranked Nlabels from\nTwitter(accordingtothetermweightingmeasure)withthepredicted genrescorrect genresconfusions for C224a\n83.3\n11.5\n10.2\n9.4\n5.2\n6.3\n1.6\n2.1\n5.293.8\n0.8\n3.1\n1.6\n11.593.8\n6.3\n0.855.2\n8.3\n0.8\n0.8\n3.13.1\n44.5\n2.16.3\n37.5\n6.3\n0.8\n12.5\n0.8\n5.23.9\n3.1\n93.8\n14.3\n6.3\n4.78.3\n90.6\n0.88.3\n3.1\n2.1\n3.9\n6.3\n56.3\n6.3\n3.9\n9.4\n3.18.3\n3.1\n6.3\n75\n1.66.3\n6.3\n22.7\n14.3\n93.8\n7.8\n8.3753.1\n0.8\n3.1\n0.8\n0.8\n55.2\n6.33.1\n9.4\n7\n20.8\n3.1\n3.9\n1.6\n25\n65.6\nARBluClaCouEleFolkHMJazzPopPunkRapRegRnBRnRAlternative Rock/Indie\nBlues\nClassical\nCountry\nElectronica\nFolk\nHeavy Metal/Hard Rock\nJazz\nPop\nPunk\nRap/Hip−Hop\nReggae\nRnB/Soul\nRock ’n’ Roll\nFigure 3. Confusion matrix for the 8-NN clas-\nsiﬁer on the C224acollection using the term list\nlast.fm toptags collection.\ntop-rank\nedNtags from last.fm. To this end, we calculate\nanoverlap score between the two term sets. Aggregat-\ning this score over all artists in the collection reveals the\naverage percentage of overlapping terms, considering dif-\nferent quantities Nof top-ranked terms. More formally,\ntheoverlap@top-N is calculated according to Formula 3,\nwhere Adenotestheartistset, #artists Nisthenumberof\nartistswithatleast Ntermsassigned,and overlap tw,fm,a,N\nis the number of terms in Twitter’s set of top-N terms for\n450\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)0 50 100 150 200 250 300 35000.0050.010.0150.020.0250.03\nTags / NOverlap / Number of Artists (x100,000)Autotagging Experiments with C3000a\n  \noverlap with last.fm tags (TF)\noverlap with last.fm tags (TFIDF)\nnumber of artists with at least N tags\nFigure 4. Results of the labeling experiments using C3000aand the set of 56,396 tags.\n0 5 10 15 20 25 3000.050.10.150.20.250.30.35\nTags / NOverlap / Number of Artists (x10,000)Autotagging Experiments with C3000a\n  \noverlap with last.fm tags (TF)\noverlap with last.fm tags (TFIDF)\nnumber of artists with at least N tags\nFigure 5. Results of the labeling experiments using C3000aand the term set last.fm toptags overall.\nartistathat\nalso occur in last.fm’s set of top-ranked tags\nfora.\noverlap @top −N=/summationdisplay\na∈Aoverlap tw,fm,a,N\nN\n#artists N(3)\n4.3.1\nResults\nFigures4and5showtheaggregatedoverlapscoresforcol-\nlectionC3000aat different levels of top-N terms/tags us-\ningthetermsetof56,396tagsandthetermset last.fm\ntoptags overall, respectively. The dash-dotted line\nre\nvealsthenumberofartistswithatleast Ntermsassigned.\nThesolidlinegivestheoverlapscoreusing tf·idfforterm\nweighting, whereas the dotted line gives the score using\ntf-weighting.\nThe low maximum overlap of 2.36%for the 56,396-tag-\nset (tf ·idf) is likely caused by a large amount of noise in\nthelast.fmtags. Using last.fm toptags overall,\nthe\nmaximum overlap scores are 13.53%(tf) and11.67%\n(tf·idf). Takingintoaccountthatthisisaverychallenging\ntask (an overlap of 100%for a certain level of Nwould\nmean that the top-N terms according to the Twitterposts\ncorrespond exactly to the top-N tags from last.fmfor allartists), these results are better than the sole numbers sug-\ngest.\nThecorrespondingmaximumoverlapscoresforcollection\nC224ausing the 56,396-tag-set amount to 6.68%(tf·idf)\nand5.39%(tf). Termset last.fm toptags overall\nyields\nmaximum overlap scores of 16.36%(tf·idf) and\n15.22%(tf).\n5. CONCLUSIONS AND OUTLOOK\nWe have shown that Twitterposts provide a valuable data\nsource for music information research. In particular for\nthetaskofsimilaritymeasurement ontheartistlevel, clas-\nsiﬁcation results resemble the ones achieved with other\ntext-based approaches using community or cultural data\nsources, e.g., [10,25], on the same artist set. For the task\nof automated labeling, in contrast, only weak to medium\noverlaps between Twitterposts and last.fmtags could be\ndetermined.\nAs part of future work, we would like to analyze the\nlocalization capabilities of the TwitterAPI. Provided suf-\nﬁcient accuracy, additional geographic data could be used,\nfor example, to spot the most popular artists within a re-\ngion or country. Successively, such information may be\nused to reveal the spreading of listening trends around the\n451\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)world. Usinggeolocationinformationmayalsohelpbuild-\ning\ncountry-speciﬁc or culture-speciﬁc models of music\nsimilarity.\n6. ACKNOWLEDGMENTS\nThis research is supported by the Austrian Fonds zur F ¨or-\nderung der Wissenschaftlichen Forschung (FWF) under\nproject numbers L511-N15 and Z159.\n7. REFERENCES\n[1] http://www.allmusic.com (access: January 2010).\n[2] Claudio Baccigalupo, Enric Plaza, and Justin Donald-\nson. Uncovering Afﬁnity of Artists to Multiple Genres\nfromSocial Behaviour Data. In Proc. of ISMIR, 2008.\n[3] Stephan Baumann and Oliver Hummel. Using Cul-\ntural Metadata for Artist Recommendation. In Proc. of\nWEDELMUSIC, 2003.\n[4] William W. Cohen and Wei Fan. Web-Collaborative\nFiltering: Recommending Music by Crawling The\nWeb.WWW9 / Computer Networks, 33(1–6):685–698,\n2000.\n[5] Darcy DiNucci. Fragmented Future. Design & New\nMedia, 53(4), 1999.\n[6] Douglas Eck, Thierry Bertin-Mahieux, and Paul\nLamere. Autotagging Music Using Supervised Ma-\nchine Learning. In Proc. of ISMIR, 2007.\n[7] Daniel P.W. Ellis, Brian Whitman, Adam Berenzweig,\nand Steve Lawrence. The Quest For Ground Truth in\nMusical ArtistSimilarity. In Proc. of ISMIR, 2002.\n[8] Gijs Geleijnse and Jan Korst. Web-based Artist Cate-\ngorization. In Proc. of ISMIR, 2006.\n[9] Andy Kazeniac. Social Networks: Face-\nbook Takes Over Top Spot, Twitter Climbs.\nhttp://blog.compete.com/2009/02/09/facebook-\nmyspace-twitter-social-network (access: March 2010).\n[10] Peter Knees, Elias Pampalk, and Gerhard Widmer.\nArtist Classiﬁcation with Web-based Data. In Proc. of\nISMIR, 2004.\n[11] Paul Lamere. Social Tagging and Music Information\nRetrieval. JournalofNewMusicResearch: FromGen-\nrestoTags–MusicInformationRetrievalintheAgeof\nSocial Tagging, 37(2):101–114, 2008.\n[12] http://last.fm(access: March 2010).\n[13] http://last.fm/api(access: March 2010).\n[14] E.Law,L.vonAhn,R.Dannenberg,andM.Crawford.\nTagatune: A Game for Music and Sound Annotation.\nInProc. of ISMIR, Vienna, Austria, September 2007.\n[15] Mark Levy and Mark Sandler. A semantic space for\nmusic derived from social tags. In Proc. of ISMIR, Vi-\nenna, Austria, September 2007.\n[16] http://lucene.apache.org (access: February 2010).[17] MichaelI.MandelandDanielP.W.Ellis.AWeb-based\nGame for Collecting Music Metadata. In Proc. of IS-\nMIR, Vienna, Austria, September 2007.\n[18] http://music.strands.com (access: November 2009).\n[19] Tim O’Reilly. What Is Web 2.0 – Design Pat-\nterns and Business Models for the Next Generation\nof Software. http://oreilly.com/web2/archive/what-is-\nweb-20.html (access: March 2010).\n[20] Franc ¸ois Pachet and Daniel Cazaly. A Taxonomy of\nMusical Genre. In Proc. of RIAO, 2000.\n[21] Elias Pampalk, Arthur Flexer, and Gerhard Widmer.\nHierarchical Organization and Description of Music\nCollectionsattheArtistLevel.In Proc.ofECDL,2005.\n[22] Matei Ripeanu. Peer-to-Peer Architecture Case Study:\nGnutellaNetwork.In Proc.ofIEEEPeer-to-PeerCom-\nputing, 2001.\n[23] GerardSalton.TheUseofCitationsasanAidtoAuto-\nmatic Content Analysis. Technical Report ISR-2, Sec-\ntionIII,HarvardComputationLaboratory,Cambridge,\nMA, USA, 1962.\n[24] Markus Schedl and Peter Knees. Investigating Dif-\nferent Term Weighting Functions for Browsing\nArtist-Related Web Pages by Means of Term Co-\nOccurrences. In Proc. of LSAS, 2008.\n[25] Markus Schedl, Peter Knees, and Gerhard Widmer. A\nWeb-Based Approach to Assessing Artist Similarity\nusing Co-Occurrences. In Proc. of CBMI, 2005.\n[26] Markus Schedl, Tim Pohle, Peter Knees, and Gerhard\nWidmer. Assigning and Visualizing Music Genres by\nWeb-based Co-Occurrence Analysis. In Proc. of IS-\nMIR, 2006.\n[27] Yuval Shavitt and Udi Weinsberg. Songs Clustering\nUsing Peer-to-Peer Co-occurrences. In Proc. of Ad-\nMIRe (IEEE ISM), 2009.\n[28] http://www.sysomos.com/insidetwitter\n(access: March 2010).\n[29] D.Turnbull,R.Liu,L.Barrington,andG.Lanckriet.A\nGame-based Approach for Collecting Semantic Anno-\ntations of Music. In Proc. of ISMIR, Vienna, Austria,\nSeptember 2007.\n[30] http://twitter.com (access: February 2010).\n[31] http://apiwiki.twitter.com/Twitter-API-Documentation\n(access: February 2010).\n[32] Brian Whitman and Steve Lawrence. Inferring De-\nscriptions and Similarity for Music from Community\nMetadata. In Proc. of ICMC, 2002.\n[33] Justin Zobel and Alistair Moffat. Exploring the Simi-\nlarity Space. ACM SIGIR Forum, 32(1):18–34, 1998.\n[34] JustinZobelandAlistairMoffat.InvertedFilesforText\nSearch Engines. ACM Computing Surveys, 38:1–56,\n2006.\n452\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "What&apos;s Hot? Estimating Country-specific Artist Popularity.",
        "author": [
            "Markus Schedl",
            "Tim Pohle",
            "Noam Koenigstein",
            "Peter Knees"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1415870",
        "url": "https://doi.org/10.5281/zenodo.1415870",
        "ee": "https://zenodo.org/records/1415870/files/SchedlPKK10.pdf",
        "abstract": "Predicting artists that are popular in certain regions of the world is a well desired task, especially for the music indus- try. Also the cosmopolitan and cultural-aware music afi- cionado is likely be interested in which music is currently “hot” in other parts of the world. We therefore propose four approaches to determine artist popularity rankings on the country-level. To this end, we mine the following data sources: page counts from Web search engines, user posts on Twitter, shared folders on the Gnutella file sharing net- work, and playcount data from last.fm. We propose meth- ods to derive artist rankings based on these four sources and perform cross-comparison of the resulting rankings via overlap scores. We further elaborate on the advantages and disadvantages of all approaches as they yield interestingly diverse results.",
        "zenodo_id": 1415870,
        "dblp_key": "conf/ismir/SchedlPKK10",
        "keywords": [
            "predicting",
            "artists",
            "popularity",
            "regions",
            "music",
            "industry",
            "cosmopolitan",
            "cultural-aware",
            "music",
            "enthusiast"
        ],
        "content": "WHAT’S HOT ?\nESTIMATING COUNTRY-SPECIFIC ARTIST POPULARITY\nMarkus Schedl1, Tim Pohle1, Noam Koenigstein2, Peter Knees1\n1Department of Computational Perception\nJohannes Kepler University, Linz, Austria\n2Faculty of Engineering\nTel Aviv University, Tel Aviv, Israel\nABSTRACT\nPredicting artists that are popular in certain regions of the\nworld is a well desired task, especially for the music indus-\ntry. Also the cosmopolitan and cultural-aware music aﬁ-\ncionado is likely be interested in which music is currently\n“hot” in other parts of the world. We therefore propose\nfour approaches to determine artist popularity rankings on\nthe country-level. To this end, we mine the following data\nsources: page counts from Web search engines, user posts\non Twitter, shared folders on the Gnutella ﬁle sharing net-\nwork, and playcount data from last.fm. We propose meth-\nods to derive artist rankings based on these four sources\nand perform cross-comparison of the resulting rankings via\noverlap scores. We further elaborate on the advantages and\ndisadvantages of all approaches as they yield interestingly\ndiverse results.\n1. INTRODUCTION\nTo determine popular artists for a certain country or cul-\ntural region of the world, one can obviously look into pub-\nlicly available music charts, such as the “Billboard Hot\n100”, released weekly for the United States of America\nby the Billboard Magazine [6]. However, this straightfor-\nward strategy is hardly feasibly when we aim at broaden\nthe scope to the whole world. The reasons are manifold.\nFirst, not all countries do release music charts for vari-\nous reasons. Causes may be, for example, a lack of capa-\nbility to determine music sales or an underdevelopment of\nmusic distribution at large. Even if data is available, it is\noften not publicly accessible, and even if so, not always in\nan easy-to-use format, e.g., via a Web service.\nSecond, even if charts are available for a speciﬁc country,\nthey often cover only certain ways of music distribution.\nCommonly they are strongly biased towards sales ﬁgures\nof music albums. In some countries, however, they also\ninclude digital music sales via online stores. This inho-\nmogeneity between countries, i.e., the in- or exclusion of\ncertain distribution channels, make such data hardly com-\nparable between different countries of the world. Another\naspect to be considered here are possible heavy distortions\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2010 International Society for Music Information Retrieval.caused by (illegal) music sharing channels, since legisla-\ntion in this area varies severely between countries. In fact,\nthe majority of today’s music distribution is affected via\nﬁle sharing networks [2]. Thus, traditional charts, such as\nthe “Billboard Hot 100”, are becoming less and less rele-\nvant.\nThird, if the aim is to come up with a list of the most pop-\nular artists ever, countries lacking solid historical records\nconstitute an obvious problem.\nSummarizing these challenges, we conclude that ana-\nlyzing which kind of music is popular in a speciﬁc coun-\ntry or cultural region necessitates taking a deeper look into\nvarious distribution channels and data sources. In this pa-\nper, we therefore present four different approaches to esti-\nmate artist popularity rankings on the country-level, each\nof which makes use of a different data source. The ﬁrst\none is based on page count estimates of Web search en-\ngines, the second approach analyzes Twitter posts, the third\none derives information from meta-data of users’ shared\nfolders in a Peer-to-Peer network, and the fourth one uses\nplaycount data from last.fm.\nIn the remainder of this paper we review related lit-\nerature (Section 2), present four approaches to determine\nartist popularity on the country-level (Section 3), elabo-\nrate on the conducted evaluation experiments and discuss\ntheir results (Section 4), and ﬁnally draw conclusions (Sec-\ntion 5).\n2. RELATED WORK\nRelated work falls into two categories: literature that par-\nticularly tackles the task of chart prediction, and work that\nrelates to the four approaches we propose for this task.\nTargeting the problem of predicting music charts, Koe-\nnigstein and Shavitt [26] present an approach to predict the\ncharts based on search queries issued within the Peer-to-\nPeer (P2P) ﬁle sharing network Gnutella [35]. The authors\nshow that a song’s popularity in the P2P network highly\ncorrelates with its ranking in the Billboard charts. The au-\nthors’ approach can further predict upcoming charts with\nhigh accuracy. However, for their analysis Koenigstein and\nShavitt only consider the United States.\nPachet and Roy [33] try to predict the popularity of a song\nbased on audio features and a variety of manual labels. The\nauthors’ conclusion is, however, that even state-of-the-art\nmachine learning techniques fail to learn factors that de-\ntermine a song’s popularity, irrespective of whether they\nare trained on signal-based features or on high-level hu-\nman annotations.\n117\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)In [38] Schedl et al. propose several heuristics to determine\nwhich artists are popular within a certain genre. They re-\nlate occurrence counts of artist names on Web pages via\nan approach similar to Google’s backlink and forward link\nanalysis [34]. The authors show that downranking factors\nfor artist names equaling common speech terms improve\naccuracy when comparing the resulting rankings against a\nground truth popularity categorization extracted from all-\nmusic.com [3].\nIn [22] Grace et al. derive popularity rankings from user\ncomments in the social network MySpace [32]. To this end,\nthe authors apply various annotators to crawled MySpace\nartist pages in order to spot, for example, names of artists,\nalbums, and tracks, sentiments, and spam. Subsequently,\na data hypercube (OLAP cube) is used to represent struc-\ntured and unstructured data, and to project the data to a\npopularity dimension. A user study showed that the list\ngenerated by this procedure was on average preferred to\nthe Billboard charts.\nPrevious work that relates to the four approaches pro-\nposed here comprise the following.\nOur heuristic that uses page counts returned by search en-\ngines builds upon work from [20, 39], where Web co-oc-\ncurrences of artist names and terms speciﬁc to the mu-\nsic domain are used to categorize artists, a process also\nknown as “autotagging” [13]. In [37] Schedl et al. propose\na similar approach to estimate artist similarity. The authors\nsuggest a simple probabilistic model that deﬁnes similarity\nbetween two artists aandbas the conditional probability\nofato be mentioned on a Web page known to relate to b\nand vice versa. Accuracies of up to 85% were reported for\ngenre classiﬁcation.\nTo the best of our knowledge, Twitter [41] has not been\nscientiﬁcally investigated for music information extraction\nand retrieval yet. Although there do exist certain commer-\ncial services, such as BigChampagne [7] and Band Met-\nrics [9], which seem to incorporate microblogging data\ninto their artist and song rankings, no details on their ap-\nproach are available. Furthermore, they strongly focus their\nservices on the USA. A general study on the use of Twit-\ntercan be found in [24]. Java et al. report that Twitter is\nmost popular in North America, Europe, and Asia (Japan),\nand that same language is an important factor for cross-\nconnections (“followers” and “friends”) over continents.\nThe authors also distill certain categories of user intentions\nto microblog. Employing the HITS algorithm [25] on the\nnetwork constructed by “friend”-relations, Java et al. de-\nrive user intentions from structural properties. They iden-\ntiﬁed the following categories: information sharing, infor-\nmation seeking, and friendship-wise relationships. Ana-\nlyzing the content of Twitter posts, the authors distill the\nfollowing intentions: daily chatter, conversations, sharing\ninformation/URLs, and reporting news.\nUsing Peer-to-Peer networks as data source for music in-\nformation retrieval, [8, 14, 31, 43] rely on data extracted\nfrom OpenNap to derive music similarity information. All\nof these papers seem to build upon the same data set, which\ncomprises of metadata on shared content (approximately\n3,000 shared music collections were analyzed). Logan et\nal. [31] compare similarities deﬁned by artist co-occur-\nrences in shared folders, by expert opinions from allmu-\nsic.com, by playlist co-occurrences from Art of the Mix [4],\nby data gathered from a Web survey, and by MFCC fea-\ntures [5]. To this end, they calculate a “ranking agree-\nment score”, i.e., the pairwise overlap between the Nmost\nsimilar artists according to each data source. The mainﬁndings are that the co-occurrence data from OpenNap\nand from Art of the Mix show a high degree of overlap,\nthe experts from allmusic.com and the participants of the\nWeb survey show a moderate agreement, and the signal-\nbased MFCC measure had a rather low agreement with the\nmusic context-based data sources. More recently, in [40]\nShavitt and Weinsberg mine the Gnutella ﬁle sharing net-\nwork to derive artist and song similarities. The authors\ngathered metadata of shared music ﬁles from about one\nmillion Gnutella users in November 2007, which yielded\ninformation on half a million songs. Analyzing the 2-mode\ngraph of users and songs revealed that most users share\nsimilar ﬁles. The authors further propose a method for\nartist recommendation based on the gathered data.\nTaking a closer look at the data source of music informa-\ntion systems, which corresponds to the fourth approach,\nnot only last.fm [28] provides popularity rankings via their\nAPI [29]. Echonest [15] offers a function to retrieve a rank-\ning based on the so-called “hotttness” of an artist [17]. This\nranking is based on editorial, social, and mainstream as-\npects [16]. However, this Web service does not provide\ncountry-speciﬁc information.\n3. DETERMINING ARTIST POPULARITY ON\nTHE COUNTRY LEVEL\nWe propose the following four heuristics to determine an\nartist’s popularity in a certain country, and consequently\ncreate an artist popularity ranking. To this end, we ﬁrst\nretrieve a list of 240 countries from last.fm [30], based on\nwhich the following approaches operate.\n3.1 Search Engine Page Counts\nThis approach makes use of a search engine’s number of\nindexed Web pages for a given query, a count usually re-\nferred to as page count. These page counts are, however,\nonly rough estimates of the real number of available Web\npages related to the query. Nevertheless, for the purpose\nof classifying music artists into genres [20, 37, 39] and for\nclassifying general instances according to a given ontol-\nogy as well as for learning sub- and superconcept rela-\ntions [11, 12], this method yielded respectable results.\nFor the paper at hand, we queried the search engines\nGoogle [21] and Exalead [18], using their API or issu-\ning HTTP requests. The page count values returned for\nallhartist, countryi tuples were retrieved. To avoid ex-\ncessive bandwidth consumption, we restrict the number of\nsearch results to be transmitted to the smallest value (this\nis usually one result). Since we are only interested in the\npage count estimates, this restriction effectively reduces\nnetwork trafﬁc without effecting the results.\nThe two main challenges of this approach are directing the\nsearch towards pages related to the music domain and al-\nleviating the distortions caused by artist names that equal\ncommon speech words. We address these issues by using\nqueries of the form\n\"artist name\" \"country name\" music\nand weighting the resulting page count values with a fac-\ntor resembling the inverse document frequency (idf) [46].\nThe ﬁnal ranking score is thus calculated according to For-\nmula 1, where pcc;ais the page count value returned for the\ncountry-speciﬁc query for artist aand country c,Nis the\ntotal number of countries for which data is available, and\nd fais the number of countries in which artist ais known\n118\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)according to the data source (i.e., the number of countries\nwithpcc;a>0).\npopularity pcc;a=pcc;a\u0001log2\u0012\n1 +N\nd fa\u0013\n(1)\n3.2 Twitter Posts\nMany Twitter posts reveal information about what people\nare doing or thinking right now. We are interested in posts\ncontaining information about which music is currently be-\ning played by users in a given country. To accomplish this,\nwe retrieve posts using the Twitter Search API [42]. The\nposts are then narrowed in two ways. First, we only search\nfor posts containing the hashtag #nowplaying. This restric-\ntion is directly supported by the Twitter API. As a second\nrestriction, the search is narrowed to a speciﬁc country.\nNot being aware of a more direct implementation for the\nsecond restriction, we search only for posts whose users\nare located within a certain radius around a GPS coordi-\nnate. More speciﬁcally, for a given country, we determine\nthe coordinates of larger cities (with more than 100,000\ninhabitants) and search for posts originating from a circle\nof 100 kilometers around the respective coordinates. The\nnames of the cities are taken from Wikipedia, e.g., [45], and\nthe coordinates are determined by using Freebase [19]. For\neach city location for which geolocation data is resolved\nsuccessfully, all Twitter posts available through the Twitter\nAPI are retrieved, which yields a maximum of about 1,500\nposts per city location.\nOne of the advantages of using this kind of data is cer-\ntainly its recentness. Thus, the retrieved data may contain\nartists that do not appear in our list of most popular artists\n(cf. Section 4.1). A ﬁrst look at the format of the texts\nreveals that automatic tokenization seems not easily to ac-\ncomplish due to the large variation of wording and creative\nmethods to use the available number of characters. We\ntherefore opt to scan the retrieved texts for the artists con-\ntained in the artist list, and we count the number of their ap-\npearances for a given country c. This count equals the term\nfrequency (tf c;a) ofain an aggregated document compris-\ning all posts gathered for cities in country c. Formula 2\ngives the ranking score. The rightward term again repre-\nsents an idf-factor that downranks artists that are popular\neverywhere, and thus not speciﬁc to country c.Nis the\ntotal number of countries, and d fais the number of aggre-\ngated country documents in which artist aoccur.\npopularity twic;a=tfc;a\u0001log2\u0012\n1 +N\nd fa\u0013\n(2)\n3.3 Shared Folders in a P2P Network\nCollecting shared folder data from Gnutella users is a two-\nstaged-process. First, a crawler needs to discover the cur-\nrent network topology (which is very dynamic). Subse-\nquently, a browser queries the active users for their shared\nfolders data. The crawler treats the network as a graph, and\nperforms a breadth-ﬁrst exploration, where newly discov-\nered nodes are enqueued in a list of un-crawled addresses.\nThe crawler provides a list of active IP addresses to the\nbrowser, which sends Gnutella “Query” messages [1] to\nthe clients. The clients reply with “QueryHit” messages,\nthat lists their shared folder content. These messages are\nthe basis for our P2P data set.The system described above is a different system than\nthe one used by Koenigstein and Shavitt in [26], which col-\nlected Gnutella search queries for song ranking. One ad-\nvantage of a shared folder data set over queries is the avail-\nability of ID3 tags and hash keys, which simpliﬁes the pro-\ncess of associating the digital content with a musical artist.\nHowever, when singles ranking is considered (as in [26]),\nqueries tend to better reﬂect the changing popularity trends\nof pop songs over short time intervals. In this study, we as-\nsociate artists with digital content by matching the artist\nnames against the content of the ID3 tags. Occasionally,\nthe content in ID3 tags is missing or misspelled. We there-\nfore, match the artists names against the ﬁle names as well.\nIn order to build popularity charts for speciﬁc countries,\none needs to resolve the geographical location of the users.\nThe geo-identiﬁcation is based on the IP addresses. First,\nwe generate a list of all unique IP addresses in the data\nset (typically over a million). We resolve the geography\nof IP addresses using the commercial IP2Location [23]\ndatabase. Each IP address is bounded with its country\ncode, city name, and latitude-longitude values. This accu-\nrate geographical information pin points artists’ fans and\nenables tracking spatial diffusion of artists popularity [27].\nAfter the digital ﬁles are associated with artists names\nand geography, building popularity charts is straightfor-\nward. For each country, we aggregate the total number of\ndigital content that is associated with each artist. Ranking\nis then performed according to frequency.\n3.4 Last.fm Playcounts\nWe further estimate country-speciﬁc artist popularity based\non the community of last.fm users. Despite the issues of\nhacking and vandalism as well as the community bias [36],\nwhich are inherent to collaborative music information sys-\ntems, the playcounts of last.fm users can be expected to\nreﬂect to a certain extent which music is currently popu-\nlar. We therefore gathered the top 400 listeners of each\ncountry at the end of 2009. We subsequently extracted the\ntop-played artists for each of the resulting top-listeners-\nsets.1Aggregating the playcounts for each artist over a\ncountry’s top listeners ﬁnally yielded a popularity ranking\nfor the country under consideration.\n4. EVALUATION\n4.1 Test Set\nWe used last.fm’s Web API [29] to gather the most popular\nartists for each country of the world, as of November 2009.\nWe then aggregated this data into a single list of 201,135\nunique artist names.\n4.2 Experiments\nAs we aim at assessing the pros and cons of the various\napproaches, without yet having an established ground truth\nfor this kind of experiments, we choose to perform a pair-\nwise comparison of the approaches. Each approach pro-\nduces a ranked list of artists for the various countries. Ex-\npecting that the absolute numbers obtained by the various\napproaches are not immediately comparable, we compare\nthe produced artist popularity rankings of two approaches\n1In the meantime, last.fm has extended its API with a\nGeo.getTopArtists function, which can be used to directly\nretrieve the top-played artists among a certain country’s users. Quick\nempirical comparisons showed that the implementation behind this\nfunction seems to resemble our approach.\n119\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)AjandAk. This comparison is done separately for each\ncountry c. In the next subsections, we describe the applied\ndata preprocessing steps and the used evaluation measure\nin detail.\n4.2.1 Preprocessing\nWe start our analysis by processing the artist names in\nthe artist popularity list for country cof each approach\nin a basic way (e.g., each artist name is represented in\nlower case, repeated whitespace characters are removed,\nand UTF-8-encoded characters are transformed to canoni-\ncal ASCII representations).\nInstead of using raw artist counts directly, we normalize\nthem, attempting to avoid dominance of common-speech\nwords, or globally popular artists whose popularity is not\nhighly country speciﬁc. For each artist, the number of\ncountries this artist appears in is counted. Each country-\nspeciﬁc artist count acc;ais then normalized as indicated\nin Equation 1.\nArtist names appearing in the two lists (given by the pair\nof approaches under investigation) are matched against each\nother, and only artists appearing in both lists are kept. Based\non this data, we calculate the overlap between the rankings\nobtained with the two prediction approaches, as described\nnext.\n4.2.2 Evaluation Measures\nThe top-n rank overlap for country cbetween approaches\nAjandAkis calculated as\nroc;Aj;Ak;n=1\nn\u0001\f\ffajmax\u0000\nrAj;c;a; rAk;c;a\u0001\n\u0014ng\f\f(3)\nwhere rAj;c;adenotes the ranking of artist ain country\ncaccording to approach Aj, only considering the artists\nfor which both approaches (A jandAk) yield a ranking\nscore. In other words, the top-n rank overlap is the fraction\nof artists appearing within the top nranked artists in both\napproaches. For example, if one artist is within the top-2\nranked artists of both approaches, the top-2 rank overlap\nis0:5. Obviously, ncan take values up to the number of\nartists nmax;c for which both approaches deliver rank data\nfor country c, and the top-n max;c rank overlap is always 1.\nTo obtain an overall measure for two approaches and a\ngiven country, we deﬁne the country-wise rank overlap as\ncroc;Aj;Ak=1\nnmax;cnmax;cX\nn=1roc;Aj;Ak;n (4)\nwhich has a trivial (random) baseline of about 0:5and\na maximum value of 1:0when both rankings are identical.\nThe country-wise rank overlaps are further combined to\nobtain one overall scalar value for approaches AjandAk.\nTo account for the different quantity of available informa-\ntion, we weight the overlap score of each country with the\nnumber of artists for which information is available. We\ndeﬁne the overall overlap measure between approaches Aj\nandAkas\nov (Aj; Ak) =P\nc2Cnmax;c\u0001croc;Aj;Ak\nP\nc2Cnmax;c(5)\nThe measure ovalso has a trivial baseline of about 0:5\nand a maximum value of 1:0.\nFigure 1. Top 8countries for pcgoogle vsp2p.\nTo give an illustrative example, Figure 1 shows for the\ncomparison of approach pcgoogle andp2pthe8countries\nwith highest rovalue, as a chart from 1::nmax;c .\n4.3 Results and Discussion\nEach approach offers at least a slightly different view on\nreality since the data sources are of distinct nature. There\nis also no such thing as a “ground truth” for this task, as\neach data source (even “Billboard”-style charts) is biased,\nas elaborated below. Nevertheless, we would like to point\nout certain interesting observations.\nLooking at Figure 2, the highest overlap score of 0:67 is\nfound between Google page counts andP2P. One reason\nmay be that the two sources have broadest coverage. An-\nother explanation may be the time dependency. Twitter\nandlast.fm are much more time dependent, whereas P2P\nshared folders andamounts of Web pages change much\nslower. In fact, the content of the data sources behind P2P\nnetworks and Web search engines, i.e., users’ music col-\nlections and Web pages, respectively, is accumulated over\nyears. Microblogging posts and last.fm data, in contrast,\nchange much faster and are therefore more likely to reﬂect\ntrends.\nSecond, the page counts approach using Google and the\nsame approach using Exalead do not produce similar re-\nsults, as we would have expected. In fact, the rankings\nreveal a non-signiﬁcant overlap of 0:51. A possible expla-\nnation is that the two search engine providers may use very\ndifferent page count estimation techniques.\nExalead shows the lowest overlap with other sources. Its\nhighest overlap is realized, not surprisingly, with Google\nand with P2P, but it remains slightly above the baseline\n(0:53). An explanation for Exalead ’s low overlap score\nbecomes apparent when looking at Figure 3. Exalead has\nby far the highest number of matching artists, which may\ninduce a high noisiness.\nIn terms of country coverage (cf. Figure 3), the last.fm\nand the page counts approaches offer data for nearly ev-\nery country in the world.\nTo account for the different nature and scope of the pro-\nposed approaches (and underlying data sources), we com-\npare them according to several aspects in Table 1, elaborat-\ning on speciﬁc advantages and disadvantages. One issue is\nthat certain approaches are prone to a speciﬁc bias. For\nexample, the average last.fm user does not represent the\naverage music listener of a country, i.e., last.fm data is dis-\ntorted by a “community bias”. The same is true for Twitter,\nwhich is biased towards artists with very active fans. On\nthe other hand, some very popular artists may have fans\n120\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Figure 2. Overlap ovbetween each pair of approaches.\nFigure 3. Number of countries with non-empty overlap.\nthat twitter to a much lower degree. This issue becomes\nespecially apparent when thinking of live artists vs. dead\nones: The live ones keep making new headlines, and prob-\nably also have many more active fans, while the dead ones\nhave an inherent problem with this. Traditional charts are\nbiased towards the data the music industry uses to derive\nthem, usually record sales ﬁgures.\nAnother aspect according to which the approaches differ\nconsiderably is the availability of data. While page count\nestimates are available for all countries of the world, the\nP2P andTwitter approaches suffer from a very unbalanced\ncoverage, strongly depending on the country under con-\nsideration. Also traditional music charts vary strongly be-\ntween countries and continents with respect to availability.\nAccording to [44], only one country in Africa publishes\nofﬁcial music charts, while this number amounts to 19 for\nEurope.\nA big advantage of traditional charts is their virtual im-\nmunity against noise. Page count estimates, in contrast,\nare easily distorted by ambiguous artist or country names.\nlast.fm data suffers from hacking and vandalism [10], as\nwell as from unintentional input of wrong information and\nmisspellings.\nIn the dimension of time dependence, the approaches can\nbe categorized into “current” and “accumulating”, depend-\ning on whether they reﬂect the instantaneous popularity, or\na general, all-time popularity in that they accumulate pop-\nularity levels over time.\nFigure 4. Average number of artists per country (n max;c ).\n5. CONCLUSIONS AND FUTURE WORK\nWe presented four approaches to determine country-speciﬁc\nartist popularity rankings based on different data sources\n(search engine’s page counts, Twitter posts, shared folders\nin the Gnutella network, and playcounts of last.fm users).\nIn the absence of a standardized ground truth, we performed\npairwise comparison of the approaches and elaborated on\nparticular advantages and disadvantages. Most approaches\nshowed only weak overlaps, probably due to the different\nnature of their data sources. We found, however, a con-\nsiderable overlap between Google page counts and P2P\ndata, which is probably explained by the similar time scope\nthe two data sources cover. As a general conclusion, we\ncan state that artist popularity can be derived from various,\nquite inhomogeneous data sources. The remarkably weak\noverlap between most of them indicates that the quest for\nartist popularity is a multifaceted and challenging task, in\nparticular in today’s era of multi-channel music distribu-\ntion. To derive one overall popularity measure, we will\nneed to combine the different sources.\nFuture work will hence foremost aim at elaborating hybrid\napproaches that account for the different quantity and qual-\nity of information output by the four heuristics. We will\nalso work on reﬁning our approaches to capture artist pop-\nularity within certain genres, e.g., by incorporating meth-\nods similar to [38]. We will further look at the various\nprocessing steps in more detail. Most of the current imple-\nmentations were created in an ad-hoc manner, and some of\nthe choices might degrade the performance. For example,\nbetter string comparison algorithms may improve results\nfor artists whose names may be spelled in various ways.\nAlternative ways of normalizing artist counts for the indi-\nvidual approaches are also likely to yield improvements.\n6. ACKNOWLEDGMENTS\nThis research is supported by the Austrian Fonds zur F ¨or-\nderung der Wissenschaftlichen Forschung (FWF) under proj-\nect numbers L511-N15 and Z159.\n7. REFERENCES\n[1] The Gnutella Protocol Speciﬁcation v0.41.\nhttp://www9.limewire.com/developer/\ngnutella protocol 0.4.pdf (access: March 2010).\n121\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Source/Aspect Bias Availability Noisiness Time Dependence\nPage Counts Web users comprehensive high accumulating\nTwitter community country-dependent medium current\nP2P community country-dependent low–medium accumulating\nLast.fm community high medium–high accumulating\nTraditional Charts music industry country-dependent low current\nTable 1. A comparison of different approaches according to various dimensions.\n[2] Digital Music Report 2009.\nhttp://www.ifpi.org/content/library/DMR2009.pdf\n(access: May 2010), January 2009.\n[3] http://www.allmusic.com (access: January 2010).\n[4] http://www.artofthemix.org (access: February 2008).\n[5] Jean-Julien Aucouturier, Franc ¸ois Pachet, and Mark\nSandler. ”The Way It Sounds”: Timbre Models for\nAnalysis and Retrieval of Music Signals. IEEE Trans-\nactions on Multimedia, 7(6):1028–1035, December\n2005.\n[6] http://en.wikipedia.org/wiki/Billboard Hot100\n(access: May 2009).\n[7] http://www.bigchampagne.com (access: May 2010).\n[8] Adam Berenzweig, Beth Logan, Daniel P.W. Ellis, and\nBrian Whitman. A Large-Scale Evaluation of Acoustic\nand Subjective Music Similarity Measures. In Proceed-\nings of ISMIR.\n[9] www.bandmetrics.com (access: May 2010).\n[10] `Oscar Celma and Paul Lamere. ISMIR 2007 Tutorial:\nMusic Recommendation.\n[11] Philipp Cimiano, Siegfried Handschuh, and Steffen\nStaab. Towards the Self-Annotating Web. In Proceed-\nings of ACM WWW, 2004.\n[12] Philipp Cimiano and Steffen Staab. Learning by\nGoogling. ACM SIGKDD Explorations Newsletter,\n6(2):24–33, 2004.\n[13] Douglas Eck, Thierry Bertin-Mahieux, and Paul\nLamere. Autotagging Music Using Supervised Ma-\nchine Learning. In Proceedings of ISMIR, 2007.\n[14] Daniel P.W. Ellis, Brian Whitman, Adam Berenzweig,\nand Steve Lawrence. The Quest For Ground Truth in\nMusical Artist Similarity. In Proceedings of ISMIR,\n2002.\n[15] http://echonest.com (access: March 2010).\n[16] http://developer.echonest.com/docs/method/\ngethotttnesss (access: March 2010).\n[17] http://developer.echonest.com/docs/method/\ngettophottt artists (access: March 2010).\n[18] http://www.exalead.com (access: February 2010).\n[19] http://www.freebase.com (access: March 2010).\n[20] Gijs Geleijnse and Jan Korst. Web-based Artist Cate-\ngorization. In Proceedings of ISMIR, 2006.\n[21] http://www.google.com (access: March 2010).\n[22] Julia Grace, Daniel Gruhl, Kevin Haas, Meenakshi\nNagarajan, Christine Robson, and Nachiketa Sahoo.\nArtist Ranking Through Analysis of On-line Commu-\nnity Comments. In Proceedings of ACM WWW, 2008.\n[23] http://www.ip2location.com (access: March 2010).\n[24] Akshay Java, Xiaodan Song, Tim Finin, and Belle\nTseng. Why We Twitter: Understanding Microblog-\nging Usage and Communities. In Proceedings of\nWebKDD/SNA-KDD, 2007.\n[25] Jon M. Kleinberg. Authoritative Sources in a Hyper-\nlinked Environment. Journal of the ACM, 46(5), 1999.\n[26] Noam Koenigstein and Yuval Shavitt. Song RankingBased on Piracy in Peer-to-Peer Networks. In Proceed-\nings of ISMIR, 2009.\n[27] Noam Koenigstein, Yuval Shavitt, and Tomer Tankel.\nSpotting Out Emerging Artists Using Geo-aware Anal-\nysis of P2P Query Strings. In Proceedings of the 14th\nACM SIGKDD International Conference on Knowl-\nedge Discovery and Data Mining, 2008.\n[28] http://last.fm (access: March 2010).\n[29] http://last.fm/api (access: March 2010).\n[30] http://www.last.fm/community/users\n(access: March 2010).\n[31] Beth Logan, Daniel P.W. Ellis, and Adam Berenzweig.\nToward Evaluation Techniques for Music Similarity. In\nProceedings of ACM SIGIR: Workshop on the Evalua-\ntion of Music Information Retrieval Systems, 2003.\n[32] http://www.myspace.com (access: November 2009).\n[33] Franc ¸ois Pachet and Pierre Roy. Hit Song Science is\nNot Yet a Science. In Proceedings of ISMIR, 2008.\n[34] Lawrence Page, Sergey Brin, Rajeev Motwani, and\nTerry Winograd. The PageRank Citation Ranking:\nBringing Order to the Web. In Proceedings of ASIS,\n1998.\n[35] Matei Ripeanu. Peer-to-Peer Architecture Case Study:\nGnutella Network. In Proceedings of IEEE Peer-to-\nPeer Computing, 2001.\n[36] Markus Schedl and Peter Knees. Context-based Music\nSimilarity Estimation. In Proceedings of LSAS, 2009.\n[37] Markus Schedl, Peter Knees, and Gerhard Widmer. A\nWeb-Based Approach to Assessing Artist Similarity\nusing Co-Occurrences. In Proceedings of CBMI, 2005.\n[38] Markus Schedl, Peter Knees, and Gerhard Widmer. In-\nvestigating Web-Based Approaches to Revealing Pro-\ntotypical Music Artists in Genre Taxonomies. In Pro-\nceedings of ICDIM, 2006.\n[39] Markus Schedl, Tim Pohle, Peter Knees, and Gerhard\nWidmer. Assigning and Visualizing Music Genres by\nWeb-based Co-Occurrence Analysis. In Proceedings of\nISMIR, 2006.\n[40] Yuval Shavitt and Udi Weinsberg. Songs Clustering\nUsing Peer-to-Peer Co-occurrences. In Proceedings of\nthe IEEE ISM: International Workshop on Advances\nin Music Information Research (AdMIRe), San Diego,\nCA, USA, 2009.\n[41] http://twitter.com (access: February 2010).\n[42] http://apiwiki.twitter.com/Twitter-API-Documentation\n(access: March 2010).\n[43] Brian Whitman and Steve Lawrence. Inferring De-\nscriptions and Similarity for Music from Community\nMetadata. In Proceedings of ICMC, 2002.\n[44] http://en.wikipedia.org/wiki/Music charts\n(access: March 2010).\n[45] http://en.wikipedia.org/wiki/List oftowns andcities\nwith 100,000 ormore inhabitants/country: A-B\n(access: March 2010).\n[46] Justin Zobel and Alistair Moffat. Exploring the Simi-\nlarity Space. ACM SIGIR Forum, 32(1):18–34, 1998.\n122\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "Prediction of Time-varying Musical Mood Distributions from Audio.",
        "author": [
            "Erik M. Schmidt",
            "Youngmoo E. Kim"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1416238",
        "url": "https://doi.org/10.5281/zenodo.1416238",
        "ee": "https://zenodo.org/records/1416238/files/SchmidtK10.pdf",
        "abstract": "The appeal of music lies in its ability to express emotions, and it is natural for us to organize music in terms of emo- tional associations. But the ambiguities of emotions make the determination of a single, unequivocal response label for the mood of a piece of music unrealistic. We address this lack of specificity by modeling human response labels to music in the arousal-valence (A-V) representation of af- fect as a stochastic distribution. Based upon our collected data, we present and evaluate methods using multiple sets of acoustic features to estimate these mood distributions parametrically using multivariate regression. Furthermore, since the emotional content of music often varies within a song, we explore the estimation of these A-V distributions in a time-varying context, demonstrating the ability of our system to track changes on a short-time basis.",
        "zenodo_id": 1416238,
        "dblp_key": "conf/ismir/SchmidtK10",
        "keywords": [
            "music",
            "expressions",
            "emotion",
            "ambiguities",
            "determination",
            "specificity",
            "A-V representation",
            "af- fect",
            "parametric",
            "multivariate regression"
        ],
        "content": "PREDICTION OF TIME-VARYING MUSICAL\nMOOD DISTRIBUTIONS FROM AUDIO\nErik M. Schmidt and Youngmoo E. Kim\nElectrical and Computer Engineering, Drexel University\nfeschmidt,ykimg@drexel.edu\nABSTRACT\nThe appeal of music lies in its ability to express emotions,\nand it is natural for us to organize music in terms of emo-\ntional associations. But the ambiguities of emotions make\nthe determination of a single, unequivocal response label\nfor the mood of a piece of music unrealistic. We address\nthis lack of speciﬁcity by modeling human response labels\nto music in the arousal-valence (A-V) representation of af-\nfect as a stochastic distribution. Based upon our collected\ndata, we present and evaluate methods using multiple sets\nof acoustic features to estimate these mood distributions\nparametrically using multivariate regression. Furthermore,\nsince the emotional content of music often varies within a\nsong, we explore the estimation of these A-V distributions\nin atime-varying context, demonstrating the ability of our\nsystem to track changes on a short-time basis.\n1. INTRODUCTION\nThe problem of automated recognition of emotional con-\ntent (mood) within music is the subject of increasing atten-\ntion among music information retrieval (MIR) researchers\n[1–3]. Human judgements are necessary for deriving emo-\ntion labels and associations, but perceptions of the emo-\ntional content of a given song or musical excerpt are bound\nto vary and reﬂect some degree of disagreement between\nlisteners. In developing computational systems for recog-\nnizing musical affect, this lack of speciﬁcity presents sig-\nniﬁcant challenges for the traditional approach of using su-\npervised machine learning systems for classiﬁcation. In-\nstead of viewing musical mood as a singular label or value,\nthe modeling of emotional “ground-truth” as a probability\ndistribution potentially provides a more realistic (and ac-\ncurate) reﬂection of the perceived emotions conveyed by a\nsong.\nA variety of methods are used for collecting mood-\nspeciﬁc labels for music corpora, for example, annotations\ncurated by experts (e.g., Allmusic.com) and the analysis\nof unstructured user-generated tags (e.g., Last.fm). While\nthese approaches efﬁciently provide data for large collec-\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2010 International Society for Music Information Retrieval.tions, they are not well-suited for reﬂecting variations in\nthe emotional content as the music changes. In prior work\nwe created MoodSwings [4], an online collaborative activ-\nity designed to collect second-by-second labels of music\nusing the two-dimensional, arousal-valence (A-V) model\nof human emotion, where valence indicates positive vs.\nnegative emotions and arousal reﬂects emotional intensity\n[5]. The game was designed speciﬁcally to capture A-V\nlabels dynamically (over time) to reﬂect emotion changes\nin synchrony with music and also to collect a distribution\nof labels across multiple players for a given song or even\na moment within a song. This method potentially pro-\nvides quantitative labels that are well-suited to computa-\ntional methods for parameter estimation.\nIn previous work we have investigated short-time re-\ngression approaches for emotional modeling, developing\na functional mapping from a large number of acoustic fea-\ntures directly to A-V space coordinates [1]. Since the ap-\nplication of a single, unvarying mood label across an entire\nsong belies the time-varying nature of music, we focused\non using short-time segments to track emotional changes\nover time. In our current work we demonstrate that not\nonly does the emotional content change over time, but also\nthat a distribution of (as opposed to singular) ratings is\nappropriate for even short time slices (down to one sec-\nond). In observing the collected data, we have found that\nmost examples can be well represented by a single two-\ndimensional Gaussian distribution.\nTo perform the mapping from acoustic features to the A-\nV mood space, we explore parameter prediction using mul-\ntiple linear regression (MLR), partial least-squares (PLS)\nregression, and support vector regression (SVR). In mod-\neling the data as a two dimensional Gaussian, our goal is to\nbe able to predict the A-V distribution parameters N(\u0016;\u0006)\nfrom the acoustic content. We ﬁrst evaluate the effective-\nness of this system in predicting emotion distributions for\n15 second clips and subsequently shorten the analysis win-\ndow length to demonstrate its ability to follow changes in\nA-V label distributions over time.\nNo dominant acoustic feature has yet emerged for mu-\nsic emotion recognition, and previous work has focused\non combining multiple feature sets [1–3, 6]. We evalu-\nate multiple sets of acoustic features for each task, in-\ncluding psychoacoustic (mel-cepstrum and statistical fre-\nquency spectrum descriptors) and music-theoretic (esti-\nmated pitch chroma) representations of the labeled audio.\nAlthough the large number of potential features can present\n465\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)problems, rather than employing dimensionality reduction\nmethods (e.g., principal components analysis) we explore\nan alternative method for combining different feature sets,\nusing ensemble methods to determine the relative contri-\nbution of single-feature systems for improved overall per-\nformance.\n2. BACKGROUND\nThe general approach to implementing automatic mood de-\ntection from audio has been to use supervised machine\nlearning to train statistical models based on acoustic fea-\ntures. Recent work has also indicated that regression ap-\nproaches often outperform classiﬁcation when using simi-\nlar features [1, 2].\nYang et al. introduced the use of regression for map-\nping of high-dimensional acoustic features into the two-\ndimensional space [6]. Support vector regression (SVR),\nas well as a variety of boosting algorithms including Ad-\naBoost.RT, were applied to solve the regression problem.\nThe ground-truth A-V labels were collected by recruiting\n253 college students to annotate the data, and only one la-\nbel was collected per clip. Compiling a wide corpus of\nfeatures totaling 114 feature dimensions, they applied prin-\ncipal component analysis (PCA) before regression.\nFurther conﬁrming the robustness of regression for A-V\nemotion prediction, Han et al. demonstrated that regres-\nsion approaches can outperform classiﬁcation when ap-\nplied to the same problem [2]. Their classiﬁcation task\nconsisted of a quantized version of the A-V space into 11\nblocks. Using a wide variety of audio features, they ini-\ntially investigated the use of classiﬁcation, obtaining only\n\u001833%. Still mapping to the same 11 quantized categories,\napplying regression they obtained up to \u001895% accuracy.\nEerola et al. introduced the use of a three-dimensional\nparametric emotion model for labeling music [3]. In their\nwork they investigated multiple regression approaches in-\ncluding Partial Least-Squares (PLS) regression, an ap-\nproach that considers correlation between label dimen-\nsions. They achieve R2performance of 0.72, 0.85, and\n0.79 for valence, activity, and tension, respectively, using\nPLS and also report peak R2prediction rates for 5 basic\nemotion classes (angry, scary, happy, sad, and tender) as\nranging from 0.58 to 0.74.\n3. GROUND TRUTH DATA COLLECTION\nTraditional methods for collecting perceived mood labels,\nsuch as the soliciting and hiring of human subjects, can be\nﬂawed. In MoodSwings, participants use a graphical inter-\nface to indicate a dynamic position within the A-V space to\nannotate ﬁve 30-second music clips. Each subject provides\na check against the other, reducing the probability of non-\nsense labels. The song clips used are drawn from the “us-\npop2002” database,1and overall we have collected over\n150,000 individual A-V labels spanning more than 1,000\nsongs.\n1uspop2002 dataset: http://labrosa.ee.columbia.edu/projects/musicsim/\nuspop2002.htmlSince the database consists entirely of popular music,\nthe labels collected thus far display an expected bias to-\nwards high-valence and high-arousal values. Although in-\nclusion of this bias could be useful for optimizing classiﬁ-\ncation performance, it is not as helpful for learning a map-\nping from acoustic features that provides coverage of the\nentire emotion space. Because of this trend, we developed\na reduced dataset consisting of 15-second music clips from\n240 songs, selected using the original label set, to approxi-\nmate an even distribution across the four primary quadrants\nof the A-V space. These clips were subjected to intense fo-\ncus within the game in order to form a corpus referred to\nhere as MoodSwings Lite, with signiﬁcantly more labels\nper song clip, which is used in this analysis.\n4. ACOUSTIC FEATURE COLLECTION\nAs previously stated, there is no single dominant feature,\nbut rather many that play a role (e.g., loudness, timbre,\nharmony) in determining the emotional content of music.\nSince our experiments focus on the tracking of emotion\nover time, we chose to focus on solely on time-varying\nfeatures. Our collection (Table 1) contains many features\nthat are popular in Music-IR and speech processing, en-\ncompassing both psychoacoustic and music-theoretic rep-\nresentations. Instead of raw chroma we utilize the auto-\ncorrelation of each short-time chroma vector, providing\na shift-invariant feature. In preliminary experiments we\nfound this feature to perform better than raw chroma, since\nit promotes similarity in terms of the modes of harmony\n(e.g. major, minor, augmented, and diminished chords) as\nopposed to particular chords (e.g., A major vs. D major).\nFeature Description\nMel-frequency\ncepstral coefﬁcients\n(MFCCs) [7]Low-dimensional representation of\nthe spectrum warped according to the\nmel-scale. 20-dimensions used.\nChroma (i.e., Pitch\nClass Proﬁle) [8]Autocorrelation of chroma is used,\nproviding an indication of modality.\nSpectral Spectrum\nDescriptors (SSDs)\n[9]Includes spectral centroid, ﬂux,\nrolloff, and ﬂatness. Often related to\ntimbral texture.\nSpectral Contrast\n[10]Rough representation of the harmonic\ncontent in the frequency domain.\nTable 1. Acoustic feature collection for music emotion\nregression.\n5. EXPERIMENTS AND RESULTS\nGiven the continuous nature of our problem, the predic-\ntion of a 2-d Gaussian within the A-V space, we explored\nseveral methods for multi-variate parameter regression. In\nthese experiments we employ multiple linear regression\n(MLR), partial least-squares (PLS), and support vector re-\ngression (SVR) to create optimal projections from each of\nthe acoustic feature sets described above. For our initial\ndistribution regression experiments, we averaged feature\n466\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Feature/ Regression Average Mean Average KL Average Randomized T-test\nTopology Method Distance Divergence KL Divergence\nMFCC MLR 0:161\u00060:008 4: 098\u00060:513 8 :516\u00061:566 5:306\nChroma MLR 0:185\u00060:010 5: 617\u00060:707 7 :765\u00062:135 5:659\nS. Shape MLR 0:167\u00060:009 4: 183\u00060:656 7 :691\u00061:573 5:582\nS. Contrast MLR 0:151\u00060:008 3: 696\u00060:657 8:601 \u00061:467 5: 192\nMFCC PLS 0:155\u00060:008 3:863 \u00060:56712 8 :306\u00061:389 5:540\nChroma PLS 0:183\u00060:010 5:286 \u00060:96019 7 :146\u00061:665 5:565\nS. Shape PLS 0:151\u00060:008 3:770 \u00060:84026 8 :278\u00061:527 4:951\nS. Contrast PLS 0:151\u00060:008 3: 684\u00060:644 8:700 \u00061:831 5 :171\nMFCC SVR 0:140\u00060:008 3: 186\u00060:597 7:744 \u00061:252 5: 176\nChroma SVR 0:186\u00060:008 4: 831\u00060:737 6 :466\u00060:935 5:655\nS. Shape SVR 0:176\u00060:008 4: 611\u00060:841 7 :348\u00061:025 5:251\nS. Contrast SVR 0:150\u00060:008 3: 357\u00060:500 7 :356\u00061:341 5:301\nStacked Features MLR 0:152\u00060:007 3: 917\u00060:496 9 :355\u00061:879 5:737\nFusion Unweighted MLR 0:149\u00060:007 3: 333\u00060:433 6 :785\u00060:996 5:879\nFusion Weighted MLR 0:147\u00060:007 3: 280\u00060:423 6 :803\u00061:309 5:980\nM.L. Seperate MLR 0:147\u00060:007 3: 399\u00060:478 8 :235\u00061:598 5:598\nM.L. Combined MLR 0:145\u00060:007 3: 198\u00060:454 7:637 \u00061:389 5: 551\nStacked Features PLS 0:145\u00060:006 3: 403\u00060:467 8 :407\u00061:635 5:543\nFusion Unweighted PLS 0:145\u00060:007 3: 332\u00060:508 7 :123\u00061:461 5:681\nFusion Weighted PLS 0:145\u00060:006 3: 309\u00060:501 7 :160\u00061:373 5:619\nM.L. Seperate PLS 0:145\u00060:008 3: 465\u00060:577 8 :426\u00061:705 5:433\nM.L. Combined PLS 0:144\u00060:007 3: 206\u00060:515 7:889 \u00061:656 5: 485\nTable 2. Distribution regression results for ﬁfteen second clips.\ndimensions across all frames of a given 15-second mu-\nsic clip, thus representing each clip with a single vector\nof features. Preliminary experiments were performed us-\ning second- and higher-order statistics with the 15-second\nclips, but in all cases the inclusions of such data failed to\nshow any signiﬁcant performance gains.\nIn all experiments, to avoid the well-known “album-\neffect”, we ensured that any songs that were recorded on\nthe same album were either placed entirely in the training\nor testing set. Additionally, each experiment was subject to\nover 50 cross-validations, varying the distribution of train-\ning and testing data sets.\n5.1 Single Feature Emotion Distribution Prediction\nThere are many possible methods for evaluating the perfor-\nmance of our system. Kullback-Liebler (KL) divergence\n(relative entropy) is commonly used to compare proba-\nbility distributions. Since the regression problem targets\nknown distributions, our primary performance metric is the\nnon-symmetrized (one-way) KL divergence (from the pro-\njected distribution to that of the collected A-V labels). To\nprovide an additional qualitative metric, we also provide\nresults as the Euclidean distance between the projected\nmeans as a normalized percentage of the A-V space. How-\never, to provide context to KL values and to benchmark the\nsigniﬁcance of the regression results, we compared the pro-\njections to those of an essentially random baseline. Given a\ntrained regressor and a set of labeled testing examples, we\nﬁrst determined an A-V distribution for each sample. The\nresulting KL divergence to the corresponding A-V distri-\nbution was compared to that of another randomly selected\nA-V distribution from the test set. Comparing these casesover 50 cross-validations, we computed Student’s T-test for\npaired samples to verify the statistical signiﬁcance of our\nresults.\nFrom Table 2 it can be seen that the best performing\nsingle feature system is SVR with MFCC features at an\naverage KL of 3.186. However, in both the MLR and PLS\nsystem the highest performing single feature is spectral\ncontrast with 3.696 and 3.684, respectively. As the main\nadvantage of PLS over MLR is that it observes any corre-\nlation between dimensions in the multivariate regression,\nit is surprising that the performance difference between\nthe two is nearly negligible. Given our degrees of free-\ndom (72 test samples), even our lowest T-test value (5.171)\nproduces conﬁdence of statistical signiﬁcance greater than\n99.999%.\nShown in Figure 1 is the projection of six 15-second\nclips into the (A-V) space resulting from multiple regres-\nsion methods and acoustic features. The standard deviation\nof the ground truth as well as each projection is shown as\nan ellipse. The performance of the regression can be eval-\nuated in terms of the total amount of overlap between a\nprojection and its ground truth.\n5.2 Feature Fusion\nWhile most individual features perform reasonably in map-\nping to A-V coordinates, a method for combining in-\nformation from these domains (more informed than sim-\nply concatenating the features) could potentially lead to\nhigher performance. In this section we investigate mul-\ntiple schemes for feature fusion. Given the very small per-\nformance gains and high computational overhead of SVR,\nwe chose to narrow our focus to MLR and PLS for these\n467\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)−0.500.5Ben Folds Five\nBrick, 108 to 122 secsValenceCake\nShe’ll Come Back To Me, 66 to 80 secsCollective Soul\nBreathe, 34 to 48 secsCounting Crows\nPerfect Blue Buildings, 51 to 65 secs\n−0.5 0 0.5−0.500.5Dave Matthews Band\nThe Song That Jane Likes, 56 to 70 secs\nArousalValence\n−0.5 0 0.5Jackson, Michael\nRock With You, 99 to 113 secs\nArousal−0.5 0 0.5Queen\nSave Me, 20 to 34 secs\nArousalEmotion Distribution Projected From Acoustic Features\n−0.5 0 0.5Wonder, Stevie\nSir Duke, 124 to 138 secs\nArousalFigure 1. Collected A-V labels and distribution projections resulting from regression analysis. A-V labels: second-by-\nsecond labels per song (gray \u000f),\u0006of collected labels (solid red ellipse), \u0006of MLR projection from spectral contrast features\n(dash-dot blue ellipse), \u0006of MLR Multi-Level combined projection (dashed green ellipse).\nexperiments. As our ultimate system will require many\npredictions over time in order to reﬂect emotional changes,\nthe costs of SVR outweigh the beneﬁts.\nIn our fusion results the performance for simply stack-\ning features into one large feature vector is provided to give\ncontext to the other fusion methods. Our more simple ap-\nproach consists of a fusion system that is a combination of\nthe outputs from the individual feature regression systems.\nIn the unweighted approach we simply average the param-\neter outputs from each individual feature regressor, and in\nthe weighted approach we weight each individual feature\nregressor by its ability to predict a particular parameter,\nwhich is determined by leave-one-out cross-validation.\nIn addition, we develop a two-level regression scheme\nby feeding the outputs of individual regressors, each\ntrained using distinct features, into a second-stage regres-\nsor determining the ﬁnal prediction. We investigated two\ntopologies (Figure 2): in one case the secondary arousal\nand valence regressors receive only arousal and valence\nestimates, respectively; in the second case the secondary\narousal and valence regressors receive both arousal and va-\nlence estimates from the ﬁrst-stage. We refer to these two\ntopologies as multi-layer separate and multi-layer com-\nbined. In all cases the secondary regressors are trained\nusing a leave-one-out method (on each iteration we train\nthe ﬁrst-stage regressors leaving one example out and use\nthe estimates of that example from the ﬁrst stage to train\nthe second stage). The results for both cases are shown in\nTable 2.\nμMFCC RegressorContrast RegressorFeature LayerRegressorsSecond LayerRegressorsMulti-LayerSeparateμ1Σ1μ1Σ1MFCCs\nSpectralContrastMulti-LayerSeparateμ1\nΣ1Parameterized Emotion DistributionMulti-Layer SeparateMFCC RegressorContrast RegressorFeature LayerRegressorsΣμΣSecond LayerRegressorsMulti-LayerCombinedμΣMFCCsSpectralContrastMulti-Layer CombinedParameterized Emotion DistributionFigure 2. Multi-layer regression topologies.\n5.3 Time-varying emotion distribution prediction\nIn attempting to predict the emotion distribution over time,\nwe next shorten label observation rate to once per-second\nand attempt to regress multiple feature windows from each\nsong. For the ground truth data collection this means that\nfor each 15-second clip we now have 15 examples, increas-\ning our total corpus to 3600 examples. Of course for any\n468\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Feature/ Average Mean Average KL Average Randomized T-test\nTopology Distance Divergence KL Divergence\nMFCC 0:169\u00060:007 14:61 \u00063:751 27 :00\u000610:33 10:77\nChroma 0:190\u00060:007 18:71 \u00066:819 22 :53\u00066:984 9: 403\nS. Shape 0:173\u00060:007 15:46 \u00066:402 24 :61\u00069:220 11:06\nS. Contrast 0:160\u00060:006 13:61 \u00065:007 27:29 \u00069:861 10:23\nM.L. Combined 0:154\u00060:006 13: 10\u00065:359 28:39 \u000610:35 10:08\nTable 3. Distribution regression results for short-time (one-second) A-V labels.\nexperiment, multiple examples from the same song must\nbe either all in the training or testing set. In addition, as\nit is clear that some past data may be necessary to accu-\nrately determine the current emotional content, we include\npast features and investigate the optimal feature window\nlength.\nGiven the similar performance of MLR and PLS in fu-\nsion methods, for our short time analysis we will restrict\nourselves to only the MLR methods. The similarity in\nperformance is likely due to the fact that in the multi-\nlayer combined system, both MLR and PLS are able to\naccount for the correlation between label dimensions. In\nmoving forward with time-varying regression, we wish\nto be able to apply all methods in real-time as a “vir-\ntual annotator” for MoodSwings. This directly addresses\nthe bootstrapping problem inherent to the system in cases\nwhere multiple annotators are not available at the same\ntime. A preliminary single-user version of MoodSwings\ncalled MoodSwings Single Player,2which demonstrates\nour real-time regression system, is available online.\n! \" #! #\" $! $\" %! %\" &! &\"!'#(!'#(\"!'#)!'#)\"!'#*!'#*\"!'#+!'#+\"!'$,-./0.1234564770.128019.:2;415/<2=1>?@707\nA0-42B74C.197DE.6->?0F492G07/>1C4HIJJ\nJ<6.->K<>L4\nJ.1/6>7/\n41s\n14s\n9s\n10s\nFigure 3. Window length analysis for different acoustic\nfeatures.\nFor our time-varying approach, we seek to develop re-\ngressors that can predict the emotion for a single second\nusing only current and past audio data. In terms of our data\ncollection this implies that we have 15 distributions for\neach 15-second music clip (for 240 clips this yields a to-\ntal of 3600 distributions). But we should also consider the\noptimal analysis window length for regression from each\nacoustic feature set. In Figure 3 we perform a regression\nanalysis for each window length from 1 to 45 seconds (in\nincrements of one second) and plot the average KL diver-\n2MoodSwings Single Player: http://music.ece.drexel.edu/msspgence from the projections to the collected distributions.\nAs in previous experiments, the training/testing data is split\n70%/30% and cross-validated 50 times. From the window\nlength analysis in Figure 3, it can be seen that the optimal\nwindow length is not the same for all feature domains. For\nMFCCs we obtain the most accurate prediction using 13\nseconds of past feature data, also 13 seconds for SSDs, 15\nfor spectral contrast, and 41 seconds for chroma. We use\nthese feature window lengths in the regression analysis to\nfollow.\nIn moving to short-time labels, it can be seen from Table\n3 that our overall KL has increased, but our average dis-\ntance ratings have mostly remained the same. This is most\nlikely attributed to the fact that the underlying label covari-\nance is less consistent due to the smaller quantity of col-\nlected A-V labels. Out T-test values have increased as well,\nwhich can be attributed to the overall increase in examples\n(from 240 to 3600). Considering our short-time degrees\nof freedom (1080 testing examples), our lowest T value\n(9.403) produces conﬁdence of statistical signiﬁcance (vs.\nrandomly selected projections) higher than 99.999%. To\nvisualize emotion regression over time, we have chosen\nthree clips which display a clear shift in emotion distribu-\ntion, plotting both the collected and projected distributions\nat one second intervals (Figure 4).\n6. DISCUSSION AND FUTURE WORK\nIn working with highly subjective emotional labels, where\nthere is not necessarily a singular rating for even the small-\nest time slice, it is clear that we can develop a more ac-\ncurate system (in terms of predicting actual human labels)\nby representing the ground truth as a distribution. While\naccounting for potential dependence between the distribu-\ntion parameters in the A-V emotion space seemed to be of\nhigh importance, some of the best performing techniques\nassumed total independence of parameters. In particular,\ncombining MLR in multiple stages produces results com-\nparable to more computationally complex methods.\nOne of our targeted applications, a “virtual annotator”\nto be used in MoodSwings, requires real-time calculation\nof projections, which also favors the simpler regression im-\nplementations. For the activity, the required degree of ac-\ncuracy is questionable to begin with [11]. In our observa-\ntions, we have found that it is more important for a virtual\nannotator to behave “realistically” (appropriate movement\nwhen the emotion changes) in order to keep a human par-\nticipant engaged in the activity. But as we implement the\n469\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Figure 4. Time-varying emotion distribution regression\nresults for three example 15-second music clips (markers\nbecome darker as time advances): second-by-second labels\nper song (gray\u000f),\u0006of the collected labels over 1-second\nintervals (red ellipse), and \u0006of the distribution projected\nfrom acoustic features in 1-second intervals (blue ellipse).virtual annotator to facilitate the collection of more human\ndata, we hope to continue increasing the accuracy of our\nregression system.\n7. ACKNOWLEDGMENTS\nThis work is supported by National Science Foundation\naward IIS-0644151.\n8. REFERENCES\n[1] E. M. Schmidt, D. Turnbull, and Y . E. Kim, “Feature\nselection for content-based, time-varying musical emo-\ntion regression,” in MIR ’10: Proc. of the Intl. Conf. on\nMultimedia Information Retrieval, Philadelphia, PA,\n2010, pp. 267–274.\n[2] B. Han, S. Rho, R. B. Dannenberg, and E. Hwang,\n“Smers: Music emotion recognition using support vec-\ntor regression,” in Proc. of the 10th Intl. Society for\nMusic Information Conf., Kobe, Japan, 2009.\n[3] T. Eerola, O. Lartillot, and P. Toiviainen, “Prediction\nof multidimensional emotional ratings in music from\naudio using multivariate regression models,” in Proc.\nof the 10th Intl. Society for Music Information Conf.,\nKobe, Japan, 2009.\n[4] Y . E. Kim, E. Schmidt, and L. Emelle, “Moodswings:\nA collaborative game for music mood label collection,”\ninProc. of the 9th Intl. Conf. on Music Information\nRetrieval, Philadelphia, PA, September 2008.\n[5] R. E. Thayer, The Biopsychology of Mood and Arousal.\nOxford, U.K.: Oxford Univ. Press, 1989.\n[6] Y .-H. Yang, Y .-C. Lin, Y .-F. Su, and H. Chen, “A re-\ngression approach to music emotion recognition,” Au-\ndio, Speech, and Language Processing, IEEE Transac-\ntions on, vol. 16, no. 2, pp. 448–457, 2008.\n[7] S. Davis and P. Mermelstein, “Comparison of paramet-\nric representations for monosyllabic word recognition\nin continuously spoken sentences,” Acoustics, Speech\nand Signal Processing, IEEE Transactions on, vol. 28,\nno. 4, pp. 357–366, 1980.\n[8] T. Fujishima, “Realtime chord recognition of musical\nsound: a system using common lisp music.” in Proc. of\nthe Intl. Computer Music Conf., 1999.\n[9] G. Tzanetakis and P. Cook, “Musical genre classiﬁca-\ntion of audio signals,” Speech and Audio Processing,\nIEEE Transactions on, vol. 10, no. 5, pp. 293–302,\n2002.\n[10] D. Jiang, L. Lu, H. Zhang, J. Tao, and L. Cai, “Mu-\nsic type classiﬁcation by spectral contrast feature,” in\nProc. Intl. Conf. on Multimedia and Expo, vol. 1, 2002,\npp. 113–116.\n[11] B. G. Morton, J. A. Speck, E. M. Schmidt, and Y . E.\nKim, “Improving music emotion labeling using hu-\nman computation,” in HCOMP ’10: Proc. of the ACM\nSIGKDD Workshop on Human Computation, Washin-\nton, D.C., 2010.\n470\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "Islands of Gaussians: The Self Organizing Map and Gaussian Music Similarity Features.",
        "author": [
            "Dominik Schnitzer",
            "Arthur Flexer",
            "Gerhard Widmer",
            "Martin Gasser"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1415248",
        "url": "https://doi.org/10.5281/zenodo.1415248",
        "ee": "https://zenodo.org/records/1415248/files/SchnitzerFWG10.pdf",
        "abstract": "Multivariate Gaussians are of special interest in the MIR field of automatic music recommendation. They are used as the de facto standard representation of music timbre to compute music similarity. However, standard algorithms for clustering and visualization are usually not designed to handle Gaussian distributions and their attached metrics (e.g. the Kullback-Leibler divergence). Hence to use these features the algorithms generally handle them indirectly by first mapping them to a vector space, for example by deriv- ing a feature vector representation from a similarity matrix. This paper uses the symmetrized Kullback-Leibler cen- troid of Gaussians to show how to avoid the vectorization detour for the Self Organizing Maps (SOM) data visualiza- tion algorithm. We propose an approach so that the algo- rithm can directly and naturally work on Gaussian music similarity features to compute maps of music collections. We show that by using our approach we can create SOMs which (1) better preserve the original similarity topology and (2) are far less complex to compute, as the often costly vectorization step is eliminated.",
        "zenodo_id": 1415248,
        "dblp_key": "conf/ismir/SchnitzerFWG10",
        "keywords": [
            "Multivariate Gaussians",
            "MIR field",
            "Automatic music recommendation",
            "Clustering",
            "Visualization",
            "Symmetrized Kullback-Leibler centroid",
            "Self Organizing Maps (SOM)",
            "Music similarity",
            "Feature vector representation",
            "Music collections"
        ],
        "content": "ISLANDS OF GAUSSIANS: THE SELF ORGANIZING MAP\nAND GAUSSIAN MUSIC SIMILARITY FEATURES\nDominik Schnitzer1;2, Arthur Flexer1, Gerhard Widmer1;2, Martin Gasser1\n1Austrian Research Institute for Artiﬁcial Intelligence (OFAI), Vienna, Austria\n2Department of Computational Perception, Johannes Kepler University, Linz, Austria\ndominik.schnitzer@ofai.at, arthur.flexer@ofai.at,\ngerhard.widmer@jku.at, martin.gasser@ofai.at\nABSTRACT\nMultivariate Gaussians are of special interest in the MIR\nﬁeld of automatic music recommendation. They are used\nas the de facto standard representation of music timbre to\ncompute music similarity. However, standard algorithms\nfor clustering and visualization are usually not designed\nto handle Gaussian distributions and their attached metrics\n(e.g. the Kullback-Leibler divergence). Hence to use these\nfeatures the algorithms generally handle them indirectly by\nﬁrst mapping them to a vector space, for example by deriv-\ning a feature vector representation from a similarity matrix.\nThis paper uses the symmetrized Kullback-Leibler cen-\ntroid of Gaussians to show how to avoid the vectorization\ndetour for the Self Organizing Maps (SOM) data visualiza-\ntion algorithm. We propose an approach so that the algo-\nrithm can directly and naturally work on Gaussian music\nsimilarity features to compute maps of music collections.\nWe show that by using our approach we can create SOMs\nwhich (1) better preserve the original similarity topology\nand (2) are far less complex to compute, as the often costly\nvectorization step is eliminated.\n1. INTRODUCTION\nGood content-based music recommendation systems are\ncurrently on the wish list of many music services since they\nhelp handling the massive audio databases which are cur-\nrently emerging: be it simple music recommendations in\nthe form of automatically generated playlists, advanced vi-\nsualizations of the collections, or other new ideas for music\ndiscovery and listening.\nOne of the basic foundations of automatic content-\nbased music recommendation systems is the ability to\ncompute music similarity. In research it is not yet settled\nhow to extract and represent good music similarity fea-\ntures that correspond well with the human perception of\ngeneral music similarity. However, the currently best per-\nforming methods have one thing in common: A central\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2010 International Society for Music Information Retrieval.component in most of the currently best working meth-\nods is a representation of timbre in terms of a multi-\nvariate Gaussian. Take for example, the top three algo-\nrithms in the MIREX12009 (Music Information Retrieval\nExchange [4]) Automatic Music Recommendation eval-\nuations. All used multivariate Gaussians and Kullback-\nLeibler-related divergences to describe and compare their\nsimilarity features. The basic idea of using a single multi-\nvariate Gaussian to model timbre similarity was ﬁrst used\nby Mandel and Ellis [10]. In their case the Gaussian is\ncomputed over the Mel Frequency Cepstrum Coefﬁcient\nrepresentation [8] of the song.\nWe see the Gaussian representation of the music fea-\ntures as a very powerful way to describe the variability\nof the features in a song. With the Kullback-Leibler di-\nvergence and related divergence measures (Symmetrized\nKullback-Leibler, Jensen-Shannon divergence) there also\nexist well founded ways to compute a distance/similarity\nvalue between the features (even though there are some\nproblems related to the non-metricity of the divergences).\nThings get interesting when we leave the path of simple\nfeature representation and similarity computation. Stan-\ndard algorithms for indexing, clustering or visualization\nare usually just not designed to work with Gaussian\ndistributions and non-standard metrics like the one the\nKullback-Leibler divergence induces. For instance, how\nwould computing a simple average be performed with\nGaussian features?\nTo work around that limitation the feature data is often\nartiﬁcially vectorized. In the domain of content based mu-\nsic recommendation techniques we have seen approaches\ncomputing the full distance matrix and using each row of\nthe matrix as a feature vector [5,14], or more venturesome\nones reshaping the Gaussian covariance matrix and mean\nvector into a single long vector [11]. The ﬁrst solution is\nexpensive to compute the larger the music collection is,\nand the latter one, although fast, takes away the sense of\nusing Gaussians.\nIn 2005 Banerjee et al. published an important pa-\nper in the machine learning literature where they show\nhow to generalize the k-means clustering algorithm to the\nbroad class of Bregman divergences [1]. This generaliza-\ntion practically opened all centroid-based algorithms to the\n1http://www.music-ir.org/mirex/2009\n327\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)wide range of Bregman divergences, which the Kullback-\nLeibler divergence is part of.\nThis paper builds on these ﬁndings and deﬁnes the\nweighted symmetrized Kullback-Leibler centroid to show\nhow the Self Organizing Map (SOM) algorithm can work\ndirectly and naturally with Gaussians. The approach is able\nto create higher-quality two dimensional visualizations of\nmusic archives while retaining the nice scalability charac-\nteristics of the general SOM algorithm.\n2. RELATED WORK\nThere already exists a wide range of publications deal-\ning with visualizing acoustic music similarity features\non SOMs. One of the ﬁrst to do so were Rauber and\nFr¨uhwirth [18] who use a very basic music similarity fea-\nture and a simple tabular grid which displays the clus-\ntered song titles on the map. This idea was extended by\nPampalk et al. who use rhythmic similarity features and\nthe Smoothed Data Histogram [16] (SDH) visualization\nto draw the SOM. Their visualization is inspired by geo-\ngraphical maps: blue regions (oceans) indicate areas onto\nwhich very few pieces of music are mapped, whereas clus-\nters containing a larger quantity of pieces are colored in\nbrown and white (mountains and snow). It was published\nunder the name “Islands of Music” [15], which inspired\nthe title of the presented paper.\n‘Neptune’ [5] developed by Knees et al. improved\nPampalk’s visualization by taking the two dimensional\nmap into the third dimension. They add crawled meta-\ninformation and pictures from the web and allow a 3D walk\nthrough a music collection. They use a mix of rhythmic\nand timbre based similarity measures.\nThe ‘Globe of Music’ [7] by Leitich and Topf uses a\nGeoSOM [20] to map the music collection onto a globe for\nexploration. L ¨ubbers et al. developed the ‘SoniXplorer’ [9]\nto navigate through music archives. They use a multimodal\nnavigation model where the auralization of music supports\nthe user on the SOM visualization of the music collection.\nM¨orchen et al. use the Emergent SOM algorithm to\nvisualize and cluster music collections in their ‘Music\nMiner’ [12] system. For music similarity they use a large\nset of low-level features. In their paper they also point out\nthat they cannot use Gaussian music similarity features as\n“they can not be used with datamining algorithms requir-\ning the calculation of a centroid”. Solving that is the focus\nof the next sections of this paper.\n3. PRELIMINARIES\nTo demonstrate how to use the SOM algorithm with Gaus-\nsian features we use the standard music similarity al-\ngorithm proposed by Mandel and Ellis [10]. This ap-\nproach computes a single Gaussian music-timbre similar-\nity feature. Similarity is computed with the symmetrized\nKullback-Leibler divergence.\nSince its publication this approach has been modiﬁed\nand improved in various ways, yet most stayed with theGaussian feature representation. So everything presented\nhere will of course work with the derived approaches too.\n3.1 Music Features and Similarity Computation\nTo extract the timbre music similarity features we extract\n25Mel Cepstrum Frequency Coefﬁcients [8] (MFCCs) for\nevery 46ms of audio. This corresponds to a window size of\n1024 audio samples at 22:05kHz . In this way a Gaussian\ntimbre model xﬁnally consists of a 25-dimensional mean\nvector\u0016, and a 25\u000225-dimensional covariance matrix \u0006.\nTo compute the similarity between two Gaussians the\nKullback-Leibler divergence (kld) can be used. There ex-\nists a closed form of this divergence [17] which allows the\ndivergence to be computed between two m-dimensional\nGaussiansx1;2\u0018N(\u0016;\u0006).\n2kld(x 1jjx2) =\nloge\u0012det \u0006 2\ndet \u0006 1\u0013\n+ tr\u0000\n\u0006\u00001\n2\u00061\u0001\n+\n(\u00162\u0000\u00161)>\u0006\u00001\n1(\u00162\u0000\u00161)\u0000m (1)\nSince thekldis asymmetric usually a symmetrized vari-\nant (skld) of the divergence is used for music similarity\nestimation:\nskld(x 1jjx2) =kld(x 1jjx2) +kld(x 2jjx1)\n2(2)\n3.2 Self Organizing Map (SOM) Algorithm\nThe SOM [6] is an unsupervised neural network that or-\nganizes multivariate data on a two dimensional map and\nis suited well for visualizations. It maps items which are\nsimilar in the original high-dimensional space onto loca-\ntions close to each other on the map.\nBasically the SOM consists of an ordered set of so-\ncalled map unitsri, each of which is assigned a reference\nvector (ormodel vector )miin the feature space. The set\nof all reference vectors of a SOM is called its codebook. In\nthe simplest case the codebook is initialized by a random\nstrategy.\nTo compute a SOM, ﬁrst the map dimensions and the\nnumber of training iterations (t) are ﬁxed. Training is done\nin four basic repeating steps:\n1. At iteration tselect a random vector v(t)from the\nset of features.\n2. Search for the best matching map unit con the SOM\nby computing the (Euclidean) distance of v(t)to all\nmi.\n3. The codebook is updated by calculating a weighted\ncentroid between v(t)and the best matching unit rc.\nBased on a neighborhood weighting function hci(t)\nall map units participate in the adaptations depend-\ning on their distance on the two-dimensional outout\n328\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)map. Equation 3 shows a standard Gaussian neigh-\nborhood function.\nhci(t) =\u000b(t) exp\u0012\n\u0000jjrc\u0000rijj\n2\u000b2(t)\u0013\n(3)\nmi(t+ 1) =mi(t) +hci(t) [v(t)\u0000mi(t)] (4)\n4. The adaptation strength \u000b(t)is decreased gradually\nwith the iteration cycle t. This supports the forma-\ntion of large clusters in the beginning and a ﬁne-\ntuning toward the end of the training.\nUsually, the iterative training is continued until a conver-\ngence criterion is fulﬁlled or a preselected number of train-\ning iterations is ﬁnished. In the ﬁnal step all items are as-\nsigned to the map unit they are most similar to.\nA popular way of visualizing SOMs trained with mu-\nsic similarity features is the Smoothed Data Histogram\n(SDH) [16].\n4. FROM VECTORS TO GAUSSIAN\nDISTRIBUTIONS\nAlthough originally SOMs were deﬁned for Euclidean fea-\nture vectors only, the algorithm per se is not limited to the\nvector space. Kohonen himself mentions this in the most\nrecent edition of his standard work on Self-Organizing\nMaps [6]. This observation will be the basis for extend-\ning the SOM algorithm to the ‘distribution space’.\nA closer look at the SOM algorithm sketched in the last\nsection, shows that there is only a single step where the\nalgorithm in fact depends on vectors. It is the computation\nof the weighted centroid in Equation 42. We now rewrite\nEquation 4 so that it is more obvious that a centroid (a\nweighted mean of several vectors) is computed:\nmi(t+ 1) = (1\u0000hci(t))mi(t) +hci(t)v(t) (5)\nThe essence of this is if a weighted centroid can be com-\nputed for Gaussians and the symmetrized Kullback-Leibler\ndivergence, the SOM algorithm would, without modiﬁca-\ntions, work for our data.\n4.1 Weighted Symmetrized Kullback-Leibler\nCentroid\nThe Kullback-Leibler divergence is part of the broad fam-\nily of Bregman divergences [3]. In 2005 Banerjee et al.\nshowed that a unique centroid exists for any Bregman di-\nvergence and proved that the standard k-means (and with\nthat basically any centroid-based) works in this rich family\nof divergences [1].\nAs Bregman divergences are asymmetric divergences,\nthere exist three uniquely deﬁned centroids for a diver-\ngenceDand a set of points xi: the left-sided centroid cL,\nright-sided centroid cRand symmetrized centroid cS. The\n2The distancejjrc\u0000rijjin Equation 3 is computed in ‘map space’\nand does not need to be modiﬁed.centroids are the optimizers of the minimum average dis-\ntance:\ncL= argmin\nc1\nnnX\ni=1D(cjjx i) (6)\ncR= argmin\nc1\nnnX\ni=1D(xijjc) (7)\ncS= argmin\nc1\nnnX\ni=1D(xijjc) +D(cjjx i)\n2(8)\nAs Nielsen and Nock show [13], no closed analytical form\nto compute the symmetrized centroid exists. In their pa-\nper they present an efﬁcient geodesic walk algorithm to\nﬁnd the symmetrized Bregman centroid cusing the left cL\nand rightcRcentroids. In the last section they also deﬁne\nthe three centroids for the Kullback-Leibler divergence and\nxi\u0018N(\u0016xi;\u0006xi)multivariate Gaussians.\nTo use these centroids in the SOM algorithm we need to\nmodify them and add a weighing term \u0015i(withPn\ni=1\u0015i=\n1) for each Gaussian. The individual weighted centroid\ndeﬁnitions are given in the next paragraphs3.\n4.1.1 Weighted Right-Sided Gaussian kld-Centroid\nThe right-type kld-centroid Gaussian cR\u0018N (\u0016cR;\u0006cR)\ncoincides with the center of mass. For Gaussians xiand\nthekldit is deﬁned as:\n\u0016cR=nX\ni=1\u0015i\u0016i (9)\n\u0006cR=nX\ni=1\u0015i\u0000\n\u0016i\u0002\u0016T\ni+ \u0006i\u0001\n\u0000\u0016cR\u0002\u0016T\ncR(10)\nwith\u0015ias deﬁned above.\n4.1.2 Weighted Left-Sided Gaussian kld-Centroid\nThe left-type kld-centroid Gaussian cL\u0018 N (\u0016cL;\u0006cL)\nis obtained equivalently by minimizing the dual right-type\ncentroid problem. For Gaussians xiand thekldit is de-\nﬁned as:\n\u0016cL= \u0006cL\u0002nX\ni=1\u0015i(\u0006\u00001\ni\u0002\u0016i) (11)\n\u0006cL= nX\ni=1\u0015i\u0006\u00001\ni!\u00001\n(12)\nwith\u0015ias deﬁned above.\n4.1.3 Weighted (mid-point) Gaussian skld-Centroid\nTo compute the weighted symmetrized Kullback-Leibler\ncentroid, the weighted left and right centroids need to be\ncomputed. We then use the mid-point centroid approxi-\nmation instead of computing the exact centroid. The mid-\npoint empirically proved to be a good approximation of\nthe true centroid of the skld [19]. The approximation\n3A detailed listing and explaination of the derivation of each centroid\ncan not be given due to the length constraints of this paper.\n329\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)(a) Euclidean Vectors (from the similarity\nmatrix)\n(b) Gaussians\nFigure 1. A 10\u000210SOM, computed with the two different approaches. The SOMs are visualized using the Matlab\nSmoothed Data Histogram Toolbox [16] clustering a collection of 1 000 music pieces. They were created using identical\nparameters for initialization and learning.\ncS\u0018N(\u0016cS;\u0006cS)merges the weighted left and right cen-\ntroids in one step:\n\u0016cS=1\n2(\u0016cL+\u0016cR) (13)\n\u0006cS=1\n2X\ni=fL;Rg\u0000\n\u0016ci\u0002\u0016T\nci+ \u0006ci\u0001\n\u0000\u0016cS\u0002\u0016T\ncS\n(14)\n4.2 The Generalized SOM\nWith the deﬁnition of the weighted skld-centroid every-\nthing is in place to use the SOM algorithm with Gaussian\nmusic-timbre models and the skld:\n1. Initialization of the SOM and its miis done by\nselecting random Gaussians from the music-timbre\nmodels.\n2. Most importantly the iterative computation of the\nweighted centroid during the training of the code-\nbook can now be replaced with the weighted sym-\nmetrizedkldcentroid.\n3. The learning rate adaptation and neighborhood func-\ntions do not need to be changed. They are not depen-\ndent on the features.\n4. In the ﬁnal step the Gaussians are assigned to the\nnearest map units according to skld.\nThe approach of using the SOM directly with the Gaussian\nfeatures is very close to the data and the original intention\nof the algorithm. We evaluate the generalized SOM in the\nnext section.\n5. EVALUATION\nTo compare SOMs generated with different approaches,\nwe quantify how well the original neighborhood topology\nis preserved in a SOM mapping. As we need to com-\npare SOMs using different metrics it is not possible to use\nstandard SOM quality measures like the quantization error.Therefore we are using a rank distance. We search for the\nnnearest neighbors of every item xiin the original space\nand check their location on the SOM. Ideally the nearest\nneighbors should also be mapped close to each other on\nthe SOM. For a given number nof nearest neighbors and a\nGaussianxithis will be measured as the nnearest neigh-\nbor rank distance:\n1. Assign all Gaussians to their corresponding map unit\non the SOM.\n2. For Gaussian xicompute the Euclidean distance of\nits assigned map unit on the SOM to the map units\nassigned to all other Gaussians.\n3. Sort the list of Euclidean distances in ascending or-\nder and transform it into a list of ranks.\n4. Find the nnearest neighbors of xiin the original\nspace and average across their corresponding ranks.\nTheaveragennearest neighbor rank distance of allxiis a\nvalue describing the whole SOM. The lower its value, the\nbetter the preservation of the neighborhood on the SOM.\n5.1 Setup\nTo test how the SOM algorithm performs operating di-\nrectly on the Gaussians we used a test collection of 16 754\nsongs. The songs are typical full three to ﬁve minutes\nsongs of a mixed genre music collection. We compute the\nGaussian timbre music similarity features for these songs\n(see Section 3.1) so that every song is characterized by a\n25-dimensional Gaussian.\nTo compare the quality of the SOMs generated with our\napproach, we also generate SOMs with vectorized features.\nFor each Gaussian feature we build a vector by computing\nthe distance to all other features and normalizing this dis-\ntance vector to zero mean and unit variance. This is equiv-\nalent to computing the full similarity matrix and using each\nrow as a feature vector (done e.g. in [5, 14]).\nAs a baseline for our experiments we also use a ran-\ndomly initialized SOM without any training. In our exper-\niments we vary various SOM parameters to test different\n330\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)151020020406080100120140160\nNearest Neighbors (NNs)Rank Distance\nto NNs\n151020050100150200250300350\nNearest Neighbors (NNs)1510200100200300400500600\nNearest Neighbors (NNs)1510200100200300400500600700800\nGaussians\nEuclidean Vectors\nRandom Ini-tialization\nNearest Neighbors (NNs)500 songs, 5x5 SOM 1000 songs, 7x7 SOM 1500 songs, 10x10 SOM 2000 songs, 12x12 SOMFigure 2. Four plots of the average n-nearest neighbor rank distance using different SOM conﬁgurations. Each plot\ncompares the two strategies (Gaussian/Vectorized) to compute a SOM. As a baseline a randomly initialized SOM (black\nbar) is added. Lower rank distance values indicate that the original nearest neighbors (NNs) are mapped closer to each other\non the SOM. The plots clearly shows that using Gaussians directly produces SOMs better preserving the neighborhood.\nconﬁgurations: (1) we used SOM grid sizes of 5\u00025,7\u00027,\n10\u000210,12\u000212, (2) and mapped 500,1 000, 1 500 or2 000\nsongs (randomly drawn from the base collection).\nTo ensure a fair evaluation we took the following pre-\ncautions:\n\u000fIn each run the same random seed is used for the ran-\ndom, vectorized and Gaussian SOM. This ensures\nidentical random initialization and use of the same\nrandomly chosen features during the training phase.\n\u000fThe previously deﬁned average n-nearest neigh-\nbor rank distance is computed for each map (n =\n1;5;10;20).\n\u000fEach unique experiment conﬁguration is repeated\nten times. Results are averaged.\n5.2 Results\nFigure 2 shows the average n-nearest neighbor rank dis-\ntance for four selected SOM conﬁgurations which have\nbeen evaluated. In these experiments it can be seen that\ndirectly using Gaussians to train a SOM results in maps\nwhich are able to better preserve the neighborhood.\nThe results of all experiments conducted are summa-\nrized in Table 1. The table expresses the improvement of\nSOMs created with the Gaussian and vectorized approach\nrelative to the randomly initialized SOMs. It conﬁrms the\nresults of the previous ﬁgure that throughout the conﬁgura-\ntions SOMs computed directly with the Gaussians produce\nhigher-quality mappings and that this method should be\npreferred.\nAn illustration comparing two SOMs created with the\ntwo approaches is plotted in Figure 1. Albeit we can not\nmake any judgements concerning the quality of the SOMs\nfrom the plots, we can see that a more structured SOM\nemerged from directly using the Gaussian features.\nBesides producing higher-quality SOMs, we emphasize\nthat this approach is also far less complex to compute than\na variant working with vectorized features: (1) it is al-\nmost impossible to compute the full similarity matrix on a\nlarge collection of songs (i.e. over 100 000 songs) and (2) a\nSOM with 100 000-dimensional (or larger) vectors wouldNearest Neighbors\nFeatures Type 1 5 10 20\n500 Gaussians 0:90 0:41 0:41 0:43\nVectors 0:97:55 0:56 0:54\n1000 Gaussians 0:80 0:40 0:40 0:41\nVectors 1:07 0:64 0:63 0:62\n1500 Gaussians 0:75 0:38 0:38 0:40\nVectors 0:75 0:60 0:60 0:60\n2000 Gaussians 0:76 0:41 0:42 0:43\nVectors 0:77 0:70 0:70 0:69\nTable 1. The table shows the average n-nearest neighbor\nrank distance of a SOM in relation to a randomly initialized\none. Lower ratios indicate a better neighborhood topology\npreservation. It can be seen that in each conﬁguration the\nGaussian approach produces better mappings.\nbe very expensive to compute. By using random projec-\ntions [2] one can overcome that, but that would probably\ncome with a loss of mapping quality. A SOM computed di-\nrectly with the Gaussians, on the other hand, requires only\na fraction of the computational effort, as the full similar-\nity matrix does not need to be computed and the original\nfeatures are used as intended.\n6. DISCUSSION & FUTURE WORK\nWe have shown how to compute a weighted symmetrized\nKullback-Leibler centroid on multivariate Gaussians and\non top of that how to directly and naturally compute a SOM\nwith Gaussian music similarity features. The SOMs com-\nputed with that approach are shown to produce better map-\npings and omit the so far necessary step of vectorizing the\ndata to compute a SOM.\nThe approach easily ﬁts into the large number of ex-\nisting SOM visualizations using Gaussian music similar-\nity features together with a Kullback-Leibler related diver-\ngence and is of course not limited to music similarity. By\nusing it the quality of the produced SOM should increase\nand the application can scale to larger collections of fea-\ntures.\n331\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Besides computing SOMs we also gave a clear def-\ninition of how to compute the weighted symmetrized\nKullback-Leibler centroid so that it can be re-used to solve\ndifferent problems where the features are also parametric\nGaussians: for example to do a k-means cluster analysis\nin music collections directly with the Gaussian features.\nMaybe the centroid could also be of use to build an index-\ning algorithm for faster nearest neighbor retrieval.\nACKNOWLEDGMENTS\nThis research is supported by the Austrian Research Fund\n(FWF) under grants L511-N15, P21247 and the Vienna\nScience and Technology Fund (WWTF). The Austrian Re-\nsearch Institute for Artiﬁcial Intelligence (OFAI) acknowl-\nedges ﬁnancial support form the Austrian Federal Min-\nistries BMVIT and BMWF.\n7. REFERENCES\n[1] A. Banerjee, S. Merugu, I.S. Dhillon, and J. Ghosh.\nClustering with Bregman divergences. The Journal of\nMachine Learning Research, 6:1705–1749, 2005.\n[2] E. Bingham and H. Mannila. Random projection in di-\nmensionality reduction: applications to image and text\ndata. In Proceedings of the 7th ACM SIGKDD inter-\nnational conference on Knowledge discovery and data\nmining, pages 245–250. ACM New York, NY , USA,\n2001.\n[3] L.M. Bregman. The relaxation method of ﬁnding the\ncommon point of convex sets and its application to\nthe solution of problems in convex programming.\nUSSR Computational Mathematics and Mathematical\nPhysics, 7(3):200–217, 1967.\n[4] J. Stephen Downie. The music information retrieval\nevaluation exchange (2005–2007): A window into mu-\nsic information retrieval research. Acoustical Science\nand Technology, 29(4):247–255, 2008.\n[5] P. Knees, M. Schedl, T. Pohle, and G. Widmer. Ex-\nploring music collections in virtual landscapes. IEEE\nmultimedia, 14(3):46–54, 2007.\n[6] T. Kohonen. Self-Organizing Maps. Springer, 2001.\n[7] S. Leitich and M. Topf. Globe of music: Music li-\nbrary visualization using geosom. In Proceedings of\nthe 8th International Conference on Music Information\nRetrieval (ISMIR07), Vienna, Austria, 2007.\n[8] B. Logan. Mel frequency cepstral coefﬁcients for mu-\nsic modeling. In International Symposium on Music In-\nformation Retrieval, 2000.\n[9] D. L ¨ubbers. SoniXplorer: Combining visualization\nand auralization for content-based exploration of mu-\nsic collections. In Proceedings of the 6th Interna-\ntional Conference on Music Information Retrieval (IS-\nMIR05), 2005.[10] M. Mandel and D. Ellis. Song-level features and sup-\nport vector machines for music classiﬁcation. In Pro-\nceedings of the 6th International Conference on Music\nInformation Retrieval (ISMIR), London, UK, 2005.\n[11] M. Mandel and D. Ellis. Labrosas audio music similar-\nity and classiﬁcation submissions. Music Information\nRetrieval Information Exchange (MIREX), 2007.\n[12] F. M ¨orchen, A. Ultsch, M. N ¨ocker, and C. Stamm.\nDatabionic visualization of music collections accord-\ning to perceptual distance. In Proceedings of the 6th\nInternational Conference on Music Information Re-\ntrieval (ISMIR), London, UK, 2005.\n[13] F. Nielsen and R. Nock. Sided and symmetrized Breg-\nman centroids. IEEE Transactions on Information The-\nory, 55(6):2048–2059, 2009.\n[14] E. Pampalk. Computational models of music similar-\nity and their application in music information retrieval.\nDocteral dissertation, Vienna University of Technol-\nogy, Austria, 2006.\n[15] E. Pampalk, A. Rauber, and D. Merkl. Content-based\norganization and visualization of music archives. In\nProceedings of the 10th ACM international conference\non Multimedia, page 579. ACM, 2002.\n[16] E. Pampalk, A. Rauber, and D. Merkl. Using smoothed\ndata histograms for cluster visualization in self-\norganizing maps. Lecture notes in computer science,\npages 871–876, 2002.\n[17] W.D. Penny. Kullback-Leibler divergences of normal,\ngamma, Dirichlet and Wishart densities. Wellcome De-\npartment of Cognitive Neurology, 2001.\n[18] A. Rauber and M. Fr ¨uhwirth. Automatically analyzing\nand organizing music archives. Lecture Notes in Com-\nputer Science, pages 402–414, 2001.\n[19] R. Veldhuis. The centroid of the symmetrical Kullback-\nLeibler distance. IEEE signal processing letters,\n9(3):96–99, 2002.\n[20] Y . Wu and M. Takatsuka. Spherical self-organizing\nmap using efﬁcient indexed geodesic data structure.\nNeural Networks, 19(6-7):900–910, 2006.\n332\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "Vocalist Gender Recognition in Recorded Popular Music.",
        "author": [
            "Björn W. Schuller",
            "Christoph Kozielski",
            "Felix Weninger",
            "Florian Eyben",
            "Gerhard Rigoll"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1415984",
        "url": "https://doi.org/10.5281/zenodo.1415984",
        "ee": "https://zenodo.org/records/1415984/files/SchullerKWER10.pdf",
        "abstract": "We introduce the task of vocalist gender recognition in popular music and evaluate the benefit of Non-Negative Matrix Factorization based enhancement of melodic com- ponents to this aim. The underlying automatic separation of drum beats is described in detail, and the obtained sig- nificant gain by its use is verified in extensive test-runs on a novel database of 1.5 days of MP3 coded popular songs based on transcriptions of the Karaoke-game UltraStar. As classifiers serve Support Vector Machines and Hidden Naive Bayes. Overall, the suggested methods lead to fully auto- matic recognition of the pre-dominant vocalist gender at 87.31 % accuracy on song level for artists unkown to the system in originally recorded music.",
        "zenodo_id": 1415984,
        "dblp_key": "conf/ismir/SchullerKWER10",
        "keywords": [
            "vocalist gender recognition",
            "Non-Negative Matrix Factorization",
            "melodic components",
            "automatic separation",
            "drum beats",
            "novel database",
            "Karaoke-game UltraStar",
            "Support Vector Machines",
            "Hidden Naive Bayes",
            "87.31% accuracy"
        ],
        "content": "VOCALIST GENDER RECOGNITION IN RECORDED POPULAR MUSIC\nBj¨orn Schuller, Christoph Kozielski, Felix Weninger, Florian Eyben and Gerhard Rigoll\nInstitute for Human-Machine Communication\nTechnische Universit ¨at M ¨unchen\nMunich, Germany\nflastnameg@tum.de\nABSTRACT\nWe introduce the task of vocalist gender recognition in\npopular music and evaluate the beneﬁt of Non-Negative\nMatrix Factorization based enhancement of melodic com-\nponents to this aim. The underlying automatic separation\nof drum beats is described in detail, and the obtained sig-\nniﬁcant gain by its use is veriﬁed in extensive test-runs on\na novel database of 1.5 days of MP3 coded popular songs\nbased on transcriptions of the Karaoke-game UltraStar. As\nclassiﬁers serve Support Vector Machines and Hidden Naive\nBayes. Overall, the suggested methods lead to fully auto-\nmatic recognition of the pre-dominant vocalist gender at\n87.31 % accuracy on song level for artists unkown to the\nsystem in originally recorded music.\n1. INTRODUCTION\nDetermination of the gender of the (main) vocalist(s) is an\nastonishingly untouched task in the ﬁeld of Music Informa-\ntion Retrieval (MIR): while there is a substantial body of\nliterature dealing with gender in spoken language, e. g. to\nimprove automatic speech recognition systems by switch-\ning or adapting acoustic models (e. g. [1]) or accordingly\nto improve emotion recognition systems (e. g. [22]), only\nsome works consider singer identiﬁcation (on artiﬁcial sig-\nnals) [3, 10]. However, explicit recognition of the gender of\nthe main performing vocal artist in original audio record-\nings of e. g. contemporary popular music has apparently not\nbeen addressed in MIR research, yet, which is to overcome,\nas like genre, mood or style, it can be an important feature\nfor organizing and querying music collections, for example\nto ﬁnd a song whose artist’s name is unknown to the user, or\nfor recommendation systems in on-line stores. In addition,\nit might be considered interesting as mid-level attribute for\nother MIR tasks as audio mood classiﬁcation [14] or tran-\nscription of the sung lyrics with gender-adapted models –\nshown to be beneﬁcial in [9].\nApart from ﬁnding suitable features and an appropriate\nclassiﬁcation method, as is the pre-concern for gender iden-\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2010 International Society for Music Information Retrieval.tiﬁcation in (spoken) speech analysis, a setting dealing with\nthe named original audio recordings of music demands for\nreasonable enhancement of the singer(s) voice given the\nbackground ‘noise’ of musical and rhythmic accompani-\nment. It is comparably easy to eliminate the main singer’s\nvoice in stereophonic recordings, e. g. for Karaoke appli-\ncation: often stereophonic channel subtraction killing the\nmid-panned parts sufﬁces, as the lead singer’s voice is usu-\nally panned there to be well audible at any position carrying\nthe main melody (in fact the bass is usually panned there as\nwell, which can be by-passed ﬁrst). However, to separate\nthese vocals is a non-trivial and challenging task – ‘intel-\nligent’, i. e. data-driven, spectral decomposition is usually\nrequired to this end.\nNon-negative Matrix Factorization (NMF) is one of the\nincreasingly popular algorithms used within blind source\nseparation of audio signals. Among other ﬁelds (e. g. speech\nrecognition [18] or non-linguistic vocalisation recognition\n[16]), it has been successfully used in speaker separation\n[12], instrument separation [17, 23], especially drum beat\nseparation [4, 15, 20], and vocal separation [10, 21]. While\nthese methods provide audible results of great quality, it\nis not fully clear to which extent blind source separation\ncan aid in general Music Information Retrieval tasks [15].\nHere, we employ it to separate drum-beats from the rest\nof a popular music piece. While one could directly aim at\nseparation of the vocals, this is considerably more difﬁcult\ngiving the large spectral overlap with other instruments. We\nthus decided to remove the relatively easier separable drum\nand percussion part and recognize gender in combination\nwith general vocal presence in the remaining audio.\nIn this work the recognition system has to identify\nthe gender of the performing artist particularily on ‘real-\nworld’ data, i. e. originally recorded music without any pre-\nselection of ‘friendly cases’, which is a challenging task not\nonly due to the above-named instrumental accompaniment,\nbut also to the variety of genre and singing styles. In our\nexperiments we introduce a database of popular songs from\nthe Karaoke game UltraStar, which includes annotations\nof the tempo and the location of the sung parts. Gender is\nadditionally labelled for the following experiments.\nIn the remainder of this paper, we ﬁrst introduce the\nUltraStar database in section 2, then explain the acoustic\nfeatures and the classiﬁers used for vocalist gender recogni-\ntion in music in section 3, and the methodology applied for\nseparating the drum beat with NMF in section 4. Then, sec-\n613\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)tion 5 presents the data partitioning throughout test-runs and\nthe experimental results before ﬁnally deriving conclusions\nin section 6.\n2. ULTRASTAR DATABASE\nWe ﬁrst introduce a data set of songs with annotations in\nthe ﬁle format of the open-source Karaoke game Ultra-\nStar [2] for vocalist gender evaluation, referred to as Ul-\ntraStar database in the ongoing. This set contains 582\ncomplete songs and is encoded in MP3 (MPEG-1 Audio\nLayer 3) format with 44.1 kHz PCM and variable bit rate\nwith a minimum of 128 kbit/s. In total length, this set corre-\nsponds to 37 h 06 min of music, i. e. 1.5 days of continuous\nmusic. The set covers generations from the 1960s until\ntoday and is a good example of typical popular music from\ndiverse genres like Rock, Pop, Electronic Music, Ballads or\nMusical.\nAs annotation, we use the tempo and the information\non the location of vocal presence in a song. In addition,\nwe carried out a gender annotation on song level for this\ndata set: per song we assigned the gender of the vocalist\nthat is perceived as pre-dominant over the total run-time\nafter listening to each full song. This was done by two\nlabellers individually and in random order without any dis-\nagreement. Overall, 178 songs were labelled as female,\nand 404 songs as male, respectively 11 h 08 min female and\n25 h 58 min male playtime. Prior to the processing all songs\nwere down-mixed to monophonic by non-clipping stereo\nchannel-addition.\nSince every user of the UltraStar Karaoke game has the\npossibility to contribute annotations to the game’s website,\nwe chose songs according to their popularity among users,\nassuming that high popularity of a song indicates that a\nrobust ground truth can be established.\n3. GENDER RECOGNITION\n3.1 Acoustic Features\nFor the actual gender recognition we consider the short-time\nenergy, zero-, and mean-crossing rate known to indicate vo-\ncal presence [24]. In addition we extract values from the nor-\nmalized autocorrelation sequence of the DFT coefﬁcients,\nnamely voicing probability, F-zero, and harmonics-to-noise\nratio (HNR). F-zero is the location of the highest peak of\nthe autocorrelation sequence aside from the maximum at\nzero. HNR is computed by the value of this peak. We fur-\nther calculate Mel frequency cepstral coefﬁcients (MFCC)\n0–13 and their respective ﬁrst-order delta regression co-\nefﬁcients. MFCC are known to capture the characteristic\nqualities of individual voices in speech and music for singer\nidentiﬁcation [8,10,11] and have proven highly meaningful\nin various speech gender recognition tasks [1, 22]. Thus,\naltogether we employ a set of 32 features.\nV ocals in popular music are synchronous to the beats of a\nsong most of the time. For every quarter beat we know from\nthe annotations in the UltraStar database whether sung vo-\ncals are present or not. From this we derived an annotationon beat level by a non-ambiguous majority vote procedure:\nwe judged vocals to be present in a beat if they are present\nin at least two of the quarter beats.\nBased on this, every song in our data set is divided into\nanalysis frames corresponding to the beats of the song. As\nthe tempo and the locations of vocal presence are known for\neach song, beat synchronous chopping is possible for the\ntraining section and test section, to focus on the problem\nat hand. However, using the highly reliable automatic beat-\ntracker as introduced in [13] led to non-signiﬁcant (one-\ntailed test, for testing conditions cf. below) differences in\naccuracy on song level. We divide the signal into non-\noverlapping frames with a Hamming window function of\nthe length of a beat of the particular song – a strategy found\nbeneﬁcial over smaller units in previous tests. Per likewise\nbeat-synchronous frame the above mentioned features are\ncomputed.\nFor easy reproducibility of the results we decided for\nopen-source feature extraction by using the real-time toolkit\nfor ‘Speech and Music Interpretation by Large Space Ex-\ntraction’ (openSMILE)1.\n3.2 Classiﬁers\nWe evaluate Support Vector Machines (SVM) with polyno-\nmial Kernel, sequential minimal optimization learning, and\npairwise multi-class decision, as well as different Bayesian\nclassiﬁers for our gender recognition task to be more inde-\npendent of classiﬁer inﬂuence.\nA Bayesian network in general is a directed graph in\nwhich nodes represent attributes and branches represent\nattribute dependencies. It is quantiﬁed by conditional prob-\nabilities for each node dependent on its parents. In naive\nBayes, each attribute node has the class node as its parent\nonly, without any relation to other attributes, so it is the sim-\nplest Bayesian network [6]. In structure learned Bayesian\nnetworks every attribute node can have other attribute nodes\nas its parents, thus all dependencies between attributes are\nconsidered. However, achieving an optimal structure by\nlearning from data is often impracticable in reasonable time.\nHidden naive Bayes represents attribute dependencies by\ncreating a hidden parent for each attribute node. This par-\nent combines all inﬂuences of other attributes. Although\nattribute dependencies are considered, it keeps the structure\nof naive Bayes and does not need to be structure learnt [25].\nFor Bayes classiﬁcation we found discretization by\nKononenko’s minimal description length criterion [5] based\non the training instances beneﬁcial (signiﬁcant gain in av-\nerage accuracy of 5.54 % on beat level with and without\nenhancement in the experiments as follows, for testing con-\nditions cf. below), and Hidden Naive Bayes (HNB) superior\nto the considered alternatives (signiﬁcant gain as before of\n2.41 % over structure learned Bayesian networks, and of\n7.68 % over Naive Bayes).\n1http://www.openaudio.eu\n614\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)4. DRUM BEAT SEPARATION USING\nNON-NEGATIVE MATRIX FACTORIZATION\n4.1 Deﬁnition of NMF\nGiven a matrix V2Rm\u0002n\n\u00150 and a constant r2N, non-\nnegative matrix factorization (NMF) computes two matrices\nW2Rm\u0002r\n\u00150 andH2Rr\u0002n\n\u00150, such that\nV\u0019W\u0001H (1)\nFor information reduction one generally chooses rsuch\nthat(m+n)r\u001cmn.\n4.2 Application to Blind Source Separation\nAn important application area of NMF in signal processing\nis blind source separation. In the particular ﬁeld of music\nprocessing, NMF has been successfully used to separate\ndrum from harmonic sounds [4, 15, 20].\nNMF-based blind source separation is usually realized\nin the frequency domain. Thereby the signal is split into\noverlapping frames of constant size. In our experiments, a\nframe size of 60 ms and an overlap of 50 % produced best\nresults. Each frame is multiplied by a window function and\ntransformed to the frequency domain using Discrete Fourier\nTransformation (DFT), with transformation size equal to the\nnumber of samples in each frame. We use the square root\nof the Hann function for windowing, as this helps to reduce\nartifacts when transforming back to the time domain [4].\nOnly the magnitudes of the DFT coefﬁcients are retained,\nand the frame spectra are put in the columns of a matrix.\nDenoting the number of frames by nand the frame size by\nT, and considering the symmetry of the coefﬁcients, this\nyields a (bT=2c+ 1)\u0002nreal matrix.\nTo exploit NMF for blind source separation, one assumes\nalinear signal model. Note that Eq. 1 can be written as\nfollows (the subscripts :; tand:; jdenote the tthandjth\nmatrix columns, respectively):\nV:;t\u0019rX\nj=1Hj;tW:;j;1\u0014t\u0014n (2)\nThus, if Vis the magnitude spectrogram of a signal (with\nshort-time spectra in columns), the factorization from Eq. 1\nrepresents each short-time spectrum V:;tas a linear com-\nbination of spectral basis vectors W:;jwith non-negative\ncoefﬁcients Hj;t(1\u0014j\u0014r).\nWe deﬁne the jthcomponent of the signal to be the pair\n(wj;hj)of a spectrum wj:=W:;jalong with its time-\nvarying gains hj:=Hj;:(the subscript j;:denotes the jth\nmatrix row).\nIt has turned out that the non-negativity constraint on the\ncoefﬁcients alone is sufﬁcient to decompose a signal into\nthe underlying sources [17, 20]. Note that a ‘source’ in the\nintuitive sense, e. g. an instrument, can consist of multiple\ncomponents.\nWhen there is no prior knowledge about the number of\nspectra that can describe the source signal, the number of\ncomponents rhas to be chosen empirically. In our experi-\nments, best results were achieved by setting r= 30.4.3 Factorization Algorithm\nA factorization according to Eq. 1 is usually achieved by\niterative minimization of cost functions. For the purpose\nof drum beat separation, an extended form of the Kullback-\nLeibler (KL) divergence has been shown to yield good\nresults [15, 20]:\ncd(W;H) =\nmX\ni=1nX\nt=1\u0012\nVi;tlogVi;t\n(WH) i;t\u0000(V\u0000WH) i;t\u0013(3)\nEq. 3 can be enhanced by a term that enforces temporal\ncontinuity of the gains, improving separation quality [20] at\nthe expense of increased computational costs. Because the\nperceived audio quality of components is of minor relevance\nfor our task, we chose cdfrom Eq. 3 as cost function and\nminimized it using Lee and Seung’s multiplicative update\nalgorithm [7]. It performs the following iterative updates of\nthe matrices WandH:\nHj;t Hj;tPm\ni=1Wi;jVi;t=(WH) itPm\ni=1Wi;j(4)\nforj= 1; : : : ; r ;t= 1; : : : ; n and\nWi;j Wi;jPn\nt=1Hj;tVi;t=(WH) i;tPn\nt=1Hj;t(5)\nfori= 1; : : : ; m ;j= 1; : : : ; r .\nSince in our scenario the spectral characteristics of the\ndrum and harmonic sources in the signal are not known\nbeforehand, we initialize WandHwith random numbers\ndrawn from a uniform distribution on the interval ]0;1].\nTo reduce computational cost, instead of detecting con-\nvergence by computing the cost function (Eq. 3) after each\niteration step, we run the algorithm for 100 iterations, after\nwhich in separation of popular music a reasonable sep-\naration quality is reached and convergence slows down\nconsiderably [15].\n4.4 Synthesis of Harmonic Signals\nOur goal is to obtain a drum-free signal from the NMF rep-\nresentation computed according to the previous section. To\nthis end, we ﬁrst classify the signal components (wj;hj),\n1\u0014j\u0014rinto two classes, ‘drum’ and ‘harmonic’. Note\nthat the gains hjmay also contain valuable features for\ndiscrimination of drum and harmonic sounds, since e. g.\ndrum sounds are expected to be more periodic than har-\nmonic sounds. The exact feature set and parameters used\nfor classiﬁcation will be described in the next section.\nAfter classiﬁcation, we compute a magnitude spectro-\ngramVharmof a signal that contains only harmonic sounds:\nLetJharm=fj: (wj;hj)classiﬁed as harmonicg. Then,\nVharm=X\nj2J harmwjhj (6)\nWe transfer Vharmback to the time domain applying a\ncolumn-wise inverse DFT, using the phase matrix from the\n615\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)# beats training develop test sum\nfemale 39 267 25 354 12 856 77 477\nmale 60 210 55 429 40 805 156 444\nno voice 73 512 64 568 37 447 175 527\nsum 172 989 145 351 91 108 409 448\nTable 1 : Number of beats per set of the UltraStar database.\noriginal signal. Finally, we obtain a ‘harmonic’ time signal\nby windowing each time frame with the square root of the\nHann function, then employing an overlap-add procedure.\n4.5 Discrimination of Drum and Harmonic\nComponents\nFor discrimination of drum and harmonic components, we\nuse a linear SVM classiﬁer with a feature set similar to the\nones proposed in [4] and [15].\nFrom the spectral vectors wj, we compute MFCC, using\n26 triangular ﬁlters on a bank ranging from 20 Hz to 8 kHz.\n10 ﬁrst MFCC plus the zeroth (energy) coefﬁcient are con-\nsidered. Furthermore, we add sample standard deviation\n(using the common unbiased estimator), spectral centroid,\n95 % roll-off point, and noise-likeness [19]. While these\nspectral features are quite common in pattern recognition,\nthe temporal features computed from the gains vectors hj\nare more speciﬁc to the drum beat separation task. In de-\ntail, we use percussiveness [19], periodicity, average peak\nlength, and peak ﬂuctuation [4, 15].\nFor NMF computation and extraction of the named fea-\ntures the open source ‘Blind Source Separation for Audio\nRetrieval Tasks’ (BliSSART)2package is used for repro-\nducibility reasons. To train the SVM classiﬁer, we used a\ndata set generated from a popular music collection which\nis presented in detail in [15]. It consists of 95 drum and\n249 harmonic components computed by NMF on song ex-\ntracts. In 10-fold stratiﬁed cross validation of this data set,\na classiﬁcation accuracy of 96.2 % was achieved.\n5. EXPERIMENTS\n5.1 Data Partitioning\nWe partitioned the UltraStar database introduced in section\n2 into the three groups: training, develop, and test. The\ntraining set (241 songs) contains all artists beginning with\nA,D,G,..., the develop set (207 songs) artists beginning with\nB,E,H,..., and the test set (134 artists) those beginning with\nC,F,I,...,0-9. Note that this dividing setup provides that all\nsongs of one artist are in the same set, thus processing is\nstrictly independent of the artist. See Table 1 for the gender\ndistribution on beat level.\nThe features for the actual gender determination as in-\ntroduced in section 3 were extracted from the original song\nﬁles. In addition we created the same sets with the features\nextracted from the discriminated harmonic segments after\n2http://www.openaudio.eu\n101\nFPRTPR\n  \nw/o NMF: AUC = 0.712\nw/ NMF:   AUC = 0.848(a) SVM, 2-class\nFigure 1 : ROC by true (TPR) over false positive rate\n(FPR), and the area under the curve (AUC) for the two-\n(female / male) class task.\napplying our NMF algorithm and drum beat separation as\nintroduced in section 4 on the original song.\nWe evaluate two different tasks: ﬁrst, three classes (no\nvoice / female / male) to evaluate vocal presence localiza-\ntion in combination with gender recognition; second, two\nclasses (female / male) where we only consider beats with\nvocal presence to judge whether performance is increased\nparticularly in gender discrimination by NMF-based drum\nseparation.\nWe trained with our training set and evaluated on the\ndevelop set to verify feature relevance before and after drum-\nbeat separation. For optimization, we applied random down-\nsampling – i. e. elimination of instances – to the training\nset to achieve a balanced distribution of instances among\nclasses.\n5.2 Results\nWhile training with the training and evaluating with the\ndevelop set, we found that every extracted feature was rele-\nvant, as classiﬁcation performance could not be improved\nby removing features of a certain type. We thus kept the\nfull feature-set as described in section 3 for the oncoming\nevaluations. For our ﬁnal results we next merged the train-\ning and develop sets for classiﬁer training, and evaluated\non the test set for representative performances.\nFirst we consider the results with three classes on beat\nlevel (cf. Table 2, columns ‘beat’): performance is improved\nfor SVM and HNB by NMF-based drum-beat separation,\nand a maximum accuracy of 58.84 % is reached by HNB.\nNext looking at the obtained performances with only two\nclasses (female / male) on beat level, one again notices a\nconsiderable boost by drum-beat separation for all classi-\nﬁers. Again, SVM beneﬁt most, while HNB reaches highest\nlevel at 80.22 % accuracy.\nNext, we shift to the level of the whole song, and identify\nthe gender of the mainly heard vocalist(s): after classiﬁ-\ncation per beat we estimate a song’s overall gender either\nby majority vote or alternatively by adding the prediction\nscores per gender and choosing the maximum overall score.\nTable 2 (columns ‘song’) shows the according results.\nMinor differences – and only for HNB – are observable\nbetween majority vote and the maximum added score. The\n616\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)w/o NMF w/ NMF\nAccuracy [%] classiﬁcation scheme beat song beat song\nHNB vote58.5479.8558.8484.33\n- / f / m HNB added score 79.85 85.07\nSVM vote / added score 52.06 79.85 56.54 87.31\nHNB vote70.3582.0980.2285.82\nf / m HNB added score 83.58 86.57\nSVM vote / added score 67.52 82.09 79.97 89.55\nTable 2 :Accuracies of vocalist gender recognition on beat level and on song level by majority voting or maximum added\nscore for HNB and SVM as classiﬁer – once with (w/) and once without (w/o) separation of drum-beats by NMF . Considered\nare no voice (-), female (f), and male (m) discrimination on all beats or only gender on those with vocal presence.\n101\nFPRTPR\n  \nw/o NMF: AUC = 0.715\nw/ NMF:   AUC = 0.680\n(a) no voice (SVM, 3-class)\n101\nFPRTPR\n  \nw/o NMF: AUC = 0.713\nw/ NMF:   AUC = 0.816 (b) female (SVM, 3-class)\n101\nFPRTPR\n  \nw/o NMF: AUC = 0.689\nw/ NMF:   AUC = 0.734 (c) male (SVM, 3-class)\nFigure 2 : ROC by true (TPR) over false positive rate (FPR), and the area under the curve (AUC) for the three-class\n(female / male / no voice) task.\nlatter is found beneﬁcial – as one would assume – as in-\nformation on certainty is preserved prior to the song level\ndecision. The results further indicate that the accuracy of\nclassiﬁcation on beat level is sufﬁciently above chance level\nto allow for repairing of mispredictions over the duration of\na song. Here, SVM perform slightly better with a maximum\nof 87.31 % accuracy for the three classes, and the difference\nto the two-class task is drastically reduced. Overall, the\nstatistical signiﬁcance on song level for the improvement\ngained by NMF utilization is 0.05. Thus, we can state that\ndrum separation by NMF helps recognizing gender even on\nthe song level.\nTo shed light on this effect per class, according Receiver\nOperating Characteristics (ROC) are depicted in Figure 1\nfor the two-class task of gender recognition, and in Figure\n2 for the three-class task with additional determination of\npositions that contain vocals by SVM. To provide a single\nvalue rather than a curve, one can calculate the area under\nthe ROC curve, called AUC. The highest possible AUC is\n1.0, equal to the whole graph area, and achievable only by\na perfect classiﬁer. Random guessing has an AUC of 0.5\nsince it corresponds to the diagonal line in the ROC space.\nA reasonable classiﬁer should therefore have an AUC that is\nsigniﬁcantly greater than 0.5, with better classiﬁers yielding\nhigher values. The values obtained are also shown in Figure\n1: For the two-class task (female / male) the difference in\nthe AUC with and without NMF is highly signiﬁcant at the\n10\u00003level. In the three-class problem clear differences are\nobservable: the highest beneﬁt is reached for female vocals,next come male vocals, and interestingly the recognition\nof parts without vocal presence is negatively affected by\nreduction of the drum beat presence.\n6. CONCLUSION\nIt was our primary goal to predict the vocalist gender in\noriginally recorded popular music, and our secondary to\nanalyze whether NMF usage for separation of the drum-\nbeat can help improve on this task. The results clearly\ndemonstrate the signiﬁcant improvement obtained, and we\nare by that able to fully automatically identify the gender of\nthe main vocalist in popular music at a high and reasonable\naccuracy for system unknown artists and songs. On beat\nlevel NMF application slightly impairs vocals presence\nestimation, but increases the overall performance of gender\nclassiﬁcation explaining the better results on song level.\nConsidering the choice of classiﬁer, no clear tendency\nwas found, apart from the fact that the overall best result\nwas obtained by SVM.\nFuture reﬁnement can be invested in improved annota-\ntion: as mentioned, the UltraStar annotations were created\nby members of the game community. Therefore errors\namong the ground truth tempo and vocals’ locations might\nbe present though we chose the most frequently used ﬁles.\nA widespread veriﬁcation of the annotations would mini-\nmize the error rate and maybe reduce false classiﬁcations.\nBut that would need a huge investment of time.\nFurther, we assigned the main vocalist gender. How-\n617\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)ever, alternatively local labeling and consideration of mixed\ngender or choir passages could be provided.\nFinally and self-evident, tailored vocal instead of drum\nseparation should be targeted now that the more robustly\nobtainable separation of drum-beats was already found sig-\nniﬁcantly beneﬁcial.\n7. ACKNOWLEDGMENT\nThis work was supported by the Federal Republic of Ger-\nmany through the German Research Foundation (DFG)\nunder the grant no. SCHU 2508/2-1 (“Non-Negative Ma-\ntrix Factorization for Robust Feature Extraction in Speech\nProcessing”).\n8. REFERENCES\n[1]W. H. Abdulla and N. K. Kasabov. Improving speech\nrecognition performance through gender separation.\nInProc. of ANNES, pages 218–222, Dunedin, New\nZealand, 2001.\n[2]P. Cebula. UltraStar - PC conversion of famous karaoke\ngame SingStar, 2009.\n[3]H. Fujihara, T. Kitahara, M. Goto, K. Komatani,\nT. Ogata, and H. G. Okuno. Singer identiﬁcation based\non accompaniment sound reduction and reliable frame\nselection. In Proc. of ISMIR, pages 329–336, 2005.\n[4]M. Helen and T. Virtanen. Separation of drums from\npolyphonic music using non-negative matrix factoriza-\ntion and support vector machine. In Proc. of EUSIPCO,\nAntalya, Turkey, 2005.\n[5]I. Kononenko. On biases in estimating multi-valued at-\ntributes. In Proc. of IJCAI, pages 1034–1040, Montreal,\nQuebec, Canada, 1995.\n[6]P. Langley, W. Iba, and K. Thompson. An analysis of\nBayesian classiﬁers. In Proc. of AAAI, pages 223–228,\nSan Jose, CA, USA, 1992.\n[7]D. D. Lee and H. S. Seung. Algorithms for non-negative\nmatrix factorization. In Proc. of NIPS , pages 556–562,\nVancouver, Canada, 2001.\n[8]Beth Logan. Mel frequency cepstral coefﬁcients for\nmusic modeling. In Proc. of MUSIC-IR, Plymouth, MA,\nUSA, 2000.\n[9]A. Mesaros and T. Virtanen. Automatic recognition of\nlyrics in singing. EURASIP Journal on Audio, Speech,\nand Music Processing, 2009:24 pages, 2009.\n[10] A. Mesaros, T. Virtanen, and A. Klapuri. Singer identi-\nﬁcation in polyphonic music using vocal separation and\npattern recognition methods. In Proc. of ISMIR, pages\n375–378, 2007.\n[11] D. A. Reynolds. Experimental evaluation of features\nfor robust speaker identiﬁcation. IEEE Transactions on\nSpeech and Audio Processing, 2(4):639–643, 1994.[12] M. N. Schmidt and R. K. Olsson. Single-channel speech\nseparation using sparse non- negative matrix factoriza-\ntion. In Proc. of Interspeech, Pittsburgh, Pennsylvania,\nUSA, 2006.\n[13] B. Schuller, F. Eyben, and G. Rigoll. Tango or Waltz?:\nPutting ballroom dance style into tempo detection.\nEURASIP Journal on Audio, Speech, and Music Pro-\ncessing, 2008(846135):12 pages, 2008.\n[14] B. Schuller, C. Hage, D. Schuller, and G. Rigoll. ’Mister\nD.J., Cheer Me Up!’: Musical and Textual Features for\nAutomatic Mood Classiﬁcation. Journal of New Music\nResearch, 38:33 pages, 2009.\n[15] B. Schuller, A. Lehmann, F. Weninger, F. Eyben, and\nG. Rigoll. Blind enhancement of the rhythmic and har-\nmonic sections by NMF: Does it help? In Proc. of Inter-\nnational Conference on Acoustics (NAG/DAGA 2009),\npages 361–364, Rotterdam, The Netherlands, 2009.\n[16] B. Schuller and F. Weninger. Discrimination of speech\nand non-linguistic vocalizations by non-negative matrix\nfactorization. In Proc. of ICASSP, Dallas, TX, 2010.\n[17] P. Smaragdis and J. C. Brown. Non-negative matrix fac-\ntorization for polyphonic music transcription. In Proc.\nof WASPAA, pages 177–180, 2003.\n[18] V . Stouten, K. Demuynck, and H. van Hamme. Dis-\ncovering phone patterns in spoken utterances by non-\nnegative matrix factorization. IEEE Signal Processing\nLetters, 15:131–134, 2008.\n[19] C. Uhle, C. Dittmar, and T. Sporer. Extraction of drum\ntracks from polyphonic music using independent sub-\nspace analysis. In Proc. of ICA, Nara, Japan, 2003.\n[20] T. Virtanen. Monaural sound source separation by non-\nnegative matrix factorization with temporal continuity\nand sparseness criteria. IEEE Transactions on Audio,\nSpeech and Language Processing, 15(3):1066–1074,\nMarch 2007.\n[21] T. Virtanen, A. Mesaros, and M. Ryyn ¨anen. Combining\npitch-based inference and non-negative spectrogram fac-\ntorization in separating vocals from polyphonic music.\nInProc. of SAPA, Brisbane, Australia, 2008.\n[22] T. V ogt and E. Andre. Improving automatic emotion\nrecognition from speech via gender differentiation. In\nProc. of LREC, Genoa, Italy, 2006.\n[23] B. Wang and M. D. Plumbley. Musical audio stream sep-\naration by non-negative matrix factorization. In Proc. of\nDMRN Summer Conference, Glasgow, Scotland, 2005.\n[24] C. Xu, N. C. Maddage, and X. Shao. Automatic music\nclassiﬁcation and summarization. IEEE Transactions\non Speech and Audio Processing, 13(3):441–450, 2005.\n[25] H. Zhang, L. Jiang, and J. Su. Hidden naive Bayes. In\nProc. of AAAI, pages 919–924, Pittsburgh, PA, USA,\n2005.\n618\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "An Audio Processing Library for MIR Application Development in Flash.",
        "author": [
            "Jeffrey J. Scott",
            "Raymond Migneco",
            "Brandon G. Morton",
            "Christian M. Hahn",
            "Paul J. Diefenbach",
            "Youngmoo E. Kim"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1414740",
        "url": "https://doi.org/10.5281/zenodo.1414740",
        "ee": "https://zenodo.org/records/1414740/files/ScottMMHDK10.pdf",
        "abstract": "In recent years, the Adobe Flash platform has risen as a credible and universal platform for rapid development and deployment of interactive web-based applications. It is also the accepted standard for delivery of streaming me- dia, and many web applications related to music informa- tion retrieval, such as Pandora, Last.fm and Musicovery, are built using Flash. The limitations of Flash, however, have made it difficult for music-IR researchers and de- velopers to utilize complex sound and music signal pro- cessing within their web applications. Furthermore, the real-time audio processing and synchronization required for some music-IR-related activities demands significant computational power and specialized audio algorithms, far beyond what is possible to implement using Flash script- ing. By taking advantage of features recently added to the platform, including dynamic audio control and C cross- compilation for near-native performance, we have devel- oped the Audio-processing Library for Flash (ALF), pro- viding developers with a library of common audio pro- cessing routines and affording Flash developers a degree of sound interaction previously unavailable through web- based platforms. We present several music-IR-driven ap- plications that incorporate ALF to demonstrate its utility.",
        "zenodo_id": 1414740,
        "dblp_key": "conf/ismir/ScottMMHDK10",
        "keywords": [
            "Adobe Flash platform",
            "rapid development",
            "universal platform",
            "interactive web-based applications",
            "streaming media",
            "music information retrieval",
            "complex sound processing",
            "real-time audio processing",
            "Flash limitations",
            "Audio-processing Library for Flash (ALF)"
        ],
        "content": "AN AUDIO PROCESSING LIBRARY FOR MIR APPLICATION\nDEVELOPMENT IN FLASH\nJeffrey Scotty, Raymond Mignecoy, Brandon Mortony;Christian M. Hahnz\nPaul Diefenbachz, Youngmoo E. Kimy\nElectrical and Computer Engineering, Drexel Universityy\nMedia Arts and Design, Drexel Universityz\nfjjscott, rmigneco, bmorton, cmhahn, pjdief, ykim g@drexel.edu\nABSTRACT\nIn recent years, the Adobe Flash platform has risen as a\ncredible and universal platform for rapid development and\ndeployment of interactive web-based applications. It is\nalso the accepted standard for delivery of streaming me-\ndia, and many web applications related to music informa-\ntion retrieval, such as Pandora, Last.fm and Musicovery,\nare built using Flash. The limitations of Flash, however,\nhave made it difﬁcult for music-IR researchers and de-\nvelopers to utilize complex sound and music signal pro-\ncessing within their web applications. Furthermore, the\nreal-time audio processing and synchronization required\nfor some music-IR-related activities demands signiﬁcant\ncomputational power and specialized audio algorithms, far\nbeyond what is possible to implement using Flash script-\ning. By taking advantage of features recently added to the\nplatform, including dynamic audio control and C cross-\ncompilation for near-native performance, we have devel-\noped the Audio-processing Library for Flash (ALF), pro-\nviding developers with a library of common audio pro-\ncessing routines and affording Flash developers a degree\nof sound interaction previously unavailable through web-\nbased platforms. We present several music-IR-driven ap-\nplications that incorporate ALF to demonstrate its utility.\n1. INTRODUCTION\nThe use of web applications is now commonplace due to\nthe widespread availability of broadband connections, im-\nproved client processing power, and the capabilities af-\nforded by Adobe Flash. Flash has become the dominant\nplatform for the development of web-based interactive me-\ndia applications by providing tools for easily implementing\nrich graphics, animation and user interface controls as well\nas cross-platform deployment. Despite its popularity, how-\never, Flash’s support for sound and music processing has\nhistorically been limited. ActionScript, Flash’s native de-\nvelopment language, was never intended to accommodate\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2010 International Society for Music Information Retrieval.computationally intensive algorithms, such as the signal\nprocessing required for real-time audio feature extraction\nand analysis.\nRecognizing the potential for developing audio- and\nmusic-centric applications on the web, we have developed\ntheAudio processing Library for Flash (ALF), which ad-\ndresses the audio limitations of the Flash platform. ALF\nis based on Flash version 10 and capitalizes on the the re-\ncently introduced Adobe Alchemy framework, which al-\nlows existing algorithms written in C/C++ to be compiled\ninto byte code optimized for the ActionScript Virtual Ma-\nchine for signiﬁcantly improved performance [1, 2]. By\nutilizing the dynamic audio capabilities recently added to\nFlash 10 and the computational beneﬁts of Alchemy, ALF\nprovides Flash developers with a library of common au-\ndio processing routines that can be incorporated into ap-\nplications, such as spectral feature extraction and analysis,\nﬁltering and reverberation.\nBy including real-time audio processing capabilities to\nFlash, ALF provides web applications with an additional\ndegree of sound interaction that has previously only been\navailable on native PC platforms. For example, ALF is\ncapable of supporting music-based games in Flash requir-\ning responses from the player precisely timed to music.\nAlthough ALF can be used to enhance the audio of any\nFlash application, our goal is to enable a new form of web\napps that can be driven by user-supplied audio. This poten-\ntially allows a user to choose a wide range of customized\nmusical inputs, such as selections from their personal col-\nlection or completely user-generated music content (song\nremixes and mashups, which are becoming increasingly\ncommonplace). As we will demonstrate, ALF facilitates\nthe development of games that are dynamically driven by\nthe acoustic features of songs from a user’s music library,\nthus creating unique game play experiences depending on\nthe provided audio content.\n2. RELATED WORK\nThere are many software packages available that provide\nlibraries for feature extraction and audio synthesis that\nexist as open source projects for research and develop-\nment. While many provide similar functionality, each li-\nbrary was developed to address particular implementation\nissues, such as cross-platform support, computational ef-\n643\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)ﬁciency and ease of implementation. In this section, we\nprovide a brief description of some existing libraries.\nMarsyas (Music Analysis, Retrieval and Synthesis for\nAudio Signals) is an audio processing and MIR framework\nbuilt in C++ with a GUI based on Qt4 [3]. The project in-\ncludes a wide variety of functions for analysis and synthe-\nsis as well as audio features and classiﬁcation algorithms.\nBeing one of the ﬁrst such projects, the scope of Marsyas is\nsigniﬁcant and it has been used in many research projects\nas well as commercial endeavors.\njAudio was developed to be an easy to use Java-based\nsystem for feature extraction. The cross-platform nature\nof Java and GUI tools were the motivating factors for the\nchoice of development language. The creators attempted to\nmake the system as easily extensible as possible, avoid re-\ndundant computation, and ensure the algorithms were sep-\narate from other functionality to increase ease of portabil-\nity [4].\nM2K is a project under the International Music Infor-\nmation Retrieval System Evaluation Laboratory which is\nbased off of the Data to Knowledge (D2K) machine learn-\ning and data mining environment [5]. D2K is employs\na visual programming environment in which users con-\nnect modules together to prototype algorithms. The M2K\nproject has taken this framework and built in an array of\nMIR tools for rapid development and testing of MIR sys-\ntems.\nTheMIRToolbox is an audio feature extraction library\nbuilt in MATLAB that emphasizes a modular, parameter-\nizable framework [6]. The project offers a wide range of\nlow-level and high-level features as well as tools for statis-\ntical analysis, segmentation and clustering.\nCLAM is an analysis/synthesis system written in C++\ndesigned to be entirely object-oriented to allow for signif-\nicant re-usability of code and functionality [7]. It provides\naudio and MIDI input/output, supports XML and provides\ntools for data visualization.\nFEAPI is a platform-independent programming appli-\ncation interface for low-level feature extraction written in\nC [8]. In contrast to the previously described systems,\nFEAPI allows developers to create their own applications\nusing C/C++ without being required to use the interfaces\ndesigned to work with the above libraries.\n3. IMPLEMENTATION\nThe driving force behind the development of ALF was to\nprovide developers with an efﬁcient, cross-platform and\nopen source MIR and audio synthesis library. By choosing\nFlash as the development platform, we target developers\nseeking to rapidly develop and deploy web-based and/or\ncross-platform desktop applications. As we will discuss,\nthe multi-layered and open source architecture of ALF also\npermits ease of development for programmers with various\nexpertise and does not require prior knowledge or experi-\nence in audio programming.\nWrite hopSize samples to input buﬀer copy samples to ﬀtFrameDSP/Feature Function CallsSynthesis?YesPerform ProcessinghopSizehopSizeWrite hopSize samples to input buﬀer \nOutput AudioNoAnalysis Computation SchemeCompute magnitude spectrumSpectral FeaturesMFCCsChromaSynthesis Computation SchemeDetermine if alternate frame rate/size is neededCheck if needed FFT size is already computedExecute synthesis algorithmhopSizeWrite hopSize samples to input buﬀer Figure 1. Frame-based computation and processing ﬂow\nin ALF.\n3.1 Architecture\nThe dynamic audio functionality in the current version of\nFlash is somewhat asynchronous, allowing sound to be\nprocessed outside of the main application thread. Thus the\nDSP routines can execute independently of the Flash script\nrather than having to wait for C/C++ functions to ﬁnish\nexecuting, allowing front-end UI and other operations to\ncontinue if they are not dependent on data computed using\nALF functions.\nThere are several layers of abstraction in ALF provid-\ning a ﬂexible framework with various levels of control de-\npending on the needs of the developer. The heavy compu-\ntation is executed by the C/C++ functions which are com-\npiled using the Adobe supplied Alchemy compiler for use\nwith ActionScript (AS). We provide a basic AS wrapper to\nproperly handle the shared memory management between\nC/C++ and ActionScript for those wishing to have basic\naccess to the C functionality. The top layer streamlines\naudio input/output and provides simple calls to perform\nfeature extraction and analysis-synthesis tasks. The entire\nproject is open source so that a developer may customize\nthe architecture to meet application-speciﬁc needs. ALF is\nfully documented and is currently available via a subver-\nsion repository online1.\nTo ensure tight synchrony between the video and au-\ndio output in Flash, the processing ﬂow was developed ac-\ncording to the diagram shown in Figure 1. The frame size\nis set by the the video frame rate since ALF is designed\nwith graphical oriented applications in mind, thus the time-\nfrequency resolution of the system is also determined by\nthis parameter. Whenever possible, a single FFT is used in\ncomputing the features returned to the user, however, cer-\ntain algorithms require transforms of sizes other than the\n1http://music.ece.drexel.edu/ALF\n644\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Table 1. ALF Functions\nFunction Name Description\nSpectrum Computes the magnitude spectrum using the FFT algorithm\nHarmonics Finds the harmonics of the frequency spectrum\nMFCC Calculates the Mel-Frequency Cepstral Coefﬁcients\nLPC Performs Linear Prediction and returns the coefﬁcients and gain\nBandwidth The frequency range present in the signal\nCentroid The center of gravity of the frequency spectrum\nAnalysis Flux The change in energy from the previous frame\nIntensity Calculates the energy of the spectrum\nRolloff The frequency below which %85 of the spectral energy lies\nAutocorrelation Computes the autocorrelation via the FFT\nChroma An representation of the spectral energy present in the 12 individual semitones\nBeat Tracking Returns whether a beat occurs on each frame (based on bandwise autocorrelation)\nFilter Filters the audio signal - FIR and IIR implementation\nSynthesis Reverb Applies reverb by using a room impulse response (RIR) as an FIR ﬁlter\nPhase V ocoder Alters the tempo and/or pitch of the audio\ndefault size. A shared buffer system is also used so that we\ncan perform operations at variable frame rates and over-\nlap lengths without having to read in the data again using\ndifferent frame sizes.\n3.2 Performance\nAs previously mentioned, the computationally inten-\nsive routines in ALF are implemented in the Alchemy-\noptimized C code to avoid the limitations of ActionScript.\nWhile slightly slower than native C code, the Alchemy-\noptimized code provides signiﬁcant performance gains\nover identical algorithms implemented with ActionScript.\nIn a related paper, we performed a benchmark analysis\nof the FFT algorithm using the ActionScript as3mathlib\nimplementation versus our Alchemy-compiled C imple-\nmentation as well as Java’s JTransforms. Averaging the\ncomputation speed over 10,000 iterations, we showed our\nimplementation to be nearly 30 times faster than the Ac-\ntionScript version [1]. The results of this performance\ncomparison are outlined in Table 2. These computational\ngains open up myriad possibilities for developing interac-\ntive music-IR driven applications in the Flash framework.\nTable 2. Comparison of FFT Computation Time for Ac-\ntionScript and Alchemy-compiled C code in milliseconds.\nTar\nget FFT Size\nPlatform 8192 4096\n2048 1024 512 256\nActionScript 45.157 20.818\n9.276 4.460 2.041 0.925\nJav\na 20.703 9.393\n4.345 1.956 0.901 0.385\nAlchemy-C 1.371 0.628\n0.297 0.139 0.067 0.034\n3.3 ALF Functions\nThe functions available in ALF are categorized as either\n“analysis” or “synthesis” and are outlined in Table 1. The\nanalysis functions include several spectral processing rou-\ntines and features, such as partial extraction and MFCCs,\nthat are useful in many MIR tasks [9]. Synthesis functionsare also available so that the developer can dynamically\nmodify the audio output stream to achieve a desired effect.\nIn a related paper, we discuss the implementation and al-\ngorithms used for the reverb and ﬁlter functions [2]. The\nremainder of this section will brieﬂy discuss the implemen-\ntation of two additional synthesis functions added to ALF:\nphase vocoding and beat tracking.\nThe most important consideration in developing the\nbeat tracking algorithm was the stipulation that it run in\nreal-time. Our beat tracking algorithm is based off of\nthat proposed by Klapuri but uses an autocorrelation as\nopposed to a bank of comb ﬁlters for computational ef-\nﬁciency [10]. We ﬁrst compute the power spectrum and\nseparate it into six octave-based sub-bands. The energy\nenvelope in each sub-band is calculated and the bandwise\nautocorrelation of these vectors is computed. Summing the\nresulting six autocorrelations and ﬁnding the highest peak\nafter the zeroth lag yields an estimate of the tempo.\nThe phase vocoder is based on a popular, FFT-based\nimplementation in which overlapping frames (speciﬁed by\nALF’s frame rate) are analyzed and re-synthesized using\noverlap-add in order to perform pitch and/or time-scale\nmodiﬁcation in real-time [11]. Each frame is processed\nby a FFT, which is used to determine the phase offset for\neach frequency bin and thus the estimated, true bin fre-\nquency. Pitch modiﬁcation is achieved by multiplying the\nbin frequencies by a pitch shift factor, which shifts the\naudio’s pitch in the desired manner after performing the\nIFFT. Time stretching is achieved by ﬁrst applying the ap-\npropriate pitch-shift factor, performing an IFFT and and\nre-sampling the audio frame in the time domain to achieve\nthe desired speed.\n4. DEVELOPING WITH ALF\nMany of the applications developed with ALF thus far have\nfollowed the same basic program structure, which is de-\ntailed in Figure 3. Input audio is analyzed on a per frame\nbasis and feature values are returned in real-time for the\n645\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)developer to incorporate into their application. Any addi-\ntional processing required for synthesis functions is exe-\ncuted in a separate processing chain, which eliminates any\ncomputational overhead when synthesis functions are not\nrequired.\nFigure 2. Application demonstrating ALF functionality.\nThe ﬂexible nature of the architecture shown in Figure\n3 combined with the low learning curve of Flash allows\ndevelopers to rapidly create audio and music-based appli-\ncations to serve a variety of target audiences and purposes.\nPossible applications include:\n\u000fMusic-centric games requiring real-time feature ex-\ntraction to drive the game environment\n\u000fMusic exploration interfaces that group user li-\nbraries into categories (emotional, genre, etc.) based\non extraction and comparison of audio features\n\u000fEducational activities for enhancing K-12 curricula\nin natural science and/or mathematics [12]\nCurrently, we have several applications developed using\nALF for the purpose of audio-based experimentation, anal-\nysis/synthesis and music-driven games for entertainment,\nwhich we will discuss in the subsequent sections.\n4.1 ALF Workbench\nFigure 2 demonstrates the ALF Workbench, which allows\ndevelopers to interactively experiment with different au-\ndio ﬁles and some of the functions available in ALF. The\nleft panel of the interface showcases the spectral features,\nwhich are updated during audio playback and can be ex-\nported in a CSV ﬁle when the ﬁle completes. A pitch wheel\nis also shown, which allows the user to determine the chro-\nmatic notes present in the spectrum of tonal audio. The\nright panel of the work bench features the room reverb and\nphase vocoding functions. The reverb interface allows the\nuser to manipulate the positions of the source and listener\nin a virtual room to simulate immersive environments.\nFigure 4. Sound analysis-synthesis app showing linear\npredictive analysis and magnitude spectrum of speech.\n4.2 Beat-Sync-Mash-Coder\nRecently, so-called artist “mashups”, blending two or\nmore songs in a creative way, have emerged as a popular\nform of expression for musicians and hobbyists. To this\nend, the Beat-Sync-Mash-Coder is a tool developed for\nsemi-automated, real-time creation of beat-synchronous\nmashups [13]. This application utilizes the beat-tracking\nand phase vocoding functions available in ALF along with\nan intuitive, Flash-based GUI to help automate the task\nof synchronizing various clips without the complexities\nincurred with traditional digital audio workstations. The\nBeat-Sync-Mash-Coder is capable of sustaining real-time\nphase vocoding on 5-9 audio tracks, depending on the\navailable hardware, thus allowing the user to create dy-\nnamic, intricate and musically coherent soundscapes.\n4.3 Sound Analysis and Synthesis Application\nThe application depicted in Figure 4 uses ALF’s analy-\nsis and synthesis capabilities to perform linear-predictive\nanalysis on speech signals in order to re-synthesize it using\ndifferent excitation signals. Linear prediction coefﬁcients\nare extracted at each frame using the Levinson-Durbin re-\ncursion to obtain a time-varying model of the vocal tract\n[14]. The user can then simulate the effect of various exci-\ntation sources by using ALF’s ﬁltering function to sample\nthe vocal tract with impulsive, noisy or mixed-spectra sig-\nnals.\n4.4 Applications For Music-Driven Gameplay\nWe present two novel music-driven games which resulted\nfrom a collaboration between departments at our univer-\nsity. Both games harness MIR functionality in ALF to cre-\nate unique and immersive gameplay experiences.\n4.4.1 Pulse\nPulse is a musically reactive, side-scrolling platform game\nthat utilizes a player’s personal music collection to drive\nthe gameplay. Unlike other music games, which rely on\n646\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Developer ApplicationMusic LibraryActionScriptALF Playlist\nVisual OutputAudio OutputTrack 1Track 2Render Video FrameExtract Audio FrameAdditional TracksParameter MappingSpectral AnalysisFeature ExtractionAudio Processing ReverbPhase VocodingAudio AnalysisFigure 3. Typical implementation of an application using ALF.\noff-line audio analysis to determine the gaming environ-\nment, Pulse utilizes ALF functionality to update the game\nenvironment in real-time, mapping the quantitative fea-\ntures extracted from the audio to changes in the game’s\nenvironment variables. This concept increases the replay\nvalue of Pulse, since the gamer’s experience is limited only\nby the number of tracks in their music library.\nBy employing ALF’s frame-based processing structure,\nALF maps features extracted from the user-selected au-\ndio to environment parameters so they are updated in sync\nwith the user-speciﬁed frame rate. To permit ample render-\ning time for the graphics, a “frame-look-ahead” parame-\nter is speciﬁed which delays audio playback while features\nare accumulated from ALF functions. Game environment\nvariables that react to changes in the game’s audio include\nthe background scenery, enemies and obstacles of the Pulse\ncharacter as well as the slope of platform supporting the\ncharacter. The effect of the audio on the gameplay is ev-\nident in Figure 5 where (a) shows the game screen when\nthere is no audio playing and (b) is typical realization of\nthe parameter mapping to game output.\n4.4.2 Surge\nThe concept behind Surge is to facilitate exploration of\none’s own music library though an interactive, DJ-style\nbeat matching game. This expands the concept of audio\nfeature-based gaming environments to include tempo anal-\nysis and modiﬁcation of the game’s music. Whereas game-\nplay in Pulse depends on audio features to dictate the envi-\nronment, Surge uses game environment parameters to alter\nthe audio in real-time.\nThe Surge game environment, shown in Figure 6, con-\nsists of planets that represent songs the player has provided\nfrom their music library. Each song is analyzed with ALF’s\nbeat tracker function so that the planet is associated with a\nsong tempo. The game audio depends on which planet the\nplayer is on and their proximity to nearby planets. As the\nplayer nears a new planet, they will hear the music asso-\nciated with the new planet. In order to move from planet-\n(a)\n(b)\nFigure 5. Pulse game environment during static (a) and\ndynamic (b) moments in the game’s music.\nto-planet, the player (by moving their character) must ad-\njust the rotation of their current planet (altering tempo and\nbeats of the song) to match that of the target planet. That\nis, the music tempo is adjusted using ALF’s phase vocoder\naccording to the planet’s rotation, which is dependent upon\nthe player’s actions in the game environment.\n5. FUTURE WORK\nThere are several features we would still like to add to ALF\nincluding spectral contrast features and other less com-\n647\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Figure 6. Surge game environment.\nmonly used statistical spectrum descriptors. The most sig-\nniﬁcant component that would augment the usefulness of\nALF for the music-IR community would be classiﬁcation.\nThere are many open source classiﬁcation libraries avail-\nable to perform common classiﬁcation methods such as\nGMM, SVM, and naive Bayes classiﬁcation that can be\nintegrated into the current framework.\nWe will continue to emphasize the real-time capabili-\nties of ALF and optimize the algorithms and architecture\nto ensure additional algorithms operate in real-time. The\nnewest version of the Flash Player (10.1) will allow byte\nlevel access to the audio input creating potential for even\nfurther user interaction via real-time analysis and process-\ning of voice/music input to a microphone or other audio\ndevice connected to a computer.\n6. CONCLUSIONS\nThe Audio processing Library for Flash affords music-IR\nresearchers the opportunity to generate rich, interactive,\nreal-time music-IR driven applications. The various lev-\nels of complexity and control as well as the capability to\nexecute analysis and synthesis simultaneously provide a\nmeans to generate unique programs that integrate content\nbased retrieval of audio features. We have demonstrated\nthe versatility and usefulness of ALF through the variety\nof applications described in this paper. As interest in mu-\nsic driven applications intensiﬁes, it is our goal to enable\nthe community of developers and researchers in music-IR\nand related ﬁelds to generate interactive web-based media.\n7. REFERENCES\n[1] T. M. Doll, R. Migneco, J. J. Scott, and Y . Kim, “An\naudio DSP toolkit for rapid application development in\nﬂash,” in IEEE International Workshop on Multimedia\nSignal Processing, 2009.\n[2] R. Migneco, T. Doll, J. Scott, C. Hahn, P. Diefenbach,\nand Y . Kim, “An audio processing library for game de-\nvelopment in Flash,” in Proc. of the IEEE Games Inno-\nvations Conference (ICE-GIC 2009), Aug. 2009, pp.\n201 –209.\n[3] G. Tzanetakis and K. Lemstrom, “Marsyas-0.2: A casestudy in implementing music information retrieval sys-\ntems,” in Intelligent Music Information Systems: Tools\nand Methodologies. Information Science Reference,\n2008, pp. 31–49.\n[4] D. McEnnis, C. McKay, I. Fujinaga, and P. Depalle,\n“jAudio: A feature extraction library,” in Proc. of the\n6th International Conference on Music Information\nRetrieval. London, U.K.: ISMIR, 2005.\n[5] J. S. Downie, A. F. Ehmann, and X. Hu, “Music-to-\nknowledge (M2K): a prototyping and evaluation envi-\nronment for music digital library research,” in Proc. of\nthe 5th ACM/IEEE-CS Joint Conf. on Digital Libraries.\nNew York, NY , USA: ACM, 2005, pp. 376–376.\n[6] O. Lartillot, P. Toiviainen, and T. Eerola, A Matlab\nToolbox for Music Information Retrieval., ser. Studies\nin Classiﬁcation, Data Analysis, and Knowledge Orga-\nnization. Springer, 2007, pp. 261–268.\n[7] X. Amatriain, M. De Boer, and E. Robledo, “CLAM:\nAn OO framework for developing audio and music ap-\nplications,” in Proc. of the 17th Annual Conference on\nObject-Oriented Programming, Systems, Languages\nand Applications, 2002.\n[8] A. Lerch, G. Eisenberg, and K. Tanghe, “FEAPI: A low\nlevel feature extraction plugin api,” in In Proc. of 8th\nInt. Conference on Digital Audio Effects (DaFX ’05),\n2005.\n[9] G. Tzanetakis and P. Cook, “Musical genre classiﬁca-\ntion of audio signals,” IEEE Transactions on Speech\nand Audio Processing, vol. 10, no. 5, pp. 293–302,\n2002.\n[10] A. P. Klapuri, A. J. Eronen, and J. T. Astola, “Analy-\nsis of the meter of acoustic musical signals,” in IEEE\nTransactions Speech and Audio Processing, 2004, pp.\n342–355.\n[11] M. Dolson, “The phase vocoder: A tutorial,” in Com-\nputer Music Journal, vol. 10, no. 4. MIT Press, 1986,\npp. 14–27.\n[12] T. M. Doll, R. V . Migneco, and Y . E. Kim, “Online\nactivities for music informatioin and acoustics educa-\ntion and psychoacoustics data collection,” in Proc. of\nthe International Conference on Music Information Re-\ntrieval. Philadelphia, PA: ISMIR, 2008.\n[13] G. Grifﬁn, Y . E. Kim, and D. Turnbull, “Beat-sync-\nmash-coder: A web application for real-time creation\nof beat-synchronous music mashups,” in Proc. of the\nIEEE Conf. on Acoustics, Speech, and Signal Process-\ning, 2010.\n[14] T. F. Quatieri, Discrete-Time Speech Signal Process-\ning, A. V . Oppenheim, Ed. Prentice Hall Signal Pro-\ncessing Series, 2002.\n648\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "Beat Critic: Beat Tracking Octave Error Identification By Metrical Profile Analysis.",
        "author": [
            "Leigh M. Smith"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1417891",
        "url": "https://doi.org/10.5281/zenodo.1417891",
        "ee": "https://zenodo.org/records/1417891/files/Smith10.pdf",
        "abstract": "Computational models of beat tracking of musical au- dio have been well explored, however, such systems often make “octave errors”, identifying the beat period at dou- ble or half the beat rate than that actually recorded in the music. A method is described to detect if octave errors have occurred in beat tracking. Following an initial beat tracking estimation, a feature vector of metrical profile sep- arated by spectral subbands is computed. A measure of subbeat quaver (1/8th note) alternation is used to compare half time and double time measures against the initial beat track estimation and indicate a likely octave error. This er- ror estimate can then be used to re-estimate the beat rate. The performance of the approach is evaluated against the RWC database, showing successful identification of octave errors for an existing beat tracker. Using the octave error detector together with the existing beat tracking model im- proved beat tracking by reducing octave errors to 43% of the previous error rate.",
        "zenodo_id": 1417891,
        "dblp_key": "conf/ismir/Smith10",
        "keywords": [
            "octave errors",
            "beat tracking",
            "metrical profile",
            "spectral subbands",
            "subbeat quaver",
            "beat rate",
            "RWC database",
            "performance evaluation",
            "re-estimation",
            "reduction"
        ],
        "content": "BEAT CRITIC: BEAT TRACKING OCTA VE ERROR IDENTIFICATION BY\nMETRICAL PROFILE ANALYSIS\nLeigh M. Smith\nIRCAM\nleigh.smith@ircam.fr\nABSTRACT\nComputational models of beat tracking of musical au-\ndio have been well explored, however, such systems often\nmake “octave errors”, identifying the beat period at dou-\nble or half the beat rate than that actually recorded in the\nmusic. A method is described to detect if octave errors\nhave occurred in beat tracking. Following an initial beat\ntracking estimation, a feature vector of metrical proﬁle sep-\narated by spectral subbands is computed. A measure of\nsubbeat quaver (1/8th note) alternation is used to compare\nhalf time and double time measures against the initial beat\ntrack estimation and indicate a likely octave error. This er-\nror estimate can then be used to re-estimate the beat rate.\nThe performance of the approach is evaluated against the\nRWC database, showing successful identiﬁcation of octave\nerrors for an existing beat tracker. Using the octave error\ndetector together with the existing beat tracking model im-\nproved beat tracking by reducing octave errors to 43% of\nthe previous error rate.\n1. STRUCTURAL LEVELS IN BEAT\nPERCEPTION\nThe psychological and computational representation of lis-\nteners experience of musical time is of great application to\nmusic information retrieval. Correctly identifying the beat\nrate (tactus) facilitates further understanding of the impor-\ntance of other elements in musical signals, such as the rel-\native importance of tonal features.\nConsiderable research has proposed theories of an hi-\nerarchical structuring of musical time [12–14, 18, 20, 27],\nwith the favouring of particular temporal levels. The tac-\ntus has been shown to be inﬂuenced by temporal prefer-\nence levels [10], proposed as a resonance or inertia to vari-\nation [25]. At the metrical level1, [21] argue that pre-\nestablished mental frameworks (“schemas”) for musical me-\nter are used during listening. They found a signiﬁcant dif-\nference in performance between musicians and non-music-\n1A periodic repetition of perceived accentuation, notated in music as\n4\n4,3\n4etc.\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2010 International Society for Music Information Retrieval.ians, arguing that musicians hold more resilient represen-\ntations of meter, which favours hierarchical subdivision of\nthe measure, than the non-musicians.\nThe fastest pulse has been used in ethnomusicology [16,\n24] or reciprocally, the tatum in cognitive musicology [1]\nas a descriptive mechanism for characterising rhythmic struc-\nture. While it is not assumed to be a model of perception\nused by listeners and performers [16], the tatum is used to\nform a rhythmic grid of equally spaced intervals. It there-\nfore represents the limit of hierarchical temporal organisa-\ntion in complex rhythmic structures.\n2. ERRORS IN BEAT TRACKING\nBeat tracking or foot-tapping has a long history [7, 19],\nspurred on by the demands of music information retrieval\n[8, 15, 22, 23]. Common methods of beat tracking involve\nextraction of a mid-level representation, or onset detec-\ntion function [23], typically derived from the spectral ﬂux,\nthereby avoiding the requirement of identifying each indi-\nvidual onset. A number of methods have been proposed to\nthen determine a time varying frequency analysis of the\nonset detection function, including comb ﬁlterbanks [6,\n15, 23], autocorrelation [2, 9], dynamic time warping [8],\nBayesian estimation [3], combined frequency and time lag\nanalysis [22], coupled oscillators [17] and wavelet analy-\nsis [4].\nDespite reporting very good results, there are areas for\nimprovement to these approaches. A common task faced\nby many of these approaches is selecting the appropriate\nstructural level from several viable candidates. It is a com-\nmon occurance to select a beat rate which is twice as fast\nas the actual performed rate, termed an “octave error”. For\nmany of these systems, a reselection of the correct struc-\ntural level from the candidates would be possible if the oc-\ntave error could be detected.\nThe concept of fastest pulse can be used as an indica-\ntor of the highest structural level and therefore a datum.\nThis appears in terms of the fastest alternation of events.\nChecking for quaver (1/8 note) alternation indicates if there\nis evidence of the fastest pulse appearing at the expected\nstructural level, given the assumed tactus level. This pa-\nper proposes a method to evaluate the beat tracking and\nidentify octave errors using an analysis of metrical pro-\nﬁles. This forms a combined feature vector of metrical\nproﬁle over separate spectral subbands, described in Sec-\ntion 3. The behaviour of the metrical proﬁle is analysed in\n99\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)terms of quaver alternation to identify beat tracking which\nhas performed an octave error. This approach is evaluated\nagainst an annotated dataset for beat tracking and tempo\nestimation as described in Section 4. The results of eval-\nuation against datasets of recorded music are reported in\nSection 5.\n3. METHOD\nTo identify the fastest pulse or tatum requires identifying\nthe higher level rhythmic structural levels. To do so, the\nbeat period (tactus) and metrical period (duration of the\nbar) is computed from the audio signal of the musical ex-\nample using a beat-tracker, in this case as developed by\nPeeters [22]. From the nominated beat times, a metrical\nproﬁle is computed.\n3.1 Metrical Proﬁle\nThe metrical proﬁle, indicating the relative occurrence of\nevents in each metrical position within the measure, has\nbeen demonstrated by [21] to represent metrical structure\nand matches closely with listeners judgements of metrical\nwell-formedness. The metrical proﬁle is computed from\nthe likelihood of an onset at each tatum (shortest temporal\ninterval) within a measure. The likelihood of onsets are\ndetermined from the presence of onset detection function\n(ODF) energy edescribed in [22]. The probability of an\nonset otat each tatum location tis\not=/braceleftBigg\n¯et\n¯e+γσ e+/epsilon1, o t<1\n1 ot>1(1)\nwhere ¯etis the mean energy of the ODF over the region of\nthe tatum t,¯eandσeare the mean and standard deviation\nof the entire ODF energy respectively, /epsilon1is a small value to\nguard against zero ¯e, and γis a free parameter determin-\ning the maximum number of standard deviations above the\nmean to assure an onset has occurred. By informal testing,\nγ= 2. The onset likelihoods are then used to create an\nhistogram mt, fort= 1, . . . , n, of the relative amplitude\nand occurrence at each tatum, by averaging each otacross\nallMmeasures\nmt=/summationtextM\nµ=0ot+nµ\nM. (2)\nTo normalise for varying tempo across each piece and\nbetween pieces, the duration of each measure is derived\nfrom the beat-tracker [22]. Using the beat locations iden-\ntiﬁed by the beat-tracker, each beat duration is uniformly\nsubdivided into 1/64th notes (hemi-demi-semiquavers), that\nis0< t < 64for a measure of a semibreve (whole note)\nduration. Such a high subdivision attempts to categorise\nswing timing occurring within the measure and to provide\nsufﬁcient resolution for accurate comparisons of metrical\nstructure. Using the tatum duration set to equal subdivi-\nsions of each beat duration does not capture expressive tim-\ning occuring within that time period. However, the error\nproduced from this is minimal since the expressive timing\nwhich modiﬁes each beat and measure period is respected.Channel cLow band ωc(Hz) High band ω/prime\nc(Hz)\n1 60 106\n2 106 186\n3 186 327\n4 327 575\n5 575 1012\n6 1012 1781\n7 1781 3133\n8 3133 5512\nTable 1. Sub-band channel frequency ranges used to calcu-\nlate local spectrum onset detection functions in Equation 3.\nThe effect of this error is to blur the peak of each tatum\nonset. The metrical proﬁle is then downsampled (by local\naveraging of 4 tatums) to semiquavers (1/16 notes).\n3.2 Spectral Sub-band Proﬁles\nListeners categorise sounds using their individual spectral\ncharacter, and the identiﬁcation of their reoccurance aids\nrhythmic organisation. To distinguish the possibly com-\npeting timing of different instruments and in order to match\ncategorization used by listeners, metrical proﬁles are sep-\narated by spectral energy. This is produced by computing\nspectral sub-bands of the half wave rectiﬁed spectral en-\nergy. The sub-bands are computed by summing over non-\noverlapping frequencies:\nFc,t=b/prime\nc/summationdisplay\nb=b ceHWR (ωb, t), (3)\nwhere Fc,tis the spectral ﬂux for the sub-band channel\nc= 1, . . . , C at time t, over the spectral bands b= [ωc, ω/prime\nc]\nof the half-wave rectiﬁed spectral energy eHWR (ωb, t)at\nfrequency band ωbcomputed as described by [22]. The\nsub-band channels used are listed in Table 1 for C= 8.\nThese form logarithmically spaced spectral bands that ap-\nproximate different time keeping functions in many forms\nof music. A set of subband metrical proﬁles is then mtc\nfort= 1, 2, . . . , n, c = 1, . . . , C .\n3.3 Quaver Alternation\nWith the metrical proﬁle reduced to semiquavers, a mea-\nsure of the regularity of variation at the supposed qua-\nver period can be calculated. Since the tatums at strong\nmetrical locations are expected to vary strongly regard-\nless of metrical level, only the variation for the sub-beats\nfalling at metrically weaker locations is used. For exam-\nple, in a4\n4measure, n= 16, metrically strong semiqua-\nvers are r={1,5,9,13}. The subbeat vector of length S\nis deﬁned as s=r/negationslash∩t. Using the same example meter,\ns={2,3,4,6,7,8,10,11,12,14,15,16}.\nThe average quaver alternation qfor a rhythm is the nor-\nmalised ﬁrst order difference of subbeat proﬁles m/prime\ns\nq=/summationtextC\nc=1/summationtext\ni∈s|m/prime\nic|\nSCmax(m s). (4)\n100\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Figure 1. Metrical proﬁles of an example from the RWC dataset\nwhich was beat tracked with octave error. The top plot displays\na metrical proﬁle of 16 semiquavers per measure for each of the\nspectral subands (c = 1, . . . , 8). The second, third and fourth\nplots displays the subband metrical proﬁles created for half time,\nhalf time counterphase and double time interpretations respec-\ntively.\nA low quaver alternation measure indicates that varia-\ntion between adjacent sub-beat semiquavers is low. This\nis most likely either in the case that there is little activity\nin the music, or the structural level chosen as the quaver\nis incorrect, i.e an octave error has occurred. To identify\nthe case of an octave error, the quaver alternation of the\nmetrical proﬁle of a track is compared to metrical proﬁles\nof the same track formed from half and double the number\nof beats. The half tempo proﬁle ˙qis formed from simply\nskipping every second beat identiﬁed by the beat tracker.\nA similar counter-phase half tempo proﬁle ´qis formed by\nalso skipping the initial beat. The double time proﬁle ¨qis\nformed from sampling at onsets otlinearly bisecting each\noriginal inter-beat interval.\nComparisons between metrical proﬁles of an example\nrhythm is shown in Figure 1. The metrical pattern is dis-\nplayed on the top plot, with n= 16 tatums per measure,\ntheC= 8subband proﬁles arranged adjacent in increasing\nfrequency band. On the lower plots, the patterns created by\nassuming half tempo, half tempo counterphase, and dou-\nble tempo are displayed. It can be seen that the alternation\nwhich occurs on the half tempo and half tempo counter-\nphase plots is more regular than the original metrical pat-\ntern or the double time pattern. This indicates that for this\nexample, an octave error has occurred.\nA measure of octave error eis computed by comparing\nthe ratio of the half tempo quaver alternation to original\nquaver alternation and the ratio of double tempo to original\nquaver alternation,\ne=˙q+ ´q\n2q+¨q\nq. (5)\nEquation 5 represents the degree that the alternation at\nthe half or double tempo exceeds the original quaver al-\nternation. Values of˙q+´q\n2q>1or¨q\nq>1indicates there is\nan octave error from either the double or half quaver alter-\nnation being greater, but in practice, the threshold e > e/prime\nneeds to be higher. The threshold was determined exper-imentally as half a standard deviation above ¯eas derived\nfrom the RWC dataset at e/prime= 3.34.\n3.4 Reestimation of Tempo\nThe beat tracking for each piece which was nominated by\nthe algorithm as being an octave error is then recomputed\nwith the prior tempo estimate set to half the tempo ﬁrst\ncomputed. In the case of the Viterbi decoding of the beat\ntracker used [22], this prior tempo estimate weights the\nlikely path of meter and tempo selection towards the half\nrate. In this case, even if the prior tempo is set at half, it\nis not guaranteed to be chosen as half the rate, if the orig-\ninal tempo is a more likely path which outweighs the new\nreestimation. This makes the beat tracker robust to false\npositive classiﬁcations from the beat critic.\n4. EVALUATION\nTwo evaluation strategies for octave errors are possible: 1)\nevaluation of beat tracking, where the phase of the beat\ntracking is correct, but the beat frequency is twice the true\nrate and 2) evaluation of tempo alone, where the beat fre-\nquency is twice the true rate and the phase of the beat track-\ning is not assessed. These two evaluations meet different\nneeds, the former if beat tracking accuracy is required, the\nlatter if a correct median tempo measure is sufﬁcient.\nTo evaluate the discrimination of the algorithm, the com-\nmonly used RWC dataset was used [11]. This dataset con-\nsists of 328 tracks in 5 sets (Classical, Jazz, Popular, “Genre”\nand “Royalty Free”) annotated for beat times. A subset of\n284 tracks was produced by eliminating pieces whose an-\nnotations were incorrect or incomplete in the RWC dataset.2\nSince the algorithm evaluates metrical proﬁles, this re-\nquires meter changes to be accurately identiﬁed by the beat\ntracker, which currently lacks that capability. Therefore\npieces with changing meters are expected to reduce the\nperformance of the algorithm. However since this would\nhave reduced the dataset further, and added beats or time\nsignature changes are common in many genres of music,\nthe dataset was used with these potential noise sources.\nTo evaluate octave error detection independent of the\nquality of the beat tracking, pieces which were incorrectly\nbeat tracked were eliminated from the test set. This was\ndeﬁned as a beat tracking F-score below 0.5 using a tem-\nporal window of each annotated beat position within 15%\nof each inter-beat interval [5,26]. A ground truth set of oc-\ntave error examples was produced by comparing the ratio\nof the beat tracking recall Rto precision Pmeasures, with:\nˆe=⌊R/P + 0.5⌋, (6)\nwhere ˆe= 2indicates an octave error. These ground truth\ncandidates were then manually auditioned to verify that\nthey were truly octave errors.\nThis produced a resulting dataset of 195 pieces, termed\n“Good”, with 46 pieces identiﬁed as actually being beat\ntracked at double time (an octave error). This formed the\n2For several of the Jazz examples and the Genre examples, only the\nminim (half note) level was annotated.\n101\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Dataset C. True S. Prec. Rec. F\nGood 30 46 55 0.545 0.652 0.594\nFull 29 46 82 0.354 0.630 0.453\nTable 2. Results of octave error detection by metrical pro-\nﬁle analysis (beat critic). “C.” indicates the number of\ntracks correctly identiﬁed as an octave error, “True” as the\nground truth number of octave errors manually identiﬁed.\n“S.” indicates the number of tracks selected as being an oc-\ntave error. “Prec.”, “Rec.” and “F” indicates the precision,\nrecall and F-score measures respectively.\nPre-Reest. Post-Reest.\nDataset Meth. Size OE NE OE NE %\nGood BT 195 46 20 43\nGood BPM 195 44 10 24 12 54\nFull BT 284 63 37 58\nFull BPM 284 57 42 38 46 66\nTable 3. Number of tracks with beat tracking octave er-\nrors (OE) before (Pre) and after (Post) reestimation using\nthe beat critic. The column labelled “%” indicates the re-\nduction in octave errors. NE columns indicates non-octave\nerrors.\nground truth to evaluate the octave error identiﬁcation al-\ngorithm. From these, standard precision, recall and F-score\nmeasures can be computed [26]. The entire set of 284\npieces (termed “Full”) was also used to evaluate perfor-\nmance when beat tracking does not perform optimally.\nTo determine the improvement the beat critic makes to\nbeat tracking, pieces which were determined to be beat\ntracked with octave error were recomputed with half the\nprior tempo. This would occur for false as well as true\npositives. The beat tracker would then use the new weight-\ning towards the half tempo, but could produce the same\nresult as the original beat tracking if the Viterbi decoding\nstill biased towards the original tempo estimate [22].\nThe Good and Full datasets were also assessed for their\nﬁdelity to the annotated median tempo measurement τof\neach track. This was computed as τ= 60/ ˜i, where ˜iis\nthe median inter-beat interval in seconds. A beat tracked\ntempo which was within 3% of the annotated tempo was\ndeemed a successful tempo estimation.\n5. RESULTS\nThe results of evaluating the beat critic with the Good and\nFull RWC datasets appear in Table 2. On the “Good”\ndataset, while the critic is able to identify 65% of the pieces\nwith octave errors (the recall), it produces a sizeable num-\nber of false positives (the precision) which reduces the F-\nscore. As to be expected, with the “Full” dataset, the per-\nformance is worse. The substantially higher number of\nfalse positives for this dataset indicate that the octave er-\nror measure is sensitive to beat tracking error. As the al-\ngorithm is deﬁned, the measure of sub-beat alternation isprobably too reliant on the expectation that the beat is cor-\nrectly tracked.\nDespite the relatively low scoring results, Table 3 in-\ndicates the success of the beat critic when used to rees-\ntimate the beat tracker. The column “Meth.” describes\nthe method of evaluation, either “BT” for beat tracking,\ncomparing each beat location against annotated beats, or\n“BPM”, comparing estimated tempo against annotated tempo.\n“Size” describes the number of tracks in the dataset. “OE”\nindicates the number of tracks that were beat tracked that\nare evaluated to have been an octave error. “Pre” and “Post”\nindicates the number of tracks before and after reestimat-\ning using the beat critic to bias prior tempo of the beat\ntracker. “NE” indicates the number of tracks that were not\nbeat tracked correctly but were not octave errors. While it\nis possible to identify non-octave errors with BPM evalu-\nation within a perceptually meaningful tolerance (3%, see\nSection 4), this can not be deﬁned properly when the mea-\nsure of beat tracking is calculated in terms of precision,\nrecall and F-score.\nIn the case of the BT evaluation, the number of oc-\ntave errors were reduced to 43% and 58% of the former\nnumber of errors for the Good and Full datasets respec-\ntively. This indicates that the Viterbi decoding of the beat\ntracker has beneﬁtted from reestimation and is reasonably\nrobust to the false positives identiﬁed as octave errors. The\ntempo evaluation also showed similar improvements, re-\nducing octave errors to 54% and 66% (Good and Full). The\nslight increase in non-octave errors after reestimation indi-\ncates cases where the false positives have lead to mistrack-\ning. Depending on the application, this may be an unac-\nceptable deterioration in performance despite an increase\nin the overall number of correctly tracked pieces.\n6. CONCLUSIONS\nA method for the detection of octave errors in beat track-\ning has been proposed and evaluated. The approach was\nevaluated with an audio dataset that represents a variety of\ngenres of music. This approach, while currently applied\nto only one beat tracker, depends only on the presence of a\nmid-level representation, and the determination of beat and\nmeter periods, commonly produced by many beat trackers.\nIt is applicable to beat trackers which beneﬁt from reesti-\nmation or convergence in the selection of the beat tracking\nfrequency.\nWhile the performance of the beat critic is well below\nperfection, when applied to a beat tracker, it has been shown\nto improve overall performance, reducing the number of\noctave errors, at the cost of a slight increase in mistracking.\nThe beat critic’s applicability and usefulness is ultimately\ndependent on the cost of false positives.\nA number of improvements are possible. The use of\na threshold for the octave error classiﬁcation is simplistic\nand possibly difﬁcult to set accurately. A machine learning\nclassiﬁer promises to perform better in this task. However,\nthe best features to be used are not yet clear, preliminary\nexperiments with the quaver alternation measures q,´q,˙q\nand ¨qindicate that these are insufﬁcient features to dis-\n102\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)criminate the octave error classiﬁcation. The alternative,\nusing the entire proﬁles, or reductions thereof, as features\nproduces too high a dimensionality for accurate learning.\nAnother issue is the relative computational cost of such an\napproach, when the current threshold approach is compu-\ntationally low. In principle the approach could be used\nto identify beat tracking at half the correct rate, although\nsuch beat tracking errors did not occur using the dataset\nand therefore have not been evaluated.\nThe beat critic exploits knowledge of rhythmic behaviour\nas represented in musicologically based models of metrical\nproﬁles to compare temporal levels. The comparison of the\nrelative activity of levels is used to identify octave errors.\nBy examining the behaviour of events in the time domain,\nthe goal has been to circumvent limitations in the temporal\nresolution of frequency based analysis in the identiﬁcation\nof beat levels.\n7. ACKNOWLEDGEMENTS\nThis research was supported by the French project Oseo\n“Quaero”. Thanks are due to Geoffroy Peeters for provi-\nsion of the beat-tracker and onset detection code.\n8. REFERENCES\n[1] Jeffrey A. Bilmes. Timing is of the essence: Percep-\ntual and computational techniques for representing,\nlearning, and reproducing expressive timing in percus-\nsive rhythm. Master’s thesis, Massachusetts Institute of\nTechnology, September 1993.\n[2] Judith C. Brown. Determination of the meter of musi-\ncal scores by autocorrelation. Journal of the Acoustical\nSociety of America, 94(4):1953–7, 1993.\n[3] Ali Taylan Cemgil and Bert Kappen. Monte Carlo\nmethods for tempo tracking and rhythm quantization.\nJournal of Artiﬁcal Intelligence Research, 18:45–81,\n2003.\n[4] Martin Coath, Susan Denham, Leigh M. Smith, Henk-\njan Honing, Amaury Hazan, Piotr Holonowicz, and\nHendrik Purwins. Model cortical responses for the\ndetection of perceptual onsets and beat tracking in\nsinging. Connection Science, 21(2):193—205, 2009.\n[5] Matthew E. P. Davies and Mark D. Plumbley. A spec-\ntral difference approach to downbeat extraction in mu-\nsical audio. In EUSIPCO, 2006.\n[6] Matthew E. P. Davies and Mark D. Plumbley. Context-\ndependent beat tracking of musical audio. IEEE Trans-\nactions on Audio, Speech and Language Processing,\n15(3):1009–20, 2007.\n[7] Peter Desain and Henkjan Honing. Foot-tapping: A\nbrief introduction to beat induction. In Proceedings\nof the International Computer Music Conference,\npages 78–9. International Computer Music Associa-\ntion, 1994.[8] Simon Dixon. Evaluation of the audio beat tracking\nsystem BeatRoot. Journal of New Music Research,\n36(1):39–50, 2007.\n[9] Douglas Eck. Beat induction with an autocorrelation\nphase matrix. In M. Baroni, A. R. Addessi, R. Caterina,\nand M. Costa, editors, Proceedings of the 9th Interna-\ntional Conference on Music Perception and Cognition\n(ICMPC), page 931, Bologna, Italy, 2006. SMPC and\nESCOM.\n[10] Paul Fraisse. Rhythm and tempo. In Diana Deutsch,\neditor, The Psychology of Music, pages 149–80. Aca-\ndemic Press, New York, 1st edition, 1982.\n[11] Masataka Goto, Hiroki Hashiguchi, Takuichi\nNishimura, and Ryuichi Oka. RWC music database:\nPopular, Classical, and Jazz music databases. In\nProceedings of the International Symposium on Music\nInformation Retrieval, pages 287–288, October 2002.\n[12] Mari Riess Jones. Time, our lost dimension: Toward a\nnew theory of perception, attention and memory. Psy-\nchological Review, 83(5):323–55, 1976.\n[13] Mari Riess Jones. Musical time. In Oxford Handbook\nof Music Psychology, pages 81–92. Oxford University\nPress, 2009.\n[14] Mari Riess Jones and Marilyn Boltz. Dynamic at-\ntending and responses to time. Psychological Review,\n96(3):459–91, 1989.\n[15] Anssi P. Klapuri, Antti J. Eronen, and Jaakko T. Astola.\nAnalysis of the meter of acoustic musical signals. IEEE\nTransactions on Audio, Speech and Language Process-\ning, 14(1):342–55, 2006.\n[16] James Koetting. What do we know about African\nrhythm? Ethnomusicology, 30(1):58–63, 1986.\n[17] Edward W. Large and John F. Kolen. Resonance and\nthe perception of musical meter. Connection Science,\n6(2+3):177–208, 1994.\n[18] Justin London. Hearing in Time: Psychological As-\npects of Musical Meter. Oxford University Press, 2004.\n[19] H. Christopher Longuet-Higgins and Christopher S.\nLee. The perception of musical rhythms. Perception,\n11:115–28, 1982.\n[20] James G. Martin. Rhythmic (hierarchical) versus serial\nstructure in speech and other behaviour. Psychological\nReview, 79(6):487–509, 1972.\n[21] Caroline Palmer and Carol L. Krumhansl. Mental rep-\nresentations for musical meter. Journal of Experimen-\ntal Psychology - Human Perception and Performance,\n16(4):728–41, 1990.\n[22] Geoffroy Peeters. Template-based estimation of time-\nvarying tempo. EURASIP Journal on Advances\nin Signal Processing, (67215):14 pages, 2007.\ndoi:10.1155/2007/67215.\n103\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)[23] Eric D. Scheirer. Tempo and beat analysis of acoustic\nmusical signals. Journal of the Acoustical Society of\nAmerica, 103(1):588–601, 1998.\n[24] Uwe Seifert, Fabian Olk, and Albrecht Schneider. On\nrhythm perception: Theoretical issues, empirical ﬁnd-\nings. Journal of New Music Research, 24(2):164–95,\n1995.\n[25] Leon van Noorden and Dirk Moelants. Resonance in\nthe perception of musical pulse. Journal of New Music\nResearch, 28(1):43–66, 1999.\n[26] C. V. van Rijsbergen. Information Retrieval. Butter-\nworth, London; Boston, 2nd edition, 1979.\n[27] Maury Yeston. The Stratiﬁcation of Musical Rhythm.\nYale University Press, New Haven, 1976. 155p.\n104\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "Musical Instrument Recognition using Biologically Inspired Filtering of Temporal Dictionary Atoms.",
        "author": [
            "Steven K. Tjoa",
            "K. J. Ray Liu"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1416166",
        "url": "https://doi.org/10.5281/zenodo.1416166",
        "ee": "https://zenodo.org/records/1416166/files/TjoaL10.pdf",
        "abstract": "Most musical instrument recognition systems rely entirely upon spectral information instead of temporal information. In this paper, we test the hypothesis that temporal informa- tion can improve upon the accuracy achievable by the state of the art in instrument recognition. Unlike existing tem- poral classification methods which use traditional features such as temporal moments, we extract novel features from temporal atoms generated by nonnegative matrix factoriza- tion by using a multiresolution gamma filterbank. Among isolated sounds taken from twenty-four instrument classes, the proposed system can achieve 92.3% accuracy, thus im- proving upon the state of the art.",
        "zenodo_id": 1416166,
        "dblp_key": "conf/ismir/TjoaL10",
        "keywords": [
            "spectral information",
            "temporal information",
            "instrument recognition",
            "temporal classification",
            "traditional features",
            "nonnegative matrix factorization",
            "multiresolution gamma filterbank",
            "isolated sounds",
            "twenty-four instrument classes",
            "92.3% accuracy"
        ],
        "content": "MUSICAL INSTRUMENT RECOGNITION USING BIOLOGICALLY\nINSPIRED FILTERING OF TEMPORAL DICTIONARY ATOMS\nSteven K. Tjoa and K. J. Ray Liu\nSignals and Information Group, Department of Electrical and Computer Engineering\nUniversity of Maryland – College Park, MD 20742 USA\nfkiemyang, kjrliug@umd.edu\nABSTRACT\nMost musical instrument recognition systems rely entirely\nupon spectral information instead of temporal information.\nIn this paper, we test the hypothesis that temporal informa-\ntion can improve upon the accuracy achievable by the state\nof the art in instrument recognition. Unlike existing tem-\nporal classiﬁcation methods which use traditional features\nsuch as temporal moments, we extract novel features from\ntemporal atoms generated by nonnegative matrix factoriza-\ntion by using a multiresolution gamma ﬁlterbank. Among\nisolated sounds taken from twenty-four instrument classes,\nthe proposed system can achieve 92.3% accuracy, thus im-\nproving upon the state of the art.\n1. INTRODUCTION\nAdvances in sparse coding and dictionary learning have in-\nﬂuenced much of the recent progress in musical instrument\nrecognition. Many of these methods depend upon nonneg-\native matrix factorization (NMF) – a popular, convenient,\nand effective method for decomposing matrices – to ob-\ntain low-rank approximations of audio spectrograms [9].\nNMF yields a set of vectors, spectral atoms, which ap-\nproximately span the frequency space of the spectrogram,\nand another set of vectors, temporal atoms, which corre-\nspond to the temporal activation of each spectral atom.\nThe spectral atoms can then be classiﬁed by instrument\nusing features such as mel-frequency cepstral coefﬁcients\n(MFCCs).\nWhile these methods are effective in exploiting the spec-\ntral redundancy in a signal, redundancy remains in the tem-\nporal domain. Psychoacoustic studies have shown that\nspectral and temporal information are equally important in\nthe deﬁnition of acoustic timbre [10]. Classiﬁcation meth-\nods that only utilize spectral information are discarding the\npotentially useful temporal information that could be used\nto improve classiﬁcation performance.\nIn this paper, we combine advances in dictionary learn-\ning, auditory modeling, and music information retrieval to\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2010 International Society for Music Information Retrieval.propose a new timbral representation. This representation\nis inspired by another widely accepted timbral model, the\ncortical representation, which estimates the spectral and\ntemporal modulation content of the auditory spectrogram.\nOur method of extracting temporal information uses a mul-\ntiresolution gamma ﬁlterbank which is computed from the\ntemporal atoms extracted from spectrograms using NMF.\nExtracting and classifying this feature is simple yet effec-\ntivefor musical instrument recognition.\nAfter deﬁning the proposed feature extraction and clas-\nsiﬁcation method, we test the hypothesis that the proposed\nfeature improves upon the accuracy achievable by the state\nof the art in musical instrument recognition. For isolated\nsounds, we show that temporal information can be used to\nbuild a classiﬁer capable of 72.9% accuracy when tested\namong 24 instrument classes. However, when combining\ntemporal and spectral features, the proposed classiﬁer can\nachieve an accuracy of 92.3%, thus reﬂecting state of the\nart performance.\n2. TEMPORAL INFORMATION\nTemporal information is incorporated into timbral mod-\nels in different ways. Many attempts to incorporate tem-\nporal information use features such as the temporal cen-\ntroid, spread, skewness, kurtosis, attack time, decay time,\nslope, and locations of maxima and minima [5,6]. One tim-\nbral representation, the cortical representation, incorpo-\nrates both spectral and temporal information. Essentially,\nthe cortical representation embodies the output of cortical\ncells as sound is processed by earlier stages in the audi-\ntory system. Fig. 1 illustrates the relationship between the\nearly and middle stages of processing in the mammalian\nauditory system. The early stage models the transforma-\ntion by the cochlea of an acoustic input signal into a neural\nrepresentation known as the auditory spectrogram, while\nthe middle stage models the analysis of the auditory spec-\ntrogram by the primary auditory cortex.\nOne property of cortical cells, the spectrotemporal re-\nceptive ﬁeld (STRF), summarizes the way a single corti-\ncal cell responds to a stimulus. Mathematically, the STRF\nis like a two-dimensional impulse response deﬁned across\ntime and frequency. Each STRF has three parameters: scale,\nrate, and orientation. Scale deﬁnes the spectral resolution\nof an STRF, rate deﬁnes its temporal resolution, and ori-\nentation determines if the STRF selects upward or down-\n435\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Multiresolution Filter BankConstant-Q Filter Bank\nInner Hair Cell Stages\nLateral Inhibitory Network\nCortical Representation\n(time, frequency, rate, scale)Auditory \nSpectrogram\n(time, frequency)Acoustic Waveform (time)\nEarly Stage: \nCochlea\nMiddle Stage:\nPrimary \nAuditory\nCortexSTRFSTRFSTRFSTRFSTRFSTRF\nSTRFSTRF\nSTRFSTRFSTRFSTRF\nSTRFSTRFSTRFSTRF\nSTRFSTRF\nRateScaleSTRFSTRFSTRFSTRFSTRFSTRFFigure 1. Early and middle stages of the auditory system.\nThe auditory spectrogram is convolved across time and fre-\nquency with STRFs of different rates and scales to produce\nthe four-dimensional cortical representation.\nward frequency modulations. Fig. 2 illustrates the STRF\nas a function of these three parameters. Each cortical cell\ncan be interpreted as a ﬁlter whose impulse response is an\nSTRF with a particular rate, scale, and orientation. There-\nfore, a collection of cortical cells constitutes a ﬁlterbank.\nIndeed, it turns out that the cortical representation is math-\nematically equivalent to a multiresolution wavelet ﬁlter-\nbank [2].\nDespite the biological relevance between the cortical\nrepresentation and timbre, this representation has disad-\nvantages for classiﬁcation purposes. First, because the cor-\ntical representation is a complex-valued four-dimensional\nﬁlterbank output, it is massively redundant. Like many\ntypes of redundant data, the cortical representation could\nbeneﬁt from some form of coding, decomposition, or di-\nmensionality reduction. However, proper application of\nthese tools to the cortical representation for engineering\npurposes such as speech recognition and MIR is not yet\nwell understood. Therefore, these are ongoing areas of re-\nsearch [11]. Second, the STRF is not time-frequency sep-\narable [2]. In other words, computation of the cortical rep-\nresentation cannot be decomposed into two procedures that\noperate on the time and frequency dimensions separately.\nBecause spectral and temporal information require differ-\nent classiﬁcation methods, this obstacle impedes classiﬁ-\ncation.\nUnlike the cortical representation, the spectrogram com-\nputed via short-time Fourier transform (STFT) is easily de-\ncomposed, particularly for musical signals. For example,\nmany works have applied decomposition methods to mag-\nnitude spectrograms of musical sounds in order to identify\nRate: 2 Hz\n Rate: 1 HzOrientation: Downward\nRate: 1 Hz\n Rate: 2 HzOrientation: Upward\nScale: 4 cyc/oct\nFrequency\nScale: 2 cyc/oct\nFrequency\nScale: 1 cyc/oct\nFrequency\nTime\n Time\n Time\n TimeFigure 2. Twelve example STRFs. Together, they con-\nstitute a ﬁlterbank. The left six STRFs select downward-\nmodulating frequencies, and the right six STRFs select\nupward-modulating frequencies. Top row: seed functions\nfor rate determination. Left column: seed functions for\nscale determination.\na set of spectral and temporal basis vectors from which the\nmagnitude spectrogram can be parameterized [15]. One\nsuch decomposition method is NMF [9]. Given an element-\nwise nonnegative matrix X, NMF attempts to ﬁnd two\nnonnegative matrices, AandS, that minimize some di-\nvergence between XandAS. Among the algorithms that\ncan perform this minimization, one of the most convenient\nalgorithms uses a multiplicative update rule during each it-\neration in order to maintain nonnegativity of the matrices\nAandS[9].\nMany researchers have already demonstrated the use-\nfulness of NMF for separating a musical signal into indi-\nvidual notes [7,15,16]. By ﬁrst expressing a time-frequency\nrepresentation of the signal as a matrix, these methods de-\ncompose the matrix into a summation of a few individual\natoms, each corresponding to one musical source or one\nnote. Fig. 3 illustrates the use of NMF upon the spectro-\ngram of a musical signal. We deﬁne each column of A\nas aspectral atom and each row of Sas atemporal atom.\nThe temporal atoms usually resemble envelopes of known\nsounds, particularly in musical signals. For example, ob-\nserve the difference between the proﬁles of the temporal\natoms in Fig. 3. The three beats generated by the kick\ndrum share the same temporal proﬁles, and the two beats\ngenerated by the snare drum share the same proﬁles. This\ngeneral observation motivates the hypothesis that the en-\nergy distribution of temporal NMF atoms is a valid timbral\nrepresentation that can be used to classify instruments.\nIn the next section, we propose one technique that ex-\ntracts timbral information from temporal NMF atoms simi-\nlar to that of the cortical representation. Our technique uses\namultiresolution gamma ﬁlterbank to perform multires-\nolution analysis upon the factorized spectrogram. How-\never, unlike the cortical representation, this multiresolution\nanalysis is particularly suited to the energy proﬁles con-\ntained in the temporal NMF atoms.\n436\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)1\nFrequency\n1\n2\nTime\n2\nSpectrogramFigure 3. The NMF of a spectrogram drum beats. Compo-\nnent 1: kick drum. Component 2: snare drum. Top right:\nX. Left: A. Bottom: S.\n3. PROPOSED METHOD: MULTIRESOLUTION\nGAMMA FILTERBANK\nThe multiresolution gamma ﬁlterbank is a collection of\ngamma ﬁlters. For this work, we deﬁne the gamma ker-\nnel to be\ng(t;n;b) =\u000btn\u00001e\u0000btu(t) (1)\nwhereb>0,n\u00151,u(t)is the unit step function, and\n\u000b=s\n(2b)2n\u00001\n\u0000(2n\u00001)(2)\nensures thatR\njg(t;n;b)j2dt= 1 for any value of nand\nb, where \u0000(n) is the Gamma function. Let Ibe the total\nnumber of gamma ﬁlters in the ﬁlterbank. For each i2\nf1;:::;Ig, deﬁne the correlation kernel (i.e., time-reversed\nimpulse response) of each gamma ﬁlter to be\ngi(t) =g(t;ni;bi): (3)\nThe set of kernels fg1;g2;:::;g Igdeﬁnes the multiresolu-\ntion gamma ﬁlterbank. Fig. 4 illustrates some example\nkernels of the ﬁlterbank.\nFor eachi, let the ﬁlter output be the cross-correlation\nbetween the input atom, s(t), and the kernel, gi(t):\nyi(\u001c) =Z1\n\u00001s(t)g i(t\u0000\u001c)dt (4)\nThe set of outputs fy1;y2;:::;y Igfrom the ﬁlterbank is\ncalled the multiresolution gamma ﬁlterbank response (MGFR).\nThe gamma ﬁlter has convenient temporal properties.\nWe deﬁne the attack time of the kernel g(t)to be the time\nelapsed until the kernel achieves its maximum. By differ-\nentiating logg(t), we determine the attack time to be\nta= (n\u00001)=b seconds. (5)\nFig. 4 illustrates the relationship between the attack time\nand the parameter b. Also, astbecomes large, logg(t)\u0019\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\ng(t):n= 2,b= 1\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\ng(t):n= 2,b= 2\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\nTime (seconds)\ng(t):n= 2,b= 4\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\ng(t):n= 4,b= 3\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\ng(t):n= 4,b= 6\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\nTime (seconds)\ng(t):n= 4,b= 12Figure 4. Kernels of gamma ﬁlters. The dashed vertical\nline indicates the location of the maxima. Left column:\nn= 2. Right column: n= 4.\n\u0000btplus a constant. Therefore, bis the decay parameter of\ng(t), where we deﬁne the decay rate ofg(t)to be\nrd= 20b log10e\u00198:7b dB per second. (6)\nTogether, these two temporal properties imply that a gamma\nkernel with anyattack time and decay rate can be created\nfrom the proper combination of nandb.\nFig. 5 illustrates the operation of the multiresolution\ngamma ﬁlterbank. When a temporal NMF atom is sent\nthrough the multiresolution gamma ﬁlterbank, the MGFR\nreveals the strength of the attacks and decays of the atom’s\nenvelope for different values for nandb. Observe how the\nﬁlterbank response is largest for those ﬁlters whose attack\ntime matches that of the input atom.\nThe multiresolution gamma ﬁlterbank behaves like a set\nof STRFs. Both systems perform multiresolution analy-\nsis on the input data. Each STRF passes a different spec-\ntrotemporal pattern depending upon the rate and scale. In\nfact, the seed function used to determine the rate of an\nSTRF is a gammatone kernel – a sinusoid whose envelope\nis a gamma kernel. By altering the parameters of the gam-\nmatone kernel, STRFs can select different rates. Similarly,\nin the multiresolution gamma ﬁlterbank, each ﬁlter passes\ndifferent envelope shapes depending upon the parameters\nnandbwhich completely characterize the attack and de-\ncay of the envelope. Intuitively, the ﬁlter with kernel gi(t)\npasses envelopes with attack times equal to (ni\u00001)=bi\nseconds and envelopes with decay rates equal to 8:7bidB\nper second.\n4. PROPOSED FEATURE EXTRACTION AND\nCLASSIFICATION\nTo extract a shift-invariant feature from the MGFR, we\ncompute the norm for each ﬁlter response:\nzi=\u0012Z1\n\u00001jyi(t)jpdt\u00131=p\n(7)\n437\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)ta= 0.010\nta= 0.020\nta= 0.040\nta= 0.080\nta= 0.160\nta= 0.320\nta= 0.640\nta= 1.280\n0\n 1\n 2\n 3\n 4\n 5\nTime (seconds)\nFigure 5. Top: MGFR as a function of time for n= 2.\nBottom: input atom containing two pulses with attack\ntimes of 160 ms.\nThe vector z= [z 1;z2;:::;z I]is the extracted feature vec-\ntor. To eliminate scaling ambiguities among the input atoms,\nevery feature vector zis normalized to have unit Euclidean\nnorm. Different choices of pprovide different interpreta-\ntions of z. For this work, we use p=1. Our future work\nwill include an investigation into the impact of pon classi-\nﬁcation performance.\nThe proposed feature extraction algorithm is summa-\nrized below.\n1. Perform NMF on the magnitude spectrogram, X, to\nobtain AandS.\n2. Initialize the multiresolution gamma ﬁlterbank in (3).\n3. For each temporal atom (i.e., row of S), compute the\nMGFR in (4).\n4. Compute the feature vector zin (7).\nFinally, we formulate the instrument recognition prob-\nlem as a typical supervised classiﬁcation problem: given\na set of training features extracted from signals of known\nmusical instruments, identify all of the instruments present\nin a test signal. To perform supervised classiﬁcation, tem-\nporal atoms are extracted from training signals of known\nmusical instruments using NMF. The feature vector zcom-\nputed from the atom plus its instrument label are used for\ntraining. To predict the label of an unknown sample, zis\nextracted from the unknown sample and classiﬁed using\nthe trained model.\nAn advantage of the proposed feature extraction and\nclassiﬁcation procedure is its simplicity. The proposed sys-\ntem requires no rule-based preprocessing. Unlike other\nsystems that contain safeguards, thresholds, and hierarchies,\nthe proposed system uses straightforward ﬁltering and a\nﬂat classiﬁer. As the next section shows, this simple pro-\ncedure can achieve state-of-the-art accuracy for instrument\nrecognition.n b t a n b t a\n1.2 0.200 1.000 1.5 0.500 1.000\n1.2 0.250 0.800 1.5 0.625 0.800\n1.2 0.333 0.600 1.5 0.833 0.600\n1.2 0.500 0.400 1.5 1.25 0.400\n1.2 1.00 0.200 1.5 2.50 0.200\n1.2 2.00 0.100 1.5 5.00 0.100\n1.2 4.00 0.050 1.5 10.0 0.050\n1.2 10.0 0.020 1.5 25.0 0.020\n2.0 1.00 1.000 3.0 2.00 1.000\n2.0 1.25 0.800 3.0 2.50 0.800\n2.0 1.67 0.600 3.0 3.33 0.600\n2.0 2.50 0.400 3.0 5.00 0.400\n2.0 5.00 0.200 3.0 10.0 0.200\n2.0 10.0 0.100 3.0 20.0 0.100\n2.0 20.0 0.050 3.0 40.0 0.050\n2.0 50.0 0.020 3.0 100 0.020\nTable 1. Gamma ﬁlterbank parameters used in the follow-\ning experiments.\n5. EXPERIMENTS\nWe perform experiments on an extensive set of isolated\nsounds. The data set for these experiments combines sam-\nples from the University of Iowa database of Musical In-\nstrument Samples [4], McGill University Master Samples\n[14], the OLPC Samples Collection [13], and the Freesound\nProject [12]. All of these samples consist of isolated sounds\ngenerated by real musical instruments. We have parsed the\naudio ﬁles such that each ﬁle consists of a single musical\nnote (for harmonic sounds) or beat (for percussive sounds).\nFrom each input signal, x(t), we obtain the magnitude\nspectrogram, X, via STFT using frames of length 46.4 ms\n(i.e., 2048/44100) windowed using a Hamming window\nand a hop size of 10.0 ms. Then, we perform NMF using\nthe Kullback-Leibler update rules [9] with an inner dimen-\nsion ofK= 1 to obtain AandS. When applicable, we\nuse a multiresolution gamma ﬁlterbank of thirty-two ﬁlters\nwith the parameters shown in Table 1. These attack times\nand decay rates cover a wide range of sounds produced by\ncommon musical instruments. Each 32-dimensional fea-\nture vector, z, is then classiﬁed.\nFor supervised classiﬁcation, we use the LIBSVM im-\nplementation [1] of the support vector machine (SVM) with\nthe radial basis kernel. For multiple classes, LIBSVM uses\nthe one-versus-one classiﬁcation strategy by default. The\nremaining programs and simulations were written entirely\nin Python using the SciPy package [8]. Source code is\navailable upon request.\nIn total, there are 3907 feature vectors collected among\ntwenty-four instrument classes. Table 2 summarizes this\ndata set. With few exceptions [3], this selection of instru-\nments is more comprehensive than any existing work on\nisolated instrument recognition. Recognition accuracy for\nclasscis deﬁned to be the percentage of the feature vec-\ntors whose true class is cthat are correctly classiﬁed by the\nSVM as belonging in class c. Overall recognition accuracy\nis the average of the accuracy rates for each class.\n438\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Instrument # S T ST\nBassoon 131 99.2 75.6 96.9\nClarinet 145 80.7 73.1 86.2\nFlute 236 84.7 60.6 89.0\nOboe 118 72.0 77.1 91.5\nSaxophone 196 93.4 65.8 86.7\nHorn 92 80.4 62.0 85.9\nTrombone 99 93.9 53.5 89.9\nTrumpet 236 97.5 82.2 97.9\nTuba 111 98.2 75.7 99.1\nCello 349 94.8 89.7 97.4\nViola 309 94.2 67.6 90.9\nViolin 390 97.2 86.2 96.2\nCello Pizz. 321 98.1 87.5 98.4\nViola Pizz. 254 99.6 81.9 99.6\nViolin Pizz. 315 97.5 85.4 99.0\nGlockensp. 10 100.0 90.0 100.0\nGuitar 27 51.9 29.6 63.0\nMarimba 39 46.2 25.6 79.5\nPiano 260 95.0 89.2 98.5\nXylophone 13 61.5 53.8 84.6\nKick 90 98.9 95.6 100.0\nSnare 86 96.5 88.4 98.8\nTimpani 47 85.1 61.7 87.2\nToms 33 100.0 90.9 100.0\nTotal 3907 88.2 72.9 92.3\nTable 2. Sample sizes and accuracy rates. S: spectral in-\nformation. T: temporal information. ST: spectral plus tem-\nporal information.\n5.1 Spectral Information\nAs a control experiment, we evaluate the classiﬁcation abil-\nity of spectral features using MFCCs. From each column\nofA, we extract 32 MFCCs with center frequencies log-\narithmically spaced over 5.3 octaves between 110 Hz and\n3951 Hz. From the 3907 32-dimensional feature vectors,\nwe evaluate classiﬁcation performance through ten-fold cross\nvalidation.\nFig. 6 illustrates the confusion matrix for this experi-\nment, and Table 2 shows the accuracy rates for each class.\nThe average of the 24 accuracy rates is 88.2%. We no-\ntice some understandable misclassiﬁcations. For example,\n18.5% of guitar samples are misclassiﬁed as cello pizzi-\ncato and 14.8% are misclassiﬁed as piano. 5.5% of clarinet\nsamples and 13.6% of oboe samples are misclassiﬁed as\nﬂute. 10.3% of marimba samples are misclassiﬁed as xy-\nlophone. In general, these spectral features can accurately\nclassify the drums, brass, and string instruments. However,\naccuracy is poor among the woodwinds and pitched per-\ncussive instruments. Some of these misclassiﬁcations are\ndue to an imbalance in the sample size of each class. De-\nspite its ability to improve the average accuracy rate, the\nreduction of class imbalance in supervised classiﬁcation is\nbeyond the scope of this paper.\n5.2 Temporal Information\nNext, we evaluate the classiﬁcation ability of temporal fea-\ntures using the proposed feature extraction algorithm with\nthe parameters shown in Table 1. One feature vector z\nis computed for each temporal NMF atom as described\nin Section 4. Like the previous experiment, we evaluate\nBassoon\nClarinet\nFlute\nOboe\nSaxophone\nHorn\nTrombone\nTrumpet\nTuba\nCello\nViola\nViolin\nCello Pizz\nViola Pizz\nViolin Pizz\nGlockenspiel\nGuitar\nMarimba\nPiano\nXylophone\nKick\nSnare\nTimpani\nToms\nBassoon\nClarinet\nFlute\nOboe\nSaxophone\nHorn\nTrombone\nTrumpet\nTuba\nCello\nViola\nViolin\nCello Pizz\nViola Pizz\nViolin Pizz\nGlockenspiel\nGuitar\nMarimba\nPiano\nXylophone\nKick\nSnare\nTimpani\nToms\n0.01\n0.10\n0.20\n0.40\n0.60\n0.80\n1.00Figure 6. Classiﬁcation accuracy using spectral informa-\ntion. Row labels: True class. Column labels: Estimated\nclass. Average accuracy: 88.2%.\nclassiﬁcation performance through ten-fold cross valida-\ntion among the 3907 32-dimensional feature vectors.\nTable 2 shows the accuracy rates for each class. The av-\nerage accuracy rate is 72.9%. Fig. 7 illustrates the confu-\nsion matrix for this experiment. We observe that temporal\nfeatures alone do not classify instruments as well as spec-\ntral features. Nevertheless, for 11 out of the 24 classes,\naccuracy remains above 80%. In particular, there are very\nfew misclassiﬁcations between percussion instruments and\nnon-percussion instruments. Most misclassiﬁcations occur\nwithin instrument families, e.g., cello and viola, bassoon\nand clarinet, and guitar and piano.\n5.3 Spectral Plus Temporal Information\nFinally, we evaluate the classiﬁcation performance when\nconcatenating spectral and temporal features. The features\nextracted during the previous two experiments are concate-\nnated to form 3907 64-dimensional feature vectors. Table\n2 shows the accuracy rates, and Fig. 8 illustrates the con-\nfusion matrix. The total accuracy rate is 92.3%. Temporal\ninformation improves classiﬁcation accuracy for 16 of the\n24 instrument classes along with the overall accuracy. Ac-\ncuracy improves most for the string pizzicato, percussion,\nbrass, and certain woodwind instruments. The remaining\nmisclassiﬁcations occur mostly within families, e.g., clar-\ninet and ﬂute, and guitar and piano. For isolated sounds,\nthis experiment veriﬁes the hypothesis that temporal infor-\nmation can improve instrument recognition accuracy over\nmethods that use only spectral information.\n6. CONCLUSION\nFrom the experiments, we conclude that a combination of\nspectral and temporal information can improve upon those\ninstrument recognition systems that only use spectral in-\nformation. The proposed method extracts temporal infor-\n439\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Bassoon\nClarinet\nFlute\nOboe\nSaxophone\nHorn\nTrombone\nTrumpet\nTuba\nCello\nViola\nViolin\nCello Pizz\nViola Pizz\nViolin Pizz\nGlockenspiel\nGuitar\nMarimba\nPiano\nXylophone\nKick\nSnare\nTimpani\nToms\nBassoon\nClarinet\nFlute\nOboe\nSaxophone\nHorn\nTrombone\nTrumpet\nTuba\nCello\nViola\nViolin\nCello Pizz\nViola Pizz\nViolin Pizz\nGlockenspiel\nGuitar\nMarimba\nPiano\nXylophone\nKick\nSnare\nTimpani\nToms\n0.01\n0.10\n0.20\n0.40\n0.60\n0.80\n1.00Figure 7. Classiﬁcation accuracy using temporal informa-\ntion. Row labels: True class. Column labels: Estimated\nclass. Average accuracy: 72.9%.\nmation using a multiresolution gamma ﬁlterbank which\nparameterizes each temporal dictionary atom by its most\nprominent attack times and decay rates. Like the cortical\nrepresentation, the spectral and temporal dictionary atoms\ngenerated by NMF provide a complete timbral representa-\ntion of musical sounds. However, unlike the cortical rep-\nresentation, each of these dictionary atoms typically rep-\nresent an individual musical note, thus facilitating music\ninstrument recognition further.\nWe have already begun an investigation of the proposed\nmethod for both solo melodic excerpts and polyphonic mix-\ntures. Also, because the proposed method classiﬁes each\nindividual NMF atom by instrument, we are investigating\nthe use of the proposed method for source separation by\ngrouping, emphasizing, or removing atoms that correspond\nto chosen instruments.\n7. REFERENCES\n[1] C.-C. Chang and C.-J. Lin, “LIBSVM: a library for\nsupport vector machines,” 2001–. [Online]. Available:\nhttp://www.csie.ntu.edu.tw/\u0018cjlin/libsvm\n[2] T. Chi, P. Ru, and S. A. Shamma, “Multiresolution spec-\ntrotemporal analysis of complex sounds,” J. Acoustical Soc.\nAmerica, vol. 118, no. 2, pp. 887–906, Aug. 2005.\n[3] A. Eronen, “Automatic musical instrument recognition,”\nMaster’s thesis, Tampere University of Technology, Oct.\n2001.\n[4] L. Fritts, “Musical Instrument Samples,” Univ. Iowa\nElectronic Music Studios, 1997–. [Online]. Available:\nhttp://theremin.music.uiowa.edu/MIS.html\n[5] F. Fuhrmann, M. Haro, and P. Herrera, “Scalability, genera-\nbility, and temporal aspects in automatic recognition of pre-\ndominant musical instruments in polyphonic music,” in Proc.\nIntl. Soc. Music Information Retrieval Conf. (ISMIR), 2009,\npp. 321–326.\nBassoon\nClarinet\nFlute\nOboe\nSaxophone\nHorn\nTrombone\nTrumpet\nTuba\nCello\nViola\nViolin\nCello Pizz\nViola Pizz\nViolin Pizz\nGlockenspiel\nGuitar\nMarimba\nPiano\nXylophone\nKick\nSnare\nTimpani\nToms\nBassoon\nClarinet\nFlute\nOboe\nSaxophone\nHorn\nTrombone\nTrumpet\nTuba\nCello\nViola\nViolin\nCello Pizz\nViola Pizz\nViolin Pizz\nGlockenspiel\nGuitar\nMarimba\nPiano\nXylophone\nKick\nSnare\nTimpani\nToms\n0.01\n0.10\n0.20\n0.40\n0.60\n0.80\n1.00Figure 8. Classiﬁcation accuracy using spectral plus tem-\nporal information. Row labels: True class. Column labels:\nEstimated class. Average accuracy: 92.3%.\n[6] P. Herrera-Boyer, A. Klapuri, and M. Davy, Signal Process-\ning Methods for Music Transcription. New York: Springer,\n2006, ch. 6, pp. 163–200.\n[7] A. Holzapfel and Y . Stylianou, “Musical genre classiﬁcation\nusing nonnegative matrix factorization-based features,” IEEE\nTrans. Audio, Speech, Language Processing, vol. 16, no. 2,\npp. 424–434, Feb. 2008.\n[8] E. Jones, T. Oliphant, P. Peterson et al., “SciPy: Open source\nscientiﬁc tools for Python,” 2001–. [Online]. Available:\nhttp://www.scipy.org\n[9] D. D. Lee and H. S. Seung, “Algorithms for non-negative\nmatrix factorization,” in Adv. Neural Information Processing\nSyst., vol. 13, Denver, 2001, pp. 556–562.\n[10] R. Lyon and S. Shamma, “Auditory representations of tim-\nbre and pitch,” in Auditory Computation, H. L. Hawkins, Ed.\nSpringer, 1996, ch. 6, pp. 221–270.\n[11] N. Mesgarani, M. Slaney, and S. A. Shamma, “Discrimina-\ntion of speech from nonspeech based on multiscale spectro-\ntemporal modulations,” IEEE Trans. Audio, Speech, Lan-\nguage Processing, vol. 14, no. 3, pp. 920–930, May 2006.\n[12] “Freesound Project,” Music Technology Group, Univ.\nPompeu Fabra. [Online]. Available: http://www.freesound.\norg\n[13] “Free Sound Samples – OLPC,” One Laptop per Child.\n[Online]. Available: http://wiki.laptop.org/go/Sound samples\n[14] F. Opolko and J. Wapnick, “McGill University Master Sam-\nples,” McGill Univ., 1987.\n[15] P. Smaragdis and J. C. Brown, “Non-negative matrix factor-\nization for polyphonic music transcription,” in Proc. IEEE\nWorkshop on Appl. Signal Processing to Audio and Acous-\ntics, New Paltz, NY , Oct. 2003, pp. 177–180.\n[16] T. Virtanen, “Monaural sound source separation by nonnega-\ntive matrix factorization with temporal continuity and sparse-\nness criteria,” IEEE Trans. Audio, Speech, and Language Pro-\ncessing, vol. 15, no. 3, pp. 1066–1074, Mar. 2007.\n440\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "Improving the Generation of Ground Truths Based on Partially Ordered Lists.",
        "author": [
            "Julián Urbano",
            "Mónica Marrero",
            "Diego Martín 0001",
            "Juan Lloréns"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1417525",
        "url": "https://doi.org/10.5281/zenodo.1417525",
        "ee": "https://zenodo.org/records/1417525/files/UrbanoMML10.pdf",
        "abstract": "Ground truths based on partially ordered lists have been used for some years now to evaluate the effectiveness of Music Information Retrieval systems, especially in tasks related to symbolic melodic similarity. However, there has been practically no meta-evaluation to measure or improve the correctness of these evaluations. In this paper we revise the methodology used to generate these ground truths and disclose some issues that need to be addressed. In particular, we focus on the arrangement and aggrega- tion of the relevant results, and show that it is not possi- ble to ensure lists completely consistent. We develop a measure of consistency based on Average Dynamic Re- call and propose several alternatives to arrange the lists, all of which prove to be more consistent than the original method. The results of the MIREX 2005 evaluation are revisited using these alternative ground truths.",
        "zenodo_id": 1417525,
        "dblp_key": "conf/ismir/UrbanoMML10",
        "keywords": [
            "Ground truths",
            "Music Information Retrieval",
            "Symbolic melodic similarity",
            "Meta-evaluation",
            "Partially ordered lists",
            "Revision of methodology",
            "Issues addressed",
            "Consistency measurement",
            "Alternative arrangements",
            "MIREX 2005 evaluation"
        ],
        "content": "IMPROVING THE GENERATION OF GROUND TRUTHS \nB\nASED ON PARTIALLY ORDERED LISTS \nJulián Urbano, Mónica Marrero, Diego Martín and Juan Lloréns \nUniversity Carlos III of Madrid \nDe\npartment of Computer Science \n{jurbano, mmarrero, dmandres, llorens}@inf.uc3m.es  \nABSTRACT \nG\nround truths based on partially ordered lists have been \nused for some years now to evaluate the effectiveness of \nMusic Information Retrieval systems, especially in tasks \nrelated to symbolic melodic similarity. However, there \nhas been practically no meta-evaluation to measure or \nimprove the correctness of these evaluations. In this paper \nwe revise the methodology used to generate these ground \ntruths and disclose some issues that need to be addressed. \nIn particular, we focus on the arrangement and aggrega-\ntion of the relevant results, and show that it is not possi-\nble to ensure lists completely consistent. We develop a \nmeasure of consistency based on Average Dynamic Re-\ncall and propose several alternatives to arrange the lists, \nall of which prove to be more consistent than the original \nmethod. The results of the MIREX 2005 evaluation are \nrevisited using these alternative ground truths. \n1. INTRODUCTION \nInformation Retrieval (IR) is known for having evolved \nas a highly experimental discipline. New techniques ap-\npear every year, and it is necessary to perform an exhaus-\ntive and methodological evaluation to figure out which of \nthese techniques really mean a step forward in the field. \nThese evaluations have been carried out since the late \n50's in what has come to be known as the Cranfield para-\ndigm. Given a fixed document collection, IR systems \nprovide their results for certain information needs. Then, \nthey are evaluated against the so called ground truths, \nwhich contain information about the documents that \nshould ideally be retrieved by a system. Usually, these \nground truths take the form of a matrix, containing the \nrelevance, assessed by humans, for each document to an \ninformation need (traditional values are \"irrelevant\", \"re-\nlevant\" and \"highly relevant\"). \nThese evaluations have been carried out mostly in Text \nInformation Retrieval, with the TREC conferences as its \nflagship [1]. Music Information Retrieval (MIR), on the \nother hand, is a relatively young discipline, and this kind \nof evaluations has been somewhat scarce until the arrival \nof MIREX in 2005 as a first attempt to perform TREC-\nlike evaluations in the musical domain [2]. Music IR dif-fers from Text IR in many aspects [3], making the con-\nstruction and maintenance of such test collections very \ndifficult. In particular, it is unclear what relevance level \nto assign to a document for a given information need. \nIn the case of melodic similarity, some studies indicate \nthat relevance is continuous [4]. Single melodic changes \nsuch as moving a note up or down in pitch, or extending \nor shortening its duration, are not perceived to change the \noverall melody. Nonetheless, the relationship with the \noriginal melody is gradually weaker as more changes are \napplied to it. There does not seem to be common criteria \nto split the degree of relevance into different levels, so \nassessments with a fixed scale seem inappropriate.  \nGround truths based on partially ordered lists at-\ntempted to handle this problem with relevance assessment \nby the beginning of 2005 [5]. Instead of having docu-\nments with a fixed relevance level, these ground truths \nare lists with ordered groups of documents. The earlier a \ngroup appears in the list, the more relevant its documents \nare, and documents within the same group are assumed to \nbe equally relevant. That way, the ideal retrieval should \nreturn these documents in order of relevance, although \npermutations within the same group are allowed. Because \ntraditional effectiveness measures such as precision or \nrecall need relevance assessments with a fixed scale, a \nnew measure, called Average Dynamic Recall (ADR) [6], \nwas developed also in 2005 to evaluate retrieval systems \nwith ground truths based on partially ordered lists. \nThe first edition of MIREX had a task for symbolic \nmelodic similarity [7], where 11 ground truths based on \npartially ordered lists were used along with ADR to eva-\nluate state-of-the-art retrieval systems. Similar methods \nwere used in the 2006 and 2007 editions, as well as in \nprivate evaluations external to MIREX, such as [8] [9] \n[10] and [11]. However, we are not aware of any meta-\nevaluation work addressing the correctness or improve-\nment of these ground truths. Indeed, a thorough examina-\ntion shows that the lists have some inconsistencies as to \nthe arrangement and aggregation of documents in groups. \nThe paper is organized as follows. In Section 2 we re-\nview the methodology followed to create these ground \ntruths. Section 3 unveils some inconsistencies and shows \nthat it is not possible to ensure fully-consistent lists. In \nSection 4 we propose several alternatives to set up the \ngroups, and present a measure to quantify consistency. \nSection 5 shows the results of the alternatives proposed \nand revise the MIREX 2005 evaluation using them. The \npaper ends with conclusions and lines for future work. Permission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fee provided that copies \nare not made or distributed for profit or commercial advantage and that \ncopies bear this notice and the full citation on the first page.  \n© 2010 International Society for Music Information Retrieval  \n285\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \n 2\n. CURRENT METHODOLOGY \nThe original method to create ground truths based on par-\ntially ordered lists, as described in [5], was used with the \nRISM A/II collection [12], which at the time contained \nabout half a million musical incipits. The methodology \nfollowed may be divided in four steps: filtering, ranking, \narranging and aggregating. \nFirst, several features were calculated for each docu-\nment (musical incipits in this case), such as pitch range, \ninterval histogram or motive repetitions. Filtering by \nthese features, the initial collection was gradually nar-\nrowed down to under 300 incipits per query. Then, clear-\nly irrelevant incipits were manually excluded, and several \nmelodic similarity algorithms were used to add supposed-\nly relevant incipits. Second, and once the lists had about \n50 candidate incipits each, 35 experts ranked them in \nterms of melodic similarity to the corresponding query: \nthe more similar a candidate was to the query, the higher \nit had to be ranked in the list. Incipits that seemed very \ndifferent from the query could be left unranked. Third, \nincipits were arranged according to the median of their \nrank sample. If two incipits had the same median rank, \nthe means were used to resolve the tie. Therefore, the in-\ncipits that on average were ranked higher by the experts \nappeared with higher ranks in the ordered list. Fourth, in-\ncipits whose rank samples were similar were aggregated \nwithin a group, so as to indicate that they were similarly \nrelevant to the query. Thus, a retrieval system could re-\nturn them with their rank swapped and still be considered \ncorrect. The Mann-Whitney U test (also known as Wil-\ncoxon Rank Sum test, see Appendix) [13], was used to \ntell whether two incipits had similar ranks or not. \n \nFigure 1. F irst three results for query 000.111.706-1.1.1. Top to bottom: \nsame incipit as the query, incipit 000.113.506-1.1.1 and incipit \n000.116.073-1.1.1.  \nThe ground truths generated have some odd results, as \na\nlready noted in [5] and [9]. For example, in the list for \nthe query incipit 270.000.749-1.19.1, the first result is the \nsame as the query; the second one (incipit 270.000.746-\n1.41.1) is written with a different clef, but otherwise iden-\ntical to the query; and the third result (incipit \n270.000.748-1.19.1) is the same as the first half of the \nquery. Although the experts were told to disregard these \nkinds of changes in the melody, these three results ended \nup in different groups, indicating that their relevances to \nthe query were significantly different. Also, incipits with \nvirtually the same changes in the melody were sometimes \nplaced in different groups, as it occurs with incipits \n000.113.506-1.1.1 and 000.116.073-1.1.1 with respect to \nthe query 000.111.706-1.1.1 (see Figure 1). \nThese rare results seem to be caused by the second \nstep of the methodology, when experts ranked the results. Though important, we will not focus on them in this pa-\nper. There are other problems with this kind of ground \ntruths that have not been addressed yet and lead to incon-\nsistent result lists and incorrect evaluation. These incon-\nsistencies arise at steps three and four, and they are the \nones we address here. \n3. INCONSISTENCIES DUE TO \nARRANGEMENT AND AGGREGATION \nWe thoroughly examined the 11 ground truth lists used in \nthe evaluation of the symbolic melodic similarity task in \nMIREX 2005 ( Eval05 for short), and found that there are \npairs of incipits contained in the same group of relevance \nalthough there is a significant difference between the \nranks the experts gave them (i.e. an intra-group inconsis-\ntency). For example, incipits 453.001.547-1.1.3 and \n451.509.336-1.1.1, for query 190.011.224-1.1.1, are in \nthe same group (see Figure 2), but their difference is sig-\nnificant. That means that if a retrieval system returned \nthem in reverse order it would be considered correct, de-\nspite the experts clearly ranked them differently. On the \nother hand, incipits for which no significant difference \ncould be found form part of different groups (i.e. an inter-\ngroup inconsistency). Incipits 700.000.686-1.1.1 and \n450.034.972-1.1.1 for the previous query are an example \n(see Figure 2). Similarly, if a retrieval system returned \nthem in reverse order, it would not be considered correct, \ndespite no difference was found in the experts rankings. \nThese inconsistencies appear throughout the lists, and \nthey are caused by the initial arrangement and the aggre-\ngation function used in the third and fourth steps. \n3.1 Arrangement \nIn the third step of the methodology, incipits are arranged \naccording to the median and mean ranks they were given \nby the experts. Because the Mann-Whitney U test is used \nlater on to find statistically significant differences be-\ntween the incipits’ ranks, using central-tendency meas-\nures such as the median and the mean might not be ap-\npropriate to arrange the results, because they do not ac-\ncount for the dispersion in the samples. \nAlthough rare, this phenomenon may happen: we ex-\namined the 11 ground truths of the Eval05 collection and \nfound it. For example, incipits 850.014.902-1.1.1 and \n451.002.538-1.1.1 are ranked 20th and 22nd, respective-\nly, for query 400.065.784-1.1.1. Their sample median \nranks are 12 and 12.5, so the first one is ranked higher. \nHowever, a 1-tailed Mann-Whitney U test shows that it is \nhighly probable for the true medians to be ordered the \nother way around, so the second incipit should be ranked \nhigher than the first one. \n3.2 Aggregation \nIn the fourth step of the methodology, incipits are aggre-\ngated in groups according to their relevance to the query. \nThe rationale originally used by the aggregation function \nis as follows: traverse from top to bottom the list of inci-\npits already arranged by median and mean, and begin a \n286\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)X \n \n new relevance group if the \ndifferent from \nfore, it will probably allow \nin the beginning and the end of the \ncause they are not different \ngroup will begin only \nly different from \nlikely to grow a lot.\ntency in the 11 lists of the \nthat out of the\nsame relevance group, \nent. All these \nincorrect evaluation when allowing an incipit to appear \nearlier in the results list \nsame group as\nThere can also be cases where the aggregation function\nplaces an incipit in a new group, but the \nsignificantly different \nfinished. This next incipit should be in the previous \ngroup, but that is not possible since it has been already \nclosed because of the previous \n453.001.547- 1.1.3 started group 4 for the query \n190.011.224- 1.1.1, because it was different \npits in group 3. However, the incipit 700.000.686\nin group 3, is not significantly different \n450.034.972- 1.1.1, which is in group 4\nthese inter- group inconsistent pairs also translate to inco\nrect evaluation\nearlier in the results for being misplaced in a later group \nstarted by another incipit that was sufficiently different.\n3.3 Fully Consistent \nInconsistencies come from two different sources: the in\ntial arrangemen\nof documents in the wrong order, and the aggregation \nfunction may combine significantly different incipits or \nset apart similar ones. The aggregation function can mit\ngate these problems, but \nlem: hypoth esis testing is not transitive.\nLet X, Y and \nferent inci pits\nthat the median of \nthe median of \nsuggest that the median\nX < Y, Y < Z \npletely paradoxical, it actually happens\nthe ground truth \nand Z be the rank samples of incipits 702.002.512\n804.002.648- 1.1.2 and 450.021.643\nand 16th respectively. A 1\nshows that the median of \nthe one of Y \nof Y is significantly smaller than the one of \n0.239), but the median rank of \ncantly smaller than the one of \nthough p- values this large would not usually be accepted \nto reject a null hypothesis, they are valid in our case, \nsince the significance level new relevance group if the \nfrom all incipits in the current group\n, it will probably allow \nin the beginning and the end of the \ncause they are not different \ngroup will begin only when a\nfrom all the previous ones, so the group is \nlikely to grow a lot.  We looked for this kind of inconsi\ntency in the 11 lists of the \nout of the  total 509 ordered pairs of incipi\nsame relevance group, 178\nhese intra-group inconsistent pairs translate \nincorrect evaluation when allowing an incipit to appear \nin the results list just \nsame group as  another one  \nThere can also be cases where the aggregation function\nplaces an incipit in a new group, but the \nsignificantly different from \nThis next incipit should be in the previous \ngroup, but that is not possible since it has been already \nclosed because of the previous \n1.1.3 started group 4 for the query \n1.1.1, because it was different \npits in group 3. However, the incipit 700.000.686\nin group 3, is not significantly different \n1.1.1, which is in group 4\ngroup inconsistent pairs also translate to inco\nrect evaluation  when not permitting an incipit to appear \nearlier in the results for being misplaced in a later group \nstarted by another incipit that was sufficiently different.\nonsistent Lists\nInconsistencies come from two different sources: the in\ntial arrangemen t by median and mean may arrange pairs \nof documents in the wrong order, and the aggregation \nfunction may combine significantly different incipits or \nset apart similar ones. The aggregation function can mit\ngate these problems, but there is\nesis testing is not transitive.\nand Z be the rank samples given to three di\npits. The Mann\nthat the median of Y is less than the median of \nthe median of X is less than the median of \nsuggest that the median s of \nZ but X = Z). Although this might seem co\npletely paradoxical, it actually happens\nthe ground truth list for query 400.065.784\nbe the rank samples of incipits 702.002.512\n1.1.2 and 450.021.643\nand 16th respectively. A 1\nshows that the median of \n (p-value = 0.238) and t\nis significantly smaller than the one of \n0.239), but the median rank of \ncantly smaller than the one of \nvalues this large would not usually be accepted \nnull hypothesis, they are valid in our case, \nsignificance level new relevance group if the pivot incipit is significantl\nall incipits in the current group\n, it will probably allow significantly different \nin the beginning and the end of the same \ncause they are not different from a third one.\nwhen an incipit is very significan\nall the previous ones, so the group is \nWe looked for this kind of inconsi\ntency in the 11 lists of the Eval05 collection\nordered pairs of incipi\n178 (35%) are significantly diffe\ninconsistent pairs translate \nincorrect evaluation when allowing an incipit to appear \njust for being \n ranked a little\nThere can also be cases where the aggregation function\nplaces an incipit in a new group, but the \nfrom some others in the group just \nThis next incipit should be in the previous \ngroup, but that is not possible since it has been already \nclosed because of the previous one. For example, \n1.1.3 started group 4 for the query \n1.1.1, because it was different \npits in group 3. However, the incipit 700.000.686\nin group 3, is not significantly different \n1.1.1, which is in group 4  (see Figure 2)\ngroup inconsistent pairs also translate to inco\nwhen not permitting an incipit to appear \nearlier in the results for being misplaced in a later group \nstarted by another incipit that was sufficiently different.\nists \nInconsistencies come from two different sources: the in\nt by median and mean may arrange pairs \nof documents in the wrong order, and the aggregation \nfunction may combine significantly different incipits or \nset apart similar ones. The aggregation function can mit\nthere is a more profound \nesis testing is not transitive.  \nbe the rank samples given to three di\n. The Mann -Whitney U test may suggest \nis less than the median of \nis less than the median of \nof X and Z are not different\n. Although this might seem co\npletely paradoxical, it actually happens\nfor query 400.065.784\nbe the rank samples of incipits 702.002.512\n1.1.2 and 450.021.643 -1.1.1, ranked 6th, 8th \nand 16th respectively. A 1 -tailed Mann\nshows that the median of X is significantly smaller than \nvalue = 0.238) and t hat the median rank \nis significantly smaller than the one of \n0.239), but the median rank of X does not seem signif\ncantly smaller than the one of Z (p-value = 0.272). A\nvalues this large would not usually be accepted \nnull hypothesis, they are valid in our case, \nsignificance level originally used was 0.25 incipit is significantl\nall incipits in the current group  [5]. Ther\nsignificantly different incipits \nsame group, just b\na third one.  A new \nis very significan\nall the previous ones, so the group is \nWe looked for this kind of inconsi\ncollection , and found \nordered pairs of incipi ts in the \nsignificantly diffe\ninconsistent pairs translate \nincorrect evaluation when allowing an incipit to appear \nfor being misplaced in the \na little higher. \nThere can also be cases where the aggregation function\nplaces an incipit in a new group, but the next one is not \nsome others in the group just \nThis next incipit should be in the previous \ngroup, but that is not possible since it has been already \nFor example, incipit \n1.1.3 started group 4 for the query \n1.1.1, because it was different from all inc\npits in group 3. However, the incipit 700.000.686 -1.1.1, \nin group 3, is not significantly different from incipit \n(see Figure 2) . \ngroup inconsistent pairs also translate to inco\nwhen not permitting an incipit to appear \nearlier in the results for being misplaced in a later group \nstarted by another incipit that was sufficiently different.\nInconsistencies come from two different sources: the in\nt by median and mean may arrange pairs \nof documents in the wrong order, and the aggregation \nfunction may combine significantly different incipits or \nset apart similar ones. The aggregation function can mit\na more profound pro\n \nbe the rank samples given to three di\nWhitney U test may suggest \nis less than the median of Z, and that \nis less than the median of Y. Still, it may \nare not different  \n. Although this might seem co\npletely paradoxical, it actually happens , for example\nfor query 400.065.784 -1.1.1. Let \nbe the rank samples of incipits 702.002.512 -1.1.1, \n1.1.1, ranked 6th, 8th \ntailed Mann -Whitney U test \nis significantly smaller than \nhat the median rank \nis significantly smaller than the one of Z (p-value = \ndoes not seem signif\nvalue = 0.272). A\nvalues this large would not usually be accepted \nnull hypothesis, they are valid in our case, \noriginally used was 0.25 [5]incipit is significantl y \nThere-\nincipits \ngroup, just b e-\nA new \nis very significan t-\nall the previous ones, so the group is \nWe looked for this kind of inconsi s-\n, and found \nts in the \nsignificantly diffe r-\ninconsistent pairs translate into \nincorrect evaluation when allowing an incipit to appear \nin the \nThere can also be cases where the aggregation function  \nis not \nsome others in the group just \nThis next incipit should be in the previous \ngroup, but that is not possible since it has been already \nincipit \n1.1.3 started group 4 for the query \nall inci-\n1.1.1, \nincipit \n. All \ngroup inconsistent pairs also translate to inco r-\nwhen not permitting an incipit to appear \nearlier in the results for being misplaced in a later group \nstarted by another incipit that was sufficiently different.  \nInconsistencies come from two different sources: the in i-\nt by median and mean may arrange pairs \nof documents in the wrong order, and the aggregation \nfunction may combine significantly different incipits or \nset apart similar ones. The aggregation function can mit i-\nprob-\nbe the rank samples given to three di f-\nWhitney U test may suggest \n, and that \n. Still, it may \n (i.e. \n. Although this might seem co m-\n, for example  in \n1.1.1. Let X, Y \n1.1.1, \n1.1.1, ranked 6th, 8th \nWhitney U test \nis significantly smaller than \nhat the median rank \nvalue = \ndoes not seem signif i-\nvalue = 0.272). A l-\nvalues this large would not usually be accepted \nnull hypothesis, they are valid in our case, \n[5]. A: 700.000.686\nB: 453.001.547\nC: 450.034.972\nD: 451.509.336\nFigure \nAccording to the experts: \ngroup inconsistency), \nTherefore, it is \nlists with this method\nshould be in a group ranked higher than \nbe in a group ranked higher than \nshould be in the same group, which is clearly impossible.\nSimila\nexample in \n4. \nThe \npend\npermissive, l\nwith \nfunction too restrictive leads to \nlikelihood of \nfunction should minimize these pro\nlists as consistent as possible\nWe consider three different rationales to be followed\nAt this point, we have to consider whether 2\n1-tailed te\nwere used, looking for 2\nranks. \ndian and mean, we believe the tests should be 1\nlooking for 1\nincipits in step three, we may assume that an incipit a\npearing after another one has a rank either lower or equ\nbut not higher. In these situations, 1\npowerful than their 2\nprobable for them to find a difference between two sa\nples if there really is one\nTherefore, we obtain six different f\ning each of the three rationales with each of the two st\ntistical tests. We call these functions \nAny\noriginally used \nare pr\nA: 700.000.686 -1.1.1 \nB: 453.001.547 -1.1.3 \nC: 450.034.972 -1.1.1 \nD: 451.509.336 -1.1.1 \nFigure 2. Excerpt of the ground truth for query\nAccording to the experts: \ngroup inconsistency), \nTherefore, it is \nlists with this method\nshould be in a group ranked higher than \nbe in a group ranked higher than \nshould be in the same group, which is clearly impossible.\nSimilar cases can be found with 2\nexample in Figure 2\n ALTERNATIVE AGGREGAT\nThe number of intra\npend on the aggregation function used. A function too \npermissive, l ike the original one, leads to \nwith more likelihood of\nfunction too restrictive leads to \nlikelihood of inter\nfunction should minimize these pro\nlists as consistent as possible\nWe consider three different rationales to be followed\n• All: a new group is started if the \nsignificantly different \ncurrent group. This should lead to larger groups.\n• Any: a new group is started if the \nsignificantly different \nrent group. This should lead to smaller groups.\n• Prev: a new group is started if t\nsignificantly different \nAt this point, we have to consider whether 2\ntailed tests should be used\nwere used, looking for 2\nranks. But, because the i\ndian and mean, we believe the tests should be 1\nlooking for 1 -way differences in ranks. After arranging \nincipits in step three, we may assume that an incipit a\npearing after another one has a rank either lower or equ\nbut not higher. In these situations, 1\npowerful than their 2\nprobable for them to find a difference between two sa\nples if there really is one\nTherefore, we obtain six different f\ning each of the three rationales with each of the two st\ntistical tests. We call these functions \nAny-1, Prev-2 and \noriginally used by Typke et.al. \nare proposed in this paper.\nQuery : 190.011.224\nGroup 3\nGroup 4\nExcerpt of the ground truth for query\nAccording to the experts: B≠D (intra -\ngroup inconsistency), A≠B and B=C (2\nTherefore, it is not possible \nlists with this method . In the example above, incipit \nshould be in a group ranked higher than \nbe in a group ranked higher than \nshould be in the same group, which is clearly impossible.\nr cases can be found with 2\nFigure 2 (X ≠ Y, X = Z\nALTERNATIVE AGGREGAT\nnumber of intra - and inter\non the aggregation function used. A function too \nike the original one, leads to \nlikelihood of  intra-group inconsistencies, but a \nfunction too restrictive leads to \ninter-group inconsistencies.\nfunction should minimize these pro\nlists as consistent as possible . \nWe consider three different rationales to be followed\n: a new group is started if the \nsignificantly different \ncurrent group. This should lead to larger groups.\n: a new group is started if the \nsignificantly different from any\nrent group. This should lead to smaller groups.\n: a new group is started if t\nsignificantly different from \nAt this point, we have to consider whether 2\nsts should be used\nwere used, looking for 2 -way differences in median \n, because the i ncipits are already sorted by m\ndian and mean, we believe the tests should be 1\nway differences in ranks. After arranging \nincipits in step three, we may assume that an incipit a\npearing after another one has a rank either lower or equ\nbut not higher. In these situations, 1\npowerful than their 2 -tailed counterparts, so it is more \nprobable for them to find a difference between two sa\nples if there really is one  (see Appendix)\nTherefore, we obtain six different f\ning each of the three rationales with each of the two st\ntistical tests. We call these functions \nand Prev-1. Note that \nby Typke et.al. \noposed in this paper.  \n190.011.224 -1.1.1 \nGroup 3 \nGroup 4 \nExcerpt of the ground truth for query  190.011.224\n-group inconsistency), \n(2-tailed non- transitivity as \npossible to ensure fully consistent \n. In the example above, incipit \nshould be in a group ranked higher than Y, \nbe in a group ranked higher than Z. However, \nshould be in the same group, which is clearly impossible.\nr cases can be found with 2 -tailed tests, such as the \nX = Z but Y = Z\nALTERNATIVE AGGREGAT ION FUNCTIONS\nand inter-group inconsistencies d\non the aggregation function used. A function too \nike the original one, leads to \ngroup inconsistencies, but a \nfunction too restrictive leads to smaller groups with \ngroup inconsistencies.  The aggregation \nfunction should minimize these pro blems and generate \n \nWe consider three different rationales to be followed\n: a new group is started if the pivot\nsignificantly different from every incipit in the \ncurrent group. This should lead to larger groups.\n: a new group is started if the pivot\nfrom any incipit in the cu\nrent group. This should lead to smaller groups.\n: a new group is started if t he pivot\nfrom the previous one.\nAt this point, we have to consider whether 2\nsts should be used . Originally, 2\nway differences in median \nncipits are already sorted by m\ndian and mean, we believe the tests should be 1\nway differences in ranks. After arranging \nincipits in step three, we may assume that an incipit a\npearing after another one has a rank either lower or equ\nbut not higher. In these situations, 1 -tailed tests are more \ntailed counterparts, so it is more \nprobable for them to find a difference between two sa\n(see Appendix) . \nTherefore, we obtain six different f unctions, combi\ning each of the three rationales with each of the two st\ntistical tests. We call these functions All-2,\nNote that All-2 is the function \nby Typke et.al. [5], while the other five \n \n \n190.011.224 -1.1.1\ngroup inconsistency), A=C (inter\ntransitivity as A=C). \nto ensure fully consistent \n. In the example above, incipit X\n, which should \n. However, X and Z\nshould be in the same group, which is clearly impossible.\ntailed tests, such as the \nY = Z). \nION FUNCTIONS\ngroup inconsistencies d e-\non the aggregation function used. A function too \nike the original one, leads to larger groups \ngroup inconsistencies, but a \nsmaller groups with more \nhe aggregation \nblems and generate \nWe consider three different rationales to be followed : \npivot incipit is \nevery incipit in the \ncurrent group. This should lead to larger groups.  \npivot incipit is \nincipit in the cu r-\nrent group. This should lead to smaller groups.  \npivot incipit is \nthe previous one.  \nAt this point, we have to consider whether 2 -tailed or \n. Originally, 2 -tailed tests \nway differences in median \nncipits are already sorted by m e-\ndian and mean, we believe the tests should be 1 -tailed, \nway differences in ranks. After arranging \nincipits in step three, we may assume that an incipit a p-\npearing after another one has a rank either lower or equ al, \ntailed tests are more \ntailed counterparts, so it is more \nprobable for them to find a difference between two sa m-\nunctions, combi n-\ning each of the three rationales with each of the two st a-\n, All-1, Any-2\nis the function \n, while the other five \n \n \n \n1.1.1. \n(inter-\n \nto ensure fully consistent \nX \nshould \nZ \nshould be in the same group, which is clearly impossible.  \ntailed tests, such as the \nION FUNCTIONS  \ne-\non the aggregation function used. A function too \nlarger groups \ngroup inconsistencies, but a \nmore \nhe aggregation \nblems and generate \n \nincipit is \nevery incipit in the \nincipit is \nr-\nincipit is \ntailed or \ntailed tests \nway differences in median \ne-\ntailed, \nway differences in ranks. After arranging \np-\nal, \ntailed tests are more \ntailed counterparts, so it is more \nm-\nn-\na-\n2, \nis the function \n, while the other five \n287\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \n 4\n.1 Measure of list consistency \nTo evaluate the 5 alternative functions presented above, \nand compare them with the original one, we developed a \nmeasure of consistency based on Average Dynamic Re-\ncall [6]. ADR is the main effectiveness measure used to \nevaluate retrieval systems against ground truths based on \npartially ordered lists, so we followed its same idea to \nmeasure their consistency, and hence the correctness of \nthe evaluation itself. ADR measures the average recall \nover the first n documents, where n is the number of doc-\numents in the ground truth. At each point, the set of rele-\nvant documents allowed comprises all previous docu-\nments in the list plus all those in the same group as the \npivot, because they are supposed to be equally relevant. \nWith a ground truth list like 〈(A, B), (C), (D, E, F) 〉, \nand a retrieval list such as 〈B, C, A, G, H, D 〉. ADR would \nbe calculated as in Table 1. In the first two positions, ei-\nther document A or B is considered correct because they \nare in the same relevance group, so both of them can be \nexpected. At position 3, both A and B are expected be-\ncause they appear before in the list, and only C is added \nwhen expanding the second group. However, when posi-\ntion 4 is reached, every document in the third group may \nbe expected. Recall is calculated at each position, and the \noverall ADR is the mean average of these recalls, 0.753 \nin this case. \nPosition Retrieved Expected Correct  Recall \n1 B A,B 1 1 \n2 B,C A, B 1 0.5 \n3 B,C,A A,B,C 3 1 \n4 B,C,A,G A,B,C,D,E,F 3 0.75 \n5 B,C,A,G,H A,B,C,D,E,F 3 0.6 \n6 B,C,A,G,H,D A,B,C,D,E,F 4 0.667 \nTable 1. Example of ADR calculation. \nTo measure the consistency, the list is traversed from \nt\nop to bottom, expanding the group corresponding to the \npivot incipit. At each position, it is calculated the percen-\ntage of incipits expanded that are actually correct accord-\ning to the experts rankings. At the end, the mean of those \npercentages is calculated. Therefore, a final value of 1 \nmeans that every expansion is correct and hence the list is \nfully-consistent. A value of 0 means that every expansion \nis incorrect. The pivot incipit is never considered for the \ncalculation, because it will always be correctly expanded. \nThere are two types of incorrect expansion: false posi-\ntives (i.e. an incipit is included in the set of expected, but \nit is significantly different from the pivot) and false nega-\ntives (i.e. an incipit is not included in the set of expected, \nbut it is not significantly different from the pivot). Note \nthat false positives correspond to intra-group inconsisten-\ncies, and false negatives correspond to inter-group incon-\nsistencies. In the example above, imagine A and C are not \nsignificantly different, but D and F are. In that case, the \nexpansion at position 1 is missing incipit C (a false nega-\ntive due to an inter-group inconsistency between groups 1 \nand 2). Also, the expansion at position 4 would incorrect-\nly include incipit F (a false positive due to an intra-group inconsistency in group 3). Note that at position 2, C \nwould not be correctly expanded, as it is still significantly \ndifferent from B. Note also that position 6 is not consi-\ndered, as there is actually no expansion at the end of the \nlist. In this case, the overall list consistency would be \n0.86 (see Table 2). \nPosition Correct \nexpansion Actual \nexpansion % of correct  \nexpansions \n1 B, C B 0.5 \n2 A A 1 \n3 A,B A,B 1 \n4 A,B,C,E A,B,C,E, F 0.8 \n5 A,B,C,D,F A,B,C,D,F 1 \nTable 2. Example of list consistency calculation. \nAs before, we can measure the inconsistencies using \nb\noth 2-tailed and 1-tailed tests. In the former, two incipits \nare expected to be in the same expanded set if a 2-tailed \nMann-Whitney U test is not rejected. In the latter, they \nare expected to be in the same expanded set if none of the \ntwo 1-tailed tests is rejected (i.e. the true median rank of \none incipit seems to be neither less nor greater than the \nother’s). Note that both the 2-tailed and the 1-tailed \nmeasures account for inconsistencies originated by the \naggregation function but only the 1-tailed version ac-\ncounts for inconsistencies due to the simple arrangement \nby median and mean. We call these two measures ADR-2 \nand ADR-1 consistency. \nBecause of the non-transitivity problem, lists are not \nexpected to have an overall consistency of 1. However, it \ncould be maximized by changing the aggregation func-\ntion, thus improving the correctness of the evaluation. \n5. RESULTS \nThe five alternative aggregation functions proposed back \nin Section 4 were used to re-generate the 11 lists in the \nEval05 collection and compare them with the original \nfunction All-2. We used the ADR-1 consistency measure \nto calculate the overall consistency of each list. The re-\nsults are in Figure 3 and in Table 3. \n \nFigure 3 .  ADR-1 consistency for the six aggregation functions. \nSolid circles indicate the mean value. Notches mark the 95% confidence \ninterval around the median. \nAs can be seen, the original function, Al l-2, is outper-\nformed by all of the five alternatives proposed. All-2 \nleads to an average consistency of 0.844, which is the All-2 Any-2 Prev-2 All-1 Any-1 Prev-10.70 0.75 0.80 0.85 0.90 0.95 1.00\nAggregation functionADR-1 consistency\n288\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \n sm\nallest of the six. However, Prev-2 and All-1 are not \nsignificantly better according to a 1-tailed t-test at the \n0.10 significance level. Moreover, the All functions lead \nto results with more variability, while the Any functions \nare more stable in terms of consistency. These results in-\ndicate that if the lists were generated with the Any-2, Any-\n1 or Prev-1 aggregation functions, they would be more \nconsistent, and so would be the evaluation with them. \nInterestingly, the relative order for each of the three 2-\ntailed and 1-tailed functions is maintained. That is, the All \nfunctions perform the worst, followed by Prev and Any, \nwhich perform the best both in terms of average consis-\ntency and variability. \nOur guess back in Section 3.2 was that the larger the \nsizes of the relevance groups are, the more inconsistent \nthe lists are too. To examine this, we calculated the mean \nnumber of incipits per group for each of the 11 resultant \nlists. Figure 4 and Table 3 show the results. \n \nFigure 4. M ean number of incipits per group for each aggregation func-\ntion. Solid circles indicate the mean value. Notches mark the 95% con-\nfidence interval around the median. \nAs expected, the Al l functions lead to larger groups, \nbecause an incipit goes to a new group only if it is differ-\nent from all the previous ones. On the other hand, the Any \nfunctions generate smaller groups, because only one dif-\nference needs to be found to place the incipit in a new \ngroup. Similarly, the Any-2 function leads to significantly \nsmaller groups than All-2 at the 0.10 significance level, \nand Any-1 is significantly smaller at the 0.05 level.  \nAggregation \nfunc\ntion ADR-1 \nconsistency Incipits \nper group Pearson’s r \nAll-2 0.844 3.752 -0.892*** \nAny-2 0.913** 2.539* -0.862*** \nPrev-2 0.857 3.683 -0.937*** \nAll-1 0.881 3.297 -0.954*** \nAny-1 0.926** 1.981** -0.749*** \nPrev-1 0.916* 2.858 -0.939*** \nTable 3. Summary of results. * for significant difference at the 0.10 lev-\nel,\n ** at the 0.05 level and *** at the 0.01 level. \nFollowing these results, there seems to be a direct rela-\nt\nionship between the size of the groups and the overall \nconsistency of the lists. We checked this by calculating \nthe Pearson’s r correlation coefficient between the two \nvariables and, as expected, there is a strong negative cor-\nrelation, indicating that the size of the groups affects the consistency of the lists (see Table 3). This is why the All \nfunctions perform worse and the Any functions perform \nbetter: the All versions generate larger groups. Doing so, \nthey allow for many incorrect expansions in the form of \nfalse positives due to intra-group inconsistencies. \n5.1 MIREX 2005 Results Revisited \nIn the 2005 edition of the MIREX evaluations, there was \na task for symbolic melodic similarity that used 11 \nground truths based on partially ordered lists (what so far \nwe have called the Eval05 collection). In particular, 7 dif-\nferent systems were evaluated. \nWe calculated the ADR score of each system with the \nlists generated by the five alternative aggregation func-\ntions (see Table 4). Every alternative evaluation produces \nworse results than the original, except for Prev-2, which \nleads to the same scores. Indeed, every system performed \nworse for every alternative set of ground truths, with re-\nductions in ADR score of up to 12%. \nSystem All-2 Any-2  Prev-2 All-1 Any-1  Prev-1 \nGAM 0.66 0.59 0.66 0.624 0.583 0.605 \nO 0.65 0.607 0.65 0.643 0.593 0.639 \nUS 0.642 0.604 0.642 0.639 0.594 0.628 \nTWV 0.571 0.558 0.571 0.566 0.556 0.564 \nL(P3) 0.558 0.52 0.558 0.54 0.515 0.534 \nL(DP) 0.543 0.503 0.543 0.511 0.494 0.506 \nFM 0.518 0.498 0.518 0.507 0.483 0.507 \nτ - 0.81 1 0.81 0.714 0.714 \nTable 4. ADR results of the systems that participated in MIREX 2005 \nwith the lists resulting from the alternative aggregation functions. GAM \n= Grachten, Arcos and Mántaras; O = Orio; US = Uitdenbogerd and \nSuyoto; TWV = Typke, Wiering and Veltkamp; L(P3) = Lemström \n(P3), L(DP) = Lemström (DP); FM = Frieler and Müllensiefen. Best \nscores appear in bold face. \nMore importantly, the relative order of the systems, in \nt\nerms of their mean ADR score, is also modified. For ex-\nample, with the original lists GAM was the best system, \nfollowed by O and US. With the Any-2 lists, O is ranked \nfirst, before US and GAM. However, with the Any-1 lists \nthe order is reversed: US, O and GAM. We calculated \nKendall’s τ correlation coefficient to measure the differ-\nences in the ranking of systems (see Table 4). A value of \n1 means that two rankings are equal, and a value of -1 \nmeans that they are reversed. Except for Prev-2, which \nproduces the same results as All-2, the correlation coeffi-\ncients tell us that the resulting rankings are different. \n6. CONCLUSIONS AND FUTURE WORK \nWith their appearance in 2005, ground truths based on \npartially ordered lists represented a big leap towards the \nscientific evaluation of Music Information Retrieval sys-\ntems, particularly for melodic similarity tasks. They have \nbeen widely accepted and used by the community, both in \nMIREX and other private evaluations. \nWe have revised the methodology used to generate \nthese lists, unveiling some unaddressed problems. We \nhave shown that the lists generated have inconsistencies, \nand propose several alternatives to minimize them. Using \nADR-1 consistency, we have shown that our alternatives All-2 Any-2 Prev-2 All-1 Any-1 Prev-11 2 3 4 5 6 7\nAggregation functionAverage number of incipits per group\n289\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \n l\nead to better results. We have also seen how would have \nchanged the evaluation of the symbolic melodic similarity \ntask in MIREX 2005, showing that the absolute effec-\ntiveness figures would have changed notably, and the \nranking of systems would have been different too. \nMore meta-evaluation work in this line has to be car-\nried out to improve the evaluation in MIR. In this paper \nwe have focused on the last two steps of the methodolo-\ngy, analyzing the evaluation collection used in MIREX \n2005. Other test collections should be analyzed, and the \nfirst two steps of the methodology should be studied as \nwell because they are known to produce odd results too. \nOne of the reasons may be the subjectivity on the judg-\nments that the loose definition of the task can lead to, as \nalready noted in [2] and [3]. More precise definitions of \nthe information need sought by these tasks would surely \nlead to more coherent judgments by the experts.  \nOne point that has not been discussed in the literature \neither is the significance level used by the aggregation \nfunction, which was 0.25 for the original lists. Our meas-\nure of consistency also works with a significance level to \ndecide whether incipits are correctly arranged or not, and \nthough they should probably be the same, we should \nstudy what value is more appropriate in both cases. \nFinally, the lists generated with the alternative aggre-\ngation functions show diverse characteristics, mainly in \nterms of group sizes and differences among incipits in the \nsame group. Other effectiveness measures, besides ADR, \ncould be proposed to exploit these characteristics, while \naccounting for the unavoidable inconsistencies. \nACKNOWLEDGEMENTS \nWe thank Carlos Gómez, Rainier Typke and the \nIMIRSEL group, especially Mert Bay and Stephen \nDownie, for providing us with the MIREX evaluation da-\nta. We also thank William Frakes and Gabriella Belli for \ntheir insight regarding statistics and hypothesis testing. \nAPPENDIX. THE MANN-WHITNEY U TEST \nThe Mann-Whitney U test [13], or Wilcoxon Rank-Sum \ntest, is a non-parametric statistical test to assess whether \nthe true medians of two independent samples, say X and Y, \nare significantly different or not. Consider X as the sam-\nple of ranks of 600.258.342-1.1.2 for query 600.053.481-\n1.1.1, and Y the ranks given to incipit 850.020.721-1.1.1. \nThe test statistic U  is calculated as: \nU=|X|·|Y|+|Y|/g4666|Y|+1/g4667\n2-/g3533rank/g3435yi/g3439|Y|\ni\n=1 \nw\nhere rank(y i) is the rank that the i-th number of Y would \nhave in the set X ∪ Y. In our example, U = 131. The criti-\ncal value is calculated depending on the alternative hypo-\nthesis H1. For a 2-tailed test, H1 w ould be that the true \nmedians are different, but if a 1-tailed is chosen instead \nH1 would be that the true median of X  is less than the true \nmedian of Y (or the other way around). In the 2-tailed \ncase, the rejection region is spread around both sides of the critical value, while in the 1-tailed case it is only in \none side. Therefore, the 2-tailed case accounts for 2-way \ndifferences ( X > Y or X < Y), while the 1-tailed case \nlooks only for 1-way differences ( X < Y in our case). \nWith a significance level of 0.25, the critical value for \nthe 2-tailed test is U2 = 121,  while for the 1-tailed test it is \nU1 = 136.  Thus, the 1-tailed null hypothesis would be re-\njected because U < U 1, but the 2-tailed would not because \nU\n > U2. In this case, the 2-tailed test fails to detect that \nt\nhe medians are, in fact, different. Because the 1-tailed \ntest looks for a signed difference, it is more powerful and \nrejects the null hypothesis ( H0 = X ≤ Y in our example). \nREFERENCES \n[1] E.M. Voorhees and D.K. Harman, T REC: Experiment and \nEvaluation in Information Retrieval , MIT Press, 2005. \n[2] J.S. Downie, A.F. Ehmann, M. Bay, and M.C. Jones, \"The \nMusic Information Retrieval Evaluation eXchange: Some \nObservations and Insights,\" Advances in Music Informa-\ntion Retrieval , W.R. Zbigniew and A.A. Wieczorkowska, \nSpringer, 2010, pp. 93-115. \n[3] J.S. Downie, \"The Scientific Evaluation of Music Infor-\nmation Retrieval Systems: Foundations and Future,\" Com-\nputer Music Journal , vol. 28, no. 2, 2004, pp. 12-23. \n[4] E. Selfridge-Field, \"Conceptual and Representational Is-\nsues in Melodic Comparison,\" Computing in Musicology, \nvol. 11, 1998, pp. 3-64. \n[5] R. Typke, M. den Hoed, J. de Nooijer, F. Wiering, and \nR.C. Veltkamp, \"A Ground Truth For Half A Million \nMusical Incipits,\" Journal of Digital Information Man-\nagement, vol. 3, no. 1, 2005, pp. 34-39. \n[6] R. Typke, R.C. Veltkamp, and F. Wiering, \"A Measure for \nEvaluating Retrieval Techniques based on Partially Or-\ndered Ground Truth Lists,\" IEEE International Confe-\nrence on Multimedia and Expo, 2006, pp. 1793-1796. \n[7] J.S. Downie, K. West, A.F. Ehmann, and E. Vincent, \"The \n2005 Music Information Retrieval Evaluation Exchange \n(MIREX 2005): Preliminary Overview,\" International \nConference on Music Information Retrieval , 2005, pp. \n320-323. \n[8] M. Grachten, J. Arcos, and R. López, \"A Case Based Ap-\nproach to Expressivity-Aware Tempo Transformation,\" \nMachine Learning, vol. 65, no. 2, 2006, pp. 411-437. \n[9] P. Hanna, P. Ferraro, and M. Robine, \"On Optimizing the \nEditing Algorithms for Evaluating Similarity Between \nMonophonic Musical Sequences,\" Journal of New Music \nResearch, vol. 36, no. 4, 2007, pp. 267-279. \n[10] J. Urbano, J. Lloréns, J. Morato, and S. Sánchez-\nCuadrado, \"Using the Shape of Music to Compute the Si-\nmilarity between Symbolic Musical Pieces,\" International \nConference on Computer Music Modeling and Retrieval , \n2010. \n[11] A. Pinto and P. Tagliolato, \"A Generalized Graph-Spectral \nApproach to Melodic Modeling and Retrieval,\" Interna-\ntional ACM Conference on Multimedia Information Re-\ntrieval, 2008, pp. 89-96. \n[12] K. Saur Verlag, \"Répertoire International des Sources \nMusicales (RISM). Serie A/II, Manuscrits Musicaux après \n1600,\" 2002. \n[13] H.B. Mann and D.R. Whitney, \"On a Test of Whether One \nof Two Random Variables is Stochastically Larger than \nthe Other,\" Annals of Mathematical Statistics , vol. 18, no. \n1, 1947, pp. 50-60.  \n290\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "AMUSE (Advanced MUSic Explorer) - A Multitool Framework for Music Data Analysis.",
        "author": [
            "Igor Vatolkin",
            "Wolfgang M. Theimer",
            "Martin Botteck"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.5651429",
        "url": "https://doi.org/10.5281/zenodo.5651429",
        "ee": "https://zenodo.org/records/5651429/files/LMD-aligned_Datasets.tar.gz",
        "abstract": "Multi-modal dataset for music genre recognition based on six different modalities for the LMD-aligned [1] and SLAC [2] datasets. Further details are provided in [3].\n\nDescriptions of files\n\n\n\t\n\t\t\n\t\t\tLink\n\t\t\tDescription\n\t\t\n\t\n\t\n\t\t\n\t\t\tLMD-aligned_Filelist.arff\n\t\t\tFile list with 1575 music tracks selected from the LMD-aligned dataset [1] with tagtraum genre annotations [4] (only a subset of LMD-aligned is used, which includes only pieces for which all six modalities were accessible, and which includes only well-represented genres)\n\t\t\n\t\t\n\t\t\tLMD-aligned_ExtractedFeatures.tar.gz\n\t\t\tRaw audio signal and model-based features extracted with AMUSE [5]\n\t\t\n\t\t\n\t\t\tLMD-aligned_ProcessedFeatures.tar.gz\n\t\t\tProcessed features: audio signal and model-based features aggregated for 4 s time frames with 2 s step size / all other features (see the table below) with the same values for all time frames\n\t\t\n\t\t\n\t\t\tLMD-aligned_Datasets.tar.gz\n\t\t\tTraining, optimization, and test datasets for 3 splits for the recognition of 5 genres in [3]\n\t\t\n\t\t\n\t\t\tSLAC_Filelist.arff\n\t\t\tFile list with 250 music tracks from the SLAC dataset [2] (genres and sub-genres are provided in the folder structure)\n\t\t\n\t\t\n\t\t\tSLAC_ExtractedFeatures.tar.gz\n\t\t\tRaw audio signal and model-based features extracted with AMUSE [5]\n\t\t\n\t\t\n\t\t\tSLAC_ProcessedFeatures.tar.gz\n\t\t\tProcessed features: audio signal and model-based features aggregated for 4 s time frames with 2 s step size / all other features (see the table below) with the same values for all time frames\n\t\t\n\t\t\n\t\t\tSLAC_Datasets.tar.gz\n\t\t\tTraining, optimization, and test datasets for 3 splits for the recognition of 5 genres and 10 sub-genres in [3]\n\t\t\n\t\n\n\nModalities and feature sub-groups\n\n\n\t\n\t\t\n\t\t\tModality\n\t\t\tSub-group\n\t\t\t\n\t\t\tDimensions in processed\n\n\t\t\tfeatures of LMD-aligned\n\t\t\t\n\t\t\t\n\t\t\tDimensions in processed\n\n\t\t\tfeatures of SLAC\n\t\t\t\n\t\t\n\t\n\t\n\t\t\n\t\t\tAudio signal\n\t\t\tLow-level\n\t\t\t1-524\n\t\t\t1-524\n\t\t\n\t\t\n\t\t\tAudio signal\n\t\t\tSemantic\n\t\t\t525-810\n\t\t\t525-810\n\t\t\n\t\t\n\t\t\tAudio signal\n\t\t\tStructural complexity\n\t\t\t811-908\n\t\t\t811-908\n\t\t\n\t\t\n\t\t\tModel-based\n\t\t\tInstruments\n\t\t\t909-1018\n\t\t\t909-1018\n\t\t\n\t\t\n\t\t\tModel-based\n\t\t\tMoods\n\t\t\t1019-1146\n\t\t\t1019-1146\n\t\t\n\t\t\n\t\t\tModel-based\n\t\t\tVarious\n\t\t\t1147-1402\n\t\t\t1147-1402\n\t\t\n\t\t\n\t\t\tPlaylists\n\t\t\tGenres\n\t\t\t1403-1973\n\t\t\t1403-1973\n\t\t\n\t\t\n\t\t\tPlaylists\n\t\t\tStyles\n\t\t\t1974-1695\n\t\t\t1974-1695\n\t\t\n\t\t\n\t\t\tSymbolic\n\t\t\tPitch\n\t\t\t1696-1757\n\t\t\t1696-1757\n\t\t\n\t\t\n\t\t\tSymbolic\n\t\t\tMelodic\n\t\t\t1758-1781\n\t\t\t1758-1781\n\t\t\n\t\t\n\t\t\tSymbolic\n\t\t\tChords\n\t\t\t1782-1836\n\t\t\t1782-1836\n\t\t\n\t\t\n\t\t\tSymbolic\n\t\t\tRhythm\n\t\t\t1837-1935\n\t\t\t1837-1935\n\t\t\n\t\t\n\t\t\tSymbolic\n\t\t\tTempo\n\t\t\t1936-1963\n\t\t\t1936-1963\n\t\t\n\t\t\n\t\t\tSymbolic\n\t\t\tInstrument presence\n\t\t\t1964-2441\n\t\t\t1964-2441\n\t\t\n\t\t\n\t\t\tSymbolic\n\t\t\tInstruments\n\t\t\t2442-2456\n\t\t\t2442-2456\n\t\t\n\t\t\n\t\t\tSymbolic\n\t\t\tTexture\n\t\t\t2457-2480\n\t\t\t2457-2480\n\t\t\n\t\t\n\t\t\tSymbolic\n\t\t\tDynamics\n\t\t\t2481-2484\n\t\t\t2481-2484\n\t\t\n\t\t\n\t\t\tAlbum covers\n\t\t\tSIFT\n\t\t\t2485-2584\n\t\t\t2485-2584\n\t\t\n\t\t\n\t\t\tLyrics\n\t\t\tjLyrics descriptors\n\t\t\t2585-2603\n\t\t\t2585-2671\n\t\t\n\t\t\n\t\t\tLyrics\n\t\t\tBag-of-Words\n\t\t\t2604-2703\n\t\t\t\n\t\t\n\t\t\n\t\t\tLyrics\n\t\t\tDoc2Vec\n\t\t\t2704-2803\n\t\t\t\n\t\t\n\t\n",
        "zenodo_id": 5651429,
        "dblp_key": "conf/ismir/VatolkinTB10",
        "keywords": [
            "Audio signal",
            "Model-based",
            "Playlists",
            "Symbolic",
            "Album covers",
            "Lyrics",
            "Dimensions in processed",
            "Sub-group",
            "Genres",
            "Styles"
        ]
    },
    {
        "title": "Using jWebMiner 2.0 to Improve Music Classification Performance by Combining Different Types of Features Mined from the Web.",
        "author": [
            "Gabriel Vigliensoni",
            "Cory McKay",
            "Ichiro Fujinaga"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1416590",
        "url": "https://doi.org/10.5281/zenodo.1416590",
        "ee": "https://zenodo.org/records/1416590/files/VigliensoniMF10.pdf",
        "abstract": "This paper presents the jWebMiner 2.0 cultural feature extraction software and describes the results of several musical genre classification experiments performed with it. jWebMiner 2.0 is an easy-to-use and open-source tool that allows users to mine the Internet in order to extract features based on both Last.fm social tags and general web search string co-occurrences extracted using the Yahoo! API. The experiments performed found that the features based on social tags were more effective at classifying music into a small (5-genre) genre ontology, but the features based on general web co-occurrences were more effective at classifying a moderate (10-genre) ontology. It was also found that combining the two types of features resulted in improved performance overall.",
        "zenodo_id": 1416590,
        "dblp_key": "conf/ismir/VigliensoniMF10",
        "keywords": [
            "jWebMiner 2.0",
            "cultural feature extraction",
            "Last.fm social tags",
            "general web search string co-occurrences",
            "Yahoo! API",
            "music genre classification",
            "5-genre",
            "10-genre",
            "combination of features",
            "improved performance"
        ],
        "content": "USING JWEBMINER 2.0 TO IMPROVE MUSIC \nCLASSIFICATION PERFORMANCE BY COMBINING \nDIFFERENT TYPES OF FEATURES MINED FROM THE WEB  \nGabriel Vigliensoni  Cory McKay  Ichiro Fujinaga  \nCIRMMT  \nMcGill University  \ngabriel@music.mcgill.ca  CIRMMT  \nMcGill University  \ncory.mckay@mail.mcgill.ca  CIRMMT  \nMcGill  University  \nich@music.mcgill.ca  \n \nABSTRACT \nThis paper presents the jWebMiner 2.0 cultural feature \nextraction software and describes the results of several \nmusical genre classification experiments performed with \nit. jWebMiner 2.0 is an easy- to-use and open-source tool \nthat allows users to mine the Internet in order to extract features based on both Last.fm social tags and general \nweb search string co-occurrences extracted using the \nYahoo! API. The experiments performed found that the \nfeatures based on social tags were more effective at \nclassifying music into a small (5-genre) genre ontology, \nbut the features based on general web co-occurrences \nwere more effective at classifying a moderate (10-genre) ontology. It was also found that combining the two types \nof features resulted in improved performance overall. \n1. INTRODUCTION \nThe field of music information retrieval (MIR) has \nbenefited greatly from the explosion of information that is available on the Internet. The musical information that can be mined from the web is extremely rich in both \ndepth and breadth, and the on-line contributions of both \nmusical experts and general listeners has provided music researchers with a rich resource of cultural information. \nThis information can be accessed not only via traditional \nweb mining approaches like web scraping and crawling, but also via powerful APIs provided by a variety of on-\nline organizations, such as Last.fm [19] and Yahoo! [20]. \nThis information is of particular value to researchers in automatic music classification, as they can harvest it in the form of numerical features that can then be processed \nby machine learning algorithms in order to automatically \nlabel music with categories associated with domains of interest such as genre, mood or listening scenario. \nThis paper has two main foci. The first is an \ninvestigation of the relative utility of features mined from \nthe web in general and features mined from listener tags, in this case from Yahoo! and Last.fm, respectively. This investigation involves genre classification experiments based on features derived using the APIs provided by \nthese two organizations. The classification effectiveness \nof each of these two groups of features is analysed both individually and in combination. Results derived from several different feature extraction configurations are also studied, as different configurations can have an important impact on results, but this area has not been methodically investigated to date in the MIR literature. \nThe emphasis on genre classification in this inquiry is \ndue to the fact that it can be a particularly difficult type of classification that examines the effectiveness of various classification approaches. Although genre classification \ncan certainly have value in and of itself [11], the ultimate \ngoal of this research is to evaluate approaches that can, hopefully, be effectively extended to other types of music classification as well. \nThe second primary focus of this paper is the \npresentation of jWebMiner 2.0, a feature extraction tool for mining data from the Internet. This tool has been expanded since the publication of the original jWebMiner 1.0 [12], and the updated version is presented here to the MIR community for their research use. jWebMiner 2.0 was used to perform all of the experiments described in this paper. \n2. BACKGROUND INFORMATI ON \n2.1 Mining the Internet for Musical Features \nThere has been too much research on mining useful \ninformation from the Internet to cite with any completeness here. It is, however, valuable to emphasize \ncertain particularly influential papers, namely [2, 5–10, \n16–18]. \n2.2 Social Tags \nAs noted in by McKay and Fujinaga [11], genre (and \nother types of musical categories) can be strongly characterized by how an audience understands and \nperceives music and musicians, not just on objective \ncontent-based characteristics. This has important \nimplications for genre classification. “True” class labels are essentially specified by the opinion of millions of \nlisteners and evolve over time. These labels are \ninfluenced by many cultural factors, some of which may \n \nPermission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fee provided that copies \nare not made or distributed for profit or commercial advantage and that \ncopies bear this notice and the full citation on the first page.  \n© 2010 International Society for Music Information Retrieval  \n \n607\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n be independent of purely sonic musical characteristics. \nThe labels that one might assign to a song are based not only on the song itself, but one’s overall perception of an \nartist. Furthermore, it might be said that there is no one \nground truth, as is the “truth” is indeed the sum of the opinions of all listeners.  \n“Social tags” are unstructured text labels assigned by \nnon-expert users to an entity, such as a song, an album or \nthe collected work of an artist. A variety of on-line services, such as Last.fm, permit users to aggregate their tags for a variety of musical resources. There are typically no restrictions on the choice of words or phrases that users can tag resources with, although users do in practice often seem to select tags that other users have already used, thus creating a kind of shared and navigable system [10]. Tagged resources can therefore be said to, in some way, share certain characteristics with other resources that have been tagged with the same category \nor categories in the perception of users [1]. The value of \nsocial tags increases when they are aggregated into such a large public access community repository, as they provide access to information on how all the users of the system perceives and organize the resources [8]. In the context of musical communities and libraries, social tags are used by people for playlist organization; personal song retrieval; the expression of taste and opinion; and general contribution to public knowledge [10]. \nIt is clear that social tags are a source of valuable \nhuman-generated contextual knowledge about music. \nThey provide researchers with information about mood, \nemotion, genre and other types of categories that are based on the subjective perceptions of millions of users [3]. Mining this data from the web can thus be effective in acquiring information that can be used in th e \nevaluation and training of MIR systems, and at least in the short term, as the aggregation of cultural perceptions that are in constant flux as culture and individual opinions change [12]. \n2.3 Last.fm \nLast.fm is a music service that has been in operation \nsince 2003. Internet radio is the core service that it offers to its users, but it also provides them with a broad range \nof additional functionality. For example, Last.fm allows \nusers to create personal profiles and contribute social tags to songs, albums and artists. Of particular interest, \nLast.fm automatically generates custom radio playlists \nusing recommendation algorithms based primarily on collaborative filtering. These algorithms consider both user tags and listening behaviour, as mined by Last.fm’s \n“Scrobbler” software, which monitors the music played \nby listeners both on the Last.fm site and on their enabled local media players.  \nThe amount of information managed by Last.fm is \nenormous, consisting of more than 39 billion tracks \nscrobbled to date, a number that is increasing at a rate of \nover 400 million tracks per week.\n1 The company provides \nfree access to portions of its data through an API [19], \n                                                             \n1 http://www.last.fm/community something that permits developers to build their own \ntools. \n2.4 jWebMiner and jMIR \njWebMiner [12] is part of the jMIR automatic music \nclassification research suite [14]. In addition to \njWebMiner, jMIR also includes tools for extracting features from audio files and MIDI files; a machine \nlearning engine based on metalearning; datasets to serve \nas ground truth for training and testing classification models; and software for profiling and detecting \nmetadata errors in music collections. jMIR is designed \nspecifically to facilitate the integration of information extracted from different types of musical data, and jWebMiner is the component that provides access to \nsocial context features (i.e., cultural musical information) \navailable on-line. \njWebMiner, like all of the jMIR software components, \nis designed to be usable by users with both technical and \nnon-technical backgrounds, and as such includes an easy-\nto-use and flexible graphical interface. All of the jMIR \ncomponents, including jWebMiner, are open-source and \navailable for free at http://jmir.sourceforge.net. \nAt its most basic level, the original jWebMiner 1.0 \noperates by accessing Yahoo’s web search API to acquire hit counts for various search strings. For example, calculations measuring how often the names of different musicians or composers co-occur on the same web pages (taking into account how often they occur individually) can provide insights on the relative similarity of the musicians to each other. Similarly, the cross tabulation of \nsong or artist names with musical class labels associated \nwith genre, or mood, for example, can be used to classify music. Of course, basic hit counts can result in noisy data, so it is necessary to include additional processing to improve results. \njWebMiner begins by parsing either iTunes XML, \nACE XML, Weka ARFF or text files in order to acquire strings to use in searches. Users may also manually enter search strings in the GUI. The software then accesses Yahoo’s API to either measure the co-occurrence of each value in one field with other values in the same field, or to measure the cross tabulation of values in different \nfields. \nThe optimal statistical procedure for processing hit \ncounts and dealing with noise contained in them can vary \ndepending on the task at hand. One must consider not only the accuracy of an approach, but also its search complexity, as web services typically involve daily limits on queries. jWebMiner therefore allows users to choose between a variety of metrics and scoring systems. \nOne option offered by jWebMiner is the ability to \nallow users to specify “string synonyms” so that hit counts will be combined for linked synonyms. This \nwould be useful, for example, in a genre classification \ntask where the class names “R&B” and “RnB” are equivalent. \njWebMiner also allows user-definable “filter strings.” \nThis permits the software to be set to ignore all web pages that do not contain general filter terms such as “music,” for example, or application-specific terms such \n608\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n as “genre” or “mood.” This can be useful in avoiding \nirrelevant and noisy hit counts. For instance, a feature extraction should not count co-occurrences of “The \nDoors” with “Metal” or “Rap” unless they refer to music rather than unrelated topics such as the building industry or door knockers.  \nIt is also possible to set jWebMiner to limit searches to \nparticular sites, such as the All Music Guide or Pitchfork, in order to emphasize musically relevant and reliable \nsites. jWebMiner also allows users to assign varying \nweights to particular sites as well as to the web as a whole when feature values are calculated.  \nThe feature values generated by jWebMiner essentially \nconsist of relative similarities measured between various specified search strings, after appropriate statistical processing. These feature values can be exported to ACE XML, Weka ARFF, delimited text or HTML files. Feature values may also be browsed directly via the GUI.  \n3. MINING LAST.FM WITH JWEBMINER 2.0 \nThe most significant improvement incorporated into the \nnew jWebMiner 2.0, in addition to the existing functionality described in Section 2.4, is the ability to \nextract social tag-based information using the Last.fm \nAPI [19]. For example, users can specify artist names \nand have the software extract the most common tags for \nthat artist from the Last.fm API, ranked by popularity. \nThe user can also specify class labels of interest, and have jWebMiner derive features based on whether each \nartist has been tagged by Last.fm with any of these labels \nand, if so, have the feature value reflect the tag’s relative Last.fm ranking. \njWebMiner 2.0 can extract just the Last.fm-derived \nfeatures, just the Yahoo!-derived features or both. If the \nlatter option is selected, then jWebMiner will not only \nextract each of the two feature sets individually, but will also provide the user with levels of support associated with each class label based on the normalized combination of the Last.fm-derived and Yahoo!-derived \nfeatures. This normalization process is performed to level all queries to the same number-base in the case of the \nYahoo!-mined features, and to represent the position of a given artist’s Last.fm tag on a normalized scale. For an artist and genre we define a scoring function S(a,g ), \nwhere P(a,g) is the Last.fm position of the queried tag \nand P(a,i)  the position for  all genre tags:  \n \n   \n(1) \n \nTo exemplify the normalization process let us query \nthe german band Tarwater  with the tags indie, post-rock  \nand electronic . These tags appear respectively in the \nposition 6, 7 and 1 of the top tags for that artist . Being n \nequals 3, the value of the sum is 1/6 + 1/7 + 1/1, which is \n55/42. Thus, the scoring function values are 7/55 for indie , 6/55 for post-rock, and 42/55 for electronica.   jWebMiner automatically bases its score on only the \nYahoo!-derived features if a particular artist is not on Last.fm, or if an artist has not been tagged with any of the queried class names. In addition, jWebMiner can show the web search normalized feature score, the Last.fm normalized ranking score, and the averaged results. These values can be processed independently afterwards. \nIt is hoped that the combination of social tag-based \nfeature extraction with more general web search-based \nfeature extraction will provide MIR researchers with a \nunified and accessible cultural feature extraction tool that provides access to two different kinds of valuable cultural musical information available on-line. Although \njWebMiner 2.0 can certainly be used alone, it also carries the significant advantage of allowing features extracted with it to be easily processed using jMIR’s ACE machine learning tool, or combined with features extracted from audio or MIDI by, respectively, jMIR’s jAudio and jSymbolic feature extractors [13]. \n4. EXPERIMENTS \n4.1 Overview \nA series of experiments were performed in order to \ninvestigate the relative performance of features derived \nfrom Last.fm social tags, features derived from Yahoo! web searches and the combination of features derived \nfrom both sources. Attention was also given to various \npossible web search feature extraction configurations, involving the use of various different filter words and site \nweightings (see Section 2.4). \nThe feature groups and extraction configurations were \nevaluated based on their performance in genre \nclassification. As noted in Section 1, genre classification was chosen because it can be a particularly difficult task, and is thus a good stress test for features. \nAll experiments were performed using jWebMiner 2.0, \nwhich harvested features using the Last.fm and Yahoo! web services, as described above. \n4.2 Dataset used \nThe experiments were conducted using the SAC \n(Symbolic, Audio and Cultural) dataset [13]. This dataset \nconsists of 250 matching MIDI files and audio \nrecordings, as well as accompanying metadata (e.g., title, artist, etc.). This metadata was stored in an iTunes XML \nfile, which was parsed by jWebMiner in order to extract \ncultural features from the web [13]. \nThe files of the SAC dataset are divided into 10 \ndifferent genres with equal numbers of artists per genre \n(Modern Blues, Traditional Blues, Baroque, Romantic, \nBebop, Swing, Hardcore Rap, Pop Rap, Alternative Rock and Metal ). It is clear upon observation that these 10 \ngenres consist of 5 pairs of similar genres. This arrangement makes it possible to perform 5-class genre classification experiments as well as 10-class experiments \non the same dataset simply by combining each pair of related genres into one class. An additional advantage is that it becomes possible to measure an indication of how serious misclassification errors are in 10-class \n609\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n experiments by examining how many misclassifications \nare in an instance’s partner genre rather than one of the other 8 genres. The ground truth was created using expert sources, such as AllMusic.com, combined with the personal expertise of the authors of the dataset. \nSAC was chosen partly because it provides two tiers of \ngenre classification; partly because the similarity of each of the 5 genre pairs makes 10-class classification \nparticularly difficult, and thus a good test of jWebMiner’s \neffectiveness; and partly because it can be used in other \nresearch to investigate the utility of combing the cultural features extracted by jWebMiner with other kinds of features, such as features extracted from audio, symbolic and lyrical data. This latter application was previously investigated with jWebMiner 1.0 in [13], and an update to this research using jWebMiner 2.0 is presented in [15] .  \n4.3 Text filtering and site weighting \nIn order to optimize classification accuracy using \njWebMiner’s filtering capabilities, we designed and tested several sample ﬁlters for the Required Filter \nWords  and Excluded Filter Words  ﬁelds. Sources such as \n[2], [4], [7], and [17] have recommended certain required \nfilter words, such as music , review , like, work  and artist  x \nplayed  y music , and have also recommended certain \nexcluded filter words, such as mp3, download , videos , \ncart, prices  and login . In general, our results matched \nthose obtained in [7], which is to say that better \nperformance was achieved when only simple filters were \nused. Thus, only mp3 and store were used as excluded \nﬁlter words, and no required filter words were used. It was found through informal experimentation that too \nmuch noise was otherwise introduced by web pages in which many of these terms co-occur with in non-specific \nways with many other artists and genres. \nWe also develop ed a set of synonyms for different \ngenres and artist names in order to take into account the \nvariety that one finds in practice . So, for example , we \nused the term s Bebop  and Be-bop as synonyms for Bop. \nOn the other hand, an artist such as Derek and the \nDominos  could be found as Derek and the Dominoes.  \nWe also tested different site weighting schemes. To \nbegin with, we tried simply querying the whole network (NC, as described in Table 1) using the 5-genre \ntaxono my. We then queried the web as a whole as well as \nthree predetermined websites ( wikipedia.org , \nallmusic.com  and amazon.com ), using weight values of \n0.5 for the whole network and 0.166 for each one of the sites ( W1). For 10-genre classification, we tried a third \narrangement that did not take in to account the whole \nnetwork, and where each of the above three sites was assigned a weight of 0.333 ( W2). Experiments with these \nthese different arrangements were performed in order to gain insights into how data mined from the web as a whole performed relative to specialized websites. \n  \n MK08  Previous experiment with jWebMiner performed in \n[13].  \nNC No constraints involving weighting, required filter \nwords or excluded filter words.  \nF/W1  MP3  and  store  as excluded filter words . Site \nweight ings of 1/6 for  wikipedia.or g, allmusic.com, \nand amazon.com; t he whole web weighted by 1/2.   \nF/W1/S  Same settings as F/W1,  plus a set of syn onyms for \nthe genres and artist names . \nST Classification results when using only social tags.  \nC NC  Combined and averaged results using web search \nwith no constraints, as well as social tags.  \nC F/W1  Combined and averaged results using  social tags and \nweb search in F/W1 configuration.  \nC F/W2  Combined and averaged results  of social tags and \nweb search , MP3  and  store  as excluded filter words, \nand site weight ings of 1/3 for  wikipedia.org, \nallmusic.com, and amazon.com.   \n \nTable 1.  The different configurations tested on the SAC \ndataset. Some experiments involved 5-genre \nclassification, while others involved 10-genre \nclassification.  \n4.4 Results \n4.4.1  SAC dataset 5-genre classification \nTable 1 provides brief descriptions of each experimental \nsetup, as well as specifications of the identifying notation used. The average classification accuracy rates for \nexperiments involving only web search-based 5-genre \nclassification are shown in Table 2.  \n \nMK08   NC  F/W1   F/W1/ S  ST  C F/W1  \n87.2   82.4   90.1   93.1   95.4   96.9  \n \nTable 2.  Average classification accuracy rates for 5-\ngenre classification experiments on the SAC dataset. \nIt can be seen that the NC experiment classification \naccuracy was worse than MK08 , which is not surprising \nbecause the MK08  experiments used some ﬁltering \nconstraints, namely the use of “music” as a required filter \nword. However, with the F/W1  and F/W1/S  \nconfigurations we observed improvements of 2.9% and 5.2% over MK08 , and 7.7% and 10.7% over NC, \nrespectively. These results suggest that highlighting particular music-related sites can provide better results than simply extracting information from the web as a whole. \nOn the other hand, retrieving social tags alone resulted \nin improvements in genre classification performance of \n610\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n 8.2% over the MK08 mark, and the combined approach \nof retrieving social tags as well as querying the web \nresulted in the highest classification rate of 96.9%. Hence, it appears that combining the features from web searches and social tags can increase accuracy and diminish the problems associated with each method, such as noisy web search hits (as reviewed in [13] and [6]) and the cold start , polysemy , annotation accuracy , popularity \nbias, and malicious behaviour  issues associated with \nsocial tags (as reviewed in [8] and [14]). \n4.4.2  SAC dataset 10-subgenre classification \nFor 10-genre classification, we performed the same \nexperiments but separated synonyms, ﬁlter words and site \nweights in order to gain insights on how each one of them affect results. The same SAC genre-pair weighting \nscheme used in [13] and described in Section 4.2 was also used to evaluate the seriousness of those classification errors that did occur Specifically, if a misclassification is within a genre pair, the error is reduced to 0.5 of an error, and if the misclassification is outside of a pair, then the error is increased to 1.5. Table 3 shows the weighted (W) and unweighted ( UW) Yahoo! web search-only accuracy \nrates that we found. \n \n   MK08   NC  F/W1/S  F/W1  F/W2   ST \nUW 61.2   56.5   50.4   67.2   77.9   43.5  \nW  67.4   63.7   51.9   76.7   82.8   43.9  \n \nTable 3.  Classification accuracy rates in 10-genre \nclassification experiments, including both unweighted \n(UW) and weighted ( W) results.  \nAs in the 5-genre experiments, NC performed worse \nthan MK08 . On the other hand, F/W1  and especially \nF/W2  were more accurate than MK08  by 6.0% and 16.7%  \nrespectively in terms of unweighted classification accuracy, and by 9.3% and 15.4% respectively in terms of weighted classification accuracy. Also, in the 5-genre classification experiments social tags gave excellent \nresults. In contrast, in the 10-genre classification \nexperiments using social tags ( ST) social tags actually \nperformed the worst amongst all configurations.  \nTo understand this phenomenon, we studied the tags \nthat Last.fm users use and found that, in general, they are \nvery well deﬁned for broad genres, such as those used in the 5-genre experiment. However, when one delves into finer classification, there are many subtle differences in the ways that different people perceive genres. For example, how substantially different is the genre Rock  \nfrom Punk  if we think of a band such as Green Day ? In \naddition, a number of problems related to tagging such as polysemy, synonymy, accuracy, and spam are present in \nlarge collections of social tags [8]. Furthermore, tags do \nnot represent only genres or styles, but anything that individual users want, from mood  to BPM , so they can be \nvery noisy if one wishes to extract detailed class labels. To overcome these problems, we tried combining the \nfeatures extracted from both Last.fm  social tags and Yahoo! web searches. The obtained results are shown in Table 4. \n \n   C NC   C F/W1   C F/W2  \nUW  75.6   74.8   77.9  \nW  78.6   79.8   81.3  \n \nTable 4.  Classification accuracy rates for 10-genre \nclassification experiments using combined data from both \nweb search and social tags, unweighted and weighted. \nIt can be seen that for the C NC  experiment, the \ncombined data results in improvements of 19.1% ( UW) \nand 14.9% ( W) relative to using only web searches \nwithout any constraints. For the C F/W1  experiment, the \nresults were 7.6% ( UW) and 3.1% ( W) better. The genre \nclassification accuracy was thus improved by using the \ncombined approach, despite the fact that social tags alone performed very poorly. However, for the last experiment C F/W2 , the results were the same for the W scale (as \nF/W2) , and slightly worse in the case of the UW scale. \nThese last results make some sense because the F/W2  \nexperiment weights only the three predetermined \nwebsites mentioned above, rather than querying the \nwhole web.   \n5. CONCLUSIONS \nWe have provided jWebMiner with new functionality, \nnamely the ability to extract features based on Last.fm \nsocial tags. These features can be used alone or combined with jWebMiner’s already existing Yahoo! search-based \nfeatures, and the feature values can be summarized and \nsaved in convenient formats for machine learning processing in future research.  \nWe also performed web search experiments on \ndifferent sets of excluded ﬁlter words and site weightings, \nin order to investigate their effect on genre classification accuracy. The experiments were performed on the SAC  ground truth dataset, and improvements of 5.9% and 16.7% were achieved respectively for 5- and 10-genre classification compared to the results from earlier published experiments [13].  \nWhen performing the same experiments using social \ntags, we achieved an improvement of 8.2% for the 5-genre taxonomy, but also observed a 17.7% decrease in performance with the 10-genre taxonomy. It was thus \nfound that social tags performed very well for broad \ngenres, but lacked sufficient precision for more detailed \nsub-genre classes. However, the best results overall were achieved when both social tags and the web search data were combined. We conclude that the information obtained by each approach is at least partially, complementary. \nIn summary, the genre classification results obtained \nusing both Last.fm social tags and Yahoo! web search- co-\noccurences were the highest observed amongst all \n611\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n configurations experimented with. Indeed, this combined \napproach actually achieved results comparable to those found in [13], where symbolic and audio data were also available and more sophisticated machine learning-based classification methodologies were used. The combined approach described in this paper was also extremely successful in a just-published followup to the [13] experiments, as described in [15]. \n6. FUTURE RESEARCH \nAlthough the music classification results obtained in our experiments with jWebMiner 2.0 are very promising, we believe that there is still more space for improvement in accuracy. First, in order to properly filter music tags used by contributors to social sites, we will work on the development of a thesaurus of terms that can group \ntogether related words, something that should result in \nmore precise tag rankings. Secondly, we will develop software for customizing web search site weightings through the automated application of genetic algorithms. A third research step will be to explicitly experiment with other types of classification, such as mood classification. Finally, efforts will be made to take advantage of any open-source web services in order to reduce the \ndependency on the excellent but proprietary Last.fm and Yahoo! services. \n7. REFERENCES \n[1] Aucouturier, J.-J., and F. Pachet. 2003. Representing \nmusical genre: A state of the art. Journal of New \nMusic Research . 32 (1): 83–93.  \n[2] Baumann, S., and O. Hummel. 2003. Using cultural metadata for artist recommendations. Proceedings of \nthe Third International Conference on Web \nDelivering of Music . 138–41.  \n[3] Casey, M.A., R. Veltkamp, M. Goto, M. Rhodes, C. \nRhodes, and M. Slaney. 2008. Content-based music \ninformation retrieval: current directions and future \nchallenges. Proceedings of the IEEE . 96 (4): 668–\n96.  \n[4] Geleijnse, G., and J. Korst. 2006. Learning effective surface text patterns for information extraction. \nProceedings of the EACL Workshop on Adaptative \nText Extraction and Mining . 1–8.  \n[5] Geleijnse, G., M. Schedl, and P. Knees. 2007. The \nquest for ground truth in musical artist tagging in the \nsocial web era. Proceedings of the 8th International \nConference on Music Information Retrieval . 525–30.  \n[6] Knees, P., E. Pampalk, and G. Widmer. 2004. Artist classification with web-based data. Proceedings of \nthe 5\nth International Conference on Music \nInformation Retrieval . 517–24.  \n[7] Knees, P., M. Schedl, and T. Pohle. 2008. A deeper \nlook into web-based classification of music artists. Proceedings of the 2nd Workshop on Learning the \nSemantics of Audio Signals . 31–44.  \n[8] Lamere, P. 2008. Social tagging and music information retrieval. Journal of New Music \nResearch . 37 (2): 101–14.  \n[9] Levy, M., and M. Sandler. 2007. A semantic space \nfor music derived from social tags. Proceedings of \nthe 8\nth International Conference on Music \nInformation Retrieval . 411–6.  \n[10] Levy, M., and M. Sandler. 2009. Music information \nretrieval using social tags and audio. IEEE \nTransactions on Multimedia. 11 (3): 383–95.  \n[11] McKay, C., and I. Fujinaga. 2006. Musical genre classification: Is it worth pursuing and how can it be improved? Proceedings of the 7\nth International \nConference on Music Information Retrieval. 101–6.  \n[12] McKay, C., and I. Fujinaga. 2007. jWebMiner: a web-based feature extractor. Proceedings of the 8\nth \nInternational Conference on Music Information \nRetrieval . 113–4.  \n[13] McKay, C., and I. Fujinaga. 2008. Combining features extracted from audio, symbolic and cultural \nsources. Proceedings of the 9\nth International \nConference on Music Information Retrieval . 597–\n602.  \n[14] McKay, C., and I. Fujinaga. 2009. jMIR: Tools for automatic music classification. Proceedings of the \nInternational Computer Music Conference . 65– 8. \n[15] McKay, C., J. A. Burgoyne, J. Hockman, J. Smith, G. Vigliensoni, and I. Fujinaga. 2010. Evaluating the genre classification performance of lyrical features \nrelative to audio, symbolic and cultural features. \nAccepted for publication at the  Int. Society for \nMusic Information Retrieval Conference. Utrecht, \nNetherlands. \n[16] Turnbull, D., L. Barrington, and G. Lanckriet. 2008. Five approaches to collecting tags for music. \nProceedings of the 9\nth International Conference on \nMusic Information Retrieval . 225–30.  \n[17] Whitman, B., and S. Lawrence. 2002. Inferring \ndescriptions and similarity for music from \ncommunity metadata. Proceedings of the 2002 \nInternational Computer Music Conference . 591–8.  \n[18] Zadel, M., and I. Fujinaga. 2004. Web services for music information retrieval. Proceedings of 5\nth \nInternational Conference on Music Information Retrieval . 478– 83. \n[19] Last.fm Web Services. Retrieved 19 March 2010, \nfrom http://www.last.fm/api. \n[20] Yahoo! Developer Network. Retrieved 19 March \n2010, from http://developer.yahoo.com. \n612\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "A Roadmap Towards Versatile MIR.",
        "author": [
            "Emmanuel Vincent 0001",
            "Stanislaw Andrzej Raczynski",
            "Nobutaka Ono",
            "Shigeki Sagayama"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1418141",
        "url": "https://doi.org/10.5281/zenodo.1418141",
        "ee": "https://zenodo.org/records/1418141/files/VincentROS10.pdf",
        "abstract": "Most MIR systems are specifically designed for one appli- cation and one cultural context and suffer from the seman- tic gap between the data and the application. Advances in the theory of Bayesian language and information process- ing enable the vision of a versatile, meaningful and accu- rate MIR system integrating all levels of information. We propose a roadmap to collectively achieve this vision.",
        "zenodo_id": 1418141,
        "dblp_key": "conf/ismir/VincentROS10",
        "keywords": [
            "Bayesian language",
            "versatile MIR system",
            "semantic gap",
            "theory of Bayesian",
            "meaningful integration",
            "cultural context",
            "all levels of information",
            "integration",
            "meaningful integration",
            "accurate MIR"
        ],
        "content": "A ROADMAP TOWARDS VERSATILE MIR\nEmmanuel\nVincent\nINRIAStanisław A. Raczy ´nski, Nobutaka Ono, ShigekiSagayama\nThe University of Tokyo\nABSTRACT\nMostMIRsystemsarespeciﬁcallydesignedforoneappli-\ncationandoneculturalcontextandsufferfromtheseman-\ntic gap between the data and the application. Advances in\nthe theory of Bayesian language and information process-\ning enable the vision of a versatile, meaningful andaccu-\nrateMIR system integrating all levels of information. We\npropose a roadmap to collectively achieve this vision.\n1. INTRODUCTION\nMIR has the vocation of covering all music and all music-\nrelatedapplications, e.g.transcription,structuration,align-\nment, tagging, personalization, composition and interac-\ntion. Yet,mostsystemstodatearedesignedforoneclassof\napplicationsandoneculturalcontext,namelyWesternpop-\nular music, which limits their reusability and their mean-\ningfulness in the sense of [12]. In addition, most systems\nrelyongeneralpatternrecognitiontechniquesappliedonto\nabagoflow-levelfeatures,whichboundstheiraccuracyto\nsome glass ceiling. A system integrating all levels of in-\nformation would make it possible to address virtually any\napplication on any data in a versatile, meaningful and ac-\ncurate fashion. For instance, it would enable much higher-\nlevelinteraction, e.g.changingthemusicgenreofarecord-\ning without affecting some of its other features. While\nmanysharethevisionofthiscompletesystem[1],nofully\nsatisfying approach has yet been proposed to achieve it.\nOne integration approach adopted e.g.by the NEMA1\nproject or by [5] is to queue several audio and symbolic\nfeature extraction modules, so as to compute higher-level\n“features of features”. This bottom-up approach greatly\nimproves accuracy but will eventually reach a glass ceil-\ning too due to the propagation of errors from one process-\ning stage to the next. Also, it is not fully versatile since\neach application requires the implementation of a speciﬁc\nworkﬂowandtheinputsandoutputsofamodulecannotbe\nswappedotherwisethanbydevelopinganewmodule. Top-\ndown approaches based on probabilistic graphical models\naddress these issues by estimating the hidden features best\naccountingfortheobservedfeatures[10]. Allapplications\n1http://nema.lis.uiuc.edu/\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnotmadeordistributedforproﬁtorcommercialadvantageandthatcopies\nbear this noticeand thefull citation on theﬁrst page.\nc/circlecopyrt2010International Society for Music Information Retrieval.then amount to inferring and possibly manipulating some\nofthehiddenfeatures,withoutchangingthemodelnorthe\ngeneral decoding algorithm. For instance, small graphical\nmodels such as Hidden Markov Models (HMMs) integrat-\ning harmony and pitch are routinely used to infer the most\nprobable chord sequence given a set of MIDI notes [8] or\nconversely to generate the most probable melody given a\nchord sequence [2] using the general Viterbi algorithm.\nItiscommonbeliefthattheformalismofgraphicalmod-\nels has the potential to yield a versatile and accurate MIR\nsystembyintegratingmoreandmorefeaturesintoa hierar-\nchical model. Yet, this formalism alone does not sufﬁce to\nachieve this vision, as challenging issues pertaining to the\ndeﬁnition of the model structure, the parameterization of\nconditionaldistributions,thedesignofunsupervisedlearn-\ningalgorithmsandthecollectionofdevelopmentdatahave\noften been overlooked. In this paper, we explicitly state\nand clarify these issues and derive a feasible roadmap.\n2. COMPLETE MODEL STRUCTURE\nThe ﬁrst task is to collectively deﬁne a taxonomy of mu-\nsicfeaturesandtheirstatisticaldependencies thatvirtually\ncovers all existing and future music. This involves the fol-\nlowing steps: building an exhaustive list of music features\nand their deﬁnition, identifying features amenable to the\nsametheoreticaltreatment, e.g.genreandmood,andorga-\nnizing these features into a dependency network. This is\nnot straightforward, since the current MPEG-7 standard2\nmostlyaddresseslow-level orapplication-speciﬁc features\nand agreed-upon deﬁnitions of features such as musical\nstructureorrhythmremaintofound. Also,thedependency\nnetwork is not unique and a sparse network is preferred.\nWeproposeadraftmodelofamusicpieceasa dynamic\nBayesiannetwork inFigure1. Whileitmaybeincomplete,\nwe believe that it provides a useful basis for community\ndiscussion. In this graph, each node represents a sequence\nofuni-ormultidimensionalfeaturesconsideredasavector\nrandom variable. Statistical dependencies are indicated by\narrows such that the conditional distribution of a variable\ngiven its ancestors depends on its parents only. “Vertical”\nhierarchical dependencies are explicitly displayed, while\n“horizontal” temporal dependencies within and between\nnodes are implicitly accounted for. We adopt a generative\nmodeling pointofview[10]wherelower-levelfeaturesde-\npend on higher-level features. The joint distribution of all\nvariables then factors as the product of the distribution of\neach variable given its parents [10].\n2http://mpeg.chiariglione.org/standards/mpeg-7/mpeg-7.htm\n662\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)O\nT1 T2 T3\nS1 S2 S3 S4 S5 S6 S7\nE1 E2 E3 E4 E5\nA1 A2 A3\nOverall features\nOTa\ngs: setoftagsin Vtagscoveringallorpartofthepiece, e.g.\ngenre, mood, composer, performer, place and user preference\nTemporal organization features\nT1Structure: set of possibly overlapping/nested sections, each\ndeﬁned by its quantized duration in bars and by a section\nsymbol inVsect\nT2Meter: sequence of bars, each deﬁned by its reference beat\nand time signature in Vmeterand by the associated metrical\naccentuation level of each beat and beat subdivision\nT3Rhythm: sequenceof eventsassociatedtooneormoresimul-\ntaneousnoteonsets,eachdeﬁnedbyitsquantizeddurationin\nbeats and by the associated number of onsets[11]\nSymbolic features\nS1Notated tempo: beat-synchronous sequence of tempo and\ntempo variation symbolsin Vtempo\nS2Notated loudness : beat-synchronous sequence of loudness\nand loudness variation symbolsin Vloud\nS3Key/mode: beat-synchronous sequence ofkey/mode symbols\ninVkey\nS4Harmony: beat-synchronous sequence of chord symbols in\nVchord\nS5Instrumentation: beat-synchronoussequenceofsetsofactive\nvoices, each deﬁned by a voice symbol in Vinst(including\ninstruments, orchestra sections, singer identities, and sample\nIDs, with various playing or singing styles)\nS6Lyrics: event-synchronous sequence(s) of syllables in Vsyll\nS7Quantized notes: set of notes (including pitched/drum notes,\nvoices or samples), each deﬁned by quantized onset and du-\nration in beats, its articulation symbol in Vartic, its loudness\nand loudness variation symbol in Vloud, its quantized pitch\nandpitchvariationsymbolin Vpitch,itsvoicesymbolin Vinst\nand itssyllable in Vsyll\nExpressive performance features\nE1Expressive tempo: beat-synchronous sequence of actual\ntempo values in bpm\nE2Expressive loudness : beat-synchronous sequence of actual\nglobal loudness values in sones\nE3Instrumental timbre: beat-synchronous sequence of vectors\nof parameters modeling the timbre space of each voice\nE4Expressive notes : set of notes, each deﬁned by its actual\nonset time and duration in s, its loudness curve in sones, its\npitch curve in Hz, and itstrajectory in the timbre space\nE5Rendering: time-synchronous sequence of vectors of param-\neters characterizing the recording setup (e.g. reverberation\ntime,micspacing)orthesoftwaremixingeffectsandthespa-\ntial position and spatial width of each voice\nAcoustic features\nA1Tracks: rendered acoustic signal of each voice\nA2Mix: overall acoustic signal\nA3Classicallow-level features: MFCCs,chroma, etc\nFigure 1. Draft model of a music piece. Dependencies\nupon overall features are shown in light gray for legibility.Awideapplicationrangeisensuredbyallowingthefea-\nturevalue sets Vsect,Vmeter,Vtempo,Vloud,Vchord,Vinst,\nVartic,Vpitchtobeeitherﬁxedoradaptiveandtocontaina\n∅symbol denoting the lack of structure, meter and so on.\nThe variables T1,T3,S4andS7also implicitly depend on\na set of structural, rhythmic, harmonic, melodic and bass\npatternsdenoted Psect,Prhythm,Pchord,PmeloandPbass.\nMore generally, all model parameters can themselves be\nregarded as variables [10].\nAnyvariablemaybeeitherfullyobserved,partiallyob-\nservedorhidden,leadingtoahugerangeofscenarios. For\ninstance, automatic accompaniment consists of inferring\nA2given part of A1, while symbolic genre classiﬁcation\nconsistsofinferringpartof OgivenS7. Playlistgeneration\norcoverdetectionmayalsobeaddressedbycomparingall\nfeatures O,T∗,S∗andE∗inferred from A2in each piece\nof a database according to some criterion.\n3. SCALABLE CONDITIONAL DISTRIBUTIONS\nOnce the variables have been deﬁned, the next step con-\nsists of designing conditional distributions between these\nvariables. While recent studies in the ﬁeld of audio source\nseparation have already led to complete family of acoustic\nmodels P(A1|E4,E5)[9], many other dependencies have\neither been studied in a deterministic setting or not inves-\ntigated yet. More crucially, current probabilistic symbolic\nmodels,e.g.[4,8,11], only account for short-term depen-\ndencies between two or three variables taking few possi-\nblevaluesandrelyonhandtyingofprobabilitiesbasedon\nWestern musicology. This design method does not scale\nwith the long-term high-dimensional dependencies found\ninFigure1. Forinstance,thevirtuallyinﬁnitesetofoverall\nfeatures Oaffects most other features and the probability\nof quantized notes P(S7|O,T1,T2,T3,S2,S3,S4,S5,S6)\ndepends on as many as 9 other features. Scalable meth-\nods must hence be found to parameterize each conditional\ndistribution soas to avoid overﬁtting.\nA promising approach consists of modeling the condi-\ntional distribution of a variable given its parents by inter-\npolationof its conditional distributions given each parent\nindividually. This approach is widely used in the language\nprocessing community [3] but does not account for possi-\nbleinteractionsbetweenparents. Thisissuemaybetackled\nby reparameterizing the space of parent variables in terms\nof a smaller number of factors using e.g. Latent Semantic\nIndexing (LSI) techniques [7] developed for text retrieval\nand collaborative ﬁltering. We believe that the extension\noftheseapproachestoallsymbolicmusicdatawillleadto\na similar breakthrough as inthe above domains.\n4. UNSUPERVISED LEARNING ALGORITHMS\nThedesignofconditionaldistributionsiscloselyrelatedto\nthat of learning and decoding algorithms. Indeed, due to\ntheabovedimensionalityissuesandtothevarietyofmusic\nand individual music experiences, most distributions can-\nnotbeﬁxedaprioributmustbelearnedfrompossiblyuser-\nspeciﬁc training data or from the test data. Similarly, the\n663\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)feature value sets V∗and the patterns P∗must be learned\nto identify e.g.the most relevant set of chord symbols and\npatterns for a given song, in line with human listening that\npicks up regularities based on prior exposure without ac-\ntually naming them [12]. These learning tasks are always\nunsupervised sincetrainingdataannotatedwithallfeatures\nof Figure 1 will most probably never exist.\nTheestimationofsomehiddenvariablesconsistsofmar-\nginalizing i.e.integrating the likelihood over the values of\ntheotherhiddenvariables[10]. Thiscanbeachievedusing\nthe modular sum-product and max-product junction tree\nalgorithms [10] that generalize the classical Baum-Welch\nand Viterbi algorithms for HMMs. The considered objec-\ntive is often the maximization of the posterior distribution\noftheinferredvariablesgiventhedata. AlthoughthisMax-\nimumAPosteriori(MAP)objectivemaybeusedforunsu-\npervisedlearningofthemodelparameters(i.e. conditional\nprobabilities) [8], it cannot infer the model order (i.e. the\ndimension, the value set and the parents of each feature).\nUnsupervised pruning of feature dependencies would also\nconsiderablyacceleratethespeedofthejunctiontreealgo-\nrithm, that is exponential in the number of dependencies,\nand make it possible to match the available computational\npower in an optimal fashion. Suitable model selection cri-\nteria and algorithms are hence of utmost importance.\nAutomaticRelevanceDetermination (ARD)andseveral\notherpopularmodelselectiontechniquesemploypriordis-\ntributionsoverthemodelparametersfavoringsmallmodel\norders [7]. The alternative variational Bayesian inference\ntechnique[10]selectsthemodelwithhighestmarginalprob-\nability by integrating the posterior distribution of all hid-\ndenvariables. Thistechnique alsoprovidesanapproxima-\ntion of the posterior distribution of all hidden variables as\na by-product. This increases the meaningfulness and in-\nterpretability of the results compared to the estimation of\ntheMAPvariablevaluesonly,atthecostofhighercompu-\ntational complexity. Again, we believe that advances will\neventually be achieved by combining these approaches.\nThe choice of algorithms will guide that of feature for-\nmats. Efﬁcient graph formats exist for the representation\nof posterior distributions of symbolic feature sequences\n[6], but they must yet be extended to e.g.polyphonic note\nsequences. More generally, the high dimensionality of all\nfeatures will necessitate compressed feature formats.\n5. MULTI-FEATURE ANNOTATED DATABASE\nFinally, although the development of a database annotated\nwithallfeaturesofFigure1appearsinfeasible,somemulti-\nfeature annotated data will nevertheless be needed to ini-\ntializeandevaluatetheunsupervisedlearningprocess. This\nimpliesstrongcommunitycoordinationtopushcurrentan-\nnotation efforts towards the same data and bridge the cul-\ntural gap between experts of different music styles. Also,\nthis advocates for the evaluation of conditional feature es-\ntimationtasks within MIREX, where some other features\nwouldbeknown,asopposedtothecurrentfull-ﬂedgedes-\ntimation tasks, where only audio or MIDI are given.6. SUMMARY AND IMPLICATIONS\nWe provided a feasible roadmap towards a complete MIR\nsystem, emphasizing challenges such as scalable parame-\nterization and unsupervised model selection. As recom-\nmended in [1], this implies that most efforts in the MIR\ncommunity now focus on symbolic data. Yet, other strong\nimplicationsalsoariseregardingtheneedforacoordinated\neffort and the deﬁnition of MIREX tasks and data.\n7. ACKNOWLEDGMENT\nThisworkissupportedbyINRIAundertheAssociateTeam\nProgram VERSAMUS (http://versamus.inria.fr/).\n8. REFERENCES\n[1] J. S. Downie, D. Byrd, and T. Crawford. Ten years of\nISMIR:Reﬂectionsonchallengesandopportunities.In\nProc. ISMIR, pages 13–18, 2009.\n[2] S. Fukayama, K. Nakatsuma, et al. Orpheus: Au-\ntomatic composition system considering prosody of\njapanese lyrics. In Proc. ICEC, pages 309–310, 2009.\n[3] D. Klakow. Log-linear interpolation of language mod-\nels. InProc. ICSLP, pages 1695–1699, 1998.\n[4] K. Lee. A system for automatic chord transcription\nusing genre-speciﬁc hidden Markov models. In Proc.\nAMR, pages 134–146, 2007.\n[5] A. Mesaros, T. Virtanen, and A. Klapuri. Singer iden-\ntiﬁcation in polyphonic music using vocal separa-\ntion and pattern recognition methods. In Proc. ISMIR,\npages 375–378, 2007.\n[6] S.Ortmanns,H.Ney, andX.Aubert. Awordgraphal-\ngorithmforlargevocabularycontinuousspeechrecog-\nnition.Computer Speech and Language, 11(1):43–72,\n1997.\n[7] M. K. Petersen, M. Mørup, and L. K. Hansen. Sparse\nbut emotional decomposition of lyrics. In Proc. LSAS,\npages 31–43, 2009.\n[8] C. Raphael and J. Stoddard. Harmonic analysis with\nprobabilistic graphical models. Computer Music Jour-\nnal, 28(3):45–52, 2004.\n[9] E. Vincent, M. G. Jafari, et al. Probabilistic modeling\nparadigmsforaudiosourceseparation.In MachineAu-\ndition : Priciples, Algorithms and Systems. IGI, 2010.\n[10] M. J. Wainwright and M. I. Jordan. Graphical models,\nexponential families, and variational inference. Foun-\ndations and Trends in Machine Learning, 1(1-2):1–\n305, 2008.\n[11] N. Whiteley, A. T. Cemgil, and S. J. Godsill. Bayesian\nmodelling of temporal structure in musical audio. In\nProc. ISMIR, pages 29–34, 2006.\n[12] F. Wiering. Meaningful music retrieval. In Proc.\nf(MIR), 2009.\n664\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "Predicting High-level Music Semantics Using Social Tags via Ontology-based Reasoning.",
        "author": [
            "Jun Wang",
            "Xiaoou Chen",
            "Yajie Hu",
            "Tao Feng"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1417785",
        "url": "https://doi.org/10.5281/zenodo.1417785",
        "ee": "https://zenodo.org/records/1417785/files/WangCHF10.pdf",
        "abstract": "High-level semantics such as “mood” and “usage” are very useful in music retrieval and recommendation but they are normally hard to acquire. Can we predict them from a cloud of social tags? We propose a semantic iden- tification and reasoning method: Given a music taxonomy system, we map it to an ontology’s terminology, map its finite set of terms to the ontology’s assertional axioms, and then map tags to the closest conceptual level of the referenced terms in WordNet to enrich the knowledge base, then we predict richer high-level semantic informa- tion with a set of reasoning rules. We find this method predicts mood annotations for music with higher accuracy, as well as giving richer semantic association information, than alternative SVM-based methods do.",
        "zenodo_id": 1417785,
        "dblp_key": "conf/ismir/WangCHF10",
        "keywords": [
            "cloud",
            "social tags",
            "semantic identification",
            "ontology",
            "finite set",
            "assertional axioms",
            "WordNet",
            "reasoning rules",
            "music taxonomy",
            "music retrieval"
        ],
        "content": "Predicting High-level Music Sema ntics using Social Tags via On-\ntology-based Reasoning \nJun Wang, Xiaoou Chen, Yajie Hu, Tao Feng \nInstitute of Computer Science and Technology, Peking University \n{wangjun, chenxiaoou, huyajie, fengtao}@icst.pku.edu.cn  \n \nABSTRACT \nHigh-level semantics such as “mood” and “usage” are \nvery useful in music retrieval and recommendation but \nthey are normally hard to acquire. Can we predict them \nfrom a cloud of social tags? We propose a semantic iden-tification and reasoning method: Given a music taxonomy system, we map it to an ontology’s terminology, map its finite set of terms to the ontology’s assertional axioms, and then map tags to the clos est conceptual level of the \nreferenced terms in WordNet to enrich the knowledge base, then we predict richer high-level semantic informa-tion with a set of reasoning rules. We find this method predicts mood annotations for music with higher accuracy, as well as giving richer semantic association information, \nthan alternative SVM-based methods do. \n1. INTRODUCTION \nSemantic information extraction of music is given more \nand more emphasis based on the explosive growth of mu-sic resources. However, despite its high importance in a wide range of applications, th ere are various challenges in \nextracting semantic information from different existing resources. We sum up these existing information re-sources as three main classes: \nProfessional databases, web services, ontologies : \nThese resources are created by professional data entry staff, editors, and writers.  They commonly consist of ba-\nsic editorial metadata such as names, titles, product num-\nbers, biographies, nationalities, reviews etc., relational content such as similar artists and albums, influences, etc., and some culturally descriptiv e content such as styles, \ntones, moods, themes, etc. There are standard taxonomies \nforcing objects into predefined categories and the infor-\nmation is normally very precise, trustful and useful. However, information like descriptive content is expen-sive to generate, besides, the explosive growth of music has brought more and more challenge for manipulating such large scale content. Professional editors of those sys-\ntems such as Allmusic and Pandora are hardly keeping \npace with the ever-growing content. \nAudio content : Currently content-based methods are \nthe dominant players for automatic music information extraction. Some of the repr esentative works can be re-\nferred to the Music Information Retrieval Evaluation eX-\nchange (MIREX) [1]. However, the acoustic aspect is just one facet of music, besides there are unneglectable influ-\nences from subjectivity, social  and cultural aspects, so \nhigh-level semantic information extraction purely from audio is quite an arduous ch allenge. For example, in the \nAudio Mood Classification evaluation (Hu et al. 2008), \nthe resulting accuracies for 5-cluster mood classification was up to 61.5% in 2007, to 63.7% in 2008, and to 65.67% in 2009. Some mood perceptions are just too subtle and subjective, such as autumnal, brash, passionate, to be cap-\ntured well enough by audio features only. \nSocial tags: Fortunately, nowadays the Web has be-\ncome a primary host of a sizeable amount of text-based \nand semantic information. Web 2.0 technologies— e.g., Last.fm, MusicBrainz, and the so-called Shared Station in \nPandora— have drastically augmented social media with \nrich context, such as user-provided tags, comments, re-views, folksonomies etc. By contrast to the above profes-\nsional systems, these resources have some nontrivial ad-vantages: flexibility to rapid content changes, intrinsically containing rich high level semantic information, etc. \nHowever, due to the noisy and unstructured data, existing \nsystems are mainly based on simple keyword matching approaches, so knowledge from these resources is barely being well discovered. \nThe motivation is that the prediction of high level se-\nmantic metadata could benefit from a comprehensive \nconsideration of information from multiple resources. We \nwere inspired by a WordNet-based method proposed in [2] acquiring open-domain class attributes. In this work we propose a way to automatically identify the social tags’ concepts. By mapping a music ontology to a semantic \nlexicon such as WordNet, we acquire more lexicalization \nof the concepts and better semantically classify/cluster the social tags (i.e. with more  coverage), and we are also \nable to acquire in the ontology-based system the meaning and association between tags, to conduct reasoning on the resultant knowledge base giving a declarative representa-\ntion with well-defined semantics, and to produce higher \nprediction accuracy for high level semantic data. By con-trast to [2], our work is domain-specific, so it does not require applying extraction patterns to text and mining query logs to capture attributes. Instead, existing prede-fined professional taxonomies from reference systems are \nfirstly mapped to an ontology’s terminology, i.e. an on-\ntology’s terminology (TBox) consists of classes and roles, and secondly, we consider their finite set of terms as seed \n405\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \naxioms and propose a WordNet-based method to use \nthese seed axioms to identify the most appropriate classes \nand roles for social tags, so that social tags can be \nmapped to the ontology’s assertional axioms (ABox) as-sociated with the constructed TBox. Lastly, we consider one of the most challenging tasks in MIR, i.e. mood clus-ter prediction, and perform a set of DL-safe reasoning \nrules on the resultant knowledge base (KB) to further \naugment the ABox with enriched mood annotation. \n2. RELATED WORK \nRecently researchers have brought up novel web-based methods for MIR tasks. In particular, some researchers have proposed approaches ab out automatically extracting \nmusic semantic information from the social tags. Luke et al. [3] consider social tags and web-mined documents as feature vectors and input th em to Support Vector Ma-\nchine (SVM), for classification to determine whether a song represents a certain tag.  Bischoff et al. [4] apply \nSVM classifier to audio features and apply Naïve Bayes Multinomial to tag features, and then combine them in a programming way. Although significant improvements by combining web information are reported, these ap-proaches dismiss the semantics of social tags or web-mined documents and we argue that some valuable in-\nformation goes lost. We will look into a detailed compar-\nison in our evaluation section. Algorithms originally de-veloped in text information retrieval domain, such as La-tent Semantic Analysis (LSA), probabilistic Latent Se-mantic Analysis (pLSA) and Latent Dirichlet Allocation (LDA) [5] can also be successfully adopted in MIR here, \ne.g., Levy et al. [6] and Laurier et al. [7] apply LSA me-\nthod to gain an effective feature space with low dimen-sionality for capturing similarity. However, a LSA me-thod has intrinsic limitations that the resultant dimen-sions might not have interpretable meaning, i.e., the de-\nrived semantic spaces still do not have explicitly defined \nsemantics. \nOn the other hand, the extension of semantic informa-\ntion extraction to the field of knowledge representation \nformalisms has been widely deployed in the non-music-specific multimedia community. Great emphasis has been \ngiven to the extensional aspects of multimedia ontologies. \nThere are many works in the literature proposed for man-aging multimedia data using ontologies, including image annotation, video annotation and recommendation [8, 9]. Exclusively for the domain of image and video annota-tion, novel works have been proposed for obtaining high \nlevel semantics. For example, Peraldi et al. [8] give a \nconcrete example considering the interpretation of images of a sports event, and show how retrieval and interpreta-\ntion of image data can be obtained by abductive reason-ing; Penta et al. [9] proposed a novel ontology model for \norganizing low level multimedia data and semantic de-\nscription. It exploits abductive reasoning to provide the most probable explanation of observed facts. All these works are using the benefits of ontology systems, which \nhave scalability and extendibility capabilities to achieve \neffective image retrieval. However, to the best of our knowledge, ontology-based system for combining high level semantic information derived from social tags and professional taxonomies with information from audio fea-tures has rarely been studied in the music annotation do-\nmain. \n3. SOCIAL TAG SEMANTIC IDENTIFICATION \nFor open-domain tasks as in [2], they heuristically \nchoose the first sense uniformly in WordNet. Meanwhile the authors have pointed out, this heuristic is bound to make errors yet proved to be efficient enough in open-\ndomain experimental results. However, this solution does \nnot suit our work. As in a domain-specific system, the correct sense should be exclusive, e.g., Blues should be a kind of music genre rather than a color. Our approach will consider the fact that in professional music databas-\nes or web services, there are standard taxonomies forcing \nobjects into predefined categories. While manually-constructed language resource WordNet has open-domain, wide-coverage conceptual hierarchies, by group-ing terms and phrases with the same meaning into sets of synomyms, associated with the same definition. By \nmapping those predefined categories to WordNet, we \nacquire more lexicalization of the concepts and better semantically classify the social tags with more coverage. \nFigure 1 . Social tag semantic identification framework. \n406\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \n3.1 Mapping to WordNet Concept Hierarchies \nAs shown in Fig. 1, the first task is to identify the most \nappropriate concept level in WordNet which best represents each category in the professional taxonomy. \nFor each category ࣝ in the professional taxonomy, we \nconsider its instances as seed words and retrieve them in \nWordNet. For each pair of s eed nodes in WordNet, we \nfind the closest common node in the upper level (ances-\ntor node) which connects the two seed nodes via shortest \npath, then we get a set ࣮ of candidate ancestor nodes. \nHere we define a scoring function to select the best an-cestor node in ࣮ as below: \n) log(##) (S S\nSSlevel levelSeedsds descentSeeS Score   \n     (1) \nWhere, #ݏ݀݁݁ܵݐ݊݁ܿݏ݁݀ ௌ means the number of seed \nwords that node  ܵ covers in its descent nodes,  #ݏ݀݁݁ܵ ௌ \nmeans the number of seed wo rds in the corresponding \ncategory  ࣝ , and  ݈݁ݒ݈݁ ௌ means the depth from ܵ to the \ntop concept. Finally ܵ with the highest score in ࣮ will be \nselected as the most appropr iate concept in WordNet for \nthe corresponding category ࣝ .As an example in Fig.1, \ngiven a set of seed nodes <driving, dancing, dating, exer-\ncising, reading>, the approach detect “activity” as the most appropriate concept for this set rather than “action” or “abstraction”. \nTwo facets have been considered in equation (1) defin-\ning the scoring function: concept specificity and concept coverage. On the one hand, the score is constrained by \n݈݁ݒ݈݁\nௌ because if the level is too close to the top concept, \nthen the node ܵ would be too general and would harm the \nidentification precision; on the other hand, the score is also constrained by #ݏ݀݁݁ܵݐ݊݁ܿݏ݁݀\nௌ, because if the level \nis too low and too specific, it would cause an insufficient \ncoverage and harm the recall since many potential words \nwhich belong to the category would not be identified. \nComparing to a simple linear function of ݈݁ݒ݈݁ ௌ, the func-\ntion defined in (1) experimentally gives an optimal tra-\ndeoff between coverage and identification precision. \n3.2 Expanding Word List of Pre-defined Taxonomies \nIn this work, we adopt the taxonomies from Allmusic — \na large-scale music database that provides professional \nreviews and metadata for albums, tracks and artists, and are frequently used for MIR re search purposes [1]. In \nparticular for mood annotation, for the convenience of \nevaluation and comparison to state of the art, we adopt \nthe five cluster mood taxonomies from MIREX, which \nhave been commonly adopted by the community. \nThe taxonomies are mapped to an ontology and it re-\nsults in a TBox consisting of classes, related subclasses, roles of objects and datatype properties. Details about \nconstructing the music ontology are dismissed here. Re-\nlated similar works can be referred to [10]. XMLMapper tools can automatically tr ansform available Web XML \nbased resources (e.g. Allmusic.com) to an OWL ontology.  \nOnce the concept has been identified via the approach \nas described in section 3.1, we construct a word list with more coverage for each pre-de fined classes by retrieving \nthe hyponyms, their synonyms  and siblings, each with \ndifferent weights (hyponyms> synonyms> siblings). In \nall, based on the pre-defined taxonomies it generates a \nword list with 71,022 words. While matching a tag with the word list, if the tag exactly matches a word in the list, it is then identified as the corresponding class directly; if the tag has words matching with different concepts and each with different weight, we  only consider the word \nwith the highest weight and match the tag with this word’s related class; or else  if the weights of different \nconcepts equal, we simply consider the front words, as users usually put highly descriptive word in front of a less informing word, for example, “road trip”, “brash band”, \nalthough it is not always the case.  \n4. ONTOLOGY-BASED REASONING \nSo far, knowledge bases have been constructed using in-\nformation from several different sources, including: \n Social tags identified with well-defined semantics \n Editorial and relational metadata from professional \ntaxonomy systems \n Probabilistic classification output extracted from au-\ndio content \n4.1 TBox and ABox Construction \nAs previously described, we map the taxonomies from Allmusic to the TBox of our ontology, and result in 155 \nclasses and 62 roles in all. These roles consist of object \nproperties indicating relationships between classes, such as <artist, song> <hasStyles > <genre>, <artist> <simi-\nlarTo, follows, followedBy, inflences> <artist>, <artist> <performedVia> <instrument>, <song> <playedBy> \n<artist>, etc., and several datatype properties indicating \ndata attributes of classes, such as <artist, song> <has-MoodProbability1> <”float”>, <artist, song, genre, in-\nstrument, …> <hasConfidenceFactor> <”float”>, etc. \nIn the following we illustrate steps and rules for ontol-\nogy-based reasoning on music mood: \nInitialization.  Firstly, we define datatype properties \n<hasMoodProbability1, hasMoodProbability2,… , has-MoodProbability5>, of which each denotes prediction \nprobability that the individual be classified into mood cluster1, cluster2,… cluster5. As shown in Fig.2, initial assertions about these mood probability properties of \nsongs and tags are added in ABox. Given a tag having \nbeen identified into one of the mood clusters in the con-cept identification step, we assert an initial mood prob-\nability property, e.g., <0.0, 0.0, 1.0, 0.0, 0.0> for a tag identified as in mood cluster 3. For songs, we extract \n407\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \n112-dimension audio feature vectors via the library in \njAudio toolkit, including intensity, timbre and rhythm \nfeatures as well as the overall standard deviation and av-erage values of Mel-frequen cy cepstral coefficients \n(MFCCs) and spectral shape features etc. We apply the feature selection library CfsSubsetEval in WEKA [11] and reduce the feature space from 112 to 23 dimensions, \nthen we apply the SMV classification library in WEKA, \nand output the predication probabilities for each of the \nfive mood clusters. For more details about the above con-tent-based system, audience could refer to our previous work as FCY1 system in MIREX 2009 audio mood clas-sification tasks. The output probabilities are asserted in \nABox as the songs’ initial va lue of datatype properties < \nhasProbabilityMood1, hasProbabilityMood2,…, hasPro-\nbabilityMood5>. These audio individuals initialized with mood probability properties, e.g., <0.25, 0.12, 0.33, 0.14, 0.16> in fig. 2, are to be considered as seed atoms as well. \nReasoning:  Secondly, a set of DL-safe rules are applied \non the ABox to infer mood probability for target atoms \nfrom seed atoms, as shown in fig. 2. Heuristically, differ-ent classes and roles should have distinct importance. For example, a song’s mood could be inferred with higher confidence from its social tags describing mood than \nfrom its audio content. For another example, a song’s \nmood could be inferred with higher confidence from its artist’s mood style than from its genres’. Thanks to the well-defined semantics in the ontology, these factors can be efficiently considered in a semantic reasoning engine, e.g. Racerpro. We use nRQL in Racerpro [12], an expres-\nsive ABox query language for the very expressive DL ALCQHI _R+ (D-), to apply rules and generate new asser-\ntions to ABox. Besides many other advantages, the main reason we chose nRQL is that it allows for the formula-tion of complex retrieval conditions on concrete domain \nattribute fillers of ABox individuals by means of complex \nconcrete domain predicates. Atoms of different classes \nare attached with several datatype properties which indi-cate their corresponding confidence/importance degree during the inferring process: \n Role Factor (RF): constant value related to the seed \natom’s object property, e.g, an artist <plays> a song, a \nsong <hasStyle> a genre. \n Confidence Factor (CF): dynamic value indicating the overall confidence estimation about the precision of its mood prediction. Initial CF values for song and tag atoms are typically set as 0.3 and 1.0. \n Weighting Factor (WF): weighting value that a seed atom has while propagating its mood prediction to a target atom, so that mood prediction value that the target atom acquires could be weighted. We simply consider WF= CF*RF \nWe then apply rules on nRQL and generate new asser-\ntions in ABox. Given a set of triggered seed atoms of mood tags ൏t\nଵ,tଶ,…,t ୫൐ and songs ൏s ଵ,sଶ,…,s ୬൐, \nfor example, Rule I is an illustrative rule as below:  \n \nwhere  ܫdenotes the index of mood clusters. The accumu-\nlated mood probability values are summed up and nor-malized to ensure the sum probability of all clusters equals 1. In the above example, the artist atom x is trig-\ngered and continues to be use as seed atoms for further \nreasoning iterations. The rules are applied iteratively until no more atoms are triggered. Rule II and III are another two illustrative rules as below: \n \n5. EXPERIMENTAL SETTING \nOur album consists of 1804 songs, covers about 21 major genres, 1022 different artists, and evenly covers mood labels created by professional Allmusic editors with one or more terms in one of the five mood clusters. \nOn one hand, each song is processed into 30s, mono, \n22.05 kHz, .wav audio clips. We then apply the content-based system described in section 4.1. This system gives an accuracy of >60% for the data set of MIREX mood classification task, yet the same system gives a much lower accuracy of ~40% for our data set, which mainly xxx x xzxxx x xy\nc xm I xz x m c RFz x I zc xm I xx y m c RFy x I y\nFactor  Confidence  has                                                             ;  bability hasMoodPro                                                            ;  styles  has                                            genre;  a  is      artist; an  is           bability hasMoodPro     : III  RuleFactor  Confidence  has                                                             ;  bability hasMoodPro                                                            ;  similar to  is                                            artist; an is    artist; an  is           babilit y hasMoodPro   : II Rule\n　　　　　　　　　\n  > < Factor   Weighting have  > <                                                  >; <  bability hasMoodPro > <                                                  >; <                                                                         Factor   Weighting have  > <                                                    >; <                       } maximum{bability hasMoodPro > <        Factor     Confidence  have   >; <  with  tagged is                              }; plays               {artist; an  is           bability hasMoodPro : I  Rule\n3 1 3 13 1 3 18 7 48 7 48 7 4 3 1 8 7 48 7 43 1 3 3 1 18 7 4 8 8 7 7 4 4\n,w w ,t t,m m I ,t t,w ,w w,s ,s s,m ,m m ,w , w ,w ,w w cI ,s ,s s x,t t x m ,w m w>; ,s ,s <s  x , m , w m , w m wx I x \nx    \nFigure 2 . Applying reasoning rules between atoms. \n408\n11th International Society for Music Information Retrieval Conference (ISMIR 2010) \n \ndue to the al b\ning of manua l\nOn the ot h\ncollected 65, 2\nwhile the re m\nter removing \ntinct tags in a\nness in these them with se\nm\nof genre, 13 %\nclasses of usfinally augm\ne\n6.1 Social T a\nWe test the p\nmood, usage concepts wh\ni\nand the deep e\nGround T\nevaluation, w\n15,400 tags. classes by th\nr\nthe ones wit h\nscure tags. T\ndeep as one c\n1085 tags la b\nclasses, and t\nscure ones, t h\nnot belongin g\nas artist nam e\nrors. \nIdenti ficat\nthe tested 10 8\nognized by h u\nerrant spellin g\nare 137 tags there are 20\n3\ntags correctl y\nbel, the 150 o\nof the label, a\nof the label, t\nfuture work\nprocessing ( N\nniques \nFigure 3 . D\n8189bum’s larger s\nlly pre-filteri n\nher hand, we c\n272 tags in al\nmaining 353 s o\nthe duplicate \nall. Despite m\n15,400 tags, \nmantic-rich cl\n% into subc l\nage, 2% int o\nent the ABox \n6. EVA\nag Semantic \nprecision of i d\nand instrume n\nich are relate d\nest level is 4.  \nTruth: To cr e\nwe randomly \nThese tags a r\nree people. T h\nh inconsisten t\nThe level of t h\ncan identify.  A\neled with sub c\nthe remainin g\nhe ones belon\ng to any class e\nes and manu a\ntion Precisio n\n85 tags, there\numan yet not \ng, which is v e\nyet not cov e\n3 tags mis-i d\ny identified in \nones exactly i\nand the 8 on e\nthe result giv e\n, we will \nNLP) and i n\ninclu\nDistribution o f \n137203scale, larger d\nng of ambigu o\ncrawled tags \nl for 1364 so n\nongs do not h\nones, we ha v\nmany spelling \nwe manage t o\nasses: 47% t a\nlasses of mo o\no subclasses o\nwith 10,015 c\nALUATION \nIdentificatio n\ndentifying su b\nnt. There are \nd to the a bov\neate the gro u\nsampled 33 0\nre then man u\nhe labels are c\nt labels are c\nhe classes is \nAmong the 3 3\nclasses belon g\ng 1915 tags c o\nnging to othe r\nes, concrete s p\nally unrecogn i\nn: As shown \n are 89 tags w\nin WordNet b\nery common i\nered in our r e\ndentified. Co u\nthe same m a\nidentified in t\nes identified i n\nes a precisio n\nadd some \nnformation re\nuding \nfTagIdentifi c\n150\n498diversity and l\nous songs. \nfrom Last.fm \nngs in our al b\nhave any tags.\nve got 15,400 \nerrors and n\no identify mo s\nags into subcl a\nod, 3% into \nof instrument \nconcept asser t\nn \nbclasses of g e\nin all 141 at o\nve 4 main cl a\nund truth for \n00 tags fro m\nually labeled w\ncross-checke d\nconsidered as \nrequired to b\n300 tags, ther e\nging to the 4 m\nonsists of: th e\nr classes, the o\npecific terms s\nizable spellin g\nin Fig.3, a m\nwhich can be \nbecause of th e\nin user tags; t\nesultant word \nunting in the\nain class of th e\nthe same sub c\nn the sibling c\nn of 60.5%. I n\nnature lang u\ntrieve (IR) t\nstem m\n \ncation Resul t.\nlack-\nand \nbum, \n Af-\ndis-\noisi-\nst of \nasses \nsub-\nand \ntions. \nenre, \nomic \nasses \nthe \nm the \nwith \nd and \nob-\nbe as \ne are \nmain \ne ob-\nones \nsuch \ng er-\nmong \nrec-\ne ab-\nthere \nlist; \n150 \ne la-\nclass \nclass \nn the \nuage \ntech-\nming, lem\nsoc\n6.2\nSVM\nhas \nof M\nmis\nlear\ngin me\nt\nclas\nsafe\nAB\nsequ\nF\ninfo\nlarg\nsocdi\nm\nocc\napp\nfurt\ndim\nfeat\nvec\nin 3\ndim\ncros\nture\nT\nSVM\ncon\nas \nbas\ncial\nT\nR\nPrmmatization a n\nial tag seman t\nOntology-b a\nM is a widel y\nbeen found s\nMIR. It take s\nssing their m\nrns the se para\nbetween two \nthod consid e\nsses and appl\ne rules, i.e., g\nox assertions \nuences are pr o\nFor SVM- bas\normation int o\nge 15,400-di m\nial tag disru p\nmension to 29\ncurrence freq u\nply Principal \nther reduce \nmension socia l\nture space. T h\nctors reaches \n3-fold cross v\nmension featu r\nss validation. \ne space for th e\nTo ensure th\nM-based hav e\nnsider both th e\ninput inform\ned system, w\nl tags  \nTable 1. Con\nReasoning- base\nActual \n \nredictionC\nC1 0.2\nC2 0. 2\nC3 0. 0\nC4 0. 1\nC5 0. 2\n \nC1 0.5\nC2 0. 1\nC3 0. 0\nC4 0. 0\nC5 0. 1nd token nor m\ntic iden tificat i\nased Reason i\ny applied m a\nsuperior to ot h\ns attributes a s\neanings and \nating hyperpl a\nclasses. In c o\ners the sem a\nies inference \ngiven a preco n\nas conseque n\nobability pre d\nsed system, w\no feature vec t\nmension an d v\npt the SVM- ba\n19 by filteri n\nuency less th\nComponent A\nthe redunda n\nl tag feature \nhe system us i\naverage clas s\nvalidation, w h\nre vectors re\nHence we a d\ne SVM- based \nat the reaso n\ne input infor m\ne social tags a\nation for bo t\nwe add asserti o\ninto AB o\nfusion matri x\ned systems \nS\n1 C2 \n29 0.11 \n24 0.54 \n080 . 1 6  \n150 . 1 8  \n240 . 0 1  \nRea\n59 0.18 \n13 0.54 \n090 . 1 5  \n080 . 1 2  \n110 . 0 1  malization, t o\nion precision. \ning versus S V\nachine learni n\nher classifiers \ns features ve c\nsemantic ass\nane that maxi m\nontrast, the A B\nantic associa t\nservices wit h\nndition, it out c\nnces. In our c a\ndiction about a\nwe transform \ntors as input .\nvery sparse f e\nased system, \nng out the ta g\nhan 3 times. M\nAnalysis (PC A\nncy and m a\nspace to a 9\ning 2919-di m\nsification acc u\nhile the syst e\naches 54.69 2\ndopt the 982- d\nsystem. \nning-based s y\nmation as fair a\nand conten t-ba\nth systems: f\nons about the \nox and i n\nxes of SVM -\nSVM-based \nC3 C4 \n0.01 0.0 8\n0.08 0.1 1\n0.71 0.16\n0.15 0.52\n0.05 0.1 3\nasoning-based \n0.06 0.1 0\n0.08 0.0 7\n0.68 0.16\n0.09 0.59\n0.09 0.0 6o improve th e\nVM Method\nng method an d\nin many case\nctor input, di s\nociations, an d\nmizes the ma r\nBox reasonin g\ntions betwee n\nh a set of D L\ncomes a set o\nase, these co n\na song’s moo d\nthe social ta g\n. As the ver y\nature space o\nwe reduce th e\ngs which hav e\nMoreover, w e\nA) method t o\nap the 291 9\n982-dimensio n\nmension featur e\nuracy 52.052 8\nem using 98 2\n21% in 3-fol d\ndimension fe a\nystem and th e\nas possible, w e\nased attribute\nfor reasonin g\nidentified s o\nnitiate eac h\n-based and \nC5 \n8 0.12 \n1 0.03 \n6 0.06 \n2 0.18 \n3 0.61 \n0 0.16 \n7 0.01 \n6 0.07 \n9 0.07 \n6 0.69  \ne \nd \ns \ns-\nd \nr-\ng \nn \nL-\nof \nn-\nd.  \ng \ny \nof \ne \ne \ne \no \n9-\nn \ne \n8% \n2-\nd \na-\ne \ne \ns \ng-\no-\nh \n409\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \nsong’s datatype properties— hasProbabilityMood1 etc.— \nwith mood prediction probabilities learned from the 112-\ndimension audio features, as described in the experiment setting section; for SVM-based system, we construct its feature space by combining the 112-dimension audio fea-tures with the 982-dimension feature space mapped from social tags.  \nTab. 1 gives the confusion matrix of each system, \nwhere C1 to C5 indicate the five mood clusters. The SVM-based system achieves an average classification accuracy of 55.7185%  in 3-fold cross validation. The \nreasoning-based system achieves prediction accuracy of 62.07% , which outperforms the SVM-based system, as \nwell as having a more even precision distribution among clusters. The SVM-based system gives better precision only in predicting mood cluster3, indicating that SVM-based method can well discriminate cluster3 (brooding, poignant, sad, somber, etc.). Th is has also been reflected \nin MIREX [1] reports. \n6.3 Knowledge Base Enrichment \nRelational content such as similar artists and albums, in-\nfluences, follows, etc., are mu ch less expensive to acquire \nfrom professional systems than for high level semantic information like music mood and usage. In all, there are \n29,253 assertions acquired from Allmusic about the rela-\ntional content such as <artist> <influences, similar to, fol-lows> <other artists>. \nTo evaluate the prediction performance, we conduct a \nprediction process on artist atoms in the Knowledge Base. To simplify the process, we co nsider an artist’s tags and \nmood cluster the same as his song. We partition the artist axioms who are players of the album— so that we have the ground truth as their song’s mood label— into two complementary subsets: a “k nown” subset A_516 (with \n516 artist atoms) having ABox assertions generated from \neditorial metadata and social tag information, and the \nother is “unknown” subset A_512 (with 512 artist atoms) to be predicted and validated. To reduce variability, we perform another round by changing the A_512 to “known” subset. After the reasoning process, we have got 461 art-ists in A_512, and 469 artists in A_516, who gained mood prediction via the inferring rules. The prediction \nprecision is 50.76% for A_512 and 50.32% for A_516 \nand the average precision is 50.54%. This prediction me-thod could be effective, given random five-mood-cluster classification’s precision is as low as 20%. \nSome interesting knowledge can also be discovered. \nFor example, genre atoms gain  a set of mood prediction \ndatatype value during the semantic reasoning, and after accumulation and normalization,  some of them reflect \nvery strong associations with mood. Tab. 2 lists the result of genre atoms ranked by their bias degree among mood clusters, which is in good accordance with people’s judgement and discovers the implied semantic associa-\ntions. \n7. CONCLUSION \nWe found that by unleashing music related information \nfrom various resources via an ontology-based system and by considering the internal se mantic links for reasoning, \nwe achieve a significant precision improvement for pre-dicting mood. To augment the knowledge base efficiently and to make it free of manual annotation, we propose a WordNet-based method to map social tags to a pre-defined taxomony. Although in this work we mainly dis-cuss mood, since it is one of the most representative high-\nlevel music semantic information, we argue that the pro-posed method could also be applied for predicting other high-level semantics, for example, if music usage or ge-nre style are of interest for an  application, we could adjust \nthe initiation processes and m odify corresponding reason-\ning rules accordingly, so this work has potential applica-\ntions for other tasks of music recommendation, indexing, \ndocumentation and retrieval. \n8. REFERENCES \n[1] Hu, X.; Downie, J. S.; Laurier, C.; Bay, M.; Ehmann, A. F.: “The \n2007 MIREX Audio Mood Classification Task: Lessons Learned,” \nISMIR’08. \n[2] Pasca, M.: “Turing Web Text and Search Queries into Factual Know-\nledge: Hierarchical Class Attribute Extraction,” In Proceedings of the 23rd AAAI Conference, 1225-1230. \n[3] Barrington, L.; Yazdani, M.; Turnbull, D.; Lanckriet, G.: “Combining Feature Kernels for Semantic Music Retrieval,” ISMIR’08. \n[4] Bischoff, K.; Firan, C.S.; Paiu R.; Nejdl, W.; Laurier, C.; Sordo, M.: \n“Music Mood and Theme Classification- a Hybrid Approach,” Proc. of ISMIR 2009. \n[5] Blei D.; Ng A.; Jordan M.: “Laten t Dirichlet Allocation,” Journal of \nMachine Learning Research, Vol. 3, pp.993–1022, Jan. 2003, MIT \nPress. \n[6] Levy, M.; Sandler, M.: “A Semantic Space for Music Derived from \nSocial Tags,” In Proceedings of ISMIR 2007. \n[7] C. Laurier, M. Sordo, J. Serrà, P. Herrera: “Music Mood Representa-\ntions from Social Tags,” Proc. of ISMIR 2009. \n[8] Peraldi, S. E.; Kaya, A.; Melzer, S.; Moller, R.; Wessel, M.: “Multi-media Interpretation as Abduction”. Proc. DL 2007. \n[9] Penta, A.; Picariello, A.; Tanca,  L.: “Multimedia Knowledge Man-\nagement Using Ontologies,” IEEE Intelligent Systems, 2003. \n[10] F. Giasson and Y. Raimond: \"Music ontology specification. Online ontology,\" 2008. \n[11] Hall, M.; Frank, E.; Holmes, G.; Pf ahringer, B. Reutemann, P.; Witten \nI.H.: “The Weka Data Mining Software: An Update,” SIGKDD Ex-plorations, Vol. 11, Issue1. \n[12] Haarslev,V.; Moller,R.; Wessel,M.: “RacerPro User's Guide and Ref-erence Manual Version 1.9.2”.  Table 2 . Ranking genre atoms according to mood bias \nGenre Mood probability prediction \nCluster1 Cluster2 Cluster3 Cluster4 Cluster5\nSolo istru. 0 0.14 0.83 0.03 0 \nHalloween 0.01 0.23 0 0.76 0 \nNoise 0.13 0.07 0 0.07 0.73 \nComedy 0.1 0.06 0.06 0.71 0.07 \nSad core 0.01 0.03 0.71 0.09 0.16 \nPunk metal 0.32 0 0.04 0 0.64 \nChildren’s 0 0.61 0 0.39 0 \nSweet band 0.20 0.58 0.14 0.08 0 \nHair metal 0.54 0.13 0.05 0.09 0.18 \nSkiffle 0.53 0.31 0 0.04 0.12 \n410\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "An Improved Query by Singing/Humming System Using Melody and Lyrics Information.",
        "author": [
            "Chung-Che Wang",
            "Jyh-Shing Roger Jang",
            "Wennen Wang"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1414802",
        "url": "https://doi.org/10.5281/zenodo.1414802",
        "ee": "https://zenodo.org/records/1414802/files/WangJW10.pdf",
        "abstract": "This paper proposes an improved query by sing- ing/humming (QBSH) system using both melody and lyr- ics information for achieving better performance. Sing- ing/humming discrimination (SHD) is first performed to distinguish singing from humming queries. For a hum- ming query, we apply a pitch-only melody recognition method that has been used for QBSH task at MIREX with rank-1 performance. For a singing query, we combine the scores from melody recognition and lyrics recognition to take advantage of the extra lyrics information. Lyrics rec- ognition is based on a modified tree lexicon that is com- monly used in speech recognition. The performance of the overall QBSH system achieves 39.01% and 23.53% error reduction rates, respectively, for top-20 recognition under two experimental settings, indicating the feasibility of the proposed method.",
        "zenodo_id": 1414802,
        "dblp_key": "conf/ismir/WangJW10",
        "keywords": [
            "query",
            "sing-humming",
            "system",
            "melody",
            "lyrics",
            "performance",
            "improved",
            "QBSH",
            "discrimination",
            "SHD"
        ],
        "content": "An Improved Query by Singing/Humming System Using Melody \nand Lyrics Information \nChung -Che Wang  Jyh-Shing Roger Jang  Wennen Wang  \nMIR Lab, Dept. of CS,  \nTsing Hua Univ ., Taiwan   \ngeniustur tle \n@mirlab.org  MIR Lab, Dept. of CS,  \nTsing Hua Univ ., Taiwan   \njang@mirlab.org  Institute for Information \nIndustry, Taiwan  \nwennen@iii.org.tw  \nABSTRACT \nThis paper proposes an improved query by sin g-\ning/humming (QBSH) system using both melody and ly r-\nics information for achieving better performance. Sin g-\ning/humming discrimination (SHD) is first performed to \ndistinguish singing from humming queries. For a hu m-\nming query, we apply a pitch-only melody recognitio n \nmethod that has been used for QBSH task at MIREX with \nrank-1 performance. For a singing query, we combine the \nscores from melody recognition and lyrics recognition to \ntake advantage of the extra lyrics information. Lyrics re c-\nognition is based on a modified tree lexicon that is co m-\nmonly used in speech recognition. The performance of \nthe overall QBSH system achieves 39.01% and 23.53 % \nerror reduction rates, respectively, for top-20 recognition \nunder two experimental settings, indicating the feasibility \nof the proposed method. \n1. INTRODUCTION \nQuery by singing/humming (QBSH) is an intuitive m e-\nthod for music retrieval. With a QBSH system, users are \nable to retrieve intended songs by singing or humming a \nportion of the intended song in order to retrieve it. Most \nof the QBSH researches  so far utilize melody information \nas the only cue for retrieval [1-3]. Ghias et al. [1] pr o-\nposed one of the early papers of query by humming , \nwhich used three different characters („U‟, „D‟, and „S‟)  \nare to represent pitch contours. McNab et al. [2] enhanced \nthe representation by considering rhythm information of \nsegmented notes.  Jang and Gao [3] proposed the first \nQBSH system using dynamic time warping (DTW) over \nframe-based pitch contours, which accommodates natural \nsinging/humming for better performance . More recently, \nQBSH task is held in MIREX since 2006 and quite a few \nrelated methods and corresponding performance can be \nfound therein [ 12]. \nLyrics are also an important part of a song which serve \nthe cue for detecting the song's identity, or its mood or \ngenre. However, the use of lyrics for content-based music \nanalysis appears much later. Wu et al. [4] and Chen [ 14] \nused lyrics to enhance music mood estimation. Wang et al. [5] proposed one of the few music information retrie v-\nal systems which used both lyrics and melody inform a-\ntion. However, queried lyrics were input by the user i n-\nstead of decoded from the user's acoustic input. Xu et al. \n[6] suggested that acoustic distance must be considered \nfor a lyrics search when the user input approximate lyrics \nquery . Our method also takes advantage of the extra i n-\nformation provided by the lyrics, except that we attempt \nto decode the queried lyrics from the user's singing input \ndirectly, which does not impose extra efforts on the user. \nSuzuki et al. [13] also proposed a similar system which \ntook singing input for lyrics recognition, and the results \nwere verified by the corresponding melody information. \nHowever, their system could not handle humming input, \nwhich is likely to happen in a music retrieval system. \nMoreover, the corpus used for their experiments is too \nsmall to justify the method ‟s feasibility. \nThe proposed QBSH system uses singing/humming \ndiscrimination (SHD) to detect whether there exists lyrics \ninformation. If yes, we apply speech recognition to d e-\ncode the lyrics information and come up with a lyrics \nscore. The lyrics score is combined with the melody score \nto enhance the recognition performance. \nThe remainder of this paper is organized as follows. \nThe proposed QBSH system is introduced in section 2. \nExperiment al results are shown in section 3. We conclude \nthis paper and address directions for future work in se c-\ntion 4. \n2. SYSTEM OVERVIEW \nFigure 1 shows the schematic diagram of the proposed \nsystem, in which blocks enclosed by thicker lines are the \nmethods proposed by this paper. In the offline part, \nacoustic models and test corpus are used to obtain the \nmodel similarity, where each model is characterized by \nan RCD (right-context dependent) bi-phone. Phone-level \nsimilarity is used for SHD, and syllable-level similarity \nis used for lyric scoring. Lexico n network is also created \nin this part for lyrics recognition . In the online part, SHD \nis first performed to decide if the acoustic input is sin g-\ning or humming. If the input is classified as humming, \nthe result is based on melody recognition alone . On the \nother hand, if the input is classified as singing, lyrics \nrecognition is performed to obtain a decoded string of \nlyrics. The output of our system then uses the combined \nscores of melody and lyrics. The melody recognition \nmodule uses UPDUDP [ 11] for pitch extraction and l i-\nnear scaling (LS) [7] for comparison, which achieved the  \nPermission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fee provided that copies \nare not made or distributed for profit or commercial advantage and that \ncopies bear this notice and the full cit ation on the first page.   \n© 2010 International Society for Music Information Retrieval  \n45\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \nbest performance during QBSH task in MIREX2009 [ 12]. \nThe other components will be explained in detail in the \nfollowing subsections. \n \nFigure 1 . The proposed system.  \n \n2.1 Phone and Syllable Similarity \nAn intuitive approach to SHD is based on the number of \ndistinct phones decoded in the user's acoustic input. The \nmore distinct phones in an acoustic query, the more likely \nthe query is singing instead of humming. In counting di s-\ntinct phones, we also need to take phone similarity into \nconsideration for achieve most robust results. Moreover, \nwe also need to have similarity between syllables for o b-\ntaining lyrics score . The procedure for computing phone \nand syllable similarity is explained next. \nFirstly, we can obtain the confusion matrix of 156 bi -\nphone models by performing free phone decoding on a \nspeech corpus by HTK [8]. Then we can use the accuracy \nrates of the confusion matrix as the similarity measure \nbetween any two phone models. It should be noted that \nthe similarity is not symmetric, but this does not affect \nthe proposed methods.  \nAfter defining phone similarity 𝐾, the similarity matrix \nof 423 Mandarin syllables can be computed via dynamic \nprogramming (DP) as follows. Considering two syllables \n𝑆𝑦𝑙𝐴 and 𝑆𝑦𝑙𝐵, with phone sequence 𝑎1,𝑎2,…,𝑎𝑚 and \n𝑏1,𝑏2,…,𝑏𝑛, respectively, the definition of the similarity \nbetween 𝑆𝑦𝑙𝐴 and 𝑆𝑦𝑙𝐵 is: \n \n𝑠𝑖𝑚 𝑆𝑦𝑙𝐴,𝑆𝑦𝑙𝐵 =  𝑡𝐴,𝐵(𝑚,𝑛)\nmax (𝑚,𝑛) ,                  (1) \n \nwhere the recursive formula of 𝑡𝐴,𝐵 is: \n \n𝑡𝐴,𝐵 𝑖,𝑗 = max 𝑡𝐴,𝐵 𝑖 −1,𝑗 \n𝑡𝐴,𝐵 𝑖,𝑗−1 \n𝐾(𝑎𝑖,𝑏𝑗)+𝑡𝐴,𝐵 𝑖 −1,𝑗−1  ,     (2) \n \nwith boundary conditions : \n 𝑡𝐴,𝐵 𝑖,𝑗 =  0,𝑖𝑓 𝑖= 0 𝑜𝑟 𝑗= 0.                 (3) \n \nFig\nure 2 shows the image of 423 x 423 similarity matrix \nin gray scale, where white points represent 1, and black \npoints represent 0. \n \nFigure 2 . The similarity matrix of 423 Mandarin sy l-\nlables displayed as a gray-scale image.  \n \n2.2 Singing/Humming Discrimination \nThe basic rationale behind SHD is that the number of dis-\ntinct phones occurring in humming is often less than that \nin singing. Thus, free phone decoding is performed on the \nsinging input to obtain a phone sequence. If these phones \nare similarity in acoustics, then the effective count of dis-\ntinct phones should be less. Assume there are n distinct \nphones 𝑎1,𝑎2,…,𝑎𝑛 in the decoded phone sequence, then \nthe effective count is defined as follows. Let 𝑃 to be a sub \nmatrix of 𝑠𝑖𝑚 for 𝑎1𝑎2,…,𝑎𝑛, then we can calculate the \neffective phone count in the sequence: \n \n𝑟=𝑛+ 1−𝑚𝑒𝑑𝑖𝑎𝑛 (𝑠)                   (4) \n \nwhere 𝑟 is the effective phone count, and 𝑠 is the column \nsum of 𝑃.  A lower effective phone count indicates that \nthe acoustic query is more likely to be humming instead \nof singing. In particular, when 𝑎1𝑎2,…,𝑎𝑛 are very di f-\nferent in pronunciation, then 𝑚𝑒𝑑𝑖𝑎𝑛 𝑠  is close to 1 and \n𝑟 is close to 𝑛. On the other hand, if these phones are \nvery similar in pronunciation, then 𝑚𝑒𝑑𝑖𝑎𝑛 𝑠  is close to \n𝑛 and 𝑟 is close to 1. \nThus an optimum threshold of effective phone count \ncan be set for SHD for minimizing classification error. \n2.3 Lyrics Recognition \nIf an acoustic query is classified as singing, we can apply \nlyrics recognition for better performance. Since the ave r-\n46\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \nage length of a singing query is about 7 seconds, the first \n30 syllables of each song are used to build up the recogn i-\ntion network. (Without loss of generality, here we assume \nthe anchor position of each query is the beginning of a \nsong. If not, then we can simply use the phrase onset as \nthe beginning to select the first 30 syllables for building \nthe network. ) By considering the recognition network as a \nfinite state machine, this network is determinized and \nminimized by AT&T FSM tool [9]. Moreover, to handle \nthe case of \"stop  in the middle \", an epsilon transition is \nadded between each internal state and the terminal  state. \nFigure 3  shows an example  of the network , consisting of \nthe first 3 syllables of 2 songs , where S ong-𝑖-Syl-𝑗 de-\nnotes the 𝑗-th syllable o f the 𝑖-th song, and <eps> denotes \nthe epsilon transition.  \n \nFigure 3 . An example of the recognition network.  \n \n2.4 Lyrics Scoring and Combination \nAfter running Viterbi search over the recognition network, \nwe can decode a syllable sequence that has the maximum \nlikelihood. To obtain the similarity score based on lyrics, \nwe then compare the decoded syllable sequence with the \n30 syllables of each song. This is again achieved by DP \ninstead of using exact string matching since we want to \nhave a score indicating the similarity. Consider two syll a-\nbles sequences, 𝑆𝑒𝑞𝐴 and 𝑆𝑒𝑞𝐵 containing syllables \n𝐴1,𝐴2,…,𝐴𝑚 and 𝐵1,𝐵2,…,𝐵𝑛 respectively. The recu r-\nsive formula for DP is: \n \n𝑡(𝑖,𝑗) =𝑚𝑎𝑥 𝑡(𝑖 −1,𝑗)\n𝑡(𝑖,𝑗−1)\n𝑠𝑖𝑚 𝐴 𝑖,𝐵𝑗 +𝑡(𝑖 −1,𝑗−1)  ,  (5) \n \nwith boundary conditions :  \n \n𝑡 𝑖,𝑗 =  0,𝑖𝑓 𝑖= 0 𝑜𝑟 𝑗= 0               (6) \n \nThus, 𝑡(𝑚,𝑛) can be taken as a similarity score between \nthe decoded string from the query and the lyrics of each \nsong in the database. In implementation, we let 𝑆𝑒𝑞𝐴 to \nbe the decoded string, and the first 𝑘 syllables of the ly r-\nics (with 𝑘 equal to the length of 𝑆𝑒𝑞𝐴) to be 𝑆𝑒𝑞𝐵 for \ncomputing the score. \nFor score combination, we need to normalize each i n-\ndividual score. For a given query to a song database of \n2000 songs, we will eventually obtain vector 𝐿 of size 2000 representing the lyrics raw similarity scores, and \nvector 𝑀 of size 2000 representing the melody raw di s-\ntance measures (computed by LS).  We than linearly \nnormalize 𝑀 and −𝐿, respectively, to the range [0, 1]. \nThe normalized distance vectors 𝑀′ and 𝐿′ are then co m-\nbined via the following formula:  \n \n   𝐶=𝑝× log𝑀′+ (1−𝑝) × log𝐿′            ( 7) \n \nThen the minimum entry in C corresponds to the most \nlikely song considering both lyrics and melody. Note that \nif 𝑝 is equal to 0, only the lyr ic information is considered . \nOn the other hand, if 𝑝 is equal to 1, only the melody i n-\nformation is considered. The value of 𝑝 was set to 0.5 \nempirically in our experiments. \n \n3. EXPERIMENT \n3.1 The Dataset \nThe public corpus MIR-QBSH [10] is used extensively in \nour experiment, where the anchor positions for all queries \nare from the beginning. Since our speech recognition e n-\ngine is for Mandarin, therefore we selected 2469 clips \nfrom the corpus which correspond to 23 Mandarin songs. \nTo increase the complexity of the comparison, we added \n2131 noise songs to the database, such that the total size \nof the database is 2154. \nFirst of all, 80 clips are selected from the corpus with \neven ly distribut ed gender and types (singing/humming) . \nThese clips were hand labeled to have the ground truth, \nand then used to train the threshold-based classifier for \nSHD. The remaining 2389 clips are used for testing the \noverall performance of our QBSH system .  \nAcoustic models of RCD bi-phones for computing \nphone/syllable similarity were obtained by training over a \nnormal Mandarin speech of 80 subjects. \n3.2 Experimental Results \n3.2.1 Results of SHD \nFigure 4 shows the detection error tradeoff (DET) curve \nof SHD using the training data of 80 clips, where singing \nclips are view ed as positive while humming clips neg a-\ntive. Based on this plot, the threshold of effective count is \nset to 20.4958 for SHD to achieve equal error rates of \nfalse positive and false negative. Figure 5 gives the dis-\ntribution of the effective phone counts of the training data , \ntogether with the identified Gaussian models via max i-\nmum likelihood estimate. The Gaussian models for pos i-\ntive data (of size 𝑛1)  and negative data (of size 𝑛2) are \ndenoted as 𝑔1 and 𝑔1, respectively, in the figure. \n47\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \n \nFigure 4 . The DET curve of the training data for SHD. \n \n  \nFigure 5 . The distribution of the effective phone counts \nof the training data of SHD. The Gaussian models for \npositive data (of size 𝑛1)  and negative data (of size 𝑛2) \nare 𝑔1 and 𝑔1, respectively. \n \nTo evaluate the performance of SHD over unseen data , \n1183 clips were selected (out of the 2389 clips) and hand \nlabeled as singing or humming. Table 1 shows confusion \nmatrix of SHD, with a recognition rate of 78.61% . In pa r-\nticular, 10.99% of the humming clips are misclassified as \nsinging, which may generate erroneous output in lyrics \nrecognition. Initial error analysis indicates that some of \nthe misclassified humming clips are caused by a variety \nof pronunciation during the humming. On the other hand, \n24.51% of the singing clips are misclassified as humming , \nwhich is not so detrimental to the overall performance \nsince the accuracy of melody recognition is already high. \n \n \n recognition r esults  \nground truth  singing  humming  \nsinging  75.49%  \n(687)  24.51%  \n(223)  \nhumming  10.99%  \n(30) 89.01%  \n(243)  \nTable 1.  Recognition result of SHD. \n \n3.2.2 Lyrics recognition and Combined Results \nWhen we applied SHD to 2389 clips, 1477 of them were \nclassified as singing. Figure 6 shows the top-20 recogni-\ntion rate of these 1477 clips over a song database of size \n2154 . The top- 20 recognition rate is 72.99%. \n \nFigure 6 . The top-20 lyrics recognition rate of 1477 clips \nclassified as singing. \n \nThe value of 𝑝 in Eq. (7) was set to 0.5 empirically. \nNow it is time to do a post analysis by plotting the overall \nrecognition rates versus the values of 𝑝, as shown in Fi g-\nure 7. Apparently the performance stays much the same \nfor these two cases of LS resolution equal to 11 and 51, \nrespectively, as long as the value of p lies within [0.3, \n0.9]. This confirms our selection of p value of 0.5. \n \n48\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \n \nFigure 7 . The plots of overall recognition rates with r e-\nspect to the value of 𝑝, for two cases of different LS res o-\nlutions. The maximum of these two curves are labeled \nwith circles.  \n \nFigure 8 shows the overall performance of the pr o-\nposed QBSH system. Different values of resolution in LS, \n11 and 51, were used in this experiment. The lower and \nupper ratios of LS were set to be 0.5 and 2. The top-20 \nrecognition rates of resolution 11 and 51 are 92.80% and \n96.19%, respectively , which out perform the baseline sy s-\ntem (88.20% and 95.02%). The error reduction rates are \n39.01 % and 23.53 %, respectively . \n \n \nFigure 8 . The top-N recognition rate of 2389 clips. \n \n4. CO\nNCLUSIONS AND FUTURE WORK \nIn this paper, we have proposed an improved QBSH sy s-\ntem that distinguishes singing queries from humming \nones, and then applies different procedures in order to \ntake advantage of the lyric information of singing input. \nThe experimental results demonstrate the effectiveness of \nthe proposed system, with error reduction rates of 39.01 % and 23.53 % for LS with resolutions of 11 and 51,  respe c-\ntively . \nSeveral directions for immediate future work are under \nway. Currently, our acoustic models were obtained by \ntraining on normal speech corpus. This can be improved \nby training or simply adapting using singing corpora i n-\nstead. Moreover, it would be desirable to incorporate \nmulti-lingual speech recognition since there are quite a \nfew famous songs with the same tune but different lyrics \nin different languages.  \n \n5. ACKNOWLEDGEMENT \nThis study is conducted under the “III Innovative and \nProspective Technologies Project” of the Institute for I n-\nformation Industry which is subsidized by the Ministry of \nEconomy Affairs of the Republic of China. \n6. REFERENCES \n[1] A. J. Ghias, D. C. Logan, and B. C. Smith, “Query \nby humming-musical information retrieval in an \naudio database,” in Proc. ACM Multimedia’95 , San \nFrancisco, 1995, pp. 216 –221. \n[2] R. J. McNab, L. A. Smith, I. H. Witten, C. L. \nHenderson, and S. J. Cunningham, “Toward the \ndigital music library: Tune retrieval from acoustic \ninput,” in Proc. ACM Digital Libraries , 1996, pp. \n11–18. \n[3] J.-S. R. Jang and M.- Y. Gao, “A query -by-Singing \nsystem based on dynamic programming,” in Proc. \nInt. Workshop Intell. Syst. Resolutions (8th Bellman \nContinuum) , Hsinchu, Taiwan, R.O.C., Dec. 2000, \npp. 85 –89. \n[4] Y.-S. Wu, W.-r. Chu, C.-Y. Chi, D. C. Wu, R. T.-H. \nTsai, and J Y.- j Hsu, “ The Power of Words: \nEnhancing Music Mood Estimation with Textual \nInput of Lyrics ,” International Conference on \nAffective Computing & Intelligent Interaction , pp. \n1–6, 2009. \n[5] T. Wang, D.-J. Kim, K.-S. Hong, and J.-S. Youn, \n“Music Information Retrieval System using Lyrics \nand Melody Information ,” Asia-Pacific Conference \non Information Processing , pp. 601 –604, 2009. \n[6] X. Xu, M. Naito, T. Kato, and H. Kawai, “Robust \nand Fast Lyric Search Based on Phonetic Confusion \nMatrix,” Proceedings of the International \nSymposium on Music Information Retrieval , pp. \n417–422, 2009. \n[7] J.-S. R. Jang, H.-R. Lee, M.- Y. Kao, “Content -based \nMusic Retrieval Using Linear Scaling and Branch-\nand-Bound Tree search,” in Proc. of IEEE \n49\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)  \n \nInternational Conference on Multimedia and Expo , \nAugust 2001. \n[8] Cambridge University Engineering Department , \nHTK Web- Site, http://htk.eng.cam.ac.uk/, 2006 \n[9] AT&T Labs Research , AT&T Labs Research - FSM \nLibrary , \nhttp://www2.research.att.com/~fsmtools/fsm/, 2008 \n[10] J.-S. R. Jang, \"MIR-QBSH Corpus\", MIR Lab, CS \nDept, Tsing Hua Univ, Taiwan. Available at the \n\"MIR-QBSH Corpus\" link at \nhttp://www.cs.nthu.edu.tw/~jang. \n[11] J.-C. Chen, J.- S. R. Jang, \"TRUES: Tone \nRecognition Using Extended Segments\",  ACM \nTransactions on Asian Language Information \nProcessing, No. 10, Vol. 7 , Aug 2008. \n[12] MIREX 2009 , http://www.music-\nir.org/mirexwiki/index.php/Main_Page, 2009 \n[13] M. Suzuki, T. Hosoya, A. Ito, and S. Makino, \n“Music Information Retrieval from a Singing Voice \nUsing Lyrics and Melody Information,” EURASIP \nJournal on Advances in Signal Processing , vol. 2007, \nArticle ID 38727, 8 pages, 2007. \ndoi:10.1155/2007/38727 \n[14] J.-H C hen, “ Content-based Music Emotion Analysis \nand Recognition”, Master Thesis, CS Dept., \nNational Tsing Hua University, June 2006 \n50\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "Are Tags Better Than Audio? The Effect of Joint Use of Tags and Audio Content Features for Artistic Style Clustering.",
        "author": [
            "Dingding Wang 0001",
            "Tao Li 0001",
            "Mitsunori Ogihara"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1417543",
        "url": "https://doi.org/10.5281/zenodo.1417543",
        "ee": "https://zenodo.org/records/1417543/files/WangLO10.pdf",
        "abstract": "Social tags are receiving growing interests in informa- tion retrieval. In music information retrieval previous re- search has demonstrated that tags can assist in music clas- sification and clustering. This paper studies the problem of combining tags and audio contents for artistic style clus- tering. After studying the effectiveness of using tags and audio contents separately for clustering, this paper pro- poses a novel language model that makes use of both data sources. Experiments with various methods for combining feature sets demonstrate that tag features are more useful than audio content features for style clustering and that the proposed model can marginally improve clustering perfor- mance by combing tags and audio contents.",
        "zenodo_id": 1417543,
        "dblp_key": "conf/ismir/WangLO10",
        "keywords": [
            "Social tags",
            "information retrieval",
            "music classification",
            "clustering",
            "language model",
            "audio contents",
            "artistic style clustering",
            "tag features",
            "audio content features",
            "performance improvement"
        ],
        "content": "ARE T AGS BETTER THAN AUDIO FEATURES? THE EFFECT OF JOINT\nUSE OF TAGS AND AUDIO CONTENT FEATURES FOR ARTISTIC STYLE\nCLUSTERING\nDingding Wang\nSchool of Computer Science\nFlorida International University\nMiami, FL USA\ndwang003@cs.fiu.eduTao Li\nSchool of Computer Science\nFlorida International University\nMiami, FL USA\ntaoli@cs.fiu.eduMitsunori Ogihara\nDepartment of Computer Science\nUniversity of Miami\nCoral Gables, FL USA\nogihara@cs.miami.edu\nABSTRACT\nSocial tags are receiving growing interests in informa-\ntion retrieval. In music information retrieval previous re-\nsearch has demonstrated that tags can assist in music clas-\nsiﬁcation and clustering. This paper studies the problem of\ncombining tags and audio contents for artistic style clus-\ntering. After studying the effectiveness of using tags and\naudio contents separately for clustering, this paper pro-\nposes a novel language model that makes use of both data\nsources. Experiments with various methods for combining\nfeature sets demonstrate that tag features are more useful\nthan audio content features for style clustering and that the\nproposed model can marginally improve clustering perfor-\nmance by combing tags and audio contents.\n1. INTRODUCTION\nThe rapid growth of music the Internet both in quantity and\nin diversity has raised the importance of music style analy-\nsis (e.g., music style classiﬁcation and clustering) in music\ninformation retrieval research [10]. Since a music style is\ngenerally included in a music genre (e.g., the style Progres-\nsive Rock within the genre of Rock) a style provides ﬁner\ncategorization of music than its enclosing genre. Also, for\nmuch the same reason that all music in a single genre has\nsome commonality, all music in a single style has some\ncommonality belonging to a same style, and the degree\nof commonality is stronger within a style than within its\nenclosing genre. These properties suggest that by way of\nappropriate music analysis, it is possible to computation-\nally organize music sources into not only musicologically\nmeaningful groups but also into hierarchical clusters that\nreﬂect style and genre similarities. Such organizations are\nlikely to enable efﬁcient browsing and navigation of music\nitems.\nMuch of the past work on music style analysis meth-\nods is based solely on audio contents and various feature\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc°2010 International Society for Music Information Retrieval.extraction methods have been tested. For example, [32]\npresents a study on music classiﬁcation using short-time\nanalysis along with data mining techniques to distinguish\namong ﬁve music styles. Pampalk et al. [17] combine\ndifferent similarity sources based on ﬂuctuation patterns\nand use a nearest neighbor classiﬁer to categorize music\nitems. More recently Chen and Chen [3] use long-term\nand short-term features that represent the time-varying be-\nhavior of music and apply support vector machines (SVM)\nto classify music into genres. Although these audio-\ncontent-based classiﬁcation methods are successful, music\nstyle classiﬁcation and clustering are difﬁcult problems to\ntackle, in part because music style classes are more nu-\nmerous than music genres and thus computation quickly\nreaches a limit in terms of the number of styles to classify\nmusic into. One then naturally asks whether adding non-\naudio features push style classiﬁcation/clustering beyond\nthe limit of audio-feature-based analysis.\nFortunately, the rapid development of web technologies\nhas made available a large quantity of non-acoustic infor-\nmation about music, including lyrics and social tags, latter\nof which can be collected by a variety of approaches [24].\nThere has already been some work toward social tag based\nmusic information retrieval [1,11,13,16,23]. For example,\nLevy and Sandler [16] demonstrate that the co-occurrence\npatterns of words in social tags are highly effective in\ncapturing music similarity, Bischoff et al. [1] discuss the\npotential of different kinds of tags for improving music\nsearch, and Symeonidis et al. [23] propose a music recom-\nmendation system by performing latent semantic analysis\nand dimensionality reduction using the higher order SVD\ntechnique on a user-tag-item tensor.\nIn this paper we consider social tags as the source of\nnon-audio information. We naturally ask whether we can\neffectively combine the non-audio and audio information\nsources to improve performance of music retrieval. Some\nprior work has demonstrated that using both text and audio\nfeatures can improve the ranking quality in music search\nsystems. For example, Turnbull et al. [25] successfully\ncombine audio-content features (MFCC and Chroma) with\nsocial tags via machine learning methods for music search-\ning and ranking. Also, Knees et al. [12] incorporate au-\ndio contents into a text-based similarity ranking process.\n57\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Howe\nver, few efforts have been made to examine the ef-\nfect of combining tags and audio-contents for music style\nanalysis. We thus the question of, given tags and repre-\nsentative pieces for each artist of concern, whether the tags\nand the audio-contents of the representative pieces com-\nplement each other with respect to artist style clustering,\nand if so, how efﬁciently those pieces of information can\nbe combined.\nIn this paper, we study the above questions by treat-\ning the artist style clustering problem as an unsupervised\nclustering problem. We ﬁrst apply various clustering algo-\nrithms using tags and audio features separately, and exam-\nine the usefulness of the two data sources for style cluster-\ning. Then we propose a new tag+content (TC) model for\nintegrating tags and audio contents. A set of experiments\nis conducted on a small data set to compare our model with\nother methods, and then we explore whether combining the\ntwo information sources can improve the clustering perfor-\nmance or not.\nThe rest of this paper is organized as follows. In Section\n2 we brieﬂy discuss the related work. In Section 3 we intro-\nduce our proposed TC model for combining tags and con-\ntents for artist style clustering. We conduct comprehensive\nexperiments on a real world dataset and the experimental\nresults are presented in Section 4. Section 5 concludes.\n2. RELATED WORK\nAudio content based automatic music analysis (clustering,\nclassiﬁcation, and similarity search in particular) is one of\nthe most important topics in music information retrieval.\nThe most widely used audio features are timbral texture\nfeatures (see, e.g., [26]), which usually consist of Short\nTerm Fourier Transform (STFT) and Mel-Frequency Cep-\nstral Coefﬁcients (MFCC) [20]. Researchers have applied\nvarious data mining and statistically methods on these fea-\ntures for classifying or clustering artists, albums, and songs\n(see, e.g., [3, 5, 18, 19, 26]).\nMusic social tags have recently emerged as a popular\ninformation source for curating music collections on the\nweb and for enabling visitors of such collections to express\ntheir feelings about particular artists, albums, and pieces.\nSocial tags are free-text descriptions of any length (though\nin practice there sometimes is a limit in terms of number of\ncharacters) with no restriction on the words that are used.\nSocial tags thus can be as simple as a single word and as\ncomplicated as a long, full sentence. Popular short tags\ninclude heavy rock, black metal, and indie pop and long\ntags can be like “I love you baby, can I have some more?”\nAs can be easily seen social tags are not as formal as\ndescriptions that experts such as musicologists provide.\nHowever, by collecting a large number of tags for one sin-\ngle piece of music or for one single artist, it seems pos-\nsible to gain understanding of how the song or the artist\nis received by the general listeners. As Lamere and Pam-\npalk point out [13] social tags are widely used to enhance\nsimple search, similarity analysis, and clustering of music\nitems [13]. Lehwark, Risi, and Ultsi [15] use Emergent-\nSelf-Organizing-Maps (ESOM) and U-Map techniques ontagged music data to conduct clustering and visualization\nin music collections. Levy and Sandler [16] apply latent\nsemantic dimension reduction methods to discover new se-\nmantics from social tags for music. Karydisi et al. [11] pro-\npose a tensor-based algorithm to cluster music items using\n3-way relational data involving song, users, and tags.\nIn the information retrieval community a few attempts\nhave been made to complement document clustering us-\ning user-generated tags as an additional information source\n(see, e.g., [21]). In such work the role that social tags play\nis only supplementary because the texts appearing in the\noriginal data are, naturally, highly more informative than\ntags.\nThe situation in the MIR community seems different\nfrom this and the use of tags seems to show much stronger\npromise. This is because audio contents, which are the\nstandard source of information, have to go through feature\nextraction for syntactic or semantic understanding and thus\nthe distance between the original data source and the tag\nin terms of informativeness appears to be much smaller in\nMIR than in IR.\nThere has been some work exploring the effectiveness\nof joint use of the two types of information sources for re-\ntrieval, including including the work in [25] and [12] where\naudio contents and tags are combined for searching and\nranking and the work in [30] that attempts to integrate au-\ndio contents and tags for multi-label classiﬁcation of mu-\nsic styles. These prior efforts are concerned with super-\nvised learning (i.e., classiﬁcation) while the present paper\nis concerned with unsupervised learning (i.e., clustering).\n3. TAG+CONTENT MODEL (TC)\nHere we present our novel language model for integrating\ntags and audio contents and how to use the model for artis-\ntic style clustering.\n3.1 The Model\nLetAbe the set of artists of interest, Sthe set of styles of\ninterest, and Tthe set of tags of interest. We assume that\nfor each artist, for each style, and for each artist-style pair,\nits tag set (as a multiset in which same elements may be re-\npeated more than once) is generated by mutually indepen-\ndent selections. That is, for each artist a2 A and for each\nnonempty set of tags t= (t 1; : : : ; t n),t1; : : : ; t n2 T, we\ndeﬁne the language model, p(tja), by\np(tja) =nY\ni=1p(tija)\nSimilarly, for each style s2 S , we deﬁne its language\nmodel p(tjs), by\np(tjs) =nY\ni=1p(tijs)\nAlthough we might want to consider the artist-style joint\nlanguage model p(tja; s), we assume that the model is\n58\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)dictated only\nby the style and that it is independent of the\nartist. Thus, we assume\np(tja; s) = p(tjs)\nfor all tags t2 T. Then the artist language model can be\ndecomposed into several common style language models:\np(tja) =X\ns2Sp(tjs)p(sja):\nInstead of directly choosing one style for artist a, we as-\nsume that the style language models are mixtures of some\nmodels for the artists linked to a, i.e.,\np(sja) =X\nb2Ap(sjb)p(bja);\nwhere bis an artist linked to artist a. Combining these\nyields the following model:\np(~tja) =nY\ni=1X\ns2SX\nb2Ap(tijs)p(sjb)p(bja):\nWe use the empirical distribution of the observed artists\nsimilarity graph for p(bja)and let Bb;a= ~p(bja). The\nmodel parameters are (U;V), where\nUt;s=p(tjs);Vb;s=p(bjs):\nThus, p(tija) = [UV>B]t;a.\nThe artist similarity graph can be obtained using meth-\nods described in Section 3.2. Now we take the Dirichlet\ndistribution, the conjugate prior of multinomial distribu-\ntion, as the prior distribution of UandV. The parame-\nter estimation is maximum a posteriori (MAP) estimation.\nThe task is\nU;V= arg min\nU;V`(U;V); (1)\nwhere `(U;V) = KL³\nAkUV>B´\n¡ln Pr(U;V).\nUsing an algorithm similar to the nonnegative matrix\nfactorization (NMF) algorithm in [14], we obtain the fol-\nlowing updating rules:\nUtsÃUtsh\nCB>Vi\nts\nVbsÃVbsh\nBC>Ui\nbs\nwhere Cij=Aij=[UV>B]ij. The computational algo-\nrithm is given in Section 3.3.\n3.2 Artist Similarity Graph Construction\nBased on the audio content features, we can construct the\nartist similarity graph using one of the following popular\nmethods, which is due to Zhu [33].\n²NN graphs A strategy for artist graph construction is the\n²-nearest neighbor algorithm based on the distance\nbetween the feature values of two artists. For a pair\nof artists iandj, if the distance d(i; j)is at most ²,\ndraw an edge between them. The parameter ²con-\ntrols the neighborhood radius. For the distance mea-\nsured, the Euclidean distance is used throughout the\nexperiments.exp-weighted graphs This is a continuous weighting\nscheme where Wij= exp(¡d(i; j )2=®2). The pa-\nrameter ®controls the decay rate and is set to 0.05\nempirically.\n3.3 The Algorithm\nAlgorithm 1 is our method for estimating the model pa-\nrameters.\nAlgorithm 1 P\narameter Estimation\nInput: A: tag-artist\nmatrix.\nB: artist-artist relation matrix;\nOutput: U: tag-style matrix;\nV: artist-style matrix.\nbegin\n1.Initialization:\nInitialize UandVrandomly,\n2.Iteration:\nrepeat\n2.1 Compute Cij=Aij=[UV>B]ij;\n2.2 Assign UtsÃUtsh\nCB>Vi\nts,\n2.3 Compute Cij=Aij=[BUV>]ij;\n2.4 Assign VbsÃVbsh\nBC>Ui\nbs,\nuntil convergence\n3.Return V\nend\n3.4 Relations\nwith Other Models\nThe TC model uses mixtures of some existing base lan-\nguage models as topic language models. The model is dif-\nferent with some well-known topic models such as Prob-\nabilistic Latent Semantic Indexing (PLSI) [8] or Latent\nDirichlet Allocation (LDA) [2] since they assume the topic\ndistribution of each object is independent of those of oth-\ners. However, this assumption does not always hold in\npractice since in music style analysis, artists (as well as\nsongs) are usually related to each other in certain ways.\nOur TC model incorporates an external information source\nto model such relationships among artists. Also, when the\nbase matrix Bis an identity matrix, this model is iden-\ntical to PLSI (or LDA), and the algorithm is the same\nas the NMF algorithm with Kullback-Leibler (KL) diver-\ngence loss [6, 29].\n4. EXPERIMENTS\n4.1 Data Set\nFor experimental purpose, we use the data set in [30]. The\ndata set consists of 403 artists and one representative song\nper artist. The style and tag descriptions are obtained re-\nspectively from All Music Guide and Last.fm, as described\nbelow.\n59\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)4.1.1 Music\nTag Information\nTags were collected from Last.fm (http://www.last.fm). A\ntotal of 8,529 tags were collected. The number of tags\nfor an artist ranged from 3 to 100. On average an artist\nhad 89.5 tags. Note that, the tag set is a multiset in that\nthe same tag may be assigned to the same artist more than\nonce. For example, Michael Jackson was assigned “80s”\nfor 453 times.\n4.1.2 Audio Content Features\nFor each song we extracted 30 seconds of audio after the\nﬁrst 60 seconds. Then from each of the 30-second audio\nclips, we extracted 12 timbral features using short-term\nFourier transform following the method described in [27].\nThe twelve features are based on Spectral Centroid, Spec-\ntral Rolloff, and Spectral Flux. For each of these three\nspectral dynamics, we calculate the mean and the standard\ndeviation over a sliding window of 40 frames. Then from\nthese means and variances we compute the mean and the\nstandard deviation across the entire 30 seconds, which re-\nsults in 2£2£3 = 12 features. We mention here that we\nactually began our exploration with a much larger feature\nset of size 80, which included STFT, MFCC, and DWCH,\nbut in an attempt to improve results all the features but\nSTFT were consolidated which was consistent with the ob-\nservations in [9].\n4.1.3 Style Information\nStyle information was collected from All Music Guide\n(http://www.allmusic.com). All Music Guide’s data are all\ncreated by musicologists. Style terms are nouns like Rock\n& Roll, Greek Folk, and Chinese Pop as well as adjec-\ntives like Joyous, Energetic, and New Romantic. Styles\nfor each artist/track are different from the music tags de-\nscribed in the above, since each style name appears only\nonce for each artist. We group the styles into ﬁve clus-\nters, and assign each artist to one style cluster. In the ex-\nperiments, the ﬁve groups of styles are: (1) Dance-Pop,\nPop/Rock, Club/Dance, etc., consisting of 100 artists in-\ncluding Michael Jackson ; (2) Urban, Motown, New Jack\nSwing, etc., consisting of 72 artists including Bell Biv De-\nVoe; (3) Free Jazz, Avant-Garden, Modern Creative, etc.,\nconsisting of 51 artists including Air Band ; (4) Hip-Hop,\nElectronica, and etc., consisting 70 artists including Afrika\nBambaataa; (5) Heavy Metal, Hard Rock, etc., consisting\nof 110 artists including Aerosmith .\n4.2 Baselines\nWe compare our proposed method with several state-of-\nthe-art clustering methods including K-means, spectral\nclustering (Ncuts) [31], and NMF [14]. For each cluster-\ning method, we perform it on two data matrices, i.e., the\ntag-artist matrix and the content-artist matrix, respectively.\nWe also perform them on an artist similarity graph which is\nthe linear combination of two similarity graphs generated\nbased on tags and contents respectively using the graph\nconstruction method described in Section 3.2. NMF is not\nsuitable for symmetric similarity matrices, there exists itsclustering methods tags only content only both\nK-meansp p p\nNcutsp p p\nNMFp p\nSNMFp\nPHITS-PLSAp\nTable\n1. The implemented baseline methods.\nK-means Ncuts NMF\nAccuracy 0.2953 0.4119 0.4020\nNMI 0.0570 0.1166 0.1298\nTable\n2. Clustering results using tag information only.\nsymmetric matrix version, SNMF [28]. We use SNMF to\ndeal with the artist similarity matrix. We also use PHITS-\nPLSI, a probabilistic model [4] which is a weighted sum of\nPLSI and PHITS, to integrate tag and audio content infor-\nmation for artist clustering. The summary of the baseline\nmethods is listed in Table 4.2.\n4.3 Evaluation Methods\nTo measure the clustering quality, we use accuracy and\nnormalized mutual information (NMI) as performance\nmeasures.\n²Accuracy measures the relationship between each\ncluster and the ground truth class assignments. It is\nthe total matching degree between all pairs of clus-\nters and classes. The greater accuracy, the better\nclustering performance.\n²NMI [22] measures the amount of statistical infor-\nmation shared by two random variables representing\ncluster assignment and underlying class label.\n4.4 Experimental Results\n4.4.1 Tags-only or Content-only\nTables 2 and 3 respectively show the clustering perfor-\nmance using tag information only and the performance us-\ning content features only. We observe that the tags are\nmore effective than the audio content features for artist\nstyle clustering. Figure 1 better illustrates this observation.\n4.4.2 Combining Tags and Content\nTable 4.4.2 show the performance of different clustering\nmethods using both tag and content information. Since the\nK-means Ncuts NMF\nAccuracy 0.2407 0.2803 0.2878\nNMI 0.0168 0.0317 0.0349\nTable\n3. Clustering results using content features only.\n60\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)K−means Ncuts NMF00.050.10.150.20.250.30.350.40.45Accuracy\n  \ntag\ncontent(a) Accurac\ny\nK−means Ncuts NMF00.020.040.060.080.10.120.14NMI\n  \ntag\ncontent\n(b) NMI\nFigur\ne 1. Clustering performance using tag or content in-\nformation.\nﬁrst three clustering algorithms are originally designed for\nclustering one data matrix, we ﬁrst construct an artist sim-\nilarity graph as follows. (1) We compute the pairwise Eu-\nclidean distances of artists using the tag-artist matrix (nor-\nmalized by tags (rows)) to obtain a symmetric distance ma-\ntrixdt, and another distance matrix dccan be calculated in\nthe similar way using the content- artist matrix. (2) Since\ndtanddcare in the same scale, we can simply combine\nthem linearly to obtain the pairwise artist distance. (3) The\ncorresponding artist similarity graph can be constructed us-\ning the strategies introduced in Section 3.2. Once the artist\nsimilarity graph is generated, the clustering can be con-\nducted using any clustering method. Since both PHITS-\nPLSI and our proposed method are designed to combine\ntwo types of information, we can directly use the tag-artist\nmatrix as the original data matrix, and the similarity graph\nis constructed based on content features. Figure 2 illus-\ntrates the results visually.\nFrom the results, we observe the following:\n²The artist clustering performance is not necessarily\nimproved by incorporating content features. This\nmeans that the tags are more informative than con-\ntents for clustering artist styles.\n²Advanced methods, e.g. PHITS-PLSI and our pro-\nposed method, can naturally integrate different types\nof information and they outperform other tradi-\ntional clustering methods. In addition, our proposedmethod outperforms PHITS-PLSI because PHITS-\nPLSI is more suitable for incorporating explicit link\ninformation while our method is more suitable for\nhandling implicit links (graph).\n²Continuous similarity graph construction such as\nexp-weighted method performs better than discrete\nmethods, e.g. ²NN.\n²Our proposed method with combined tags and con-\ntents using ²NN graph construction outperforms\nall the methods using only tag information. This\ndemonstrates our model is effective for combining\ndifferent sources of information, although the con-\ntent features do not contribute much.\nK−means Ncuts SNMF PHITS−PLSI TC00.050.10.150.20.250.30.350.40.45Accuracy\n(a) Accurac\ny\nK−means Ncuts SNMF PHITS−PLSI TC00.050.10.150.20.25NMI\n(b) NMI\nFigur\ne 2. Clustering performance combining tags and con-\ntents.\n5. CONCLUSION\nIn this paper, we study artistic style clustering based on two\ntypes of data sources, i.e., user-generated tags and audio\ncontent features. A novel language model is also proposed\nto make use of both types of information. Experimental re-\nsults on a real world data set demonstrate that tag informa-\ntion is more effective than music content information for\nartistic style clustering, and our model-based method can\nmarginally improve the clustering performance by combin-\ning tags and contents. However, other simple combination\nmethods fail to enhance the clustering results by incorpo-\nrating content features into tag-based analysis.\n61\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)K-means Ncuts SNMF PHITS-PLSI TC\n²NN Acc 0.2680 0.2804 0.2630 0.3152 0.3648\ngraph NMI 0.0193 0.0312 0.0261 0.0709 0.1587\nexp-weighted Acc 0.2730 0.2903 0.2953 0.3316 0.4417\ngraph NMI 0.0226 0.0321 0.0389 0.1347 0.2008\nTa\nble 4 . Clustering results combining tags and content.\n6. ACKNOWLEDGMENT\nThe work is partially supported by the FIU Dissertation\nYear Fellowship, NSF grants IIS-0546280, CCF-0939179,\nand CCF-0958490, and an NIH Grant 1-RC2-HG005668-\n01.\n7. REFERENCES\n[1]K. Bischoff, C. Firan, W. Nejdl, and R. Paiu: “Can all tags be\nused for search?,” Proceedings of CIKM , 2008.\n[2]D. Blei, A. Ng, and M. Jordan: “Latent Dirichlet allocation,”\nNIPS, 2002.\n[3]S. Chen and S. Chen: “Content-based music genre classiﬁ-\ncation Usinu timbral feature vectors and support vector ma-\nchine,” Proceedings of ICIS , 2009.\n[4]D. Cohn and T. Hofmann: “The missing link - a probabilis-\ntic model of document content and hypertext connectivity,”\nNIPS, 2000.\n[5]H. Deshpande, R. Singh, and U. Nam: “Classiﬁcation of\nmusic signals in the visual domain,” Proceedings of the the\nCOST-G6 Conference on Digital Audio Effects, 2001.\n[6]C. Ding, T. Li, and W. Peng: “On the equivalence between\nNon-negative Matrix Factorization and Probabilistic Latent\nSemantic Indexing,”. Comput. Stat. Data Anal. , 52(8):3913-\n3927.\n[7]C. Ding, T. Li, W. Peng, and H. Park. “Orthogonal nonnega-\ntive matrix tri-factorizations for clustering,” SIGKDD, 2006.\n[8]T. Hofmann: “Probabilistic latent semantic indexing,” SIGIR,\n1999.\n[9]T. Li, M. Ogihara, and Q. Li: “A comparative study on\ncontent-based music genre classiﬁcation,” SIGIR, 2003.\n[10] T. Li and M. Ogihara: “Towards intelligent music informa-\ntion retrieval,” IEEE Transactions on Multimedia, 8(3):564-\n575, 2006.\n[11] I. Karydis, A. Nanopoulos, H. Gabriel, and M. Spiliopoulou:\n“Tag-aware spectral clustering of music items,” ISMIR,\npp. 159–164, 2009.\n[12] P. Knees, T. Pohle, M. Schedl, D. Schnitzer, K. Seyerlehner,\nand G. Widmer: “Augmenting text-based music retrieval\nwith audio similarity,” ISMIR, 2009.\n[13] P. Lamere and E. Pampalk: “Social tags and music informa-\ntion Retrieval,” ISMIR, 2008.\n[14] D. Lee and H. Seung: “Algorithms for non-negative matrix\nfactorization,” NIPS, 2001.\n[15] P. Lehwark, S. Risi, and A. Ultsch: “Data analysis, machine\nlearning and applications,” in Visualization and Clustering of\nTagged Music Data , pp. 673–680. Springer Berlin Heidel-\nberg, 2008.[16] M. Levy and M. Sandler: “Learning latent semantic models\nfor music from social tags” Journal of New Music Research,\n37:137–150, 2008.\n[17] E. Pampalk, A. Flexer, and G. Widmer: “Improvements of\naudio-based music similarity and genre classiﬁcaton,” IS-\nMIR, 2005.\n[18] W. Peng, T. Li, and M. Ogihara: “Music clustering with con-\nstraints,” ISMIR, 2007.\n[19] D. Pye: “Content-based methods for managing electronic\nmusic,” ISCASSP, 2000.\n[20] L. Rabiner and B. Juang: Fundamentals of Speech Recogni-\ntion, Prentice-Hall, NJ, 1993.\n[21] D. Ramage, P. Heymann, C. Manning, and H. Garcia: “Clus-\ntering the tagged web,” ACM International Conference on\nWeb Search and Data Mining, 2009.\n[22] A. Strehl and J. Ghosh: “Clustering ensembles - a knowledge\nreuse framework for combining multiple partitions,” Journal\nof Machine Learning Research , 3:583-617, 2003.\n[23] P. Symeonidis, M. Ruxanda, A. Nanopoulos, and Y .\nManolopoulos: “Ternary semantic analysis of social tags for\npersonalized music Recommendation,” ISMIR, 2008.\n[24] D. Turnbull, L. Barrington, and G. Lanckriet: “Five ap-\nproaches to collecting tags for music,” ISMIR, 2008.\n[25] D. Turnbull, L. Barrington, M. Yazdani, and G. Lanckriet:\n“Combining audio content and social context for semantic\nmusic discovery,” SIGIR, 2009.\n[26] G. Tzanetakis and P. Cook: “Musical Genre Classiﬁcation\nof Audio Signals,” IEEE Transactions on Speech and Audio\nProcessing , 10:5, 2002.\n[27] G. Tzanetakis: “Marsyas submissions to MIREX 2007,”\nMIREX 2007.\n[28] D. Wang, S. Zhu, T. Li, and C. Ding: “Multi-document sum-\nmarization via sentence-level semantic analysis and symmet-\nric matrix factorization,” SIGIR, 2008.\n[29] D. Wang, S. Zhu, T. Li, Y . Chi, and Y . Gong: “Integrat-\ning clustering and multi-document summarization to improve\ndocument understanding,” in CIKM. pp. 1435-1436, 2008.\n[30] F. Wang, X. Wang, B. Shao, T. Li, and M. Ogihara: “Tag\nintegrated multi-label music style classiﬁcation with hyper-\ngraph,” in ISMIR, pp. 363–368, 2008.\n[31] J. Shi and J. Malik: “Normalized Cuts and Image Segmen-\ntation,” IEEE Trans. Pattern Anal. Mach. Intell. , 22(8):888–\n905, 2002.\n[32] Y . Zhang and J. Zhou: “A study on content-based music\nClassiﬁcation,” IEEE Signal Processing and Its Applications ,\n2003.\n[33] X. Zhu: “Semi-supervised learning with graphs,” Doctoral\nThesis, Carnegie Mellon University, 2005.\n62\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "Identifying Repeated Patterns in Music Using Sparse Convolutive Non-negative Matrix Factorization.",
        "author": [
            "Ron J. Weiss",
            "Juan Pablo Bello"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1415934",
        "url": "https://doi.org/10.5281/zenodo.1415934",
        "ee": "https://zenodo.org/records/1415934/files/WeissB10.pdf",
        "abstract": "We describe an unsupervised, data-driven, method for auto- matically identifying repeated patterns in music by analyz- ing a feature matrix using a variant of sparse convolutive non-negative matrix factorization. We utilize sparsity con- straints to automatically identify the number of patterns and their lengths, parameters that would normally need to be fixed in advance. The proposed analysis is applied to beat- synchronous chromagrams in order to concurrently extract repeated harmonic motifs and their locations within a song. Finally, we show how this analysis can be used for long- term structure segmentation, resulting in an algorithm that is competitive with other state-of-the-art segmentation algo- rithms based on hidden Markov models and self similarity matrices.",
        "zenodo_id": 1415934,
        "dblp_key": "conf/ismir/WeissB10",
        "keywords": [
            "unsupervised",
            "data-driven",
            "auto-matically",
            "analyzing",
            "feature matrix",
            "sparse convolutive non-negative matrix factorization",
            "sparsity constraints",
            "beat- synchronous chromagrams",
            "concurrently extract",
            "long- term structure segmentation"
        ],
        "content": "IDENTIFYING REPEATED PATTERNS IN MUSIC USING SPARSE\nCONVOLUTIVE NON-NEGATIVE MATRIX FACTORIZATION\nRon J. Weiss and Juan Pablo Bello\nMusic and Audio Research Lab (MARL), New York University\nfronw,jpbellog@nyu.edu\nABSTRACT\nWe describe an unsupervised, data-driven, method for auto-\nmatically identifying repeated patterns in music by analyz-\ning a feature matrix using a variant of sparse convolutive\nnon-negative matrix factorization. We utilize sparsity con-\nstraints to automatically identify the number of patterns and\ntheir lengths, parameters that would normally need to be\nﬁxed in advance. The proposed analysis is applied to beat-\nsynchronous chromagrams in order to concurrently extract\nrepeated harmonic motifs and their locations within a song.\nFinally, we show how this analysis can be used for long-\nterm structure segmentation, resulting in an algorithm that\nis competitive with other state-of-the-art segmentation algo-\nrithms based on hidden Markov models and self similarity\nmatrices.\n1. INTRODUCTION\nRepetition has been widely-recognized to be a ubiquitous\nfeature of music, closely related to structural units in music,\nsuch as beats, bars, motives and sections [ 10]. This applies\nboth to popular music, often composed of nearly exact rep-\netitions of a small number of sections, e.g. verse, chorus,\nand bridge; and to more sophisticated genres, e.g. jazz or\norchestral music, where recurrences are often masked by\ncomplex transformations, including key modulations and\ntempo variations. The analysis of repeated patterns and their\ntemporal organization is central to the understanding of mu-\nsic. However, while repetitions are apparent in symbolic\nrepresentations of music, their extraction from musical au-\ndio poses a number of challenges stemming from factors\nsuch as the presence of background noise, the inﬂuence of\nmultiple instruments and sonic textures, timing variations\nand other attributes of musical expression, etc.\nThe automatic analysis of repetition in music audio has\nbeen an important focus of attention in MIR, with appli-\ncations including thumbnailing [ 1], retrieval [ 2], and, no-\ntably, long-term segmentation using methods such as self-\nsimilarity matrices and hidden Markov models [ 11,8,5].\nHowever, with a few exceptions [ 7,1], the emphasis has\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2010 International Society for Music Information Retrieval.been on locating repetitions rather than on extracting of\ncharacteristic, repetitive patterns. Previous research on de-\ntecting motif occurrences across a collection [ 9] and cover-\nsong retrieval based on short-snippets [ 3], illustrate the\nutility of extracting such patterns.\nIn this paper we propose a novel approach for the auto-\nmatic extraction and localization of repeated patterns in mu-\nsic audio. The approach is based on sparse shift-invariant\nprobabilistic latent component analysis [ 14] (SI-PLCA),\na probabilistic variant of convolutive non-negative matrix\nfactorization (NMF). The algorithm treats a musical record-\ning as a concatenation of a small subset of short, repeated\npatterns, and is able to simultaneously estimate both the\npatterns and their repetitions throughout the song. The anal-\nysis naturally identiﬁes the long-term harmonic structure\nwithin a song, while the short-term structure is encoded\nwithin the patterns themselves. Furthermore, we show how\nit is possible to utilize sparse prior distributions to learn\nthe number of patterns and their respective lengths, min-\nimizing the number of parameters that must be speciﬁed\nexactly in advance. Finally, we explore the application of\nthis approach to long-term segmentation of musical pieces.\nThe remainder of this paper is organized as follows:\nSection 2 reviews the proposed analysis based on SI-PLCA\nand describes its relationship to NMF. Sections 3 and 4\ndescribe prior distributions over the SI-PLCA parameters\nand the expectation maximization algorithm for parameter\nestimation. Sections 5 and 6 discuss how the proposed\nanalysis can be used for structure segmentation and provide\nexperimental results. Finally, we conclude in Section 7.\n2. PROPOSED APPROACH\n2.1 From NMF to PLCA\nConventional NMF decomposes a non-negative matrix V\ninto the product of two non-negative matrices WandH:\nV\u0019WH (1)\nIn the context of audio analysis, if Vrepresents a time-\nfrequency decomposition of an audio signal, each column\nofWcan be thought of as a frequency template used re-\npeatedly throughout V, and each row of Hcan be thought\nof as the activations of the corresponding basis in time. In\nthis paper we focus on the analysis of beat-synchronous\nchromagrams [ 4], but the method is equally applicable to\nany non-negative time-frequency representation such as a\nmagnitude spectrogram.\n123\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Figure 1 . Demonstration of the SI-PLCA analysis of a chromagram. The decomposition was initialized with L= 40 , and\nK= 10 with\u000bz= 0:98 , and no sparsity on WkorhT\nk. The parameter estimation algorithm pruned out most of the initial\nbases due to the sparse prior on z, converging on only 4bases.\nProbabilistic Latent Component Analysis (PLCA) [ 14]\nrecasts this analysis in a probabilistic framework. PLCA\nrepresents each column of Wand each row of Has multi-\nnomial probability distributions and adds an additional dis-\ntribution over each basis, i.e. a mixing weight. The decom-\nposition can be rewritten in NMF terms as follows:\nV\u0019WZH =K\u00001X\nk=0wkzkhT\nk (2)\nwhereZ= diag(z) is a diagonal matrix of mixing weights\nzandKis the rank of the decomposition (i.e. the number\nof bases inW). Contrary to standard NMF, each of V,wk,\nz, andhT\nkare normalized to sum to 1 since they correspond\nto probability distributions.\nThe probabilistic foundation makes for a convenient\nframework for imposing constraints on the parameters wk,\nhT\nk, andzthrough the use of prior distributions. This will\nbe discussed in detail in Section 3.\n2.2 Adding shift-invariance\nA shift-invariant extension to the PLCA model which allows\nforconvolutive bases is described in [ 14]. Unlike the single\nframe bases wkdescribed in Section 2.1, each SI-PLCA\nbasis is expanded to form a ﬁxed duration template Wk\ncontainingLframes. Therefore, the F\u0002KmatrixW\nbecomes an F\u0002L\u0002KtensorW, and the normalized\nbasiswkbecomes a normalized matrix Wk. The factorsW\nandHare combined via a convolution operation instead of\nmatrix multiplication in a process analogous to equation (2):\nV\u0019X\nkWk\u0003zkhT\nk (3)\nFigure 1 shows an example SI-PLCA decomposition of a\nchromagram using K= 4basis patterns of length L= 40 .\n3. SPARSE PRIOR DISTRIBUTIONS\nA common strategy used throughout the NMF literature is\nto favor sparse settings, i.e. one containing many zeros, for\nWorHin order to learn parsimonious, parts-based decom-\npositions of the data. Sparse solutions can be encouraged\nwhen estimating the parameters in equation (3) by impos-\ning constraints using an appropriate prior distribution. In\n0 50 100 150 200\nIteration0246810121416Effective rank (K)Figure 2 . Typical behavior of the automatic relevance de-\ntermination effect of a sparse prior on z. The initial rank of\nthe decomposition is set to K= 15 , and as the estimation\nalgorithm iterates it is pruned down to a ﬁnal effective rank\n(the number of bases with non-zero zk) of4.\nthe following sections we describe how this process can be\nused to automatically learn the number and length of the\nrepeated patterns within a song.\n3.1 Learning the number of patterns K\nThe Dirichlet distribution is conjugate to the multinomial\ndistributions Wk,z, andhT\nk, making it a natural choice for\na prior. The Dirichlet prior on zhas the following form:\nP\u0000\nzj\u000bz\u0001\n/Y\nkz\u000bz\u00001\nk; \u000bz\u00150 (4)\nwhere the hyperparameter \u000bzis ﬁxed across all Kcompo-\nnents. If\u000bz<1this prior favors solutions where many\ncomponents are zero, i.e. where the distributions are sparse.\nIfzis forced to be sparse, the learning algorithm will\nattempt to use as few bases as possible. This enables an\nautomatic relevance determination strategy in which: (a)\nthe algorithm is initialized to use many bases (large K),\nand (b) the sparse prior on zprunes out bases that do not\ncontribute signiﬁcantly to the reconstruction of V. Only the\nmost relevant patterns “survive” to the end of the parameter\nestimation process, as is shown in the example in Figure 2.\nThis approach is useful because it removes the need to spec-\nify the exact rank of the decomposition Kin advance. The\nparameter estimation simply learns the underlying num-\nber of patterns needed by the data. A similar approach\nto automatically determining the rank of a standard NMF\ndecomposition is described in [15].\n124\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)The Beatles/07-Revolver/08-Good_Day_Sunshine\n*Figure 3 . Demonstration of the SI-PLCA decomposition of\na chromagram using L= 60 and sparsity in all parameters\n(\u000bz= 0:98,c= 16,m=\u000010\u00008, and\u000bh= 1\u000010\u00005).\n3.2 Learning the pattern length L\nThe other parameter that must be speciﬁed in advance is the\nlengthLof the convolutive bases. In fact, different patterns\nwithin the same piece often have different intrinsic lengths,\ne.g. if the chorus uses a shorter riff than the verse or if the\ntime signature changes. Therefore it is useful to automati-\ncally identify the length of each basis independently instead\nof using a ﬁxed length across all bases.\nWe employ a similar strategy to that described in Sec-\ntion 3.1 by setting Lto an upper bound on the expected\npattern length and constructing a prior distribution that en-\ncourages the use of shorter bases. This is accomplished by\nusing a Dirichlet prior on Wkwith a parameter that depends\non the time position \u001cwithin each basis:\nP\u0000\nWkj\u000bw\u0001\n/Y\n\u001cY\nfw\u000bw\u001c\u00001\nkf\u001c(5)\n\u000bwis constructed as a piecewise function which is uninfor-\nmative for small \u001cand then becomes increasingly sparse:\n\u000bw\u001c=(\n1; \u001c <c\n1 +m(\u001c\u0000c); \u001c\u0015c(6)\nThis prior only effects patterns longer than cframes with a\npenalty that increases with the pattern length.\nAn example of the effect of this prior is shown in Fig-\nure 3. Most of the information in the top basis is contained\nwithin the ﬁrst 12 columns, while the other bases have\neffective lengths between 30 and 40.\n3.3 Basis/activation trade-off\nIt is often worthwhile to enforce sparsity on hT\nkusing a\nsimilar approach to equation (4), with a single parameter\n\u000bhtied across all points within hT\nk. The rationale is that\nif most of the activations in hT\nkare zero, then more of the\ninformation in Vwill be captured by Wk, and vice versa.\nA sparse hT\nkpromotes more parsimonious patterns for Wk,\nat the cost of a reduced time resolution.\nThis is illustrated by the example in Figure 1. The second\nbasis pattern is relatively sparse, while the corresponding\nrow ofHcontains many non-zero entries. In fact, thespacing between adjacent activations in hT\n1is smaller than\nthe length of the pattern; i.e. it is continually mixed with\ndelayed versions of itself. The pattern repeats about every\n8 beats, roughly corresponding to the underlying meter.\nIn contrast, the bottom two bases contain signiﬁcantly\nmore information while the corresponding rows of Hcon-\ntain only about 4 peaks. The sparsity setting \u000bh, in combi-\nnation with\u000bw\u001c, control the trade-off between these quali-\ntatively different solutions. A sparse Hleads to more musi-\ncally meaningful bases that are exactly repeated throughout\nthe piece, while a sparse Wleads to temporal patterns in H\nthat are organized according to to the underlying rhythm.\n4. PARAMETER ESTIMATION\nThe decomposition of equation (3) can be computed itera-\ntively using an expectation maximization (EM) algorithm.\nThe full derivation of the algorithm can be found in [ 13].\nHere we extend it to incorporate the prior distributions de-\nscribed in Section 3. Since we are using conjugate prior\ndistributions, this extension is straightforward to derive.\nIn the E-step, the posterior distribution over the hidden\nvariableskand\u001cis computed for each cell in V. For\nnotational convenience we represent this distribution as a\nset of matricesfRk\u001cgfor each setting of kand\u001c. Each\npoint in theF\u0002TmatrixRk\u001ccorresponds to the probability\nthat the corresponding point in Vwas generated by basis k\nat time delay \u001c. It can be computed as follows:\nRk\u001c/wk\u001c\nzk!\u001c\nhT\nk (7)\nwhere\ndenotes the outer product, and!txshiftsxtplaces\nto the right. The set of Rk\u001cmatrices are normalized such\nthat each point inP\nk\u001cRk\u001cis one.\nGiven this posterior distribution, the parameters can be\nupdated in the M-step as follows:\nzk/X\n\u001cX\nftV\u0001Rk\u001c+\u000bz\u00001 (8)\nwk\u001c/X\ntV\u0001Rk\u001c+\u000bw\u001c\u00001 (9)\nhT\nk/X\n\u001cX\nf \u001c\nV\u0001 \u001c\nRk\u001c+\u000bh\u00001 (10)\nwhere\u0001denotes the element-wise matrix product and the\nparameters are normalized so that z,Wk, andhT\nksum to 1.\nThe overall EM algorithm proceeds by initializing Wk,\nz, andhT\nkrandomly, and then iterating equations (7) to (10)\nuntil convergence. This algorithm is only guaranteed to con-\nverge to a local optimum, so the quality of the factorization\nis somewhat dependent on initialization. In our experiments\nwe found that initializing zandhT\nkuniformly while setting\nthe initialWkrandomly leads to more consistent results.\n5. STRUCTURE SEGMENTATION\nAs mentioned in the introduction, the analysis described\nin this paper can be applied to the task of music structure\nsegmentation. It naturally identiﬁes the long-term temporal\n125\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)VThe Beatles/11-Abbey_Road/01-Come_Together\nW0∗z0hT\n0\n W1∗z1hT\n1\n W2∗z2hT\n2\nW3∗z3hT\n3\n3210k\n0 100 200 300 400 500 600 700\nTime (beats)SegmentsEstimated\nGround TruthFigure 4 . Song structure segmentation using the SI-PLCA\ndecomposition shown in Figure 1. The pairwise F-measure\nof the estimated segmentation is 0.52.\nstructure within a song, encoded by H. At the same time,\nthe short-term structure is captured within the bases W.\nWe use the beat-synchronous chroma feature extraction\nfrom [ 4]. Each frame of Vis normalized so that the max-\nimum energy is one. Analysis of these features identiﬁes\nrepeated motifs in the form of chord patterns. We assume a\none-to-one mapping between these chord patterns and the\nunderlying song structure, i.e. we assume that each pattern\nis used within only one segment. The mapping is derived by\ncomputing the contribution of each pattern to the chroma\ngram by summing equation (3) across all pitch classes:\n`k(t) =X\nfWk\u0003zkhT\nk (11)\nThe segmentation labels are then found by smoothing the K\n“pattern usage” functions `k(t)using a rectangular window,\nand ﬁnding the most active pattern at each frame:\n`(t) = argmax\nk`k(t)\u00031S (12)\nwhere 1Sis a lengthSvector of ones. Finally, the per-frame\nsegment labels `(t)are post-processed to remove segments\nshorter than a given minimum segment length.\n5.1 Examples\nAn example of this segmentation procedure is shown in\nFigure 4. The top panel shows the original chromagram of\nthe song. The following four panels show the contribution\nVThe Beatles/07-Revolver/08-Good_Day_Sunshine\nW0∗z0hT\n0\n W1∗z1hT\n1\n W2∗z2hT\n2\nW3∗z3hT\n3\n0 100 200 300 400 500 600 700\nTime (beats)SegmentsEstimated\nGround TruthFigure 5 . Song structure segmentation using the SI-PLCA\ndecomposition shown in Figure 3 (PFM = 0.69).\nof each pattern to the chromagram, and the bottom two\npanels show the smoothed `k(t)and the ﬁnal segmentation.\nThere are some interesting differences between the ground\ntruth segmentation and that derived from the proposed al-\ngorithm in Figure 4. For example, the proposed algorithm\nbreaks the beginning of the song into repeated subsections:\nbasis 2 (mid-gray) !basis 0 (white), while the ground\ntruth labels this sequence as a single segment. When in-\nspecting the actual patterns it is clear that these segments\nare composed of distinct chord patterns, despite serving a\nsingle musical role together (“intro/verse” as annotated in\nthe ground truth). In fact the mid-gray and white segments\nare reused in different contexts throughout the song in re-\ngions with different ground-truth annotations. The analysis\nhas no notion of musical role, so it tends to converge on\nsolutions in which bases are reused as often as possible.\nOne way to address this limitation is to increase the\nlengthLof the convolutive bases (or the corresponding pa-\nrameters of\u000bw\u001c), in which case the repeated sub-segments\nwould be merged into a single long segment. This highlights\nan inherent trade-off in the proposed analysis between iden-\ntifying simple chord patterns that are frequently repeated\n(shortWk, many activations in hT\nk) as opposed to deriv-\ning long-term musical structure (longer Wk, sparser hT\nk).\nThis trade-off is a recognized ambiguity in the concept of\nmusical segmentation [12].\nWhen high-level segments are more closely correlated\nwith the harmonic structure identiﬁed by our method, the\nproposed analysis leads to good segmentation. An exam-\nple of this is shown in Figure 5. Note that the ground\ntruth labels make a distinction between “verse”(white) and\n“verse/break” (black) which is not present in our analysis.\n126\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)0.95 0.96 0.97 0.98 0.99 1.00 1.01\nαz0.300.350.400.450.500.550.60Pairwise F-measure1.3±0.52.7±0.63.2±0.5\n3.9±0.8\n5.4±1.2\n8.9±2.1\n14.5±1.3154\n6\n8\n10\nK=15\nK=4\nK=6\nK=8\nK=10Figure 6 . PFM as a function of \u000bz(solid line). K= 15 ,\nL= 60 , and no other priors are used. The average effective\nrank for each setting of \u000bzis displayed. Also plotted is\nPFM for\u000bz= 1for different settings of K(dashed lines).\n6. EXPERIMENTS\nIn this section we evaluate the proposed approach to struc-\nture segmentation. We quantify the effect of the various\nprior distributions described in Section 3 and compare our\napproach to other state-of-the-art algorithms. The test set\nconsists of 180 songs from the recorded catalog of The\nBeatles, annotated into verse, chorus, refrain, etc. sections\nby the Centre for Digital Music.1Each song contains an\naverage of about 10 segments and 5.6 unique labels.\nSegmentation performance is measured using the pair-\nwise recall rate (PRR), precision rate (PPR), and F-measure\n(PFM) metrics proposed in [ 5] which measure the frame-\nwise agreement between the ground truth and estimated\nsegmentation regardless of the exact segment label. We\nalso report the entropy-based over- and under-segmentation\nscores (SoandSu, respectively) as proposed in [6].\n6.1 Number of patterns\nSince our segmentation algorithm assumes a one-to-one\nrelationship between patterns and segments, the appropriate\nchoice of the number of patterns Kis critical to obtaining\ngood performance. We evaluate this effect by segmenting\nthe data set with varying settings for Kwith\u000bz= 1, and\nby ﬁxingKto 15 and varying \u000bz. No smoothing of the\nresulting labels is performed (S = 1).\nThe results are shown in Figure 6. For \u000bz= 1, segmen-\ntation performance decreases as Kincreases, peaking at\nK= 4. Performance improves when the sparse prior is\napplied for most settings of \u000bz. The average effective rank\nand its standard deviation both increase with decreasing \u000bz\n(increasing sparsity). The best performance is obtained for\n\u000bz= 0:98 , leading to an average effective rank of 3:2\u00060:5.\nThese results demonstrate the advantage of allowing the\nnumber of patterns to adapt to each song.\n6.2 Pattern length\nAs described in Section 5.1, the length of the patterns used\nin the decomposition has a large qualitative effect on the\n1http://isophonics.net/content/reference-annotations-beatles\n20 40 60 80 100 120\nL0.300.350.400.450.50Pairwise F-measureFigure 7 . PFM as a function of the pattern length L. The\nrank is ﬁxed at K= 6and no sparse priors are used.\nsegmentation. To measure this effect, we segmented the\nentire corpus varying Lbetween 10 and 120 beats. No spar-\nsity was enforced, so the pattern length remained ﬁxed for\nall bases and all songs. The results are shown in Figure 7.\nAs predicted, segmentation performance is poor for\nsmallLsince the ground truth segments are often divided\ninto many distinct short segments. Performance improves\nwith increasing L, until it reaches a peak at L= 70 . When\nLgrows larger than the average segment length in the\nground truth (78 beats) the performance decreases.\nEnforcing sparsity on Wkand varying cleads to similar\nresults. However, we have found that allowing for vary-\ning pattern length has negligible effect on segmentation\nperformance, despite often resulting in qualitatively better\npatterns. Following this trend, we have also found that\n\u000bh6= 1 has minimal effect on performance, so it is set\nto 1 in the remaining experiments. These results are not\nsurprising since the segmentation is derived from the com-\nbination ofWandH. Shifting the sparsity from one factor\nto another should not have signiﬁcant impact on `k(t).\n6.3 Comparison to the state-of-the-art\nWe compare the proposed segmentation system with other\nstate-of-the-art approaches, including Levy and Sandler’s\nHMM-based segmentation system2[5] (QMUL) and a\nmore recent system from Mauch et al [ 8] based on analysis\nof self-similarity matrices derived from beat-synchronous\nchroma. As in Section 6.1, we found that QMUL has opti-\nmal PFM when the number of segments is set to 4.\nWe compare these to the proposed system using ﬁxed\nrankK= 4(SI-PLCA) and a variant using sparse zwith\n\u000bz= 0:995 andK= 15 (SI-PLCA-\u000bz).Lwas ﬁxed at 70\nfor both systems, and the minimum segment length Swas\nset to 32. Also included is a baseline random segmentation\nwhere each frame is given one of 4 randomly selected labels.\nThe results are shown in Table 1. The system from\nMauch et al performs best, followed by SI-PLCA- \u000bz, SI-\nPLCA, and QMUL. All systems perform signiﬁcantly better\nthan the baseline. All of the segmentation systems have\nroughly comparable pairwise precision and Su. The differ-\nences are primarily in the recall (and So) with Mauch et al\n2Available: http://vamp-plugins.org/plugin-doc/qm-vamp-plugins.html\n127\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)System PFM PPR PRR SoSu\nMauch et al [8] 0.66 0.61 0.77 0.76 0.64\nSI-PLCA-\u000b Z 0.60 0.58 0.68 0.61 0.56\nSI-PLCA 0.58 0.60 0.59 0.56 0.59\nQMUL [5] 0.54 0.58 0.53 0.50 0.57\nRandom 0.30 0.36 0.26 0.07 0.24\nTable 1 . Segmentation performance on the Beatles data set.\nThe number of labels per song was ﬁxed to 4for SI-PLCA,\nQMUL, and Random. The average effective ranks for SI-\nPLCA-\u000bzand Mauch et al were 3.9 and 5.5, respectively.\noutperforming SI-PLCA- \u000bzby 12% (15%), and SI-PLCA-\n\u000bzin turn outperforming QMUL by 15% (11%).\nAside from our algorithm’s tendency to over-segment,\nthe most obvious qualitative difference between Mauch et\nal’s and the proposed system lies in more accurate boundary\ndetection in the former system. This is partially a result\nof the smoothing performed in equation (12) which tends\nto blur out the segmentation. A more sophisticated set of\nheuristics for deriving segment labels from the SI-PLCA\ndecomposition might not suffer from this problem.\n7. CONCLUSION\nWe have described an algorithm for identifying repeated\npatterns in music using shift-invariant probabilistic com-\nponent analysis and shown how it can be applied to music\nsegmentation. The source code is freely available online.3\nWe demonstrate that the use of simple sparse prior distri-\nbutions on the SI-PLCA parameters can be used to automat-\nically identify the bases that are most relevant for modeling\nthe data and discard those whose contribution is small. We\nalso demonstrate a similar approach to estimating the opti-\nmal length of each basis. The use of these prior distributions\nenables a more ﬂexible analysis and eliminates the need to\nspecify these parameters exactly in advance.\nAlthough this paper has focused on structure segmenta-\ntion, the proposed analysis has many other potential applica-\ntions. For example, basis patterns could be extracted from\na collection of pieces to search for common motifs used\nthroughout a corpus of music, e.g. retrieval of cover songs\nor musical variations. Similarly, Mauch et al demonstrate\nthat chord recognition performance can be improved by\npooling data from repeated sections to smooth over vari-\nations [ 8]. In the context of the proposed analysis this\namounts to simply analyzing the bases Wk.\nOther potential future work includes extracting the hier-\narchical structure within a piece by repeating the SI-PLCA\nanalysis at different time scales. Finally, we mention that\nit is possible to extend the SI-PLCA decomposition to be\nkey-invariant by using the 2D extension to SI-PLCA which\nallow for shifts in pitch class/frequency as well as time [ 14].\nSuch an extension would allow for structure segmentation\nthat is insensitive to key modulations within a piece.\n3http://marl.smusic.nyu.edu/resources/siplca-segmentation/8. ACKNOWLEDGEMENTS\nThe authors would like to thank Matthias Mauch for shar-\ning the implementation of the algorithm from [ 8]. This\nmaterial is based upon work supported by the NSF (grant\nIIS-0844654) and by the IMLS (grant LG-06-08-0073-08).\n9. REFERENCES\n[1]M.A. Bartsch and G.H. Wakeﬁeld. Audio thumbnailing\nof popular music using chroma-based representations.\nIEEE Trans. Multimedia, 7(1):96–104, 2005.\n[2]J.P. Bello. Grouping recorded music by structural simi-\nlarity. In Proc. ISMIR, pages 531–536, 2009.\n[3]M. Casey and M. Slaney. Song Intersection by Approx-\nimate Nearest Neighbor Search. In Proc. ISMIR, 2006.\n[4]D.P.W. Ellis and G.E. Poliner. Identifying ‘cover songs’\nwith chroma features and dynamic programming beat\ntracking. In Proc. ICASSP, pages IV–1429–1432, 2007.\n[5]M. Levy and M. Sandler. Structural Segmentation of\nMusical Audio by Constrained Clustering. IEEE Trans.\nAudio, Speech, and Language Processing, 16(2), 2008.\n[6]H. Lukashevich. Towards quantitative measures of eval-\nuating song segmentation. In Proc. ISMIR, 2008.\n[7]M. Marolt. A mid-level representation for melody-based\nretrieval in audio collections. IEEE Trans. Multimedia,\n10(8):1617–1625, 2008.\n[8]M. Mauch, K. C. Noland, and S. Dixon. Using musical\nstructure to enhance automatic chord transcription. In\nProc. ISMIR, pages 231–236, 2009.\n[9]M. M ¨uller, F. Kurth, and M. Clausen. Audio matching\nvia chroma-based statistical features. In Proc. ISMIR,\npages 288–295, 2005.\n[10] A. Ockelford. Repetition in music: theoretical and\nmetatheoretical perspectives. Volume 13 of Royal Musi-\ncal Association monographs. Ashgate Publishing, 2005.\n[11] J. Paulus and A. Klapuri. Music structure analysis us-\ning a probabilistic ﬁtness measure and a greedy search\nalgorithm. IEEE Trans. Audio, Speech, and Language\nProcessing, 17(6):1159–1170, 2009.\n[12] G. Peeters and E. Deruty. Is Music Structure Annota-\ntion Multi-Dimensional? A Proposal for Robust Local\nMusic Annotation. In Proc. LSAS, 2009.\n[13] P. Smaragdis and B. Raj. Shift-Invariant Probabilistic\nLatent Component Analysis. Technical Report TR2007-\n009, MERL, December 2007.\n[14] P. Smaragdis, B. Raj, and M. Shashanka. Sparse and\nshift-invariant feature extraction from non-negative data.\nInProc. ICASSP, pages 2069–2072, 2008.\n[15] V .Y .F. Tan and C. F ´evotte. Automatic Relevance Deter-\nmination in Nonnegative Matrix Factorization. In Proc.\nSPARS, 2009.\n128\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "Predicting Development of Research in Music Based on Parallels with Natural Language Processing.",
        "author": [
            "Jacek Wolkowicz",
            "Vlado Keselj"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1416808",
        "url": "https://doi.org/10.5281/zenodo.1416808",
        "ee": "https://zenodo.org/records/1416808/files/WolkowiczK10.pdf",
        "abstract": "The hypothesis of the paper is that the domain of Nat- ural Languages Processing (NLP) resembles current re- search in music so one could benefit from this by employ- ing NLP techniques to music. In this paper the similarity between both domains is described. The levels of NLP are listed with pointers to respective tasks within the research of computational music. A brief introduction to history of NLP enables locating music research in this history. Pos- sible directions of research in music, assuming its affinity to NLP, are introduced. Current research in generational and statistical music modeling is compared to similar NLP theories. The paper is concluded with guidelines for music research and information retrieval.",
        "zenodo_id": 1416808,
        "dblp_key": "conf/ismir/WolkowiczK10",
        "keywords": [
            "Natural Languages Processing",
            "research in music",
            "benefit from NLP techniques",
            "described similarity",
            "levels of NLP",
            "computational music",
            "history of NLP",
            "possible directions of research",
            "generational and statistical music modeling",
            "information retrieval"
        ],
        "content": "PREDICTING DEVELOPMENT OF RESEARCH IN MUSIC BASED ON\nPARALLELS WITH NATURAL LANGUAGE PROCESSING\nJacek Wołkowicz\nDalhousie University\nFaculty of Computer Science\njacek@cs.dal.caVlado Ke ˇselj\nDalhousie University\nFaculty of Computer Science\nvlado@cs.dal.ca\nABSTRACT\nThe hypothesis of the paper is that the domain of Nat-\nural Languages Processing (NLP) resembles current re-\nsearch in music so one could beneﬁt from this by employ-\ning NLP techniques to music. In this paper the similarity\nbetween both domains is described. The levels of NLP are\nlisted with pointers to respective tasks within the research\nof computational music. A brief introduction to history of\nNLP enables locating music research in this history. Pos-\nsible directions of research in music, assuming its afﬁnity\nto NLP, are introduced. Current research in generational\nand statistical music modeling is compared to similar NLP\ntheories. The paper is concluded with guidelines for music\nresearch and information retrieval.\n1. INTRODUCTION\nAlong with the information revolution triggered by the in-\ntroduction of computers, new opportunities have emerged\nfor music artists and researchers. When some began exe-\ncuting computational problems on large mainframe com-\nputers others used them to generate early computer music.\nAt this point of time one started to think how computers\nmight be used to process and analyze music matter. Mu-\nsic, similarly to human speech, accompanied human evo-\nlution from its beginning, so deep understanding of mu-\nsic can allow better understanding of better human cog-\nnition. However, music data is in most cases still treated\nas unstructured binary data left on the same shelf with im-\nages, movies, computer programs; opposite to textual data,\nwhich are easy to process, search, index, driven by a large\nnumber of available computer aided techniques provided\nby natural language processing, information retrieval or\ntext data mining like classiﬁcation, analysis, generation,\nsummarization, indexing, searching, translation and much\nmore. However, music can be treated as a natural lan-\nguage and could be processed in the similar way as text.\nAlthough there are substantial differences between written\ntext and music, they have many features in common.\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2010 International Society for Music Information Retrieval.NLP level Music research areas\nphonetics Waveform analysis, audio signals\nphonology Sound events identiﬁcation\nmorphology Score symbols, symbolization\nsyntax N-grams, shallow reduction and parsing\nsemantics Harmonics, phrase level, parsing\npragmatics Phrases, voice leading\ndiscourse Interpretations, context of a piece\nTable 1. NLP Levels with respective music research tasks.\n2. MUSIC AS A NATURAL LANGUAGE\nBy deﬁnition, a natural language is any language which\narises in an unpremeditated fashion as the result of the in-\nnate facility for language possessed by the human intel-\nlect. Not everybody agrees that music ﬁts this deﬁnition,\nbut music researchers, who know the rules of music, are\nusually more prone to agree with it. Music, as well as text,\nhas the symbolic representation that has its origins dated\nback in ancient times. Music and language are the only old\nhuman creative activities where symbolic representation is\ncommonly used. Others, like painting, sculpture, dance did\nnot have such common symbolic notation. Music notation\ncannot be directly ported into computers like text is, but\nthis is only a representation issue that could be easily over-\ncome. For instance, the argument that text can easily be\nsplit into words - the basic features for Natural Language\nProcessing (NLP) and Information Retrieval (IR), which is\nnot the case for music, can be countered if one mentions\nthat there are natural languages that do not use anything to\nseparate words, like Thai.\n3. MUSIC RESEARCH PARALLEL TO NLP\nIn order to treat music as a natural language, one has to\nshow that music processing works on the same classes of\nproblems as NLP does. One distinguishes certain levels of\na text processing, listed in the Table 1. NLP tries to con-\nvey the research through all those levels, from recording (a\nvoice, speech) to understanding (the meaning of a speech).\nThese levels also exist for music. Similarly to a natural\nlanguage, music can be recorded and presented primarily\nas a waveform. On the ‘phonetics’ level one tries to in-\nvestigate the structure of a sound, separate and distinguish\n665\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)between notes or instruments. However, music is much\nmore complex in this area and sound recognition tasks are\nstill facing basic problems.\nThe second very important similarity results from the\nfact both domains use symbolic notations. Music score\nalso consists of characters which are called notes. Simi-\nlarly to NLP’s morphology and syntax — music has hid-\nden, grammar-like structure, hidden rules. Part of it is the\nharmony. It determines how to put words (notes) together,\nhow to build well-formed phrases using them. It also man-\nages the musical meaning of a piece of which the basic ex-\nempliﬁcation is a progression of chords and notes. In the\ncase of notes and their dependencies — we may talk about\nthe syntax of the music while in the case of chords or har-\nmonic progressions — about the semantics of the certain\nphrase or given the phrasing — the pragmatics of the ex-\ncerpt. This is very similar in its form to one of the main\nareas of NLP, which is grammatical analysis. The highest\nlevel of NLP (discourse) is also common in music in a form\nof ideas, desires or aspirations (romantic music) of a com-\nposer as well as pictures and actions behind it (program\nmusic). Dukas’s “The Sorcerer’s Apprentice” or Smetana’s\n“die Moldau” are the very good example of such music.\n4. HISTORY OF NLP AND MUSIC RESEARCH\nThe history of NLP reaches beginnings of the history of\ncomputation since it is believed that the ability of com-\nputers to process natural language as easily as we do will\nsignal creation of real intelligent machines. It comes from\nthe assumption that the use of language is an inherent part\nof human cognitive abilities. Based on that a test, known\nas the Turing test, has been proposed to determine if a ma-\nchine is intelligent. The main objective of this test is that a\ntruly intelligent machine could carry on a discussion with a\nhuman so that the latter could not recognize if he is talking\nto a human or to a machine. This deﬁnition of intelligence\ntriggered the research in NLP with the ultimate goal to cre-\nate such a conversational program. If it is a feasible goal\none has to actually and physically implement the way peo-\nple think, reason and formulate thoughts.\nAt this point we can see a resemblance between human\nspeech and music. Assuming music is a natural language,\nthe Turing test would consist in generating musical pieces\nso that an expert could not recognize if an author is a ma-\nchine or not. Why not a layman? Because it would be\nequivalent to a Turing test where an interlocutor does not\nknow the language of the talk. We would call it a Soft Tur-\ning Test which, unlike for regular human natural languages,\nmakes more sense for music.\n4.1 Introduction of Grammars\nThere has been some pioneering work in both areas, NLP\nand Music Research. Some natural language analysis in\na form of linguistics theories were made before the intro-\nduction of computing machines. As an early, pre-computer\nmusic research one can point work of Heinrich Schenkerwith his Reduction theory in the beginnings of 20thcen-\ntury.\nThese things changed rapidly for NLP just after com-\nputers were invented. Interface to computers is textual and\nthis is a natural format of computer ﬁles. Moreover, there\nwas a very strong need for developing automatic machine\ntranslation. The beginning of this - the Noam Chomsky’s\ntheory of context free grammars for natural texts - dates\nback to 1956. Representing and processing music was not\nthe top priority of that times. As a similar work in the\nﬁeld of music one can point the book ”A Generative The-\nory of Tonal Music” by Lerdahl and Jackendoff published\nin 1983. Both of this approaches deal with the respective\nareas in the same way - by introducing a formal grammar\nthat may generate instances in the given areas.\n4.2 The period of ‘Look ma, no hands’\nThe early NLP researchers were very optimistic. The re-\nsearch was driven by the goal of developing automatic ma-\nchine translation. Various systems were created but, al-\nthough they worked perfectly on several, very limited ex-\namples, they were failing in the real-world applications.\nThe research came to the point where nothing more could\nbe achieved, and yet they haven’t created any robust sys-\ntem that will work on real data. This created a crisis in\nthe whole ﬁeld. It looked that despite their complicated\nsystem, it is not possible to mimic human cognition in the\narea of natural languages.\nIt is likely the time where the music research has just\ncome. It is not that crucial as for natural languages, since\none can still try to trick unexperienced listeners and thus\npass the Soft Turing Test with the system that does not\ndemonstrate the full understanding of the matter it deals\nwith. Another sign that the ﬁeld of music research might\nbe in this kind of situation is the introduction of this kind\nof tracks, where one asks about the future of the ﬁeld(e.g.\nfMIR).\n4.3 Present NLP\nThe old approach - to create a model that will solve all\nour problems in the area of cognition of human speech\n— seemed to be wrong. Current research of NLP lies on\none hand on creating more precise generative models that\nincludes probability or other aspects of other features of\nlanguages (for example using attribute value matrices —\nA VMs). On the other hand, stochastic NLP has gone to-\nwards classical Data Mining (DM) where with the use of\nshallow parsing with mining on textual data gives much\nbetter results than simple DM. NLP techniques are also\nbeing injected to Information Retrieval increasing perfor-\nmance of this systems. Finally, the great improvement has\nbeen done in machine translation, the ﬁrst goal of NLP re-\nsearchers, where Google Translate or Wave’s Rosy are the\nexamples. This is the direction which current music re-\nsearch may follow.\n666\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)5. RELATED WORK\nCurrent research in music concentrates around Music In-\nformation Retrieval, both for the signal and symbolic mu-\nsic representations. In most cases it deals on basic issues\nhow computers should deal with music data in general.\nThe level of music interpretation does not go into seman-\ntics, probably because it is vague what the meaning in mu-\nsic is. However, one should notice that current text Infor-\nmation Retrieval beneﬁts from the semantic layer of text\n(text classiﬁcation, ontologies and relations between terms,\ndependencies between documents, linguistic layer of text).\nWe would like to emphasize the work of Lerdahl and\nJackendoff [5], who ﬁrst describe a generative approach\nthat one can use toward the music. They describe it in a\ncomputational linguistics manner, using preference rules\napproach, mentioning that it could be possible to imple-\nment their rules in a working system. For the implementa-\ntion of their system we had to wait for a long time, since\nthey did several elisions of some tough to deﬁne, impor-\ntant basic notions, understandable by humans, but hard to\nimplement on a machine. A recent try, ATTA [3], deals\nwith all the implementation issues by introducing several\nimportant limitations to the system, which does not go be-\nyond syntactic level, leaving behind harmony issues.\nAnother preference rules generative approach, that in-\ntroduces very important component of modern NLP - prob-\nability, is described by Temperley [7]. Probabilities and\ncorpus based statistics is an inherent part of all modern\nNLP theories hence probabilities can model the meaning\nof text by inferring dependencies within it [6]. This work\nreminds of the idea of probabilistic grammars introduced\nearlier for text and proposed for music by Bod [1].\nStatistical analysis is a very important component of\nNLP models and it has played (Cope [2]) and will play\na major role in music research. In many cases, solutions\nto many problems that gave good results for texts, could\ngive comparable results in music area. As an example,\nthe n-gram method of authorship attribution developed for\nnatural language texts [4] gave good results for composer\nrecognition of musical pieces [8].\n6. FUTURE OF MUSIC RESEARCH\nIf the hypothesis that NLP and current computational mu-\nsic research are in a sense the same but operate in two sim-\nilar but not identical ﬁelds, both ﬁelds could beneﬁt from\nthis legacy. For instance, applications that span large num-\nber of levels of NLP (e.g. try to draw some high level con-\nclusions based on low level music representations) would\nwork better, if they focus on a few levels only. As we have\npointed out the layers of NLP, some of them are not that\nwell covered for the music matter. Lots have been done in\nthe areas of music ‘phonetics’, ‘phonology’ and ‘morphol-\nogy’. We notice some recent work in the area of ‘seman-\ntics’ but there is no models in higher, much more interest-\ning but complicated levels: ‘semantics’ ‘pragmatics’ and\n‘discourse’. Those areas deﬁne the meaning of the data we\ndeal with, the understanding of undergoing structure andthe ﬂow of composers ideas within a piece. In general, one\ncan stack different applications given the structure of NLP\ni.e. the output of a model that operates on syntactic level\ncould be an input of a model operating on semantic level.\nA few tasks that are relevant for music research and are\nwell developed within NLP are sentiment analysis, genre\nclassiﬁcation, automatic summarization or idiom extrac-\ntion. Other approach would be to enhance MIR with some\nsemantic aspects of music matter - music ontologies with\nan application of shallow parsing (or alternatively, local\nreductions) to reach the level of current state-of-the-art of\ntextual Information Retrieval. However, it is still not clear\nhow to represent meaning of music in computational tasks\nbut in this case statistical approach and data mining tech-\nniques may be relevant tool to describe this phenomenon.\n7. CONCLUSIONS\nThe domain of music research resembles research in NLP.\nBoth ﬁelds operate on the similar types of data that share\ncommon features. Both domains deal with data that are\neasily perceivable by humans but pose a lot of problems to\nmake them fully understandable by computers. Research\nin both areas uses similar techniques and should be able\nto take from each other in the areas that are more devel-\noped in one of them. Music researchers could share their\ninsight in tasks like voices separation or boundaries de-\ntection while beneﬁt from NLP’s statistical methods, au-\ntomatic approaches to semantics or aiding information re-\ntrieval and data mining with natural language understand-\ning. It is not necessary that all those inherited techniques\nand approaches will work but deﬁnitely, it is worth trying.\n8. REFERENCES\n[1] Rens Bod. A uniﬁed model of structural organization\nin language and music. JAIR, 17(1):289–308, 2002.\n[2] David Cope. Computer modeling of musical intelli-\ngence in emi. Comp. Music Journ., 16(2):69–83, 1992.\n[3] Masatoshi Hamanaka, Keiji Hirata, and Satoshi Tojo.\nImplementing a generative theory of tonal music. Jour-\nnal of New Music Research, 35:249–277, 2006.\n[4] Vlado Keselj, Fuchun Peng, Nick Cercone, and Calvin\nThomas. N-gram-based author proﬁles for authorship\nattribution. In Proc. of the PACLING03 Conf., 2003.\n[5] Fred Lerdahl and Ray Jackendoff. A Generative Theory\nof Tonal Music. The MIT Press, 1983.\n[6] Christopher D. Manning and Hinrich Sch ¨utze. Founda-\ntions of Statistical Natural Language Processing. The\nMIT Press, June 1999.\n[7] David Temperley. Music and Probability. The MIT\nPress, 2007.\n[8] Jacek Wołkowicz, Zbigniew Kulka, and Vlado Ke-\nselj. N-gram-based approach to composer recognition.\nArchives of Acoustics, 33(2008)(1):43–55, Jan 2008.\n667\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "Infinite Latent Harmonic Allocation: A Nonparametric Bayesian Approach to Multipitch Analysis.",
        "author": [
            "Kazuyoshi Yoshii",
            "Masataka Goto"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1414912",
        "url": "https://doi.org/10.5281/zenodo.1414912",
        "ee": "https://zenodo.org/records/1414912/files/YoshiiG10.pdf",
        "abstract": "This paper presents a statistical method called Infinite La- tent Harmonic Allocation (iLHA) for detecting multiple fundamental frequencies in polyphonicaudio signals. Con- ventional methods face a crucial problem known as model selection because they assume that the observed spectra are superpositions of a certain fixed number of bases (sound sources and/or finer parts). iLHA avoids this problem by assuming that the observed spectra are superpositions of a stochastically-distributed unbounded (theoretically infi- nite) number of bases. Such uncertainty can be treated in a principled way by leveraging the state-of-the-art paradigm of machine-learning called Bayesian nonparametrics. To represent a set of time-sliced spectral strips, we formulated nested infinite Gaussian mixture models (GMMs) based on hierarchical and generalized Dirichlet processes. Each strip is allowed to contain an unbounded number of sound sources (GMMs), each of which is allowed to contain an unbounded number of harmonic partials (Gaussians). To train the nested infinite GMMs efficiently, we used a mod- ern inference technique called collapsed variational Bayes (CVB). Our experiments using audio recordings of real pi- ano and guitar performances showed that fully automated iLHA based on noninformative priors performed as well as optimally tuned conventional methods.",
        "zenodo_id": 1414912,
        "dblp_key": "conf/ismir/YoshiiG10",
        "keywords": [
            "Infinite Latent Harmonic Allocation (iLHA)",
            "detecting multiple fundamental frequencies",
            "polyphonic audio signals",
            "model selection problem",
            "conventional methods",
            "stochastic distribution",
            "Bayesian nonparametrics",
            "nested infinite Gaussian mixture models",
            "collapsed variational Bayes",
            "audio recordings"
        ],
        "content": "INFINITE LATENT HARMONIC ALLOCATION: A NONPARAMETRIC\nBAYESIAN APPROACH TO MULTIPITCH ANALYSIS\nKazuyoshi Yoshii Masataka Goto\nNational Institute of Advanced Industrial Science and Technology (AIST), Japan\n{k.yoshii, m.goto }@aist.go.jp\nABSTRACT\nThis paper presents a statistical method called Inﬁnite La-\ntent Harmonic Allocation (iLHA) for detecting multiple\nfundamental frequencies in polyphonic audio signals. Con-\nventional methods face a crucial problem known as model\nselection because they assume th at the observed spectra are\nsuperpositions of a certain ﬁxed number of bases (sound\nsources and/or ﬁner parts). iLHA avoids this problem byassuming that the observed spectra are superpositions ofastochastically-distributed unbounded (theoretically inﬁ-\nnite) number of bases. Such uncertainty can be treated in a\nprincipled way by leveraging the state-of-the-art paradigm\nof machine-learning called Bayesian nonparametrics. Torepresent a set of time-sliced spectral strips, we formulated\nnested inﬁnite Gaussian mixture models (GMMs) based\non hierarchical and generali zed Dirichlet processes. Each\nstrip is allowed to contain an unbounded number of soundsources (GMMs), each of which is allowed to contain an\nunbounded number of harmonic partials (Gaussians). To\ntrain the nested inﬁnite GMMs efﬁciently, we used a mod-ern inference technique called collapsed variational Bayes\n(CVB). Our experiments using audio recordings of real pi-\nano and guitar performances sh owed that fully automated\niLHA based on noninformative priors performed as well asoptimally tuned conventional methods.\n1. INTRODUCTION\nMultipitch analysis of polyphonic audio signals [1–11] isone of the most important issues because it is the basisof many applications such as music transcription, chordrecognition, and musical instrument recognition. We focus\non principled methods based on machine learning, which\nhave recently yielded promising results. Some researchers,for example, have proposed generative probabilistic mod-\nels that explain how multiple spectral/signal bases (compo-\nsitional units) are mixed to form polyphonic music [3–6].The model parameters can be trained by means of statisti-\ncal inference. Others have used nonnegative matrix factor-\nization (NMF) to decompose polyphonic spectra into indi-vidual spectral bases [7–11]. NMF can be interpreted fromthe viewpoint of statistical inference [10–12].\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted w ithout fee provided that copies are\nnot made or distributed for proﬁt or c ommercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2010 International Society for Music Information Retrieval.Model #2Unified\nModel\nSelect an optimal modelCarefully tune prior distributionsAssume the number of spectral bases \nOutput: \nFundamental frequencies (F0s)Input: Polyphonic \naudio signalsConventional methods\n…Model #1Assume the number of harmonic partialsOur method (iLHA)\nNo prior tuningRequire\nNo assumptions\nFigure 1. Methodological advantage of our method.\nA crucial problem in these methods, known as model\nselection, is that they perform best only if an appropriate\nmodel complexity (the number of bases) is speciﬁed in ad-\nvance . One might think that the optimal number of bases\nmust be equal to the number of sound sources, but it is notclear how many bases are most suited to represent a singlesource if the spectral shape varies through time. Although\nuncertainty is inherent in model selection, conventional\nmethods assume that a certain complexity exists uniquely\nas an oracle. As shown in Figure 1, they require possiblemodels to be examined separately and exhaustively and the\noptimal model selected in retrospect. Such a determinis-\ntic framework is not easy-to-use in practice although opti-mally tuned methods can achieve good performance.\nTo avoid model selection, we propose a novel statistical\nmethod called Inﬁnite Latent Harmonic Allocation (iLHA)\nbased on a modern paradigm of machine learning calledBayesian nonparametrics. Note that the term “nonpara-\nmetric” means that we do not have to ﬁx model complexity\nuniquely. We assume that an unbounded but ﬁnite numberof bases stochastically appears in a limited amount of avail-able data although an inﬁnite number of bases theoretically\nexists in the universe. Uncertainty in model selection can\nbe treated reasonably in a probabilistic framework.\niLHA can be derived by taking the inﬁnite limit of con-\nventional ﬁnite models [3, 4]. Conventionally, each spec-\ntral basis is often parameterized by means of a Gaussian\nmixture model (GMM) in which a ﬁxed number of Gaus-sians corresponds to the spectral peaks of harmonic par-\ntials, and a time-sliced polyphonic spectral strip is modeled\nby mixing a ﬁxed number of GMMs. Here, we considerboth the number of bases and the number of partials to ap-proach inﬁnity, where most are regarded as unnecessary\nand automatically removed through statistical inference.\nA fundamental and practically-important advantage of\niLHA is that precise prior knowledge is not required. Con-\nventional methods [3–5] heavily rely on prior distributions\nregarding the relative strengths of harmonic partials, which\n309\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)have too much impact on performance, and forced us to\ntune priors and their weighting factors by hand according\nto the properties of target sound sources. iLHA, in contrast,\ncan be fully automated by layering noninformative hyper-priors on inﬂuential priors in a hierarchical Bayesian man-ner. This is consistent with the fact that humans can adap-\ntively distinguish individual notes of various instruments.\nOne of major contributions of this study is to embody thefundamental Bayesian principle “Let the data speak for it-\nself” in the context of multipitch analysis.\nThe rest of this paper is organized as follows: Section 2\ndescribes statistical interpretation of polyphonic spectra.\nSection 3 discusses related work. Sections 4 and 5 explain\nﬁnite models (LHA) and inﬁnite models (iLHA). Section 6\nreports our experiments. Section 7 concludes this paper.\n2. STATISTICAL INTERPRETATION\nWe interpret polyphonic spectra as histograms of observedfrequencies that independently occur. This interpretation\nbasically follows conventional studies [3–5].\n2.1 Assumptions\nSuppose given polyphonic audio signals are generated from\nKbases, each of which consists of Mharmonic partials\nlocated on a linear frequency scale at integral multiples of\nthe fundamental frequency (F0). Note that each basis can\nbe associated with multiple sounds of different temporal\npositions if these sounds are derived from the same pitch\nof the same instrument. We transform the audio signals\ninto wavelet spectra. Let Dbe the number of frames. If a\nspectral strip at frame d(1≤d≤D)has amplitude aat\nfrequency f, we assume that frequency fwas observed a\ntimes in frame d. Assuming that amplitudes are additive,\nwe can consider each observed frequency to be generated\nfrom one of Mpartials in one of Kbases.\nThese notations are for the ﬁnite case. In Bayesian non-\nparametrics, we take the limit as KandMgo to inﬁnity.\n2.2 Observed and Latent Variables\nLet the total observed variables over all Dframes be rep-\nresented by X={X1,···,XD},w h e r e Xdis a set of\nobserved frequencies Xd={xd1,···,xdNd}in frame d.\nNdis the number of frequency observations. That is, Ndis\nequal to the sum of spectral amplitudes over all frequency\nbins in frame d.xdn(1≤n≤Nd)is an one-dimensional\nvector that represents an observed frequency.\nLet the total latent variables corresponding to Xbe sim-\nilarly represented by Z={Z1,···,ZD},w h e r e Zd=\n{zd1,···,zdNd}.zdnis aKM -dimensional vector in which\nonly one entry, zdnkm , takes a value of 1 and the others take\nvalues of 0 when frequency xdnis generated from partial\nm(1≤m≤M)of basisk(1≤k≤K).\n3. COMPARISON WITH RELATED WORK\nThe properties of iLHA are intermediate between those of\ntwo successful approaches–statistical inference and NMF–which are discussed here for comparison and to clarify the\npositioning of our approach.Frequency \n(Cent)Density\nkµ2ok+µm ko+μLL1−Λk 1kτ\n2kτ kmτ\nFigure 2. Probabilistic model of a single basis.\nFrequency \n(Cent)Density\nFrequency \n(Cent)DensityAssumption: Amplitude additivity\nin the frequency domain is satisfied\nwhen multiple sounds are mixedMix two GMMs\nThis density distribution corresponds to \nobserved frequency histogram (spectral strip)\nFigure 3. Probabilistic model of mixed multiple bases.\n3.1 Statistical Inference\nStatistical methods [3–6] assume probabilistic models us-\ning a limited number of paramet ers to represent the gen-\nerative process of observed spectra (audio signals). F0 es-\ntimation directly corresponds to ﬁnding model parametersthat provide the best explanations of the given data.\nGoto [3] ﬁrst proposed probabilistic models of harmonic\nsounds by regarding frequency spectra as probabilistic den-\nsities (histograms of observed frequencies).\nAs shown in Figure 2, the spectral distribution of basis\nk(1≤k≤K)is modeled by a harmonic GMM:\nMk(x)=M/summationdisplay\nm=1τkmN/parenleftbig\nx/vextendsingle/vextendsingleμk+om,Λ−1\nk/parenrightbig\n, (1)\nwherexis a one-dimensional vector that indicates an ob-\nserved frequency [cents].1The Gaussian parameters, mean\nμkand precision Λk, indicate F0 [cents] of basis kand a\ndegree of energy concentration around the F0 in the fre-\nquency domain. τkmis a relative strength of the m-th har-\nmonic partial ( 1≤m≤M)i nb a s i s k.W e s e t omto\n[1200log2m]. This means MGaussians are located to have\nharmonic relationships on the logarithmic frequency scale.\nAs shown in Figure 3, the spectral strip of frame dis\nmodeled by mixing Kharmonic GMMs as follows:\nMd(x)=K/summationdisplay\nk=1πdkMk(x) (2)\nwhereπdkis a relative strength of basis kin frame d.T h e r e -\nfore, the polyphonic spectral strip is represented by nestedﬁnite Gaussian mixture models.\nSeveral inference methods that have been proposed for\nparameter estimation are listed in Table 1. Goto [3] pro-\n1Linear frequency fhin hertz can be converted to logarithmic fre-\nquency fcin cents as fc= 1200 log2(fh/(4403\n12−5)).\n310\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)#(bases) #(partials) Temporal modeling\nPreFEst [3]\n Fixed Fixed None\nHC [4]\n Inferred Fixed None\nHTC [5]\n Fixed Fixed Continuity treated\nNMF [7]\n Fixed Not used Exchangeable\niLHA\n Inﬁnite Inﬁnite Exchangeable\nTable 1 . Comparison of multipitch analysis methods.\nposed a method called PreFEst that estimates only relative\nstrengths τandπwhileμandΛare ﬁxed by allocating\nmany GMMs to cover the entire frequency range as F0 can-didates. Kameoka et al. [4] then proposed harmonic clus-\ntering (HC), which estimates al l the parameters and selects\nthe optimal number of bases by using the Akaike informa-tion criterion (AIC). Although these methods yielded the\npromising results, they analyze the spectral strips of differ-\nent frames independently. Thus, Kameoka et al. [5] pro-\nposed harmonic-temporal-structured clustering (HTC) thatcaptures temporal continuity of spectral bases. Note that\nall these methods are based on maximum-likelihood and\nmaximum-a-posteriori training of the parameters by intro-ducing prior distributions of relative strengths\nτ,w h i c h\nhave a strong impact on the accuracy of F0 estimation.\nOur method called iLHA is based on hierarchical non-\nparametric Bayesian modeling t hat requires no prior tuning\nand avoids specifying KandMin advance. More specif-\nically, the limit of the conventional nested ﬁnite GMMs isconsidered as\nKandMdiverge to inﬁnity.\n3.2 Nonnegative Matrix Factorization\nNMF-based methods [7–12] factorize observed frequency\nspectra into the product of spectral bases and time-varying\nenvelopes under the nonnegativity constraint. Kbases are\nestimated by sweeping all frames of the given spectra. Al-\nthough several methods [10, 12] take temporal continuityinto account, standard methods are based on temporal ex-\nchangeability. In other words, exchange of arbitrary frames\ndoes not affect the factorized results. Although such tem-poral modeling is not sufﬁcient, it is known to work well\nin practice. Therefore, iLHA adopted the exchangeability.\n4. LATENT HARMONIC ALLOCATION\nThis section explains LHA, the ﬁnite version of iLHA, as a\npreliminary step to deriving iLHA. We formulate the con-\nventional nested ﬁnite GMMs in a Bayesian manner.\n4.1 Model Formulation\nFigure 4 illustrates a graphical representation of the LHA\nmodel. The full joint distribution is given by\np(X,Z,π,τ,μ,Λ)\n=p(X|Z,μ,Λ)p(Z|π,τ)p(π)p(τ)p(μ,Λ) (3)\nwhere the ﬁrst two terms on the right-hand side are likeli-hood functions and the other three terms are prior distribu-\ntions. The likelihood functions are deﬁned as\np(X|Z,μ,Λ)=/productdisplay\ndnkmN/parenleftbig\nxdn/vextendsingle/vextendsingleμk+om,Λ−1\nk/parenrightbigzdnkm(4)\np(Z|π,τ)=/productdisplay\ndnkm(πdkτkm)zdnkm (5)dnzdnx\n/glyph817D\nkmτkµ\n0 0,b mkΛ\n0 0,c WβKdπ α\nν\nυM\nFigure 4. A graphical representation of LHA.\nThen, we introduce conjugate priors as follows:\np(π)=D/productdisplay\nd=1Dir(πd|αν)∝D/productdisplay\nd=1K/productdisplay\nk=1πανk−1\ndk (6)\np(τ)=K/productdisplay\nk=1Dir(τk|βυ)∝K/productdisplay\nk=1M/productdisplay\nm=1τβυm−1\nkm (7)\np(μ,Λ)=K/productdisplay\nk=1N/parenleftbig\nμk/vextendsingle/vextendsinglem\n0,(b0Λk)−1/parenrightbig\nW/parenleftbig\nΛk/vextendsingle/vextendsingleW\n0,c0/parenrightbig(8)\nwherep(π)andp(τ)are products of Dirichlet distributions\nandp(μ,Λ)is a product of Gaussian-Wishart distributions.\nανandβυare hyperparameters and αandβare called\nconcentration parameters when νandυsum to unity. m0,\nb0,W0,a n dc0are also hyperparameters; W0is a scale\nmatrix and c0is a degree of freedom.\n4.2 Variational Bayesian Inference\nThe objective of Bayesian inference is to compute a true\nposterior distribution of all variables: p(Z,π,τ,μ,Λ|X).\nBecause analytical cal culation of the posterior distribution\nis intractable, we instead approximate it by using iterativeinference techniques such as variational Bayes (VB) andMarkov chain Monte Carlo (MCMC). Although MCMC\nis considered to be more accurate in general, we use VB\nbecause it converges much faster.\nIn the VB framework, we introduce a variational pos-\nterior distribution\nq(Z,π,τ,μ,Λ)and make it close to the\ntrue posterior p(Z,π,τ,μ,Λ|X)iteratively. Here, we as-\nsume that the variational distribution can be factorized as\nq(Z,π,τ,μ,Λ)=q(Z)q(π,τ,μ,Λ) (9)\nTo optimize q(Z,π,τ,μ,Λ),w eu s eav a r i a t i o n a lv e r -\nsion of the Expectation-Maximization (EM) algorithm [13].We iterate VB-E and VB-M steps until a variational lowerbound of evidence\np(X)converges as follows:\nq∗(Z)∝exp/parenleftbig\nEπ,τ,μ,Λ[logp(X,Z,π,τ,μ,Λ)]/parenrightbig(10)\nq∗(π,τ,μ,Λ)∝exp(EZ[logp(X,Z,π,τ,μ,Λ)])(11)\n4.3 Updating Formula\nWe derive the formulas for updating variational posterior\ndistributions according to Eqns. (10) and (11).\n4.3.1 VB-E Step\nAn optimal variational posterior distribution of latent vari-\nablesZcan be computed as follows:\nlogq∗(Z)=Eπ,τ,μ,Λ[logp(X,Z,π,τ,μ,Λ)]+const.\n=Eμ,Λ[logp(X|Z,μ,Λ)]+Eπ,τ[logp(Z|π,τ)]+const.\n=/summationdisplay\ndnkmzdnkmlogρdnkm+const. (12)\n311\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)1~ν1~1ν−\n2~ν2~1ν−\n3~ν3~1ν−1ν2ν3νL\n:::\nRecursively break \nthe stick of length 1\nFigure 5. Stick-breaking construction of Dirichlet process.\nwhereρdnkm is deﬁned as\nlogρdnkm=Eπd[logπdk]+Eτk[logτkm]\n+Eμk,Λk/bracketleftbig\nlogN/parenleftbig\nxdn/vextendsingle/vextendsingleμk+om,Λ−1\nk/parenrightbig/bracketrightbig(13)\nq∗(Z)is obtained as multinomial distributions given by\nq∗(Z)=/productdisplay\ndnkmγzdnkm\ndnkm (14)\nwhereγdnkm is given by γdnkm=ρdnkm\n/summationtext\nkmρdnkmand is called\na responsibility that indicates how likely it is that observed\nfrequency xdnis generated from harmonic partial mof ba-\nsisk. Here, let ndkmbe an observation count that indicates\nhow many frequencies were generated from harmonic par-\ntialmof basis kin frame d.ndkm and its expected value\ncan be calculated as follows:\nndkm=/summationdisplay\nnzdnkm E[ndkm]=/summationdisplay\nnγdnkm (15)\nFor convenience in executing the VB-M step, we com-\npute several sufﬁcient statistics as follows:\nSk[1]≡/summationdisplay\ndnmγdnkm Sk[x]≡/summationdisplay\ndnmγdnkmxdnm (16)\nSk[xxT]≡/summationdisplay\ndnmγdnkmxdnmxT\ndnm (17)\nwherexdnm is deﬁned as xdnm=xdn−om.\n4.3.2 VB-M Step\nConsequently, an optimal variational posterior distribution\nof parameters π,τ,μ,Λi ss h o w nt ob eg i v e nb y\nq∗(π,τ,μ,Λ)=D/productdisplay\nd=1q∗(πd)K/productdisplay\nk=1q∗(τk)K/productdisplay\nk=1q∗(μk,Λk)(18)\nSince we use conjugate priors , each posterior has the same\nform of the corresponding prior as follows:\nq∗(πd)=Dir(πd|αd) (19)\nq∗(τk)=Dir(τk|βk) (20)\nq∗(μk,Λk)=N/parenleftbig\nμk/vextendsingle/vextendsinglemk,(bkΛk)−1/parenrightbig\nW/parenleftbig\nΛk/vextendsingle/vextendsingleW\nk,ck/parenrightbig(21)\nwhere the variational parameters are given by\nαdk=ανk+E[ndk·]βkm=βυm+E[n·km] (22)\nbk=b0+Sk[1] ck=c0+Sk[1] (23)\nmk=b0m0+Sk[x]\nb0+Sk[1]=b0m0+Sk[x]\nbk(24)\nW−1\nk=W−1\n0+b0m0mT\n0+Sk[xxT]−bkmkmTk(25)\nHere dot ( ·) denotes the sum over that index.\n5. INFINITE LATENT HARMONIC ALLOCATION\nThis section derives hierarchical nonparametric Bayesian\nmodels, i.e., nested inﬁnite GMMs for polyphonic spectra.5.1 Model Formulation\nFirst we let Kapproach inﬁnity, where the inﬁnite number\nof harmonic GMMs is assumed to exist in the universe.\nMore speciﬁcally, the dimensionality of the Dirichlet dis-\ntributions in Eqn. (6) is cons idered to be inﬁnite. At each\nframed,πdis an inﬁnite vector of normalized probabili-\nties (mixing weights) drawn from the inﬁnite-dimensionalDirichlet prior. Such stochastic process is called a Dirichlet\nprocess (DP). Every time frequency\nxdnis generated, one\nof the inﬁnite number of harmonic GMMs is drawn accord-\ning toπd. Note that most entries of πdtake extremely tiny\nvalues because all entries sum to unity. If we can observethe inﬁnite number of frequencies (\nNd→∞ ), the inﬁnite\nnumber of harmonic GMMs can be drawn. However, Nd\nis ﬁnite in practice. Therefore, only the ﬁnite number ofharmonic GMMs,\nK+/lessmuch∞ , is drawn at frame d. Here,\na problem is that harmonic GMMs that are actually drawnat frame\ndare completely disjointed from those drawn at\nanother frame d/prime. This is not a reasonable situation.\nTo solve this problem, we use the hierarchical Dirichlet\nProcess (HDP) [14]. More speciﬁcally, we assume that\ninﬁnite-dimensional hyperparameter νin Eqn. (6), which\nis shared among all Dframes, is a draw from a top-level\nDP. A generative interpretation is that after an unbounded\nnumber of harmonic GMMs is initially drawn from the top-\nlevel DP, an unbounded subset is further drawn according\nto the local DP at each frame. This effectively ties frame d\nto another frame d/prime. As shown in Figure 5, νis known to\nfollow the stick-breaking construction [14] as follows:\nνk=˜νkk−1/productdisplay\nk/prime=1(1−˜νk/prime)˜νk∼Beta(1,γ) (26)\nwhereγis a concentration param eter of the top-level DP.\nTherefore νcan be converted into ˜ν.\nNow we let Mapproach inﬁnity, where each harmonic\nGMM consists of the inﬁnite number of harmonic partials.To put effective priors on\nτ, we use generalized DPs called\nBeta two-parameter processes as follows:\nτkm=˜τkmm−1/productdisplay\nm/prime=1(1−˜τkm/prime)˜τkm∼Beta(βλ1,βλ 2)(27)\nwhereβis a positive scalar and λ1+λ2=1.\nBecause α,β,γ andλare inﬂuential hyperparameters,\nwe put Gamma and Beta hyperpriors on them as follows:\np(α)=Gam(α|aα,bα)p(γ)=Gam(γ|aγ,bγ) (28)\np(β)=Gam(β|aβ,bβ)p(λ)=Beta(λ|u1,u2)(29)\nwherea{α,β,γ}andb{α,β,γ}are shape and rate parameters.\nFigure 6 shows a graphical representation of the iLHA\nmodel. The full joint distribution is given by\np(X,Z,π,˜τ,μ,Λ,α,β,γ,λ ,˜ν)=p(X|Z,μ,Λ)p(μ,Λ)\np(Z|π,˜τ)p(π|α,˜ν)p(˜τ|β,λ)p(α)p(β)p(γ)p(λ)p(˜ν|γ)(30)\nwherep(Z|π,˜τ)is obtained by plugging Eqn. (27) into\nEqn. (5) and p(π|α,ν)is the same as Eqn. (6). p(˜ν|γ)and\np(˜τ|β,λ)are deﬁned according to Eqns. (26) and (27) as\np(˜ν|γ)=/productdisplay\nkBeta(˜τk|1,γ)p(˜τ|β,λ)=/productdisplay\nkmBeta(˜τkm|βλ)(31)\n312\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)dπ\n/glyph817D\nkmτ~\n0 0,b m0 0,c WK\nββ βb a,αα αb a,\nγkν~\nM\n∞ →M∞ →KkµkΛγ γb a,dnzdnx\nλ u\nFigure 6. A graphical representation of iLHA.\n5.2 Collapsed Variatio nal Bayesian Inference\nTo train the HDP model we use a sophisticated version of\nVB called collapsed variational Bayes (CVB) [15]. CVB\nenables more accurate posterior approximation in the space\nof latent variables where parameters are integrated out.\nFigure 7 shows a collapsed iLHA model. By integrating\noutπ,˜τ,μ,Λ, we obtain the marginal distribution given by\np(X,Z,α,β,γ,λ ,˜ν)\n=p(X|Z)p(Z|α,β,λ,˜ν)p(α)p(β)p(γ)p(λ)p(˜ν|γ)(32)\nwhere the ﬁrst two terms are calculated as follows:\np(X|Z)=( 2π)−n···\n2/productdisplay\nk/parenleftbiggb0\nbzk/parenrightbigg1\n2B(W0,c0)\nB(Wzk,czk)(33)\np(Z|α,β,λ,˜ν)=/productdisplay\ndΓ(α)\nΓ(α+nd··)/productdisplay\nkΓ(ανk+ndk·)\nΓ(ανk)\n/productdisplay\nkmΓ(β)Γ(βλ 1+n·km)Γ(βλ 2+n·k>m)\nΓ(βλ1)Γ(βλ 2)Γ(β+n·k≥m)(34)\nwherebzk,Wzk,czkare obtained by substituting zdnkm for\nγdnkm in calculating Eqns. (23) and (25).\nBecause CVB cannot be applied directly to Eqn. (32),\nwe introduce auxiliary variables by using a technique called\ndata augmentation [15]. Let ηdandξkmbe Beta-distributed\nvariables and sdkandtkmbe positive integers that satisfy\n1≤sdk≤ndk·,1≤tkm1≤n·km,a n d1≤tkm2≤n·k>m.\nEqn. (34) can be augmented as\np(Z,η,ξ,s,t|α,β,λ,˜ν)=/productdisplay\ndηα−1\nd(1−ηd)nd··−1\nΓ(nd··)/productdisplay\nk/bracketleftbigg\nndk·\nsdk/bracketrightbigg\n(ανk)sdk\n/productdisplay\nkmξβ−1\nkm(1−ξkm)n·k≥m−1\nΓ(n·k≥m)/bracketleftbigg\nn·km\ntkm1/bracketrightbigg\n(βλ1)tkm1/bracketleftbigg\nn·k>m\ntkm2/bracketrightbigg\n(βλ2)tkm2\nwhere[]denotes a Stirling number of the ﬁrst kind. The\naugmented marginal distribution is given by\np(X,Z,η,ξ,s,t,α,β,γ,λ ,˜ν)\n=p(Z,η,ξ,s,t|α,β,λ,˜ν)p(α)p(β)p(γ)p(λ)p(˜ν|γ)(35)\nIn the CVB framework, we assume that the variational\nposterior distribution can be factorized as follows:\nq(Z,η,ξ,s,t,α,β,γ,λ ,˜ν)\n=q(α,β,γ,λ)q(˜ν)q(η,ξ,s,t|Z)/productdisplay\ndnq(zdn)(36)\nWe also use an approximation technique called varia-\ntional posterior truncation. More speciﬁcally, we assume\nq(zdnkm)=0 whenk>K andm>M . In practice, it is\nenough that KandMare set to sufﬁciently large integers.\n5.3 Updating Formula\nWe descrive the formulas for updating variational posterior\ndistributions./glyph817D\nKα\nγkν~\n∞ →M∞ →Kdηds\n0 0,b m0 0,c Wα αb a,\nγ γb a,dnzdnx\nββ βb a, uMkmξ\nkmt\nλ\nFigure 7. A collapsed model with auxiliary variables.\n5.3.1 CVB-E Step\nA variational probability of zdnkm=1is given by\nlogq∗(zdnkm=1 )=E z¬dn/bracketleftBig\nlog/parenleftBig\nG[ανk]+n¬dn\ndk·/parenrightBig/bracketrightBig\n+Ez¬dn/bracketleftBigg\nlog/parenleftBigg\nG[βλ1]+n¬dn\n·km\nE[β]+n¬dn\n·k≥mm−1/productdisplay\nm/prime=1G[βλ2]+n¬dn\n·k>m/prime\nE[β]+n¬dn\n·k≥m/prime/parenrightBigg/bracketrightBigg\n+Ez¬dn/bracketleftBig\nlogS(xdnm|m¬dn\nzk,L¬dn\nzk,c¬dn\nzk)/bracketrightBig\n+const. (37)\nwhere subscript ¬dndenotes a set of indices without dand\nn,G[x]denotes the geometric average exp(E[logx]),a n d\nSis the Student-t distribution. L¬dn\nzkis given by L¬dn\nzk=\nb¬dn\nzk\n1+b¬dn\nzkc¬dn\nzkW¬dn\nzk,w h e r em¬dn\nzk,b¬dn\nzk,W¬dn\nzk,c¬dn\nzkare ob-\ntained by substituting zdnkm forγdnkm required by Eqns.\n(23), (24), and (25) and calculating sum without zdn. Each\nterm of Eqn. (37) can be calculated efﬁciently [15, 16].\n5.3.2 CVB-M Step\nFirst,α,β andγare Gamma distributed as follows:\nq(α)∝αaα+E[s··]−1e−α(bα−/summationtext\ndE[logηd])(38)\nq(β)∝βaβ+E[t···]−1e−β(bβ−/summationtext\nkmE[logξkm])(39)\nq(γ)∝γaγ+K−1e−γ(bγ−/summationtext\nkE[log(1−˜νk)])(40)\nThen,λand˜τare Beta distributed as follows:\nq∗(λ)∝λu1+E[t··1]−1\n1 λu2+E[t··2]−1\n2 (41)\nq∗(˜νk)∝˜ν1+E[s·k]−1\nk (1−˜νk)E[γ]+E[s·>k]−1(42)\nFinally, the variational posteriors of η,ξ,s,tare given by\nq∗(ηd)∝ηE[α]−1\nd(1−ηd)nd··−1(43)\nq∗(ξkm|Z)∝ξE[β]−1\nkm(1−ξkm)n·k≥m−1(44)\nq∗(sdk=s|Z)∝/bracketleftBigndk·\ns/bracketrightBig\nG[ανk]s(45)\nq∗(tkm1=t|Z)∝/bracketleftBign·km\nt/bracketrightBig\nG[βλ1]t(46)\nq∗(tkm2=t|Z)∝/bracketleftBign·k>m\nt/bracketrightBig\nG[βλ2]t(47)\nTo calculate E[sdk](average sdkoverZ), we exactly treat\nthe case ndk·=0 and apply second-order approximation\nwhenndk·>0(see details in [15]). E[logξkm],E[tkm1],\nandE[tkm2]can be calculated in the same way.\nTo estimate F0s, we need explicitly compute the varia-\ntional posteriors of the integrated-out parameters μ,Λ.T o\ndo this, we execute the standard VB-M step once by using\nthe responsibilities q(Z)obtained in the CVB-E step.\n6. EVALUATION\nThis section reports our comparative experiments evaluat-\ning the performance of iLHA.\n313\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Piece number\n Optimally tuned\n Fully automated\nRWC-MDB-\n PreFEst [3] HTC [5]\n LHA iLHA\nJ-2001 No.1\n 75.8 79.0\n 70.7 82.2\nJ-2001 No.2\n 78.5 78.0\n 69.1 77.9\nJ-2001 No.6\n 70.4 78.3\n 49.8 71.2\nJ-2001 No.7\n 83.0 86.0\n 70.2 85.5\nJ-2001 No.8\n 85.7 84.4\n 55.9 84.6\nJ-2001 No.9\n 85.9 89.5\n 68.9 84.7\nC-2001 No.30\n 76.0 83.6\n 81.4 81.6\nC-2001 No.35\n 72.8 76.0\n 58.9 79.6\nTotal\n 79.4 82.0\n 65.8 81.7\nTable 2 . Frame-level F-measures of F0 detection.\n6.1 Experimental Conditions\nWe evaluated LHA and iLHA on the same test set used in\n[5], which consisted of nine pieces of piano and guitar soloperformances excerpted from the RWC music database [17].The ﬁrst 23 [s] of each piece were used for evaluation. The\nspectral analysis was conducted by the wavelet transform\nusing Gabor wavelets with a time resolution of 16 [ms].The values and temporal positions of actual F0s were pre-\npared by hand as ground truth. We evaluated performance\nin terms of frame-level F-measures. The priors and hyper-priors of LHA and iLHA were set to noninformative uni-form distributions.\nKandMwere set to sufﬁciently large\nnumbers, 60 and 15. iLHA is not sensitive to these values.No other tuning was require d. To output F0s at each frame,\nwe extracted bases whose expected weights\nπwere over a\nthreshold, which was optimized as in [5].\nFor comparison, we referred to the experimental results\nof PreFEst and HTC reported in [5]. Although the ground-truth data was slightly different from ours, it would be suf-\nﬁcient for roughly evaluating performance comparatively.\nThe number of bases, priors, and weighting factors werecarefully tuned by using the ground-truth data to optimize\nthe results. Although this is not realistic, the upper bounds\nof potential performance were investigated in [5].\n6.2 Experimental Results\nThe results listed in Table 2 show that the performance of\niLHA approached and sometimes surpassed that of HTC.\nThis is consistent with the empirical ﬁndings of many stud-\nies on Bayesian nonparametrics that nonparametric modelswere competitive against optimally-tuned parametric mod-els. HTC outperformed PreFEst because HTC can appro-\npriately deal with temporal continuity of spectral bases.\nThis implies that incorporating temporal modeling wouldimprove the performance of iLHA.\nThe results of LHA were worse than those of iLHA be-\ncause LHA is not based on hierarchical Bayesian modelingand requires precise priors. In fact, we conﬁrmed that theresults of PreFEst and HTC based on MAP estimation were\ndrastically degraded when we used noninformative priors.\nIn contrast, iLHA stably showed the good performance.\n7. CONCLUSION\nThis paper presented a novel statistical method for detect-ing multiple F0s in polyphonic a udio signals. The method\nallows polyphonic spectra to contain an unbounded num-\nber of spectral bases, each of which can consist of an un-bounded number of harmonic partials. These numbers can\nbe statistically inferred at the same time that F0s are esti-\nmated. Even in experimental evaluation using noninforma-\ntive priors, our automated method performed well or betterthan conventional methods manually optimized by trial anderror. To our knowledge, this is the ﬁrst attempt to apply\nBayesian nonparametrics to multipitch analysis.\nBayesian nonparametrics is an ultimate methodological\nframework avoiding the model selection problem faced invarious areas of MIR. For example, how many sections\nshould one use for structuring a musical piece? How manygroups should one use for clustering listeners according totheir tastes or musical pieces according to their contents?\nWe are freed from these problems by assuming that in the-\nory there is an inﬁnite number of classes behind observeddata. Unnecessary classes are automatically removed from\nconsideration through statistical inference. We plan to use\nthis powerful framework in a wide range of applications.\nAcknowledgement : This study was partially supported by\nJST CREST an d JSPS KAKE NHI 20800084.\n8. REFERENCES\n[1] A. Klapuri. Multipitch ana lysis of polyphonic music and\nspeech signals using an auditory model. IEEE Trans. on\nASLP , V ol. 16, No. 2, pp. 255–266, 2008.\n[2] M. Marolt. A connectionist approach to transcription of poly-\nphonic piano music. IEEE Trans. on Multimedia ,V o l .6 ,\nNo. 3, pp. 439–449, 2004.\n[3] M. Goto. A real-time music scene description system:\nPredominant-F0 estimation for detecting melody and bass\nlines in real-world audio signals. Speech Communication ,\nV ol. 43, No. 4, pp. 311–329, 2004.\n[4] H. Kameoka et al . Separation of harmonic structures based\non tied Gaussian mixture model and information criterion forconcurrent sounds. ICASSP , V ol. 4, pp. 297–300, 2004.\n[5] H. Kameoka et al . A multipitch analyzer based on har-\nmonic temporal structured clustering. IEEE Trans. on ASLP ,\nV ol. 15, No. 3, pp. 982–994, 2007.\n[6] A. Cemgil et al. A generative model for music transcription.\nIEEE Trans. on ASLP , V ol. 14, No. 2, pp. 679–694, 2006.\n[7] P. Smaragdis and J. Brown. Nonnegative matrix factorization\nfor polyphonic music transcription. WASPAA , 2003.\n[8] A. Cont. Realtime multiple pitch observation using sparse\nnon-negative constraints. ISMIR , pp. 206–211, 2006.\n[9] E. Vincent et al . Adaptive harmonic spectral decomposition\nfor multiple pitch estimation. IEEE Trans. on ASLP , V ol. 18,\nNo. 3, pp. 528–537, 2010.\n[10] N. Bertin et al . Enforcing harmonicity and smoothness in\nBayesian non-negative matrix factorization applied to poly-\nphonic music transcription. IEEE Trans. on ASLP , V ol. 18,\nNo. 3, pp. 538–549, 2010.\n[11] P. Peeling et al . Generative spectrogram factorization mod-\nels for polyphonic piano transcription. IEEE Trans. on ASLP ,\nV ol. 18, No. 3, pp. 519–527, 2010.\n[12] T. Virtanen et al . Bayesian extensions to nonnegative matrix\nfactorisation for audio signal modelling. ICASSP , 2008.\n[13] H. Attias. A variational Bayesian framework for graphical\nmodels. NIPS , pp. 209–215, 2000.\n[14] Y . W. Teh et al . Hierarchical Dirichlet processes. J. of Am.\nStat. Assoc. , V ol. 101, No. 476, pp. 1566–1581, 2006.\n[15] Y . W. Teh et al . Collapsed variational inference for HDP.\nNIPS , V ol. 20, 2008.\n[16] J. Sung et al. Latent-space variational Bayes. IEEE Trans. on\nPAMI , V ol. 30, No. 12, pp. 2236–2242, 2008.\n[17] M. Goto et al . RWC music database: Popular, classical, and\njazz music database. ISMIR , pp. 287–288, 2002.\n314\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "Automatic Mood Classification Using TF*IDF Based on Lyrics.",
        "author": [
            "Menno van Zaanen",
            "Pieter Kanters"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1417287",
        "url": "https://doi.org/10.5281/zenodo.1417287",
        "ee": "https://zenodo.org/records/1417287/files/ZaanenK10.pdf",
        "abstract": "This paper presents the outcomes of research into using lingual parts of music in an automatic mood classification system. Using a collection of lyrics and corresponding user-tagged moods, we build classifiers that classify lyrics of songs into moods. By comparing the performance of different mood frameworks (or dimensions), we examine to what extent the linguistic part of music reveals adequate information for assigning a mood category and which as- pects of mood can be classified best. Our results show that word oriented metrics provide a valuable source of information for automatic mood clas- sification of music, based on lyrics only. Metrics such as term frequencies and tf*idf values are used to measure rel- evance of words to the different mood classes. These met- rics are incorporated in a machine learning classifier setup. Different partitions of the mood plane are investigated and we show that there is no large difference in mood predic- tion based on the mood division. Predictions on the va- lence, tension and combinations of aspects lead to similar performance.",
        "zenodo_id": 1417287,
        "dblp_key": "conf/ismir/ZaanenK10",
        "keywords": [
            "lingual",
            "music",
            "automatic",
            "mood",
            "classification",
            "system",
            "lyrics",
            "moods",
            "performance",
            "frameworks"
        ],
        "content": "AUTOMATIC MOOD CLASSIFICATION\nUSING\nTF*IDF BASED ON LYRICS\nMennovan Zaanen\nTilburg Center for Cognition and Communication\nTilburg University\nTilburg, TheNetherlands\nmvzaanen@uvt.nlPieterKanters\nTilburg Center for Cognition and Communication\nTilburg University\nTilburg, The Netherlands\npieterkanters@gmail.com\nABSTRACT\nThis paper presents the outcomes of research into using\nlingual parts of music in an automatic mood classiﬁcation\nsystem. Using a collection of lyrics and corresponding\nuser-tagged moods, we build classiﬁers that classify lyrics\nof songs into moods. By comparing the performance of\ndifferent mood frameworks (or dimensions), we examine\ntowhatextentthelinguisticpartofmusicrevealsadequate\ninformation for assigning a mood category and which as-\npects of mood can be classiﬁed best.\nOur results show that word oriented metrics provide a\nvaluable source of information for automatic mood clas-\nsiﬁcation of music, based on lyrics only. Metrics such as\ntermfrequenciesandtf*idfvaluesareusedtomeasurerel-\nevance of words to the different mood classes. These met-\nricsareincorporatedinamachinelearningclassiﬁersetup.\nDifferentpartitionsofthemoodplaneareinvestigatedand\nwe show that there is no large difference in mood predic-\ntion based on the mood division. Predictions on the va-\nlence, tension and combinations of aspects lead to similar\nperformance.\n1. INTRODUCTION\nWiththecurrentboostinmusicsharing(alongsidesharing\nﬁlesinotherformats)[6],CelmaandLamere[4]statethat\nwe see a transformation from albums to individual MP3s\nand mixes. This also changes the way people interact with\ntheir music collection and the demands they place on the\nsoftware that allows this interaction.\nDuetotheincreasingsizeofonlineordigitalmusiccol-\nlections, users would like to be able to access their collec-\ntionsthroughmoreandmoreadvancedmeans[13]. Forin-\nstance,userswouldliketobeabletosearchforsongsbased\non various properties, such as year, genre, play count, on-\nline recommendation (Web 2.0) or even based on a set\nof songs used as seed to ﬁnd similar ones. One partic-\nular property that people use when creating playlists is\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnotmadeordistributedforproﬁtorcommercialadvantageandthatcopies\nbear this noticeand thefull citation on theﬁrst page.\nc/circlecopyrt2010International Society for Music Information Retrieval.mood [17]. Currently, this is often done manually by se-\nlecting songs that belong to a particular mood and nam-\ning the playlist according to the mood, such as “relaxing”.\nHereweinvestigatethepossibilityofassigningsuchinfor-\nmation automatically, without user interaction.\nIn recent years, automatic playlist generation has been\nintroduced to cope with the problem of the tedious and\ntime consuming manual playlist selection. Furthermore,\nbrowsing the entire music library manually to select songs\nfor the playlist is felt to be difﬁcult by most music listen-\ners. This becomes especially difﬁcult if music collections\nbecome prohibitively large, as the user will not know or\nremember all songs in it.\nIn fact, it turns out that people have large amounts of\nmusic in their music collection that they never even listen\nto. This phenomenon is called The Long Tail [2] as many\nsongs fall in the region of songs that are hardly ever lis-\ntenedto,whichisvisualizedasalongtailontherightsize\nin a histogram. Automatically generating playlists based\noncertainproperties,suchasmood,canexposesongsfrom\nthe long tail and allow for the user to explore “lost music”\nin their music collections.\nHere, we will focus on the automatic classiﬁcation of\nmusicintomoods,whichissometimescalled“musicemo-\ntionclassiﬁcation”[18]. Givenamusiccollectioncontain-\ning songs that do not yet have these moods assigned to\nthem (or when adding a new, untagged song to the collec-\ntion), the process automatically adds the mood tags to the\nsong, allowing selection of songs based on moods.\nWe think the melodic part of songs contains important\ninformationthatcanhelpthemoodclassiﬁcation[10]. How-\never, we will currently focus on the linguistic aspects of\nsongs only. The idea is that lyrics contain lexical items\nthat emphasize a certain mood and as such can be used\nto identify the underlying mood. Even though in spoken\nlanguage, just like in music, other aspects such as loud-\nness and pitch may also be important triggers to identify\nthe song’s emotion, we assume here that the actual words\ncan have an emotional load without being spoken or sung.\nForinstance,wordssuchas“happy”or“dead”donothave\nto be pronounced to have an emotional load. This corre-\nsponds to Beukeboom and Semin’s idea [3] that mood af-\nfectswordchoiceandthatlexicalitemscanexpressmoods.\nTherehasbeenpreviousworkontheinﬂuenceoflyrics\non the mood of a song, such as approaches that concen-\n75\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)trate on more generic properties present in the lyrics [7],\ncombining\nmusic and lyrics using LSA and vector space\nmodels[12]orusingonlythemusicalaspects[9]. Ourap-\nproach is quite similar to the work presented in [8], where\nmulti-labelclassiﬁcationisperformed,resultinginadiffer-\nent classiﬁcation task. In this paper we concentrate on the\ninﬂuence of the different divisions of classes on the ﬁnal\nresults.\n2. APPROACH\nIn this article, we describe the development of a machine\nlearningapproachtoclassifyingsongs,basedonlyricsonly,\ninto classes that describe the mood of the song [11]. For\nthis,weneedseveralcomponents. Firstly,weneedtoiden-\ntify which classes (moods) we are going to classify into.\nSecondly, we need to select a collection of features that\nallow us to describe properties of the lyrics. Using these\ncomponents, we can take a collection of lyrics (describing\nsongs),extractthefeaturesandclassifythelyricsintotheir\nmood class. This mood information can be used to auto-\nmatically create mood-oriented playlists.\n2.1 Class divisions\nWhenbuildingasystemthatcanclassifysongsintomoods,\nwe require a set of classes that describes the allowable\nmoods. For this, we follow [15] in which two dimensions\nare identiﬁed: arousal and valence. These dimensions cre-\nate a two-dimensional plane with four areas, dividing the\nplane in positive and negative parts on both dimensions.\nThemoodsinthisplanerangefrom“angry”and“nervous”\n(negative valence, positive arousal) to “happy” and “ex-\ncited” (positive valence and arousal) and from “sad” and\n“sleepy” (negative valence and arousal) to “relaxed” and\n“calm” (positive valence, negative arousal). These words\nusedhereareonlyusedasexamples, indicative ofthearea\nin the plane. Other emotionally-laden words can also be\nplaced in this plane.\nTobeabletoworkwithamoreﬁnegrainedsetofclasses,\nwe partition the arousal/valence plane into sixteen parts.\nThis divides both arousal and valence axes into four parts\n(two on the positive and two on the negative side of the\naxis). The arousal parts are called A–D and the valence\nparts 1–4, which leads to individual classes described by a\nletter and a number. Based on this division, we investigate\nfourdifferentclassdivisions. Theﬁrstdivisionusesallsix-\nteenclasses. Thisdivisioniscalled ﬁne-grained andranges\nfrom A1–D4. The second, arousal, and third, valence, fo-\ncusonlyononeaspectoftheemotionalplane. Theseclass\ndivisions are created by merging four classes. They corre-\nspond to only using A–D and 1–4 of the ﬁne-grained divi-\nsion,respectively. Finally,wewillusethe Thayerdivision,\nwhichclustersallﬁne-grainedclassesintofourareasbased\non the positive/negative areas in Thayer’s arousal/valence\nplane.2.2 Features\nWe will experiment with a collection of features. These\nare divided into two classes: globalandword-based. The\nglobal features describe an aspect of the lyric as a whole.\nHere, we have experimented with very simple features,\nwhich should be treated as an informed baseline. We con-\nsider character count cc(the number of characters in a\nlyric), word count wc(the number of words in a lyric) and\nline count lc(the number of lines in a lyric).\nThe word-based features are more complex and use in-\nformation of the speciﬁc words used and their typical oc-\ncurrence in the lyrics of a particular mood. These features\nareheavilyinﬂuencedbymetricsfromtheﬁeldofinforma-\ntion retrieval [16]. In particular, we use the tf*idf metric\nand its components. This is a powerful technique to em-\nphasize the importance of a term (word) compared to all\ndocumentsinalargedocumentcollection[14]. Originally,\ntf*idfwasdevisedtosearchforrelevantdocumentsinlarge\ndocumentcollectionsgivenoneormoresearchterms. The\nmetricisusedtocomputerelevanceofthedocumentswith\nrespect to the search terms.\nIn this research, we consider the use of tf*idf to de-\nscribe the relative importance of a word for a particular\nmoodclass. Incontrasttothetypicalcontext,however,we\nstart with the lyrics of a song instead of search keywords.\nThe tf*idf value of each word in the lyrics under consider-\nation is used as weights to indicate relevance with respect\nto mood classes. This allows us to compute which mood\nis most relevant given lyrics, where the mood is described\nbythecombinedlyricsofallsongsthathavethatparticular\nmood assigned.\nThe approach sketched indicates that we take the lyrics\nof all songs of a particular mood and combine them as if\nthey are one document. This “document” can be seen as\ndescribing a particular mood. This means that there will\nbe as many documents as there are moods. Each mood\nclass corresponds to one document.\nThetf*idfmetricconsistsoftwocomponents: termfre-\nquency(tf)andtheinversedocumentfrequency(idf). These\ncomponents are multiplied when computing the tf*idf.\nThe ﬁrst word-based feature is the term frequency (tf).\nThis metric measures the importance of word tiin docu-\nment, i.e. mood, djwithni,joccurrences of the word in\ndocument dj, divided by the sum of the number of occur-\nrences of all words in document dj.\ntfi,j=ni,j/summationtext\nknk,j(1)\nIn\nthis situation, it measures the number of times a word\noccurs with a particular document (or mood). Words oc-\ncurring more often in the lyrics of a particular mood will\nhave a higher tf for that mood.\nThe problem with using term frequency is that most\nwords that typically occur very often are function words,\nsuch as “the”, “a” or “in”. These words are not likely to\nhelpinclassifyinglyricstomoodsastheydonotrepresent\nterms that typically describe a mood. What we are really\ninterestedinarewordsthatoccurinonlyasub-set(oronly\n76\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)one) of the moods. The inverse document frequency (idf)\nmeasures\ntheimportanceofthewordwithrespecttoadoc-\nument.\nidfi= log|D|\n|{dj:ti∈dj}|(2)\nThe\ntotal number of documents (representing moods)\nDis divided by the number of documents in which the\nword (t i) appears, and taking the logarithm of that quo-\ntient. Theidfmeasurestheimportanceofawordcombined\nwith a speciﬁc mood against all moods. In this particular\nsituation, idf will be high if it occurs in the text of one or\nonlyfewmoodsandwillbelowwhenitoccursinmultiple\nmoods (or even zero when it occurs with all moods).\nThe idf value by itself is not particularly useful as it is\ntoocourse-grained(especiallywhenthereareonlyahand-\nful of moods), but can be multiplied to weigh the tf value,\nresulting in the tf*idf.\ntf∗idfi,j=tfi,j×idfi (3)\nThetf*idfisusedtocalculatetherelevanceofawordfora\nparticularmood: hightf*idfvaluesindicatehighrelevance\nof the word to the mood.\nThe tf*idf provides for one particular word, a score or\nweight for each of the classes. Lyrics typically contain\nmore than one word, which allows for a more robust com-\nputation of the relevance of the mood document for the\nlyricsunderconsideration. Practically,wecancombinethe\ntf*idfvalues ofallthewordsinthelyricsforclassiﬁcation\nby adding the values of the separate words.\nTakingthelyricsofallsongsofaparticularmoodasone\ndocument results in having between four and sixteen doc-\numents (depending on the mood division). This is signiﬁ-\ncantly less than the amount of documents normally under\nconsideration in a tf*idf setting. In fact, many words will\noccur in all moods, which means that in those cases the\nidf will be zero which results in a zero tf*idf for all mood\nclasses for that word. This turns out to be a very useful\naspect of tf*idf weights in small document collections. In\nparticular, words that do not help in deciding the correct\nmood of lyrics, such as function words, are automatically\nﬁltered out, as their tf*idf value will be zero. There is no\nway these words can contribute to the ﬁnal weight of the\nlyrics, so there is no need to consider these words when\nanalyzing the lyrics.\nTo investigate whether the zero tf*idf scores really are\nuseful,wealsoexperimentedwithLaplacesmoothing,also\nknownas“add-onesmoothing”,whichreducestheamount\nofwordsthathaveazerotf*idf. Beforecomputingidf,one\nisaddedtothetotalnumberofdocuments. Thismeansthat\nthe idf will now always be non-zero, albeit very small. In\nthecasewherenormallytheidfwouldbezero,theidfwill\nnow be small and the same for all classes, but this allows\nthe system to use the information from the tf (which is not\npossible if idf is zero).\nA potential advantage of the smoothed tf*idf is that in\nthe case of all words having a zero non-smoothed tf*idf\n(for example in the case of very short lyrics), which leads\nto a zero tf*idf for all classes (and requiring a randomchoice for the class), the smoothing lets the system back-\noff to using tf. By not multiplying tf with zero (idf), the tf\nis retained in the ﬁnal score, which makes it still possible\nto classifyusing tf.\nAnother extension that is implemented normalizes over\nthelengthofthelyrics. Thetfcanbelargeriflongerlyrics\nare used (simply because more words are present in those\nlyrics). Thenormalizedtf*idfsimplydividesthetf*idfval-\nues computed from the lyrics by the length (i.e. number of\nwords)ofthelyrics. Thisshouldremovethepreferencefor\nhigher tf in longer lyrics.\nUsing tf*idf has several advantages. No linguistically\nmotivatedtoolsarerequiredandtheapproachisinherently\nlanguageindependent. Thereisnoneedforthelyricstobe\nEnglish(oranyotherlanguage). Thesimpleoccurrenceof\nthesamewordsinthetrainingdataandinthetestdatawill\nallow for classiﬁcation. Obviously, it may be the case that\ncertain words in one language may also occur in another\nlanguage, but we expect that lyrics in different languages\ntypicallyusedifferentwords. However,moreresearchinto\nthe impact of language dependency needs to be done.\n3. RESULTS\nTo measure the effectiveness (and illustrate the feasibility)\nofusingtf*idfinclassifyingsongsintomoods,wesetupa\nset of experiments. Taking a collection of songs of which\nthe mood class is known, we extract the lyrics and apply a\nmachinelearningclassiﬁertothese,allowingustoclassify\nthelyricsintoclassesbasedonthedifferentclassdivisions.\nFor each of these combinations, we discuss the results.\n3.1 Experimental settings\nTo be able to train a machine learning classiﬁer and to\nevaluate our experiments, we require a data set contain-\ning a set of pairs of song (or at least the lyrics) and the\ncorresponding mood. The data set is provided by Cray-\nonroom (http://www.crayonroom.com/), a small\ncompany creating music applications. The data set comes\nfromtheir Moody application.\nMoody lets users tag songs in iTunes in order to gener-\natemood-basedplaylists. Thetaggingisdonebymanually\nassigning colors to songs where each color corresponds to\naparticularmood. Theusercanchoosebetween16moods,\nwhich are presented in a four by four square. The colors\nprovided are similar to the hue colors of mood [1]. Note\nthat according to Voong and Beale [17] it is easier for a\nusertotagusingcolorsinsteadoftaggingusingkeywords.\nThemoodinformationisstoredinthecommentﬁeldof\nthe song’s ID3-tag and is exported to Moody’s database.\nThe information stored in Moody’s database, which con-\ntains artist and song title information combined with the\nmoodtagcanalsobeusedtoautomaticallytagnewsongs.\nThis application relies on user input to collect the mood\ninformation, but using that information it also helps users\ntagmoresongsintheirpersonalcollection. Assuch,itcan\nbe seen as a Web 2.0 application, which relies on collabo-\nrative tagging of songs.\n77\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)ThemoodsetusedbyMoodycorrespondswellwiththe\ntw\no dimensional mood plane by Thayer [15]. The sixteen\nclasses, placed in a four by four grid, can be mapped ex-\nactlyontheplanewithfourmoodtagsineachoftheareas\nin the plane.\nCrayonroom provided us with a set of 10,000 random\nentries from the Moody database. This is a subset of the\nentire database containing mostly popular songs in differ-\nent genres. Most of the songs have an English title, but\nthere has not been an explicit selection of songs that have\nlyrics (as this information is not present in the database it-\nself).\nThe information we received is a list of pairs of artist\nandsongtitle,combinedwiththecorrespondingmoodtag.\nBased on this information we started collecting the lyrics\nof the songs. Many lyrics can be found online, so we used\nthe artist and song titles to ﬁnd the lyrics automatically.\nThis was done by automatically searching a collection of\nlyrics databases given the artistand song information.\nUnfortunately, all spaces were removed from the artist\nand title ﬁelds in the database. This makes automatically\nﬁnding lyrics hard. Furthermore, there are situations such\nas “AC/DC” which may be spelled in different ways, such\nas“ACDC”,“AC-DC”,or“ACDC”.Weexperimentedwith\nseveralheuristicstore-introducespacesandreducethepunc-\ntuation problems in the artist and song ﬁelds. Applying\nthese heuristics and trying to ﬁnd the resulting artists and\nsong titles led to 5,631 lyrics to be found in the online\ndatabases.\nThe lyrics were then cleaned up and normalized. All\nHTMLinformationwasremoved,leavingplaintextlyrics.\nFurthermore, labels such as “chorus”, “repeat until fade\nout” and “4x” were removed as they are not properly part\nof the lyrics. We realize that this may inﬂuence the count\nof certain words in the lyrics. However, it is often unclear,\nforinstance,wherethechorusendsexactly. Similarly,itis\noften unclear how many repeats are required (in the case\nof “repeat until fade out”). Simply removing these labels\nwill affect the tf, but apart from manually analyzing the\nmusicandcorrectingalllyrics,wedonotseeaneasysolu-\ntion. Manual correction is not a feasible alternative at the\nmoment.\nFrom the found lyrics we extracted features and com-\nbinedthemtogetherwiththeirmoodtagintomachinelearn-\ninginstances. Eachinstancecorrespondstoonesong. This\ninformation is then used for training and testing (allowing\nfor evaluation) in a machine learning setting. The differ-\nentclassdivisionsandthedistributionsofinstancescanbe\nfound inTable 1 and Table 2.\nEachoftheexperimentsiscomputedusingtenfoldcross-\nvalidation. This meant that the collection of songs is di-\nvided into ten parts and ten experiments are performed,\nleaving out one part for evaluation. It is important to re-\nalize that for the tf*idf features, the tf*idf values for each\nof the words are recomputed for each experiment. This\nis needed, because the distribution of words in the train-\ning data may be different for each experiment. Intermedi-\nate tf*idf tables are computed from the training data ﬁrst,Fine- Arousal\ngrained A B C D\n1295 236 248 182 961\n2387 575 564 261 1,787\n3360 650 531 205 1,746Valence4253 413 338 133 1,137\n1,295 1,874 1,681 781 5,631\nTable\n1. Distribution of instances: ﬁne-grained (16\nclasses), Valence and Arousal (both 4 classes).\n1 = A3+A4+B3+B4 1,676\n2 =\nA1+A2+B1+B2 1,493\n3 =\nC1+C2+D1+D2 1,255\n4 =\nC3+C4+D3+D4 1,207\nTable\n2. Distribution of instances: Thayer division (4\nclasses).\nwhicharethenusedtocomputetheactualtf*idfvaluesfor\nthe lyrics to be classiﬁed. Similarly, the tf*idf tables will\nbe different for each of the different class divisions.\nAlsokeepinmindthatforthecomputationofthetf*idf\nvalues, all lyrics belonging to a particular class are com-\nbined to serve as one document as described above. When\ncomputing the features for each instance separately, the\ntf*idfvaluesthathavebeencomputedbeforehand(andstored\ninatf*idftable)areusedtocomputetf*idfscoresforeach\nof the classes.\nToclassifythetestdataweusedTiMBL,a k-NNclassi-\nﬁer [5]. This classiﬁer has been developed at Tilburg Uni-\nversity and contains a collection of algorithms with many\nparameters to set. In the experiments described here, we\nsimplyusedthedefaultparametersetting. Thismeansthat\nthe IB1 algorithm (k nearest distances with k= 1) is used\nwiththeweightedoverlapmetricandGainRatioweighting.\nThis means that higher accuracy scores may be reached\nwhen ﬁne-tuning the classiﬁer parameters. In this paper,\nwe are mostly interested in the feasibility of the approach.\n3.2 Experimental results\nThe results of applying TiMBL to the mood data are sum-\nmarized in Table 3. The table shows results on the four\ndifferent mood divisions and different feature settings.\nThe baseline shown in the table is the majority class\nbaseline. This shows that the data is relatively well bal-\nanced as can also be seen from Tables 1 and 2. Keep\nin mind that the Arousal, Valence, and Thayer divisions\nall contain four classes, whereas ﬁne-grained is a 16 class\ndivision. A completely random distribution of instances\nwould lead to a baseline of 25.00 (four classes) and 6.25\n(sixteen classes).\nAll global features and all of their combinations have\nworse performance with respect to the baseline. It turns\nout that the information present in these features is simply\nnot speciﬁc enough. For instance, one of the initial ideas\nwe had before we started this research, that the length of\nthelyricsmaybedifferentforlyricsinthedifferentclasses,\n78\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)Arousal Valence Thayer Fine-grained\nBaseline 33.28 31.74 29.76 11.54\ncc 30.12 (1.77) 29.02 (1.08) 28.15 (1.92) 9.73 (0.67)\nwc 31.44 (0.84) 32.59 (1.62) 28.16 (1.65) 11.06 (1.09)\nlc 31.81 (1.45) 29.37 (1.69) 27.58 (0.85) 9.85 (0.84)\ncc+wc 29.02 (2.26) 28.84 (1.71) 28.47 (2.00) 8.77 (0.90)\ncc+lc 29.61 (1.13) 27.77 (1.74) 26.94 (1.74) 8.12 (0.78)\nwc+lc 28.92 (1.28) 28.43 (2.05) 27.01 (1.08) 7.74 (0.91)\ncc+wc+lc 28.42 (1.69) 27.65 (1.96) 27.03 (1.84) 8.08 (0.84)\ntf 33.36 (0.18) 31.77 (0.13) 29.85 (0.15) 11.45 (0.12)\ntf*idf 77.18 (1.02) 76.26 (2.03) 75.79 (1.34) 70.89 (1.51)\ntf+tf*idf 77.23 (1.02) 76.29 (2.07) 75.85 (1.37) 70.89 (1.50)\nTable 3. Mean accuracy and standard deviation of different feature settings and class divisions.\nis\nnottrue. Thisisalsoreﬂectedintheotherfeaturesused.\nToallowforclassiﬁcationintomoods,morespeciﬁcinfor-\nmation is required.\nAll advanced features based on tf*idf (apart from tf by\nitself) signiﬁcantly outperform the baseline. The tf by it-\nself does not help to classify songs into the correct mood\nclass. The reason for this is that the words that occur most\nfrequently(typicallyfunctionwords,suchas“the”)greatly\noutnumber the content words. Even when function words\noccur approximately the same for each class, minor vari-\nations still have a large impact with respect to otherwise\nmore useful content words, which normally do not occur\nvery often. For classiﬁcation purposes, we are mostly in-\nterested in words that help identifying the mood of the\nlyrics. Words that occur in the lyrics of all moods have\nlimited usefulness. Unfortunately, because function words\noccur most frequent, they have a large impact on the tf.\nWhen adding idf, which happens with the tf*idf fea-\ntures, the accuracy goes up dramatically. Adding idf re-\nmoves(orreduces)allweightsforwordsthatoccurinlyrics\nof all or several classes. This means that only words that\ndonotoccurinlyricsofallmoodsremainorhaveahigher\nimpact. This metric seems to coincide with the notion of\nusefulness that we are trying to implement.\nThe words with the highest tf*idf score for a particular\nclassarenotwhatweexpected. Thesearewordsthatoccur\nvery frequently in only one song. Examples of words with\nhigh idf and tf are: “aaah”, “dah”, or “yoy”. However,\nthese words are not often used in classiﬁcation either.\nThe results of the experiments using a combination of\nthe tf*idf metric and the tf metric is slightly better than\nsimplyusingthetf*idfmetriconly. Weexpectthatthishas\nto do with the situation where there are none or not many\nwords with a non-zero tf*idf in the lyrics. This may oc-\ncur,forinstance,whenasongcontainsnon-Englishlyrics.\nIn that case, the tf*idf values are too often zero, but the\ntf features allow for a back-off strategy. The differences,\nhowever, are minor and non-signiﬁcant.\nAs mentioned earlier, we have also implemented a nor-\nmalized (dividing by the number of words in all the lyrics\nof a particular mood) and a Laplace smoothed version of\nthe metrics. Since the normalization and smoothing can\nalso be applied together, this leads to three more versionsof all the tf and tf*idf experiments described so far. The\nresults of these experiments are not shown in Table 3 as\nthese experiments yield exactly the same mean accuracy\nandstandarddeviationasthenormaltfandtf*idffeatures.\nObviously, the tf and tf*idf values for each of the words\naredifferentineachcase,buttheclassiﬁcationisthesame.\nWethinkthatlengthnormalizationdoesnothelpinclas-\nsiﬁcation because the length of the lyrics in each class is\ntoo similar. This means that all tf*idf values are divided\nby a (near) constant. Effectively, similar ﬁgures are then\nusedtoclassify. Furthermore,Laplacesmoothingdoesnot\nhelp because most of the time the lyrics contain enough\nnon-zero idf words to allow for correct classiﬁcation. Ad-\nditionally, when smoothing, words occurring in all classes\nareusedaswell,butsincetheyoccurinallclasses,theydo\nnot have a large impact in deciding the correct class.\nThe different class divisions (arousal, valence, Thayer,\nand ﬁne-grained) were devised to show which aspect of\nemotioniseasiesttoclassify. Theresultsshowthatatleast\nusingthetechniquedescribedhere,thereisnocleardiffer-\nence. We originally thought that valence would be easier\nto classify. Positive or negative moods can easily be de-\nscribed using words such as “happy” and “sad”. However,\nthe intensity (described by arousal) can just as easily be\nclassiﬁed. Most interesting is the fact that the ﬁne-grained\nclass division can be classiﬁed effectively as well. Re-\nmember that the ﬁne-grained division has sixteen classes\nwhereas the other divisions only have four.\n4. CONCLUSION AND FUTURE WORK\nThis paper describes an attempt to design, implement and\nevaluateamood-basedclassiﬁcationsystemformusicbased\non lyrics. The ultimate aim is the automatic assignment\nof mood-based tags for songs in a users’ music database,\nbased on lyrics only. By automatically assigning mood\ntags to songs, users do not have to assign mood properties\nto all songs in a potentially large music collection man-\nually. Having access to the mood information ultimately\nallows for the easy creation of playlists based on moods.\nTo measure the usefulness of words in lyrics with re-\nspect to the mood classes, we used a standard information\nretrieval metric: tf*idf. This metric is normally used to\n79\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)measure relevance of terms with respect to documents in\na lar\nge document collection, but when the same metric is\nusedinaverysmallsetofdocuments,itshowssomeinter-\nesting and useful properties. The main property used here\nisthatinverysmalldocumentcollections,thetf*idfﬁlters\nout words occurring in all documents. These are words\nthat are not useful for ﬁnding out which document (mood\nin our case) ﬁts best.\nTheresultsshowthatthetf*idffeatureimprovesthere-\nsults signiﬁcantly with respect to the majority class base-\nline. Thisshowsthattf*idfcanbeusedeffectivelytoiden-\ntify words that typically describe mood aspects of lyrics.\nThis outcome shows that the lingual part of music reveals\nuseful information on mood.\nOne has to keep in mind that the experiments reported\nhere only take the linguistic aspects of songs into account.\nIn order to improve results further, other characteristics,\nsuch as tempo, timbre or key, should be taken into con-\nsideration as well. However, using these aspects requires\naccess to the music (inaddition to the lyrics).\nTheevaluationofthecurrentsystemisagainstthemood\ntags provided by Moody. These tags are based on human\nannotation. However,itmaybethatdifferentpeopleassign\n(slightly) different tags to the songs. We do not know ex-\nactlyhowthisishandledintheMoodyapplication,butthis\nmayhaveanimpactontheevaluationofthesystem. Also,\nwe do not know what the inter-annotator agreement is. In\nfutureresearchweneedtoconsiderthispotentialspreadof\nhumanannotation,forexamplebytakingtheconﬁdenceof\nthe systemfor the different moods into account.\nA related problem is that the boundaries between the\ndifferentmoodsisnotclear-cut. Apossiblesolutiontothis\nproblem and that of the possible variation of annotation\nis to evaluate using a metric that takes distances between\nmoods in to account. For instance, classifying A2 instead\nof A1 is better than classifying D4.\n5. REFERENCES\n[1] A. Albert. Color hue and mood: The effect of varia-\ntion of red hues on positive and negative mood states.\nJournal of the Behavioral Sciences, 1, 2007.\n[2] Chris Anderson. The long tail. Wired, 12.10, October\n2004.\n[3] C.J. Beukeboom and G.R. Semin. How mood turns on\nlanguage. Journal of experimental social psychology,\n42(5):553–566, 2005.\n[4] O.CelmaandP.Lamere.Tutorialonmusicrecommen-\ndation.EightInternationalConferenceonMusicInfor-\nmation Retrieval: ISMIR 2007; Vienna, Austria, 2007.\n[5] WalterDaelemans,JakubZavrel,KovanderSloot,and\nAntal van den Bosch. TiMBL: Tilburg memory-based\nlearner. Technical Report ILK 02-10, Tilburg Univer-\nsity, Tilburg, the Netherlands, November 2002.\n[6] B. Haring. Beyond the Charts: Mp3 and the Digital\nMusic Revolution. JM Northern Media LLC, 2000.[7] Hui He, Jianming Jin, Yuhong Xiong, Bo Chen,\nWu Sun, and Ling Zhao. Language feature mining for\nmusic emotion classiﬁcation via supervised learning\nfrom lyrics. In Proceedings of the Third International\nSymposium, ISICA 2008; Wuhan, China, volume 5370\nofLecture Notes in Computer Science , pages 426–\n435, Berlin Heidelberg, Germany, December 2008.\nSpringer-Verlag.\n[8] Xiao Hu, J. Stephen Downie, and Andreas F. Ehman.\nLyrictextmininginmusicmoodclassiﬁcation.In Pro-\nceedings of the tenth International Society for Mu-\nsic Information Retrieval Conference (ISMIR); Kobe,\nJapan, pages 411–416, October 2009.\n[9] Xiao Hu, J. Stephen Downie, Cyril Laurier, Mert Bay,\nand Andreas F. Ehrmann. The 2007 mirex audio mood\nclassiﬁcation task: Lessons learned. In Proceedings\nof the 9th International Conference on Music Infor-\nmation Retrieval (ISMIR08), Philadelphia:PA, USA,\npages 462–467, 2008.\n[10] Patrik N. Juslin and Petri Laukka. Expression, percep-\ntion, and induction of musical emotions: A review and\na questionnaire study of everyday listening. Journal of\nNewMusicResearch,33(3):217–238,September2004.\n[11] Pieter Kanters. Automatic mood classiﬁcation for mu-\nsic. Master’s thesis, Tilburg University, Tilburg, the\nNetherlands, June 2009.\n[12] C. Laurier, J. Grivolla, and P. Herrera. Multimodal\nmusic mood classiﬁcation using audio and lyrics. In\nProceedings of the International Conference on Ma-\nchine Learning and Applications (ICMLA08); San\nDiego:CA, USA, pages 688–693, Los Alamitos:CA,\nUSA, 2008. IEEE Computer Society Press.\n[13] O.C. Meyers. A mood-based music classiﬁcation and\nexploration system. Master’s thesis, Massachusetts In-\nstituteof Technology, Cambridge:MA, USA, 2007.\n[14] J. Ramos. Using tf-idf to determine word relevance in\ndocumentqueries.In FirstInternationalConferenceon\nMachine Learning, New Brunswick:NJ, USA, 2003.\nRutgers University.\n[15] R.E.Thayer. TheBiopsychologyofMoodandArousal.\nOxford University Press, New York:NY, USA, 1982.\n[16] C.J.vanRijsbergen. InformationRetrieval.University\nofGlasgow,Glasgow,UK,2ndedition,1979.Printout.\n[17] Michael Voong and Russell Beale. Music organisation\nusing colour synaesthesia. In CHI ’07: CHI ’07 ex-\ntended abstracts on Human factors in computing sys-\ntems, pages 1869–1874, New York:NY, USA, 2007.\nACM Press.\n[18] Y. Yang, C. Liu, and H. Chen. Music emotion classiﬁ-\ncation: A fuzzy approach. In Proceedings of the 14th\nannual ACM international conference on Multimedia;\nSanta Barbara:CA, USA, pages 81–84, New York:NY,\nUSA, 2007. ACM Press.\n80\n11th International Society for Music Information Retrieval Conference (ISMIR 2010)"
    },
    {
        "title": "Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010, Utrecht, Netherlands, August 9-13, 2010",
        "author": [
            "J. Stephen Downie",
            "Remco C. Veltkamp"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.5651429",
        "url": "https://doi.org/10.5281/zenodo.5651429",
        "ee": null,
        "abstract": "Multi-modal dataset for music genre recognition based on six different modalities for the LMD-aligned [1] and SLAC [2] datasets. Further details are provided in [3].\n\nDescriptions of files\n\n\n\t\n\t\t\n\t\t\tLink\n\t\t\tDescription\n\t\t\n\t\n\t\n\t\t\n\t\t\tLMD-aligned_Filelist.arff\n\t\t\tFile list with 1575 music tracks selected from the LMD-aligned dataset [1] with tagtraum genre annotations [4] (only a subset of LMD-aligned is used, which includes only pieces for which all six modalities were accessible, and which includes only well-represented genres)\n\t\t\n\t\t\n\t\t\tLMD-aligned_ExtractedFeatures.tar.gz\n\t\t\tRaw audio signal and model-based features extracted with AMUSE [5]\n\t\t\n\t\t\n\t\t\tLMD-aligned_ProcessedFeatures.tar.gz\n\t\t\tProcessed features: audio signal and model-based features aggregated for 4 s time frames with 2 s step size / all other features (see the table below) with the same values for all time frames\n\t\t\n\t\t\n\t\t\tLMD-aligned_Datasets.tar.gz\n\t\t\tTraining, optimization, and test datasets for 3 splits for the recognition of 5 genres in [3]\n\t\t\n\t\t\n\t\t\tSLAC_Filelist.arff\n\t\t\tFile list with 250 music tracks from the SLAC dataset [2] (genres and sub-genres are provided in the folder structure)\n\t\t\n\t\t\n\t\t\tSLAC_ExtractedFeatures.tar.gz\n\t\t\tRaw audio signal and model-based features extracted with AMUSE [5]\n\t\t\n\t\t\n\t\t\tSLAC_ProcessedFeatures.tar.gz\n\t\t\tProcessed features: audio signal and model-based features aggregated for 4 s time frames with 2 s step size / all other features (see the table below) with the same values for all time frames\n\t\t\n\t\t\n\t\t\tSLAC_Datasets.tar.gz\n\t\t\tTraining, optimization, and test datasets for 3 splits for the recognition of 5 genres and 10 sub-genres in [3]\n\t\t\n\t\n\n\nModalities and feature sub-groups\n\n\n\t\n\t\t\n\t\t\tModality\n\t\t\tSub-group\n\t\t\t\n\t\t\tDimensions in processed\n\n\t\t\tfeatures of LMD-aligned\n\t\t\t\n\t\t\t\n\t\t\tDimensions in processed\n\n\t\t\tfeatures of SLAC\n\t\t\t\n\t\t\n\t\n\t\n\t\t\n\t\t\tAudio signal\n\t\t\tLow-level\n\t\t\t1-524\n\t\t\t1-524\n\t\t\n\t\t\n\t\t\tAudio signal\n\t\t\tSemantic\n\t\t\t525-810\n\t\t\t525-810\n\t\t\n\t\t\n\t\t\tAudio signal\n\t\t\tStructural complexity\n\t\t\t811-908\n\t\t\t811-908\n\t\t\n\t\t\n\t\t\tModel-based\n\t\t\tInstruments\n\t\t\t909-1018\n\t\t\t909-1018\n\t\t\n\t\t\n\t\t\tModel-based\n\t\t\tMoods\n\t\t\t1019-1146\n\t\t\t1019-1146\n\t\t\n\t\t\n\t\t\tModel-based\n\t\t\tVarious\n\t\t\t1147-1402\n\t\t\t1147-1402\n\t\t\n\t\t\n\t\t\tPlaylists\n\t\t\tGenres\n\t\t\t1403-1973\n\t\t\t1403-1973\n\t\t\n\t\t\n\t\t\tPlaylists\n\t\t\tStyles\n\t\t\t1974-1695\n\t\t\t1974-1695\n\t\t\n\t\t\n\t\t\tSymbolic\n\t\t\tPitch\n\t\t\t1696-1757\n\t\t\t1696-1757\n\t\t\n\t\t\n\t\t\tSymbolic\n\t\t\tMelodic\n\t\t\t1758-1781\n\t\t\t1758-1781\n\t\t\n\t\t\n\t\t\tSymbolic\n\t\t\tChords\n\t\t\t1782-1836\n\t\t\t1782-1836\n\t\t\n\t\t\n\t\t\tSymbolic\n\t\t\tRhythm\n\t\t\t1837-1935\n\t\t\t1837-1935\n\t\t\n\t\t\n\t\t\tSymbolic\n\t\t\tTempo\n\t\t\t1936-1963\n\t\t\t1936-1963\n\t\t\n\t\t\n\t\t\tSymbolic\n\t\t\tInstrument presence\n\t\t\t1964-2441\n\t\t\t1964-2441\n\t\t\n\t\t\n\t\t\tSymbolic\n\t\t\tInstruments\n\t\t\t2442-2456\n\t\t\t2442-2456\n\t\t\n\t\t\n\t\t\tSymbolic\n\t\t\tTexture\n\t\t\t2457-2480\n\t\t\t2457-2480\n\t\t\n\t\t\n\t\t\tSymbolic\n\t\t\tDynamics\n\t\t\t2481-2484\n\t\t\t2481-2484\n\t\t\n\t\t\n\t\t\tAlbum covers\n\t\t\tSIFT\n\t\t\t2485-2584\n\t\t\t2485-2584\n\t\t\n\t\t\n\t\t\tLyrics\n\t\t\tjLyrics descriptors\n\t\t\t2585-2603\n\t\t\t2585-2671\n\t\t\n\t\t\n\t\t\tLyrics\n\t\t\tBag-of-Words\n\t\t\t2604-2703\n\t\t\t\n\t\t\n\t\t\n\t\t\tLyrics\n\t\t\tDoc2Vec\n\t\t\t2704-2803\n\t\t\t\n\t\t\n\t\n",
        "zenodo_id": 5651429,
        "dblp_key": "conf/ismir/2010",
        "keywords": [
            "Audio signal",
            "Model-based",
            "Playlists",
            "Symbolic",
            "Album covers",
            "Lyrics",
            "Dimensions in processed",
            "Sub-group",
            "Genres",
            "Styles"
        ]
    }
]